<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251123.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "REArtGS: Reconstructing and Generating Articulated Objects via 3D Gaussian Splatting with Geometric and Motion Constraints", "author": "Di Wu and Liu Liu and Zhou Linli and Anran Huang and Liangtu Song and Qiaojun Yu and Qi Wu and Cewu Lu", "abstract": "Articulated objects, as prevalent entities in human life, their 3D representations play crucial roles across various applications. However, achieving both high-fidelity textured surface reconstruction and dynamic generation for articulated objects remains challenging for existing methods. In this paper, we present REArtGS, a novel framework that introduces additional geometric and motion constraints to 3D Gaussian primitives, enabling realistic surface reconstruction and generation for articulated objects. Specifically, given multi-view RGB images of arbitrary two states of articulated objects, we first introduce an unbiased Signed Distance Field (SDF) guidance to regularize Gaussian opacity fields, enhancing geometry constraints and improving surface reconstruction quality. Then we establish deformable fields for 3D Gaussians constrained by the kinematic structures of articulated objects, achieving unsupervised generation of surface meshes in unseen states. Extensive experiments on both synthetic and real datasets demonstrate our approach achieves high-quality textured surface reconstruction for given states, and enables high-fidelity surface generation for unseen states. Project site: https://sites.google.com/view/reartgs/home.", "link": "http://arxiv.org/abs/2503.06677v5", "date": "2025-11-21", "relevancy": 3.4797, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7076}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6953}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REArtGS%3A%20Reconstructing%20and%20Generating%20Articulated%20Objects%20via%203D%20Gaussian%20Splatting%20with%20Geometric%20and%20Motion%20Constraints&body=Title%3A%20REArtGS%3A%20Reconstructing%20and%20Generating%20Articulated%20Objects%20via%203D%20Gaussian%20Splatting%20with%20Geometric%20and%20Motion%20Constraints%0AAuthor%3A%20Di%20Wu%20and%20Liu%20Liu%20and%20Zhou%20Linli%20and%20Anran%20Huang%20and%20Liangtu%20Song%20and%20Qiaojun%20Yu%20and%20Qi%20Wu%20and%20Cewu%20Lu%0AAbstract%3A%20Articulated%20objects%2C%20as%20prevalent%20entities%20in%20human%20life%2C%20their%203D%20representations%20play%20crucial%20roles%20across%20various%20applications.%20However%2C%20achieving%20both%20high-fidelity%20textured%20surface%20reconstruction%20and%20dynamic%20generation%20for%20articulated%20objects%20remains%20challenging%20for%20existing%20methods.%20In%20this%20paper%2C%20we%20present%20REArtGS%2C%20a%20novel%20framework%20that%20introduces%20additional%20geometric%20and%20motion%20constraints%20to%203D%20Gaussian%20primitives%2C%20enabling%20realistic%20surface%20reconstruction%20and%20generation%20for%20articulated%20objects.%20Specifically%2C%20given%20multi-view%20RGB%20images%20of%20arbitrary%20two%20states%20of%20articulated%20objects%2C%20we%20first%20introduce%20an%20unbiased%20Signed%20Distance%20Field%20%28SDF%29%20guidance%20to%20regularize%20Gaussian%20opacity%20fields%2C%20enhancing%20geometry%20constraints%20and%20improving%20surface%20reconstruction%20quality.%20Then%20we%20establish%20deformable%20fields%20for%203D%20Gaussians%20constrained%20by%20the%20kinematic%20structures%20of%20articulated%20objects%2C%20achieving%20unsupervised%20generation%20of%20surface%20meshes%20in%20unseen%20states.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real%20datasets%20demonstrate%20our%20approach%20achieves%20high-quality%20textured%20surface%20reconstruction%20for%20given%20states%2C%20and%20enables%20high-fidelity%20surface%20generation%20for%20unseen%20states.%20Project%20site%3A%20https%3A//sites.google.com/view/reartgs/home.%0ALink%3A%20http%3A//arxiv.org/abs/2503.06677v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREArtGS%253A%2520Reconstructing%2520and%2520Generating%2520Articulated%2520Objects%2520via%25203D%2520Gaussian%2520Splatting%2520with%2520Geometric%2520and%2520Motion%2520Constraints%26entry.906535625%3DDi%2520Wu%2520and%2520Liu%2520Liu%2520and%2520Zhou%2520Linli%2520and%2520Anran%2520Huang%2520and%2520Liangtu%2520Song%2520and%2520Qiaojun%2520Yu%2520and%2520Qi%2520Wu%2520and%2520Cewu%2520Lu%26entry.1292438233%3DArticulated%2520objects%252C%2520as%2520prevalent%2520entities%2520in%2520human%2520life%252C%2520their%25203D%2520representations%2520play%2520crucial%2520roles%2520across%2520various%2520applications.%2520However%252C%2520achieving%2520both%2520high-fidelity%2520textured%2520surface%2520reconstruction%2520and%2520dynamic%2520generation%2520for%2520articulated%2520objects%2520remains%2520challenging%2520for%2520existing%2520methods.%2520In%2520this%2520paper%252C%2520we%2520present%2520REArtGS%252C%2520a%2520novel%2520framework%2520that%2520introduces%2520additional%2520geometric%2520and%2520motion%2520constraints%2520to%25203D%2520Gaussian%2520primitives%252C%2520enabling%2520realistic%2520surface%2520reconstruction%2520and%2520generation%2520for%2520articulated%2520objects.%2520Specifically%252C%2520given%2520multi-view%2520RGB%2520images%2520of%2520arbitrary%2520two%2520states%2520of%2520articulated%2520objects%252C%2520we%2520first%2520introduce%2520an%2520unbiased%2520Signed%2520Distance%2520Field%2520%2528SDF%2529%2520guidance%2520to%2520regularize%2520Gaussian%2520opacity%2520fields%252C%2520enhancing%2520geometry%2520constraints%2520and%2520improving%2520surface%2520reconstruction%2520quality.%2520Then%2520we%2520establish%2520deformable%2520fields%2520for%25203D%2520Gaussians%2520constrained%2520by%2520the%2520kinematic%2520structures%2520of%2520articulated%2520objects%252C%2520achieving%2520unsupervised%2520generation%2520of%2520surface%2520meshes%2520in%2520unseen%2520states.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520real%2520datasets%2520demonstrate%2520our%2520approach%2520achieves%2520high-quality%2520textured%2520surface%2520reconstruction%2520for%2520given%2520states%252C%2520and%2520enables%2520high-fidelity%2520surface%2520generation%2520for%2520unseen%2520states.%2520Project%2520site%253A%2520https%253A//sites.google.com/view/reartgs/home.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06677v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REArtGS%3A%20Reconstructing%20and%20Generating%20Articulated%20Objects%20via%203D%20Gaussian%20Splatting%20with%20Geometric%20and%20Motion%20Constraints&entry.906535625=Di%20Wu%20and%20Liu%20Liu%20and%20Zhou%20Linli%20and%20Anran%20Huang%20and%20Liangtu%20Song%20and%20Qiaojun%20Yu%20and%20Qi%20Wu%20and%20Cewu%20Lu&entry.1292438233=Articulated%20objects%2C%20as%20prevalent%20entities%20in%20human%20life%2C%20their%203D%20representations%20play%20crucial%20roles%20across%20various%20applications.%20However%2C%20achieving%20both%20high-fidelity%20textured%20surface%20reconstruction%20and%20dynamic%20generation%20for%20articulated%20objects%20remains%20challenging%20for%20existing%20methods.%20In%20this%20paper%2C%20we%20present%20REArtGS%2C%20a%20novel%20framework%20that%20introduces%20additional%20geometric%20and%20motion%20constraints%20to%203D%20Gaussian%20primitives%2C%20enabling%20realistic%20surface%20reconstruction%20and%20generation%20for%20articulated%20objects.%20Specifically%2C%20given%20multi-view%20RGB%20images%20of%20arbitrary%20two%20states%20of%20articulated%20objects%2C%20we%20first%20introduce%20an%20unbiased%20Signed%20Distance%20Field%20%28SDF%29%20guidance%20to%20regularize%20Gaussian%20opacity%20fields%2C%20enhancing%20geometry%20constraints%20and%20improving%20surface%20reconstruction%20quality.%20Then%20we%20establish%20deformable%20fields%20for%203D%20Gaussians%20constrained%20by%20the%20kinematic%20structures%20of%20articulated%20objects%2C%20achieving%20unsupervised%20generation%20of%20surface%20meshes%20in%20unseen%20states.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real%20datasets%20demonstrate%20our%20approach%20achieves%20high-quality%20textured%20surface%20reconstruction%20for%20given%20states%2C%20and%20enables%20high-fidelity%20surface%20generation%20for%20unseen%20states.%20Project%20site%3A%20https%3A//sites.google.com/view/reartgs/home.&entry.1838667208=http%3A//arxiv.org/abs/2503.06677v5&entry.124074799=Read"},
{"title": "SING3R-SLAM: Submap-based Indoor Monocular Gaussian SLAM with 3D Reconstruction Priors", "author": "Kunyi Li and Michael Niemeyer and Sen Wang and Stefano Gasperini and Nassir Navab and Federico Tombari", "abstract": "Recent advances in dense 3D reconstruction enable the accurate capture of local geometry; however, integrating them into SLAM is challenging due to drift and redundant point maps, which limit efficiency and downstream tasks, such as novel view synthesis. To address these issues, we propose SING3R-SLAM, a globally consistent and compact Gaussian-based dense RGB SLAM framework. The key idea is to combine locally consistent 3D reconstructions with a unified global Gaussian representation that jointly refines scene geometry and camera poses, enabling efficient and versatile 3D mapping for multiple downstream applications. SING3R-SLAM first builds locally consistent submaps through our lightweight tracking and reconstruction module, and then progressively aligns and fuses them into a global Gaussian map that enforces cross-view geometric consistency. This global map, in turn, provides feedback to correct local drift and enhance the robustness of tracking. Extensive experiments demonstrate that SING3R-SLAM achieves state-of-the-art tracking, 3D reconstruction, and novel view rendering, resulting in over 12% improvement in tracking and producing finer, more detailed geometry, all while maintaining a compact and memory-efficient global representation on real-world datasets.", "link": "http://arxiv.org/abs/2511.17207v1", "date": "2025-11-21", "relevancy": 3.364, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7677}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6257}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SING3R-SLAM%3A%20Submap-based%20Indoor%20Monocular%20Gaussian%20SLAM%20with%203D%20Reconstruction%20Priors&body=Title%3A%20SING3R-SLAM%3A%20Submap-based%20Indoor%20Monocular%20Gaussian%20SLAM%20with%203D%20Reconstruction%20Priors%0AAuthor%3A%20Kunyi%20Li%20and%20Michael%20Niemeyer%20and%20Sen%20Wang%20and%20Stefano%20Gasperini%20and%20Nassir%20Navab%20and%20Federico%20Tombari%0AAbstract%3A%20Recent%20advances%20in%20dense%203D%20reconstruction%20enable%20the%20accurate%20capture%20of%20local%20geometry%3B%20however%2C%20integrating%20them%20into%20SLAM%20is%20challenging%20due%20to%20drift%20and%20redundant%20point%20maps%2C%20which%20limit%20efficiency%20and%20downstream%20tasks%2C%20such%20as%20novel%20view%20synthesis.%20To%20address%20these%20issues%2C%20we%20propose%20SING3R-SLAM%2C%20a%20globally%20consistent%20and%20compact%20Gaussian-based%20dense%20RGB%20SLAM%20framework.%20The%20key%20idea%20is%20to%20combine%20locally%20consistent%203D%20reconstructions%20with%20a%20unified%20global%20Gaussian%20representation%20that%20jointly%20refines%20scene%20geometry%20and%20camera%20poses%2C%20enabling%20efficient%20and%20versatile%203D%20mapping%20for%20multiple%20downstream%20applications.%20SING3R-SLAM%20first%20builds%20locally%20consistent%20submaps%20through%20our%20lightweight%20tracking%20and%20reconstruction%20module%2C%20and%20then%20progressively%20aligns%20and%20fuses%20them%20into%20a%20global%20Gaussian%20map%20that%20enforces%20cross-view%20geometric%20consistency.%20This%20global%20map%2C%20in%20turn%2C%20provides%20feedback%20to%20correct%20local%20drift%20and%20enhance%20the%20robustness%20of%20tracking.%20Extensive%20experiments%20demonstrate%20that%20SING3R-SLAM%20achieves%20state-of-the-art%20tracking%2C%203D%20reconstruction%2C%20and%20novel%20view%20rendering%2C%20resulting%20in%20over%2012%25%20improvement%20in%20tracking%20and%20producing%20finer%2C%20more%20detailed%20geometry%2C%20all%20while%20maintaining%20a%20compact%20and%20memory-efficient%20global%20representation%20on%20real-world%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSING3R-SLAM%253A%2520Submap-based%2520Indoor%2520Monocular%2520Gaussian%2520SLAM%2520with%25203D%2520Reconstruction%2520Priors%26entry.906535625%3DKunyi%2520Li%2520and%2520Michael%2520Niemeyer%2520and%2520Sen%2520Wang%2520and%2520Stefano%2520Gasperini%2520and%2520Nassir%2520Navab%2520and%2520Federico%2520Tombari%26entry.1292438233%3DRecent%2520advances%2520in%2520dense%25203D%2520reconstruction%2520enable%2520the%2520accurate%2520capture%2520of%2520local%2520geometry%253B%2520however%252C%2520integrating%2520them%2520into%2520SLAM%2520is%2520challenging%2520due%2520to%2520drift%2520and%2520redundant%2520point%2520maps%252C%2520which%2520limit%2520efficiency%2520and%2520downstream%2520tasks%252C%2520such%2520as%2520novel%2520view%2520synthesis.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520SING3R-SLAM%252C%2520a%2520globally%2520consistent%2520and%2520compact%2520Gaussian-based%2520dense%2520RGB%2520SLAM%2520framework.%2520The%2520key%2520idea%2520is%2520to%2520combine%2520locally%2520consistent%25203D%2520reconstructions%2520with%2520a%2520unified%2520global%2520Gaussian%2520representation%2520that%2520jointly%2520refines%2520scene%2520geometry%2520and%2520camera%2520poses%252C%2520enabling%2520efficient%2520and%2520versatile%25203D%2520mapping%2520for%2520multiple%2520downstream%2520applications.%2520SING3R-SLAM%2520first%2520builds%2520locally%2520consistent%2520submaps%2520through%2520our%2520lightweight%2520tracking%2520and%2520reconstruction%2520module%252C%2520and%2520then%2520progressively%2520aligns%2520and%2520fuses%2520them%2520into%2520a%2520global%2520Gaussian%2520map%2520that%2520enforces%2520cross-view%2520geometric%2520consistency.%2520This%2520global%2520map%252C%2520in%2520turn%252C%2520provides%2520feedback%2520to%2520correct%2520local%2520drift%2520and%2520enhance%2520the%2520robustness%2520of%2520tracking.%2520Extensive%2520experiments%2520demonstrate%2520that%2520SING3R-SLAM%2520achieves%2520state-of-the-art%2520tracking%252C%25203D%2520reconstruction%252C%2520and%2520novel%2520view%2520rendering%252C%2520resulting%2520in%2520over%252012%2525%2520improvement%2520in%2520tracking%2520and%2520producing%2520finer%252C%2520more%2520detailed%2520geometry%252C%2520all%2520while%2520maintaining%2520a%2520compact%2520and%2520memory-efficient%2520global%2520representation%2520on%2520real-world%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SING3R-SLAM%3A%20Submap-based%20Indoor%20Monocular%20Gaussian%20SLAM%20with%203D%20Reconstruction%20Priors&entry.906535625=Kunyi%20Li%20and%20Michael%20Niemeyer%20and%20Sen%20Wang%20and%20Stefano%20Gasperini%20and%20Nassir%20Navab%20and%20Federico%20Tombari&entry.1292438233=Recent%20advances%20in%20dense%203D%20reconstruction%20enable%20the%20accurate%20capture%20of%20local%20geometry%3B%20however%2C%20integrating%20them%20into%20SLAM%20is%20challenging%20due%20to%20drift%20and%20redundant%20point%20maps%2C%20which%20limit%20efficiency%20and%20downstream%20tasks%2C%20such%20as%20novel%20view%20synthesis.%20To%20address%20these%20issues%2C%20we%20propose%20SING3R-SLAM%2C%20a%20globally%20consistent%20and%20compact%20Gaussian-based%20dense%20RGB%20SLAM%20framework.%20The%20key%20idea%20is%20to%20combine%20locally%20consistent%203D%20reconstructions%20with%20a%20unified%20global%20Gaussian%20representation%20that%20jointly%20refines%20scene%20geometry%20and%20camera%20poses%2C%20enabling%20efficient%20and%20versatile%203D%20mapping%20for%20multiple%20downstream%20applications.%20SING3R-SLAM%20first%20builds%20locally%20consistent%20submaps%20through%20our%20lightweight%20tracking%20and%20reconstruction%20module%2C%20and%20then%20progressively%20aligns%20and%20fuses%20them%20into%20a%20global%20Gaussian%20map%20that%20enforces%20cross-view%20geometric%20consistency.%20This%20global%20map%2C%20in%20turn%2C%20provides%20feedback%20to%20correct%20local%20drift%20and%20enhance%20the%20robustness%20of%20tracking.%20Extensive%20experiments%20demonstrate%20that%20SING3R-SLAM%20achieves%20state-of-the-art%20tracking%2C%203D%20reconstruction%2C%20and%20novel%20view%20rendering%2C%20resulting%20in%20over%2012%25%20improvement%20in%20tracking%20and%20producing%20finer%2C%20more%20detailed%20geometry%2C%20all%20while%20maintaining%20a%20compact%20and%20memory-efficient%20global%20representation%20on%20real-world%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2511.17207v1&entry.124074799=Read"},
{"title": "PEGS: Physics-Event Enhanced Large Spatiotemporal Motion Reconstruction via 3D Gaussian Splatting", "author": "Yijun Xu and Jingrui Zhang and Hongyi Liu and Yuhan Chen and Yuanyang Wang and Qingyao Guo and Dingwen Wang and Lei Yu and Chu He", "abstract": "Reconstruction of rigid motion over large spatiotemporal scales remains a challenging task due to limitations in modeling paradigms, severe motion blur, and insufficient physical consistency. In this work, we propose PEGS, a framework that integrates Physical priors with Event stream enhancement within a 3D Gaussian Splatting pipeline to perform deblurred target-focused modeling and motion recovery. We introduce a cohesive triple-level supervision scheme that enforces physical plausibility via an acceleration constraint, leverages event streams for high-temporal resolution guidance, and employs a Kalman regularizer to fuse multi-source observations. Furthermore, we design a motion-aware simulated annealing strategy that adaptively schedules the training process based on real-time kinematic states. We also contribute the first RGB-Event paired dataset targeting natural, fast rigid motion across diverse scenarios. Experiments show PEGS's superior performance in reconstructing motion over large spatiotemporal scales compared to mainstream dynamic methods.", "link": "http://arxiv.org/abs/2511.17116v1", "date": "2025-11-21", "relevancy": 3.3153, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7056}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6497}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PEGS%3A%20Physics-Event%20Enhanced%20Large%20Spatiotemporal%20Motion%20Reconstruction%20via%203D%20Gaussian%20Splatting&body=Title%3A%20PEGS%3A%20Physics-Event%20Enhanced%20Large%20Spatiotemporal%20Motion%20Reconstruction%20via%203D%20Gaussian%20Splatting%0AAuthor%3A%20Yijun%20Xu%20and%20Jingrui%20Zhang%20and%20Hongyi%20Liu%20and%20Yuhan%20Chen%20and%20Yuanyang%20Wang%20and%20Qingyao%20Guo%20and%20Dingwen%20Wang%20and%20Lei%20Yu%20and%20Chu%20He%0AAbstract%3A%20Reconstruction%20of%20rigid%20motion%20over%20large%20spatiotemporal%20scales%20remains%20a%20challenging%20task%20due%20to%20limitations%20in%20modeling%20paradigms%2C%20severe%20motion%20blur%2C%20and%20insufficient%20physical%20consistency.%20In%20this%20work%2C%20we%20propose%20PEGS%2C%20a%20framework%20that%20integrates%20Physical%20priors%20with%20Event%20stream%20enhancement%20within%20a%203D%20Gaussian%20Splatting%20pipeline%20to%20perform%20deblurred%20target-focused%20modeling%20and%20motion%20recovery.%20We%20introduce%20a%20cohesive%20triple-level%20supervision%20scheme%20that%20enforces%20physical%20plausibility%20via%20an%20acceleration%20constraint%2C%20leverages%20event%20streams%20for%20high-temporal%20resolution%20guidance%2C%20and%20employs%20a%20Kalman%20regularizer%20to%20fuse%20multi-source%20observations.%20Furthermore%2C%20we%20design%20a%20motion-aware%20simulated%20annealing%20strategy%20that%20adaptively%20schedules%20the%20training%20process%20based%20on%20real-time%20kinematic%20states.%20We%20also%20contribute%20the%20first%20RGB-Event%20paired%20dataset%20targeting%20natural%2C%20fast%20rigid%20motion%20across%20diverse%20scenarios.%20Experiments%20show%20PEGS%27s%20superior%20performance%20in%20reconstructing%20motion%20over%20large%20spatiotemporal%20scales%20compared%20to%20mainstream%20dynamic%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17116v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPEGS%253A%2520Physics-Event%2520Enhanced%2520Large%2520Spatiotemporal%2520Motion%2520Reconstruction%2520via%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DYijun%2520Xu%2520and%2520Jingrui%2520Zhang%2520and%2520Hongyi%2520Liu%2520and%2520Yuhan%2520Chen%2520and%2520Yuanyang%2520Wang%2520and%2520Qingyao%2520Guo%2520and%2520Dingwen%2520Wang%2520and%2520Lei%2520Yu%2520and%2520Chu%2520He%26entry.1292438233%3DReconstruction%2520of%2520rigid%2520motion%2520over%2520large%2520spatiotemporal%2520scales%2520remains%2520a%2520challenging%2520task%2520due%2520to%2520limitations%2520in%2520modeling%2520paradigms%252C%2520severe%2520motion%2520blur%252C%2520and%2520insufficient%2520physical%2520consistency.%2520In%2520this%2520work%252C%2520we%2520propose%2520PEGS%252C%2520a%2520framework%2520that%2520integrates%2520Physical%2520priors%2520with%2520Event%2520stream%2520enhancement%2520within%2520a%25203D%2520Gaussian%2520Splatting%2520pipeline%2520to%2520perform%2520deblurred%2520target-focused%2520modeling%2520and%2520motion%2520recovery.%2520We%2520introduce%2520a%2520cohesive%2520triple-level%2520supervision%2520scheme%2520that%2520enforces%2520physical%2520plausibility%2520via%2520an%2520acceleration%2520constraint%252C%2520leverages%2520event%2520streams%2520for%2520high-temporal%2520resolution%2520guidance%252C%2520and%2520employs%2520a%2520Kalman%2520regularizer%2520to%2520fuse%2520multi-source%2520observations.%2520Furthermore%252C%2520we%2520design%2520a%2520motion-aware%2520simulated%2520annealing%2520strategy%2520that%2520adaptively%2520schedules%2520the%2520training%2520process%2520based%2520on%2520real-time%2520kinematic%2520states.%2520We%2520also%2520contribute%2520the%2520first%2520RGB-Event%2520paired%2520dataset%2520targeting%2520natural%252C%2520fast%2520rigid%2520motion%2520across%2520diverse%2520scenarios.%2520Experiments%2520show%2520PEGS%2527s%2520superior%2520performance%2520in%2520reconstructing%2520motion%2520over%2520large%2520spatiotemporal%2520scales%2520compared%2520to%2520mainstream%2520dynamic%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17116v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PEGS%3A%20Physics-Event%20Enhanced%20Large%20Spatiotemporal%20Motion%20Reconstruction%20via%203D%20Gaussian%20Splatting&entry.906535625=Yijun%20Xu%20and%20Jingrui%20Zhang%20and%20Hongyi%20Liu%20and%20Yuhan%20Chen%20and%20Yuanyang%20Wang%20and%20Qingyao%20Guo%20and%20Dingwen%20Wang%20and%20Lei%20Yu%20and%20Chu%20He&entry.1292438233=Reconstruction%20of%20rigid%20motion%20over%20large%20spatiotemporal%20scales%20remains%20a%20challenging%20task%20due%20to%20limitations%20in%20modeling%20paradigms%2C%20severe%20motion%20blur%2C%20and%20insufficient%20physical%20consistency.%20In%20this%20work%2C%20we%20propose%20PEGS%2C%20a%20framework%20that%20integrates%20Physical%20priors%20with%20Event%20stream%20enhancement%20within%20a%203D%20Gaussian%20Splatting%20pipeline%20to%20perform%20deblurred%20target-focused%20modeling%20and%20motion%20recovery.%20We%20introduce%20a%20cohesive%20triple-level%20supervision%20scheme%20that%20enforces%20physical%20plausibility%20via%20an%20acceleration%20constraint%2C%20leverages%20event%20streams%20for%20high-temporal%20resolution%20guidance%2C%20and%20employs%20a%20Kalman%20regularizer%20to%20fuse%20multi-source%20observations.%20Furthermore%2C%20we%20design%20a%20motion-aware%20simulated%20annealing%20strategy%20that%20adaptively%20schedules%20the%20training%20process%20based%20on%20real-time%20kinematic%20states.%20We%20also%20contribute%20the%20first%20RGB-Event%20paired%20dataset%20targeting%20natural%2C%20fast%20rigid%20motion%20across%20diverse%20scenarios.%20Experiments%20show%20PEGS%27s%20superior%20performance%20in%20reconstructing%20motion%20over%20large%20spatiotemporal%20scales%20compared%20to%20mainstream%20dynamic%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.17116v1&entry.124074799=Read"},
{"title": "TrackGS: Optimizing COLMAP-Free 3D Gaussian Splatting with Global Track Constraints", "author": "Dongbo Shi and Shen Cao and Lubin Fan and Bojian Wu and Jinhui Guo and Ligang Liu and Renjie Chen", "abstract": "We present TrackGS, a novel method to integrate global feature tracks with 3D Gaussian Splatting (3DGS) for COLMAP-free novel view synthesis. While 3DGS delivers impressive rendering quality, its reliance on accurate precomputed camera parameters remains a significant limitation. Existing COLMAP-free approaches depend on local constraints that fail in complex scenarios. Our key innovation lies in leveraging feature tracks to establish global geometric constraints, enabling simultaneous optimization of camera parameters and 3D Gaussians. Specifically, we: (1) introduce track-constrained Gaussians that serve as geometric anchors, (2) propose novel 2D and 3D track losses to enforce multi-view consistency, and (3) derive differentiable formulations for camera intrinsics optimization. Extensive experiments on challenging real-world and synthetic datasets demonstrate state-of-the-art performance, with much lower pose error than previous methods while maintaining superior rendering quality. Our approach eliminates the need for COLMAP preprocessing, making 3DGS more accessible for practical applications.", "link": "http://arxiv.org/abs/2502.19800v3", "date": "2025-11-21", "relevancy": 3.2525, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6889}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.637}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TrackGS%3A%20Optimizing%20COLMAP-Free%203D%20Gaussian%20Splatting%20with%20Global%20Track%20Constraints&body=Title%3A%20TrackGS%3A%20Optimizing%20COLMAP-Free%203D%20Gaussian%20Splatting%20with%20Global%20Track%20Constraints%0AAuthor%3A%20Dongbo%20Shi%20and%20Shen%20Cao%20and%20Lubin%20Fan%20and%20Bojian%20Wu%20and%20Jinhui%20Guo%20and%20Ligang%20Liu%20and%20Renjie%20Chen%0AAbstract%3A%20We%20present%20TrackGS%2C%20a%20novel%20method%20to%20integrate%20global%20feature%20tracks%20with%203D%20Gaussian%20Splatting%20%283DGS%29%20for%20COLMAP-free%20novel%20view%20synthesis.%20While%203DGS%20delivers%20impressive%20rendering%20quality%2C%20its%20reliance%20on%20accurate%20precomputed%20camera%20parameters%20remains%20a%20significant%20limitation.%20Existing%20COLMAP-free%20approaches%20depend%20on%20local%20constraints%20that%20fail%20in%20complex%20scenarios.%20Our%20key%20innovation%20lies%20in%20leveraging%20feature%20tracks%20to%20establish%20global%20geometric%20constraints%2C%20enabling%20simultaneous%20optimization%20of%20camera%20parameters%20and%203D%20Gaussians.%20Specifically%2C%20we%3A%20%281%29%20introduce%20track-constrained%20Gaussians%20that%20serve%20as%20geometric%20anchors%2C%20%282%29%20propose%20novel%202D%20and%203D%20track%20losses%20to%20enforce%20multi-view%20consistency%2C%20and%20%283%29%20derive%20differentiable%20formulations%20for%20camera%20intrinsics%20optimization.%20Extensive%20experiments%20on%20challenging%20real-world%20and%20synthetic%20datasets%20demonstrate%20state-of-the-art%20performance%2C%20with%20much%20lower%20pose%20error%20than%20previous%20methods%20while%20maintaining%20superior%20rendering%20quality.%20Our%20approach%20eliminates%20the%20need%20for%20COLMAP%20preprocessing%2C%20making%203DGS%20more%20accessible%20for%20practical%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2502.19800v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrackGS%253A%2520Optimizing%2520COLMAP-Free%25203D%2520Gaussian%2520Splatting%2520with%2520Global%2520Track%2520Constraints%26entry.906535625%3DDongbo%2520Shi%2520and%2520Shen%2520Cao%2520and%2520Lubin%2520Fan%2520and%2520Bojian%2520Wu%2520and%2520Jinhui%2520Guo%2520and%2520Ligang%2520Liu%2520and%2520Renjie%2520Chen%26entry.1292438233%3DWe%2520present%2520TrackGS%252C%2520a%2520novel%2520method%2520to%2520integrate%2520global%2520feature%2520tracks%2520with%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520for%2520COLMAP-free%2520novel%2520view%2520synthesis.%2520While%25203DGS%2520delivers%2520impressive%2520rendering%2520quality%252C%2520its%2520reliance%2520on%2520accurate%2520precomputed%2520camera%2520parameters%2520remains%2520a%2520significant%2520limitation.%2520Existing%2520COLMAP-free%2520approaches%2520depend%2520on%2520local%2520constraints%2520that%2520fail%2520in%2520complex%2520scenarios.%2520Our%2520key%2520innovation%2520lies%2520in%2520leveraging%2520feature%2520tracks%2520to%2520establish%2520global%2520geometric%2520constraints%252C%2520enabling%2520simultaneous%2520optimization%2520of%2520camera%2520parameters%2520and%25203D%2520Gaussians.%2520Specifically%252C%2520we%253A%2520%25281%2529%2520introduce%2520track-constrained%2520Gaussians%2520that%2520serve%2520as%2520geometric%2520anchors%252C%2520%25282%2529%2520propose%2520novel%25202D%2520and%25203D%2520track%2520losses%2520to%2520enforce%2520multi-view%2520consistency%252C%2520and%2520%25283%2529%2520derive%2520differentiable%2520formulations%2520for%2520camera%2520intrinsics%2520optimization.%2520Extensive%2520experiments%2520on%2520challenging%2520real-world%2520and%2520synthetic%2520datasets%2520demonstrate%2520state-of-the-art%2520performance%252C%2520with%2520much%2520lower%2520pose%2520error%2520than%2520previous%2520methods%2520while%2520maintaining%2520superior%2520rendering%2520quality.%2520Our%2520approach%2520eliminates%2520the%2520need%2520for%2520COLMAP%2520preprocessing%252C%2520making%25203DGS%2520more%2520accessible%2520for%2520practical%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19800v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TrackGS%3A%20Optimizing%20COLMAP-Free%203D%20Gaussian%20Splatting%20with%20Global%20Track%20Constraints&entry.906535625=Dongbo%20Shi%20and%20Shen%20Cao%20and%20Lubin%20Fan%20and%20Bojian%20Wu%20and%20Jinhui%20Guo%20and%20Ligang%20Liu%20and%20Renjie%20Chen&entry.1292438233=We%20present%20TrackGS%2C%20a%20novel%20method%20to%20integrate%20global%20feature%20tracks%20with%203D%20Gaussian%20Splatting%20%283DGS%29%20for%20COLMAP-free%20novel%20view%20synthesis.%20While%203DGS%20delivers%20impressive%20rendering%20quality%2C%20its%20reliance%20on%20accurate%20precomputed%20camera%20parameters%20remains%20a%20significant%20limitation.%20Existing%20COLMAP-free%20approaches%20depend%20on%20local%20constraints%20that%20fail%20in%20complex%20scenarios.%20Our%20key%20innovation%20lies%20in%20leveraging%20feature%20tracks%20to%20establish%20global%20geometric%20constraints%2C%20enabling%20simultaneous%20optimization%20of%20camera%20parameters%20and%203D%20Gaussians.%20Specifically%2C%20we%3A%20%281%29%20introduce%20track-constrained%20Gaussians%20that%20serve%20as%20geometric%20anchors%2C%20%282%29%20propose%20novel%202D%20and%203D%20track%20losses%20to%20enforce%20multi-view%20consistency%2C%20and%20%283%29%20derive%20differentiable%20formulations%20for%20camera%20intrinsics%20optimization.%20Extensive%20experiments%20on%20challenging%20real-world%20and%20synthetic%20datasets%20demonstrate%20state-of-the-art%20performance%2C%20with%20much%20lower%20pose%20error%20than%20previous%20methods%20while%20maintaining%20superior%20rendering%20quality.%20Our%20approach%20eliminates%20the%20need%20for%20COLMAP%20preprocessing%2C%20making%203DGS%20more%20accessible%20for%20practical%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2502.19800v3&entry.124074799=Read"},
{"title": "MuM: Multi-View Masked Image Modeling for 3D Vision", "author": "David Nordstr\u00f6m and Johan Edstedt and Fredrik Kahl and Georg B\u00f6kman", "abstract": "Self-supervised learning on images seeks to extract meaningful visual representations from unlabeled data. When scaled to large datasets, this paradigm has achieved state-of-the-art performance and the resulting trained models such as DINOv3 have seen widespread adoption. However, most prior efforts are optimized for semantic understanding rather than geometric reasoning. One important exception is Cross-View Completion, CroCo, which is a form of masked autoencoding (MAE) tailored for 3D understanding. In this work, we continue on the path proposed by CroCo and focus on learning features tailored for 3D vision. In a nutshell, we extend MAE to arbitrarily many views of the same scene. By uniformly masking all views and employing a lightweight decoder with inter-frame attention, our approach is inherently simpler and more scalable than CroCo. We evaluate the resulting model, MuM, extensively on downstream tasks including feedforward reconstruction, dense image matching and relative pose estimation, finding that it outperforms the state-of-the-art visual encoders DINOv3 and CroCo v2.", "link": "http://arxiv.org/abs/2511.17309v1", "date": "2025-11-21", "relevancy": 3.1183, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6355}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6178}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MuM%3A%20Multi-View%20Masked%20Image%20Modeling%20for%203D%20Vision&body=Title%3A%20MuM%3A%20Multi-View%20Masked%20Image%20Modeling%20for%203D%20Vision%0AAuthor%3A%20David%20Nordstr%C3%B6m%20and%20Johan%20Edstedt%20and%20Fredrik%20Kahl%20and%20Georg%20B%C3%B6kman%0AAbstract%3A%20Self-supervised%20learning%20on%20images%20seeks%20to%20extract%20meaningful%20visual%20representations%20from%20unlabeled%20data.%20When%20scaled%20to%20large%20datasets%2C%20this%20paradigm%20has%20achieved%20state-of-the-art%20performance%20and%20the%20resulting%20trained%20models%20such%20as%20DINOv3%20have%20seen%20widespread%20adoption.%20However%2C%20most%20prior%20efforts%20are%20optimized%20for%20semantic%20understanding%20rather%20than%20geometric%20reasoning.%20One%20important%20exception%20is%20Cross-View%20Completion%2C%20CroCo%2C%20which%20is%20a%20form%20of%20masked%20autoencoding%20%28MAE%29%20tailored%20for%203D%20understanding.%20In%20this%20work%2C%20we%20continue%20on%20the%20path%20proposed%20by%20CroCo%20and%20focus%20on%20learning%20features%20tailored%20for%203D%20vision.%20In%20a%20nutshell%2C%20we%20extend%20MAE%20to%20arbitrarily%20many%20views%20of%20the%20same%20scene.%20By%20uniformly%20masking%20all%20views%20and%20employing%20a%20lightweight%20decoder%20with%20inter-frame%20attention%2C%20our%20approach%20is%20inherently%20simpler%20and%20more%20scalable%20than%20CroCo.%20We%20evaluate%20the%20resulting%20model%2C%20MuM%2C%20extensively%20on%20downstream%20tasks%20including%20feedforward%20reconstruction%2C%20dense%20image%20matching%20and%20relative%20pose%20estimation%2C%20finding%20that%20it%20outperforms%20the%20state-of-the-art%20visual%20encoders%20DINOv3%20and%20CroCo%20v2.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMuM%253A%2520Multi-View%2520Masked%2520Image%2520Modeling%2520for%25203D%2520Vision%26entry.906535625%3DDavid%2520Nordstr%25C3%25B6m%2520and%2520Johan%2520Edstedt%2520and%2520Fredrik%2520Kahl%2520and%2520Georg%2520B%25C3%25B6kman%26entry.1292438233%3DSelf-supervised%2520learning%2520on%2520images%2520seeks%2520to%2520extract%2520meaningful%2520visual%2520representations%2520from%2520unlabeled%2520data.%2520When%2520scaled%2520to%2520large%2520datasets%252C%2520this%2520paradigm%2520has%2520achieved%2520state-of-the-art%2520performance%2520and%2520the%2520resulting%2520trained%2520models%2520such%2520as%2520DINOv3%2520have%2520seen%2520widespread%2520adoption.%2520However%252C%2520most%2520prior%2520efforts%2520are%2520optimized%2520for%2520semantic%2520understanding%2520rather%2520than%2520geometric%2520reasoning.%2520One%2520important%2520exception%2520is%2520Cross-View%2520Completion%252C%2520CroCo%252C%2520which%2520is%2520a%2520form%2520of%2520masked%2520autoencoding%2520%2528MAE%2529%2520tailored%2520for%25203D%2520understanding.%2520In%2520this%2520work%252C%2520we%2520continue%2520on%2520the%2520path%2520proposed%2520by%2520CroCo%2520and%2520focus%2520on%2520learning%2520features%2520tailored%2520for%25203D%2520vision.%2520In%2520a%2520nutshell%252C%2520we%2520extend%2520MAE%2520to%2520arbitrarily%2520many%2520views%2520of%2520the%2520same%2520scene.%2520By%2520uniformly%2520masking%2520all%2520views%2520and%2520employing%2520a%2520lightweight%2520decoder%2520with%2520inter-frame%2520attention%252C%2520our%2520approach%2520is%2520inherently%2520simpler%2520and%2520more%2520scalable%2520than%2520CroCo.%2520We%2520evaluate%2520the%2520resulting%2520model%252C%2520MuM%252C%2520extensively%2520on%2520downstream%2520tasks%2520including%2520feedforward%2520reconstruction%252C%2520dense%2520image%2520matching%2520and%2520relative%2520pose%2520estimation%252C%2520finding%2520that%2520it%2520outperforms%2520the%2520state-of-the-art%2520visual%2520encoders%2520DINOv3%2520and%2520CroCo%2520v2.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MuM%3A%20Multi-View%20Masked%20Image%20Modeling%20for%203D%20Vision&entry.906535625=David%20Nordstr%C3%B6m%20and%20Johan%20Edstedt%20and%20Fredrik%20Kahl%20and%20Georg%20B%C3%B6kman&entry.1292438233=Self-supervised%20learning%20on%20images%20seeks%20to%20extract%20meaningful%20visual%20representations%20from%20unlabeled%20data.%20When%20scaled%20to%20large%20datasets%2C%20this%20paradigm%20has%20achieved%20state-of-the-art%20performance%20and%20the%20resulting%20trained%20models%20such%20as%20DINOv3%20have%20seen%20widespread%20adoption.%20However%2C%20most%20prior%20efforts%20are%20optimized%20for%20semantic%20understanding%20rather%20than%20geometric%20reasoning.%20One%20important%20exception%20is%20Cross-View%20Completion%2C%20CroCo%2C%20which%20is%20a%20form%20of%20masked%20autoencoding%20%28MAE%29%20tailored%20for%203D%20understanding.%20In%20this%20work%2C%20we%20continue%20on%20the%20path%20proposed%20by%20CroCo%20and%20focus%20on%20learning%20features%20tailored%20for%203D%20vision.%20In%20a%20nutshell%2C%20we%20extend%20MAE%20to%20arbitrarily%20many%20views%20of%20the%20same%20scene.%20By%20uniformly%20masking%20all%20views%20and%20employing%20a%20lightweight%20decoder%20with%20inter-frame%20attention%2C%20our%20approach%20is%20inherently%20simpler%20and%20more%20scalable%20than%20CroCo.%20We%20evaluate%20the%20resulting%20model%2C%20MuM%2C%20extensively%20on%20downstream%20tasks%20including%20feedforward%20reconstruction%2C%20dense%20image%20matching%20and%20relative%20pose%20estimation%2C%20finding%20that%20it%20outperforms%20the%20state-of-the-art%20visual%20encoders%20DINOv3%20and%20CroCo%20v2.&entry.1838667208=http%3A//arxiv.org/abs/2511.17309v1&entry.124074799=Read"},
{"title": "SVRecon: Sparse Voxel Rasterization for Surface Reconstruction", "author": "Seunghun Oh and Jaesung Choe and Dongjae Lee and Daeun Lee and Seunghoon Jeong and Yu-Chiang Frank Wang and Jaesik Park", "abstract": "We extend the recently proposed sparse voxel rasterization paradigm to the task of high-fidelity surface reconstruction by integrating Signed Distance Function (SDF), named SVRecon. Unlike 3D Gaussians, sparse voxels are spatially disentangled from their neighbors and have sharp boundaries, which makes them prone to local minima during optimization. Although SDF values provide a naturally smooth and continuous geometric field, preserving this smoothness across independently parameterized sparse voxels is nontrivial. To address this challenge, we promote coherent and smooth voxel-wise structure through (1) robust geometric initialization using a visual geometry model and (2) a spatial smoothness loss that enforces coherent relationships across parent-child and sibling voxel groups. Extensive experiments across various benchmarks show that our method achieves strong reconstruction accuracy while having consistently speedy convergence. The code will be made public.", "link": "http://arxiv.org/abs/2511.17364v1", "date": "2025-11-21", "relevancy": 3.0153, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6617}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5982}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SVRecon%3A%20Sparse%20Voxel%20Rasterization%20for%20Surface%20Reconstruction&body=Title%3A%20SVRecon%3A%20Sparse%20Voxel%20Rasterization%20for%20Surface%20Reconstruction%0AAuthor%3A%20Seunghun%20Oh%20and%20Jaesung%20Choe%20and%20Dongjae%20Lee%20and%20Daeun%20Lee%20and%20Seunghoon%20Jeong%20and%20Yu-Chiang%20Frank%20Wang%20and%20Jaesik%20Park%0AAbstract%3A%20We%20extend%20the%20recently%20proposed%20sparse%20voxel%20rasterization%20paradigm%20to%20the%20task%20of%20high-fidelity%20surface%20reconstruction%20by%20integrating%20Signed%20Distance%20Function%20%28SDF%29%2C%20named%20SVRecon.%20Unlike%203D%20Gaussians%2C%20sparse%20voxels%20are%20spatially%20disentangled%20from%20their%20neighbors%20and%20have%20sharp%20boundaries%2C%20which%20makes%20them%20prone%20to%20local%20minima%20during%20optimization.%20Although%20SDF%20values%20provide%20a%20naturally%20smooth%20and%20continuous%20geometric%20field%2C%20preserving%20this%20smoothness%20across%20independently%20parameterized%20sparse%20voxels%20is%20nontrivial.%20To%20address%20this%20challenge%2C%20we%20promote%20coherent%20and%20smooth%20voxel-wise%20structure%20through%20%281%29%20robust%20geometric%20initialization%20using%20a%20visual%20geometry%20model%20and%20%282%29%20a%20spatial%20smoothness%20loss%20that%20enforces%20coherent%20relationships%20across%20parent-child%20and%20sibling%20voxel%20groups.%20Extensive%20experiments%20across%20various%20benchmarks%20show%20that%20our%20method%20achieves%20strong%20reconstruction%20accuracy%20while%20having%20consistently%20speedy%20convergence.%20The%20code%20will%20be%20made%20public.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17364v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSVRecon%253A%2520Sparse%2520Voxel%2520Rasterization%2520for%2520Surface%2520Reconstruction%26entry.906535625%3DSeunghun%2520Oh%2520and%2520Jaesung%2520Choe%2520and%2520Dongjae%2520Lee%2520and%2520Daeun%2520Lee%2520and%2520Seunghoon%2520Jeong%2520and%2520Yu-Chiang%2520Frank%2520Wang%2520and%2520Jaesik%2520Park%26entry.1292438233%3DWe%2520extend%2520the%2520recently%2520proposed%2520sparse%2520voxel%2520rasterization%2520paradigm%2520to%2520the%2520task%2520of%2520high-fidelity%2520surface%2520reconstruction%2520by%2520integrating%2520Signed%2520Distance%2520Function%2520%2528SDF%2529%252C%2520named%2520SVRecon.%2520Unlike%25203D%2520Gaussians%252C%2520sparse%2520voxels%2520are%2520spatially%2520disentangled%2520from%2520their%2520neighbors%2520and%2520have%2520sharp%2520boundaries%252C%2520which%2520makes%2520them%2520prone%2520to%2520local%2520minima%2520during%2520optimization.%2520Although%2520SDF%2520values%2520provide%2520a%2520naturally%2520smooth%2520and%2520continuous%2520geometric%2520field%252C%2520preserving%2520this%2520smoothness%2520across%2520independently%2520parameterized%2520sparse%2520voxels%2520is%2520nontrivial.%2520To%2520address%2520this%2520challenge%252C%2520we%2520promote%2520coherent%2520and%2520smooth%2520voxel-wise%2520structure%2520through%2520%25281%2529%2520robust%2520geometric%2520initialization%2520using%2520a%2520visual%2520geometry%2520model%2520and%2520%25282%2529%2520a%2520spatial%2520smoothness%2520loss%2520that%2520enforces%2520coherent%2520relationships%2520across%2520parent-child%2520and%2520sibling%2520voxel%2520groups.%2520Extensive%2520experiments%2520across%2520various%2520benchmarks%2520show%2520that%2520our%2520method%2520achieves%2520strong%2520reconstruction%2520accuracy%2520while%2520having%2520consistently%2520speedy%2520convergence.%2520The%2520code%2520will%2520be%2520made%2520public.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17364v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SVRecon%3A%20Sparse%20Voxel%20Rasterization%20for%20Surface%20Reconstruction&entry.906535625=Seunghun%20Oh%20and%20Jaesung%20Choe%20and%20Dongjae%20Lee%20and%20Daeun%20Lee%20and%20Seunghoon%20Jeong%20and%20Yu-Chiang%20Frank%20Wang%20and%20Jaesik%20Park&entry.1292438233=We%20extend%20the%20recently%20proposed%20sparse%20voxel%20rasterization%20paradigm%20to%20the%20task%20of%20high-fidelity%20surface%20reconstruction%20by%20integrating%20Signed%20Distance%20Function%20%28SDF%29%2C%20named%20SVRecon.%20Unlike%203D%20Gaussians%2C%20sparse%20voxels%20are%20spatially%20disentangled%20from%20their%20neighbors%20and%20have%20sharp%20boundaries%2C%20which%20makes%20them%20prone%20to%20local%20minima%20during%20optimization.%20Although%20SDF%20values%20provide%20a%20naturally%20smooth%20and%20continuous%20geometric%20field%2C%20preserving%20this%20smoothness%20across%20independently%20parameterized%20sparse%20voxels%20is%20nontrivial.%20To%20address%20this%20challenge%2C%20we%20promote%20coherent%20and%20smooth%20voxel-wise%20structure%20through%20%281%29%20robust%20geometric%20initialization%20using%20a%20visual%20geometry%20model%20and%20%282%29%20a%20spatial%20smoothness%20loss%20that%20enforces%20coherent%20relationships%20across%20parent-child%20and%20sibling%20voxel%20groups.%20Extensive%20experiments%20across%20various%20benchmarks%20show%20that%20our%20method%20achieves%20strong%20reconstruction%20accuracy%20while%20having%20consistently%20speedy%20convergence.%20The%20code%20will%20be%20made%20public.&entry.1838667208=http%3A//arxiv.org/abs/2511.17364v1&entry.124074799=Read"},
{"title": "POMA-3D: The Point Map Way to 3D Scene Understanding", "author": "Ye Mao and Weixun Luo and Ranran Huang and Junpeng Jing and Krystian Mikolajczyk", "abstract": "In this paper, we introduce POMA-3D, the first self-supervised 3D representation model learned from point maps. Point maps encode explicit 3D coordinates on a structured 2D grid, preserving global 3D geometry while remaining compatible with the input format of 2D foundation models. To transfer rich 2D priors into POMA-3D, a view-to-scene alignment strategy is designed. Moreover, as point maps are view-dependent with respect to a canonical space, we introduce POMA-JEPA, a joint embedding-predictive architecture that enforces geometrically consistent point map features across multiple views. Additionally, we introduce ScenePoint, a point map dataset constructed from 6.5K room-level RGB-D scenes and 1M 2D image scenes to facilitate large-scale POMA-3D pretraining. Experiments show that POMA-3D serves as a strong backbone for both specialist and generalist 3D understanding. It benefits diverse tasks, including 3D question answering, embodied navigation, scene retrieval, and embodied localization, all achieved using only geometric inputs (i.e., 3D coordinates). Overall, our POMA-3D explores a point map way to 3D scene understanding, addressing the scarcity of pretrained priors and limited data in 3D representation learning. Project Page: https://matchlab-imperial.github.io/poma3d/", "link": "http://arxiv.org/abs/2511.16567v2", "date": "2025-11-21", "relevancy": 2.967, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5969}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5965}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20POMA-3D%3A%20The%20Point%20Map%20Way%20to%203D%20Scene%20Understanding&body=Title%3A%20POMA-3D%3A%20The%20Point%20Map%20Way%20to%203D%20Scene%20Understanding%0AAuthor%3A%20Ye%20Mao%20and%20Weixun%20Luo%20and%20Ranran%20Huang%20and%20Junpeng%20Jing%20and%20Krystian%20Mikolajczyk%0AAbstract%3A%20In%20this%20paper%2C%20we%20introduce%20POMA-3D%2C%20the%20first%20self-supervised%203D%20representation%20model%20learned%20from%20point%20maps.%20Point%20maps%20encode%20explicit%203D%20coordinates%20on%20a%20structured%202D%20grid%2C%20preserving%20global%203D%20geometry%20while%20remaining%20compatible%20with%20the%20input%20format%20of%202D%20foundation%20models.%20To%20transfer%20rich%202D%20priors%20into%20POMA-3D%2C%20a%20view-to-scene%20alignment%20strategy%20is%20designed.%20Moreover%2C%20as%20point%20maps%20are%20view-dependent%20with%20respect%20to%20a%20canonical%20space%2C%20we%20introduce%20POMA-JEPA%2C%20a%20joint%20embedding-predictive%20architecture%20that%20enforces%20geometrically%20consistent%20point%20map%20features%20across%20multiple%20views.%20Additionally%2C%20we%20introduce%20ScenePoint%2C%20a%20point%20map%20dataset%20constructed%20from%206.5K%20room-level%20RGB-D%20scenes%20and%201M%202D%20image%20scenes%20to%20facilitate%20large-scale%20POMA-3D%20pretraining.%20Experiments%20show%20that%20POMA-3D%20serves%20as%20a%20strong%20backbone%20for%20both%20specialist%20and%20generalist%203D%20understanding.%20It%20benefits%20diverse%20tasks%2C%20including%203D%20question%20answering%2C%20embodied%20navigation%2C%20scene%20retrieval%2C%20and%20embodied%20localization%2C%20all%20achieved%20using%20only%20geometric%20inputs%20%28i.e.%2C%203D%20coordinates%29.%20Overall%2C%20our%20POMA-3D%20explores%20a%20point%20map%20way%20to%203D%20scene%20understanding%2C%20addressing%20the%20scarcity%20of%20pretrained%20priors%20and%20limited%20data%20in%203D%20representation%20learning.%20Project%20Page%3A%20https%3A//matchlab-imperial.github.io/poma3d/%0ALink%3A%20http%3A//arxiv.org/abs/2511.16567v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPOMA-3D%253A%2520The%2520Point%2520Map%2520Way%2520to%25203D%2520Scene%2520Understanding%26entry.906535625%3DYe%2520Mao%2520and%2520Weixun%2520Luo%2520and%2520Ranran%2520Huang%2520and%2520Junpeng%2520Jing%2520and%2520Krystian%2520Mikolajczyk%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520introduce%2520POMA-3D%252C%2520the%2520first%2520self-supervised%25203D%2520representation%2520model%2520learned%2520from%2520point%2520maps.%2520Point%2520maps%2520encode%2520explicit%25203D%2520coordinates%2520on%2520a%2520structured%25202D%2520grid%252C%2520preserving%2520global%25203D%2520geometry%2520while%2520remaining%2520compatible%2520with%2520the%2520input%2520format%2520of%25202D%2520foundation%2520models.%2520To%2520transfer%2520rich%25202D%2520priors%2520into%2520POMA-3D%252C%2520a%2520view-to-scene%2520alignment%2520strategy%2520is%2520designed.%2520Moreover%252C%2520as%2520point%2520maps%2520are%2520view-dependent%2520with%2520respect%2520to%2520a%2520canonical%2520space%252C%2520we%2520introduce%2520POMA-JEPA%252C%2520a%2520joint%2520embedding-predictive%2520architecture%2520that%2520enforces%2520geometrically%2520consistent%2520point%2520map%2520features%2520across%2520multiple%2520views.%2520Additionally%252C%2520we%2520introduce%2520ScenePoint%252C%2520a%2520point%2520map%2520dataset%2520constructed%2520from%25206.5K%2520room-level%2520RGB-D%2520scenes%2520and%25201M%25202D%2520image%2520scenes%2520to%2520facilitate%2520large-scale%2520POMA-3D%2520pretraining.%2520Experiments%2520show%2520that%2520POMA-3D%2520serves%2520as%2520a%2520strong%2520backbone%2520for%2520both%2520specialist%2520and%2520generalist%25203D%2520understanding.%2520It%2520benefits%2520diverse%2520tasks%252C%2520including%25203D%2520question%2520answering%252C%2520embodied%2520navigation%252C%2520scene%2520retrieval%252C%2520and%2520embodied%2520localization%252C%2520all%2520achieved%2520using%2520only%2520geometric%2520inputs%2520%2528i.e.%252C%25203D%2520coordinates%2529.%2520Overall%252C%2520our%2520POMA-3D%2520explores%2520a%2520point%2520map%2520way%2520to%25203D%2520scene%2520understanding%252C%2520addressing%2520the%2520scarcity%2520of%2520pretrained%2520priors%2520and%2520limited%2520data%2520in%25203D%2520representation%2520learning.%2520Project%2520Page%253A%2520https%253A//matchlab-imperial.github.io/poma3d/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16567v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=POMA-3D%3A%20The%20Point%20Map%20Way%20to%203D%20Scene%20Understanding&entry.906535625=Ye%20Mao%20and%20Weixun%20Luo%20and%20Ranran%20Huang%20and%20Junpeng%20Jing%20and%20Krystian%20Mikolajczyk&entry.1292438233=In%20this%20paper%2C%20we%20introduce%20POMA-3D%2C%20the%20first%20self-supervised%203D%20representation%20model%20learned%20from%20point%20maps.%20Point%20maps%20encode%20explicit%203D%20coordinates%20on%20a%20structured%202D%20grid%2C%20preserving%20global%203D%20geometry%20while%20remaining%20compatible%20with%20the%20input%20format%20of%202D%20foundation%20models.%20To%20transfer%20rich%202D%20priors%20into%20POMA-3D%2C%20a%20view-to-scene%20alignment%20strategy%20is%20designed.%20Moreover%2C%20as%20point%20maps%20are%20view-dependent%20with%20respect%20to%20a%20canonical%20space%2C%20we%20introduce%20POMA-JEPA%2C%20a%20joint%20embedding-predictive%20architecture%20that%20enforces%20geometrically%20consistent%20point%20map%20features%20across%20multiple%20views.%20Additionally%2C%20we%20introduce%20ScenePoint%2C%20a%20point%20map%20dataset%20constructed%20from%206.5K%20room-level%20RGB-D%20scenes%20and%201M%202D%20image%20scenes%20to%20facilitate%20large-scale%20POMA-3D%20pretraining.%20Experiments%20show%20that%20POMA-3D%20serves%20as%20a%20strong%20backbone%20for%20both%20specialist%20and%20generalist%203D%20understanding.%20It%20benefits%20diverse%20tasks%2C%20including%203D%20question%20answering%2C%20embodied%20navigation%2C%20scene%20retrieval%2C%20and%20embodied%20localization%2C%20all%20achieved%20using%20only%20geometric%20inputs%20%28i.e.%2C%203D%20coordinates%29.%20Overall%2C%20our%20POMA-3D%20explores%20a%20point%20map%20way%20to%203D%20scene%20understanding%2C%20addressing%20the%20scarcity%20of%20pretrained%20priors%20and%20limited%20data%20in%203D%20representation%20learning.%20Project%20Page%3A%20https%3A//matchlab-imperial.github.io/poma3d/&entry.1838667208=http%3A//arxiv.org/abs/2511.16567v2&entry.124074799=Read"},
{"title": "Sparse Reasoning is Enough: Biological-Inspired Framework for Video Anomaly Detection with Large Pre-trained Models", "author": "He Huang and Zixuan Hu and Dongxiao Li and Yao Xiao and Ling-Yu Duan", "abstract": "Video anomaly detection (VAD) plays a vital role in real-world applications such as security surveillance, autonomous driving, and industrial monitoring. Recent advances in large pre-trained models have opened new opportunities for training-free VAD by leveraging rich prior knowledge and general reasoning capabilities. However, existing studies typically rely on dense frame-level inference, incurring high computational costs and latency. This raises a fundamental question: Is dense reasoning truly necessary when using powerful pre-trained models in VAD systems? To answer this, we propose ReCoVAD, a novel framework inspired by the dual reflex and conscious pathways of the human nervous system, enabling selective frame processing to reduce redundant computation. ReCoVAD consists of two core pathways: (i) a Reflex pathway that uses a lightweight CLIP-based module to fuse visual features with prototype prompts and produce decision vectors, which query a dynamic memory of past frames and anomaly scores for fast response; and (ii) a Conscious pathway that employs a medium-scale vision-language model to generate textual event descriptions and refined anomaly scores for novel frames. It continuously updates the memory and prototype prompts, while an integrated large language model periodically reviews accumulated descriptions to identify unseen anomalies, correct errors, and refine prototypes. Extensive experiments show that ReCoVAD achieves state-of-the-art training-free performance while processing only 28.55\\% and 16.04\\% of the frames used by previous methods on the UCF-Crime and XD-Violence datasets, demonstrating that sparse reasoning is sufficient for effective large-model-based VAD.", "link": "http://arxiv.org/abs/2511.17094v1", "date": "2025-11-21", "relevancy": 2.9652, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5945}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5923}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Reasoning%20is%20Enough%3A%20Biological-Inspired%20Framework%20for%20Video%20Anomaly%20Detection%20with%20Large%20Pre-trained%20Models&body=Title%3A%20Sparse%20Reasoning%20is%20Enough%3A%20Biological-Inspired%20Framework%20for%20Video%20Anomaly%20Detection%20with%20Large%20Pre-trained%20Models%0AAuthor%3A%20He%20Huang%20and%20Zixuan%20Hu%20and%20Dongxiao%20Li%20and%20Yao%20Xiao%20and%20Ling-Yu%20Duan%0AAbstract%3A%20Video%20anomaly%20detection%20%28VAD%29%20plays%20a%20vital%20role%20in%20real-world%20applications%20such%20as%20security%20surveillance%2C%20autonomous%20driving%2C%20and%20industrial%20monitoring.%20Recent%20advances%20in%20large%20pre-trained%20models%20have%20opened%20new%20opportunities%20for%20training-free%20VAD%20by%20leveraging%20rich%20prior%20knowledge%20and%20general%20reasoning%20capabilities.%20However%2C%20existing%20studies%20typically%20rely%20on%20dense%20frame-level%20inference%2C%20incurring%20high%20computational%20costs%20and%20latency.%20This%20raises%20a%20fundamental%20question%3A%20Is%20dense%20reasoning%20truly%20necessary%20when%20using%20powerful%20pre-trained%20models%20in%20VAD%20systems%3F%20To%20answer%20this%2C%20we%20propose%20ReCoVAD%2C%20a%20novel%20framework%20inspired%20by%20the%20dual%20reflex%20and%20conscious%20pathways%20of%20the%20human%20nervous%20system%2C%20enabling%20selective%20frame%20processing%20to%20reduce%20redundant%20computation.%20ReCoVAD%20consists%20of%20two%20core%20pathways%3A%20%28i%29%20a%20Reflex%20pathway%20that%20uses%20a%20lightweight%20CLIP-based%20module%20to%20fuse%20visual%20features%20with%20prototype%20prompts%20and%20produce%20decision%20vectors%2C%20which%20query%20a%20dynamic%20memory%20of%20past%20frames%20and%20anomaly%20scores%20for%20fast%20response%3B%20and%20%28ii%29%20a%20Conscious%20pathway%20that%20employs%20a%20medium-scale%20vision-language%20model%20to%20generate%20textual%20event%20descriptions%20and%20refined%20anomaly%20scores%20for%20novel%20frames.%20It%20continuously%20updates%20the%20memory%20and%20prototype%20prompts%2C%20while%20an%20integrated%20large%20language%20model%20periodically%20reviews%20accumulated%20descriptions%20to%20identify%20unseen%20anomalies%2C%20correct%20errors%2C%20and%20refine%20prototypes.%20Extensive%20experiments%20show%20that%20ReCoVAD%20achieves%20state-of-the-art%20training-free%20performance%20while%20processing%20only%2028.55%5C%25%20and%2016.04%5C%25%20of%20the%20frames%20used%20by%20previous%20methods%20on%20the%20UCF-Crime%20and%20XD-Violence%20datasets%2C%20demonstrating%20that%20sparse%20reasoning%20is%20sufficient%20for%20effective%20large-model-based%20VAD.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Reasoning%2520is%2520Enough%253A%2520Biological-Inspired%2520Framework%2520for%2520Video%2520Anomaly%2520Detection%2520with%2520Large%2520Pre-trained%2520Models%26entry.906535625%3DHe%2520Huang%2520and%2520Zixuan%2520Hu%2520and%2520Dongxiao%2520Li%2520and%2520Yao%2520Xiao%2520and%2520Ling-Yu%2520Duan%26entry.1292438233%3DVideo%2520anomaly%2520detection%2520%2528VAD%2529%2520plays%2520a%2520vital%2520role%2520in%2520real-world%2520applications%2520such%2520as%2520security%2520surveillance%252C%2520autonomous%2520driving%252C%2520and%2520industrial%2520monitoring.%2520Recent%2520advances%2520in%2520large%2520pre-trained%2520models%2520have%2520opened%2520new%2520opportunities%2520for%2520training-free%2520VAD%2520by%2520leveraging%2520rich%2520prior%2520knowledge%2520and%2520general%2520reasoning%2520capabilities.%2520However%252C%2520existing%2520studies%2520typically%2520rely%2520on%2520dense%2520frame-level%2520inference%252C%2520incurring%2520high%2520computational%2520costs%2520and%2520latency.%2520This%2520raises%2520a%2520fundamental%2520question%253A%2520Is%2520dense%2520reasoning%2520truly%2520necessary%2520when%2520using%2520powerful%2520pre-trained%2520models%2520in%2520VAD%2520systems%253F%2520To%2520answer%2520this%252C%2520we%2520propose%2520ReCoVAD%252C%2520a%2520novel%2520framework%2520inspired%2520by%2520the%2520dual%2520reflex%2520and%2520conscious%2520pathways%2520of%2520the%2520human%2520nervous%2520system%252C%2520enabling%2520selective%2520frame%2520processing%2520to%2520reduce%2520redundant%2520computation.%2520ReCoVAD%2520consists%2520of%2520two%2520core%2520pathways%253A%2520%2528i%2529%2520a%2520Reflex%2520pathway%2520that%2520uses%2520a%2520lightweight%2520CLIP-based%2520module%2520to%2520fuse%2520visual%2520features%2520with%2520prototype%2520prompts%2520and%2520produce%2520decision%2520vectors%252C%2520which%2520query%2520a%2520dynamic%2520memory%2520of%2520past%2520frames%2520and%2520anomaly%2520scores%2520for%2520fast%2520response%253B%2520and%2520%2528ii%2529%2520a%2520Conscious%2520pathway%2520that%2520employs%2520a%2520medium-scale%2520vision-language%2520model%2520to%2520generate%2520textual%2520event%2520descriptions%2520and%2520refined%2520anomaly%2520scores%2520for%2520novel%2520frames.%2520It%2520continuously%2520updates%2520the%2520memory%2520and%2520prototype%2520prompts%252C%2520while%2520an%2520integrated%2520large%2520language%2520model%2520periodically%2520reviews%2520accumulated%2520descriptions%2520to%2520identify%2520unseen%2520anomalies%252C%2520correct%2520errors%252C%2520and%2520refine%2520prototypes.%2520Extensive%2520experiments%2520show%2520that%2520ReCoVAD%2520achieves%2520state-of-the-art%2520training-free%2520performance%2520while%2520processing%2520only%252028.55%255C%2525%2520and%252016.04%255C%2525%2520of%2520the%2520frames%2520used%2520by%2520previous%2520methods%2520on%2520the%2520UCF-Crime%2520and%2520XD-Violence%2520datasets%252C%2520demonstrating%2520that%2520sparse%2520reasoning%2520is%2520sufficient%2520for%2520effective%2520large-model-based%2520VAD.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Reasoning%20is%20Enough%3A%20Biological-Inspired%20Framework%20for%20Video%20Anomaly%20Detection%20with%20Large%20Pre-trained%20Models&entry.906535625=He%20Huang%20and%20Zixuan%20Hu%20and%20Dongxiao%20Li%20and%20Yao%20Xiao%20and%20Ling-Yu%20Duan&entry.1292438233=Video%20anomaly%20detection%20%28VAD%29%20plays%20a%20vital%20role%20in%20real-world%20applications%20such%20as%20security%20surveillance%2C%20autonomous%20driving%2C%20and%20industrial%20monitoring.%20Recent%20advances%20in%20large%20pre-trained%20models%20have%20opened%20new%20opportunities%20for%20training-free%20VAD%20by%20leveraging%20rich%20prior%20knowledge%20and%20general%20reasoning%20capabilities.%20However%2C%20existing%20studies%20typically%20rely%20on%20dense%20frame-level%20inference%2C%20incurring%20high%20computational%20costs%20and%20latency.%20This%20raises%20a%20fundamental%20question%3A%20Is%20dense%20reasoning%20truly%20necessary%20when%20using%20powerful%20pre-trained%20models%20in%20VAD%20systems%3F%20To%20answer%20this%2C%20we%20propose%20ReCoVAD%2C%20a%20novel%20framework%20inspired%20by%20the%20dual%20reflex%20and%20conscious%20pathways%20of%20the%20human%20nervous%20system%2C%20enabling%20selective%20frame%20processing%20to%20reduce%20redundant%20computation.%20ReCoVAD%20consists%20of%20two%20core%20pathways%3A%20%28i%29%20a%20Reflex%20pathway%20that%20uses%20a%20lightweight%20CLIP-based%20module%20to%20fuse%20visual%20features%20with%20prototype%20prompts%20and%20produce%20decision%20vectors%2C%20which%20query%20a%20dynamic%20memory%20of%20past%20frames%20and%20anomaly%20scores%20for%20fast%20response%3B%20and%20%28ii%29%20a%20Conscious%20pathway%20that%20employs%20a%20medium-scale%20vision-language%20model%20to%20generate%20textual%20event%20descriptions%20and%20refined%20anomaly%20scores%20for%20novel%20frames.%20It%20continuously%20updates%20the%20memory%20and%20prototype%20prompts%2C%20while%20an%20integrated%20large%20language%20model%20periodically%20reviews%20accumulated%20descriptions%20to%20identify%20unseen%20anomalies%2C%20correct%20errors%2C%20and%20refine%20prototypes.%20Extensive%20experiments%20show%20that%20ReCoVAD%20achieves%20state-of-the-art%20training-free%20performance%20while%20processing%20only%2028.55%5C%25%20and%2016.04%5C%25%20of%20the%20frames%20used%20by%20previous%20methods%20on%20the%20UCF-Crime%20and%20XD-Violence%20datasets%2C%20demonstrating%20that%20sparse%20reasoning%20is%20sufficient%20for%20effective%20large-model-based%20VAD.&entry.1838667208=http%3A//arxiv.org/abs/2511.17094v1&entry.124074799=Read"},
{"title": "DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding", "author": "Tanveer Hannan and Dimitrios Mallios and Parth Pathak and Faegheh Sardari and Thomas Seidl and Gedas Bertasius and Mohsen Fayyaz and Sunando Sengupta", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated strong multimodal reasoning capabilities on long and complex documents. However, their high memory footprint makes them impractical for deployment on resource-constrained edge devices. We present DocSLM, an efficient Small Vision-Language Model designed for long-document understanding under constrained memory resources. DocSLM incorporates a Hierarchical Multimodal Compressor that jointly encodes visual, textual, and layout information from each page into a fixed-length sequence, greatly reducing memory consumption while preserving both local and global semantics. To enable scalable processing over arbitrarily long inputs, we introduce a Streaming Abstention mechanism that operates on document segments sequentially and filters low-confidence responses using an entropy-based uncertainty calibrator. Across multiple long multimodal document benchmarks, DocSLM matches or surpasses state-of-the-art methods while using 82\\% fewer visual tokens, 75\\% fewer parameters, and 71\\% lower latency, delivering reliable multimodal document understanding on lightweight edge devices. Code and Model are available in https://github.com/Tanveer81/DocSLM.git.", "link": "http://arxiv.org/abs/2511.11313v3", "date": "2025-11-21", "relevancy": 2.9462, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5994}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5994}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DocSLM%3A%20A%20Small%20Vision-Language%20Model%20for%20Long%20Multimodal%20Document%20Understanding&body=Title%3A%20DocSLM%3A%20A%20Small%20Vision-Language%20Model%20for%20Long%20Multimodal%20Document%20Understanding%0AAuthor%3A%20Tanveer%20Hannan%20and%20Dimitrios%20Mallios%20and%20Parth%20Pathak%20and%20Faegheh%20Sardari%20and%20Thomas%20Seidl%20and%20Gedas%20Bertasius%20and%20Mohsen%20Fayyaz%20and%20Sunando%20Sengupta%0AAbstract%3A%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20strong%20multimodal%20reasoning%20capabilities%20on%20long%20and%20complex%20documents.%20However%2C%20their%20high%20memory%20footprint%20makes%20them%20impractical%20for%20deployment%20on%20resource-constrained%20edge%20devices.%20We%20present%20DocSLM%2C%20an%20efficient%20Small%20Vision-Language%20Model%20designed%20for%20long-document%20understanding%20under%20constrained%20memory%20resources.%20DocSLM%20incorporates%20a%20Hierarchical%20Multimodal%20Compressor%20that%20jointly%20encodes%20visual%2C%20textual%2C%20and%20layout%20information%20from%20each%20page%20into%20a%20fixed-length%20sequence%2C%20greatly%20reducing%20memory%20consumption%20while%20preserving%20both%20local%20and%20global%20semantics.%20To%20enable%20scalable%20processing%20over%20arbitrarily%20long%20inputs%2C%20we%20introduce%20a%20Streaming%20Abstention%20mechanism%20that%20operates%20on%20document%20segments%20sequentially%20and%20filters%20low-confidence%20responses%20using%20an%20entropy-based%20uncertainty%20calibrator.%20Across%20multiple%20long%20multimodal%20document%20benchmarks%2C%20DocSLM%20matches%20or%20surpasses%20state-of-the-art%20methods%20while%20using%2082%5C%25%20fewer%20visual%20tokens%2C%2075%5C%25%20fewer%20parameters%2C%20and%2071%5C%25%20lower%20latency%2C%20delivering%20reliable%20multimodal%20document%20understanding%20on%20lightweight%20edge%20devices.%20Code%20and%20Model%20are%20available%20in%20https%3A//github.com/Tanveer81/DocSLM.git.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11313v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocSLM%253A%2520A%2520Small%2520Vision-Language%2520Model%2520for%2520Long%2520Multimodal%2520Document%2520Understanding%26entry.906535625%3DTanveer%2520Hannan%2520and%2520Dimitrios%2520Mallios%2520and%2520Parth%2520Pathak%2520and%2520Faegheh%2520Sardari%2520and%2520Thomas%2520Seidl%2520and%2520Gedas%2520Bertasius%2520and%2520Mohsen%2520Fayyaz%2520and%2520Sunando%2520Sengupta%26entry.1292438233%3DLarge%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520strong%2520multimodal%2520reasoning%2520capabilities%2520on%2520long%2520and%2520complex%2520documents.%2520However%252C%2520their%2520high%2520memory%2520footprint%2520makes%2520them%2520impractical%2520for%2520deployment%2520on%2520resource-constrained%2520edge%2520devices.%2520We%2520present%2520DocSLM%252C%2520an%2520efficient%2520Small%2520Vision-Language%2520Model%2520designed%2520for%2520long-document%2520understanding%2520under%2520constrained%2520memory%2520resources.%2520DocSLM%2520incorporates%2520a%2520Hierarchical%2520Multimodal%2520Compressor%2520that%2520jointly%2520encodes%2520visual%252C%2520textual%252C%2520and%2520layout%2520information%2520from%2520each%2520page%2520into%2520a%2520fixed-length%2520sequence%252C%2520greatly%2520reducing%2520memory%2520consumption%2520while%2520preserving%2520both%2520local%2520and%2520global%2520semantics.%2520To%2520enable%2520scalable%2520processing%2520over%2520arbitrarily%2520long%2520inputs%252C%2520we%2520introduce%2520a%2520Streaming%2520Abstention%2520mechanism%2520that%2520operates%2520on%2520document%2520segments%2520sequentially%2520and%2520filters%2520low-confidence%2520responses%2520using%2520an%2520entropy-based%2520uncertainty%2520calibrator.%2520Across%2520multiple%2520long%2520multimodal%2520document%2520benchmarks%252C%2520DocSLM%2520matches%2520or%2520surpasses%2520state-of-the-art%2520methods%2520while%2520using%252082%255C%2525%2520fewer%2520visual%2520tokens%252C%252075%255C%2525%2520fewer%2520parameters%252C%2520and%252071%255C%2525%2520lower%2520latency%252C%2520delivering%2520reliable%2520multimodal%2520document%2520understanding%2520on%2520lightweight%2520edge%2520devices.%2520Code%2520and%2520Model%2520are%2520available%2520in%2520https%253A//github.com/Tanveer81/DocSLM.git.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11313v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DocSLM%3A%20A%20Small%20Vision-Language%20Model%20for%20Long%20Multimodal%20Document%20Understanding&entry.906535625=Tanveer%20Hannan%20and%20Dimitrios%20Mallios%20and%20Parth%20Pathak%20and%20Faegheh%20Sardari%20and%20Thomas%20Seidl%20and%20Gedas%20Bertasius%20and%20Mohsen%20Fayyaz%20and%20Sunando%20Sengupta&entry.1292438233=Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20strong%20multimodal%20reasoning%20capabilities%20on%20long%20and%20complex%20documents.%20However%2C%20their%20high%20memory%20footprint%20makes%20them%20impractical%20for%20deployment%20on%20resource-constrained%20edge%20devices.%20We%20present%20DocSLM%2C%20an%20efficient%20Small%20Vision-Language%20Model%20designed%20for%20long-document%20understanding%20under%20constrained%20memory%20resources.%20DocSLM%20incorporates%20a%20Hierarchical%20Multimodal%20Compressor%20that%20jointly%20encodes%20visual%2C%20textual%2C%20and%20layout%20information%20from%20each%20page%20into%20a%20fixed-length%20sequence%2C%20greatly%20reducing%20memory%20consumption%20while%20preserving%20both%20local%20and%20global%20semantics.%20To%20enable%20scalable%20processing%20over%20arbitrarily%20long%20inputs%2C%20we%20introduce%20a%20Streaming%20Abstention%20mechanism%20that%20operates%20on%20document%20segments%20sequentially%20and%20filters%20low-confidence%20responses%20using%20an%20entropy-based%20uncertainty%20calibrator.%20Across%20multiple%20long%20multimodal%20document%20benchmarks%2C%20DocSLM%20matches%20or%20surpasses%20state-of-the-art%20methods%20while%20using%2082%5C%25%20fewer%20visual%20tokens%2C%2075%5C%25%20fewer%20parameters%2C%20and%2071%5C%25%20lower%20latency%2C%20delivering%20reliable%20multimodal%20document%20understanding%20on%20lightweight%20edge%20devices.%20Code%20and%20Model%20are%20available%20in%20https%3A//github.com/Tanveer81/DocSLM.git.&entry.1838667208=http%3A//arxiv.org/abs/2511.11313v3&entry.124074799=Read"},
{"title": "SuperQuadricOcc: Multi-Layer Gaussian Approximation of Superquadrics for Real-Time Self-Supervised Occupancy Estimation", "author": "Seamie Hayes and Reenu Mohandas and Tim Brophy and Alexandre Boulch and Ganesh Sistu and Ciaran Eising", "abstract": "Semantic occupancy estimation enables comprehensive scene understanding for automated driving, providing dense spatial and semantic information essential for perception and planning. While Gaussian representations have been widely adopted in self-supervised occupancy estimation, the deployment of a large number of Gaussian primitives drastically increases memory requirements and is not suitable for real-time inference. In contrast, superquadrics permit reduced primitive count and lower memory requirements due to their diverse shape set. However, implementation into a self-supervised occupancy model is nontrivial due to the absence of a superquadric rasterizer to enable model supervision. Our proposed method, SuperQuadricOcc, employs a superquadric-based scene representation. By leveraging a multi-layer icosphere-tessellated Gaussian approximation of superquadrics, we enable Gaussian rasterization for supervision during training. On the Occ3D dataset, SuperQuadricOcc achieves a 75\\% reduction in memory footprint, 124\\% faster inference, and a 5.9\\% improvement in mIoU compared to previous Gaussian-based methods, without the use of temporal labels. To our knowledge, this is the first occupancy model to enable real-time inference while maintaining competitive performance. The use of superquadrics reduces the number of primitives required for scene modeling by 84\\% relative to Gaussian-based approaches. Finally, evaluation against prior methods is facilitated by our fast superquadric voxelization module. The code will be released as open source.", "link": "http://arxiv.org/abs/2511.17361v1", "date": "2025-11-21", "relevancy": 2.92, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6318}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5667}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuperQuadricOcc%3A%20Multi-Layer%20Gaussian%20Approximation%20of%20Superquadrics%20for%20Real-Time%20Self-Supervised%20Occupancy%20Estimation&body=Title%3A%20SuperQuadricOcc%3A%20Multi-Layer%20Gaussian%20Approximation%20of%20Superquadrics%20for%20Real-Time%20Self-Supervised%20Occupancy%20Estimation%0AAuthor%3A%20Seamie%20Hayes%20and%20Reenu%20Mohandas%20and%20Tim%20Brophy%20and%20Alexandre%20Boulch%20and%20Ganesh%20Sistu%20and%20Ciaran%20Eising%0AAbstract%3A%20Semantic%20occupancy%20estimation%20enables%20comprehensive%20scene%20understanding%20for%20automated%20driving%2C%20providing%20dense%20spatial%20and%20semantic%20information%20essential%20for%20perception%20and%20planning.%20While%20Gaussian%20representations%20have%20been%20widely%20adopted%20in%20self-supervised%20occupancy%20estimation%2C%20the%20deployment%20of%20a%20large%20number%20of%20Gaussian%20primitives%20drastically%20increases%20memory%20requirements%20and%20is%20not%20suitable%20for%20real-time%20inference.%20In%20contrast%2C%20superquadrics%20permit%20reduced%20primitive%20count%20and%20lower%20memory%20requirements%20due%20to%20their%20diverse%20shape%20set.%20However%2C%20implementation%20into%20a%20self-supervised%20occupancy%20model%20is%20nontrivial%20due%20to%20the%20absence%20of%20a%20superquadric%20rasterizer%20to%20enable%20model%20supervision.%20Our%20proposed%20method%2C%20SuperQuadricOcc%2C%20employs%20a%20superquadric-based%20scene%20representation.%20By%20leveraging%20a%20multi-layer%20icosphere-tessellated%20Gaussian%20approximation%20of%20superquadrics%2C%20we%20enable%20Gaussian%20rasterization%20for%20supervision%20during%20training.%20On%20the%20Occ3D%20dataset%2C%20SuperQuadricOcc%20achieves%20a%2075%5C%25%20reduction%20in%20memory%20footprint%2C%20124%5C%25%20faster%20inference%2C%20and%20a%205.9%5C%25%20improvement%20in%20mIoU%20compared%20to%20previous%20Gaussian-based%20methods%2C%20without%20the%20use%20of%20temporal%20labels.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20occupancy%20model%20to%20enable%20real-time%20inference%20while%20maintaining%20competitive%20performance.%20The%20use%20of%20superquadrics%20reduces%20the%20number%20of%20primitives%20required%20for%20scene%20modeling%20by%2084%5C%25%20relative%20to%20Gaussian-based%20approaches.%20Finally%2C%20evaluation%20against%20prior%20methods%20is%20facilitated%20by%20our%20fast%20superquadric%20voxelization%20module.%20The%20code%20will%20be%20released%20as%20open%20source.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17361v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperQuadricOcc%253A%2520Multi-Layer%2520Gaussian%2520Approximation%2520of%2520Superquadrics%2520for%2520Real-Time%2520Self-Supervised%2520Occupancy%2520Estimation%26entry.906535625%3DSeamie%2520Hayes%2520and%2520Reenu%2520Mohandas%2520and%2520Tim%2520Brophy%2520and%2520Alexandre%2520Boulch%2520and%2520Ganesh%2520Sistu%2520and%2520Ciaran%2520Eising%26entry.1292438233%3DSemantic%2520occupancy%2520estimation%2520enables%2520comprehensive%2520scene%2520understanding%2520for%2520automated%2520driving%252C%2520providing%2520dense%2520spatial%2520and%2520semantic%2520information%2520essential%2520for%2520perception%2520and%2520planning.%2520While%2520Gaussian%2520representations%2520have%2520been%2520widely%2520adopted%2520in%2520self-supervised%2520occupancy%2520estimation%252C%2520the%2520deployment%2520of%2520a%2520large%2520number%2520of%2520Gaussian%2520primitives%2520drastically%2520increases%2520memory%2520requirements%2520and%2520is%2520not%2520suitable%2520for%2520real-time%2520inference.%2520In%2520contrast%252C%2520superquadrics%2520permit%2520reduced%2520primitive%2520count%2520and%2520lower%2520memory%2520requirements%2520due%2520to%2520their%2520diverse%2520shape%2520set.%2520However%252C%2520implementation%2520into%2520a%2520self-supervised%2520occupancy%2520model%2520is%2520nontrivial%2520due%2520to%2520the%2520absence%2520of%2520a%2520superquadric%2520rasterizer%2520to%2520enable%2520model%2520supervision.%2520Our%2520proposed%2520method%252C%2520SuperQuadricOcc%252C%2520employs%2520a%2520superquadric-based%2520scene%2520representation.%2520By%2520leveraging%2520a%2520multi-layer%2520icosphere-tessellated%2520Gaussian%2520approximation%2520of%2520superquadrics%252C%2520we%2520enable%2520Gaussian%2520rasterization%2520for%2520supervision%2520during%2520training.%2520On%2520the%2520Occ3D%2520dataset%252C%2520SuperQuadricOcc%2520achieves%2520a%252075%255C%2525%2520reduction%2520in%2520memory%2520footprint%252C%2520124%255C%2525%2520faster%2520inference%252C%2520and%2520a%25205.9%255C%2525%2520improvement%2520in%2520mIoU%2520compared%2520to%2520previous%2520Gaussian-based%2520methods%252C%2520without%2520the%2520use%2520of%2520temporal%2520labels.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520occupancy%2520model%2520to%2520enable%2520real-time%2520inference%2520while%2520maintaining%2520competitive%2520performance.%2520The%2520use%2520of%2520superquadrics%2520reduces%2520the%2520number%2520of%2520primitives%2520required%2520for%2520scene%2520modeling%2520by%252084%255C%2525%2520relative%2520to%2520Gaussian-based%2520approaches.%2520Finally%252C%2520evaluation%2520against%2520prior%2520methods%2520is%2520facilitated%2520by%2520our%2520fast%2520superquadric%2520voxelization%2520module.%2520The%2520code%2520will%2520be%2520released%2520as%2520open%2520source.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17361v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuperQuadricOcc%3A%20Multi-Layer%20Gaussian%20Approximation%20of%20Superquadrics%20for%20Real-Time%20Self-Supervised%20Occupancy%20Estimation&entry.906535625=Seamie%20Hayes%20and%20Reenu%20Mohandas%20and%20Tim%20Brophy%20and%20Alexandre%20Boulch%20and%20Ganesh%20Sistu%20and%20Ciaran%20Eising&entry.1292438233=Semantic%20occupancy%20estimation%20enables%20comprehensive%20scene%20understanding%20for%20automated%20driving%2C%20providing%20dense%20spatial%20and%20semantic%20information%20essential%20for%20perception%20and%20planning.%20While%20Gaussian%20representations%20have%20been%20widely%20adopted%20in%20self-supervised%20occupancy%20estimation%2C%20the%20deployment%20of%20a%20large%20number%20of%20Gaussian%20primitives%20drastically%20increases%20memory%20requirements%20and%20is%20not%20suitable%20for%20real-time%20inference.%20In%20contrast%2C%20superquadrics%20permit%20reduced%20primitive%20count%20and%20lower%20memory%20requirements%20due%20to%20their%20diverse%20shape%20set.%20However%2C%20implementation%20into%20a%20self-supervised%20occupancy%20model%20is%20nontrivial%20due%20to%20the%20absence%20of%20a%20superquadric%20rasterizer%20to%20enable%20model%20supervision.%20Our%20proposed%20method%2C%20SuperQuadricOcc%2C%20employs%20a%20superquadric-based%20scene%20representation.%20By%20leveraging%20a%20multi-layer%20icosphere-tessellated%20Gaussian%20approximation%20of%20superquadrics%2C%20we%20enable%20Gaussian%20rasterization%20for%20supervision%20during%20training.%20On%20the%20Occ3D%20dataset%2C%20SuperQuadricOcc%20achieves%20a%2075%5C%25%20reduction%20in%20memory%20footprint%2C%20124%5C%25%20faster%20inference%2C%20and%20a%205.9%5C%25%20improvement%20in%20mIoU%20compared%20to%20previous%20Gaussian-based%20methods%2C%20without%20the%20use%20of%20temporal%20labels.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20occupancy%20model%20to%20enable%20real-time%20inference%20while%20maintaining%20competitive%20performance.%20The%20use%20of%20superquadrics%20reduces%20the%20number%20of%20primitives%20required%20for%20scene%20modeling%20by%2084%5C%25%20relative%20to%20Gaussian-based%20approaches.%20Finally%2C%20evaluation%20against%20prior%20methods%20is%20facilitated%20by%20our%20fast%20superquadric%20voxelization%20module.%20The%20code%20will%20be%20released%20as%20open%20source.&entry.1838667208=http%3A//arxiv.org/abs/2511.17361v1&entry.124074799=Read"},
{"title": "SpatialGeo:Boosting Spatial Reasoning in Multimodal LLMs via Geometry-Semantics Fusion", "author": "Jiajie Guo and Qingpeng Zhu and Jin Zeng and Xiaolong Wu and Changyong He and Weida Wang", "abstract": "Multimodal large language models (MLLMs) have achieved significant progress in image and language tasks due to the strong reasoning capability of large language models (LLMs). Nevertheless, most MLLMs suffer from limited spatial reasoning ability to interpret and infer spatial arrangements in three-dimensional space. In this work, we propose a novel vision encoder based on hierarchical fusion of geometry and semantics features, generating spatial-aware visual embedding and boosting the spatial grounding capability of MLLMs. Specifically, we first unveil that the spatial ambiguity shortcoming stems from the lossy embedding of the vision encoder utilized in most existing MLLMs (e.g., CLIP), restricted to instance-level semantic features. This motivates us to complement CLIP with the geometry features from vision-only self-supervised learning via a hierarchical adapter, enhancing the spatial awareness in the proposed SpatialGeo. The network is efficiently trained using pretrained LLaVA model and optimized with random feature dropping to avoid trivial solutions relying solely on the CLIP encoder. Experimental results show that SpatialGeo improves the accuracy in spatial reasoning tasks, enhancing state-of-the-art models by at least 8.0% in SpatialRGPT-Bench with approximately 50% less memory cost during inference. The source code is available via https://ricky-plus.github.io/SpatialGeoPages/.", "link": "http://arxiv.org/abs/2511.17308v1", "date": "2025-11-21", "relevancy": 2.9065, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5997}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5721}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpatialGeo%3ABoosting%20Spatial%20Reasoning%20in%20Multimodal%20LLMs%20via%20Geometry-Semantics%20Fusion&body=Title%3A%20SpatialGeo%3ABoosting%20Spatial%20Reasoning%20in%20Multimodal%20LLMs%20via%20Geometry-Semantics%20Fusion%0AAuthor%3A%20Jiajie%20Guo%20and%20Qingpeng%20Zhu%20and%20Jin%20Zeng%20and%20Xiaolong%20Wu%20and%20Changyong%20He%20and%20Weida%20Wang%0AAbstract%3A%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20achieved%20significant%20progress%20in%20image%20and%20language%20tasks%20due%20to%20the%20strong%20reasoning%20capability%20of%20large%20language%20models%20%28LLMs%29.%20Nevertheless%2C%20most%20MLLMs%20suffer%20from%20limited%20spatial%20reasoning%20ability%20to%20interpret%20and%20infer%20spatial%20arrangements%20in%20three-dimensional%20space.%20In%20this%20work%2C%20we%20propose%20a%20novel%20vision%20encoder%20based%20on%20hierarchical%20fusion%20of%20geometry%20and%20semantics%20features%2C%20generating%20spatial-aware%20visual%20embedding%20and%20boosting%20the%20spatial%20grounding%20capability%20of%20MLLMs.%20Specifically%2C%20we%20first%20unveil%20that%20the%20spatial%20ambiguity%20shortcoming%20stems%20from%20the%20lossy%20embedding%20of%20the%20vision%20encoder%20utilized%20in%20most%20existing%20MLLMs%20%28e.g.%2C%20CLIP%29%2C%20restricted%20to%20instance-level%20semantic%20features.%20This%20motivates%20us%20to%20complement%20CLIP%20with%20the%20geometry%20features%20from%20vision-only%20self-supervised%20learning%20via%20a%20hierarchical%20adapter%2C%20enhancing%20the%20spatial%20awareness%20in%20the%20proposed%20SpatialGeo.%20The%20network%20is%20efficiently%20trained%20using%20pretrained%20LLaVA%20model%20and%20optimized%20with%20random%20feature%20dropping%20to%20avoid%20trivial%20solutions%20relying%20solely%20on%20the%20CLIP%20encoder.%20Experimental%20results%20show%20that%20SpatialGeo%20improves%20the%20accuracy%20in%20spatial%20reasoning%20tasks%2C%20enhancing%20state-of-the-art%20models%20by%20at%20least%208.0%25%20in%20SpatialRGPT-Bench%20with%20approximately%2050%25%20less%20memory%20cost%20during%20inference.%20The%20source%20code%20is%20available%20via%20https%3A//ricky-plus.github.io/SpatialGeoPages/.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17308v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatialGeo%253ABoosting%2520Spatial%2520Reasoning%2520in%2520Multimodal%2520LLMs%2520via%2520Geometry-Semantics%2520Fusion%26entry.906535625%3DJiajie%2520Guo%2520and%2520Qingpeng%2520Zhu%2520and%2520Jin%2520Zeng%2520and%2520Xiaolong%2520Wu%2520and%2520Changyong%2520He%2520and%2520Weida%2520Wang%26entry.1292438233%3DMultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520achieved%2520significant%2520progress%2520in%2520image%2520and%2520language%2520tasks%2520due%2520to%2520the%2520strong%2520reasoning%2520capability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Nevertheless%252C%2520most%2520MLLMs%2520suffer%2520from%2520limited%2520spatial%2520reasoning%2520ability%2520to%2520interpret%2520and%2520infer%2520spatial%2520arrangements%2520in%2520three-dimensional%2520space.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520vision%2520encoder%2520based%2520on%2520hierarchical%2520fusion%2520of%2520geometry%2520and%2520semantics%2520features%252C%2520generating%2520spatial-aware%2520visual%2520embedding%2520and%2520boosting%2520the%2520spatial%2520grounding%2520capability%2520of%2520MLLMs.%2520Specifically%252C%2520we%2520first%2520unveil%2520that%2520the%2520spatial%2520ambiguity%2520shortcoming%2520stems%2520from%2520the%2520lossy%2520embedding%2520of%2520the%2520vision%2520encoder%2520utilized%2520in%2520most%2520existing%2520MLLMs%2520%2528e.g.%252C%2520CLIP%2529%252C%2520restricted%2520to%2520instance-level%2520semantic%2520features.%2520This%2520motivates%2520us%2520to%2520complement%2520CLIP%2520with%2520the%2520geometry%2520features%2520from%2520vision-only%2520self-supervised%2520learning%2520via%2520a%2520hierarchical%2520adapter%252C%2520enhancing%2520the%2520spatial%2520awareness%2520in%2520the%2520proposed%2520SpatialGeo.%2520The%2520network%2520is%2520efficiently%2520trained%2520using%2520pretrained%2520LLaVA%2520model%2520and%2520optimized%2520with%2520random%2520feature%2520dropping%2520to%2520avoid%2520trivial%2520solutions%2520relying%2520solely%2520on%2520the%2520CLIP%2520encoder.%2520Experimental%2520results%2520show%2520that%2520SpatialGeo%2520improves%2520the%2520accuracy%2520in%2520spatial%2520reasoning%2520tasks%252C%2520enhancing%2520state-of-the-art%2520models%2520by%2520at%2520least%25208.0%2525%2520in%2520SpatialRGPT-Bench%2520with%2520approximately%252050%2525%2520less%2520memory%2520cost%2520during%2520inference.%2520The%2520source%2520code%2520is%2520available%2520via%2520https%253A//ricky-plus.github.io/SpatialGeoPages/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17308v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpatialGeo%3ABoosting%20Spatial%20Reasoning%20in%20Multimodal%20LLMs%20via%20Geometry-Semantics%20Fusion&entry.906535625=Jiajie%20Guo%20and%20Qingpeng%20Zhu%20and%20Jin%20Zeng%20and%20Xiaolong%20Wu%20and%20Changyong%20He%20and%20Weida%20Wang&entry.1292438233=Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20achieved%20significant%20progress%20in%20image%20and%20language%20tasks%20due%20to%20the%20strong%20reasoning%20capability%20of%20large%20language%20models%20%28LLMs%29.%20Nevertheless%2C%20most%20MLLMs%20suffer%20from%20limited%20spatial%20reasoning%20ability%20to%20interpret%20and%20infer%20spatial%20arrangements%20in%20three-dimensional%20space.%20In%20this%20work%2C%20we%20propose%20a%20novel%20vision%20encoder%20based%20on%20hierarchical%20fusion%20of%20geometry%20and%20semantics%20features%2C%20generating%20spatial-aware%20visual%20embedding%20and%20boosting%20the%20spatial%20grounding%20capability%20of%20MLLMs.%20Specifically%2C%20we%20first%20unveil%20that%20the%20spatial%20ambiguity%20shortcoming%20stems%20from%20the%20lossy%20embedding%20of%20the%20vision%20encoder%20utilized%20in%20most%20existing%20MLLMs%20%28e.g.%2C%20CLIP%29%2C%20restricted%20to%20instance-level%20semantic%20features.%20This%20motivates%20us%20to%20complement%20CLIP%20with%20the%20geometry%20features%20from%20vision-only%20self-supervised%20learning%20via%20a%20hierarchical%20adapter%2C%20enhancing%20the%20spatial%20awareness%20in%20the%20proposed%20SpatialGeo.%20The%20network%20is%20efficiently%20trained%20using%20pretrained%20LLaVA%20model%20and%20optimized%20with%20random%20feature%20dropping%20to%20avoid%20trivial%20solutions%20relying%20solely%20on%20the%20CLIP%20encoder.%20Experimental%20results%20show%20that%20SpatialGeo%20improves%20the%20accuracy%20in%20spatial%20reasoning%20tasks%2C%20enhancing%20state-of-the-art%20models%20by%20at%20least%208.0%25%20in%20SpatialRGPT-Bench%20with%20approximately%2050%25%20less%20memory%20cost%20during%20inference.%20The%20source%20code%20is%20available%20via%20https%3A//ricky-plus.github.io/SpatialGeoPages/.&entry.1838667208=http%3A//arxiv.org/abs/2511.17308v1&entry.124074799=Read"},
{"title": "Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models", "author": "Mark Endo and Serena Yeung-Levy", "abstract": "Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models, examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities, rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space.", "link": "http://arxiv.org/abs/2511.17487v1", "date": "2025-11-21", "relevancy": 2.8828, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5806}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5806}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Downscaling%20Intelligence%3A%20Exploring%20Perception%20and%20Reasoning%20Bottlenecks%20in%20Small%20Multimodal%20Models&body=Title%3A%20Downscaling%20Intelligence%3A%20Exploring%20Perception%20and%20Reasoning%20Bottlenecks%20in%20Small%20Multimodal%20Models%0AAuthor%3A%20Mark%20Endo%20and%20Serena%20Yeung-Levy%0AAbstract%3A%20Scaling%20up%20multimodal%20models%20has%20enabled%20remarkable%20advances%20in%20visual%20understanding%20and%20reasoning%2C%20but%20practical%20demands%20call%20for%20smaller%2C%20efficient%20systems.%20In%20this%20work%2C%20we%20conduct%20a%20principled%20analysis%20of%20downscaling%20intelligence%20in%20multimodal%20models%2C%20examining%20how%20reduced%20large%20language%20model%20%28LLM%29%20capacity%20affects%20multimodal%20capabilities.%20Our%20initial%20findings%20reveal%20an%20interesting%20trend%3A%20LLM%20downscaling%20disproportionately%20affects%20visual%20capabilities%2C%20rather%20than%20abilities%20inherited%20from%20the%20LLM.%20We%20then%20examine%20whether%20this%20drop%20mainly%20reflects%20the%20expected%20decline%20in%20visual%20reasoning%20or%20a%20more%20fundamental%20loss%20of%20perceptual%20abilities.%20Isolating%20the%20effect%20of%20LLM%20downscaling%20on%20perception%2C%20we%20find%20performance%20still%20drops%20sharply%2C%20often%20matching%20or%20exceeding%20the%20impact%20on%20reasoning.%20To%20address%20this%20bottleneck%2C%20we%20introduce%20visual%20extraction%20tuning%2C%20which%20explicitly%20trains%20the%20model%20to%20extract%20instruction-relevant%20visual%20details%20consistently%20across%20tasks.%20With%20these%20extracted%20visual%20details%2C%20we%20then%20apply%20step-by-step%20reasoning%20to%20generate%20answers.%20Together%2C%20these%20components%20form%20our%20Extract%2BThink%20approach%2C%20setting%20a%20new%20standard%20for%20efficiency%20and%20performance%20in%20this%20space.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDownscaling%2520Intelligence%253A%2520Exploring%2520Perception%2520and%2520Reasoning%2520Bottlenecks%2520in%2520Small%2520Multimodal%2520Models%26entry.906535625%3DMark%2520Endo%2520and%2520Serena%2520Yeung-Levy%26entry.1292438233%3DScaling%2520up%2520multimodal%2520models%2520has%2520enabled%2520remarkable%2520advances%2520in%2520visual%2520understanding%2520and%2520reasoning%252C%2520but%2520practical%2520demands%2520call%2520for%2520smaller%252C%2520efficient%2520systems.%2520In%2520this%2520work%252C%2520we%2520conduct%2520a%2520principled%2520analysis%2520of%2520downscaling%2520intelligence%2520in%2520multimodal%2520models%252C%2520examining%2520how%2520reduced%2520large%2520language%2520model%2520%2528LLM%2529%2520capacity%2520affects%2520multimodal%2520capabilities.%2520Our%2520initial%2520findings%2520reveal%2520an%2520interesting%2520trend%253A%2520LLM%2520downscaling%2520disproportionately%2520affects%2520visual%2520capabilities%252C%2520rather%2520than%2520abilities%2520inherited%2520from%2520the%2520LLM.%2520We%2520then%2520examine%2520whether%2520this%2520drop%2520mainly%2520reflects%2520the%2520expected%2520decline%2520in%2520visual%2520reasoning%2520or%2520a%2520more%2520fundamental%2520loss%2520of%2520perceptual%2520abilities.%2520Isolating%2520the%2520effect%2520of%2520LLM%2520downscaling%2520on%2520perception%252C%2520we%2520find%2520performance%2520still%2520drops%2520sharply%252C%2520often%2520matching%2520or%2520exceeding%2520the%2520impact%2520on%2520reasoning.%2520To%2520address%2520this%2520bottleneck%252C%2520we%2520introduce%2520visual%2520extraction%2520tuning%252C%2520which%2520explicitly%2520trains%2520the%2520model%2520to%2520extract%2520instruction-relevant%2520visual%2520details%2520consistently%2520across%2520tasks.%2520With%2520these%2520extracted%2520visual%2520details%252C%2520we%2520then%2520apply%2520step-by-step%2520reasoning%2520to%2520generate%2520answers.%2520Together%252C%2520these%2520components%2520form%2520our%2520Extract%252BThink%2520approach%252C%2520setting%2520a%2520new%2520standard%2520for%2520efficiency%2520and%2520performance%2520in%2520this%2520space.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Downscaling%20Intelligence%3A%20Exploring%20Perception%20and%20Reasoning%20Bottlenecks%20in%20Small%20Multimodal%20Models&entry.906535625=Mark%20Endo%20and%20Serena%20Yeung-Levy&entry.1292438233=Scaling%20up%20multimodal%20models%20has%20enabled%20remarkable%20advances%20in%20visual%20understanding%20and%20reasoning%2C%20but%20practical%20demands%20call%20for%20smaller%2C%20efficient%20systems.%20In%20this%20work%2C%20we%20conduct%20a%20principled%20analysis%20of%20downscaling%20intelligence%20in%20multimodal%20models%2C%20examining%20how%20reduced%20large%20language%20model%20%28LLM%29%20capacity%20affects%20multimodal%20capabilities.%20Our%20initial%20findings%20reveal%20an%20interesting%20trend%3A%20LLM%20downscaling%20disproportionately%20affects%20visual%20capabilities%2C%20rather%20than%20abilities%20inherited%20from%20the%20LLM.%20We%20then%20examine%20whether%20this%20drop%20mainly%20reflects%20the%20expected%20decline%20in%20visual%20reasoning%20or%20a%20more%20fundamental%20loss%20of%20perceptual%20abilities.%20Isolating%20the%20effect%20of%20LLM%20downscaling%20on%20perception%2C%20we%20find%20performance%20still%20drops%20sharply%2C%20often%20matching%20or%20exceeding%20the%20impact%20on%20reasoning.%20To%20address%20this%20bottleneck%2C%20we%20introduce%20visual%20extraction%20tuning%2C%20which%20explicitly%20trains%20the%20model%20to%20extract%20instruction-relevant%20visual%20details%20consistently%20across%20tasks.%20With%20these%20extracted%20visual%20details%2C%20we%20then%20apply%20step-by-step%20reasoning%20to%20generate%20answers.%20Together%2C%20these%20components%20form%20our%20Extract%2BThink%20approach%2C%20setting%20a%20new%20standard%20for%20efficiency%20and%20performance%20in%20this%20space.&entry.1838667208=http%3A//arxiv.org/abs/2511.17487v1&entry.124074799=Read"},
{"title": "Wideband RF Radiance Field Modeling Using Frequency-embedded 3D Gaussian Splatting", "author": "Zechen Li and Lanqing Yang and Yiheng Bian and Hao Pan and Yongjian Fu and Yezhou Wang and Zhuxi Chen and Yi-Chao Chen and Guangtao Xue", "abstract": "Indoor environments typically contain diverse RF signals distributed across multiple frequency bands, including NB-IoT, Wi-Fi, and millimeter-wave. Consequently, wideband RF modeling is essential for practical applications such as joint deployment of heterogeneous RF systems, cross-band communication, and distributed RF sensing. Although 3D Gaussian Splatting (3DGS) techniques effectively reconstruct RF radiance fields at a single frequency, they cannot model fields at arbitrary or unknown frequencies across a wide range. In this paper, we present a novel 3DGS algorithm for unified wideband RF radiance field modeling. RF wave propagation depends on signal frequency and the 3D spatial environment, including geometry and material electromagnetic (EM) properties. To address these factors, we introduce a frequency-embedded EM feature network that utilizes 3D Gaussian spheres at each spatial location to learn the relationship between frequency and transmission characteristics, such as attenuation and radiance intensity. With a dataset containing sparse frequency samples in a specific 3D environment, our model can efficiently reconstruct RF radiance fields at arbitrary and unseen frequencies. To assess our approach, we introduce a large-scale power angular spectrum (PAS) dataset with 50,000 samples spanning 1 to 94 GHz across six indoor environments. Experimental results show that the proposed model trained on multiple frequencies achieves a Structural Similarity Index Measure (SSIM) of 0.922 for PAS reconstruction, surpassing state-of-the-art single-frequency 3DGS models with SSIM of 0.863.", "link": "http://arxiv.org/abs/2505.20714v2", "date": "2025-11-21", "relevancy": 2.8334, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6098}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5609}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wideband%20RF%20Radiance%20Field%20Modeling%20Using%20Frequency-embedded%203D%20Gaussian%20Splatting&body=Title%3A%20Wideband%20RF%20Radiance%20Field%20Modeling%20Using%20Frequency-embedded%203D%20Gaussian%20Splatting%0AAuthor%3A%20Zechen%20Li%20and%20Lanqing%20Yang%20and%20Yiheng%20Bian%20and%20Hao%20Pan%20and%20Yongjian%20Fu%20and%20Yezhou%20Wang%20and%20Zhuxi%20Chen%20and%20Yi-Chao%20Chen%20and%20Guangtao%20Xue%0AAbstract%3A%20Indoor%20environments%20typically%20contain%20diverse%20RF%20signals%20distributed%20across%20multiple%20frequency%20bands%2C%20including%20NB-IoT%2C%20Wi-Fi%2C%20and%20millimeter-wave.%20Consequently%2C%20wideband%20RF%20modeling%20is%20essential%20for%20practical%20applications%20such%20as%20joint%20deployment%20of%20heterogeneous%20RF%20systems%2C%20cross-band%20communication%2C%20and%20distributed%20RF%20sensing.%20Although%203D%20Gaussian%20Splatting%20%283DGS%29%20techniques%20effectively%20reconstruct%20RF%20radiance%20fields%20at%20a%20single%20frequency%2C%20they%20cannot%20model%20fields%20at%20arbitrary%20or%20unknown%20frequencies%20across%20a%20wide%20range.%20In%20this%20paper%2C%20we%20present%20a%20novel%203DGS%20algorithm%20for%20unified%20wideband%20RF%20radiance%20field%20modeling.%20RF%20wave%20propagation%20depends%20on%20signal%20frequency%20and%20the%203D%20spatial%20environment%2C%20including%20geometry%20and%20material%20electromagnetic%20%28EM%29%20properties.%20To%20address%20these%20factors%2C%20we%20introduce%20a%20frequency-embedded%20EM%20feature%20network%20that%20utilizes%203D%20Gaussian%20spheres%20at%20each%20spatial%20location%20to%20learn%20the%20relationship%20between%20frequency%20and%20transmission%20characteristics%2C%20such%20as%20attenuation%20and%20radiance%20intensity.%20With%20a%20dataset%20containing%20sparse%20frequency%20samples%20in%20a%20specific%203D%20environment%2C%20our%20model%20can%20efficiently%20reconstruct%20RF%20radiance%20fields%20at%20arbitrary%20and%20unseen%20frequencies.%20To%20assess%20our%20approach%2C%20we%20introduce%20a%20large-scale%20power%20angular%20spectrum%20%28PAS%29%20dataset%20with%2050%2C000%20samples%20spanning%201%20to%2094%20GHz%20across%20six%20indoor%20environments.%20Experimental%20results%20show%20that%20the%20proposed%20model%20trained%20on%20multiple%20frequencies%20achieves%20a%20Structural%20Similarity%20Index%20Measure%20%28SSIM%29%20of%200.922%20for%20PAS%20reconstruction%2C%20surpassing%20state-of-the-art%20single-frequency%203DGS%20models%20with%20SSIM%20of%200.863.%0ALink%3A%20http%3A//arxiv.org/abs/2505.20714v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWideband%2520RF%2520Radiance%2520Field%2520Modeling%2520Using%2520Frequency-embedded%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DZechen%2520Li%2520and%2520Lanqing%2520Yang%2520and%2520Yiheng%2520Bian%2520and%2520Hao%2520Pan%2520and%2520Yongjian%2520Fu%2520and%2520Yezhou%2520Wang%2520and%2520Zhuxi%2520Chen%2520and%2520Yi-Chao%2520Chen%2520and%2520Guangtao%2520Xue%26entry.1292438233%3DIndoor%2520environments%2520typically%2520contain%2520diverse%2520RF%2520signals%2520distributed%2520across%2520multiple%2520frequency%2520bands%252C%2520including%2520NB-IoT%252C%2520Wi-Fi%252C%2520and%2520millimeter-wave.%2520Consequently%252C%2520wideband%2520RF%2520modeling%2520is%2520essential%2520for%2520practical%2520applications%2520such%2520as%2520joint%2520deployment%2520of%2520heterogeneous%2520RF%2520systems%252C%2520cross-band%2520communication%252C%2520and%2520distributed%2520RF%2520sensing.%2520Although%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520techniques%2520effectively%2520reconstruct%2520RF%2520radiance%2520fields%2520at%2520a%2520single%2520frequency%252C%2520they%2520cannot%2520model%2520fields%2520at%2520arbitrary%2520or%2520unknown%2520frequencies%2520across%2520a%2520wide%2520range.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%25203DGS%2520algorithm%2520for%2520unified%2520wideband%2520RF%2520radiance%2520field%2520modeling.%2520RF%2520wave%2520propagation%2520depends%2520on%2520signal%2520frequency%2520and%2520the%25203D%2520spatial%2520environment%252C%2520including%2520geometry%2520and%2520material%2520electromagnetic%2520%2528EM%2529%2520properties.%2520To%2520address%2520these%2520factors%252C%2520we%2520introduce%2520a%2520frequency-embedded%2520EM%2520feature%2520network%2520that%2520utilizes%25203D%2520Gaussian%2520spheres%2520at%2520each%2520spatial%2520location%2520to%2520learn%2520the%2520relationship%2520between%2520frequency%2520and%2520transmission%2520characteristics%252C%2520such%2520as%2520attenuation%2520and%2520radiance%2520intensity.%2520With%2520a%2520dataset%2520containing%2520sparse%2520frequency%2520samples%2520in%2520a%2520specific%25203D%2520environment%252C%2520our%2520model%2520can%2520efficiently%2520reconstruct%2520RF%2520radiance%2520fields%2520at%2520arbitrary%2520and%2520unseen%2520frequencies.%2520To%2520assess%2520our%2520approach%252C%2520we%2520introduce%2520a%2520large-scale%2520power%2520angular%2520spectrum%2520%2528PAS%2529%2520dataset%2520with%252050%252C000%2520samples%2520spanning%25201%2520to%252094%2520GHz%2520across%2520six%2520indoor%2520environments.%2520Experimental%2520results%2520show%2520that%2520the%2520proposed%2520model%2520trained%2520on%2520multiple%2520frequencies%2520achieves%2520a%2520Structural%2520Similarity%2520Index%2520Measure%2520%2528SSIM%2529%2520of%25200.922%2520for%2520PAS%2520reconstruction%252C%2520surpassing%2520state-of-the-art%2520single-frequency%25203DGS%2520models%2520with%2520SSIM%2520of%25200.863.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20714v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wideband%20RF%20Radiance%20Field%20Modeling%20Using%20Frequency-embedded%203D%20Gaussian%20Splatting&entry.906535625=Zechen%20Li%20and%20Lanqing%20Yang%20and%20Yiheng%20Bian%20and%20Hao%20Pan%20and%20Yongjian%20Fu%20and%20Yezhou%20Wang%20and%20Zhuxi%20Chen%20and%20Yi-Chao%20Chen%20and%20Guangtao%20Xue&entry.1292438233=Indoor%20environments%20typically%20contain%20diverse%20RF%20signals%20distributed%20across%20multiple%20frequency%20bands%2C%20including%20NB-IoT%2C%20Wi-Fi%2C%20and%20millimeter-wave.%20Consequently%2C%20wideband%20RF%20modeling%20is%20essential%20for%20practical%20applications%20such%20as%20joint%20deployment%20of%20heterogeneous%20RF%20systems%2C%20cross-band%20communication%2C%20and%20distributed%20RF%20sensing.%20Although%203D%20Gaussian%20Splatting%20%283DGS%29%20techniques%20effectively%20reconstruct%20RF%20radiance%20fields%20at%20a%20single%20frequency%2C%20they%20cannot%20model%20fields%20at%20arbitrary%20or%20unknown%20frequencies%20across%20a%20wide%20range.%20In%20this%20paper%2C%20we%20present%20a%20novel%203DGS%20algorithm%20for%20unified%20wideband%20RF%20radiance%20field%20modeling.%20RF%20wave%20propagation%20depends%20on%20signal%20frequency%20and%20the%203D%20spatial%20environment%2C%20including%20geometry%20and%20material%20electromagnetic%20%28EM%29%20properties.%20To%20address%20these%20factors%2C%20we%20introduce%20a%20frequency-embedded%20EM%20feature%20network%20that%20utilizes%203D%20Gaussian%20spheres%20at%20each%20spatial%20location%20to%20learn%20the%20relationship%20between%20frequency%20and%20transmission%20characteristics%2C%20such%20as%20attenuation%20and%20radiance%20intensity.%20With%20a%20dataset%20containing%20sparse%20frequency%20samples%20in%20a%20specific%203D%20environment%2C%20our%20model%20can%20efficiently%20reconstruct%20RF%20radiance%20fields%20at%20arbitrary%20and%20unseen%20frequencies.%20To%20assess%20our%20approach%2C%20we%20introduce%20a%20large-scale%20power%20angular%20spectrum%20%28PAS%29%20dataset%20with%2050%2C000%20samples%20spanning%201%20to%2094%20GHz%20across%20six%20indoor%20environments.%20Experimental%20results%20show%20that%20the%20proposed%20model%20trained%20on%20multiple%20frequencies%20achieves%20a%20Structural%20Similarity%20Index%20Measure%20%28SSIM%29%20of%200.922%20for%20PAS%20reconstruction%2C%20surpassing%20state-of-the-art%20single-frequency%203DGS%20models%20with%20SSIM%20of%200.863.&entry.1838667208=http%3A//arxiv.org/abs/2505.20714v2&entry.124074799=Read"},
{"title": "GPR-OdomNet: Difference and Similarity-Driven Odometry Estimation Network for Ground Penetrating Radar-Based Localization", "author": "Huaichao Wang and Xuanxin Fan and Ji Liu and Haifeng Li and Dezhen Song", "abstract": "When performing robot/vehicle localization using ground penetrating radar (GPR) to handle adverse weather and environmental conditions, existing techniques often struggle to accurately estimate distances when processing B-scan images with minor distinctions. This study introduces a new neural network-based odometry method that leverages the similarity and difference features of GPR B-scan images for precise estimation of the Euclidean distances traveled between the B-scan images. The new custom neural network extracts multi-scale features from B-scan images taken at consecutive moments and then determines the Euclidean distance traveled by analyzing the similarities and differences between these features. To evaluate our method, an ablation study and comparison experiments have been conducted using the publicly available CMU-GPR dataset. The experimental results show that our method consistently outperforms state-of-the-art counterparts in all tests. Specifically, our method achieves a root mean square error (RMSE), and achieves an overall weighted RMSE of 0.449 m across all data sets, which is a 10.2\\% reduction in RMSE when compared to the best state-of-the-art method.", "link": "http://arxiv.org/abs/2511.17457v1", "date": "2025-11-21", "relevancy": 2.8225, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5843}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5677}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPR-OdomNet%3A%20Difference%20and%20Similarity-Driven%20Odometry%20Estimation%20Network%20for%20Ground%20Penetrating%20Radar-Based%20Localization&body=Title%3A%20GPR-OdomNet%3A%20Difference%20and%20Similarity-Driven%20Odometry%20Estimation%20Network%20for%20Ground%20Penetrating%20Radar-Based%20Localization%0AAuthor%3A%20Huaichao%20Wang%20and%20Xuanxin%20Fan%20and%20Ji%20Liu%20and%20Haifeng%20Li%20and%20Dezhen%20Song%0AAbstract%3A%20When%20performing%20robot/vehicle%20localization%20using%20ground%20penetrating%20radar%20%28GPR%29%20to%20handle%20adverse%20weather%20and%20environmental%20conditions%2C%20existing%20techniques%20often%20struggle%20to%20accurately%20estimate%20distances%20when%20processing%20B-scan%20images%20with%20minor%20distinctions.%20This%20study%20introduces%20a%20new%20neural%20network-based%20odometry%20method%20that%20leverages%20the%20similarity%20and%20difference%20features%20of%20GPR%20B-scan%20images%20for%20precise%20estimation%20of%20the%20Euclidean%20distances%20traveled%20between%20the%20B-scan%20images.%20The%20new%20custom%20neural%20network%20extracts%20multi-scale%20features%20from%20B-scan%20images%20taken%20at%20consecutive%20moments%20and%20then%20determines%20the%20Euclidean%20distance%20traveled%20by%20analyzing%20the%20similarities%20and%20differences%20between%20these%20features.%20To%20evaluate%20our%20method%2C%20an%20ablation%20study%20and%20comparison%20experiments%20have%20been%20conducted%20using%20the%20publicly%20available%20CMU-GPR%20dataset.%20The%20experimental%20results%20show%20that%20our%20method%20consistently%20outperforms%20state-of-the-art%20counterparts%20in%20all%20tests.%20Specifically%2C%20our%20method%20achieves%20a%20root%20mean%20square%20error%20%28RMSE%29%2C%20and%20achieves%20an%20overall%20weighted%20RMSE%20of%200.449%20m%20across%20all%20data%20sets%2C%20which%20is%20a%2010.2%5C%25%20reduction%20in%20RMSE%20when%20compared%20to%20the%20best%20state-of-the-art%20method.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPR-OdomNet%253A%2520Difference%2520and%2520Similarity-Driven%2520Odometry%2520Estimation%2520Network%2520for%2520Ground%2520Penetrating%2520Radar-Based%2520Localization%26entry.906535625%3DHuaichao%2520Wang%2520and%2520Xuanxin%2520Fan%2520and%2520Ji%2520Liu%2520and%2520Haifeng%2520Li%2520and%2520Dezhen%2520Song%26entry.1292438233%3DWhen%2520performing%2520robot/vehicle%2520localization%2520using%2520ground%2520penetrating%2520radar%2520%2528GPR%2529%2520to%2520handle%2520adverse%2520weather%2520and%2520environmental%2520conditions%252C%2520existing%2520techniques%2520often%2520struggle%2520to%2520accurately%2520estimate%2520distances%2520when%2520processing%2520B-scan%2520images%2520with%2520minor%2520distinctions.%2520This%2520study%2520introduces%2520a%2520new%2520neural%2520network-based%2520odometry%2520method%2520that%2520leverages%2520the%2520similarity%2520and%2520difference%2520features%2520of%2520GPR%2520B-scan%2520images%2520for%2520precise%2520estimation%2520of%2520the%2520Euclidean%2520distances%2520traveled%2520between%2520the%2520B-scan%2520images.%2520The%2520new%2520custom%2520neural%2520network%2520extracts%2520multi-scale%2520features%2520from%2520B-scan%2520images%2520taken%2520at%2520consecutive%2520moments%2520and%2520then%2520determines%2520the%2520Euclidean%2520distance%2520traveled%2520by%2520analyzing%2520the%2520similarities%2520and%2520differences%2520between%2520these%2520features.%2520To%2520evaluate%2520our%2520method%252C%2520an%2520ablation%2520study%2520and%2520comparison%2520experiments%2520have%2520been%2520conducted%2520using%2520the%2520publicly%2520available%2520CMU-GPR%2520dataset.%2520The%2520experimental%2520results%2520show%2520that%2520our%2520method%2520consistently%2520outperforms%2520state-of-the-art%2520counterparts%2520in%2520all%2520tests.%2520Specifically%252C%2520our%2520method%2520achieves%2520a%2520root%2520mean%2520square%2520error%2520%2528RMSE%2529%252C%2520and%2520achieves%2520an%2520overall%2520weighted%2520RMSE%2520of%25200.449%2520m%2520across%2520all%2520data%2520sets%252C%2520which%2520is%2520a%252010.2%255C%2525%2520reduction%2520in%2520RMSE%2520when%2520compared%2520to%2520the%2520best%2520state-of-the-art%2520method.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPR-OdomNet%3A%20Difference%20and%20Similarity-Driven%20Odometry%20Estimation%20Network%20for%20Ground%20Penetrating%20Radar-Based%20Localization&entry.906535625=Huaichao%20Wang%20and%20Xuanxin%20Fan%20and%20Ji%20Liu%20and%20Haifeng%20Li%20and%20Dezhen%20Song&entry.1292438233=When%20performing%20robot/vehicle%20localization%20using%20ground%20penetrating%20radar%20%28GPR%29%20to%20handle%20adverse%20weather%20and%20environmental%20conditions%2C%20existing%20techniques%20often%20struggle%20to%20accurately%20estimate%20distances%20when%20processing%20B-scan%20images%20with%20minor%20distinctions.%20This%20study%20introduces%20a%20new%20neural%20network-based%20odometry%20method%20that%20leverages%20the%20similarity%20and%20difference%20features%20of%20GPR%20B-scan%20images%20for%20precise%20estimation%20of%20the%20Euclidean%20distances%20traveled%20between%20the%20B-scan%20images.%20The%20new%20custom%20neural%20network%20extracts%20multi-scale%20features%20from%20B-scan%20images%20taken%20at%20consecutive%20moments%20and%20then%20determines%20the%20Euclidean%20distance%20traveled%20by%20analyzing%20the%20similarities%20and%20differences%20between%20these%20features.%20To%20evaluate%20our%20method%2C%20an%20ablation%20study%20and%20comparison%20experiments%20have%20been%20conducted%20using%20the%20publicly%20available%20CMU-GPR%20dataset.%20The%20experimental%20results%20show%20that%20our%20method%20consistently%20outperforms%20state-of-the-art%20counterparts%20in%20all%20tests.%20Specifically%2C%20our%20method%20achieves%20a%20root%20mean%20square%20error%20%28RMSE%29%2C%20and%20achieves%20an%20overall%20weighted%20RMSE%20of%200.449%20m%20across%20all%20data%20sets%2C%20which%20is%20a%2010.2%5C%25%20reduction%20in%20RMSE%20when%20compared%20to%20the%20best%20state-of-the-art%20method.&entry.1838667208=http%3A//arxiv.org/abs/2511.17457v1&entry.124074799=Read"},
{"title": "NoPe-NeRF++: Local-to-Global Optimization of NeRF with No Pose Prior", "author": "Dongbo Shi and Shen Cao and Bojian Wu and Jinhui Guo and Lubin Fan and Renjie Chen and Ligang Liu and Jieping Ye", "abstract": "In this paper, we introduce NoPe-NeRF++, a novel local-to-global optimization algorithm for training Neural Radiance Fields (NeRF) without requiring pose priors. Existing methods, particularly NoPe-NeRF, which focus solely on the local relationships within images, often struggle to recover accurate camera poses in complex scenarios. To overcome the challenges, our approach begins with a relative pose initialization with explicit feature matching, followed by a local joint optimization to enhance the pose estimation for training a more robust NeRF representation. This method significantly improves the quality of initial poses. Additionally, we introduce global optimization phase that incorporates geometric consistency constraints through bundle adjustment, which integrates feature trajectories to further refine poses and collectively boost the quality of NeRF. Notably, our method is the first work that seamlessly combines the local and global cues with NeRF, and outperforms state-of-the-art methods in both pose estimation accuracy and novel view synthesis. Extensive evaluations on benchmark datasets demonstrate our superior performance and robustness, even in challenging scenes, thus validating our design choices.", "link": "http://arxiv.org/abs/2511.17322v1", "date": "2025-11-21", "relevancy": 2.8089, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6027}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5438}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NoPe-NeRF%2B%2B%3A%20Local-to-Global%20Optimization%20of%20NeRF%20with%20No%20Pose%20Prior&body=Title%3A%20NoPe-NeRF%2B%2B%3A%20Local-to-Global%20Optimization%20of%20NeRF%20with%20No%20Pose%20Prior%0AAuthor%3A%20Dongbo%20Shi%20and%20Shen%20Cao%20and%20Bojian%20Wu%20and%20Jinhui%20Guo%20and%20Lubin%20Fan%20and%20Renjie%20Chen%20and%20Ligang%20Liu%20and%20Jieping%20Ye%0AAbstract%3A%20In%20this%20paper%2C%20we%20introduce%20NoPe-NeRF%2B%2B%2C%20a%20novel%20local-to-global%20optimization%20algorithm%20for%20training%20Neural%20Radiance%20Fields%20%28NeRF%29%20without%20requiring%20pose%20priors.%20Existing%20methods%2C%20particularly%20NoPe-NeRF%2C%20which%20focus%20solely%20on%20the%20local%20relationships%20within%20images%2C%20often%20struggle%20to%20recover%20accurate%20camera%20poses%20in%20complex%20scenarios.%20To%20overcome%20the%20challenges%2C%20our%20approach%20begins%20with%20a%20relative%20pose%20initialization%20with%20explicit%20feature%20matching%2C%20followed%20by%20a%20local%20joint%20optimization%20to%20enhance%20the%20pose%20estimation%20for%20training%20a%20more%20robust%20NeRF%20representation.%20This%20method%20significantly%20improves%20the%20quality%20of%20initial%20poses.%20Additionally%2C%20we%20introduce%20global%20optimization%20phase%20that%20incorporates%20geometric%20consistency%20constraints%20through%20bundle%20adjustment%2C%20which%20integrates%20feature%20trajectories%20to%20further%20refine%20poses%20and%20collectively%20boost%20the%20quality%20of%20NeRF.%20Notably%2C%20our%20method%20is%20the%20first%20work%20that%20seamlessly%20combines%20the%20local%20and%20global%20cues%20with%20NeRF%2C%20and%20outperforms%20state-of-the-art%20methods%20in%20both%20pose%20estimation%20accuracy%20and%20novel%20view%20synthesis.%20Extensive%20evaluations%20on%20benchmark%20datasets%20demonstrate%20our%20superior%20performance%20and%20robustness%2C%20even%20in%20challenging%20scenes%2C%20thus%20validating%20our%20design%20choices.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoPe-NeRF%252B%252B%253A%2520Local-to-Global%2520Optimization%2520of%2520NeRF%2520with%2520No%2520Pose%2520Prior%26entry.906535625%3DDongbo%2520Shi%2520and%2520Shen%2520Cao%2520and%2520Bojian%2520Wu%2520and%2520Jinhui%2520Guo%2520and%2520Lubin%2520Fan%2520and%2520Renjie%2520Chen%2520and%2520Ligang%2520Liu%2520and%2520Jieping%2520Ye%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520introduce%2520NoPe-NeRF%252B%252B%252C%2520a%2520novel%2520local-to-global%2520optimization%2520algorithm%2520for%2520training%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520without%2520requiring%2520pose%2520priors.%2520Existing%2520methods%252C%2520particularly%2520NoPe-NeRF%252C%2520which%2520focus%2520solely%2520on%2520the%2520local%2520relationships%2520within%2520images%252C%2520often%2520struggle%2520to%2520recover%2520accurate%2520camera%2520poses%2520in%2520complex%2520scenarios.%2520To%2520overcome%2520the%2520challenges%252C%2520our%2520approach%2520begins%2520with%2520a%2520relative%2520pose%2520initialization%2520with%2520explicit%2520feature%2520matching%252C%2520followed%2520by%2520a%2520local%2520joint%2520optimization%2520to%2520enhance%2520the%2520pose%2520estimation%2520for%2520training%2520a%2520more%2520robust%2520NeRF%2520representation.%2520This%2520method%2520significantly%2520improves%2520the%2520quality%2520of%2520initial%2520poses.%2520Additionally%252C%2520we%2520introduce%2520global%2520optimization%2520phase%2520that%2520incorporates%2520geometric%2520consistency%2520constraints%2520through%2520bundle%2520adjustment%252C%2520which%2520integrates%2520feature%2520trajectories%2520to%2520further%2520refine%2520poses%2520and%2520collectively%2520boost%2520the%2520quality%2520of%2520NeRF.%2520Notably%252C%2520our%2520method%2520is%2520the%2520first%2520work%2520that%2520seamlessly%2520combines%2520the%2520local%2520and%2520global%2520cues%2520with%2520NeRF%252C%2520and%2520outperforms%2520state-of-the-art%2520methods%2520in%2520both%2520pose%2520estimation%2520accuracy%2520and%2520novel%2520view%2520synthesis.%2520Extensive%2520evaluations%2520on%2520benchmark%2520datasets%2520demonstrate%2520our%2520superior%2520performance%2520and%2520robustness%252C%2520even%2520in%2520challenging%2520scenes%252C%2520thus%2520validating%2520our%2520design%2520choices.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NoPe-NeRF%2B%2B%3A%20Local-to-Global%20Optimization%20of%20NeRF%20with%20No%20Pose%20Prior&entry.906535625=Dongbo%20Shi%20and%20Shen%20Cao%20and%20Bojian%20Wu%20and%20Jinhui%20Guo%20and%20Lubin%20Fan%20and%20Renjie%20Chen%20and%20Ligang%20Liu%20and%20Jieping%20Ye&entry.1292438233=In%20this%20paper%2C%20we%20introduce%20NoPe-NeRF%2B%2B%2C%20a%20novel%20local-to-global%20optimization%20algorithm%20for%20training%20Neural%20Radiance%20Fields%20%28NeRF%29%20without%20requiring%20pose%20priors.%20Existing%20methods%2C%20particularly%20NoPe-NeRF%2C%20which%20focus%20solely%20on%20the%20local%20relationships%20within%20images%2C%20often%20struggle%20to%20recover%20accurate%20camera%20poses%20in%20complex%20scenarios.%20To%20overcome%20the%20challenges%2C%20our%20approach%20begins%20with%20a%20relative%20pose%20initialization%20with%20explicit%20feature%20matching%2C%20followed%20by%20a%20local%20joint%20optimization%20to%20enhance%20the%20pose%20estimation%20for%20training%20a%20more%20robust%20NeRF%20representation.%20This%20method%20significantly%20improves%20the%20quality%20of%20initial%20poses.%20Additionally%2C%20we%20introduce%20global%20optimization%20phase%20that%20incorporates%20geometric%20consistency%20constraints%20through%20bundle%20adjustment%2C%20which%20integrates%20feature%20trajectories%20to%20further%20refine%20poses%20and%20collectively%20boost%20the%20quality%20of%20NeRF.%20Notably%2C%20our%20method%20is%20the%20first%20work%20that%20seamlessly%20combines%20the%20local%20and%20global%20cues%20with%20NeRF%2C%20and%20outperforms%20state-of-the-art%20methods%20in%20both%20pose%20estimation%20accuracy%20and%20novel%20view%20synthesis.%20Extensive%20evaluations%20on%20benchmark%20datasets%20demonstrate%20our%20superior%20performance%20and%20robustness%2C%20even%20in%20challenging%20scenes%2C%20thus%20validating%20our%20design%20choices.&entry.1838667208=http%3A//arxiv.org/abs/2511.17322v1&entry.124074799=Read"},
{"title": "Sparse Mixture-of-Experts for Multi-Channel Imaging: Are All Channel Interactions Required?", "author": "Sukwon Yun and Heming Yao and Burkhard Hoeckendorf and David Richmond and Aviv Regev and Russell Littman", "abstract": "Vision Transformers ($\\text{ViTs}$) have become the backbone of vision foundation models, yet their optimization for multi-channel domains - such as cell painting or satellite imagery - remains underexplored. A key challenge in these domains is capturing interactions between channels, as each channel carries different information. While existing works have shown efficacy by treating each channel independently during tokenization, this approach naturally introduces a major computational bottleneck in the attention block - channel-wise comparisons leads to a quadratic growth in attention, resulting in excessive $\\text{FLOPs}$ and high training cost. In this work, we shift focus from efficacy to the overlooked efficiency challenge in cross-channel attention and ask: \"Is it necessary to model all channel interactions?\". Inspired by the philosophy of Sparse Mixture-of-Experts ($\\text{MoE}$), we propose MoE-ViT, a Mixture-of-Experts architecture for multi-channel images in $\\text{ViTs}$, which treats each channel as an expert and employs a lightweight router to select only the most relevant experts per patch for attention. Proof-of-concept experiments on real-world datasets - JUMP-CP and So2Sat - demonstrate that $\\text{MoE-ViT}$ achieves substantial efficiency gains without sacrificing, and in some cases enhancing, performance, making it a practical and attractive backbone for multi-channel imaging.", "link": "http://arxiv.org/abs/2511.17400v1", "date": "2025-11-21", "relevancy": 2.7736, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5566}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Mixture-of-Experts%20for%20Multi-Channel%20Imaging%3A%20Are%20All%20Channel%20Interactions%20Required%3F&body=Title%3A%20Sparse%20Mixture-of-Experts%20for%20Multi-Channel%20Imaging%3A%20Are%20All%20Channel%20Interactions%20Required%3F%0AAuthor%3A%20Sukwon%20Yun%20and%20Heming%20Yao%20and%20Burkhard%20Hoeckendorf%20and%20David%20Richmond%20and%20Aviv%20Regev%20and%20Russell%20Littman%0AAbstract%3A%20Vision%20Transformers%20%28%24%5Ctext%7BViTs%7D%24%29%20have%20become%20the%20backbone%20of%20vision%20foundation%20models%2C%20yet%20their%20optimization%20for%20multi-channel%20domains%20-%20such%20as%20cell%20painting%20or%20satellite%20imagery%20-%20remains%20underexplored.%20A%20key%20challenge%20in%20these%20domains%20is%20capturing%20interactions%20between%20channels%2C%20as%20each%20channel%20carries%20different%20information.%20While%20existing%20works%20have%20shown%20efficacy%20by%20treating%20each%20channel%20independently%20during%20tokenization%2C%20this%20approach%20naturally%20introduces%20a%20major%20computational%20bottleneck%20in%20the%20attention%20block%20-%20channel-wise%20comparisons%20leads%20to%20a%20quadratic%20growth%20in%20attention%2C%20resulting%20in%20excessive%20%24%5Ctext%7BFLOPs%7D%24%20and%20high%20training%20cost.%20In%20this%20work%2C%20we%20shift%20focus%20from%20efficacy%20to%20the%20overlooked%20efficiency%20challenge%20in%20cross-channel%20attention%20and%20ask%3A%20%22Is%20it%20necessary%20to%20model%20all%20channel%20interactions%3F%22.%20Inspired%20by%20the%20philosophy%20of%20Sparse%20Mixture-of-Experts%20%28%24%5Ctext%7BMoE%7D%24%29%2C%20we%20propose%20MoE-ViT%2C%20a%20Mixture-of-Experts%20architecture%20for%20multi-channel%20images%20in%20%24%5Ctext%7BViTs%7D%24%2C%20which%20treats%20each%20channel%20as%20an%20expert%20and%20employs%20a%20lightweight%20router%20to%20select%20only%20the%20most%20relevant%20experts%20per%20patch%20for%20attention.%20Proof-of-concept%20experiments%20on%20real-world%20datasets%20-%20JUMP-CP%20and%20So2Sat%20-%20demonstrate%20that%20%24%5Ctext%7BMoE-ViT%7D%24%20achieves%20substantial%20efficiency%20gains%20without%20sacrificing%2C%20and%20in%20some%20cases%20enhancing%2C%20performance%2C%20making%20it%20a%20practical%20and%20attractive%20backbone%20for%20multi-channel%20imaging.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17400v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Mixture-of-Experts%2520for%2520Multi-Channel%2520Imaging%253A%2520Are%2520All%2520Channel%2520Interactions%2520Required%253F%26entry.906535625%3DSukwon%2520Yun%2520and%2520Heming%2520Yao%2520and%2520Burkhard%2520Hoeckendorf%2520and%2520David%2520Richmond%2520and%2520Aviv%2520Regev%2520and%2520Russell%2520Littman%26entry.1292438233%3DVision%2520Transformers%2520%2528%2524%255Ctext%257BViTs%257D%2524%2529%2520have%2520become%2520the%2520backbone%2520of%2520vision%2520foundation%2520models%252C%2520yet%2520their%2520optimization%2520for%2520multi-channel%2520domains%2520-%2520such%2520as%2520cell%2520painting%2520or%2520satellite%2520imagery%2520-%2520remains%2520underexplored.%2520A%2520key%2520challenge%2520in%2520these%2520domains%2520is%2520capturing%2520interactions%2520between%2520channels%252C%2520as%2520each%2520channel%2520carries%2520different%2520information.%2520While%2520existing%2520works%2520have%2520shown%2520efficacy%2520by%2520treating%2520each%2520channel%2520independently%2520during%2520tokenization%252C%2520this%2520approach%2520naturally%2520introduces%2520a%2520major%2520computational%2520bottleneck%2520in%2520the%2520attention%2520block%2520-%2520channel-wise%2520comparisons%2520leads%2520to%2520a%2520quadratic%2520growth%2520in%2520attention%252C%2520resulting%2520in%2520excessive%2520%2524%255Ctext%257BFLOPs%257D%2524%2520and%2520high%2520training%2520cost.%2520In%2520this%2520work%252C%2520we%2520shift%2520focus%2520from%2520efficacy%2520to%2520the%2520overlooked%2520efficiency%2520challenge%2520in%2520cross-channel%2520attention%2520and%2520ask%253A%2520%2522Is%2520it%2520necessary%2520to%2520model%2520all%2520channel%2520interactions%253F%2522.%2520Inspired%2520by%2520the%2520philosophy%2520of%2520Sparse%2520Mixture-of-Experts%2520%2528%2524%255Ctext%257BMoE%257D%2524%2529%252C%2520we%2520propose%2520MoE-ViT%252C%2520a%2520Mixture-of-Experts%2520architecture%2520for%2520multi-channel%2520images%2520in%2520%2524%255Ctext%257BViTs%257D%2524%252C%2520which%2520treats%2520each%2520channel%2520as%2520an%2520expert%2520and%2520employs%2520a%2520lightweight%2520router%2520to%2520select%2520only%2520the%2520most%2520relevant%2520experts%2520per%2520patch%2520for%2520attention.%2520Proof-of-concept%2520experiments%2520on%2520real-world%2520datasets%2520-%2520JUMP-CP%2520and%2520So2Sat%2520-%2520demonstrate%2520that%2520%2524%255Ctext%257BMoE-ViT%257D%2524%2520achieves%2520substantial%2520efficiency%2520gains%2520without%2520sacrificing%252C%2520and%2520in%2520some%2520cases%2520enhancing%252C%2520performance%252C%2520making%2520it%2520a%2520practical%2520and%2520attractive%2520backbone%2520for%2520multi-channel%2520imaging.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17400v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Mixture-of-Experts%20for%20Multi-Channel%20Imaging%3A%20Are%20All%20Channel%20Interactions%20Required%3F&entry.906535625=Sukwon%20Yun%20and%20Heming%20Yao%20and%20Burkhard%20Hoeckendorf%20and%20David%20Richmond%20and%20Aviv%20Regev%20and%20Russell%20Littman&entry.1292438233=Vision%20Transformers%20%28%24%5Ctext%7BViTs%7D%24%29%20have%20become%20the%20backbone%20of%20vision%20foundation%20models%2C%20yet%20their%20optimization%20for%20multi-channel%20domains%20-%20such%20as%20cell%20painting%20or%20satellite%20imagery%20-%20remains%20underexplored.%20A%20key%20challenge%20in%20these%20domains%20is%20capturing%20interactions%20between%20channels%2C%20as%20each%20channel%20carries%20different%20information.%20While%20existing%20works%20have%20shown%20efficacy%20by%20treating%20each%20channel%20independently%20during%20tokenization%2C%20this%20approach%20naturally%20introduces%20a%20major%20computational%20bottleneck%20in%20the%20attention%20block%20-%20channel-wise%20comparisons%20leads%20to%20a%20quadratic%20growth%20in%20attention%2C%20resulting%20in%20excessive%20%24%5Ctext%7BFLOPs%7D%24%20and%20high%20training%20cost.%20In%20this%20work%2C%20we%20shift%20focus%20from%20efficacy%20to%20the%20overlooked%20efficiency%20challenge%20in%20cross-channel%20attention%20and%20ask%3A%20%22Is%20it%20necessary%20to%20model%20all%20channel%20interactions%3F%22.%20Inspired%20by%20the%20philosophy%20of%20Sparse%20Mixture-of-Experts%20%28%24%5Ctext%7BMoE%7D%24%29%2C%20we%20propose%20MoE-ViT%2C%20a%20Mixture-of-Experts%20architecture%20for%20multi-channel%20images%20in%20%24%5Ctext%7BViTs%7D%24%2C%20which%20treats%20each%20channel%20as%20an%20expert%20and%20employs%20a%20lightweight%20router%20to%20select%20only%20the%20most%20relevant%20experts%20per%20patch%20for%20attention.%20Proof-of-concept%20experiments%20on%20real-world%20datasets%20-%20JUMP-CP%20and%20So2Sat%20-%20demonstrate%20that%20%24%5Ctext%7BMoE-ViT%7D%24%20achieves%20substantial%20efficiency%20gains%20without%20sacrificing%2C%20and%20in%20some%20cases%20enhancing%2C%20performance%2C%20making%20it%20a%20practical%20and%20attractive%20backbone%20for%20multi-channel%20imaging.&entry.1838667208=http%3A//arxiv.org/abs/2511.17400v1&entry.124074799=Read"},
{"title": "Investigating self-supervised representations for audio-visual deepfake detection", "author": "Dragos-Alexandru Boldisor and Stefan Smeu and Dan Oneata and Elisabeta Oneata", "abstract": "Self-supervised representations excel at many vision and speech tasks, but their potential for audio-visual deepfake detection remains underexplored. Unlike prior work that uses these features in isolation or buried within complex architectures, we systematically evaluate them across modalities (audio, video, multimodal) and domains (lip movements, generic visual content). We assess three key dimensions: detection effectiveness, interpretability of encoded information, and cross-modal complementarity. We find that most self-supervised features capture deepfake-relevant information, and that this information is complementary. Moreover, models primarily attend to semantically meaningful regions rather than spurious artifacts. Yet none generalize reliably across datasets. This generalization failure likely stems from dataset characteristics, not from the features themselves latching onto superficial patterns. These results expose both the promise and fundamental challenges of self-supervised representations for deepfake detection: while they learn meaningful patterns, achieving robust cross-domain performance remains elusive.", "link": "http://arxiv.org/abs/2511.17181v1", "date": "2025-11-21", "relevancy": 2.7599, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5754}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5403}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20self-supervised%20representations%20for%20audio-visual%20deepfake%20detection&body=Title%3A%20Investigating%20self-supervised%20representations%20for%20audio-visual%20deepfake%20detection%0AAuthor%3A%20Dragos-Alexandru%20Boldisor%20and%20Stefan%20Smeu%20and%20Dan%20Oneata%20and%20Elisabeta%20Oneata%0AAbstract%3A%20Self-supervised%20representations%20excel%20at%20many%20vision%20and%20speech%20tasks%2C%20but%20their%20potential%20for%20audio-visual%20deepfake%20detection%20remains%20underexplored.%20Unlike%20prior%20work%20that%20uses%20these%20features%20in%20isolation%20or%20buried%20within%20complex%20architectures%2C%20we%20systematically%20evaluate%20them%20across%20modalities%20%28audio%2C%20video%2C%20multimodal%29%20and%20domains%20%28lip%20movements%2C%20generic%20visual%20content%29.%20We%20assess%20three%20key%20dimensions%3A%20detection%20effectiveness%2C%20interpretability%20of%20encoded%20information%2C%20and%20cross-modal%20complementarity.%20We%20find%20that%20most%20self-supervised%20features%20capture%20deepfake-relevant%20information%2C%20and%20that%20this%20information%20is%20complementary.%20Moreover%2C%20models%20primarily%20attend%20to%20semantically%20meaningful%20regions%20rather%20than%20spurious%20artifacts.%20Yet%20none%20generalize%20reliably%20across%20datasets.%20This%20generalization%20failure%20likely%20stems%20from%20dataset%20characteristics%2C%20not%20from%20the%20features%20themselves%20latching%20onto%20superficial%20patterns.%20These%20results%20expose%20both%20the%20promise%20and%20fundamental%20challenges%20of%20self-supervised%20representations%20for%20deepfake%20detection%3A%20while%20they%20learn%20meaningful%20patterns%2C%20achieving%20robust%20cross-domain%20performance%20remains%20elusive.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520self-supervised%2520representations%2520for%2520audio-visual%2520deepfake%2520detection%26entry.906535625%3DDragos-Alexandru%2520Boldisor%2520and%2520Stefan%2520Smeu%2520and%2520Dan%2520Oneata%2520and%2520Elisabeta%2520Oneata%26entry.1292438233%3DSelf-supervised%2520representations%2520excel%2520at%2520many%2520vision%2520and%2520speech%2520tasks%252C%2520but%2520their%2520potential%2520for%2520audio-visual%2520deepfake%2520detection%2520remains%2520underexplored.%2520Unlike%2520prior%2520work%2520that%2520uses%2520these%2520features%2520in%2520isolation%2520or%2520buried%2520within%2520complex%2520architectures%252C%2520we%2520systematically%2520evaluate%2520them%2520across%2520modalities%2520%2528audio%252C%2520video%252C%2520multimodal%2529%2520and%2520domains%2520%2528lip%2520movements%252C%2520generic%2520visual%2520content%2529.%2520We%2520assess%2520three%2520key%2520dimensions%253A%2520detection%2520effectiveness%252C%2520interpretability%2520of%2520encoded%2520information%252C%2520and%2520cross-modal%2520complementarity.%2520We%2520find%2520that%2520most%2520self-supervised%2520features%2520capture%2520deepfake-relevant%2520information%252C%2520and%2520that%2520this%2520information%2520is%2520complementary.%2520Moreover%252C%2520models%2520primarily%2520attend%2520to%2520semantically%2520meaningful%2520regions%2520rather%2520than%2520spurious%2520artifacts.%2520Yet%2520none%2520generalize%2520reliably%2520across%2520datasets.%2520This%2520generalization%2520failure%2520likely%2520stems%2520from%2520dataset%2520characteristics%252C%2520not%2520from%2520the%2520features%2520themselves%2520latching%2520onto%2520superficial%2520patterns.%2520These%2520results%2520expose%2520both%2520the%2520promise%2520and%2520fundamental%2520challenges%2520of%2520self-supervised%2520representations%2520for%2520deepfake%2520detection%253A%2520while%2520they%2520learn%2520meaningful%2520patterns%252C%2520achieving%2520robust%2520cross-domain%2520performance%2520remains%2520elusive.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20self-supervised%20representations%20for%20audio-visual%20deepfake%20detection&entry.906535625=Dragos-Alexandru%20Boldisor%20and%20Stefan%20Smeu%20and%20Dan%20Oneata%20and%20Elisabeta%20Oneata&entry.1292438233=Self-supervised%20representations%20excel%20at%20many%20vision%20and%20speech%20tasks%2C%20but%20their%20potential%20for%20audio-visual%20deepfake%20detection%20remains%20underexplored.%20Unlike%20prior%20work%20that%20uses%20these%20features%20in%20isolation%20or%20buried%20within%20complex%20architectures%2C%20we%20systematically%20evaluate%20them%20across%20modalities%20%28audio%2C%20video%2C%20multimodal%29%20and%20domains%20%28lip%20movements%2C%20generic%20visual%20content%29.%20We%20assess%20three%20key%20dimensions%3A%20detection%20effectiveness%2C%20interpretability%20of%20encoded%20information%2C%20and%20cross-modal%20complementarity.%20We%20find%20that%20most%20self-supervised%20features%20capture%20deepfake-relevant%20information%2C%20and%20that%20this%20information%20is%20complementary.%20Moreover%2C%20models%20primarily%20attend%20to%20semantically%20meaningful%20regions%20rather%20than%20spurious%20artifacts.%20Yet%20none%20generalize%20reliably%20across%20datasets.%20This%20generalization%20failure%20likely%20stems%20from%20dataset%20characteristics%2C%20not%20from%20the%20features%20themselves%20latching%20onto%20superficial%20patterns.%20These%20results%20expose%20both%20the%20promise%20and%20fundamental%20challenges%20of%20self-supervised%20representations%20for%20deepfake%20detection%3A%20while%20they%20learn%20meaningful%20patterns%2C%20achieving%20robust%20cross-domain%20performance%20remains%20elusive.&entry.1838667208=http%3A//arxiv.org/abs/2511.17181v1&entry.124074799=Read"},
{"title": "Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models", "author": "Siyou Li and Huanan Wu and Juexi Shao and Yinghao Ma and Yujian Gan and Yihao Luo and Yuwei Wang and Dong Nie and Lu Wang and Wengqing Wu and Le Zhang and Massimo Poesio and Juntao Yu", "abstract": "Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \\emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \\emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.\n  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \\textbf{89\\%} and reduces end-to-end latency by \\textbf{28\\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \\textbf{+20.5} and \\textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.", "link": "http://arxiv.org/abs/2511.11910v2", "date": "2025-11-21", "relevancy": 2.7595, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20the%20Forest%20and%20the%20Trees%3A%20Query-Aware%20Tokenizer%20for%20Long-Video%20Multimodal%20Language%20Models&body=Title%3A%20Seeing%20the%20Forest%20and%20the%20Trees%3A%20Query-Aware%20Tokenizer%20for%20Long-Video%20Multimodal%20Language%20Models%0AAuthor%3A%20Siyou%20Li%20and%20Huanan%20Wu%20and%20Juexi%20Shao%20and%20Yinghao%20Ma%20and%20Yujian%20Gan%20and%20Yihao%20Luo%20and%20Yuwei%20Wang%20and%20Dong%20Nie%20and%20Lu%20Wang%20and%20Wengqing%20Wu%20and%20Le%20Zhang%20and%20Massimo%20Poesio%20and%20Juntao%20Yu%0AAbstract%3A%20Despite%20the%20recent%20advances%20in%20the%20video%20understanding%20ability%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20long%20video%20understanding%20remains%20a%20challenge.%20One%20of%20the%20main%20issues%20is%20that%20the%20number%20of%20vision%20tokens%20grows%20linearly%20with%20video%20length%2C%20which%20causes%20an%20explosion%20in%20attention%20cost%2C%20memory%2C%20and%20latency.%20To%20solve%20this%20challenge%2C%20we%20present%20Query-aware%20Token%20Selector%20%28%5Ctextbf%7BQTSplus%7D%29%2C%20a%20lightweight%20yet%20powerful%20visual%20token%20selection%20module%20that%20serves%20as%20an%20information%20gate%20between%20the%20vision%20encoder%20and%20LLMs.%20Given%20a%20text%20query%20and%20video%20tokens%2C%20QTSplus%20dynamically%20selects%20the%20most%20important%20visual%20evidence%20for%20the%20input%20text%20query%20by%20%28i%29%20scoring%20visual%20tokens%20via%20cross-attention%2C%20%28ii%29%20%5Cemph%7Bpredicting%7D%20an%20instance-specific%20retention%20budget%20based%20on%20the%20complexity%20of%20the%20query%2C%20and%20%28iii%29%20%5Cemph%7Bselecting%7D%20Top-%24n%24%20tokens%20with%20a%20differentiable%20straight-through%20estimator%20during%20training%20and%20a%20hard%20gate%20at%20inference.%20Furthermore%2C%20a%20small%20re-encoder%20preserves%20temporal%20order%20using%20absolute%20time%20information%2C%20enabling%20second-level%20localization%20while%20maintaining%20global%20coverage.%0A%20%20Integrated%20into%20Qwen2.5-VL%2C%20QTSplus%20compresses%20the%20vision%20stream%20by%20up%20to%20%5Ctextbf%7B89%5C%25%7D%20and%20reduces%20end-to-end%20latency%20by%20%5Ctextbf%7B28%5C%25%7D%20on%20long%20videos.%20The%20evaluation%20on%20eight%20long%20video%20understanding%20benchmarks%20shows%20near-parity%20accuracy%20overall%20when%20compared%20with%20the%20original%20Qwen%20models%20and%20outperforms%20the%20original%20model%20by%20%5Ctextbf%7B%2B20.5%7D%20and%20%5Ctextbf%7B%2B5.6%7D%20points%20respectively%20on%20TempCompass%20direction%20and%20order%20accuracies.%20These%20results%20show%20that%20QTSplus%20is%20an%20effective%2C%20general%20mechanism%20for%20scaling%20MLLMs%20to%20real-world%20long-video%20scenarios%20while%20preserving%20task-relevant%20evidence.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11910v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520the%2520Forest%2520and%2520the%2520Trees%253A%2520Query-Aware%2520Tokenizer%2520for%2520Long-Video%2520Multimodal%2520Language%2520Models%26entry.906535625%3DSiyou%2520Li%2520and%2520Huanan%2520Wu%2520and%2520Juexi%2520Shao%2520and%2520Yinghao%2520Ma%2520and%2520Yujian%2520Gan%2520and%2520Yihao%2520Luo%2520and%2520Yuwei%2520Wang%2520and%2520Dong%2520Nie%2520and%2520Lu%2520Wang%2520and%2520Wengqing%2520Wu%2520and%2520Le%2520Zhang%2520and%2520Massimo%2520Poesio%2520and%2520Juntao%2520Yu%26entry.1292438233%3DDespite%2520the%2520recent%2520advances%2520in%2520the%2520video%2520understanding%2520ability%2520of%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520long%2520video%2520understanding%2520remains%2520a%2520challenge.%2520One%2520of%2520the%2520main%2520issues%2520is%2520that%2520the%2520number%2520of%2520vision%2520tokens%2520grows%2520linearly%2520with%2520video%2520length%252C%2520which%2520causes%2520an%2520explosion%2520in%2520attention%2520cost%252C%2520memory%252C%2520and%2520latency.%2520To%2520solve%2520this%2520challenge%252C%2520we%2520present%2520Query-aware%2520Token%2520Selector%2520%2528%255Ctextbf%257BQTSplus%257D%2529%252C%2520a%2520lightweight%2520yet%2520powerful%2520visual%2520token%2520selection%2520module%2520that%2520serves%2520as%2520an%2520information%2520gate%2520between%2520the%2520vision%2520encoder%2520and%2520LLMs.%2520Given%2520a%2520text%2520query%2520and%2520video%2520tokens%252C%2520QTSplus%2520dynamically%2520selects%2520the%2520most%2520important%2520visual%2520evidence%2520for%2520the%2520input%2520text%2520query%2520by%2520%2528i%2529%2520scoring%2520visual%2520tokens%2520via%2520cross-attention%252C%2520%2528ii%2529%2520%255Cemph%257Bpredicting%257D%2520an%2520instance-specific%2520retention%2520budget%2520based%2520on%2520the%2520complexity%2520of%2520the%2520query%252C%2520and%2520%2528iii%2529%2520%255Cemph%257Bselecting%257D%2520Top-%2524n%2524%2520tokens%2520with%2520a%2520differentiable%2520straight-through%2520estimator%2520during%2520training%2520and%2520a%2520hard%2520gate%2520at%2520inference.%2520Furthermore%252C%2520a%2520small%2520re-encoder%2520preserves%2520temporal%2520order%2520using%2520absolute%2520time%2520information%252C%2520enabling%2520second-level%2520localization%2520while%2520maintaining%2520global%2520coverage.%250A%2520%2520Integrated%2520into%2520Qwen2.5-VL%252C%2520QTSplus%2520compresses%2520the%2520vision%2520stream%2520by%2520up%2520to%2520%255Ctextbf%257B89%255C%2525%257D%2520and%2520reduces%2520end-to-end%2520latency%2520by%2520%255Ctextbf%257B28%255C%2525%257D%2520on%2520long%2520videos.%2520The%2520evaluation%2520on%2520eight%2520long%2520video%2520understanding%2520benchmarks%2520shows%2520near-parity%2520accuracy%2520overall%2520when%2520compared%2520with%2520the%2520original%2520Qwen%2520models%2520and%2520outperforms%2520the%2520original%2520model%2520by%2520%255Ctextbf%257B%252B20.5%257D%2520and%2520%255Ctextbf%257B%252B5.6%257D%2520points%2520respectively%2520on%2520TempCompass%2520direction%2520and%2520order%2520accuracies.%2520These%2520results%2520show%2520that%2520QTSplus%2520is%2520an%2520effective%252C%2520general%2520mechanism%2520for%2520scaling%2520MLLMs%2520to%2520real-world%2520long-video%2520scenarios%2520while%2520preserving%2520task-relevant%2520evidence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11910v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20the%20Forest%20and%20the%20Trees%3A%20Query-Aware%20Tokenizer%20for%20Long-Video%20Multimodal%20Language%20Models&entry.906535625=Siyou%20Li%20and%20Huanan%20Wu%20and%20Juexi%20Shao%20and%20Yinghao%20Ma%20and%20Yujian%20Gan%20and%20Yihao%20Luo%20and%20Yuwei%20Wang%20and%20Dong%20Nie%20and%20Lu%20Wang%20and%20Wengqing%20Wu%20and%20Le%20Zhang%20and%20Massimo%20Poesio%20and%20Juntao%20Yu&entry.1292438233=Despite%20the%20recent%20advances%20in%20the%20video%20understanding%20ability%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20long%20video%20understanding%20remains%20a%20challenge.%20One%20of%20the%20main%20issues%20is%20that%20the%20number%20of%20vision%20tokens%20grows%20linearly%20with%20video%20length%2C%20which%20causes%20an%20explosion%20in%20attention%20cost%2C%20memory%2C%20and%20latency.%20To%20solve%20this%20challenge%2C%20we%20present%20Query-aware%20Token%20Selector%20%28%5Ctextbf%7BQTSplus%7D%29%2C%20a%20lightweight%20yet%20powerful%20visual%20token%20selection%20module%20that%20serves%20as%20an%20information%20gate%20between%20the%20vision%20encoder%20and%20LLMs.%20Given%20a%20text%20query%20and%20video%20tokens%2C%20QTSplus%20dynamically%20selects%20the%20most%20important%20visual%20evidence%20for%20the%20input%20text%20query%20by%20%28i%29%20scoring%20visual%20tokens%20via%20cross-attention%2C%20%28ii%29%20%5Cemph%7Bpredicting%7D%20an%20instance-specific%20retention%20budget%20based%20on%20the%20complexity%20of%20the%20query%2C%20and%20%28iii%29%20%5Cemph%7Bselecting%7D%20Top-%24n%24%20tokens%20with%20a%20differentiable%20straight-through%20estimator%20during%20training%20and%20a%20hard%20gate%20at%20inference.%20Furthermore%2C%20a%20small%20re-encoder%20preserves%20temporal%20order%20using%20absolute%20time%20information%2C%20enabling%20second-level%20localization%20while%20maintaining%20global%20coverage.%0A%20%20Integrated%20into%20Qwen2.5-VL%2C%20QTSplus%20compresses%20the%20vision%20stream%20by%20up%20to%20%5Ctextbf%7B89%5C%25%7D%20and%20reduces%20end-to-end%20latency%20by%20%5Ctextbf%7B28%5C%25%7D%20on%20long%20videos.%20The%20evaluation%20on%20eight%20long%20video%20understanding%20benchmarks%20shows%20near-parity%20accuracy%20overall%20when%20compared%20with%20the%20original%20Qwen%20models%20and%20outperforms%20the%20original%20model%20by%20%5Ctextbf%7B%2B20.5%7D%20and%20%5Ctextbf%7B%2B5.6%7D%20points%20respectively%20on%20TempCompass%20direction%20and%20order%20accuracies.%20These%20results%20show%20that%20QTSplus%20is%20an%20effective%2C%20general%20mechanism%20for%20scaling%20MLLMs%20to%20real-world%20long-video%20scenarios%20while%20preserving%20task-relevant%20evidence.&entry.1838667208=http%3A//arxiv.org/abs/2511.11910v2&entry.124074799=Read"},
{"title": "Continual Alignment for SAM: Rethinking Foundation Models for Medical Image Segmentation in Continual Learning", "author": "Jiayi Wang and Wei Dai and Haoyu Wang and Sihan Yang and Haixia Bi and Jian Sun", "abstract": "In medical image segmentation, heterogeneous privacy policies across institutions often make joint training on pooled datasets infeasible, motivating continual image segmentation-learning from data streams without catastrophic forgetting. While the Segment Anything Model (SAM) offers strong zero-shot priors and has been widely fine-tuned across downstream tasks, its large parameter count and computational overhead challenge practical deployment. This paper demonstrates that the SAM paradigm is highly promising once its computational efficiency and performance can be balanced. To this end, we introduce the Alignment Layer, a lightweight, plug-and-play module which aligns encoder-decoder feature distributions to efficiently adapt SAM to specific medical images, improving accuracy while reducing computation. Building on SAM and the Alignment Layer, we then propose Continual Alignment for SAM (CA-SAM), a continual learning strategy that automatically adapts the appropriate Alignment Layer to mitigate catastrophic forgetting, while leveraging SAM's zero-shot priors to preserve strong performance on unseen medical datasets. Experimented across nine medical segmentation datasets under continual-learning scenario, CA-SAM achieves state-of-the-art performance. Our code, models and datasets will be released on \\mbox{https://github.com/azzzzyo/Continual-Alignment-for-SAM.}", "link": "http://arxiv.org/abs/2511.17201v1", "date": "2025-11-21", "relevancy": 2.7463, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5891}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5305}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Alignment%20for%20SAM%3A%20Rethinking%20Foundation%20Models%20for%20Medical%20Image%20Segmentation%20in%20Continual%20Learning&body=Title%3A%20Continual%20Alignment%20for%20SAM%3A%20Rethinking%20Foundation%20Models%20for%20Medical%20Image%20Segmentation%20in%20Continual%20Learning%0AAuthor%3A%20Jiayi%20Wang%20and%20Wei%20Dai%20and%20Haoyu%20Wang%20and%20Sihan%20Yang%20and%20Haixia%20Bi%20and%20Jian%20Sun%0AAbstract%3A%20In%20medical%20image%20segmentation%2C%20heterogeneous%20privacy%20policies%20across%20institutions%20often%20make%20joint%20training%20on%20pooled%20datasets%20infeasible%2C%20motivating%20continual%20image%20segmentation-learning%20from%20data%20streams%20without%20catastrophic%20forgetting.%20While%20the%20Segment%20Anything%20Model%20%28SAM%29%20offers%20strong%20zero-shot%20priors%20and%20has%20been%20widely%20fine-tuned%20across%20downstream%20tasks%2C%20its%20large%20parameter%20count%20and%20computational%20overhead%20challenge%20practical%20deployment.%20This%20paper%20demonstrates%20that%20the%20SAM%20paradigm%20is%20highly%20promising%20once%20its%20computational%20efficiency%20and%20performance%20can%20be%20balanced.%20To%20this%20end%2C%20we%20introduce%20the%20Alignment%20Layer%2C%20a%20lightweight%2C%20plug-and-play%20module%20which%20aligns%20encoder-decoder%20feature%20distributions%20to%20efficiently%20adapt%20SAM%20to%20specific%20medical%20images%2C%20improving%20accuracy%20while%20reducing%20computation.%20Building%20on%20SAM%20and%20the%20Alignment%20Layer%2C%20we%20then%20propose%20Continual%20Alignment%20for%20SAM%20%28CA-SAM%29%2C%20a%20continual%20learning%20strategy%20that%20automatically%20adapts%20the%20appropriate%20Alignment%20Layer%20to%20mitigate%20catastrophic%20forgetting%2C%20while%20leveraging%20SAM%27s%20zero-shot%20priors%20to%20preserve%20strong%20performance%20on%20unseen%20medical%20datasets.%20Experimented%20across%20nine%20medical%20segmentation%20datasets%20under%20continual-learning%20scenario%2C%20CA-SAM%20achieves%20state-of-the-art%20performance.%20Our%20code%2C%20models%20and%20datasets%20will%20be%20released%20on%20%5Cmbox%7Bhttps%3A//github.com/azzzzyo/Continual-Alignment-for-SAM.%7D%0ALink%3A%20http%3A//arxiv.org/abs/2511.17201v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Alignment%2520for%2520SAM%253A%2520Rethinking%2520Foundation%2520Models%2520for%2520Medical%2520Image%2520Segmentation%2520in%2520Continual%2520Learning%26entry.906535625%3DJiayi%2520Wang%2520and%2520Wei%2520Dai%2520and%2520Haoyu%2520Wang%2520and%2520Sihan%2520Yang%2520and%2520Haixia%2520Bi%2520and%2520Jian%2520Sun%26entry.1292438233%3DIn%2520medical%2520image%2520segmentation%252C%2520heterogeneous%2520privacy%2520policies%2520across%2520institutions%2520often%2520make%2520joint%2520training%2520on%2520pooled%2520datasets%2520infeasible%252C%2520motivating%2520continual%2520image%2520segmentation-learning%2520from%2520data%2520streams%2520without%2520catastrophic%2520forgetting.%2520While%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520offers%2520strong%2520zero-shot%2520priors%2520and%2520has%2520been%2520widely%2520fine-tuned%2520across%2520downstream%2520tasks%252C%2520its%2520large%2520parameter%2520count%2520and%2520computational%2520overhead%2520challenge%2520practical%2520deployment.%2520This%2520paper%2520demonstrates%2520that%2520the%2520SAM%2520paradigm%2520is%2520highly%2520promising%2520once%2520its%2520computational%2520efficiency%2520and%2520performance%2520can%2520be%2520balanced.%2520To%2520this%2520end%252C%2520we%2520introduce%2520the%2520Alignment%2520Layer%252C%2520a%2520lightweight%252C%2520plug-and-play%2520module%2520which%2520aligns%2520encoder-decoder%2520feature%2520distributions%2520to%2520efficiently%2520adapt%2520SAM%2520to%2520specific%2520medical%2520images%252C%2520improving%2520accuracy%2520while%2520reducing%2520computation.%2520Building%2520on%2520SAM%2520and%2520the%2520Alignment%2520Layer%252C%2520we%2520then%2520propose%2520Continual%2520Alignment%2520for%2520SAM%2520%2528CA-SAM%2529%252C%2520a%2520continual%2520learning%2520strategy%2520that%2520automatically%2520adapts%2520the%2520appropriate%2520Alignment%2520Layer%2520to%2520mitigate%2520catastrophic%2520forgetting%252C%2520while%2520leveraging%2520SAM%2527s%2520zero-shot%2520priors%2520to%2520preserve%2520strong%2520performance%2520on%2520unseen%2520medical%2520datasets.%2520Experimented%2520across%2520nine%2520medical%2520segmentation%2520datasets%2520under%2520continual-learning%2520scenario%252C%2520CA-SAM%2520achieves%2520state-of-the-art%2520performance.%2520Our%2520code%252C%2520models%2520and%2520datasets%2520will%2520be%2520released%2520on%2520%255Cmbox%257Bhttps%253A//github.com/azzzzyo/Continual-Alignment-for-SAM.%257D%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17201v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Alignment%20for%20SAM%3A%20Rethinking%20Foundation%20Models%20for%20Medical%20Image%20Segmentation%20in%20Continual%20Learning&entry.906535625=Jiayi%20Wang%20and%20Wei%20Dai%20and%20Haoyu%20Wang%20and%20Sihan%20Yang%20and%20Haixia%20Bi%20and%20Jian%20Sun&entry.1292438233=In%20medical%20image%20segmentation%2C%20heterogeneous%20privacy%20policies%20across%20institutions%20often%20make%20joint%20training%20on%20pooled%20datasets%20infeasible%2C%20motivating%20continual%20image%20segmentation-learning%20from%20data%20streams%20without%20catastrophic%20forgetting.%20While%20the%20Segment%20Anything%20Model%20%28SAM%29%20offers%20strong%20zero-shot%20priors%20and%20has%20been%20widely%20fine-tuned%20across%20downstream%20tasks%2C%20its%20large%20parameter%20count%20and%20computational%20overhead%20challenge%20practical%20deployment.%20This%20paper%20demonstrates%20that%20the%20SAM%20paradigm%20is%20highly%20promising%20once%20its%20computational%20efficiency%20and%20performance%20can%20be%20balanced.%20To%20this%20end%2C%20we%20introduce%20the%20Alignment%20Layer%2C%20a%20lightweight%2C%20plug-and-play%20module%20which%20aligns%20encoder-decoder%20feature%20distributions%20to%20efficiently%20adapt%20SAM%20to%20specific%20medical%20images%2C%20improving%20accuracy%20while%20reducing%20computation.%20Building%20on%20SAM%20and%20the%20Alignment%20Layer%2C%20we%20then%20propose%20Continual%20Alignment%20for%20SAM%20%28CA-SAM%29%2C%20a%20continual%20learning%20strategy%20that%20automatically%20adapts%20the%20appropriate%20Alignment%20Layer%20to%20mitigate%20catastrophic%20forgetting%2C%20while%20leveraging%20SAM%27s%20zero-shot%20priors%20to%20preserve%20strong%20performance%20on%20unseen%20medical%20datasets.%20Experimented%20across%20nine%20medical%20segmentation%20datasets%20under%20continual-learning%20scenario%2C%20CA-SAM%20achieves%20state-of-the-art%20performance.%20Our%20code%2C%20models%20and%20datasets%20will%20be%20released%20on%20%5Cmbox%7Bhttps%3A//github.com/azzzzyo/Continual-Alignment-for-SAM.%7D&entry.1838667208=http%3A//arxiv.org/abs/2511.17201v1&entry.124074799=Read"},
{"title": "Learning to Compress: Unlocking the Potential of Large Language Models for Text Representation", "author": "Yeqin Zhang and Yizheng Zhao and Chen Hu and Binxing Jiao and Daxin Jiang and Ruihang Miao and Cam-Tu Nguyen", "abstract": "Text representation plays a critical role in tasks like clustering, retrieval, and other downstream applications. With the emergence of large language models (LLMs), there is increasing interest in harnessing their capabilities for this purpose. However, most of the LLMs are inherently causal and optimized for next-token prediction, making them suboptimal for producing holistic representations. To address this, recent studies introduced pretext tasks to adapt LLMs for text representation. Most of these tasks, however, rely on token-level prediction objectives, such as the masked next-token prediction (MNTP) used in LLM2Vec. In this work, we explore the untapped potential of context compression as a pretext task for unsupervised adaptation of LLMs. During compression pre-training, the model learns to generate compact memory tokens, which substitute the whole context for downstream sequence prediction. Experiments demonstrate that a well-designed compression objective can significantly enhance LLM-based text representations, outperforming models trained with token-level pretext tasks. Further improvements through contrastive learning produce a strong representation model (LLM2Comp) that outperforms contemporary LLM-based text encoders on a wide range of tasks while being more sample-efficient, requiring significantly less training data.", "link": "http://arxiv.org/abs/2511.17129v1", "date": "2025-11-21", "relevancy": 2.7072, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5468}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5468}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Compress%3A%20Unlocking%20the%20Potential%20of%20Large%20Language%20Models%20for%20Text%20Representation&body=Title%3A%20Learning%20to%20Compress%3A%20Unlocking%20the%20Potential%20of%20Large%20Language%20Models%20for%20Text%20Representation%0AAuthor%3A%20Yeqin%20Zhang%20and%20Yizheng%20Zhao%20and%20Chen%20Hu%20and%20Binxing%20Jiao%20and%20Daxin%20Jiang%20and%20Ruihang%20Miao%20and%20Cam-Tu%20Nguyen%0AAbstract%3A%20Text%20representation%20plays%20a%20critical%20role%20in%20tasks%20like%20clustering%2C%20retrieval%2C%20and%20other%20downstream%20applications.%20With%20the%20emergence%20of%20large%20language%20models%20%28LLMs%29%2C%20there%20is%20increasing%20interest%20in%20harnessing%20their%20capabilities%20for%20this%20purpose.%20However%2C%20most%20of%20the%20LLMs%20are%20inherently%20causal%20and%20optimized%20for%20next-token%20prediction%2C%20making%20them%20suboptimal%20for%20producing%20holistic%20representations.%20To%20address%20this%2C%20recent%20studies%20introduced%20pretext%20tasks%20to%20adapt%20LLMs%20for%20text%20representation.%20Most%20of%20these%20tasks%2C%20however%2C%20rely%20on%20token-level%20prediction%20objectives%2C%20such%20as%20the%20masked%20next-token%20prediction%20%28MNTP%29%20used%20in%20LLM2Vec.%20In%20this%20work%2C%20we%20explore%20the%20untapped%20potential%20of%20context%20compression%20as%20a%20pretext%20task%20for%20unsupervised%20adaptation%20of%20LLMs.%20During%20compression%20pre-training%2C%20the%20model%20learns%20to%20generate%20compact%20memory%20tokens%2C%20which%20substitute%20the%20whole%20context%20for%20downstream%20sequence%20prediction.%20Experiments%20demonstrate%20that%20a%20well-designed%20compression%20objective%20can%20significantly%20enhance%20LLM-based%20text%20representations%2C%20outperforming%20models%20trained%20with%20token-level%20pretext%20tasks.%20Further%20improvements%20through%20contrastive%20learning%20produce%20a%20strong%20representation%20model%20%28LLM2Comp%29%20that%20outperforms%20contemporary%20LLM-based%20text%20encoders%20on%20a%20wide%20range%20of%20tasks%20while%20being%20more%20sample-efficient%2C%20requiring%20significantly%20less%20training%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Compress%253A%2520Unlocking%2520the%2520Potential%2520of%2520Large%2520Language%2520Models%2520for%2520Text%2520Representation%26entry.906535625%3DYeqin%2520Zhang%2520and%2520Yizheng%2520Zhao%2520and%2520Chen%2520Hu%2520and%2520Binxing%2520Jiao%2520and%2520Daxin%2520Jiang%2520and%2520Ruihang%2520Miao%2520and%2520Cam-Tu%2520Nguyen%26entry.1292438233%3DText%2520representation%2520plays%2520a%2520critical%2520role%2520in%2520tasks%2520like%2520clustering%252C%2520retrieval%252C%2520and%2520other%2520downstream%2520applications.%2520With%2520the%2520emergence%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520there%2520is%2520increasing%2520interest%2520in%2520harnessing%2520their%2520capabilities%2520for%2520this%2520purpose.%2520However%252C%2520most%2520of%2520the%2520LLMs%2520are%2520inherently%2520causal%2520and%2520optimized%2520for%2520next-token%2520prediction%252C%2520making%2520them%2520suboptimal%2520for%2520producing%2520holistic%2520representations.%2520To%2520address%2520this%252C%2520recent%2520studies%2520introduced%2520pretext%2520tasks%2520to%2520adapt%2520LLMs%2520for%2520text%2520representation.%2520Most%2520of%2520these%2520tasks%252C%2520however%252C%2520rely%2520on%2520token-level%2520prediction%2520objectives%252C%2520such%2520as%2520the%2520masked%2520next-token%2520prediction%2520%2528MNTP%2529%2520used%2520in%2520LLM2Vec.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520untapped%2520potential%2520of%2520context%2520compression%2520as%2520a%2520pretext%2520task%2520for%2520unsupervised%2520adaptation%2520of%2520LLMs.%2520During%2520compression%2520pre-training%252C%2520the%2520model%2520learns%2520to%2520generate%2520compact%2520memory%2520tokens%252C%2520which%2520substitute%2520the%2520whole%2520context%2520for%2520downstream%2520sequence%2520prediction.%2520Experiments%2520demonstrate%2520that%2520a%2520well-designed%2520compression%2520objective%2520can%2520significantly%2520enhance%2520LLM-based%2520text%2520representations%252C%2520outperforming%2520models%2520trained%2520with%2520token-level%2520pretext%2520tasks.%2520Further%2520improvements%2520through%2520contrastive%2520learning%2520produce%2520a%2520strong%2520representation%2520model%2520%2528LLM2Comp%2529%2520that%2520outperforms%2520contemporary%2520LLM-based%2520text%2520encoders%2520on%2520a%2520wide%2520range%2520of%2520tasks%2520while%2520being%2520more%2520sample-efficient%252C%2520requiring%2520significantly%2520less%2520training%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Compress%3A%20Unlocking%20the%20Potential%20of%20Large%20Language%20Models%20for%20Text%20Representation&entry.906535625=Yeqin%20Zhang%20and%20Yizheng%20Zhao%20and%20Chen%20Hu%20and%20Binxing%20Jiao%20and%20Daxin%20Jiang%20and%20Ruihang%20Miao%20and%20Cam-Tu%20Nguyen&entry.1292438233=Text%20representation%20plays%20a%20critical%20role%20in%20tasks%20like%20clustering%2C%20retrieval%2C%20and%20other%20downstream%20applications.%20With%20the%20emergence%20of%20large%20language%20models%20%28LLMs%29%2C%20there%20is%20increasing%20interest%20in%20harnessing%20their%20capabilities%20for%20this%20purpose.%20However%2C%20most%20of%20the%20LLMs%20are%20inherently%20causal%20and%20optimized%20for%20next-token%20prediction%2C%20making%20them%20suboptimal%20for%20producing%20holistic%20representations.%20To%20address%20this%2C%20recent%20studies%20introduced%20pretext%20tasks%20to%20adapt%20LLMs%20for%20text%20representation.%20Most%20of%20these%20tasks%2C%20however%2C%20rely%20on%20token-level%20prediction%20objectives%2C%20such%20as%20the%20masked%20next-token%20prediction%20%28MNTP%29%20used%20in%20LLM2Vec.%20In%20this%20work%2C%20we%20explore%20the%20untapped%20potential%20of%20context%20compression%20as%20a%20pretext%20task%20for%20unsupervised%20adaptation%20of%20LLMs.%20During%20compression%20pre-training%2C%20the%20model%20learns%20to%20generate%20compact%20memory%20tokens%2C%20which%20substitute%20the%20whole%20context%20for%20downstream%20sequence%20prediction.%20Experiments%20demonstrate%20that%20a%20well-designed%20compression%20objective%20can%20significantly%20enhance%20LLM-based%20text%20representations%2C%20outperforming%20models%20trained%20with%20token-level%20pretext%20tasks.%20Further%20improvements%20through%20contrastive%20learning%20produce%20a%20strong%20representation%20model%20%28LLM2Comp%29%20that%20outperforms%20contemporary%20LLM-based%20text%20encoders%20on%20a%20wide%20range%20of%20tasks%20while%20being%20more%20sample-efficient%2C%20requiring%20significantly%20less%20training%20data.&entry.1838667208=http%3A//arxiv.org/abs/2511.17129v1&entry.124074799=Read"},
{"title": "Radar2Shape: 3D Shape Reconstruction from High-Frequency Radar using Multiresolution Signed Distance Functions", "author": "Neel Sortur and Justin Goodwin and Purvik Patel and Luis Enrique Martinez and Tzofi Klinghoffer and Rajmonda S. Caceres and Robin Walters", "abstract": "Determining the shape of 3D objects from high-frequency radar signals is analytically complex but critical for commercial and aerospace applications. Previous deep learning methods have been applied to radar modeling; however, they often fail to represent arbitrary shapes or have difficulty with real-world radar signals which are collected over limited viewing angles. Existing methods in optical 3D reconstruction can generate arbitrary shapes from limited camera views, but struggle when they naively treat the radar signal as a camera view. In this work, we present Radar2Shape, a denoising diffusion model that handles a partially observable radar signal for 3D reconstruction by correlating its frequencies with multiresolution shape features. Our method consists of a two-stage approach: first, Radar2Shape learns a regularized latent space with hierarchical resolutions of shape features, and second, it diffuses into this latent space by conditioning on the frequencies of the radar signal in an analogous coarse-to-fine manner. We demonstrate that Radar2Shape can successfully reconstruct arbitrary 3D shapes even from partially-observed radar signals, and we show robust generalization to two different simulation methods and real-world data. Additionally, we release two synthetic benchmark datasets to encourage future research in the high-frequency radar domain so that models like Radar2Shape can safely be adapted into real-world radar systems.", "link": "http://arxiv.org/abs/2511.17484v1", "date": "2025-11-21", "relevancy": 2.6349, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5519}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5161}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Radar2Shape%3A%203D%20Shape%20Reconstruction%20from%20High-Frequency%20Radar%20using%20Multiresolution%20Signed%20Distance%20Functions&body=Title%3A%20Radar2Shape%3A%203D%20Shape%20Reconstruction%20from%20High-Frequency%20Radar%20using%20Multiresolution%20Signed%20Distance%20Functions%0AAuthor%3A%20Neel%20Sortur%20and%20Justin%20Goodwin%20and%20Purvik%20Patel%20and%20Luis%20Enrique%20Martinez%20and%20Tzofi%20Klinghoffer%20and%20Rajmonda%20S.%20Caceres%20and%20Robin%20Walters%0AAbstract%3A%20Determining%20the%20shape%20of%203D%20objects%20from%20high-frequency%20radar%20signals%20is%20analytically%20complex%20but%20critical%20for%20commercial%20and%20aerospace%20applications.%20Previous%20deep%20learning%20methods%20have%20been%20applied%20to%20radar%20modeling%3B%20however%2C%20they%20often%20fail%20to%20represent%20arbitrary%20shapes%20or%20have%20difficulty%20with%20real-world%20radar%20signals%20which%20are%20collected%20over%20limited%20viewing%20angles.%20Existing%20methods%20in%20optical%203D%20reconstruction%20can%20generate%20arbitrary%20shapes%20from%20limited%20camera%20views%2C%20but%20struggle%20when%20they%20naively%20treat%20the%20radar%20signal%20as%20a%20camera%20view.%20In%20this%20work%2C%20we%20present%20Radar2Shape%2C%20a%20denoising%20diffusion%20model%20that%20handles%20a%20partially%20observable%20radar%20signal%20for%203D%20reconstruction%20by%20correlating%20its%20frequencies%20with%20multiresolution%20shape%20features.%20Our%20method%20consists%20of%20a%20two-stage%20approach%3A%20first%2C%20Radar2Shape%20learns%20a%20regularized%20latent%20space%20with%20hierarchical%20resolutions%20of%20shape%20features%2C%20and%20second%2C%20it%20diffuses%20into%20this%20latent%20space%20by%20conditioning%20on%20the%20frequencies%20of%20the%20radar%20signal%20in%20an%20analogous%20coarse-to-fine%20manner.%20We%20demonstrate%20that%20Radar2Shape%20can%20successfully%20reconstruct%20arbitrary%203D%20shapes%20even%20from%20partially-observed%20radar%20signals%2C%20and%20we%20show%20robust%20generalization%20to%20two%20different%20simulation%20methods%20and%20real-world%20data.%20Additionally%2C%20we%20release%20two%20synthetic%20benchmark%20datasets%20to%20encourage%20future%20research%20in%20the%20high-frequency%20radar%20domain%20so%20that%20models%20like%20Radar2Shape%20can%20safely%20be%20adapted%20into%20real-world%20radar%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17484v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadar2Shape%253A%25203D%2520Shape%2520Reconstruction%2520from%2520High-Frequency%2520Radar%2520using%2520Multiresolution%2520Signed%2520Distance%2520Functions%26entry.906535625%3DNeel%2520Sortur%2520and%2520Justin%2520Goodwin%2520and%2520Purvik%2520Patel%2520and%2520Luis%2520Enrique%2520Martinez%2520and%2520Tzofi%2520Klinghoffer%2520and%2520Rajmonda%2520S.%2520Caceres%2520and%2520Robin%2520Walters%26entry.1292438233%3DDetermining%2520the%2520shape%2520of%25203D%2520objects%2520from%2520high-frequency%2520radar%2520signals%2520is%2520analytically%2520complex%2520but%2520critical%2520for%2520commercial%2520and%2520aerospace%2520applications.%2520Previous%2520deep%2520learning%2520methods%2520have%2520been%2520applied%2520to%2520radar%2520modeling%253B%2520however%252C%2520they%2520often%2520fail%2520to%2520represent%2520arbitrary%2520shapes%2520or%2520have%2520difficulty%2520with%2520real-world%2520radar%2520signals%2520which%2520are%2520collected%2520over%2520limited%2520viewing%2520angles.%2520Existing%2520methods%2520in%2520optical%25203D%2520reconstruction%2520can%2520generate%2520arbitrary%2520shapes%2520from%2520limited%2520camera%2520views%252C%2520but%2520struggle%2520when%2520they%2520naively%2520treat%2520the%2520radar%2520signal%2520as%2520a%2520camera%2520view.%2520In%2520this%2520work%252C%2520we%2520present%2520Radar2Shape%252C%2520a%2520denoising%2520diffusion%2520model%2520that%2520handles%2520a%2520partially%2520observable%2520radar%2520signal%2520for%25203D%2520reconstruction%2520by%2520correlating%2520its%2520frequencies%2520with%2520multiresolution%2520shape%2520features.%2520Our%2520method%2520consists%2520of%2520a%2520two-stage%2520approach%253A%2520first%252C%2520Radar2Shape%2520learns%2520a%2520regularized%2520latent%2520space%2520with%2520hierarchical%2520resolutions%2520of%2520shape%2520features%252C%2520and%2520second%252C%2520it%2520diffuses%2520into%2520this%2520latent%2520space%2520by%2520conditioning%2520on%2520the%2520frequencies%2520of%2520the%2520radar%2520signal%2520in%2520an%2520analogous%2520coarse-to-fine%2520manner.%2520We%2520demonstrate%2520that%2520Radar2Shape%2520can%2520successfully%2520reconstruct%2520arbitrary%25203D%2520shapes%2520even%2520from%2520partially-observed%2520radar%2520signals%252C%2520and%2520we%2520show%2520robust%2520generalization%2520to%2520two%2520different%2520simulation%2520methods%2520and%2520real-world%2520data.%2520Additionally%252C%2520we%2520release%2520two%2520synthetic%2520benchmark%2520datasets%2520to%2520encourage%2520future%2520research%2520in%2520the%2520high-frequency%2520radar%2520domain%2520so%2520that%2520models%2520like%2520Radar2Shape%2520can%2520safely%2520be%2520adapted%2520into%2520real-world%2520radar%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17484v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Radar2Shape%3A%203D%20Shape%20Reconstruction%20from%20High-Frequency%20Radar%20using%20Multiresolution%20Signed%20Distance%20Functions&entry.906535625=Neel%20Sortur%20and%20Justin%20Goodwin%20and%20Purvik%20Patel%20and%20Luis%20Enrique%20Martinez%20and%20Tzofi%20Klinghoffer%20and%20Rajmonda%20S.%20Caceres%20and%20Robin%20Walters&entry.1292438233=Determining%20the%20shape%20of%203D%20objects%20from%20high-frequency%20radar%20signals%20is%20analytically%20complex%20but%20critical%20for%20commercial%20and%20aerospace%20applications.%20Previous%20deep%20learning%20methods%20have%20been%20applied%20to%20radar%20modeling%3B%20however%2C%20they%20often%20fail%20to%20represent%20arbitrary%20shapes%20or%20have%20difficulty%20with%20real-world%20radar%20signals%20which%20are%20collected%20over%20limited%20viewing%20angles.%20Existing%20methods%20in%20optical%203D%20reconstruction%20can%20generate%20arbitrary%20shapes%20from%20limited%20camera%20views%2C%20but%20struggle%20when%20they%20naively%20treat%20the%20radar%20signal%20as%20a%20camera%20view.%20In%20this%20work%2C%20we%20present%20Radar2Shape%2C%20a%20denoising%20diffusion%20model%20that%20handles%20a%20partially%20observable%20radar%20signal%20for%203D%20reconstruction%20by%20correlating%20its%20frequencies%20with%20multiresolution%20shape%20features.%20Our%20method%20consists%20of%20a%20two-stage%20approach%3A%20first%2C%20Radar2Shape%20learns%20a%20regularized%20latent%20space%20with%20hierarchical%20resolutions%20of%20shape%20features%2C%20and%20second%2C%20it%20diffuses%20into%20this%20latent%20space%20by%20conditioning%20on%20the%20frequencies%20of%20the%20radar%20signal%20in%20an%20analogous%20coarse-to-fine%20manner.%20We%20demonstrate%20that%20Radar2Shape%20can%20successfully%20reconstruct%20arbitrary%203D%20shapes%20even%20from%20partially-observed%20radar%20signals%2C%20and%20we%20show%20robust%20generalization%20to%20two%20different%20simulation%20methods%20and%20real-world%20data.%20Additionally%2C%20we%20release%20two%20synthetic%20benchmark%20datasets%20to%20encourage%20future%20research%20in%20the%20high-frequency%20radar%20domain%20so%20that%20models%20like%20Radar2Shape%20can%20safely%20be%20adapted%20into%20real-world%20radar%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2511.17484v1&entry.124074799=Read"},
{"title": "Navigating in the Dark: A Multimodal Framework and Dataset for Nighttime Traffic Sign Recognition", "author": "Aditya Mishra and Akshay Agarwal and Haroon Lone", "abstract": "Traffic signboards are vital for road safety and intelligent transportation systems, enabling navigation and autonomous driving. Yet, recognizing traffic signs at night remains challenging due to visual noise and scarcity of public nighttime datasets. Despite advances in vision architectures, existing methods struggle with robustness under low illumination and fail to leverage complementary mutlimodal cues effectively. To overcome these limitations, firstly, we introduce INTSD, a large-scale dataset comprising street-level night-time images of traffic signboards collected across diverse regions of India. The dataset spans 41 traffic signboard classes captured under varying lighting and weather conditions, providing a comprehensive benchmark for both detection and classification tasks. To benchmark INTSD for night-time sign recognition, we conduct extensive evaluations using state-of-the-art detection and classification models. Secondly, we propose LENS-Net, which integrates an adaptive image enhancement detector for joint illumination correction and sign localization, followed by a structured multimodal CLIP-GCNN classifier that leverages cross-modal attention and graph-based reasoning for robust and semantically consistent recognition. Our method surpasses existing frameworks, with ablation studies confirming the effectiveness of its key components. The dataset and code for LENS-Net is publicly available for research.", "link": "http://arxiv.org/abs/2511.17183v1", "date": "2025-11-21", "relevancy": 2.631, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5344}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5309}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Navigating%20in%20the%20Dark%3A%20A%20Multimodal%20Framework%20and%20Dataset%20for%20Nighttime%20Traffic%20Sign%20Recognition&body=Title%3A%20Navigating%20in%20the%20Dark%3A%20A%20Multimodal%20Framework%20and%20Dataset%20for%20Nighttime%20Traffic%20Sign%20Recognition%0AAuthor%3A%20Aditya%20Mishra%20and%20Akshay%20Agarwal%20and%20Haroon%20Lone%0AAbstract%3A%20Traffic%20signboards%20are%20vital%20for%20road%20safety%20and%20intelligent%20transportation%20systems%2C%20enabling%20navigation%20and%20autonomous%20driving.%20Yet%2C%20recognizing%20traffic%20signs%20at%20night%20remains%20challenging%20due%20to%20visual%20noise%20and%20scarcity%20of%20public%20nighttime%20datasets.%20Despite%20advances%20in%20vision%20architectures%2C%20existing%20methods%20struggle%20with%20robustness%20under%20low%20illumination%20and%20fail%20to%20leverage%20complementary%20mutlimodal%20cues%20effectively.%20To%20overcome%20these%20limitations%2C%20firstly%2C%20we%20introduce%20INTSD%2C%20a%20large-scale%20dataset%20comprising%20street-level%20night-time%20images%20of%20traffic%20signboards%20collected%20across%20diverse%20regions%20of%20India.%20The%20dataset%20spans%2041%20traffic%20signboard%20classes%20captured%20under%20varying%20lighting%20and%20weather%20conditions%2C%20providing%20a%20comprehensive%20benchmark%20for%20both%20detection%20and%20classification%20tasks.%20To%20benchmark%20INTSD%20for%20night-time%20sign%20recognition%2C%20we%20conduct%20extensive%20evaluations%20using%20state-of-the-art%20detection%20and%20classification%20models.%20Secondly%2C%20we%20propose%20LENS-Net%2C%20which%20integrates%20an%20adaptive%20image%20enhancement%20detector%20for%20joint%20illumination%20correction%20and%20sign%20localization%2C%20followed%20by%20a%20structured%20multimodal%20CLIP-GCNN%20classifier%20that%20leverages%20cross-modal%20attention%20and%20graph-based%20reasoning%20for%20robust%20and%20semantically%20consistent%20recognition.%20Our%20method%20surpasses%20existing%20frameworks%2C%20with%20ablation%20studies%20confirming%20the%20effectiveness%20of%20its%20key%20components.%20The%20dataset%20and%20code%20for%20LENS-Net%20is%20publicly%20available%20for%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigating%2520in%2520the%2520Dark%253A%2520A%2520Multimodal%2520Framework%2520and%2520Dataset%2520for%2520Nighttime%2520Traffic%2520Sign%2520Recognition%26entry.906535625%3DAditya%2520Mishra%2520and%2520Akshay%2520Agarwal%2520and%2520Haroon%2520Lone%26entry.1292438233%3DTraffic%2520signboards%2520are%2520vital%2520for%2520road%2520safety%2520and%2520intelligent%2520transportation%2520systems%252C%2520enabling%2520navigation%2520and%2520autonomous%2520driving.%2520Yet%252C%2520recognizing%2520traffic%2520signs%2520at%2520night%2520remains%2520challenging%2520due%2520to%2520visual%2520noise%2520and%2520scarcity%2520of%2520public%2520nighttime%2520datasets.%2520Despite%2520advances%2520in%2520vision%2520architectures%252C%2520existing%2520methods%2520struggle%2520with%2520robustness%2520under%2520low%2520illumination%2520and%2520fail%2520to%2520leverage%2520complementary%2520mutlimodal%2520cues%2520effectively.%2520To%2520overcome%2520these%2520limitations%252C%2520firstly%252C%2520we%2520introduce%2520INTSD%252C%2520a%2520large-scale%2520dataset%2520comprising%2520street-level%2520night-time%2520images%2520of%2520traffic%2520signboards%2520collected%2520across%2520diverse%2520regions%2520of%2520India.%2520The%2520dataset%2520spans%252041%2520traffic%2520signboard%2520classes%2520captured%2520under%2520varying%2520lighting%2520and%2520weather%2520conditions%252C%2520providing%2520a%2520comprehensive%2520benchmark%2520for%2520both%2520detection%2520and%2520classification%2520tasks.%2520To%2520benchmark%2520INTSD%2520for%2520night-time%2520sign%2520recognition%252C%2520we%2520conduct%2520extensive%2520evaluations%2520using%2520state-of-the-art%2520detection%2520and%2520classification%2520models.%2520Secondly%252C%2520we%2520propose%2520LENS-Net%252C%2520which%2520integrates%2520an%2520adaptive%2520image%2520enhancement%2520detector%2520for%2520joint%2520illumination%2520correction%2520and%2520sign%2520localization%252C%2520followed%2520by%2520a%2520structured%2520multimodal%2520CLIP-GCNN%2520classifier%2520that%2520leverages%2520cross-modal%2520attention%2520and%2520graph-based%2520reasoning%2520for%2520robust%2520and%2520semantically%2520consistent%2520recognition.%2520Our%2520method%2520surpasses%2520existing%2520frameworks%252C%2520with%2520ablation%2520studies%2520confirming%2520the%2520effectiveness%2520of%2520its%2520key%2520components.%2520The%2520dataset%2520and%2520code%2520for%2520LENS-Net%2520is%2520publicly%2520available%2520for%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Navigating%20in%20the%20Dark%3A%20A%20Multimodal%20Framework%20and%20Dataset%20for%20Nighttime%20Traffic%20Sign%20Recognition&entry.906535625=Aditya%20Mishra%20and%20Akshay%20Agarwal%20and%20Haroon%20Lone&entry.1292438233=Traffic%20signboards%20are%20vital%20for%20road%20safety%20and%20intelligent%20transportation%20systems%2C%20enabling%20navigation%20and%20autonomous%20driving.%20Yet%2C%20recognizing%20traffic%20signs%20at%20night%20remains%20challenging%20due%20to%20visual%20noise%20and%20scarcity%20of%20public%20nighttime%20datasets.%20Despite%20advances%20in%20vision%20architectures%2C%20existing%20methods%20struggle%20with%20robustness%20under%20low%20illumination%20and%20fail%20to%20leverage%20complementary%20mutlimodal%20cues%20effectively.%20To%20overcome%20these%20limitations%2C%20firstly%2C%20we%20introduce%20INTSD%2C%20a%20large-scale%20dataset%20comprising%20street-level%20night-time%20images%20of%20traffic%20signboards%20collected%20across%20diverse%20regions%20of%20India.%20The%20dataset%20spans%2041%20traffic%20signboard%20classes%20captured%20under%20varying%20lighting%20and%20weather%20conditions%2C%20providing%20a%20comprehensive%20benchmark%20for%20both%20detection%20and%20classification%20tasks.%20To%20benchmark%20INTSD%20for%20night-time%20sign%20recognition%2C%20we%20conduct%20extensive%20evaluations%20using%20state-of-the-art%20detection%20and%20classification%20models.%20Secondly%2C%20we%20propose%20LENS-Net%2C%20which%20integrates%20an%20adaptive%20image%20enhancement%20detector%20for%20joint%20illumination%20correction%20and%20sign%20localization%2C%20followed%20by%20a%20structured%20multimodal%20CLIP-GCNN%20classifier%20that%20leverages%20cross-modal%20attention%20and%20graph-based%20reasoning%20for%20robust%20and%20semantically%20consistent%20recognition.%20Our%20method%20surpasses%20existing%20frameworks%2C%20with%20ablation%20studies%20confirming%20the%20effectiveness%20of%20its%20key%20components.%20The%20dataset%20and%20code%20for%20LENS-Net%20is%20publicly%20available%20for%20research.&entry.1838667208=http%3A//arxiv.org/abs/2511.17183v1&entry.124074799=Read"},
{"title": "OmniLens++: Blind Lens Aberration Correction via Large LensLib Pre-Training and Latent PSF Representation", "author": "Qi Jiang and Xiaolong Qian and Yao Gao and Lei Sun and Kailun Yang and Zhonghua Yi and Wenyong Li and Ming-Hsuan Yang and Luc Van Gool and Kaiwei Wang", "abstract": "Emerging deep-learning-based lens library pre-training (LensLib-PT) pipeline offers a new avenue for blind lens aberration correction by training a universal neural network, demonstrating strong capability in handling diverse unknown optical degradations. This work proposes the OmniLens++ framework, which resolves two challenges that hinder the generalization ability of existing pipelines: the difficulty of scaling data and the absence of prior guidance characterizing optical degradation. To improve data scalability, we expand the design specifications to increase the degradation diversity of the lens source, and we sample a more uniform distribution by quantifying the spatial-variation patterns and severity of optical degradation. In terms of model design, to leverage the Point Spread Functions (PSFs), which intuitively describe optical degradation, as guidance in a blind paradigm, we propose the Latent PSF Representation (LPR). The VQVAE framework is introduced to learn latent features of LensLib's PSFs, which is assisted by modeling the optical degradation process to constrain the learning of degradation priors. Experiments on diverse aberrations of real-world lenses and synthetic LensLib show that OmniLens++ exhibits state-of-the-art generalization capacity in blind aberration correction. Beyond performance, the AODLibpro is verified as a scalable foundation for more effective training across diverse aberrations, and LPR can further tap the potential of large-scale LensLib. The source code and datasets will be made publicly available at https://github.com/zju-jiangqi/OmniLens2.", "link": "http://arxiv.org/abs/2511.17126v1", "date": "2025-11-21", "relevancy": 2.5912, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5262}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5143}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniLens%2B%2B%3A%20Blind%20Lens%20Aberration%20Correction%20via%20Large%20LensLib%20Pre-Training%20and%20Latent%20PSF%20Representation&body=Title%3A%20OmniLens%2B%2B%3A%20Blind%20Lens%20Aberration%20Correction%20via%20Large%20LensLib%20Pre-Training%20and%20Latent%20PSF%20Representation%0AAuthor%3A%20Qi%20Jiang%20and%20Xiaolong%20Qian%20and%20Yao%20Gao%20and%20Lei%20Sun%20and%20Kailun%20Yang%20and%20Zhonghua%20Yi%20and%20Wenyong%20Li%20and%20Ming-Hsuan%20Yang%20and%20Luc%20Van%20Gool%20and%20Kaiwei%20Wang%0AAbstract%3A%20Emerging%20deep-learning-based%20lens%20library%20pre-training%20%28LensLib-PT%29%20pipeline%20offers%20a%20new%20avenue%20for%20blind%20lens%20aberration%20correction%20by%20training%20a%20universal%20neural%20network%2C%20demonstrating%20strong%20capability%20in%20handling%20diverse%20unknown%20optical%20degradations.%20This%20work%20proposes%20the%20OmniLens%2B%2B%20framework%2C%20which%20resolves%20two%20challenges%20that%20hinder%20the%20generalization%20ability%20of%20existing%20pipelines%3A%20the%20difficulty%20of%20scaling%20data%20and%20the%20absence%20of%20prior%20guidance%20characterizing%20optical%20degradation.%20To%20improve%20data%20scalability%2C%20we%20expand%20the%20design%20specifications%20to%20increase%20the%20degradation%20diversity%20of%20the%20lens%20source%2C%20and%20we%20sample%20a%20more%20uniform%20distribution%20by%20quantifying%20the%20spatial-variation%20patterns%20and%20severity%20of%20optical%20degradation.%20In%20terms%20of%20model%20design%2C%20to%20leverage%20the%20Point%20Spread%20Functions%20%28PSFs%29%2C%20which%20intuitively%20describe%20optical%20degradation%2C%20as%20guidance%20in%20a%20blind%20paradigm%2C%20we%20propose%20the%20Latent%20PSF%20Representation%20%28LPR%29.%20The%20VQVAE%20framework%20is%20introduced%20to%20learn%20latent%20features%20of%20LensLib%27s%20PSFs%2C%20which%20is%20assisted%20by%20modeling%20the%20optical%20degradation%20process%20to%20constrain%20the%20learning%20of%20degradation%20priors.%20Experiments%20on%20diverse%20aberrations%20of%20real-world%20lenses%20and%20synthetic%20LensLib%20show%20that%20OmniLens%2B%2B%20exhibits%20state-of-the-art%20generalization%20capacity%20in%20blind%20aberration%20correction.%20Beyond%20performance%2C%20the%20AODLibpro%20is%20verified%20as%20a%20scalable%20foundation%20for%20more%20effective%20training%20across%20diverse%20aberrations%2C%20and%20LPR%20can%20further%20tap%20the%20potential%20of%20large-scale%20LensLib.%20The%20source%20code%20and%20datasets%20will%20be%20made%20publicly%20available%20at%20https%3A//github.com/zju-jiangqi/OmniLens2.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17126v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniLens%252B%252B%253A%2520Blind%2520Lens%2520Aberration%2520Correction%2520via%2520Large%2520LensLib%2520Pre-Training%2520and%2520Latent%2520PSF%2520Representation%26entry.906535625%3DQi%2520Jiang%2520and%2520Xiaolong%2520Qian%2520and%2520Yao%2520Gao%2520and%2520Lei%2520Sun%2520and%2520Kailun%2520Yang%2520and%2520Zhonghua%2520Yi%2520and%2520Wenyong%2520Li%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Luc%2520Van%2520Gool%2520and%2520Kaiwei%2520Wang%26entry.1292438233%3DEmerging%2520deep-learning-based%2520lens%2520library%2520pre-training%2520%2528LensLib-PT%2529%2520pipeline%2520offers%2520a%2520new%2520avenue%2520for%2520blind%2520lens%2520aberration%2520correction%2520by%2520training%2520a%2520universal%2520neural%2520network%252C%2520demonstrating%2520strong%2520capability%2520in%2520handling%2520diverse%2520unknown%2520optical%2520degradations.%2520This%2520work%2520proposes%2520the%2520OmniLens%252B%252B%2520framework%252C%2520which%2520resolves%2520two%2520challenges%2520that%2520hinder%2520the%2520generalization%2520ability%2520of%2520existing%2520pipelines%253A%2520the%2520difficulty%2520of%2520scaling%2520data%2520and%2520the%2520absence%2520of%2520prior%2520guidance%2520characterizing%2520optical%2520degradation.%2520To%2520improve%2520data%2520scalability%252C%2520we%2520expand%2520the%2520design%2520specifications%2520to%2520increase%2520the%2520degradation%2520diversity%2520of%2520the%2520lens%2520source%252C%2520and%2520we%2520sample%2520a%2520more%2520uniform%2520distribution%2520by%2520quantifying%2520the%2520spatial-variation%2520patterns%2520and%2520severity%2520of%2520optical%2520degradation.%2520In%2520terms%2520of%2520model%2520design%252C%2520to%2520leverage%2520the%2520Point%2520Spread%2520Functions%2520%2528PSFs%2529%252C%2520which%2520intuitively%2520describe%2520optical%2520degradation%252C%2520as%2520guidance%2520in%2520a%2520blind%2520paradigm%252C%2520we%2520propose%2520the%2520Latent%2520PSF%2520Representation%2520%2528LPR%2529.%2520The%2520VQVAE%2520framework%2520is%2520introduced%2520to%2520learn%2520latent%2520features%2520of%2520LensLib%2527s%2520PSFs%252C%2520which%2520is%2520assisted%2520by%2520modeling%2520the%2520optical%2520degradation%2520process%2520to%2520constrain%2520the%2520learning%2520of%2520degradation%2520priors.%2520Experiments%2520on%2520diverse%2520aberrations%2520of%2520real-world%2520lenses%2520and%2520synthetic%2520LensLib%2520show%2520that%2520OmniLens%252B%252B%2520exhibits%2520state-of-the-art%2520generalization%2520capacity%2520in%2520blind%2520aberration%2520correction.%2520Beyond%2520performance%252C%2520the%2520AODLibpro%2520is%2520verified%2520as%2520a%2520scalable%2520foundation%2520for%2520more%2520effective%2520training%2520across%2520diverse%2520aberrations%252C%2520and%2520LPR%2520can%2520further%2520tap%2520the%2520potential%2520of%2520large-scale%2520LensLib.%2520The%2520source%2520code%2520and%2520datasets%2520will%2520be%2520made%2520publicly%2520available%2520at%2520https%253A//github.com/zju-jiangqi/OmniLens2.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17126v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniLens%2B%2B%3A%20Blind%20Lens%20Aberration%20Correction%20via%20Large%20LensLib%20Pre-Training%20and%20Latent%20PSF%20Representation&entry.906535625=Qi%20Jiang%20and%20Xiaolong%20Qian%20and%20Yao%20Gao%20and%20Lei%20Sun%20and%20Kailun%20Yang%20and%20Zhonghua%20Yi%20and%20Wenyong%20Li%20and%20Ming-Hsuan%20Yang%20and%20Luc%20Van%20Gool%20and%20Kaiwei%20Wang&entry.1292438233=Emerging%20deep-learning-based%20lens%20library%20pre-training%20%28LensLib-PT%29%20pipeline%20offers%20a%20new%20avenue%20for%20blind%20lens%20aberration%20correction%20by%20training%20a%20universal%20neural%20network%2C%20demonstrating%20strong%20capability%20in%20handling%20diverse%20unknown%20optical%20degradations.%20This%20work%20proposes%20the%20OmniLens%2B%2B%20framework%2C%20which%20resolves%20two%20challenges%20that%20hinder%20the%20generalization%20ability%20of%20existing%20pipelines%3A%20the%20difficulty%20of%20scaling%20data%20and%20the%20absence%20of%20prior%20guidance%20characterizing%20optical%20degradation.%20To%20improve%20data%20scalability%2C%20we%20expand%20the%20design%20specifications%20to%20increase%20the%20degradation%20diversity%20of%20the%20lens%20source%2C%20and%20we%20sample%20a%20more%20uniform%20distribution%20by%20quantifying%20the%20spatial-variation%20patterns%20and%20severity%20of%20optical%20degradation.%20In%20terms%20of%20model%20design%2C%20to%20leverage%20the%20Point%20Spread%20Functions%20%28PSFs%29%2C%20which%20intuitively%20describe%20optical%20degradation%2C%20as%20guidance%20in%20a%20blind%20paradigm%2C%20we%20propose%20the%20Latent%20PSF%20Representation%20%28LPR%29.%20The%20VQVAE%20framework%20is%20introduced%20to%20learn%20latent%20features%20of%20LensLib%27s%20PSFs%2C%20which%20is%20assisted%20by%20modeling%20the%20optical%20degradation%20process%20to%20constrain%20the%20learning%20of%20degradation%20priors.%20Experiments%20on%20diverse%20aberrations%20of%20real-world%20lenses%20and%20synthetic%20LensLib%20show%20that%20OmniLens%2B%2B%20exhibits%20state-of-the-art%20generalization%20capacity%20in%20blind%20aberration%20correction.%20Beyond%20performance%2C%20the%20AODLibpro%20is%20verified%20as%20a%20scalable%20foundation%20for%20more%20effective%20training%20across%20diverse%20aberrations%2C%20and%20LPR%20can%20further%20tap%20the%20potential%20of%20large-scale%20LensLib.%20The%20source%20code%20and%20datasets%20will%20be%20made%20publicly%20available%20at%20https%3A//github.com/zju-jiangqi/OmniLens2.&entry.1838667208=http%3A//arxiv.org/abs/2511.17126v1&entry.124074799=Read"},
{"title": "MolSight: Optical Chemical Structure Recognition with SMILES Pretraining, Multi-Granularity Learning and Reinforcement Learning", "author": "Wenrui Zhang and Xinggang Wang and Bin Feng and Wenyu Liu", "abstract": "Optical Chemical Structure Recognition (OCSR) plays a pivotal role in modern chemical informatics, enabling the automated conversion of chemical structure images from scientific literature, patents, and educational materials into machine-readable molecular representations. This capability is essential for large-scale chemical data mining, drug discovery pipelines, and Large Language Model (LLM) applications in related domains. However, existing OCSR systems face significant challenges in accurately recognizing stereochemical information due to the subtle visual cues that distinguish stereoisomers, such as wedge and dash bonds, ring conformations, and spatial arrangements. To address these challenges, we propose MolSight, a comprehensive learning framework for OCSR that employs a three-stage training paradigm. In the first stage, we conduct pre-training on large-scale but noisy datasets to endow the model with fundamental perception capabilities for chemical structure images. In the second stage, we perform multi-granularity fine-tuning using datasets with richer supervisory signals, systematically exploring how auxiliary tasks-specifically chemical bond classification and atom localization-contribute to molecular formula recognition. Finally, we employ reinforcement learning for post-training optimization and introduce a novel stereochemical structure dataset. Remarkably, we find that even with MolSight's relatively compact parameter size, the Group Relative Policy Optimization (GRPO) algorithm can further enhance the model's performance on stereomolecular. Through extensive experiments across diverse datasets, our results demonstrate that MolSight achieves state-of-the-art performance in (stereo)chemical optical structure recognition.", "link": "http://arxiv.org/abs/2511.17300v1", "date": "2025-11-21", "relevancy": 2.5822, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5211}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MolSight%3A%20Optical%20Chemical%20Structure%20Recognition%20with%20SMILES%20Pretraining%2C%20Multi-Granularity%20Learning%20and%20Reinforcement%20Learning&body=Title%3A%20MolSight%3A%20Optical%20Chemical%20Structure%20Recognition%20with%20SMILES%20Pretraining%2C%20Multi-Granularity%20Learning%20and%20Reinforcement%20Learning%0AAuthor%3A%20Wenrui%20Zhang%20and%20Xinggang%20Wang%20and%20Bin%20Feng%20and%20Wenyu%20Liu%0AAbstract%3A%20Optical%20Chemical%20Structure%20Recognition%20%28OCSR%29%20plays%20a%20pivotal%20role%20in%20modern%20chemical%20informatics%2C%20enabling%20the%20automated%20conversion%20of%20chemical%20structure%20images%20from%20scientific%20literature%2C%20patents%2C%20and%20educational%20materials%20into%20machine-readable%20molecular%20representations.%20This%20capability%20is%20essential%20for%20large-scale%20chemical%20data%20mining%2C%20drug%20discovery%20pipelines%2C%20and%20Large%20Language%20Model%20%28LLM%29%20applications%20in%20related%20domains.%20However%2C%20existing%20OCSR%20systems%20face%20significant%20challenges%20in%20accurately%20recognizing%20stereochemical%20information%20due%20to%20the%20subtle%20visual%20cues%20that%20distinguish%20stereoisomers%2C%20such%20as%20wedge%20and%20dash%20bonds%2C%20ring%20conformations%2C%20and%20spatial%20arrangements.%20To%20address%20these%20challenges%2C%20we%20propose%20MolSight%2C%20a%20comprehensive%20learning%20framework%20for%20OCSR%20that%20employs%20a%20three-stage%20training%20paradigm.%20In%20the%20first%20stage%2C%20we%20conduct%20pre-training%20on%20large-scale%20but%20noisy%20datasets%20to%20endow%20the%20model%20with%20fundamental%20perception%20capabilities%20for%20chemical%20structure%20images.%20In%20the%20second%20stage%2C%20we%20perform%20multi-granularity%20fine-tuning%20using%20datasets%20with%20richer%20supervisory%20signals%2C%20systematically%20exploring%20how%20auxiliary%20tasks-specifically%20chemical%20bond%20classification%20and%20atom%20localization-contribute%20to%20molecular%20formula%20recognition.%20Finally%2C%20we%20employ%20reinforcement%20learning%20for%20post-training%20optimization%20and%20introduce%20a%20novel%20stereochemical%20structure%20dataset.%20Remarkably%2C%20we%20find%20that%20even%20with%20MolSight%27s%20relatively%20compact%20parameter%20size%2C%20the%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20algorithm%20can%20further%20enhance%20the%20model%27s%20performance%20on%20stereomolecular.%20Through%20extensive%20experiments%20across%20diverse%20datasets%2C%20our%20results%20demonstrate%20that%20MolSight%20achieves%20state-of-the-art%20performance%20in%20%28stereo%29chemical%20optical%20structure%20recognition.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMolSight%253A%2520Optical%2520Chemical%2520Structure%2520Recognition%2520with%2520SMILES%2520Pretraining%252C%2520Multi-Granularity%2520Learning%2520and%2520Reinforcement%2520Learning%26entry.906535625%3DWenrui%2520Zhang%2520and%2520Xinggang%2520Wang%2520and%2520Bin%2520Feng%2520and%2520Wenyu%2520Liu%26entry.1292438233%3DOptical%2520Chemical%2520Structure%2520Recognition%2520%2528OCSR%2529%2520plays%2520a%2520pivotal%2520role%2520in%2520modern%2520chemical%2520informatics%252C%2520enabling%2520the%2520automated%2520conversion%2520of%2520chemical%2520structure%2520images%2520from%2520scientific%2520literature%252C%2520patents%252C%2520and%2520educational%2520materials%2520into%2520machine-readable%2520molecular%2520representations.%2520This%2520capability%2520is%2520essential%2520for%2520large-scale%2520chemical%2520data%2520mining%252C%2520drug%2520discovery%2520pipelines%252C%2520and%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520applications%2520in%2520related%2520domains.%2520However%252C%2520existing%2520OCSR%2520systems%2520face%2520significant%2520challenges%2520in%2520accurately%2520recognizing%2520stereochemical%2520information%2520due%2520to%2520the%2520subtle%2520visual%2520cues%2520that%2520distinguish%2520stereoisomers%252C%2520such%2520as%2520wedge%2520and%2520dash%2520bonds%252C%2520ring%2520conformations%252C%2520and%2520spatial%2520arrangements.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520MolSight%252C%2520a%2520comprehensive%2520learning%2520framework%2520for%2520OCSR%2520that%2520employs%2520a%2520three-stage%2520training%2520paradigm.%2520In%2520the%2520first%2520stage%252C%2520we%2520conduct%2520pre-training%2520on%2520large-scale%2520but%2520noisy%2520datasets%2520to%2520endow%2520the%2520model%2520with%2520fundamental%2520perception%2520capabilities%2520for%2520chemical%2520structure%2520images.%2520In%2520the%2520second%2520stage%252C%2520we%2520perform%2520multi-granularity%2520fine-tuning%2520using%2520datasets%2520with%2520richer%2520supervisory%2520signals%252C%2520systematically%2520exploring%2520how%2520auxiliary%2520tasks-specifically%2520chemical%2520bond%2520classification%2520and%2520atom%2520localization-contribute%2520to%2520molecular%2520formula%2520recognition.%2520Finally%252C%2520we%2520employ%2520reinforcement%2520learning%2520for%2520post-training%2520optimization%2520and%2520introduce%2520a%2520novel%2520stereochemical%2520structure%2520dataset.%2520Remarkably%252C%2520we%2520find%2520that%2520even%2520with%2520MolSight%2527s%2520relatively%2520compact%2520parameter%2520size%252C%2520the%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520algorithm%2520can%2520further%2520enhance%2520the%2520model%2527s%2520performance%2520on%2520stereomolecular.%2520Through%2520extensive%2520experiments%2520across%2520diverse%2520datasets%252C%2520our%2520results%2520demonstrate%2520that%2520MolSight%2520achieves%2520state-of-the-art%2520performance%2520in%2520%2528stereo%2529chemical%2520optical%2520structure%2520recognition.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MolSight%3A%20Optical%20Chemical%20Structure%20Recognition%20with%20SMILES%20Pretraining%2C%20Multi-Granularity%20Learning%20and%20Reinforcement%20Learning&entry.906535625=Wenrui%20Zhang%20and%20Xinggang%20Wang%20and%20Bin%20Feng%20and%20Wenyu%20Liu&entry.1292438233=Optical%20Chemical%20Structure%20Recognition%20%28OCSR%29%20plays%20a%20pivotal%20role%20in%20modern%20chemical%20informatics%2C%20enabling%20the%20automated%20conversion%20of%20chemical%20structure%20images%20from%20scientific%20literature%2C%20patents%2C%20and%20educational%20materials%20into%20machine-readable%20molecular%20representations.%20This%20capability%20is%20essential%20for%20large-scale%20chemical%20data%20mining%2C%20drug%20discovery%20pipelines%2C%20and%20Large%20Language%20Model%20%28LLM%29%20applications%20in%20related%20domains.%20However%2C%20existing%20OCSR%20systems%20face%20significant%20challenges%20in%20accurately%20recognizing%20stereochemical%20information%20due%20to%20the%20subtle%20visual%20cues%20that%20distinguish%20stereoisomers%2C%20such%20as%20wedge%20and%20dash%20bonds%2C%20ring%20conformations%2C%20and%20spatial%20arrangements.%20To%20address%20these%20challenges%2C%20we%20propose%20MolSight%2C%20a%20comprehensive%20learning%20framework%20for%20OCSR%20that%20employs%20a%20three-stage%20training%20paradigm.%20In%20the%20first%20stage%2C%20we%20conduct%20pre-training%20on%20large-scale%20but%20noisy%20datasets%20to%20endow%20the%20model%20with%20fundamental%20perception%20capabilities%20for%20chemical%20structure%20images.%20In%20the%20second%20stage%2C%20we%20perform%20multi-granularity%20fine-tuning%20using%20datasets%20with%20richer%20supervisory%20signals%2C%20systematically%20exploring%20how%20auxiliary%20tasks-specifically%20chemical%20bond%20classification%20and%20atom%20localization-contribute%20to%20molecular%20formula%20recognition.%20Finally%2C%20we%20employ%20reinforcement%20learning%20for%20post-training%20optimization%20and%20introduce%20a%20novel%20stereochemical%20structure%20dataset.%20Remarkably%2C%20we%20find%20that%20even%20with%20MolSight%27s%20relatively%20compact%20parameter%20size%2C%20the%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20algorithm%20can%20further%20enhance%20the%20model%27s%20performance%20on%20stereomolecular.%20Through%20extensive%20experiments%20across%20diverse%20datasets%2C%20our%20results%20demonstrate%20that%20MolSight%20achieves%20state-of-the-art%20performance%20in%20%28stereo%29chemical%20optical%20structure%20recognition.&entry.1838667208=http%3A//arxiv.org/abs/2511.17300v1&entry.124074799=Read"},
{"title": "Topology Aware Neural Interpolation of Scalar Fields", "author": "Mohamed Kissi and Keanu Sisouk and Joshua A. Levine and Julien Tierny", "abstract": "This paper presents a neural scheme for the topology-aware interpolation of time-varying scalar fields. Given a time-varying sequence of persistence diagrams, along with a sparse temporal sampling of the corresponding scalar fields, denoted as keyframes, our interpolation approach aims at \"inverting\" the non-keyframe diagrams to produce plausible estimations of the corresponding, missing data. For this, we rely on a neural architecture which learns the relation from a time value to the corresponding scalar field, based on the keyframe examples, and reliably extends this relation to the non-keyframe time steps. We show how augmenting this architecture with specific topological losses exploiting the input diagrams both improves the geometrical and topological reconstruction of the non-keyframe time steps. At query time, given an input time value for which an interpolation is desired, our approach instantaneously produces an output, via a single propagation of the time input through the network. Experiments interpolating 2D and 3D time-varying datasets show our approach superiority, both in terms of data and topological fitting, with regard to reference interpolation schemes. Our implementation is available at this GitHub link : https://github.com/MohamedKISSI/Topology-Aware-Neural-Interpolation-of-Scalar-Fields.git.", "link": "http://arxiv.org/abs/2508.17995v2", "date": "2025-11-21", "relevancy": 2.5608, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5228}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.52}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topology%20Aware%20Neural%20Interpolation%20of%20Scalar%20Fields&body=Title%3A%20Topology%20Aware%20Neural%20Interpolation%20of%20Scalar%20Fields%0AAuthor%3A%20Mohamed%20Kissi%20and%20Keanu%20Sisouk%20and%20Joshua%20A.%20Levine%20and%20Julien%20Tierny%0AAbstract%3A%20This%20paper%20presents%20a%20neural%20scheme%20for%20the%20topology-aware%20interpolation%20of%20time-varying%20scalar%20fields.%20Given%20a%20time-varying%20sequence%20of%20persistence%20diagrams%2C%20along%20with%20a%20sparse%20temporal%20sampling%20of%20the%20corresponding%20scalar%20fields%2C%20denoted%20as%20keyframes%2C%20our%20interpolation%20approach%20aims%20at%20%22inverting%22%20the%20non-keyframe%20diagrams%20to%20produce%20plausible%20estimations%20of%20the%20corresponding%2C%20missing%20data.%20For%20this%2C%20we%20rely%20on%20a%20neural%20architecture%20which%20learns%20the%20relation%20from%20a%20time%20value%20to%20the%20corresponding%20scalar%20field%2C%20based%20on%20the%20keyframe%20examples%2C%20and%20reliably%20extends%20this%20relation%20to%20the%20non-keyframe%20time%20steps.%20We%20show%20how%20augmenting%20this%20architecture%20with%20specific%20topological%20losses%20exploiting%20the%20input%20diagrams%20both%20improves%20the%20geometrical%20and%20topological%20reconstruction%20of%20the%20non-keyframe%20time%20steps.%20At%20query%20time%2C%20given%20an%20input%20time%20value%20for%20which%20an%20interpolation%20is%20desired%2C%20our%20approach%20instantaneously%20produces%20an%20output%2C%20via%20a%20single%20propagation%20of%20the%20time%20input%20through%20the%20network.%20Experiments%20interpolating%202D%20and%203D%20time-varying%20datasets%20show%20our%20approach%20superiority%2C%20both%20in%20terms%20of%20data%20and%20topological%20fitting%2C%20with%20regard%20to%20reference%20interpolation%20schemes.%20Our%20implementation%20is%20available%20at%20this%20GitHub%20link%20%3A%20https%3A//github.com/MohamedKISSI/Topology-Aware-Neural-Interpolation-of-Scalar-Fields.git.%0ALink%3A%20http%3A//arxiv.org/abs/2508.17995v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopology%2520Aware%2520Neural%2520Interpolation%2520of%2520Scalar%2520Fields%26entry.906535625%3DMohamed%2520Kissi%2520and%2520Keanu%2520Sisouk%2520and%2520Joshua%2520A.%2520Levine%2520and%2520Julien%2520Tierny%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520neural%2520scheme%2520for%2520the%2520topology-aware%2520interpolation%2520of%2520time-varying%2520scalar%2520fields.%2520Given%2520a%2520time-varying%2520sequence%2520of%2520persistence%2520diagrams%252C%2520along%2520with%2520a%2520sparse%2520temporal%2520sampling%2520of%2520the%2520corresponding%2520scalar%2520fields%252C%2520denoted%2520as%2520keyframes%252C%2520our%2520interpolation%2520approach%2520aims%2520at%2520%2522inverting%2522%2520the%2520non-keyframe%2520diagrams%2520to%2520produce%2520plausible%2520estimations%2520of%2520the%2520corresponding%252C%2520missing%2520data.%2520For%2520this%252C%2520we%2520rely%2520on%2520a%2520neural%2520architecture%2520which%2520learns%2520the%2520relation%2520from%2520a%2520time%2520value%2520to%2520the%2520corresponding%2520scalar%2520field%252C%2520based%2520on%2520the%2520keyframe%2520examples%252C%2520and%2520reliably%2520extends%2520this%2520relation%2520to%2520the%2520non-keyframe%2520time%2520steps.%2520We%2520show%2520how%2520augmenting%2520this%2520architecture%2520with%2520specific%2520topological%2520losses%2520exploiting%2520the%2520input%2520diagrams%2520both%2520improves%2520the%2520geometrical%2520and%2520topological%2520reconstruction%2520of%2520the%2520non-keyframe%2520time%2520steps.%2520At%2520query%2520time%252C%2520given%2520an%2520input%2520time%2520value%2520for%2520which%2520an%2520interpolation%2520is%2520desired%252C%2520our%2520approach%2520instantaneously%2520produces%2520an%2520output%252C%2520via%2520a%2520single%2520propagation%2520of%2520the%2520time%2520input%2520through%2520the%2520network.%2520Experiments%2520interpolating%25202D%2520and%25203D%2520time-varying%2520datasets%2520show%2520our%2520approach%2520superiority%252C%2520both%2520in%2520terms%2520of%2520data%2520and%2520topological%2520fitting%252C%2520with%2520regard%2520to%2520reference%2520interpolation%2520schemes.%2520Our%2520implementation%2520is%2520available%2520at%2520this%2520GitHub%2520link%2520%253A%2520https%253A//github.com/MohamedKISSI/Topology-Aware-Neural-Interpolation-of-Scalar-Fields.git.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17995v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topology%20Aware%20Neural%20Interpolation%20of%20Scalar%20Fields&entry.906535625=Mohamed%20Kissi%20and%20Keanu%20Sisouk%20and%20Joshua%20A.%20Levine%20and%20Julien%20Tierny&entry.1292438233=This%20paper%20presents%20a%20neural%20scheme%20for%20the%20topology-aware%20interpolation%20of%20time-varying%20scalar%20fields.%20Given%20a%20time-varying%20sequence%20of%20persistence%20diagrams%2C%20along%20with%20a%20sparse%20temporal%20sampling%20of%20the%20corresponding%20scalar%20fields%2C%20denoted%20as%20keyframes%2C%20our%20interpolation%20approach%20aims%20at%20%22inverting%22%20the%20non-keyframe%20diagrams%20to%20produce%20plausible%20estimations%20of%20the%20corresponding%2C%20missing%20data.%20For%20this%2C%20we%20rely%20on%20a%20neural%20architecture%20which%20learns%20the%20relation%20from%20a%20time%20value%20to%20the%20corresponding%20scalar%20field%2C%20based%20on%20the%20keyframe%20examples%2C%20and%20reliably%20extends%20this%20relation%20to%20the%20non-keyframe%20time%20steps.%20We%20show%20how%20augmenting%20this%20architecture%20with%20specific%20topological%20losses%20exploiting%20the%20input%20diagrams%20both%20improves%20the%20geometrical%20and%20topological%20reconstruction%20of%20the%20non-keyframe%20time%20steps.%20At%20query%20time%2C%20given%20an%20input%20time%20value%20for%20which%20an%20interpolation%20is%20desired%2C%20our%20approach%20instantaneously%20produces%20an%20output%2C%20via%20a%20single%20propagation%20of%20the%20time%20input%20through%20the%20network.%20Experiments%20interpolating%202D%20and%203D%20time-varying%20datasets%20show%20our%20approach%20superiority%2C%20both%20in%20terms%20of%20data%20and%20topological%20fitting%2C%20with%20regard%20to%20reference%20interpolation%20schemes.%20Our%20implementation%20is%20available%20at%20this%20GitHub%20link%20%3A%20https%3A//github.com/MohamedKISSI/Topology-Aware-Neural-Interpolation-of-Scalar-Fields.git.&entry.1838667208=http%3A//arxiv.org/abs/2508.17995v2&entry.124074799=Read"},
{"title": "Self-Supervised Learning by Curvature Alignment", "author": "Benyamin Ghojogh and M. Hadi Sepanj and Paul Fieguth", "abstract": "Self-supervised learning (SSL) has recently advanced through non-contrastive methods that couple an invariance term with variance, covariance, or redundancy-reduction penalties. While such objectives shape first- and second-order statistics of the representation, they largely ignore the local geometry of the underlying data manifold. In this paper, we introduce CurvSSL, a curvature-regularized self-supervised learning framework, and its RKHS extension, kernel CurvSSL. Our approach retains a standard two-view encoder-projector architecture with a Barlow Twins-style redundancy-reduction loss on projected features, but augments it with a curvature-based regularizer. Each embedding is treated as a vertex whose $k$ nearest neighbors define a discrete curvature score via cosine interactions on the unit hypersphere; in the kernel variant, curvature is computed from a normalized local Gram matrix in an RKHS. These scores are aligned and decorrelated across augmentations by a Barlow-style loss on a curvature-derived matrix, encouraging both view invariance and consistency of local manifold bending. Experiments on MNIST and CIFAR-10 datasets with a ResNet-18 backbone show that curvature-regularized SSL yields competitive or improved linear evaluation performance compared to Barlow Twins and VICReg. Our results indicate that explicitly shaping local geometry is a simple and effective complement to purely statistical SSL regularizers.", "link": "http://arxiv.org/abs/2511.17426v1", "date": "2025-11-21", "relevancy": 2.5574, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5253}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5202}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20by%20Curvature%20Alignment&body=Title%3A%20Self-Supervised%20Learning%20by%20Curvature%20Alignment%0AAuthor%3A%20Benyamin%20Ghojogh%20and%20M.%20Hadi%20Sepanj%20and%20Paul%20Fieguth%0AAbstract%3A%20Self-supervised%20learning%20%28SSL%29%20has%20recently%20advanced%20through%20non-contrastive%20methods%20that%20couple%20an%20invariance%20term%20with%20variance%2C%20covariance%2C%20or%20redundancy-reduction%20penalties.%20While%20such%20objectives%20shape%20first-%20and%20second-order%20statistics%20of%20the%20representation%2C%20they%20largely%20ignore%20the%20local%20geometry%20of%20the%20underlying%20data%20manifold.%20In%20this%20paper%2C%20we%20introduce%20CurvSSL%2C%20a%20curvature-regularized%20self-supervised%20learning%20framework%2C%20and%20its%20RKHS%20extension%2C%20kernel%20CurvSSL.%20Our%20approach%20retains%20a%20standard%20two-view%20encoder-projector%20architecture%20with%20a%20Barlow%20Twins-style%20redundancy-reduction%20loss%20on%20projected%20features%2C%20but%20augments%20it%20with%20a%20curvature-based%20regularizer.%20Each%20embedding%20is%20treated%20as%20a%20vertex%20whose%20%24k%24%20nearest%20neighbors%20define%20a%20discrete%20curvature%20score%20via%20cosine%20interactions%20on%20the%20unit%20hypersphere%3B%20in%20the%20kernel%20variant%2C%20curvature%20is%20computed%20from%20a%20normalized%20local%20Gram%20matrix%20in%20an%20RKHS.%20These%20scores%20are%20aligned%20and%20decorrelated%20across%20augmentations%20by%20a%20Barlow-style%20loss%20on%20a%20curvature-derived%20matrix%2C%20encouraging%20both%20view%20invariance%20and%20consistency%20of%20local%20manifold%20bending.%20Experiments%20on%20MNIST%20and%20CIFAR-10%20datasets%20with%20a%20ResNet-18%20backbone%20show%20that%20curvature-regularized%20SSL%20yields%20competitive%20or%20improved%20linear%20evaluation%20performance%20compared%20to%20Barlow%20Twins%20and%20VICReg.%20Our%20results%20indicate%20that%20explicitly%20shaping%20local%20geometry%20is%20a%20simple%20and%20effective%20complement%20to%20purely%20statistical%20SSL%20regularizers.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Learning%2520by%2520Curvature%2520Alignment%26entry.906535625%3DBenyamin%2520Ghojogh%2520and%2520M.%2520Hadi%2520Sepanj%2520and%2520Paul%2520Fieguth%26entry.1292438233%3DSelf-supervised%2520learning%2520%2528SSL%2529%2520has%2520recently%2520advanced%2520through%2520non-contrastive%2520methods%2520that%2520couple%2520an%2520invariance%2520term%2520with%2520variance%252C%2520covariance%252C%2520or%2520redundancy-reduction%2520penalties.%2520While%2520such%2520objectives%2520shape%2520first-%2520and%2520second-order%2520statistics%2520of%2520the%2520representation%252C%2520they%2520largely%2520ignore%2520the%2520local%2520geometry%2520of%2520the%2520underlying%2520data%2520manifold.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520CurvSSL%252C%2520a%2520curvature-regularized%2520self-supervised%2520learning%2520framework%252C%2520and%2520its%2520RKHS%2520extension%252C%2520kernel%2520CurvSSL.%2520Our%2520approach%2520retains%2520a%2520standard%2520two-view%2520encoder-projector%2520architecture%2520with%2520a%2520Barlow%2520Twins-style%2520redundancy-reduction%2520loss%2520on%2520projected%2520features%252C%2520but%2520augments%2520it%2520with%2520a%2520curvature-based%2520regularizer.%2520Each%2520embedding%2520is%2520treated%2520as%2520a%2520vertex%2520whose%2520%2524k%2524%2520nearest%2520neighbors%2520define%2520a%2520discrete%2520curvature%2520score%2520via%2520cosine%2520interactions%2520on%2520the%2520unit%2520hypersphere%253B%2520in%2520the%2520kernel%2520variant%252C%2520curvature%2520is%2520computed%2520from%2520a%2520normalized%2520local%2520Gram%2520matrix%2520in%2520an%2520RKHS.%2520These%2520scores%2520are%2520aligned%2520and%2520decorrelated%2520across%2520augmentations%2520by%2520a%2520Barlow-style%2520loss%2520on%2520a%2520curvature-derived%2520matrix%252C%2520encouraging%2520both%2520view%2520invariance%2520and%2520consistency%2520of%2520local%2520manifold%2520bending.%2520Experiments%2520on%2520MNIST%2520and%2520CIFAR-10%2520datasets%2520with%2520a%2520ResNet-18%2520backbone%2520show%2520that%2520curvature-regularized%2520SSL%2520yields%2520competitive%2520or%2520improved%2520linear%2520evaluation%2520performance%2520compared%2520to%2520Barlow%2520Twins%2520and%2520VICReg.%2520Our%2520results%2520indicate%2520that%2520explicitly%2520shaping%2520local%2520geometry%2520is%2520a%2520simple%2520and%2520effective%2520complement%2520to%2520purely%2520statistical%2520SSL%2520regularizers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20by%20Curvature%20Alignment&entry.906535625=Benyamin%20Ghojogh%20and%20M.%20Hadi%20Sepanj%20and%20Paul%20Fieguth&entry.1292438233=Self-supervised%20learning%20%28SSL%29%20has%20recently%20advanced%20through%20non-contrastive%20methods%20that%20couple%20an%20invariance%20term%20with%20variance%2C%20covariance%2C%20or%20redundancy-reduction%20penalties.%20While%20such%20objectives%20shape%20first-%20and%20second-order%20statistics%20of%20the%20representation%2C%20they%20largely%20ignore%20the%20local%20geometry%20of%20the%20underlying%20data%20manifold.%20In%20this%20paper%2C%20we%20introduce%20CurvSSL%2C%20a%20curvature-regularized%20self-supervised%20learning%20framework%2C%20and%20its%20RKHS%20extension%2C%20kernel%20CurvSSL.%20Our%20approach%20retains%20a%20standard%20two-view%20encoder-projector%20architecture%20with%20a%20Barlow%20Twins-style%20redundancy-reduction%20loss%20on%20projected%20features%2C%20but%20augments%20it%20with%20a%20curvature-based%20regularizer.%20Each%20embedding%20is%20treated%20as%20a%20vertex%20whose%20%24k%24%20nearest%20neighbors%20define%20a%20discrete%20curvature%20score%20via%20cosine%20interactions%20on%20the%20unit%20hypersphere%3B%20in%20the%20kernel%20variant%2C%20curvature%20is%20computed%20from%20a%20normalized%20local%20Gram%20matrix%20in%20an%20RKHS.%20These%20scores%20are%20aligned%20and%20decorrelated%20across%20augmentations%20by%20a%20Barlow-style%20loss%20on%20a%20curvature-derived%20matrix%2C%20encouraging%20both%20view%20invariance%20and%20consistency%20of%20local%20manifold%20bending.%20Experiments%20on%20MNIST%20and%20CIFAR-10%20datasets%20with%20a%20ResNet-18%20backbone%20show%20that%20curvature-regularized%20SSL%20yields%20competitive%20or%20improved%20linear%20evaluation%20performance%20compared%20to%20Barlow%20Twins%20and%20VICReg.%20Our%20results%20indicate%20that%20explicitly%20shaping%20local%20geometry%20is%20a%20simple%20and%20effective%20complement%20to%20purely%20statistical%20SSL%20regularizers.&entry.1838667208=http%3A//arxiv.org/abs/2511.17426v1&entry.124074799=Read"},
{"title": "Quantum Masked Autoencoders for Vision Learning", "author": "Emma Andrews and Prabhat Mishra", "abstract": "Classical autoencoders are widely used to learn features of input data. To improve the feature learning, classical masked autoencoders extend classical autoencoders to learn the features of the original input sample in the presence of masked-out data. While quantum autoencoders exist, there is no design and implementation of quantum masked autoencoders that can leverage the benefits of quantum computing and quantum autoencoders. In this paper, we propose quantum masked autoencoders (QMAEs) that can effectively learn missing features of a data sample within quantum states instead of classical embeddings. We showcase that our QMAE architecture can learn the masked features of an image and can reconstruct the masked input image with improved visual fidelity in MNIST images. Experimental evaluation highlights that QMAE can significantly outperform (12.86% on average) in classification accuracy compared to state-of-the-art quantum autoencoders in the presence of masks.", "link": "http://arxiv.org/abs/2511.17372v1", "date": "2025-11-21", "relevancy": 2.5567, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5761}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4839}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Masked%20Autoencoders%20for%20Vision%20Learning&body=Title%3A%20Quantum%20Masked%20Autoencoders%20for%20Vision%20Learning%0AAuthor%3A%20Emma%20Andrews%20and%20Prabhat%20Mishra%0AAbstract%3A%20Classical%20autoencoders%20are%20widely%20used%20to%20learn%20features%20of%20input%20data.%20To%20improve%20the%20feature%20learning%2C%20classical%20masked%20autoencoders%20extend%20classical%20autoencoders%20to%20learn%20the%20features%20of%20the%20original%20input%20sample%20in%20the%20presence%20of%20masked-out%20data.%20While%20quantum%20autoencoders%20exist%2C%20there%20is%20no%20design%20and%20implementation%20of%20quantum%20masked%20autoencoders%20that%20can%20leverage%20the%20benefits%20of%20quantum%20computing%20and%20quantum%20autoencoders.%20In%20this%20paper%2C%20we%20propose%20quantum%20masked%20autoencoders%20%28QMAEs%29%20that%20can%20effectively%20learn%20missing%20features%20of%20a%20data%20sample%20within%20quantum%20states%20instead%20of%20classical%20embeddings.%20We%20showcase%20that%20our%20QMAE%20architecture%20can%20learn%20the%20masked%20features%20of%20an%20image%20and%20can%20reconstruct%20the%20masked%20input%20image%20with%20improved%20visual%20fidelity%20in%20MNIST%20images.%20Experimental%20evaluation%20highlights%20that%20QMAE%20can%20significantly%20outperform%20%2812.86%25%20on%20average%29%20in%20classification%20accuracy%20compared%20to%20state-of-the-art%20quantum%20autoencoders%20in%20the%20presence%20of%20masks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Masked%2520Autoencoders%2520for%2520Vision%2520Learning%26entry.906535625%3DEmma%2520Andrews%2520and%2520Prabhat%2520Mishra%26entry.1292438233%3DClassical%2520autoencoders%2520are%2520widely%2520used%2520to%2520learn%2520features%2520of%2520input%2520data.%2520To%2520improve%2520the%2520feature%2520learning%252C%2520classical%2520masked%2520autoencoders%2520extend%2520classical%2520autoencoders%2520to%2520learn%2520the%2520features%2520of%2520the%2520original%2520input%2520sample%2520in%2520the%2520presence%2520of%2520masked-out%2520data.%2520While%2520quantum%2520autoencoders%2520exist%252C%2520there%2520is%2520no%2520design%2520and%2520implementation%2520of%2520quantum%2520masked%2520autoencoders%2520that%2520can%2520leverage%2520the%2520benefits%2520of%2520quantum%2520computing%2520and%2520quantum%2520autoencoders.%2520In%2520this%2520paper%252C%2520we%2520propose%2520quantum%2520masked%2520autoencoders%2520%2528QMAEs%2529%2520that%2520can%2520effectively%2520learn%2520missing%2520features%2520of%2520a%2520data%2520sample%2520within%2520quantum%2520states%2520instead%2520of%2520classical%2520embeddings.%2520We%2520showcase%2520that%2520our%2520QMAE%2520architecture%2520can%2520learn%2520the%2520masked%2520features%2520of%2520an%2520image%2520and%2520can%2520reconstruct%2520the%2520masked%2520input%2520image%2520with%2520improved%2520visual%2520fidelity%2520in%2520MNIST%2520images.%2520Experimental%2520evaluation%2520highlights%2520that%2520QMAE%2520can%2520significantly%2520outperform%2520%252812.86%2525%2520on%2520average%2529%2520in%2520classification%2520accuracy%2520compared%2520to%2520state-of-the-art%2520quantum%2520autoencoders%2520in%2520the%2520presence%2520of%2520masks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Masked%20Autoencoders%20for%20Vision%20Learning&entry.906535625=Emma%20Andrews%20and%20Prabhat%20Mishra&entry.1292438233=Classical%20autoencoders%20are%20widely%20used%20to%20learn%20features%20of%20input%20data.%20To%20improve%20the%20feature%20learning%2C%20classical%20masked%20autoencoders%20extend%20classical%20autoencoders%20to%20learn%20the%20features%20of%20the%20original%20input%20sample%20in%20the%20presence%20of%20masked-out%20data.%20While%20quantum%20autoencoders%20exist%2C%20there%20is%20no%20design%20and%20implementation%20of%20quantum%20masked%20autoencoders%20that%20can%20leverage%20the%20benefits%20of%20quantum%20computing%20and%20quantum%20autoencoders.%20In%20this%20paper%2C%20we%20propose%20quantum%20masked%20autoencoders%20%28QMAEs%29%20that%20can%20effectively%20learn%20missing%20features%20of%20a%20data%20sample%20within%20quantum%20states%20instead%20of%20classical%20embeddings.%20We%20showcase%20that%20our%20QMAE%20architecture%20can%20learn%20the%20masked%20features%20of%20an%20image%20and%20can%20reconstruct%20the%20masked%20input%20image%20with%20improved%20visual%20fidelity%20in%20MNIST%20images.%20Experimental%20evaluation%20highlights%20that%20QMAE%20can%20significantly%20outperform%20%2812.86%25%20on%20average%29%20in%20classification%20accuracy%20compared%20to%20state-of-the-art%20quantum%20autoencoders%20in%20the%20presence%20of%20masks.&entry.1838667208=http%3A//arxiv.org/abs/2511.17372v1&entry.124074799=Read"},
{"title": "VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference", "author": "Ziyan Liu and Yeqiu Chen and Hongyi Cai and Tao Lin and Shuo Yang and Zheng Liu and Bo Zhao", "abstract": "Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.", "link": "http://arxiv.org/abs/2511.16449v2", "date": "2025-11-21", "relevancy": 2.5505, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLA-Pruner%3A%20Temporal-Aware%20Dual-Level%20Visual%20Token%20Pruning%20for%20Efficient%20Vision-Language-Action%20Inference&body=Title%3A%20VLA-Pruner%3A%20Temporal-Aware%20Dual-Level%20Visual%20Token%20Pruning%20for%20Efficient%20Vision-Language-Action%20Inference%0AAuthor%3A%20Ziyan%20Liu%20and%20Yeqiu%20Chen%20and%20Hongyi%20Cai%20and%20Tao%20Lin%20and%20Shuo%20Yang%20and%20Zheng%20Liu%20and%20Bo%20Zhao%0AAbstract%3A%20Vision-Language-Action%20%28VLA%29%20models%20have%20shown%20great%20promise%20for%20embodied%20AI%2C%20yet%20the%20heavy%20computational%20cost%20of%20processing%20continuous%20visual%20streams%20severely%20limits%20their%20real-time%20deployment.%20Token%20pruning%20%28keeping%20salient%20visual%20tokens%20and%20dropping%20redundant%20ones%29%20has%20emerged%20as%20an%20effective%20approach%20for%20accelerating%20Vision-Language%20Models%20%28VLMs%29%2C%20offering%20a%20solution%20for%20efficient%20VLA.%20However%2C%20these%20VLM-specific%20token%20pruning%20methods%20select%20tokens%20based%20solely%20on%20semantic%20salience%20metrics%20%28e.g.%2C%20prefill%20attention%29%2C%20while%20overlooking%20the%20VLA%27s%20intrinsic%20dual-system%20nature%20of%20high-level%20semantic%20understanding%20and%20low-level%20action%20execution.%20Consequently%2C%20these%20methods%20bias%20token%20retention%20toward%20semantic%20cues%2C%20discard%20critical%20information%20for%20action%20generation%2C%20and%20significantly%20degrade%20VLA%20performance.%20To%20bridge%20this%20gap%2C%20we%20propose%20VLA-Pruner%2C%20a%20versatile%20plug-and-play%20VLA-specific%20token%20prune%20method%20that%20aligns%20with%20the%20dual-system%20nature%20of%20VLA%20models%20and%20exploits%20the%20temporal%20continuity%20in%20robot%20manipulation.%20Specifically%2C%20VLA-Pruner%20adopts%20a%20dual-level%20importance%20criterion%20for%20visual%20token%20retention%3A%20vision-language%20prefill%20attention%20for%20semantic-level%20relevance%20and%20action%20decode%20attention%2C%20estimated%20via%20temporal%20smoothing%2C%20for%20action-level%20importance.%20Based%20on%20this%20criterion%2C%20VLA-Pruner%20proposes%20a%20novel%20dual-level%20token%20selection%20strategy%20that%20adaptively%20preserves%20a%20compact%2C%20informative%20set%20of%20visual%20tokens%20for%20both%20semantic%20understanding%20and%20action%20execution%20under%20given%20compute%20budget.%20Experiments%20show%20that%20VLA-Pruner%20achieves%20state-of-the-art%20performance%20across%20multiple%20VLA%20architectures%20and%20diverse%20robotic%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16449v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLA-Pruner%253A%2520Temporal-Aware%2520Dual-Level%2520Visual%2520Token%2520Pruning%2520for%2520Efficient%2520Vision-Language-Action%2520Inference%26entry.906535625%3DZiyan%2520Liu%2520and%2520Yeqiu%2520Chen%2520and%2520Hongyi%2520Cai%2520and%2520Tao%2520Lin%2520and%2520Shuo%2520Yang%2520and%2520Zheng%2520Liu%2520and%2520Bo%2520Zhao%26entry.1292438233%3DVision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520shown%2520great%2520promise%2520for%2520embodied%2520AI%252C%2520yet%2520the%2520heavy%2520computational%2520cost%2520of%2520processing%2520continuous%2520visual%2520streams%2520severely%2520limits%2520their%2520real-time%2520deployment.%2520Token%2520pruning%2520%2528keeping%2520salient%2520visual%2520tokens%2520and%2520dropping%2520redundant%2520ones%2529%2520has%2520emerged%2520as%2520an%2520effective%2520approach%2520for%2520accelerating%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520offering%2520a%2520solution%2520for%2520efficient%2520VLA.%2520However%252C%2520these%2520VLM-specific%2520token%2520pruning%2520methods%2520select%2520tokens%2520based%2520solely%2520on%2520semantic%2520salience%2520metrics%2520%2528e.g.%252C%2520prefill%2520attention%2529%252C%2520while%2520overlooking%2520the%2520VLA%2527s%2520intrinsic%2520dual-system%2520nature%2520of%2520high-level%2520semantic%2520understanding%2520and%2520low-level%2520action%2520execution.%2520Consequently%252C%2520these%2520methods%2520bias%2520token%2520retention%2520toward%2520semantic%2520cues%252C%2520discard%2520critical%2520information%2520for%2520action%2520generation%252C%2520and%2520significantly%2520degrade%2520VLA%2520performance.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520VLA-Pruner%252C%2520a%2520versatile%2520plug-and-play%2520VLA-specific%2520token%2520prune%2520method%2520that%2520aligns%2520with%2520the%2520dual-system%2520nature%2520of%2520VLA%2520models%2520and%2520exploits%2520the%2520temporal%2520continuity%2520in%2520robot%2520manipulation.%2520Specifically%252C%2520VLA-Pruner%2520adopts%2520a%2520dual-level%2520importance%2520criterion%2520for%2520visual%2520token%2520retention%253A%2520vision-language%2520prefill%2520attention%2520for%2520semantic-level%2520relevance%2520and%2520action%2520decode%2520attention%252C%2520estimated%2520via%2520temporal%2520smoothing%252C%2520for%2520action-level%2520importance.%2520Based%2520on%2520this%2520criterion%252C%2520VLA-Pruner%2520proposes%2520a%2520novel%2520dual-level%2520token%2520selection%2520strategy%2520that%2520adaptively%2520preserves%2520a%2520compact%252C%2520informative%2520set%2520of%2520visual%2520tokens%2520for%2520both%2520semantic%2520understanding%2520and%2520action%2520execution%2520under%2520given%2520compute%2520budget.%2520Experiments%2520show%2520that%2520VLA-Pruner%2520achieves%2520state-of-the-art%2520performance%2520across%2520multiple%2520VLA%2520architectures%2520and%2520diverse%2520robotic%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16449v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLA-Pruner%3A%20Temporal-Aware%20Dual-Level%20Visual%20Token%20Pruning%20for%20Efficient%20Vision-Language-Action%20Inference&entry.906535625=Ziyan%20Liu%20and%20Yeqiu%20Chen%20and%20Hongyi%20Cai%20and%20Tao%20Lin%20and%20Shuo%20Yang%20and%20Zheng%20Liu%20and%20Bo%20Zhao&entry.1292438233=Vision-Language-Action%20%28VLA%29%20models%20have%20shown%20great%20promise%20for%20embodied%20AI%2C%20yet%20the%20heavy%20computational%20cost%20of%20processing%20continuous%20visual%20streams%20severely%20limits%20their%20real-time%20deployment.%20Token%20pruning%20%28keeping%20salient%20visual%20tokens%20and%20dropping%20redundant%20ones%29%20has%20emerged%20as%20an%20effective%20approach%20for%20accelerating%20Vision-Language%20Models%20%28VLMs%29%2C%20offering%20a%20solution%20for%20efficient%20VLA.%20However%2C%20these%20VLM-specific%20token%20pruning%20methods%20select%20tokens%20based%20solely%20on%20semantic%20salience%20metrics%20%28e.g.%2C%20prefill%20attention%29%2C%20while%20overlooking%20the%20VLA%27s%20intrinsic%20dual-system%20nature%20of%20high-level%20semantic%20understanding%20and%20low-level%20action%20execution.%20Consequently%2C%20these%20methods%20bias%20token%20retention%20toward%20semantic%20cues%2C%20discard%20critical%20information%20for%20action%20generation%2C%20and%20significantly%20degrade%20VLA%20performance.%20To%20bridge%20this%20gap%2C%20we%20propose%20VLA-Pruner%2C%20a%20versatile%20plug-and-play%20VLA-specific%20token%20prune%20method%20that%20aligns%20with%20the%20dual-system%20nature%20of%20VLA%20models%20and%20exploits%20the%20temporal%20continuity%20in%20robot%20manipulation.%20Specifically%2C%20VLA-Pruner%20adopts%20a%20dual-level%20importance%20criterion%20for%20visual%20token%20retention%3A%20vision-language%20prefill%20attention%20for%20semantic-level%20relevance%20and%20action%20decode%20attention%2C%20estimated%20via%20temporal%20smoothing%2C%20for%20action-level%20importance.%20Based%20on%20this%20criterion%2C%20VLA-Pruner%20proposes%20a%20novel%20dual-level%20token%20selection%20strategy%20that%20adaptively%20preserves%20a%20compact%2C%20informative%20set%20of%20visual%20tokens%20for%20both%20semantic%20understanding%20and%20action%20execution%20under%20given%20compute%20budget.%20Experiments%20show%20that%20VLA-Pruner%20achieves%20state-of-the-art%20performance%20across%20multiple%20VLA%20architectures%20and%20diverse%20robotic%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2511.16449v2&entry.124074799=Read"},
{"title": "ID-Crafter: VLM-Grounded Online RL for Compositional Multi-Subject Video Generation", "author": "Panwang Pan and Jingjing Zhao and Yuchen Lin and Chenguo Lin and Chenxin Li and Hengyu Liu and Tingting Shen and Yadong MU", "abstract": "Significant progress has been achieved in high-fidelity video synthesis, yet current paradigms often fall short in effectively integrating identity information from multiple subjects. This leads to semantic conflicts and suboptimal performance in preserving identities and interactions, limiting controllability and applicability. To tackle this issue, we introduce ID-Crafter, a framework for multi-subject video generation that achieves superior identity preservation and semantic coherence. ID-Crafter integrates three key components: (i) a hierarchical identity-preserving attention mechanism that progressively aggregates features at intra-subject, inter-subject, and cross-modal levels; (ii) a semantic understanding module powered by a pretrained Vision-Language Model (VLM) to provide fine-grained guidance and capture complex inter-subject relationships; and (iii) an online reinforcement learning phase to further refine the model for critical concepts. Furthermore, we construct a new dataset to facilitate robust training and evaluation. Extensive experiments demonstrate that ID-Crafter establishes new state-of-the-art performance on multi-subject video generation benchmarks, excelling in identity preservation, temporal consistency, and overall video quality.", "link": "http://arxiv.org/abs/2511.00511v3", "date": "2025-11-21", "relevancy": 2.5367, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7254}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5785}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ID-Crafter%3A%20VLM-Grounded%20Online%20RL%20for%20Compositional%20Multi-Subject%20Video%20Generation&body=Title%3A%20ID-Crafter%3A%20VLM-Grounded%20Online%20RL%20for%20Compositional%20Multi-Subject%20Video%20Generation%0AAuthor%3A%20Panwang%20Pan%20and%20Jingjing%20Zhao%20and%20Yuchen%20Lin%20and%20Chenguo%20Lin%20and%20Chenxin%20Li%20and%20Hengyu%20Liu%20and%20Tingting%20Shen%20and%20Yadong%20MU%0AAbstract%3A%20Significant%20progress%20has%20been%20achieved%20in%20high-fidelity%20video%20synthesis%2C%20yet%20current%20paradigms%20often%20fall%20short%20in%20effectively%20integrating%20identity%20information%20from%20multiple%20subjects.%20This%20leads%20to%20semantic%20conflicts%20and%20suboptimal%20performance%20in%20preserving%20identities%20and%20interactions%2C%20limiting%20controllability%20and%20applicability.%20To%20tackle%20this%20issue%2C%20we%20introduce%20ID-Crafter%2C%20a%20framework%20for%20multi-subject%20video%20generation%20that%20achieves%20superior%20identity%20preservation%20and%20semantic%20coherence.%20ID-Crafter%20integrates%20three%20key%20components%3A%20%28i%29%20a%20hierarchical%20identity-preserving%20attention%20mechanism%20that%20progressively%20aggregates%20features%20at%20intra-subject%2C%20inter-subject%2C%20and%20cross-modal%20levels%3B%20%28ii%29%20a%20semantic%20understanding%20module%20powered%20by%20a%20pretrained%20Vision-Language%20Model%20%28VLM%29%20to%20provide%20fine-grained%20guidance%20and%20capture%20complex%20inter-subject%20relationships%3B%20and%20%28iii%29%20an%20online%20reinforcement%20learning%20phase%20to%20further%20refine%20the%20model%20for%20critical%20concepts.%20Furthermore%2C%20we%20construct%20a%20new%20dataset%20to%20facilitate%20robust%20training%20and%20evaluation.%20Extensive%20experiments%20demonstrate%20that%20ID-Crafter%20establishes%20new%20state-of-the-art%20performance%20on%20multi-subject%20video%20generation%20benchmarks%2C%20excelling%20in%20identity%20preservation%2C%20temporal%20consistency%2C%20and%20overall%20video%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2511.00511v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DID-Crafter%253A%2520VLM-Grounded%2520Online%2520RL%2520for%2520Compositional%2520Multi-Subject%2520Video%2520Generation%26entry.906535625%3DPanwang%2520Pan%2520and%2520Jingjing%2520Zhao%2520and%2520Yuchen%2520Lin%2520and%2520Chenguo%2520Lin%2520and%2520Chenxin%2520Li%2520and%2520Hengyu%2520Liu%2520and%2520Tingting%2520Shen%2520and%2520Yadong%2520MU%26entry.1292438233%3DSignificant%2520progress%2520has%2520been%2520achieved%2520in%2520high-fidelity%2520video%2520synthesis%252C%2520yet%2520current%2520paradigms%2520often%2520fall%2520short%2520in%2520effectively%2520integrating%2520identity%2520information%2520from%2520multiple%2520subjects.%2520This%2520leads%2520to%2520semantic%2520conflicts%2520and%2520suboptimal%2520performance%2520in%2520preserving%2520identities%2520and%2520interactions%252C%2520limiting%2520controllability%2520and%2520applicability.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520introduce%2520ID-Crafter%252C%2520a%2520framework%2520for%2520multi-subject%2520video%2520generation%2520that%2520achieves%2520superior%2520identity%2520preservation%2520and%2520semantic%2520coherence.%2520ID-Crafter%2520integrates%2520three%2520key%2520components%253A%2520%2528i%2529%2520a%2520hierarchical%2520identity-preserving%2520attention%2520mechanism%2520that%2520progressively%2520aggregates%2520features%2520at%2520intra-subject%252C%2520inter-subject%252C%2520and%2520cross-modal%2520levels%253B%2520%2528ii%2529%2520a%2520semantic%2520understanding%2520module%2520powered%2520by%2520a%2520pretrained%2520Vision-Language%2520Model%2520%2528VLM%2529%2520to%2520provide%2520fine-grained%2520guidance%2520and%2520capture%2520complex%2520inter-subject%2520relationships%253B%2520and%2520%2528iii%2529%2520an%2520online%2520reinforcement%2520learning%2520phase%2520to%2520further%2520refine%2520the%2520model%2520for%2520critical%2520concepts.%2520Furthermore%252C%2520we%2520construct%2520a%2520new%2520dataset%2520to%2520facilitate%2520robust%2520training%2520and%2520evaluation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520ID-Crafter%2520establishes%2520new%2520state-of-the-art%2520performance%2520on%2520multi-subject%2520video%2520generation%2520benchmarks%252C%2520excelling%2520in%2520identity%2520preservation%252C%2520temporal%2520consistency%252C%2520and%2520overall%2520video%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.00511v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ID-Crafter%3A%20VLM-Grounded%20Online%20RL%20for%20Compositional%20Multi-Subject%20Video%20Generation&entry.906535625=Panwang%20Pan%20and%20Jingjing%20Zhao%20and%20Yuchen%20Lin%20and%20Chenguo%20Lin%20and%20Chenxin%20Li%20and%20Hengyu%20Liu%20and%20Tingting%20Shen%20and%20Yadong%20MU&entry.1292438233=Significant%20progress%20has%20been%20achieved%20in%20high-fidelity%20video%20synthesis%2C%20yet%20current%20paradigms%20often%20fall%20short%20in%20effectively%20integrating%20identity%20information%20from%20multiple%20subjects.%20This%20leads%20to%20semantic%20conflicts%20and%20suboptimal%20performance%20in%20preserving%20identities%20and%20interactions%2C%20limiting%20controllability%20and%20applicability.%20To%20tackle%20this%20issue%2C%20we%20introduce%20ID-Crafter%2C%20a%20framework%20for%20multi-subject%20video%20generation%20that%20achieves%20superior%20identity%20preservation%20and%20semantic%20coherence.%20ID-Crafter%20integrates%20three%20key%20components%3A%20%28i%29%20a%20hierarchical%20identity-preserving%20attention%20mechanism%20that%20progressively%20aggregates%20features%20at%20intra-subject%2C%20inter-subject%2C%20and%20cross-modal%20levels%3B%20%28ii%29%20a%20semantic%20understanding%20module%20powered%20by%20a%20pretrained%20Vision-Language%20Model%20%28VLM%29%20to%20provide%20fine-grained%20guidance%20and%20capture%20complex%20inter-subject%20relationships%3B%20and%20%28iii%29%20an%20online%20reinforcement%20learning%20phase%20to%20further%20refine%20the%20model%20for%20critical%20concepts.%20Furthermore%2C%20we%20construct%20a%20new%20dataset%20to%20facilitate%20robust%20training%20and%20evaluation.%20Extensive%20experiments%20demonstrate%20that%20ID-Crafter%20establishes%20new%20state-of-the-art%20performance%20on%20multi-subject%20video%20generation%20benchmarks%2C%20excelling%20in%20identity%20preservation%2C%20temporal%20consistency%2C%20and%20overall%20video%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2511.00511v3&entry.124074799=Read"},
{"title": "SAVeD: Semantic Aware Version Discovery", "author": "Artem Frenk and Roee Shraga", "abstract": "Our work introduces SAVeD (Semantically Aware Version Detection), a contrastive learning-based framework for identifying versions of structured datasets without relying on metadata, labels, or integration-based assumptions. SAVeD addresses a common challenge in data science of repeated labor due to a difficulty of similar work or transformations on datasets. SAVeD employs a modified SimCLR pipeline, generating augmented table views through random transformations (e.g., row deletion, encoding perturbations). These views are embedded via a custom transformer encoder and contrasted in latent space to optimize semantic similarity. Our model learns to minimize distances between augmented views of the same dataset and maximize those between unrelated tables. We evaluate performance using validation accuracy and separation, defined respectively as the proportion of correctly classified version/non-version pairs on a hold-out set, and the difference between average similarities of versioned and non-versioned tables (defined by a benchmark, and not provided to the model). Our experiments span five canonical datasets from the Semantic Versioning in Databases Benchmark, and demonstrate substantial gains post-training. SAVeD achieves significantly higher accuracy on completely unseen tables in, and a significant boost in separation scores, confirming its capability to distinguish semantically altered versions. Compared to untrained baselines and prior state-of-the-art dataset-discovery methods like Starmie, our custom encoder achieves competitive or superior results.", "link": "http://arxiv.org/abs/2511.17298v1", "date": "2025-11-21", "relevancy": 2.5362, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5102}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5057}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAVeD%3A%20Semantic%20Aware%20Version%20Discovery&body=Title%3A%20SAVeD%3A%20Semantic%20Aware%20Version%20Discovery%0AAuthor%3A%20Artem%20Frenk%20and%20Roee%20Shraga%0AAbstract%3A%20Our%20work%20introduces%20SAVeD%20%28Semantically%20Aware%20Version%20Detection%29%2C%20a%20contrastive%20learning-based%20framework%20for%20identifying%20versions%20of%20structured%20datasets%20without%20relying%20on%20metadata%2C%20labels%2C%20or%20integration-based%20assumptions.%20SAVeD%20addresses%20a%20common%20challenge%20in%20data%20science%20of%20repeated%20labor%20due%20to%20a%20difficulty%20of%20similar%20work%20or%20transformations%20on%20datasets.%20SAVeD%20employs%20a%20modified%20SimCLR%20pipeline%2C%20generating%20augmented%20table%20views%20through%20random%20transformations%20%28e.g.%2C%20row%20deletion%2C%20encoding%20perturbations%29.%20These%20views%20are%20embedded%20via%20a%20custom%20transformer%20encoder%20and%20contrasted%20in%20latent%20space%20to%20optimize%20semantic%20similarity.%20Our%20model%20learns%20to%20minimize%20distances%20between%20augmented%20views%20of%20the%20same%20dataset%20and%20maximize%20those%20between%20unrelated%20tables.%20We%20evaluate%20performance%20using%20validation%20accuracy%20and%20separation%2C%20defined%20respectively%20as%20the%20proportion%20of%20correctly%20classified%20version/non-version%20pairs%20on%20a%20hold-out%20set%2C%20and%20the%20difference%20between%20average%20similarities%20of%20versioned%20and%20non-versioned%20tables%20%28defined%20by%20a%20benchmark%2C%20and%20not%20provided%20to%20the%20model%29.%20Our%20experiments%20span%20five%20canonical%20datasets%20from%20the%20Semantic%20Versioning%20in%20Databases%20Benchmark%2C%20and%20demonstrate%20substantial%20gains%20post-training.%20SAVeD%20achieves%20significantly%20higher%20accuracy%20on%20completely%20unseen%20tables%20in%2C%20and%20a%20significant%20boost%20in%20separation%20scores%2C%20confirming%20its%20capability%20to%20distinguish%20semantically%20altered%20versions.%20Compared%20to%20untrained%20baselines%20and%20prior%20state-of-the-art%20dataset-discovery%20methods%20like%20Starmie%2C%20our%20custom%20encoder%20achieves%20competitive%20or%20superior%20results.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17298v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAVeD%253A%2520Semantic%2520Aware%2520Version%2520Discovery%26entry.906535625%3DArtem%2520Frenk%2520and%2520Roee%2520Shraga%26entry.1292438233%3DOur%2520work%2520introduces%2520SAVeD%2520%2528Semantically%2520Aware%2520Version%2520Detection%2529%252C%2520a%2520contrastive%2520learning-based%2520framework%2520for%2520identifying%2520versions%2520of%2520structured%2520datasets%2520without%2520relying%2520on%2520metadata%252C%2520labels%252C%2520or%2520integration-based%2520assumptions.%2520SAVeD%2520addresses%2520a%2520common%2520challenge%2520in%2520data%2520science%2520of%2520repeated%2520labor%2520due%2520to%2520a%2520difficulty%2520of%2520similar%2520work%2520or%2520transformations%2520on%2520datasets.%2520SAVeD%2520employs%2520a%2520modified%2520SimCLR%2520pipeline%252C%2520generating%2520augmented%2520table%2520views%2520through%2520random%2520transformations%2520%2528e.g.%252C%2520row%2520deletion%252C%2520encoding%2520perturbations%2529.%2520These%2520views%2520are%2520embedded%2520via%2520a%2520custom%2520transformer%2520encoder%2520and%2520contrasted%2520in%2520latent%2520space%2520to%2520optimize%2520semantic%2520similarity.%2520Our%2520model%2520learns%2520to%2520minimize%2520distances%2520between%2520augmented%2520views%2520of%2520the%2520same%2520dataset%2520and%2520maximize%2520those%2520between%2520unrelated%2520tables.%2520We%2520evaluate%2520performance%2520using%2520validation%2520accuracy%2520and%2520separation%252C%2520defined%2520respectively%2520as%2520the%2520proportion%2520of%2520correctly%2520classified%2520version/non-version%2520pairs%2520on%2520a%2520hold-out%2520set%252C%2520and%2520the%2520difference%2520between%2520average%2520similarities%2520of%2520versioned%2520and%2520non-versioned%2520tables%2520%2528defined%2520by%2520a%2520benchmark%252C%2520and%2520not%2520provided%2520to%2520the%2520model%2529.%2520Our%2520experiments%2520span%2520five%2520canonical%2520datasets%2520from%2520the%2520Semantic%2520Versioning%2520in%2520Databases%2520Benchmark%252C%2520and%2520demonstrate%2520substantial%2520gains%2520post-training.%2520SAVeD%2520achieves%2520significantly%2520higher%2520accuracy%2520on%2520completely%2520unseen%2520tables%2520in%252C%2520and%2520a%2520significant%2520boost%2520in%2520separation%2520scores%252C%2520confirming%2520its%2520capability%2520to%2520distinguish%2520semantically%2520altered%2520versions.%2520Compared%2520to%2520untrained%2520baselines%2520and%2520prior%2520state-of-the-art%2520dataset-discovery%2520methods%2520like%2520Starmie%252C%2520our%2520custom%2520encoder%2520achieves%2520competitive%2520or%2520superior%2520results.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17298v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAVeD%3A%20Semantic%20Aware%20Version%20Discovery&entry.906535625=Artem%20Frenk%20and%20Roee%20Shraga&entry.1292438233=Our%20work%20introduces%20SAVeD%20%28Semantically%20Aware%20Version%20Detection%29%2C%20a%20contrastive%20learning-based%20framework%20for%20identifying%20versions%20of%20structured%20datasets%20without%20relying%20on%20metadata%2C%20labels%2C%20or%20integration-based%20assumptions.%20SAVeD%20addresses%20a%20common%20challenge%20in%20data%20science%20of%20repeated%20labor%20due%20to%20a%20difficulty%20of%20similar%20work%20or%20transformations%20on%20datasets.%20SAVeD%20employs%20a%20modified%20SimCLR%20pipeline%2C%20generating%20augmented%20table%20views%20through%20random%20transformations%20%28e.g.%2C%20row%20deletion%2C%20encoding%20perturbations%29.%20These%20views%20are%20embedded%20via%20a%20custom%20transformer%20encoder%20and%20contrasted%20in%20latent%20space%20to%20optimize%20semantic%20similarity.%20Our%20model%20learns%20to%20minimize%20distances%20between%20augmented%20views%20of%20the%20same%20dataset%20and%20maximize%20those%20between%20unrelated%20tables.%20We%20evaluate%20performance%20using%20validation%20accuracy%20and%20separation%2C%20defined%20respectively%20as%20the%20proportion%20of%20correctly%20classified%20version/non-version%20pairs%20on%20a%20hold-out%20set%2C%20and%20the%20difference%20between%20average%20similarities%20of%20versioned%20and%20non-versioned%20tables%20%28defined%20by%20a%20benchmark%2C%20and%20not%20provided%20to%20the%20model%29.%20Our%20experiments%20span%20five%20canonical%20datasets%20from%20the%20Semantic%20Versioning%20in%20Databases%20Benchmark%2C%20and%20demonstrate%20substantial%20gains%20post-training.%20SAVeD%20achieves%20significantly%20higher%20accuracy%20on%20completely%20unseen%20tables%20in%2C%20and%20a%20significant%20boost%20in%20separation%20scores%2C%20confirming%20its%20capability%20to%20distinguish%20semantically%20altered%20versions.%20Compared%20to%20untrained%20baselines%20and%20prior%20state-of-the-art%20dataset-discovery%20methods%20like%20Starmie%2C%20our%20custom%20encoder%20achieves%20competitive%20or%20superior%20results.&entry.1838667208=http%3A//arxiv.org/abs/2511.17298v1&entry.124074799=Read"},
{"title": "Label-Efficient Skeleton-based Recognition with Stable-Invertible Graph Convolutional Networks", "author": "Hichem Sahbi", "abstract": "Skeleton-based action recognition is a hotspot in image processing. A key challenge of this task lies in its dependence on large, manually labeled datasets whose acquisition is costly and time-consuming. This paper devises a novel, label-efficient method for skeleton-based action recognition using graph convolutional networks (GCNs). The contribution of the proposed method resides in learning a novel acquisition function -- scoring the most informative subsets for labeling -- as the optimum of an objective function mixing data representativity, diversity and uncertainty. We also extend this approach by learning the most informative subsets using an invertible GCN which allows mapping data from ambient to latent spaces where the inherent distribution of the data is more easily captured. Extensive experiments, conducted on two challenging skeleton-based recognition datasets, show the effectiveness and the outperformance of our label-frugal GCNs against the related work.", "link": "http://arxiv.org/abs/2511.17345v1", "date": "2025-11-21", "relevancy": 2.5316, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5185}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5017}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Label-Efficient%20Skeleton-based%20Recognition%20with%20Stable-Invertible%20Graph%20Convolutional%20Networks&body=Title%3A%20Label-Efficient%20Skeleton-based%20Recognition%20with%20Stable-Invertible%20Graph%20Convolutional%20Networks%0AAuthor%3A%20Hichem%20Sahbi%0AAbstract%3A%20Skeleton-based%20action%20recognition%20is%20a%20hotspot%20in%20image%20processing.%20A%20key%20challenge%20of%20this%20task%20lies%20in%20its%20dependence%20on%20large%2C%20manually%20labeled%20datasets%20whose%20acquisition%20is%20costly%20and%20time-consuming.%20This%20paper%20devises%20a%20novel%2C%20label-efficient%20method%20for%20skeleton-based%20action%20recognition%20using%20graph%20convolutional%20networks%20%28GCNs%29.%20The%20contribution%20of%20the%20proposed%20method%20resides%20in%20learning%20a%20novel%20acquisition%20function%20--%20scoring%20the%20most%20informative%20subsets%20for%20labeling%20--%20as%20the%20optimum%20of%20an%20objective%20function%20mixing%20data%20representativity%2C%20diversity%20and%20uncertainty.%20We%20also%20extend%20this%20approach%20by%20learning%20the%20most%20informative%20subsets%20using%20an%20invertible%20GCN%20which%20allows%20mapping%20data%20from%20ambient%20to%20latent%20spaces%20where%20the%20inherent%20distribution%20of%20the%20data%20is%20more%20easily%20captured.%20Extensive%20experiments%2C%20conducted%20on%20two%20challenging%20skeleton-based%20recognition%20datasets%2C%20show%20the%20effectiveness%20and%20the%20outperformance%20of%20our%20label-frugal%20GCNs%20against%20the%20related%20work.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17345v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLabel-Efficient%2520Skeleton-based%2520Recognition%2520with%2520Stable-Invertible%2520Graph%2520Convolutional%2520Networks%26entry.906535625%3DHichem%2520Sahbi%26entry.1292438233%3DSkeleton-based%2520action%2520recognition%2520is%2520a%2520hotspot%2520in%2520image%2520processing.%2520A%2520key%2520challenge%2520of%2520this%2520task%2520lies%2520in%2520its%2520dependence%2520on%2520large%252C%2520manually%2520labeled%2520datasets%2520whose%2520acquisition%2520is%2520costly%2520and%2520time-consuming.%2520This%2520paper%2520devises%2520a%2520novel%252C%2520label-efficient%2520method%2520for%2520skeleton-based%2520action%2520recognition%2520using%2520graph%2520convolutional%2520networks%2520%2528GCNs%2529.%2520The%2520contribution%2520of%2520the%2520proposed%2520method%2520resides%2520in%2520learning%2520a%2520novel%2520acquisition%2520function%2520--%2520scoring%2520the%2520most%2520informative%2520subsets%2520for%2520labeling%2520--%2520as%2520the%2520optimum%2520of%2520an%2520objective%2520function%2520mixing%2520data%2520representativity%252C%2520diversity%2520and%2520uncertainty.%2520We%2520also%2520extend%2520this%2520approach%2520by%2520learning%2520the%2520most%2520informative%2520subsets%2520using%2520an%2520invertible%2520GCN%2520which%2520allows%2520mapping%2520data%2520from%2520ambient%2520to%2520latent%2520spaces%2520where%2520the%2520inherent%2520distribution%2520of%2520the%2520data%2520is%2520more%2520easily%2520captured.%2520Extensive%2520experiments%252C%2520conducted%2520on%2520two%2520challenging%2520skeleton-based%2520recognition%2520datasets%252C%2520show%2520the%2520effectiveness%2520and%2520the%2520outperformance%2520of%2520our%2520label-frugal%2520GCNs%2520against%2520the%2520related%2520work.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17345v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label-Efficient%20Skeleton-based%20Recognition%20with%20Stable-Invertible%20Graph%20Convolutional%20Networks&entry.906535625=Hichem%20Sahbi&entry.1292438233=Skeleton-based%20action%20recognition%20is%20a%20hotspot%20in%20image%20processing.%20A%20key%20challenge%20of%20this%20task%20lies%20in%20its%20dependence%20on%20large%2C%20manually%20labeled%20datasets%20whose%20acquisition%20is%20costly%20and%20time-consuming.%20This%20paper%20devises%20a%20novel%2C%20label-efficient%20method%20for%20skeleton-based%20action%20recognition%20using%20graph%20convolutional%20networks%20%28GCNs%29.%20The%20contribution%20of%20the%20proposed%20method%20resides%20in%20learning%20a%20novel%20acquisition%20function%20--%20scoring%20the%20most%20informative%20subsets%20for%20labeling%20--%20as%20the%20optimum%20of%20an%20objective%20function%20mixing%20data%20representativity%2C%20diversity%20and%20uncertainty.%20We%20also%20extend%20this%20approach%20by%20learning%20the%20most%20informative%20subsets%20using%20an%20invertible%20GCN%20which%20allows%20mapping%20data%20from%20ambient%20to%20latent%20spaces%20where%20the%20inherent%20distribution%20of%20the%20data%20is%20more%20easily%20captured.%20Extensive%20experiments%2C%20conducted%20on%20two%20challenging%20skeleton-based%20recognition%20datasets%2C%20show%20the%20effectiveness%20and%20the%20outperformance%20of%20our%20label-frugal%20GCNs%20against%20the%20related%20work.&entry.1838667208=http%3A//arxiv.org/abs/2511.17345v1&entry.124074799=Read"},
{"title": "MusicAIR: A Multimodal AI Music Generation Framework Powered by an Algorithm-Driven Core", "author": "Callie C. Liao and Duoduo Liao and Ellie L. Zhang", "abstract": "Recent advances in generative AI have made music generation a prominent research focus. However, many neural-based models rely on large datasets, raising concerns about copyright infringement and high-performance costs. In contrast, we propose MusicAIR, an innovative multimodal AI music generation framework powered by a novel algorithm-driven symbolic music core, effectively mitigating copyright infringement risks. The music core algorithms connect critical lyrical and rhythmic information to automatically derive musical features, creating a complete, coherent melodic score solely from the lyrics. The MusicAIR framework facilitates music generation from lyrics, text, and images. The generated score adheres to established principles of music theory, lyrical structure, and rhythmic conventions. We developed Generate AI Music (GenAIM), a web tool using MusicAIR for lyric-to-song, text-to-music, and image-to-music generation. In our experiments, we evaluated AI-generated music scores produced by the system using both standard music metrics and innovative analysis that compares these compositions with original works. The system achieves an average key confidence of 85%, outperforming human composers at 79%, and aligns closely with established music theory standards, demonstrating its ability to generate diverse, human-like compositions. As a co-pilot tool, GenAIM can serve as a reliable music composition assistant and a possible educational composition tutor while simultaneously lowering the entry barrier for all aspiring musicians, which is innovative and significantly contributes to AI for music generation.", "link": "http://arxiv.org/abs/2511.17323v1", "date": "2025-11-21", "relevancy": 2.5164, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5285}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.491}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MusicAIR%3A%20A%20Multimodal%20AI%20Music%20Generation%20Framework%20Powered%20by%20an%20Algorithm-Driven%20Core&body=Title%3A%20MusicAIR%3A%20A%20Multimodal%20AI%20Music%20Generation%20Framework%20Powered%20by%20an%20Algorithm-Driven%20Core%0AAuthor%3A%20Callie%20C.%20Liao%20and%20Duoduo%20Liao%20and%20Ellie%20L.%20Zhang%0AAbstract%3A%20Recent%20advances%20in%20generative%20AI%20have%20made%20music%20generation%20a%20prominent%20research%20focus.%20However%2C%20many%20neural-based%20models%20rely%20on%20large%20datasets%2C%20raising%20concerns%20about%20copyright%20infringement%20and%20high-performance%20costs.%20In%20contrast%2C%20we%20propose%20MusicAIR%2C%20an%20innovative%20multimodal%20AI%20music%20generation%20framework%20powered%20by%20a%20novel%20algorithm-driven%20symbolic%20music%20core%2C%20effectively%20mitigating%20copyright%20infringement%20risks.%20The%20music%20core%20algorithms%20connect%20critical%20lyrical%20and%20rhythmic%20information%20to%20automatically%20derive%20musical%20features%2C%20creating%20a%20complete%2C%20coherent%20melodic%20score%20solely%20from%20the%20lyrics.%20The%20MusicAIR%20framework%20facilitates%20music%20generation%20from%20lyrics%2C%20text%2C%20and%20images.%20The%20generated%20score%20adheres%20to%20established%20principles%20of%20music%20theory%2C%20lyrical%20structure%2C%20and%20rhythmic%20conventions.%20We%20developed%20Generate%20AI%20Music%20%28GenAIM%29%2C%20a%20web%20tool%20using%20MusicAIR%20for%20lyric-to-song%2C%20text-to-music%2C%20and%20image-to-music%20generation.%20In%20our%20experiments%2C%20we%20evaluated%20AI-generated%20music%20scores%20produced%20by%20the%20system%20using%20both%20standard%20music%20metrics%20and%20innovative%20analysis%20that%20compares%20these%20compositions%20with%20original%20works.%20The%20system%20achieves%20an%20average%20key%20confidence%20of%2085%25%2C%20outperforming%20human%20composers%20at%2079%25%2C%20and%20aligns%20closely%20with%20established%20music%20theory%20standards%2C%20demonstrating%20its%20ability%20to%20generate%20diverse%2C%20human-like%20compositions.%20As%20a%20co-pilot%20tool%2C%20GenAIM%20can%20serve%20as%20a%20reliable%20music%20composition%20assistant%20and%20a%20possible%20educational%20composition%20tutor%20while%20simultaneously%20lowering%20the%20entry%20barrier%20for%20all%20aspiring%20musicians%2C%20which%20is%20innovative%20and%20significantly%20contributes%20to%20AI%20for%20music%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMusicAIR%253A%2520A%2520Multimodal%2520AI%2520Music%2520Generation%2520Framework%2520Powered%2520by%2520an%2520Algorithm-Driven%2520Core%26entry.906535625%3DCallie%2520C.%2520Liao%2520and%2520Duoduo%2520Liao%2520and%2520Ellie%2520L.%2520Zhang%26entry.1292438233%3DRecent%2520advances%2520in%2520generative%2520AI%2520have%2520made%2520music%2520generation%2520a%2520prominent%2520research%2520focus.%2520However%252C%2520many%2520neural-based%2520models%2520rely%2520on%2520large%2520datasets%252C%2520raising%2520concerns%2520about%2520copyright%2520infringement%2520and%2520high-performance%2520costs.%2520In%2520contrast%252C%2520we%2520propose%2520MusicAIR%252C%2520an%2520innovative%2520multimodal%2520AI%2520music%2520generation%2520framework%2520powered%2520by%2520a%2520novel%2520algorithm-driven%2520symbolic%2520music%2520core%252C%2520effectively%2520mitigating%2520copyright%2520infringement%2520risks.%2520The%2520music%2520core%2520algorithms%2520connect%2520critical%2520lyrical%2520and%2520rhythmic%2520information%2520to%2520automatically%2520derive%2520musical%2520features%252C%2520creating%2520a%2520complete%252C%2520coherent%2520melodic%2520score%2520solely%2520from%2520the%2520lyrics.%2520The%2520MusicAIR%2520framework%2520facilitates%2520music%2520generation%2520from%2520lyrics%252C%2520text%252C%2520and%2520images.%2520The%2520generated%2520score%2520adheres%2520to%2520established%2520principles%2520of%2520music%2520theory%252C%2520lyrical%2520structure%252C%2520and%2520rhythmic%2520conventions.%2520We%2520developed%2520Generate%2520AI%2520Music%2520%2528GenAIM%2529%252C%2520a%2520web%2520tool%2520using%2520MusicAIR%2520for%2520lyric-to-song%252C%2520text-to-music%252C%2520and%2520image-to-music%2520generation.%2520In%2520our%2520experiments%252C%2520we%2520evaluated%2520AI-generated%2520music%2520scores%2520produced%2520by%2520the%2520system%2520using%2520both%2520standard%2520music%2520metrics%2520and%2520innovative%2520analysis%2520that%2520compares%2520these%2520compositions%2520with%2520original%2520works.%2520The%2520system%2520achieves%2520an%2520average%2520key%2520confidence%2520of%252085%2525%252C%2520outperforming%2520human%2520composers%2520at%252079%2525%252C%2520and%2520aligns%2520closely%2520with%2520established%2520music%2520theory%2520standards%252C%2520demonstrating%2520its%2520ability%2520to%2520generate%2520diverse%252C%2520human-like%2520compositions.%2520As%2520a%2520co-pilot%2520tool%252C%2520GenAIM%2520can%2520serve%2520as%2520a%2520reliable%2520music%2520composition%2520assistant%2520and%2520a%2520possible%2520educational%2520composition%2520tutor%2520while%2520simultaneously%2520lowering%2520the%2520entry%2520barrier%2520for%2520all%2520aspiring%2520musicians%252C%2520which%2520is%2520innovative%2520and%2520significantly%2520contributes%2520to%2520AI%2520for%2520music%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MusicAIR%3A%20A%20Multimodal%20AI%20Music%20Generation%20Framework%20Powered%20by%20an%20Algorithm-Driven%20Core&entry.906535625=Callie%20C.%20Liao%20and%20Duoduo%20Liao%20and%20Ellie%20L.%20Zhang&entry.1292438233=Recent%20advances%20in%20generative%20AI%20have%20made%20music%20generation%20a%20prominent%20research%20focus.%20However%2C%20many%20neural-based%20models%20rely%20on%20large%20datasets%2C%20raising%20concerns%20about%20copyright%20infringement%20and%20high-performance%20costs.%20In%20contrast%2C%20we%20propose%20MusicAIR%2C%20an%20innovative%20multimodal%20AI%20music%20generation%20framework%20powered%20by%20a%20novel%20algorithm-driven%20symbolic%20music%20core%2C%20effectively%20mitigating%20copyright%20infringement%20risks.%20The%20music%20core%20algorithms%20connect%20critical%20lyrical%20and%20rhythmic%20information%20to%20automatically%20derive%20musical%20features%2C%20creating%20a%20complete%2C%20coherent%20melodic%20score%20solely%20from%20the%20lyrics.%20The%20MusicAIR%20framework%20facilitates%20music%20generation%20from%20lyrics%2C%20text%2C%20and%20images.%20The%20generated%20score%20adheres%20to%20established%20principles%20of%20music%20theory%2C%20lyrical%20structure%2C%20and%20rhythmic%20conventions.%20We%20developed%20Generate%20AI%20Music%20%28GenAIM%29%2C%20a%20web%20tool%20using%20MusicAIR%20for%20lyric-to-song%2C%20text-to-music%2C%20and%20image-to-music%20generation.%20In%20our%20experiments%2C%20we%20evaluated%20AI-generated%20music%20scores%20produced%20by%20the%20system%20using%20both%20standard%20music%20metrics%20and%20innovative%20analysis%20that%20compares%20these%20compositions%20with%20original%20works.%20The%20system%20achieves%20an%20average%20key%20confidence%20of%2085%25%2C%20outperforming%20human%20composers%20at%2079%25%2C%20and%20aligns%20closely%20with%20established%20music%20theory%20standards%2C%20demonstrating%20its%20ability%20to%20generate%20diverse%2C%20human-like%20compositions.%20As%20a%20co-pilot%20tool%2C%20GenAIM%20can%20serve%20as%20a%20reliable%20music%20composition%20assistant%20and%20a%20possible%20educational%20composition%20tutor%20while%20simultaneously%20lowering%20the%20entry%20barrier%20for%20all%20aspiring%20musicians%2C%20which%20is%20innovative%20and%20significantly%20contributes%20to%20AI%20for%20music%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2511.17323v1&entry.124074799=Read"},
{"title": "InTAct: Interval-based Task Activation Consolidation for Continual Learning", "author": "Patryk Krukowski and Jan Miksa and Piotr Helm and Jacek Tabor and Pawe\u0142 Wawrzy\u0144ski and Przemys\u0142aw Spurek", "abstract": "Continual learning aims to enable neural networks to acquire new knowledge without forgetting previously learned information. While recent prompt-based methods perform strongly in class-incremental settings, they remain vulnerable under domain shifts, where the input distribution changes but the label space remains fixed. This exposes a persistent problem known as representation drift. Shared representations evolve in ways that overwrite previously useful features and cause forgetting even when prompts isolate task-specific parameters. To address this issue, we introduce InTAct, a method that preserves functional behavior in shared layers without freezing parameters or storing past data. InTAct captures the characteristic activation ranges associated with previously learned tasks and constrains updates to ensure the network remains consistent within these regions, while still allowing for flexible adaptation elsewhere. In doing so, InTAct stabilizes the functional role of important neurons rather than directly restricting parameter values. The approach is architecture-agnostic and integrates seamlessly into existing prompt-based continual learning frameworks. By regulating representation changes where past knowledge is encoded, InTAct achieves a principled balance between stability and plasticity. Across diverse domain-incremental benchmarks, including DomainNet and ImageNet-R, InTAct consistently reduces representation drift and improves performance, increasing Average Accuracy by up to 8 percentage points over state-of-the-art baselines.", "link": "http://arxiv.org/abs/2511.17439v1", "date": "2025-11-21", "relevancy": 2.5139, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5096}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5084}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InTAct%3A%20Interval-based%20Task%20Activation%20Consolidation%20for%20Continual%20Learning&body=Title%3A%20InTAct%3A%20Interval-based%20Task%20Activation%20Consolidation%20for%20Continual%20Learning%0AAuthor%3A%20Patryk%20Krukowski%20and%20Jan%20Miksa%20and%20Piotr%20Helm%20and%20Jacek%20Tabor%20and%20Pawe%C5%82%20Wawrzy%C5%84ski%20and%20Przemys%C5%82aw%20Spurek%0AAbstract%3A%20Continual%20learning%20aims%20to%20enable%20neural%20networks%20to%20acquire%20new%20knowledge%20without%20forgetting%20previously%20learned%20information.%20While%20recent%20prompt-based%20methods%20perform%20strongly%20in%20class-incremental%20settings%2C%20they%20remain%20vulnerable%20under%20domain%20shifts%2C%20where%20the%20input%20distribution%20changes%20but%20the%20label%20space%20remains%20fixed.%20This%20exposes%20a%20persistent%20problem%20known%20as%20representation%20drift.%20Shared%20representations%20evolve%20in%20ways%20that%20overwrite%20previously%20useful%20features%20and%20cause%20forgetting%20even%20when%20prompts%20isolate%20task-specific%20parameters.%20To%20address%20this%20issue%2C%20we%20introduce%20InTAct%2C%20a%20method%20that%20preserves%20functional%20behavior%20in%20shared%20layers%20without%20freezing%20parameters%20or%20storing%20past%20data.%20InTAct%20captures%20the%20characteristic%20activation%20ranges%20associated%20with%20previously%20learned%20tasks%20and%20constrains%20updates%20to%20ensure%20the%20network%20remains%20consistent%20within%20these%20regions%2C%20while%20still%20allowing%20for%20flexible%20adaptation%20elsewhere.%20In%20doing%20so%2C%20InTAct%20stabilizes%20the%20functional%20role%20of%20important%20neurons%20rather%20than%20directly%20restricting%20parameter%20values.%20The%20approach%20is%20architecture-agnostic%20and%20integrates%20seamlessly%20into%20existing%20prompt-based%20continual%20learning%20frameworks.%20By%20regulating%20representation%20changes%20where%20past%20knowledge%20is%20encoded%2C%20InTAct%20achieves%20a%20principled%20balance%20between%20stability%20and%20plasticity.%20Across%20diverse%20domain-incremental%20benchmarks%2C%20including%20DomainNet%20and%20ImageNet-R%2C%20InTAct%20consistently%20reduces%20representation%20drift%20and%20improves%20performance%2C%20increasing%20Average%20Accuracy%20by%20up%20to%208%20percentage%20points%20over%20state-of-the-art%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInTAct%253A%2520Interval-based%2520Task%2520Activation%2520Consolidation%2520for%2520Continual%2520Learning%26entry.906535625%3DPatryk%2520Krukowski%2520and%2520Jan%2520Miksa%2520and%2520Piotr%2520Helm%2520and%2520Jacek%2520Tabor%2520and%2520Pawe%25C5%2582%2520Wawrzy%25C5%2584ski%2520and%2520Przemys%25C5%2582aw%2520Spurek%26entry.1292438233%3DContinual%2520learning%2520aims%2520to%2520enable%2520neural%2520networks%2520to%2520acquire%2520new%2520knowledge%2520without%2520forgetting%2520previously%2520learned%2520information.%2520While%2520recent%2520prompt-based%2520methods%2520perform%2520strongly%2520in%2520class-incremental%2520settings%252C%2520they%2520remain%2520vulnerable%2520under%2520domain%2520shifts%252C%2520where%2520the%2520input%2520distribution%2520changes%2520but%2520the%2520label%2520space%2520remains%2520fixed.%2520This%2520exposes%2520a%2520persistent%2520problem%2520known%2520as%2520representation%2520drift.%2520Shared%2520representations%2520evolve%2520in%2520ways%2520that%2520overwrite%2520previously%2520useful%2520features%2520and%2520cause%2520forgetting%2520even%2520when%2520prompts%2520isolate%2520task-specific%2520parameters.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520InTAct%252C%2520a%2520method%2520that%2520preserves%2520functional%2520behavior%2520in%2520shared%2520layers%2520without%2520freezing%2520parameters%2520or%2520storing%2520past%2520data.%2520InTAct%2520captures%2520the%2520characteristic%2520activation%2520ranges%2520associated%2520with%2520previously%2520learned%2520tasks%2520and%2520constrains%2520updates%2520to%2520ensure%2520the%2520network%2520remains%2520consistent%2520within%2520these%2520regions%252C%2520while%2520still%2520allowing%2520for%2520flexible%2520adaptation%2520elsewhere.%2520In%2520doing%2520so%252C%2520InTAct%2520stabilizes%2520the%2520functional%2520role%2520of%2520important%2520neurons%2520rather%2520than%2520directly%2520restricting%2520parameter%2520values.%2520The%2520approach%2520is%2520architecture-agnostic%2520and%2520integrates%2520seamlessly%2520into%2520existing%2520prompt-based%2520continual%2520learning%2520frameworks.%2520By%2520regulating%2520representation%2520changes%2520where%2520past%2520knowledge%2520is%2520encoded%252C%2520InTAct%2520achieves%2520a%2520principled%2520balance%2520between%2520stability%2520and%2520plasticity.%2520Across%2520diverse%2520domain-incremental%2520benchmarks%252C%2520including%2520DomainNet%2520and%2520ImageNet-R%252C%2520InTAct%2520consistently%2520reduces%2520representation%2520drift%2520and%2520improves%2520performance%252C%2520increasing%2520Average%2520Accuracy%2520by%2520up%2520to%25208%2520percentage%2520points%2520over%2520state-of-the-art%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InTAct%3A%20Interval-based%20Task%20Activation%20Consolidation%20for%20Continual%20Learning&entry.906535625=Patryk%20Krukowski%20and%20Jan%20Miksa%20and%20Piotr%20Helm%20and%20Jacek%20Tabor%20and%20Pawe%C5%82%20Wawrzy%C5%84ski%20and%20Przemys%C5%82aw%20Spurek&entry.1292438233=Continual%20learning%20aims%20to%20enable%20neural%20networks%20to%20acquire%20new%20knowledge%20without%20forgetting%20previously%20learned%20information.%20While%20recent%20prompt-based%20methods%20perform%20strongly%20in%20class-incremental%20settings%2C%20they%20remain%20vulnerable%20under%20domain%20shifts%2C%20where%20the%20input%20distribution%20changes%20but%20the%20label%20space%20remains%20fixed.%20This%20exposes%20a%20persistent%20problem%20known%20as%20representation%20drift.%20Shared%20representations%20evolve%20in%20ways%20that%20overwrite%20previously%20useful%20features%20and%20cause%20forgetting%20even%20when%20prompts%20isolate%20task-specific%20parameters.%20To%20address%20this%20issue%2C%20we%20introduce%20InTAct%2C%20a%20method%20that%20preserves%20functional%20behavior%20in%20shared%20layers%20without%20freezing%20parameters%20or%20storing%20past%20data.%20InTAct%20captures%20the%20characteristic%20activation%20ranges%20associated%20with%20previously%20learned%20tasks%20and%20constrains%20updates%20to%20ensure%20the%20network%20remains%20consistent%20within%20these%20regions%2C%20while%20still%20allowing%20for%20flexible%20adaptation%20elsewhere.%20In%20doing%20so%2C%20InTAct%20stabilizes%20the%20functional%20role%20of%20important%20neurons%20rather%20than%20directly%20restricting%20parameter%20values.%20The%20approach%20is%20architecture-agnostic%20and%20integrates%20seamlessly%20into%20existing%20prompt-based%20continual%20learning%20frameworks.%20By%20regulating%20representation%20changes%20where%20past%20knowledge%20is%20encoded%2C%20InTAct%20achieves%20a%20principled%20balance%20between%20stability%20and%20plasticity.%20Across%20diverse%20domain-incremental%20benchmarks%2C%20including%20DomainNet%20and%20ImageNet-R%2C%20InTAct%20consistently%20reduces%20representation%20drift%20and%20improves%20performance%2C%20increasing%20Average%20Accuracy%20by%20up%20to%208%20percentage%20points%20over%20state-of-the-art%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2511.17439v1&entry.124074799=Read"},
{"title": "PostCam: Camera-Controllable Novel-View Video Generation with Query-Shared Cross-Attention", "author": "Yipeng Chen and Zhichao Ye and Zhenzhou Fang and Xinyu Chen and Xiaoyu Zhang and Jialing Liu and Nan Wang and Haomin Liu and Guofeng Zhang", "abstract": "We propose PostCam, a framework for novel-view video generation that enables post-capture editing of camera trajectories in dynamic scenes. We find that existing video recapture methods suffer from suboptimal camera motion injection strategies; such suboptimal designs not only limit camera control precision but also result in generated videos that fail to preserve fine visual details from the source video. To achieve more accurate and flexible motion manipulation, PostCam introduces a query-shared cross-attention module. It integrates two distinct forms of control signals: the 6-DoF camera poses and the 2D rendered video frames. By fusing them into a unified representation within a shared feature space, our model can extract underlying motion cues, which enhances both control precision and generation quality. Furthermore, we adopt a two-stage training strategy: the model first learns coarse camera control from pose inputs, and then incorporates visual information to refine motion accuracy and enhance visual fidelity. Experiments on both real-world and synthetic datasets demonstrate that PostCam outperforms state-of-the-art methods by over 20% in camera control precision and view consistency, while achieving the highest video generation quality. Our project webpage is publicly available at: https://cccqaq.github.io/PostCam.github.io/", "link": "http://arxiv.org/abs/2511.17185v1", "date": "2025-11-21", "relevancy": 2.51, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.7141}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6102}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PostCam%3A%20Camera-Controllable%20Novel-View%20Video%20Generation%20with%20Query-Shared%20Cross-Attention&body=Title%3A%20PostCam%3A%20Camera-Controllable%20Novel-View%20Video%20Generation%20with%20Query-Shared%20Cross-Attention%0AAuthor%3A%20Yipeng%20Chen%20and%20Zhichao%20Ye%20and%20Zhenzhou%20Fang%20and%20Xinyu%20Chen%20and%20Xiaoyu%20Zhang%20and%20Jialing%20Liu%20and%20Nan%20Wang%20and%20Haomin%20Liu%20and%20Guofeng%20Zhang%0AAbstract%3A%20We%20propose%20PostCam%2C%20a%20framework%20for%20novel-view%20video%20generation%20that%20enables%20post-capture%20editing%20of%20camera%20trajectories%20in%20dynamic%20scenes.%20We%20find%20that%20existing%20video%20recapture%20methods%20suffer%20from%20suboptimal%20camera%20motion%20injection%20strategies%3B%20such%20suboptimal%20designs%20not%20only%20limit%20camera%20control%20precision%20but%20also%20result%20in%20generated%20videos%20that%20fail%20to%20preserve%20fine%20visual%20details%20from%20the%20source%20video.%20To%20achieve%20more%20accurate%20and%20flexible%20motion%20manipulation%2C%20PostCam%20introduces%20a%20query-shared%20cross-attention%20module.%20It%20integrates%20two%20distinct%20forms%20of%20control%20signals%3A%20the%206-DoF%20camera%20poses%20and%20the%202D%20rendered%20video%20frames.%20By%20fusing%20them%20into%20a%20unified%20representation%20within%20a%20shared%20feature%20space%2C%20our%20model%20can%20extract%20underlying%20motion%20cues%2C%20which%20enhances%20both%20control%20precision%20and%20generation%20quality.%20Furthermore%2C%20we%20adopt%20a%20two-stage%20training%20strategy%3A%20the%20model%20first%20learns%20coarse%20camera%20control%20from%20pose%20inputs%2C%20and%20then%20incorporates%20visual%20information%20to%20refine%20motion%20accuracy%20and%20enhance%20visual%20fidelity.%20Experiments%20on%20both%20real-world%20and%20synthetic%20datasets%20demonstrate%20that%20PostCam%20outperforms%20state-of-the-art%20methods%20by%20over%2020%25%20in%20camera%20control%20precision%20and%20view%20consistency%2C%20while%20achieving%20the%20highest%20video%20generation%20quality.%20Our%20project%20webpage%20is%20publicly%20available%20at%3A%20https%3A//cccqaq.github.io/PostCam.github.io/%0ALink%3A%20http%3A//arxiv.org/abs/2511.17185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPostCam%253A%2520Camera-Controllable%2520Novel-View%2520Video%2520Generation%2520with%2520Query-Shared%2520Cross-Attention%26entry.906535625%3DYipeng%2520Chen%2520and%2520Zhichao%2520Ye%2520and%2520Zhenzhou%2520Fang%2520and%2520Xinyu%2520Chen%2520and%2520Xiaoyu%2520Zhang%2520and%2520Jialing%2520Liu%2520and%2520Nan%2520Wang%2520and%2520Haomin%2520Liu%2520and%2520Guofeng%2520Zhang%26entry.1292438233%3DWe%2520propose%2520PostCam%252C%2520a%2520framework%2520for%2520novel-view%2520video%2520generation%2520that%2520enables%2520post-capture%2520editing%2520of%2520camera%2520trajectories%2520in%2520dynamic%2520scenes.%2520We%2520find%2520that%2520existing%2520video%2520recapture%2520methods%2520suffer%2520from%2520suboptimal%2520camera%2520motion%2520injection%2520strategies%253B%2520such%2520suboptimal%2520designs%2520not%2520only%2520limit%2520camera%2520control%2520precision%2520but%2520also%2520result%2520in%2520generated%2520videos%2520that%2520fail%2520to%2520preserve%2520fine%2520visual%2520details%2520from%2520the%2520source%2520video.%2520To%2520achieve%2520more%2520accurate%2520and%2520flexible%2520motion%2520manipulation%252C%2520PostCam%2520introduces%2520a%2520query-shared%2520cross-attention%2520module.%2520It%2520integrates%2520two%2520distinct%2520forms%2520of%2520control%2520signals%253A%2520the%25206-DoF%2520camera%2520poses%2520and%2520the%25202D%2520rendered%2520video%2520frames.%2520By%2520fusing%2520them%2520into%2520a%2520unified%2520representation%2520within%2520a%2520shared%2520feature%2520space%252C%2520our%2520model%2520can%2520extract%2520underlying%2520motion%2520cues%252C%2520which%2520enhances%2520both%2520control%2520precision%2520and%2520generation%2520quality.%2520Furthermore%252C%2520we%2520adopt%2520a%2520two-stage%2520training%2520strategy%253A%2520the%2520model%2520first%2520learns%2520coarse%2520camera%2520control%2520from%2520pose%2520inputs%252C%2520and%2520then%2520incorporates%2520visual%2520information%2520to%2520refine%2520motion%2520accuracy%2520and%2520enhance%2520visual%2520fidelity.%2520Experiments%2520on%2520both%2520real-world%2520and%2520synthetic%2520datasets%2520demonstrate%2520that%2520PostCam%2520outperforms%2520state-of-the-art%2520methods%2520by%2520over%252020%2525%2520in%2520camera%2520control%2520precision%2520and%2520view%2520consistency%252C%2520while%2520achieving%2520the%2520highest%2520video%2520generation%2520quality.%2520Our%2520project%2520webpage%2520is%2520publicly%2520available%2520at%253A%2520https%253A//cccqaq.github.io/PostCam.github.io/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PostCam%3A%20Camera-Controllable%20Novel-View%20Video%20Generation%20with%20Query-Shared%20Cross-Attention&entry.906535625=Yipeng%20Chen%20and%20Zhichao%20Ye%20and%20Zhenzhou%20Fang%20and%20Xinyu%20Chen%20and%20Xiaoyu%20Zhang%20and%20Jialing%20Liu%20and%20Nan%20Wang%20and%20Haomin%20Liu%20and%20Guofeng%20Zhang&entry.1292438233=We%20propose%20PostCam%2C%20a%20framework%20for%20novel-view%20video%20generation%20that%20enables%20post-capture%20editing%20of%20camera%20trajectories%20in%20dynamic%20scenes.%20We%20find%20that%20existing%20video%20recapture%20methods%20suffer%20from%20suboptimal%20camera%20motion%20injection%20strategies%3B%20such%20suboptimal%20designs%20not%20only%20limit%20camera%20control%20precision%20but%20also%20result%20in%20generated%20videos%20that%20fail%20to%20preserve%20fine%20visual%20details%20from%20the%20source%20video.%20To%20achieve%20more%20accurate%20and%20flexible%20motion%20manipulation%2C%20PostCam%20introduces%20a%20query-shared%20cross-attention%20module.%20It%20integrates%20two%20distinct%20forms%20of%20control%20signals%3A%20the%206-DoF%20camera%20poses%20and%20the%202D%20rendered%20video%20frames.%20By%20fusing%20them%20into%20a%20unified%20representation%20within%20a%20shared%20feature%20space%2C%20our%20model%20can%20extract%20underlying%20motion%20cues%2C%20which%20enhances%20both%20control%20precision%20and%20generation%20quality.%20Furthermore%2C%20we%20adopt%20a%20two-stage%20training%20strategy%3A%20the%20model%20first%20learns%20coarse%20camera%20control%20from%20pose%20inputs%2C%20and%20then%20incorporates%20visual%20information%20to%20refine%20motion%20accuracy%20and%20enhance%20visual%20fidelity.%20Experiments%20on%20both%20real-world%20and%20synthetic%20datasets%20demonstrate%20that%20PostCam%20outperforms%20state-of-the-art%20methods%20by%20over%2020%25%20in%20camera%20control%20precision%20and%20view%20consistency%2C%20while%20achieving%20the%20highest%20video%20generation%20quality.%20Our%20project%20webpage%20is%20publicly%20available%20at%3A%20https%3A//cccqaq.github.io/PostCam.github.io/&entry.1838667208=http%3A//arxiv.org/abs/2511.17185v1&entry.124074799=Read"},
{"title": "SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense", "author": "Patryk Krukowski and \u0141ukasz Gorczyca and Piotr Helm and Kamil Ksi\u0105\u017cek and Przemys\u0142aw Spurek", "abstract": "Continual learning under adversarial conditions remains an open problem, as existing methods often compromise either robustness, scalability, or both. We propose a novel framework that integrates Interval Bound Propagation (IBP) with a hypernetwork-based architecture to enable certifiably robust continual learning across sequential tasks. Our method, SHIELD, generates task-specific model parameters via a shared hypernetwork conditioned solely on compact task embeddings, eliminating the need for replay buffers or full model copies and enabling efficient over time. To further enhance robustness, we introduce Interval MixUp, a novel training strategy that blends virtual examples represented as $\\ell_{\\infty}$ balls centered around MixUp points. Leveraging interval arithmetic, this technique guarantees certified robustness while mitigating the wrapping effect, resulting in smoother decision boundaries. We evaluate SHIELD under strong white-box adversarial attacks, including PGD and AutoAttack, across multiple benchmarks. It consistently outperforms existing robust continual learning methods, achieving state-of-the-art average accuracy while maintaining both scalability and certification. These results represent a significant step toward practical and theoretically grounded continual learning in adversarial settings.", "link": "http://arxiv.org/abs/2506.08255v3", "date": "2025-11-21", "relevancy": 2.5082, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5256}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4958}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SHIELD%3A%20Secure%20Hypernetworks%20for%20Incremental%20Expansion%20Learning%20Defense&body=Title%3A%20SHIELD%3A%20Secure%20Hypernetworks%20for%20Incremental%20Expansion%20Learning%20Defense%0AAuthor%3A%20Patryk%20Krukowski%20and%20%C5%81ukasz%20Gorczyca%20and%20Piotr%20Helm%20and%20Kamil%20Ksi%C4%85%C5%BCek%20and%20Przemys%C5%82aw%20Spurek%0AAbstract%3A%20Continual%20learning%20under%20adversarial%20conditions%20remains%20an%20open%20problem%2C%20as%20existing%20methods%20often%20compromise%20either%20robustness%2C%20scalability%2C%20or%20both.%20We%20propose%20a%20novel%20framework%20that%20integrates%20Interval%20Bound%20Propagation%20%28IBP%29%20with%20a%20hypernetwork-based%20architecture%20to%20enable%20certifiably%20robust%20continual%20learning%20across%20sequential%20tasks.%20Our%20method%2C%20SHIELD%2C%20generates%20task-specific%20model%20parameters%20via%20a%20shared%20hypernetwork%20conditioned%20solely%20on%20compact%20task%20embeddings%2C%20eliminating%20the%20need%20for%20replay%20buffers%20or%20full%20model%20copies%20and%20enabling%20efficient%20over%20time.%20To%20further%20enhance%20robustness%2C%20we%20introduce%20Interval%20MixUp%2C%20a%20novel%20training%20strategy%20that%20blends%20virtual%20examples%20represented%20as%20%24%5Cell_%7B%5Cinfty%7D%24%20balls%20centered%20around%20MixUp%20points.%20Leveraging%20interval%20arithmetic%2C%20this%20technique%20guarantees%20certified%20robustness%20while%20mitigating%20the%20wrapping%20effect%2C%20resulting%20in%20smoother%20decision%20boundaries.%20We%20evaluate%20SHIELD%20under%20strong%20white-box%20adversarial%20attacks%2C%20including%20PGD%20and%20AutoAttack%2C%20across%20multiple%20benchmarks.%20It%20consistently%20outperforms%20existing%20robust%20continual%20learning%20methods%2C%20achieving%20state-of-the-art%20average%20accuracy%20while%20maintaining%20both%20scalability%20and%20certification.%20These%20results%20represent%20a%20significant%20step%20toward%20practical%20and%20theoretically%20grounded%20continual%20learning%20in%20adversarial%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2506.08255v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSHIELD%253A%2520Secure%2520Hypernetworks%2520for%2520Incremental%2520Expansion%2520Learning%2520Defense%26entry.906535625%3DPatryk%2520Krukowski%2520and%2520%25C5%2581ukasz%2520Gorczyca%2520and%2520Piotr%2520Helm%2520and%2520Kamil%2520Ksi%25C4%2585%25C5%25BCek%2520and%2520Przemys%25C5%2582aw%2520Spurek%26entry.1292438233%3DContinual%2520learning%2520under%2520adversarial%2520conditions%2520remains%2520an%2520open%2520problem%252C%2520as%2520existing%2520methods%2520often%2520compromise%2520either%2520robustness%252C%2520scalability%252C%2520or%2520both.%2520We%2520propose%2520a%2520novel%2520framework%2520that%2520integrates%2520Interval%2520Bound%2520Propagation%2520%2528IBP%2529%2520with%2520a%2520hypernetwork-based%2520architecture%2520to%2520enable%2520certifiably%2520robust%2520continual%2520learning%2520across%2520sequential%2520tasks.%2520Our%2520method%252C%2520SHIELD%252C%2520generates%2520task-specific%2520model%2520parameters%2520via%2520a%2520shared%2520hypernetwork%2520conditioned%2520solely%2520on%2520compact%2520task%2520embeddings%252C%2520eliminating%2520the%2520need%2520for%2520replay%2520buffers%2520or%2520full%2520model%2520copies%2520and%2520enabling%2520efficient%2520over%2520time.%2520To%2520further%2520enhance%2520robustness%252C%2520we%2520introduce%2520Interval%2520MixUp%252C%2520a%2520novel%2520training%2520strategy%2520that%2520blends%2520virtual%2520examples%2520represented%2520as%2520%2524%255Cell_%257B%255Cinfty%257D%2524%2520balls%2520centered%2520around%2520MixUp%2520points.%2520Leveraging%2520interval%2520arithmetic%252C%2520this%2520technique%2520guarantees%2520certified%2520robustness%2520while%2520mitigating%2520the%2520wrapping%2520effect%252C%2520resulting%2520in%2520smoother%2520decision%2520boundaries.%2520We%2520evaluate%2520SHIELD%2520under%2520strong%2520white-box%2520adversarial%2520attacks%252C%2520including%2520PGD%2520and%2520AutoAttack%252C%2520across%2520multiple%2520benchmarks.%2520It%2520consistently%2520outperforms%2520existing%2520robust%2520continual%2520learning%2520methods%252C%2520achieving%2520state-of-the-art%2520average%2520accuracy%2520while%2520maintaining%2520both%2520scalability%2520and%2520certification.%2520These%2520results%2520represent%2520a%2520significant%2520step%2520toward%2520practical%2520and%2520theoretically%2520grounded%2520continual%2520learning%2520in%2520adversarial%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08255v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHIELD%3A%20Secure%20Hypernetworks%20for%20Incremental%20Expansion%20Learning%20Defense&entry.906535625=Patryk%20Krukowski%20and%20%C5%81ukasz%20Gorczyca%20and%20Piotr%20Helm%20and%20Kamil%20Ksi%C4%85%C5%BCek%20and%20Przemys%C5%82aw%20Spurek&entry.1292438233=Continual%20learning%20under%20adversarial%20conditions%20remains%20an%20open%20problem%2C%20as%20existing%20methods%20often%20compromise%20either%20robustness%2C%20scalability%2C%20or%20both.%20We%20propose%20a%20novel%20framework%20that%20integrates%20Interval%20Bound%20Propagation%20%28IBP%29%20with%20a%20hypernetwork-based%20architecture%20to%20enable%20certifiably%20robust%20continual%20learning%20across%20sequential%20tasks.%20Our%20method%2C%20SHIELD%2C%20generates%20task-specific%20model%20parameters%20via%20a%20shared%20hypernetwork%20conditioned%20solely%20on%20compact%20task%20embeddings%2C%20eliminating%20the%20need%20for%20replay%20buffers%20or%20full%20model%20copies%20and%20enabling%20efficient%20over%20time.%20To%20further%20enhance%20robustness%2C%20we%20introduce%20Interval%20MixUp%2C%20a%20novel%20training%20strategy%20that%20blends%20virtual%20examples%20represented%20as%20%24%5Cell_%7B%5Cinfty%7D%24%20balls%20centered%20around%20MixUp%20points.%20Leveraging%20interval%20arithmetic%2C%20this%20technique%20guarantees%20certified%20robustness%20while%20mitigating%20the%20wrapping%20effect%2C%20resulting%20in%20smoother%20decision%20boundaries.%20We%20evaluate%20SHIELD%20under%20strong%20white-box%20adversarial%20attacks%2C%20including%20PGD%20and%20AutoAttack%2C%20across%20multiple%20benchmarks.%20It%20consistently%20outperforms%20existing%20robust%20continual%20learning%20methods%2C%20achieving%20state-of-the-art%20average%20accuracy%20while%20maintaining%20both%20scalability%20and%20certification.%20These%20results%20represent%20a%20significant%20step%20toward%20practical%20and%20theoretically%20grounded%20continual%20learning%20in%20adversarial%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2506.08255v3&entry.124074799=Read"},
{"title": "SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding", "author": "Nikolay Nikolov and Giuliano Albanese and Sombit Dey and Aleksandar Yanev and Luc Van Gool and Jan-Nico Zaech and Danda Pani Paudel", "abstract": "Robotic Foundation Models (RFMs) hold great promise as generalist, end-to-end systems for robot control. Yet their ability to generalize across new environments, tasks, and embodiments remains limited. We argue that a major bottleneck lies in their foundations: most RFMs are built by fine-tuning internet-pretrained Vision-Language Models (VLMs). However, these VLMs are trained on 2D image-language tasks and lack the 3D spatial reasoning inherently required for embodied control in the 3D world. Bridging this gap directly with large-scale robotic data is costly and difficult to scale. Instead, we propose to enrich easy-to-collect non-robotic image data with 3D annotations and enhance a pretrained VLM with 3D understanding capabilities. Following this strategy, we train SPEAR-VLM, a 3D-aware VLM that infers object coordinates in 3D space from a single 2D image. Building on SPEAR-VLM, we introduce our main contribution, $~\\textbf{SPEAR-1}$: a robotic foundation model that integrates grounded 3D perception with language-instructed embodied control. Trained on $\\sim$45M frames from 24 Open X-Embodiment datasets, SPEAR-1 outperforms or matches state-of-the-art models such as $\u03c0_0$-FAST and $\u03c0_{0.5}$, while it uses 20$\\times$ fewer robot demonstrations. This carefully-engineered training strategy unlocks new VLM capabilities and as a consequence boosts the reliability of embodied control beyond what is achievable with only robotic data. We make our model weights and 3D-annotated datasets publicly available.", "link": "http://arxiv.org/abs/2511.17411v1", "date": "2025-11-21", "relevancy": 2.4887, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6334}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6334}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPEAR-1%3A%20Scaling%20Beyond%20Robot%20Demonstrations%20via%203D%20Understanding&body=Title%3A%20SPEAR-1%3A%20Scaling%20Beyond%20Robot%20Demonstrations%20via%203D%20Understanding%0AAuthor%3A%20Nikolay%20Nikolov%20and%20Giuliano%20Albanese%20and%20Sombit%20Dey%20and%20Aleksandar%20Yanev%20and%20Luc%20Van%20Gool%20and%20Jan-Nico%20Zaech%20and%20Danda%20Pani%20Paudel%0AAbstract%3A%20Robotic%20Foundation%20Models%20%28RFMs%29%20hold%20great%20promise%20as%20generalist%2C%20end-to-end%20systems%20for%20robot%20control.%20Yet%20their%20ability%20to%20generalize%20across%20new%20environments%2C%20tasks%2C%20and%20embodiments%20remains%20limited.%20We%20argue%20that%20a%20major%20bottleneck%20lies%20in%20their%20foundations%3A%20most%20RFMs%20are%20built%20by%20fine-tuning%20internet-pretrained%20Vision-Language%20Models%20%28VLMs%29.%20However%2C%20these%20VLMs%20are%20trained%20on%202D%20image-language%20tasks%20and%20lack%20the%203D%20spatial%20reasoning%20inherently%20required%20for%20embodied%20control%20in%20the%203D%20world.%20Bridging%20this%20gap%20directly%20with%20large-scale%20robotic%20data%20is%20costly%20and%20difficult%20to%20scale.%20Instead%2C%20we%20propose%20to%20enrich%20easy-to-collect%20non-robotic%20image%20data%20with%203D%20annotations%20and%20enhance%20a%20pretrained%20VLM%20with%203D%20understanding%20capabilities.%20Following%20this%20strategy%2C%20we%20train%20SPEAR-VLM%2C%20a%203D-aware%20VLM%20that%20infers%20object%20coordinates%20in%203D%20space%20from%20a%20single%202D%20image.%20Building%20on%20SPEAR-VLM%2C%20we%20introduce%20our%20main%20contribution%2C%20%24~%5Ctextbf%7BSPEAR-1%7D%24%3A%20a%20robotic%20foundation%20model%20that%20integrates%20grounded%203D%20perception%20with%20language-instructed%20embodied%20control.%20Trained%20on%20%24%5Csim%2445M%20frames%20from%2024%20Open%20X-Embodiment%20datasets%2C%20SPEAR-1%20outperforms%20or%20matches%20state-of-the-art%20models%20such%20as%20%24%CF%80_0%24-FAST%20and%20%24%CF%80_%7B0.5%7D%24%2C%20while%20it%20uses%2020%24%5Ctimes%24%20fewer%20robot%20demonstrations.%20This%20carefully-engineered%20training%20strategy%20unlocks%20new%20VLM%20capabilities%20and%20as%20a%20consequence%20boosts%20the%20reliability%20of%20embodied%20control%20beyond%20what%20is%20achievable%20with%20only%20robotic%20data.%20We%20make%20our%20model%20weights%20and%203D-annotated%20datasets%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPEAR-1%253A%2520Scaling%2520Beyond%2520Robot%2520Demonstrations%2520via%25203D%2520Understanding%26entry.906535625%3DNikolay%2520Nikolov%2520and%2520Giuliano%2520Albanese%2520and%2520Sombit%2520Dey%2520and%2520Aleksandar%2520Yanev%2520and%2520Luc%2520Van%2520Gool%2520and%2520Jan-Nico%2520Zaech%2520and%2520Danda%2520Pani%2520Paudel%26entry.1292438233%3DRobotic%2520Foundation%2520Models%2520%2528RFMs%2529%2520hold%2520great%2520promise%2520as%2520generalist%252C%2520end-to-end%2520systems%2520for%2520robot%2520control.%2520Yet%2520their%2520ability%2520to%2520generalize%2520across%2520new%2520environments%252C%2520tasks%252C%2520and%2520embodiments%2520remains%2520limited.%2520We%2520argue%2520that%2520a%2520major%2520bottleneck%2520lies%2520in%2520their%2520foundations%253A%2520most%2520RFMs%2520are%2520built%2520by%2520fine-tuning%2520internet-pretrained%2520Vision-Language%2520Models%2520%2528VLMs%2529.%2520However%252C%2520these%2520VLMs%2520are%2520trained%2520on%25202D%2520image-language%2520tasks%2520and%2520lack%2520the%25203D%2520spatial%2520reasoning%2520inherently%2520required%2520for%2520embodied%2520control%2520in%2520the%25203D%2520world.%2520Bridging%2520this%2520gap%2520directly%2520with%2520large-scale%2520robotic%2520data%2520is%2520costly%2520and%2520difficult%2520to%2520scale.%2520Instead%252C%2520we%2520propose%2520to%2520enrich%2520easy-to-collect%2520non-robotic%2520image%2520data%2520with%25203D%2520annotations%2520and%2520enhance%2520a%2520pretrained%2520VLM%2520with%25203D%2520understanding%2520capabilities.%2520Following%2520this%2520strategy%252C%2520we%2520train%2520SPEAR-VLM%252C%2520a%25203D-aware%2520VLM%2520that%2520infers%2520object%2520coordinates%2520in%25203D%2520space%2520from%2520a%2520single%25202D%2520image.%2520Building%2520on%2520SPEAR-VLM%252C%2520we%2520introduce%2520our%2520main%2520contribution%252C%2520%2524~%255Ctextbf%257BSPEAR-1%257D%2524%253A%2520a%2520robotic%2520foundation%2520model%2520that%2520integrates%2520grounded%25203D%2520perception%2520with%2520language-instructed%2520embodied%2520control.%2520Trained%2520on%2520%2524%255Csim%252445M%2520frames%2520from%252024%2520Open%2520X-Embodiment%2520datasets%252C%2520SPEAR-1%2520outperforms%2520or%2520matches%2520state-of-the-art%2520models%2520such%2520as%2520%2524%25CF%2580_0%2524-FAST%2520and%2520%2524%25CF%2580_%257B0.5%257D%2524%252C%2520while%2520it%2520uses%252020%2524%255Ctimes%2524%2520fewer%2520robot%2520demonstrations.%2520This%2520carefully-engineered%2520training%2520strategy%2520unlocks%2520new%2520VLM%2520capabilities%2520and%2520as%2520a%2520consequence%2520boosts%2520the%2520reliability%2520of%2520embodied%2520control%2520beyond%2520what%2520is%2520achievable%2520with%2520only%2520robotic%2520data.%2520We%2520make%2520our%2520model%2520weights%2520and%25203D-annotated%2520datasets%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPEAR-1%3A%20Scaling%20Beyond%20Robot%20Demonstrations%20via%203D%20Understanding&entry.906535625=Nikolay%20Nikolov%20and%20Giuliano%20Albanese%20and%20Sombit%20Dey%20and%20Aleksandar%20Yanev%20and%20Luc%20Van%20Gool%20and%20Jan-Nico%20Zaech%20and%20Danda%20Pani%20Paudel&entry.1292438233=Robotic%20Foundation%20Models%20%28RFMs%29%20hold%20great%20promise%20as%20generalist%2C%20end-to-end%20systems%20for%20robot%20control.%20Yet%20their%20ability%20to%20generalize%20across%20new%20environments%2C%20tasks%2C%20and%20embodiments%20remains%20limited.%20We%20argue%20that%20a%20major%20bottleneck%20lies%20in%20their%20foundations%3A%20most%20RFMs%20are%20built%20by%20fine-tuning%20internet-pretrained%20Vision-Language%20Models%20%28VLMs%29.%20However%2C%20these%20VLMs%20are%20trained%20on%202D%20image-language%20tasks%20and%20lack%20the%203D%20spatial%20reasoning%20inherently%20required%20for%20embodied%20control%20in%20the%203D%20world.%20Bridging%20this%20gap%20directly%20with%20large-scale%20robotic%20data%20is%20costly%20and%20difficult%20to%20scale.%20Instead%2C%20we%20propose%20to%20enrich%20easy-to-collect%20non-robotic%20image%20data%20with%203D%20annotations%20and%20enhance%20a%20pretrained%20VLM%20with%203D%20understanding%20capabilities.%20Following%20this%20strategy%2C%20we%20train%20SPEAR-VLM%2C%20a%203D-aware%20VLM%20that%20infers%20object%20coordinates%20in%203D%20space%20from%20a%20single%202D%20image.%20Building%20on%20SPEAR-VLM%2C%20we%20introduce%20our%20main%20contribution%2C%20%24~%5Ctextbf%7BSPEAR-1%7D%24%3A%20a%20robotic%20foundation%20model%20that%20integrates%20grounded%203D%20perception%20with%20language-instructed%20embodied%20control.%20Trained%20on%20%24%5Csim%2445M%20frames%20from%2024%20Open%20X-Embodiment%20datasets%2C%20SPEAR-1%20outperforms%20or%20matches%20state-of-the-art%20models%20such%20as%20%24%CF%80_0%24-FAST%20and%20%24%CF%80_%7B0.5%7D%24%2C%20while%20it%20uses%2020%24%5Ctimes%24%20fewer%20robot%20demonstrations.%20This%20carefully-engineered%20training%20strategy%20unlocks%20new%20VLM%20capabilities%20and%20as%20a%20consequence%20boosts%20the%20reliability%20of%20embodied%20control%20beyond%20what%20is%20achievable%20with%20only%20robotic%20data.%20We%20make%20our%20model%20weights%20and%203D-annotated%20datasets%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2511.17411v1&entry.124074799=Read"},
{"title": "AutoGraphAD: A novel approach using Variational Graph Autoencoders for anomalous network flow detection", "author": "Georgios Anyfantis and Pere Barlet-Ros", "abstract": "Network Intrusion Detection Systems (NIDS) are essential tools for detecting network attacks and intrusions. While extensive research has explored the use of supervised Machine Learning for attack detection and characterisation, these methods require accurately labelled datasets, which are very costly to obtain. Moreover, existing public datasets have limited and/or outdated attacks, and many of them suffer from mislabelled data. To reduce the reliance on labelled data, we propose AutoGraphAD, a novel unsupervised anomaly detection approach based on a Heterogeneous Variational Graph Autoencoder. AutoGraphAD operates on heterogeneous graphs, made from connection and IP nodes that capture network activity within a time window. The model is trained using unsupervised and contrastive learning, without relying on any labelled data. The reconstruction, structural loss, and KL divergence are then weighted and combined in an anomaly score that is then used for anomaly detection. Overall, AutoGraphAD yields the same, and in some cases better, results than previous unsupervised approaches, such as Anomal-E, but without requiring costly downstream anomaly detectors. As a result, AutoGraphAD achieves around 1.18 orders of magnitude faster training and 1.03 orders of magnitude faster inference, which represents a significant advantage for operational deployment.", "link": "http://arxiv.org/abs/2511.17113v1", "date": "2025-11-21", "relevancy": 2.4849, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5351}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4873}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoGraphAD%3A%20A%20novel%20approach%20using%20Variational%20Graph%20Autoencoders%20for%20anomalous%20network%20flow%20detection&body=Title%3A%20AutoGraphAD%3A%20A%20novel%20approach%20using%20Variational%20Graph%20Autoencoders%20for%20anomalous%20network%20flow%20detection%0AAuthor%3A%20Georgios%20Anyfantis%20and%20Pere%20Barlet-Ros%0AAbstract%3A%20Network%20Intrusion%20Detection%20Systems%20%28NIDS%29%20are%20essential%20tools%20for%20detecting%20network%20attacks%20and%20intrusions.%20While%20extensive%20research%20has%20explored%20the%20use%20of%20supervised%20Machine%20Learning%20for%20attack%20detection%20and%20characterisation%2C%20these%20methods%20require%20accurately%20labelled%20datasets%2C%20which%20are%20very%20costly%20to%20obtain.%20Moreover%2C%20existing%20public%20datasets%20have%20limited%20and/or%20outdated%20attacks%2C%20and%20many%20of%20them%20suffer%20from%20mislabelled%20data.%20To%20reduce%20the%20reliance%20on%20labelled%20data%2C%20we%20propose%20AutoGraphAD%2C%20a%20novel%20unsupervised%20anomaly%20detection%20approach%20based%20on%20a%20Heterogeneous%20Variational%20Graph%20Autoencoder.%20AutoGraphAD%20operates%20on%20heterogeneous%20graphs%2C%20made%20from%20connection%20and%20IP%20nodes%20that%20capture%20network%20activity%20within%20a%20time%20window.%20The%20model%20is%20trained%20using%20unsupervised%20and%20contrastive%20learning%2C%20without%20relying%20on%20any%20labelled%20data.%20The%20reconstruction%2C%20structural%20loss%2C%20and%20KL%20divergence%20are%20then%20weighted%20and%20combined%20in%20an%20anomaly%20score%20that%20is%20then%20used%20for%20anomaly%20detection.%20Overall%2C%20AutoGraphAD%20yields%20the%20same%2C%20and%20in%20some%20cases%20better%2C%20results%20than%20previous%20unsupervised%20approaches%2C%20such%20as%20Anomal-E%2C%20but%20without%20requiring%20costly%20downstream%20anomaly%20detectors.%20As%20a%20result%2C%20AutoGraphAD%20achieves%20around%201.18%20orders%20of%20magnitude%20faster%20training%20and%201.03%20orders%20of%20magnitude%20faster%20inference%2C%20which%20represents%20a%20significant%20advantage%20for%20operational%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoGraphAD%253A%2520A%2520novel%2520approach%2520using%2520Variational%2520Graph%2520Autoencoders%2520for%2520anomalous%2520network%2520flow%2520detection%26entry.906535625%3DGeorgios%2520Anyfantis%2520and%2520Pere%2520Barlet-Ros%26entry.1292438233%3DNetwork%2520Intrusion%2520Detection%2520Systems%2520%2528NIDS%2529%2520are%2520essential%2520tools%2520for%2520detecting%2520network%2520attacks%2520and%2520intrusions.%2520While%2520extensive%2520research%2520has%2520explored%2520the%2520use%2520of%2520supervised%2520Machine%2520Learning%2520for%2520attack%2520detection%2520and%2520characterisation%252C%2520these%2520methods%2520require%2520accurately%2520labelled%2520datasets%252C%2520which%2520are%2520very%2520costly%2520to%2520obtain.%2520Moreover%252C%2520existing%2520public%2520datasets%2520have%2520limited%2520and/or%2520outdated%2520attacks%252C%2520and%2520many%2520of%2520them%2520suffer%2520from%2520mislabelled%2520data.%2520To%2520reduce%2520the%2520reliance%2520on%2520labelled%2520data%252C%2520we%2520propose%2520AutoGraphAD%252C%2520a%2520novel%2520unsupervised%2520anomaly%2520detection%2520approach%2520based%2520on%2520a%2520Heterogeneous%2520Variational%2520Graph%2520Autoencoder.%2520AutoGraphAD%2520operates%2520on%2520heterogeneous%2520graphs%252C%2520made%2520from%2520connection%2520and%2520IP%2520nodes%2520that%2520capture%2520network%2520activity%2520within%2520a%2520time%2520window.%2520The%2520model%2520is%2520trained%2520using%2520unsupervised%2520and%2520contrastive%2520learning%252C%2520without%2520relying%2520on%2520any%2520labelled%2520data.%2520The%2520reconstruction%252C%2520structural%2520loss%252C%2520and%2520KL%2520divergence%2520are%2520then%2520weighted%2520and%2520combined%2520in%2520an%2520anomaly%2520score%2520that%2520is%2520then%2520used%2520for%2520anomaly%2520detection.%2520Overall%252C%2520AutoGraphAD%2520yields%2520the%2520same%252C%2520and%2520in%2520some%2520cases%2520better%252C%2520results%2520than%2520previous%2520unsupervised%2520approaches%252C%2520such%2520as%2520Anomal-E%252C%2520but%2520without%2520requiring%2520costly%2520downstream%2520anomaly%2520detectors.%2520As%2520a%2520result%252C%2520AutoGraphAD%2520achieves%2520around%25201.18%2520orders%2520of%2520magnitude%2520faster%2520training%2520and%25201.03%2520orders%2520of%2520magnitude%2520faster%2520inference%252C%2520which%2520represents%2520a%2520significant%2520advantage%2520for%2520operational%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoGraphAD%3A%20A%20novel%20approach%20using%20Variational%20Graph%20Autoencoders%20for%20anomalous%20network%20flow%20detection&entry.906535625=Georgios%20Anyfantis%20and%20Pere%20Barlet-Ros&entry.1292438233=Network%20Intrusion%20Detection%20Systems%20%28NIDS%29%20are%20essential%20tools%20for%20detecting%20network%20attacks%20and%20intrusions.%20While%20extensive%20research%20has%20explored%20the%20use%20of%20supervised%20Machine%20Learning%20for%20attack%20detection%20and%20characterisation%2C%20these%20methods%20require%20accurately%20labelled%20datasets%2C%20which%20are%20very%20costly%20to%20obtain.%20Moreover%2C%20existing%20public%20datasets%20have%20limited%20and/or%20outdated%20attacks%2C%20and%20many%20of%20them%20suffer%20from%20mislabelled%20data.%20To%20reduce%20the%20reliance%20on%20labelled%20data%2C%20we%20propose%20AutoGraphAD%2C%20a%20novel%20unsupervised%20anomaly%20detection%20approach%20based%20on%20a%20Heterogeneous%20Variational%20Graph%20Autoencoder.%20AutoGraphAD%20operates%20on%20heterogeneous%20graphs%2C%20made%20from%20connection%20and%20IP%20nodes%20that%20capture%20network%20activity%20within%20a%20time%20window.%20The%20model%20is%20trained%20using%20unsupervised%20and%20contrastive%20learning%2C%20without%20relying%20on%20any%20labelled%20data.%20The%20reconstruction%2C%20structural%20loss%2C%20and%20KL%20divergence%20are%20then%20weighted%20and%20combined%20in%20an%20anomaly%20score%20that%20is%20then%20used%20for%20anomaly%20detection.%20Overall%2C%20AutoGraphAD%20yields%20the%20same%2C%20and%20in%20some%20cases%20better%2C%20results%20than%20previous%20unsupervised%20approaches%2C%20such%20as%20Anomal-E%2C%20but%20without%20requiring%20costly%20downstream%20anomaly%20detectors.%20As%20a%20result%2C%20AutoGraphAD%20achieves%20around%201.18%20orders%20of%20magnitude%20faster%20training%20and%201.03%20orders%20of%20magnitude%20faster%20inference%2C%20which%20represents%20a%20significant%20advantage%20for%20operational%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2511.17113v1&entry.124074799=Read"},
{"title": "Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment Formats", "author": "Jiaye Qian and Ge Zheng and Yuchen Zhu and Sibei Yang", "abstract": "Despite their impressive performance across a wide range of tasks, Large Vision-Language Models (LVLMs) remain prone to hallucination. In this study, we propose a comprehensive intervention framework aligned with the transformer's causal architecture in LVLMs, integrating the effects of different intervention paths on hallucination. We find that hallucinations in LVLMs do not arise from a single causal path, but rather from the interplay among image-to-input-text, image-to-output-text, and text-to-text pathways. For the first time, we also find that LVLMs rely on different pathways depending on the question-answer alignment format. Building on these insights, we propose simple yet effective methods to identify and intervene on critical hallucination heads within each pathway, tailored to discriminative and generative formats. Experiments across multiple benchmarks demonstrate that our approach consistently reduces hallucinations across diverse alignment types.", "link": "http://arxiv.org/abs/2511.17254v1", "date": "2025-11-21", "relevancy": 2.4821, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5122}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4885}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intervene-All-Paths%3A%20Unified%20Mitigation%20of%20LVLM%20Hallucinations%20across%20Alignment%20Formats&body=Title%3A%20Intervene-All-Paths%3A%20Unified%20Mitigation%20of%20LVLM%20Hallucinations%20across%20Alignment%20Formats%0AAuthor%3A%20Jiaye%20Qian%20and%20Ge%20Zheng%20and%20Yuchen%20Zhu%20and%20Sibei%20Yang%0AAbstract%3A%20Despite%20their%20impressive%20performance%20across%20a%20wide%20range%20of%20tasks%2C%20Large%20Vision-Language%20Models%20%28LVLMs%29%20remain%20prone%20to%20hallucination.%20In%20this%20study%2C%20we%20propose%20a%20comprehensive%20intervention%20framework%20aligned%20with%20the%20transformer%27s%20causal%20architecture%20in%20LVLMs%2C%20integrating%20the%20effects%20of%20different%20intervention%20paths%20on%20hallucination.%20We%20find%20that%20hallucinations%20in%20LVLMs%20do%20not%20arise%20from%20a%20single%20causal%20path%2C%20but%20rather%20from%20the%20interplay%20among%20image-to-input-text%2C%20image-to-output-text%2C%20and%20text-to-text%20pathways.%20For%20the%20first%20time%2C%20we%20also%20find%20that%20LVLMs%20rely%20on%20different%20pathways%20depending%20on%20the%20question-answer%20alignment%20format.%20Building%20on%20these%20insights%2C%20we%20propose%20simple%20yet%20effective%20methods%20to%20identify%20and%20intervene%20on%20critical%20hallucination%20heads%20within%20each%20pathway%2C%20tailored%20to%20discriminative%20and%20generative%20formats.%20Experiments%20across%20multiple%20benchmarks%20demonstrate%20that%20our%20approach%20consistently%20reduces%20hallucinations%20across%20diverse%20alignment%20types.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17254v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntervene-All-Paths%253A%2520Unified%2520Mitigation%2520of%2520LVLM%2520Hallucinations%2520across%2520Alignment%2520Formats%26entry.906535625%3DJiaye%2520Qian%2520and%2520Ge%2520Zheng%2520and%2520Yuchen%2520Zhu%2520and%2520Sibei%2520Yang%26entry.1292438233%3DDespite%2520their%2520impressive%2520performance%2520across%2520a%2520wide%2520range%2520of%2520tasks%252C%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520remain%2520prone%2520to%2520hallucination.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520comprehensive%2520intervention%2520framework%2520aligned%2520with%2520the%2520transformer%2527s%2520causal%2520architecture%2520in%2520LVLMs%252C%2520integrating%2520the%2520effects%2520of%2520different%2520intervention%2520paths%2520on%2520hallucination.%2520We%2520find%2520that%2520hallucinations%2520in%2520LVLMs%2520do%2520not%2520arise%2520from%2520a%2520single%2520causal%2520path%252C%2520but%2520rather%2520from%2520the%2520interplay%2520among%2520image-to-input-text%252C%2520image-to-output-text%252C%2520and%2520text-to-text%2520pathways.%2520For%2520the%2520first%2520time%252C%2520we%2520also%2520find%2520that%2520LVLMs%2520rely%2520on%2520different%2520pathways%2520depending%2520on%2520the%2520question-answer%2520alignment%2520format.%2520Building%2520on%2520these%2520insights%252C%2520we%2520propose%2520simple%2520yet%2520effective%2520methods%2520to%2520identify%2520and%2520intervene%2520on%2520critical%2520hallucination%2520heads%2520within%2520each%2520pathway%252C%2520tailored%2520to%2520discriminative%2520and%2520generative%2520formats.%2520Experiments%2520across%2520multiple%2520benchmarks%2520demonstrate%2520that%2520our%2520approach%2520consistently%2520reduces%2520hallucinations%2520across%2520diverse%2520alignment%2520types.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17254v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intervene-All-Paths%3A%20Unified%20Mitigation%20of%20LVLM%20Hallucinations%20across%20Alignment%20Formats&entry.906535625=Jiaye%20Qian%20and%20Ge%20Zheng%20and%20Yuchen%20Zhu%20and%20Sibei%20Yang&entry.1292438233=Despite%20their%20impressive%20performance%20across%20a%20wide%20range%20of%20tasks%2C%20Large%20Vision-Language%20Models%20%28LVLMs%29%20remain%20prone%20to%20hallucination.%20In%20this%20study%2C%20we%20propose%20a%20comprehensive%20intervention%20framework%20aligned%20with%20the%20transformer%27s%20causal%20architecture%20in%20LVLMs%2C%20integrating%20the%20effects%20of%20different%20intervention%20paths%20on%20hallucination.%20We%20find%20that%20hallucinations%20in%20LVLMs%20do%20not%20arise%20from%20a%20single%20causal%20path%2C%20but%20rather%20from%20the%20interplay%20among%20image-to-input-text%2C%20image-to-output-text%2C%20and%20text-to-text%20pathways.%20For%20the%20first%20time%2C%20we%20also%20find%20that%20LVLMs%20rely%20on%20different%20pathways%20depending%20on%20the%20question-answer%20alignment%20format.%20Building%20on%20these%20insights%2C%20we%20propose%20simple%20yet%20effective%20methods%20to%20identify%20and%20intervene%20on%20critical%20hallucination%20heads%20within%20each%20pathway%2C%20tailored%20to%20discriminative%20and%20generative%20formats.%20Experiments%20across%20multiple%20benchmarks%20demonstrate%20that%20our%20approach%20consistently%20reduces%20hallucinations%20across%20diverse%20alignment%20types.&entry.1838667208=http%3A//arxiv.org/abs/2511.17254v1&entry.124074799=Read"},
{"title": "SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation", "author": "Shrikant Kendre and Austin Xu and Honglu Zhou and Michael Ryoo and Shafiq Joty and Juan Carlos Niebles", "abstract": "Traditional evaluation metrics for textual and visual question answering, like ROUGE, METEOR, and Exact Match (EM), focus heavily on n-gram based lexical similarity, often missing the deeper semantic understanding needed for accurate assessment. While measures like BERTScore and MoverScore leverage contextual embeddings to address this limitation, they lack flexibility in balancing sentence-level and keyword-level semantics and ignore lexical similarity, which remains important. Large Language Model (LLM) based evaluators, though powerful, come with drawbacks like high costs, bias, inconsistency, and hallucinations. To address these issues, we introduce SMILE: Semantic Metric Integrating Lexical Exactness, a novel approach that combines sentence-level semantic understanding with keyword-level semantic understanding and easy keyword matching. This composite method balances lexical precision and semantic relevance, offering a comprehensive evaluation. Extensive benchmarks across text, image, and video QA tasks show SMILE is highly correlated with human judgments and computationally lightweight, bridging the gap between lexical and semantic evaluation.", "link": "http://arxiv.org/abs/2511.17432v1", "date": "2025-11-21", "relevancy": 2.4645, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5016}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5016}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMILE%3A%20A%20Composite%20Lexical-Semantic%20Metric%20for%20Question-Answering%20Evaluation&body=Title%3A%20SMILE%3A%20A%20Composite%20Lexical-Semantic%20Metric%20for%20Question-Answering%20Evaluation%0AAuthor%3A%20Shrikant%20Kendre%20and%20Austin%20Xu%20and%20Honglu%20Zhou%20and%20Michael%20Ryoo%20and%20Shafiq%20Joty%20and%20Juan%20Carlos%20Niebles%0AAbstract%3A%20Traditional%20evaluation%20metrics%20for%20textual%20and%20visual%20question%20answering%2C%20like%20ROUGE%2C%20METEOR%2C%20and%20Exact%20Match%20%28EM%29%2C%20focus%20heavily%20on%20n-gram%20based%20lexical%20similarity%2C%20often%20missing%20the%20deeper%20semantic%20understanding%20needed%20for%20accurate%20assessment.%20While%20measures%20like%20BERTScore%20and%20MoverScore%20leverage%20contextual%20embeddings%20to%20address%20this%20limitation%2C%20they%20lack%20flexibility%20in%20balancing%20sentence-level%20and%20keyword-level%20semantics%20and%20ignore%20lexical%20similarity%2C%20which%20remains%20important.%20Large%20Language%20Model%20%28LLM%29%20based%20evaluators%2C%20though%20powerful%2C%20come%20with%20drawbacks%20like%20high%20costs%2C%20bias%2C%20inconsistency%2C%20and%20hallucinations.%20To%20address%20these%20issues%2C%20we%20introduce%20SMILE%3A%20Semantic%20Metric%20Integrating%20Lexical%20Exactness%2C%20a%20novel%20approach%20that%20combines%20sentence-level%20semantic%20understanding%20with%20keyword-level%20semantic%20understanding%20and%20easy%20keyword%20matching.%20This%20composite%20method%20balances%20lexical%20precision%20and%20semantic%20relevance%2C%20offering%20a%20comprehensive%20evaluation.%20Extensive%20benchmarks%20across%20text%2C%20image%2C%20and%20video%20QA%20tasks%20show%20SMILE%20is%20highly%20correlated%20with%20human%20judgments%20and%20computationally%20lightweight%2C%20bridging%20the%20gap%20between%20lexical%20and%20semantic%20evaluation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMILE%253A%2520A%2520Composite%2520Lexical-Semantic%2520Metric%2520for%2520Question-Answering%2520Evaluation%26entry.906535625%3DShrikant%2520Kendre%2520and%2520Austin%2520Xu%2520and%2520Honglu%2520Zhou%2520and%2520Michael%2520Ryoo%2520and%2520Shafiq%2520Joty%2520and%2520Juan%2520Carlos%2520Niebles%26entry.1292438233%3DTraditional%2520evaluation%2520metrics%2520for%2520textual%2520and%2520visual%2520question%2520answering%252C%2520like%2520ROUGE%252C%2520METEOR%252C%2520and%2520Exact%2520Match%2520%2528EM%2529%252C%2520focus%2520heavily%2520on%2520n-gram%2520based%2520lexical%2520similarity%252C%2520often%2520missing%2520the%2520deeper%2520semantic%2520understanding%2520needed%2520for%2520accurate%2520assessment.%2520While%2520measures%2520like%2520BERTScore%2520and%2520MoverScore%2520leverage%2520contextual%2520embeddings%2520to%2520address%2520this%2520limitation%252C%2520they%2520lack%2520flexibility%2520in%2520balancing%2520sentence-level%2520and%2520keyword-level%2520semantics%2520and%2520ignore%2520lexical%2520similarity%252C%2520which%2520remains%2520important.%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520based%2520evaluators%252C%2520though%2520powerful%252C%2520come%2520with%2520drawbacks%2520like%2520high%2520costs%252C%2520bias%252C%2520inconsistency%252C%2520and%2520hallucinations.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520SMILE%253A%2520Semantic%2520Metric%2520Integrating%2520Lexical%2520Exactness%252C%2520a%2520novel%2520approach%2520that%2520combines%2520sentence-level%2520semantic%2520understanding%2520with%2520keyword-level%2520semantic%2520understanding%2520and%2520easy%2520keyword%2520matching.%2520This%2520composite%2520method%2520balances%2520lexical%2520precision%2520and%2520semantic%2520relevance%252C%2520offering%2520a%2520comprehensive%2520evaluation.%2520Extensive%2520benchmarks%2520across%2520text%252C%2520image%252C%2520and%2520video%2520QA%2520tasks%2520show%2520SMILE%2520is%2520highly%2520correlated%2520with%2520human%2520judgments%2520and%2520computationally%2520lightweight%252C%2520bridging%2520the%2520gap%2520between%2520lexical%2520and%2520semantic%2520evaluation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMILE%3A%20A%20Composite%20Lexical-Semantic%20Metric%20for%20Question-Answering%20Evaluation&entry.906535625=Shrikant%20Kendre%20and%20Austin%20Xu%20and%20Honglu%20Zhou%20and%20Michael%20Ryoo%20and%20Shafiq%20Joty%20and%20Juan%20Carlos%20Niebles&entry.1292438233=Traditional%20evaluation%20metrics%20for%20textual%20and%20visual%20question%20answering%2C%20like%20ROUGE%2C%20METEOR%2C%20and%20Exact%20Match%20%28EM%29%2C%20focus%20heavily%20on%20n-gram%20based%20lexical%20similarity%2C%20often%20missing%20the%20deeper%20semantic%20understanding%20needed%20for%20accurate%20assessment.%20While%20measures%20like%20BERTScore%20and%20MoverScore%20leverage%20contextual%20embeddings%20to%20address%20this%20limitation%2C%20they%20lack%20flexibility%20in%20balancing%20sentence-level%20and%20keyword-level%20semantics%20and%20ignore%20lexical%20similarity%2C%20which%20remains%20important.%20Large%20Language%20Model%20%28LLM%29%20based%20evaluators%2C%20though%20powerful%2C%20come%20with%20drawbacks%20like%20high%20costs%2C%20bias%2C%20inconsistency%2C%20and%20hallucinations.%20To%20address%20these%20issues%2C%20we%20introduce%20SMILE%3A%20Semantic%20Metric%20Integrating%20Lexical%20Exactness%2C%20a%20novel%20approach%20that%20combines%20sentence-level%20semantic%20understanding%20with%20keyword-level%20semantic%20understanding%20and%20easy%20keyword%20matching.%20This%20composite%20method%20balances%20lexical%20precision%20and%20semantic%20relevance%2C%20offering%20a%20comprehensive%20evaluation.%20Extensive%20benchmarks%20across%20text%2C%20image%2C%20and%20video%20QA%20tasks%20show%20SMILE%20is%20highly%20correlated%20with%20human%20judgments%20and%20computationally%20lightweight%2C%20bridging%20the%20gap%20between%20lexical%20and%20semantic%20evaluation.&entry.1838667208=http%3A//arxiv.org/abs/2511.17432v1&entry.124074799=Read"},
{"title": "LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation", "author": "Yushi Huang and Xingtong Ge and Ruihao Gong and Chengtao Lv and Jun Zhang", "abstract": "Video diffusion models (DMs) have enabled high-quality video synthesis. However, their computation costs scale quadratically with sequence length because self-attention has quadratic complexity. While linear attention lowers the cost, fully replacing quadratic attention requires expensive pretraining due to the limited expressiveness of linear attention and the complexity of spatiotemporal modeling in video generation. In this paper, we present LinVideo, an efficient data-free post-training framework that replaces a target number of self-attention modules with linear attention while preserving the original model's performance. First, we observe a significant disparity in the replaceability of different layers. Instead of manual or heuristic choices, we frame layer selection as a binary classification problem and propose selective transfer, which automatically and progressively converts layers to linear attention with minimal performance impact. Additionally, to overcome the ineffectiveness and inefficiency of existing objectives for this transfer process, we introduce an anytime distribution matching (ADM) objective that aligns the distributions of samples across any timestep along the sampling trajectory. This objective is efficient and recovers model performance. Extensive experiments show that our method achieves a 1.25-2.00x speedup while preserving generation quality, and our 4-step distilled model further delivers a 15.92x latency reduction with minimal visual quality drop.", "link": "http://arxiv.org/abs/2510.08318v2", "date": "2025-11-21", "relevancy": 2.4482, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6346}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.614}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LinVideo%3A%20A%20Post-Training%20Framework%20towards%20O%28n%29%20Attention%20in%20Efficient%20Video%20Generation&body=Title%3A%20LinVideo%3A%20A%20Post-Training%20Framework%20towards%20O%28n%29%20Attention%20in%20Efficient%20Video%20Generation%0AAuthor%3A%20Yushi%20Huang%20and%20Xingtong%20Ge%20and%20Ruihao%20Gong%20and%20Chengtao%20Lv%20and%20Jun%20Zhang%0AAbstract%3A%20Video%20diffusion%20models%20%28DMs%29%20have%20enabled%20high-quality%20video%20synthesis.%20However%2C%20their%20computation%20costs%20scale%20quadratically%20with%20sequence%20length%20because%20self-attention%20has%20quadratic%20complexity.%20While%20linear%20attention%20lowers%20the%20cost%2C%20fully%20replacing%20quadratic%20attention%20requires%20expensive%20pretraining%20due%20to%20the%20limited%20expressiveness%20of%20linear%20attention%20and%20the%20complexity%20of%20spatiotemporal%20modeling%20in%20video%20generation.%20In%20this%20paper%2C%20we%20present%20LinVideo%2C%20an%20efficient%20data-free%20post-training%20framework%20that%20replaces%20a%20target%20number%20of%20self-attention%20modules%20with%20linear%20attention%20while%20preserving%20the%20original%20model%27s%20performance.%20First%2C%20we%20observe%20a%20significant%20disparity%20in%20the%20replaceability%20of%20different%20layers.%20Instead%20of%20manual%20or%20heuristic%20choices%2C%20we%20frame%20layer%20selection%20as%20a%20binary%20classification%20problem%20and%20propose%20selective%20transfer%2C%20which%20automatically%20and%20progressively%20converts%20layers%20to%20linear%20attention%20with%20minimal%20performance%20impact.%20Additionally%2C%20to%20overcome%20the%20ineffectiveness%20and%20inefficiency%20of%20existing%20objectives%20for%20this%20transfer%20process%2C%20we%20introduce%20an%20anytime%20distribution%20matching%20%28ADM%29%20objective%20that%20aligns%20the%20distributions%20of%20samples%20across%20any%20timestep%20along%20the%20sampling%20trajectory.%20This%20objective%20is%20efficient%20and%20recovers%20model%20performance.%20Extensive%20experiments%20show%20that%20our%20method%20achieves%20a%201.25-2.00x%20speedup%20while%20preserving%20generation%20quality%2C%20and%20our%204-step%20distilled%20model%20further%20delivers%20a%2015.92x%20latency%20reduction%20with%20minimal%20visual%20quality%20drop.%0ALink%3A%20http%3A//arxiv.org/abs/2510.08318v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinVideo%253A%2520A%2520Post-Training%2520Framework%2520towards%2520O%2528n%2529%2520Attention%2520in%2520Efficient%2520Video%2520Generation%26entry.906535625%3DYushi%2520Huang%2520and%2520Xingtong%2520Ge%2520and%2520Ruihao%2520Gong%2520and%2520Chengtao%2520Lv%2520and%2520Jun%2520Zhang%26entry.1292438233%3DVideo%2520diffusion%2520models%2520%2528DMs%2529%2520have%2520enabled%2520high-quality%2520video%2520synthesis.%2520However%252C%2520their%2520computation%2520costs%2520scale%2520quadratically%2520with%2520sequence%2520length%2520because%2520self-attention%2520has%2520quadratic%2520complexity.%2520While%2520linear%2520attention%2520lowers%2520the%2520cost%252C%2520fully%2520replacing%2520quadratic%2520attention%2520requires%2520expensive%2520pretraining%2520due%2520to%2520the%2520limited%2520expressiveness%2520of%2520linear%2520attention%2520and%2520the%2520complexity%2520of%2520spatiotemporal%2520modeling%2520in%2520video%2520generation.%2520In%2520this%2520paper%252C%2520we%2520present%2520LinVideo%252C%2520an%2520efficient%2520data-free%2520post-training%2520framework%2520that%2520replaces%2520a%2520target%2520number%2520of%2520self-attention%2520modules%2520with%2520linear%2520attention%2520while%2520preserving%2520the%2520original%2520model%2527s%2520performance.%2520First%252C%2520we%2520observe%2520a%2520significant%2520disparity%2520in%2520the%2520replaceability%2520of%2520different%2520layers.%2520Instead%2520of%2520manual%2520or%2520heuristic%2520choices%252C%2520we%2520frame%2520layer%2520selection%2520as%2520a%2520binary%2520classification%2520problem%2520and%2520propose%2520selective%2520transfer%252C%2520which%2520automatically%2520and%2520progressively%2520converts%2520layers%2520to%2520linear%2520attention%2520with%2520minimal%2520performance%2520impact.%2520Additionally%252C%2520to%2520overcome%2520the%2520ineffectiveness%2520and%2520inefficiency%2520of%2520existing%2520objectives%2520for%2520this%2520transfer%2520process%252C%2520we%2520introduce%2520an%2520anytime%2520distribution%2520matching%2520%2528ADM%2529%2520objective%2520that%2520aligns%2520the%2520distributions%2520of%2520samples%2520across%2520any%2520timestep%2520along%2520the%2520sampling%2520trajectory.%2520This%2520objective%2520is%2520efficient%2520and%2520recovers%2520model%2520performance.%2520Extensive%2520experiments%2520show%2520that%2520our%2520method%2520achieves%2520a%25201.25-2.00x%2520speedup%2520while%2520preserving%2520generation%2520quality%252C%2520and%2520our%25204-step%2520distilled%2520model%2520further%2520delivers%2520a%252015.92x%2520latency%2520reduction%2520with%2520minimal%2520visual%2520quality%2520drop.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08318v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LinVideo%3A%20A%20Post-Training%20Framework%20towards%20O%28n%29%20Attention%20in%20Efficient%20Video%20Generation&entry.906535625=Yushi%20Huang%20and%20Xingtong%20Ge%20and%20Ruihao%20Gong%20and%20Chengtao%20Lv%20and%20Jun%20Zhang&entry.1292438233=Video%20diffusion%20models%20%28DMs%29%20have%20enabled%20high-quality%20video%20synthesis.%20However%2C%20their%20computation%20costs%20scale%20quadratically%20with%20sequence%20length%20because%20self-attention%20has%20quadratic%20complexity.%20While%20linear%20attention%20lowers%20the%20cost%2C%20fully%20replacing%20quadratic%20attention%20requires%20expensive%20pretraining%20due%20to%20the%20limited%20expressiveness%20of%20linear%20attention%20and%20the%20complexity%20of%20spatiotemporal%20modeling%20in%20video%20generation.%20In%20this%20paper%2C%20we%20present%20LinVideo%2C%20an%20efficient%20data-free%20post-training%20framework%20that%20replaces%20a%20target%20number%20of%20self-attention%20modules%20with%20linear%20attention%20while%20preserving%20the%20original%20model%27s%20performance.%20First%2C%20we%20observe%20a%20significant%20disparity%20in%20the%20replaceability%20of%20different%20layers.%20Instead%20of%20manual%20or%20heuristic%20choices%2C%20we%20frame%20layer%20selection%20as%20a%20binary%20classification%20problem%20and%20propose%20selective%20transfer%2C%20which%20automatically%20and%20progressively%20converts%20layers%20to%20linear%20attention%20with%20minimal%20performance%20impact.%20Additionally%2C%20to%20overcome%20the%20ineffectiveness%20and%20inefficiency%20of%20existing%20objectives%20for%20this%20transfer%20process%2C%20we%20introduce%20an%20anytime%20distribution%20matching%20%28ADM%29%20objective%20that%20aligns%20the%20distributions%20of%20samples%20across%20any%20timestep%20along%20the%20sampling%20trajectory.%20This%20objective%20is%20efficient%20and%20recovers%20model%20performance.%20Extensive%20experiments%20show%20that%20our%20method%20achieves%20a%201.25-2.00x%20speedup%20while%20preserving%20generation%20quality%2C%20and%20our%204-step%20distilled%20model%20further%20delivers%20a%2015.92x%20latency%20reduction%20with%20minimal%20visual%20quality%20drop.&entry.1838667208=http%3A//arxiv.org/abs/2510.08318v2&entry.124074799=Read"},
{"title": "TRACE: Time SeRies PArameter EffiCient FinE-tuning", "author": "Yuze Li and Wei Zhu", "abstract": "We propose an efficient fine-tuning method for time series foundation models, termed TRACE: Time Series Parameter Efficient Fine-tuning. While pretrained time series foundation models are gaining popularity, they face the following challenges: (1) Unlike natural language tasks, time series data vary in frequency, channel numbers, historical/prediction lengths. For long-term forecasting tasks in particular, tailored fine-tuning can significantly enhance performance.(2) Existing parameter-efficient tuning methods like LoRA remain applicable but require adaptation to temporal characteristics.\n  To address these challenges, our TRACE framework introduces two key innovations: (1) Gated DSIC (Gated Dynamic Simulation Importance Calculation), an unbiased LoRA module importance selection mechanism that ensures conditional parameter consistency before and after masking. Experiments demonstrate that Gated DSIC outperforms common fine-tuning. (2) Reconstructed prediction heads for long-term forecasting tasks, which achieve comparable or superior performance to linear probing heads while drastically reducing parameter counts.\n  Extensive experiments on long-/short-term forecasting, anomaly detection and natural language tasks across diverse datasets, coupled with ablation studies, validate the effectiveness of our method.", "link": "http://arxiv.org/abs/2503.16991v3", "date": "2025-11-21", "relevancy": 2.4237, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4848}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4848}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRACE%3A%20Time%20SeRies%20PArameter%20EffiCient%20FinE-tuning&body=Title%3A%20TRACE%3A%20Time%20SeRies%20PArameter%20EffiCient%20FinE-tuning%0AAuthor%3A%20Yuze%20Li%20and%20Wei%20Zhu%0AAbstract%3A%20We%20propose%20an%20efficient%20fine-tuning%20method%20for%20time%20series%20foundation%20models%2C%20termed%20TRACE%3A%20Time%20Series%20Parameter%20Efficient%20Fine-tuning.%20While%20pretrained%20time%20series%20foundation%20models%20are%20gaining%20popularity%2C%20they%20face%20the%20following%20challenges%3A%20%281%29%20Unlike%20natural%20language%20tasks%2C%20time%20series%20data%20vary%20in%20frequency%2C%20channel%20numbers%2C%20historical/prediction%20lengths.%20For%20long-term%20forecasting%20tasks%20in%20particular%2C%20tailored%20fine-tuning%20can%20significantly%20enhance%20performance.%282%29%20Existing%20parameter-efficient%20tuning%20methods%20like%20LoRA%20remain%20applicable%20but%20require%20adaptation%20to%20temporal%20characteristics.%0A%20%20To%20address%20these%20challenges%2C%20our%20TRACE%20framework%20introduces%20two%20key%20innovations%3A%20%281%29%20Gated%20DSIC%20%28Gated%20Dynamic%20Simulation%20Importance%20Calculation%29%2C%20an%20unbiased%20LoRA%20module%20importance%20selection%20mechanism%20that%20ensures%20conditional%20parameter%20consistency%20before%20and%20after%20masking.%20Experiments%20demonstrate%20that%20Gated%20DSIC%20outperforms%20common%20fine-tuning.%20%282%29%20Reconstructed%20prediction%20heads%20for%20long-term%20forecasting%20tasks%2C%20which%20achieve%20comparable%20or%20superior%20performance%20to%20linear%20probing%20heads%20while%20drastically%20reducing%20parameter%20counts.%0A%20%20Extensive%20experiments%20on%20long-/short-term%20forecasting%2C%20anomaly%20detection%20and%20natural%20language%20tasks%20across%20diverse%20datasets%2C%20coupled%20with%20ablation%20studies%2C%20validate%20the%20effectiveness%20of%20our%20method.%0ALink%3A%20http%3A//arxiv.org/abs/2503.16991v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRACE%253A%2520Time%2520SeRies%2520PArameter%2520EffiCient%2520FinE-tuning%26entry.906535625%3DYuze%2520Li%2520and%2520Wei%2520Zhu%26entry.1292438233%3DWe%2520propose%2520an%2520efficient%2520fine-tuning%2520method%2520for%2520time%2520series%2520foundation%2520models%252C%2520termed%2520TRACE%253A%2520Time%2520Series%2520Parameter%2520Efficient%2520Fine-tuning.%2520While%2520pretrained%2520time%2520series%2520foundation%2520models%2520are%2520gaining%2520popularity%252C%2520they%2520face%2520the%2520following%2520challenges%253A%2520%25281%2529%2520Unlike%2520natural%2520language%2520tasks%252C%2520time%2520series%2520data%2520vary%2520in%2520frequency%252C%2520channel%2520numbers%252C%2520historical/prediction%2520lengths.%2520For%2520long-term%2520forecasting%2520tasks%2520in%2520particular%252C%2520tailored%2520fine-tuning%2520can%2520significantly%2520enhance%2520performance.%25282%2529%2520Existing%2520parameter-efficient%2520tuning%2520methods%2520like%2520LoRA%2520remain%2520applicable%2520but%2520require%2520adaptation%2520to%2520temporal%2520characteristics.%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520our%2520TRACE%2520framework%2520introduces%2520two%2520key%2520innovations%253A%2520%25281%2529%2520Gated%2520DSIC%2520%2528Gated%2520Dynamic%2520Simulation%2520Importance%2520Calculation%2529%252C%2520an%2520unbiased%2520LoRA%2520module%2520importance%2520selection%2520mechanism%2520that%2520ensures%2520conditional%2520parameter%2520consistency%2520before%2520and%2520after%2520masking.%2520Experiments%2520demonstrate%2520that%2520Gated%2520DSIC%2520outperforms%2520common%2520fine-tuning.%2520%25282%2529%2520Reconstructed%2520prediction%2520heads%2520for%2520long-term%2520forecasting%2520tasks%252C%2520which%2520achieve%2520comparable%2520or%2520superior%2520performance%2520to%2520linear%2520probing%2520heads%2520while%2520drastically%2520reducing%2520parameter%2520counts.%250A%2520%2520Extensive%2520experiments%2520on%2520long-/short-term%2520forecasting%252C%2520anomaly%2520detection%2520and%2520natural%2520language%2520tasks%2520across%2520diverse%2520datasets%252C%2520coupled%2520with%2520ablation%2520studies%252C%2520validate%2520the%2520effectiveness%2520of%2520our%2520method.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.16991v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRACE%3A%20Time%20SeRies%20PArameter%20EffiCient%20FinE-tuning&entry.906535625=Yuze%20Li%20and%20Wei%20Zhu&entry.1292438233=We%20propose%20an%20efficient%20fine-tuning%20method%20for%20time%20series%20foundation%20models%2C%20termed%20TRACE%3A%20Time%20Series%20Parameter%20Efficient%20Fine-tuning.%20While%20pretrained%20time%20series%20foundation%20models%20are%20gaining%20popularity%2C%20they%20face%20the%20following%20challenges%3A%20%281%29%20Unlike%20natural%20language%20tasks%2C%20time%20series%20data%20vary%20in%20frequency%2C%20channel%20numbers%2C%20historical/prediction%20lengths.%20For%20long-term%20forecasting%20tasks%20in%20particular%2C%20tailored%20fine-tuning%20can%20significantly%20enhance%20performance.%282%29%20Existing%20parameter-efficient%20tuning%20methods%20like%20LoRA%20remain%20applicable%20but%20require%20adaptation%20to%20temporal%20characteristics.%0A%20%20To%20address%20these%20challenges%2C%20our%20TRACE%20framework%20introduces%20two%20key%20innovations%3A%20%281%29%20Gated%20DSIC%20%28Gated%20Dynamic%20Simulation%20Importance%20Calculation%29%2C%20an%20unbiased%20LoRA%20module%20importance%20selection%20mechanism%20that%20ensures%20conditional%20parameter%20consistency%20before%20and%20after%20masking.%20Experiments%20demonstrate%20that%20Gated%20DSIC%20outperforms%20common%20fine-tuning.%20%282%29%20Reconstructed%20prediction%20heads%20for%20long-term%20forecasting%20tasks%2C%20which%20achieve%20comparable%20or%20superior%20performance%20to%20linear%20probing%20heads%20while%20drastically%20reducing%20parameter%20counts.%0A%20%20Extensive%20experiments%20on%20long-/short-term%20forecasting%2C%20anomaly%20detection%20and%20natural%20language%20tasks%20across%20diverse%20datasets%2C%20coupled%20with%20ablation%20studies%2C%20validate%20the%20effectiveness%20of%20our%20method.&entry.1838667208=http%3A//arxiv.org/abs/2503.16991v3&entry.124074799=Read"},
{"title": "Planning with Sketch-Guided Verification for Physics-Aware Video Generation", "author": "Yidong Huang and Zun Wang and Han Lin and Dong-Ki Kim and Shayegan Omidshafiei and Jaehong Yoon and Yue Zhang and Mohit Bansal", "abstract": "Recent video generation approaches increasingly rely on planning intermediate control signals such as object trajectories to improve temporal coherence and motion fidelity. However, these methods mostly employ single-shot plans that are typically limited to simple motions, or iterative refinement which requires multiple calls to the video generator, incuring high computational cost. To overcome these limitations, we propose SketchVerify, a training-free, sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions) prior to full video generation by introducing a test-time sampling and verification loop. Given a prompt and a reference image, our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility. To efficiently score candidate motion plans, we render each trajectory as a lightweight video sketch by compositing objects over a static background, which bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance. We iteratively refine the motion plan until a satisfactory one is identified, which is then passed to the trajectory-conditioned generator for final synthesis. Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality, physical realism, and long-term consistency compared to competitive baselines while being substantially more efficient. Our ablation study further shows that scaling up the number of trajectory candidates consistently enhances overall performance.", "link": "http://arxiv.org/abs/2511.17450v1", "date": "2025-11-21", "relevancy": 2.4213, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.63}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6056}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Planning%20with%20Sketch-Guided%20Verification%20for%20Physics-Aware%20Video%20Generation&body=Title%3A%20Planning%20with%20Sketch-Guided%20Verification%20for%20Physics-Aware%20Video%20Generation%0AAuthor%3A%20Yidong%20Huang%20and%20Zun%20Wang%20and%20Han%20Lin%20and%20Dong-Ki%20Kim%20and%20Shayegan%20Omidshafiei%20and%20Jaehong%20Yoon%20and%20Yue%20Zhang%20and%20Mohit%20Bansal%0AAbstract%3A%20Recent%20video%20generation%20approaches%20increasingly%20rely%20on%20planning%20intermediate%20control%20signals%20such%20as%20object%20trajectories%20to%20improve%20temporal%20coherence%20and%20motion%20fidelity.%20However%2C%20these%20methods%20mostly%20employ%20single-shot%20plans%20that%20are%20typically%20limited%20to%20simple%20motions%2C%20or%20iterative%20refinement%20which%20requires%20multiple%20calls%20to%20the%20video%20generator%2C%20incuring%20high%20computational%20cost.%20To%20overcome%20these%20limitations%2C%20we%20propose%20SketchVerify%2C%20a%20training-free%2C%20sketch-verification-based%20planning%20framework%20that%20improves%20motion%20planning%20quality%20with%20more%20dynamically%20coherent%20trajectories%20%28i.e.%2C%20physically%20plausible%20and%20instruction-consistent%20motions%29%20prior%20to%20full%20video%20generation%20by%20introducing%20a%20test-time%20sampling%20and%20verification%20loop.%20Given%20a%20prompt%20and%20a%20reference%20image%2C%20our%20method%20predicts%20multiple%20candidate%20motion%20plans%20and%20ranks%20them%20using%20a%20vision-language%20verifier%20that%20jointly%20evaluates%20semantic%20alignment%20with%20the%20instruction%20and%20physical%20plausibility.%20To%20efficiently%20score%20candidate%20motion%20plans%2C%20we%20render%20each%20trajectory%20as%20a%20lightweight%20video%20sketch%20by%20compositing%20objects%20over%20a%20static%20background%2C%20which%20bypasses%20the%20need%20for%20expensive%2C%20repeated%20diffusion-based%20synthesis%20while%20achieving%20comparable%20performance.%20We%20iteratively%20refine%20the%20motion%20plan%20until%20a%20satisfactory%20one%20is%20identified%2C%20which%20is%20then%20passed%20to%20the%20trajectory-conditioned%20generator%20for%20final%20synthesis.%20Experiments%20on%20WorldModelBench%20and%20PhyWorldBench%20demonstrate%20that%20our%20method%20significantly%20improves%20motion%20quality%2C%20physical%20realism%2C%20and%20long-term%20consistency%20compared%20to%20competitive%20baselines%20while%20being%20substantially%20more%20efficient.%20Our%20ablation%20study%20further%20shows%20that%20scaling%20up%20the%20number%20of%20trajectory%20candidates%20consistently%20enhances%20overall%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17450v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlanning%2520with%2520Sketch-Guided%2520Verification%2520for%2520Physics-Aware%2520Video%2520Generation%26entry.906535625%3DYidong%2520Huang%2520and%2520Zun%2520Wang%2520and%2520Han%2520Lin%2520and%2520Dong-Ki%2520Kim%2520and%2520Shayegan%2520Omidshafiei%2520and%2520Jaehong%2520Yoon%2520and%2520Yue%2520Zhang%2520and%2520Mohit%2520Bansal%26entry.1292438233%3DRecent%2520video%2520generation%2520approaches%2520increasingly%2520rely%2520on%2520planning%2520intermediate%2520control%2520signals%2520such%2520as%2520object%2520trajectories%2520to%2520improve%2520temporal%2520coherence%2520and%2520motion%2520fidelity.%2520However%252C%2520these%2520methods%2520mostly%2520employ%2520single-shot%2520plans%2520that%2520are%2520typically%2520limited%2520to%2520simple%2520motions%252C%2520or%2520iterative%2520refinement%2520which%2520requires%2520multiple%2520calls%2520to%2520the%2520video%2520generator%252C%2520incuring%2520high%2520computational%2520cost.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520SketchVerify%252C%2520a%2520training-free%252C%2520sketch-verification-based%2520planning%2520framework%2520that%2520improves%2520motion%2520planning%2520quality%2520with%2520more%2520dynamically%2520coherent%2520trajectories%2520%2528i.e.%252C%2520physically%2520plausible%2520and%2520instruction-consistent%2520motions%2529%2520prior%2520to%2520full%2520video%2520generation%2520by%2520introducing%2520a%2520test-time%2520sampling%2520and%2520verification%2520loop.%2520Given%2520a%2520prompt%2520and%2520a%2520reference%2520image%252C%2520our%2520method%2520predicts%2520multiple%2520candidate%2520motion%2520plans%2520and%2520ranks%2520them%2520using%2520a%2520vision-language%2520verifier%2520that%2520jointly%2520evaluates%2520semantic%2520alignment%2520with%2520the%2520instruction%2520and%2520physical%2520plausibility.%2520To%2520efficiently%2520score%2520candidate%2520motion%2520plans%252C%2520we%2520render%2520each%2520trajectory%2520as%2520a%2520lightweight%2520video%2520sketch%2520by%2520compositing%2520objects%2520over%2520a%2520static%2520background%252C%2520which%2520bypasses%2520the%2520need%2520for%2520expensive%252C%2520repeated%2520diffusion-based%2520synthesis%2520while%2520achieving%2520comparable%2520performance.%2520We%2520iteratively%2520refine%2520the%2520motion%2520plan%2520until%2520a%2520satisfactory%2520one%2520is%2520identified%252C%2520which%2520is%2520then%2520passed%2520to%2520the%2520trajectory-conditioned%2520generator%2520for%2520final%2520synthesis.%2520Experiments%2520on%2520WorldModelBench%2520and%2520PhyWorldBench%2520demonstrate%2520that%2520our%2520method%2520significantly%2520improves%2520motion%2520quality%252C%2520physical%2520realism%252C%2520and%2520long-term%2520consistency%2520compared%2520to%2520competitive%2520baselines%2520while%2520being%2520substantially%2520more%2520efficient.%2520Our%2520ablation%2520study%2520further%2520shows%2520that%2520scaling%2520up%2520the%2520number%2520of%2520trajectory%2520candidates%2520consistently%2520enhances%2520overall%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17450v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Planning%20with%20Sketch-Guided%20Verification%20for%20Physics-Aware%20Video%20Generation&entry.906535625=Yidong%20Huang%20and%20Zun%20Wang%20and%20Han%20Lin%20and%20Dong-Ki%20Kim%20and%20Shayegan%20Omidshafiei%20and%20Jaehong%20Yoon%20and%20Yue%20Zhang%20and%20Mohit%20Bansal&entry.1292438233=Recent%20video%20generation%20approaches%20increasingly%20rely%20on%20planning%20intermediate%20control%20signals%20such%20as%20object%20trajectories%20to%20improve%20temporal%20coherence%20and%20motion%20fidelity.%20However%2C%20these%20methods%20mostly%20employ%20single-shot%20plans%20that%20are%20typically%20limited%20to%20simple%20motions%2C%20or%20iterative%20refinement%20which%20requires%20multiple%20calls%20to%20the%20video%20generator%2C%20incuring%20high%20computational%20cost.%20To%20overcome%20these%20limitations%2C%20we%20propose%20SketchVerify%2C%20a%20training-free%2C%20sketch-verification-based%20planning%20framework%20that%20improves%20motion%20planning%20quality%20with%20more%20dynamically%20coherent%20trajectories%20%28i.e.%2C%20physically%20plausible%20and%20instruction-consistent%20motions%29%20prior%20to%20full%20video%20generation%20by%20introducing%20a%20test-time%20sampling%20and%20verification%20loop.%20Given%20a%20prompt%20and%20a%20reference%20image%2C%20our%20method%20predicts%20multiple%20candidate%20motion%20plans%20and%20ranks%20them%20using%20a%20vision-language%20verifier%20that%20jointly%20evaluates%20semantic%20alignment%20with%20the%20instruction%20and%20physical%20plausibility.%20To%20efficiently%20score%20candidate%20motion%20plans%2C%20we%20render%20each%20trajectory%20as%20a%20lightweight%20video%20sketch%20by%20compositing%20objects%20over%20a%20static%20background%2C%20which%20bypasses%20the%20need%20for%20expensive%2C%20repeated%20diffusion-based%20synthesis%20while%20achieving%20comparable%20performance.%20We%20iteratively%20refine%20the%20motion%20plan%20until%20a%20satisfactory%20one%20is%20identified%2C%20which%20is%20then%20passed%20to%20the%20trajectory-conditioned%20generator%20for%20final%20synthesis.%20Experiments%20on%20WorldModelBench%20and%20PhyWorldBench%20demonstrate%20that%20our%20method%20significantly%20improves%20motion%20quality%2C%20physical%20realism%2C%20and%20long-term%20consistency%20compared%20to%20competitive%20baselines%20while%20being%20substantially%20more%20efficient.%20Our%20ablation%20study%20further%20shows%20that%20scaling%20up%20the%20number%20of%20trajectory%20candidates%20consistently%20enhances%20overall%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2511.17450v1&entry.124074799=Read"},
{"title": "Improving Multimodal Distillation for 3D Semantic Segmentation under Domain Shift", "author": "Bj\u00f6rn Michele and Alexandre Boulch and Gilles Puy and Tuan-Hung Vu and Renaud Marlet and Nicolas Courty", "abstract": "Semantic segmentation networks trained under full supervision for one type of lidar fail to generalize to unseen lidars without intervention. To reduce the performance gap under domain shifts, a recent trend is to leverage vision foundation models (VFMs) providing robust features across domains. In this work, we conduct an exhaustive study to identify recipes for exploiting VFMs in unsupervised domain adaptation for semantic segmentation of lidar point clouds. Building upon unsupervised image-to-lidar knowledge distillation, our study reveals that: (1) the architecture of the lidar backbone is key to maximize the generalization performance on a target domain; (2) it is possible to pretrain a single backbone once and for all, and use it to address many domain shifts; (3) best results are obtained by keeping the pretrained backbone frozen and training an MLP head for semantic segmentation. The resulting pipeline achieves state-of-the-art results in four widely-recognized and challenging settings. The code will be available at: https://github.com/valeoai/muddos.", "link": "http://arxiv.org/abs/2511.17455v1", "date": "2025-11-21", "relevancy": 2.4146, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6209}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5946}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Multimodal%20Distillation%20for%203D%20Semantic%20Segmentation%20under%20Domain%20Shift&body=Title%3A%20Improving%20Multimodal%20Distillation%20for%203D%20Semantic%20Segmentation%20under%20Domain%20Shift%0AAuthor%3A%20Bj%C3%B6rn%20Michele%20and%20Alexandre%20Boulch%20and%20Gilles%20Puy%20and%20Tuan-Hung%20Vu%20and%20Renaud%20Marlet%20and%20Nicolas%20Courty%0AAbstract%3A%20Semantic%20segmentation%20networks%20trained%20under%20full%20supervision%20for%20one%20type%20of%20lidar%20fail%20to%20generalize%20to%20unseen%20lidars%20without%20intervention.%20To%20reduce%20the%20performance%20gap%20under%20domain%20shifts%2C%20a%20recent%20trend%20is%20to%20leverage%20vision%20foundation%20models%20%28VFMs%29%20providing%20robust%20features%20across%20domains.%20In%20this%20work%2C%20we%20conduct%20an%20exhaustive%20study%20to%20identify%20recipes%20for%20exploiting%20VFMs%20in%20unsupervised%20domain%20adaptation%20for%20semantic%20segmentation%20of%20lidar%20point%20clouds.%20Building%20upon%20unsupervised%20image-to-lidar%20knowledge%20distillation%2C%20our%20study%20reveals%20that%3A%20%281%29%20the%20architecture%20of%20the%20lidar%20backbone%20is%20key%20to%20maximize%20the%20generalization%20performance%20on%20a%20target%20domain%3B%20%282%29%20it%20is%20possible%20to%20pretrain%20a%20single%20backbone%20once%20and%20for%20all%2C%20and%20use%20it%20to%20address%20many%20domain%20shifts%3B%20%283%29%20best%20results%20are%20obtained%20by%20keeping%20the%20pretrained%20backbone%20frozen%20and%20training%20an%20MLP%20head%20for%20semantic%20segmentation.%20The%20resulting%20pipeline%20achieves%20state-of-the-art%20results%20in%20four%20widely-recognized%20and%20challenging%20settings.%20The%20code%20will%20be%20available%20at%3A%20https%3A//github.com/valeoai/muddos.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17455v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Multimodal%2520Distillation%2520for%25203D%2520Semantic%2520Segmentation%2520under%2520Domain%2520Shift%26entry.906535625%3DBj%25C3%25B6rn%2520Michele%2520and%2520Alexandre%2520Boulch%2520and%2520Gilles%2520Puy%2520and%2520Tuan-Hung%2520Vu%2520and%2520Renaud%2520Marlet%2520and%2520Nicolas%2520Courty%26entry.1292438233%3DSemantic%2520segmentation%2520networks%2520trained%2520under%2520full%2520supervision%2520for%2520one%2520type%2520of%2520lidar%2520fail%2520to%2520generalize%2520to%2520unseen%2520lidars%2520without%2520intervention.%2520To%2520reduce%2520the%2520performance%2520gap%2520under%2520domain%2520shifts%252C%2520a%2520recent%2520trend%2520is%2520to%2520leverage%2520vision%2520foundation%2520models%2520%2528VFMs%2529%2520providing%2520robust%2520features%2520across%2520domains.%2520In%2520this%2520work%252C%2520we%2520conduct%2520an%2520exhaustive%2520study%2520to%2520identify%2520recipes%2520for%2520exploiting%2520VFMs%2520in%2520unsupervised%2520domain%2520adaptation%2520for%2520semantic%2520segmentation%2520of%2520lidar%2520point%2520clouds.%2520Building%2520upon%2520unsupervised%2520image-to-lidar%2520knowledge%2520distillation%252C%2520our%2520study%2520reveals%2520that%253A%2520%25281%2529%2520the%2520architecture%2520of%2520the%2520lidar%2520backbone%2520is%2520key%2520to%2520maximize%2520the%2520generalization%2520performance%2520on%2520a%2520target%2520domain%253B%2520%25282%2529%2520it%2520is%2520possible%2520to%2520pretrain%2520a%2520single%2520backbone%2520once%2520and%2520for%2520all%252C%2520and%2520use%2520it%2520to%2520address%2520many%2520domain%2520shifts%253B%2520%25283%2529%2520best%2520results%2520are%2520obtained%2520by%2520keeping%2520the%2520pretrained%2520backbone%2520frozen%2520and%2520training%2520an%2520MLP%2520head%2520for%2520semantic%2520segmentation.%2520The%2520resulting%2520pipeline%2520achieves%2520state-of-the-art%2520results%2520in%2520four%2520widely-recognized%2520and%2520challenging%2520settings.%2520The%2520code%2520will%2520be%2520available%2520at%253A%2520https%253A//github.com/valeoai/muddos.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17455v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Multimodal%20Distillation%20for%203D%20Semantic%20Segmentation%20under%20Domain%20Shift&entry.906535625=Bj%C3%B6rn%20Michele%20and%20Alexandre%20Boulch%20and%20Gilles%20Puy%20and%20Tuan-Hung%20Vu%20and%20Renaud%20Marlet%20and%20Nicolas%20Courty&entry.1292438233=Semantic%20segmentation%20networks%20trained%20under%20full%20supervision%20for%20one%20type%20of%20lidar%20fail%20to%20generalize%20to%20unseen%20lidars%20without%20intervention.%20To%20reduce%20the%20performance%20gap%20under%20domain%20shifts%2C%20a%20recent%20trend%20is%20to%20leverage%20vision%20foundation%20models%20%28VFMs%29%20providing%20robust%20features%20across%20domains.%20In%20this%20work%2C%20we%20conduct%20an%20exhaustive%20study%20to%20identify%20recipes%20for%20exploiting%20VFMs%20in%20unsupervised%20domain%20adaptation%20for%20semantic%20segmentation%20of%20lidar%20point%20clouds.%20Building%20upon%20unsupervised%20image-to-lidar%20knowledge%20distillation%2C%20our%20study%20reveals%20that%3A%20%281%29%20the%20architecture%20of%20the%20lidar%20backbone%20is%20key%20to%20maximize%20the%20generalization%20performance%20on%20a%20target%20domain%3B%20%282%29%20it%20is%20possible%20to%20pretrain%20a%20single%20backbone%20once%20and%20for%20all%2C%20and%20use%20it%20to%20address%20many%20domain%20shifts%3B%20%283%29%20best%20results%20are%20obtained%20by%20keeping%20the%20pretrained%20backbone%20frozen%20and%20training%20an%20MLP%20head%20for%20semantic%20segmentation.%20The%20resulting%20pipeline%20achieves%20state-of-the-art%20results%20in%20four%20widely-recognized%20and%20challenging%20settings.%20The%20code%20will%20be%20available%20at%3A%20https%3A//github.com/valeoai/muddos.&entry.1838667208=http%3A//arxiv.org/abs/2511.17455v1&entry.124074799=Read"},
{"title": "Feasibility of Embodied Dynamics Based Bayesian Learning for Continuous Pursuit Motion Control of Assistive Mobile Robots in the Built Environment", "author": "Xiaoshan Zhou and Carol C. Menassa and Vineet R. Kamat", "abstract": "Non-invasive electroencephalography (EEG)-based brain-computer interfaces (BCIs) offer an intuitive means for individuals with severe motor impairments to independently operate assistive robotic wheelchairs and navigate built environments. Despite considerable progress in BCI research, most current motion control systems are limited to discrete commands, rather than supporting continuous pursuit, where users can freely adjust speed and direction in real time. Such natural mobility control is, however, essential for wheelchair users to navigate complex public spaces, such as transit stations, airports, hospitals, and indoor corridors, to interact socially with the dynamic populations with agility, and to move flexibly and comfortably as autonomous driving is refined to allow movement at will. In this study, we address the gap of continuous pursuit motion control in BCIs by proposing and validating a brain-inspired Bayesian inference framework, where embodied dynamics in acceleration-based motor representations are decoded. This approach contrasts with conventional kinematics-level decoding and deep learning-based methods. Using a public dataset with sixteen hours of EEG from four subjects performing motor imagery-based target-following, we demonstrate that our method, utilizing Automatic Relevance Determination for feature selection and continual online learning, reduces the normalized mean squared error between predicted and true velocities by 72% compared to autoregressive and EEGNet-based methods in a session-accumulative transfer learning setting. Theoretically, these findings empirically support embodied cognition theory and reveal the brain's intrinsic motor control dynamics in an embodied and predictive nature. Practically, grounding EEG decoding in the same dynamical principles that govern biological motion offers a promising path toward more stable and intuitive BCI control.", "link": "http://arxiv.org/abs/2511.17401v1", "date": "2025-11-21", "relevancy": 2.4124, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6478}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6459}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feasibility%20of%20Embodied%20Dynamics%20Based%20Bayesian%20Learning%20for%20Continuous%20Pursuit%20Motion%20Control%20of%20Assistive%20Mobile%20Robots%20in%20the%20Built%20Environment&body=Title%3A%20Feasibility%20of%20Embodied%20Dynamics%20Based%20Bayesian%20Learning%20for%20Continuous%20Pursuit%20Motion%20Control%20of%20Assistive%20Mobile%20Robots%20in%20the%20Built%20Environment%0AAuthor%3A%20Xiaoshan%20Zhou%20and%20Carol%20C.%20Menassa%20and%20Vineet%20R.%20Kamat%0AAbstract%3A%20Non-invasive%20electroencephalography%20%28EEG%29-based%20brain-computer%20interfaces%20%28BCIs%29%20offer%20an%20intuitive%20means%20for%20individuals%20with%20severe%20motor%20impairments%20to%20independently%20operate%20assistive%20robotic%20wheelchairs%20and%20navigate%20built%20environments.%20Despite%20considerable%20progress%20in%20BCI%20research%2C%20most%20current%20motion%20control%20systems%20are%20limited%20to%20discrete%20commands%2C%20rather%20than%20supporting%20continuous%20pursuit%2C%20where%20users%20can%20freely%20adjust%20speed%20and%20direction%20in%20real%20time.%20Such%20natural%20mobility%20control%20is%2C%20however%2C%20essential%20for%20wheelchair%20users%20to%20navigate%20complex%20public%20spaces%2C%20such%20as%20transit%20stations%2C%20airports%2C%20hospitals%2C%20and%20indoor%20corridors%2C%20to%20interact%20socially%20with%20the%20dynamic%20populations%20with%20agility%2C%20and%20to%20move%20flexibly%20and%20comfortably%20as%20autonomous%20driving%20is%20refined%20to%20allow%20movement%20at%20will.%20In%20this%20study%2C%20we%20address%20the%20gap%20of%20continuous%20pursuit%20motion%20control%20in%20BCIs%20by%20proposing%20and%20validating%20a%20brain-inspired%20Bayesian%20inference%20framework%2C%20where%20embodied%20dynamics%20in%20acceleration-based%20motor%20representations%20are%20decoded.%20This%20approach%20contrasts%20with%20conventional%20kinematics-level%20decoding%20and%20deep%20learning-based%20methods.%20Using%20a%20public%20dataset%20with%20sixteen%20hours%20of%20EEG%20from%20four%20subjects%20performing%20motor%20imagery-based%20target-following%2C%20we%20demonstrate%20that%20our%20method%2C%20utilizing%20Automatic%20Relevance%20Determination%20for%20feature%20selection%20and%20continual%20online%20learning%2C%20reduces%20the%20normalized%20mean%20squared%20error%20between%20predicted%20and%20true%20velocities%20by%2072%25%20compared%20to%20autoregressive%20and%20EEGNet-based%20methods%20in%20a%20session-accumulative%20transfer%20learning%20setting.%20Theoretically%2C%20these%20findings%20empirically%20support%20embodied%20cognition%20theory%20and%20reveal%20the%20brain%27s%20intrinsic%20motor%20control%20dynamics%20in%20an%20embodied%20and%20predictive%20nature.%20Practically%2C%20grounding%20EEG%20decoding%20in%20the%20same%20dynamical%20principles%20that%20govern%20biological%20motion%20offers%20a%20promising%20path%20toward%20more%20stable%20and%20intuitive%20BCI%20control.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeasibility%2520of%2520Embodied%2520Dynamics%2520Based%2520Bayesian%2520Learning%2520for%2520Continuous%2520Pursuit%2520Motion%2520Control%2520of%2520Assistive%2520Mobile%2520Robots%2520in%2520the%2520Built%2520Environment%26entry.906535625%3DXiaoshan%2520Zhou%2520and%2520Carol%2520C.%2520Menassa%2520and%2520Vineet%2520R.%2520Kamat%26entry.1292438233%3DNon-invasive%2520electroencephalography%2520%2528EEG%2529-based%2520brain-computer%2520interfaces%2520%2528BCIs%2529%2520offer%2520an%2520intuitive%2520means%2520for%2520individuals%2520with%2520severe%2520motor%2520impairments%2520to%2520independently%2520operate%2520assistive%2520robotic%2520wheelchairs%2520and%2520navigate%2520built%2520environments.%2520Despite%2520considerable%2520progress%2520in%2520BCI%2520research%252C%2520most%2520current%2520motion%2520control%2520systems%2520are%2520limited%2520to%2520discrete%2520commands%252C%2520rather%2520than%2520supporting%2520continuous%2520pursuit%252C%2520where%2520users%2520can%2520freely%2520adjust%2520speed%2520and%2520direction%2520in%2520real%2520time.%2520Such%2520natural%2520mobility%2520control%2520is%252C%2520however%252C%2520essential%2520for%2520wheelchair%2520users%2520to%2520navigate%2520complex%2520public%2520spaces%252C%2520such%2520as%2520transit%2520stations%252C%2520airports%252C%2520hospitals%252C%2520and%2520indoor%2520corridors%252C%2520to%2520interact%2520socially%2520with%2520the%2520dynamic%2520populations%2520with%2520agility%252C%2520and%2520to%2520move%2520flexibly%2520and%2520comfortably%2520as%2520autonomous%2520driving%2520is%2520refined%2520to%2520allow%2520movement%2520at%2520will.%2520In%2520this%2520study%252C%2520we%2520address%2520the%2520gap%2520of%2520continuous%2520pursuit%2520motion%2520control%2520in%2520BCIs%2520by%2520proposing%2520and%2520validating%2520a%2520brain-inspired%2520Bayesian%2520inference%2520framework%252C%2520where%2520embodied%2520dynamics%2520in%2520acceleration-based%2520motor%2520representations%2520are%2520decoded.%2520This%2520approach%2520contrasts%2520with%2520conventional%2520kinematics-level%2520decoding%2520and%2520deep%2520learning-based%2520methods.%2520Using%2520a%2520public%2520dataset%2520with%2520sixteen%2520hours%2520of%2520EEG%2520from%2520four%2520subjects%2520performing%2520motor%2520imagery-based%2520target-following%252C%2520we%2520demonstrate%2520that%2520our%2520method%252C%2520utilizing%2520Automatic%2520Relevance%2520Determination%2520for%2520feature%2520selection%2520and%2520continual%2520online%2520learning%252C%2520reduces%2520the%2520normalized%2520mean%2520squared%2520error%2520between%2520predicted%2520and%2520true%2520velocities%2520by%252072%2525%2520compared%2520to%2520autoregressive%2520and%2520EEGNet-based%2520methods%2520in%2520a%2520session-accumulative%2520transfer%2520learning%2520setting.%2520Theoretically%252C%2520these%2520findings%2520empirically%2520support%2520embodied%2520cognition%2520theory%2520and%2520reveal%2520the%2520brain%2527s%2520intrinsic%2520motor%2520control%2520dynamics%2520in%2520an%2520embodied%2520and%2520predictive%2520nature.%2520Practically%252C%2520grounding%2520EEG%2520decoding%2520in%2520the%2520same%2520dynamical%2520principles%2520that%2520govern%2520biological%2520motion%2520offers%2520a%2520promising%2520path%2520toward%2520more%2520stable%2520and%2520intuitive%2520BCI%2520control.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feasibility%20of%20Embodied%20Dynamics%20Based%20Bayesian%20Learning%20for%20Continuous%20Pursuit%20Motion%20Control%20of%20Assistive%20Mobile%20Robots%20in%20the%20Built%20Environment&entry.906535625=Xiaoshan%20Zhou%20and%20Carol%20C.%20Menassa%20and%20Vineet%20R.%20Kamat&entry.1292438233=Non-invasive%20electroencephalography%20%28EEG%29-based%20brain-computer%20interfaces%20%28BCIs%29%20offer%20an%20intuitive%20means%20for%20individuals%20with%20severe%20motor%20impairments%20to%20independently%20operate%20assistive%20robotic%20wheelchairs%20and%20navigate%20built%20environments.%20Despite%20considerable%20progress%20in%20BCI%20research%2C%20most%20current%20motion%20control%20systems%20are%20limited%20to%20discrete%20commands%2C%20rather%20than%20supporting%20continuous%20pursuit%2C%20where%20users%20can%20freely%20adjust%20speed%20and%20direction%20in%20real%20time.%20Such%20natural%20mobility%20control%20is%2C%20however%2C%20essential%20for%20wheelchair%20users%20to%20navigate%20complex%20public%20spaces%2C%20such%20as%20transit%20stations%2C%20airports%2C%20hospitals%2C%20and%20indoor%20corridors%2C%20to%20interact%20socially%20with%20the%20dynamic%20populations%20with%20agility%2C%20and%20to%20move%20flexibly%20and%20comfortably%20as%20autonomous%20driving%20is%20refined%20to%20allow%20movement%20at%20will.%20In%20this%20study%2C%20we%20address%20the%20gap%20of%20continuous%20pursuit%20motion%20control%20in%20BCIs%20by%20proposing%20and%20validating%20a%20brain-inspired%20Bayesian%20inference%20framework%2C%20where%20embodied%20dynamics%20in%20acceleration-based%20motor%20representations%20are%20decoded.%20This%20approach%20contrasts%20with%20conventional%20kinematics-level%20decoding%20and%20deep%20learning-based%20methods.%20Using%20a%20public%20dataset%20with%20sixteen%20hours%20of%20EEG%20from%20four%20subjects%20performing%20motor%20imagery-based%20target-following%2C%20we%20demonstrate%20that%20our%20method%2C%20utilizing%20Automatic%20Relevance%20Determination%20for%20feature%20selection%20and%20continual%20online%20learning%2C%20reduces%20the%20normalized%20mean%20squared%20error%20between%20predicted%20and%20true%20velocities%20by%2072%25%20compared%20to%20autoregressive%20and%20EEGNet-based%20methods%20in%20a%20session-accumulative%20transfer%20learning%20setting.%20Theoretically%2C%20these%20findings%20empirically%20support%20embodied%20cognition%20theory%20and%20reveal%20the%20brain%27s%20intrinsic%20motor%20control%20dynamics%20in%20an%20embodied%20and%20predictive%20nature.%20Practically%2C%20grounding%20EEG%20decoding%20in%20the%20same%20dynamical%20principles%20that%20govern%20biological%20motion%20offers%20a%20promising%20path%20toward%20more%20stable%20and%20intuitive%20BCI%20control.&entry.1838667208=http%3A//arxiv.org/abs/2511.17401v1&entry.124074799=Read"},
{"title": "CLIMB-3D: Continual Learning for Imbalanced 3D Instance Segmentation", "author": "Vishal Thengane and Jean Lahoud and Hisham Cholakkal and Rao Muhammad Anwer and Lu Yin and Xiatian Zhu and Salman Khan", "abstract": "While 3D instance segmentation (3DIS) has advanced significantly, most existing methods assume that all object classes are known in advance and uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new classes emerge gradually and exhibit natural imbalance. Although some approaches address the emergence of new classes, they often overlook class imbalance, which leads to suboptimal performance, particularly on rare categories. To tackle this, we propose \\ourmethodbf, a unified framework for \\textbf{CL}ass-incremental \\textbf{Imb}alance-aware \\textbf{3D}IS. Building upon established exemplar replay (ER) strategies, we show that ER alone is insufficient to achieve robust performance under memory constraints. To mitigate this, we introduce a novel pseudo-label generator (PLG) that extends supervision to previously learned categories by leveraging predictions from a frozen model trained on prior tasks. Despite its promise, PLG tends to be biased towards frequent classes. Therefore, we propose a class-balanced re-weighting (CBR) scheme that estimates object frequencies from pseudo-labels and dynamically adjusts training bias, without requiring access to past data. We design and evaluate three incremental scenarios for 3DIS on the challenging ScanNet200 dataset and additionally validate our method for semantic segmentation on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by up to 16.76\\% mAP for instance segmentation and approximately 30\\% mIoU for semantic segmentation, demonstrating strong generalisation across both frequent and rare classes. Code is available at: https://github.com/vgthengane/CLIMB3D", "link": "http://arxiv.org/abs/2502.17429v3", "date": "2025-11-21", "relevancy": 2.4088, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6316}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.583}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIMB-3D%3A%20Continual%20Learning%20for%20Imbalanced%203D%20Instance%20Segmentation&body=Title%3A%20CLIMB-3D%3A%20Continual%20Learning%20for%20Imbalanced%203D%20Instance%20Segmentation%0AAuthor%3A%20Vishal%20Thengane%20and%20Jean%20Lahoud%20and%20Hisham%20Cholakkal%20and%20Rao%20Muhammad%20Anwer%20and%20Lu%20Yin%20and%20Xiatian%20Zhu%20and%20Salman%20Khan%0AAbstract%3A%20While%203D%20instance%20segmentation%20%283DIS%29%20has%20advanced%20significantly%2C%20most%20existing%20methods%20assume%20that%20all%20object%20classes%20are%20known%20in%20advance%20and%20uniformly%20distributed.%20However%2C%20this%20assumption%20is%20unrealistic%20in%20dynamic%2C%20real-world%20environments%20where%20new%20classes%20emerge%20gradually%20and%20exhibit%20natural%20imbalance.%20Although%20some%20approaches%20address%20the%20emergence%20of%20new%20classes%2C%20they%20often%20overlook%20class%20imbalance%2C%20which%20leads%20to%20suboptimal%20performance%2C%20particularly%20on%20rare%20categories.%20To%20tackle%20this%2C%20we%20propose%20%5Courmethodbf%2C%20a%20unified%20framework%20for%20%5Ctextbf%7BCL%7Dass-incremental%20%5Ctextbf%7BImb%7Dalance-aware%20%5Ctextbf%7B3D%7DIS.%20Building%20upon%20established%20exemplar%20replay%20%28ER%29%20strategies%2C%20we%20show%20that%20ER%20alone%20is%20insufficient%20to%20achieve%20robust%20performance%20under%20memory%20constraints.%20To%20mitigate%20this%2C%20we%20introduce%20a%20novel%20pseudo-label%20generator%20%28PLG%29%20that%20extends%20supervision%20to%20previously%20learned%20categories%20by%20leveraging%20predictions%20from%20a%20frozen%20model%20trained%20on%20prior%20tasks.%20Despite%20its%20promise%2C%20PLG%20tends%20to%20be%20biased%20towards%20frequent%20classes.%20Therefore%2C%20we%20propose%20a%20class-balanced%20re-weighting%20%28CBR%29%20scheme%20that%20estimates%20object%20frequencies%20from%20pseudo-labels%20and%20dynamically%20adjusts%20training%20bias%2C%20without%20requiring%20access%20to%20past%20data.%20We%20design%20and%20evaluate%20three%20incremental%20scenarios%20for%203DIS%20on%20the%20challenging%20ScanNet200%20dataset%20and%20additionally%20validate%20our%20method%20for%20semantic%20segmentation%20on%20ScanNetV2.%20Our%20approach%20achieves%20state-of-the-art%20results%2C%20surpassing%20prior%20work%20by%20up%20to%2016.76%5C%25%20mAP%20for%20instance%20segmentation%20and%20approximately%2030%5C%25%20mIoU%20for%20semantic%20segmentation%2C%20demonstrating%20strong%20generalisation%20across%20both%20frequent%20and%20rare%20classes.%20Code%20is%20available%20at%3A%20https%3A//github.com/vgthengane/CLIMB3D%0ALink%3A%20http%3A//arxiv.org/abs/2502.17429v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIMB-3D%253A%2520Continual%2520Learning%2520for%2520Imbalanced%25203D%2520Instance%2520Segmentation%26entry.906535625%3DVishal%2520Thengane%2520and%2520Jean%2520Lahoud%2520and%2520Hisham%2520Cholakkal%2520and%2520Rao%2520Muhammad%2520Anwer%2520and%2520Lu%2520Yin%2520and%2520Xiatian%2520Zhu%2520and%2520Salman%2520Khan%26entry.1292438233%3DWhile%25203D%2520instance%2520segmentation%2520%25283DIS%2529%2520has%2520advanced%2520significantly%252C%2520most%2520existing%2520methods%2520assume%2520that%2520all%2520object%2520classes%2520are%2520known%2520in%2520advance%2520and%2520uniformly%2520distributed.%2520However%252C%2520this%2520assumption%2520is%2520unrealistic%2520in%2520dynamic%252C%2520real-world%2520environments%2520where%2520new%2520classes%2520emerge%2520gradually%2520and%2520exhibit%2520natural%2520imbalance.%2520Although%2520some%2520approaches%2520address%2520the%2520emergence%2520of%2520new%2520classes%252C%2520they%2520often%2520overlook%2520class%2520imbalance%252C%2520which%2520leads%2520to%2520suboptimal%2520performance%252C%2520particularly%2520on%2520rare%2520categories.%2520To%2520tackle%2520this%252C%2520we%2520propose%2520%255Courmethodbf%252C%2520a%2520unified%2520framework%2520for%2520%255Ctextbf%257BCL%257Dass-incremental%2520%255Ctextbf%257BImb%257Dalance-aware%2520%255Ctextbf%257B3D%257DIS.%2520Building%2520upon%2520established%2520exemplar%2520replay%2520%2528ER%2529%2520strategies%252C%2520we%2520show%2520that%2520ER%2520alone%2520is%2520insufficient%2520to%2520achieve%2520robust%2520performance%2520under%2520memory%2520constraints.%2520To%2520mitigate%2520this%252C%2520we%2520introduce%2520a%2520novel%2520pseudo-label%2520generator%2520%2528PLG%2529%2520that%2520extends%2520supervision%2520to%2520previously%2520learned%2520categories%2520by%2520leveraging%2520predictions%2520from%2520a%2520frozen%2520model%2520trained%2520on%2520prior%2520tasks.%2520Despite%2520its%2520promise%252C%2520PLG%2520tends%2520to%2520be%2520biased%2520towards%2520frequent%2520classes.%2520Therefore%252C%2520we%2520propose%2520a%2520class-balanced%2520re-weighting%2520%2528CBR%2529%2520scheme%2520that%2520estimates%2520object%2520frequencies%2520from%2520pseudo-labels%2520and%2520dynamically%2520adjusts%2520training%2520bias%252C%2520without%2520requiring%2520access%2520to%2520past%2520data.%2520We%2520design%2520and%2520evaluate%2520three%2520incremental%2520scenarios%2520for%25203DIS%2520on%2520the%2520challenging%2520ScanNet200%2520dataset%2520and%2520additionally%2520validate%2520our%2520method%2520for%2520semantic%2520segmentation%2520on%2520ScanNetV2.%2520Our%2520approach%2520achieves%2520state-of-the-art%2520results%252C%2520surpassing%2520prior%2520work%2520by%2520up%2520to%252016.76%255C%2525%2520mAP%2520for%2520instance%2520segmentation%2520and%2520approximately%252030%255C%2525%2520mIoU%2520for%2520semantic%2520segmentation%252C%2520demonstrating%2520strong%2520generalisation%2520across%2520both%2520frequent%2520and%2520rare%2520classes.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/vgthengane/CLIMB3D%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17429v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIMB-3D%3A%20Continual%20Learning%20for%20Imbalanced%203D%20Instance%20Segmentation&entry.906535625=Vishal%20Thengane%20and%20Jean%20Lahoud%20and%20Hisham%20Cholakkal%20and%20Rao%20Muhammad%20Anwer%20and%20Lu%20Yin%20and%20Xiatian%20Zhu%20and%20Salman%20Khan&entry.1292438233=While%203D%20instance%20segmentation%20%283DIS%29%20has%20advanced%20significantly%2C%20most%20existing%20methods%20assume%20that%20all%20object%20classes%20are%20known%20in%20advance%20and%20uniformly%20distributed.%20However%2C%20this%20assumption%20is%20unrealistic%20in%20dynamic%2C%20real-world%20environments%20where%20new%20classes%20emerge%20gradually%20and%20exhibit%20natural%20imbalance.%20Although%20some%20approaches%20address%20the%20emergence%20of%20new%20classes%2C%20they%20often%20overlook%20class%20imbalance%2C%20which%20leads%20to%20suboptimal%20performance%2C%20particularly%20on%20rare%20categories.%20To%20tackle%20this%2C%20we%20propose%20%5Courmethodbf%2C%20a%20unified%20framework%20for%20%5Ctextbf%7BCL%7Dass-incremental%20%5Ctextbf%7BImb%7Dalance-aware%20%5Ctextbf%7B3D%7DIS.%20Building%20upon%20established%20exemplar%20replay%20%28ER%29%20strategies%2C%20we%20show%20that%20ER%20alone%20is%20insufficient%20to%20achieve%20robust%20performance%20under%20memory%20constraints.%20To%20mitigate%20this%2C%20we%20introduce%20a%20novel%20pseudo-label%20generator%20%28PLG%29%20that%20extends%20supervision%20to%20previously%20learned%20categories%20by%20leveraging%20predictions%20from%20a%20frozen%20model%20trained%20on%20prior%20tasks.%20Despite%20its%20promise%2C%20PLG%20tends%20to%20be%20biased%20towards%20frequent%20classes.%20Therefore%2C%20we%20propose%20a%20class-balanced%20re-weighting%20%28CBR%29%20scheme%20that%20estimates%20object%20frequencies%20from%20pseudo-labels%20and%20dynamically%20adjusts%20training%20bias%2C%20without%20requiring%20access%20to%20past%20data.%20We%20design%20and%20evaluate%20three%20incremental%20scenarios%20for%203DIS%20on%20the%20challenging%20ScanNet200%20dataset%20and%20additionally%20validate%20our%20method%20for%20semantic%20segmentation%20on%20ScanNetV2.%20Our%20approach%20achieves%20state-of-the-art%20results%2C%20surpassing%20prior%20work%20by%20up%20to%2016.76%5C%25%20mAP%20for%20instance%20segmentation%20and%20approximately%2030%5C%25%20mIoU%20for%20semantic%20segmentation%2C%20demonstrating%20strong%20generalisation%20across%20both%20frequent%20and%20rare%20classes.%20Code%20is%20available%20at%3A%20https%3A//github.com/vgthengane/CLIMB3D&entry.1838667208=http%3A//arxiv.org/abs/2502.17429v3&entry.124074799=Read"},
{"title": "MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment", "author": "Elena Camuffo and Francesco Barbato and Mete Ozay and Simone Milani and Umberto Michieli", "abstract": "Personalized object detection aims to adapt a general-purpose detector to recognize user-specific instances from only a few examples. Lightweight models often struggle in this setting due to their weak semantic priors, while large vision-language models (VLMs) offer strong object-level understanding but are too computationally demanding for real-time or on-device applications. We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment), a distillation framework that transfers multimodal region-level knowledge from a frozen VLM teacher into a lightweight vision-only detector. MOCHA extracts fused visual and textual teacher's embeddings and uses them to guide student training through a dual-objective loss that enforces accurate local alignment and global relational consistency across regions. This process enables efficient transfer of semantics without the need for teacher modifications or textual input at inference. MOCHA consistently outperforms prior baselines across four personalized detection benchmarks under strict few-shot regimes, yielding a +10.1 average improvement, with minimal inference cost.", "link": "http://arxiv.org/abs/2509.14001v4", "date": "2025-11-21", "relevancy": 2.397, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6252}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5895}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOCHA%3A%20Multi-modal%20Objects-aware%20Cross-arcHitecture%20Alignment&body=Title%3A%20MOCHA%3A%20Multi-modal%20Objects-aware%20Cross-arcHitecture%20Alignment%0AAuthor%3A%20Elena%20Camuffo%20and%20Francesco%20Barbato%20and%20Mete%20Ozay%20and%20Simone%20Milani%20and%20Umberto%20Michieli%0AAbstract%3A%20Personalized%20object%20detection%20aims%20to%20adapt%20a%20general-purpose%20detector%20to%20recognize%20user-specific%20instances%20from%20only%20a%20few%20examples.%20Lightweight%20models%20often%20struggle%20in%20this%20setting%20due%20to%20their%20weak%20semantic%20priors%2C%20while%20large%20vision-language%20models%20%28VLMs%29%20offer%20strong%20object-level%20understanding%20but%20are%20too%20computationally%20demanding%20for%20real-time%20or%20on-device%20applications.%20We%20introduce%20MOCHA%20%28Multi-modal%20Objects-aware%20Cross-arcHitecture%20Alignment%29%2C%20a%20distillation%20framework%20that%20transfers%20multimodal%20region-level%20knowledge%20from%20a%20frozen%20VLM%20teacher%20into%20a%20lightweight%20vision-only%20detector.%20MOCHA%20extracts%20fused%20visual%20and%20textual%20teacher%27s%20embeddings%20and%20uses%20them%20to%20guide%20student%20training%20through%20a%20dual-objective%20loss%20that%20enforces%20accurate%20local%20alignment%20and%20global%20relational%20consistency%20across%20regions.%20This%20process%20enables%20efficient%20transfer%20of%20semantics%20without%20the%20need%20for%20teacher%20modifications%20or%20textual%20input%20at%20inference.%20MOCHA%20consistently%20outperforms%20prior%20baselines%20across%20four%20personalized%20detection%20benchmarks%20under%20strict%20few-shot%20regimes%2C%20yielding%20a%20%2B10.1%20average%20improvement%2C%20with%20minimal%20inference%20cost.%0ALink%3A%20http%3A//arxiv.org/abs/2509.14001v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOCHA%253A%2520Multi-modal%2520Objects-aware%2520Cross-arcHitecture%2520Alignment%26entry.906535625%3DElena%2520Camuffo%2520and%2520Francesco%2520Barbato%2520and%2520Mete%2520Ozay%2520and%2520Simone%2520Milani%2520and%2520Umberto%2520Michieli%26entry.1292438233%3DPersonalized%2520object%2520detection%2520aims%2520to%2520adapt%2520a%2520general-purpose%2520detector%2520to%2520recognize%2520user-specific%2520instances%2520from%2520only%2520a%2520few%2520examples.%2520Lightweight%2520models%2520often%2520struggle%2520in%2520this%2520setting%2520due%2520to%2520their%2520weak%2520semantic%2520priors%252C%2520while%2520large%2520vision-language%2520models%2520%2528VLMs%2529%2520offer%2520strong%2520object-level%2520understanding%2520but%2520are%2520too%2520computationally%2520demanding%2520for%2520real-time%2520or%2520on-device%2520applications.%2520We%2520introduce%2520MOCHA%2520%2528Multi-modal%2520Objects-aware%2520Cross-arcHitecture%2520Alignment%2529%252C%2520a%2520distillation%2520framework%2520that%2520transfers%2520multimodal%2520region-level%2520knowledge%2520from%2520a%2520frozen%2520VLM%2520teacher%2520into%2520a%2520lightweight%2520vision-only%2520detector.%2520MOCHA%2520extracts%2520fused%2520visual%2520and%2520textual%2520teacher%2527s%2520embeddings%2520and%2520uses%2520them%2520to%2520guide%2520student%2520training%2520through%2520a%2520dual-objective%2520loss%2520that%2520enforces%2520accurate%2520local%2520alignment%2520and%2520global%2520relational%2520consistency%2520across%2520regions.%2520This%2520process%2520enables%2520efficient%2520transfer%2520of%2520semantics%2520without%2520the%2520need%2520for%2520teacher%2520modifications%2520or%2520textual%2520input%2520at%2520inference.%2520MOCHA%2520consistently%2520outperforms%2520prior%2520baselines%2520across%2520four%2520personalized%2520detection%2520benchmarks%2520under%2520strict%2520few-shot%2520regimes%252C%2520yielding%2520a%2520%252B10.1%2520average%2520improvement%252C%2520with%2520minimal%2520inference%2520cost.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14001v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOCHA%3A%20Multi-modal%20Objects-aware%20Cross-arcHitecture%20Alignment&entry.906535625=Elena%20Camuffo%20and%20Francesco%20Barbato%20and%20Mete%20Ozay%20and%20Simone%20Milani%20and%20Umberto%20Michieli&entry.1292438233=Personalized%20object%20detection%20aims%20to%20adapt%20a%20general-purpose%20detector%20to%20recognize%20user-specific%20instances%20from%20only%20a%20few%20examples.%20Lightweight%20models%20often%20struggle%20in%20this%20setting%20due%20to%20their%20weak%20semantic%20priors%2C%20while%20large%20vision-language%20models%20%28VLMs%29%20offer%20strong%20object-level%20understanding%20but%20are%20too%20computationally%20demanding%20for%20real-time%20or%20on-device%20applications.%20We%20introduce%20MOCHA%20%28Multi-modal%20Objects-aware%20Cross-arcHitecture%20Alignment%29%2C%20a%20distillation%20framework%20that%20transfers%20multimodal%20region-level%20knowledge%20from%20a%20frozen%20VLM%20teacher%20into%20a%20lightweight%20vision-only%20detector.%20MOCHA%20extracts%20fused%20visual%20and%20textual%20teacher%27s%20embeddings%20and%20uses%20them%20to%20guide%20student%20training%20through%20a%20dual-objective%20loss%20that%20enforces%20accurate%20local%20alignment%20and%20global%20relational%20consistency%20across%20regions.%20This%20process%20enables%20efficient%20transfer%20of%20semantics%20without%20the%20need%20for%20teacher%20modifications%20or%20textual%20input%20at%20inference.%20MOCHA%20consistently%20outperforms%20prior%20baselines%20across%20four%20personalized%20detection%20benchmarks%20under%20strict%20few-shot%20regimes%2C%20yielding%20a%20%2B10.1%20average%20improvement%2C%20with%20minimal%20inference%20cost.&entry.1838667208=http%3A//arxiv.org/abs/2509.14001v4&entry.124074799=Read"},
{"title": "Native 3D Editing with Full Attention", "author": "Weiwei Cai and Shuangkang Fang and Weicai Ye and Xin Dong and Yunhan Yang and Xuanyang Zhang and Wei Cheng and Yanpei Cao and Gang Yu and Tao Chen", "abstract": "Instruction-guided 3D editing is a rapidly emerging field with the potential to broaden access to 3D content creation. However, existing methods face critical limitations: optimization-based approaches are prohibitively slow, while feed-forward approaches relying on multi-view 2D editing often suffer from inconsistent geometry and degraded visual quality. To address these issues, we propose a novel native 3D editing framework that directly manipulates 3D representations in a single, efficient feed-forward pass. Specifically, we create a large-scale, multi-modal dataset for instruction-guided 3D editing, covering diverse addition, deletion, and modification tasks. This dataset is meticulously curated to ensure that edited objects faithfully adhere to the instructional changes while preserving the consistency of unedited regions with the source object. Building upon this dataset, we explore two distinct conditioning strategies for our model: a conventional cross-attention mechanism and a novel 3D token concatenation approach. Our results demonstrate that token concatenation is more parameter-efficient and achieves superior performance. Extensive evaluations show that our method outperforms existing 2D-lifting approaches, setting a new benchmark in generation quality, 3D consistency, and instruction fidelity.", "link": "http://arxiv.org/abs/2511.17501v1", "date": "2025-11-21", "relevancy": 2.3945, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6069}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6024}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Native%203D%20Editing%20with%20Full%20Attention&body=Title%3A%20Native%203D%20Editing%20with%20Full%20Attention%0AAuthor%3A%20Weiwei%20Cai%20and%20Shuangkang%20Fang%20and%20Weicai%20Ye%20and%20Xin%20Dong%20and%20Yunhan%20Yang%20and%20Xuanyang%20Zhang%20and%20Wei%20Cheng%20and%20Yanpei%20Cao%20and%20Gang%20Yu%20and%20Tao%20Chen%0AAbstract%3A%20Instruction-guided%203D%20editing%20is%20a%20rapidly%20emerging%20field%20with%20the%20potential%20to%20broaden%20access%20to%203D%20content%20creation.%20However%2C%20existing%20methods%20face%20critical%20limitations%3A%20optimization-based%20approaches%20are%20prohibitively%20slow%2C%20while%20feed-forward%20approaches%20relying%20on%20multi-view%202D%20editing%20often%20suffer%20from%20inconsistent%20geometry%20and%20degraded%20visual%20quality.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20native%203D%20editing%20framework%20that%20directly%20manipulates%203D%20representations%20in%20a%20single%2C%20efficient%20feed-forward%20pass.%20Specifically%2C%20we%20create%20a%20large-scale%2C%20multi-modal%20dataset%20for%20instruction-guided%203D%20editing%2C%20covering%20diverse%20addition%2C%20deletion%2C%20and%20modification%20tasks.%20This%20dataset%20is%20meticulously%20curated%20to%20ensure%20that%20edited%20objects%20faithfully%20adhere%20to%20the%20instructional%20changes%20while%20preserving%20the%20consistency%20of%20unedited%20regions%20with%20the%20source%20object.%20Building%20upon%20this%20dataset%2C%20we%20explore%20two%20distinct%20conditioning%20strategies%20for%20our%20model%3A%20a%20conventional%20cross-attention%20mechanism%20and%20a%20novel%203D%20token%20concatenation%20approach.%20Our%20results%20demonstrate%20that%20token%20concatenation%20is%20more%20parameter-efficient%20and%20achieves%20superior%20performance.%20Extensive%20evaluations%20show%20that%20our%20method%20outperforms%20existing%202D-lifting%20approaches%2C%20setting%20a%20new%20benchmark%20in%20generation%20quality%2C%203D%20consistency%2C%20and%20instruction%20fidelity.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17501v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNative%25203D%2520Editing%2520with%2520Full%2520Attention%26entry.906535625%3DWeiwei%2520Cai%2520and%2520Shuangkang%2520Fang%2520and%2520Weicai%2520Ye%2520and%2520Xin%2520Dong%2520and%2520Yunhan%2520Yang%2520and%2520Xuanyang%2520Zhang%2520and%2520Wei%2520Cheng%2520and%2520Yanpei%2520Cao%2520and%2520Gang%2520Yu%2520and%2520Tao%2520Chen%26entry.1292438233%3DInstruction-guided%25203D%2520editing%2520is%2520a%2520rapidly%2520emerging%2520field%2520with%2520the%2520potential%2520to%2520broaden%2520access%2520to%25203D%2520content%2520creation.%2520However%252C%2520existing%2520methods%2520face%2520critical%2520limitations%253A%2520optimization-based%2520approaches%2520are%2520prohibitively%2520slow%252C%2520while%2520feed-forward%2520approaches%2520relying%2520on%2520multi-view%25202D%2520editing%2520often%2520suffer%2520from%2520inconsistent%2520geometry%2520and%2520degraded%2520visual%2520quality.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520native%25203D%2520editing%2520framework%2520that%2520directly%2520manipulates%25203D%2520representations%2520in%2520a%2520single%252C%2520efficient%2520feed-forward%2520pass.%2520Specifically%252C%2520we%2520create%2520a%2520large-scale%252C%2520multi-modal%2520dataset%2520for%2520instruction-guided%25203D%2520editing%252C%2520covering%2520diverse%2520addition%252C%2520deletion%252C%2520and%2520modification%2520tasks.%2520This%2520dataset%2520is%2520meticulously%2520curated%2520to%2520ensure%2520that%2520edited%2520objects%2520faithfully%2520adhere%2520to%2520the%2520instructional%2520changes%2520while%2520preserving%2520the%2520consistency%2520of%2520unedited%2520regions%2520with%2520the%2520source%2520object.%2520Building%2520upon%2520this%2520dataset%252C%2520we%2520explore%2520two%2520distinct%2520conditioning%2520strategies%2520for%2520our%2520model%253A%2520a%2520conventional%2520cross-attention%2520mechanism%2520and%2520a%2520novel%25203D%2520token%2520concatenation%2520approach.%2520Our%2520results%2520demonstrate%2520that%2520token%2520concatenation%2520is%2520more%2520parameter-efficient%2520and%2520achieves%2520superior%2520performance.%2520Extensive%2520evaluations%2520show%2520that%2520our%2520method%2520outperforms%2520existing%25202D-lifting%2520approaches%252C%2520setting%2520a%2520new%2520benchmark%2520in%2520generation%2520quality%252C%25203D%2520consistency%252C%2520and%2520instruction%2520fidelity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17501v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Native%203D%20Editing%20with%20Full%20Attention&entry.906535625=Weiwei%20Cai%20and%20Shuangkang%20Fang%20and%20Weicai%20Ye%20and%20Xin%20Dong%20and%20Yunhan%20Yang%20and%20Xuanyang%20Zhang%20and%20Wei%20Cheng%20and%20Yanpei%20Cao%20and%20Gang%20Yu%20and%20Tao%20Chen&entry.1292438233=Instruction-guided%203D%20editing%20is%20a%20rapidly%20emerging%20field%20with%20the%20potential%20to%20broaden%20access%20to%203D%20content%20creation.%20However%2C%20existing%20methods%20face%20critical%20limitations%3A%20optimization-based%20approaches%20are%20prohibitively%20slow%2C%20while%20feed-forward%20approaches%20relying%20on%20multi-view%202D%20editing%20often%20suffer%20from%20inconsistent%20geometry%20and%20degraded%20visual%20quality.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20native%203D%20editing%20framework%20that%20directly%20manipulates%203D%20representations%20in%20a%20single%2C%20efficient%20feed-forward%20pass.%20Specifically%2C%20we%20create%20a%20large-scale%2C%20multi-modal%20dataset%20for%20instruction-guided%203D%20editing%2C%20covering%20diverse%20addition%2C%20deletion%2C%20and%20modification%20tasks.%20This%20dataset%20is%20meticulously%20curated%20to%20ensure%20that%20edited%20objects%20faithfully%20adhere%20to%20the%20instructional%20changes%20while%20preserving%20the%20consistency%20of%20unedited%20regions%20with%20the%20source%20object.%20Building%20upon%20this%20dataset%2C%20we%20explore%20two%20distinct%20conditioning%20strategies%20for%20our%20model%3A%20a%20conventional%20cross-attention%20mechanism%20and%20a%20novel%203D%20token%20concatenation%20approach.%20Our%20results%20demonstrate%20that%20token%20concatenation%20is%20more%20parameter-efficient%20and%20achieves%20superior%20performance.%20Extensive%20evaluations%20show%20that%20our%20method%20outperforms%20existing%202D-lifting%20approaches%2C%20setting%20a%20new%20benchmark%20in%20generation%20quality%2C%203D%20consistency%2C%20and%20instruction%20fidelity.&entry.1838667208=http%3A//arxiv.org/abs/2511.17501v1&entry.124074799=Read"},
{"title": "QueryOcc: Query-based Self-Supervision for 3D Semantic Occupancy", "author": "Adam Lilja and Ji Lan and Junsheng Fu and Lars Hammarstrand", "abstract": "Learning 3D scene geometry and semantics from images is a core challenge in computer vision and a key capability for autonomous driving. Since large-scale 3D annotation is prohibitively expensive, recent work explores self-supervised learning directly from sensor data without manual labels. Existing approaches either rely on 2D rendering consistency, where 3D structure emerges only implicitly, or on discretized voxel grids from accumulated lidar point clouds, limiting spatial precision and scalability. We introduce QueryOcc, a query-based self-supervised framework that learns continuous 3D semantic occupancy directly through independent 4D spatio-temporal queries sampled across adjacent frames. The framework supports supervision from either pseudo-point clouds derived from vision foundation models or raw lidar data. To enable long-range supervision and reasoning under constant memory, we introduce a contractive scene representation that preserves near-field detail while smoothly compressing distant regions. QueryOcc surpasses previous camera-based methods by 26% in semantic RayIoU on the self-supervised Occ3D-nuScenes benchmark while running at 11.6 FPS, demonstrating that direct 4D query supervision enables strong self-supervised occupancy learning. https://research.zenseact.com/publications/queryocc/", "link": "http://arxiv.org/abs/2511.17221v1", "date": "2025-11-21", "relevancy": 2.3766, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5971}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5971}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QueryOcc%3A%20Query-based%20Self-Supervision%20for%203D%20Semantic%20Occupancy&body=Title%3A%20QueryOcc%3A%20Query-based%20Self-Supervision%20for%203D%20Semantic%20Occupancy%0AAuthor%3A%20Adam%20Lilja%20and%20Ji%20Lan%20and%20Junsheng%20Fu%20and%20Lars%20Hammarstrand%0AAbstract%3A%20Learning%203D%20scene%20geometry%20and%20semantics%20from%20images%20is%20a%20core%20challenge%20in%20computer%20vision%20and%20a%20key%20capability%20for%20autonomous%20driving.%20Since%20large-scale%203D%20annotation%20is%20prohibitively%20expensive%2C%20recent%20work%20explores%20self-supervised%20learning%20directly%20from%20sensor%20data%20without%20manual%20labels.%20Existing%20approaches%20either%20rely%20on%202D%20rendering%20consistency%2C%20where%203D%20structure%20emerges%20only%20implicitly%2C%20or%20on%20discretized%20voxel%20grids%20from%20accumulated%20lidar%20point%20clouds%2C%20limiting%20spatial%20precision%20and%20scalability.%20We%20introduce%20QueryOcc%2C%20a%20query-based%20self-supervised%20framework%20that%20learns%20continuous%203D%20semantic%20occupancy%20directly%20through%20independent%204D%20spatio-temporal%20queries%20sampled%20across%20adjacent%20frames.%20The%20framework%20supports%20supervision%20from%20either%20pseudo-point%20clouds%20derived%20from%20vision%20foundation%20models%20or%20raw%20lidar%20data.%20To%20enable%20long-range%20supervision%20and%20reasoning%20under%20constant%20memory%2C%20we%20introduce%20a%20contractive%20scene%20representation%20that%20preserves%20near-field%20detail%20while%20smoothly%20compressing%20distant%20regions.%20QueryOcc%20surpasses%20previous%20camera-based%20methods%20by%2026%25%20in%20semantic%20RayIoU%20on%20the%20self-supervised%20Occ3D-nuScenes%20benchmark%20while%20running%20at%2011.6%20FPS%2C%20demonstrating%20that%20direct%204D%20query%20supervision%20enables%20strong%20self-supervised%20occupancy%20learning.%20https%3A//research.zenseact.com/publications/queryocc/%0ALink%3A%20http%3A//arxiv.org/abs/2511.17221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQueryOcc%253A%2520Query-based%2520Self-Supervision%2520for%25203D%2520Semantic%2520Occupancy%26entry.906535625%3DAdam%2520Lilja%2520and%2520Ji%2520Lan%2520and%2520Junsheng%2520Fu%2520and%2520Lars%2520Hammarstrand%26entry.1292438233%3DLearning%25203D%2520scene%2520geometry%2520and%2520semantics%2520from%2520images%2520is%2520a%2520core%2520challenge%2520in%2520computer%2520vision%2520and%2520a%2520key%2520capability%2520for%2520autonomous%2520driving.%2520Since%2520large-scale%25203D%2520annotation%2520is%2520prohibitively%2520expensive%252C%2520recent%2520work%2520explores%2520self-supervised%2520learning%2520directly%2520from%2520sensor%2520data%2520without%2520manual%2520labels.%2520Existing%2520approaches%2520either%2520rely%2520on%25202D%2520rendering%2520consistency%252C%2520where%25203D%2520structure%2520emerges%2520only%2520implicitly%252C%2520or%2520on%2520discretized%2520voxel%2520grids%2520from%2520accumulated%2520lidar%2520point%2520clouds%252C%2520limiting%2520spatial%2520precision%2520and%2520scalability.%2520We%2520introduce%2520QueryOcc%252C%2520a%2520query-based%2520self-supervised%2520framework%2520that%2520learns%2520continuous%25203D%2520semantic%2520occupancy%2520directly%2520through%2520independent%25204D%2520spatio-temporal%2520queries%2520sampled%2520across%2520adjacent%2520frames.%2520The%2520framework%2520supports%2520supervision%2520from%2520either%2520pseudo-point%2520clouds%2520derived%2520from%2520vision%2520foundation%2520models%2520or%2520raw%2520lidar%2520data.%2520To%2520enable%2520long-range%2520supervision%2520and%2520reasoning%2520under%2520constant%2520memory%252C%2520we%2520introduce%2520a%2520contractive%2520scene%2520representation%2520that%2520preserves%2520near-field%2520detail%2520while%2520smoothly%2520compressing%2520distant%2520regions.%2520QueryOcc%2520surpasses%2520previous%2520camera-based%2520methods%2520by%252026%2525%2520in%2520semantic%2520RayIoU%2520on%2520the%2520self-supervised%2520Occ3D-nuScenes%2520benchmark%2520while%2520running%2520at%252011.6%2520FPS%252C%2520demonstrating%2520that%2520direct%25204D%2520query%2520supervision%2520enables%2520strong%2520self-supervised%2520occupancy%2520learning.%2520https%253A//research.zenseact.com/publications/queryocc/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QueryOcc%3A%20Query-based%20Self-Supervision%20for%203D%20Semantic%20Occupancy&entry.906535625=Adam%20Lilja%20and%20Ji%20Lan%20and%20Junsheng%20Fu%20and%20Lars%20Hammarstrand&entry.1292438233=Learning%203D%20scene%20geometry%20and%20semantics%20from%20images%20is%20a%20core%20challenge%20in%20computer%20vision%20and%20a%20key%20capability%20for%20autonomous%20driving.%20Since%20large-scale%203D%20annotation%20is%20prohibitively%20expensive%2C%20recent%20work%20explores%20self-supervised%20learning%20directly%20from%20sensor%20data%20without%20manual%20labels.%20Existing%20approaches%20either%20rely%20on%202D%20rendering%20consistency%2C%20where%203D%20structure%20emerges%20only%20implicitly%2C%20or%20on%20discretized%20voxel%20grids%20from%20accumulated%20lidar%20point%20clouds%2C%20limiting%20spatial%20precision%20and%20scalability.%20We%20introduce%20QueryOcc%2C%20a%20query-based%20self-supervised%20framework%20that%20learns%20continuous%203D%20semantic%20occupancy%20directly%20through%20independent%204D%20spatio-temporal%20queries%20sampled%20across%20adjacent%20frames.%20The%20framework%20supports%20supervision%20from%20either%20pseudo-point%20clouds%20derived%20from%20vision%20foundation%20models%20or%20raw%20lidar%20data.%20To%20enable%20long-range%20supervision%20and%20reasoning%20under%20constant%20memory%2C%20we%20introduce%20a%20contractive%20scene%20representation%20that%20preserves%20near-field%20detail%20while%20smoothly%20compressing%20distant%20regions.%20QueryOcc%20surpasses%20previous%20camera-based%20methods%20by%2026%25%20in%20semantic%20RayIoU%20on%20the%20self-supervised%20Occ3D-nuScenes%20benchmark%20while%20running%20at%2011.6%20FPS%2C%20demonstrating%20that%20direct%204D%20query%20supervision%20enables%20strong%20self-supervised%20occupancy%20learning.%20https%3A//research.zenseact.com/publications/queryocc/&entry.1838667208=http%3A//arxiv.org/abs/2511.17221v1&entry.124074799=Read"},
{"title": "RapidPoseTriangulation: Multi-view Multi-person Whole-body Human Pose Triangulation in a Millisecond", "author": "Daniel Bermuth and Alexander Poeppel and Wolfgang Reif", "abstract": "The integration of multi-view imaging and pose estimation represents a significant advance in computer vision applications, offering new possibilities for understanding human movement and interactions. This work presents a new algorithm that improves multi-view multi-person pose estimation, focusing on fast triangulation speeds and good generalization capabilities. The approach extends to whole-body pose estimation, capturing details from facial expressions to finger movements across multiple individuals and viewpoints. Adaptability to different settings is demonstrated through strong performance across unseen datasets and configurations. To support further progress in this field, all of this work is publicly accessible.", "link": "http://arxiv.org/abs/2503.21692v4", "date": "2025-11-21", "relevancy": 2.3723, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5992}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.597}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RapidPoseTriangulation%3A%20Multi-view%20Multi-person%20Whole-body%20Human%20Pose%20Triangulation%20in%20a%20Millisecond&body=Title%3A%20RapidPoseTriangulation%3A%20Multi-view%20Multi-person%20Whole-body%20Human%20Pose%20Triangulation%20in%20a%20Millisecond%0AAuthor%3A%20Daniel%20Bermuth%20and%20Alexander%20Poeppel%20and%20Wolfgang%20Reif%0AAbstract%3A%20The%20integration%20of%20multi-view%20imaging%20and%20pose%20estimation%20represents%20a%20significant%20advance%20in%20computer%20vision%20applications%2C%20offering%20new%20possibilities%20for%20understanding%20human%20movement%20and%20interactions.%20This%20work%20presents%20a%20new%20algorithm%20that%20improves%20multi-view%20multi-person%20pose%20estimation%2C%20focusing%20on%20fast%20triangulation%20speeds%20and%20good%20generalization%20capabilities.%20The%20approach%20extends%20to%20whole-body%20pose%20estimation%2C%20capturing%20details%20from%20facial%20expressions%20to%20finger%20movements%20across%20multiple%20individuals%20and%20viewpoints.%20Adaptability%20to%20different%20settings%20is%20demonstrated%20through%20strong%20performance%20across%20unseen%20datasets%20and%20configurations.%20To%20support%20further%20progress%20in%20this%20field%2C%20all%20of%20this%20work%20is%20publicly%20accessible.%0ALink%3A%20http%3A//arxiv.org/abs/2503.21692v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRapidPoseTriangulation%253A%2520Multi-view%2520Multi-person%2520Whole-body%2520Human%2520Pose%2520Triangulation%2520in%2520a%2520Millisecond%26entry.906535625%3DDaniel%2520Bermuth%2520and%2520Alexander%2520Poeppel%2520and%2520Wolfgang%2520Reif%26entry.1292438233%3DThe%2520integration%2520of%2520multi-view%2520imaging%2520and%2520pose%2520estimation%2520represents%2520a%2520significant%2520advance%2520in%2520computer%2520vision%2520applications%252C%2520offering%2520new%2520possibilities%2520for%2520understanding%2520human%2520movement%2520and%2520interactions.%2520This%2520work%2520presents%2520a%2520new%2520algorithm%2520that%2520improves%2520multi-view%2520multi-person%2520pose%2520estimation%252C%2520focusing%2520on%2520fast%2520triangulation%2520speeds%2520and%2520good%2520generalization%2520capabilities.%2520The%2520approach%2520extends%2520to%2520whole-body%2520pose%2520estimation%252C%2520capturing%2520details%2520from%2520facial%2520expressions%2520to%2520finger%2520movements%2520across%2520multiple%2520individuals%2520and%2520viewpoints.%2520Adaptability%2520to%2520different%2520settings%2520is%2520demonstrated%2520through%2520strong%2520performance%2520across%2520unseen%2520datasets%2520and%2520configurations.%2520To%2520support%2520further%2520progress%2520in%2520this%2520field%252C%2520all%2520of%2520this%2520work%2520is%2520publicly%2520accessible.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21692v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RapidPoseTriangulation%3A%20Multi-view%20Multi-person%20Whole-body%20Human%20Pose%20Triangulation%20in%20a%20Millisecond&entry.906535625=Daniel%20Bermuth%20and%20Alexander%20Poeppel%20and%20Wolfgang%20Reif&entry.1292438233=The%20integration%20of%20multi-view%20imaging%20and%20pose%20estimation%20represents%20a%20significant%20advance%20in%20computer%20vision%20applications%2C%20offering%20new%20possibilities%20for%20understanding%20human%20movement%20and%20interactions.%20This%20work%20presents%20a%20new%20algorithm%20that%20improves%20multi-view%20multi-person%20pose%20estimation%2C%20focusing%20on%20fast%20triangulation%20speeds%20and%20good%20generalization%20capabilities.%20The%20approach%20extends%20to%20whole-body%20pose%20estimation%2C%20capturing%20details%20from%20facial%20expressions%20to%20finger%20movements%20across%20multiple%20individuals%20and%20viewpoints.%20Adaptability%20to%20different%20settings%20is%20demonstrated%20through%20strong%20performance%20across%20unseen%20datasets%20and%20configurations.%20To%20support%20further%20progress%20in%20this%20field%2C%20all%20of%20this%20work%20is%20publicly%20accessible.&entry.1838667208=http%3A//arxiv.org/abs/2503.21692v4&entry.124074799=Read"},
{"title": "Automated Interpretable 2D Video Extraction from 3D Echocardiography", "author": "Milos Vukadinovic and Hirotaka Ieki and Yuki Sahashi and David Ouyang and Bryan He", "abstract": "Although the heart has complex three-dimensional (3D) anatomy, conventional medical imaging with cardiac ultrasound relies on a series of 2D videos showing individual cardiac structures. 3D echocardiography is a developing modality that now offers adequate image quality for clinical use, with potential to streamline acquisition and improve assessment of off-axis features. We propose an automated method to select standard 2D views from 3D cardiac ultrasound volumes, allowing physicians to interpret the data in their usual format while benefiting from the speed and usability of 3D scanning. Applying a deep learning view classifier and downstream heuristics based on anatomical landmarks together with heuristics provided by cardiologists, we reconstruct standard echocardiography views. This approach was validated by three cardiologists in blinded evaluation (96\\% accuracy in 1,600 videos from 2 hospitals). The downstream 2D videos were also validated in their ability to detect cardiac abnormalities using AI echocardiography models (EchoPrime and PanEcho) as well as ability to generate clinical-grade measurements of cardiac anatomy (EchoNet-Measurement). We demonstrated that the extracted 2D videos preserve spatial calibration and diagnostic features, allowing clinicians to obtain accurate real-world interpretations from 3D volumes. We release the code and a dataset of 29 3D echocardiography videos https://github.com/echonet/3d-echo .", "link": "http://arxiv.org/abs/2511.15946v2", "date": "2025-11-21", "relevancy": 2.3701, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5957}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5919}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Interpretable%202D%20Video%20Extraction%20from%203D%20Echocardiography&body=Title%3A%20Automated%20Interpretable%202D%20Video%20Extraction%20from%203D%20Echocardiography%0AAuthor%3A%20Milos%20Vukadinovic%20and%20Hirotaka%20Ieki%20and%20Yuki%20Sahashi%20and%20David%20Ouyang%20and%20Bryan%20He%0AAbstract%3A%20Although%20the%20heart%20has%20complex%20three-dimensional%20%283D%29%20anatomy%2C%20conventional%20medical%20imaging%20with%20cardiac%20ultrasound%20relies%20on%20a%20series%20of%202D%20videos%20showing%20individual%20cardiac%20structures.%203D%20echocardiography%20is%20a%20developing%20modality%20that%20now%20offers%20adequate%20image%20quality%20for%20clinical%20use%2C%20with%20potential%20to%20streamline%20acquisition%20and%20improve%20assessment%20of%20off-axis%20features.%20We%20propose%20an%20automated%20method%20to%20select%20standard%202D%20views%20from%203D%20cardiac%20ultrasound%20volumes%2C%20allowing%20physicians%20to%20interpret%20the%20data%20in%20their%20usual%20format%20while%20benefiting%20from%20the%20speed%20and%20usability%20of%203D%20scanning.%20Applying%20a%20deep%20learning%20view%20classifier%20and%20downstream%20heuristics%20based%20on%20anatomical%20landmarks%20together%20with%20heuristics%20provided%20by%20cardiologists%2C%20we%20reconstruct%20standard%20echocardiography%20views.%20This%20approach%20was%20validated%20by%20three%20cardiologists%20in%20blinded%20evaluation%20%2896%5C%25%20accuracy%20in%201%2C600%20videos%20from%202%20hospitals%29.%20The%20downstream%202D%20videos%20were%20also%20validated%20in%20their%20ability%20to%20detect%20cardiac%20abnormalities%20using%20AI%20echocardiography%20models%20%28EchoPrime%20and%20PanEcho%29%20as%20well%20as%20ability%20to%20generate%20clinical-grade%20measurements%20of%20cardiac%20anatomy%20%28EchoNet-Measurement%29.%20We%20demonstrated%20that%20the%20extracted%202D%20videos%20preserve%20spatial%20calibration%20and%20diagnostic%20features%2C%20allowing%20clinicians%20to%20obtain%20accurate%20real-world%20interpretations%20from%203D%20volumes.%20We%20release%20the%20code%20and%20a%20dataset%20of%2029%203D%20echocardiography%20videos%20https%3A//github.com/echonet/3d-echo%20.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15946v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Interpretable%25202D%2520Video%2520Extraction%2520from%25203D%2520Echocardiography%26entry.906535625%3DMilos%2520Vukadinovic%2520and%2520Hirotaka%2520Ieki%2520and%2520Yuki%2520Sahashi%2520and%2520David%2520Ouyang%2520and%2520Bryan%2520He%26entry.1292438233%3DAlthough%2520the%2520heart%2520has%2520complex%2520three-dimensional%2520%25283D%2529%2520anatomy%252C%2520conventional%2520medical%2520imaging%2520with%2520cardiac%2520ultrasound%2520relies%2520on%2520a%2520series%2520of%25202D%2520videos%2520showing%2520individual%2520cardiac%2520structures.%25203D%2520echocardiography%2520is%2520a%2520developing%2520modality%2520that%2520now%2520offers%2520adequate%2520image%2520quality%2520for%2520clinical%2520use%252C%2520with%2520potential%2520to%2520streamline%2520acquisition%2520and%2520improve%2520assessment%2520of%2520off-axis%2520features.%2520We%2520propose%2520an%2520automated%2520method%2520to%2520select%2520standard%25202D%2520views%2520from%25203D%2520cardiac%2520ultrasound%2520volumes%252C%2520allowing%2520physicians%2520to%2520interpret%2520the%2520data%2520in%2520their%2520usual%2520format%2520while%2520benefiting%2520from%2520the%2520speed%2520and%2520usability%2520of%25203D%2520scanning.%2520Applying%2520a%2520deep%2520learning%2520view%2520classifier%2520and%2520downstream%2520heuristics%2520based%2520on%2520anatomical%2520landmarks%2520together%2520with%2520heuristics%2520provided%2520by%2520cardiologists%252C%2520we%2520reconstruct%2520standard%2520echocardiography%2520views.%2520This%2520approach%2520was%2520validated%2520by%2520three%2520cardiologists%2520in%2520blinded%2520evaluation%2520%252896%255C%2525%2520accuracy%2520in%25201%252C600%2520videos%2520from%25202%2520hospitals%2529.%2520The%2520downstream%25202D%2520videos%2520were%2520also%2520validated%2520in%2520their%2520ability%2520to%2520detect%2520cardiac%2520abnormalities%2520using%2520AI%2520echocardiography%2520models%2520%2528EchoPrime%2520and%2520PanEcho%2529%2520as%2520well%2520as%2520ability%2520to%2520generate%2520clinical-grade%2520measurements%2520of%2520cardiac%2520anatomy%2520%2528EchoNet-Measurement%2529.%2520We%2520demonstrated%2520that%2520the%2520extracted%25202D%2520videos%2520preserve%2520spatial%2520calibration%2520and%2520diagnostic%2520features%252C%2520allowing%2520clinicians%2520to%2520obtain%2520accurate%2520real-world%2520interpretations%2520from%25203D%2520volumes.%2520We%2520release%2520the%2520code%2520and%2520a%2520dataset%2520of%252029%25203D%2520echocardiography%2520videos%2520https%253A//github.com/echonet/3d-echo%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15946v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Interpretable%202D%20Video%20Extraction%20from%203D%20Echocardiography&entry.906535625=Milos%20Vukadinovic%20and%20Hirotaka%20Ieki%20and%20Yuki%20Sahashi%20and%20David%20Ouyang%20and%20Bryan%20He&entry.1292438233=Although%20the%20heart%20has%20complex%20three-dimensional%20%283D%29%20anatomy%2C%20conventional%20medical%20imaging%20with%20cardiac%20ultrasound%20relies%20on%20a%20series%20of%202D%20videos%20showing%20individual%20cardiac%20structures.%203D%20echocardiography%20is%20a%20developing%20modality%20that%20now%20offers%20adequate%20image%20quality%20for%20clinical%20use%2C%20with%20potential%20to%20streamline%20acquisition%20and%20improve%20assessment%20of%20off-axis%20features.%20We%20propose%20an%20automated%20method%20to%20select%20standard%202D%20views%20from%203D%20cardiac%20ultrasound%20volumes%2C%20allowing%20physicians%20to%20interpret%20the%20data%20in%20their%20usual%20format%20while%20benefiting%20from%20the%20speed%20and%20usability%20of%203D%20scanning.%20Applying%20a%20deep%20learning%20view%20classifier%20and%20downstream%20heuristics%20based%20on%20anatomical%20landmarks%20together%20with%20heuristics%20provided%20by%20cardiologists%2C%20we%20reconstruct%20standard%20echocardiography%20views.%20This%20approach%20was%20validated%20by%20three%20cardiologists%20in%20blinded%20evaluation%20%2896%5C%25%20accuracy%20in%201%2C600%20videos%20from%202%20hospitals%29.%20The%20downstream%202D%20videos%20were%20also%20validated%20in%20their%20ability%20to%20detect%20cardiac%20abnormalities%20using%20AI%20echocardiography%20models%20%28EchoPrime%20and%20PanEcho%29%20as%20well%20as%20ability%20to%20generate%20clinical-grade%20measurements%20of%20cardiac%20anatomy%20%28EchoNet-Measurement%29.%20We%20demonstrated%20that%20the%20extracted%202D%20videos%20preserve%20spatial%20calibration%20and%20diagnostic%20features%2C%20allowing%20clinicians%20to%20obtain%20accurate%20real-world%20interpretations%20from%203D%20volumes.%20We%20release%20the%20code%20and%20a%20dataset%20of%2029%203D%20echocardiography%20videos%20https%3A//github.com/echonet/3d-echo%20.&entry.1838667208=http%3A//arxiv.org/abs/2511.15946v2&entry.124074799=Read"},
{"title": "LLM one-shot style transfer for Authorship Attribution and Verification", "author": "Pablo Miralles-Gonz\u00e1lez and Javier Huertas-Tato and Alejandro Mart\u00edn and David Camacho", "abstract": "Computational stylometry analyzes writing style through quantitative patterns in text, supporting applications from forensic tasks such as identity linking and plagiarism detection to literary attribution in the humanities. Supervised and contrastive approaches rely on data with spurious correlations and often confuse style with topic. Despite their natural use in AI-generated text detection, the CLM pre-training of modern LLMs has been scarcely leveraged for general authorship problems. We propose a novel unsupervised approach based on this extensive pre-training and the in-context learning capabilities of LLMs, employing the log-probabilities of an LLM to measure style transferability from one text to another. Our method significantly outperforms LLM prompting approaches of comparable scale and achieves higher accuracy than contrastively trained baselines when controlling for topical correlations. Moreover, performance scales fairly consistently with the size of the base model and, in the case of authorship verification, with an additional mechanism that increases test-time computation; enabling flexible trade-offs between computational cost and accuracy.", "link": "http://arxiv.org/abs/2510.13302v2", "date": "2025-11-21", "relevancy": 2.3565, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4804}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4793}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20one-shot%20style%20transfer%20for%20Authorship%20Attribution%20and%20Verification&body=Title%3A%20LLM%20one-shot%20style%20transfer%20for%20Authorship%20Attribution%20and%20Verification%0AAuthor%3A%20Pablo%20Miralles-Gonz%C3%A1lez%20and%20Javier%20Huertas-Tato%20and%20Alejandro%20Mart%C3%ADn%20and%20David%20Camacho%0AAbstract%3A%20Computational%20stylometry%20analyzes%20writing%20style%20through%20quantitative%20patterns%20in%20text%2C%20supporting%20applications%20from%20forensic%20tasks%20such%20as%20identity%20linking%20and%20plagiarism%20detection%20to%20literary%20attribution%20in%20the%20humanities.%20Supervised%20and%20contrastive%20approaches%20rely%20on%20data%20with%20spurious%20correlations%20and%20often%20confuse%20style%20with%20topic.%20Despite%20their%20natural%20use%20in%20AI-generated%20text%20detection%2C%20the%20CLM%20pre-training%20of%20modern%20LLMs%20has%20been%20scarcely%20leveraged%20for%20general%20authorship%20problems.%20We%20propose%20a%20novel%20unsupervised%20approach%20based%20on%20this%20extensive%20pre-training%20and%20the%20in-context%20learning%20capabilities%20of%20LLMs%2C%20employing%20the%20log-probabilities%20of%20an%20LLM%20to%20measure%20style%20transferability%20from%20one%20text%20to%20another.%20Our%20method%20significantly%20outperforms%20LLM%20prompting%20approaches%20of%20comparable%20scale%20and%20achieves%20higher%20accuracy%20than%20contrastively%20trained%20baselines%20when%20controlling%20for%20topical%20correlations.%20Moreover%2C%20performance%20scales%20fairly%20consistently%20with%20the%20size%20of%20the%20base%20model%20and%2C%20in%20the%20case%20of%20authorship%20verification%2C%20with%20an%20additional%20mechanism%20that%20increases%20test-time%20computation%3B%20enabling%20flexible%20trade-offs%20between%20computational%20cost%20and%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2510.13302v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520one-shot%2520style%2520transfer%2520for%2520Authorship%2520Attribution%2520and%2520Verification%26entry.906535625%3DPablo%2520Miralles-Gonz%25C3%25A1lez%2520and%2520Javier%2520Huertas-Tato%2520and%2520Alejandro%2520Mart%25C3%25ADn%2520and%2520David%2520Camacho%26entry.1292438233%3DComputational%2520stylometry%2520analyzes%2520writing%2520style%2520through%2520quantitative%2520patterns%2520in%2520text%252C%2520supporting%2520applications%2520from%2520forensic%2520tasks%2520such%2520as%2520identity%2520linking%2520and%2520plagiarism%2520detection%2520to%2520literary%2520attribution%2520in%2520the%2520humanities.%2520Supervised%2520and%2520contrastive%2520approaches%2520rely%2520on%2520data%2520with%2520spurious%2520correlations%2520and%2520often%2520confuse%2520style%2520with%2520topic.%2520Despite%2520their%2520natural%2520use%2520in%2520AI-generated%2520text%2520detection%252C%2520the%2520CLM%2520pre-training%2520of%2520modern%2520LLMs%2520has%2520been%2520scarcely%2520leveraged%2520for%2520general%2520authorship%2520problems.%2520We%2520propose%2520a%2520novel%2520unsupervised%2520approach%2520based%2520on%2520this%2520extensive%2520pre-training%2520and%2520the%2520in-context%2520learning%2520capabilities%2520of%2520LLMs%252C%2520employing%2520the%2520log-probabilities%2520of%2520an%2520LLM%2520to%2520measure%2520style%2520transferability%2520from%2520one%2520text%2520to%2520another.%2520Our%2520method%2520significantly%2520outperforms%2520LLM%2520prompting%2520approaches%2520of%2520comparable%2520scale%2520and%2520achieves%2520higher%2520accuracy%2520than%2520contrastively%2520trained%2520baselines%2520when%2520controlling%2520for%2520topical%2520correlations.%2520Moreover%252C%2520performance%2520scales%2520fairly%2520consistently%2520with%2520the%2520size%2520of%2520the%2520base%2520model%2520and%252C%2520in%2520the%2520case%2520of%2520authorship%2520verification%252C%2520with%2520an%2520additional%2520mechanism%2520that%2520increases%2520test-time%2520computation%253B%2520enabling%2520flexible%2520trade-offs%2520between%2520computational%2520cost%2520and%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13302v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20one-shot%20style%20transfer%20for%20Authorship%20Attribution%20and%20Verification&entry.906535625=Pablo%20Miralles-Gonz%C3%A1lez%20and%20Javier%20Huertas-Tato%20and%20Alejandro%20Mart%C3%ADn%20and%20David%20Camacho&entry.1292438233=Computational%20stylometry%20analyzes%20writing%20style%20through%20quantitative%20patterns%20in%20text%2C%20supporting%20applications%20from%20forensic%20tasks%20such%20as%20identity%20linking%20and%20plagiarism%20detection%20to%20literary%20attribution%20in%20the%20humanities.%20Supervised%20and%20contrastive%20approaches%20rely%20on%20data%20with%20spurious%20correlations%20and%20often%20confuse%20style%20with%20topic.%20Despite%20their%20natural%20use%20in%20AI-generated%20text%20detection%2C%20the%20CLM%20pre-training%20of%20modern%20LLMs%20has%20been%20scarcely%20leveraged%20for%20general%20authorship%20problems.%20We%20propose%20a%20novel%20unsupervised%20approach%20based%20on%20this%20extensive%20pre-training%20and%20the%20in-context%20learning%20capabilities%20of%20LLMs%2C%20employing%20the%20log-probabilities%20of%20an%20LLM%20to%20measure%20style%20transferability%20from%20one%20text%20to%20another.%20Our%20method%20significantly%20outperforms%20LLM%20prompting%20approaches%20of%20comparable%20scale%20and%20achieves%20higher%20accuracy%20than%20contrastively%20trained%20baselines%20when%20controlling%20for%20topical%20correlations.%20Moreover%2C%20performance%20scales%20fairly%20consistently%20with%20the%20size%20of%20the%20base%20model%20and%2C%20in%20the%20case%20of%20authorship%20verification%2C%20with%20an%20additional%20mechanism%20that%20increases%20test-time%20computation%3B%20enabling%20flexible%20trade-offs%20between%20computational%20cost%20and%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2510.13302v2&entry.124074799=Read"},
{"title": "Estimating Global Input Relevance and Enforcing Sparse Representations with a Scalable Spectral Neural Network Approach", "author": "Lorenzo Chicchi and Lorenzo Buffoni and Diego Febbe and Lorenzo Giambagli and Raffaele Marino and Duccio Fanelli", "abstract": "In machine learning practice it is often useful to identify relevant input features. Isolating key input elements, ranked according their respective degree of relevance, can help to elaborate on the process of decision making. Here, we propose a novel method to estimate the relative importance of the input components for a Deep Neural Network. This is achieved by leveraging on a spectral re-parametrization of the optimization process. Eigenvalues associated to input nodes provide in fact a robust proxy to gauge the relevance of the supplied entry features. Notably, the spectral features ranking is performed automatically, as a byproduct of the network training, with no additional processing to be carried out. Moreover, by leveraging on the regularization of the eigenvalues, it is possible to enforce solutions making use of a minimum subset of the input components, increasing the explainability of the model and providing sparse input representations. The technique is compared to the most common methods in the literature and is successfully challenged against both synthetic and real data.", "link": "http://arxiv.org/abs/2406.01183v3", "date": "2025-11-21", "relevancy": 2.3561, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4843}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.467}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20Global%20Input%20Relevance%20and%20Enforcing%20Sparse%20Representations%20with%20a%20Scalable%20Spectral%20Neural%20Network%20Approach&body=Title%3A%20Estimating%20Global%20Input%20Relevance%20and%20Enforcing%20Sparse%20Representations%20with%20a%20Scalable%20Spectral%20Neural%20Network%20Approach%0AAuthor%3A%20Lorenzo%20Chicchi%20and%20Lorenzo%20Buffoni%20and%20Diego%20Febbe%20and%20Lorenzo%20Giambagli%20and%20Raffaele%20Marino%20and%20Duccio%20Fanelli%0AAbstract%3A%20In%20machine%20learning%20practice%20it%20is%20often%20useful%20to%20identify%20relevant%20input%20features.%20Isolating%20key%20input%20elements%2C%20ranked%20according%20their%20respective%20degree%20of%20relevance%2C%20can%20help%20to%20elaborate%20on%20the%20process%20of%20decision%20making.%20Here%2C%20we%20propose%20a%20novel%20method%20to%20estimate%20the%20relative%20importance%20of%20the%20input%20components%20for%20a%20Deep%20Neural%20Network.%20This%20is%20achieved%20by%20leveraging%20on%20a%20spectral%20re-parametrization%20of%20the%20optimization%20process.%20Eigenvalues%20associated%20to%20input%20nodes%20provide%20in%20fact%20a%20robust%20proxy%20to%20gauge%20the%20relevance%20of%20the%20supplied%20entry%20features.%20Notably%2C%20the%20spectral%20features%20ranking%20is%20performed%20automatically%2C%20as%20a%20byproduct%20of%20the%20network%20training%2C%20with%20no%20additional%20processing%20to%20be%20carried%20out.%20Moreover%2C%20by%20leveraging%20on%20the%20regularization%20of%20the%20eigenvalues%2C%20it%20is%20possible%20to%20enforce%20solutions%20making%20use%20of%20a%20minimum%20subset%20of%20the%20input%20components%2C%20increasing%20the%20explainability%20of%20the%20model%20and%20providing%20sparse%20input%20representations.%20The%20technique%20is%20compared%20to%20the%20most%20common%20methods%20in%20the%20literature%20and%20is%20successfully%20challenged%20against%20both%20synthetic%20and%20real%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2406.01183v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520Global%2520Input%2520Relevance%2520and%2520Enforcing%2520Sparse%2520Representations%2520with%2520a%2520Scalable%2520Spectral%2520Neural%2520Network%2520Approach%26entry.906535625%3DLorenzo%2520Chicchi%2520and%2520Lorenzo%2520Buffoni%2520and%2520Diego%2520Febbe%2520and%2520Lorenzo%2520Giambagli%2520and%2520Raffaele%2520Marino%2520and%2520Duccio%2520Fanelli%26entry.1292438233%3DIn%2520machine%2520learning%2520practice%2520it%2520is%2520often%2520useful%2520to%2520identify%2520relevant%2520input%2520features.%2520Isolating%2520key%2520input%2520elements%252C%2520ranked%2520according%2520their%2520respective%2520degree%2520of%2520relevance%252C%2520can%2520help%2520to%2520elaborate%2520on%2520the%2520process%2520of%2520decision%2520making.%2520Here%252C%2520we%2520propose%2520a%2520novel%2520method%2520to%2520estimate%2520the%2520relative%2520importance%2520of%2520the%2520input%2520components%2520for%2520a%2520Deep%2520Neural%2520Network.%2520This%2520is%2520achieved%2520by%2520leveraging%2520on%2520a%2520spectral%2520re-parametrization%2520of%2520the%2520optimization%2520process.%2520Eigenvalues%2520associated%2520to%2520input%2520nodes%2520provide%2520in%2520fact%2520a%2520robust%2520proxy%2520to%2520gauge%2520the%2520relevance%2520of%2520the%2520supplied%2520entry%2520features.%2520Notably%252C%2520the%2520spectral%2520features%2520ranking%2520is%2520performed%2520automatically%252C%2520as%2520a%2520byproduct%2520of%2520the%2520network%2520training%252C%2520with%2520no%2520additional%2520processing%2520to%2520be%2520carried%2520out.%2520Moreover%252C%2520by%2520leveraging%2520on%2520the%2520regularization%2520of%2520the%2520eigenvalues%252C%2520it%2520is%2520possible%2520to%2520enforce%2520solutions%2520making%2520use%2520of%2520a%2520minimum%2520subset%2520of%2520the%2520input%2520components%252C%2520increasing%2520the%2520explainability%2520of%2520the%2520model%2520and%2520providing%2520sparse%2520input%2520representations.%2520The%2520technique%2520is%2520compared%2520to%2520the%2520most%2520common%2520methods%2520in%2520the%2520literature%2520and%2520is%2520successfully%2520challenged%2520against%2520both%2520synthetic%2520and%2520real%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01183v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20Global%20Input%20Relevance%20and%20Enforcing%20Sparse%20Representations%20with%20a%20Scalable%20Spectral%20Neural%20Network%20Approach&entry.906535625=Lorenzo%20Chicchi%20and%20Lorenzo%20Buffoni%20and%20Diego%20Febbe%20and%20Lorenzo%20Giambagli%20and%20Raffaele%20Marino%20and%20Duccio%20Fanelli&entry.1292438233=In%20machine%20learning%20practice%20it%20is%20often%20useful%20to%20identify%20relevant%20input%20features.%20Isolating%20key%20input%20elements%2C%20ranked%20according%20their%20respective%20degree%20of%20relevance%2C%20can%20help%20to%20elaborate%20on%20the%20process%20of%20decision%20making.%20Here%2C%20we%20propose%20a%20novel%20method%20to%20estimate%20the%20relative%20importance%20of%20the%20input%20components%20for%20a%20Deep%20Neural%20Network.%20This%20is%20achieved%20by%20leveraging%20on%20a%20spectral%20re-parametrization%20of%20the%20optimization%20process.%20Eigenvalues%20associated%20to%20input%20nodes%20provide%20in%20fact%20a%20robust%20proxy%20to%20gauge%20the%20relevance%20of%20the%20supplied%20entry%20features.%20Notably%2C%20the%20spectral%20features%20ranking%20is%20performed%20automatically%2C%20as%20a%20byproduct%20of%20the%20network%20training%2C%20with%20no%20additional%20processing%20to%20be%20carried%20out.%20Moreover%2C%20by%20leveraging%20on%20the%20regularization%20of%20the%20eigenvalues%2C%20it%20is%20possible%20to%20enforce%20solutions%20making%20use%20of%20a%20minimum%20subset%20of%20the%20input%20components%2C%20increasing%20the%20explainability%20of%20the%20model%20and%20providing%20sparse%20input%20representations.%20The%20technique%20is%20compared%20to%20the%20most%20common%20methods%20in%20the%20literature%20and%20is%20successfully%20challenged%20against%20both%20synthetic%20and%20real%20data.&entry.1838667208=http%3A//arxiv.org/abs/2406.01183v3&entry.124074799=Read"},
{"title": "PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly", "author": "Liang Ma and Jiajun Wen and Min Lin and Rongtao Xu and Xiwen Liang and Bingqian Lin and Jun Ma and Yongxin Wang and Ziming Wei and Haokun Lin and Mingfei Han and Meng Cao and Bokui Chen and Ivan Laptev and Xiaodan Liang", "abstract": "While vision-language models (VLMs) have demonstrated promising capabilities in reasoning and planning for embodied agents, their ability to comprehend physical phenomena, particularly within structured 3D environments, remains severely limited. To close this gap, we introduce PhyBlock, a progressive benchmark designed to assess VLMs on physical understanding and planning through robotic 3D block assembly tasks. PhyBlock integrates a novel four-level cognitive hierarchy assembly task alongside targeted Visual Question Answering (VQA) samples, collectively aimed at evaluating progressive spatial reasoning and fundamental physical comprehension, including object properties, spatial relationships, and holistic scene understanding. PhyBlock includes 2600 block tasks (400 assembly tasks, 2200 VQA tasks) and evaluates models across three key dimensions: partial completion, failure diagnosis, and planning robustness. We benchmark 21 state-of-the-art VLMs, highlighting their strengths and limitations in physically grounded, multi-step planning. Our empirical findings indicate that the performance of VLMs exhibits pronounced limitations in high-level planning and reasoning capabilities, leading to a notable decline in performance for the growing complexity of the tasks. Error analysis reveals persistent difficulties in spatial orientation and dependency reasoning. Surprisingly, chain-of-thought prompting offers minimal improvements, suggesting spatial tasks heavily rely on intuitive model comprehension. We position PhyBlock as a unified testbed to advance embodied reasoning, bridging vision-language understanding and real-world physical problem-solving.", "link": "http://arxiv.org/abs/2506.08708v2", "date": "2025-11-21", "relevancy": 2.3423, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5919}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5919}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhyBlock%3A%20A%20Progressive%20Benchmark%20for%20Physical%20Understanding%20and%20Planning%20via%203D%20Block%20Assembly&body=Title%3A%20PhyBlock%3A%20A%20Progressive%20Benchmark%20for%20Physical%20Understanding%20and%20Planning%20via%203D%20Block%20Assembly%0AAuthor%3A%20Liang%20Ma%20and%20Jiajun%20Wen%20and%20Min%20Lin%20and%20Rongtao%20Xu%20and%20Xiwen%20Liang%20and%20Bingqian%20Lin%20and%20Jun%20Ma%20and%20Yongxin%20Wang%20and%20Ziming%20Wei%20and%20Haokun%20Lin%20and%20Mingfei%20Han%20and%20Meng%20Cao%20and%20Bokui%20Chen%20and%20Ivan%20Laptev%20and%20Xiaodan%20Liang%0AAbstract%3A%20While%20vision-language%20models%20%28VLMs%29%20have%20demonstrated%20promising%20capabilities%20in%20reasoning%20and%20planning%20for%20embodied%20agents%2C%20their%20ability%20to%20comprehend%20physical%20phenomena%2C%20particularly%20within%20structured%203D%20environments%2C%20remains%20severely%20limited.%20To%20close%20this%20gap%2C%20we%20introduce%20PhyBlock%2C%20a%20progressive%20benchmark%20designed%20to%20assess%20VLMs%20on%20physical%20understanding%20and%20planning%20through%20robotic%203D%20block%20assembly%20tasks.%20PhyBlock%20integrates%20a%20novel%20four-level%20cognitive%20hierarchy%20assembly%20task%20alongside%20targeted%20Visual%20Question%20Answering%20%28VQA%29%20samples%2C%20collectively%20aimed%20at%20evaluating%20progressive%20spatial%20reasoning%20and%20fundamental%20physical%20comprehension%2C%20including%20object%20properties%2C%20spatial%20relationships%2C%20and%20holistic%20scene%20understanding.%20PhyBlock%20includes%202600%20block%20tasks%20%28400%20assembly%20tasks%2C%202200%20VQA%20tasks%29%20and%20evaluates%20models%20across%20three%20key%20dimensions%3A%20partial%20completion%2C%20failure%20diagnosis%2C%20and%20planning%20robustness.%20We%20benchmark%2021%20state-of-the-art%20VLMs%2C%20highlighting%20their%20strengths%20and%20limitations%20in%20physically%20grounded%2C%20multi-step%20planning.%20Our%20empirical%20findings%20indicate%20that%20the%20performance%20of%20VLMs%20exhibits%20pronounced%20limitations%20in%20high-level%20planning%20and%20reasoning%20capabilities%2C%20leading%20to%20a%20notable%20decline%20in%20performance%20for%20the%20growing%20complexity%20of%20the%20tasks.%20Error%20analysis%20reveals%20persistent%20difficulties%20in%20spatial%20orientation%20and%20dependency%20reasoning.%20Surprisingly%2C%20chain-of-thought%20prompting%20offers%20minimal%20improvements%2C%20suggesting%20spatial%20tasks%20heavily%20rely%20on%20intuitive%20model%20comprehension.%20We%20position%20PhyBlock%20as%20a%20unified%20testbed%20to%20advance%20embodied%20reasoning%2C%20bridging%20vision-language%20understanding%20and%20real-world%20physical%20problem-solving.%0ALink%3A%20http%3A//arxiv.org/abs/2506.08708v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhyBlock%253A%2520A%2520Progressive%2520Benchmark%2520for%2520Physical%2520Understanding%2520and%2520Planning%2520via%25203D%2520Block%2520Assembly%26entry.906535625%3DLiang%2520Ma%2520and%2520Jiajun%2520Wen%2520and%2520Min%2520Lin%2520and%2520Rongtao%2520Xu%2520and%2520Xiwen%2520Liang%2520and%2520Bingqian%2520Lin%2520and%2520Jun%2520Ma%2520and%2520Yongxin%2520Wang%2520and%2520Ziming%2520Wei%2520and%2520Haokun%2520Lin%2520and%2520Mingfei%2520Han%2520and%2520Meng%2520Cao%2520and%2520Bokui%2520Chen%2520and%2520Ivan%2520Laptev%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3DWhile%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520demonstrated%2520promising%2520capabilities%2520in%2520reasoning%2520and%2520planning%2520for%2520embodied%2520agents%252C%2520their%2520ability%2520to%2520comprehend%2520physical%2520phenomena%252C%2520particularly%2520within%2520structured%25203D%2520environments%252C%2520remains%2520severely%2520limited.%2520To%2520close%2520this%2520gap%252C%2520we%2520introduce%2520PhyBlock%252C%2520a%2520progressive%2520benchmark%2520designed%2520to%2520assess%2520VLMs%2520on%2520physical%2520understanding%2520and%2520planning%2520through%2520robotic%25203D%2520block%2520assembly%2520tasks.%2520PhyBlock%2520integrates%2520a%2520novel%2520four-level%2520cognitive%2520hierarchy%2520assembly%2520task%2520alongside%2520targeted%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520samples%252C%2520collectively%2520aimed%2520at%2520evaluating%2520progressive%2520spatial%2520reasoning%2520and%2520fundamental%2520physical%2520comprehension%252C%2520including%2520object%2520properties%252C%2520spatial%2520relationships%252C%2520and%2520holistic%2520scene%2520understanding.%2520PhyBlock%2520includes%25202600%2520block%2520tasks%2520%2528400%2520assembly%2520tasks%252C%25202200%2520VQA%2520tasks%2529%2520and%2520evaluates%2520models%2520across%2520three%2520key%2520dimensions%253A%2520partial%2520completion%252C%2520failure%2520diagnosis%252C%2520and%2520planning%2520robustness.%2520We%2520benchmark%252021%2520state-of-the-art%2520VLMs%252C%2520highlighting%2520their%2520strengths%2520and%2520limitations%2520in%2520physically%2520grounded%252C%2520multi-step%2520planning.%2520Our%2520empirical%2520findings%2520indicate%2520that%2520the%2520performance%2520of%2520VLMs%2520exhibits%2520pronounced%2520limitations%2520in%2520high-level%2520planning%2520and%2520reasoning%2520capabilities%252C%2520leading%2520to%2520a%2520notable%2520decline%2520in%2520performance%2520for%2520the%2520growing%2520complexity%2520of%2520the%2520tasks.%2520Error%2520analysis%2520reveals%2520persistent%2520difficulties%2520in%2520spatial%2520orientation%2520and%2520dependency%2520reasoning.%2520Surprisingly%252C%2520chain-of-thought%2520prompting%2520offers%2520minimal%2520improvements%252C%2520suggesting%2520spatial%2520tasks%2520heavily%2520rely%2520on%2520intuitive%2520model%2520comprehension.%2520We%2520position%2520PhyBlock%2520as%2520a%2520unified%2520testbed%2520to%2520advance%2520embodied%2520reasoning%252C%2520bridging%2520vision-language%2520understanding%2520and%2520real-world%2520physical%2520problem-solving.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08708v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhyBlock%3A%20A%20Progressive%20Benchmark%20for%20Physical%20Understanding%20and%20Planning%20via%203D%20Block%20Assembly&entry.906535625=Liang%20Ma%20and%20Jiajun%20Wen%20and%20Min%20Lin%20and%20Rongtao%20Xu%20and%20Xiwen%20Liang%20and%20Bingqian%20Lin%20and%20Jun%20Ma%20and%20Yongxin%20Wang%20and%20Ziming%20Wei%20and%20Haokun%20Lin%20and%20Mingfei%20Han%20and%20Meng%20Cao%20and%20Bokui%20Chen%20and%20Ivan%20Laptev%20and%20Xiaodan%20Liang&entry.1292438233=While%20vision-language%20models%20%28VLMs%29%20have%20demonstrated%20promising%20capabilities%20in%20reasoning%20and%20planning%20for%20embodied%20agents%2C%20their%20ability%20to%20comprehend%20physical%20phenomena%2C%20particularly%20within%20structured%203D%20environments%2C%20remains%20severely%20limited.%20To%20close%20this%20gap%2C%20we%20introduce%20PhyBlock%2C%20a%20progressive%20benchmark%20designed%20to%20assess%20VLMs%20on%20physical%20understanding%20and%20planning%20through%20robotic%203D%20block%20assembly%20tasks.%20PhyBlock%20integrates%20a%20novel%20four-level%20cognitive%20hierarchy%20assembly%20task%20alongside%20targeted%20Visual%20Question%20Answering%20%28VQA%29%20samples%2C%20collectively%20aimed%20at%20evaluating%20progressive%20spatial%20reasoning%20and%20fundamental%20physical%20comprehension%2C%20including%20object%20properties%2C%20spatial%20relationships%2C%20and%20holistic%20scene%20understanding.%20PhyBlock%20includes%202600%20block%20tasks%20%28400%20assembly%20tasks%2C%202200%20VQA%20tasks%29%20and%20evaluates%20models%20across%20three%20key%20dimensions%3A%20partial%20completion%2C%20failure%20diagnosis%2C%20and%20planning%20robustness.%20We%20benchmark%2021%20state-of-the-art%20VLMs%2C%20highlighting%20their%20strengths%20and%20limitations%20in%20physically%20grounded%2C%20multi-step%20planning.%20Our%20empirical%20findings%20indicate%20that%20the%20performance%20of%20VLMs%20exhibits%20pronounced%20limitations%20in%20high-level%20planning%20and%20reasoning%20capabilities%2C%20leading%20to%20a%20notable%20decline%20in%20performance%20for%20the%20growing%20complexity%20of%20the%20tasks.%20Error%20analysis%20reveals%20persistent%20difficulties%20in%20spatial%20orientation%20and%20dependency%20reasoning.%20Surprisingly%2C%20chain-of-thought%20prompting%20offers%20minimal%20improvements%2C%20suggesting%20spatial%20tasks%20heavily%20rely%20on%20intuitive%20model%20comprehension.%20We%20position%20PhyBlock%20as%20a%20unified%20testbed%20to%20advance%20embodied%20reasoning%2C%20bridging%20vision-language%20understanding%20and%20real-world%20physical%20problem-solving.&entry.1838667208=http%3A//arxiv.org/abs/2506.08708v2&entry.124074799=Read"},
{"title": "A Unified Stability Analysis of SAM vs SGD: Role of Data Coherence and Emergence of Simplicity Bias", "author": "Wei-Kai Chang and Rajiv Khanna", "abstract": "Understanding the dynamics of optimization in deep learning is increasingly important as models scale. While stochastic gradient descent (SGD) and its variants reliably find solutions that generalize well, the mechanisms driving this generalization remain unclear. Notably, these algorithms often prefer flatter or simpler minima, particularly in overparameterized settings. Prior work has linked flatness to generalization, and methods like Sharpness-Aware Minimization (SAM) explicitly encourage flatness, but a unified theory connecting data structure, optimization dynamics, and the nature of learned solutions is still lacking. In this work, we develop a linear stability framework that analyzes the behavior of SGD, random perturbations, and SAM, particularly in two layer ReLU networks. Central to our analysis is a coherence measure that quantifies how gradient curvature aligns across data points, revealing why certain minima are stable and favored during training.", "link": "http://arxiv.org/abs/2511.17378v1", "date": "2025-11-21", "relevancy": 2.3301, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4702}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4679}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Stability%20Analysis%20of%20SAM%20vs%20SGD%3A%20Role%20of%20Data%20Coherence%20and%20Emergence%20of%20Simplicity%20Bias&body=Title%3A%20A%20Unified%20Stability%20Analysis%20of%20SAM%20vs%20SGD%3A%20Role%20of%20Data%20Coherence%20and%20Emergence%20of%20Simplicity%20Bias%0AAuthor%3A%20Wei-Kai%20Chang%20and%20Rajiv%20Khanna%0AAbstract%3A%20Understanding%20the%20dynamics%20of%20optimization%20in%20deep%20learning%20is%20increasingly%20important%20as%20models%20scale.%20While%20stochastic%20gradient%20descent%20%28SGD%29%20and%20its%20variants%20reliably%20find%20solutions%20that%20generalize%20well%2C%20the%20mechanisms%20driving%20this%20generalization%20remain%20unclear.%20Notably%2C%20these%20algorithms%20often%20prefer%20flatter%20or%20simpler%20minima%2C%20particularly%20in%20overparameterized%20settings.%20Prior%20work%20has%20linked%20flatness%20to%20generalization%2C%20and%20methods%20like%20Sharpness-Aware%20Minimization%20%28SAM%29%20explicitly%20encourage%20flatness%2C%20but%20a%20unified%20theory%20connecting%20data%20structure%2C%20optimization%20dynamics%2C%20and%20the%20nature%20of%20learned%20solutions%20is%20still%20lacking.%20In%20this%20work%2C%20we%20develop%20a%20linear%20stability%20framework%20that%20analyzes%20the%20behavior%20of%20SGD%2C%20random%20perturbations%2C%20and%20SAM%2C%20particularly%20in%20two%20layer%20ReLU%20networks.%20Central%20to%20our%20analysis%20is%20a%20coherence%20measure%20that%20quantifies%20how%20gradient%20curvature%20aligns%20across%20data%20points%2C%20revealing%20why%20certain%20minima%20are%20stable%20and%20favored%20during%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17378v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Stability%2520Analysis%2520of%2520SAM%2520vs%2520SGD%253A%2520Role%2520of%2520Data%2520Coherence%2520and%2520Emergence%2520of%2520Simplicity%2520Bias%26entry.906535625%3DWei-Kai%2520Chang%2520and%2520Rajiv%2520Khanna%26entry.1292438233%3DUnderstanding%2520the%2520dynamics%2520of%2520optimization%2520in%2520deep%2520learning%2520is%2520increasingly%2520important%2520as%2520models%2520scale.%2520While%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%2520and%2520its%2520variants%2520reliably%2520find%2520solutions%2520that%2520generalize%2520well%252C%2520the%2520mechanisms%2520driving%2520this%2520generalization%2520remain%2520unclear.%2520Notably%252C%2520these%2520algorithms%2520often%2520prefer%2520flatter%2520or%2520simpler%2520minima%252C%2520particularly%2520in%2520overparameterized%2520settings.%2520Prior%2520work%2520has%2520linked%2520flatness%2520to%2520generalization%252C%2520and%2520methods%2520like%2520Sharpness-Aware%2520Minimization%2520%2528SAM%2529%2520explicitly%2520encourage%2520flatness%252C%2520but%2520a%2520unified%2520theory%2520connecting%2520data%2520structure%252C%2520optimization%2520dynamics%252C%2520and%2520the%2520nature%2520of%2520learned%2520solutions%2520is%2520still%2520lacking.%2520In%2520this%2520work%252C%2520we%2520develop%2520a%2520linear%2520stability%2520framework%2520that%2520analyzes%2520the%2520behavior%2520of%2520SGD%252C%2520random%2520perturbations%252C%2520and%2520SAM%252C%2520particularly%2520in%2520two%2520layer%2520ReLU%2520networks.%2520Central%2520to%2520our%2520analysis%2520is%2520a%2520coherence%2520measure%2520that%2520quantifies%2520how%2520gradient%2520curvature%2520aligns%2520across%2520data%2520points%252C%2520revealing%2520why%2520certain%2520minima%2520are%2520stable%2520and%2520favored%2520during%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17378v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Stability%20Analysis%20of%20SAM%20vs%20SGD%3A%20Role%20of%20Data%20Coherence%20and%20Emergence%20of%20Simplicity%20Bias&entry.906535625=Wei-Kai%20Chang%20and%20Rajiv%20Khanna&entry.1292438233=Understanding%20the%20dynamics%20of%20optimization%20in%20deep%20learning%20is%20increasingly%20important%20as%20models%20scale.%20While%20stochastic%20gradient%20descent%20%28SGD%29%20and%20its%20variants%20reliably%20find%20solutions%20that%20generalize%20well%2C%20the%20mechanisms%20driving%20this%20generalization%20remain%20unclear.%20Notably%2C%20these%20algorithms%20often%20prefer%20flatter%20or%20simpler%20minima%2C%20particularly%20in%20overparameterized%20settings.%20Prior%20work%20has%20linked%20flatness%20to%20generalization%2C%20and%20methods%20like%20Sharpness-Aware%20Minimization%20%28SAM%29%20explicitly%20encourage%20flatness%2C%20but%20a%20unified%20theory%20connecting%20data%20structure%2C%20optimization%20dynamics%2C%20and%20the%20nature%20of%20learned%20solutions%20is%20still%20lacking.%20In%20this%20work%2C%20we%20develop%20a%20linear%20stability%20framework%20that%20analyzes%20the%20behavior%20of%20SGD%2C%20random%20perturbations%2C%20and%20SAM%2C%20particularly%20in%20two%20layer%20ReLU%20networks.%20Central%20to%20our%20analysis%20is%20a%20coherence%20measure%20that%20quantifies%20how%20gradient%20curvature%20aligns%20across%20data%20points%2C%20revealing%20why%20certain%20minima%20are%20stable%20and%20favored%20during%20training.&entry.1838667208=http%3A//arxiv.org/abs/2511.17378v1&entry.124074799=Read"},
{"title": "VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding", "author": "Jianxiang He and Meisheng Hong and Jungang Li and Ziyang Chen and Weiyu Guo and Xuming Hu and Hui Xiong", "abstract": "Multimodal large language models (MLLMs) demonstrate exceptional performance in vision-language tasks, yet their processing of long videos is constrained by input context length and high computational costs. Sparse frame sampling thus becomes a necessary preprocessing step, with sampled frame quality directly impacting downstream performance. Existing keyframe search algorithms achieve a balance between efficiency and sampled frame quality but heavily rely on the visual modality alone. This makes them difficult to adapt to text-related tasks and often leads to retrieval results deviating from core semantic content. To address this, we propose the VISUAL-SUBTITLE INTEGRATION (VSI), a multimodal keyframe retrieval framework. It employs a dual-branch collaborative retrieval approach combining Video Search and Subtitle Match to fuse complementary visual and textual information for precise localization. Experiments on LongVideoBench and VideoMME demonstrate that VSI achieves state-of-the-art accuracy in keyframe retrieval while delivering breakthrough performance in text-related tasks and exhibiting strong generalization across other tasks.", "link": "http://arxiv.org/abs/2508.06869v3", "date": "2025-11-21", "relevancy": 2.3242, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5826}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5826}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VSI%3A%20Visual%20Subtitle%20Integration%20for%20Keyframe%20Selection%20to%20enhance%20Long%20Video%20Understanding&body=Title%3A%20VSI%3A%20Visual%20Subtitle%20Integration%20for%20Keyframe%20Selection%20to%20enhance%20Long%20Video%20Understanding%0AAuthor%3A%20Jianxiang%20He%20and%20Meisheng%20Hong%20and%20Jungang%20Li%20and%20Ziyang%20Chen%20and%20Weiyu%20Guo%20and%20Xuming%20Hu%20and%20Hui%20Xiong%0AAbstract%3A%20Multimodal%20large%20language%20models%20%28MLLMs%29%20demonstrate%20exceptional%20performance%20in%20vision-language%20tasks%2C%20yet%20their%20processing%20of%20long%20videos%20is%20constrained%20by%20input%20context%20length%20and%20high%20computational%20costs.%20Sparse%20frame%20sampling%20thus%20becomes%20a%20necessary%20preprocessing%20step%2C%20with%20sampled%20frame%20quality%20directly%20impacting%20downstream%20performance.%20Existing%20keyframe%20search%20algorithms%20achieve%20a%20balance%20between%20efficiency%20and%20sampled%20frame%20quality%20but%20heavily%20rely%20on%20the%20visual%20modality%20alone.%20This%20makes%20them%20difficult%20to%20adapt%20to%20text-related%20tasks%20and%20often%20leads%20to%20retrieval%20results%20deviating%20from%20core%20semantic%20content.%20To%20address%20this%2C%20we%20propose%20the%20VISUAL-SUBTITLE%20INTEGRATION%20%28VSI%29%2C%20a%20multimodal%20keyframe%20retrieval%20framework.%20It%20employs%20a%20dual-branch%20collaborative%20retrieval%20approach%20combining%20Video%20Search%20and%20Subtitle%20Match%20to%20fuse%20complementary%20visual%20and%20textual%20information%20for%20precise%20localization.%20Experiments%20on%20LongVideoBench%20and%20VideoMME%20demonstrate%20that%20VSI%20achieves%20state-of-the-art%20accuracy%20in%20keyframe%20retrieval%20while%20delivering%20breakthrough%20performance%20in%20text-related%20tasks%20and%20exhibiting%20strong%20generalization%20across%20other%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2508.06869v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVSI%253A%2520Visual%2520Subtitle%2520Integration%2520for%2520Keyframe%2520Selection%2520to%2520enhance%2520Long%2520Video%2520Understanding%26entry.906535625%3DJianxiang%2520He%2520and%2520Meisheng%2520Hong%2520and%2520Jungang%2520Li%2520and%2520Ziyang%2520Chen%2520and%2520Weiyu%2520Guo%2520and%2520Xuming%2520Hu%2520and%2520Hui%2520Xiong%26entry.1292438233%3DMultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520demonstrate%2520exceptional%2520performance%2520in%2520vision-language%2520tasks%252C%2520yet%2520their%2520processing%2520of%2520long%2520videos%2520is%2520constrained%2520by%2520input%2520context%2520length%2520and%2520high%2520computational%2520costs.%2520Sparse%2520frame%2520sampling%2520thus%2520becomes%2520a%2520necessary%2520preprocessing%2520step%252C%2520with%2520sampled%2520frame%2520quality%2520directly%2520impacting%2520downstream%2520performance.%2520Existing%2520keyframe%2520search%2520algorithms%2520achieve%2520a%2520balance%2520between%2520efficiency%2520and%2520sampled%2520frame%2520quality%2520but%2520heavily%2520rely%2520on%2520the%2520visual%2520modality%2520alone.%2520This%2520makes%2520them%2520difficult%2520to%2520adapt%2520to%2520text-related%2520tasks%2520and%2520often%2520leads%2520to%2520retrieval%2520results%2520deviating%2520from%2520core%2520semantic%2520content.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520VISUAL-SUBTITLE%2520INTEGRATION%2520%2528VSI%2529%252C%2520a%2520multimodal%2520keyframe%2520retrieval%2520framework.%2520It%2520employs%2520a%2520dual-branch%2520collaborative%2520retrieval%2520approach%2520combining%2520Video%2520Search%2520and%2520Subtitle%2520Match%2520to%2520fuse%2520complementary%2520visual%2520and%2520textual%2520information%2520for%2520precise%2520localization.%2520Experiments%2520on%2520LongVideoBench%2520and%2520VideoMME%2520demonstrate%2520that%2520VSI%2520achieves%2520state-of-the-art%2520accuracy%2520in%2520keyframe%2520retrieval%2520while%2520delivering%2520breakthrough%2520performance%2520in%2520text-related%2520tasks%2520and%2520exhibiting%2520strong%2520generalization%2520across%2520other%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06869v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VSI%3A%20Visual%20Subtitle%20Integration%20for%20Keyframe%20Selection%20to%20enhance%20Long%20Video%20Understanding&entry.906535625=Jianxiang%20He%20and%20Meisheng%20Hong%20and%20Jungang%20Li%20and%20Ziyang%20Chen%20and%20Weiyu%20Guo%20and%20Xuming%20Hu%20and%20Hui%20Xiong&entry.1292438233=Multimodal%20large%20language%20models%20%28MLLMs%29%20demonstrate%20exceptional%20performance%20in%20vision-language%20tasks%2C%20yet%20their%20processing%20of%20long%20videos%20is%20constrained%20by%20input%20context%20length%20and%20high%20computational%20costs.%20Sparse%20frame%20sampling%20thus%20becomes%20a%20necessary%20preprocessing%20step%2C%20with%20sampled%20frame%20quality%20directly%20impacting%20downstream%20performance.%20Existing%20keyframe%20search%20algorithms%20achieve%20a%20balance%20between%20efficiency%20and%20sampled%20frame%20quality%20but%20heavily%20rely%20on%20the%20visual%20modality%20alone.%20This%20makes%20them%20difficult%20to%20adapt%20to%20text-related%20tasks%20and%20often%20leads%20to%20retrieval%20results%20deviating%20from%20core%20semantic%20content.%20To%20address%20this%2C%20we%20propose%20the%20VISUAL-SUBTITLE%20INTEGRATION%20%28VSI%29%2C%20a%20multimodal%20keyframe%20retrieval%20framework.%20It%20employs%20a%20dual-branch%20collaborative%20retrieval%20approach%20combining%20Video%20Search%20and%20Subtitle%20Match%20to%20fuse%20complementary%20visual%20and%20textual%20information%20for%20precise%20localization.%20Experiments%20on%20LongVideoBench%20and%20VideoMME%20demonstrate%20that%20VSI%20achieves%20state-of-the-art%20accuracy%20in%20keyframe%20retrieval%20while%20delivering%20breakthrough%20performance%20in%20text-related%20tasks%20and%20exhibiting%20strong%20generalization%20across%20other%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2508.06869v3&entry.124074799=Read"},
{"title": "Lung-DDPM+: Efficient Thoracic CT Image Synthesis using Diffusion Probabilistic Model", "author": "Yifan Jiang and Ahmad Shariftabrizi and Venkata SK. Manem", "abstract": "Generative artificial intelligence (AI) has been playing an important role in various domains. Leveraging its high capability to generate high-fidelity and diverse synthetic data, generative AI is widely applied in diagnostic tasks, such as lung cancer diagnosis using computed tomography (CT). However, existing generative models for lung cancer diagnosis suffer from low efficiency and anatomical imprecision, which limit their clinical applicability. To address these drawbacks, we propose Lung-DDPM+, an improved version of our previous model, Lung-DDPM. This novel approach is a denoising diffusion probabilistic model (DDPM) guided by nodule semantic layouts and accelerated by a pulmonary DPM-solver, enabling the method to focus on lesion areas while achieving a better trade-off between sampling efficiency and quality. Evaluation results on the public LIDC-IDRI dataset suggest that the proposed method achieves 8$\\times$ fewer FLOPs (floating point operations per second), 6.8$\\times$ lower GPU memory consumption, and 14$\\times$ faster sampling compared to Lung-DDPM. Moreover, it maintains comparable sample quality to both Lung-DDPM and other state-of-the-art (SOTA) generative models in two downstream segmentation tasks. We also conducted a Visual Turing Test by an experienced radiologist, showing the advanced quality and fidelity of synthetic samples generated by the proposed method. These experimental results demonstrate that Lung-DDPM+ can effectively generate high-quality thoracic CT images with lung nodules, highlighting its potential for broader applications, such as general tumor synthesis and lesion generation in medical imaging. The code and pretrained models are available at https://github.com/Manem-Lab/Lung-DDPM-PLUS.", "link": "http://arxiv.org/abs/2508.09327v2", "date": "2025-11-21", "relevancy": 2.3223, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6107}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5746}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lung-DDPM%2B%3A%20Efficient%20Thoracic%20CT%20Image%20Synthesis%20using%20Diffusion%20Probabilistic%20Model&body=Title%3A%20Lung-DDPM%2B%3A%20Efficient%20Thoracic%20CT%20Image%20Synthesis%20using%20Diffusion%20Probabilistic%20Model%0AAuthor%3A%20Yifan%20Jiang%20and%20Ahmad%20Shariftabrizi%20and%20Venkata%20SK.%20Manem%0AAbstract%3A%20Generative%20artificial%20intelligence%20%28AI%29%20has%20been%20playing%20an%20important%20role%20in%20various%20domains.%20Leveraging%20its%20high%20capability%20to%20generate%20high-fidelity%20and%20diverse%20synthetic%20data%2C%20generative%20AI%20is%20widely%20applied%20in%20diagnostic%20tasks%2C%20such%20as%20lung%20cancer%20diagnosis%20using%20computed%20tomography%20%28CT%29.%20However%2C%20existing%20generative%20models%20for%20lung%20cancer%20diagnosis%20suffer%20from%20low%20efficiency%20and%20anatomical%20imprecision%2C%20which%20limit%20their%20clinical%20applicability.%20To%20address%20these%20drawbacks%2C%20we%20propose%20Lung-DDPM%2B%2C%20an%20improved%20version%20of%20our%20previous%20model%2C%20Lung-DDPM.%20This%20novel%20approach%20is%20a%20denoising%20diffusion%20probabilistic%20model%20%28DDPM%29%20guided%20by%20nodule%20semantic%20layouts%20and%20accelerated%20by%20a%20pulmonary%20DPM-solver%2C%20enabling%20the%20method%20to%20focus%20on%20lesion%20areas%20while%20achieving%20a%20better%20trade-off%20between%20sampling%20efficiency%20and%20quality.%20Evaluation%20results%20on%20the%20public%20LIDC-IDRI%20dataset%20suggest%20that%20the%20proposed%20method%20achieves%208%24%5Ctimes%24%20fewer%20FLOPs%20%28floating%20point%20operations%20per%20second%29%2C%206.8%24%5Ctimes%24%20lower%20GPU%20memory%20consumption%2C%20and%2014%24%5Ctimes%24%20faster%20sampling%20compared%20to%20Lung-DDPM.%20Moreover%2C%20it%20maintains%20comparable%20sample%20quality%20to%20both%20Lung-DDPM%20and%20other%20state-of-the-art%20%28SOTA%29%20generative%20models%20in%20two%20downstream%20segmentation%20tasks.%20We%20also%20conducted%20a%20Visual%20Turing%20Test%20by%20an%20experienced%20radiologist%2C%20showing%20the%20advanced%20quality%20and%20fidelity%20of%20synthetic%20samples%20generated%20by%20the%20proposed%20method.%20These%20experimental%20results%20demonstrate%20that%20Lung-DDPM%2B%20can%20effectively%20generate%20high-quality%20thoracic%20CT%20images%20with%20lung%20nodules%2C%20highlighting%20its%20potential%20for%20broader%20applications%2C%20such%20as%20general%20tumor%20synthesis%20and%20lesion%20generation%20in%20medical%20imaging.%20The%20code%20and%20pretrained%20models%20are%20available%20at%20https%3A//github.com/Manem-Lab/Lung-DDPM-PLUS.%0ALink%3A%20http%3A//arxiv.org/abs/2508.09327v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLung-DDPM%252B%253A%2520Efficient%2520Thoracic%2520CT%2520Image%2520Synthesis%2520using%2520Diffusion%2520Probabilistic%2520Model%26entry.906535625%3DYifan%2520Jiang%2520and%2520Ahmad%2520Shariftabrizi%2520and%2520Venkata%2520SK.%2520Manem%26entry.1292438233%3DGenerative%2520artificial%2520intelligence%2520%2528AI%2529%2520has%2520been%2520playing%2520an%2520important%2520role%2520in%2520various%2520domains.%2520Leveraging%2520its%2520high%2520capability%2520to%2520generate%2520high-fidelity%2520and%2520diverse%2520synthetic%2520data%252C%2520generative%2520AI%2520is%2520widely%2520applied%2520in%2520diagnostic%2520tasks%252C%2520such%2520as%2520lung%2520cancer%2520diagnosis%2520using%2520computed%2520tomography%2520%2528CT%2529.%2520However%252C%2520existing%2520generative%2520models%2520for%2520lung%2520cancer%2520diagnosis%2520suffer%2520from%2520low%2520efficiency%2520and%2520anatomical%2520imprecision%252C%2520which%2520limit%2520their%2520clinical%2520applicability.%2520To%2520address%2520these%2520drawbacks%252C%2520we%2520propose%2520Lung-DDPM%252B%252C%2520an%2520improved%2520version%2520of%2520our%2520previous%2520model%252C%2520Lung-DDPM.%2520This%2520novel%2520approach%2520is%2520a%2520denoising%2520diffusion%2520probabilistic%2520model%2520%2528DDPM%2529%2520guided%2520by%2520nodule%2520semantic%2520layouts%2520and%2520accelerated%2520by%2520a%2520pulmonary%2520DPM-solver%252C%2520enabling%2520the%2520method%2520to%2520focus%2520on%2520lesion%2520areas%2520while%2520achieving%2520a%2520better%2520trade-off%2520between%2520sampling%2520efficiency%2520and%2520quality.%2520Evaluation%2520results%2520on%2520the%2520public%2520LIDC-IDRI%2520dataset%2520suggest%2520that%2520the%2520proposed%2520method%2520achieves%25208%2524%255Ctimes%2524%2520fewer%2520FLOPs%2520%2528floating%2520point%2520operations%2520per%2520second%2529%252C%25206.8%2524%255Ctimes%2524%2520lower%2520GPU%2520memory%2520consumption%252C%2520and%252014%2524%255Ctimes%2524%2520faster%2520sampling%2520compared%2520to%2520Lung-DDPM.%2520Moreover%252C%2520it%2520maintains%2520comparable%2520sample%2520quality%2520to%2520both%2520Lung-DDPM%2520and%2520other%2520state-of-the-art%2520%2528SOTA%2529%2520generative%2520models%2520in%2520two%2520downstream%2520segmentation%2520tasks.%2520We%2520also%2520conducted%2520a%2520Visual%2520Turing%2520Test%2520by%2520an%2520experienced%2520radiologist%252C%2520showing%2520the%2520advanced%2520quality%2520and%2520fidelity%2520of%2520synthetic%2520samples%2520generated%2520by%2520the%2520proposed%2520method.%2520These%2520experimental%2520results%2520demonstrate%2520that%2520Lung-DDPM%252B%2520can%2520effectively%2520generate%2520high-quality%2520thoracic%2520CT%2520images%2520with%2520lung%2520nodules%252C%2520highlighting%2520its%2520potential%2520for%2520broader%2520applications%252C%2520such%2520as%2520general%2520tumor%2520synthesis%2520and%2520lesion%2520generation%2520in%2520medical%2520imaging.%2520The%2520code%2520and%2520pretrained%2520models%2520are%2520available%2520at%2520https%253A//github.com/Manem-Lab/Lung-DDPM-PLUS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.09327v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lung-DDPM%2B%3A%20Efficient%20Thoracic%20CT%20Image%20Synthesis%20using%20Diffusion%20Probabilistic%20Model&entry.906535625=Yifan%20Jiang%20and%20Ahmad%20Shariftabrizi%20and%20Venkata%20SK.%20Manem&entry.1292438233=Generative%20artificial%20intelligence%20%28AI%29%20has%20been%20playing%20an%20important%20role%20in%20various%20domains.%20Leveraging%20its%20high%20capability%20to%20generate%20high-fidelity%20and%20diverse%20synthetic%20data%2C%20generative%20AI%20is%20widely%20applied%20in%20diagnostic%20tasks%2C%20such%20as%20lung%20cancer%20diagnosis%20using%20computed%20tomography%20%28CT%29.%20However%2C%20existing%20generative%20models%20for%20lung%20cancer%20diagnosis%20suffer%20from%20low%20efficiency%20and%20anatomical%20imprecision%2C%20which%20limit%20their%20clinical%20applicability.%20To%20address%20these%20drawbacks%2C%20we%20propose%20Lung-DDPM%2B%2C%20an%20improved%20version%20of%20our%20previous%20model%2C%20Lung-DDPM.%20This%20novel%20approach%20is%20a%20denoising%20diffusion%20probabilistic%20model%20%28DDPM%29%20guided%20by%20nodule%20semantic%20layouts%20and%20accelerated%20by%20a%20pulmonary%20DPM-solver%2C%20enabling%20the%20method%20to%20focus%20on%20lesion%20areas%20while%20achieving%20a%20better%20trade-off%20between%20sampling%20efficiency%20and%20quality.%20Evaluation%20results%20on%20the%20public%20LIDC-IDRI%20dataset%20suggest%20that%20the%20proposed%20method%20achieves%208%24%5Ctimes%24%20fewer%20FLOPs%20%28floating%20point%20operations%20per%20second%29%2C%206.8%24%5Ctimes%24%20lower%20GPU%20memory%20consumption%2C%20and%2014%24%5Ctimes%24%20faster%20sampling%20compared%20to%20Lung-DDPM.%20Moreover%2C%20it%20maintains%20comparable%20sample%20quality%20to%20both%20Lung-DDPM%20and%20other%20state-of-the-art%20%28SOTA%29%20generative%20models%20in%20two%20downstream%20segmentation%20tasks.%20We%20also%20conducted%20a%20Visual%20Turing%20Test%20by%20an%20experienced%20radiologist%2C%20showing%20the%20advanced%20quality%20and%20fidelity%20of%20synthetic%20samples%20generated%20by%20the%20proposed%20method.%20These%20experimental%20results%20demonstrate%20that%20Lung-DDPM%2B%20can%20effectively%20generate%20high-quality%20thoracic%20CT%20images%20with%20lung%20nodules%2C%20highlighting%20its%20potential%20for%20broader%20applications%2C%20such%20as%20general%20tumor%20synthesis%20and%20lesion%20generation%20in%20medical%20imaging.%20The%20code%20and%20pretrained%20models%20are%20available%20at%20https%3A//github.com/Manem-Lab/Lung-DDPM-PLUS.&entry.1838667208=http%3A//arxiv.org/abs/2508.09327v2&entry.124074799=Read"},
{"title": "Leveraging CVAE for Joint Configuration Estimation of Multifingered Grippers from Point Cloud Data", "author": "Julien Merand and Boris Meden and Mathieu Grossard", "abstract": "This paper presents an efficient approach for determining the joint configuration of a multifingered gripper solely from the point cloud data of its poly-articulated chain, as generated by visual sensors, simulations or even generative neural networks. Well-known inverse kinematics (IK) techniques can provide mathematically exact solutions (when they exist) for joint configuration determination based solely on the fingertip pose, but often require post-hoc decision-making by considering the positions of all intermediate phalanges in the gripper's fingers, or rely on algorithms to numerically approximate solutions for more complex kinematics. In contrast, our method leverages machine learning to implicitly overcome these challenges. This is achieved through a Conditional Variational Auto-Encoder (CVAE), which takes point cloud data of key structural elements as input and reconstructs the corresponding joint configurations. We validate our approach on the MultiDex grasping dataset using the Allegro Hand, operating within 0.05 milliseconds and achieving accuracy comparable to state-of-the-art methods. This highlights the effectiveness of our pipeline for joint configuration estimation within the broader context of AI-driven techniques for grasp planning.", "link": "http://arxiv.org/abs/2511.17276v1", "date": "2025-11-21", "relevancy": 2.3157, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6115}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5737}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20CVAE%20for%20Joint%20Configuration%20Estimation%20of%20Multifingered%20Grippers%20from%20Point%20Cloud%20Data&body=Title%3A%20Leveraging%20CVAE%20for%20Joint%20Configuration%20Estimation%20of%20Multifingered%20Grippers%20from%20Point%20Cloud%20Data%0AAuthor%3A%20Julien%20Merand%20and%20Boris%20Meden%20and%20Mathieu%20Grossard%0AAbstract%3A%20This%20paper%20presents%20an%20efficient%20approach%20for%20determining%20the%20joint%20configuration%20of%20a%20multifingered%20gripper%20solely%20from%20the%20point%20cloud%20data%20of%20its%20poly-articulated%20chain%2C%20as%20generated%20by%20visual%20sensors%2C%20simulations%20or%20even%20generative%20neural%20networks.%20Well-known%20inverse%20kinematics%20%28IK%29%20techniques%20can%20provide%20mathematically%20exact%20solutions%20%28when%20they%20exist%29%20for%20joint%20configuration%20determination%20based%20solely%20on%20the%20fingertip%20pose%2C%20but%20often%20require%20post-hoc%20decision-making%20by%20considering%20the%20positions%20of%20all%20intermediate%20phalanges%20in%20the%20gripper%27s%20fingers%2C%20or%20rely%20on%20algorithms%20to%20numerically%20approximate%20solutions%20for%20more%20complex%20kinematics.%20In%20contrast%2C%20our%20method%20leverages%20machine%20learning%20to%20implicitly%20overcome%20these%20challenges.%20This%20is%20achieved%20through%20a%20Conditional%20Variational%20Auto-Encoder%20%28CVAE%29%2C%20which%20takes%20point%20cloud%20data%20of%20key%20structural%20elements%20as%20input%20and%20reconstructs%20the%20corresponding%20joint%20configurations.%20We%20validate%20our%20approach%20on%20the%20MultiDex%20grasping%20dataset%20using%20the%20Allegro%20Hand%2C%20operating%20within%200.05%20milliseconds%20and%20achieving%20accuracy%20comparable%20to%20state-of-the-art%20methods.%20This%20highlights%20the%20effectiveness%20of%20our%20pipeline%20for%20joint%20configuration%20estimation%20within%20the%20broader%20context%20of%20AI-driven%20techniques%20for%20grasp%20planning.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17276v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520CVAE%2520for%2520Joint%2520Configuration%2520Estimation%2520of%2520Multifingered%2520Grippers%2520from%2520Point%2520Cloud%2520Data%26entry.906535625%3DJulien%2520Merand%2520and%2520Boris%2520Meden%2520and%2520Mathieu%2520Grossard%26entry.1292438233%3DThis%2520paper%2520presents%2520an%2520efficient%2520approach%2520for%2520determining%2520the%2520joint%2520configuration%2520of%2520a%2520multifingered%2520gripper%2520solely%2520from%2520the%2520point%2520cloud%2520data%2520of%2520its%2520poly-articulated%2520chain%252C%2520as%2520generated%2520by%2520visual%2520sensors%252C%2520simulations%2520or%2520even%2520generative%2520neural%2520networks.%2520Well-known%2520inverse%2520kinematics%2520%2528IK%2529%2520techniques%2520can%2520provide%2520mathematically%2520exact%2520solutions%2520%2528when%2520they%2520exist%2529%2520for%2520joint%2520configuration%2520determination%2520based%2520solely%2520on%2520the%2520fingertip%2520pose%252C%2520but%2520often%2520require%2520post-hoc%2520decision-making%2520by%2520considering%2520the%2520positions%2520of%2520all%2520intermediate%2520phalanges%2520in%2520the%2520gripper%2527s%2520fingers%252C%2520or%2520rely%2520on%2520algorithms%2520to%2520numerically%2520approximate%2520solutions%2520for%2520more%2520complex%2520kinematics.%2520In%2520contrast%252C%2520our%2520method%2520leverages%2520machine%2520learning%2520to%2520implicitly%2520overcome%2520these%2520challenges.%2520This%2520is%2520achieved%2520through%2520a%2520Conditional%2520Variational%2520Auto-Encoder%2520%2528CVAE%2529%252C%2520which%2520takes%2520point%2520cloud%2520data%2520of%2520key%2520structural%2520elements%2520as%2520input%2520and%2520reconstructs%2520the%2520corresponding%2520joint%2520configurations.%2520We%2520validate%2520our%2520approach%2520on%2520the%2520MultiDex%2520grasping%2520dataset%2520using%2520the%2520Allegro%2520Hand%252C%2520operating%2520within%25200.05%2520milliseconds%2520and%2520achieving%2520accuracy%2520comparable%2520to%2520state-of-the-art%2520methods.%2520This%2520highlights%2520the%2520effectiveness%2520of%2520our%2520pipeline%2520for%2520joint%2520configuration%2520estimation%2520within%2520the%2520broader%2520context%2520of%2520AI-driven%2520techniques%2520for%2520grasp%2520planning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17276v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20CVAE%20for%20Joint%20Configuration%20Estimation%20of%20Multifingered%20Grippers%20from%20Point%20Cloud%20Data&entry.906535625=Julien%20Merand%20and%20Boris%20Meden%20and%20Mathieu%20Grossard&entry.1292438233=This%20paper%20presents%20an%20efficient%20approach%20for%20determining%20the%20joint%20configuration%20of%20a%20multifingered%20gripper%20solely%20from%20the%20point%20cloud%20data%20of%20its%20poly-articulated%20chain%2C%20as%20generated%20by%20visual%20sensors%2C%20simulations%20or%20even%20generative%20neural%20networks.%20Well-known%20inverse%20kinematics%20%28IK%29%20techniques%20can%20provide%20mathematically%20exact%20solutions%20%28when%20they%20exist%29%20for%20joint%20configuration%20determination%20based%20solely%20on%20the%20fingertip%20pose%2C%20but%20often%20require%20post-hoc%20decision-making%20by%20considering%20the%20positions%20of%20all%20intermediate%20phalanges%20in%20the%20gripper%27s%20fingers%2C%20or%20rely%20on%20algorithms%20to%20numerically%20approximate%20solutions%20for%20more%20complex%20kinematics.%20In%20contrast%2C%20our%20method%20leverages%20machine%20learning%20to%20implicitly%20overcome%20these%20challenges.%20This%20is%20achieved%20through%20a%20Conditional%20Variational%20Auto-Encoder%20%28CVAE%29%2C%20which%20takes%20point%20cloud%20data%20of%20key%20structural%20elements%20as%20input%20and%20reconstructs%20the%20corresponding%20joint%20configurations.%20We%20validate%20our%20approach%20on%20the%20MultiDex%20grasping%20dataset%20using%20the%20Allegro%20Hand%2C%20operating%20within%200.05%20milliseconds%20and%20achieving%20accuracy%20comparable%20to%20state-of-the-art%20methods.%20This%20highlights%20the%20effectiveness%20of%20our%20pipeline%20for%20joint%20configuration%20estimation%20within%20the%20broader%20context%20of%20AI-driven%20techniques%20for%20grasp%20planning.&entry.1838667208=http%3A//arxiv.org/abs/2511.17276v1&entry.124074799=Read"},
{"title": "Large Language Models for Sentiment Analysis to Detect Social Challenges: A Use Case with South African Languages", "author": "Koena Ronny Mabokela and Tim Schlippe and Matthias W\u00f6lfel", "abstract": "Sentiment analysis can aid in understanding people's opinions and emotions on social issues. In multilingual communities sentiment analysis systems can be used to quickly identify social challenges in social media posts, enabling government departments to detect and address these issues more precisely and effectively. Recently, large-language models (LLMs) have become available to the wide public and initial analyses have shown that they exhibit magnificent zero-shot sentiment analysis abilities in English. However, there is no work that has investigated to leverage LLMs for sentiment analysis on social media posts in South African languages and detect social challenges. Consequently, in this work, we analyse the zero-shot performance of the state-of-the-art LLMs GPT-3.5, GPT-4, LlaMa 2, PaLM 2, and Dolly 2 to investigate the sentiment polarities of the 10 most emerging topics in English, Sepedi and Setswana social media posts that fall within the jurisdictional areas of 10 South African government departments. Our results demonstrate that there are big differences between the various LLMs, topics, and languages. In addition, we show that a fusion of the outcomes of different LLMs provides large gains in sentiment classification performance with sentiment classification errors below 1%. Consequently, it is now feasible to provide systems that generate reliable information about sentiment analysis to detect social challenges and draw conclusions about possible needs for actions on specific topics and within different language groups.", "link": "http://arxiv.org/abs/2511.17301v1", "date": "2025-11-21", "relevancy": 2.3125, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4685}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4685}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20for%20Sentiment%20Analysis%20to%20Detect%20Social%20Challenges%3A%20A%20Use%20Case%20with%20South%20African%20Languages&body=Title%3A%20Large%20Language%20Models%20for%20Sentiment%20Analysis%20to%20Detect%20Social%20Challenges%3A%20A%20Use%20Case%20with%20South%20African%20Languages%0AAuthor%3A%20Koena%20Ronny%20Mabokela%20and%20Tim%20Schlippe%20and%20Matthias%20W%C3%B6lfel%0AAbstract%3A%20Sentiment%20analysis%20can%20aid%20in%20understanding%20people%27s%20opinions%20and%20emotions%20on%20social%20issues.%20In%20multilingual%20communities%20sentiment%20analysis%20systems%20can%20be%20used%20to%20quickly%20identify%20social%20challenges%20in%20social%20media%20posts%2C%20enabling%20government%20departments%20to%20detect%20and%20address%20these%20issues%20more%20precisely%20and%20effectively.%20Recently%2C%20large-language%20models%20%28LLMs%29%20have%20become%20available%20to%20the%20wide%20public%20and%20initial%20analyses%20have%20shown%20that%20they%20exhibit%20magnificent%20zero-shot%20sentiment%20analysis%20abilities%20in%20English.%20However%2C%20there%20is%20no%20work%20that%20has%20investigated%20to%20leverage%20LLMs%20for%20sentiment%20analysis%20on%20social%20media%20posts%20in%20South%20African%20languages%20and%20detect%20social%20challenges.%20Consequently%2C%20in%20this%20work%2C%20we%20analyse%20the%20zero-shot%20performance%20of%20the%20state-of-the-art%20LLMs%20GPT-3.5%2C%20GPT-4%2C%20LlaMa%202%2C%20PaLM%202%2C%20and%20Dolly%202%20to%20investigate%20the%20sentiment%20polarities%20of%20the%2010%20most%20emerging%20topics%20in%20English%2C%20Sepedi%20and%20Setswana%20social%20media%20posts%20that%20fall%20within%20the%20jurisdictional%20areas%20of%2010%20South%20African%20government%20departments.%20Our%20results%20demonstrate%20that%20there%20are%20big%20differences%20between%20the%20various%20LLMs%2C%20topics%2C%20and%20languages.%20In%20addition%2C%20we%20show%20that%20a%20fusion%20of%20the%20outcomes%20of%20different%20LLMs%20provides%20large%20gains%20in%20sentiment%20classification%20performance%20with%20sentiment%20classification%20errors%20below%201%25.%20Consequently%2C%20it%20is%20now%20feasible%20to%20provide%20systems%20that%20generate%20reliable%20information%20about%20sentiment%20analysis%20to%20detect%20social%20challenges%20and%20draw%20conclusions%20about%20possible%20needs%20for%20actions%20on%20specific%20topics%20and%20within%20different%20language%20groups.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17301v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520for%2520Sentiment%2520Analysis%2520to%2520Detect%2520Social%2520Challenges%253A%2520A%2520Use%2520Case%2520with%2520South%2520African%2520Languages%26entry.906535625%3DKoena%2520Ronny%2520Mabokela%2520and%2520Tim%2520Schlippe%2520and%2520Matthias%2520W%25C3%25B6lfel%26entry.1292438233%3DSentiment%2520analysis%2520can%2520aid%2520in%2520understanding%2520people%2527s%2520opinions%2520and%2520emotions%2520on%2520social%2520issues.%2520In%2520multilingual%2520communities%2520sentiment%2520analysis%2520systems%2520can%2520be%2520used%2520to%2520quickly%2520identify%2520social%2520challenges%2520in%2520social%2520media%2520posts%252C%2520enabling%2520government%2520departments%2520to%2520detect%2520and%2520address%2520these%2520issues%2520more%2520precisely%2520and%2520effectively.%2520Recently%252C%2520large-language%2520models%2520%2528LLMs%2529%2520have%2520become%2520available%2520to%2520the%2520wide%2520public%2520and%2520initial%2520analyses%2520have%2520shown%2520that%2520they%2520exhibit%2520magnificent%2520zero-shot%2520sentiment%2520analysis%2520abilities%2520in%2520English.%2520However%252C%2520there%2520is%2520no%2520work%2520that%2520has%2520investigated%2520to%2520leverage%2520LLMs%2520for%2520sentiment%2520analysis%2520on%2520social%2520media%2520posts%2520in%2520South%2520African%2520languages%2520and%2520detect%2520social%2520challenges.%2520Consequently%252C%2520in%2520this%2520work%252C%2520we%2520analyse%2520the%2520zero-shot%2520performance%2520of%2520the%2520state-of-the-art%2520LLMs%2520GPT-3.5%252C%2520GPT-4%252C%2520LlaMa%25202%252C%2520PaLM%25202%252C%2520and%2520Dolly%25202%2520to%2520investigate%2520the%2520sentiment%2520polarities%2520of%2520the%252010%2520most%2520emerging%2520topics%2520in%2520English%252C%2520Sepedi%2520and%2520Setswana%2520social%2520media%2520posts%2520that%2520fall%2520within%2520the%2520jurisdictional%2520areas%2520of%252010%2520South%2520African%2520government%2520departments.%2520Our%2520results%2520demonstrate%2520that%2520there%2520are%2520big%2520differences%2520between%2520the%2520various%2520LLMs%252C%2520topics%252C%2520and%2520languages.%2520In%2520addition%252C%2520we%2520show%2520that%2520a%2520fusion%2520of%2520the%2520outcomes%2520of%2520different%2520LLMs%2520provides%2520large%2520gains%2520in%2520sentiment%2520classification%2520performance%2520with%2520sentiment%2520classification%2520errors%2520below%25201%2525.%2520Consequently%252C%2520it%2520is%2520now%2520feasible%2520to%2520provide%2520systems%2520that%2520generate%2520reliable%2520information%2520about%2520sentiment%2520analysis%2520to%2520detect%2520social%2520challenges%2520and%2520draw%2520conclusions%2520about%2520possible%2520needs%2520for%2520actions%2520on%2520specific%2520topics%2520and%2520within%2520different%2520language%2520groups.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17301v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20for%20Sentiment%20Analysis%20to%20Detect%20Social%20Challenges%3A%20A%20Use%20Case%20with%20South%20African%20Languages&entry.906535625=Koena%20Ronny%20Mabokela%20and%20Tim%20Schlippe%20and%20Matthias%20W%C3%B6lfel&entry.1292438233=Sentiment%20analysis%20can%20aid%20in%20understanding%20people%27s%20opinions%20and%20emotions%20on%20social%20issues.%20In%20multilingual%20communities%20sentiment%20analysis%20systems%20can%20be%20used%20to%20quickly%20identify%20social%20challenges%20in%20social%20media%20posts%2C%20enabling%20government%20departments%20to%20detect%20and%20address%20these%20issues%20more%20precisely%20and%20effectively.%20Recently%2C%20large-language%20models%20%28LLMs%29%20have%20become%20available%20to%20the%20wide%20public%20and%20initial%20analyses%20have%20shown%20that%20they%20exhibit%20magnificent%20zero-shot%20sentiment%20analysis%20abilities%20in%20English.%20However%2C%20there%20is%20no%20work%20that%20has%20investigated%20to%20leverage%20LLMs%20for%20sentiment%20analysis%20on%20social%20media%20posts%20in%20South%20African%20languages%20and%20detect%20social%20challenges.%20Consequently%2C%20in%20this%20work%2C%20we%20analyse%20the%20zero-shot%20performance%20of%20the%20state-of-the-art%20LLMs%20GPT-3.5%2C%20GPT-4%2C%20LlaMa%202%2C%20PaLM%202%2C%20and%20Dolly%202%20to%20investigate%20the%20sentiment%20polarities%20of%20the%2010%20most%20emerging%20topics%20in%20English%2C%20Sepedi%20and%20Setswana%20social%20media%20posts%20that%20fall%20within%20the%20jurisdictional%20areas%20of%2010%20South%20African%20government%20departments.%20Our%20results%20demonstrate%20that%20there%20are%20big%20differences%20between%20the%20various%20LLMs%2C%20topics%2C%20and%20languages.%20In%20addition%2C%20we%20show%20that%20a%20fusion%20of%20the%20outcomes%20of%20different%20LLMs%20provides%20large%20gains%20in%20sentiment%20classification%20performance%20with%20sentiment%20classification%20errors%20below%201%25.%20Consequently%2C%20it%20is%20now%20feasible%20to%20provide%20systems%20that%20generate%20reliable%20information%20about%20sentiment%20analysis%20to%20detect%20social%20challenges%20and%20draw%20conclusions%20about%20possible%20needs%20for%20actions%20on%20specific%20topics%20and%20within%20different%20language%20groups.&entry.1838667208=http%3A//arxiv.org/abs/2511.17301v1&entry.124074799=Read"},
{"title": "MDG: Masked Denoising Generation for Multi-Agent Behavior Modeling in Traffic Environments", "author": "Zhiyu Huang and Zewei Zhou and Tianhui Cai and Yun Zhang and Jiaqi Ma", "abstract": "Modeling realistic and interactive multi-agent behavior is critical to autonomous driving and traffic simulation. However, existing diffusion and autoregressive approaches are limited by iterative sampling, sequential decoding, or task-specific designs, which hinder efficiency and reuse. We propose Masked Denoising Generation (MDG), a unified generative framework that reformulates multi-agent behavior modeling as the reconstruction of independently noised spatiotemporal tensors. Instead of relying on diffusion time steps or discrete tokenization, MDG applies continuous, per-agent and per-timestep noise masks that enable localized denoising and controllable trajectory generation in a single or few forward passes. This mask-driven formulation generalizes across open-loop prediction, closed-loop simulation, motion planning, and conditional generation within one model. Trained on large-scale real-world driving datasets, MDG achieves competitive closed-loop performance on the Waymo Sim Agents and nuPlan Planning benchmarks, while providing efficient, consistent, and controllable open-loop multi-agent trajectory generation. These results position MDG as a simple yet versatile paradigm for multi-agent behavior modeling.", "link": "http://arxiv.org/abs/2511.17496v1", "date": "2025-11-21", "relevancy": 2.3082, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5927}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5876}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MDG%3A%20Masked%20Denoising%20Generation%20for%20Multi-Agent%20Behavior%20Modeling%20in%20Traffic%20Environments&body=Title%3A%20MDG%3A%20Masked%20Denoising%20Generation%20for%20Multi-Agent%20Behavior%20Modeling%20in%20Traffic%20Environments%0AAuthor%3A%20Zhiyu%20Huang%20and%20Zewei%20Zhou%20and%20Tianhui%20Cai%20and%20Yun%20Zhang%20and%20Jiaqi%20Ma%0AAbstract%3A%20Modeling%20realistic%20and%20interactive%20multi-agent%20behavior%20is%20critical%20to%20autonomous%20driving%20and%20traffic%20simulation.%20However%2C%20existing%20diffusion%20and%20autoregressive%20approaches%20are%20limited%20by%20iterative%20sampling%2C%20sequential%20decoding%2C%20or%20task-specific%20designs%2C%20which%20hinder%20efficiency%20and%20reuse.%20We%20propose%20Masked%20Denoising%20Generation%20%28MDG%29%2C%20a%20unified%20generative%20framework%20that%20reformulates%20multi-agent%20behavior%20modeling%20as%20the%20reconstruction%20of%20independently%20noised%20spatiotemporal%20tensors.%20Instead%20of%20relying%20on%20diffusion%20time%20steps%20or%20discrete%20tokenization%2C%20MDG%20applies%20continuous%2C%20per-agent%20and%20per-timestep%20noise%20masks%20that%20enable%20localized%20denoising%20and%20controllable%20trajectory%20generation%20in%20a%20single%20or%20few%20forward%20passes.%20This%20mask-driven%20formulation%20generalizes%20across%20open-loop%20prediction%2C%20closed-loop%20simulation%2C%20motion%20planning%2C%20and%20conditional%20generation%20within%20one%20model.%20Trained%20on%20large-scale%20real-world%20driving%20datasets%2C%20MDG%20achieves%20competitive%20closed-loop%20performance%20on%20the%20Waymo%20Sim%20Agents%20and%20nuPlan%20Planning%20benchmarks%2C%20while%20providing%20efficient%2C%20consistent%2C%20and%20controllable%20open-loop%20multi-agent%20trajectory%20generation.%20These%20results%20position%20MDG%20as%20a%20simple%20yet%20versatile%20paradigm%20for%20multi-agent%20behavior%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMDG%253A%2520Masked%2520Denoising%2520Generation%2520for%2520Multi-Agent%2520Behavior%2520Modeling%2520in%2520Traffic%2520Environments%26entry.906535625%3DZhiyu%2520Huang%2520and%2520Zewei%2520Zhou%2520and%2520Tianhui%2520Cai%2520and%2520Yun%2520Zhang%2520and%2520Jiaqi%2520Ma%26entry.1292438233%3DModeling%2520realistic%2520and%2520interactive%2520multi-agent%2520behavior%2520is%2520critical%2520to%2520autonomous%2520driving%2520and%2520traffic%2520simulation.%2520However%252C%2520existing%2520diffusion%2520and%2520autoregressive%2520approaches%2520are%2520limited%2520by%2520iterative%2520sampling%252C%2520sequential%2520decoding%252C%2520or%2520task-specific%2520designs%252C%2520which%2520hinder%2520efficiency%2520and%2520reuse.%2520We%2520propose%2520Masked%2520Denoising%2520Generation%2520%2528MDG%2529%252C%2520a%2520unified%2520generative%2520framework%2520that%2520reformulates%2520multi-agent%2520behavior%2520modeling%2520as%2520the%2520reconstruction%2520of%2520independently%2520noised%2520spatiotemporal%2520tensors.%2520Instead%2520of%2520relying%2520on%2520diffusion%2520time%2520steps%2520or%2520discrete%2520tokenization%252C%2520MDG%2520applies%2520continuous%252C%2520per-agent%2520and%2520per-timestep%2520noise%2520masks%2520that%2520enable%2520localized%2520denoising%2520and%2520controllable%2520trajectory%2520generation%2520in%2520a%2520single%2520or%2520few%2520forward%2520passes.%2520This%2520mask-driven%2520formulation%2520generalizes%2520across%2520open-loop%2520prediction%252C%2520closed-loop%2520simulation%252C%2520motion%2520planning%252C%2520and%2520conditional%2520generation%2520within%2520one%2520model.%2520Trained%2520on%2520large-scale%2520real-world%2520driving%2520datasets%252C%2520MDG%2520achieves%2520competitive%2520closed-loop%2520performance%2520on%2520the%2520Waymo%2520Sim%2520Agents%2520and%2520nuPlan%2520Planning%2520benchmarks%252C%2520while%2520providing%2520efficient%252C%2520consistent%252C%2520and%2520controllable%2520open-loop%2520multi-agent%2520trajectory%2520generation.%2520These%2520results%2520position%2520MDG%2520as%2520a%2520simple%2520yet%2520versatile%2520paradigm%2520for%2520multi-agent%2520behavior%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MDG%3A%20Masked%20Denoising%20Generation%20for%20Multi-Agent%20Behavior%20Modeling%20in%20Traffic%20Environments&entry.906535625=Zhiyu%20Huang%20and%20Zewei%20Zhou%20and%20Tianhui%20Cai%20and%20Yun%20Zhang%20and%20Jiaqi%20Ma&entry.1292438233=Modeling%20realistic%20and%20interactive%20multi-agent%20behavior%20is%20critical%20to%20autonomous%20driving%20and%20traffic%20simulation.%20However%2C%20existing%20diffusion%20and%20autoregressive%20approaches%20are%20limited%20by%20iterative%20sampling%2C%20sequential%20decoding%2C%20or%20task-specific%20designs%2C%20which%20hinder%20efficiency%20and%20reuse.%20We%20propose%20Masked%20Denoising%20Generation%20%28MDG%29%2C%20a%20unified%20generative%20framework%20that%20reformulates%20multi-agent%20behavior%20modeling%20as%20the%20reconstruction%20of%20independently%20noised%20spatiotemporal%20tensors.%20Instead%20of%20relying%20on%20diffusion%20time%20steps%20or%20discrete%20tokenization%2C%20MDG%20applies%20continuous%2C%20per-agent%20and%20per-timestep%20noise%20masks%20that%20enable%20localized%20denoising%20and%20controllable%20trajectory%20generation%20in%20a%20single%20or%20few%20forward%20passes.%20This%20mask-driven%20formulation%20generalizes%20across%20open-loop%20prediction%2C%20closed-loop%20simulation%2C%20motion%20planning%2C%20and%20conditional%20generation%20within%20one%20model.%20Trained%20on%20large-scale%20real-world%20driving%20datasets%2C%20MDG%20achieves%20competitive%20closed-loop%20performance%20on%20the%20Waymo%20Sim%20Agents%20and%20nuPlan%20Planning%20benchmarks%2C%20while%20providing%20efficient%2C%20consistent%2C%20and%20controllable%20open-loop%20multi-agent%20trajectory%20generation.%20These%20results%20position%20MDG%20as%20a%20simple%20yet%20versatile%20paradigm%20for%20multi-agent%20behavior%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2511.17496v1&entry.124074799=Read"},
{"title": "The promise and limits of LLMs in constructing proofs and hints for logic problems in intelligent tutoring systems", "author": "Sutapa Dey Tithi and Arun Kumar Ramesh and Clara DiMarco and Xiaoyi Tian and Nazia Alam and Kimia Fazeli and Tiffany Barnes", "abstract": "Intelligent tutoring systems have demonstrated effectiveness in teaching formal propositional logic proofs, but their reliance on template-based explanations limits their ability to provide personalized student feedback. While large language models (LLMs) offer promising capabilities for dynamic feedback generation, they risk producing hallucinations or pedagogically unsound explanations. We evaluated the stepwise accuracy of LLMs in constructing multi-step symbolic logic proofs, comparing six prompting techniques across four state-of-the-art LLMs on 358 propositional logic problems. Results show that DeepSeek-V3 achieved superior performance up to 86.7% accuracy on stepwise proof construction and excelled particularly in simpler rules. We further used the best-performing LLM to generate explanatory hints for 1,050 unique student problem-solving states from a logic ITS and evaluated them on 4 criteria with both an LLM grader and human expert ratings on a 20% sample. Our analysis finds that LLM-generated hints were 75% accurate and rated highly by human evaluators on consistency and clarity, but did not perform as well explaining why the hint was provided or its larger context. Our results demonstrate that LLMs may be used to augment tutoring systems with logic tutoring hints, but require additional modifications to ensure accuracy and pedagogical appropriateness.", "link": "http://arxiv.org/abs/2505.04736v2", "date": "2025-11-21", "relevancy": 2.3054, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4791}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4521}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20promise%20and%20limits%20of%20LLMs%20in%20constructing%20proofs%20and%20hints%20for%20logic%20problems%20in%20intelligent%20tutoring%20systems&body=Title%3A%20The%20promise%20and%20limits%20of%20LLMs%20in%20constructing%20proofs%20and%20hints%20for%20logic%20problems%20in%20intelligent%20tutoring%20systems%0AAuthor%3A%20Sutapa%20Dey%20Tithi%20and%20Arun%20Kumar%20Ramesh%20and%20Clara%20DiMarco%20and%20Xiaoyi%20Tian%20and%20Nazia%20Alam%20and%20Kimia%20Fazeli%20and%20Tiffany%20Barnes%0AAbstract%3A%20Intelligent%20tutoring%20systems%20have%20demonstrated%20effectiveness%20in%20teaching%20formal%20propositional%20logic%20proofs%2C%20but%20their%20reliance%20on%20template-based%20explanations%20limits%20their%20ability%20to%20provide%20personalized%20student%20feedback.%20While%20large%20language%20models%20%28LLMs%29%20offer%20promising%20capabilities%20for%20dynamic%20feedback%20generation%2C%20they%20risk%20producing%20hallucinations%20or%20pedagogically%20unsound%20explanations.%20We%20evaluated%20the%20stepwise%20accuracy%20of%20LLMs%20in%20constructing%20multi-step%20symbolic%20logic%20proofs%2C%20comparing%20six%20prompting%20techniques%20across%20four%20state-of-the-art%20LLMs%20on%20358%20propositional%20logic%20problems.%20Results%20show%20that%20DeepSeek-V3%20achieved%20superior%20performance%20up%20to%2086.7%25%20accuracy%20on%20stepwise%20proof%20construction%20and%20excelled%20particularly%20in%20simpler%20rules.%20We%20further%20used%20the%20best-performing%20LLM%20to%20generate%20explanatory%20hints%20for%201%2C050%20unique%20student%20problem-solving%20states%20from%20a%20logic%20ITS%20and%20evaluated%20them%20on%204%20criteria%20with%20both%20an%20LLM%20grader%20and%20human%20expert%20ratings%20on%20a%2020%25%20sample.%20Our%20analysis%20finds%20that%20LLM-generated%20hints%20were%2075%25%20accurate%20and%20rated%20highly%20by%20human%20evaluators%20on%20consistency%20and%20clarity%2C%20but%20did%20not%20perform%20as%20well%20explaining%20why%20the%20hint%20was%20provided%20or%20its%20larger%20context.%20Our%20results%20demonstrate%20that%20LLMs%20may%20be%20used%20to%20augment%20tutoring%20systems%20with%20logic%20tutoring%20hints%2C%20but%20require%20additional%20modifications%20to%20ensure%20accuracy%20and%20pedagogical%20appropriateness.%0ALink%3A%20http%3A//arxiv.org/abs/2505.04736v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520promise%2520and%2520limits%2520of%2520LLMs%2520in%2520constructing%2520proofs%2520and%2520hints%2520for%2520logic%2520problems%2520in%2520intelligent%2520tutoring%2520systems%26entry.906535625%3DSutapa%2520Dey%2520Tithi%2520and%2520Arun%2520Kumar%2520Ramesh%2520and%2520Clara%2520DiMarco%2520and%2520Xiaoyi%2520Tian%2520and%2520Nazia%2520Alam%2520and%2520Kimia%2520Fazeli%2520and%2520Tiffany%2520Barnes%26entry.1292438233%3DIntelligent%2520tutoring%2520systems%2520have%2520demonstrated%2520effectiveness%2520in%2520teaching%2520formal%2520propositional%2520logic%2520proofs%252C%2520but%2520their%2520reliance%2520on%2520template-based%2520explanations%2520limits%2520their%2520ability%2520to%2520provide%2520personalized%2520student%2520feedback.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520offer%2520promising%2520capabilities%2520for%2520dynamic%2520feedback%2520generation%252C%2520they%2520risk%2520producing%2520hallucinations%2520or%2520pedagogically%2520unsound%2520explanations.%2520We%2520evaluated%2520the%2520stepwise%2520accuracy%2520of%2520LLMs%2520in%2520constructing%2520multi-step%2520symbolic%2520logic%2520proofs%252C%2520comparing%2520six%2520prompting%2520techniques%2520across%2520four%2520state-of-the-art%2520LLMs%2520on%2520358%2520propositional%2520logic%2520problems.%2520Results%2520show%2520that%2520DeepSeek-V3%2520achieved%2520superior%2520performance%2520up%2520to%252086.7%2525%2520accuracy%2520on%2520stepwise%2520proof%2520construction%2520and%2520excelled%2520particularly%2520in%2520simpler%2520rules.%2520We%2520further%2520used%2520the%2520best-performing%2520LLM%2520to%2520generate%2520explanatory%2520hints%2520for%25201%252C050%2520unique%2520student%2520problem-solving%2520states%2520from%2520a%2520logic%2520ITS%2520and%2520evaluated%2520them%2520on%25204%2520criteria%2520with%2520both%2520an%2520LLM%2520grader%2520and%2520human%2520expert%2520ratings%2520on%2520a%252020%2525%2520sample.%2520Our%2520analysis%2520finds%2520that%2520LLM-generated%2520hints%2520were%252075%2525%2520accurate%2520and%2520rated%2520highly%2520by%2520human%2520evaluators%2520on%2520consistency%2520and%2520clarity%252C%2520but%2520did%2520not%2520perform%2520as%2520well%2520explaining%2520why%2520the%2520hint%2520was%2520provided%2520or%2520its%2520larger%2520context.%2520Our%2520results%2520demonstrate%2520that%2520LLMs%2520may%2520be%2520used%2520to%2520augment%2520tutoring%2520systems%2520with%2520logic%2520tutoring%2520hints%252C%2520but%2520require%2520additional%2520modifications%2520to%2520ensure%2520accuracy%2520and%2520pedagogical%2520appropriateness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04736v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20promise%20and%20limits%20of%20LLMs%20in%20constructing%20proofs%20and%20hints%20for%20logic%20problems%20in%20intelligent%20tutoring%20systems&entry.906535625=Sutapa%20Dey%20Tithi%20and%20Arun%20Kumar%20Ramesh%20and%20Clara%20DiMarco%20and%20Xiaoyi%20Tian%20and%20Nazia%20Alam%20and%20Kimia%20Fazeli%20and%20Tiffany%20Barnes&entry.1292438233=Intelligent%20tutoring%20systems%20have%20demonstrated%20effectiveness%20in%20teaching%20formal%20propositional%20logic%20proofs%2C%20but%20their%20reliance%20on%20template-based%20explanations%20limits%20their%20ability%20to%20provide%20personalized%20student%20feedback.%20While%20large%20language%20models%20%28LLMs%29%20offer%20promising%20capabilities%20for%20dynamic%20feedback%20generation%2C%20they%20risk%20producing%20hallucinations%20or%20pedagogically%20unsound%20explanations.%20We%20evaluated%20the%20stepwise%20accuracy%20of%20LLMs%20in%20constructing%20multi-step%20symbolic%20logic%20proofs%2C%20comparing%20six%20prompting%20techniques%20across%20four%20state-of-the-art%20LLMs%20on%20358%20propositional%20logic%20problems.%20Results%20show%20that%20DeepSeek-V3%20achieved%20superior%20performance%20up%20to%2086.7%25%20accuracy%20on%20stepwise%20proof%20construction%20and%20excelled%20particularly%20in%20simpler%20rules.%20We%20further%20used%20the%20best-performing%20LLM%20to%20generate%20explanatory%20hints%20for%201%2C050%20unique%20student%20problem-solving%20states%20from%20a%20logic%20ITS%20and%20evaluated%20them%20on%204%20criteria%20with%20both%20an%20LLM%20grader%20and%20human%20expert%20ratings%20on%20a%2020%25%20sample.%20Our%20analysis%20finds%20that%20LLM-generated%20hints%20were%2075%25%20accurate%20and%20rated%20highly%20by%20human%20evaluators%20on%20consistency%20and%20clarity%2C%20but%20did%20not%20perform%20as%20well%20explaining%20why%20the%20hint%20was%20provided%20or%20its%20larger%20context.%20Our%20results%20demonstrate%20that%20LLMs%20may%20be%20used%20to%20augment%20tutoring%20systems%20with%20logic%20tutoring%20hints%2C%20but%20require%20additional%20modifications%20to%20ensure%20accuracy%20and%20pedagogical%20appropriateness.&entry.1838667208=http%3A//arxiv.org/abs/2505.04736v2&entry.124074799=Read"},
{"title": "CATCODER: Repository-Level Code Generation with Relevant Code and Type Context", "author": "Zhiyuan Pan and Xing Hu and Xin Xia and Xiaohu Yang", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, repository-level code generation presents unique challenges, particularly due to the need to utilize information spread across multiple files within a repository. Specifically, successful generation depends on a solid grasp of both general, context-agnostic knowledge and specific, context-dependent knowledge. While LLMs are widely used for the context-agnostic aspect, existing retrieval-based approaches sometimes fall short as they are limited in obtaining a broader and deeper repository context. In this paper, we present CatCoder, a novel code generation framework designed for statically typed programming languages. CatCoder enhances repository-level code generation by integrating relevant code and type context. Specifically, it leverages static analyzers to extract type dependencies and merges this information with retrieved code to create comprehensive prompts for LLMs. To evaluate the effectiveness of CatCoder, we adapt and construct benchmarks that include 199 Java tasks and 90 Rust tasks. The results show that CatCoder outperforms the RepoCoder baseline by up to 14.44% and 17.35%, in terms of compile@k and pass@k scores. In addition, the generalizability of CatCoder is assessed using various LLMs, including both code-specialized models and general-purpose models. Our findings indicate consistent performance improvements across all models, which underlines the practicality of CatCoder. Furthermore, we evaluate the time consumption of CatCoder in a large open source repository, and the results demonstrate the scalability of CatCoder.", "link": "http://arxiv.org/abs/2406.03283v2", "date": "2025-11-21", "relevancy": 2.3035, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CATCODER%3A%20Repository-Level%20Code%20Generation%20with%20Relevant%20Code%20and%20Type%20Context&body=Title%3A%20CATCODER%3A%20Repository-Level%20Code%20Generation%20with%20Relevant%20Code%20and%20Type%20Context%0AAuthor%3A%20Zhiyuan%20Pan%20and%20Xing%20Hu%20and%20Xin%20Xia%20and%20Xiaohu%20Yang%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%20code%20generation%20tasks.%20However%2C%20repository-level%20code%20generation%20presents%20unique%20challenges%2C%20particularly%20due%20to%20the%20need%20to%20utilize%20information%20spread%20across%20multiple%20files%20within%20a%20repository.%20Specifically%2C%20successful%20generation%20depends%20on%20a%20solid%20grasp%20of%20both%20general%2C%20context-agnostic%20knowledge%20and%20specific%2C%20context-dependent%20knowledge.%20While%20LLMs%20are%20widely%20used%20for%20the%20context-agnostic%20aspect%2C%20existing%20retrieval-based%20approaches%20sometimes%20fall%20short%20as%20they%20are%20limited%20in%20obtaining%20a%20broader%20and%20deeper%20repository%20context.%20In%20this%20paper%2C%20we%20present%20CatCoder%2C%20a%20novel%20code%20generation%20framework%20designed%20for%20statically%20typed%20programming%20languages.%20CatCoder%20enhances%20repository-level%20code%20generation%20by%20integrating%20relevant%20code%20and%20type%20context.%20Specifically%2C%20it%20leverages%20static%20analyzers%20to%20extract%20type%20dependencies%20and%20merges%20this%20information%20with%20retrieved%20code%20to%20create%20comprehensive%20prompts%20for%20LLMs.%20To%20evaluate%20the%20effectiveness%20of%20CatCoder%2C%20we%20adapt%20and%20construct%20benchmarks%20that%20include%20199%20Java%20tasks%20and%2090%20Rust%20tasks.%20The%20results%20show%20that%20CatCoder%20outperforms%20the%20RepoCoder%20baseline%20by%20up%20to%2014.44%25%20and%2017.35%25%2C%20in%20terms%20of%20compile%40k%20and%20pass%40k%20scores.%20In%20addition%2C%20the%20generalizability%20of%20CatCoder%20is%20assessed%20using%20various%20LLMs%2C%20including%20both%20code-specialized%20models%20and%20general-purpose%20models.%20Our%20findings%20indicate%20consistent%20performance%20improvements%20across%20all%20models%2C%20which%20underlines%20the%20practicality%20of%20CatCoder.%20Furthermore%2C%20we%20evaluate%20the%20time%20consumption%20of%20CatCoder%20in%20a%20large%20open%20source%20repository%2C%20and%20the%20results%20demonstrate%20the%20scalability%20of%20CatCoder.%0ALink%3A%20http%3A//arxiv.org/abs/2406.03283v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCATCODER%253A%2520Repository-Level%2520Code%2520Generation%2520with%2520Relevant%2520Code%2520and%2520Type%2520Context%26entry.906535625%3DZhiyuan%2520Pan%2520and%2520Xing%2520Hu%2520and%2520Xin%2520Xia%2520and%2520Xiaohu%2520Yang%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%2520code%2520generation%2520tasks.%2520However%252C%2520repository-level%2520code%2520generation%2520presents%2520unique%2520challenges%252C%2520particularly%2520due%2520to%2520the%2520need%2520to%2520utilize%2520information%2520spread%2520across%2520multiple%2520files%2520within%2520a%2520repository.%2520Specifically%252C%2520successful%2520generation%2520depends%2520on%2520a%2520solid%2520grasp%2520of%2520both%2520general%252C%2520context-agnostic%2520knowledge%2520and%2520specific%252C%2520context-dependent%2520knowledge.%2520While%2520LLMs%2520are%2520widely%2520used%2520for%2520the%2520context-agnostic%2520aspect%252C%2520existing%2520retrieval-based%2520approaches%2520sometimes%2520fall%2520short%2520as%2520they%2520are%2520limited%2520in%2520obtaining%2520a%2520broader%2520and%2520deeper%2520repository%2520context.%2520In%2520this%2520paper%252C%2520we%2520present%2520CatCoder%252C%2520a%2520novel%2520code%2520generation%2520framework%2520designed%2520for%2520statically%2520typed%2520programming%2520languages.%2520CatCoder%2520enhances%2520repository-level%2520code%2520generation%2520by%2520integrating%2520relevant%2520code%2520and%2520type%2520context.%2520Specifically%252C%2520it%2520leverages%2520static%2520analyzers%2520to%2520extract%2520type%2520dependencies%2520and%2520merges%2520this%2520information%2520with%2520retrieved%2520code%2520to%2520create%2520comprehensive%2520prompts%2520for%2520LLMs.%2520To%2520evaluate%2520the%2520effectiveness%2520of%2520CatCoder%252C%2520we%2520adapt%2520and%2520construct%2520benchmarks%2520that%2520include%2520199%2520Java%2520tasks%2520and%252090%2520Rust%2520tasks.%2520The%2520results%2520show%2520that%2520CatCoder%2520outperforms%2520the%2520RepoCoder%2520baseline%2520by%2520up%2520to%252014.44%2525%2520and%252017.35%2525%252C%2520in%2520terms%2520of%2520compile%2540k%2520and%2520pass%2540k%2520scores.%2520In%2520addition%252C%2520the%2520generalizability%2520of%2520CatCoder%2520is%2520assessed%2520using%2520various%2520LLMs%252C%2520including%2520both%2520code-specialized%2520models%2520and%2520general-purpose%2520models.%2520Our%2520findings%2520indicate%2520consistent%2520performance%2520improvements%2520across%2520all%2520models%252C%2520which%2520underlines%2520the%2520practicality%2520of%2520CatCoder.%2520Furthermore%252C%2520we%2520evaluate%2520the%2520time%2520consumption%2520of%2520CatCoder%2520in%2520a%2520large%2520open%2520source%2520repository%252C%2520and%2520the%2520results%2520demonstrate%2520the%2520scalability%2520of%2520CatCoder.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03283v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CATCODER%3A%20Repository-Level%20Code%20Generation%20with%20Relevant%20Code%20and%20Type%20Context&entry.906535625=Zhiyuan%20Pan%20and%20Xing%20Hu%20and%20Xin%20Xia%20and%20Xiaohu%20Yang&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%20code%20generation%20tasks.%20However%2C%20repository-level%20code%20generation%20presents%20unique%20challenges%2C%20particularly%20due%20to%20the%20need%20to%20utilize%20information%20spread%20across%20multiple%20files%20within%20a%20repository.%20Specifically%2C%20successful%20generation%20depends%20on%20a%20solid%20grasp%20of%20both%20general%2C%20context-agnostic%20knowledge%20and%20specific%2C%20context-dependent%20knowledge.%20While%20LLMs%20are%20widely%20used%20for%20the%20context-agnostic%20aspect%2C%20existing%20retrieval-based%20approaches%20sometimes%20fall%20short%20as%20they%20are%20limited%20in%20obtaining%20a%20broader%20and%20deeper%20repository%20context.%20In%20this%20paper%2C%20we%20present%20CatCoder%2C%20a%20novel%20code%20generation%20framework%20designed%20for%20statically%20typed%20programming%20languages.%20CatCoder%20enhances%20repository-level%20code%20generation%20by%20integrating%20relevant%20code%20and%20type%20context.%20Specifically%2C%20it%20leverages%20static%20analyzers%20to%20extract%20type%20dependencies%20and%20merges%20this%20information%20with%20retrieved%20code%20to%20create%20comprehensive%20prompts%20for%20LLMs.%20To%20evaluate%20the%20effectiveness%20of%20CatCoder%2C%20we%20adapt%20and%20construct%20benchmarks%20that%20include%20199%20Java%20tasks%20and%2090%20Rust%20tasks.%20The%20results%20show%20that%20CatCoder%20outperforms%20the%20RepoCoder%20baseline%20by%20up%20to%2014.44%25%20and%2017.35%25%2C%20in%20terms%20of%20compile%40k%20and%20pass%40k%20scores.%20In%20addition%2C%20the%20generalizability%20of%20CatCoder%20is%20assessed%20using%20various%20LLMs%2C%20including%20both%20code-specialized%20models%20and%20general-purpose%20models.%20Our%20findings%20indicate%20consistent%20performance%20improvements%20across%20all%20models%2C%20which%20underlines%20the%20practicality%20of%20CatCoder.%20Furthermore%2C%20we%20evaluate%20the%20time%20consumption%20of%20CatCoder%20in%20a%20large%20open%20source%20repository%2C%20and%20the%20results%20demonstrate%20the%20scalability%20of%20CatCoder.&entry.1838667208=http%3A//arxiv.org/abs/2406.03283v2&entry.124074799=Read"},
{"title": "Progress-Think: Semantic Progress Reasoning for Vision-Language Navigation", "author": "Shuo Wang and Yucheng Wang and Guoxin Lian and Yongcai Wang and Maiyue Chen and Kaihui Wang and Bo Zhang and Zhizhong Su and Yutian Zhou and Wanting Li and Deying Li and Zhaoxin Fan", "abstract": "Vision-Language Navigation requires agents to act coherently over long horizons by understanding not only local visual context but also how far they have advanced within a multi-step instruction. However, recent Vision-Language-Action models focus on direct action prediction and earlier progress methods predict numeric achievements; both overlook the monotonic co-progression property of the observation and instruction sequences. Building on this insight, Progress-Think introduces semantic progress reasoning, predicting instruction-style progress from visual observations to enable more accurate navigation. To achieve this without expensive annotations, we propose a three-stage framework. In the initial stage, Self-Aligned Progress Pretraining bootstraps a reasoning module via a novel differentiable alignment between visual history and instruction prefixes. Then, Progress-Guided Policy Pretraining injects learned progress states into the navigation context, guiding the policy toward consistent actions. Finally, Progress-Policy Co-Finetuning jointly optimizes both modules with tailored progress-aware reinforcement objectives. Experiments on R2R-CE and RxR-CE show state-of-the-art success and efficiency, demonstrating that semantic progress yields a more consistent representation of navigation advancement.", "link": "http://arxiv.org/abs/2511.17097v1", "date": "2025-11-21", "relevancy": 2.2969, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5931}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progress-Think%3A%20Semantic%20Progress%20Reasoning%20for%20Vision-Language%20Navigation&body=Title%3A%20Progress-Think%3A%20Semantic%20Progress%20Reasoning%20for%20Vision-Language%20Navigation%0AAuthor%3A%20Shuo%20Wang%20and%20Yucheng%20Wang%20and%20Guoxin%20Lian%20and%20Yongcai%20Wang%20and%20Maiyue%20Chen%20and%20Kaihui%20Wang%20and%20Bo%20Zhang%20and%20Zhizhong%20Su%20and%20Yutian%20Zhou%20and%20Wanting%20Li%20and%20Deying%20Li%20and%20Zhaoxin%20Fan%0AAbstract%3A%20Vision-Language%20Navigation%20requires%20agents%20to%20act%20coherently%20over%20long%20horizons%20by%20understanding%20not%20only%20local%20visual%20context%20but%20also%20how%20far%20they%20have%20advanced%20within%20a%20multi-step%20instruction.%20However%2C%20recent%20Vision-Language-Action%20models%20focus%20on%20direct%20action%20prediction%20and%20earlier%20progress%20methods%20predict%20numeric%20achievements%3B%20both%20overlook%20the%20monotonic%20co-progression%20property%20of%20the%20observation%20and%20instruction%20sequences.%20Building%20on%20this%20insight%2C%20Progress-Think%20introduces%20semantic%20progress%20reasoning%2C%20predicting%20instruction-style%20progress%20from%20visual%20observations%20to%20enable%20more%20accurate%20navigation.%20To%20achieve%20this%20without%20expensive%20annotations%2C%20we%20propose%20a%20three-stage%20framework.%20In%20the%20initial%20stage%2C%20Self-Aligned%20Progress%20Pretraining%20bootstraps%20a%20reasoning%20module%20via%20a%20novel%20differentiable%20alignment%20between%20visual%20history%20and%20instruction%20prefixes.%20Then%2C%20Progress-Guided%20Policy%20Pretraining%20injects%20learned%20progress%20states%20into%20the%20navigation%20context%2C%20guiding%20the%20policy%20toward%20consistent%20actions.%20Finally%2C%20Progress-Policy%20Co-Finetuning%20jointly%20optimizes%20both%20modules%20with%20tailored%20progress-aware%20reinforcement%20objectives.%20Experiments%20on%20R2R-CE%20and%20RxR-CE%20show%20state-of-the-art%20success%20and%20efficiency%2C%20demonstrating%20that%20semantic%20progress%20yields%20a%20more%20consistent%20representation%20of%20navigation%20advancement.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgress-Think%253A%2520Semantic%2520Progress%2520Reasoning%2520for%2520Vision-Language%2520Navigation%26entry.906535625%3DShuo%2520Wang%2520and%2520Yucheng%2520Wang%2520and%2520Guoxin%2520Lian%2520and%2520Yongcai%2520Wang%2520and%2520Maiyue%2520Chen%2520and%2520Kaihui%2520Wang%2520and%2520Bo%2520Zhang%2520and%2520Zhizhong%2520Su%2520and%2520Yutian%2520Zhou%2520and%2520Wanting%2520Li%2520and%2520Deying%2520Li%2520and%2520Zhaoxin%2520Fan%26entry.1292438233%3DVision-Language%2520Navigation%2520requires%2520agents%2520to%2520act%2520coherently%2520over%2520long%2520horizons%2520by%2520understanding%2520not%2520only%2520local%2520visual%2520context%2520but%2520also%2520how%2520far%2520they%2520have%2520advanced%2520within%2520a%2520multi-step%2520instruction.%2520However%252C%2520recent%2520Vision-Language-Action%2520models%2520focus%2520on%2520direct%2520action%2520prediction%2520and%2520earlier%2520progress%2520methods%2520predict%2520numeric%2520achievements%253B%2520both%2520overlook%2520the%2520monotonic%2520co-progression%2520property%2520of%2520the%2520observation%2520and%2520instruction%2520sequences.%2520Building%2520on%2520this%2520insight%252C%2520Progress-Think%2520introduces%2520semantic%2520progress%2520reasoning%252C%2520predicting%2520instruction-style%2520progress%2520from%2520visual%2520observations%2520to%2520enable%2520more%2520accurate%2520navigation.%2520To%2520achieve%2520this%2520without%2520expensive%2520annotations%252C%2520we%2520propose%2520a%2520three-stage%2520framework.%2520In%2520the%2520initial%2520stage%252C%2520Self-Aligned%2520Progress%2520Pretraining%2520bootstraps%2520a%2520reasoning%2520module%2520via%2520a%2520novel%2520differentiable%2520alignment%2520between%2520visual%2520history%2520and%2520instruction%2520prefixes.%2520Then%252C%2520Progress-Guided%2520Policy%2520Pretraining%2520injects%2520learned%2520progress%2520states%2520into%2520the%2520navigation%2520context%252C%2520guiding%2520the%2520policy%2520toward%2520consistent%2520actions.%2520Finally%252C%2520Progress-Policy%2520Co-Finetuning%2520jointly%2520optimizes%2520both%2520modules%2520with%2520tailored%2520progress-aware%2520reinforcement%2520objectives.%2520Experiments%2520on%2520R2R-CE%2520and%2520RxR-CE%2520show%2520state-of-the-art%2520success%2520and%2520efficiency%252C%2520demonstrating%2520that%2520semantic%2520progress%2520yields%2520a%2520more%2520consistent%2520representation%2520of%2520navigation%2520advancement.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progress-Think%3A%20Semantic%20Progress%20Reasoning%20for%20Vision-Language%20Navigation&entry.906535625=Shuo%20Wang%20and%20Yucheng%20Wang%20and%20Guoxin%20Lian%20and%20Yongcai%20Wang%20and%20Maiyue%20Chen%20and%20Kaihui%20Wang%20and%20Bo%20Zhang%20and%20Zhizhong%20Su%20and%20Yutian%20Zhou%20and%20Wanting%20Li%20and%20Deying%20Li%20and%20Zhaoxin%20Fan&entry.1292438233=Vision-Language%20Navigation%20requires%20agents%20to%20act%20coherently%20over%20long%20horizons%20by%20understanding%20not%20only%20local%20visual%20context%20but%20also%20how%20far%20they%20have%20advanced%20within%20a%20multi-step%20instruction.%20However%2C%20recent%20Vision-Language-Action%20models%20focus%20on%20direct%20action%20prediction%20and%20earlier%20progress%20methods%20predict%20numeric%20achievements%3B%20both%20overlook%20the%20monotonic%20co-progression%20property%20of%20the%20observation%20and%20instruction%20sequences.%20Building%20on%20this%20insight%2C%20Progress-Think%20introduces%20semantic%20progress%20reasoning%2C%20predicting%20instruction-style%20progress%20from%20visual%20observations%20to%20enable%20more%20accurate%20navigation.%20To%20achieve%20this%20without%20expensive%20annotations%2C%20we%20propose%20a%20three-stage%20framework.%20In%20the%20initial%20stage%2C%20Self-Aligned%20Progress%20Pretraining%20bootstraps%20a%20reasoning%20module%20via%20a%20novel%20differentiable%20alignment%20between%20visual%20history%20and%20instruction%20prefixes.%20Then%2C%20Progress-Guided%20Policy%20Pretraining%20injects%20learned%20progress%20states%20into%20the%20navigation%20context%2C%20guiding%20the%20policy%20toward%20consistent%20actions.%20Finally%2C%20Progress-Policy%20Co-Finetuning%20jointly%20optimizes%20both%20modules%20with%20tailored%20progress-aware%20reinforcement%20objectives.%20Experiments%20on%20R2R-CE%20and%20RxR-CE%20show%20state-of-the-art%20success%20and%20efficiency%2C%20demonstrating%20that%20semantic%20progress%20yields%20a%20more%20consistent%20representation%20of%20navigation%20advancement.&entry.1838667208=http%3A//arxiv.org/abs/2511.17097v1&entry.124074799=Read"},
{"title": "CleverDistiller: Simple and Spatially Consistent Cross-modal Distillation", "author": "Hariprasath Govindarajan and Maciej K. Wozniak and Marvin Klingner and Camille Maurice and B Ravi Kiran and Senthil Yogamani", "abstract": "Vision foundation models (VFMs) such as DINO have led to a paradigm shift in 2D camera-based perception towards extracting generalized features to support many downstream tasks. Recent works introduce self-supervised cross-modal knowledge distillation (KD) as a way to transfer these powerful generalization capabilities into 3D LiDAR-based models. However, they either rely on highly complex distillation losses, pseudo-semantic maps, or limit KD to features useful for semantic segmentation only. In this work, we propose CleverDistiller, a self-supervised, cross-modal 2D-to-3D KD framework introducing a set of simple yet effective design choices: Unlike contrastive approaches relying on complex loss design choices, our method employs a direct feature similarity loss in combination with a multi layer perceptron (MLP) projection head to allow the 3D network to learn complex semantic dependencies throughout the projection. Crucially, our approach does not depend on pseudo-semantic maps, allowing for direct knowledge transfer from a VFM without explicit semantic supervision. Additionally, we introduce the auxiliary self-supervised spatial task of occupancy prediction to enhance the semantic knowledge, obtained from a VFM through KD, with 3D spatial reasoning capabilities. Experiments on standard autonomous driving benchmarks for 2D-to-3D KD demonstrate that CleverDistiller achieves state-of-the-art performance in both semantic segmentation and 3D object detection (3DOD) by up to 10% mIoU, especially when fine tuning on really low data amounts, showing the effectiveness of our simple yet powerful KD strategy", "link": "http://arxiv.org/abs/2503.09878v4", "date": "2025-11-21", "relevancy": 2.2957, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5922}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5625}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CleverDistiller%3A%20Simple%20and%20Spatially%20Consistent%20Cross-modal%20Distillation&body=Title%3A%20CleverDistiller%3A%20Simple%20and%20Spatially%20Consistent%20Cross-modal%20Distillation%0AAuthor%3A%20Hariprasath%20Govindarajan%20and%20Maciej%20K.%20Wozniak%20and%20Marvin%20Klingner%20and%20Camille%20Maurice%20and%20B%20Ravi%20Kiran%20and%20Senthil%20Yogamani%0AAbstract%3A%20Vision%20foundation%20models%20%28VFMs%29%20such%20as%20DINO%20have%20led%20to%20a%20paradigm%20shift%20in%202D%20camera-based%20perception%20towards%20extracting%20generalized%20features%20to%20support%20many%20downstream%20tasks.%20Recent%20works%20introduce%20self-supervised%20cross-modal%20knowledge%20distillation%20%28KD%29%20as%20a%20way%20to%20transfer%20these%20powerful%20generalization%20capabilities%20into%203D%20LiDAR-based%20models.%20However%2C%20they%20either%20rely%20on%20highly%20complex%20distillation%20losses%2C%20pseudo-semantic%20maps%2C%20or%20limit%20KD%20to%20features%20useful%20for%20semantic%20segmentation%20only.%20In%20this%20work%2C%20we%20propose%20CleverDistiller%2C%20a%20self-supervised%2C%20cross-modal%202D-to-3D%20KD%20framework%20introducing%20a%20set%20of%20simple%20yet%20effective%20design%20choices%3A%20Unlike%20contrastive%20approaches%20relying%20on%20complex%20loss%20design%20choices%2C%20our%20method%20employs%20a%20direct%20feature%20similarity%20loss%20in%20combination%20with%20a%20multi%20layer%20perceptron%20%28MLP%29%20projection%20head%20to%20allow%20the%203D%20network%20to%20learn%20complex%20semantic%20dependencies%20throughout%20the%20projection.%20Crucially%2C%20our%20approach%20does%20not%20depend%20on%20pseudo-semantic%20maps%2C%20allowing%20for%20direct%20knowledge%20transfer%20from%20a%20VFM%20without%20explicit%20semantic%20supervision.%20Additionally%2C%20we%20introduce%20the%20auxiliary%20self-supervised%20spatial%20task%20of%20occupancy%20prediction%20to%20enhance%20the%20semantic%20knowledge%2C%20obtained%20from%20a%20VFM%20through%20KD%2C%20with%203D%20spatial%20reasoning%20capabilities.%20Experiments%20on%20standard%20autonomous%20driving%20benchmarks%20for%202D-to-3D%20KD%20demonstrate%20that%20CleverDistiller%20achieves%20state-of-the-art%20performance%20in%20both%20semantic%20segmentation%20and%203D%20object%20detection%20%283DOD%29%20by%20up%20to%2010%25%20mIoU%2C%20especially%20when%20fine%20tuning%20on%20really%20low%20data%20amounts%2C%20showing%20the%20effectiveness%20of%20our%20simple%20yet%20powerful%20KD%20strategy%0ALink%3A%20http%3A//arxiv.org/abs/2503.09878v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCleverDistiller%253A%2520Simple%2520and%2520Spatially%2520Consistent%2520Cross-modal%2520Distillation%26entry.906535625%3DHariprasath%2520Govindarajan%2520and%2520Maciej%2520K.%2520Wozniak%2520and%2520Marvin%2520Klingner%2520and%2520Camille%2520Maurice%2520and%2520B%2520Ravi%2520Kiran%2520and%2520Senthil%2520Yogamani%26entry.1292438233%3DVision%2520foundation%2520models%2520%2528VFMs%2529%2520such%2520as%2520DINO%2520have%2520led%2520to%2520a%2520paradigm%2520shift%2520in%25202D%2520camera-based%2520perception%2520towards%2520extracting%2520generalized%2520features%2520to%2520support%2520many%2520downstream%2520tasks.%2520Recent%2520works%2520introduce%2520self-supervised%2520cross-modal%2520knowledge%2520distillation%2520%2528KD%2529%2520as%2520a%2520way%2520to%2520transfer%2520these%2520powerful%2520generalization%2520capabilities%2520into%25203D%2520LiDAR-based%2520models.%2520However%252C%2520they%2520either%2520rely%2520on%2520highly%2520complex%2520distillation%2520losses%252C%2520pseudo-semantic%2520maps%252C%2520or%2520limit%2520KD%2520to%2520features%2520useful%2520for%2520semantic%2520segmentation%2520only.%2520In%2520this%2520work%252C%2520we%2520propose%2520CleverDistiller%252C%2520a%2520self-supervised%252C%2520cross-modal%25202D-to-3D%2520KD%2520framework%2520introducing%2520a%2520set%2520of%2520simple%2520yet%2520effective%2520design%2520choices%253A%2520Unlike%2520contrastive%2520approaches%2520relying%2520on%2520complex%2520loss%2520design%2520choices%252C%2520our%2520method%2520employs%2520a%2520direct%2520feature%2520similarity%2520loss%2520in%2520combination%2520with%2520a%2520multi%2520layer%2520perceptron%2520%2528MLP%2529%2520projection%2520head%2520to%2520allow%2520the%25203D%2520network%2520to%2520learn%2520complex%2520semantic%2520dependencies%2520throughout%2520the%2520projection.%2520Crucially%252C%2520our%2520approach%2520does%2520not%2520depend%2520on%2520pseudo-semantic%2520maps%252C%2520allowing%2520for%2520direct%2520knowledge%2520transfer%2520from%2520a%2520VFM%2520without%2520explicit%2520semantic%2520supervision.%2520Additionally%252C%2520we%2520introduce%2520the%2520auxiliary%2520self-supervised%2520spatial%2520task%2520of%2520occupancy%2520prediction%2520to%2520enhance%2520the%2520semantic%2520knowledge%252C%2520obtained%2520from%2520a%2520VFM%2520through%2520KD%252C%2520with%25203D%2520spatial%2520reasoning%2520capabilities.%2520Experiments%2520on%2520standard%2520autonomous%2520driving%2520benchmarks%2520for%25202D-to-3D%2520KD%2520demonstrate%2520that%2520CleverDistiller%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%2520semantic%2520segmentation%2520and%25203D%2520object%2520detection%2520%25283DOD%2529%2520by%2520up%2520to%252010%2525%2520mIoU%252C%2520especially%2520when%2520fine%2520tuning%2520on%2520really%2520low%2520data%2520amounts%252C%2520showing%2520the%2520effectiveness%2520of%2520our%2520simple%2520yet%2520powerful%2520KD%2520strategy%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09878v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CleverDistiller%3A%20Simple%20and%20Spatially%20Consistent%20Cross-modal%20Distillation&entry.906535625=Hariprasath%20Govindarajan%20and%20Maciej%20K.%20Wozniak%20and%20Marvin%20Klingner%20and%20Camille%20Maurice%20and%20B%20Ravi%20Kiran%20and%20Senthil%20Yogamani&entry.1292438233=Vision%20foundation%20models%20%28VFMs%29%20such%20as%20DINO%20have%20led%20to%20a%20paradigm%20shift%20in%202D%20camera-based%20perception%20towards%20extracting%20generalized%20features%20to%20support%20many%20downstream%20tasks.%20Recent%20works%20introduce%20self-supervised%20cross-modal%20knowledge%20distillation%20%28KD%29%20as%20a%20way%20to%20transfer%20these%20powerful%20generalization%20capabilities%20into%203D%20LiDAR-based%20models.%20However%2C%20they%20either%20rely%20on%20highly%20complex%20distillation%20losses%2C%20pseudo-semantic%20maps%2C%20or%20limit%20KD%20to%20features%20useful%20for%20semantic%20segmentation%20only.%20In%20this%20work%2C%20we%20propose%20CleverDistiller%2C%20a%20self-supervised%2C%20cross-modal%202D-to-3D%20KD%20framework%20introducing%20a%20set%20of%20simple%20yet%20effective%20design%20choices%3A%20Unlike%20contrastive%20approaches%20relying%20on%20complex%20loss%20design%20choices%2C%20our%20method%20employs%20a%20direct%20feature%20similarity%20loss%20in%20combination%20with%20a%20multi%20layer%20perceptron%20%28MLP%29%20projection%20head%20to%20allow%20the%203D%20network%20to%20learn%20complex%20semantic%20dependencies%20throughout%20the%20projection.%20Crucially%2C%20our%20approach%20does%20not%20depend%20on%20pseudo-semantic%20maps%2C%20allowing%20for%20direct%20knowledge%20transfer%20from%20a%20VFM%20without%20explicit%20semantic%20supervision.%20Additionally%2C%20we%20introduce%20the%20auxiliary%20self-supervised%20spatial%20task%20of%20occupancy%20prediction%20to%20enhance%20the%20semantic%20knowledge%2C%20obtained%20from%20a%20VFM%20through%20KD%2C%20with%203D%20spatial%20reasoning%20capabilities.%20Experiments%20on%20standard%20autonomous%20driving%20benchmarks%20for%202D-to-3D%20KD%20demonstrate%20that%20CleverDistiller%20achieves%20state-of-the-art%20performance%20in%20both%20semantic%20segmentation%20and%203D%20object%20detection%20%283DOD%29%20by%20up%20to%2010%25%20mIoU%2C%20especially%20when%20fine%20tuning%20on%20really%20low%20data%20amounts%2C%20showing%20the%20effectiveness%20of%20our%20simple%20yet%20powerful%20KD%20strategy&entry.1838667208=http%3A//arxiv.org/abs/2503.09878v4&entry.124074799=Read"},
{"title": "Scaling Self-Supervised and Cross-Modal Pretraining for Volumetric CT Transformers", "author": "Cris Claessens and Christiaan Viviers and Giacomo D'Amicantonio and Egor Bondarev and Fons van der Sommen", "abstract": "We introduce SPECTRE, a fully transformer-based foundation model for volumetric computed tomography (CT). Our Self-Supervised & Cross-Modal Pretraining for CT Representation Extraction (SPECTRE) approach utilizes scalable 3D Vision Transformer architectures and modern self-supervised and vision-language pretraining strategies to learn general-purpose CT representations. Volumetric CT poses unique challenges, such as extreme token scaling, geometric anisotropy, and weak or noisy clinical supervision, that make standard transformer and contrastive learning recipes ineffective out of the box. The framework jointly optimizes a local transformer for high-resolution volumetric feature extraction and a global transformer for whole-scan context modeling, making large-scale 3D attention computationally tractable. Notably, SPECTRE is trained exclusively on openly available CT datasets, demonstrating that high-performing, generalizable representations can be achieved without relying on private data. Pretraining combines DINO-style self-distillation with SigLIP-based vision-language alignment using paired radiology reports, yielding features that are both geometrically consistent and clinically meaningful. Across multiple CT benchmarks, SPECTRE consistently outperforms prior CT foundation models in both zero-shot and fine-tuned settings, establishing SPECTRE as a scalable, open, and fully transformer-based foundation model for 3D medical imaging.", "link": "http://arxiv.org/abs/2511.17209v1", "date": "2025-11-21", "relevancy": 2.2913, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5769}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5735}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Self-Supervised%20and%20Cross-Modal%20Pretraining%20for%20Volumetric%20CT%20Transformers&body=Title%3A%20Scaling%20Self-Supervised%20and%20Cross-Modal%20Pretraining%20for%20Volumetric%20CT%20Transformers%0AAuthor%3A%20Cris%20Claessens%20and%20Christiaan%20Viviers%20and%20Giacomo%20D%27Amicantonio%20and%20Egor%20Bondarev%20and%20Fons%20van%20der%20Sommen%0AAbstract%3A%20We%20introduce%20SPECTRE%2C%20a%20fully%20transformer-based%20foundation%20model%20for%20volumetric%20computed%20tomography%20%28CT%29.%20Our%20Self-Supervised%20%26%20Cross-Modal%20Pretraining%20for%20CT%20Representation%20Extraction%20%28SPECTRE%29%20approach%20utilizes%20scalable%203D%20Vision%20Transformer%20architectures%20and%20modern%20self-supervised%20and%20vision-language%20pretraining%20strategies%20to%20learn%20general-purpose%20CT%20representations.%20Volumetric%20CT%20poses%20unique%20challenges%2C%20such%20as%20extreme%20token%20scaling%2C%20geometric%20anisotropy%2C%20and%20weak%20or%20noisy%20clinical%20supervision%2C%20that%20make%20standard%20transformer%20and%20contrastive%20learning%20recipes%20ineffective%20out%20of%20the%20box.%20The%20framework%20jointly%20optimizes%20a%20local%20transformer%20for%20high-resolution%20volumetric%20feature%20extraction%20and%20a%20global%20transformer%20for%20whole-scan%20context%20modeling%2C%20making%20large-scale%203D%20attention%20computationally%20tractable.%20Notably%2C%20SPECTRE%20is%20trained%20exclusively%20on%20openly%20available%20CT%20datasets%2C%20demonstrating%20that%20high-performing%2C%20generalizable%20representations%20can%20be%20achieved%20without%20relying%20on%20private%20data.%20Pretraining%20combines%20DINO-style%20self-distillation%20with%20SigLIP-based%20vision-language%20alignment%20using%20paired%20radiology%20reports%2C%20yielding%20features%20that%20are%20both%20geometrically%20consistent%20and%20clinically%20meaningful.%20Across%20multiple%20CT%20benchmarks%2C%20SPECTRE%20consistently%20outperforms%20prior%20CT%20foundation%20models%20in%20both%20zero-shot%20and%20fine-tuned%20settings%2C%20establishing%20SPECTRE%20as%20a%20scalable%2C%20open%2C%20and%20fully%20transformer-based%20foundation%20model%20for%203D%20medical%20imaging.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Self-Supervised%2520and%2520Cross-Modal%2520Pretraining%2520for%2520Volumetric%2520CT%2520Transformers%26entry.906535625%3DCris%2520Claessens%2520and%2520Christiaan%2520Viviers%2520and%2520Giacomo%2520D%2527Amicantonio%2520and%2520Egor%2520Bondarev%2520and%2520Fons%2520van%2520der%2520Sommen%26entry.1292438233%3DWe%2520introduce%2520SPECTRE%252C%2520a%2520fully%2520transformer-based%2520foundation%2520model%2520for%2520volumetric%2520computed%2520tomography%2520%2528CT%2529.%2520Our%2520Self-Supervised%2520%2526%2520Cross-Modal%2520Pretraining%2520for%2520CT%2520Representation%2520Extraction%2520%2528SPECTRE%2529%2520approach%2520utilizes%2520scalable%25203D%2520Vision%2520Transformer%2520architectures%2520and%2520modern%2520self-supervised%2520and%2520vision-language%2520pretraining%2520strategies%2520to%2520learn%2520general-purpose%2520CT%2520representations.%2520Volumetric%2520CT%2520poses%2520unique%2520challenges%252C%2520such%2520as%2520extreme%2520token%2520scaling%252C%2520geometric%2520anisotropy%252C%2520and%2520weak%2520or%2520noisy%2520clinical%2520supervision%252C%2520that%2520make%2520standard%2520transformer%2520and%2520contrastive%2520learning%2520recipes%2520ineffective%2520out%2520of%2520the%2520box.%2520The%2520framework%2520jointly%2520optimizes%2520a%2520local%2520transformer%2520for%2520high-resolution%2520volumetric%2520feature%2520extraction%2520and%2520a%2520global%2520transformer%2520for%2520whole-scan%2520context%2520modeling%252C%2520making%2520large-scale%25203D%2520attention%2520computationally%2520tractable.%2520Notably%252C%2520SPECTRE%2520is%2520trained%2520exclusively%2520on%2520openly%2520available%2520CT%2520datasets%252C%2520demonstrating%2520that%2520high-performing%252C%2520generalizable%2520representations%2520can%2520be%2520achieved%2520without%2520relying%2520on%2520private%2520data.%2520Pretraining%2520combines%2520DINO-style%2520self-distillation%2520with%2520SigLIP-based%2520vision-language%2520alignment%2520using%2520paired%2520radiology%2520reports%252C%2520yielding%2520features%2520that%2520are%2520both%2520geometrically%2520consistent%2520and%2520clinically%2520meaningful.%2520Across%2520multiple%2520CT%2520benchmarks%252C%2520SPECTRE%2520consistently%2520outperforms%2520prior%2520CT%2520foundation%2520models%2520in%2520both%2520zero-shot%2520and%2520fine-tuned%2520settings%252C%2520establishing%2520SPECTRE%2520as%2520a%2520scalable%252C%2520open%252C%2520and%2520fully%2520transformer-based%2520foundation%2520model%2520for%25203D%2520medical%2520imaging.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Self-Supervised%20and%20Cross-Modal%20Pretraining%20for%20Volumetric%20CT%20Transformers&entry.906535625=Cris%20Claessens%20and%20Christiaan%20Viviers%20and%20Giacomo%20D%27Amicantonio%20and%20Egor%20Bondarev%20and%20Fons%20van%20der%20Sommen&entry.1292438233=We%20introduce%20SPECTRE%2C%20a%20fully%20transformer-based%20foundation%20model%20for%20volumetric%20computed%20tomography%20%28CT%29.%20Our%20Self-Supervised%20%26%20Cross-Modal%20Pretraining%20for%20CT%20Representation%20Extraction%20%28SPECTRE%29%20approach%20utilizes%20scalable%203D%20Vision%20Transformer%20architectures%20and%20modern%20self-supervised%20and%20vision-language%20pretraining%20strategies%20to%20learn%20general-purpose%20CT%20representations.%20Volumetric%20CT%20poses%20unique%20challenges%2C%20such%20as%20extreme%20token%20scaling%2C%20geometric%20anisotropy%2C%20and%20weak%20or%20noisy%20clinical%20supervision%2C%20that%20make%20standard%20transformer%20and%20contrastive%20learning%20recipes%20ineffective%20out%20of%20the%20box.%20The%20framework%20jointly%20optimizes%20a%20local%20transformer%20for%20high-resolution%20volumetric%20feature%20extraction%20and%20a%20global%20transformer%20for%20whole-scan%20context%20modeling%2C%20making%20large-scale%203D%20attention%20computationally%20tractable.%20Notably%2C%20SPECTRE%20is%20trained%20exclusively%20on%20openly%20available%20CT%20datasets%2C%20demonstrating%20that%20high-performing%2C%20generalizable%20representations%20can%20be%20achieved%20without%20relying%20on%20private%20data.%20Pretraining%20combines%20DINO-style%20self-distillation%20with%20SigLIP-based%20vision-language%20alignment%20using%20paired%20radiology%20reports%2C%20yielding%20features%20that%20are%20both%20geometrically%20consistent%20and%20clinically%20meaningful.%20Across%20multiple%20CT%20benchmarks%2C%20SPECTRE%20consistently%20outperforms%20prior%20CT%20foundation%20models%20in%20both%20zero-shot%20and%20fine-tuned%20settings%2C%20establishing%20SPECTRE%20as%20a%20scalable%2C%20open%2C%20and%20fully%20transformer-based%20foundation%20model%20for%203D%20medical%20imaging.&entry.1838667208=http%3A//arxiv.org/abs/2511.17209v1&entry.124074799=Read"},
{"title": "Perception, Control and Hardware for In-Hand Slip-Aware Object Manipulation with Parallel Grippers", "author": "Gabriel Arslan Waltersson and Yiannis Karayiannidis", "abstract": "Dexterous in-hand manipulation offers significant potential to enhance robotic manipulator capabilities. This paper presents a sensori-motor architecture for in-hand slip-aware control, being embodied in a sensorized gripper. The gripper in our architecture features rapid closed-loop, low-level force control, and is equipped with sensors capable of independently measuring contact forces and sliding velocities. Our system can quickly estimate essential object properties during pick-up using only in-hand sensing, without relying on prior object information. We introduce four distinct slippage controllers: gravity-assisted trajectory following for both rotational and linear slippage, a hinge controller that maintains the object's orientation while the gripper rotates, and a slip-avoidance controller. The gripper is mounted on a robot arm and validated through extensive experiments involving a diverse range of objects, demonstrating the architecture's novel capabilities for manipulating objects with flat surfaces.", "link": "http://arxiv.org/abs/2410.19660v2", "date": "2025-11-21", "relevancy": 2.2882, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5734}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5713}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perception%2C%20Control%20and%20Hardware%20for%20In-Hand%20Slip-Aware%20Object%20Manipulation%20with%20Parallel%20Grippers&body=Title%3A%20Perception%2C%20Control%20and%20Hardware%20for%20In-Hand%20Slip-Aware%20Object%20Manipulation%20with%20Parallel%20Grippers%0AAuthor%3A%20Gabriel%20Arslan%20Waltersson%20and%20Yiannis%20Karayiannidis%0AAbstract%3A%20Dexterous%20in-hand%20manipulation%20offers%20significant%20potential%20to%20enhance%20robotic%20manipulator%20capabilities.%20This%20paper%20presents%20a%20sensori-motor%20architecture%20for%20in-hand%20slip-aware%20control%2C%20being%20embodied%20in%20a%20sensorized%20gripper.%20The%20gripper%20in%20our%20architecture%20features%20rapid%20closed-loop%2C%20low-level%20force%20control%2C%20and%20is%20equipped%20with%20sensors%20capable%20of%20independently%20measuring%20contact%20forces%20and%20sliding%20velocities.%20Our%20system%20can%20quickly%20estimate%20essential%20object%20properties%20during%20pick-up%20using%20only%20in-hand%20sensing%2C%20without%20relying%20on%20prior%20object%20information.%20We%20introduce%20four%20distinct%20slippage%20controllers%3A%20gravity-assisted%20trajectory%20following%20for%20both%20rotational%20and%20linear%20slippage%2C%20a%20hinge%20controller%20that%20maintains%20the%20object%27s%20orientation%20while%20the%20gripper%20rotates%2C%20and%20a%20slip-avoidance%20controller.%20The%20gripper%20is%20mounted%20on%20a%20robot%20arm%20and%20validated%20through%20extensive%20experiments%20involving%20a%20diverse%20range%20of%20objects%2C%20demonstrating%20the%20architecture%27s%20novel%20capabilities%20for%20manipulating%20objects%20with%20flat%20surfaces.%0ALink%3A%20http%3A//arxiv.org/abs/2410.19660v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerception%252C%2520Control%2520and%2520Hardware%2520for%2520In-Hand%2520Slip-Aware%2520Object%2520Manipulation%2520with%2520Parallel%2520Grippers%26entry.906535625%3DGabriel%2520Arslan%2520Waltersson%2520and%2520Yiannis%2520Karayiannidis%26entry.1292438233%3DDexterous%2520in-hand%2520manipulation%2520offers%2520significant%2520potential%2520to%2520enhance%2520robotic%2520manipulator%2520capabilities.%2520This%2520paper%2520presents%2520a%2520sensori-motor%2520architecture%2520for%2520in-hand%2520slip-aware%2520control%252C%2520being%2520embodied%2520in%2520a%2520sensorized%2520gripper.%2520The%2520gripper%2520in%2520our%2520architecture%2520features%2520rapid%2520closed-loop%252C%2520low-level%2520force%2520control%252C%2520and%2520is%2520equipped%2520with%2520sensors%2520capable%2520of%2520independently%2520measuring%2520contact%2520forces%2520and%2520sliding%2520velocities.%2520Our%2520system%2520can%2520quickly%2520estimate%2520essential%2520object%2520properties%2520during%2520pick-up%2520using%2520only%2520in-hand%2520sensing%252C%2520without%2520relying%2520on%2520prior%2520object%2520information.%2520We%2520introduce%2520four%2520distinct%2520slippage%2520controllers%253A%2520gravity-assisted%2520trajectory%2520following%2520for%2520both%2520rotational%2520and%2520linear%2520slippage%252C%2520a%2520hinge%2520controller%2520that%2520maintains%2520the%2520object%2527s%2520orientation%2520while%2520the%2520gripper%2520rotates%252C%2520and%2520a%2520slip-avoidance%2520controller.%2520The%2520gripper%2520is%2520mounted%2520on%2520a%2520robot%2520arm%2520and%2520validated%2520through%2520extensive%2520experiments%2520involving%2520a%2520diverse%2520range%2520of%2520objects%252C%2520demonstrating%2520the%2520architecture%2527s%2520novel%2520capabilities%2520for%2520manipulating%2520objects%2520with%2520flat%2520surfaces.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19660v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perception%2C%20Control%20and%20Hardware%20for%20In-Hand%20Slip-Aware%20Object%20Manipulation%20with%20Parallel%20Grippers&entry.906535625=Gabriel%20Arslan%20Waltersson%20and%20Yiannis%20Karayiannidis&entry.1292438233=Dexterous%20in-hand%20manipulation%20offers%20significant%20potential%20to%20enhance%20robotic%20manipulator%20capabilities.%20This%20paper%20presents%20a%20sensori-motor%20architecture%20for%20in-hand%20slip-aware%20control%2C%20being%20embodied%20in%20a%20sensorized%20gripper.%20The%20gripper%20in%20our%20architecture%20features%20rapid%20closed-loop%2C%20low-level%20force%20control%2C%20and%20is%20equipped%20with%20sensors%20capable%20of%20independently%20measuring%20contact%20forces%20and%20sliding%20velocities.%20Our%20system%20can%20quickly%20estimate%20essential%20object%20properties%20during%20pick-up%20using%20only%20in-hand%20sensing%2C%20without%20relying%20on%20prior%20object%20information.%20We%20introduce%20four%20distinct%20slippage%20controllers%3A%20gravity-assisted%20trajectory%20following%20for%20both%20rotational%20and%20linear%20slippage%2C%20a%20hinge%20controller%20that%20maintains%20the%20object%27s%20orientation%20while%20the%20gripper%20rotates%2C%20and%20a%20slip-avoidance%20controller.%20The%20gripper%20is%20mounted%20on%20a%20robot%20arm%20and%20validated%20through%20extensive%20experiments%20involving%20a%20diverse%20range%20of%20objects%2C%20demonstrating%20the%20architecture%27s%20novel%20capabilities%20for%20manipulating%20objects%20with%20flat%20surfaces.&entry.1838667208=http%3A//arxiv.org/abs/2410.19660v2&entry.124074799=Read"},
{"title": "OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model", "author": "Xingcheng Zhou and Xuyuan Han and Feng Yang and Yunpu Ma and Volker Tresp and Alois Knoll", "abstract": "We present OpenDriveVLA, a Vision Language Action model designed for end-to-end autonomous driving, built upon open-source large language models. OpenDriveVLA generates spatially grounded driving actions by leveraging multimodal inputs, including 2D and 3D instance-aware visual representations, ego vehicle states, and language commands. To bridge the modality gap between driving visual representations and language embeddings, we introduce a hierarchical vision language alignment process, projecting both 2D and 3D structured visual tokens into a unified semantic space. Furthermore, we incorporate structured agent environment ego interaction modeling into the autoregressive decoding process, enabling the model to capture fine-grained spatial dependencies and behavior-aware dynamics critical for reliable trajectory planning. Extensive experiments on the nuScenes dataset demonstrate that OpenDriveVLA achieves state-of-the-art results across open-loop trajectory planning and driving-related question answering tasks. Qualitative analyses further illustrate its capability to follow high-level driving commands and generate trajectories under challenging scenarios, highlighting its potential for next-generation end-to-end autonomous driving.", "link": "http://arxiv.org/abs/2503.23463v2", "date": "2025-11-21", "relevancy": 2.2858, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5768}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5768}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenDriveVLA%3A%20Towards%20End-to-end%20Autonomous%20Driving%20with%20Large%20Vision%20Language%20Action%20Model&body=Title%3A%20OpenDriveVLA%3A%20Towards%20End-to-end%20Autonomous%20Driving%20with%20Large%20Vision%20Language%20Action%20Model%0AAuthor%3A%20Xingcheng%20Zhou%20and%20Xuyuan%20Han%20and%20Feng%20Yang%20and%20Yunpu%20Ma%20and%20Volker%20Tresp%20and%20Alois%20Knoll%0AAbstract%3A%20We%20present%20OpenDriveVLA%2C%20a%20Vision%20Language%20Action%20model%20designed%20for%20end-to-end%20autonomous%20driving%2C%20built%20upon%20open-source%20large%20language%20models.%20OpenDriveVLA%20generates%20spatially%20grounded%20driving%20actions%20by%20leveraging%20multimodal%20inputs%2C%20including%202D%20and%203D%20instance-aware%20visual%20representations%2C%20ego%20vehicle%20states%2C%20and%20language%20commands.%20To%20bridge%20the%20modality%20gap%20between%20driving%20visual%20representations%20and%20language%20embeddings%2C%20we%20introduce%20a%20hierarchical%20vision%20language%20alignment%20process%2C%20projecting%20both%202D%20and%203D%20structured%20visual%20tokens%20into%20a%20unified%20semantic%20space.%20Furthermore%2C%20we%20incorporate%20structured%20agent%20environment%20ego%20interaction%20modeling%20into%20the%20autoregressive%20decoding%20process%2C%20enabling%20the%20model%20to%20capture%20fine-grained%20spatial%20dependencies%20and%20behavior-aware%20dynamics%20critical%20for%20reliable%20trajectory%20planning.%20Extensive%20experiments%20on%20the%20nuScenes%20dataset%20demonstrate%20that%20OpenDriveVLA%20achieves%20state-of-the-art%20results%20across%20open-loop%20trajectory%20planning%20and%20driving-related%20question%20answering%20tasks.%20Qualitative%20analyses%20further%20illustrate%20its%20capability%20to%20follow%20high-level%20driving%20commands%20and%20generate%20trajectories%20under%20challenging%20scenarios%2C%20highlighting%20its%20potential%20for%20next-generation%20end-to-end%20autonomous%20driving.%0ALink%3A%20http%3A//arxiv.org/abs/2503.23463v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenDriveVLA%253A%2520Towards%2520End-to-end%2520Autonomous%2520Driving%2520with%2520Large%2520Vision%2520Language%2520Action%2520Model%26entry.906535625%3DXingcheng%2520Zhou%2520and%2520Xuyuan%2520Han%2520and%2520Feng%2520Yang%2520and%2520Yunpu%2520Ma%2520and%2520Volker%2520Tresp%2520and%2520Alois%2520Knoll%26entry.1292438233%3DWe%2520present%2520OpenDriveVLA%252C%2520a%2520Vision%2520Language%2520Action%2520model%2520designed%2520for%2520end-to-end%2520autonomous%2520driving%252C%2520built%2520upon%2520open-source%2520large%2520language%2520models.%2520OpenDriveVLA%2520generates%2520spatially%2520grounded%2520driving%2520actions%2520by%2520leveraging%2520multimodal%2520inputs%252C%2520including%25202D%2520and%25203D%2520instance-aware%2520visual%2520representations%252C%2520ego%2520vehicle%2520states%252C%2520and%2520language%2520commands.%2520To%2520bridge%2520the%2520modality%2520gap%2520between%2520driving%2520visual%2520representations%2520and%2520language%2520embeddings%252C%2520we%2520introduce%2520a%2520hierarchical%2520vision%2520language%2520alignment%2520process%252C%2520projecting%2520both%25202D%2520and%25203D%2520structured%2520visual%2520tokens%2520into%2520a%2520unified%2520semantic%2520space.%2520Furthermore%252C%2520we%2520incorporate%2520structured%2520agent%2520environment%2520ego%2520interaction%2520modeling%2520into%2520the%2520autoregressive%2520decoding%2520process%252C%2520enabling%2520the%2520model%2520to%2520capture%2520fine-grained%2520spatial%2520dependencies%2520and%2520behavior-aware%2520dynamics%2520critical%2520for%2520reliable%2520trajectory%2520planning.%2520Extensive%2520experiments%2520on%2520the%2520nuScenes%2520dataset%2520demonstrate%2520that%2520OpenDriveVLA%2520achieves%2520state-of-the-art%2520results%2520across%2520open-loop%2520trajectory%2520planning%2520and%2520driving-related%2520question%2520answering%2520tasks.%2520Qualitative%2520analyses%2520further%2520illustrate%2520its%2520capability%2520to%2520follow%2520high-level%2520driving%2520commands%2520and%2520generate%2520trajectories%2520under%2520challenging%2520scenarios%252C%2520highlighting%2520its%2520potential%2520for%2520next-generation%2520end-to-end%2520autonomous%2520driving.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.23463v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenDriveVLA%3A%20Towards%20End-to-end%20Autonomous%20Driving%20with%20Large%20Vision%20Language%20Action%20Model&entry.906535625=Xingcheng%20Zhou%20and%20Xuyuan%20Han%20and%20Feng%20Yang%20and%20Yunpu%20Ma%20and%20Volker%20Tresp%20and%20Alois%20Knoll&entry.1292438233=We%20present%20OpenDriveVLA%2C%20a%20Vision%20Language%20Action%20model%20designed%20for%20end-to-end%20autonomous%20driving%2C%20built%20upon%20open-source%20large%20language%20models.%20OpenDriveVLA%20generates%20spatially%20grounded%20driving%20actions%20by%20leveraging%20multimodal%20inputs%2C%20including%202D%20and%203D%20instance-aware%20visual%20representations%2C%20ego%20vehicle%20states%2C%20and%20language%20commands.%20To%20bridge%20the%20modality%20gap%20between%20driving%20visual%20representations%20and%20language%20embeddings%2C%20we%20introduce%20a%20hierarchical%20vision%20language%20alignment%20process%2C%20projecting%20both%202D%20and%203D%20structured%20visual%20tokens%20into%20a%20unified%20semantic%20space.%20Furthermore%2C%20we%20incorporate%20structured%20agent%20environment%20ego%20interaction%20modeling%20into%20the%20autoregressive%20decoding%20process%2C%20enabling%20the%20model%20to%20capture%20fine-grained%20spatial%20dependencies%20and%20behavior-aware%20dynamics%20critical%20for%20reliable%20trajectory%20planning.%20Extensive%20experiments%20on%20the%20nuScenes%20dataset%20demonstrate%20that%20OpenDriveVLA%20achieves%20state-of-the-art%20results%20across%20open-loop%20trajectory%20planning%20and%20driving-related%20question%20answering%20tasks.%20Qualitative%20analyses%20further%20illustrate%20its%20capability%20to%20follow%20high-level%20driving%20commands%20and%20generate%20trajectories%20under%20challenging%20scenarios%2C%20highlighting%20its%20potential%20for%20next-generation%20end-to-end%20autonomous%20driving.&entry.1838667208=http%3A//arxiv.org/abs/2503.23463v2&entry.124074799=Read"},
{"title": "Disentangled Concepts Speak Louder Than Words: Explainable Video Action Recognition", "author": "Jongseo Lee and Wooil Lee and Gyeong-Moon Park and Seong Tae Kim and Jinwoo Choi", "abstract": "Effective explanations of video action recognition models should disentangle how movements unfold over time from the surrounding spatial context. However, existing methods based on saliency produce entangled explanations, making it unclear whether predictions rely on motion or spatial context. Language-based approaches offer structure but often fail to explain motions due to their tacit nature -- intuitively understood but difficult to verbalize. To address these challenges, we propose Disentangled Action aNd Context concept-based Explainable (DANCE) video action recognition, a framework that predicts actions through disentangled concept types: motion dynamics, objects, and scenes. We define motion dynamics concepts as human pose sequences. We employ a large language model to automatically extract object and scene concepts. Built on an ante-hoc concept bottleneck design, DANCE enforces prediction through these concepts. Experiments on four datasets -- KTH, Penn Action, HAA500, and UCF-101 -- demonstrate that DANCE significantly improves explanation clarity with competitive performance. We validate the superior interpretability of DANCE through a user study. Experimental results also show that DANCE is beneficial for model debugging, editing, and failure analysis.", "link": "http://arxiv.org/abs/2511.03725v2", "date": "2025-11-21", "relevancy": 2.2805, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5728}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5728}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangled%20Concepts%20Speak%20Louder%20Than%20Words%3A%20Explainable%20Video%20Action%20Recognition&body=Title%3A%20Disentangled%20Concepts%20Speak%20Louder%20Than%20Words%3A%20Explainable%20Video%20Action%20Recognition%0AAuthor%3A%20Jongseo%20Lee%20and%20Wooil%20Lee%20and%20Gyeong-Moon%20Park%20and%20Seong%20Tae%20Kim%20and%20Jinwoo%20Choi%0AAbstract%3A%20Effective%20explanations%20of%20video%20action%20recognition%20models%20should%20disentangle%20how%20movements%20unfold%20over%20time%20from%20the%20surrounding%20spatial%20context.%20However%2C%20existing%20methods%20based%20on%20saliency%20produce%20entangled%20explanations%2C%20making%20it%20unclear%20whether%20predictions%20rely%20on%20motion%20or%20spatial%20context.%20Language-based%20approaches%20offer%20structure%20but%20often%20fail%20to%20explain%20motions%20due%20to%20their%20tacit%20nature%20--%20intuitively%20understood%20but%20difficult%20to%20verbalize.%20To%20address%20these%20challenges%2C%20we%20propose%20Disentangled%20Action%20aNd%20Context%20concept-based%20Explainable%20%28DANCE%29%20video%20action%20recognition%2C%20a%20framework%20that%20predicts%20actions%20through%20disentangled%20concept%20types%3A%20motion%20dynamics%2C%20objects%2C%20and%20scenes.%20We%20define%20motion%20dynamics%20concepts%20as%20human%20pose%20sequences.%20We%20employ%20a%20large%20language%20model%20to%20automatically%20extract%20object%20and%20scene%20concepts.%20Built%20on%20an%20ante-hoc%20concept%20bottleneck%20design%2C%20DANCE%20enforces%20prediction%20through%20these%20concepts.%20Experiments%20on%20four%20datasets%20--%20KTH%2C%20Penn%20Action%2C%20HAA500%2C%20and%20UCF-101%20--%20demonstrate%20that%20DANCE%20significantly%20improves%20explanation%20clarity%20with%20competitive%20performance.%20We%20validate%20the%20superior%20interpretability%20of%20DANCE%20through%20a%20user%20study.%20Experimental%20results%20also%20show%20that%20DANCE%20is%20beneficial%20for%20model%20debugging%2C%20editing%2C%20and%20failure%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2511.03725v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangled%2520Concepts%2520Speak%2520Louder%2520Than%2520Words%253A%2520Explainable%2520Video%2520Action%2520Recognition%26entry.906535625%3DJongseo%2520Lee%2520and%2520Wooil%2520Lee%2520and%2520Gyeong-Moon%2520Park%2520and%2520Seong%2520Tae%2520Kim%2520and%2520Jinwoo%2520Choi%26entry.1292438233%3DEffective%2520explanations%2520of%2520video%2520action%2520recognition%2520models%2520should%2520disentangle%2520how%2520movements%2520unfold%2520over%2520time%2520from%2520the%2520surrounding%2520spatial%2520context.%2520However%252C%2520existing%2520methods%2520based%2520on%2520saliency%2520produce%2520entangled%2520explanations%252C%2520making%2520it%2520unclear%2520whether%2520predictions%2520rely%2520on%2520motion%2520or%2520spatial%2520context.%2520Language-based%2520approaches%2520offer%2520structure%2520but%2520often%2520fail%2520to%2520explain%2520motions%2520due%2520to%2520their%2520tacit%2520nature%2520--%2520intuitively%2520understood%2520but%2520difficult%2520to%2520verbalize.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Disentangled%2520Action%2520aNd%2520Context%2520concept-based%2520Explainable%2520%2528DANCE%2529%2520video%2520action%2520recognition%252C%2520a%2520framework%2520that%2520predicts%2520actions%2520through%2520disentangled%2520concept%2520types%253A%2520motion%2520dynamics%252C%2520objects%252C%2520and%2520scenes.%2520We%2520define%2520motion%2520dynamics%2520concepts%2520as%2520human%2520pose%2520sequences.%2520We%2520employ%2520a%2520large%2520language%2520model%2520to%2520automatically%2520extract%2520object%2520and%2520scene%2520concepts.%2520Built%2520on%2520an%2520ante-hoc%2520concept%2520bottleneck%2520design%252C%2520DANCE%2520enforces%2520prediction%2520through%2520these%2520concepts.%2520Experiments%2520on%2520four%2520datasets%2520--%2520KTH%252C%2520Penn%2520Action%252C%2520HAA500%252C%2520and%2520UCF-101%2520--%2520demonstrate%2520that%2520DANCE%2520significantly%2520improves%2520explanation%2520clarity%2520with%2520competitive%2520performance.%2520We%2520validate%2520the%2520superior%2520interpretability%2520of%2520DANCE%2520through%2520a%2520user%2520study.%2520Experimental%2520results%2520also%2520show%2520that%2520DANCE%2520is%2520beneficial%2520for%2520model%2520debugging%252C%2520editing%252C%2520and%2520failure%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.03725v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20Concepts%20Speak%20Louder%20Than%20Words%3A%20Explainable%20Video%20Action%20Recognition&entry.906535625=Jongseo%20Lee%20and%20Wooil%20Lee%20and%20Gyeong-Moon%20Park%20and%20Seong%20Tae%20Kim%20and%20Jinwoo%20Choi&entry.1292438233=Effective%20explanations%20of%20video%20action%20recognition%20models%20should%20disentangle%20how%20movements%20unfold%20over%20time%20from%20the%20surrounding%20spatial%20context.%20However%2C%20existing%20methods%20based%20on%20saliency%20produce%20entangled%20explanations%2C%20making%20it%20unclear%20whether%20predictions%20rely%20on%20motion%20or%20spatial%20context.%20Language-based%20approaches%20offer%20structure%20but%20often%20fail%20to%20explain%20motions%20due%20to%20their%20tacit%20nature%20--%20intuitively%20understood%20but%20difficult%20to%20verbalize.%20To%20address%20these%20challenges%2C%20we%20propose%20Disentangled%20Action%20aNd%20Context%20concept-based%20Explainable%20%28DANCE%29%20video%20action%20recognition%2C%20a%20framework%20that%20predicts%20actions%20through%20disentangled%20concept%20types%3A%20motion%20dynamics%2C%20objects%2C%20and%20scenes.%20We%20define%20motion%20dynamics%20concepts%20as%20human%20pose%20sequences.%20We%20employ%20a%20large%20language%20model%20to%20automatically%20extract%20object%20and%20scene%20concepts.%20Built%20on%20an%20ante-hoc%20concept%20bottleneck%20design%2C%20DANCE%20enforces%20prediction%20through%20these%20concepts.%20Experiments%20on%20four%20datasets%20--%20KTH%2C%20Penn%20Action%2C%20HAA500%2C%20and%20UCF-101%20--%20demonstrate%20that%20DANCE%20significantly%20improves%20explanation%20clarity%20with%20competitive%20performance.%20We%20validate%20the%20superior%20interpretability%20of%20DANCE%20through%20a%20user%20study.%20Experimental%20results%20also%20show%20that%20DANCE%20is%20beneficial%20for%20model%20debugging%2C%20editing%2C%20and%20failure%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2511.03725v2&entry.124074799=Read"},
{"title": "TP-MDDN: Task-Preferenced Multi-Demand-Driven Navigation with Autonomous Decision-Making", "author": "Shanshan Li and Da Huang and Yu He and Yanwei Fu and Yu-Gang Jiang and Xiangyang Xue", "abstract": "In daily life, people often move through spaces to find objects that meet their needs, posing a key challenge in embodied AI. Traditional Demand-Driven Navigation (DDN) handles one need at a time but does not reflect the complexity of real-world tasks involving multiple needs and personal choices. To bridge this gap, we introduce Task-Preferenced Multi-Demand-Driven Navigation (TP-MDDN), a new benchmark for long-horizon navigation involving multiple sub-demands with explicit task preferences. To solve TP-MDDN, we propose AWMSystem, an autonomous decision-making system composed of three key modules: BreakLLM (instruction decomposition), LocateLLM (goal selection), and StatusMLLM (task monitoring). For spatial memory, we design MASMap, which combines 3D point cloud accumulation with 2D semantic mapping for accurate and efficient environmental understanding. Our Dual-Tempo action generation framework integrates zero-shot planning with policy-based fine control, and is further supported by an Adaptive Error Corrector that handles failure cases in real time. Experiments demonstrate that our approach outperforms state-of-the-art baselines in both perception accuracy and navigation robustness.", "link": "http://arxiv.org/abs/2511.17225v1", "date": "2025-11-21", "relevancy": 2.2732, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5923}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.555}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TP-MDDN%3A%20Task-Preferenced%20Multi-Demand-Driven%20Navigation%20with%20Autonomous%20Decision-Making&body=Title%3A%20TP-MDDN%3A%20Task-Preferenced%20Multi-Demand-Driven%20Navigation%20with%20Autonomous%20Decision-Making%0AAuthor%3A%20Shanshan%20Li%20and%20Da%20Huang%20and%20Yu%20He%20and%20Yanwei%20Fu%20and%20Yu-Gang%20Jiang%20and%20Xiangyang%20Xue%0AAbstract%3A%20In%20daily%20life%2C%20people%20often%20move%20through%20spaces%20to%20find%20objects%20that%20meet%20their%20needs%2C%20posing%20a%20key%20challenge%20in%20embodied%20AI.%20Traditional%20Demand-Driven%20Navigation%20%28DDN%29%20handles%20one%20need%20at%20a%20time%20but%20does%20not%20reflect%20the%20complexity%20of%20real-world%20tasks%20involving%20multiple%20needs%20and%20personal%20choices.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Task-Preferenced%20Multi-Demand-Driven%20Navigation%20%28TP-MDDN%29%2C%20a%20new%20benchmark%20for%20long-horizon%20navigation%20involving%20multiple%20sub-demands%20with%20explicit%20task%20preferences.%20To%20solve%20TP-MDDN%2C%20we%20propose%20AWMSystem%2C%20an%20autonomous%20decision-making%20system%20composed%20of%20three%20key%20modules%3A%20BreakLLM%20%28instruction%20decomposition%29%2C%20LocateLLM%20%28goal%20selection%29%2C%20and%20StatusMLLM%20%28task%20monitoring%29.%20For%20spatial%20memory%2C%20we%20design%20MASMap%2C%20which%20combines%203D%20point%20cloud%20accumulation%20with%202D%20semantic%20mapping%20for%20accurate%20and%20efficient%20environmental%20understanding.%20Our%20Dual-Tempo%20action%20generation%20framework%20integrates%20zero-shot%20planning%20with%20policy-based%20fine%20control%2C%20and%20is%20further%20supported%20by%20an%20Adaptive%20Error%20Corrector%20that%20handles%20failure%20cases%20in%20real%20time.%20Experiments%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%20baselines%20in%20both%20perception%20accuracy%20and%20navigation%20robustness.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTP-MDDN%253A%2520Task-Preferenced%2520Multi-Demand-Driven%2520Navigation%2520with%2520Autonomous%2520Decision-Making%26entry.906535625%3DShanshan%2520Li%2520and%2520Da%2520Huang%2520and%2520Yu%2520He%2520and%2520Yanwei%2520Fu%2520and%2520Yu-Gang%2520Jiang%2520and%2520Xiangyang%2520Xue%26entry.1292438233%3DIn%2520daily%2520life%252C%2520people%2520often%2520move%2520through%2520spaces%2520to%2520find%2520objects%2520that%2520meet%2520their%2520needs%252C%2520posing%2520a%2520key%2520challenge%2520in%2520embodied%2520AI.%2520Traditional%2520Demand-Driven%2520Navigation%2520%2528DDN%2529%2520handles%2520one%2520need%2520at%2520a%2520time%2520but%2520does%2520not%2520reflect%2520the%2520complexity%2520of%2520real-world%2520tasks%2520involving%2520multiple%2520needs%2520and%2520personal%2520choices.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520Task-Preferenced%2520Multi-Demand-Driven%2520Navigation%2520%2528TP-MDDN%2529%252C%2520a%2520new%2520benchmark%2520for%2520long-horizon%2520navigation%2520involving%2520multiple%2520sub-demands%2520with%2520explicit%2520task%2520preferences.%2520To%2520solve%2520TP-MDDN%252C%2520we%2520propose%2520AWMSystem%252C%2520an%2520autonomous%2520decision-making%2520system%2520composed%2520of%2520three%2520key%2520modules%253A%2520BreakLLM%2520%2528instruction%2520decomposition%2529%252C%2520LocateLLM%2520%2528goal%2520selection%2529%252C%2520and%2520StatusMLLM%2520%2528task%2520monitoring%2529.%2520For%2520spatial%2520memory%252C%2520we%2520design%2520MASMap%252C%2520which%2520combines%25203D%2520point%2520cloud%2520accumulation%2520with%25202D%2520semantic%2520mapping%2520for%2520accurate%2520and%2520efficient%2520environmental%2520understanding.%2520Our%2520Dual-Tempo%2520action%2520generation%2520framework%2520integrates%2520zero-shot%2520planning%2520with%2520policy-based%2520fine%2520control%252C%2520and%2520is%2520further%2520supported%2520by%2520an%2520Adaptive%2520Error%2520Corrector%2520that%2520handles%2520failure%2520cases%2520in%2520real%2520time.%2520Experiments%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520state-of-the-art%2520baselines%2520in%2520both%2520perception%2520accuracy%2520and%2520navigation%2520robustness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TP-MDDN%3A%20Task-Preferenced%20Multi-Demand-Driven%20Navigation%20with%20Autonomous%20Decision-Making&entry.906535625=Shanshan%20Li%20and%20Da%20Huang%20and%20Yu%20He%20and%20Yanwei%20Fu%20and%20Yu-Gang%20Jiang%20and%20Xiangyang%20Xue&entry.1292438233=In%20daily%20life%2C%20people%20often%20move%20through%20spaces%20to%20find%20objects%20that%20meet%20their%20needs%2C%20posing%20a%20key%20challenge%20in%20embodied%20AI.%20Traditional%20Demand-Driven%20Navigation%20%28DDN%29%20handles%20one%20need%20at%20a%20time%20but%20does%20not%20reflect%20the%20complexity%20of%20real-world%20tasks%20involving%20multiple%20needs%20and%20personal%20choices.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Task-Preferenced%20Multi-Demand-Driven%20Navigation%20%28TP-MDDN%29%2C%20a%20new%20benchmark%20for%20long-horizon%20navigation%20involving%20multiple%20sub-demands%20with%20explicit%20task%20preferences.%20To%20solve%20TP-MDDN%2C%20we%20propose%20AWMSystem%2C%20an%20autonomous%20decision-making%20system%20composed%20of%20three%20key%20modules%3A%20BreakLLM%20%28instruction%20decomposition%29%2C%20LocateLLM%20%28goal%20selection%29%2C%20and%20StatusMLLM%20%28task%20monitoring%29.%20For%20spatial%20memory%2C%20we%20design%20MASMap%2C%20which%20combines%203D%20point%20cloud%20accumulation%20with%202D%20semantic%20mapping%20for%20accurate%20and%20efficient%20environmental%20understanding.%20Our%20Dual-Tempo%20action%20generation%20framework%20integrates%20zero-shot%20planning%20with%20policy-based%20fine%20control%2C%20and%20is%20further%20supported%20by%20an%20Adaptive%20Error%20Corrector%20that%20handles%20failure%20cases%20in%20real%20time.%20Experiments%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%20baselines%20in%20both%20perception%20accuracy%20and%20navigation%20robustness.&entry.1838667208=http%3A//arxiv.org/abs/2511.17225v1&entry.124074799=Read"},
{"title": "SweeperBot: Making 3D Browsing Accessible through View Analysis and Visual Question Answering", "author": "Chen Chen and Cuong Nguyen and Alexa Siu and Dingzeyu Li and Nadir Weibel", "abstract": "Accessing 3D models remains challenging for Screen Reader (SR) users. While some existing 3D viewers allow creators to provide alternative text, they often lack sufficient detail about the 3D models. Grounded on a formative study, this paper introduces SweeperBot, a system that enables SR users to leverage visual question answering to explore and compare 3D models. SweeperBot answers SR users' visual questions by combining an optimal view selection technique with the strength of generative- and recognition-based foundation models. An expert review with 10 Blind and Low-Vision (BLV) users with SR experience demonstrated the feasibility of using SweeperBot to assist BLV users in exploring and comparing 3D models. The quality of the descriptions generated by SweeperBot was validated by a second survey study with 30 sighted participants.", "link": "http://arxiv.org/abs/2511.14567v3", "date": "2025-11-21", "relevancy": 2.2668, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SweeperBot%3A%20Making%203D%20Browsing%20Accessible%20through%20View%20Analysis%20and%20Visual%20Question%20Answering&body=Title%3A%20SweeperBot%3A%20Making%203D%20Browsing%20Accessible%20through%20View%20Analysis%20and%20Visual%20Question%20Answering%0AAuthor%3A%20Chen%20Chen%20and%20Cuong%20Nguyen%20and%20Alexa%20Siu%20and%20Dingzeyu%20Li%20and%20Nadir%20Weibel%0AAbstract%3A%20Accessing%203D%20models%20remains%20challenging%20for%20Screen%20Reader%20%28SR%29%20users.%20While%20some%20existing%203D%20viewers%20allow%20creators%20to%20provide%20alternative%20text%2C%20they%20often%20lack%20sufficient%20detail%20about%20the%203D%20models.%20Grounded%20on%20a%20formative%20study%2C%20this%20paper%20introduces%20SweeperBot%2C%20a%20system%20that%20enables%20SR%20users%20to%20leverage%20visual%20question%20answering%20to%20explore%20and%20compare%203D%20models.%20SweeperBot%20answers%20SR%20users%27%20visual%20questions%20by%20combining%20an%20optimal%20view%20selection%20technique%20with%20the%20strength%20of%20generative-%20and%20recognition-based%20foundation%20models.%20An%20expert%20review%20with%2010%20Blind%20and%20Low-Vision%20%28BLV%29%20users%20with%20SR%20experience%20demonstrated%20the%20feasibility%20of%20using%20SweeperBot%20to%20assist%20BLV%20users%20in%20exploring%20and%20comparing%203D%20models.%20The%20quality%20of%20the%20descriptions%20generated%20by%20SweeperBot%20was%20validated%20by%20a%20second%20survey%20study%20with%2030%20sighted%20participants.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14567v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSweeperBot%253A%2520Making%25203D%2520Browsing%2520Accessible%2520through%2520View%2520Analysis%2520and%2520Visual%2520Question%2520Answering%26entry.906535625%3DChen%2520Chen%2520and%2520Cuong%2520Nguyen%2520and%2520Alexa%2520Siu%2520and%2520Dingzeyu%2520Li%2520and%2520Nadir%2520Weibel%26entry.1292438233%3DAccessing%25203D%2520models%2520remains%2520challenging%2520for%2520Screen%2520Reader%2520%2528SR%2529%2520users.%2520While%2520some%2520existing%25203D%2520viewers%2520allow%2520creators%2520to%2520provide%2520alternative%2520text%252C%2520they%2520often%2520lack%2520sufficient%2520detail%2520about%2520the%25203D%2520models.%2520Grounded%2520on%2520a%2520formative%2520study%252C%2520this%2520paper%2520introduces%2520SweeperBot%252C%2520a%2520system%2520that%2520enables%2520SR%2520users%2520to%2520leverage%2520visual%2520question%2520answering%2520to%2520explore%2520and%2520compare%25203D%2520models.%2520SweeperBot%2520answers%2520SR%2520users%2527%2520visual%2520questions%2520by%2520combining%2520an%2520optimal%2520view%2520selection%2520technique%2520with%2520the%2520strength%2520of%2520generative-%2520and%2520recognition-based%2520foundation%2520models.%2520An%2520expert%2520review%2520with%252010%2520Blind%2520and%2520Low-Vision%2520%2528BLV%2529%2520users%2520with%2520SR%2520experience%2520demonstrated%2520the%2520feasibility%2520of%2520using%2520SweeperBot%2520to%2520assist%2520BLV%2520users%2520in%2520exploring%2520and%2520comparing%25203D%2520models.%2520The%2520quality%2520of%2520the%2520descriptions%2520generated%2520by%2520SweeperBot%2520was%2520validated%2520by%2520a%2520second%2520survey%2520study%2520with%252030%2520sighted%2520participants.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14567v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SweeperBot%3A%20Making%203D%20Browsing%20Accessible%20through%20View%20Analysis%20and%20Visual%20Question%20Answering&entry.906535625=Chen%20Chen%20and%20Cuong%20Nguyen%20and%20Alexa%20Siu%20and%20Dingzeyu%20Li%20and%20Nadir%20Weibel&entry.1292438233=Accessing%203D%20models%20remains%20challenging%20for%20Screen%20Reader%20%28SR%29%20users.%20While%20some%20existing%203D%20viewers%20allow%20creators%20to%20provide%20alternative%20text%2C%20they%20often%20lack%20sufficient%20detail%20about%20the%203D%20models.%20Grounded%20on%20a%20formative%20study%2C%20this%20paper%20introduces%20SweeperBot%2C%20a%20system%20that%20enables%20SR%20users%20to%20leverage%20visual%20question%20answering%20to%20explore%20and%20compare%203D%20models.%20SweeperBot%20answers%20SR%20users%27%20visual%20questions%20by%20combining%20an%20optimal%20view%20selection%20technique%20with%20the%20strength%20of%20generative-%20and%20recognition-based%20foundation%20models.%20An%20expert%20review%20with%2010%20Blind%20and%20Low-Vision%20%28BLV%29%20users%20with%20SR%20experience%20demonstrated%20the%20feasibility%20of%20using%20SweeperBot%20to%20assist%20BLV%20users%20in%20exploring%20and%20comparing%203D%20models.%20The%20quality%20of%20the%20descriptions%20generated%20by%20SweeperBot%20was%20validated%20by%20a%20second%20survey%20study%20with%2030%20sighted%20participants.&entry.1838667208=http%3A//arxiv.org/abs/2511.14567v3&entry.124074799=Read"},
{"title": "DelTriC: A Novel Clustering Method with Accurate Outlier", "author": "Tomas Javurek and Michal Gregor and Sebastian Kula and Marian Simko", "abstract": "The paper introduces DelTriC (Delaunay Triangulation Clustering), a clustering algorithm which integrates PCA/UMAP-based projection, Delaunay triangulation, and a novel back-projection mechanism to form clusters in the original high-dimensional space. DelTriC decouples neighborhood construction from decision-making by first triangulating in a low-dimensional proxy to index local adjacency, and then back-projecting to the original space to perform robust edge pruning, merging, and anomaly detection. DelTriC can outperform traditional methods such as k-means, DBSCAN, and HDBSCAN in many scenarios; it is both scalable and accurate, and it also significantly improves outlier detection.", "link": "http://arxiv.org/abs/2511.17219v1", "date": "2025-11-21", "relevancy": 2.2577, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4612}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4467}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DelTriC%3A%20A%20Novel%20Clustering%20Method%20with%20Accurate%20Outlier&body=Title%3A%20DelTriC%3A%20A%20Novel%20Clustering%20Method%20with%20Accurate%20Outlier%0AAuthor%3A%20Tomas%20Javurek%20and%20Michal%20Gregor%20and%20Sebastian%20Kula%20and%20Marian%20Simko%0AAbstract%3A%20The%20paper%20introduces%20DelTriC%20%28Delaunay%20Triangulation%20Clustering%29%2C%20a%20clustering%20algorithm%20which%20integrates%20PCA/UMAP-based%20projection%2C%20Delaunay%20triangulation%2C%20and%20a%20novel%20back-projection%20mechanism%20to%20form%20clusters%20in%20the%20original%20high-dimensional%20space.%20DelTriC%20decouples%20neighborhood%20construction%20from%20decision-making%20by%20first%20triangulating%20in%20a%20low-dimensional%20proxy%20to%20index%20local%20adjacency%2C%20and%20then%20back-projecting%20to%20the%20original%20space%20to%20perform%20robust%20edge%20pruning%2C%20merging%2C%20and%20anomaly%20detection.%20DelTriC%20can%20outperform%20traditional%20methods%20such%20as%20k-means%2C%20DBSCAN%2C%20and%20HDBSCAN%20in%20many%20scenarios%3B%20it%20is%20both%20scalable%20and%20accurate%2C%20and%20it%20also%20significantly%20improves%20outlier%20detection.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17219v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDelTriC%253A%2520A%2520Novel%2520Clustering%2520Method%2520with%2520Accurate%2520Outlier%26entry.906535625%3DTomas%2520Javurek%2520and%2520Michal%2520Gregor%2520and%2520Sebastian%2520Kula%2520and%2520Marian%2520Simko%26entry.1292438233%3DThe%2520paper%2520introduces%2520DelTriC%2520%2528Delaunay%2520Triangulation%2520Clustering%2529%252C%2520a%2520clustering%2520algorithm%2520which%2520integrates%2520PCA/UMAP-based%2520projection%252C%2520Delaunay%2520triangulation%252C%2520and%2520a%2520novel%2520back-projection%2520mechanism%2520to%2520form%2520clusters%2520in%2520the%2520original%2520high-dimensional%2520space.%2520DelTriC%2520decouples%2520neighborhood%2520construction%2520from%2520decision-making%2520by%2520first%2520triangulating%2520in%2520a%2520low-dimensional%2520proxy%2520to%2520index%2520local%2520adjacency%252C%2520and%2520then%2520back-projecting%2520to%2520the%2520original%2520space%2520to%2520perform%2520robust%2520edge%2520pruning%252C%2520merging%252C%2520and%2520anomaly%2520detection.%2520DelTriC%2520can%2520outperform%2520traditional%2520methods%2520such%2520as%2520k-means%252C%2520DBSCAN%252C%2520and%2520HDBSCAN%2520in%2520many%2520scenarios%253B%2520it%2520is%2520both%2520scalable%2520and%2520accurate%252C%2520and%2520it%2520also%2520significantly%2520improves%2520outlier%2520detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17219v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DelTriC%3A%20A%20Novel%20Clustering%20Method%20with%20Accurate%20Outlier&entry.906535625=Tomas%20Javurek%20and%20Michal%20Gregor%20and%20Sebastian%20Kula%20and%20Marian%20Simko&entry.1292438233=The%20paper%20introduces%20DelTriC%20%28Delaunay%20Triangulation%20Clustering%29%2C%20a%20clustering%20algorithm%20which%20integrates%20PCA/UMAP-based%20projection%2C%20Delaunay%20triangulation%2C%20and%20a%20novel%20back-projection%20mechanism%20to%20form%20clusters%20in%20the%20original%20high-dimensional%20space.%20DelTriC%20decouples%20neighborhood%20construction%20from%20decision-making%20by%20first%20triangulating%20in%20a%20low-dimensional%20proxy%20to%20index%20local%20adjacency%2C%20and%20then%20back-projecting%20to%20the%20original%20space%20to%20perform%20robust%20edge%20pruning%2C%20merging%2C%20and%20anomaly%20detection.%20DelTriC%20can%20outperform%20traditional%20methods%20such%20as%20k-means%2C%20DBSCAN%2C%20and%20HDBSCAN%20in%20many%20scenarios%3B%20it%20is%20both%20scalable%20and%20accurate%2C%20and%20it%20also%20significantly%20improves%20outlier%20detection.&entry.1838667208=http%3A//arxiv.org/abs/2511.17219v1&entry.124074799=Read"},
{"title": "Reflection-Based Relative Localization for Cooperative UAV Teams Using Active Markers", "author": "Tim Lakemann and Daniel Bonilla Licea and Viktor Walter and Martin Saska", "abstract": "Reflections of active markers in the environment are a common source of ambiguity in onboard visual relative localization. This work presents a novel approach for onboard relative localization in multi-robot teams that exploits these typically unwanted reflections of active markers in the environment. It operates without prior knowledge of robot size or predefined marker configurations and remains independent of surface properties, an essential feature for heterogeneous micro-aerial swarms cooperating in unknown environments. It explicitly accounts for uncertainties caused by non-flat surfaces, with a particular focus on dynamic water surfaces, which are especially relevant for marine deployments. We validated the approach in both indoor and outdoor experiments, demonstrating that the proposed reflection-based localization system operates reliably without prior knowledge of team member size and achieves greater effective range (above 30 m) and accuracy than state-of-the-art methods. The video and source code of this work will be made publicly available after publication.", "link": "http://arxiv.org/abs/2511.17166v1", "date": "2025-11-21", "relevancy": 2.2571, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6053}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5611}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reflection-Based%20Relative%20Localization%20for%20Cooperative%20UAV%20Teams%20Using%20Active%20Markers&body=Title%3A%20Reflection-Based%20Relative%20Localization%20for%20Cooperative%20UAV%20Teams%20Using%20Active%20Markers%0AAuthor%3A%20Tim%20Lakemann%20and%20Daniel%20Bonilla%20Licea%20and%20Viktor%20Walter%20and%20Martin%20Saska%0AAbstract%3A%20Reflections%20of%20active%20markers%20in%20the%20environment%20are%20a%20common%20source%20of%20ambiguity%20in%20onboard%20visual%20relative%20localization.%20This%20work%20presents%20a%20novel%20approach%20for%20onboard%20relative%20localization%20in%20multi-robot%20teams%20that%20exploits%20these%20typically%20unwanted%20reflections%20of%20active%20markers%20in%20the%20environment.%20It%20operates%20without%20prior%20knowledge%20of%20robot%20size%20or%20predefined%20marker%20configurations%20and%20remains%20independent%20of%20surface%20properties%2C%20an%20essential%20feature%20for%20heterogeneous%20micro-aerial%20swarms%20cooperating%20in%20unknown%20environments.%20It%20explicitly%20accounts%20for%20uncertainties%20caused%20by%20non-flat%20surfaces%2C%20with%20a%20particular%20focus%20on%20dynamic%20water%20surfaces%2C%20which%20are%20especially%20relevant%20for%20marine%20deployments.%20We%20validated%20the%20approach%20in%20both%20indoor%20and%20outdoor%20experiments%2C%20demonstrating%20that%20the%20proposed%20reflection-based%20localization%20system%20operates%20reliably%20without%20prior%20knowledge%20of%20team%20member%20size%20and%20achieves%20greater%20effective%20range%20%28above%2030%20m%29%20and%20accuracy%20than%20state-of-the-art%20methods.%20The%20video%20and%20source%20code%20of%20this%20work%20will%20be%20made%20publicly%20available%20after%20publication.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17166v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReflection-Based%2520Relative%2520Localization%2520for%2520Cooperative%2520UAV%2520Teams%2520Using%2520Active%2520Markers%26entry.906535625%3DTim%2520Lakemann%2520and%2520Daniel%2520Bonilla%2520Licea%2520and%2520Viktor%2520Walter%2520and%2520Martin%2520Saska%26entry.1292438233%3DReflections%2520of%2520active%2520markers%2520in%2520the%2520environment%2520are%2520a%2520common%2520source%2520of%2520ambiguity%2520in%2520onboard%2520visual%2520relative%2520localization.%2520This%2520work%2520presents%2520a%2520novel%2520approach%2520for%2520onboard%2520relative%2520localization%2520in%2520multi-robot%2520teams%2520that%2520exploits%2520these%2520typically%2520unwanted%2520reflections%2520of%2520active%2520markers%2520in%2520the%2520environment.%2520It%2520operates%2520without%2520prior%2520knowledge%2520of%2520robot%2520size%2520or%2520predefined%2520marker%2520configurations%2520and%2520remains%2520independent%2520of%2520surface%2520properties%252C%2520an%2520essential%2520feature%2520for%2520heterogeneous%2520micro-aerial%2520swarms%2520cooperating%2520in%2520unknown%2520environments.%2520It%2520explicitly%2520accounts%2520for%2520uncertainties%2520caused%2520by%2520non-flat%2520surfaces%252C%2520with%2520a%2520particular%2520focus%2520on%2520dynamic%2520water%2520surfaces%252C%2520which%2520are%2520especially%2520relevant%2520for%2520marine%2520deployments.%2520We%2520validated%2520the%2520approach%2520in%2520both%2520indoor%2520and%2520outdoor%2520experiments%252C%2520demonstrating%2520that%2520the%2520proposed%2520reflection-based%2520localization%2520system%2520operates%2520reliably%2520without%2520prior%2520knowledge%2520of%2520team%2520member%2520size%2520and%2520achieves%2520greater%2520effective%2520range%2520%2528above%252030%2520m%2529%2520and%2520accuracy%2520than%2520state-of-the-art%2520methods.%2520The%2520video%2520and%2520source%2520code%2520of%2520this%2520work%2520will%2520be%2520made%2520publicly%2520available%2520after%2520publication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17166v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reflection-Based%20Relative%20Localization%20for%20Cooperative%20UAV%20Teams%20Using%20Active%20Markers&entry.906535625=Tim%20Lakemann%20and%20Daniel%20Bonilla%20Licea%20and%20Viktor%20Walter%20and%20Martin%20Saska&entry.1292438233=Reflections%20of%20active%20markers%20in%20the%20environment%20are%20a%20common%20source%20of%20ambiguity%20in%20onboard%20visual%20relative%20localization.%20This%20work%20presents%20a%20novel%20approach%20for%20onboard%20relative%20localization%20in%20multi-robot%20teams%20that%20exploits%20these%20typically%20unwanted%20reflections%20of%20active%20markers%20in%20the%20environment.%20It%20operates%20without%20prior%20knowledge%20of%20robot%20size%20or%20predefined%20marker%20configurations%20and%20remains%20independent%20of%20surface%20properties%2C%20an%20essential%20feature%20for%20heterogeneous%20micro-aerial%20swarms%20cooperating%20in%20unknown%20environments.%20It%20explicitly%20accounts%20for%20uncertainties%20caused%20by%20non-flat%20surfaces%2C%20with%20a%20particular%20focus%20on%20dynamic%20water%20surfaces%2C%20which%20are%20especially%20relevant%20for%20marine%20deployments.%20We%20validated%20the%20approach%20in%20both%20indoor%20and%20outdoor%20experiments%2C%20demonstrating%20that%20the%20proposed%20reflection-based%20localization%20system%20operates%20reliably%20without%20prior%20knowledge%20of%20team%20member%20size%20and%20achieves%20greater%20effective%20range%20%28above%2030%20m%29%20and%20accuracy%20than%20state-of-the-art%20methods.%20The%20video%20and%20source%20code%20of%20this%20work%20will%20be%20made%20publicly%20available%20after%20publication.&entry.1838667208=http%3A//arxiv.org/abs/2511.17166v1&entry.124074799=Read"},
{"title": "Fairness Evaluation of Large Language Models in Academic Library Reference Services", "author": "Haining Wang and Jason Clark and Yueru Yan and Star Bradley and Ruiyang Chen and Yiqiong Zhang and Hengyi Fu and Zuoyu Tian", "abstract": "As libraries explore large language models (LLMs) for use in virtual reference services, a key question arises: Can LLMs serve all users equitably, regardless of demographics or social status? While they offer great potential for scalable support, LLMs may also reproduce societal biases embedded in their training data, risking the integrity of libraries' commitment to equitable service. To address this concern, we evaluate whether LLMs differentiate responses across user identities by prompting six state-of-the-art LLMs to assist patrons differing in sex, race/ethnicity, and institutional role. We find no evidence of differentiation by race or ethnicity, and only minor evidence of stereotypical bias against women in one model. LLMs demonstrate nuanced accommodation of institutional roles through the use of linguistic choices related to formality, politeness, and domain-specific vocabularies, reflecting professional norms rather than discriminatory treatment. These findings suggest that current LLMs show a promising degree of readiness to support equitable and contextually appropriate communication in academic library reference services.", "link": "http://arxiv.org/abs/2507.04224v3", "date": "2025-11-21", "relevancy": 2.2468, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4556}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fairness%20Evaluation%20of%20Large%20Language%20Models%20in%20Academic%20Library%20Reference%20Services&body=Title%3A%20Fairness%20Evaluation%20of%20Large%20Language%20Models%20in%20Academic%20Library%20Reference%20Services%0AAuthor%3A%20Haining%20Wang%20and%20Jason%20Clark%20and%20Yueru%20Yan%20and%20Star%20Bradley%20and%20Ruiyang%20Chen%20and%20Yiqiong%20Zhang%20and%20Hengyi%20Fu%20and%20Zuoyu%20Tian%0AAbstract%3A%20As%20libraries%20explore%20large%20language%20models%20%28LLMs%29%20for%20use%20in%20virtual%20reference%20services%2C%20a%20key%20question%20arises%3A%20Can%20LLMs%20serve%20all%20users%20equitably%2C%20regardless%20of%20demographics%20or%20social%20status%3F%20While%20they%20offer%20great%20potential%20for%20scalable%20support%2C%20LLMs%20may%20also%20reproduce%20societal%20biases%20embedded%20in%20their%20training%20data%2C%20risking%20the%20integrity%20of%20libraries%27%20commitment%20to%20equitable%20service.%20To%20address%20this%20concern%2C%20we%20evaluate%20whether%20LLMs%20differentiate%20responses%20across%20user%20identities%20by%20prompting%20six%20state-of-the-art%20LLMs%20to%20assist%20patrons%20differing%20in%20sex%2C%20race/ethnicity%2C%20and%20institutional%20role.%20We%20find%20no%20evidence%20of%20differentiation%20by%20race%20or%20ethnicity%2C%20and%20only%20minor%20evidence%20of%20stereotypical%20bias%20against%20women%20in%20one%20model.%20LLMs%20demonstrate%20nuanced%20accommodation%20of%20institutional%20roles%20through%20the%20use%20of%20linguistic%20choices%20related%20to%20formality%2C%20politeness%2C%20and%20domain-specific%20vocabularies%2C%20reflecting%20professional%20norms%20rather%20than%20discriminatory%20treatment.%20These%20findings%20suggest%20that%20current%20LLMs%20show%20a%20promising%20degree%20of%20readiness%20to%20support%20equitable%20and%20contextually%20appropriate%20communication%20in%20academic%20library%20reference%20services.%0ALink%3A%20http%3A//arxiv.org/abs/2507.04224v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairness%2520Evaluation%2520of%2520Large%2520Language%2520Models%2520in%2520Academic%2520Library%2520Reference%2520Services%26entry.906535625%3DHaining%2520Wang%2520and%2520Jason%2520Clark%2520and%2520Yueru%2520Yan%2520and%2520Star%2520Bradley%2520and%2520Ruiyang%2520Chen%2520and%2520Yiqiong%2520Zhang%2520and%2520Hengyi%2520Fu%2520and%2520Zuoyu%2520Tian%26entry.1292438233%3DAs%2520libraries%2520explore%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520use%2520in%2520virtual%2520reference%2520services%252C%2520a%2520key%2520question%2520arises%253A%2520Can%2520LLMs%2520serve%2520all%2520users%2520equitably%252C%2520regardless%2520of%2520demographics%2520or%2520social%2520status%253F%2520While%2520they%2520offer%2520great%2520potential%2520for%2520scalable%2520support%252C%2520LLMs%2520may%2520also%2520reproduce%2520societal%2520biases%2520embedded%2520in%2520their%2520training%2520data%252C%2520risking%2520the%2520integrity%2520of%2520libraries%2527%2520commitment%2520to%2520equitable%2520service.%2520To%2520address%2520this%2520concern%252C%2520we%2520evaluate%2520whether%2520LLMs%2520differentiate%2520responses%2520across%2520user%2520identities%2520by%2520prompting%2520six%2520state-of-the-art%2520LLMs%2520to%2520assist%2520patrons%2520differing%2520in%2520sex%252C%2520race/ethnicity%252C%2520and%2520institutional%2520role.%2520We%2520find%2520no%2520evidence%2520of%2520differentiation%2520by%2520race%2520or%2520ethnicity%252C%2520and%2520only%2520minor%2520evidence%2520of%2520stereotypical%2520bias%2520against%2520women%2520in%2520one%2520model.%2520LLMs%2520demonstrate%2520nuanced%2520accommodation%2520of%2520institutional%2520roles%2520through%2520the%2520use%2520of%2520linguistic%2520choices%2520related%2520to%2520formality%252C%2520politeness%252C%2520and%2520domain-specific%2520vocabularies%252C%2520reflecting%2520professional%2520norms%2520rather%2520than%2520discriminatory%2520treatment.%2520These%2520findings%2520suggest%2520that%2520current%2520LLMs%2520show%2520a%2520promising%2520degree%2520of%2520readiness%2520to%2520support%2520equitable%2520and%2520contextually%2520appropriate%2520communication%2520in%2520academic%2520library%2520reference%2520services.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04224v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fairness%20Evaluation%20of%20Large%20Language%20Models%20in%20Academic%20Library%20Reference%20Services&entry.906535625=Haining%20Wang%20and%20Jason%20Clark%20and%20Yueru%20Yan%20and%20Star%20Bradley%20and%20Ruiyang%20Chen%20and%20Yiqiong%20Zhang%20and%20Hengyi%20Fu%20and%20Zuoyu%20Tian&entry.1292438233=As%20libraries%20explore%20large%20language%20models%20%28LLMs%29%20for%20use%20in%20virtual%20reference%20services%2C%20a%20key%20question%20arises%3A%20Can%20LLMs%20serve%20all%20users%20equitably%2C%20regardless%20of%20demographics%20or%20social%20status%3F%20While%20they%20offer%20great%20potential%20for%20scalable%20support%2C%20LLMs%20may%20also%20reproduce%20societal%20biases%20embedded%20in%20their%20training%20data%2C%20risking%20the%20integrity%20of%20libraries%27%20commitment%20to%20equitable%20service.%20To%20address%20this%20concern%2C%20we%20evaluate%20whether%20LLMs%20differentiate%20responses%20across%20user%20identities%20by%20prompting%20six%20state-of-the-art%20LLMs%20to%20assist%20patrons%20differing%20in%20sex%2C%20race/ethnicity%2C%20and%20institutional%20role.%20We%20find%20no%20evidence%20of%20differentiation%20by%20race%20or%20ethnicity%2C%20and%20only%20minor%20evidence%20of%20stereotypical%20bias%20against%20women%20in%20one%20model.%20LLMs%20demonstrate%20nuanced%20accommodation%20of%20institutional%20roles%20through%20the%20use%20of%20linguistic%20choices%20related%20to%20formality%2C%20politeness%2C%20and%20domain-specific%20vocabularies%2C%20reflecting%20professional%20norms%20rather%20than%20discriminatory%20treatment.%20These%20findings%20suggest%20that%20current%20LLMs%20show%20a%20promising%20degree%20of%20readiness%20to%20support%20equitable%20and%20contextually%20appropriate%20communication%20in%20academic%20library%20reference%20services.&entry.1838667208=http%3A//arxiv.org/abs/2507.04224v3&entry.124074799=Read"},
{"title": "RynnVLA-002: A Unified Vision-Language-Action and World Model", "author": "Jun Cen and Siteng Huang and Yuqian Yuan and Hangjie Yuan and Chaohui Yu and Yuming Jiang and Jiayan Guo and Kehan Li and Hao Luo and Fan Wang and Xin Li and Deli Zhao and Hao Chen", "abstract": "We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model. The world model leverages action and visual inputs to predict future image states, learning the underlying physics of the environment to refine action generation. Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%.", "link": "http://arxiv.org/abs/2511.17502v1", "date": "2025-11-21", "relevancy": 2.2279, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5634}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5634}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RynnVLA-002%3A%20A%20Unified%20Vision-Language-Action%20and%20World%20Model&body=Title%3A%20RynnVLA-002%3A%20A%20Unified%20Vision-Language-Action%20and%20World%20Model%0AAuthor%3A%20Jun%20Cen%20and%20Siteng%20Huang%20and%20Yuqian%20Yuan%20and%20Hangjie%20Yuan%20and%20Chaohui%20Yu%20and%20Yuming%20Jiang%20and%20Jiayan%20Guo%20and%20Kehan%20Li%20and%20Hao%20Luo%20and%20Fan%20Wang%20and%20Xin%20Li%20and%20Deli%20Zhao%20and%20Hao%20Chen%0AAbstract%3A%20We%20introduce%20RynnVLA-002%2C%20a%20unified%20Vision-Language-Action%20%28VLA%29%20and%20world%20model.%20The%20world%20model%20leverages%20action%20and%20visual%20inputs%20to%20predict%20future%20image%20states%2C%20learning%20the%20underlying%20physics%20of%20the%20environment%20to%20refine%20action%20generation.%20Conversely%2C%20the%20VLA%20model%20produces%20subsequent%20actions%20from%20image%20observations%2C%20enhancing%20visual%20understanding%20and%20supporting%20the%20world%20model%27s%20image%20generation.%20The%20unified%20framework%20of%20RynnVLA-002%20enables%20joint%20learning%20of%20environmental%20dynamics%20and%20action%20planning.%20Our%20experiments%20show%20that%20RynnVLA-002%20surpasses%20individual%20VLA%20and%20world%20models%2C%20demonstrating%20their%20mutual%20enhancement.%20We%20evaluate%20RynnVLA-002%20in%20both%20simulation%20and%20real-world%20robot%20tasks.%20RynnVLA-002%20achieves%2097.4%25%20success%20rate%20on%20the%20LIBERO%20simulation%20benchmark%20without%20pretraining%2C%20while%20in%20real-world%20LeRobot%20experiments%2C%20its%20integrated%20world%20model%20boosts%20the%20overall%20success%20rate%20by%2050%25.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRynnVLA-002%253A%2520A%2520Unified%2520Vision-Language-Action%2520and%2520World%2520Model%26entry.906535625%3DJun%2520Cen%2520and%2520Siteng%2520Huang%2520and%2520Yuqian%2520Yuan%2520and%2520Hangjie%2520Yuan%2520and%2520Chaohui%2520Yu%2520and%2520Yuming%2520Jiang%2520and%2520Jiayan%2520Guo%2520and%2520Kehan%2520Li%2520and%2520Hao%2520Luo%2520and%2520Fan%2520Wang%2520and%2520Xin%2520Li%2520and%2520Deli%2520Zhao%2520and%2520Hao%2520Chen%26entry.1292438233%3DWe%2520introduce%2520RynnVLA-002%252C%2520a%2520unified%2520Vision-Language-Action%2520%2528VLA%2529%2520and%2520world%2520model.%2520The%2520world%2520model%2520leverages%2520action%2520and%2520visual%2520inputs%2520to%2520predict%2520future%2520image%2520states%252C%2520learning%2520the%2520underlying%2520physics%2520of%2520the%2520environment%2520to%2520refine%2520action%2520generation.%2520Conversely%252C%2520the%2520VLA%2520model%2520produces%2520subsequent%2520actions%2520from%2520image%2520observations%252C%2520enhancing%2520visual%2520understanding%2520and%2520supporting%2520the%2520world%2520model%2527s%2520image%2520generation.%2520The%2520unified%2520framework%2520of%2520RynnVLA-002%2520enables%2520joint%2520learning%2520of%2520environmental%2520dynamics%2520and%2520action%2520planning.%2520Our%2520experiments%2520show%2520that%2520RynnVLA-002%2520surpasses%2520individual%2520VLA%2520and%2520world%2520models%252C%2520demonstrating%2520their%2520mutual%2520enhancement.%2520We%2520evaluate%2520RynnVLA-002%2520in%2520both%2520simulation%2520and%2520real-world%2520robot%2520tasks.%2520RynnVLA-002%2520achieves%252097.4%2525%2520success%2520rate%2520on%2520the%2520LIBERO%2520simulation%2520benchmark%2520without%2520pretraining%252C%2520while%2520in%2520real-world%2520LeRobot%2520experiments%252C%2520its%2520integrated%2520world%2520model%2520boosts%2520the%2520overall%2520success%2520rate%2520by%252050%2525.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RynnVLA-002%3A%20A%20Unified%20Vision-Language-Action%20and%20World%20Model&entry.906535625=Jun%20Cen%20and%20Siteng%20Huang%20and%20Yuqian%20Yuan%20and%20Hangjie%20Yuan%20and%20Chaohui%20Yu%20and%20Yuming%20Jiang%20and%20Jiayan%20Guo%20and%20Kehan%20Li%20and%20Hao%20Luo%20and%20Fan%20Wang%20and%20Xin%20Li%20and%20Deli%20Zhao%20and%20Hao%20Chen&entry.1292438233=We%20introduce%20RynnVLA-002%2C%20a%20unified%20Vision-Language-Action%20%28VLA%29%20and%20world%20model.%20The%20world%20model%20leverages%20action%20and%20visual%20inputs%20to%20predict%20future%20image%20states%2C%20learning%20the%20underlying%20physics%20of%20the%20environment%20to%20refine%20action%20generation.%20Conversely%2C%20the%20VLA%20model%20produces%20subsequent%20actions%20from%20image%20observations%2C%20enhancing%20visual%20understanding%20and%20supporting%20the%20world%20model%27s%20image%20generation.%20The%20unified%20framework%20of%20RynnVLA-002%20enables%20joint%20learning%20of%20environmental%20dynamics%20and%20action%20planning.%20Our%20experiments%20show%20that%20RynnVLA-002%20surpasses%20individual%20VLA%20and%20world%20models%2C%20demonstrating%20their%20mutual%20enhancement.%20We%20evaluate%20RynnVLA-002%20in%20both%20simulation%20and%20real-world%20robot%20tasks.%20RynnVLA-002%20achieves%2097.4%25%20success%20rate%20on%20the%20LIBERO%20simulation%20benchmark%20without%20pretraining%2C%20while%20in%20real-world%20LeRobot%20experiments%2C%20its%20integrated%20world%20model%20boosts%20the%20overall%20success%20rate%20by%2050%25.&entry.1838667208=http%3A//arxiv.org/abs/2511.17502v1&entry.124074799=Read"},
{"title": "A Little More Like This: Text-to-Image Retrieval with Vision-Language Models Using Relevance Feedback", "author": "Bulat Khaertdinov and Mirela Popa and Nava Tintarev", "abstract": "Large vision-language models (VLMs) enable intuitive visual search using natural language queries. However, improving their performance often requires fine-tuning and scaling to larger model variants. In this work, we propose a mechanism inspired by traditional text-based search to improve retrieval performance at inference time: relevance feedback. While relevance feedback can serve as an alternative to fine-tuning, its model-agnostic design also enables use with fine-tuned VLMs. Specifically, we introduce and evaluate four feedback strategies for VLM-based retrieval. First, we revise classical pseudo-relevance feedback (PRF), which refines query embeddings based on top-ranked results. To address its limitations, we propose generative relevance feedback (GRF), which uses synthetic captions for query refinement. Furthermore, we introduce an attentive feedback summarizer (AFS), a custom transformer-based model that integrates multimodal fine-grained features from relevant items. Finally, we simulate explicit feedback using ground-truth captions as an upper-bound baseline. Experiments on Flickr30k and COCO with the VLM backbones show that GRF, AFS, and explicit feedback improve retrieval performance by 3-5% in MRR@5 for smaller VLMs, and 1-3% for larger ones, compared to retrieval with no feedback. Moreover, AFS, similarly to explicit feedback, mitigates query drift and is more robust than GRF in iterative, multi-turn retrieval settings. Our findings demonstrate that relevance feedback can consistently enhance retrieval across VLMs and open up opportunities for interactive and adaptive visual search.", "link": "http://arxiv.org/abs/2511.17255v1", "date": "2025-11-21", "relevancy": 2.2272, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5643}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5643}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Little%20More%20Like%20This%3A%20Text-to-Image%20Retrieval%20with%20Vision-Language%20Models%20Using%20Relevance%20Feedback&body=Title%3A%20A%20Little%20More%20Like%20This%3A%20Text-to-Image%20Retrieval%20with%20Vision-Language%20Models%20Using%20Relevance%20Feedback%0AAuthor%3A%20Bulat%20Khaertdinov%20and%20Mirela%20Popa%20and%20Nava%20Tintarev%0AAbstract%3A%20Large%20vision-language%20models%20%28VLMs%29%20enable%20intuitive%20visual%20search%20using%20natural%20language%20queries.%20However%2C%20improving%20their%20performance%20often%20requires%20fine-tuning%20and%20scaling%20to%20larger%20model%20variants.%20In%20this%20work%2C%20we%20propose%20a%20mechanism%20inspired%20by%20traditional%20text-based%20search%20to%20improve%20retrieval%20performance%20at%20inference%20time%3A%20relevance%20feedback.%20While%20relevance%20feedback%20can%20serve%20as%20an%20alternative%20to%20fine-tuning%2C%20its%20model-agnostic%20design%20also%20enables%20use%20with%20fine-tuned%20VLMs.%20Specifically%2C%20we%20introduce%20and%20evaluate%20four%20feedback%20strategies%20for%20VLM-based%20retrieval.%20First%2C%20we%20revise%20classical%20pseudo-relevance%20feedback%20%28PRF%29%2C%20which%20refines%20query%20embeddings%20based%20on%20top-ranked%20results.%20To%20address%20its%20limitations%2C%20we%20propose%20generative%20relevance%20feedback%20%28GRF%29%2C%20which%20uses%20synthetic%20captions%20for%20query%20refinement.%20Furthermore%2C%20we%20introduce%20an%20attentive%20feedback%20summarizer%20%28AFS%29%2C%20a%20custom%20transformer-based%20model%20that%20integrates%20multimodal%20fine-grained%20features%20from%20relevant%20items.%20Finally%2C%20we%20simulate%20explicit%20feedback%20using%20ground-truth%20captions%20as%20an%20upper-bound%20baseline.%20Experiments%20on%20Flickr30k%20and%20COCO%20with%20the%20VLM%20backbones%20show%20that%20GRF%2C%20AFS%2C%20and%20explicit%20feedback%20improve%20retrieval%20performance%20by%203-5%25%20in%20MRR%405%20for%20smaller%20VLMs%2C%20and%201-3%25%20for%20larger%20ones%2C%20compared%20to%20retrieval%20with%20no%20feedback.%20Moreover%2C%20AFS%2C%20similarly%20to%20explicit%20feedback%2C%20mitigates%20query%20drift%20and%20is%20more%20robust%20than%20GRF%20in%20iterative%2C%20multi-turn%20retrieval%20settings.%20Our%20findings%20demonstrate%20that%20relevance%20feedback%20can%20consistently%20enhance%20retrieval%20across%20VLMs%20and%20open%20up%20opportunities%20for%20interactive%20and%20adaptive%20visual%20search.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Little%2520More%2520Like%2520This%253A%2520Text-to-Image%2520Retrieval%2520with%2520Vision-Language%2520Models%2520Using%2520Relevance%2520Feedback%26entry.906535625%3DBulat%2520Khaertdinov%2520and%2520Mirela%2520Popa%2520and%2520Nava%2520Tintarev%26entry.1292438233%3DLarge%2520vision-language%2520models%2520%2528VLMs%2529%2520enable%2520intuitive%2520visual%2520search%2520using%2520natural%2520language%2520queries.%2520However%252C%2520improving%2520their%2520performance%2520often%2520requires%2520fine-tuning%2520and%2520scaling%2520to%2520larger%2520model%2520variants.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520mechanism%2520inspired%2520by%2520traditional%2520text-based%2520search%2520to%2520improve%2520retrieval%2520performance%2520at%2520inference%2520time%253A%2520relevance%2520feedback.%2520While%2520relevance%2520feedback%2520can%2520serve%2520as%2520an%2520alternative%2520to%2520fine-tuning%252C%2520its%2520model-agnostic%2520design%2520also%2520enables%2520use%2520with%2520fine-tuned%2520VLMs.%2520Specifically%252C%2520we%2520introduce%2520and%2520evaluate%2520four%2520feedback%2520strategies%2520for%2520VLM-based%2520retrieval.%2520First%252C%2520we%2520revise%2520classical%2520pseudo-relevance%2520feedback%2520%2528PRF%2529%252C%2520which%2520refines%2520query%2520embeddings%2520based%2520on%2520top-ranked%2520results.%2520To%2520address%2520its%2520limitations%252C%2520we%2520propose%2520generative%2520relevance%2520feedback%2520%2528GRF%2529%252C%2520which%2520uses%2520synthetic%2520captions%2520for%2520query%2520refinement.%2520Furthermore%252C%2520we%2520introduce%2520an%2520attentive%2520feedback%2520summarizer%2520%2528AFS%2529%252C%2520a%2520custom%2520transformer-based%2520model%2520that%2520integrates%2520multimodal%2520fine-grained%2520features%2520from%2520relevant%2520items.%2520Finally%252C%2520we%2520simulate%2520explicit%2520feedback%2520using%2520ground-truth%2520captions%2520as%2520an%2520upper-bound%2520baseline.%2520Experiments%2520on%2520Flickr30k%2520and%2520COCO%2520with%2520the%2520VLM%2520backbones%2520show%2520that%2520GRF%252C%2520AFS%252C%2520and%2520explicit%2520feedback%2520improve%2520retrieval%2520performance%2520by%25203-5%2525%2520in%2520MRR%25405%2520for%2520smaller%2520VLMs%252C%2520and%25201-3%2525%2520for%2520larger%2520ones%252C%2520compared%2520to%2520retrieval%2520with%2520no%2520feedback.%2520Moreover%252C%2520AFS%252C%2520similarly%2520to%2520explicit%2520feedback%252C%2520mitigates%2520query%2520drift%2520and%2520is%2520more%2520robust%2520than%2520GRF%2520in%2520iterative%252C%2520multi-turn%2520retrieval%2520settings.%2520Our%2520findings%2520demonstrate%2520that%2520relevance%2520feedback%2520can%2520consistently%2520enhance%2520retrieval%2520across%2520VLMs%2520and%2520open%2520up%2520opportunities%2520for%2520interactive%2520and%2520adaptive%2520visual%2520search.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Little%20More%20Like%20This%3A%20Text-to-Image%20Retrieval%20with%20Vision-Language%20Models%20Using%20Relevance%20Feedback&entry.906535625=Bulat%20Khaertdinov%20and%20Mirela%20Popa%20and%20Nava%20Tintarev&entry.1292438233=Large%20vision-language%20models%20%28VLMs%29%20enable%20intuitive%20visual%20search%20using%20natural%20language%20queries.%20However%2C%20improving%20their%20performance%20often%20requires%20fine-tuning%20and%20scaling%20to%20larger%20model%20variants.%20In%20this%20work%2C%20we%20propose%20a%20mechanism%20inspired%20by%20traditional%20text-based%20search%20to%20improve%20retrieval%20performance%20at%20inference%20time%3A%20relevance%20feedback.%20While%20relevance%20feedback%20can%20serve%20as%20an%20alternative%20to%20fine-tuning%2C%20its%20model-agnostic%20design%20also%20enables%20use%20with%20fine-tuned%20VLMs.%20Specifically%2C%20we%20introduce%20and%20evaluate%20four%20feedback%20strategies%20for%20VLM-based%20retrieval.%20First%2C%20we%20revise%20classical%20pseudo-relevance%20feedback%20%28PRF%29%2C%20which%20refines%20query%20embeddings%20based%20on%20top-ranked%20results.%20To%20address%20its%20limitations%2C%20we%20propose%20generative%20relevance%20feedback%20%28GRF%29%2C%20which%20uses%20synthetic%20captions%20for%20query%20refinement.%20Furthermore%2C%20we%20introduce%20an%20attentive%20feedback%20summarizer%20%28AFS%29%2C%20a%20custom%20transformer-based%20model%20that%20integrates%20multimodal%20fine-grained%20features%20from%20relevant%20items.%20Finally%2C%20we%20simulate%20explicit%20feedback%20using%20ground-truth%20captions%20as%20an%20upper-bound%20baseline.%20Experiments%20on%20Flickr30k%20and%20COCO%20with%20the%20VLM%20backbones%20show%20that%20GRF%2C%20AFS%2C%20and%20explicit%20feedback%20improve%20retrieval%20performance%20by%203-5%25%20in%20MRR%405%20for%20smaller%20VLMs%2C%20and%201-3%25%20for%20larger%20ones%2C%20compared%20to%20retrieval%20with%20no%20feedback.%20Moreover%2C%20AFS%2C%20similarly%20to%20explicit%20feedback%2C%20mitigates%20query%20drift%20and%20is%20more%20robust%20than%20GRF%20in%20iterative%2C%20multi-turn%20retrieval%20settings.%20Our%20findings%20demonstrate%20that%20relevance%20feedback%20can%20consistently%20enhance%20retrieval%20across%20VLMs%20and%20open%20up%20opportunities%20for%20interactive%20and%20adaptive%20visual%20search.&entry.1838667208=http%3A//arxiv.org/abs/2511.17255v1&entry.124074799=Read"},
{"title": "MMT-ARD: Multimodal Multi-Teacher Adversarial Distillation for Robust Vision-Language Models", "author": "Yuqi Li and Junhao Dong and Chuanguang Yang and Shiping Wen and Piotr Koniusz and Tingwen Huang and Yingli Tian and Yew-Soon Ong", "abstract": "Vision-Language Models (VLMs) are increasingly deployed in safety-critical applications, making their adversarial robustness a crucial concern. While adversarial knowledge distillation has shown promise in transferring robustness from teacher to student models, traditional single-teacher approaches suffer from limited knowledge diversity, slow convergence, and difficulty in balancing robustness and accuracy. To address these challenges, we propose MMT-ARD: a Multimodal Multi-Teacher Adversarial Robust Distillation framework. Our key innovation is a dual-teacher knowledge fusion architecture that collaboratively optimizes clean feature preservation and robust feature enhancement. To better handle challenging adversarial examples, we introduce a dynamic weight allocation strategy based on teacher confidence, enabling adaptive focus on harder samples. Moreover, to mitigate bias among teachers, we design an adaptive sigmoid-based weighting function that balances the strength of knowledge transfer across modalities. Extensive experiments on ImageNet and zero-shot benchmarks demonstrate that MMT-ARD improves robust accuracy by +4.32% and zero-shot accuracy by +3.5% on the ViT-B-32 model, while achieving a 2.3x increase in training efficiency over traditional single-teacher methods. These results highlight the effectiveness and scalability of MMT-ARD in enhancing the adversarial robustness of multimodal large models. Our codes are available at https://github.com/itsnotacie/MMT-ARD.", "link": "http://arxiv.org/abs/2511.17448v1", "date": "2025-11-21", "relevancy": 2.2096, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5911}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5254}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMT-ARD%3A%20Multimodal%20Multi-Teacher%20Adversarial%20Distillation%20for%20Robust%20Vision-Language%20Models&body=Title%3A%20MMT-ARD%3A%20Multimodal%20Multi-Teacher%20Adversarial%20Distillation%20for%20Robust%20Vision-Language%20Models%0AAuthor%3A%20Yuqi%20Li%20and%20Junhao%20Dong%20and%20Chuanguang%20Yang%20and%20Shiping%20Wen%20and%20Piotr%20Koniusz%20and%20Tingwen%20Huang%20and%20Yingli%20Tian%20and%20Yew-Soon%20Ong%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20are%20increasingly%20deployed%20in%20safety-critical%20applications%2C%20making%20their%20adversarial%20robustness%20a%20crucial%20concern.%20While%20adversarial%20knowledge%20distillation%20has%20shown%20promise%20in%20transferring%20robustness%20from%20teacher%20to%20student%20models%2C%20traditional%20single-teacher%20approaches%20suffer%20from%20limited%20knowledge%20diversity%2C%20slow%20convergence%2C%20and%20difficulty%20in%20balancing%20robustness%20and%20accuracy.%20To%20address%20these%20challenges%2C%20we%20propose%20MMT-ARD%3A%20a%20Multimodal%20Multi-Teacher%20Adversarial%20Robust%20Distillation%20framework.%20Our%20key%20innovation%20is%20a%20dual-teacher%20knowledge%20fusion%20architecture%20that%20collaboratively%20optimizes%20clean%20feature%20preservation%20and%20robust%20feature%20enhancement.%20To%20better%20handle%20challenging%20adversarial%20examples%2C%20we%20introduce%20a%20dynamic%20weight%20allocation%20strategy%20based%20on%20teacher%20confidence%2C%20enabling%20adaptive%20focus%20on%20harder%20samples.%20Moreover%2C%20to%20mitigate%20bias%20among%20teachers%2C%20we%20design%20an%20adaptive%20sigmoid-based%20weighting%20function%20that%20balances%20the%20strength%20of%20knowledge%20transfer%20across%20modalities.%20Extensive%20experiments%20on%20ImageNet%20and%20zero-shot%20benchmarks%20demonstrate%20that%20MMT-ARD%20improves%20robust%20accuracy%20by%20%2B4.32%25%20and%20zero-shot%20accuracy%20by%20%2B3.5%25%20on%20the%20ViT-B-32%20model%2C%20while%20achieving%20a%202.3x%20increase%20in%20training%20efficiency%20over%20traditional%20single-teacher%20methods.%20These%20results%20highlight%20the%20effectiveness%20and%20scalability%20of%20MMT-ARD%20in%20enhancing%20the%20adversarial%20robustness%20of%20multimodal%20large%20models.%20Our%20codes%20are%20available%20at%20https%3A//github.com/itsnotacie/MMT-ARD.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMT-ARD%253A%2520Multimodal%2520Multi-Teacher%2520Adversarial%2520Distillation%2520for%2520Robust%2520Vision-Language%2520Models%26entry.906535625%3DYuqi%2520Li%2520and%2520Junhao%2520Dong%2520and%2520Chuanguang%2520Yang%2520and%2520Shiping%2520Wen%2520and%2520Piotr%2520Koniusz%2520and%2520Tingwen%2520Huang%2520and%2520Yingli%2520Tian%2520and%2520Yew-Soon%2520Ong%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520are%2520increasingly%2520deployed%2520in%2520safety-critical%2520applications%252C%2520making%2520their%2520adversarial%2520robustness%2520a%2520crucial%2520concern.%2520While%2520adversarial%2520knowledge%2520distillation%2520has%2520shown%2520promise%2520in%2520transferring%2520robustness%2520from%2520teacher%2520to%2520student%2520models%252C%2520traditional%2520single-teacher%2520approaches%2520suffer%2520from%2520limited%2520knowledge%2520diversity%252C%2520slow%2520convergence%252C%2520and%2520difficulty%2520in%2520balancing%2520robustness%2520and%2520accuracy.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520MMT-ARD%253A%2520a%2520Multimodal%2520Multi-Teacher%2520Adversarial%2520Robust%2520Distillation%2520framework.%2520Our%2520key%2520innovation%2520is%2520a%2520dual-teacher%2520knowledge%2520fusion%2520architecture%2520that%2520collaboratively%2520optimizes%2520clean%2520feature%2520preservation%2520and%2520robust%2520feature%2520enhancement.%2520To%2520better%2520handle%2520challenging%2520adversarial%2520examples%252C%2520we%2520introduce%2520a%2520dynamic%2520weight%2520allocation%2520strategy%2520based%2520on%2520teacher%2520confidence%252C%2520enabling%2520adaptive%2520focus%2520on%2520harder%2520samples.%2520Moreover%252C%2520to%2520mitigate%2520bias%2520among%2520teachers%252C%2520we%2520design%2520an%2520adaptive%2520sigmoid-based%2520weighting%2520function%2520that%2520balances%2520the%2520strength%2520of%2520knowledge%2520transfer%2520across%2520modalities.%2520Extensive%2520experiments%2520on%2520ImageNet%2520and%2520zero-shot%2520benchmarks%2520demonstrate%2520that%2520MMT-ARD%2520improves%2520robust%2520accuracy%2520by%2520%252B4.32%2525%2520and%2520zero-shot%2520accuracy%2520by%2520%252B3.5%2525%2520on%2520the%2520ViT-B-32%2520model%252C%2520while%2520achieving%2520a%25202.3x%2520increase%2520in%2520training%2520efficiency%2520over%2520traditional%2520single-teacher%2520methods.%2520These%2520results%2520highlight%2520the%2520effectiveness%2520and%2520scalability%2520of%2520MMT-ARD%2520in%2520enhancing%2520the%2520adversarial%2520robustness%2520of%2520multimodal%2520large%2520models.%2520Our%2520codes%2520are%2520available%2520at%2520https%253A//github.com/itsnotacie/MMT-ARD.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMT-ARD%3A%20Multimodal%20Multi-Teacher%20Adversarial%20Distillation%20for%20Robust%20Vision-Language%20Models&entry.906535625=Yuqi%20Li%20and%20Junhao%20Dong%20and%20Chuanguang%20Yang%20and%20Shiping%20Wen%20and%20Piotr%20Koniusz%20and%20Tingwen%20Huang%20and%20Yingli%20Tian%20and%20Yew-Soon%20Ong&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20are%20increasingly%20deployed%20in%20safety-critical%20applications%2C%20making%20their%20adversarial%20robustness%20a%20crucial%20concern.%20While%20adversarial%20knowledge%20distillation%20has%20shown%20promise%20in%20transferring%20robustness%20from%20teacher%20to%20student%20models%2C%20traditional%20single-teacher%20approaches%20suffer%20from%20limited%20knowledge%20diversity%2C%20slow%20convergence%2C%20and%20difficulty%20in%20balancing%20robustness%20and%20accuracy.%20To%20address%20these%20challenges%2C%20we%20propose%20MMT-ARD%3A%20a%20Multimodal%20Multi-Teacher%20Adversarial%20Robust%20Distillation%20framework.%20Our%20key%20innovation%20is%20a%20dual-teacher%20knowledge%20fusion%20architecture%20that%20collaboratively%20optimizes%20clean%20feature%20preservation%20and%20robust%20feature%20enhancement.%20To%20better%20handle%20challenging%20adversarial%20examples%2C%20we%20introduce%20a%20dynamic%20weight%20allocation%20strategy%20based%20on%20teacher%20confidence%2C%20enabling%20adaptive%20focus%20on%20harder%20samples.%20Moreover%2C%20to%20mitigate%20bias%20among%20teachers%2C%20we%20design%20an%20adaptive%20sigmoid-based%20weighting%20function%20that%20balances%20the%20strength%20of%20knowledge%20transfer%20across%20modalities.%20Extensive%20experiments%20on%20ImageNet%20and%20zero-shot%20benchmarks%20demonstrate%20that%20MMT-ARD%20improves%20robust%20accuracy%20by%20%2B4.32%25%20and%20zero-shot%20accuracy%20by%20%2B3.5%25%20on%20the%20ViT-B-32%20model%2C%20while%20achieving%20a%202.3x%20increase%20in%20training%20efficiency%20over%20traditional%20single-teacher%20methods.%20These%20results%20highlight%20the%20effectiveness%20and%20scalability%20of%20MMT-ARD%20in%20enhancing%20the%20adversarial%20robustness%20of%20multimodal%20large%20models.%20Our%20codes%20are%20available%20at%20https%3A//github.com/itsnotacie/MMT-ARD.&entry.1838667208=http%3A//arxiv.org/abs/2511.17448v1&entry.124074799=Read"},
{"title": "Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination", "author": "Yolo Yunlong Tang and Daiki Shimada and Hang Hua and Chao Huang and Jing Bi and Rogerio Feris and Chenliang Xu", "abstract": "Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning.", "link": "http://arxiv.org/abs/2511.17490v1", "date": "2025-11-21", "relevancy": 2.2076, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5526}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-R4%3A%20Reinforcing%20Text-Rich%20Video%20Reasoning%20with%20Visual%20Rumination&body=Title%3A%20Video-R4%3A%20Reinforcing%20Text-Rich%20Video%20Reasoning%20with%20Visual%20Rumination%0AAuthor%3A%20Yolo%20Yunlong%20Tang%20and%20Daiki%20Shimada%20and%20Hang%20Hua%20and%20Chao%20Huang%20and%20Jing%20Bi%20and%20Rogerio%20Feris%20and%20Chenliang%20Xu%0AAbstract%3A%20Understanding%20text-rich%20videos%20requires%20reading%20small%2C%20transient%20textual%20cues%20that%20often%20demand%20repeated%20inspection.%20Yet%20most%20video%20QA%20models%20rely%20on%20single-pass%20perception%20over%20fixed%20frames%2C%20leading%20to%20hallucinations%20and%20failures%20on%20fine-grained%20evidence.%20Inspired%20by%20how%20humans%20pause%2C%20zoom%2C%20and%20re-read%20critical%20regions%2C%20we%20introduce%20Video-R4%20%28Reinforcing%20Text-Rich%20Video%20Reasoning%20with%20Visual%20Rumination%29%2C%20a%20video%20reasoning%20LMM%20that%20performs%20visual%20rumination%3A%20iteratively%20selecting%20frames%2C%20zooming%20into%20informative%20regions%2C%20re-encoding%20retrieved%20pixels%2C%20and%20updating%20its%20reasoning%20state.%20We%20construct%20two%20datasets%20with%20executable%20rumination%20trajectories%3A%20Video-R4-CoT-17k%20for%20supervised%20practice%20and%20Video-R4-RL-30k%20for%20reinforcement%20learning.%20We%20propose%20a%20multi-stage%20rumination%20learning%20framework%20that%20progressively%20finetunes%20a%207B%20LMM%20to%20learn%20atomic%20and%20mixing%20visual%20operations%20via%20SFT%20and%20GRPO-based%20RL.%20Video-R4-7B%20achieves%20state-of-the-art%20results%20on%20M4-ViteVQA%20and%20further%20generalizes%20to%20multi-page%20document%20QA%2C%20slides%20QA%2C%20and%20generic%20video%20QA%2C%20demonstrating%20that%20iterative%20rumination%20is%20an%20effective%20paradigm%20for%20pixel-grounded%20multimodal%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-R4%253A%2520Reinforcing%2520Text-Rich%2520Video%2520Reasoning%2520with%2520Visual%2520Rumination%26entry.906535625%3DYolo%2520Yunlong%2520Tang%2520and%2520Daiki%2520Shimada%2520and%2520Hang%2520Hua%2520and%2520Chao%2520Huang%2520and%2520Jing%2520Bi%2520and%2520Rogerio%2520Feris%2520and%2520Chenliang%2520Xu%26entry.1292438233%3DUnderstanding%2520text-rich%2520videos%2520requires%2520reading%2520small%252C%2520transient%2520textual%2520cues%2520that%2520often%2520demand%2520repeated%2520inspection.%2520Yet%2520most%2520video%2520QA%2520models%2520rely%2520on%2520single-pass%2520perception%2520over%2520fixed%2520frames%252C%2520leading%2520to%2520hallucinations%2520and%2520failures%2520on%2520fine-grained%2520evidence.%2520Inspired%2520by%2520how%2520humans%2520pause%252C%2520zoom%252C%2520and%2520re-read%2520critical%2520regions%252C%2520we%2520introduce%2520Video-R4%2520%2528Reinforcing%2520Text-Rich%2520Video%2520Reasoning%2520with%2520Visual%2520Rumination%2529%252C%2520a%2520video%2520reasoning%2520LMM%2520that%2520performs%2520visual%2520rumination%253A%2520iteratively%2520selecting%2520frames%252C%2520zooming%2520into%2520informative%2520regions%252C%2520re-encoding%2520retrieved%2520pixels%252C%2520and%2520updating%2520its%2520reasoning%2520state.%2520We%2520construct%2520two%2520datasets%2520with%2520executable%2520rumination%2520trajectories%253A%2520Video-R4-CoT-17k%2520for%2520supervised%2520practice%2520and%2520Video-R4-RL-30k%2520for%2520reinforcement%2520learning.%2520We%2520propose%2520a%2520multi-stage%2520rumination%2520learning%2520framework%2520that%2520progressively%2520finetunes%2520a%25207B%2520LMM%2520to%2520learn%2520atomic%2520and%2520mixing%2520visual%2520operations%2520via%2520SFT%2520and%2520GRPO-based%2520RL.%2520Video-R4-7B%2520achieves%2520state-of-the-art%2520results%2520on%2520M4-ViteVQA%2520and%2520further%2520generalizes%2520to%2520multi-page%2520document%2520QA%252C%2520slides%2520QA%252C%2520and%2520generic%2520video%2520QA%252C%2520demonstrating%2520that%2520iterative%2520rumination%2520is%2520an%2520effective%2520paradigm%2520for%2520pixel-grounded%2520multimodal%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-R4%3A%20Reinforcing%20Text-Rich%20Video%20Reasoning%20with%20Visual%20Rumination&entry.906535625=Yolo%20Yunlong%20Tang%20and%20Daiki%20Shimada%20and%20Hang%20Hua%20and%20Chao%20Huang%20and%20Jing%20Bi%20and%20Rogerio%20Feris%20and%20Chenliang%20Xu&entry.1292438233=Understanding%20text-rich%20videos%20requires%20reading%20small%2C%20transient%20textual%20cues%20that%20often%20demand%20repeated%20inspection.%20Yet%20most%20video%20QA%20models%20rely%20on%20single-pass%20perception%20over%20fixed%20frames%2C%20leading%20to%20hallucinations%20and%20failures%20on%20fine-grained%20evidence.%20Inspired%20by%20how%20humans%20pause%2C%20zoom%2C%20and%20re-read%20critical%20regions%2C%20we%20introduce%20Video-R4%20%28Reinforcing%20Text-Rich%20Video%20Reasoning%20with%20Visual%20Rumination%29%2C%20a%20video%20reasoning%20LMM%20that%20performs%20visual%20rumination%3A%20iteratively%20selecting%20frames%2C%20zooming%20into%20informative%20regions%2C%20re-encoding%20retrieved%20pixels%2C%20and%20updating%20its%20reasoning%20state.%20We%20construct%20two%20datasets%20with%20executable%20rumination%20trajectories%3A%20Video-R4-CoT-17k%20for%20supervised%20practice%20and%20Video-R4-RL-30k%20for%20reinforcement%20learning.%20We%20propose%20a%20multi-stage%20rumination%20learning%20framework%20that%20progressively%20finetunes%20a%207B%20LMM%20to%20learn%20atomic%20and%20mixing%20visual%20operations%20via%20SFT%20and%20GRPO-based%20RL.%20Video-R4-7B%20achieves%20state-of-the-art%20results%20on%20M4-ViteVQA%20and%20further%20generalizes%20to%20multi-page%20document%20QA%2C%20slides%20QA%2C%20and%20generic%20video%20QA%2C%20demonstrating%20that%20iterative%20rumination%20is%20an%20effective%20paradigm%20for%20pixel-grounded%20multimodal%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2511.17490v1&entry.124074799=Read"},
{"title": "MorphSeek: Fine-grained Latent Representation-Level Policy Optimization for Deformable Image Registration", "author": "Runxun Zhang and Yizhou Liu and Li Dongrui and Bo XU and Jingwei Wei", "abstract": "Deformable image registration (DIR) remains a fundamental yet challenging problem in medical image analysis, largely due to the prohibitively high-dimensional deformation space of dense displacement fields and the scarcity of voxel-level supervision. Existing reinforcement learning frameworks often project this space into coarse, low-dimensional representations, limiting their ability to capture spatially variant deformations. We propose MorphSeek, a fine-grained representation-level policy optimization paradigm that reformulates DIR as a spatially continuous optimization process in the latent feature space. MorphSeek introduces a stochastic Gaussian policy head atop the encoder to model a distribution over latent features, facilitating efficient exploration and coarse-to-fine refinement. The framework integrates unsupervised warm-up with weakly supervised fine-tuning through Group Relative Policy Optimization, where multi-trajectory sampling stabilizes training and improves label efficiency. Across three 3D registration benchmarks (OASIS brain MRI, LiTS liver CT, and Abdomen MR-CT), MorphSeek achieves consistent Dice improvements over competitive baselines while maintaining high label efficiency with minimal parameter cost and low step-level latency overhead. Beyond optimizer specifics, MorphSeek advances a representation-level policy learning paradigm that achieves spatially coherent and data-efficient deformation optimization, offering a principled, backbone-agnostic, and optimizer-agnostic solution for scalable visual alignment in high-dimensional settings.", "link": "http://arxiv.org/abs/2511.17392v1", "date": "2025-11-21", "relevancy": 2.1975, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5638}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5396}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MorphSeek%3A%20Fine-grained%20Latent%20Representation-Level%20Policy%20Optimization%20for%20Deformable%20Image%20Registration&body=Title%3A%20MorphSeek%3A%20Fine-grained%20Latent%20Representation-Level%20Policy%20Optimization%20for%20Deformable%20Image%20Registration%0AAuthor%3A%20Runxun%20Zhang%20and%20Yizhou%20Liu%20and%20Li%20Dongrui%20and%20Bo%20XU%20and%20Jingwei%20Wei%0AAbstract%3A%20Deformable%20image%20registration%20%28DIR%29%20remains%20a%20fundamental%20yet%20challenging%20problem%20in%20medical%20image%20analysis%2C%20largely%20due%20to%20the%20prohibitively%20high-dimensional%20deformation%20space%20of%20dense%20displacement%20fields%20and%20the%20scarcity%20of%20voxel-level%20supervision.%20Existing%20reinforcement%20learning%20frameworks%20often%20project%20this%20space%20into%20coarse%2C%20low-dimensional%20representations%2C%20limiting%20their%20ability%20to%20capture%20spatially%20variant%20deformations.%20We%20propose%20MorphSeek%2C%20a%20fine-grained%20representation-level%20policy%20optimization%20paradigm%20that%20reformulates%20DIR%20as%20a%20spatially%20continuous%20optimization%20process%20in%20the%20latent%20feature%20space.%20MorphSeek%20introduces%20a%20stochastic%20Gaussian%20policy%20head%20atop%20the%20encoder%20to%20model%20a%20distribution%20over%20latent%20features%2C%20facilitating%20efficient%20exploration%20and%20coarse-to-fine%20refinement.%20The%20framework%20integrates%20unsupervised%20warm-up%20with%20weakly%20supervised%20fine-tuning%20through%20Group%20Relative%20Policy%20Optimization%2C%20where%20multi-trajectory%20sampling%20stabilizes%20training%20and%20improves%20label%20efficiency.%20Across%20three%203D%20registration%20benchmarks%20%28OASIS%20brain%20MRI%2C%20LiTS%20liver%20CT%2C%20and%20Abdomen%20MR-CT%29%2C%20MorphSeek%20achieves%20consistent%20Dice%20improvements%20over%20competitive%20baselines%20while%20maintaining%20high%20label%20efficiency%20with%20minimal%20parameter%20cost%20and%20low%20step-level%20latency%20overhead.%20Beyond%20optimizer%20specifics%2C%20MorphSeek%20advances%20a%20representation-level%20policy%20learning%20paradigm%20that%20achieves%20spatially%20coherent%20and%20data-efficient%20deformation%20optimization%2C%20offering%20a%20principled%2C%20backbone-agnostic%2C%20and%20optimizer-agnostic%20solution%20for%20scalable%20visual%20alignment%20in%20high-dimensional%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMorphSeek%253A%2520Fine-grained%2520Latent%2520Representation-Level%2520Policy%2520Optimization%2520for%2520Deformable%2520Image%2520Registration%26entry.906535625%3DRunxun%2520Zhang%2520and%2520Yizhou%2520Liu%2520and%2520Li%2520Dongrui%2520and%2520Bo%2520XU%2520and%2520Jingwei%2520Wei%26entry.1292438233%3DDeformable%2520image%2520registration%2520%2528DIR%2529%2520remains%2520a%2520fundamental%2520yet%2520challenging%2520problem%2520in%2520medical%2520image%2520analysis%252C%2520largely%2520due%2520to%2520the%2520prohibitively%2520high-dimensional%2520deformation%2520space%2520of%2520dense%2520displacement%2520fields%2520and%2520the%2520scarcity%2520of%2520voxel-level%2520supervision.%2520Existing%2520reinforcement%2520learning%2520frameworks%2520often%2520project%2520this%2520space%2520into%2520coarse%252C%2520low-dimensional%2520representations%252C%2520limiting%2520their%2520ability%2520to%2520capture%2520spatially%2520variant%2520deformations.%2520We%2520propose%2520MorphSeek%252C%2520a%2520fine-grained%2520representation-level%2520policy%2520optimization%2520paradigm%2520that%2520reformulates%2520DIR%2520as%2520a%2520spatially%2520continuous%2520optimization%2520process%2520in%2520the%2520latent%2520feature%2520space.%2520MorphSeek%2520introduces%2520a%2520stochastic%2520Gaussian%2520policy%2520head%2520atop%2520the%2520encoder%2520to%2520model%2520a%2520distribution%2520over%2520latent%2520features%252C%2520facilitating%2520efficient%2520exploration%2520and%2520coarse-to-fine%2520refinement.%2520The%2520framework%2520integrates%2520unsupervised%2520warm-up%2520with%2520weakly%2520supervised%2520fine-tuning%2520through%2520Group%2520Relative%2520Policy%2520Optimization%252C%2520where%2520multi-trajectory%2520sampling%2520stabilizes%2520training%2520and%2520improves%2520label%2520efficiency.%2520Across%2520three%25203D%2520registration%2520benchmarks%2520%2528OASIS%2520brain%2520MRI%252C%2520LiTS%2520liver%2520CT%252C%2520and%2520Abdomen%2520MR-CT%2529%252C%2520MorphSeek%2520achieves%2520consistent%2520Dice%2520improvements%2520over%2520competitive%2520baselines%2520while%2520maintaining%2520high%2520label%2520efficiency%2520with%2520minimal%2520parameter%2520cost%2520and%2520low%2520step-level%2520latency%2520overhead.%2520Beyond%2520optimizer%2520specifics%252C%2520MorphSeek%2520advances%2520a%2520representation-level%2520policy%2520learning%2520paradigm%2520that%2520achieves%2520spatially%2520coherent%2520and%2520data-efficient%2520deformation%2520optimization%252C%2520offering%2520a%2520principled%252C%2520backbone-agnostic%252C%2520and%2520optimizer-agnostic%2520solution%2520for%2520scalable%2520visual%2520alignment%2520in%2520high-dimensional%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MorphSeek%3A%20Fine-grained%20Latent%20Representation-Level%20Policy%20Optimization%20for%20Deformable%20Image%20Registration&entry.906535625=Runxun%20Zhang%20and%20Yizhou%20Liu%20and%20Li%20Dongrui%20and%20Bo%20XU%20and%20Jingwei%20Wei&entry.1292438233=Deformable%20image%20registration%20%28DIR%29%20remains%20a%20fundamental%20yet%20challenging%20problem%20in%20medical%20image%20analysis%2C%20largely%20due%20to%20the%20prohibitively%20high-dimensional%20deformation%20space%20of%20dense%20displacement%20fields%20and%20the%20scarcity%20of%20voxel-level%20supervision.%20Existing%20reinforcement%20learning%20frameworks%20often%20project%20this%20space%20into%20coarse%2C%20low-dimensional%20representations%2C%20limiting%20their%20ability%20to%20capture%20spatially%20variant%20deformations.%20We%20propose%20MorphSeek%2C%20a%20fine-grained%20representation-level%20policy%20optimization%20paradigm%20that%20reformulates%20DIR%20as%20a%20spatially%20continuous%20optimization%20process%20in%20the%20latent%20feature%20space.%20MorphSeek%20introduces%20a%20stochastic%20Gaussian%20policy%20head%20atop%20the%20encoder%20to%20model%20a%20distribution%20over%20latent%20features%2C%20facilitating%20efficient%20exploration%20and%20coarse-to-fine%20refinement.%20The%20framework%20integrates%20unsupervised%20warm-up%20with%20weakly%20supervised%20fine-tuning%20through%20Group%20Relative%20Policy%20Optimization%2C%20where%20multi-trajectory%20sampling%20stabilizes%20training%20and%20improves%20label%20efficiency.%20Across%20three%203D%20registration%20benchmarks%20%28OASIS%20brain%20MRI%2C%20LiTS%20liver%20CT%2C%20and%20Abdomen%20MR-CT%29%2C%20MorphSeek%20achieves%20consistent%20Dice%20improvements%20over%20competitive%20baselines%20while%20maintaining%20high%20label%20efficiency%20with%20minimal%20parameter%20cost%20and%20low%20step-level%20latency%20overhead.%20Beyond%20optimizer%20specifics%2C%20MorphSeek%20advances%20a%20representation-level%20policy%20learning%20paradigm%20that%20achieves%20spatially%20coherent%20and%20data-efficient%20deformation%20optimization%2C%20offering%20a%20principled%2C%20backbone-agnostic%2C%20and%20optimizer-agnostic%20solution%20for%20scalable%20visual%20alignment%20in%20high-dimensional%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2511.17392v1&entry.124074799=Read"},
{"title": "ATAC: Augmentation-Based Test-Time Adversarial Correction for CLIP", "author": "Linxiang Su and Andr\u00e1s Balogh", "abstract": "Despite its remarkable success in zero-shot image-text matching, CLIP remains highly vulnerable to adversarial perturbations on images. As adversarial fine-tuning is prohibitively costly, recent works explore various test-time defense strategies; however, these approaches still exhibit limited robustness. In this work, we revisit this problem and propose a simple yet effective strategy: Augmentation-based Test-time Adversarial Correction (ATAC). Our method operates directly in the embedding space of CLIP, calculating augmentation-induced drift vectors to infer a semantic recovery direction and correcting the embedding based on the angular consistency of these latent drifts. Across a wide range of benchmarks, ATAC consistently achieves remarkably high robustness, surpassing that of previous state-of-the-art methods by nearly 50\\% on average, all while requiring minimal computational overhead. Furthermore, ATAC retains state-of-the-art robustness in unconventional and extreme settings and even achieves nontrivial robustness against adaptive attacks. Our results demonstrate that ATAC is an efficient method in a novel paradigm for test-time adversarial defenses in the embedding space of CLIP.", "link": "http://arxiv.org/abs/2511.17362v1", "date": "2025-11-21", "relevancy": 2.1892, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5768}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5334}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ATAC%3A%20Augmentation-Based%20Test-Time%20Adversarial%20Correction%20for%20CLIP&body=Title%3A%20ATAC%3A%20Augmentation-Based%20Test-Time%20Adversarial%20Correction%20for%20CLIP%0AAuthor%3A%20Linxiang%20Su%20and%20Andr%C3%A1s%20Balogh%0AAbstract%3A%20Despite%20its%20remarkable%20success%20in%20zero-shot%20image-text%20matching%2C%20CLIP%20remains%20highly%20vulnerable%20to%20adversarial%20perturbations%20on%20images.%20As%20adversarial%20fine-tuning%20is%20prohibitively%20costly%2C%20recent%20works%20explore%20various%20test-time%20defense%20strategies%3B%20however%2C%20these%20approaches%20still%20exhibit%20limited%20robustness.%20In%20this%20work%2C%20we%20revisit%20this%20problem%20and%20propose%20a%20simple%20yet%20effective%20strategy%3A%20Augmentation-based%20Test-time%20Adversarial%20Correction%20%28ATAC%29.%20Our%20method%20operates%20directly%20in%20the%20embedding%20space%20of%20CLIP%2C%20calculating%20augmentation-induced%20drift%20vectors%20to%20infer%20a%20semantic%20recovery%20direction%20and%20correcting%20the%20embedding%20based%20on%20the%20angular%20consistency%20of%20these%20latent%20drifts.%20Across%20a%20wide%20range%20of%20benchmarks%2C%20ATAC%20consistently%20achieves%20remarkably%20high%20robustness%2C%20surpassing%20that%20of%20previous%20state-of-the-art%20methods%20by%20nearly%2050%5C%25%20on%20average%2C%20all%20while%20requiring%20minimal%20computational%20overhead.%20Furthermore%2C%20ATAC%20retains%20state-of-the-art%20robustness%20in%20unconventional%20and%20extreme%20settings%20and%20even%20achieves%20nontrivial%20robustness%20against%20adaptive%20attacks.%20Our%20results%20demonstrate%20that%20ATAC%20is%20an%20efficient%20method%20in%20a%20novel%20paradigm%20for%20test-time%20adversarial%20defenses%20in%20the%20embedding%20space%20of%20CLIP.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17362v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DATAC%253A%2520Augmentation-Based%2520Test-Time%2520Adversarial%2520Correction%2520for%2520CLIP%26entry.906535625%3DLinxiang%2520Su%2520and%2520Andr%25C3%25A1s%2520Balogh%26entry.1292438233%3DDespite%2520its%2520remarkable%2520success%2520in%2520zero-shot%2520image-text%2520matching%252C%2520CLIP%2520remains%2520highly%2520vulnerable%2520to%2520adversarial%2520perturbations%2520on%2520images.%2520As%2520adversarial%2520fine-tuning%2520is%2520prohibitively%2520costly%252C%2520recent%2520works%2520explore%2520various%2520test-time%2520defense%2520strategies%253B%2520however%252C%2520these%2520approaches%2520still%2520exhibit%2520limited%2520robustness.%2520In%2520this%2520work%252C%2520we%2520revisit%2520this%2520problem%2520and%2520propose%2520a%2520simple%2520yet%2520effective%2520strategy%253A%2520Augmentation-based%2520Test-time%2520Adversarial%2520Correction%2520%2528ATAC%2529.%2520Our%2520method%2520operates%2520directly%2520in%2520the%2520embedding%2520space%2520of%2520CLIP%252C%2520calculating%2520augmentation-induced%2520drift%2520vectors%2520to%2520infer%2520a%2520semantic%2520recovery%2520direction%2520and%2520correcting%2520the%2520embedding%2520based%2520on%2520the%2520angular%2520consistency%2520of%2520these%2520latent%2520drifts.%2520Across%2520a%2520wide%2520range%2520of%2520benchmarks%252C%2520ATAC%2520consistently%2520achieves%2520remarkably%2520high%2520robustness%252C%2520surpassing%2520that%2520of%2520previous%2520state-of-the-art%2520methods%2520by%2520nearly%252050%255C%2525%2520on%2520average%252C%2520all%2520while%2520requiring%2520minimal%2520computational%2520overhead.%2520Furthermore%252C%2520ATAC%2520retains%2520state-of-the-art%2520robustness%2520in%2520unconventional%2520and%2520extreme%2520settings%2520and%2520even%2520achieves%2520nontrivial%2520robustness%2520against%2520adaptive%2520attacks.%2520Our%2520results%2520demonstrate%2520that%2520ATAC%2520is%2520an%2520efficient%2520method%2520in%2520a%2520novel%2520paradigm%2520for%2520test-time%2520adversarial%2520defenses%2520in%2520the%2520embedding%2520space%2520of%2520CLIP.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17362v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ATAC%3A%20Augmentation-Based%20Test-Time%20Adversarial%20Correction%20for%20CLIP&entry.906535625=Linxiang%20Su%20and%20Andr%C3%A1s%20Balogh&entry.1292438233=Despite%20its%20remarkable%20success%20in%20zero-shot%20image-text%20matching%2C%20CLIP%20remains%20highly%20vulnerable%20to%20adversarial%20perturbations%20on%20images.%20As%20adversarial%20fine-tuning%20is%20prohibitively%20costly%2C%20recent%20works%20explore%20various%20test-time%20defense%20strategies%3B%20however%2C%20these%20approaches%20still%20exhibit%20limited%20robustness.%20In%20this%20work%2C%20we%20revisit%20this%20problem%20and%20propose%20a%20simple%20yet%20effective%20strategy%3A%20Augmentation-based%20Test-time%20Adversarial%20Correction%20%28ATAC%29.%20Our%20method%20operates%20directly%20in%20the%20embedding%20space%20of%20CLIP%2C%20calculating%20augmentation-induced%20drift%20vectors%20to%20infer%20a%20semantic%20recovery%20direction%20and%20correcting%20the%20embedding%20based%20on%20the%20angular%20consistency%20of%20these%20latent%20drifts.%20Across%20a%20wide%20range%20of%20benchmarks%2C%20ATAC%20consistently%20achieves%20remarkably%20high%20robustness%2C%20surpassing%20that%20of%20previous%20state-of-the-art%20methods%20by%20nearly%2050%5C%25%20on%20average%2C%20all%20while%20requiring%20minimal%20computational%20overhead.%20Furthermore%2C%20ATAC%20retains%20state-of-the-art%20robustness%20in%20unconventional%20and%20extreme%20settings%20and%20even%20achieves%20nontrivial%20robustness%20against%20adaptive%20attacks.%20Our%20results%20demonstrate%20that%20ATAC%20is%20an%20efficient%20method%20in%20a%20novel%20paradigm%20for%20test-time%20adversarial%20defenses%20in%20the%20embedding%20space%20of%20CLIP.&entry.1838667208=http%3A//arxiv.org/abs/2511.17362v1&entry.124074799=Read"},
{"title": "How LLMs Learn to Reason: A Complex Network Perspective", "author": "Sihan Hu and Xiansheng Cai and Yuan Huang and Zhiyuan Yao and Linfeng Zhang and Pan Zhang and Youjin Deng and Kun Chen", "abstract": "Training large language models with Reinforcement Learning with Verifiable Rewards (RLVR) exhibits a set of distinctive and puzzling behaviors that remain poorly understood, including a two-stage learning curve, a V-shaped response-length trajectory, and a pronounced vulnerability to catastrophic forgetting. In this work, we propose that these behaviors are emergent collective phenomena governed not by neural implementation details, but by the topological evolution of the latent reasoning graph in semantic space. By demonstrating a dynamical isomorphism between a 1.5B-parameter LLM and a minimal Concept Network Model (CoNet), we trace the causal source to the self-organization of a sparse concept web pinned to an average degree of two. This geometric perspective provides a unified physical explanation for the observed anomalies: the V-shaped trajectory tracks the evolution from parallel local skill optimization to global network integration; catastrophic forgetting stems from the topological disconnection of critical ``trunk'' edges; and policy collapse arises from the accumulation of sequential transitions at the web's leaf nodes, where broad exploration abruptly freezes into rigid, high-reward trajectories. Identifying a ``maximally frustrated state'' at the transition between learning stages, we propose Annealed-RLVR, a principled algorithm that injects a targeted SFT ``heating'' step to resolve this topological bottleneck. Experiments confirm that this theory-driven intervention outperforms standard RLVR on both in-distribution and out-of-distribution benchmarks (including Minerva and AIME). By recasting RLVR from black-box optimization into a predictable process of structural self-organization, our work provides a new physical intuition for engineering the emergent reasoning capabilities of future AI systems.", "link": "http://arxiv.org/abs/2509.23629v2", "date": "2025-11-21", "relevancy": 2.1804, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5486}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20LLMs%20Learn%20to%20Reason%3A%20A%20Complex%20Network%20Perspective&body=Title%3A%20How%20LLMs%20Learn%20to%20Reason%3A%20A%20Complex%20Network%20Perspective%0AAuthor%3A%20Sihan%20Hu%20and%20Xiansheng%20Cai%20and%20Yuan%20Huang%20and%20Zhiyuan%20Yao%20and%20Linfeng%20Zhang%20and%20Pan%20Zhang%20and%20Youjin%20Deng%20and%20Kun%20Chen%0AAbstract%3A%20Training%20large%20language%20models%20with%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20exhibits%20a%20set%20of%20distinctive%20and%20puzzling%20behaviors%20that%20remain%20poorly%20understood%2C%20including%20a%20two-stage%20learning%20curve%2C%20a%20V-shaped%20response-length%20trajectory%2C%20and%20a%20pronounced%20vulnerability%20to%20catastrophic%20forgetting.%20In%20this%20work%2C%20we%20propose%20that%20these%20behaviors%20are%20emergent%20collective%20phenomena%20governed%20not%20by%20neural%20implementation%20details%2C%20but%20by%20the%20topological%20evolution%20of%20the%20latent%20reasoning%20graph%20in%20semantic%20space.%20By%20demonstrating%20a%20dynamical%20isomorphism%20between%20a%201.5B-parameter%20LLM%20and%20a%20minimal%20Concept%20Network%20Model%20%28CoNet%29%2C%20we%20trace%20the%20causal%20source%20to%20the%20self-organization%20of%20a%20sparse%20concept%20web%20pinned%20to%20an%20average%20degree%20of%20two.%20This%20geometric%20perspective%20provides%20a%20unified%20physical%20explanation%20for%20the%20observed%20anomalies%3A%20the%20V-shaped%20trajectory%20tracks%20the%20evolution%20from%20parallel%20local%20skill%20optimization%20to%20global%20network%20integration%3B%20catastrophic%20forgetting%20stems%20from%20the%20topological%20disconnection%20of%20critical%20%60%60trunk%27%27%20edges%3B%20and%20policy%20collapse%20arises%20from%20the%20accumulation%20of%20sequential%20transitions%20at%20the%20web%27s%20leaf%20nodes%2C%20where%20broad%20exploration%20abruptly%20freezes%20into%20rigid%2C%20high-reward%20trajectories.%20Identifying%20a%20%60%60maximally%20frustrated%20state%27%27%20at%20the%20transition%20between%20learning%20stages%2C%20we%20propose%20Annealed-RLVR%2C%20a%20principled%20algorithm%20that%20injects%20a%20targeted%20SFT%20%60%60heating%27%27%20step%20to%20resolve%20this%20topological%20bottleneck.%20Experiments%20confirm%20that%20this%20theory-driven%20intervention%20outperforms%20standard%20RLVR%20on%20both%20in-distribution%20and%20out-of-distribution%20benchmarks%20%28including%20Minerva%20and%20AIME%29.%20By%20recasting%20RLVR%20from%20black-box%20optimization%20into%20a%20predictable%20process%20of%20structural%20self-organization%2C%20our%20work%20provides%20a%20new%20physical%20intuition%20for%20engineering%20the%20emergent%20reasoning%20capabilities%20of%20future%20AI%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2509.23629v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520LLMs%2520Learn%2520to%2520Reason%253A%2520A%2520Complex%2520Network%2520Perspective%26entry.906535625%3DSihan%2520Hu%2520and%2520Xiansheng%2520Cai%2520and%2520Yuan%2520Huang%2520and%2520Zhiyuan%2520Yao%2520and%2520Linfeng%2520Zhang%2520and%2520Pan%2520Zhang%2520and%2520Youjin%2520Deng%2520and%2520Kun%2520Chen%26entry.1292438233%3DTraining%2520large%2520language%2520models%2520with%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520exhibits%2520a%2520set%2520of%2520distinctive%2520and%2520puzzling%2520behaviors%2520that%2520remain%2520poorly%2520understood%252C%2520including%2520a%2520two-stage%2520learning%2520curve%252C%2520a%2520V-shaped%2520response-length%2520trajectory%252C%2520and%2520a%2520pronounced%2520vulnerability%2520to%2520catastrophic%2520forgetting.%2520In%2520this%2520work%252C%2520we%2520propose%2520that%2520these%2520behaviors%2520are%2520emergent%2520collective%2520phenomena%2520governed%2520not%2520by%2520neural%2520implementation%2520details%252C%2520but%2520by%2520the%2520topological%2520evolution%2520of%2520the%2520latent%2520reasoning%2520graph%2520in%2520semantic%2520space.%2520By%2520demonstrating%2520a%2520dynamical%2520isomorphism%2520between%2520a%25201.5B-parameter%2520LLM%2520and%2520a%2520minimal%2520Concept%2520Network%2520Model%2520%2528CoNet%2529%252C%2520we%2520trace%2520the%2520causal%2520source%2520to%2520the%2520self-organization%2520of%2520a%2520sparse%2520concept%2520web%2520pinned%2520to%2520an%2520average%2520degree%2520of%2520two.%2520This%2520geometric%2520perspective%2520provides%2520a%2520unified%2520physical%2520explanation%2520for%2520the%2520observed%2520anomalies%253A%2520the%2520V-shaped%2520trajectory%2520tracks%2520the%2520evolution%2520from%2520parallel%2520local%2520skill%2520optimization%2520to%2520global%2520network%2520integration%253B%2520catastrophic%2520forgetting%2520stems%2520from%2520the%2520topological%2520disconnection%2520of%2520critical%2520%2560%2560trunk%2527%2527%2520edges%253B%2520and%2520policy%2520collapse%2520arises%2520from%2520the%2520accumulation%2520of%2520sequential%2520transitions%2520at%2520the%2520web%2527s%2520leaf%2520nodes%252C%2520where%2520broad%2520exploration%2520abruptly%2520freezes%2520into%2520rigid%252C%2520high-reward%2520trajectories.%2520Identifying%2520a%2520%2560%2560maximally%2520frustrated%2520state%2527%2527%2520at%2520the%2520transition%2520between%2520learning%2520stages%252C%2520we%2520propose%2520Annealed-RLVR%252C%2520a%2520principled%2520algorithm%2520that%2520injects%2520a%2520targeted%2520SFT%2520%2560%2560heating%2527%2527%2520step%2520to%2520resolve%2520this%2520topological%2520bottleneck.%2520Experiments%2520confirm%2520that%2520this%2520theory-driven%2520intervention%2520outperforms%2520standard%2520RLVR%2520on%2520both%2520in-distribution%2520and%2520out-of-distribution%2520benchmarks%2520%2528including%2520Minerva%2520and%2520AIME%2529.%2520By%2520recasting%2520RLVR%2520from%2520black-box%2520optimization%2520into%2520a%2520predictable%2520process%2520of%2520structural%2520self-organization%252C%2520our%2520work%2520provides%2520a%2520new%2520physical%2520intuition%2520for%2520engineering%2520the%2520emergent%2520reasoning%2520capabilities%2520of%2520future%2520AI%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23629v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20LLMs%20Learn%20to%20Reason%3A%20A%20Complex%20Network%20Perspective&entry.906535625=Sihan%20Hu%20and%20Xiansheng%20Cai%20and%20Yuan%20Huang%20and%20Zhiyuan%20Yao%20and%20Linfeng%20Zhang%20and%20Pan%20Zhang%20and%20Youjin%20Deng%20and%20Kun%20Chen&entry.1292438233=Training%20large%20language%20models%20with%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20exhibits%20a%20set%20of%20distinctive%20and%20puzzling%20behaviors%20that%20remain%20poorly%20understood%2C%20including%20a%20two-stage%20learning%20curve%2C%20a%20V-shaped%20response-length%20trajectory%2C%20and%20a%20pronounced%20vulnerability%20to%20catastrophic%20forgetting.%20In%20this%20work%2C%20we%20propose%20that%20these%20behaviors%20are%20emergent%20collective%20phenomena%20governed%20not%20by%20neural%20implementation%20details%2C%20but%20by%20the%20topological%20evolution%20of%20the%20latent%20reasoning%20graph%20in%20semantic%20space.%20By%20demonstrating%20a%20dynamical%20isomorphism%20between%20a%201.5B-parameter%20LLM%20and%20a%20minimal%20Concept%20Network%20Model%20%28CoNet%29%2C%20we%20trace%20the%20causal%20source%20to%20the%20self-organization%20of%20a%20sparse%20concept%20web%20pinned%20to%20an%20average%20degree%20of%20two.%20This%20geometric%20perspective%20provides%20a%20unified%20physical%20explanation%20for%20the%20observed%20anomalies%3A%20the%20V-shaped%20trajectory%20tracks%20the%20evolution%20from%20parallel%20local%20skill%20optimization%20to%20global%20network%20integration%3B%20catastrophic%20forgetting%20stems%20from%20the%20topological%20disconnection%20of%20critical%20%60%60trunk%27%27%20edges%3B%20and%20policy%20collapse%20arises%20from%20the%20accumulation%20of%20sequential%20transitions%20at%20the%20web%27s%20leaf%20nodes%2C%20where%20broad%20exploration%20abruptly%20freezes%20into%20rigid%2C%20high-reward%20trajectories.%20Identifying%20a%20%60%60maximally%20frustrated%20state%27%27%20at%20the%20transition%20between%20learning%20stages%2C%20we%20propose%20Annealed-RLVR%2C%20a%20principled%20algorithm%20that%20injects%20a%20targeted%20SFT%20%60%60heating%27%27%20step%20to%20resolve%20this%20topological%20bottleneck.%20Experiments%20confirm%20that%20this%20theory-driven%20intervention%20outperforms%20standard%20RLVR%20on%20both%20in-distribution%20and%20out-of-distribution%20benchmarks%20%28including%20Minerva%20and%20AIME%29.%20By%20recasting%20RLVR%20from%20black-box%20optimization%20into%20a%20predictable%20process%20of%20structural%20self-organization%2C%20our%20work%20provides%20a%20new%20physical%20intuition%20for%20engineering%20the%20emergent%20reasoning%20capabilities%20of%20future%20AI%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2509.23629v2&entry.124074799=Read"},
{"title": "DSeq-JEPA: Discriminative Sequential Joint-Embedding Predictive Architecture", "author": "Xiangteng He and Shunsuke Sakai and Kun Yuan and Nicolas Padoy and Tatsuhito Hasegawa and Leonid Sigal", "abstract": "Image-based Joint-Embedding Predictive Architecture (I-JEPA) learns visual representations by predicting latent embeddings of masked regions from visible context. However, it treats all regions uniformly and independently, lacking an explicit notion of where or in what order predictions should be made. Inspired by human visual perception, which deploys attention selectively and sequentially from the most informative to secondary regions, we propose DSeq-JEPA, a Discriminative Sequential Joint-Embedding Predictive Architecture that bridges predictive and autoregressive self-supervised learning, integrating JEPA-style latent prediction with GPT-style sequential reasoning. Specifically, DSeq-JEPA (i) first identifies primary discriminative regions based on a transformer-derived saliency map, emphasizing the distribution of visual importance, and then (ii) predicts subsequent regions in this discriminative order, progressively forming a curriculum-like semantic progression from primary to secondary cues -- a form of GPT-style pre-training. Extensive experiments across diverse tasks, including image classification (ImageNet), fine-grained visual categorization (iNaturalist21, CUB-200-2011, Stanford-Cars), detection and segmentation (MS-COCO, ADE20K), and low-level reasoning tasks (Clevr/Count, Clevr/Dist), demonstrate that DSeq-JEPA consistently focuses on more discriminative and generalizable representations than I-JEPA variants. Project page: https://github.com/SkyShunsuke/DSeq-JEPA.", "link": "http://arxiv.org/abs/2511.17354v1", "date": "2025-11-21", "relevancy": 2.172, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5679}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.527}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DSeq-JEPA%3A%20Discriminative%20Sequential%20Joint-Embedding%20Predictive%20Architecture&body=Title%3A%20DSeq-JEPA%3A%20Discriminative%20Sequential%20Joint-Embedding%20Predictive%20Architecture%0AAuthor%3A%20Xiangteng%20He%20and%20Shunsuke%20Sakai%20and%20Kun%20Yuan%20and%20Nicolas%20Padoy%20and%20Tatsuhito%20Hasegawa%20and%20Leonid%20Sigal%0AAbstract%3A%20Image-based%20Joint-Embedding%20Predictive%20Architecture%20%28I-JEPA%29%20learns%20visual%20representations%20by%20predicting%20latent%20embeddings%20of%20masked%20regions%20from%20visible%20context.%20However%2C%20it%20treats%20all%20regions%20uniformly%20and%20independently%2C%20lacking%20an%20explicit%20notion%20of%20where%20or%20in%20what%20order%20predictions%20should%20be%20made.%20Inspired%20by%20human%20visual%20perception%2C%20which%20deploys%20attention%20selectively%20and%20sequentially%20from%20the%20most%20informative%20to%20secondary%20regions%2C%20we%20propose%20DSeq-JEPA%2C%20a%20Discriminative%20Sequential%20Joint-Embedding%20Predictive%20Architecture%20that%20bridges%20predictive%20and%20autoregressive%20self-supervised%20learning%2C%20integrating%20JEPA-style%20latent%20prediction%20with%20GPT-style%20sequential%20reasoning.%20Specifically%2C%20DSeq-JEPA%20%28i%29%20first%20identifies%20primary%20discriminative%20regions%20based%20on%20a%20transformer-derived%20saliency%20map%2C%20emphasizing%20the%20distribution%20of%20visual%20importance%2C%20and%20then%20%28ii%29%20predicts%20subsequent%20regions%20in%20this%20discriminative%20order%2C%20progressively%20forming%20a%20curriculum-like%20semantic%20progression%20from%20primary%20to%20secondary%20cues%20--%20a%20form%20of%20GPT-style%20pre-training.%20Extensive%20experiments%20across%20diverse%20tasks%2C%20including%20image%20classification%20%28ImageNet%29%2C%20fine-grained%20visual%20categorization%20%28iNaturalist21%2C%20CUB-200-2011%2C%20Stanford-Cars%29%2C%20detection%20and%20segmentation%20%28MS-COCO%2C%20ADE20K%29%2C%20and%20low-level%20reasoning%20tasks%20%28Clevr/Count%2C%20Clevr/Dist%29%2C%20demonstrate%20that%20DSeq-JEPA%20consistently%20focuses%20on%20more%20discriminative%20and%20generalizable%20representations%20than%20I-JEPA%20variants.%20Project%20page%3A%20https%3A//github.com/SkyShunsuke/DSeq-JEPA.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDSeq-JEPA%253A%2520Discriminative%2520Sequential%2520Joint-Embedding%2520Predictive%2520Architecture%26entry.906535625%3DXiangteng%2520He%2520and%2520Shunsuke%2520Sakai%2520and%2520Kun%2520Yuan%2520and%2520Nicolas%2520Padoy%2520and%2520Tatsuhito%2520Hasegawa%2520and%2520Leonid%2520Sigal%26entry.1292438233%3DImage-based%2520Joint-Embedding%2520Predictive%2520Architecture%2520%2528I-JEPA%2529%2520learns%2520visual%2520representations%2520by%2520predicting%2520latent%2520embeddings%2520of%2520masked%2520regions%2520from%2520visible%2520context.%2520However%252C%2520it%2520treats%2520all%2520regions%2520uniformly%2520and%2520independently%252C%2520lacking%2520an%2520explicit%2520notion%2520of%2520where%2520or%2520in%2520what%2520order%2520predictions%2520should%2520be%2520made.%2520Inspired%2520by%2520human%2520visual%2520perception%252C%2520which%2520deploys%2520attention%2520selectively%2520and%2520sequentially%2520from%2520the%2520most%2520informative%2520to%2520secondary%2520regions%252C%2520we%2520propose%2520DSeq-JEPA%252C%2520a%2520Discriminative%2520Sequential%2520Joint-Embedding%2520Predictive%2520Architecture%2520that%2520bridges%2520predictive%2520and%2520autoregressive%2520self-supervised%2520learning%252C%2520integrating%2520JEPA-style%2520latent%2520prediction%2520with%2520GPT-style%2520sequential%2520reasoning.%2520Specifically%252C%2520DSeq-JEPA%2520%2528i%2529%2520first%2520identifies%2520primary%2520discriminative%2520regions%2520based%2520on%2520a%2520transformer-derived%2520saliency%2520map%252C%2520emphasizing%2520the%2520distribution%2520of%2520visual%2520importance%252C%2520and%2520then%2520%2528ii%2529%2520predicts%2520subsequent%2520regions%2520in%2520this%2520discriminative%2520order%252C%2520progressively%2520forming%2520a%2520curriculum-like%2520semantic%2520progression%2520from%2520primary%2520to%2520secondary%2520cues%2520--%2520a%2520form%2520of%2520GPT-style%2520pre-training.%2520Extensive%2520experiments%2520across%2520diverse%2520tasks%252C%2520including%2520image%2520classification%2520%2528ImageNet%2529%252C%2520fine-grained%2520visual%2520categorization%2520%2528iNaturalist21%252C%2520CUB-200-2011%252C%2520Stanford-Cars%2529%252C%2520detection%2520and%2520segmentation%2520%2528MS-COCO%252C%2520ADE20K%2529%252C%2520and%2520low-level%2520reasoning%2520tasks%2520%2528Clevr/Count%252C%2520Clevr/Dist%2529%252C%2520demonstrate%2520that%2520DSeq-JEPA%2520consistently%2520focuses%2520on%2520more%2520discriminative%2520and%2520generalizable%2520representations%2520than%2520I-JEPA%2520variants.%2520Project%2520page%253A%2520https%253A//github.com/SkyShunsuke/DSeq-JEPA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DSeq-JEPA%3A%20Discriminative%20Sequential%20Joint-Embedding%20Predictive%20Architecture&entry.906535625=Xiangteng%20He%20and%20Shunsuke%20Sakai%20and%20Kun%20Yuan%20and%20Nicolas%20Padoy%20and%20Tatsuhito%20Hasegawa%20and%20Leonid%20Sigal&entry.1292438233=Image-based%20Joint-Embedding%20Predictive%20Architecture%20%28I-JEPA%29%20learns%20visual%20representations%20by%20predicting%20latent%20embeddings%20of%20masked%20regions%20from%20visible%20context.%20However%2C%20it%20treats%20all%20regions%20uniformly%20and%20independently%2C%20lacking%20an%20explicit%20notion%20of%20where%20or%20in%20what%20order%20predictions%20should%20be%20made.%20Inspired%20by%20human%20visual%20perception%2C%20which%20deploys%20attention%20selectively%20and%20sequentially%20from%20the%20most%20informative%20to%20secondary%20regions%2C%20we%20propose%20DSeq-JEPA%2C%20a%20Discriminative%20Sequential%20Joint-Embedding%20Predictive%20Architecture%20that%20bridges%20predictive%20and%20autoregressive%20self-supervised%20learning%2C%20integrating%20JEPA-style%20latent%20prediction%20with%20GPT-style%20sequential%20reasoning.%20Specifically%2C%20DSeq-JEPA%20%28i%29%20first%20identifies%20primary%20discriminative%20regions%20based%20on%20a%20transformer-derived%20saliency%20map%2C%20emphasizing%20the%20distribution%20of%20visual%20importance%2C%20and%20then%20%28ii%29%20predicts%20subsequent%20regions%20in%20this%20discriminative%20order%2C%20progressively%20forming%20a%20curriculum-like%20semantic%20progression%20from%20primary%20to%20secondary%20cues%20--%20a%20form%20of%20GPT-style%20pre-training.%20Extensive%20experiments%20across%20diverse%20tasks%2C%20including%20image%20classification%20%28ImageNet%29%2C%20fine-grained%20visual%20categorization%20%28iNaturalist21%2C%20CUB-200-2011%2C%20Stanford-Cars%29%2C%20detection%20and%20segmentation%20%28MS-COCO%2C%20ADE20K%29%2C%20and%20low-level%20reasoning%20tasks%20%28Clevr/Count%2C%20Clevr/Dist%29%2C%20demonstrate%20that%20DSeq-JEPA%20consistently%20focuses%20on%20more%20discriminative%20and%20generalizable%20representations%20than%20I-JEPA%20variants.%20Project%20page%3A%20https%3A//github.com/SkyShunsuke/DSeq-JEPA.&entry.1838667208=http%3A//arxiv.org/abs/2511.17354v1&entry.124074799=Read"},
{"title": "Designing and Generating Diverse, Equitable Face Image Datasets for Face Verification Tasks", "author": "Georgia Baltsou and Ioannis Sarridis and Christos Koutlis and Symeon Papadopoulos", "abstract": "Face verification is a significant component of identity authentication in various applications including online banking and secure access to personal devices. The majority of the existing face image datasets often suffer from notable biases related to race, gender, and other demographic characteristics, limiting the effectiveness and fairness of face verification systems. In response to these challenges, we propose a comprehensive methodology that integrates advanced generative models to create varied and diverse high-quality synthetic face images. This methodology emphasizes the representation of a diverse range of facial traits, ensuring adherence to characteristics permissible in identity card photographs. Furthermore, we introduce the Diverse and Inclusive Faces for Verification (DIF-V) dataset, comprising 27,780 images of 926 unique identities, designed as a benchmark for future research in face verification. Our analysis reveals that existing verification models exhibit biases toward certain genders and races, and notably, applying identity style modifications negatively impacts model performance. By tackling the inherent inequities in existing datasets, this work not only enriches the discussion on diversity and ethics in artificial intelligence but also lays the foundation for developing more inclusive and reliable face verification technologies", "link": "http://arxiv.org/abs/2511.17393v1", "date": "2025-11-21", "relevancy": 2.1635, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5606}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5336}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Designing%20and%20Generating%20Diverse%2C%20Equitable%20Face%20Image%20Datasets%20for%20Face%20Verification%20Tasks&body=Title%3A%20Designing%20and%20Generating%20Diverse%2C%20Equitable%20Face%20Image%20Datasets%20for%20Face%20Verification%20Tasks%0AAuthor%3A%20Georgia%20Baltsou%20and%20Ioannis%20Sarridis%20and%20Christos%20Koutlis%20and%20Symeon%20Papadopoulos%0AAbstract%3A%20Face%20verification%20is%20a%20significant%20component%20of%20identity%20authentication%20in%20various%20applications%20including%20online%20banking%20and%20secure%20access%20to%20personal%20devices.%20The%20majority%20of%20the%20existing%20face%20image%20datasets%20often%20suffer%20from%20notable%20biases%20related%20to%20race%2C%20gender%2C%20and%20other%20demographic%20characteristics%2C%20limiting%20the%20effectiveness%20and%20fairness%20of%20face%20verification%20systems.%20In%20response%20to%20these%20challenges%2C%20we%20propose%20a%20comprehensive%20methodology%20that%20integrates%20advanced%20generative%20models%20to%20create%20varied%20and%20diverse%20high-quality%20synthetic%20face%20images.%20This%20methodology%20emphasizes%20the%20representation%20of%20a%20diverse%20range%20of%20facial%20traits%2C%20ensuring%20adherence%20to%20characteristics%20permissible%20in%20identity%20card%20photographs.%20Furthermore%2C%20we%20introduce%20the%20Diverse%20and%20Inclusive%20Faces%20for%20Verification%20%28DIF-V%29%20dataset%2C%20comprising%2027%2C780%20images%20of%20926%20unique%20identities%2C%20designed%20as%20a%20benchmark%20for%20future%20research%20in%20face%20verification.%20Our%20analysis%20reveals%20that%20existing%20verification%20models%20exhibit%20biases%20toward%20certain%20genders%20and%20races%2C%20and%20notably%2C%20applying%20identity%20style%20modifications%20negatively%20impacts%20model%20performance.%20By%20tackling%20the%20inherent%20inequities%20in%20existing%20datasets%2C%20this%20work%20not%20only%20enriches%20the%20discussion%20on%20diversity%20and%20ethics%20in%20artificial%20intelligence%20but%20also%20lays%20the%20foundation%20for%20developing%20more%20inclusive%20and%20reliable%20face%20verification%20technologies%0ALink%3A%20http%3A//arxiv.org/abs/2511.17393v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesigning%2520and%2520Generating%2520Diverse%252C%2520Equitable%2520Face%2520Image%2520Datasets%2520for%2520Face%2520Verification%2520Tasks%26entry.906535625%3DGeorgia%2520Baltsou%2520and%2520Ioannis%2520Sarridis%2520and%2520Christos%2520Koutlis%2520and%2520Symeon%2520Papadopoulos%26entry.1292438233%3DFace%2520verification%2520is%2520a%2520significant%2520component%2520of%2520identity%2520authentication%2520in%2520various%2520applications%2520including%2520online%2520banking%2520and%2520secure%2520access%2520to%2520personal%2520devices.%2520The%2520majority%2520of%2520the%2520existing%2520face%2520image%2520datasets%2520often%2520suffer%2520from%2520notable%2520biases%2520related%2520to%2520race%252C%2520gender%252C%2520and%2520other%2520demographic%2520characteristics%252C%2520limiting%2520the%2520effectiveness%2520and%2520fairness%2520of%2520face%2520verification%2520systems.%2520In%2520response%2520to%2520these%2520challenges%252C%2520we%2520propose%2520a%2520comprehensive%2520methodology%2520that%2520integrates%2520advanced%2520generative%2520models%2520to%2520create%2520varied%2520and%2520diverse%2520high-quality%2520synthetic%2520face%2520images.%2520This%2520methodology%2520emphasizes%2520the%2520representation%2520of%2520a%2520diverse%2520range%2520of%2520facial%2520traits%252C%2520ensuring%2520adherence%2520to%2520characteristics%2520permissible%2520in%2520identity%2520card%2520photographs.%2520Furthermore%252C%2520we%2520introduce%2520the%2520Diverse%2520and%2520Inclusive%2520Faces%2520for%2520Verification%2520%2528DIF-V%2529%2520dataset%252C%2520comprising%252027%252C780%2520images%2520of%2520926%2520unique%2520identities%252C%2520designed%2520as%2520a%2520benchmark%2520for%2520future%2520research%2520in%2520face%2520verification.%2520Our%2520analysis%2520reveals%2520that%2520existing%2520verification%2520models%2520exhibit%2520biases%2520toward%2520certain%2520genders%2520and%2520races%252C%2520and%2520notably%252C%2520applying%2520identity%2520style%2520modifications%2520negatively%2520impacts%2520model%2520performance.%2520By%2520tackling%2520the%2520inherent%2520inequities%2520in%2520existing%2520datasets%252C%2520this%2520work%2520not%2520only%2520enriches%2520the%2520discussion%2520on%2520diversity%2520and%2520ethics%2520in%2520artificial%2520intelligence%2520but%2520also%2520lays%2520the%2520foundation%2520for%2520developing%2520more%2520inclusive%2520and%2520reliable%2520face%2520verification%2520technologies%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17393v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Designing%20and%20Generating%20Diverse%2C%20Equitable%20Face%20Image%20Datasets%20for%20Face%20Verification%20Tasks&entry.906535625=Georgia%20Baltsou%20and%20Ioannis%20Sarridis%20and%20Christos%20Koutlis%20and%20Symeon%20Papadopoulos&entry.1292438233=Face%20verification%20is%20a%20significant%20component%20of%20identity%20authentication%20in%20various%20applications%20including%20online%20banking%20and%20secure%20access%20to%20personal%20devices.%20The%20majority%20of%20the%20existing%20face%20image%20datasets%20often%20suffer%20from%20notable%20biases%20related%20to%20race%2C%20gender%2C%20and%20other%20demographic%20characteristics%2C%20limiting%20the%20effectiveness%20and%20fairness%20of%20face%20verification%20systems.%20In%20response%20to%20these%20challenges%2C%20we%20propose%20a%20comprehensive%20methodology%20that%20integrates%20advanced%20generative%20models%20to%20create%20varied%20and%20diverse%20high-quality%20synthetic%20face%20images.%20This%20methodology%20emphasizes%20the%20representation%20of%20a%20diverse%20range%20of%20facial%20traits%2C%20ensuring%20adherence%20to%20characteristics%20permissible%20in%20identity%20card%20photographs.%20Furthermore%2C%20we%20introduce%20the%20Diverse%20and%20Inclusive%20Faces%20for%20Verification%20%28DIF-V%29%20dataset%2C%20comprising%2027%2C780%20images%20of%20926%20unique%20identities%2C%20designed%20as%20a%20benchmark%20for%20future%20research%20in%20face%20verification.%20Our%20analysis%20reveals%20that%20existing%20verification%20models%20exhibit%20biases%20toward%20certain%20genders%20and%20races%2C%20and%20notably%2C%20applying%20identity%20style%20modifications%20negatively%20impacts%20model%20performance.%20By%20tackling%20the%20inherent%20inequities%20in%20existing%20datasets%2C%20this%20work%20not%20only%20enriches%20the%20discussion%20on%20diversity%20and%20ethics%20in%20artificial%20intelligence%20but%20also%20lays%20the%20foundation%20for%20developing%20more%20inclusive%20and%20reliable%20face%20verification%20technologies&entry.1838667208=http%3A//arxiv.org/abs/2511.17393v1&entry.124074799=Read"},
{"title": "Physically Interpretable World Models via Weakly Supervised Representation Learning", "author": "Zhenjiang Mao and Mrinall Eashaan Umasudhan and Ivan Ruchkin", "abstract": "Learning predictive models from high-dimensional sensory observations is fundamental for cyber-physical systems, yet the latent representations learned by standard world models lack physical interpretability. This limits their reliability, generalizability, and applicability to safety-critical tasks. We introduce Physically Interpretable World Models (PIWM), a framework that aligns latent representations with real-world physical quantities and constrains their evolution through partially known physical dynamics. Physical interpretability in PIWM is defined by two complementary properties: (i) the learned latent state corresponds to meaningful physical variables, and (ii) its temporal evolution follows physically consistent dynamics. To achieve this without requiring ground-truth physical annotations, PIWM employs weak distribution-based supervision that captures state uncertainty naturally arising from real-world sensing pipelines. The architecture integrates a VQ-based visual encoder, a transformer-based physical encoder, and a learnable dynamics model grounded in known physical equations. Across three case studies (Cart Pole, Lunar Lander, and Donkey Car), PIWM achieves accurate long-horizon prediction, recovers true system parameters, and significantly improves physical grounding over purely data-driven models. These results demonstrate the feasibility and advantages of learning physically interpretable world models directly from images under weak supervision.", "link": "http://arxiv.org/abs/2412.12870v5", "date": "2025-11-21", "relevancy": 2.1606, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5804}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5437}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physically%20Interpretable%20World%20Models%20via%20Weakly%20Supervised%20Representation%20Learning&body=Title%3A%20Physically%20Interpretable%20World%20Models%20via%20Weakly%20Supervised%20Representation%20Learning%0AAuthor%3A%20Zhenjiang%20Mao%20and%20Mrinall%20Eashaan%20Umasudhan%20and%20Ivan%20Ruchkin%0AAbstract%3A%20Learning%20predictive%20models%20from%20high-dimensional%20sensory%20observations%20is%20fundamental%20for%20cyber-physical%20systems%2C%20yet%20the%20latent%20representations%20learned%20by%20standard%20world%20models%20lack%20physical%20interpretability.%20This%20limits%20their%20reliability%2C%20generalizability%2C%20and%20applicability%20to%20safety-critical%20tasks.%20We%20introduce%20Physically%20Interpretable%20World%20Models%20%28PIWM%29%2C%20a%20framework%20that%20aligns%20latent%20representations%20with%20real-world%20physical%20quantities%20and%20constrains%20their%20evolution%20through%20partially%20known%20physical%20dynamics.%20Physical%20interpretability%20in%20PIWM%20is%20defined%20by%20two%20complementary%20properties%3A%20%28i%29%20the%20learned%20latent%20state%20corresponds%20to%20meaningful%20physical%20variables%2C%20and%20%28ii%29%20its%20temporal%20evolution%20follows%20physically%20consistent%20dynamics.%20To%20achieve%20this%20without%20requiring%20ground-truth%20physical%20annotations%2C%20PIWM%20employs%20weak%20distribution-based%20supervision%20that%20captures%20state%20uncertainty%20naturally%20arising%20from%20real-world%20sensing%20pipelines.%20The%20architecture%20integrates%20a%20VQ-based%20visual%20encoder%2C%20a%20transformer-based%20physical%20encoder%2C%20and%20a%20learnable%20dynamics%20model%20grounded%20in%20known%20physical%20equations.%20Across%20three%20case%20studies%20%28Cart%20Pole%2C%20Lunar%20Lander%2C%20and%20Donkey%20Car%29%2C%20PIWM%20achieves%20accurate%20long-horizon%20prediction%2C%20recovers%20true%20system%20parameters%2C%20and%20significantly%20improves%20physical%20grounding%20over%20purely%20data-driven%20models.%20These%20results%20demonstrate%20the%20feasibility%20and%20advantages%20of%20learning%20physically%20interpretable%20world%20models%20directly%20from%20images%20under%20weak%20supervision.%0ALink%3A%20http%3A//arxiv.org/abs/2412.12870v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysically%2520Interpretable%2520World%2520Models%2520via%2520Weakly%2520Supervised%2520Representation%2520Learning%26entry.906535625%3DZhenjiang%2520Mao%2520and%2520Mrinall%2520Eashaan%2520Umasudhan%2520and%2520Ivan%2520Ruchkin%26entry.1292438233%3DLearning%2520predictive%2520models%2520from%2520high-dimensional%2520sensory%2520observations%2520is%2520fundamental%2520for%2520cyber-physical%2520systems%252C%2520yet%2520the%2520latent%2520representations%2520learned%2520by%2520standard%2520world%2520models%2520lack%2520physical%2520interpretability.%2520This%2520limits%2520their%2520reliability%252C%2520generalizability%252C%2520and%2520applicability%2520to%2520safety-critical%2520tasks.%2520We%2520introduce%2520Physically%2520Interpretable%2520World%2520Models%2520%2528PIWM%2529%252C%2520a%2520framework%2520that%2520aligns%2520latent%2520representations%2520with%2520real-world%2520physical%2520quantities%2520and%2520constrains%2520their%2520evolution%2520through%2520partially%2520known%2520physical%2520dynamics.%2520Physical%2520interpretability%2520in%2520PIWM%2520is%2520defined%2520by%2520two%2520complementary%2520properties%253A%2520%2528i%2529%2520the%2520learned%2520latent%2520state%2520corresponds%2520to%2520meaningful%2520physical%2520variables%252C%2520and%2520%2528ii%2529%2520its%2520temporal%2520evolution%2520follows%2520physically%2520consistent%2520dynamics.%2520To%2520achieve%2520this%2520without%2520requiring%2520ground-truth%2520physical%2520annotations%252C%2520PIWM%2520employs%2520weak%2520distribution-based%2520supervision%2520that%2520captures%2520state%2520uncertainty%2520naturally%2520arising%2520from%2520real-world%2520sensing%2520pipelines.%2520The%2520architecture%2520integrates%2520a%2520VQ-based%2520visual%2520encoder%252C%2520a%2520transformer-based%2520physical%2520encoder%252C%2520and%2520a%2520learnable%2520dynamics%2520model%2520grounded%2520in%2520known%2520physical%2520equations.%2520Across%2520three%2520case%2520studies%2520%2528Cart%2520Pole%252C%2520Lunar%2520Lander%252C%2520and%2520Donkey%2520Car%2529%252C%2520PIWM%2520achieves%2520accurate%2520long-horizon%2520prediction%252C%2520recovers%2520true%2520system%2520parameters%252C%2520and%2520significantly%2520improves%2520physical%2520grounding%2520over%2520purely%2520data-driven%2520models.%2520These%2520results%2520demonstrate%2520the%2520feasibility%2520and%2520advantages%2520of%2520learning%2520physically%2520interpretable%2520world%2520models%2520directly%2520from%2520images%2520under%2520weak%2520supervision.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12870v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physically%20Interpretable%20World%20Models%20via%20Weakly%20Supervised%20Representation%20Learning&entry.906535625=Zhenjiang%20Mao%20and%20Mrinall%20Eashaan%20Umasudhan%20and%20Ivan%20Ruchkin&entry.1292438233=Learning%20predictive%20models%20from%20high-dimensional%20sensory%20observations%20is%20fundamental%20for%20cyber-physical%20systems%2C%20yet%20the%20latent%20representations%20learned%20by%20standard%20world%20models%20lack%20physical%20interpretability.%20This%20limits%20their%20reliability%2C%20generalizability%2C%20and%20applicability%20to%20safety-critical%20tasks.%20We%20introduce%20Physically%20Interpretable%20World%20Models%20%28PIWM%29%2C%20a%20framework%20that%20aligns%20latent%20representations%20with%20real-world%20physical%20quantities%20and%20constrains%20their%20evolution%20through%20partially%20known%20physical%20dynamics.%20Physical%20interpretability%20in%20PIWM%20is%20defined%20by%20two%20complementary%20properties%3A%20%28i%29%20the%20learned%20latent%20state%20corresponds%20to%20meaningful%20physical%20variables%2C%20and%20%28ii%29%20its%20temporal%20evolution%20follows%20physically%20consistent%20dynamics.%20To%20achieve%20this%20without%20requiring%20ground-truth%20physical%20annotations%2C%20PIWM%20employs%20weak%20distribution-based%20supervision%20that%20captures%20state%20uncertainty%20naturally%20arising%20from%20real-world%20sensing%20pipelines.%20The%20architecture%20integrates%20a%20VQ-based%20visual%20encoder%2C%20a%20transformer-based%20physical%20encoder%2C%20and%20a%20learnable%20dynamics%20model%20grounded%20in%20known%20physical%20equations.%20Across%20three%20case%20studies%20%28Cart%20Pole%2C%20Lunar%20Lander%2C%20and%20Donkey%20Car%29%2C%20PIWM%20achieves%20accurate%20long-horizon%20prediction%2C%20recovers%20true%20system%20parameters%2C%20and%20significantly%20improves%20physical%20grounding%20over%20purely%20data-driven%20models.%20These%20results%20demonstrate%20the%20feasibility%20and%20advantages%20of%20learning%20physically%20interpretable%20world%20models%20directly%20from%20images%20under%20weak%20supervision.&entry.1838667208=http%3A//arxiv.org/abs/2412.12870v5&entry.124074799=Read"},
{"title": "Bridging Visual Affective Gap: Borrowing Textual Knowledge by Learning from Noisy Image-Text Pairs", "author": "Daiqing Wu and Dongbao Yang and Yu Zhou and Can Ma", "abstract": "Visual emotion recognition (VER) is a longstanding field that has garnered increasing attention with the advancement of deep neural networks. Although recent studies have achieved notable improvements by leveraging the knowledge embedded within pre-trained visual models, the lack of direct association between factual-level features and emotional categories, called the \"affective gap\", limits the applicability of pre-training knowledge for VER tasks. On the contrary, the explicit emotional expression and high information density in textual modality eliminate the \"affective gap\". Therefore, we propose borrowing the knowledge from the pre-trained textual model to enhance the emotional perception of pre-trained visual models. We focus on the factual and emotional connections between images and texts in noisy social media data, and propose Partitioned Adaptive Contrastive Learning (PACL) to leverage these connections. Specifically, we manage to separate different types of samples and devise distinct contrastive learning strategies for each type. By dynamically constructing negative and positive pairs, we fully exploit the potential of noisy samples. Through comprehensive experiments, we demonstrate that bridging the \"affective gap\" significantly improves the performance of various pre-trained visual models in downstream emotion-related tasks. Our code is released on https://github.com/wdqqdw/PACL.", "link": "http://arxiv.org/abs/2511.17103v1", "date": "2025-11-21", "relevancy": 2.153, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5463}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5404}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Visual%20Affective%20Gap%3A%20Borrowing%20Textual%20Knowledge%20by%20Learning%20from%20Noisy%20Image-Text%20Pairs&body=Title%3A%20Bridging%20Visual%20Affective%20Gap%3A%20Borrowing%20Textual%20Knowledge%20by%20Learning%20from%20Noisy%20Image-Text%20Pairs%0AAuthor%3A%20Daiqing%20Wu%20and%20Dongbao%20Yang%20and%20Yu%20Zhou%20and%20Can%20Ma%0AAbstract%3A%20Visual%20emotion%20recognition%20%28VER%29%20is%20a%20longstanding%20field%20that%20has%20garnered%20increasing%20attention%20with%20the%20advancement%20of%20deep%20neural%20networks.%20Although%20recent%20studies%20have%20achieved%20notable%20improvements%20by%20leveraging%20the%20knowledge%20embedded%20within%20pre-trained%20visual%20models%2C%20the%20lack%20of%20direct%20association%20between%20factual-level%20features%20and%20emotional%20categories%2C%20called%20the%20%22affective%20gap%22%2C%20limits%20the%20applicability%20of%20pre-training%20knowledge%20for%20VER%20tasks.%20On%20the%20contrary%2C%20the%20explicit%20emotional%20expression%20and%20high%20information%20density%20in%20textual%20modality%20eliminate%20the%20%22affective%20gap%22.%20Therefore%2C%20we%20propose%20borrowing%20the%20knowledge%20from%20the%20pre-trained%20textual%20model%20to%20enhance%20the%20emotional%20perception%20of%20pre-trained%20visual%20models.%20We%20focus%20on%20the%20factual%20and%20emotional%20connections%20between%20images%20and%20texts%20in%20noisy%20social%20media%20data%2C%20and%20propose%20Partitioned%20Adaptive%20Contrastive%20Learning%20%28PACL%29%20to%20leverage%20these%20connections.%20Specifically%2C%20we%20manage%20to%20separate%20different%20types%20of%20samples%20and%20devise%20distinct%20contrastive%20learning%20strategies%20for%20each%20type.%20By%20dynamically%20constructing%20negative%20and%20positive%20pairs%2C%20we%20fully%20exploit%20the%20potential%20of%20noisy%20samples.%20Through%20comprehensive%20experiments%2C%20we%20demonstrate%20that%20bridging%20the%20%22affective%20gap%22%20significantly%20improves%20the%20performance%20of%20various%20pre-trained%20visual%20models%20in%20downstream%20emotion-related%20tasks.%20Our%20code%20is%20released%20on%20https%3A//github.com/wdqqdw/PACL.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17103v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Visual%2520Affective%2520Gap%253A%2520Borrowing%2520Textual%2520Knowledge%2520by%2520Learning%2520from%2520Noisy%2520Image-Text%2520Pairs%26entry.906535625%3DDaiqing%2520Wu%2520and%2520Dongbao%2520Yang%2520and%2520Yu%2520Zhou%2520and%2520Can%2520Ma%26entry.1292438233%3DVisual%2520emotion%2520recognition%2520%2528VER%2529%2520is%2520a%2520longstanding%2520field%2520that%2520has%2520garnered%2520increasing%2520attention%2520with%2520the%2520advancement%2520of%2520deep%2520neural%2520networks.%2520Although%2520recent%2520studies%2520have%2520achieved%2520notable%2520improvements%2520by%2520leveraging%2520the%2520knowledge%2520embedded%2520within%2520pre-trained%2520visual%2520models%252C%2520the%2520lack%2520of%2520direct%2520association%2520between%2520factual-level%2520features%2520and%2520emotional%2520categories%252C%2520called%2520the%2520%2522affective%2520gap%2522%252C%2520limits%2520the%2520applicability%2520of%2520pre-training%2520knowledge%2520for%2520VER%2520tasks.%2520On%2520the%2520contrary%252C%2520the%2520explicit%2520emotional%2520expression%2520and%2520high%2520information%2520density%2520in%2520textual%2520modality%2520eliminate%2520the%2520%2522affective%2520gap%2522.%2520Therefore%252C%2520we%2520propose%2520borrowing%2520the%2520knowledge%2520from%2520the%2520pre-trained%2520textual%2520model%2520to%2520enhance%2520the%2520emotional%2520perception%2520of%2520pre-trained%2520visual%2520models.%2520We%2520focus%2520on%2520the%2520factual%2520and%2520emotional%2520connections%2520between%2520images%2520and%2520texts%2520in%2520noisy%2520social%2520media%2520data%252C%2520and%2520propose%2520Partitioned%2520Adaptive%2520Contrastive%2520Learning%2520%2528PACL%2529%2520to%2520leverage%2520these%2520connections.%2520Specifically%252C%2520we%2520manage%2520to%2520separate%2520different%2520types%2520of%2520samples%2520and%2520devise%2520distinct%2520contrastive%2520learning%2520strategies%2520for%2520each%2520type.%2520By%2520dynamically%2520constructing%2520negative%2520and%2520positive%2520pairs%252C%2520we%2520fully%2520exploit%2520the%2520potential%2520of%2520noisy%2520samples.%2520Through%2520comprehensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520bridging%2520the%2520%2522affective%2520gap%2522%2520significantly%2520improves%2520the%2520performance%2520of%2520various%2520pre-trained%2520visual%2520models%2520in%2520downstream%2520emotion-related%2520tasks.%2520Our%2520code%2520is%2520released%2520on%2520https%253A//github.com/wdqqdw/PACL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17103v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Visual%20Affective%20Gap%3A%20Borrowing%20Textual%20Knowledge%20by%20Learning%20from%20Noisy%20Image-Text%20Pairs&entry.906535625=Daiqing%20Wu%20and%20Dongbao%20Yang%20and%20Yu%20Zhou%20and%20Can%20Ma&entry.1292438233=Visual%20emotion%20recognition%20%28VER%29%20is%20a%20longstanding%20field%20that%20has%20garnered%20increasing%20attention%20with%20the%20advancement%20of%20deep%20neural%20networks.%20Although%20recent%20studies%20have%20achieved%20notable%20improvements%20by%20leveraging%20the%20knowledge%20embedded%20within%20pre-trained%20visual%20models%2C%20the%20lack%20of%20direct%20association%20between%20factual-level%20features%20and%20emotional%20categories%2C%20called%20the%20%22affective%20gap%22%2C%20limits%20the%20applicability%20of%20pre-training%20knowledge%20for%20VER%20tasks.%20On%20the%20contrary%2C%20the%20explicit%20emotional%20expression%20and%20high%20information%20density%20in%20textual%20modality%20eliminate%20the%20%22affective%20gap%22.%20Therefore%2C%20we%20propose%20borrowing%20the%20knowledge%20from%20the%20pre-trained%20textual%20model%20to%20enhance%20the%20emotional%20perception%20of%20pre-trained%20visual%20models.%20We%20focus%20on%20the%20factual%20and%20emotional%20connections%20between%20images%20and%20texts%20in%20noisy%20social%20media%20data%2C%20and%20propose%20Partitioned%20Adaptive%20Contrastive%20Learning%20%28PACL%29%20to%20leverage%20these%20connections.%20Specifically%2C%20we%20manage%20to%20separate%20different%20types%20of%20samples%20and%20devise%20distinct%20contrastive%20learning%20strategies%20for%20each%20type.%20By%20dynamically%20constructing%20negative%20and%20positive%20pairs%2C%20we%20fully%20exploit%20the%20potential%20of%20noisy%20samples.%20Through%20comprehensive%20experiments%2C%20we%20demonstrate%20that%20bridging%20the%20%22affective%20gap%22%20significantly%20improves%20the%20performance%20of%20various%20pre-trained%20visual%20models%20in%20downstream%20emotion-related%20tasks.%20Our%20code%20is%20released%20on%20https%3A//github.com/wdqqdw/PACL.&entry.1838667208=http%3A//arxiv.org/abs/2511.17103v1&entry.124074799=Read"},
{"title": "Human Imitated Bipedal Locomotion with Frequency Based Gait Generator Network", "author": "Yusuf Baran Ates and Omer Morgul", "abstract": "Learning human-like, robust bipedal walking remains difficult due to hybrid dynamics and terrain variability. We propose a lightweight framework that combines a gait generator network learned from human motion with Proximal Policy Optimization (PPO) controller for torque control. Despite being trained only on flat or mildly sloped ground, the learned policies generalize to steeper ramps and rough surfaces. Results suggest that pairing spectral motion priors with Deep Reinforcement Learning (DRL) offers a practical path toward natural and robust bipedal locomotion with modest training cost.", "link": "http://arxiv.org/abs/2511.17387v1", "date": "2025-11-21", "relevancy": 2.1513, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5585}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5436}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Imitated%20Bipedal%20Locomotion%20with%20Frequency%20Based%20Gait%20Generator%20Network&body=Title%3A%20Human%20Imitated%20Bipedal%20Locomotion%20with%20Frequency%20Based%20Gait%20Generator%20Network%0AAuthor%3A%20Yusuf%20Baran%20Ates%20and%20Omer%20Morgul%0AAbstract%3A%20Learning%20human-like%2C%20robust%20bipedal%20walking%20remains%20difficult%20due%20to%20hybrid%20dynamics%20and%20terrain%20variability.%20We%20propose%20a%20lightweight%20framework%20that%20combines%20a%20gait%20generator%20network%20learned%20from%20human%20motion%20with%20Proximal%20Policy%20Optimization%20%28PPO%29%20controller%20for%20torque%20control.%20Despite%20being%20trained%20only%20on%20flat%20or%20mildly%20sloped%20ground%2C%20the%20learned%20policies%20generalize%20to%20steeper%20ramps%20and%20rough%20surfaces.%20Results%20suggest%20that%20pairing%20spectral%20motion%20priors%20with%20Deep%20Reinforcement%20Learning%20%28DRL%29%20offers%20a%20practical%20path%20toward%20natural%20and%20robust%20bipedal%20locomotion%20with%20modest%20training%20cost.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Imitated%2520Bipedal%2520Locomotion%2520with%2520Frequency%2520Based%2520Gait%2520Generator%2520Network%26entry.906535625%3DYusuf%2520Baran%2520Ates%2520and%2520Omer%2520Morgul%26entry.1292438233%3DLearning%2520human-like%252C%2520robust%2520bipedal%2520walking%2520remains%2520difficult%2520due%2520to%2520hybrid%2520dynamics%2520and%2520terrain%2520variability.%2520We%2520propose%2520a%2520lightweight%2520framework%2520that%2520combines%2520a%2520gait%2520generator%2520network%2520learned%2520from%2520human%2520motion%2520with%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%2520controller%2520for%2520torque%2520control.%2520Despite%2520being%2520trained%2520only%2520on%2520flat%2520or%2520mildly%2520sloped%2520ground%252C%2520the%2520learned%2520policies%2520generalize%2520to%2520steeper%2520ramps%2520and%2520rough%2520surfaces.%2520Results%2520suggest%2520that%2520pairing%2520spectral%2520motion%2520priors%2520with%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520offers%2520a%2520practical%2520path%2520toward%2520natural%2520and%2520robust%2520bipedal%2520locomotion%2520with%2520modest%2520training%2520cost.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Imitated%20Bipedal%20Locomotion%20with%20Frequency%20Based%20Gait%20Generator%20Network&entry.906535625=Yusuf%20Baran%20Ates%20and%20Omer%20Morgul&entry.1292438233=Learning%20human-like%2C%20robust%20bipedal%20walking%20remains%20difficult%20due%20to%20hybrid%20dynamics%20and%20terrain%20variability.%20We%20propose%20a%20lightweight%20framework%20that%20combines%20a%20gait%20generator%20network%20learned%20from%20human%20motion%20with%20Proximal%20Policy%20Optimization%20%28PPO%29%20controller%20for%20torque%20control.%20Despite%20being%20trained%20only%20on%20flat%20or%20mildly%20sloped%20ground%2C%20the%20learned%20policies%20generalize%20to%20steeper%20ramps%20and%20rough%20surfaces.%20Results%20suggest%20that%20pairing%20spectral%20motion%20priors%20with%20Deep%20Reinforcement%20Learning%20%28DRL%29%20offers%20a%20practical%20path%20toward%20natural%20and%20robust%20bipedal%20locomotion%20with%20modest%20training%20cost.&entry.1838667208=http%3A//arxiv.org/abs/2511.17387v1&entry.124074799=Read"},
{"title": "Multi-Agent Pointer Transformer: Seq-to-Seq Reinforcement Learning for Multi-Vehicle Dynamic Pickup-Delivery Problems", "author": "Zengyu Zou and Jingyuan Wang and Yixuan Huang and Junjie Wu", "abstract": "This paper addresses the cooperative Multi-Vehicle Dynamic Pickup and Delivery Problem with Stochastic Requests (MVDPDPSR) and proposes an end-to-end centralized decision-making framework based on sequence-to-sequence, named Multi-Agent Pointer Transformer (MAPT). MVDPDPSR is an extension of the vehicle routing problem and a spatio-temporal system optimization problem, widely applied in scenarios such as on-demand delivery. Classical operations research methods face bottlenecks in computational complexity and time efficiency when handling large-scale dynamic problems. Although existing reinforcement learning methods have achieved some progress, they still encounter several challenges: 1) Independent decoding across multiple vehicles fails to model joint action distributions; 2) The feature extraction network struggles to capture inter-entity relationships; 3) The joint action space is exponentially large. To address these issues, we designed the MAPT framework, which employs a Transformer Encoder to extract entity representations, combines a Transformer Decoder with a Pointer Network to generate joint action sequences in an AutoRegressive manner, and introduces a Relation-Aware Attention module to capture inter-entity relationships. Additionally, we guide the model's decision-making using informative priors to facilitate effective exploration. Experiments on 8 datasets demonstrate that MAPT significantly outperforms existing baseline methods in terms of performance and exhibits substantial computational time advantages compared to classical operations research methods.", "link": "http://arxiv.org/abs/2511.17435v1", "date": "2025-11-21", "relevancy": 2.1502, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5854}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5377}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Agent%20Pointer%20Transformer%3A%20Seq-to-Seq%20Reinforcement%20Learning%20for%20Multi-Vehicle%20Dynamic%20Pickup-Delivery%20Problems&body=Title%3A%20Multi-Agent%20Pointer%20Transformer%3A%20Seq-to-Seq%20Reinforcement%20Learning%20for%20Multi-Vehicle%20Dynamic%20Pickup-Delivery%20Problems%0AAuthor%3A%20Zengyu%20Zou%20and%20Jingyuan%20Wang%20and%20Yixuan%20Huang%20and%20Junjie%20Wu%0AAbstract%3A%20This%20paper%20addresses%20the%20cooperative%20Multi-Vehicle%20Dynamic%20Pickup%20and%20Delivery%20Problem%20with%20Stochastic%20Requests%20%28MVDPDPSR%29%20and%20proposes%20an%20end-to-end%20centralized%20decision-making%20framework%20based%20on%20sequence-to-sequence%2C%20named%20Multi-Agent%20Pointer%20Transformer%20%28MAPT%29.%20MVDPDPSR%20is%20an%20extension%20of%20the%20vehicle%20routing%20problem%20and%20a%20spatio-temporal%20system%20optimization%20problem%2C%20widely%20applied%20in%20scenarios%20such%20as%20on-demand%20delivery.%20Classical%20operations%20research%20methods%20face%20bottlenecks%20in%20computational%20complexity%20and%20time%20efficiency%20when%20handling%20large-scale%20dynamic%20problems.%20Although%20existing%20reinforcement%20learning%20methods%20have%20achieved%20some%20progress%2C%20they%20still%20encounter%20several%20challenges%3A%201%29%20Independent%20decoding%20across%20multiple%20vehicles%20fails%20to%20model%20joint%20action%20distributions%3B%202%29%20The%20feature%20extraction%20network%20struggles%20to%20capture%20inter-entity%20relationships%3B%203%29%20The%20joint%20action%20space%20is%20exponentially%20large.%20To%20address%20these%20issues%2C%20we%20designed%20the%20MAPT%20framework%2C%20which%20employs%20a%20Transformer%20Encoder%20to%20extract%20entity%20representations%2C%20combines%20a%20Transformer%20Decoder%20with%20a%20Pointer%20Network%20to%20generate%20joint%20action%20sequences%20in%20an%20AutoRegressive%20manner%2C%20and%20introduces%20a%20Relation-Aware%20Attention%20module%20to%20capture%20inter-entity%20relationships.%20Additionally%2C%20we%20guide%20the%20model%27s%20decision-making%20using%20informative%20priors%20to%20facilitate%20effective%20exploration.%20Experiments%20on%208%20datasets%20demonstrate%20that%20MAPT%20significantly%20outperforms%20existing%20baseline%20methods%20in%20terms%20of%20performance%20and%20exhibits%20substantial%20computational%20time%20advantages%20compared%20to%20classical%20operations%20research%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Agent%2520Pointer%2520Transformer%253A%2520Seq-to-Seq%2520Reinforcement%2520Learning%2520for%2520Multi-Vehicle%2520Dynamic%2520Pickup-Delivery%2520Problems%26entry.906535625%3DZengyu%2520Zou%2520and%2520Jingyuan%2520Wang%2520and%2520Yixuan%2520Huang%2520and%2520Junjie%2520Wu%26entry.1292438233%3DThis%2520paper%2520addresses%2520the%2520cooperative%2520Multi-Vehicle%2520Dynamic%2520Pickup%2520and%2520Delivery%2520Problem%2520with%2520Stochastic%2520Requests%2520%2528MVDPDPSR%2529%2520and%2520proposes%2520an%2520end-to-end%2520centralized%2520decision-making%2520framework%2520based%2520on%2520sequence-to-sequence%252C%2520named%2520Multi-Agent%2520Pointer%2520Transformer%2520%2528MAPT%2529.%2520MVDPDPSR%2520is%2520an%2520extension%2520of%2520the%2520vehicle%2520routing%2520problem%2520and%2520a%2520spatio-temporal%2520system%2520optimization%2520problem%252C%2520widely%2520applied%2520in%2520scenarios%2520such%2520as%2520on-demand%2520delivery.%2520Classical%2520operations%2520research%2520methods%2520face%2520bottlenecks%2520in%2520computational%2520complexity%2520and%2520time%2520efficiency%2520when%2520handling%2520large-scale%2520dynamic%2520problems.%2520Although%2520existing%2520reinforcement%2520learning%2520methods%2520have%2520achieved%2520some%2520progress%252C%2520they%2520still%2520encounter%2520several%2520challenges%253A%25201%2529%2520Independent%2520decoding%2520across%2520multiple%2520vehicles%2520fails%2520to%2520model%2520joint%2520action%2520distributions%253B%25202%2529%2520The%2520feature%2520extraction%2520network%2520struggles%2520to%2520capture%2520inter-entity%2520relationships%253B%25203%2529%2520The%2520joint%2520action%2520space%2520is%2520exponentially%2520large.%2520To%2520address%2520these%2520issues%252C%2520we%2520designed%2520the%2520MAPT%2520framework%252C%2520which%2520employs%2520a%2520Transformer%2520Encoder%2520to%2520extract%2520entity%2520representations%252C%2520combines%2520a%2520Transformer%2520Decoder%2520with%2520a%2520Pointer%2520Network%2520to%2520generate%2520joint%2520action%2520sequences%2520in%2520an%2520AutoRegressive%2520manner%252C%2520and%2520introduces%2520a%2520Relation-Aware%2520Attention%2520module%2520to%2520capture%2520inter-entity%2520relationships.%2520Additionally%252C%2520we%2520guide%2520the%2520model%2527s%2520decision-making%2520using%2520informative%2520priors%2520to%2520facilitate%2520effective%2520exploration.%2520Experiments%2520on%25208%2520datasets%2520demonstrate%2520that%2520MAPT%2520significantly%2520outperforms%2520existing%2520baseline%2520methods%2520in%2520terms%2520of%2520performance%2520and%2520exhibits%2520substantial%2520computational%2520time%2520advantages%2520compared%2520to%2520classical%2520operations%2520research%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agent%20Pointer%20Transformer%3A%20Seq-to-Seq%20Reinforcement%20Learning%20for%20Multi-Vehicle%20Dynamic%20Pickup-Delivery%20Problems&entry.906535625=Zengyu%20Zou%20and%20Jingyuan%20Wang%20and%20Yixuan%20Huang%20and%20Junjie%20Wu&entry.1292438233=This%20paper%20addresses%20the%20cooperative%20Multi-Vehicle%20Dynamic%20Pickup%20and%20Delivery%20Problem%20with%20Stochastic%20Requests%20%28MVDPDPSR%29%20and%20proposes%20an%20end-to-end%20centralized%20decision-making%20framework%20based%20on%20sequence-to-sequence%2C%20named%20Multi-Agent%20Pointer%20Transformer%20%28MAPT%29.%20MVDPDPSR%20is%20an%20extension%20of%20the%20vehicle%20routing%20problem%20and%20a%20spatio-temporal%20system%20optimization%20problem%2C%20widely%20applied%20in%20scenarios%20such%20as%20on-demand%20delivery.%20Classical%20operations%20research%20methods%20face%20bottlenecks%20in%20computational%20complexity%20and%20time%20efficiency%20when%20handling%20large-scale%20dynamic%20problems.%20Although%20existing%20reinforcement%20learning%20methods%20have%20achieved%20some%20progress%2C%20they%20still%20encounter%20several%20challenges%3A%201%29%20Independent%20decoding%20across%20multiple%20vehicles%20fails%20to%20model%20joint%20action%20distributions%3B%202%29%20The%20feature%20extraction%20network%20struggles%20to%20capture%20inter-entity%20relationships%3B%203%29%20The%20joint%20action%20space%20is%20exponentially%20large.%20To%20address%20these%20issues%2C%20we%20designed%20the%20MAPT%20framework%2C%20which%20employs%20a%20Transformer%20Encoder%20to%20extract%20entity%20representations%2C%20combines%20a%20Transformer%20Decoder%20with%20a%20Pointer%20Network%20to%20generate%20joint%20action%20sequences%20in%20an%20AutoRegressive%20manner%2C%20and%20introduces%20a%20Relation-Aware%20Attention%20module%20to%20capture%20inter-entity%20relationships.%20Additionally%2C%20we%20guide%20the%20model%27s%20decision-making%20using%20informative%20priors%20to%20facilitate%20effective%20exploration.%20Experiments%20on%208%20datasets%20demonstrate%20that%20MAPT%20significantly%20outperforms%20existing%20baseline%20methods%20in%20terms%20of%20performance%20and%20exhibits%20substantial%20computational%20time%20advantages%20compared%20to%20classical%20operations%20research%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.17435v1&entry.124074799=Read"},
{"title": "GLOBE: Accurate and Generalizable PDE Surrogates using Domain-Inspired Architectures and Equivariances", "author": "Peter Sharpe", "abstract": "We introduce GLOBE, a new neural surrogate for homogeneous PDEs that draws inductive bias from boundary-element methods and equivariant ML. GLOBE represents solutions as superpositions of learnable Green's-function-like kernels evaluated from boundary faces to targets, composed across multiscale branches and communication hyperlayers. The architecture is translation-, rotation-, and parity-equivariant; discretization-invariant in the fine-mesh limit; and units-invariant via rigorous nondimensionalization. An explicit far-field decay envelope stabilizes extrapolation, boundary-to-boundary hyperlayer communication mediates long-range coupling, and the all-to-all boundary-to-target evaluation yields a global receptive field that respects PDE information flow, even for elliptic PDEs.\n  On AirFRANS (steady incompressible RANS over NACA airfoils), GLOBE achieves substantial accuracy improvements. On the \"Full\" split, it reduces mean-squared error by roughly 200x on all fields relative to the dataset's reference baselines, and roughly 50x relative to the next-best-performing model. In the \"Scarce\" split, it achieves over 100x lower error on velocity and pressure fields and over 600x lower error on surface pressure than Transolver. Qualitative results show sharp near-wall gradients, coherent wakes, and limited errors under modest extrapolation in Reynolds number and angle of attack.\n  In addition to this accuracy, the model is quite compact (117k parameters), and fields can be evaluated at arbitrary points during inference. We also demonstrate the ability to train and predict with non-watertight meshes, which has strong practical implications.\n  These results show that rigorous physics- and domain-inspired inductive biases can achieve large gains in accuracy, generalizability, and practicality for ML-based PDE surrogates for industrial computer-aided engineering (CAE).", "link": "http://arxiv.org/abs/2511.15856v2", "date": "2025-11-21", "relevancy": 2.1484, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5482}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5294}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLOBE%3A%20Accurate%20and%20Generalizable%20PDE%20Surrogates%20using%20Domain-Inspired%20Architectures%20and%20Equivariances&body=Title%3A%20GLOBE%3A%20Accurate%20and%20Generalizable%20PDE%20Surrogates%20using%20Domain-Inspired%20Architectures%20and%20Equivariances%0AAuthor%3A%20Peter%20Sharpe%0AAbstract%3A%20We%20introduce%20GLOBE%2C%20a%20new%20neural%20surrogate%20for%20homogeneous%20PDEs%20that%20draws%20inductive%20bias%20from%20boundary-element%20methods%20and%20equivariant%20ML.%20GLOBE%20represents%20solutions%20as%20superpositions%20of%20learnable%20Green%27s-function-like%20kernels%20evaluated%20from%20boundary%20faces%20to%20targets%2C%20composed%20across%20multiscale%20branches%20and%20communication%20hyperlayers.%20The%20architecture%20is%20translation-%2C%20rotation-%2C%20and%20parity-equivariant%3B%20discretization-invariant%20in%20the%20fine-mesh%20limit%3B%20and%20units-invariant%20via%20rigorous%20nondimensionalization.%20An%20explicit%20far-field%20decay%20envelope%20stabilizes%20extrapolation%2C%20boundary-to-boundary%20hyperlayer%20communication%20mediates%20long-range%20coupling%2C%20and%20the%20all-to-all%20boundary-to-target%20evaluation%20yields%20a%20global%20receptive%20field%20that%20respects%20PDE%20information%20flow%2C%20even%20for%20elliptic%20PDEs.%0A%20%20On%20AirFRANS%20%28steady%20incompressible%20RANS%20over%20NACA%20airfoils%29%2C%20GLOBE%20achieves%20substantial%20accuracy%20improvements.%20On%20the%20%22Full%22%20split%2C%20it%20reduces%20mean-squared%20error%20by%20roughly%20200x%20on%20all%20fields%20relative%20to%20the%20dataset%27s%20reference%20baselines%2C%20and%20roughly%2050x%20relative%20to%20the%20next-best-performing%20model.%20In%20the%20%22Scarce%22%20split%2C%20it%20achieves%20over%20100x%20lower%20error%20on%20velocity%20and%20pressure%20fields%20and%20over%20600x%20lower%20error%20on%20surface%20pressure%20than%20Transolver.%20Qualitative%20results%20show%20sharp%20near-wall%20gradients%2C%20coherent%20wakes%2C%20and%20limited%20errors%20under%20modest%20extrapolation%20in%20Reynolds%20number%20and%20angle%20of%20attack.%0A%20%20In%20addition%20to%20this%20accuracy%2C%20the%20model%20is%20quite%20compact%20%28117k%20parameters%29%2C%20and%20fields%20can%20be%20evaluated%20at%20arbitrary%20points%20during%20inference.%20We%20also%20demonstrate%20the%20ability%20to%20train%20and%20predict%20with%20non-watertight%20meshes%2C%20which%20has%20strong%20practical%20implications.%0A%20%20These%20results%20show%20that%20rigorous%20physics-%20and%20domain-inspired%20inductive%20biases%20can%20achieve%20large%20gains%20in%20accuracy%2C%20generalizability%2C%20and%20practicality%20for%20ML-based%20PDE%20surrogates%20for%20industrial%20computer-aided%20engineering%20%28CAE%29.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15856v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLOBE%253A%2520Accurate%2520and%2520Generalizable%2520PDE%2520Surrogates%2520using%2520Domain-Inspired%2520Architectures%2520and%2520Equivariances%26entry.906535625%3DPeter%2520Sharpe%26entry.1292438233%3DWe%2520introduce%2520GLOBE%252C%2520a%2520new%2520neural%2520surrogate%2520for%2520homogeneous%2520PDEs%2520that%2520draws%2520inductive%2520bias%2520from%2520boundary-element%2520methods%2520and%2520equivariant%2520ML.%2520GLOBE%2520represents%2520solutions%2520as%2520superpositions%2520of%2520learnable%2520Green%2527s-function-like%2520kernels%2520evaluated%2520from%2520boundary%2520faces%2520to%2520targets%252C%2520composed%2520across%2520multiscale%2520branches%2520and%2520communication%2520hyperlayers.%2520The%2520architecture%2520is%2520translation-%252C%2520rotation-%252C%2520and%2520parity-equivariant%253B%2520discretization-invariant%2520in%2520the%2520fine-mesh%2520limit%253B%2520and%2520units-invariant%2520via%2520rigorous%2520nondimensionalization.%2520An%2520explicit%2520far-field%2520decay%2520envelope%2520stabilizes%2520extrapolation%252C%2520boundary-to-boundary%2520hyperlayer%2520communication%2520mediates%2520long-range%2520coupling%252C%2520and%2520the%2520all-to-all%2520boundary-to-target%2520evaluation%2520yields%2520a%2520global%2520receptive%2520field%2520that%2520respects%2520PDE%2520information%2520flow%252C%2520even%2520for%2520elliptic%2520PDEs.%250A%2520%2520On%2520AirFRANS%2520%2528steady%2520incompressible%2520RANS%2520over%2520NACA%2520airfoils%2529%252C%2520GLOBE%2520achieves%2520substantial%2520accuracy%2520improvements.%2520On%2520the%2520%2522Full%2522%2520split%252C%2520it%2520reduces%2520mean-squared%2520error%2520by%2520roughly%2520200x%2520on%2520all%2520fields%2520relative%2520to%2520the%2520dataset%2527s%2520reference%2520baselines%252C%2520and%2520roughly%252050x%2520relative%2520to%2520the%2520next-best-performing%2520model.%2520In%2520the%2520%2522Scarce%2522%2520split%252C%2520it%2520achieves%2520over%2520100x%2520lower%2520error%2520on%2520velocity%2520and%2520pressure%2520fields%2520and%2520over%2520600x%2520lower%2520error%2520on%2520surface%2520pressure%2520than%2520Transolver.%2520Qualitative%2520results%2520show%2520sharp%2520near-wall%2520gradients%252C%2520coherent%2520wakes%252C%2520and%2520limited%2520errors%2520under%2520modest%2520extrapolation%2520in%2520Reynolds%2520number%2520and%2520angle%2520of%2520attack.%250A%2520%2520In%2520addition%2520to%2520this%2520accuracy%252C%2520the%2520model%2520is%2520quite%2520compact%2520%2528117k%2520parameters%2529%252C%2520and%2520fields%2520can%2520be%2520evaluated%2520at%2520arbitrary%2520points%2520during%2520inference.%2520We%2520also%2520demonstrate%2520the%2520ability%2520to%2520train%2520and%2520predict%2520with%2520non-watertight%2520meshes%252C%2520which%2520has%2520strong%2520practical%2520implications.%250A%2520%2520These%2520results%2520show%2520that%2520rigorous%2520physics-%2520and%2520domain-inspired%2520inductive%2520biases%2520can%2520achieve%2520large%2520gains%2520in%2520accuracy%252C%2520generalizability%252C%2520and%2520practicality%2520for%2520ML-based%2520PDE%2520surrogates%2520for%2520industrial%2520computer-aided%2520engineering%2520%2528CAE%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15856v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLOBE%3A%20Accurate%20and%20Generalizable%20PDE%20Surrogates%20using%20Domain-Inspired%20Architectures%20and%20Equivariances&entry.906535625=Peter%20Sharpe&entry.1292438233=We%20introduce%20GLOBE%2C%20a%20new%20neural%20surrogate%20for%20homogeneous%20PDEs%20that%20draws%20inductive%20bias%20from%20boundary-element%20methods%20and%20equivariant%20ML.%20GLOBE%20represents%20solutions%20as%20superpositions%20of%20learnable%20Green%27s-function-like%20kernels%20evaluated%20from%20boundary%20faces%20to%20targets%2C%20composed%20across%20multiscale%20branches%20and%20communication%20hyperlayers.%20The%20architecture%20is%20translation-%2C%20rotation-%2C%20and%20parity-equivariant%3B%20discretization-invariant%20in%20the%20fine-mesh%20limit%3B%20and%20units-invariant%20via%20rigorous%20nondimensionalization.%20An%20explicit%20far-field%20decay%20envelope%20stabilizes%20extrapolation%2C%20boundary-to-boundary%20hyperlayer%20communication%20mediates%20long-range%20coupling%2C%20and%20the%20all-to-all%20boundary-to-target%20evaluation%20yields%20a%20global%20receptive%20field%20that%20respects%20PDE%20information%20flow%2C%20even%20for%20elliptic%20PDEs.%0A%20%20On%20AirFRANS%20%28steady%20incompressible%20RANS%20over%20NACA%20airfoils%29%2C%20GLOBE%20achieves%20substantial%20accuracy%20improvements.%20On%20the%20%22Full%22%20split%2C%20it%20reduces%20mean-squared%20error%20by%20roughly%20200x%20on%20all%20fields%20relative%20to%20the%20dataset%27s%20reference%20baselines%2C%20and%20roughly%2050x%20relative%20to%20the%20next-best-performing%20model.%20In%20the%20%22Scarce%22%20split%2C%20it%20achieves%20over%20100x%20lower%20error%20on%20velocity%20and%20pressure%20fields%20and%20over%20600x%20lower%20error%20on%20surface%20pressure%20than%20Transolver.%20Qualitative%20results%20show%20sharp%20near-wall%20gradients%2C%20coherent%20wakes%2C%20and%20limited%20errors%20under%20modest%20extrapolation%20in%20Reynolds%20number%20and%20angle%20of%20attack.%0A%20%20In%20addition%20to%20this%20accuracy%2C%20the%20model%20is%20quite%20compact%20%28117k%20parameters%29%2C%20and%20fields%20can%20be%20evaluated%20at%20arbitrary%20points%20during%20inference.%20We%20also%20demonstrate%20the%20ability%20to%20train%20and%20predict%20with%20non-watertight%20meshes%2C%20which%20has%20strong%20practical%20implications.%0A%20%20These%20results%20show%20that%20rigorous%20physics-%20and%20domain-inspired%20inductive%20biases%20can%20achieve%20large%20gains%20in%20accuracy%2C%20generalizability%2C%20and%20practicality%20for%20ML-based%20PDE%20surrogates%20for%20industrial%20computer-aided%20engineering%20%28CAE%29.&entry.1838667208=http%3A//arxiv.org/abs/2511.15856v2&entry.124074799=Read"},
{"title": "UAM: A Unified Attention-Mamba Backbone of Multimodal Framework for Tumor Cell Classification", "author": "Taixi Chen and Jingyun Chen and Nancy Guo", "abstract": "Cell-level radiomics features provide fine-grained insights into tumor phenotypes and have the potential to significantly enhance diagnostic accuracy on hematoxylin and eosin (H&E) images. By capturing micro-level morphological and intensity patterns, these features support more precise tumor identification and improve AI interpretability by highlighting diagnostically relevant cells for pathologist review. However, most existing studies focus on slide-level or patch-level tumor classification, leaving cell-level radiomics analysis largely unexplored. Moreover, there is currently no dedicated backbone specifically designed for radiomics data. Inspired by the recent success of the Mamba architecture in vision and language domains, we introduce a Unified Attention-Mamba (UAM) backbone for cell-level classification using radiomics features. Unlike previous hybrid approaches that integrate Attention and Mamba modules in fixed proportions, our unified design flexibly combines their capabilities within a single cohesive architecture, eliminating the need for manual ratio tuning and improving encode capability. We develop two UAM variants to comprehensively evaluate the benefits of this unified structure. Building on this backbone, we further propose a multimodal UAM framework that jointly performs cell-level classification and image segmentation. Experimental results demonstrate that UAM achieves state-of-the-art performance across both tasks on public benchmarks, surpassing leading image-based foundation models. It improves cell classification accuracy from 74% to 78% ($n$=349,882 cells), and tumor segmentation precision from 75% to 80% ($n$=406 patches). These findings highlight the effectiveness and promise of UAM as a unified and extensible multimodal foundation for radiomics-driven cancer diagnosis.", "link": "http://arxiv.org/abs/2511.17355v1", "date": "2025-11-21", "relevancy": 2.1464, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5507}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5384}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UAM%3A%20A%20Unified%20Attention-Mamba%20Backbone%20of%20Multimodal%20Framework%20for%20Tumor%20Cell%20Classification&body=Title%3A%20UAM%3A%20A%20Unified%20Attention-Mamba%20Backbone%20of%20Multimodal%20Framework%20for%20Tumor%20Cell%20Classification%0AAuthor%3A%20Taixi%20Chen%20and%20Jingyun%20Chen%20and%20Nancy%20Guo%0AAbstract%3A%20Cell-level%20radiomics%20features%20provide%20fine-grained%20insights%20into%20tumor%20phenotypes%20and%20have%20the%20potential%20to%20significantly%20enhance%20diagnostic%20accuracy%20on%20hematoxylin%20and%20eosin%20%28H%26E%29%20images.%20By%20capturing%20micro-level%20morphological%20and%20intensity%20patterns%2C%20these%20features%20support%20more%20precise%20tumor%20identification%20and%20improve%20AI%20interpretability%20by%20highlighting%20diagnostically%20relevant%20cells%20for%20pathologist%20review.%20However%2C%20most%20existing%20studies%20focus%20on%20slide-level%20or%20patch-level%20tumor%20classification%2C%20leaving%20cell-level%20radiomics%20analysis%20largely%20unexplored.%20Moreover%2C%20there%20is%20currently%20no%20dedicated%20backbone%20specifically%20designed%20for%20radiomics%20data.%20Inspired%20by%20the%20recent%20success%20of%20the%20Mamba%20architecture%20in%20vision%20and%20language%20domains%2C%20we%20introduce%20a%20Unified%20Attention-Mamba%20%28UAM%29%20backbone%20for%20cell-level%20classification%20using%20radiomics%20features.%20Unlike%20previous%20hybrid%20approaches%20that%20integrate%20Attention%20and%20Mamba%20modules%20in%20fixed%20proportions%2C%20our%20unified%20design%20flexibly%20combines%20their%20capabilities%20within%20a%20single%20cohesive%20architecture%2C%20eliminating%20the%20need%20for%20manual%20ratio%20tuning%20and%20improving%20encode%20capability.%20We%20develop%20two%20UAM%20variants%20to%20comprehensively%20evaluate%20the%20benefits%20of%20this%20unified%20structure.%20Building%20on%20this%20backbone%2C%20we%20further%20propose%20a%20multimodal%20UAM%20framework%20that%20jointly%20performs%20cell-level%20classification%20and%20image%20segmentation.%20Experimental%20results%20demonstrate%20that%20UAM%20achieves%20state-of-the-art%20performance%20across%20both%20tasks%20on%20public%20benchmarks%2C%20surpassing%20leading%20image-based%20foundation%20models.%20It%20improves%20cell%20classification%20accuracy%20from%2074%25%20to%2078%25%20%28%24n%24%3D349%2C882%20cells%29%2C%20and%20tumor%20segmentation%20precision%20from%2075%25%20to%2080%25%20%28%24n%24%3D406%20patches%29.%20These%20findings%20highlight%20the%20effectiveness%20and%20promise%20of%20UAM%20as%20a%20unified%20and%20extensible%20multimodal%20foundation%20for%20radiomics-driven%20cancer%20diagnosis.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17355v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUAM%253A%2520A%2520Unified%2520Attention-Mamba%2520Backbone%2520of%2520Multimodal%2520Framework%2520for%2520Tumor%2520Cell%2520Classification%26entry.906535625%3DTaixi%2520Chen%2520and%2520Jingyun%2520Chen%2520and%2520Nancy%2520Guo%26entry.1292438233%3DCell-level%2520radiomics%2520features%2520provide%2520fine-grained%2520insights%2520into%2520tumor%2520phenotypes%2520and%2520have%2520the%2520potential%2520to%2520significantly%2520enhance%2520diagnostic%2520accuracy%2520on%2520hematoxylin%2520and%2520eosin%2520%2528H%2526E%2529%2520images.%2520By%2520capturing%2520micro-level%2520morphological%2520and%2520intensity%2520patterns%252C%2520these%2520features%2520support%2520more%2520precise%2520tumor%2520identification%2520and%2520improve%2520AI%2520interpretability%2520by%2520highlighting%2520diagnostically%2520relevant%2520cells%2520for%2520pathologist%2520review.%2520However%252C%2520most%2520existing%2520studies%2520focus%2520on%2520slide-level%2520or%2520patch-level%2520tumor%2520classification%252C%2520leaving%2520cell-level%2520radiomics%2520analysis%2520largely%2520unexplored.%2520Moreover%252C%2520there%2520is%2520currently%2520no%2520dedicated%2520backbone%2520specifically%2520designed%2520for%2520radiomics%2520data.%2520Inspired%2520by%2520the%2520recent%2520success%2520of%2520the%2520Mamba%2520architecture%2520in%2520vision%2520and%2520language%2520domains%252C%2520we%2520introduce%2520a%2520Unified%2520Attention-Mamba%2520%2528UAM%2529%2520backbone%2520for%2520cell-level%2520classification%2520using%2520radiomics%2520features.%2520Unlike%2520previous%2520hybrid%2520approaches%2520that%2520integrate%2520Attention%2520and%2520Mamba%2520modules%2520in%2520fixed%2520proportions%252C%2520our%2520unified%2520design%2520flexibly%2520combines%2520their%2520capabilities%2520within%2520a%2520single%2520cohesive%2520architecture%252C%2520eliminating%2520the%2520need%2520for%2520manual%2520ratio%2520tuning%2520and%2520improving%2520encode%2520capability.%2520We%2520develop%2520two%2520UAM%2520variants%2520to%2520comprehensively%2520evaluate%2520the%2520benefits%2520of%2520this%2520unified%2520structure.%2520Building%2520on%2520this%2520backbone%252C%2520we%2520further%2520propose%2520a%2520multimodal%2520UAM%2520framework%2520that%2520jointly%2520performs%2520cell-level%2520classification%2520and%2520image%2520segmentation.%2520Experimental%2520results%2520demonstrate%2520that%2520UAM%2520achieves%2520state-of-the-art%2520performance%2520across%2520both%2520tasks%2520on%2520public%2520benchmarks%252C%2520surpassing%2520leading%2520image-based%2520foundation%2520models.%2520It%2520improves%2520cell%2520classification%2520accuracy%2520from%252074%2525%2520to%252078%2525%2520%2528%2524n%2524%253D349%252C882%2520cells%2529%252C%2520and%2520tumor%2520segmentation%2520precision%2520from%252075%2525%2520to%252080%2525%2520%2528%2524n%2524%253D406%2520patches%2529.%2520These%2520findings%2520highlight%2520the%2520effectiveness%2520and%2520promise%2520of%2520UAM%2520as%2520a%2520unified%2520and%2520extensible%2520multimodal%2520foundation%2520for%2520radiomics-driven%2520cancer%2520diagnosis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17355v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UAM%3A%20A%20Unified%20Attention-Mamba%20Backbone%20of%20Multimodal%20Framework%20for%20Tumor%20Cell%20Classification&entry.906535625=Taixi%20Chen%20and%20Jingyun%20Chen%20and%20Nancy%20Guo&entry.1292438233=Cell-level%20radiomics%20features%20provide%20fine-grained%20insights%20into%20tumor%20phenotypes%20and%20have%20the%20potential%20to%20significantly%20enhance%20diagnostic%20accuracy%20on%20hematoxylin%20and%20eosin%20%28H%26E%29%20images.%20By%20capturing%20micro-level%20morphological%20and%20intensity%20patterns%2C%20these%20features%20support%20more%20precise%20tumor%20identification%20and%20improve%20AI%20interpretability%20by%20highlighting%20diagnostically%20relevant%20cells%20for%20pathologist%20review.%20However%2C%20most%20existing%20studies%20focus%20on%20slide-level%20or%20patch-level%20tumor%20classification%2C%20leaving%20cell-level%20radiomics%20analysis%20largely%20unexplored.%20Moreover%2C%20there%20is%20currently%20no%20dedicated%20backbone%20specifically%20designed%20for%20radiomics%20data.%20Inspired%20by%20the%20recent%20success%20of%20the%20Mamba%20architecture%20in%20vision%20and%20language%20domains%2C%20we%20introduce%20a%20Unified%20Attention-Mamba%20%28UAM%29%20backbone%20for%20cell-level%20classification%20using%20radiomics%20features.%20Unlike%20previous%20hybrid%20approaches%20that%20integrate%20Attention%20and%20Mamba%20modules%20in%20fixed%20proportions%2C%20our%20unified%20design%20flexibly%20combines%20their%20capabilities%20within%20a%20single%20cohesive%20architecture%2C%20eliminating%20the%20need%20for%20manual%20ratio%20tuning%20and%20improving%20encode%20capability.%20We%20develop%20two%20UAM%20variants%20to%20comprehensively%20evaluate%20the%20benefits%20of%20this%20unified%20structure.%20Building%20on%20this%20backbone%2C%20we%20further%20propose%20a%20multimodal%20UAM%20framework%20that%20jointly%20performs%20cell-level%20classification%20and%20image%20segmentation.%20Experimental%20results%20demonstrate%20that%20UAM%20achieves%20state-of-the-art%20performance%20across%20both%20tasks%20on%20public%20benchmarks%2C%20surpassing%20leading%20image-based%20foundation%20models.%20It%20improves%20cell%20classification%20accuracy%20from%2074%25%20to%2078%25%20%28%24n%24%3D349%2C882%20cells%29%2C%20and%20tumor%20segmentation%20precision%20from%2075%25%20to%2080%25%20%28%24n%24%3D406%20patches%29.%20These%20findings%20highlight%20the%20effectiveness%20and%20promise%20of%20UAM%20as%20a%20unified%20and%20extensible%20multimodal%20foundation%20for%20radiomics-driven%20cancer%20diagnosis.&entry.1838667208=http%3A//arxiv.org/abs/2511.17355v1&entry.124074799=Read"},
{"title": "A lightweight detector for real-time detection of remote sensing images", "author": "Qianyi Wang and Guoqiang Ren", "abstract": "Remote sensing imagery is widely used across various fields, yet real-time detection remains challenging due to the prevalence of small objects and the need to balance accuracy with efficiency. To address this, we propose DMG-YOLO, a lightweight real-time detector tailored for small object detection in remote sensing images. Specifically, we design a Dual-branch Feature Extraction (DFE) module in the backbone, which partitions feature maps into two parallel branches: one extracts local features via depthwise separable convolutions, and the other captures global context using a vision transformer with a gating mechanism. Additionally, a Multi-scale Feature Fusion (MFF) module with dilated convolutions enhances multi-scale integration while preserving fine details. In the neck, we introduce the Global and Local Aggregate Feature Pyramid Network (GLAFPN) to further boost small object detection through global-local feature fusion. Extensive experiments on the VisDrone2019 and NWPU VHR-10 datasets show that DMG-YOLO achieves competitive performance in terms of mAP, model size, and other key metrics.", "link": "http://arxiv.org/abs/2511.17147v1", "date": "2025-11-21", "relevancy": 2.1448, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5734}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5195}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20lightweight%20detector%20for%20real-time%20detection%20of%20remote%20sensing%20images&body=Title%3A%20A%20lightweight%20detector%20for%20real-time%20detection%20of%20remote%20sensing%20images%0AAuthor%3A%20Qianyi%20Wang%20and%20Guoqiang%20Ren%0AAbstract%3A%20Remote%20sensing%20imagery%20is%20widely%20used%20across%20various%20fields%2C%20yet%20real-time%20detection%20remains%20challenging%20due%20to%20the%20prevalence%20of%20small%20objects%20and%20the%20need%20to%20balance%20accuracy%20with%20efficiency.%20To%20address%20this%2C%20we%20propose%20DMG-YOLO%2C%20a%20lightweight%20real-time%20detector%20tailored%20for%20small%20object%20detection%20in%20remote%20sensing%20images.%20Specifically%2C%20we%20design%20a%20Dual-branch%20Feature%20Extraction%20%28DFE%29%20module%20in%20the%20backbone%2C%20which%20partitions%20feature%20maps%20into%20two%20parallel%20branches%3A%20one%20extracts%20local%20features%20via%20depthwise%20separable%20convolutions%2C%20and%20the%20other%20captures%20global%20context%20using%20a%20vision%20transformer%20with%20a%20gating%20mechanism.%20Additionally%2C%20a%20Multi-scale%20Feature%20Fusion%20%28MFF%29%20module%20with%20dilated%20convolutions%20enhances%20multi-scale%20integration%20while%20preserving%20fine%20details.%20In%20the%20neck%2C%20we%20introduce%20the%20Global%20and%20Local%20Aggregate%20Feature%20Pyramid%20Network%20%28GLAFPN%29%20to%20further%20boost%20small%20object%20detection%20through%20global-local%20feature%20fusion.%20Extensive%20experiments%20on%20the%20VisDrone2019%20and%20NWPU%20VHR-10%20datasets%20show%20that%20DMG-YOLO%20achieves%20competitive%20performance%20in%20terms%20of%20mAP%2C%20model%20size%2C%20and%20other%20key%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520lightweight%2520detector%2520for%2520real-time%2520detection%2520of%2520remote%2520sensing%2520images%26entry.906535625%3DQianyi%2520Wang%2520and%2520Guoqiang%2520Ren%26entry.1292438233%3DRemote%2520sensing%2520imagery%2520is%2520widely%2520used%2520across%2520various%2520fields%252C%2520yet%2520real-time%2520detection%2520remains%2520challenging%2520due%2520to%2520the%2520prevalence%2520of%2520small%2520objects%2520and%2520the%2520need%2520to%2520balance%2520accuracy%2520with%2520efficiency.%2520To%2520address%2520this%252C%2520we%2520propose%2520DMG-YOLO%252C%2520a%2520lightweight%2520real-time%2520detector%2520tailored%2520for%2520small%2520object%2520detection%2520in%2520remote%2520sensing%2520images.%2520Specifically%252C%2520we%2520design%2520a%2520Dual-branch%2520Feature%2520Extraction%2520%2528DFE%2529%2520module%2520in%2520the%2520backbone%252C%2520which%2520partitions%2520feature%2520maps%2520into%2520two%2520parallel%2520branches%253A%2520one%2520extracts%2520local%2520features%2520via%2520depthwise%2520separable%2520convolutions%252C%2520and%2520the%2520other%2520captures%2520global%2520context%2520using%2520a%2520vision%2520transformer%2520with%2520a%2520gating%2520mechanism.%2520Additionally%252C%2520a%2520Multi-scale%2520Feature%2520Fusion%2520%2528MFF%2529%2520module%2520with%2520dilated%2520convolutions%2520enhances%2520multi-scale%2520integration%2520while%2520preserving%2520fine%2520details.%2520In%2520the%2520neck%252C%2520we%2520introduce%2520the%2520Global%2520and%2520Local%2520Aggregate%2520Feature%2520Pyramid%2520Network%2520%2528GLAFPN%2529%2520to%2520further%2520boost%2520small%2520object%2520detection%2520through%2520global-local%2520feature%2520fusion.%2520Extensive%2520experiments%2520on%2520the%2520VisDrone2019%2520and%2520NWPU%2520VHR-10%2520datasets%2520show%2520that%2520DMG-YOLO%2520achieves%2520competitive%2520performance%2520in%2520terms%2520of%2520mAP%252C%2520model%2520size%252C%2520and%2520other%2520key%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20lightweight%20detector%20for%20real-time%20detection%20of%20remote%20sensing%20images&entry.906535625=Qianyi%20Wang%20and%20Guoqiang%20Ren&entry.1292438233=Remote%20sensing%20imagery%20is%20widely%20used%20across%20various%20fields%2C%20yet%20real-time%20detection%20remains%20challenging%20due%20to%20the%20prevalence%20of%20small%20objects%20and%20the%20need%20to%20balance%20accuracy%20with%20efficiency.%20To%20address%20this%2C%20we%20propose%20DMG-YOLO%2C%20a%20lightweight%20real-time%20detector%20tailored%20for%20small%20object%20detection%20in%20remote%20sensing%20images.%20Specifically%2C%20we%20design%20a%20Dual-branch%20Feature%20Extraction%20%28DFE%29%20module%20in%20the%20backbone%2C%20which%20partitions%20feature%20maps%20into%20two%20parallel%20branches%3A%20one%20extracts%20local%20features%20via%20depthwise%20separable%20convolutions%2C%20and%20the%20other%20captures%20global%20context%20using%20a%20vision%20transformer%20with%20a%20gating%20mechanism.%20Additionally%2C%20a%20Multi-scale%20Feature%20Fusion%20%28MFF%29%20module%20with%20dilated%20convolutions%20enhances%20multi-scale%20integration%20while%20preserving%20fine%20details.%20In%20the%20neck%2C%20we%20introduce%20the%20Global%20and%20Local%20Aggregate%20Feature%20Pyramid%20Network%20%28GLAFPN%29%20to%20further%20boost%20small%20object%20detection%20through%20global-local%20feature%20fusion.%20Extensive%20experiments%20on%20the%20VisDrone2019%20and%20NWPU%20VHR-10%20datasets%20show%20that%20DMG-YOLO%20achieves%20competitive%20performance%20in%20terms%20of%20mAP%2C%20model%20size%2C%20and%20other%20key%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2511.17147v1&entry.124074799=Read"},
{"title": "The PLLuM Instruction Corpus", "author": "Piotr P\u0119zik and Filip \u017barnecki and Konrad Kaczy\u0144ski and Anna Cichosz and Zuzanna Deckert and Monika Garnys and Izabela Grabarczyk and Wojciech Janowski and Sylwia Karasi\u0144ska and Aleksandra Kujawiak and Piotr Misztela and Maria Szyma\u0144ska and Karolina Walkusz and Igor Siek and Maciej Chrab\u0105szcz and Anna Ko\u0142os and Agnieszka Karli\u0144ska and Karolina Seweryn and Aleksandra Krasnod\u0119bska and Paula Betscher and Zofia Cie\u015bli\u0144ska and Katarzyna Kowol and Artur Wilczek and Maciej Trzci\u0144ski and Katarzyna Dziewulska and Roman Roszko and Tomasz Berna\u015b and Jurgita Vai\u010denonien\u0117 and Danuta Roszko and Pawe\u0142 Levchuk and Pawe\u0142 Kowalski and Irena Prawdzic-Jankowska and Marek Koz\u0142owski and S\u0142awomir Dadas and Rafa\u0142 Po\u015bwiata and Alina Wr\u00f3blewska and Katarzyna Krasnowska-Kiera\u015b and Maciej Ogrodniczuk and Micha\u0142 Rudolf and Piotr Rybak and Karolina Saputa and Joanna Wo\u0142oszyn and Marcin Oleksy and Bart\u0142omiej Koptyra and Teddy Ferdinan and Stanis\u0142aw Wo\u017aniak and Maciej Piasecki and Pawe\u0142 Walkowiak and Konrad Wojtasik and Arkadiusz Janz and Przemys\u0142aw Kazienko and Julia Moska and Jan Koco\u0144", "abstract": "This paper describes the instruction dataset used to fine-tune a set of transformer-based large language models (LLMs) developed in the PLLuM (Polish Large Language Model) project. We present a functional typology of the organic, converted, and synthetic instructions used in PLLuM and share some observations about the implications of using human-authored versus synthetic instruction datasets in the linguistic adaptation of base LLMs. Additionally, we release the first representative subset of the PLLuM instruction corpus (PLLuMIC), which we believe to be useful in guiding and planning the development of similar datasets for other LLMs.", "link": "http://arxiv.org/abs/2511.17161v1", "date": "2025-11-21", "relevancy": 2.1435, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4433}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4214}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20PLLuM%20Instruction%20Corpus&body=Title%3A%20The%20PLLuM%20Instruction%20Corpus%0AAuthor%3A%20Piotr%20P%C4%99zik%20and%20Filip%20%C5%BBarnecki%20and%20Konrad%20Kaczy%C5%84ski%20and%20Anna%20Cichosz%20and%20Zuzanna%20Deckert%20and%20Monika%20Garnys%20and%20Izabela%20Grabarczyk%20and%20Wojciech%20Janowski%20and%20Sylwia%20Karasi%C5%84ska%20and%20Aleksandra%20Kujawiak%20and%20Piotr%20Misztela%20and%20Maria%20Szyma%C5%84ska%20and%20Karolina%20Walkusz%20and%20Igor%20Siek%20and%20Maciej%20Chrab%C4%85szcz%20and%20Anna%20Ko%C5%82os%20and%20Agnieszka%20Karli%C5%84ska%20and%20Karolina%20Seweryn%20and%20Aleksandra%20Krasnod%C4%99bska%20and%20Paula%20Betscher%20and%20Zofia%20Cie%C5%9Bli%C5%84ska%20and%20Katarzyna%20Kowol%20and%20Artur%20Wilczek%20and%20Maciej%20Trzci%C5%84ski%20and%20Katarzyna%20Dziewulska%20and%20Roman%20Roszko%20and%20Tomasz%20Berna%C5%9B%20and%20Jurgita%20Vai%C4%8Denonien%C4%97%20and%20Danuta%20Roszko%20and%20Pawe%C5%82%20Levchuk%20and%20Pawe%C5%82%20Kowalski%20and%20Irena%20Prawdzic-Jankowska%20and%20Marek%20Koz%C5%82owski%20and%20S%C5%82awomir%20Dadas%20and%20Rafa%C5%82%20Po%C5%9Bwiata%20and%20Alina%20Wr%C3%B3blewska%20and%20Katarzyna%20Krasnowska-Kiera%C5%9B%20and%20Maciej%20Ogrodniczuk%20and%20Micha%C5%82%20Rudolf%20and%20Piotr%20Rybak%20and%20Karolina%20Saputa%20and%20Joanna%20Wo%C5%82oszyn%20and%20Marcin%20Oleksy%20and%20Bart%C5%82omiej%20Koptyra%20and%20Teddy%20Ferdinan%20and%20Stanis%C5%82aw%20Wo%C5%BAniak%20and%20Maciej%20Piasecki%20and%20Pawe%C5%82%20Walkowiak%20and%20Konrad%20Wojtasik%20and%20Arkadiusz%20Janz%20and%20Przemys%C5%82aw%20Kazienko%20and%20Julia%20Moska%20and%20Jan%20Koco%C5%84%0AAbstract%3A%20This%20paper%20describes%20the%20instruction%20dataset%20used%20to%20fine-tune%20a%20set%20of%20transformer-based%20large%20language%20models%20%28LLMs%29%20developed%20in%20the%20PLLuM%20%28Polish%20Large%20Language%20Model%29%20project.%20We%20present%20a%20functional%20typology%20of%20the%20organic%2C%20converted%2C%20and%20synthetic%20instructions%20used%20in%20PLLuM%20and%20share%20some%20observations%20about%20the%20implications%20of%20using%20human-authored%20versus%20synthetic%20instruction%20datasets%20in%20the%20linguistic%20adaptation%20of%20base%20LLMs.%20Additionally%2C%20we%20release%20the%20first%20representative%20subset%20of%20the%20PLLuM%20instruction%20corpus%20%28PLLuMIC%29%2C%20which%20we%20believe%20to%20be%20useful%20in%20guiding%20and%20planning%20the%20development%20of%20similar%20datasets%20for%20other%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17161v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520PLLuM%2520Instruction%2520Corpus%26entry.906535625%3DPiotr%2520P%25C4%2599zik%2520and%2520Filip%2520%25C5%25BBarnecki%2520and%2520Konrad%2520Kaczy%25C5%2584ski%2520and%2520Anna%2520Cichosz%2520and%2520Zuzanna%2520Deckert%2520and%2520Monika%2520Garnys%2520and%2520Izabela%2520Grabarczyk%2520and%2520Wojciech%2520Janowski%2520and%2520Sylwia%2520Karasi%25C5%2584ska%2520and%2520Aleksandra%2520Kujawiak%2520and%2520Piotr%2520Misztela%2520and%2520Maria%2520Szyma%25C5%2584ska%2520and%2520Karolina%2520Walkusz%2520and%2520Igor%2520Siek%2520and%2520Maciej%2520Chrab%25C4%2585szcz%2520and%2520Anna%2520Ko%25C5%2582os%2520and%2520Agnieszka%2520Karli%25C5%2584ska%2520and%2520Karolina%2520Seweryn%2520and%2520Aleksandra%2520Krasnod%25C4%2599bska%2520and%2520Paula%2520Betscher%2520and%2520Zofia%2520Cie%25C5%259Bli%25C5%2584ska%2520and%2520Katarzyna%2520Kowol%2520and%2520Artur%2520Wilczek%2520and%2520Maciej%2520Trzci%25C5%2584ski%2520and%2520Katarzyna%2520Dziewulska%2520and%2520Roman%2520Roszko%2520and%2520Tomasz%2520Berna%25C5%259B%2520and%2520Jurgita%2520Vai%25C4%258Denonien%25C4%2597%2520and%2520Danuta%2520Roszko%2520and%2520Pawe%25C5%2582%2520Levchuk%2520and%2520Pawe%25C5%2582%2520Kowalski%2520and%2520Irena%2520Prawdzic-Jankowska%2520and%2520Marek%2520Koz%25C5%2582owski%2520and%2520S%25C5%2582awomir%2520Dadas%2520and%2520Rafa%25C5%2582%2520Po%25C5%259Bwiata%2520and%2520Alina%2520Wr%25C3%25B3blewska%2520and%2520Katarzyna%2520Krasnowska-Kiera%25C5%259B%2520and%2520Maciej%2520Ogrodniczuk%2520and%2520Micha%25C5%2582%2520Rudolf%2520and%2520Piotr%2520Rybak%2520and%2520Karolina%2520Saputa%2520and%2520Joanna%2520Wo%25C5%2582oszyn%2520and%2520Marcin%2520Oleksy%2520and%2520Bart%25C5%2582omiej%2520Koptyra%2520and%2520Teddy%2520Ferdinan%2520and%2520Stanis%25C5%2582aw%2520Wo%25C5%25BAniak%2520and%2520Maciej%2520Piasecki%2520and%2520Pawe%25C5%2582%2520Walkowiak%2520and%2520Konrad%2520Wojtasik%2520and%2520Arkadiusz%2520Janz%2520and%2520Przemys%25C5%2582aw%2520Kazienko%2520and%2520Julia%2520Moska%2520and%2520Jan%2520Koco%25C5%2584%26entry.1292438233%3DThis%2520paper%2520describes%2520the%2520instruction%2520dataset%2520used%2520to%2520fine-tune%2520a%2520set%2520of%2520transformer-based%2520large%2520language%2520models%2520%2528LLMs%2529%2520developed%2520in%2520the%2520PLLuM%2520%2528Polish%2520Large%2520Language%2520Model%2529%2520project.%2520We%2520present%2520a%2520functional%2520typology%2520of%2520the%2520organic%252C%2520converted%252C%2520and%2520synthetic%2520instructions%2520used%2520in%2520PLLuM%2520and%2520share%2520some%2520observations%2520about%2520the%2520implications%2520of%2520using%2520human-authored%2520versus%2520synthetic%2520instruction%2520datasets%2520in%2520the%2520linguistic%2520adaptation%2520of%2520base%2520LLMs.%2520Additionally%252C%2520we%2520release%2520the%2520first%2520representative%2520subset%2520of%2520the%2520PLLuM%2520instruction%2520corpus%2520%2528PLLuMIC%2529%252C%2520which%2520we%2520believe%2520to%2520be%2520useful%2520in%2520guiding%2520and%2520planning%2520the%2520development%2520of%2520similar%2520datasets%2520for%2520other%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17161v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20PLLuM%20Instruction%20Corpus&entry.906535625=Piotr%20P%C4%99zik%20and%20Filip%20%C5%BBarnecki%20and%20Konrad%20Kaczy%C5%84ski%20and%20Anna%20Cichosz%20and%20Zuzanna%20Deckert%20and%20Monika%20Garnys%20and%20Izabela%20Grabarczyk%20and%20Wojciech%20Janowski%20and%20Sylwia%20Karasi%C5%84ska%20and%20Aleksandra%20Kujawiak%20and%20Piotr%20Misztela%20and%20Maria%20Szyma%C5%84ska%20and%20Karolina%20Walkusz%20and%20Igor%20Siek%20and%20Maciej%20Chrab%C4%85szcz%20and%20Anna%20Ko%C5%82os%20and%20Agnieszka%20Karli%C5%84ska%20and%20Karolina%20Seweryn%20and%20Aleksandra%20Krasnod%C4%99bska%20and%20Paula%20Betscher%20and%20Zofia%20Cie%C5%9Bli%C5%84ska%20and%20Katarzyna%20Kowol%20and%20Artur%20Wilczek%20and%20Maciej%20Trzci%C5%84ski%20and%20Katarzyna%20Dziewulska%20and%20Roman%20Roszko%20and%20Tomasz%20Berna%C5%9B%20and%20Jurgita%20Vai%C4%8Denonien%C4%97%20and%20Danuta%20Roszko%20and%20Pawe%C5%82%20Levchuk%20and%20Pawe%C5%82%20Kowalski%20and%20Irena%20Prawdzic-Jankowska%20and%20Marek%20Koz%C5%82owski%20and%20S%C5%82awomir%20Dadas%20and%20Rafa%C5%82%20Po%C5%9Bwiata%20and%20Alina%20Wr%C3%B3blewska%20and%20Katarzyna%20Krasnowska-Kiera%C5%9B%20and%20Maciej%20Ogrodniczuk%20and%20Micha%C5%82%20Rudolf%20and%20Piotr%20Rybak%20and%20Karolina%20Saputa%20and%20Joanna%20Wo%C5%82oszyn%20and%20Marcin%20Oleksy%20and%20Bart%C5%82omiej%20Koptyra%20and%20Teddy%20Ferdinan%20and%20Stanis%C5%82aw%20Wo%C5%BAniak%20and%20Maciej%20Piasecki%20and%20Pawe%C5%82%20Walkowiak%20and%20Konrad%20Wojtasik%20and%20Arkadiusz%20Janz%20and%20Przemys%C5%82aw%20Kazienko%20and%20Julia%20Moska%20and%20Jan%20Koco%C5%84&entry.1292438233=This%20paper%20describes%20the%20instruction%20dataset%20used%20to%20fine-tune%20a%20set%20of%20transformer-based%20large%20language%20models%20%28LLMs%29%20developed%20in%20the%20PLLuM%20%28Polish%20Large%20Language%20Model%29%20project.%20We%20present%20a%20functional%20typology%20of%20the%20organic%2C%20converted%2C%20and%20synthetic%20instructions%20used%20in%20PLLuM%20and%20share%20some%20observations%20about%20the%20implications%20of%20using%20human-authored%20versus%20synthetic%20instruction%20datasets%20in%20the%20linguistic%20adaptation%20of%20base%20LLMs.%20Additionally%2C%20we%20release%20the%20first%20representative%20subset%20of%20the%20PLLuM%20instruction%20corpus%20%28PLLuMIC%29%2C%20which%20we%20believe%20to%20be%20useful%20in%20guiding%20and%20planning%20the%20development%20of%20similar%20datasets%20for%20other%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2511.17161v1&entry.124074799=Read"},
{"title": "Resolving Sentiment Discrepancy for Multimodal Sentiment Detection via Semantics Completion and Decomposition", "author": "Daiqing Wu and Dongbao Yang and Huawen Shen and Can Ma and Yu Zhou", "abstract": "With the proliferation of social media posts in recent years, the need to detect sentiments in multimodal (image-text) content has grown rapidly. Since posts are user-generated, the image and text from the same post can express different or even contradictory sentiments, leading to potential \\textbf{sentiment discrepancy}. However, existing works mainly adopt a single-branch fusion structure that primarily captures the consistent sentiment between image and text. The ignorance or implicit modeling of discrepant sentiment results in compromised unimodal encoding and limited performance. In this paper, we propose a semantics Completion and Decomposition (CoDe) network to resolve the above issue. In the semantics completion module, we complement image and text representations with the semantics of the in-image text, helping bridge the sentiment gap. In the semantics decomposition module, we decompose image and text representations with exclusive projection and contrastive learning, thereby explicitly capturing the discrepant sentiment between modalities. Finally, we fuse image and text representations by cross-attention and combine them with the learned discrepant sentiment for final classification. Extensive experiments on four datasets demonstrate the superiority of CoDe and the effectiveness of each proposed module.", "link": "http://arxiv.org/abs/2407.07026v2", "date": "2025-11-21", "relevancy": 2.1334, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5485}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.524}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resolving%20Sentiment%20Discrepancy%20for%20Multimodal%20Sentiment%20Detection%20via%20Semantics%20Completion%20and%20Decomposition&body=Title%3A%20Resolving%20Sentiment%20Discrepancy%20for%20Multimodal%20Sentiment%20Detection%20via%20Semantics%20Completion%20and%20Decomposition%0AAuthor%3A%20Daiqing%20Wu%20and%20Dongbao%20Yang%20and%20Huawen%20Shen%20and%20Can%20Ma%20and%20Yu%20Zhou%0AAbstract%3A%20With%20the%20proliferation%20of%20social%20media%20posts%20in%20recent%20years%2C%20the%20need%20to%20detect%20sentiments%20in%20multimodal%20%28image-text%29%20content%20has%20grown%20rapidly.%20Since%20posts%20are%20user-generated%2C%20the%20image%20and%20text%20from%20the%20same%20post%20can%20express%20different%20or%20even%20contradictory%20sentiments%2C%20leading%20to%20potential%20%5Ctextbf%7Bsentiment%20discrepancy%7D.%20However%2C%20existing%20works%20mainly%20adopt%20a%20single-branch%20fusion%20structure%20that%20primarily%20captures%20the%20consistent%20sentiment%20between%20image%20and%20text.%20The%20ignorance%20or%20implicit%20modeling%20of%20discrepant%20sentiment%20results%20in%20compromised%20unimodal%20encoding%20and%20limited%20performance.%20In%20this%20paper%2C%20we%20propose%20a%20semantics%20Completion%20and%20Decomposition%20%28CoDe%29%20network%20to%20resolve%20the%20above%20issue.%20In%20the%20semantics%20completion%20module%2C%20we%20complement%20image%20and%20text%20representations%20with%20the%20semantics%20of%20the%20in-image%20text%2C%20helping%20bridge%20the%20sentiment%20gap.%20In%20the%20semantics%20decomposition%20module%2C%20we%20decompose%20image%20and%20text%20representations%20with%20exclusive%20projection%20and%20contrastive%20learning%2C%20thereby%20explicitly%20capturing%20the%20discrepant%20sentiment%20between%20modalities.%20Finally%2C%20we%20fuse%20image%20and%20text%20representations%20by%20cross-attention%20and%20combine%20them%20with%20the%20learned%20discrepant%20sentiment%20for%20final%20classification.%20Extensive%20experiments%20on%20four%20datasets%20demonstrate%20the%20superiority%20of%20CoDe%20and%20the%20effectiveness%20of%20each%20proposed%20module.%0ALink%3A%20http%3A//arxiv.org/abs/2407.07026v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResolving%2520Sentiment%2520Discrepancy%2520for%2520Multimodal%2520Sentiment%2520Detection%2520via%2520Semantics%2520Completion%2520and%2520Decomposition%26entry.906535625%3DDaiqing%2520Wu%2520and%2520Dongbao%2520Yang%2520and%2520Huawen%2520Shen%2520and%2520Can%2520Ma%2520and%2520Yu%2520Zhou%26entry.1292438233%3DWith%2520the%2520proliferation%2520of%2520social%2520media%2520posts%2520in%2520recent%2520years%252C%2520the%2520need%2520to%2520detect%2520sentiments%2520in%2520multimodal%2520%2528image-text%2529%2520content%2520has%2520grown%2520rapidly.%2520Since%2520posts%2520are%2520user-generated%252C%2520the%2520image%2520and%2520text%2520from%2520the%2520same%2520post%2520can%2520express%2520different%2520or%2520even%2520contradictory%2520sentiments%252C%2520leading%2520to%2520potential%2520%255Ctextbf%257Bsentiment%2520discrepancy%257D.%2520However%252C%2520existing%2520works%2520mainly%2520adopt%2520a%2520single-branch%2520fusion%2520structure%2520that%2520primarily%2520captures%2520the%2520consistent%2520sentiment%2520between%2520image%2520and%2520text.%2520The%2520ignorance%2520or%2520implicit%2520modeling%2520of%2520discrepant%2520sentiment%2520results%2520in%2520compromised%2520unimodal%2520encoding%2520and%2520limited%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520semantics%2520Completion%2520and%2520Decomposition%2520%2528CoDe%2529%2520network%2520to%2520resolve%2520the%2520above%2520issue.%2520In%2520the%2520semantics%2520completion%2520module%252C%2520we%2520complement%2520image%2520and%2520text%2520representations%2520with%2520the%2520semantics%2520of%2520the%2520in-image%2520text%252C%2520helping%2520bridge%2520the%2520sentiment%2520gap.%2520In%2520the%2520semantics%2520decomposition%2520module%252C%2520we%2520decompose%2520image%2520and%2520text%2520representations%2520with%2520exclusive%2520projection%2520and%2520contrastive%2520learning%252C%2520thereby%2520explicitly%2520capturing%2520the%2520discrepant%2520sentiment%2520between%2520modalities.%2520Finally%252C%2520we%2520fuse%2520image%2520and%2520text%2520representations%2520by%2520cross-attention%2520and%2520combine%2520them%2520with%2520the%2520learned%2520discrepant%2520sentiment%2520for%2520final%2520classification.%2520Extensive%2520experiments%2520on%2520four%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520CoDe%2520and%2520the%2520effectiveness%2520of%2520each%2520proposed%2520module.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07026v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resolving%20Sentiment%20Discrepancy%20for%20Multimodal%20Sentiment%20Detection%20via%20Semantics%20Completion%20and%20Decomposition&entry.906535625=Daiqing%20Wu%20and%20Dongbao%20Yang%20and%20Huawen%20Shen%20and%20Can%20Ma%20and%20Yu%20Zhou&entry.1292438233=With%20the%20proliferation%20of%20social%20media%20posts%20in%20recent%20years%2C%20the%20need%20to%20detect%20sentiments%20in%20multimodal%20%28image-text%29%20content%20has%20grown%20rapidly.%20Since%20posts%20are%20user-generated%2C%20the%20image%20and%20text%20from%20the%20same%20post%20can%20express%20different%20or%20even%20contradictory%20sentiments%2C%20leading%20to%20potential%20%5Ctextbf%7Bsentiment%20discrepancy%7D.%20However%2C%20existing%20works%20mainly%20adopt%20a%20single-branch%20fusion%20structure%20that%20primarily%20captures%20the%20consistent%20sentiment%20between%20image%20and%20text.%20The%20ignorance%20or%20implicit%20modeling%20of%20discrepant%20sentiment%20results%20in%20compromised%20unimodal%20encoding%20and%20limited%20performance.%20In%20this%20paper%2C%20we%20propose%20a%20semantics%20Completion%20and%20Decomposition%20%28CoDe%29%20network%20to%20resolve%20the%20above%20issue.%20In%20the%20semantics%20completion%20module%2C%20we%20complement%20image%20and%20text%20representations%20with%20the%20semantics%20of%20the%20in-image%20text%2C%20helping%20bridge%20the%20sentiment%20gap.%20In%20the%20semantics%20decomposition%20module%2C%20we%20decompose%20image%20and%20text%20representations%20with%20exclusive%20projection%20and%20contrastive%20learning%2C%20thereby%20explicitly%20capturing%20the%20discrepant%20sentiment%20between%20modalities.%20Finally%2C%20we%20fuse%20image%20and%20text%20representations%20by%20cross-attention%20and%20combine%20them%20with%20the%20learned%20discrepant%20sentiment%20for%20final%20classification.%20Extensive%20experiments%20on%20four%20datasets%20demonstrate%20the%20superiority%20of%20CoDe%20and%20the%20effectiveness%20of%20each%20proposed%20module.&entry.1838667208=http%3A//arxiv.org/abs/2407.07026v2&entry.124074799=Read"},
{"title": "ChainV: Atomic Visual Hints Make Multimodal Reasoning Shorter and Better", "author": "Yuan Zhang and Ming Lu and Junwen Pan and Tao Huang and Kuan Cheng and Qi She and Shanghang Zhang", "abstract": "Recent advances in multimodal reasoning models have demonstrated impressive capabilities across text and vision. However, even leading models exhibit redundant self-reflection when generating lengthy reasoning chains. While training-free CoT compression methods have emerged in the LLMs domain, they rely on static visual references and thus provide limited gains for multimodal reasoning. Therefore, we propose ChainV, a framework that dynamically integrates visual hints into the reasoning process, thereby making multimodal reasoning shorter and better. Specifically, ChainV first performs a coarse visual patch selection based on the previous reasoning step, then refines it by identifying the most representative atomic visual hint according to the averaged attention intensity. Additionally, ChainV introduces a consistency-based evaluation mechanism to assess the reliability of the chosen hint, guiding the model to adaptively adjust its level of self-reflection. Eventually, the pixel coordinates of the selected visual hint and its reliability are incorporated into thinking with a Bernoulli stochastic process. Experiments indicate that our method significantly improves reasoning accuracy and efficiency, especially on math-intensive benchmarks where visual hints are crucial for multi-step symbolic reasoning. For example, ChainV achieves $2.3\\%$ improvement on the MathVista within MIMO-VL-RL, while reducing inference latency by $51.4\\%$ and shortening output token length by $24.5\\%$.", "link": "http://arxiv.org/abs/2511.17106v1", "date": "2025-11-21", "relevancy": 2.1043, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5271}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5271}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChainV%3A%20Atomic%20Visual%20Hints%20Make%20Multimodal%20Reasoning%20Shorter%20and%20Better&body=Title%3A%20ChainV%3A%20Atomic%20Visual%20Hints%20Make%20Multimodal%20Reasoning%20Shorter%20and%20Better%0AAuthor%3A%20Yuan%20Zhang%20and%20Ming%20Lu%20and%20Junwen%20Pan%20and%20Tao%20Huang%20and%20Kuan%20Cheng%20and%20Qi%20She%20and%20Shanghang%20Zhang%0AAbstract%3A%20Recent%20advances%20in%20multimodal%20reasoning%20models%20have%20demonstrated%20impressive%20capabilities%20across%20text%20and%20vision.%20However%2C%20even%20leading%20models%20exhibit%20redundant%20self-reflection%20when%20generating%20lengthy%20reasoning%20chains.%20While%20training-free%20CoT%20compression%20methods%20have%20emerged%20in%20the%20LLMs%20domain%2C%20they%20rely%20on%20static%20visual%20references%20and%20thus%20provide%20limited%20gains%20for%20multimodal%20reasoning.%20Therefore%2C%20we%20propose%20ChainV%2C%20a%20framework%20that%20dynamically%20integrates%20visual%20hints%20into%20the%20reasoning%20process%2C%20thereby%20making%20multimodal%20reasoning%20shorter%20and%20better.%20Specifically%2C%20ChainV%20first%20performs%20a%20coarse%20visual%20patch%20selection%20based%20on%20the%20previous%20reasoning%20step%2C%20then%20refines%20it%20by%20identifying%20the%20most%20representative%20atomic%20visual%20hint%20according%20to%20the%20averaged%20attention%20intensity.%20Additionally%2C%20ChainV%20introduces%20a%20consistency-based%20evaluation%20mechanism%20to%20assess%20the%20reliability%20of%20the%20chosen%20hint%2C%20guiding%20the%20model%20to%20adaptively%20adjust%20its%20level%20of%20self-reflection.%20Eventually%2C%20the%20pixel%20coordinates%20of%20the%20selected%20visual%20hint%20and%20its%20reliability%20are%20incorporated%20into%20thinking%20with%20a%20Bernoulli%20stochastic%20process.%20Experiments%20indicate%20that%20our%20method%20significantly%20improves%20reasoning%20accuracy%20and%20efficiency%2C%20especially%20on%20math-intensive%20benchmarks%20where%20visual%20hints%20are%20crucial%20for%20multi-step%20symbolic%20reasoning.%20For%20example%2C%20ChainV%20achieves%20%242.3%5C%25%24%20improvement%20on%20the%20MathVista%20within%20MIMO-VL-RL%2C%20while%20reducing%20inference%20latency%20by%20%2451.4%5C%25%24%20and%20shortening%20output%20token%20length%20by%20%2424.5%5C%25%24.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChainV%253A%2520Atomic%2520Visual%2520Hints%2520Make%2520Multimodal%2520Reasoning%2520Shorter%2520and%2520Better%26entry.906535625%3DYuan%2520Zhang%2520and%2520Ming%2520Lu%2520and%2520Junwen%2520Pan%2520and%2520Tao%2520Huang%2520and%2520Kuan%2520Cheng%2520and%2520Qi%2520She%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3DRecent%2520advances%2520in%2520multimodal%2520reasoning%2520models%2520have%2520demonstrated%2520impressive%2520capabilities%2520across%2520text%2520and%2520vision.%2520However%252C%2520even%2520leading%2520models%2520exhibit%2520redundant%2520self-reflection%2520when%2520generating%2520lengthy%2520reasoning%2520chains.%2520While%2520training-free%2520CoT%2520compression%2520methods%2520have%2520emerged%2520in%2520the%2520LLMs%2520domain%252C%2520they%2520rely%2520on%2520static%2520visual%2520references%2520and%2520thus%2520provide%2520limited%2520gains%2520for%2520multimodal%2520reasoning.%2520Therefore%252C%2520we%2520propose%2520ChainV%252C%2520a%2520framework%2520that%2520dynamically%2520integrates%2520visual%2520hints%2520into%2520the%2520reasoning%2520process%252C%2520thereby%2520making%2520multimodal%2520reasoning%2520shorter%2520and%2520better.%2520Specifically%252C%2520ChainV%2520first%2520performs%2520a%2520coarse%2520visual%2520patch%2520selection%2520based%2520on%2520the%2520previous%2520reasoning%2520step%252C%2520then%2520refines%2520it%2520by%2520identifying%2520the%2520most%2520representative%2520atomic%2520visual%2520hint%2520according%2520to%2520the%2520averaged%2520attention%2520intensity.%2520Additionally%252C%2520ChainV%2520introduces%2520a%2520consistency-based%2520evaluation%2520mechanism%2520to%2520assess%2520the%2520reliability%2520of%2520the%2520chosen%2520hint%252C%2520guiding%2520the%2520model%2520to%2520adaptively%2520adjust%2520its%2520level%2520of%2520self-reflection.%2520Eventually%252C%2520the%2520pixel%2520coordinates%2520of%2520the%2520selected%2520visual%2520hint%2520and%2520its%2520reliability%2520are%2520incorporated%2520into%2520thinking%2520with%2520a%2520Bernoulli%2520stochastic%2520process.%2520Experiments%2520indicate%2520that%2520our%2520method%2520significantly%2520improves%2520reasoning%2520accuracy%2520and%2520efficiency%252C%2520especially%2520on%2520math-intensive%2520benchmarks%2520where%2520visual%2520hints%2520are%2520crucial%2520for%2520multi-step%2520symbolic%2520reasoning.%2520For%2520example%252C%2520ChainV%2520achieves%2520%25242.3%255C%2525%2524%2520improvement%2520on%2520the%2520MathVista%2520within%2520MIMO-VL-RL%252C%2520while%2520reducing%2520inference%2520latency%2520by%2520%252451.4%255C%2525%2524%2520and%2520shortening%2520output%2520token%2520length%2520by%2520%252424.5%255C%2525%2524.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChainV%3A%20Atomic%20Visual%20Hints%20Make%20Multimodal%20Reasoning%20Shorter%20and%20Better&entry.906535625=Yuan%20Zhang%20and%20Ming%20Lu%20and%20Junwen%20Pan%20and%20Tao%20Huang%20and%20Kuan%20Cheng%20and%20Qi%20She%20and%20Shanghang%20Zhang&entry.1292438233=Recent%20advances%20in%20multimodal%20reasoning%20models%20have%20demonstrated%20impressive%20capabilities%20across%20text%20and%20vision.%20However%2C%20even%20leading%20models%20exhibit%20redundant%20self-reflection%20when%20generating%20lengthy%20reasoning%20chains.%20While%20training-free%20CoT%20compression%20methods%20have%20emerged%20in%20the%20LLMs%20domain%2C%20they%20rely%20on%20static%20visual%20references%20and%20thus%20provide%20limited%20gains%20for%20multimodal%20reasoning.%20Therefore%2C%20we%20propose%20ChainV%2C%20a%20framework%20that%20dynamically%20integrates%20visual%20hints%20into%20the%20reasoning%20process%2C%20thereby%20making%20multimodal%20reasoning%20shorter%20and%20better.%20Specifically%2C%20ChainV%20first%20performs%20a%20coarse%20visual%20patch%20selection%20based%20on%20the%20previous%20reasoning%20step%2C%20then%20refines%20it%20by%20identifying%20the%20most%20representative%20atomic%20visual%20hint%20according%20to%20the%20averaged%20attention%20intensity.%20Additionally%2C%20ChainV%20introduces%20a%20consistency-based%20evaluation%20mechanism%20to%20assess%20the%20reliability%20of%20the%20chosen%20hint%2C%20guiding%20the%20model%20to%20adaptively%20adjust%20its%20level%20of%20self-reflection.%20Eventually%2C%20the%20pixel%20coordinates%20of%20the%20selected%20visual%20hint%20and%20its%20reliability%20are%20incorporated%20into%20thinking%20with%20a%20Bernoulli%20stochastic%20process.%20Experiments%20indicate%20that%20our%20method%20significantly%20improves%20reasoning%20accuracy%20and%20efficiency%2C%20especially%20on%20math-intensive%20benchmarks%20where%20visual%20hints%20are%20crucial%20for%20multi-step%20symbolic%20reasoning.%20For%20example%2C%20ChainV%20achieves%20%242.3%5C%25%24%20improvement%20on%20the%20MathVista%20within%20MIMO-VL-RL%2C%20while%20reducing%20inference%20latency%20by%20%2451.4%5C%25%24%20and%20shortening%20output%20token%20length%20by%20%2424.5%5C%25%24.&entry.1838667208=http%3A//arxiv.org/abs/2511.17106v1&entry.124074799=Read"},
{"title": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models", "author": "W. K. M Mithsara and Ning Yang and Ahmed Imteaj and Hussein Zangoti and Abdur R. Shahid", "abstract": "The widespread integration of wearable sensing devices in Internet of Things (IoT) ecosystems, particularly in healthcare, smart homes, and industrial applications, has required robust human activity recognition (HAR) techniques to improve functionality and user experience. Although machine learning models have advanced HAR, they are increasingly susceptible to data poisoning attacks that compromise the data integrity and reliability of these systems. Conventional approaches to defending against such attacks often require extensive task-specific training with large, labeled datasets, which limits adaptability in dynamic IoT environments. This work proposes a novel framework that uses large language models (LLMs) to perform poisoning detection and sanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot learning paradigms. Our approach incorporates \\textit{role play} prompting, whereby the LLM assumes the role of expert to contextualize and evaluate sensor anomalies, and \\textit{think step-by-step} reasoning, guiding the LLM to infer poisoning indicators in the raw sensor data and plausible clean alternatives. These strategies minimize reliance on curation of extensive datasets and enable robust, adaptable defense mechanisms in real-time. We perform an extensive evaluation of the framework, quantifying detection accuracy, sanitization quality, latency, and communication cost, thus demonstrating the practicality and effectiveness of LLMs in improving the security and reliability of wearable IoT systems.", "link": "http://arxiv.org/abs/2511.02894v3", "date": "2025-11-21", "relevancy": 1.4488, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5046}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4777}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20and%20Robust%20Data%20Poisoning%20Detection%20and%20Sanitization%20in%20Wearable%20IoT%20Systems%20using%20Large%20Language%20Models&body=Title%3A%20Adaptive%20and%20Robust%20Data%20Poisoning%20Detection%20and%20Sanitization%20in%20Wearable%20IoT%20Systems%20using%20Large%20Language%20Models%0AAuthor%3A%20W.%20K.%20M%20Mithsara%20and%20Ning%20Yang%20and%20Ahmed%20Imteaj%20and%20Hussein%20Zangoti%20and%20Abdur%20R.%20Shahid%0AAbstract%3A%20The%20widespread%20integration%20of%20wearable%20sensing%20devices%20in%20Internet%20of%20Things%20%28IoT%29%20ecosystems%2C%20particularly%20in%20healthcare%2C%20smart%20homes%2C%20and%20industrial%20applications%2C%20has%20required%20robust%20human%20activity%20recognition%20%28HAR%29%20techniques%20to%20improve%20functionality%20and%20user%20experience.%20Although%20machine%20learning%20models%20have%20advanced%20HAR%2C%20they%20are%20increasingly%20susceptible%20to%20data%20poisoning%20attacks%20that%20compromise%20the%20data%20integrity%20and%20reliability%20of%20these%20systems.%20Conventional%20approaches%20to%20defending%20against%20such%20attacks%20often%20require%20extensive%20task-specific%20training%20with%20large%2C%20labeled%20datasets%2C%20which%20limits%20adaptability%20in%20dynamic%20IoT%20environments.%20This%20work%20proposes%20a%20novel%20framework%20that%20uses%20large%20language%20models%20%28LLMs%29%20to%20perform%20poisoning%20detection%20and%20sanitization%20in%20HAR%20systems%2C%20utilizing%20zero-shot%2C%20one-shot%2C%20and%20few-shot%20learning%20paradigms.%20Our%20approach%20incorporates%20%5Ctextit%7Brole%20play%7D%20prompting%2C%20whereby%20the%20LLM%20assumes%20the%20role%20of%20expert%20to%20contextualize%20and%20evaluate%20sensor%20anomalies%2C%20and%20%5Ctextit%7Bthink%20step-by-step%7D%20reasoning%2C%20guiding%20the%20LLM%20to%20infer%20poisoning%20indicators%20in%20the%20raw%20sensor%20data%20and%20plausible%20clean%20alternatives.%20These%20strategies%20minimize%20reliance%20on%20curation%20of%20extensive%20datasets%20and%20enable%20robust%2C%20adaptable%20defense%20mechanisms%20in%20real-time.%20We%20perform%20an%20extensive%20evaluation%20of%20the%20framework%2C%20quantifying%20detection%20accuracy%2C%20sanitization%20quality%2C%20latency%2C%20and%20communication%20cost%2C%20thus%20demonstrating%20the%20practicality%20and%20effectiveness%20of%20LLMs%20in%20improving%20the%20security%20and%20reliability%20of%20wearable%20IoT%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.02894v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520and%2520Robust%2520Data%2520Poisoning%2520Detection%2520and%2520Sanitization%2520in%2520Wearable%2520IoT%2520Systems%2520using%2520Large%2520Language%2520Models%26entry.906535625%3DW.%2520K.%2520M%2520Mithsara%2520and%2520Ning%2520Yang%2520and%2520Ahmed%2520Imteaj%2520and%2520Hussein%2520Zangoti%2520and%2520Abdur%2520R.%2520Shahid%26entry.1292438233%3DThe%2520widespread%2520integration%2520of%2520wearable%2520sensing%2520devices%2520in%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520ecosystems%252C%2520particularly%2520in%2520healthcare%252C%2520smart%2520homes%252C%2520and%2520industrial%2520applications%252C%2520has%2520required%2520robust%2520human%2520activity%2520recognition%2520%2528HAR%2529%2520techniques%2520to%2520improve%2520functionality%2520and%2520user%2520experience.%2520Although%2520machine%2520learning%2520models%2520have%2520advanced%2520HAR%252C%2520they%2520are%2520increasingly%2520susceptible%2520to%2520data%2520poisoning%2520attacks%2520that%2520compromise%2520the%2520data%2520integrity%2520and%2520reliability%2520of%2520these%2520systems.%2520Conventional%2520approaches%2520to%2520defending%2520against%2520such%2520attacks%2520often%2520require%2520extensive%2520task-specific%2520training%2520with%2520large%252C%2520labeled%2520datasets%252C%2520which%2520limits%2520adaptability%2520in%2520dynamic%2520IoT%2520environments.%2520This%2520work%2520proposes%2520a%2520novel%2520framework%2520that%2520uses%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520perform%2520poisoning%2520detection%2520and%2520sanitization%2520in%2520HAR%2520systems%252C%2520utilizing%2520zero-shot%252C%2520one-shot%252C%2520and%2520few-shot%2520learning%2520paradigms.%2520Our%2520approach%2520incorporates%2520%255Ctextit%257Brole%2520play%257D%2520prompting%252C%2520whereby%2520the%2520LLM%2520assumes%2520the%2520role%2520of%2520expert%2520to%2520contextualize%2520and%2520evaluate%2520sensor%2520anomalies%252C%2520and%2520%255Ctextit%257Bthink%2520step-by-step%257D%2520reasoning%252C%2520guiding%2520the%2520LLM%2520to%2520infer%2520poisoning%2520indicators%2520in%2520the%2520raw%2520sensor%2520data%2520and%2520plausible%2520clean%2520alternatives.%2520These%2520strategies%2520minimize%2520reliance%2520on%2520curation%2520of%2520extensive%2520datasets%2520and%2520enable%2520robust%252C%2520adaptable%2520defense%2520mechanisms%2520in%2520real-time.%2520We%2520perform%2520an%2520extensive%2520evaluation%2520of%2520the%2520framework%252C%2520quantifying%2520detection%2520accuracy%252C%2520sanitization%2520quality%252C%2520latency%252C%2520and%2520communication%2520cost%252C%2520thus%2520demonstrating%2520the%2520practicality%2520and%2520effectiveness%2520of%2520LLMs%2520in%2520improving%2520the%2520security%2520and%2520reliability%2520of%2520wearable%2520IoT%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.02894v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20and%20Robust%20Data%20Poisoning%20Detection%20and%20Sanitization%20in%20Wearable%20IoT%20Systems%20using%20Large%20Language%20Models&entry.906535625=W.%20K.%20M%20Mithsara%20and%20Ning%20Yang%20and%20Ahmed%20Imteaj%20and%20Hussein%20Zangoti%20and%20Abdur%20R.%20Shahid&entry.1292438233=The%20widespread%20integration%20of%20wearable%20sensing%20devices%20in%20Internet%20of%20Things%20%28IoT%29%20ecosystems%2C%20particularly%20in%20healthcare%2C%20smart%20homes%2C%20and%20industrial%20applications%2C%20has%20required%20robust%20human%20activity%20recognition%20%28HAR%29%20techniques%20to%20improve%20functionality%20and%20user%20experience.%20Although%20machine%20learning%20models%20have%20advanced%20HAR%2C%20they%20are%20increasingly%20susceptible%20to%20data%20poisoning%20attacks%20that%20compromise%20the%20data%20integrity%20and%20reliability%20of%20these%20systems.%20Conventional%20approaches%20to%20defending%20against%20such%20attacks%20often%20require%20extensive%20task-specific%20training%20with%20large%2C%20labeled%20datasets%2C%20which%20limits%20adaptability%20in%20dynamic%20IoT%20environments.%20This%20work%20proposes%20a%20novel%20framework%20that%20uses%20large%20language%20models%20%28LLMs%29%20to%20perform%20poisoning%20detection%20and%20sanitization%20in%20HAR%20systems%2C%20utilizing%20zero-shot%2C%20one-shot%2C%20and%20few-shot%20learning%20paradigms.%20Our%20approach%20incorporates%20%5Ctextit%7Brole%20play%7D%20prompting%2C%20whereby%20the%20LLM%20assumes%20the%20role%20of%20expert%20to%20contextualize%20and%20evaluate%20sensor%20anomalies%2C%20and%20%5Ctextit%7Bthink%20step-by-step%7D%20reasoning%2C%20guiding%20the%20LLM%20to%20infer%20poisoning%20indicators%20in%20the%20raw%20sensor%20data%20and%20plausible%20clean%20alternatives.%20These%20strategies%20minimize%20reliance%20on%20curation%20of%20extensive%20datasets%20and%20enable%20robust%2C%20adaptable%20defense%20mechanisms%20in%20real-time.%20We%20perform%20an%20extensive%20evaluation%20of%20the%20framework%2C%20quantifying%20detection%20accuracy%2C%20sanitization%20quality%2C%20latency%2C%20and%20communication%20cost%2C%20thus%20demonstrating%20the%20practicality%20and%20effectiveness%20of%20LLMs%20in%20improving%20the%20security%20and%20reliability%20of%20wearable%20IoT%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2511.02894v3&entry.124074799=Read"},
{"title": "DS-Span: Single-Phase Discriminative Subgraph Mining for Efficient Graph Embeddings", "author": "Yeamin Kaiser and Muhammed Tasnim Bin Anwar and Bholanath Das and Chowdhury Farhan Ahmed and Md. Tanvir Alam", "abstract": "Graph representation learning seeks to transform complex, high-dimensional graph structures into compact vector spaces that preserve both topology and semantics. Among the various strategies, subgraph-based methods provide an interpretable bridge between symbolic pattern discovery and continuous embedding learning. Yet, existing frequent or discriminative subgraph mining approaches often suffer from redundant multi-phase pipelines, high computational cost, and weak coupling between mined structures and their discriminative relevance. We propose DS-Span, a single-phase discriminative subgraph mining framework that unifies pattern growth, pruning, and supervision-driven scoring within one traversal of the search space. DS-Span introduces a coverage-capped eligibility mechanism that dynamically limits exploration once a graph is sufficiently represented, and an information-gain-guided selection that promotes subgraphs with strong class-separating ability while minimizing redundancy. The resulting subgraph set serves as an efficient, interpretable basis for downstream graph embedding and classification. Extensive experiments across benchmarks demonstrate that DS-Span generates more compact and discriminative subgraph features than prior multi-stage methods, achieving higher or comparable accuracy with significantly reduced runtime. These results highlight the potential of unified, single-phase discriminative mining as a foundation for scalable and interpretable graph representation learning.", "link": "http://arxiv.org/abs/2511.17419v1", "date": "2025-11-21", "relevancy": 1.9808, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5086}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4872}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DS-Span%3A%20Single-Phase%20Discriminative%20Subgraph%20Mining%20for%20Efficient%20Graph%20Embeddings&body=Title%3A%20DS-Span%3A%20Single-Phase%20Discriminative%20Subgraph%20Mining%20for%20Efficient%20Graph%20Embeddings%0AAuthor%3A%20Yeamin%20Kaiser%20and%20Muhammed%20Tasnim%20Bin%20Anwar%20and%20Bholanath%20Das%20and%20Chowdhury%20Farhan%20Ahmed%20and%20Md.%20Tanvir%20Alam%0AAbstract%3A%20Graph%20representation%20learning%20seeks%20to%20transform%20complex%2C%20high-dimensional%20graph%20structures%20into%20compact%20vector%20spaces%20that%20preserve%20both%20topology%20and%20semantics.%20Among%20the%20various%20strategies%2C%20subgraph-based%20methods%20provide%20an%20interpretable%20bridge%20between%20symbolic%20pattern%20discovery%20and%20continuous%20embedding%20learning.%20Yet%2C%20existing%20frequent%20or%20discriminative%20subgraph%20mining%20approaches%20often%20suffer%20from%20redundant%20multi-phase%20pipelines%2C%20high%20computational%20cost%2C%20and%20weak%20coupling%20between%20mined%20structures%20and%20their%20discriminative%20relevance.%20We%20propose%20DS-Span%2C%20a%20single-phase%20discriminative%20subgraph%20mining%20framework%20that%20unifies%20pattern%20growth%2C%20pruning%2C%20and%20supervision-driven%20scoring%20within%20one%20traversal%20of%20the%20search%20space.%20DS-Span%20introduces%20a%20coverage-capped%20eligibility%20mechanism%20that%20dynamically%20limits%20exploration%20once%20a%20graph%20is%20sufficiently%20represented%2C%20and%20an%20information-gain-guided%20selection%20that%20promotes%20subgraphs%20with%20strong%20class-separating%20ability%20while%20minimizing%20redundancy.%20The%20resulting%20subgraph%20set%20serves%20as%20an%20efficient%2C%20interpretable%20basis%20for%20downstream%20graph%20embedding%20and%20classification.%20Extensive%20experiments%20across%20benchmarks%20demonstrate%20that%20DS-Span%20generates%20more%20compact%20and%20discriminative%20subgraph%20features%20than%20prior%20multi-stage%20methods%2C%20achieving%20higher%20or%20comparable%20accuracy%20with%20significantly%20reduced%20runtime.%20These%20results%20highlight%20the%20potential%20of%20unified%2C%20single-phase%20discriminative%20mining%20as%20a%20foundation%20for%20scalable%20and%20interpretable%20graph%20representation%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDS-Span%253A%2520Single-Phase%2520Discriminative%2520Subgraph%2520Mining%2520for%2520Efficient%2520Graph%2520Embeddings%26entry.906535625%3DYeamin%2520Kaiser%2520and%2520Muhammed%2520Tasnim%2520Bin%2520Anwar%2520and%2520Bholanath%2520Das%2520and%2520Chowdhury%2520Farhan%2520Ahmed%2520and%2520Md.%2520Tanvir%2520Alam%26entry.1292438233%3DGraph%2520representation%2520learning%2520seeks%2520to%2520transform%2520complex%252C%2520high-dimensional%2520graph%2520structures%2520into%2520compact%2520vector%2520spaces%2520that%2520preserve%2520both%2520topology%2520and%2520semantics.%2520Among%2520the%2520various%2520strategies%252C%2520subgraph-based%2520methods%2520provide%2520an%2520interpretable%2520bridge%2520between%2520symbolic%2520pattern%2520discovery%2520and%2520continuous%2520embedding%2520learning.%2520Yet%252C%2520existing%2520frequent%2520or%2520discriminative%2520subgraph%2520mining%2520approaches%2520often%2520suffer%2520from%2520redundant%2520multi-phase%2520pipelines%252C%2520high%2520computational%2520cost%252C%2520and%2520weak%2520coupling%2520between%2520mined%2520structures%2520and%2520their%2520discriminative%2520relevance.%2520We%2520propose%2520DS-Span%252C%2520a%2520single-phase%2520discriminative%2520subgraph%2520mining%2520framework%2520that%2520unifies%2520pattern%2520growth%252C%2520pruning%252C%2520and%2520supervision-driven%2520scoring%2520within%2520one%2520traversal%2520of%2520the%2520search%2520space.%2520DS-Span%2520introduces%2520a%2520coverage-capped%2520eligibility%2520mechanism%2520that%2520dynamically%2520limits%2520exploration%2520once%2520a%2520graph%2520is%2520sufficiently%2520represented%252C%2520and%2520an%2520information-gain-guided%2520selection%2520that%2520promotes%2520subgraphs%2520with%2520strong%2520class-separating%2520ability%2520while%2520minimizing%2520redundancy.%2520The%2520resulting%2520subgraph%2520set%2520serves%2520as%2520an%2520efficient%252C%2520interpretable%2520basis%2520for%2520downstream%2520graph%2520embedding%2520and%2520classification.%2520Extensive%2520experiments%2520across%2520benchmarks%2520demonstrate%2520that%2520DS-Span%2520generates%2520more%2520compact%2520and%2520discriminative%2520subgraph%2520features%2520than%2520prior%2520multi-stage%2520methods%252C%2520achieving%2520higher%2520or%2520comparable%2520accuracy%2520with%2520significantly%2520reduced%2520runtime.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520unified%252C%2520single-phase%2520discriminative%2520mining%2520as%2520a%2520foundation%2520for%2520scalable%2520and%2520interpretable%2520graph%2520representation%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DS-Span%3A%20Single-Phase%20Discriminative%20Subgraph%20Mining%20for%20Efficient%20Graph%20Embeddings&entry.906535625=Yeamin%20Kaiser%20and%20Muhammed%20Tasnim%20Bin%20Anwar%20and%20Bholanath%20Das%20and%20Chowdhury%20Farhan%20Ahmed%20and%20Md.%20Tanvir%20Alam&entry.1292438233=Graph%20representation%20learning%20seeks%20to%20transform%20complex%2C%20high-dimensional%20graph%20structures%20into%20compact%20vector%20spaces%20that%20preserve%20both%20topology%20and%20semantics.%20Among%20the%20various%20strategies%2C%20subgraph-based%20methods%20provide%20an%20interpretable%20bridge%20between%20symbolic%20pattern%20discovery%20and%20continuous%20embedding%20learning.%20Yet%2C%20existing%20frequent%20or%20discriminative%20subgraph%20mining%20approaches%20often%20suffer%20from%20redundant%20multi-phase%20pipelines%2C%20high%20computational%20cost%2C%20and%20weak%20coupling%20between%20mined%20structures%20and%20their%20discriminative%20relevance.%20We%20propose%20DS-Span%2C%20a%20single-phase%20discriminative%20subgraph%20mining%20framework%20that%20unifies%20pattern%20growth%2C%20pruning%2C%20and%20supervision-driven%20scoring%20within%20one%20traversal%20of%20the%20search%20space.%20DS-Span%20introduces%20a%20coverage-capped%20eligibility%20mechanism%20that%20dynamically%20limits%20exploration%20once%20a%20graph%20is%20sufficiently%20represented%2C%20and%20an%20information-gain-guided%20selection%20that%20promotes%20subgraphs%20with%20strong%20class-separating%20ability%20while%20minimizing%20redundancy.%20The%20resulting%20subgraph%20set%20serves%20as%20an%20efficient%2C%20interpretable%20basis%20for%20downstream%20graph%20embedding%20and%20classification.%20Extensive%20experiments%20across%20benchmarks%20demonstrate%20that%20DS-Span%20generates%20more%20compact%20and%20discriminative%20subgraph%20features%20than%20prior%20multi-stage%20methods%2C%20achieving%20higher%20or%20comparable%20accuracy%20with%20significantly%20reduced%20runtime.%20These%20results%20highlight%20the%20potential%20of%20unified%2C%20single-phase%20discriminative%20mining%20as%20a%20foundation%20for%20scalable%20and%20interpretable%20graph%20representation%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2511.17419v1&entry.124074799=Read"},
{"title": "CREST: Improving Interpretability and Effectiveness of Troubleshooting at Ericsson through Criterion-Specific Trouble Report Retrieval", "author": "Soroush Javdan and Pragash Krishnamoorthy and Olga Baysal", "abstract": "The rapid evolution of the telecommunication industry necessitates efficient troubleshooting processes to maintain network reliability, software maintainability, and service quality. Trouble Reports (TRs), which document issues in Ericsson's production system, play a critical role in facilitating the timely resolution of software faults. However, the complexity and volume of TR data, along with the presence of diverse criteria that reflect different aspects of each fault, present challenges for retrieval systems. Building on prior work at Ericsson, which utilized a two-stage workflow, comprising Initial Retrieval (IR) and Re-Ranking (RR) stages, this study investigates different TR observation criteria and their impact on the performance of retrieval models. We propose \\textbf{CREST} (\\textbf{C}riteria-specific \\textbf{R}etrieval via \\textbf{E}nsemble of \\textbf{S}pecialized \\textbf{T}R models), a criterion-driven retrieval approach that leverages specialized models for different TR fields to improve both effectiveness and interpretability, thereby enabling quicker fault resolution and supporting software maintenance. CREST utilizes specialized models trained on specific TR criteria and aggregates their outputs to capture diverse and complementary signals. This approach leads to enhanced retrieval accuracy, better calibration of predicted scores, and improved interpretability by providing relevance scores for each criterion, helping users understand why specific TRs were retrieved. Using a subset of Ericsson's internal TRs, this research demonstrates that criterion-specific models significantly outperform a single model approach across key evaluation metrics. This highlights the importance of all targeted criteria used in this study for optimizing the performance of retrieval systems.", "link": "http://arxiv.org/abs/2511.17417v1", "date": "2025-11-21", "relevancy": 1.6636, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4183}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4183}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CREST%3A%20Improving%20Interpretability%20and%20Effectiveness%20of%20Troubleshooting%20at%20Ericsson%20through%20Criterion-Specific%20Trouble%20Report%20Retrieval&body=Title%3A%20CREST%3A%20Improving%20Interpretability%20and%20Effectiveness%20of%20Troubleshooting%20at%20Ericsson%20through%20Criterion-Specific%20Trouble%20Report%20Retrieval%0AAuthor%3A%20Soroush%20Javdan%20and%20Pragash%20Krishnamoorthy%20and%20Olga%20Baysal%0AAbstract%3A%20The%20rapid%20evolution%20of%20the%20telecommunication%20industry%20necessitates%20efficient%20troubleshooting%20processes%20to%20maintain%20network%20reliability%2C%20software%20maintainability%2C%20and%20service%20quality.%20Trouble%20Reports%20%28TRs%29%2C%20which%20document%20issues%20in%20Ericsson%27s%20production%20system%2C%20play%20a%20critical%20role%20in%20facilitating%20the%20timely%20resolution%20of%20software%20faults.%20However%2C%20the%20complexity%20and%20volume%20of%20TR%20data%2C%20along%20with%20the%20presence%20of%20diverse%20criteria%20that%20reflect%20different%20aspects%20of%20each%20fault%2C%20present%20challenges%20for%20retrieval%20systems.%20Building%20on%20prior%20work%20at%20Ericsson%2C%20which%20utilized%20a%20two-stage%20workflow%2C%20comprising%20Initial%20Retrieval%20%28IR%29%20and%20Re-Ranking%20%28RR%29%20stages%2C%20this%20study%20investigates%20different%20TR%20observation%20criteria%20and%20their%20impact%20on%20the%20performance%20of%20retrieval%20models.%20We%20propose%20%5Ctextbf%7BCREST%7D%20%28%5Ctextbf%7BC%7Driteria-specific%20%5Ctextbf%7BR%7Detrieval%20via%20%5Ctextbf%7BE%7Dnsemble%20of%20%5Ctextbf%7BS%7Dpecialized%20%5Ctextbf%7BT%7DR%20models%29%2C%20a%20criterion-driven%20retrieval%20approach%20that%20leverages%20specialized%20models%20for%20different%20TR%20fields%20to%20improve%20both%20effectiveness%20and%20interpretability%2C%20thereby%20enabling%20quicker%20fault%20resolution%20and%20supporting%20software%20maintenance.%20CREST%20utilizes%20specialized%20models%20trained%20on%20specific%20TR%20criteria%20and%20aggregates%20their%20outputs%20to%20capture%20diverse%20and%20complementary%20signals.%20This%20approach%20leads%20to%20enhanced%20retrieval%20accuracy%2C%20better%20calibration%20of%20predicted%20scores%2C%20and%20improved%20interpretability%20by%20providing%20relevance%20scores%20for%20each%20criterion%2C%20helping%20users%20understand%20why%20specific%20TRs%20were%20retrieved.%20Using%20a%20subset%20of%20Ericsson%27s%20internal%20TRs%2C%20this%20research%20demonstrates%20that%20criterion-specific%20models%20significantly%20outperform%20a%20single%20model%20approach%20across%20key%20evaluation%20metrics.%20This%20highlights%20the%20importance%20of%20all%20targeted%20criteria%20used%20in%20this%20study%20for%20optimizing%20the%20performance%20of%20retrieval%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCREST%253A%2520Improving%2520Interpretability%2520and%2520Effectiveness%2520of%2520Troubleshooting%2520at%2520Ericsson%2520through%2520Criterion-Specific%2520Trouble%2520Report%2520Retrieval%26entry.906535625%3DSoroush%2520Javdan%2520and%2520Pragash%2520Krishnamoorthy%2520and%2520Olga%2520Baysal%26entry.1292438233%3DThe%2520rapid%2520evolution%2520of%2520the%2520telecommunication%2520industry%2520necessitates%2520efficient%2520troubleshooting%2520processes%2520to%2520maintain%2520network%2520reliability%252C%2520software%2520maintainability%252C%2520and%2520service%2520quality.%2520Trouble%2520Reports%2520%2528TRs%2529%252C%2520which%2520document%2520issues%2520in%2520Ericsson%2527s%2520production%2520system%252C%2520play%2520a%2520critical%2520role%2520in%2520facilitating%2520the%2520timely%2520resolution%2520of%2520software%2520faults.%2520However%252C%2520the%2520complexity%2520and%2520volume%2520of%2520TR%2520data%252C%2520along%2520with%2520the%2520presence%2520of%2520diverse%2520criteria%2520that%2520reflect%2520different%2520aspects%2520of%2520each%2520fault%252C%2520present%2520challenges%2520for%2520retrieval%2520systems.%2520Building%2520on%2520prior%2520work%2520at%2520Ericsson%252C%2520which%2520utilized%2520a%2520two-stage%2520workflow%252C%2520comprising%2520Initial%2520Retrieval%2520%2528IR%2529%2520and%2520Re-Ranking%2520%2528RR%2529%2520stages%252C%2520this%2520study%2520investigates%2520different%2520TR%2520observation%2520criteria%2520and%2520their%2520impact%2520on%2520the%2520performance%2520of%2520retrieval%2520models.%2520We%2520propose%2520%255Ctextbf%257BCREST%257D%2520%2528%255Ctextbf%257BC%257Driteria-specific%2520%255Ctextbf%257BR%257Detrieval%2520via%2520%255Ctextbf%257BE%257Dnsemble%2520of%2520%255Ctextbf%257BS%257Dpecialized%2520%255Ctextbf%257BT%257DR%2520models%2529%252C%2520a%2520criterion-driven%2520retrieval%2520approach%2520that%2520leverages%2520specialized%2520models%2520for%2520different%2520TR%2520fields%2520to%2520improve%2520both%2520effectiveness%2520and%2520interpretability%252C%2520thereby%2520enabling%2520quicker%2520fault%2520resolution%2520and%2520supporting%2520software%2520maintenance.%2520CREST%2520utilizes%2520specialized%2520models%2520trained%2520on%2520specific%2520TR%2520criteria%2520and%2520aggregates%2520their%2520outputs%2520to%2520capture%2520diverse%2520and%2520complementary%2520signals.%2520This%2520approach%2520leads%2520to%2520enhanced%2520retrieval%2520accuracy%252C%2520better%2520calibration%2520of%2520predicted%2520scores%252C%2520and%2520improved%2520interpretability%2520by%2520providing%2520relevance%2520scores%2520for%2520each%2520criterion%252C%2520helping%2520users%2520understand%2520why%2520specific%2520TRs%2520were%2520retrieved.%2520Using%2520a%2520subset%2520of%2520Ericsson%2527s%2520internal%2520TRs%252C%2520this%2520research%2520demonstrates%2520that%2520criterion-specific%2520models%2520significantly%2520outperform%2520a%2520single%2520model%2520approach%2520across%2520key%2520evaluation%2520metrics.%2520This%2520highlights%2520the%2520importance%2520of%2520all%2520targeted%2520criteria%2520used%2520in%2520this%2520study%2520for%2520optimizing%2520the%2520performance%2520of%2520retrieval%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CREST%3A%20Improving%20Interpretability%20and%20Effectiveness%20of%20Troubleshooting%20at%20Ericsson%20through%20Criterion-Specific%20Trouble%20Report%20Retrieval&entry.906535625=Soroush%20Javdan%20and%20Pragash%20Krishnamoorthy%20and%20Olga%20Baysal&entry.1292438233=The%20rapid%20evolution%20of%20the%20telecommunication%20industry%20necessitates%20efficient%20troubleshooting%20processes%20to%20maintain%20network%20reliability%2C%20software%20maintainability%2C%20and%20service%20quality.%20Trouble%20Reports%20%28TRs%29%2C%20which%20document%20issues%20in%20Ericsson%27s%20production%20system%2C%20play%20a%20critical%20role%20in%20facilitating%20the%20timely%20resolution%20of%20software%20faults.%20However%2C%20the%20complexity%20and%20volume%20of%20TR%20data%2C%20along%20with%20the%20presence%20of%20diverse%20criteria%20that%20reflect%20different%20aspects%20of%20each%20fault%2C%20present%20challenges%20for%20retrieval%20systems.%20Building%20on%20prior%20work%20at%20Ericsson%2C%20which%20utilized%20a%20two-stage%20workflow%2C%20comprising%20Initial%20Retrieval%20%28IR%29%20and%20Re-Ranking%20%28RR%29%20stages%2C%20this%20study%20investigates%20different%20TR%20observation%20criteria%20and%20their%20impact%20on%20the%20performance%20of%20retrieval%20models.%20We%20propose%20%5Ctextbf%7BCREST%7D%20%28%5Ctextbf%7BC%7Driteria-specific%20%5Ctextbf%7BR%7Detrieval%20via%20%5Ctextbf%7BE%7Dnsemble%20of%20%5Ctextbf%7BS%7Dpecialized%20%5Ctextbf%7BT%7DR%20models%29%2C%20a%20criterion-driven%20retrieval%20approach%20that%20leverages%20specialized%20models%20for%20different%20TR%20fields%20to%20improve%20both%20effectiveness%20and%20interpretability%2C%20thereby%20enabling%20quicker%20fault%20resolution%20and%20supporting%20software%20maintenance.%20CREST%20utilizes%20specialized%20models%20trained%20on%20specific%20TR%20criteria%20and%20aggregates%20their%20outputs%20to%20capture%20diverse%20and%20complementary%20signals.%20This%20approach%20leads%20to%20enhanced%20retrieval%20accuracy%2C%20better%20calibration%20of%20predicted%20scores%2C%20and%20improved%20interpretability%20by%20providing%20relevance%20scores%20for%20each%20criterion%2C%20helping%20users%20understand%20why%20specific%20TRs%20were%20retrieved.%20Using%20a%20subset%20of%20Ericsson%27s%20internal%20TRs%2C%20this%20research%20demonstrates%20that%20criterion-specific%20models%20significantly%20outperform%20a%20single%20model%20approach%20across%20key%20evaluation%20metrics.%20This%20highlights%20the%20importance%20of%20all%20targeted%20criteria%20used%20in%20this%20study%20for%20optimizing%20the%20performance%20of%20retrieval%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2511.17417v1&entry.124074799=Read"},
{"title": "Exploring the added value of pretherapeutic MR descriptors in predicting breast cancer pathologic complete response to neoadjuvant chemotherapy", "author": "Caroline Malhaire and Fatine Selhane and Marie-Judith Saint-Martin and Vincent Cockenpot and Pia Akl and Enora Laas and Audrey Bellesoeur and Catherine Ala Eddine and Melodie Bereby-Kahane and Julie Manceau and Delphine Sebbag-Sfez and Jean-Yves Pierga and Fabien Reyal and Anne Vincent-Salomon and Herve Brisse and Frederique Frouin", "abstract": "Objectives: To evaluate the association between pretreatment MRI descriptors and breast cancer (BC) pathological complete response (pCR) to neoadjuvant chemotherapy (NAC). Materials \\& Methods: Patients with BC treated by NAC with a breast MRI between 2016 and 2020 were included in this retrospective observational single-center study. MR studies were described using the standardized BI-RADS and breast edema score on T2-weighted MRI. Univariable and multivariable logistic regression analyses were performed to assess variables association with pCR according to residual cancer burden. Random forest classifiers were trained to predict pCR on a random split including 70% of the database and were validated on the remaining cases. Results: Among 129 BC, 59 (46%) achieved pCR after NAC (luminal (n=7/37, 19%), triple negative (TN) (n=30/55, 55%), HER2+ (n=22/37, 59%). Clinical and biological items associated with pCR were BC subtype (p<0.001), T stage 0/I/II (p=0.008), higher Ki67 (p=0.005) and higher tumor-infiltrating lymphocytes levels (p=0.016). Univariate analysis showed that the following MRI features, oval or round shape (p=0.047), unifocality (p=0.026), non-spiculated margins (p=0.018), no associated non-mass enhancement (NME) (p = 0.024) and a lower MRI size (p = 0.031) were significantly associated with pCR. Unifocality and non-spiculated margins remained independently associated with pCR at multivariable analysis. Adding significant MRI features to clinicobiological variables in random forest classifiers significantly increased sensitivity (0.67 versus 0.62), specificity (0.69 versus 0.67) and precision (0.71 versus 0.67) for pCR prediction. Conclusion: Non-spiculated margins and unifocality are independently associated with pCR and can increase models performance to predict BC response to NAC. Clinical Relevance Statement: A multimodal approach integrating pretreatment MRI features with clinicobiological predictors, including TILs, could be employed to develop machine learning models for identifying patients at risk of non-response. This may enable consideration of alternative therapeutic strategies to optimize treatment outcomes", "link": "http://arxiv.org/abs/2511.17158v1", "date": "2025-11-21", "relevancy": 1.4814, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3748}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3695}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20added%20value%20of%20pretherapeutic%20MR%20descriptors%20in%20predicting%20breast%20cancer%20pathologic%20complete%20response%20to%20neoadjuvant%20chemotherapy&body=Title%3A%20Exploring%20the%20added%20value%20of%20pretherapeutic%20MR%20descriptors%20in%20predicting%20breast%20cancer%20pathologic%20complete%20response%20to%20neoadjuvant%20chemotherapy%0AAuthor%3A%20Caroline%20Malhaire%20and%20Fatine%20Selhane%20and%20Marie-Judith%20Saint-Martin%20and%20Vincent%20Cockenpot%20and%20Pia%20Akl%20and%20Enora%20Laas%20and%20Audrey%20Bellesoeur%20and%20Catherine%20Ala%20Eddine%20and%20Melodie%20Bereby-Kahane%20and%20Julie%20Manceau%20and%20Delphine%20Sebbag-Sfez%20and%20Jean-Yves%20Pierga%20and%20Fabien%20Reyal%20and%20Anne%20Vincent-Salomon%20and%20Herve%20Brisse%20and%20Frederique%20Frouin%0AAbstract%3A%20Objectives%3A%20To%20evaluate%20the%20association%20between%20pretreatment%20MRI%20descriptors%20and%20breast%20cancer%20%28BC%29%20pathological%20complete%20response%20%28pCR%29%20to%20neoadjuvant%20chemotherapy%20%28NAC%29.%20Materials%20%5C%26%20Methods%3A%20Patients%20with%20BC%20treated%20by%20NAC%20with%20a%20breast%20MRI%20between%202016%20and%202020%20were%20included%20in%20this%20retrospective%20observational%20single-center%20study.%20MR%20studies%20were%20described%20using%20the%20standardized%20BI-RADS%20and%20breast%20edema%20score%20on%20T2-weighted%20MRI.%20Univariable%20and%20multivariable%20logistic%20regression%20analyses%20were%20performed%20to%20assess%20variables%20association%20with%20pCR%20according%20to%20residual%20cancer%20burden.%20Random%20forest%20classifiers%20were%20trained%20to%20predict%20pCR%20on%20a%20random%20split%20including%2070%25%20of%20the%20database%20and%20were%20validated%20on%20the%20remaining%20cases.%20Results%3A%20Among%20129%20BC%2C%2059%20%2846%25%29%20achieved%20pCR%20after%20NAC%20%28luminal%20%28n%3D7/37%2C%2019%25%29%2C%20triple%20negative%20%28TN%29%20%28n%3D30/55%2C%2055%25%29%2C%20HER2%2B%20%28n%3D22/37%2C%2059%25%29.%20Clinical%20and%20biological%20items%20associated%20with%20pCR%20were%20BC%20subtype%20%28p%3C0.001%29%2C%20T%20stage%200/I/II%20%28p%3D0.008%29%2C%20higher%20Ki67%20%28p%3D0.005%29%20and%20higher%20tumor-infiltrating%20lymphocytes%20levels%20%28p%3D0.016%29.%20Univariate%20analysis%20showed%20that%20the%20following%20MRI%20features%2C%20oval%20or%20round%20shape%20%28p%3D0.047%29%2C%20unifocality%20%28p%3D0.026%29%2C%20non-spiculated%20margins%20%28p%3D0.018%29%2C%20no%20associated%20non-mass%20enhancement%20%28NME%29%20%28p%20%3D%200.024%29%20and%20a%20lower%20MRI%20size%20%28p%20%3D%200.031%29%20were%20significantly%20associated%20with%20pCR.%20Unifocality%20and%20non-spiculated%20margins%20remained%20independently%20associated%20with%20pCR%20at%20multivariable%20analysis.%20Adding%20significant%20MRI%20features%20to%20clinicobiological%20variables%20in%20random%20forest%20classifiers%20significantly%20increased%20sensitivity%20%280.67%20versus%200.62%29%2C%20specificity%20%280.69%20versus%200.67%29%20and%20precision%20%280.71%20versus%200.67%29%20for%20pCR%20prediction.%20Conclusion%3A%20Non-spiculated%20margins%20and%20unifocality%20are%20independently%20associated%20with%20pCR%20and%20can%20increase%20models%20performance%20to%20predict%20BC%20response%20to%20NAC.%20Clinical%20Relevance%20Statement%3A%20A%20multimodal%20approach%20integrating%20pretreatment%20MRI%20features%20with%20clinicobiological%20predictors%2C%20including%20TILs%2C%20could%20be%20employed%20to%20develop%20machine%20learning%20models%20for%20identifying%20patients%20at%20risk%20of%20non-response.%20This%20may%20enable%20consideration%20of%20alternative%20therapeutic%20strategies%20to%20optimize%20treatment%20outcomes%0ALink%3A%20http%3A//arxiv.org/abs/2511.17158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520added%2520value%2520of%2520pretherapeutic%2520MR%2520descriptors%2520in%2520predicting%2520breast%2520cancer%2520pathologic%2520complete%2520response%2520to%2520neoadjuvant%2520chemotherapy%26entry.906535625%3DCaroline%2520Malhaire%2520and%2520Fatine%2520Selhane%2520and%2520Marie-Judith%2520Saint-Martin%2520and%2520Vincent%2520Cockenpot%2520and%2520Pia%2520Akl%2520and%2520Enora%2520Laas%2520and%2520Audrey%2520Bellesoeur%2520and%2520Catherine%2520Ala%2520Eddine%2520and%2520Melodie%2520Bereby-Kahane%2520and%2520Julie%2520Manceau%2520and%2520Delphine%2520Sebbag-Sfez%2520and%2520Jean-Yves%2520Pierga%2520and%2520Fabien%2520Reyal%2520and%2520Anne%2520Vincent-Salomon%2520and%2520Herve%2520Brisse%2520and%2520Frederique%2520Frouin%26entry.1292438233%3DObjectives%253A%2520To%2520evaluate%2520the%2520association%2520between%2520pretreatment%2520MRI%2520descriptors%2520and%2520breast%2520cancer%2520%2528BC%2529%2520pathological%2520complete%2520response%2520%2528pCR%2529%2520to%2520neoadjuvant%2520chemotherapy%2520%2528NAC%2529.%2520Materials%2520%255C%2526%2520Methods%253A%2520Patients%2520with%2520BC%2520treated%2520by%2520NAC%2520with%2520a%2520breast%2520MRI%2520between%25202016%2520and%25202020%2520were%2520included%2520in%2520this%2520retrospective%2520observational%2520single-center%2520study.%2520MR%2520studies%2520were%2520described%2520using%2520the%2520standardized%2520BI-RADS%2520and%2520breast%2520edema%2520score%2520on%2520T2-weighted%2520MRI.%2520Univariable%2520and%2520multivariable%2520logistic%2520regression%2520analyses%2520were%2520performed%2520to%2520assess%2520variables%2520association%2520with%2520pCR%2520according%2520to%2520residual%2520cancer%2520burden.%2520Random%2520forest%2520classifiers%2520were%2520trained%2520to%2520predict%2520pCR%2520on%2520a%2520random%2520split%2520including%252070%2525%2520of%2520the%2520database%2520and%2520were%2520validated%2520on%2520the%2520remaining%2520cases.%2520Results%253A%2520Among%2520129%2520BC%252C%252059%2520%252846%2525%2529%2520achieved%2520pCR%2520after%2520NAC%2520%2528luminal%2520%2528n%253D7/37%252C%252019%2525%2529%252C%2520triple%2520negative%2520%2528TN%2529%2520%2528n%253D30/55%252C%252055%2525%2529%252C%2520HER2%252B%2520%2528n%253D22/37%252C%252059%2525%2529.%2520Clinical%2520and%2520biological%2520items%2520associated%2520with%2520pCR%2520were%2520BC%2520subtype%2520%2528p%253C0.001%2529%252C%2520T%2520stage%25200/I/II%2520%2528p%253D0.008%2529%252C%2520higher%2520Ki67%2520%2528p%253D0.005%2529%2520and%2520higher%2520tumor-infiltrating%2520lymphocytes%2520levels%2520%2528p%253D0.016%2529.%2520Univariate%2520analysis%2520showed%2520that%2520the%2520following%2520MRI%2520features%252C%2520oval%2520or%2520round%2520shape%2520%2528p%253D0.047%2529%252C%2520unifocality%2520%2528p%253D0.026%2529%252C%2520non-spiculated%2520margins%2520%2528p%253D0.018%2529%252C%2520no%2520associated%2520non-mass%2520enhancement%2520%2528NME%2529%2520%2528p%2520%253D%25200.024%2529%2520and%2520a%2520lower%2520MRI%2520size%2520%2528p%2520%253D%25200.031%2529%2520were%2520significantly%2520associated%2520with%2520pCR.%2520Unifocality%2520and%2520non-spiculated%2520margins%2520remained%2520independently%2520associated%2520with%2520pCR%2520at%2520multivariable%2520analysis.%2520Adding%2520significant%2520MRI%2520features%2520to%2520clinicobiological%2520variables%2520in%2520random%2520forest%2520classifiers%2520significantly%2520increased%2520sensitivity%2520%25280.67%2520versus%25200.62%2529%252C%2520specificity%2520%25280.69%2520versus%25200.67%2529%2520and%2520precision%2520%25280.71%2520versus%25200.67%2529%2520for%2520pCR%2520prediction.%2520Conclusion%253A%2520Non-spiculated%2520margins%2520and%2520unifocality%2520are%2520independently%2520associated%2520with%2520pCR%2520and%2520can%2520increase%2520models%2520performance%2520to%2520predict%2520BC%2520response%2520to%2520NAC.%2520Clinical%2520Relevance%2520Statement%253A%2520A%2520multimodal%2520approach%2520integrating%2520pretreatment%2520MRI%2520features%2520with%2520clinicobiological%2520predictors%252C%2520including%2520TILs%252C%2520could%2520be%2520employed%2520to%2520develop%2520machine%2520learning%2520models%2520for%2520identifying%2520patients%2520at%2520risk%2520of%2520non-response.%2520This%2520may%2520enable%2520consideration%2520of%2520alternative%2520therapeutic%2520strategies%2520to%2520optimize%2520treatment%2520outcomes%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20added%20value%20of%20pretherapeutic%20MR%20descriptors%20in%20predicting%20breast%20cancer%20pathologic%20complete%20response%20to%20neoadjuvant%20chemotherapy&entry.906535625=Caroline%20Malhaire%20and%20Fatine%20Selhane%20and%20Marie-Judith%20Saint-Martin%20and%20Vincent%20Cockenpot%20and%20Pia%20Akl%20and%20Enora%20Laas%20and%20Audrey%20Bellesoeur%20and%20Catherine%20Ala%20Eddine%20and%20Melodie%20Bereby-Kahane%20and%20Julie%20Manceau%20and%20Delphine%20Sebbag-Sfez%20and%20Jean-Yves%20Pierga%20and%20Fabien%20Reyal%20and%20Anne%20Vincent-Salomon%20and%20Herve%20Brisse%20and%20Frederique%20Frouin&entry.1292438233=Objectives%3A%20To%20evaluate%20the%20association%20between%20pretreatment%20MRI%20descriptors%20and%20breast%20cancer%20%28BC%29%20pathological%20complete%20response%20%28pCR%29%20to%20neoadjuvant%20chemotherapy%20%28NAC%29.%20Materials%20%5C%26%20Methods%3A%20Patients%20with%20BC%20treated%20by%20NAC%20with%20a%20breast%20MRI%20between%202016%20and%202020%20were%20included%20in%20this%20retrospective%20observational%20single-center%20study.%20MR%20studies%20were%20described%20using%20the%20standardized%20BI-RADS%20and%20breast%20edema%20score%20on%20T2-weighted%20MRI.%20Univariable%20and%20multivariable%20logistic%20regression%20analyses%20were%20performed%20to%20assess%20variables%20association%20with%20pCR%20according%20to%20residual%20cancer%20burden.%20Random%20forest%20classifiers%20were%20trained%20to%20predict%20pCR%20on%20a%20random%20split%20including%2070%25%20of%20the%20database%20and%20were%20validated%20on%20the%20remaining%20cases.%20Results%3A%20Among%20129%20BC%2C%2059%20%2846%25%29%20achieved%20pCR%20after%20NAC%20%28luminal%20%28n%3D7/37%2C%2019%25%29%2C%20triple%20negative%20%28TN%29%20%28n%3D30/55%2C%2055%25%29%2C%20HER2%2B%20%28n%3D22/37%2C%2059%25%29.%20Clinical%20and%20biological%20items%20associated%20with%20pCR%20were%20BC%20subtype%20%28p%3C0.001%29%2C%20T%20stage%200/I/II%20%28p%3D0.008%29%2C%20higher%20Ki67%20%28p%3D0.005%29%20and%20higher%20tumor-infiltrating%20lymphocytes%20levels%20%28p%3D0.016%29.%20Univariate%20analysis%20showed%20that%20the%20following%20MRI%20features%2C%20oval%20or%20round%20shape%20%28p%3D0.047%29%2C%20unifocality%20%28p%3D0.026%29%2C%20non-spiculated%20margins%20%28p%3D0.018%29%2C%20no%20associated%20non-mass%20enhancement%20%28NME%29%20%28p%20%3D%200.024%29%20and%20a%20lower%20MRI%20size%20%28p%20%3D%200.031%29%20were%20significantly%20associated%20with%20pCR.%20Unifocality%20and%20non-spiculated%20margins%20remained%20independently%20associated%20with%20pCR%20at%20multivariable%20analysis.%20Adding%20significant%20MRI%20features%20to%20clinicobiological%20variables%20in%20random%20forest%20classifiers%20significantly%20increased%20sensitivity%20%280.67%20versus%200.62%29%2C%20specificity%20%280.69%20versus%200.67%29%20and%20precision%20%280.71%20versus%200.67%29%20for%20pCR%20prediction.%20Conclusion%3A%20Non-spiculated%20margins%20and%20unifocality%20are%20independently%20associated%20with%20pCR%20and%20can%20increase%20models%20performance%20to%20predict%20BC%20response%20to%20NAC.%20Clinical%20Relevance%20Statement%3A%20A%20multimodal%20approach%20integrating%20pretreatment%20MRI%20features%20with%20clinicobiological%20predictors%2C%20including%20TILs%2C%20could%20be%20employed%20to%20develop%20machine%20learning%20models%20for%20identifying%20patients%20at%20risk%20of%20non-response.%20This%20may%20enable%20consideration%20of%20alternative%20therapeutic%20strategies%20to%20optimize%20treatment%20outcomes&entry.1838667208=http%3A//arxiv.org/abs/2511.17158v1&entry.124074799=Read"},
{"title": "R2PS: Worst-Case Robust Real-Time Pursuit Strategies under Partial Observability", "author": "Runyu Lu and Ruochuan Shi and Yuanheng Zhu and Dongbin Zhao", "abstract": "Computing worst-case robust strategies in pursuit-evasion games (PEGs) is time-consuming, especially when real-world factors like partial observability are considered. While important for general security purposes, real-time applicable pursuit strategies for graph-based PEGs are currently missing when the pursuers only have imperfect information about the evader's position. Although state-of-the-art reinforcement learning (RL) methods like Equilibrium Policy Generalization (EPG) and Grasper provide guidelines for learning graph neural network (GNN) policies robust to different game dynamics, they are restricted to the scenario of perfect information and do not take into account the possible case where the evader can predict the pursuers' actions. This paper introduces the first approach to worst-case robust real-time pursuit strategies (R2PS) under partial observability. We first prove that a traditional dynamic programming (DP) algorithm for solving Markov PEGs maintains optimality under the asynchronous moves by the evader. Then, we propose a belief preservation mechanism about the evader's possible positions, extending the DP pursuit strategies to a partially observable setting. Finally, we embed the belief preservation into the state-of-the-art EPG framework to finish our R2PS learning scheme, which leads to a real-time pursuer policy through cross-graph reinforcement learning against the asynchronous-move DP evasion strategies. After reinforcement learning, our policy achieves robust zero-shot generalization to unseen real-world graph structures and consistently outperforms the policy directly trained on the test graphs by the existing game RL approach.", "link": "http://arxiv.org/abs/2511.17367v1", "date": "2025-11-21", "relevancy": 1.3968, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.48}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4777}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R2PS%3A%20Worst-Case%20Robust%20Real-Time%20Pursuit%20Strategies%20under%20Partial%20Observability&body=Title%3A%20R2PS%3A%20Worst-Case%20Robust%20Real-Time%20Pursuit%20Strategies%20under%20Partial%20Observability%0AAuthor%3A%20Runyu%20Lu%20and%20Ruochuan%20Shi%20and%20Yuanheng%20Zhu%20and%20Dongbin%20Zhao%0AAbstract%3A%20Computing%20worst-case%20robust%20strategies%20in%20pursuit-evasion%20games%20%28PEGs%29%20is%20time-consuming%2C%20especially%20when%20real-world%20factors%20like%20partial%20observability%20are%20considered.%20While%20important%20for%20general%20security%20purposes%2C%20real-time%20applicable%20pursuit%20strategies%20for%20graph-based%20PEGs%20are%20currently%20missing%20when%20the%20pursuers%20only%20have%20imperfect%20information%20about%20the%20evader%27s%20position.%20Although%20state-of-the-art%20reinforcement%20learning%20%28RL%29%20methods%20like%20Equilibrium%20Policy%20Generalization%20%28EPG%29%20and%20Grasper%20provide%20guidelines%20for%20learning%20graph%20neural%20network%20%28GNN%29%20policies%20robust%20to%20different%20game%20dynamics%2C%20they%20are%20restricted%20to%20the%20scenario%20of%20perfect%20information%20and%20do%20not%20take%20into%20account%20the%20possible%20case%20where%20the%20evader%20can%20predict%20the%20pursuers%27%20actions.%20This%20paper%20introduces%20the%20first%20approach%20to%20worst-case%20robust%20real-time%20pursuit%20strategies%20%28R2PS%29%20under%20partial%20observability.%20We%20first%20prove%20that%20a%20traditional%20dynamic%20programming%20%28DP%29%20algorithm%20for%20solving%20Markov%20PEGs%20maintains%20optimality%20under%20the%20asynchronous%20moves%20by%20the%20evader.%20Then%2C%20we%20propose%20a%20belief%20preservation%20mechanism%20about%20the%20evader%27s%20possible%20positions%2C%20extending%20the%20DP%20pursuit%20strategies%20to%20a%20partially%20observable%20setting.%20Finally%2C%20we%20embed%20the%20belief%20preservation%20into%20the%20state-of-the-art%20EPG%20framework%20to%20finish%20our%20R2PS%20learning%20scheme%2C%20which%20leads%20to%20a%20real-time%20pursuer%20policy%20through%20cross-graph%20reinforcement%20learning%20against%20the%20asynchronous-move%20DP%20evasion%20strategies.%20After%20reinforcement%20learning%2C%20our%20policy%20achieves%20robust%20zero-shot%20generalization%20to%20unseen%20real-world%20graph%20structures%20and%20consistently%20outperforms%20the%20policy%20directly%20trained%20on%20the%20test%20graphs%20by%20the%20existing%20game%20RL%20approach.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR2PS%253A%2520Worst-Case%2520Robust%2520Real-Time%2520Pursuit%2520Strategies%2520under%2520Partial%2520Observability%26entry.906535625%3DRunyu%2520Lu%2520and%2520Ruochuan%2520Shi%2520and%2520Yuanheng%2520Zhu%2520and%2520Dongbin%2520Zhao%26entry.1292438233%3DComputing%2520worst-case%2520robust%2520strategies%2520in%2520pursuit-evasion%2520games%2520%2528PEGs%2529%2520is%2520time-consuming%252C%2520especially%2520when%2520real-world%2520factors%2520like%2520partial%2520observability%2520are%2520considered.%2520While%2520important%2520for%2520general%2520security%2520purposes%252C%2520real-time%2520applicable%2520pursuit%2520strategies%2520for%2520graph-based%2520PEGs%2520are%2520currently%2520missing%2520when%2520the%2520pursuers%2520only%2520have%2520imperfect%2520information%2520about%2520the%2520evader%2527s%2520position.%2520Although%2520state-of-the-art%2520reinforcement%2520learning%2520%2528RL%2529%2520methods%2520like%2520Equilibrium%2520Policy%2520Generalization%2520%2528EPG%2529%2520and%2520Grasper%2520provide%2520guidelines%2520for%2520learning%2520graph%2520neural%2520network%2520%2528GNN%2529%2520policies%2520robust%2520to%2520different%2520game%2520dynamics%252C%2520they%2520are%2520restricted%2520to%2520the%2520scenario%2520of%2520perfect%2520information%2520and%2520do%2520not%2520take%2520into%2520account%2520the%2520possible%2520case%2520where%2520the%2520evader%2520can%2520predict%2520the%2520pursuers%2527%2520actions.%2520This%2520paper%2520introduces%2520the%2520first%2520approach%2520to%2520worst-case%2520robust%2520real-time%2520pursuit%2520strategies%2520%2528R2PS%2529%2520under%2520partial%2520observability.%2520We%2520first%2520prove%2520that%2520a%2520traditional%2520dynamic%2520programming%2520%2528DP%2529%2520algorithm%2520for%2520solving%2520Markov%2520PEGs%2520maintains%2520optimality%2520under%2520the%2520asynchronous%2520moves%2520by%2520the%2520evader.%2520Then%252C%2520we%2520propose%2520a%2520belief%2520preservation%2520mechanism%2520about%2520the%2520evader%2527s%2520possible%2520positions%252C%2520extending%2520the%2520DP%2520pursuit%2520strategies%2520to%2520a%2520partially%2520observable%2520setting.%2520Finally%252C%2520we%2520embed%2520the%2520belief%2520preservation%2520into%2520the%2520state-of-the-art%2520EPG%2520framework%2520to%2520finish%2520our%2520R2PS%2520learning%2520scheme%252C%2520which%2520leads%2520to%2520a%2520real-time%2520pursuer%2520policy%2520through%2520cross-graph%2520reinforcement%2520learning%2520against%2520the%2520asynchronous-move%2520DP%2520evasion%2520strategies.%2520After%2520reinforcement%2520learning%252C%2520our%2520policy%2520achieves%2520robust%2520zero-shot%2520generalization%2520to%2520unseen%2520real-world%2520graph%2520structures%2520and%2520consistently%2520outperforms%2520the%2520policy%2520directly%2520trained%2520on%2520the%2520test%2520graphs%2520by%2520the%2520existing%2520game%2520RL%2520approach.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R2PS%3A%20Worst-Case%20Robust%20Real-Time%20Pursuit%20Strategies%20under%20Partial%20Observability&entry.906535625=Runyu%20Lu%20and%20Ruochuan%20Shi%20and%20Yuanheng%20Zhu%20and%20Dongbin%20Zhao&entry.1292438233=Computing%20worst-case%20robust%20strategies%20in%20pursuit-evasion%20games%20%28PEGs%29%20is%20time-consuming%2C%20especially%20when%20real-world%20factors%20like%20partial%20observability%20are%20considered.%20While%20important%20for%20general%20security%20purposes%2C%20real-time%20applicable%20pursuit%20strategies%20for%20graph-based%20PEGs%20are%20currently%20missing%20when%20the%20pursuers%20only%20have%20imperfect%20information%20about%20the%20evader%27s%20position.%20Although%20state-of-the-art%20reinforcement%20learning%20%28RL%29%20methods%20like%20Equilibrium%20Policy%20Generalization%20%28EPG%29%20and%20Grasper%20provide%20guidelines%20for%20learning%20graph%20neural%20network%20%28GNN%29%20policies%20robust%20to%20different%20game%20dynamics%2C%20they%20are%20restricted%20to%20the%20scenario%20of%20perfect%20information%20and%20do%20not%20take%20into%20account%20the%20possible%20case%20where%20the%20evader%20can%20predict%20the%20pursuers%27%20actions.%20This%20paper%20introduces%20the%20first%20approach%20to%20worst-case%20robust%20real-time%20pursuit%20strategies%20%28R2PS%29%20under%20partial%20observability.%20We%20first%20prove%20that%20a%20traditional%20dynamic%20programming%20%28DP%29%20algorithm%20for%20solving%20Markov%20PEGs%20maintains%20optimality%20under%20the%20asynchronous%20moves%20by%20the%20evader.%20Then%2C%20we%20propose%20a%20belief%20preservation%20mechanism%20about%20the%20evader%27s%20possible%20positions%2C%20extending%20the%20DP%20pursuit%20strategies%20to%20a%20partially%20observable%20setting.%20Finally%2C%20we%20embed%20the%20belief%20preservation%20into%20the%20state-of-the-art%20EPG%20framework%20to%20finish%20our%20R2PS%20learning%20scheme%2C%20which%20leads%20to%20a%20real-time%20pursuer%20policy%20through%20cross-graph%20reinforcement%20learning%20against%20the%20asynchronous-move%20DP%20evasion%20strategies.%20After%20reinforcement%20learning%2C%20our%20policy%20achieves%20robust%20zero-shot%20generalization%20to%20unseen%20real-world%20graph%20structures%20and%20consistently%20outperforms%20the%20policy%20directly%20trained%20on%20the%20test%20graphs%20by%20the%20existing%20game%20RL%20approach.&entry.1838667208=http%3A//arxiv.org/abs/2511.17367v1&entry.124074799=Read"},
{"title": "Dual-domain Adaptation Networks for Realistic Image Super-resolution", "author": "Chaowei Fang and Bolin Fu and De Cheng and Lechao Cheng and Guanbin Li", "abstract": "Realistic image super-resolution (SR) focuses on transforming real-world low-resolution (LR) images into high-resolution (HR) ones, handling more complex degradation patterns than synthetic SR tasks. This is critical for applications like surveillance, medical imaging, and consumer electronics. However, current methods struggle with limited real-world LR-HR data, impacting the learning of basic image features. Pre-trained SR models from large-scale synthetic datasets offer valuable prior knowledge, which can improve generalization, speed up training, and reduce the need for extensive real-world data in realistic SR tasks. In this paper, we introduce a novel approach, Dual-domain Adaptation Networks, which is able to efficiently adapt pre-trained image SR models from simulated to real-world datasets. To achieve this target, we first set up a spatial-domain adaptation strategy through selectively updating parameters of pre-trained models and employing the low-rank adaptation technique to adjust frozen parameters. Recognizing that image super-resolution involves recovering high-frequency components, we further integrate a frequency domain adaptation branch into the adapted model, which combines the spectral data of the input and the spatial-domain backbone's intermediate features to infer HR frequency maps, enhancing the SR result. Experimental evaluations on public realistic image SR benchmarks, including RealSR, D2CRealSR, and DRealSR, demonstrate the superiority of our proposed method over existing state-of-the-art models. Codes are available at: https://github.com/dummerchen/DAN.", "link": "http://arxiv.org/abs/2511.17217v1", "date": "2025-11-21", "relevancy": 1.6013, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5519}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5339}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-domain%20Adaptation%20Networks%20for%20Realistic%20Image%20Super-resolution&body=Title%3A%20Dual-domain%20Adaptation%20Networks%20for%20Realistic%20Image%20Super-resolution%0AAuthor%3A%20Chaowei%20Fang%20and%20Bolin%20Fu%20and%20De%20Cheng%20and%20Lechao%20Cheng%20and%20Guanbin%20Li%0AAbstract%3A%20Realistic%20image%20super-resolution%20%28SR%29%20focuses%20on%20transforming%20real-world%20low-resolution%20%28LR%29%20images%20into%20high-resolution%20%28HR%29%20ones%2C%20handling%20more%20complex%20degradation%20patterns%20than%20synthetic%20SR%20tasks.%20This%20is%20critical%20for%20applications%20like%20surveillance%2C%20medical%20imaging%2C%20and%20consumer%20electronics.%20However%2C%20current%20methods%20struggle%20with%20limited%20real-world%20LR-HR%20data%2C%20impacting%20the%20learning%20of%20basic%20image%20features.%20Pre-trained%20SR%20models%20from%20large-scale%20synthetic%20datasets%20offer%20valuable%20prior%20knowledge%2C%20which%20can%20improve%20generalization%2C%20speed%20up%20training%2C%20and%20reduce%20the%20need%20for%20extensive%20real-world%20data%20in%20realistic%20SR%20tasks.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%2C%20Dual-domain%20Adaptation%20Networks%2C%20which%20is%20able%20to%20efficiently%20adapt%20pre-trained%20image%20SR%20models%20from%20simulated%20to%20real-world%20datasets.%20To%20achieve%20this%20target%2C%20we%20first%20set%20up%20a%20spatial-domain%20adaptation%20strategy%20through%20selectively%20updating%20parameters%20of%20pre-trained%20models%20and%20employing%20the%20low-rank%20adaptation%20technique%20to%20adjust%20frozen%20parameters.%20Recognizing%20that%20image%20super-resolution%20involves%20recovering%20high-frequency%20components%2C%20we%20further%20integrate%20a%20frequency%20domain%20adaptation%20branch%20into%20the%20adapted%20model%2C%20which%20combines%20the%20spectral%20data%20of%20the%20input%20and%20the%20spatial-domain%20backbone%27s%20intermediate%20features%20to%20infer%20HR%20frequency%20maps%2C%20enhancing%20the%20SR%20result.%20Experimental%20evaluations%20on%20public%20realistic%20image%20SR%20benchmarks%2C%20including%20RealSR%2C%20D2CRealSR%2C%20and%20DRealSR%2C%20demonstrate%20the%20superiority%20of%20our%20proposed%20method%20over%20existing%20state-of-the-art%20models.%20Codes%20are%20available%20at%3A%20https%3A//github.com/dummerchen/DAN.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17217v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-domain%2520Adaptation%2520Networks%2520for%2520Realistic%2520Image%2520Super-resolution%26entry.906535625%3DChaowei%2520Fang%2520and%2520Bolin%2520Fu%2520and%2520De%2520Cheng%2520and%2520Lechao%2520Cheng%2520and%2520Guanbin%2520Li%26entry.1292438233%3DRealistic%2520image%2520super-resolution%2520%2528SR%2529%2520focuses%2520on%2520transforming%2520real-world%2520low-resolution%2520%2528LR%2529%2520images%2520into%2520high-resolution%2520%2528HR%2529%2520ones%252C%2520handling%2520more%2520complex%2520degradation%2520patterns%2520than%2520synthetic%2520SR%2520tasks.%2520This%2520is%2520critical%2520for%2520applications%2520like%2520surveillance%252C%2520medical%2520imaging%252C%2520and%2520consumer%2520electronics.%2520However%252C%2520current%2520methods%2520struggle%2520with%2520limited%2520real-world%2520LR-HR%2520data%252C%2520impacting%2520the%2520learning%2520of%2520basic%2520image%2520features.%2520Pre-trained%2520SR%2520models%2520from%2520large-scale%2520synthetic%2520datasets%2520offer%2520valuable%2520prior%2520knowledge%252C%2520which%2520can%2520improve%2520generalization%252C%2520speed%2520up%2520training%252C%2520and%2520reduce%2520the%2520need%2520for%2520extensive%2520real-world%2520data%2520in%2520realistic%2520SR%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520approach%252C%2520Dual-domain%2520Adaptation%2520Networks%252C%2520which%2520is%2520able%2520to%2520efficiently%2520adapt%2520pre-trained%2520image%2520SR%2520models%2520from%2520simulated%2520to%2520real-world%2520datasets.%2520To%2520achieve%2520this%2520target%252C%2520we%2520first%2520set%2520up%2520a%2520spatial-domain%2520adaptation%2520strategy%2520through%2520selectively%2520updating%2520parameters%2520of%2520pre-trained%2520models%2520and%2520employing%2520the%2520low-rank%2520adaptation%2520technique%2520to%2520adjust%2520frozen%2520parameters.%2520Recognizing%2520that%2520image%2520super-resolution%2520involves%2520recovering%2520high-frequency%2520components%252C%2520we%2520further%2520integrate%2520a%2520frequency%2520domain%2520adaptation%2520branch%2520into%2520the%2520adapted%2520model%252C%2520which%2520combines%2520the%2520spectral%2520data%2520of%2520the%2520input%2520and%2520the%2520spatial-domain%2520backbone%2527s%2520intermediate%2520features%2520to%2520infer%2520HR%2520frequency%2520maps%252C%2520enhancing%2520the%2520SR%2520result.%2520Experimental%2520evaluations%2520on%2520public%2520realistic%2520image%2520SR%2520benchmarks%252C%2520including%2520RealSR%252C%2520D2CRealSR%252C%2520and%2520DRealSR%252C%2520demonstrate%2520the%2520superiority%2520of%2520our%2520proposed%2520method%2520over%2520existing%2520state-of-the-art%2520models.%2520Codes%2520are%2520available%2520at%253A%2520https%253A//github.com/dummerchen/DAN.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17217v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-domain%20Adaptation%20Networks%20for%20Realistic%20Image%20Super-resolution&entry.906535625=Chaowei%20Fang%20and%20Bolin%20Fu%20and%20De%20Cheng%20and%20Lechao%20Cheng%20and%20Guanbin%20Li&entry.1292438233=Realistic%20image%20super-resolution%20%28SR%29%20focuses%20on%20transforming%20real-world%20low-resolution%20%28LR%29%20images%20into%20high-resolution%20%28HR%29%20ones%2C%20handling%20more%20complex%20degradation%20patterns%20than%20synthetic%20SR%20tasks.%20This%20is%20critical%20for%20applications%20like%20surveillance%2C%20medical%20imaging%2C%20and%20consumer%20electronics.%20However%2C%20current%20methods%20struggle%20with%20limited%20real-world%20LR-HR%20data%2C%20impacting%20the%20learning%20of%20basic%20image%20features.%20Pre-trained%20SR%20models%20from%20large-scale%20synthetic%20datasets%20offer%20valuable%20prior%20knowledge%2C%20which%20can%20improve%20generalization%2C%20speed%20up%20training%2C%20and%20reduce%20the%20need%20for%20extensive%20real-world%20data%20in%20realistic%20SR%20tasks.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%2C%20Dual-domain%20Adaptation%20Networks%2C%20which%20is%20able%20to%20efficiently%20adapt%20pre-trained%20image%20SR%20models%20from%20simulated%20to%20real-world%20datasets.%20To%20achieve%20this%20target%2C%20we%20first%20set%20up%20a%20spatial-domain%20adaptation%20strategy%20through%20selectively%20updating%20parameters%20of%20pre-trained%20models%20and%20employing%20the%20low-rank%20adaptation%20technique%20to%20adjust%20frozen%20parameters.%20Recognizing%20that%20image%20super-resolution%20involves%20recovering%20high-frequency%20components%2C%20we%20further%20integrate%20a%20frequency%20domain%20adaptation%20branch%20into%20the%20adapted%20model%2C%20which%20combines%20the%20spectral%20data%20of%20the%20input%20and%20the%20spatial-domain%20backbone%27s%20intermediate%20features%20to%20infer%20HR%20frequency%20maps%2C%20enhancing%20the%20SR%20result.%20Experimental%20evaluations%20on%20public%20realistic%20image%20SR%20benchmarks%2C%20including%20RealSR%2C%20D2CRealSR%2C%20and%20DRealSR%2C%20demonstrate%20the%20superiority%20of%20our%20proposed%20method%20over%20existing%20state-of-the-art%20models.%20Codes%20are%20available%20at%3A%20https%3A//github.com/dummerchen/DAN.&entry.1838667208=http%3A//arxiv.org/abs/2511.17217v1&entry.124074799=Read"},
{"title": "Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration", "author": "Jiaxun Fang and Li Zhang and Shaoyi Huang", "abstract": "Systolic array accelerators execute CNNs with energy dominated by the switching activity of multiply accumulate (MAC) units. Although prior work exploits weight dependent MAC power for compression, existing methods often use global activation models, coarse energy proxies, or layer-agnostic policies, which limits their effectiveness on real hardware. We propose an energy aware, layer-wise compression framework that explicitly leverages MAC and layer level energy characteristics. First, we build a layer-aware MAC energy model that combines per-layer activation statistics with an MSB-Hamming distance grouping of 22-bit partial sum transitions, and integrate it with a tile-level systolic mapping to estimate convolution-layer energy. On top of this model, we introduce an energy accuracy co-optimized weight selection algorithm within quantization aware training and an energy-prioritized layer-wise schedule that compresses high energy layers more aggressively under a global accuracy constraint. Experiments on different CNN models demonstrate up to 58.6\\% energy reduction with 2-3\\% accuracy drop, outperforming a state-of-the-art power-aware baseline.", "link": "http://arxiv.org/abs/2511.17123v1", "date": "2025-11-21", "relevancy": 1.4238, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4943}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4708}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Layer-wise%20Weight%20Selection%20for%20Power-Efficient%20Neural%20Network%20Acceleration&body=Title%3A%20Layer-wise%20Weight%20Selection%20for%20Power-Efficient%20Neural%20Network%20Acceleration%0AAuthor%3A%20Jiaxun%20Fang%20and%20Li%20Zhang%20and%20Shaoyi%20Huang%0AAbstract%3A%20Systolic%20array%20accelerators%20execute%20CNNs%20with%20energy%20dominated%20by%20the%20switching%20activity%20of%20multiply%20accumulate%20%28MAC%29%20units.%20Although%20prior%20work%20exploits%20weight%20dependent%20MAC%20power%20for%20compression%2C%20existing%20methods%20often%20use%20global%20activation%20models%2C%20coarse%20energy%20proxies%2C%20or%20layer-agnostic%20policies%2C%20which%20limits%20their%20effectiveness%20on%20real%20hardware.%20We%20propose%20an%20energy%20aware%2C%20layer-wise%20compression%20framework%20that%20explicitly%20leverages%20MAC%20and%20layer%20level%20energy%20characteristics.%20First%2C%20we%20build%20a%20layer-aware%20MAC%20energy%20model%20that%20combines%20per-layer%20activation%20statistics%20with%20an%20MSB-Hamming%20distance%20grouping%20of%2022-bit%20partial%20sum%20transitions%2C%20and%20integrate%20it%20with%20a%20tile-level%20systolic%20mapping%20to%20estimate%20convolution-layer%20energy.%20On%20top%20of%20this%20model%2C%20we%20introduce%20an%20energy%20accuracy%20co-optimized%20weight%20selection%20algorithm%20within%20quantization%20aware%20training%20and%20an%20energy-prioritized%20layer-wise%20schedule%20that%20compresses%20high%20energy%20layers%20more%20aggressively%20under%20a%20global%20accuracy%20constraint.%20Experiments%20on%20different%20CNN%20models%20demonstrate%20up%20to%2058.6%5C%25%20energy%20reduction%20with%202-3%5C%25%20accuracy%20drop%2C%20outperforming%20a%20state-of-the-art%20power-aware%20baseline.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17123v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayer-wise%2520Weight%2520Selection%2520for%2520Power-Efficient%2520Neural%2520Network%2520Acceleration%26entry.906535625%3DJiaxun%2520Fang%2520and%2520Li%2520Zhang%2520and%2520Shaoyi%2520Huang%26entry.1292438233%3DSystolic%2520array%2520accelerators%2520execute%2520CNNs%2520with%2520energy%2520dominated%2520by%2520the%2520switching%2520activity%2520of%2520multiply%2520accumulate%2520%2528MAC%2529%2520units.%2520Although%2520prior%2520work%2520exploits%2520weight%2520dependent%2520MAC%2520power%2520for%2520compression%252C%2520existing%2520methods%2520often%2520use%2520global%2520activation%2520models%252C%2520coarse%2520energy%2520proxies%252C%2520or%2520layer-agnostic%2520policies%252C%2520which%2520limits%2520their%2520effectiveness%2520on%2520real%2520hardware.%2520We%2520propose%2520an%2520energy%2520aware%252C%2520layer-wise%2520compression%2520framework%2520that%2520explicitly%2520leverages%2520MAC%2520and%2520layer%2520level%2520energy%2520characteristics.%2520First%252C%2520we%2520build%2520a%2520layer-aware%2520MAC%2520energy%2520model%2520that%2520combines%2520per-layer%2520activation%2520statistics%2520with%2520an%2520MSB-Hamming%2520distance%2520grouping%2520of%252022-bit%2520partial%2520sum%2520transitions%252C%2520and%2520integrate%2520it%2520with%2520a%2520tile-level%2520systolic%2520mapping%2520to%2520estimate%2520convolution-layer%2520energy.%2520On%2520top%2520of%2520this%2520model%252C%2520we%2520introduce%2520an%2520energy%2520accuracy%2520co-optimized%2520weight%2520selection%2520algorithm%2520within%2520quantization%2520aware%2520training%2520and%2520an%2520energy-prioritized%2520layer-wise%2520schedule%2520that%2520compresses%2520high%2520energy%2520layers%2520more%2520aggressively%2520under%2520a%2520global%2520accuracy%2520constraint.%2520Experiments%2520on%2520different%2520CNN%2520models%2520demonstrate%2520up%2520to%252058.6%255C%2525%2520energy%2520reduction%2520with%25202-3%255C%2525%2520accuracy%2520drop%252C%2520outperforming%2520a%2520state-of-the-art%2520power-aware%2520baseline.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17123v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Layer-wise%20Weight%20Selection%20for%20Power-Efficient%20Neural%20Network%20Acceleration&entry.906535625=Jiaxun%20Fang%20and%20Li%20Zhang%20and%20Shaoyi%20Huang&entry.1292438233=Systolic%20array%20accelerators%20execute%20CNNs%20with%20energy%20dominated%20by%20the%20switching%20activity%20of%20multiply%20accumulate%20%28MAC%29%20units.%20Although%20prior%20work%20exploits%20weight%20dependent%20MAC%20power%20for%20compression%2C%20existing%20methods%20often%20use%20global%20activation%20models%2C%20coarse%20energy%20proxies%2C%20or%20layer-agnostic%20policies%2C%20which%20limits%20their%20effectiveness%20on%20real%20hardware.%20We%20propose%20an%20energy%20aware%2C%20layer-wise%20compression%20framework%20that%20explicitly%20leverages%20MAC%20and%20layer%20level%20energy%20characteristics.%20First%2C%20we%20build%20a%20layer-aware%20MAC%20energy%20model%20that%20combines%20per-layer%20activation%20statistics%20with%20an%20MSB-Hamming%20distance%20grouping%20of%2022-bit%20partial%20sum%20transitions%2C%20and%20integrate%20it%20with%20a%20tile-level%20systolic%20mapping%20to%20estimate%20convolution-layer%20energy.%20On%20top%20of%20this%20model%2C%20we%20introduce%20an%20energy%20accuracy%20co-optimized%20weight%20selection%20algorithm%20within%20quantization%20aware%20training%20and%20an%20energy-prioritized%20layer-wise%20schedule%20that%20compresses%20high%20energy%20layers%20more%20aggressively%20under%20a%20global%20accuracy%20constraint.%20Experiments%20on%20different%20CNN%20models%20demonstrate%20up%20to%2058.6%5C%25%20energy%20reduction%20with%202-3%5C%25%20accuracy%20drop%2C%20outperforming%20a%20state-of-the-art%20power-aware%20baseline.&entry.1838667208=http%3A//arxiv.org/abs/2511.17123v1&entry.124074799=Read"},
{"title": "FlexiFlow: decomposable flow matching for generation of flexible molecular ensemble", "author": "Riccardo Tedoldi and Ola Engkvist and Patrick Bryant and Hossein Azizpour and Jon Paul Janet and Alessandro Tibo", "abstract": "Sampling useful three-dimensional molecular structures along with their most favorable conformations is a key challenge in drug discovery. Current state-of-the-art 3D de-novo design flow matching or diffusion-based models are limited to generating a single conformation. However, the conformational landscape of a molecule determines its observable properties and how tightly it is able to bind to a given protein target. By generating a representative set of low-energy conformers, we can more directly assess these properties and potentially improve the ability to generate molecules with desired thermodynamic observables. Towards this aim, we propose FlexiFlow, a novel architecture that extends flow-matching models, allowing for the joint sampling of molecules along with multiple conformations while preserving both equivariance and permutation invariance. We demonstrate the effectiveness of our approach on the QM9 and GEOM Drugs datasets, achieving state-of-the-art results in molecular generation tasks. Our results show that FlexiFlow can generate valid, unstrained, unique, and novel molecules with high fidelity to the training data distribution, while also capturing the conformational diversity of molecules. Moreover, we show that our model can generate conformational ensembles that provide similar coverage to state-of-the-art physics-based methods at a fraction of the inference time. Finally, FlexiFlow can be successfully transferred to the protein-conditioned ligand generation task, even when the dataset contains only static pockets without accompanying conformations.", "link": "http://arxiv.org/abs/2511.17249v1", "date": "2025-11-21", "relevancy": 1.5135, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5157}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.507}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexiFlow%3A%20decomposable%20flow%20matching%20for%20generation%20of%20flexible%20molecular%20ensemble&body=Title%3A%20FlexiFlow%3A%20decomposable%20flow%20matching%20for%20generation%20of%20flexible%20molecular%20ensemble%0AAuthor%3A%20Riccardo%20Tedoldi%20and%20Ola%20Engkvist%20and%20Patrick%20Bryant%20and%20Hossein%20Azizpour%20and%20Jon%20Paul%20Janet%20and%20Alessandro%20Tibo%0AAbstract%3A%20Sampling%20useful%20three-dimensional%20molecular%20structures%20along%20with%20their%20most%20favorable%20conformations%20is%20a%20key%20challenge%20in%20drug%20discovery.%20Current%20state-of-the-art%203D%20de-novo%20design%20flow%20matching%20or%20diffusion-based%20models%20are%20limited%20to%20generating%20a%20single%20conformation.%20However%2C%20the%20conformational%20landscape%20of%20a%20molecule%20determines%20its%20observable%20properties%20and%20how%20tightly%20it%20is%20able%20to%20bind%20to%20a%20given%20protein%20target.%20By%20generating%20a%20representative%20set%20of%20low-energy%20conformers%2C%20we%20can%20more%20directly%20assess%20these%20properties%20and%20potentially%20improve%20the%20ability%20to%20generate%20molecules%20with%20desired%20thermodynamic%20observables.%20Towards%20this%20aim%2C%20we%20propose%20FlexiFlow%2C%20a%20novel%20architecture%20that%20extends%20flow-matching%20models%2C%20allowing%20for%20the%20joint%20sampling%20of%20molecules%20along%20with%20multiple%20conformations%20while%20preserving%20both%20equivariance%20and%20permutation%20invariance.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20on%20the%20QM9%20and%20GEOM%20Drugs%20datasets%2C%20achieving%20state-of-the-art%20results%20in%20molecular%20generation%20tasks.%20Our%20results%20show%20that%20FlexiFlow%20can%20generate%20valid%2C%20unstrained%2C%20unique%2C%20and%20novel%20molecules%20with%20high%20fidelity%20to%20the%20training%20data%20distribution%2C%20while%20also%20capturing%20the%20conformational%20diversity%20of%20molecules.%20Moreover%2C%20we%20show%20that%20our%20model%20can%20generate%20conformational%20ensembles%20that%20provide%20similar%20coverage%20to%20state-of-the-art%20physics-based%20methods%20at%20a%20fraction%20of%20the%20inference%20time.%20Finally%2C%20FlexiFlow%20can%20be%20successfully%20transferred%20to%20the%20protein-conditioned%20ligand%20generation%20task%2C%20even%20when%20the%20dataset%20contains%20only%20static%20pockets%20without%20accompanying%20conformations.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexiFlow%253A%2520decomposable%2520flow%2520matching%2520for%2520generation%2520of%2520flexible%2520molecular%2520ensemble%26entry.906535625%3DRiccardo%2520Tedoldi%2520and%2520Ola%2520Engkvist%2520and%2520Patrick%2520Bryant%2520and%2520Hossein%2520Azizpour%2520and%2520Jon%2520Paul%2520Janet%2520and%2520Alessandro%2520Tibo%26entry.1292438233%3DSampling%2520useful%2520three-dimensional%2520molecular%2520structures%2520along%2520with%2520their%2520most%2520favorable%2520conformations%2520is%2520a%2520key%2520challenge%2520in%2520drug%2520discovery.%2520Current%2520state-of-the-art%25203D%2520de-novo%2520design%2520flow%2520matching%2520or%2520diffusion-based%2520models%2520are%2520limited%2520to%2520generating%2520a%2520single%2520conformation.%2520However%252C%2520the%2520conformational%2520landscape%2520of%2520a%2520molecule%2520determines%2520its%2520observable%2520properties%2520and%2520how%2520tightly%2520it%2520is%2520able%2520to%2520bind%2520to%2520a%2520given%2520protein%2520target.%2520By%2520generating%2520a%2520representative%2520set%2520of%2520low-energy%2520conformers%252C%2520we%2520can%2520more%2520directly%2520assess%2520these%2520properties%2520and%2520potentially%2520improve%2520the%2520ability%2520to%2520generate%2520molecules%2520with%2520desired%2520thermodynamic%2520observables.%2520Towards%2520this%2520aim%252C%2520we%2520propose%2520FlexiFlow%252C%2520a%2520novel%2520architecture%2520that%2520extends%2520flow-matching%2520models%252C%2520allowing%2520for%2520the%2520joint%2520sampling%2520of%2520molecules%2520along%2520with%2520multiple%2520conformations%2520while%2520preserving%2520both%2520equivariance%2520and%2520permutation%2520invariance.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520on%2520the%2520QM9%2520and%2520GEOM%2520Drugs%2520datasets%252C%2520achieving%2520state-of-the-art%2520results%2520in%2520molecular%2520generation%2520tasks.%2520Our%2520results%2520show%2520that%2520FlexiFlow%2520can%2520generate%2520valid%252C%2520unstrained%252C%2520unique%252C%2520and%2520novel%2520molecules%2520with%2520high%2520fidelity%2520to%2520the%2520training%2520data%2520distribution%252C%2520while%2520also%2520capturing%2520the%2520conformational%2520diversity%2520of%2520molecules.%2520Moreover%252C%2520we%2520show%2520that%2520our%2520model%2520can%2520generate%2520conformational%2520ensembles%2520that%2520provide%2520similar%2520coverage%2520to%2520state-of-the-art%2520physics-based%2520methods%2520at%2520a%2520fraction%2520of%2520the%2520inference%2520time.%2520Finally%252C%2520FlexiFlow%2520can%2520be%2520successfully%2520transferred%2520to%2520the%2520protein-conditioned%2520ligand%2520generation%2520task%252C%2520even%2520when%2520the%2520dataset%2520contains%2520only%2520static%2520pockets%2520without%2520accompanying%2520conformations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexiFlow%3A%20decomposable%20flow%20matching%20for%20generation%20of%20flexible%20molecular%20ensemble&entry.906535625=Riccardo%20Tedoldi%20and%20Ola%20Engkvist%20and%20Patrick%20Bryant%20and%20Hossein%20Azizpour%20and%20Jon%20Paul%20Janet%20and%20Alessandro%20Tibo&entry.1292438233=Sampling%20useful%20three-dimensional%20molecular%20structures%20along%20with%20their%20most%20favorable%20conformations%20is%20a%20key%20challenge%20in%20drug%20discovery.%20Current%20state-of-the-art%203D%20de-novo%20design%20flow%20matching%20or%20diffusion-based%20models%20are%20limited%20to%20generating%20a%20single%20conformation.%20However%2C%20the%20conformational%20landscape%20of%20a%20molecule%20determines%20its%20observable%20properties%20and%20how%20tightly%20it%20is%20able%20to%20bind%20to%20a%20given%20protein%20target.%20By%20generating%20a%20representative%20set%20of%20low-energy%20conformers%2C%20we%20can%20more%20directly%20assess%20these%20properties%20and%20potentially%20improve%20the%20ability%20to%20generate%20molecules%20with%20desired%20thermodynamic%20observables.%20Towards%20this%20aim%2C%20we%20propose%20FlexiFlow%2C%20a%20novel%20architecture%20that%20extends%20flow-matching%20models%2C%20allowing%20for%20the%20joint%20sampling%20of%20molecules%20along%20with%20multiple%20conformations%20while%20preserving%20both%20equivariance%20and%20permutation%20invariance.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20on%20the%20QM9%20and%20GEOM%20Drugs%20datasets%2C%20achieving%20state-of-the-art%20results%20in%20molecular%20generation%20tasks.%20Our%20results%20show%20that%20FlexiFlow%20can%20generate%20valid%2C%20unstrained%2C%20unique%2C%20and%20novel%20molecules%20with%20high%20fidelity%20to%20the%20training%20data%20distribution%2C%20while%20also%20capturing%20the%20conformational%20diversity%20of%20molecules.%20Moreover%2C%20we%20show%20that%20our%20model%20can%20generate%20conformational%20ensembles%20that%20provide%20similar%20coverage%20to%20state-of-the-art%20physics-based%20methods%20at%20a%20fraction%20of%20the%20inference%20time.%20Finally%2C%20FlexiFlow%20can%20be%20successfully%20transferred%20to%20the%20protein-conditioned%20ligand%20generation%20task%2C%20even%20when%20the%20dataset%20contains%20only%20static%20pockets%20without%20accompanying%20conformations.&entry.1838667208=http%3A//arxiv.org/abs/2511.17249v1&entry.124074799=Read"},
{"title": "\"Normalized Stress\" is Not Normalized: How to Interpret Stress Correctly", "author": "Kiran Smelser and Jacob Miller and Stephen Kobourov", "abstract": "Stress is among the most commonly employed quality metrics and optimization criteria for dimension reduction projections of high dimensional data. Complex, high dimensional data is ubiquitous across many scientific disciplines, including machine learning, biology, and the social sciences. One of the primary methods of visualizing these datasets is with two dimensional scatter plots that visually capture some properties of the data. Because visually determining the accuracy of these plots is challenging, researchers often use quality metrics to measure projection accuracy or faithfulness to the full data. One of the most commonly employed metrics, normalized stress, is sensitive to uniform scaling of the projection, despite this act not meaningfully changing anything about the projection. We investigate the effect of scaling on stress and other distance based quality metrics analytically and empirically by showing just how much the values change and how this affects dimension reduction technique evaluations. We introduce a simple technique to make normalized stress scale invariant and show that it accurately captures expected behavior on a small benchmark.", "link": "http://arxiv.org/abs/2408.07724v2", "date": "2025-11-21", "relevancy": 1.5272, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4059}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3822}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.3717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22Normalized%20Stress%22%20is%20Not%20Normalized%3A%20How%20to%20Interpret%20Stress%20Correctly&body=Title%3A%20%22Normalized%20Stress%22%20is%20Not%20Normalized%3A%20How%20to%20Interpret%20Stress%20Correctly%0AAuthor%3A%20Kiran%20Smelser%20and%20Jacob%20Miller%20and%20Stephen%20Kobourov%0AAbstract%3A%20Stress%20is%20among%20the%20most%20commonly%20employed%20quality%20metrics%20and%20optimization%20criteria%20for%20dimension%20reduction%20projections%20of%20high%20dimensional%20data.%20Complex%2C%20high%20dimensional%20data%20is%20ubiquitous%20across%20many%20scientific%20disciplines%2C%20including%20machine%20learning%2C%20biology%2C%20and%20the%20social%20sciences.%20One%20of%20the%20primary%20methods%20of%20visualizing%20these%20datasets%20is%20with%20two%20dimensional%20scatter%20plots%20that%20visually%20capture%20some%20properties%20of%20the%20data.%20Because%20visually%20determining%20the%20accuracy%20of%20these%20plots%20is%20challenging%2C%20researchers%20often%20use%20quality%20metrics%20to%20measure%20projection%20accuracy%20or%20faithfulness%20to%20the%20full%20data.%20One%20of%20the%20most%20commonly%20employed%20metrics%2C%20normalized%20stress%2C%20is%20sensitive%20to%20uniform%20scaling%20of%20the%20projection%2C%20despite%20this%20act%20not%20meaningfully%20changing%20anything%20about%20the%20projection.%20We%20investigate%20the%20effect%20of%20scaling%20on%20stress%20and%20other%20distance%20based%20quality%20metrics%20analytically%20and%20empirically%20by%20showing%20just%20how%20much%20the%20values%20change%20and%20how%20this%20affects%20dimension%20reduction%20technique%20evaluations.%20We%20introduce%20a%20simple%20technique%20to%20make%20normalized%20stress%20scale%20invariant%20and%20show%20that%20it%20accurately%20captures%20expected%20behavior%20on%20a%20small%20benchmark.%0ALink%3A%20http%3A//arxiv.org/abs/2408.07724v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522Normalized%2520Stress%2522%2520is%2520Not%2520Normalized%253A%2520How%2520to%2520Interpret%2520Stress%2520Correctly%26entry.906535625%3DKiran%2520Smelser%2520and%2520Jacob%2520Miller%2520and%2520Stephen%2520Kobourov%26entry.1292438233%3DStress%2520is%2520among%2520the%2520most%2520commonly%2520employed%2520quality%2520metrics%2520and%2520optimization%2520criteria%2520for%2520dimension%2520reduction%2520projections%2520of%2520high%2520dimensional%2520data.%2520Complex%252C%2520high%2520dimensional%2520data%2520is%2520ubiquitous%2520across%2520many%2520scientific%2520disciplines%252C%2520including%2520machine%2520learning%252C%2520biology%252C%2520and%2520the%2520social%2520sciences.%2520One%2520of%2520the%2520primary%2520methods%2520of%2520visualizing%2520these%2520datasets%2520is%2520with%2520two%2520dimensional%2520scatter%2520plots%2520that%2520visually%2520capture%2520some%2520properties%2520of%2520the%2520data.%2520Because%2520visually%2520determining%2520the%2520accuracy%2520of%2520these%2520plots%2520is%2520challenging%252C%2520researchers%2520often%2520use%2520quality%2520metrics%2520to%2520measure%2520projection%2520accuracy%2520or%2520faithfulness%2520to%2520the%2520full%2520data.%2520One%2520of%2520the%2520most%2520commonly%2520employed%2520metrics%252C%2520normalized%2520stress%252C%2520is%2520sensitive%2520to%2520uniform%2520scaling%2520of%2520the%2520projection%252C%2520despite%2520this%2520act%2520not%2520meaningfully%2520changing%2520anything%2520about%2520the%2520projection.%2520We%2520investigate%2520the%2520effect%2520of%2520scaling%2520on%2520stress%2520and%2520other%2520distance%2520based%2520quality%2520metrics%2520analytically%2520and%2520empirically%2520by%2520showing%2520just%2520how%2520much%2520the%2520values%2520change%2520and%2520how%2520this%2520affects%2520dimension%2520reduction%2520technique%2520evaluations.%2520We%2520introduce%2520a%2520simple%2520technique%2520to%2520make%2520normalized%2520stress%2520scale%2520invariant%2520and%2520show%2520that%2520it%2520accurately%2520captures%2520expected%2520behavior%2520on%2520a%2520small%2520benchmark.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07724v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22Normalized%20Stress%22%20is%20Not%20Normalized%3A%20How%20to%20Interpret%20Stress%20Correctly&entry.906535625=Kiran%20Smelser%20and%20Jacob%20Miller%20and%20Stephen%20Kobourov&entry.1292438233=Stress%20is%20among%20the%20most%20commonly%20employed%20quality%20metrics%20and%20optimization%20criteria%20for%20dimension%20reduction%20projections%20of%20high%20dimensional%20data.%20Complex%2C%20high%20dimensional%20data%20is%20ubiquitous%20across%20many%20scientific%20disciplines%2C%20including%20machine%20learning%2C%20biology%2C%20and%20the%20social%20sciences.%20One%20of%20the%20primary%20methods%20of%20visualizing%20these%20datasets%20is%20with%20two%20dimensional%20scatter%20plots%20that%20visually%20capture%20some%20properties%20of%20the%20data.%20Because%20visually%20determining%20the%20accuracy%20of%20these%20plots%20is%20challenging%2C%20researchers%20often%20use%20quality%20metrics%20to%20measure%20projection%20accuracy%20or%20faithfulness%20to%20the%20full%20data.%20One%20of%20the%20most%20commonly%20employed%20metrics%2C%20normalized%20stress%2C%20is%20sensitive%20to%20uniform%20scaling%20of%20the%20projection%2C%20despite%20this%20act%20not%20meaningfully%20changing%20anything%20about%20the%20projection.%20We%20investigate%20the%20effect%20of%20scaling%20on%20stress%20and%20other%20distance%20based%20quality%20metrics%20analytically%20and%20empirically%20by%20showing%20just%20how%20much%20the%20values%20change%20and%20how%20this%20affects%20dimension%20reduction%20technique%20evaluations.%20We%20introduce%20a%20simple%20technique%20to%20make%20normalized%20stress%20scale%20invariant%20and%20show%20that%20it%20accurately%20captures%20expected%20behavior%20on%20a%20small%20benchmark.&entry.1838667208=http%3A//arxiv.org/abs/2408.07724v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


