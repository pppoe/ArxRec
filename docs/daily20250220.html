<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250219.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "3D Gaussian Splatting aided Localization for Large and Complex\n  Indoor-Environments", "author": "Vincent Ress and Jonas Meyer and Wei Zhang and David Skuddis and Uwe Soergel and Norbert Haala", "abstract": "  The field of visual localization has been researched for several decades and\nhas meanwhile found many practical applications. Despite the strong progress in\nthis field, there are still challenging situations in which established methods\nfail. We present an approach to significantly improve the accuracy and\nreliability of established visual localization methods by adding rendered\nimages. In detail, we first use a modern visual SLAM approach that provides a\n3D Gaussian Splatting (3DGS) based map to create reference data. We demonstrate\nthat enriching reference data with images rendered from 3DGS at randomly\nsampled poses significantly improves the performance of both geometry-based\nvisual localization and Scene Coordinate Regression (SCR) methods. Through\ncomprehensive evaluation in a large industrial environment, we analyze the\nperformance impact of incorporating these additional rendered views.\n", "link": "http://arxiv.org/abs/2502.13803v1", "date": "2025-02-19", "relevancy": 3.3958, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7479}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6574}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Gaussian%20Splatting%20aided%20Localization%20for%20Large%20and%20Complex%0A%20%20Indoor-Environments&body=Title%3A%203D%20Gaussian%20Splatting%20aided%20Localization%20for%20Large%20and%20Complex%0A%20%20Indoor-Environments%0AAuthor%3A%20Vincent%20Ress%20and%20Jonas%20Meyer%20and%20Wei%20Zhang%20and%20David%20Skuddis%20and%20Uwe%20Soergel%20and%20Norbert%20Haala%0AAbstract%3A%20%20%20The%20field%20of%20visual%20localization%20has%20been%20researched%20for%20several%20decades%20and%0Ahas%20meanwhile%20found%20many%20practical%20applications.%20Despite%20the%20strong%20progress%20in%0Athis%20field%2C%20there%20are%20still%20challenging%20situations%20in%20which%20established%20methods%0Afail.%20We%20present%20an%20approach%20to%20significantly%20improve%20the%20accuracy%20and%0Areliability%20of%20established%20visual%20localization%20methods%20by%20adding%20rendered%0Aimages.%20In%20detail%2C%20we%20first%20use%20a%20modern%20visual%20SLAM%20approach%20that%20provides%20a%0A3D%20Gaussian%20Splatting%20%283DGS%29%20based%20map%20to%20create%20reference%20data.%20We%20demonstrate%0Athat%20enriching%20reference%20data%20with%20images%20rendered%20from%203DGS%20at%20randomly%0Asampled%20poses%20significantly%20improves%20the%20performance%20of%20both%20geometry-based%0Avisual%20localization%20and%20Scene%20Coordinate%20Regression%20%28SCR%29%20methods.%20Through%0Acomprehensive%20evaluation%20in%20a%20large%20industrial%20environment%2C%20we%20analyze%20the%0Aperformance%20impact%20of%20incorporating%20these%20additional%20rendered%20views.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Gaussian%2520Splatting%2520aided%2520Localization%2520for%2520Large%2520and%2520Complex%250A%2520%2520Indoor-Environments%26entry.906535625%3DVincent%2520Ress%2520and%2520Jonas%2520Meyer%2520and%2520Wei%2520Zhang%2520and%2520David%2520Skuddis%2520and%2520Uwe%2520Soergel%2520and%2520Norbert%2520Haala%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520visual%2520localization%2520has%2520been%2520researched%2520for%2520several%2520decades%2520and%250Ahas%2520meanwhile%2520found%2520many%2520practical%2520applications.%2520Despite%2520the%2520strong%2520progress%2520in%250Athis%2520field%252C%2520there%2520are%2520still%2520challenging%2520situations%2520in%2520which%2520established%2520methods%250Afail.%2520We%2520present%2520an%2520approach%2520to%2520significantly%2520improve%2520the%2520accuracy%2520and%250Areliability%2520of%2520established%2520visual%2520localization%2520methods%2520by%2520adding%2520rendered%250Aimages.%2520In%2520detail%252C%2520we%2520first%2520use%2520a%2520modern%2520visual%2520SLAM%2520approach%2520that%2520provides%2520a%250A3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520based%2520map%2520to%2520create%2520reference%2520data.%2520We%2520demonstrate%250Athat%2520enriching%2520reference%2520data%2520with%2520images%2520rendered%2520from%25203DGS%2520at%2520randomly%250Asampled%2520poses%2520significantly%2520improves%2520the%2520performance%2520of%2520both%2520geometry-based%250Avisual%2520localization%2520and%2520Scene%2520Coordinate%2520Regression%2520%2528SCR%2529%2520methods.%2520Through%250Acomprehensive%2520evaluation%2520in%2520a%2520large%2520industrial%2520environment%252C%2520we%2520analyze%2520the%250Aperformance%2520impact%2520of%2520incorporating%2520these%2520additional%2520rendered%2520views.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Gaussian%20Splatting%20aided%20Localization%20for%20Large%20and%20Complex%0A%20%20Indoor-Environments&entry.906535625=Vincent%20Ress%20and%20Jonas%20Meyer%20and%20Wei%20Zhang%20and%20David%20Skuddis%20and%20Uwe%20Soergel%20and%20Norbert%20Haala&entry.1292438233=%20%20The%20field%20of%20visual%20localization%20has%20been%20researched%20for%20several%20decades%20and%0Ahas%20meanwhile%20found%20many%20practical%20applications.%20Despite%20the%20strong%20progress%20in%0Athis%20field%2C%20there%20are%20still%20challenging%20situations%20in%20which%20established%20methods%0Afail.%20We%20present%20an%20approach%20to%20significantly%20improve%20the%20accuracy%20and%0Areliability%20of%20established%20visual%20localization%20methods%20by%20adding%20rendered%0Aimages.%20In%20detail%2C%20we%20first%20use%20a%20modern%20visual%20SLAM%20approach%20that%20provides%20a%0A3D%20Gaussian%20Splatting%20%283DGS%29%20based%20map%20to%20create%20reference%20data.%20We%20demonstrate%0Athat%20enriching%20reference%20data%20with%20images%20rendered%20from%203DGS%20at%20randomly%0Asampled%20poses%20significantly%20improves%20the%20performance%20of%20both%20geometry-based%0Avisual%20localization%20and%20Scene%20Coordinate%20Regression%20%28SCR%29%20methods.%20Through%0Acomprehensive%20evaluation%20in%20a%20large%20industrial%20environment%2C%20we%20analyze%20the%0Aperformance%20impact%20of%20incorporating%20these%20additional%20rendered%20views.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13803v1&entry.124074799=Read"},
{"title": "Generalized Robot 3D Vision-Language Model with Fast Rendering and\n  Pre-Training Vision-Language Alignment", "author": "Kangcheng Liu and Yong-Jin Liu and Baoquan Chen", "abstract": "  Deep neural network models have achieved remarkable progress in 3D scene\nunderstanding while trained in the closed-set setting and with full labels.\nHowever, the major bottleneck is that these models do not have the capacity to\nrecognize any unseen novel classes beyond the training categories in diverse\nreal-world applications. Therefore, we are in urgent need of a framework that\ncan simultaneously be applicable to both 3D point cloud segmentation and\ndetection, particularly in the circumstances where the labels are rather\nscarce. This work presents a generalized and straightforward framework for\ndealing with 3D scene understanding when the labeled scenes are quite limited.\nTo extract knowledge for novel categories from the pre-trained vision-language\nmodels, we propose a hierarchical feature-aligned pre-training and knowledge\ndistillation strategy to extract and distill meaningful information from\nlarge-scale vision-language models, which helps benefit the open-vocabulary\nscene understanding tasks. To encourage latent instance discrimination and to\nguarantee efficiency, we propose the unsupervised region-level semantic\ncontrastive learning scheme for point clouds, using confident predictions of\nthe neural network to discriminate the intermediate feature embeddings at\nmultiple stages. In the limited reconstruction case, our proposed approach,\ntermed WS3D++, ranks 1st on the large-scale ScanNet benchmark on both the task\nof semantic segmentation and instance segmentation. Extensive experiments with\nboth indoor and outdoor scenes demonstrated the effectiveness of our approach\nin both data-efficient learning and open-world few-shot learning. The code is\nmade publicly available at:\nhttps://drive.google.com/drive/folders/1M58V-PtR8DBEwD296zJkNg_m2qq-MTAP?usp=sharing.\n", "link": "http://arxiv.org/abs/2312.00663v2", "date": "2025-02-19", "relevancy": 3.3365, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6872}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6872}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Robot%203D%20Vision-Language%20Model%20with%20Fast%20Rendering%20and%0A%20%20Pre-Training%20Vision-Language%20Alignment&body=Title%3A%20Generalized%20Robot%203D%20Vision-Language%20Model%20with%20Fast%20Rendering%20and%0A%20%20Pre-Training%20Vision-Language%20Alignment%0AAuthor%3A%20Kangcheng%20Liu%20and%20Yong-Jin%20Liu%20and%20Baoquan%20Chen%0AAbstract%3A%20%20%20Deep%20neural%20network%20models%20have%20achieved%20remarkable%20progress%20in%203D%20scene%0Aunderstanding%20while%20trained%20in%20the%20closed-set%20setting%20and%20with%20full%20labels.%0AHowever%2C%20the%20major%20bottleneck%20is%20that%20these%20models%20do%20not%20have%20the%20capacity%20to%0Arecognize%20any%20unseen%20novel%20classes%20beyond%20the%20training%20categories%20in%20diverse%0Areal-world%20applications.%20Therefore%2C%20we%20are%20in%20urgent%20need%20of%20a%20framework%20that%0Acan%20simultaneously%20be%20applicable%20to%20both%203D%20point%20cloud%20segmentation%20and%0Adetection%2C%20particularly%20in%20the%20circumstances%20where%20the%20labels%20are%20rather%0Ascarce.%20This%20work%20presents%20a%20generalized%20and%20straightforward%20framework%20for%0Adealing%20with%203D%20scene%20understanding%20when%20the%20labeled%20scenes%20are%20quite%20limited.%0ATo%20extract%20knowledge%20for%20novel%20categories%20from%20the%20pre-trained%20vision-language%0Amodels%2C%20we%20propose%20a%20hierarchical%20feature-aligned%20pre-training%20and%20knowledge%0Adistillation%20strategy%20to%20extract%20and%20distill%20meaningful%20information%20from%0Alarge-scale%20vision-language%20models%2C%20which%20helps%20benefit%20the%20open-vocabulary%0Ascene%20understanding%20tasks.%20To%20encourage%20latent%20instance%20discrimination%20and%20to%0Aguarantee%20efficiency%2C%20we%20propose%20the%20unsupervised%20region-level%20semantic%0Acontrastive%20learning%20scheme%20for%20point%20clouds%2C%20using%20confident%20predictions%20of%0Athe%20neural%20network%20to%20discriminate%20the%20intermediate%20feature%20embeddings%20at%0Amultiple%20stages.%20In%20the%20limited%20reconstruction%20case%2C%20our%20proposed%20approach%2C%0Atermed%20WS3D%2B%2B%2C%20ranks%201st%20on%20the%20large-scale%20ScanNet%20benchmark%20on%20both%20the%20task%0Aof%20semantic%20segmentation%20and%20instance%20segmentation.%20Extensive%20experiments%20with%0Aboth%20indoor%20and%20outdoor%20scenes%20demonstrated%20the%20effectiveness%20of%20our%20approach%0Ain%20both%20data-efficient%20learning%20and%20open-world%20few-shot%20learning.%20The%20code%20is%0Amade%20publicly%20available%20at%3A%0Ahttps%3A//drive.google.com/drive/folders/1M58V-PtR8DBEwD296zJkNg_m2qq-MTAP%3Fusp%3Dsharing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00663v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Robot%25203D%2520Vision-Language%2520Model%2520with%2520Fast%2520Rendering%2520and%250A%2520%2520Pre-Training%2520Vision-Language%2520Alignment%26entry.906535625%3DKangcheng%2520Liu%2520and%2520Yong-Jin%2520Liu%2520and%2520Baoquan%2520Chen%26entry.1292438233%3D%2520%2520Deep%2520neural%2520network%2520models%2520have%2520achieved%2520remarkable%2520progress%2520in%25203D%2520scene%250Aunderstanding%2520while%2520trained%2520in%2520the%2520closed-set%2520setting%2520and%2520with%2520full%2520labels.%250AHowever%252C%2520the%2520major%2520bottleneck%2520is%2520that%2520these%2520models%2520do%2520not%2520have%2520the%2520capacity%2520to%250Arecognize%2520any%2520unseen%2520novel%2520classes%2520beyond%2520the%2520training%2520categories%2520in%2520diverse%250Areal-world%2520applications.%2520Therefore%252C%2520we%2520are%2520in%2520urgent%2520need%2520of%2520a%2520framework%2520that%250Acan%2520simultaneously%2520be%2520applicable%2520to%2520both%25203D%2520point%2520cloud%2520segmentation%2520and%250Adetection%252C%2520particularly%2520in%2520the%2520circumstances%2520where%2520the%2520labels%2520are%2520rather%250Ascarce.%2520This%2520work%2520presents%2520a%2520generalized%2520and%2520straightforward%2520framework%2520for%250Adealing%2520with%25203D%2520scene%2520understanding%2520when%2520the%2520labeled%2520scenes%2520are%2520quite%2520limited.%250ATo%2520extract%2520knowledge%2520for%2520novel%2520categories%2520from%2520the%2520pre-trained%2520vision-language%250Amodels%252C%2520we%2520propose%2520a%2520hierarchical%2520feature-aligned%2520pre-training%2520and%2520knowledge%250Adistillation%2520strategy%2520to%2520extract%2520and%2520distill%2520meaningful%2520information%2520from%250Alarge-scale%2520vision-language%2520models%252C%2520which%2520helps%2520benefit%2520the%2520open-vocabulary%250Ascene%2520understanding%2520tasks.%2520To%2520encourage%2520latent%2520instance%2520discrimination%2520and%2520to%250Aguarantee%2520efficiency%252C%2520we%2520propose%2520the%2520unsupervised%2520region-level%2520semantic%250Acontrastive%2520learning%2520scheme%2520for%2520point%2520clouds%252C%2520using%2520confident%2520predictions%2520of%250Athe%2520neural%2520network%2520to%2520discriminate%2520the%2520intermediate%2520feature%2520embeddings%2520at%250Amultiple%2520stages.%2520In%2520the%2520limited%2520reconstruction%2520case%252C%2520our%2520proposed%2520approach%252C%250Atermed%2520WS3D%252B%252B%252C%2520ranks%25201st%2520on%2520the%2520large-scale%2520ScanNet%2520benchmark%2520on%2520both%2520the%2520task%250Aof%2520semantic%2520segmentation%2520and%2520instance%2520segmentation.%2520Extensive%2520experiments%2520with%250Aboth%2520indoor%2520and%2520outdoor%2520scenes%2520demonstrated%2520the%2520effectiveness%2520of%2520our%2520approach%250Ain%2520both%2520data-efficient%2520learning%2520and%2520open-world%2520few-shot%2520learning.%2520The%2520code%2520is%250Amade%2520publicly%2520available%2520at%253A%250Ahttps%253A//drive.google.com/drive/folders/1M58V-PtR8DBEwD296zJkNg_m2qq-MTAP%253Fusp%253Dsharing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00663v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Robot%203D%20Vision-Language%20Model%20with%20Fast%20Rendering%20and%0A%20%20Pre-Training%20Vision-Language%20Alignment&entry.906535625=Kangcheng%20Liu%20and%20Yong-Jin%20Liu%20and%20Baoquan%20Chen&entry.1292438233=%20%20Deep%20neural%20network%20models%20have%20achieved%20remarkable%20progress%20in%203D%20scene%0Aunderstanding%20while%20trained%20in%20the%20closed-set%20setting%20and%20with%20full%20labels.%0AHowever%2C%20the%20major%20bottleneck%20is%20that%20these%20models%20do%20not%20have%20the%20capacity%20to%0Arecognize%20any%20unseen%20novel%20classes%20beyond%20the%20training%20categories%20in%20diverse%0Areal-world%20applications.%20Therefore%2C%20we%20are%20in%20urgent%20need%20of%20a%20framework%20that%0Acan%20simultaneously%20be%20applicable%20to%20both%203D%20point%20cloud%20segmentation%20and%0Adetection%2C%20particularly%20in%20the%20circumstances%20where%20the%20labels%20are%20rather%0Ascarce.%20This%20work%20presents%20a%20generalized%20and%20straightforward%20framework%20for%0Adealing%20with%203D%20scene%20understanding%20when%20the%20labeled%20scenes%20are%20quite%20limited.%0ATo%20extract%20knowledge%20for%20novel%20categories%20from%20the%20pre-trained%20vision-language%0Amodels%2C%20we%20propose%20a%20hierarchical%20feature-aligned%20pre-training%20and%20knowledge%0Adistillation%20strategy%20to%20extract%20and%20distill%20meaningful%20information%20from%0Alarge-scale%20vision-language%20models%2C%20which%20helps%20benefit%20the%20open-vocabulary%0Ascene%20understanding%20tasks.%20To%20encourage%20latent%20instance%20discrimination%20and%20to%0Aguarantee%20efficiency%2C%20we%20propose%20the%20unsupervised%20region-level%20semantic%0Acontrastive%20learning%20scheme%20for%20point%20clouds%2C%20using%20confident%20predictions%20of%0Athe%20neural%20network%20to%20discriminate%20the%20intermediate%20feature%20embeddings%20at%0Amultiple%20stages.%20In%20the%20limited%20reconstruction%20case%2C%20our%20proposed%20approach%2C%0Atermed%20WS3D%2B%2B%2C%20ranks%201st%20on%20the%20large-scale%20ScanNet%20benchmark%20on%20both%20the%20task%0Aof%20semantic%20segmentation%20and%20instance%20segmentation.%20Extensive%20experiments%20with%0Aboth%20indoor%20and%20outdoor%20scenes%20demonstrated%20the%20effectiveness%20of%20our%20approach%0Ain%20both%20data-efficient%20learning%20and%20open-world%20few-shot%20learning.%20The%20code%20is%0Amade%20publicly%20available%20at%3A%0Ahttps%3A//drive.google.com/drive/folders/1M58V-PtR8DBEwD296zJkNg_m2qq-MTAP%3Fusp%3Dsharing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00663v2&entry.124074799=Read"},
{"title": "Betsu-Betsu: Multi-View Separable 3D Reconstruction of Two Interacting\n  Objects", "author": "Suhas Gopal and Rishabh Dabral and Vladislav Golyanik and Christian Theobalt", "abstract": "  Separable 3D reconstruction of multiple objects from multi-view RGB images --\nresulting in two different 3D shapes for the two objects with a clear\nseparation between them -- remains a sparsely researched problem. It is\nchallenging due to severe mutual occlusions and ambiguities along the objects'\ninteraction boundaries. This paper investigates the setting and introduces a\nnew neuro-implicit method that can reconstruct the geometry and appearance of\ntwo objects undergoing close interactions while disjoining both in 3D, avoiding\nsurface inter-penetrations and enabling novel-view synthesis of the observed\nscene. The framework is end-to-end trainable and supervised using a novel\nalpha-blending regularisation that ensures that the two geometries are well\nseparated even under extreme occlusions. Our reconstruction method is\nmarkerless and can be applied to rigid as well as articulated objects. We\nintroduce a new dataset consisting of close interactions between a human and an\nobject and also evaluate on two scenes of humans performing martial arts. The\nexperiments confirm the effectiveness of our framework and substantial\nimprovements using 3D and novel view synthesis metrics compared to several\nexisting approaches applicable in our setting.\n", "link": "http://arxiv.org/abs/2502.13968v1", "date": "2025-02-19", "relevancy": 3.1319, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6311}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6311}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Betsu-Betsu%3A%20Multi-View%20Separable%203D%20Reconstruction%20of%20Two%20Interacting%0A%20%20Objects&body=Title%3A%20Betsu-Betsu%3A%20Multi-View%20Separable%203D%20Reconstruction%20of%20Two%20Interacting%0A%20%20Objects%0AAuthor%3A%20Suhas%20Gopal%20and%20Rishabh%20Dabral%20and%20Vladislav%20Golyanik%20and%20Christian%20Theobalt%0AAbstract%3A%20%20%20Separable%203D%20reconstruction%20of%20multiple%20objects%20from%20multi-view%20RGB%20images%20--%0Aresulting%20in%20two%20different%203D%20shapes%20for%20the%20two%20objects%20with%20a%20clear%0Aseparation%20between%20them%20--%20remains%20a%20sparsely%20researched%20problem.%20It%20is%0Achallenging%20due%20to%20severe%20mutual%20occlusions%20and%20ambiguities%20along%20the%20objects%27%0Ainteraction%20boundaries.%20This%20paper%20investigates%20the%20setting%20and%20introduces%20a%0Anew%20neuro-implicit%20method%20that%20can%20reconstruct%20the%20geometry%20and%20appearance%20of%0Atwo%20objects%20undergoing%20close%20interactions%20while%20disjoining%20both%20in%203D%2C%20avoiding%0Asurface%20inter-penetrations%20and%20enabling%20novel-view%20synthesis%20of%20the%20observed%0Ascene.%20The%20framework%20is%20end-to-end%20trainable%20and%20supervised%20using%20a%20novel%0Aalpha-blending%20regularisation%20that%20ensures%20that%20the%20two%20geometries%20are%20well%0Aseparated%20even%20under%20extreme%20occlusions.%20Our%20reconstruction%20method%20is%0Amarkerless%20and%20can%20be%20applied%20to%20rigid%20as%20well%20as%20articulated%20objects.%20We%0Aintroduce%20a%20new%20dataset%20consisting%20of%20close%20interactions%20between%20a%20human%20and%20an%0Aobject%20and%20also%20evaluate%20on%20two%20scenes%20of%20humans%20performing%20martial%20arts.%20The%0Aexperiments%20confirm%20the%20effectiveness%20of%20our%20framework%20and%20substantial%0Aimprovements%20using%203D%20and%20novel%20view%20synthesis%20metrics%20compared%20to%20several%0Aexisting%20approaches%20applicable%20in%20our%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13968v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBetsu-Betsu%253A%2520Multi-View%2520Separable%25203D%2520Reconstruction%2520of%2520Two%2520Interacting%250A%2520%2520Objects%26entry.906535625%3DSuhas%2520Gopal%2520and%2520Rishabh%2520Dabral%2520and%2520Vladislav%2520Golyanik%2520and%2520Christian%2520Theobalt%26entry.1292438233%3D%2520%2520Separable%25203D%2520reconstruction%2520of%2520multiple%2520objects%2520from%2520multi-view%2520RGB%2520images%2520--%250Aresulting%2520in%2520two%2520different%25203D%2520shapes%2520for%2520the%2520two%2520objects%2520with%2520a%2520clear%250Aseparation%2520between%2520them%2520--%2520remains%2520a%2520sparsely%2520researched%2520problem.%2520It%2520is%250Achallenging%2520due%2520to%2520severe%2520mutual%2520occlusions%2520and%2520ambiguities%2520along%2520the%2520objects%2527%250Ainteraction%2520boundaries.%2520This%2520paper%2520investigates%2520the%2520setting%2520and%2520introduces%2520a%250Anew%2520neuro-implicit%2520method%2520that%2520can%2520reconstruct%2520the%2520geometry%2520and%2520appearance%2520of%250Atwo%2520objects%2520undergoing%2520close%2520interactions%2520while%2520disjoining%2520both%2520in%25203D%252C%2520avoiding%250Asurface%2520inter-penetrations%2520and%2520enabling%2520novel-view%2520synthesis%2520of%2520the%2520observed%250Ascene.%2520The%2520framework%2520is%2520end-to-end%2520trainable%2520and%2520supervised%2520using%2520a%2520novel%250Aalpha-blending%2520regularisation%2520that%2520ensures%2520that%2520the%2520two%2520geometries%2520are%2520well%250Aseparated%2520even%2520under%2520extreme%2520occlusions.%2520Our%2520reconstruction%2520method%2520is%250Amarkerless%2520and%2520can%2520be%2520applied%2520to%2520rigid%2520as%2520well%2520as%2520articulated%2520objects.%2520We%250Aintroduce%2520a%2520new%2520dataset%2520consisting%2520of%2520close%2520interactions%2520between%2520a%2520human%2520and%2520an%250Aobject%2520and%2520also%2520evaluate%2520on%2520two%2520scenes%2520of%2520humans%2520performing%2520martial%2520arts.%2520The%250Aexperiments%2520confirm%2520the%2520effectiveness%2520of%2520our%2520framework%2520and%2520substantial%250Aimprovements%2520using%25203D%2520and%2520novel%2520view%2520synthesis%2520metrics%2520compared%2520to%2520several%250Aexisting%2520approaches%2520applicable%2520in%2520our%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13968v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Betsu-Betsu%3A%20Multi-View%20Separable%203D%20Reconstruction%20of%20Two%20Interacting%0A%20%20Objects&entry.906535625=Suhas%20Gopal%20and%20Rishabh%20Dabral%20and%20Vladislav%20Golyanik%20and%20Christian%20Theobalt&entry.1292438233=%20%20Separable%203D%20reconstruction%20of%20multiple%20objects%20from%20multi-view%20RGB%20images%20--%0Aresulting%20in%20two%20different%203D%20shapes%20for%20the%20two%20objects%20with%20a%20clear%0Aseparation%20between%20them%20--%20remains%20a%20sparsely%20researched%20problem.%20It%20is%0Achallenging%20due%20to%20severe%20mutual%20occlusions%20and%20ambiguities%20along%20the%20objects%27%0Ainteraction%20boundaries.%20This%20paper%20investigates%20the%20setting%20and%20introduces%20a%0Anew%20neuro-implicit%20method%20that%20can%20reconstruct%20the%20geometry%20and%20appearance%20of%0Atwo%20objects%20undergoing%20close%20interactions%20while%20disjoining%20both%20in%203D%2C%20avoiding%0Asurface%20inter-penetrations%20and%20enabling%20novel-view%20synthesis%20of%20the%20observed%0Ascene.%20The%20framework%20is%20end-to-end%20trainable%20and%20supervised%20using%20a%20novel%0Aalpha-blending%20regularisation%20that%20ensures%20that%20the%20two%20geometries%20are%20well%0Aseparated%20even%20under%20extreme%20occlusions.%20Our%20reconstruction%20method%20is%0Amarkerless%20and%20can%20be%20applied%20to%20rigid%20as%20well%20as%20articulated%20objects.%20We%0Aintroduce%20a%20new%20dataset%20consisting%20of%20close%20interactions%20between%20a%20human%20and%20an%0Aobject%20and%20also%20evaluate%20on%20two%20scenes%20of%20humans%20performing%20martial%20arts.%20The%0Aexperiments%20confirm%20the%20effectiveness%20of%20our%20framework%20and%20substantial%0Aimprovements%20using%203D%20and%20novel%20view%20synthesis%20metrics%20compared%20to%20several%0Aexisting%20approaches%20applicable%20in%20our%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13968v1&entry.124074799=Read"},
{"title": "Spherical Dense Text-to-Image Synthesis", "author": "Timon Winter and Stanislav Frolov and Brian Bernhard Moser and Andreas Dengel", "abstract": "  Recent advancements in text-to-image (T2I) have improved synthesis results,\nbut challenges remain in layout control and generating omnidirectional\npanoramic images. Dense T2I (DT2I) and spherical T2I (ST2I) models address\nthese issues, but so far no unified approach exists. Trivial approaches, like\nprompting a DT2I model to generate panoramas can not generate proper spherical\ndistortions and seamless transitions at the borders. Our work shows that\nspherical dense text-to-image (SDT2I) can be achieved by integrating\ntraining-free DT2I approaches into finetuned panorama models. Specifically, we\npropose MultiStitchDiffusion (MSTD) and MultiPanFusion (MPF) by integrating\nMultiDiffusion into StitchDiffusion and PanFusion, respectively. Since no\nbenchmark for SDT2I exists, we further construct Dense-Synthetic-View\n(DSynView), a new synthetic dataset containing spherical layouts to evaluate\nour models. Our results show that MSTD outperforms MPF across image quality as\nwell as prompt- and layout adherence. MultiPanFusion generates more diverse\nimages but struggles to synthesize flawless foreground objects. We propose\nbootstrap-coupling and turning off equirectangular perspective-projection\nattention in the foreground as an improvement of MPF.\n", "link": "http://arxiv.org/abs/2502.12691v2", "date": "2025-02-19", "relevancy": 3.0884, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6246}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6246}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spherical%20Dense%20Text-to-Image%20Synthesis&body=Title%3A%20Spherical%20Dense%20Text-to-Image%20Synthesis%0AAuthor%3A%20Timon%20Winter%20and%20Stanislav%20Frolov%20and%20Brian%20Bernhard%20Moser%20and%20Andreas%20Dengel%0AAbstract%3A%20%20%20Recent%20advancements%20in%20text-to-image%20%28T2I%29%20have%20improved%20synthesis%20results%2C%0Abut%20challenges%20remain%20in%20layout%20control%20and%20generating%20omnidirectional%0Apanoramic%20images.%20Dense%20T2I%20%28DT2I%29%20and%20spherical%20T2I%20%28ST2I%29%20models%20address%0Athese%20issues%2C%20but%20so%20far%20no%20unified%20approach%20exists.%20Trivial%20approaches%2C%20like%0Aprompting%20a%20DT2I%20model%20to%20generate%20panoramas%20can%20not%20generate%20proper%20spherical%0Adistortions%20and%20seamless%20transitions%20at%20the%20borders.%20Our%20work%20shows%20that%0Aspherical%20dense%20text-to-image%20%28SDT2I%29%20can%20be%20achieved%20by%20integrating%0Atraining-free%20DT2I%20approaches%20into%20finetuned%20panorama%20models.%20Specifically%2C%20we%0Apropose%20MultiStitchDiffusion%20%28MSTD%29%20and%20MultiPanFusion%20%28MPF%29%20by%20integrating%0AMultiDiffusion%20into%20StitchDiffusion%20and%20PanFusion%2C%20respectively.%20Since%20no%0Abenchmark%20for%20SDT2I%20exists%2C%20we%20further%20construct%20Dense-Synthetic-View%0A%28DSynView%29%2C%20a%20new%20synthetic%20dataset%20containing%20spherical%20layouts%20to%20evaluate%0Aour%20models.%20Our%20results%20show%20that%20MSTD%20outperforms%20MPF%20across%20image%20quality%20as%0Awell%20as%20prompt-%20and%20layout%20adherence.%20MultiPanFusion%20generates%20more%20diverse%0Aimages%20but%20struggles%20to%20synthesize%20flawless%20foreground%20objects.%20We%20propose%0Abootstrap-coupling%20and%20turning%20off%20equirectangular%20perspective-projection%0Aattention%20in%20the%20foreground%20as%20an%20improvement%20of%20MPF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12691v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpherical%2520Dense%2520Text-to-Image%2520Synthesis%26entry.906535625%3DTimon%2520Winter%2520and%2520Stanislav%2520Frolov%2520and%2520Brian%2520Bernhard%2520Moser%2520and%2520Andreas%2520Dengel%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520text-to-image%2520%2528T2I%2529%2520have%2520improved%2520synthesis%2520results%252C%250Abut%2520challenges%2520remain%2520in%2520layout%2520control%2520and%2520generating%2520omnidirectional%250Apanoramic%2520images.%2520Dense%2520T2I%2520%2528DT2I%2529%2520and%2520spherical%2520T2I%2520%2528ST2I%2529%2520models%2520address%250Athese%2520issues%252C%2520but%2520so%2520far%2520no%2520unified%2520approach%2520exists.%2520Trivial%2520approaches%252C%2520like%250Aprompting%2520a%2520DT2I%2520model%2520to%2520generate%2520panoramas%2520can%2520not%2520generate%2520proper%2520spherical%250Adistortions%2520and%2520seamless%2520transitions%2520at%2520the%2520borders.%2520Our%2520work%2520shows%2520that%250Aspherical%2520dense%2520text-to-image%2520%2528SDT2I%2529%2520can%2520be%2520achieved%2520by%2520integrating%250Atraining-free%2520DT2I%2520approaches%2520into%2520finetuned%2520panorama%2520models.%2520Specifically%252C%2520we%250Apropose%2520MultiStitchDiffusion%2520%2528MSTD%2529%2520and%2520MultiPanFusion%2520%2528MPF%2529%2520by%2520integrating%250AMultiDiffusion%2520into%2520StitchDiffusion%2520and%2520PanFusion%252C%2520respectively.%2520Since%2520no%250Abenchmark%2520for%2520SDT2I%2520exists%252C%2520we%2520further%2520construct%2520Dense-Synthetic-View%250A%2528DSynView%2529%252C%2520a%2520new%2520synthetic%2520dataset%2520containing%2520spherical%2520layouts%2520to%2520evaluate%250Aour%2520models.%2520Our%2520results%2520show%2520that%2520MSTD%2520outperforms%2520MPF%2520across%2520image%2520quality%2520as%250Awell%2520as%2520prompt-%2520and%2520layout%2520adherence.%2520MultiPanFusion%2520generates%2520more%2520diverse%250Aimages%2520but%2520struggles%2520to%2520synthesize%2520flawless%2520foreground%2520objects.%2520We%2520propose%250Abootstrap-coupling%2520and%2520turning%2520off%2520equirectangular%2520perspective-projection%250Aattention%2520in%2520the%2520foreground%2520as%2520an%2520improvement%2520of%2520MPF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12691v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spherical%20Dense%20Text-to-Image%20Synthesis&entry.906535625=Timon%20Winter%20and%20Stanislav%20Frolov%20and%20Brian%20Bernhard%20Moser%20and%20Andreas%20Dengel&entry.1292438233=%20%20Recent%20advancements%20in%20text-to-image%20%28T2I%29%20have%20improved%20synthesis%20results%2C%0Abut%20challenges%20remain%20in%20layout%20control%20and%20generating%20omnidirectional%0Apanoramic%20images.%20Dense%20T2I%20%28DT2I%29%20and%20spherical%20T2I%20%28ST2I%29%20models%20address%0Athese%20issues%2C%20but%20so%20far%20no%20unified%20approach%20exists.%20Trivial%20approaches%2C%20like%0Aprompting%20a%20DT2I%20model%20to%20generate%20panoramas%20can%20not%20generate%20proper%20spherical%0Adistortions%20and%20seamless%20transitions%20at%20the%20borders.%20Our%20work%20shows%20that%0Aspherical%20dense%20text-to-image%20%28SDT2I%29%20can%20be%20achieved%20by%20integrating%0Atraining-free%20DT2I%20approaches%20into%20finetuned%20panorama%20models.%20Specifically%2C%20we%0Apropose%20MultiStitchDiffusion%20%28MSTD%29%20and%20MultiPanFusion%20%28MPF%29%20by%20integrating%0AMultiDiffusion%20into%20StitchDiffusion%20and%20PanFusion%2C%20respectively.%20Since%20no%0Abenchmark%20for%20SDT2I%20exists%2C%20we%20further%20construct%20Dense-Synthetic-View%0A%28DSynView%29%2C%20a%20new%20synthetic%20dataset%20containing%20spherical%20layouts%20to%20evaluate%0Aour%20models.%20Our%20results%20show%20that%20MSTD%20outperforms%20MPF%20across%20image%20quality%20as%0Awell%20as%20prompt-%20and%20layout%20adherence.%20MultiPanFusion%20generates%20more%20diverse%0Aimages%20but%20struggles%20to%20synthesize%20flawless%20foreground%20objects.%20We%20propose%0Abootstrap-coupling%20and%20turning%20off%20equirectangular%20perspective-projection%0Aattention%20in%20the%20foreground%20as%20an%20improvement%20of%20MPF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12691v2&entry.124074799=Read"},
{"title": "High-Quality 3D Creation from A Single Image Using Subject-Specific\n  Knowledge Prior", "author": "Nan Huang and Ting Zhang and Yuhui Yuan and Dong Chen and Shanghang Zhang", "abstract": "  In this paper, we address the critical bottleneck in robotics caused by the\nscarcity of diverse 3D data by presenting a novel two-stage approach for\ngenerating high-quality 3D models from a single image. This method is motivated\nby the need to efficiently expand 3D asset creation, particularly for robotics\ndatasets, where the variety of object types is currently limited compared to\ngeneral image datasets. Unlike previous methods that primarily rely on general\ndiffusion priors, which often struggle to align with the reference image, our\napproach leverages subject-specific prior knowledge. By incorporating\nsubject-specific priors in both geometry and texture, we ensure precise\nalignment between the generated 3D content and the reference object.\nSpecifically, we introduce a shading mode-aware prior into the NeRF\noptimization process, enhancing the geometry and refining texture in the coarse\noutputs to achieve superior quality. Extensive experiments demonstrate that our\nmethod significantly outperforms prior approaches.\n", "link": "http://arxiv.org/abs/2312.11535v3", "date": "2025-02-19", "relevancy": 3.0778, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.619}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.619}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Quality%203D%20Creation%20from%20A%20Single%20Image%20Using%20Subject-Specific%0A%20%20Knowledge%20Prior&body=Title%3A%20High-Quality%203D%20Creation%20from%20A%20Single%20Image%20Using%20Subject-Specific%0A%20%20Knowledge%20Prior%0AAuthor%3A%20Nan%20Huang%20and%20Ting%20Zhang%20and%20Yuhui%20Yuan%20and%20Dong%20Chen%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20the%20critical%20bottleneck%20in%20robotics%20caused%20by%20the%0Ascarcity%20of%20diverse%203D%20data%20by%20presenting%20a%20novel%20two-stage%20approach%20for%0Agenerating%20high-quality%203D%20models%20from%20a%20single%20image.%20This%20method%20is%20motivated%0Aby%20the%20need%20to%20efficiently%20expand%203D%20asset%20creation%2C%20particularly%20for%20robotics%0Adatasets%2C%20where%20the%20variety%20of%20object%20types%20is%20currently%20limited%20compared%20to%0Ageneral%20image%20datasets.%20Unlike%20previous%20methods%20that%20primarily%20rely%20on%20general%0Adiffusion%20priors%2C%20which%20often%20struggle%20to%20align%20with%20the%20reference%20image%2C%20our%0Aapproach%20leverages%20subject-specific%20prior%20knowledge.%20By%20incorporating%0Asubject-specific%20priors%20in%20both%20geometry%20and%20texture%2C%20we%20ensure%20precise%0Aalignment%20between%20the%20generated%203D%20content%20and%20the%20reference%20object.%0ASpecifically%2C%20we%20introduce%20a%20shading%20mode-aware%20prior%20into%20the%20NeRF%0Aoptimization%20process%2C%20enhancing%20the%20geometry%20and%20refining%20texture%20in%20the%20coarse%0Aoutputs%20to%20achieve%20superior%20quality.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20significantly%20outperforms%20prior%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.11535v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Quality%25203D%2520Creation%2520from%2520A%2520Single%2520Image%2520Using%2520Subject-Specific%250A%2520%2520Knowledge%2520Prior%26entry.906535625%3DNan%2520Huang%2520and%2520Ting%2520Zhang%2520and%2520Yuhui%2520Yuan%2520and%2520Dong%2520Chen%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520critical%2520bottleneck%2520in%2520robotics%2520caused%2520by%2520the%250Ascarcity%2520of%2520diverse%25203D%2520data%2520by%2520presenting%2520a%2520novel%2520two-stage%2520approach%2520for%250Agenerating%2520high-quality%25203D%2520models%2520from%2520a%2520single%2520image.%2520This%2520method%2520is%2520motivated%250Aby%2520the%2520need%2520to%2520efficiently%2520expand%25203D%2520asset%2520creation%252C%2520particularly%2520for%2520robotics%250Adatasets%252C%2520where%2520the%2520variety%2520of%2520object%2520types%2520is%2520currently%2520limited%2520compared%2520to%250Ageneral%2520image%2520datasets.%2520Unlike%2520previous%2520methods%2520that%2520primarily%2520rely%2520on%2520general%250Adiffusion%2520priors%252C%2520which%2520often%2520struggle%2520to%2520align%2520with%2520the%2520reference%2520image%252C%2520our%250Aapproach%2520leverages%2520subject-specific%2520prior%2520knowledge.%2520By%2520incorporating%250Asubject-specific%2520priors%2520in%2520both%2520geometry%2520and%2520texture%252C%2520we%2520ensure%2520precise%250Aalignment%2520between%2520the%2520generated%25203D%2520content%2520and%2520the%2520reference%2520object.%250ASpecifically%252C%2520we%2520introduce%2520a%2520shading%2520mode-aware%2520prior%2520into%2520the%2520NeRF%250Aoptimization%2520process%252C%2520enhancing%2520the%2520geometry%2520and%2520refining%2520texture%2520in%2520the%2520coarse%250Aoutputs%2520to%2520achieve%2520superior%2520quality.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Amethod%2520significantly%2520outperforms%2520prior%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.11535v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Quality%203D%20Creation%20from%20A%20Single%20Image%20Using%20Subject-Specific%0A%20%20Knowledge%20Prior&entry.906535625=Nan%20Huang%20and%20Ting%20Zhang%20and%20Yuhui%20Yuan%20and%20Dong%20Chen%20and%20Shanghang%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20critical%20bottleneck%20in%20robotics%20caused%20by%20the%0Ascarcity%20of%20diverse%203D%20data%20by%20presenting%20a%20novel%20two-stage%20approach%20for%0Agenerating%20high-quality%203D%20models%20from%20a%20single%20image.%20This%20method%20is%20motivated%0Aby%20the%20need%20to%20efficiently%20expand%203D%20asset%20creation%2C%20particularly%20for%20robotics%0Adatasets%2C%20where%20the%20variety%20of%20object%20types%20is%20currently%20limited%20compared%20to%0Ageneral%20image%20datasets.%20Unlike%20previous%20methods%20that%20primarily%20rely%20on%20general%0Adiffusion%20priors%2C%20which%20often%20struggle%20to%20align%20with%20the%20reference%20image%2C%20our%0Aapproach%20leverages%20subject-specific%20prior%20knowledge.%20By%20incorporating%0Asubject-specific%20priors%20in%20both%20geometry%20and%20texture%2C%20we%20ensure%20precise%0Aalignment%20between%20the%20generated%203D%20content%20and%20the%20reference%20object.%0ASpecifically%2C%20we%20introduce%20a%20shading%20mode-aware%20prior%20into%20the%20NeRF%0Aoptimization%20process%2C%20enhancing%20the%20geometry%20and%20refining%20texture%20in%20the%20coarse%0Aoutputs%20to%20achieve%20superior%20quality.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20significantly%20outperforms%20prior%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11535v3&entry.124074799=Read"},
{"title": "A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning\n  with Large Vision and Language Models", "author": "Hao Huang and Shuaihang Yuan and Yu Hao and Congcong Wen and Yi Fang", "abstract": "  A large-scale vision and language model that has been pretrained on massive\ndata encodes visual and linguistic prior, which makes it easier to generate\nimages and language that are more natural and realistic. Despite this, there is\nstill a significant domain gap between the modalities of vision and language,\nespecially when training data is scarce in few-shot settings, where only very\nlimited data are available for training. In order to mitigate this issue, a\nmulti-modal meta-learning framework has been proposed to bridge the gap between\ntwo frozen pretrained large vision and language models by introducing a tunable\nprompt connecting these two large models. For few-shot image captioning, the\nexisting multi-model meta-learning framework utilizes a one-step prompting\nscheme to accumulate the visual features of input images to guide the language\nmodel, which struggles to generate accurate image descriptions with only a few\ntraining samples. Instead, we propose a chain-of-thought (CoT) meta-learning\nscheme as a multi-step image captioning procedure to better imitate how humans\ndescribe images. In addition, we further propose to learn different\nmeta-parameters of the model corresponding to each CoT step in distinct\nsubspaces to avoid interference. We evaluated our method on three commonly used\nimage captioning datasets, i.e., MSCOCO, Flickr8k, and Flickr30k, under\nfew-shot settings. The results of our experiments indicate that our\nchain-of-thought subspace meta-learning strategy is superior to the baselines\nin terms of performance across different datasets measured by different\nmetrics.\n", "link": "http://arxiv.org/abs/2502.13942v1", "date": "2025-02-19", "relevancy": 2.9867, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6014}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Chain-of-Thought%20Subspace%20Meta-Learning%20for%20Few-shot%20Image%20Captioning%0A%20%20with%20Large%20Vision%20and%20Language%20Models&body=Title%3A%20A%20Chain-of-Thought%20Subspace%20Meta-Learning%20for%20Few-shot%20Image%20Captioning%0A%20%20with%20Large%20Vision%20and%20Language%20Models%0AAuthor%3A%20Hao%20Huang%20and%20Shuaihang%20Yuan%20and%20Yu%20Hao%20and%20Congcong%20Wen%20and%20Yi%20Fang%0AAbstract%3A%20%20%20A%20large-scale%20vision%20and%20language%20model%20that%20has%20been%20pretrained%20on%20massive%0Adata%20encodes%20visual%20and%20linguistic%20prior%2C%20which%20makes%20it%20easier%20to%20generate%0Aimages%20and%20language%20that%20are%20more%20natural%20and%20realistic.%20Despite%20this%2C%20there%20is%0Astill%20a%20significant%20domain%20gap%20between%20the%20modalities%20of%20vision%20and%20language%2C%0Aespecially%20when%20training%20data%20is%20scarce%20in%20few-shot%20settings%2C%20where%20only%20very%0Alimited%20data%20are%20available%20for%20training.%20In%20order%20to%20mitigate%20this%20issue%2C%20a%0Amulti-modal%20meta-learning%20framework%20has%20been%20proposed%20to%20bridge%20the%20gap%20between%0Atwo%20frozen%20pretrained%20large%20vision%20and%20language%20models%20by%20introducing%20a%20tunable%0Aprompt%20connecting%20these%20two%20large%20models.%20For%20few-shot%20image%20captioning%2C%20the%0Aexisting%20multi-model%20meta-learning%20framework%20utilizes%20a%20one-step%20prompting%0Ascheme%20to%20accumulate%20the%20visual%20features%20of%20input%20images%20to%20guide%20the%20language%0Amodel%2C%20which%20struggles%20to%20generate%20accurate%20image%20descriptions%20with%20only%20a%20few%0Atraining%20samples.%20Instead%2C%20we%20propose%20a%20chain-of-thought%20%28CoT%29%20meta-learning%0Ascheme%20as%20a%20multi-step%20image%20captioning%20procedure%20to%20better%20imitate%20how%20humans%0Adescribe%20images.%20In%20addition%2C%20we%20further%20propose%20to%20learn%20different%0Ameta-parameters%20of%20the%20model%20corresponding%20to%20each%20CoT%20step%20in%20distinct%0Asubspaces%20to%20avoid%20interference.%20We%20evaluated%20our%20method%20on%20three%20commonly%20used%0Aimage%20captioning%20datasets%2C%20i.e.%2C%20MSCOCO%2C%20Flickr8k%2C%20and%20Flickr30k%2C%20under%0Afew-shot%20settings.%20The%20results%20of%20our%20experiments%20indicate%20that%20our%0Achain-of-thought%20subspace%20meta-learning%20strategy%20is%20superior%20to%20the%20baselines%0Ain%20terms%20of%20performance%20across%20different%20datasets%20measured%20by%20different%0Ametrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Chain-of-Thought%2520Subspace%2520Meta-Learning%2520for%2520Few-shot%2520Image%2520Captioning%250A%2520%2520with%2520Large%2520Vision%2520and%2520Language%2520Models%26entry.906535625%3DHao%2520Huang%2520and%2520Shuaihang%2520Yuan%2520and%2520Yu%2520Hao%2520and%2520Congcong%2520Wen%2520and%2520Yi%2520Fang%26entry.1292438233%3D%2520%2520A%2520large-scale%2520vision%2520and%2520language%2520model%2520that%2520has%2520been%2520pretrained%2520on%2520massive%250Adata%2520encodes%2520visual%2520and%2520linguistic%2520prior%252C%2520which%2520makes%2520it%2520easier%2520to%2520generate%250Aimages%2520and%2520language%2520that%2520are%2520more%2520natural%2520and%2520realistic.%2520Despite%2520this%252C%2520there%2520is%250Astill%2520a%2520significant%2520domain%2520gap%2520between%2520the%2520modalities%2520of%2520vision%2520and%2520language%252C%250Aespecially%2520when%2520training%2520data%2520is%2520scarce%2520in%2520few-shot%2520settings%252C%2520where%2520only%2520very%250Alimited%2520data%2520are%2520available%2520for%2520training.%2520In%2520order%2520to%2520mitigate%2520this%2520issue%252C%2520a%250Amulti-modal%2520meta-learning%2520framework%2520has%2520been%2520proposed%2520to%2520bridge%2520the%2520gap%2520between%250Atwo%2520frozen%2520pretrained%2520large%2520vision%2520and%2520language%2520models%2520by%2520introducing%2520a%2520tunable%250Aprompt%2520connecting%2520these%2520two%2520large%2520models.%2520For%2520few-shot%2520image%2520captioning%252C%2520the%250Aexisting%2520multi-model%2520meta-learning%2520framework%2520utilizes%2520a%2520one-step%2520prompting%250Ascheme%2520to%2520accumulate%2520the%2520visual%2520features%2520of%2520input%2520images%2520to%2520guide%2520the%2520language%250Amodel%252C%2520which%2520struggles%2520to%2520generate%2520accurate%2520image%2520descriptions%2520with%2520only%2520a%2520few%250Atraining%2520samples.%2520Instead%252C%2520we%2520propose%2520a%2520chain-of-thought%2520%2528CoT%2529%2520meta-learning%250Ascheme%2520as%2520a%2520multi-step%2520image%2520captioning%2520procedure%2520to%2520better%2520imitate%2520how%2520humans%250Adescribe%2520images.%2520In%2520addition%252C%2520we%2520further%2520propose%2520to%2520learn%2520different%250Ameta-parameters%2520of%2520the%2520model%2520corresponding%2520to%2520each%2520CoT%2520step%2520in%2520distinct%250Asubspaces%2520to%2520avoid%2520interference.%2520We%2520evaluated%2520our%2520method%2520on%2520three%2520commonly%2520used%250Aimage%2520captioning%2520datasets%252C%2520i.e.%252C%2520MSCOCO%252C%2520Flickr8k%252C%2520and%2520Flickr30k%252C%2520under%250Afew-shot%2520settings.%2520The%2520results%2520of%2520our%2520experiments%2520indicate%2520that%2520our%250Achain-of-thought%2520subspace%2520meta-learning%2520strategy%2520is%2520superior%2520to%2520the%2520baselines%250Ain%2520terms%2520of%2520performance%2520across%2520different%2520datasets%2520measured%2520by%2520different%250Ametrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Chain-of-Thought%20Subspace%20Meta-Learning%20for%20Few-shot%20Image%20Captioning%0A%20%20with%20Large%20Vision%20and%20Language%20Models&entry.906535625=Hao%20Huang%20and%20Shuaihang%20Yuan%20and%20Yu%20Hao%20and%20Congcong%20Wen%20and%20Yi%20Fang&entry.1292438233=%20%20A%20large-scale%20vision%20and%20language%20model%20that%20has%20been%20pretrained%20on%20massive%0Adata%20encodes%20visual%20and%20linguistic%20prior%2C%20which%20makes%20it%20easier%20to%20generate%0Aimages%20and%20language%20that%20are%20more%20natural%20and%20realistic.%20Despite%20this%2C%20there%20is%0Astill%20a%20significant%20domain%20gap%20between%20the%20modalities%20of%20vision%20and%20language%2C%0Aespecially%20when%20training%20data%20is%20scarce%20in%20few-shot%20settings%2C%20where%20only%20very%0Alimited%20data%20are%20available%20for%20training.%20In%20order%20to%20mitigate%20this%20issue%2C%20a%0Amulti-modal%20meta-learning%20framework%20has%20been%20proposed%20to%20bridge%20the%20gap%20between%0Atwo%20frozen%20pretrained%20large%20vision%20and%20language%20models%20by%20introducing%20a%20tunable%0Aprompt%20connecting%20these%20two%20large%20models.%20For%20few-shot%20image%20captioning%2C%20the%0Aexisting%20multi-model%20meta-learning%20framework%20utilizes%20a%20one-step%20prompting%0Ascheme%20to%20accumulate%20the%20visual%20features%20of%20input%20images%20to%20guide%20the%20language%0Amodel%2C%20which%20struggles%20to%20generate%20accurate%20image%20descriptions%20with%20only%20a%20few%0Atraining%20samples.%20Instead%2C%20we%20propose%20a%20chain-of-thought%20%28CoT%29%20meta-learning%0Ascheme%20as%20a%20multi-step%20image%20captioning%20procedure%20to%20better%20imitate%20how%20humans%0Adescribe%20images.%20In%20addition%2C%20we%20further%20propose%20to%20learn%20different%0Ameta-parameters%20of%20the%20model%20corresponding%20to%20each%20CoT%20step%20in%20distinct%0Asubspaces%20to%20avoid%20interference.%20We%20evaluated%20our%20method%20on%20three%20commonly%20used%0Aimage%20captioning%20datasets%2C%20i.e.%2C%20MSCOCO%2C%20Flickr8k%2C%20and%20Flickr30k%2C%20under%0Afew-shot%20settings.%20The%20results%20of%20our%20experiments%20indicate%20that%20our%0Achain-of-thought%20subspace%20meta-learning%20strategy%20is%20superior%20to%20the%20baselines%0Ain%20terms%20of%20performance%20across%20different%20datasets%20measured%20by%20different%0Ametrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13942v1&entry.124074799=Read"},
{"title": "Generative Video Semantic Communication via Multimodal Semantic Fusion\n  with Large Model", "author": "Hang Yin and Li Qiao and Yu Ma and Shuo Sun and Kan Li and Zhen Gao and Dusit Niyato", "abstract": "  Despite significant advancements in traditional syntactic communications\nbased on Shannon's theory, these methods struggle to meet the requirements of\n6G immersive communications, especially under challenging transmission\nconditions. With the development of generative artificial intelligence (GenAI),\nprogress has been made in reconstructing videos using high-level semantic\ninformation. In this paper, we propose a scalable generative video semantic\ncommunication framework that extracts and transmits semantic information to\nachieve high-quality video reconstruction. Specifically, at the transmitter,\ndescription and other condition signals (e.g., first frame, sketches, etc.) are\nextracted from the source video, functioning as text and structural semantics,\nrespectively. At the receiver, the diffusion-based GenAI large models are\nutilized to fuse the semantics of the multiple modalities for reconstructing\nthe video. Simulation results demonstrate that, at an ultra-low channel\nbandwidth ratio (CBR), our scheme effectively captures semantic information to\nreconstruct videos aligned with human perception under different\nsignal-to-noise ratios. Notably, the proposed ``First Frame+Desc.\" scheme\nconsistently achieves CLIP score exceeding 0.92 at CBR = 0.0057 for SNR > 0 dB.\nThis demonstrates its robust performance even under low SNR conditions.\n", "link": "http://arxiv.org/abs/2502.13838v1", "date": "2025-02-19", "relevancy": 2.978, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6237}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5835}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Video%20Semantic%20Communication%20via%20Multimodal%20Semantic%20Fusion%0A%20%20with%20Large%20Model&body=Title%3A%20Generative%20Video%20Semantic%20Communication%20via%20Multimodal%20Semantic%20Fusion%0A%20%20with%20Large%20Model%0AAuthor%3A%20Hang%20Yin%20and%20Li%20Qiao%20and%20Yu%20Ma%20and%20Shuo%20Sun%20and%20Kan%20Li%20and%20Zhen%20Gao%20and%20Dusit%20Niyato%0AAbstract%3A%20%20%20Despite%20significant%20advancements%20in%20traditional%20syntactic%20communications%0Abased%20on%20Shannon%27s%20theory%2C%20these%20methods%20struggle%20to%20meet%20the%20requirements%20of%0A6G%20immersive%20communications%2C%20especially%20under%20challenging%20transmission%0Aconditions.%20With%20the%20development%20of%20generative%20artificial%20intelligence%20%28GenAI%29%2C%0Aprogress%20has%20been%20made%20in%20reconstructing%20videos%20using%20high-level%20semantic%0Ainformation.%20In%20this%20paper%2C%20we%20propose%20a%20scalable%20generative%20video%20semantic%0Acommunication%20framework%20that%20extracts%20and%20transmits%20semantic%20information%20to%0Aachieve%20high-quality%20video%20reconstruction.%20Specifically%2C%20at%20the%20transmitter%2C%0Adescription%20and%20other%20condition%20signals%20%28e.g.%2C%20first%20frame%2C%20sketches%2C%20etc.%29%20are%0Aextracted%20from%20the%20source%20video%2C%20functioning%20as%20text%20and%20structural%20semantics%2C%0Arespectively.%20At%20the%20receiver%2C%20the%20diffusion-based%20GenAI%20large%20models%20are%0Autilized%20to%20fuse%20the%20semantics%20of%20the%20multiple%20modalities%20for%20reconstructing%0Athe%20video.%20Simulation%20results%20demonstrate%20that%2C%20at%20an%20ultra-low%20channel%0Abandwidth%20ratio%20%28CBR%29%2C%20our%20scheme%20effectively%20captures%20semantic%20information%20to%0Areconstruct%20videos%20aligned%20with%20human%20perception%20under%20different%0Asignal-to-noise%20ratios.%20Notably%2C%20the%20proposed%20%60%60First%20Frame%2BDesc.%22%20scheme%0Aconsistently%20achieves%20CLIP%20score%20exceeding%200.92%20at%20CBR%20%3D%200.0057%20for%20SNR%20%3E%200%20dB.%0AThis%20demonstrates%20its%20robust%20performance%20even%20under%20low%20SNR%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Video%2520Semantic%2520Communication%2520via%2520Multimodal%2520Semantic%2520Fusion%250A%2520%2520with%2520Large%2520Model%26entry.906535625%3DHang%2520Yin%2520and%2520Li%2520Qiao%2520and%2520Yu%2520Ma%2520and%2520Shuo%2520Sun%2520and%2520Kan%2520Li%2520and%2520Zhen%2520Gao%2520and%2520Dusit%2520Niyato%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advancements%2520in%2520traditional%2520syntactic%2520communications%250Abased%2520on%2520Shannon%2527s%2520theory%252C%2520these%2520methods%2520struggle%2520to%2520meet%2520the%2520requirements%2520of%250A6G%2520immersive%2520communications%252C%2520especially%2520under%2520challenging%2520transmission%250Aconditions.%2520With%2520the%2520development%2520of%2520generative%2520artificial%2520intelligence%2520%2528GenAI%2529%252C%250Aprogress%2520has%2520been%2520made%2520in%2520reconstructing%2520videos%2520using%2520high-level%2520semantic%250Ainformation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520scalable%2520generative%2520video%2520semantic%250Acommunication%2520framework%2520that%2520extracts%2520and%2520transmits%2520semantic%2520information%2520to%250Aachieve%2520high-quality%2520video%2520reconstruction.%2520Specifically%252C%2520at%2520the%2520transmitter%252C%250Adescription%2520and%2520other%2520condition%2520signals%2520%2528e.g.%252C%2520first%2520frame%252C%2520sketches%252C%2520etc.%2529%2520are%250Aextracted%2520from%2520the%2520source%2520video%252C%2520functioning%2520as%2520text%2520and%2520structural%2520semantics%252C%250Arespectively.%2520At%2520the%2520receiver%252C%2520the%2520diffusion-based%2520GenAI%2520large%2520models%2520are%250Autilized%2520to%2520fuse%2520the%2520semantics%2520of%2520the%2520multiple%2520modalities%2520for%2520reconstructing%250Athe%2520video.%2520Simulation%2520results%2520demonstrate%2520that%252C%2520at%2520an%2520ultra-low%2520channel%250Abandwidth%2520ratio%2520%2528CBR%2529%252C%2520our%2520scheme%2520effectively%2520captures%2520semantic%2520information%2520to%250Areconstruct%2520videos%2520aligned%2520with%2520human%2520perception%2520under%2520different%250Asignal-to-noise%2520ratios.%2520Notably%252C%2520the%2520proposed%2520%2560%2560First%2520Frame%252BDesc.%2522%2520scheme%250Aconsistently%2520achieves%2520CLIP%2520score%2520exceeding%25200.92%2520at%2520CBR%2520%253D%25200.0057%2520for%2520SNR%2520%253E%25200%2520dB.%250AThis%2520demonstrates%2520its%2520robust%2520performance%2520even%2520under%2520low%2520SNR%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Video%20Semantic%20Communication%20via%20Multimodal%20Semantic%20Fusion%0A%20%20with%20Large%20Model&entry.906535625=Hang%20Yin%20and%20Li%20Qiao%20and%20Yu%20Ma%20and%20Shuo%20Sun%20and%20Kan%20Li%20and%20Zhen%20Gao%20and%20Dusit%20Niyato&entry.1292438233=%20%20Despite%20significant%20advancements%20in%20traditional%20syntactic%20communications%0Abased%20on%20Shannon%27s%20theory%2C%20these%20methods%20struggle%20to%20meet%20the%20requirements%20of%0A6G%20immersive%20communications%2C%20especially%20under%20challenging%20transmission%0Aconditions.%20With%20the%20development%20of%20generative%20artificial%20intelligence%20%28GenAI%29%2C%0Aprogress%20has%20been%20made%20in%20reconstructing%20videos%20using%20high-level%20semantic%0Ainformation.%20In%20this%20paper%2C%20we%20propose%20a%20scalable%20generative%20video%20semantic%0Acommunication%20framework%20that%20extracts%20and%20transmits%20semantic%20information%20to%0Aachieve%20high-quality%20video%20reconstruction.%20Specifically%2C%20at%20the%20transmitter%2C%0Adescription%20and%20other%20condition%20signals%20%28e.g.%2C%20first%20frame%2C%20sketches%2C%20etc.%29%20are%0Aextracted%20from%20the%20source%20video%2C%20functioning%20as%20text%20and%20structural%20semantics%2C%0Arespectively.%20At%20the%20receiver%2C%20the%20diffusion-based%20GenAI%20large%20models%20are%0Autilized%20to%20fuse%20the%20semantics%20of%20the%20multiple%20modalities%20for%20reconstructing%0Athe%20video.%20Simulation%20results%20demonstrate%20that%2C%20at%20an%20ultra-low%20channel%0Abandwidth%20ratio%20%28CBR%29%2C%20our%20scheme%20effectively%20captures%20semantic%20information%20to%0Areconstruct%20videos%20aligned%20with%20human%20perception%20under%20different%0Asignal-to-noise%20ratios.%20Notably%2C%20the%20proposed%20%60%60First%20Frame%2BDesc.%22%20scheme%0Aconsistently%20achieves%20CLIP%20score%20exceeding%200.92%20at%20CBR%20%3D%200.0057%20for%20SNR%20%3E%200%20dB.%0AThis%20demonstrates%20its%20robust%20performance%20even%20under%20low%20SNR%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13838v1&entry.124074799=Read"},
{"title": "Continually Learning Structured Visual Representations via Network\n  Refinement with Rerelation", "author": "Zeki Doruk Erden and Boi Faltings", "abstract": "  Current machine learning paradigm relies on continuous representations like\nneural networks, which iteratively adjust parameters to approximate outcomes\nrather than directly learning the structure of problem. This spreads\ninformation across the network, causing issues like information loss and\nincomprehensibility Building on prior work in environment dynamics modeling, we\npropose a method that learns visual space in a structured, continual manner.\nOur approach refines networks to capture the core structure of objects while\nrepresenting significant subvariants in structure efficiently. We demonstrate\nthis with 2D shape detection, showing incremental learning on MNIST without\noverwriting knowledge and creating compact, comprehensible representations.\nThese results offer a promising step toward a transparent, continually learning\nalternative to traditional neural networks for visual processing.\n", "link": "http://arxiv.org/abs/2502.13935v1", "date": "2025-02-19", "relevancy": 2.9451, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6196}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5758}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continually%20Learning%20Structured%20Visual%20Representations%20via%20Network%0A%20%20Refinement%20with%20Rerelation&body=Title%3A%20Continually%20Learning%20Structured%20Visual%20Representations%20via%20Network%0A%20%20Refinement%20with%20Rerelation%0AAuthor%3A%20Zeki%20Doruk%20Erden%20and%20Boi%20Faltings%0AAbstract%3A%20%20%20Current%20machine%20learning%20paradigm%20relies%20on%20continuous%20representations%20like%0Aneural%20networks%2C%20which%20iteratively%20adjust%20parameters%20to%20approximate%20outcomes%0Arather%20than%20directly%20learning%20the%20structure%20of%20problem.%20This%20spreads%0Ainformation%20across%20the%20network%2C%20causing%20issues%20like%20information%20loss%20and%0Aincomprehensibility%20Building%20on%20prior%20work%20in%20environment%20dynamics%20modeling%2C%20we%0Apropose%20a%20method%20that%20learns%20visual%20space%20in%20a%20structured%2C%20continual%20manner.%0AOur%20approach%20refines%20networks%20to%20capture%20the%20core%20structure%20of%20objects%20while%0Arepresenting%20significant%20subvariants%20in%20structure%20efficiently.%20We%20demonstrate%0Athis%20with%202D%20shape%20detection%2C%20showing%20incremental%20learning%20on%20MNIST%20without%0Aoverwriting%20knowledge%20and%20creating%20compact%2C%20comprehensible%20representations.%0AThese%20results%20offer%20a%20promising%20step%20toward%20a%20transparent%2C%20continually%20learning%0Aalternative%20to%20traditional%20neural%20networks%20for%20visual%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13935v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinually%2520Learning%2520Structured%2520Visual%2520Representations%2520via%2520Network%250A%2520%2520Refinement%2520with%2520Rerelation%26entry.906535625%3DZeki%2520Doruk%2520Erden%2520and%2520Boi%2520Faltings%26entry.1292438233%3D%2520%2520Current%2520machine%2520learning%2520paradigm%2520relies%2520on%2520continuous%2520representations%2520like%250Aneural%2520networks%252C%2520which%2520iteratively%2520adjust%2520parameters%2520to%2520approximate%2520outcomes%250Arather%2520than%2520directly%2520learning%2520the%2520structure%2520of%2520problem.%2520This%2520spreads%250Ainformation%2520across%2520the%2520network%252C%2520causing%2520issues%2520like%2520information%2520loss%2520and%250Aincomprehensibility%2520Building%2520on%2520prior%2520work%2520in%2520environment%2520dynamics%2520modeling%252C%2520we%250Apropose%2520a%2520method%2520that%2520learns%2520visual%2520space%2520in%2520a%2520structured%252C%2520continual%2520manner.%250AOur%2520approach%2520refines%2520networks%2520to%2520capture%2520the%2520core%2520structure%2520of%2520objects%2520while%250Arepresenting%2520significant%2520subvariants%2520in%2520structure%2520efficiently.%2520We%2520demonstrate%250Athis%2520with%25202D%2520shape%2520detection%252C%2520showing%2520incremental%2520learning%2520on%2520MNIST%2520without%250Aoverwriting%2520knowledge%2520and%2520creating%2520compact%252C%2520comprehensible%2520representations.%250AThese%2520results%2520offer%2520a%2520promising%2520step%2520toward%2520a%2520transparent%252C%2520continually%2520learning%250Aalternative%2520to%2520traditional%2520neural%2520networks%2520for%2520visual%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13935v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continually%20Learning%20Structured%20Visual%20Representations%20via%20Network%0A%20%20Refinement%20with%20Rerelation&entry.906535625=Zeki%20Doruk%20Erden%20and%20Boi%20Faltings&entry.1292438233=%20%20Current%20machine%20learning%20paradigm%20relies%20on%20continuous%20representations%20like%0Aneural%20networks%2C%20which%20iteratively%20adjust%20parameters%20to%20approximate%20outcomes%0Arather%20than%20directly%20learning%20the%20structure%20of%20problem.%20This%20spreads%0Ainformation%20across%20the%20network%2C%20causing%20issues%20like%20information%20loss%20and%0Aincomprehensibility%20Building%20on%20prior%20work%20in%20environment%20dynamics%20modeling%2C%20we%0Apropose%20a%20method%20that%20learns%20visual%20space%20in%20a%20structured%2C%20continual%20manner.%0AOur%20approach%20refines%20networks%20to%20capture%20the%20core%20structure%20of%20objects%20while%0Arepresenting%20significant%20subvariants%20in%20structure%20efficiently.%20We%20demonstrate%0Athis%20with%202D%20shape%20detection%2C%20showing%20incremental%20learning%20on%20MNIST%20without%0Aoverwriting%20knowledge%20and%20creating%20compact%2C%20comprehensible%20representations.%0AThese%20results%20offer%20a%20promising%20step%20toward%20a%20transparent%2C%20continually%20learning%0Aalternative%20to%20traditional%20neural%20networks%20for%20visual%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13935v1&entry.124074799=Read"},
{"title": "Multi-view Video-Pose Pretraining for Operating Room Surgical Activity\n  Recognition", "author": "Idris Hamoud and Vinkle Srivastav and Muhammad Abdullah Jamal and Didier Mutter and Omid Mohareri and Nicolas Padoy", "abstract": "  Understanding the workflow of surgical procedures in complex operating rooms\nrequires a deep understanding of the interactions between clinicians and their\nenvironment. Surgical activity recognition (SAR) is a key computer vision task\nthat detects activities or phases from multi-view camera recordings. Existing\nSAR models often fail to account for fine-grained clinician movements and\nmulti-view knowledge, or they require calibrated multi-view camera setups and\nadvanced point-cloud processing to obtain better results. In this work, we\npropose a novel calibration-free multi-view multi-modal pretraining framework\ncalled Multiview Pretraining for Video-Pose Surgical Activity Recognition\nPreViPS, which aligns 2D pose and vision embeddings across camera views. Our\nmodel follows CLIP-style dual-encoder architecture: one encoder processes\nvisual features, while the other encodes human pose embeddings. To handle the\ncontinuous 2D human pose coordinates, we introduce a tokenized discrete\nrepresentation to convert the continuous 2D pose coordinates into discrete pose\nembeddings, thereby enabling efficient integration within the dual-encoder\nframework. To bridge the gap between these two modalities, we propose several\npretraining objectives using cross- and in-modality geometric constraints\nwithin the embedding space and incorporating masked pose token prediction\nstrategy to enhance representation learning. Extensive experiments and ablation\nstudies demonstrate improvements over the strong baselines, while\ndata-efficiency experiments on two distinct operating room datasets further\nhighlight the effectiveness of our approach. We highlight the benefits of our\napproach for surgical activity recognition in both multi-view and single-view\nsettings, showcasing its practical applicability in complex surgical\nenvironments. Code will be made available at:\nhttps://github.com/CAMMA-public/PreViPS.\n", "link": "http://arxiv.org/abs/2502.13883v1", "date": "2025-02-19", "relevancy": 2.8989, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5933}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.573}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-view%20Video-Pose%20Pretraining%20for%20Operating%20Room%20Surgical%20Activity%0A%20%20Recognition&body=Title%3A%20Multi-view%20Video-Pose%20Pretraining%20for%20Operating%20Room%20Surgical%20Activity%0A%20%20Recognition%0AAuthor%3A%20Idris%20Hamoud%20and%20Vinkle%20Srivastav%20and%20Muhammad%20Abdullah%20Jamal%20and%20Didier%20Mutter%20and%20Omid%20Mohareri%20and%20Nicolas%20Padoy%0AAbstract%3A%20%20%20Understanding%20the%20workflow%20of%20surgical%20procedures%20in%20complex%20operating%20rooms%0Arequires%20a%20deep%20understanding%20of%20the%20interactions%20between%20clinicians%20and%20their%0Aenvironment.%20Surgical%20activity%20recognition%20%28SAR%29%20is%20a%20key%20computer%20vision%20task%0Athat%20detects%20activities%20or%20phases%20from%20multi-view%20camera%20recordings.%20Existing%0ASAR%20models%20often%20fail%20to%20account%20for%20fine-grained%20clinician%20movements%20and%0Amulti-view%20knowledge%2C%20or%20they%20require%20calibrated%20multi-view%20camera%20setups%20and%0Aadvanced%20point-cloud%20processing%20to%20obtain%20better%20results.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20calibration-free%20multi-view%20multi-modal%20pretraining%20framework%0Acalled%20Multiview%20Pretraining%20for%20Video-Pose%20Surgical%20Activity%20Recognition%0APreViPS%2C%20which%20aligns%202D%20pose%20and%20vision%20embeddings%20across%20camera%20views.%20Our%0Amodel%20follows%20CLIP-style%20dual-encoder%20architecture%3A%20one%20encoder%20processes%0Avisual%20features%2C%20while%20the%20other%20encodes%20human%20pose%20embeddings.%20To%20handle%20the%0Acontinuous%202D%20human%20pose%20coordinates%2C%20we%20introduce%20a%20tokenized%20discrete%0Arepresentation%20to%20convert%20the%20continuous%202D%20pose%20coordinates%20into%20discrete%20pose%0Aembeddings%2C%20thereby%20enabling%20efficient%20integration%20within%20the%20dual-encoder%0Aframework.%20To%20bridge%20the%20gap%20between%20these%20two%20modalities%2C%20we%20propose%20several%0Apretraining%20objectives%20using%20cross-%20and%20in-modality%20geometric%20constraints%0Awithin%20the%20embedding%20space%20and%20incorporating%20masked%20pose%20token%20prediction%0Astrategy%20to%20enhance%20representation%20learning.%20Extensive%20experiments%20and%20ablation%0Astudies%20demonstrate%20improvements%20over%20the%20strong%20baselines%2C%20while%0Adata-efficiency%20experiments%20on%20two%20distinct%20operating%20room%20datasets%20further%0Ahighlight%20the%20effectiveness%20of%20our%20approach.%20We%20highlight%20the%20benefits%20of%20our%0Aapproach%20for%20surgical%20activity%20recognition%20in%20both%20multi-view%20and%20single-view%0Asettings%2C%20showcasing%20its%20practical%20applicability%20in%20complex%20surgical%0Aenvironments.%20Code%20will%20be%20made%20available%20at%3A%0Ahttps%3A//github.com/CAMMA-public/PreViPS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13883v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-view%2520Video-Pose%2520Pretraining%2520for%2520Operating%2520Room%2520Surgical%2520Activity%250A%2520%2520Recognition%26entry.906535625%3DIdris%2520Hamoud%2520and%2520Vinkle%2520Srivastav%2520and%2520Muhammad%2520Abdullah%2520Jamal%2520and%2520Didier%2520Mutter%2520and%2520Omid%2520Mohareri%2520and%2520Nicolas%2520Padoy%26entry.1292438233%3D%2520%2520Understanding%2520the%2520workflow%2520of%2520surgical%2520procedures%2520in%2520complex%2520operating%2520rooms%250Arequires%2520a%2520deep%2520understanding%2520of%2520the%2520interactions%2520between%2520clinicians%2520and%2520their%250Aenvironment.%2520Surgical%2520activity%2520recognition%2520%2528SAR%2529%2520is%2520a%2520key%2520computer%2520vision%2520task%250Athat%2520detects%2520activities%2520or%2520phases%2520from%2520multi-view%2520camera%2520recordings.%2520Existing%250ASAR%2520models%2520often%2520fail%2520to%2520account%2520for%2520fine-grained%2520clinician%2520movements%2520and%250Amulti-view%2520knowledge%252C%2520or%2520they%2520require%2520calibrated%2520multi-view%2520camera%2520setups%2520and%250Aadvanced%2520point-cloud%2520processing%2520to%2520obtain%2520better%2520results.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520calibration-free%2520multi-view%2520multi-modal%2520pretraining%2520framework%250Acalled%2520Multiview%2520Pretraining%2520for%2520Video-Pose%2520Surgical%2520Activity%2520Recognition%250APreViPS%252C%2520which%2520aligns%25202D%2520pose%2520and%2520vision%2520embeddings%2520across%2520camera%2520views.%2520Our%250Amodel%2520follows%2520CLIP-style%2520dual-encoder%2520architecture%253A%2520one%2520encoder%2520processes%250Avisual%2520features%252C%2520while%2520the%2520other%2520encodes%2520human%2520pose%2520embeddings.%2520To%2520handle%2520the%250Acontinuous%25202D%2520human%2520pose%2520coordinates%252C%2520we%2520introduce%2520a%2520tokenized%2520discrete%250Arepresentation%2520to%2520convert%2520the%2520continuous%25202D%2520pose%2520coordinates%2520into%2520discrete%2520pose%250Aembeddings%252C%2520thereby%2520enabling%2520efficient%2520integration%2520within%2520the%2520dual-encoder%250Aframework.%2520To%2520bridge%2520the%2520gap%2520between%2520these%2520two%2520modalities%252C%2520we%2520propose%2520several%250Apretraining%2520objectives%2520using%2520cross-%2520and%2520in-modality%2520geometric%2520constraints%250Awithin%2520the%2520embedding%2520space%2520and%2520incorporating%2520masked%2520pose%2520token%2520prediction%250Astrategy%2520to%2520enhance%2520representation%2520learning.%2520Extensive%2520experiments%2520and%2520ablation%250Astudies%2520demonstrate%2520improvements%2520over%2520the%2520strong%2520baselines%252C%2520while%250Adata-efficiency%2520experiments%2520on%2520two%2520distinct%2520operating%2520room%2520datasets%2520further%250Ahighlight%2520the%2520effectiveness%2520of%2520our%2520approach.%2520We%2520highlight%2520the%2520benefits%2520of%2520our%250Aapproach%2520for%2520surgical%2520activity%2520recognition%2520in%2520both%2520multi-view%2520and%2520single-view%250Asettings%252C%2520showcasing%2520its%2520practical%2520applicability%2520in%2520complex%2520surgical%250Aenvironments.%2520Code%2520will%2520be%2520made%2520available%2520at%253A%250Ahttps%253A//github.com/CAMMA-public/PreViPS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13883v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-view%20Video-Pose%20Pretraining%20for%20Operating%20Room%20Surgical%20Activity%0A%20%20Recognition&entry.906535625=Idris%20Hamoud%20and%20Vinkle%20Srivastav%20and%20Muhammad%20Abdullah%20Jamal%20and%20Didier%20Mutter%20and%20Omid%20Mohareri%20and%20Nicolas%20Padoy&entry.1292438233=%20%20Understanding%20the%20workflow%20of%20surgical%20procedures%20in%20complex%20operating%20rooms%0Arequires%20a%20deep%20understanding%20of%20the%20interactions%20between%20clinicians%20and%20their%0Aenvironment.%20Surgical%20activity%20recognition%20%28SAR%29%20is%20a%20key%20computer%20vision%20task%0Athat%20detects%20activities%20or%20phases%20from%20multi-view%20camera%20recordings.%20Existing%0ASAR%20models%20often%20fail%20to%20account%20for%20fine-grained%20clinician%20movements%20and%0Amulti-view%20knowledge%2C%20or%20they%20require%20calibrated%20multi-view%20camera%20setups%20and%0Aadvanced%20point-cloud%20processing%20to%20obtain%20better%20results.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20calibration-free%20multi-view%20multi-modal%20pretraining%20framework%0Acalled%20Multiview%20Pretraining%20for%20Video-Pose%20Surgical%20Activity%20Recognition%0APreViPS%2C%20which%20aligns%202D%20pose%20and%20vision%20embeddings%20across%20camera%20views.%20Our%0Amodel%20follows%20CLIP-style%20dual-encoder%20architecture%3A%20one%20encoder%20processes%0Avisual%20features%2C%20while%20the%20other%20encodes%20human%20pose%20embeddings.%20To%20handle%20the%0Acontinuous%202D%20human%20pose%20coordinates%2C%20we%20introduce%20a%20tokenized%20discrete%0Arepresentation%20to%20convert%20the%20continuous%202D%20pose%20coordinates%20into%20discrete%20pose%0Aembeddings%2C%20thereby%20enabling%20efficient%20integration%20within%20the%20dual-encoder%0Aframework.%20To%20bridge%20the%20gap%20between%20these%20two%20modalities%2C%20we%20propose%20several%0Apretraining%20objectives%20using%20cross-%20and%20in-modality%20geometric%20constraints%0Awithin%20the%20embedding%20space%20and%20incorporating%20masked%20pose%20token%20prediction%0Astrategy%20to%20enhance%20representation%20learning.%20Extensive%20experiments%20and%20ablation%0Astudies%20demonstrate%20improvements%20over%20the%20strong%20baselines%2C%20while%0Adata-efficiency%20experiments%20on%20two%20distinct%20operating%20room%20datasets%20further%0Ahighlight%20the%20effectiveness%20of%20our%20approach.%20We%20highlight%20the%20benefits%20of%20our%0Aapproach%20for%20surgical%20activity%20recognition%20in%20both%20multi-view%20and%20single-view%0Asettings%2C%20showcasing%20its%20practical%20applicability%20in%20complex%20surgical%0Aenvironments.%20Code%20will%20be%20made%20available%20at%3A%0Ahttps%3A//github.com/CAMMA-public/PreViPS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13883v1&entry.124074799=Read"},
{"title": "LaVCa: LLM-assisted Visual Cortex Captioning", "author": "Takuya Matsuyama and Shinji Nishimoto and Yu Takagi", "abstract": "  Understanding the property of neural populations (or voxels) in the human\nbrain can advance our comprehension of human perceptual and cognitive\nprocessing capabilities and contribute to developing brain-inspired computer\nmodels. Recent encoding models using deep neural networks (DNNs) have\nsuccessfully predicted voxel-wise activity. However, interpreting the\nproperties that explain voxel responses remains challenging because of the\nblack-box nature of DNNs. As a solution, we propose LLM-assisted Visual Cortex\nCaptioning (LaVCa), a data-driven approach that uses large language models\n(LLMs) to generate natural-language captions for images to which voxels are\nselective. By applying LaVCa for image-evoked brain activity, we demonstrate\nthat LaVCa generates captions that describe voxel selectivity more accurately\nthan the previously proposed method. Furthermore, the captions generated by\nLaVCa quantitatively capture more detailed properties than the existing method\nat both the inter-voxel and intra-voxel levels. Furthermore, a more detailed\nanalysis of the voxel-specific properties generated by LaVCa reveals\nfine-grained functional differentiation within regions of interest (ROIs) in\nthe visual cortex and voxels that simultaneously represent multiple distinct\nconcepts. These findings offer profound insights into human visual\nrepresentations by assigning detailed captions throughout the visual cortex\nwhile highlighting the potential of LLM-based methods in understanding brain\nrepresentations. Please check out our webpage at\nhttps://sites.google.com/view/lavca-llm/\n", "link": "http://arxiv.org/abs/2502.13606v1", "date": "2025-02-19", "relevancy": 2.8164, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5775}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5775}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaVCa%3A%20LLM-assisted%20Visual%20Cortex%20Captioning&body=Title%3A%20LaVCa%3A%20LLM-assisted%20Visual%20Cortex%20Captioning%0AAuthor%3A%20Takuya%20Matsuyama%20and%20Shinji%20Nishimoto%20and%20Yu%20Takagi%0AAbstract%3A%20%20%20Understanding%20the%20property%20of%20neural%20populations%20%28or%20voxels%29%20in%20the%20human%0Abrain%20can%20advance%20our%20comprehension%20of%20human%20perceptual%20and%20cognitive%0Aprocessing%20capabilities%20and%20contribute%20to%20developing%20brain-inspired%20computer%0Amodels.%20Recent%20encoding%20models%20using%20deep%20neural%20networks%20%28DNNs%29%20have%0Asuccessfully%20predicted%20voxel-wise%20activity.%20However%2C%20interpreting%20the%0Aproperties%20that%20explain%20voxel%20responses%20remains%20challenging%20because%20of%20the%0Ablack-box%20nature%20of%20DNNs.%20As%20a%20solution%2C%20we%20propose%20LLM-assisted%20Visual%20Cortex%0ACaptioning%20%28LaVCa%29%2C%20a%20data-driven%20approach%20that%20uses%20large%20language%20models%0A%28LLMs%29%20to%20generate%20natural-language%20captions%20for%20images%20to%20which%20voxels%20are%0Aselective.%20By%20applying%20LaVCa%20for%20image-evoked%20brain%20activity%2C%20we%20demonstrate%0Athat%20LaVCa%20generates%20captions%20that%20describe%20voxel%20selectivity%20more%20accurately%0Athan%20the%20previously%20proposed%20method.%20Furthermore%2C%20the%20captions%20generated%20by%0ALaVCa%20quantitatively%20capture%20more%20detailed%20properties%20than%20the%20existing%20method%0Aat%20both%20the%20inter-voxel%20and%20intra-voxel%20levels.%20Furthermore%2C%20a%20more%20detailed%0Aanalysis%20of%20the%20voxel-specific%20properties%20generated%20by%20LaVCa%20reveals%0Afine-grained%20functional%20differentiation%20within%20regions%20of%20interest%20%28ROIs%29%20in%0Athe%20visual%20cortex%20and%20voxels%20that%20simultaneously%20represent%20multiple%20distinct%0Aconcepts.%20These%20findings%20offer%20profound%20insights%20into%20human%20visual%0Arepresentations%20by%20assigning%20detailed%20captions%20throughout%20the%20visual%20cortex%0Awhile%20highlighting%20the%20potential%20of%20LLM-based%20methods%20in%20understanding%20brain%0Arepresentations.%20Please%20check%20out%20our%20webpage%20at%0Ahttps%3A//sites.google.com/view/lavca-llm/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaVCa%253A%2520LLM-assisted%2520Visual%2520Cortex%2520Captioning%26entry.906535625%3DTakuya%2520Matsuyama%2520and%2520Shinji%2520Nishimoto%2520and%2520Yu%2520Takagi%26entry.1292438233%3D%2520%2520Understanding%2520the%2520property%2520of%2520neural%2520populations%2520%2528or%2520voxels%2529%2520in%2520the%2520human%250Abrain%2520can%2520advance%2520our%2520comprehension%2520of%2520human%2520perceptual%2520and%2520cognitive%250Aprocessing%2520capabilities%2520and%2520contribute%2520to%2520developing%2520brain-inspired%2520computer%250Amodels.%2520Recent%2520encoding%2520models%2520using%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520have%250Asuccessfully%2520predicted%2520voxel-wise%2520activity.%2520However%252C%2520interpreting%2520the%250Aproperties%2520that%2520explain%2520voxel%2520responses%2520remains%2520challenging%2520because%2520of%2520the%250Ablack-box%2520nature%2520of%2520DNNs.%2520As%2520a%2520solution%252C%2520we%2520propose%2520LLM-assisted%2520Visual%2520Cortex%250ACaptioning%2520%2528LaVCa%2529%252C%2520a%2520data-driven%2520approach%2520that%2520uses%2520large%2520language%2520models%250A%2528LLMs%2529%2520to%2520generate%2520natural-language%2520captions%2520for%2520images%2520to%2520which%2520voxels%2520are%250Aselective.%2520By%2520applying%2520LaVCa%2520for%2520image-evoked%2520brain%2520activity%252C%2520we%2520demonstrate%250Athat%2520LaVCa%2520generates%2520captions%2520that%2520describe%2520voxel%2520selectivity%2520more%2520accurately%250Athan%2520the%2520previously%2520proposed%2520method.%2520Furthermore%252C%2520the%2520captions%2520generated%2520by%250ALaVCa%2520quantitatively%2520capture%2520more%2520detailed%2520properties%2520than%2520the%2520existing%2520method%250Aat%2520both%2520the%2520inter-voxel%2520and%2520intra-voxel%2520levels.%2520Furthermore%252C%2520a%2520more%2520detailed%250Aanalysis%2520of%2520the%2520voxel-specific%2520properties%2520generated%2520by%2520LaVCa%2520reveals%250Afine-grained%2520functional%2520differentiation%2520within%2520regions%2520of%2520interest%2520%2528ROIs%2529%2520in%250Athe%2520visual%2520cortex%2520and%2520voxels%2520that%2520simultaneously%2520represent%2520multiple%2520distinct%250Aconcepts.%2520These%2520findings%2520offer%2520profound%2520insights%2520into%2520human%2520visual%250Arepresentations%2520by%2520assigning%2520detailed%2520captions%2520throughout%2520the%2520visual%2520cortex%250Awhile%2520highlighting%2520the%2520potential%2520of%2520LLM-based%2520methods%2520in%2520understanding%2520brain%250Arepresentations.%2520Please%2520check%2520out%2520our%2520webpage%2520at%250Ahttps%253A//sites.google.com/view/lavca-llm/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaVCa%3A%20LLM-assisted%20Visual%20Cortex%20Captioning&entry.906535625=Takuya%20Matsuyama%20and%20Shinji%20Nishimoto%20and%20Yu%20Takagi&entry.1292438233=%20%20Understanding%20the%20property%20of%20neural%20populations%20%28or%20voxels%29%20in%20the%20human%0Abrain%20can%20advance%20our%20comprehension%20of%20human%20perceptual%20and%20cognitive%0Aprocessing%20capabilities%20and%20contribute%20to%20developing%20brain-inspired%20computer%0Amodels.%20Recent%20encoding%20models%20using%20deep%20neural%20networks%20%28DNNs%29%20have%0Asuccessfully%20predicted%20voxel-wise%20activity.%20However%2C%20interpreting%20the%0Aproperties%20that%20explain%20voxel%20responses%20remains%20challenging%20because%20of%20the%0Ablack-box%20nature%20of%20DNNs.%20As%20a%20solution%2C%20we%20propose%20LLM-assisted%20Visual%20Cortex%0ACaptioning%20%28LaVCa%29%2C%20a%20data-driven%20approach%20that%20uses%20large%20language%20models%0A%28LLMs%29%20to%20generate%20natural-language%20captions%20for%20images%20to%20which%20voxels%20are%0Aselective.%20By%20applying%20LaVCa%20for%20image-evoked%20brain%20activity%2C%20we%20demonstrate%0Athat%20LaVCa%20generates%20captions%20that%20describe%20voxel%20selectivity%20more%20accurately%0Athan%20the%20previously%20proposed%20method.%20Furthermore%2C%20the%20captions%20generated%20by%0ALaVCa%20quantitatively%20capture%20more%20detailed%20properties%20than%20the%20existing%20method%0Aat%20both%20the%20inter-voxel%20and%20intra-voxel%20levels.%20Furthermore%2C%20a%20more%20detailed%0Aanalysis%20of%20the%20voxel-specific%20properties%20generated%20by%20LaVCa%20reveals%0Afine-grained%20functional%20differentiation%20within%20regions%20of%20interest%20%28ROIs%29%20in%0Athe%20visual%20cortex%20and%20voxels%20that%20simultaneously%20represent%20multiple%20distinct%0Aconcepts.%20These%20findings%20offer%20profound%20insights%20into%20human%20visual%0Arepresentations%20by%20assigning%20detailed%20captions%20throughout%20the%20visual%20cortex%0Awhile%20highlighting%20the%20potential%20of%20LLM-based%20methods%20in%20understanding%20brain%0Arepresentations.%20Please%20check%20out%20our%20webpage%20at%0Ahttps%3A//sites.google.com/view/lavca-llm/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13606v1&entry.124074799=Read"},
{"title": "ChineseSimpleVQA -- \"See the World, Discover Knowledge\": A Chinese\n  Factuality Evaluation for Large Vision Language Models", "author": "Jihao Gu and Yingyao Wang and Pi Bu and Chen Wang and Ziming Wang and Tengtao Song and Donglai Wei and Jiale Yuan and Yingxiu Zhao and Yancheng He and Shilong Li and Jiaheng Liu and Meng Cao and Jun Song and Yingshui Tan and Xiang Li and Wenbo Su and Zhicheng Zheng and Xiaoyong Zhu and Bo Zheng", "abstract": "  The evaluation of factual accuracy in large vision language models (LVLMs)\nhas lagged behind their rapid development, making it challenging to fully\nreflect these models' knowledge capacity and reliability. In this paper, we\nintroduce the first factuality-based visual question-answering benchmark in\nChinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of\nLVLMs across 8 major topics and 56 subtopics. The key features of this\nbenchmark include a focus on the Chinese language, diverse knowledge types, a\nmulti-hop question construction, high-quality data, static consistency, and\neasy-to-evaluate through short answers. Moreover, we contribute a rigorous data\nconstruction pipeline and decouple the visual factuality into two parts: seeing\nthe world (i.e., object recognition) and discovering knowledge. This decoupling\nallows us to analyze the capability boundaries and execution mechanisms of\nLVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source\nmodels, revealing critical performance gaps within this field.\n", "link": "http://arxiv.org/abs/2502.11718v2", "date": "2025-02-19", "relevancy": 2.8077, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5798}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5798}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChineseSimpleVQA%20--%20%22See%20the%20World%2C%20Discover%20Knowledge%22%3A%20A%20Chinese%0A%20%20Factuality%20Evaluation%20for%20Large%20Vision%20Language%20Models&body=Title%3A%20ChineseSimpleVQA%20--%20%22See%20the%20World%2C%20Discover%20Knowledge%22%3A%20A%20Chinese%0A%20%20Factuality%20Evaluation%20for%20Large%20Vision%20Language%20Models%0AAuthor%3A%20Jihao%20Gu%20and%20Yingyao%20Wang%20and%20Pi%20Bu%20and%20Chen%20Wang%20and%20Ziming%20Wang%20and%20Tengtao%20Song%20and%20Donglai%20Wei%20and%20Jiale%20Yuan%20and%20Yingxiu%20Zhao%20and%20Yancheng%20He%20and%20Shilong%20Li%20and%20Jiaheng%20Liu%20and%20Meng%20Cao%20and%20Jun%20Song%20and%20Yingshui%20Tan%20and%20Xiang%20Li%20and%20Wenbo%20Su%20and%20Zhicheng%20Zheng%20and%20Xiaoyong%20Zhu%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20The%20evaluation%20of%20factual%20accuracy%20in%20large%20vision%20language%20models%20%28LVLMs%29%0Ahas%20lagged%20behind%20their%20rapid%20development%2C%20making%20it%20challenging%20to%20fully%0Areflect%20these%20models%27%20knowledge%20capacity%20and%20reliability.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20first%20factuality-based%20visual%20question-answering%20benchmark%20in%0AChinese%2C%20named%20ChineseSimpleVQA%2C%20aimed%20at%20assessing%20the%20visual%20factuality%20of%0ALVLMs%20across%208%20major%20topics%20and%2056%20subtopics.%20The%20key%20features%20of%20this%0Abenchmark%20include%20a%20focus%20on%20the%20Chinese%20language%2C%20diverse%20knowledge%20types%2C%20a%0Amulti-hop%20question%20construction%2C%20high-quality%20data%2C%20static%20consistency%2C%20and%0Aeasy-to-evaluate%20through%20short%20answers.%20Moreover%2C%20we%20contribute%20a%20rigorous%20data%0Aconstruction%20pipeline%20and%20decouple%20the%20visual%20factuality%20into%20two%20parts%3A%20seeing%0Athe%20world%20%28i.e.%2C%20object%20recognition%29%20and%20discovering%20knowledge.%20This%20decoupling%0Aallows%20us%20to%20analyze%20the%20capability%20boundaries%20and%20execution%20mechanisms%20of%0ALVLMs.%20Subsequently%2C%20we%20evaluate%2034%20advanced%20open-source%20and%20closed-source%0Amodels%2C%20revealing%20critical%20performance%20gaps%20within%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11718v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChineseSimpleVQA%2520--%2520%2522See%2520the%2520World%252C%2520Discover%2520Knowledge%2522%253A%2520A%2520Chinese%250A%2520%2520Factuality%2520Evaluation%2520for%2520Large%2520Vision%2520Language%2520Models%26entry.906535625%3DJihao%2520Gu%2520and%2520Yingyao%2520Wang%2520and%2520Pi%2520Bu%2520and%2520Chen%2520Wang%2520and%2520Ziming%2520Wang%2520and%2520Tengtao%2520Song%2520and%2520Donglai%2520Wei%2520and%2520Jiale%2520Yuan%2520and%2520Yingxiu%2520Zhao%2520and%2520Yancheng%2520He%2520and%2520Shilong%2520Li%2520and%2520Jiaheng%2520Liu%2520and%2520Meng%2520Cao%2520and%2520Jun%2520Song%2520and%2520Yingshui%2520Tan%2520and%2520Xiang%2520Li%2520and%2520Wenbo%2520Su%2520and%2520Zhicheng%2520Zheng%2520and%2520Xiaoyong%2520Zhu%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520The%2520evaluation%2520of%2520factual%2520accuracy%2520in%2520large%2520vision%2520language%2520models%2520%2528LVLMs%2529%250Ahas%2520lagged%2520behind%2520their%2520rapid%2520development%252C%2520making%2520it%2520challenging%2520to%2520fully%250Areflect%2520these%2520models%2527%2520knowledge%2520capacity%2520and%2520reliability.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520the%2520first%2520factuality-based%2520visual%2520question-answering%2520benchmark%2520in%250AChinese%252C%2520named%2520ChineseSimpleVQA%252C%2520aimed%2520at%2520assessing%2520the%2520visual%2520factuality%2520of%250ALVLMs%2520across%25208%2520major%2520topics%2520and%252056%2520subtopics.%2520The%2520key%2520features%2520of%2520this%250Abenchmark%2520include%2520a%2520focus%2520on%2520the%2520Chinese%2520language%252C%2520diverse%2520knowledge%2520types%252C%2520a%250Amulti-hop%2520question%2520construction%252C%2520high-quality%2520data%252C%2520static%2520consistency%252C%2520and%250Aeasy-to-evaluate%2520through%2520short%2520answers.%2520Moreover%252C%2520we%2520contribute%2520a%2520rigorous%2520data%250Aconstruction%2520pipeline%2520and%2520decouple%2520the%2520visual%2520factuality%2520into%2520two%2520parts%253A%2520seeing%250Athe%2520world%2520%2528i.e.%252C%2520object%2520recognition%2529%2520and%2520discovering%2520knowledge.%2520This%2520decoupling%250Aallows%2520us%2520to%2520analyze%2520the%2520capability%2520boundaries%2520and%2520execution%2520mechanisms%2520of%250ALVLMs.%2520Subsequently%252C%2520we%2520evaluate%252034%2520advanced%2520open-source%2520and%2520closed-source%250Amodels%252C%2520revealing%2520critical%2520performance%2520gaps%2520within%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11718v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChineseSimpleVQA%20--%20%22See%20the%20World%2C%20Discover%20Knowledge%22%3A%20A%20Chinese%0A%20%20Factuality%20Evaluation%20for%20Large%20Vision%20Language%20Models&entry.906535625=Jihao%20Gu%20and%20Yingyao%20Wang%20and%20Pi%20Bu%20and%20Chen%20Wang%20and%20Ziming%20Wang%20and%20Tengtao%20Song%20and%20Donglai%20Wei%20and%20Jiale%20Yuan%20and%20Yingxiu%20Zhao%20and%20Yancheng%20He%20and%20Shilong%20Li%20and%20Jiaheng%20Liu%20and%20Meng%20Cao%20and%20Jun%20Song%20and%20Yingshui%20Tan%20and%20Xiang%20Li%20and%20Wenbo%20Su%20and%20Zhicheng%20Zheng%20and%20Xiaoyong%20Zhu%20and%20Bo%20Zheng&entry.1292438233=%20%20The%20evaluation%20of%20factual%20accuracy%20in%20large%20vision%20language%20models%20%28LVLMs%29%0Ahas%20lagged%20behind%20their%20rapid%20development%2C%20making%20it%20challenging%20to%20fully%0Areflect%20these%20models%27%20knowledge%20capacity%20and%20reliability.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20first%20factuality-based%20visual%20question-answering%20benchmark%20in%0AChinese%2C%20named%20ChineseSimpleVQA%2C%20aimed%20at%20assessing%20the%20visual%20factuality%20of%0ALVLMs%20across%208%20major%20topics%20and%2056%20subtopics.%20The%20key%20features%20of%20this%0Abenchmark%20include%20a%20focus%20on%20the%20Chinese%20language%2C%20diverse%20knowledge%20types%2C%20a%0Amulti-hop%20question%20construction%2C%20high-quality%20data%2C%20static%20consistency%2C%20and%0Aeasy-to-evaluate%20through%20short%20answers.%20Moreover%2C%20we%20contribute%20a%20rigorous%20data%0Aconstruction%20pipeline%20and%20decouple%20the%20visual%20factuality%20into%20two%20parts%3A%20seeing%0Athe%20world%20%28i.e.%2C%20object%20recognition%29%20and%20discovering%20knowledge.%20This%20decoupling%0Aallows%20us%20to%20analyze%20the%20capability%20boundaries%20and%20execution%20mechanisms%20of%0ALVLMs.%20Subsequently%2C%20we%20evaluate%2034%20advanced%20open-source%20and%20closed-source%0Amodels%2C%20revealing%20critical%20performance%20gaps%20within%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11718v2&entry.124074799=Read"},
{"title": "A Framework for Building Point Cloud Cleaning, Plane Detection and\n  Semantic Segmentation", "author": "Ilyass Abouelaziz and Youssef Mourchid", "abstract": "  This paper presents a framework to address the challenges involved in\nbuilding point cloud cleaning, plane detection, and semantic segmentation, with\nthe ultimate goal of enhancing building modeling. We focus in the cleaning\nstage on removing outliers from the acquired point cloud data by employing an\nadaptive threshold technique based on z-score measure. Following the cleaning\nprocess, we perform plane detection using the robust RANSAC paradigm. The goal\nis to carry out multiple plane segmentations, and to classify segments into\ndistinct categories, such as floors, ceilings, and walls. The resulting\nsegments can generate accurate and detailed point clouds representing the\nbuilding's architectural elements. Moreover, we address the problem of semantic\nsegmentation, which plays a vital role in the identification and classification\nof different components within the building, such as walls, windows, doors,\nroofs, and objects. Inspired by the PointNet architecture, we propose a deep\nlearning architecture for efficient semantic segmentation in buildings. The\nresults demonstrate the effectiveness of the proposed framework in handling\nbuilding modeling tasks, paving the way for improved accuracy and efficiency in\nthe field of building modelization.\n", "link": "http://arxiv.org/abs/2402.00692v2", "date": "2025-02-19", "relevancy": 2.726, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5483}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5436}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Framework%20for%20Building%20Point%20Cloud%20Cleaning%2C%20Plane%20Detection%20and%0A%20%20Semantic%20Segmentation&body=Title%3A%20A%20Framework%20for%20Building%20Point%20Cloud%20Cleaning%2C%20Plane%20Detection%20and%0A%20%20Semantic%20Segmentation%0AAuthor%3A%20Ilyass%20Abouelaziz%20and%20Youssef%20Mourchid%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20framework%20to%20address%20the%20challenges%20involved%20in%0Abuilding%20point%20cloud%20cleaning%2C%20plane%20detection%2C%20and%20semantic%20segmentation%2C%20with%0Athe%20ultimate%20goal%20of%20enhancing%20building%20modeling.%20We%20focus%20in%20the%20cleaning%0Astage%20on%20removing%20outliers%20from%20the%20acquired%20point%20cloud%20data%20by%20employing%20an%0Aadaptive%20threshold%20technique%20based%20on%20z-score%20measure.%20Following%20the%20cleaning%0Aprocess%2C%20we%20perform%20plane%20detection%20using%20the%20robust%20RANSAC%20paradigm.%20The%20goal%0Ais%20to%20carry%20out%20multiple%20plane%20segmentations%2C%20and%20to%20classify%20segments%20into%0Adistinct%20categories%2C%20such%20as%20floors%2C%20ceilings%2C%20and%20walls.%20The%20resulting%0Asegments%20can%20generate%20accurate%20and%20detailed%20point%20clouds%20representing%20the%0Abuilding%27s%20architectural%20elements.%20Moreover%2C%20we%20address%20the%20problem%20of%20semantic%0Asegmentation%2C%20which%20plays%20a%20vital%20role%20in%20the%20identification%20and%20classification%0Aof%20different%20components%20within%20the%20building%2C%20such%20as%20walls%2C%20windows%2C%20doors%2C%0Aroofs%2C%20and%20objects.%20Inspired%20by%20the%20PointNet%20architecture%2C%20we%20propose%20a%20deep%0Alearning%20architecture%20for%20efficient%20semantic%20segmentation%20in%20buildings.%20The%0Aresults%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20framework%20in%20handling%0Abuilding%20modeling%20tasks%2C%20paving%20the%20way%20for%20improved%20accuracy%20and%20efficiency%20in%0Athe%20field%20of%20building%20modelization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00692v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Framework%2520for%2520Building%2520Point%2520Cloud%2520Cleaning%252C%2520Plane%2520Detection%2520and%250A%2520%2520Semantic%2520Segmentation%26entry.906535625%3DIlyass%2520Abouelaziz%2520and%2520Youssef%2520Mourchid%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520framework%2520to%2520address%2520the%2520challenges%2520involved%2520in%250Abuilding%2520point%2520cloud%2520cleaning%252C%2520plane%2520detection%252C%2520and%2520semantic%2520segmentation%252C%2520with%250Athe%2520ultimate%2520goal%2520of%2520enhancing%2520building%2520modeling.%2520We%2520focus%2520in%2520the%2520cleaning%250Astage%2520on%2520removing%2520outliers%2520from%2520the%2520acquired%2520point%2520cloud%2520data%2520by%2520employing%2520an%250Aadaptive%2520threshold%2520technique%2520based%2520on%2520z-score%2520measure.%2520Following%2520the%2520cleaning%250Aprocess%252C%2520we%2520perform%2520plane%2520detection%2520using%2520the%2520robust%2520RANSAC%2520paradigm.%2520The%2520goal%250Ais%2520to%2520carry%2520out%2520multiple%2520plane%2520segmentations%252C%2520and%2520to%2520classify%2520segments%2520into%250Adistinct%2520categories%252C%2520such%2520as%2520floors%252C%2520ceilings%252C%2520and%2520walls.%2520The%2520resulting%250Asegments%2520can%2520generate%2520accurate%2520and%2520detailed%2520point%2520clouds%2520representing%2520the%250Abuilding%2527s%2520architectural%2520elements.%2520Moreover%252C%2520we%2520address%2520the%2520problem%2520of%2520semantic%250Asegmentation%252C%2520which%2520plays%2520a%2520vital%2520role%2520in%2520the%2520identification%2520and%2520classification%250Aof%2520different%2520components%2520within%2520the%2520building%252C%2520such%2520as%2520walls%252C%2520windows%252C%2520doors%252C%250Aroofs%252C%2520and%2520objects.%2520Inspired%2520by%2520the%2520PointNet%2520architecture%252C%2520we%2520propose%2520a%2520deep%250Alearning%2520architecture%2520for%2520efficient%2520semantic%2520segmentation%2520in%2520buildings.%2520The%250Aresults%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520framework%2520in%2520handling%250Abuilding%2520modeling%2520tasks%252C%2520paving%2520the%2520way%2520for%2520improved%2520accuracy%2520and%2520efficiency%2520in%250Athe%2520field%2520of%2520building%2520modelization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00692v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Framework%20for%20Building%20Point%20Cloud%20Cleaning%2C%20Plane%20Detection%20and%0A%20%20Semantic%20Segmentation&entry.906535625=Ilyass%20Abouelaziz%20and%20Youssef%20Mourchid&entry.1292438233=%20%20This%20paper%20presents%20a%20framework%20to%20address%20the%20challenges%20involved%20in%0Abuilding%20point%20cloud%20cleaning%2C%20plane%20detection%2C%20and%20semantic%20segmentation%2C%20with%0Athe%20ultimate%20goal%20of%20enhancing%20building%20modeling.%20We%20focus%20in%20the%20cleaning%0Astage%20on%20removing%20outliers%20from%20the%20acquired%20point%20cloud%20data%20by%20employing%20an%0Aadaptive%20threshold%20technique%20based%20on%20z-score%20measure.%20Following%20the%20cleaning%0Aprocess%2C%20we%20perform%20plane%20detection%20using%20the%20robust%20RANSAC%20paradigm.%20The%20goal%0Ais%20to%20carry%20out%20multiple%20plane%20segmentations%2C%20and%20to%20classify%20segments%20into%0Adistinct%20categories%2C%20such%20as%20floors%2C%20ceilings%2C%20and%20walls.%20The%20resulting%0Asegments%20can%20generate%20accurate%20and%20detailed%20point%20clouds%20representing%20the%0Abuilding%27s%20architectural%20elements.%20Moreover%2C%20we%20address%20the%20problem%20of%20semantic%0Asegmentation%2C%20which%20plays%20a%20vital%20role%20in%20the%20identification%20and%20classification%0Aof%20different%20components%20within%20the%20building%2C%20such%20as%20walls%2C%20windows%2C%20doors%2C%0Aroofs%2C%20and%20objects.%20Inspired%20by%20the%20PointNet%20architecture%2C%20we%20propose%20a%20deep%0Alearning%20architecture%20for%20efficient%20semantic%20segmentation%20in%20buildings.%20The%0Aresults%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20framework%20in%20handling%0Abuilding%20modeling%20tasks%2C%20paving%20the%20way%20for%20improved%20accuracy%20and%20efficiency%20in%0Athe%20field%20of%20building%20modelization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00692v2&entry.124074799=Read"},
{"title": "Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and\n  Human-Like Reasoning Framework", "author": "Zirui Song and Jingpu Yang and Yuan Huang and Jonathan Tonglet and Zeyu Zhang and Tao Cheng and Meng Fang and Iryna Gurevych and Xiuying Chen", "abstract": "  Geolocation, the task of identifying an image's location, requires complex\nreasoning and is crucial for navigation, monitoring, and cultural preservation.\nHowever, current methods often produce coarse, imprecise, and non-interpretable\nlocalization. A major challenge lies in the quality and scale of existing\ngeolocation datasets. These datasets are typically small-scale and\nautomatically constructed, leading to noisy data and inconsistent task\ndifficulty, with images that either reveal answers too easily or lack\nsufficient clues for reliable inference. To address these challenges, we\nintroduce a comprehensive geolocation framework with three key components:\nGeoComp, a large-scale dataset; GeoCoT, a novel reasoning method; and GeoEval,\nan evaluation metric, collectively designed to address critical challenges and\ndrive advancements in geolocation research. At the core of this framework is\nGeoComp (Geolocation Competition Dataset), a large-scale dataset collected from\na geolocation game platform involving 740K users over two years. It comprises\n25 million entries of metadata and 3 million geo-tagged locations spanning much\nof the globe, with each location annotated thousands to tens of thousands of\ntimes by human users. The dataset offers diverse difficulty levels for detailed\nanalysis and highlights key gaps in current models. Building on this dataset,\nwe propose Geographical Chain-of-Thought (GeoCoT), a novel multi-step reasoning\nframework designed to enhance the reasoning capabilities of Large Vision Models\n(LVMs) in geolocation tasks. GeoCoT improves performance by integrating\ncontextual and spatial cues through a multi-step process that mimics human\ngeolocation reasoning. Finally, using the GeoEval metric, we demonstrate that\nGeoCoT significantly boosts geolocation accuracy by up to 25% while enhancing\ninterpretability.\n", "link": "http://arxiv.org/abs/2502.13759v1", "date": "2025-02-19", "relevancy": 2.717, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5346}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geolocation%20with%20Real%20Human%20Gameplay%20Data%3A%20A%20Large-Scale%20Dataset%20and%0A%20%20Human-Like%20Reasoning%20Framework&body=Title%3A%20Geolocation%20with%20Real%20Human%20Gameplay%20Data%3A%20A%20Large-Scale%20Dataset%20and%0A%20%20Human-Like%20Reasoning%20Framework%0AAuthor%3A%20Zirui%20Song%20and%20Jingpu%20Yang%20and%20Yuan%20Huang%20and%20Jonathan%20Tonglet%20and%20Zeyu%20Zhang%20and%20Tao%20Cheng%20and%20Meng%20Fang%20and%20Iryna%20Gurevych%20and%20Xiuying%20Chen%0AAbstract%3A%20%20%20Geolocation%2C%20the%20task%20of%20identifying%20an%20image%27s%20location%2C%20requires%20complex%0Areasoning%20and%20is%20crucial%20for%20navigation%2C%20monitoring%2C%20and%20cultural%20preservation.%0AHowever%2C%20current%20methods%20often%20produce%20coarse%2C%20imprecise%2C%20and%20non-interpretable%0Alocalization.%20A%20major%20challenge%20lies%20in%20the%20quality%20and%20scale%20of%20existing%0Ageolocation%20datasets.%20These%20datasets%20are%20typically%20small-scale%20and%0Aautomatically%20constructed%2C%20leading%20to%20noisy%20data%20and%20inconsistent%20task%0Adifficulty%2C%20with%20images%20that%20either%20reveal%20answers%20too%20easily%20or%20lack%0Asufficient%20clues%20for%20reliable%20inference.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20a%20comprehensive%20geolocation%20framework%20with%20three%20key%20components%3A%0AGeoComp%2C%20a%20large-scale%20dataset%3B%20GeoCoT%2C%20a%20novel%20reasoning%20method%3B%20and%20GeoEval%2C%0Aan%20evaluation%20metric%2C%20collectively%20designed%20to%20address%20critical%20challenges%20and%0Adrive%20advancements%20in%20geolocation%20research.%20At%20the%20core%20of%20this%20framework%20is%0AGeoComp%20%28Geolocation%20Competition%20Dataset%29%2C%20a%20large-scale%20dataset%20collected%20from%0Aa%20geolocation%20game%20platform%20involving%20740K%20users%20over%20two%20years.%20It%20comprises%0A25%20million%20entries%20of%20metadata%20and%203%20million%20geo-tagged%20locations%20spanning%20much%0Aof%20the%20globe%2C%20with%20each%20location%20annotated%20thousands%20to%20tens%20of%20thousands%20of%0Atimes%20by%20human%20users.%20The%20dataset%20offers%20diverse%20difficulty%20levels%20for%20detailed%0Aanalysis%20and%20highlights%20key%20gaps%20in%20current%20models.%20Building%20on%20this%20dataset%2C%0Awe%20propose%20Geographical%20Chain-of-Thought%20%28GeoCoT%29%2C%20a%20novel%20multi-step%20reasoning%0Aframework%20designed%20to%20enhance%20the%20reasoning%20capabilities%20of%20Large%20Vision%20Models%0A%28LVMs%29%20in%20geolocation%20tasks.%20GeoCoT%20improves%20performance%20by%20integrating%0Acontextual%20and%20spatial%20cues%20through%20a%20multi-step%20process%20that%20mimics%20human%0Ageolocation%20reasoning.%20Finally%2C%20using%20the%20GeoEval%20metric%2C%20we%20demonstrate%20that%0AGeoCoT%20significantly%20boosts%20geolocation%20accuracy%20by%20up%20to%2025%25%20while%20enhancing%0Ainterpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeolocation%2520with%2520Real%2520Human%2520Gameplay%2520Data%253A%2520A%2520Large-Scale%2520Dataset%2520and%250A%2520%2520Human-Like%2520Reasoning%2520Framework%26entry.906535625%3DZirui%2520Song%2520and%2520Jingpu%2520Yang%2520and%2520Yuan%2520Huang%2520and%2520Jonathan%2520Tonglet%2520and%2520Zeyu%2520Zhang%2520and%2520Tao%2520Cheng%2520and%2520Meng%2520Fang%2520and%2520Iryna%2520Gurevych%2520and%2520Xiuying%2520Chen%26entry.1292438233%3D%2520%2520Geolocation%252C%2520the%2520task%2520of%2520identifying%2520an%2520image%2527s%2520location%252C%2520requires%2520complex%250Areasoning%2520and%2520is%2520crucial%2520for%2520navigation%252C%2520monitoring%252C%2520and%2520cultural%2520preservation.%250AHowever%252C%2520current%2520methods%2520often%2520produce%2520coarse%252C%2520imprecise%252C%2520and%2520non-interpretable%250Alocalization.%2520A%2520major%2520challenge%2520lies%2520in%2520the%2520quality%2520and%2520scale%2520of%2520existing%250Ageolocation%2520datasets.%2520These%2520datasets%2520are%2520typically%2520small-scale%2520and%250Aautomatically%2520constructed%252C%2520leading%2520to%2520noisy%2520data%2520and%2520inconsistent%2520task%250Adifficulty%252C%2520with%2520images%2520that%2520either%2520reveal%2520answers%2520too%2520easily%2520or%2520lack%250Asufficient%2520clues%2520for%2520reliable%2520inference.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520a%2520comprehensive%2520geolocation%2520framework%2520with%2520three%2520key%2520components%253A%250AGeoComp%252C%2520a%2520large-scale%2520dataset%253B%2520GeoCoT%252C%2520a%2520novel%2520reasoning%2520method%253B%2520and%2520GeoEval%252C%250Aan%2520evaluation%2520metric%252C%2520collectively%2520designed%2520to%2520address%2520critical%2520challenges%2520and%250Adrive%2520advancements%2520in%2520geolocation%2520research.%2520At%2520the%2520core%2520of%2520this%2520framework%2520is%250AGeoComp%2520%2528Geolocation%2520Competition%2520Dataset%2529%252C%2520a%2520large-scale%2520dataset%2520collected%2520from%250Aa%2520geolocation%2520game%2520platform%2520involving%2520740K%2520users%2520over%2520two%2520years.%2520It%2520comprises%250A25%2520million%2520entries%2520of%2520metadata%2520and%25203%2520million%2520geo-tagged%2520locations%2520spanning%2520much%250Aof%2520the%2520globe%252C%2520with%2520each%2520location%2520annotated%2520thousands%2520to%2520tens%2520of%2520thousands%2520of%250Atimes%2520by%2520human%2520users.%2520The%2520dataset%2520offers%2520diverse%2520difficulty%2520levels%2520for%2520detailed%250Aanalysis%2520and%2520highlights%2520key%2520gaps%2520in%2520current%2520models.%2520Building%2520on%2520this%2520dataset%252C%250Awe%2520propose%2520Geographical%2520Chain-of-Thought%2520%2528GeoCoT%2529%252C%2520a%2520novel%2520multi-step%2520reasoning%250Aframework%2520designed%2520to%2520enhance%2520the%2520reasoning%2520capabilities%2520of%2520Large%2520Vision%2520Models%250A%2528LVMs%2529%2520in%2520geolocation%2520tasks.%2520GeoCoT%2520improves%2520performance%2520by%2520integrating%250Acontextual%2520and%2520spatial%2520cues%2520through%2520a%2520multi-step%2520process%2520that%2520mimics%2520human%250Ageolocation%2520reasoning.%2520Finally%252C%2520using%2520the%2520GeoEval%2520metric%252C%2520we%2520demonstrate%2520that%250AGeoCoT%2520significantly%2520boosts%2520geolocation%2520accuracy%2520by%2520up%2520to%252025%2525%2520while%2520enhancing%250Ainterpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geolocation%20with%20Real%20Human%20Gameplay%20Data%3A%20A%20Large-Scale%20Dataset%20and%0A%20%20Human-Like%20Reasoning%20Framework&entry.906535625=Zirui%20Song%20and%20Jingpu%20Yang%20and%20Yuan%20Huang%20and%20Jonathan%20Tonglet%20and%20Zeyu%20Zhang%20and%20Tao%20Cheng%20and%20Meng%20Fang%20and%20Iryna%20Gurevych%20and%20Xiuying%20Chen&entry.1292438233=%20%20Geolocation%2C%20the%20task%20of%20identifying%20an%20image%27s%20location%2C%20requires%20complex%0Areasoning%20and%20is%20crucial%20for%20navigation%2C%20monitoring%2C%20and%20cultural%20preservation.%0AHowever%2C%20current%20methods%20often%20produce%20coarse%2C%20imprecise%2C%20and%20non-interpretable%0Alocalization.%20A%20major%20challenge%20lies%20in%20the%20quality%20and%20scale%20of%20existing%0Ageolocation%20datasets.%20These%20datasets%20are%20typically%20small-scale%20and%0Aautomatically%20constructed%2C%20leading%20to%20noisy%20data%20and%20inconsistent%20task%0Adifficulty%2C%20with%20images%20that%20either%20reveal%20answers%20too%20easily%20or%20lack%0Asufficient%20clues%20for%20reliable%20inference.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20a%20comprehensive%20geolocation%20framework%20with%20three%20key%20components%3A%0AGeoComp%2C%20a%20large-scale%20dataset%3B%20GeoCoT%2C%20a%20novel%20reasoning%20method%3B%20and%20GeoEval%2C%0Aan%20evaluation%20metric%2C%20collectively%20designed%20to%20address%20critical%20challenges%20and%0Adrive%20advancements%20in%20geolocation%20research.%20At%20the%20core%20of%20this%20framework%20is%0AGeoComp%20%28Geolocation%20Competition%20Dataset%29%2C%20a%20large-scale%20dataset%20collected%20from%0Aa%20geolocation%20game%20platform%20involving%20740K%20users%20over%20two%20years.%20It%20comprises%0A25%20million%20entries%20of%20metadata%20and%203%20million%20geo-tagged%20locations%20spanning%20much%0Aof%20the%20globe%2C%20with%20each%20location%20annotated%20thousands%20to%20tens%20of%20thousands%20of%0Atimes%20by%20human%20users.%20The%20dataset%20offers%20diverse%20difficulty%20levels%20for%20detailed%0Aanalysis%20and%20highlights%20key%20gaps%20in%20current%20models.%20Building%20on%20this%20dataset%2C%0Awe%20propose%20Geographical%20Chain-of-Thought%20%28GeoCoT%29%2C%20a%20novel%20multi-step%20reasoning%0Aframework%20designed%20to%20enhance%20the%20reasoning%20capabilities%20of%20Large%20Vision%20Models%0A%28LVMs%29%20in%20geolocation%20tasks.%20GeoCoT%20improves%20performance%20by%20integrating%0Acontextual%20and%20spatial%20cues%20through%20a%20multi-step%20process%20that%20mimics%20human%0Ageolocation%20reasoning.%20Finally%2C%20using%20the%20GeoEval%20metric%2C%20we%20demonstrate%20that%0AGeoCoT%20significantly%20boosts%20geolocation%20accuracy%20by%20up%20to%2025%25%20while%20enhancing%0Ainterpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13759v1&entry.124074799=Read"},
{"title": "MSVCOD:A Large-Scale Multi-Scene Dataset for Video Camouflage Object\n  Detection", "author": "Shuyong Gao and Yu'ang Feng and Qishan Wang and Lingyi Hong and Xinyu Zhou and Liu Fei and Yan Wang and Wenqiang Zhang", "abstract": "  Video Camouflaged Object Detection (VCOD) is a challenging task which aims to\nidentify objects that seamlessly concealed within the background in videos. The\ndynamic properties of video enable detection of camouflaged objects through\nmotion cues or varied perspectives. Previous VCOD datasets primarily contain\nanimal objects, limiting the scope of research to wildlife scenarios. However,\nthe applications of VCOD extend beyond wildlife and have significant\nimplications in security, art, and medical fields. Addressing this problem, we\nconstruct a new large-scale multi-domain VCOD dataset MSVCOD. To achieve\nhigh-quality annotations, we design a semi-automatic iterative annotation\npipeline that reduces costs while maintaining annotation accuracy. Our MSVCOD\nis the largest VCOD dataset to date, introducing multiple object categories\nincluding human, animal, medical, and vehicle objects for the first time, while\nalso expanding background diversity across various environments. This expanded\nscope increases the practical applicability of the VCOD task in camouflaged\nobject detection. Alongside this dataset, we introduce a one-steam video\ncamouflage object detection model that performs both feature extraction and\ninformation fusion without additional motion feature fusion modules. Our\nframework achieves state-of-the-art results on the existing VCOD animal dataset\nand the proposed MSVCOD. The dataset and code will be made publicly available.\n", "link": "http://arxiv.org/abs/2502.13859v1", "date": "2025-02-19", "relevancy": 2.7167, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSVCOD%3AA%20Large-Scale%20Multi-Scene%20Dataset%20for%20Video%20Camouflage%20Object%0A%20%20Detection&body=Title%3A%20MSVCOD%3AA%20Large-Scale%20Multi-Scene%20Dataset%20for%20Video%20Camouflage%20Object%0A%20%20Detection%0AAuthor%3A%20Shuyong%20Gao%20and%20Yu%27ang%20Feng%20and%20Qishan%20Wang%20and%20Lingyi%20Hong%20and%20Xinyu%20Zhou%20and%20Liu%20Fei%20and%20Yan%20Wang%20and%20Wenqiang%20Zhang%0AAbstract%3A%20%20%20Video%20Camouflaged%20Object%20Detection%20%28VCOD%29%20is%20a%20challenging%20task%20which%20aims%20to%0Aidentify%20objects%20that%20seamlessly%20concealed%20within%20the%20background%20in%20videos.%20The%0Adynamic%20properties%20of%20video%20enable%20detection%20of%20camouflaged%20objects%20through%0Amotion%20cues%20or%20varied%20perspectives.%20Previous%20VCOD%20datasets%20primarily%20contain%0Aanimal%20objects%2C%20limiting%20the%20scope%20of%20research%20to%20wildlife%20scenarios.%20However%2C%0Athe%20applications%20of%20VCOD%20extend%20beyond%20wildlife%20and%20have%20significant%0Aimplications%20in%20security%2C%20art%2C%20and%20medical%20fields.%20Addressing%20this%20problem%2C%20we%0Aconstruct%20a%20new%20large-scale%20multi-domain%20VCOD%20dataset%20MSVCOD.%20To%20achieve%0Ahigh-quality%20annotations%2C%20we%20design%20a%20semi-automatic%20iterative%20annotation%0Apipeline%20that%20reduces%20costs%20while%20maintaining%20annotation%20accuracy.%20Our%20MSVCOD%0Ais%20the%20largest%20VCOD%20dataset%20to%20date%2C%20introducing%20multiple%20object%20categories%0Aincluding%20human%2C%20animal%2C%20medical%2C%20and%20vehicle%20objects%20for%20the%20first%20time%2C%20while%0Aalso%20expanding%20background%20diversity%20across%20various%20environments.%20This%20expanded%0Ascope%20increases%20the%20practical%20applicability%20of%20the%20VCOD%20task%20in%20camouflaged%0Aobject%20detection.%20Alongside%20this%20dataset%2C%20we%20introduce%20a%20one-steam%20video%0Acamouflage%20object%20detection%20model%20that%20performs%20both%20feature%20extraction%20and%0Ainformation%20fusion%20without%20additional%20motion%20feature%20fusion%20modules.%20Our%0Aframework%20achieves%20state-of-the-art%20results%20on%20the%20existing%20VCOD%20animal%20dataset%0Aand%20the%20proposed%20MSVCOD.%20The%20dataset%20and%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13859v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSVCOD%253AA%2520Large-Scale%2520Multi-Scene%2520Dataset%2520for%2520Video%2520Camouflage%2520Object%250A%2520%2520Detection%26entry.906535625%3DShuyong%2520Gao%2520and%2520Yu%2527ang%2520Feng%2520and%2520Qishan%2520Wang%2520and%2520Lingyi%2520Hong%2520and%2520Xinyu%2520Zhou%2520and%2520Liu%2520Fei%2520and%2520Yan%2520Wang%2520and%2520Wenqiang%2520Zhang%26entry.1292438233%3D%2520%2520Video%2520Camouflaged%2520Object%2520Detection%2520%2528VCOD%2529%2520is%2520a%2520challenging%2520task%2520which%2520aims%2520to%250Aidentify%2520objects%2520that%2520seamlessly%2520concealed%2520within%2520the%2520background%2520in%2520videos.%2520The%250Adynamic%2520properties%2520of%2520video%2520enable%2520detection%2520of%2520camouflaged%2520objects%2520through%250Amotion%2520cues%2520or%2520varied%2520perspectives.%2520Previous%2520VCOD%2520datasets%2520primarily%2520contain%250Aanimal%2520objects%252C%2520limiting%2520the%2520scope%2520of%2520research%2520to%2520wildlife%2520scenarios.%2520However%252C%250Athe%2520applications%2520of%2520VCOD%2520extend%2520beyond%2520wildlife%2520and%2520have%2520significant%250Aimplications%2520in%2520security%252C%2520art%252C%2520and%2520medical%2520fields.%2520Addressing%2520this%2520problem%252C%2520we%250Aconstruct%2520a%2520new%2520large-scale%2520multi-domain%2520VCOD%2520dataset%2520MSVCOD.%2520To%2520achieve%250Ahigh-quality%2520annotations%252C%2520we%2520design%2520a%2520semi-automatic%2520iterative%2520annotation%250Apipeline%2520that%2520reduces%2520costs%2520while%2520maintaining%2520annotation%2520accuracy.%2520Our%2520MSVCOD%250Ais%2520the%2520largest%2520VCOD%2520dataset%2520to%2520date%252C%2520introducing%2520multiple%2520object%2520categories%250Aincluding%2520human%252C%2520animal%252C%2520medical%252C%2520and%2520vehicle%2520objects%2520for%2520the%2520first%2520time%252C%2520while%250Aalso%2520expanding%2520background%2520diversity%2520across%2520various%2520environments.%2520This%2520expanded%250Ascope%2520increases%2520the%2520practical%2520applicability%2520of%2520the%2520VCOD%2520task%2520in%2520camouflaged%250Aobject%2520detection.%2520Alongside%2520this%2520dataset%252C%2520we%2520introduce%2520a%2520one-steam%2520video%250Acamouflage%2520object%2520detection%2520model%2520that%2520performs%2520both%2520feature%2520extraction%2520and%250Ainformation%2520fusion%2520without%2520additional%2520motion%2520feature%2520fusion%2520modules.%2520Our%250Aframework%2520achieves%2520state-of-the-art%2520results%2520on%2520the%2520existing%2520VCOD%2520animal%2520dataset%250Aand%2520the%2520proposed%2520MSVCOD.%2520The%2520dataset%2520and%2520code%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13859v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSVCOD%3AA%20Large-Scale%20Multi-Scene%20Dataset%20for%20Video%20Camouflage%20Object%0A%20%20Detection&entry.906535625=Shuyong%20Gao%20and%20Yu%27ang%20Feng%20and%20Qishan%20Wang%20and%20Lingyi%20Hong%20and%20Xinyu%20Zhou%20and%20Liu%20Fei%20and%20Yan%20Wang%20and%20Wenqiang%20Zhang&entry.1292438233=%20%20Video%20Camouflaged%20Object%20Detection%20%28VCOD%29%20is%20a%20challenging%20task%20which%20aims%20to%0Aidentify%20objects%20that%20seamlessly%20concealed%20within%20the%20background%20in%20videos.%20The%0Adynamic%20properties%20of%20video%20enable%20detection%20of%20camouflaged%20objects%20through%0Amotion%20cues%20or%20varied%20perspectives.%20Previous%20VCOD%20datasets%20primarily%20contain%0Aanimal%20objects%2C%20limiting%20the%20scope%20of%20research%20to%20wildlife%20scenarios.%20However%2C%0Athe%20applications%20of%20VCOD%20extend%20beyond%20wildlife%20and%20have%20significant%0Aimplications%20in%20security%2C%20art%2C%20and%20medical%20fields.%20Addressing%20this%20problem%2C%20we%0Aconstruct%20a%20new%20large-scale%20multi-domain%20VCOD%20dataset%20MSVCOD.%20To%20achieve%0Ahigh-quality%20annotations%2C%20we%20design%20a%20semi-automatic%20iterative%20annotation%0Apipeline%20that%20reduces%20costs%20while%20maintaining%20annotation%20accuracy.%20Our%20MSVCOD%0Ais%20the%20largest%20VCOD%20dataset%20to%20date%2C%20introducing%20multiple%20object%20categories%0Aincluding%20human%2C%20animal%2C%20medical%2C%20and%20vehicle%20objects%20for%20the%20first%20time%2C%20while%0Aalso%20expanding%20background%20diversity%20across%20various%20environments.%20This%20expanded%0Ascope%20increases%20the%20practical%20applicability%20of%20the%20VCOD%20task%20in%20camouflaged%0Aobject%20detection.%20Alongside%20this%20dataset%2C%20we%20introduce%20a%20one-steam%20video%0Acamouflage%20object%20detection%20model%20that%20performs%20both%20feature%20extraction%20and%0Ainformation%20fusion%20without%20additional%20motion%20feature%20fusion%20modules.%20Our%0Aframework%20achieves%20state-of-the-art%20results%20on%20the%20existing%20VCOD%20animal%20dataset%0Aand%20the%20proposed%20MSVCOD.%20The%20dataset%20and%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13859v1&entry.124074799=Read"},
{"title": "Quantifying Memorization and Retriever Performance in\n  Retrieval-Augmented Vision-Language Models", "author": "Peter Carragher and Abhinand Jha and R Raghav and Kathleen M. Carley", "abstract": "  Large Language Models (LLMs) demonstrate remarkable capabilities in question\nanswering (QA), but metrics for assessing their reliance on memorization versus\nretrieval remain underdeveloped. Moreover, while finetuned models are\nstate-of-the-art on closed-domain tasks, general-purpose models like GPT-4o\nexhibit strong zero-shot performance. This raises questions about the\ntrade-offs between memorization, generalization, and retrieval. In this work,\nwe analyze the extent to which multimodal retrieval-augmented VLMs memorize\ntraining data compared to baseline VLMs. Using the WebQA benchmark, we contrast\nfinetuned models with baseline VLMs on multihop retrieval and question\nanswering, examining the impact of finetuning on data memorization. To quantify\nmemorization in end-to-end retrieval and QA systems, we propose several proxy\nmetrics by investigating instances where QA succeeds despite retrieval failing.\nOur results reveal the extent to which finetuned models rely on memorization.\nIn contrast, retrieval-augmented VLMs have lower memorization scores, at the\ncost of accuracy (72% vs 52% on WebQA test set). As such, our measures pose a\nchallenge for future work to reconcile memorization and generalization in both\nOpen-Domain QA and joint Retrieval-QA tasks.\n", "link": "http://arxiv.org/abs/2502.13836v1", "date": "2025-02-19", "relevancy": 2.714, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5598}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5598}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20Memorization%20and%20Retriever%20Performance%20in%0A%20%20Retrieval-Augmented%20Vision-Language%20Models&body=Title%3A%20Quantifying%20Memorization%20and%20Retriever%20Performance%20in%0A%20%20Retrieval-Augmented%20Vision-Language%20Models%0AAuthor%3A%20Peter%20Carragher%20and%20Abhinand%20Jha%20and%20R%20Raghav%20and%20Kathleen%20M.%20Carley%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20remarkable%20capabilities%20in%20question%0Aanswering%20%28QA%29%2C%20but%20metrics%20for%20assessing%20their%20reliance%20on%20memorization%20versus%0Aretrieval%20remain%20underdeveloped.%20Moreover%2C%20while%20finetuned%20models%20are%0Astate-of-the-art%20on%20closed-domain%20tasks%2C%20general-purpose%20models%20like%20GPT-4o%0Aexhibit%20strong%20zero-shot%20performance.%20This%20raises%20questions%20about%20the%0Atrade-offs%20between%20memorization%2C%20generalization%2C%20and%20retrieval.%20In%20this%20work%2C%0Awe%20analyze%20the%20extent%20to%20which%20multimodal%20retrieval-augmented%20VLMs%20memorize%0Atraining%20data%20compared%20to%20baseline%20VLMs.%20Using%20the%20WebQA%20benchmark%2C%20we%20contrast%0Afinetuned%20models%20with%20baseline%20VLMs%20on%20multihop%20retrieval%20and%20question%0Aanswering%2C%20examining%20the%20impact%20of%20finetuning%20on%20data%20memorization.%20To%20quantify%0Amemorization%20in%20end-to-end%20retrieval%20and%20QA%20systems%2C%20we%20propose%20several%20proxy%0Ametrics%20by%20investigating%20instances%20where%20QA%20succeeds%20despite%20retrieval%20failing.%0AOur%20results%20reveal%20the%20extent%20to%20which%20finetuned%20models%20rely%20on%20memorization.%0AIn%20contrast%2C%20retrieval-augmented%20VLMs%20have%20lower%20memorization%20scores%2C%20at%20the%0Acost%20of%20accuracy%20%2872%25%20vs%2052%25%20on%20WebQA%20test%20set%29.%20As%20such%2C%20our%20measures%20pose%20a%0Achallenge%20for%20future%20work%20to%20reconcile%20memorization%20and%20generalization%20in%20both%0AOpen-Domain%20QA%20and%20joint%20Retrieval-QA%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520Memorization%2520and%2520Retriever%2520Performance%2520in%250A%2520%2520Retrieval-Augmented%2520Vision-Language%2520Models%26entry.906535625%3DPeter%2520Carragher%2520and%2520Abhinand%2520Jha%2520and%2520R%2520Raghav%2520and%2520Kathleen%2520M.%2520Carley%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrate%2520remarkable%2520capabilities%2520in%2520question%250Aanswering%2520%2528QA%2529%252C%2520but%2520metrics%2520for%2520assessing%2520their%2520reliance%2520on%2520memorization%2520versus%250Aretrieval%2520remain%2520underdeveloped.%2520Moreover%252C%2520while%2520finetuned%2520models%2520are%250Astate-of-the-art%2520on%2520closed-domain%2520tasks%252C%2520general-purpose%2520models%2520like%2520GPT-4o%250Aexhibit%2520strong%2520zero-shot%2520performance.%2520This%2520raises%2520questions%2520about%2520the%250Atrade-offs%2520between%2520memorization%252C%2520generalization%252C%2520and%2520retrieval.%2520In%2520this%2520work%252C%250Awe%2520analyze%2520the%2520extent%2520to%2520which%2520multimodal%2520retrieval-augmented%2520VLMs%2520memorize%250Atraining%2520data%2520compared%2520to%2520baseline%2520VLMs.%2520Using%2520the%2520WebQA%2520benchmark%252C%2520we%2520contrast%250Afinetuned%2520models%2520with%2520baseline%2520VLMs%2520on%2520multihop%2520retrieval%2520and%2520question%250Aanswering%252C%2520examining%2520the%2520impact%2520of%2520finetuning%2520on%2520data%2520memorization.%2520To%2520quantify%250Amemorization%2520in%2520end-to-end%2520retrieval%2520and%2520QA%2520systems%252C%2520we%2520propose%2520several%2520proxy%250Ametrics%2520by%2520investigating%2520instances%2520where%2520QA%2520succeeds%2520despite%2520retrieval%2520failing.%250AOur%2520results%2520reveal%2520the%2520extent%2520to%2520which%2520finetuned%2520models%2520rely%2520on%2520memorization.%250AIn%2520contrast%252C%2520retrieval-augmented%2520VLMs%2520have%2520lower%2520memorization%2520scores%252C%2520at%2520the%250Acost%2520of%2520accuracy%2520%252872%2525%2520vs%252052%2525%2520on%2520WebQA%2520test%2520set%2529.%2520As%2520such%252C%2520our%2520measures%2520pose%2520a%250Achallenge%2520for%2520future%2520work%2520to%2520reconcile%2520memorization%2520and%2520generalization%2520in%2520both%250AOpen-Domain%2520QA%2520and%2520joint%2520Retrieval-QA%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20Memorization%20and%20Retriever%20Performance%20in%0A%20%20Retrieval-Augmented%20Vision-Language%20Models&entry.906535625=Peter%20Carragher%20and%20Abhinand%20Jha%20and%20R%20Raghav%20and%20Kathleen%20M.%20Carley&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20remarkable%20capabilities%20in%20question%0Aanswering%20%28QA%29%2C%20but%20metrics%20for%20assessing%20their%20reliance%20on%20memorization%20versus%0Aretrieval%20remain%20underdeveloped.%20Moreover%2C%20while%20finetuned%20models%20are%0Astate-of-the-art%20on%20closed-domain%20tasks%2C%20general-purpose%20models%20like%20GPT-4o%0Aexhibit%20strong%20zero-shot%20performance.%20This%20raises%20questions%20about%20the%0Atrade-offs%20between%20memorization%2C%20generalization%2C%20and%20retrieval.%20In%20this%20work%2C%0Awe%20analyze%20the%20extent%20to%20which%20multimodal%20retrieval-augmented%20VLMs%20memorize%0Atraining%20data%20compared%20to%20baseline%20VLMs.%20Using%20the%20WebQA%20benchmark%2C%20we%20contrast%0Afinetuned%20models%20with%20baseline%20VLMs%20on%20multihop%20retrieval%20and%20question%0Aanswering%2C%20examining%20the%20impact%20of%20finetuning%20on%20data%20memorization.%20To%20quantify%0Amemorization%20in%20end-to-end%20retrieval%20and%20QA%20systems%2C%20we%20propose%20several%20proxy%0Ametrics%20by%20investigating%20instances%20where%20QA%20succeeds%20despite%20retrieval%20failing.%0AOur%20results%20reveal%20the%20extent%20to%20which%20finetuned%20models%20rely%20on%20memorization.%0AIn%20contrast%2C%20retrieval-augmented%20VLMs%20have%20lower%20memorization%20scores%2C%20at%20the%0Acost%20of%20accuracy%20%2872%25%20vs%2052%25%20on%20WebQA%20test%20set%29.%20As%20such%2C%20our%20measures%20pose%20a%0Achallenge%20for%20future%20work%20to%20reconcile%20memorization%20and%20generalization%20in%20both%0AOpen-Domain%20QA%20and%20joint%20Retrieval-QA%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13836v1&entry.124074799=Read"},
{"title": "Explaining the Impact of Training on Vision Models via Activation\n  Clustering", "author": "Ahc\u00e8ne Boubekki and Samuel G. Fadel and Sebastian Mair", "abstract": "  Recent developments in the field of explainable artificial intelligence (XAI)\nfor vision models investigate the information extracted by their feature\nencoder. We contribute to this effort and propose Neuro-Activated Vision\nExplanations (NAVE), which extracts the information captured by the encoder by\nclustering the feature activations of the frozen network to be explained. The\nmethod does not aim to explain the model's prediction but to answer questions\nsuch as which parts of the image are processed similarly or which information\nis kept in deeper layers. Experimentally, we leverage NAVE to show that the\ntraining dataset and the level of supervision affect which concepts are\ncaptured. In addition, our method reveals the impact of registers on vision\ntransformers (ViT) and the information saturation caused by the watermark\nClever Hans effect in the training set.\n", "link": "http://arxiv.org/abs/2411.19700v2", "date": "2025-02-19", "relevancy": 2.7045, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5708}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5708}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaining%20the%20Impact%20of%20Training%20on%20Vision%20Models%20via%20Activation%0A%20%20Clustering&body=Title%3A%20Explaining%20the%20Impact%20of%20Training%20on%20Vision%20Models%20via%20Activation%0A%20%20Clustering%0AAuthor%3A%20Ahc%C3%A8ne%20Boubekki%20and%20Samuel%20G.%20Fadel%20and%20Sebastian%20Mair%0AAbstract%3A%20%20%20Recent%20developments%20in%20the%20field%20of%20explainable%20artificial%20intelligence%20%28XAI%29%0Afor%20vision%20models%20investigate%20the%20information%20extracted%20by%20their%20feature%0Aencoder.%20We%20contribute%20to%20this%20effort%20and%20propose%20Neuro-Activated%20Vision%0AExplanations%20%28NAVE%29%2C%20which%20extracts%20the%20information%20captured%20by%20the%20encoder%20by%0Aclustering%20the%20feature%20activations%20of%20the%20frozen%20network%20to%20be%20explained.%20The%0Amethod%20does%20not%20aim%20to%20explain%20the%20model%27s%20prediction%20but%20to%20answer%20questions%0Asuch%20as%20which%20parts%20of%20the%20image%20are%20processed%20similarly%20or%20which%20information%0Ais%20kept%20in%20deeper%20layers.%20Experimentally%2C%20we%20leverage%20NAVE%20to%20show%20that%20the%0Atraining%20dataset%20and%20the%20level%20of%20supervision%20affect%20which%20concepts%20are%0Acaptured.%20In%20addition%2C%20our%20method%20reveals%20the%20impact%20of%20registers%20on%20vision%0Atransformers%20%28ViT%29%20and%20the%20information%20saturation%20caused%20by%20the%20watermark%0AClever%20Hans%20effect%20in%20the%20training%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19700v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaining%2520the%2520Impact%2520of%2520Training%2520on%2520Vision%2520Models%2520via%2520Activation%250A%2520%2520Clustering%26entry.906535625%3DAhc%25C3%25A8ne%2520Boubekki%2520and%2520Samuel%2520G.%2520Fadel%2520and%2520Sebastian%2520Mair%26entry.1292438233%3D%2520%2520Recent%2520developments%2520in%2520the%2520field%2520of%2520explainable%2520artificial%2520intelligence%2520%2528XAI%2529%250Afor%2520vision%2520models%2520investigate%2520the%2520information%2520extracted%2520by%2520their%2520feature%250Aencoder.%2520We%2520contribute%2520to%2520this%2520effort%2520and%2520propose%2520Neuro-Activated%2520Vision%250AExplanations%2520%2528NAVE%2529%252C%2520which%2520extracts%2520the%2520information%2520captured%2520by%2520the%2520encoder%2520by%250Aclustering%2520the%2520feature%2520activations%2520of%2520the%2520frozen%2520network%2520to%2520be%2520explained.%2520The%250Amethod%2520does%2520not%2520aim%2520to%2520explain%2520the%2520model%2527s%2520prediction%2520but%2520to%2520answer%2520questions%250Asuch%2520as%2520which%2520parts%2520of%2520the%2520image%2520are%2520processed%2520similarly%2520or%2520which%2520information%250Ais%2520kept%2520in%2520deeper%2520layers.%2520Experimentally%252C%2520we%2520leverage%2520NAVE%2520to%2520show%2520that%2520the%250Atraining%2520dataset%2520and%2520the%2520level%2520of%2520supervision%2520affect%2520which%2520concepts%2520are%250Acaptured.%2520In%2520addition%252C%2520our%2520method%2520reveals%2520the%2520impact%2520of%2520registers%2520on%2520vision%250Atransformers%2520%2528ViT%2529%2520and%2520the%2520information%2520saturation%2520caused%2520by%2520the%2520watermark%250AClever%2520Hans%2520effect%2520in%2520the%2520training%2520set.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19700v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaining%20the%20Impact%20of%20Training%20on%20Vision%20Models%20via%20Activation%0A%20%20Clustering&entry.906535625=Ahc%C3%A8ne%20Boubekki%20and%20Samuel%20G.%20Fadel%20and%20Sebastian%20Mair&entry.1292438233=%20%20Recent%20developments%20in%20the%20field%20of%20explainable%20artificial%20intelligence%20%28XAI%29%0Afor%20vision%20models%20investigate%20the%20information%20extracted%20by%20their%20feature%0Aencoder.%20We%20contribute%20to%20this%20effort%20and%20propose%20Neuro-Activated%20Vision%0AExplanations%20%28NAVE%29%2C%20which%20extracts%20the%20information%20captured%20by%20the%20encoder%20by%0Aclustering%20the%20feature%20activations%20of%20the%20frozen%20network%20to%20be%20explained.%20The%0Amethod%20does%20not%20aim%20to%20explain%20the%20model%27s%20prediction%20but%20to%20answer%20questions%0Asuch%20as%20which%20parts%20of%20the%20image%20are%20processed%20similarly%20or%20which%20information%0Ais%20kept%20in%20deeper%20layers.%20Experimentally%2C%20we%20leverage%20NAVE%20to%20show%20that%20the%0Atraining%20dataset%20and%20the%20level%20of%20supervision%20affect%20which%20concepts%20are%0Acaptured.%20In%20addition%2C%20our%20method%20reveals%20the%20impact%20of%20registers%20on%20vision%0Atransformers%20%28ViT%29%20and%20the%20information%20saturation%20caused%20by%20the%20watermark%0AClever%20Hans%20effect%20in%20the%20training%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19700v2&entry.124074799=Read"},
{"title": "Concept Layers: Enhancing Interpretability and Intervenability via LLM\n  Conceptualization", "author": "Or Raphael Bidusa and Shaul Markovitch", "abstract": "  The opaque nature of Large Language Models (LLMs) has led to significant\nresearch efforts aimed at enhancing their interpretability, primarily through\npost-hoc methods. More recent in-hoc approaches, such as Concept Bottleneck\nModels (CBMs), offer both interpretability and intervenability by incorporating\nexplicit concept representations. However, these methods suffer from key\nlimitations, including reliance on labeled concept datasets and significant\narchitectural modifications that challenges re-integration into existing system\npipelines. In this work, we introduce a new methodology for incorporating\ninterpretability and intervenability into an existing model by integrating\nConcept Layers (CLs) into its architecture. Our approach projects the model's\ninternal vector representations into a conceptual, explainable vector space\nbefore reconstructing and feeding them back into the model. Furthermore, we\neliminate the need for a human-selected concept set by algorithmically\nsearching an ontology for a set of concepts that can be either task-specific or\ntask-agnostic. We evaluate CLs across multiple tasks, demonstrating that they\nmaintain the original model's performance and agreement while enabling\nmeaningful interventions. Additionally, we present a proof of concept\nshowcasing an intervenability interface, allowing users to adjust model\nbehavior dynamically, such as mitigating biases during inference.\n", "link": "http://arxiv.org/abs/2502.13632v1", "date": "2025-02-19", "relevancy": 2.6453, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5388}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5388}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concept%20Layers%3A%20Enhancing%20Interpretability%20and%20Intervenability%20via%20LLM%0A%20%20Conceptualization&body=Title%3A%20Concept%20Layers%3A%20Enhancing%20Interpretability%20and%20Intervenability%20via%20LLM%0A%20%20Conceptualization%0AAuthor%3A%20Or%20Raphael%20Bidusa%20and%20Shaul%20Markovitch%0AAbstract%3A%20%20%20The%20opaque%20nature%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20led%20to%20significant%0Aresearch%20efforts%20aimed%20at%20enhancing%20their%20interpretability%2C%20primarily%20through%0Apost-hoc%20methods.%20More%20recent%20in-hoc%20approaches%2C%20such%20as%20Concept%20Bottleneck%0AModels%20%28CBMs%29%2C%20offer%20both%20interpretability%20and%20intervenability%20by%20incorporating%0Aexplicit%20concept%20representations.%20However%2C%20these%20methods%20suffer%20from%20key%0Alimitations%2C%20including%20reliance%20on%20labeled%20concept%20datasets%20and%20significant%0Aarchitectural%20modifications%20that%20challenges%20re-integration%20into%20existing%20system%0Apipelines.%20In%20this%20work%2C%20we%20introduce%20a%20new%20methodology%20for%20incorporating%0Ainterpretability%20and%20intervenability%20into%20an%20existing%20model%20by%20integrating%0AConcept%20Layers%20%28CLs%29%20into%20its%20architecture.%20Our%20approach%20projects%20the%20model%27s%0Ainternal%20vector%20representations%20into%20a%20conceptual%2C%20explainable%20vector%20space%0Abefore%20reconstructing%20and%20feeding%20them%20back%20into%20the%20model.%20Furthermore%2C%20we%0Aeliminate%20the%20need%20for%20a%20human-selected%20concept%20set%20by%20algorithmically%0Asearching%20an%20ontology%20for%20a%20set%20of%20concepts%20that%20can%20be%20either%20task-specific%20or%0Atask-agnostic.%20We%20evaluate%20CLs%20across%20multiple%20tasks%2C%20demonstrating%20that%20they%0Amaintain%20the%20original%20model%27s%20performance%20and%20agreement%20while%20enabling%0Ameaningful%20interventions.%20Additionally%2C%20we%20present%20a%20proof%20of%20concept%0Ashowcasing%20an%20intervenability%20interface%2C%20allowing%20users%20to%20adjust%20model%0Abehavior%20dynamically%2C%20such%20as%20mitigating%20biases%20during%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcept%2520Layers%253A%2520Enhancing%2520Interpretability%2520and%2520Intervenability%2520via%2520LLM%250A%2520%2520Conceptualization%26entry.906535625%3DOr%2520Raphael%2520Bidusa%2520and%2520Shaul%2520Markovitch%26entry.1292438233%3D%2520%2520The%2520opaque%2520nature%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520led%2520to%2520significant%250Aresearch%2520efforts%2520aimed%2520at%2520enhancing%2520their%2520interpretability%252C%2520primarily%2520through%250Apost-hoc%2520methods.%2520More%2520recent%2520in-hoc%2520approaches%252C%2520such%2520as%2520Concept%2520Bottleneck%250AModels%2520%2528CBMs%2529%252C%2520offer%2520both%2520interpretability%2520and%2520intervenability%2520by%2520incorporating%250Aexplicit%2520concept%2520representations.%2520However%252C%2520these%2520methods%2520suffer%2520from%2520key%250Alimitations%252C%2520including%2520reliance%2520on%2520labeled%2520concept%2520datasets%2520and%2520significant%250Aarchitectural%2520modifications%2520that%2520challenges%2520re-integration%2520into%2520existing%2520system%250Apipelines.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520new%2520methodology%2520for%2520incorporating%250Ainterpretability%2520and%2520intervenability%2520into%2520an%2520existing%2520model%2520by%2520integrating%250AConcept%2520Layers%2520%2528CLs%2529%2520into%2520its%2520architecture.%2520Our%2520approach%2520projects%2520the%2520model%2527s%250Ainternal%2520vector%2520representations%2520into%2520a%2520conceptual%252C%2520explainable%2520vector%2520space%250Abefore%2520reconstructing%2520and%2520feeding%2520them%2520back%2520into%2520the%2520model.%2520Furthermore%252C%2520we%250Aeliminate%2520the%2520need%2520for%2520a%2520human-selected%2520concept%2520set%2520by%2520algorithmically%250Asearching%2520an%2520ontology%2520for%2520a%2520set%2520of%2520concepts%2520that%2520can%2520be%2520either%2520task-specific%2520or%250Atask-agnostic.%2520We%2520evaluate%2520CLs%2520across%2520multiple%2520tasks%252C%2520demonstrating%2520that%2520they%250Amaintain%2520the%2520original%2520model%2527s%2520performance%2520and%2520agreement%2520while%2520enabling%250Ameaningful%2520interventions.%2520Additionally%252C%2520we%2520present%2520a%2520proof%2520of%2520concept%250Ashowcasing%2520an%2520intervenability%2520interface%252C%2520allowing%2520users%2520to%2520adjust%2520model%250Abehavior%2520dynamically%252C%2520such%2520as%2520mitigating%2520biases%2520during%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concept%20Layers%3A%20Enhancing%20Interpretability%20and%20Intervenability%20via%20LLM%0A%20%20Conceptualization&entry.906535625=Or%20Raphael%20Bidusa%20and%20Shaul%20Markovitch&entry.1292438233=%20%20The%20opaque%20nature%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20led%20to%20significant%0Aresearch%20efforts%20aimed%20at%20enhancing%20their%20interpretability%2C%20primarily%20through%0Apost-hoc%20methods.%20More%20recent%20in-hoc%20approaches%2C%20such%20as%20Concept%20Bottleneck%0AModels%20%28CBMs%29%2C%20offer%20both%20interpretability%20and%20intervenability%20by%20incorporating%0Aexplicit%20concept%20representations.%20However%2C%20these%20methods%20suffer%20from%20key%0Alimitations%2C%20including%20reliance%20on%20labeled%20concept%20datasets%20and%20significant%0Aarchitectural%20modifications%20that%20challenges%20re-integration%20into%20existing%20system%0Apipelines.%20In%20this%20work%2C%20we%20introduce%20a%20new%20methodology%20for%20incorporating%0Ainterpretability%20and%20intervenability%20into%20an%20existing%20model%20by%20integrating%0AConcept%20Layers%20%28CLs%29%20into%20its%20architecture.%20Our%20approach%20projects%20the%20model%27s%0Ainternal%20vector%20representations%20into%20a%20conceptual%2C%20explainable%20vector%20space%0Abefore%20reconstructing%20and%20feeding%20them%20back%20into%20the%20model.%20Furthermore%2C%20we%0Aeliminate%20the%20need%20for%20a%20human-selected%20concept%20set%20by%20algorithmically%0Asearching%20an%20ontology%20for%20a%20set%20of%20concepts%20that%20can%20be%20either%20task-specific%20or%0Atask-agnostic.%20We%20evaluate%20CLs%20across%20multiple%20tasks%2C%20demonstrating%20that%20they%0Amaintain%20the%20original%20model%27s%20performance%20and%20agreement%20while%20enabling%0Ameaningful%20interventions.%20Additionally%2C%20we%20present%20a%20proof%20of%20concept%0Ashowcasing%20an%20intervenability%20interface%2C%20allowing%20users%20to%20adjust%20model%0Abehavior%20dynamically%2C%20such%20as%20mitigating%20biases%20during%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13632v1&entry.124074799=Read"},
{"title": "SLAMSpoof: Practical LiDAR Spoofing Attacks on Localization Systems\n  Guided by Scan Matching Vulnerability Analysis", "author": "Rokuto Nagata and Kenji Koide and Yuki Hayakawa and Ryo Suzuki and Kazuma Ikeda and Ozora Sako and Qi Alfred Chen and Takami Sato and Kentaro Yoshioka", "abstract": "  Accurate localization is essential for enabling modern full self-driving\nservices. These services heavily rely on map-based traffic information to\nreduce uncertainties in recognizing lane shapes, traffic light locations, and\ntraffic signs. Achieving this level of reliance on map information requires\ncentimeter-level localization accuracy, which is currently only achievable with\nLiDAR sensors. However, LiDAR is known to be vulnerable to spoofing attacks\nthat emit malicious lasers against LiDAR to overwrite its measurements. Once\nlocalization is compromised, the attack could lead the victim off roads or make\nthem ignore traffic lights. Motivated by these serious safety implications, we\ndesign SLAMSpoof, the first practical LiDAR spoofing attack on localization\nsystems for self-driving to assess the actual attack significance on autonomous\nvehicles. SLAMSpoof can effectively find the effective attack location based on\nour scan matching vulnerability score (SMVS), a point-wise metric representing\nthe potential vulnerability to spoofing attacks. To evaluate the effectiveness\nof the attack, we conduct real-world experiments on ground vehicles and confirm\nits high capability in real-world scenarios, inducing position errors of\n$\\geq$4.2 meters (more than typical lane width) for all 3 popular LiDAR-based\nlocalization algorithms. We finally discuss the potential countermeasures of\nthis attack. Code is available at https://github.com/Keio-CSG/slamspoof\n", "link": "http://arxiv.org/abs/2502.13641v1", "date": "2025-02-19", "relevancy": 2.6146, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5671}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5173}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLAMSpoof%3A%20Practical%20LiDAR%20Spoofing%20Attacks%20on%20Localization%20Systems%0A%20%20Guided%20by%20Scan%20Matching%20Vulnerability%20Analysis&body=Title%3A%20SLAMSpoof%3A%20Practical%20LiDAR%20Spoofing%20Attacks%20on%20Localization%20Systems%0A%20%20Guided%20by%20Scan%20Matching%20Vulnerability%20Analysis%0AAuthor%3A%20Rokuto%20Nagata%20and%20Kenji%20Koide%20and%20Yuki%20Hayakawa%20and%20Ryo%20Suzuki%20and%20Kazuma%20Ikeda%20and%20Ozora%20Sako%20and%20Qi%20Alfred%20Chen%20and%20Takami%20Sato%20and%20Kentaro%20Yoshioka%0AAbstract%3A%20%20%20Accurate%20localization%20is%20essential%20for%20enabling%20modern%20full%20self-driving%0Aservices.%20These%20services%20heavily%20rely%20on%20map-based%20traffic%20information%20to%0Areduce%20uncertainties%20in%20recognizing%20lane%20shapes%2C%20traffic%20light%20locations%2C%20and%0Atraffic%20signs.%20Achieving%20this%20level%20of%20reliance%20on%20map%20information%20requires%0Acentimeter-level%20localization%20accuracy%2C%20which%20is%20currently%20only%20achievable%20with%0ALiDAR%20sensors.%20However%2C%20LiDAR%20is%20known%20to%20be%20vulnerable%20to%20spoofing%20attacks%0Athat%20emit%20malicious%20lasers%20against%20LiDAR%20to%20overwrite%20its%20measurements.%20Once%0Alocalization%20is%20compromised%2C%20the%20attack%20could%20lead%20the%20victim%20off%20roads%20or%20make%0Athem%20ignore%20traffic%20lights.%20Motivated%20by%20these%20serious%20safety%20implications%2C%20we%0Adesign%20SLAMSpoof%2C%20the%20first%20practical%20LiDAR%20spoofing%20attack%20on%20localization%0Asystems%20for%20self-driving%20to%20assess%20the%20actual%20attack%20significance%20on%20autonomous%0Avehicles.%20SLAMSpoof%20can%20effectively%20find%20the%20effective%20attack%20location%20based%20on%0Aour%20scan%20matching%20vulnerability%20score%20%28SMVS%29%2C%20a%20point-wise%20metric%20representing%0Athe%20potential%20vulnerability%20to%20spoofing%20attacks.%20To%20evaluate%20the%20effectiveness%0Aof%20the%20attack%2C%20we%20conduct%20real-world%20experiments%20on%20ground%20vehicles%20and%20confirm%0Aits%20high%20capability%20in%20real-world%20scenarios%2C%20inducing%20position%20errors%20of%0A%24%5Cgeq%244.2%20meters%20%28more%20than%20typical%20lane%20width%29%20for%20all%203%20popular%20LiDAR-based%0Alocalization%20algorithms.%20We%20finally%20discuss%20the%20potential%20countermeasures%20of%0Athis%20attack.%20Code%20is%20available%20at%20https%3A//github.com/Keio-CSG/slamspoof%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLAMSpoof%253A%2520Practical%2520LiDAR%2520Spoofing%2520Attacks%2520on%2520Localization%2520Systems%250A%2520%2520Guided%2520by%2520Scan%2520Matching%2520Vulnerability%2520Analysis%26entry.906535625%3DRokuto%2520Nagata%2520and%2520Kenji%2520Koide%2520and%2520Yuki%2520Hayakawa%2520and%2520Ryo%2520Suzuki%2520and%2520Kazuma%2520Ikeda%2520and%2520Ozora%2520Sako%2520and%2520Qi%2520Alfred%2520Chen%2520and%2520Takami%2520Sato%2520and%2520Kentaro%2520Yoshioka%26entry.1292438233%3D%2520%2520Accurate%2520localization%2520is%2520essential%2520for%2520enabling%2520modern%2520full%2520self-driving%250Aservices.%2520These%2520services%2520heavily%2520rely%2520on%2520map-based%2520traffic%2520information%2520to%250Areduce%2520uncertainties%2520in%2520recognizing%2520lane%2520shapes%252C%2520traffic%2520light%2520locations%252C%2520and%250Atraffic%2520signs.%2520Achieving%2520this%2520level%2520of%2520reliance%2520on%2520map%2520information%2520requires%250Acentimeter-level%2520localization%2520accuracy%252C%2520which%2520is%2520currently%2520only%2520achievable%2520with%250ALiDAR%2520sensors.%2520However%252C%2520LiDAR%2520is%2520known%2520to%2520be%2520vulnerable%2520to%2520spoofing%2520attacks%250Athat%2520emit%2520malicious%2520lasers%2520against%2520LiDAR%2520to%2520overwrite%2520its%2520measurements.%2520Once%250Alocalization%2520is%2520compromised%252C%2520the%2520attack%2520could%2520lead%2520the%2520victim%2520off%2520roads%2520or%2520make%250Athem%2520ignore%2520traffic%2520lights.%2520Motivated%2520by%2520these%2520serious%2520safety%2520implications%252C%2520we%250Adesign%2520SLAMSpoof%252C%2520the%2520first%2520practical%2520LiDAR%2520spoofing%2520attack%2520on%2520localization%250Asystems%2520for%2520self-driving%2520to%2520assess%2520the%2520actual%2520attack%2520significance%2520on%2520autonomous%250Avehicles.%2520SLAMSpoof%2520can%2520effectively%2520find%2520the%2520effective%2520attack%2520location%2520based%2520on%250Aour%2520scan%2520matching%2520vulnerability%2520score%2520%2528SMVS%2529%252C%2520a%2520point-wise%2520metric%2520representing%250Athe%2520potential%2520vulnerability%2520to%2520spoofing%2520attacks.%2520To%2520evaluate%2520the%2520effectiveness%250Aof%2520the%2520attack%252C%2520we%2520conduct%2520real-world%2520experiments%2520on%2520ground%2520vehicles%2520and%2520confirm%250Aits%2520high%2520capability%2520in%2520real-world%2520scenarios%252C%2520inducing%2520position%2520errors%2520of%250A%2524%255Cgeq%25244.2%2520meters%2520%2528more%2520than%2520typical%2520lane%2520width%2529%2520for%2520all%25203%2520popular%2520LiDAR-based%250Alocalization%2520algorithms.%2520We%2520finally%2520discuss%2520the%2520potential%2520countermeasures%2520of%250Athis%2520attack.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Keio-CSG/slamspoof%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLAMSpoof%3A%20Practical%20LiDAR%20Spoofing%20Attacks%20on%20Localization%20Systems%0A%20%20Guided%20by%20Scan%20Matching%20Vulnerability%20Analysis&entry.906535625=Rokuto%20Nagata%20and%20Kenji%20Koide%20and%20Yuki%20Hayakawa%20and%20Ryo%20Suzuki%20and%20Kazuma%20Ikeda%20and%20Ozora%20Sako%20and%20Qi%20Alfred%20Chen%20and%20Takami%20Sato%20and%20Kentaro%20Yoshioka&entry.1292438233=%20%20Accurate%20localization%20is%20essential%20for%20enabling%20modern%20full%20self-driving%0Aservices.%20These%20services%20heavily%20rely%20on%20map-based%20traffic%20information%20to%0Areduce%20uncertainties%20in%20recognizing%20lane%20shapes%2C%20traffic%20light%20locations%2C%20and%0Atraffic%20signs.%20Achieving%20this%20level%20of%20reliance%20on%20map%20information%20requires%0Acentimeter-level%20localization%20accuracy%2C%20which%20is%20currently%20only%20achievable%20with%0ALiDAR%20sensors.%20However%2C%20LiDAR%20is%20known%20to%20be%20vulnerable%20to%20spoofing%20attacks%0Athat%20emit%20malicious%20lasers%20against%20LiDAR%20to%20overwrite%20its%20measurements.%20Once%0Alocalization%20is%20compromised%2C%20the%20attack%20could%20lead%20the%20victim%20off%20roads%20or%20make%0Athem%20ignore%20traffic%20lights.%20Motivated%20by%20these%20serious%20safety%20implications%2C%20we%0Adesign%20SLAMSpoof%2C%20the%20first%20practical%20LiDAR%20spoofing%20attack%20on%20localization%0Asystems%20for%20self-driving%20to%20assess%20the%20actual%20attack%20significance%20on%20autonomous%0Avehicles.%20SLAMSpoof%20can%20effectively%20find%20the%20effective%20attack%20location%20based%20on%0Aour%20scan%20matching%20vulnerability%20score%20%28SMVS%29%2C%20a%20point-wise%20metric%20representing%0Athe%20potential%20vulnerability%20to%20spoofing%20attacks.%20To%20evaluate%20the%20effectiveness%0Aof%20the%20attack%2C%20we%20conduct%20real-world%20experiments%20on%20ground%20vehicles%20and%20confirm%0Aits%20high%20capability%20in%20real-world%20scenarios%2C%20inducing%20position%20errors%20of%0A%24%5Cgeq%244.2%20meters%20%28more%20than%20typical%20lane%20width%29%20for%20all%203%20popular%20LiDAR-based%0Alocalization%20algorithms.%20We%20finally%20discuss%20the%20potential%20countermeasures%20of%0Athis%20attack.%20Code%20is%20available%20at%20https%3A//github.com/Keio-CSG/slamspoof%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13641v1&entry.124074799=Read"},
{"title": "LongPO: Long Context Self-Evolution of Large Language Models through\n  Short-to-Long Preference Optimization", "author": "Guanzheng Chen and Xin Li and Michael Qizhe Shieh and Lidong Bing", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable capabilities\nthrough pretraining and alignment. However, superior short-context LLMs may\nunderperform in long-context scenarios due to insufficient long-context\nalignment. This alignment process remains challenging due to the impracticality\nof human annotation for extended contexts and the difficulty in balancing\nshort- and long-context performance. To address these challenges, we introduce\nLongPO, that enables short-context LLMs to self-evolve to excel on long-context\ntasks by internally transferring short-context capabilities. LongPO harnesses\nLLMs to learn from self-generated short-to-long preference data, comprising\npaired responses generated for identical instructions with long-context inputs\nand their compressed short-context counterparts, respectively. This preference\nreveals capabilities and potentials of LLMs cultivated during short-context\nalignment that may be diminished in under-aligned long-context scenarios.\nAdditionally, LongPO incorporates a short-to-long KL constraint to mitigate\nshort-context performance decline during long-context alignment. When applied\nto Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully\nretains short-context performance and largely outperforms naive SFT and DPO in\nboth long- and short-context tasks. Specifically, \\ourMethod-trained models can\nachieve results on long-context benchmarks comparable to, or even surpassing,\nthose of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context\nannotation and larger parameter scales.\n", "link": "http://arxiv.org/abs/2502.13922v1", "date": "2025-02-19", "relevancy": 2.5954, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5207}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5183}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongPO%3A%20Long%20Context%20Self-Evolution%20of%20Large%20Language%20Models%20through%0A%20%20Short-to-Long%20Preference%20Optimization&body=Title%3A%20LongPO%3A%20Long%20Context%20Self-Evolution%20of%20Large%20Language%20Models%20through%0A%20%20Short-to-Long%20Preference%20Optimization%0AAuthor%3A%20Guanzheng%20Chen%20and%20Xin%20Li%20and%20Michael%20Qizhe%20Shieh%20and%20Lidong%20Bing%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%0Athrough%20pretraining%20and%20alignment.%20However%2C%20superior%20short-context%20LLMs%20may%0Aunderperform%20in%20long-context%20scenarios%20due%20to%20insufficient%20long-context%0Aalignment.%20This%20alignment%20process%20remains%20challenging%20due%20to%20the%20impracticality%0Aof%20human%20annotation%20for%20extended%20contexts%20and%20the%20difficulty%20in%20balancing%0Ashort-%20and%20long-context%20performance.%20To%20address%20these%20challenges%2C%20we%20introduce%0ALongPO%2C%20that%20enables%20short-context%20LLMs%20to%20self-evolve%20to%20excel%20on%20long-context%0Atasks%20by%20internally%20transferring%20short-context%20capabilities.%20LongPO%20harnesses%0ALLMs%20to%20learn%20from%20self-generated%20short-to-long%20preference%20data%2C%20comprising%0Apaired%20responses%20generated%20for%20identical%20instructions%20with%20long-context%20inputs%0Aand%20their%20compressed%20short-context%20counterparts%2C%20respectively.%20This%20preference%0Areveals%20capabilities%20and%20potentials%20of%20LLMs%20cultivated%20during%20short-context%0Aalignment%20that%20may%20be%20diminished%20in%20under-aligned%20long-context%20scenarios.%0AAdditionally%2C%20LongPO%20incorporates%20a%20short-to-long%20KL%20constraint%20to%20mitigate%0Ashort-context%20performance%20decline%20during%20long-context%20alignment.%20When%20applied%0Ato%20Mistral-7B-Instruct-v0.2%20from%20128K%20to%20512K%20context%20lengths%2C%20LongPO%20fully%0Aretains%20short-context%20performance%20and%20largely%20outperforms%20naive%20SFT%20and%20DPO%20in%0Aboth%20long-%20and%20short-context%20tasks.%20Specifically%2C%20%5CourMethod-trained%20models%20can%0Aachieve%20results%20on%20long-context%20benchmarks%20comparable%20to%2C%20or%20even%20surpassing%2C%0Athose%20of%20superior%20LLMs%20%28e.g.%2C%20GPT-4-128K%29%20that%20involve%20extensive%20long-context%0Aannotation%20and%20larger%20parameter%20scales.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongPO%253A%2520Long%2520Context%2520Self-Evolution%2520of%2520Large%2520Language%2520Models%2520through%250A%2520%2520Short-to-Long%2520Preference%2520Optimization%26entry.906535625%3DGuanzheng%2520Chen%2520and%2520Xin%2520Li%2520and%2520Michael%2520Qizhe%2520Shieh%2520and%2520Lidong%2520Bing%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%250Athrough%2520pretraining%2520and%2520alignment.%2520However%252C%2520superior%2520short-context%2520LLMs%2520may%250Aunderperform%2520in%2520long-context%2520scenarios%2520due%2520to%2520insufficient%2520long-context%250Aalignment.%2520This%2520alignment%2520process%2520remains%2520challenging%2520due%2520to%2520the%2520impracticality%250Aof%2520human%2520annotation%2520for%2520extended%2520contexts%2520and%2520the%2520difficulty%2520in%2520balancing%250Ashort-%2520and%2520long-context%2520performance.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250ALongPO%252C%2520that%2520enables%2520short-context%2520LLMs%2520to%2520self-evolve%2520to%2520excel%2520on%2520long-context%250Atasks%2520by%2520internally%2520transferring%2520short-context%2520capabilities.%2520LongPO%2520harnesses%250ALLMs%2520to%2520learn%2520from%2520self-generated%2520short-to-long%2520preference%2520data%252C%2520comprising%250Apaired%2520responses%2520generated%2520for%2520identical%2520instructions%2520with%2520long-context%2520inputs%250Aand%2520their%2520compressed%2520short-context%2520counterparts%252C%2520respectively.%2520This%2520preference%250Areveals%2520capabilities%2520and%2520potentials%2520of%2520LLMs%2520cultivated%2520during%2520short-context%250Aalignment%2520that%2520may%2520be%2520diminished%2520in%2520under-aligned%2520long-context%2520scenarios.%250AAdditionally%252C%2520LongPO%2520incorporates%2520a%2520short-to-long%2520KL%2520constraint%2520to%2520mitigate%250Ashort-context%2520performance%2520decline%2520during%2520long-context%2520alignment.%2520When%2520applied%250Ato%2520Mistral-7B-Instruct-v0.2%2520from%2520128K%2520to%2520512K%2520context%2520lengths%252C%2520LongPO%2520fully%250Aretains%2520short-context%2520performance%2520and%2520largely%2520outperforms%2520naive%2520SFT%2520and%2520DPO%2520in%250Aboth%2520long-%2520and%2520short-context%2520tasks.%2520Specifically%252C%2520%255CourMethod-trained%2520models%2520can%250Aachieve%2520results%2520on%2520long-context%2520benchmarks%2520comparable%2520to%252C%2520or%2520even%2520surpassing%252C%250Athose%2520of%2520superior%2520LLMs%2520%2528e.g.%252C%2520GPT-4-128K%2529%2520that%2520involve%2520extensive%2520long-context%250Aannotation%2520and%2520larger%2520parameter%2520scales.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongPO%3A%20Long%20Context%20Self-Evolution%20of%20Large%20Language%20Models%20through%0A%20%20Short-to-Long%20Preference%20Optimization&entry.906535625=Guanzheng%20Chen%20and%20Xin%20Li%20and%20Michael%20Qizhe%20Shieh%20and%20Lidong%20Bing&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%0Athrough%20pretraining%20and%20alignment.%20However%2C%20superior%20short-context%20LLMs%20may%0Aunderperform%20in%20long-context%20scenarios%20due%20to%20insufficient%20long-context%0Aalignment.%20This%20alignment%20process%20remains%20challenging%20due%20to%20the%20impracticality%0Aof%20human%20annotation%20for%20extended%20contexts%20and%20the%20difficulty%20in%20balancing%0Ashort-%20and%20long-context%20performance.%20To%20address%20these%20challenges%2C%20we%20introduce%0ALongPO%2C%20that%20enables%20short-context%20LLMs%20to%20self-evolve%20to%20excel%20on%20long-context%0Atasks%20by%20internally%20transferring%20short-context%20capabilities.%20LongPO%20harnesses%0ALLMs%20to%20learn%20from%20self-generated%20short-to-long%20preference%20data%2C%20comprising%0Apaired%20responses%20generated%20for%20identical%20instructions%20with%20long-context%20inputs%0Aand%20their%20compressed%20short-context%20counterparts%2C%20respectively.%20This%20preference%0Areveals%20capabilities%20and%20potentials%20of%20LLMs%20cultivated%20during%20short-context%0Aalignment%20that%20may%20be%20diminished%20in%20under-aligned%20long-context%20scenarios.%0AAdditionally%2C%20LongPO%20incorporates%20a%20short-to-long%20KL%20constraint%20to%20mitigate%0Ashort-context%20performance%20decline%20during%20long-context%20alignment.%20When%20applied%0Ato%20Mistral-7B-Instruct-v0.2%20from%20128K%20to%20512K%20context%20lengths%2C%20LongPO%20fully%0Aretains%20short-context%20performance%20and%20largely%20outperforms%20naive%20SFT%20and%20DPO%20in%0Aboth%20long-%20and%20short-context%20tasks.%20Specifically%2C%20%5CourMethod-trained%20models%20can%0Aachieve%20results%20on%20long-context%20benchmarks%20comparable%20to%2C%20or%20even%20surpassing%2C%0Athose%20of%20superior%20LLMs%20%28e.g.%2C%20GPT-4-128K%29%20that%20involve%20extensive%20long-context%0Aannotation%20and%20larger%20parameter%20scales.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13922v1&entry.124074799=Read"},
{"title": "GPU-Friendly Laplacian Texture Blending", "author": "Bartlomiej Wronski", "abstract": "  Texture and material blending is one of the leading methods for adding\nvariety to rendered virtual worlds, creating composite materials, and\ngenerating procedural content. When done naively, it can introduce either\nvisible seams or contrast loss, leading to an unnatural look not representative\nof blended textures. Earlier work proposed addressing this problem through\ncareful manual parameter tuning, lengthy per-texture statistics precomputation,\nlook-up tables, or training deep neural networks. In this work, we propose an\nalternative approach based on insights from image processing and Laplacian\npyramid blending. Our approach does not require any precomputation or increased\nmemory usage (other than the presence of a regular, non-Laplacian, texture\nmipmap chain), does not produce ghosting, preserves sharp local features, and\ncan run in real time on the GPU at the cost of a few additional lower mipmap\ntexture taps.\n", "link": "http://arxiv.org/abs/2502.13945v1", "date": "2025-02-19", "relevancy": 2.5943, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5329}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5167}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPU-Friendly%20Laplacian%20Texture%20Blending&body=Title%3A%20GPU-Friendly%20Laplacian%20Texture%20Blending%0AAuthor%3A%20Bartlomiej%20Wronski%0AAbstract%3A%20%20%20Texture%20and%20material%20blending%20is%20one%20of%20the%20leading%20methods%20for%20adding%0Avariety%20to%20rendered%20virtual%20worlds%2C%20creating%20composite%20materials%2C%20and%0Agenerating%20procedural%20content.%20When%20done%20naively%2C%20it%20can%20introduce%20either%0Avisible%20seams%20or%20contrast%20loss%2C%20leading%20to%20an%20unnatural%20look%20not%20representative%0Aof%20blended%20textures.%20Earlier%20work%20proposed%20addressing%20this%20problem%20through%0Acareful%20manual%20parameter%20tuning%2C%20lengthy%20per-texture%20statistics%20precomputation%2C%0Alook-up%20tables%2C%20or%20training%20deep%20neural%20networks.%20In%20this%20work%2C%20we%20propose%20an%0Aalternative%20approach%20based%20on%20insights%20from%20image%20processing%20and%20Laplacian%0Apyramid%20blending.%20Our%20approach%20does%20not%20require%20any%20precomputation%20or%20increased%0Amemory%20usage%20%28other%20than%20the%20presence%20of%20a%20regular%2C%20non-Laplacian%2C%20texture%0Amipmap%20chain%29%2C%20does%20not%20produce%20ghosting%2C%20preserves%20sharp%20local%20features%2C%20and%0Acan%20run%20in%20real%20time%20on%20the%20GPU%20at%20the%20cost%20of%20a%20few%20additional%20lower%20mipmap%0Atexture%20taps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13945v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPU-Friendly%2520Laplacian%2520Texture%2520Blending%26entry.906535625%3DBartlomiej%2520Wronski%26entry.1292438233%3D%2520%2520Texture%2520and%2520material%2520blending%2520is%2520one%2520of%2520the%2520leading%2520methods%2520for%2520adding%250Avariety%2520to%2520rendered%2520virtual%2520worlds%252C%2520creating%2520composite%2520materials%252C%2520and%250Agenerating%2520procedural%2520content.%2520When%2520done%2520naively%252C%2520it%2520can%2520introduce%2520either%250Avisible%2520seams%2520or%2520contrast%2520loss%252C%2520leading%2520to%2520an%2520unnatural%2520look%2520not%2520representative%250Aof%2520blended%2520textures.%2520Earlier%2520work%2520proposed%2520addressing%2520this%2520problem%2520through%250Acareful%2520manual%2520parameter%2520tuning%252C%2520lengthy%2520per-texture%2520statistics%2520precomputation%252C%250Alook-up%2520tables%252C%2520or%2520training%2520deep%2520neural%2520networks.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%250Aalternative%2520approach%2520based%2520on%2520insights%2520from%2520image%2520processing%2520and%2520Laplacian%250Apyramid%2520blending.%2520Our%2520approach%2520does%2520not%2520require%2520any%2520precomputation%2520or%2520increased%250Amemory%2520usage%2520%2528other%2520than%2520the%2520presence%2520of%2520a%2520regular%252C%2520non-Laplacian%252C%2520texture%250Amipmap%2520chain%2529%252C%2520does%2520not%2520produce%2520ghosting%252C%2520preserves%2520sharp%2520local%2520features%252C%2520and%250Acan%2520run%2520in%2520real%2520time%2520on%2520the%2520GPU%2520at%2520the%2520cost%2520of%2520a%2520few%2520additional%2520lower%2520mipmap%250Atexture%2520taps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13945v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPU-Friendly%20Laplacian%20Texture%20Blending&entry.906535625=Bartlomiej%20Wronski&entry.1292438233=%20%20Texture%20and%20material%20blending%20is%20one%20of%20the%20leading%20methods%20for%20adding%0Avariety%20to%20rendered%20virtual%20worlds%2C%20creating%20composite%20materials%2C%20and%0Agenerating%20procedural%20content.%20When%20done%20naively%2C%20it%20can%20introduce%20either%0Avisible%20seams%20or%20contrast%20loss%2C%20leading%20to%20an%20unnatural%20look%20not%20representative%0Aof%20blended%20textures.%20Earlier%20work%20proposed%20addressing%20this%20problem%20through%0Acareful%20manual%20parameter%20tuning%2C%20lengthy%20per-texture%20statistics%20precomputation%2C%0Alook-up%20tables%2C%20or%20training%20deep%20neural%20networks.%20In%20this%20work%2C%20we%20propose%20an%0Aalternative%20approach%20based%20on%20insights%20from%20image%20processing%20and%20Laplacian%0Apyramid%20blending.%20Our%20approach%20does%20not%20require%20any%20precomputation%20or%20increased%0Amemory%20usage%20%28other%20than%20the%20presence%20of%20a%20regular%2C%20non-Laplacian%2C%20texture%0Amipmap%20chain%29%2C%20does%20not%20produce%20ghosting%2C%20preserves%20sharp%20local%20features%2C%20and%0Acan%20run%20in%20real%20time%20on%20the%20GPU%20at%20the%20cost%20of%20a%20few%20additional%20lower%20mipmap%0Atexture%20taps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13945v1&entry.124074799=Read"},
{"title": "Herglotz-NET: Implicit Neural Representation of Spherical~Data with\n  Harmonic Positional Encoding", "author": "Th\u00e9o Hanon and Nicolas Mil-Homens Cavaco and John Kiely and Laurent Jacques", "abstract": "  Representing and processing data in spherical domains presents unique\nchallenges, primarily due to the curvature of the domain, which complicates the\napplication of classical Euclidean techniques. Implicit neural representations\n(INRs) have emerged as a promising alternative for high-fidelity data\nrepresentation; however, to effectively handle spherical domains, these methods\nmust be adapted to the inherent geometry of the sphere to maintain both\naccuracy and stability. In this context, we propose Herglotz-NET (HNET), a\nnovel INR architecture that employs a harmonic positional encoding based on\ncomplex Herglotz mappings. This encoding yields a well-posed representation on\nthe sphere with interpretable and robust spectral properties. Moreover, we\npresent a unified expressivity analysis showing that any spherical-based INR\nsatisfying a mild condition exhibits a predictable spectral expansion that\nscales with network depth. Our results establish HNET as a scalable and\nflexible framework for accurate modeling of spherical data.\n", "link": "http://arxiv.org/abs/2502.13777v1", "date": "2025-02-19", "relevancy": 2.5798, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5663}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4969}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Herglotz-NET%3A%20Implicit%20Neural%20Representation%20of%20Spherical~Data%20with%0A%20%20Harmonic%20Positional%20Encoding&body=Title%3A%20Herglotz-NET%3A%20Implicit%20Neural%20Representation%20of%20Spherical~Data%20with%0A%20%20Harmonic%20Positional%20Encoding%0AAuthor%3A%20Th%C3%A9o%20Hanon%20and%20Nicolas%20Mil-Homens%20Cavaco%20and%20John%20Kiely%20and%20Laurent%20Jacques%0AAbstract%3A%20%20%20Representing%20and%20processing%20data%20in%20spherical%20domains%20presents%20unique%0Achallenges%2C%20primarily%20due%20to%20the%20curvature%20of%20the%20domain%2C%20which%20complicates%20the%0Aapplication%20of%20classical%20Euclidean%20techniques.%20Implicit%20neural%20representations%0A%28INRs%29%20have%20emerged%20as%20a%20promising%20alternative%20for%20high-fidelity%20data%0Arepresentation%3B%20however%2C%20to%20effectively%20handle%20spherical%20domains%2C%20these%20methods%0Amust%20be%20adapted%20to%20the%20inherent%20geometry%20of%20the%20sphere%20to%20maintain%20both%0Aaccuracy%20and%20stability.%20In%20this%20context%2C%20we%20propose%20Herglotz-NET%20%28HNET%29%2C%20a%0Anovel%20INR%20architecture%20that%20employs%20a%20harmonic%20positional%20encoding%20based%20on%0Acomplex%20Herglotz%20mappings.%20This%20encoding%20yields%20a%20well-posed%20representation%20on%0Athe%20sphere%20with%20interpretable%20and%20robust%20spectral%20properties.%20Moreover%2C%20we%0Apresent%20a%20unified%20expressivity%20analysis%20showing%20that%20any%20spherical-based%20INR%0Asatisfying%20a%20mild%20condition%20exhibits%20a%20predictable%20spectral%20expansion%20that%0Ascales%20with%20network%20depth.%20Our%20results%20establish%20HNET%20as%20a%20scalable%20and%0Aflexible%20framework%20for%20accurate%20modeling%20of%20spherical%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHerglotz-NET%253A%2520Implicit%2520Neural%2520Representation%2520of%2520Spherical~Data%2520with%250A%2520%2520Harmonic%2520Positional%2520Encoding%26entry.906535625%3DTh%25C3%25A9o%2520Hanon%2520and%2520Nicolas%2520Mil-Homens%2520Cavaco%2520and%2520John%2520Kiely%2520and%2520Laurent%2520Jacques%26entry.1292438233%3D%2520%2520Representing%2520and%2520processing%2520data%2520in%2520spherical%2520domains%2520presents%2520unique%250Achallenges%252C%2520primarily%2520due%2520to%2520the%2520curvature%2520of%2520the%2520domain%252C%2520which%2520complicates%2520the%250Aapplication%2520of%2520classical%2520Euclidean%2520techniques.%2520Implicit%2520neural%2520representations%250A%2528INRs%2529%2520have%2520emerged%2520as%2520a%2520promising%2520alternative%2520for%2520high-fidelity%2520data%250Arepresentation%253B%2520however%252C%2520to%2520effectively%2520handle%2520spherical%2520domains%252C%2520these%2520methods%250Amust%2520be%2520adapted%2520to%2520the%2520inherent%2520geometry%2520of%2520the%2520sphere%2520to%2520maintain%2520both%250Aaccuracy%2520and%2520stability.%2520In%2520this%2520context%252C%2520we%2520propose%2520Herglotz-NET%2520%2528HNET%2529%252C%2520a%250Anovel%2520INR%2520architecture%2520that%2520employs%2520a%2520harmonic%2520positional%2520encoding%2520based%2520on%250Acomplex%2520Herglotz%2520mappings.%2520This%2520encoding%2520yields%2520a%2520well-posed%2520representation%2520on%250Athe%2520sphere%2520with%2520interpretable%2520and%2520robust%2520spectral%2520properties.%2520Moreover%252C%2520we%250Apresent%2520a%2520unified%2520expressivity%2520analysis%2520showing%2520that%2520any%2520spherical-based%2520INR%250Asatisfying%2520a%2520mild%2520condition%2520exhibits%2520a%2520predictable%2520spectral%2520expansion%2520that%250Ascales%2520with%2520network%2520depth.%2520Our%2520results%2520establish%2520HNET%2520as%2520a%2520scalable%2520and%250Aflexible%2520framework%2520for%2520accurate%2520modeling%2520of%2520spherical%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Herglotz-NET%3A%20Implicit%20Neural%20Representation%20of%20Spherical~Data%20with%0A%20%20Harmonic%20Positional%20Encoding&entry.906535625=Th%C3%A9o%20Hanon%20and%20Nicolas%20Mil-Homens%20Cavaco%20and%20John%20Kiely%20and%20Laurent%20Jacques&entry.1292438233=%20%20Representing%20and%20processing%20data%20in%20spherical%20domains%20presents%20unique%0Achallenges%2C%20primarily%20due%20to%20the%20curvature%20of%20the%20domain%2C%20which%20complicates%20the%0Aapplication%20of%20classical%20Euclidean%20techniques.%20Implicit%20neural%20representations%0A%28INRs%29%20have%20emerged%20as%20a%20promising%20alternative%20for%20high-fidelity%20data%0Arepresentation%3B%20however%2C%20to%20effectively%20handle%20spherical%20domains%2C%20these%20methods%0Amust%20be%20adapted%20to%20the%20inherent%20geometry%20of%20the%20sphere%20to%20maintain%20both%0Aaccuracy%20and%20stability.%20In%20this%20context%2C%20we%20propose%20Herglotz-NET%20%28HNET%29%2C%20a%0Anovel%20INR%20architecture%20that%20employs%20a%20harmonic%20positional%20encoding%20based%20on%0Acomplex%20Herglotz%20mappings.%20This%20encoding%20yields%20a%20well-posed%20representation%20on%0Athe%20sphere%20with%20interpretable%20and%20robust%20spectral%20properties.%20Moreover%2C%20we%0Apresent%20a%20unified%20expressivity%20analysis%20showing%20that%20any%20spherical-based%20INR%0Asatisfying%20a%20mild%20condition%20exhibits%20a%20predictable%20spectral%20expansion%20that%0Ascales%20with%20network%20depth.%20Our%20results%20establish%20HNET%20as%20a%20scalable%20and%0Aflexible%20framework%20for%20accurate%20modeling%20of%20spherical%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13777v1&entry.124074799=Read"},
{"title": "Complex Ontology Matching with Large Language Model Embeddings", "author": "Guilherme Sousa and Rinaldo Lima and Cassia Trojahn", "abstract": "  Ontology, and more broadly, Knowledge Graph Matching is a challenging task in\nwhich expressiveness has not been fully addressed. Despite the increasing use\nof embeddings and language models for this task, approaches for generating\nexpressive correspondences still do not take full advantage of these models, in\nparticular, large language models (LLMs). This paper proposes to integrate LLMs\ninto an approach for generating expressive correspondences based on alignment\nneed and ABox-based relation discovery. The generation of correspondences is\nperformed by matching similar surroundings of instance sub-graphs. The\nintegration of LLMs results in different architectural modifications, including\nlabel similarity, sub-graph matching, and entity matching. The performance word\nembeddings, sentence embeddings, and LLM-based embeddings, was compared. The\nresults demonstrate that integrating LLMs surpasses all other models, enhancing\nthe baseline version of the approach with a 45\\% increase in F-measure.\n", "link": "http://arxiv.org/abs/2502.13619v1", "date": "2025-02-19", "relevancy": 2.5687, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5398}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5398}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Complex%20Ontology%20Matching%20with%20Large%20Language%20Model%20Embeddings&body=Title%3A%20Complex%20Ontology%20Matching%20with%20Large%20Language%20Model%20Embeddings%0AAuthor%3A%20Guilherme%20Sousa%20and%20Rinaldo%20Lima%20and%20Cassia%20Trojahn%0AAbstract%3A%20%20%20Ontology%2C%20and%20more%20broadly%2C%20Knowledge%20Graph%20Matching%20is%20a%20challenging%20task%20in%0Awhich%20expressiveness%20has%20not%20been%20fully%20addressed.%20Despite%20the%20increasing%20use%0Aof%20embeddings%20and%20language%20models%20for%20this%20task%2C%20approaches%20for%20generating%0Aexpressive%20correspondences%20still%20do%20not%20take%20full%20advantage%20of%20these%20models%2C%20in%0Aparticular%2C%20large%20language%20models%20%28LLMs%29.%20This%20paper%20proposes%20to%20integrate%20LLMs%0Ainto%20an%20approach%20for%20generating%20expressive%20correspondences%20based%20on%20alignment%0Aneed%20and%20ABox-based%20relation%20discovery.%20The%20generation%20of%20correspondences%20is%0Aperformed%20by%20matching%20similar%20surroundings%20of%20instance%20sub-graphs.%20The%0Aintegration%20of%20LLMs%20results%20in%20different%20architectural%20modifications%2C%20including%0Alabel%20similarity%2C%20sub-graph%20matching%2C%20and%20entity%20matching.%20The%20performance%20word%0Aembeddings%2C%20sentence%20embeddings%2C%20and%20LLM-based%20embeddings%2C%20was%20compared.%20The%0Aresults%20demonstrate%20that%20integrating%20LLMs%20surpasses%20all%20other%20models%2C%20enhancing%0Athe%20baseline%20version%20of%20the%20approach%20with%20a%2045%5C%25%20increase%20in%20F-measure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComplex%2520Ontology%2520Matching%2520with%2520Large%2520Language%2520Model%2520Embeddings%26entry.906535625%3DGuilherme%2520Sousa%2520and%2520Rinaldo%2520Lima%2520and%2520Cassia%2520Trojahn%26entry.1292438233%3D%2520%2520Ontology%252C%2520and%2520more%2520broadly%252C%2520Knowledge%2520Graph%2520Matching%2520is%2520a%2520challenging%2520task%2520in%250Awhich%2520expressiveness%2520has%2520not%2520been%2520fully%2520addressed.%2520Despite%2520the%2520increasing%2520use%250Aof%2520embeddings%2520and%2520language%2520models%2520for%2520this%2520task%252C%2520approaches%2520for%2520generating%250Aexpressive%2520correspondences%2520still%2520do%2520not%2520take%2520full%2520advantage%2520of%2520these%2520models%252C%2520in%250Aparticular%252C%2520large%2520language%2520models%2520%2528LLMs%2529.%2520This%2520paper%2520proposes%2520to%2520integrate%2520LLMs%250Ainto%2520an%2520approach%2520for%2520generating%2520expressive%2520correspondences%2520based%2520on%2520alignment%250Aneed%2520and%2520ABox-based%2520relation%2520discovery.%2520The%2520generation%2520of%2520correspondences%2520is%250Aperformed%2520by%2520matching%2520similar%2520surroundings%2520of%2520instance%2520sub-graphs.%2520The%250Aintegration%2520of%2520LLMs%2520results%2520in%2520different%2520architectural%2520modifications%252C%2520including%250Alabel%2520similarity%252C%2520sub-graph%2520matching%252C%2520and%2520entity%2520matching.%2520The%2520performance%2520word%250Aembeddings%252C%2520sentence%2520embeddings%252C%2520and%2520LLM-based%2520embeddings%252C%2520was%2520compared.%2520The%250Aresults%2520demonstrate%2520that%2520integrating%2520LLMs%2520surpasses%2520all%2520other%2520models%252C%2520enhancing%250Athe%2520baseline%2520version%2520of%2520the%2520approach%2520with%2520a%252045%255C%2525%2520increase%2520in%2520F-measure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Complex%20Ontology%20Matching%20with%20Large%20Language%20Model%20Embeddings&entry.906535625=Guilherme%20Sousa%20and%20Rinaldo%20Lima%20and%20Cassia%20Trojahn&entry.1292438233=%20%20Ontology%2C%20and%20more%20broadly%2C%20Knowledge%20Graph%20Matching%20is%20a%20challenging%20task%20in%0Awhich%20expressiveness%20has%20not%20been%20fully%20addressed.%20Despite%20the%20increasing%20use%0Aof%20embeddings%20and%20language%20models%20for%20this%20task%2C%20approaches%20for%20generating%0Aexpressive%20correspondences%20still%20do%20not%20take%20full%20advantage%20of%20these%20models%2C%20in%0Aparticular%2C%20large%20language%20models%20%28LLMs%29.%20This%20paper%20proposes%20to%20integrate%20LLMs%0Ainto%20an%20approach%20for%20generating%20expressive%20correspondences%20based%20on%20alignment%0Aneed%20and%20ABox-based%20relation%20discovery.%20The%20generation%20of%20correspondences%20is%0Aperformed%20by%20matching%20similar%20surroundings%20of%20instance%20sub-graphs.%20The%0Aintegration%20of%20LLMs%20results%20in%20different%20architectural%20modifications%2C%20including%0Alabel%20similarity%2C%20sub-graph%20matching%2C%20and%20entity%20matching.%20The%20performance%20word%0Aembeddings%2C%20sentence%20embeddings%2C%20and%20LLM-based%20embeddings%2C%20was%20compared.%20The%0Aresults%20demonstrate%20that%20integrating%20LLMs%20surpasses%20all%20other%20models%2C%20enhancing%0Athe%20baseline%20version%20of%20the%20approach%20with%20a%2045%5C%25%20increase%20in%20F-measure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13619v1&entry.124074799=Read"},
{"title": "IM360: Textured Mesh Reconstruction for Large-scale Indoor Mapping with\n  360$^\\circ$ Cameras", "author": "Dongki Jung and Jaehoon Choi and Yonghan Lee and Dinesh Manocha", "abstract": "  We present a novel 3D reconstruction pipeline for 360$^\\circ$ cameras for 3D\nmapping and rendering of indoor environments. Traditional Structure-from-Motion\n(SfM) methods may not work well in large-scale indoor scenes due to the\nprevalence of textureless and repetitive regions. To overcome these challenges,\nour approach (IM360) leverages the wide field of view of omnidirectional images\nand integrates the spherical camera model into every core component of the SfM\npipeline. In order to develop a comprehensive 3D reconstruction solution, we\nintegrate a neural implicit surface reconstruction technique to generate\nhigh-quality surfaces from sparse input data. Additionally, we utilize a\nmesh-based neural rendering approach to refine texture maps and accurately\ncapture view-dependent properties by combining diffuse and specular components.\nWe evaluate our pipeline on large-scale indoor scenes from the Matterport3D and\nStanford2D3D datasets. In practice, IM360 demonstrate superior performance in\nterms of textured mesh reconstruction over SOTA. We observe accuracy\nimprovements in terms of camera localization and registration as well as\nrendering high frequency details.\n", "link": "http://arxiv.org/abs/2502.12545v2", "date": "2025-02-19", "relevancy": 2.5672, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6426}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6416}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IM360%3A%20Textured%20Mesh%20Reconstruction%20for%20Large-scale%20Indoor%20Mapping%20with%0A%20%20360%24%5E%5Ccirc%24%20Cameras&body=Title%3A%20IM360%3A%20Textured%20Mesh%20Reconstruction%20for%20Large-scale%20Indoor%20Mapping%20with%0A%20%20360%24%5E%5Ccirc%24%20Cameras%0AAuthor%3A%20Dongki%20Jung%20and%20Jaehoon%20Choi%20and%20Yonghan%20Lee%20and%20Dinesh%20Manocha%0AAbstract%3A%20%20%20We%20present%20a%20novel%203D%20reconstruction%20pipeline%20for%20360%24%5E%5Ccirc%24%20cameras%20for%203D%0Amapping%20and%20rendering%20of%20indoor%20environments.%20Traditional%20Structure-from-Motion%0A%28SfM%29%20methods%20may%20not%20work%20well%20in%20large-scale%20indoor%20scenes%20due%20to%20the%0Aprevalence%20of%20textureless%20and%20repetitive%20regions.%20To%20overcome%20these%20challenges%2C%0Aour%20approach%20%28IM360%29%20leverages%20the%20wide%20field%20of%20view%20of%20omnidirectional%20images%0Aand%20integrates%20the%20spherical%20camera%20model%20into%20every%20core%20component%20of%20the%20SfM%0Apipeline.%20In%20order%20to%20develop%20a%20comprehensive%203D%20reconstruction%20solution%2C%20we%0Aintegrate%20a%20neural%20implicit%20surface%20reconstruction%20technique%20to%20generate%0Ahigh-quality%20surfaces%20from%20sparse%20input%20data.%20Additionally%2C%20we%20utilize%20a%0Amesh-based%20neural%20rendering%20approach%20to%20refine%20texture%20maps%20and%20accurately%0Acapture%20view-dependent%20properties%20by%20combining%20diffuse%20and%20specular%20components.%0AWe%20evaluate%20our%20pipeline%20on%20large-scale%20indoor%20scenes%20from%20the%20Matterport3D%20and%0AStanford2D3D%20datasets.%20In%20practice%2C%20IM360%20demonstrate%20superior%20performance%20in%0Aterms%20of%20textured%20mesh%20reconstruction%20over%20SOTA.%20We%20observe%20accuracy%0Aimprovements%20in%20terms%20of%20camera%20localization%20and%20registration%20as%20well%20as%0Arendering%20high%20frequency%20details.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12545v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIM360%253A%2520Textured%2520Mesh%2520Reconstruction%2520for%2520Large-scale%2520Indoor%2520Mapping%2520with%250A%2520%2520360%2524%255E%255Ccirc%2524%2520Cameras%26entry.906535625%3DDongki%2520Jung%2520and%2520Jaehoon%2520Choi%2520and%2520Yonghan%2520Lee%2520and%2520Dinesh%2520Manocha%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%25203D%2520reconstruction%2520pipeline%2520for%2520360%2524%255E%255Ccirc%2524%2520cameras%2520for%25203D%250Amapping%2520and%2520rendering%2520of%2520indoor%2520environments.%2520Traditional%2520Structure-from-Motion%250A%2528SfM%2529%2520methods%2520may%2520not%2520work%2520well%2520in%2520large-scale%2520indoor%2520scenes%2520due%2520to%2520the%250Aprevalence%2520of%2520textureless%2520and%2520repetitive%2520regions.%2520To%2520overcome%2520these%2520challenges%252C%250Aour%2520approach%2520%2528IM360%2529%2520leverages%2520the%2520wide%2520field%2520of%2520view%2520of%2520omnidirectional%2520images%250Aand%2520integrates%2520the%2520spherical%2520camera%2520model%2520into%2520every%2520core%2520component%2520of%2520the%2520SfM%250Apipeline.%2520In%2520order%2520to%2520develop%2520a%2520comprehensive%25203D%2520reconstruction%2520solution%252C%2520we%250Aintegrate%2520a%2520neural%2520implicit%2520surface%2520reconstruction%2520technique%2520to%2520generate%250Ahigh-quality%2520surfaces%2520from%2520sparse%2520input%2520data.%2520Additionally%252C%2520we%2520utilize%2520a%250Amesh-based%2520neural%2520rendering%2520approach%2520to%2520refine%2520texture%2520maps%2520and%2520accurately%250Acapture%2520view-dependent%2520properties%2520by%2520combining%2520diffuse%2520and%2520specular%2520components.%250AWe%2520evaluate%2520our%2520pipeline%2520on%2520large-scale%2520indoor%2520scenes%2520from%2520the%2520Matterport3D%2520and%250AStanford2D3D%2520datasets.%2520In%2520practice%252C%2520IM360%2520demonstrate%2520superior%2520performance%2520in%250Aterms%2520of%2520textured%2520mesh%2520reconstruction%2520over%2520SOTA.%2520We%2520observe%2520accuracy%250Aimprovements%2520in%2520terms%2520of%2520camera%2520localization%2520and%2520registration%2520as%2520well%2520as%250Arendering%2520high%2520frequency%2520details.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12545v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IM360%3A%20Textured%20Mesh%20Reconstruction%20for%20Large-scale%20Indoor%20Mapping%20with%0A%20%20360%24%5E%5Ccirc%24%20Cameras&entry.906535625=Dongki%20Jung%20and%20Jaehoon%20Choi%20and%20Yonghan%20Lee%20and%20Dinesh%20Manocha&entry.1292438233=%20%20We%20present%20a%20novel%203D%20reconstruction%20pipeline%20for%20360%24%5E%5Ccirc%24%20cameras%20for%203D%0Amapping%20and%20rendering%20of%20indoor%20environments.%20Traditional%20Structure-from-Motion%0A%28SfM%29%20methods%20may%20not%20work%20well%20in%20large-scale%20indoor%20scenes%20due%20to%20the%0Aprevalence%20of%20textureless%20and%20repetitive%20regions.%20To%20overcome%20these%20challenges%2C%0Aour%20approach%20%28IM360%29%20leverages%20the%20wide%20field%20of%20view%20of%20omnidirectional%20images%0Aand%20integrates%20the%20spherical%20camera%20model%20into%20every%20core%20component%20of%20the%20SfM%0Apipeline.%20In%20order%20to%20develop%20a%20comprehensive%203D%20reconstruction%20solution%2C%20we%0Aintegrate%20a%20neural%20implicit%20surface%20reconstruction%20technique%20to%20generate%0Ahigh-quality%20surfaces%20from%20sparse%20input%20data.%20Additionally%2C%20we%20utilize%20a%0Amesh-based%20neural%20rendering%20approach%20to%20refine%20texture%20maps%20and%20accurately%0Acapture%20view-dependent%20properties%20by%20combining%20diffuse%20and%20specular%20components.%0AWe%20evaluate%20our%20pipeline%20on%20large-scale%20indoor%20scenes%20from%20the%20Matterport3D%20and%0AStanford2D3D%20datasets.%20In%20practice%2C%20IM360%20demonstrate%20superior%20performance%20in%0Aterms%20of%20textured%20mesh%20reconstruction%20over%20SOTA.%20We%20observe%20accuracy%0Aimprovements%20in%20terms%20of%20camera%20localization%20and%20registration%20as%20well%20as%0Arendering%20high%20frequency%20details.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12545v2&entry.124074799=Read"},
{"title": "FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer", "author": "Pavol Harar and Lukas Herrmann and Philipp Grohs and David Haselbach", "abstract": "  In cryo-electron microscopy, accurate particle localization and\nclassification are imperative. Recent deep learning solutions, though\nsuccessful, require extensive training data sets. The protracted generation\ntime of physics-based models, often employed to produce these data sets, limits\ntheir broad applicability. We introduce FakET, a method based on Neural Style\nTransfer, capable of simulating the forward operator of any cryo transmission\nelectron microscope. It can be used to adapt a synthetic training data set\naccording to reference data producing high-quality simulated micrographs or\ntilt-series. To assess the quality of our generated data, we used it to train a\nstate-of-the-art localization and classification architecture and compared its\nperformance with a counterpart trained on benchmark data. Remarkably, our\ntechnique matches the performance, boosts data generation speed 750 times, uses\n33 times less memory, and scales well to typical transmission electron\nmicroscope detector sizes. It leverages GPU acceleration and parallel\nprocessing. The source code is available at https://github.com/paloha/faket.\n", "link": "http://arxiv.org/abs/2304.02011v4", "date": "2025-02-19", "relevancy": 2.5661, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5256}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5073}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FakET%3A%20Simulating%20Cryo-Electron%20Tomograms%20with%20Neural%20Style%20Transfer&body=Title%3A%20FakET%3A%20Simulating%20Cryo-Electron%20Tomograms%20with%20Neural%20Style%20Transfer%0AAuthor%3A%20Pavol%20Harar%20and%20Lukas%20Herrmann%20and%20Philipp%20Grohs%20and%20David%20Haselbach%0AAbstract%3A%20%20%20In%20cryo-electron%20microscopy%2C%20accurate%20particle%20localization%20and%0Aclassification%20are%20imperative.%20Recent%20deep%20learning%20solutions%2C%20though%0Asuccessful%2C%20require%20extensive%20training%20data%20sets.%20The%20protracted%20generation%0Atime%20of%20physics-based%20models%2C%20often%20employed%20to%20produce%20these%20data%20sets%2C%20limits%0Atheir%20broad%20applicability.%20We%20introduce%20FakET%2C%20a%20method%20based%20on%20Neural%20Style%0ATransfer%2C%20capable%20of%20simulating%20the%20forward%20operator%20of%20any%20cryo%20transmission%0Aelectron%20microscope.%20It%20can%20be%20used%20to%20adapt%20a%20synthetic%20training%20data%20set%0Aaccording%20to%20reference%20data%20producing%20high-quality%20simulated%20micrographs%20or%0Atilt-series.%20To%20assess%20the%20quality%20of%20our%20generated%20data%2C%20we%20used%20it%20to%20train%20a%0Astate-of-the-art%20localization%20and%20classification%20architecture%20and%20compared%20its%0Aperformance%20with%20a%20counterpart%20trained%20on%20benchmark%20data.%20Remarkably%2C%20our%0Atechnique%20matches%20the%20performance%2C%20boosts%20data%20generation%20speed%20750%20times%2C%20uses%0A33%20times%20less%20memory%2C%20and%20scales%20well%20to%20typical%20transmission%20electron%0Amicroscope%20detector%20sizes.%20It%20leverages%20GPU%20acceleration%20and%20parallel%0Aprocessing.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/paloha/faket.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.02011v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFakET%253A%2520Simulating%2520Cryo-Electron%2520Tomograms%2520with%2520Neural%2520Style%2520Transfer%26entry.906535625%3DPavol%2520Harar%2520and%2520Lukas%2520Herrmann%2520and%2520Philipp%2520Grohs%2520and%2520David%2520Haselbach%26entry.1292438233%3D%2520%2520In%2520cryo-electron%2520microscopy%252C%2520accurate%2520particle%2520localization%2520and%250Aclassification%2520are%2520imperative.%2520Recent%2520deep%2520learning%2520solutions%252C%2520though%250Asuccessful%252C%2520require%2520extensive%2520training%2520data%2520sets.%2520The%2520protracted%2520generation%250Atime%2520of%2520physics-based%2520models%252C%2520often%2520employed%2520to%2520produce%2520these%2520data%2520sets%252C%2520limits%250Atheir%2520broad%2520applicability.%2520We%2520introduce%2520FakET%252C%2520a%2520method%2520based%2520on%2520Neural%2520Style%250ATransfer%252C%2520capable%2520of%2520simulating%2520the%2520forward%2520operator%2520of%2520any%2520cryo%2520transmission%250Aelectron%2520microscope.%2520It%2520can%2520be%2520used%2520to%2520adapt%2520a%2520synthetic%2520training%2520data%2520set%250Aaccording%2520to%2520reference%2520data%2520producing%2520high-quality%2520simulated%2520micrographs%2520or%250Atilt-series.%2520To%2520assess%2520the%2520quality%2520of%2520our%2520generated%2520data%252C%2520we%2520used%2520it%2520to%2520train%2520a%250Astate-of-the-art%2520localization%2520and%2520classification%2520architecture%2520and%2520compared%2520its%250Aperformance%2520with%2520a%2520counterpart%2520trained%2520on%2520benchmark%2520data.%2520Remarkably%252C%2520our%250Atechnique%2520matches%2520the%2520performance%252C%2520boosts%2520data%2520generation%2520speed%2520750%2520times%252C%2520uses%250A33%2520times%2520less%2520memory%252C%2520and%2520scales%2520well%2520to%2520typical%2520transmission%2520electron%250Amicroscope%2520detector%2520sizes.%2520It%2520leverages%2520GPU%2520acceleration%2520and%2520parallel%250Aprocessing.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/paloha/faket.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.02011v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FakET%3A%20Simulating%20Cryo-Electron%20Tomograms%20with%20Neural%20Style%20Transfer&entry.906535625=Pavol%20Harar%20and%20Lukas%20Herrmann%20and%20Philipp%20Grohs%20and%20David%20Haselbach&entry.1292438233=%20%20In%20cryo-electron%20microscopy%2C%20accurate%20particle%20localization%20and%0Aclassification%20are%20imperative.%20Recent%20deep%20learning%20solutions%2C%20though%0Asuccessful%2C%20require%20extensive%20training%20data%20sets.%20The%20protracted%20generation%0Atime%20of%20physics-based%20models%2C%20often%20employed%20to%20produce%20these%20data%20sets%2C%20limits%0Atheir%20broad%20applicability.%20We%20introduce%20FakET%2C%20a%20method%20based%20on%20Neural%20Style%0ATransfer%2C%20capable%20of%20simulating%20the%20forward%20operator%20of%20any%20cryo%20transmission%0Aelectron%20microscope.%20It%20can%20be%20used%20to%20adapt%20a%20synthetic%20training%20data%20set%0Aaccording%20to%20reference%20data%20producing%20high-quality%20simulated%20micrographs%20or%0Atilt-series.%20To%20assess%20the%20quality%20of%20our%20generated%20data%2C%20we%20used%20it%20to%20train%20a%0Astate-of-the-art%20localization%20and%20classification%20architecture%20and%20compared%20its%0Aperformance%20with%20a%20counterpart%20trained%20on%20benchmark%20data.%20Remarkably%2C%20our%0Atechnique%20matches%20the%20performance%2C%20boosts%20data%20generation%20speed%20750%20times%2C%20uses%0A33%20times%20less%20memory%2C%20and%20scales%20well%20to%20typical%20transmission%20electron%0Amicroscope%20detector%20sizes.%20It%20leverages%20GPU%20acceleration%20and%20parallel%0Aprocessing.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/paloha/faket.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.02011v4&entry.124074799=Read"},
{"title": "The Computational Advantage of Depth: Learning High-Dimensional\n  Hierarchical Functions with Gradient Descent", "author": "Yatin Dandi and Luca Pesce and Lenka Zdeborov\u00e1 and Florent Krzakala", "abstract": "  Understanding the advantages of deep neural networks trained by gradient\ndescent (GD) compared to shallow models remains an open theoretical challenge.\nWhile the study of multi-index models with Gaussian data in high dimensions has\nprovided analytical insights into the benefits of GD-trained neural networks\nover kernels, the role of depth in improving sample complexity and\ngeneralization in GD-trained networks remains poorly understood. In this paper,\nwe introduce a class of target functions (single and multi-index Gaussian\nhierarchical targets) that incorporate a hierarchy of latent subspace\ndimensionalities. This framework enables us to analytically study the learning\ndynamics and generalization performance of deep networks compared to shallow\nones in the high-dimensional limit. Specifically, our main theorem shows that\nfeature learning with GD reduces the effective dimensionality, transforming a\nhigh-dimensional problem into a sequence of lower-dimensional ones. This\nenables learning the target function with drastically less samples than with\nshallow networks. While the results are proven in a controlled training\nsetting, we also discuss more common training procedures and argue that they\nlearn through the same mechanisms. These findings open the way to further\nquantitative studies of the crucial role of depth in learning hierarchical\nstructures with deep networks.\n", "link": "http://arxiv.org/abs/2502.13961v1", "date": "2025-02-19", "relevancy": 2.5632, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5168}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5124}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Computational%20Advantage%20of%20Depth%3A%20Learning%20High-Dimensional%0A%20%20Hierarchical%20Functions%20with%20Gradient%20Descent&body=Title%3A%20The%20Computational%20Advantage%20of%20Depth%3A%20Learning%20High-Dimensional%0A%20%20Hierarchical%20Functions%20with%20Gradient%20Descent%0AAuthor%3A%20Yatin%20Dandi%20and%20Luca%20Pesce%20and%20Lenka%20Zdeborov%C3%A1%20and%20Florent%20Krzakala%0AAbstract%3A%20%20%20Understanding%20the%20advantages%20of%20deep%20neural%20networks%20trained%20by%20gradient%0Adescent%20%28GD%29%20compared%20to%20shallow%20models%20remains%20an%20open%20theoretical%20challenge.%0AWhile%20the%20study%20of%20multi-index%20models%20with%20Gaussian%20data%20in%20high%20dimensions%20has%0Aprovided%20analytical%20insights%20into%20the%20benefits%20of%20GD-trained%20neural%20networks%0Aover%20kernels%2C%20the%20role%20of%20depth%20in%20improving%20sample%20complexity%20and%0Ageneralization%20in%20GD-trained%20networks%20remains%20poorly%20understood.%20In%20this%20paper%2C%0Awe%20introduce%20a%20class%20of%20target%20functions%20%28single%20and%20multi-index%20Gaussian%0Ahierarchical%20targets%29%20that%20incorporate%20a%20hierarchy%20of%20latent%20subspace%0Adimensionalities.%20This%20framework%20enables%20us%20to%20analytically%20study%20the%20learning%0Adynamics%20and%20generalization%20performance%20of%20deep%20networks%20compared%20to%20shallow%0Aones%20in%20the%20high-dimensional%20limit.%20Specifically%2C%20our%20main%20theorem%20shows%20that%0Afeature%20learning%20with%20GD%20reduces%20the%20effective%20dimensionality%2C%20transforming%20a%0Ahigh-dimensional%20problem%20into%20a%20sequence%20of%20lower-dimensional%20ones.%20This%0Aenables%20learning%20the%20target%20function%20with%20drastically%20less%20samples%20than%20with%0Ashallow%20networks.%20While%20the%20results%20are%20proven%20in%20a%20controlled%20training%0Asetting%2C%20we%20also%20discuss%20more%20common%20training%20procedures%20and%20argue%20that%20they%0Alearn%20through%20the%20same%20mechanisms.%20These%20findings%20open%20the%20way%20to%20further%0Aquantitative%20studies%20of%20the%20crucial%20role%20of%20depth%20in%20learning%20hierarchical%0Astructures%20with%20deep%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13961v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Computational%2520Advantage%2520of%2520Depth%253A%2520Learning%2520High-Dimensional%250A%2520%2520Hierarchical%2520Functions%2520with%2520Gradient%2520Descent%26entry.906535625%3DYatin%2520Dandi%2520and%2520Luca%2520Pesce%2520and%2520Lenka%2520Zdeborov%25C3%25A1%2520and%2520Florent%2520Krzakala%26entry.1292438233%3D%2520%2520Understanding%2520the%2520advantages%2520of%2520deep%2520neural%2520networks%2520trained%2520by%2520gradient%250Adescent%2520%2528GD%2529%2520compared%2520to%2520shallow%2520models%2520remains%2520an%2520open%2520theoretical%2520challenge.%250AWhile%2520the%2520study%2520of%2520multi-index%2520models%2520with%2520Gaussian%2520data%2520in%2520high%2520dimensions%2520has%250Aprovided%2520analytical%2520insights%2520into%2520the%2520benefits%2520of%2520GD-trained%2520neural%2520networks%250Aover%2520kernels%252C%2520the%2520role%2520of%2520depth%2520in%2520improving%2520sample%2520complexity%2520and%250Ageneralization%2520in%2520GD-trained%2520networks%2520remains%2520poorly%2520understood.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520a%2520class%2520of%2520target%2520functions%2520%2528single%2520and%2520multi-index%2520Gaussian%250Ahierarchical%2520targets%2529%2520that%2520incorporate%2520a%2520hierarchy%2520of%2520latent%2520subspace%250Adimensionalities.%2520This%2520framework%2520enables%2520us%2520to%2520analytically%2520study%2520the%2520learning%250Adynamics%2520and%2520generalization%2520performance%2520of%2520deep%2520networks%2520compared%2520to%2520shallow%250Aones%2520in%2520the%2520high-dimensional%2520limit.%2520Specifically%252C%2520our%2520main%2520theorem%2520shows%2520that%250Afeature%2520learning%2520with%2520GD%2520reduces%2520the%2520effective%2520dimensionality%252C%2520transforming%2520a%250Ahigh-dimensional%2520problem%2520into%2520a%2520sequence%2520of%2520lower-dimensional%2520ones.%2520This%250Aenables%2520learning%2520the%2520target%2520function%2520with%2520drastically%2520less%2520samples%2520than%2520with%250Ashallow%2520networks.%2520While%2520the%2520results%2520are%2520proven%2520in%2520a%2520controlled%2520training%250Asetting%252C%2520we%2520also%2520discuss%2520more%2520common%2520training%2520procedures%2520and%2520argue%2520that%2520they%250Alearn%2520through%2520the%2520same%2520mechanisms.%2520These%2520findings%2520open%2520the%2520way%2520to%2520further%250Aquantitative%2520studies%2520of%2520the%2520crucial%2520role%2520of%2520depth%2520in%2520learning%2520hierarchical%250Astructures%2520with%2520deep%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13961v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Computational%20Advantage%20of%20Depth%3A%20Learning%20High-Dimensional%0A%20%20Hierarchical%20Functions%20with%20Gradient%20Descent&entry.906535625=Yatin%20Dandi%20and%20Luca%20Pesce%20and%20Lenka%20Zdeborov%C3%A1%20and%20Florent%20Krzakala&entry.1292438233=%20%20Understanding%20the%20advantages%20of%20deep%20neural%20networks%20trained%20by%20gradient%0Adescent%20%28GD%29%20compared%20to%20shallow%20models%20remains%20an%20open%20theoretical%20challenge.%0AWhile%20the%20study%20of%20multi-index%20models%20with%20Gaussian%20data%20in%20high%20dimensions%20has%0Aprovided%20analytical%20insights%20into%20the%20benefits%20of%20GD-trained%20neural%20networks%0Aover%20kernels%2C%20the%20role%20of%20depth%20in%20improving%20sample%20complexity%20and%0Ageneralization%20in%20GD-trained%20networks%20remains%20poorly%20understood.%20In%20this%20paper%2C%0Awe%20introduce%20a%20class%20of%20target%20functions%20%28single%20and%20multi-index%20Gaussian%0Ahierarchical%20targets%29%20that%20incorporate%20a%20hierarchy%20of%20latent%20subspace%0Adimensionalities.%20This%20framework%20enables%20us%20to%20analytically%20study%20the%20learning%0Adynamics%20and%20generalization%20performance%20of%20deep%20networks%20compared%20to%20shallow%0Aones%20in%20the%20high-dimensional%20limit.%20Specifically%2C%20our%20main%20theorem%20shows%20that%0Afeature%20learning%20with%20GD%20reduces%20the%20effective%20dimensionality%2C%20transforming%20a%0Ahigh-dimensional%20problem%20into%20a%20sequence%20of%20lower-dimensional%20ones.%20This%0Aenables%20learning%20the%20target%20function%20with%20drastically%20less%20samples%20than%20with%0Ashallow%20networks.%20While%20the%20results%20are%20proven%20in%20a%20controlled%20training%0Asetting%2C%20we%20also%20discuss%20more%20common%20training%20procedures%20and%20argue%20that%20they%0Alearn%20through%20the%20same%20mechanisms.%20These%20findings%20open%20the%20way%20to%20further%0Aquantitative%20studies%20of%20the%20crucial%20role%20of%20depth%20in%20learning%20hierarchical%0Astructures%20with%20deep%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13961v1&entry.124074799=Read"},
{"title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large\n  Language Models", "author": "Shuqi Liu and Han Wu and Bowei He and Xiongwei Han and Mingxuan Yuan and Linqi Song", "abstract": "  Recent advances in large language models have led to numerous\ntask-specialized fine-tuned variants, creating a need for efficient model\nmerging techniques that preserve specialized capabilities while avoiding costly\nretraining. While existing task vector-based merging methods show promise, they\ntypically apply uniform coefficients across all parameters, overlooking varying\nparameter importance both within and across tasks. We present Sens-Merging, a\nsensitivity-guided coefficient adjustment method that enhances existing model\nmerging techniques by operating at both task-specific and cross-task levels.\nOur method analyzes parameter sensitivity within individual tasks and evaluates\ncross-task transferability to determine optimal merging coefficients. Extensive\nexperiments on Mistral 7B and LLaMA2-7B/13B models demonstrate that\nSens-Merging significantly improves performance across general knowledge,\nmathematical reasoning, and code generation tasks. Notably, when combined with\nexisting merging techniques, our method enables merged models to outperform\nspecialized fine-tuned models, particularly in code generation tasks. Our\nfindings reveal important trade-offs between task-specific and cross-task\nscalings, providing insights for future model merging strategies.\n", "link": "http://arxiv.org/abs/2502.12420v2", "date": "2025-02-19", "relevancy": 2.5523, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sens-Merging%3A%20Sensitivity-Guided%20Parameter%20Balancing%20for%20Merging%20Large%0A%20%20Language%20Models&body=Title%3A%20Sens-Merging%3A%20Sensitivity-Guided%20Parameter%20Balancing%20for%20Merging%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Shuqi%20Liu%20and%20Han%20Wu%20and%20Bowei%20He%20and%20Xiongwei%20Han%20and%20Mingxuan%20Yuan%20and%20Linqi%20Song%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20have%20led%20to%20numerous%0Atask-specialized%20fine-tuned%20variants%2C%20creating%20a%20need%20for%20efficient%20model%0Amerging%20techniques%20that%20preserve%20specialized%20capabilities%20while%20avoiding%20costly%0Aretraining.%20While%20existing%20task%20vector-based%20merging%20methods%20show%20promise%2C%20they%0Atypically%20apply%20uniform%20coefficients%20across%20all%20parameters%2C%20overlooking%20varying%0Aparameter%20importance%20both%20within%20and%20across%20tasks.%20We%20present%20Sens-Merging%2C%20a%0Asensitivity-guided%20coefficient%20adjustment%20method%20that%20enhances%20existing%20model%0Amerging%20techniques%20by%20operating%20at%20both%20task-specific%20and%20cross-task%20levels.%0AOur%20method%20analyzes%20parameter%20sensitivity%20within%20individual%20tasks%20and%20evaluates%0Across-task%20transferability%20to%20determine%20optimal%20merging%20coefficients.%20Extensive%0Aexperiments%20on%20Mistral%207B%20and%20LLaMA2-7B/13B%20models%20demonstrate%20that%0ASens-Merging%20significantly%20improves%20performance%20across%20general%20knowledge%2C%0Amathematical%20reasoning%2C%20and%20code%20generation%20tasks.%20Notably%2C%20when%20combined%20with%0Aexisting%20merging%20techniques%2C%20our%20method%20enables%20merged%20models%20to%20outperform%0Aspecialized%20fine-tuned%20models%2C%20particularly%20in%20code%20generation%20tasks.%20Our%0Afindings%20reveal%20important%20trade-offs%20between%20task-specific%20and%20cross-task%0Ascalings%2C%20providing%20insights%20for%20future%20model%20merging%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12420v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSens-Merging%253A%2520Sensitivity-Guided%2520Parameter%2520Balancing%2520for%2520Merging%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DShuqi%2520Liu%2520and%2520Han%2520Wu%2520and%2520Bowei%2520He%2520and%2520Xiongwei%2520Han%2520and%2520Mingxuan%2520Yuan%2520and%2520Linqi%2520Song%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520have%2520led%2520to%2520numerous%250Atask-specialized%2520fine-tuned%2520variants%252C%2520creating%2520a%2520need%2520for%2520efficient%2520model%250Amerging%2520techniques%2520that%2520preserve%2520specialized%2520capabilities%2520while%2520avoiding%2520costly%250Aretraining.%2520While%2520existing%2520task%2520vector-based%2520merging%2520methods%2520show%2520promise%252C%2520they%250Atypically%2520apply%2520uniform%2520coefficients%2520across%2520all%2520parameters%252C%2520overlooking%2520varying%250Aparameter%2520importance%2520both%2520within%2520and%2520across%2520tasks.%2520We%2520present%2520Sens-Merging%252C%2520a%250Asensitivity-guided%2520coefficient%2520adjustment%2520method%2520that%2520enhances%2520existing%2520model%250Amerging%2520techniques%2520by%2520operating%2520at%2520both%2520task-specific%2520and%2520cross-task%2520levels.%250AOur%2520method%2520analyzes%2520parameter%2520sensitivity%2520within%2520individual%2520tasks%2520and%2520evaluates%250Across-task%2520transferability%2520to%2520determine%2520optimal%2520merging%2520coefficients.%2520Extensive%250Aexperiments%2520on%2520Mistral%25207B%2520and%2520LLaMA2-7B/13B%2520models%2520demonstrate%2520that%250ASens-Merging%2520significantly%2520improves%2520performance%2520across%2520general%2520knowledge%252C%250Amathematical%2520reasoning%252C%2520and%2520code%2520generation%2520tasks.%2520Notably%252C%2520when%2520combined%2520with%250Aexisting%2520merging%2520techniques%252C%2520our%2520method%2520enables%2520merged%2520models%2520to%2520outperform%250Aspecialized%2520fine-tuned%2520models%252C%2520particularly%2520in%2520code%2520generation%2520tasks.%2520Our%250Afindings%2520reveal%2520important%2520trade-offs%2520between%2520task-specific%2520and%2520cross-task%250Ascalings%252C%2520providing%2520insights%2520for%2520future%2520model%2520merging%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12420v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sens-Merging%3A%20Sensitivity-Guided%20Parameter%20Balancing%20for%20Merging%20Large%0A%20%20Language%20Models&entry.906535625=Shuqi%20Liu%20and%20Han%20Wu%20and%20Bowei%20He%20and%20Xiongwei%20Han%20and%20Mingxuan%20Yuan%20and%20Linqi%20Song&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20have%20led%20to%20numerous%0Atask-specialized%20fine-tuned%20variants%2C%20creating%20a%20need%20for%20efficient%20model%0Amerging%20techniques%20that%20preserve%20specialized%20capabilities%20while%20avoiding%20costly%0Aretraining.%20While%20existing%20task%20vector-based%20merging%20methods%20show%20promise%2C%20they%0Atypically%20apply%20uniform%20coefficients%20across%20all%20parameters%2C%20overlooking%20varying%0Aparameter%20importance%20both%20within%20and%20across%20tasks.%20We%20present%20Sens-Merging%2C%20a%0Asensitivity-guided%20coefficient%20adjustment%20method%20that%20enhances%20existing%20model%0Amerging%20techniques%20by%20operating%20at%20both%20task-specific%20and%20cross-task%20levels.%0AOur%20method%20analyzes%20parameter%20sensitivity%20within%20individual%20tasks%20and%20evaluates%0Across-task%20transferability%20to%20determine%20optimal%20merging%20coefficients.%20Extensive%0Aexperiments%20on%20Mistral%207B%20and%20LLaMA2-7B/13B%20models%20demonstrate%20that%0ASens-Merging%20significantly%20improves%20performance%20across%20general%20knowledge%2C%0Amathematical%20reasoning%2C%20and%20code%20generation%20tasks.%20Notably%2C%20when%20combined%20with%0Aexisting%20merging%20techniques%2C%20our%20method%20enables%20merged%20models%20to%20outperform%0Aspecialized%20fine-tuned%20models%2C%20particularly%20in%20code%20generation%20tasks.%20Our%0Afindings%20reveal%20important%20trade-offs%20between%20task-specific%20and%20cross-task%0Ascalings%2C%20providing%20insights%20for%20future%20model%20merging%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12420v2&entry.124074799=Read"},
{"title": "Carefully Blending Adversarial Training, Purification, and Aggregation\n  Improves Adversarial Robustness", "author": "Emanuele Ballarin and Alessio Ansuini and Luca Bortolussi", "abstract": "  In this work, we propose a novel adversarial defence mechanism for image\nclassification - CARSO - blending the paradigms of adversarial training and\nadversarial purification in a synergistic robustness-enhancing way. The method\nbuilds upon an adversarially-trained classifier, and learns to map its internal\nrepresentation associated with a potentially perturbed input onto a\ndistribution of tentative clean reconstructions. Multiple samples from such\ndistribution are classified by the same adversarially-trained model, and a\ncarefully chosen aggregation of its outputs finally constitutes the robust\nprediction of interest. Experimental evaluation by a well-established benchmark\nof strong adaptive attacks, across different image datasets, shows that CARSO\nis able to defend itself against adaptive end-to-end white-box attacks devised\nfor stochastic defences. Paying a modest clean accuracy toll, our method\nimproves by a significant margin the state-of-the-art for Cifar-10, Cifar-100,\nand TinyImageNet-200 $\\ell_\\infty$ robust classification accuracy against\nAutoAttack. Code, and instructions to obtain pre-trained models are available\nat: https://github.com/emaballarin/CARSO .\n", "link": "http://arxiv.org/abs/2306.06081v5", "date": "2025-02-19", "relevancy": 2.5241, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5397}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4883}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Carefully%20Blending%20Adversarial%20Training%2C%20Purification%2C%20and%20Aggregation%0A%20%20Improves%20Adversarial%20Robustness&body=Title%3A%20Carefully%20Blending%20Adversarial%20Training%2C%20Purification%2C%20and%20Aggregation%0A%20%20Improves%20Adversarial%20Robustness%0AAuthor%3A%20Emanuele%20Ballarin%20and%20Alessio%20Ansuini%20and%20Luca%20Bortolussi%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20adversarial%20defence%20mechanism%20for%20image%0Aclassification%20-%20CARSO%20-%20blending%20the%20paradigms%20of%20adversarial%20training%20and%0Aadversarial%20purification%20in%20a%20synergistic%20robustness-enhancing%20way.%20The%20method%0Abuilds%20upon%20an%20adversarially-trained%20classifier%2C%20and%20learns%20to%20map%20its%20internal%0Arepresentation%20associated%20with%20a%20potentially%20perturbed%20input%20onto%20a%0Adistribution%20of%20tentative%20clean%20reconstructions.%20Multiple%20samples%20from%20such%0Adistribution%20are%20classified%20by%20the%20same%20adversarially-trained%20model%2C%20and%20a%0Acarefully%20chosen%20aggregation%20of%20its%20outputs%20finally%20constitutes%20the%20robust%0Aprediction%20of%20interest.%20Experimental%20evaluation%20by%20a%20well-established%20benchmark%0Aof%20strong%20adaptive%20attacks%2C%20across%20different%20image%20datasets%2C%20shows%20that%20CARSO%0Ais%20able%20to%20defend%20itself%20against%20adaptive%20end-to-end%20white-box%20attacks%20devised%0Afor%20stochastic%20defences.%20Paying%20a%20modest%20clean%20accuracy%20toll%2C%20our%20method%0Aimproves%20by%20a%20significant%20margin%20the%20state-of-the-art%20for%20Cifar-10%2C%20Cifar-100%2C%0Aand%20TinyImageNet-200%20%24%5Cell_%5Cinfty%24%20robust%20classification%20accuracy%20against%0AAutoAttack.%20Code%2C%20and%20instructions%20to%20obtain%20pre-trained%20models%20are%20available%0Aat%3A%20https%3A//github.com/emaballarin/CARSO%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.06081v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCarefully%2520Blending%2520Adversarial%2520Training%252C%2520Purification%252C%2520and%2520Aggregation%250A%2520%2520Improves%2520Adversarial%2520Robustness%26entry.906535625%3DEmanuele%2520Ballarin%2520and%2520Alessio%2520Ansuini%2520and%2520Luca%2520Bortolussi%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520adversarial%2520defence%2520mechanism%2520for%2520image%250Aclassification%2520-%2520CARSO%2520-%2520blending%2520the%2520paradigms%2520of%2520adversarial%2520training%2520and%250Aadversarial%2520purification%2520in%2520a%2520synergistic%2520robustness-enhancing%2520way.%2520The%2520method%250Abuilds%2520upon%2520an%2520adversarially-trained%2520classifier%252C%2520and%2520learns%2520to%2520map%2520its%2520internal%250Arepresentation%2520associated%2520with%2520a%2520potentially%2520perturbed%2520input%2520onto%2520a%250Adistribution%2520of%2520tentative%2520clean%2520reconstructions.%2520Multiple%2520samples%2520from%2520such%250Adistribution%2520are%2520classified%2520by%2520the%2520same%2520adversarially-trained%2520model%252C%2520and%2520a%250Acarefully%2520chosen%2520aggregation%2520of%2520its%2520outputs%2520finally%2520constitutes%2520the%2520robust%250Aprediction%2520of%2520interest.%2520Experimental%2520evaluation%2520by%2520a%2520well-established%2520benchmark%250Aof%2520strong%2520adaptive%2520attacks%252C%2520across%2520different%2520image%2520datasets%252C%2520shows%2520that%2520CARSO%250Ais%2520able%2520to%2520defend%2520itself%2520against%2520adaptive%2520end-to-end%2520white-box%2520attacks%2520devised%250Afor%2520stochastic%2520defences.%2520Paying%2520a%2520modest%2520clean%2520accuracy%2520toll%252C%2520our%2520method%250Aimproves%2520by%2520a%2520significant%2520margin%2520the%2520state-of-the-art%2520for%2520Cifar-10%252C%2520Cifar-100%252C%250Aand%2520TinyImageNet-200%2520%2524%255Cell_%255Cinfty%2524%2520robust%2520classification%2520accuracy%2520against%250AAutoAttack.%2520Code%252C%2520and%2520instructions%2520to%2520obtain%2520pre-trained%2520models%2520are%2520available%250Aat%253A%2520https%253A//github.com/emaballarin/CARSO%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.06081v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Carefully%20Blending%20Adversarial%20Training%2C%20Purification%2C%20and%20Aggregation%0A%20%20Improves%20Adversarial%20Robustness&entry.906535625=Emanuele%20Ballarin%20and%20Alessio%20Ansuini%20and%20Luca%20Bortolussi&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20adversarial%20defence%20mechanism%20for%20image%0Aclassification%20-%20CARSO%20-%20blending%20the%20paradigms%20of%20adversarial%20training%20and%0Aadversarial%20purification%20in%20a%20synergistic%20robustness-enhancing%20way.%20The%20method%0Abuilds%20upon%20an%20adversarially-trained%20classifier%2C%20and%20learns%20to%20map%20its%20internal%0Arepresentation%20associated%20with%20a%20potentially%20perturbed%20input%20onto%20a%0Adistribution%20of%20tentative%20clean%20reconstructions.%20Multiple%20samples%20from%20such%0Adistribution%20are%20classified%20by%20the%20same%20adversarially-trained%20model%2C%20and%20a%0Acarefully%20chosen%20aggregation%20of%20its%20outputs%20finally%20constitutes%20the%20robust%0Aprediction%20of%20interest.%20Experimental%20evaluation%20by%20a%20well-established%20benchmark%0Aof%20strong%20adaptive%20attacks%2C%20across%20different%20image%20datasets%2C%20shows%20that%20CARSO%0Ais%20able%20to%20defend%20itself%20against%20adaptive%20end-to-end%20white-box%20attacks%20devised%0Afor%20stochastic%20defences.%20Paying%20a%20modest%20clean%20accuracy%20toll%2C%20our%20method%0Aimproves%20by%20a%20significant%20margin%20the%20state-of-the-art%20for%20Cifar-10%2C%20Cifar-100%2C%0Aand%20TinyImageNet-200%20%24%5Cell_%5Cinfty%24%20robust%20classification%20accuracy%20against%0AAutoAttack.%20Code%2C%20and%20instructions%20to%20obtain%20pre-trained%20models%20are%20available%0Aat%3A%20https%3A//github.com/emaballarin/CARSO%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.06081v5&entry.124074799=Read"},
{"title": "MAAT: Mamba Adaptive Anomaly Transformer with association discrepancy\n  for time series", "author": "Abdellah Zakaria Sellam and Ilyes Benaissa and Abdelmalik Taleb-Ahmed and Luigi Patrono and Cosimo Distante", "abstract": "  Anomaly detection in time series is essential for industrial monitoring and\nenvironmental sensing, yet distinguishing anomalies from complex patterns\nremains challenging. Existing methods like the Anomaly Transformer and\nDCdetector have progressed, but they face limitations such as sensitivity to\nshort-term contexts and inefficiency in noisy, non-stationary environments.\n  To overcome these issues, we introduce MAAT, an improved architecture that\nenhances association discrepancy modeling and reconstruction quality. MAAT\nfeatures Sparse Attention, efficiently capturing long-range dependencies by\nfocusing on relevant time steps, thereby reducing computational redundancy.\nAdditionally, a Mamba-Selective State Space Model is incorporated into the\nreconstruction module, utilizing a skip connection and Gated Attention to\nimprove anomaly localization and detection performance.\n  Extensive experiments show that MAAT significantly outperforms previous\nmethods, achieving better anomaly distinguishability and generalization across\nvarious time series applications, setting a new standard for unsupervised time\nseries anomaly detection in real-world scenarios.\n", "link": "http://arxiv.org/abs/2502.07858v2", "date": "2025-02-19", "relevancy": 2.5189, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5083}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5075}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAAT%3A%20Mamba%20Adaptive%20Anomaly%20Transformer%20with%20association%20discrepancy%0A%20%20for%20time%20series&body=Title%3A%20MAAT%3A%20Mamba%20Adaptive%20Anomaly%20Transformer%20with%20association%20discrepancy%0A%20%20for%20time%20series%0AAuthor%3A%20Abdellah%20Zakaria%20Sellam%20and%20Ilyes%20Benaissa%20and%20Abdelmalik%20Taleb-Ahmed%20and%20Luigi%20Patrono%20and%20Cosimo%20Distante%0AAbstract%3A%20%20%20Anomaly%20detection%20in%20time%20series%20is%20essential%20for%20industrial%20monitoring%20and%0Aenvironmental%20sensing%2C%20yet%20distinguishing%20anomalies%20from%20complex%20patterns%0Aremains%20challenging.%20Existing%20methods%20like%20the%20Anomaly%20Transformer%20and%0ADCdetector%20have%20progressed%2C%20but%20they%20face%20limitations%20such%20as%20sensitivity%20to%0Ashort-term%20contexts%20and%20inefficiency%20in%20noisy%2C%20non-stationary%20environments.%0A%20%20To%20overcome%20these%20issues%2C%20we%20introduce%20MAAT%2C%20an%20improved%20architecture%20that%0Aenhances%20association%20discrepancy%20modeling%20and%20reconstruction%20quality.%20MAAT%0Afeatures%20Sparse%20Attention%2C%20efficiently%20capturing%20long-range%20dependencies%20by%0Afocusing%20on%20relevant%20time%20steps%2C%20thereby%20reducing%20computational%20redundancy.%0AAdditionally%2C%20a%20Mamba-Selective%20State%20Space%20Model%20is%20incorporated%20into%20the%0Areconstruction%20module%2C%20utilizing%20a%20skip%20connection%20and%20Gated%20Attention%20to%0Aimprove%20anomaly%20localization%20and%20detection%20performance.%0A%20%20Extensive%20experiments%20show%20that%20MAAT%20significantly%20outperforms%20previous%0Amethods%2C%20achieving%20better%20anomaly%20distinguishability%20and%20generalization%20across%0Avarious%20time%20series%20applications%2C%20setting%20a%20new%20standard%20for%20unsupervised%20time%0Aseries%20anomaly%20detection%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07858v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAAT%253A%2520Mamba%2520Adaptive%2520Anomaly%2520Transformer%2520with%2520association%2520discrepancy%250A%2520%2520for%2520time%2520series%26entry.906535625%3DAbdellah%2520Zakaria%2520Sellam%2520and%2520Ilyes%2520Benaissa%2520and%2520Abdelmalik%2520Taleb-Ahmed%2520and%2520Luigi%2520Patrono%2520and%2520Cosimo%2520Distante%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520in%2520time%2520series%2520is%2520essential%2520for%2520industrial%2520monitoring%2520and%250Aenvironmental%2520sensing%252C%2520yet%2520distinguishing%2520anomalies%2520from%2520complex%2520patterns%250Aremains%2520challenging.%2520Existing%2520methods%2520like%2520the%2520Anomaly%2520Transformer%2520and%250ADCdetector%2520have%2520progressed%252C%2520but%2520they%2520face%2520limitations%2520such%2520as%2520sensitivity%2520to%250Ashort-term%2520contexts%2520and%2520inefficiency%2520in%2520noisy%252C%2520non-stationary%2520environments.%250A%2520%2520To%2520overcome%2520these%2520issues%252C%2520we%2520introduce%2520MAAT%252C%2520an%2520improved%2520architecture%2520that%250Aenhances%2520association%2520discrepancy%2520modeling%2520and%2520reconstruction%2520quality.%2520MAAT%250Afeatures%2520Sparse%2520Attention%252C%2520efficiently%2520capturing%2520long-range%2520dependencies%2520by%250Afocusing%2520on%2520relevant%2520time%2520steps%252C%2520thereby%2520reducing%2520computational%2520redundancy.%250AAdditionally%252C%2520a%2520Mamba-Selective%2520State%2520Space%2520Model%2520is%2520incorporated%2520into%2520the%250Areconstruction%2520module%252C%2520utilizing%2520a%2520skip%2520connection%2520and%2520Gated%2520Attention%2520to%250Aimprove%2520anomaly%2520localization%2520and%2520detection%2520performance.%250A%2520%2520Extensive%2520experiments%2520show%2520that%2520MAAT%2520significantly%2520outperforms%2520previous%250Amethods%252C%2520achieving%2520better%2520anomaly%2520distinguishability%2520and%2520generalization%2520across%250Avarious%2520time%2520series%2520applications%252C%2520setting%2520a%2520new%2520standard%2520for%2520unsupervised%2520time%250Aseries%2520anomaly%2520detection%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07858v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAAT%3A%20Mamba%20Adaptive%20Anomaly%20Transformer%20with%20association%20discrepancy%0A%20%20for%20time%20series&entry.906535625=Abdellah%20Zakaria%20Sellam%20and%20Ilyes%20Benaissa%20and%20Abdelmalik%20Taleb-Ahmed%20and%20Luigi%20Patrono%20and%20Cosimo%20Distante&entry.1292438233=%20%20Anomaly%20detection%20in%20time%20series%20is%20essential%20for%20industrial%20monitoring%20and%0Aenvironmental%20sensing%2C%20yet%20distinguishing%20anomalies%20from%20complex%20patterns%0Aremains%20challenging.%20Existing%20methods%20like%20the%20Anomaly%20Transformer%20and%0ADCdetector%20have%20progressed%2C%20but%20they%20face%20limitations%20such%20as%20sensitivity%20to%0Ashort-term%20contexts%20and%20inefficiency%20in%20noisy%2C%20non-stationary%20environments.%0A%20%20To%20overcome%20these%20issues%2C%20we%20introduce%20MAAT%2C%20an%20improved%20architecture%20that%0Aenhances%20association%20discrepancy%20modeling%20and%20reconstruction%20quality.%20MAAT%0Afeatures%20Sparse%20Attention%2C%20efficiently%20capturing%20long-range%20dependencies%20by%0Afocusing%20on%20relevant%20time%20steps%2C%20thereby%20reducing%20computational%20redundancy.%0AAdditionally%2C%20a%20Mamba-Selective%20State%20Space%20Model%20is%20incorporated%20into%20the%0Areconstruction%20module%2C%20utilizing%20a%20skip%20connection%20and%20Gated%20Attention%20to%0Aimprove%20anomaly%20localization%20and%20detection%20performance.%0A%20%20Extensive%20experiments%20show%20that%20MAAT%20significantly%20outperforms%20previous%0Amethods%2C%20achieving%20better%20anomaly%20distinguishability%20and%20generalization%20across%0Avarious%20time%20series%20applications%2C%20setting%20a%20new%20standard%20for%20unsupervised%20time%0Aseries%20anomaly%20detection%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07858v2&entry.124074799=Read"},
{"title": "Theory on Mixture-of-Experts in Continual Learning", "author": "Hongbo Li and Sen Lin and Lingjie Duan and Yingbin Liang and Ness B. Shroff", "abstract": "  Continual learning (CL) has garnered significant attention because of its\nability to adapt to new tasks that arrive over time. Catastrophic forgetting\n(of old tasks) has been identified as a major issue in CL, as the model adapts\nto new tasks. The Mixture-of-Experts (MoE) model has recently been shown to\neffectively mitigate catastrophic forgetting in CL, by employing a gating\nnetwork to sparsify and distribute diverse tasks among multiple experts.\nHowever, there is a lack of theoretical analysis of MoE and its impact on the\nlearning performance in CL. This paper provides the first theoretical results\nto characterize the impact of MoE in CL via the lens of overparameterized\nlinear regression tasks. We establish the benefit of MoE over a single expert\nby proving that the MoE model can diversify its experts to specialize in\ndifferent tasks, while its router learns to select the right expert for each\ntask and balance the loads across all experts. Our study further suggests an\nintriguing fact that the MoE in CL needs to terminate the update of the gating\nnetwork after sufficient training rounds to attain system convergence, which is\nnot needed in the existing MoE studies that do not consider the continual task\narrival. Furthermore, we provide explicit expressions for the expected\nforgetting and overall generalization error to characterize the benefit of MoE\nin the learning performance in CL. Interestingly, adding more experts requires\nadditional rounds before convergence, which may not enhance the learning\nperformance. Finally, we conduct experiments on both synthetic and real\ndatasets to extend these insights from linear models to deep neural networks\n(DNNs), which also shed light on the practical algorithm design for MoE in CL.\n", "link": "http://arxiv.org/abs/2406.16437v3", "date": "2025-02-19", "relevancy": 2.5095, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5277}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4924}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Theory%20on%20Mixture-of-Experts%20in%20Continual%20Learning&body=Title%3A%20Theory%20on%20Mixture-of-Experts%20in%20Continual%20Learning%0AAuthor%3A%20Hongbo%20Li%20and%20Sen%20Lin%20and%20Lingjie%20Duan%20and%20Yingbin%20Liang%20and%20Ness%20B.%20Shroff%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20has%20garnered%20significant%20attention%20because%20of%20its%0Aability%20to%20adapt%20to%20new%20tasks%20that%20arrive%20over%20time.%20Catastrophic%20forgetting%0A%28of%20old%20tasks%29%20has%20been%20identified%20as%20a%20major%20issue%20in%20CL%2C%20as%20the%20model%20adapts%0Ato%20new%20tasks.%20The%20Mixture-of-Experts%20%28MoE%29%20model%20has%20recently%20been%20shown%20to%0Aeffectively%20mitigate%20catastrophic%20forgetting%20in%20CL%2C%20by%20employing%20a%20gating%0Anetwork%20to%20sparsify%20and%20distribute%20diverse%20tasks%20among%20multiple%20experts.%0AHowever%2C%20there%20is%20a%20lack%20of%20theoretical%20analysis%20of%20MoE%20and%20its%20impact%20on%20the%0Alearning%20performance%20in%20CL.%20This%20paper%20provides%20the%20first%20theoretical%20results%0Ato%20characterize%20the%20impact%20of%20MoE%20in%20CL%20via%20the%20lens%20of%20overparameterized%0Alinear%20regression%20tasks.%20We%20establish%20the%20benefit%20of%20MoE%20over%20a%20single%20expert%0Aby%20proving%20that%20the%20MoE%20model%20can%20diversify%20its%20experts%20to%20specialize%20in%0Adifferent%20tasks%2C%20while%20its%20router%20learns%20to%20select%20the%20right%20expert%20for%20each%0Atask%20and%20balance%20the%20loads%20across%20all%20experts.%20Our%20study%20further%20suggests%20an%0Aintriguing%20fact%20that%20the%20MoE%20in%20CL%20needs%20to%20terminate%20the%20update%20of%20the%20gating%0Anetwork%20after%20sufficient%20training%20rounds%20to%20attain%20system%20convergence%2C%20which%20is%0Anot%20needed%20in%20the%20existing%20MoE%20studies%20that%20do%20not%20consider%20the%20continual%20task%0Aarrival.%20Furthermore%2C%20we%20provide%20explicit%20expressions%20for%20the%20expected%0Aforgetting%20and%20overall%20generalization%20error%20to%20characterize%20the%20benefit%20of%20MoE%0Ain%20the%20learning%20performance%20in%20CL.%20Interestingly%2C%20adding%20more%20experts%20requires%0Aadditional%20rounds%20before%20convergence%2C%20which%20may%20not%20enhance%20the%20learning%0Aperformance.%20Finally%2C%20we%20conduct%20experiments%20on%20both%20synthetic%20and%20real%0Adatasets%20to%20extend%20these%20insights%20from%20linear%20models%20to%20deep%20neural%20networks%0A%28DNNs%29%2C%20which%20also%20shed%20light%20on%20the%20practical%20algorithm%20design%20for%20MoE%20in%20CL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16437v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTheory%2520on%2520Mixture-of-Experts%2520in%2520Continual%2520Learning%26entry.906535625%3DHongbo%2520Li%2520and%2520Sen%2520Lin%2520and%2520Lingjie%2520Duan%2520and%2520Yingbin%2520Liang%2520and%2520Ness%2520B.%2520Shroff%26entry.1292438233%3D%2520%2520Continual%2520learning%2520%2528CL%2529%2520has%2520garnered%2520significant%2520attention%2520because%2520of%2520its%250Aability%2520to%2520adapt%2520to%2520new%2520tasks%2520that%2520arrive%2520over%2520time.%2520Catastrophic%2520forgetting%250A%2528of%2520old%2520tasks%2529%2520has%2520been%2520identified%2520as%2520a%2520major%2520issue%2520in%2520CL%252C%2520as%2520the%2520model%2520adapts%250Ato%2520new%2520tasks.%2520The%2520Mixture-of-Experts%2520%2528MoE%2529%2520model%2520has%2520recently%2520been%2520shown%2520to%250Aeffectively%2520mitigate%2520catastrophic%2520forgetting%2520in%2520CL%252C%2520by%2520employing%2520a%2520gating%250Anetwork%2520to%2520sparsify%2520and%2520distribute%2520diverse%2520tasks%2520among%2520multiple%2520experts.%250AHowever%252C%2520there%2520is%2520a%2520lack%2520of%2520theoretical%2520analysis%2520of%2520MoE%2520and%2520its%2520impact%2520on%2520the%250Alearning%2520performance%2520in%2520CL.%2520This%2520paper%2520provides%2520the%2520first%2520theoretical%2520results%250Ato%2520characterize%2520the%2520impact%2520of%2520MoE%2520in%2520CL%2520via%2520the%2520lens%2520of%2520overparameterized%250Alinear%2520regression%2520tasks.%2520We%2520establish%2520the%2520benefit%2520of%2520MoE%2520over%2520a%2520single%2520expert%250Aby%2520proving%2520that%2520the%2520MoE%2520model%2520can%2520diversify%2520its%2520experts%2520to%2520specialize%2520in%250Adifferent%2520tasks%252C%2520while%2520its%2520router%2520learns%2520to%2520select%2520the%2520right%2520expert%2520for%2520each%250Atask%2520and%2520balance%2520the%2520loads%2520across%2520all%2520experts.%2520Our%2520study%2520further%2520suggests%2520an%250Aintriguing%2520fact%2520that%2520the%2520MoE%2520in%2520CL%2520needs%2520to%2520terminate%2520the%2520update%2520of%2520the%2520gating%250Anetwork%2520after%2520sufficient%2520training%2520rounds%2520to%2520attain%2520system%2520convergence%252C%2520which%2520is%250Anot%2520needed%2520in%2520the%2520existing%2520MoE%2520studies%2520that%2520do%2520not%2520consider%2520the%2520continual%2520task%250Aarrival.%2520Furthermore%252C%2520we%2520provide%2520explicit%2520expressions%2520for%2520the%2520expected%250Aforgetting%2520and%2520overall%2520generalization%2520error%2520to%2520characterize%2520the%2520benefit%2520of%2520MoE%250Ain%2520the%2520learning%2520performance%2520in%2520CL.%2520Interestingly%252C%2520adding%2520more%2520experts%2520requires%250Aadditional%2520rounds%2520before%2520convergence%252C%2520which%2520may%2520not%2520enhance%2520the%2520learning%250Aperformance.%2520Finally%252C%2520we%2520conduct%2520experiments%2520on%2520both%2520synthetic%2520and%2520real%250Adatasets%2520to%2520extend%2520these%2520insights%2520from%2520linear%2520models%2520to%2520deep%2520neural%2520networks%250A%2528DNNs%2529%252C%2520which%2520also%2520shed%2520light%2520on%2520the%2520practical%2520algorithm%2520design%2520for%2520MoE%2520in%2520CL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16437v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Theory%20on%20Mixture-of-Experts%20in%20Continual%20Learning&entry.906535625=Hongbo%20Li%20and%20Sen%20Lin%20and%20Lingjie%20Duan%20and%20Yingbin%20Liang%20and%20Ness%20B.%20Shroff&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20has%20garnered%20significant%20attention%20because%20of%20its%0Aability%20to%20adapt%20to%20new%20tasks%20that%20arrive%20over%20time.%20Catastrophic%20forgetting%0A%28of%20old%20tasks%29%20has%20been%20identified%20as%20a%20major%20issue%20in%20CL%2C%20as%20the%20model%20adapts%0Ato%20new%20tasks.%20The%20Mixture-of-Experts%20%28MoE%29%20model%20has%20recently%20been%20shown%20to%0Aeffectively%20mitigate%20catastrophic%20forgetting%20in%20CL%2C%20by%20employing%20a%20gating%0Anetwork%20to%20sparsify%20and%20distribute%20diverse%20tasks%20among%20multiple%20experts.%0AHowever%2C%20there%20is%20a%20lack%20of%20theoretical%20analysis%20of%20MoE%20and%20its%20impact%20on%20the%0Alearning%20performance%20in%20CL.%20This%20paper%20provides%20the%20first%20theoretical%20results%0Ato%20characterize%20the%20impact%20of%20MoE%20in%20CL%20via%20the%20lens%20of%20overparameterized%0Alinear%20regression%20tasks.%20We%20establish%20the%20benefit%20of%20MoE%20over%20a%20single%20expert%0Aby%20proving%20that%20the%20MoE%20model%20can%20diversify%20its%20experts%20to%20specialize%20in%0Adifferent%20tasks%2C%20while%20its%20router%20learns%20to%20select%20the%20right%20expert%20for%20each%0Atask%20and%20balance%20the%20loads%20across%20all%20experts.%20Our%20study%20further%20suggests%20an%0Aintriguing%20fact%20that%20the%20MoE%20in%20CL%20needs%20to%20terminate%20the%20update%20of%20the%20gating%0Anetwork%20after%20sufficient%20training%20rounds%20to%20attain%20system%20convergence%2C%20which%20is%0Anot%20needed%20in%20the%20existing%20MoE%20studies%20that%20do%20not%20consider%20the%20continual%20task%0Aarrival.%20Furthermore%2C%20we%20provide%20explicit%20expressions%20for%20the%20expected%0Aforgetting%20and%20overall%20generalization%20error%20to%20characterize%20the%20benefit%20of%20MoE%0Ain%20the%20learning%20performance%20in%20CL.%20Interestingly%2C%20adding%20more%20experts%20requires%0Aadditional%20rounds%20before%20convergence%2C%20which%20may%20not%20enhance%20the%20learning%0Aperformance.%20Finally%2C%20we%20conduct%20experiments%20on%20both%20synthetic%20and%20real%0Adatasets%20to%20extend%20these%20insights%20from%20linear%20models%20to%20deep%20neural%20networks%0A%28DNNs%29%2C%20which%20also%20shed%20light%20on%20the%20practical%20algorithm%20design%20for%20MoE%20in%20CL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16437v3&entry.124074799=Read"},
{"title": "V2C-Long: Longitudinal Cortex Reconstruction with Spatiotemporal\n  Correspondence", "author": "Fabian Bongratz and Jan Fecht and Anne-Marie Rickmann and Christian Wachinger", "abstract": "  Reconstructing the cortex from longitudinal magnetic resonance imaging (MRI)\nis indispensable for analyzing morphological alterations in the human brain.\nDespite the recent advancement of cortical surface reconstruction with deep\nlearning, challenges arising from longitudinal data are still persistent.\nEspecially the lack of strong spatiotemporal point correspondence between\nhighly convoluted brain surfaces hinders downstream analyses, as local\nmorphology is not directly comparable if the anatomical location is not matched\nprecisely. To address this issue, we present V2C-Long, the first dedicated deep\nlearning-based cortex reconstruction method for longitudinal MRI. V2C-Long\nexhibits strong inherent spatiotemporal correspondence across subjects and\nvisits, thereby reducing the need for surface-based post-processing. We\nestablish this correspondence directly during the reconstruction via the\ncomposition of two deep template-deformation networks and innovative\naggregation of within-subject templates in mesh space. We validate V2C-Long on\ntwo large neuroimaging studies, focusing on surface accuracy, consistency,\ngeneralization, test-retest reliability, and sensitivity. The results reveal a\nsubstantial improvement in longitudinal consistency and accuracy compared to\nexisting methods. In addition, we demonstrate stronger evidence for\nlongitudinal cortical atrophy in Alzheimer's disease than longitudinal\nFreeSurfer.\n", "link": "http://arxiv.org/abs/2402.17438v2", "date": "2025-02-19", "relevancy": 2.482, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5101}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4895}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V2C-Long%3A%20Longitudinal%20Cortex%20Reconstruction%20with%20Spatiotemporal%0A%20%20Correspondence&body=Title%3A%20V2C-Long%3A%20Longitudinal%20Cortex%20Reconstruction%20with%20Spatiotemporal%0A%20%20Correspondence%0AAuthor%3A%20Fabian%20Bongratz%20and%20Jan%20Fecht%20and%20Anne-Marie%20Rickmann%20and%20Christian%20Wachinger%0AAbstract%3A%20%20%20Reconstructing%20the%20cortex%20from%20longitudinal%20magnetic%20resonance%20imaging%20%28MRI%29%0Ais%20indispensable%20for%20analyzing%20morphological%20alterations%20in%20the%20human%20brain.%0ADespite%20the%20recent%20advancement%20of%20cortical%20surface%20reconstruction%20with%20deep%0Alearning%2C%20challenges%20arising%20from%20longitudinal%20data%20are%20still%20persistent.%0AEspecially%20the%20lack%20of%20strong%20spatiotemporal%20point%20correspondence%20between%0Ahighly%20convoluted%20brain%20surfaces%20hinders%20downstream%20analyses%2C%20as%20local%0Amorphology%20is%20not%20directly%20comparable%20if%20the%20anatomical%20location%20is%20not%20matched%0Aprecisely.%20To%20address%20this%20issue%2C%20we%20present%20V2C-Long%2C%20the%20first%20dedicated%20deep%0Alearning-based%20cortex%20reconstruction%20method%20for%20longitudinal%20MRI.%20V2C-Long%0Aexhibits%20strong%20inherent%20spatiotemporal%20correspondence%20across%20subjects%20and%0Avisits%2C%20thereby%20reducing%20the%20need%20for%20surface-based%20post-processing.%20We%0Aestablish%20this%20correspondence%20directly%20during%20the%20reconstruction%20via%20the%0Acomposition%20of%20two%20deep%20template-deformation%20networks%20and%20innovative%0Aaggregation%20of%20within-subject%20templates%20in%20mesh%20space.%20We%20validate%20V2C-Long%20on%0Atwo%20large%20neuroimaging%20studies%2C%20focusing%20on%20surface%20accuracy%2C%20consistency%2C%0Ageneralization%2C%20test-retest%20reliability%2C%20and%20sensitivity.%20The%20results%20reveal%20a%0Asubstantial%20improvement%20in%20longitudinal%20consistency%20and%20accuracy%20compared%20to%0Aexisting%20methods.%20In%20addition%2C%20we%20demonstrate%20stronger%20evidence%20for%0Alongitudinal%20cortical%20atrophy%20in%20Alzheimer%27s%20disease%20than%20longitudinal%0AFreeSurfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17438v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV2C-Long%253A%2520Longitudinal%2520Cortex%2520Reconstruction%2520with%2520Spatiotemporal%250A%2520%2520Correspondence%26entry.906535625%3DFabian%2520Bongratz%2520and%2520Jan%2520Fecht%2520and%2520Anne-Marie%2520Rickmann%2520and%2520Christian%2520Wachinger%26entry.1292438233%3D%2520%2520Reconstructing%2520the%2520cortex%2520from%2520longitudinal%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%250Ais%2520indispensable%2520for%2520analyzing%2520morphological%2520alterations%2520in%2520the%2520human%2520brain.%250ADespite%2520the%2520recent%2520advancement%2520of%2520cortical%2520surface%2520reconstruction%2520with%2520deep%250Alearning%252C%2520challenges%2520arising%2520from%2520longitudinal%2520data%2520are%2520still%2520persistent.%250AEspecially%2520the%2520lack%2520of%2520strong%2520spatiotemporal%2520point%2520correspondence%2520between%250Ahighly%2520convoluted%2520brain%2520surfaces%2520hinders%2520downstream%2520analyses%252C%2520as%2520local%250Amorphology%2520is%2520not%2520directly%2520comparable%2520if%2520the%2520anatomical%2520location%2520is%2520not%2520matched%250Aprecisely.%2520To%2520address%2520this%2520issue%252C%2520we%2520present%2520V2C-Long%252C%2520the%2520first%2520dedicated%2520deep%250Alearning-based%2520cortex%2520reconstruction%2520method%2520for%2520longitudinal%2520MRI.%2520V2C-Long%250Aexhibits%2520strong%2520inherent%2520spatiotemporal%2520correspondence%2520across%2520subjects%2520and%250Avisits%252C%2520thereby%2520reducing%2520the%2520need%2520for%2520surface-based%2520post-processing.%2520We%250Aestablish%2520this%2520correspondence%2520directly%2520during%2520the%2520reconstruction%2520via%2520the%250Acomposition%2520of%2520two%2520deep%2520template-deformation%2520networks%2520and%2520innovative%250Aaggregation%2520of%2520within-subject%2520templates%2520in%2520mesh%2520space.%2520We%2520validate%2520V2C-Long%2520on%250Atwo%2520large%2520neuroimaging%2520studies%252C%2520focusing%2520on%2520surface%2520accuracy%252C%2520consistency%252C%250Ageneralization%252C%2520test-retest%2520reliability%252C%2520and%2520sensitivity.%2520The%2520results%2520reveal%2520a%250Asubstantial%2520improvement%2520in%2520longitudinal%2520consistency%2520and%2520accuracy%2520compared%2520to%250Aexisting%2520methods.%2520In%2520addition%252C%2520we%2520demonstrate%2520stronger%2520evidence%2520for%250Alongitudinal%2520cortical%2520atrophy%2520in%2520Alzheimer%2527s%2520disease%2520than%2520longitudinal%250AFreeSurfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17438v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V2C-Long%3A%20Longitudinal%20Cortex%20Reconstruction%20with%20Spatiotemporal%0A%20%20Correspondence&entry.906535625=Fabian%20Bongratz%20and%20Jan%20Fecht%20and%20Anne-Marie%20Rickmann%20and%20Christian%20Wachinger&entry.1292438233=%20%20Reconstructing%20the%20cortex%20from%20longitudinal%20magnetic%20resonance%20imaging%20%28MRI%29%0Ais%20indispensable%20for%20analyzing%20morphological%20alterations%20in%20the%20human%20brain.%0ADespite%20the%20recent%20advancement%20of%20cortical%20surface%20reconstruction%20with%20deep%0Alearning%2C%20challenges%20arising%20from%20longitudinal%20data%20are%20still%20persistent.%0AEspecially%20the%20lack%20of%20strong%20spatiotemporal%20point%20correspondence%20between%0Ahighly%20convoluted%20brain%20surfaces%20hinders%20downstream%20analyses%2C%20as%20local%0Amorphology%20is%20not%20directly%20comparable%20if%20the%20anatomical%20location%20is%20not%20matched%0Aprecisely.%20To%20address%20this%20issue%2C%20we%20present%20V2C-Long%2C%20the%20first%20dedicated%20deep%0Alearning-based%20cortex%20reconstruction%20method%20for%20longitudinal%20MRI.%20V2C-Long%0Aexhibits%20strong%20inherent%20spatiotemporal%20correspondence%20across%20subjects%20and%0Avisits%2C%20thereby%20reducing%20the%20need%20for%20surface-based%20post-processing.%20We%0Aestablish%20this%20correspondence%20directly%20during%20the%20reconstruction%20via%20the%0Acomposition%20of%20two%20deep%20template-deformation%20networks%20and%20innovative%0Aaggregation%20of%20within-subject%20templates%20in%20mesh%20space.%20We%20validate%20V2C-Long%20on%0Atwo%20large%20neuroimaging%20studies%2C%20focusing%20on%20surface%20accuracy%2C%20consistency%2C%0Ageneralization%2C%20test-retest%20reliability%2C%20and%20sensitivity.%20The%20results%20reveal%20a%0Asubstantial%20improvement%20in%20longitudinal%20consistency%20and%20accuracy%20compared%20to%0Aexisting%20methods.%20In%20addition%2C%20we%20demonstrate%20stronger%20evidence%20for%0Alongitudinal%20cortical%20atrophy%20in%20Alzheimer%27s%20disease%20than%20longitudinal%0AFreeSurfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17438v2&entry.124074799=Read"},
{"title": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning", "author": "Zhen Li and Yupeng Su and Runming Yang and Congkai Xie and Zheng Wang and Zhongwei Xie and Ngai Wong and Hongxia Yang", "abstract": "  Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. Our results demonstrate that aggressive\nquantization methods like AWQ and GPTQ introduce up to 32.39% accuracy\ndegradation (average 11.31%) on Llama-3 models, particularly in numerical\ncomputation and reasoning planning. To address this, we introduce a\nmultidimensional evaluation framework combining qualitative capability analysis\nand quantitative error assessment. We further develop targeted recovery\nstrategies, showing that fine-tuning quantized models on only 545 task-specific\nexamples for 3 minutes on 4 GPUs effectively restores reasoning capabilities to\nnear full-precision levels. Additionally, our error assessment pipeline\nachieves 98.9% accuracy in diagnosing and localizing errors across 3,366\nfailure cases, providing actionable insights for mitigating\nquantization-induced degradation.\n", "link": "http://arxiv.org/abs/2501.03035v3", "date": "2025-02-19", "relevancy": 2.4566, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5037}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5037}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantization%20Meets%20Reasoning%3A%20Exploring%20LLM%20Low-Bit%20Quantization%0A%20%20Degradation%20for%20Mathematical%20Reasoning&body=Title%3A%20Quantization%20Meets%20Reasoning%3A%20Exploring%20LLM%20Low-Bit%20Quantization%0A%20%20Degradation%20for%20Mathematical%20Reasoning%0AAuthor%3A%20Zhen%20Li%20and%20Yupeng%20Su%20and%20Runming%20Yang%20and%20Congkai%20Xie%20and%20Zheng%20Wang%20and%20Zhongwei%20Xie%20and%20Ngai%20Wong%20and%20Hongxia%20Yang%0AAbstract%3A%20%20%20Large%20language%20models%20have%20achieved%20significant%20advancements%20in%20complex%0Amathematical%20reasoning%20benchmarks%2C%20such%20as%20MATH.%20However%2C%20their%20substantial%0Acomputational%20requirements%20present%20challenges%20for%20practical%20deployment.%20Model%0Aquantization%20has%20emerged%20as%20an%20effective%20strategy%20to%20reduce%20memory%20usage%20and%0Acomputational%20costs%20by%20employing%20lower%20precision%20and%20bit-width%20representations.%0AIn%20this%20study%2C%20we%20systematically%20evaluate%20the%20impact%20of%20quantization%20on%0Amathematical%20reasoning%20tasks.%20Our%20results%20demonstrate%20that%20aggressive%0Aquantization%20methods%20like%20AWQ%20and%20GPTQ%20introduce%20up%20to%2032.39%25%20accuracy%0Adegradation%20%28average%2011.31%25%29%20on%20Llama-3%20models%2C%20particularly%20in%20numerical%0Acomputation%20and%20reasoning%20planning.%20To%20address%20this%2C%20we%20introduce%20a%0Amultidimensional%20evaluation%20framework%20combining%20qualitative%20capability%20analysis%0Aand%20quantitative%20error%20assessment.%20We%20further%20develop%20targeted%20recovery%0Astrategies%2C%20showing%20that%20fine-tuning%20quantized%20models%20on%20only%20545%20task-specific%0Aexamples%20for%203%20minutes%20on%204%20GPUs%20effectively%20restores%20reasoning%20capabilities%20to%0Anear%20full-precision%20levels.%20Additionally%2C%20our%20error%20assessment%20pipeline%0Aachieves%2098.9%25%20accuracy%20in%20diagnosing%20and%20localizing%20errors%20across%203%2C366%0Afailure%20cases%2C%20providing%20actionable%20insights%20for%20mitigating%0Aquantization-induced%20degradation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03035v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantization%2520Meets%2520Reasoning%253A%2520Exploring%2520LLM%2520Low-Bit%2520Quantization%250A%2520%2520Degradation%2520for%2520Mathematical%2520Reasoning%26entry.906535625%3DZhen%2520Li%2520and%2520Yupeng%2520Su%2520and%2520Runming%2520Yang%2520and%2520Congkai%2520Xie%2520and%2520Zheng%2520Wang%2520and%2520Zhongwei%2520Xie%2520and%2520Ngai%2520Wong%2520and%2520Hongxia%2520Yang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520achieved%2520significant%2520advancements%2520in%2520complex%250Amathematical%2520reasoning%2520benchmarks%252C%2520such%2520as%2520MATH.%2520However%252C%2520their%2520substantial%250Acomputational%2520requirements%2520present%2520challenges%2520for%2520practical%2520deployment.%2520Model%250Aquantization%2520has%2520emerged%2520as%2520an%2520effective%2520strategy%2520to%2520reduce%2520memory%2520usage%2520and%250Acomputational%2520costs%2520by%2520employing%2520lower%2520precision%2520and%2520bit-width%2520representations.%250AIn%2520this%2520study%252C%2520we%2520systematically%2520evaluate%2520the%2520impact%2520of%2520quantization%2520on%250Amathematical%2520reasoning%2520tasks.%2520Our%2520results%2520demonstrate%2520that%2520aggressive%250Aquantization%2520methods%2520like%2520AWQ%2520and%2520GPTQ%2520introduce%2520up%2520to%252032.39%2525%2520accuracy%250Adegradation%2520%2528average%252011.31%2525%2529%2520on%2520Llama-3%2520models%252C%2520particularly%2520in%2520numerical%250Acomputation%2520and%2520reasoning%2520planning.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%250Amultidimensional%2520evaluation%2520framework%2520combining%2520qualitative%2520capability%2520analysis%250Aand%2520quantitative%2520error%2520assessment.%2520We%2520further%2520develop%2520targeted%2520recovery%250Astrategies%252C%2520showing%2520that%2520fine-tuning%2520quantized%2520models%2520on%2520only%2520545%2520task-specific%250Aexamples%2520for%25203%2520minutes%2520on%25204%2520GPUs%2520effectively%2520restores%2520reasoning%2520capabilities%2520to%250Anear%2520full-precision%2520levels.%2520Additionally%252C%2520our%2520error%2520assessment%2520pipeline%250Aachieves%252098.9%2525%2520accuracy%2520in%2520diagnosing%2520and%2520localizing%2520errors%2520across%25203%252C366%250Afailure%2520cases%252C%2520providing%2520actionable%2520insights%2520for%2520mitigating%250Aquantization-induced%2520degradation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03035v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantization%20Meets%20Reasoning%3A%20Exploring%20LLM%20Low-Bit%20Quantization%0A%20%20Degradation%20for%20Mathematical%20Reasoning&entry.906535625=Zhen%20Li%20and%20Yupeng%20Su%20and%20Runming%20Yang%20and%20Congkai%20Xie%20and%20Zheng%20Wang%20and%20Zhongwei%20Xie%20and%20Ngai%20Wong%20and%20Hongxia%20Yang&entry.1292438233=%20%20Large%20language%20models%20have%20achieved%20significant%20advancements%20in%20complex%0Amathematical%20reasoning%20benchmarks%2C%20such%20as%20MATH.%20However%2C%20their%20substantial%0Acomputational%20requirements%20present%20challenges%20for%20practical%20deployment.%20Model%0Aquantization%20has%20emerged%20as%20an%20effective%20strategy%20to%20reduce%20memory%20usage%20and%0Acomputational%20costs%20by%20employing%20lower%20precision%20and%20bit-width%20representations.%0AIn%20this%20study%2C%20we%20systematically%20evaluate%20the%20impact%20of%20quantization%20on%0Amathematical%20reasoning%20tasks.%20Our%20results%20demonstrate%20that%20aggressive%0Aquantization%20methods%20like%20AWQ%20and%20GPTQ%20introduce%20up%20to%2032.39%25%20accuracy%0Adegradation%20%28average%2011.31%25%29%20on%20Llama-3%20models%2C%20particularly%20in%20numerical%0Acomputation%20and%20reasoning%20planning.%20To%20address%20this%2C%20we%20introduce%20a%0Amultidimensional%20evaluation%20framework%20combining%20qualitative%20capability%20analysis%0Aand%20quantitative%20error%20assessment.%20We%20further%20develop%20targeted%20recovery%0Astrategies%2C%20showing%20that%20fine-tuning%20quantized%20models%20on%20only%20545%20task-specific%0Aexamples%20for%203%20minutes%20on%204%20GPUs%20effectively%20restores%20reasoning%20capabilities%20to%0Anear%20full-precision%20levels.%20Additionally%2C%20our%20error%20assessment%20pipeline%0Aachieves%2098.9%25%20accuracy%20in%20diagnosing%20and%20localizing%20errors%20across%203%2C366%0Afailure%20cases%2C%20providing%20actionable%20insights%20for%20mitigating%0Aquantization-induced%20degradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03035v3&entry.124074799=Read"},
{"title": "C2T: A Classifier-Based Tree Construction Method in Speculative Decoding", "author": "Feiye Huo and Jianchao Tan and Kefeng Zhang and Xunliang Cai and Shengli Sun", "abstract": "  The growing scale of Large Language Models (LLMs) has exacerbated inference\nlatency and computational costs. Speculative decoding methods, which aim to\nmitigate these issues, often face inefficiencies in the construction of token\ntrees and the verification of candidate tokens. Existing strategies, including\nchain mode, static tree, and dynamic tree approaches, have limitations in\naccurately preparing candidate token trees for verification. We propose a novel\nmethod named C2T that adopts a lightweight classifier to generate and prune\ntoken trees dynamically. Our classifier considers additional feature variables\nbeyond the commonly used joint probability to predict the confidence score for\neach draft token to determine whether it is the candidate token for\nverification. This method outperforms state-of-the-art (SOTA) methods such as\nEAGLE-2 on multiple benchmarks, by reducing the total number of candidate\ntokens by 25% while maintaining or even improving the acceptance length.\n", "link": "http://arxiv.org/abs/2502.13652v1", "date": "2025-02-19", "relevancy": 2.4403, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.512}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4779}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C2T%3A%20A%20Classifier-Based%20Tree%20Construction%20Method%20in%20Speculative%20Decoding&body=Title%3A%20C2T%3A%20A%20Classifier-Based%20Tree%20Construction%20Method%20in%20Speculative%20Decoding%0AAuthor%3A%20Feiye%20Huo%20and%20Jianchao%20Tan%20and%20Kefeng%20Zhang%20and%20Xunliang%20Cai%20and%20Shengli%20Sun%0AAbstract%3A%20%20%20The%20growing%20scale%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20exacerbated%20inference%0Alatency%20and%20computational%20costs.%20Speculative%20decoding%20methods%2C%20which%20aim%20to%0Amitigate%20these%20issues%2C%20often%20face%20inefficiencies%20in%20the%20construction%20of%20token%0Atrees%20and%20the%20verification%20of%20candidate%20tokens.%20Existing%20strategies%2C%20including%0Achain%20mode%2C%20static%20tree%2C%20and%20dynamic%20tree%20approaches%2C%20have%20limitations%20in%0Aaccurately%20preparing%20candidate%20token%20trees%20for%20verification.%20We%20propose%20a%20novel%0Amethod%20named%20C2T%20that%20adopts%20a%20lightweight%20classifier%20to%20generate%20and%20prune%0Atoken%20trees%20dynamically.%20Our%20classifier%20considers%20additional%20feature%20variables%0Abeyond%20the%20commonly%20used%20joint%20probability%20to%20predict%20the%20confidence%20score%20for%0Aeach%20draft%20token%20to%20determine%20whether%20it%20is%20the%20candidate%20token%20for%0Averification.%20This%20method%20outperforms%20state-of-the-art%20%28SOTA%29%20methods%20such%20as%0AEAGLE-2%20on%20multiple%20benchmarks%2C%20by%20reducing%20the%20total%20number%20of%20candidate%0Atokens%20by%2025%25%20while%20maintaining%20or%20even%20improving%20the%20acceptance%20length.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13652v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC2T%253A%2520A%2520Classifier-Based%2520Tree%2520Construction%2520Method%2520in%2520Speculative%2520Decoding%26entry.906535625%3DFeiye%2520Huo%2520and%2520Jianchao%2520Tan%2520and%2520Kefeng%2520Zhang%2520and%2520Xunliang%2520Cai%2520and%2520Shengli%2520Sun%26entry.1292438233%3D%2520%2520The%2520growing%2520scale%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520exacerbated%2520inference%250Alatency%2520and%2520computational%2520costs.%2520Speculative%2520decoding%2520methods%252C%2520which%2520aim%2520to%250Amitigate%2520these%2520issues%252C%2520often%2520face%2520inefficiencies%2520in%2520the%2520construction%2520of%2520token%250Atrees%2520and%2520the%2520verification%2520of%2520candidate%2520tokens.%2520Existing%2520strategies%252C%2520including%250Achain%2520mode%252C%2520static%2520tree%252C%2520and%2520dynamic%2520tree%2520approaches%252C%2520have%2520limitations%2520in%250Aaccurately%2520preparing%2520candidate%2520token%2520trees%2520for%2520verification.%2520We%2520propose%2520a%2520novel%250Amethod%2520named%2520C2T%2520that%2520adopts%2520a%2520lightweight%2520classifier%2520to%2520generate%2520and%2520prune%250Atoken%2520trees%2520dynamically.%2520Our%2520classifier%2520considers%2520additional%2520feature%2520variables%250Abeyond%2520the%2520commonly%2520used%2520joint%2520probability%2520to%2520predict%2520the%2520confidence%2520score%2520for%250Aeach%2520draft%2520token%2520to%2520determine%2520whether%2520it%2520is%2520the%2520candidate%2520token%2520for%250Averification.%2520This%2520method%2520outperforms%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520such%2520as%250AEAGLE-2%2520on%2520multiple%2520benchmarks%252C%2520by%2520reducing%2520the%2520total%2520number%2520of%2520candidate%250Atokens%2520by%252025%2525%2520while%2520maintaining%2520or%2520even%2520improving%2520the%2520acceptance%2520length.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13652v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C2T%3A%20A%20Classifier-Based%20Tree%20Construction%20Method%20in%20Speculative%20Decoding&entry.906535625=Feiye%20Huo%20and%20Jianchao%20Tan%20and%20Kefeng%20Zhang%20and%20Xunliang%20Cai%20and%20Shengli%20Sun&entry.1292438233=%20%20The%20growing%20scale%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20exacerbated%20inference%0Alatency%20and%20computational%20costs.%20Speculative%20decoding%20methods%2C%20which%20aim%20to%0Amitigate%20these%20issues%2C%20often%20face%20inefficiencies%20in%20the%20construction%20of%20token%0Atrees%20and%20the%20verification%20of%20candidate%20tokens.%20Existing%20strategies%2C%20including%0Achain%20mode%2C%20static%20tree%2C%20and%20dynamic%20tree%20approaches%2C%20have%20limitations%20in%0Aaccurately%20preparing%20candidate%20token%20trees%20for%20verification.%20We%20propose%20a%20novel%0Amethod%20named%20C2T%20that%20adopts%20a%20lightweight%20classifier%20to%20generate%20and%20prune%0Atoken%20trees%20dynamically.%20Our%20classifier%20considers%20additional%20feature%20variables%0Abeyond%20the%20commonly%20used%20joint%20probability%20to%20predict%20the%20confidence%20score%20for%0Aeach%20draft%20token%20to%20determine%20whether%20it%20is%20the%20candidate%20token%20for%0Averification.%20This%20method%20outperforms%20state-of-the-art%20%28SOTA%29%20methods%20such%20as%0AEAGLE-2%20on%20multiple%20benchmarks%2C%20by%20reducing%20the%20total%20number%20of%20candidate%0Atokens%20by%2025%25%20while%20maintaining%20or%20even%20improving%20the%20acceptance%20length.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13652v1&entry.124074799=Read"},
{"title": "MGFI-Net: A Multi-Grained Feature Integration Network for Enhanced\n  Medical Image Segmentation", "author": "Yucheng Zeng", "abstract": "  Medical image segmentation plays a crucial role in various clinical\napplications. A major challenge in medical image segmentation is achieving\naccurate delineation of regions of interest in the presence of noise, low\ncontrast, or complex anatomical structures. Existing segmentation models often\nneglect the integration of multi-grained information and fail to preserve edge\ndetails, which are critical for precise segmentation. To address these\nchallenges, we propose a novel image semantic segmentation model called the\nMulti-Grained Feature Integration Network (MGFI-Net). Our MGFI-Net is designed\nwith two dedicated modules to tackle these issues. First, to enhance\nsegmentation accuracy, we introduce a Multi-Grained Feature Extraction Module,\nwhich leverages hierarchical relationships between different feature scales to\nselectively focus on the most relevant information. Second, to preserve edge\ndetails, we incorporate an Edge Enhancement Module that effectively retains and\nintegrates boundary information to refine segmentation results. Extensive\nexperiments demonstrate that MGFI-Net not only outperforms state-of-the-art\nmethods in terms of segmentation accuracy but also achieves superior time\nefficiency, establishing it as a leading solution for real-time medical image\nsegmentation.\n", "link": "http://arxiv.org/abs/2502.13808v1", "date": "2025-02-19", "relevancy": 2.4238, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4879}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4837}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MGFI-Net%3A%20A%20Multi-Grained%20Feature%20Integration%20Network%20for%20Enhanced%0A%20%20Medical%20Image%20Segmentation&body=Title%3A%20MGFI-Net%3A%20A%20Multi-Grained%20Feature%20Integration%20Network%20for%20Enhanced%0A%20%20Medical%20Image%20Segmentation%0AAuthor%3A%20Yucheng%20Zeng%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20plays%20a%20crucial%20role%20in%20various%20clinical%0Aapplications.%20A%20major%20challenge%20in%20medical%20image%20segmentation%20is%20achieving%0Aaccurate%20delineation%20of%20regions%20of%20interest%20in%20the%20presence%20of%20noise%2C%20low%0Acontrast%2C%20or%20complex%20anatomical%20structures.%20Existing%20segmentation%20models%20often%0Aneglect%20the%20integration%20of%20multi-grained%20information%20and%20fail%20to%20preserve%20edge%0Adetails%2C%20which%20are%20critical%20for%20precise%20segmentation.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20novel%20image%20semantic%20segmentation%20model%20called%20the%0AMulti-Grained%20Feature%20Integration%20Network%20%28MGFI-Net%29.%20Our%20MGFI-Net%20is%20designed%0Awith%20two%20dedicated%20modules%20to%20tackle%20these%20issues.%20First%2C%20to%20enhance%0Asegmentation%20accuracy%2C%20we%20introduce%20a%20Multi-Grained%20Feature%20Extraction%20Module%2C%0Awhich%20leverages%20hierarchical%20relationships%20between%20different%20feature%20scales%20to%0Aselectively%20focus%20on%20the%20most%20relevant%20information.%20Second%2C%20to%20preserve%20edge%0Adetails%2C%20we%20incorporate%20an%20Edge%20Enhancement%20Module%20that%20effectively%20retains%20and%0Aintegrates%20boundary%20information%20to%20refine%20segmentation%20results.%20Extensive%0Aexperiments%20demonstrate%20that%20MGFI-Net%20not%20only%20outperforms%20state-of-the-art%0Amethods%20in%20terms%20of%20segmentation%20accuracy%20but%20also%20achieves%20superior%20time%0Aefficiency%2C%20establishing%20it%20as%20a%20leading%20solution%20for%20real-time%20medical%20image%0Asegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMGFI-Net%253A%2520A%2520Multi-Grained%2520Feature%2520Integration%2520Network%2520for%2520Enhanced%250A%2520%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DYucheng%2520Zeng%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520plays%2520a%2520crucial%2520role%2520in%2520various%2520clinical%250Aapplications.%2520A%2520major%2520challenge%2520in%2520medical%2520image%2520segmentation%2520is%2520achieving%250Aaccurate%2520delineation%2520of%2520regions%2520of%2520interest%2520in%2520the%2520presence%2520of%2520noise%252C%2520low%250Acontrast%252C%2520or%2520complex%2520anatomical%2520structures.%2520Existing%2520segmentation%2520models%2520often%250Aneglect%2520the%2520integration%2520of%2520multi-grained%2520information%2520and%2520fail%2520to%2520preserve%2520edge%250Adetails%252C%2520which%2520are%2520critical%2520for%2520precise%2520segmentation.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520novel%2520image%2520semantic%2520segmentation%2520model%2520called%2520the%250AMulti-Grained%2520Feature%2520Integration%2520Network%2520%2528MGFI-Net%2529.%2520Our%2520MGFI-Net%2520is%2520designed%250Awith%2520two%2520dedicated%2520modules%2520to%2520tackle%2520these%2520issues.%2520First%252C%2520to%2520enhance%250Asegmentation%2520accuracy%252C%2520we%2520introduce%2520a%2520Multi-Grained%2520Feature%2520Extraction%2520Module%252C%250Awhich%2520leverages%2520hierarchical%2520relationships%2520between%2520different%2520feature%2520scales%2520to%250Aselectively%2520focus%2520on%2520the%2520most%2520relevant%2520information.%2520Second%252C%2520to%2520preserve%2520edge%250Adetails%252C%2520we%2520incorporate%2520an%2520Edge%2520Enhancement%2520Module%2520that%2520effectively%2520retains%2520and%250Aintegrates%2520boundary%2520information%2520to%2520refine%2520segmentation%2520results.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520MGFI-Net%2520not%2520only%2520outperforms%2520state-of-the-art%250Amethods%2520in%2520terms%2520of%2520segmentation%2520accuracy%2520but%2520also%2520achieves%2520superior%2520time%250Aefficiency%252C%2520establishing%2520it%2520as%2520a%2520leading%2520solution%2520for%2520real-time%2520medical%2520image%250Asegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MGFI-Net%3A%20A%20Multi-Grained%20Feature%20Integration%20Network%20for%20Enhanced%0A%20%20Medical%20Image%20Segmentation&entry.906535625=Yucheng%20Zeng&entry.1292438233=%20%20Medical%20image%20segmentation%20plays%20a%20crucial%20role%20in%20various%20clinical%0Aapplications.%20A%20major%20challenge%20in%20medical%20image%20segmentation%20is%20achieving%0Aaccurate%20delineation%20of%20regions%20of%20interest%20in%20the%20presence%20of%20noise%2C%20low%0Acontrast%2C%20or%20complex%20anatomical%20structures.%20Existing%20segmentation%20models%20often%0Aneglect%20the%20integration%20of%20multi-grained%20information%20and%20fail%20to%20preserve%20edge%0Adetails%2C%20which%20are%20critical%20for%20precise%20segmentation.%20To%20address%20these%0Achallenges%2C%20we%20propose%20a%20novel%20image%20semantic%20segmentation%20model%20called%20the%0AMulti-Grained%20Feature%20Integration%20Network%20%28MGFI-Net%29.%20Our%20MGFI-Net%20is%20designed%0Awith%20two%20dedicated%20modules%20to%20tackle%20these%20issues.%20First%2C%20to%20enhance%0Asegmentation%20accuracy%2C%20we%20introduce%20a%20Multi-Grained%20Feature%20Extraction%20Module%2C%0Awhich%20leverages%20hierarchical%20relationships%20between%20different%20feature%20scales%20to%0Aselectively%20focus%20on%20the%20most%20relevant%20information.%20Second%2C%20to%20preserve%20edge%0Adetails%2C%20we%20incorporate%20an%20Edge%20Enhancement%20Module%20that%20effectively%20retains%20and%0Aintegrates%20boundary%20information%20to%20refine%20segmentation%20results.%20Extensive%0Aexperiments%20demonstrate%20that%20MGFI-Net%20not%20only%20outperforms%20state-of-the-art%0Amethods%20in%20terms%20of%20segmentation%20accuracy%20but%20also%20achieves%20superior%20time%0Aefficiency%2C%20establishing%20it%20as%20a%20leading%20solution%20for%20real-time%20medical%20image%0Asegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13808v1&entry.124074799=Read"},
{"title": "Neural Green's Operators for Parametric Partial Differential Equations", "author": "Hugo Melchers and Joost Prins and Michael Abdelmalik", "abstract": "  This work introduces neural Green's operators (NGOs), a novel neural operator\nnetwork architecture that learns the solution operator for a parametric family\nof linear partial differential equations (PDEs). Our construction of NGOs is\nderived directly from the Green's formulation of such a solution operator.\nSimilar to deep operator networks (DeepONets) and variationally mimetic\noperator networks (VarMiONs), NGOs constitutes an expansion of the solution to\nthe PDE in terms of basis functions, that is returned from a sub-network,\ncontracted with coefficients, that are returned from another sub-network.\nHowever, in accordance with the Green's formulation, NGOs accept weighted\naverages of the input functions, rather than sampled values thereof, as is the\ncase in DeepONets and VarMiONs. Application of NGOs to canonical linear\nparametric PDEs shows that, while they remain competitive with DeepONets,\nVarMiONs and Fourier neural operators when testing on data that lie within the\ntraining distribution, they robustly generalize when testing on finer-scale\ndata generated outside of the training distribution. Furthermore, we show that\nthe explicit representation of the Green's function that is returned by NGOs\nenables the construction of effective preconditioners for numerical solvers for\nPDEs.\n", "link": "http://arxiv.org/abs/2406.01857v2", "date": "2025-02-19", "relevancy": 2.4232, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4938}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4802}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Green%27s%20Operators%20for%20Parametric%20Partial%20Differential%20Equations&body=Title%3A%20Neural%20Green%27s%20Operators%20for%20Parametric%20Partial%20Differential%20Equations%0AAuthor%3A%20Hugo%20Melchers%20and%20Joost%20Prins%20and%20Michael%20Abdelmalik%0AAbstract%3A%20%20%20This%20work%20introduces%20neural%20Green%27s%20operators%20%28NGOs%29%2C%20a%20novel%20neural%20operator%0Anetwork%20architecture%20that%20learns%20the%20solution%20operator%20for%20a%20parametric%20family%0Aof%20linear%20partial%20differential%20equations%20%28PDEs%29.%20Our%20construction%20of%20NGOs%20is%0Aderived%20directly%20from%20the%20Green%27s%20formulation%20of%20such%20a%20solution%20operator.%0ASimilar%20to%20deep%20operator%20networks%20%28DeepONets%29%20and%20variationally%20mimetic%0Aoperator%20networks%20%28VarMiONs%29%2C%20NGOs%20constitutes%20an%20expansion%20of%20the%20solution%20to%0Athe%20PDE%20in%20terms%20of%20basis%20functions%2C%20that%20is%20returned%20from%20a%20sub-network%2C%0Acontracted%20with%20coefficients%2C%20that%20are%20returned%20from%20another%20sub-network.%0AHowever%2C%20in%20accordance%20with%20the%20Green%27s%20formulation%2C%20NGOs%20accept%20weighted%0Aaverages%20of%20the%20input%20functions%2C%20rather%20than%20sampled%20values%20thereof%2C%20as%20is%20the%0Acase%20in%20DeepONets%20and%20VarMiONs.%20Application%20of%20NGOs%20to%20canonical%20linear%0Aparametric%20PDEs%20shows%20that%2C%20while%20they%20remain%20competitive%20with%20DeepONets%2C%0AVarMiONs%20and%20Fourier%20neural%20operators%20when%20testing%20on%20data%20that%20lie%20within%20the%0Atraining%20distribution%2C%20they%20robustly%20generalize%20when%20testing%20on%20finer-scale%0Adata%20generated%20outside%20of%20the%20training%20distribution.%20Furthermore%2C%20we%20show%20that%0Athe%20explicit%20representation%20of%20the%20Green%27s%20function%20that%20is%20returned%20by%20NGOs%0Aenables%20the%20construction%20of%20effective%20preconditioners%20for%20numerical%20solvers%20for%0APDEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01857v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Green%2527s%2520Operators%2520for%2520Parametric%2520Partial%2520Differential%2520Equations%26entry.906535625%3DHugo%2520Melchers%2520and%2520Joost%2520Prins%2520and%2520Michael%2520Abdelmalik%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520neural%2520Green%2527s%2520operators%2520%2528NGOs%2529%252C%2520a%2520novel%2520neural%2520operator%250Anetwork%2520architecture%2520that%2520learns%2520the%2520solution%2520operator%2520for%2520a%2520parametric%2520family%250Aof%2520linear%2520partial%2520differential%2520equations%2520%2528PDEs%2529.%2520Our%2520construction%2520of%2520NGOs%2520is%250Aderived%2520directly%2520from%2520the%2520Green%2527s%2520formulation%2520of%2520such%2520a%2520solution%2520operator.%250ASimilar%2520to%2520deep%2520operator%2520networks%2520%2528DeepONets%2529%2520and%2520variationally%2520mimetic%250Aoperator%2520networks%2520%2528VarMiONs%2529%252C%2520NGOs%2520constitutes%2520an%2520expansion%2520of%2520the%2520solution%2520to%250Athe%2520PDE%2520in%2520terms%2520of%2520basis%2520functions%252C%2520that%2520is%2520returned%2520from%2520a%2520sub-network%252C%250Acontracted%2520with%2520coefficients%252C%2520that%2520are%2520returned%2520from%2520another%2520sub-network.%250AHowever%252C%2520in%2520accordance%2520with%2520the%2520Green%2527s%2520formulation%252C%2520NGOs%2520accept%2520weighted%250Aaverages%2520of%2520the%2520input%2520functions%252C%2520rather%2520than%2520sampled%2520values%2520thereof%252C%2520as%2520is%2520the%250Acase%2520in%2520DeepONets%2520and%2520VarMiONs.%2520Application%2520of%2520NGOs%2520to%2520canonical%2520linear%250Aparametric%2520PDEs%2520shows%2520that%252C%2520while%2520they%2520remain%2520competitive%2520with%2520DeepONets%252C%250AVarMiONs%2520and%2520Fourier%2520neural%2520operators%2520when%2520testing%2520on%2520data%2520that%2520lie%2520within%2520the%250Atraining%2520distribution%252C%2520they%2520robustly%2520generalize%2520when%2520testing%2520on%2520finer-scale%250Adata%2520generated%2520outside%2520of%2520the%2520training%2520distribution.%2520Furthermore%252C%2520we%2520show%2520that%250Athe%2520explicit%2520representation%2520of%2520the%2520Green%2527s%2520function%2520that%2520is%2520returned%2520by%2520NGOs%250Aenables%2520the%2520construction%2520of%2520effective%2520preconditioners%2520for%2520numerical%2520solvers%2520for%250APDEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01857v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Green%27s%20Operators%20for%20Parametric%20Partial%20Differential%20Equations&entry.906535625=Hugo%20Melchers%20and%20Joost%20Prins%20and%20Michael%20Abdelmalik&entry.1292438233=%20%20This%20work%20introduces%20neural%20Green%27s%20operators%20%28NGOs%29%2C%20a%20novel%20neural%20operator%0Anetwork%20architecture%20that%20learns%20the%20solution%20operator%20for%20a%20parametric%20family%0Aof%20linear%20partial%20differential%20equations%20%28PDEs%29.%20Our%20construction%20of%20NGOs%20is%0Aderived%20directly%20from%20the%20Green%27s%20formulation%20of%20such%20a%20solution%20operator.%0ASimilar%20to%20deep%20operator%20networks%20%28DeepONets%29%20and%20variationally%20mimetic%0Aoperator%20networks%20%28VarMiONs%29%2C%20NGOs%20constitutes%20an%20expansion%20of%20the%20solution%20to%0Athe%20PDE%20in%20terms%20of%20basis%20functions%2C%20that%20is%20returned%20from%20a%20sub-network%2C%0Acontracted%20with%20coefficients%2C%20that%20are%20returned%20from%20another%20sub-network.%0AHowever%2C%20in%20accordance%20with%20the%20Green%27s%20formulation%2C%20NGOs%20accept%20weighted%0Aaverages%20of%20the%20input%20functions%2C%20rather%20than%20sampled%20values%20thereof%2C%20as%20is%20the%0Acase%20in%20DeepONets%20and%20VarMiONs.%20Application%20of%20NGOs%20to%20canonical%20linear%0Aparametric%20PDEs%20shows%20that%2C%20while%20they%20remain%20competitive%20with%20DeepONets%2C%0AVarMiONs%20and%20Fourier%20neural%20operators%20when%20testing%20on%20data%20that%20lie%20within%20the%0Atraining%20distribution%2C%20they%20robustly%20generalize%20when%20testing%20on%20finer-scale%0Adata%20generated%20outside%20of%20the%20training%20distribution.%20Furthermore%2C%20we%20show%20that%0Athe%20explicit%20representation%20of%20the%20Green%27s%20function%20that%20is%20returned%20by%20NGOs%0Aenables%20the%20construction%20of%20effective%20preconditioners%20for%20numerical%20solvers%20for%0APDEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01857v2&entry.124074799=Read"},
{"title": "Exploring Code Language Models for Automated HLS-based Hardware\n  Generation: Benchmark, Infrastructure and Analysis", "author": "Jiahao Gai and  Hao and  Chen and Zhican Wang and Hongyu Zhou and Wanru Zhao and Nicholas Lane and Hongxiang Fan", "abstract": "  Recent advances in code generation have illuminated the potential of\nemploying large language models (LLMs) for general-purpose programming\nlanguages such as Python and C++, opening new opportunities for automating\nsoftware development and enhancing programmer productivity. The potential of\nLLMs in software programming has sparked significant interest in exploring\nautomated hardware generation and automation. Although preliminary endeavors\nhave been made to adopt LLMs in generating hardware description languages\n(HDLs), several challenges persist in this direction. First, the volume of\navailable HDL training data is substantially smaller compared to that for\nsoftware programming languages. Second, the pre-trained LLMs, mainly tailored\nfor software code, tend to produce HDL designs that are more error-prone.\nThird, the generation of HDL requires a significantly higher number of tokens\ncompared to software programming, leading to inefficiencies in cost and energy\nconsumption. To tackle these challenges, this paper explores leveraging LLMs to\ngenerate High-Level Synthesis (HLS)-based hardware design. Although code\ngeneration for domain-specific programming languages is not new in the\nliterature, we aim to provide experimental results, insights, benchmarks, and\nevaluation infrastructure to investigate the suitability of HLS over low-level\nHDLs for LLM-assisted hardware design generation. To achieve this, we first\nfinetune pre-trained models for HLS-based hardware generation, using a\ncollected dataset with text prompts and corresponding reference HLS designs. An\nLLM-assisted framework is then proposed to automate end-to-end hardware code\ngeneration, which also investigates the impact of chain-of-thought and feedback\nloops promoting techniques on HLS-design generation. Limited by the timeframe\nof this research, we plan to evaluate more advanced reasoning models in the\nfuture.\n", "link": "http://arxiv.org/abs/2502.13921v1", "date": "2025-02-19", "relevancy": 2.4151, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4879}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4879}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Code%20Language%20Models%20for%20Automated%20HLS-based%20Hardware%0A%20%20Generation%3A%20Benchmark%2C%20Infrastructure%20and%20Analysis&body=Title%3A%20Exploring%20Code%20Language%20Models%20for%20Automated%20HLS-based%20Hardware%0A%20%20Generation%3A%20Benchmark%2C%20Infrastructure%20and%20Analysis%0AAuthor%3A%20Jiahao%20Gai%20and%20%20Hao%20and%20%20Chen%20and%20Zhican%20Wang%20and%20Hongyu%20Zhou%20and%20Wanru%20Zhao%20and%20Nicholas%20Lane%20and%20Hongxiang%20Fan%0AAbstract%3A%20%20%20Recent%20advances%20in%20code%20generation%20have%20illuminated%20the%20potential%20of%0Aemploying%20large%20language%20models%20%28LLMs%29%20for%20general-purpose%20programming%0Alanguages%20such%20as%20Python%20and%20C%2B%2B%2C%20opening%20new%20opportunities%20for%20automating%0Asoftware%20development%20and%20enhancing%20programmer%20productivity.%20The%20potential%20of%0ALLMs%20in%20software%20programming%20has%20sparked%20significant%20interest%20in%20exploring%0Aautomated%20hardware%20generation%20and%20automation.%20Although%20preliminary%20endeavors%0Ahave%20been%20made%20to%20adopt%20LLMs%20in%20generating%20hardware%20description%20languages%0A%28HDLs%29%2C%20several%20challenges%20persist%20in%20this%20direction.%20First%2C%20the%20volume%20of%0Aavailable%20HDL%20training%20data%20is%20substantially%20smaller%20compared%20to%20that%20for%0Asoftware%20programming%20languages.%20Second%2C%20the%20pre-trained%20LLMs%2C%20mainly%20tailored%0Afor%20software%20code%2C%20tend%20to%20produce%20HDL%20designs%20that%20are%20more%20error-prone.%0AThird%2C%20the%20generation%20of%20HDL%20requires%20a%20significantly%20higher%20number%20of%20tokens%0Acompared%20to%20software%20programming%2C%20leading%20to%20inefficiencies%20in%20cost%20and%20energy%0Aconsumption.%20To%20tackle%20these%20challenges%2C%20this%20paper%20explores%20leveraging%20LLMs%20to%0Agenerate%20High-Level%20Synthesis%20%28HLS%29-based%20hardware%20design.%20Although%20code%0Ageneration%20for%20domain-specific%20programming%20languages%20is%20not%20new%20in%20the%0Aliterature%2C%20we%20aim%20to%20provide%20experimental%20results%2C%20insights%2C%20benchmarks%2C%20and%0Aevaluation%20infrastructure%20to%20investigate%20the%20suitability%20of%20HLS%20over%20low-level%0AHDLs%20for%20LLM-assisted%20hardware%20design%20generation.%20To%20achieve%20this%2C%20we%20first%0Afinetune%20pre-trained%20models%20for%20HLS-based%20hardware%20generation%2C%20using%20a%0Acollected%20dataset%20with%20text%20prompts%20and%20corresponding%20reference%20HLS%20designs.%20An%0ALLM-assisted%20framework%20is%20then%20proposed%20to%20automate%20end-to-end%20hardware%20code%0Ageneration%2C%20which%20also%20investigates%20the%20impact%20of%20chain-of-thought%20and%20feedback%0Aloops%20promoting%20techniques%20on%20HLS-design%20generation.%20Limited%20by%20the%20timeframe%0Aof%20this%20research%2C%20we%20plan%20to%20evaluate%20more%20advanced%20reasoning%20models%20in%20the%0Afuture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Code%2520Language%2520Models%2520for%2520Automated%2520HLS-based%2520Hardware%250A%2520%2520Generation%253A%2520Benchmark%252C%2520Infrastructure%2520and%2520Analysis%26entry.906535625%3DJiahao%2520Gai%2520and%2520%2520Hao%2520and%2520%2520Chen%2520and%2520Zhican%2520Wang%2520and%2520Hongyu%2520Zhou%2520and%2520Wanru%2520Zhao%2520and%2520Nicholas%2520Lane%2520and%2520Hongxiang%2520Fan%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520code%2520generation%2520have%2520illuminated%2520the%2520potential%2520of%250Aemploying%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520general-purpose%2520programming%250Alanguages%2520such%2520as%2520Python%2520and%2520C%252B%252B%252C%2520opening%2520new%2520opportunities%2520for%2520automating%250Asoftware%2520development%2520and%2520enhancing%2520programmer%2520productivity.%2520The%2520potential%2520of%250ALLMs%2520in%2520software%2520programming%2520has%2520sparked%2520significant%2520interest%2520in%2520exploring%250Aautomated%2520hardware%2520generation%2520and%2520automation.%2520Although%2520preliminary%2520endeavors%250Ahave%2520been%2520made%2520to%2520adopt%2520LLMs%2520in%2520generating%2520hardware%2520description%2520languages%250A%2528HDLs%2529%252C%2520several%2520challenges%2520persist%2520in%2520this%2520direction.%2520First%252C%2520the%2520volume%2520of%250Aavailable%2520HDL%2520training%2520data%2520is%2520substantially%2520smaller%2520compared%2520to%2520that%2520for%250Asoftware%2520programming%2520languages.%2520Second%252C%2520the%2520pre-trained%2520LLMs%252C%2520mainly%2520tailored%250Afor%2520software%2520code%252C%2520tend%2520to%2520produce%2520HDL%2520designs%2520that%2520are%2520more%2520error-prone.%250AThird%252C%2520the%2520generation%2520of%2520HDL%2520requires%2520a%2520significantly%2520higher%2520number%2520of%2520tokens%250Acompared%2520to%2520software%2520programming%252C%2520leading%2520to%2520inefficiencies%2520in%2520cost%2520and%2520energy%250Aconsumption.%2520To%2520tackle%2520these%2520challenges%252C%2520this%2520paper%2520explores%2520leveraging%2520LLMs%2520to%250Agenerate%2520High-Level%2520Synthesis%2520%2528HLS%2529-based%2520hardware%2520design.%2520Although%2520code%250Ageneration%2520for%2520domain-specific%2520programming%2520languages%2520is%2520not%2520new%2520in%2520the%250Aliterature%252C%2520we%2520aim%2520to%2520provide%2520experimental%2520results%252C%2520insights%252C%2520benchmarks%252C%2520and%250Aevaluation%2520infrastructure%2520to%2520investigate%2520the%2520suitability%2520of%2520HLS%2520over%2520low-level%250AHDLs%2520for%2520LLM-assisted%2520hardware%2520design%2520generation.%2520To%2520achieve%2520this%252C%2520we%2520first%250Afinetune%2520pre-trained%2520models%2520for%2520HLS-based%2520hardware%2520generation%252C%2520using%2520a%250Acollected%2520dataset%2520with%2520text%2520prompts%2520and%2520corresponding%2520reference%2520HLS%2520designs.%2520An%250ALLM-assisted%2520framework%2520is%2520then%2520proposed%2520to%2520automate%2520end-to-end%2520hardware%2520code%250Ageneration%252C%2520which%2520also%2520investigates%2520the%2520impact%2520of%2520chain-of-thought%2520and%2520feedback%250Aloops%2520promoting%2520techniques%2520on%2520HLS-design%2520generation.%2520Limited%2520by%2520the%2520timeframe%250Aof%2520this%2520research%252C%2520we%2520plan%2520to%2520evaluate%2520more%2520advanced%2520reasoning%2520models%2520in%2520the%250Afuture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Code%20Language%20Models%20for%20Automated%20HLS-based%20Hardware%0A%20%20Generation%3A%20Benchmark%2C%20Infrastructure%20and%20Analysis&entry.906535625=Jiahao%20Gai%20and%20%20Hao%20and%20%20Chen%20and%20Zhican%20Wang%20and%20Hongyu%20Zhou%20and%20Wanru%20Zhao%20and%20Nicholas%20Lane%20and%20Hongxiang%20Fan&entry.1292438233=%20%20Recent%20advances%20in%20code%20generation%20have%20illuminated%20the%20potential%20of%0Aemploying%20large%20language%20models%20%28LLMs%29%20for%20general-purpose%20programming%0Alanguages%20such%20as%20Python%20and%20C%2B%2B%2C%20opening%20new%20opportunities%20for%20automating%0Asoftware%20development%20and%20enhancing%20programmer%20productivity.%20The%20potential%20of%0ALLMs%20in%20software%20programming%20has%20sparked%20significant%20interest%20in%20exploring%0Aautomated%20hardware%20generation%20and%20automation.%20Although%20preliminary%20endeavors%0Ahave%20been%20made%20to%20adopt%20LLMs%20in%20generating%20hardware%20description%20languages%0A%28HDLs%29%2C%20several%20challenges%20persist%20in%20this%20direction.%20First%2C%20the%20volume%20of%0Aavailable%20HDL%20training%20data%20is%20substantially%20smaller%20compared%20to%20that%20for%0Asoftware%20programming%20languages.%20Second%2C%20the%20pre-trained%20LLMs%2C%20mainly%20tailored%0Afor%20software%20code%2C%20tend%20to%20produce%20HDL%20designs%20that%20are%20more%20error-prone.%0AThird%2C%20the%20generation%20of%20HDL%20requires%20a%20significantly%20higher%20number%20of%20tokens%0Acompared%20to%20software%20programming%2C%20leading%20to%20inefficiencies%20in%20cost%20and%20energy%0Aconsumption.%20To%20tackle%20these%20challenges%2C%20this%20paper%20explores%20leveraging%20LLMs%20to%0Agenerate%20High-Level%20Synthesis%20%28HLS%29-based%20hardware%20design.%20Although%20code%0Ageneration%20for%20domain-specific%20programming%20languages%20is%20not%20new%20in%20the%0Aliterature%2C%20we%20aim%20to%20provide%20experimental%20results%2C%20insights%2C%20benchmarks%2C%20and%0Aevaluation%20infrastructure%20to%20investigate%20the%20suitability%20of%20HLS%20over%20low-level%0AHDLs%20for%20LLM-assisted%20hardware%20design%20generation.%20To%20achieve%20this%2C%20we%20first%0Afinetune%20pre-trained%20models%20for%20HLS-based%20hardware%20generation%2C%20using%20a%0Acollected%20dataset%20with%20text%20prompts%20and%20corresponding%20reference%20HLS%20designs.%20An%0ALLM-assisted%20framework%20is%20then%20proposed%20to%20automate%20end-to-end%20hardware%20code%0Ageneration%2C%20which%20also%20investigates%20the%20impact%20of%20chain-of-thought%20and%20feedback%0Aloops%20promoting%20techniques%20on%20HLS-design%20generation.%20Limited%20by%20the%20timeframe%0Aof%20this%20research%2C%20we%20plan%20to%20evaluate%20more%20advanced%20reasoning%20models%20in%20the%0Afuture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13921v1&entry.124074799=Read"},
{"title": "FlexTok: Resampling Images into 1D Token Sequences of Flexible Length", "author": "Roman Bachmann and Jesse Allardice and David Mizrahi and Enrico Fini and O\u011fuzhan Fatih Kar and Elmira Amirloo and Alaaeldin El-Nouby and Amir Zamir and Afshin Dehghan", "abstract": "  Image tokenization has enabled major advances in autoregressive image\ngeneration by providing compressed, discrete representations that are more\nefficient to process than raw pixels. While traditional approaches use 2D grid\ntokenization, recent methods like TiTok have shown that 1D tokenization can\nachieve high generation quality by eliminating grid redundancies. However,\nthese methods typically use a fixed number of tokens and thus cannot adapt to\nan image's inherent complexity. We introduce FlexTok, a tokenizer that projects\n2D images into variable-length, ordered 1D token sequences. For example, a\n256x256 image can be resampled into anywhere from 1 to 256 discrete tokens,\nhierarchically and semantically compressing its information. By training a\nrectified flow model as the decoder and using nested dropout, FlexTok produces\nplausible reconstructions regardless of the chosen token sequence length. We\nevaluate our approach in an autoregressive generation setting using a simple\nGPT-style Transformer. On ImageNet, this approach achieves an FID<2 across 8 to\n128 tokens, outperforming TiTok and matching state-of-the-art methods with far\nfewer tokens. We further extend the model to support to text-conditioned image\ngeneration and examine how FlexTok relates to traditional 2D tokenization. A\nkey finding is that FlexTok enables next-token prediction to describe images in\na coarse-to-fine \"visual vocabulary\", and that the number of tokens to generate\ndepends on the complexity of the generation task.\n", "link": "http://arxiv.org/abs/2502.13967v1", "date": "2025-02-19", "relevancy": 2.4034, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6586}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5766}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlexTok%3A%20Resampling%20Images%20into%201D%20Token%20Sequences%20of%20Flexible%20Length&body=Title%3A%20FlexTok%3A%20Resampling%20Images%20into%201D%20Token%20Sequences%20of%20Flexible%20Length%0AAuthor%3A%20Roman%20Bachmann%20and%20Jesse%20Allardice%20and%20David%20Mizrahi%20and%20Enrico%20Fini%20and%20O%C4%9Fuzhan%20Fatih%20Kar%20and%20Elmira%20Amirloo%20and%20Alaaeldin%20El-Nouby%20and%20Amir%20Zamir%20and%20Afshin%20Dehghan%0AAbstract%3A%20%20%20Image%20tokenization%20has%20enabled%20major%20advances%20in%20autoregressive%20image%0Ageneration%20by%20providing%20compressed%2C%20discrete%20representations%20that%20are%20more%0Aefficient%20to%20process%20than%20raw%20pixels.%20While%20traditional%20approaches%20use%202D%20grid%0Atokenization%2C%20recent%20methods%20like%20TiTok%20have%20shown%20that%201D%20tokenization%20can%0Aachieve%20high%20generation%20quality%20by%20eliminating%20grid%20redundancies.%20However%2C%0Athese%20methods%20typically%20use%20a%20fixed%20number%20of%20tokens%20and%20thus%20cannot%20adapt%20to%0Aan%20image%27s%20inherent%20complexity.%20We%20introduce%20FlexTok%2C%20a%20tokenizer%20that%20projects%0A2D%20images%20into%20variable-length%2C%20ordered%201D%20token%20sequences.%20For%20example%2C%20a%0A256x256%20image%20can%20be%20resampled%20into%20anywhere%20from%201%20to%20256%20discrete%20tokens%2C%0Ahierarchically%20and%20semantically%20compressing%20its%20information.%20By%20training%20a%0Arectified%20flow%20model%20as%20the%20decoder%20and%20using%20nested%20dropout%2C%20FlexTok%20produces%0Aplausible%20reconstructions%20regardless%20of%20the%20chosen%20token%20sequence%20length.%20We%0Aevaluate%20our%20approach%20in%20an%20autoregressive%20generation%20setting%20using%20a%20simple%0AGPT-style%20Transformer.%20On%20ImageNet%2C%20this%20approach%20achieves%20an%20FID%3C2%20across%208%20to%0A128%20tokens%2C%20outperforming%20TiTok%20and%20matching%20state-of-the-art%20methods%20with%20far%0Afewer%20tokens.%20We%20further%20extend%20the%20model%20to%20support%20to%20text-conditioned%20image%0Ageneration%20and%20examine%20how%20FlexTok%20relates%20to%20traditional%202D%20tokenization.%20A%0Akey%20finding%20is%20that%20FlexTok%20enables%20next-token%20prediction%20to%20describe%20images%20in%0Aa%20coarse-to-fine%20%22visual%20vocabulary%22%2C%20and%20that%20the%20number%20of%20tokens%20to%20generate%0Adepends%20on%20the%20complexity%20of%20the%20generation%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlexTok%253A%2520Resampling%2520Images%2520into%25201D%2520Token%2520Sequences%2520of%2520Flexible%2520Length%26entry.906535625%3DRoman%2520Bachmann%2520and%2520Jesse%2520Allardice%2520and%2520David%2520Mizrahi%2520and%2520Enrico%2520Fini%2520and%2520O%25C4%259Fuzhan%2520Fatih%2520Kar%2520and%2520Elmira%2520Amirloo%2520and%2520Alaaeldin%2520El-Nouby%2520and%2520Amir%2520Zamir%2520and%2520Afshin%2520Dehghan%26entry.1292438233%3D%2520%2520Image%2520tokenization%2520has%2520enabled%2520major%2520advances%2520in%2520autoregressive%2520image%250Ageneration%2520by%2520providing%2520compressed%252C%2520discrete%2520representations%2520that%2520are%2520more%250Aefficient%2520to%2520process%2520than%2520raw%2520pixels.%2520While%2520traditional%2520approaches%2520use%25202D%2520grid%250Atokenization%252C%2520recent%2520methods%2520like%2520TiTok%2520have%2520shown%2520that%25201D%2520tokenization%2520can%250Aachieve%2520high%2520generation%2520quality%2520by%2520eliminating%2520grid%2520redundancies.%2520However%252C%250Athese%2520methods%2520typically%2520use%2520a%2520fixed%2520number%2520of%2520tokens%2520and%2520thus%2520cannot%2520adapt%2520to%250Aan%2520image%2527s%2520inherent%2520complexity.%2520We%2520introduce%2520FlexTok%252C%2520a%2520tokenizer%2520that%2520projects%250A2D%2520images%2520into%2520variable-length%252C%2520ordered%25201D%2520token%2520sequences.%2520For%2520example%252C%2520a%250A256x256%2520image%2520can%2520be%2520resampled%2520into%2520anywhere%2520from%25201%2520to%2520256%2520discrete%2520tokens%252C%250Ahierarchically%2520and%2520semantically%2520compressing%2520its%2520information.%2520By%2520training%2520a%250Arectified%2520flow%2520model%2520as%2520the%2520decoder%2520and%2520using%2520nested%2520dropout%252C%2520FlexTok%2520produces%250Aplausible%2520reconstructions%2520regardless%2520of%2520the%2520chosen%2520token%2520sequence%2520length.%2520We%250Aevaluate%2520our%2520approach%2520in%2520an%2520autoregressive%2520generation%2520setting%2520using%2520a%2520simple%250AGPT-style%2520Transformer.%2520On%2520ImageNet%252C%2520this%2520approach%2520achieves%2520an%2520FID%253C2%2520across%25208%2520to%250A128%2520tokens%252C%2520outperforming%2520TiTok%2520and%2520matching%2520state-of-the-art%2520methods%2520with%2520far%250Afewer%2520tokens.%2520We%2520further%2520extend%2520the%2520model%2520to%2520support%2520to%2520text-conditioned%2520image%250Ageneration%2520and%2520examine%2520how%2520FlexTok%2520relates%2520to%2520traditional%25202D%2520tokenization.%2520A%250Akey%2520finding%2520is%2520that%2520FlexTok%2520enables%2520next-token%2520prediction%2520to%2520describe%2520images%2520in%250Aa%2520coarse-to-fine%2520%2522visual%2520vocabulary%2522%252C%2520and%2520that%2520the%2520number%2520of%2520tokens%2520to%2520generate%250Adepends%2520on%2520the%2520complexity%2520of%2520the%2520generation%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexTok%3A%20Resampling%20Images%20into%201D%20Token%20Sequences%20of%20Flexible%20Length&entry.906535625=Roman%20Bachmann%20and%20Jesse%20Allardice%20and%20David%20Mizrahi%20and%20Enrico%20Fini%20and%20O%C4%9Fuzhan%20Fatih%20Kar%20and%20Elmira%20Amirloo%20and%20Alaaeldin%20El-Nouby%20and%20Amir%20Zamir%20and%20Afshin%20Dehghan&entry.1292438233=%20%20Image%20tokenization%20has%20enabled%20major%20advances%20in%20autoregressive%20image%0Ageneration%20by%20providing%20compressed%2C%20discrete%20representations%20that%20are%20more%0Aefficient%20to%20process%20than%20raw%20pixels.%20While%20traditional%20approaches%20use%202D%20grid%0Atokenization%2C%20recent%20methods%20like%20TiTok%20have%20shown%20that%201D%20tokenization%20can%0Aachieve%20high%20generation%20quality%20by%20eliminating%20grid%20redundancies.%20However%2C%0Athese%20methods%20typically%20use%20a%20fixed%20number%20of%20tokens%20and%20thus%20cannot%20adapt%20to%0Aan%20image%27s%20inherent%20complexity.%20We%20introduce%20FlexTok%2C%20a%20tokenizer%20that%20projects%0A2D%20images%20into%20variable-length%2C%20ordered%201D%20token%20sequences.%20For%20example%2C%20a%0A256x256%20image%20can%20be%20resampled%20into%20anywhere%20from%201%20to%20256%20discrete%20tokens%2C%0Ahierarchically%20and%20semantically%20compressing%20its%20information.%20By%20training%20a%0Arectified%20flow%20model%20as%20the%20decoder%20and%20using%20nested%20dropout%2C%20FlexTok%20produces%0Aplausible%20reconstructions%20regardless%20of%20the%20chosen%20token%20sequence%20length.%20We%0Aevaluate%20our%20approach%20in%20an%20autoregressive%20generation%20setting%20using%20a%20simple%0AGPT-style%20Transformer.%20On%20ImageNet%2C%20this%20approach%20achieves%20an%20FID%3C2%20across%208%20to%0A128%20tokens%2C%20outperforming%20TiTok%20and%20matching%20state-of-the-art%20methods%20with%20far%0Afewer%20tokens.%20We%20further%20extend%20the%20model%20to%20support%20to%20text-conditioned%20image%0Ageneration%20and%20examine%20how%20FlexTok%20relates%20to%20traditional%202D%20tokenization.%20A%0Akey%20finding%20is%20that%20FlexTok%20enables%20next-token%20prediction%20to%20describe%20images%20in%0Aa%20coarse-to-fine%20%22visual%20vocabulary%22%2C%20and%20that%20the%20number%20of%20tokens%20to%20generate%0Adepends%20on%20the%20complexity%20of%20the%20generation%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13967v1&entry.124074799=Read"},
{"title": "Helix-mRNA: A Hybrid Foundation Model For Full Sequence mRNA\n  Therapeutics", "author": "Matthew Wood and Mathieu Klop and Maxime Allard", "abstract": "  mRNA-based vaccines have become a major focus in the pharmaceutical industry.\nThe coding sequence as well as the Untranslated Regions (UTRs) of an mRNA can\nstrongly influence translation efficiency, stability, degradation, and other\nfactors that collectively determine a vaccine's effectiveness. However,\noptimizing mRNA sequences for those properties remains a complex challenge.\nExisting deep learning models often focus solely on coding region optimization,\noverlooking the UTRs. We present Helix-mRNA, a structured state-space-based and\nattention hybrid model to address these challenges. In addition to a first\npre-training, a second pre-training stage allows us to specialise the model\nwith high-quality data. We employ single nucleotide tokenization of mRNA\nsequences with codon separation, ensuring prior biological and structural\ninformation from the original mRNA sequence is not lost. Our model, Helix-mRNA,\noutperforms existing methods in analysing both UTRs and coding region\nproperties. It can process sequences 6x longer than current approaches while\nusing only 10% of the parameters of existing foundation models. Its predictive\ncapabilities extend to all mRNA regions. We open-source the model\n(https://github.com/helicalAI/helical) and model weights\n(https://huggingface.co/helical-ai/helix-mRNA).\n", "link": "http://arxiv.org/abs/2502.13785v1", "date": "2025-02-19", "relevancy": 2.4021, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4931}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4931}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Helix-mRNA%3A%20A%20Hybrid%20Foundation%20Model%20For%20Full%20Sequence%20mRNA%0A%20%20Therapeutics&body=Title%3A%20Helix-mRNA%3A%20A%20Hybrid%20Foundation%20Model%20For%20Full%20Sequence%20mRNA%0A%20%20Therapeutics%0AAuthor%3A%20Matthew%20Wood%20and%20Mathieu%20Klop%20and%20Maxime%20Allard%0AAbstract%3A%20%20%20mRNA-based%20vaccines%20have%20become%20a%20major%20focus%20in%20the%20pharmaceutical%20industry.%0AThe%20coding%20sequence%20as%20well%20as%20the%20Untranslated%20Regions%20%28UTRs%29%20of%20an%20mRNA%20can%0Astrongly%20influence%20translation%20efficiency%2C%20stability%2C%20degradation%2C%20and%20other%0Afactors%20that%20collectively%20determine%20a%20vaccine%27s%20effectiveness.%20However%2C%0Aoptimizing%20mRNA%20sequences%20for%20those%20properties%20remains%20a%20complex%20challenge.%0AExisting%20deep%20learning%20models%20often%20focus%20solely%20on%20coding%20region%20optimization%2C%0Aoverlooking%20the%20UTRs.%20We%20present%20Helix-mRNA%2C%20a%20structured%20state-space-based%20and%0Aattention%20hybrid%20model%20to%20address%20these%20challenges.%20In%20addition%20to%20a%20first%0Apre-training%2C%20a%20second%20pre-training%20stage%20allows%20us%20to%20specialise%20the%20model%0Awith%20high-quality%20data.%20We%20employ%20single%20nucleotide%20tokenization%20of%20mRNA%0Asequences%20with%20codon%20separation%2C%20ensuring%20prior%20biological%20and%20structural%0Ainformation%20from%20the%20original%20mRNA%20sequence%20is%20not%20lost.%20Our%20model%2C%20Helix-mRNA%2C%0Aoutperforms%20existing%20methods%20in%20analysing%20both%20UTRs%20and%20coding%20region%0Aproperties.%20It%20can%20process%20sequences%206x%20longer%20than%20current%20approaches%20while%0Ausing%20only%2010%25%20of%20the%20parameters%20of%20existing%20foundation%20models.%20Its%20predictive%0Acapabilities%20extend%20to%20all%20mRNA%20regions.%20We%20open-source%20the%20model%0A%28https%3A//github.com/helicalAI/helical%29%20and%20model%20weights%0A%28https%3A//huggingface.co/helical-ai/helix-mRNA%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHelix-mRNA%253A%2520A%2520Hybrid%2520Foundation%2520Model%2520For%2520Full%2520Sequence%2520mRNA%250A%2520%2520Therapeutics%26entry.906535625%3DMatthew%2520Wood%2520and%2520Mathieu%2520Klop%2520and%2520Maxime%2520Allard%26entry.1292438233%3D%2520%2520mRNA-based%2520vaccines%2520have%2520become%2520a%2520major%2520focus%2520in%2520the%2520pharmaceutical%2520industry.%250AThe%2520coding%2520sequence%2520as%2520well%2520as%2520the%2520Untranslated%2520Regions%2520%2528UTRs%2529%2520of%2520an%2520mRNA%2520can%250Astrongly%2520influence%2520translation%2520efficiency%252C%2520stability%252C%2520degradation%252C%2520and%2520other%250Afactors%2520that%2520collectively%2520determine%2520a%2520vaccine%2527s%2520effectiveness.%2520However%252C%250Aoptimizing%2520mRNA%2520sequences%2520for%2520those%2520properties%2520remains%2520a%2520complex%2520challenge.%250AExisting%2520deep%2520learning%2520models%2520often%2520focus%2520solely%2520on%2520coding%2520region%2520optimization%252C%250Aoverlooking%2520the%2520UTRs.%2520We%2520present%2520Helix-mRNA%252C%2520a%2520structured%2520state-space-based%2520and%250Aattention%2520hybrid%2520model%2520to%2520address%2520these%2520challenges.%2520In%2520addition%2520to%2520a%2520first%250Apre-training%252C%2520a%2520second%2520pre-training%2520stage%2520allows%2520us%2520to%2520specialise%2520the%2520model%250Awith%2520high-quality%2520data.%2520We%2520employ%2520single%2520nucleotide%2520tokenization%2520of%2520mRNA%250Asequences%2520with%2520codon%2520separation%252C%2520ensuring%2520prior%2520biological%2520and%2520structural%250Ainformation%2520from%2520the%2520original%2520mRNA%2520sequence%2520is%2520not%2520lost.%2520Our%2520model%252C%2520Helix-mRNA%252C%250Aoutperforms%2520existing%2520methods%2520in%2520analysing%2520both%2520UTRs%2520and%2520coding%2520region%250Aproperties.%2520It%2520can%2520process%2520sequences%25206x%2520longer%2520than%2520current%2520approaches%2520while%250Ausing%2520only%252010%2525%2520of%2520the%2520parameters%2520of%2520existing%2520foundation%2520models.%2520Its%2520predictive%250Acapabilities%2520extend%2520to%2520all%2520mRNA%2520regions.%2520We%2520open-source%2520the%2520model%250A%2528https%253A//github.com/helicalAI/helical%2529%2520and%2520model%2520weights%250A%2528https%253A//huggingface.co/helical-ai/helix-mRNA%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Helix-mRNA%3A%20A%20Hybrid%20Foundation%20Model%20For%20Full%20Sequence%20mRNA%0A%20%20Therapeutics&entry.906535625=Matthew%20Wood%20and%20Mathieu%20Klop%20and%20Maxime%20Allard&entry.1292438233=%20%20mRNA-based%20vaccines%20have%20become%20a%20major%20focus%20in%20the%20pharmaceutical%20industry.%0AThe%20coding%20sequence%20as%20well%20as%20the%20Untranslated%20Regions%20%28UTRs%29%20of%20an%20mRNA%20can%0Astrongly%20influence%20translation%20efficiency%2C%20stability%2C%20degradation%2C%20and%20other%0Afactors%20that%20collectively%20determine%20a%20vaccine%27s%20effectiveness.%20However%2C%0Aoptimizing%20mRNA%20sequences%20for%20those%20properties%20remains%20a%20complex%20challenge.%0AExisting%20deep%20learning%20models%20often%20focus%20solely%20on%20coding%20region%20optimization%2C%0Aoverlooking%20the%20UTRs.%20We%20present%20Helix-mRNA%2C%20a%20structured%20state-space-based%20and%0Aattention%20hybrid%20model%20to%20address%20these%20challenges.%20In%20addition%20to%20a%20first%0Apre-training%2C%20a%20second%20pre-training%20stage%20allows%20us%20to%20specialise%20the%20model%0Awith%20high-quality%20data.%20We%20employ%20single%20nucleotide%20tokenization%20of%20mRNA%0Asequences%20with%20codon%20separation%2C%20ensuring%20prior%20biological%20and%20structural%0Ainformation%20from%20the%20original%20mRNA%20sequence%20is%20not%20lost.%20Our%20model%2C%20Helix-mRNA%2C%0Aoutperforms%20existing%20methods%20in%20analysing%20both%20UTRs%20and%20coding%20region%0Aproperties.%20It%20can%20process%20sequences%206x%20longer%20than%20current%20approaches%20while%0Ausing%20only%2010%25%20of%20the%20parameters%20of%20existing%20foundation%20models.%20Its%20predictive%0Acapabilities%20extend%20to%20all%20mRNA%20regions.%20We%20open-source%20the%20model%0A%28https%3A//github.com/helicalAI/helical%29%20and%20model%20weights%0A%28https%3A//huggingface.co/helical-ai/helix-mRNA%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13785v1&entry.124074799=Read"},
{"title": "BNEM: A Boltzmann Sampler Based on Bootstrapped Noised Energy Matching", "author": "RuiKang OuYang and Bo Qiang and Zixing Song and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "abstract": "  Developing an efficient sampler capable of generating independent and\nidentically distributed (IID) samples from a Boltzmann distribution is a\ncrucial challenge in scientific research, e.g. molecular dynamics. In this\nwork, we intend to learn neural samplers given energy functions instead of data\nsampled from the Boltzmann distribution. By learning the energies of the noised\ndata, we propose a diffusion-based sampler, Noised Energy Matching, which\ntheoretically has lower variance and more complexity compared to related works.\nFurthermore, a novel bootstrapping technique is applied to NEM to balance\nbetween bias and variance. We evaluate NEM and BNEM on a 2-dimensional 40\nGaussian Mixture Model (GMM) and a 4-particle double-well potential (DW-4). The\nexperimental results demonstrate that BNEM can achieve state-of-the-art\nperformance while being more robust.\n", "link": "http://arxiv.org/abs/2409.09787v3", "date": "2025-02-19", "relevancy": 2.3755, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5369}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4444}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BNEM%3A%20A%20Boltzmann%20Sampler%20Based%20on%20Bootstrapped%20Noised%20Energy%20Matching&body=Title%3A%20BNEM%3A%20A%20Boltzmann%20Sampler%20Based%20on%20Bootstrapped%20Noised%20Energy%20Matching%0AAuthor%3A%20RuiKang%20OuYang%20and%20Bo%20Qiang%20and%20Zixing%20Song%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato%0AAbstract%3A%20%20%20Developing%20an%20efficient%20sampler%20capable%20of%20generating%20independent%20and%0Aidentically%20distributed%20%28IID%29%20samples%20from%20a%20Boltzmann%20distribution%20is%20a%0Acrucial%20challenge%20in%20scientific%20research%2C%20e.g.%20molecular%20dynamics.%20In%20this%0Awork%2C%20we%20intend%20to%20learn%20neural%20samplers%20given%20energy%20functions%20instead%20of%20data%0Asampled%20from%20the%20Boltzmann%20distribution.%20By%20learning%20the%20energies%20of%20the%20noised%0Adata%2C%20we%20propose%20a%20diffusion-based%20sampler%2C%20Noised%20Energy%20Matching%2C%20which%0Atheoretically%20has%20lower%20variance%20and%20more%20complexity%20compared%20to%20related%20works.%0AFurthermore%2C%20a%20novel%20bootstrapping%20technique%20is%20applied%20to%20NEM%20to%20balance%0Abetween%20bias%20and%20variance.%20We%20evaluate%20NEM%20and%20BNEM%20on%20a%202-dimensional%2040%0AGaussian%20Mixture%20Model%20%28GMM%29%20and%20a%204-particle%20double-well%20potential%20%28DW-4%29.%20The%0Aexperimental%20results%20demonstrate%20that%20BNEM%20can%20achieve%20state-of-the-art%0Aperformance%20while%20being%20more%20robust.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09787v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBNEM%253A%2520A%2520Boltzmann%2520Sampler%2520Based%2520on%2520Bootstrapped%2520Noised%2520Energy%2520Matching%26entry.906535625%3DRuiKang%2520OuYang%2520and%2520Bo%2520Qiang%2520and%2520Zixing%2520Song%2520and%2520Jos%25C3%25A9%2520Miguel%2520Hern%25C3%25A1ndez-Lobato%26entry.1292438233%3D%2520%2520Developing%2520an%2520efficient%2520sampler%2520capable%2520of%2520generating%2520independent%2520and%250Aidentically%2520distributed%2520%2528IID%2529%2520samples%2520from%2520a%2520Boltzmann%2520distribution%2520is%2520a%250Acrucial%2520challenge%2520in%2520scientific%2520research%252C%2520e.g.%2520molecular%2520dynamics.%2520In%2520this%250Awork%252C%2520we%2520intend%2520to%2520learn%2520neural%2520samplers%2520given%2520energy%2520functions%2520instead%2520of%2520data%250Asampled%2520from%2520the%2520Boltzmann%2520distribution.%2520By%2520learning%2520the%2520energies%2520of%2520the%2520noised%250Adata%252C%2520we%2520propose%2520a%2520diffusion-based%2520sampler%252C%2520Noised%2520Energy%2520Matching%252C%2520which%250Atheoretically%2520has%2520lower%2520variance%2520and%2520more%2520complexity%2520compared%2520to%2520related%2520works.%250AFurthermore%252C%2520a%2520novel%2520bootstrapping%2520technique%2520is%2520applied%2520to%2520NEM%2520to%2520balance%250Abetween%2520bias%2520and%2520variance.%2520We%2520evaluate%2520NEM%2520and%2520BNEM%2520on%2520a%25202-dimensional%252040%250AGaussian%2520Mixture%2520Model%2520%2528GMM%2529%2520and%2520a%25204-particle%2520double-well%2520potential%2520%2528DW-4%2529.%2520The%250Aexperimental%2520results%2520demonstrate%2520that%2520BNEM%2520can%2520achieve%2520state-of-the-art%250Aperformance%2520while%2520being%2520more%2520robust.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09787v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BNEM%3A%20A%20Boltzmann%20Sampler%20Based%20on%20Bootstrapped%20Noised%20Energy%20Matching&entry.906535625=RuiKang%20OuYang%20and%20Bo%20Qiang%20and%20Zixing%20Song%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato&entry.1292438233=%20%20Developing%20an%20efficient%20sampler%20capable%20of%20generating%20independent%20and%0Aidentically%20distributed%20%28IID%29%20samples%20from%20a%20Boltzmann%20distribution%20is%20a%0Acrucial%20challenge%20in%20scientific%20research%2C%20e.g.%20molecular%20dynamics.%20In%20this%0Awork%2C%20we%20intend%20to%20learn%20neural%20samplers%20given%20energy%20functions%20instead%20of%20data%0Asampled%20from%20the%20Boltzmann%20distribution.%20By%20learning%20the%20energies%20of%20the%20noised%0Adata%2C%20we%20propose%20a%20diffusion-based%20sampler%2C%20Noised%20Energy%20Matching%2C%20which%0Atheoretically%20has%20lower%20variance%20and%20more%20complexity%20compared%20to%20related%20works.%0AFurthermore%2C%20a%20novel%20bootstrapping%20technique%20is%20applied%20to%20NEM%20to%20balance%0Abetween%20bias%20and%20variance.%20We%20evaluate%20NEM%20and%20BNEM%20on%20a%202-dimensional%2040%0AGaussian%20Mixture%20Model%20%28GMM%29%20and%20a%204-particle%20double-well%20potential%20%28DW-4%29.%20The%0Aexperimental%20results%20demonstrate%20that%20BNEM%20can%20achieve%20state-of-the-art%0Aperformance%20while%20being%20more%20robust.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09787v3&entry.124074799=Read"},
{"title": "Exploring Mutual Cross-Modal Attention for Context-Aware Human\n  Affordance Generation", "author": "Prasun Roy and Saumik Bhattacharya and Subhankar Ghosh and Umapada Pal and Michael Blumenstein", "abstract": "  Human affordance learning investigates contextually relevant novel pose\nprediction such that the estimated pose represents a valid human action within\nthe scene. While the task is fundamental to machine perception and automated\ninteractive navigation agents, the exponentially large number of probable pose\nand action variations make the problem challenging and non-trivial. However,\nthe existing datasets and methods for human affordance prediction in 2D scenes\nare significantly limited in the literature. In this paper, we propose a novel\ncross-attention mechanism to encode the scene context for affordance prediction\nby mutually attending spatial feature maps from two different modalities. The\nproposed method is disentangled among individual subtasks to efficiently reduce\nthe problem complexity. First, we sample a probable location for a person\nwithin the scene using a variational autoencoder (VAE) conditioned on the\nglobal scene context encoding. Next, we predict a potential pose template from\na set of existing human pose candidates using a classifier on the local context\nencoding around the predicted location. In the subsequent steps, we use two\nVAEs to sample the scale and deformation parameters for the predicted pose\ntemplate by conditioning on the local context and template class. Our\nexperiments show significant improvements over the previous baseline of human\naffordance injection into complex 2D scenes.\n", "link": "http://arxiv.org/abs/2502.13637v1", "date": "2025-02-19", "relevancy": 2.3685, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6006}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5946}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Mutual%20Cross-Modal%20Attention%20for%20Context-Aware%20Human%0A%20%20Affordance%20Generation&body=Title%3A%20Exploring%20Mutual%20Cross-Modal%20Attention%20for%20Context-Aware%20Human%0A%20%20Affordance%20Generation%0AAuthor%3A%20Prasun%20Roy%20and%20Saumik%20Bhattacharya%20and%20Subhankar%20Ghosh%20and%20Umapada%20Pal%20and%20Michael%20Blumenstein%0AAbstract%3A%20%20%20Human%20affordance%20learning%20investigates%20contextually%20relevant%20novel%20pose%0Aprediction%20such%20that%20the%20estimated%20pose%20represents%20a%20valid%20human%20action%20within%0Athe%20scene.%20While%20the%20task%20is%20fundamental%20to%20machine%20perception%20and%20automated%0Ainteractive%20navigation%20agents%2C%20the%20exponentially%20large%20number%20of%20probable%20pose%0Aand%20action%20variations%20make%20the%20problem%20challenging%20and%20non-trivial.%20However%2C%0Athe%20existing%20datasets%20and%20methods%20for%20human%20affordance%20prediction%20in%202D%20scenes%0Aare%20significantly%20limited%20in%20the%20literature.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Across-attention%20mechanism%20to%20encode%20the%20scene%20context%20for%20affordance%20prediction%0Aby%20mutually%20attending%20spatial%20feature%20maps%20from%20two%20different%20modalities.%20The%0Aproposed%20method%20is%20disentangled%20among%20individual%20subtasks%20to%20efficiently%20reduce%0Athe%20problem%20complexity.%20First%2C%20we%20sample%20a%20probable%20location%20for%20a%20person%0Awithin%20the%20scene%20using%20a%20variational%20autoencoder%20%28VAE%29%20conditioned%20on%20the%0Aglobal%20scene%20context%20encoding.%20Next%2C%20we%20predict%20a%20potential%20pose%20template%20from%0Aa%20set%20of%20existing%20human%20pose%20candidates%20using%20a%20classifier%20on%20the%20local%20context%0Aencoding%20around%20the%20predicted%20location.%20In%20the%20subsequent%20steps%2C%20we%20use%20two%0AVAEs%20to%20sample%20the%20scale%20and%20deformation%20parameters%20for%20the%20predicted%20pose%0Atemplate%20by%20conditioning%20on%20the%20local%20context%20and%20template%20class.%20Our%0Aexperiments%20show%20significant%20improvements%20over%20the%20previous%20baseline%20of%20human%0Aaffordance%20injection%20into%20complex%202D%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13637v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Mutual%2520Cross-Modal%2520Attention%2520for%2520Context-Aware%2520Human%250A%2520%2520Affordance%2520Generation%26entry.906535625%3DPrasun%2520Roy%2520and%2520Saumik%2520Bhattacharya%2520and%2520Subhankar%2520Ghosh%2520and%2520Umapada%2520Pal%2520and%2520Michael%2520Blumenstein%26entry.1292438233%3D%2520%2520Human%2520affordance%2520learning%2520investigates%2520contextually%2520relevant%2520novel%2520pose%250Aprediction%2520such%2520that%2520the%2520estimated%2520pose%2520represents%2520a%2520valid%2520human%2520action%2520within%250Athe%2520scene.%2520While%2520the%2520task%2520is%2520fundamental%2520to%2520machine%2520perception%2520and%2520automated%250Ainteractive%2520navigation%2520agents%252C%2520the%2520exponentially%2520large%2520number%2520of%2520probable%2520pose%250Aand%2520action%2520variations%2520make%2520the%2520problem%2520challenging%2520and%2520non-trivial.%2520However%252C%250Athe%2520existing%2520datasets%2520and%2520methods%2520for%2520human%2520affordance%2520prediction%2520in%25202D%2520scenes%250Aare%2520significantly%2520limited%2520in%2520the%2520literature.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Across-attention%2520mechanism%2520to%2520encode%2520the%2520scene%2520context%2520for%2520affordance%2520prediction%250Aby%2520mutually%2520attending%2520spatial%2520feature%2520maps%2520from%2520two%2520different%2520modalities.%2520The%250Aproposed%2520method%2520is%2520disentangled%2520among%2520individual%2520subtasks%2520to%2520efficiently%2520reduce%250Athe%2520problem%2520complexity.%2520First%252C%2520we%2520sample%2520a%2520probable%2520location%2520for%2520a%2520person%250Awithin%2520the%2520scene%2520using%2520a%2520variational%2520autoencoder%2520%2528VAE%2529%2520conditioned%2520on%2520the%250Aglobal%2520scene%2520context%2520encoding.%2520Next%252C%2520we%2520predict%2520a%2520potential%2520pose%2520template%2520from%250Aa%2520set%2520of%2520existing%2520human%2520pose%2520candidates%2520using%2520a%2520classifier%2520on%2520the%2520local%2520context%250Aencoding%2520around%2520the%2520predicted%2520location.%2520In%2520the%2520subsequent%2520steps%252C%2520we%2520use%2520two%250AVAEs%2520to%2520sample%2520the%2520scale%2520and%2520deformation%2520parameters%2520for%2520the%2520predicted%2520pose%250Atemplate%2520by%2520conditioning%2520on%2520the%2520local%2520context%2520and%2520template%2520class.%2520Our%250Aexperiments%2520show%2520significant%2520improvements%2520over%2520the%2520previous%2520baseline%2520of%2520human%250Aaffordance%2520injection%2520into%2520complex%25202D%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13637v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Mutual%20Cross-Modal%20Attention%20for%20Context-Aware%20Human%0A%20%20Affordance%20Generation&entry.906535625=Prasun%20Roy%20and%20Saumik%20Bhattacharya%20and%20Subhankar%20Ghosh%20and%20Umapada%20Pal%20and%20Michael%20Blumenstein&entry.1292438233=%20%20Human%20affordance%20learning%20investigates%20contextually%20relevant%20novel%20pose%0Aprediction%20such%20that%20the%20estimated%20pose%20represents%20a%20valid%20human%20action%20within%0Athe%20scene.%20While%20the%20task%20is%20fundamental%20to%20machine%20perception%20and%20automated%0Ainteractive%20navigation%20agents%2C%20the%20exponentially%20large%20number%20of%20probable%20pose%0Aand%20action%20variations%20make%20the%20problem%20challenging%20and%20non-trivial.%20However%2C%0Athe%20existing%20datasets%20and%20methods%20for%20human%20affordance%20prediction%20in%202D%20scenes%0Aare%20significantly%20limited%20in%20the%20literature.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Across-attention%20mechanism%20to%20encode%20the%20scene%20context%20for%20affordance%20prediction%0Aby%20mutually%20attending%20spatial%20feature%20maps%20from%20two%20different%20modalities.%20The%0Aproposed%20method%20is%20disentangled%20among%20individual%20subtasks%20to%20efficiently%20reduce%0Athe%20problem%20complexity.%20First%2C%20we%20sample%20a%20probable%20location%20for%20a%20person%0Awithin%20the%20scene%20using%20a%20variational%20autoencoder%20%28VAE%29%20conditioned%20on%20the%0Aglobal%20scene%20context%20encoding.%20Next%2C%20we%20predict%20a%20potential%20pose%20template%20from%0Aa%20set%20of%20existing%20human%20pose%20candidates%20using%20a%20classifier%20on%20the%20local%20context%0Aencoding%20around%20the%20predicted%20location.%20In%20the%20subsequent%20steps%2C%20we%20use%20two%0AVAEs%20to%20sample%20the%20scale%20and%20deformation%20parameters%20for%20the%20predicted%20pose%0Atemplate%20by%20conditioning%20on%20the%20local%20context%20and%20template%20class.%20Our%0Aexperiments%20show%20significant%20improvements%20over%20the%20previous%20baseline%20of%20human%0Aaffordance%20injection%20into%20complex%202D%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13637v1&entry.124074799=Read"},
{"title": "Piece of Table: A Divide-and-Conquer Approach for Selecting Subtables in\n  Table Question Answering", "author": "Wonjin Lee and Kyumin Kim and Sungjae Lee and Jihun Lee and Kwang In Kim", "abstract": "  Applying language models (LMs) to tables is challenging due to the inherent\nstructural differences between two-dimensional tables and one-dimensional text\nfor which the LMs were originally designed. Furthermore, when applying\nlinearized tables to LMs, the maximum token lengths often imposed in\nself-attention calculations make it difficult to comprehensively understand the\ncontext spread across large tables. To address these challenges, we present\nPieTa (Piece of Table), a new framework for subtable-based question answering\n(QA). PieTa operates through an iterative process of dividing tables into\nsmaller windows, using LMs to select relevant cells within each window, and\nmerging these cells into a subtable. This multi-resolution approach captures\ndependencies across multiple rows and columns while avoiding the limitations\ncaused by long context inputs. Instantiated as a simple iterative subtable\nunion algorithm, PieTa demonstrates improved performance over previous\nsubtable-based QA approaches.\n", "link": "http://arxiv.org/abs/2412.07629v4", "date": "2025-02-19", "relevancy": 2.3477, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4711}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4688}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Piece%20of%20Table%3A%20A%20Divide-and-Conquer%20Approach%20for%20Selecting%20Subtables%20in%0A%20%20Table%20Question%20Answering&body=Title%3A%20Piece%20of%20Table%3A%20A%20Divide-and-Conquer%20Approach%20for%20Selecting%20Subtables%20in%0A%20%20Table%20Question%20Answering%0AAuthor%3A%20Wonjin%20Lee%20and%20Kyumin%20Kim%20and%20Sungjae%20Lee%20and%20Jihun%20Lee%20and%20Kwang%20In%20Kim%0AAbstract%3A%20%20%20Applying%20language%20models%20%28LMs%29%20to%20tables%20is%20challenging%20due%20to%20the%20inherent%0Astructural%20differences%20between%20two-dimensional%20tables%20and%20one-dimensional%20text%0Afor%20which%20the%20LMs%20were%20originally%20designed.%20Furthermore%2C%20when%20applying%0Alinearized%20tables%20to%20LMs%2C%20the%20maximum%20token%20lengths%20often%20imposed%20in%0Aself-attention%20calculations%20make%20it%20difficult%20to%20comprehensively%20understand%20the%0Acontext%20spread%20across%20large%20tables.%20To%20address%20these%20challenges%2C%20we%20present%0APieTa%20%28Piece%20of%20Table%29%2C%20a%20new%20framework%20for%20subtable-based%20question%20answering%0A%28QA%29.%20PieTa%20operates%20through%20an%20iterative%20process%20of%20dividing%20tables%20into%0Asmaller%20windows%2C%20using%20LMs%20to%20select%20relevant%20cells%20within%20each%20window%2C%20and%0Amerging%20these%20cells%20into%20a%20subtable.%20This%20multi-resolution%20approach%20captures%0Adependencies%20across%20multiple%20rows%20and%20columns%20while%20avoiding%20the%20limitations%0Acaused%20by%20long%20context%20inputs.%20Instantiated%20as%20a%20simple%20iterative%20subtable%0Aunion%20algorithm%2C%20PieTa%20demonstrates%20improved%20performance%20over%20previous%0Asubtable-based%20QA%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07629v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPiece%2520of%2520Table%253A%2520A%2520Divide-and-Conquer%2520Approach%2520for%2520Selecting%2520Subtables%2520in%250A%2520%2520Table%2520Question%2520Answering%26entry.906535625%3DWonjin%2520Lee%2520and%2520Kyumin%2520Kim%2520and%2520Sungjae%2520Lee%2520and%2520Jihun%2520Lee%2520and%2520Kwang%2520In%2520Kim%26entry.1292438233%3D%2520%2520Applying%2520language%2520models%2520%2528LMs%2529%2520to%2520tables%2520is%2520challenging%2520due%2520to%2520the%2520inherent%250Astructural%2520differences%2520between%2520two-dimensional%2520tables%2520and%2520one-dimensional%2520text%250Afor%2520which%2520the%2520LMs%2520were%2520originally%2520designed.%2520Furthermore%252C%2520when%2520applying%250Alinearized%2520tables%2520to%2520LMs%252C%2520the%2520maximum%2520token%2520lengths%2520often%2520imposed%2520in%250Aself-attention%2520calculations%2520make%2520it%2520difficult%2520to%2520comprehensively%2520understand%2520the%250Acontext%2520spread%2520across%2520large%2520tables.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%250APieTa%2520%2528Piece%2520of%2520Table%2529%252C%2520a%2520new%2520framework%2520for%2520subtable-based%2520question%2520answering%250A%2528QA%2529.%2520PieTa%2520operates%2520through%2520an%2520iterative%2520process%2520of%2520dividing%2520tables%2520into%250Asmaller%2520windows%252C%2520using%2520LMs%2520to%2520select%2520relevant%2520cells%2520within%2520each%2520window%252C%2520and%250Amerging%2520these%2520cells%2520into%2520a%2520subtable.%2520This%2520multi-resolution%2520approach%2520captures%250Adependencies%2520across%2520multiple%2520rows%2520and%2520columns%2520while%2520avoiding%2520the%2520limitations%250Acaused%2520by%2520long%2520context%2520inputs.%2520Instantiated%2520as%2520a%2520simple%2520iterative%2520subtable%250Aunion%2520algorithm%252C%2520PieTa%2520demonstrates%2520improved%2520performance%2520over%2520previous%250Asubtable-based%2520QA%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07629v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Piece%20of%20Table%3A%20A%20Divide-and-Conquer%20Approach%20for%20Selecting%20Subtables%20in%0A%20%20Table%20Question%20Answering&entry.906535625=Wonjin%20Lee%20and%20Kyumin%20Kim%20and%20Sungjae%20Lee%20and%20Jihun%20Lee%20and%20Kwang%20In%20Kim&entry.1292438233=%20%20Applying%20language%20models%20%28LMs%29%20to%20tables%20is%20challenging%20due%20to%20the%20inherent%0Astructural%20differences%20between%20two-dimensional%20tables%20and%20one-dimensional%20text%0Afor%20which%20the%20LMs%20were%20originally%20designed.%20Furthermore%2C%20when%20applying%0Alinearized%20tables%20to%20LMs%2C%20the%20maximum%20token%20lengths%20often%20imposed%20in%0Aself-attention%20calculations%20make%20it%20difficult%20to%20comprehensively%20understand%20the%0Acontext%20spread%20across%20large%20tables.%20To%20address%20these%20challenges%2C%20we%20present%0APieTa%20%28Piece%20of%20Table%29%2C%20a%20new%20framework%20for%20subtable-based%20question%20answering%0A%28QA%29.%20PieTa%20operates%20through%20an%20iterative%20process%20of%20dividing%20tables%20into%0Asmaller%20windows%2C%20using%20LMs%20to%20select%20relevant%20cells%20within%20each%20window%2C%20and%0Amerging%20these%20cells%20into%20a%20subtable.%20This%20multi-resolution%20approach%20captures%0Adependencies%20across%20multiple%20rows%20and%20columns%20while%20avoiding%20the%20limitations%0Acaused%20by%20long%20context%20inputs.%20Instantiated%20as%20a%20simple%20iterative%20subtable%0Aunion%20algorithm%2C%20PieTa%20demonstrates%20improved%20performance%20over%20previous%0Asubtable-based%20QA%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07629v4&entry.124074799=Read"},
{"title": "Mesh-based Super-Resolution of Fluid Flows with Multiscale Graph Neural\n  Networks", "author": "Shivam Barwey and Pinaki Pal and Saumil Patel and Riccardo Balin and Bethany Lusch and Venkatram Vishwanath and Romit Maulik and Ramesh Balakrishnan", "abstract": "  A graph neural network (GNN) approach is introduced in this work which\nenables mesh-based three-dimensional super-resolution of fluid flows. In this\nframework, the GNN is designed to operate not on the full mesh-based field at\nonce, but on localized meshes of elements (or cells) directly. To facilitate\nmesh-based GNN representations in a manner similar to spectral (or finite)\nelement discretizations, a baseline GNN layer (termed a message passing layer,\nwhich updates local node properties) is modified to account for synchronization\nof coincident graph nodes, rendering compatibility with commonly used\nelement-based mesh connectivities. The architecture is multiscale in nature,\nand is comprised of a combination of coarse-scale and fine-scale message\npassing layer sequences (termed processors) separated by a graph unpooling\nlayer. The coarse-scale processor embeds a query element (alongside a set\nnumber of neighboring coarse elements) into a single latent graph\nrepresentation using coarse-scale synchronized message passing over the element\nneighborhood, and the fine-scale processor leverages additional message passing\noperations on this latent graph to correct for interpolation errors.\nDemonstration studies are performed using hexahedral mesh-based data from\nTaylor-Green Vortex and backward-facing step flow simulations at Reynolds\nnumbers of 1600 and 3200. Through analysis of both global and local errors, the\nresults ultimately show how the GNN is able to produce accurate super-resolved\nfields compared to targets in both coarse-scale and multiscale model\nconfigurations. Reconstruction errors for fixed architectures were found to\nincrease in proportion to the Reynolds number. Geometry extrapolation studies\non a separate cavity flow configuration show promising cross-mesh capabilities\nof the super-resolution strategy.\n", "link": "http://arxiv.org/abs/2409.07769v3", "date": "2025-02-19", "relevancy": 2.3452, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6384}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5848}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mesh-based%20Super-Resolution%20of%20Fluid%20Flows%20with%20Multiscale%20Graph%20Neural%0A%20%20Networks&body=Title%3A%20Mesh-based%20Super-Resolution%20of%20Fluid%20Flows%20with%20Multiscale%20Graph%20Neural%0A%20%20Networks%0AAuthor%3A%20Shivam%20Barwey%20and%20Pinaki%20Pal%20and%20Saumil%20Patel%20and%20Riccardo%20Balin%20and%20Bethany%20Lusch%20and%20Venkatram%20Vishwanath%20and%20Romit%20Maulik%20and%20Ramesh%20Balakrishnan%0AAbstract%3A%20%20%20A%20graph%20neural%20network%20%28GNN%29%20approach%20is%20introduced%20in%20this%20work%20which%0Aenables%20mesh-based%20three-dimensional%20super-resolution%20of%20fluid%20flows.%20In%20this%0Aframework%2C%20the%20GNN%20is%20designed%20to%20operate%20not%20on%20the%20full%20mesh-based%20field%20at%0Aonce%2C%20but%20on%20localized%20meshes%20of%20elements%20%28or%20cells%29%20directly.%20To%20facilitate%0Amesh-based%20GNN%20representations%20in%20a%20manner%20similar%20to%20spectral%20%28or%20finite%29%0Aelement%20discretizations%2C%20a%20baseline%20GNN%20layer%20%28termed%20a%20message%20passing%20layer%2C%0Awhich%20updates%20local%20node%20properties%29%20is%20modified%20to%20account%20for%20synchronization%0Aof%20coincident%20graph%20nodes%2C%20rendering%20compatibility%20with%20commonly%20used%0Aelement-based%20mesh%20connectivities.%20The%20architecture%20is%20multiscale%20in%20nature%2C%0Aand%20is%20comprised%20of%20a%20combination%20of%20coarse-scale%20and%20fine-scale%20message%0Apassing%20layer%20sequences%20%28termed%20processors%29%20separated%20by%20a%20graph%20unpooling%0Alayer.%20The%20coarse-scale%20processor%20embeds%20a%20query%20element%20%28alongside%20a%20set%0Anumber%20of%20neighboring%20coarse%20elements%29%20into%20a%20single%20latent%20graph%0Arepresentation%20using%20coarse-scale%20synchronized%20message%20passing%20over%20the%20element%0Aneighborhood%2C%20and%20the%20fine-scale%20processor%20leverages%20additional%20message%20passing%0Aoperations%20on%20this%20latent%20graph%20to%20correct%20for%20interpolation%20errors.%0ADemonstration%20studies%20are%20performed%20using%20hexahedral%20mesh-based%20data%20from%0ATaylor-Green%20Vortex%20and%20backward-facing%20step%20flow%20simulations%20at%20Reynolds%0Anumbers%20of%201600%20and%203200.%20Through%20analysis%20of%20both%20global%20and%20local%20errors%2C%20the%0Aresults%20ultimately%20show%20how%20the%20GNN%20is%20able%20to%20produce%20accurate%20super-resolved%0Afields%20compared%20to%20targets%20in%20both%20coarse-scale%20and%20multiscale%20model%0Aconfigurations.%20Reconstruction%20errors%20for%20fixed%20architectures%20were%20found%20to%0Aincrease%20in%20proportion%20to%20the%20Reynolds%20number.%20Geometry%20extrapolation%20studies%0Aon%20a%20separate%20cavity%20flow%20configuration%20show%20promising%20cross-mesh%20capabilities%0Aof%20the%20super-resolution%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07769v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMesh-based%2520Super-Resolution%2520of%2520Fluid%2520Flows%2520with%2520Multiscale%2520Graph%2520Neural%250A%2520%2520Networks%26entry.906535625%3DShivam%2520Barwey%2520and%2520Pinaki%2520Pal%2520and%2520Saumil%2520Patel%2520and%2520Riccardo%2520Balin%2520and%2520Bethany%2520Lusch%2520and%2520Venkatram%2520Vishwanath%2520and%2520Romit%2520Maulik%2520and%2520Ramesh%2520Balakrishnan%26entry.1292438233%3D%2520%2520A%2520graph%2520neural%2520network%2520%2528GNN%2529%2520approach%2520is%2520introduced%2520in%2520this%2520work%2520which%250Aenables%2520mesh-based%2520three-dimensional%2520super-resolution%2520of%2520fluid%2520flows.%2520In%2520this%250Aframework%252C%2520the%2520GNN%2520is%2520designed%2520to%2520operate%2520not%2520on%2520the%2520full%2520mesh-based%2520field%2520at%250Aonce%252C%2520but%2520on%2520localized%2520meshes%2520of%2520elements%2520%2528or%2520cells%2529%2520directly.%2520To%2520facilitate%250Amesh-based%2520GNN%2520representations%2520in%2520a%2520manner%2520similar%2520to%2520spectral%2520%2528or%2520finite%2529%250Aelement%2520discretizations%252C%2520a%2520baseline%2520GNN%2520layer%2520%2528termed%2520a%2520message%2520passing%2520layer%252C%250Awhich%2520updates%2520local%2520node%2520properties%2529%2520is%2520modified%2520to%2520account%2520for%2520synchronization%250Aof%2520coincident%2520graph%2520nodes%252C%2520rendering%2520compatibility%2520with%2520commonly%2520used%250Aelement-based%2520mesh%2520connectivities.%2520The%2520architecture%2520is%2520multiscale%2520in%2520nature%252C%250Aand%2520is%2520comprised%2520of%2520a%2520combination%2520of%2520coarse-scale%2520and%2520fine-scale%2520message%250Apassing%2520layer%2520sequences%2520%2528termed%2520processors%2529%2520separated%2520by%2520a%2520graph%2520unpooling%250Alayer.%2520The%2520coarse-scale%2520processor%2520embeds%2520a%2520query%2520element%2520%2528alongside%2520a%2520set%250Anumber%2520of%2520neighboring%2520coarse%2520elements%2529%2520into%2520a%2520single%2520latent%2520graph%250Arepresentation%2520using%2520coarse-scale%2520synchronized%2520message%2520passing%2520over%2520the%2520element%250Aneighborhood%252C%2520and%2520the%2520fine-scale%2520processor%2520leverages%2520additional%2520message%2520passing%250Aoperations%2520on%2520this%2520latent%2520graph%2520to%2520correct%2520for%2520interpolation%2520errors.%250ADemonstration%2520studies%2520are%2520performed%2520using%2520hexahedral%2520mesh-based%2520data%2520from%250ATaylor-Green%2520Vortex%2520and%2520backward-facing%2520step%2520flow%2520simulations%2520at%2520Reynolds%250Anumbers%2520of%25201600%2520and%25203200.%2520Through%2520analysis%2520of%2520both%2520global%2520and%2520local%2520errors%252C%2520the%250Aresults%2520ultimately%2520show%2520how%2520the%2520GNN%2520is%2520able%2520to%2520produce%2520accurate%2520super-resolved%250Afields%2520compared%2520to%2520targets%2520in%2520both%2520coarse-scale%2520and%2520multiscale%2520model%250Aconfigurations.%2520Reconstruction%2520errors%2520for%2520fixed%2520architectures%2520were%2520found%2520to%250Aincrease%2520in%2520proportion%2520to%2520the%2520Reynolds%2520number.%2520Geometry%2520extrapolation%2520studies%250Aon%2520a%2520separate%2520cavity%2520flow%2520configuration%2520show%2520promising%2520cross-mesh%2520capabilities%250Aof%2520the%2520super-resolution%2520strategy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07769v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mesh-based%20Super-Resolution%20of%20Fluid%20Flows%20with%20Multiscale%20Graph%20Neural%0A%20%20Networks&entry.906535625=Shivam%20Barwey%20and%20Pinaki%20Pal%20and%20Saumil%20Patel%20and%20Riccardo%20Balin%20and%20Bethany%20Lusch%20and%20Venkatram%20Vishwanath%20and%20Romit%20Maulik%20and%20Ramesh%20Balakrishnan&entry.1292438233=%20%20A%20graph%20neural%20network%20%28GNN%29%20approach%20is%20introduced%20in%20this%20work%20which%0Aenables%20mesh-based%20three-dimensional%20super-resolution%20of%20fluid%20flows.%20In%20this%0Aframework%2C%20the%20GNN%20is%20designed%20to%20operate%20not%20on%20the%20full%20mesh-based%20field%20at%0Aonce%2C%20but%20on%20localized%20meshes%20of%20elements%20%28or%20cells%29%20directly.%20To%20facilitate%0Amesh-based%20GNN%20representations%20in%20a%20manner%20similar%20to%20spectral%20%28or%20finite%29%0Aelement%20discretizations%2C%20a%20baseline%20GNN%20layer%20%28termed%20a%20message%20passing%20layer%2C%0Awhich%20updates%20local%20node%20properties%29%20is%20modified%20to%20account%20for%20synchronization%0Aof%20coincident%20graph%20nodes%2C%20rendering%20compatibility%20with%20commonly%20used%0Aelement-based%20mesh%20connectivities.%20The%20architecture%20is%20multiscale%20in%20nature%2C%0Aand%20is%20comprised%20of%20a%20combination%20of%20coarse-scale%20and%20fine-scale%20message%0Apassing%20layer%20sequences%20%28termed%20processors%29%20separated%20by%20a%20graph%20unpooling%0Alayer.%20The%20coarse-scale%20processor%20embeds%20a%20query%20element%20%28alongside%20a%20set%0Anumber%20of%20neighboring%20coarse%20elements%29%20into%20a%20single%20latent%20graph%0Arepresentation%20using%20coarse-scale%20synchronized%20message%20passing%20over%20the%20element%0Aneighborhood%2C%20and%20the%20fine-scale%20processor%20leverages%20additional%20message%20passing%0Aoperations%20on%20this%20latent%20graph%20to%20correct%20for%20interpolation%20errors.%0ADemonstration%20studies%20are%20performed%20using%20hexahedral%20mesh-based%20data%20from%0ATaylor-Green%20Vortex%20and%20backward-facing%20step%20flow%20simulations%20at%20Reynolds%0Anumbers%20of%201600%20and%203200.%20Through%20analysis%20of%20both%20global%20and%20local%20errors%2C%20the%0Aresults%20ultimately%20show%20how%20the%20GNN%20is%20able%20to%20produce%20accurate%20super-resolved%0Afields%20compared%20to%20targets%20in%20both%20coarse-scale%20and%20multiscale%20model%0Aconfigurations.%20Reconstruction%20errors%20for%20fixed%20architectures%20were%20found%20to%0Aincrease%20in%20proportion%20to%20the%20Reynolds%20number.%20Geometry%20extrapolation%20studies%0Aon%20a%20separate%20cavity%20flow%20configuration%20show%20promising%20cross-mesh%20capabilities%0Aof%20the%20super-resolution%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07769v3&entry.124074799=Read"},
{"title": "Bias Similarity Across Large Language Models", "author": "Hyejun Jeong and Shiqing Ma and Amir Houmansadr", "abstract": "  Bias in machine learning models, particularly in Large Language Models, is a\ncritical issue as these systems shape important societal decisions. While\nprevious studies have examined bias in individual LLMs, comparisons of bias\nacross models remain underexplored. To address this gap, we analyze 13 LLMs\nfrom five families, evaluating bias through output distribution across multiple\ndimensions using two datasets (4K and 1M questions). Our results show that\nfine-tuning has minimal impact on output distributions, and proprietary models\ntend to overly response as unknowns to minimize bias, compromising accuracy and\nutility. In addition, open-source models like Llama3-Chat and Gemma2-it\ndemonstrate fairness comparable to proprietary models like GPT-4, challenging\nthe assumption that larger, closed-source models are inherently less biased. We\nalso find that bias scores for disambiguated questions are more extreme,\nraising concerns about reverse discrimination. These findings highlight the\nneed for improved bias mitigation strategies and more comprehensive evaluation\nmetrics for fairness in LLMs.\n", "link": "http://arxiv.org/abs/2410.12010v2", "date": "2025-02-19", "relevancy": 2.3407, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4766}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4766}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bias%20Similarity%20Across%20Large%20Language%20Models&body=Title%3A%20Bias%20Similarity%20Across%20Large%20Language%20Models%0AAuthor%3A%20Hyejun%20Jeong%20and%20Shiqing%20Ma%20and%20Amir%20Houmansadr%0AAbstract%3A%20%20%20Bias%20in%20machine%20learning%20models%2C%20particularly%20in%20Large%20Language%20Models%2C%20is%20a%0Acritical%20issue%20as%20these%20systems%20shape%20important%20societal%20decisions.%20While%0Aprevious%20studies%20have%20examined%20bias%20in%20individual%20LLMs%2C%20comparisons%20of%20bias%0Aacross%20models%20remain%20underexplored.%20To%20address%20this%20gap%2C%20we%20analyze%2013%20LLMs%0Afrom%20five%20families%2C%20evaluating%20bias%20through%20output%20distribution%20across%20multiple%0Adimensions%20using%20two%20datasets%20%284K%20and%201M%20questions%29.%20Our%20results%20show%20that%0Afine-tuning%20has%20minimal%20impact%20on%20output%20distributions%2C%20and%20proprietary%20models%0Atend%20to%20overly%20response%20as%20unknowns%20to%20minimize%20bias%2C%20compromising%20accuracy%20and%0Autility.%20In%20addition%2C%20open-source%20models%20like%20Llama3-Chat%20and%20Gemma2-it%0Ademonstrate%20fairness%20comparable%20to%20proprietary%20models%20like%20GPT-4%2C%20challenging%0Athe%20assumption%20that%20larger%2C%20closed-source%20models%20are%20inherently%20less%20biased.%20We%0Aalso%20find%20that%20bias%20scores%20for%20disambiguated%20questions%20are%20more%20extreme%2C%0Araising%20concerns%20about%20reverse%20discrimination.%20These%20findings%20highlight%20the%0Aneed%20for%20improved%20bias%20mitigation%20strategies%20and%20more%20comprehensive%20evaluation%0Ametrics%20for%20fairness%20in%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12010v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBias%2520Similarity%2520Across%2520Large%2520Language%2520Models%26entry.906535625%3DHyejun%2520Jeong%2520and%2520Shiqing%2520Ma%2520and%2520Amir%2520Houmansadr%26entry.1292438233%3D%2520%2520Bias%2520in%2520machine%2520learning%2520models%252C%2520particularly%2520in%2520Large%2520Language%2520Models%252C%2520is%2520a%250Acritical%2520issue%2520as%2520these%2520systems%2520shape%2520important%2520societal%2520decisions.%2520While%250Aprevious%2520studies%2520have%2520examined%2520bias%2520in%2520individual%2520LLMs%252C%2520comparisons%2520of%2520bias%250Aacross%2520models%2520remain%2520underexplored.%2520To%2520address%2520this%2520gap%252C%2520we%2520analyze%252013%2520LLMs%250Afrom%2520five%2520families%252C%2520evaluating%2520bias%2520through%2520output%2520distribution%2520across%2520multiple%250Adimensions%2520using%2520two%2520datasets%2520%25284K%2520and%25201M%2520questions%2529.%2520Our%2520results%2520show%2520that%250Afine-tuning%2520has%2520minimal%2520impact%2520on%2520output%2520distributions%252C%2520and%2520proprietary%2520models%250Atend%2520to%2520overly%2520response%2520as%2520unknowns%2520to%2520minimize%2520bias%252C%2520compromising%2520accuracy%2520and%250Autility.%2520In%2520addition%252C%2520open-source%2520models%2520like%2520Llama3-Chat%2520and%2520Gemma2-it%250Ademonstrate%2520fairness%2520comparable%2520to%2520proprietary%2520models%2520like%2520GPT-4%252C%2520challenging%250Athe%2520assumption%2520that%2520larger%252C%2520closed-source%2520models%2520are%2520inherently%2520less%2520biased.%2520We%250Aalso%2520find%2520that%2520bias%2520scores%2520for%2520disambiguated%2520questions%2520are%2520more%2520extreme%252C%250Araising%2520concerns%2520about%2520reverse%2520discrimination.%2520These%2520findings%2520highlight%2520the%250Aneed%2520for%2520improved%2520bias%2520mitigation%2520strategies%2520and%2520more%2520comprehensive%2520evaluation%250Ametrics%2520for%2520fairness%2520in%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12010v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bias%20Similarity%20Across%20Large%20Language%20Models&entry.906535625=Hyejun%20Jeong%20and%20Shiqing%20Ma%20and%20Amir%20Houmansadr&entry.1292438233=%20%20Bias%20in%20machine%20learning%20models%2C%20particularly%20in%20Large%20Language%20Models%2C%20is%20a%0Acritical%20issue%20as%20these%20systems%20shape%20important%20societal%20decisions.%20While%0Aprevious%20studies%20have%20examined%20bias%20in%20individual%20LLMs%2C%20comparisons%20of%20bias%0Aacross%20models%20remain%20underexplored.%20To%20address%20this%20gap%2C%20we%20analyze%2013%20LLMs%0Afrom%20five%20families%2C%20evaluating%20bias%20through%20output%20distribution%20across%20multiple%0Adimensions%20using%20two%20datasets%20%284K%20and%201M%20questions%29.%20Our%20results%20show%20that%0Afine-tuning%20has%20minimal%20impact%20on%20output%20distributions%2C%20and%20proprietary%20models%0Atend%20to%20overly%20response%20as%20unknowns%20to%20minimize%20bias%2C%20compromising%20accuracy%20and%0Autility.%20In%20addition%2C%20open-source%20models%20like%20Llama3-Chat%20and%20Gemma2-it%0Ademonstrate%20fairness%20comparable%20to%20proprietary%20models%20like%20GPT-4%2C%20challenging%0Athe%20assumption%20that%20larger%2C%20closed-source%20models%20are%20inherently%20less%20biased.%20We%0Aalso%20find%20that%20bias%20scores%20for%20disambiguated%20questions%20are%20more%20extreme%2C%0Araising%20concerns%20about%20reverse%20discrimination.%20These%20findings%20highlight%20the%0Aneed%20for%20improved%20bias%20mitigation%20strategies%20and%20more%20comprehensive%20evaluation%0Ametrics%20for%20fairness%20in%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12010v2&entry.124074799=Read"},
{"title": "Why Sample Space Matters: Keyframe Sampling Optimization for LiDAR-based\n  Place Recognition", "author": "Nikolaos Stathoulopoulos and Vidya Sumathy and Christoforos Kanellakis and George Nikolakopoulos", "abstract": "  Recent advances in robotics are driving real-world autonomy for long-term and\nlarge-scale missions, where loop closures via place recognition are vital for\nmitigating pose estimation drift. However, achieving real-time performance\nremains challenging for resource-constrained mobile robots and multi-robot\nsystems due to the computational burden of high-density sampling, which\nincreases the complexity of comparing and verifying query samples against a\ngrowing map database. Conventional methods often retain redundant information\nor miss critical data by relying on fixed sampling intervals or operating in\n3-D space instead of the descriptor feature space. To address these challenges,\nwe introduce the concept of sample space and propose a novel keyframe sampling\napproach for LiDAR-based place recognition. Our method minimizes redundancy\nwhile preserving essential information in the hyper-dimensional descriptor\nspace, supporting both learning-based and handcrafted descriptors. The proposed\napproach incorporates a sliding window optimization strategy to ensure\nefficient keyframe selection and real-time performance, enabling seamless\nintegration into robotic pipelines. In sum, our approach demonstrates robust\nperformance across diverse datasets, with the ability to adapt seamlessly from\nindoor to outdoor scenarios without parameter tuning, reducing loop closure\ndetection times and memory requirements.\n", "link": "http://arxiv.org/abs/2410.02643v2", "date": "2025-02-19", "relevancy": 2.3197, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5843}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.58}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Sample%20Space%20Matters%3A%20Keyframe%20Sampling%20Optimization%20for%20LiDAR-based%0A%20%20Place%20Recognition&body=Title%3A%20Why%20Sample%20Space%20Matters%3A%20Keyframe%20Sampling%20Optimization%20for%20LiDAR-based%0A%20%20Place%20Recognition%0AAuthor%3A%20Nikolaos%20Stathoulopoulos%20and%20Vidya%20Sumathy%20and%20Christoforos%20Kanellakis%20and%20George%20Nikolakopoulos%0AAbstract%3A%20%20%20Recent%20advances%20in%20robotics%20are%20driving%20real-world%20autonomy%20for%20long-term%20and%0Alarge-scale%20missions%2C%20where%20loop%20closures%20via%20place%20recognition%20are%20vital%20for%0Amitigating%20pose%20estimation%20drift.%20However%2C%20achieving%20real-time%20performance%0Aremains%20challenging%20for%20resource-constrained%20mobile%20robots%20and%20multi-robot%0Asystems%20due%20to%20the%20computational%20burden%20of%20high-density%20sampling%2C%20which%0Aincreases%20the%20complexity%20of%20comparing%20and%20verifying%20query%20samples%20against%20a%0Agrowing%20map%20database.%20Conventional%20methods%20often%20retain%20redundant%20information%0Aor%20miss%20critical%20data%20by%20relying%20on%20fixed%20sampling%20intervals%20or%20operating%20in%0A3-D%20space%20instead%20of%20the%20descriptor%20feature%20space.%20To%20address%20these%20challenges%2C%0Awe%20introduce%20the%20concept%20of%20sample%20space%20and%20propose%20a%20novel%20keyframe%20sampling%0Aapproach%20for%20LiDAR-based%20place%20recognition.%20Our%20method%20minimizes%20redundancy%0Awhile%20preserving%20essential%20information%20in%20the%20hyper-dimensional%20descriptor%0Aspace%2C%20supporting%20both%20learning-based%20and%20handcrafted%20descriptors.%20The%20proposed%0Aapproach%20incorporates%20a%20sliding%20window%20optimization%20strategy%20to%20ensure%0Aefficient%20keyframe%20selection%20and%20real-time%20performance%2C%20enabling%20seamless%0Aintegration%20into%20robotic%20pipelines.%20In%20sum%2C%20our%20approach%20demonstrates%20robust%0Aperformance%20across%20diverse%20datasets%2C%20with%20the%20ability%20to%20adapt%20seamlessly%20from%0Aindoor%20to%20outdoor%20scenarios%20without%20parameter%20tuning%2C%20reducing%20loop%20closure%0Adetection%20times%20and%20memory%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02643v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Sample%2520Space%2520Matters%253A%2520Keyframe%2520Sampling%2520Optimization%2520for%2520LiDAR-based%250A%2520%2520Place%2520Recognition%26entry.906535625%3DNikolaos%2520Stathoulopoulos%2520and%2520Vidya%2520Sumathy%2520and%2520Christoforos%2520Kanellakis%2520and%2520George%2520Nikolakopoulos%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520robotics%2520are%2520driving%2520real-world%2520autonomy%2520for%2520long-term%2520and%250Alarge-scale%2520missions%252C%2520where%2520loop%2520closures%2520via%2520place%2520recognition%2520are%2520vital%2520for%250Amitigating%2520pose%2520estimation%2520drift.%2520However%252C%2520achieving%2520real-time%2520performance%250Aremains%2520challenging%2520for%2520resource-constrained%2520mobile%2520robots%2520and%2520multi-robot%250Asystems%2520due%2520to%2520the%2520computational%2520burden%2520of%2520high-density%2520sampling%252C%2520which%250Aincreases%2520the%2520complexity%2520of%2520comparing%2520and%2520verifying%2520query%2520samples%2520against%2520a%250Agrowing%2520map%2520database.%2520Conventional%2520methods%2520often%2520retain%2520redundant%2520information%250Aor%2520miss%2520critical%2520data%2520by%2520relying%2520on%2520fixed%2520sampling%2520intervals%2520or%2520operating%2520in%250A3-D%2520space%2520instead%2520of%2520the%2520descriptor%2520feature%2520space.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520introduce%2520the%2520concept%2520of%2520sample%2520space%2520and%2520propose%2520a%2520novel%2520keyframe%2520sampling%250Aapproach%2520for%2520LiDAR-based%2520place%2520recognition.%2520Our%2520method%2520minimizes%2520redundancy%250Awhile%2520preserving%2520essential%2520information%2520in%2520the%2520hyper-dimensional%2520descriptor%250Aspace%252C%2520supporting%2520both%2520learning-based%2520and%2520handcrafted%2520descriptors.%2520The%2520proposed%250Aapproach%2520incorporates%2520a%2520sliding%2520window%2520optimization%2520strategy%2520to%2520ensure%250Aefficient%2520keyframe%2520selection%2520and%2520real-time%2520performance%252C%2520enabling%2520seamless%250Aintegration%2520into%2520robotic%2520pipelines.%2520In%2520sum%252C%2520our%2520approach%2520demonstrates%2520robust%250Aperformance%2520across%2520diverse%2520datasets%252C%2520with%2520the%2520ability%2520to%2520adapt%2520seamlessly%2520from%250Aindoor%2520to%2520outdoor%2520scenarios%2520without%2520parameter%2520tuning%252C%2520reducing%2520loop%2520closure%250Adetection%2520times%2520and%2520memory%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02643v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Sample%20Space%20Matters%3A%20Keyframe%20Sampling%20Optimization%20for%20LiDAR-based%0A%20%20Place%20Recognition&entry.906535625=Nikolaos%20Stathoulopoulos%20and%20Vidya%20Sumathy%20and%20Christoforos%20Kanellakis%20and%20George%20Nikolakopoulos&entry.1292438233=%20%20Recent%20advances%20in%20robotics%20are%20driving%20real-world%20autonomy%20for%20long-term%20and%0Alarge-scale%20missions%2C%20where%20loop%20closures%20via%20place%20recognition%20are%20vital%20for%0Amitigating%20pose%20estimation%20drift.%20However%2C%20achieving%20real-time%20performance%0Aremains%20challenging%20for%20resource-constrained%20mobile%20robots%20and%20multi-robot%0Asystems%20due%20to%20the%20computational%20burden%20of%20high-density%20sampling%2C%20which%0Aincreases%20the%20complexity%20of%20comparing%20and%20verifying%20query%20samples%20against%20a%0Agrowing%20map%20database.%20Conventional%20methods%20often%20retain%20redundant%20information%0Aor%20miss%20critical%20data%20by%20relying%20on%20fixed%20sampling%20intervals%20or%20operating%20in%0A3-D%20space%20instead%20of%20the%20descriptor%20feature%20space.%20To%20address%20these%20challenges%2C%0Awe%20introduce%20the%20concept%20of%20sample%20space%20and%20propose%20a%20novel%20keyframe%20sampling%0Aapproach%20for%20LiDAR-based%20place%20recognition.%20Our%20method%20minimizes%20redundancy%0Awhile%20preserving%20essential%20information%20in%20the%20hyper-dimensional%20descriptor%0Aspace%2C%20supporting%20both%20learning-based%20and%20handcrafted%20descriptors.%20The%20proposed%0Aapproach%20incorporates%20a%20sliding%20window%20optimization%20strategy%20to%20ensure%0Aefficient%20keyframe%20selection%20and%20real-time%20performance%2C%20enabling%20seamless%0Aintegration%20into%20robotic%20pipelines.%20In%20sum%2C%20our%20approach%20demonstrates%20robust%0Aperformance%20across%20diverse%20datasets%2C%20with%20the%20ability%20to%20adapt%20seamlessly%20from%0Aindoor%20to%20outdoor%20scenarios%20without%20parameter%20tuning%2C%20reducing%20loop%20closure%0Adetection%20times%20and%20memory%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02643v2&entry.124074799=Read"},
{"title": "Where's the Bug? Attention Probing for Scalable Fault Localization", "author": "Adam Stein and Arthur Wayne and Aaditya Naik and Mayur Naik and Eric Wong", "abstract": "  Ensuring code correctness remains a challenging problem even as large\nlanguage models (LLMs) become increasingly capable at code-related tasks. While\nLLM-based program repair systems can propose bug fixes using only a user's bug\nreport, their effectiveness is fundamentally limited by their ability to\nperform fault localization (FL), a challenging problem for both humans and\nLLMs. Existing FL approaches rely on executable test cases, require training on\ncostly and often noisy line-level annotations, or demand resource-intensive\nLLMs. In this paper, we present Bug Attention Probe (BAP), a method which\nlearns state-of-the-art fault localization without any direct localization\nlabels, outperforming traditional FL baselines and prompting of large-scale\nLLMs. We evaluate our approach across a variety of code settings, including\nreal-world Java bugs from the standard Defects4J dataset as well as seven other\ndatasets which span a diverse set of bug types and languages. Averaged across\nall eight datasets, BAP improves by 34.6% top-1 accuracy compared to the\nstrongest baseline and 93.4% over zero-shot prompting GPT-4o. BAP is also\nsignificantly more efficient than prompting, outperforming large open-weight\nmodels at a small fraction of the computational cost.\n", "link": "http://arxiv.org/abs/2502.13966v1", "date": "2025-02-19", "relevancy": 2.319, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4813}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Where%27s%20the%20Bug%3F%20Attention%20Probing%20for%20Scalable%20Fault%20Localization&body=Title%3A%20Where%27s%20the%20Bug%3F%20Attention%20Probing%20for%20Scalable%20Fault%20Localization%0AAuthor%3A%20Adam%20Stein%20and%20Arthur%20Wayne%20and%20Aaditya%20Naik%20and%20Mayur%20Naik%20and%20Eric%20Wong%0AAbstract%3A%20%20%20Ensuring%20code%20correctness%20remains%20a%20challenging%20problem%20even%20as%20large%0Alanguage%20models%20%28LLMs%29%20become%20increasingly%20capable%20at%20code-related%20tasks.%20While%0ALLM-based%20program%20repair%20systems%20can%20propose%20bug%20fixes%20using%20only%20a%20user%27s%20bug%0Areport%2C%20their%20effectiveness%20is%20fundamentally%20limited%20by%20their%20ability%20to%0Aperform%20fault%20localization%20%28FL%29%2C%20a%20challenging%20problem%20for%20both%20humans%20and%0ALLMs.%20Existing%20FL%20approaches%20rely%20on%20executable%20test%20cases%2C%20require%20training%20on%0Acostly%20and%20often%20noisy%20line-level%20annotations%2C%20or%20demand%20resource-intensive%0ALLMs.%20In%20this%20paper%2C%20we%20present%20Bug%20Attention%20Probe%20%28BAP%29%2C%20a%20method%20which%0Alearns%20state-of-the-art%20fault%20localization%20without%20any%20direct%20localization%0Alabels%2C%20outperforming%20traditional%20FL%20baselines%20and%20prompting%20of%20large-scale%0ALLMs.%20We%20evaluate%20our%20approach%20across%20a%20variety%20of%20code%20settings%2C%20including%0Areal-world%20Java%20bugs%20from%20the%20standard%20Defects4J%20dataset%20as%20well%20as%20seven%20other%0Adatasets%20which%20span%20a%20diverse%20set%20of%20bug%20types%20and%20languages.%20Averaged%20across%0Aall%20eight%20datasets%2C%20BAP%20improves%20by%2034.6%25%20top-1%20accuracy%20compared%20to%20the%0Astrongest%20baseline%20and%2093.4%25%20over%20zero-shot%20prompting%20GPT-4o.%20BAP%20is%20also%0Asignificantly%20more%20efficient%20than%20prompting%2C%20outperforming%20large%20open-weight%0Amodels%20at%20a%20small%20fraction%20of%20the%20computational%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhere%2527s%2520the%2520Bug%253F%2520Attention%2520Probing%2520for%2520Scalable%2520Fault%2520Localization%26entry.906535625%3DAdam%2520Stein%2520and%2520Arthur%2520Wayne%2520and%2520Aaditya%2520Naik%2520and%2520Mayur%2520Naik%2520and%2520Eric%2520Wong%26entry.1292438233%3D%2520%2520Ensuring%2520code%2520correctness%2520remains%2520a%2520challenging%2520problem%2520even%2520as%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520become%2520increasingly%2520capable%2520at%2520code-related%2520tasks.%2520While%250ALLM-based%2520program%2520repair%2520systems%2520can%2520propose%2520bug%2520fixes%2520using%2520only%2520a%2520user%2527s%2520bug%250Areport%252C%2520their%2520effectiveness%2520is%2520fundamentally%2520limited%2520by%2520their%2520ability%2520to%250Aperform%2520fault%2520localization%2520%2528FL%2529%252C%2520a%2520challenging%2520problem%2520for%2520both%2520humans%2520and%250ALLMs.%2520Existing%2520FL%2520approaches%2520rely%2520on%2520executable%2520test%2520cases%252C%2520require%2520training%2520on%250Acostly%2520and%2520often%2520noisy%2520line-level%2520annotations%252C%2520or%2520demand%2520resource-intensive%250ALLMs.%2520In%2520this%2520paper%252C%2520we%2520present%2520Bug%2520Attention%2520Probe%2520%2528BAP%2529%252C%2520a%2520method%2520which%250Alearns%2520state-of-the-art%2520fault%2520localization%2520without%2520any%2520direct%2520localization%250Alabels%252C%2520outperforming%2520traditional%2520FL%2520baselines%2520and%2520prompting%2520of%2520large-scale%250ALLMs.%2520We%2520evaluate%2520our%2520approach%2520across%2520a%2520variety%2520of%2520code%2520settings%252C%2520including%250Areal-world%2520Java%2520bugs%2520from%2520the%2520standard%2520Defects4J%2520dataset%2520as%2520well%2520as%2520seven%2520other%250Adatasets%2520which%2520span%2520a%2520diverse%2520set%2520of%2520bug%2520types%2520and%2520languages.%2520Averaged%2520across%250Aall%2520eight%2520datasets%252C%2520BAP%2520improves%2520by%252034.6%2525%2520top-1%2520accuracy%2520compared%2520to%2520the%250Astrongest%2520baseline%2520and%252093.4%2525%2520over%2520zero-shot%2520prompting%2520GPT-4o.%2520BAP%2520is%2520also%250Asignificantly%2520more%2520efficient%2520than%2520prompting%252C%2520outperforming%2520large%2520open-weight%250Amodels%2520at%2520a%2520small%2520fraction%2520of%2520the%2520computational%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Where%27s%20the%20Bug%3F%20Attention%20Probing%20for%20Scalable%20Fault%20Localization&entry.906535625=Adam%20Stein%20and%20Arthur%20Wayne%20and%20Aaditya%20Naik%20and%20Mayur%20Naik%20and%20Eric%20Wong&entry.1292438233=%20%20Ensuring%20code%20correctness%20remains%20a%20challenging%20problem%20even%20as%20large%0Alanguage%20models%20%28LLMs%29%20become%20increasingly%20capable%20at%20code-related%20tasks.%20While%0ALLM-based%20program%20repair%20systems%20can%20propose%20bug%20fixes%20using%20only%20a%20user%27s%20bug%0Areport%2C%20their%20effectiveness%20is%20fundamentally%20limited%20by%20their%20ability%20to%0Aperform%20fault%20localization%20%28FL%29%2C%20a%20challenging%20problem%20for%20both%20humans%20and%0ALLMs.%20Existing%20FL%20approaches%20rely%20on%20executable%20test%20cases%2C%20require%20training%20on%0Acostly%20and%20often%20noisy%20line-level%20annotations%2C%20or%20demand%20resource-intensive%0ALLMs.%20In%20this%20paper%2C%20we%20present%20Bug%20Attention%20Probe%20%28BAP%29%2C%20a%20method%20which%0Alearns%20state-of-the-art%20fault%20localization%20without%20any%20direct%20localization%0Alabels%2C%20outperforming%20traditional%20FL%20baselines%20and%20prompting%20of%20large-scale%0ALLMs.%20We%20evaluate%20our%20approach%20across%20a%20variety%20of%20code%20settings%2C%20including%0Areal-world%20Java%20bugs%20from%20the%20standard%20Defects4J%20dataset%20as%20well%20as%20seven%20other%0Adatasets%20which%20span%20a%20diverse%20set%20of%20bug%20types%20and%20languages.%20Averaged%20across%0Aall%20eight%20datasets%2C%20BAP%20improves%20by%2034.6%25%20top-1%20accuracy%20compared%20to%20the%0Astrongest%20baseline%20and%2093.4%25%20over%20zero-shot%20prompting%20GPT-4o.%20BAP%20is%20also%0Asignificantly%20more%20efficient%20than%20prompting%2C%20outperforming%20large%20open-weight%0Amodels%20at%20a%20small%20fraction%20of%20the%20computational%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13966v1&entry.124074799=Read"},
{"title": "Personalized Instance-based Navigation Toward User-Specific Objects in\n  Realistic Environments", "author": "Luca Barsellotti and Roberto Bigazzi and Marcella Cornia and Lorenzo Baraldi and Rita Cucchiara", "abstract": "  In the last years, the research interest in visual navigation towards objects\nin indoor environments has grown significantly. This growth can be attributed\nto the recent availability of large navigation datasets in photo-realistic\nsimulated environments, like Gibson and Matterport3D. However, the navigation\ntasks supported by these datasets are often restricted to the objects present\nin the environment at acquisition time. Also, they fail to account for the\nrealistic scenario in which the target object is a user-specific instance that\ncan be easily confused with similar objects and may be found in multiple\nlocations within the environment. To address these limitations, we propose a\nnew task denominated Personalized Instance-based Navigation (PIN), in which an\nembodied agent is tasked with locating and reaching a specific personal object\nby distinguishing it among multiple instances of the same category. The task is\naccompanied by PInNED, a dedicated new dataset composed of photo-realistic\nscenes augmented with additional 3D objects. In each episode, the target object\nis presented to the agent using two modalities: a set of visual reference\nimages on a neutral background and manually annotated textual descriptions.\nThrough comprehensive evaluations and analyses, we showcase the challenges of\nthe PIN task as well as the performance and shortcomings of currently available\nmethods designed for object-driven navigation, considering modular and\nend-to-end agents.\n", "link": "http://arxiv.org/abs/2410.18195v2", "date": "2025-02-19", "relevancy": 2.3148, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5906}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5756}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Instance-based%20Navigation%20Toward%20User-Specific%20Objects%20in%0A%20%20Realistic%20Environments&body=Title%3A%20Personalized%20Instance-based%20Navigation%20Toward%20User-Specific%20Objects%20in%0A%20%20Realistic%20Environments%0AAuthor%3A%20Luca%20Barsellotti%20and%20Roberto%20Bigazzi%20and%20Marcella%20Cornia%20and%20Lorenzo%20Baraldi%20and%20Rita%20Cucchiara%0AAbstract%3A%20%20%20In%20the%20last%20years%2C%20the%20research%20interest%20in%20visual%20navigation%20towards%20objects%0Ain%20indoor%20environments%20has%20grown%20significantly.%20This%20growth%20can%20be%20attributed%0Ato%20the%20recent%20availability%20of%20large%20navigation%20datasets%20in%20photo-realistic%0Asimulated%20environments%2C%20like%20Gibson%20and%20Matterport3D.%20However%2C%20the%20navigation%0Atasks%20supported%20by%20these%20datasets%20are%20often%20restricted%20to%20the%20objects%20present%0Ain%20the%20environment%20at%20acquisition%20time.%20Also%2C%20they%20fail%20to%20account%20for%20the%0Arealistic%20scenario%20in%20which%20the%20target%20object%20is%20a%20user-specific%20instance%20that%0Acan%20be%20easily%20confused%20with%20similar%20objects%20and%20may%20be%20found%20in%20multiple%0Alocations%20within%20the%20environment.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Anew%20task%20denominated%20Personalized%20Instance-based%20Navigation%20%28PIN%29%2C%20in%20which%20an%0Aembodied%20agent%20is%20tasked%20with%20locating%20and%20reaching%20a%20specific%20personal%20object%0Aby%20distinguishing%20it%20among%20multiple%20instances%20of%20the%20same%20category.%20The%20task%20is%0Aaccompanied%20by%20PInNED%2C%20a%20dedicated%20new%20dataset%20composed%20of%20photo-realistic%0Ascenes%20augmented%20with%20additional%203D%20objects.%20In%20each%20episode%2C%20the%20target%20object%0Ais%20presented%20to%20the%20agent%20using%20two%20modalities%3A%20a%20set%20of%20visual%20reference%0Aimages%20on%20a%20neutral%20background%20and%20manually%20annotated%20textual%20descriptions.%0AThrough%20comprehensive%20evaluations%20and%20analyses%2C%20we%20showcase%20the%20challenges%20of%0Athe%20PIN%20task%20as%20well%20as%20the%20performance%20and%20shortcomings%20of%20currently%20available%0Amethods%20designed%20for%20object-driven%20navigation%2C%20considering%20modular%20and%0Aend-to-end%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18195v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Instance-based%2520Navigation%2520Toward%2520User-Specific%2520Objects%2520in%250A%2520%2520Realistic%2520Environments%26entry.906535625%3DLuca%2520Barsellotti%2520and%2520Roberto%2520Bigazzi%2520and%2520Marcella%2520Cornia%2520and%2520Lorenzo%2520Baraldi%2520and%2520Rita%2520Cucchiara%26entry.1292438233%3D%2520%2520In%2520the%2520last%2520years%252C%2520the%2520research%2520interest%2520in%2520visual%2520navigation%2520towards%2520objects%250Ain%2520indoor%2520environments%2520has%2520grown%2520significantly.%2520This%2520growth%2520can%2520be%2520attributed%250Ato%2520the%2520recent%2520availability%2520of%2520large%2520navigation%2520datasets%2520in%2520photo-realistic%250Asimulated%2520environments%252C%2520like%2520Gibson%2520and%2520Matterport3D.%2520However%252C%2520the%2520navigation%250Atasks%2520supported%2520by%2520these%2520datasets%2520are%2520often%2520restricted%2520to%2520the%2520objects%2520present%250Ain%2520the%2520environment%2520at%2520acquisition%2520time.%2520Also%252C%2520they%2520fail%2520to%2520account%2520for%2520the%250Arealistic%2520scenario%2520in%2520which%2520the%2520target%2520object%2520is%2520a%2520user-specific%2520instance%2520that%250Acan%2520be%2520easily%2520confused%2520with%2520similar%2520objects%2520and%2520may%2520be%2520found%2520in%2520multiple%250Alocations%2520within%2520the%2520environment.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%250Anew%2520task%2520denominated%2520Personalized%2520Instance-based%2520Navigation%2520%2528PIN%2529%252C%2520in%2520which%2520an%250Aembodied%2520agent%2520is%2520tasked%2520with%2520locating%2520and%2520reaching%2520a%2520specific%2520personal%2520object%250Aby%2520distinguishing%2520it%2520among%2520multiple%2520instances%2520of%2520the%2520same%2520category.%2520The%2520task%2520is%250Aaccompanied%2520by%2520PInNED%252C%2520a%2520dedicated%2520new%2520dataset%2520composed%2520of%2520photo-realistic%250Ascenes%2520augmented%2520with%2520additional%25203D%2520objects.%2520In%2520each%2520episode%252C%2520the%2520target%2520object%250Ais%2520presented%2520to%2520the%2520agent%2520using%2520two%2520modalities%253A%2520a%2520set%2520of%2520visual%2520reference%250Aimages%2520on%2520a%2520neutral%2520background%2520and%2520manually%2520annotated%2520textual%2520descriptions.%250AThrough%2520comprehensive%2520evaluations%2520and%2520analyses%252C%2520we%2520showcase%2520the%2520challenges%2520of%250Athe%2520PIN%2520task%2520as%2520well%2520as%2520the%2520performance%2520and%2520shortcomings%2520of%2520currently%2520available%250Amethods%2520designed%2520for%2520object-driven%2520navigation%252C%2520considering%2520modular%2520and%250Aend-to-end%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18195v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Instance-based%20Navigation%20Toward%20User-Specific%20Objects%20in%0A%20%20Realistic%20Environments&entry.906535625=Luca%20Barsellotti%20and%20Roberto%20Bigazzi%20and%20Marcella%20Cornia%20and%20Lorenzo%20Baraldi%20and%20Rita%20Cucchiara&entry.1292438233=%20%20In%20the%20last%20years%2C%20the%20research%20interest%20in%20visual%20navigation%20towards%20objects%0Ain%20indoor%20environments%20has%20grown%20significantly.%20This%20growth%20can%20be%20attributed%0Ato%20the%20recent%20availability%20of%20large%20navigation%20datasets%20in%20photo-realistic%0Asimulated%20environments%2C%20like%20Gibson%20and%20Matterport3D.%20However%2C%20the%20navigation%0Atasks%20supported%20by%20these%20datasets%20are%20often%20restricted%20to%20the%20objects%20present%0Ain%20the%20environment%20at%20acquisition%20time.%20Also%2C%20they%20fail%20to%20account%20for%20the%0Arealistic%20scenario%20in%20which%20the%20target%20object%20is%20a%20user-specific%20instance%20that%0Acan%20be%20easily%20confused%20with%20similar%20objects%20and%20may%20be%20found%20in%20multiple%0Alocations%20within%20the%20environment.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Anew%20task%20denominated%20Personalized%20Instance-based%20Navigation%20%28PIN%29%2C%20in%20which%20an%0Aembodied%20agent%20is%20tasked%20with%20locating%20and%20reaching%20a%20specific%20personal%20object%0Aby%20distinguishing%20it%20among%20multiple%20instances%20of%20the%20same%20category.%20The%20task%20is%0Aaccompanied%20by%20PInNED%2C%20a%20dedicated%20new%20dataset%20composed%20of%20photo-realistic%0Ascenes%20augmented%20with%20additional%203D%20objects.%20In%20each%20episode%2C%20the%20target%20object%0Ais%20presented%20to%20the%20agent%20using%20two%20modalities%3A%20a%20set%20of%20visual%20reference%0Aimages%20on%20a%20neutral%20background%20and%20manually%20annotated%20textual%20descriptions.%0AThrough%20comprehensive%20evaluations%20and%20analyses%2C%20we%20showcase%20the%20challenges%20of%0Athe%20PIN%20task%20as%20well%20as%20the%20performance%20and%20shortcomings%20of%20currently%20available%0Amethods%20designed%20for%20object-driven%20navigation%2C%20considering%20modular%20and%0Aend-to-end%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18195v2&entry.124074799=Read"},
{"title": "Benchmarking of Different YOLO Models for CAPTCHAs Detection and\n  Classification", "author": "Miko\u0142aj Wysocki and Henryk Gierszal and Piotr Tyczka and Sophia Karagiorgou and George Pantelis", "abstract": "  This paper provides an analysis and comparison of the YOLOv5, YOLOv8 and\nYOLOv10 models for webpage CAPTCHAs detection using the datasets collected from\nthe web and darknet as well as synthetized data of webpages. The study examines\nthe nano (n), small (s), and medium (m) variants of YOLO architectures and use\nmetrics such as Precision, Recall, F1 score, mAP@50 and inference speed to\ndetermine the real-life utility. Additionally, the possibility of tuning the\ntrained model to detect new CAPTCHA patterns efficiently was examined as it is\na crucial part of real-life applications. The image slicing method was proposed\nas a way to improve the metrics of detection on oversized input images which\ncan be a common scenario in webpages analysis. Models in version nano achieved\nthe best results in terms of speed, while more complexed architectures scored\nbetter in terms of other metrics.\n", "link": "http://arxiv.org/abs/2502.13740v1", "date": "2025-02-19", "relevancy": 2.2977, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4615}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4615}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20of%20Different%20YOLO%20Models%20for%20CAPTCHAs%20Detection%20and%0A%20%20Classification&body=Title%3A%20Benchmarking%20of%20Different%20YOLO%20Models%20for%20CAPTCHAs%20Detection%20and%0A%20%20Classification%0AAuthor%3A%20Miko%C5%82aj%20Wysocki%20and%20Henryk%20Gierszal%20and%20Piotr%20Tyczka%20and%20Sophia%20Karagiorgou%20and%20George%20Pantelis%0AAbstract%3A%20%20%20This%20paper%20provides%20an%20analysis%20and%20comparison%20of%20the%20YOLOv5%2C%20YOLOv8%20and%0AYOLOv10%20models%20for%20webpage%20CAPTCHAs%20detection%20using%20the%20datasets%20collected%20from%0Athe%20web%20and%20darknet%20as%20well%20as%20synthetized%20data%20of%20webpages.%20The%20study%20examines%0Athe%20nano%20%28n%29%2C%20small%20%28s%29%2C%20and%20medium%20%28m%29%20variants%20of%20YOLO%20architectures%20and%20use%0Ametrics%20such%20as%20Precision%2C%20Recall%2C%20F1%20score%2C%20mAP%4050%20and%20inference%20speed%20to%0Adetermine%20the%20real-life%20utility.%20Additionally%2C%20the%20possibility%20of%20tuning%20the%0Atrained%20model%20to%20detect%20new%20CAPTCHA%20patterns%20efficiently%20was%20examined%20as%20it%20is%0Aa%20crucial%20part%20of%20real-life%20applications.%20The%20image%20slicing%20method%20was%20proposed%0Aas%20a%20way%20to%20improve%20the%20metrics%20of%20detection%20on%20oversized%20input%20images%20which%0Acan%20be%20a%20common%20scenario%20in%20webpages%20analysis.%20Models%20in%20version%20nano%20achieved%0Athe%20best%20results%20in%20terms%20of%20speed%2C%20while%20more%20complexed%20architectures%20scored%0Abetter%20in%20terms%20of%20other%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520of%2520Different%2520YOLO%2520Models%2520for%2520CAPTCHAs%2520Detection%2520and%250A%2520%2520Classification%26entry.906535625%3DMiko%25C5%2582aj%2520Wysocki%2520and%2520Henryk%2520Gierszal%2520and%2520Piotr%2520Tyczka%2520and%2520Sophia%2520Karagiorgou%2520and%2520George%2520Pantelis%26entry.1292438233%3D%2520%2520This%2520paper%2520provides%2520an%2520analysis%2520and%2520comparison%2520of%2520the%2520YOLOv5%252C%2520YOLOv8%2520and%250AYOLOv10%2520models%2520for%2520webpage%2520CAPTCHAs%2520detection%2520using%2520the%2520datasets%2520collected%2520from%250Athe%2520web%2520and%2520darknet%2520as%2520well%2520as%2520synthetized%2520data%2520of%2520webpages.%2520The%2520study%2520examines%250Athe%2520nano%2520%2528n%2529%252C%2520small%2520%2528s%2529%252C%2520and%2520medium%2520%2528m%2529%2520variants%2520of%2520YOLO%2520architectures%2520and%2520use%250Ametrics%2520such%2520as%2520Precision%252C%2520Recall%252C%2520F1%2520score%252C%2520mAP%254050%2520and%2520inference%2520speed%2520to%250Adetermine%2520the%2520real-life%2520utility.%2520Additionally%252C%2520the%2520possibility%2520of%2520tuning%2520the%250Atrained%2520model%2520to%2520detect%2520new%2520CAPTCHA%2520patterns%2520efficiently%2520was%2520examined%2520as%2520it%2520is%250Aa%2520crucial%2520part%2520of%2520real-life%2520applications.%2520The%2520image%2520slicing%2520method%2520was%2520proposed%250Aas%2520a%2520way%2520to%2520improve%2520the%2520metrics%2520of%2520detection%2520on%2520oversized%2520input%2520images%2520which%250Acan%2520be%2520a%2520common%2520scenario%2520in%2520webpages%2520analysis.%2520Models%2520in%2520version%2520nano%2520achieved%250Athe%2520best%2520results%2520in%2520terms%2520of%2520speed%252C%2520while%2520more%2520complexed%2520architectures%2520scored%250Abetter%2520in%2520terms%2520of%2520other%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20of%20Different%20YOLO%20Models%20for%20CAPTCHAs%20Detection%20and%0A%20%20Classification&entry.906535625=Miko%C5%82aj%20Wysocki%20and%20Henryk%20Gierszal%20and%20Piotr%20Tyczka%20and%20Sophia%20Karagiorgou%20and%20George%20Pantelis&entry.1292438233=%20%20This%20paper%20provides%20an%20analysis%20and%20comparison%20of%20the%20YOLOv5%2C%20YOLOv8%20and%0AYOLOv10%20models%20for%20webpage%20CAPTCHAs%20detection%20using%20the%20datasets%20collected%20from%0Athe%20web%20and%20darknet%20as%20well%20as%20synthetized%20data%20of%20webpages.%20The%20study%20examines%0Athe%20nano%20%28n%29%2C%20small%20%28s%29%2C%20and%20medium%20%28m%29%20variants%20of%20YOLO%20architectures%20and%20use%0Ametrics%20such%20as%20Precision%2C%20Recall%2C%20F1%20score%2C%20mAP%4050%20and%20inference%20speed%20to%0Adetermine%20the%20real-life%20utility.%20Additionally%2C%20the%20possibility%20of%20tuning%20the%0Atrained%20model%20to%20detect%20new%20CAPTCHA%20patterns%20efficiently%20was%20examined%20as%20it%20is%0Aa%20crucial%20part%20of%20real-life%20applications.%20The%20image%20slicing%20method%20was%20proposed%0Aas%20a%20way%20to%20improve%20the%20metrics%20of%20detection%20on%20oversized%20input%20images%20which%0Acan%20be%20a%20common%20scenario%20in%20webpages%20analysis.%20Models%20in%20version%20nano%20achieved%0Athe%20best%20results%20in%20terms%20of%20speed%2C%20while%20more%20complexed%20architectures%20scored%0Abetter%20in%20terms%20of%20other%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13740v1&entry.124074799=Read"},
{"title": "MetaSSC: Enhancing 3D Semantic Scene Completion for Autonomous Driving\n  through Meta-Learning and Long-sequence Modeling", "author": "Yansong Qu and Zixuan Xu and Zilin Huang and Zihao Sheng and Tiantian Chen and Sikai Chen", "abstract": "  Semantic scene completion (SSC) is essential for achieving comprehensive\nperception in autonomous driving systems. However, existing SSC methods often\noverlook the high deployment costs in real-world applications. Traditional\narchitectures, such as 3D Convolutional Neural Networks (3D CNNs) and\nself-attention mechanisms, face challenges in efficiently capturing long-range\ndependencies within 3D voxel grids, limiting their effectiveness. To address\nthese issues, we introduce MetaSSC, a novel meta-learning-based framework for\nSSC that leverages deformable convolution, large-kernel attention, and the\nMamba (D-LKA-M) model. Our approach begins with a voxel-based semantic\nsegmentation (SS) pretraining task, aimed at exploring the semantics and\ngeometry of incomplete regions while acquiring transferable meta-knowledge.\nUsing simulated cooperative perception datasets, we supervise the perception\ntraining of a single vehicle using aggregated sensor data from multiple nearby\nconnected autonomous vehicles (CAVs), generating richer and more comprehensive\nlabels. This meta-knowledge is then adapted to the target domain through a\ndual-phase training strategy that does not add extra model parameters, enabling\nefficient deployment. To further enhance the model's capability in capturing\nlong-sequence relationships within 3D voxel grids, we integrate Mamba blocks\nwith deformable convolution and large-kernel attention into the backbone\nnetwork. Extensive experiments demonstrate that MetaSSC achieves\nstate-of-the-art performance, significantly outperforming competing models\nwhile also reducing deployment costs.\n", "link": "http://arxiv.org/abs/2411.03672v2", "date": "2025-02-19", "relevancy": 2.2955, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5749}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5749}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaSSC%3A%20Enhancing%203D%20Semantic%20Scene%20Completion%20for%20Autonomous%20Driving%0A%20%20through%20Meta-Learning%20and%20Long-sequence%20Modeling&body=Title%3A%20MetaSSC%3A%20Enhancing%203D%20Semantic%20Scene%20Completion%20for%20Autonomous%20Driving%0A%20%20through%20Meta-Learning%20and%20Long-sequence%20Modeling%0AAuthor%3A%20Yansong%20Qu%20and%20Zixuan%20Xu%20and%20Zilin%20Huang%20and%20Zihao%20Sheng%20and%20Tiantian%20Chen%20and%20Sikai%20Chen%0AAbstract%3A%20%20%20Semantic%20scene%20completion%20%28SSC%29%20is%20essential%20for%20achieving%20comprehensive%0Aperception%20in%20autonomous%20driving%20systems.%20However%2C%20existing%20SSC%20methods%20often%0Aoverlook%20the%20high%20deployment%20costs%20in%20real-world%20applications.%20Traditional%0Aarchitectures%2C%20such%20as%203D%20Convolutional%20Neural%20Networks%20%283D%20CNNs%29%20and%0Aself-attention%20mechanisms%2C%20face%20challenges%20in%20efficiently%20capturing%20long-range%0Adependencies%20within%203D%20voxel%20grids%2C%20limiting%20their%20effectiveness.%20To%20address%0Athese%20issues%2C%20we%20introduce%20MetaSSC%2C%20a%20novel%20meta-learning-based%20framework%20for%0ASSC%20that%20leverages%20deformable%20convolution%2C%20large-kernel%20attention%2C%20and%20the%0AMamba%20%28D-LKA-M%29%20model.%20Our%20approach%20begins%20with%20a%20voxel-based%20semantic%0Asegmentation%20%28SS%29%20pretraining%20task%2C%20aimed%20at%20exploring%20the%20semantics%20and%0Ageometry%20of%20incomplete%20regions%20while%20acquiring%20transferable%20meta-knowledge.%0AUsing%20simulated%20cooperative%20perception%20datasets%2C%20we%20supervise%20the%20perception%0Atraining%20of%20a%20single%20vehicle%20using%20aggregated%20sensor%20data%20from%20multiple%20nearby%0Aconnected%20autonomous%20vehicles%20%28CAVs%29%2C%20generating%20richer%20and%20more%20comprehensive%0Alabels.%20This%20meta-knowledge%20is%20then%20adapted%20to%20the%20target%20domain%20through%20a%0Adual-phase%20training%20strategy%20that%20does%20not%20add%20extra%20model%20parameters%2C%20enabling%0Aefficient%20deployment.%20To%20further%20enhance%20the%20model%27s%20capability%20in%20capturing%0Along-sequence%20relationships%20within%203D%20voxel%20grids%2C%20we%20integrate%20Mamba%20blocks%0Awith%20deformable%20convolution%20and%20large-kernel%20attention%20into%20the%20backbone%0Anetwork.%20Extensive%20experiments%20demonstrate%20that%20MetaSSC%20achieves%0Astate-of-the-art%20performance%2C%20significantly%20outperforming%20competing%20models%0Awhile%20also%20reducing%20deployment%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03672v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaSSC%253A%2520Enhancing%25203D%2520Semantic%2520Scene%2520Completion%2520for%2520Autonomous%2520Driving%250A%2520%2520through%2520Meta-Learning%2520and%2520Long-sequence%2520Modeling%26entry.906535625%3DYansong%2520Qu%2520and%2520Zixuan%2520Xu%2520and%2520Zilin%2520Huang%2520and%2520Zihao%2520Sheng%2520and%2520Tiantian%2520Chen%2520and%2520Sikai%2520Chen%26entry.1292438233%3D%2520%2520Semantic%2520scene%2520completion%2520%2528SSC%2529%2520is%2520essential%2520for%2520achieving%2520comprehensive%250Aperception%2520in%2520autonomous%2520driving%2520systems.%2520However%252C%2520existing%2520SSC%2520methods%2520often%250Aoverlook%2520the%2520high%2520deployment%2520costs%2520in%2520real-world%2520applications.%2520Traditional%250Aarchitectures%252C%2520such%2520as%25203D%2520Convolutional%2520Neural%2520Networks%2520%25283D%2520CNNs%2529%2520and%250Aself-attention%2520mechanisms%252C%2520face%2520challenges%2520in%2520efficiently%2520capturing%2520long-range%250Adependencies%2520within%25203D%2520voxel%2520grids%252C%2520limiting%2520their%2520effectiveness.%2520To%2520address%250Athese%2520issues%252C%2520we%2520introduce%2520MetaSSC%252C%2520a%2520novel%2520meta-learning-based%2520framework%2520for%250ASSC%2520that%2520leverages%2520deformable%2520convolution%252C%2520large-kernel%2520attention%252C%2520and%2520the%250AMamba%2520%2528D-LKA-M%2529%2520model.%2520Our%2520approach%2520begins%2520with%2520a%2520voxel-based%2520semantic%250Asegmentation%2520%2528SS%2529%2520pretraining%2520task%252C%2520aimed%2520at%2520exploring%2520the%2520semantics%2520and%250Ageometry%2520of%2520incomplete%2520regions%2520while%2520acquiring%2520transferable%2520meta-knowledge.%250AUsing%2520simulated%2520cooperative%2520perception%2520datasets%252C%2520we%2520supervise%2520the%2520perception%250Atraining%2520of%2520a%2520single%2520vehicle%2520using%2520aggregated%2520sensor%2520data%2520from%2520multiple%2520nearby%250Aconnected%2520autonomous%2520vehicles%2520%2528CAVs%2529%252C%2520generating%2520richer%2520and%2520more%2520comprehensive%250Alabels.%2520This%2520meta-knowledge%2520is%2520then%2520adapted%2520to%2520the%2520target%2520domain%2520through%2520a%250Adual-phase%2520training%2520strategy%2520that%2520does%2520not%2520add%2520extra%2520model%2520parameters%252C%2520enabling%250Aefficient%2520deployment.%2520To%2520further%2520enhance%2520the%2520model%2527s%2520capability%2520in%2520capturing%250Along-sequence%2520relationships%2520within%25203D%2520voxel%2520grids%252C%2520we%2520integrate%2520Mamba%2520blocks%250Awith%2520deformable%2520convolution%2520and%2520large-kernel%2520attention%2520into%2520the%2520backbone%250Anetwork.%2520Extensive%2520experiments%2520demonstrate%2520that%2520MetaSSC%2520achieves%250Astate-of-the-art%2520performance%252C%2520significantly%2520outperforming%2520competing%2520models%250Awhile%2520also%2520reducing%2520deployment%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03672v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaSSC%3A%20Enhancing%203D%20Semantic%20Scene%20Completion%20for%20Autonomous%20Driving%0A%20%20through%20Meta-Learning%20and%20Long-sequence%20Modeling&entry.906535625=Yansong%20Qu%20and%20Zixuan%20Xu%20and%20Zilin%20Huang%20and%20Zihao%20Sheng%20and%20Tiantian%20Chen%20and%20Sikai%20Chen&entry.1292438233=%20%20Semantic%20scene%20completion%20%28SSC%29%20is%20essential%20for%20achieving%20comprehensive%0Aperception%20in%20autonomous%20driving%20systems.%20However%2C%20existing%20SSC%20methods%20often%0Aoverlook%20the%20high%20deployment%20costs%20in%20real-world%20applications.%20Traditional%0Aarchitectures%2C%20such%20as%203D%20Convolutional%20Neural%20Networks%20%283D%20CNNs%29%20and%0Aself-attention%20mechanisms%2C%20face%20challenges%20in%20efficiently%20capturing%20long-range%0Adependencies%20within%203D%20voxel%20grids%2C%20limiting%20their%20effectiveness.%20To%20address%0Athese%20issues%2C%20we%20introduce%20MetaSSC%2C%20a%20novel%20meta-learning-based%20framework%20for%0ASSC%20that%20leverages%20deformable%20convolution%2C%20large-kernel%20attention%2C%20and%20the%0AMamba%20%28D-LKA-M%29%20model.%20Our%20approach%20begins%20with%20a%20voxel-based%20semantic%0Asegmentation%20%28SS%29%20pretraining%20task%2C%20aimed%20at%20exploring%20the%20semantics%20and%0Ageometry%20of%20incomplete%20regions%20while%20acquiring%20transferable%20meta-knowledge.%0AUsing%20simulated%20cooperative%20perception%20datasets%2C%20we%20supervise%20the%20perception%0Atraining%20of%20a%20single%20vehicle%20using%20aggregated%20sensor%20data%20from%20multiple%20nearby%0Aconnected%20autonomous%20vehicles%20%28CAVs%29%2C%20generating%20richer%20and%20more%20comprehensive%0Alabels.%20This%20meta-knowledge%20is%20then%20adapted%20to%20the%20target%20domain%20through%20a%0Adual-phase%20training%20strategy%20that%20does%20not%20add%20extra%20model%20parameters%2C%20enabling%0Aefficient%20deployment.%20To%20further%20enhance%20the%20model%27s%20capability%20in%20capturing%0Along-sequence%20relationships%20within%203D%20voxel%20grids%2C%20we%20integrate%20Mamba%20blocks%0Awith%20deformable%20convolution%20and%20large-kernel%20attention%20into%20the%20backbone%0Anetwork.%20Extensive%20experiments%20demonstrate%20that%20MetaSSC%20achieves%0Astate-of-the-art%20performance%2C%20significantly%20outperforming%20competing%20models%0Awhile%20also%20reducing%20deployment%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03672v2&entry.124074799=Read"},
{"title": "Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning", "author": "Zenan Li and Zhaoyu Li and Wen Tang and Xian Zhang and Yuan Yao and Xujie Si and Fan Yang and Kaiyu Yang and Xiaoxing Ma", "abstract": "  Large language models (LLMs) can prove mathematical theorems formally by\ngenerating proof steps (\\textit{a.k.a.} tactics) within a proof system.\nHowever, the space of possible tactics is vast and complex, while the available\ntraining data for formal proofs is limited, posing a significant challenge to\nLLM-based tactic generation. To address this, we introduce a neuro-symbolic\ntactic generator that synergizes the mathematical intuition learned by LLMs\nwith domain-specific insights encoded by symbolic methods. The key aspect of\nthis integration is identifying which parts of mathematical reasoning are best\nsuited to LLMs and which to symbolic methods. While the high-level idea of\nneuro-symbolic integration is broadly applicable to various mathematical\nproblems, in this paper, we focus specifically on Olympiad inequalities\n(Figure~1). We analyze how humans solve these problems and distill the\ntechniques into two types of tactics: (1) scaling, handled by symbolic methods,\nand (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with\nLLMs to prune and rank the proof goals for efficient proof search. We evaluate\nour framework on 161 challenging inequalities from multiple mathematics\ncompetitions, achieving state-of-the-art performance and significantly\noutperforming existing LLM and symbolic approaches without requiring additional\ntraining data.\n", "link": "http://arxiv.org/abs/2502.13834v1", "date": "2025-02-19", "relevancy": 2.2839, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4649}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4649}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proving%20Olympiad%20Inequalities%20by%20Synergizing%20LLMs%20and%20Symbolic%20Reasoning&body=Title%3A%20Proving%20Olympiad%20Inequalities%20by%20Synergizing%20LLMs%20and%20Symbolic%20Reasoning%0AAuthor%3A%20Zenan%20Li%20and%20Zhaoyu%20Li%20and%20Wen%20Tang%20and%20Xian%20Zhang%20and%20Yuan%20Yao%20and%20Xujie%20Si%20and%20Fan%20Yang%20and%20Kaiyu%20Yang%20and%20Xiaoxing%20Ma%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20can%20prove%20mathematical%20theorems%20formally%20by%0Agenerating%20proof%20steps%20%28%5Ctextit%7Ba.k.a.%7D%20tactics%29%20within%20a%20proof%20system.%0AHowever%2C%20the%20space%20of%20possible%20tactics%20is%20vast%20and%20complex%2C%20while%20the%20available%0Atraining%20data%20for%20formal%20proofs%20is%20limited%2C%20posing%20a%20significant%20challenge%20to%0ALLM-based%20tactic%20generation.%20To%20address%20this%2C%20we%20introduce%20a%20neuro-symbolic%0Atactic%20generator%20that%20synergizes%20the%20mathematical%20intuition%20learned%20by%20LLMs%0Awith%20domain-specific%20insights%20encoded%20by%20symbolic%20methods.%20The%20key%20aspect%20of%0Athis%20integration%20is%20identifying%20which%20parts%20of%20mathematical%20reasoning%20are%20best%0Asuited%20to%20LLMs%20and%20which%20to%20symbolic%20methods.%20While%20the%20high-level%20idea%20of%0Aneuro-symbolic%20integration%20is%20broadly%20applicable%20to%20various%20mathematical%0Aproblems%2C%20in%20this%20paper%2C%20we%20focus%20specifically%20on%20Olympiad%20inequalities%0A%28Figure~1%29.%20We%20analyze%20how%20humans%20solve%20these%20problems%20and%20distill%20the%0Atechniques%20into%20two%20types%20of%20tactics%3A%20%281%29%20scaling%2C%20handled%20by%20symbolic%20methods%2C%0Aand%20%282%29%20rewriting%2C%20handled%20by%20LLMs.%20In%20addition%2C%20we%20combine%20symbolic%20tools%20with%0ALLMs%20to%20prune%20and%20rank%20the%20proof%20goals%20for%20efficient%20proof%20search.%20We%20evaluate%0Aour%20framework%20on%20161%20challenging%20inequalities%20from%20multiple%20mathematics%0Acompetitions%2C%20achieving%20state-of-the-art%20performance%20and%20significantly%0Aoutperforming%20existing%20LLM%20and%20symbolic%20approaches%20without%20requiring%20additional%0Atraining%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProving%2520Olympiad%2520Inequalities%2520by%2520Synergizing%2520LLMs%2520and%2520Symbolic%2520Reasoning%26entry.906535625%3DZenan%2520Li%2520and%2520Zhaoyu%2520Li%2520and%2520Wen%2520Tang%2520and%2520Xian%2520Zhang%2520and%2520Yuan%2520Yao%2520and%2520Xujie%2520Si%2520and%2520Fan%2520Yang%2520and%2520Kaiyu%2520Yang%2520and%2520Xiaoxing%2520Ma%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520can%2520prove%2520mathematical%2520theorems%2520formally%2520by%250Agenerating%2520proof%2520steps%2520%2528%255Ctextit%257Ba.k.a.%257D%2520tactics%2529%2520within%2520a%2520proof%2520system.%250AHowever%252C%2520the%2520space%2520of%2520possible%2520tactics%2520is%2520vast%2520and%2520complex%252C%2520while%2520the%2520available%250Atraining%2520data%2520for%2520formal%2520proofs%2520is%2520limited%252C%2520posing%2520a%2520significant%2520challenge%2520to%250ALLM-based%2520tactic%2520generation.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520neuro-symbolic%250Atactic%2520generator%2520that%2520synergizes%2520the%2520mathematical%2520intuition%2520learned%2520by%2520LLMs%250Awith%2520domain-specific%2520insights%2520encoded%2520by%2520symbolic%2520methods.%2520The%2520key%2520aspect%2520of%250Athis%2520integration%2520is%2520identifying%2520which%2520parts%2520of%2520mathematical%2520reasoning%2520are%2520best%250Asuited%2520to%2520LLMs%2520and%2520which%2520to%2520symbolic%2520methods.%2520While%2520the%2520high-level%2520idea%2520of%250Aneuro-symbolic%2520integration%2520is%2520broadly%2520applicable%2520to%2520various%2520mathematical%250Aproblems%252C%2520in%2520this%2520paper%252C%2520we%2520focus%2520specifically%2520on%2520Olympiad%2520inequalities%250A%2528Figure~1%2529.%2520We%2520analyze%2520how%2520humans%2520solve%2520these%2520problems%2520and%2520distill%2520the%250Atechniques%2520into%2520two%2520types%2520of%2520tactics%253A%2520%25281%2529%2520scaling%252C%2520handled%2520by%2520symbolic%2520methods%252C%250Aand%2520%25282%2529%2520rewriting%252C%2520handled%2520by%2520LLMs.%2520In%2520addition%252C%2520we%2520combine%2520symbolic%2520tools%2520with%250ALLMs%2520to%2520prune%2520and%2520rank%2520the%2520proof%2520goals%2520for%2520efficient%2520proof%2520search.%2520We%2520evaluate%250Aour%2520framework%2520on%2520161%2520challenging%2520inequalities%2520from%2520multiple%2520mathematics%250Acompetitions%252C%2520achieving%2520state-of-the-art%2520performance%2520and%2520significantly%250Aoutperforming%2520existing%2520LLM%2520and%2520symbolic%2520approaches%2520without%2520requiring%2520additional%250Atraining%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proving%20Olympiad%20Inequalities%20by%20Synergizing%20LLMs%20and%20Symbolic%20Reasoning&entry.906535625=Zenan%20Li%20and%20Zhaoyu%20Li%20and%20Wen%20Tang%20and%20Xian%20Zhang%20and%20Yuan%20Yao%20and%20Xujie%20Si%20and%20Fan%20Yang%20and%20Kaiyu%20Yang%20and%20Xiaoxing%20Ma&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20can%20prove%20mathematical%20theorems%20formally%20by%0Agenerating%20proof%20steps%20%28%5Ctextit%7Ba.k.a.%7D%20tactics%29%20within%20a%20proof%20system.%0AHowever%2C%20the%20space%20of%20possible%20tactics%20is%20vast%20and%20complex%2C%20while%20the%20available%0Atraining%20data%20for%20formal%20proofs%20is%20limited%2C%20posing%20a%20significant%20challenge%20to%0ALLM-based%20tactic%20generation.%20To%20address%20this%2C%20we%20introduce%20a%20neuro-symbolic%0Atactic%20generator%20that%20synergizes%20the%20mathematical%20intuition%20learned%20by%20LLMs%0Awith%20domain-specific%20insights%20encoded%20by%20symbolic%20methods.%20The%20key%20aspect%20of%0Athis%20integration%20is%20identifying%20which%20parts%20of%20mathematical%20reasoning%20are%20best%0Asuited%20to%20LLMs%20and%20which%20to%20symbolic%20methods.%20While%20the%20high-level%20idea%20of%0Aneuro-symbolic%20integration%20is%20broadly%20applicable%20to%20various%20mathematical%0Aproblems%2C%20in%20this%20paper%2C%20we%20focus%20specifically%20on%20Olympiad%20inequalities%0A%28Figure~1%29.%20We%20analyze%20how%20humans%20solve%20these%20problems%20and%20distill%20the%0Atechniques%20into%20two%20types%20of%20tactics%3A%20%281%29%20scaling%2C%20handled%20by%20symbolic%20methods%2C%0Aand%20%282%29%20rewriting%2C%20handled%20by%20LLMs.%20In%20addition%2C%20we%20combine%20symbolic%20tools%20with%0ALLMs%20to%20prune%20and%20rank%20the%20proof%20goals%20for%20efficient%20proof%20search.%20We%20evaluate%0Aour%20framework%20on%20161%20challenging%20inequalities%20from%20multiple%20mathematics%0Acompetitions%2C%20achieving%20state-of-the-art%20performance%20and%20significantly%0Aoutperforming%20existing%20LLM%20and%20symbolic%20approaches%20without%20requiring%20additional%0Atraining%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13834v1&entry.124074799=Read"},
{"title": "Event-Based Video Frame Interpolation With Cross-Modal Asymmetric\n  Bidirectional Motion Fields", "author": "Taewoo Kim and Yujeong Chae and Hyun-Kurl Jang and Kuk-Jin Yoon", "abstract": "  Video Frame Interpolation (VFI) aims to generate intermediate video frames\nbetween consecutive input frames. Since the event cameras are bio-inspired\nsensors that only encode brightness changes with a micro-second temporal\nresolution, several works utilized the event camera to enhance the performance\nof VFI. However, existing methods estimate bidirectional inter-frame motion\nfields with only events or approximations, which can not consider the complex\nmotion in real-world scenarios. In this paper, we propose a novel event-based\nVFI framework with cross-modal asymmetric bidirectional motion field\nestimation. In detail, our EIF-BiOFNet utilizes each valuable characteristic of\nthe events and images for direct estimation of inter-frame motion fields\nwithout any approximation methods. Moreover, we develop an interactive\nattention-based frame synthesis network to efficiently leverage the\ncomplementary warping-based and synthesis-based features. Finally, we build a\nlarge-scale event-based VFI dataset, ERF-X170FPS, with a high frame rate,\nextreme motion, and dynamic textures to overcome the limitations of previous\nevent-based VFI datasets. Extensive experimental results validate that our\nmethod shows significant performance improvement over the state-of-the-art VFI\nmethods on various datasets. Our project pages are available at:\nhttps://github.com/intelpro/CBMNet\n", "link": "http://arxiv.org/abs/2502.13716v1", "date": "2025-02-19", "relevancy": 2.2818, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5885}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5578}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event-Based%20Video%20Frame%20Interpolation%20With%20Cross-Modal%20Asymmetric%0A%20%20Bidirectional%20Motion%20Fields&body=Title%3A%20Event-Based%20Video%20Frame%20Interpolation%20With%20Cross-Modal%20Asymmetric%0A%20%20Bidirectional%20Motion%20Fields%0AAuthor%3A%20Taewoo%20Kim%20and%20Yujeong%20Chae%20and%20Hyun-Kurl%20Jang%20and%20Kuk-Jin%20Yoon%0AAbstract%3A%20%20%20Video%20Frame%20Interpolation%20%28VFI%29%20aims%20to%20generate%20intermediate%20video%20frames%0Abetween%20consecutive%20input%20frames.%20Since%20the%20event%20cameras%20are%20bio-inspired%0Asensors%20that%20only%20encode%20brightness%20changes%20with%20a%20micro-second%20temporal%0Aresolution%2C%20several%20works%20utilized%20the%20event%20camera%20to%20enhance%20the%20performance%0Aof%20VFI.%20However%2C%20existing%20methods%20estimate%20bidirectional%20inter-frame%20motion%0Afields%20with%20only%20events%20or%20approximations%2C%20which%20can%20not%20consider%20the%20complex%0Amotion%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20event-based%0AVFI%20framework%20with%20cross-modal%20asymmetric%20bidirectional%20motion%20field%0Aestimation.%20In%20detail%2C%20our%20EIF-BiOFNet%20utilizes%20each%20valuable%20characteristic%20of%0Athe%20events%20and%20images%20for%20direct%20estimation%20of%20inter-frame%20motion%20fields%0Awithout%20any%20approximation%20methods.%20Moreover%2C%20we%20develop%20an%20interactive%0Aattention-based%20frame%20synthesis%20network%20to%20efficiently%20leverage%20the%0Acomplementary%20warping-based%20and%20synthesis-based%20features.%20Finally%2C%20we%20build%20a%0Alarge-scale%20event-based%20VFI%20dataset%2C%20ERF-X170FPS%2C%20with%20a%20high%20frame%20rate%2C%0Aextreme%20motion%2C%20and%20dynamic%20textures%20to%20overcome%20the%20limitations%20of%20previous%0Aevent-based%20VFI%20datasets.%20Extensive%20experimental%20results%20validate%20that%20our%0Amethod%20shows%20significant%20performance%20improvement%20over%20the%20state-of-the-art%20VFI%0Amethods%20on%20various%20datasets.%20Our%20project%20pages%20are%20available%20at%3A%0Ahttps%3A//github.com/intelpro/CBMNet%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13716v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent-Based%2520Video%2520Frame%2520Interpolation%2520With%2520Cross-Modal%2520Asymmetric%250A%2520%2520Bidirectional%2520Motion%2520Fields%26entry.906535625%3DTaewoo%2520Kim%2520and%2520Yujeong%2520Chae%2520and%2520Hyun-Kurl%2520Jang%2520and%2520Kuk-Jin%2520Yoon%26entry.1292438233%3D%2520%2520Video%2520Frame%2520Interpolation%2520%2528VFI%2529%2520aims%2520to%2520generate%2520intermediate%2520video%2520frames%250Abetween%2520consecutive%2520input%2520frames.%2520Since%2520the%2520event%2520cameras%2520are%2520bio-inspired%250Asensors%2520that%2520only%2520encode%2520brightness%2520changes%2520with%2520a%2520micro-second%2520temporal%250Aresolution%252C%2520several%2520works%2520utilized%2520the%2520event%2520camera%2520to%2520enhance%2520the%2520performance%250Aof%2520VFI.%2520However%252C%2520existing%2520methods%2520estimate%2520bidirectional%2520inter-frame%2520motion%250Afields%2520with%2520only%2520events%2520or%2520approximations%252C%2520which%2520can%2520not%2520consider%2520the%2520complex%250Amotion%2520in%2520real-world%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520event-based%250AVFI%2520framework%2520with%2520cross-modal%2520asymmetric%2520bidirectional%2520motion%2520field%250Aestimation.%2520In%2520detail%252C%2520our%2520EIF-BiOFNet%2520utilizes%2520each%2520valuable%2520characteristic%2520of%250Athe%2520events%2520and%2520images%2520for%2520direct%2520estimation%2520of%2520inter-frame%2520motion%2520fields%250Awithout%2520any%2520approximation%2520methods.%2520Moreover%252C%2520we%2520develop%2520an%2520interactive%250Aattention-based%2520frame%2520synthesis%2520network%2520to%2520efficiently%2520leverage%2520the%250Acomplementary%2520warping-based%2520and%2520synthesis-based%2520features.%2520Finally%252C%2520we%2520build%2520a%250Alarge-scale%2520event-based%2520VFI%2520dataset%252C%2520ERF-X170FPS%252C%2520with%2520a%2520high%2520frame%2520rate%252C%250Aextreme%2520motion%252C%2520and%2520dynamic%2520textures%2520to%2520overcome%2520the%2520limitations%2520of%2520previous%250Aevent-based%2520VFI%2520datasets.%2520Extensive%2520experimental%2520results%2520validate%2520that%2520our%250Amethod%2520shows%2520significant%2520performance%2520improvement%2520over%2520the%2520state-of-the-art%2520VFI%250Amethods%2520on%2520various%2520datasets.%2520Our%2520project%2520pages%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/intelpro/CBMNet%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13716v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event-Based%20Video%20Frame%20Interpolation%20With%20Cross-Modal%20Asymmetric%0A%20%20Bidirectional%20Motion%20Fields&entry.906535625=Taewoo%20Kim%20and%20Yujeong%20Chae%20and%20Hyun-Kurl%20Jang%20and%20Kuk-Jin%20Yoon&entry.1292438233=%20%20Video%20Frame%20Interpolation%20%28VFI%29%20aims%20to%20generate%20intermediate%20video%20frames%0Abetween%20consecutive%20input%20frames.%20Since%20the%20event%20cameras%20are%20bio-inspired%0Asensors%20that%20only%20encode%20brightness%20changes%20with%20a%20micro-second%20temporal%0Aresolution%2C%20several%20works%20utilized%20the%20event%20camera%20to%20enhance%20the%20performance%0Aof%20VFI.%20However%2C%20existing%20methods%20estimate%20bidirectional%20inter-frame%20motion%0Afields%20with%20only%20events%20or%20approximations%2C%20which%20can%20not%20consider%20the%20complex%0Amotion%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20event-based%0AVFI%20framework%20with%20cross-modal%20asymmetric%20bidirectional%20motion%20field%0Aestimation.%20In%20detail%2C%20our%20EIF-BiOFNet%20utilizes%20each%20valuable%20characteristic%20of%0Athe%20events%20and%20images%20for%20direct%20estimation%20of%20inter-frame%20motion%20fields%0Awithout%20any%20approximation%20methods.%20Moreover%2C%20we%20develop%20an%20interactive%0Aattention-based%20frame%20synthesis%20network%20to%20efficiently%20leverage%20the%0Acomplementary%20warping-based%20and%20synthesis-based%20features.%20Finally%2C%20we%20build%20a%0Alarge-scale%20event-based%20VFI%20dataset%2C%20ERF-X170FPS%2C%20with%20a%20high%20frame%20rate%2C%0Aextreme%20motion%2C%20and%20dynamic%20textures%20to%20overcome%20the%20limitations%20of%20previous%0Aevent-based%20VFI%20datasets.%20Extensive%20experimental%20results%20validate%20that%20our%0Amethod%20shows%20significant%20performance%20improvement%20over%20the%20state-of-the-art%20VFI%0Amethods%20on%20various%20datasets.%20Our%20project%20pages%20are%20available%20at%3A%0Ahttps%3A//github.com/intelpro/CBMNet%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13716v1&entry.124074799=Read"},
{"title": "Symmetrical Visual Contrastive Optimization: Aligning Vision-Language\n  Models with Minimal Contrastive Images", "author": "Shengguang Wu and Fan-Yun Sun and Kaiyue Wen and Nick Haber", "abstract": "  Recent studies have shown that Large Vision-Language Models (VLMs) tend to\nneglect image content and over-rely on language-model priors, resulting in\nerrors in visually grounded tasks and hallucinations. We hypothesize that this\nissue arises because existing VLMs are not explicitly trained to generate texts\nthat are accurately grounded in fine-grained image details. To enhance visual\nfeedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive\nOptimization), a novel finetuning objective that steers the model toward\ncapturing important visual details and aligning them with corresponding text\ntokens. To further facilitate this detailed alignment, we introduce MVC, a\npaired image-text dataset built by automatically filtering and augmenting\nvisual counterfactual data to challenge the model with hard contrastive cases\ninvolving Minimal Visual Contrasts. Experiments show that our method\nconsistently improves VLM performance across diverse benchmarks covering\nvarious abilities and domains, achieving up to a 22% reduction in\nhallucinations, and significant gains in vision-centric and general tasks.\nNotably, these improvements become increasingly pronounced in benchmarks with\nhigher visual dependency. In short, S-VCO offers a significant enhancement of\nVLM's visually-dependent task performance while retaining or even improving the\nmodel's general abilities. We opensource our code at https://s-vco.github.io/\n", "link": "http://arxiv.org/abs/2502.13928v1", "date": "2025-02-19", "relevancy": 2.2714, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5712}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5712}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Symmetrical%20Visual%20Contrastive%20Optimization%3A%20Aligning%20Vision-Language%0A%20%20Models%20with%20Minimal%20Contrastive%20Images&body=Title%3A%20Symmetrical%20Visual%20Contrastive%20Optimization%3A%20Aligning%20Vision-Language%0A%20%20Models%20with%20Minimal%20Contrastive%20Images%0AAuthor%3A%20Shengguang%20Wu%20and%20Fan-Yun%20Sun%20and%20Kaiyue%20Wen%20and%20Nick%20Haber%0AAbstract%3A%20%20%20Recent%20studies%20have%20shown%20that%20Large%20Vision-Language%20Models%20%28VLMs%29%20tend%20to%0Aneglect%20image%20content%20and%20over-rely%20on%20language-model%20priors%2C%20resulting%20in%0Aerrors%20in%20visually%20grounded%20tasks%20and%20hallucinations.%20We%20hypothesize%20that%20this%0Aissue%20arises%20because%20existing%20VLMs%20are%20not%20explicitly%20trained%20to%20generate%20texts%0Athat%20are%20accurately%20grounded%20in%20fine-grained%20image%20details.%20To%20enhance%20visual%0Afeedback%20during%20VLM%20training%2C%20we%20propose%20S-VCO%20%28Symmetrical%20Visual%20Contrastive%0AOptimization%29%2C%20a%20novel%20finetuning%20objective%20that%20steers%20the%20model%20toward%0Acapturing%20important%20visual%20details%20and%20aligning%20them%20with%20corresponding%20text%0Atokens.%20To%20further%20facilitate%20this%20detailed%20alignment%2C%20we%20introduce%20MVC%2C%20a%0Apaired%20image-text%20dataset%20built%20by%20automatically%20filtering%20and%20augmenting%0Avisual%20counterfactual%20data%20to%20challenge%20the%20model%20with%20hard%20contrastive%20cases%0Ainvolving%20Minimal%20Visual%20Contrasts.%20Experiments%20show%20that%20our%20method%0Aconsistently%20improves%20VLM%20performance%20across%20diverse%20benchmarks%20covering%0Avarious%20abilities%20and%20domains%2C%20achieving%20up%20to%20a%2022%25%20reduction%20in%0Ahallucinations%2C%20and%20significant%20gains%20in%20vision-centric%20and%20general%20tasks.%0ANotably%2C%20these%20improvements%20become%20increasingly%20pronounced%20in%20benchmarks%20with%0Ahigher%20visual%20dependency.%20In%20short%2C%20S-VCO%20offers%20a%20significant%20enhancement%20of%0AVLM%27s%20visually-dependent%20task%20performance%20while%20retaining%20or%20even%20improving%20the%0Amodel%27s%20general%20abilities.%20We%20opensource%20our%20code%20at%20https%3A//s-vco.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSymmetrical%2520Visual%2520Contrastive%2520Optimization%253A%2520Aligning%2520Vision-Language%250A%2520%2520Models%2520with%2520Minimal%2520Contrastive%2520Images%26entry.906535625%3DShengguang%2520Wu%2520and%2520Fan-Yun%2520Sun%2520and%2520Kaiyue%2520Wen%2520and%2520Nick%2520Haber%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520shown%2520that%2520Large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520tend%2520to%250Aneglect%2520image%2520content%2520and%2520over-rely%2520on%2520language-model%2520priors%252C%2520resulting%2520in%250Aerrors%2520in%2520visually%2520grounded%2520tasks%2520and%2520hallucinations.%2520We%2520hypothesize%2520that%2520this%250Aissue%2520arises%2520because%2520existing%2520VLMs%2520are%2520not%2520explicitly%2520trained%2520to%2520generate%2520texts%250Athat%2520are%2520accurately%2520grounded%2520in%2520fine-grained%2520image%2520details.%2520To%2520enhance%2520visual%250Afeedback%2520during%2520VLM%2520training%252C%2520we%2520propose%2520S-VCO%2520%2528Symmetrical%2520Visual%2520Contrastive%250AOptimization%2529%252C%2520a%2520novel%2520finetuning%2520objective%2520that%2520steers%2520the%2520model%2520toward%250Acapturing%2520important%2520visual%2520details%2520and%2520aligning%2520them%2520with%2520corresponding%2520text%250Atokens.%2520To%2520further%2520facilitate%2520this%2520detailed%2520alignment%252C%2520we%2520introduce%2520MVC%252C%2520a%250Apaired%2520image-text%2520dataset%2520built%2520by%2520automatically%2520filtering%2520and%2520augmenting%250Avisual%2520counterfactual%2520data%2520to%2520challenge%2520the%2520model%2520with%2520hard%2520contrastive%2520cases%250Ainvolving%2520Minimal%2520Visual%2520Contrasts.%2520Experiments%2520show%2520that%2520our%2520method%250Aconsistently%2520improves%2520VLM%2520performance%2520across%2520diverse%2520benchmarks%2520covering%250Avarious%2520abilities%2520and%2520domains%252C%2520achieving%2520up%2520to%2520a%252022%2525%2520reduction%2520in%250Ahallucinations%252C%2520and%2520significant%2520gains%2520in%2520vision-centric%2520and%2520general%2520tasks.%250ANotably%252C%2520these%2520improvements%2520become%2520increasingly%2520pronounced%2520in%2520benchmarks%2520with%250Ahigher%2520visual%2520dependency.%2520In%2520short%252C%2520S-VCO%2520offers%2520a%2520significant%2520enhancement%2520of%250AVLM%2527s%2520visually-dependent%2520task%2520performance%2520while%2520retaining%2520or%2520even%2520improving%2520the%250Amodel%2527s%2520general%2520abilities.%2520We%2520opensource%2520our%2520code%2520at%2520https%253A//s-vco.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symmetrical%20Visual%20Contrastive%20Optimization%3A%20Aligning%20Vision-Language%0A%20%20Models%20with%20Minimal%20Contrastive%20Images&entry.906535625=Shengguang%20Wu%20and%20Fan-Yun%20Sun%20and%20Kaiyue%20Wen%20and%20Nick%20Haber&entry.1292438233=%20%20Recent%20studies%20have%20shown%20that%20Large%20Vision-Language%20Models%20%28VLMs%29%20tend%20to%0Aneglect%20image%20content%20and%20over-rely%20on%20language-model%20priors%2C%20resulting%20in%0Aerrors%20in%20visually%20grounded%20tasks%20and%20hallucinations.%20We%20hypothesize%20that%20this%0Aissue%20arises%20because%20existing%20VLMs%20are%20not%20explicitly%20trained%20to%20generate%20texts%0Athat%20are%20accurately%20grounded%20in%20fine-grained%20image%20details.%20To%20enhance%20visual%0Afeedback%20during%20VLM%20training%2C%20we%20propose%20S-VCO%20%28Symmetrical%20Visual%20Contrastive%0AOptimization%29%2C%20a%20novel%20finetuning%20objective%20that%20steers%20the%20model%20toward%0Acapturing%20important%20visual%20details%20and%20aligning%20them%20with%20corresponding%20text%0Atokens.%20To%20further%20facilitate%20this%20detailed%20alignment%2C%20we%20introduce%20MVC%2C%20a%0Apaired%20image-text%20dataset%20built%20by%20automatically%20filtering%20and%20augmenting%0Avisual%20counterfactual%20data%20to%20challenge%20the%20model%20with%20hard%20contrastive%20cases%0Ainvolving%20Minimal%20Visual%20Contrasts.%20Experiments%20show%20that%20our%20method%0Aconsistently%20improves%20VLM%20performance%20across%20diverse%20benchmarks%20covering%0Avarious%20abilities%20and%20domains%2C%20achieving%20up%20to%20a%2022%25%20reduction%20in%0Ahallucinations%2C%20and%20significant%20gains%20in%20vision-centric%20and%20general%20tasks.%0ANotably%2C%20these%20improvements%20become%20increasingly%20pronounced%20in%20benchmarks%20with%0Ahigher%20visual%20dependency.%20In%20short%2C%20S-VCO%20offers%20a%20significant%20enhancement%20of%0AVLM%27s%20visually-dependent%20task%20performance%20while%20retaining%20or%20even%20improving%20the%0Amodel%27s%20general%20abilities.%20We%20opensource%20our%20code%20at%20https%3A//s-vco.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13928v1&entry.124074799=Read"},
{"title": "Forward-Forward Learning achieves Highly Selective Latent\n  Representations for Out-of-Distribution Detection in Fully Spiking Neural\n  Networks", "author": "Erik B. Terres-Escudero and Javier Del Ser and Aitor Mart\u00ednez-Seras and Pablo Garcia-Bringas", "abstract": "  In recent years, Artificial Intelligence (AI) models have achieved remarkable\nsuccess across various domains, yet challenges persist in two critical areas:\nensuring robustness against uncertain inputs and drastically increasing model\nefficiency during training and inference. Spiking Neural Networks (SNNs),\ninspired by biological systems, offer a promising avenue for overcoming these\nlimitations. By operating in an event-driven manner, SNNs achieve low energy\nconsumption and can naturally implement biological methods known for their high\nnoise tolerance. In this work, we explore the potential of the spiking\nForward-Forward Algorithm (FFA) to address these challenges, leveraging its\nrepresentational properties for both Out-of-Distribution (OoD) detection and\ninterpretability. To achieve this, we exploit the sparse and highly specialized\nneural latent space of FF networks to estimate the likelihood of a sample\nbelonging to the training distribution. Additionally, we propose a novel,\ngradient-free attribution method to detect features that drive a sample away\nfrom class distributions, addressing the challenges posed by the lack of\ngradients in most visual interpretability methods for spiking models. We\nevaluate our OoD detection algorithm on well-known image datasets (e.g.,\nOmniglot, Not-MNIST, CIFAR10), outperforming previous methods proposed in the\nrecent literature for OoD detection in spiking networks. Furthermore, our\nattribution method precisely identifies salient OoD features, such as artifacts\nor missing regions, hence providing a visual explanatory interface for the user\nto understand why unknown inputs are identified as such by the proposed method.\n", "link": "http://arxiv.org/abs/2407.14097v2", "date": "2025-02-19", "relevancy": 2.2586, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5819}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5528}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forward-Forward%20Learning%20achieves%20Highly%20Selective%20Latent%0A%20%20Representations%20for%20Out-of-Distribution%20Detection%20in%20Fully%20Spiking%20Neural%0A%20%20Networks&body=Title%3A%20Forward-Forward%20Learning%20achieves%20Highly%20Selective%20Latent%0A%20%20Representations%20for%20Out-of-Distribution%20Detection%20in%20Fully%20Spiking%20Neural%0A%20%20Networks%0AAuthor%3A%20Erik%20B.%20Terres-Escudero%20and%20Javier%20Del%20Ser%20and%20Aitor%20Mart%C3%ADnez-Seras%20and%20Pablo%20Garcia-Bringas%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Artificial%20Intelligence%20%28AI%29%20models%20have%20achieved%20remarkable%0Asuccess%20across%20various%20domains%2C%20yet%20challenges%20persist%20in%20two%20critical%20areas%3A%0Aensuring%20robustness%20against%20uncertain%20inputs%20and%20drastically%20increasing%20model%0Aefficiency%20during%20training%20and%20inference.%20Spiking%20Neural%20Networks%20%28SNNs%29%2C%0Ainspired%20by%20biological%20systems%2C%20offer%20a%20promising%20avenue%20for%20overcoming%20these%0Alimitations.%20By%20operating%20in%20an%20event-driven%20manner%2C%20SNNs%20achieve%20low%20energy%0Aconsumption%20and%20can%20naturally%20implement%20biological%20methods%20known%20for%20their%20high%0Anoise%20tolerance.%20In%20this%20work%2C%20we%20explore%20the%20potential%20of%20the%20spiking%0AForward-Forward%20Algorithm%20%28FFA%29%20to%20address%20these%20challenges%2C%20leveraging%20its%0Arepresentational%20properties%20for%20both%20Out-of-Distribution%20%28OoD%29%20detection%20and%0Ainterpretability.%20To%20achieve%20this%2C%20we%20exploit%20the%20sparse%20and%20highly%20specialized%0Aneural%20latent%20space%20of%20FF%20networks%20to%20estimate%20the%20likelihood%20of%20a%20sample%0Abelonging%20to%20the%20training%20distribution.%20Additionally%2C%20we%20propose%20a%20novel%2C%0Agradient-free%20attribution%20method%20to%20detect%20features%20that%20drive%20a%20sample%20away%0Afrom%20class%20distributions%2C%20addressing%20the%20challenges%20posed%20by%20the%20lack%20of%0Agradients%20in%20most%20visual%20interpretability%20methods%20for%20spiking%20models.%20We%0Aevaluate%20our%20OoD%20detection%20algorithm%20on%20well-known%20image%20datasets%20%28e.g.%2C%0AOmniglot%2C%20Not-MNIST%2C%20CIFAR10%29%2C%20outperforming%20previous%20methods%20proposed%20in%20the%0Arecent%20literature%20for%20OoD%20detection%20in%20spiking%20networks.%20Furthermore%2C%20our%0Aattribution%20method%20precisely%20identifies%20salient%20OoD%20features%2C%20such%20as%20artifacts%0Aor%20missing%20regions%2C%20hence%20providing%20a%20visual%20explanatory%20interface%20for%20the%20user%0Ato%20understand%20why%20unknown%20inputs%20are%20identified%20as%20such%20by%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14097v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForward-Forward%2520Learning%2520achieves%2520Highly%2520Selective%2520Latent%250A%2520%2520Representations%2520for%2520Out-of-Distribution%2520Detection%2520in%2520Fully%2520Spiking%2520Neural%250A%2520%2520Networks%26entry.906535625%3DErik%2520B.%2520Terres-Escudero%2520and%2520Javier%2520Del%2520Ser%2520and%2520Aitor%2520Mart%25C3%25ADnez-Seras%2520and%2520Pablo%2520Garcia-Bringas%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Artificial%2520Intelligence%2520%2528AI%2529%2520models%2520have%2520achieved%2520remarkable%250Asuccess%2520across%2520various%2520domains%252C%2520yet%2520challenges%2520persist%2520in%2520two%2520critical%2520areas%253A%250Aensuring%2520robustness%2520against%2520uncertain%2520inputs%2520and%2520drastically%2520increasing%2520model%250Aefficiency%2520during%2520training%2520and%2520inference.%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%252C%250Ainspired%2520by%2520biological%2520systems%252C%2520offer%2520a%2520promising%2520avenue%2520for%2520overcoming%2520these%250Alimitations.%2520By%2520operating%2520in%2520an%2520event-driven%2520manner%252C%2520SNNs%2520achieve%2520low%2520energy%250Aconsumption%2520and%2520can%2520naturally%2520implement%2520biological%2520methods%2520known%2520for%2520their%2520high%250Anoise%2520tolerance.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520potential%2520of%2520the%2520spiking%250AForward-Forward%2520Algorithm%2520%2528FFA%2529%2520to%2520address%2520these%2520challenges%252C%2520leveraging%2520its%250Arepresentational%2520properties%2520for%2520both%2520Out-of-Distribution%2520%2528OoD%2529%2520detection%2520and%250Ainterpretability.%2520To%2520achieve%2520this%252C%2520we%2520exploit%2520the%2520sparse%2520and%2520highly%2520specialized%250Aneural%2520latent%2520space%2520of%2520FF%2520networks%2520to%2520estimate%2520the%2520likelihood%2520of%2520a%2520sample%250Abelonging%2520to%2520the%2520training%2520distribution.%2520Additionally%252C%2520we%2520propose%2520a%2520novel%252C%250Agradient-free%2520attribution%2520method%2520to%2520detect%2520features%2520that%2520drive%2520a%2520sample%2520away%250Afrom%2520class%2520distributions%252C%2520addressing%2520the%2520challenges%2520posed%2520by%2520the%2520lack%2520of%250Agradients%2520in%2520most%2520visual%2520interpretability%2520methods%2520for%2520spiking%2520models.%2520We%250Aevaluate%2520our%2520OoD%2520detection%2520algorithm%2520on%2520well-known%2520image%2520datasets%2520%2528e.g.%252C%250AOmniglot%252C%2520Not-MNIST%252C%2520CIFAR10%2529%252C%2520outperforming%2520previous%2520methods%2520proposed%2520in%2520the%250Arecent%2520literature%2520for%2520OoD%2520detection%2520in%2520spiking%2520networks.%2520Furthermore%252C%2520our%250Aattribution%2520method%2520precisely%2520identifies%2520salient%2520OoD%2520features%252C%2520such%2520as%2520artifacts%250Aor%2520missing%2520regions%252C%2520hence%2520providing%2520a%2520visual%2520explanatory%2520interface%2520for%2520the%2520user%250Ato%2520understand%2520why%2520unknown%2520inputs%2520are%2520identified%2520as%2520such%2520by%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14097v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forward-Forward%20Learning%20achieves%20Highly%20Selective%20Latent%0A%20%20Representations%20for%20Out-of-Distribution%20Detection%20in%20Fully%20Spiking%20Neural%0A%20%20Networks&entry.906535625=Erik%20B.%20Terres-Escudero%20and%20Javier%20Del%20Ser%20and%20Aitor%20Mart%C3%ADnez-Seras%20and%20Pablo%20Garcia-Bringas&entry.1292438233=%20%20In%20recent%20years%2C%20Artificial%20Intelligence%20%28AI%29%20models%20have%20achieved%20remarkable%0Asuccess%20across%20various%20domains%2C%20yet%20challenges%20persist%20in%20two%20critical%20areas%3A%0Aensuring%20robustness%20against%20uncertain%20inputs%20and%20drastically%20increasing%20model%0Aefficiency%20during%20training%20and%20inference.%20Spiking%20Neural%20Networks%20%28SNNs%29%2C%0Ainspired%20by%20biological%20systems%2C%20offer%20a%20promising%20avenue%20for%20overcoming%20these%0Alimitations.%20By%20operating%20in%20an%20event-driven%20manner%2C%20SNNs%20achieve%20low%20energy%0Aconsumption%20and%20can%20naturally%20implement%20biological%20methods%20known%20for%20their%20high%0Anoise%20tolerance.%20In%20this%20work%2C%20we%20explore%20the%20potential%20of%20the%20spiking%0AForward-Forward%20Algorithm%20%28FFA%29%20to%20address%20these%20challenges%2C%20leveraging%20its%0Arepresentational%20properties%20for%20both%20Out-of-Distribution%20%28OoD%29%20detection%20and%0Ainterpretability.%20To%20achieve%20this%2C%20we%20exploit%20the%20sparse%20and%20highly%20specialized%0Aneural%20latent%20space%20of%20FF%20networks%20to%20estimate%20the%20likelihood%20of%20a%20sample%0Abelonging%20to%20the%20training%20distribution.%20Additionally%2C%20we%20propose%20a%20novel%2C%0Agradient-free%20attribution%20method%20to%20detect%20features%20that%20drive%20a%20sample%20away%0Afrom%20class%20distributions%2C%20addressing%20the%20challenges%20posed%20by%20the%20lack%20of%0Agradients%20in%20most%20visual%20interpretability%20methods%20for%20spiking%20models.%20We%0Aevaluate%20our%20OoD%20detection%20algorithm%20on%20well-known%20image%20datasets%20%28e.g.%2C%0AOmniglot%2C%20Not-MNIST%2C%20CIFAR10%29%2C%20outperforming%20previous%20methods%20proposed%20in%20the%0Arecent%20literature%20for%20OoD%20detection%20in%20spiking%20networks.%20Furthermore%2C%20our%0Aattribution%20method%20precisely%20identifies%20salient%20OoD%20features%2C%20such%20as%20artifacts%0Aor%20missing%20regions%2C%20hence%20providing%20a%20visual%20explanatory%20interface%20for%20the%20user%0Ato%20understand%20why%20unknown%20inputs%20are%20identified%20as%20such%20by%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14097v2&entry.124074799=Read"},
{"title": "Heterophily-Aware Fair Recommendation using Graph Convolutional Networks", "author": "Nemat Gholinejad and Mostafa Haghir Chehreghani", "abstract": "  In recent years, graph neural networks (GNNs) have become a popular tool to\nimprove the accuracy and performance of recommender systems. Modern recommender\nsystems are not only designed to serve end users, but also to benefit other\nparticipants, such as items and item providers. These participants may have\ndifferent or conflicting goals and interests, which raises the need for\nfairness and popularity bias considerations. GNN-based recommendation methods\nalso face the challenges of unfairness and popularity bias, and their\nnormalization and aggregation processes suffer from these challenges. In this\npaper, we propose a fair GNN-based recommender system, called HetroFair, to\nimprove item-side fairness. HetroFair uses two separate components to generate\nfairness-aware embeddings: i) Fairness-aware attention, which incorporates the\ndot product in the normalization process of GNNs to decrease the effect of\nnodes' degrees. ii) Heterophily feature weighting, to assign distinct weights\nto different features during the aggregation process. To evaluate the\neffectiveness of HetroFair, we conduct extensive experiments over six\nreal-world datasets. Our experimental results reveal that HetroFair not only\nalleviates unfairness and popularity bias on the item side but also achieves\nsuperior accuracy on the user side. Our implementation is publicly available at\nhttps://github.com/NematGH/HetroFair.\n", "link": "http://arxiv.org/abs/2402.03365v3", "date": "2025-02-19", "relevancy": 2.2574, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4826}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4394}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterophily-Aware%20Fair%20Recommendation%20using%20Graph%20Convolutional%20Networks&body=Title%3A%20Heterophily-Aware%20Fair%20Recommendation%20using%20Graph%20Convolutional%20Networks%0AAuthor%3A%20Nemat%20Gholinejad%20and%20Mostafa%20Haghir%20Chehreghani%0AAbstract%3A%20%20%20In%20recent%20years%2C%20graph%20neural%20networks%20%28GNNs%29%20have%20become%20a%20popular%20tool%20to%0Aimprove%20the%20accuracy%20and%20performance%20of%20recommender%20systems.%20Modern%20recommender%0Asystems%20are%20not%20only%20designed%20to%20serve%20end%20users%2C%20but%20also%20to%20benefit%20other%0Aparticipants%2C%20such%20as%20items%20and%20item%20providers.%20These%20participants%20may%20have%0Adifferent%20or%20conflicting%20goals%20and%20interests%2C%20which%20raises%20the%20need%20for%0Afairness%20and%20popularity%20bias%20considerations.%20GNN-based%20recommendation%20methods%0Aalso%20face%20the%20challenges%20of%20unfairness%20and%20popularity%20bias%2C%20and%20their%0Anormalization%20and%20aggregation%20processes%20suffer%20from%20these%20challenges.%20In%20this%0Apaper%2C%20we%20propose%20a%20fair%20GNN-based%20recommender%20system%2C%20called%20HetroFair%2C%20to%0Aimprove%20item-side%20fairness.%20HetroFair%20uses%20two%20separate%20components%20to%20generate%0Afairness-aware%20embeddings%3A%20i%29%20Fairness-aware%20attention%2C%20which%20incorporates%20the%0Adot%20product%20in%20the%20normalization%20process%20of%20GNNs%20to%20decrease%20the%20effect%20of%0Anodes%27%20degrees.%20ii%29%20Heterophily%20feature%20weighting%2C%20to%20assign%20distinct%20weights%0Ato%20different%20features%20during%20the%20aggregation%20process.%20To%20evaluate%20the%0Aeffectiveness%20of%20HetroFair%2C%20we%20conduct%20extensive%20experiments%20over%20six%0Areal-world%20datasets.%20Our%20experimental%20results%20reveal%20that%20HetroFair%20not%20only%0Aalleviates%20unfairness%20and%20popularity%20bias%20on%20the%20item%20side%20but%20also%20achieves%0Asuperior%20accuracy%20on%20the%20user%20side.%20Our%20implementation%20is%20publicly%20available%20at%0Ahttps%3A//github.com/NematGH/HetroFair.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03365v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterophily-Aware%2520Fair%2520Recommendation%2520using%2520Graph%2520Convolutional%2520Networks%26entry.906535625%3DNemat%2520Gholinejad%2520and%2520Mostafa%2520Haghir%2520Chehreghani%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520become%2520a%2520popular%2520tool%2520to%250Aimprove%2520the%2520accuracy%2520and%2520performance%2520of%2520recommender%2520systems.%2520Modern%2520recommender%250Asystems%2520are%2520not%2520only%2520designed%2520to%2520serve%2520end%2520users%252C%2520but%2520also%2520to%2520benefit%2520other%250Aparticipants%252C%2520such%2520as%2520items%2520and%2520item%2520providers.%2520These%2520participants%2520may%2520have%250Adifferent%2520or%2520conflicting%2520goals%2520and%2520interests%252C%2520which%2520raises%2520the%2520need%2520for%250Afairness%2520and%2520popularity%2520bias%2520considerations.%2520GNN-based%2520recommendation%2520methods%250Aalso%2520face%2520the%2520challenges%2520of%2520unfairness%2520and%2520popularity%2520bias%252C%2520and%2520their%250Anormalization%2520and%2520aggregation%2520processes%2520suffer%2520from%2520these%2520challenges.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520fair%2520GNN-based%2520recommender%2520system%252C%2520called%2520HetroFair%252C%2520to%250Aimprove%2520item-side%2520fairness.%2520HetroFair%2520uses%2520two%2520separate%2520components%2520to%2520generate%250Afairness-aware%2520embeddings%253A%2520i%2529%2520Fairness-aware%2520attention%252C%2520which%2520incorporates%2520the%250Adot%2520product%2520in%2520the%2520normalization%2520process%2520of%2520GNNs%2520to%2520decrease%2520the%2520effect%2520of%250Anodes%2527%2520degrees.%2520ii%2529%2520Heterophily%2520feature%2520weighting%252C%2520to%2520assign%2520distinct%2520weights%250Ato%2520different%2520features%2520during%2520the%2520aggregation%2520process.%2520To%2520evaluate%2520the%250Aeffectiveness%2520of%2520HetroFair%252C%2520we%2520conduct%2520extensive%2520experiments%2520over%2520six%250Areal-world%2520datasets.%2520Our%2520experimental%2520results%2520reveal%2520that%2520HetroFair%2520not%2520only%250Aalleviates%2520unfairness%2520and%2520popularity%2520bias%2520on%2520the%2520item%2520side%2520but%2520also%2520achieves%250Asuperior%2520accuracy%2520on%2520the%2520user%2520side.%2520Our%2520implementation%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/NematGH/HetroFair.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03365v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterophily-Aware%20Fair%20Recommendation%20using%20Graph%20Convolutional%20Networks&entry.906535625=Nemat%20Gholinejad%20and%20Mostafa%20Haghir%20Chehreghani&entry.1292438233=%20%20In%20recent%20years%2C%20graph%20neural%20networks%20%28GNNs%29%20have%20become%20a%20popular%20tool%20to%0Aimprove%20the%20accuracy%20and%20performance%20of%20recommender%20systems.%20Modern%20recommender%0Asystems%20are%20not%20only%20designed%20to%20serve%20end%20users%2C%20but%20also%20to%20benefit%20other%0Aparticipants%2C%20such%20as%20items%20and%20item%20providers.%20These%20participants%20may%20have%0Adifferent%20or%20conflicting%20goals%20and%20interests%2C%20which%20raises%20the%20need%20for%0Afairness%20and%20popularity%20bias%20considerations.%20GNN-based%20recommendation%20methods%0Aalso%20face%20the%20challenges%20of%20unfairness%20and%20popularity%20bias%2C%20and%20their%0Anormalization%20and%20aggregation%20processes%20suffer%20from%20these%20challenges.%20In%20this%0Apaper%2C%20we%20propose%20a%20fair%20GNN-based%20recommender%20system%2C%20called%20HetroFair%2C%20to%0Aimprove%20item-side%20fairness.%20HetroFair%20uses%20two%20separate%20components%20to%20generate%0Afairness-aware%20embeddings%3A%20i%29%20Fairness-aware%20attention%2C%20which%20incorporates%20the%0Adot%20product%20in%20the%20normalization%20process%20of%20GNNs%20to%20decrease%20the%20effect%20of%0Anodes%27%20degrees.%20ii%29%20Heterophily%20feature%20weighting%2C%20to%20assign%20distinct%20weights%0Ato%20different%20features%20during%20the%20aggregation%20process.%20To%20evaluate%20the%0Aeffectiveness%20of%20HetroFair%2C%20we%20conduct%20extensive%20experiments%20over%20six%0Areal-world%20datasets.%20Our%20experimental%20results%20reveal%20that%20HetroFair%20not%20only%0Aalleviates%20unfairness%20and%20popularity%20bias%20on%20the%20item%20side%20but%20also%20achieves%0Asuperior%20accuracy%20on%20the%20user%20side.%20Our%20implementation%20is%20publicly%20available%20at%0Ahttps%3A//github.com/NematGH/HetroFair.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03365v3&entry.124074799=Read"},
{"title": "Active Illumination for Visual Ego-Motion Estimation in the Dark", "author": "Francesco Crocetti and Alberto Dionigi and Raffaele Brilli and Gabriele Costante and Paolo Valigi", "abstract": "  Visual Odometry (VO) and Visual SLAM (V-SLAM) systems often struggle in\nlow-light and dark environments due to the lack of robust visual features. In\nthis paper, we propose a novel active illumination framework to enhance the\nperformance of VO and V-SLAM algorithms in these challenging conditions. The\ndeveloped approach dynamically controls a moving light source to illuminate\nhighly textured areas, thereby improving feature extraction and tracking.\nSpecifically, a detector block, which incorporates a deep learning-based\nenhancing network, identifies regions with relevant features. Then, a pan-tilt\ncontroller is responsible for guiding the light beam toward these areas, so\nthat to provide information-rich images to the ego-motion estimation algorithm.\nExperimental results on a real robotic platform demonstrate the effectiveness\nof the proposed method, showing a reduction in the pose estimation error up to\n75% with respect to a traditional fixed lighting technique.\n", "link": "http://arxiv.org/abs/2502.13708v1", "date": "2025-02-19", "relevancy": 2.2504, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5746}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5546}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Illumination%20for%20Visual%20Ego-Motion%20Estimation%20in%20the%20Dark&body=Title%3A%20Active%20Illumination%20for%20Visual%20Ego-Motion%20Estimation%20in%20the%20Dark%0AAuthor%3A%20Francesco%20Crocetti%20and%20Alberto%20Dionigi%20and%20Raffaele%20Brilli%20and%20Gabriele%20Costante%20and%20Paolo%20Valigi%0AAbstract%3A%20%20%20Visual%20Odometry%20%28VO%29%20and%20Visual%20SLAM%20%28V-SLAM%29%20systems%20often%20struggle%20in%0Alow-light%20and%20dark%20environments%20due%20to%20the%20lack%20of%20robust%20visual%20features.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20active%20illumination%20framework%20to%20enhance%20the%0Aperformance%20of%20VO%20and%20V-SLAM%20algorithms%20in%20these%20challenging%20conditions.%20The%0Adeveloped%20approach%20dynamically%20controls%20a%20moving%20light%20source%20to%20illuminate%0Ahighly%20textured%20areas%2C%20thereby%20improving%20feature%20extraction%20and%20tracking.%0ASpecifically%2C%20a%20detector%20block%2C%20which%20incorporates%20a%20deep%20learning-based%0Aenhancing%20network%2C%20identifies%20regions%20with%20relevant%20features.%20Then%2C%20a%20pan-tilt%0Acontroller%20is%20responsible%20for%20guiding%20the%20light%20beam%20toward%20these%20areas%2C%20so%0Athat%20to%20provide%20information-rich%20images%20to%20the%20ego-motion%20estimation%20algorithm.%0AExperimental%20results%20on%20a%20real%20robotic%20platform%20demonstrate%20the%20effectiveness%0Aof%20the%20proposed%20method%2C%20showing%20a%20reduction%20in%20the%20pose%20estimation%20error%20up%20to%0A75%25%20with%20respect%20to%20a%20traditional%20fixed%20lighting%20technique.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Illumination%2520for%2520Visual%2520Ego-Motion%2520Estimation%2520in%2520the%2520Dark%26entry.906535625%3DFrancesco%2520Crocetti%2520and%2520Alberto%2520Dionigi%2520and%2520Raffaele%2520Brilli%2520and%2520Gabriele%2520Costante%2520and%2520Paolo%2520Valigi%26entry.1292438233%3D%2520%2520Visual%2520Odometry%2520%2528VO%2529%2520and%2520Visual%2520SLAM%2520%2528V-SLAM%2529%2520systems%2520often%2520struggle%2520in%250Alow-light%2520and%2520dark%2520environments%2520due%2520to%2520the%2520lack%2520of%2520robust%2520visual%2520features.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520novel%2520active%2520illumination%2520framework%2520to%2520enhance%2520the%250Aperformance%2520of%2520VO%2520and%2520V-SLAM%2520algorithms%2520in%2520these%2520challenging%2520conditions.%2520The%250Adeveloped%2520approach%2520dynamically%2520controls%2520a%2520moving%2520light%2520source%2520to%2520illuminate%250Ahighly%2520textured%2520areas%252C%2520thereby%2520improving%2520feature%2520extraction%2520and%2520tracking.%250ASpecifically%252C%2520a%2520detector%2520block%252C%2520which%2520incorporates%2520a%2520deep%2520learning-based%250Aenhancing%2520network%252C%2520identifies%2520regions%2520with%2520relevant%2520features.%2520Then%252C%2520a%2520pan-tilt%250Acontroller%2520is%2520responsible%2520for%2520guiding%2520the%2520light%2520beam%2520toward%2520these%2520areas%252C%2520so%250Athat%2520to%2520provide%2520information-rich%2520images%2520to%2520the%2520ego-motion%2520estimation%2520algorithm.%250AExperimental%2520results%2520on%2520a%2520real%2520robotic%2520platform%2520demonstrate%2520the%2520effectiveness%250Aof%2520the%2520proposed%2520method%252C%2520showing%2520a%2520reduction%2520in%2520the%2520pose%2520estimation%2520error%2520up%2520to%250A75%2525%2520with%2520respect%2520to%2520a%2520traditional%2520fixed%2520lighting%2520technique.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Illumination%20for%20Visual%20Ego-Motion%20Estimation%20in%20the%20Dark&entry.906535625=Francesco%20Crocetti%20and%20Alberto%20Dionigi%20and%20Raffaele%20Brilli%20and%20Gabriele%20Costante%20and%20Paolo%20Valigi&entry.1292438233=%20%20Visual%20Odometry%20%28VO%29%20and%20Visual%20SLAM%20%28V-SLAM%29%20systems%20often%20struggle%20in%0Alow-light%20and%20dark%20environments%20due%20to%20the%20lack%20of%20robust%20visual%20features.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20active%20illumination%20framework%20to%20enhance%20the%0Aperformance%20of%20VO%20and%20V-SLAM%20algorithms%20in%20these%20challenging%20conditions.%20The%0Adeveloped%20approach%20dynamically%20controls%20a%20moving%20light%20source%20to%20illuminate%0Ahighly%20textured%20areas%2C%20thereby%20improving%20feature%20extraction%20and%20tracking.%0ASpecifically%2C%20a%20detector%20block%2C%20which%20incorporates%20a%20deep%20learning-based%0Aenhancing%20network%2C%20identifies%20regions%20with%20relevant%20features.%20Then%2C%20a%20pan-tilt%0Acontroller%20is%20responsible%20for%20guiding%20the%20light%20beam%20toward%20these%20areas%2C%20so%0Athat%20to%20provide%20information-rich%20images%20to%20the%20ego-motion%20estimation%20algorithm.%0AExperimental%20results%20on%20a%20real%20robotic%20platform%20demonstrate%20the%20effectiveness%0Aof%20the%20proposed%20method%2C%20showing%20a%20reduction%20in%20the%20pose%20estimation%20error%20up%20to%0A75%25%20with%20respect%20to%20a%20traditional%20fixed%20lighting%20technique.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13708v1&entry.124074799=Read"},
{"title": "Na'vi or Knave: Jailbreaking Language Models via Metaphorical Avatars", "author": "Yu Yan and Sheng Sun and Junqi Tong and Min Liu and Qi Li", "abstract": "  Metaphor serves as an implicit approach to convey information, while enabling\nthe generalized comprehension of complex subjects. However, metaphor can\npotentially be exploited to bypass the safety alignment mechanisms of Large\nLanguage Models (LLMs), leading to the theft of harmful knowledge. In our\nstudy, we introduce a novel attack framework that exploits the imaginative\ncapacity of LLMs to achieve jailbreaking, the J\\underline{\\textbf{A}}ilbreak\n\\underline{\\textbf{V}}ia \\underline{\\textbf{A}}dversarial\nMe\\underline{\\textbf{TA}} -pho\\underline{\\textbf{R}} (\\textit{AVATAR}).\nSpecifically, to elicit the harmful response, AVATAR extracts harmful entities\nfrom a given harmful target and maps them to innocuous adversarial entities\nbased on LLM's imagination. Then, according to these metaphors, the harmful\ntarget is nested within human-like interaction for jailbreaking adaptively.\nExperimental results demonstrate that AVATAR can effectively and transferablly\njailbreak LLMs and achieve a state-of-the-art attack success rate across\nmultiple advanced LLMs. Our study exposes a security risk in LLMs from their\nendogenous imaginative capabilities. Furthermore, the analytical study reveals\nthe vulnerability of LLM to adversarial metaphors and the necessity of\ndeveloping defense methods against jailbreaking caused by the adversarial\nmetaphor. \\textcolor{orange}{ \\textbf{Warning: This paper contains potentially\nharmful content from LLMs.}}\n", "link": "http://arxiv.org/abs/2412.12145v3", "date": "2025-02-19", "relevancy": 2.2499, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Na%27vi%20or%20Knave%3A%20Jailbreaking%20Language%20Models%20via%20Metaphorical%20Avatars&body=Title%3A%20Na%27vi%20or%20Knave%3A%20Jailbreaking%20Language%20Models%20via%20Metaphorical%20Avatars%0AAuthor%3A%20Yu%20Yan%20and%20Sheng%20Sun%20and%20Junqi%20Tong%20and%20Min%20Liu%20and%20Qi%20Li%0AAbstract%3A%20%20%20Metaphor%20serves%20as%20an%20implicit%20approach%20to%20convey%20information%2C%20while%20enabling%0Athe%20generalized%20comprehension%20of%20complex%20subjects.%20However%2C%20metaphor%20can%0Apotentially%20be%20exploited%20to%20bypass%20the%20safety%20alignment%20mechanisms%20of%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20leading%20to%20the%20theft%20of%20harmful%20knowledge.%20In%20our%0Astudy%2C%20we%20introduce%20a%20novel%20attack%20framework%20that%20exploits%20the%20imaginative%0Acapacity%20of%20LLMs%20to%20achieve%20jailbreaking%2C%20the%20J%5Cunderline%7B%5Ctextbf%7BA%7D%7Dilbreak%0A%5Cunderline%7B%5Ctextbf%7BV%7D%7Dia%20%5Cunderline%7B%5Ctextbf%7BA%7D%7Ddversarial%0AMe%5Cunderline%7B%5Ctextbf%7BTA%7D%7D%20-pho%5Cunderline%7B%5Ctextbf%7BR%7D%7D%20%28%5Ctextit%7BAVATAR%7D%29.%0ASpecifically%2C%20to%20elicit%20the%20harmful%20response%2C%20AVATAR%20extracts%20harmful%20entities%0Afrom%20a%20given%20harmful%20target%20and%20maps%20them%20to%20innocuous%20adversarial%20entities%0Abased%20on%20LLM%27s%20imagination.%20Then%2C%20according%20to%20these%20metaphors%2C%20the%20harmful%0Atarget%20is%20nested%20within%20human-like%20interaction%20for%20jailbreaking%20adaptively.%0AExperimental%20results%20demonstrate%20that%20AVATAR%20can%20effectively%20and%20transferablly%0Ajailbreak%20LLMs%20and%20achieve%20a%20state-of-the-art%20attack%20success%20rate%20across%0Amultiple%20advanced%20LLMs.%20Our%20study%20exposes%20a%20security%20risk%20in%20LLMs%20from%20their%0Aendogenous%20imaginative%20capabilities.%20Furthermore%2C%20the%20analytical%20study%20reveals%0Athe%20vulnerability%20of%20LLM%20to%20adversarial%20metaphors%20and%20the%20necessity%20of%0Adeveloping%20defense%20methods%20against%20jailbreaking%20caused%20by%20the%20adversarial%0Ametaphor.%20%5Ctextcolor%7Borange%7D%7B%20%5Ctextbf%7BWarning%3A%20This%20paper%20contains%20potentially%0Aharmful%20content%20from%20LLMs.%7D%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12145v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNa%2527vi%2520or%2520Knave%253A%2520Jailbreaking%2520Language%2520Models%2520via%2520Metaphorical%2520Avatars%26entry.906535625%3DYu%2520Yan%2520and%2520Sheng%2520Sun%2520and%2520Junqi%2520Tong%2520and%2520Min%2520Liu%2520and%2520Qi%2520Li%26entry.1292438233%3D%2520%2520Metaphor%2520serves%2520as%2520an%2520implicit%2520approach%2520to%2520convey%2520information%252C%2520while%2520enabling%250Athe%2520generalized%2520comprehension%2520of%2520complex%2520subjects.%2520However%252C%2520metaphor%2520can%250Apotentially%2520be%2520exploited%2520to%2520bypass%2520the%2520safety%2520alignment%2520mechanisms%2520of%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%252C%2520leading%2520to%2520the%2520theft%2520of%2520harmful%2520knowledge.%2520In%2520our%250Astudy%252C%2520we%2520introduce%2520a%2520novel%2520attack%2520framework%2520that%2520exploits%2520the%2520imaginative%250Acapacity%2520of%2520LLMs%2520to%2520achieve%2520jailbreaking%252C%2520the%2520J%255Cunderline%257B%255Ctextbf%257BA%257D%257Dilbreak%250A%255Cunderline%257B%255Ctextbf%257BV%257D%257Dia%2520%255Cunderline%257B%255Ctextbf%257BA%257D%257Ddversarial%250AMe%255Cunderline%257B%255Ctextbf%257BTA%257D%257D%2520-pho%255Cunderline%257B%255Ctextbf%257BR%257D%257D%2520%2528%255Ctextit%257BAVATAR%257D%2529.%250ASpecifically%252C%2520to%2520elicit%2520the%2520harmful%2520response%252C%2520AVATAR%2520extracts%2520harmful%2520entities%250Afrom%2520a%2520given%2520harmful%2520target%2520and%2520maps%2520them%2520to%2520innocuous%2520adversarial%2520entities%250Abased%2520on%2520LLM%2527s%2520imagination.%2520Then%252C%2520according%2520to%2520these%2520metaphors%252C%2520the%2520harmful%250Atarget%2520is%2520nested%2520within%2520human-like%2520interaction%2520for%2520jailbreaking%2520adaptively.%250AExperimental%2520results%2520demonstrate%2520that%2520AVATAR%2520can%2520effectively%2520and%2520transferablly%250Ajailbreak%2520LLMs%2520and%2520achieve%2520a%2520state-of-the-art%2520attack%2520success%2520rate%2520across%250Amultiple%2520advanced%2520LLMs.%2520Our%2520study%2520exposes%2520a%2520security%2520risk%2520in%2520LLMs%2520from%2520their%250Aendogenous%2520imaginative%2520capabilities.%2520Furthermore%252C%2520the%2520analytical%2520study%2520reveals%250Athe%2520vulnerability%2520of%2520LLM%2520to%2520adversarial%2520metaphors%2520and%2520the%2520necessity%2520of%250Adeveloping%2520defense%2520methods%2520against%2520jailbreaking%2520caused%2520by%2520the%2520adversarial%250Ametaphor.%2520%255Ctextcolor%257Borange%257D%257B%2520%255Ctextbf%257BWarning%253A%2520This%2520paper%2520contains%2520potentially%250Aharmful%2520content%2520from%2520LLMs.%257D%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12145v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Na%27vi%20or%20Knave%3A%20Jailbreaking%20Language%20Models%20via%20Metaphorical%20Avatars&entry.906535625=Yu%20Yan%20and%20Sheng%20Sun%20and%20Junqi%20Tong%20and%20Min%20Liu%20and%20Qi%20Li&entry.1292438233=%20%20Metaphor%20serves%20as%20an%20implicit%20approach%20to%20convey%20information%2C%20while%20enabling%0Athe%20generalized%20comprehension%20of%20complex%20subjects.%20However%2C%20metaphor%20can%0Apotentially%20be%20exploited%20to%20bypass%20the%20safety%20alignment%20mechanisms%20of%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20leading%20to%20the%20theft%20of%20harmful%20knowledge.%20In%20our%0Astudy%2C%20we%20introduce%20a%20novel%20attack%20framework%20that%20exploits%20the%20imaginative%0Acapacity%20of%20LLMs%20to%20achieve%20jailbreaking%2C%20the%20J%5Cunderline%7B%5Ctextbf%7BA%7D%7Dilbreak%0A%5Cunderline%7B%5Ctextbf%7BV%7D%7Dia%20%5Cunderline%7B%5Ctextbf%7BA%7D%7Ddversarial%0AMe%5Cunderline%7B%5Ctextbf%7BTA%7D%7D%20-pho%5Cunderline%7B%5Ctextbf%7BR%7D%7D%20%28%5Ctextit%7BAVATAR%7D%29.%0ASpecifically%2C%20to%20elicit%20the%20harmful%20response%2C%20AVATAR%20extracts%20harmful%20entities%0Afrom%20a%20given%20harmful%20target%20and%20maps%20them%20to%20innocuous%20adversarial%20entities%0Abased%20on%20LLM%27s%20imagination.%20Then%2C%20according%20to%20these%20metaphors%2C%20the%20harmful%0Atarget%20is%20nested%20within%20human-like%20interaction%20for%20jailbreaking%20adaptively.%0AExperimental%20results%20demonstrate%20that%20AVATAR%20can%20effectively%20and%20transferablly%0Ajailbreak%20LLMs%20and%20achieve%20a%20state-of-the-art%20attack%20success%20rate%20across%0Amultiple%20advanced%20LLMs.%20Our%20study%20exposes%20a%20security%20risk%20in%20LLMs%20from%20their%0Aendogenous%20imaginative%20capabilities.%20Furthermore%2C%20the%20analytical%20study%20reveals%0Athe%20vulnerability%20of%20LLM%20to%20adversarial%20metaphors%20and%20the%20necessity%20of%0Adeveloping%20defense%20methods%20against%20jailbreaking%20caused%20by%20the%20adversarial%0Ametaphor.%20%5Ctextcolor%7Borange%7D%7B%20%5Ctextbf%7BWarning%3A%20This%20paper%20contains%20potentially%0Aharmful%20content%20from%20LLMs.%7D%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12145v3&entry.124074799=Read"},
{"title": "pySLAM: An Open-Source, Modular, and Extensible Framework for SLAM", "author": "Luigi Freda", "abstract": "  pySLAM is an open-source Python framework for Visual SLAM, supporting\nmonocular, stereo, and RGB-D cameras. It provides a flexible interface for\nintegrating both classical and modern local features, making it adaptable to\nvarious SLAM tasks. The framework includes different loop closure methods, a\nvolumetric reconstruction pipeline, and support for depth prediction models.\nAdditionally, it offers a suite of tools for visual odometry and SLAM\napplications. Designed for both beginners and experienced researchers, pySLAM\nencourages community contributions, fostering collaborative development in the\nfield of Visual SLAM.\n", "link": "http://arxiv.org/abs/2502.11955v2", "date": "2025-02-19", "relevancy": 2.2457, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5989}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5453}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20pySLAM%3A%20An%20Open-Source%2C%20Modular%2C%20and%20Extensible%20Framework%20for%20SLAM&body=Title%3A%20pySLAM%3A%20An%20Open-Source%2C%20Modular%2C%20and%20Extensible%20Framework%20for%20SLAM%0AAuthor%3A%20Luigi%20Freda%0AAbstract%3A%20%20%20pySLAM%20is%20an%20open-source%20Python%20framework%20for%20Visual%20SLAM%2C%20supporting%0Amonocular%2C%20stereo%2C%20and%20RGB-D%20cameras.%20It%20provides%20a%20flexible%20interface%20for%0Aintegrating%20both%20classical%20and%20modern%20local%20features%2C%20making%20it%20adaptable%20to%0Avarious%20SLAM%20tasks.%20The%20framework%20includes%20different%20loop%20closure%20methods%2C%20a%0Avolumetric%20reconstruction%20pipeline%2C%20and%20support%20for%20depth%20prediction%20models.%0AAdditionally%2C%20it%20offers%20a%20suite%20of%20tools%20for%20visual%20odometry%20and%20SLAM%0Aapplications.%20Designed%20for%20both%20beginners%20and%20experienced%20researchers%2C%20pySLAM%0Aencourages%20community%20contributions%2C%20fostering%20collaborative%20development%20in%20the%0Afield%20of%20Visual%20SLAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11955v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DpySLAM%253A%2520An%2520Open-Source%252C%2520Modular%252C%2520and%2520Extensible%2520Framework%2520for%2520SLAM%26entry.906535625%3DLuigi%2520Freda%26entry.1292438233%3D%2520%2520pySLAM%2520is%2520an%2520open-source%2520Python%2520framework%2520for%2520Visual%2520SLAM%252C%2520supporting%250Amonocular%252C%2520stereo%252C%2520and%2520RGB-D%2520cameras.%2520It%2520provides%2520a%2520flexible%2520interface%2520for%250Aintegrating%2520both%2520classical%2520and%2520modern%2520local%2520features%252C%2520making%2520it%2520adaptable%2520to%250Avarious%2520SLAM%2520tasks.%2520The%2520framework%2520includes%2520different%2520loop%2520closure%2520methods%252C%2520a%250Avolumetric%2520reconstruction%2520pipeline%252C%2520and%2520support%2520for%2520depth%2520prediction%2520models.%250AAdditionally%252C%2520it%2520offers%2520a%2520suite%2520of%2520tools%2520for%2520visual%2520odometry%2520and%2520SLAM%250Aapplications.%2520Designed%2520for%2520both%2520beginners%2520and%2520experienced%2520researchers%252C%2520pySLAM%250Aencourages%2520community%2520contributions%252C%2520fostering%2520collaborative%2520development%2520in%2520the%250Afield%2520of%2520Visual%2520SLAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11955v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=pySLAM%3A%20An%20Open-Source%2C%20Modular%2C%20and%20Extensible%20Framework%20for%20SLAM&entry.906535625=Luigi%20Freda&entry.1292438233=%20%20pySLAM%20is%20an%20open-source%20Python%20framework%20for%20Visual%20SLAM%2C%20supporting%0Amonocular%2C%20stereo%2C%20and%20RGB-D%20cameras.%20It%20provides%20a%20flexible%20interface%20for%0Aintegrating%20both%20classical%20and%20modern%20local%20features%2C%20making%20it%20adaptable%20to%0Avarious%20SLAM%20tasks.%20The%20framework%20includes%20different%20loop%20closure%20methods%2C%20a%0Avolumetric%20reconstruction%20pipeline%2C%20and%20support%20for%20depth%20prediction%20models.%0AAdditionally%2C%20it%20offers%20a%20suite%20of%20tools%20for%20visual%20odometry%20and%20SLAM%0Aapplications.%20Designed%20for%20both%20beginners%20and%20experienced%20researchers%2C%20pySLAM%0Aencourages%20community%20contributions%2C%20fostering%20collaborative%20development%20in%20the%0Afield%20of%20Visual%20SLAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11955v2&entry.124074799=Read"},
{"title": "Emergence of the Primacy Effect in Structured State-Space Models", "author": "Takashi Morita", "abstract": "  Human and animal memory for sequentially presented items is well-documented\nto be more accurate for those at the beginning and end of a sequence, phenomena\nknown as the primacy and recency effects, respectively. By contrast, artificial\nneural network (ANN) models are typically designed with a memory that decays\nmonotonically over time. Accordingly, ANNs are expected to show the recency\neffect but not the primacy effect. Contrary to this theoretical expectation,\nhowever, the present study reveals a counterintuitive finding: a recently\ndeveloped ANN architecture, called structured state-space models, exhibits the\nprimacy effect when trained and evaluated on a synthetic task that mirrors\npsychological memory experiments. Given that this model was originally designed\nfor recovering neuronal activity patterns observed in biological brains, this\nresult provides a novel perspective on the psychological primacy effect while\nalso posing a non-trivial puzzle for the current theories in machine learning.\n", "link": "http://arxiv.org/abs/2502.13729v1", "date": "2025-02-19", "relevancy": 2.2441, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4555}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergence%20of%20the%20Primacy%20Effect%20in%20Structured%20State-Space%20Models&body=Title%3A%20Emergence%20of%20the%20Primacy%20Effect%20in%20Structured%20State-Space%20Models%0AAuthor%3A%20Takashi%20Morita%0AAbstract%3A%20%20%20Human%20and%20animal%20memory%20for%20sequentially%20presented%20items%20is%20well-documented%0Ato%20be%20more%20accurate%20for%20those%20at%20the%20beginning%20and%20end%20of%20a%20sequence%2C%20phenomena%0Aknown%20as%20the%20primacy%20and%20recency%20effects%2C%20respectively.%20By%20contrast%2C%20artificial%0Aneural%20network%20%28ANN%29%20models%20are%20typically%20designed%20with%20a%20memory%20that%20decays%0Amonotonically%20over%20time.%20Accordingly%2C%20ANNs%20are%20expected%20to%20show%20the%20recency%0Aeffect%20but%20not%20the%20primacy%20effect.%20Contrary%20to%20this%20theoretical%20expectation%2C%0Ahowever%2C%20the%20present%20study%20reveals%20a%20counterintuitive%20finding%3A%20a%20recently%0Adeveloped%20ANN%20architecture%2C%20called%20structured%20state-space%20models%2C%20exhibits%20the%0Aprimacy%20effect%20when%20trained%20and%20evaluated%20on%20a%20synthetic%20task%20that%20mirrors%0Apsychological%20memory%20experiments.%20Given%20that%20this%20model%20was%20originally%20designed%0Afor%20recovering%20neuronal%20activity%20patterns%20observed%20in%20biological%20brains%2C%20this%0Aresult%20provides%20a%20novel%20perspective%20on%20the%20psychological%20primacy%20effect%20while%0Aalso%20posing%20a%20non-trivial%20puzzle%20for%20the%20current%20theories%20in%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13729v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergence%2520of%2520the%2520Primacy%2520Effect%2520in%2520Structured%2520State-Space%2520Models%26entry.906535625%3DTakashi%2520Morita%26entry.1292438233%3D%2520%2520Human%2520and%2520animal%2520memory%2520for%2520sequentially%2520presented%2520items%2520is%2520well-documented%250Ato%2520be%2520more%2520accurate%2520for%2520those%2520at%2520the%2520beginning%2520and%2520end%2520of%2520a%2520sequence%252C%2520phenomena%250Aknown%2520as%2520the%2520primacy%2520and%2520recency%2520effects%252C%2520respectively.%2520By%2520contrast%252C%2520artificial%250Aneural%2520network%2520%2528ANN%2529%2520models%2520are%2520typically%2520designed%2520with%2520a%2520memory%2520that%2520decays%250Amonotonically%2520over%2520time.%2520Accordingly%252C%2520ANNs%2520are%2520expected%2520to%2520show%2520the%2520recency%250Aeffect%2520but%2520not%2520the%2520primacy%2520effect.%2520Contrary%2520to%2520this%2520theoretical%2520expectation%252C%250Ahowever%252C%2520the%2520present%2520study%2520reveals%2520a%2520counterintuitive%2520finding%253A%2520a%2520recently%250Adeveloped%2520ANN%2520architecture%252C%2520called%2520structured%2520state-space%2520models%252C%2520exhibits%2520the%250Aprimacy%2520effect%2520when%2520trained%2520and%2520evaluated%2520on%2520a%2520synthetic%2520task%2520that%2520mirrors%250Apsychological%2520memory%2520experiments.%2520Given%2520that%2520this%2520model%2520was%2520originally%2520designed%250Afor%2520recovering%2520neuronal%2520activity%2520patterns%2520observed%2520in%2520biological%2520brains%252C%2520this%250Aresult%2520provides%2520a%2520novel%2520perspective%2520on%2520the%2520psychological%2520primacy%2520effect%2520while%250Aalso%2520posing%2520a%2520non-trivial%2520puzzle%2520for%2520the%2520current%2520theories%2520in%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13729v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergence%20of%20the%20Primacy%20Effect%20in%20Structured%20State-Space%20Models&entry.906535625=Takashi%20Morita&entry.1292438233=%20%20Human%20and%20animal%20memory%20for%20sequentially%20presented%20items%20is%20well-documented%0Ato%20be%20more%20accurate%20for%20those%20at%20the%20beginning%20and%20end%20of%20a%20sequence%2C%20phenomena%0Aknown%20as%20the%20primacy%20and%20recency%20effects%2C%20respectively.%20By%20contrast%2C%20artificial%0Aneural%20network%20%28ANN%29%20models%20are%20typically%20designed%20with%20a%20memory%20that%20decays%0Amonotonically%20over%20time.%20Accordingly%2C%20ANNs%20are%20expected%20to%20show%20the%20recency%0Aeffect%20but%20not%20the%20primacy%20effect.%20Contrary%20to%20this%20theoretical%20expectation%2C%0Ahowever%2C%20the%20present%20study%20reveals%20a%20counterintuitive%20finding%3A%20a%20recently%0Adeveloped%20ANN%20architecture%2C%20called%20structured%20state-space%20models%2C%20exhibits%20the%0Aprimacy%20effect%20when%20trained%20and%20evaluated%20on%20a%20synthetic%20task%20that%20mirrors%0Apsychological%20memory%20experiments.%20Given%20that%20this%20model%20was%20originally%20designed%0Afor%20recovering%20neuronal%20activity%20patterns%20observed%20in%20biological%20brains%2C%20this%0Aresult%20provides%20a%20novel%20perspective%20on%20the%20psychological%20primacy%20effect%20while%0Aalso%20posing%20a%20non-trivial%20puzzle%20for%20the%20current%20theories%20in%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13729v1&entry.124074799=Read"},
{"title": "Infinite Width Limits of Self Supervised Neural Networks", "author": "Maximilian Fleissner and Gautham Govind Anil and Debarghya Ghoshdastidar", "abstract": "  The NTK is a widely used tool in the theoretical analysis of deep learning,\nallowing us to look at supervised deep neural networks through the lenses of\nkernel regression. Recently, several works have investigated kernel models for\nself-supervised learning, hypothesizing that these also shed light on the\nbehavior of wide neural networks by virtue of the NTK. However, it remains an\nopen question to what extent this connection is mathematically sound -- it is a\ncommonly encountered misbelief that the kernel behavior of wide neural networks\nemerges irrespective of the loss function it is trained on. In this paper, we\nbridge the gap between the NTK and self-supervised learning, focusing on\ntwo-layer neural networks trained under the Barlow Twins loss. We prove that\nthe NTK of Barlow Twins indeed becomes constant as the width of the network\napproaches infinity. Our analysis technique is a bit different from previous\nworks on the NTK and may be of independent interest. Overall, our work provides\na first justification for the use of classic kernel theory to understand\nself-supervised learning of wide neural networks. Building on this result, we\nderive generalization error bounds for kernelized Barlow Twins and connect them\nto neural networks of finite width.\n", "link": "http://arxiv.org/abs/2411.11176v2", "date": "2025-02-19", "relevancy": 2.2335, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4618}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4488}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Infinite%20Width%20Limits%20of%20Self%20Supervised%20Neural%20Networks&body=Title%3A%20Infinite%20Width%20Limits%20of%20Self%20Supervised%20Neural%20Networks%0AAuthor%3A%20Maximilian%20Fleissner%20and%20Gautham%20Govind%20Anil%20and%20Debarghya%20Ghoshdastidar%0AAbstract%3A%20%20%20The%20NTK%20is%20a%20widely%20used%20tool%20in%20the%20theoretical%20analysis%20of%20deep%20learning%2C%0Aallowing%20us%20to%20look%20at%20supervised%20deep%20neural%20networks%20through%20the%20lenses%20of%0Akernel%20regression.%20Recently%2C%20several%20works%20have%20investigated%20kernel%20models%20for%0Aself-supervised%20learning%2C%20hypothesizing%20that%20these%20also%20shed%20light%20on%20the%0Abehavior%20of%20wide%20neural%20networks%20by%20virtue%20of%20the%20NTK.%20However%2C%20it%20remains%20an%0Aopen%20question%20to%20what%20extent%20this%20connection%20is%20mathematically%20sound%20--%20it%20is%20a%0Acommonly%20encountered%20misbelief%20that%20the%20kernel%20behavior%20of%20wide%20neural%20networks%0Aemerges%20irrespective%20of%20the%20loss%20function%20it%20is%20trained%20on.%20In%20this%20paper%2C%20we%0Abridge%20the%20gap%20between%20the%20NTK%20and%20self-supervised%20learning%2C%20focusing%20on%0Atwo-layer%20neural%20networks%20trained%20under%20the%20Barlow%20Twins%20loss.%20We%20prove%20that%0Athe%20NTK%20of%20Barlow%20Twins%20indeed%20becomes%20constant%20as%20the%20width%20of%20the%20network%0Aapproaches%20infinity.%20Our%20analysis%20technique%20is%20a%20bit%20different%20from%20previous%0Aworks%20on%20the%20NTK%20and%20may%20be%20of%20independent%20interest.%20Overall%2C%20our%20work%20provides%0Aa%20first%20justification%20for%20the%20use%20of%20classic%20kernel%20theory%20to%20understand%0Aself-supervised%20learning%20of%20wide%20neural%20networks.%20Building%20on%20this%20result%2C%20we%0Aderive%20generalization%20error%20bounds%20for%20kernelized%20Barlow%20Twins%20and%20connect%20them%0Ato%20neural%20networks%20of%20finite%20width.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11176v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfinite%2520Width%2520Limits%2520of%2520Self%2520Supervised%2520Neural%2520Networks%26entry.906535625%3DMaximilian%2520Fleissner%2520and%2520Gautham%2520Govind%2520Anil%2520and%2520Debarghya%2520Ghoshdastidar%26entry.1292438233%3D%2520%2520The%2520NTK%2520is%2520a%2520widely%2520used%2520tool%2520in%2520the%2520theoretical%2520analysis%2520of%2520deep%2520learning%252C%250Aallowing%2520us%2520to%2520look%2520at%2520supervised%2520deep%2520neural%2520networks%2520through%2520the%2520lenses%2520of%250Akernel%2520regression.%2520Recently%252C%2520several%2520works%2520have%2520investigated%2520kernel%2520models%2520for%250Aself-supervised%2520learning%252C%2520hypothesizing%2520that%2520these%2520also%2520shed%2520light%2520on%2520the%250Abehavior%2520of%2520wide%2520neural%2520networks%2520by%2520virtue%2520of%2520the%2520NTK.%2520However%252C%2520it%2520remains%2520an%250Aopen%2520question%2520to%2520what%2520extent%2520this%2520connection%2520is%2520mathematically%2520sound%2520--%2520it%2520is%2520a%250Acommonly%2520encountered%2520misbelief%2520that%2520the%2520kernel%2520behavior%2520of%2520wide%2520neural%2520networks%250Aemerges%2520irrespective%2520of%2520the%2520loss%2520function%2520it%2520is%2520trained%2520on.%2520In%2520this%2520paper%252C%2520we%250Abridge%2520the%2520gap%2520between%2520the%2520NTK%2520and%2520self-supervised%2520learning%252C%2520focusing%2520on%250Atwo-layer%2520neural%2520networks%2520trained%2520under%2520the%2520Barlow%2520Twins%2520loss.%2520We%2520prove%2520that%250Athe%2520NTK%2520of%2520Barlow%2520Twins%2520indeed%2520becomes%2520constant%2520as%2520the%2520width%2520of%2520the%2520network%250Aapproaches%2520infinity.%2520Our%2520analysis%2520technique%2520is%2520a%2520bit%2520different%2520from%2520previous%250Aworks%2520on%2520the%2520NTK%2520and%2520may%2520be%2520of%2520independent%2520interest.%2520Overall%252C%2520our%2520work%2520provides%250Aa%2520first%2520justification%2520for%2520the%2520use%2520of%2520classic%2520kernel%2520theory%2520to%2520understand%250Aself-supervised%2520learning%2520of%2520wide%2520neural%2520networks.%2520Building%2520on%2520this%2520result%252C%2520we%250Aderive%2520generalization%2520error%2520bounds%2520for%2520kernelized%2520Barlow%2520Twins%2520and%2520connect%2520them%250Ato%2520neural%2520networks%2520of%2520finite%2520width.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11176v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infinite%20Width%20Limits%20of%20Self%20Supervised%20Neural%20Networks&entry.906535625=Maximilian%20Fleissner%20and%20Gautham%20Govind%20Anil%20and%20Debarghya%20Ghoshdastidar&entry.1292438233=%20%20The%20NTK%20is%20a%20widely%20used%20tool%20in%20the%20theoretical%20analysis%20of%20deep%20learning%2C%0Aallowing%20us%20to%20look%20at%20supervised%20deep%20neural%20networks%20through%20the%20lenses%20of%0Akernel%20regression.%20Recently%2C%20several%20works%20have%20investigated%20kernel%20models%20for%0Aself-supervised%20learning%2C%20hypothesizing%20that%20these%20also%20shed%20light%20on%20the%0Abehavior%20of%20wide%20neural%20networks%20by%20virtue%20of%20the%20NTK.%20However%2C%20it%20remains%20an%0Aopen%20question%20to%20what%20extent%20this%20connection%20is%20mathematically%20sound%20--%20it%20is%20a%0Acommonly%20encountered%20misbelief%20that%20the%20kernel%20behavior%20of%20wide%20neural%20networks%0Aemerges%20irrespective%20of%20the%20loss%20function%20it%20is%20trained%20on.%20In%20this%20paper%2C%20we%0Abridge%20the%20gap%20between%20the%20NTK%20and%20self-supervised%20learning%2C%20focusing%20on%0Atwo-layer%20neural%20networks%20trained%20under%20the%20Barlow%20Twins%20loss.%20We%20prove%20that%0Athe%20NTK%20of%20Barlow%20Twins%20indeed%20becomes%20constant%20as%20the%20width%20of%20the%20network%0Aapproaches%20infinity.%20Our%20analysis%20technique%20is%20a%20bit%20different%20from%20previous%0Aworks%20on%20the%20NTK%20and%20may%20be%20of%20independent%20interest.%20Overall%2C%20our%20work%20provides%0Aa%20first%20justification%20for%20the%20use%20of%20classic%20kernel%20theory%20to%20understand%0Aself-supervised%20learning%20of%20wide%20neural%20networks.%20Building%20on%20this%20result%2C%20we%0Aderive%20generalization%20error%20bounds%20for%20kernelized%20Barlow%20Twins%20and%20connect%20them%0Ato%20neural%20networks%20of%20finite%20width.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11176v2&entry.124074799=Read"},
{"title": "Generalization error bound for denoising score matching under relaxed\n  manifold assumption", "author": "Konstantin Yakovlev and Nikita Puchkin", "abstract": "  We examine theoretical properties of the denoising score matching estimate.\nWe model the density of observations with a nonparametric Gaussian mixture. We\nsignificantly relax the standard manifold assumption allowing the samples step\naway from the manifold. At the same time, we are still able to leverage a nice\ndistribution structure. We derive non-asymptotic bounds on the approximation\nand generalization errors of the denoising score matching estimate. The rates\nof convergence are determined by the intrinsic dimension. Furthermore, our\nbounds remain valid even if we allow the ambient dimension grow polynomially\nwith the sample size.\n", "link": "http://arxiv.org/abs/2502.13662v1", "date": "2025-02-19", "relevancy": 2.2201, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4549}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4463}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalization%20error%20bound%20for%20denoising%20score%20matching%20under%20relaxed%0A%20%20manifold%20assumption&body=Title%3A%20Generalization%20error%20bound%20for%20denoising%20score%20matching%20under%20relaxed%0A%20%20manifold%20assumption%0AAuthor%3A%20Konstantin%20Yakovlev%20and%20Nikita%20Puchkin%0AAbstract%3A%20%20%20We%20examine%20theoretical%20properties%20of%20the%20denoising%20score%20matching%20estimate.%0AWe%20model%20the%20density%20of%20observations%20with%20a%20nonparametric%20Gaussian%20mixture.%20We%0Asignificantly%20relax%20the%20standard%20manifold%20assumption%20allowing%20the%20samples%20step%0Aaway%20from%20the%20manifold.%20At%20the%20same%20time%2C%20we%20are%20still%20able%20to%20leverage%20a%20nice%0Adistribution%20structure.%20We%20derive%20non-asymptotic%20bounds%20on%20the%20approximation%0Aand%20generalization%20errors%20of%20the%20denoising%20score%20matching%20estimate.%20The%20rates%0Aof%20convergence%20are%20determined%20by%20the%20intrinsic%20dimension.%20Furthermore%2C%20our%0Abounds%20remain%20valid%20even%20if%20we%20allow%20the%20ambient%20dimension%20grow%20polynomially%0Awith%20the%20sample%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralization%2520error%2520bound%2520for%2520denoising%2520score%2520matching%2520under%2520relaxed%250A%2520%2520manifold%2520assumption%26entry.906535625%3DKonstantin%2520Yakovlev%2520and%2520Nikita%2520Puchkin%26entry.1292438233%3D%2520%2520We%2520examine%2520theoretical%2520properties%2520of%2520the%2520denoising%2520score%2520matching%2520estimate.%250AWe%2520model%2520the%2520density%2520of%2520observations%2520with%2520a%2520nonparametric%2520Gaussian%2520mixture.%2520We%250Asignificantly%2520relax%2520the%2520standard%2520manifold%2520assumption%2520allowing%2520the%2520samples%2520step%250Aaway%2520from%2520the%2520manifold.%2520At%2520the%2520same%2520time%252C%2520we%2520are%2520still%2520able%2520to%2520leverage%2520a%2520nice%250Adistribution%2520structure.%2520We%2520derive%2520non-asymptotic%2520bounds%2520on%2520the%2520approximation%250Aand%2520generalization%2520errors%2520of%2520the%2520denoising%2520score%2520matching%2520estimate.%2520The%2520rates%250Aof%2520convergence%2520are%2520determined%2520by%2520the%2520intrinsic%2520dimension.%2520Furthermore%252C%2520our%250Abounds%2520remain%2520valid%2520even%2520if%2520we%2520allow%2520the%2520ambient%2520dimension%2520grow%2520polynomially%250Awith%2520the%2520sample%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20error%20bound%20for%20denoising%20score%20matching%20under%20relaxed%0A%20%20manifold%20assumption&entry.906535625=Konstantin%20Yakovlev%20and%20Nikita%20Puchkin&entry.1292438233=%20%20We%20examine%20theoretical%20properties%20of%20the%20denoising%20score%20matching%20estimate.%0AWe%20model%20the%20density%20of%20observations%20with%20a%20nonparametric%20Gaussian%20mixture.%20We%0Asignificantly%20relax%20the%20standard%20manifold%20assumption%20allowing%20the%20samples%20step%0Aaway%20from%20the%20manifold.%20At%20the%20same%20time%2C%20we%20are%20still%20able%20to%20leverage%20a%20nice%0Adistribution%20structure.%20We%20derive%20non-asymptotic%20bounds%20on%20the%20approximation%0Aand%20generalization%20errors%20of%20the%20denoising%20score%20matching%20estimate.%20The%20rates%0Aof%20convergence%20are%20determined%20by%20the%20intrinsic%20dimension.%20Furthermore%2C%20our%0Abounds%20remain%20valid%20even%20if%20we%20allow%20the%20ambient%20dimension%20grow%20polynomially%0Awith%20the%20sample%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13662v1&entry.124074799=Read"},
{"title": "Learning to explore when mistakes are not allowed", "author": "Charly Pecqueux-Gu\u00e9z\u00e9nec and St\u00e9phane Doncieux and Nicolas Perrin-Gilbert", "abstract": "  Goal-Conditioned Reinforcement Learning (GCRL) provides a versatile framework\nfor developing unified controllers capable of handling wide ranges of tasks,\nexploring environments, and adapting behaviors. However, its reliance on\ntrial-and-error poses challenges for real-world applications, as errors can\nresult in costly and potentially damaging consequences. To address the need for\nsafer learning, we propose a method that enables agents to learn\ngoal-conditioned behaviors that explore without the risk of making harmful\nmistakes. Exploration without risks can seem paradoxical, but environment\ndynamics are often uniform in space, therefore a policy trained for safety\nwithout exploration purposes can still be exploited globally. Our proposed\napproach involves two distinct phases. First, during a pretraining phase, we\nemploy safe reinforcement learning and distributional techniques to train a\nsafety policy that actively tries to avoid failures in various situations. In\nthe subsequent safe exploration phase, a goal-conditioned (GC) policy is\nlearned while ensuring safety. To achieve this, we implement an\naction-selection mechanism leveraging the previously learned distributional\nsafety critics to arbitrate between the safety policy and the GC policy,\nensuring safe exploration by switching to the safety policy when needed. We\nevaluate our method in simulated environments and demonstrate that it not only\nprovides substantial coverage of the goal space but also reduces the occurrence\nof mistakes to a minimum, in stark contrast to traditional GCRL approaches.\nAdditionally, we conduct an ablation study and analyze failure modes, offering\ninsights for future research directions.\n", "link": "http://arxiv.org/abs/2502.13801v1", "date": "2025-02-19", "relevancy": 2.218, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6141}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5639}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20explore%20when%20mistakes%20are%20not%20allowed&body=Title%3A%20Learning%20to%20explore%20when%20mistakes%20are%20not%20allowed%0AAuthor%3A%20Charly%20Pecqueux-Gu%C3%A9z%C3%A9nec%20and%20St%C3%A9phane%20Doncieux%20and%20Nicolas%20Perrin-Gilbert%0AAbstract%3A%20%20%20Goal-Conditioned%20Reinforcement%20Learning%20%28GCRL%29%20provides%20a%20versatile%20framework%0Afor%20developing%20unified%20controllers%20capable%20of%20handling%20wide%20ranges%20of%20tasks%2C%0Aexploring%20environments%2C%20and%20adapting%20behaviors.%20However%2C%20its%20reliance%20on%0Atrial-and-error%20poses%20challenges%20for%20real-world%20applications%2C%20as%20errors%20can%0Aresult%20in%20costly%20and%20potentially%20damaging%20consequences.%20To%20address%20the%20need%20for%0Asafer%20learning%2C%20we%20propose%20a%20method%20that%20enables%20agents%20to%20learn%0Agoal-conditioned%20behaviors%20that%20explore%20without%20the%20risk%20of%20making%20harmful%0Amistakes.%20Exploration%20without%20risks%20can%20seem%20paradoxical%2C%20but%20environment%0Adynamics%20are%20often%20uniform%20in%20space%2C%20therefore%20a%20policy%20trained%20for%20safety%0Awithout%20exploration%20purposes%20can%20still%20be%20exploited%20globally.%20Our%20proposed%0Aapproach%20involves%20two%20distinct%20phases.%20First%2C%20during%20a%20pretraining%20phase%2C%20we%0Aemploy%20safe%20reinforcement%20learning%20and%20distributional%20techniques%20to%20train%20a%0Asafety%20policy%20that%20actively%20tries%20to%20avoid%20failures%20in%20various%20situations.%20In%0Athe%20subsequent%20safe%20exploration%20phase%2C%20a%20goal-conditioned%20%28GC%29%20policy%20is%0Alearned%20while%20ensuring%20safety.%20To%20achieve%20this%2C%20we%20implement%20an%0Aaction-selection%20mechanism%20leveraging%20the%20previously%20learned%20distributional%0Asafety%20critics%20to%20arbitrate%20between%20the%20safety%20policy%20and%20the%20GC%20policy%2C%0Aensuring%20safe%20exploration%20by%20switching%20to%20the%20safety%20policy%20when%20needed.%20We%0Aevaluate%20our%20method%20in%20simulated%20environments%20and%20demonstrate%20that%20it%20not%20only%0Aprovides%20substantial%20coverage%20of%20the%20goal%20space%20but%20also%20reduces%20the%20occurrence%0Aof%20mistakes%20to%20a%20minimum%2C%20in%20stark%20contrast%20to%20traditional%20GCRL%20approaches.%0AAdditionally%2C%20we%20conduct%20an%20ablation%20study%20and%20analyze%20failure%20modes%2C%20offering%0Ainsights%20for%20future%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520explore%2520when%2520mistakes%2520are%2520not%2520allowed%26entry.906535625%3DCharly%2520Pecqueux-Gu%25C3%25A9z%25C3%25A9nec%2520and%2520St%25C3%25A9phane%2520Doncieux%2520and%2520Nicolas%2520Perrin-Gilbert%26entry.1292438233%3D%2520%2520Goal-Conditioned%2520Reinforcement%2520Learning%2520%2528GCRL%2529%2520provides%2520a%2520versatile%2520framework%250Afor%2520developing%2520unified%2520controllers%2520capable%2520of%2520handling%2520wide%2520ranges%2520of%2520tasks%252C%250Aexploring%2520environments%252C%2520and%2520adapting%2520behaviors.%2520However%252C%2520its%2520reliance%2520on%250Atrial-and-error%2520poses%2520challenges%2520for%2520real-world%2520applications%252C%2520as%2520errors%2520can%250Aresult%2520in%2520costly%2520and%2520potentially%2520damaging%2520consequences.%2520To%2520address%2520the%2520need%2520for%250Asafer%2520learning%252C%2520we%2520propose%2520a%2520method%2520that%2520enables%2520agents%2520to%2520learn%250Agoal-conditioned%2520behaviors%2520that%2520explore%2520without%2520the%2520risk%2520of%2520making%2520harmful%250Amistakes.%2520Exploration%2520without%2520risks%2520can%2520seem%2520paradoxical%252C%2520but%2520environment%250Adynamics%2520are%2520often%2520uniform%2520in%2520space%252C%2520therefore%2520a%2520policy%2520trained%2520for%2520safety%250Awithout%2520exploration%2520purposes%2520can%2520still%2520be%2520exploited%2520globally.%2520Our%2520proposed%250Aapproach%2520involves%2520two%2520distinct%2520phases.%2520First%252C%2520during%2520a%2520pretraining%2520phase%252C%2520we%250Aemploy%2520safe%2520reinforcement%2520learning%2520and%2520distributional%2520techniques%2520to%2520train%2520a%250Asafety%2520policy%2520that%2520actively%2520tries%2520to%2520avoid%2520failures%2520in%2520various%2520situations.%2520In%250Athe%2520subsequent%2520safe%2520exploration%2520phase%252C%2520a%2520goal-conditioned%2520%2528GC%2529%2520policy%2520is%250Alearned%2520while%2520ensuring%2520safety.%2520To%2520achieve%2520this%252C%2520we%2520implement%2520an%250Aaction-selection%2520mechanism%2520leveraging%2520the%2520previously%2520learned%2520distributional%250Asafety%2520critics%2520to%2520arbitrate%2520between%2520the%2520safety%2520policy%2520and%2520the%2520GC%2520policy%252C%250Aensuring%2520safe%2520exploration%2520by%2520switching%2520to%2520the%2520safety%2520policy%2520when%2520needed.%2520We%250Aevaluate%2520our%2520method%2520in%2520simulated%2520environments%2520and%2520demonstrate%2520that%2520it%2520not%2520only%250Aprovides%2520substantial%2520coverage%2520of%2520the%2520goal%2520space%2520but%2520also%2520reduces%2520the%2520occurrence%250Aof%2520mistakes%2520to%2520a%2520minimum%252C%2520in%2520stark%2520contrast%2520to%2520traditional%2520GCRL%2520approaches.%250AAdditionally%252C%2520we%2520conduct%2520an%2520ablation%2520study%2520and%2520analyze%2520failure%2520modes%252C%2520offering%250Ainsights%2520for%2520future%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20explore%20when%20mistakes%20are%20not%20allowed&entry.906535625=Charly%20Pecqueux-Gu%C3%A9z%C3%A9nec%20and%20St%C3%A9phane%20Doncieux%20and%20Nicolas%20Perrin-Gilbert&entry.1292438233=%20%20Goal-Conditioned%20Reinforcement%20Learning%20%28GCRL%29%20provides%20a%20versatile%20framework%0Afor%20developing%20unified%20controllers%20capable%20of%20handling%20wide%20ranges%20of%20tasks%2C%0Aexploring%20environments%2C%20and%20adapting%20behaviors.%20However%2C%20its%20reliance%20on%0Atrial-and-error%20poses%20challenges%20for%20real-world%20applications%2C%20as%20errors%20can%0Aresult%20in%20costly%20and%20potentially%20damaging%20consequences.%20To%20address%20the%20need%20for%0Asafer%20learning%2C%20we%20propose%20a%20method%20that%20enables%20agents%20to%20learn%0Agoal-conditioned%20behaviors%20that%20explore%20without%20the%20risk%20of%20making%20harmful%0Amistakes.%20Exploration%20without%20risks%20can%20seem%20paradoxical%2C%20but%20environment%0Adynamics%20are%20often%20uniform%20in%20space%2C%20therefore%20a%20policy%20trained%20for%20safety%0Awithout%20exploration%20purposes%20can%20still%20be%20exploited%20globally.%20Our%20proposed%0Aapproach%20involves%20two%20distinct%20phases.%20First%2C%20during%20a%20pretraining%20phase%2C%20we%0Aemploy%20safe%20reinforcement%20learning%20and%20distributional%20techniques%20to%20train%20a%0Asafety%20policy%20that%20actively%20tries%20to%20avoid%20failures%20in%20various%20situations.%20In%0Athe%20subsequent%20safe%20exploration%20phase%2C%20a%20goal-conditioned%20%28GC%29%20policy%20is%0Alearned%20while%20ensuring%20safety.%20To%20achieve%20this%2C%20we%20implement%20an%0Aaction-selection%20mechanism%20leveraging%20the%20previously%20learned%20distributional%0Asafety%20critics%20to%20arbitrate%20between%20the%20safety%20policy%20and%20the%20GC%20policy%2C%0Aensuring%20safe%20exploration%20by%20switching%20to%20the%20safety%20policy%20when%20needed.%20We%0Aevaluate%20our%20method%20in%20simulated%20environments%20and%20demonstrate%20that%20it%20not%20only%0Aprovides%20substantial%20coverage%20of%20the%20goal%20space%20but%20also%20reduces%20the%20occurrence%0Aof%20mistakes%20to%20a%20minimum%2C%20in%20stark%20contrast%20to%20traditional%20GCRL%20approaches.%0AAdditionally%2C%20we%20conduct%20an%20ablation%20study%20and%20analyze%20failure%20modes%2C%20offering%0Ainsights%20for%20future%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13801v1&entry.124074799=Read"},
{"title": "IP-Composer: Semantic Composition of Visual Concepts", "author": "Sara Dorfman and Dana Cohen-Bar and Rinon Gal and Daniel Cohen-Or", "abstract": "  Content creators often draw inspiration from multiple visual sources,\ncombining distinct elements to craft new compositions. Modern computational\napproaches now aim to emulate this fundamental creative process. Although\nrecent diffusion models excel at text-guided compositional synthesis, text as a\nmedium often lacks precise control over visual details. Image-based composition\napproaches can capture more nuanced features, but existing methods are\ntypically limited in the range of concepts they can capture, and require\nexpensive training procedures or specialized data. We present IP-Composer, a\nnovel training-free approach for compositional image generation that leverages\nmultiple image references simultaneously, while using natural language to\ndescribe the concept to be extracted from each image. Our method builds on\nIP-Adapter, which synthesizes novel images conditioned on an input image's CLIP\nembedding. We extend this approach to multiple visual inputs by crafting\ncomposite embeddings, stitched from the projections of multiple input images\nonto concept-specific CLIP-subspaces identified through text. Through\ncomprehensive evaluation, we show that our approach enables more precise\ncontrol over a larger range of visual concept compositions.\n", "link": "http://arxiv.org/abs/2502.13951v1", "date": "2025-02-19", "relevancy": 2.2149, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6256}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5394}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IP-Composer%3A%20Semantic%20Composition%20of%20Visual%20Concepts&body=Title%3A%20IP-Composer%3A%20Semantic%20Composition%20of%20Visual%20Concepts%0AAuthor%3A%20Sara%20Dorfman%20and%20Dana%20Cohen-Bar%20and%20Rinon%20Gal%20and%20Daniel%20Cohen-Or%0AAbstract%3A%20%20%20Content%20creators%20often%20draw%20inspiration%20from%20multiple%20visual%20sources%2C%0Acombining%20distinct%20elements%20to%20craft%20new%20compositions.%20Modern%20computational%0Aapproaches%20now%20aim%20to%20emulate%20this%20fundamental%20creative%20process.%20Although%0Arecent%20diffusion%20models%20excel%20at%20text-guided%20compositional%20synthesis%2C%20text%20as%20a%0Amedium%20often%20lacks%20precise%20control%20over%20visual%20details.%20Image-based%20composition%0Aapproaches%20can%20capture%20more%20nuanced%20features%2C%20but%20existing%20methods%20are%0Atypically%20limited%20in%20the%20range%20of%20concepts%20they%20can%20capture%2C%20and%20require%0Aexpensive%20training%20procedures%20or%20specialized%20data.%20We%20present%20IP-Composer%2C%20a%0Anovel%20training-free%20approach%20for%20compositional%20image%20generation%20that%20leverages%0Amultiple%20image%20references%20simultaneously%2C%20while%20using%20natural%20language%20to%0Adescribe%20the%20concept%20to%20be%20extracted%20from%20each%20image.%20Our%20method%20builds%20on%0AIP-Adapter%2C%20which%20synthesizes%20novel%20images%20conditioned%20on%20an%20input%20image%27s%20CLIP%0Aembedding.%20We%20extend%20this%20approach%20to%20multiple%20visual%20inputs%20by%20crafting%0Acomposite%20embeddings%2C%20stitched%20from%20the%20projections%20of%20multiple%20input%20images%0Aonto%20concept-specific%20CLIP-subspaces%20identified%20through%20text.%20Through%0Acomprehensive%20evaluation%2C%20we%20show%20that%20our%20approach%20enables%20more%20precise%0Acontrol%20over%20a%20larger%20range%20of%20visual%20concept%20compositions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIP-Composer%253A%2520Semantic%2520Composition%2520of%2520Visual%2520Concepts%26entry.906535625%3DSara%2520Dorfman%2520and%2520Dana%2520Cohen-Bar%2520and%2520Rinon%2520Gal%2520and%2520Daniel%2520Cohen-Or%26entry.1292438233%3D%2520%2520Content%2520creators%2520often%2520draw%2520inspiration%2520from%2520multiple%2520visual%2520sources%252C%250Acombining%2520distinct%2520elements%2520to%2520craft%2520new%2520compositions.%2520Modern%2520computational%250Aapproaches%2520now%2520aim%2520to%2520emulate%2520this%2520fundamental%2520creative%2520process.%2520Although%250Arecent%2520diffusion%2520models%2520excel%2520at%2520text-guided%2520compositional%2520synthesis%252C%2520text%2520as%2520a%250Amedium%2520often%2520lacks%2520precise%2520control%2520over%2520visual%2520details.%2520Image-based%2520composition%250Aapproaches%2520can%2520capture%2520more%2520nuanced%2520features%252C%2520but%2520existing%2520methods%2520are%250Atypically%2520limited%2520in%2520the%2520range%2520of%2520concepts%2520they%2520can%2520capture%252C%2520and%2520require%250Aexpensive%2520training%2520procedures%2520or%2520specialized%2520data.%2520We%2520present%2520IP-Composer%252C%2520a%250Anovel%2520training-free%2520approach%2520for%2520compositional%2520image%2520generation%2520that%2520leverages%250Amultiple%2520image%2520references%2520simultaneously%252C%2520while%2520using%2520natural%2520language%2520to%250Adescribe%2520the%2520concept%2520to%2520be%2520extracted%2520from%2520each%2520image.%2520Our%2520method%2520builds%2520on%250AIP-Adapter%252C%2520which%2520synthesizes%2520novel%2520images%2520conditioned%2520on%2520an%2520input%2520image%2527s%2520CLIP%250Aembedding.%2520We%2520extend%2520this%2520approach%2520to%2520multiple%2520visual%2520inputs%2520by%2520crafting%250Acomposite%2520embeddings%252C%2520stitched%2520from%2520the%2520projections%2520of%2520multiple%2520input%2520images%250Aonto%2520concept-specific%2520CLIP-subspaces%2520identified%2520through%2520text.%2520Through%250Acomprehensive%2520evaluation%252C%2520we%2520show%2520that%2520our%2520approach%2520enables%2520more%2520precise%250Acontrol%2520over%2520a%2520larger%2520range%2520of%2520visual%2520concept%2520compositions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IP-Composer%3A%20Semantic%20Composition%20of%20Visual%20Concepts&entry.906535625=Sara%20Dorfman%20and%20Dana%20Cohen-Bar%20and%20Rinon%20Gal%20and%20Daniel%20Cohen-Or&entry.1292438233=%20%20Content%20creators%20often%20draw%20inspiration%20from%20multiple%20visual%20sources%2C%0Acombining%20distinct%20elements%20to%20craft%20new%20compositions.%20Modern%20computational%0Aapproaches%20now%20aim%20to%20emulate%20this%20fundamental%20creative%20process.%20Although%0Arecent%20diffusion%20models%20excel%20at%20text-guided%20compositional%20synthesis%2C%20text%20as%20a%0Amedium%20often%20lacks%20precise%20control%20over%20visual%20details.%20Image-based%20composition%0Aapproaches%20can%20capture%20more%20nuanced%20features%2C%20but%20existing%20methods%20are%0Atypically%20limited%20in%20the%20range%20of%20concepts%20they%20can%20capture%2C%20and%20require%0Aexpensive%20training%20procedures%20or%20specialized%20data.%20We%20present%20IP-Composer%2C%20a%0Anovel%20training-free%20approach%20for%20compositional%20image%20generation%20that%20leverages%0Amultiple%20image%20references%20simultaneously%2C%20while%20using%20natural%20language%20to%0Adescribe%20the%20concept%20to%20be%20extracted%20from%20each%20image.%20Our%20method%20builds%20on%0AIP-Adapter%2C%20which%20synthesizes%20novel%20images%20conditioned%20on%20an%20input%20image%27s%20CLIP%0Aembedding.%20We%20extend%20this%20approach%20to%20multiple%20visual%20inputs%20by%20crafting%0Acomposite%20embeddings%2C%20stitched%20from%20the%20projections%20of%20multiple%20input%20images%0Aonto%20concept-specific%20CLIP-subspaces%20identified%20through%20text.%20Through%0Acomprehensive%20evaluation%2C%20we%20show%20that%20our%20approach%20enables%20more%20precise%0Acontrol%20over%20a%20larger%20range%20of%20visual%20concept%20compositions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13951v1&entry.124074799=Read"},
{"title": "Qwen2.5-VL Technical Report", "author": "Shuai Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Sibo Song and Kai Dang and Peng Wang and Shijie Wang and Jun Tang and Humen Zhong and Yuanzhi Zhu and Mingkun Yang and Zhaohai Li and Jianqiang Wan and Pengfei Wang and Wei Ding and Zheren Fu and Yiheng Xu and Jiabo Ye and Xi Zhang and Tianbao Xie and Zesen Cheng and Hang Zhang and Zhibo Yang and Haiyang Xu and Junyang Lin", "abstract": "  We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language\nseries, which demonstrates significant advancements in both foundational\ncapabilities and innovative functionalities. Qwen2.5-VL achieves a major leap\nforward in understanding and interacting with the world through enhanced visual\nrecognition, precise object localization, robust document parsing, and\nlong-video comprehension. A standout feature of Qwen2.5-VL is its ability to\nlocalize objects using bounding boxes or points accurately. It provides robust\nstructured data extraction from invoices, forms, and tables, as well as\ndetailed analysis of charts, diagrams, and layouts. To handle complex inputs,\nQwen2.5-VL introduces dynamic resolution processing and absolute time encoding,\nenabling it to process images of varying sizes and videos of extended durations\n(up to hours) with second-level event localization. This allows the model to\nnatively perceive spatial scales and temporal dynamics without relying on\ntraditional normalization techniques. By training a native dynamic-resolution\nVision Transformer (ViT) from scratch and incorporating Window Attention, we\nreduce computational overhead while maintaining native resolution. As a result,\nQwen2.5-VL excels not only in static image and document understanding but also\nas an interactive visual agent capable of reasoning, tool usage, and task\nexecution in real-world scenarios such as operating computers and mobile\ndevices. Qwen2.5-VL is available in three sizes, addressing diverse use cases\nfrom edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model\nmatches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly\nexcelling in document and diagram understanding. Additionally, Qwen2.5-VL\nmaintains robust linguistic performance, preserving the core language\ncompetencies of the Qwen2.5 LLM.\n", "link": "http://arxiv.org/abs/2502.13923v1", "date": "2025-02-19", "relevancy": 2.1978, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5587}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5587}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Qwen2.5-VL%20Technical%20Report&body=Title%3A%20Qwen2.5-VL%20Technical%20Report%0AAuthor%3A%20Shuai%20Bai%20and%20Keqin%20Chen%20and%20Xuejing%20Liu%20and%20Jialin%20Wang%20and%20Wenbin%20Ge%20and%20Sibo%20Song%20and%20Kai%20Dang%20and%20Peng%20Wang%20and%20Shijie%20Wang%20and%20Jun%20Tang%20and%20Humen%20Zhong%20and%20Yuanzhi%20Zhu%20and%20Mingkun%20Yang%20and%20Zhaohai%20Li%20and%20Jianqiang%20Wan%20and%20Pengfei%20Wang%20and%20Wei%20Ding%20and%20Zheren%20Fu%20and%20Yiheng%20Xu%20and%20Jiabo%20Ye%20and%20Xi%20Zhang%20and%20Tianbao%20Xie%20and%20Zesen%20Cheng%20and%20Hang%20Zhang%20and%20Zhibo%20Yang%20and%20Haiyang%20Xu%20and%20Junyang%20Lin%0AAbstract%3A%20%20%20We%20introduce%20Qwen2.5-VL%2C%20the%20latest%20flagship%20model%20of%20Qwen%20vision-language%0Aseries%2C%20which%20demonstrates%20significant%20advancements%20in%20both%20foundational%0Acapabilities%20and%20innovative%20functionalities.%20Qwen2.5-VL%20achieves%20a%20major%20leap%0Aforward%20in%20understanding%20and%20interacting%20with%20the%20world%20through%20enhanced%20visual%0Arecognition%2C%20precise%20object%20localization%2C%20robust%20document%20parsing%2C%20and%0Along-video%20comprehension.%20A%20standout%20feature%20of%20Qwen2.5-VL%20is%20its%20ability%20to%0Alocalize%20objects%20using%20bounding%20boxes%20or%20points%20accurately.%20It%20provides%20robust%0Astructured%20data%20extraction%20from%20invoices%2C%20forms%2C%20and%20tables%2C%20as%20well%20as%0Adetailed%20analysis%20of%20charts%2C%20diagrams%2C%20and%20layouts.%20To%20handle%20complex%20inputs%2C%0AQwen2.5-VL%20introduces%20dynamic%20resolution%20processing%20and%20absolute%20time%20encoding%2C%0Aenabling%20it%20to%20process%20images%20of%20varying%20sizes%20and%20videos%20of%20extended%20durations%0A%28up%20to%20hours%29%20with%20second-level%20event%20localization.%20This%20allows%20the%20model%20to%0Anatively%20perceive%20spatial%20scales%20and%20temporal%20dynamics%20without%20relying%20on%0Atraditional%20normalization%20techniques.%20By%20training%20a%20native%20dynamic-resolution%0AVision%20Transformer%20%28ViT%29%20from%20scratch%20and%20incorporating%20Window%20Attention%2C%20we%0Areduce%20computational%20overhead%20while%20maintaining%20native%20resolution.%20As%20a%20result%2C%0AQwen2.5-VL%20excels%20not%20only%20in%20static%20image%20and%20document%20understanding%20but%20also%0Aas%20an%20interactive%20visual%20agent%20capable%20of%20reasoning%2C%20tool%20usage%2C%20and%20task%0Aexecution%20in%20real-world%20scenarios%20such%20as%20operating%20computers%20and%20mobile%0Adevices.%20Qwen2.5-VL%20is%20available%20in%20three%20sizes%2C%20addressing%20diverse%20use%20cases%0Afrom%20edge%20AI%20to%20high-performance%20computing.%20The%20flagship%20Qwen2.5-VL-72B%20model%0Amatches%20state-of-the-art%20models%20like%20GPT-4o%20and%20Claude%203.5%20Sonnet%2C%20particularly%0Aexcelling%20in%20document%20and%20diagram%20understanding.%20Additionally%2C%20Qwen2.5-VL%0Amaintains%20robust%20linguistic%20performance%2C%20preserving%20the%20core%20language%0Acompetencies%20of%20the%20Qwen2.5%20LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQwen2.5-VL%2520Technical%2520Report%26entry.906535625%3DShuai%2520Bai%2520and%2520Keqin%2520Chen%2520and%2520Xuejing%2520Liu%2520and%2520Jialin%2520Wang%2520and%2520Wenbin%2520Ge%2520and%2520Sibo%2520Song%2520and%2520Kai%2520Dang%2520and%2520Peng%2520Wang%2520and%2520Shijie%2520Wang%2520and%2520Jun%2520Tang%2520and%2520Humen%2520Zhong%2520and%2520Yuanzhi%2520Zhu%2520and%2520Mingkun%2520Yang%2520and%2520Zhaohai%2520Li%2520and%2520Jianqiang%2520Wan%2520and%2520Pengfei%2520Wang%2520and%2520Wei%2520Ding%2520and%2520Zheren%2520Fu%2520and%2520Yiheng%2520Xu%2520and%2520Jiabo%2520Ye%2520and%2520Xi%2520Zhang%2520and%2520Tianbao%2520Xie%2520and%2520Zesen%2520Cheng%2520and%2520Hang%2520Zhang%2520and%2520Zhibo%2520Yang%2520and%2520Haiyang%2520Xu%2520and%2520Junyang%2520Lin%26entry.1292438233%3D%2520%2520We%2520introduce%2520Qwen2.5-VL%252C%2520the%2520latest%2520flagship%2520model%2520of%2520Qwen%2520vision-language%250Aseries%252C%2520which%2520demonstrates%2520significant%2520advancements%2520in%2520both%2520foundational%250Acapabilities%2520and%2520innovative%2520functionalities.%2520Qwen2.5-VL%2520achieves%2520a%2520major%2520leap%250Aforward%2520in%2520understanding%2520and%2520interacting%2520with%2520the%2520world%2520through%2520enhanced%2520visual%250Arecognition%252C%2520precise%2520object%2520localization%252C%2520robust%2520document%2520parsing%252C%2520and%250Along-video%2520comprehension.%2520A%2520standout%2520feature%2520of%2520Qwen2.5-VL%2520is%2520its%2520ability%2520to%250Alocalize%2520objects%2520using%2520bounding%2520boxes%2520or%2520points%2520accurately.%2520It%2520provides%2520robust%250Astructured%2520data%2520extraction%2520from%2520invoices%252C%2520forms%252C%2520and%2520tables%252C%2520as%2520well%2520as%250Adetailed%2520analysis%2520of%2520charts%252C%2520diagrams%252C%2520and%2520layouts.%2520To%2520handle%2520complex%2520inputs%252C%250AQwen2.5-VL%2520introduces%2520dynamic%2520resolution%2520processing%2520and%2520absolute%2520time%2520encoding%252C%250Aenabling%2520it%2520to%2520process%2520images%2520of%2520varying%2520sizes%2520and%2520videos%2520of%2520extended%2520durations%250A%2528up%2520to%2520hours%2529%2520with%2520second-level%2520event%2520localization.%2520This%2520allows%2520the%2520model%2520to%250Anatively%2520perceive%2520spatial%2520scales%2520and%2520temporal%2520dynamics%2520without%2520relying%2520on%250Atraditional%2520normalization%2520techniques.%2520By%2520training%2520a%2520native%2520dynamic-resolution%250AVision%2520Transformer%2520%2528ViT%2529%2520from%2520scratch%2520and%2520incorporating%2520Window%2520Attention%252C%2520we%250Areduce%2520computational%2520overhead%2520while%2520maintaining%2520native%2520resolution.%2520As%2520a%2520result%252C%250AQwen2.5-VL%2520excels%2520not%2520only%2520in%2520static%2520image%2520and%2520document%2520understanding%2520but%2520also%250Aas%2520an%2520interactive%2520visual%2520agent%2520capable%2520of%2520reasoning%252C%2520tool%2520usage%252C%2520and%2520task%250Aexecution%2520in%2520real-world%2520scenarios%2520such%2520as%2520operating%2520computers%2520and%2520mobile%250Adevices.%2520Qwen2.5-VL%2520is%2520available%2520in%2520three%2520sizes%252C%2520addressing%2520diverse%2520use%2520cases%250Afrom%2520edge%2520AI%2520to%2520high-performance%2520computing.%2520The%2520flagship%2520Qwen2.5-VL-72B%2520model%250Amatches%2520state-of-the-art%2520models%2520like%2520GPT-4o%2520and%2520Claude%25203.5%2520Sonnet%252C%2520particularly%250Aexcelling%2520in%2520document%2520and%2520diagram%2520understanding.%2520Additionally%252C%2520Qwen2.5-VL%250Amaintains%2520robust%2520linguistic%2520performance%252C%2520preserving%2520the%2520core%2520language%250Acompetencies%2520of%2520the%2520Qwen2.5%2520LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Qwen2.5-VL%20Technical%20Report&entry.906535625=Shuai%20Bai%20and%20Keqin%20Chen%20and%20Xuejing%20Liu%20and%20Jialin%20Wang%20and%20Wenbin%20Ge%20and%20Sibo%20Song%20and%20Kai%20Dang%20and%20Peng%20Wang%20and%20Shijie%20Wang%20and%20Jun%20Tang%20and%20Humen%20Zhong%20and%20Yuanzhi%20Zhu%20and%20Mingkun%20Yang%20and%20Zhaohai%20Li%20and%20Jianqiang%20Wan%20and%20Pengfei%20Wang%20and%20Wei%20Ding%20and%20Zheren%20Fu%20and%20Yiheng%20Xu%20and%20Jiabo%20Ye%20and%20Xi%20Zhang%20and%20Tianbao%20Xie%20and%20Zesen%20Cheng%20and%20Hang%20Zhang%20and%20Zhibo%20Yang%20and%20Haiyang%20Xu%20and%20Junyang%20Lin&entry.1292438233=%20%20We%20introduce%20Qwen2.5-VL%2C%20the%20latest%20flagship%20model%20of%20Qwen%20vision-language%0Aseries%2C%20which%20demonstrates%20significant%20advancements%20in%20both%20foundational%0Acapabilities%20and%20innovative%20functionalities.%20Qwen2.5-VL%20achieves%20a%20major%20leap%0Aforward%20in%20understanding%20and%20interacting%20with%20the%20world%20through%20enhanced%20visual%0Arecognition%2C%20precise%20object%20localization%2C%20robust%20document%20parsing%2C%20and%0Along-video%20comprehension.%20A%20standout%20feature%20of%20Qwen2.5-VL%20is%20its%20ability%20to%0Alocalize%20objects%20using%20bounding%20boxes%20or%20points%20accurately.%20It%20provides%20robust%0Astructured%20data%20extraction%20from%20invoices%2C%20forms%2C%20and%20tables%2C%20as%20well%20as%0Adetailed%20analysis%20of%20charts%2C%20diagrams%2C%20and%20layouts.%20To%20handle%20complex%20inputs%2C%0AQwen2.5-VL%20introduces%20dynamic%20resolution%20processing%20and%20absolute%20time%20encoding%2C%0Aenabling%20it%20to%20process%20images%20of%20varying%20sizes%20and%20videos%20of%20extended%20durations%0A%28up%20to%20hours%29%20with%20second-level%20event%20localization.%20This%20allows%20the%20model%20to%0Anatively%20perceive%20spatial%20scales%20and%20temporal%20dynamics%20without%20relying%20on%0Atraditional%20normalization%20techniques.%20By%20training%20a%20native%20dynamic-resolution%0AVision%20Transformer%20%28ViT%29%20from%20scratch%20and%20incorporating%20Window%20Attention%2C%20we%0Areduce%20computational%20overhead%20while%20maintaining%20native%20resolution.%20As%20a%20result%2C%0AQwen2.5-VL%20excels%20not%20only%20in%20static%20image%20and%20document%20understanding%20but%20also%0Aas%20an%20interactive%20visual%20agent%20capable%20of%20reasoning%2C%20tool%20usage%2C%20and%20task%0Aexecution%20in%20real-world%20scenarios%20such%20as%20operating%20computers%20and%20mobile%0Adevices.%20Qwen2.5-VL%20is%20available%20in%20three%20sizes%2C%20addressing%20diverse%20use%20cases%0Afrom%20edge%20AI%20to%20high-performance%20computing.%20The%20flagship%20Qwen2.5-VL-72B%20model%0Amatches%20state-of-the-art%20models%20like%20GPT-4o%20and%20Claude%203.5%20Sonnet%2C%20particularly%0Aexcelling%20in%20document%20and%20diagram%20understanding.%20Additionally%2C%20Qwen2.5-VL%0Amaintains%20robust%20linguistic%20performance%2C%20preserving%20the%20core%20language%0Acompetencies%20of%20the%20Qwen2.5%20LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13923v1&entry.124074799=Read"},
{"title": "MagicGeo: Training-Free Text-Guided Geometric Diagram Generation", "author": "Junxiao Wang and Ting Zhang and Heng Yu and Jingdong Wang and Hua Huang", "abstract": "  Geometric diagrams are critical in conveying mathematical and scientific\nconcepts, yet traditional diagram generation methods are often manual and\nresource-intensive. While text-to-image generation has made strides in\nphotorealistic imagery, creating accurate geometric diagrams remains a\nchallenge due to the need for precise spatial relationships and the scarcity of\ngeometry-specific datasets. This paper presents MagicGeo, a training-free\nframework for generating geometric diagrams from textual descriptions. MagicGeo\nformulates the diagram generation process as a coordinate optimization problem,\nensuring geometric correctness through a formal language solver, and then\nemploys coordinate-aware generation. The framework leverages the strong\nlanguage translation capability of large language models, while formal\nmathematical solving ensures geometric correctness. We further introduce\nMagicGeoBench, a benchmark dataset of 220 geometric diagram descriptions, and\ndemonstrate that MagicGeo outperforms current methods in both qualitative and\nquantitative evaluations. This work provides a scalable, accurate solution for\nautomated diagram generation, with significant implications for educational and\nacademic applications.\n", "link": "http://arxiv.org/abs/2502.13855v1", "date": "2025-02-19", "relevancy": 2.177, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5642}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5347}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MagicGeo%3A%20Training-Free%20Text-Guided%20Geometric%20Diagram%20Generation&body=Title%3A%20MagicGeo%3A%20Training-Free%20Text-Guided%20Geometric%20Diagram%20Generation%0AAuthor%3A%20Junxiao%20Wang%20and%20Ting%20Zhang%20and%20Heng%20Yu%20and%20Jingdong%20Wang%20and%20Hua%20Huang%0AAbstract%3A%20%20%20Geometric%20diagrams%20are%20critical%20in%20conveying%20mathematical%20and%20scientific%0Aconcepts%2C%20yet%20traditional%20diagram%20generation%20methods%20are%20often%20manual%20and%0Aresource-intensive.%20While%20text-to-image%20generation%20has%20made%20strides%20in%0Aphotorealistic%20imagery%2C%20creating%20accurate%20geometric%20diagrams%20remains%20a%0Achallenge%20due%20to%20the%20need%20for%20precise%20spatial%20relationships%20and%20the%20scarcity%20of%0Ageometry-specific%20datasets.%20This%20paper%20presents%20MagicGeo%2C%20a%20training-free%0Aframework%20for%20generating%20geometric%20diagrams%20from%20textual%20descriptions.%20MagicGeo%0Aformulates%20the%20diagram%20generation%20process%20as%20a%20coordinate%20optimization%20problem%2C%0Aensuring%20geometric%20correctness%20through%20a%20formal%20language%20solver%2C%20and%20then%0Aemploys%20coordinate-aware%20generation.%20The%20framework%20leverages%20the%20strong%0Alanguage%20translation%20capability%20of%20large%20language%20models%2C%20while%20formal%0Amathematical%20solving%20ensures%20geometric%20correctness.%20We%20further%20introduce%0AMagicGeoBench%2C%20a%20benchmark%20dataset%20of%20220%20geometric%20diagram%20descriptions%2C%20and%0Ademonstrate%20that%20MagicGeo%20outperforms%20current%20methods%20in%20both%20qualitative%20and%0Aquantitative%20evaluations.%20This%20work%20provides%20a%20scalable%2C%20accurate%20solution%20for%0Aautomated%20diagram%20generation%2C%20with%20significant%20implications%20for%20educational%20and%0Aacademic%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagicGeo%253A%2520Training-Free%2520Text-Guided%2520Geometric%2520Diagram%2520Generation%26entry.906535625%3DJunxiao%2520Wang%2520and%2520Ting%2520Zhang%2520and%2520Heng%2520Yu%2520and%2520Jingdong%2520Wang%2520and%2520Hua%2520Huang%26entry.1292438233%3D%2520%2520Geometric%2520diagrams%2520are%2520critical%2520in%2520conveying%2520mathematical%2520and%2520scientific%250Aconcepts%252C%2520yet%2520traditional%2520diagram%2520generation%2520methods%2520are%2520often%2520manual%2520and%250Aresource-intensive.%2520While%2520text-to-image%2520generation%2520has%2520made%2520strides%2520in%250Aphotorealistic%2520imagery%252C%2520creating%2520accurate%2520geometric%2520diagrams%2520remains%2520a%250Achallenge%2520due%2520to%2520the%2520need%2520for%2520precise%2520spatial%2520relationships%2520and%2520the%2520scarcity%2520of%250Ageometry-specific%2520datasets.%2520This%2520paper%2520presents%2520MagicGeo%252C%2520a%2520training-free%250Aframework%2520for%2520generating%2520geometric%2520diagrams%2520from%2520textual%2520descriptions.%2520MagicGeo%250Aformulates%2520the%2520diagram%2520generation%2520process%2520as%2520a%2520coordinate%2520optimization%2520problem%252C%250Aensuring%2520geometric%2520correctness%2520through%2520a%2520formal%2520language%2520solver%252C%2520and%2520then%250Aemploys%2520coordinate-aware%2520generation.%2520The%2520framework%2520leverages%2520the%2520strong%250Alanguage%2520translation%2520capability%2520of%2520large%2520language%2520models%252C%2520while%2520formal%250Amathematical%2520solving%2520ensures%2520geometric%2520correctness.%2520We%2520further%2520introduce%250AMagicGeoBench%252C%2520a%2520benchmark%2520dataset%2520of%2520220%2520geometric%2520diagram%2520descriptions%252C%2520and%250Ademonstrate%2520that%2520MagicGeo%2520outperforms%2520current%2520methods%2520in%2520both%2520qualitative%2520and%250Aquantitative%2520evaluations.%2520This%2520work%2520provides%2520a%2520scalable%252C%2520accurate%2520solution%2520for%250Aautomated%2520diagram%2520generation%252C%2520with%2520significant%2520implications%2520for%2520educational%2520and%250Aacademic%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagicGeo%3A%20Training-Free%20Text-Guided%20Geometric%20Diagram%20Generation&entry.906535625=Junxiao%20Wang%20and%20Ting%20Zhang%20and%20Heng%20Yu%20and%20Jingdong%20Wang%20and%20Hua%20Huang&entry.1292438233=%20%20Geometric%20diagrams%20are%20critical%20in%20conveying%20mathematical%20and%20scientific%0Aconcepts%2C%20yet%20traditional%20diagram%20generation%20methods%20are%20often%20manual%20and%0Aresource-intensive.%20While%20text-to-image%20generation%20has%20made%20strides%20in%0Aphotorealistic%20imagery%2C%20creating%20accurate%20geometric%20diagrams%20remains%20a%0Achallenge%20due%20to%20the%20need%20for%20precise%20spatial%20relationships%20and%20the%20scarcity%20of%0Ageometry-specific%20datasets.%20This%20paper%20presents%20MagicGeo%2C%20a%20training-free%0Aframework%20for%20generating%20geometric%20diagrams%20from%20textual%20descriptions.%20MagicGeo%0Aformulates%20the%20diagram%20generation%20process%20as%20a%20coordinate%20optimization%20problem%2C%0Aensuring%20geometric%20correctness%20through%20a%20formal%20language%20solver%2C%20and%20then%0Aemploys%20coordinate-aware%20generation.%20The%20framework%20leverages%20the%20strong%0Alanguage%20translation%20capability%20of%20large%20language%20models%2C%20while%20formal%0Amathematical%20solving%20ensures%20geometric%20correctness.%20We%20further%20introduce%0AMagicGeoBench%2C%20a%20benchmark%20dataset%20of%20220%20geometric%20diagram%20descriptions%2C%20and%0Ademonstrate%20that%20MagicGeo%20outperforms%20current%20methods%20in%20both%20qualitative%20and%0Aquantitative%20evaluations.%20This%20work%20provides%20a%20scalable%2C%20accurate%20solution%20for%0Aautomated%20diagram%20generation%2C%20with%20significant%20implications%20for%20educational%20and%0Aacademic%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13855v1&entry.124074799=Read"},
{"title": "Traffic Scene Generation from Natural Language Description for\n  Autonomous Vehicles with Large Language Model", "author": "Bo-Kai Ruan and Hao-Tang Tsui and Yung-Hui Li and Hong-Han Shuai", "abstract": "  Text-to-scene generation typically limits environmental diversity by\ngenerating key scenarios along predetermined paths. To address these\nconstraints, we propose a novel text-to-traffic scene framework that leverages\na large language model (LLM) to autonomously generate diverse traffic scenarios\nfor the CARLA simulator based on natural language descriptions. Our pipeline\ncomprises several key stages: (1) Prompt Analysis, where natural language\ninputs are decomposed; (2) Road Retrieval, selecting optimal roads from a\ndatabase; (3) Agent Planning, detailing agent types and behaviors; (4) Road\nRanking, scoring roads to match scenario requirements; and (5) Scene\nGeneration, rendering the planned scenarios in the simulator. This framework\nsupports both routine and critical traffic scenarios, enhancing its\napplicability. We demonstrate that our approach not only diversifies agent\nplanning and road selection but also significantly reduces the average\ncollision rate from 8% to 3.5% in SafeBench. Additionally, our framework\nimproves narration and reasoning for driving captioning tasks. Our\ncontributions and resources are publicly available at\nhttps://basiclab.github.io/TTSG.\n", "link": "http://arxiv.org/abs/2409.09575v2", "date": "2025-02-19", "relevancy": 2.1648, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5386}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Traffic%20Scene%20Generation%20from%20Natural%20Language%20Description%20for%0A%20%20Autonomous%20Vehicles%20with%20Large%20Language%20Model&body=Title%3A%20Traffic%20Scene%20Generation%20from%20Natural%20Language%20Description%20for%0A%20%20Autonomous%20Vehicles%20with%20Large%20Language%20Model%0AAuthor%3A%20Bo-Kai%20Ruan%20and%20Hao-Tang%20Tsui%20and%20Yung-Hui%20Li%20and%20Hong-Han%20Shuai%0AAbstract%3A%20%20%20Text-to-scene%20generation%20typically%20limits%20environmental%20diversity%20by%0Agenerating%20key%20scenarios%20along%20predetermined%20paths.%20To%20address%20these%0Aconstraints%2C%20we%20propose%20a%20novel%20text-to-traffic%20scene%20framework%20that%20leverages%0Aa%20large%20language%20model%20%28LLM%29%20to%20autonomously%20generate%20diverse%20traffic%20scenarios%0Afor%20the%20CARLA%20simulator%20based%20on%20natural%20language%20descriptions.%20Our%20pipeline%0Acomprises%20several%20key%20stages%3A%20%281%29%20Prompt%20Analysis%2C%20where%20natural%20language%0Ainputs%20are%20decomposed%3B%20%282%29%20Road%20Retrieval%2C%20selecting%20optimal%20roads%20from%20a%0Adatabase%3B%20%283%29%20Agent%20Planning%2C%20detailing%20agent%20types%20and%20behaviors%3B%20%284%29%20Road%0ARanking%2C%20scoring%20roads%20to%20match%20scenario%20requirements%3B%20and%20%285%29%20Scene%0AGeneration%2C%20rendering%20the%20planned%20scenarios%20in%20the%20simulator.%20This%20framework%0Asupports%20both%20routine%20and%20critical%20traffic%20scenarios%2C%20enhancing%20its%0Aapplicability.%20We%20demonstrate%20that%20our%20approach%20not%20only%20diversifies%20agent%0Aplanning%20and%20road%20selection%20but%20also%20significantly%20reduces%20the%20average%0Acollision%20rate%20from%208%25%20to%203.5%25%20in%20SafeBench.%20Additionally%2C%20our%20framework%0Aimproves%20narration%20and%20reasoning%20for%20driving%20captioning%20tasks.%20Our%0Acontributions%20and%20resources%20are%20publicly%20available%20at%0Ahttps%3A//basiclab.github.io/TTSG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09575v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraffic%2520Scene%2520Generation%2520from%2520Natural%2520Language%2520Description%2520for%250A%2520%2520Autonomous%2520Vehicles%2520with%2520Large%2520Language%2520Model%26entry.906535625%3DBo-Kai%2520Ruan%2520and%2520Hao-Tang%2520Tsui%2520and%2520Yung-Hui%2520Li%2520and%2520Hong-Han%2520Shuai%26entry.1292438233%3D%2520%2520Text-to-scene%2520generation%2520typically%2520limits%2520environmental%2520diversity%2520by%250Agenerating%2520key%2520scenarios%2520along%2520predetermined%2520paths.%2520To%2520address%2520these%250Aconstraints%252C%2520we%2520propose%2520a%2520novel%2520text-to-traffic%2520scene%2520framework%2520that%2520leverages%250Aa%2520large%2520language%2520model%2520%2528LLM%2529%2520to%2520autonomously%2520generate%2520diverse%2520traffic%2520scenarios%250Afor%2520the%2520CARLA%2520simulator%2520based%2520on%2520natural%2520language%2520descriptions.%2520Our%2520pipeline%250Acomprises%2520several%2520key%2520stages%253A%2520%25281%2529%2520Prompt%2520Analysis%252C%2520where%2520natural%2520language%250Ainputs%2520are%2520decomposed%253B%2520%25282%2529%2520Road%2520Retrieval%252C%2520selecting%2520optimal%2520roads%2520from%2520a%250Adatabase%253B%2520%25283%2529%2520Agent%2520Planning%252C%2520detailing%2520agent%2520types%2520and%2520behaviors%253B%2520%25284%2529%2520Road%250ARanking%252C%2520scoring%2520roads%2520to%2520match%2520scenario%2520requirements%253B%2520and%2520%25285%2529%2520Scene%250AGeneration%252C%2520rendering%2520the%2520planned%2520scenarios%2520in%2520the%2520simulator.%2520This%2520framework%250Asupports%2520both%2520routine%2520and%2520critical%2520traffic%2520scenarios%252C%2520enhancing%2520its%250Aapplicability.%2520We%2520demonstrate%2520that%2520our%2520approach%2520not%2520only%2520diversifies%2520agent%250Aplanning%2520and%2520road%2520selection%2520but%2520also%2520significantly%2520reduces%2520the%2520average%250Acollision%2520rate%2520from%25208%2525%2520to%25203.5%2525%2520in%2520SafeBench.%2520Additionally%252C%2520our%2520framework%250Aimproves%2520narration%2520and%2520reasoning%2520for%2520driving%2520captioning%2520tasks.%2520Our%250Acontributions%2520and%2520resources%2520are%2520publicly%2520available%2520at%250Ahttps%253A//basiclab.github.io/TTSG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09575v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Traffic%20Scene%20Generation%20from%20Natural%20Language%20Description%20for%0A%20%20Autonomous%20Vehicles%20with%20Large%20Language%20Model&entry.906535625=Bo-Kai%20Ruan%20and%20Hao-Tang%20Tsui%20and%20Yung-Hui%20Li%20and%20Hong-Han%20Shuai&entry.1292438233=%20%20Text-to-scene%20generation%20typically%20limits%20environmental%20diversity%20by%0Agenerating%20key%20scenarios%20along%20predetermined%20paths.%20To%20address%20these%0Aconstraints%2C%20we%20propose%20a%20novel%20text-to-traffic%20scene%20framework%20that%20leverages%0Aa%20large%20language%20model%20%28LLM%29%20to%20autonomously%20generate%20diverse%20traffic%20scenarios%0Afor%20the%20CARLA%20simulator%20based%20on%20natural%20language%20descriptions.%20Our%20pipeline%0Acomprises%20several%20key%20stages%3A%20%281%29%20Prompt%20Analysis%2C%20where%20natural%20language%0Ainputs%20are%20decomposed%3B%20%282%29%20Road%20Retrieval%2C%20selecting%20optimal%20roads%20from%20a%0Adatabase%3B%20%283%29%20Agent%20Planning%2C%20detailing%20agent%20types%20and%20behaviors%3B%20%284%29%20Road%0ARanking%2C%20scoring%20roads%20to%20match%20scenario%20requirements%3B%20and%20%285%29%20Scene%0AGeneration%2C%20rendering%20the%20planned%20scenarios%20in%20the%20simulator.%20This%20framework%0Asupports%20both%20routine%20and%20critical%20traffic%20scenarios%2C%20enhancing%20its%0Aapplicability.%20We%20demonstrate%20that%20our%20approach%20not%20only%20diversifies%20agent%0Aplanning%20and%20road%20selection%20but%20also%20significantly%20reduces%20the%20average%0Acollision%20rate%20from%208%25%20to%203.5%25%20in%20SafeBench.%20Additionally%2C%20our%20framework%0Aimproves%20narration%20and%20reasoning%20for%20driving%20captioning%20tasks.%20Our%0Acontributions%20and%20resources%20are%20publicly%20available%20at%0Ahttps%3A//basiclab.github.io/TTSG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09575v2&entry.124074799=Read"},
{"title": "Rethinking Audio-Visual Adversarial Vulnerability from Temporal and\n  Modality Perspectives", "author": "Zeliang Zhang and Susan Liang and Daiki Shimada and Chenliang Xu", "abstract": "  While audio-visual learning equips models with a richer understanding of the\nreal world by leveraging multiple sensory modalities, this integration also\nintroduces new vulnerabilities to adversarial attacks.\n  In this paper, we present a comprehensive study of the adversarial robustness\nof audio-visual models, considering both temporal and modality-specific\nvulnerabilities. We propose two powerful adversarial attacks: 1) a temporal\ninvariance attack that exploits the inherent temporal redundancy across\nconsecutive time segments and 2) a modality misalignment attack that introduces\nincongruence between the audio and visual modalities. These attacks are\ndesigned to thoroughly assess the robustness of audio-visual models against\ndiverse threats. Furthermore, to defend against such attacks, we introduce a\nnovel audio-visual adversarial training framework. This framework addresses key\nchallenges in vanilla adversarial training by incorporating efficient\nadversarial perturbation crafting tailored to multi-modal data and an\nadversarial curriculum strategy. Extensive experiments in the Kinetics-Sounds\ndataset demonstrate that our proposed temporal and modality-based attacks in\ndegrading model performance can achieve state-of-the-art performance, while our\nadversarial training defense largely improves the adversarial robustness as\nwell as the adversarial training efficiency.\n", "link": "http://arxiv.org/abs/2502.11858v2", "date": "2025-02-19", "relevancy": 2.1577, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5512}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5332}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Audio-Visual%20Adversarial%20Vulnerability%20from%20Temporal%20and%0A%20%20Modality%20Perspectives&body=Title%3A%20Rethinking%20Audio-Visual%20Adversarial%20Vulnerability%20from%20Temporal%20and%0A%20%20Modality%20Perspectives%0AAuthor%3A%20Zeliang%20Zhang%20and%20Susan%20Liang%20and%20Daiki%20Shimada%20and%20Chenliang%20Xu%0AAbstract%3A%20%20%20While%20audio-visual%20learning%20equips%20models%20with%20a%20richer%20understanding%20of%20the%0Areal%20world%20by%20leveraging%20multiple%20sensory%20modalities%2C%20this%20integration%20also%0Aintroduces%20new%20vulnerabilities%20to%20adversarial%20attacks.%0A%20%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20study%20of%20the%20adversarial%20robustness%0Aof%20audio-visual%20models%2C%20considering%20both%20temporal%20and%20modality-specific%0Avulnerabilities.%20We%20propose%20two%20powerful%20adversarial%20attacks%3A%201%29%20a%20temporal%0Ainvariance%20attack%20that%20exploits%20the%20inherent%20temporal%20redundancy%20across%0Aconsecutive%20time%20segments%20and%202%29%20a%20modality%20misalignment%20attack%20that%20introduces%0Aincongruence%20between%20the%20audio%20and%20visual%20modalities.%20These%20attacks%20are%0Adesigned%20to%20thoroughly%20assess%20the%20robustness%20of%20audio-visual%20models%20against%0Adiverse%20threats.%20Furthermore%2C%20to%20defend%20against%20such%20attacks%2C%20we%20introduce%20a%0Anovel%20audio-visual%20adversarial%20training%20framework.%20This%20framework%20addresses%20key%0Achallenges%20in%20vanilla%20adversarial%20training%20by%20incorporating%20efficient%0Aadversarial%20perturbation%20crafting%20tailored%20to%20multi-modal%20data%20and%20an%0Aadversarial%20curriculum%20strategy.%20Extensive%20experiments%20in%20the%20Kinetics-Sounds%0Adataset%20demonstrate%20that%20our%20proposed%20temporal%20and%20modality-based%20attacks%20in%0Adegrading%20model%20performance%20can%20achieve%20state-of-the-art%20performance%2C%20while%20our%0Aadversarial%20training%20defense%20largely%20improves%20the%20adversarial%20robustness%20as%0Awell%20as%20the%20adversarial%20training%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11858v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Audio-Visual%2520Adversarial%2520Vulnerability%2520from%2520Temporal%2520and%250A%2520%2520Modality%2520Perspectives%26entry.906535625%3DZeliang%2520Zhang%2520and%2520Susan%2520Liang%2520and%2520Daiki%2520Shimada%2520and%2520Chenliang%2520Xu%26entry.1292438233%3D%2520%2520While%2520audio-visual%2520learning%2520equips%2520models%2520with%2520a%2520richer%2520understanding%2520of%2520the%250Areal%2520world%2520by%2520leveraging%2520multiple%2520sensory%2520modalities%252C%2520this%2520integration%2520also%250Aintroduces%2520new%2520vulnerabilities%2520to%2520adversarial%2520attacks.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520comprehensive%2520study%2520of%2520the%2520adversarial%2520robustness%250Aof%2520audio-visual%2520models%252C%2520considering%2520both%2520temporal%2520and%2520modality-specific%250Avulnerabilities.%2520We%2520propose%2520two%2520powerful%2520adversarial%2520attacks%253A%25201%2529%2520a%2520temporal%250Ainvariance%2520attack%2520that%2520exploits%2520the%2520inherent%2520temporal%2520redundancy%2520across%250Aconsecutive%2520time%2520segments%2520and%25202%2529%2520a%2520modality%2520misalignment%2520attack%2520that%2520introduces%250Aincongruence%2520between%2520the%2520audio%2520and%2520visual%2520modalities.%2520These%2520attacks%2520are%250Adesigned%2520to%2520thoroughly%2520assess%2520the%2520robustness%2520of%2520audio-visual%2520models%2520against%250Adiverse%2520threats.%2520Furthermore%252C%2520to%2520defend%2520against%2520such%2520attacks%252C%2520we%2520introduce%2520a%250Anovel%2520audio-visual%2520adversarial%2520training%2520framework.%2520This%2520framework%2520addresses%2520key%250Achallenges%2520in%2520vanilla%2520adversarial%2520training%2520by%2520incorporating%2520efficient%250Aadversarial%2520perturbation%2520crafting%2520tailored%2520to%2520multi-modal%2520data%2520and%2520an%250Aadversarial%2520curriculum%2520strategy.%2520Extensive%2520experiments%2520in%2520the%2520Kinetics-Sounds%250Adataset%2520demonstrate%2520that%2520our%2520proposed%2520temporal%2520and%2520modality-based%2520attacks%2520in%250Adegrading%2520model%2520performance%2520can%2520achieve%2520state-of-the-art%2520performance%252C%2520while%2520our%250Aadversarial%2520training%2520defense%2520largely%2520improves%2520the%2520adversarial%2520robustness%2520as%250Awell%2520as%2520the%2520adversarial%2520training%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11858v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Audio-Visual%20Adversarial%20Vulnerability%20from%20Temporal%20and%0A%20%20Modality%20Perspectives&entry.906535625=Zeliang%20Zhang%20and%20Susan%20Liang%20and%20Daiki%20Shimada%20and%20Chenliang%20Xu&entry.1292438233=%20%20While%20audio-visual%20learning%20equips%20models%20with%20a%20richer%20understanding%20of%20the%0Areal%20world%20by%20leveraging%20multiple%20sensory%20modalities%2C%20this%20integration%20also%0Aintroduces%20new%20vulnerabilities%20to%20adversarial%20attacks.%0A%20%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20study%20of%20the%20adversarial%20robustness%0Aof%20audio-visual%20models%2C%20considering%20both%20temporal%20and%20modality-specific%0Avulnerabilities.%20We%20propose%20two%20powerful%20adversarial%20attacks%3A%201%29%20a%20temporal%0Ainvariance%20attack%20that%20exploits%20the%20inherent%20temporal%20redundancy%20across%0Aconsecutive%20time%20segments%20and%202%29%20a%20modality%20misalignment%20attack%20that%20introduces%0Aincongruence%20between%20the%20audio%20and%20visual%20modalities.%20These%20attacks%20are%0Adesigned%20to%20thoroughly%20assess%20the%20robustness%20of%20audio-visual%20models%20against%0Adiverse%20threats.%20Furthermore%2C%20to%20defend%20against%20such%20attacks%2C%20we%20introduce%20a%0Anovel%20audio-visual%20adversarial%20training%20framework.%20This%20framework%20addresses%20key%0Achallenges%20in%20vanilla%20adversarial%20training%20by%20incorporating%20efficient%0Aadversarial%20perturbation%20crafting%20tailored%20to%20multi-modal%20data%20and%20an%0Aadversarial%20curriculum%20strategy.%20Extensive%20experiments%20in%20the%20Kinetics-Sounds%0Adataset%20demonstrate%20that%20our%20proposed%20temporal%20and%20modality-based%20attacks%20in%0Adegrading%20model%20performance%20can%20achieve%20state-of-the-art%20performance%2C%20while%20our%0Aadversarial%20training%20defense%20largely%20improves%20the%20adversarial%20robustness%20as%0Awell%20as%20the%20adversarial%20training%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11858v2&entry.124074799=Read"},
{"title": "MEX: Memory-efficient Approach to Referring Multi-Object Tracking", "author": "Huu-Thien Tran and Phuoc-Sang Pham and Thai-Son Tran and Khoa Luu", "abstract": "  Referring Multi-Object Tracking (RMOT) is a relatively new concept that has\nrapidly gained traction as a promising research direction at the intersection\nof computer vision and natural language processing. Unlike traditional\nmulti-object tracking, RMOT identifies and tracks objects and incorporates\ntextual descriptions for object class names, making the approach more\nintuitive. Various techniques have been proposed to address this challenging\nproblem; however, most require the training of the entire network due to their\nend-to-end nature. Among these methods, iKUN has emerged as a particularly\npromising solution. Therefore, we further explore its pipeline and enhance its\nperformance. In this paper, we introduce a practical module dubbed\nMemory-Efficient Cross-modality -- MEX. This memory-efficient technique can be\ndirectly applied to off-the-shelf trackers like iKUN, resulting in significant\narchitectural improvements. Our method proves effective during inference on a\nsingle GPU with 4 GB of memory. Among the various benchmarks, the Refer-KITTI\ndataset, which offers diverse autonomous driving scenes with relevant language\nexpressions, is particularly useful for studying this problem. Empirically, our\nmethod demonstrates effectiveness and efficiency regarding HOTA tracking\nscores, substantially improving memory allocation and processing speed.\n", "link": "http://arxiv.org/abs/2502.13875v1", "date": "2025-02-19", "relevancy": 2.1549, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.55}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5312}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEX%3A%20Memory-efficient%20Approach%20to%20Referring%20Multi-Object%20Tracking&body=Title%3A%20MEX%3A%20Memory-efficient%20Approach%20to%20Referring%20Multi-Object%20Tracking%0AAuthor%3A%20Huu-Thien%20Tran%20and%20Phuoc-Sang%20Pham%20and%20Thai-Son%20Tran%20and%20Khoa%20Luu%0AAbstract%3A%20%20%20Referring%20Multi-Object%20Tracking%20%28RMOT%29%20is%20a%20relatively%20new%20concept%20that%20has%0Arapidly%20gained%20traction%20as%20a%20promising%20research%20direction%20at%20the%20intersection%0Aof%20computer%20vision%20and%20natural%20language%20processing.%20Unlike%20traditional%0Amulti-object%20tracking%2C%20RMOT%20identifies%20and%20tracks%20objects%20and%20incorporates%0Atextual%20descriptions%20for%20object%20class%20names%2C%20making%20the%20approach%20more%0Aintuitive.%20Various%20techniques%20have%20been%20proposed%20to%20address%20this%20challenging%0Aproblem%3B%20however%2C%20most%20require%20the%20training%20of%20the%20entire%20network%20due%20to%20their%0Aend-to-end%20nature.%20Among%20these%20methods%2C%20iKUN%20has%20emerged%20as%20a%20particularly%0Apromising%20solution.%20Therefore%2C%20we%20further%20explore%20its%20pipeline%20and%20enhance%20its%0Aperformance.%20In%20this%20paper%2C%20we%20introduce%20a%20practical%20module%20dubbed%0AMemory-Efficient%20Cross-modality%20--%20MEX.%20This%20memory-efficient%20technique%20can%20be%0Adirectly%20applied%20to%20off-the-shelf%20trackers%20like%20iKUN%2C%20resulting%20in%20significant%0Aarchitectural%20improvements.%20Our%20method%20proves%20effective%20during%20inference%20on%20a%0Asingle%20GPU%20with%204%20GB%20of%20memory.%20Among%20the%20various%20benchmarks%2C%20the%20Refer-KITTI%0Adataset%2C%20which%20offers%20diverse%20autonomous%20driving%20scenes%20with%20relevant%20language%0Aexpressions%2C%20is%20particularly%20useful%20for%20studying%20this%20problem.%20Empirically%2C%20our%0Amethod%20demonstrates%20effectiveness%20and%20efficiency%20regarding%20HOTA%20tracking%0Ascores%2C%20substantially%20improving%20memory%20allocation%20and%20processing%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEX%253A%2520Memory-efficient%2520Approach%2520to%2520Referring%2520Multi-Object%2520Tracking%26entry.906535625%3DHuu-Thien%2520Tran%2520and%2520Phuoc-Sang%2520Pham%2520and%2520Thai-Son%2520Tran%2520and%2520Khoa%2520Luu%26entry.1292438233%3D%2520%2520Referring%2520Multi-Object%2520Tracking%2520%2528RMOT%2529%2520is%2520a%2520relatively%2520new%2520concept%2520that%2520has%250Arapidly%2520gained%2520traction%2520as%2520a%2520promising%2520research%2520direction%2520at%2520the%2520intersection%250Aof%2520computer%2520vision%2520and%2520natural%2520language%2520processing.%2520Unlike%2520traditional%250Amulti-object%2520tracking%252C%2520RMOT%2520identifies%2520and%2520tracks%2520objects%2520and%2520incorporates%250Atextual%2520descriptions%2520for%2520object%2520class%2520names%252C%2520making%2520the%2520approach%2520more%250Aintuitive.%2520Various%2520techniques%2520have%2520been%2520proposed%2520to%2520address%2520this%2520challenging%250Aproblem%253B%2520however%252C%2520most%2520require%2520the%2520training%2520of%2520the%2520entire%2520network%2520due%2520to%2520their%250Aend-to-end%2520nature.%2520Among%2520these%2520methods%252C%2520iKUN%2520has%2520emerged%2520as%2520a%2520particularly%250Apromising%2520solution.%2520Therefore%252C%2520we%2520further%2520explore%2520its%2520pipeline%2520and%2520enhance%2520its%250Aperformance.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520practical%2520module%2520dubbed%250AMemory-Efficient%2520Cross-modality%2520--%2520MEX.%2520This%2520memory-efficient%2520technique%2520can%2520be%250Adirectly%2520applied%2520to%2520off-the-shelf%2520trackers%2520like%2520iKUN%252C%2520resulting%2520in%2520significant%250Aarchitectural%2520improvements.%2520Our%2520method%2520proves%2520effective%2520during%2520inference%2520on%2520a%250Asingle%2520GPU%2520with%25204%2520GB%2520of%2520memory.%2520Among%2520the%2520various%2520benchmarks%252C%2520the%2520Refer-KITTI%250Adataset%252C%2520which%2520offers%2520diverse%2520autonomous%2520driving%2520scenes%2520with%2520relevant%2520language%250Aexpressions%252C%2520is%2520particularly%2520useful%2520for%2520studying%2520this%2520problem.%2520Empirically%252C%2520our%250Amethod%2520demonstrates%2520effectiveness%2520and%2520efficiency%2520regarding%2520HOTA%2520tracking%250Ascores%252C%2520substantially%2520improving%2520memory%2520allocation%2520and%2520processing%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEX%3A%20Memory-efficient%20Approach%20to%20Referring%20Multi-Object%20Tracking&entry.906535625=Huu-Thien%20Tran%20and%20Phuoc-Sang%20Pham%20and%20Thai-Son%20Tran%20and%20Khoa%20Luu&entry.1292438233=%20%20Referring%20Multi-Object%20Tracking%20%28RMOT%29%20is%20a%20relatively%20new%20concept%20that%20has%0Arapidly%20gained%20traction%20as%20a%20promising%20research%20direction%20at%20the%20intersection%0Aof%20computer%20vision%20and%20natural%20language%20processing.%20Unlike%20traditional%0Amulti-object%20tracking%2C%20RMOT%20identifies%20and%20tracks%20objects%20and%20incorporates%0Atextual%20descriptions%20for%20object%20class%20names%2C%20making%20the%20approach%20more%0Aintuitive.%20Various%20techniques%20have%20been%20proposed%20to%20address%20this%20challenging%0Aproblem%3B%20however%2C%20most%20require%20the%20training%20of%20the%20entire%20network%20due%20to%20their%0Aend-to-end%20nature.%20Among%20these%20methods%2C%20iKUN%20has%20emerged%20as%20a%20particularly%0Apromising%20solution.%20Therefore%2C%20we%20further%20explore%20its%20pipeline%20and%20enhance%20its%0Aperformance.%20In%20this%20paper%2C%20we%20introduce%20a%20practical%20module%20dubbed%0AMemory-Efficient%20Cross-modality%20--%20MEX.%20This%20memory-efficient%20technique%20can%20be%0Adirectly%20applied%20to%20off-the-shelf%20trackers%20like%20iKUN%2C%20resulting%20in%20significant%0Aarchitectural%20improvements.%20Our%20method%20proves%20effective%20during%20inference%20on%20a%0Asingle%20GPU%20with%204%20GB%20of%20memory.%20Among%20the%20various%20benchmarks%2C%20the%20Refer-KITTI%0Adataset%2C%20which%20offers%20diverse%20autonomous%20driving%20scenes%20with%20relevant%20language%0Aexpressions%2C%20is%20particularly%20useful%20for%20studying%20this%20problem.%20Empirically%2C%20our%0Amethod%20demonstrates%20effectiveness%20and%20efficiency%20regarding%20HOTA%20tracking%0Ascores%2C%20substantially%20improving%20memory%20allocation%20and%20processing%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13875v1&entry.124074799=Read"},
{"title": "Multimodal Emotion Recognition using Audio-Video Transformer Fusion with\n  Cross Attention", "author": "Joe Dhanith P R and Shravan Venkatraman and Vigya Sharma and Santhosh Malarvannan and Modigari Narendra", "abstract": "  Understanding emotions is a fundamental aspect of human communication.\nIntegrating audio and video signals offers a more comprehensive understanding\nof emotional states compared to traditional methods that rely on a single data\nsource, such as speech or facial expressions. Despite its potential, multimodal\nemotion recognition faces significant challenges, particularly in\nsynchronization, feature extraction, and fusion of diverse data sources. To\naddress these issues, this paper introduces a novel transformer-based model\nnamed Audio-Video Transformer Fusion with Cross Attention (AVT-CA). The AVT-CA\nmodel employs a transformer fusion approach to effectively capture and\nsynchronize interlinked features from both audio and video inputs, thereby\nresolving synchronization problems. Additionally, the Cross Attention mechanism\nwithin AVT-CA selectively extracts and emphasizes critical features while\ndiscarding irrelevant ones from both modalities, addressing feature extraction\nand fusion challenges. Extensive experimental analysis conducted on the\nCMU-MOSEI, RAVDESS and CREMA-D datasets demonstrates the efficacy of the\nproposed model. The results underscore the importance of AVT-CA in developing\nprecise and reliable multimodal emotion recognition systems for practical\napplications.\n", "link": "http://arxiv.org/abs/2407.18552v3", "date": "2025-02-19", "relevancy": 2.1418, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5521}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5301}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Emotion%20Recognition%20using%20Audio-Video%20Transformer%20Fusion%20with%0A%20%20Cross%20Attention&body=Title%3A%20Multimodal%20Emotion%20Recognition%20using%20Audio-Video%20Transformer%20Fusion%20with%0A%20%20Cross%20Attention%0AAuthor%3A%20Joe%20Dhanith%20P%20R%20and%20Shravan%20Venkatraman%20and%20Vigya%20Sharma%20and%20Santhosh%20Malarvannan%20and%20Modigari%20Narendra%0AAbstract%3A%20%20%20Understanding%20emotions%20is%20a%20fundamental%20aspect%20of%20human%20communication.%0AIntegrating%20audio%20and%20video%20signals%20offers%20a%20more%20comprehensive%20understanding%0Aof%20emotional%20states%20compared%20to%20traditional%20methods%20that%20rely%20on%20a%20single%20data%0Asource%2C%20such%20as%20speech%20or%20facial%20expressions.%20Despite%20its%20potential%2C%20multimodal%0Aemotion%20recognition%20faces%20significant%20challenges%2C%20particularly%20in%0Asynchronization%2C%20feature%20extraction%2C%20and%20fusion%20of%20diverse%20data%20sources.%20To%0Aaddress%20these%20issues%2C%20this%20paper%20introduces%20a%20novel%20transformer-based%20model%0Anamed%20Audio-Video%20Transformer%20Fusion%20with%20Cross%20Attention%20%28AVT-CA%29.%20The%20AVT-CA%0Amodel%20employs%20a%20transformer%20fusion%20approach%20to%20effectively%20capture%20and%0Asynchronize%20interlinked%20features%20from%20both%20audio%20and%20video%20inputs%2C%20thereby%0Aresolving%20synchronization%20problems.%20Additionally%2C%20the%20Cross%20Attention%20mechanism%0Awithin%20AVT-CA%20selectively%20extracts%20and%20emphasizes%20critical%20features%20while%0Adiscarding%20irrelevant%20ones%20from%20both%20modalities%2C%20addressing%20feature%20extraction%0Aand%20fusion%20challenges.%20Extensive%20experimental%20analysis%20conducted%20on%20the%0ACMU-MOSEI%2C%20RAVDESS%20and%20CREMA-D%20datasets%20demonstrates%20the%20efficacy%20of%20the%0Aproposed%20model.%20The%20results%20underscore%20the%20importance%20of%20AVT-CA%20in%20developing%0Aprecise%20and%20reliable%20multimodal%20emotion%20recognition%20systems%20for%20practical%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18552v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Emotion%2520Recognition%2520using%2520Audio-Video%2520Transformer%2520Fusion%2520with%250A%2520%2520Cross%2520Attention%26entry.906535625%3DJoe%2520Dhanith%2520P%2520R%2520and%2520Shravan%2520Venkatraman%2520and%2520Vigya%2520Sharma%2520and%2520Santhosh%2520Malarvannan%2520and%2520Modigari%2520Narendra%26entry.1292438233%3D%2520%2520Understanding%2520emotions%2520is%2520a%2520fundamental%2520aspect%2520of%2520human%2520communication.%250AIntegrating%2520audio%2520and%2520video%2520signals%2520offers%2520a%2520more%2520comprehensive%2520understanding%250Aof%2520emotional%2520states%2520compared%2520to%2520traditional%2520methods%2520that%2520rely%2520on%2520a%2520single%2520data%250Asource%252C%2520such%2520as%2520speech%2520or%2520facial%2520expressions.%2520Despite%2520its%2520potential%252C%2520multimodal%250Aemotion%2520recognition%2520faces%2520significant%2520challenges%252C%2520particularly%2520in%250Asynchronization%252C%2520feature%2520extraction%252C%2520and%2520fusion%2520of%2520diverse%2520data%2520sources.%2520To%250Aaddress%2520these%2520issues%252C%2520this%2520paper%2520introduces%2520a%2520novel%2520transformer-based%2520model%250Anamed%2520Audio-Video%2520Transformer%2520Fusion%2520with%2520Cross%2520Attention%2520%2528AVT-CA%2529.%2520The%2520AVT-CA%250Amodel%2520employs%2520a%2520transformer%2520fusion%2520approach%2520to%2520effectively%2520capture%2520and%250Asynchronize%2520interlinked%2520features%2520from%2520both%2520audio%2520and%2520video%2520inputs%252C%2520thereby%250Aresolving%2520synchronization%2520problems.%2520Additionally%252C%2520the%2520Cross%2520Attention%2520mechanism%250Awithin%2520AVT-CA%2520selectively%2520extracts%2520and%2520emphasizes%2520critical%2520features%2520while%250Adiscarding%2520irrelevant%2520ones%2520from%2520both%2520modalities%252C%2520addressing%2520feature%2520extraction%250Aand%2520fusion%2520challenges.%2520Extensive%2520experimental%2520analysis%2520conducted%2520on%2520the%250ACMU-MOSEI%252C%2520RAVDESS%2520and%2520CREMA-D%2520datasets%2520demonstrates%2520the%2520efficacy%2520of%2520the%250Aproposed%2520model.%2520The%2520results%2520underscore%2520the%2520importance%2520of%2520AVT-CA%2520in%2520developing%250Aprecise%2520and%2520reliable%2520multimodal%2520emotion%2520recognition%2520systems%2520for%2520practical%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18552v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Emotion%20Recognition%20using%20Audio-Video%20Transformer%20Fusion%20with%0A%20%20Cross%20Attention&entry.906535625=Joe%20Dhanith%20P%20R%20and%20Shravan%20Venkatraman%20and%20Vigya%20Sharma%20and%20Santhosh%20Malarvannan%20and%20Modigari%20Narendra&entry.1292438233=%20%20Understanding%20emotions%20is%20a%20fundamental%20aspect%20of%20human%20communication.%0AIntegrating%20audio%20and%20video%20signals%20offers%20a%20more%20comprehensive%20understanding%0Aof%20emotional%20states%20compared%20to%20traditional%20methods%20that%20rely%20on%20a%20single%20data%0Asource%2C%20such%20as%20speech%20or%20facial%20expressions.%20Despite%20its%20potential%2C%20multimodal%0Aemotion%20recognition%20faces%20significant%20challenges%2C%20particularly%20in%0Asynchronization%2C%20feature%20extraction%2C%20and%20fusion%20of%20diverse%20data%20sources.%20To%0Aaddress%20these%20issues%2C%20this%20paper%20introduces%20a%20novel%20transformer-based%20model%0Anamed%20Audio-Video%20Transformer%20Fusion%20with%20Cross%20Attention%20%28AVT-CA%29.%20The%20AVT-CA%0Amodel%20employs%20a%20transformer%20fusion%20approach%20to%20effectively%20capture%20and%0Asynchronize%20interlinked%20features%20from%20both%20audio%20and%20video%20inputs%2C%20thereby%0Aresolving%20synchronization%20problems.%20Additionally%2C%20the%20Cross%20Attention%20mechanism%0Awithin%20AVT-CA%20selectively%20extracts%20and%20emphasizes%20critical%20features%20while%0Adiscarding%20irrelevant%20ones%20from%20both%20modalities%2C%20addressing%20feature%20extraction%0Aand%20fusion%20challenges.%20Extensive%20experimental%20analysis%20conducted%20on%20the%0ACMU-MOSEI%2C%20RAVDESS%20and%20CREMA-D%20datasets%20demonstrates%20the%20efficacy%20of%20the%0Aproposed%20model.%20The%20results%20underscore%20the%20importance%20of%20AVT-CA%20in%20developing%0Aprecise%20and%20reliable%20multimodal%20emotion%20recognition%20systems%20for%20practical%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18552v3&entry.124074799=Read"},
{"title": "Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment\n  Revealing Hidden Fault Lines in Large Language Models", "author": "Rubing Li and Jo\u00e3o Sedoc and Arun Sundararajan", "abstract": "  When encountering increasingly frequent performance improvements or cost\nreductions from a new large language model (LLM), developers of applications\nleveraging LLMs must decide whether to take advantage of these improvements or\nstay with older tried-and-tested models. Low perceived switching frictions can\nlead to choices that do not consider more subtle behavior changes that the\ntransition may induce. Our experiments use a popular game-theoretic behavioral\neconomics model of trust to show stark differences in the trusting behavior of\nOpenAI's and DeepSeek's models. We highlight a collapse in the economic trust\nbehavior of the o1-mini and o3-mini models as they reconcile profit-maximizing\nand risk-seeking with future returns from trust, and contrast it with\nDeepSeek's more sophisticated and profitable trusting behavior that stems from\nan ability to incorporate deeper concepts like forward planning and\ntheory-of-mind. As LLMs form the basis for high-stakes commercial systems, our\nresults highlight the perils of relying on LLM performance benchmarks that are\ntoo narrowly defined and suggest that careful analysis of their hidden fault\nlines should be part of any organization's AI strategy.\n", "link": "http://arxiv.org/abs/2502.12825v2", "date": "2025-02-19", "relevancy": 2.1362, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5371}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5371}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20and%20the%20Trusting%20Behavior%20of%20DeepSeek%20and%20GPT%3A%20An%20Experiment%0A%20%20Revealing%20Hidden%20Fault%20Lines%20in%20Large%20Language%20Models&body=Title%3A%20Reasoning%20and%20the%20Trusting%20Behavior%20of%20DeepSeek%20and%20GPT%3A%20An%20Experiment%0A%20%20Revealing%20Hidden%20Fault%20Lines%20in%20Large%20Language%20Models%0AAuthor%3A%20Rubing%20Li%20and%20Jo%C3%A3o%20Sedoc%20and%20Arun%20Sundararajan%0AAbstract%3A%20%20%20When%20encountering%20increasingly%20frequent%20performance%20improvements%20or%20cost%0Areductions%20from%20a%20new%20large%20language%20model%20%28LLM%29%2C%20developers%20of%20applications%0Aleveraging%20LLMs%20must%20decide%20whether%20to%20take%20advantage%20of%20these%20improvements%20or%0Astay%20with%20older%20tried-and-tested%20models.%20Low%20perceived%20switching%20frictions%20can%0Alead%20to%20choices%20that%20do%20not%20consider%20more%20subtle%20behavior%20changes%20that%20the%0Atransition%20may%20induce.%20Our%20experiments%20use%20a%20popular%20game-theoretic%20behavioral%0Aeconomics%20model%20of%20trust%20to%20show%20stark%20differences%20in%20the%20trusting%20behavior%20of%0AOpenAI%27s%20and%20DeepSeek%27s%20models.%20We%20highlight%20a%20collapse%20in%20the%20economic%20trust%0Abehavior%20of%20the%20o1-mini%20and%20o3-mini%20models%20as%20they%20reconcile%20profit-maximizing%0Aand%20risk-seeking%20with%20future%20returns%20from%20trust%2C%20and%20contrast%20it%20with%0ADeepSeek%27s%20more%20sophisticated%20and%20profitable%20trusting%20behavior%20that%20stems%20from%0Aan%20ability%20to%20incorporate%20deeper%20concepts%20like%20forward%20planning%20and%0Atheory-of-mind.%20As%20LLMs%20form%20the%20basis%20for%20high-stakes%20commercial%20systems%2C%20our%0Aresults%20highlight%20the%20perils%20of%20relying%20on%20LLM%20performance%20benchmarks%20that%20are%0Atoo%20narrowly%20defined%20and%20suggest%20that%20careful%20analysis%20of%20their%20hidden%20fault%0Alines%20should%20be%20part%20of%20any%20organization%27s%20AI%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12825v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520and%2520the%2520Trusting%2520Behavior%2520of%2520DeepSeek%2520and%2520GPT%253A%2520An%2520Experiment%250A%2520%2520Revealing%2520Hidden%2520Fault%2520Lines%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DRubing%2520Li%2520and%2520Jo%25C3%25A3o%2520Sedoc%2520and%2520Arun%2520Sundararajan%26entry.1292438233%3D%2520%2520When%2520encountering%2520increasingly%2520frequent%2520performance%2520improvements%2520or%2520cost%250Areductions%2520from%2520a%2520new%2520large%2520language%2520model%2520%2528LLM%2529%252C%2520developers%2520of%2520applications%250Aleveraging%2520LLMs%2520must%2520decide%2520whether%2520to%2520take%2520advantage%2520of%2520these%2520improvements%2520or%250Astay%2520with%2520older%2520tried-and-tested%2520models.%2520Low%2520perceived%2520switching%2520frictions%2520can%250Alead%2520to%2520choices%2520that%2520do%2520not%2520consider%2520more%2520subtle%2520behavior%2520changes%2520that%2520the%250Atransition%2520may%2520induce.%2520Our%2520experiments%2520use%2520a%2520popular%2520game-theoretic%2520behavioral%250Aeconomics%2520model%2520of%2520trust%2520to%2520show%2520stark%2520differences%2520in%2520the%2520trusting%2520behavior%2520of%250AOpenAI%2527s%2520and%2520DeepSeek%2527s%2520models.%2520We%2520highlight%2520a%2520collapse%2520in%2520the%2520economic%2520trust%250Abehavior%2520of%2520the%2520o1-mini%2520and%2520o3-mini%2520models%2520as%2520they%2520reconcile%2520profit-maximizing%250Aand%2520risk-seeking%2520with%2520future%2520returns%2520from%2520trust%252C%2520and%2520contrast%2520it%2520with%250ADeepSeek%2527s%2520more%2520sophisticated%2520and%2520profitable%2520trusting%2520behavior%2520that%2520stems%2520from%250Aan%2520ability%2520to%2520incorporate%2520deeper%2520concepts%2520like%2520forward%2520planning%2520and%250Atheory-of-mind.%2520As%2520LLMs%2520form%2520the%2520basis%2520for%2520high-stakes%2520commercial%2520systems%252C%2520our%250Aresults%2520highlight%2520the%2520perils%2520of%2520relying%2520on%2520LLM%2520performance%2520benchmarks%2520that%2520are%250Atoo%2520narrowly%2520defined%2520and%2520suggest%2520that%2520careful%2520analysis%2520of%2520their%2520hidden%2520fault%250Alines%2520should%2520be%2520part%2520of%2520any%2520organization%2527s%2520AI%2520strategy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12825v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20and%20the%20Trusting%20Behavior%20of%20DeepSeek%20and%20GPT%3A%20An%20Experiment%0A%20%20Revealing%20Hidden%20Fault%20Lines%20in%20Large%20Language%20Models&entry.906535625=Rubing%20Li%20and%20Jo%C3%A3o%20Sedoc%20and%20Arun%20Sundararajan&entry.1292438233=%20%20When%20encountering%20increasingly%20frequent%20performance%20improvements%20or%20cost%0Areductions%20from%20a%20new%20large%20language%20model%20%28LLM%29%2C%20developers%20of%20applications%0Aleveraging%20LLMs%20must%20decide%20whether%20to%20take%20advantage%20of%20these%20improvements%20or%0Astay%20with%20older%20tried-and-tested%20models.%20Low%20perceived%20switching%20frictions%20can%0Alead%20to%20choices%20that%20do%20not%20consider%20more%20subtle%20behavior%20changes%20that%20the%0Atransition%20may%20induce.%20Our%20experiments%20use%20a%20popular%20game-theoretic%20behavioral%0Aeconomics%20model%20of%20trust%20to%20show%20stark%20differences%20in%20the%20trusting%20behavior%20of%0AOpenAI%27s%20and%20DeepSeek%27s%20models.%20We%20highlight%20a%20collapse%20in%20the%20economic%20trust%0Abehavior%20of%20the%20o1-mini%20and%20o3-mini%20models%20as%20they%20reconcile%20profit-maximizing%0Aand%20risk-seeking%20with%20future%20returns%20from%20trust%2C%20and%20contrast%20it%20with%0ADeepSeek%27s%20more%20sophisticated%20and%20profitable%20trusting%20behavior%20that%20stems%20from%0Aan%20ability%20to%20incorporate%20deeper%20concepts%20like%20forward%20planning%20and%0Atheory-of-mind.%20As%20LLMs%20form%20the%20basis%20for%20high-stakes%20commercial%20systems%2C%20our%0Aresults%20highlight%20the%20perils%20of%20relying%20on%20LLM%20performance%20benchmarks%20that%20are%0Atoo%20narrowly%20defined%20and%20suggest%20that%20careful%20analysis%20of%20their%20hidden%20fault%0Alines%20should%20be%20part%20of%20any%20organization%27s%20AI%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12825v2&entry.124074799=Read"},
{"title": "Synthetic Tabular Data Generation for Imbalanced Classification: The\n  Surprising Effectiveness of an Overlap Class", "author": "Annie D'souza and Swetha M and Sunita Sarawagi", "abstract": "  Handling imbalance in class distribution when building a classifier over\ntabular data has been a problem of long-standing interest. One popular approach\nis augmenting the training dataset with synthetically generated data. While\nclassical augmentation techniques were limited to linear interpolation of\nexisting minority class examples, recently higher capacity deep generative\nmodels are providing greater promise.\n  However, handling of imbalance in class distribution when building a deep\ngenerative model is also a challenging problem, that has not been studied as\nextensively as imbalanced classifier model training. We show that\nstate-of-the-art deep generative models yield significantly lower-quality\nminority examples than majority examples. %In this paper, we start with the\nobservation that imbalanced data training of generative models trained\nimbalanced dataset which under-represent the minority class. We propose a novel\ntechnique of converting the binary class labels to ternary class labels by\nintroducing a class for the region where minority and majority distributions\noverlap. We show that just this pre-processing of the training set,\nsignificantly improves the quality of data generated spanning several\nstate-of-the-art diffusion and GAN-based models. While training the classifier\nusing synthetic data, we remove the overlap class from the training data and\njustify the reasons behind the enhanced accuracy. We perform extensive\nexperiments on four real-life datasets, five different classifiers, and five\ngenerative models demonstrating that our method enhances not only the\nsynthesizer performance of state-of-the-art models but also the classifier\nperformance.\n", "link": "http://arxiv.org/abs/2412.15657v2", "date": "2025-02-19", "relevancy": 2.1182, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5503}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5261}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Tabular%20Data%20Generation%20for%20Imbalanced%20Classification%3A%20The%0A%20%20Surprising%20Effectiveness%20of%20an%20Overlap%20Class&body=Title%3A%20Synthetic%20Tabular%20Data%20Generation%20for%20Imbalanced%20Classification%3A%20The%0A%20%20Surprising%20Effectiveness%20of%20an%20Overlap%20Class%0AAuthor%3A%20Annie%20D%27souza%20and%20Swetha%20M%20and%20Sunita%20Sarawagi%0AAbstract%3A%20%20%20Handling%20imbalance%20in%20class%20distribution%20when%20building%20a%20classifier%20over%0Atabular%20data%20has%20been%20a%20problem%20of%20long-standing%20interest.%20One%20popular%20approach%0Ais%20augmenting%20the%20training%20dataset%20with%20synthetically%20generated%20data.%20While%0Aclassical%20augmentation%20techniques%20were%20limited%20to%20linear%20interpolation%20of%0Aexisting%20minority%20class%20examples%2C%20recently%20higher%20capacity%20deep%20generative%0Amodels%20are%20providing%20greater%20promise.%0A%20%20However%2C%20handling%20of%20imbalance%20in%20class%20distribution%20when%20building%20a%20deep%0Agenerative%20model%20is%20also%20a%20challenging%20problem%2C%20that%20has%20not%20been%20studied%20as%0Aextensively%20as%20imbalanced%20classifier%20model%20training.%20We%20show%20that%0Astate-of-the-art%20deep%20generative%20models%20yield%20significantly%20lower-quality%0Aminority%20examples%20than%20majority%20examples.%20%25In%20this%20paper%2C%20we%20start%20with%20the%0Aobservation%20that%20imbalanced%20data%20training%20of%20generative%20models%20trained%0Aimbalanced%20dataset%20which%20under-represent%20the%20minority%20class.%20We%20propose%20a%20novel%0Atechnique%20of%20converting%20the%20binary%20class%20labels%20to%20ternary%20class%20labels%20by%0Aintroducing%20a%20class%20for%20the%20region%20where%20minority%20and%20majority%20distributions%0Aoverlap.%20We%20show%20that%20just%20this%20pre-processing%20of%20the%20training%20set%2C%0Asignificantly%20improves%20the%20quality%20of%20data%20generated%20spanning%20several%0Astate-of-the-art%20diffusion%20and%20GAN-based%20models.%20While%20training%20the%20classifier%0Ausing%20synthetic%20data%2C%20we%20remove%20the%20overlap%20class%20from%20the%20training%20data%20and%0Ajustify%20the%20reasons%20behind%20the%20enhanced%20accuracy.%20We%20perform%20extensive%0Aexperiments%20on%20four%20real-life%20datasets%2C%20five%20different%20classifiers%2C%20and%20five%0Agenerative%20models%20demonstrating%20that%20our%20method%20enhances%20not%20only%20the%0Asynthesizer%20performance%20of%20state-of-the-art%20models%20but%20also%20the%20classifier%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15657v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Tabular%2520Data%2520Generation%2520for%2520Imbalanced%2520Classification%253A%2520The%250A%2520%2520Surprising%2520Effectiveness%2520of%2520an%2520Overlap%2520Class%26entry.906535625%3DAnnie%2520D%2527souza%2520and%2520Swetha%2520M%2520and%2520Sunita%2520Sarawagi%26entry.1292438233%3D%2520%2520Handling%2520imbalance%2520in%2520class%2520distribution%2520when%2520building%2520a%2520classifier%2520over%250Atabular%2520data%2520has%2520been%2520a%2520problem%2520of%2520long-standing%2520interest.%2520One%2520popular%2520approach%250Ais%2520augmenting%2520the%2520training%2520dataset%2520with%2520synthetically%2520generated%2520data.%2520While%250Aclassical%2520augmentation%2520techniques%2520were%2520limited%2520to%2520linear%2520interpolation%2520of%250Aexisting%2520minority%2520class%2520examples%252C%2520recently%2520higher%2520capacity%2520deep%2520generative%250Amodels%2520are%2520providing%2520greater%2520promise.%250A%2520%2520However%252C%2520handling%2520of%2520imbalance%2520in%2520class%2520distribution%2520when%2520building%2520a%2520deep%250Agenerative%2520model%2520is%2520also%2520a%2520challenging%2520problem%252C%2520that%2520has%2520not%2520been%2520studied%2520as%250Aextensively%2520as%2520imbalanced%2520classifier%2520model%2520training.%2520We%2520show%2520that%250Astate-of-the-art%2520deep%2520generative%2520models%2520yield%2520significantly%2520lower-quality%250Aminority%2520examples%2520than%2520majority%2520examples.%2520%2525In%2520this%2520paper%252C%2520we%2520start%2520with%2520the%250Aobservation%2520that%2520imbalanced%2520data%2520training%2520of%2520generative%2520models%2520trained%250Aimbalanced%2520dataset%2520which%2520under-represent%2520the%2520minority%2520class.%2520We%2520propose%2520a%2520novel%250Atechnique%2520of%2520converting%2520the%2520binary%2520class%2520labels%2520to%2520ternary%2520class%2520labels%2520by%250Aintroducing%2520a%2520class%2520for%2520the%2520region%2520where%2520minority%2520and%2520majority%2520distributions%250Aoverlap.%2520We%2520show%2520that%2520just%2520this%2520pre-processing%2520of%2520the%2520training%2520set%252C%250Asignificantly%2520improves%2520the%2520quality%2520of%2520data%2520generated%2520spanning%2520several%250Astate-of-the-art%2520diffusion%2520and%2520GAN-based%2520models.%2520While%2520training%2520the%2520classifier%250Ausing%2520synthetic%2520data%252C%2520we%2520remove%2520the%2520overlap%2520class%2520from%2520the%2520training%2520data%2520and%250Ajustify%2520the%2520reasons%2520behind%2520the%2520enhanced%2520accuracy.%2520We%2520perform%2520extensive%250Aexperiments%2520on%2520four%2520real-life%2520datasets%252C%2520five%2520different%2520classifiers%252C%2520and%2520five%250Agenerative%2520models%2520demonstrating%2520that%2520our%2520method%2520enhances%2520not%2520only%2520the%250Asynthesizer%2520performance%2520of%2520state-of-the-art%2520models%2520but%2520also%2520the%2520classifier%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15657v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Tabular%20Data%20Generation%20for%20Imbalanced%20Classification%3A%20The%0A%20%20Surprising%20Effectiveness%20of%20an%20Overlap%20Class&entry.906535625=Annie%20D%27souza%20and%20Swetha%20M%20and%20Sunita%20Sarawagi&entry.1292438233=%20%20Handling%20imbalance%20in%20class%20distribution%20when%20building%20a%20classifier%20over%0Atabular%20data%20has%20been%20a%20problem%20of%20long-standing%20interest.%20One%20popular%20approach%0Ais%20augmenting%20the%20training%20dataset%20with%20synthetically%20generated%20data.%20While%0Aclassical%20augmentation%20techniques%20were%20limited%20to%20linear%20interpolation%20of%0Aexisting%20minority%20class%20examples%2C%20recently%20higher%20capacity%20deep%20generative%0Amodels%20are%20providing%20greater%20promise.%0A%20%20However%2C%20handling%20of%20imbalance%20in%20class%20distribution%20when%20building%20a%20deep%0Agenerative%20model%20is%20also%20a%20challenging%20problem%2C%20that%20has%20not%20been%20studied%20as%0Aextensively%20as%20imbalanced%20classifier%20model%20training.%20We%20show%20that%0Astate-of-the-art%20deep%20generative%20models%20yield%20significantly%20lower-quality%0Aminority%20examples%20than%20majority%20examples.%20%25In%20this%20paper%2C%20we%20start%20with%20the%0Aobservation%20that%20imbalanced%20data%20training%20of%20generative%20models%20trained%0Aimbalanced%20dataset%20which%20under-represent%20the%20minority%20class.%20We%20propose%20a%20novel%0Atechnique%20of%20converting%20the%20binary%20class%20labels%20to%20ternary%20class%20labels%20by%0Aintroducing%20a%20class%20for%20the%20region%20where%20minority%20and%20majority%20distributions%0Aoverlap.%20We%20show%20that%20just%20this%20pre-processing%20of%20the%20training%20set%2C%0Asignificantly%20improves%20the%20quality%20of%20data%20generated%20spanning%20several%0Astate-of-the-art%20diffusion%20and%20GAN-based%20models.%20While%20training%20the%20classifier%0Ausing%20synthetic%20data%2C%20we%20remove%20the%20overlap%20class%20from%20the%20training%20data%20and%0Ajustify%20the%20reasons%20behind%20the%20enhanced%20accuracy.%20We%20perform%20extensive%0Aexperiments%20on%20four%20real-life%20datasets%2C%20five%20different%20classifiers%2C%20and%20five%0Agenerative%20models%20demonstrating%20that%20our%20method%20enhances%20not%20only%20the%0Asynthesizer%20performance%20of%20state-of-the-art%20models%20but%20also%20the%20classifier%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15657v2&entry.124074799=Read"},
{"title": "SPEX: Scaling Feature Interaction Explanations for LLMs", "author": "Justin Singh Kang and Landon Butler and Abhineet Agarwal and Yigit Efe Erginbas and Ramtin Pedarsani and Kannan Ramchandran and Bin Yu", "abstract": "  Large language models (LLMs) have revolutionized machine learning due to\ntheir ability to capture complex interactions between input features. Popular\npost-hoc explanation methods like SHAP provide marginal feature attributions,\nwhile their extensions to interaction importances only scale to small input\nlengths ($\\approx 20$). We propose Spectral Explainer (SPEX), a model-agnostic\ninteraction attribution algorithm that efficiently scales to large input\nlengths ($\\approx 1000)$. SPEX exploits underlying natural sparsity among\ninteractions -- common in real-world data -- and applies a sparse Fourier\ntransform using a channel decoding algorithm to efficiently identify important\ninteractions. We perform experiments across three difficult long-context\ndatasets that require LLMs to utilize interactions between inputs to complete\nthe task. For large inputs, SPEX outperforms marginal attribution methods by up\nto 20% in terms of faithfully reconstructing LLM outputs. Further, SPEX\nsuccessfully identifies key features and interactions that strongly influence\nmodel output. For one of our datasets, HotpotQA, SPEX provides interactions\nthat align with human annotations. Finally, we use our model-agnostic approach\nto generate explanations to demonstrate abstract reasoning in closed-source\nLLMs (GPT-4o mini) and compositional reasoning in vision-language models.\n", "link": "http://arxiv.org/abs/2502.13870v1", "date": "2025-02-19", "relevancy": 2.1114, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5364}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5364}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPEX%3A%20Scaling%20Feature%20Interaction%20Explanations%20for%20LLMs&body=Title%3A%20SPEX%3A%20Scaling%20Feature%20Interaction%20Explanations%20for%20LLMs%0AAuthor%3A%20Justin%20Singh%20Kang%20and%20Landon%20Butler%20and%20Abhineet%20Agarwal%20and%20Yigit%20Efe%20Erginbas%20and%20Ramtin%20Pedarsani%20and%20Kannan%20Ramchandran%20and%20Bin%20Yu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20revolutionized%20machine%20learning%20due%20to%0Atheir%20ability%20to%20capture%20complex%20interactions%20between%20input%20features.%20Popular%0Apost-hoc%20explanation%20methods%20like%20SHAP%20provide%20marginal%20feature%20attributions%2C%0Awhile%20their%20extensions%20to%20interaction%20importances%20only%20scale%20to%20small%20input%0Alengths%20%28%24%5Capprox%2020%24%29.%20We%20propose%20Spectral%20Explainer%20%28SPEX%29%2C%20a%20model-agnostic%0Ainteraction%20attribution%20algorithm%20that%20efficiently%20scales%20to%20large%20input%0Alengths%20%28%24%5Capprox%201000%29%24.%20SPEX%20exploits%20underlying%20natural%20sparsity%20among%0Ainteractions%20--%20common%20in%20real-world%20data%20--%20and%20applies%20a%20sparse%20Fourier%0Atransform%20using%20a%20channel%20decoding%20algorithm%20to%20efficiently%20identify%20important%0Ainteractions.%20We%20perform%20experiments%20across%20three%20difficult%20long-context%0Adatasets%20that%20require%20LLMs%20to%20utilize%20interactions%20between%20inputs%20to%20complete%0Athe%20task.%20For%20large%20inputs%2C%20SPEX%20outperforms%20marginal%20attribution%20methods%20by%20up%0Ato%2020%25%20in%20terms%20of%20faithfully%20reconstructing%20LLM%20outputs.%20Further%2C%20SPEX%0Asuccessfully%20identifies%20key%20features%20and%20interactions%20that%20strongly%20influence%0Amodel%20output.%20For%20one%20of%20our%20datasets%2C%20HotpotQA%2C%20SPEX%20provides%20interactions%0Athat%20align%20with%20human%20annotations.%20Finally%2C%20we%20use%20our%20model-agnostic%20approach%0Ato%20generate%20explanations%20to%20demonstrate%20abstract%20reasoning%20in%20closed-source%0ALLMs%20%28GPT-4o%20mini%29%20and%20compositional%20reasoning%20in%20vision-language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPEX%253A%2520Scaling%2520Feature%2520Interaction%2520Explanations%2520for%2520LLMs%26entry.906535625%3DJustin%2520Singh%2520Kang%2520and%2520Landon%2520Butler%2520and%2520Abhineet%2520Agarwal%2520and%2520Yigit%2520Efe%2520Erginbas%2520and%2520Ramtin%2520Pedarsani%2520and%2520Kannan%2520Ramchandran%2520and%2520Bin%2520Yu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520revolutionized%2520machine%2520learning%2520due%2520to%250Atheir%2520ability%2520to%2520capture%2520complex%2520interactions%2520between%2520input%2520features.%2520Popular%250Apost-hoc%2520explanation%2520methods%2520like%2520SHAP%2520provide%2520marginal%2520feature%2520attributions%252C%250Awhile%2520their%2520extensions%2520to%2520interaction%2520importances%2520only%2520scale%2520to%2520small%2520input%250Alengths%2520%2528%2524%255Capprox%252020%2524%2529.%2520We%2520propose%2520Spectral%2520Explainer%2520%2528SPEX%2529%252C%2520a%2520model-agnostic%250Ainteraction%2520attribution%2520algorithm%2520that%2520efficiently%2520scales%2520to%2520large%2520input%250Alengths%2520%2528%2524%255Capprox%25201000%2529%2524.%2520SPEX%2520exploits%2520underlying%2520natural%2520sparsity%2520among%250Ainteractions%2520--%2520common%2520in%2520real-world%2520data%2520--%2520and%2520applies%2520a%2520sparse%2520Fourier%250Atransform%2520using%2520a%2520channel%2520decoding%2520algorithm%2520to%2520efficiently%2520identify%2520important%250Ainteractions.%2520We%2520perform%2520experiments%2520across%2520three%2520difficult%2520long-context%250Adatasets%2520that%2520require%2520LLMs%2520to%2520utilize%2520interactions%2520between%2520inputs%2520to%2520complete%250Athe%2520task.%2520For%2520large%2520inputs%252C%2520SPEX%2520outperforms%2520marginal%2520attribution%2520methods%2520by%2520up%250Ato%252020%2525%2520in%2520terms%2520of%2520faithfully%2520reconstructing%2520LLM%2520outputs.%2520Further%252C%2520SPEX%250Asuccessfully%2520identifies%2520key%2520features%2520and%2520interactions%2520that%2520strongly%2520influence%250Amodel%2520output.%2520For%2520one%2520of%2520our%2520datasets%252C%2520HotpotQA%252C%2520SPEX%2520provides%2520interactions%250Athat%2520align%2520with%2520human%2520annotations.%2520Finally%252C%2520we%2520use%2520our%2520model-agnostic%2520approach%250Ato%2520generate%2520explanations%2520to%2520demonstrate%2520abstract%2520reasoning%2520in%2520closed-source%250ALLMs%2520%2528GPT-4o%2520mini%2529%2520and%2520compositional%2520reasoning%2520in%2520vision-language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPEX%3A%20Scaling%20Feature%20Interaction%20Explanations%20for%20LLMs&entry.906535625=Justin%20Singh%20Kang%20and%20Landon%20Butler%20and%20Abhineet%20Agarwal%20and%20Yigit%20Efe%20Erginbas%20and%20Ramtin%20Pedarsani%20and%20Kannan%20Ramchandran%20and%20Bin%20Yu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20revolutionized%20machine%20learning%20due%20to%0Atheir%20ability%20to%20capture%20complex%20interactions%20between%20input%20features.%20Popular%0Apost-hoc%20explanation%20methods%20like%20SHAP%20provide%20marginal%20feature%20attributions%2C%0Awhile%20their%20extensions%20to%20interaction%20importances%20only%20scale%20to%20small%20input%0Alengths%20%28%24%5Capprox%2020%24%29.%20We%20propose%20Spectral%20Explainer%20%28SPEX%29%2C%20a%20model-agnostic%0Ainteraction%20attribution%20algorithm%20that%20efficiently%20scales%20to%20large%20input%0Alengths%20%28%24%5Capprox%201000%29%24.%20SPEX%20exploits%20underlying%20natural%20sparsity%20among%0Ainteractions%20--%20common%20in%20real-world%20data%20--%20and%20applies%20a%20sparse%20Fourier%0Atransform%20using%20a%20channel%20decoding%20algorithm%20to%20efficiently%20identify%20important%0Ainteractions.%20We%20perform%20experiments%20across%20three%20difficult%20long-context%0Adatasets%20that%20require%20LLMs%20to%20utilize%20interactions%20between%20inputs%20to%20complete%0Athe%20task.%20For%20large%20inputs%2C%20SPEX%20outperforms%20marginal%20attribution%20methods%20by%20up%0Ato%2020%25%20in%20terms%20of%20faithfully%20reconstructing%20LLM%20outputs.%20Further%2C%20SPEX%0Asuccessfully%20identifies%20key%20features%20and%20interactions%20that%20strongly%20influence%0Amodel%20output.%20For%20one%20of%20our%20datasets%2C%20HotpotQA%2C%20SPEX%20provides%20interactions%0Athat%20align%20with%20human%20annotations.%20Finally%2C%20we%20use%20our%20model-agnostic%20approach%0Ato%20generate%20explanations%20to%20demonstrate%20abstract%20reasoning%20in%20closed-source%0ALLMs%20%28GPT-4o%20mini%29%20and%20compositional%20reasoning%20in%20vision-language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13870v1&entry.124074799=Read"},
{"title": "Regularization by Neural Style Transfer for MRI Field-Transfer\n  Reconstruction with Limited Data", "author": "Guoyao Shen and Yancheng Zhu and Mengyu Li and Ryan McNaughton and Hernan Jara and Sean B. Andersson and Chad W. Farris and Stephan Anderson and Xin Zhang", "abstract": "  Recent advances in MRI reconstruction have demonstrated remarkable success\nthrough deep learning-based models. However, most existing methods rely heavily\non large-scale, task-specific datasets, making reconstruction in data-limited\nsettings a critical yet underexplored challenge. While regularization by\ndenoising (RED) leverages denoisers as priors for reconstruction, we propose\nRegularization by Neural Style Transfer (RNST), a novel framework that\nintegrates a neural style transfer (NST) engine with a denoiser to enable\nmagnetic field-transfer reconstruction. RNST generates high-field-quality\nimages from low-field inputs without requiring paired training data, leveraging\nstyle priors to address limited-data settings. Our experiment results\ndemonstrate RNST's ability to reconstruct high-quality images across diverse\nanatomical planes (axial, coronal, sagittal) and noise levels, achieving\nsuperior clarity, contrast, and structural fidelity compared to lower-field\nreferences. Crucially, RNST maintains robustness even when style and content\nimages lack exact alignment, broadening its applicability in clinical\nenvironments where precise reference matches are unavailable. By combining the\nstrengths of NST and denoising, RNST offers a scalable, data-efficient solution\nfor MRI field-transfer reconstruction, demonstrating significant potential for\nresource-limited settings.\n", "link": "http://arxiv.org/abs/2308.10968v3", "date": "2025-02-19", "relevancy": 2.1095, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5431}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5335}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regularization%20by%20Neural%20Style%20Transfer%20for%20MRI%20Field-Transfer%0A%20%20Reconstruction%20with%20Limited%20Data&body=Title%3A%20Regularization%20by%20Neural%20Style%20Transfer%20for%20MRI%20Field-Transfer%0A%20%20Reconstruction%20with%20Limited%20Data%0AAuthor%3A%20Guoyao%20Shen%20and%20Yancheng%20Zhu%20and%20Mengyu%20Li%20and%20Ryan%20McNaughton%20and%20Hernan%20Jara%20and%20Sean%20B.%20Andersson%20and%20Chad%20W.%20Farris%20and%20Stephan%20Anderson%20and%20Xin%20Zhang%0AAbstract%3A%20%20%20Recent%20advances%20in%20MRI%20reconstruction%20have%20demonstrated%20remarkable%20success%0Athrough%20deep%20learning-based%20models.%20However%2C%20most%20existing%20methods%20rely%20heavily%0Aon%20large-scale%2C%20task-specific%20datasets%2C%20making%20reconstruction%20in%20data-limited%0Asettings%20a%20critical%20yet%20underexplored%20challenge.%20While%20regularization%20by%0Adenoising%20%28RED%29%20leverages%20denoisers%20as%20priors%20for%20reconstruction%2C%20we%20propose%0ARegularization%20by%20Neural%20Style%20Transfer%20%28RNST%29%2C%20a%20novel%20framework%20that%0Aintegrates%20a%20neural%20style%20transfer%20%28NST%29%20engine%20with%20a%20denoiser%20to%20enable%0Amagnetic%20field-transfer%20reconstruction.%20RNST%20generates%20high-field-quality%0Aimages%20from%20low-field%20inputs%20without%20requiring%20paired%20training%20data%2C%20leveraging%0Astyle%20priors%20to%20address%20limited-data%20settings.%20Our%20experiment%20results%0Ademonstrate%20RNST%27s%20ability%20to%20reconstruct%20high-quality%20images%20across%20diverse%0Aanatomical%20planes%20%28axial%2C%20coronal%2C%20sagittal%29%20and%20noise%20levels%2C%20achieving%0Asuperior%20clarity%2C%20contrast%2C%20and%20structural%20fidelity%20compared%20to%20lower-field%0Areferences.%20Crucially%2C%20RNST%20maintains%20robustness%20even%20when%20style%20and%20content%0Aimages%20lack%20exact%20alignment%2C%20broadening%20its%20applicability%20in%20clinical%0Aenvironments%20where%20precise%20reference%20matches%20are%20unavailable.%20By%20combining%20the%0Astrengths%20of%20NST%20and%20denoising%2C%20RNST%20offers%20a%20scalable%2C%20data-efficient%20solution%0Afor%20MRI%20field-transfer%20reconstruction%2C%20demonstrating%20significant%20potential%20for%0Aresource-limited%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.10968v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegularization%2520by%2520Neural%2520Style%2520Transfer%2520for%2520MRI%2520Field-Transfer%250A%2520%2520Reconstruction%2520with%2520Limited%2520Data%26entry.906535625%3DGuoyao%2520Shen%2520and%2520Yancheng%2520Zhu%2520and%2520Mengyu%2520Li%2520and%2520Ryan%2520McNaughton%2520and%2520Hernan%2520Jara%2520and%2520Sean%2520B.%2520Andersson%2520and%2520Chad%2520W.%2520Farris%2520and%2520Stephan%2520Anderson%2520and%2520Xin%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520MRI%2520reconstruction%2520have%2520demonstrated%2520remarkable%2520success%250Athrough%2520deep%2520learning-based%2520models.%2520However%252C%2520most%2520existing%2520methods%2520rely%2520heavily%250Aon%2520large-scale%252C%2520task-specific%2520datasets%252C%2520making%2520reconstruction%2520in%2520data-limited%250Asettings%2520a%2520critical%2520yet%2520underexplored%2520challenge.%2520While%2520regularization%2520by%250Adenoising%2520%2528RED%2529%2520leverages%2520denoisers%2520as%2520priors%2520for%2520reconstruction%252C%2520we%2520propose%250ARegularization%2520by%2520Neural%2520Style%2520Transfer%2520%2528RNST%2529%252C%2520a%2520novel%2520framework%2520that%250Aintegrates%2520a%2520neural%2520style%2520transfer%2520%2528NST%2529%2520engine%2520with%2520a%2520denoiser%2520to%2520enable%250Amagnetic%2520field-transfer%2520reconstruction.%2520RNST%2520generates%2520high-field-quality%250Aimages%2520from%2520low-field%2520inputs%2520without%2520requiring%2520paired%2520training%2520data%252C%2520leveraging%250Astyle%2520priors%2520to%2520address%2520limited-data%2520settings.%2520Our%2520experiment%2520results%250Ademonstrate%2520RNST%2527s%2520ability%2520to%2520reconstruct%2520high-quality%2520images%2520across%2520diverse%250Aanatomical%2520planes%2520%2528axial%252C%2520coronal%252C%2520sagittal%2529%2520and%2520noise%2520levels%252C%2520achieving%250Asuperior%2520clarity%252C%2520contrast%252C%2520and%2520structural%2520fidelity%2520compared%2520to%2520lower-field%250Areferences.%2520Crucially%252C%2520RNST%2520maintains%2520robustness%2520even%2520when%2520style%2520and%2520content%250Aimages%2520lack%2520exact%2520alignment%252C%2520broadening%2520its%2520applicability%2520in%2520clinical%250Aenvironments%2520where%2520precise%2520reference%2520matches%2520are%2520unavailable.%2520By%2520combining%2520the%250Astrengths%2520of%2520NST%2520and%2520denoising%252C%2520RNST%2520offers%2520a%2520scalable%252C%2520data-efficient%2520solution%250Afor%2520MRI%2520field-transfer%2520reconstruction%252C%2520demonstrating%2520significant%2520potential%2520for%250Aresource-limited%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.10968v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regularization%20by%20Neural%20Style%20Transfer%20for%20MRI%20Field-Transfer%0A%20%20Reconstruction%20with%20Limited%20Data&entry.906535625=Guoyao%20Shen%20and%20Yancheng%20Zhu%20and%20Mengyu%20Li%20and%20Ryan%20McNaughton%20and%20Hernan%20Jara%20and%20Sean%20B.%20Andersson%20and%20Chad%20W.%20Farris%20and%20Stephan%20Anderson%20and%20Xin%20Zhang&entry.1292438233=%20%20Recent%20advances%20in%20MRI%20reconstruction%20have%20demonstrated%20remarkable%20success%0Athrough%20deep%20learning-based%20models.%20However%2C%20most%20existing%20methods%20rely%20heavily%0Aon%20large-scale%2C%20task-specific%20datasets%2C%20making%20reconstruction%20in%20data-limited%0Asettings%20a%20critical%20yet%20underexplored%20challenge.%20While%20regularization%20by%0Adenoising%20%28RED%29%20leverages%20denoisers%20as%20priors%20for%20reconstruction%2C%20we%20propose%0ARegularization%20by%20Neural%20Style%20Transfer%20%28RNST%29%2C%20a%20novel%20framework%20that%0Aintegrates%20a%20neural%20style%20transfer%20%28NST%29%20engine%20with%20a%20denoiser%20to%20enable%0Amagnetic%20field-transfer%20reconstruction.%20RNST%20generates%20high-field-quality%0Aimages%20from%20low-field%20inputs%20without%20requiring%20paired%20training%20data%2C%20leveraging%0Astyle%20priors%20to%20address%20limited-data%20settings.%20Our%20experiment%20results%0Ademonstrate%20RNST%27s%20ability%20to%20reconstruct%20high-quality%20images%20across%20diverse%0Aanatomical%20planes%20%28axial%2C%20coronal%2C%20sagittal%29%20and%20noise%20levels%2C%20achieving%0Asuperior%20clarity%2C%20contrast%2C%20and%20structural%20fidelity%20compared%20to%20lower-field%0Areferences.%20Crucially%2C%20RNST%20maintains%20robustness%20even%20when%20style%20and%20content%0Aimages%20lack%20exact%20alignment%2C%20broadening%20its%20applicability%20in%20clinical%0Aenvironments%20where%20precise%20reference%20matches%20are%20unavailable.%20By%20combining%20the%0Astrengths%20of%20NST%20and%20denoising%2C%20RNST%20offers%20a%20scalable%2C%20data-efficient%20solution%0Afor%20MRI%20field-transfer%20reconstruction%2C%20demonstrating%20significant%20potential%20for%0Aresource-limited%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.10968v3&entry.124074799=Read"},
{"title": "A Query-Driven Approach to Space-Efficient Range Searching", "author": "Dimitris Fotakis and Andreas Kalavas and Ioannis Psarros", "abstract": "  We initiate a study of a query-driven approach to designing partition trees\nfor range-searching problems. Our model assumes that a data structure is to be\nbuilt for an unknown query distribution that we can access through a sampling\noracle, and must be selected such that it optimizes a meaningful performance\nparameter on expectation. Our first contribution is to show that a near-linear\nsample of queries allows the construction of a partition tree with a\nnear-optimal expected number of nodes visited during querying. We enhance this\napproach by treating node processing as a classification problem, leveraging\nfast classifiers like shallow neural networks to obtain experimentally\nefficient query times. Our second contribution is to develop partition trees\nusing sparse geometric separators. Our preprocessing algorithm, based on a\nsample of queries, builds a balanced tree with nodes associated with separators\nthat minimize query stabs on expectation; this yields both fast processing of\neach node and a small number of visited nodes, significantly reducing query\ntime.\n", "link": "http://arxiv.org/abs/2502.13653v1", "date": "2025-02-19", "relevancy": 2.1051, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4251}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4248}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Query-Driven%20Approach%20to%20Space-Efficient%20Range%20Searching&body=Title%3A%20A%20Query-Driven%20Approach%20to%20Space-Efficient%20Range%20Searching%0AAuthor%3A%20Dimitris%20Fotakis%20and%20Andreas%20Kalavas%20and%20Ioannis%20Psarros%0AAbstract%3A%20%20%20We%20initiate%20a%20study%20of%20a%20query-driven%20approach%20to%20designing%20partition%20trees%0Afor%20range-searching%20problems.%20Our%20model%20assumes%20that%20a%20data%20structure%20is%20to%20be%0Abuilt%20for%20an%20unknown%20query%20distribution%20that%20we%20can%20access%20through%20a%20sampling%0Aoracle%2C%20and%20must%20be%20selected%20such%20that%20it%20optimizes%20a%20meaningful%20performance%0Aparameter%20on%20expectation.%20Our%20first%20contribution%20is%20to%20show%20that%20a%20near-linear%0Asample%20of%20queries%20allows%20the%20construction%20of%20a%20partition%20tree%20with%20a%0Anear-optimal%20expected%20number%20of%20nodes%20visited%20during%20querying.%20We%20enhance%20this%0Aapproach%20by%20treating%20node%20processing%20as%20a%20classification%20problem%2C%20leveraging%0Afast%20classifiers%20like%20shallow%20neural%20networks%20to%20obtain%20experimentally%0Aefficient%20query%20times.%20Our%20second%20contribution%20is%20to%20develop%20partition%20trees%0Ausing%20sparse%20geometric%20separators.%20Our%20preprocessing%20algorithm%2C%20based%20on%20a%0Asample%20of%20queries%2C%20builds%20a%20balanced%20tree%20with%20nodes%20associated%20with%20separators%0Athat%20minimize%20query%20stabs%20on%20expectation%3B%20this%20yields%20both%20fast%20processing%20of%0Aeach%20node%20and%20a%20small%20number%20of%20visited%20nodes%2C%20significantly%20reducing%20query%0Atime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Query-Driven%2520Approach%2520to%2520Space-Efficient%2520Range%2520Searching%26entry.906535625%3DDimitris%2520Fotakis%2520and%2520Andreas%2520Kalavas%2520and%2520Ioannis%2520Psarros%26entry.1292438233%3D%2520%2520We%2520initiate%2520a%2520study%2520of%2520a%2520query-driven%2520approach%2520to%2520designing%2520partition%2520trees%250Afor%2520range-searching%2520problems.%2520Our%2520model%2520assumes%2520that%2520a%2520data%2520structure%2520is%2520to%2520be%250Abuilt%2520for%2520an%2520unknown%2520query%2520distribution%2520that%2520we%2520can%2520access%2520through%2520a%2520sampling%250Aoracle%252C%2520and%2520must%2520be%2520selected%2520such%2520that%2520it%2520optimizes%2520a%2520meaningful%2520performance%250Aparameter%2520on%2520expectation.%2520Our%2520first%2520contribution%2520is%2520to%2520show%2520that%2520a%2520near-linear%250Asample%2520of%2520queries%2520allows%2520the%2520construction%2520of%2520a%2520partition%2520tree%2520with%2520a%250Anear-optimal%2520expected%2520number%2520of%2520nodes%2520visited%2520during%2520querying.%2520We%2520enhance%2520this%250Aapproach%2520by%2520treating%2520node%2520processing%2520as%2520a%2520classification%2520problem%252C%2520leveraging%250Afast%2520classifiers%2520like%2520shallow%2520neural%2520networks%2520to%2520obtain%2520experimentally%250Aefficient%2520query%2520times.%2520Our%2520second%2520contribution%2520is%2520to%2520develop%2520partition%2520trees%250Ausing%2520sparse%2520geometric%2520separators.%2520Our%2520preprocessing%2520algorithm%252C%2520based%2520on%2520a%250Asample%2520of%2520queries%252C%2520builds%2520a%2520balanced%2520tree%2520with%2520nodes%2520associated%2520with%2520separators%250Athat%2520minimize%2520query%2520stabs%2520on%2520expectation%253B%2520this%2520yields%2520both%2520fast%2520processing%2520of%250Aeach%2520node%2520and%2520a%2520small%2520number%2520of%2520visited%2520nodes%252C%2520significantly%2520reducing%2520query%250Atime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Query-Driven%20Approach%20to%20Space-Efficient%20Range%20Searching&entry.906535625=Dimitris%20Fotakis%20and%20Andreas%20Kalavas%20and%20Ioannis%20Psarros&entry.1292438233=%20%20We%20initiate%20a%20study%20of%20a%20query-driven%20approach%20to%20designing%20partition%20trees%0Afor%20range-searching%20problems.%20Our%20model%20assumes%20that%20a%20data%20structure%20is%20to%20be%0Abuilt%20for%20an%20unknown%20query%20distribution%20that%20we%20can%20access%20through%20a%20sampling%0Aoracle%2C%20and%20must%20be%20selected%20such%20that%20it%20optimizes%20a%20meaningful%20performance%0Aparameter%20on%20expectation.%20Our%20first%20contribution%20is%20to%20show%20that%20a%20near-linear%0Asample%20of%20queries%20allows%20the%20construction%20of%20a%20partition%20tree%20with%20a%0Anear-optimal%20expected%20number%20of%20nodes%20visited%20during%20querying.%20We%20enhance%20this%0Aapproach%20by%20treating%20node%20processing%20as%20a%20classification%20problem%2C%20leveraging%0Afast%20classifiers%20like%20shallow%20neural%20networks%20to%20obtain%20experimentally%0Aefficient%20query%20times.%20Our%20second%20contribution%20is%20to%20develop%20partition%20trees%0Ausing%20sparse%20geometric%20separators.%20Our%20preprocessing%20algorithm%2C%20based%20on%20a%0Asample%20of%20queries%2C%20builds%20a%20balanced%20tree%20with%20nodes%20associated%20with%20separators%0Athat%20minimize%20query%20stabs%20on%20expectation%3B%20this%20yields%20both%20fast%20processing%20of%0Aeach%20node%20and%20a%20small%20number%20of%20visited%20nodes%2C%20significantly%20reducing%20query%0Atime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13653v1&entry.124074799=Read"},
{"title": "GQSA: Group Quantization and Sparsity for Accelerating Large Language\n  Model Inference", "author": "Chao Zeng and Songwei Liu and Shu Yang and Fangmin Chen and Xing Mei and Lean Fu", "abstract": "  Model compression has emerged as a mainstream solution to reduce memory usage\nand computational overhead. This paper presents Group Quantization and Sparse\nAcceleration (GQSA), a novel compression technique tailored for LLMs.\nTraditional methods typically focus exclusively on either quantization or\nsparsification, but relying on a single strategy often results in significant\nperformance loss at high compression rates. In contrast, GQSA integrates\nquantization and sparsification in a tightly coupled manner, leveraging\nGPU-friendly structured group sparsity and quantization for efficient\nacceleration. Building upon system-algorithm co-design principles, we propose a\ntwo-stage sparse optimization strategy that ensures the performance superiority\nof the compressed model. On the engine side, we introduce a \"task-centric\"\nparallel strategy, which, to the best of our knowledge, is the first\napplication in the domain of sparse computing. Compared to the traditional 2:4\nsparse method, the GQSA offers a more flexible and adjustable sparsity rate, as\nwell as a higher weight compression rate, and is efficiently compatible with\nweight-only quantization methods. Experimental results demonstrate that, under\nthe GQSA W4S50% compression setting, the model's accuracy surpasses that of\nboth 2:4 pruning and W2 quantization. Furthermore, at the inference level, GQSA\noutperforms W2 by 1.26$\\times$ and 2:4 pruning by 2.35$\\times$ in terms of\nspeed.\n", "link": "http://arxiv.org/abs/2412.17560v2", "date": "2025-02-19", "relevancy": 2.1039, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5412}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5352}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GQSA%3A%20Group%20Quantization%20and%20Sparsity%20for%20Accelerating%20Large%20Language%0A%20%20Model%20Inference&body=Title%3A%20GQSA%3A%20Group%20Quantization%20and%20Sparsity%20for%20Accelerating%20Large%20Language%0A%20%20Model%20Inference%0AAuthor%3A%20Chao%20Zeng%20and%20Songwei%20Liu%20and%20Shu%20Yang%20and%20Fangmin%20Chen%20and%20Xing%20Mei%20and%20Lean%20Fu%0AAbstract%3A%20%20%20Model%20compression%20has%20emerged%20as%20a%20mainstream%20solution%20to%20reduce%20memory%20usage%0Aand%20computational%20overhead.%20This%20paper%20presents%20Group%20Quantization%20and%20Sparse%0AAcceleration%20%28GQSA%29%2C%20a%20novel%20compression%20technique%20tailored%20for%20LLMs.%0ATraditional%20methods%20typically%20focus%20exclusively%20on%20either%20quantization%20or%0Asparsification%2C%20but%20relying%20on%20a%20single%20strategy%20often%20results%20in%20significant%0Aperformance%20loss%20at%20high%20compression%20rates.%20In%20contrast%2C%20GQSA%20integrates%0Aquantization%20and%20sparsification%20in%20a%20tightly%20coupled%20manner%2C%20leveraging%0AGPU-friendly%20structured%20group%20sparsity%20and%20quantization%20for%20efficient%0Aacceleration.%20Building%20upon%20system-algorithm%20co-design%20principles%2C%20we%20propose%20a%0Atwo-stage%20sparse%20optimization%20strategy%20that%20ensures%20the%20performance%20superiority%0Aof%20the%20compressed%20model.%20On%20the%20engine%20side%2C%20we%20introduce%20a%20%22task-centric%22%0Aparallel%20strategy%2C%20which%2C%20to%20the%20best%20of%20our%20knowledge%2C%20is%20the%20first%0Aapplication%20in%20the%20domain%20of%20sparse%20computing.%20Compared%20to%20the%20traditional%202%3A4%0Asparse%20method%2C%20the%20GQSA%20offers%20a%20more%20flexible%20and%20adjustable%20sparsity%20rate%2C%20as%0Awell%20as%20a%20higher%20weight%20compression%20rate%2C%20and%20is%20efficiently%20compatible%20with%0Aweight-only%20quantization%20methods.%20Experimental%20results%20demonstrate%20that%2C%20under%0Athe%20GQSA%20W4S50%25%20compression%20setting%2C%20the%20model%27s%20accuracy%20surpasses%20that%20of%0Aboth%202%3A4%20pruning%20and%20W2%20quantization.%20Furthermore%2C%20at%20the%20inference%20level%2C%20GQSA%0Aoutperforms%20W2%20by%201.26%24%5Ctimes%24%20and%202%3A4%20pruning%20by%202.35%24%5Ctimes%24%20in%20terms%20of%0Aspeed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17560v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGQSA%253A%2520Group%2520Quantization%2520and%2520Sparsity%2520for%2520Accelerating%2520Large%2520Language%250A%2520%2520Model%2520Inference%26entry.906535625%3DChao%2520Zeng%2520and%2520Songwei%2520Liu%2520and%2520Shu%2520Yang%2520and%2520Fangmin%2520Chen%2520and%2520Xing%2520Mei%2520and%2520Lean%2520Fu%26entry.1292438233%3D%2520%2520Model%2520compression%2520has%2520emerged%2520as%2520a%2520mainstream%2520solution%2520to%2520reduce%2520memory%2520usage%250Aand%2520computational%2520overhead.%2520This%2520paper%2520presents%2520Group%2520Quantization%2520and%2520Sparse%250AAcceleration%2520%2528GQSA%2529%252C%2520a%2520novel%2520compression%2520technique%2520tailored%2520for%2520LLMs.%250ATraditional%2520methods%2520typically%2520focus%2520exclusively%2520on%2520either%2520quantization%2520or%250Asparsification%252C%2520but%2520relying%2520on%2520a%2520single%2520strategy%2520often%2520results%2520in%2520significant%250Aperformance%2520loss%2520at%2520high%2520compression%2520rates.%2520In%2520contrast%252C%2520GQSA%2520integrates%250Aquantization%2520and%2520sparsification%2520in%2520a%2520tightly%2520coupled%2520manner%252C%2520leveraging%250AGPU-friendly%2520structured%2520group%2520sparsity%2520and%2520quantization%2520for%2520efficient%250Aacceleration.%2520Building%2520upon%2520system-algorithm%2520co-design%2520principles%252C%2520we%2520propose%2520a%250Atwo-stage%2520sparse%2520optimization%2520strategy%2520that%2520ensures%2520the%2520performance%2520superiority%250Aof%2520the%2520compressed%2520model.%2520On%2520the%2520engine%2520side%252C%2520we%2520introduce%2520a%2520%2522task-centric%2522%250Aparallel%2520strategy%252C%2520which%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520is%2520the%2520first%250Aapplication%2520in%2520the%2520domain%2520of%2520sparse%2520computing.%2520Compared%2520to%2520the%2520traditional%25202%253A4%250Asparse%2520method%252C%2520the%2520GQSA%2520offers%2520a%2520more%2520flexible%2520and%2520adjustable%2520sparsity%2520rate%252C%2520as%250Awell%2520as%2520a%2520higher%2520weight%2520compression%2520rate%252C%2520and%2520is%2520efficiently%2520compatible%2520with%250Aweight-only%2520quantization%2520methods.%2520Experimental%2520results%2520demonstrate%2520that%252C%2520under%250Athe%2520GQSA%2520W4S50%2525%2520compression%2520setting%252C%2520the%2520model%2527s%2520accuracy%2520surpasses%2520that%2520of%250Aboth%25202%253A4%2520pruning%2520and%2520W2%2520quantization.%2520Furthermore%252C%2520at%2520the%2520inference%2520level%252C%2520GQSA%250Aoutperforms%2520W2%2520by%25201.26%2524%255Ctimes%2524%2520and%25202%253A4%2520pruning%2520by%25202.35%2524%255Ctimes%2524%2520in%2520terms%2520of%250Aspeed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17560v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GQSA%3A%20Group%20Quantization%20and%20Sparsity%20for%20Accelerating%20Large%20Language%0A%20%20Model%20Inference&entry.906535625=Chao%20Zeng%20and%20Songwei%20Liu%20and%20Shu%20Yang%20and%20Fangmin%20Chen%20and%20Xing%20Mei%20and%20Lean%20Fu&entry.1292438233=%20%20Model%20compression%20has%20emerged%20as%20a%20mainstream%20solution%20to%20reduce%20memory%20usage%0Aand%20computational%20overhead.%20This%20paper%20presents%20Group%20Quantization%20and%20Sparse%0AAcceleration%20%28GQSA%29%2C%20a%20novel%20compression%20technique%20tailored%20for%20LLMs.%0ATraditional%20methods%20typically%20focus%20exclusively%20on%20either%20quantization%20or%0Asparsification%2C%20but%20relying%20on%20a%20single%20strategy%20often%20results%20in%20significant%0Aperformance%20loss%20at%20high%20compression%20rates.%20In%20contrast%2C%20GQSA%20integrates%0Aquantization%20and%20sparsification%20in%20a%20tightly%20coupled%20manner%2C%20leveraging%0AGPU-friendly%20structured%20group%20sparsity%20and%20quantization%20for%20efficient%0Aacceleration.%20Building%20upon%20system-algorithm%20co-design%20principles%2C%20we%20propose%20a%0Atwo-stage%20sparse%20optimization%20strategy%20that%20ensures%20the%20performance%20superiority%0Aof%20the%20compressed%20model.%20On%20the%20engine%20side%2C%20we%20introduce%20a%20%22task-centric%22%0Aparallel%20strategy%2C%20which%2C%20to%20the%20best%20of%20our%20knowledge%2C%20is%20the%20first%0Aapplication%20in%20the%20domain%20of%20sparse%20computing.%20Compared%20to%20the%20traditional%202%3A4%0Asparse%20method%2C%20the%20GQSA%20offers%20a%20more%20flexible%20and%20adjustable%20sparsity%20rate%2C%20as%0Awell%20as%20a%20higher%20weight%20compression%20rate%2C%20and%20is%20efficiently%20compatible%20with%0Aweight-only%20quantization%20methods.%20Experimental%20results%20demonstrate%20that%2C%20under%0Athe%20GQSA%20W4S50%25%20compression%20setting%2C%20the%20model%27s%20accuracy%20surpasses%20that%20of%0Aboth%202%3A4%20pruning%20and%20W2%20quantization.%20Furthermore%2C%20at%20the%20inference%20level%2C%20GQSA%0Aoutperforms%20W2%20by%201.26%24%5Ctimes%24%20and%202%3A4%20pruning%20by%202.35%24%5Ctimes%24%20in%20terms%20of%0Aspeed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17560v2&entry.124074799=Read"},
{"title": "Mixup Regularization: A Probabilistic Perspective", "author": "Yousef El-Laham and Niccolo Dalmasso and Svitlana Vyetrenko and Vamsi Potluru and Manuela Veloso", "abstract": "  In recent years, mixup regularization has gained popularity as an effective\nway to improve the generalization performance of deep learning models by\ntraining on convex combinations of training data. While many mixup variants\nhave been explored, the proper adoption of the technique to conditional density\nestimation and probabilistic machine learning remains relatively unexplored.\nThis work introduces a novel framework for mixup regularization based on\nprobabilistic fusion that is better suited for conditional density estimation\ntasks. For data distributed according to a member of the exponential family, we\nshow that likelihood functions can be analytically fused using log-linear\npooling. We further propose an extension of probabilistic mixup, which allows\nfor fusion of inputs at an arbitrary intermediate layer of the neural network.\nWe provide a theoretical analysis comparing our approach to standard mixup\nvariants. Empirical results on synthetic and real datasets demonstrate the\nbenefits of our proposed framework compared to existing mixup variants.\n", "link": "http://arxiv.org/abs/2502.13825v1", "date": "2025-02-19", "relevancy": 2.1033, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5412}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.541}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixup%20Regularization%3A%20A%20Probabilistic%20Perspective&body=Title%3A%20Mixup%20Regularization%3A%20A%20Probabilistic%20Perspective%0AAuthor%3A%20Yousef%20El-Laham%20and%20Niccolo%20Dalmasso%20and%20Svitlana%20Vyetrenko%20and%20Vamsi%20Potluru%20and%20Manuela%20Veloso%0AAbstract%3A%20%20%20In%20recent%20years%2C%20mixup%20regularization%20has%20gained%20popularity%20as%20an%20effective%0Away%20to%20improve%20the%20generalization%20performance%20of%20deep%20learning%20models%20by%0Atraining%20on%20convex%20combinations%20of%20training%20data.%20While%20many%20mixup%20variants%0Ahave%20been%20explored%2C%20the%20proper%20adoption%20of%20the%20technique%20to%20conditional%20density%0Aestimation%20and%20probabilistic%20machine%20learning%20remains%20relatively%20unexplored.%0AThis%20work%20introduces%20a%20novel%20framework%20for%20mixup%20regularization%20based%20on%0Aprobabilistic%20fusion%20that%20is%20better%20suited%20for%20conditional%20density%20estimation%0Atasks.%20For%20data%20distributed%20according%20to%20a%20member%20of%20the%20exponential%20family%2C%20we%0Ashow%20that%20likelihood%20functions%20can%20be%20analytically%20fused%20using%20log-linear%0Apooling.%20We%20further%20propose%20an%20extension%20of%20probabilistic%20mixup%2C%20which%20allows%0Afor%20fusion%20of%20inputs%20at%20an%20arbitrary%20intermediate%20layer%20of%20the%20neural%20network.%0AWe%20provide%20a%20theoretical%20analysis%20comparing%20our%20approach%20to%20standard%20mixup%0Avariants.%20Empirical%20results%20on%20synthetic%20and%20real%20datasets%20demonstrate%20the%0Abenefits%20of%20our%20proposed%20framework%20compared%20to%20existing%20mixup%20variants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixup%2520Regularization%253A%2520A%2520Probabilistic%2520Perspective%26entry.906535625%3DYousef%2520El-Laham%2520and%2520Niccolo%2520Dalmasso%2520and%2520Svitlana%2520Vyetrenko%2520and%2520Vamsi%2520Potluru%2520and%2520Manuela%2520Veloso%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520mixup%2520regularization%2520has%2520gained%2520popularity%2520as%2520an%2520effective%250Away%2520to%2520improve%2520the%2520generalization%2520performance%2520of%2520deep%2520learning%2520models%2520by%250Atraining%2520on%2520convex%2520combinations%2520of%2520training%2520data.%2520While%2520many%2520mixup%2520variants%250Ahave%2520been%2520explored%252C%2520the%2520proper%2520adoption%2520of%2520the%2520technique%2520to%2520conditional%2520density%250Aestimation%2520and%2520probabilistic%2520machine%2520learning%2520remains%2520relatively%2520unexplored.%250AThis%2520work%2520introduces%2520a%2520novel%2520framework%2520for%2520mixup%2520regularization%2520based%2520on%250Aprobabilistic%2520fusion%2520that%2520is%2520better%2520suited%2520for%2520conditional%2520density%2520estimation%250Atasks.%2520For%2520data%2520distributed%2520according%2520to%2520a%2520member%2520of%2520the%2520exponential%2520family%252C%2520we%250Ashow%2520that%2520likelihood%2520functions%2520can%2520be%2520analytically%2520fused%2520using%2520log-linear%250Apooling.%2520We%2520further%2520propose%2520an%2520extension%2520of%2520probabilistic%2520mixup%252C%2520which%2520allows%250Afor%2520fusion%2520of%2520inputs%2520at%2520an%2520arbitrary%2520intermediate%2520layer%2520of%2520the%2520neural%2520network.%250AWe%2520provide%2520a%2520theoretical%2520analysis%2520comparing%2520our%2520approach%2520to%2520standard%2520mixup%250Avariants.%2520Empirical%2520results%2520on%2520synthetic%2520and%2520real%2520datasets%2520demonstrate%2520the%250Abenefits%2520of%2520our%2520proposed%2520framework%2520compared%2520to%2520existing%2520mixup%2520variants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixup%20Regularization%3A%20A%20Probabilistic%20Perspective&entry.906535625=Yousef%20El-Laham%20and%20Niccolo%20Dalmasso%20and%20Svitlana%20Vyetrenko%20and%20Vamsi%20Potluru%20and%20Manuela%20Veloso&entry.1292438233=%20%20In%20recent%20years%2C%20mixup%20regularization%20has%20gained%20popularity%20as%20an%20effective%0Away%20to%20improve%20the%20generalization%20performance%20of%20deep%20learning%20models%20by%0Atraining%20on%20convex%20combinations%20of%20training%20data.%20While%20many%20mixup%20variants%0Ahave%20been%20explored%2C%20the%20proper%20adoption%20of%20the%20technique%20to%20conditional%20density%0Aestimation%20and%20probabilistic%20machine%20learning%20remains%20relatively%20unexplored.%0AThis%20work%20introduces%20a%20novel%20framework%20for%20mixup%20regularization%20based%20on%0Aprobabilistic%20fusion%20that%20is%20better%20suited%20for%20conditional%20density%20estimation%0Atasks.%20For%20data%20distributed%20according%20to%20a%20member%20of%20the%20exponential%20family%2C%20we%0Ashow%20that%20likelihood%20functions%20can%20be%20analytically%20fused%20using%20log-linear%0Apooling.%20We%20further%20propose%20an%20extension%20of%20probabilistic%20mixup%2C%20which%20allows%0Afor%20fusion%20of%20inputs%20at%20an%20arbitrary%20intermediate%20layer%20of%20the%20neural%20network.%0AWe%20provide%20a%20theoretical%20analysis%20comparing%20our%20approach%20to%20standard%20mixup%0Avariants.%20Empirical%20results%20on%20synthetic%20and%20real%20datasets%20demonstrate%20the%0Abenefits%20of%20our%20proposed%20framework%20compared%20to%20existing%20mixup%20variants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13825v1&entry.124074799=Read"},
{"title": "Tight Generalization Bounds for Large-Margin Halfspaces", "author": "Kasper Green Larsen and Natascha Schalburg", "abstract": "  We prove the first generalization bound for large-margin halfspaces that is\nasymptotically tight in the tradeoff between the margin, the fraction of\ntraining points with the given margin, the failure probability and the number\nof training points.\n", "link": "http://arxiv.org/abs/2502.13692v1", "date": "2025-02-19", "relevancy": 2.1009, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4248}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4233}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tight%20Generalization%20Bounds%20for%20Large-Margin%20Halfspaces&body=Title%3A%20Tight%20Generalization%20Bounds%20for%20Large-Margin%20Halfspaces%0AAuthor%3A%20Kasper%20Green%20Larsen%20and%20Natascha%20Schalburg%0AAbstract%3A%20%20%20We%20prove%20the%20first%20generalization%20bound%20for%20large-margin%20halfspaces%20that%20is%0Aasymptotically%20tight%20in%20the%20tradeoff%20between%20the%20margin%2C%20the%20fraction%20of%0Atraining%20points%20with%20the%20given%20margin%2C%20the%20failure%20probability%20and%20the%20number%0Aof%20training%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTight%2520Generalization%2520Bounds%2520for%2520Large-Margin%2520Halfspaces%26entry.906535625%3DKasper%2520Green%2520Larsen%2520and%2520Natascha%2520Schalburg%26entry.1292438233%3D%2520%2520We%2520prove%2520the%2520first%2520generalization%2520bound%2520for%2520large-margin%2520halfspaces%2520that%2520is%250Aasymptotically%2520tight%2520in%2520the%2520tradeoff%2520between%2520the%2520margin%252C%2520the%2520fraction%2520of%250Atraining%2520points%2520with%2520the%2520given%2520margin%252C%2520the%2520failure%2520probability%2520and%2520the%2520number%250Aof%2520training%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tight%20Generalization%20Bounds%20for%20Large-Margin%20Halfspaces&entry.906535625=Kasper%20Green%20Larsen%20and%20Natascha%20Schalburg&entry.1292438233=%20%20We%20prove%20the%20first%20generalization%20bound%20for%20large-margin%20halfspaces%20that%20is%0Aasymptotically%20tight%20in%20the%20tradeoff%20between%20the%20margin%2C%20the%20fraction%20of%0Atraining%20points%20with%20the%20given%20margin%2C%20the%20failure%20probability%20and%20the%20number%0Aof%20training%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13692v1&entry.124074799=Read"},
{"title": "Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs\n  with Refined Values", "author": "Hongbo Zhang and Han Cui and Guangsheng Bao and Linyi Yang and Jun Wang and Yue Zhang", "abstract": "  We introduce Direct Value Optimization (DVO), an innovative reinforcement\nlearning framework for enhancing large language models in complex reasoning\ntasks. Unlike traditional methods relying on preference labels, DVO utilizes\nvalue signals at individual reasoning steps, optimizing models via a mean\nsquared error loss. The key benefit of DVO lies in its fine-grained\nsupervision, circumventing the need for labor-intensive human annotations.\nTarget values within the DVO are estimated using either Monte Carlo Tree Search\nor an outcome value model. Our empirical analysis on both mathematical and\ncommonsense reasoning tasks shows that DVO consistently outperforms existing\noffline preference optimization techniques, even with fewer training steps.\nThese findings underscore the importance of value signals in advancing\nreasoning capabilities and highlight DVO as a superior methodology under\nscenarios lacking explicit human preference information.\n", "link": "http://arxiv.org/abs/2502.13723v1", "date": "2025-02-19", "relevancy": 2.0937, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Direct%20Value%20Optimization%3A%20Improving%20Chain-of-Thought%20Reasoning%20in%20LLMs%0A%20%20with%20Refined%20Values&body=Title%3A%20Direct%20Value%20Optimization%3A%20Improving%20Chain-of-Thought%20Reasoning%20in%20LLMs%0A%20%20with%20Refined%20Values%0AAuthor%3A%20Hongbo%20Zhang%20and%20Han%20Cui%20and%20Guangsheng%20Bao%20and%20Linyi%20Yang%20and%20Jun%20Wang%20and%20Yue%20Zhang%0AAbstract%3A%20%20%20We%20introduce%20Direct%20Value%20Optimization%20%28DVO%29%2C%20an%20innovative%20reinforcement%0Alearning%20framework%20for%20enhancing%20large%20language%20models%20in%20complex%20reasoning%0Atasks.%20Unlike%20traditional%20methods%20relying%20on%20preference%20labels%2C%20DVO%20utilizes%0Avalue%20signals%20at%20individual%20reasoning%20steps%2C%20optimizing%20models%20via%20a%20mean%0Asquared%20error%20loss.%20The%20key%20benefit%20of%20DVO%20lies%20in%20its%20fine-grained%0Asupervision%2C%20circumventing%20the%20need%20for%20labor-intensive%20human%20annotations.%0ATarget%20values%20within%20the%20DVO%20are%20estimated%20using%20either%20Monte%20Carlo%20Tree%20Search%0Aor%20an%20outcome%20value%20model.%20Our%20empirical%20analysis%20on%20both%20mathematical%20and%0Acommonsense%20reasoning%20tasks%20shows%20that%20DVO%20consistently%20outperforms%20existing%0Aoffline%20preference%20optimization%20techniques%2C%20even%20with%20fewer%20training%20steps.%0AThese%20findings%20underscore%20the%20importance%20of%20value%20signals%20in%20advancing%0Areasoning%20capabilities%20and%20highlight%20DVO%20as%20a%20superior%20methodology%20under%0Ascenarios%20lacking%20explicit%20human%20preference%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirect%2520Value%2520Optimization%253A%2520Improving%2520Chain-of-Thought%2520Reasoning%2520in%2520LLMs%250A%2520%2520with%2520Refined%2520Values%26entry.906535625%3DHongbo%2520Zhang%2520and%2520Han%2520Cui%2520and%2520Guangsheng%2520Bao%2520and%2520Linyi%2520Yang%2520and%2520Jun%2520Wang%2520and%2520Yue%2520Zhang%26entry.1292438233%3D%2520%2520We%2520introduce%2520Direct%2520Value%2520Optimization%2520%2528DVO%2529%252C%2520an%2520innovative%2520reinforcement%250Alearning%2520framework%2520for%2520enhancing%2520large%2520language%2520models%2520in%2520complex%2520reasoning%250Atasks.%2520Unlike%2520traditional%2520methods%2520relying%2520on%2520preference%2520labels%252C%2520DVO%2520utilizes%250Avalue%2520signals%2520at%2520individual%2520reasoning%2520steps%252C%2520optimizing%2520models%2520via%2520a%2520mean%250Asquared%2520error%2520loss.%2520The%2520key%2520benefit%2520of%2520DVO%2520lies%2520in%2520its%2520fine-grained%250Asupervision%252C%2520circumventing%2520the%2520need%2520for%2520labor-intensive%2520human%2520annotations.%250ATarget%2520values%2520within%2520the%2520DVO%2520are%2520estimated%2520using%2520either%2520Monte%2520Carlo%2520Tree%2520Search%250Aor%2520an%2520outcome%2520value%2520model.%2520Our%2520empirical%2520analysis%2520on%2520both%2520mathematical%2520and%250Acommonsense%2520reasoning%2520tasks%2520shows%2520that%2520DVO%2520consistently%2520outperforms%2520existing%250Aoffline%2520preference%2520optimization%2520techniques%252C%2520even%2520with%2520fewer%2520training%2520steps.%250AThese%2520findings%2520underscore%2520the%2520importance%2520of%2520value%2520signals%2520in%2520advancing%250Areasoning%2520capabilities%2520and%2520highlight%2520DVO%2520as%2520a%2520superior%2520methodology%2520under%250Ascenarios%2520lacking%2520explicit%2520human%2520preference%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Direct%20Value%20Optimization%3A%20Improving%20Chain-of-Thought%20Reasoning%20in%20LLMs%0A%20%20with%20Refined%20Values&entry.906535625=Hongbo%20Zhang%20and%20Han%20Cui%20and%20Guangsheng%20Bao%20and%20Linyi%20Yang%20and%20Jun%20Wang%20and%20Yue%20Zhang&entry.1292438233=%20%20We%20introduce%20Direct%20Value%20Optimization%20%28DVO%29%2C%20an%20innovative%20reinforcement%0Alearning%20framework%20for%20enhancing%20large%20language%20models%20in%20complex%20reasoning%0Atasks.%20Unlike%20traditional%20methods%20relying%20on%20preference%20labels%2C%20DVO%20utilizes%0Avalue%20signals%20at%20individual%20reasoning%20steps%2C%20optimizing%20models%20via%20a%20mean%0Asquared%20error%20loss.%20The%20key%20benefit%20of%20DVO%20lies%20in%20its%20fine-grained%0Asupervision%2C%20circumventing%20the%20need%20for%20labor-intensive%20human%20annotations.%0ATarget%20values%20within%20the%20DVO%20are%20estimated%20using%20either%20Monte%20Carlo%20Tree%20Search%0Aor%20an%20outcome%20value%20model.%20Our%20empirical%20analysis%20on%20both%20mathematical%20and%0Acommonsense%20reasoning%20tasks%20shows%20that%20DVO%20consistently%20outperforms%20existing%0Aoffline%20preference%20optimization%20techniques%2C%20even%20with%20fewer%20training%20steps.%0AThese%20findings%20underscore%20the%20importance%20of%20value%20signals%20in%20advancing%0Areasoning%20capabilities%20and%20highlight%20DVO%20as%20a%20superior%20methodology%20under%0Ascenarios%20lacking%20explicit%20human%20preference%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13723v1&entry.124074799=Read"},
{"title": "CardiacMamba: A Multimodal RGB-RF Fusion Framework with State Space\n  Models for Remote Physiological Measurement", "author": "Zheng Wu and Yiping Xie and Bo Zhao and Jiguang He and Fei Luo and Ning Deng and Zitong Yu", "abstract": "  Heart rate (HR) estimation via remote photoplethysmography (rPPG) offers a\nnon-invasive solution for health monitoring. However, traditional\nsingle-modality approaches (RGB or Radio Frequency (RF)) face challenges in\nbalancing robustness and accuracy due to lighting variations, motion artifacts,\nand skin tone bias. In this paper, we propose CardiacMamba, a multimodal RGB-RF\nfusion framework that leverages the complementary strengths of both modalities.\nIt introduces the Temporal Difference Mamba Module (TDMM) to capture dynamic\nchanges in RF signals using timing differences between frames, enhancing the\nextraction of local and global features. Additionally, CardiacMamba employs a\nBidirectional SSM for cross-modal alignment and a Channel-wise Fast Fourier\nTransform (CFFT) to effectively capture and refine the frequency domain\ncharacteristics of RGB and RF signals, ultimately improving heart rate\nestimation accuracy and periodicity detection. Extensive experiments on the\nEquiPleth dataset demonstrate state-of-the-art performance, achieving marked\nimprovements in accuracy and robustness. CardiacMamba significantly mitigates\nskin tone bias, reducing performance disparities across demographic groups, and\nmaintains resilience under missing-modality scenarios. By addressing critical\nchallenges in fairness, adaptability, and precision, the framework advances\nrPPG technology toward reliable real-world deployment in healthcare. The codes\nare available at: https://github.com/WuZheng42/CardiacMamba.\n", "link": "http://arxiv.org/abs/2502.13624v1", "date": "2025-02-19", "relevancy": 2.093, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5423}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5257}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CardiacMamba%3A%20A%20Multimodal%20RGB-RF%20Fusion%20Framework%20with%20State%20Space%0A%20%20Models%20for%20Remote%20Physiological%20Measurement&body=Title%3A%20CardiacMamba%3A%20A%20Multimodal%20RGB-RF%20Fusion%20Framework%20with%20State%20Space%0A%20%20Models%20for%20Remote%20Physiological%20Measurement%0AAuthor%3A%20Zheng%20Wu%20and%20Yiping%20Xie%20and%20Bo%20Zhao%20and%20Jiguang%20He%20and%20Fei%20Luo%20and%20Ning%20Deng%20and%20Zitong%20Yu%0AAbstract%3A%20%20%20Heart%20rate%20%28HR%29%20estimation%20via%20remote%20photoplethysmography%20%28rPPG%29%20offers%20a%0Anon-invasive%20solution%20for%20health%20monitoring.%20However%2C%20traditional%0Asingle-modality%20approaches%20%28RGB%20or%20Radio%20Frequency%20%28RF%29%29%20face%20challenges%20in%0Abalancing%20robustness%20and%20accuracy%20due%20to%20lighting%20variations%2C%20motion%20artifacts%2C%0Aand%20skin%20tone%20bias.%20In%20this%20paper%2C%20we%20propose%20CardiacMamba%2C%20a%20multimodal%20RGB-RF%0Afusion%20framework%20that%20leverages%20the%20complementary%20strengths%20of%20both%20modalities.%0AIt%20introduces%20the%20Temporal%20Difference%20Mamba%20Module%20%28TDMM%29%20to%20capture%20dynamic%0Achanges%20in%20RF%20signals%20using%20timing%20differences%20between%20frames%2C%20enhancing%20the%0Aextraction%20of%20local%20and%20global%20features.%20Additionally%2C%20CardiacMamba%20employs%20a%0ABidirectional%20SSM%20for%20cross-modal%20alignment%20and%20a%20Channel-wise%20Fast%20Fourier%0ATransform%20%28CFFT%29%20to%20effectively%20capture%20and%20refine%20the%20frequency%20domain%0Acharacteristics%20of%20RGB%20and%20RF%20signals%2C%20ultimately%20improving%20heart%20rate%0Aestimation%20accuracy%20and%20periodicity%20detection.%20Extensive%20experiments%20on%20the%0AEquiPleth%20dataset%20demonstrate%20state-of-the-art%20performance%2C%20achieving%20marked%0Aimprovements%20in%20accuracy%20and%20robustness.%20CardiacMamba%20significantly%20mitigates%0Askin%20tone%20bias%2C%20reducing%20performance%20disparities%20across%20demographic%20groups%2C%20and%0Amaintains%20resilience%20under%20missing-modality%20scenarios.%20By%20addressing%20critical%0Achallenges%20in%20fairness%2C%20adaptability%2C%20and%20precision%2C%20the%20framework%20advances%0ArPPG%20technology%20toward%20reliable%20real-world%20deployment%20in%20healthcare.%20The%20codes%0Aare%20available%20at%3A%20https%3A//github.com/WuZheng42/CardiacMamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCardiacMamba%253A%2520A%2520Multimodal%2520RGB-RF%2520Fusion%2520Framework%2520with%2520State%2520Space%250A%2520%2520Models%2520for%2520Remote%2520Physiological%2520Measurement%26entry.906535625%3DZheng%2520Wu%2520and%2520Yiping%2520Xie%2520and%2520Bo%2520Zhao%2520and%2520Jiguang%2520He%2520and%2520Fei%2520Luo%2520and%2520Ning%2520Deng%2520and%2520Zitong%2520Yu%26entry.1292438233%3D%2520%2520Heart%2520rate%2520%2528HR%2529%2520estimation%2520via%2520remote%2520photoplethysmography%2520%2528rPPG%2529%2520offers%2520a%250Anon-invasive%2520solution%2520for%2520health%2520monitoring.%2520However%252C%2520traditional%250Asingle-modality%2520approaches%2520%2528RGB%2520or%2520Radio%2520Frequency%2520%2528RF%2529%2529%2520face%2520challenges%2520in%250Abalancing%2520robustness%2520and%2520accuracy%2520due%2520to%2520lighting%2520variations%252C%2520motion%2520artifacts%252C%250Aand%2520skin%2520tone%2520bias.%2520In%2520this%2520paper%252C%2520we%2520propose%2520CardiacMamba%252C%2520a%2520multimodal%2520RGB-RF%250Afusion%2520framework%2520that%2520leverages%2520the%2520complementary%2520strengths%2520of%2520both%2520modalities.%250AIt%2520introduces%2520the%2520Temporal%2520Difference%2520Mamba%2520Module%2520%2528TDMM%2529%2520to%2520capture%2520dynamic%250Achanges%2520in%2520RF%2520signals%2520using%2520timing%2520differences%2520between%2520frames%252C%2520enhancing%2520the%250Aextraction%2520of%2520local%2520and%2520global%2520features.%2520Additionally%252C%2520CardiacMamba%2520employs%2520a%250ABidirectional%2520SSM%2520for%2520cross-modal%2520alignment%2520and%2520a%2520Channel-wise%2520Fast%2520Fourier%250ATransform%2520%2528CFFT%2529%2520to%2520effectively%2520capture%2520and%2520refine%2520the%2520frequency%2520domain%250Acharacteristics%2520of%2520RGB%2520and%2520RF%2520signals%252C%2520ultimately%2520improving%2520heart%2520rate%250Aestimation%2520accuracy%2520and%2520periodicity%2520detection.%2520Extensive%2520experiments%2520on%2520the%250AEquiPleth%2520dataset%2520demonstrate%2520state-of-the-art%2520performance%252C%2520achieving%2520marked%250Aimprovements%2520in%2520accuracy%2520and%2520robustness.%2520CardiacMamba%2520significantly%2520mitigates%250Askin%2520tone%2520bias%252C%2520reducing%2520performance%2520disparities%2520across%2520demographic%2520groups%252C%2520and%250Amaintains%2520resilience%2520under%2520missing-modality%2520scenarios.%2520By%2520addressing%2520critical%250Achallenges%2520in%2520fairness%252C%2520adaptability%252C%2520and%2520precision%252C%2520the%2520framework%2520advances%250ArPPG%2520technology%2520toward%2520reliable%2520real-world%2520deployment%2520in%2520healthcare.%2520The%2520codes%250Aare%2520available%2520at%253A%2520https%253A//github.com/WuZheng42/CardiacMamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CardiacMamba%3A%20A%20Multimodal%20RGB-RF%20Fusion%20Framework%20with%20State%20Space%0A%20%20Models%20for%20Remote%20Physiological%20Measurement&entry.906535625=Zheng%20Wu%20and%20Yiping%20Xie%20and%20Bo%20Zhao%20and%20Jiguang%20He%20and%20Fei%20Luo%20and%20Ning%20Deng%20and%20Zitong%20Yu&entry.1292438233=%20%20Heart%20rate%20%28HR%29%20estimation%20via%20remote%20photoplethysmography%20%28rPPG%29%20offers%20a%0Anon-invasive%20solution%20for%20health%20monitoring.%20However%2C%20traditional%0Asingle-modality%20approaches%20%28RGB%20or%20Radio%20Frequency%20%28RF%29%29%20face%20challenges%20in%0Abalancing%20robustness%20and%20accuracy%20due%20to%20lighting%20variations%2C%20motion%20artifacts%2C%0Aand%20skin%20tone%20bias.%20In%20this%20paper%2C%20we%20propose%20CardiacMamba%2C%20a%20multimodal%20RGB-RF%0Afusion%20framework%20that%20leverages%20the%20complementary%20strengths%20of%20both%20modalities.%0AIt%20introduces%20the%20Temporal%20Difference%20Mamba%20Module%20%28TDMM%29%20to%20capture%20dynamic%0Achanges%20in%20RF%20signals%20using%20timing%20differences%20between%20frames%2C%20enhancing%20the%0Aextraction%20of%20local%20and%20global%20features.%20Additionally%2C%20CardiacMamba%20employs%20a%0ABidirectional%20SSM%20for%20cross-modal%20alignment%20and%20a%20Channel-wise%20Fast%20Fourier%0ATransform%20%28CFFT%29%20to%20effectively%20capture%20and%20refine%20the%20frequency%20domain%0Acharacteristics%20of%20RGB%20and%20RF%20signals%2C%20ultimately%20improving%20heart%20rate%0Aestimation%20accuracy%20and%20periodicity%20detection.%20Extensive%20experiments%20on%20the%0AEquiPleth%20dataset%20demonstrate%20state-of-the-art%20performance%2C%20achieving%20marked%0Aimprovements%20in%20accuracy%20and%20robustness.%20CardiacMamba%20significantly%20mitigates%0Askin%20tone%20bias%2C%20reducing%20performance%20disparities%20across%20demographic%20groups%2C%20and%0Amaintains%20resilience%20under%20missing-modality%20scenarios.%20By%20addressing%20critical%0Achallenges%20in%20fairness%2C%20adaptability%2C%20and%20precision%2C%20the%20framework%20advances%0ArPPG%20technology%20toward%20reliable%20real-world%20deployment%20in%20healthcare.%20The%20codes%0Aare%20available%20at%3A%20https%3A//github.com/WuZheng42/CardiacMamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13624v1&entry.124074799=Read"},
{"title": "Building Age Estimation: A New Multi-Modal Benchmark Dataset and\n  Community Challenge", "author": "Nikolaos Dionelis and Nicolas Long\u00e9p\u00e9 and Alessandra Feliciotti and Mattia Marconcini and Devis Peressutti and Nika Oman Kadunc and JaeWan Park and Hagai Raja Sinulingga and Steve Andreas Immanuel and Ba Tran and Caroline Arnold", "abstract": "  Estimating the construction year of buildings is of great importance for\nsustainability. Sustainable buildings minimize energy consumption and are a key\npart of responsible and sustainable urban planning and development to\neffectively combat climate change. By using Artificial Intelligence (AI) and\nrecently proposed Transformer models, we are able to estimate the construction\nepoch of buildings from a multi-modal dataset. In this paper, we introduce a\nnew benchmark multi-modal dataset, i.e. the Map your City Dataset (MyCD),\ncontaining top-view Very High Resolution (VHR) images, Earth Observation (EO)\nmulti-spectral data from the Copernicus Sentinel-2 satellite constellation, and\nstreet-view images in many different cities in Europe, co-localized with\nrespect to the building under study and labelled with the construction epoch.\nWe assess EO generalization performance on new/ previously unseen cities that\nhave been held-out from training and appear only during inference. In this\nwork, we present the community-based data challenge we organized based on MyCD.\nThe ESA AI4EO Challenge MapYourCity was opened in 2024 for 4 months. Here, we\npresent the Top-4 performing models, and the main evaluation results. During\ninference, the performance of the models using both all three input modalities\nand only the two top-view modalities, i.e. without the street-view images, is\nexamined. The evaluation results show that the models are effective and can\nachieve good performance on this difficult real-world task of estimating the\nage of buildings, even on previously unseen cities, as well as even using only\nthe two top-view modalities (i.e. VHR and Sentinel-2) during inference.\n", "link": "http://arxiv.org/abs/2502.13818v1", "date": "2025-02-19", "relevancy": 2.0927, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5623}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5153}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20Age%20Estimation%3A%20A%20New%20Multi-Modal%20Benchmark%20Dataset%20and%0A%20%20Community%20Challenge&body=Title%3A%20Building%20Age%20Estimation%3A%20A%20New%20Multi-Modal%20Benchmark%20Dataset%20and%0A%20%20Community%20Challenge%0AAuthor%3A%20Nikolaos%20Dionelis%20and%20Nicolas%20Long%C3%A9p%C3%A9%20and%20Alessandra%20Feliciotti%20and%20Mattia%20Marconcini%20and%20Devis%20Peressutti%20and%20Nika%20Oman%20Kadunc%20and%20JaeWan%20Park%20and%20Hagai%20Raja%20Sinulingga%20and%20Steve%20Andreas%20Immanuel%20and%20Ba%20Tran%20and%20Caroline%20Arnold%0AAbstract%3A%20%20%20Estimating%20the%20construction%20year%20of%20buildings%20is%20of%20great%20importance%20for%0Asustainability.%20Sustainable%20buildings%20minimize%20energy%20consumption%20and%20are%20a%20key%0Apart%20of%20responsible%20and%20sustainable%20urban%20planning%20and%20development%20to%0Aeffectively%20combat%20climate%20change.%20By%20using%20Artificial%20Intelligence%20%28AI%29%20and%0Arecently%20proposed%20Transformer%20models%2C%20we%20are%20able%20to%20estimate%20the%20construction%0Aepoch%20of%20buildings%20from%20a%20multi-modal%20dataset.%20In%20this%20paper%2C%20we%20introduce%20a%0Anew%20benchmark%20multi-modal%20dataset%2C%20i.e.%20the%20Map%20your%20City%20Dataset%20%28MyCD%29%2C%0Acontaining%20top-view%20Very%20High%20Resolution%20%28VHR%29%20images%2C%20Earth%20Observation%20%28EO%29%0Amulti-spectral%20data%20from%20the%20Copernicus%20Sentinel-2%20satellite%20constellation%2C%20and%0Astreet-view%20images%20in%20many%20different%20cities%20in%20Europe%2C%20co-localized%20with%0Arespect%20to%20the%20building%20under%20study%20and%20labelled%20with%20the%20construction%20epoch.%0AWe%20assess%20EO%20generalization%20performance%20on%20new/%20previously%20unseen%20cities%20that%0Ahave%20been%20held-out%20from%20training%20and%20appear%20only%20during%20inference.%20In%20this%0Awork%2C%20we%20present%20the%20community-based%20data%20challenge%20we%20organized%20based%20on%20MyCD.%0AThe%20ESA%20AI4EO%20Challenge%20MapYourCity%20was%20opened%20in%202024%20for%204%20months.%20Here%2C%20we%0Apresent%20the%20Top-4%20performing%20models%2C%20and%20the%20main%20evaluation%20results.%20During%0Ainference%2C%20the%20performance%20of%20the%20models%20using%20both%20all%20three%20input%20modalities%0Aand%20only%20the%20two%20top-view%20modalities%2C%20i.e.%20without%20the%20street-view%20images%2C%20is%0Aexamined.%20The%20evaluation%20results%20show%20that%20the%20models%20are%20effective%20and%20can%0Aachieve%20good%20performance%20on%20this%20difficult%20real-world%20task%20of%20estimating%20the%0Aage%20of%20buildings%2C%20even%20on%20previously%20unseen%20cities%2C%20as%20well%20as%20even%20using%20only%0Athe%20two%20top-view%20modalities%20%28i.e.%20VHR%20and%20Sentinel-2%29%20during%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520Age%2520Estimation%253A%2520A%2520New%2520Multi-Modal%2520Benchmark%2520Dataset%2520and%250A%2520%2520Community%2520Challenge%26entry.906535625%3DNikolaos%2520Dionelis%2520and%2520Nicolas%2520Long%25C3%25A9p%25C3%25A9%2520and%2520Alessandra%2520Feliciotti%2520and%2520Mattia%2520Marconcini%2520and%2520Devis%2520Peressutti%2520and%2520Nika%2520Oman%2520Kadunc%2520and%2520JaeWan%2520Park%2520and%2520Hagai%2520Raja%2520Sinulingga%2520and%2520Steve%2520Andreas%2520Immanuel%2520and%2520Ba%2520Tran%2520and%2520Caroline%2520Arnold%26entry.1292438233%3D%2520%2520Estimating%2520the%2520construction%2520year%2520of%2520buildings%2520is%2520of%2520great%2520importance%2520for%250Asustainability.%2520Sustainable%2520buildings%2520minimize%2520energy%2520consumption%2520and%2520are%2520a%2520key%250Apart%2520of%2520responsible%2520and%2520sustainable%2520urban%2520planning%2520and%2520development%2520to%250Aeffectively%2520combat%2520climate%2520change.%2520By%2520using%2520Artificial%2520Intelligence%2520%2528AI%2529%2520and%250Arecently%2520proposed%2520Transformer%2520models%252C%2520we%2520are%2520able%2520to%2520estimate%2520the%2520construction%250Aepoch%2520of%2520buildings%2520from%2520a%2520multi-modal%2520dataset.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Anew%2520benchmark%2520multi-modal%2520dataset%252C%2520i.e.%2520the%2520Map%2520your%2520City%2520Dataset%2520%2528MyCD%2529%252C%250Acontaining%2520top-view%2520Very%2520High%2520Resolution%2520%2528VHR%2529%2520images%252C%2520Earth%2520Observation%2520%2528EO%2529%250Amulti-spectral%2520data%2520from%2520the%2520Copernicus%2520Sentinel-2%2520satellite%2520constellation%252C%2520and%250Astreet-view%2520images%2520in%2520many%2520different%2520cities%2520in%2520Europe%252C%2520co-localized%2520with%250Arespect%2520to%2520the%2520building%2520under%2520study%2520and%2520labelled%2520with%2520the%2520construction%2520epoch.%250AWe%2520assess%2520EO%2520generalization%2520performance%2520on%2520new/%2520previously%2520unseen%2520cities%2520that%250Ahave%2520been%2520held-out%2520from%2520training%2520and%2520appear%2520only%2520during%2520inference.%2520In%2520this%250Awork%252C%2520we%2520present%2520the%2520community-based%2520data%2520challenge%2520we%2520organized%2520based%2520on%2520MyCD.%250AThe%2520ESA%2520AI4EO%2520Challenge%2520MapYourCity%2520was%2520opened%2520in%25202024%2520for%25204%2520months.%2520Here%252C%2520we%250Apresent%2520the%2520Top-4%2520performing%2520models%252C%2520and%2520the%2520main%2520evaluation%2520results.%2520During%250Ainference%252C%2520the%2520performance%2520of%2520the%2520models%2520using%2520both%2520all%2520three%2520input%2520modalities%250Aand%2520only%2520the%2520two%2520top-view%2520modalities%252C%2520i.e.%2520without%2520the%2520street-view%2520images%252C%2520is%250Aexamined.%2520The%2520evaluation%2520results%2520show%2520that%2520the%2520models%2520are%2520effective%2520and%2520can%250Aachieve%2520good%2520performance%2520on%2520this%2520difficult%2520real-world%2520task%2520of%2520estimating%2520the%250Aage%2520of%2520buildings%252C%2520even%2520on%2520previously%2520unseen%2520cities%252C%2520as%2520well%2520as%2520even%2520using%2520only%250Athe%2520two%2520top-view%2520modalities%2520%2528i.e.%2520VHR%2520and%2520Sentinel-2%2529%2520during%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20Age%20Estimation%3A%20A%20New%20Multi-Modal%20Benchmark%20Dataset%20and%0A%20%20Community%20Challenge&entry.906535625=Nikolaos%20Dionelis%20and%20Nicolas%20Long%C3%A9p%C3%A9%20and%20Alessandra%20Feliciotti%20and%20Mattia%20Marconcini%20and%20Devis%20Peressutti%20and%20Nika%20Oman%20Kadunc%20and%20JaeWan%20Park%20and%20Hagai%20Raja%20Sinulingga%20and%20Steve%20Andreas%20Immanuel%20and%20Ba%20Tran%20and%20Caroline%20Arnold&entry.1292438233=%20%20Estimating%20the%20construction%20year%20of%20buildings%20is%20of%20great%20importance%20for%0Asustainability.%20Sustainable%20buildings%20minimize%20energy%20consumption%20and%20are%20a%20key%0Apart%20of%20responsible%20and%20sustainable%20urban%20planning%20and%20development%20to%0Aeffectively%20combat%20climate%20change.%20By%20using%20Artificial%20Intelligence%20%28AI%29%20and%0Arecently%20proposed%20Transformer%20models%2C%20we%20are%20able%20to%20estimate%20the%20construction%0Aepoch%20of%20buildings%20from%20a%20multi-modal%20dataset.%20In%20this%20paper%2C%20we%20introduce%20a%0Anew%20benchmark%20multi-modal%20dataset%2C%20i.e.%20the%20Map%20your%20City%20Dataset%20%28MyCD%29%2C%0Acontaining%20top-view%20Very%20High%20Resolution%20%28VHR%29%20images%2C%20Earth%20Observation%20%28EO%29%0Amulti-spectral%20data%20from%20the%20Copernicus%20Sentinel-2%20satellite%20constellation%2C%20and%0Astreet-view%20images%20in%20many%20different%20cities%20in%20Europe%2C%20co-localized%20with%0Arespect%20to%20the%20building%20under%20study%20and%20labelled%20with%20the%20construction%20epoch.%0AWe%20assess%20EO%20generalization%20performance%20on%20new/%20previously%20unseen%20cities%20that%0Ahave%20been%20held-out%20from%20training%20and%20appear%20only%20during%20inference.%20In%20this%0Awork%2C%20we%20present%20the%20community-based%20data%20challenge%20we%20organized%20based%20on%20MyCD.%0AThe%20ESA%20AI4EO%20Challenge%20MapYourCity%20was%20opened%20in%202024%20for%204%20months.%20Here%2C%20we%0Apresent%20the%20Top-4%20performing%20models%2C%20and%20the%20main%20evaluation%20results.%20During%0Ainference%2C%20the%20performance%20of%20the%20models%20using%20both%20all%20three%20input%20modalities%0Aand%20only%20the%20two%20top-view%20modalities%2C%20i.e.%20without%20the%20street-view%20images%2C%20is%0Aexamined.%20The%20evaluation%20results%20show%20that%20the%20models%20are%20effective%20and%20can%0Aachieve%20good%20performance%20on%20this%20difficult%20real-world%20task%20of%20estimating%20the%0Aage%20of%20buildings%2C%20even%20on%20previously%20unseen%20cities%2C%20as%20well%20as%20even%20using%20only%0Athe%20two%20top-view%20modalities%20%28i.e.%20VHR%20and%20Sentinel-2%29%20during%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13818v1&entry.124074799=Read"},
{"title": "DataSciBench: An LLM Agent Benchmark for Data Science", "author": "Dan Zhang and Sining Zhoubian and Min Cai and Fengzu Li and Lekang Yang and Wei Wang and Tianjiao Dong and Ziniu Hu and Jie Tang and Yisong Yue", "abstract": "  This paper presents DataSciBench, a comprehensive benchmark for evaluating\nLarge Language Model (LLM) capabilities in data science. Recent related\nbenchmarks have primarily focused on single tasks, easily obtainable ground\ntruth, and straightforward evaluation metrics, which limits the scope of tasks\nthat can be evaluated. In contrast, DataSciBench is constructed based on a more\ncomprehensive and curated collection of natural and challenging prompts for\nuncertain ground truth and evaluation metrics. We develop a semi-automated\npipeline for generating ground truth (GT) and validating evaluation metrics.\nThis pipeline utilizes and implements an LLM-based self-consistency and human\nverification strategy to produce accurate GT by leveraging collected prompts,\npredefined task types, and aggregate functions (metrics). Furthermore, we\npropose an innovative Task - Function - Code (TFC) framework to assess each\ncode execution outcome based on precisely defined metrics and programmatic\nrules. Our experimental framework involves testing 6 API-based models, 8\nopen-source general models, and 9 open-source code generation models using the\ndiverse set of prompts we have gathered. This approach aims to provide a more\ncomprehensive and rigorous evaluation of LLMs in data science, revealing their\nstrengths and weaknesses. Experimental results demonstrate that API-based\nmodels outperform open-sourced models on all metrics and\nDeepseek-Coder-33B-Instruct achieves the highest score among open-sourced\nmodels. We release all code and data at https://github.com/THUDM/DataSciBench.\n", "link": "http://arxiv.org/abs/2502.13897v1", "date": "2025-02-19", "relevancy": 2.0791, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5199}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5199}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DataSciBench%3A%20An%20LLM%20Agent%20Benchmark%20for%20Data%20Science&body=Title%3A%20DataSciBench%3A%20An%20LLM%20Agent%20Benchmark%20for%20Data%20Science%0AAuthor%3A%20Dan%20Zhang%20and%20Sining%20Zhoubian%20and%20Min%20Cai%20and%20Fengzu%20Li%20and%20Lekang%20Yang%20and%20Wei%20Wang%20and%20Tianjiao%20Dong%20and%20Ziniu%20Hu%20and%20Jie%20Tang%20and%20Yisong%20Yue%0AAbstract%3A%20%20%20This%20paper%20presents%20DataSciBench%2C%20a%20comprehensive%20benchmark%20for%20evaluating%0ALarge%20Language%20Model%20%28LLM%29%20capabilities%20in%20data%20science.%20Recent%20related%0Abenchmarks%20have%20primarily%20focused%20on%20single%20tasks%2C%20easily%20obtainable%20ground%0Atruth%2C%20and%20straightforward%20evaluation%20metrics%2C%20which%20limits%20the%20scope%20of%20tasks%0Athat%20can%20be%20evaluated.%20In%20contrast%2C%20DataSciBench%20is%20constructed%20based%20on%20a%20more%0Acomprehensive%20and%20curated%20collection%20of%20natural%20and%20challenging%20prompts%20for%0Auncertain%20ground%20truth%20and%20evaluation%20metrics.%20We%20develop%20a%20semi-automated%0Apipeline%20for%20generating%20ground%20truth%20%28GT%29%20and%20validating%20evaluation%20metrics.%0AThis%20pipeline%20utilizes%20and%20implements%20an%20LLM-based%20self-consistency%20and%20human%0Averification%20strategy%20to%20produce%20accurate%20GT%20by%20leveraging%20collected%20prompts%2C%0Apredefined%20task%20types%2C%20and%20aggregate%20functions%20%28metrics%29.%20Furthermore%2C%20we%0Apropose%20an%20innovative%20Task%20-%20Function%20-%20Code%20%28TFC%29%20framework%20to%20assess%20each%0Acode%20execution%20outcome%20based%20on%20precisely%20defined%20metrics%20and%20programmatic%0Arules.%20Our%20experimental%20framework%20involves%20testing%206%20API-based%20models%2C%208%0Aopen-source%20general%20models%2C%20and%209%20open-source%20code%20generation%20models%20using%20the%0Adiverse%20set%20of%20prompts%20we%20have%20gathered.%20This%20approach%20aims%20to%20provide%20a%20more%0Acomprehensive%20and%20rigorous%20evaluation%20of%20LLMs%20in%20data%20science%2C%20revealing%20their%0Astrengths%20and%20weaknesses.%20Experimental%20results%20demonstrate%20that%20API-based%0Amodels%20outperform%20open-sourced%20models%20on%20all%20metrics%20and%0ADeepseek-Coder-33B-Instruct%20achieves%20the%20highest%20score%20among%20open-sourced%0Amodels.%20We%20release%20all%20code%20and%20data%20at%20https%3A//github.com/THUDM/DataSciBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDataSciBench%253A%2520An%2520LLM%2520Agent%2520Benchmark%2520for%2520Data%2520Science%26entry.906535625%3DDan%2520Zhang%2520and%2520Sining%2520Zhoubian%2520and%2520Min%2520Cai%2520and%2520Fengzu%2520Li%2520and%2520Lekang%2520Yang%2520and%2520Wei%2520Wang%2520and%2520Tianjiao%2520Dong%2520and%2520Ziniu%2520Hu%2520and%2520Jie%2520Tang%2520and%2520Yisong%2520Yue%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520DataSciBench%252C%2520a%2520comprehensive%2520benchmark%2520for%2520evaluating%250ALarge%2520Language%2520Model%2520%2528LLM%2529%2520capabilities%2520in%2520data%2520science.%2520Recent%2520related%250Abenchmarks%2520have%2520primarily%2520focused%2520on%2520single%2520tasks%252C%2520easily%2520obtainable%2520ground%250Atruth%252C%2520and%2520straightforward%2520evaluation%2520metrics%252C%2520which%2520limits%2520the%2520scope%2520of%2520tasks%250Athat%2520can%2520be%2520evaluated.%2520In%2520contrast%252C%2520DataSciBench%2520is%2520constructed%2520based%2520on%2520a%2520more%250Acomprehensive%2520and%2520curated%2520collection%2520of%2520natural%2520and%2520challenging%2520prompts%2520for%250Auncertain%2520ground%2520truth%2520and%2520evaluation%2520metrics.%2520We%2520develop%2520a%2520semi-automated%250Apipeline%2520for%2520generating%2520ground%2520truth%2520%2528GT%2529%2520and%2520validating%2520evaluation%2520metrics.%250AThis%2520pipeline%2520utilizes%2520and%2520implements%2520an%2520LLM-based%2520self-consistency%2520and%2520human%250Averification%2520strategy%2520to%2520produce%2520accurate%2520GT%2520by%2520leveraging%2520collected%2520prompts%252C%250Apredefined%2520task%2520types%252C%2520and%2520aggregate%2520functions%2520%2528metrics%2529.%2520Furthermore%252C%2520we%250Apropose%2520an%2520innovative%2520Task%2520-%2520Function%2520-%2520Code%2520%2528TFC%2529%2520framework%2520to%2520assess%2520each%250Acode%2520execution%2520outcome%2520based%2520on%2520precisely%2520defined%2520metrics%2520and%2520programmatic%250Arules.%2520Our%2520experimental%2520framework%2520involves%2520testing%25206%2520API-based%2520models%252C%25208%250Aopen-source%2520general%2520models%252C%2520and%25209%2520open-source%2520code%2520generation%2520models%2520using%2520the%250Adiverse%2520set%2520of%2520prompts%2520we%2520have%2520gathered.%2520This%2520approach%2520aims%2520to%2520provide%2520a%2520more%250Acomprehensive%2520and%2520rigorous%2520evaluation%2520of%2520LLMs%2520in%2520data%2520science%252C%2520revealing%2520their%250Astrengths%2520and%2520weaknesses.%2520Experimental%2520results%2520demonstrate%2520that%2520API-based%250Amodels%2520outperform%2520open-sourced%2520models%2520on%2520all%2520metrics%2520and%250ADeepseek-Coder-33B-Instruct%2520achieves%2520the%2520highest%2520score%2520among%2520open-sourced%250Amodels.%2520We%2520release%2520all%2520code%2520and%2520data%2520at%2520https%253A//github.com/THUDM/DataSciBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DataSciBench%3A%20An%20LLM%20Agent%20Benchmark%20for%20Data%20Science&entry.906535625=Dan%20Zhang%20and%20Sining%20Zhoubian%20and%20Min%20Cai%20and%20Fengzu%20Li%20and%20Lekang%20Yang%20and%20Wei%20Wang%20and%20Tianjiao%20Dong%20and%20Ziniu%20Hu%20and%20Jie%20Tang%20and%20Yisong%20Yue&entry.1292438233=%20%20This%20paper%20presents%20DataSciBench%2C%20a%20comprehensive%20benchmark%20for%20evaluating%0ALarge%20Language%20Model%20%28LLM%29%20capabilities%20in%20data%20science.%20Recent%20related%0Abenchmarks%20have%20primarily%20focused%20on%20single%20tasks%2C%20easily%20obtainable%20ground%0Atruth%2C%20and%20straightforward%20evaluation%20metrics%2C%20which%20limits%20the%20scope%20of%20tasks%0Athat%20can%20be%20evaluated.%20In%20contrast%2C%20DataSciBench%20is%20constructed%20based%20on%20a%20more%0Acomprehensive%20and%20curated%20collection%20of%20natural%20and%20challenging%20prompts%20for%0Auncertain%20ground%20truth%20and%20evaluation%20metrics.%20We%20develop%20a%20semi-automated%0Apipeline%20for%20generating%20ground%20truth%20%28GT%29%20and%20validating%20evaluation%20metrics.%0AThis%20pipeline%20utilizes%20and%20implements%20an%20LLM-based%20self-consistency%20and%20human%0Averification%20strategy%20to%20produce%20accurate%20GT%20by%20leveraging%20collected%20prompts%2C%0Apredefined%20task%20types%2C%20and%20aggregate%20functions%20%28metrics%29.%20Furthermore%2C%20we%0Apropose%20an%20innovative%20Task%20-%20Function%20-%20Code%20%28TFC%29%20framework%20to%20assess%20each%0Acode%20execution%20outcome%20based%20on%20precisely%20defined%20metrics%20and%20programmatic%0Arules.%20Our%20experimental%20framework%20involves%20testing%206%20API-based%20models%2C%208%0Aopen-source%20general%20models%2C%20and%209%20open-source%20code%20generation%20models%20using%20the%0Adiverse%20set%20of%20prompts%20we%20have%20gathered.%20This%20approach%20aims%20to%20provide%20a%20more%0Acomprehensive%20and%20rigorous%20evaluation%20of%20LLMs%20in%20data%20science%2C%20revealing%20their%0Astrengths%20and%20weaknesses.%20Experimental%20results%20demonstrate%20that%20API-based%0Amodels%20outperform%20open-sourced%20models%20on%20all%20metrics%20and%0ADeepseek-Coder-33B-Instruct%20achieves%20the%20highest%20score%20among%20open-sourced%0Amodels.%20We%20release%20all%20code%20and%20data%20at%20https%3A//github.com/THUDM/DataSciBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13897v1&entry.124074799=Read"},
{"title": "An Online Optimization-Based Trajectory Planning Approach for\n  Cooperative Landing Tasks", "author": "Jingshan Chen and Lihan Xu and Henrik Ebel and Peter Eberhard", "abstract": "  This paper presents a real-time trajectory planning scheme for a\nheterogeneous multi-robot system (consisting of a quadrotor and a ground mobile\nrobot) for a cooperative landing task, where the landing position, landing\ntime, and coordination between the robots are determined autonomously under the\nconsideration of feasibility and user specifications. The proposed framework\nleverages the potential of the complementarity constraint as a decision-maker\nand an indicator for diverse cooperative tasks and extends it to the\ncollaborative landing scenario. In a potential application of the proposed\nmethodology, a ground mobile robot may serve as a mobile charging station and\ncoordinates in real-time with a quadrotor to be charged, facilitating a safe\nand efficient rendezvous and landing. We verified the generated trajectories in\nsimulation and real-world applications, demonstrating the real-time\ncapabilities of the proposed landing planning framework.\n", "link": "http://arxiv.org/abs/2502.13823v1", "date": "2025-02-19", "relevancy": 2.0762, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5404}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5299}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Online%20Optimization-Based%20Trajectory%20Planning%20Approach%20for%0A%20%20Cooperative%20Landing%20Tasks&body=Title%3A%20An%20Online%20Optimization-Based%20Trajectory%20Planning%20Approach%20for%0A%20%20Cooperative%20Landing%20Tasks%0AAuthor%3A%20Jingshan%20Chen%20and%20Lihan%20Xu%20and%20Henrik%20Ebel%20and%20Peter%20Eberhard%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20real-time%20trajectory%20planning%20scheme%20for%20a%0Aheterogeneous%20multi-robot%20system%20%28consisting%20of%20a%20quadrotor%20and%20a%20ground%20mobile%0Arobot%29%20for%20a%20cooperative%20landing%20task%2C%20where%20the%20landing%20position%2C%20landing%0Atime%2C%20and%20coordination%20between%20the%20robots%20are%20determined%20autonomously%20under%20the%0Aconsideration%20of%20feasibility%20and%20user%20specifications.%20The%20proposed%20framework%0Aleverages%20the%20potential%20of%20the%20complementarity%20constraint%20as%20a%20decision-maker%0Aand%20an%20indicator%20for%20diverse%20cooperative%20tasks%20and%20extends%20it%20to%20the%0Acollaborative%20landing%20scenario.%20In%20a%20potential%20application%20of%20the%20proposed%0Amethodology%2C%20a%20ground%20mobile%20robot%20may%20serve%20as%20a%20mobile%20charging%20station%20and%0Acoordinates%20in%20real-time%20with%20a%20quadrotor%20to%20be%20charged%2C%20facilitating%20a%20safe%0Aand%20efficient%20rendezvous%20and%20landing.%20We%20verified%20the%20generated%20trajectories%20in%0Asimulation%20and%20real-world%20applications%2C%20demonstrating%20the%20real-time%0Acapabilities%20of%20the%20proposed%20landing%20planning%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13823v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Online%2520Optimization-Based%2520Trajectory%2520Planning%2520Approach%2520for%250A%2520%2520Cooperative%2520Landing%2520Tasks%26entry.906535625%3DJingshan%2520Chen%2520and%2520Lihan%2520Xu%2520and%2520Henrik%2520Ebel%2520and%2520Peter%2520Eberhard%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520real-time%2520trajectory%2520planning%2520scheme%2520for%2520a%250Aheterogeneous%2520multi-robot%2520system%2520%2528consisting%2520of%2520a%2520quadrotor%2520and%2520a%2520ground%2520mobile%250Arobot%2529%2520for%2520a%2520cooperative%2520landing%2520task%252C%2520where%2520the%2520landing%2520position%252C%2520landing%250Atime%252C%2520and%2520coordination%2520between%2520the%2520robots%2520are%2520determined%2520autonomously%2520under%2520the%250Aconsideration%2520of%2520feasibility%2520and%2520user%2520specifications.%2520The%2520proposed%2520framework%250Aleverages%2520the%2520potential%2520of%2520the%2520complementarity%2520constraint%2520as%2520a%2520decision-maker%250Aand%2520an%2520indicator%2520for%2520diverse%2520cooperative%2520tasks%2520and%2520extends%2520it%2520to%2520the%250Acollaborative%2520landing%2520scenario.%2520In%2520a%2520potential%2520application%2520of%2520the%2520proposed%250Amethodology%252C%2520a%2520ground%2520mobile%2520robot%2520may%2520serve%2520as%2520a%2520mobile%2520charging%2520station%2520and%250Acoordinates%2520in%2520real-time%2520with%2520a%2520quadrotor%2520to%2520be%2520charged%252C%2520facilitating%2520a%2520safe%250Aand%2520efficient%2520rendezvous%2520and%2520landing.%2520We%2520verified%2520the%2520generated%2520trajectories%2520in%250Asimulation%2520and%2520real-world%2520applications%252C%2520demonstrating%2520the%2520real-time%250Acapabilities%2520of%2520the%2520proposed%2520landing%2520planning%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13823v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Online%20Optimization-Based%20Trajectory%20Planning%20Approach%20for%0A%20%20Cooperative%20Landing%20Tasks&entry.906535625=Jingshan%20Chen%20and%20Lihan%20Xu%20and%20Henrik%20Ebel%20and%20Peter%20Eberhard&entry.1292438233=%20%20This%20paper%20presents%20a%20real-time%20trajectory%20planning%20scheme%20for%20a%0Aheterogeneous%20multi-robot%20system%20%28consisting%20of%20a%20quadrotor%20and%20a%20ground%20mobile%0Arobot%29%20for%20a%20cooperative%20landing%20task%2C%20where%20the%20landing%20position%2C%20landing%0Atime%2C%20and%20coordination%20between%20the%20robots%20are%20determined%20autonomously%20under%20the%0Aconsideration%20of%20feasibility%20and%20user%20specifications.%20The%20proposed%20framework%0Aleverages%20the%20potential%20of%20the%20complementarity%20constraint%20as%20a%20decision-maker%0Aand%20an%20indicator%20for%20diverse%20cooperative%20tasks%20and%20extends%20it%20to%20the%0Acollaborative%20landing%20scenario.%20In%20a%20potential%20application%20of%20the%20proposed%0Amethodology%2C%20a%20ground%20mobile%20robot%20may%20serve%20as%20a%20mobile%20charging%20station%20and%0Acoordinates%20in%20real-time%20with%20a%20quadrotor%20to%20be%20charged%2C%20facilitating%20a%20safe%0Aand%20efficient%20rendezvous%20and%20landing.%20We%20verified%20the%20generated%20trajectories%20in%0Asimulation%20and%20real-world%20applications%2C%20demonstrating%20the%20real-time%0Acapabilities%20of%20the%20proposed%20landing%20planning%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13823v1&entry.124074799=Read"},
{"title": "Dataset Distillation via Knowledge Distillation: Towards Efficient\n  Self-Supervised Pre-Training of Deep Networks", "author": "Siddharth Joshi and Jiayi Ni and Baharan Mirzasoleiman", "abstract": "  Dataset distillation (DD) generates small synthetic datasets that can\nefficiently train deep networks with a limited amount of memory and compute.\nDespite the success of DD methods for supervised learning, DD for\nself-supervised pre-training of deep models has remained unaddressed.\nPre-training on unlabeled data is crucial for efficiently generalizing to\ndownstream tasks with limited labeled data. In this work, we propose the first\neffective DD method for SSL pre-training. First, we show, theoretically and\nempirically, that naive application of supervised DD methods to SSL fails, due\nto the high variance of the SSL gradient. Then, we address this issue by\nrelying on insights from knowledge distillation (KD) literature. Specifically,\nwe train a small student model to match the representations of a larger teacher\nmodel trained with SSL. Then, we generate a small synthetic dataset by matching\nthe training trajectories of the student models. As the KD objective has\nconsiderably lower variance than SSL, our approach can generate synthetic\ndatasets that can successfully pre-train high-quality encoders. Through\nextensive experiments, we show that our distilled sets lead to up to 13% higher\naccuracy than prior work, on a variety of downstream tasks, in the presence of\nlimited labeled data. Code at https://github.com/BigML-CS-UCLA/MKDT.\n", "link": "http://arxiv.org/abs/2410.02116v2", "date": "2025-02-19", "relevancy": 2.0682, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5198}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.517}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dataset%20Distillation%20via%20Knowledge%20Distillation%3A%20Towards%20Efficient%0A%20%20Self-Supervised%20Pre-Training%20of%20Deep%20Networks&body=Title%3A%20Dataset%20Distillation%20via%20Knowledge%20Distillation%3A%20Towards%20Efficient%0A%20%20Self-Supervised%20Pre-Training%20of%20Deep%20Networks%0AAuthor%3A%20Siddharth%20Joshi%20and%20Jiayi%20Ni%20and%20Baharan%20Mirzasoleiman%0AAbstract%3A%20%20%20Dataset%20distillation%20%28DD%29%20generates%20small%20synthetic%20datasets%20that%20can%0Aefficiently%20train%20deep%20networks%20with%20a%20limited%20amount%20of%20memory%20and%20compute.%0ADespite%20the%20success%20of%20DD%20methods%20for%20supervised%20learning%2C%20DD%20for%0Aself-supervised%20pre-training%20of%20deep%20models%20has%20remained%20unaddressed.%0APre-training%20on%20unlabeled%20data%20is%20crucial%20for%20efficiently%20generalizing%20to%0Adownstream%20tasks%20with%20limited%20labeled%20data.%20In%20this%20work%2C%20we%20propose%20the%20first%0Aeffective%20DD%20method%20for%20SSL%20pre-training.%20First%2C%20we%20show%2C%20theoretically%20and%0Aempirically%2C%20that%20naive%20application%20of%20supervised%20DD%20methods%20to%20SSL%20fails%2C%20due%0Ato%20the%20high%20variance%20of%20the%20SSL%20gradient.%20Then%2C%20we%20address%20this%20issue%20by%0Arelying%20on%20insights%20from%20knowledge%20distillation%20%28KD%29%20literature.%20Specifically%2C%0Awe%20train%20a%20small%20student%20model%20to%20match%20the%20representations%20of%20a%20larger%20teacher%0Amodel%20trained%20with%20SSL.%20Then%2C%20we%20generate%20a%20small%20synthetic%20dataset%20by%20matching%0Athe%20training%20trajectories%20of%20the%20student%20models.%20As%20the%20KD%20objective%20has%0Aconsiderably%20lower%20variance%20than%20SSL%2C%20our%20approach%20can%20generate%20synthetic%0Adatasets%20that%20can%20successfully%20pre-train%20high-quality%20encoders.%20Through%0Aextensive%20experiments%2C%20we%20show%20that%20our%20distilled%20sets%20lead%20to%20up%20to%2013%25%20higher%0Aaccuracy%20than%20prior%20work%2C%20on%20a%20variety%20of%20downstream%20tasks%2C%20in%20the%20presence%20of%0Alimited%20labeled%20data.%20Code%20at%20https%3A//github.com/BigML-CS-UCLA/MKDT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02116v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDataset%2520Distillation%2520via%2520Knowledge%2520Distillation%253A%2520Towards%2520Efficient%250A%2520%2520Self-Supervised%2520Pre-Training%2520of%2520Deep%2520Networks%26entry.906535625%3DSiddharth%2520Joshi%2520and%2520Jiayi%2520Ni%2520and%2520Baharan%2520Mirzasoleiman%26entry.1292438233%3D%2520%2520Dataset%2520distillation%2520%2528DD%2529%2520generates%2520small%2520synthetic%2520datasets%2520that%2520can%250Aefficiently%2520train%2520deep%2520networks%2520with%2520a%2520limited%2520amount%2520of%2520memory%2520and%2520compute.%250ADespite%2520the%2520success%2520of%2520DD%2520methods%2520for%2520supervised%2520learning%252C%2520DD%2520for%250Aself-supervised%2520pre-training%2520of%2520deep%2520models%2520has%2520remained%2520unaddressed.%250APre-training%2520on%2520unlabeled%2520data%2520is%2520crucial%2520for%2520efficiently%2520generalizing%2520to%250Adownstream%2520tasks%2520with%2520limited%2520labeled%2520data.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520first%250Aeffective%2520DD%2520method%2520for%2520SSL%2520pre-training.%2520First%252C%2520we%2520show%252C%2520theoretically%2520and%250Aempirically%252C%2520that%2520naive%2520application%2520of%2520supervised%2520DD%2520methods%2520to%2520SSL%2520fails%252C%2520due%250Ato%2520the%2520high%2520variance%2520of%2520the%2520SSL%2520gradient.%2520Then%252C%2520we%2520address%2520this%2520issue%2520by%250Arelying%2520on%2520insights%2520from%2520knowledge%2520distillation%2520%2528KD%2529%2520literature.%2520Specifically%252C%250Awe%2520train%2520a%2520small%2520student%2520model%2520to%2520match%2520the%2520representations%2520of%2520a%2520larger%2520teacher%250Amodel%2520trained%2520with%2520SSL.%2520Then%252C%2520we%2520generate%2520a%2520small%2520synthetic%2520dataset%2520by%2520matching%250Athe%2520training%2520trajectories%2520of%2520the%2520student%2520models.%2520As%2520the%2520KD%2520objective%2520has%250Aconsiderably%2520lower%2520variance%2520than%2520SSL%252C%2520our%2520approach%2520can%2520generate%2520synthetic%250Adatasets%2520that%2520can%2520successfully%2520pre-train%2520high-quality%2520encoders.%2520Through%250Aextensive%2520experiments%252C%2520we%2520show%2520that%2520our%2520distilled%2520sets%2520lead%2520to%2520up%2520to%252013%2525%2520higher%250Aaccuracy%2520than%2520prior%2520work%252C%2520on%2520a%2520variety%2520of%2520downstream%2520tasks%252C%2520in%2520the%2520presence%2520of%250Alimited%2520labeled%2520data.%2520Code%2520at%2520https%253A//github.com/BigML-CS-UCLA/MKDT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02116v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dataset%20Distillation%20via%20Knowledge%20Distillation%3A%20Towards%20Efficient%0A%20%20Self-Supervised%20Pre-Training%20of%20Deep%20Networks&entry.906535625=Siddharth%20Joshi%20and%20Jiayi%20Ni%20and%20Baharan%20Mirzasoleiman&entry.1292438233=%20%20Dataset%20distillation%20%28DD%29%20generates%20small%20synthetic%20datasets%20that%20can%0Aefficiently%20train%20deep%20networks%20with%20a%20limited%20amount%20of%20memory%20and%20compute.%0ADespite%20the%20success%20of%20DD%20methods%20for%20supervised%20learning%2C%20DD%20for%0Aself-supervised%20pre-training%20of%20deep%20models%20has%20remained%20unaddressed.%0APre-training%20on%20unlabeled%20data%20is%20crucial%20for%20efficiently%20generalizing%20to%0Adownstream%20tasks%20with%20limited%20labeled%20data.%20In%20this%20work%2C%20we%20propose%20the%20first%0Aeffective%20DD%20method%20for%20SSL%20pre-training.%20First%2C%20we%20show%2C%20theoretically%20and%0Aempirically%2C%20that%20naive%20application%20of%20supervised%20DD%20methods%20to%20SSL%20fails%2C%20due%0Ato%20the%20high%20variance%20of%20the%20SSL%20gradient.%20Then%2C%20we%20address%20this%20issue%20by%0Arelying%20on%20insights%20from%20knowledge%20distillation%20%28KD%29%20literature.%20Specifically%2C%0Awe%20train%20a%20small%20student%20model%20to%20match%20the%20representations%20of%20a%20larger%20teacher%0Amodel%20trained%20with%20SSL.%20Then%2C%20we%20generate%20a%20small%20synthetic%20dataset%20by%20matching%0Athe%20training%20trajectories%20of%20the%20student%20models.%20As%20the%20KD%20objective%20has%0Aconsiderably%20lower%20variance%20than%20SSL%2C%20our%20approach%20can%20generate%20synthetic%0Adatasets%20that%20can%20successfully%20pre-train%20high-quality%20encoders.%20Through%0Aextensive%20experiments%2C%20we%20show%20that%20our%20distilled%20sets%20lead%20to%20up%20to%2013%25%20higher%0Aaccuracy%20than%20prior%20work%2C%20on%20a%20variety%20of%20downstream%20tasks%2C%20in%20the%20presence%20of%0Alimited%20labeled%20data.%20Code%20at%20https%3A//github.com/BigML-CS-UCLA/MKDT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02116v2&entry.124074799=Read"},
{"title": "Integrating Inverse and Forward Modeling for Sparse Temporal Data from\n  Sensor Networks", "author": "Julian Vexler and Bj\u00f6rn Vieten and Martin Nelke and Stefan Kramer", "abstract": "  We present CavePerception, a framework for the analysis of sparse data from\nsensor networks that incorporates elements of inverse modeling and forward\nmodeling. By integrating machine learning with physical modeling in a\nhypotheses space, we aim to improve the interpretability of sparse, noisy, and\npotentially incomplete sensor data. The framework assumes data from a\ntwo-dimensional sensor network laid out in a graph structure that detects\ncertain objects, with certain motion patterns. Examples of such sensors are\nmagnetometers. Given knowledge about the objects and the way they act on the\nsensors, one can develop a data generator that produces data from simulated\nmotions of the objects across the sensor field. The framework uses the\nsimulated data to infer object behaviors across the sensor network. The\napproach is experimentally tested on real-world data, where magnetometers are\nused on an airport to detect and identify aircraft motions. Experiments\ndemonstrate the value of integrating inverse and forward modeling, enabling\nintelligent systems to better understand and predict complex, sensor-driven\nevents.\n", "link": "http://arxiv.org/abs/2502.13638v1", "date": "2025-02-19", "relevancy": 2.0605, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5608}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5112}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Inverse%20and%20Forward%20Modeling%20for%20Sparse%20Temporal%20Data%20from%0A%20%20Sensor%20Networks&body=Title%3A%20Integrating%20Inverse%20and%20Forward%20Modeling%20for%20Sparse%20Temporal%20Data%20from%0A%20%20Sensor%20Networks%0AAuthor%3A%20Julian%20Vexler%20and%20Bj%C3%B6rn%20Vieten%20and%20Martin%20Nelke%20and%20Stefan%20Kramer%0AAbstract%3A%20%20%20We%20present%20CavePerception%2C%20a%20framework%20for%20the%20analysis%20of%20sparse%20data%20from%0Asensor%20networks%20that%20incorporates%20elements%20of%20inverse%20modeling%20and%20forward%0Amodeling.%20By%20integrating%20machine%20learning%20with%20physical%20modeling%20in%20a%0Ahypotheses%20space%2C%20we%20aim%20to%20improve%20the%20interpretability%20of%20sparse%2C%20noisy%2C%20and%0Apotentially%20incomplete%20sensor%20data.%20The%20framework%20assumes%20data%20from%20a%0Atwo-dimensional%20sensor%20network%20laid%20out%20in%20a%20graph%20structure%20that%20detects%0Acertain%20objects%2C%20with%20certain%20motion%20patterns.%20Examples%20of%20such%20sensors%20are%0Amagnetometers.%20Given%20knowledge%20about%20the%20objects%20and%20the%20way%20they%20act%20on%20the%0Asensors%2C%20one%20can%20develop%20a%20data%20generator%20that%20produces%20data%20from%20simulated%0Amotions%20of%20the%20objects%20across%20the%20sensor%20field.%20The%20framework%20uses%20the%0Asimulated%20data%20to%20infer%20object%20behaviors%20across%20the%20sensor%20network.%20The%0Aapproach%20is%20experimentally%20tested%20on%20real-world%20data%2C%20where%20magnetometers%20are%0Aused%20on%20an%20airport%20to%20detect%20and%20identify%20aircraft%20motions.%20Experiments%0Ademonstrate%20the%20value%20of%20integrating%20inverse%20and%20forward%20modeling%2C%20enabling%0Aintelligent%20systems%20to%20better%20understand%20and%20predict%20complex%2C%20sensor-driven%0Aevents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Inverse%2520and%2520Forward%2520Modeling%2520for%2520Sparse%2520Temporal%2520Data%2520from%250A%2520%2520Sensor%2520Networks%26entry.906535625%3DJulian%2520Vexler%2520and%2520Bj%25C3%25B6rn%2520Vieten%2520and%2520Martin%2520Nelke%2520and%2520Stefan%2520Kramer%26entry.1292438233%3D%2520%2520We%2520present%2520CavePerception%252C%2520a%2520framework%2520for%2520the%2520analysis%2520of%2520sparse%2520data%2520from%250Asensor%2520networks%2520that%2520incorporates%2520elements%2520of%2520inverse%2520modeling%2520and%2520forward%250Amodeling.%2520By%2520integrating%2520machine%2520learning%2520with%2520physical%2520modeling%2520in%2520a%250Ahypotheses%2520space%252C%2520we%2520aim%2520to%2520improve%2520the%2520interpretability%2520of%2520sparse%252C%2520noisy%252C%2520and%250Apotentially%2520incomplete%2520sensor%2520data.%2520The%2520framework%2520assumes%2520data%2520from%2520a%250Atwo-dimensional%2520sensor%2520network%2520laid%2520out%2520in%2520a%2520graph%2520structure%2520that%2520detects%250Acertain%2520objects%252C%2520with%2520certain%2520motion%2520patterns.%2520Examples%2520of%2520such%2520sensors%2520are%250Amagnetometers.%2520Given%2520knowledge%2520about%2520the%2520objects%2520and%2520the%2520way%2520they%2520act%2520on%2520the%250Asensors%252C%2520one%2520can%2520develop%2520a%2520data%2520generator%2520that%2520produces%2520data%2520from%2520simulated%250Amotions%2520of%2520the%2520objects%2520across%2520the%2520sensor%2520field.%2520The%2520framework%2520uses%2520the%250Asimulated%2520data%2520to%2520infer%2520object%2520behaviors%2520across%2520the%2520sensor%2520network.%2520The%250Aapproach%2520is%2520experimentally%2520tested%2520on%2520real-world%2520data%252C%2520where%2520magnetometers%2520are%250Aused%2520on%2520an%2520airport%2520to%2520detect%2520and%2520identify%2520aircraft%2520motions.%2520Experiments%250Ademonstrate%2520the%2520value%2520of%2520integrating%2520inverse%2520and%2520forward%2520modeling%252C%2520enabling%250Aintelligent%2520systems%2520to%2520better%2520understand%2520and%2520predict%2520complex%252C%2520sensor-driven%250Aevents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Inverse%20and%20Forward%20Modeling%20for%20Sparse%20Temporal%20Data%20from%0A%20%20Sensor%20Networks&entry.906535625=Julian%20Vexler%20and%20Bj%C3%B6rn%20Vieten%20and%20Martin%20Nelke%20and%20Stefan%20Kramer&entry.1292438233=%20%20We%20present%20CavePerception%2C%20a%20framework%20for%20the%20analysis%20of%20sparse%20data%20from%0Asensor%20networks%20that%20incorporates%20elements%20of%20inverse%20modeling%20and%20forward%0Amodeling.%20By%20integrating%20machine%20learning%20with%20physical%20modeling%20in%20a%0Ahypotheses%20space%2C%20we%20aim%20to%20improve%20the%20interpretability%20of%20sparse%2C%20noisy%2C%20and%0Apotentially%20incomplete%20sensor%20data.%20The%20framework%20assumes%20data%20from%20a%0Atwo-dimensional%20sensor%20network%20laid%20out%20in%20a%20graph%20structure%20that%20detects%0Acertain%20objects%2C%20with%20certain%20motion%20patterns.%20Examples%20of%20such%20sensors%20are%0Amagnetometers.%20Given%20knowledge%20about%20the%20objects%20and%20the%20way%20they%20act%20on%20the%0Asensors%2C%20one%20can%20develop%20a%20data%20generator%20that%20produces%20data%20from%20simulated%0Amotions%20of%20the%20objects%20across%20the%20sensor%20field.%20The%20framework%20uses%20the%0Asimulated%20data%20to%20infer%20object%20behaviors%20across%20the%20sensor%20network.%20The%0Aapproach%20is%20experimentally%20tested%20on%20real-world%20data%2C%20where%20magnetometers%20are%0Aused%20on%20an%20airport%20to%20detect%20and%20identify%20aircraft%20motions.%20Experiments%0Ademonstrate%20the%20value%20of%20integrating%20inverse%20and%20forward%20modeling%2C%20enabling%0Aintelligent%20systems%20to%20better%20understand%20and%20predict%20complex%2C%20sensor-driven%0Aevents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13638v1&entry.124074799=Read"},
{"title": "Refining embeddings with fill-tuning: data-efficient generalised\n  performance improvements for materials foundation models", "author": "Matthew P. Wilson and Edward O. Pyzer-Knapp and Nicolas Galichet and Luke Dicks", "abstract": "  Pretrained foundation models learn embeddings that can be used for a wide\nrange of downstream tasks. These embeddings optimise general performance, and\nif insufficiently accurate at a specific task the model can be fine-tuned to\nimprove performance. For all current methodologies this operation necessarily\ndegrades performance on all out-of-distribution tasks. In this work we present\n'fill-tuning', a novel methodology to generate datasets for continued\npretraining of foundation models that are not suited to a particular downstream\ntask, but instead aim to correct poor regions of the embedding. We present the\napplication of roughness analysis to latent space topologies and illustrate how\nit can be used to propose data that will be most valuable to improving the\nembedding. We apply fill-tuning to a set of state-of-the-art materials\nfoundation models trained on $O(10^9)$ data points and show model improvement\nof almost 1% in all downstream tasks with the addition of only 100 data points.\nThis method provides a route to the general improvement of foundation models at\nthe computational cost of fine-tuning.\n", "link": "http://arxiv.org/abs/2502.13886v1", "date": "2025-02-19", "relevancy": 2.0565, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5269}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5179}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Refining%20embeddings%20with%20fill-tuning%3A%20data-efficient%20generalised%0A%20%20performance%20improvements%20for%20materials%20foundation%20models&body=Title%3A%20Refining%20embeddings%20with%20fill-tuning%3A%20data-efficient%20generalised%0A%20%20performance%20improvements%20for%20materials%20foundation%20models%0AAuthor%3A%20Matthew%20P.%20Wilson%20and%20Edward%20O.%20Pyzer-Knapp%20and%20Nicolas%20Galichet%20and%20Luke%20Dicks%0AAbstract%3A%20%20%20Pretrained%20foundation%20models%20learn%20embeddings%20that%20can%20be%20used%20for%20a%20wide%0Arange%20of%20downstream%20tasks.%20These%20embeddings%20optimise%20general%20performance%2C%20and%0Aif%20insufficiently%20accurate%20at%20a%20specific%20task%20the%20model%20can%20be%20fine-tuned%20to%0Aimprove%20performance.%20For%20all%20current%20methodologies%20this%20operation%20necessarily%0Adegrades%20performance%20on%20all%20out-of-distribution%20tasks.%20In%20this%20work%20we%20present%0A%27fill-tuning%27%2C%20a%20novel%20methodology%20to%20generate%20datasets%20for%20continued%0Apretraining%20of%20foundation%20models%20that%20are%20not%20suited%20to%20a%20particular%20downstream%0Atask%2C%20but%20instead%20aim%20to%20correct%20poor%20regions%20of%20the%20embedding.%20We%20present%20the%0Aapplication%20of%20roughness%20analysis%20to%20latent%20space%20topologies%20and%20illustrate%20how%0Ait%20can%20be%20used%20to%20propose%20data%20that%20will%20be%20most%20valuable%20to%20improving%20the%0Aembedding.%20We%20apply%20fill-tuning%20to%20a%20set%20of%20state-of-the-art%20materials%0Afoundation%20models%20trained%20on%20%24O%2810%5E9%29%24%20data%20points%20and%20show%20model%20improvement%0Aof%20almost%201%25%20in%20all%20downstream%20tasks%20with%20the%20addition%20of%20only%20100%20data%20points.%0AThis%20method%20provides%20a%20route%20to%20the%20general%20improvement%20of%20foundation%20models%20at%0Athe%20computational%20cost%20of%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13886v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefining%2520embeddings%2520with%2520fill-tuning%253A%2520data-efficient%2520generalised%250A%2520%2520performance%2520improvements%2520for%2520materials%2520foundation%2520models%26entry.906535625%3DMatthew%2520P.%2520Wilson%2520and%2520Edward%2520O.%2520Pyzer-Knapp%2520and%2520Nicolas%2520Galichet%2520and%2520Luke%2520Dicks%26entry.1292438233%3D%2520%2520Pretrained%2520foundation%2520models%2520learn%2520embeddings%2520that%2520can%2520be%2520used%2520for%2520a%2520wide%250Arange%2520of%2520downstream%2520tasks.%2520These%2520embeddings%2520optimise%2520general%2520performance%252C%2520and%250Aif%2520insufficiently%2520accurate%2520at%2520a%2520specific%2520task%2520the%2520model%2520can%2520be%2520fine-tuned%2520to%250Aimprove%2520performance.%2520For%2520all%2520current%2520methodologies%2520this%2520operation%2520necessarily%250Adegrades%2520performance%2520on%2520all%2520out-of-distribution%2520tasks.%2520In%2520this%2520work%2520we%2520present%250A%2527fill-tuning%2527%252C%2520a%2520novel%2520methodology%2520to%2520generate%2520datasets%2520for%2520continued%250Apretraining%2520of%2520foundation%2520models%2520that%2520are%2520not%2520suited%2520to%2520a%2520particular%2520downstream%250Atask%252C%2520but%2520instead%2520aim%2520to%2520correct%2520poor%2520regions%2520of%2520the%2520embedding.%2520We%2520present%2520the%250Aapplication%2520of%2520roughness%2520analysis%2520to%2520latent%2520space%2520topologies%2520and%2520illustrate%2520how%250Ait%2520can%2520be%2520used%2520to%2520propose%2520data%2520that%2520will%2520be%2520most%2520valuable%2520to%2520improving%2520the%250Aembedding.%2520We%2520apply%2520fill-tuning%2520to%2520a%2520set%2520of%2520state-of-the-art%2520materials%250Afoundation%2520models%2520trained%2520on%2520%2524O%252810%255E9%2529%2524%2520data%2520points%2520and%2520show%2520model%2520improvement%250Aof%2520almost%25201%2525%2520in%2520all%2520downstream%2520tasks%2520with%2520the%2520addition%2520of%2520only%2520100%2520data%2520points.%250AThis%2520method%2520provides%2520a%2520route%2520to%2520the%2520general%2520improvement%2520of%2520foundation%2520models%2520at%250Athe%2520computational%2520cost%2520of%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13886v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Refining%20embeddings%20with%20fill-tuning%3A%20data-efficient%20generalised%0A%20%20performance%20improvements%20for%20materials%20foundation%20models&entry.906535625=Matthew%20P.%20Wilson%20and%20Edward%20O.%20Pyzer-Knapp%20and%20Nicolas%20Galichet%20and%20Luke%20Dicks&entry.1292438233=%20%20Pretrained%20foundation%20models%20learn%20embeddings%20that%20can%20be%20used%20for%20a%20wide%0Arange%20of%20downstream%20tasks.%20These%20embeddings%20optimise%20general%20performance%2C%20and%0Aif%20insufficiently%20accurate%20at%20a%20specific%20task%20the%20model%20can%20be%20fine-tuned%20to%0Aimprove%20performance.%20For%20all%20current%20methodologies%20this%20operation%20necessarily%0Adegrades%20performance%20on%20all%20out-of-distribution%20tasks.%20In%20this%20work%20we%20present%0A%27fill-tuning%27%2C%20a%20novel%20methodology%20to%20generate%20datasets%20for%20continued%0Apretraining%20of%20foundation%20models%20that%20are%20not%20suited%20to%20a%20particular%20downstream%0Atask%2C%20but%20instead%20aim%20to%20correct%20poor%20regions%20of%20the%20embedding.%20We%20present%20the%0Aapplication%20of%20roughness%20analysis%20to%20latent%20space%20topologies%20and%20illustrate%20how%0Ait%20can%20be%20used%20to%20propose%20data%20that%20will%20be%20most%20valuable%20to%20improving%20the%0Aembedding.%20We%20apply%20fill-tuning%20to%20a%20set%20of%20state-of-the-art%20materials%0Afoundation%20models%20trained%20on%20%24O%2810%5E9%29%24%20data%20points%20and%20show%20model%20improvement%0Aof%20almost%201%25%20in%20all%20downstream%20tasks%20with%20the%20addition%20of%20only%20100%20data%20points.%0AThis%20method%20provides%20a%20route%20to%20the%20general%20improvement%20of%20foundation%20models%20at%0Athe%20computational%20cost%20of%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13886v1&entry.124074799=Read"},
{"title": "Multimodal Fake News Video Explanation Generation: Dataset, Model, and\n  Evaluation", "author": "Lizhi Chen and Zhong Qian and Peifeng Li and Qiaoming Zhu", "abstract": "  Although existing methods have addressed fake news video detection as a\nclassification problem, it is not clear why certain news content is identified\nas fake. Without proper explanation, end users may not be able to understand\nthe potential meaning of fake news. Therefore, we propose a novel task, Fake\nNews Video Explanation (FNVE), to generate natural language explanations that\nreveal the falseness of news videos. To this end, we first developed ONVE and\nVTSE, two new datasets to explain fake news video posts. Then, we propose a\nMultimodal Relation Graph Transformer (MRGT) model to benchmark ONVE and VTSE.\nMRGT introduces a multimodal relation graph to comprehensively represent\nmultimodal relations and then introduces a BART-based decoder to explain\ngenerations. The experimental results show that the proposed MRGT outperforms\nthe strong baselines. In addition, the human evaluation on the annotated ONVE\nand VTSE also achieves high scores in terms of adequacy rating.\n", "link": "http://arxiv.org/abs/2501.08514v2", "date": "2025-02-19", "relevancy": 2.0488, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5204}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5101}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Fake%20News%20Video%20Explanation%20Generation%3A%20Dataset%2C%20Model%2C%20and%0A%20%20Evaluation&body=Title%3A%20Multimodal%20Fake%20News%20Video%20Explanation%20Generation%3A%20Dataset%2C%20Model%2C%20and%0A%20%20Evaluation%0AAuthor%3A%20Lizhi%20Chen%20and%20Zhong%20Qian%20and%20Peifeng%20Li%20and%20Qiaoming%20Zhu%0AAbstract%3A%20%20%20Although%20existing%20methods%20have%20addressed%20fake%20news%20video%20detection%20as%20a%0Aclassification%20problem%2C%20it%20is%20not%20clear%20why%20certain%20news%20content%20is%20identified%0Aas%20fake.%20Without%20proper%20explanation%2C%20end%20users%20may%20not%20be%20able%20to%20understand%0Athe%20potential%20meaning%20of%20fake%20news.%20Therefore%2C%20we%20propose%20a%20novel%20task%2C%20Fake%0ANews%20Video%20Explanation%20%28FNVE%29%2C%20to%20generate%20natural%20language%20explanations%20that%0Areveal%20the%20falseness%20of%20news%20videos.%20To%20this%20end%2C%20we%20first%20developed%20ONVE%20and%0AVTSE%2C%20two%20new%20datasets%20to%20explain%20fake%20news%20video%20posts.%20Then%2C%20we%20propose%20a%0AMultimodal%20Relation%20Graph%20Transformer%20%28MRGT%29%20model%20to%20benchmark%20ONVE%20and%20VTSE.%0AMRGT%20introduces%20a%20multimodal%20relation%20graph%20to%20comprehensively%20represent%0Amultimodal%20relations%20and%20then%20introduces%20a%20BART-based%20decoder%20to%20explain%0Agenerations.%20The%20experimental%20results%20show%20that%20the%20proposed%20MRGT%20outperforms%0Athe%20strong%20baselines.%20In%20addition%2C%20the%20human%20evaluation%20on%20the%20annotated%20ONVE%0Aand%20VTSE%20also%20achieves%20high%20scores%20in%20terms%20of%20adequacy%20rating.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08514v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Fake%2520News%2520Video%2520Explanation%2520Generation%253A%2520Dataset%252C%2520Model%252C%2520and%250A%2520%2520Evaluation%26entry.906535625%3DLizhi%2520Chen%2520and%2520Zhong%2520Qian%2520and%2520Peifeng%2520Li%2520and%2520Qiaoming%2520Zhu%26entry.1292438233%3D%2520%2520Although%2520existing%2520methods%2520have%2520addressed%2520fake%2520news%2520video%2520detection%2520as%2520a%250Aclassification%2520problem%252C%2520it%2520is%2520not%2520clear%2520why%2520certain%2520news%2520content%2520is%2520identified%250Aas%2520fake.%2520Without%2520proper%2520explanation%252C%2520end%2520users%2520may%2520not%2520be%2520able%2520to%2520understand%250Athe%2520potential%2520meaning%2520of%2520fake%2520news.%2520Therefore%252C%2520we%2520propose%2520a%2520novel%2520task%252C%2520Fake%250ANews%2520Video%2520Explanation%2520%2528FNVE%2529%252C%2520to%2520generate%2520natural%2520language%2520explanations%2520that%250Areveal%2520the%2520falseness%2520of%2520news%2520videos.%2520To%2520this%2520end%252C%2520we%2520first%2520developed%2520ONVE%2520and%250AVTSE%252C%2520two%2520new%2520datasets%2520to%2520explain%2520fake%2520news%2520video%2520posts.%2520Then%252C%2520we%2520propose%2520a%250AMultimodal%2520Relation%2520Graph%2520Transformer%2520%2528MRGT%2529%2520model%2520to%2520benchmark%2520ONVE%2520and%2520VTSE.%250AMRGT%2520introduces%2520a%2520multimodal%2520relation%2520graph%2520to%2520comprehensively%2520represent%250Amultimodal%2520relations%2520and%2520then%2520introduces%2520a%2520BART-based%2520decoder%2520to%2520explain%250Agenerations.%2520The%2520experimental%2520results%2520show%2520that%2520the%2520proposed%2520MRGT%2520outperforms%250Athe%2520strong%2520baselines.%2520In%2520addition%252C%2520the%2520human%2520evaluation%2520on%2520the%2520annotated%2520ONVE%250Aand%2520VTSE%2520also%2520achieves%2520high%2520scores%2520in%2520terms%2520of%2520adequacy%2520rating.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08514v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Fake%20News%20Video%20Explanation%20Generation%3A%20Dataset%2C%20Model%2C%20and%0A%20%20Evaluation&entry.906535625=Lizhi%20Chen%20and%20Zhong%20Qian%20and%20Peifeng%20Li%20and%20Qiaoming%20Zhu&entry.1292438233=%20%20Although%20existing%20methods%20have%20addressed%20fake%20news%20video%20detection%20as%20a%0Aclassification%20problem%2C%20it%20is%20not%20clear%20why%20certain%20news%20content%20is%20identified%0Aas%20fake.%20Without%20proper%20explanation%2C%20end%20users%20may%20not%20be%20able%20to%20understand%0Athe%20potential%20meaning%20of%20fake%20news.%20Therefore%2C%20we%20propose%20a%20novel%20task%2C%20Fake%0ANews%20Video%20Explanation%20%28FNVE%29%2C%20to%20generate%20natural%20language%20explanations%20that%0Areveal%20the%20falseness%20of%20news%20videos.%20To%20this%20end%2C%20we%20first%20developed%20ONVE%20and%0AVTSE%2C%20two%20new%20datasets%20to%20explain%20fake%20news%20video%20posts.%20Then%2C%20we%20propose%20a%0AMultimodal%20Relation%20Graph%20Transformer%20%28MRGT%29%20model%20to%20benchmark%20ONVE%20and%20VTSE.%0AMRGT%20introduces%20a%20multimodal%20relation%20graph%20to%20comprehensively%20represent%0Amultimodal%20relations%20and%20then%20introduces%20a%20BART-based%20decoder%20to%20explain%0Agenerations.%20The%20experimental%20results%20show%20that%20the%20proposed%20MRGT%20outperforms%0Athe%20strong%20baselines.%20In%20addition%2C%20the%20human%20evaluation%20on%20the%20annotated%20ONVE%0Aand%20VTSE%20also%20achieves%20high%20scores%20in%20terms%20of%20adequacy%20rating.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08514v2&entry.124074799=Read"},
{"title": "LESA: Learnable LLM Layer Scaling-Up", "author": "Yifei Yang and Zouying Cao and Xinbei Ma and Yao Yao and Libo Qin and Zhi Chen and Hai Zhao", "abstract": "  Training Large Language Models (LLMs) from scratch requires immense\ncomputational resources, making it prohibitively expensive. Model scaling-up\noffers a promising solution by leveraging the parameters of smaller models to\ncreate larger ones. However, existing depth scaling-up methods rely on\nempirical heuristic rules for layer duplication, which result in poorer\ninitialization and slower convergence during continual pre-training. We propose\n\\textbf{LESA}, a novel learnable method for depth scaling-up. By concatenating\nparameters from each layer and applying Singular Value Decomposition, we\nuncover latent patterns between layers, suggesting that inter-layer parameters\ncan be learned. LESA uses a neural network to predict the parameters inserted\nbetween adjacent layers, enabling better initialization and faster training.\nExperiments show that LESA outperforms existing baselines, achieving superior\nperformance with less than half the computational cost during continual\npre-training. Extensive analyses demonstrate its effectiveness across different\nmodel sizes and tasks.\n", "link": "http://arxiv.org/abs/2502.13794v1", "date": "2025-02-19", "relevancy": 2.0424, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5294}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4992}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LESA%3A%20Learnable%20LLM%20Layer%20Scaling-Up&body=Title%3A%20LESA%3A%20Learnable%20LLM%20Layer%20Scaling-Up%0AAuthor%3A%20Yifei%20Yang%20and%20Zouying%20Cao%20and%20Xinbei%20Ma%20and%20Yao%20Yao%20and%20Libo%20Qin%20and%20Zhi%20Chen%20and%20Hai%20Zhao%0AAbstract%3A%20%20%20Training%20Large%20Language%20Models%20%28LLMs%29%20from%20scratch%20requires%20immense%0Acomputational%20resources%2C%20making%20it%20prohibitively%20expensive.%20Model%20scaling-up%0Aoffers%20a%20promising%20solution%20by%20leveraging%20the%20parameters%20of%20smaller%20models%20to%0Acreate%20larger%20ones.%20However%2C%20existing%20depth%20scaling-up%20methods%20rely%20on%0Aempirical%20heuristic%20rules%20for%20layer%20duplication%2C%20which%20result%20in%20poorer%0Ainitialization%20and%20slower%20convergence%20during%20continual%20pre-training.%20We%20propose%0A%5Ctextbf%7BLESA%7D%2C%20a%20novel%20learnable%20method%20for%20depth%20scaling-up.%20By%20concatenating%0Aparameters%20from%20each%20layer%20and%20applying%20Singular%20Value%20Decomposition%2C%20we%0Auncover%20latent%20patterns%20between%20layers%2C%20suggesting%20that%20inter-layer%20parameters%0Acan%20be%20learned.%20LESA%20uses%20a%20neural%20network%20to%20predict%20the%20parameters%20inserted%0Abetween%20adjacent%20layers%2C%20enabling%20better%20initialization%20and%20faster%20training.%0AExperiments%20show%20that%20LESA%20outperforms%20existing%20baselines%2C%20achieving%20superior%0Aperformance%20with%20less%20than%20half%20the%20computational%20cost%20during%20continual%0Apre-training.%20Extensive%20analyses%20demonstrate%20its%20effectiveness%20across%20different%0Amodel%20sizes%20and%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLESA%253A%2520Learnable%2520LLM%2520Layer%2520Scaling-Up%26entry.906535625%3DYifei%2520Yang%2520and%2520Zouying%2520Cao%2520and%2520Xinbei%2520Ma%2520and%2520Yao%2520Yao%2520and%2520Libo%2520Qin%2520and%2520Zhi%2520Chen%2520and%2520Hai%2520Zhao%26entry.1292438233%3D%2520%2520Training%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520from%2520scratch%2520requires%2520immense%250Acomputational%2520resources%252C%2520making%2520it%2520prohibitively%2520expensive.%2520Model%2520scaling-up%250Aoffers%2520a%2520promising%2520solution%2520by%2520leveraging%2520the%2520parameters%2520of%2520smaller%2520models%2520to%250Acreate%2520larger%2520ones.%2520However%252C%2520existing%2520depth%2520scaling-up%2520methods%2520rely%2520on%250Aempirical%2520heuristic%2520rules%2520for%2520layer%2520duplication%252C%2520which%2520result%2520in%2520poorer%250Ainitialization%2520and%2520slower%2520convergence%2520during%2520continual%2520pre-training.%2520We%2520propose%250A%255Ctextbf%257BLESA%257D%252C%2520a%2520novel%2520learnable%2520method%2520for%2520depth%2520scaling-up.%2520By%2520concatenating%250Aparameters%2520from%2520each%2520layer%2520and%2520applying%2520Singular%2520Value%2520Decomposition%252C%2520we%250Auncover%2520latent%2520patterns%2520between%2520layers%252C%2520suggesting%2520that%2520inter-layer%2520parameters%250Acan%2520be%2520learned.%2520LESA%2520uses%2520a%2520neural%2520network%2520to%2520predict%2520the%2520parameters%2520inserted%250Abetween%2520adjacent%2520layers%252C%2520enabling%2520better%2520initialization%2520and%2520faster%2520training.%250AExperiments%2520show%2520that%2520LESA%2520outperforms%2520existing%2520baselines%252C%2520achieving%2520superior%250Aperformance%2520with%2520less%2520than%2520half%2520the%2520computational%2520cost%2520during%2520continual%250Apre-training.%2520Extensive%2520analyses%2520demonstrate%2520its%2520effectiveness%2520across%2520different%250Amodel%2520sizes%2520and%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LESA%3A%20Learnable%20LLM%20Layer%20Scaling-Up&entry.906535625=Yifei%20Yang%20and%20Zouying%20Cao%20and%20Xinbei%20Ma%20and%20Yao%20Yao%20and%20Libo%20Qin%20and%20Zhi%20Chen%20and%20Hai%20Zhao&entry.1292438233=%20%20Training%20Large%20Language%20Models%20%28LLMs%29%20from%20scratch%20requires%20immense%0Acomputational%20resources%2C%20making%20it%20prohibitively%20expensive.%20Model%20scaling-up%0Aoffers%20a%20promising%20solution%20by%20leveraging%20the%20parameters%20of%20smaller%20models%20to%0Acreate%20larger%20ones.%20However%2C%20existing%20depth%20scaling-up%20methods%20rely%20on%0Aempirical%20heuristic%20rules%20for%20layer%20duplication%2C%20which%20result%20in%20poorer%0Ainitialization%20and%20slower%20convergence%20during%20continual%20pre-training.%20We%20propose%0A%5Ctextbf%7BLESA%7D%2C%20a%20novel%20learnable%20method%20for%20depth%20scaling-up.%20By%20concatenating%0Aparameters%20from%20each%20layer%20and%20applying%20Singular%20Value%20Decomposition%2C%20we%0Auncover%20latent%20patterns%20between%20layers%2C%20suggesting%20that%20inter-layer%20parameters%0Acan%20be%20learned.%20LESA%20uses%20a%20neural%20network%20to%20predict%20the%20parameters%20inserted%0Abetween%20adjacent%20layers%2C%20enabling%20better%20initialization%20and%20faster%20training.%0AExperiments%20show%20that%20LESA%20outperforms%20existing%20baselines%2C%20achieving%20superior%0Aperformance%20with%20less%20than%20half%20the%20computational%20cost%20during%20continual%0Apre-training.%20Extensive%20analyses%20demonstrate%20its%20effectiveness%20across%20different%0Amodel%20sizes%20and%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13794v1&entry.124074799=Read"},
{"title": "REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large\n  Language Models", "author": "DongGeon Lee and Hwanjo Yu", "abstract": "  Hallucinations in large language model (LLM) outputs severely limit their\nreliability in knowledge-intensive tasks such as question answering. To address\nthis challenge, we introduce REFIND (Retrieval-augmented Factuality\nhallucINation Detection), a novel framework that detects hallucinated spans\nwithin LLM outputs by directly leveraging retrieved documents. As part of the\nREFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that\nquantifies the sensitivity of LLM outputs to retrieved evidence. This\ninnovative approach enables REFIND to efficiently and accurately detect\nhallucinations, setting it apart from existing methods. In the evaluation,\nREFIND demonstrated robustness across nine languages, including low-resource\nsettings, and significantly outperformed baseline models, achieving superior\nIoU scores in identifying hallucinated spans. This work highlights the\neffectiveness of quantifying context sensitivity for hallucination detection,\nthereby paving the way for more reliable and trustworthy LLM applications\nacross diverse languages.\n", "link": "http://arxiv.org/abs/2502.13622v1", "date": "2025-02-19", "relevancy": 2.0236, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5128}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5045}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REFIND%3A%20Retrieval-Augmented%20Factuality%20Hallucination%20Detection%20in%20Large%0A%20%20Language%20Models&body=Title%3A%20REFIND%3A%20Retrieval-Augmented%20Factuality%20Hallucination%20Detection%20in%20Large%0A%20%20Language%20Models%0AAuthor%3A%20DongGeon%20Lee%20and%20Hwanjo%20Yu%0AAbstract%3A%20%20%20Hallucinations%20in%20large%20language%20model%20%28LLM%29%20outputs%20severely%20limit%20their%0Areliability%20in%20knowledge-intensive%20tasks%20such%20as%20question%20answering.%20To%20address%0Athis%20challenge%2C%20we%20introduce%20REFIND%20%28Retrieval-augmented%20Factuality%0AhallucINation%20Detection%29%2C%20a%20novel%20framework%20that%20detects%20hallucinated%20spans%0Awithin%20LLM%20outputs%20by%20directly%20leveraging%20retrieved%20documents.%20As%20part%20of%20the%0AREFIND%2C%20we%20propose%20the%20Context%20Sensitivity%20Ratio%20%28CSR%29%2C%20a%20novel%20metric%20that%0Aquantifies%20the%20sensitivity%20of%20LLM%20outputs%20to%20retrieved%20evidence.%20This%0Ainnovative%20approach%20enables%20REFIND%20to%20efficiently%20and%20accurately%20detect%0Ahallucinations%2C%20setting%20it%20apart%20from%20existing%20methods.%20In%20the%20evaluation%2C%0AREFIND%20demonstrated%20robustness%20across%20nine%20languages%2C%20including%20low-resource%0Asettings%2C%20and%20significantly%20outperformed%20baseline%20models%2C%20achieving%20superior%0AIoU%20scores%20in%20identifying%20hallucinated%20spans.%20This%20work%20highlights%20the%0Aeffectiveness%20of%20quantifying%20context%20sensitivity%20for%20hallucination%20detection%2C%0Athereby%20paving%20the%20way%20for%20more%20reliable%20and%20trustworthy%20LLM%20applications%0Aacross%20diverse%20languages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREFIND%253A%2520Retrieval-Augmented%2520Factuality%2520Hallucination%2520Detection%2520in%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DDongGeon%2520Lee%2520and%2520Hwanjo%2520Yu%26entry.1292438233%3D%2520%2520Hallucinations%2520in%2520large%2520language%2520model%2520%2528LLM%2529%2520outputs%2520severely%2520limit%2520their%250Areliability%2520in%2520knowledge-intensive%2520tasks%2520such%2520as%2520question%2520answering.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520introduce%2520REFIND%2520%2528Retrieval-augmented%2520Factuality%250AhallucINation%2520Detection%2529%252C%2520a%2520novel%2520framework%2520that%2520detects%2520hallucinated%2520spans%250Awithin%2520LLM%2520outputs%2520by%2520directly%2520leveraging%2520retrieved%2520documents.%2520As%2520part%2520of%2520the%250AREFIND%252C%2520we%2520propose%2520the%2520Context%2520Sensitivity%2520Ratio%2520%2528CSR%2529%252C%2520a%2520novel%2520metric%2520that%250Aquantifies%2520the%2520sensitivity%2520of%2520LLM%2520outputs%2520to%2520retrieved%2520evidence.%2520This%250Ainnovative%2520approach%2520enables%2520REFIND%2520to%2520efficiently%2520and%2520accurately%2520detect%250Ahallucinations%252C%2520setting%2520it%2520apart%2520from%2520existing%2520methods.%2520In%2520the%2520evaluation%252C%250AREFIND%2520demonstrated%2520robustness%2520across%2520nine%2520languages%252C%2520including%2520low-resource%250Asettings%252C%2520and%2520significantly%2520outperformed%2520baseline%2520models%252C%2520achieving%2520superior%250AIoU%2520scores%2520in%2520identifying%2520hallucinated%2520spans.%2520This%2520work%2520highlights%2520the%250Aeffectiveness%2520of%2520quantifying%2520context%2520sensitivity%2520for%2520hallucination%2520detection%252C%250Athereby%2520paving%2520the%2520way%2520for%2520more%2520reliable%2520and%2520trustworthy%2520LLM%2520applications%250Aacross%2520diverse%2520languages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REFIND%3A%20Retrieval-Augmented%20Factuality%20Hallucination%20Detection%20in%20Large%0A%20%20Language%20Models&entry.906535625=DongGeon%20Lee%20and%20Hwanjo%20Yu&entry.1292438233=%20%20Hallucinations%20in%20large%20language%20model%20%28LLM%29%20outputs%20severely%20limit%20their%0Areliability%20in%20knowledge-intensive%20tasks%20such%20as%20question%20answering.%20To%20address%0Athis%20challenge%2C%20we%20introduce%20REFIND%20%28Retrieval-augmented%20Factuality%0AhallucINation%20Detection%29%2C%20a%20novel%20framework%20that%20detects%20hallucinated%20spans%0Awithin%20LLM%20outputs%20by%20directly%20leveraging%20retrieved%20documents.%20As%20part%20of%20the%0AREFIND%2C%20we%20propose%20the%20Context%20Sensitivity%20Ratio%20%28CSR%29%2C%20a%20novel%20metric%20that%0Aquantifies%20the%20sensitivity%20of%20LLM%20outputs%20to%20retrieved%20evidence.%20This%0Ainnovative%20approach%20enables%20REFIND%20to%20efficiently%20and%20accurately%20detect%0Ahallucinations%2C%20setting%20it%20apart%20from%20existing%20methods.%20In%20the%20evaluation%2C%0AREFIND%20demonstrated%20robustness%20across%20nine%20languages%2C%20including%20low-resource%0Asettings%2C%20and%20significantly%20outperformed%20baseline%20models%2C%20achieving%20superior%0AIoU%20scores%20in%20identifying%20hallucinated%20spans.%20This%20work%20highlights%20the%0Aeffectiveness%20of%20quantifying%20context%20sensitivity%20for%20hallucination%20detection%2C%0Athereby%20paving%20the%20way%20for%20more%20reliable%20and%20trustworthy%20LLM%20applications%0Aacross%20diverse%20languages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13622v1&entry.124074799=Read"},
{"title": "GroundCap: A Visually Grounded Image Captioning Dataset", "author": "Daniel A. P. Oliveira and Louren\u00e7o Teodoro and David Martins de Matos", "abstract": "  Current image captioning systems lack the ability to link descriptive text to\nspecific visual elements, making their outputs difficult to verify. While\nrecent approaches offer some grounding capabilities, they cannot track object\nidentities across multiple references or ground both actions and objects\nsimultaneously. We propose a novel ID-based grounding system that enables\nconsistent object reference tracking and action-object linking, and present\nGroundCap, a dataset containing 52,016 images from 77 movies, with 344\nhuman-annotated and 52,016 automatically generated captions. Each caption is\ngrounded on detected objects (132 classes) and actions (51 classes) using a tag\nsystem that maintains object identity while linking actions to the\ncorresponding objects. Our approach features persistent object IDs for\nreference tracking, explicit action-object linking, and segmentation of\nbackground elements through K-means clustering. We propose gMETEOR, a metric\ncombining caption quality with grounding accuracy, and establish baseline\nperformance by fine-tuning Pixtral-12B. Human evaluation demonstrates our\napproach's effectiveness in producing verifiable descriptions with coherent\nobject references.\n", "link": "http://arxiv.org/abs/2502.13898v1", "date": "2025-02-19", "relevancy": 2.0167, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5228}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5039}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GroundCap%3A%20A%20Visually%20Grounded%20Image%20Captioning%20Dataset&body=Title%3A%20GroundCap%3A%20A%20Visually%20Grounded%20Image%20Captioning%20Dataset%0AAuthor%3A%20Daniel%20A.%20P.%20Oliveira%20and%20Louren%C3%A7o%20Teodoro%20and%20David%20Martins%20de%20Matos%0AAbstract%3A%20%20%20Current%20image%20captioning%20systems%20lack%20the%20ability%20to%20link%20descriptive%20text%20to%0Aspecific%20visual%20elements%2C%20making%20their%20outputs%20difficult%20to%20verify.%20While%0Arecent%20approaches%20offer%20some%20grounding%20capabilities%2C%20they%20cannot%20track%20object%0Aidentities%20across%20multiple%20references%20or%20ground%20both%20actions%20and%20objects%0Asimultaneously.%20We%20propose%20a%20novel%20ID-based%20grounding%20system%20that%20enables%0Aconsistent%20object%20reference%20tracking%20and%20action-object%20linking%2C%20and%20present%0AGroundCap%2C%20a%20dataset%20containing%2052%2C016%20images%20from%2077%20movies%2C%20with%20344%0Ahuman-annotated%20and%2052%2C016%20automatically%20generated%20captions.%20Each%20caption%20is%0Agrounded%20on%20detected%20objects%20%28132%20classes%29%20and%20actions%20%2851%20classes%29%20using%20a%20tag%0Asystem%20that%20maintains%20object%20identity%20while%20linking%20actions%20to%20the%0Acorresponding%20objects.%20Our%20approach%20features%20persistent%20object%20IDs%20for%0Areference%20tracking%2C%20explicit%20action-object%20linking%2C%20and%20segmentation%20of%0Abackground%20elements%20through%20K-means%20clustering.%20We%20propose%20gMETEOR%2C%20a%20metric%0Acombining%20caption%20quality%20with%20grounding%20accuracy%2C%20and%20establish%20baseline%0Aperformance%20by%20fine-tuning%20Pixtral-12B.%20Human%20evaluation%20demonstrates%20our%0Aapproach%27s%20effectiveness%20in%20producing%20verifiable%20descriptions%20with%20coherent%0Aobject%20references.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13898v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGroundCap%253A%2520A%2520Visually%2520Grounded%2520Image%2520Captioning%2520Dataset%26entry.906535625%3DDaniel%2520A.%2520P.%2520Oliveira%2520and%2520Louren%25C3%25A7o%2520Teodoro%2520and%2520David%2520Martins%2520de%2520Matos%26entry.1292438233%3D%2520%2520Current%2520image%2520captioning%2520systems%2520lack%2520the%2520ability%2520to%2520link%2520descriptive%2520text%2520to%250Aspecific%2520visual%2520elements%252C%2520making%2520their%2520outputs%2520difficult%2520to%2520verify.%2520While%250Arecent%2520approaches%2520offer%2520some%2520grounding%2520capabilities%252C%2520they%2520cannot%2520track%2520object%250Aidentities%2520across%2520multiple%2520references%2520or%2520ground%2520both%2520actions%2520and%2520objects%250Asimultaneously.%2520We%2520propose%2520a%2520novel%2520ID-based%2520grounding%2520system%2520that%2520enables%250Aconsistent%2520object%2520reference%2520tracking%2520and%2520action-object%2520linking%252C%2520and%2520present%250AGroundCap%252C%2520a%2520dataset%2520containing%252052%252C016%2520images%2520from%252077%2520movies%252C%2520with%2520344%250Ahuman-annotated%2520and%252052%252C016%2520automatically%2520generated%2520captions.%2520Each%2520caption%2520is%250Agrounded%2520on%2520detected%2520objects%2520%2528132%2520classes%2529%2520and%2520actions%2520%252851%2520classes%2529%2520using%2520a%2520tag%250Asystem%2520that%2520maintains%2520object%2520identity%2520while%2520linking%2520actions%2520to%2520the%250Acorresponding%2520objects.%2520Our%2520approach%2520features%2520persistent%2520object%2520IDs%2520for%250Areference%2520tracking%252C%2520explicit%2520action-object%2520linking%252C%2520and%2520segmentation%2520of%250Abackground%2520elements%2520through%2520K-means%2520clustering.%2520We%2520propose%2520gMETEOR%252C%2520a%2520metric%250Acombining%2520caption%2520quality%2520with%2520grounding%2520accuracy%252C%2520and%2520establish%2520baseline%250Aperformance%2520by%2520fine-tuning%2520Pixtral-12B.%2520Human%2520evaluation%2520demonstrates%2520our%250Aapproach%2527s%2520effectiveness%2520in%2520producing%2520verifiable%2520descriptions%2520with%2520coherent%250Aobject%2520references.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13898v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GroundCap%3A%20A%20Visually%20Grounded%20Image%20Captioning%20Dataset&entry.906535625=Daniel%20A.%20P.%20Oliveira%20and%20Louren%C3%A7o%20Teodoro%20and%20David%20Martins%20de%20Matos&entry.1292438233=%20%20Current%20image%20captioning%20systems%20lack%20the%20ability%20to%20link%20descriptive%20text%20to%0Aspecific%20visual%20elements%2C%20making%20their%20outputs%20difficult%20to%20verify.%20While%0Arecent%20approaches%20offer%20some%20grounding%20capabilities%2C%20they%20cannot%20track%20object%0Aidentities%20across%20multiple%20references%20or%20ground%20both%20actions%20and%20objects%0Asimultaneously.%20We%20propose%20a%20novel%20ID-based%20grounding%20system%20that%20enables%0Aconsistent%20object%20reference%20tracking%20and%20action-object%20linking%2C%20and%20present%0AGroundCap%2C%20a%20dataset%20containing%2052%2C016%20images%20from%2077%20movies%2C%20with%20344%0Ahuman-annotated%20and%2052%2C016%20automatically%20generated%20captions.%20Each%20caption%20is%0Agrounded%20on%20detected%20objects%20%28132%20classes%29%20and%20actions%20%2851%20classes%29%20using%20a%20tag%0Asystem%20that%20maintains%20object%20identity%20while%20linking%20actions%20to%20the%0Acorresponding%20objects.%20Our%20approach%20features%20persistent%20object%20IDs%20for%0Areference%20tracking%2C%20explicit%20action-object%20linking%2C%20and%20segmentation%20of%0Abackground%20elements%20through%20K-means%20clustering.%20We%20propose%20gMETEOR%2C%20a%20metric%0Acombining%20caption%20quality%20with%20grounding%20accuracy%2C%20and%20establish%20baseline%0Aperformance%20by%20fine-tuning%20Pixtral-12B.%20Human%20evaluation%20demonstrates%20our%0Aapproach%27s%20effectiveness%20in%20producing%20verifiable%20descriptions%20with%20coherent%0Aobject%20references.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13898v1&entry.124074799=Read"},
{"title": "Bayesian Comparisons Between Representations", "author": "Heiko H. Sch\u00fctt", "abstract": "  Which neural networks are similar is a fundamental question for both machine\nlearning and neuroscience. Here, I propose to base comparisons on the\npredictive distributions of linear readouts from intermediate representations.\nIn Bayesian statistics, the prior predictive distribution is a full description\nof the inductive bias and generalization of a model, making it a great basis\nfor comparisons. This distribution directly gives the evidence a dataset would\nprovide in favor of the model. If we want to compare multiple models to each\nother, we can use a metric for probability distributions like the\nJensen-Shannon distance or the total variation distance. As these are metrics,\nthis induces pseudo-metrics for representations, which measure how well two\nrepresentations could be distinguished based on a linear read out. For a linear\nreadout with a Gaussian prior on the read-out weights and Gaussian noise, we\ncan analytically compute the (prior and posterior) predictive distributions\nwithout approximations. These distributions depend only on the linear kernel\nmatrix of the representations in the model. Thus, the Bayesian metrics connect\nlinear read-out based comparisons to kernel based metrics like centered kernel\nalignment and representational similarity analysis. I demonstrate the new\nmethods with deep neural networks trained on ImageNet-1k comparing them to each\nother and a small subset of the Natural Scenes Dataset. The Bayesian\ncomparisons broadly agree with existing metrics, but are more stringent.\nEmpirically, evaluations vary less across different random image samples and\nyield informative results with full uncertainty information. Thus the proposed\nBayesian metrics nicely extend our toolkit for comparing representations.\n", "link": "http://arxiv.org/abs/2411.08739v2", "date": "2025-02-19", "relevancy": 2.0066, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5699}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Comparisons%20Between%20Representations&body=Title%3A%20Bayesian%20Comparisons%20Between%20Representations%0AAuthor%3A%20Heiko%20H.%20Sch%C3%BCtt%0AAbstract%3A%20%20%20Which%20neural%20networks%20are%20similar%20is%20a%20fundamental%20question%20for%20both%20machine%0Alearning%20and%20neuroscience.%20Here%2C%20I%20propose%20to%20base%20comparisons%20on%20the%0Apredictive%20distributions%20of%20linear%20readouts%20from%20intermediate%20representations.%0AIn%20Bayesian%20statistics%2C%20the%20prior%20predictive%20distribution%20is%20a%20full%20description%0Aof%20the%20inductive%20bias%20and%20generalization%20of%20a%20model%2C%20making%20it%20a%20great%20basis%0Afor%20comparisons.%20This%20distribution%20directly%20gives%20the%20evidence%20a%20dataset%20would%0Aprovide%20in%20favor%20of%20the%20model.%20If%20we%20want%20to%20compare%20multiple%20models%20to%20each%0Aother%2C%20we%20can%20use%20a%20metric%20for%20probability%20distributions%20like%20the%0AJensen-Shannon%20distance%20or%20the%20total%20variation%20distance.%20As%20these%20are%20metrics%2C%0Athis%20induces%20pseudo-metrics%20for%20representations%2C%20which%20measure%20how%20well%20two%0Arepresentations%20could%20be%20distinguished%20based%20on%20a%20linear%20read%20out.%20For%20a%20linear%0Areadout%20with%20a%20Gaussian%20prior%20on%20the%20read-out%20weights%20and%20Gaussian%20noise%2C%20we%0Acan%20analytically%20compute%20the%20%28prior%20and%20posterior%29%20predictive%20distributions%0Awithout%20approximations.%20These%20distributions%20depend%20only%20on%20the%20linear%20kernel%0Amatrix%20of%20the%20representations%20in%20the%20model.%20Thus%2C%20the%20Bayesian%20metrics%20connect%0Alinear%20read-out%20based%20comparisons%20to%20kernel%20based%20metrics%20like%20centered%20kernel%0Aalignment%20and%20representational%20similarity%20analysis.%20I%20demonstrate%20the%20new%0Amethods%20with%20deep%20neural%20networks%20trained%20on%20ImageNet-1k%20comparing%20them%20to%20each%0Aother%20and%20a%20small%20subset%20of%20the%20Natural%20Scenes%20Dataset.%20The%20Bayesian%0Acomparisons%20broadly%20agree%20with%20existing%20metrics%2C%20but%20are%20more%20stringent.%0AEmpirically%2C%20evaluations%20vary%20less%20across%20different%20random%20image%20samples%20and%0Ayield%20informative%20results%20with%20full%20uncertainty%20information.%20Thus%20the%20proposed%0ABayesian%20metrics%20nicely%20extend%20our%20toolkit%20for%20comparing%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08739v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Comparisons%2520Between%2520Representations%26entry.906535625%3DHeiko%2520H.%2520Sch%25C3%25BCtt%26entry.1292438233%3D%2520%2520Which%2520neural%2520networks%2520are%2520similar%2520is%2520a%2520fundamental%2520question%2520for%2520both%2520machine%250Alearning%2520and%2520neuroscience.%2520Here%252C%2520I%2520propose%2520to%2520base%2520comparisons%2520on%2520the%250Apredictive%2520distributions%2520of%2520linear%2520readouts%2520from%2520intermediate%2520representations.%250AIn%2520Bayesian%2520statistics%252C%2520the%2520prior%2520predictive%2520distribution%2520is%2520a%2520full%2520description%250Aof%2520the%2520inductive%2520bias%2520and%2520generalization%2520of%2520a%2520model%252C%2520making%2520it%2520a%2520great%2520basis%250Afor%2520comparisons.%2520This%2520distribution%2520directly%2520gives%2520the%2520evidence%2520a%2520dataset%2520would%250Aprovide%2520in%2520favor%2520of%2520the%2520model.%2520If%2520we%2520want%2520to%2520compare%2520multiple%2520models%2520to%2520each%250Aother%252C%2520we%2520can%2520use%2520a%2520metric%2520for%2520probability%2520distributions%2520like%2520the%250AJensen-Shannon%2520distance%2520or%2520the%2520total%2520variation%2520distance.%2520As%2520these%2520are%2520metrics%252C%250Athis%2520induces%2520pseudo-metrics%2520for%2520representations%252C%2520which%2520measure%2520how%2520well%2520two%250Arepresentations%2520could%2520be%2520distinguished%2520based%2520on%2520a%2520linear%2520read%2520out.%2520For%2520a%2520linear%250Areadout%2520with%2520a%2520Gaussian%2520prior%2520on%2520the%2520read-out%2520weights%2520and%2520Gaussian%2520noise%252C%2520we%250Acan%2520analytically%2520compute%2520the%2520%2528prior%2520and%2520posterior%2529%2520predictive%2520distributions%250Awithout%2520approximations.%2520These%2520distributions%2520depend%2520only%2520on%2520the%2520linear%2520kernel%250Amatrix%2520of%2520the%2520representations%2520in%2520the%2520model.%2520Thus%252C%2520the%2520Bayesian%2520metrics%2520connect%250Alinear%2520read-out%2520based%2520comparisons%2520to%2520kernel%2520based%2520metrics%2520like%2520centered%2520kernel%250Aalignment%2520and%2520representational%2520similarity%2520analysis.%2520I%2520demonstrate%2520the%2520new%250Amethods%2520with%2520deep%2520neural%2520networks%2520trained%2520on%2520ImageNet-1k%2520comparing%2520them%2520to%2520each%250Aother%2520and%2520a%2520small%2520subset%2520of%2520the%2520Natural%2520Scenes%2520Dataset.%2520The%2520Bayesian%250Acomparisons%2520broadly%2520agree%2520with%2520existing%2520metrics%252C%2520but%2520are%2520more%2520stringent.%250AEmpirically%252C%2520evaluations%2520vary%2520less%2520across%2520different%2520random%2520image%2520samples%2520and%250Ayield%2520informative%2520results%2520with%2520full%2520uncertainty%2520information.%2520Thus%2520the%2520proposed%250ABayesian%2520metrics%2520nicely%2520extend%2520our%2520toolkit%2520for%2520comparing%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08739v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Comparisons%20Between%20Representations&entry.906535625=Heiko%20H.%20Sch%C3%BCtt&entry.1292438233=%20%20Which%20neural%20networks%20are%20similar%20is%20a%20fundamental%20question%20for%20both%20machine%0Alearning%20and%20neuroscience.%20Here%2C%20I%20propose%20to%20base%20comparisons%20on%20the%0Apredictive%20distributions%20of%20linear%20readouts%20from%20intermediate%20representations.%0AIn%20Bayesian%20statistics%2C%20the%20prior%20predictive%20distribution%20is%20a%20full%20description%0Aof%20the%20inductive%20bias%20and%20generalization%20of%20a%20model%2C%20making%20it%20a%20great%20basis%0Afor%20comparisons.%20This%20distribution%20directly%20gives%20the%20evidence%20a%20dataset%20would%0Aprovide%20in%20favor%20of%20the%20model.%20If%20we%20want%20to%20compare%20multiple%20models%20to%20each%0Aother%2C%20we%20can%20use%20a%20metric%20for%20probability%20distributions%20like%20the%0AJensen-Shannon%20distance%20or%20the%20total%20variation%20distance.%20As%20these%20are%20metrics%2C%0Athis%20induces%20pseudo-metrics%20for%20representations%2C%20which%20measure%20how%20well%20two%0Arepresentations%20could%20be%20distinguished%20based%20on%20a%20linear%20read%20out.%20For%20a%20linear%0Areadout%20with%20a%20Gaussian%20prior%20on%20the%20read-out%20weights%20and%20Gaussian%20noise%2C%20we%0Acan%20analytically%20compute%20the%20%28prior%20and%20posterior%29%20predictive%20distributions%0Awithout%20approximations.%20These%20distributions%20depend%20only%20on%20the%20linear%20kernel%0Amatrix%20of%20the%20representations%20in%20the%20model.%20Thus%2C%20the%20Bayesian%20metrics%20connect%0Alinear%20read-out%20based%20comparisons%20to%20kernel%20based%20metrics%20like%20centered%20kernel%0Aalignment%20and%20representational%20similarity%20analysis.%20I%20demonstrate%20the%20new%0Amethods%20with%20deep%20neural%20networks%20trained%20on%20ImageNet-1k%20comparing%20them%20to%20each%0Aother%20and%20a%20small%20subset%20of%20the%20Natural%20Scenes%20Dataset.%20The%20Bayesian%0Acomparisons%20broadly%20agree%20with%20existing%20metrics%2C%20but%20are%20more%20stringent.%0AEmpirically%2C%20evaluations%20vary%20less%20across%20different%20random%20image%20samples%20and%0Ayield%20informative%20results%20with%20full%20uncertainty%20information.%20Thus%20the%20proposed%0ABayesian%20metrics%20nicely%20extend%20our%20toolkit%20for%20comparing%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08739v2&entry.124074799=Read"},
{"title": "Evaluating Large Language Models for Public Health Classification and\n  Extraction Tasks", "author": "Joshua Harris and Timothy Laurence and Leo Loman and Fan Grayson and Toby Nonnenmacher and Harry Long and Loes WalsGriffith and Amy Douglas and Holly Fountain and Stelios Georgiou and Jo Hardstaff and Kathryn Hopkins and Y-Ling Chi and Galena Kuyumdzhieva and Lesley Larkin and Samuel Collins and Hamish Mohammed and Thomas Finnie and Luke Hounsome and Michael Borowitz and Steven Riley", "abstract": "  Advances in Large Language Models (LLMs) have led to significant interest in\ntheir potential to support human experts across a range of domains, including\npublic health. In this work we present automated evaluations of LLMs for public\nhealth tasks involving the classification and extraction of free text. We\ncombine six externally annotated datasets with seven new internally annotated\ndatasets to evaluate LLMs for processing text related to: health burden,\nepidemiological risk factors, and public health interventions. We evaluate\neleven open-weight LLMs (7-123 billion parameters) across all tasks using\nzero-shot in-context learning. We find that Llama-3.3-70B-Instruct is the\nhighest performing model, achieving the best results on 8/16 tasks (using\nmicro-F1 scores). We see significant variation across tasks with all\nopen-weight LLMs scoring below 60% micro-F1 on some challenging tasks, such as\nContact Classification, while all LLMs achieve greater than 80% micro-F1 on\nothers, such as GI Illness Classification. For a subset of 11 tasks, we also\nevaluate three GPT-4 and GPT-4o series models and find comparable results to\nLlama-3.3-70B-Instruct. Overall, based on these initial results we find\npromising signs that LLMs may be useful tools for public health experts to\nextract information from a wide variety of free text sources, and support\npublic health surveillance, research, and interventions.\n", "link": "http://arxiv.org/abs/2405.14766v2", "date": "2025-02-19", "relevancy": 2.0057, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5157}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4986}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Large%20Language%20Models%20for%20Public%20Health%20Classification%20and%0A%20%20Extraction%20Tasks&body=Title%3A%20Evaluating%20Large%20Language%20Models%20for%20Public%20Health%20Classification%20and%0A%20%20Extraction%20Tasks%0AAuthor%3A%20Joshua%20Harris%20and%20Timothy%20Laurence%20and%20Leo%20Loman%20and%20Fan%20Grayson%20and%20Toby%20Nonnenmacher%20and%20Harry%20Long%20and%20Loes%20WalsGriffith%20and%20Amy%20Douglas%20and%20Holly%20Fountain%20and%20Stelios%20Georgiou%20and%20Jo%20Hardstaff%20and%20Kathryn%20Hopkins%20and%20Y-Ling%20Chi%20and%20Galena%20Kuyumdzhieva%20and%20Lesley%20Larkin%20and%20Samuel%20Collins%20and%20Hamish%20Mohammed%20and%20Thomas%20Finnie%20and%20Luke%20Hounsome%20and%20Michael%20Borowitz%20and%20Steven%20Riley%0AAbstract%3A%20%20%20Advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20led%20to%20significant%20interest%20in%0Atheir%20potential%20to%20support%20human%20experts%20across%20a%20range%20of%20domains%2C%20including%0Apublic%20health.%20In%20this%20work%20we%20present%20automated%20evaluations%20of%20LLMs%20for%20public%0Ahealth%20tasks%20involving%20the%20classification%20and%20extraction%20of%20free%20text.%20We%0Acombine%20six%20externally%20annotated%20datasets%20with%20seven%20new%20internally%20annotated%0Adatasets%20to%20evaluate%20LLMs%20for%20processing%20text%20related%20to%3A%20health%20burden%2C%0Aepidemiological%20risk%20factors%2C%20and%20public%20health%20interventions.%20We%20evaluate%0Aeleven%20open-weight%20LLMs%20%287-123%20billion%20parameters%29%20across%20all%20tasks%20using%0Azero-shot%20in-context%20learning.%20We%20find%20that%20Llama-3.3-70B-Instruct%20is%20the%0Ahighest%20performing%20model%2C%20achieving%20the%20best%20results%20on%208/16%20tasks%20%28using%0Amicro-F1%20scores%29.%20We%20see%20significant%20variation%20across%20tasks%20with%20all%0Aopen-weight%20LLMs%20scoring%20below%2060%25%20micro-F1%20on%20some%20challenging%20tasks%2C%20such%20as%0AContact%20Classification%2C%20while%20all%20LLMs%20achieve%20greater%20than%2080%25%20micro-F1%20on%0Aothers%2C%20such%20as%20GI%20Illness%20Classification.%20For%20a%20subset%20of%2011%20tasks%2C%20we%20also%0Aevaluate%20three%20GPT-4%20and%20GPT-4o%20series%20models%20and%20find%20comparable%20results%20to%0ALlama-3.3-70B-Instruct.%20Overall%2C%20based%20on%20these%20initial%20results%20we%20find%0Apromising%20signs%20that%20LLMs%20may%20be%20useful%20tools%20for%20public%20health%20experts%20to%0Aextract%20information%20from%20a%20wide%20variety%20of%20free%20text%20sources%2C%20and%20support%0Apublic%20health%20surveillance%2C%20research%2C%20and%20interventions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14766v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Large%2520Language%2520Models%2520for%2520Public%2520Health%2520Classification%2520and%250A%2520%2520Extraction%2520Tasks%26entry.906535625%3DJoshua%2520Harris%2520and%2520Timothy%2520Laurence%2520and%2520Leo%2520Loman%2520and%2520Fan%2520Grayson%2520and%2520Toby%2520Nonnenmacher%2520and%2520Harry%2520Long%2520and%2520Loes%2520WalsGriffith%2520and%2520Amy%2520Douglas%2520and%2520Holly%2520Fountain%2520and%2520Stelios%2520Georgiou%2520and%2520Jo%2520Hardstaff%2520and%2520Kathryn%2520Hopkins%2520and%2520Y-Ling%2520Chi%2520and%2520Galena%2520Kuyumdzhieva%2520and%2520Lesley%2520Larkin%2520and%2520Samuel%2520Collins%2520and%2520Hamish%2520Mohammed%2520and%2520Thomas%2520Finnie%2520and%2520Luke%2520Hounsome%2520and%2520Michael%2520Borowitz%2520and%2520Steven%2520Riley%26entry.1292438233%3D%2520%2520Advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520led%2520to%2520significant%2520interest%2520in%250Atheir%2520potential%2520to%2520support%2520human%2520experts%2520across%2520a%2520range%2520of%2520domains%252C%2520including%250Apublic%2520health.%2520In%2520this%2520work%2520we%2520present%2520automated%2520evaluations%2520of%2520LLMs%2520for%2520public%250Ahealth%2520tasks%2520involving%2520the%2520classification%2520and%2520extraction%2520of%2520free%2520text.%2520We%250Acombine%2520six%2520externally%2520annotated%2520datasets%2520with%2520seven%2520new%2520internally%2520annotated%250Adatasets%2520to%2520evaluate%2520LLMs%2520for%2520processing%2520text%2520related%2520to%253A%2520health%2520burden%252C%250Aepidemiological%2520risk%2520factors%252C%2520and%2520public%2520health%2520interventions.%2520We%2520evaluate%250Aeleven%2520open-weight%2520LLMs%2520%25287-123%2520billion%2520parameters%2529%2520across%2520all%2520tasks%2520using%250Azero-shot%2520in-context%2520learning.%2520We%2520find%2520that%2520Llama-3.3-70B-Instruct%2520is%2520the%250Ahighest%2520performing%2520model%252C%2520achieving%2520the%2520best%2520results%2520on%25208/16%2520tasks%2520%2528using%250Amicro-F1%2520scores%2529.%2520We%2520see%2520significant%2520variation%2520across%2520tasks%2520with%2520all%250Aopen-weight%2520LLMs%2520scoring%2520below%252060%2525%2520micro-F1%2520on%2520some%2520challenging%2520tasks%252C%2520such%2520as%250AContact%2520Classification%252C%2520while%2520all%2520LLMs%2520achieve%2520greater%2520than%252080%2525%2520micro-F1%2520on%250Aothers%252C%2520such%2520as%2520GI%2520Illness%2520Classification.%2520For%2520a%2520subset%2520of%252011%2520tasks%252C%2520we%2520also%250Aevaluate%2520three%2520GPT-4%2520and%2520GPT-4o%2520series%2520models%2520and%2520find%2520comparable%2520results%2520to%250ALlama-3.3-70B-Instruct.%2520Overall%252C%2520based%2520on%2520these%2520initial%2520results%2520we%2520find%250Apromising%2520signs%2520that%2520LLMs%2520may%2520be%2520useful%2520tools%2520for%2520public%2520health%2520experts%2520to%250Aextract%2520information%2520from%2520a%2520wide%2520variety%2520of%2520free%2520text%2520sources%252C%2520and%2520support%250Apublic%2520health%2520surveillance%252C%2520research%252C%2520and%2520interventions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14766v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Large%20Language%20Models%20for%20Public%20Health%20Classification%20and%0A%20%20Extraction%20Tasks&entry.906535625=Joshua%20Harris%20and%20Timothy%20Laurence%20and%20Leo%20Loman%20and%20Fan%20Grayson%20and%20Toby%20Nonnenmacher%20and%20Harry%20Long%20and%20Loes%20WalsGriffith%20and%20Amy%20Douglas%20and%20Holly%20Fountain%20and%20Stelios%20Georgiou%20and%20Jo%20Hardstaff%20and%20Kathryn%20Hopkins%20and%20Y-Ling%20Chi%20and%20Galena%20Kuyumdzhieva%20and%20Lesley%20Larkin%20and%20Samuel%20Collins%20and%20Hamish%20Mohammed%20and%20Thomas%20Finnie%20and%20Luke%20Hounsome%20and%20Michael%20Borowitz%20and%20Steven%20Riley&entry.1292438233=%20%20Advances%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20led%20to%20significant%20interest%20in%0Atheir%20potential%20to%20support%20human%20experts%20across%20a%20range%20of%20domains%2C%20including%0Apublic%20health.%20In%20this%20work%20we%20present%20automated%20evaluations%20of%20LLMs%20for%20public%0Ahealth%20tasks%20involving%20the%20classification%20and%20extraction%20of%20free%20text.%20We%0Acombine%20six%20externally%20annotated%20datasets%20with%20seven%20new%20internally%20annotated%0Adatasets%20to%20evaluate%20LLMs%20for%20processing%20text%20related%20to%3A%20health%20burden%2C%0Aepidemiological%20risk%20factors%2C%20and%20public%20health%20interventions.%20We%20evaluate%0Aeleven%20open-weight%20LLMs%20%287-123%20billion%20parameters%29%20across%20all%20tasks%20using%0Azero-shot%20in-context%20learning.%20We%20find%20that%20Llama-3.3-70B-Instruct%20is%20the%0Ahighest%20performing%20model%2C%20achieving%20the%20best%20results%20on%208/16%20tasks%20%28using%0Amicro-F1%20scores%29.%20We%20see%20significant%20variation%20across%20tasks%20with%20all%0Aopen-weight%20LLMs%20scoring%20below%2060%25%20micro-F1%20on%20some%20challenging%20tasks%2C%20such%20as%0AContact%20Classification%2C%20while%20all%20LLMs%20achieve%20greater%20than%2080%25%20micro-F1%20on%0Aothers%2C%20such%20as%20GI%20Illness%20Classification.%20For%20a%20subset%20of%2011%20tasks%2C%20we%20also%0Aevaluate%20three%20GPT-4%20and%20GPT-4o%20series%20models%20and%20find%20comparable%20results%20to%0ALlama-3.3-70B-Instruct.%20Overall%2C%20based%20on%20these%20initial%20results%20we%20find%0Apromising%20signs%20that%20LLMs%20may%20be%20useful%20tools%20for%20public%20health%20experts%20to%0Aextract%20information%20from%20a%20wide%20variety%20of%20free%20text%20sources%2C%20and%20support%0Apublic%20health%20surveillance%2C%20research%2C%20and%20interventions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14766v2&entry.124074799=Read"},
{"title": "MMTEB: Massive Multilingual Text Embedding Benchmark", "author": "Kenneth Enevoldsen and Isaac Chung and Imene Kerboua and M\u00e1rton Kardos and Ashwin Mathur and David Stap and Jay Gala and Wissam Siblini and Dominik Krzemi\u0144ski and Genta Indra Winata and Saba Sturua and Saiteja Utpala and Mathieu Ciancone and Marion Schaeffer and Gabriel Sequeira and Diganta Misra and Shreeya Dhakal and Jonathan Rystr\u00f8m and Roman Solomatin and \u00d6mer \u00c7a\u011fatan and Akash Kundu and Martin Bernstorff and Shitao Xiao and Akshita Sukhlecha and Bhavish Pahwa and Rafa\u0142 Po\u015bwiata and Kranthi Kiran GV and Shawon Ashraf and Daniel Auras and Bj\u00f6rn Pl\u00fcster and Jan Philipp Harries and Lo\u00efc Magne and Isabelle Mohr and Mariya Hendriksen and Dawei Zhu and Hippolyte Gisserot-Boukhlef and Tom Aarsen and Jan Kostkan and Konrad Wojtasik and Taemin Lee and Marek \u0160uppa and Crystina Zhang and Roberta Rocca and Mohammed Hamdy and Andrianos Michail and John Yang and Manuel Faysse and Aleksei Vatolin and Nandan Thakur and Manan Dey and Dipam Vasani and Pranjal Chitale and Simone Tedeschi and Nguyen Tai and Artem Snegirev and Michael G\u00fcnther and Mengzhou Xia and Weijia Shi and Xing Han L\u00f9 and Jordan Clive and Gayatri Krishnakumar and Anna Maksimova and Silvan Wehrli and Maria Tikhonova and Henil Panchal and Aleksandr Abramov and Malte Ostendorff and Zheng Liu and Simon Clematide and Lester James Miranda and Alena Fenogenova and Guangyu Song and Ruqiya Bin Safi and Wen-Ding Li and Alessia Borghini and Federico Cassano and Hongjin Su and Jimmy Lin and Howard Yen and Lasse Hansen and Sara Hooker and Chenghao Xiao and Vaibhav Adlakha and Orion Weller and Siva Reddy and Niklas Muennighoff", "abstract": "  Text embeddings are typically evaluated on a limited set of tasks, which are\nconstrained by language, domain, and task diversity. To address these\nlimitations and provide a more comprehensive evaluation, we introduce the\nMassive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale,\ncommunity-driven expansion of MTEB, covering over 500 quality-controlled\nevaluation tasks across 250+ languages. MMTEB includes a diverse set of\nchallenging, novel tasks such as instruction following, long-document\nretrieval, and code retrieval, representing the largest multilingual collection\nof evaluation tasks for embedding models to date. Using this collection, we\ndevelop several highly multilingual benchmarks, which we use to evaluate a\nrepresentative set of models. We find that while large language models (LLMs)\nwith billions of parameters can achieve state-of-the-art performance on certain\nlanguage subsets and task categories, the best-performing publicly available\nmodel is multilingual-e5-large-instruct with only 560 million parameters. To\nfacilitate accessibility and reduce computational cost, we introduce a novel\ndownsampling method based on inter-task correlation, ensuring a diverse\nselection while preserving relative model rankings. Furthermore, we optimize\ntasks such as retrieval by sampling hard negatives, creating smaller but\neffective splits. These optimizations allow us to introduce benchmarks that\ndrastically reduce computational demands. For instance, our newly introduced\nzero-shot English benchmark maintains a ranking order similar to the full-scale\nversion but at a fraction of the computational cost.\n", "link": "http://arxiv.org/abs/2502.13595v1", "date": "2025-02-19", "relevancy": 1.9991, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5025}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4992}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMTEB%3A%20Massive%20Multilingual%20Text%20Embedding%20Benchmark&body=Title%3A%20MMTEB%3A%20Massive%20Multilingual%20Text%20Embedding%20Benchmark%0AAuthor%3A%20Kenneth%20Enevoldsen%20and%20Isaac%20Chung%20and%20Imene%20Kerboua%20and%20M%C3%A1rton%20Kardos%20and%20Ashwin%20Mathur%20and%20David%20Stap%20and%20Jay%20Gala%20and%20Wissam%20Siblini%20and%20Dominik%20Krzemi%C5%84ski%20and%20Genta%20Indra%20Winata%20and%20Saba%20Sturua%20and%20Saiteja%20Utpala%20and%20Mathieu%20Ciancone%20and%20Marion%20Schaeffer%20and%20Gabriel%20Sequeira%20and%20Diganta%20Misra%20and%20Shreeya%20Dhakal%20and%20Jonathan%20Rystr%C3%B8m%20and%20Roman%20Solomatin%20and%20%C3%96mer%20%C3%87a%C4%9Fatan%20and%20Akash%20Kundu%20and%20Martin%20Bernstorff%20and%20Shitao%20Xiao%20and%20Akshita%20Sukhlecha%20and%20Bhavish%20Pahwa%20and%20Rafa%C5%82%20Po%C5%9Bwiata%20and%20Kranthi%20Kiran%20GV%20and%20Shawon%20Ashraf%20and%20Daniel%20Auras%20and%20Bj%C3%B6rn%20Pl%C3%BCster%20and%20Jan%20Philipp%20Harries%20and%20Lo%C3%AFc%20Magne%20and%20Isabelle%20Mohr%20and%20Mariya%20Hendriksen%20and%20Dawei%20Zhu%20and%20Hippolyte%20Gisserot-Boukhlef%20and%20Tom%20Aarsen%20and%20Jan%20Kostkan%20and%20Konrad%20Wojtasik%20and%20Taemin%20Lee%20and%20Marek%20%C5%A0uppa%20and%20Crystina%20Zhang%20and%20Roberta%20Rocca%20and%20Mohammed%20Hamdy%20and%20Andrianos%20Michail%20and%20John%20Yang%20and%20Manuel%20Faysse%20and%20Aleksei%20Vatolin%20and%20Nandan%20Thakur%20and%20Manan%20Dey%20and%20Dipam%20Vasani%20and%20Pranjal%20Chitale%20and%20Simone%20Tedeschi%20and%20Nguyen%20Tai%20and%20Artem%20Snegirev%20and%20Michael%20G%C3%BCnther%20and%20Mengzhou%20Xia%20and%20Weijia%20Shi%20and%20Xing%20Han%20L%C3%B9%20and%20Jordan%20Clive%20and%20Gayatri%20Krishnakumar%20and%20Anna%20Maksimova%20and%20Silvan%20Wehrli%20and%20Maria%20Tikhonova%20and%20Henil%20Panchal%20and%20Aleksandr%20Abramov%20and%20Malte%20Ostendorff%20and%20Zheng%20Liu%20and%20Simon%20Clematide%20and%20Lester%20James%20Miranda%20and%20Alena%20Fenogenova%20and%20Guangyu%20Song%20and%20Ruqiya%20Bin%20Safi%20and%20Wen-Ding%20Li%20and%20Alessia%20Borghini%20and%20Federico%20Cassano%20and%20Hongjin%20Su%20and%20Jimmy%20Lin%20and%20Howard%20Yen%20and%20Lasse%20Hansen%20and%20Sara%20Hooker%20and%20Chenghao%20Xiao%20and%20Vaibhav%20Adlakha%20and%20Orion%20Weller%20and%20Siva%20Reddy%20and%20Niklas%20Muennighoff%0AAbstract%3A%20%20%20Text%20embeddings%20are%20typically%20evaluated%20on%20a%20limited%20set%20of%20tasks%2C%20which%20are%0Aconstrained%20by%20language%2C%20domain%2C%20and%20task%20diversity.%20To%20address%20these%0Alimitations%20and%20provide%20a%20more%20comprehensive%20evaluation%2C%20we%20introduce%20the%0AMassive%20Multilingual%20Text%20Embedding%20Benchmark%20%28MMTEB%29%20-%20a%20large-scale%2C%0Acommunity-driven%20expansion%20of%20MTEB%2C%20covering%20over%20500%20quality-controlled%0Aevaluation%20tasks%20across%20250%2B%20languages.%20MMTEB%20includes%20a%20diverse%20set%20of%0Achallenging%2C%20novel%20tasks%20such%20as%20instruction%20following%2C%20long-document%0Aretrieval%2C%20and%20code%20retrieval%2C%20representing%20the%20largest%20multilingual%20collection%0Aof%20evaluation%20tasks%20for%20embedding%20models%20to%20date.%20Using%20this%20collection%2C%20we%0Adevelop%20several%20highly%20multilingual%20benchmarks%2C%20which%20we%20use%20to%20evaluate%20a%0Arepresentative%20set%20of%20models.%20We%20find%20that%20while%20large%20language%20models%20%28LLMs%29%0Awith%20billions%20of%20parameters%20can%20achieve%20state-of-the-art%20performance%20on%20certain%0Alanguage%20subsets%20and%20task%20categories%2C%20the%20best-performing%20publicly%20available%0Amodel%20is%20multilingual-e5-large-instruct%20with%20only%20560%20million%20parameters.%20To%0Afacilitate%20accessibility%20and%20reduce%20computational%20cost%2C%20we%20introduce%20a%20novel%0Adownsampling%20method%20based%20on%20inter-task%20correlation%2C%20ensuring%20a%20diverse%0Aselection%20while%20preserving%20relative%20model%20rankings.%20Furthermore%2C%20we%20optimize%0Atasks%20such%20as%20retrieval%20by%20sampling%20hard%20negatives%2C%20creating%20smaller%20but%0Aeffective%20splits.%20These%20optimizations%20allow%20us%20to%20introduce%20benchmarks%20that%0Adrastically%20reduce%20computational%20demands.%20For%20instance%2C%20our%20newly%20introduced%0Azero-shot%20English%20benchmark%20maintains%20a%20ranking%20order%20similar%20to%20the%20full-scale%0Aversion%20but%20at%20a%20fraction%20of%20the%20computational%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMTEB%253A%2520Massive%2520Multilingual%2520Text%2520Embedding%2520Benchmark%26entry.906535625%3DKenneth%2520Enevoldsen%2520and%2520Isaac%2520Chung%2520and%2520Imene%2520Kerboua%2520and%2520M%25C3%25A1rton%2520Kardos%2520and%2520Ashwin%2520Mathur%2520and%2520David%2520Stap%2520and%2520Jay%2520Gala%2520and%2520Wissam%2520Siblini%2520and%2520Dominik%2520Krzemi%25C5%2584ski%2520and%2520Genta%2520Indra%2520Winata%2520and%2520Saba%2520Sturua%2520and%2520Saiteja%2520Utpala%2520and%2520Mathieu%2520Ciancone%2520and%2520Marion%2520Schaeffer%2520and%2520Gabriel%2520Sequeira%2520and%2520Diganta%2520Misra%2520and%2520Shreeya%2520Dhakal%2520and%2520Jonathan%2520Rystr%25C3%25B8m%2520and%2520Roman%2520Solomatin%2520and%2520%25C3%2596mer%2520%25C3%2587a%25C4%259Fatan%2520and%2520Akash%2520Kundu%2520and%2520Martin%2520Bernstorff%2520and%2520Shitao%2520Xiao%2520and%2520Akshita%2520Sukhlecha%2520and%2520Bhavish%2520Pahwa%2520and%2520Rafa%25C5%2582%2520Po%25C5%259Bwiata%2520and%2520Kranthi%2520Kiran%2520GV%2520and%2520Shawon%2520Ashraf%2520and%2520Daniel%2520Auras%2520and%2520Bj%25C3%25B6rn%2520Pl%25C3%25BCster%2520and%2520Jan%2520Philipp%2520Harries%2520and%2520Lo%25C3%25AFc%2520Magne%2520and%2520Isabelle%2520Mohr%2520and%2520Mariya%2520Hendriksen%2520and%2520Dawei%2520Zhu%2520and%2520Hippolyte%2520Gisserot-Boukhlef%2520and%2520Tom%2520Aarsen%2520and%2520Jan%2520Kostkan%2520and%2520Konrad%2520Wojtasik%2520and%2520Taemin%2520Lee%2520and%2520Marek%2520%25C5%25A0uppa%2520and%2520Crystina%2520Zhang%2520and%2520Roberta%2520Rocca%2520and%2520Mohammed%2520Hamdy%2520and%2520Andrianos%2520Michail%2520and%2520John%2520Yang%2520and%2520Manuel%2520Faysse%2520and%2520Aleksei%2520Vatolin%2520and%2520Nandan%2520Thakur%2520and%2520Manan%2520Dey%2520and%2520Dipam%2520Vasani%2520and%2520Pranjal%2520Chitale%2520and%2520Simone%2520Tedeschi%2520and%2520Nguyen%2520Tai%2520and%2520Artem%2520Snegirev%2520and%2520Michael%2520G%25C3%25BCnther%2520and%2520Mengzhou%2520Xia%2520and%2520Weijia%2520Shi%2520and%2520Xing%2520Han%2520L%25C3%25B9%2520and%2520Jordan%2520Clive%2520and%2520Gayatri%2520Krishnakumar%2520and%2520Anna%2520Maksimova%2520and%2520Silvan%2520Wehrli%2520and%2520Maria%2520Tikhonova%2520and%2520Henil%2520Panchal%2520and%2520Aleksandr%2520Abramov%2520and%2520Malte%2520Ostendorff%2520and%2520Zheng%2520Liu%2520and%2520Simon%2520Clematide%2520and%2520Lester%2520James%2520Miranda%2520and%2520Alena%2520Fenogenova%2520and%2520Guangyu%2520Song%2520and%2520Ruqiya%2520Bin%2520Safi%2520and%2520Wen-Ding%2520Li%2520and%2520Alessia%2520Borghini%2520and%2520Federico%2520Cassano%2520and%2520Hongjin%2520Su%2520and%2520Jimmy%2520Lin%2520and%2520Howard%2520Yen%2520and%2520Lasse%2520Hansen%2520and%2520Sara%2520Hooker%2520and%2520Chenghao%2520Xiao%2520and%2520Vaibhav%2520Adlakha%2520and%2520Orion%2520Weller%2520and%2520Siva%2520Reddy%2520and%2520Niklas%2520Muennighoff%26entry.1292438233%3D%2520%2520Text%2520embeddings%2520are%2520typically%2520evaluated%2520on%2520a%2520limited%2520set%2520of%2520tasks%252C%2520which%2520are%250Aconstrained%2520by%2520language%252C%2520domain%252C%2520and%2520task%2520diversity.%2520To%2520address%2520these%250Alimitations%2520and%2520provide%2520a%2520more%2520comprehensive%2520evaluation%252C%2520we%2520introduce%2520the%250AMassive%2520Multilingual%2520Text%2520Embedding%2520Benchmark%2520%2528MMTEB%2529%2520-%2520a%2520large-scale%252C%250Acommunity-driven%2520expansion%2520of%2520MTEB%252C%2520covering%2520over%2520500%2520quality-controlled%250Aevaluation%2520tasks%2520across%2520250%252B%2520languages.%2520MMTEB%2520includes%2520a%2520diverse%2520set%2520of%250Achallenging%252C%2520novel%2520tasks%2520such%2520as%2520instruction%2520following%252C%2520long-document%250Aretrieval%252C%2520and%2520code%2520retrieval%252C%2520representing%2520the%2520largest%2520multilingual%2520collection%250Aof%2520evaluation%2520tasks%2520for%2520embedding%2520models%2520to%2520date.%2520Using%2520this%2520collection%252C%2520we%250Adevelop%2520several%2520highly%2520multilingual%2520benchmarks%252C%2520which%2520we%2520use%2520to%2520evaluate%2520a%250Arepresentative%2520set%2520of%2520models.%2520We%2520find%2520that%2520while%2520large%2520language%2520models%2520%2528LLMs%2529%250Awith%2520billions%2520of%2520parameters%2520can%2520achieve%2520state-of-the-art%2520performance%2520on%2520certain%250Alanguage%2520subsets%2520and%2520task%2520categories%252C%2520the%2520best-performing%2520publicly%2520available%250Amodel%2520is%2520multilingual-e5-large-instruct%2520with%2520only%2520560%2520million%2520parameters.%2520To%250Afacilitate%2520accessibility%2520and%2520reduce%2520computational%2520cost%252C%2520we%2520introduce%2520a%2520novel%250Adownsampling%2520method%2520based%2520on%2520inter-task%2520correlation%252C%2520ensuring%2520a%2520diverse%250Aselection%2520while%2520preserving%2520relative%2520model%2520rankings.%2520Furthermore%252C%2520we%2520optimize%250Atasks%2520such%2520as%2520retrieval%2520by%2520sampling%2520hard%2520negatives%252C%2520creating%2520smaller%2520but%250Aeffective%2520splits.%2520These%2520optimizations%2520allow%2520us%2520to%2520introduce%2520benchmarks%2520that%250Adrastically%2520reduce%2520computational%2520demands.%2520For%2520instance%252C%2520our%2520newly%2520introduced%250Azero-shot%2520English%2520benchmark%2520maintains%2520a%2520ranking%2520order%2520similar%2520to%2520the%2520full-scale%250Aversion%2520but%2520at%2520a%2520fraction%2520of%2520the%2520computational%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMTEB%3A%20Massive%20Multilingual%20Text%20Embedding%20Benchmark&entry.906535625=Kenneth%20Enevoldsen%20and%20Isaac%20Chung%20and%20Imene%20Kerboua%20and%20M%C3%A1rton%20Kardos%20and%20Ashwin%20Mathur%20and%20David%20Stap%20and%20Jay%20Gala%20and%20Wissam%20Siblini%20and%20Dominik%20Krzemi%C5%84ski%20and%20Genta%20Indra%20Winata%20and%20Saba%20Sturua%20and%20Saiteja%20Utpala%20and%20Mathieu%20Ciancone%20and%20Marion%20Schaeffer%20and%20Gabriel%20Sequeira%20and%20Diganta%20Misra%20and%20Shreeya%20Dhakal%20and%20Jonathan%20Rystr%C3%B8m%20and%20Roman%20Solomatin%20and%20%C3%96mer%20%C3%87a%C4%9Fatan%20and%20Akash%20Kundu%20and%20Martin%20Bernstorff%20and%20Shitao%20Xiao%20and%20Akshita%20Sukhlecha%20and%20Bhavish%20Pahwa%20and%20Rafa%C5%82%20Po%C5%9Bwiata%20and%20Kranthi%20Kiran%20GV%20and%20Shawon%20Ashraf%20and%20Daniel%20Auras%20and%20Bj%C3%B6rn%20Pl%C3%BCster%20and%20Jan%20Philipp%20Harries%20and%20Lo%C3%AFc%20Magne%20and%20Isabelle%20Mohr%20and%20Mariya%20Hendriksen%20and%20Dawei%20Zhu%20and%20Hippolyte%20Gisserot-Boukhlef%20and%20Tom%20Aarsen%20and%20Jan%20Kostkan%20and%20Konrad%20Wojtasik%20and%20Taemin%20Lee%20and%20Marek%20%C5%A0uppa%20and%20Crystina%20Zhang%20and%20Roberta%20Rocca%20and%20Mohammed%20Hamdy%20and%20Andrianos%20Michail%20and%20John%20Yang%20and%20Manuel%20Faysse%20and%20Aleksei%20Vatolin%20and%20Nandan%20Thakur%20and%20Manan%20Dey%20and%20Dipam%20Vasani%20and%20Pranjal%20Chitale%20and%20Simone%20Tedeschi%20and%20Nguyen%20Tai%20and%20Artem%20Snegirev%20and%20Michael%20G%C3%BCnther%20and%20Mengzhou%20Xia%20and%20Weijia%20Shi%20and%20Xing%20Han%20L%C3%B9%20and%20Jordan%20Clive%20and%20Gayatri%20Krishnakumar%20and%20Anna%20Maksimova%20and%20Silvan%20Wehrli%20and%20Maria%20Tikhonova%20and%20Henil%20Panchal%20and%20Aleksandr%20Abramov%20and%20Malte%20Ostendorff%20and%20Zheng%20Liu%20and%20Simon%20Clematide%20and%20Lester%20James%20Miranda%20and%20Alena%20Fenogenova%20and%20Guangyu%20Song%20and%20Ruqiya%20Bin%20Safi%20and%20Wen-Ding%20Li%20and%20Alessia%20Borghini%20and%20Federico%20Cassano%20and%20Hongjin%20Su%20and%20Jimmy%20Lin%20and%20Howard%20Yen%20and%20Lasse%20Hansen%20and%20Sara%20Hooker%20and%20Chenghao%20Xiao%20and%20Vaibhav%20Adlakha%20and%20Orion%20Weller%20and%20Siva%20Reddy%20and%20Niklas%20Muennighoff&entry.1292438233=%20%20Text%20embeddings%20are%20typically%20evaluated%20on%20a%20limited%20set%20of%20tasks%2C%20which%20are%0Aconstrained%20by%20language%2C%20domain%2C%20and%20task%20diversity.%20To%20address%20these%0Alimitations%20and%20provide%20a%20more%20comprehensive%20evaluation%2C%20we%20introduce%20the%0AMassive%20Multilingual%20Text%20Embedding%20Benchmark%20%28MMTEB%29%20-%20a%20large-scale%2C%0Acommunity-driven%20expansion%20of%20MTEB%2C%20covering%20over%20500%20quality-controlled%0Aevaluation%20tasks%20across%20250%2B%20languages.%20MMTEB%20includes%20a%20diverse%20set%20of%0Achallenging%2C%20novel%20tasks%20such%20as%20instruction%20following%2C%20long-document%0Aretrieval%2C%20and%20code%20retrieval%2C%20representing%20the%20largest%20multilingual%20collection%0Aof%20evaluation%20tasks%20for%20embedding%20models%20to%20date.%20Using%20this%20collection%2C%20we%0Adevelop%20several%20highly%20multilingual%20benchmarks%2C%20which%20we%20use%20to%20evaluate%20a%0Arepresentative%20set%20of%20models.%20We%20find%20that%20while%20large%20language%20models%20%28LLMs%29%0Awith%20billions%20of%20parameters%20can%20achieve%20state-of-the-art%20performance%20on%20certain%0Alanguage%20subsets%20and%20task%20categories%2C%20the%20best-performing%20publicly%20available%0Amodel%20is%20multilingual-e5-large-instruct%20with%20only%20560%20million%20parameters.%20To%0Afacilitate%20accessibility%20and%20reduce%20computational%20cost%2C%20we%20introduce%20a%20novel%0Adownsampling%20method%20based%20on%20inter-task%20correlation%2C%20ensuring%20a%20diverse%0Aselection%20while%20preserving%20relative%20model%20rankings.%20Furthermore%2C%20we%20optimize%0Atasks%20such%20as%20retrieval%20by%20sampling%20hard%20negatives%2C%20creating%20smaller%20but%0Aeffective%20splits.%20These%20optimizations%20allow%20us%20to%20introduce%20benchmarks%20that%0Adrastically%20reduce%20computational%20demands.%20For%20instance%2C%20our%20newly%20introduced%0Azero-shot%20English%20benchmark%20maintains%20a%20ranking%20order%20similar%20to%20the%20full-scale%0Aversion%20but%20at%20a%20fraction%20of%20the%20computational%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13595v1&entry.124074799=Read"},
{"title": "Geometric Principles for Machine Learning of Dynamical Systems", "author": "Zack Xuereb Conti and David J Wagg and Nick Pepper", "abstract": "  Mathematical descriptions of dynamical systems are deeply rooted in\ntopological spaces defined by non-Euclidean geometry. This paper proposes\nleveraging structure-rich geometric spaces for machine learning to achieve\nstructural generalization when modeling physical systems from data, in contrast\nto embedding physics bias within model-free architectures. We consider model\ngeneralization to be a function of symmetry, invariance and uniqueness, defined\nas a topological mapping from state space dynamics to the parameter space. We\nillustrate this view through the machine learning of linear time-invariant\ndynamical systems, whose dynamics reside on the symmetric positive definite\nmanifold.\n", "link": "http://arxiv.org/abs/2502.13895v1", "date": "2025-02-19", "relevancy": 1.9928, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5113}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4926}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Principles%20for%20Machine%20Learning%20of%20Dynamical%20Systems&body=Title%3A%20Geometric%20Principles%20for%20Machine%20Learning%20of%20Dynamical%20Systems%0AAuthor%3A%20Zack%20Xuereb%20Conti%20and%20David%20J%20Wagg%20and%20Nick%20Pepper%0AAbstract%3A%20%20%20Mathematical%20descriptions%20of%20dynamical%20systems%20are%20deeply%20rooted%20in%0Atopological%20spaces%20defined%20by%20non-Euclidean%20geometry.%20This%20paper%20proposes%0Aleveraging%20structure-rich%20geometric%20spaces%20for%20machine%20learning%20to%20achieve%0Astructural%20generalization%20when%20modeling%20physical%20systems%20from%20data%2C%20in%20contrast%0Ato%20embedding%20physics%20bias%20within%20model-free%20architectures.%20We%20consider%20model%0Ageneralization%20to%20be%20a%20function%20of%20symmetry%2C%20invariance%20and%20uniqueness%2C%20defined%0Aas%20a%20topological%20mapping%20from%20state%20space%20dynamics%20to%20the%20parameter%20space.%20We%0Aillustrate%20this%20view%20through%20the%20machine%20learning%20of%20linear%20time-invariant%0Adynamical%20systems%2C%20whose%20dynamics%20reside%20on%20the%20symmetric%20positive%20definite%0Amanifold.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Principles%2520for%2520Machine%2520Learning%2520of%2520Dynamical%2520Systems%26entry.906535625%3DZack%2520Xuereb%2520Conti%2520and%2520David%2520J%2520Wagg%2520and%2520Nick%2520Pepper%26entry.1292438233%3D%2520%2520Mathematical%2520descriptions%2520of%2520dynamical%2520systems%2520are%2520deeply%2520rooted%2520in%250Atopological%2520spaces%2520defined%2520by%2520non-Euclidean%2520geometry.%2520This%2520paper%2520proposes%250Aleveraging%2520structure-rich%2520geometric%2520spaces%2520for%2520machine%2520learning%2520to%2520achieve%250Astructural%2520generalization%2520when%2520modeling%2520physical%2520systems%2520from%2520data%252C%2520in%2520contrast%250Ato%2520embedding%2520physics%2520bias%2520within%2520model-free%2520architectures.%2520We%2520consider%2520model%250Ageneralization%2520to%2520be%2520a%2520function%2520of%2520symmetry%252C%2520invariance%2520and%2520uniqueness%252C%2520defined%250Aas%2520a%2520topological%2520mapping%2520from%2520state%2520space%2520dynamics%2520to%2520the%2520parameter%2520space.%2520We%250Aillustrate%2520this%2520view%2520through%2520the%2520machine%2520learning%2520of%2520linear%2520time-invariant%250Adynamical%2520systems%252C%2520whose%2520dynamics%2520reside%2520on%2520the%2520symmetric%2520positive%2520definite%250Amanifold.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Principles%20for%20Machine%20Learning%20of%20Dynamical%20Systems&entry.906535625=Zack%20Xuereb%20Conti%20and%20David%20J%20Wagg%20and%20Nick%20Pepper&entry.1292438233=%20%20Mathematical%20descriptions%20of%20dynamical%20systems%20are%20deeply%20rooted%20in%0Atopological%20spaces%20defined%20by%20non-Euclidean%20geometry.%20This%20paper%20proposes%0Aleveraging%20structure-rich%20geometric%20spaces%20for%20machine%20learning%20to%20achieve%0Astructural%20generalization%20when%20modeling%20physical%20systems%20from%20data%2C%20in%20contrast%0Ato%20embedding%20physics%20bias%20within%20model-free%20architectures.%20We%20consider%20model%0Ageneralization%20to%20be%20a%20function%20of%20symmetry%2C%20invariance%20and%20uniqueness%2C%20defined%0Aas%20a%20topological%20mapping%20from%20state%20space%20dynamics%20to%20the%20parameter%20space.%20We%0Aillustrate%20this%20view%20through%20the%20machine%20learning%20of%20linear%20time-invariant%0Adynamical%20systems%2C%20whose%20dynamics%20reside%20on%20the%20symmetric%20positive%20definite%0Amanifold.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13895v1&entry.124074799=Read"},
{"title": "Minimally sufficient structures for information-feedback policies", "author": "Basak Sakcak and Vadim K. Weinstein and Kalle G. Timperi and Steven M. LaValle", "abstract": "  In this paper, we consider robotic tasks which require a desirable outcome to\nbe achieved in the physical world that the robot is embedded in and interacting\nwith. Accomplishing this objective requires designing a filter that maintains a\nuseful representation of the physical world and a policy over the filter\nstates. A filter is seen as the robot's perspective of the physical world based\non limited sensing, memory, and computation and it is represented as a\ntransition system over a space of information states. To this end, the\ninteractions result from the coupling of an internal and an external system, a\nfilter, and the physical world, respectively, through a sensor mapping and an\ninformation-feedback policy. Within this setup, we look for sufficient\nstructures, that is, sufficient internal systems and sensors, for accomplishing\na given task. We establish necessary and sufficient conditions for these\nstructures to satisfy for information-feedback policies that can be defined\nover the states of an internal system to exist. We also show that under mild\nassumptions, minimal internal systems that can represent a particular\nplan/policy described over the action-observation histories exist and are\nunique. Finally, the results are applied to determine sufficient structures for\ndistance-optimal navigation in a polygonal environment.\n", "link": "http://arxiv.org/abs/2502.13852v1", "date": "2025-02-19", "relevancy": 1.4418, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5049}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4944}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Minimally%20sufficient%20structures%20for%20information-feedback%20policies&body=Title%3A%20Minimally%20sufficient%20structures%20for%20information-feedback%20policies%0AAuthor%3A%20Basak%20Sakcak%20and%20Vadim%20K.%20Weinstein%20and%20Kalle%20G.%20Timperi%20and%20Steven%20M.%20LaValle%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20consider%20robotic%20tasks%20which%20require%20a%20desirable%20outcome%20to%0Abe%20achieved%20in%20the%20physical%20world%20that%20the%20robot%20is%20embedded%20in%20and%20interacting%0Awith.%20Accomplishing%20this%20objective%20requires%20designing%20a%20filter%20that%20maintains%20a%0Auseful%20representation%20of%20the%20physical%20world%20and%20a%20policy%20over%20the%20filter%0Astates.%20A%20filter%20is%20seen%20as%20the%20robot%27s%20perspective%20of%20the%20physical%20world%20based%0Aon%20limited%20sensing%2C%20memory%2C%20and%20computation%20and%20it%20is%20represented%20as%20a%0Atransition%20system%20over%20a%20space%20of%20information%20states.%20To%20this%20end%2C%20the%0Ainteractions%20result%20from%20the%20coupling%20of%20an%20internal%20and%20an%20external%20system%2C%20a%0Afilter%2C%20and%20the%20physical%20world%2C%20respectively%2C%20through%20a%20sensor%20mapping%20and%20an%0Ainformation-feedback%20policy.%20Within%20this%20setup%2C%20we%20look%20for%20sufficient%0Astructures%2C%20that%20is%2C%20sufficient%20internal%20systems%20and%20sensors%2C%20for%20accomplishing%0Aa%20given%20task.%20We%20establish%20necessary%20and%20sufficient%20conditions%20for%20these%0Astructures%20to%20satisfy%20for%20information-feedback%20policies%20that%20can%20be%20defined%0Aover%20the%20states%20of%20an%20internal%20system%20to%20exist.%20We%20also%20show%20that%20under%20mild%0Aassumptions%2C%20minimal%20internal%20systems%20that%20can%20represent%20a%20particular%0Aplan/policy%20described%20over%20the%20action-observation%20histories%20exist%20and%20are%0Aunique.%20Finally%2C%20the%20results%20are%20applied%20to%20determine%20sufficient%20structures%20for%0Adistance-optimal%20navigation%20in%20a%20polygonal%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinimally%2520sufficient%2520structures%2520for%2520information-feedback%2520policies%26entry.906535625%3DBasak%2520Sakcak%2520and%2520Vadim%2520K.%2520Weinstein%2520and%2520Kalle%2520G.%2520Timperi%2520and%2520Steven%2520M.%2520LaValle%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520consider%2520robotic%2520tasks%2520which%2520require%2520a%2520desirable%2520outcome%2520to%250Abe%2520achieved%2520in%2520the%2520physical%2520world%2520that%2520the%2520robot%2520is%2520embedded%2520in%2520and%2520interacting%250Awith.%2520Accomplishing%2520this%2520objective%2520requires%2520designing%2520a%2520filter%2520that%2520maintains%2520a%250Auseful%2520representation%2520of%2520the%2520physical%2520world%2520and%2520a%2520policy%2520over%2520the%2520filter%250Astates.%2520A%2520filter%2520is%2520seen%2520as%2520the%2520robot%2527s%2520perspective%2520of%2520the%2520physical%2520world%2520based%250Aon%2520limited%2520sensing%252C%2520memory%252C%2520and%2520computation%2520and%2520it%2520is%2520represented%2520as%2520a%250Atransition%2520system%2520over%2520a%2520space%2520of%2520information%2520states.%2520To%2520this%2520end%252C%2520the%250Ainteractions%2520result%2520from%2520the%2520coupling%2520of%2520an%2520internal%2520and%2520an%2520external%2520system%252C%2520a%250Afilter%252C%2520and%2520the%2520physical%2520world%252C%2520respectively%252C%2520through%2520a%2520sensor%2520mapping%2520and%2520an%250Ainformation-feedback%2520policy.%2520Within%2520this%2520setup%252C%2520we%2520look%2520for%2520sufficient%250Astructures%252C%2520that%2520is%252C%2520sufficient%2520internal%2520systems%2520and%2520sensors%252C%2520for%2520accomplishing%250Aa%2520given%2520task.%2520We%2520establish%2520necessary%2520and%2520sufficient%2520conditions%2520for%2520these%250Astructures%2520to%2520satisfy%2520for%2520information-feedback%2520policies%2520that%2520can%2520be%2520defined%250Aover%2520the%2520states%2520of%2520an%2520internal%2520system%2520to%2520exist.%2520We%2520also%2520show%2520that%2520under%2520mild%250Aassumptions%252C%2520minimal%2520internal%2520systems%2520that%2520can%2520represent%2520a%2520particular%250Aplan/policy%2520described%2520over%2520the%2520action-observation%2520histories%2520exist%2520and%2520are%250Aunique.%2520Finally%252C%2520the%2520results%2520are%2520applied%2520to%2520determine%2520sufficient%2520structures%2520for%250Adistance-optimal%2520navigation%2520in%2520a%2520polygonal%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Minimally%20sufficient%20structures%20for%20information-feedback%20policies&entry.906535625=Basak%20Sakcak%20and%20Vadim%20K.%20Weinstein%20and%20Kalle%20G.%20Timperi%20and%20Steven%20M.%20LaValle&entry.1292438233=%20%20In%20this%20paper%2C%20we%20consider%20robotic%20tasks%20which%20require%20a%20desirable%20outcome%20to%0Abe%20achieved%20in%20the%20physical%20world%20that%20the%20robot%20is%20embedded%20in%20and%20interacting%0Awith.%20Accomplishing%20this%20objective%20requires%20designing%20a%20filter%20that%20maintains%20a%0Auseful%20representation%20of%20the%20physical%20world%20and%20a%20policy%20over%20the%20filter%0Astates.%20A%20filter%20is%20seen%20as%20the%20robot%27s%20perspective%20of%20the%20physical%20world%20based%0Aon%20limited%20sensing%2C%20memory%2C%20and%20computation%20and%20it%20is%20represented%20as%20a%0Atransition%20system%20over%20a%20space%20of%20information%20states.%20To%20this%20end%2C%20the%0Ainteractions%20result%20from%20the%20coupling%20of%20an%20internal%20and%20an%20external%20system%2C%20a%0Afilter%2C%20and%20the%20physical%20world%2C%20respectively%2C%20through%20a%20sensor%20mapping%20and%20an%0Ainformation-feedback%20policy.%20Within%20this%20setup%2C%20we%20look%20for%20sufficient%0Astructures%2C%20that%20is%2C%20sufficient%20internal%20systems%20and%20sensors%2C%20for%20accomplishing%0Aa%20given%20task.%20We%20establish%20necessary%20and%20sufficient%20conditions%20for%20these%0Astructures%20to%20satisfy%20for%20information-feedback%20policies%20that%20can%20be%20defined%0Aover%20the%20states%20of%20an%20internal%20system%20to%20exist.%20We%20also%20show%20that%20under%20mild%0Aassumptions%2C%20minimal%20internal%20systems%20that%20can%20represent%20a%20particular%0Aplan/policy%20described%20over%20the%20action-observation%20histories%20exist%20and%20are%0Aunique.%20Finally%2C%20the%20results%20are%20applied%20to%20determine%20sufficient%20structures%20for%0Adistance-optimal%20navigation%20in%20a%20polygonal%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13852v1&entry.124074799=Read"},
{"title": "Lost in Sequence: Do Large Language Models Understand Sequential\n  Recommendation?", "author": "Sein Kim and Hongseok Kang and Kibum Kim and Jiwan Kim and Donghyun Kim and Minchul Yang and Kwangjin Oh and Julian McAuley and Chanyoung Park", "abstract": "  Large Language Models (LLMs) have recently emerged as promising tools for\nrecommendation thanks to their advanced textual understanding ability and\ncontext-awareness. Despite the current practice of training and evaluating\nLLM-based recommendation (LLM4Rec) models under a sequential recommendation\nscenario, we found that whether these models understand the sequential\ninformation inherent in users' item interaction sequences has been largely\noverlooked. In this paper, we first demonstrate through a series of experiments\nthat existing LLM4Rec models do not fully capture sequential information both\nduring training and inference. Then, we propose a simple yet effective\nLLM-based sequential recommender, called LLM-SRec, a method that enhances the\nintegration of sequential information into LLMs by distilling the user\nrepresentations extracted from a pre-trained CF-SRec model into LLMs. Our\nextensive experiments show that LLM-SRec enhances LLMs' ability to understand\nusers' item interaction sequences, ultimately leading to improved\nrecommendation performance. Furthermore, unlike existing LLM4Rec models that\nrequire fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by\ntraining only a few lightweight MLPs, highlighting its practicality in\nreal-world applications. Our code is available at\nhttps://github.com/Sein-Kim/LLM-SRec.\n", "link": "http://arxiv.org/abs/2502.13909v1", "date": "2025-02-19", "relevancy": 1.9921, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5089}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5089}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lost%20in%20Sequence%3A%20Do%20Large%20Language%20Models%20Understand%20Sequential%0A%20%20Recommendation%3F&body=Title%3A%20Lost%20in%20Sequence%3A%20Do%20Large%20Language%20Models%20Understand%20Sequential%0A%20%20Recommendation%3F%0AAuthor%3A%20Sein%20Kim%20and%20Hongseok%20Kang%20and%20Kibum%20Kim%20and%20Jiwan%20Kim%20and%20Donghyun%20Kim%20and%20Minchul%20Yang%20and%20Kwangjin%20Oh%20and%20Julian%20McAuley%20and%20Chanyoung%20Park%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20emerged%20as%20promising%20tools%20for%0Arecommendation%20thanks%20to%20their%20advanced%20textual%20understanding%20ability%20and%0Acontext-awareness.%20Despite%20the%20current%20practice%20of%20training%20and%20evaluating%0ALLM-based%20recommendation%20%28LLM4Rec%29%20models%20under%20a%20sequential%20recommendation%0Ascenario%2C%20we%20found%20that%20whether%20these%20models%20understand%20the%20sequential%0Ainformation%20inherent%20in%20users%27%20item%20interaction%20sequences%20has%20been%20largely%0Aoverlooked.%20In%20this%20paper%2C%20we%20first%20demonstrate%20through%20a%20series%20of%20experiments%0Athat%20existing%20LLM4Rec%20models%20do%20not%20fully%20capture%20sequential%20information%20both%0Aduring%20training%20and%20inference.%20Then%2C%20we%20propose%20a%20simple%20yet%20effective%0ALLM-based%20sequential%20recommender%2C%20called%20LLM-SRec%2C%20a%20method%20that%20enhances%20the%0Aintegration%20of%20sequential%20information%20into%20LLMs%20by%20distilling%20the%20user%0Arepresentations%20extracted%20from%20a%20pre-trained%20CF-SRec%20model%20into%20LLMs.%20Our%0Aextensive%20experiments%20show%20that%20LLM-SRec%20enhances%20LLMs%27%20ability%20to%20understand%0Ausers%27%20item%20interaction%20sequences%2C%20ultimately%20leading%20to%20improved%0Arecommendation%20performance.%20Furthermore%2C%20unlike%20existing%20LLM4Rec%20models%20that%0Arequire%20fine-tuning%20of%20LLMs%2C%20LLM-SRec%20achieves%20state-of-the-art%20performance%20by%0Atraining%20only%20a%20few%20lightweight%20MLPs%2C%20highlighting%20its%20practicality%20in%0Areal-world%20applications.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Sein-Kim/LLM-SRec.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLost%2520in%2520Sequence%253A%2520Do%2520Large%2520Language%2520Models%2520Understand%2520Sequential%250A%2520%2520Recommendation%253F%26entry.906535625%3DSein%2520Kim%2520and%2520Hongseok%2520Kang%2520and%2520Kibum%2520Kim%2520and%2520Jiwan%2520Kim%2520and%2520Donghyun%2520Kim%2520and%2520Minchul%2520Yang%2520and%2520Kwangjin%2520Oh%2520and%2520Julian%2520McAuley%2520and%2520Chanyoung%2520Park%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520recently%2520emerged%2520as%2520promising%2520tools%2520for%250Arecommendation%2520thanks%2520to%2520their%2520advanced%2520textual%2520understanding%2520ability%2520and%250Acontext-awareness.%2520Despite%2520the%2520current%2520practice%2520of%2520training%2520and%2520evaluating%250ALLM-based%2520recommendation%2520%2528LLM4Rec%2529%2520models%2520under%2520a%2520sequential%2520recommendation%250Ascenario%252C%2520we%2520found%2520that%2520whether%2520these%2520models%2520understand%2520the%2520sequential%250Ainformation%2520inherent%2520in%2520users%2527%2520item%2520interaction%2520sequences%2520has%2520been%2520largely%250Aoverlooked.%2520In%2520this%2520paper%252C%2520we%2520first%2520demonstrate%2520through%2520a%2520series%2520of%2520experiments%250Athat%2520existing%2520LLM4Rec%2520models%2520do%2520not%2520fully%2520capture%2520sequential%2520information%2520both%250Aduring%2520training%2520and%2520inference.%2520Then%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%250ALLM-based%2520sequential%2520recommender%252C%2520called%2520LLM-SRec%252C%2520a%2520method%2520that%2520enhances%2520the%250Aintegration%2520of%2520sequential%2520information%2520into%2520LLMs%2520by%2520distilling%2520the%2520user%250Arepresentations%2520extracted%2520from%2520a%2520pre-trained%2520CF-SRec%2520model%2520into%2520LLMs.%2520Our%250Aextensive%2520experiments%2520show%2520that%2520LLM-SRec%2520enhances%2520LLMs%2527%2520ability%2520to%2520understand%250Ausers%2527%2520item%2520interaction%2520sequences%252C%2520ultimately%2520leading%2520to%2520improved%250Arecommendation%2520performance.%2520Furthermore%252C%2520unlike%2520existing%2520LLM4Rec%2520models%2520that%250Arequire%2520fine-tuning%2520of%2520LLMs%252C%2520LLM-SRec%2520achieves%2520state-of-the-art%2520performance%2520by%250Atraining%2520only%2520a%2520few%2520lightweight%2520MLPs%252C%2520highlighting%2520its%2520practicality%2520in%250Areal-world%2520applications.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Sein-Kim/LLM-SRec.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lost%20in%20Sequence%3A%20Do%20Large%20Language%20Models%20Understand%20Sequential%0A%20%20Recommendation%3F&entry.906535625=Sein%20Kim%20and%20Hongseok%20Kang%20and%20Kibum%20Kim%20and%20Jiwan%20Kim%20and%20Donghyun%20Kim%20and%20Minchul%20Yang%20and%20Kwangjin%20Oh%20and%20Julian%20McAuley%20and%20Chanyoung%20Park&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20emerged%20as%20promising%20tools%20for%0Arecommendation%20thanks%20to%20their%20advanced%20textual%20understanding%20ability%20and%0Acontext-awareness.%20Despite%20the%20current%20practice%20of%20training%20and%20evaluating%0ALLM-based%20recommendation%20%28LLM4Rec%29%20models%20under%20a%20sequential%20recommendation%0Ascenario%2C%20we%20found%20that%20whether%20these%20models%20understand%20the%20sequential%0Ainformation%20inherent%20in%20users%27%20item%20interaction%20sequences%20has%20been%20largely%0Aoverlooked.%20In%20this%20paper%2C%20we%20first%20demonstrate%20through%20a%20series%20of%20experiments%0Athat%20existing%20LLM4Rec%20models%20do%20not%20fully%20capture%20sequential%20information%20both%0Aduring%20training%20and%20inference.%20Then%2C%20we%20propose%20a%20simple%20yet%20effective%0ALLM-based%20sequential%20recommender%2C%20called%20LLM-SRec%2C%20a%20method%20that%20enhances%20the%0Aintegration%20of%20sequential%20information%20into%20LLMs%20by%20distilling%20the%20user%0Arepresentations%20extracted%20from%20a%20pre-trained%20CF-SRec%20model%20into%20LLMs.%20Our%0Aextensive%20experiments%20show%20that%20LLM-SRec%20enhances%20LLMs%27%20ability%20to%20understand%0Ausers%27%20item%20interaction%20sequences%2C%20ultimately%20leading%20to%20improved%0Arecommendation%20performance.%20Furthermore%2C%20unlike%20existing%20LLM4Rec%20models%20that%0Arequire%20fine-tuning%20of%20LLMs%2C%20LLM-SRec%20achieves%20state-of-the-art%20performance%20by%0Atraining%20only%20a%20few%20lightweight%20MLPs%2C%20highlighting%20its%20practicality%20in%0Areal-world%20applications.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Sein-Kim/LLM-SRec.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13909v1&entry.124074799=Read"},
{"title": "Through the Looking-Glass: Transparency Implications and Challenges in\n  Enterprise AI Knowledge Systems", "author": "Karina Corti\u00f1as-Lorenzo and Si\u00e2n Lindley and Ida Larsen-Ledet and Bhaskar Mitra", "abstract": "  Knowledge can't be disentangled from people. As AI knowledge systems mine\nvast volumes of work-related data, the knowledge that's being extracted and\nsurfaced is intrinsically linked to the people who create and use it. When\npredictive algorithms that learn from data are used to link knowledge and\npeople, inaccuracies in knowledge extraction and surfacing can lead to\ndisproportionate harms, influencing how individuals see each other and how they\nsee themselves at work. In this paper, we present a reflective analysis of\ntransparency requirements and impacts in this type of systems. We conduct a\nmultidisciplinary literature review to understand the impacts of transparency\nin workplace settings, introducing the looking-glass metaphor to conceptualize\nAI knowledge systems as systems that reflect and distort, expanding our view on\ntransparency requirements, implications and challenges. We formulate\ntransparency as a key mediator in shaping different ways of seeing, including\nseeing into the system, which unveils its capabilities, limitations and\nbehavior, and seeing through the system, which shapes workers' perceptions of\ntheir own contributions and others within the organization. Recognizing the\nsociotechnical nature of these systems, we identify three transparency\ndimensions necessary to realize the value of AI knowledge systems, namely\nsystem transparency, procedural transparency and transparency of outcomes. We\ndiscuss key challenges hindering the implementation of these forms of\ntransparency, bringing to light the wider sociotechnical gap and highlighting\ndirections for future Computer-supported Cooperative Work (CSCW) research.\n", "link": "http://arxiv.org/abs/2401.09410v3", "date": "2025-02-19", "relevancy": 1.3052, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4405}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4389}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Through%20the%20Looking-Glass%3A%20Transparency%20Implications%20and%20Challenges%20in%0A%20%20Enterprise%20AI%20Knowledge%20Systems&body=Title%3A%20Through%20the%20Looking-Glass%3A%20Transparency%20Implications%20and%20Challenges%20in%0A%20%20Enterprise%20AI%20Knowledge%20Systems%0AAuthor%3A%20Karina%20Corti%C3%B1as-Lorenzo%20and%20Si%C3%A2n%20Lindley%20and%20Ida%20Larsen-Ledet%20and%20Bhaskar%20Mitra%0AAbstract%3A%20%20%20Knowledge%20can%27t%20be%20disentangled%20from%20people.%20As%20AI%20knowledge%20systems%20mine%0Avast%20volumes%20of%20work-related%20data%2C%20the%20knowledge%20that%27s%20being%20extracted%20and%0Asurfaced%20is%20intrinsically%20linked%20to%20the%20people%20who%20create%20and%20use%20it.%20When%0Apredictive%20algorithms%20that%20learn%20from%20data%20are%20used%20to%20link%20knowledge%20and%0Apeople%2C%20inaccuracies%20in%20knowledge%20extraction%20and%20surfacing%20can%20lead%20to%0Adisproportionate%20harms%2C%20influencing%20how%20individuals%20see%20each%20other%20and%20how%20they%0Asee%20themselves%20at%20work.%20In%20this%20paper%2C%20we%20present%20a%20reflective%20analysis%20of%0Atransparency%20requirements%20and%20impacts%20in%20this%20type%20of%20systems.%20We%20conduct%20a%0Amultidisciplinary%20literature%20review%20to%20understand%20the%20impacts%20of%20transparency%0Ain%20workplace%20settings%2C%20introducing%20the%20looking-glass%20metaphor%20to%20conceptualize%0AAI%20knowledge%20systems%20as%20systems%20that%20reflect%20and%20distort%2C%20expanding%20our%20view%20on%0Atransparency%20requirements%2C%20implications%20and%20challenges.%20We%20formulate%0Atransparency%20as%20a%20key%20mediator%20in%20shaping%20different%20ways%20of%20seeing%2C%20including%0Aseeing%20into%20the%20system%2C%20which%20unveils%20its%20capabilities%2C%20limitations%20and%0Abehavior%2C%20and%20seeing%20through%20the%20system%2C%20which%20shapes%20workers%27%20perceptions%20of%0Atheir%20own%20contributions%20and%20others%20within%20the%20organization.%20Recognizing%20the%0Asociotechnical%20nature%20of%20these%20systems%2C%20we%20identify%20three%20transparency%0Adimensions%20necessary%20to%20realize%20the%20value%20of%20AI%20knowledge%20systems%2C%20namely%0Asystem%20transparency%2C%20procedural%20transparency%20and%20transparency%20of%20outcomes.%20We%0Adiscuss%20key%20challenges%20hindering%20the%20implementation%20of%20these%20forms%20of%0Atransparency%2C%20bringing%20to%20light%20the%20wider%20sociotechnical%20gap%20and%20highlighting%0Adirections%20for%20future%20Computer-supported%20Cooperative%20Work%20%28CSCW%29%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.09410v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThrough%2520the%2520Looking-Glass%253A%2520Transparency%2520Implications%2520and%2520Challenges%2520in%250A%2520%2520Enterprise%2520AI%2520Knowledge%2520Systems%26entry.906535625%3DKarina%2520Corti%25C3%25B1as-Lorenzo%2520and%2520Si%25C3%25A2n%2520Lindley%2520and%2520Ida%2520Larsen-Ledet%2520and%2520Bhaskar%2520Mitra%26entry.1292438233%3D%2520%2520Knowledge%2520can%2527t%2520be%2520disentangled%2520from%2520people.%2520As%2520AI%2520knowledge%2520systems%2520mine%250Avast%2520volumes%2520of%2520work-related%2520data%252C%2520the%2520knowledge%2520that%2527s%2520being%2520extracted%2520and%250Asurfaced%2520is%2520intrinsically%2520linked%2520to%2520the%2520people%2520who%2520create%2520and%2520use%2520it.%2520When%250Apredictive%2520algorithms%2520that%2520learn%2520from%2520data%2520are%2520used%2520to%2520link%2520knowledge%2520and%250Apeople%252C%2520inaccuracies%2520in%2520knowledge%2520extraction%2520and%2520surfacing%2520can%2520lead%2520to%250Adisproportionate%2520harms%252C%2520influencing%2520how%2520individuals%2520see%2520each%2520other%2520and%2520how%2520they%250Asee%2520themselves%2520at%2520work.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520reflective%2520analysis%2520of%250Atransparency%2520requirements%2520and%2520impacts%2520in%2520this%2520type%2520of%2520systems.%2520We%2520conduct%2520a%250Amultidisciplinary%2520literature%2520review%2520to%2520understand%2520the%2520impacts%2520of%2520transparency%250Ain%2520workplace%2520settings%252C%2520introducing%2520the%2520looking-glass%2520metaphor%2520to%2520conceptualize%250AAI%2520knowledge%2520systems%2520as%2520systems%2520that%2520reflect%2520and%2520distort%252C%2520expanding%2520our%2520view%2520on%250Atransparency%2520requirements%252C%2520implications%2520and%2520challenges.%2520We%2520formulate%250Atransparency%2520as%2520a%2520key%2520mediator%2520in%2520shaping%2520different%2520ways%2520of%2520seeing%252C%2520including%250Aseeing%2520into%2520the%2520system%252C%2520which%2520unveils%2520its%2520capabilities%252C%2520limitations%2520and%250Abehavior%252C%2520and%2520seeing%2520through%2520the%2520system%252C%2520which%2520shapes%2520workers%2527%2520perceptions%2520of%250Atheir%2520own%2520contributions%2520and%2520others%2520within%2520the%2520organization.%2520Recognizing%2520the%250Asociotechnical%2520nature%2520of%2520these%2520systems%252C%2520we%2520identify%2520three%2520transparency%250Adimensions%2520necessary%2520to%2520realize%2520the%2520value%2520of%2520AI%2520knowledge%2520systems%252C%2520namely%250Asystem%2520transparency%252C%2520procedural%2520transparency%2520and%2520transparency%2520of%2520outcomes.%2520We%250Adiscuss%2520key%2520challenges%2520hindering%2520the%2520implementation%2520of%2520these%2520forms%2520of%250Atransparency%252C%2520bringing%2520to%2520light%2520the%2520wider%2520sociotechnical%2520gap%2520and%2520highlighting%250Adirections%2520for%2520future%2520Computer-supported%2520Cooperative%2520Work%2520%2528CSCW%2529%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.09410v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Through%20the%20Looking-Glass%3A%20Transparency%20Implications%20and%20Challenges%20in%0A%20%20Enterprise%20AI%20Knowledge%20Systems&entry.906535625=Karina%20Corti%C3%B1as-Lorenzo%20and%20Si%C3%A2n%20Lindley%20and%20Ida%20Larsen-Ledet%20and%20Bhaskar%20Mitra&entry.1292438233=%20%20Knowledge%20can%27t%20be%20disentangled%20from%20people.%20As%20AI%20knowledge%20systems%20mine%0Avast%20volumes%20of%20work-related%20data%2C%20the%20knowledge%20that%27s%20being%20extracted%20and%0Asurfaced%20is%20intrinsically%20linked%20to%20the%20people%20who%20create%20and%20use%20it.%20When%0Apredictive%20algorithms%20that%20learn%20from%20data%20are%20used%20to%20link%20knowledge%20and%0Apeople%2C%20inaccuracies%20in%20knowledge%20extraction%20and%20surfacing%20can%20lead%20to%0Adisproportionate%20harms%2C%20influencing%20how%20individuals%20see%20each%20other%20and%20how%20they%0Asee%20themselves%20at%20work.%20In%20this%20paper%2C%20we%20present%20a%20reflective%20analysis%20of%0Atransparency%20requirements%20and%20impacts%20in%20this%20type%20of%20systems.%20We%20conduct%20a%0Amultidisciplinary%20literature%20review%20to%20understand%20the%20impacts%20of%20transparency%0Ain%20workplace%20settings%2C%20introducing%20the%20looking-glass%20metaphor%20to%20conceptualize%0AAI%20knowledge%20systems%20as%20systems%20that%20reflect%20and%20distort%2C%20expanding%20our%20view%20on%0Atransparency%20requirements%2C%20implications%20and%20challenges.%20We%20formulate%0Atransparency%20as%20a%20key%20mediator%20in%20shaping%20different%20ways%20of%20seeing%2C%20including%0Aseeing%20into%20the%20system%2C%20which%20unveils%20its%20capabilities%2C%20limitations%20and%0Abehavior%2C%20and%20seeing%20through%20the%20system%2C%20which%20shapes%20workers%27%20perceptions%20of%0Atheir%20own%20contributions%20and%20others%20within%20the%20organization.%20Recognizing%20the%0Asociotechnical%20nature%20of%20these%20systems%2C%20we%20identify%20three%20transparency%0Adimensions%20necessary%20to%20realize%20the%20value%20of%20AI%20knowledge%20systems%2C%20namely%0Asystem%20transparency%2C%20procedural%20transparency%20and%20transparency%20of%20outcomes.%20We%0Adiscuss%20key%20challenges%20hindering%20the%20implementation%20of%20these%20forms%20of%0Atransparency%2C%20bringing%20to%20light%20the%20wider%20sociotechnical%20gap%20and%20highlighting%0Adirections%20for%20future%20Computer-supported%20Cooperative%20Work%20%28CSCW%29%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.09410v3&entry.124074799=Read"},
{"title": "An LLM-based Agent for Reliable Docker Environment Configuration", "author": "Ruida Hu and Chao Peng and Xinchen Wang and Cuiyun Gao", "abstract": "  Environment configuration is a critical yet time-consuming step in software\ndevelopment, especially when dealing with unfamiliar code repositories. While\nLarge Language Models (LLMs) demonstrate the potential to accomplish software\nengineering tasks, existing methods for environment configuration often rely on\nmanual efforts or fragile scripts, leading to inefficiencies and unreliable\noutcomes. We introduce Repo2Run, the first LLM-based agent designed to fully\nautomate environment configuration and generate executable Dockerfiles for\narbitrary Python repositories. We address two major challenges: (1) enabling\nthe LLM agent to configure environments within isolated Docker containers, and\n(2) ensuring the successful configuration process is recorded and accurately\ntransferred to a Dockerfile without error. To achieve this, we propose atomic\nconfiguration synthesis, featuring a dual-environment architecture (internal\nand external environment) with a rollback mechanism to prevent environment\n\"pollution\" from failed commands, guaranteeing atomic execution (execute fully\nor not at all) and a Dockerfile generator to transfer successful configuration\nsteps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark\nof 420 recent Python repositories with unit tests, where it achieves an 86.0%\nsuccess rate, outperforming the best baseline by 63.9%.\n", "link": "http://arxiv.org/abs/2502.13681v1", "date": "2025-02-19", "relevancy": 1.2457, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.457}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4055}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20LLM-based%20Agent%20for%20Reliable%20Docker%20Environment%20Configuration&body=Title%3A%20An%20LLM-based%20Agent%20for%20Reliable%20Docker%20Environment%20Configuration%0AAuthor%3A%20Ruida%20Hu%20and%20Chao%20Peng%20and%20Xinchen%20Wang%20and%20Cuiyun%20Gao%0AAbstract%3A%20%20%20Environment%20configuration%20is%20a%20critical%20yet%20time-consuming%20step%20in%20software%0Adevelopment%2C%20especially%20when%20dealing%20with%20unfamiliar%20code%20repositories.%20While%0ALarge%20Language%20Models%20%28LLMs%29%20demonstrate%20the%20potential%20to%20accomplish%20software%0Aengineering%20tasks%2C%20existing%20methods%20for%20environment%20configuration%20often%20rely%20on%0Amanual%20efforts%20or%20fragile%20scripts%2C%20leading%20to%20inefficiencies%20and%20unreliable%0Aoutcomes.%20We%20introduce%20Repo2Run%2C%20the%20first%20LLM-based%20agent%20designed%20to%20fully%0Aautomate%20environment%20configuration%20and%20generate%20executable%20Dockerfiles%20for%0Aarbitrary%20Python%20repositories.%20We%20address%20two%20major%20challenges%3A%20%281%29%20enabling%0Athe%20LLM%20agent%20to%20configure%20environments%20within%20isolated%20Docker%20containers%2C%20and%0A%282%29%20ensuring%20the%20successful%20configuration%20process%20is%20recorded%20and%20accurately%0Atransferred%20to%20a%20Dockerfile%20without%20error.%20To%20achieve%20this%2C%20we%20propose%20atomic%0Aconfiguration%20synthesis%2C%20featuring%20a%20dual-environment%20architecture%20%28internal%0Aand%20external%20environment%29%20with%20a%20rollback%20mechanism%20to%20prevent%20environment%0A%22pollution%22%20from%20failed%20commands%2C%20guaranteeing%20atomic%20execution%20%28execute%20fully%0Aor%20not%20at%20all%29%20and%20a%20Dockerfile%20generator%20to%20transfer%20successful%20configuration%0Asteps%20into%20runnable%20Dockerfiles.%20We%20evaluate%20Repo2Run~on%20our%20proposed%20benchmark%0Aof%20420%20recent%20Python%20repositories%20with%20unit%20tests%2C%20where%20it%20achieves%20an%2086.0%25%0Asuccess%20rate%2C%20outperforming%20the%20best%20baseline%20by%2063.9%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520LLM-based%2520Agent%2520for%2520Reliable%2520Docker%2520Environment%2520Configuration%26entry.906535625%3DRuida%2520Hu%2520and%2520Chao%2520Peng%2520and%2520Xinchen%2520Wang%2520and%2520Cuiyun%2520Gao%26entry.1292438233%3D%2520%2520Environment%2520configuration%2520is%2520a%2520critical%2520yet%2520time-consuming%2520step%2520in%2520software%250Adevelopment%252C%2520especially%2520when%2520dealing%2520with%2520unfamiliar%2520code%2520repositories.%2520While%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrate%2520the%2520potential%2520to%2520accomplish%2520software%250Aengineering%2520tasks%252C%2520existing%2520methods%2520for%2520environment%2520configuration%2520often%2520rely%2520on%250Amanual%2520efforts%2520or%2520fragile%2520scripts%252C%2520leading%2520to%2520inefficiencies%2520and%2520unreliable%250Aoutcomes.%2520We%2520introduce%2520Repo2Run%252C%2520the%2520first%2520LLM-based%2520agent%2520designed%2520to%2520fully%250Aautomate%2520environment%2520configuration%2520and%2520generate%2520executable%2520Dockerfiles%2520for%250Aarbitrary%2520Python%2520repositories.%2520We%2520address%2520two%2520major%2520challenges%253A%2520%25281%2529%2520enabling%250Athe%2520LLM%2520agent%2520to%2520configure%2520environments%2520within%2520isolated%2520Docker%2520containers%252C%2520and%250A%25282%2529%2520ensuring%2520the%2520successful%2520configuration%2520process%2520is%2520recorded%2520and%2520accurately%250Atransferred%2520to%2520a%2520Dockerfile%2520without%2520error.%2520To%2520achieve%2520this%252C%2520we%2520propose%2520atomic%250Aconfiguration%2520synthesis%252C%2520featuring%2520a%2520dual-environment%2520architecture%2520%2528internal%250Aand%2520external%2520environment%2529%2520with%2520a%2520rollback%2520mechanism%2520to%2520prevent%2520environment%250A%2522pollution%2522%2520from%2520failed%2520commands%252C%2520guaranteeing%2520atomic%2520execution%2520%2528execute%2520fully%250Aor%2520not%2520at%2520all%2529%2520and%2520a%2520Dockerfile%2520generator%2520to%2520transfer%2520successful%2520configuration%250Asteps%2520into%2520runnable%2520Dockerfiles.%2520We%2520evaluate%2520Repo2Run~on%2520our%2520proposed%2520benchmark%250Aof%2520420%2520recent%2520Python%2520repositories%2520with%2520unit%2520tests%252C%2520where%2520it%2520achieves%2520an%252086.0%2525%250Asuccess%2520rate%252C%2520outperforming%2520the%2520best%2520baseline%2520by%252063.9%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20LLM-based%20Agent%20for%20Reliable%20Docker%20Environment%20Configuration&entry.906535625=Ruida%20Hu%20and%20Chao%20Peng%20and%20Xinchen%20Wang%20and%20Cuiyun%20Gao&entry.1292438233=%20%20Environment%20configuration%20is%20a%20critical%20yet%20time-consuming%20step%20in%20software%0Adevelopment%2C%20especially%20when%20dealing%20with%20unfamiliar%20code%20repositories.%20While%0ALarge%20Language%20Models%20%28LLMs%29%20demonstrate%20the%20potential%20to%20accomplish%20software%0Aengineering%20tasks%2C%20existing%20methods%20for%20environment%20configuration%20often%20rely%20on%0Amanual%20efforts%20or%20fragile%20scripts%2C%20leading%20to%20inefficiencies%20and%20unreliable%0Aoutcomes.%20We%20introduce%20Repo2Run%2C%20the%20first%20LLM-based%20agent%20designed%20to%20fully%0Aautomate%20environment%20configuration%20and%20generate%20executable%20Dockerfiles%20for%0Aarbitrary%20Python%20repositories.%20We%20address%20two%20major%20challenges%3A%20%281%29%20enabling%0Athe%20LLM%20agent%20to%20configure%20environments%20within%20isolated%20Docker%20containers%2C%20and%0A%282%29%20ensuring%20the%20successful%20configuration%20process%20is%20recorded%20and%20accurately%0Atransferred%20to%20a%20Dockerfile%20without%20error.%20To%20achieve%20this%2C%20we%20propose%20atomic%0Aconfiguration%20synthesis%2C%20featuring%20a%20dual-environment%20architecture%20%28internal%0Aand%20external%20environment%29%20with%20a%20rollback%20mechanism%20to%20prevent%20environment%0A%22pollution%22%20from%20failed%20commands%2C%20guaranteeing%20atomic%20execution%20%28execute%20fully%0Aor%20not%20at%20all%29%20and%20a%20Dockerfile%20generator%20to%20transfer%20successful%20configuration%0Asteps%20into%20runnable%20Dockerfiles.%20We%20evaluate%20Repo2Run~on%20our%20proposed%20benchmark%0Aof%20420%20recent%20Python%20repositories%20with%20unit%20tests%2C%20where%20it%20achieves%20an%2086.0%25%0Asuccess%20rate%2C%20outperforming%20the%20best%20baseline%20by%2063.9%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13681v1&entry.124074799=Read"},
{"title": "From Local to Global: A Graph RAG Approach to Query-Focused\n  Summarization", "author": "Darren Edge and Ha Trinh and Newman Cheng and Joshua Bradley and Alex Chao and Apurva Mody and Steven Truitt and Dasha Metropolitansky and Robert Osazuwa Ness and Jonathan Larson", "abstract": "  The use of retrieval-augmented generation (RAG) to retrieve relevant\ninformation from an external knowledge source enables large language models\n(LLMs) to answer questions over private and/or previously unseen document\ncollections. However, RAG fails on global questions directed at an entire text\ncorpus, such as \"What are the main themes in the dataset?\", since this is\ninherently a query-focused summarization (QFS) task, rather than an explicit\nretrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of\ntext indexed by typical RAG systems. To combine the strengths of these\ncontrasting methods, we propose GraphRAG, a graph-based approach to question\nanswering over private text corpora that scales with both the generality of\nuser questions and the quantity of source text. Our approach uses an LLM to\nbuild a graph index in two stages: first, to derive an entity knowledge graph\nfrom the source documents, then to pregenerate community summaries for all\ngroups of closely related entities. Given a question, each community summary is\nused to generate a partial response, before all partial responses are again\nsummarized in a final response to the user. For a class of global sensemaking\nquestions over datasets in the 1 million token range, we show that GraphRAG\nleads to substantial improvements over a conventional RAG baseline for both the\ncomprehensiveness and diversity of generated answers.\n", "link": "http://arxiv.org/abs/2404.16130v2", "date": "2025-02-19", "relevancy": 1.8433, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4996}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4531}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Local%20to%20Global%3A%20A%20Graph%20RAG%20Approach%20to%20Query-Focused%0A%20%20Summarization&body=Title%3A%20From%20Local%20to%20Global%3A%20A%20Graph%20RAG%20Approach%20to%20Query-Focused%0A%20%20Summarization%0AAuthor%3A%20Darren%20Edge%20and%20Ha%20Trinh%20and%20Newman%20Cheng%20and%20Joshua%20Bradley%20and%20Alex%20Chao%20and%20Apurva%20Mody%20and%20Steven%20Truitt%20and%20Dasha%20Metropolitansky%20and%20Robert%20Osazuwa%20Ness%20and%20Jonathan%20Larson%0AAbstract%3A%20%20%20The%20use%20of%20retrieval-augmented%20generation%20%28RAG%29%20to%20retrieve%20relevant%0Ainformation%20from%20an%20external%20knowledge%20source%20enables%20large%20language%20models%0A%28LLMs%29%20to%20answer%20questions%20over%20private%20and/or%20previously%20unseen%20document%0Acollections.%20However%2C%20RAG%20fails%20on%20global%20questions%20directed%20at%20an%20entire%20text%0Acorpus%2C%20such%20as%20%22What%20are%20the%20main%20themes%20in%20the%20dataset%3F%22%2C%20since%20this%20is%0Ainherently%20a%20query-focused%20summarization%20%28QFS%29%20task%2C%20rather%20than%20an%20explicit%0Aretrieval%20task.%20Prior%20QFS%20methods%2C%20meanwhile%2C%20do%20not%20scale%20to%20the%20quantities%20of%0Atext%20indexed%20by%20typical%20RAG%20systems.%20To%20combine%20the%20strengths%20of%20these%0Acontrasting%20methods%2C%20we%20propose%20GraphRAG%2C%20a%20graph-based%20approach%20to%20question%0Aanswering%20over%20private%20text%20corpora%20that%20scales%20with%20both%20the%20generality%20of%0Auser%20questions%20and%20the%20quantity%20of%20source%20text.%20Our%20approach%20uses%20an%20LLM%20to%0Abuild%20a%20graph%20index%20in%20two%20stages%3A%20first%2C%20to%20derive%20an%20entity%20knowledge%20graph%0Afrom%20the%20source%20documents%2C%20then%20to%20pregenerate%20community%20summaries%20for%20all%0Agroups%20of%20closely%20related%20entities.%20Given%20a%20question%2C%20each%20community%20summary%20is%0Aused%20to%20generate%20a%20partial%20response%2C%20before%20all%20partial%20responses%20are%20again%0Asummarized%20in%20a%20final%20response%20to%20the%20user.%20For%20a%20class%20of%20global%20sensemaking%0Aquestions%20over%20datasets%20in%20the%201%20million%20token%20range%2C%20we%20show%20that%20GraphRAG%0Aleads%20to%20substantial%20improvements%20over%20a%20conventional%20RAG%20baseline%20for%20both%20the%0Acomprehensiveness%20and%20diversity%20of%20generated%20answers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16130v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Local%2520to%2520Global%253A%2520A%2520Graph%2520RAG%2520Approach%2520to%2520Query-Focused%250A%2520%2520Summarization%26entry.906535625%3DDarren%2520Edge%2520and%2520Ha%2520Trinh%2520and%2520Newman%2520Cheng%2520and%2520Joshua%2520Bradley%2520and%2520Alex%2520Chao%2520and%2520Apurva%2520Mody%2520and%2520Steven%2520Truitt%2520and%2520Dasha%2520Metropolitansky%2520and%2520Robert%2520Osazuwa%2520Ness%2520and%2520Jonathan%2520Larson%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520to%2520retrieve%2520relevant%250Ainformation%2520from%2520an%2520external%2520knowledge%2520source%2520enables%2520large%2520language%2520models%250A%2528LLMs%2529%2520to%2520answer%2520questions%2520over%2520private%2520and/or%2520previously%2520unseen%2520document%250Acollections.%2520However%252C%2520RAG%2520fails%2520on%2520global%2520questions%2520directed%2520at%2520an%2520entire%2520text%250Acorpus%252C%2520such%2520as%2520%2522What%2520are%2520the%2520main%2520themes%2520in%2520the%2520dataset%253F%2522%252C%2520since%2520this%2520is%250Ainherently%2520a%2520query-focused%2520summarization%2520%2528QFS%2529%2520task%252C%2520rather%2520than%2520an%2520explicit%250Aretrieval%2520task.%2520Prior%2520QFS%2520methods%252C%2520meanwhile%252C%2520do%2520not%2520scale%2520to%2520the%2520quantities%2520of%250Atext%2520indexed%2520by%2520typical%2520RAG%2520systems.%2520To%2520combine%2520the%2520strengths%2520of%2520these%250Acontrasting%2520methods%252C%2520we%2520propose%2520GraphRAG%252C%2520a%2520graph-based%2520approach%2520to%2520question%250Aanswering%2520over%2520private%2520text%2520corpora%2520that%2520scales%2520with%2520both%2520the%2520generality%2520of%250Auser%2520questions%2520and%2520the%2520quantity%2520of%2520source%2520text.%2520Our%2520approach%2520uses%2520an%2520LLM%2520to%250Abuild%2520a%2520graph%2520index%2520in%2520two%2520stages%253A%2520first%252C%2520to%2520derive%2520an%2520entity%2520knowledge%2520graph%250Afrom%2520the%2520source%2520documents%252C%2520then%2520to%2520pregenerate%2520community%2520summaries%2520for%2520all%250Agroups%2520of%2520closely%2520related%2520entities.%2520Given%2520a%2520question%252C%2520each%2520community%2520summary%2520is%250Aused%2520to%2520generate%2520a%2520partial%2520response%252C%2520before%2520all%2520partial%2520responses%2520are%2520again%250Asummarized%2520in%2520a%2520final%2520response%2520to%2520the%2520user.%2520For%2520a%2520class%2520of%2520global%2520sensemaking%250Aquestions%2520over%2520datasets%2520in%2520the%25201%2520million%2520token%2520range%252C%2520we%2520show%2520that%2520GraphRAG%250Aleads%2520to%2520substantial%2520improvements%2520over%2520a%2520conventional%2520RAG%2520baseline%2520for%2520both%2520the%250Acomprehensiveness%2520and%2520diversity%2520of%2520generated%2520answers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16130v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Local%20to%20Global%3A%20A%20Graph%20RAG%20Approach%20to%20Query-Focused%0A%20%20Summarization&entry.906535625=Darren%20Edge%20and%20Ha%20Trinh%20and%20Newman%20Cheng%20and%20Joshua%20Bradley%20and%20Alex%20Chao%20and%20Apurva%20Mody%20and%20Steven%20Truitt%20and%20Dasha%20Metropolitansky%20and%20Robert%20Osazuwa%20Ness%20and%20Jonathan%20Larson&entry.1292438233=%20%20The%20use%20of%20retrieval-augmented%20generation%20%28RAG%29%20to%20retrieve%20relevant%0Ainformation%20from%20an%20external%20knowledge%20source%20enables%20large%20language%20models%0A%28LLMs%29%20to%20answer%20questions%20over%20private%20and/or%20previously%20unseen%20document%0Acollections.%20However%2C%20RAG%20fails%20on%20global%20questions%20directed%20at%20an%20entire%20text%0Acorpus%2C%20such%20as%20%22What%20are%20the%20main%20themes%20in%20the%20dataset%3F%22%2C%20since%20this%20is%0Ainherently%20a%20query-focused%20summarization%20%28QFS%29%20task%2C%20rather%20than%20an%20explicit%0Aretrieval%20task.%20Prior%20QFS%20methods%2C%20meanwhile%2C%20do%20not%20scale%20to%20the%20quantities%20of%0Atext%20indexed%20by%20typical%20RAG%20systems.%20To%20combine%20the%20strengths%20of%20these%0Acontrasting%20methods%2C%20we%20propose%20GraphRAG%2C%20a%20graph-based%20approach%20to%20question%0Aanswering%20over%20private%20text%20corpora%20that%20scales%20with%20both%20the%20generality%20of%0Auser%20questions%20and%20the%20quantity%20of%20source%20text.%20Our%20approach%20uses%20an%20LLM%20to%0Abuild%20a%20graph%20index%20in%20two%20stages%3A%20first%2C%20to%20derive%20an%20entity%20knowledge%20graph%0Afrom%20the%20source%20documents%2C%20then%20to%20pregenerate%20community%20summaries%20for%20all%0Agroups%20of%20closely%20related%20entities.%20Given%20a%20question%2C%20each%20community%20summary%20is%0Aused%20to%20generate%20a%20partial%20response%2C%20before%20all%20partial%20responses%20are%20again%0Asummarized%20in%20a%20final%20response%20to%20the%20user.%20For%20a%20class%20of%20global%20sensemaking%0Aquestions%20over%20datasets%20in%20the%201%20million%20token%20range%2C%20we%20show%20that%20GraphRAG%0Aleads%20to%20substantial%20improvements%20over%20a%20conventional%20RAG%20baseline%20for%20both%20the%0Acomprehensiveness%20and%20diversity%20of%20generated%20answers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16130v2&entry.124074799=Read"},
{"title": "Early-Stage Anomaly Detection: A Study of Model Performance on Complete\n  vs. Partial Flows", "author": "Adrian Pekar and Richard Jozsa", "abstract": "  This study investigates the efficacy of machine learning models in network\nanomaly detection through the critical lens of partial versus complete flow\ninformation. We systematically evaluate how models perform under varying\ntraining and testing conditions, quantifying the performance impact when\ndealing with incomplete data typical in real-time environments. Our findings\ndemonstrate a significant performance difference, with precision and recall\ndropping by up to 30% under certain conditions when models trained on complete\nflows are tested against partial flows. Conversely, models trained and tested\non consistently complete or partial datasets maintain robustness. The study\nreveals that a minimum of 7 packets in the test set is required for maintaining\nreliable detection rates, providing valuable insights for real-time detection\nstrategies. These results offer important guidance for deploying machine\nlearning models in operational network security environments.\n", "link": "http://arxiv.org/abs/2407.02856v2", "date": "2025-02-19", "relevancy": 1.3207, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.461}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4539}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Early-Stage%20Anomaly%20Detection%3A%20A%20Study%20of%20Model%20Performance%20on%20Complete%0A%20%20vs.%20Partial%20Flows&body=Title%3A%20Early-Stage%20Anomaly%20Detection%3A%20A%20Study%20of%20Model%20Performance%20on%20Complete%0A%20%20vs.%20Partial%20Flows%0AAuthor%3A%20Adrian%20Pekar%20and%20Richard%20Jozsa%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20efficacy%20of%20machine%20learning%20models%20in%20network%0Aanomaly%20detection%20through%20the%20critical%20lens%20of%20partial%20versus%20complete%20flow%0Ainformation.%20We%20systematically%20evaluate%20how%20models%20perform%20under%20varying%0Atraining%20and%20testing%20conditions%2C%20quantifying%20the%20performance%20impact%20when%0Adealing%20with%20incomplete%20data%20typical%20in%20real-time%20environments.%20Our%20findings%0Ademonstrate%20a%20significant%20performance%20difference%2C%20with%20precision%20and%20recall%0Adropping%20by%20up%20to%2030%25%20under%20certain%20conditions%20when%20models%20trained%20on%20complete%0Aflows%20are%20tested%20against%20partial%20flows.%20Conversely%2C%20models%20trained%20and%20tested%0Aon%20consistently%20complete%20or%20partial%20datasets%20maintain%20robustness.%20The%20study%0Areveals%20that%20a%20minimum%20of%207%20packets%20in%20the%20test%20set%20is%20required%20for%20maintaining%0Areliable%20detection%20rates%2C%20providing%20valuable%20insights%20for%20real-time%20detection%0Astrategies.%20These%20results%20offer%20important%20guidance%20for%20deploying%20machine%0Alearning%20models%20in%20operational%20network%20security%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02856v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarly-Stage%2520Anomaly%2520Detection%253A%2520A%2520Study%2520of%2520Model%2520Performance%2520on%2520Complete%250A%2520%2520vs.%2520Partial%2520Flows%26entry.906535625%3DAdrian%2520Pekar%2520and%2520Richard%2520Jozsa%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520efficacy%2520of%2520machine%2520learning%2520models%2520in%2520network%250Aanomaly%2520detection%2520through%2520the%2520critical%2520lens%2520of%2520partial%2520versus%2520complete%2520flow%250Ainformation.%2520We%2520systematically%2520evaluate%2520how%2520models%2520perform%2520under%2520varying%250Atraining%2520and%2520testing%2520conditions%252C%2520quantifying%2520the%2520performance%2520impact%2520when%250Adealing%2520with%2520incomplete%2520data%2520typical%2520in%2520real-time%2520environments.%2520Our%2520findings%250Ademonstrate%2520a%2520significant%2520performance%2520difference%252C%2520with%2520precision%2520and%2520recall%250Adropping%2520by%2520up%2520to%252030%2525%2520under%2520certain%2520conditions%2520when%2520models%2520trained%2520on%2520complete%250Aflows%2520are%2520tested%2520against%2520partial%2520flows.%2520Conversely%252C%2520models%2520trained%2520and%2520tested%250Aon%2520consistently%2520complete%2520or%2520partial%2520datasets%2520maintain%2520robustness.%2520The%2520study%250Areveals%2520that%2520a%2520minimum%2520of%25207%2520packets%2520in%2520the%2520test%2520set%2520is%2520required%2520for%2520maintaining%250Areliable%2520detection%2520rates%252C%2520providing%2520valuable%2520insights%2520for%2520real-time%2520detection%250Astrategies.%2520These%2520results%2520offer%2520important%2520guidance%2520for%2520deploying%2520machine%250Alearning%2520models%2520in%2520operational%2520network%2520security%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02856v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Early-Stage%20Anomaly%20Detection%3A%20A%20Study%20of%20Model%20Performance%20on%20Complete%0A%20%20vs.%20Partial%20Flows&entry.906535625=Adrian%20Pekar%20and%20Richard%20Jozsa&entry.1292438233=%20%20This%20study%20investigates%20the%20efficacy%20of%20machine%20learning%20models%20in%20network%0Aanomaly%20detection%20through%20the%20critical%20lens%20of%20partial%20versus%20complete%20flow%0Ainformation.%20We%20systematically%20evaluate%20how%20models%20perform%20under%20varying%0Atraining%20and%20testing%20conditions%2C%20quantifying%20the%20performance%20impact%20when%0Adealing%20with%20incomplete%20data%20typical%20in%20real-time%20environments.%20Our%20findings%0Ademonstrate%20a%20significant%20performance%20difference%2C%20with%20precision%20and%20recall%0Adropping%20by%20up%20to%2030%25%20under%20certain%20conditions%20when%20models%20trained%20on%20complete%0Aflows%20are%20tested%20against%20partial%20flows.%20Conversely%2C%20models%20trained%20and%20tested%0Aon%20consistently%20complete%20or%20partial%20datasets%20maintain%20robustness.%20The%20study%0Areveals%20that%20a%20minimum%20of%207%20packets%20in%20the%20test%20set%20is%20required%20for%20maintaining%0Areliable%20detection%20rates%2C%20providing%20valuable%20insights%20for%20real-time%20detection%0Astrategies.%20These%20results%20offer%20important%20guidance%20for%20deploying%20machine%0Alearning%20models%20in%20operational%20network%20security%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02856v2&entry.124074799=Read"},
{"title": "The NavINST Dataset for Multi-Sensor Autonomous Navigation", "author": "Paulo Ricardo Marques de Araujo and Eslam Mounier and Qamar Bader and Emma Dawson and Shaza I. Kaoud Abdelaziz and Ahmed Zekry and Mohamed Elhabiby and Aboelmagd Noureldin", "abstract": "  The NavINST Laboratory has developed a comprehensive multisensory dataset\nfrom various road-test trajectories in urban environments, featuring diverse\nlighting conditions, including indoor garage scenarios with dense 3D maps. This\ndataset includes multiple commercial-grade IMUs and a high-end tactical-grade\nIMU. Additionally, it contains a wide array of perception-based sensors, such\nas a solid-state LiDAR - making it one of the first datasets to do so - a\nmechanical LiDAR, four electronically scanning RADARs, a monocular camera, and\ntwo stereo cameras. The dataset also includes forward speed measurements\nderived from the vehicle's odometer, along with accurately post-processed\nhigh-end GNSS/IMU data, providing precise ground truth positioning and\nnavigation information. The NavINST dataset is designed to support advanced\nresearch in high-precision positioning, navigation, mapping, computer vision,\nand multisensory fusion. It offers rich, multi-sensor data ideal for developing\nand validating robust algorithms for autonomous vehicles. Finally, it is fully\nintegrated with the ROS, ensuring ease of use and accessibility for the\nresearch community. The complete dataset and development tools are available at\nhttps://navinst.github.io.\n", "link": "http://arxiv.org/abs/2502.13863v1", "date": "2025-02-19", "relevancy": 1.5832, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5394}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5144}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20NavINST%20Dataset%20for%20Multi-Sensor%20Autonomous%20Navigation&body=Title%3A%20The%20NavINST%20Dataset%20for%20Multi-Sensor%20Autonomous%20Navigation%0AAuthor%3A%20Paulo%20Ricardo%20Marques%20de%20Araujo%20and%20Eslam%20Mounier%20and%20Qamar%20Bader%20and%20Emma%20Dawson%20and%20Shaza%20I.%20Kaoud%20Abdelaziz%20and%20Ahmed%20Zekry%20and%20Mohamed%20Elhabiby%20and%20Aboelmagd%20Noureldin%0AAbstract%3A%20%20%20The%20NavINST%20Laboratory%20has%20developed%20a%20comprehensive%20multisensory%20dataset%0Afrom%20various%20road-test%20trajectories%20in%20urban%20environments%2C%20featuring%20diverse%0Alighting%20conditions%2C%20including%20indoor%20garage%20scenarios%20with%20dense%203D%20maps.%20This%0Adataset%20includes%20multiple%20commercial-grade%20IMUs%20and%20a%20high-end%20tactical-grade%0AIMU.%20Additionally%2C%20it%20contains%20a%20wide%20array%20of%20perception-based%20sensors%2C%20such%0Aas%20a%20solid-state%20LiDAR%20-%20making%20it%20one%20of%20the%20first%20datasets%20to%20do%20so%20-%20a%0Amechanical%20LiDAR%2C%20four%20electronically%20scanning%20RADARs%2C%20a%20monocular%20camera%2C%20and%0Atwo%20stereo%20cameras.%20The%20dataset%20also%20includes%20forward%20speed%20measurements%0Aderived%20from%20the%20vehicle%27s%20odometer%2C%20along%20with%20accurately%20post-processed%0Ahigh-end%20GNSS/IMU%20data%2C%20providing%20precise%20ground%20truth%20positioning%20and%0Anavigation%20information.%20The%20NavINST%20dataset%20is%20designed%20to%20support%20advanced%0Aresearch%20in%20high-precision%20positioning%2C%20navigation%2C%20mapping%2C%20computer%20vision%2C%0Aand%20multisensory%20fusion.%20It%20offers%20rich%2C%20multi-sensor%20data%20ideal%20for%20developing%0Aand%20validating%20robust%20algorithms%20for%20autonomous%20vehicles.%20Finally%2C%20it%20is%20fully%0Aintegrated%20with%20the%20ROS%2C%20ensuring%20ease%20of%20use%20and%20accessibility%20for%20the%0Aresearch%20community.%20The%20complete%20dataset%20and%20development%20tools%20are%20available%20at%0Ahttps%3A//navinst.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520NavINST%2520Dataset%2520for%2520Multi-Sensor%2520Autonomous%2520Navigation%26entry.906535625%3DPaulo%2520Ricardo%2520Marques%2520de%2520Araujo%2520and%2520Eslam%2520Mounier%2520and%2520Qamar%2520Bader%2520and%2520Emma%2520Dawson%2520and%2520Shaza%2520I.%2520Kaoud%2520Abdelaziz%2520and%2520Ahmed%2520Zekry%2520and%2520Mohamed%2520Elhabiby%2520and%2520Aboelmagd%2520Noureldin%26entry.1292438233%3D%2520%2520The%2520NavINST%2520Laboratory%2520has%2520developed%2520a%2520comprehensive%2520multisensory%2520dataset%250Afrom%2520various%2520road-test%2520trajectories%2520in%2520urban%2520environments%252C%2520featuring%2520diverse%250Alighting%2520conditions%252C%2520including%2520indoor%2520garage%2520scenarios%2520with%2520dense%25203D%2520maps.%2520This%250Adataset%2520includes%2520multiple%2520commercial-grade%2520IMUs%2520and%2520a%2520high-end%2520tactical-grade%250AIMU.%2520Additionally%252C%2520it%2520contains%2520a%2520wide%2520array%2520of%2520perception-based%2520sensors%252C%2520such%250Aas%2520a%2520solid-state%2520LiDAR%2520-%2520making%2520it%2520one%2520of%2520the%2520first%2520datasets%2520to%2520do%2520so%2520-%2520a%250Amechanical%2520LiDAR%252C%2520four%2520electronically%2520scanning%2520RADARs%252C%2520a%2520monocular%2520camera%252C%2520and%250Atwo%2520stereo%2520cameras.%2520The%2520dataset%2520also%2520includes%2520forward%2520speed%2520measurements%250Aderived%2520from%2520the%2520vehicle%2527s%2520odometer%252C%2520along%2520with%2520accurately%2520post-processed%250Ahigh-end%2520GNSS/IMU%2520data%252C%2520providing%2520precise%2520ground%2520truth%2520positioning%2520and%250Anavigation%2520information.%2520The%2520NavINST%2520dataset%2520is%2520designed%2520to%2520support%2520advanced%250Aresearch%2520in%2520high-precision%2520positioning%252C%2520navigation%252C%2520mapping%252C%2520computer%2520vision%252C%250Aand%2520multisensory%2520fusion.%2520It%2520offers%2520rich%252C%2520multi-sensor%2520data%2520ideal%2520for%2520developing%250Aand%2520validating%2520robust%2520algorithms%2520for%2520autonomous%2520vehicles.%2520Finally%252C%2520it%2520is%2520fully%250Aintegrated%2520with%2520the%2520ROS%252C%2520ensuring%2520ease%2520of%2520use%2520and%2520accessibility%2520for%2520the%250Aresearch%2520community.%2520The%2520complete%2520dataset%2520and%2520development%2520tools%2520are%2520available%2520at%250Ahttps%253A//navinst.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20NavINST%20Dataset%20for%20Multi-Sensor%20Autonomous%20Navigation&entry.906535625=Paulo%20Ricardo%20Marques%20de%20Araujo%20and%20Eslam%20Mounier%20and%20Qamar%20Bader%20and%20Emma%20Dawson%20and%20Shaza%20I.%20Kaoud%20Abdelaziz%20and%20Ahmed%20Zekry%20and%20Mohamed%20Elhabiby%20and%20Aboelmagd%20Noureldin&entry.1292438233=%20%20The%20NavINST%20Laboratory%20has%20developed%20a%20comprehensive%20multisensory%20dataset%0Afrom%20various%20road-test%20trajectories%20in%20urban%20environments%2C%20featuring%20diverse%0Alighting%20conditions%2C%20including%20indoor%20garage%20scenarios%20with%20dense%203D%20maps.%20This%0Adataset%20includes%20multiple%20commercial-grade%20IMUs%20and%20a%20high-end%20tactical-grade%0AIMU.%20Additionally%2C%20it%20contains%20a%20wide%20array%20of%20perception-based%20sensors%2C%20such%0Aas%20a%20solid-state%20LiDAR%20-%20making%20it%20one%20of%20the%20first%20datasets%20to%20do%20so%20-%20a%0Amechanical%20LiDAR%2C%20four%20electronically%20scanning%20RADARs%2C%20a%20monocular%20camera%2C%20and%0Atwo%20stereo%20cameras.%20The%20dataset%20also%20includes%20forward%20speed%20measurements%0Aderived%20from%20the%20vehicle%27s%20odometer%2C%20along%20with%20accurately%20post-processed%0Ahigh-end%20GNSS/IMU%20data%2C%20providing%20precise%20ground%20truth%20positioning%20and%0Anavigation%20information.%20The%20NavINST%20dataset%20is%20designed%20to%20support%20advanced%0Aresearch%20in%20high-precision%20positioning%2C%20navigation%2C%20mapping%2C%20computer%20vision%2C%0Aand%20multisensory%20fusion.%20It%20offers%20rich%2C%20multi-sensor%20data%20ideal%20for%20developing%0Aand%20validating%20robust%20algorithms%20for%20autonomous%20vehicles.%20Finally%2C%20it%20is%20fully%0Aintegrated%20with%20the%20ROS%2C%20ensuring%20ease%20of%20use%20and%20accessibility%20for%20the%0Aresearch%20community.%20The%20complete%20dataset%20and%20development%20tools%20are%20available%20at%0Ahttps%3A//navinst.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13863v1&entry.124074799=Read"},
{"title": "Bridging Adaptivity and Safety: Learning Agile Collision-Free Locomotion\n  Across Varied Physics", "author": "Yichao Zhong and Chong Zhang and Tairan He and Guanya Shi", "abstract": "  Real-world legged locomotion systems often need to reconcile agility and\nsafety for different scenarios. Moreover, the underlying dynamics are often\nunknown and time-variant (e.g., payload, friction). In this paper, we introduce\nBAS (Bridging Adaptivity and Safety), which builds upon the pipeline of prior\nwork Agile But Safe (ABS)(He et al.) and is designed to provide adaptive safety\neven in dynamic environments with uncertainties. BAS involves an agile policy\nto avoid obstacles rapidly and a recovery policy to prevent collisions, a\nphysical parameter estimator that is concurrently trained with agile policy,\nand a learned control-theoretic RA (reach-avoid) value network that governs the\npolicy switch. Also, the agile policy and RA network are both conditioned on\nphysical parameters to make them adaptive. To mitigate the distribution shift\nissue, we further introduce an on-policy fine-tuning phase for the estimator to\nenhance its robustness and accuracy. The simulation results show that BAS\nachieves 50% better safety than baselines in dynamic environments while\nmaintaining a higher speed on average. In real-world experiments, BAS shows its\ncapability in complex environments with unknown physics (e.g., slippery floors\nwith unknown frictions, unknown payloads up to 8kg), while baselines lack\nadaptivity, leading to collisions or. degraded agility. As a result, BAS\nachieves a 19.8% increase in speed and gets a 2.36 times lower collision rate\nthan ABS in the real world. Videos: https://adaptive-safe-locomotion.github.io.\n", "link": "http://arxiv.org/abs/2501.04276v3", "date": "2025-02-19", "relevancy": 1.6313, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6293}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5262}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Adaptivity%20and%20Safety%3A%20Learning%20Agile%20Collision-Free%20Locomotion%0A%20%20Across%20Varied%20Physics&body=Title%3A%20Bridging%20Adaptivity%20and%20Safety%3A%20Learning%20Agile%20Collision-Free%20Locomotion%0A%20%20Across%20Varied%20Physics%0AAuthor%3A%20Yichao%20Zhong%20and%20Chong%20Zhang%20and%20Tairan%20He%20and%20Guanya%20Shi%0AAbstract%3A%20%20%20Real-world%20legged%20locomotion%20systems%20often%20need%20to%20reconcile%20agility%20and%0Asafety%20for%20different%20scenarios.%20Moreover%2C%20the%20underlying%20dynamics%20are%20often%0Aunknown%20and%20time-variant%20%28e.g.%2C%20payload%2C%20friction%29.%20In%20this%20paper%2C%20we%20introduce%0ABAS%20%28Bridging%20Adaptivity%20and%20Safety%29%2C%20which%20builds%20upon%20the%20pipeline%20of%20prior%0Awork%20Agile%20But%20Safe%20%28ABS%29%28He%20et%20al.%29%20and%20is%20designed%20to%20provide%20adaptive%20safety%0Aeven%20in%20dynamic%20environments%20with%20uncertainties.%20BAS%20involves%20an%20agile%20policy%0Ato%20avoid%20obstacles%20rapidly%20and%20a%20recovery%20policy%20to%20prevent%20collisions%2C%20a%0Aphysical%20parameter%20estimator%20that%20is%20concurrently%20trained%20with%20agile%20policy%2C%0Aand%20a%20learned%20control-theoretic%20RA%20%28reach-avoid%29%20value%20network%20that%20governs%20the%0Apolicy%20switch.%20Also%2C%20the%20agile%20policy%20and%20RA%20network%20are%20both%20conditioned%20on%0Aphysical%20parameters%20to%20make%20them%20adaptive.%20To%20mitigate%20the%20distribution%20shift%0Aissue%2C%20we%20further%20introduce%20an%20on-policy%20fine-tuning%20phase%20for%20the%20estimator%20to%0Aenhance%20its%20robustness%20and%20accuracy.%20The%20simulation%20results%20show%20that%20BAS%0Aachieves%2050%25%20better%20safety%20than%20baselines%20in%20dynamic%20environments%20while%0Amaintaining%20a%20higher%20speed%20on%20average.%20In%20real-world%20experiments%2C%20BAS%20shows%20its%0Acapability%20in%20complex%20environments%20with%20unknown%20physics%20%28e.g.%2C%20slippery%20floors%0Awith%20unknown%20frictions%2C%20unknown%20payloads%20up%20to%208kg%29%2C%20while%20baselines%20lack%0Aadaptivity%2C%20leading%20to%20collisions%20or.%20degraded%20agility.%20As%20a%20result%2C%20BAS%0Aachieves%20a%2019.8%25%20increase%20in%20speed%20and%20gets%20a%202.36%20times%20lower%20collision%20rate%0Athan%20ABS%20in%20the%20real%20world.%20Videos%3A%20https%3A//adaptive-safe-locomotion.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04276v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Adaptivity%2520and%2520Safety%253A%2520Learning%2520Agile%2520Collision-Free%2520Locomotion%250A%2520%2520Across%2520Varied%2520Physics%26entry.906535625%3DYichao%2520Zhong%2520and%2520Chong%2520Zhang%2520and%2520Tairan%2520He%2520and%2520Guanya%2520Shi%26entry.1292438233%3D%2520%2520Real-world%2520legged%2520locomotion%2520systems%2520often%2520need%2520to%2520reconcile%2520agility%2520and%250Asafety%2520for%2520different%2520scenarios.%2520Moreover%252C%2520the%2520underlying%2520dynamics%2520are%2520often%250Aunknown%2520and%2520time-variant%2520%2528e.g.%252C%2520payload%252C%2520friction%2529.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ABAS%2520%2528Bridging%2520Adaptivity%2520and%2520Safety%2529%252C%2520which%2520builds%2520upon%2520the%2520pipeline%2520of%2520prior%250Awork%2520Agile%2520But%2520Safe%2520%2528ABS%2529%2528He%2520et%2520al.%2529%2520and%2520is%2520designed%2520to%2520provide%2520adaptive%2520safety%250Aeven%2520in%2520dynamic%2520environments%2520with%2520uncertainties.%2520BAS%2520involves%2520an%2520agile%2520policy%250Ato%2520avoid%2520obstacles%2520rapidly%2520and%2520a%2520recovery%2520policy%2520to%2520prevent%2520collisions%252C%2520a%250Aphysical%2520parameter%2520estimator%2520that%2520is%2520concurrently%2520trained%2520with%2520agile%2520policy%252C%250Aand%2520a%2520learned%2520control-theoretic%2520RA%2520%2528reach-avoid%2529%2520value%2520network%2520that%2520governs%2520the%250Apolicy%2520switch.%2520Also%252C%2520the%2520agile%2520policy%2520and%2520RA%2520network%2520are%2520both%2520conditioned%2520on%250Aphysical%2520parameters%2520to%2520make%2520them%2520adaptive.%2520To%2520mitigate%2520the%2520distribution%2520shift%250Aissue%252C%2520we%2520further%2520introduce%2520an%2520on-policy%2520fine-tuning%2520phase%2520for%2520the%2520estimator%2520to%250Aenhance%2520its%2520robustness%2520and%2520accuracy.%2520The%2520simulation%2520results%2520show%2520that%2520BAS%250Aachieves%252050%2525%2520better%2520safety%2520than%2520baselines%2520in%2520dynamic%2520environments%2520while%250Amaintaining%2520a%2520higher%2520speed%2520on%2520average.%2520In%2520real-world%2520experiments%252C%2520BAS%2520shows%2520its%250Acapability%2520in%2520complex%2520environments%2520with%2520unknown%2520physics%2520%2528e.g.%252C%2520slippery%2520floors%250Awith%2520unknown%2520frictions%252C%2520unknown%2520payloads%2520up%2520to%25208kg%2529%252C%2520while%2520baselines%2520lack%250Aadaptivity%252C%2520leading%2520to%2520collisions%2520or.%2520degraded%2520agility.%2520As%2520a%2520result%252C%2520BAS%250Aachieves%2520a%252019.8%2525%2520increase%2520in%2520speed%2520and%2520gets%2520a%25202.36%2520times%2520lower%2520collision%2520rate%250Athan%2520ABS%2520in%2520the%2520real%2520world.%2520Videos%253A%2520https%253A//adaptive-safe-locomotion.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04276v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Adaptivity%20and%20Safety%3A%20Learning%20Agile%20Collision-Free%20Locomotion%0A%20%20Across%20Varied%20Physics&entry.906535625=Yichao%20Zhong%20and%20Chong%20Zhang%20and%20Tairan%20He%20and%20Guanya%20Shi&entry.1292438233=%20%20Real-world%20legged%20locomotion%20systems%20often%20need%20to%20reconcile%20agility%20and%0Asafety%20for%20different%20scenarios.%20Moreover%2C%20the%20underlying%20dynamics%20are%20often%0Aunknown%20and%20time-variant%20%28e.g.%2C%20payload%2C%20friction%29.%20In%20this%20paper%2C%20we%20introduce%0ABAS%20%28Bridging%20Adaptivity%20and%20Safety%29%2C%20which%20builds%20upon%20the%20pipeline%20of%20prior%0Awork%20Agile%20But%20Safe%20%28ABS%29%28He%20et%20al.%29%20and%20is%20designed%20to%20provide%20adaptive%20safety%0Aeven%20in%20dynamic%20environments%20with%20uncertainties.%20BAS%20involves%20an%20agile%20policy%0Ato%20avoid%20obstacles%20rapidly%20and%20a%20recovery%20policy%20to%20prevent%20collisions%2C%20a%0Aphysical%20parameter%20estimator%20that%20is%20concurrently%20trained%20with%20agile%20policy%2C%0Aand%20a%20learned%20control-theoretic%20RA%20%28reach-avoid%29%20value%20network%20that%20governs%20the%0Apolicy%20switch.%20Also%2C%20the%20agile%20policy%20and%20RA%20network%20are%20both%20conditioned%20on%0Aphysical%20parameters%20to%20make%20them%20adaptive.%20To%20mitigate%20the%20distribution%20shift%0Aissue%2C%20we%20further%20introduce%20an%20on-policy%20fine-tuning%20phase%20for%20the%20estimator%20to%0Aenhance%20its%20robustness%20and%20accuracy.%20The%20simulation%20results%20show%20that%20BAS%0Aachieves%2050%25%20better%20safety%20than%20baselines%20in%20dynamic%20environments%20while%0Amaintaining%20a%20higher%20speed%20on%20average.%20In%20real-world%20experiments%2C%20BAS%20shows%20its%0Acapability%20in%20complex%20environments%20with%20unknown%20physics%20%28e.g.%2C%20slippery%20floors%0Awith%20unknown%20frictions%2C%20unknown%20payloads%20up%20to%208kg%29%2C%20while%20baselines%20lack%0Aadaptivity%2C%20leading%20to%20collisions%20or.%20degraded%20agility.%20As%20a%20result%2C%20BAS%0Aachieves%20a%2019.8%25%20increase%20in%20speed%20and%20gets%20a%202.36%20times%20lower%20collision%20rate%0Athan%20ABS%20in%20the%20real%20world.%20Videos%3A%20https%3A//adaptive-safe-locomotion.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04276v3&entry.124074799=Read"},
{"title": "ACROSS: A Deformation-Based Cross-Modal Representation for Robotic\n  Tactile Perception", "author": "Wadhah Zai El Amri and Malte Kuhlmann and Nicol\u00e1s Navarro-Guerrero", "abstract": "  Tactile perception is essential for human interaction with the environment\nand is becoming increasingly crucial in robotics. Tactile sensors like the\nBioTac mimic human fingertips and provide detailed interaction data. Despite\nits utility in applications like slip detection and object identification, this\nsensor is now deprecated, making many valuable datasets obsolete. However,\nrecreating similar datasets with newer sensor technologies is both tedious and\ntime-consuming. Therefore, adapting these existing datasets for use with new\nsetups and modalities is crucial. In response, we introduce ACROSS, a novel\nframework for translating data between tactile sensors by exploiting sensor\ndeformation information. We demonstrate the approach by translating BioTac\nsignals into the DIGIT sensor. Our framework consists of first converting the\ninput signals into 3D deformation meshes. We then transition from the 3D\ndeformation mesh of one sensor to the mesh of another, and finally convert the\ngenerated 3D deformation mesh into the corresponding output space. We\ndemonstrate our approach to the most challenging problem of going from a\nlow-dimensional tactile representation to a high-dimensional one. In\nparticular, we transfer the tactile signals of a BioTac sensor to DIGIT tactile\nimages. Our approach enables the continued use of valuable datasets and data\nexchange between groups with different setups.\n", "link": "http://arxiv.org/abs/2411.08533v2", "date": "2025-02-19", "relevancy": 1.7087, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5726}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5708}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ACROSS%3A%20A%20Deformation-Based%20Cross-Modal%20Representation%20for%20Robotic%0A%20%20Tactile%20Perception&body=Title%3A%20ACROSS%3A%20A%20Deformation-Based%20Cross-Modal%20Representation%20for%20Robotic%0A%20%20Tactile%20Perception%0AAuthor%3A%20Wadhah%20Zai%20El%20Amri%20and%20Malte%20Kuhlmann%20and%20Nicol%C3%A1s%20Navarro-Guerrero%0AAbstract%3A%20%20%20Tactile%20perception%20is%20essential%20for%20human%20interaction%20with%20the%20environment%0Aand%20is%20becoming%20increasingly%20crucial%20in%20robotics.%20Tactile%20sensors%20like%20the%0ABioTac%20mimic%20human%20fingertips%20and%20provide%20detailed%20interaction%20data.%20Despite%0Aits%20utility%20in%20applications%20like%20slip%20detection%20and%20object%20identification%2C%20this%0Asensor%20is%20now%20deprecated%2C%20making%20many%20valuable%20datasets%20obsolete.%20However%2C%0Arecreating%20similar%20datasets%20with%20newer%20sensor%20technologies%20is%20both%20tedious%20and%0Atime-consuming.%20Therefore%2C%20adapting%20these%20existing%20datasets%20for%20use%20with%20new%0Asetups%20and%20modalities%20is%20crucial.%20In%20response%2C%20we%20introduce%20ACROSS%2C%20a%20novel%0Aframework%20for%20translating%20data%20between%20tactile%20sensors%20by%20exploiting%20sensor%0Adeformation%20information.%20We%20demonstrate%20the%20approach%20by%20translating%20BioTac%0Asignals%20into%20the%20DIGIT%20sensor.%20Our%20framework%20consists%20of%20first%20converting%20the%0Ainput%20signals%20into%203D%20deformation%20meshes.%20We%20then%20transition%20from%20the%203D%0Adeformation%20mesh%20of%20one%20sensor%20to%20the%20mesh%20of%20another%2C%20and%20finally%20convert%20the%0Agenerated%203D%20deformation%20mesh%20into%20the%20corresponding%20output%20space.%20We%0Ademonstrate%20our%20approach%20to%20the%20most%20challenging%20problem%20of%20going%20from%20a%0Alow-dimensional%20tactile%20representation%20to%20a%20high-dimensional%20one.%20In%0Aparticular%2C%20we%20transfer%20the%20tactile%20signals%20of%20a%20BioTac%20sensor%20to%20DIGIT%20tactile%0Aimages.%20Our%20approach%20enables%20the%20continued%20use%20of%20valuable%20datasets%20and%20data%0Aexchange%20between%20groups%20with%20different%20setups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08533v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DACROSS%253A%2520A%2520Deformation-Based%2520Cross-Modal%2520Representation%2520for%2520Robotic%250A%2520%2520Tactile%2520Perception%26entry.906535625%3DWadhah%2520Zai%2520El%2520Amri%2520and%2520Malte%2520Kuhlmann%2520and%2520Nicol%25C3%25A1s%2520Navarro-Guerrero%26entry.1292438233%3D%2520%2520Tactile%2520perception%2520is%2520essential%2520for%2520human%2520interaction%2520with%2520the%2520environment%250Aand%2520is%2520becoming%2520increasingly%2520crucial%2520in%2520robotics.%2520Tactile%2520sensors%2520like%2520the%250ABioTac%2520mimic%2520human%2520fingertips%2520and%2520provide%2520detailed%2520interaction%2520data.%2520Despite%250Aits%2520utility%2520in%2520applications%2520like%2520slip%2520detection%2520and%2520object%2520identification%252C%2520this%250Asensor%2520is%2520now%2520deprecated%252C%2520making%2520many%2520valuable%2520datasets%2520obsolete.%2520However%252C%250Arecreating%2520similar%2520datasets%2520with%2520newer%2520sensor%2520technologies%2520is%2520both%2520tedious%2520and%250Atime-consuming.%2520Therefore%252C%2520adapting%2520these%2520existing%2520datasets%2520for%2520use%2520with%2520new%250Asetups%2520and%2520modalities%2520is%2520crucial.%2520In%2520response%252C%2520we%2520introduce%2520ACROSS%252C%2520a%2520novel%250Aframework%2520for%2520translating%2520data%2520between%2520tactile%2520sensors%2520by%2520exploiting%2520sensor%250Adeformation%2520information.%2520We%2520demonstrate%2520the%2520approach%2520by%2520translating%2520BioTac%250Asignals%2520into%2520the%2520DIGIT%2520sensor.%2520Our%2520framework%2520consists%2520of%2520first%2520converting%2520the%250Ainput%2520signals%2520into%25203D%2520deformation%2520meshes.%2520We%2520then%2520transition%2520from%2520the%25203D%250Adeformation%2520mesh%2520of%2520one%2520sensor%2520to%2520the%2520mesh%2520of%2520another%252C%2520and%2520finally%2520convert%2520the%250Agenerated%25203D%2520deformation%2520mesh%2520into%2520the%2520corresponding%2520output%2520space.%2520We%250Ademonstrate%2520our%2520approach%2520to%2520the%2520most%2520challenging%2520problem%2520of%2520going%2520from%2520a%250Alow-dimensional%2520tactile%2520representation%2520to%2520a%2520high-dimensional%2520one.%2520In%250Aparticular%252C%2520we%2520transfer%2520the%2520tactile%2520signals%2520of%2520a%2520BioTac%2520sensor%2520to%2520DIGIT%2520tactile%250Aimages.%2520Our%2520approach%2520enables%2520the%2520continued%2520use%2520of%2520valuable%2520datasets%2520and%2520data%250Aexchange%2520between%2520groups%2520with%2520different%2520setups.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08533v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACROSS%3A%20A%20Deformation-Based%20Cross-Modal%20Representation%20for%20Robotic%0A%20%20Tactile%20Perception&entry.906535625=Wadhah%20Zai%20El%20Amri%20and%20Malte%20Kuhlmann%20and%20Nicol%C3%A1s%20Navarro-Guerrero&entry.1292438233=%20%20Tactile%20perception%20is%20essential%20for%20human%20interaction%20with%20the%20environment%0Aand%20is%20becoming%20increasingly%20crucial%20in%20robotics.%20Tactile%20sensors%20like%20the%0ABioTac%20mimic%20human%20fingertips%20and%20provide%20detailed%20interaction%20data.%20Despite%0Aits%20utility%20in%20applications%20like%20slip%20detection%20and%20object%20identification%2C%20this%0Asensor%20is%20now%20deprecated%2C%20making%20many%20valuable%20datasets%20obsolete.%20However%2C%0Arecreating%20similar%20datasets%20with%20newer%20sensor%20technologies%20is%20both%20tedious%20and%0Atime-consuming.%20Therefore%2C%20adapting%20these%20existing%20datasets%20for%20use%20with%20new%0Asetups%20and%20modalities%20is%20crucial.%20In%20response%2C%20we%20introduce%20ACROSS%2C%20a%20novel%0Aframework%20for%20translating%20data%20between%20tactile%20sensors%20by%20exploiting%20sensor%0Adeformation%20information.%20We%20demonstrate%20the%20approach%20by%20translating%20BioTac%0Asignals%20into%20the%20DIGIT%20sensor.%20Our%20framework%20consists%20of%20first%20converting%20the%0Ainput%20signals%20into%203D%20deformation%20meshes.%20We%20then%20transition%20from%20the%203D%0Adeformation%20mesh%20of%20one%20sensor%20to%20the%20mesh%20of%20another%2C%20and%20finally%20convert%20the%0Agenerated%203D%20deformation%20mesh%20into%20the%20corresponding%20output%20space.%20We%0Ademonstrate%20our%20approach%20to%20the%20most%20challenging%20problem%20of%20going%20from%20a%0Alow-dimensional%20tactile%20representation%20to%20a%20high-dimensional%20one.%20In%0Aparticular%2C%20we%20transfer%20the%20tactile%20signals%20of%20a%20BioTac%20sensor%20to%20DIGIT%20tactile%0Aimages.%20Our%20approach%20enables%20the%20continued%20use%20of%20valuable%20datasets%20and%20data%0Aexchange%20between%20groups%20with%20different%20setups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08533v2&entry.124074799=Read"},
{"title": "Identifying metric structures of deep latent variable models", "author": "Stas Syrota and Yevgen Zainchkovskyy and Johnny Xi and Benjamin Bloem-Reddy and S\u00f8ren Hauberg", "abstract": "  Deep latent variable models learn condensed representations of data that,\nhopefully, reflect the inner workings of the studied phenomena. Unfortunately,\nthese latent representations are not statistically identifiable, meaning they\ncannot be uniquely determined. Domain experts, therefore, need to tread\ncarefully when interpreting these. Current solutions limit the lack of\nidentifiability through additional constraints on the latent variable model,\ne.g. by requiring labeled training data, or by restricting the expressivity of\nthe model. We change the goal: instead of identifying the latent variables, we\nidentify relationships between them such as meaningful distances, angles, and\nvolumes. We prove this is feasible under very mild model conditions and without\nadditional labeled data. We empirically demonstrate that our theory results in\nmore reliable latent distances, offering a principled path forward in\nextracting trustworthy conclusions from deep latent variable models.\n", "link": "http://arxiv.org/abs/2502.13757v1", "date": "2025-02-19", "relevancy": 1.4446, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4913}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4803}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20metric%20structures%20of%20deep%20latent%20variable%20models&body=Title%3A%20Identifying%20metric%20structures%20of%20deep%20latent%20variable%20models%0AAuthor%3A%20Stas%20Syrota%20and%20Yevgen%20Zainchkovskyy%20and%20Johnny%20Xi%20and%20Benjamin%20Bloem-Reddy%20and%20S%C3%B8ren%20Hauberg%0AAbstract%3A%20%20%20Deep%20latent%20variable%20models%20learn%20condensed%20representations%20of%20data%20that%2C%0Ahopefully%2C%20reflect%20the%20inner%20workings%20of%20the%20studied%20phenomena.%20Unfortunately%2C%0Athese%20latent%20representations%20are%20not%20statistically%20identifiable%2C%20meaning%20they%0Acannot%20be%20uniquely%20determined.%20Domain%20experts%2C%20therefore%2C%20need%20to%20tread%0Acarefully%20when%20interpreting%20these.%20Current%20solutions%20limit%20the%20lack%20of%0Aidentifiability%20through%20additional%20constraints%20on%20the%20latent%20variable%20model%2C%0Ae.g.%20by%20requiring%20labeled%20training%20data%2C%20or%20by%20restricting%20the%20expressivity%20of%0Athe%20model.%20We%20change%20the%20goal%3A%20instead%20of%20identifying%20the%20latent%20variables%2C%20we%0Aidentify%20relationships%20between%20them%20such%20as%20meaningful%20distances%2C%20angles%2C%20and%0Avolumes.%20We%20prove%20this%20is%20feasible%20under%20very%20mild%20model%20conditions%20and%20without%0Aadditional%20labeled%20data.%20We%20empirically%20demonstrate%20that%20our%20theory%20results%20in%0Amore%20reliable%20latent%20distances%2C%20offering%20a%20principled%20path%20forward%20in%0Aextracting%20trustworthy%20conclusions%20from%20deep%20latent%20variable%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13757v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520metric%2520structures%2520of%2520deep%2520latent%2520variable%2520models%26entry.906535625%3DStas%2520Syrota%2520and%2520Yevgen%2520Zainchkovskyy%2520and%2520Johnny%2520Xi%2520and%2520Benjamin%2520Bloem-Reddy%2520and%2520S%25C3%25B8ren%2520Hauberg%26entry.1292438233%3D%2520%2520Deep%2520latent%2520variable%2520models%2520learn%2520condensed%2520representations%2520of%2520data%2520that%252C%250Ahopefully%252C%2520reflect%2520the%2520inner%2520workings%2520of%2520the%2520studied%2520phenomena.%2520Unfortunately%252C%250Athese%2520latent%2520representations%2520are%2520not%2520statistically%2520identifiable%252C%2520meaning%2520they%250Acannot%2520be%2520uniquely%2520determined.%2520Domain%2520experts%252C%2520therefore%252C%2520need%2520to%2520tread%250Acarefully%2520when%2520interpreting%2520these.%2520Current%2520solutions%2520limit%2520the%2520lack%2520of%250Aidentifiability%2520through%2520additional%2520constraints%2520on%2520the%2520latent%2520variable%2520model%252C%250Ae.g.%2520by%2520requiring%2520labeled%2520training%2520data%252C%2520or%2520by%2520restricting%2520the%2520expressivity%2520of%250Athe%2520model.%2520We%2520change%2520the%2520goal%253A%2520instead%2520of%2520identifying%2520the%2520latent%2520variables%252C%2520we%250Aidentify%2520relationships%2520between%2520them%2520such%2520as%2520meaningful%2520distances%252C%2520angles%252C%2520and%250Avolumes.%2520We%2520prove%2520this%2520is%2520feasible%2520under%2520very%2520mild%2520model%2520conditions%2520and%2520without%250Aadditional%2520labeled%2520data.%2520We%2520empirically%2520demonstrate%2520that%2520our%2520theory%2520results%2520in%250Amore%2520reliable%2520latent%2520distances%252C%2520offering%2520a%2520principled%2520path%2520forward%2520in%250Aextracting%2520trustworthy%2520conclusions%2520from%2520deep%2520latent%2520variable%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13757v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20metric%20structures%20of%20deep%20latent%20variable%20models&entry.906535625=Stas%20Syrota%20and%20Yevgen%20Zainchkovskyy%20and%20Johnny%20Xi%20and%20Benjamin%20Bloem-Reddy%20and%20S%C3%B8ren%20Hauberg&entry.1292438233=%20%20Deep%20latent%20variable%20models%20learn%20condensed%20representations%20of%20data%20that%2C%0Ahopefully%2C%20reflect%20the%20inner%20workings%20of%20the%20studied%20phenomena.%20Unfortunately%2C%0Athese%20latent%20representations%20are%20not%20statistically%20identifiable%2C%20meaning%20they%0Acannot%20be%20uniquely%20determined.%20Domain%20experts%2C%20therefore%2C%20need%20to%20tread%0Acarefully%20when%20interpreting%20these.%20Current%20solutions%20limit%20the%20lack%20of%0Aidentifiability%20through%20additional%20constraints%20on%20the%20latent%20variable%20model%2C%0Ae.g.%20by%20requiring%20labeled%20training%20data%2C%20or%20by%20restricting%20the%20expressivity%20of%0Athe%20model.%20We%20change%20the%20goal%3A%20instead%20of%20identifying%20the%20latent%20variables%2C%20we%0Aidentify%20relationships%20between%20them%20such%20as%20meaningful%20distances%2C%20angles%2C%20and%0Avolumes.%20We%20prove%20this%20is%20feasible%20under%20very%20mild%20model%20conditions%20and%20without%0Aadditional%20labeled%20data.%20We%20empirically%20demonstrate%20that%20our%20theory%20results%20in%0Amore%20reliable%20latent%20distances%2C%20offering%20a%20principled%20path%20forward%20in%0Aextracting%20trustworthy%20conclusions%20from%20deep%20latent%20variable%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13757v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


