<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240417.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "VG4D: Vision-Language Model Goes 4D Video Recognition", "author": "Zhichao Deng and Xiangtai Li and Xia Li and Yunhai Tong and Shen Zhao and Mengyuan Liu", "abstract": "  Understanding the real world through point cloud video is a crucial aspect of\nrobotics and autonomous driving systems. However, prevailing methods for 4D\npoint cloud recognition have limitations due to sensor resolution, which leads\nto a lack of detailed information. Recent advances have shown that\nVision-Language Models (VLM) pre-trained on web-scale text-image datasets can\nlearn fine-grained visual concepts that can be transferred to various\ndownstream tasks. However, effectively integrating VLM into the domain of 4D\npoint clouds remains an unresolved problem. In this work, we propose the\nVision-Language Models Goes 4D (VG4D) framework to transfer VLM knowledge from\nvisual-text pre-trained models to a 4D point cloud network. Our approach\ninvolves aligning the 4D encoder's representation with a VLM to learn a shared\nvisual and text space from training on large-scale image-text pairs. By\ntransferring the knowledge of the VLM to the 4D encoder and combining the VLM,\nour VG4D achieves improved recognition performance. To enhance the 4D encoder,\nwe modernize the classic dynamic point cloud backbone and propose an improved\nversion of PSTNet, im-PSTNet, which can efficiently model point cloud videos.\nExperiments demonstrate that our method achieves state-of-the-art performance\nfor action recognition on both the NTU RGB+D 60 dataset and the NTU RGB+D 120\ndataset. Code is available at \\url{https://github.com/Shark0-0/VG4D}.\n", "link": "http://arxiv.org/abs/2404.11605v1", "date": "2024-04-17", "relevancy": 2.9315, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5951}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5855}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5784}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VG4D%3A%20Vision-Language%20Model%20Goes%204D%20Video%20Recognition&body=Title%3A%20VG4D%3A%20Vision-Language%20Model%20Goes%204D%20Video%20Recognition%0AAuthor%3A%20Zhichao%20Deng%20and%20Xiangtai%20Li%20and%20Xia%20Li%20and%20Yunhai%20Tong%20and%20Shen%20Zhao%20and%20Mengyuan%20Liu%0AAbstract%3A%20%20%20Understanding%20the%20real%20world%20through%20point%20cloud%20video%20is%20a%20crucial%20aspect%20of%0Arobotics%20and%20autonomous%20driving%20systems.%20However%2C%20prevailing%20methods%20for%204D%0Apoint%20cloud%20recognition%20have%20limitations%20due%20to%20sensor%20resolution%2C%20which%20leads%0Ato%20a%20lack%20of%20detailed%20information.%20Recent%20advances%20have%20shown%20that%0AVision-Language%20Models%20%28VLM%29%20pre-trained%20on%20web-scale%20text-image%20datasets%20can%0Alearn%20fine-grained%20visual%20concepts%20that%20can%20be%20transferred%20to%20various%0Adownstream%20tasks.%20However%2C%20effectively%20integrating%20VLM%20into%20the%20domain%20of%204D%0Apoint%20clouds%20remains%20an%20unresolved%20problem.%20In%20this%20work%2C%20we%20propose%20the%0AVision-Language%20Models%20Goes%204D%20%28VG4D%29%20framework%20to%20transfer%20VLM%20knowledge%20from%0Avisual-text%20pre-trained%20models%20to%20a%204D%20point%20cloud%20network.%20Our%20approach%0Ainvolves%20aligning%20the%204D%20encoder%27s%20representation%20with%20a%20VLM%20to%20learn%20a%20shared%0Avisual%20and%20text%20space%20from%20training%20on%20large-scale%20image-text%20pairs.%20By%0Atransferring%20the%20knowledge%20of%20the%20VLM%20to%20the%204D%20encoder%20and%20combining%20the%20VLM%2C%0Aour%20VG4D%20achieves%20improved%20recognition%20performance.%20To%20enhance%20the%204D%20encoder%2C%0Awe%20modernize%20the%20classic%20dynamic%20point%20cloud%20backbone%20and%20propose%20an%20improved%0Aversion%20of%20PSTNet%2C%20im-PSTNet%2C%20which%20can%20efficiently%20model%20point%20cloud%20videos.%0AExperiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%0Afor%20action%20recognition%20on%20both%20the%20NTU%20RGB%2BD%2060%20dataset%20and%20the%20NTU%20RGB%2BD%20120%0Adataset.%20Code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/Shark0-0/VG4D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11605v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VG4D%3A%20Vision-Language%20Model%20Goes%204D%20Video%20Recognition&entry.906535625=Zhichao%20Deng%20and%20Xiangtai%20Li%20and%20Xia%20Li%20and%20Yunhai%20Tong%20and%20Shen%20Zhao%20and%20Mengyuan%20Liu&entry.1292438233=%20%20Understanding%20the%20real%20world%20through%20point%20cloud%20video%20is%20a%20crucial%20aspect%20of%0Arobotics%20and%20autonomous%20driving%20systems.%20However%2C%20prevailing%20methods%20for%204D%0Apoint%20cloud%20recognition%20have%20limitations%20due%20to%20sensor%20resolution%2C%20which%20leads%0Ato%20a%20lack%20of%20detailed%20information.%20Recent%20advances%20have%20shown%20that%0AVision-Language%20Models%20%28VLM%29%20pre-trained%20on%20web-scale%20text-image%20datasets%20can%0Alearn%20fine-grained%20visual%20concepts%20that%20can%20be%20transferred%20to%20various%0Adownstream%20tasks.%20However%2C%20effectively%20integrating%20VLM%20into%20the%20domain%20of%204D%0Apoint%20clouds%20remains%20an%20unresolved%20problem.%20In%20this%20work%2C%20we%20propose%20the%0AVision-Language%20Models%20Goes%204D%20%28VG4D%29%20framework%20to%20transfer%20VLM%20knowledge%20from%0Avisual-text%20pre-trained%20models%20to%20a%204D%20point%20cloud%20network.%20Our%20approach%0Ainvolves%20aligning%20the%204D%20encoder%27s%20representation%20with%20a%20VLM%20to%20learn%20a%20shared%0Avisual%20and%20text%20space%20from%20training%20on%20large-scale%20image-text%20pairs.%20By%0Atransferring%20the%20knowledge%20of%20the%20VLM%20to%20the%204D%20encoder%20and%20combining%20the%20VLM%2C%0Aour%20VG4D%20achieves%20improved%20recognition%20performance.%20To%20enhance%20the%204D%20encoder%2C%0Awe%20modernize%20the%20classic%20dynamic%20point%20cloud%20backbone%20and%20propose%20an%20improved%0Aversion%20of%20PSTNet%2C%20im-PSTNet%2C%20which%20can%20efficiently%20model%20point%20cloud%20videos.%0AExperiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%0Afor%20action%20recognition%20on%20both%20the%20NTU%20RGB%2BD%2060%20dataset%20and%20the%20NTU%20RGB%2BD%20120%0Adataset.%20Code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/Shark0-0/VG4D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11605v1&entry.124074799=Read"},
{"title": "Leveraging Fine-Grained Information and Noise Decoupling for Remote\n  Sensing Change Detection", "author": "Qiangang Du and Jinlong Peng and Changan Wang and Xu Chen and Qingdong He and Wenbing Zhu and Mingmin Chi and Yabiao Wang and Chengjie Wang", "abstract": "  Change detection aims to identify remote sense object changes by analyzing\ndata between bitemporal image pairs. Due to the large temporal and spatial span\nof data collection in change detection image pairs, there are often a\nsignificant amount of task-specific and task-agnostic noise. Previous effort\nhas focused excessively on denoising, with this goes a great deal of loss of\nfine-grained information. In this paper, we revisit the importance of\nfine-grained features in change detection and propose a series of operations\nfor fine-grained information compensation and noise decoupling (FINO). First,\nthe context is utilized to compensate for the fine-grained information in the\nfeature space. Next, a shape-aware and a brightness-aware module are designed\nto improve the capacity for representation learning. The shape-aware module\nguides the backbone for more precise shape estimation, guiding the backbone\nnetwork in extracting object shape features. The brightness-aware module learns\na overall brightness estimation to improve the model's robustness to\ntask-agnostic noise. Finally, a task-specific noise decoupling structure is\ndesigned as a way to improve the model's ability to separate noise interference\nfrom feature similarity. With these training schemes, our proposed method\nachieves new state-of-the-art (SOTA) results in multiple change detection\nbenchmarks. The code will be made available.\n", "link": "http://arxiv.org/abs/2404.11318v1", "date": "2024-04-17", "relevancy": 2.8088, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5817}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5582}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5454}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Fine-Grained%20Information%20and%20Noise%20Decoupling%20for%20Remote%0A%20%20Sensing%20Change%20Detection&body=Title%3A%20Leveraging%20Fine-Grained%20Information%20and%20Noise%20Decoupling%20for%20Remote%0A%20%20Sensing%20Change%20Detection%0AAuthor%3A%20Qiangang%20Du%20and%20Jinlong%20Peng%20and%20Changan%20Wang%20and%20Xu%20Chen%20and%20Qingdong%20He%20and%20Wenbing%20Zhu%20and%20Mingmin%20Chi%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%0AAbstract%3A%20%20%20Change%20detection%20aims%20to%20identify%20remote%20sense%20object%20changes%20by%20analyzing%0Adata%20between%20bitemporal%20image%20pairs.%20Due%20to%20the%20large%20temporal%20and%20spatial%20span%0Aof%20data%20collection%20in%20change%20detection%20image%20pairs%2C%20there%20are%20often%20a%0Asignificant%20amount%20of%20task-specific%20and%20task-agnostic%20noise.%20Previous%20effort%0Ahas%20focused%20excessively%20on%20denoising%2C%20with%20this%20goes%20a%20great%20deal%20of%20loss%20of%0Afine-grained%20information.%20In%20this%20paper%2C%20we%20revisit%20the%20importance%20of%0Afine-grained%20features%20in%20change%20detection%20and%20propose%20a%20series%20of%20operations%0Afor%20fine-grained%20information%20compensation%20and%20noise%20decoupling%20%28FINO%29.%20First%2C%0Athe%20context%20is%20utilized%20to%20compensate%20for%20the%20fine-grained%20information%20in%20the%0Afeature%20space.%20Next%2C%20a%20shape-aware%20and%20a%20brightness-aware%20module%20are%20designed%0Ato%20improve%20the%20capacity%20for%20representation%20learning.%20The%20shape-aware%20module%0Aguides%20the%20backbone%20for%20more%20precise%20shape%20estimation%2C%20guiding%20the%20backbone%0Anetwork%20in%20extracting%20object%20shape%20features.%20The%20brightness-aware%20module%20learns%0Aa%20overall%20brightness%20estimation%20to%20improve%20the%20model%27s%20robustness%20to%0Atask-agnostic%20noise.%20Finally%2C%20a%20task-specific%20noise%20decoupling%20structure%20is%0Adesigned%20as%20a%20way%20to%20improve%20the%20model%27s%20ability%20to%20separate%20noise%20interference%0Afrom%20feature%20similarity.%20With%20these%20training%20schemes%2C%20our%20proposed%20method%0Aachieves%20new%20state-of-the-art%20%28SOTA%29%20results%20in%20multiple%20change%20detection%0Abenchmarks.%20The%20code%20will%20be%20made%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11318v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Fine-Grained%20Information%20and%20Noise%20Decoupling%20for%20Remote%0A%20%20Sensing%20Change%20Detection&entry.906535625=Qiangang%20Du%20and%20Jinlong%20Peng%20and%20Changan%20Wang%20and%20Xu%20Chen%20and%20Qingdong%20He%20and%20Wenbing%20Zhu%20and%20Mingmin%20Chi%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang&entry.1292438233=%20%20Change%20detection%20aims%20to%20identify%20remote%20sense%20object%20changes%20by%20analyzing%0Adata%20between%20bitemporal%20image%20pairs.%20Due%20to%20the%20large%20temporal%20and%20spatial%20span%0Aof%20data%20collection%20in%20change%20detection%20image%20pairs%2C%20there%20are%20often%20a%0Asignificant%20amount%20of%20task-specific%20and%20task-agnostic%20noise.%20Previous%20effort%0Ahas%20focused%20excessively%20on%20denoising%2C%20with%20this%20goes%20a%20great%20deal%20of%20loss%20of%0Afine-grained%20information.%20In%20this%20paper%2C%20we%20revisit%20the%20importance%20of%0Afine-grained%20features%20in%20change%20detection%20and%20propose%20a%20series%20of%20operations%0Afor%20fine-grained%20information%20compensation%20and%20noise%20decoupling%20%28FINO%29.%20First%2C%0Athe%20context%20is%20utilized%20to%20compensate%20for%20the%20fine-grained%20information%20in%20the%0Afeature%20space.%20Next%2C%20a%20shape-aware%20and%20a%20brightness-aware%20module%20are%20designed%0Ato%20improve%20the%20capacity%20for%20representation%20learning.%20The%20shape-aware%20module%0Aguides%20the%20backbone%20for%20more%20precise%20shape%20estimation%2C%20guiding%20the%20backbone%0Anetwork%20in%20extracting%20object%20shape%20features.%20The%20brightness-aware%20module%20learns%0Aa%20overall%20brightness%20estimation%20to%20improve%20the%20model%27s%20robustness%20to%0Atask-agnostic%20noise.%20Finally%2C%20a%20task-specific%20noise%20decoupling%20structure%20is%0Adesigned%20as%20a%20way%20to%20improve%20the%20model%27s%20ability%20to%20separate%20noise%20interference%0Afrom%20feature%20similarity.%20With%20these%20training%20schemes%2C%20our%20proposed%20method%0Aachieves%20new%20state-of-the-art%20%28SOTA%29%20results%20in%20multiple%20change%20detection%0Abenchmarks.%20The%20code%20will%20be%20made%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11318v1&entry.124074799=Read"},
{"title": "VehicleGAN: Pair-flexible Pose Guided Image Synthesis for Vehicle\n  Re-identification", "author": "Baolu Li and Ping Liu and Lan Fu and Jinlong Li and Jianwu Fang and Zhigang Xu and Hongkai Yu", "abstract": "  Vehicle Re-identification (Re-ID) has been broadly studied in the last\ndecade; however, the different camera view angle leading to confused\ndiscrimination in the feature subspace for the vehicles of various poses, is\nstill challenging for the Vehicle Re-ID models in the real world. To promote\nthe Vehicle Re-ID models, this paper proposes to synthesize a large number of\nvehicle images in the target pose, whose idea is to project the vehicles of\ndiverse poses into the unified target pose so as to enhance feature\ndiscrimination. Considering that the paired data of the same vehicles in\ndifferent traffic surveillance cameras might be not available in the real\nworld, we propose the first Pair-flexible Pose Guided Image Synthesis method\nfor Vehicle Re-ID, named as VehicleGAN in this paper, which works for both\nsupervised and unsupervised settings without the knowledge of geometric 3D\nmodels. Because of the feature distribution difference between real and\nsynthetic data, simply training a traditional metric learning based Re-ID model\nwith data-level fusion (i.e., data augmentation) is not satisfactory, therefore\nwe propose a new Joint Metric Learning (JML) via effective feature-level fusion\nfrom both real and synthetic data. Intensive experimental results on the public\nVeRi-776 and VehicleID datasets prove the accuracy and effectiveness of our\nproposed VehicleGAN and JML.\n", "link": "http://arxiv.org/abs/2311.16278v3", "date": "2024-04-17", "relevancy": 2.7626, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5711}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5506}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5359}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VehicleGAN%3A%20Pair-flexible%20Pose%20Guided%20Image%20Synthesis%20for%20Vehicle%0A%20%20Re-identification&body=Title%3A%20VehicleGAN%3A%20Pair-flexible%20Pose%20Guided%20Image%20Synthesis%20for%20Vehicle%0A%20%20Re-identification%0AAuthor%3A%20Baolu%20Li%20and%20Ping%20Liu%20and%20Lan%20Fu%20and%20Jinlong%20Li%20and%20Jianwu%20Fang%20and%20Zhigang%20Xu%20and%20Hongkai%20Yu%0AAbstract%3A%20%20%20Vehicle%20Re-identification%20%28Re-ID%29%20has%20been%20broadly%20studied%20in%20the%20last%0Adecade%3B%20however%2C%20the%20different%20camera%20view%20angle%20leading%20to%20confused%0Adiscrimination%20in%20the%20feature%20subspace%20for%20the%20vehicles%20of%20various%20poses%2C%20is%0Astill%20challenging%20for%20the%20Vehicle%20Re-ID%20models%20in%20the%20real%20world.%20To%20promote%0Athe%20Vehicle%20Re-ID%20models%2C%20this%20paper%20proposes%20to%20synthesize%20a%20large%20number%20of%0Avehicle%20images%20in%20the%20target%20pose%2C%20whose%20idea%20is%20to%20project%20the%20vehicles%20of%0Adiverse%20poses%20into%20the%20unified%20target%20pose%20so%20as%20to%20enhance%20feature%0Adiscrimination.%20Considering%20that%20the%20paired%20data%20of%20the%20same%20vehicles%20in%0Adifferent%20traffic%20surveillance%20cameras%20might%20be%20not%20available%20in%20the%20real%0Aworld%2C%20we%20propose%20the%20first%20Pair-flexible%20Pose%20Guided%20Image%20Synthesis%20method%0Afor%20Vehicle%20Re-ID%2C%20named%20as%20VehicleGAN%20in%20this%20paper%2C%20which%20works%20for%20both%0Asupervised%20and%20unsupervised%20settings%20without%20the%20knowledge%20of%20geometric%203D%0Amodels.%20Because%20of%20the%20feature%20distribution%20difference%20between%20real%20and%0Asynthetic%20data%2C%20simply%20training%20a%20traditional%20metric%20learning%20based%20Re-ID%20model%0Awith%20data-level%20fusion%20%28i.e.%2C%20data%20augmentation%29%20is%20not%20satisfactory%2C%20therefore%0Awe%20propose%20a%20new%20Joint%20Metric%20Learning%20%28JML%29%20via%20effective%20feature-level%20fusion%0Afrom%20both%20real%20and%20synthetic%20data.%20Intensive%20experimental%20results%20on%20the%20public%0AVeRi-776%20and%20VehicleID%20datasets%20prove%20the%20accuracy%20and%20effectiveness%20of%20our%0Aproposed%20VehicleGAN%20and%20JML.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16278v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VehicleGAN%3A%20Pair-flexible%20Pose%20Guided%20Image%20Synthesis%20for%20Vehicle%0A%20%20Re-identification&entry.906535625=Baolu%20Li%20and%20Ping%20Liu%20and%20Lan%20Fu%20and%20Jinlong%20Li%20and%20Jianwu%20Fang%20and%20Zhigang%20Xu%20and%20Hongkai%20Yu&entry.1292438233=%20%20Vehicle%20Re-identification%20%28Re-ID%29%20has%20been%20broadly%20studied%20in%20the%20last%0Adecade%3B%20however%2C%20the%20different%20camera%20view%20angle%20leading%20to%20confused%0Adiscrimination%20in%20the%20feature%20subspace%20for%20the%20vehicles%20of%20various%20poses%2C%20is%0Astill%20challenging%20for%20the%20Vehicle%20Re-ID%20models%20in%20the%20real%20world.%20To%20promote%0Athe%20Vehicle%20Re-ID%20models%2C%20this%20paper%20proposes%20to%20synthesize%20a%20large%20number%20of%0Avehicle%20images%20in%20the%20target%20pose%2C%20whose%20idea%20is%20to%20project%20the%20vehicles%20of%0Adiverse%20poses%20into%20the%20unified%20target%20pose%20so%20as%20to%20enhance%20feature%0Adiscrimination.%20Considering%20that%20the%20paired%20data%20of%20the%20same%20vehicles%20in%0Adifferent%20traffic%20surveillance%20cameras%20might%20be%20not%20available%20in%20the%20real%0Aworld%2C%20we%20propose%20the%20first%20Pair-flexible%20Pose%20Guided%20Image%20Synthesis%20method%0Afor%20Vehicle%20Re-ID%2C%20named%20as%20VehicleGAN%20in%20this%20paper%2C%20which%20works%20for%20both%0Asupervised%20and%20unsupervised%20settings%20without%20the%20knowledge%20of%20geometric%203D%0Amodels.%20Because%20of%20the%20feature%20distribution%20difference%20between%20real%20and%0Asynthetic%20data%2C%20simply%20training%20a%20traditional%20metric%20learning%20based%20Re-ID%20model%0Awith%20data-level%20fusion%20%28i.e.%2C%20data%20augmentation%29%20is%20not%20satisfactory%2C%20therefore%0Awe%20propose%20a%20new%20Joint%20Metric%20Learning%20%28JML%29%20via%20effective%20feature-level%20fusion%0Afrom%20both%20real%20and%20synthetic%20data.%20Intensive%20experimental%20results%20on%20the%20public%0AVeRi-776%20and%20VehicleID%20datasets%20prove%20the%20accuracy%20and%20effectiveness%20of%20our%0Aproposed%20VehicleGAN%20and%20JML.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16278v3&entry.124074799=Read"},
{"title": "Bridging the Gap: Learning Pace Synchronization for Open-World\n  Semi-Supervised Learning", "author": "Bo Ye and Kai Gan and Tong Wei and Min-Ling Zhang", "abstract": "  In open-world semi-supervised learning, a machine learning model is tasked\nwith uncovering novel categories from unlabeled data while maintaining\nperformance on seen categories from labeled data. The central challenge is the\nsubstantial learning gap between seen and novel categories, as the model learns\nthe former faster due to accurate supervisory information. Moreover, capturing\nthe semantics of unlabeled novel category samples is also challenging due to\nthe missing label information. To address the above issues, we introduce 1) the\nadaptive synchronizing marginal loss which imposes class-specific negative\nmargins to alleviate the model bias towards seen classes, and 2) the\npseudo-label contrastive clustering which exploits pseudo-labels predicted by\nthe model to group unlabeled data from the same category together in the output\nspace. Extensive experiments on benchmark datasets demonstrate that previous\napproaches may significantly hinder novel class learning, whereas our method\nstrikingly balances the learning pace between seen and novel classes, achieving\na remarkable 3% average accuracy increase on the ImageNet dataset. Importantly,\nwe find that fine-tuning the self-supervised pre-trained model significantly\nboosts the performance, which is overlooked in prior literature. Our code is\navailable at https://github.com/yebo0216best/LPS-main.\n", "link": "http://arxiv.org/abs/2309.11930v2", "date": "2024-04-17", "relevancy": 2.7308, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5933}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5327}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5125}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bridging%20the%20Gap%3A%20Learning%20Pace%20Synchronization%20for%20Open-World%0A%20%20Semi-Supervised%20Learning&body=Title%3A%20Bridging%20the%20Gap%3A%20Learning%20Pace%20Synchronization%20for%20Open-World%0A%20%20Semi-Supervised%20Learning%0AAuthor%3A%20Bo%20Ye%20and%20Kai%20Gan%20and%20Tong%20Wei%20and%20Min-Ling%20Zhang%0AAbstract%3A%20%20%20In%20open-world%20semi-supervised%20learning%2C%20a%20machine%20learning%20model%20is%20tasked%0Awith%20uncovering%20novel%20categories%20from%20unlabeled%20data%20while%20maintaining%0Aperformance%20on%20seen%20categories%20from%20labeled%20data.%20The%20central%20challenge%20is%20the%0Asubstantial%20learning%20gap%20between%20seen%20and%20novel%20categories%2C%20as%20the%20model%20learns%0Athe%20former%20faster%20due%20to%20accurate%20supervisory%20information.%20Moreover%2C%20capturing%0Athe%20semantics%20of%20unlabeled%20novel%20category%20samples%20is%20also%20challenging%20due%20to%0Athe%20missing%20label%20information.%20To%20address%20the%20above%20issues%2C%20we%20introduce%201%29%20the%0Aadaptive%20synchronizing%20marginal%20loss%20which%20imposes%20class-specific%20negative%0Amargins%20to%20alleviate%20the%20model%20bias%20towards%20seen%20classes%2C%20and%202%29%20the%0Apseudo-label%20contrastive%20clustering%20which%20exploits%20pseudo-labels%20predicted%20by%0Athe%20model%20to%20group%20unlabeled%20data%20from%20the%20same%20category%20together%20in%20the%20output%0Aspace.%20Extensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%20previous%0Aapproaches%20may%20significantly%20hinder%20novel%20class%20learning%2C%20whereas%20our%20method%0Astrikingly%20balances%20the%20learning%20pace%20between%20seen%20and%20novel%20classes%2C%20achieving%0Aa%20remarkable%203%25%20average%20accuracy%20increase%20on%20the%20ImageNet%20dataset.%20Importantly%2C%0Awe%20find%20that%20fine-tuning%20the%20self-supervised%20pre-trained%20model%20significantly%0Aboosts%20the%20performance%2C%20which%20is%20overlooked%20in%20prior%20literature.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/yebo0216best/LPS-main.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.11930v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20the%20Gap%3A%20Learning%20Pace%20Synchronization%20for%20Open-World%0A%20%20Semi-Supervised%20Learning&entry.906535625=Bo%20Ye%20and%20Kai%20Gan%20and%20Tong%20Wei%20and%20Min-Ling%20Zhang&entry.1292438233=%20%20In%20open-world%20semi-supervised%20learning%2C%20a%20machine%20learning%20model%20is%20tasked%0Awith%20uncovering%20novel%20categories%20from%20unlabeled%20data%20while%20maintaining%0Aperformance%20on%20seen%20categories%20from%20labeled%20data.%20The%20central%20challenge%20is%20the%0Asubstantial%20learning%20gap%20between%20seen%20and%20novel%20categories%2C%20as%20the%20model%20learns%0Athe%20former%20faster%20due%20to%20accurate%20supervisory%20information.%20Moreover%2C%20capturing%0Athe%20semantics%20of%20unlabeled%20novel%20category%20samples%20is%20also%20challenging%20due%20to%0Athe%20missing%20label%20information.%20To%20address%20the%20above%20issues%2C%20we%20introduce%201%29%20the%0Aadaptive%20synchronizing%20marginal%20loss%20which%20imposes%20class-specific%20negative%0Amargins%20to%20alleviate%20the%20model%20bias%20towards%20seen%20classes%2C%20and%202%29%20the%0Apseudo-label%20contrastive%20clustering%20which%20exploits%20pseudo-labels%20predicted%20by%0Athe%20model%20to%20group%20unlabeled%20data%20from%20the%20same%20category%20together%20in%20the%20output%0Aspace.%20Extensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%20previous%0Aapproaches%20may%20significantly%20hinder%20novel%20class%20learning%2C%20whereas%20our%20method%0Astrikingly%20balances%20the%20learning%20pace%20between%20seen%20and%20novel%20classes%2C%20achieving%0Aa%20remarkable%203%25%20average%20accuracy%20increase%20on%20the%20ImageNet%20dataset.%20Importantly%2C%0Awe%20find%20that%20fine-tuning%20the%20self-supervised%20pre-trained%20model%20significantly%0Aboosts%20the%20performance%2C%20which%20is%20overlooked%20in%20prior%20literature.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/yebo0216best/LPS-main.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.11930v2&entry.124074799=Read"},
{"title": "DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in\n  Multivariate Time Series", "author": "Zahra Zamanzadeh Darban and Geoffrey I. Webb and Mahsa Salehi", "abstract": "  Time series anomaly detection (TAD) faces a significant challenge due to the\nscarcity of labelled data, which hinders the development of accurate detection\nmodels. Unsupervised domain adaptation (UDA) addresses this challenge by\nleveraging a labelled dataset from a related domain to detect anomalies in a\ntarget dataset. Existing domain adaptation techniques assume that the number of\nanomalous classes does not change between the source and target domains. In\nthis paper, we propose a novel Domain Adaptation Contrastive learning for\nAnomaly Detection in multivariate time series (DACAD) model to address this\nissue by combining UDA and contrastive representation learning. DACAD's\napproach includes an anomaly injection mechanism that introduces various types\nof synthetic anomalies, enhancing the model's ability to generalise across\nunseen anomalous classes in different domains. This method significantly\nbroadens the model's adaptability and robustness. Additionally, we propose a\nsupervised contrastive loss for the source domain and a self-supervised\ncontrastive triplet loss for the target domain, improving comprehensive feature\nrepresentation learning and extraction of domain-invariant features. Finally,\nan effective Centre-based Entropy Classifier (CEC) is proposed specifically for\nanomaly detection, facilitating accurate learning of normal boundaries in the\nsource domain. Our extensive evaluation across multiple real-world datasets\nagainst leading models in time series anomaly detection and UDA underscores\nDACAD's effectiveness. The results validate DACAD's superiority in transferring\nknowledge across domains and its potential to mitigate the challenge of limited\nlabelled data in time series anomaly detection.\n", "link": "http://arxiv.org/abs/2404.11269v1", "date": "2024-04-17", "relevancy": 2.6286, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5367}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5367}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5038}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DACAD%3A%20Domain%20Adaptation%20Contrastive%20Learning%20for%20Anomaly%20Detection%20in%0A%20%20Multivariate%20Time%20Series&body=Title%3A%20DACAD%3A%20Domain%20Adaptation%20Contrastive%20Learning%20for%20Anomaly%20Detection%20in%0A%20%20Multivariate%20Time%20Series%0AAuthor%3A%20Zahra%20Zamanzadeh%20Darban%20and%20Geoffrey%20I.%20Webb%20and%20Mahsa%20Salehi%0AAbstract%3A%20%20%20Time%20series%20anomaly%20detection%20%28TAD%29%20faces%20a%20significant%20challenge%20due%20to%20the%0Ascarcity%20of%20labelled%20data%2C%20which%20hinders%20the%20development%20of%20accurate%20detection%0Amodels.%20Unsupervised%20domain%20adaptation%20%28UDA%29%20addresses%20this%20challenge%20by%0Aleveraging%20a%20labelled%20dataset%20from%20a%20related%20domain%20to%20detect%20anomalies%20in%20a%0Atarget%20dataset.%20Existing%20domain%20adaptation%20techniques%20assume%20that%20the%20number%20of%0Aanomalous%20classes%20does%20not%20change%20between%20the%20source%20and%20target%20domains.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20Domain%20Adaptation%20Contrastive%20learning%20for%0AAnomaly%20Detection%20in%20multivariate%20time%20series%20%28DACAD%29%20model%20to%20address%20this%0Aissue%20by%20combining%20UDA%20and%20contrastive%20representation%20learning.%20DACAD%27s%0Aapproach%20includes%20an%20anomaly%20injection%20mechanism%20that%20introduces%20various%20types%0Aof%20synthetic%20anomalies%2C%20enhancing%20the%20model%27s%20ability%20to%20generalise%20across%0Aunseen%20anomalous%20classes%20in%20different%20domains.%20This%20method%20significantly%0Abroadens%20the%20model%27s%20adaptability%20and%20robustness.%20Additionally%2C%20we%20propose%20a%0Asupervised%20contrastive%20loss%20for%20the%20source%20domain%20and%20a%20self-supervised%0Acontrastive%20triplet%20loss%20for%20the%20target%20domain%2C%20improving%20comprehensive%20feature%0Arepresentation%20learning%20and%20extraction%20of%20domain-invariant%20features.%20Finally%2C%0Aan%20effective%20Centre-based%20Entropy%20Classifier%20%28CEC%29%20is%20proposed%20specifically%20for%0Aanomaly%20detection%2C%20facilitating%20accurate%20learning%20of%20normal%20boundaries%20in%20the%0Asource%20domain.%20Our%20extensive%20evaluation%20across%20multiple%20real-world%20datasets%0Aagainst%20leading%20models%20in%20time%20series%20anomaly%20detection%20and%20UDA%20underscores%0ADACAD%27s%20effectiveness.%20The%20results%20validate%20DACAD%27s%20superiority%20in%20transferring%0Aknowledge%20across%20domains%20and%20its%20potential%20to%20mitigate%20the%20challenge%20of%20limited%0Alabelled%20data%20in%20time%20series%20anomaly%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11269v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DACAD%3A%20Domain%20Adaptation%20Contrastive%20Learning%20for%20Anomaly%20Detection%20in%0A%20%20Multivariate%20Time%20Series&entry.906535625=Zahra%20Zamanzadeh%20Darban%20and%20Geoffrey%20I.%20Webb%20and%20Mahsa%20Salehi&entry.1292438233=%20%20Time%20series%20anomaly%20detection%20%28TAD%29%20faces%20a%20significant%20challenge%20due%20to%20the%0Ascarcity%20of%20labelled%20data%2C%20which%20hinders%20the%20development%20of%20accurate%20detection%0Amodels.%20Unsupervised%20domain%20adaptation%20%28UDA%29%20addresses%20this%20challenge%20by%0Aleveraging%20a%20labelled%20dataset%20from%20a%20related%20domain%20to%20detect%20anomalies%20in%20a%0Atarget%20dataset.%20Existing%20domain%20adaptation%20techniques%20assume%20that%20the%20number%20of%0Aanomalous%20classes%20does%20not%20change%20between%20the%20source%20and%20target%20domains.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20Domain%20Adaptation%20Contrastive%20learning%20for%0AAnomaly%20Detection%20in%20multivariate%20time%20series%20%28DACAD%29%20model%20to%20address%20this%0Aissue%20by%20combining%20UDA%20and%20contrastive%20representation%20learning.%20DACAD%27s%0Aapproach%20includes%20an%20anomaly%20injection%20mechanism%20that%20introduces%20various%20types%0Aof%20synthetic%20anomalies%2C%20enhancing%20the%20model%27s%20ability%20to%20generalise%20across%0Aunseen%20anomalous%20classes%20in%20different%20domains.%20This%20method%20significantly%0Abroadens%20the%20model%27s%20adaptability%20and%20robustness.%20Additionally%2C%20we%20propose%20a%0Asupervised%20contrastive%20loss%20for%20the%20source%20domain%20and%20a%20self-supervised%0Acontrastive%20triplet%20loss%20for%20the%20target%20domain%2C%20improving%20comprehensive%20feature%0Arepresentation%20learning%20and%20extraction%20of%20domain-invariant%20features.%20Finally%2C%0Aan%20effective%20Centre-based%20Entropy%20Classifier%20%28CEC%29%20is%20proposed%20specifically%20for%0Aanomaly%20detection%2C%20facilitating%20accurate%20learning%20of%20normal%20boundaries%20in%20the%0Asource%20domain.%20Our%20extensive%20evaluation%20across%20multiple%20real-world%20datasets%0Aagainst%20leading%20models%20in%20time%20series%20anomaly%20detection%20and%20UDA%20underscores%0ADACAD%27s%20effectiveness.%20The%20results%20validate%20DACAD%27s%20superiority%20in%20transferring%0Aknowledge%20across%20domains%20and%20its%20potential%20to%20mitigate%20the%20challenge%20of%20limited%0Alabelled%20data%20in%20time%20series%20anomaly%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11269v1&entry.124074799=Read"},
{"title": "Simple In-place Data Augmentation for Surveillance Object Detection", "author": "Munkh-Erdene Otgonbold and Ganzorig Batnasan and Munkhjargal Gochoo", "abstract": "  Motivated by the need to improve model performance in traffic monitoring\ntasks with limited labeled samples, we propose a straightforward augmentation\ntechnique tailored for object detection datasets, specifically designed for\nstationary camera-based applications. Our approach focuses on placing objects\nin the same positions as the originals to ensure its effectiveness. By applying\nin-place augmentation on objects from the same camera input image, we address\nthe challenge of overlapping with original and previously selected objects.\nThrough extensive testing on two traffic monitoring datasets, we illustrate the\nefficacy of our augmentation strategy in improving model performance,\nparticularly in scenarios with limited labeled samples and imbalanced class\ndistributions. Notably, our method achieves comparable performance to models\ntrained on the entire dataset while utilizing only 8.5 percent of the original\ndata. Moreover, we report significant improvements, with mAP@.5 increasing from\n0.4798 to 0.5025, and the mAP@.5:.95 rising from 0.29 to 0.3138 on the\nFishEye8K dataset. These results highlight the potential of our augmentation\napproach in enhancing object detection models for traffic monitoring\napplications.\n", "link": "http://arxiv.org/abs/2404.11226v1", "date": "2024-04-17", "relevancy": 2.6264, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5403}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5203}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5153}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Simple%20In-place%20Data%20Augmentation%20for%20Surveillance%20Object%20Detection&body=Title%3A%20Simple%20In-place%20Data%20Augmentation%20for%20Surveillance%20Object%20Detection%0AAuthor%3A%20Munkh-Erdene%20Otgonbold%20and%20Ganzorig%20Batnasan%20and%20Munkhjargal%20Gochoo%0AAbstract%3A%20%20%20Motivated%20by%20the%20need%20to%20improve%20model%20performance%20in%20traffic%20monitoring%0Atasks%20with%20limited%20labeled%20samples%2C%20we%20propose%20a%20straightforward%20augmentation%0Atechnique%20tailored%20for%20object%20detection%20datasets%2C%20specifically%20designed%20for%0Astationary%20camera-based%20applications.%20Our%20approach%20focuses%20on%20placing%20objects%0Ain%20the%20same%20positions%20as%20the%20originals%20to%20ensure%20its%20effectiveness.%20By%20applying%0Ain-place%20augmentation%20on%20objects%20from%20the%20same%20camera%20input%20image%2C%20we%20address%0Athe%20challenge%20of%20overlapping%20with%20original%20and%20previously%20selected%20objects.%0AThrough%20extensive%20testing%20on%20two%20traffic%20monitoring%20datasets%2C%20we%20illustrate%20the%0Aefficacy%20of%20our%20augmentation%20strategy%20in%20improving%20model%20performance%2C%0Aparticularly%20in%20scenarios%20with%20limited%20labeled%20samples%20and%20imbalanced%20class%0Adistributions.%20Notably%2C%20our%20method%20achieves%20comparable%20performance%20to%20models%0Atrained%20on%20the%20entire%20dataset%20while%20utilizing%20only%208.5%20percent%20of%20the%20original%0Adata.%20Moreover%2C%20we%20report%20significant%20improvements%2C%20with%20mAP%40.5%20increasing%20from%0A0.4798%20to%200.5025%2C%20and%20the%20mAP%40.5%3A.95%20rising%20from%200.29%20to%200.3138%20on%20the%0AFishEye8K%20dataset.%20These%20results%20highlight%20the%20potential%20of%20our%20augmentation%0Aapproach%20in%20enhancing%20object%20detection%20models%20for%20traffic%20monitoring%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11226v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20In-place%20Data%20Augmentation%20for%20Surveillance%20Object%20Detection&entry.906535625=Munkh-Erdene%20Otgonbold%20and%20Ganzorig%20Batnasan%20and%20Munkhjargal%20Gochoo&entry.1292438233=%20%20Motivated%20by%20the%20need%20to%20improve%20model%20performance%20in%20traffic%20monitoring%0Atasks%20with%20limited%20labeled%20samples%2C%20we%20propose%20a%20straightforward%20augmentation%0Atechnique%20tailored%20for%20object%20detection%20datasets%2C%20specifically%20designed%20for%0Astationary%20camera-based%20applications.%20Our%20approach%20focuses%20on%20placing%20objects%0Ain%20the%20same%20positions%20as%20the%20originals%20to%20ensure%20its%20effectiveness.%20By%20applying%0Ain-place%20augmentation%20on%20objects%20from%20the%20same%20camera%20input%20image%2C%20we%20address%0Athe%20challenge%20of%20overlapping%20with%20original%20and%20previously%20selected%20objects.%0AThrough%20extensive%20testing%20on%20two%20traffic%20monitoring%20datasets%2C%20we%20illustrate%20the%0Aefficacy%20of%20our%20augmentation%20strategy%20in%20improving%20model%20performance%2C%0Aparticularly%20in%20scenarios%20with%20limited%20labeled%20samples%20and%20imbalanced%20class%0Adistributions.%20Notably%2C%20our%20method%20achieves%20comparable%20performance%20to%20models%0Atrained%20on%20the%20entire%20dataset%20while%20utilizing%20only%208.5%20percent%20of%20the%20original%0Adata.%20Moreover%2C%20we%20report%20significant%20improvements%2C%20with%20mAP%40.5%20increasing%20from%0A0.4798%20to%200.5025%2C%20and%20the%20mAP%40.5%3A.95%20rising%20from%200.29%20to%200.3138%20on%20the%0AFishEye8K%20dataset.%20These%20results%20highlight%20the%20potential%20of%20our%20augmentation%0Aapproach%20in%20enhancing%20object%20detection%20models%20for%20traffic%20monitoring%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11226v1&entry.124074799=Read"},
{"title": "CarcassFormer: An End-to-end Transformer-based Framework for\n  Simultaneous Localization, Segmentation and Classification of Poultry Carcass\n  Defect", "author": "Minh Tran and Sang Truong and Arthur F. A. Fernandes and Michael T. Kidd and Ngan Le", "abstract": "  In the food industry, assessing the quality of poultry carcasses during\nprocessing is a crucial step. This study proposes an effective approach for\nautomating the assessment of carcass quality without requiring skilled labor or\ninspector involvement. The proposed system is based on machine learning (ML)\nand computer vision (CV) techniques, enabling automated defect detection and\ncarcass quality assessment. To this end, an end-to-end framework called\nCarcassFormer is introduced. It is built upon a Transformer-based architecture\ndesigned to effectively extract visual representations while simultaneously\ndetecting, segmenting, and classifying poultry carcass defects. Our proposed\nframework is capable of analyzing imperfections resulting from production and\ntransport welfare issues, as well as processing plant stunner, scalder, picker,\nand other equipment malfunctions. To benchmark the framework, a dataset of\n7,321 images was initially acquired, which contained both single and multiple\ncarcasses per image. In this study, the performance of the CarcassFormer system\nis compared with other state-of-the-art (SOTA) approaches for both\nclassification, detection, and segmentation tasks. Through extensive\nquantitative experiments, our framework consistently outperforms existing\nmethods, demonstrating remarkable improvements across various evaluation\nmetrics such as AP, AP@50, and AP@75. Furthermore, the qualitative results\nhighlight the strengths of CarcassFormer in capturing fine details, including\nfeathers, and accurately localizing and segmenting carcasses with high\nprecision. To facilitate further research and collaboration, the pre-trained\nmodel and source code of CarcassFormer is available for research purposes at:\n\\url{https://github.com/UARK-AICV/CarcassFormer}.\n", "link": "http://arxiv.org/abs/2404.11429v1", "date": "2024-04-17", "relevancy": 2.5876, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5292}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5169}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5065}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CarcassFormer%3A%20An%20End-to-end%20Transformer-based%20Framework%20for%0A%20%20Simultaneous%20Localization%2C%20Segmentation%20and%20Classification%20of%20Poultry%20Carcass%0A%20%20Defect&body=Title%3A%20CarcassFormer%3A%20An%20End-to-end%20Transformer-based%20Framework%20for%0A%20%20Simultaneous%20Localization%2C%20Segmentation%20and%20Classification%20of%20Poultry%20Carcass%0A%20%20Defect%0AAuthor%3A%20Minh%20Tran%20and%20Sang%20Truong%20and%20Arthur%20F.%20A.%20Fernandes%20and%20Michael%20T.%20Kidd%20and%20Ngan%20Le%0AAbstract%3A%20%20%20In%20the%20food%20industry%2C%20assessing%20the%20quality%20of%20poultry%20carcasses%20during%0Aprocessing%20is%20a%20crucial%20step.%20This%20study%20proposes%20an%20effective%20approach%20for%0Aautomating%20the%20assessment%20of%20carcass%20quality%20without%20requiring%20skilled%20labor%20or%0Ainspector%20involvement.%20The%20proposed%20system%20is%20based%20on%20machine%20learning%20%28ML%29%0Aand%20computer%20vision%20%28CV%29%20techniques%2C%20enabling%20automated%20defect%20detection%20and%0Acarcass%20quality%20assessment.%20To%20this%20end%2C%20an%20end-to-end%20framework%20called%0ACarcassFormer%20is%20introduced.%20It%20is%20built%20upon%20a%20Transformer-based%20architecture%0Adesigned%20to%20effectively%20extract%20visual%20representations%20while%20simultaneously%0Adetecting%2C%20segmenting%2C%20and%20classifying%20poultry%20carcass%20defects.%20Our%20proposed%0Aframework%20is%20capable%20of%20analyzing%20imperfections%20resulting%20from%20production%20and%0Atransport%20welfare%20issues%2C%20as%20well%20as%20processing%20plant%20stunner%2C%20scalder%2C%20picker%2C%0Aand%20other%20equipment%20malfunctions.%20To%20benchmark%20the%20framework%2C%20a%20dataset%20of%0A7%2C321%20images%20was%20initially%20acquired%2C%20which%20contained%20both%20single%20and%20multiple%0Acarcasses%20per%20image.%20In%20this%20study%2C%20the%20performance%20of%20the%20CarcassFormer%20system%0Ais%20compared%20with%20other%20state-of-the-art%20%28SOTA%29%20approaches%20for%20both%0Aclassification%2C%20detection%2C%20and%20segmentation%20tasks.%20Through%20extensive%0Aquantitative%20experiments%2C%20our%20framework%20consistently%20outperforms%20existing%0Amethods%2C%20demonstrating%20remarkable%20improvements%20across%20various%20evaluation%0Ametrics%20such%20as%20AP%2C%20AP%4050%2C%20and%20AP%4075.%20Furthermore%2C%20the%20qualitative%20results%0Ahighlight%20the%20strengths%20of%20CarcassFormer%20in%20capturing%20fine%20details%2C%20including%0Afeathers%2C%20and%20accurately%20localizing%20and%20segmenting%20carcasses%20with%20high%0Aprecision.%20To%20facilitate%20further%20research%20and%20collaboration%2C%20the%20pre-trained%0Amodel%20and%20source%20code%20of%20CarcassFormer%20is%20available%20for%20research%20purposes%20at%3A%0A%5Curl%7Bhttps%3A//github.com/UARK-AICV/CarcassFormer%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11429v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CarcassFormer%3A%20An%20End-to-end%20Transformer-based%20Framework%20for%0A%20%20Simultaneous%20Localization%2C%20Segmentation%20and%20Classification%20of%20Poultry%20Carcass%0A%20%20Defect&entry.906535625=Minh%20Tran%20and%20Sang%20Truong%20and%20Arthur%20F.%20A.%20Fernandes%20and%20Michael%20T.%20Kidd%20and%20Ngan%20Le&entry.1292438233=%20%20In%20the%20food%20industry%2C%20assessing%20the%20quality%20of%20poultry%20carcasses%20during%0Aprocessing%20is%20a%20crucial%20step.%20This%20study%20proposes%20an%20effective%20approach%20for%0Aautomating%20the%20assessment%20of%20carcass%20quality%20without%20requiring%20skilled%20labor%20or%0Ainspector%20involvement.%20The%20proposed%20system%20is%20based%20on%20machine%20learning%20%28ML%29%0Aand%20computer%20vision%20%28CV%29%20techniques%2C%20enabling%20automated%20defect%20detection%20and%0Acarcass%20quality%20assessment.%20To%20this%20end%2C%20an%20end-to-end%20framework%20called%0ACarcassFormer%20is%20introduced.%20It%20is%20built%20upon%20a%20Transformer-based%20architecture%0Adesigned%20to%20effectively%20extract%20visual%20representations%20while%20simultaneously%0Adetecting%2C%20segmenting%2C%20and%20classifying%20poultry%20carcass%20defects.%20Our%20proposed%0Aframework%20is%20capable%20of%20analyzing%20imperfections%20resulting%20from%20production%20and%0Atransport%20welfare%20issues%2C%20as%20well%20as%20processing%20plant%20stunner%2C%20scalder%2C%20picker%2C%0Aand%20other%20equipment%20malfunctions.%20To%20benchmark%20the%20framework%2C%20a%20dataset%20of%0A7%2C321%20images%20was%20initially%20acquired%2C%20which%20contained%20both%20single%20and%20multiple%0Acarcasses%20per%20image.%20In%20this%20study%2C%20the%20performance%20of%20the%20CarcassFormer%20system%0Ais%20compared%20with%20other%20state-of-the-art%20%28SOTA%29%20approaches%20for%20both%0Aclassification%2C%20detection%2C%20and%20segmentation%20tasks.%20Through%20extensive%0Aquantitative%20experiments%2C%20our%20framework%20consistently%20outperforms%20existing%0Amethods%2C%20demonstrating%20remarkable%20improvements%20across%20various%20evaluation%0Ametrics%20such%20as%20AP%2C%20AP%4050%2C%20and%20AP%4075.%20Furthermore%2C%20the%20qualitative%20results%0Ahighlight%20the%20strengths%20of%20CarcassFormer%20in%20capturing%20fine%20details%2C%20including%0Afeathers%2C%20and%20accurately%20localizing%20and%20segmenting%20carcasses%20with%20high%0Aprecision.%20To%20facilitate%20further%20research%20and%20collaboration%2C%20the%20pre-trained%0Amodel%20and%20source%20code%20of%20CarcassFormer%20is%20available%20for%20research%20purposes%20at%3A%0A%5Curl%7Bhttps%3A//github.com/UARK-AICV/CarcassFormer%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11429v1&entry.124074799=Read"},
{"title": "The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a\n  Clean Model on Poisoned Data", "author": "Zixuan Zhu and Rui Wang and Cong Zou and Lihua Jing", "abstract": "  Recently, backdoor attacks have posed a serious security threat to the\ntraining process of deep neural networks (DNNs). The attacked model behaves\nnormally on benign samples but outputs a specific result when the trigger is\npresent. However, compared with the rocketing progress of backdoor attacks,\nexisting defenses are difficult to deal with these threats effectively or\nrequire benign samples to work, which may be unavailable in real scenarios. In\nthis paper, we find that the poisoned samples and benign samples can be\ndistinguished with prediction entropy. This inspires us to propose a novel\ndual-network training framework: The Victim and The Beneficiary (V&B), which\nexploits a poisoned model to train a clean model without extra benign samples.\nFirstly, we sacrifice the Victim network to be a powerful poisoned sample\ndetector by training on suspicious samples. Secondly, we train the Beneficiary\nnetwork on the credible samples selected by the Victim to inhibit backdoor\ninjection. Thirdly, a semi-supervised suppression strategy is adopted for\nerasing potential backdoors and improving model performance. Furthermore, to\nbetter inhibit missed poisoned samples, we propose a strong data augmentation\nmethod, AttentionMix, which works well with our proposed V&B framework.\nExtensive experiments on two widely used datasets against 6 state-of-the-art\nattacks demonstrate that our framework is effective in preventing backdoor\ninjection and robust to various attacks while maintaining the performance on\nbenign samples. Our code is available at https://github.com/Zixuan-Zhu/VaB.\n", "link": "http://arxiv.org/abs/2404.11265v1", "date": "2024-04-17", "relevancy": 2.525, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5342}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5134}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4674}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Victim%20and%20The%20Beneficiary%3A%20Exploiting%20a%20Poisoned%20Model%20to%20Train%20a%0A%20%20Clean%20Model%20on%20Poisoned%20Data&body=Title%3A%20The%20Victim%20and%20The%20Beneficiary%3A%20Exploiting%20a%20Poisoned%20Model%20to%20Train%20a%0A%20%20Clean%20Model%20on%20Poisoned%20Data%0AAuthor%3A%20Zixuan%20Zhu%20and%20Rui%20Wang%20and%20Cong%20Zou%20and%20Lihua%20Jing%0AAbstract%3A%20%20%20Recently%2C%20backdoor%20attacks%20have%20posed%20a%20serious%20security%20threat%20to%20the%0Atraining%20process%20of%20deep%20neural%20networks%20%28DNNs%29.%20The%20attacked%20model%20behaves%0Anormally%20on%20benign%20samples%20but%20outputs%20a%20specific%20result%20when%20the%20trigger%20is%0Apresent.%20However%2C%20compared%20with%20the%20rocketing%20progress%20of%20backdoor%20attacks%2C%0Aexisting%20defenses%20are%20difficult%20to%20deal%20with%20these%20threats%20effectively%20or%0Arequire%20benign%20samples%20to%20work%2C%20which%20may%20be%20unavailable%20in%20real%20scenarios.%20In%0Athis%20paper%2C%20we%20find%20that%20the%20poisoned%20samples%20and%20benign%20samples%20can%20be%0Adistinguished%20with%20prediction%20entropy.%20This%20inspires%20us%20to%20propose%20a%20novel%0Adual-network%20training%20framework%3A%20The%20Victim%20and%20The%20Beneficiary%20%28V%26B%29%2C%20which%0Aexploits%20a%20poisoned%20model%20to%20train%20a%20clean%20model%20without%20extra%20benign%20samples.%0AFirstly%2C%20we%20sacrifice%20the%20Victim%20network%20to%20be%20a%20powerful%20poisoned%20sample%0Adetector%20by%20training%20on%20suspicious%20samples.%20Secondly%2C%20we%20train%20the%20Beneficiary%0Anetwork%20on%20the%20credible%20samples%20selected%20by%20the%20Victim%20to%20inhibit%20backdoor%0Ainjection.%20Thirdly%2C%20a%20semi-supervised%20suppression%20strategy%20is%20adopted%20for%0Aerasing%20potential%20backdoors%20and%20improving%20model%20performance.%20Furthermore%2C%20to%0Abetter%20inhibit%20missed%20poisoned%20samples%2C%20we%20propose%20a%20strong%20data%20augmentation%0Amethod%2C%20AttentionMix%2C%20which%20works%20well%20with%20our%20proposed%20V%26B%20framework.%0AExtensive%20experiments%20on%20two%20widely%20used%20datasets%20against%206%20state-of-the-art%0Aattacks%20demonstrate%20that%20our%20framework%20is%20effective%20in%20preventing%20backdoor%0Ainjection%20and%20robust%20to%20various%20attacks%20while%20maintaining%20the%20performance%20on%0Abenign%20samples.%20Our%20code%20is%20available%20at%20https%3A//github.com/Zixuan-Zhu/VaB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11265v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Victim%20and%20The%20Beneficiary%3A%20Exploiting%20a%20Poisoned%20Model%20to%20Train%20a%0A%20%20Clean%20Model%20on%20Poisoned%20Data&entry.906535625=Zixuan%20Zhu%20and%20Rui%20Wang%20and%20Cong%20Zou%20and%20Lihua%20Jing&entry.1292438233=%20%20Recently%2C%20backdoor%20attacks%20have%20posed%20a%20serious%20security%20threat%20to%20the%0Atraining%20process%20of%20deep%20neural%20networks%20%28DNNs%29.%20The%20attacked%20model%20behaves%0Anormally%20on%20benign%20samples%20but%20outputs%20a%20specific%20result%20when%20the%20trigger%20is%0Apresent.%20However%2C%20compared%20with%20the%20rocketing%20progress%20of%20backdoor%20attacks%2C%0Aexisting%20defenses%20are%20difficult%20to%20deal%20with%20these%20threats%20effectively%20or%0Arequire%20benign%20samples%20to%20work%2C%20which%20may%20be%20unavailable%20in%20real%20scenarios.%20In%0Athis%20paper%2C%20we%20find%20that%20the%20poisoned%20samples%20and%20benign%20samples%20can%20be%0Adistinguished%20with%20prediction%20entropy.%20This%20inspires%20us%20to%20propose%20a%20novel%0Adual-network%20training%20framework%3A%20The%20Victim%20and%20The%20Beneficiary%20%28V%26B%29%2C%20which%0Aexploits%20a%20poisoned%20model%20to%20train%20a%20clean%20model%20without%20extra%20benign%20samples.%0AFirstly%2C%20we%20sacrifice%20the%20Victim%20network%20to%20be%20a%20powerful%20poisoned%20sample%0Adetector%20by%20training%20on%20suspicious%20samples.%20Secondly%2C%20we%20train%20the%20Beneficiary%0Anetwork%20on%20the%20credible%20samples%20selected%20by%20the%20Victim%20to%20inhibit%20backdoor%0Ainjection.%20Thirdly%2C%20a%20semi-supervised%20suppression%20strategy%20is%20adopted%20for%0Aerasing%20potential%20backdoors%20and%20improving%20model%20performance.%20Furthermore%2C%20to%0Abetter%20inhibit%20missed%20poisoned%20samples%2C%20we%20propose%20a%20strong%20data%20augmentation%0Amethod%2C%20AttentionMix%2C%20which%20works%20well%20with%20our%20proposed%20V%26B%20framework.%0AExtensive%20experiments%20on%20two%20widely%20used%20datasets%20against%206%20state-of-the-art%0Aattacks%20demonstrate%20that%20our%20framework%20is%20effective%20in%20preventing%20backdoor%0Ainjection%20and%20robust%20to%20various%20attacks%20while%20maintaining%20the%20performance%20on%0Abenign%20samples.%20Our%20code%20is%20available%20at%20https%3A//github.com/Zixuan-Zhu/VaB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11265v1&entry.124074799=Read"},
{"title": "Spatial Context-based Self-Supervised Learning for Handwritten Text\n  Recognition", "author": "Carlos Penarrubia and Carlos Garrido-Munoz and Jose J. Valero-Mas and Jorge Calvo-Zaragoza", "abstract": "  Handwritten Text Recognition (HTR) is a relevant problem in computer vision,\nand implies unique challenges owing to its inherent variability and the rich\ncontextualization required for its interpretation. Despite the success of\nSelf-Supervised Learning (SSL) in computer vision, its application to HTR has\nbeen rather scattered, leaving key SSL methodologies unexplored. This work\nfocuses on one of them, namely Spatial Context-based SSL. We investigate how\nthis family of approaches can be adapted and optimized for HTR and propose new\nworkflows that leverage the unique features of handwritten text. Our\nexperiments demonstrate that the methods considered lead to advancements in the\nstate-of-the-art of SSL for HTR in a number of benchmark cases.\n", "link": "http://arxiv.org/abs/2404.11585v1", "date": "2024-04-17", "relevancy": 2.4705, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5465}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4696}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4662}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Spatial%20Context-based%20Self-Supervised%20Learning%20for%20Handwritten%20Text%0A%20%20Recognition&body=Title%3A%20Spatial%20Context-based%20Self-Supervised%20Learning%20for%20Handwritten%20Text%0A%20%20Recognition%0AAuthor%3A%20Carlos%20Penarrubia%20and%20Carlos%20Garrido-Munoz%20and%20Jose%20J.%20Valero-Mas%20and%20Jorge%20Calvo-Zaragoza%0AAbstract%3A%20%20%20Handwritten%20Text%20Recognition%20%28HTR%29%20is%20a%20relevant%20problem%20in%20computer%20vision%2C%0Aand%20implies%20unique%20challenges%20owing%20to%20its%20inherent%20variability%20and%20the%20rich%0Acontextualization%20required%20for%20its%20interpretation.%20Despite%20the%20success%20of%0ASelf-Supervised%20Learning%20%28SSL%29%20in%20computer%20vision%2C%20its%20application%20to%20HTR%20has%0Abeen%20rather%20scattered%2C%20leaving%20key%20SSL%20methodologies%20unexplored.%20This%20work%0Afocuses%20on%20one%20of%20them%2C%20namely%20Spatial%20Context-based%20SSL.%20We%20investigate%20how%0Athis%20family%20of%20approaches%20can%20be%20adapted%20and%20optimized%20for%20HTR%20and%20propose%20new%0Aworkflows%20that%20leverage%20the%20unique%20features%20of%20handwritten%20text.%20Our%0Aexperiments%20demonstrate%20that%20the%20methods%20considered%20lead%20to%20advancements%20in%20the%0Astate-of-the-art%20of%20SSL%20for%20HTR%20in%20a%20number%20of%20benchmark%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11585v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20Context-based%20Self-Supervised%20Learning%20for%20Handwritten%20Text%0A%20%20Recognition&entry.906535625=Carlos%20Penarrubia%20and%20Carlos%20Garrido-Munoz%20and%20Jose%20J.%20Valero-Mas%20and%20Jorge%20Calvo-Zaragoza&entry.1292438233=%20%20Handwritten%20Text%20Recognition%20%28HTR%29%20is%20a%20relevant%20problem%20in%20computer%20vision%2C%0Aand%20implies%20unique%20challenges%20owing%20to%20its%20inherent%20variability%20and%20the%20rich%0Acontextualization%20required%20for%20its%20interpretation.%20Despite%20the%20success%20of%0ASelf-Supervised%20Learning%20%28SSL%29%20in%20computer%20vision%2C%20its%20application%20to%20HTR%20has%0Abeen%20rather%20scattered%2C%20leaving%20key%20SSL%20methodologies%20unexplored.%20This%20work%0Afocuses%20on%20one%20of%20them%2C%20namely%20Spatial%20Context-based%20SSL.%20We%20investigate%20how%0Athis%20family%20of%20approaches%20can%20be%20adapted%20and%20optimized%20for%20HTR%20and%20propose%20new%0Aworkflows%20that%20leverage%20the%20unique%20features%20of%20handwritten%20text.%20Our%0Aexperiments%20demonstrate%20that%20the%20methods%20considered%20lead%20to%20advancements%20in%20the%0Astate-of-the-art%20of%20SSL%20for%20HTR%20in%20a%20number%20of%20benchmark%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11585v1&entry.124074799=Read"},
{"title": "JointViT: Modeling Oxygen Saturation Levels with Joint Supervision on\n  Long-Tailed OCTA", "author": "Zeyu Zhang and Xuyin Qi and Mingxi Chen and Guangxi Li and Ryan Pham and Ayub Zuhair and Ella Berry and Zhibin Liao and Owen Siggs and Robert Mclaughlin and Jamie Craig and Minh-Son To", "abstract": "  The oxygen saturation level in the blood (SaO2) is crucial for health,\nparticularly in relation to sleep-related breathing disorders. However,\ncontinuous monitoring of SaO2 is time-consuming and highly variable depending\non patients' conditions. Recently, optical coherence tomography angiography\n(OCTA) has shown promising development in rapidly and effectively screening\neye-related lesions, offering the potential for diagnosing sleep-related\ndisorders. To bridge this gap, our paper presents three key contributions.\nFirstly, we propose JointViT, a novel model based on the Vision Transformer\narchitecture, incorporating a joint loss function for supervision. Secondly, we\nintroduce a balancing augmentation technique during data preprocessing to\nimprove the model's performance, particularly on the long-tail distribution\nwithin the OCTA dataset. Lastly, through comprehensive experiments on the OCTA\ndataset, our proposed method significantly outperforms other state-of-the-art\nmethods, achieving improvements of up to 12.28% in overall accuracy. This\nadvancement lays the groundwork for the future utilization of OCTA in\ndiagnosing sleep-related disorders. See project website\nhttps://steve-zeyu-zhang.github.io/JointViT\n", "link": "http://arxiv.org/abs/2404.11525v1", "date": "2024-04-17", "relevancy": 2.4674, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5084}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4887}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4833}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20JointViT%3A%20Modeling%20Oxygen%20Saturation%20Levels%20with%20Joint%20Supervision%20on%0A%20%20Long-Tailed%20OCTA&body=Title%3A%20JointViT%3A%20Modeling%20Oxygen%20Saturation%20Levels%20with%20Joint%20Supervision%20on%0A%20%20Long-Tailed%20OCTA%0AAuthor%3A%20Zeyu%20Zhang%20and%20Xuyin%20Qi%20and%20Mingxi%20Chen%20and%20Guangxi%20Li%20and%20Ryan%20Pham%20and%20Ayub%20Zuhair%20and%20Ella%20Berry%20and%20Zhibin%20Liao%20and%20Owen%20Siggs%20and%20Robert%20Mclaughlin%20and%20Jamie%20Craig%20and%20Minh-Son%20To%0AAbstract%3A%20%20%20The%20oxygen%20saturation%20level%20in%20the%20blood%20%28SaO2%29%20is%20crucial%20for%20health%2C%0Aparticularly%20in%20relation%20to%20sleep-related%20breathing%20disorders.%20However%2C%0Acontinuous%20monitoring%20of%20SaO2%20is%20time-consuming%20and%20highly%20variable%20depending%0Aon%20patients%27%20conditions.%20Recently%2C%20optical%20coherence%20tomography%20angiography%0A%28OCTA%29%20has%20shown%20promising%20development%20in%20rapidly%20and%20effectively%20screening%0Aeye-related%20lesions%2C%20offering%20the%20potential%20for%20diagnosing%20sleep-related%0Adisorders.%20To%20bridge%20this%20gap%2C%20our%20paper%20presents%20three%20key%20contributions.%0AFirstly%2C%20we%20propose%20JointViT%2C%20a%20novel%20model%20based%20on%20the%20Vision%20Transformer%0Aarchitecture%2C%20incorporating%20a%20joint%20loss%20function%20for%20supervision.%20Secondly%2C%20we%0Aintroduce%20a%20balancing%20augmentation%20technique%20during%20data%20preprocessing%20to%0Aimprove%20the%20model%27s%20performance%2C%20particularly%20on%20the%20long-tail%20distribution%0Awithin%20the%20OCTA%20dataset.%20Lastly%2C%20through%20comprehensive%20experiments%20on%20the%20OCTA%0Adataset%2C%20our%20proposed%20method%20significantly%20outperforms%20other%20state-of-the-art%0Amethods%2C%20achieving%20improvements%20of%20up%20to%2012.28%25%20in%20overall%20accuracy.%20This%0Aadvancement%20lays%20the%20groundwork%20for%20the%20future%20utilization%20of%20OCTA%20in%0Adiagnosing%20sleep-related%20disorders.%20See%20project%20website%0Ahttps%3A//steve-zeyu-zhang.github.io/JointViT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11525v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JointViT%3A%20Modeling%20Oxygen%20Saturation%20Levels%20with%20Joint%20Supervision%20on%0A%20%20Long-Tailed%20OCTA&entry.906535625=Zeyu%20Zhang%20and%20Xuyin%20Qi%20and%20Mingxi%20Chen%20and%20Guangxi%20Li%20and%20Ryan%20Pham%20and%20Ayub%20Zuhair%20and%20Ella%20Berry%20and%20Zhibin%20Liao%20and%20Owen%20Siggs%20and%20Robert%20Mclaughlin%20and%20Jamie%20Craig%20and%20Minh-Son%20To&entry.1292438233=%20%20The%20oxygen%20saturation%20level%20in%20the%20blood%20%28SaO2%29%20is%20crucial%20for%20health%2C%0Aparticularly%20in%20relation%20to%20sleep-related%20breathing%20disorders.%20However%2C%0Acontinuous%20monitoring%20of%20SaO2%20is%20time-consuming%20and%20highly%20variable%20depending%0Aon%20patients%27%20conditions.%20Recently%2C%20optical%20coherence%20tomography%20angiography%0A%28OCTA%29%20has%20shown%20promising%20development%20in%20rapidly%20and%20effectively%20screening%0Aeye-related%20lesions%2C%20offering%20the%20potential%20for%20diagnosing%20sleep-related%0Adisorders.%20To%20bridge%20this%20gap%2C%20our%20paper%20presents%20three%20key%20contributions.%0AFirstly%2C%20we%20propose%20JointViT%2C%20a%20novel%20model%20based%20on%20the%20Vision%20Transformer%0Aarchitecture%2C%20incorporating%20a%20joint%20loss%20function%20for%20supervision.%20Secondly%2C%20we%0Aintroduce%20a%20balancing%20augmentation%20technique%20during%20data%20preprocessing%20to%0Aimprove%20the%20model%27s%20performance%2C%20particularly%20on%20the%20long-tail%20distribution%0Awithin%20the%20OCTA%20dataset.%20Lastly%2C%20through%20comprehensive%20experiments%20on%20the%20OCTA%0Adataset%2C%20our%20proposed%20method%20significantly%20outperforms%20other%20state-of-the-art%0Amethods%2C%20achieving%20improvements%20of%20up%20to%2012.28%25%20in%20overall%20accuracy.%20This%0Aadvancement%20lays%20the%20groundwork%20for%20the%20future%20utilization%20of%20OCTA%20in%0Adiagnosing%20sleep-related%20disorders.%20See%20project%20website%0Ahttps%3A//steve-zeyu-zhang.github.io/JointViT%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11525v1&entry.124074799=Read"},
{"title": "SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping", "author": "Vincent Cartillier and Grant Schindler and Irfan Essa", "abstract": "  We present SLAIM - Simultaneous Localization and Implicit Mapping. We propose\na novel coarse-to-fine tracking model tailored for Neural Radiance Field SLAM\n(NeRF-SLAM) to achieve state-of-the-art tracking performance. Notably, existing\nNeRF-SLAM systems consistently exhibit inferior tracking performance compared\nto traditional SLAM algorithms. NeRF-SLAM methods solve camera tracking via\nimage alignment and photometric bundle-adjustment. Such optimization processes\nare difficult to optimize due to the narrow basin of attraction of the\noptimization loss in image space (local minima) and the lack of initial\ncorrespondences. We mitigate these limitations by implementing a Gaussian\npyramid filter on top of NeRF, facilitating a coarse-to-fine tracking\noptimization strategy. Furthermore, NeRF systems encounter challenges in\nconverging to the right geometry with limited input views. While prior\napproaches use a Signed-Distance Function (SDF)-based NeRF and directly\nsupervise SDF values by approximating ground truth SDF through depth\nmeasurements, this often results in suboptimal geometry. In contrast, our\nmethod employs a volume density representation and introduces a novel KL\nregularizer on the ray termination distribution, constraining scene geometry to\nconsist of empty space and opaque surfaces. Our solution implements both local\nand global bundle-adjustment to produce a robust (coarse-to-fine) and accurate\n(KL regularizer) SLAM solution. We conduct experiments on multiple datasets\n(ScanNet, TUM, Replica) showing state-of-the-art results in tracking and in\nreconstruction accuracy.\n", "link": "http://arxiv.org/abs/2404.11419v1", "date": "2024-04-17", "relevancy": 2.4634, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6505}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5956}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5798}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SLAIM%3A%20Robust%20Dense%20Neural%20SLAM%20for%20Online%20Tracking%20and%20Mapping&body=Title%3A%20SLAIM%3A%20Robust%20Dense%20Neural%20SLAM%20for%20Online%20Tracking%20and%20Mapping%0AAuthor%3A%20Vincent%20Cartillier%20and%20Grant%20Schindler%20and%20Irfan%20Essa%0AAbstract%3A%20%20%20We%20present%20SLAIM%20-%20Simultaneous%20Localization%20and%20Implicit%20Mapping.%20We%20propose%0Aa%20novel%20coarse-to-fine%20tracking%20model%20tailored%20for%20Neural%20Radiance%20Field%20SLAM%0A%28NeRF-SLAM%29%20to%20achieve%20state-of-the-art%20tracking%20performance.%20Notably%2C%20existing%0ANeRF-SLAM%20systems%20consistently%20exhibit%20inferior%20tracking%20performance%20compared%0Ato%20traditional%20SLAM%20algorithms.%20NeRF-SLAM%20methods%20solve%20camera%20tracking%20via%0Aimage%20alignment%20and%20photometric%20bundle-adjustment.%20Such%20optimization%20processes%0Aare%20difficult%20to%20optimize%20due%20to%20the%20narrow%20basin%20of%20attraction%20of%20the%0Aoptimization%20loss%20in%20image%20space%20%28local%20minima%29%20and%20the%20lack%20of%20initial%0Acorrespondences.%20We%20mitigate%20these%20limitations%20by%20implementing%20a%20Gaussian%0Apyramid%20filter%20on%20top%20of%20NeRF%2C%20facilitating%20a%20coarse-to-fine%20tracking%0Aoptimization%20strategy.%20Furthermore%2C%20NeRF%20systems%20encounter%20challenges%20in%0Aconverging%20to%20the%20right%20geometry%20with%20limited%20input%20views.%20While%20prior%0Aapproaches%20use%20a%20Signed-Distance%20Function%20%28SDF%29-based%20NeRF%20and%20directly%0Asupervise%20SDF%20values%20by%20approximating%20ground%20truth%20SDF%20through%20depth%0Ameasurements%2C%20this%20often%20results%20in%20suboptimal%20geometry.%20In%20contrast%2C%20our%0Amethod%20employs%20a%20volume%20density%20representation%20and%20introduces%20a%20novel%20KL%0Aregularizer%20on%20the%20ray%20termination%20distribution%2C%20constraining%20scene%20geometry%20to%0Aconsist%20of%20empty%20space%20and%20opaque%20surfaces.%20Our%20solution%20implements%20both%20local%0Aand%20global%20bundle-adjustment%20to%20produce%20a%20robust%20%28coarse-to-fine%29%20and%20accurate%0A%28KL%20regularizer%29%20SLAM%20solution.%20We%20conduct%20experiments%20on%20multiple%20datasets%0A%28ScanNet%2C%20TUM%2C%20Replica%29%20showing%20state-of-the-art%20results%20in%20tracking%20and%20in%0Areconstruction%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11419v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLAIM%3A%20Robust%20Dense%20Neural%20SLAM%20for%20Online%20Tracking%20and%20Mapping&entry.906535625=Vincent%20Cartillier%20and%20Grant%20Schindler%20and%20Irfan%20Essa&entry.1292438233=%20%20We%20present%20SLAIM%20-%20Simultaneous%20Localization%20and%20Implicit%20Mapping.%20We%20propose%0Aa%20novel%20coarse-to-fine%20tracking%20model%20tailored%20for%20Neural%20Radiance%20Field%20SLAM%0A%28NeRF-SLAM%29%20to%20achieve%20state-of-the-art%20tracking%20performance.%20Notably%2C%20existing%0ANeRF-SLAM%20systems%20consistently%20exhibit%20inferior%20tracking%20performance%20compared%0Ato%20traditional%20SLAM%20algorithms.%20NeRF-SLAM%20methods%20solve%20camera%20tracking%20via%0Aimage%20alignment%20and%20photometric%20bundle-adjustment.%20Such%20optimization%20processes%0Aare%20difficult%20to%20optimize%20due%20to%20the%20narrow%20basin%20of%20attraction%20of%20the%0Aoptimization%20loss%20in%20image%20space%20%28local%20minima%29%20and%20the%20lack%20of%20initial%0Acorrespondences.%20We%20mitigate%20these%20limitations%20by%20implementing%20a%20Gaussian%0Apyramid%20filter%20on%20top%20of%20NeRF%2C%20facilitating%20a%20coarse-to-fine%20tracking%0Aoptimization%20strategy.%20Furthermore%2C%20NeRF%20systems%20encounter%20challenges%20in%0Aconverging%20to%20the%20right%20geometry%20with%20limited%20input%20views.%20While%20prior%0Aapproaches%20use%20a%20Signed-Distance%20Function%20%28SDF%29-based%20NeRF%20and%20directly%0Asupervise%20SDF%20values%20by%20approximating%20ground%20truth%20SDF%20through%20depth%0Ameasurements%2C%20this%20often%20results%20in%20suboptimal%20geometry.%20In%20contrast%2C%20our%0Amethod%20employs%20a%20volume%20density%20representation%20and%20introduces%20a%20novel%20KL%0Aregularizer%20on%20the%20ray%20termination%20distribution%2C%20constraining%20scene%20geometry%20to%0Aconsist%20of%20empty%20space%20and%20opaque%20surfaces.%20Our%20solution%20implements%20both%20local%0Aand%20global%20bundle-adjustment%20to%20produce%20a%20robust%20%28coarse-to-fine%29%20and%20accurate%0A%28KL%20regularizer%29%20SLAM%20solution.%20We%20conduct%20experiments%20on%20multiple%20datasets%0A%28ScanNet%2C%20TUM%2C%20Replica%29%20showing%20state-of-the-art%20results%20in%20tracking%20and%20in%0Areconstruction%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11419v1&entry.124074799=Read"},
{"title": "Federated Class-Incremental Learning with New-Class Augmented\n  Self-Distillation", "author": "Zhiyuan Wu and Tianliu He and Sheng Sun and Yuwei Wang and Min Liu and Bo Gao and Xuefeng Jiang", "abstract": "  Federated Learning (FL) enables collaborative model training among\nparticipants while guaranteeing the privacy of raw data. Mainstream FL\nmethodologies overlook the dynamic nature of real-world data, particularly its\ntendency to grow in volume and diversify in classes over time. This oversight\nresults in FL methods suffering from catastrophic forgetting, where the trained\nmodels inadvertently discard previously learned information upon assimilating\nnew data. In response to this challenge, we propose a novel Federated\nClass-Incremental Learning (FCIL) method, named \\underline{Fed}erated\n\\underline{C}lass-Incremental \\underline{L}earning with New-Class\n\\underline{A}ugmented \\underline{S}elf-Di\\underline{S}tillation (FedCLASS). The\ncore of FedCLASS is to enrich the class scores of historical models with new\nclass scores predicted by current models and utilize the combined knowledge for\nself-distillation, enabling a more sufficient and precise knowledge transfer\nfrom historical models to current models. Theoretical analyses demonstrate that\nFedCLASS stands on reliable foundations, considering scores of old classes\npredicted by historical models as conditional probabilities in the absence of\nnew classes, and the scores of new classes predicted by current models as the\nconditional probabilities of class scores derived from historical models.\nEmpirical experiments demonstrate the superiority of FedCLASS over four\nbaseline algorithms in reducing average forgetting rate and boosting global\naccuracy.\n", "link": "http://arxiv.org/abs/2401.00622v3", "date": "2024-04-17", "relevancy": 2.4622, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5021}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4924}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4829}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Federated%20Class-Incremental%20Learning%20with%20New-Class%20Augmented%0A%20%20Self-Distillation&body=Title%3A%20Federated%20Class-Incremental%20Learning%20with%20New-Class%20Augmented%0A%20%20Self-Distillation%0AAuthor%3A%20Zhiyuan%20Wu%20and%20Tianliu%20He%20and%20Sheng%20Sun%20and%20Yuwei%20Wang%20and%20Min%20Liu%20and%20Bo%20Gao%20and%20Xuefeng%20Jiang%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20among%0Aparticipants%20while%20guaranteeing%20the%20privacy%20of%20raw%20data.%20Mainstream%20FL%0Amethodologies%20overlook%20the%20dynamic%20nature%20of%20real-world%20data%2C%20particularly%20its%0Atendency%20to%20grow%20in%20volume%20and%20diversify%20in%20classes%20over%20time.%20This%20oversight%0Aresults%20in%20FL%20methods%20suffering%20from%20catastrophic%20forgetting%2C%20where%20the%20trained%0Amodels%20inadvertently%20discard%20previously%20learned%20information%20upon%20assimilating%0Anew%20data.%20In%20response%20to%20this%20challenge%2C%20we%20propose%20a%20novel%20Federated%0AClass-Incremental%20Learning%20%28FCIL%29%20method%2C%20named%20%5Cunderline%7BFed%7Derated%0A%5Cunderline%7BC%7Dlass-Incremental%20%5Cunderline%7BL%7Dearning%20with%20New-Class%0A%5Cunderline%7BA%7Dugmented%20%5Cunderline%7BS%7Delf-Di%5Cunderline%7BS%7Dtillation%20%28FedCLASS%29.%20The%0Acore%20of%20FedCLASS%20is%20to%20enrich%20the%20class%20scores%20of%20historical%20models%20with%20new%0Aclass%20scores%20predicted%20by%20current%20models%20and%20utilize%20the%20combined%20knowledge%20for%0Aself-distillation%2C%20enabling%20a%20more%20sufficient%20and%20precise%20knowledge%20transfer%0Afrom%20historical%20models%20to%20current%20models.%20Theoretical%20analyses%20demonstrate%20that%0AFedCLASS%20stands%20on%20reliable%20foundations%2C%20considering%20scores%20of%20old%20classes%0Apredicted%20by%20historical%20models%20as%20conditional%20probabilities%20in%20the%20absence%20of%0Anew%20classes%2C%20and%20the%20scores%20of%20new%20classes%20predicted%20by%20current%20models%20as%20the%0Aconditional%20probabilities%20of%20class%20scores%20derived%20from%20historical%20models.%0AEmpirical%20experiments%20demonstrate%20the%20superiority%20of%20FedCLASS%20over%20four%0Abaseline%20algorithms%20in%20reducing%20average%20forgetting%20rate%20and%20boosting%20global%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00622v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Class-Incremental%20Learning%20with%20New-Class%20Augmented%0A%20%20Self-Distillation&entry.906535625=Zhiyuan%20Wu%20and%20Tianliu%20He%20and%20Sheng%20Sun%20and%20Yuwei%20Wang%20and%20Min%20Liu%20and%20Bo%20Gao%20and%20Xuefeng%20Jiang&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20among%0Aparticipants%20while%20guaranteeing%20the%20privacy%20of%20raw%20data.%20Mainstream%20FL%0Amethodologies%20overlook%20the%20dynamic%20nature%20of%20real-world%20data%2C%20particularly%20its%0Atendency%20to%20grow%20in%20volume%20and%20diversify%20in%20classes%20over%20time.%20This%20oversight%0Aresults%20in%20FL%20methods%20suffering%20from%20catastrophic%20forgetting%2C%20where%20the%20trained%0Amodels%20inadvertently%20discard%20previously%20learned%20information%20upon%20assimilating%0Anew%20data.%20In%20response%20to%20this%20challenge%2C%20we%20propose%20a%20novel%20Federated%0AClass-Incremental%20Learning%20%28FCIL%29%20method%2C%20named%20%5Cunderline%7BFed%7Derated%0A%5Cunderline%7BC%7Dlass-Incremental%20%5Cunderline%7BL%7Dearning%20with%20New-Class%0A%5Cunderline%7BA%7Dugmented%20%5Cunderline%7BS%7Delf-Di%5Cunderline%7BS%7Dtillation%20%28FedCLASS%29.%20The%0Acore%20of%20FedCLASS%20is%20to%20enrich%20the%20class%20scores%20of%20historical%20models%20with%20new%0Aclass%20scores%20predicted%20by%20current%20models%20and%20utilize%20the%20combined%20knowledge%20for%0Aself-distillation%2C%20enabling%20a%20more%20sufficient%20and%20precise%20knowledge%20transfer%0Afrom%20historical%20models%20to%20current%20models.%20Theoretical%20analyses%20demonstrate%20that%0AFedCLASS%20stands%20on%20reliable%20foundations%2C%20considering%20scores%20of%20old%20classes%0Apredicted%20by%20historical%20models%20as%20conditional%20probabilities%20in%20the%20absence%20of%0Anew%20classes%2C%20and%20the%20scores%20of%20new%20classes%20predicted%20by%20current%20models%20as%20the%0Aconditional%20probabilities%20of%20class%20scores%20derived%20from%20historical%20models.%0AEmpirical%20experiments%20demonstrate%20the%20superiority%20of%20FedCLASS%20over%20four%0Abaseline%20algorithms%20in%20reducing%20average%20forgetting%20rate%20and%20boosting%20global%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00622v3&entry.124074799=Read"},
{"title": "Leave No One Behind: Online Self-Supervised Self-Distillation for\n  Sequential Recommendation", "author": "Shaowei Wei and Zhengwei Wu and Xin Li and Qintong Wu and Zhiqiang Zhang and Jun Zhou and Lihong Gu and Jinjie Gu", "abstract": "  Sequential recommendation methods play a pivotal role in modern\nrecommendation systems. A key challenge lies in accurately modeling user\npreferences in the face of data sparsity. To tackle this challenge, recent\nmethods leverage contrastive learning (CL) to derive self-supervision signals\nby maximizing the mutual information of two augmented views of the original\nuser behavior sequence. Despite their effectiveness, CL-based methods encounter\na limitation in fully exploiting self-supervision signals for users with\nlimited behavior data, as users with extensive behaviors naturally offer more\ninformation. To address this problem, we introduce a novel learning paradigm,\nnamed Online Self-Supervised Self-distillation for Sequential Recommendation\n($S^4$Rec), effectively bridging the gap between self-supervised learning and\nself-distillation methods. Specifically, we employ online clustering to\nproficiently group users by their distinct latent intents. Additionally, an\nadversarial learning strategy is utilized to ensure that the clustering\nprocedure is not affected by the behavior length factor. Subsequently, we\nemploy self-distillation to facilitate the transfer of knowledge from users\nwith extensive behaviors (teachers) to users with limited behaviors (students).\nExperiments conducted on four real-world datasets validate the effectiveness of\nthe proposed method.\n", "link": "http://arxiv.org/abs/2404.07219v2", "date": "2024-04-17", "relevancy": 2.4464, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5041}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.482}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4817}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Leave%20No%20One%20Behind%3A%20Online%20Self-Supervised%20Self-Distillation%20for%0A%20%20Sequential%20Recommendation&body=Title%3A%20Leave%20No%20One%20Behind%3A%20Online%20Self-Supervised%20Self-Distillation%20for%0A%20%20Sequential%20Recommendation%0AAuthor%3A%20Shaowei%20Wei%20and%20Zhengwei%20Wu%20and%20Xin%20Li%20and%20Qintong%20Wu%20and%20Zhiqiang%20Zhang%20and%20Jun%20Zhou%20and%20Lihong%20Gu%20and%20Jinjie%20Gu%0AAbstract%3A%20%20%20Sequential%20recommendation%20methods%20play%20a%20pivotal%20role%20in%20modern%0Arecommendation%20systems.%20A%20key%20challenge%20lies%20in%20accurately%20modeling%20user%0Apreferences%20in%20the%20face%20of%20data%20sparsity.%20To%20tackle%20this%20challenge%2C%20recent%0Amethods%20leverage%20contrastive%20learning%20%28CL%29%20to%20derive%20self-supervision%20signals%0Aby%20maximizing%20the%20mutual%20information%20of%20two%20augmented%20views%20of%20the%20original%0Auser%20behavior%20sequence.%20Despite%20their%20effectiveness%2C%20CL-based%20methods%20encounter%0Aa%20limitation%20in%20fully%20exploiting%20self-supervision%20signals%20for%20users%20with%0Alimited%20behavior%20data%2C%20as%20users%20with%20extensive%20behaviors%20naturally%20offer%20more%0Ainformation.%20To%20address%20this%20problem%2C%20we%20introduce%20a%20novel%20learning%20paradigm%2C%0Anamed%20Online%20Self-Supervised%20Self-distillation%20for%20Sequential%20Recommendation%0A%28%24S%5E4%24Rec%29%2C%20effectively%20bridging%20the%20gap%20between%20self-supervised%20learning%20and%0Aself-distillation%20methods.%20Specifically%2C%20we%20employ%20online%20clustering%20to%0Aproficiently%20group%20users%20by%20their%20distinct%20latent%20intents.%20Additionally%2C%20an%0Aadversarial%20learning%20strategy%20is%20utilized%20to%20ensure%20that%20the%20clustering%0Aprocedure%20is%20not%20affected%20by%20the%20behavior%20length%20factor.%20Subsequently%2C%20we%0Aemploy%20self-distillation%20to%20facilitate%20the%20transfer%20of%20knowledge%20from%20users%0Awith%20extensive%20behaviors%20%28teachers%29%20to%20users%20with%20limited%20behaviors%20%28students%29.%0AExperiments%20conducted%20on%20four%20real-world%20datasets%20validate%20the%20effectiveness%20of%0Athe%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07219v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leave%20No%20One%20Behind%3A%20Online%20Self-Supervised%20Self-Distillation%20for%0A%20%20Sequential%20Recommendation&entry.906535625=Shaowei%20Wei%20and%20Zhengwei%20Wu%20and%20Xin%20Li%20and%20Qintong%20Wu%20and%20Zhiqiang%20Zhang%20and%20Jun%20Zhou%20and%20Lihong%20Gu%20and%20Jinjie%20Gu&entry.1292438233=%20%20Sequential%20recommendation%20methods%20play%20a%20pivotal%20role%20in%20modern%0Arecommendation%20systems.%20A%20key%20challenge%20lies%20in%20accurately%20modeling%20user%0Apreferences%20in%20the%20face%20of%20data%20sparsity.%20To%20tackle%20this%20challenge%2C%20recent%0Amethods%20leverage%20contrastive%20learning%20%28CL%29%20to%20derive%20self-supervision%20signals%0Aby%20maximizing%20the%20mutual%20information%20of%20two%20augmented%20views%20of%20the%20original%0Auser%20behavior%20sequence.%20Despite%20their%20effectiveness%2C%20CL-based%20methods%20encounter%0Aa%20limitation%20in%20fully%20exploiting%20self-supervision%20signals%20for%20users%20with%0Alimited%20behavior%20data%2C%20as%20users%20with%20extensive%20behaviors%20naturally%20offer%20more%0Ainformation.%20To%20address%20this%20problem%2C%20we%20introduce%20a%20novel%20learning%20paradigm%2C%0Anamed%20Online%20Self-Supervised%20Self-distillation%20for%20Sequential%20Recommendation%0A%28%24S%5E4%24Rec%29%2C%20effectively%20bridging%20the%20gap%20between%20self-supervised%20learning%20and%0Aself-distillation%20methods.%20Specifically%2C%20we%20employ%20online%20clustering%20to%0Aproficiently%20group%20users%20by%20their%20distinct%20latent%20intents.%20Additionally%2C%20an%0Aadversarial%20learning%20strategy%20is%20utilized%20to%20ensure%20that%20the%20clustering%0Aprocedure%20is%20not%20affected%20by%20the%20behavior%20length%20factor.%20Subsequently%2C%20we%0Aemploy%20self-distillation%20to%20facilitate%20the%20transfer%20of%20knowledge%20from%20users%0Awith%20extensive%20behaviors%20%28teachers%29%20to%20users%20with%20limited%20behaviors%20%28students%29.%0AExperiments%20conducted%20on%20four%20real-world%20datasets%20validate%20the%20effectiveness%20of%0Athe%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07219v2&entry.124074799=Read"},
{"title": "D$^2$ST-Adapter: Disentangled-and-Deformable Spatio-Temporal Adapter for\n  Few-shot Action Recognition", "author": "Wenjie Pei and Qizhong Tan and Guangming Lu and Jiandong Tian", "abstract": "  Adapting large pre-trained image models to few-shot action recognition has\nproven to be an effective and efficient strategy for learning robust feature\nextractors, which is essential for few-shot learning. Typical fine-tuning based\nadaptation paradigm is prone to overfitting in the few-shot learning scenarios\nand offers little modeling flexibility for learning temporal features in video\ndata. In this work we present the Disentangled-and-Deformable Spatio-Temporal\nAdapter (D$^2$ST-Adapter), which is a novel adapter tuning framework\nwell-suited for few-shot action recognition due to lightweight design and low\nparameter-learning overhead. It is designed in a dual-pathway architecture to\nencode spatial and temporal features in a disentangled manner. In particular,\nwe devise the anisotropic Deformable Spatio-Temporal Attention module as the\ncore component of D$^2$ST-Adapter, which can be tailored with anisotropic\nsampling densities along spatial and temporal domains to learn spatial and\ntemporal features specifically in corresponding pathways, allowing our\nD$^2$ST-Adapter to encode features in a global view in 3D spatio-temporal space\nwhile maintaining a lightweight design. Extensive experiments with\ninstantiations of our method on both pre-trained ResNet and ViT demonstrate the\nsuperiority of our method over state-of-the-art methods for few-shot action\nrecognition. Our method is particularly well-suited to challenging scenarios\nwhere temporal dynamics are critical for action recognition.\n", "link": "http://arxiv.org/abs/2312.01431v2", "date": "2024-04-17", "relevancy": 2.4247, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6296}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5953}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5748}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20D%24%5E2%24ST-Adapter%3A%20Disentangled-and-Deformable%20Spatio-Temporal%20Adapter%20for%0A%20%20Few-shot%20Action%20Recognition&body=Title%3A%20D%24%5E2%24ST-Adapter%3A%20Disentangled-and-Deformable%20Spatio-Temporal%20Adapter%20for%0A%20%20Few-shot%20Action%20Recognition%0AAuthor%3A%20Wenjie%20Pei%20and%20Qizhong%20Tan%20and%20Guangming%20Lu%20and%20Jiandong%20Tian%0AAbstract%3A%20%20%20Adapting%20large%20pre-trained%20image%20models%20to%20few-shot%20action%20recognition%20has%0Aproven%20to%20be%20an%20effective%20and%20efficient%20strategy%20for%20learning%20robust%20feature%0Aextractors%2C%20which%20is%20essential%20for%20few-shot%20learning.%20Typical%20fine-tuning%20based%0Aadaptation%20paradigm%20is%20prone%20to%20overfitting%20in%20the%20few-shot%20learning%20scenarios%0Aand%20offers%20little%20modeling%20flexibility%20for%20learning%20temporal%20features%20in%20video%0Adata.%20In%20this%20work%20we%20present%20the%20Disentangled-and-Deformable%20Spatio-Temporal%0AAdapter%20%28D%24%5E2%24ST-Adapter%29%2C%20which%20is%20a%20novel%20adapter%20tuning%20framework%0Awell-suited%20for%20few-shot%20action%20recognition%20due%20to%20lightweight%20design%20and%20low%0Aparameter-learning%20overhead.%20It%20is%20designed%20in%20a%20dual-pathway%20architecture%20to%0Aencode%20spatial%20and%20temporal%20features%20in%20a%20disentangled%20manner.%20In%20particular%2C%0Awe%20devise%20the%20anisotropic%20Deformable%20Spatio-Temporal%20Attention%20module%20as%20the%0Acore%20component%20of%20D%24%5E2%24ST-Adapter%2C%20which%20can%20be%20tailored%20with%20anisotropic%0Asampling%20densities%20along%20spatial%20and%20temporal%20domains%20to%20learn%20spatial%20and%0Atemporal%20features%20specifically%20in%20corresponding%20pathways%2C%20allowing%20our%0AD%24%5E2%24ST-Adapter%20to%20encode%20features%20in%20a%20global%20view%20in%203D%20spatio-temporal%20space%0Awhile%20maintaining%20a%20lightweight%20design.%20Extensive%20experiments%20with%0Ainstantiations%20of%20our%20method%20on%20both%20pre-trained%20ResNet%20and%20ViT%20demonstrate%20the%0Asuperiority%20of%20our%20method%20over%20state-of-the-art%20methods%20for%20few-shot%20action%0Arecognition.%20Our%20method%20is%20particularly%20well-suited%20to%20challenging%20scenarios%0Awhere%20temporal%20dynamics%20are%20critical%20for%20action%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01431v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D%24%5E2%24ST-Adapter%3A%20Disentangled-and-Deformable%20Spatio-Temporal%20Adapter%20for%0A%20%20Few-shot%20Action%20Recognition&entry.906535625=Wenjie%20Pei%20and%20Qizhong%20Tan%20and%20Guangming%20Lu%20and%20Jiandong%20Tian&entry.1292438233=%20%20Adapting%20large%20pre-trained%20image%20models%20to%20few-shot%20action%20recognition%20has%0Aproven%20to%20be%20an%20effective%20and%20efficient%20strategy%20for%20learning%20robust%20feature%0Aextractors%2C%20which%20is%20essential%20for%20few-shot%20learning.%20Typical%20fine-tuning%20based%0Aadaptation%20paradigm%20is%20prone%20to%20overfitting%20in%20the%20few-shot%20learning%20scenarios%0Aand%20offers%20little%20modeling%20flexibility%20for%20learning%20temporal%20features%20in%20video%0Adata.%20In%20this%20work%20we%20present%20the%20Disentangled-and-Deformable%20Spatio-Temporal%0AAdapter%20%28D%24%5E2%24ST-Adapter%29%2C%20which%20is%20a%20novel%20adapter%20tuning%20framework%0Awell-suited%20for%20few-shot%20action%20recognition%20due%20to%20lightweight%20design%20and%20low%0Aparameter-learning%20overhead.%20It%20is%20designed%20in%20a%20dual-pathway%20architecture%20to%0Aencode%20spatial%20and%20temporal%20features%20in%20a%20disentangled%20manner.%20In%20particular%2C%0Awe%20devise%20the%20anisotropic%20Deformable%20Spatio-Temporal%20Attention%20module%20as%20the%0Acore%20component%20of%20D%24%5E2%24ST-Adapter%2C%20which%20can%20be%20tailored%20with%20anisotropic%0Asampling%20densities%20along%20spatial%20and%20temporal%20domains%20to%20learn%20spatial%20and%0Atemporal%20features%20specifically%20in%20corresponding%20pathways%2C%20allowing%20our%0AD%24%5E2%24ST-Adapter%20to%20encode%20features%20in%20a%20global%20view%20in%203D%20spatio-temporal%20space%0Awhile%20maintaining%20a%20lightweight%20design.%20Extensive%20experiments%20with%0Ainstantiations%20of%20our%20method%20on%20both%20pre-trained%20ResNet%20and%20ViT%20demonstrate%20the%0Asuperiority%20of%20our%20method%20over%20state-of-the-art%20methods%20for%20few-shot%20action%0Arecognition.%20Our%20method%20is%20particularly%20well-suited%20to%20challenging%20scenarios%0Awhere%20temporal%20dynamics%20are%20critical%20for%20action%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01431v2&entry.124074799=Read"},
{"title": "A Progressive Framework of Vision-language Knowledge Distillation and\n  Alignment for Multilingual Scene", "author": "Wenbo Zhang and Yifan Zhang and Jianfeng Lin and Binqiang Huang and Jinlu Zhang and Wenhao Yu", "abstract": "  Pre-trained vision-language (V-L) models such as CLIP have shown excellent\nperformance in many downstream cross-modal tasks. However, most of them are\nonly applicable to the English context. Subsequent research has focused on this\nproblem and proposed improved models, such as CN-CLIP and AltCLIP, to\nfacilitate their applicability to Chinese and even other languages.\nNevertheless, these models suffer from high latency and a large memory\nfootprint in inference, which limits their further deployment on\nresource-constrained edge devices. In this work, we propose a conceptually\nsimple yet effective multilingual CLIP Compression framework and train a\nlightweight multilingual vision-language model, called DC-CLIP, for both\nChinese and English context. In this framework, we collect high-quality Chinese\nand English text-image pairs and design two training stages, including\nmultilingual vision-language feature distillation and alignment. During the\nfirst stage, lightweight image/text student models are designed to learn robust\nvisual/multilingual textual feature representation ability from corresponding\nteacher models, respectively. Subsequently, the multilingual vision-language\nalignment stage enables effective alignment of visual and multilingual textual\nfeatures to further improve the model's multilingual performance. Comprehensive\nexperiments in zero-shot image classification, conducted based on the ELEVATER\nbenchmark, showcase that DC-CLIP achieves superior performance in the English\ncontext and competitive performance in the Chinese context, even with less\ntraining data, when compared to existing models of similar parameter magnitude.\nThe evaluation demonstrates the effectiveness of our designed training\nmechanism.\n", "link": "http://arxiv.org/abs/2404.11249v1", "date": "2024-04-17", "relevancy": 2.3719, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6276}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5752}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5509}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Progressive%20Framework%20of%20Vision-language%20Knowledge%20Distillation%20and%0A%20%20Alignment%20for%20Multilingual%20Scene&body=Title%3A%20A%20Progressive%20Framework%20of%20Vision-language%20Knowledge%20Distillation%20and%0A%20%20Alignment%20for%20Multilingual%20Scene%0AAuthor%3A%20Wenbo%20Zhang%20and%20Yifan%20Zhang%20and%20Jianfeng%20Lin%20and%20Binqiang%20Huang%20and%20Jinlu%20Zhang%20and%20Wenhao%20Yu%0AAbstract%3A%20%20%20Pre-trained%20vision-language%20%28V-L%29%20models%20such%20as%20CLIP%20have%20shown%20excellent%0Aperformance%20in%20many%20downstream%20cross-modal%20tasks.%20However%2C%20most%20of%20them%20are%0Aonly%20applicable%20to%20the%20English%20context.%20Subsequent%20research%20has%20focused%20on%20this%0Aproblem%20and%20proposed%20improved%20models%2C%20such%20as%20CN-CLIP%20and%20AltCLIP%2C%20to%0Afacilitate%20their%20applicability%20to%20Chinese%20and%20even%20other%20languages.%0ANevertheless%2C%20these%20models%20suffer%20from%20high%20latency%20and%20a%20large%20memory%0Afootprint%20in%20inference%2C%20which%20limits%20their%20further%20deployment%20on%0Aresource-constrained%20edge%20devices.%20In%20this%20work%2C%20we%20propose%20a%20conceptually%0Asimple%20yet%20effective%20multilingual%20CLIP%20Compression%20framework%20and%20train%20a%0Alightweight%20multilingual%20vision-language%20model%2C%20called%20DC-CLIP%2C%20for%20both%0AChinese%20and%20English%20context.%20In%20this%20framework%2C%20we%20collect%20high-quality%20Chinese%0Aand%20English%20text-image%20pairs%20and%20design%20two%20training%20stages%2C%20including%0Amultilingual%20vision-language%20feature%20distillation%20and%20alignment.%20During%20the%0Afirst%20stage%2C%20lightweight%20image/text%20student%20models%20are%20designed%20to%20learn%20robust%0Avisual/multilingual%20textual%20feature%20representation%20ability%20from%20corresponding%0Ateacher%20models%2C%20respectively.%20Subsequently%2C%20the%20multilingual%20vision-language%0Aalignment%20stage%20enables%20effective%20alignment%20of%20visual%20and%20multilingual%20textual%0Afeatures%20to%20further%20improve%20the%20model%27s%20multilingual%20performance.%20Comprehensive%0Aexperiments%20in%20zero-shot%20image%20classification%2C%20conducted%20based%20on%20the%20ELEVATER%0Abenchmark%2C%20showcase%20that%20DC-CLIP%20achieves%20superior%20performance%20in%20the%20English%0Acontext%20and%20competitive%20performance%20in%20the%20Chinese%20context%2C%20even%20with%20less%0Atraining%20data%2C%20when%20compared%20to%20existing%20models%20of%20similar%20parameter%20magnitude.%0AThe%20evaluation%20demonstrates%20the%20effectiveness%20of%20our%20designed%20training%0Amechanism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11249v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Progressive%20Framework%20of%20Vision-language%20Knowledge%20Distillation%20and%0A%20%20Alignment%20for%20Multilingual%20Scene&entry.906535625=Wenbo%20Zhang%20and%20Yifan%20Zhang%20and%20Jianfeng%20Lin%20and%20Binqiang%20Huang%20and%20Jinlu%20Zhang%20and%20Wenhao%20Yu&entry.1292438233=%20%20Pre-trained%20vision-language%20%28V-L%29%20models%20such%20as%20CLIP%20have%20shown%20excellent%0Aperformance%20in%20many%20downstream%20cross-modal%20tasks.%20However%2C%20most%20of%20them%20are%0Aonly%20applicable%20to%20the%20English%20context.%20Subsequent%20research%20has%20focused%20on%20this%0Aproblem%20and%20proposed%20improved%20models%2C%20such%20as%20CN-CLIP%20and%20AltCLIP%2C%20to%0Afacilitate%20their%20applicability%20to%20Chinese%20and%20even%20other%20languages.%0ANevertheless%2C%20these%20models%20suffer%20from%20high%20latency%20and%20a%20large%20memory%0Afootprint%20in%20inference%2C%20which%20limits%20their%20further%20deployment%20on%0Aresource-constrained%20edge%20devices.%20In%20this%20work%2C%20we%20propose%20a%20conceptually%0Asimple%20yet%20effective%20multilingual%20CLIP%20Compression%20framework%20and%20train%20a%0Alightweight%20multilingual%20vision-language%20model%2C%20called%20DC-CLIP%2C%20for%20both%0AChinese%20and%20English%20context.%20In%20this%20framework%2C%20we%20collect%20high-quality%20Chinese%0Aand%20English%20text-image%20pairs%20and%20design%20two%20training%20stages%2C%20including%0Amultilingual%20vision-language%20feature%20distillation%20and%20alignment.%20During%20the%0Afirst%20stage%2C%20lightweight%20image/text%20student%20models%20are%20designed%20to%20learn%20robust%0Avisual/multilingual%20textual%20feature%20representation%20ability%20from%20corresponding%0Ateacher%20models%2C%20respectively.%20Subsequently%2C%20the%20multilingual%20vision-language%0Aalignment%20stage%20enables%20effective%20alignment%20of%20visual%20and%20multilingual%20textual%0Afeatures%20to%20further%20improve%20the%20model%27s%20multilingual%20performance.%20Comprehensive%0Aexperiments%20in%20zero-shot%20image%20classification%2C%20conducted%20based%20on%20the%20ELEVATER%0Abenchmark%2C%20showcase%20that%20DC-CLIP%20achieves%20superior%20performance%20in%20the%20English%0Acontext%20and%20competitive%20performance%20in%20the%20Chinese%20context%2C%20even%20with%20less%0Atraining%20data%2C%20when%20compared%20to%20existing%20models%20of%20similar%20parameter%20magnitude.%0AThe%20evaluation%20demonstrates%20the%20effectiveness%20of%20our%20designed%20training%0Amechanism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11249v1&entry.124074799=Read"},
{"title": "HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive\n  Speech Detection via Large Language Models", "author": "Huy Nghiem and Hal Daum\u00e9 III", "abstract": "  The ubiquitousness of social media has led to the need for reliable and\nefficient detection of offensive content to limit harmful effects. This has led\nto a proliferation of datasets and models related to detecting offensive\ncontent. While sophisticated models have attained strong performance on\nindividual datasets, these models often do not generalize due to differences\nbetween how \"offensive content\" is conceptualized, and the resulting\ndifferences in how these datasets are labeled. In this paper, we introduce\nHateCOT, a dataset of 52,000 samples drawn from diverse existing sources with\nexplanations generated by GPT-3.5-Turbo and human-curated. We show that\npre-training models for the detection of offensive content on HateCOT\nsignificantly boots open-sourced Language Models on three benchmark datasets in\nboth zero and few-shot settings, despite differences in domain and task.} We\nfurther find that HateCOT enables effective K-shot fine-tuning in the\nlow-resource settings.\n", "link": "http://arxiv.org/abs/2403.11456v2", "date": "2024-04-17", "relevancy": 2.3689, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4843}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.475}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4621}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HateCOT%3A%20An%20Explanation-Enhanced%20Dataset%20for%20Generalizable%20Offensive%0A%20%20Speech%20Detection%20via%20Large%20Language%20Models&body=Title%3A%20HateCOT%3A%20An%20Explanation-Enhanced%20Dataset%20for%20Generalizable%20Offensive%0A%20%20Speech%20Detection%20via%20Large%20Language%20Models%0AAuthor%3A%20Huy%20Nghiem%20and%20Hal%20Daum%C3%A9%20III%0AAbstract%3A%20%20%20The%20ubiquitousness%20of%20social%20media%20has%20led%20to%20the%20need%20for%20reliable%20and%0Aefficient%20detection%20of%20offensive%20content%20to%20limit%20harmful%20effects.%20This%20has%20led%0Ato%20a%20proliferation%20of%20datasets%20and%20models%20related%20to%20detecting%20offensive%0Acontent.%20While%20sophisticated%20models%20have%20attained%20strong%20performance%20on%0Aindividual%20datasets%2C%20these%20models%20often%20do%20not%20generalize%20due%20to%20differences%0Abetween%20how%20%22offensive%20content%22%20is%20conceptualized%2C%20and%20the%20resulting%0Adifferences%20in%20how%20these%20datasets%20are%20labeled.%20In%20this%20paper%2C%20we%20introduce%0AHateCOT%2C%20a%20dataset%20of%2052%2C000%20samples%20drawn%20from%20diverse%20existing%20sources%20with%0Aexplanations%20generated%20by%20GPT-3.5-Turbo%20and%20human-curated.%20We%20show%20that%0Apre-training%20models%20for%20the%20detection%20of%20offensive%20content%20on%20HateCOT%0Asignificantly%20boots%20open-sourced%20Language%20Models%20on%20three%20benchmark%20datasets%20in%0Aboth%20zero%20and%20few-shot%20settings%2C%20despite%20differences%20in%20domain%20and%20task.%7D%20We%0Afurther%20find%20that%20HateCOT%20enables%20effective%20K-shot%20fine-tuning%20in%20the%0Alow-resource%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11456v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HateCOT%3A%20An%20Explanation-Enhanced%20Dataset%20for%20Generalizable%20Offensive%0A%20%20Speech%20Detection%20via%20Large%20Language%20Models&entry.906535625=Huy%20Nghiem%20and%20Hal%20Daum%C3%A9%20III&entry.1292438233=%20%20The%20ubiquitousness%20of%20social%20media%20has%20led%20to%20the%20need%20for%20reliable%20and%0Aefficient%20detection%20of%20offensive%20content%20to%20limit%20harmful%20effects.%20This%20has%20led%0Ato%20a%20proliferation%20of%20datasets%20and%20models%20related%20to%20detecting%20offensive%0Acontent.%20While%20sophisticated%20models%20have%20attained%20strong%20performance%20on%0Aindividual%20datasets%2C%20these%20models%20often%20do%20not%20generalize%20due%20to%20differences%0Abetween%20how%20%22offensive%20content%22%20is%20conceptualized%2C%20and%20the%20resulting%0Adifferences%20in%20how%20these%20datasets%20are%20labeled.%20In%20this%20paper%2C%20we%20introduce%0AHateCOT%2C%20a%20dataset%20of%2052%2C000%20samples%20drawn%20from%20diverse%20existing%20sources%20with%0Aexplanations%20generated%20by%20GPT-3.5-Turbo%20and%20human-curated.%20We%20show%20that%0Apre-training%20models%20for%20the%20detection%20of%20offensive%20content%20on%20HateCOT%0Asignificantly%20boots%20open-sourced%20Language%20Models%20on%20three%20benchmark%20datasets%20in%0Aboth%20zero%20and%20few-shot%20settings%2C%20despite%20differences%20in%20domain%20and%20task.%7D%20We%0Afurther%20find%20that%20HateCOT%20enables%20effective%20K-shot%20fine-tuning%20in%20the%0Alow-resource%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11456v2&entry.124074799=Read"},
{"title": "Exploring Missing Modality in Multimodal Egocentric Datasets", "author": "Merey Ramazanova and Alejandro Pardo and Humam Alwassel and Bernard Ghanem", "abstract": "  Multimodal video understanding is crucial for analyzing egocentric videos,\nwhere integrating multiple sensory signals significantly enhances action\nrecognition and moment localization. However, practical applications often\ngrapple with incomplete modalities due to factors like privacy concerns,\nefficiency demands, or hardware malfunctions. Addressing this, our study delves\ninto the impact of missing modalities on egocentric action recognition,\nparticularly within transformer-based models. We introduce a novel concept\n-Missing Modality Token (MMT)-to maintain performance even when modalities are\nabsent, a strategy that proves effective in the Ego4D, Epic-Kitchens, and\nEpic-Sounds datasets. Our method mitigates the performance loss, reducing it\nfrom its original $\\sim 30\\%$ drop to only $\\sim 10\\%$ when half of the test\nset is modal-incomplete. Through extensive experimentation, we demonstrate the\nadaptability of MMT to different training scenarios and its superiority in\nhandling missing modalities compared to current methods. Our research\ncontributes a comprehensive analysis and an innovative approach, opening\navenues for more resilient multimodal systems in real-world settings.\n", "link": "http://arxiv.org/abs/2401.11470v2", "date": "2024-04-17", "relevancy": 2.3053, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5879}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5692}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5676}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploring%20Missing%20Modality%20in%20Multimodal%20Egocentric%20Datasets&body=Title%3A%20Exploring%20Missing%20Modality%20in%20Multimodal%20Egocentric%20Datasets%0AAuthor%3A%20Merey%20Ramazanova%20and%20Alejandro%20Pardo%20and%20Humam%20Alwassel%20and%20Bernard%20Ghanem%0AAbstract%3A%20%20%20Multimodal%20video%20understanding%20is%20crucial%20for%20analyzing%20egocentric%20videos%2C%0Awhere%20integrating%20multiple%20sensory%20signals%20significantly%20enhances%20action%0Arecognition%20and%20moment%20localization.%20However%2C%20practical%20applications%20often%0Agrapple%20with%20incomplete%20modalities%20due%20to%20factors%20like%20privacy%20concerns%2C%0Aefficiency%20demands%2C%20or%20hardware%20malfunctions.%20Addressing%20this%2C%20our%20study%20delves%0Ainto%20the%20impact%20of%20missing%20modalities%20on%20egocentric%20action%20recognition%2C%0Aparticularly%20within%20transformer-based%20models.%20We%20introduce%20a%20novel%20concept%0A-Missing%20Modality%20Token%20%28MMT%29-to%20maintain%20performance%20even%20when%20modalities%20are%0Aabsent%2C%20a%20strategy%20that%20proves%20effective%20in%20the%20Ego4D%2C%20Epic-Kitchens%2C%20and%0AEpic-Sounds%20datasets.%20Our%20method%20mitigates%20the%20performance%20loss%2C%20reducing%20it%0Afrom%20its%20original%20%24%5Csim%2030%5C%25%24%20drop%20to%20only%20%24%5Csim%2010%5C%25%24%20when%20half%20of%20the%20test%0Aset%20is%20modal-incomplete.%20Through%20extensive%20experimentation%2C%20we%20demonstrate%20the%0Aadaptability%20of%20MMT%20to%20different%20training%20scenarios%20and%20its%20superiority%20in%0Ahandling%20missing%20modalities%20compared%20to%20current%20methods.%20Our%20research%0Acontributes%20a%20comprehensive%20analysis%20and%20an%20innovative%20approach%2C%20opening%0Aavenues%20for%20more%20resilient%20multimodal%20systems%20in%20real-world%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11470v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Missing%20Modality%20in%20Multimodal%20Egocentric%20Datasets&entry.906535625=Merey%20Ramazanova%20and%20Alejandro%20Pardo%20and%20Humam%20Alwassel%20and%20Bernard%20Ghanem&entry.1292438233=%20%20Multimodal%20video%20understanding%20is%20crucial%20for%20analyzing%20egocentric%20videos%2C%0Awhere%20integrating%20multiple%20sensory%20signals%20significantly%20enhances%20action%0Arecognition%20and%20moment%20localization.%20However%2C%20practical%20applications%20often%0Agrapple%20with%20incomplete%20modalities%20due%20to%20factors%20like%20privacy%20concerns%2C%0Aefficiency%20demands%2C%20or%20hardware%20malfunctions.%20Addressing%20this%2C%20our%20study%20delves%0Ainto%20the%20impact%20of%20missing%20modalities%20on%20egocentric%20action%20recognition%2C%0Aparticularly%20within%20transformer-based%20models.%20We%20introduce%20a%20novel%20concept%0A-Missing%20Modality%20Token%20%28MMT%29-to%20maintain%20performance%20even%20when%20modalities%20are%0Aabsent%2C%20a%20strategy%20that%20proves%20effective%20in%20the%20Ego4D%2C%20Epic-Kitchens%2C%20and%0AEpic-Sounds%20datasets.%20Our%20method%20mitigates%20the%20performance%20loss%2C%20reducing%20it%0Afrom%20its%20original%20%24%5Csim%2030%5C%25%24%20drop%20to%20only%20%24%5Csim%2010%5C%25%24%20when%20half%20of%20the%20test%0Aset%20is%20modal-incomplete.%20Through%20extensive%20experimentation%2C%20we%20demonstrate%20the%0Aadaptability%20of%20MMT%20to%20different%20training%20scenarios%20and%20its%20superiority%20in%0Ahandling%20missing%20modalities%20compared%20to%20current%20methods.%20Our%20research%0Acontributes%20a%20comprehensive%20analysis%20and%20an%20innovative%20approach%2C%20opening%0Aavenues%20for%20more%20resilient%20multimodal%20systems%20in%20real-world%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11470v2&entry.124074799=Read"},
{"title": "Inductive Cognitive Diagnosis for Fast Student Learning in Web-Based\n  Online Intelligent Education Systems", "author": "Shuo Liu and Junhao Shen and Hong Qian and Aimin Zhou", "abstract": "  Cognitive diagnosis aims to gauge students' mastery levels based on their\nresponse logs. Serving as a pivotal module in web-based online intelligent\neducation systems (WOIESs), it plays an upstream and fundamental role in\ndownstream tasks like learning item recommendation and computerized adaptive\ntesting. WOIESs are open learning environment where numerous new students\nconstantly register and complete exercises. In WOIESs, efficient cognitive\ndiagnosis is crucial to fast feedback and accelerating student learning.\nHowever, the existing cognitive diagnosis methods always employ intrinsically\ntransductive student-specific embeddings, which become slow and costly due to\nretraining when dealing with new students who are unseen during training. To\nthis end, this paper proposes an inductive cognitive diagnosis model (ICDM) for\nfast new students' mastery levels inference in WOIESs. Specifically, in ICDM,\nwe propose a novel student-centered graph (SCG). Rather than inferring mastery\nlevels through updating student-specific embedding, we derive the inductive\nmastery levels as the aggregated outcomes of students' neighbors in SCG.\nNamely, SCG enables to shift the task from finding the most suitable\nstudent-specific embedding that fits the response logs to finding the most\nsuitable representations for different node types in SCG, and the latter is\nmore efficient since it no longer requires retraining. To obtain this\nrepresentation, ICDM consists of a\nconstruction-aggregation-generation-transformation process to learn the final\nrepresentation of students, exercises and concepts. Extensive experiments\nacross real-world datasets show that, compared with the existing cognitive\ndiagnosis methods that are always transductive, ICDM is much more faster while\nmaintains the competitive inference performance for new students.\n", "link": "http://arxiv.org/abs/2404.11290v1", "date": "2024-04-17", "relevancy": 2.3023, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4692}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4605}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4517}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Inductive%20Cognitive%20Diagnosis%20for%20Fast%20Student%20Learning%20in%20Web-Based%0A%20%20Online%20Intelligent%20Education%20Systems&body=Title%3A%20Inductive%20Cognitive%20Diagnosis%20for%20Fast%20Student%20Learning%20in%20Web-Based%0A%20%20Online%20Intelligent%20Education%20Systems%0AAuthor%3A%20Shuo%20Liu%20and%20Junhao%20Shen%20and%20Hong%20Qian%20and%20Aimin%20Zhou%0AAbstract%3A%20%20%20Cognitive%20diagnosis%20aims%20to%20gauge%20students%27%20mastery%20levels%20based%20on%20their%0Aresponse%20logs.%20Serving%20as%20a%20pivotal%20module%20in%20web-based%20online%20intelligent%0Aeducation%20systems%20%28WOIESs%29%2C%20it%20plays%20an%20upstream%20and%20fundamental%20role%20in%0Adownstream%20tasks%20like%20learning%20item%20recommendation%20and%20computerized%20adaptive%0Atesting.%20WOIESs%20are%20open%20learning%20environment%20where%20numerous%20new%20students%0Aconstantly%20register%20and%20complete%20exercises.%20In%20WOIESs%2C%20efficient%20cognitive%0Adiagnosis%20is%20crucial%20to%20fast%20feedback%20and%20accelerating%20student%20learning.%0AHowever%2C%20the%20existing%20cognitive%20diagnosis%20methods%20always%20employ%20intrinsically%0Atransductive%20student-specific%20embeddings%2C%20which%20become%20slow%20and%20costly%20due%20to%0Aretraining%20when%20dealing%20with%20new%20students%20who%20are%20unseen%20during%20training.%20To%0Athis%20end%2C%20this%20paper%20proposes%20an%20inductive%20cognitive%20diagnosis%20model%20%28ICDM%29%20for%0Afast%20new%20students%27%20mastery%20levels%20inference%20in%20WOIESs.%20Specifically%2C%20in%20ICDM%2C%0Awe%20propose%20a%20novel%20student-centered%20graph%20%28SCG%29.%20Rather%20than%20inferring%20mastery%0Alevels%20through%20updating%20student-specific%20embedding%2C%20we%20derive%20the%20inductive%0Amastery%20levels%20as%20the%20aggregated%20outcomes%20of%20students%27%20neighbors%20in%20SCG.%0ANamely%2C%20SCG%20enables%20to%20shift%20the%20task%20from%20finding%20the%20most%20suitable%0Astudent-specific%20embedding%20that%20fits%20the%20response%20logs%20to%20finding%20the%20most%0Asuitable%20representations%20for%20different%20node%20types%20in%20SCG%2C%20and%20the%20latter%20is%0Amore%20efficient%20since%20it%20no%20longer%20requires%20retraining.%20To%20obtain%20this%0Arepresentation%2C%20ICDM%20consists%20of%20a%0Aconstruction-aggregation-generation-transformation%20process%20to%20learn%20the%20final%0Arepresentation%20of%20students%2C%20exercises%20and%20concepts.%20Extensive%20experiments%0Aacross%20real-world%20datasets%20show%20that%2C%20compared%20with%20the%20existing%20cognitive%0Adiagnosis%20methods%20that%20are%20always%20transductive%2C%20ICDM%20is%20much%20more%20faster%20while%0Amaintains%20the%20competitive%20inference%20performance%20for%20new%20students.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11290v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inductive%20Cognitive%20Diagnosis%20for%20Fast%20Student%20Learning%20in%20Web-Based%0A%20%20Online%20Intelligent%20Education%20Systems&entry.906535625=Shuo%20Liu%20and%20Junhao%20Shen%20and%20Hong%20Qian%20and%20Aimin%20Zhou&entry.1292438233=%20%20Cognitive%20diagnosis%20aims%20to%20gauge%20students%27%20mastery%20levels%20based%20on%20their%0Aresponse%20logs.%20Serving%20as%20a%20pivotal%20module%20in%20web-based%20online%20intelligent%0Aeducation%20systems%20%28WOIESs%29%2C%20it%20plays%20an%20upstream%20and%20fundamental%20role%20in%0Adownstream%20tasks%20like%20learning%20item%20recommendation%20and%20computerized%20adaptive%0Atesting.%20WOIESs%20are%20open%20learning%20environment%20where%20numerous%20new%20students%0Aconstantly%20register%20and%20complete%20exercises.%20In%20WOIESs%2C%20efficient%20cognitive%0Adiagnosis%20is%20crucial%20to%20fast%20feedback%20and%20accelerating%20student%20learning.%0AHowever%2C%20the%20existing%20cognitive%20diagnosis%20methods%20always%20employ%20intrinsically%0Atransductive%20student-specific%20embeddings%2C%20which%20become%20slow%20and%20costly%20due%20to%0Aretraining%20when%20dealing%20with%20new%20students%20who%20are%20unseen%20during%20training.%20To%0Athis%20end%2C%20this%20paper%20proposes%20an%20inductive%20cognitive%20diagnosis%20model%20%28ICDM%29%20for%0Afast%20new%20students%27%20mastery%20levels%20inference%20in%20WOIESs.%20Specifically%2C%20in%20ICDM%2C%0Awe%20propose%20a%20novel%20student-centered%20graph%20%28SCG%29.%20Rather%20than%20inferring%20mastery%0Alevels%20through%20updating%20student-specific%20embedding%2C%20we%20derive%20the%20inductive%0Amastery%20levels%20as%20the%20aggregated%20outcomes%20of%20students%27%20neighbors%20in%20SCG.%0ANamely%2C%20SCG%20enables%20to%20shift%20the%20task%20from%20finding%20the%20most%20suitable%0Astudent-specific%20embedding%20that%20fits%20the%20response%20logs%20to%20finding%20the%20most%0Asuitable%20representations%20for%20different%20node%20types%20in%20SCG%2C%20and%20the%20latter%20is%0Amore%20efficient%20since%20it%20no%20longer%20requires%20retraining.%20To%20obtain%20this%0Arepresentation%2C%20ICDM%20consists%20of%20a%0Aconstruction-aggregation-generation-transformation%20process%20to%20learn%20the%20final%0Arepresentation%20of%20students%2C%20exercises%20and%20concepts.%20Extensive%20experiments%0Aacross%20real-world%20datasets%20show%20that%2C%20compared%20with%20the%20existing%20cognitive%0Adiagnosis%20methods%20that%20are%20always%20transductive%2C%20ICDM%20is%20much%20more%20faster%20while%0Amaintains%20the%20competitive%20inference%20performance%20for%20new%20students.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11290v1&entry.124074799=Read"},
{"title": "A Semantic Segmentation-guided Approach for Ground-to-Aerial Image\n  Matching", "author": "Francesco Pro and Nikolaos Dionelis and Luca Maiano and Bertrand Le Saux and Irene Amerini", "abstract": "  Nowadays the accurate geo-localization of ground-view images has an important\nrole across domains as diverse as journalism, forensics analysis, transports,\nand Earth Observation. This work addresses the problem of matching a query\nground-view image with the corresponding satellite image without GPS data. This\nis done by comparing the features from a ground-view image and a satellite one,\ninnovatively leveraging the corresponding latter's segmentation mask through a\nthree-stream Siamese-like network. The proposed method, Semantic Align Net\n(SAN), focuses on limited Field-of-View (FoV) and ground panorama images\n(images with a FoV of 360{\\deg}). The novelty lies in the fusion of satellite\nimages in combination with their semantic segmentation masks, aimed at ensuring\nthat the model can extract useful features and focus on the significant parts\nof the images. This work shows how SAN through semantic analysis of images\nimproves the performance on the unlabelled CVUSA dataset for all the tested\nFoVs.\n", "link": "http://arxiv.org/abs/2404.11302v1", "date": "2024-04-17", "relevancy": 2.2758, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5859}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5646}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5375}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Semantic%20Segmentation-guided%20Approach%20for%20Ground-to-Aerial%20Image%0A%20%20Matching&body=Title%3A%20A%20Semantic%20Segmentation-guided%20Approach%20for%20Ground-to-Aerial%20Image%0A%20%20Matching%0AAuthor%3A%20Francesco%20Pro%20and%20Nikolaos%20Dionelis%20and%20Luca%20Maiano%20and%20Bertrand%20Le%20Saux%20and%20Irene%20Amerini%0AAbstract%3A%20%20%20Nowadays%20the%20accurate%20geo-localization%20of%20ground-view%20images%20has%20an%20important%0Arole%20across%20domains%20as%20diverse%20as%20journalism%2C%20forensics%20analysis%2C%20transports%2C%0Aand%20Earth%20Observation.%20This%20work%20addresses%20the%20problem%20of%20matching%20a%20query%0Aground-view%20image%20with%20the%20corresponding%20satellite%20image%20without%20GPS%20data.%20This%0Ais%20done%20by%20comparing%20the%20features%20from%20a%20ground-view%20image%20and%20a%20satellite%20one%2C%0Ainnovatively%20leveraging%20the%20corresponding%20latter%27s%20segmentation%20mask%20through%20a%0Athree-stream%20Siamese-like%20network.%20The%20proposed%20method%2C%20Semantic%20Align%20Net%0A%28SAN%29%2C%20focuses%20on%20limited%20Field-of-View%20%28FoV%29%20and%20ground%20panorama%20images%0A%28images%20with%20a%20FoV%20of%20360%7B%5Cdeg%7D%29.%20The%20novelty%20lies%20in%20the%20fusion%20of%20satellite%0Aimages%20in%20combination%20with%20their%20semantic%20segmentation%20masks%2C%20aimed%20at%20ensuring%0Athat%20the%20model%20can%20extract%20useful%20features%20and%20focus%20on%20the%20significant%20parts%0Aof%20the%20images.%20This%20work%20shows%20how%20SAN%20through%20semantic%20analysis%20of%20images%0Aimproves%20the%20performance%20on%20the%20unlabelled%20CVUSA%20dataset%20for%20all%20the%20tested%0AFoVs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11302v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Semantic%20Segmentation-guided%20Approach%20for%20Ground-to-Aerial%20Image%0A%20%20Matching&entry.906535625=Francesco%20Pro%20and%20Nikolaos%20Dionelis%20and%20Luca%20Maiano%20and%20Bertrand%20Le%20Saux%20and%20Irene%20Amerini&entry.1292438233=%20%20Nowadays%20the%20accurate%20geo-localization%20of%20ground-view%20images%20has%20an%20important%0Arole%20across%20domains%20as%20diverse%20as%20journalism%2C%20forensics%20analysis%2C%20transports%2C%0Aand%20Earth%20Observation.%20This%20work%20addresses%20the%20problem%20of%20matching%20a%20query%0Aground-view%20image%20with%20the%20corresponding%20satellite%20image%20without%20GPS%20data.%20This%0Ais%20done%20by%20comparing%20the%20features%20from%20a%20ground-view%20image%20and%20a%20satellite%20one%2C%0Ainnovatively%20leveraging%20the%20corresponding%20latter%27s%20segmentation%20mask%20through%20a%0Athree-stream%20Siamese-like%20network.%20The%20proposed%20method%2C%20Semantic%20Align%20Net%0A%28SAN%29%2C%20focuses%20on%20limited%20Field-of-View%20%28FoV%29%20and%20ground%20panorama%20images%0A%28images%20with%20a%20FoV%20of%20360%7B%5Cdeg%7D%29.%20The%20novelty%20lies%20in%20the%20fusion%20of%20satellite%0Aimages%20in%20combination%20with%20their%20semantic%20segmentation%20masks%2C%20aimed%20at%20ensuring%0Athat%20the%20model%20can%20extract%20useful%20features%20and%20focus%20on%20the%20significant%20parts%0Aof%20the%20images.%20This%20work%20shows%20how%20SAN%20through%20semantic%20analysis%20of%20images%0Aimproves%20the%20performance%20on%20the%20unlabelled%20CVUSA%20dataset%20for%20all%20the%20tested%0AFoVs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11302v1&entry.124074799=Read"},
{"title": "Breaking the Heavy-Tailed Noise Barrier in Stochastic Optimization\n  Problems", "author": "Nikita Puchkin and Eduard Gorbunov and Nikolay Kutuzov and Alexander Gasnikov", "abstract": "  We consider stochastic optimization problems with heavy-tailed noise with\nstructured density. For such problems, we show that it is possible to get\nfaster rates of convergence than $\\mathcal{O}(K^{-2(\\alpha - 1)/\\alpha})$, when\nthe stochastic gradients have finite moments of order $\\alpha \\in (1, 2]$. In\nparticular, our analysis allows the noise norm to have an unbounded\nexpectation. To achieve these results, we stabilize stochastic gradients, using\nsmoothed medians of means. We prove that the resulting estimates have\nnegligible bias and controllable variance. This allows us to carefully\nincorporate them into clipped-SGD and clipped-SSTM and derive new\nhigh-probability complexity bounds in the considered setup.\n", "link": "http://arxiv.org/abs/2311.04161v2", "date": "2024-04-17", "relevancy": 2.2747, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4697}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4591}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.436}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20Heavy-Tailed%20Noise%20Barrier%20in%20Stochastic%20Optimization%0A%20%20Problems&body=Title%3A%20Breaking%20the%20Heavy-Tailed%20Noise%20Barrier%20in%20Stochastic%20Optimization%0A%20%20Problems%0AAuthor%3A%20Nikita%20Puchkin%20and%20Eduard%20Gorbunov%20and%20Nikolay%20Kutuzov%20and%20Alexander%20Gasnikov%0AAbstract%3A%20%20%20We%20consider%20stochastic%20optimization%20problems%20with%20heavy-tailed%20noise%20with%0Astructured%20density.%20For%20such%20problems%2C%20we%20show%20that%20it%20is%20possible%20to%20get%0Afaster%20rates%20of%20convergence%20than%20%24%5Cmathcal%7BO%7D%28K%5E%7B-2%28%5Calpha%20-%201%29/%5Calpha%7D%29%24%2C%20when%0Athe%20stochastic%20gradients%20have%20finite%20moments%20of%20order%20%24%5Calpha%20%5Cin%20%281%2C%202%5D%24.%20In%0Aparticular%2C%20our%20analysis%20allows%20the%20noise%20norm%20to%20have%20an%20unbounded%0Aexpectation.%20To%20achieve%20these%20results%2C%20we%20stabilize%20stochastic%20gradients%2C%20using%0Asmoothed%20medians%20of%20means.%20We%20prove%20that%20the%20resulting%20estimates%20have%0Anegligible%20bias%20and%20controllable%20variance.%20This%20allows%20us%20to%20carefully%0Aincorporate%20them%20into%20clipped-SGD%20and%20clipped-SSTM%20and%20derive%20new%0Ahigh-probability%20complexity%20bounds%20in%20the%20considered%20setup.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04161v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20Heavy-Tailed%20Noise%20Barrier%20in%20Stochastic%20Optimization%0A%20%20Problems&entry.906535625=Nikita%20Puchkin%20and%20Eduard%20Gorbunov%20and%20Nikolay%20Kutuzov%20and%20Alexander%20Gasnikov&entry.1292438233=%20%20We%20consider%20stochastic%20optimization%20problems%20with%20heavy-tailed%20noise%20with%0Astructured%20density.%20For%20such%20problems%2C%20we%20show%20that%20it%20is%20possible%20to%20get%0Afaster%20rates%20of%20convergence%20than%20%24%5Cmathcal%7BO%7D%28K%5E%7B-2%28%5Calpha%20-%201%29/%5Calpha%7D%29%24%2C%20when%0Athe%20stochastic%20gradients%20have%20finite%20moments%20of%20order%20%24%5Calpha%20%5Cin%20%281%2C%202%5D%24.%20In%0Aparticular%2C%20our%20analysis%20allows%20the%20noise%20norm%20to%20have%20an%20unbounded%0Aexpectation.%20To%20achieve%20these%20results%2C%20we%20stabilize%20stochastic%20gradients%2C%20using%0Asmoothed%20medians%20of%20means.%20We%20prove%20that%20the%20resulting%20estimates%20have%0Anegligible%20bias%20and%20controllable%20variance.%20This%20allows%20us%20to%20carefully%0Aincorporate%20them%20into%20clipped-SGD%20and%20clipped-SSTM%20and%20derive%20new%0Ahigh-probability%20complexity%20bounds%20in%20the%20considered%20setup.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04161v2&entry.124074799=Read"},
{"title": "Learning Social Navigation from Demonstrations with Deep Neural Networks", "author": "Yigit Yildirim and Emre Ugur", "abstract": "  Traditional path-planning techniques treat humans as obstacles. This has\nchanged since robots started to enter human environments. On modern robots,\nsocial navigation has become an important aspect of navigation systems. To use\nlearning-based techniques to achieve social navigation, a powerful framework\nthat is capable of representing complex functions with as few data as possible\nis required. In this study, we benefited from recent advances in deep learning\nat both global and local planning levels to achieve human-aware navigation on a\nsimulated robot. Two distinct deep models are trained with respective\nobjectives: one for global planning and one for local planning. These models\nare then employed in the simulated robot. In the end, it has been shown that\nour model can successfully carry out both global and local planning tasks. We\nhave shown that our system could generate paths that successfully reach targets\nwhile avoiding obstacles with better performance compared to feed-forward\nneural networks.\n", "link": "http://arxiv.org/abs/2404.11246v1", "date": "2024-04-17", "relevancy": 2.2652, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5832}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5758}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5456}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Social%20Navigation%20from%20Demonstrations%20with%20Deep%20Neural%20Networks&body=Title%3A%20Learning%20Social%20Navigation%20from%20Demonstrations%20with%20Deep%20Neural%20Networks%0AAuthor%3A%20Yigit%20Yildirim%20and%20Emre%20Ugur%0AAbstract%3A%20%20%20Traditional%20path-planning%20techniques%20treat%20humans%20as%20obstacles.%20This%20has%0Achanged%20since%20robots%20started%20to%20enter%20human%20environments.%20On%20modern%20robots%2C%0Asocial%20navigation%20has%20become%20an%20important%20aspect%20of%20navigation%20systems.%20To%20use%0Alearning-based%20techniques%20to%20achieve%20social%20navigation%2C%20a%20powerful%20framework%0Athat%20is%20capable%20of%20representing%20complex%20functions%20with%20as%20few%20data%20as%20possible%0Ais%20required.%20In%20this%20study%2C%20we%20benefited%20from%20recent%20advances%20in%20deep%20learning%0Aat%20both%20global%20and%20local%20planning%20levels%20to%20achieve%20human-aware%20navigation%20on%20a%0Asimulated%20robot.%20Two%20distinct%20deep%20models%20are%20trained%20with%20respective%0Aobjectives%3A%20one%20for%20global%20planning%20and%20one%20for%20local%20planning.%20These%20models%0Aare%20then%20employed%20in%20the%20simulated%20robot.%20In%20the%20end%2C%20it%20has%20been%20shown%20that%0Aour%20model%20can%20successfully%20carry%20out%20both%20global%20and%20local%20planning%20tasks.%20We%0Ahave%20shown%20that%20our%20system%20could%20generate%20paths%20that%20successfully%20reach%20targets%0Awhile%20avoiding%20obstacles%20with%20better%20performance%20compared%20to%20feed-forward%0Aneural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11246v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Social%20Navigation%20from%20Demonstrations%20with%20Deep%20Neural%20Networks&entry.906535625=Yigit%20Yildirim%20and%20Emre%20Ugur&entry.1292438233=%20%20Traditional%20path-planning%20techniques%20treat%20humans%20as%20obstacles.%20This%20has%0Achanged%20since%20robots%20started%20to%20enter%20human%20environments.%20On%20modern%20robots%2C%0Asocial%20navigation%20has%20become%20an%20important%20aspect%20of%20navigation%20systems.%20To%20use%0Alearning-based%20techniques%20to%20achieve%20social%20navigation%2C%20a%20powerful%20framework%0Athat%20is%20capable%20of%20representing%20complex%20functions%20with%20as%20few%20data%20as%20possible%0Ais%20required.%20In%20this%20study%2C%20we%20benefited%20from%20recent%20advances%20in%20deep%20learning%0Aat%20both%20global%20and%20local%20planning%20levels%20to%20achieve%20human-aware%20navigation%20on%20a%0Asimulated%20robot.%20Two%20distinct%20deep%20models%20are%20trained%20with%20respective%0Aobjectives%3A%20one%20for%20global%20planning%20and%20one%20for%20local%20planning.%20These%20models%0Aare%20then%20employed%20in%20the%20simulated%20robot.%20In%20the%20end%2C%20it%20has%20been%20shown%20that%0Aour%20model%20can%20successfully%20carry%20out%20both%20global%20and%20local%20planning%20tasks.%20We%0Ahave%20shown%20that%20our%20system%20could%20generate%20paths%20that%20successfully%20reach%20targets%0Awhile%20avoiding%20obstacles%20with%20better%20performance%20compared%20to%20feed-forward%0Aneural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11246v1&entry.124074799=Read"},
{"title": "Single-temporal Supervised Remote Change Detection for Domain\n  Generalization", "author": "Qiangang Du and Jinlong Peng and Xu Chen and Qingdong He and Qiang Nie and Wenbing Zhu and Mingmin Chi and Yabiao Wang and Chengjie Wang", "abstract": "  Change detection is widely applied in remote sensing image analysis. Existing\nmethods require training models separately for each dataset, which leads to\npoor domain generalization. Moreover, these methods rely heavily on large\namounts of high-quality pair-labelled data for training, which is expensive and\nimpractical. In this paper, we propose a multimodal contrastive learning\n(ChangeCLIP) based on visual-language pre-training for change detection domain\ngeneralization. Additionally, we propose a dynamic context optimization for\nprompt learning. Meanwhile, to address the data dependency issue of existing\nmethods, we introduce a single-temporal and controllable AI-generated training\nstrategy (SAIN). This allows us to train the model using a large number of\nsingle-temporal images without image pairs in the real world, achieving\nexcellent generalization. Extensive experiments on series of real change\ndetection datasets validate the superiority and strong generalization of\nChangeCLIP, outperforming state-of-the-art change detection methods. Code will\nbe available.\n", "link": "http://arxiv.org/abs/2404.11326v1", "date": "2024-04-17", "relevancy": 2.2472, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5722}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5662}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5248}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Single-temporal%20Supervised%20Remote%20Change%20Detection%20for%20Domain%0A%20%20Generalization&body=Title%3A%20Single-temporal%20Supervised%20Remote%20Change%20Detection%20for%20Domain%0A%20%20Generalization%0AAuthor%3A%20Qiangang%20Du%20and%20Jinlong%20Peng%20and%20Xu%20Chen%20and%20Qingdong%20He%20and%20Qiang%20Nie%20and%20Wenbing%20Zhu%20and%20Mingmin%20Chi%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%0AAbstract%3A%20%20%20Change%20detection%20is%20widely%20applied%20in%20remote%20sensing%20image%20analysis.%20Existing%0Amethods%20require%20training%20models%20separately%20for%20each%20dataset%2C%20which%20leads%20to%0Apoor%20domain%20generalization.%20Moreover%2C%20these%20methods%20rely%20heavily%20on%20large%0Aamounts%20of%20high-quality%20pair-labelled%20data%20for%20training%2C%20which%20is%20expensive%20and%0Aimpractical.%20In%20this%20paper%2C%20we%20propose%20a%20multimodal%20contrastive%20learning%0A%28ChangeCLIP%29%20based%20on%20visual-language%20pre-training%20for%20change%20detection%20domain%0Ageneralization.%20Additionally%2C%20we%20propose%20a%20dynamic%20context%20optimization%20for%0Aprompt%20learning.%20Meanwhile%2C%20to%20address%20the%20data%20dependency%20issue%20of%20existing%0Amethods%2C%20we%20introduce%20a%20single-temporal%20and%20controllable%20AI-generated%20training%0Astrategy%20%28SAIN%29.%20This%20allows%20us%20to%20train%20the%20model%20using%20a%20large%20number%20of%0Asingle-temporal%20images%20without%20image%20pairs%20in%20the%20real%20world%2C%20achieving%0Aexcellent%20generalization.%20Extensive%20experiments%20on%20series%20of%20real%20change%0Adetection%20datasets%20validate%20the%20superiority%20and%20strong%20generalization%20of%0AChangeCLIP%2C%20outperforming%20state-of-the-art%20change%20detection%20methods.%20Code%20will%0Abe%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11326v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-temporal%20Supervised%20Remote%20Change%20Detection%20for%20Domain%0A%20%20Generalization&entry.906535625=Qiangang%20Du%20and%20Jinlong%20Peng%20and%20Xu%20Chen%20and%20Qingdong%20He%20and%20Qiang%20Nie%20and%20Wenbing%20Zhu%20and%20Mingmin%20Chi%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang&entry.1292438233=%20%20Change%20detection%20is%20widely%20applied%20in%20remote%20sensing%20image%20analysis.%20Existing%0Amethods%20require%20training%20models%20separately%20for%20each%20dataset%2C%20which%20leads%20to%0Apoor%20domain%20generalization.%20Moreover%2C%20these%20methods%20rely%20heavily%20on%20large%0Aamounts%20of%20high-quality%20pair-labelled%20data%20for%20training%2C%20which%20is%20expensive%20and%0Aimpractical.%20In%20this%20paper%2C%20we%20propose%20a%20multimodal%20contrastive%20learning%0A%28ChangeCLIP%29%20based%20on%20visual-language%20pre-training%20for%20change%20detection%20domain%0Ageneralization.%20Additionally%2C%20we%20propose%20a%20dynamic%20context%20optimization%20for%0Aprompt%20learning.%20Meanwhile%2C%20to%20address%20the%20data%20dependency%20issue%20of%20existing%0Amethods%2C%20we%20introduce%20a%20single-temporal%20and%20controllable%20AI-generated%20training%0Astrategy%20%28SAIN%29.%20This%20allows%20us%20to%20train%20the%20model%20using%20a%20large%20number%20of%0Asingle-temporal%20images%20without%20image%20pairs%20in%20the%20real%20world%2C%20achieving%0Aexcellent%20generalization.%20Extensive%20experiments%20on%20series%20of%20real%20change%0Adetection%20datasets%20validate%20the%20superiority%20and%20strong%20generalization%20of%0AChangeCLIP%2C%20outperforming%20state-of-the-art%20change%20detection%20methods.%20Code%20will%0Abe%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11326v1&entry.124074799=Read"},
{"title": "Generative Representational Instruction Tuning", "author": "Niklas Muennighoff and Hongjin Su and Liang Wang and Nan Yang and Furu Wei and Tao Yu and Amanpreet Singh and Douwe Kiela", "abstract": "  All text-based language problems can be reduced to either generation or\nembedding. Current models only perform well at one or the other. We introduce\ngenerative representational instruction tuning (GRIT) whereby a large language\nmodel is trained to handle both generative and embedding tasks by\ndistinguishing between them through instructions. Compared to other open\nmodels, our resulting GritLM 7B sets a new state of the art on the Massive Text\nEmbedding Benchmark (MTEB) and outperforms all models up to its size on a range\nof generative tasks. By scaling up further, GritLM 8x7B outperforms all open\ngenerative language models that we tried while still being among the best\nembedding models. Notably, we find that GRIT matches training on only\ngenerative or embedding data, thus we can unify both at no performance loss.\nAmong other benefits, the unification via GRIT speeds up Retrieval-Augmented\nGeneration (RAG) by > 60% for long documents, by no longer requiring separate\nretrieval and generation models. Models, code, etc. are freely available at\nhttps://github.com/ContextualAI/gritlm.\n", "link": "http://arxiv.org/abs/2402.09906v2", "date": "2024-04-17", "relevancy": 2.2437, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5902}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5469}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5373}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generative%20Representational%20Instruction%20Tuning&body=Title%3A%20Generative%20Representational%20Instruction%20Tuning%0AAuthor%3A%20Niklas%20Muennighoff%20and%20Hongjin%20Su%20and%20Liang%20Wang%20and%20Nan%20Yang%20and%20Furu%20Wei%20and%20Tao%20Yu%20and%20Amanpreet%20Singh%20and%20Douwe%20Kiela%0AAbstract%3A%20%20%20All%20text-based%20language%20problems%20can%20be%20reduced%20to%20either%20generation%20or%0Aembedding.%20Current%20models%20only%20perform%20well%20at%20one%20or%20the%20other.%20We%20introduce%0Agenerative%20representational%20instruction%20tuning%20%28GRIT%29%20whereby%20a%20large%20language%0Amodel%20is%20trained%20to%20handle%20both%20generative%20and%20embedding%20tasks%20by%0Adistinguishing%20between%20them%20through%20instructions.%20Compared%20to%20other%20open%0Amodels%2C%20our%20resulting%20GritLM%207B%20sets%20a%20new%20state%20of%20the%20art%20on%20the%20Massive%20Text%0AEmbedding%20Benchmark%20%28MTEB%29%20and%20outperforms%20all%20models%20up%20to%20its%20size%20on%20a%20range%0Aof%20generative%20tasks.%20By%20scaling%20up%20further%2C%20GritLM%208x7B%20outperforms%20all%20open%0Agenerative%20language%20models%20that%20we%20tried%20while%20still%20being%20among%20the%20best%0Aembedding%20models.%20Notably%2C%20we%20find%20that%20GRIT%20matches%20training%20on%20only%0Agenerative%20or%20embedding%20data%2C%20thus%20we%20can%20unify%20both%20at%20no%20performance%20loss.%0AAmong%20other%20benefits%2C%20the%20unification%20via%20GRIT%20speeds%20up%20Retrieval-Augmented%0AGeneration%20%28RAG%29%20by%20%3E%2060%25%20for%20long%20documents%2C%20by%20no%20longer%20requiring%20separate%0Aretrieval%20and%20generation%20models.%20Models%2C%20code%2C%20etc.%20are%20freely%20available%20at%0Ahttps%3A//github.com/ContextualAI/gritlm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09906v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Representational%20Instruction%20Tuning&entry.906535625=Niklas%20Muennighoff%20and%20Hongjin%20Su%20and%20Liang%20Wang%20and%20Nan%20Yang%20and%20Furu%20Wei%20and%20Tao%20Yu%20and%20Amanpreet%20Singh%20and%20Douwe%20Kiela&entry.1292438233=%20%20All%20text-based%20language%20problems%20can%20be%20reduced%20to%20either%20generation%20or%0Aembedding.%20Current%20models%20only%20perform%20well%20at%20one%20or%20the%20other.%20We%20introduce%0Agenerative%20representational%20instruction%20tuning%20%28GRIT%29%20whereby%20a%20large%20language%0Amodel%20is%20trained%20to%20handle%20both%20generative%20and%20embedding%20tasks%20by%0Adistinguishing%20between%20them%20through%20instructions.%20Compared%20to%20other%20open%0Amodels%2C%20our%20resulting%20GritLM%207B%20sets%20a%20new%20state%20of%20the%20art%20on%20the%20Massive%20Text%0AEmbedding%20Benchmark%20%28MTEB%29%20and%20outperforms%20all%20models%20up%20to%20its%20size%20on%20a%20range%0Aof%20generative%20tasks.%20By%20scaling%20up%20further%2C%20GritLM%208x7B%20outperforms%20all%20open%0Agenerative%20language%20models%20that%20we%20tried%20while%20still%20being%20among%20the%20best%0Aembedding%20models.%20Notably%2C%20we%20find%20that%20GRIT%20matches%20training%20on%20only%0Agenerative%20or%20embedding%20data%2C%20thus%20we%20can%20unify%20both%20at%20no%20performance%20loss.%0AAmong%20other%20benefits%2C%20the%20unification%20via%20GRIT%20speeds%20up%20Retrieval-Augmented%0AGeneration%20%28RAG%29%20by%20%3E%2060%25%20for%20long%20documents%2C%20by%20no%20longer%20requiring%20separate%0Aretrieval%20and%20generation%20models.%20Models%2C%20code%2C%20etc.%20are%20freely%20available%20at%0Ahttps%3A//github.com/ContextualAI/gritlm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09906v2&entry.124074799=Read"},
{"title": "ICSVR: Investigating Compositional and Syntactic Understanding in Video\n  Retrieval Models", "author": "Avinash Madasu and Vasudev Lal", "abstract": "  Video retrieval (VR) involves retrieving the ground truth video from the\nvideo database given a text caption or vice-versa. The two important components\nof compositionality: objects & attributes and actions are joined using correct\nsyntax to form a proper text query. These components (objects & attributes,\nactions and syntax) each play an important role to help distinguish among\nvideos and retrieve the correct ground truth video. However, it is unclear what\nis the effect of these components on the video retrieval performance. We\ntherefore, conduct a systematic study to evaluate the compositional and\nsyntactic understanding of video retrieval models on standard benchmarks such\nas MSRVTT, MSVD and DIDEMO. The study is performed on two categories of video\nretrieval models: (i) which are pre-trained on video-text pairs and fine-tuned\non downstream video retrieval datasets (Eg. Frozen-in-Time, Violet, MCQ etc.)\n(ii) which adapt pre-trained image-text representations like CLIP for video\nretrieval (Eg. CLIP4Clip, XCLIP, CLIP2Video etc.). Our experiments reveal that\nactions and syntax play a minor role compared to objects & attributes in video\nunderstanding. Moreover, video retrieval models that use pre-trained image-text\nrepresentations (CLIP) have better syntactic and compositional understanding as\ncompared to models pre-trained on video-text data. The code is available at\nhttps://github.com/IntelLabs/multimodal_cognitive_ai/tree/main/ICSVR\n", "link": "http://arxiv.org/abs/2306.16533v2", "date": "2024-04-17", "relevancy": 2.2226, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5706}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5548}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5411}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ICSVR%3A%20Investigating%20Compositional%20and%20Syntactic%20Understanding%20in%20Video%0A%20%20Retrieval%20Models&body=Title%3A%20ICSVR%3A%20Investigating%20Compositional%20and%20Syntactic%20Understanding%20in%20Video%0A%20%20Retrieval%20Models%0AAuthor%3A%20Avinash%20Madasu%20and%20Vasudev%20Lal%0AAbstract%3A%20%20%20Video%20retrieval%20%28VR%29%20involves%20retrieving%20the%20ground%20truth%20video%20from%20the%0Avideo%20database%20given%20a%20text%20caption%20or%20vice-versa.%20The%20two%20important%20components%0Aof%20compositionality%3A%20objects%20%26%20attributes%20and%20actions%20are%20joined%20using%20correct%0Asyntax%20to%20form%20a%20proper%20text%20query.%20These%20components%20%28objects%20%26%20attributes%2C%0Aactions%20and%20syntax%29%20each%20play%20an%20important%20role%20to%20help%20distinguish%20among%0Avideos%20and%20retrieve%20the%20correct%20ground%20truth%20video.%20However%2C%20it%20is%20unclear%20what%0Ais%20the%20effect%20of%20these%20components%20on%20the%20video%20retrieval%20performance.%20We%0Atherefore%2C%20conduct%20a%20systematic%20study%20to%20evaluate%20the%20compositional%20and%0Asyntactic%20understanding%20of%20video%20retrieval%20models%20on%20standard%20benchmarks%20such%0Aas%20MSRVTT%2C%20MSVD%20and%20DIDEMO.%20The%20study%20is%20performed%20on%20two%20categories%20of%20video%0Aretrieval%20models%3A%20%28i%29%20which%20are%20pre-trained%20on%20video-text%20pairs%20and%20fine-tuned%0Aon%20downstream%20video%20retrieval%20datasets%20%28Eg.%20Frozen-in-Time%2C%20Violet%2C%20MCQ%20etc.%29%0A%28ii%29%20which%20adapt%20pre-trained%20image-text%20representations%20like%20CLIP%20for%20video%0Aretrieval%20%28Eg.%20CLIP4Clip%2C%20XCLIP%2C%20CLIP2Video%20etc.%29.%20Our%20experiments%20reveal%20that%0Aactions%20and%20syntax%20play%20a%20minor%20role%20compared%20to%20objects%20%26%20attributes%20in%20video%0Aunderstanding.%20Moreover%2C%20video%20retrieval%20models%20that%20use%20pre-trained%20image-text%0Arepresentations%20%28CLIP%29%20have%20better%20syntactic%20and%20compositional%20understanding%20as%0Acompared%20to%20models%20pre-trained%20on%20video-text%20data.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/IntelLabs/multimodal_cognitive_ai/tree/main/ICSVR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.16533v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ICSVR%3A%20Investigating%20Compositional%20and%20Syntactic%20Understanding%20in%20Video%0A%20%20Retrieval%20Models&entry.906535625=Avinash%20Madasu%20and%20Vasudev%20Lal&entry.1292438233=%20%20Video%20retrieval%20%28VR%29%20involves%20retrieving%20the%20ground%20truth%20video%20from%20the%0Avideo%20database%20given%20a%20text%20caption%20or%20vice-versa.%20The%20two%20important%20components%0Aof%20compositionality%3A%20objects%20%26%20attributes%20and%20actions%20are%20joined%20using%20correct%0Asyntax%20to%20form%20a%20proper%20text%20query.%20These%20components%20%28objects%20%26%20attributes%2C%0Aactions%20and%20syntax%29%20each%20play%20an%20important%20role%20to%20help%20distinguish%20among%0Avideos%20and%20retrieve%20the%20correct%20ground%20truth%20video.%20However%2C%20it%20is%20unclear%20what%0Ais%20the%20effect%20of%20these%20components%20on%20the%20video%20retrieval%20performance.%20We%0Atherefore%2C%20conduct%20a%20systematic%20study%20to%20evaluate%20the%20compositional%20and%0Asyntactic%20understanding%20of%20video%20retrieval%20models%20on%20standard%20benchmarks%20such%0Aas%20MSRVTT%2C%20MSVD%20and%20DIDEMO.%20The%20study%20is%20performed%20on%20two%20categories%20of%20video%0Aretrieval%20models%3A%20%28i%29%20which%20are%20pre-trained%20on%20video-text%20pairs%20and%20fine-tuned%0Aon%20downstream%20video%20retrieval%20datasets%20%28Eg.%20Frozen-in-Time%2C%20Violet%2C%20MCQ%20etc.%29%0A%28ii%29%20which%20adapt%20pre-trained%20image-text%20representations%20like%20CLIP%20for%20video%0Aretrieval%20%28Eg.%20CLIP4Clip%2C%20XCLIP%2C%20CLIP2Video%20etc.%29.%20Our%20experiments%20reveal%20that%0Aactions%20and%20syntax%20play%20a%20minor%20role%20compared%20to%20objects%20%26%20attributes%20in%20video%0Aunderstanding.%20Moreover%2C%20video%20retrieval%20models%20that%20use%20pre-trained%20image-text%0Arepresentations%20%28CLIP%29%20have%20better%20syntactic%20and%20compositional%20understanding%20as%0Acompared%20to%20models%20pre-trained%20on%20video-text%20data.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/IntelLabs/multimodal_cognitive_ai/tree/main/ICSVR%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.16533v2&entry.124074799=Read"},
{"title": "VBR: A Vision Benchmark in Rome", "author": "Leonardo Brizi and Emanuele Giacomini and Luca Di Giammarino and Simone Ferrari and Omar Salem and Lorenzo De Rebotti and Giorgio Grisetti", "abstract": "  This paper presents a vision and perception research dataset collected in\nRome, featuring RGB data, 3D point clouds, IMU, and GPS data. We introduce a\nnew benchmark targeting visual odometry and SLAM, to advance the research in\nautonomous robotics and computer vision. This work complements existing\ndatasets by simultaneously addressing several issues, such as environment\ndiversity, motion patterns, and sensor frequency. It uses up-to-date devices\nand presents effective procedures to accurately calibrate the intrinsic and\nextrinsic of the sensors while addressing temporal synchronization. During\nrecording, we cover multi-floor buildings, gardens, urban and highway\nscenarios. Combining handheld and car-based data collections, our setup can\nsimulate any robot (quadrupeds, quadrotors, autonomous vehicles). The dataset\nincludes an accurate 6-dof ground truth based on a novel methodology that\nrefines the RTK-GPS estimate with LiDAR point clouds through Bundle Adjustment.\nAll sequences divided in training and testing are accessible through our\nwebsite.\n", "link": "http://arxiv.org/abs/2404.11322v1", "date": "2024-04-17", "relevancy": 2.2173, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5747}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5414}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5391}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VBR%3A%20A%20Vision%20Benchmark%20in%20Rome&body=Title%3A%20VBR%3A%20A%20Vision%20Benchmark%20in%20Rome%0AAuthor%3A%20Leonardo%20Brizi%20and%20Emanuele%20Giacomini%20and%20Luca%20Di%20Giammarino%20and%20Simone%20Ferrari%20and%20Omar%20Salem%20and%20Lorenzo%20De%20Rebotti%20and%20Giorgio%20Grisetti%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20vision%20and%20perception%20research%20dataset%20collected%20in%0ARome%2C%20featuring%20RGB%20data%2C%203D%20point%20clouds%2C%20IMU%2C%20and%20GPS%20data.%20We%20introduce%20a%0Anew%20benchmark%20targeting%20visual%20odometry%20and%20SLAM%2C%20to%20advance%20the%20research%20in%0Aautonomous%20robotics%20and%20computer%20vision.%20This%20work%20complements%20existing%0Adatasets%20by%20simultaneously%20addressing%20several%20issues%2C%20such%20as%20environment%0Adiversity%2C%20motion%20patterns%2C%20and%20sensor%20frequency.%20It%20uses%20up-to-date%20devices%0Aand%20presents%20effective%20procedures%20to%20accurately%20calibrate%20the%20intrinsic%20and%0Aextrinsic%20of%20the%20sensors%20while%20addressing%20temporal%20synchronization.%20During%0Arecording%2C%20we%20cover%20multi-floor%20buildings%2C%20gardens%2C%20urban%20and%20highway%0Ascenarios.%20Combining%20handheld%20and%20car-based%20data%20collections%2C%20our%20setup%20can%0Asimulate%20any%20robot%20%28quadrupeds%2C%20quadrotors%2C%20autonomous%20vehicles%29.%20The%20dataset%0Aincludes%20an%20accurate%206-dof%20ground%20truth%20based%20on%20a%20novel%20methodology%20that%0Arefines%20the%20RTK-GPS%20estimate%20with%20LiDAR%20point%20clouds%20through%20Bundle%20Adjustment.%0AAll%20sequences%20divided%20in%20training%20and%20testing%20are%20accessible%20through%20our%0Awebsite.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11322v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VBR%3A%20A%20Vision%20Benchmark%20in%20Rome&entry.906535625=Leonardo%20Brizi%20and%20Emanuele%20Giacomini%20and%20Luca%20Di%20Giammarino%20and%20Simone%20Ferrari%20and%20Omar%20Salem%20and%20Lorenzo%20De%20Rebotti%20and%20Giorgio%20Grisetti&entry.1292438233=%20%20This%20paper%20presents%20a%20vision%20and%20perception%20research%20dataset%20collected%20in%0ARome%2C%20featuring%20RGB%20data%2C%203D%20point%20clouds%2C%20IMU%2C%20and%20GPS%20data.%20We%20introduce%20a%0Anew%20benchmark%20targeting%20visual%20odometry%20and%20SLAM%2C%20to%20advance%20the%20research%20in%0Aautonomous%20robotics%20and%20computer%20vision.%20This%20work%20complements%20existing%0Adatasets%20by%20simultaneously%20addressing%20several%20issues%2C%20such%20as%20environment%0Adiversity%2C%20motion%20patterns%2C%20and%20sensor%20frequency.%20It%20uses%20up-to-date%20devices%0Aand%20presents%20effective%20procedures%20to%20accurately%20calibrate%20the%20intrinsic%20and%0Aextrinsic%20of%20the%20sensors%20while%20addressing%20temporal%20synchronization.%20During%0Arecording%2C%20we%20cover%20multi-floor%20buildings%2C%20gardens%2C%20urban%20and%20highway%0Ascenarios.%20Combining%20handheld%20and%20car-based%20data%20collections%2C%20our%20setup%20can%0Asimulate%20any%20robot%20%28quadrupeds%2C%20quadrotors%2C%20autonomous%20vehicles%29.%20The%20dataset%0Aincludes%20an%20accurate%206-dof%20ground%20truth%20based%20on%20a%20novel%20methodology%20that%0Arefines%20the%20RTK-GPS%20estimate%20with%20LiDAR%20point%20clouds%20through%20Bundle%20Adjustment.%0AAll%20sequences%20divided%20in%20training%20and%20testing%20are%20accessible%20through%20our%0Awebsite.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11322v1&entry.124074799=Read"},
{"title": "ShapeFormer: Shape Prior Visible-to-Amodal Transformer-based Amodal\n  Instance Segmentation", "author": "Minh Tran and Winston Bounsavy and Khoa Vo and Anh Nguyen and Tri Nguyen and Ngan Le", "abstract": "  Amodal Instance Segmentation (AIS) presents a challenging task as it involves\npredicting both visible and occluded parts of objects within images. Existing\nAIS methods rely on a bidirectional approach, encompassing both the transition\nfrom amodal features to visible features (amodal-to-visible) and from visible\nfeatures to amodal features (visible-to-amodal). Our observation shows that the\nutilization of amodal features through the amodal-to-visible can confuse the\nvisible features due to the extra information of occluded/hidden segments not\npresented in visible display. Consequently, this compromised quality of visible\nfeatures during the subsequent visible-to-amodal transition. To tackle this\nissue, we introduce ShapeFormer, a decoupled Transformer-based model with a\nvisible-to-amodal transition. It facilitates the explicit relationship between\noutput segmentations and avoids the need for amodal-to-visible transitions.\nShapeFormer comprises three key modules: (i) Visible-Occluding Mask Head for\npredicting visible segmentation with occlusion awareness, (ii) Shape-Prior\nAmodal Mask Head for predicting amodal and occluded masks, and (iii)\nCategory-Specific Shape Prior Retriever aims to provide shape prior knowledge.\nComprehensive experiments and extensive ablation studies across various AIS\nbenchmarks demonstrate the effectiveness of our ShapeFormer. The code is\navailable at: \\url{https://github.com/UARK-AICV/ShapeFormer}\n", "link": "http://arxiv.org/abs/2403.11376v4", "date": "2024-04-17", "relevancy": 2.2102, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6203}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5103}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5017}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ShapeFormer%3A%20Shape%20Prior%20Visible-to-Amodal%20Transformer-based%20Amodal%0A%20%20Instance%20Segmentation&body=Title%3A%20ShapeFormer%3A%20Shape%20Prior%20Visible-to-Amodal%20Transformer-based%20Amodal%0A%20%20Instance%20Segmentation%0AAuthor%3A%20Minh%20Tran%20and%20Winston%20Bounsavy%20and%20Khoa%20Vo%20and%20Anh%20Nguyen%20and%20Tri%20Nguyen%20and%20Ngan%20Le%0AAbstract%3A%20%20%20Amodal%20Instance%20Segmentation%20%28AIS%29%20presents%20a%20challenging%20task%20as%20it%20involves%0Apredicting%20both%20visible%20and%20occluded%20parts%20of%20objects%20within%20images.%20Existing%0AAIS%20methods%20rely%20on%20a%20bidirectional%20approach%2C%20encompassing%20both%20the%20transition%0Afrom%20amodal%20features%20to%20visible%20features%20%28amodal-to-visible%29%20and%20from%20visible%0Afeatures%20to%20amodal%20features%20%28visible-to-amodal%29.%20Our%20observation%20shows%20that%20the%0Autilization%20of%20amodal%20features%20through%20the%20amodal-to-visible%20can%20confuse%20the%0Avisible%20features%20due%20to%20the%20extra%20information%20of%20occluded/hidden%20segments%20not%0Apresented%20in%20visible%20display.%20Consequently%2C%20this%20compromised%20quality%20of%20visible%0Afeatures%20during%20the%20subsequent%20visible-to-amodal%20transition.%20To%20tackle%20this%0Aissue%2C%20we%20introduce%20ShapeFormer%2C%20a%20decoupled%20Transformer-based%20model%20with%20a%0Avisible-to-amodal%20transition.%20It%20facilitates%20the%20explicit%20relationship%20between%0Aoutput%20segmentations%20and%20avoids%20the%20need%20for%20amodal-to-visible%20transitions.%0AShapeFormer%20comprises%20three%20key%20modules%3A%20%28i%29%20Visible-Occluding%20Mask%20Head%20for%0Apredicting%20visible%20segmentation%20with%20occlusion%20awareness%2C%20%28ii%29%20Shape-Prior%0AAmodal%20Mask%20Head%20for%20predicting%20amodal%20and%20occluded%20masks%2C%20and%20%28iii%29%0ACategory-Specific%20Shape%20Prior%20Retriever%20aims%20to%20provide%20shape%20prior%20knowledge.%0AComprehensive%20experiments%20and%20extensive%20ablation%20studies%20across%20various%20AIS%0Abenchmarks%20demonstrate%20the%20effectiveness%20of%20our%20ShapeFormer.%20The%20code%20is%0Aavailable%20at%3A%20%5Curl%7Bhttps%3A//github.com/UARK-AICV/ShapeFormer%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11376v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShapeFormer%3A%20Shape%20Prior%20Visible-to-Amodal%20Transformer-based%20Amodal%0A%20%20Instance%20Segmentation&entry.906535625=Minh%20Tran%20and%20Winston%20Bounsavy%20and%20Khoa%20Vo%20and%20Anh%20Nguyen%20and%20Tri%20Nguyen%20and%20Ngan%20Le&entry.1292438233=%20%20Amodal%20Instance%20Segmentation%20%28AIS%29%20presents%20a%20challenging%20task%20as%20it%20involves%0Apredicting%20both%20visible%20and%20occluded%20parts%20of%20objects%20within%20images.%20Existing%0AAIS%20methods%20rely%20on%20a%20bidirectional%20approach%2C%20encompassing%20both%20the%20transition%0Afrom%20amodal%20features%20to%20visible%20features%20%28amodal-to-visible%29%20and%20from%20visible%0Afeatures%20to%20amodal%20features%20%28visible-to-amodal%29.%20Our%20observation%20shows%20that%20the%0Autilization%20of%20amodal%20features%20through%20the%20amodal-to-visible%20can%20confuse%20the%0Avisible%20features%20due%20to%20the%20extra%20information%20of%20occluded/hidden%20segments%20not%0Apresented%20in%20visible%20display.%20Consequently%2C%20this%20compromised%20quality%20of%20visible%0Afeatures%20during%20the%20subsequent%20visible-to-amodal%20transition.%20To%20tackle%20this%0Aissue%2C%20we%20introduce%20ShapeFormer%2C%20a%20decoupled%20Transformer-based%20model%20with%20a%0Avisible-to-amodal%20transition.%20It%20facilitates%20the%20explicit%20relationship%20between%0Aoutput%20segmentations%20and%20avoids%20the%20need%20for%20amodal-to-visible%20transitions.%0AShapeFormer%20comprises%20three%20key%20modules%3A%20%28i%29%20Visible-Occluding%20Mask%20Head%20for%0Apredicting%20visible%20segmentation%20with%20occlusion%20awareness%2C%20%28ii%29%20Shape-Prior%0AAmodal%20Mask%20Head%20for%20predicting%20amodal%20and%20occluded%20masks%2C%20and%20%28iii%29%0ACategory-Specific%20Shape%20Prior%20Retriever%20aims%20to%20provide%20shape%20prior%20knowledge.%0AComprehensive%20experiments%20and%20extensive%20ablation%20studies%20across%20various%20AIS%0Abenchmarks%20demonstrate%20the%20effectiveness%20of%20our%20ShapeFormer.%20The%20code%20is%0Aavailable%20at%3A%20%5Curl%7Bhttps%3A//github.com/UARK-AICV/ShapeFormer%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11376v4&entry.124074799=Read"},
{"title": "Re-Nerfing: Improving Novel Views Synthesis through Novel Views\n  Synthesis", "author": "Felix Tristram and Stefano Gasperini and Nassir Navab and Federico Tombari", "abstract": "  Neural Radiance Fields (NeRFs) have shown remarkable novel view synthesis\ncapabilities even in large-scale, unbounded scenes, albeit requiring hundreds\nof views or introducing artifacts in sparser settings. Their optimization\nsuffers from shape-radiance ambiguities wherever only a small visual overlap is\navailable. This leads to erroneous scene geometry and artifacts. In this paper,\nwe propose Re-Nerfing, a simple and general multi-stage data augmentation\napproach that leverages NeRF's own view synthesis ability to address these\nlimitations. With Re-Nerfing, we enhance the geometric consistency of novel\nviews as follows: First, we train a NeRF with the available views. Then, we use\nthe optimized NeRF to synthesize pseudo-views around the original ones with a\nview selection strategy to improve coverage and preserve view quality. Finally,\nwe train a second NeRF with both the original images and the pseudo views\nmasking out uncertain regions. Extensive experiments applying Re-Nerfing on\nvarious pipelines on the mip-NeRF 360 dataset, including Gaussian Splatting,\nprovide valuable insights into the improvements achievable without external\ndata or supervision, on denser and sparser input scenarios. Project page:\nhttps://renerfing.github.io\n", "link": "http://arxiv.org/abs/2312.02255v2", "date": "2024-04-17", "relevancy": 2.1895, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5847}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5517}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5083}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Re-Nerfing%3A%20Improving%20Novel%20Views%20Synthesis%20through%20Novel%20Views%0A%20%20Synthesis&body=Title%3A%20Re-Nerfing%3A%20Improving%20Novel%20Views%20Synthesis%20through%20Novel%20Views%0A%20%20Synthesis%0AAuthor%3A%20Felix%20Tristram%20and%20Stefano%20Gasperini%20and%20Nassir%20Navab%20and%20Federico%20Tombari%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20shown%20remarkable%20novel%20view%20synthesis%0Acapabilities%20even%20in%20large-scale%2C%20unbounded%20scenes%2C%20albeit%20requiring%20hundreds%0Aof%20views%20or%20introducing%20artifacts%20in%20sparser%20settings.%20Their%20optimization%0Asuffers%20from%20shape-radiance%20ambiguities%20wherever%20only%20a%20small%20visual%20overlap%20is%0Aavailable.%20This%20leads%20to%20erroneous%20scene%20geometry%20and%20artifacts.%20In%20this%20paper%2C%0Awe%20propose%20Re-Nerfing%2C%20a%20simple%20and%20general%20multi-stage%20data%20augmentation%0Aapproach%20that%20leverages%20NeRF%27s%20own%20view%20synthesis%20ability%20to%20address%20these%0Alimitations.%20With%20Re-Nerfing%2C%20we%20enhance%20the%20geometric%20consistency%20of%20novel%0Aviews%20as%20follows%3A%20First%2C%20we%20train%20a%20NeRF%20with%20the%20available%20views.%20Then%2C%20we%20use%0Athe%20optimized%20NeRF%20to%20synthesize%20pseudo-views%20around%20the%20original%20ones%20with%20a%0Aview%20selection%20strategy%20to%20improve%20coverage%20and%20preserve%20view%20quality.%20Finally%2C%0Awe%20train%20a%20second%20NeRF%20with%20both%20the%20original%20images%20and%20the%20pseudo%20views%0Amasking%20out%20uncertain%20regions.%20Extensive%20experiments%20applying%20Re-Nerfing%20on%0Avarious%20pipelines%20on%20the%20mip-NeRF%20360%20dataset%2C%20including%20Gaussian%20Splatting%2C%0Aprovide%20valuable%20insights%20into%20the%20improvements%20achievable%20without%20external%0Adata%20or%20supervision%2C%20on%20denser%20and%20sparser%20input%20scenarios.%20Project%20page%3A%0Ahttps%3A//renerfing.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02255v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-Nerfing%3A%20Improving%20Novel%20Views%20Synthesis%20through%20Novel%20Views%0A%20%20Synthesis&entry.906535625=Felix%20Tristram%20and%20Stefano%20Gasperini%20and%20Nassir%20Navab%20and%20Federico%20Tombari&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20shown%20remarkable%20novel%20view%20synthesis%0Acapabilities%20even%20in%20large-scale%2C%20unbounded%20scenes%2C%20albeit%20requiring%20hundreds%0Aof%20views%20or%20introducing%20artifacts%20in%20sparser%20settings.%20Their%20optimization%0Asuffers%20from%20shape-radiance%20ambiguities%20wherever%20only%20a%20small%20visual%20overlap%20is%0Aavailable.%20This%20leads%20to%20erroneous%20scene%20geometry%20and%20artifacts.%20In%20this%20paper%2C%0Awe%20propose%20Re-Nerfing%2C%20a%20simple%20and%20general%20multi-stage%20data%20augmentation%0Aapproach%20that%20leverages%20NeRF%27s%20own%20view%20synthesis%20ability%20to%20address%20these%0Alimitations.%20With%20Re-Nerfing%2C%20we%20enhance%20the%20geometric%20consistency%20of%20novel%0Aviews%20as%20follows%3A%20First%2C%20we%20train%20a%20NeRF%20with%20the%20available%20views.%20Then%2C%20we%20use%0Athe%20optimized%20NeRF%20to%20synthesize%20pseudo-views%20around%20the%20original%20ones%20with%20a%0Aview%20selection%20strategy%20to%20improve%20coverage%20and%20preserve%20view%20quality.%20Finally%2C%0Awe%20train%20a%20second%20NeRF%20with%20both%20the%20original%20images%20and%20the%20pseudo%20views%0Amasking%20out%20uncertain%20regions.%20Extensive%20experiments%20applying%20Re-Nerfing%20on%0Avarious%20pipelines%20on%20the%20mip-NeRF%20360%20dataset%2C%20including%20Gaussian%20Splatting%2C%0Aprovide%20valuable%20insights%20into%20the%20improvements%20achievable%20without%20external%0Adata%20or%20supervision%2C%20on%20denser%20and%20sparser%20input%20scenarios.%20Project%20page%3A%0Ahttps%3A//renerfing.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02255v2&entry.124074799=Read"},
{"title": "Dynamic Typography: Bringing Words to Life", "author": "Zichen Liu and Yihao Meng and Hao Ouyang and Yue Yu and Bolin Zhao and Daniel Cohen-Or and Huamin Qu", "abstract": "  Text animation serves as an expressive medium, transforming static\ncommunication into dynamic experiences by infusing words with motion to evoke\nemotions, emphasize meanings, and construct compelling narratives. Crafting\nanimations that are semantically aware poses significant challenges, demanding\nexpertise in graphic design and animation. We present an automated text\nanimation scheme, termed \"Dynamic Typography\", which combines two challenging\ntasks. It deforms letters to convey semantic meaning and infuses them with\nvibrant movements based on user prompts. Our technique harnesses vector\ngraphics representations and an end-to-end optimization-based framework. This\nframework employs neural displacement fields to convert letters into base\nshapes and applies per-frame motion, encouraging coherence with the intended\ntextual concept. Shape preservation techniques and perceptual loss\nregularization are employed to maintain legibility and structural integrity\nthroughout the animation process. We demonstrate the generalizability of our\napproach across various text-to-video models and highlight the superiority of\nour end-to-end methodology over baseline methods, which might comprise separate\ntasks. Through quantitative and qualitative evaluations, we demonstrate the\neffectiveness of our framework in generating coherent text animations that\nfaithfully interpret user prompts while maintaining readability. Our code is\navailable at: https://animate-your-word.github.io/demo/.\n", "link": "http://arxiv.org/abs/2404.11614v1", "date": "2024-04-17", "relevancy": 2.1765, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6165}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5333}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.526}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Typography%3A%20Bringing%20Words%20to%20Life&body=Title%3A%20Dynamic%20Typography%3A%20Bringing%20Words%20to%20Life%0AAuthor%3A%20Zichen%20Liu%20and%20Yihao%20Meng%20and%20Hao%20Ouyang%20and%20Yue%20Yu%20and%20Bolin%20Zhao%20and%20Daniel%20Cohen-Or%20and%20Huamin%20Qu%0AAbstract%3A%20%20%20Text%20animation%20serves%20as%20an%20expressive%20medium%2C%20transforming%20static%0Acommunication%20into%20dynamic%20experiences%20by%20infusing%20words%20with%20motion%20to%20evoke%0Aemotions%2C%20emphasize%20meanings%2C%20and%20construct%20compelling%20narratives.%20Crafting%0Aanimations%20that%20are%20semantically%20aware%20poses%20significant%20challenges%2C%20demanding%0Aexpertise%20in%20graphic%20design%20and%20animation.%20We%20present%20an%20automated%20text%0Aanimation%20scheme%2C%20termed%20%22Dynamic%20Typography%22%2C%20which%20combines%20two%20challenging%0Atasks.%20It%20deforms%20letters%20to%20convey%20semantic%20meaning%20and%20infuses%20them%20with%0Avibrant%20movements%20based%20on%20user%20prompts.%20Our%20technique%20harnesses%20vector%0Agraphics%20representations%20and%20an%20end-to-end%20optimization-based%20framework.%20This%0Aframework%20employs%20neural%20displacement%20fields%20to%20convert%20letters%20into%20base%0Ashapes%20and%20applies%20per-frame%20motion%2C%20encouraging%20coherence%20with%20the%20intended%0Atextual%20concept.%20Shape%20preservation%20techniques%20and%20perceptual%20loss%0Aregularization%20are%20employed%20to%20maintain%20legibility%20and%20structural%20integrity%0Athroughout%20the%20animation%20process.%20We%20demonstrate%20the%20generalizability%20of%20our%0Aapproach%20across%20various%20text-to-video%20models%20and%20highlight%20the%20superiority%20of%0Aour%20end-to-end%20methodology%20over%20baseline%20methods%2C%20which%20might%20comprise%20separate%0Atasks.%20Through%20quantitative%20and%20qualitative%20evaluations%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20our%20framework%20in%20generating%20coherent%20text%20animations%20that%0Afaithfully%20interpret%20user%20prompts%20while%20maintaining%20readability.%20Our%20code%20is%0Aavailable%20at%3A%20https%3A//animate-your-word.github.io/demo/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11614v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Typography%3A%20Bringing%20Words%20to%20Life&entry.906535625=Zichen%20Liu%20and%20Yihao%20Meng%20and%20Hao%20Ouyang%20and%20Yue%20Yu%20and%20Bolin%20Zhao%20and%20Daniel%20Cohen-Or%20and%20Huamin%20Qu&entry.1292438233=%20%20Text%20animation%20serves%20as%20an%20expressive%20medium%2C%20transforming%20static%0Acommunication%20into%20dynamic%20experiences%20by%20infusing%20words%20with%20motion%20to%20evoke%0Aemotions%2C%20emphasize%20meanings%2C%20and%20construct%20compelling%20narratives.%20Crafting%0Aanimations%20that%20are%20semantically%20aware%20poses%20significant%20challenges%2C%20demanding%0Aexpertise%20in%20graphic%20design%20and%20animation.%20We%20present%20an%20automated%20text%0Aanimation%20scheme%2C%20termed%20%22Dynamic%20Typography%22%2C%20which%20combines%20two%20challenging%0Atasks.%20It%20deforms%20letters%20to%20convey%20semantic%20meaning%20and%20infuses%20them%20with%0Avibrant%20movements%20based%20on%20user%20prompts.%20Our%20technique%20harnesses%20vector%0Agraphics%20representations%20and%20an%20end-to-end%20optimization-based%20framework.%20This%0Aframework%20employs%20neural%20displacement%20fields%20to%20convert%20letters%20into%20base%0Ashapes%20and%20applies%20per-frame%20motion%2C%20encouraging%20coherence%20with%20the%20intended%0Atextual%20concept.%20Shape%20preservation%20techniques%20and%20perceptual%20loss%0Aregularization%20are%20employed%20to%20maintain%20legibility%20and%20structural%20integrity%0Athroughout%20the%20animation%20process.%20We%20demonstrate%20the%20generalizability%20of%20our%0Aapproach%20across%20various%20text-to-video%20models%20and%20highlight%20the%20superiority%20of%0Aour%20end-to-end%20methodology%20over%20baseline%20methods%2C%20which%20might%20comprise%20separate%0Atasks.%20Through%20quantitative%20and%20qualitative%20evaluations%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20our%20framework%20in%20generating%20coherent%20text%20animations%20that%0Afaithfully%20interpret%20user%20prompts%20while%20maintaining%20readability.%20Our%20code%20is%0Aavailable%20at%3A%20https%3A//animate-your-word.github.io/demo/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11614v1&entry.124074799=Read"},
{"title": "High-throughput Visual Nano-drone to Nano-drone Relative Localization\n  using Onboard Fully Convolutional Networks", "author": "Luca Crupi and Alessandro Giusti and Daniele Palossi", "abstract": "  Relative drone-to-drone localization is a fundamental building block for any\nswarm operations. We address this task in the context of miniaturized\nnano-drones, i.e., 10cm in diameter, which show an ever-growing interest due to\nnovel use cases enabled by their reduced form factor. The price for their\nversatility comes with limited onboard resources, i.e., sensors, processing\nunits, and memory, which limits the complexity of the onboard algorithms. A\ntraditional solution to overcome these limitations is represented by\nlightweight deep learning models directly deployed aboard nano-drones. This\nwork tackles the challenging relative pose estimation between nano-drones using\nonly a gray-scale low-resolution camera and an ultra-low-power System-on-Chip\n(SoC) hosted onboard. We present a vertically integrated system based on a\nnovel vision-based fully convolutional neural network (FCNN), which runs at\n39Hz within 101mW onboard a Crazyflie nano-drone extended with the GWT GAP8\nSoC. We compare our FCNN against three State-of-the-Art (SoA) systems.\nConsidering the best-performing SoA approach, our model results in an R-squared\nimprovement from 32 to 47% on the horizontal image coordinate and from 18 to\n55% on the vertical image coordinate, on a real-world dataset of 30k images.\nFinally, our in-field tests show a reduction of the average tracking error of\n37% compared to a previous SoA work and an endurance performance up to the\nentire battery lifetime of 4 minutes.\n", "link": "http://arxiv.org/abs/2402.13756v3", "date": "2024-04-17", "relevancy": 2.1646, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.56}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5413}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4936}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20High-throughput%20Visual%20Nano-drone%20to%20Nano-drone%20Relative%20Localization%0A%20%20using%20Onboard%20Fully%20Convolutional%20Networks&body=Title%3A%20High-throughput%20Visual%20Nano-drone%20to%20Nano-drone%20Relative%20Localization%0A%20%20using%20Onboard%20Fully%20Convolutional%20Networks%0AAuthor%3A%20Luca%20Crupi%20and%20Alessandro%20Giusti%20and%20Daniele%20Palossi%0AAbstract%3A%20%20%20Relative%20drone-to-drone%20localization%20is%20a%20fundamental%20building%20block%20for%20any%0Aswarm%20operations.%20We%20address%20this%20task%20in%20the%20context%20of%20miniaturized%0Anano-drones%2C%20i.e.%2C%2010cm%20in%20diameter%2C%20which%20show%20an%20ever-growing%20interest%20due%20to%0Anovel%20use%20cases%20enabled%20by%20their%20reduced%20form%20factor.%20The%20price%20for%20their%0Aversatility%20comes%20with%20limited%20onboard%20resources%2C%20i.e.%2C%20sensors%2C%20processing%0Aunits%2C%20and%20memory%2C%20which%20limits%20the%20complexity%20of%20the%20onboard%20algorithms.%20A%0Atraditional%20solution%20to%20overcome%20these%20limitations%20is%20represented%20by%0Alightweight%20deep%20learning%20models%20directly%20deployed%20aboard%20nano-drones.%20This%0Awork%20tackles%20the%20challenging%20relative%20pose%20estimation%20between%20nano-drones%20using%0Aonly%20a%20gray-scale%20low-resolution%20camera%20and%20an%20ultra-low-power%20System-on-Chip%0A%28SoC%29%20hosted%20onboard.%20We%20present%20a%20vertically%20integrated%20system%20based%20on%20a%0Anovel%20vision-based%20fully%20convolutional%20neural%20network%20%28FCNN%29%2C%20which%20runs%20at%0A39Hz%20within%20101mW%20onboard%20a%20Crazyflie%20nano-drone%20extended%20with%20the%20GWT%20GAP8%0ASoC.%20We%20compare%20our%20FCNN%20against%20three%20State-of-the-Art%20%28SoA%29%20systems.%0AConsidering%20the%20best-performing%20SoA%20approach%2C%20our%20model%20results%20in%20an%20R-squared%0Aimprovement%20from%2032%20to%2047%25%20on%20the%20horizontal%20image%20coordinate%20and%20from%2018%20to%0A55%25%20on%20the%20vertical%20image%20coordinate%2C%20on%20a%20real-world%20dataset%20of%2030k%20images.%0AFinally%2C%20our%20in-field%20tests%20show%20a%20reduction%20of%20the%20average%20tracking%20error%20of%0A37%25%20compared%20to%20a%20previous%20SoA%20work%20and%20an%20endurance%20performance%20up%20to%20the%0Aentire%20battery%20lifetime%20of%204%20minutes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13756v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-throughput%20Visual%20Nano-drone%20to%20Nano-drone%20Relative%20Localization%0A%20%20using%20Onboard%20Fully%20Convolutional%20Networks&entry.906535625=Luca%20Crupi%20and%20Alessandro%20Giusti%20and%20Daniele%20Palossi&entry.1292438233=%20%20Relative%20drone-to-drone%20localization%20is%20a%20fundamental%20building%20block%20for%20any%0Aswarm%20operations.%20We%20address%20this%20task%20in%20the%20context%20of%20miniaturized%0Anano-drones%2C%20i.e.%2C%2010cm%20in%20diameter%2C%20which%20show%20an%20ever-growing%20interest%20due%20to%0Anovel%20use%20cases%20enabled%20by%20their%20reduced%20form%20factor.%20The%20price%20for%20their%0Aversatility%20comes%20with%20limited%20onboard%20resources%2C%20i.e.%2C%20sensors%2C%20processing%0Aunits%2C%20and%20memory%2C%20which%20limits%20the%20complexity%20of%20the%20onboard%20algorithms.%20A%0Atraditional%20solution%20to%20overcome%20these%20limitations%20is%20represented%20by%0Alightweight%20deep%20learning%20models%20directly%20deployed%20aboard%20nano-drones.%20This%0Awork%20tackles%20the%20challenging%20relative%20pose%20estimation%20between%20nano-drones%20using%0Aonly%20a%20gray-scale%20low-resolution%20camera%20and%20an%20ultra-low-power%20System-on-Chip%0A%28SoC%29%20hosted%20onboard.%20We%20present%20a%20vertically%20integrated%20system%20based%20on%20a%0Anovel%20vision-based%20fully%20convolutional%20neural%20network%20%28FCNN%29%2C%20which%20runs%20at%0A39Hz%20within%20101mW%20onboard%20a%20Crazyflie%20nano-drone%20extended%20with%20the%20GWT%20GAP8%0ASoC.%20We%20compare%20our%20FCNN%20against%20three%20State-of-the-Art%20%28SoA%29%20systems.%0AConsidering%20the%20best-performing%20SoA%20approach%2C%20our%20model%20results%20in%20an%20R-squared%0Aimprovement%20from%2032%20to%2047%25%20on%20the%20horizontal%20image%20coordinate%20and%20from%2018%20to%0A55%25%20on%20the%20vertical%20image%20coordinate%2C%20on%20a%20real-world%20dataset%20of%2030k%20images.%0AFinally%2C%20our%20in-field%20tests%20show%20a%20reduction%20of%20the%20average%20tracking%20error%20of%0A37%25%20compared%20to%20a%20previous%20SoA%20work%20and%20an%20endurance%20performance%20up%20to%20the%0Aentire%20battery%20lifetime%20of%204%20minutes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13756v3&entry.124074799=Read"},
{"title": "Learning from Unlabelled Data with Transformers: Domain Adaptation for\n  Semantic Segmentation of High Resolution Aerial Images", "author": "Nikolaos Dionelis and Francesco Pro and Luca Maiano and Irene Amerini and Bertrand Le Saux", "abstract": "  Data from satellites or aerial vehicles are most of the times unlabelled.\nAnnotating such data accurately is difficult, requires expertise, and is costly\nin terms of time. Even if Earth Observation (EO) data were correctly labelled,\nlabels might change over time. Learning from unlabelled data within a\nsemi-supervised learning framework for segmentation of aerial images is\nchallenging. In this paper, we develop a new model for semantic segmentation of\nunlabelled images, the Non-annotated Earth Observation Semantic Segmentation\n(NEOS) model. NEOS performs domain adaptation as the target domain does not\nhave ground truth semantic segmentation masks. The distribution inconsistencies\nbetween the target and source domains are due to differences in acquisition\nscenes, environment conditions, sensors, and times. Our model aligns the\nlearned representations of the different domains to make them coincide. The\nevaluation results show that NEOS is successful and outperforms other models\nfor semantic segmentation of unlabelled data.\n", "link": "http://arxiv.org/abs/2404.11299v1", "date": "2024-04-17", "relevancy": 2.1569, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5437}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5423}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5203}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Unlabelled%20Data%20with%20Transformers%3A%20Domain%20Adaptation%20for%0A%20%20Semantic%20Segmentation%20of%20High%20Resolution%20Aerial%20Images&body=Title%3A%20Learning%20from%20Unlabelled%20Data%20with%20Transformers%3A%20Domain%20Adaptation%20for%0A%20%20Semantic%20Segmentation%20of%20High%20Resolution%20Aerial%20Images%0AAuthor%3A%20Nikolaos%20Dionelis%20and%20Francesco%20Pro%20and%20Luca%20Maiano%20and%20Irene%20Amerini%20and%20Bertrand%20Le%20Saux%0AAbstract%3A%20%20%20Data%20from%20satellites%20or%20aerial%20vehicles%20are%20most%20of%20the%20times%20unlabelled.%0AAnnotating%20such%20data%20accurately%20is%20difficult%2C%20requires%20expertise%2C%20and%20is%20costly%0Ain%20terms%20of%20time.%20Even%20if%20Earth%20Observation%20%28EO%29%20data%20were%20correctly%20labelled%2C%0Alabels%20might%20change%20over%20time.%20Learning%20from%20unlabelled%20data%20within%20a%0Asemi-supervised%20learning%20framework%20for%20segmentation%20of%20aerial%20images%20is%0Achallenging.%20In%20this%20paper%2C%20we%20develop%20a%20new%20model%20for%20semantic%20segmentation%20of%0Aunlabelled%20images%2C%20the%20Non-annotated%20Earth%20Observation%20Semantic%20Segmentation%0A%28NEOS%29%20model.%20NEOS%20performs%20domain%20adaptation%20as%20the%20target%20domain%20does%20not%0Ahave%20ground%20truth%20semantic%20segmentation%20masks.%20The%20distribution%20inconsistencies%0Abetween%20the%20target%20and%20source%20domains%20are%20due%20to%20differences%20in%20acquisition%0Ascenes%2C%20environment%20conditions%2C%20sensors%2C%20and%20times.%20Our%20model%20aligns%20the%0Alearned%20representations%20of%20the%20different%20domains%20to%20make%20them%20coincide.%20The%0Aevaluation%20results%20show%20that%20NEOS%20is%20successful%20and%20outperforms%20other%20models%0Afor%20semantic%20segmentation%20of%20unlabelled%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11299v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Unlabelled%20Data%20with%20Transformers%3A%20Domain%20Adaptation%20for%0A%20%20Semantic%20Segmentation%20of%20High%20Resolution%20Aerial%20Images&entry.906535625=Nikolaos%20Dionelis%20and%20Francesco%20Pro%20and%20Luca%20Maiano%20and%20Irene%20Amerini%20and%20Bertrand%20Le%20Saux&entry.1292438233=%20%20Data%20from%20satellites%20or%20aerial%20vehicles%20are%20most%20of%20the%20times%20unlabelled.%0AAnnotating%20such%20data%20accurately%20is%20difficult%2C%20requires%20expertise%2C%20and%20is%20costly%0Ain%20terms%20of%20time.%20Even%20if%20Earth%20Observation%20%28EO%29%20data%20were%20correctly%20labelled%2C%0Alabels%20might%20change%20over%20time.%20Learning%20from%20unlabelled%20data%20within%20a%0Asemi-supervised%20learning%20framework%20for%20segmentation%20of%20aerial%20images%20is%0Achallenging.%20In%20this%20paper%2C%20we%20develop%20a%20new%20model%20for%20semantic%20segmentation%20of%0Aunlabelled%20images%2C%20the%20Non-annotated%20Earth%20Observation%20Semantic%20Segmentation%0A%28NEOS%29%20model.%20NEOS%20performs%20domain%20adaptation%20as%20the%20target%20domain%20does%20not%0Ahave%20ground%20truth%20semantic%20segmentation%20masks.%20The%20distribution%20inconsistencies%0Abetween%20the%20target%20and%20source%20domains%20are%20due%20to%20differences%20in%20acquisition%0Ascenes%2C%20environment%20conditions%2C%20sensors%2C%20and%20times.%20Our%20model%20aligns%20the%0Alearned%20representations%20of%20the%20different%20domains%20to%20make%20them%20coincide.%20The%0Aevaluation%20results%20show%20that%20NEOS%20is%20successful%20and%20outperforms%20other%20models%0Afor%20semantic%20segmentation%20of%20unlabelled%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11299v1&entry.124074799=Read"},
{"title": "Exploring Key Point Analysis with Pairwise Generation and Graph\n  Partitioning", "author": "Xiao Li and Yong Jiang and Shen Huang and Pengjun Xie and Gong Cheng and Fei Huang", "abstract": "  Key Point Analysis (KPA), the summarization of multiple arguments into a\nconcise collection of key points, continues to be a significant and unresolved\nissue within the field of argument mining. Existing models adapt a two-stage\npipeline of clustering arguments or generating key points for argument\nclusters. This approach rely on semantic similarity instead of measuring the\nexistence of shared key points among arguments. Additionally, it only models\nthe intra-cluster relationship among arguments, disregarding the inter-cluster\nrelationship between arguments that do not share key points. To address these\nlimitations, we propose a novel approach for KPA with pairwise generation and\ngraph partitioning. Our objective is to train a generative model that can\nsimultaneously provide a score indicating the presence of shared key point\nbetween a pair of arguments and generate the shared key point. Subsequently, to\nmap generated redundant key points to a concise set of key points, we proceed\nto construct an arguments graph by considering the arguments as vertices, the\ngenerated key points as edges, and the scores as edge weights. We then propose\na graph partitioning algorithm to partition all arguments sharing the same key\npoints to the same subgraph. Notably, our experimental findings demonstrate\nthat our proposed model surpasses previous models when evaluated on both the\nArgKP and QAM datasets.\n", "link": "http://arxiv.org/abs/2404.11384v1", "date": "2024-04-17", "relevancy": 2.1372, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.431}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4288}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4225}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploring%20Key%20Point%20Analysis%20with%20Pairwise%20Generation%20and%20Graph%0A%20%20Partitioning&body=Title%3A%20Exploring%20Key%20Point%20Analysis%20with%20Pairwise%20Generation%20and%20Graph%0A%20%20Partitioning%0AAuthor%3A%20Xiao%20Li%20and%20Yong%20Jiang%20and%20Shen%20Huang%20and%20Pengjun%20Xie%20and%20Gong%20Cheng%20and%20Fei%20Huang%0AAbstract%3A%20%20%20Key%20Point%20Analysis%20%28KPA%29%2C%20the%20summarization%20of%20multiple%20arguments%20into%20a%0Aconcise%20collection%20of%20key%20points%2C%20continues%20to%20be%20a%20significant%20and%20unresolved%0Aissue%20within%20the%20field%20of%20argument%20mining.%20Existing%20models%20adapt%20a%20two-stage%0Apipeline%20of%20clustering%20arguments%20or%20generating%20key%20points%20for%20argument%0Aclusters.%20This%20approach%20rely%20on%20semantic%20similarity%20instead%20of%20measuring%20the%0Aexistence%20of%20shared%20key%20points%20among%20arguments.%20Additionally%2C%20it%20only%20models%0Athe%20intra-cluster%20relationship%20among%20arguments%2C%20disregarding%20the%20inter-cluster%0Arelationship%20between%20arguments%20that%20do%20not%20share%20key%20points.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20novel%20approach%20for%20KPA%20with%20pairwise%20generation%20and%0Agraph%20partitioning.%20Our%20objective%20is%20to%20train%20a%20generative%20model%20that%20can%0Asimultaneously%20provide%20a%20score%20indicating%20the%20presence%20of%20shared%20key%20point%0Abetween%20a%20pair%20of%20arguments%20and%20generate%20the%20shared%20key%20point.%20Subsequently%2C%20to%0Amap%20generated%20redundant%20key%20points%20to%20a%20concise%20set%20of%20key%20points%2C%20we%20proceed%0Ato%20construct%20an%20arguments%20graph%20by%20considering%20the%20arguments%20as%20vertices%2C%20the%0Agenerated%20key%20points%20as%20edges%2C%20and%20the%20scores%20as%20edge%20weights.%20We%20then%20propose%0Aa%20graph%20partitioning%20algorithm%20to%20partition%20all%20arguments%20sharing%20the%20same%20key%0Apoints%20to%20the%20same%20subgraph.%20Notably%2C%20our%20experimental%20findings%20demonstrate%0Athat%20our%20proposed%20model%20surpasses%20previous%20models%20when%20evaluated%20on%20both%20the%0AArgKP%20and%20QAM%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11384v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Key%20Point%20Analysis%20with%20Pairwise%20Generation%20and%20Graph%0A%20%20Partitioning&entry.906535625=Xiao%20Li%20and%20Yong%20Jiang%20and%20Shen%20Huang%20and%20Pengjun%20Xie%20and%20Gong%20Cheng%20and%20Fei%20Huang&entry.1292438233=%20%20Key%20Point%20Analysis%20%28KPA%29%2C%20the%20summarization%20of%20multiple%20arguments%20into%20a%0Aconcise%20collection%20of%20key%20points%2C%20continues%20to%20be%20a%20significant%20and%20unresolved%0Aissue%20within%20the%20field%20of%20argument%20mining.%20Existing%20models%20adapt%20a%20two-stage%0Apipeline%20of%20clustering%20arguments%20or%20generating%20key%20points%20for%20argument%0Aclusters.%20This%20approach%20rely%20on%20semantic%20similarity%20instead%20of%20measuring%20the%0Aexistence%20of%20shared%20key%20points%20among%20arguments.%20Additionally%2C%20it%20only%20models%0Athe%20intra-cluster%20relationship%20among%20arguments%2C%20disregarding%20the%20inter-cluster%0Arelationship%20between%20arguments%20that%20do%20not%20share%20key%20points.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20novel%20approach%20for%20KPA%20with%20pairwise%20generation%20and%0Agraph%20partitioning.%20Our%20objective%20is%20to%20train%20a%20generative%20model%20that%20can%0Asimultaneously%20provide%20a%20score%20indicating%20the%20presence%20of%20shared%20key%20point%0Abetween%20a%20pair%20of%20arguments%20and%20generate%20the%20shared%20key%20point.%20Subsequently%2C%20to%0Amap%20generated%20redundant%20key%20points%20to%20a%20concise%20set%20of%20key%20points%2C%20we%20proceed%0Ato%20construct%20an%20arguments%20graph%20by%20considering%20the%20arguments%20as%20vertices%2C%20the%0Agenerated%20key%20points%20as%20edges%2C%20and%20the%20scores%20as%20edge%20weights.%20We%20then%20propose%0Aa%20graph%20partitioning%20algorithm%20to%20partition%20all%20arguments%20sharing%20the%20same%20key%0Apoints%20to%20the%20same%20subgraph.%20Notably%2C%20our%20experimental%20findings%20demonstrate%0Athat%20our%20proposed%20model%20surpasses%20previous%20models%20when%20evaluated%20on%20both%20the%0AArgKP%20and%20QAM%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11384v1&entry.124074799=Read"},
{"title": "Revisiting Noise Resilience Strategies in Gesture Recognition:\n  Short-Term Enhancement in Surface Electromyographic Signal Analysis", "author": "Weiyu Guo and Ziyue Qiao and Ying Sun and Hui Xiong", "abstract": "  Gesture recognition based on surface electromyography (sEMG) has been gaining\nimportance in many 3D Interactive Scenes. However, sEMG is easily influenced by\nvarious forms of noise in real-world environments, leading to challenges in\nproviding long-term stable interactions through sEMG. Existing methods often\nstruggle to enhance model noise resilience through various predefined data\naugmentation techniques. In this work, we revisit the problem from a short term\nenhancement perspective to improve precision and robustness against various\ncommon noisy scenarios with learnable denoise using sEMG intrinsic pattern\ninformation and sliding-window attention. We propose a Short Term Enhancement\nModule(STEM) which can be easily integrated with various models. STEM offers\nseveral benefits: 1) Learnable denoise, enabling noise reduction without manual\ndata augmentation; 2) Scalability, adaptable to various models; and 3)\nCost-effectiveness, achieving short-term enhancement through minimal\nweight-sharing in an efficient attention mechanism. In particular, we\nincorporate STEM into a transformer, creating the Short Term Enhanced\nTransformer (STET). Compared with best-competing approaches, the impact of\nnoise on STET is reduced by more than 20%. We also report promising results on\nboth classification and regression datasets and demonstrate that STEM\ngeneralizes across different gesture recognition tasks.\n", "link": "http://arxiv.org/abs/2404.11213v1", "date": "2024-04-17", "relevancy": 2.131, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5414}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5348}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5234}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Noise%20Resilience%20Strategies%20in%20Gesture%20Recognition%3A%0A%20%20Short-Term%20Enhancement%20in%20Surface%20Electromyographic%20Signal%20Analysis&body=Title%3A%20Revisiting%20Noise%20Resilience%20Strategies%20in%20Gesture%20Recognition%3A%0A%20%20Short-Term%20Enhancement%20in%20Surface%20Electromyographic%20Signal%20Analysis%0AAuthor%3A%20Weiyu%20Guo%20and%20Ziyue%20Qiao%20and%20Ying%20Sun%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20Gesture%20recognition%20based%20on%20surface%20electromyography%20%28sEMG%29%20has%20been%20gaining%0Aimportance%20in%20many%203D%20Interactive%20Scenes.%20However%2C%20sEMG%20is%20easily%20influenced%20by%0Avarious%20forms%20of%20noise%20in%20real-world%20environments%2C%20leading%20to%20challenges%20in%0Aproviding%20long-term%20stable%20interactions%20through%20sEMG.%20Existing%20methods%20often%0Astruggle%20to%20enhance%20model%20noise%20resilience%20through%20various%20predefined%20data%0Aaugmentation%20techniques.%20In%20this%20work%2C%20we%20revisit%20the%20problem%20from%20a%20short%20term%0Aenhancement%20perspective%20to%20improve%20precision%20and%20robustness%20against%20various%0Acommon%20noisy%20scenarios%20with%20learnable%20denoise%20using%20sEMG%20intrinsic%20pattern%0Ainformation%20and%20sliding-window%20attention.%20We%20propose%20a%20Short%20Term%20Enhancement%0AModule%28STEM%29%20which%20can%20be%20easily%20integrated%20with%20various%20models.%20STEM%20offers%0Aseveral%20benefits%3A%201%29%20Learnable%20denoise%2C%20enabling%20noise%20reduction%20without%20manual%0Adata%20augmentation%3B%202%29%20Scalability%2C%20adaptable%20to%20various%20models%3B%20and%203%29%0ACost-effectiveness%2C%20achieving%20short-term%20enhancement%20through%20minimal%0Aweight-sharing%20in%20an%20efficient%20attention%20mechanism.%20In%20particular%2C%20we%0Aincorporate%20STEM%20into%20a%20transformer%2C%20creating%20the%20Short%20Term%20Enhanced%0ATransformer%20%28STET%29.%20Compared%20with%20best-competing%20approaches%2C%20the%20impact%20of%0Anoise%20on%20STET%20is%20reduced%20by%20more%20than%2020%25.%20We%20also%20report%20promising%20results%20on%0Aboth%20classification%20and%20regression%20datasets%20and%20demonstrate%20that%20STEM%0Ageneralizes%20across%20different%20gesture%20recognition%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11213v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Noise%20Resilience%20Strategies%20in%20Gesture%20Recognition%3A%0A%20%20Short-Term%20Enhancement%20in%20Surface%20Electromyographic%20Signal%20Analysis&entry.906535625=Weiyu%20Guo%20and%20Ziyue%20Qiao%20and%20Ying%20Sun%20and%20Hui%20Xiong&entry.1292438233=%20%20Gesture%20recognition%20based%20on%20surface%20electromyography%20%28sEMG%29%20has%20been%20gaining%0Aimportance%20in%20many%203D%20Interactive%20Scenes.%20However%2C%20sEMG%20is%20easily%20influenced%20by%0Avarious%20forms%20of%20noise%20in%20real-world%20environments%2C%20leading%20to%20challenges%20in%0Aproviding%20long-term%20stable%20interactions%20through%20sEMG.%20Existing%20methods%20often%0Astruggle%20to%20enhance%20model%20noise%20resilience%20through%20various%20predefined%20data%0Aaugmentation%20techniques.%20In%20this%20work%2C%20we%20revisit%20the%20problem%20from%20a%20short%20term%0Aenhancement%20perspective%20to%20improve%20precision%20and%20robustness%20against%20various%0Acommon%20noisy%20scenarios%20with%20learnable%20denoise%20using%20sEMG%20intrinsic%20pattern%0Ainformation%20and%20sliding-window%20attention.%20We%20propose%20a%20Short%20Term%20Enhancement%0AModule%28STEM%29%20which%20can%20be%20easily%20integrated%20with%20various%20models.%20STEM%20offers%0Aseveral%20benefits%3A%201%29%20Learnable%20denoise%2C%20enabling%20noise%20reduction%20without%20manual%0Adata%20augmentation%3B%202%29%20Scalability%2C%20adaptable%20to%20various%20models%3B%20and%203%29%0ACost-effectiveness%2C%20achieving%20short-term%20enhancement%20through%20minimal%0Aweight-sharing%20in%20an%20efficient%20attention%20mechanism.%20In%20particular%2C%20we%0Aincorporate%20STEM%20into%20a%20transformer%2C%20creating%20the%20Short%20Term%20Enhanced%0ATransformer%20%28STET%29.%20Compared%20with%20best-competing%20approaches%2C%20the%20impact%20of%0Anoise%20on%20STET%20is%20reduced%20by%20more%20than%2020%25.%20We%20also%20report%20promising%20results%20on%0Aboth%20classification%20and%20regression%20datasets%20and%20demonstrate%20that%20STEM%0Ageneralizes%20across%20different%20gesture%20recognition%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11213v1&entry.124074799=Read"},
{"title": "SuperPrimitive: Scene Reconstruction at a Primitive Level", "author": "Kirill Mazur and Gwangbin Bae and Andrew J. Davison", "abstract": "  Joint camera pose and dense geometry estimation from a set of images or a\nmonocular video remains a challenging problem due to its computational\ncomplexity and inherent visual ambiguities. Most dense incremental\nreconstruction systems operate directly on image pixels and solve for their 3D\npositions using multi-view geometry cues. Such pixel-level approaches suffer\nfrom ambiguities or violations of multi-view consistency (e.g. caused by\ntextureless or specular surfaces).\n  We address this issue with a new image representation which we call a\nSuperPrimitive. SuperPrimitives are obtained by splitting images into\nsemantically correlated local regions and enhancing them with estimated surface\nnormal directions, both of which are predicted by state-of-the-art single image\nneural networks. This provides a local geometry estimate per SuperPrimitive,\nwhile their relative positions are adjusted based on multi-view observations.\n  We demonstrate the versatility of our new representation by addressing three\n3D reconstruction tasks: depth completion, few-view structure from motion, and\nmonocular dense visual odometry.\n", "link": "http://arxiv.org/abs/2312.05889v2", "date": "2024-04-17", "relevancy": 2.1264, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5373}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5278}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5274}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SuperPrimitive%3A%20Scene%20Reconstruction%20at%20a%20Primitive%20Level&body=Title%3A%20SuperPrimitive%3A%20Scene%20Reconstruction%20at%20a%20Primitive%20Level%0AAuthor%3A%20Kirill%20Mazur%20and%20Gwangbin%20Bae%20and%20Andrew%20J.%20Davison%0AAbstract%3A%20%20%20Joint%20camera%20pose%20and%20dense%20geometry%20estimation%20from%20a%20set%20of%20images%20or%20a%0Amonocular%20video%20remains%20a%20challenging%20problem%20due%20to%20its%20computational%0Acomplexity%20and%20inherent%20visual%20ambiguities.%20Most%20dense%20incremental%0Areconstruction%20systems%20operate%20directly%20on%20image%20pixels%20and%20solve%20for%20their%203D%0Apositions%20using%20multi-view%20geometry%20cues.%20Such%20pixel-level%20approaches%20suffer%0Afrom%20ambiguities%20or%20violations%20of%20multi-view%20consistency%20%28e.g.%20caused%20by%0Atextureless%20or%20specular%20surfaces%29.%0A%20%20We%20address%20this%20issue%20with%20a%20new%20image%20representation%20which%20we%20call%20a%0ASuperPrimitive.%20SuperPrimitives%20are%20obtained%20by%20splitting%20images%20into%0Asemantically%20correlated%20local%20regions%20and%20enhancing%20them%20with%20estimated%20surface%0Anormal%20directions%2C%20both%20of%20which%20are%20predicted%20by%20state-of-the-art%20single%20image%0Aneural%20networks.%20This%20provides%20a%20local%20geometry%20estimate%20per%20SuperPrimitive%2C%0Awhile%20their%20relative%20positions%20are%20adjusted%20based%20on%20multi-view%20observations.%0A%20%20We%20demonstrate%20the%20versatility%20of%20our%20new%20representation%20by%20addressing%20three%0A3D%20reconstruction%20tasks%3A%20depth%20completion%2C%20few-view%20structure%20from%20motion%2C%20and%0Amonocular%20dense%20visual%20odometry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05889v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuperPrimitive%3A%20Scene%20Reconstruction%20at%20a%20Primitive%20Level&entry.906535625=Kirill%20Mazur%20and%20Gwangbin%20Bae%20and%20Andrew%20J.%20Davison&entry.1292438233=%20%20Joint%20camera%20pose%20and%20dense%20geometry%20estimation%20from%20a%20set%20of%20images%20or%20a%0Amonocular%20video%20remains%20a%20challenging%20problem%20due%20to%20its%20computational%0Acomplexity%20and%20inherent%20visual%20ambiguities.%20Most%20dense%20incremental%0Areconstruction%20systems%20operate%20directly%20on%20image%20pixels%20and%20solve%20for%20their%203D%0Apositions%20using%20multi-view%20geometry%20cues.%20Such%20pixel-level%20approaches%20suffer%0Afrom%20ambiguities%20or%20violations%20of%20multi-view%20consistency%20%28e.g.%20caused%20by%0Atextureless%20or%20specular%20surfaces%29.%0A%20%20We%20address%20this%20issue%20with%20a%20new%20image%20representation%20which%20we%20call%20a%0ASuperPrimitive.%20SuperPrimitives%20are%20obtained%20by%20splitting%20images%20into%0Asemantically%20correlated%20local%20regions%20and%20enhancing%20them%20with%20estimated%20surface%0Anormal%20directions%2C%20both%20of%20which%20are%20predicted%20by%20state-of-the-art%20single%20image%0Aneural%20networks.%20This%20provides%20a%20local%20geometry%20estimate%20per%20SuperPrimitive%2C%0Awhile%20their%20relative%20positions%20are%20adjusted%20based%20on%20multi-view%20observations.%0A%20%20We%20demonstrate%20the%20versatility%20of%20our%20new%20representation%20by%20addressing%20three%0A3D%20reconstruction%20tasks%3A%20depth%20completion%2C%20few-view%20structure%20from%20motion%2C%20and%0Amonocular%20dense%20visual%20odometry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05889v2&entry.124074799=Read"},
{"title": "ODM: A Text-Image Further Alignment Pre-training Approach for Scene Text\n  Detection and Spotting", "author": "Chen Duan and Pei Fu and Shan Guo and Qianyi Jiang and Xiaoming Wei", "abstract": "  In recent years, text-image joint pre-training techniques have shown\npromising results in various tasks. However, in Optical Character Recognition\n(OCR) tasks, aligning text instances with their corresponding text regions in\nimages poses a challenge, as it requires effective alignment between text and\nOCR-Text (referring to the text in images as OCR-Text to distinguish from the\ntext in natural language) rather than a holistic understanding of the overall\nimage content. In this paper, we propose a new pre-training method called\nOCR-Text Destylization Modeling (ODM) that transfers diverse styles of text\nfound in images to a uniform style based on the text prompt. With ODM, we\nachieve better alignment between text and OCR-Text and enable pre-trained\nmodels to adapt to the complex and diverse styles of scene text detection and\nspotting tasks. Additionally, we have designed a new labeling generation method\nspecifically for ODM and combined it with our proposed Text-Controller module\nto address the challenge of annotation costs in OCR tasks, allowing a larger\namount of unlabeled data to participate in pre-training. Extensive experiments\non multiple public datasets demonstrate that our method significantly improves\nperformance and outperforms current pre-training methods in scene text\ndetection and spotting tasks. Code is available at\nhttps://github.com/PriNing/ODM.\n", "link": "http://arxiv.org/abs/2403.00303v2", "date": "2024-04-17", "relevancy": 2.1181, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5488}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5181}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5099}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ODM%3A%20A%20Text-Image%20Further%20Alignment%20Pre-training%20Approach%20for%20Scene%20Text%0A%20%20Detection%20and%20Spotting&body=Title%3A%20ODM%3A%20A%20Text-Image%20Further%20Alignment%20Pre-training%20Approach%20for%20Scene%20Text%0A%20%20Detection%20and%20Spotting%0AAuthor%3A%20Chen%20Duan%20and%20Pei%20Fu%20and%20Shan%20Guo%20and%20Qianyi%20Jiang%20and%20Xiaoming%20Wei%0AAbstract%3A%20%20%20In%20recent%20years%2C%20text-image%20joint%20pre-training%20techniques%20have%20shown%0Apromising%20results%20in%20various%20tasks.%20However%2C%20in%20Optical%20Character%20Recognition%0A%28OCR%29%20tasks%2C%20aligning%20text%20instances%20with%20their%20corresponding%20text%20regions%20in%0Aimages%20poses%20a%20challenge%2C%20as%20it%20requires%20effective%20alignment%20between%20text%20and%0AOCR-Text%20%28referring%20to%20the%20text%20in%20images%20as%20OCR-Text%20to%20distinguish%20from%20the%0Atext%20in%20natural%20language%29%20rather%20than%20a%20holistic%20understanding%20of%20the%20overall%0Aimage%20content.%20In%20this%20paper%2C%20we%20propose%20a%20new%20pre-training%20method%20called%0AOCR-Text%20Destylization%20Modeling%20%28ODM%29%20that%20transfers%20diverse%20styles%20of%20text%0Afound%20in%20images%20to%20a%20uniform%20style%20based%20on%20the%20text%20prompt.%20With%20ODM%2C%20we%0Aachieve%20better%20alignment%20between%20text%20and%20OCR-Text%20and%20enable%20pre-trained%0Amodels%20to%20adapt%20to%20the%20complex%20and%20diverse%20styles%20of%20scene%20text%20detection%20and%0Aspotting%20tasks.%20Additionally%2C%20we%20have%20designed%20a%20new%20labeling%20generation%20method%0Aspecifically%20for%20ODM%20and%20combined%20it%20with%20our%20proposed%20Text-Controller%20module%0Ato%20address%20the%20challenge%20of%20annotation%20costs%20in%20OCR%20tasks%2C%20allowing%20a%20larger%0Aamount%20of%20unlabeled%20data%20to%20participate%20in%20pre-training.%20Extensive%20experiments%0Aon%20multiple%20public%20datasets%20demonstrate%20that%20our%20method%20significantly%20improves%0Aperformance%20and%20outperforms%20current%20pre-training%20methods%20in%20scene%20text%0Adetection%20and%20spotting%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/PriNing/ODM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00303v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ODM%3A%20A%20Text-Image%20Further%20Alignment%20Pre-training%20Approach%20for%20Scene%20Text%0A%20%20Detection%20and%20Spotting&entry.906535625=Chen%20Duan%20and%20Pei%20Fu%20and%20Shan%20Guo%20and%20Qianyi%20Jiang%20and%20Xiaoming%20Wei&entry.1292438233=%20%20In%20recent%20years%2C%20text-image%20joint%20pre-training%20techniques%20have%20shown%0Apromising%20results%20in%20various%20tasks.%20However%2C%20in%20Optical%20Character%20Recognition%0A%28OCR%29%20tasks%2C%20aligning%20text%20instances%20with%20their%20corresponding%20text%20regions%20in%0Aimages%20poses%20a%20challenge%2C%20as%20it%20requires%20effective%20alignment%20between%20text%20and%0AOCR-Text%20%28referring%20to%20the%20text%20in%20images%20as%20OCR-Text%20to%20distinguish%20from%20the%0Atext%20in%20natural%20language%29%20rather%20than%20a%20holistic%20understanding%20of%20the%20overall%0Aimage%20content.%20In%20this%20paper%2C%20we%20propose%20a%20new%20pre-training%20method%20called%0AOCR-Text%20Destylization%20Modeling%20%28ODM%29%20that%20transfers%20diverse%20styles%20of%20text%0Afound%20in%20images%20to%20a%20uniform%20style%20based%20on%20the%20text%20prompt.%20With%20ODM%2C%20we%0Aachieve%20better%20alignment%20between%20text%20and%20OCR-Text%20and%20enable%20pre-trained%0Amodels%20to%20adapt%20to%20the%20complex%20and%20diverse%20styles%20of%20scene%20text%20detection%20and%0Aspotting%20tasks.%20Additionally%2C%20we%20have%20designed%20a%20new%20labeling%20generation%20method%0Aspecifically%20for%20ODM%20and%20combined%20it%20with%20our%20proposed%20Text-Controller%20module%0Ato%20address%20the%20challenge%20of%20annotation%20costs%20in%20OCR%20tasks%2C%20allowing%20a%20larger%0Aamount%20of%20unlabeled%20data%20to%20participate%20in%20pre-training.%20Extensive%20experiments%0Aon%20multiple%20public%20datasets%20demonstrate%20that%20our%20method%20significantly%20improves%0Aperformance%20and%20outperforms%20current%20pre-training%20methods%20in%20scene%20text%0Adetection%20and%20spotting%20tasks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/PriNing/ODM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00303v2&entry.124074799=Read"},
{"title": "SPAMming Labels: Efficient Annotations for the Trackers of Tomorrow", "author": "Orcun Cetintas and Tim Meinhardt and Guillem Bras\u00f3 and Laura Leal-Taix\u00e9", "abstract": "  Increasing the annotation efficiency of trajectory annotations from videos\nhas the potential to enable the next generation of data-hungry tracking\nalgorithms to thrive on large-scale datasets. Despite the importance of this\ntask, there are currently very few works exploring how to efficiently label\ntracking datasets comprehensively. In this work, we introduce SPAM, a tracking\ndata engine that provides high-quality labels with minimal human intervention.\nSPAM is built around two key insights: i) most tracking scenarios can be easily\nresolved. To take advantage of this, we utilize a pre-trained model to generate\nhigh-quality pseudo-labels, reserving human involvement for a smaller subset of\nmore difficult instances; ii) handling the spatiotemporal dependencies of track\nannotations across time can be elegantly and efficiently formulated through\ngraphs. Therefore, we use a unified graph formulation to address the annotation\nof both detections and identity association for tracks across time. Based on\nthese insights, SPAM produces high-quality annotations with a fraction of\nground truth labeling cost. We demonstrate that trackers trained on SPAM labels\nachieve comparable performance to those trained on human annotations while\nrequiring only 3-20% of the human labeling effort. Hence, SPAM paves the way\ntowards highly efficient labeling of large-scale tracking datasets. Our code\nand models will be available upon acceptance.\n", "link": "http://arxiv.org/abs/2404.11426v1", "date": "2024-04-17", "relevancy": 2.1172, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5409}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5294}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5001}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SPAMming%20Labels%3A%20Efficient%20Annotations%20for%20the%20Trackers%20of%20Tomorrow&body=Title%3A%20SPAMming%20Labels%3A%20Efficient%20Annotations%20for%20the%20Trackers%20of%20Tomorrow%0AAuthor%3A%20Orcun%20Cetintas%20and%20Tim%20Meinhardt%20and%20Guillem%20Bras%C3%B3%20and%20Laura%20Leal-Taix%C3%A9%0AAbstract%3A%20%20%20Increasing%20the%20annotation%20efficiency%20of%20trajectory%20annotations%20from%20videos%0Ahas%20the%20potential%20to%20enable%20the%20next%20generation%20of%20data-hungry%20tracking%0Aalgorithms%20to%20thrive%20on%20large-scale%20datasets.%20Despite%20the%20importance%20of%20this%0Atask%2C%20there%20are%20currently%20very%20few%20works%20exploring%20how%20to%20efficiently%20label%0Atracking%20datasets%20comprehensively.%20In%20this%20work%2C%20we%20introduce%20SPAM%2C%20a%20tracking%0Adata%20engine%20that%20provides%20high-quality%20labels%20with%20minimal%20human%20intervention.%0ASPAM%20is%20built%20around%20two%20key%20insights%3A%20i%29%20most%20tracking%20scenarios%20can%20be%20easily%0Aresolved.%20To%20take%20advantage%20of%20this%2C%20we%20utilize%20a%20pre-trained%20model%20to%20generate%0Ahigh-quality%20pseudo-labels%2C%20reserving%20human%20involvement%20for%20a%20smaller%20subset%20of%0Amore%20difficult%20instances%3B%20ii%29%20handling%20the%20spatiotemporal%20dependencies%20of%20track%0Aannotations%20across%20time%20can%20be%20elegantly%20and%20efficiently%20formulated%20through%0Agraphs.%20Therefore%2C%20we%20use%20a%20unified%20graph%20formulation%20to%20address%20the%20annotation%0Aof%20both%20detections%20and%20identity%20association%20for%20tracks%20across%20time.%20Based%20on%0Athese%20insights%2C%20SPAM%20produces%20high-quality%20annotations%20with%20a%20fraction%20of%0Aground%20truth%20labeling%20cost.%20We%20demonstrate%20that%20trackers%20trained%20on%20SPAM%20labels%0Aachieve%20comparable%20performance%20to%20those%20trained%20on%20human%20annotations%20while%0Arequiring%20only%203-20%25%20of%20the%20human%20labeling%20effort.%20Hence%2C%20SPAM%20paves%20the%20way%0Atowards%20highly%20efficient%20labeling%20of%20large-scale%20tracking%20datasets.%20Our%20code%0Aand%20models%20will%20be%20available%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11426v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPAMming%20Labels%3A%20Efficient%20Annotations%20for%20the%20Trackers%20of%20Tomorrow&entry.906535625=Orcun%20Cetintas%20and%20Tim%20Meinhardt%20and%20Guillem%20Bras%C3%B3%20and%20Laura%20Leal-Taix%C3%A9&entry.1292438233=%20%20Increasing%20the%20annotation%20efficiency%20of%20trajectory%20annotations%20from%20videos%0Ahas%20the%20potential%20to%20enable%20the%20next%20generation%20of%20data-hungry%20tracking%0Aalgorithms%20to%20thrive%20on%20large-scale%20datasets.%20Despite%20the%20importance%20of%20this%0Atask%2C%20there%20are%20currently%20very%20few%20works%20exploring%20how%20to%20efficiently%20label%0Atracking%20datasets%20comprehensively.%20In%20this%20work%2C%20we%20introduce%20SPAM%2C%20a%20tracking%0Adata%20engine%20that%20provides%20high-quality%20labels%20with%20minimal%20human%20intervention.%0ASPAM%20is%20built%20around%20two%20key%20insights%3A%20i%29%20most%20tracking%20scenarios%20can%20be%20easily%0Aresolved.%20To%20take%20advantage%20of%20this%2C%20we%20utilize%20a%20pre-trained%20model%20to%20generate%0Ahigh-quality%20pseudo-labels%2C%20reserving%20human%20involvement%20for%20a%20smaller%20subset%20of%0Amore%20difficult%20instances%3B%20ii%29%20handling%20the%20spatiotemporal%20dependencies%20of%20track%0Aannotations%20across%20time%20can%20be%20elegantly%20and%20efficiently%20formulated%20through%0Agraphs.%20Therefore%2C%20we%20use%20a%20unified%20graph%20formulation%20to%20address%20the%20annotation%0Aof%20both%20detections%20and%20identity%20association%20for%20tracks%20across%20time.%20Based%20on%0Athese%20insights%2C%20SPAM%20produces%20high-quality%20annotations%20with%20a%20fraction%20of%0Aground%20truth%20labeling%20cost.%20We%20demonstrate%20that%20trackers%20trained%20on%20SPAM%20labels%0Aachieve%20comparable%20performance%20to%20those%20trained%20on%20human%20annotations%20while%0Arequiring%20only%203-20%25%20of%20the%20human%20labeling%20effort.%20Hence%2C%20SPAM%20paves%20the%20way%0Atowards%20highly%20efficient%20labeling%20of%20large-scale%20tracking%20datasets.%20Our%20code%0Aand%20models%20will%20be%20available%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11426v1&entry.124074799=Read"},
{"title": "Hybrid Functional Maps for Crease-Aware Non-Isometric Shape Matching", "author": "Lennart Bastian and Yizheng Xie and Nassir Navab and Zorah L\u00e4hner", "abstract": "  Non-isometric shape correspondence remains a fundamental challenge in\ncomputer vision. Traditional methods using Laplace-Beltrami operator (LBO)\neigenmodes face limitations in characterizing high-frequency extrinsic shape\nchanges like bending and creases. We propose a novel approach of combining the\nnon-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shell\nhessian with the intrinsic ones of the LBO, creating a hybrid spectral space in\nwhich we construct functional maps. To this end, we present a theoretical\nframework to effectively integrate non-orthogonal basis functions into\ndescriptor- and learning-based functional map methods. Our approach can be\nincorporated easily into existing functional map pipelines across varying\napplications and is able to handle complex deformations beyond isometries. We\nshow extensive evaluations across various supervised and unsupervised settings\nand demonstrate significant improvements. Notably, our approach achieves up to\n15% better mean geodesic error for non-isometric correspondence settings and up\nto 45% improvement in scenarios with topological noise.\n", "link": "http://arxiv.org/abs/2312.03678v2", "date": "2024-04-17", "relevancy": 2.1152, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5386}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5231}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5185}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Functional%20Maps%20for%20Crease-Aware%20Non-Isometric%20Shape%20Matching&body=Title%3A%20Hybrid%20Functional%20Maps%20for%20Crease-Aware%20Non-Isometric%20Shape%20Matching%0AAuthor%3A%20Lennart%20Bastian%20and%20Yizheng%20Xie%20and%20Nassir%20Navab%20and%20Zorah%20L%C3%A4hner%0AAbstract%3A%20%20%20Non-isometric%20shape%20correspondence%20remains%20a%20fundamental%20challenge%20in%0Acomputer%20vision.%20Traditional%20methods%20using%20Laplace-Beltrami%20operator%20%28LBO%29%0Aeigenmodes%20face%20limitations%20in%20characterizing%20high-frequency%20extrinsic%20shape%0Achanges%20like%20bending%20and%20creases.%20We%20propose%20a%20novel%20approach%20of%20combining%20the%0Anon-orthogonal%20extrinsic%20basis%20of%20eigenfunctions%20of%20the%20elastic%20thin-shell%0Ahessian%20with%20the%20intrinsic%20ones%20of%20the%20LBO%2C%20creating%20a%20hybrid%20spectral%20space%20in%0Awhich%20we%20construct%20functional%20maps.%20To%20this%20end%2C%20we%20present%20a%20theoretical%0Aframework%20to%20effectively%20integrate%20non-orthogonal%20basis%20functions%20into%0Adescriptor-%20and%20learning-based%20functional%20map%20methods.%20Our%20approach%20can%20be%0Aincorporated%20easily%20into%20existing%20functional%20map%20pipelines%20across%20varying%0Aapplications%20and%20is%20able%20to%20handle%20complex%20deformations%20beyond%20isometries.%20We%0Ashow%20extensive%20evaluations%20across%20various%20supervised%20and%20unsupervised%20settings%0Aand%20demonstrate%20significant%20improvements.%20Notably%2C%20our%20approach%20achieves%20up%20to%0A15%25%20better%20mean%20geodesic%20error%20for%20non-isometric%20correspondence%20settings%20and%20up%0Ato%2045%25%20improvement%20in%20scenarios%20with%20topological%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03678v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Functional%20Maps%20for%20Crease-Aware%20Non-Isometric%20Shape%20Matching&entry.906535625=Lennart%20Bastian%20and%20Yizheng%20Xie%20and%20Nassir%20Navab%20and%20Zorah%20L%C3%A4hner&entry.1292438233=%20%20Non-isometric%20shape%20correspondence%20remains%20a%20fundamental%20challenge%20in%0Acomputer%20vision.%20Traditional%20methods%20using%20Laplace-Beltrami%20operator%20%28LBO%29%0Aeigenmodes%20face%20limitations%20in%20characterizing%20high-frequency%20extrinsic%20shape%0Achanges%20like%20bending%20and%20creases.%20We%20propose%20a%20novel%20approach%20of%20combining%20the%0Anon-orthogonal%20extrinsic%20basis%20of%20eigenfunctions%20of%20the%20elastic%20thin-shell%0Ahessian%20with%20the%20intrinsic%20ones%20of%20the%20LBO%2C%20creating%20a%20hybrid%20spectral%20space%20in%0Awhich%20we%20construct%20functional%20maps.%20To%20this%20end%2C%20we%20present%20a%20theoretical%0Aframework%20to%20effectively%20integrate%20non-orthogonal%20basis%20functions%20into%0Adescriptor-%20and%20learning-based%20functional%20map%20methods.%20Our%20approach%20can%20be%0Aincorporated%20easily%20into%20existing%20functional%20map%20pipelines%20across%20varying%0Aapplications%20and%20is%20able%20to%20handle%20complex%20deformations%20beyond%20isometries.%20We%0Ashow%20extensive%20evaluations%20across%20various%20supervised%20and%20unsupervised%20settings%0Aand%20demonstrate%20significant%20improvements.%20Notably%2C%20our%20approach%20achieves%20up%20to%0A15%25%20better%20mean%20geodesic%20error%20for%20non-isometric%20correspondence%20settings%20and%20up%0Ato%2045%25%20improvement%20in%20scenarios%20with%20topological%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03678v2&entry.124074799=Read"},
{"title": "Calibrating Bayesian Learning via Regularization, Confidence\n  Minimization, and Selective Inference", "author": "Jiayi Huang and Sangwoo Park and Osvaldo Simeone", "abstract": "  The application of artificial intelligence (AI) models in fields such as\nengineering is limited by the known difficulty of quantifying the reliability\nof an AI's decision. A well-calibrated AI model must correctly report its\naccuracy on in-distribution (ID) inputs, while also enabling the detection of\nout-of-distribution (OOD) inputs. A conventional approach to improve\ncalibration is the application of Bayesian ensembling. However, owing to\ncomputational limitations and model misspecification, practical ensembling\nstrategies do not necessarily enhance calibration. This paper proposes an\nextension of variational inference (VI)-based Bayesian learning that integrates\ncalibration regularization for improved ID performance, confidence minimization\nfor OOD detection, and selective calibration to ensure a synergistic use of\ncalibration regularization and confidence minimization. The scheme is\nconstructed successively by first introducing calibration-regularized Bayesian\nlearning (CBNN), then incorporating out-of-distribution confidence minimization\n(OCM) to yield CBNN-OCM, and finally integrating also selective calibration to\nproduce selective CBNN-OCM (SCBNN-OCM). Selective calibration rejects inputs\nfor which the calibration performance is expected to be insufficient. Numerical\nresults illustrate the trade-offs between ID accuracy, ID calibration, and OOD\ncalibration attained by both frequentist and Bayesian learning methods. Among\nthe main conclusions, SCBNN-OCM is seen to achieve best ID and OOD performance\nas compared to existing state-of-the-art approaches at the cost of rejecting a\nsufficiently large number of inputs.\n", "link": "http://arxiv.org/abs/2404.11350v1", "date": "2024-04-17", "relevancy": 2.1087, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5633}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5302}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5098}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Calibrating%20Bayesian%20Learning%20via%20Regularization%2C%20Confidence%0A%20%20Minimization%2C%20and%20Selective%20Inference&body=Title%3A%20Calibrating%20Bayesian%20Learning%20via%20Regularization%2C%20Confidence%0A%20%20Minimization%2C%20and%20Selective%20Inference%0AAuthor%3A%20Jiayi%20Huang%20and%20Sangwoo%20Park%20and%20Osvaldo%20Simeone%0AAbstract%3A%20%20%20The%20application%20of%20artificial%20intelligence%20%28AI%29%20models%20in%20fields%20such%20as%0Aengineering%20is%20limited%20by%20the%20known%20difficulty%20of%20quantifying%20the%20reliability%0Aof%20an%20AI%27s%20decision.%20A%20well-calibrated%20AI%20model%20must%20correctly%20report%20its%0Aaccuracy%20on%20in-distribution%20%28ID%29%20inputs%2C%20while%20also%20enabling%20the%20detection%20of%0Aout-of-distribution%20%28OOD%29%20inputs.%20A%20conventional%20approach%20to%20improve%0Acalibration%20is%20the%20application%20of%20Bayesian%20ensembling.%20However%2C%20owing%20to%0Acomputational%20limitations%20and%20model%20misspecification%2C%20practical%20ensembling%0Astrategies%20do%20not%20necessarily%20enhance%20calibration.%20This%20paper%20proposes%20an%0Aextension%20of%20variational%20inference%20%28VI%29-based%20Bayesian%20learning%20that%20integrates%0Acalibration%20regularization%20for%20improved%20ID%20performance%2C%20confidence%20minimization%0Afor%20OOD%20detection%2C%20and%20selective%20calibration%20to%20ensure%20a%20synergistic%20use%20of%0Acalibration%20regularization%20and%20confidence%20minimization.%20The%20scheme%20is%0Aconstructed%20successively%20by%20first%20introducing%20calibration-regularized%20Bayesian%0Alearning%20%28CBNN%29%2C%20then%20incorporating%20out-of-distribution%20confidence%20minimization%0A%28OCM%29%20to%20yield%20CBNN-OCM%2C%20and%20finally%20integrating%20also%20selective%20calibration%20to%0Aproduce%20selective%20CBNN-OCM%20%28SCBNN-OCM%29.%20Selective%20calibration%20rejects%20inputs%0Afor%20which%20the%20calibration%20performance%20is%20expected%20to%20be%20insufficient.%20Numerical%0Aresults%20illustrate%20the%20trade-offs%20between%20ID%20accuracy%2C%20ID%20calibration%2C%20and%20OOD%0Acalibration%20attained%20by%20both%20frequentist%20and%20Bayesian%20learning%20methods.%20Among%0Athe%20main%20conclusions%2C%20SCBNN-OCM%20is%20seen%20to%20achieve%20best%20ID%20and%20OOD%20performance%0Aas%20compared%20to%20existing%20state-of-the-art%20approaches%20at%20the%20cost%20of%20rejecting%20a%0Asufficiently%20large%20number%20of%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11350v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibrating%20Bayesian%20Learning%20via%20Regularization%2C%20Confidence%0A%20%20Minimization%2C%20and%20Selective%20Inference&entry.906535625=Jiayi%20Huang%20and%20Sangwoo%20Park%20and%20Osvaldo%20Simeone&entry.1292438233=%20%20The%20application%20of%20artificial%20intelligence%20%28AI%29%20models%20in%20fields%20such%20as%0Aengineering%20is%20limited%20by%20the%20known%20difficulty%20of%20quantifying%20the%20reliability%0Aof%20an%20AI%27s%20decision.%20A%20well-calibrated%20AI%20model%20must%20correctly%20report%20its%0Aaccuracy%20on%20in-distribution%20%28ID%29%20inputs%2C%20while%20also%20enabling%20the%20detection%20of%0Aout-of-distribution%20%28OOD%29%20inputs.%20A%20conventional%20approach%20to%20improve%0Acalibration%20is%20the%20application%20of%20Bayesian%20ensembling.%20However%2C%20owing%20to%0Acomputational%20limitations%20and%20model%20misspecification%2C%20practical%20ensembling%0Astrategies%20do%20not%20necessarily%20enhance%20calibration.%20This%20paper%20proposes%20an%0Aextension%20of%20variational%20inference%20%28VI%29-based%20Bayesian%20learning%20that%20integrates%0Acalibration%20regularization%20for%20improved%20ID%20performance%2C%20confidence%20minimization%0Afor%20OOD%20detection%2C%20and%20selective%20calibration%20to%20ensure%20a%20synergistic%20use%20of%0Acalibration%20regularization%20and%20confidence%20minimization.%20The%20scheme%20is%0Aconstructed%20successively%20by%20first%20introducing%20calibration-regularized%20Bayesian%0Alearning%20%28CBNN%29%2C%20then%20incorporating%20out-of-distribution%20confidence%20minimization%0A%28OCM%29%20to%20yield%20CBNN-OCM%2C%20and%20finally%20integrating%20also%20selective%20calibration%20to%0Aproduce%20selective%20CBNN-OCM%20%28SCBNN-OCM%29.%20Selective%20calibration%20rejects%20inputs%0Afor%20which%20the%20calibration%20performance%20is%20expected%20to%20be%20insufficient.%20Numerical%0Aresults%20illustrate%20the%20trade-offs%20between%20ID%20accuracy%2C%20ID%20calibration%2C%20and%20OOD%0Acalibration%20attained%20by%20both%20frequentist%20and%20Bayesian%20learning%20methods.%20Among%0Athe%20main%20conclusions%2C%20SCBNN-OCM%20is%20seen%20to%20achieve%20best%20ID%20and%20OOD%20performance%0Aas%20compared%20to%20existing%20state-of-the-art%20approaches%20at%20the%20cost%20of%20rejecting%20a%0Asufficiently%20large%20number%20of%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11350v1&entry.124074799=Read"},
{"title": "Octopus v3: Technical Report for On-device Sub-billion Multimodal AI\n  Agent", "author": "Wei Chen and Zhiyuan Li", "abstract": "  A multimodal AI agent is characterized by its ability to process and learn\nfrom various types of data, including natural language, visual, and audio\ninputs, to inform its actions. Despite advancements in large language models\nthat incorporate visual data, such as GPT-4V, effectively translating\nimage-based data into actionable outcomes for AI agents continues to be\nchallenging. In this paper, we introduce a multimodal model that incorporates\nthe concept of functional token specifically designed for AI agent\napplications. To ensure compatibility with edge devices, our model is optimized\nto a compact size of less than 1B parameters. Like GPT-4, our model can process\nboth English and Chinese. We demonstrate that this model is capable of\noperating efficiently on a wide range of edge devices, including as constrained\nas a Raspberry Pi.\n", "link": "http://arxiv.org/abs/2404.11459v1", "date": "2024-04-17", "relevancy": 2.0994, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5697}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5178}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.514}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Octopus%20v3%3A%20Technical%20Report%20for%20On-device%20Sub-billion%20Multimodal%20AI%0A%20%20Agent&body=Title%3A%20Octopus%20v3%3A%20Technical%20Report%20for%20On-device%20Sub-billion%20Multimodal%20AI%0A%20%20Agent%0AAuthor%3A%20Wei%20Chen%20and%20Zhiyuan%20Li%0AAbstract%3A%20%20%20A%20multimodal%20AI%20agent%20is%20characterized%20by%20its%20ability%20to%20process%20and%20learn%0Afrom%20various%20types%20of%20data%2C%20including%20natural%20language%2C%20visual%2C%20and%20audio%0Ainputs%2C%20to%20inform%20its%20actions.%20Despite%20advancements%20in%20large%20language%20models%0Athat%20incorporate%20visual%20data%2C%20such%20as%20GPT-4V%2C%20effectively%20translating%0Aimage-based%20data%20into%20actionable%20outcomes%20for%20AI%20agents%20continues%20to%20be%0Achallenging.%20In%20this%20paper%2C%20we%20introduce%20a%20multimodal%20model%20that%20incorporates%0Athe%20concept%20of%20functional%20token%20specifically%20designed%20for%20AI%20agent%0Aapplications.%20To%20ensure%20compatibility%20with%20edge%20devices%2C%20our%20model%20is%20optimized%0Ato%20a%20compact%20size%20of%20less%20than%201B%20parameters.%20Like%20GPT-4%2C%20our%20model%20can%20process%0Aboth%20English%20and%20Chinese.%20We%20demonstrate%20that%20this%20model%20is%20capable%20of%0Aoperating%20efficiently%20on%20a%20wide%20range%20of%20edge%20devices%2C%20including%20as%20constrained%0Aas%20a%20Raspberry%20Pi.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11459v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Octopus%20v3%3A%20Technical%20Report%20for%20On-device%20Sub-billion%20Multimodal%20AI%0A%20%20Agent&entry.906535625=Wei%20Chen%20and%20Zhiyuan%20Li&entry.1292438233=%20%20A%20multimodal%20AI%20agent%20is%20characterized%20by%20its%20ability%20to%20process%20and%20learn%0Afrom%20various%20types%20of%20data%2C%20including%20natural%20language%2C%20visual%2C%20and%20audio%0Ainputs%2C%20to%20inform%20its%20actions.%20Despite%20advancements%20in%20large%20language%20models%0Athat%20incorporate%20visual%20data%2C%20such%20as%20GPT-4V%2C%20effectively%20translating%0Aimage-based%20data%20into%20actionable%20outcomes%20for%20AI%20agents%20continues%20to%20be%0Achallenging.%20In%20this%20paper%2C%20we%20introduce%20a%20multimodal%20model%20that%20incorporates%0Athe%20concept%20of%20functional%20token%20specifically%20designed%20for%20AI%20agent%0Aapplications.%20To%20ensure%20compatibility%20with%20edge%20devices%2C%20our%20model%20is%20optimized%0Ato%20a%20compact%20size%20of%20less%20than%201B%20parameters.%20Like%20GPT-4%2C%20our%20model%20can%20process%0Aboth%20English%20and%20Chinese.%20We%20demonstrate%20that%20this%20model%20is%20capable%20of%0Aoperating%20efficiently%20on%20a%20wide%20range%20of%20edge%20devices%2C%20including%20as%20constrained%0Aas%20a%20Raspberry%20Pi.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11459v1&entry.124074799=Read"},
{"title": "Multi-resolution Rescored ByteTrack for Video Object Detection on\n  Ultra-low-power Embedded Systems", "author": "Luca Bompani and Manuele Rusci and Daniele Palossi and Francesco Conti and Luca Benini", "abstract": "  This paper introduces Multi-Resolution Rescored Byte-Track (MR2-ByteTrack), a\nnovel video object detection framework for ultra-low-power embedded processors.\nThis method reduces the average compute load of an off-the-shelf Deep Neural\nNetwork (DNN) based object detector by up to 2.25$\\times$ by alternating the\nprocessing of high-resolution images (320$\\times$320 pixels) with multiple\ndown-sized frames (192$\\times$192 pixels). To tackle the accuracy degradation\ndue to the reduced image input size, MR2-ByteTrack correlates the output\ndetections over time using the ByteTrack tracker and corrects potential\nmisclassification using a novel probabilistic Rescore algorithm. By\ninterleaving two down-sized images for every high-resolution one as the input\nof different state-of-the-art DNN object detectors with our MR2-ByteTrack, we\ndemonstrate an average accuracy increase of 2.16% and a latency reduction of\n43% on the GAP9 microcontroller compared to a baseline frame-by-frame inference\nscheme using exclusively full-resolution images. Code available at:\nhttps://github.com/Bomps4/Multi_Resolution_Rescored_ByteTrack\n", "link": "http://arxiv.org/abs/2404.11488v1", "date": "2024-04-17", "relevancy": 2.0992, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5391}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5283}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5091}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Multi-resolution%20Rescored%20ByteTrack%20for%20Video%20Object%20Detection%20on%0A%20%20Ultra-low-power%20Embedded%20Systems&body=Title%3A%20Multi-resolution%20Rescored%20ByteTrack%20for%20Video%20Object%20Detection%20on%0A%20%20Ultra-low-power%20Embedded%20Systems%0AAuthor%3A%20Luca%20Bompani%20and%20Manuele%20Rusci%20and%20Daniele%20Palossi%20and%20Francesco%20Conti%20and%20Luca%20Benini%0AAbstract%3A%20%20%20This%20paper%20introduces%20Multi-Resolution%20Rescored%20Byte-Track%20%28MR2-ByteTrack%29%2C%20a%0Anovel%20video%20object%20detection%20framework%20for%20ultra-low-power%20embedded%20processors.%0AThis%20method%20reduces%20the%20average%20compute%20load%20of%20an%20off-the-shelf%20Deep%20Neural%0ANetwork%20%28DNN%29%20based%20object%20detector%20by%20up%20to%202.25%24%5Ctimes%24%20by%20alternating%20the%0Aprocessing%20of%20high-resolution%20images%20%28320%24%5Ctimes%24320%20pixels%29%20with%20multiple%0Adown-sized%20frames%20%28192%24%5Ctimes%24192%20pixels%29.%20To%20tackle%20the%20accuracy%20degradation%0Adue%20to%20the%20reduced%20image%20input%20size%2C%20MR2-ByteTrack%20correlates%20the%20output%0Adetections%20over%20time%20using%20the%20ByteTrack%20tracker%20and%20corrects%20potential%0Amisclassification%20using%20a%20novel%20probabilistic%20Rescore%20algorithm.%20By%0Ainterleaving%20two%20down-sized%20images%20for%20every%20high-resolution%20one%20as%20the%20input%0Aof%20different%20state-of-the-art%20DNN%20object%20detectors%20with%20our%20MR2-ByteTrack%2C%20we%0Ademonstrate%20an%20average%20accuracy%20increase%20of%202.16%25%20and%20a%20latency%20reduction%20of%0A43%25%20on%20the%20GAP9%20microcontroller%20compared%20to%20a%20baseline%20frame-by-frame%20inference%0Ascheme%20using%20exclusively%20full-resolution%20images.%20Code%20available%20at%3A%0Ahttps%3A//github.com/Bomps4/Multi_Resolution_Rescored_ByteTrack%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11488v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-resolution%20Rescored%20ByteTrack%20for%20Video%20Object%20Detection%20on%0A%20%20Ultra-low-power%20Embedded%20Systems&entry.906535625=Luca%20Bompani%20and%20Manuele%20Rusci%20and%20Daniele%20Palossi%20and%20Francesco%20Conti%20and%20Luca%20Benini&entry.1292438233=%20%20This%20paper%20introduces%20Multi-Resolution%20Rescored%20Byte-Track%20%28MR2-ByteTrack%29%2C%20a%0Anovel%20video%20object%20detection%20framework%20for%20ultra-low-power%20embedded%20processors.%0AThis%20method%20reduces%20the%20average%20compute%20load%20of%20an%20off-the-shelf%20Deep%20Neural%0ANetwork%20%28DNN%29%20based%20object%20detector%20by%20up%20to%202.25%24%5Ctimes%24%20by%20alternating%20the%0Aprocessing%20of%20high-resolution%20images%20%28320%24%5Ctimes%24320%20pixels%29%20with%20multiple%0Adown-sized%20frames%20%28192%24%5Ctimes%24192%20pixels%29.%20To%20tackle%20the%20accuracy%20degradation%0Adue%20to%20the%20reduced%20image%20input%20size%2C%20MR2-ByteTrack%20correlates%20the%20output%0Adetections%20over%20time%20using%20the%20ByteTrack%20tracker%20and%20corrects%20potential%0Amisclassification%20using%20a%20novel%20probabilistic%20Rescore%20algorithm.%20By%0Ainterleaving%20two%20down-sized%20images%20for%20every%20high-resolution%20one%20as%20the%20input%0Aof%20different%20state-of-the-art%20DNN%20object%20detectors%20with%20our%20MR2-ByteTrack%2C%20we%0Ademonstrate%20an%20average%20accuracy%20increase%20of%202.16%25%20and%20a%20latency%20reduction%20of%0A43%25%20on%20the%20GAP9%20microcontroller%20compared%20to%20a%20baseline%20frame-by-frame%20inference%0Ascheme%20using%20exclusively%20full-resolution%20images.%20Code%20available%20at%3A%0Ahttps%3A//github.com/Bomps4/Multi_Resolution_Rescored_ByteTrack%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11488v1&entry.124074799=Read"},
{"title": "A2XP: Towards Private Domain Generalization", "author": "Geunhyeok Yu and Hyoseok Hwang", "abstract": "  Deep Neural Networks (DNNs) have become pivotal in various fields, especially\nin computer vision, outperforming previous methodologies. A critical challenge\nin their deployment is the bias inherent in data across different domains, such\nas image style and environmental conditions, leading to domain gaps. This\nnecessitates techniques for learning general representations from biased\ntraining data, known as domain generalization. This paper presents Attend to\neXpert Prompts (A2XP), a novel approach for domain generalization that\npreserves the privacy and integrity of the network architecture. A2XP consists\nof two phases: Expert Adaptation and Domain Generalization. In the first phase,\nprompts for each source domain are optimized to guide the model towards the\noptimal direction. In the second phase, two embedder networks are trained to\neffectively amalgamate these expert prompts, aiming for an optimal output. Our\nextensive experiments demonstrate that A2XP achieves state-of-the-art results\nover existing non-private domain generalization methods. The experimental\nresults validate that the proposed approach not only tackles the domain\ngeneralization challenge in DNNs but also offers a privacy-preserving,\nefficient solution to the broader field of computer vision.\n", "link": "http://arxiv.org/abs/2311.10339v2", "date": "2024-04-17", "relevancy": 2.0928, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5468}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5093}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5052}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A2XP%3A%20Towards%20Private%20Domain%20Generalization&body=Title%3A%20A2XP%3A%20Towards%20Private%20Domain%20Generalization%0AAuthor%3A%20Geunhyeok%20Yu%20and%20Hyoseok%20Hwang%0AAbstract%3A%20%20%20Deep%20Neural%20Networks%20%28DNNs%29%20have%20become%20pivotal%20in%20various%20fields%2C%20especially%0Ain%20computer%20vision%2C%20outperforming%20previous%20methodologies.%20A%20critical%20challenge%0Ain%20their%20deployment%20is%20the%20bias%20inherent%20in%20data%20across%20different%20domains%2C%20such%0Aas%20image%20style%20and%20environmental%20conditions%2C%20leading%20to%20domain%20gaps.%20This%0Anecessitates%20techniques%20for%20learning%20general%20representations%20from%20biased%0Atraining%20data%2C%20known%20as%20domain%20generalization.%20This%20paper%20presents%20Attend%20to%0AeXpert%20Prompts%20%28A2XP%29%2C%20a%20novel%20approach%20for%20domain%20generalization%20that%0Apreserves%20the%20privacy%20and%20integrity%20of%20the%20network%20architecture.%20A2XP%20consists%0Aof%20two%20phases%3A%20Expert%20Adaptation%20and%20Domain%20Generalization.%20In%20the%20first%20phase%2C%0Aprompts%20for%20each%20source%20domain%20are%20optimized%20to%20guide%20the%20model%20towards%20the%0Aoptimal%20direction.%20In%20the%20second%20phase%2C%20two%20embedder%20networks%20are%20trained%20to%0Aeffectively%20amalgamate%20these%20expert%20prompts%2C%20aiming%20for%20an%20optimal%20output.%20Our%0Aextensive%20experiments%20demonstrate%20that%20A2XP%20achieves%20state-of-the-art%20results%0Aover%20existing%20non-private%20domain%20generalization%20methods.%20The%20experimental%0Aresults%20validate%20that%20the%20proposed%20approach%20not%20only%20tackles%20the%20domain%0Ageneralization%20challenge%20in%20DNNs%20but%20also%20offers%20a%20privacy-preserving%2C%0Aefficient%20solution%20to%20the%20broader%20field%20of%20computer%20vision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10339v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A2XP%3A%20Towards%20Private%20Domain%20Generalization&entry.906535625=Geunhyeok%20Yu%20and%20Hyoseok%20Hwang&entry.1292438233=%20%20Deep%20Neural%20Networks%20%28DNNs%29%20have%20become%20pivotal%20in%20various%20fields%2C%20especially%0Ain%20computer%20vision%2C%20outperforming%20previous%20methodologies.%20A%20critical%20challenge%0Ain%20their%20deployment%20is%20the%20bias%20inherent%20in%20data%20across%20different%20domains%2C%20such%0Aas%20image%20style%20and%20environmental%20conditions%2C%20leading%20to%20domain%20gaps.%20This%0Anecessitates%20techniques%20for%20learning%20general%20representations%20from%20biased%0Atraining%20data%2C%20known%20as%20domain%20generalization.%20This%20paper%20presents%20Attend%20to%0AeXpert%20Prompts%20%28A2XP%29%2C%20a%20novel%20approach%20for%20domain%20generalization%20that%0Apreserves%20the%20privacy%20and%20integrity%20of%20the%20network%20architecture.%20A2XP%20consists%0Aof%20two%20phases%3A%20Expert%20Adaptation%20and%20Domain%20Generalization.%20In%20the%20first%20phase%2C%0Aprompts%20for%20each%20source%20domain%20are%20optimized%20to%20guide%20the%20model%20towards%20the%0Aoptimal%20direction.%20In%20the%20second%20phase%2C%20two%20embedder%20networks%20are%20trained%20to%0Aeffectively%20amalgamate%20these%20expert%20prompts%2C%20aiming%20for%20an%20optimal%20output.%20Our%0Aextensive%20experiments%20demonstrate%20that%20A2XP%20achieves%20state-of-the-art%20results%0Aover%20existing%20non-private%20domain%20generalization%20methods.%20The%20experimental%0Aresults%20validate%20that%20the%20proposed%20approach%20not%20only%20tackles%20the%20domain%0Ageneralization%20challenge%20in%20DNNs%20but%20also%20offers%20a%20privacy-preserving%2C%0Aefficient%20solution%20to%20the%20broader%20field%20of%20computer%20vision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10339v2&entry.124074799=Read"},
{"title": "Vision-based control for landing an aerial vehicle on a marine vessel", "author": "Haohua Dong", "abstract": "  This work addresses the landing problem of an aerial vehicle, exemplified by\na simple quadrotor, on a moving platform using image-based visual servo\ncontrol. First, the mathematical model of the quadrotor aircraft is introduced,\nfollowed by the design of the inner-loop control. At the second stage, the\nimage features on the textured target plane are exploited to derive a\nvision-based control law. The image of the spherical centroid of a set of\nlandmarks present in the landing target is used as a position measurement,\nwhereas the translational optical flow is used as velocity measurement. The\nkinematics of the vision-based system is expressed in terms of the observable\nfeatures, and the proposed control law guarantees convergence without\nestimating the unknown distance between the vision system and the target, which\nis also guaranteed to remain strictly positive, avoiding undesired collisions.\nThe performance of the proposed control law is evaluated in MATLAB and 3-D\nsimulation software Gazebo. Simulation results for a quadrotor UAV are provided\nfor different velocity profiles of the moving target, showcasing the robustness\nof the proposed controller.\n", "link": "http://arxiv.org/abs/2404.11336v1", "date": "2024-04-17", "relevancy": 2.0837, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.543}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5251}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4972}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Vision-based%20control%20for%20landing%20an%20aerial%20vehicle%20on%20a%20marine%20vessel&body=Title%3A%20Vision-based%20control%20for%20landing%20an%20aerial%20vehicle%20on%20a%20marine%20vessel%0AAuthor%3A%20Haohua%20Dong%0AAbstract%3A%20%20%20This%20work%20addresses%20the%20landing%20problem%20of%20an%20aerial%20vehicle%2C%20exemplified%20by%0Aa%20simple%20quadrotor%2C%20on%20a%20moving%20platform%20using%20image-based%20visual%20servo%0Acontrol.%20First%2C%20the%20mathematical%20model%20of%20the%20quadrotor%20aircraft%20is%20introduced%2C%0Afollowed%20by%20the%20design%20of%20the%20inner-loop%20control.%20At%20the%20second%20stage%2C%20the%0Aimage%20features%20on%20the%20textured%20target%20plane%20are%20exploited%20to%20derive%20a%0Avision-based%20control%20law.%20The%20image%20of%20the%20spherical%20centroid%20of%20a%20set%20of%0Alandmarks%20present%20in%20the%20landing%20target%20is%20used%20as%20a%20position%20measurement%2C%0Awhereas%20the%20translational%20optical%20flow%20is%20used%20as%20velocity%20measurement.%20The%0Akinematics%20of%20the%20vision-based%20system%20is%20expressed%20in%20terms%20of%20the%20observable%0Afeatures%2C%20and%20the%20proposed%20control%20law%20guarantees%20convergence%20without%0Aestimating%20the%20unknown%20distance%20between%20the%20vision%20system%20and%20the%20target%2C%20which%0Ais%20also%20guaranteed%20to%20remain%20strictly%20positive%2C%20avoiding%20undesired%20collisions.%0AThe%20performance%20of%20the%20proposed%20control%20law%20is%20evaluated%20in%20MATLAB%20and%203-D%0Asimulation%20software%20Gazebo.%20Simulation%20results%20for%20a%20quadrotor%20UAV%20are%20provided%0Afor%20different%20velocity%20profiles%20of%20the%20moving%20target%2C%20showcasing%20the%20robustness%0Aof%20the%20proposed%20controller.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11336v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-based%20control%20for%20landing%20an%20aerial%20vehicle%20on%20a%20marine%20vessel&entry.906535625=Haohua%20Dong&entry.1292438233=%20%20This%20work%20addresses%20the%20landing%20problem%20of%20an%20aerial%20vehicle%2C%20exemplified%20by%0Aa%20simple%20quadrotor%2C%20on%20a%20moving%20platform%20using%20image-based%20visual%20servo%0Acontrol.%20First%2C%20the%20mathematical%20model%20of%20the%20quadrotor%20aircraft%20is%20introduced%2C%0Afollowed%20by%20the%20design%20of%20the%20inner-loop%20control.%20At%20the%20second%20stage%2C%20the%0Aimage%20features%20on%20the%20textured%20target%20plane%20are%20exploited%20to%20derive%20a%0Avision-based%20control%20law.%20The%20image%20of%20the%20spherical%20centroid%20of%20a%20set%20of%0Alandmarks%20present%20in%20the%20landing%20target%20is%20used%20as%20a%20position%20measurement%2C%0Awhereas%20the%20translational%20optical%20flow%20is%20used%20as%20velocity%20measurement.%20The%0Akinematics%20of%20the%20vision-based%20system%20is%20expressed%20in%20terms%20of%20the%20observable%0Afeatures%2C%20and%20the%20proposed%20control%20law%20guarantees%20convergence%20without%0Aestimating%20the%20unknown%20distance%20between%20the%20vision%20system%20and%20the%20target%2C%20which%0Ais%20also%20guaranteed%20to%20remain%20strictly%20positive%2C%20avoiding%20undesired%20collisions.%0AThe%20performance%20of%20the%20proposed%20control%20law%20is%20evaluated%20in%20MATLAB%20and%203-D%0Asimulation%20software%20Gazebo.%20Simulation%20results%20for%20a%20quadrotor%20UAV%20are%20provided%0Afor%20different%20velocity%20profiles%20of%20the%20moving%20target%2C%20showcasing%20the%20robustness%0Aof%20the%20proposed%20controller.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11336v1&entry.124074799=Read"},
{"title": "Neural Shr\u00f6dinger Bridge Matching for Pansharpening", "author": "Zihan Cao and Xiao Wu and Liang-Jian Deng", "abstract": "  Recent diffusion probabilistic models (DPM) in the field of pansharpening\nhave been gradually gaining attention and have achieved state-of-the-art (SOTA)\nperformance. In this paper, we identify shortcomings in directly applying DPMs\nto the task of pansharpening as an inverse problem: 1) initiating sampling\ndirectly from Gaussian noise neglects the low-resolution multispectral image\n(LRMS) as a prior; 2) low sampling efficiency often necessitates a higher\nnumber of sampling steps. We first reformulate pansharpening into the\nstochastic differential equation (SDE) form of an inverse problem. Building\nupon this, we propose a Schr\\\"odinger bridge matching method that addresses\nboth issues.\n  We design an efficient deep neural network architecture tailored for the\nproposed SB matching.\n  In comparison to the well-established DL-regressive-based framework and the\nrecent DPM framework, our method demonstrates SOTA performance with fewer\nsampling steps. Moreover, we discuss the relationship between SB matching and\nother methods based on SDEs and ordinary differential equations (ODEs), as well\nas its connection with optimal transport.\n  Code will be available.\n", "link": "http://arxiv.org/abs/2404.11416v1", "date": "2024-04-17", "relevancy": 2.0828, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5347}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5213}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5145}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Neural%20Shr%C3%B6dinger%20Bridge%20Matching%20for%20Pansharpening&body=Title%3A%20Neural%20Shr%C3%B6dinger%20Bridge%20Matching%20for%20Pansharpening%0AAuthor%3A%20Zihan%20Cao%20and%20Xiao%20Wu%20and%20Liang-Jian%20Deng%0AAbstract%3A%20%20%20Recent%20diffusion%20probabilistic%20models%20%28DPM%29%20in%20the%20field%20of%20pansharpening%0Ahave%20been%20gradually%20gaining%20attention%20and%20have%20achieved%20state-of-the-art%20%28SOTA%29%0Aperformance.%20In%20this%20paper%2C%20we%20identify%20shortcomings%20in%20directly%20applying%20DPMs%0Ato%20the%20task%20of%20pansharpening%20as%20an%20inverse%20problem%3A%201%29%20initiating%20sampling%0Adirectly%20from%20Gaussian%20noise%20neglects%20the%20low-resolution%20multispectral%20image%0A%28LRMS%29%20as%20a%20prior%3B%202%29%20low%20sampling%20efficiency%20often%20necessitates%20a%20higher%0Anumber%20of%20sampling%20steps.%20We%20first%20reformulate%20pansharpening%20into%20the%0Astochastic%20differential%20equation%20%28SDE%29%20form%20of%20an%20inverse%20problem.%20Building%0Aupon%20this%2C%20we%20propose%20a%20Schr%5C%22odinger%20bridge%20matching%20method%20that%20addresses%0Aboth%20issues.%0A%20%20We%20design%20an%20efficient%20deep%20neural%20network%20architecture%20tailored%20for%20the%0Aproposed%20SB%20matching.%0A%20%20In%20comparison%20to%20the%20well-established%20DL-regressive-based%20framework%20and%20the%0Arecent%20DPM%20framework%2C%20our%20method%20demonstrates%20SOTA%20performance%20with%20fewer%0Asampling%20steps.%20Moreover%2C%20we%20discuss%20the%20relationship%20between%20SB%20matching%20and%0Aother%20methods%20based%20on%20SDEs%20and%20ordinary%20differential%20equations%20%28ODEs%29%2C%20as%20well%0Aas%20its%20connection%20with%20optimal%20transport.%0A%20%20Code%20will%20be%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11416v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Shr%C3%B6dinger%20Bridge%20Matching%20for%20Pansharpening&entry.906535625=Zihan%20Cao%20and%20Xiao%20Wu%20and%20Liang-Jian%20Deng&entry.1292438233=%20%20Recent%20diffusion%20probabilistic%20models%20%28DPM%29%20in%20the%20field%20of%20pansharpening%0Ahave%20been%20gradually%20gaining%20attention%20and%20have%20achieved%20state-of-the-art%20%28SOTA%29%0Aperformance.%20In%20this%20paper%2C%20we%20identify%20shortcomings%20in%20directly%20applying%20DPMs%0Ato%20the%20task%20of%20pansharpening%20as%20an%20inverse%20problem%3A%201%29%20initiating%20sampling%0Adirectly%20from%20Gaussian%20noise%20neglects%20the%20low-resolution%20multispectral%20image%0A%28LRMS%29%20as%20a%20prior%3B%202%29%20low%20sampling%20efficiency%20often%20necessitates%20a%20higher%0Anumber%20of%20sampling%20steps.%20We%20first%20reformulate%20pansharpening%20into%20the%0Astochastic%20differential%20equation%20%28SDE%29%20form%20of%20an%20inverse%20problem.%20Building%0Aupon%20this%2C%20we%20propose%20a%20Schr%5C%22odinger%20bridge%20matching%20method%20that%20addresses%0Aboth%20issues.%0A%20%20We%20design%20an%20efficient%20deep%20neural%20network%20architecture%20tailored%20for%20the%0Aproposed%20SB%20matching.%0A%20%20In%20comparison%20to%20the%20well-established%20DL-regressive-based%20framework%20and%20the%0Arecent%20DPM%20framework%2C%20our%20method%20demonstrates%20SOTA%20performance%20with%20fewer%0Asampling%20steps.%20Moreover%2C%20we%20discuss%20the%20relationship%20between%20SB%20matching%20and%0Aother%20methods%20based%20on%20SDEs%20and%20ordinary%20differential%20equations%20%28ODEs%29%2C%20as%20well%0Aas%20its%20connection%20with%20optimal%20transport.%0A%20%20Code%20will%20be%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11416v1&entry.124074799=Read"},
{"title": "ControlMTR: Control-Guided Motion Transformer with Scene-Compliant\n  Intention Points for Feasible Motion Prediction", "author": "Jiawei Sun and Chengran Yuan and Shuo Sun and Shanze Wang and Yuhang Han and Shuailei Ma and Zefan Huang and Anthony Wong and Keng Peng Tee and Marcelo H. Ang Jr", "abstract": "  The ability to accurately predict feasible multimodal future trajectories of\nsurrounding traffic participants is crucial for behavior planning in autonomous\nvehicles. The Motion Transformer (MTR), a state-of-the-art motion prediction\nmethod, alleviated mode collapse and instability during training and enhanced\noverall prediction performance by replacing conventional dense future endpoints\nwith a small set of fixed prior motion intention points. However, the fixed\nprior intention points make the MTR multi-modal prediction distribution\nover-scattered and infeasible in many scenarios. In this paper, we propose the\nControlMTR framework to tackle the aforementioned issues by generating\nscene-compliant intention points and additionally predicting driving control\ncommands, which are then converted into trajectories by a simple kinematic\nmodel with soft constraints. These control-generated trajectories will guide\nthe directly predicted trajectories by an auxiliary loss function. Together\nwith our proposed scene-compliant intention points, they can effectively\nrestrict the prediction distribution within the road boundaries and suppress\ninfeasible off-road predictions while enhancing prediction performance.\nRemarkably, without resorting to additional model ensemble techniques, our\nmethod surpasses the baseline MTR model across all performance metrics,\nachieving notable improvements of 5.22% in SoftmAP and a 4.15% reduction in\nMissRate. Our approach notably results in a 41.85% reduction in the\ncross-boundary rate of the MTR, effectively ensuring that the prediction\ndistribution is confined within the drivable area.\n", "link": "http://arxiv.org/abs/2404.10295v2", "date": "2024-04-17", "relevancy": 2.0772, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5946}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5079}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5006}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ControlMTR%3A%20Control-Guided%20Motion%20Transformer%20with%20Scene-Compliant%0A%20%20Intention%20Points%20for%20Feasible%20Motion%20Prediction&body=Title%3A%20ControlMTR%3A%20Control-Guided%20Motion%20Transformer%20with%20Scene-Compliant%0A%20%20Intention%20Points%20for%20Feasible%20Motion%20Prediction%0AAuthor%3A%20Jiawei%20Sun%20and%20Chengran%20Yuan%20and%20Shuo%20Sun%20and%20Shanze%20Wang%20and%20Yuhang%20Han%20and%20Shuailei%20Ma%20and%20Zefan%20Huang%20and%20Anthony%20Wong%20and%20Keng%20Peng%20Tee%20and%20Marcelo%20H.%20Ang%20Jr%0AAbstract%3A%20%20%20The%20ability%20to%20accurately%20predict%20feasible%20multimodal%20future%20trajectories%20of%0Asurrounding%20traffic%20participants%20is%20crucial%20for%20behavior%20planning%20in%20autonomous%0Avehicles.%20The%20Motion%20Transformer%20%28MTR%29%2C%20a%20state-of-the-art%20motion%20prediction%0Amethod%2C%20alleviated%20mode%20collapse%20and%20instability%20during%20training%20and%20enhanced%0Aoverall%20prediction%20performance%20by%20replacing%20conventional%20dense%20future%20endpoints%0Awith%20a%20small%20set%20of%20fixed%20prior%20motion%20intention%20points.%20However%2C%20the%20fixed%0Aprior%20intention%20points%20make%20the%20MTR%20multi-modal%20prediction%20distribution%0Aover-scattered%20and%20infeasible%20in%20many%20scenarios.%20In%20this%20paper%2C%20we%20propose%20the%0AControlMTR%20framework%20to%20tackle%20the%20aforementioned%20issues%20by%20generating%0Ascene-compliant%20intention%20points%20and%20additionally%20predicting%20driving%20control%0Acommands%2C%20which%20are%20then%20converted%20into%20trajectories%20by%20a%20simple%20kinematic%0Amodel%20with%20soft%20constraints.%20These%20control-generated%20trajectories%20will%20guide%0Athe%20directly%20predicted%20trajectories%20by%20an%20auxiliary%20loss%20function.%20Together%0Awith%20our%20proposed%20scene-compliant%20intention%20points%2C%20they%20can%20effectively%0Arestrict%20the%20prediction%20distribution%20within%20the%20road%20boundaries%20and%20suppress%0Ainfeasible%20off-road%20predictions%20while%20enhancing%20prediction%20performance.%0ARemarkably%2C%20without%20resorting%20to%20additional%20model%20ensemble%20techniques%2C%20our%0Amethod%20surpasses%20the%20baseline%20MTR%20model%20across%20all%20performance%20metrics%2C%0Aachieving%20notable%20improvements%20of%205.22%25%20in%20SoftmAP%20and%20a%204.15%25%20reduction%20in%0AMissRate.%20Our%20approach%20notably%20results%20in%20a%2041.85%25%20reduction%20in%20the%0Across-boundary%20rate%20of%20the%20MTR%2C%20effectively%20ensuring%20that%20the%20prediction%0Adistribution%20is%20confined%20within%20the%20drivable%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10295v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ControlMTR%3A%20Control-Guided%20Motion%20Transformer%20with%20Scene-Compliant%0A%20%20Intention%20Points%20for%20Feasible%20Motion%20Prediction&entry.906535625=Jiawei%20Sun%20and%20Chengran%20Yuan%20and%20Shuo%20Sun%20and%20Shanze%20Wang%20and%20Yuhang%20Han%20and%20Shuailei%20Ma%20and%20Zefan%20Huang%20and%20Anthony%20Wong%20and%20Keng%20Peng%20Tee%20and%20Marcelo%20H.%20Ang%20Jr&entry.1292438233=%20%20The%20ability%20to%20accurately%20predict%20feasible%20multimodal%20future%20trajectories%20of%0Asurrounding%20traffic%20participants%20is%20crucial%20for%20behavior%20planning%20in%20autonomous%0Avehicles.%20The%20Motion%20Transformer%20%28MTR%29%2C%20a%20state-of-the-art%20motion%20prediction%0Amethod%2C%20alleviated%20mode%20collapse%20and%20instability%20during%20training%20and%20enhanced%0Aoverall%20prediction%20performance%20by%20replacing%20conventional%20dense%20future%20endpoints%0Awith%20a%20small%20set%20of%20fixed%20prior%20motion%20intention%20points.%20However%2C%20the%20fixed%0Aprior%20intention%20points%20make%20the%20MTR%20multi-modal%20prediction%20distribution%0Aover-scattered%20and%20infeasible%20in%20many%20scenarios.%20In%20this%20paper%2C%20we%20propose%20the%0AControlMTR%20framework%20to%20tackle%20the%20aforementioned%20issues%20by%20generating%0Ascene-compliant%20intention%20points%20and%20additionally%20predicting%20driving%20control%0Acommands%2C%20which%20are%20then%20converted%20into%20trajectories%20by%20a%20simple%20kinematic%0Amodel%20with%20soft%20constraints.%20These%20control-generated%20trajectories%20will%20guide%0Athe%20directly%20predicted%20trajectories%20by%20an%20auxiliary%20loss%20function.%20Together%0Awith%20our%20proposed%20scene-compliant%20intention%20points%2C%20they%20can%20effectively%0Arestrict%20the%20prediction%20distribution%20within%20the%20road%20boundaries%20and%20suppress%0Ainfeasible%20off-road%20predictions%20while%20enhancing%20prediction%20performance.%0ARemarkably%2C%20without%20resorting%20to%20additional%20model%20ensemble%20techniques%2C%20our%0Amethod%20surpasses%20the%20baseline%20MTR%20model%20across%20all%20performance%20metrics%2C%0Aachieving%20notable%20improvements%20of%205.22%25%20in%20SoftmAP%20and%20a%204.15%25%20reduction%20in%0AMissRate.%20Our%20approach%20notably%20results%20in%20a%2041.85%25%20reduction%20in%20the%0Across-boundary%20rate%20of%20the%20MTR%2C%20effectively%20ensuring%20that%20the%20prediction%0Adistribution%20is%20confined%20within%20the%20drivable%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10295v2&entry.124074799=Read"},
{"title": "RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled\n  Neural Rendering", "author": "Xianqiang Lyu and Hui Liu and Junhui Hou", "abstract": "  We propose RainyScape, an unsupervised framework for reconstructing clean\nscenes from a collection of multi-view rainy images. RainyScape consists of two\nmain modules: a neural rendering module and a rain-prediction module that\nincorporates a predictor network and a learnable latent embedding that captures\nthe rain characteristics of the scene. Specifically, based on the spectral bias\nproperty of neural networks, we first optimize the neural rendering pipeline to\nobtain a low-frequency scene representation. Subsequently, we jointly optimize\nthe two modules, driven by the proposed adaptive direction-sensitive\ngradient-based reconstruction loss, which encourages the network to distinguish\nbetween scene details and rain streaks, facilitating the propagation of\ngradients to the relevant components. Extensive experiments on both the classic\nneural radiance field and the recently proposed 3D Gaussian splatting\ndemonstrate the superiority of our method in effectively eliminating rain\nstreaks and rendering clean images, achieving state-of-the-art performance. The\nconstructed high-quality dataset and source code will be publicly available.\n", "link": "http://arxiv.org/abs/2404.11401v1", "date": "2024-04-17", "relevancy": 2.0769, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5284}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5207}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5141}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RainyScape%3A%20Unsupervised%20Rainy%20Scene%20Reconstruction%20using%20Decoupled%0A%20%20Neural%20Rendering&body=Title%3A%20RainyScape%3A%20Unsupervised%20Rainy%20Scene%20Reconstruction%20using%20Decoupled%0A%20%20Neural%20Rendering%0AAuthor%3A%20Xianqiang%20Lyu%20and%20Hui%20Liu%20and%20Junhui%20Hou%0AAbstract%3A%20%20%20We%20propose%20RainyScape%2C%20an%20unsupervised%20framework%20for%20reconstructing%20clean%0Ascenes%20from%20a%20collection%20of%20multi-view%20rainy%20images.%20RainyScape%20consists%20of%20two%0Amain%20modules%3A%20a%20neural%20rendering%20module%20and%20a%20rain-prediction%20module%20that%0Aincorporates%20a%20predictor%20network%20and%20a%20learnable%20latent%20embedding%20that%20captures%0Athe%20rain%20characteristics%20of%20the%20scene.%20Specifically%2C%20based%20on%20the%20spectral%20bias%0Aproperty%20of%20neural%20networks%2C%20we%20first%20optimize%20the%20neural%20rendering%20pipeline%20to%0Aobtain%20a%20low-frequency%20scene%20representation.%20Subsequently%2C%20we%20jointly%20optimize%0Athe%20two%20modules%2C%20driven%20by%20the%20proposed%20adaptive%20direction-sensitive%0Agradient-based%20reconstruction%20loss%2C%20which%20encourages%20the%20network%20to%20distinguish%0Abetween%20scene%20details%20and%20rain%20streaks%2C%20facilitating%20the%20propagation%20of%0Agradients%20to%20the%20relevant%20components.%20Extensive%20experiments%20on%20both%20the%20classic%0Aneural%20radiance%20field%20and%20the%20recently%20proposed%203D%20Gaussian%20splatting%0Ademonstrate%20the%20superiority%20of%20our%20method%20in%20effectively%20eliminating%20rain%0Astreaks%20and%20rendering%20clean%20images%2C%20achieving%20state-of-the-art%20performance.%20The%0Aconstructed%20high-quality%20dataset%20and%20source%20code%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11401v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RainyScape%3A%20Unsupervised%20Rainy%20Scene%20Reconstruction%20using%20Decoupled%0A%20%20Neural%20Rendering&entry.906535625=Xianqiang%20Lyu%20and%20Hui%20Liu%20and%20Junhui%20Hou&entry.1292438233=%20%20We%20propose%20RainyScape%2C%20an%20unsupervised%20framework%20for%20reconstructing%20clean%0Ascenes%20from%20a%20collection%20of%20multi-view%20rainy%20images.%20RainyScape%20consists%20of%20two%0Amain%20modules%3A%20a%20neural%20rendering%20module%20and%20a%20rain-prediction%20module%20that%0Aincorporates%20a%20predictor%20network%20and%20a%20learnable%20latent%20embedding%20that%20captures%0Athe%20rain%20characteristics%20of%20the%20scene.%20Specifically%2C%20based%20on%20the%20spectral%20bias%0Aproperty%20of%20neural%20networks%2C%20we%20first%20optimize%20the%20neural%20rendering%20pipeline%20to%0Aobtain%20a%20low-frequency%20scene%20representation.%20Subsequently%2C%20we%20jointly%20optimize%0Athe%20two%20modules%2C%20driven%20by%20the%20proposed%20adaptive%20direction-sensitive%0Agradient-based%20reconstruction%20loss%2C%20which%20encourages%20the%20network%20to%20distinguish%0Abetween%20scene%20details%20and%20rain%20streaks%2C%20facilitating%20the%20propagation%20of%0Agradients%20to%20the%20relevant%20components.%20Extensive%20experiments%20on%20both%20the%20classic%0Aneural%20radiance%20field%20and%20the%20recently%20proposed%203D%20Gaussian%20splatting%0Ademonstrate%20the%20superiority%20of%20our%20method%20in%20effectively%20eliminating%20rain%0Astreaks%20and%20rendering%20clean%20images%2C%20achieving%20state-of-the-art%20performance.%20The%0Aconstructed%20high-quality%20dataset%20and%20source%20code%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11401v1&entry.124074799=Read"},
{"title": "Boosting Medical Image Segmentation Performance with Adaptive\n  Convolution Layer", "author": "Seyed M. R. Modaresi and Aomar Osmani and Mohammadreza Razzazi and Abdelghani Chibani", "abstract": "  Medical image segmentation plays a vital role in various clinical\napplications, enabling accurate delineation and analysis of anatomical\nstructures or pathological regions. Traditional CNNs have achieved remarkable\nsuccess in this field. However, they often rely on fixed kernel sizes, which\ncan limit their performance and adaptability in medical images where features\nexhibit diverse scales and configurations due to variability in equipment,\ntarget sizes, and expert interpretations.\n  In this paper, we propose an adaptive layer placed ahead of leading\ndeep-learning models such as UCTransNet, which dynamically adjusts the kernel\nsize based on the local context of the input image.\n  By adaptively capturing and fusing features at multiple scales, our approach\nenhances the network's ability to handle diverse anatomical structures and\nsubtle image details, even for recently performing architectures that\ninternally implement intra-scale modules, such as UCTransnet.\n  Extensive experiments are conducted on\n  benchmark medical image datasets to evaluate the effectiveness of our\nproposal. It consistently outperforms traditional \\glspl{CNN} with fixed kernel\nsizes with a similar number of parameters, achieving superior segmentation\nAccuracy, Dice, and IoU in popular datasets such as SegPC2021 and ISIC2018. The\nmodel and data are published in the open-source repository, ensuring\ntransparency and reproducibility of our promising results.\n", "link": "http://arxiv.org/abs/2404.11361v1", "date": "2024-04-17", "relevancy": 2.069, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5259}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5114}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.511}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Boosting%20Medical%20Image%20Segmentation%20Performance%20with%20Adaptive%0A%20%20Convolution%20Layer&body=Title%3A%20Boosting%20Medical%20Image%20Segmentation%20Performance%20with%20Adaptive%0A%20%20Convolution%20Layer%0AAuthor%3A%20Seyed%20M.%20R.%20Modaresi%20and%20Aomar%20Osmani%20and%20Mohammadreza%20Razzazi%20and%20Abdelghani%20Chibani%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20plays%20a%20vital%20role%20in%20various%20clinical%0Aapplications%2C%20enabling%20accurate%20delineation%20and%20analysis%20of%20anatomical%0Astructures%20or%20pathological%20regions.%20Traditional%20CNNs%20have%20achieved%20remarkable%0Asuccess%20in%20this%20field.%20However%2C%20they%20often%20rely%20on%20fixed%20kernel%20sizes%2C%20which%0Acan%20limit%20their%20performance%20and%20adaptability%20in%20medical%20images%20where%20features%0Aexhibit%20diverse%20scales%20and%20configurations%20due%20to%20variability%20in%20equipment%2C%0Atarget%20sizes%2C%20and%20expert%20interpretations.%0A%20%20In%20this%20paper%2C%20we%20propose%20an%20adaptive%20layer%20placed%20ahead%20of%20leading%0Adeep-learning%20models%20such%20as%20UCTransNet%2C%20which%20dynamically%20adjusts%20the%20kernel%0Asize%20based%20on%20the%20local%20context%20of%20the%20input%20image.%0A%20%20By%20adaptively%20capturing%20and%20fusing%20features%20at%20multiple%20scales%2C%20our%20approach%0Aenhances%20the%20network%27s%20ability%20to%20handle%20diverse%20anatomical%20structures%20and%0Asubtle%20image%20details%2C%20even%20for%20recently%20performing%20architectures%20that%0Ainternally%20implement%20intra-scale%20modules%2C%20such%20as%20UCTransnet.%0A%20%20Extensive%20experiments%20are%20conducted%20on%0A%20%20benchmark%20medical%20image%20datasets%20to%20evaluate%20the%20effectiveness%20of%20our%0Aproposal.%20It%20consistently%20outperforms%20traditional%20%5Cglspl%7BCNN%7D%20with%20fixed%20kernel%0Asizes%20with%20a%20similar%20number%20of%20parameters%2C%20achieving%20superior%20segmentation%0AAccuracy%2C%20Dice%2C%20and%20IoU%20in%20popular%20datasets%20such%20as%20SegPC2021%20and%20ISIC2018.%20The%0Amodel%20and%20data%20are%20published%20in%20the%20open-source%20repository%2C%20ensuring%0Atransparency%20and%20reproducibility%20of%20our%20promising%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11361v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Medical%20Image%20Segmentation%20Performance%20with%20Adaptive%0A%20%20Convolution%20Layer&entry.906535625=Seyed%20M.%20R.%20Modaresi%20and%20Aomar%20Osmani%20and%20Mohammadreza%20Razzazi%20and%20Abdelghani%20Chibani&entry.1292438233=%20%20Medical%20image%20segmentation%20plays%20a%20vital%20role%20in%20various%20clinical%0Aapplications%2C%20enabling%20accurate%20delineation%20and%20analysis%20of%20anatomical%0Astructures%20or%20pathological%20regions.%20Traditional%20CNNs%20have%20achieved%20remarkable%0Asuccess%20in%20this%20field.%20However%2C%20they%20often%20rely%20on%20fixed%20kernel%20sizes%2C%20which%0Acan%20limit%20their%20performance%20and%20adaptability%20in%20medical%20images%20where%20features%0Aexhibit%20diverse%20scales%20and%20configurations%20due%20to%20variability%20in%20equipment%2C%0Atarget%20sizes%2C%20and%20expert%20interpretations.%0A%20%20In%20this%20paper%2C%20we%20propose%20an%20adaptive%20layer%20placed%20ahead%20of%20leading%0Adeep-learning%20models%20such%20as%20UCTransNet%2C%20which%20dynamically%20adjusts%20the%20kernel%0Asize%20based%20on%20the%20local%20context%20of%20the%20input%20image.%0A%20%20By%20adaptively%20capturing%20and%20fusing%20features%20at%20multiple%20scales%2C%20our%20approach%0Aenhances%20the%20network%27s%20ability%20to%20handle%20diverse%20anatomical%20structures%20and%0Asubtle%20image%20details%2C%20even%20for%20recently%20performing%20architectures%20that%0Ainternally%20implement%20intra-scale%20modules%2C%20such%20as%20UCTransnet.%0A%20%20Extensive%20experiments%20are%20conducted%20on%0A%20%20benchmark%20medical%20image%20datasets%20to%20evaluate%20the%20effectiveness%20of%20our%0Aproposal.%20It%20consistently%20outperforms%20traditional%20%5Cglspl%7BCNN%7D%20with%20fixed%20kernel%0Asizes%20with%20a%20similar%20number%20of%20parameters%2C%20achieving%20superior%20segmentation%0AAccuracy%2C%20Dice%2C%20and%20IoU%20in%20popular%20datasets%20such%20as%20SegPC2021%20and%20ISIC2018.%20The%0Amodel%20and%20data%20are%20published%20in%20the%20open-source%20repository%2C%20ensuring%0Atransparency%20and%20reproducibility%20of%20our%20promising%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11361v1&entry.124074799=Read"},
{"title": "Mastering Diverse Domains through World Models", "author": "Danijar Hafner and Jurgis Pasukonis and Jimmy Ba and Timothy Lillicrap", "abstract": "  Developing a general algorithm that learns to solve tasks across a wide range\nof applications has been a fundamental challenge in artificial intelligence.\nAlthough current reinforcement learning algorithms can be readily applied to\ntasks similar to what they have been developed for, configuring them for new\napplication domains requires significant human expertise and experimentation.\nWe present DreamerV3, a general algorithm that outperforms specialized methods\nacross over 150 diverse tasks, with a single configuration. Dreamer learns a\nmodel of the environment and improves its behavior by imagining future\nscenarios. Robustness techniques based on normalization, balancing, and\ntransformations enable stable learning across domains. Applied out of the box,\nDreamer is the first algorithm to collect diamonds in Minecraft from scratch\nwithout human data or curricula. This achievement has been posed as a\nsignificant challenge in artificial intelligence that requires exploring\nfarsighted strategies from pixels and sparse rewards in an open world. Our work\nallows solving challenging control problems without extensive experimentation,\nmaking reinforcement learning broadly applicable.\n", "link": "http://arxiv.org/abs/2301.04104v2", "date": "2024-04-17", "relevancy": 2.0668, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5403}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5226}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4907}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Mastering%20Diverse%20Domains%20through%20World%20Models&body=Title%3A%20Mastering%20Diverse%20Domains%20through%20World%20Models%0AAuthor%3A%20Danijar%20Hafner%20and%20Jurgis%20Pasukonis%20and%20Jimmy%20Ba%20and%20Timothy%20Lillicrap%0AAbstract%3A%20%20%20Developing%20a%20general%20algorithm%20that%20learns%20to%20solve%20tasks%20across%20a%20wide%20range%0Aof%20applications%20has%20been%20a%20fundamental%20challenge%20in%20artificial%20intelligence.%0AAlthough%20current%20reinforcement%20learning%20algorithms%20can%20be%20readily%20applied%20to%0Atasks%20similar%20to%20what%20they%20have%20been%20developed%20for%2C%20configuring%20them%20for%20new%0Aapplication%20domains%20requires%20significant%20human%20expertise%20and%20experimentation.%0AWe%20present%20DreamerV3%2C%20a%20general%20algorithm%20that%20outperforms%20specialized%20methods%0Aacross%20over%20150%20diverse%20tasks%2C%20with%20a%20single%20configuration.%20Dreamer%20learns%20a%0Amodel%20of%20the%20environment%20and%20improves%20its%20behavior%20by%20imagining%20future%0Ascenarios.%20Robustness%20techniques%20based%20on%20normalization%2C%20balancing%2C%20and%0Atransformations%20enable%20stable%20learning%20across%20domains.%20Applied%20out%20of%20the%20box%2C%0ADreamer%20is%20the%20first%20algorithm%20to%20collect%20diamonds%20in%20Minecraft%20from%20scratch%0Awithout%20human%20data%20or%20curricula.%20This%20achievement%20has%20been%20posed%20as%20a%0Asignificant%20challenge%20in%20artificial%20intelligence%20that%20requires%20exploring%0Afarsighted%20strategies%20from%20pixels%20and%20sparse%20rewards%20in%20an%20open%20world.%20Our%20work%0Aallows%20solving%20challenging%20control%20problems%20without%20extensive%20experimentation%2C%0Amaking%20reinforcement%20learning%20broadly%20applicable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.04104v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mastering%20Diverse%20Domains%20through%20World%20Models&entry.906535625=Danijar%20Hafner%20and%20Jurgis%20Pasukonis%20and%20Jimmy%20Ba%20and%20Timothy%20Lillicrap&entry.1292438233=%20%20Developing%20a%20general%20algorithm%20that%20learns%20to%20solve%20tasks%20across%20a%20wide%20range%0Aof%20applications%20has%20been%20a%20fundamental%20challenge%20in%20artificial%20intelligence.%0AAlthough%20current%20reinforcement%20learning%20algorithms%20can%20be%20readily%20applied%20to%0Atasks%20similar%20to%20what%20they%20have%20been%20developed%20for%2C%20configuring%20them%20for%20new%0Aapplication%20domains%20requires%20significant%20human%20expertise%20and%20experimentation.%0AWe%20present%20DreamerV3%2C%20a%20general%20algorithm%20that%20outperforms%20specialized%20methods%0Aacross%20over%20150%20diverse%20tasks%2C%20with%20a%20single%20configuration.%20Dreamer%20learns%20a%0Amodel%20of%20the%20environment%20and%20improves%20its%20behavior%20by%20imagining%20future%0Ascenarios.%20Robustness%20techniques%20based%20on%20normalization%2C%20balancing%2C%20and%0Atransformations%20enable%20stable%20learning%20across%20domains.%20Applied%20out%20of%20the%20box%2C%0ADreamer%20is%20the%20first%20algorithm%20to%20collect%20diamonds%20in%20Minecraft%20from%20scratch%0Awithout%20human%20data%20or%20curricula.%20This%20achievement%20has%20been%20posed%20as%20a%0Asignificant%20challenge%20in%20artificial%20intelligence%20that%20requires%20exploring%0Afarsighted%20strategies%20from%20pixels%20and%20sparse%20rewards%20in%20an%20open%20world.%20Our%20work%0Aallows%20solving%20challenging%20control%20problems%20without%20extensive%20experimentation%2C%0Amaking%20reinforcement%20learning%20broadly%20applicable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.04104v2&entry.124074799=Read"},
{"title": "TCJA-SNN: Temporal-Channel Joint Attention for Spiking Neural Networks", "author": "Rui-Jie Zhu and Malu Zhang and Qihang Zhao and Haoyu Deng and Yule Duan and Liang-Jian Deng", "abstract": "  Spiking Neural Networks (SNNs) are attracting widespread interest due to\ntheir biological plausibility, energy efficiency, and powerful spatio-temporal\ninformation representation ability. Given the critical role of attention\nmechanisms in enhancing neural network performance, the integration of SNNs and\nattention mechanisms exhibits potential to deliver energy-efficient and\nhigh-performance computing paradigms. We present a novel Temporal-Channel Joint\nAttention mechanism for SNNs, referred to as TCJA-SNN. The proposed TCJA-SNN\nframework can effectively assess the significance of spike sequence from both\nspatial and temporal dimensions. More specifically, our essential technical\ncontribution lies on: 1) We employ the squeeze operation to compress the spike\nstream into an average matrix. Then, we leverage two local attention mechanisms\nbased on efficient 1D convolutions to facilitate comprehensive feature\nextraction at the temporal and channel levels independently. 2) We introduce\nthe Cross Convolutional Fusion (CCF) layer as a novel approach to model the\ninter-dependencies between the temporal and channel scopes. This layer breaks\nthe independence of these two dimensions and enables the interaction between\nfeatures. Experimental results demonstrate that the proposed TCJA-SNN\noutperforms SOTA by up to 15.7% accuracy on standard static and neuromorphic\ndatasets, including Fashion-MNIST, CIFAR10-DVS, N-Caltech 101, and DVS128\nGesture. Furthermore, we apply the TCJA-SNN framework to image generation tasks\nby leveraging a variation autoencoder. To the best of our knowledge, this study\nis the first instance where the SNN-attention mechanism has been employed for\nimage classification and generation tasks. Notably, our approach has achieved\nSOTA performance in both domains, establishing a significant advancement in the\nfield. Codes are available at https://github.com/ridgerchu/TCJA.\n", "link": "http://arxiv.org/abs/2206.10177v3", "date": "2024-04-17", "relevancy": 2.0635, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5468}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4939}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4937}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TCJA-SNN%3A%20Temporal-Channel%20Joint%20Attention%20for%20Spiking%20Neural%20Networks&body=Title%3A%20TCJA-SNN%3A%20Temporal-Channel%20Joint%20Attention%20for%20Spiking%20Neural%20Networks%0AAuthor%3A%20Rui-Jie%20Zhu%20and%20Malu%20Zhang%20and%20Qihang%20Zhao%20and%20Haoyu%20Deng%20and%20Yule%20Duan%20and%20Liang-Jian%20Deng%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20attracting%20widespread%20interest%20due%20to%0Atheir%20biological%20plausibility%2C%20energy%20efficiency%2C%20and%20powerful%20spatio-temporal%0Ainformation%20representation%20ability.%20Given%20the%20critical%20role%20of%20attention%0Amechanisms%20in%20enhancing%20neural%20network%20performance%2C%20the%20integration%20of%20SNNs%20and%0Aattention%20mechanisms%20exhibits%20potential%20to%20deliver%20energy-efficient%20and%0Ahigh-performance%20computing%20paradigms.%20We%20present%20a%20novel%20Temporal-Channel%20Joint%0AAttention%20mechanism%20for%20SNNs%2C%20referred%20to%20as%20TCJA-SNN.%20The%20proposed%20TCJA-SNN%0Aframework%20can%20effectively%20assess%20the%20significance%20of%20spike%20sequence%20from%20both%0Aspatial%20and%20temporal%20dimensions.%20More%20specifically%2C%20our%20essential%20technical%0Acontribution%20lies%20on%3A%201%29%20We%20employ%20the%20squeeze%20operation%20to%20compress%20the%20spike%0Astream%20into%20an%20average%20matrix.%20Then%2C%20we%20leverage%20two%20local%20attention%20mechanisms%0Abased%20on%20efficient%201D%20convolutions%20to%20facilitate%20comprehensive%20feature%0Aextraction%20at%20the%20temporal%20and%20channel%20levels%20independently.%202%29%20We%20introduce%0Athe%20Cross%20Convolutional%20Fusion%20%28CCF%29%20layer%20as%20a%20novel%20approach%20to%20model%20the%0Ainter-dependencies%20between%20the%20temporal%20and%20channel%20scopes.%20This%20layer%20breaks%0Athe%20independence%20of%20these%20two%20dimensions%20and%20enables%20the%20interaction%20between%0Afeatures.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20TCJA-SNN%0Aoutperforms%20SOTA%20by%20up%20to%2015.7%25%20accuracy%20on%20standard%20static%20and%20neuromorphic%0Adatasets%2C%20including%20Fashion-MNIST%2C%20CIFAR10-DVS%2C%20N-Caltech%20101%2C%20and%20DVS128%0AGesture.%20Furthermore%2C%20we%20apply%20the%20TCJA-SNN%20framework%20to%20image%20generation%20tasks%0Aby%20leveraging%20a%20variation%20autoencoder.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20study%0Ais%20the%20first%20instance%20where%20the%20SNN-attention%20mechanism%20has%20been%20employed%20for%0Aimage%20classification%20and%20generation%20tasks.%20Notably%2C%20our%20approach%20has%20achieved%0ASOTA%20performance%20in%20both%20domains%2C%20establishing%20a%20significant%20advancement%20in%20the%0Afield.%20Codes%20are%20available%20at%20https%3A//github.com/ridgerchu/TCJA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2206.10177v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TCJA-SNN%3A%20Temporal-Channel%20Joint%20Attention%20for%20Spiking%20Neural%20Networks&entry.906535625=Rui-Jie%20Zhu%20and%20Malu%20Zhang%20and%20Qihang%20Zhao%20and%20Haoyu%20Deng%20and%20Yule%20Duan%20and%20Liang-Jian%20Deng&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20attracting%20widespread%20interest%20due%20to%0Atheir%20biological%20plausibility%2C%20energy%20efficiency%2C%20and%20powerful%20spatio-temporal%0Ainformation%20representation%20ability.%20Given%20the%20critical%20role%20of%20attention%0Amechanisms%20in%20enhancing%20neural%20network%20performance%2C%20the%20integration%20of%20SNNs%20and%0Aattention%20mechanisms%20exhibits%20potential%20to%20deliver%20energy-efficient%20and%0Ahigh-performance%20computing%20paradigms.%20We%20present%20a%20novel%20Temporal-Channel%20Joint%0AAttention%20mechanism%20for%20SNNs%2C%20referred%20to%20as%20TCJA-SNN.%20The%20proposed%20TCJA-SNN%0Aframework%20can%20effectively%20assess%20the%20significance%20of%20spike%20sequence%20from%20both%0Aspatial%20and%20temporal%20dimensions.%20More%20specifically%2C%20our%20essential%20technical%0Acontribution%20lies%20on%3A%201%29%20We%20employ%20the%20squeeze%20operation%20to%20compress%20the%20spike%0Astream%20into%20an%20average%20matrix.%20Then%2C%20we%20leverage%20two%20local%20attention%20mechanisms%0Abased%20on%20efficient%201D%20convolutions%20to%20facilitate%20comprehensive%20feature%0Aextraction%20at%20the%20temporal%20and%20channel%20levels%20independently.%202%29%20We%20introduce%0Athe%20Cross%20Convolutional%20Fusion%20%28CCF%29%20layer%20as%20a%20novel%20approach%20to%20model%20the%0Ainter-dependencies%20between%20the%20temporal%20and%20channel%20scopes.%20This%20layer%20breaks%0Athe%20independence%20of%20these%20two%20dimensions%20and%20enables%20the%20interaction%20between%0Afeatures.%20Experimental%20results%20demonstrate%20that%20the%20proposed%20TCJA-SNN%0Aoutperforms%20SOTA%20by%20up%20to%2015.7%25%20accuracy%20on%20standard%20static%20and%20neuromorphic%0Adatasets%2C%20including%20Fashion-MNIST%2C%20CIFAR10-DVS%2C%20N-Caltech%20101%2C%20and%20DVS128%0AGesture.%20Furthermore%2C%20we%20apply%20the%20TCJA-SNN%20framework%20to%20image%20generation%20tasks%0Aby%20leveraging%20a%20variation%20autoencoder.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20study%0Ais%20the%20first%20instance%20where%20the%20SNN-attention%20mechanism%20has%20been%20employed%20for%0Aimage%20classification%20and%20generation%20tasks.%20Notably%2C%20our%20approach%20has%20achieved%0ASOTA%20performance%20in%20both%20domains%2C%20establishing%20a%20significant%20advancement%20in%20the%0Afield.%20Codes%20are%20available%20at%20https%3A//github.com/ridgerchu/TCJA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2206.10177v3&entry.124074799=Read"},
{"title": "Energy-Efficient Uncertainty-Aware Biomass Composition Prediction at the\n  Edge", "author": "Muhammad Zawish and Paul Albert and Flavio Esposito and Steven Davy and Lizy Abraham", "abstract": "  Clover fixates nitrogen from the atmosphere to the ground, making\ngrass-clover mixtures highly desirable to reduce external nitrogen\nfertilization. Herbage containing clover additionally promotes higher food\nintake, resulting in higher milk production. Herbage probing however remains\nlargely unused as it requires a time-intensive manual laboratory analysis.\nWithout this information, farmers are unable to perform localized clover sowing\nor take targeted fertilization decisions. Deep learning algorithms have been\nproposed with the goal to estimate the dry biomass composition from images of\nthe grass directly in the fields. The energy-intensive nature of deep learning\nhowever limits deployment to practical edge devices such as smartphones. This\npaper proposes to fill this gap by applying filter pruning to reduce the energy\nrequirement of existing deep learning solutions. We report that although pruned\nnetworks are accurate on controlled, high-quality images of the grass, they\nstruggle to generalize to real-world smartphone images that are blurry or taken\nfrom challenging angles. We address this challenge by training filter-pruned\nmodels using a variance attenuation loss so they can predict the uncertainty of\ntheir predictions. When the uncertainty exceeds a threshold, we re-infer using\na more accurate unpruned model. This hybrid approach allows us to reduce energy\nconsumption while retaining a high accuracy. We evaluate our algorithm on two\ndatasets: the GrassClover and the Irish clover using an NVIDIA Jetson Nano edge\ndevice. We find that we reduce energy reduction with respect to\nstate-of-the-art solutions by 50% on average with only 4% accuracy loss.\n", "link": "http://arxiv.org/abs/2404.11230v1", "date": "2024-04-17", "relevancy": 2.0558, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5342}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5158}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.493}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Energy-Efficient%20Uncertainty-Aware%20Biomass%20Composition%20Prediction%20at%20the%0A%20%20Edge&body=Title%3A%20Energy-Efficient%20Uncertainty-Aware%20Biomass%20Composition%20Prediction%20at%20the%0A%20%20Edge%0AAuthor%3A%20Muhammad%20Zawish%20and%20Paul%20Albert%20and%20Flavio%20Esposito%20and%20Steven%20Davy%20and%20Lizy%20Abraham%0AAbstract%3A%20%20%20Clover%20fixates%20nitrogen%20from%20the%20atmosphere%20to%20the%20ground%2C%20making%0Agrass-clover%20mixtures%20highly%20desirable%20to%20reduce%20external%20nitrogen%0Afertilization.%20Herbage%20containing%20clover%20additionally%20promotes%20higher%20food%0Aintake%2C%20resulting%20in%20higher%20milk%20production.%20Herbage%20probing%20however%20remains%0Alargely%20unused%20as%20it%20requires%20a%20time-intensive%20manual%20laboratory%20analysis.%0AWithout%20this%20information%2C%20farmers%20are%20unable%20to%20perform%20localized%20clover%20sowing%0Aor%20take%20targeted%20fertilization%20decisions.%20Deep%20learning%20algorithms%20have%20been%0Aproposed%20with%20the%20goal%20to%20estimate%20the%20dry%20biomass%20composition%20from%20images%20of%0Athe%20grass%20directly%20in%20the%20fields.%20The%20energy-intensive%20nature%20of%20deep%20learning%0Ahowever%20limits%20deployment%20to%20practical%20edge%20devices%20such%20as%20smartphones.%20This%0Apaper%20proposes%20to%20fill%20this%20gap%20by%20applying%20filter%20pruning%20to%20reduce%20the%20energy%0Arequirement%20of%20existing%20deep%20learning%20solutions.%20We%20report%20that%20although%20pruned%0Anetworks%20are%20accurate%20on%20controlled%2C%20high-quality%20images%20of%20the%20grass%2C%20they%0Astruggle%20to%20generalize%20to%20real-world%20smartphone%20images%20that%20are%20blurry%20or%20taken%0Afrom%20challenging%20angles.%20We%20address%20this%20challenge%20by%20training%20filter-pruned%0Amodels%20using%20a%20variance%20attenuation%20loss%20so%20they%20can%20predict%20the%20uncertainty%20of%0Atheir%20predictions.%20When%20the%20uncertainty%20exceeds%20a%20threshold%2C%20we%20re-infer%20using%0Aa%20more%20accurate%20unpruned%20model.%20This%20hybrid%20approach%20allows%20us%20to%20reduce%20energy%0Aconsumption%20while%20retaining%20a%20high%20accuracy.%20We%20evaluate%20our%20algorithm%20on%20two%0Adatasets%3A%20the%20GrassClover%20and%20the%20Irish%20clover%20using%20an%20NVIDIA%20Jetson%20Nano%20edge%0Adevice.%20We%20find%20that%20we%20reduce%20energy%20reduction%20with%20respect%20to%0Astate-of-the-art%20solutions%20by%2050%25%20on%20average%20with%20only%204%25%20accuracy%20loss.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11230v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy-Efficient%20Uncertainty-Aware%20Biomass%20Composition%20Prediction%20at%20the%0A%20%20Edge&entry.906535625=Muhammad%20Zawish%20and%20Paul%20Albert%20and%20Flavio%20Esposito%20and%20Steven%20Davy%20and%20Lizy%20Abraham&entry.1292438233=%20%20Clover%20fixates%20nitrogen%20from%20the%20atmosphere%20to%20the%20ground%2C%20making%0Agrass-clover%20mixtures%20highly%20desirable%20to%20reduce%20external%20nitrogen%0Afertilization.%20Herbage%20containing%20clover%20additionally%20promotes%20higher%20food%0Aintake%2C%20resulting%20in%20higher%20milk%20production.%20Herbage%20probing%20however%20remains%0Alargely%20unused%20as%20it%20requires%20a%20time-intensive%20manual%20laboratory%20analysis.%0AWithout%20this%20information%2C%20farmers%20are%20unable%20to%20perform%20localized%20clover%20sowing%0Aor%20take%20targeted%20fertilization%20decisions.%20Deep%20learning%20algorithms%20have%20been%0Aproposed%20with%20the%20goal%20to%20estimate%20the%20dry%20biomass%20composition%20from%20images%20of%0Athe%20grass%20directly%20in%20the%20fields.%20The%20energy-intensive%20nature%20of%20deep%20learning%0Ahowever%20limits%20deployment%20to%20practical%20edge%20devices%20such%20as%20smartphones.%20This%0Apaper%20proposes%20to%20fill%20this%20gap%20by%20applying%20filter%20pruning%20to%20reduce%20the%20energy%0Arequirement%20of%20existing%20deep%20learning%20solutions.%20We%20report%20that%20although%20pruned%0Anetworks%20are%20accurate%20on%20controlled%2C%20high-quality%20images%20of%20the%20grass%2C%20they%0Astruggle%20to%20generalize%20to%20real-world%20smartphone%20images%20that%20are%20blurry%20or%20taken%0Afrom%20challenging%20angles.%20We%20address%20this%20challenge%20by%20training%20filter-pruned%0Amodels%20using%20a%20variance%20attenuation%20loss%20so%20they%20can%20predict%20the%20uncertainty%20of%0Atheir%20predictions.%20When%20the%20uncertainty%20exceeds%20a%20threshold%2C%20we%20re-infer%20using%0Aa%20more%20accurate%20unpruned%20model.%20This%20hybrid%20approach%20allows%20us%20to%20reduce%20energy%0Aconsumption%20while%20retaining%20a%20high%20accuracy.%20We%20evaluate%20our%20algorithm%20on%20two%0Adatasets%3A%20the%20GrassClover%20and%20the%20Irish%20clover%20using%20an%20NVIDIA%20Jetson%20Nano%20edge%0Adevice.%20We%20find%20that%20we%20reduce%20energy%20reduction%20with%20respect%20to%0Astate-of-the-art%20solutions%20by%2050%25%20on%20average%20with%20only%204%25%20accuracy%20loss.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11230v1&entry.124074799=Read"},
{"title": "A Subspace-Constrained Tyler's Estimator and its Applications to\n  Structure from Motion", "author": "Feng Yu and Teng Zhang and Gilad Lerman", "abstract": "  We present the subspace-constrained Tyler's estimator (STE) designed for\nrecovering a low-dimensional subspace within a dataset that may be highly\ncorrupted with outliers. STE is a fusion of the Tyler's M-estimator (TME) and a\nvariant of the fast median subspace. Our theoretical analysis suggests that,\nunder a common inlier-outlier model, STE can effectively recover the underlying\nsubspace, even when it contains a smaller fraction of inliers relative to other\nmethods in the field of robust subspace recovery. We apply STE in the context\nof Structure from Motion (SfM) in two ways: for robust estimation of the\nfundamental matrix and for the removal of outlying cameras, enhancing the\nrobustness of the SfM pipeline. Numerical experiments confirm the\nstate-of-the-art performance of our method in these applications. This research\nmakes significant contributions to the field of robust subspace recovery,\nparticularly in the context of computer vision and 3D reconstruction.\n", "link": "http://arxiv.org/abs/2404.11590v1", "date": "2024-04-17", "relevancy": 2.0486, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.524}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5214}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4965}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Subspace-Constrained%20Tyler%27s%20Estimator%20and%20its%20Applications%20to%0A%20%20Structure%20from%20Motion&body=Title%3A%20A%20Subspace-Constrained%20Tyler%27s%20Estimator%20and%20its%20Applications%20to%0A%20%20Structure%20from%20Motion%0AAuthor%3A%20Feng%20Yu%20and%20Teng%20Zhang%20and%20Gilad%20Lerman%0AAbstract%3A%20%20%20We%20present%20the%20subspace-constrained%20Tyler%27s%20estimator%20%28STE%29%20designed%20for%0Arecovering%20a%20low-dimensional%20subspace%20within%20a%20dataset%20that%20may%20be%20highly%0Acorrupted%20with%20outliers.%20STE%20is%20a%20fusion%20of%20the%20Tyler%27s%20M-estimator%20%28TME%29%20and%20a%0Avariant%20of%20the%20fast%20median%20subspace.%20Our%20theoretical%20analysis%20suggests%20that%2C%0Aunder%20a%20common%20inlier-outlier%20model%2C%20STE%20can%20effectively%20recover%20the%20underlying%0Asubspace%2C%20even%20when%20it%20contains%20a%20smaller%20fraction%20of%20inliers%20relative%20to%20other%0Amethods%20in%20the%20field%20of%20robust%20subspace%20recovery.%20We%20apply%20STE%20in%20the%20context%0Aof%20Structure%20from%20Motion%20%28SfM%29%20in%20two%20ways%3A%20for%20robust%20estimation%20of%20the%0Afundamental%20matrix%20and%20for%20the%20removal%20of%20outlying%20cameras%2C%20enhancing%20the%0Arobustness%20of%20the%20SfM%20pipeline.%20Numerical%20experiments%20confirm%20the%0Astate-of-the-art%20performance%20of%20our%20method%20in%20these%20applications.%20This%20research%0Amakes%20significant%20contributions%20to%20the%20field%20of%20robust%20subspace%20recovery%2C%0Aparticularly%20in%20the%20context%20of%20computer%20vision%20and%203D%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11590v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Subspace-Constrained%20Tyler%27s%20Estimator%20and%20its%20Applications%20to%0A%20%20Structure%20from%20Motion&entry.906535625=Feng%20Yu%20and%20Teng%20Zhang%20and%20Gilad%20Lerman&entry.1292438233=%20%20We%20present%20the%20subspace-constrained%20Tyler%27s%20estimator%20%28STE%29%20designed%20for%0Arecovering%20a%20low-dimensional%20subspace%20within%20a%20dataset%20that%20may%20be%20highly%0Acorrupted%20with%20outliers.%20STE%20is%20a%20fusion%20of%20the%20Tyler%27s%20M-estimator%20%28TME%29%20and%20a%0Avariant%20of%20the%20fast%20median%20subspace.%20Our%20theoretical%20analysis%20suggests%20that%2C%0Aunder%20a%20common%20inlier-outlier%20model%2C%20STE%20can%20effectively%20recover%20the%20underlying%0Asubspace%2C%20even%20when%20it%20contains%20a%20smaller%20fraction%20of%20inliers%20relative%20to%20other%0Amethods%20in%20the%20field%20of%20robust%20subspace%20recovery.%20We%20apply%20STE%20in%20the%20context%0Aof%20Structure%20from%20Motion%20%28SfM%29%20in%20two%20ways%3A%20for%20robust%20estimation%20of%20the%0Afundamental%20matrix%20and%20for%20the%20removal%20of%20outlying%20cameras%2C%20enhancing%20the%0Arobustness%20of%20the%20SfM%20pipeline.%20Numerical%20experiments%20confirm%20the%0Astate-of-the-art%20performance%20of%20our%20method%20in%20these%20applications.%20This%20research%0Amakes%20significant%20contributions%20to%20the%20field%20of%20robust%20subspace%20recovery%2C%0Aparticularly%20in%20the%20context%20of%20computer%20vision%20and%203D%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11590v1&entry.124074799=Read"},
{"title": "AgentKit: Flow Engineering with Graphs, not Coding", "author": "Yue Wu and Yewen Fan and So Yeon Min and Shrimai Prabhumoye and Stephen McAleer and Yonatan Bisk and Ruslan Salakhutdinov and Yuanzhi Li and Tom Mitchell", "abstract": "  We propose an intuitive LLM prompting framework (AgentKit) for\nmultifunctional agents. AgentKit offers a unified framework for explicitly\nconstructing a complex \"thought process\" from simple natural language prompts.\nThe basic building block in AgentKit is a node, containing a natural language\nprompt for a specific subtask. The user then puts together chains of nodes,\nlike stacking LEGO pieces. The chains of nodes can be designed to explicitly\nenforce a naturally structured \"thought process\". For example, for the task of\nwriting a paper, one may start with the thought process of 1) identify a core\nmessage, 2) identify prior research gaps, etc. The nodes in AgentKit can be\ndesigned and combined in different ways to implement multiple advanced\ncapabilities including on-the-fly hierarchical planning, reflection, and\nlearning from interactions. In addition, due to the modular nature and the\nintuitive design to simulate explicit human thought process, a basic agent\ncould be implemented as simple as a list of prompts for the subtasks and\ntherefore could be designed and tuned by someone without any programming\nexperience. Quantitatively, we show that agents designed through AgentKit\nachieve SOTA performance on WebShop and Crafter. These advances underscore\nAgentKit's potential in making LLM agents effective and accessible for a wider\nrange of applications. https://github.com/holmeswww/AgentKit\n", "link": "http://arxiv.org/abs/2404.11483v1", "date": "2024-04-17", "relevancy": 2.0417, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5619}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5011}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4992}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AgentKit%3A%20Flow%20Engineering%20with%20Graphs%2C%20not%20Coding&body=Title%3A%20AgentKit%3A%20Flow%20Engineering%20with%20Graphs%2C%20not%20Coding%0AAuthor%3A%20Yue%20Wu%20and%20Yewen%20Fan%20and%20So%20Yeon%20Min%20and%20Shrimai%20Prabhumoye%20and%20Stephen%20McAleer%20and%20Yonatan%20Bisk%20and%20Ruslan%20Salakhutdinov%20and%20Yuanzhi%20Li%20and%20Tom%20Mitchell%0AAbstract%3A%20%20%20We%20propose%20an%20intuitive%20LLM%20prompting%20framework%20%28AgentKit%29%20for%0Amultifunctional%20agents.%20AgentKit%20offers%20a%20unified%20framework%20for%20explicitly%0Aconstructing%20a%20complex%20%22thought%20process%22%20from%20simple%20natural%20language%20prompts.%0AThe%20basic%20building%20block%20in%20AgentKit%20is%20a%20node%2C%20containing%20a%20natural%20language%0Aprompt%20for%20a%20specific%20subtask.%20The%20user%20then%20puts%20together%20chains%20of%20nodes%2C%0Alike%20stacking%20LEGO%20pieces.%20The%20chains%20of%20nodes%20can%20be%20designed%20to%20explicitly%0Aenforce%20a%20naturally%20structured%20%22thought%20process%22.%20For%20example%2C%20for%20the%20task%20of%0Awriting%20a%20paper%2C%20one%20may%20start%20with%20the%20thought%20process%20of%201%29%20identify%20a%20core%0Amessage%2C%202%29%20identify%20prior%20research%20gaps%2C%20etc.%20The%20nodes%20in%20AgentKit%20can%20be%0Adesigned%20and%20combined%20in%20different%20ways%20to%20implement%20multiple%20advanced%0Acapabilities%20including%20on-the-fly%20hierarchical%20planning%2C%20reflection%2C%20and%0Alearning%20from%20interactions.%20In%20addition%2C%20due%20to%20the%20modular%20nature%20and%20the%0Aintuitive%20design%20to%20simulate%20explicit%20human%20thought%20process%2C%20a%20basic%20agent%0Acould%20be%20implemented%20as%20simple%20as%20a%20list%20of%20prompts%20for%20the%20subtasks%20and%0Atherefore%20could%20be%20designed%20and%20tuned%20by%20someone%20without%20any%20programming%0Aexperience.%20Quantitatively%2C%20we%20show%20that%20agents%20designed%20through%20AgentKit%0Aachieve%20SOTA%20performance%20on%20WebShop%20and%20Crafter.%20These%20advances%20underscore%0AAgentKit%27s%20potential%20in%20making%20LLM%20agents%20effective%20and%20accessible%20for%20a%20wider%0Arange%20of%20applications.%20https%3A//github.com/holmeswww/AgentKit%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11483v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentKit%3A%20Flow%20Engineering%20with%20Graphs%2C%20not%20Coding&entry.906535625=Yue%20Wu%20and%20Yewen%20Fan%20and%20So%20Yeon%20Min%20and%20Shrimai%20Prabhumoye%20and%20Stephen%20McAleer%20and%20Yonatan%20Bisk%20and%20Ruslan%20Salakhutdinov%20and%20Yuanzhi%20Li%20and%20Tom%20Mitchell&entry.1292438233=%20%20We%20propose%20an%20intuitive%20LLM%20prompting%20framework%20%28AgentKit%29%20for%0Amultifunctional%20agents.%20AgentKit%20offers%20a%20unified%20framework%20for%20explicitly%0Aconstructing%20a%20complex%20%22thought%20process%22%20from%20simple%20natural%20language%20prompts.%0AThe%20basic%20building%20block%20in%20AgentKit%20is%20a%20node%2C%20containing%20a%20natural%20language%0Aprompt%20for%20a%20specific%20subtask.%20The%20user%20then%20puts%20together%20chains%20of%20nodes%2C%0Alike%20stacking%20LEGO%20pieces.%20The%20chains%20of%20nodes%20can%20be%20designed%20to%20explicitly%0Aenforce%20a%20naturally%20structured%20%22thought%20process%22.%20For%20example%2C%20for%20the%20task%20of%0Awriting%20a%20paper%2C%20one%20may%20start%20with%20the%20thought%20process%20of%201%29%20identify%20a%20core%0Amessage%2C%202%29%20identify%20prior%20research%20gaps%2C%20etc.%20The%20nodes%20in%20AgentKit%20can%20be%0Adesigned%20and%20combined%20in%20different%20ways%20to%20implement%20multiple%20advanced%0Acapabilities%20including%20on-the-fly%20hierarchical%20planning%2C%20reflection%2C%20and%0Alearning%20from%20interactions.%20In%20addition%2C%20due%20to%20the%20modular%20nature%20and%20the%0Aintuitive%20design%20to%20simulate%20explicit%20human%20thought%20process%2C%20a%20basic%20agent%0Acould%20be%20implemented%20as%20simple%20as%20a%20list%20of%20prompts%20for%20the%20subtasks%20and%0Atherefore%20could%20be%20designed%20and%20tuned%20by%20someone%20without%20any%20programming%0Aexperience.%20Quantitatively%2C%20we%20show%20that%20agents%20designed%20through%20AgentKit%0Aachieve%20SOTA%20performance%20on%20WebShop%20and%20Crafter.%20These%20advances%20underscore%0AAgentKit%27s%20potential%20in%20making%20LLM%20agents%20effective%20and%20accessible%20for%20a%20wider%0Arange%20of%20applications.%20https%3A//github.com/holmeswww/AgentKit%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11483v1&entry.124074799=Read"},
{"title": "In-Context Learning State Vector with Inner and Momentum Optimization", "author": "Dongfang Li and Zhenyu Liu and Xinshuo Hu and Zetian Sun and Baotian Hu and Min Zhang", "abstract": "  Large Language Models (LLMs) have exhibited an impressive ability to perform\nIn-Context Learning (ICL) from only a few examples. Recent works have indicated\nthat the functions learned by ICL can be represented through compressed vectors\nderived from the transformer. However, the working mechanisms and optimization\nof these vectors are yet to be thoroughly explored. In this paper, we address\nthis gap by presenting a comprehensive analysis of these compressed vectors,\ndrawing parallels to the parameters trained with gradient descent, and\nintroduce the concept of state vector. Inspired by the works on model soup and\nmomentum-based gradient descent, we propose inner and momentum optimization\nmethods that are applied to refine the state vector progressively as test-time\nadaptation. Moreover, we simulate state vector aggregation in the multiple\nexample setting, where demonstrations comprising numerous examples are usually\ntoo lengthy for regular ICL, and further propose a divide-and-conquer\naggregation method to address this challenge. We conduct extensive experiments\nusing Llama-2 and GPT-J in both zero-shot setting and few-shot setting. The\nexperimental results show that our optimization method effectively enhances the\nstate vector and achieves the state-of-the-art performance on diverse tasks.\nCode is available at https://github.com/HITsz-TMG/ICL-State-Vector\n", "link": "http://arxiv.org/abs/2404.11225v1", "date": "2024-04-17", "relevancy": 2.0373, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.523}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5022}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.493}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20In-Context%20Learning%20State%20Vector%20with%20Inner%20and%20Momentum%20Optimization&body=Title%3A%20In-Context%20Learning%20State%20Vector%20with%20Inner%20and%20Momentum%20Optimization%0AAuthor%3A%20Dongfang%20Li%20and%20Zhenyu%20Liu%20and%20Xinshuo%20Hu%20and%20Zetian%20Sun%20and%20Baotian%20Hu%20and%20Min%20Zhang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20exhibited%20an%20impressive%20ability%20to%20perform%0AIn-Context%20Learning%20%28ICL%29%20from%20only%20a%20few%20examples.%20Recent%20works%20have%20indicated%0Athat%20the%20functions%20learned%20by%20ICL%20can%20be%20represented%20through%20compressed%20vectors%0Aderived%20from%20the%20transformer.%20However%2C%20the%20working%20mechanisms%20and%20optimization%0Aof%20these%20vectors%20are%20yet%20to%20be%20thoroughly%20explored.%20In%20this%20paper%2C%20we%20address%0Athis%20gap%20by%20presenting%20a%20comprehensive%20analysis%20of%20these%20compressed%20vectors%2C%0Adrawing%20parallels%20to%20the%20parameters%20trained%20with%20gradient%20descent%2C%20and%0Aintroduce%20the%20concept%20of%20state%20vector.%20Inspired%20by%20the%20works%20on%20model%20soup%20and%0Amomentum-based%20gradient%20descent%2C%20we%20propose%20inner%20and%20momentum%20optimization%0Amethods%20that%20are%20applied%20to%20refine%20the%20state%20vector%20progressively%20as%20test-time%0Aadaptation.%20Moreover%2C%20we%20simulate%20state%20vector%20aggregation%20in%20the%20multiple%0Aexample%20setting%2C%20where%20demonstrations%20comprising%20numerous%20examples%20are%20usually%0Atoo%20lengthy%20for%20regular%20ICL%2C%20and%20further%20propose%20a%20divide-and-conquer%0Aaggregation%20method%20to%20address%20this%20challenge.%20We%20conduct%20extensive%20experiments%0Ausing%20Llama-2%20and%20GPT-J%20in%20both%20zero-shot%20setting%20and%20few-shot%20setting.%20The%0Aexperimental%20results%20show%20that%20our%20optimization%20method%20effectively%20enhances%20the%0Astate%20vector%20and%20achieves%20the%20state-of-the-art%20performance%20on%20diverse%20tasks.%0ACode%20is%20available%20at%20https%3A//github.com/HITsz-TMG/ICL-State-Vector%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11225v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Context%20Learning%20State%20Vector%20with%20Inner%20and%20Momentum%20Optimization&entry.906535625=Dongfang%20Li%20and%20Zhenyu%20Liu%20and%20Xinshuo%20Hu%20and%20Zetian%20Sun%20and%20Baotian%20Hu%20and%20Min%20Zhang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20exhibited%20an%20impressive%20ability%20to%20perform%0AIn-Context%20Learning%20%28ICL%29%20from%20only%20a%20few%20examples.%20Recent%20works%20have%20indicated%0Athat%20the%20functions%20learned%20by%20ICL%20can%20be%20represented%20through%20compressed%20vectors%0Aderived%20from%20the%20transformer.%20However%2C%20the%20working%20mechanisms%20and%20optimization%0Aof%20these%20vectors%20are%20yet%20to%20be%20thoroughly%20explored.%20In%20this%20paper%2C%20we%20address%0Athis%20gap%20by%20presenting%20a%20comprehensive%20analysis%20of%20these%20compressed%20vectors%2C%0Adrawing%20parallels%20to%20the%20parameters%20trained%20with%20gradient%20descent%2C%20and%0Aintroduce%20the%20concept%20of%20state%20vector.%20Inspired%20by%20the%20works%20on%20model%20soup%20and%0Amomentum-based%20gradient%20descent%2C%20we%20propose%20inner%20and%20momentum%20optimization%0Amethods%20that%20are%20applied%20to%20refine%20the%20state%20vector%20progressively%20as%20test-time%0Aadaptation.%20Moreover%2C%20we%20simulate%20state%20vector%20aggregation%20in%20the%20multiple%0Aexample%20setting%2C%20where%20demonstrations%20comprising%20numerous%20examples%20are%20usually%0Atoo%20lengthy%20for%20regular%20ICL%2C%20and%20further%20propose%20a%20divide-and-conquer%0Aaggregation%20method%20to%20address%20this%20challenge.%20We%20conduct%20extensive%20experiments%0Ausing%20Llama-2%20and%20GPT-J%20in%20both%20zero-shot%20setting%20and%20few-shot%20setting.%20The%0Aexperimental%20results%20show%20that%20our%20optimization%20method%20effectively%20enhances%20the%0Astate%20vector%20and%20achieves%20the%20state-of-the-art%20performance%20on%20diverse%20tasks.%0ACode%20is%20available%20at%20https%3A//github.com/HITsz-TMG/ICL-State-Vector%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11225v1&entry.124074799=Read"},
{"title": "ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs", "author": "Zihao Zhao and Sheng Wang and Jinchen Gu and Yitao Zhu and Lanzhuju Mei and Zixu Zhuang and Zhiming Cui and Qian Wang and Dinggang Shen", "abstract": "  The integration of Computer-Aided Diagnosis (CAD) with Large Language Models\n(LLMs) presents a promising frontier in clinical applications, notably in\nautomating diagnostic processes akin to those performed by radiologists and\nproviding consultations similar to a virtual family doctor. Despite the\npromising potential of this integration, current works face at least two\nlimitations: (1) From the perspective of a radiologist, existing studies\ntypically have a restricted scope of applicable imaging domains, failing to\nmeet the diagnostic needs of different patients. Also, the insufficient\ndiagnostic capability of LLMs further undermine the quality and reliability of\nthe generated medical reports. (2) Current LLMs lack the requisite depth in\nmedical expertise, rendering them less effective as virtual family doctors due\nto the potential unreliability of the advice provided during patient\nconsultations. To address these limitations, we introduce ChatCAD+, to be\nuniversal and reliable. Specifically, it is featured by two main modules: (1)\nReliable Report Generation and (2) Reliable Interaction. The Reliable Report\nGeneration module is capable of interpreting medical images from diverse\ndomains and generate high-quality medical reports via our proposed hierarchical\nin-context learning. Concurrently, the interaction module leverages up-to-date\ninformation from reputable medical websites to provide reliable medical advice.\nTogether, these designed modules synergize to closely align with the expertise\nof human medical professionals, offering enhanced consistency and reliability\nfor interpretation and advice. The source code is available at\nhttps://github.com/zhaozh10/ChatCAD.\n", "link": "http://arxiv.org/abs/2305.15964v5", "date": "2024-04-17", "relevancy": 2.0369, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.535}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5163}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4806}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ChatCAD%2B%3A%20Towards%20a%20Universal%20and%20Reliable%20Interactive%20CAD%20using%20LLMs&body=Title%3A%20ChatCAD%2B%3A%20Towards%20a%20Universal%20and%20Reliable%20Interactive%20CAD%20using%20LLMs%0AAuthor%3A%20Zihao%20Zhao%20and%20Sheng%20Wang%20and%20Jinchen%20Gu%20and%20Yitao%20Zhu%20and%20Lanzhuju%20Mei%20and%20Zixu%20Zhuang%20and%20Zhiming%20Cui%20and%20Qian%20Wang%20and%20Dinggang%20Shen%0AAbstract%3A%20%20%20The%20integration%20of%20Computer-Aided%20Diagnosis%20%28CAD%29%20with%20Large%20Language%20Models%0A%28LLMs%29%20presents%20a%20promising%20frontier%20in%20clinical%20applications%2C%20notably%20in%0Aautomating%20diagnostic%20processes%20akin%20to%20those%20performed%20by%20radiologists%20and%0Aproviding%20consultations%20similar%20to%20a%20virtual%20family%20doctor.%20Despite%20the%0Apromising%20potential%20of%20this%20integration%2C%20current%20works%20face%20at%20least%20two%0Alimitations%3A%20%281%29%20From%20the%20perspective%20of%20a%20radiologist%2C%20existing%20studies%0Atypically%20have%20a%20restricted%20scope%20of%20applicable%20imaging%20domains%2C%20failing%20to%0Ameet%20the%20diagnostic%20needs%20of%20different%20patients.%20Also%2C%20the%20insufficient%0Adiagnostic%20capability%20of%20LLMs%20further%20undermine%20the%20quality%20and%20reliability%20of%0Athe%20generated%20medical%20reports.%20%282%29%20Current%20LLMs%20lack%20the%20requisite%20depth%20in%0Amedical%20expertise%2C%20rendering%20them%20less%20effective%20as%20virtual%20family%20doctors%20due%0Ato%20the%20potential%20unreliability%20of%20the%20advice%20provided%20during%20patient%0Aconsultations.%20To%20address%20these%20limitations%2C%20we%20introduce%20ChatCAD%2B%2C%20to%20be%0Auniversal%20and%20reliable.%20Specifically%2C%20it%20is%20featured%20by%20two%20main%20modules%3A%20%281%29%0AReliable%20Report%20Generation%20and%20%282%29%20Reliable%20Interaction.%20The%20Reliable%20Report%0AGeneration%20module%20is%20capable%20of%20interpreting%20medical%20images%20from%20diverse%0Adomains%20and%20generate%20high-quality%20medical%20reports%20via%20our%20proposed%20hierarchical%0Ain-context%20learning.%20Concurrently%2C%20the%20interaction%20module%20leverages%20up-to-date%0Ainformation%20from%20reputable%20medical%20websites%20to%20provide%20reliable%20medical%20advice.%0ATogether%2C%20these%20designed%20modules%20synergize%20to%20closely%20align%20with%20the%20expertise%0Aof%20human%20medical%20professionals%2C%20offering%20enhanced%20consistency%20and%20reliability%0Afor%20interpretation%20and%20advice.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/zhaozh10/ChatCAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.15964v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatCAD%2B%3A%20Towards%20a%20Universal%20and%20Reliable%20Interactive%20CAD%20using%20LLMs&entry.906535625=Zihao%20Zhao%20and%20Sheng%20Wang%20and%20Jinchen%20Gu%20and%20Yitao%20Zhu%20and%20Lanzhuju%20Mei%20and%20Zixu%20Zhuang%20and%20Zhiming%20Cui%20and%20Qian%20Wang%20and%20Dinggang%20Shen&entry.1292438233=%20%20The%20integration%20of%20Computer-Aided%20Diagnosis%20%28CAD%29%20with%20Large%20Language%20Models%0A%28LLMs%29%20presents%20a%20promising%20frontier%20in%20clinical%20applications%2C%20notably%20in%0Aautomating%20diagnostic%20processes%20akin%20to%20those%20performed%20by%20radiologists%20and%0Aproviding%20consultations%20similar%20to%20a%20virtual%20family%20doctor.%20Despite%20the%0Apromising%20potential%20of%20this%20integration%2C%20current%20works%20face%20at%20least%20two%0Alimitations%3A%20%281%29%20From%20the%20perspective%20of%20a%20radiologist%2C%20existing%20studies%0Atypically%20have%20a%20restricted%20scope%20of%20applicable%20imaging%20domains%2C%20failing%20to%0Ameet%20the%20diagnostic%20needs%20of%20different%20patients.%20Also%2C%20the%20insufficient%0Adiagnostic%20capability%20of%20LLMs%20further%20undermine%20the%20quality%20and%20reliability%20of%0Athe%20generated%20medical%20reports.%20%282%29%20Current%20LLMs%20lack%20the%20requisite%20depth%20in%0Amedical%20expertise%2C%20rendering%20them%20less%20effective%20as%20virtual%20family%20doctors%20due%0Ato%20the%20potential%20unreliability%20of%20the%20advice%20provided%20during%20patient%0Aconsultations.%20To%20address%20these%20limitations%2C%20we%20introduce%20ChatCAD%2B%2C%20to%20be%0Auniversal%20and%20reliable.%20Specifically%2C%20it%20is%20featured%20by%20two%20main%20modules%3A%20%281%29%0AReliable%20Report%20Generation%20and%20%282%29%20Reliable%20Interaction.%20The%20Reliable%20Report%0AGeneration%20module%20is%20capable%20of%20interpreting%20medical%20images%20from%20diverse%0Adomains%20and%20generate%20high-quality%20medical%20reports%20via%20our%20proposed%20hierarchical%0Ain-context%20learning.%20Concurrently%2C%20the%20interaction%20module%20leverages%20up-to-date%0Ainformation%20from%20reputable%20medical%20websites%20to%20provide%20reliable%20medical%20advice.%0ATogether%2C%20these%20designed%20modules%20synergize%20to%20closely%20align%20with%20the%20expertise%0Aof%20human%20medical%20professionals%2C%20offering%20enhanced%20consistency%20and%20reliability%0Afor%20interpretation%20and%20advice.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/zhaozh10/ChatCAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.15964v5&entry.124074799=Read"},
{"title": "GenFighter: A Generative and Evolutive Textual Attack Removal", "author": "Md Athikul Islam and Edoardo Serra and Sushil Jajodia", "abstract": "  Adversarial attacks pose significant challenges to deep neural networks\n(DNNs) such as Transformer models in natural language processing (NLP). This\npaper introduces a novel defense strategy, called GenFighter, which enhances\nadversarial robustness by learning and reasoning on the training classification\ndistribution. GenFighter identifies potentially malicious instances deviating\nfrom the distribution, transforms them into semantically equivalent instances\naligned with the training data, and employs ensemble techniques for a unified\nand robust response. By conducting extensive experiments, we show that\nGenFighter outperforms state-of-the-art defenses in accuracy under attack and\nattack success rate metrics. Additionally, it requires a high number of queries\nper attack, making the attack more challenging in real scenarios. The ablation\nstudy shows that our approach integrates transfer learning, a\ngenerative/evolutive procedure, and an ensemble method, providing an effective\ndefense against NLP adversarial attacks.\n", "link": "http://arxiv.org/abs/2404.11538v1", "date": "2024-04-17", "relevancy": 2.0255, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5279}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5089}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4838}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GenFighter%3A%20A%20Generative%20and%20Evolutive%20Textual%20Attack%20Removal&body=Title%3A%20GenFighter%3A%20A%20Generative%20and%20Evolutive%20Textual%20Attack%20Removal%0AAuthor%3A%20Md%20Athikul%20Islam%20and%20Edoardo%20Serra%20and%20Sushil%20Jajodia%0AAbstract%3A%20%20%20Adversarial%20attacks%20pose%20significant%20challenges%20to%20deep%20neural%20networks%0A%28DNNs%29%20such%20as%20Transformer%20models%20in%20natural%20language%20processing%20%28NLP%29.%20This%0Apaper%20introduces%20a%20novel%20defense%20strategy%2C%20called%20GenFighter%2C%20which%20enhances%0Aadversarial%20robustness%20by%20learning%20and%20reasoning%20on%20the%20training%20classification%0Adistribution.%20GenFighter%20identifies%20potentially%20malicious%20instances%20deviating%0Afrom%20the%20distribution%2C%20transforms%20them%20into%20semantically%20equivalent%20instances%0Aaligned%20with%20the%20training%20data%2C%20and%20employs%20ensemble%20techniques%20for%20a%20unified%0Aand%20robust%20response.%20By%20conducting%20extensive%20experiments%2C%20we%20show%20that%0AGenFighter%20outperforms%20state-of-the-art%20defenses%20in%20accuracy%20under%20attack%20and%0Aattack%20success%20rate%20metrics.%20Additionally%2C%20it%20requires%20a%20high%20number%20of%20queries%0Aper%20attack%2C%20making%20the%20attack%20more%20challenging%20in%20real%20scenarios.%20The%20ablation%0Astudy%20shows%20that%20our%20approach%20integrates%20transfer%20learning%2C%20a%0Agenerative/evolutive%20procedure%2C%20and%20an%20ensemble%20method%2C%20providing%20an%20effective%0Adefense%20against%20NLP%20adversarial%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11538v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenFighter%3A%20A%20Generative%20and%20Evolutive%20Textual%20Attack%20Removal&entry.906535625=Md%20Athikul%20Islam%20and%20Edoardo%20Serra%20and%20Sushil%20Jajodia&entry.1292438233=%20%20Adversarial%20attacks%20pose%20significant%20challenges%20to%20deep%20neural%20networks%0A%28DNNs%29%20such%20as%20Transformer%20models%20in%20natural%20language%20processing%20%28NLP%29.%20This%0Apaper%20introduces%20a%20novel%20defense%20strategy%2C%20called%20GenFighter%2C%20which%20enhances%0Aadversarial%20robustness%20by%20learning%20and%20reasoning%20on%20the%20training%20classification%0Adistribution.%20GenFighter%20identifies%20potentially%20malicious%20instances%20deviating%0Afrom%20the%20distribution%2C%20transforms%20them%20into%20semantically%20equivalent%20instances%0Aaligned%20with%20the%20training%20data%2C%20and%20employs%20ensemble%20techniques%20for%20a%20unified%0Aand%20robust%20response.%20By%20conducting%20extensive%20experiments%2C%20we%20show%20that%0AGenFighter%20outperforms%20state-of-the-art%20defenses%20in%20accuracy%20under%20attack%20and%0Aattack%20success%20rate%20metrics.%20Additionally%2C%20it%20requires%20a%20high%20number%20of%20queries%0Aper%20attack%2C%20making%20the%20attack%20more%20challenging%20in%20real%20scenarios.%20The%20ablation%0Astudy%20shows%20that%20our%20approach%20integrates%20transfer%20learning%2C%20a%0Agenerative/evolutive%20procedure%2C%20and%20an%20ensemble%20method%2C%20providing%20an%20effective%0Adefense%20against%20NLP%20adversarial%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11538v1&entry.124074799=Read"},
{"title": "BurstAttention: An Efficient Distributed Attention Framework for\n  Extremely Long Sequences", "author": "Sun Ao and Weilin Zhao and Xu Han and Cheng Yang and Zhiyuan Liu and Chuan Shi and Maosong Sun and Shengnan Wang and Teng Su", "abstract": "  Effective attention modules have played a crucial role in the success of\nTransformer-based large language models (LLMs), but the quadratic time and\nmemory complexities of these attention modules also pose a challenge when\nprocessing long sequences. One potential solution for the long sequence problem\nis to utilize distributed clusters to parallelize the computation of attention\nmodules across multiple devices (e.g., GPUs). However, adopting a distributed\napproach inevitably introduces extra memory overheads to store local attention\nresults and incurs additional communication costs to aggregate local results\ninto global ones. In this paper, we propose a distributed attention framework\nnamed ``BurstAttention'' to optimize memory access and communication operations\nat both the global cluster and local device levels. In our experiments, we\ncompare BurstAttention with other competitive distributed attention solutions\nfor long sequence processing. The experimental results under different length\nsettings demonstrate that BurstAttention offers significant advantages for\nprocessing long sequences compared with these competitive baselines, reducing\n40% communication overheads and achieving 1.37 X speedup during training 128K\nsequence length on 32 X A100.\n", "link": "http://arxiv.org/abs/2403.09347v2", "date": "2024-04-17", "relevancy": 2.0172, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5295}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.487}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4846}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20BurstAttention%3A%20An%20Efficient%20Distributed%20Attention%20Framework%20for%0A%20%20Extremely%20Long%20Sequences&body=Title%3A%20BurstAttention%3A%20An%20Efficient%20Distributed%20Attention%20Framework%20for%0A%20%20Extremely%20Long%20Sequences%0AAuthor%3A%20Sun%20Ao%20and%20Weilin%20Zhao%20and%20Xu%20Han%20and%20Cheng%20Yang%20and%20Zhiyuan%20Liu%20and%20Chuan%20Shi%20and%20Maosong%20Sun%20and%20Shengnan%20Wang%20and%20Teng%20Su%0AAbstract%3A%20%20%20Effective%20attention%20modules%20have%20played%20a%20crucial%20role%20in%20the%20success%20of%0ATransformer-based%20large%20language%20models%20%28LLMs%29%2C%20but%20the%20quadratic%20time%20and%0Amemory%20complexities%20of%20these%20attention%20modules%20also%20pose%20a%20challenge%20when%0Aprocessing%20long%20sequences.%20One%20potential%20solution%20for%20the%20long%20sequence%20problem%0Ais%20to%20utilize%20distributed%20clusters%20to%20parallelize%20the%20computation%20of%20attention%0Amodules%20across%20multiple%20devices%20%28e.g.%2C%20GPUs%29.%20However%2C%20adopting%20a%20distributed%0Aapproach%20inevitably%20introduces%20extra%20memory%20overheads%20to%20store%20local%20attention%0Aresults%20and%20incurs%20additional%20communication%20costs%20to%20aggregate%20local%20results%0Ainto%20global%20ones.%20In%20this%20paper%2C%20we%20propose%20a%20distributed%20attention%20framework%0Anamed%20%60%60BurstAttention%27%27%20to%20optimize%20memory%20access%20and%20communication%20operations%0Aat%20both%20the%20global%20cluster%20and%20local%20device%20levels.%20In%20our%20experiments%2C%20we%0Acompare%20BurstAttention%20with%20other%20competitive%20distributed%20attention%20solutions%0Afor%20long%20sequence%20processing.%20The%20experimental%20results%20under%20different%20length%0Asettings%20demonstrate%20that%20BurstAttention%20offers%20significant%20advantages%20for%0Aprocessing%20long%20sequences%20compared%20with%20these%20competitive%20baselines%2C%20reducing%0A40%25%20communication%20overheads%20and%20achieving%201.37%20X%20speedup%20during%20training%20128K%0Asequence%20length%20on%2032%20X%20A100.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09347v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BurstAttention%3A%20An%20Efficient%20Distributed%20Attention%20Framework%20for%0A%20%20Extremely%20Long%20Sequences&entry.906535625=Sun%20Ao%20and%20Weilin%20Zhao%20and%20Xu%20Han%20and%20Cheng%20Yang%20and%20Zhiyuan%20Liu%20and%20Chuan%20Shi%20and%20Maosong%20Sun%20and%20Shengnan%20Wang%20and%20Teng%20Su&entry.1292438233=%20%20Effective%20attention%20modules%20have%20played%20a%20crucial%20role%20in%20the%20success%20of%0ATransformer-based%20large%20language%20models%20%28LLMs%29%2C%20but%20the%20quadratic%20time%20and%0Amemory%20complexities%20of%20these%20attention%20modules%20also%20pose%20a%20challenge%20when%0Aprocessing%20long%20sequences.%20One%20potential%20solution%20for%20the%20long%20sequence%20problem%0Ais%20to%20utilize%20distributed%20clusters%20to%20parallelize%20the%20computation%20of%20attention%0Amodules%20across%20multiple%20devices%20%28e.g.%2C%20GPUs%29.%20However%2C%20adopting%20a%20distributed%0Aapproach%20inevitably%20introduces%20extra%20memory%20overheads%20to%20store%20local%20attention%0Aresults%20and%20incurs%20additional%20communication%20costs%20to%20aggregate%20local%20results%0Ainto%20global%20ones.%20In%20this%20paper%2C%20we%20propose%20a%20distributed%20attention%20framework%0Anamed%20%60%60BurstAttention%27%27%20to%20optimize%20memory%20access%20and%20communication%20operations%0Aat%20both%20the%20global%20cluster%20and%20local%20device%20levels.%20In%20our%20experiments%2C%20we%0Acompare%20BurstAttention%20with%20other%20competitive%20distributed%20attention%20solutions%0Afor%20long%20sequence%20processing.%20The%20experimental%20results%20under%20different%20length%0Asettings%20demonstrate%20that%20BurstAttention%20offers%20significant%20advantages%20for%0Aprocessing%20long%20sequences%20compared%20with%20these%20competitive%20baselines%2C%20reducing%0A40%25%20communication%20overheads%20and%20achieving%201.37%20X%20speedup%20during%20training%20128K%0Asequence%20length%20on%2032%20X%20A100.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09347v2&entry.124074799=Read"},
{"title": "DiscDiff: Latent Diffusion Model for DNA Sequence Generation", "author": "Zehui Li and Yuhao Ni and William A V Beardall and Guoxuan Xia and Akashaditya Das and Guy-Bart Stan and Yiren Zhao", "abstract": "  This paper introduces a novel framework for DNA sequence generation,\ncomprising two key components: DiscDiff, a Latent Diffusion Model (LDM)\ntailored for generating discrete DNA sequences, and Absorb-Escape, a\npost-training algorithm designed to refine these sequences. Absorb-Escape\nenhances the realism of the generated sequences by correcting `round errors'\ninherent in the conversion process between latent and input spaces. Our\napproach not only sets new standards in DNA sequence generation but also\ndemonstrates superior performance over existing diffusion models, in generating\nboth short and long DNA sequences. Additionally, we introduce EPD-GenDNA, the\nfirst comprehensive, multi-species dataset for DNA generation, encompassing\n160,000 unique sequences from 15 species. We hope this study will advance the\ngenerative modelling of DNA, with potential implications for gene therapy and\nprotein production.\n", "link": "http://arxiv.org/abs/2402.06079v2", "date": "2024-04-17", "relevancy": 2.0164, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5403}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.51}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4837}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DiscDiff%3A%20Latent%20Diffusion%20Model%20for%20DNA%20Sequence%20Generation&body=Title%3A%20DiscDiff%3A%20Latent%20Diffusion%20Model%20for%20DNA%20Sequence%20Generation%0AAuthor%3A%20Zehui%20Li%20and%20Yuhao%20Ni%20and%20William%20A%20V%20Beardall%20and%20Guoxuan%20Xia%20and%20Akashaditya%20Das%20and%20Guy-Bart%20Stan%20and%20Yiren%20Zhao%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20framework%20for%20DNA%20sequence%20generation%2C%0Acomprising%20two%20key%20components%3A%20DiscDiff%2C%20a%20Latent%20Diffusion%20Model%20%28LDM%29%0Atailored%20for%20generating%20discrete%20DNA%20sequences%2C%20and%20Absorb-Escape%2C%20a%0Apost-training%20algorithm%20designed%20to%20refine%20these%20sequences.%20Absorb-Escape%0Aenhances%20the%20realism%20of%20the%20generated%20sequences%20by%20correcting%20%60round%20errors%27%0Ainherent%20in%20the%20conversion%20process%20between%20latent%20and%20input%20spaces.%20Our%0Aapproach%20not%20only%20sets%20new%20standards%20in%20DNA%20sequence%20generation%20but%20also%0Ademonstrates%20superior%20performance%20over%20existing%20diffusion%20models%2C%20in%20generating%0Aboth%20short%20and%20long%20DNA%20sequences.%20Additionally%2C%20we%20introduce%20EPD-GenDNA%2C%20the%0Afirst%20comprehensive%2C%20multi-species%20dataset%20for%20DNA%20generation%2C%20encompassing%0A160%2C000%20unique%20sequences%20from%2015%20species.%20We%20hope%20this%20study%20will%20advance%20the%0Agenerative%20modelling%20of%20DNA%2C%20with%20potential%20implications%20for%20gene%20therapy%20and%0Aprotein%20production.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06079v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiscDiff%3A%20Latent%20Diffusion%20Model%20for%20DNA%20Sequence%20Generation&entry.906535625=Zehui%20Li%20and%20Yuhao%20Ni%20and%20William%20A%20V%20Beardall%20and%20Guoxuan%20Xia%20and%20Akashaditya%20Das%20and%20Guy-Bart%20Stan%20and%20Yiren%20Zhao&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20framework%20for%20DNA%20sequence%20generation%2C%0Acomprising%20two%20key%20components%3A%20DiscDiff%2C%20a%20Latent%20Diffusion%20Model%20%28LDM%29%0Atailored%20for%20generating%20discrete%20DNA%20sequences%2C%20and%20Absorb-Escape%2C%20a%0Apost-training%20algorithm%20designed%20to%20refine%20these%20sequences.%20Absorb-Escape%0Aenhances%20the%20realism%20of%20the%20generated%20sequences%20by%20correcting%20%60round%20errors%27%0Ainherent%20in%20the%20conversion%20process%20between%20latent%20and%20input%20spaces.%20Our%0Aapproach%20not%20only%20sets%20new%20standards%20in%20DNA%20sequence%20generation%20but%20also%0Ademonstrates%20superior%20performance%20over%20existing%20diffusion%20models%2C%20in%20generating%0Aboth%20short%20and%20long%20DNA%20sequences.%20Additionally%2C%20we%20introduce%20EPD-GenDNA%2C%20the%0Afirst%20comprehensive%2C%20multi-species%20dataset%20for%20DNA%20generation%2C%20encompassing%0A160%2C000%20unique%20sequences%20from%2015%20species.%20We%20hope%20this%20study%20will%20advance%20the%0Agenerative%20modelling%20of%20DNA%2C%20with%20potential%20implications%20for%20gene%20therapy%20and%0Aprotein%20production.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06079v2&entry.124074799=Read"},
{"title": "FedPFT: Federated Proxy Fine-Tuning of Foundation Models", "author": "Zhaopeng Peng and Xiaoliang Fan and Yufan Chen and Zheng Wang and Shirui Pan and Chenglu Wen and Ruisheng Zhang and Cheng Wang", "abstract": "  Adapting Foundation Models (FMs) for downstream tasks through Federated\nLearning (FL) emerges a promising strategy for protecting data privacy and\nvaluable FMs. Existing methods fine-tune FM by allocating sub-FM to clients in\nFL, however, leading to suboptimal performance due to insufficient tuning and\ninevitable error accumulations of gradients. In this paper, we propose\nFederated Proxy Fine-Tuning (FedPFT), a novel method enhancing FMs adaptation\nin downstream tasks through FL by two key modules. First, the sub-FM\nconstruction module employs a layer-wise compression approach, facilitating\ncomprehensive FM fine-tuning across all layers by emphasizing those crucial\nneurons. Second, the sub-FM alignment module conducts a two-step\ndistillations-layer-level and neuron-level-before and during FL fine-tuning\nrespectively, to reduce error of gradient by accurately aligning sub-FM with FM\nunder theoretical guarantees. Experimental results on seven commonly used\ndatasets (i.e., four text and three vision) demonstrate the superiority of\nFedPFT.\n", "link": "http://arxiv.org/abs/2404.11536v1", "date": "2024-04-17", "relevancy": 2.0079, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5138}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5008}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4984}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FedPFT%3A%20Federated%20Proxy%20Fine-Tuning%20of%20Foundation%20Models&body=Title%3A%20FedPFT%3A%20Federated%20Proxy%20Fine-Tuning%20of%20Foundation%20Models%0AAuthor%3A%20Zhaopeng%20Peng%20and%20Xiaoliang%20Fan%20and%20Yufan%20Chen%20and%20Zheng%20Wang%20and%20Shirui%20Pan%20and%20Chenglu%20Wen%20and%20Ruisheng%20Zhang%20and%20Cheng%20Wang%0AAbstract%3A%20%20%20Adapting%20Foundation%20Models%20%28FMs%29%20for%20downstream%20tasks%20through%20Federated%0ALearning%20%28FL%29%20emerges%20a%20promising%20strategy%20for%20protecting%20data%20privacy%20and%0Avaluable%20FMs.%20Existing%20methods%20fine-tune%20FM%20by%20allocating%20sub-FM%20to%20clients%20in%0AFL%2C%20however%2C%20leading%20to%20suboptimal%20performance%20due%20to%20insufficient%20tuning%20and%0Ainevitable%20error%20accumulations%20of%20gradients.%20In%20this%20paper%2C%20we%20propose%0AFederated%20Proxy%20Fine-Tuning%20%28FedPFT%29%2C%20a%20novel%20method%20enhancing%20FMs%20adaptation%0Ain%20downstream%20tasks%20through%20FL%20by%20two%20key%20modules.%20First%2C%20the%20sub-FM%0Aconstruction%20module%20employs%20a%20layer-wise%20compression%20approach%2C%20facilitating%0Acomprehensive%20FM%20fine-tuning%20across%20all%20layers%20by%20emphasizing%20those%20crucial%0Aneurons.%20Second%2C%20the%20sub-FM%20alignment%20module%20conducts%20a%20two-step%0Adistillations-layer-level%20and%20neuron-level-before%20and%20during%20FL%20fine-tuning%0Arespectively%2C%20to%20reduce%20error%20of%20gradient%20by%20accurately%20aligning%20sub-FM%20with%20FM%0Aunder%20theoretical%20guarantees.%20Experimental%20results%20on%20seven%20commonly%20used%0Adatasets%20%28i.e.%2C%20four%20text%20and%20three%20vision%29%20demonstrate%20the%20superiority%20of%0AFedPFT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11536v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedPFT%3A%20Federated%20Proxy%20Fine-Tuning%20of%20Foundation%20Models&entry.906535625=Zhaopeng%20Peng%20and%20Xiaoliang%20Fan%20and%20Yufan%20Chen%20and%20Zheng%20Wang%20and%20Shirui%20Pan%20and%20Chenglu%20Wen%20and%20Ruisheng%20Zhang%20and%20Cheng%20Wang&entry.1292438233=%20%20Adapting%20Foundation%20Models%20%28FMs%29%20for%20downstream%20tasks%20through%20Federated%0ALearning%20%28FL%29%20emerges%20a%20promising%20strategy%20for%20protecting%20data%20privacy%20and%0Avaluable%20FMs.%20Existing%20methods%20fine-tune%20FM%20by%20allocating%20sub-FM%20to%20clients%20in%0AFL%2C%20however%2C%20leading%20to%20suboptimal%20performance%20due%20to%20insufficient%20tuning%20and%0Ainevitable%20error%20accumulations%20of%20gradients.%20In%20this%20paper%2C%20we%20propose%0AFederated%20Proxy%20Fine-Tuning%20%28FedPFT%29%2C%20a%20novel%20method%20enhancing%20FMs%20adaptation%0Ain%20downstream%20tasks%20through%20FL%20by%20two%20key%20modules.%20First%2C%20the%20sub-FM%0Aconstruction%20module%20employs%20a%20layer-wise%20compression%20approach%2C%20facilitating%0Acomprehensive%20FM%20fine-tuning%20across%20all%20layers%20by%20emphasizing%20those%20crucial%0Aneurons.%20Second%2C%20the%20sub-FM%20alignment%20module%20conducts%20a%20two-step%0Adistillations-layer-level%20and%20neuron-level-before%20and%20during%20FL%20fine-tuning%0Arespectively%2C%20to%20reduce%20error%20of%20gradient%20by%20accurately%20aligning%20sub-FM%20with%20FM%0Aunder%20theoretical%20guarantees.%20Experimental%20results%20on%20seven%20commonly%20used%0Adatasets%20%28i.e.%2C%20four%20text%20and%20three%20vision%29%20demonstrate%20the%20superiority%20of%0AFedPFT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11536v1&entry.124074799=Read"},
{"title": "Simple Image Signal Processing using Global Context Guidance", "author": "Omar Elezabi and Marcos V. Conde and Radu Timofte", "abstract": "  In modern smartphone cameras, the Image Signal Processor (ISP) is the core\nelement that converts the RAW readings from the sensor into perceptually\npleasant RGB images for the end users. The ISP is typically proprietary and\nhandcrafted and consists of several blocks such as white balance, color\ncorrection, and tone mapping. Deep learning-based ISPs aim to transform RAW\nimages into DSLR-like RGB images using deep neural networks. However, most\nlearned ISPs are trained using patches (small regions) due to computational\nlimitations. Such methods lack global context, which limits their efficacy on\nfull-resolution images and harms their ability to capture global properties\nsuch as color constancy or illumination. First, we propose a novel module that\ncan be integrated into any neural ISP to capture the global context information\nfrom the full RAW images. Second, we propose an efficient and simple neural ISP\nthat utilizes our proposed module. Our model achieves state-of-the-art results\non different benchmarks using diverse and real smartphone images.\n", "link": "http://arxiv.org/abs/2404.11569v1", "date": "2024-04-17", "relevancy": 2.0077, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5426}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4985}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4891}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Simple%20Image%20Signal%20Processing%20using%20Global%20Context%20Guidance&body=Title%3A%20Simple%20Image%20Signal%20Processing%20using%20Global%20Context%20Guidance%0AAuthor%3A%20Omar%20Elezabi%20and%20Marcos%20V.%20Conde%20and%20Radu%20Timofte%0AAbstract%3A%20%20%20In%20modern%20smartphone%20cameras%2C%20the%20Image%20Signal%20Processor%20%28ISP%29%20is%20the%20core%0Aelement%20that%20converts%20the%20RAW%20readings%20from%20the%20sensor%20into%20perceptually%0Apleasant%20RGB%20images%20for%20the%20end%20users.%20The%20ISP%20is%20typically%20proprietary%20and%0Ahandcrafted%20and%20consists%20of%20several%20blocks%20such%20as%20white%20balance%2C%20color%0Acorrection%2C%20and%20tone%20mapping.%20Deep%20learning-based%20ISPs%20aim%20to%20transform%20RAW%0Aimages%20into%20DSLR-like%20RGB%20images%20using%20deep%20neural%20networks.%20However%2C%20most%0Alearned%20ISPs%20are%20trained%20using%20patches%20%28small%20regions%29%20due%20to%20computational%0Alimitations.%20Such%20methods%20lack%20global%20context%2C%20which%20limits%20their%20efficacy%20on%0Afull-resolution%20images%20and%20harms%20their%20ability%20to%20capture%20global%20properties%0Asuch%20as%20color%20constancy%20or%20illumination.%20First%2C%20we%20propose%20a%20novel%20module%20that%0Acan%20be%20integrated%20into%20any%20neural%20ISP%20to%20capture%20the%20global%20context%20information%0Afrom%20the%20full%20RAW%20images.%20Second%2C%20we%20propose%20an%20efficient%20and%20simple%20neural%20ISP%0Athat%20utilizes%20our%20proposed%20module.%20Our%20model%20achieves%20state-of-the-art%20results%0Aon%20different%20benchmarks%20using%20diverse%20and%20real%20smartphone%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11569v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20Image%20Signal%20Processing%20using%20Global%20Context%20Guidance&entry.906535625=Omar%20Elezabi%20and%20Marcos%20V.%20Conde%20and%20Radu%20Timofte&entry.1292438233=%20%20In%20modern%20smartphone%20cameras%2C%20the%20Image%20Signal%20Processor%20%28ISP%29%20is%20the%20core%0Aelement%20that%20converts%20the%20RAW%20readings%20from%20the%20sensor%20into%20perceptually%0Apleasant%20RGB%20images%20for%20the%20end%20users.%20The%20ISP%20is%20typically%20proprietary%20and%0Ahandcrafted%20and%20consists%20of%20several%20blocks%20such%20as%20white%20balance%2C%20color%0Acorrection%2C%20and%20tone%20mapping.%20Deep%20learning-based%20ISPs%20aim%20to%20transform%20RAW%0Aimages%20into%20DSLR-like%20RGB%20images%20using%20deep%20neural%20networks.%20However%2C%20most%0Alearned%20ISPs%20are%20trained%20using%20patches%20%28small%20regions%29%20due%20to%20computational%0Alimitations.%20Such%20methods%20lack%20global%20context%2C%20which%20limits%20their%20efficacy%20on%0Afull-resolution%20images%20and%20harms%20their%20ability%20to%20capture%20global%20properties%0Asuch%20as%20color%20constancy%20or%20illumination.%20First%2C%20we%20propose%20a%20novel%20module%20that%0Acan%20be%20integrated%20into%20any%20neural%20ISP%20to%20capture%20the%20global%20context%20information%0Afrom%20the%20full%20RAW%20images.%20Second%2C%20we%20propose%20an%20efficient%20and%20simple%20neural%20ISP%0Athat%20utilizes%20our%20proposed%20module.%20Our%20model%20achieves%20state-of-the-art%20results%0Aon%20different%20benchmarks%20using%20diverse%20and%20real%20smartphone%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11569v1&entry.124074799=Read"},
{"title": "One-Prompt to Segment All Medical Images", "author": "Junde Wu and Jiayuan Zhu and Yueming Jin and Min Xu", "abstract": "  Large foundation models, known for their strong zero-shot generalization,\nhave excelled in visual and language applications. However, applying them to\nmedical image segmentation, a domain with diverse imaging types and target\nlabels, remains an open challenge. Current approaches, such as adapting\ninteractive segmentation models like Segment Anything Model (SAM), require user\nprompts for each sample during inference. Alternatively, transfer learning\nmethods like few/one-shot models demand labeled samples, leading to high costs.\nThis paper introduces a new paradigm toward the universal medical image\nsegmentation, termed 'One-Prompt Segmentation.' One-Prompt Segmentation\ncombines the strengths of one-shot and interactive methods. In the inference\nstage, with just \\textbf{one prompted sample}, it can adeptly handle the unseen\ntask in a single forward pass. We train One-Prompt Model on 64 open-source\nmedical datasets, accompanied by the collection of over 3,000 clinician-labeled\nprompts. Tested on 14 previously unseen datasets, the One-Prompt Model\nshowcases superior zero-shot segmentation capabilities, outperforming a wide\nrange of related methods. The code and data is released as\nhttps://github.com/KidsWithTokens/one-prompt.\n", "link": "http://arxiv.org/abs/2305.10300v5", "date": "2024-04-17", "relevancy": 2.0068, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5299}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5096}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4825}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20One-Prompt%20to%20Segment%20All%20Medical%20Images&body=Title%3A%20One-Prompt%20to%20Segment%20All%20Medical%20Images%0AAuthor%3A%20Junde%20Wu%20and%20Jiayuan%20Zhu%20and%20Yueming%20Jin%20and%20Min%20Xu%0AAbstract%3A%20%20%20Large%20foundation%20models%2C%20known%20for%20their%20strong%20zero-shot%20generalization%2C%0Ahave%20excelled%20in%20visual%20and%20language%20applications.%20However%2C%20applying%20them%20to%0Amedical%20image%20segmentation%2C%20a%20domain%20with%20diverse%20imaging%20types%20and%20target%0Alabels%2C%20remains%20an%20open%20challenge.%20Current%20approaches%2C%20such%20as%20adapting%0Ainteractive%20segmentation%20models%20like%20Segment%20Anything%20Model%20%28SAM%29%2C%20require%20user%0Aprompts%20for%20each%20sample%20during%20inference.%20Alternatively%2C%20transfer%20learning%0Amethods%20like%20few/one-shot%20models%20demand%20labeled%20samples%2C%20leading%20to%20high%20costs.%0AThis%20paper%20introduces%20a%20new%20paradigm%20toward%20the%20universal%20medical%20image%0Asegmentation%2C%20termed%20%27One-Prompt%20Segmentation.%27%20One-Prompt%20Segmentation%0Acombines%20the%20strengths%20of%20one-shot%20and%20interactive%20methods.%20In%20the%20inference%0Astage%2C%20with%20just%20%5Ctextbf%7Bone%20prompted%20sample%7D%2C%20it%20can%20adeptly%20handle%20the%20unseen%0Atask%20in%20a%20single%20forward%20pass.%20We%20train%20One-Prompt%20Model%20on%2064%20open-source%0Amedical%20datasets%2C%20accompanied%20by%20the%20collection%20of%20over%203%2C000%20clinician-labeled%0Aprompts.%20Tested%20on%2014%20previously%20unseen%20datasets%2C%20the%20One-Prompt%20Model%0Ashowcases%20superior%20zero-shot%20segmentation%20capabilities%2C%20outperforming%20a%20wide%0Arange%20of%20related%20methods.%20The%20code%20and%20data%20is%20released%20as%0Ahttps%3A//github.com/KidsWithTokens/one-prompt.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.10300v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Prompt%20to%20Segment%20All%20Medical%20Images&entry.906535625=Junde%20Wu%20and%20Jiayuan%20Zhu%20and%20Yueming%20Jin%20and%20Min%20Xu&entry.1292438233=%20%20Large%20foundation%20models%2C%20known%20for%20their%20strong%20zero-shot%20generalization%2C%0Ahave%20excelled%20in%20visual%20and%20language%20applications.%20However%2C%20applying%20them%20to%0Amedical%20image%20segmentation%2C%20a%20domain%20with%20diverse%20imaging%20types%20and%20target%0Alabels%2C%20remains%20an%20open%20challenge.%20Current%20approaches%2C%20such%20as%20adapting%0Ainteractive%20segmentation%20models%20like%20Segment%20Anything%20Model%20%28SAM%29%2C%20require%20user%0Aprompts%20for%20each%20sample%20during%20inference.%20Alternatively%2C%20transfer%20learning%0Amethods%20like%20few/one-shot%20models%20demand%20labeled%20samples%2C%20leading%20to%20high%20costs.%0AThis%20paper%20introduces%20a%20new%20paradigm%20toward%20the%20universal%20medical%20image%0Asegmentation%2C%20termed%20%27One-Prompt%20Segmentation.%27%20One-Prompt%20Segmentation%0Acombines%20the%20strengths%20of%20one-shot%20and%20interactive%20methods.%20In%20the%20inference%0Astage%2C%20with%20just%20%5Ctextbf%7Bone%20prompted%20sample%7D%2C%20it%20can%20adeptly%20handle%20the%20unseen%0Atask%20in%20a%20single%20forward%20pass.%20We%20train%20One-Prompt%20Model%20on%2064%20open-source%0Amedical%20datasets%2C%20accompanied%20by%20the%20collection%20of%20over%203%2C000%20clinician-labeled%0Aprompts.%20Tested%20on%2014%20previously%20unseen%20datasets%2C%20the%20One-Prompt%20Model%0Ashowcases%20superior%20zero-shot%20segmentation%20capabilities%2C%20outperforming%20a%20wide%0Arange%20of%20related%20methods.%20The%20code%20and%20data%20is%20released%20as%0Ahttps%3A//github.com/KidsWithTokens/one-prompt.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.10300v5&entry.124074799=Read"},
{"title": "ONOT: a High-Quality ICAO-compliant Synthetic Mugshot Dataset", "author": "Nicol\u00f2 Di Domenico and Guido Borghi and Annalisa Franco and Davide Maltoni", "abstract": "  Nowadays, state-of-the-art AI-based generative models represent a viable\nsolution to overcome privacy issues and biases in the collection of datasets\ncontaining personal information, such as faces. Following this intuition, in\nthis paper we introduce ONOT, a synthetic dataset specifically focused on the\ngeneration of high-quality faces in adherence to the requirements of the\nISO/IEC 39794-5 standards that, following the guidelines of the International\nCivil Aviation Organization (ICAO), defines the interchange formats of face\nimages in electronic Machine-Readable Travel Documents (eMRTD). The strictly\ncontrolled and varied mugshot images included in ONOT are useful in research\nfields related to the analysis of face images in eMRTD, such as Morphing Attack\nDetection and Face Quality Assessment. The dataset is publicly released, in\ncombination with the generation procedure details in order to improve the\nreproducibility and enable future extensions.\n", "link": "http://arxiv.org/abs/2404.11236v1", "date": "2024-04-17", "relevancy": 2.0052, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5131}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5019}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4892}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ONOT%3A%20a%20High-Quality%20ICAO-compliant%20Synthetic%20Mugshot%20Dataset&body=Title%3A%20ONOT%3A%20a%20High-Quality%20ICAO-compliant%20Synthetic%20Mugshot%20Dataset%0AAuthor%3A%20Nicol%C3%B2%20Di%20Domenico%20and%20Guido%20Borghi%20and%20Annalisa%20Franco%20and%20Davide%20Maltoni%0AAbstract%3A%20%20%20Nowadays%2C%20state-of-the-art%20AI-based%20generative%20models%20represent%20a%20viable%0Asolution%20to%20overcome%20privacy%20issues%20and%20biases%20in%20the%20collection%20of%20datasets%0Acontaining%20personal%20information%2C%20such%20as%20faces.%20Following%20this%20intuition%2C%20in%0Athis%20paper%20we%20introduce%20ONOT%2C%20a%20synthetic%20dataset%20specifically%20focused%20on%20the%0Ageneration%20of%20high-quality%20faces%20in%20adherence%20to%20the%20requirements%20of%20the%0AISO/IEC%2039794-5%20standards%20that%2C%20following%20the%20guidelines%20of%20the%20International%0ACivil%20Aviation%20Organization%20%28ICAO%29%2C%20defines%20the%20interchange%20formats%20of%20face%0Aimages%20in%20electronic%20Machine-Readable%20Travel%20Documents%20%28eMRTD%29.%20The%20strictly%0Acontrolled%20and%20varied%20mugshot%20images%20included%20in%20ONOT%20are%20useful%20in%20research%0Afields%20related%20to%20the%20analysis%20of%20face%20images%20in%20eMRTD%2C%20such%20as%20Morphing%20Attack%0ADetection%20and%20Face%20Quality%20Assessment.%20The%20dataset%20is%20publicly%20released%2C%20in%0Acombination%20with%20the%20generation%20procedure%20details%20in%20order%20to%20improve%20the%0Areproducibility%20and%20enable%20future%20extensions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11236v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ONOT%3A%20a%20High-Quality%20ICAO-compliant%20Synthetic%20Mugshot%20Dataset&entry.906535625=Nicol%C3%B2%20Di%20Domenico%20and%20Guido%20Borghi%20and%20Annalisa%20Franco%20and%20Davide%20Maltoni&entry.1292438233=%20%20Nowadays%2C%20state-of-the-art%20AI-based%20generative%20models%20represent%20a%20viable%0Asolution%20to%20overcome%20privacy%20issues%20and%20biases%20in%20the%20collection%20of%20datasets%0Acontaining%20personal%20information%2C%20such%20as%20faces.%20Following%20this%20intuition%2C%20in%0Athis%20paper%20we%20introduce%20ONOT%2C%20a%20synthetic%20dataset%20specifically%20focused%20on%20the%0Ageneration%20of%20high-quality%20faces%20in%20adherence%20to%20the%20requirements%20of%20the%0AISO/IEC%2039794-5%20standards%20that%2C%20following%20the%20guidelines%20of%20the%20International%0ACivil%20Aviation%20Organization%20%28ICAO%29%2C%20defines%20the%20interchange%20formats%20of%20face%0Aimages%20in%20electronic%20Machine-Readable%20Travel%20Documents%20%28eMRTD%29.%20The%20strictly%0Acontrolled%20and%20varied%20mugshot%20images%20included%20in%20ONOT%20are%20useful%20in%20research%0Afields%20related%20to%20the%20analysis%20of%20face%20images%20in%20eMRTD%2C%20such%20as%20Morphing%20Attack%0ADetection%20and%20Face%20Quality%20Assessment.%20The%20dataset%20is%20publicly%20released%2C%20in%0Acombination%20with%20the%20generation%20procedure%20details%20in%20order%20to%20improve%20the%0Areproducibility%20and%20enable%20future%20extensions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11236v1&entry.124074799=Read"},
{"title": "On the Scalability of GNNs for Molecular Graphs", "author": "Maciej Sypetkowski and Frederik Wenkel and Farimah Poursafaei and Nia Dickson and Karush Suri and Philip Fradkin and Dominique Beaini", "abstract": "  Scaling deep learning models has been at the heart of recent revolutions in\nlanguage modelling and image generation. Practitioners have observed a strong\nrelationship between model size, dataset size, and performance. However,\nstructure-based architectures such as Graph Neural Networks (GNNs) are yet to\nshow the benefits of scale mainly due to the lower efficiency of sparse\noperations, large data requirements, and lack of clarity about the\neffectiveness of various architectures. We address this drawback of GNNs by\nstudying their scaling behavior. Specifically, we analyze message-passing\nnetworks, graph Transformers, and hybrid architectures on the largest public\ncollection of 2D molecular graphs. For the first time, we observe that GNNs\nbenefit tremendously from the increasing scale of depth, width, number of\nmolecules, number of labels, and the diversity in the pretraining datasets,\nresulting in a 30.25% improvement when scaling to 1 billion parameters and\n28.98% improvement when increasing size of dataset to eightfold. We further\ndemonstrate strong finetuning scaling behavior on 38 tasks, outclassing\nprevious large models. We hope that our work paves the way for an era where\nfoundational GNNs drive pharmaceutical drug discovery.\n", "link": "http://arxiv.org/abs/2404.11568v1", "date": "2024-04-17", "relevancy": 1.9939, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5325}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5045}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4789}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20the%20Scalability%20of%20GNNs%20for%20Molecular%20Graphs&body=Title%3A%20On%20the%20Scalability%20of%20GNNs%20for%20Molecular%20Graphs%0AAuthor%3A%20Maciej%20Sypetkowski%20and%20Frederik%20Wenkel%20and%20Farimah%20Poursafaei%20and%20Nia%20Dickson%20and%20Karush%20Suri%20and%20Philip%20Fradkin%20and%20Dominique%20Beaini%0AAbstract%3A%20%20%20Scaling%20deep%20learning%20models%20has%20been%20at%20the%20heart%20of%20recent%20revolutions%20in%0Alanguage%20modelling%20and%20image%20generation.%20Practitioners%20have%20observed%20a%20strong%0Arelationship%20between%20model%20size%2C%20dataset%20size%2C%20and%20performance.%20However%2C%0Astructure-based%20architectures%20such%20as%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20yet%20to%0Ashow%20the%20benefits%20of%20scale%20mainly%20due%20to%20the%20lower%20efficiency%20of%20sparse%0Aoperations%2C%20large%20data%20requirements%2C%20and%20lack%20of%20clarity%20about%20the%0Aeffectiveness%20of%20various%20architectures.%20We%20address%20this%20drawback%20of%20GNNs%20by%0Astudying%20their%20scaling%20behavior.%20Specifically%2C%20we%20analyze%20message-passing%0Anetworks%2C%20graph%20Transformers%2C%20and%20hybrid%20architectures%20on%20the%20largest%20public%0Acollection%20of%202D%20molecular%20graphs.%20For%20the%20first%20time%2C%20we%20observe%20that%20GNNs%0Abenefit%20tremendously%20from%20the%20increasing%20scale%20of%20depth%2C%20width%2C%20number%20of%0Amolecules%2C%20number%20of%20labels%2C%20and%20the%20diversity%20in%20the%20pretraining%20datasets%2C%0Aresulting%20in%20a%2030.25%25%20improvement%20when%20scaling%20to%201%20billion%20parameters%20and%0A28.98%25%20improvement%20when%20increasing%20size%20of%20dataset%20to%20eightfold.%20We%20further%0Ademonstrate%20strong%20finetuning%20scaling%20behavior%20on%2038%20tasks%2C%20outclassing%0Aprevious%20large%20models.%20We%20hope%20that%20our%20work%20paves%20the%20way%20for%20an%20era%20where%0Afoundational%20GNNs%20drive%20pharmaceutical%20drug%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11568v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Scalability%20of%20GNNs%20for%20Molecular%20Graphs&entry.906535625=Maciej%20Sypetkowski%20and%20Frederik%20Wenkel%20and%20Farimah%20Poursafaei%20and%20Nia%20Dickson%20and%20Karush%20Suri%20and%20Philip%20Fradkin%20and%20Dominique%20Beaini&entry.1292438233=%20%20Scaling%20deep%20learning%20models%20has%20been%20at%20the%20heart%20of%20recent%20revolutions%20in%0Alanguage%20modelling%20and%20image%20generation.%20Practitioners%20have%20observed%20a%20strong%0Arelationship%20between%20model%20size%2C%20dataset%20size%2C%20and%20performance.%20However%2C%0Astructure-based%20architectures%20such%20as%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20yet%20to%0Ashow%20the%20benefits%20of%20scale%20mainly%20due%20to%20the%20lower%20efficiency%20of%20sparse%0Aoperations%2C%20large%20data%20requirements%2C%20and%20lack%20of%20clarity%20about%20the%0Aeffectiveness%20of%20various%20architectures.%20We%20address%20this%20drawback%20of%20GNNs%20by%0Astudying%20their%20scaling%20behavior.%20Specifically%2C%20we%20analyze%20message-passing%0Anetworks%2C%20graph%20Transformers%2C%20and%20hybrid%20architectures%20on%20the%20largest%20public%0Acollection%20of%202D%20molecular%20graphs.%20For%20the%20first%20time%2C%20we%20observe%20that%20GNNs%0Abenefit%20tremendously%20from%20the%20increasing%20scale%20of%20depth%2C%20width%2C%20number%20of%0Amolecules%2C%20number%20of%20labels%2C%20and%20the%20diversity%20in%20the%20pretraining%20datasets%2C%0Aresulting%20in%20a%2030.25%25%20improvement%20when%20scaling%20to%201%20billion%20parameters%20and%0A28.98%25%20improvement%20when%20increasing%20size%20of%20dataset%20to%20eightfold.%20We%20further%0Ademonstrate%20strong%20finetuning%20scaling%20behavior%20on%2038%20tasks%2C%20outclassing%0Aprevious%20large%20models.%20We%20hope%20that%20our%20work%20paves%20the%20way%20for%20an%20era%20where%0Afoundational%20GNNs%20drive%20pharmaceutical%20drug%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11568v1&entry.124074799=Read"},
{"title": "SoccerNet Game State Reconstruction: End-to-End Athlete Tracking and\n  Identification on a Minimap", "author": "Vladimir Somers and Victor Joos and Anthony Cioppa and Silvio Giancola and Seyed Abolfazl Ghasemzadeh and Floriane Magera and Baptiste Standaert and Amir Mohammad Mansourian and Xin Zhou and Shohreh Kasaei and Bernard Ghanem and Alexandre Alahi and Marc Van Droogenbroeck and Christophe De Vleeschouwer", "abstract": "  Tracking and identifying athletes on the pitch holds a central role in\ncollecting essential insights from the game, such as estimating the total\ndistance covered by players or understanding team tactics. This tracking and\nidentification process is crucial for reconstructing the game state, defined by\nthe athletes' positions and identities on a 2D top-view of the pitch, (i.e. a\nminimap). However, reconstructing the game state from videos captured by a\nsingle camera is challenging. It requires understanding the position of the\nathletes and the viewpoint of the camera to localize and identify players\nwithin the field. In this work, we formalize the task of Game State\nReconstruction and introduce SoccerNet-GSR, a novel Game State Reconstruction\ndataset focusing on football videos. SoccerNet-GSR is composed of 200 video\nsequences of 30 seconds, annotated with 9.37 million line points for pitch\nlocalization and camera calibration, as well as over 2.36 million athlete\npositions on the pitch with their respective role, team, and jersey number.\nFurthermore, we introduce GS-HOTA, a novel metric to evaluate game state\nreconstruction methods. Finally, we propose and release an end-to-end baseline\nfor game state reconstruction, bootstrapping the research on this task. Our\nexperiments show that GSR is a challenging novel task, which opens the field\nfor future research. Our dataset and codebase are publicly available at\nhttps://github.com/SoccerNet/sn-gamestate.\n", "link": "http://arxiv.org/abs/2404.11335v1", "date": "2024-04-17", "relevancy": 1.9899, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5095}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4987}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.485}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SoccerNet%20Game%20State%20Reconstruction%3A%20End-to-End%20Athlete%20Tracking%20and%0A%20%20Identification%20on%20a%20Minimap&body=Title%3A%20SoccerNet%20Game%20State%20Reconstruction%3A%20End-to-End%20Athlete%20Tracking%20and%0A%20%20Identification%20on%20a%20Minimap%0AAuthor%3A%20Vladimir%20Somers%20and%20Victor%20Joos%20and%20Anthony%20Cioppa%20and%20Silvio%20Giancola%20and%20Seyed%20Abolfazl%20Ghasemzadeh%20and%20Floriane%20Magera%20and%20Baptiste%20Standaert%20and%20Amir%20Mohammad%20Mansourian%20and%20Xin%20Zhou%20and%20Shohreh%20Kasaei%20and%20Bernard%20Ghanem%20and%20Alexandre%20Alahi%20and%20Marc%20Van%20Droogenbroeck%20and%20Christophe%20De%20Vleeschouwer%0AAbstract%3A%20%20%20Tracking%20and%20identifying%20athletes%20on%20the%20pitch%20holds%20a%20central%20role%20in%0Acollecting%20essential%20insights%20from%20the%20game%2C%20such%20as%20estimating%20the%20total%0Adistance%20covered%20by%20players%20or%20understanding%20team%20tactics.%20This%20tracking%20and%0Aidentification%20process%20is%20crucial%20for%20reconstructing%20the%20game%20state%2C%20defined%20by%0Athe%20athletes%27%20positions%20and%20identities%20on%20a%202D%20top-view%20of%20the%20pitch%2C%20%28i.e.%20a%0Aminimap%29.%20However%2C%20reconstructing%20the%20game%20state%20from%20videos%20captured%20by%20a%0Asingle%20camera%20is%20challenging.%20It%20requires%20understanding%20the%20position%20of%20the%0Aathletes%20and%20the%20viewpoint%20of%20the%20camera%20to%20localize%20and%20identify%20players%0Awithin%20the%20field.%20In%20this%20work%2C%20we%20formalize%20the%20task%20of%20Game%20State%0AReconstruction%20and%20introduce%20SoccerNet-GSR%2C%20a%20novel%20Game%20State%20Reconstruction%0Adataset%20focusing%20on%20football%20videos.%20SoccerNet-GSR%20is%20composed%20of%20200%20video%0Asequences%20of%2030%20seconds%2C%20annotated%20with%209.37%20million%20line%20points%20for%20pitch%0Alocalization%20and%20camera%20calibration%2C%20as%20well%20as%20over%202.36%20million%20athlete%0Apositions%20on%20the%20pitch%20with%20their%20respective%20role%2C%20team%2C%20and%20jersey%20number.%0AFurthermore%2C%20we%20introduce%20GS-HOTA%2C%20a%20novel%20metric%20to%20evaluate%20game%20state%0Areconstruction%20methods.%20Finally%2C%20we%20propose%20and%20release%20an%20end-to-end%20baseline%0Afor%20game%20state%20reconstruction%2C%20bootstrapping%20the%20research%20on%20this%20task.%20Our%0Aexperiments%20show%20that%20GSR%20is%20a%20challenging%20novel%20task%2C%20which%20opens%20the%20field%0Afor%20future%20research.%20Our%20dataset%20and%20codebase%20are%20publicly%20available%20at%0Ahttps%3A//github.com/SoccerNet/sn-gamestate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11335v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoccerNet%20Game%20State%20Reconstruction%3A%20End-to-End%20Athlete%20Tracking%20and%0A%20%20Identification%20on%20a%20Minimap&entry.906535625=Vladimir%20Somers%20and%20Victor%20Joos%20and%20Anthony%20Cioppa%20and%20Silvio%20Giancola%20and%20Seyed%20Abolfazl%20Ghasemzadeh%20and%20Floriane%20Magera%20and%20Baptiste%20Standaert%20and%20Amir%20Mohammad%20Mansourian%20and%20Xin%20Zhou%20and%20Shohreh%20Kasaei%20and%20Bernard%20Ghanem%20and%20Alexandre%20Alahi%20and%20Marc%20Van%20Droogenbroeck%20and%20Christophe%20De%20Vleeschouwer&entry.1292438233=%20%20Tracking%20and%20identifying%20athletes%20on%20the%20pitch%20holds%20a%20central%20role%20in%0Acollecting%20essential%20insights%20from%20the%20game%2C%20such%20as%20estimating%20the%20total%0Adistance%20covered%20by%20players%20or%20understanding%20team%20tactics.%20This%20tracking%20and%0Aidentification%20process%20is%20crucial%20for%20reconstructing%20the%20game%20state%2C%20defined%20by%0Athe%20athletes%27%20positions%20and%20identities%20on%20a%202D%20top-view%20of%20the%20pitch%2C%20%28i.e.%20a%0Aminimap%29.%20However%2C%20reconstructing%20the%20game%20state%20from%20videos%20captured%20by%20a%0Asingle%20camera%20is%20challenging.%20It%20requires%20understanding%20the%20position%20of%20the%0Aathletes%20and%20the%20viewpoint%20of%20the%20camera%20to%20localize%20and%20identify%20players%0Awithin%20the%20field.%20In%20this%20work%2C%20we%20formalize%20the%20task%20of%20Game%20State%0AReconstruction%20and%20introduce%20SoccerNet-GSR%2C%20a%20novel%20Game%20State%20Reconstruction%0Adataset%20focusing%20on%20football%20videos.%20SoccerNet-GSR%20is%20composed%20of%20200%20video%0Asequences%20of%2030%20seconds%2C%20annotated%20with%209.37%20million%20line%20points%20for%20pitch%0Alocalization%20and%20camera%20calibration%2C%20as%20well%20as%20over%202.36%20million%20athlete%0Apositions%20on%20the%20pitch%20with%20their%20respective%20role%2C%20team%2C%20and%20jersey%20number.%0AFurthermore%2C%20we%20introduce%20GS-HOTA%2C%20a%20novel%20metric%20to%20evaluate%20game%20state%0Areconstruction%20methods.%20Finally%2C%20we%20propose%20and%20release%20an%20end-to-end%20baseline%0Afor%20game%20state%20reconstruction%2C%20bootstrapping%20the%20research%20on%20this%20task.%20Our%0Aexperiments%20show%20that%20GSR%20is%20a%20challenging%20novel%20task%2C%20which%20opens%20the%20field%0Afor%20future%20research.%20Our%20dataset%20and%20codebase%20are%20publicly%20available%20at%0Ahttps%3A//github.com/SoccerNet/sn-gamestate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11335v1&entry.124074799=Read"},
{"title": "Consisaug: A Consistency-based Augmentation for Polyp Detection in\n  Endoscopy Image Analysis", "author": "Ziyu Zhou and Wenyuan Shen and Chang Liu", "abstract": "  Colorectal cancer (CRC), which frequently originates from initially benign\npolyps, remains a significant contributor to global cancer-related mortality.\nEarly and accurate detection of these polyps via colonoscopy is crucial for CRC\nprevention. However, traditional colonoscopy methods depend heavily on the\noperator's experience, leading to suboptimal polyp detection rates. Besides,\nthe public database are limited in polyp size and shape diversity. To enhance\nthe available data for polyp detection, we introduce Consisaug, an innovative\nand effective methodology to augment data that leverages deep learning. We\nutilize the constraint that when the image is flipped the class label should be\nequal and the bonding boxes should be consistent. We implement our Consisaug on\nfive public polyp datasets and at three backbones, and the results show the\neffectiveness of our method.\n", "link": "http://arxiv.org/abs/2404.11355v1", "date": "2024-04-17", "relevancy": 1.982, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5159}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4948}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4881}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Consisaug%3A%20A%20Consistency-based%20Augmentation%20for%20Polyp%20Detection%20in%0A%20%20Endoscopy%20Image%20Analysis&body=Title%3A%20Consisaug%3A%20A%20Consistency-based%20Augmentation%20for%20Polyp%20Detection%20in%0A%20%20Endoscopy%20Image%20Analysis%0AAuthor%3A%20Ziyu%20Zhou%20and%20Wenyuan%20Shen%20and%20Chang%20Liu%0AAbstract%3A%20%20%20Colorectal%20cancer%20%28CRC%29%2C%20which%20frequently%20originates%20from%20initially%20benign%0Apolyps%2C%20remains%20a%20significant%20contributor%20to%20global%20cancer-related%20mortality.%0AEarly%20and%20accurate%20detection%20of%20these%20polyps%20via%20colonoscopy%20is%20crucial%20for%20CRC%0Aprevention.%20However%2C%20traditional%20colonoscopy%20methods%20depend%20heavily%20on%20the%0Aoperator%27s%20experience%2C%20leading%20to%20suboptimal%20polyp%20detection%20rates.%20Besides%2C%0Athe%20public%20database%20are%20limited%20in%20polyp%20size%20and%20shape%20diversity.%20To%20enhance%0Athe%20available%20data%20for%20polyp%20detection%2C%20we%20introduce%20Consisaug%2C%20an%20innovative%0Aand%20effective%20methodology%20to%20augment%20data%20that%20leverages%20deep%20learning.%20We%0Autilize%20the%20constraint%20that%20when%20the%20image%20is%20flipped%20the%20class%20label%20should%20be%0Aequal%20and%20the%20bonding%20boxes%20should%20be%20consistent.%20We%20implement%20our%20Consisaug%20on%0Afive%20public%20polyp%20datasets%20and%20at%20three%20backbones%2C%20and%20the%20results%20show%20the%0Aeffectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11355v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consisaug%3A%20A%20Consistency-based%20Augmentation%20for%20Polyp%20Detection%20in%0A%20%20Endoscopy%20Image%20Analysis&entry.906535625=Ziyu%20Zhou%20and%20Wenyuan%20Shen%20and%20Chang%20Liu&entry.1292438233=%20%20Colorectal%20cancer%20%28CRC%29%2C%20which%20frequently%20originates%20from%20initially%20benign%0Apolyps%2C%20remains%20a%20significant%20contributor%20to%20global%20cancer-related%20mortality.%0AEarly%20and%20accurate%20detection%20of%20these%20polyps%20via%20colonoscopy%20is%20crucial%20for%20CRC%0Aprevention.%20However%2C%20traditional%20colonoscopy%20methods%20depend%20heavily%20on%20the%0Aoperator%27s%20experience%2C%20leading%20to%20suboptimal%20polyp%20detection%20rates.%20Besides%2C%0Athe%20public%20database%20are%20limited%20in%20polyp%20size%20and%20shape%20diversity.%20To%20enhance%0Athe%20available%20data%20for%20polyp%20detection%2C%20we%20introduce%20Consisaug%2C%20an%20innovative%0Aand%20effective%20methodology%20to%20augment%20data%20that%20leverages%20deep%20learning.%20We%0Autilize%20the%20constraint%20that%20when%20the%20image%20is%20flipped%20the%20class%20label%20should%20be%0Aequal%20and%20the%20bonding%20boxes%20should%20be%20consistent.%20We%20implement%20our%20Consisaug%20on%0Afive%20public%20polyp%20datasets%20and%20at%20three%20backbones%2C%20and%20the%20results%20show%20the%0Aeffectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11355v1&entry.124074799=Read"},
{"title": "A Data-Driven Representation for Sign Language Production", "author": "Harry Walsh and Abolfazl Ravanshad and Mariam Rahmani and Richard Bowden", "abstract": "  Phonetic representations are used when recording spoken languages, but no\nequivalent exists for recording signed languages. As a result, linguists have\nproposed several annotation systems that operate on the gloss or sub-unit\nlevel; however, these resources are notably irregular and scarce.\n  Sign Language Production (SLP) aims to automatically translate spoken\nlanguage sentences into continuous sequences of sign language. However, current\nstate-of-the-art approaches rely on scarce linguistic resources to work. This\nhas limited progress in the field. This paper introduces an innovative solution\nby transforming the continuous pose generation problem into a discrete sequence\ngeneration problem. Thus, overcoming the need for costly annotation. Although,\nif available, we leverage the additional information to enhance our approach.\n  By applying Vector Quantisation (VQ) to sign language data, we first learn a\ncodebook of short motions that can be combined to create a natural sequence of\nsign. Where each token in the codebook can be thought of as the lexicon of our\nrepresentation. Then using a transformer we perform a translation from spoken\nlanguage text to a sequence of codebook tokens. Each token can be directly\nmapped to a sequence of poses allowing the translation to be performed by a\nsingle network. Furthermore, we present a sign stitching method to effectively\njoin tokens together. We evaluate on the RWTH-PHOENIX-Weather-2014T\n(PHOENIX14T) and the more challenging Meine DGS Annotated (mDGS) datasets. An\nextensive evaluation shows our approach outperforms previous methods,\nincreasing the BLEU-1 back translation score by up to 72%.\n", "link": "http://arxiv.org/abs/2404.11499v1", "date": "2024-04-17", "relevancy": 1.9555, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4975}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4973}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.477}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Data-Driven%20Representation%20for%20Sign%20Language%20Production&body=Title%3A%20A%20Data-Driven%20Representation%20for%20Sign%20Language%20Production%0AAuthor%3A%20Harry%20Walsh%20and%20Abolfazl%20Ravanshad%20and%20Mariam%20Rahmani%20and%20Richard%20Bowden%0AAbstract%3A%20%20%20Phonetic%20representations%20are%20used%20when%20recording%20spoken%20languages%2C%20but%20no%0Aequivalent%20exists%20for%20recording%20signed%20languages.%20As%20a%20result%2C%20linguists%20have%0Aproposed%20several%20annotation%20systems%20that%20operate%20on%20the%20gloss%20or%20sub-unit%0Alevel%3B%20however%2C%20these%20resources%20are%20notably%20irregular%20and%20scarce.%0A%20%20Sign%20Language%20Production%20%28SLP%29%20aims%20to%20automatically%20translate%20spoken%0Alanguage%20sentences%20into%20continuous%20sequences%20of%20sign%20language.%20However%2C%20current%0Astate-of-the-art%20approaches%20rely%20on%20scarce%20linguistic%20resources%20to%20work.%20This%0Ahas%20limited%20progress%20in%20the%20field.%20This%20paper%20introduces%20an%20innovative%20solution%0Aby%20transforming%20the%20continuous%20pose%20generation%20problem%20into%20a%20discrete%20sequence%0Ageneration%20problem.%20Thus%2C%20overcoming%20the%20need%20for%20costly%20annotation.%20Although%2C%0Aif%20available%2C%20we%20leverage%20the%20additional%20information%20to%20enhance%20our%20approach.%0A%20%20By%20applying%20Vector%20Quantisation%20%28VQ%29%20to%20sign%20language%20data%2C%20we%20first%20learn%20a%0Acodebook%20of%20short%20motions%20that%20can%20be%20combined%20to%20create%20a%20natural%20sequence%20of%0Asign.%20Where%20each%20token%20in%20the%20codebook%20can%20be%20thought%20of%20as%20the%20lexicon%20of%20our%0Arepresentation.%20Then%20using%20a%20transformer%20we%20perform%20a%20translation%20from%20spoken%0Alanguage%20text%20to%20a%20sequence%20of%20codebook%20tokens.%20Each%20token%20can%20be%20directly%0Amapped%20to%20a%20sequence%20of%20poses%20allowing%20the%20translation%20to%20be%20performed%20by%20a%0Asingle%20network.%20Furthermore%2C%20we%20present%20a%20sign%20stitching%20method%20to%20effectively%0Ajoin%20tokens%20together.%20We%20evaluate%20on%20the%20RWTH-PHOENIX-Weather-2014T%0A%28PHOENIX14T%29%20and%20the%20more%20challenging%20Meine%20DGS%20Annotated%20%28mDGS%29%20datasets.%20An%0Aextensive%20evaluation%20shows%20our%20approach%20outperforms%20previous%20methods%2C%0Aincreasing%20the%20BLEU-1%20back%20translation%20score%20by%20up%20to%2072%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11499v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Data-Driven%20Representation%20for%20Sign%20Language%20Production&entry.906535625=Harry%20Walsh%20and%20Abolfazl%20Ravanshad%20and%20Mariam%20Rahmani%20and%20Richard%20Bowden&entry.1292438233=%20%20Phonetic%20representations%20are%20used%20when%20recording%20spoken%20languages%2C%20but%20no%0Aequivalent%20exists%20for%20recording%20signed%20languages.%20As%20a%20result%2C%20linguists%20have%0Aproposed%20several%20annotation%20systems%20that%20operate%20on%20the%20gloss%20or%20sub-unit%0Alevel%3B%20however%2C%20these%20resources%20are%20notably%20irregular%20and%20scarce.%0A%20%20Sign%20Language%20Production%20%28SLP%29%20aims%20to%20automatically%20translate%20spoken%0Alanguage%20sentences%20into%20continuous%20sequences%20of%20sign%20language.%20However%2C%20current%0Astate-of-the-art%20approaches%20rely%20on%20scarce%20linguistic%20resources%20to%20work.%20This%0Ahas%20limited%20progress%20in%20the%20field.%20This%20paper%20introduces%20an%20innovative%20solution%0Aby%20transforming%20the%20continuous%20pose%20generation%20problem%20into%20a%20discrete%20sequence%0Ageneration%20problem.%20Thus%2C%20overcoming%20the%20need%20for%20costly%20annotation.%20Although%2C%0Aif%20available%2C%20we%20leverage%20the%20additional%20information%20to%20enhance%20our%20approach.%0A%20%20By%20applying%20Vector%20Quantisation%20%28VQ%29%20to%20sign%20language%20data%2C%20we%20first%20learn%20a%0Acodebook%20of%20short%20motions%20that%20can%20be%20combined%20to%20create%20a%20natural%20sequence%20of%0Asign.%20Where%20each%20token%20in%20the%20codebook%20can%20be%20thought%20of%20as%20the%20lexicon%20of%20our%0Arepresentation.%20Then%20using%20a%20transformer%20we%20perform%20a%20translation%20from%20spoken%0Alanguage%20text%20to%20a%20sequence%20of%20codebook%20tokens.%20Each%20token%20can%20be%20directly%0Amapped%20to%20a%20sequence%20of%20poses%20allowing%20the%20translation%20to%20be%20performed%20by%20a%0Asingle%20network.%20Furthermore%2C%20we%20present%20a%20sign%20stitching%20method%20to%20effectively%0Ajoin%20tokens%20together.%20We%20evaluate%20on%20the%20RWTH-PHOENIX-Weather-2014T%0A%28PHOENIX14T%29%20and%20the%20more%20challenging%20Meine%20DGS%20Annotated%20%28mDGS%29%20datasets.%20An%0Aextensive%20evaluation%20shows%20our%20approach%20outperforms%20previous%20methods%2C%0Aincreasing%20the%20BLEU-1%20back%20translation%20score%20by%20up%20to%2072%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11499v1&entry.124074799=Read"},
{"title": "Do Counterfactual Examples Complicate Adversarial Training?", "author": "Eric Yeats and Cameron Darwin and Eduardo Ortega and Frank Liu and Hai Li", "abstract": "  We leverage diffusion models to study the robustness-performance tradeoff of\nrobust classifiers. Our approach introduces a simple, pretrained diffusion\nmethod to generate low-norm counterfactual examples (CEs): semantically altered\ndata which results in different true class membership. We report that the\nconfidence and accuracy of robust models on their clean training data are\nassociated with the proximity of the data to their CEs. Moreover, robust models\nperform very poorly when evaluated on the CEs directly, as they become\nincreasingly invariant to the low-norm, semantic changes brought by CEs. The\nresults indicate a significant overlap between non-robust and semantic\nfeatures, countering the common assumption that non-robust features are not\ninterpretable.\n", "link": "http://arxiv.org/abs/2404.10588v2", "date": "2024-04-17", "relevancy": 1.9553, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5098}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4993}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.47}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Do%20Counterfactual%20Examples%20Complicate%20Adversarial%20Training%3F&body=Title%3A%20Do%20Counterfactual%20Examples%20Complicate%20Adversarial%20Training%3F%0AAuthor%3A%20Eric%20Yeats%20and%20Cameron%20Darwin%20and%20Eduardo%20Ortega%20and%20Frank%20Liu%20and%20Hai%20Li%0AAbstract%3A%20%20%20We%20leverage%20diffusion%20models%20to%20study%20the%20robustness-performance%20tradeoff%20of%0Arobust%20classifiers.%20Our%20approach%20introduces%20a%20simple%2C%20pretrained%20diffusion%0Amethod%20to%20generate%20low-norm%20counterfactual%20examples%20%28CEs%29%3A%20semantically%20altered%0Adata%20which%20results%20in%20different%20true%20class%20membership.%20We%20report%20that%20the%0Aconfidence%20and%20accuracy%20of%20robust%20models%20on%20their%20clean%20training%20data%20are%0Aassociated%20with%20the%20proximity%20of%20the%20data%20to%20their%20CEs.%20Moreover%2C%20robust%20models%0Aperform%20very%20poorly%20when%20evaluated%20on%20the%20CEs%20directly%2C%20as%20they%20become%0Aincreasingly%20invariant%20to%20the%20low-norm%2C%20semantic%20changes%20brought%20by%20CEs.%20The%0Aresults%20indicate%20a%20significant%20overlap%20between%20non-robust%20and%20semantic%0Afeatures%2C%20countering%20the%20common%20assumption%20that%20non-robust%20features%20are%20not%0Ainterpretable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10588v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Counterfactual%20Examples%20Complicate%20Adversarial%20Training%3F&entry.906535625=Eric%20Yeats%20and%20Cameron%20Darwin%20and%20Eduardo%20Ortega%20and%20Frank%20Liu%20and%20Hai%20Li&entry.1292438233=%20%20We%20leverage%20diffusion%20models%20to%20study%20the%20robustness-performance%20tradeoff%20of%0Arobust%20classifiers.%20Our%20approach%20introduces%20a%20simple%2C%20pretrained%20diffusion%0Amethod%20to%20generate%20low-norm%20counterfactual%20examples%20%28CEs%29%3A%20semantically%20altered%0Adata%20which%20results%20in%20different%20true%20class%20membership.%20We%20report%20that%20the%0Aconfidence%20and%20accuracy%20of%20robust%20models%20on%20their%20clean%20training%20data%20are%0Aassociated%20with%20the%20proximity%20of%20the%20data%20to%20their%20CEs.%20Moreover%2C%20robust%20models%0Aperform%20very%20poorly%20when%20evaluated%20on%20the%20CEs%20directly%2C%20as%20they%20become%0Aincreasingly%20invariant%20to%20the%20low-norm%2C%20semantic%20changes%20brought%20by%20CEs.%20The%0Aresults%20indicate%20a%20significant%20overlap%20between%20non-robust%20and%20semantic%0Afeatures%2C%20countering%20the%20common%20assumption%20that%20non-robust%20features%20are%20not%0Ainterpretable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10588v2&entry.124074799=Read"},
{"title": "StructComp: Substituting propagation with Structural Compression in\n  Training Graph Contrastive Learning", "author": "Shengzhong Zhang and Wenjie Yang and Xinyuan Cao and Hongwei Zhang and Zengfeng Huang", "abstract": "  Graph contrastive learning (GCL) has become a powerful tool for learning\ngraph data, but its scalability remains a significant challenge. In this work,\nwe propose a simple yet effective training framework called Structural\nCompression (StructComp) to address this issue. Inspired by a sparse low-rank\napproximation on the diffusion matrix, StructComp trains the encoder with the\ncompressed nodes. This allows the encoder not to perform any message passing\nduring the training stage, and significantly reduces the number of sample pairs\nin the contrastive loss. We theoretically prove that the original GCL loss can\nbe approximated with the contrastive loss computed by StructComp. Moreover,\nStructComp can be regarded as an additional regularization term for GCL models,\nresulting in a more robust encoder. Empirical studies on various datasets show\nthat StructComp greatly reduces the time and memory consumption while improving\nmodel performance compared to the vanilla GCL models and scalable training\nmethods.\n", "link": "http://arxiv.org/abs/2312.04865v2", "date": "2024-04-17", "relevancy": 1.9524, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5141}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4728}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4682}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20StructComp%3A%20Substituting%20propagation%20with%20Structural%20Compression%20in%0A%20%20Training%20Graph%20Contrastive%20Learning&body=Title%3A%20StructComp%3A%20Substituting%20propagation%20with%20Structural%20Compression%20in%0A%20%20Training%20Graph%20Contrastive%20Learning%0AAuthor%3A%20Shengzhong%20Zhang%20and%20Wenjie%20Yang%20and%20Xinyuan%20Cao%20and%20Hongwei%20Zhang%20and%20Zengfeng%20Huang%0AAbstract%3A%20%20%20Graph%20contrastive%20learning%20%28GCL%29%20has%20become%20a%20powerful%20tool%20for%20learning%0Agraph%20data%2C%20but%20its%20scalability%20remains%20a%20significant%20challenge.%20In%20this%20work%2C%0Awe%20propose%20a%20simple%20yet%20effective%20training%20framework%20called%20Structural%0ACompression%20%28StructComp%29%20to%20address%20this%20issue.%20Inspired%20by%20a%20sparse%20low-rank%0Aapproximation%20on%20the%20diffusion%20matrix%2C%20StructComp%20trains%20the%20encoder%20with%20the%0Acompressed%20nodes.%20This%20allows%20the%20encoder%20not%20to%20perform%20any%20message%20passing%0Aduring%20the%20training%20stage%2C%20and%20significantly%20reduces%20the%20number%20of%20sample%20pairs%0Ain%20the%20contrastive%20loss.%20We%20theoretically%20prove%20that%20the%20original%20GCL%20loss%20can%0Abe%20approximated%20with%20the%20contrastive%20loss%20computed%20by%20StructComp.%20Moreover%2C%0AStructComp%20can%20be%20regarded%20as%20an%20additional%20regularization%20term%20for%20GCL%20models%2C%0Aresulting%20in%20a%20more%20robust%20encoder.%20Empirical%20studies%20on%20various%20datasets%20show%0Athat%20StructComp%20greatly%20reduces%20the%20time%20and%20memory%20consumption%20while%20improving%0Amodel%20performance%20compared%20to%20the%20vanilla%20GCL%20models%20and%20scalable%20training%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04865v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StructComp%3A%20Substituting%20propagation%20with%20Structural%20Compression%20in%0A%20%20Training%20Graph%20Contrastive%20Learning&entry.906535625=Shengzhong%20Zhang%20and%20Wenjie%20Yang%20and%20Xinyuan%20Cao%20and%20Hongwei%20Zhang%20and%20Zengfeng%20Huang&entry.1292438233=%20%20Graph%20contrastive%20learning%20%28GCL%29%20has%20become%20a%20powerful%20tool%20for%20learning%0Agraph%20data%2C%20but%20its%20scalability%20remains%20a%20significant%20challenge.%20In%20this%20work%2C%0Awe%20propose%20a%20simple%20yet%20effective%20training%20framework%20called%20Structural%0ACompression%20%28StructComp%29%20to%20address%20this%20issue.%20Inspired%20by%20a%20sparse%20low-rank%0Aapproximation%20on%20the%20diffusion%20matrix%2C%20StructComp%20trains%20the%20encoder%20with%20the%0Acompressed%20nodes.%20This%20allows%20the%20encoder%20not%20to%20perform%20any%20message%20passing%0Aduring%20the%20training%20stage%2C%20and%20significantly%20reduces%20the%20number%20of%20sample%20pairs%0Ain%20the%20contrastive%20loss.%20We%20theoretically%20prove%20that%20the%20original%20GCL%20loss%20can%0Abe%20approximated%20with%20the%20contrastive%20loss%20computed%20by%20StructComp.%20Moreover%2C%0AStructComp%20can%20be%20regarded%20as%20an%20additional%20regularization%20term%20for%20GCL%20models%2C%0Aresulting%20in%20a%20more%20robust%20encoder.%20Empirical%20studies%20on%20various%20datasets%20show%0Athat%20StructComp%20greatly%20reduces%20the%20time%20and%20memory%20consumption%20while%20improving%0Amodel%20performance%20compared%20to%20the%20vanilla%20GCL%20models%20and%20scalable%20training%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04865v2&entry.124074799=Read"},
{"title": "Autonomous aerial perching and unperching using omnidirectional\n  tiltrotor and switching controller", "author": "Dongjae Lee and Sunwoo Hwang and Jeonghyun Byun and Seung Jae Lee and H. Jin Kim", "abstract": "  Aerial unperching of multirotors has received little attention as opposed to\nperching that has been investigated to elongate operation time. This study\npresents a new aerial robot capable of both perching and unperching\nautonomously on/from a ferromagnetic surface during flight, and a switching\ncontroller to avoid rotor saturation and mitigate overshoot during transition\nbetween free-flight and perching. To enable stable perching and unperching\nmaneuvers on/from a vertical surface, a lightweight ($\\approx$ $1$ \\si{kg}),\nfully actuated tiltrotor that can hover at $90^\\circ$ pitch angle is first\ndeveloped. We design a perching/unperching module composed of a single\nservomotor and a magnet, which is then mounted on the tiltrotor. A switching\ncontroller including exclusive control modes for transitions between\nfree-flight and perching is proposed. Lastly, we propose a simple yet effective\nstrategy to ensure robust perching in the presence of measurement and control\nerrors and avoid collisions with the perching site immediately after\nunperching. We validate the proposed framework in experiments where the\ntiltrotor successfully performs perching and unperching on/from a vertical\nsurface during flight. We further show effectiveness of the proposed transition\nmode in the switching controller by ablation studies where large overshoot and\neven collision with a perching site occur. To the best of the authors'\nknowledge, this work presents the first autonomous aerial unperching framework\nusing a fully actuated tiltrotor.\n", "link": "http://arxiv.org/abs/2404.11310v1", "date": "2024-04-17", "relevancy": 1.9522, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5142}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4824}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4642}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Autonomous%20aerial%20perching%20and%20unperching%20using%20omnidirectional%0A%20%20tiltrotor%20and%20switching%20controller&body=Title%3A%20Autonomous%20aerial%20perching%20and%20unperching%20using%20omnidirectional%0A%20%20tiltrotor%20and%20switching%20controller%0AAuthor%3A%20Dongjae%20Lee%20and%20Sunwoo%20Hwang%20and%20Jeonghyun%20Byun%20and%20Seung%20Jae%20Lee%20and%20H.%20Jin%20Kim%0AAbstract%3A%20%20%20Aerial%20unperching%20of%20multirotors%20has%20received%20little%20attention%20as%20opposed%20to%0Aperching%20that%20has%20been%20investigated%20to%20elongate%20operation%20time.%20This%20study%0Apresents%20a%20new%20aerial%20robot%20capable%20of%20both%20perching%20and%20unperching%0Aautonomously%20on/from%20a%20ferromagnetic%20surface%20during%20flight%2C%20and%20a%20switching%0Acontroller%20to%20avoid%20rotor%20saturation%20and%20mitigate%20overshoot%20during%20transition%0Abetween%20free-flight%20and%20perching.%20To%20enable%20stable%20perching%20and%20unperching%0Amaneuvers%20on/from%20a%20vertical%20surface%2C%20a%20lightweight%20%28%24%5Capprox%24%20%241%24%20%5Csi%7Bkg%7D%29%2C%0Afully%20actuated%20tiltrotor%20that%20can%20hover%20at%20%2490%5E%5Ccirc%24%20pitch%20angle%20is%20first%0Adeveloped.%20We%20design%20a%20perching/unperching%20module%20composed%20of%20a%20single%0Aservomotor%20and%20a%20magnet%2C%20which%20is%20then%20mounted%20on%20the%20tiltrotor.%20A%20switching%0Acontroller%20including%20exclusive%20control%20modes%20for%20transitions%20between%0Afree-flight%20and%20perching%20is%20proposed.%20Lastly%2C%20we%20propose%20a%20simple%20yet%20effective%0Astrategy%20to%20ensure%20robust%20perching%20in%20the%20presence%20of%20measurement%20and%20control%0Aerrors%20and%20avoid%20collisions%20with%20the%20perching%20site%20immediately%20after%0Aunperching.%20We%20validate%20the%20proposed%20framework%20in%20experiments%20where%20the%0Atiltrotor%20successfully%20performs%20perching%20and%20unperching%20on/from%20a%20vertical%0Asurface%20during%20flight.%20We%20further%20show%20effectiveness%20of%20the%20proposed%20transition%0Amode%20in%20the%20switching%20controller%20by%20ablation%20studies%20where%20large%20overshoot%20and%0Aeven%20collision%20with%20a%20perching%20site%20occur.%20To%20the%20best%20of%20the%20authors%27%0Aknowledge%2C%20this%20work%20presents%20the%20first%20autonomous%20aerial%20unperching%20framework%0Ausing%20a%20fully%20actuated%20tiltrotor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11310v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20aerial%20perching%20and%20unperching%20using%20omnidirectional%0A%20%20tiltrotor%20and%20switching%20controller&entry.906535625=Dongjae%20Lee%20and%20Sunwoo%20Hwang%20and%20Jeonghyun%20Byun%20and%20Seung%20Jae%20Lee%20and%20H.%20Jin%20Kim&entry.1292438233=%20%20Aerial%20unperching%20of%20multirotors%20has%20received%20little%20attention%20as%20opposed%20to%0Aperching%20that%20has%20been%20investigated%20to%20elongate%20operation%20time.%20This%20study%0Apresents%20a%20new%20aerial%20robot%20capable%20of%20both%20perching%20and%20unperching%0Aautonomously%20on/from%20a%20ferromagnetic%20surface%20during%20flight%2C%20and%20a%20switching%0Acontroller%20to%20avoid%20rotor%20saturation%20and%20mitigate%20overshoot%20during%20transition%0Abetween%20free-flight%20and%20perching.%20To%20enable%20stable%20perching%20and%20unperching%0Amaneuvers%20on/from%20a%20vertical%20surface%2C%20a%20lightweight%20%28%24%5Capprox%24%20%241%24%20%5Csi%7Bkg%7D%29%2C%0Afully%20actuated%20tiltrotor%20that%20can%20hover%20at%20%2490%5E%5Ccirc%24%20pitch%20angle%20is%20first%0Adeveloped.%20We%20design%20a%20perching/unperching%20module%20composed%20of%20a%20single%0Aservomotor%20and%20a%20magnet%2C%20which%20is%20then%20mounted%20on%20the%20tiltrotor.%20A%20switching%0Acontroller%20including%20exclusive%20control%20modes%20for%20transitions%20between%0Afree-flight%20and%20perching%20is%20proposed.%20Lastly%2C%20we%20propose%20a%20simple%20yet%20effective%0Astrategy%20to%20ensure%20robust%20perching%20in%20the%20presence%20of%20measurement%20and%20control%0Aerrors%20and%20avoid%20collisions%20with%20the%20perching%20site%20immediately%20after%0Aunperching.%20We%20validate%20the%20proposed%20framework%20in%20experiments%20where%20the%0Atiltrotor%20successfully%20performs%20perching%20and%20unperching%20on/from%20a%20vertical%0Asurface%20during%20flight.%20We%20further%20show%20effectiveness%20of%20the%20proposed%20transition%0Amode%20in%20the%20switching%20controller%20by%20ablation%20studies%20where%20large%20overshoot%20and%0Aeven%20collision%20with%20a%20perching%20site%20occur.%20To%20the%20best%20of%20the%20authors%27%0Aknowledge%2C%20this%20work%20presents%20the%20first%20autonomous%20aerial%20unperching%20framework%0Ausing%20a%20fully%20actuated%20tiltrotor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11310v1&entry.124074799=Read"},
{"title": "Best Practices for a Handwritten Text Recognition System", "author": "George Retsinas and Giorgos Sfikas and Basilis Gatos and Christophoros Nikou", "abstract": "  Handwritten text recognition has been developed rapidly in the recent years,\nfollowing the rise of deep learning and its applications. Though deep learning\nmethods provide notable boost in performance concerning text recognition,\nnon-trivial deviation in performance can be detected even when small\npre-processing or architectural/optimization elements are changed. This work\nfollows a ``best practice'' rationale; highlight simple yet effective empirical\npractices that can further help training and provide well-performing\nhandwritten text recognition systems. Specifically, we considered three basic\naspects of a deep HTR system and we proposed simple yet effective solutions: 1)\nretain the aspect ratio of the images in the preprocessing step, 2) use\nmax-pooling for converting the 3D feature map of CNN output into a sequence of\nfeatures and 3) assist the training procedure via an additional CTC loss which\nacts as a shortcut on the max-pooled sequential features. Using these proposed\nsimple modifications, one can attain close to state-of-the-art results, while\nconsidering a basic convolutional-recurrent (CNN+LSTM) architecture, for both\nIAM and RIMES datasets. Code is available at\nhttps://github.com/georgeretsi/HTR-best-practices/.\n", "link": "http://arxiv.org/abs/2404.11339v1", "date": "2024-04-17", "relevancy": 1.9431, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4889}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4875}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4828}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Best%20Practices%20for%20a%20Handwritten%20Text%20Recognition%20System&body=Title%3A%20Best%20Practices%20for%20a%20Handwritten%20Text%20Recognition%20System%0AAuthor%3A%20George%20Retsinas%20and%20Giorgos%20Sfikas%20and%20Basilis%20Gatos%20and%20Christophoros%20Nikou%0AAbstract%3A%20%20%20Handwritten%20text%20recognition%20has%20been%20developed%20rapidly%20in%20the%20recent%20years%2C%0Afollowing%20the%20rise%20of%20deep%20learning%20and%20its%20applications.%20Though%20deep%20learning%0Amethods%20provide%20notable%20boost%20in%20performance%20concerning%20text%20recognition%2C%0Anon-trivial%20deviation%20in%20performance%20can%20be%20detected%20even%20when%20small%0Apre-processing%20or%20architectural/optimization%20elements%20are%20changed.%20This%20work%0Afollows%20a%20%60%60best%20practice%27%27%20rationale%3B%20highlight%20simple%20yet%20effective%20empirical%0Apractices%20that%20can%20further%20help%20training%20and%20provide%20well-performing%0Ahandwritten%20text%20recognition%20systems.%20Specifically%2C%20we%20considered%20three%20basic%0Aaspects%20of%20a%20deep%20HTR%20system%20and%20we%20proposed%20simple%20yet%20effective%20solutions%3A%201%29%0Aretain%20the%20aspect%20ratio%20of%20the%20images%20in%20the%20preprocessing%20step%2C%202%29%20use%0Amax-pooling%20for%20converting%20the%203D%20feature%20map%20of%20CNN%20output%20into%20a%20sequence%20of%0Afeatures%20and%203%29%20assist%20the%20training%20procedure%20via%20an%20additional%20CTC%20loss%20which%0Aacts%20as%20a%20shortcut%20on%20the%20max-pooled%20sequential%20features.%20Using%20these%20proposed%0Asimple%20modifications%2C%20one%20can%20attain%20close%20to%20state-of-the-art%20results%2C%20while%0Aconsidering%20a%20basic%20convolutional-recurrent%20%28CNN%2BLSTM%29%20architecture%2C%20for%20both%0AIAM%20and%20RIMES%20datasets.%20Code%20is%20available%20at%0Ahttps%3A//github.com/georgeretsi/HTR-best-practices/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11339v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Best%20Practices%20for%20a%20Handwritten%20Text%20Recognition%20System&entry.906535625=George%20Retsinas%20and%20Giorgos%20Sfikas%20and%20Basilis%20Gatos%20and%20Christophoros%20Nikou&entry.1292438233=%20%20Handwritten%20text%20recognition%20has%20been%20developed%20rapidly%20in%20the%20recent%20years%2C%0Afollowing%20the%20rise%20of%20deep%20learning%20and%20its%20applications.%20Though%20deep%20learning%0Amethods%20provide%20notable%20boost%20in%20performance%20concerning%20text%20recognition%2C%0Anon-trivial%20deviation%20in%20performance%20can%20be%20detected%20even%20when%20small%0Apre-processing%20or%20architectural/optimization%20elements%20are%20changed.%20This%20work%0Afollows%20a%20%60%60best%20practice%27%27%20rationale%3B%20highlight%20simple%20yet%20effective%20empirical%0Apractices%20that%20can%20further%20help%20training%20and%20provide%20well-performing%0Ahandwritten%20text%20recognition%20systems.%20Specifically%2C%20we%20considered%20three%20basic%0Aaspects%20of%20a%20deep%20HTR%20system%20and%20we%20proposed%20simple%20yet%20effective%20solutions%3A%201%29%0Aretain%20the%20aspect%20ratio%20of%20the%20images%20in%20the%20preprocessing%20step%2C%202%29%20use%0Amax-pooling%20for%20converting%20the%203D%20feature%20map%20of%20CNN%20output%20into%20a%20sequence%20of%0Afeatures%20and%203%29%20assist%20the%20training%20procedure%20via%20an%20additional%20CTC%20loss%20which%0Aacts%20as%20a%20shortcut%20on%20the%20max-pooled%20sequential%20features.%20Using%20these%20proposed%0Asimple%20modifications%2C%20one%20can%20attain%20close%20to%20state-of-the-art%20results%2C%20while%0Aconsidering%20a%20basic%20convolutional-recurrent%20%28CNN%2BLSTM%29%20architecture%2C%20for%20both%0AIAM%20and%20RIMES%20datasets.%20Code%20is%20available%20at%0Ahttps%3A//github.com/georgeretsi/HTR-best-practices/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11339v1&entry.124074799=Read"},
{"title": "Learn to Tour: Operator Design For Solution Feasibility Mapping in\n  Pickup-and-delivery Traveling Salesman Problem", "author": "Bowen Fang and Xu Chen and Xuan Di", "abstract": "  This paper aims to develop a learning method for a special class of traveling\nsalesman problems (TSP), namely, the pickup-and-delivery TSP (PDTSP), which\nfinds the shortest tour along a sequence of one-to-one pickup-and-delivery\nnodes. One-to-one here means that the transported people or goods are\nassociated with designated pairs of pickup and delivery nodes, in contrast to\nthat indistinguishable goods can be delivered to any nodes. In PDTSP,\nprecedence constraints need to be satisfied that each pickup node must be\nvisited before its corresponding delivery node. Classic operations research\n(OR) algorithms for PDTSP are difficult to scale to large-sized problems.\nRecently, reinforcement learning (RL) has been applied to TSPs. The basic idea\nis to explore and evaluate visiting sequences in a solution space. However,\nthis approach could be less computationally efficient, as it has to potentially\nevaluate many infeasible solutions of which precedence constraints are\nviolated. To restrict solution search within a feasible space, we utilize\noperators that always map one feasible solution to another, without spending\ntime exploring the infeasible solution space. Such operators are evaluated and\nselected as policies to solve PDTSPs in an RL framework. We make a comparison\nof our method and baselines, including classic OR algorithms and existing\nlearning methods. Results show that our approach can find tours shorter than\nbaselines.\n", "link": "http://arxiv.org/abs/2404.11458v1", "date": "2024-04-17", "relevancy": 1.9425, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4958}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4797}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.475}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learn%20to%20Tour%3A%20Operator%20Design%20For%20Solution%20Feasibility%20Mapping%20in%0A%20%20Pickup-and-delivery%20Traveling%20Salesman%20Problem&body=Title%3A%20Learn%20to%20Tour%3A%20Operator%20Design%20For%20Solution%20Feasibility%20Mapping%20in%0A%20%20Pickup-and-delivery%20Traveling%20Salesman%20Problem%0AAuthor%3A%20Bowen%20Fang%20and%20Xu%20Chen%20and%20Xuan%20Di%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20develop%20a%20learning%20method%20for%20a%20special%20class%20of%20traveling%0Asalesman%20problems%20%28TSP%29%2C%20namely%2C%20the%20pickup-and-delivery%20TSP%20%28PDTSP%29%2C%20which%0Afinds%20the%20shortest%20tour%20along%20a%20sequence%20of%20one-to-one%20pickup-and-delivery%0Anodes.%20One-to-one%20here%20means%20that%20the%20transported%20people%20or%20goods%20are%0Aassociated%20with%20designated%20pairs%20of%20pickup%20and%20delivery%20nodes%2C%20in%20contrast%20to%0Athat%20indistinguishable%20goods%20can%20be%20delivered%20to%20any%20nodes.%20In%20PDTSP%2C%0Aprecedence%20constraints%20need%20to%20be%20satisfied%20that%20each%20pickup%20node%20must%20be%0Avisited%20before%20its%20corresponding%20delivery%20node.%20Classic%20operations%20research%0A%28OR%29%20algorithms%20for%20PDTSP%20are%20difficult%20to%20scale%20to%20large-sized%20problems.%0ARecently%2C%20reinforcement%20learning%20%28RL%29%20has%20been%20applied%20to%20TSPs.%20The%20basic%20idea%0Ais%20to%20explore%20and%20evaluate%20visiting%20sequences%20in%20a%20solution%20space.%20However%2C%0Athis%20approach%20could%20be%20less%20computationally%20efficient%2C%20as%20it%20has%20to%20potentially%0Aevaluate%20many%20infeasible%20solutions%20of%20which%20precedence%20constraints%20are%0Aviolated.%20To%20restrict%20solution%20search%20within%20a%20feasible%20space%2C%20we%20utilize%0Aoperators%20that%20always%20map%20one%20feasible%20solution%20to%20another%2C%20without%20spending%0Atime%20exploring%20the%20infeasible%20solution%20space.%20Such%20operators%20are%20evaluated%20and%0Aselected%20as%20policies%20to%20solve%20PDTSPs%20in%20an%20RL%20framework.%20We%20make%20a%20comparison%0Aof%20our%20method%20and%20baselines%2C%20including%20classic%20OR%20algorithms%20and%20existing%0Alearning%20methods.%20Results%20show%20that%20our%20approach%20can%20find%20tours%20shorter%20than%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11458v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn%20to%20Tour%3A%20Operator%20Design%20For%20Solution%20Feasibility%20Mapping%20in%0A%20%20Pickup-and-delivery%20Traveling%20Salesman%20Problem&entry.906535625=Bowen%20Fang%20and%20Xu%20Chen%20and%20Xuan%20Di&entry.1292438233=%20%20This%20paper%20aims%20to%20develop%20a%20learning%20method%20for%20a%20special%20class%20of%20traveling%0Asalesman%20problems%20%28TSP%29%2C%20namely%2C%20the%20pickup-and-delivery%20TSP%20%28PDTSP%29%2C%20which%0Afinds%20the%20shortest%20tour%20along%20a%20sequence%20of%20one-to-one%20pickup-and-delivery%0Anodes.%20One-to-one%20here%20means%20that%20the%20transported%20people%20or%20goods%20are%0Aassociated%20with%20designated%20pairs%20of%20pickup%20and%20delivery%20nodes%2C%20in%20contrast%20to%0Athat%20indistinguishable%20goods%20can%20be%20delivered%20to%20any%20nodes.%20In%20PDTSP%2C%0Aprecedence%20constraints%20need%20to%20be%20satisfied%20that%20each%20pickup%20node%20must%20be%0Avisited%20before%20its%20corresponding%20delivery%20node.%20Classic%20operations%20research%0A%28OR%29%20algorithms%20for%20PDTSP%20are%20difficult%20to%20scale%20to%20large-sized%20problems.%0ARecently%2C%20reinforcement%20learning%20%28RL%29%20has%20been%20applied%20to%20TSPs.%20The%20basic%20idea%0Ais%20to%20explore%20and%20evaluate%20visiting%20sequences%20in%20a%20solution%20space.%20However%2C%0Athis%20approach%20could%20be%20less%20computationally%20efficient%2C%20as%20it%20has%20to%20potentially%0Aevaluate%20many%20infeasible%20solutions%20of%20which%20precedence%20constraints%20are%0Aviolated.%20To%20restrict%20solution%20search%20within%20a%20feasible%20space%2C%20we%20utilize%0Aoperators%20that%20always%20map%20one%20feasible%20solution%20to%20another%2C%20without%20spending%0Atime%20exploring%20the%20infeasible%20solution%20space.%20Such%20operators%20are%20evaluated%20and%0Aselected%20as%20policies%20to%20solve%20PDTSPs%20in%20an%20RL%20framework.%20We%20make%20a%20comparison%0Aof%20our%20method%20and%20baselines%2C%20including%20classic%20OR%20algorithms%20and%20existing%0Alearning%20methods.%20Results%20show%20that%20our%20approach%20can%20find%20tours%20shorter%20than%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11458v1&entry.124074799=Read"},
{"title": "GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI\n  collaboration", "author": "Ben Wang", "abstract": "  The advent of ChatGPT and similar large language models (LLMs) has\nrevolutionized the human-AI interaction and information-seeking process.\nLeveraging LLMs as an alternative to search engines, users can now access\nsummarized information tailored to their queries, significantly reducing the\ncognitive load associated with navigating vast information resources. This\nshift underscores the potential of LLMs in redefining information access\nparadigms. Drawing on the foundation of task-focused information retrieval and\nLLMs' task planning ability, this research extends the scope of LLM\ncapabilities beyond routine task automation to support users in navigating\nlong-term and significant life tasks. It introduces the GOLF framework\n(Goal-Oriented Long-term liFe tasks), which focuses on enhancing LLMs' ability\nto assist in significant life decisions through goal orientation and long-term\nplanning. The methodology encompasses a comprehensive simulation study to test\nthe framework's efficacy, followed by model and human evaluations to develop a\ndataset benchmark for long-term life tasks, and experiments across different\nmodels and settings. By shifting the focus from short-term tasks to the broader\nspectrum of long-term life goals, this research underscores the transformative\npotential of LLMs in enhancing human decision-making processes and task\nmanagement, marking a significant step forward in the evolution of human-AI\ncollaboration.\n", "link": "http://arxiv.org/abs/2403.17089v2", "date": "2024-04-17", "relevancy": 1.9397, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5206}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4953}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4603}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GOLF%3A%20Goal-Oriented%20Long-term%20liFe%20tasks%20supported%20by%20human-AI%0A%20%20collaboration&body=Title%3A%20GOLF%3A%20Goal-Oriented%20Long-term%20liFe%20tasks%20supported%20by%20human-AI%0A%20%20collaboration%0AAuthor%3A%20Ben%20Wang%0AAbstract%3A%20%20%20The%20advent%20of%20ChatGPT%20and%20similar%20large%20language%20models%20%28LLMs%29%20has%0Arevolutionized%20the%20human-AI%20interaction%20and%20information-seeking%20process.%0ALeveraging%20LLMs%20as%20an%20alternative%20to%20search%20engines%2C%20users%20can%20now%20access%0Asummarized%20information%20tailored%20to%20their%20queries%2C%20significantly%20reducing%20the%0Acognitive%20load%20associated%20with%20navigating%20vast%20information%20resources.%20This%0Ashift%20underscores%20the%20potential%20of%20LLMs%20in%20redefining%20information%20access%0Aparadigms.%20Drawing%20on%20the%20foundation%20of%20task-focused%20information%20retrieval%20and%0ALLMs%27%20task%20planning%20ability%2C%20this%20research%20extends%20the%20scope%20of%20LLM%0Acapabilities%20beyond%20routine%20task%20automation%20to%20support%20users%20in%20navigating%0Along-term%20and%20significant%20life%20tasks.%20It%20introduces%20the%20GOLF%20framework%0A%28Goal-Oriented%20Long-term%20liFe%20tasks%29%2C%20which%20focuses%20on%20enhancing%20LLMs%27%20ability%0Ato%20assist%20in%20significant%20life%20decisions%20through%20goal%20orientation%20and%20long-term%0Aplanning.%20The%20methodology%20encompasses%20a%20comprehensive%20simulation%20study%20to%20test%0Athe%20framework%27s%20efficacy%2C%20followed%20by%20model%20and%20human%20evaluations%20to%20develop%20a%0Adataset%20benchmark%20for%20long-term%20life%20tasks%2C%20and%20experiments%20across%20different%0Amodels%20and%20settings.%20By%20shifting%20the%20focus%20from%20short-term%20tasks%20to%20the%20broader%0Aspectrum%20of%20long-term%20life%20goals%2C%20this%20research%20underscores%20the%20transformative%0Apotential%20of%20LLMs%20in%20enhancing%20human%20decision-making%20processes%20and%20task%0Amanagement%2C%20marking%20a%20significant%20step%20forward%20in%20the%20evolution%20of%20human-AI%0Acollaboration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17089v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GOLF%3A%20Goal-Oriented%20Long-term%20liFe%20tasks%20supported%20by%20human-AI%0A%20%20collaboration&entry.906535625=Ben%20Wang&entry.1292438233=%20%20The%20advent%20of%20ChatGPT%20and%20similar%20large%20language%20models%20%28LLMs%29%20has%0Arevolutionized%20the%20human-AI%20interaction%20and%20information-seeking%20process.%0ALeveraging%20LLMs%20as%20an%20alternative%20to%20search%20engines%2C%20users%20can%20now%20access%0Asummarized%20information%20tailored%20to%20their%20queries%2C%20significantly%20reducing%20the%0Acognitive%20load%20associated%20with%20navigating%20vast%20information%20resources.%20This%0Ashift%20underscores%20the%20potential%20of%20LLMs%20in%20redefining%20information%20access%0Aparadigms.%20Drawing%20on%20the%20foundation%20of%20task-focused%20information%20retrieval%20and%0ALLMs%27%20task%20planning%20ability%2C%20this%20research%20extends%20the%20scope%20of%20LLM%0Acapabilities%20beyond%20routine%20task%20automation%20to%20support%20users%20in%20navigating%0Along-term%20and%20significant%20life%20tasks.%20It%20introduces%20the%20GOLF%20framework%0A%28Goal-Oriented%20Long-term%20liFe%20tasks%29%2C%20which%20focuses%20on%20enhancing%20LLMs%27%20ability%0Ato%20assist%20in%20significant%20life%20decisions%20through%20goal%20orientation%20and%20long-term%0Aplanning.%20The%20methodology%20encompasses%20a%20comprehensive%20simulation%20study%20to%20test%0Athe%20framework%27s%20efficacy%2C%20followed%20by%20model%20and%20human%20evaluations%20to%20develop%20a%0Adataset%20benchmark%20for%20long-term%20life%20tasks%2C%20and%20experiments%20across%20different%0Amodels%20and%20settings.%20By%20shifting%20the%20focus%20from%20short-term%20tasks%20to%20the%20broader%0Aspectrum%20of%20long-term%20life%20goals%2C%20this%20research%20underscores%20the%20transformative%0Apotential%20of%20LLMs%20in%20enhancing%20human%20decision-making%20processes%20and%20task%0Amanagement%2C%20marking%20a%20significant%20step%20forward%20in%20the%20evolution%20of%20human-AI%0Acollaboration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17089v2&entry.124074799=Read"},
{"title": "Can LLMs perform structured graph reasoning?", "author": "Palaash Agrawal and Shavak Vasania and Cheston Tan", "abstract": "  Pretrained Large Language Models (LLMs) have demonstrated various reasoning\ncapabilities through language-based prompts alone, particularly in unstructured\ntask settings (tasks purely based on language semantics). However, LLMs often\nstruggle with structured tasks, because of the inherent incompatibility of\ninput representation. Reducing structured tasks to uni-dimensional language\nsemantics often renders the problem trivial. Keeping the trade-off between LLM\ncompatibility and structure complexity in mind, we design various graph\nreasoning tasks as a proxy to semi-structured tasks in this paper, in order to\ntest the ability to navigate through representations beyond plain text in\nvarious LLMs. Particularly, we design 10 distinct problems of graph traversal,\neach representing increasing levels of complexity, and benchmark 5 different\ninstruct-finetuned LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) on the\naforementioned tasks. Further, we analyse the performance of models across\nvarious settings such as varying sizes of graphs as well as different forms of\nk-shot prompting. We highlight various limitations, biases and properties of\nLLMs through this benchmarking process, such as an inverse relation to the\naverage degrees of freedom of traversal per node in graphs, the overall\nnegative impact of k-shot prompting on graph reasoning tasks, and a positive\nresponse bias which prevents LLMs from identifying the absence of a valid\nsolution. Finally, we introduce a new prompting technique specially designed\nfor graph traversal tasks (PathCompare), which demonstrates a notable increase\nin the performance of LLMs in comparison to standard prompting techniques such\nas Chain-of-Thought (CoT).\n", "link": "http://arxiv.org/abs/2402.01805v2", "date": "2024-04-17", "relevancy": 1.9334, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5007}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4925}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4624}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Can%20LLMs%20perform%20structured%20graph%20reasoning%3F&body=Title%3A%20Can%20LLMs%20perform%20structured%20graph%20reasoning%3F%0AAuthor%3A%20Palaash%20Agrawal%20and%20Shavak%20Vasania%20and%20Cheston%20Tan%0AAbstract%3A%20%20%20Pretrained%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20various%20reasoning%0Acapabilities%20through%20language-based%20prompts%20alone%2C%20particularly%20in%20unstructured%0Atask%20settings%20%28tasks%20purely%20based%20on%20language%20semantics%29.%20However%2C%20LLMs%20often%0Astruggle%20with%20structured%20tasks%2C%20because%20of%20the%20inherent%20incompatibility%20of%0Ainput%20representation.%20Reducing%20structured%20tasks%20to%20uni-dimensional%20language%0Asemantics%20often%20renders%20the%20problem%20trivial.%20Keeping%20the%20trade-off%20between%20LLM%0Acompatibility%20and%20structure%20complexity%20in%20mind%2C%20we%20design%20various%20graph%0Areasoning%20tasks%20as%20a%20proxy%20to%20semi-structured%20tasks%20in%20this%20paper%2C%20in%20order%20to%0Atest%20the%20ability%20to%20navigate%20through%20representations%20beyond%20plain%20text%20in%0Avarious%20LLMs.%20Particularly%2C%20we%20design%2010%20distinct%20problems%20of%20graph%20traversal%2C%0Aeach%20representing%20increasing%20levels%20of%20complexity%2C%20and%20benchmark%205%20different%0Ainstruct-finetuned%20LLMs%20%28GPT-4%2C%20GPT-3.5%2C%20Claude-2%2C%20Llama-2%20and%20Palm-2%29%20on%20the%0Aaforementioned%20tasks.%20Further%2C%20we%20analyse%20the%20performance%20of%20models%20across%0Avarious%20settings%20such%20as%20varying%20sizes%20of%20graphs%20as%20well%20as%20different%20forms%20of%0Ak-shot%20prompting.%20We%20highlight%20various%20limitations%2C%20biases%20and%20properties%20of%0ALLMs%20through%20this%20benchmarking%20process%2C%20such%20as%20an%20inverse%20relation%20to%20the%0Aaverage%20degrees%20of%20freedom%20of%20traversal%20per%20node%20in%20graphs%2C%20the%20overall%0Anegative%20impact%20of%20k-shot%20prompting%20on%20graph%20reasoning%20tasks%2C%20and%20a%20positive%0Aresponse%20bias%20which%20prevents%20LLMs%20from%20identifying%20the%20absence%20of%20a%20valid%0Asolution.%20Finally%2C%20we%20introduce%20a%20new%20prompting%20technique%20specially%20designed%0Afor%20graph%20traversal%20tasks%20%28PathCompare%29%2C%20which%20demonstrates%20a%20notable%20increase%0Ain%20the%20performance%20of%20LLMs%20in%20comparison%20to%20standard%20prompting%20techniques%20such%0Aas%20Chain-of-Thought%20%28CoT%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01805v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLMs%20perform%20structured%20graph%20reasoning%3F&entry.906535625=Palaash%20Agrawal%20and%20Shavak%20Vasania%20and%20Cheston%20Tan&entry.1292438233=%20%20Pretrained%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20various%20reasoning%0Acapabilities%20through%20language-based%20prompts%20alone%2C%20particularly%20in%20unstructured%0Atask%20settings%20%28tasks%20purely%20based%20on%20language%20semantics%29.%20However%2C%20LLMs%20often%0Astruggle%20with%20structured%20tasks%2C%20because%20of%20the%20inherent%20incompatibility%20of%0Ainput%20representation.%20Reducing%20structured%20tasks%20to%20uni-dimensional%20language%0Asemantics%20often%20renders%20the%20problem%20trivial.%20Keeping%20the%20trade-off%20between%20LLM%0Acompatibility%20and%20structure%20complexity%20in%20mind%2C%20we%20design%20various%20graph%0Areasoning%20tasks%20as%20a%20proxy%20to%20semi-structured%20tasks%20in%20this%20paper%2C%20in%20order%20to%0Atest%20the%20ability%20to%20navigate%20through%20representations%20beyond%20plain%20text%20in%0Avarious%20LLMs.%20Particularly%2C%20we%20design%2010%20distinct%20problems%20of%20graph%20traversal%2C%0Aeach%20representing%20increasing%20levels%20of%20complexity%2C%20and%20benchmark%205%20different%0Ainstruct-finetuned%20LLMs%20%28GPT-4%2C%20GPT-3.5%2C%20Claude-2%2C%20Llama-2%20and%20Palm-2%29%20on%20the%0Aaforementioned%20tasks.%20Further%2C%20we%20analyse%20the%20performance%20of%20models%20across%0Avarious%20settings%20such%20as%20varying%20sizes%20of%20graphs%20as%20well%20as%20different%20forms%20of%0Ak-shot%20prompting.%20We%20highlight%20various%20limitations%2C%20biases%20and%20properties%20of%0ALLMs%20through%20this%20benchmarking%20process%2C%20such%20as%20an%20inverse%20relation%20to%20the%0Aaverage%20degrees%20of%20freedom%20of%20traversal%20per%20node%20in%20graphs%2C%20the%20overall%0Anegative%20impact%20of%20k-shot%20prompting%20on%20graph%20reasoning%20tasks%2C%20and%20a%20positive%0Aresponse%20bias%20which%20prevents%20LLMs%20from%20identifying%20the%20absence%20of%20a%20valid%0Asolution.%20Finally%2C%20we%20introduce%20a%20new%20prompting%20technique%20specially%20designed%0Afor%20graph%20traversal%20tasks%20%28PathCompare%29%2C%20which%20demonstrates%20a%20notable%20increase%0Ain%20the%20performance%20of%20LLMs%20in%20comparison%20to%20standard%20prompting%20techniques%20such%0Aas%20Chain-of-Thought%20%28CoT%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01805v2&entry.124074799=Read"},
{"title": "Towards Reliable Empirical Machine Unlearning Evaluation: A\n  Game-Theoretic View", "author": "Yiwen Tu and Pingbang Hu and Jiaqi Ma", "abstract": "  Machine unlearning is the process of updating machine learning models to\nremove the information of specific training data samples, in order to comply\nwith data protection regulations that allow individuals to request the removal\nof their personal data. Despite the recent development of numerous unlearning\nalgorithms, reliable evaluation of these algorithms remains an open research\nquestion. In this work, we focus on membership inference attack (MIA) based\nevaluation, one of the most common approaches for evaluating unlearning\nalgorithms, and address various pitfalls of existing evaluation metrics that\nlack reliability. Specifically, we propose a game-theoretic framework that\nformalizes the evaluation process as a game between unlearning algorithms and\nMIA adversaries, measuring the data removal efficacy of unlearning algorithms\nby the capability of the MIA adversaries. Through careful design of the game,\nwe demonstrate that the natural evaluation metric induced from the game enjoys\nprovable guarantees that the existing evaluation metrics fail to satisfy.\nFurthermore, we propose a practical and efficient algorithm to estimate the\nevaluation metric induced from the game, and demonstrate its effectiveness\nthrough both theoretical analysis and empirical experiments. This work presents\na novel and reliable approach to empirically evaluating unlearning algorithms,\npaving the way for the development of more effective unlearning techniques.\n", "link": "http://arxiv.org/abs/2404.11577v1", "date": "2024-04-17", "relevancy": 1.9314, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5417}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4789}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4633}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Reliable%20Empirical%20Machine%20Unlearning%20Evaluation%3A%20A%0A%20%20Game-Theoretic%20View&body=Title%3A%20Towards%20Reliable%20Empirical%20Machine%20Unlearning%20Evaluation%3A%20A%0A%20%20Game-Theoretic%20View%0AAuthor%3A%20Yiwen%20Tu%20and%20Pingbang%20Hu%20and%20Jiaqi%20Ma%0AAbstract%3A%20%20%20Machine%20unlearning%20is%20the%20process%20of%20updating%20machine%20learning%20models%20to%0Aremove%20the%20information%20of%20specific%20training%20data%20samples%2C%20in%20order%20to%20comply%0Awith%20data%20protection%20regulations%20that%20allow%20individuals%20to%20request%20the%20removal%0Aof%20their%20personal%20data.%20Despite%20the%20recent%20development%20of%20numerous%20unlearning%0Aalgorithms%2C%20reliable%20evaluation%20of%20these%20algorithms%20remains%20an%20open%20research%0Aquestion.%20In%20this%20work%2C%20we%20focus%20on%20membership%20inference%20attack%20%28MIA%29%20based%0Aevaluation%2C%20one%20of%20the%20most%20common%20approaches%20for%20evaluating%20unlearning%0Aalgorithms%2C%20and%20address%20various%20pitfalls%20of%20existing%20evaluation%20metrics%20that%0Alack%20reliability.%20Specifically%2C%20we%20propose%20a%20game-theoretic%20framework%20that%0Aformalizes%20the%20evaluation%20process%20as%20a%20game%20between%20unlearning%20algorithms%20and%0AMIA%20adversaries%2C%20measuring%20the%20data%20removal%20efficacy%20of%20unlearning%20algorithms%0Aby%20the%20capability%20of%20the%20MIA%20adversaries.%20Through%20careful%20design%20of%20the%20game%2C%0Awe%20demonstrate%20that%20the%20natural%20evaluation%20metric%20induced%20from%20the%20game%20enjoys%0Aprovable%20guarantees%20that%20the%20existing%20evaluation%20metrics%20fail%20to%20satisfy.%0AFurthermore%2C%20we%20propose%20a%20practical%20and%20efficient%20algorithm%20to%20estimate%20the%0Aevaluation%20metric%20induced%20from%20the%20game%2C%20and%20demonstrate%20its%20effectiveness%0Athrough%20both%20theoretical%20analysis%20and%20empirical%20experiments.%20This%20work%20presents%0Aa%20novel%20and%20reliable%20approach%20to%20empirically%20evaluating%20unlearning%20algorithms%2C%0Apaving%20the%20way%20for%20the%20development%20of%20more%20effective%20unlearning%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11577v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Reliable%20Empirical%20Machine%20Unlearning%20Evaluation%3A%20A%0A%20%20Game-Theoretic%20View&entry.906535625=Yiwen%20Tu%20and%20Pingbang%20Hu%20and%20Jiaqi%20Ma&entry.1292438233=%20%20Machine%20unlearning%20is%20the%20process%20of%20updating%20machine%20learning%20models%20to%0Aremove%20the%20information%20of%20specific%20training%20data%20samples%2C%20in%20order%20to%20comply%0Awith%20data%20protection%20regulations%20that%20allow%20individuals%20to%20request%20the%20removal%0Aof%20their%20personal%20data.%20Despite%20the%20recent%20development%20of%20numerous%20unlearning%0Aalgorithms%2C%20reliable%20evaluation%20of%20these%20algorithms%20remains%20an%20open%20research%0Aquestion.%20In%20this%20work%2C%20we%20focus%20on%20membership%20inference%20attack%20%28MIA%29%20based%0Aevaluation%2C%20one%20of%20the%20most%20common%20approaches%20for%20evaluating%20unlearning%0Aalgorithms%2C%20and%20address%20various%20pitfalls%20of%20existing%20evaluation%20metrics%20that%0Alack%20reliability.%20Specifically%2C%20we%20propose%20a%20game-theoretic%20framework%20that%0Aformalizes%20the%20evaluation%20process%20as%20a%20game%20between%20unlearning%20algorithms%20and%0AMIA%20adversaries%2C%20measuring%20the%20data%20removal%20efficacy%20of%20unlearning%20algorithms%0Aby%20the%20capability%20of%20the%20MIA%20adversaries.%20Through%20careful%20design%20of%20the%20game%2C%0Awe%20demonstrate%20that%20the%20natural%20evaluation%20metric%20induced%20from%20the%20game%20enjoys%0Aprovable%20guarantees%20that%20the%20existing%20evaluation%20metrics%20fail%20to%20satisfy.%0AFurthermore%2C%20we%20propose%20a%20practical%20and%20efficient%20algorithm%20to%20estimate%20the%0Aevaluation%20metric%20induced%20from%20the%20game%2C%20and%20demonstrate%20its%20effectiveness%0Athrough%20both%20theoretical%20analysis%20and%20empirical%20experiments.%20This%20work%20presents%0Aa%20novel%20and%20reliable%20approach%20to%20empirically%20evaluating%20unlearning%20algorithms%2C%0Apaving%20the%20way%20for%20the%20development%20of%20more%20effective%20unlearning%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11577v1&entry.124074799=Read"},
{"title": "DUPE: Detection Undermining via Prompt Engineering for Deepfake Text", "author": "James Weichert and Chinecherem Dimobi", "abstract": "  As large language models (LLMs) become increasingly commonplace, concern\nabout distinguishing between human and AI text increases as well. The growing\npower of these models is of particular concern to teachers, who may worry that\nstudents will use LLMs to write school assignments. Facing a technology with\nwhich they are unfamiliar, teachers may turn to publicly-available AI text\ndetectors. Yet the accuracy of many of these detectors has not been thoroughly\nverified, posing potential harm to students who are falsely accused of academic\ndishonesty. In this paper, we evaluate three different AI text\ndetectors-Kirchenbauer et al. watermarks, ZeroGPT, and GPTZero-against human\nand AI-generated essays. We find that watermarking results in a high false\npositive rate, and that ZeroGPT has both high false positive and false negative\nrates. Further, we are able to significantly increase the false negative rate\nof all detectors by using ChatGPT 3.5 to paraphrase the original AI-generated\ntexts, thereby effectively bypassing the detectors.\n", "link": "http://arxiv.org/abs/2404.11408v1", "date": "2024-04-17", "relevancy": 1.9271, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5112}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4861}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4656}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DUPE%3A%20Detection%20Undermining%20via%20Prompt%20Engineering%20for%20Deepfake%20Text&body=Title%3A%20DUPE%3A%20Detection%20Undermining%20via%20Prompt%20Engineering%20for%20Deepfake%20Text%0AAuthor%3A%20James%20Weichert%20and%20Chinecherem%20Dimobi%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20become%20increasingly%20commonplace%2C%20concern%0Aabout%20distinguishing%20between%20human%20and%20AI%20text%20increases%20as%20well.%20The%20growing%0Apower%20of%20these%20models%20is%20of%20particular%20concern%20to%20teachers%2C%20who%20may%20worry%20that%0Astudents%20will%20use%20LLMs%20to%20write%20school%20assignments.%20Facing%20a%20technology%20with%0Awhich%20they%20are%20unfamiliar%2C%20teachers%20may%20turn%20to%20publicly-available%20AI%20text%0Adetectors.%20Yet%20the%20accuracy%20of%20many%20of%20these%20detectors%20has%20not%20been%20thoroughly%0Averified%2C%20posing%20potential%20harm%20to%20students%20who%20are%20falsely%20accused%20of%20academic%0Adishonesty.%20In%20this%20paper%2C%20we%20evaluate%20three%20different%20AI%20text%0Adetectors-Kirchenbauer%20et%20al.%20watermarks%2C%20ZeroGPT%2C%20and%20GPTZero-against%20human%0Aand%20AI-generated%20essays.%20We%20find%20that%20watermarking%20results%20in%20a%20high%20false%0Apositive%20rate%2C%20and%20that%20ZeroGPT%20has%20both%20high%20false%20positive%20and%20false%20negative%0Arates.%20Further%2C%20we%20are%20able%20to%20significantly%20increase%20the%20false%20negative%20rate%0Aof%20all%20detectors%20by%20using%20ChatGPT%203.5%20to%20paraphrase%20the%20original%20AI-generated%0Atexts%2C%20thereby%20effectively%20bypassing%20the%20detectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11408v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DUPE%3A%20Detection%20Undermining%20via%20Prompt%20Engineering%20for%20Deepfake%20Text&entry.906535625=James%20Weichert%20and%20Chinecherem%20Dimobi&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20become%20increasingly%20commonplace%2C%20concern%0Aabout%20distinguishing%20between%20human%20and%20AI%20text%20increases%20as%20well.%20The%20growing%0Apower%20of%20these%20models%20is%20of%20particular%20concern%20to%20teachers%2C%20who%20may%20worry%20that%0Astudents%20will%20use%20LLMs%20to%20write%20school%20assignments.%20Facing%20a%20technology%20with%0Awhich%20they%20are%20unfamiliar%2C%20teachers%20may%20turn%20to%20publicly-available%20AI%20text%0Adetectors.%20Yet%20the%20accuracy%20of%20many%20of%20these%20detectors%20has%20not%20been%20thoroughly%0Averified%2C%20posing%20potential%20harm%20to%20students%20who%20are%20falsely%20accused%20of%20academic%0Adishonesty.%20In%20this%20paper%2C%20we%20evaluate%20three%20different%20AI%20text%0Adetectors-Kirchenbauer%20et%20al.%20watermarks%2C%20ZeroGPT%2C%20and%20GPTZero-against%20human%0Aand%20AI-generated%20essays.%20We%20find%20that%20watermarking%20results%20in%20a%20high%20false%0Apositive%20rate%2C%20and%20that%20ZeroGPT%20has%20both%20high%20false%20positive%20and%20false%20negative%0Arates.%20Further%2C%20we%20are%20able%20to%20significantly%20increase%20the%20false%20negative%20rate%0Aof%20all%20detectors%20by%20using%20ChatGPT%203.5%20to%20paraphrase%20the%20original%20AI-generated%0Atexts%2C%20thereby%20effectively%20bypassing%20the%20detectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11408v1&entry.124074799=Read"},
{"title": "Distributed Fractional Bayesian Learning for Adaptive Optimization", "author": "Yaqun Yang and Jinlong Lei and Guanghui Wen and Yiguang Hong", "abstract": "  This paper considers a distributed adaptive optimization problem, where all\nagents only have access to their local cost functions with a common unknown\nparameter, whereas they mean to collaboratively estimate the true parameter and\nfind the optimal solution over a connected network. A general mathematical\nframework for such a problem has not been studied yet. We aim to provide\nvaluable insights for addressing parameter uncertainty in distributed\noptimization problems and simultaneously find the optimal solution. Thus, we\npropose a novel Prediction while Optimization scheme, which utilizes\ndistributed fractional Bayesian learning through weighted averaging on the\nlog-beliefs to update the beliefs of unknown parameters, and distributed\ngradient descent for renewing the estimation of the optimal solution. Then\nunder suitable assumptions, we prove that all agents' beliefs and decision\nvariables converge almost surely to the true parameter and the optimal solution\nunder the true parameter, respectively. We further establish a sublinear\nconvergence rate for the belief sequence. Finally, numerical experiments are\nimplemented to corroborate the theoretical analysis.\n", "link": "http://arxiv.org/abs/2404.11354v1", "date": "2024-04-17", "relevancy": 1.9169, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5196}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4801}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4622}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Distributed%20Fractional%20Bayesian%20Learning%20for%20Adaptive%20Optimization&body=Title%3A%20Distributed%20Fractional%20Bayesian%20Learning%20for%20Adaptive%20Optimization%0AAuthor%3A%20Yaqun%20Yang%20and%20Jinlong%20Lei%20and%20Guanghui%20Wen%20and%20Yiguang%20Hong%0AAbstract%3A%20%20%20This%20paper%20considers%20a%20distributed%20adaptive%20optimization%20problem%2C%20where%20all%0Aagents%20only%20have%20access%20to%20their%20local%20cost%20functions%20with%20a%20common%20unknown%0Aparameter%2C%20whereas%20they%20mean%20to%20collaboratively%20estimate%20the%20true%20parameter%20and%0Afind%20the%20optimal%20solution%20over%20a%20connected%20network.%20A%20general%20mathematical%0Aframework%20for%20such%20a%20problem%20has%20not%20been%20studied%20yet.%20We%20aim%20to%20provide%0Avaluable%20insights%20for%20addressing%20parameter%20uncertainty%20in%20distributed%0Aoptimization%20problems%20and%20simultaneously%20find%20the%20optimal%20solution.%20Thus%2C%20we%0Apropose%20a%20novel%20Prediction%20while%20Optimization%20scheme%2C%20which%20utilizes%0Adistributed%20fractional%20Bayesian%20learning%20through%20weighted%20averaging%20on%20the%0Alog-beliefs%20to%20update%20the%20beliefs%20of%20unknown%20parameters%2C%20and%20distributed%0Agradient%20descent%20for%20renewing%20the%20estimation%20of%20the%20optimal%20solution.%20Then%0Aunder%20suitable%20assumptions%2C%20we%20prove%20that%20all%20agents%27%20beliefs%20and%20decision%0Avariables%20converge%20almost%20surely%20to%20the%20true%20parameter%20and%20the%20optimal%20solution%0Aunder%20the%20true%20parameter%2C%20respectively.%20We%20further%20establish%20a%20sublinear%0Aconvergence%20rate%20for%20the%20belief%20sequence.%20Finally%2C%20numerical%20experiments%20are%0Aimplemented%20to%20corroborate%20the%20theoretical%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11354v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20Fractional%20Bayesian%20Learning%20for%20Adaptive%20Optimization&entry.906535625=Yaqun%20Yang%20and%20Jinlong%20Lei%20and%20Guanghui%20Wen%20and%20Yiguang%20Hong&entry.1292438233=%20%20This%20paper%20considers%20a%20distributed%20adaptive%20optimization%20problem%2C%20where%20all%0Aagents%20only%20have%20access%20to%20their%20local%20cost%20functions%20with%20a%20common%20unknown%0Aparameter%2C%20whereas%20they%20mean%20to%20collaboratively%20estimate%20the%20true%20parameter%20and%0Afind%20the%20optimal%20solution%20over%20a%20connected%20network.%20A%20general%20mathematical%0Aframework%20for%20such%20a%20problem%20has%20not%20been%20studied%20yet.%20We%20aim%20to%20provide%0Avaluable%20insights%20for%20addressing%20parameter%20uncertainty%20in%20distributed%0Aoptimization%20problems%20and%20simultaneously%20find%20the%20optimal%20solution.%20Thus%2C%20we%0Apropose%20a%20novel%20Prediction%20while%20Optimization%20scheme%2C%20which%20utilizes%0Adistributed%20fractional%20Bayesian%20learning%20through%20weighted%20averaging%20on%20the%0Alog-beliefs%20to%20update%20the%20beliefs%20of%20unknown%20parameters%2C%20and%20distributed%0Agradient%20descent%20for%20renewing%20the%20estimation%20of%20the%20optimal%20solution.%20Then%0Aunder%20suitable%20assumptions%2C%20we%20prove%20that%20all%20agents%27%20beliefs%20and%20decision%0Avariables%20converge%20almost%20surely%20to%20the%20true%20parameter%20and%20the%20optimal%20solution%0Aunder%20the%20true%20parameter%2C%20respectively.%20We%20further%20establish%20a%20sublinear%0Aconvergence%20rate%20for%20the%20belief%20sequence.%20Finally%2C%20numerical%20experiments%20are%0Aimplemented%20to%20corroborate%20the%20theoretical%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11354v1&entry.124074799=Read"},
{"title": "Achieving Rotation Invariance in Convolution Operations: Shifting from\n  Data-Driven to Mechanism-Assured", "author": "Hanlin Mo and Guoying Zhao", "abstract": "  Achieving rotation invariance in deep neural networks without relying on data\nhas always been a hot research topic. Intrinsic rotation invariance can enhance\nthe model's feature representation capability, enabling better performance in\ntasks such as multi-orientation object recognition and detection. Based on\nvarious types of non-learnable operators, including gradient, sort, local\nbinary pattern, maximum, etc., this paper designs a set of new convolution\noperations that are natually invariant to arbitrary rotations. Unlike most\nprevious studies, these rotation-invariant convolutions (RIConvs) have the same\nnumber of learnable parameters and a similar computational process as\nconventional convolution operations, allowing them to be interchangeable. Using\nthe MNIST-Rot dataset, we first verify the invariance of these RIConvs under\nvarious rotation angles and compare their performance with previous\nrotation-invariant convolutional neural networks (RI-CNNs). Two types of\nRIConvs based on gradient operators achieve state-of-the-art results.\nSubsequently, we combine RIConvs with different types and depths of classic CNN\nbackbones. Using the OuTex_00012, MTARSI, and NWPU-RESISC-45 datasets, we test\ntheir performance on texture recognition, aircraft type recognition, and remote\nsensing image classification tasks. The results show that RIConvs significantly\nimprove the accuracy of these CNN backbones, especially when the training data\nis limited. Furthermore, we find that even with data augmentation, RIConvs can\nfurther enhance model performance.\n", "link": "http://arxiv.org/abs/2404.11309v1", "date": "2024-04-17", "relevancy": 1.9141, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5137}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4797}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4632}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Achieving%20Rotation%20Invariance%20in%20Convolution%20Operations%3A%20Shifting%20from%0A%20%20Data-Driven%20to%20Mechanism-Assured&body=Title%3A%20Achieving%20Rotation%20Invariance%20in%20Convolution%20Operations%3A%20Shifting%20from%0A%20%20Data-Driven%20to%20Mechanism-Assured%0AAuthor%3A%20Hanlin%20Mo%20and%20Guoying%20Zhao%0AAbstract%3A%20%20%20Achieving%20rotation%20invariance%20in%20deep%20neural%20networks%20without%20relying%20on%20data%0Ahas%20always%20been%20a%20hot%20research%20topic.%20Intrinsic%20rotation%20invariance%20can%20enhance%0Athe%20model%27s%20feature%20representation%20capability%2C%20enabling%20better%20performance%20in%0Atasks%20such%20as%20multi-orientation%20object%20recognition%20and%20detection.%20Based%20on%0Avarious%20types%20of%20non-learnable%20operators%2C%20including%20gradient%2C%20sort%2C%20local%0Abinary%20pattern%2C%20maximum%2C%20etc.%2C%20this%20paper%20designs%20a%20set%20of%20new%20convolution%0Aoperations%20that%20are%20natually%20invariant%20to%20arbitrary%20rotations.%20Unlike%20most%0Aprevious%20studies%2C%20these%20rotation-invariant%20convolutions%20%28RIConvs%29%20have%20the%20same%0Anumber%20of%20learnable%20parameters%20and%20a%20similar%20computational%20process%20as%0Aconventional%20convolution%20operations%2C%20allowing%20them%20to%20be%20interchangeable.%20Using%0Athe%20MNIST-Rot%20dataset%2C%20we%20first%20verify%20the%20invariance%20of%20these%20RIConvs%20under%0Avarious%20rotation%20angles%20and%20compare%20their%20performance%20with%20previous%0Arotation-invariant%20convolutional%20neural%20networks%20%28RI-CNNs%29.%20Two%20types%20of%0ARIConvs%20based%20on%20gradient%20operators%20achieve%20state-of-the-art%20results.%0ASubsequently%2C%20we%20combine%20RIConvs%20with%20different%20types%20and%20depths%20of%20classic%20CNN%0Abackbones.%20Using%20the%20OuTex_00012%2C%20MTARSI%2C%20and%20NWPU-RESISC-45%20datasets%2C%20we%20test%0Atheir%20performance%20on%20texture%20recognition%2C%20aircraft%20type%20recognition%2C%20and%20remote%0Asensing%20image%20classification%20tasks.%20The%20results%20show%20that%20RIConvs%20significantly%0Aimprove%20the%20accuracy%20of%20these%20CNN%20backbones%2C%20especially%20when%20the%20training%20data%0Ais%20limited.%20Furthermore%2C%20we%20find%20that%20even%20with%20data%20augmentation%2C%20RIConvs%20can%0Afurther%20enhance%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11309v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Achieving%20Rotation%20Invariance%20in%20Convolution%20Operations%3A%20Shifting%20from%0A%20%20Data-Driven%20to%20Mechanism-Assured&entry.906535625=Hanlin%20Mo%20and%20Guoying%20Zhao&entry.1292438233=%20%20Achieving%20rotation%20invariance%20in%20deep%20neural%20networks%20without%20relying%20on%20data%0Ahas%20always%20been%20a%20hot%20research%20topic.%20Intrinsic%20rotation%20invariance%20can%20enhance%0Athe%20model%27s%20feature%20representation%20capability%2C%20enabling%20better%20performance%20in%0Atasks%20such%20as%20multi-orientation%20object%20recognition%20and%20detection.%20Based%20on%0Avarious%20types%20of%20non-learnable%20operators%2C%20including%20gradient%2C%20sort%2C%20local%0Abinary%20pattern%2C%20maximum%2C%20etc.%2C%20this%20paper%20designs%20a%20set%20of%20new%20convolution%0Aoperations%20that%20are%20natually%20invariant%20to%20arbitrary%20rotations.%20Unlike%20most%0Aprevious%20studies%2C%20these%20rotation-invariant%20convolutions%20%28RIConvs%29%20have%20the%20same%0Anumber%20of%20learnable%20parameters%20and%20a%20similar%20computational%20process%20as%0Aconventional%20convolution%20operations%2C%20allowing%20them%20to%20be%20interchangeable.%20Using%0Athe%20MNIST-Rot%20dataset%2C%20we%20first%20verify%20the%20invariance%20of%20these%20RIConvs%20under%0Avarious%20rotation%20angles%20and%20compare%20their%20performance%20with%20previous%0Arotation-invariant%20convolutional%20neural%20networks%20%28RI-CNNs%29.%20Two%20types%20of%0ARIConvs%20based%20on%20gradient%20operators%20achieve%20state-of-the-art%20results.%0ASubsequently%2C%20we%20combine%20RIConvs%20with%20different%20types%20and%20depths%20of%20classic%20CNN%0Abackbones.%20Using%20the%20OuTex_00012%2C%20MTARSI%2C%20and%20NWPU-RESISC-45%20datasets%2C%20we%20test%0Atheir%20performance%20on%20texture%20recognition%2C%20aircraft%20type%20recognition%2C%20and%20remote%0Asensing%20image%20classification%20tasks.%20The%20results%20show%20that%20RIConvs%20significantly%0Aimprove%20the%20accuracy%20of%20these%20CNN%20backbones%2C%20especially%20when%20the%20training%20data%0Ais%20limited.%20Furthermore%2C%20we%20find%20that%20even%20with%20data%20augmentation%2C%20RIConvs%20can%0Afurther%20enhance%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11309v1&entry.124074799=Read"},
{"title": "Online Robot Navigation and Manipulation with Distilled Vision-Language\n  Models", "author": "Kangcheng Liu and Xinhu Zheng and Chaoqun Wang and Hesheng Wang and Ming Liu and Kai Tang", "abstract": "  Autonomous robot navigation within the dynamic unknown environment is of\ncrucial significance for mobile robotic applications including robot navigation\nin last-mile delivery and robot-enabled automated supplies in industrial and\nhospital delivery applications. Current solutions still suffer from\nlimitations, such as the robot cannot recognize unknown objects in real time\nand cannot navigate freely in a dynamic, narrow, and complex environment. We\npropose a complete software framework for autonomous robot perception and\nnavigation within very dense obstacles and dense human crowds. First, we\npropose a framework that accurately detects and segments open-world object\ncategories in a zero-shot manner, which overcomes the over-segmentation\nlimitation of the current SAM model. Second, we proposed the distillation\nstrategy to distill the knowledge to segment the free space of the walkway for\nrobot navigation without the label. In the meantime, we design the trimming\nstrategy that works collaboratively with distillation to enable lightweight\ninference to deploy the neural network on edge devices such as NVIDIA-TX2 or\nXavier NX during autonomous navigation. Integrated into the robot navigation\nsystem, extensive experiments demonstrate that our proposed framework has\nachieved superior performance in terms of both accuracy and efficiency in robot\nscene perception and autonomous robot navigation.\n", "link": "http://arxiv.org/abs/2401.17083v3", "date": "2024-04-17", "relevancy": 1.9109, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6508}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6243}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6151}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Online%20Robot%20Navigation%20and%20Manipulation%20with%20Distilled%20Vision-Language%0A%20%20Models&body=Title%3A%20Online%20Robot%20Navigation%20and%20Manipulation%20with%20Distilled%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Kangcheng%20Liu%20and%20Xinhu%20Zheng%20and%20Chaoqun%20Wang%20and%20Hesheng%20Wang%20and%20Ming%20Liu%20and%20Kai%20Tang%0AAbstract%3A%20%20%20Autonomous%20robot%20navigation%20within%20the%20dynamic%20unknown%20environment%20is%20of%0Acrucial%20significance%20for%20mobile%20robotic%20applications%20including%20robot%20navigation%0Ain%20last-mile%20delivery%20and%20robot-enabled%20automated%20supplies%20in%20industrial%20and%0Ahospital%20delivery%20applications.%20Current%20solutions%20still%20suffer%20from%0Alimitations%2C%20such%20as%20the%20robot%20cannot%20recognize%20unknown%20objects%20in%20real%20time%0Aand%20cannot%20navigate%20freely%20in%20a%20dynamic%2C%20narrow%2C%20and%20complex%20environment.%20We%0Apropose%20a%20complete%20software%20framework%20for%20autonomous%20robot%20perception%20and%0Anavigation%20within%20very%20dense%20obstacles%20and%20dense%20human%20crowds.%20First%2C%20we%0Apropose%20a%20framework%20that%20accurately%20detects%20and%20segments%20open-world%20object%0Acategories%20in%20a%20zero-shot%20manner%2C%20which%20overcomes%20the%20over-segmentation%0Alimitation%20of%20the%20current%20SAM%20model.%20Second%2C%20we%20proposed%20the%20distillation%0Astrategy%20to%20distill%20the%20knowledge%20to%20segment%20the%20free%20space%20of%20the%20walkway%20for%0Arobot%20navigation%20without%20the%20label.%20In%20the%20meantime%2C%20we%20design%20the%20trimming%0Astrategy%20that%20works%20collaboratively%20with%20distillation%20to%20enable%20lightweight%0Ainference%20to%20deploy%20the%20neural%20network%20on%20edge%20devices%20such%20as%20NVIDIA-TX2%20or%0AXavier%20NX%20during%20autonomous%20navigation.%20Integrated%20into%20the%20robot%20navigation%0Asystem%2C%20extensive%20experiments%20demonstrate%20that%20our%20proposed%20framework%20has%0Aachieved%20superior%20performance%20in%20terms%20of%20both%20accuracy%20and%20efficiency%20in%20robot%0Ascene%20perception%20and%20autonomous%20robot%20navigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.17083v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Robot%20Navigation%20and%20Manipulation%20with%20Distilled%20Vision-Language%0A%20%20Models&entry.906535625=Kangcheng%20Liu%20and%20Xinhu%20Zheng%20and%20Chaoqun%20Wang%20and%20Hesheng%20Wang%20and%20Ming%20Liu%20and%20Kai%20Tang&entry.1292438233=%20%20Autonomous%20robot%20navigation%20within%20the%20dynamic%20unknown%20environment%20is%20of%0Acrucial%20significance%20for%20mobile%20robotic%20applications%20including%20robot%20navigation%0Ain%20last-mile%20delivery%20and%20robot-enabled%20automated%20supplies%20in%20industrial%20and%0Ahospital%20delivery%20applications.%20Current%20solutions%20still%20suffer%20from%0Alimitations%2C%20such%20as%20the%20robot%20cannot%20recognize%20unknown%20objects%20in%20real%20time%0Aand%20cannot%20navigate%20freely%20in%20a%20dynamic%2C%20narrow%2C%20and%20complex%20environment.%20We%0Apropose%20a%20complete%20software%20framework%20for%20autonomous%20robot%20perception%20and%0Anavigation%20within%20very%20dense%20obstacles%20and%20dense%20human%20crowds.%20First%2C%20we%0Apropose%20a%20framework%20that%20accurately%20detects%20and%20segments%20open-world%20object%0Acategories%20in%20a%20zero-shot%20manner%2C%20which%20overcomes%20the%20over-segmentation%0Alimitation%20of%20the%20current%20SAM%20model.%20Second%2C%20we%20proposed%20the%20distillation%0Astrategy%20to%20distill%20the%20knowledge%20to%20segment%20the%20free%20space%20of%20the%20walkway%20for%0Arobot%20navigation%20without%20the%20label.%20In%20the%20meantime%2C%20we%20design%20the%20trimming%0Astrategy%20that%20works%20collaboratively%20with%20distillation%20to%20enable%20lightweight%0Ainference%20to%20deploy%20the%20neural%20network%20on%20edge%20devices%20such%20as%20NVIDIA-TX2%20or%0AXavier%20NX%20during%20autonomous%20navigation.%20Integrated%20into%20the%20robot%20navigation%0Asystem%2C%20extensive%20experiments%20demonstrate%20that%20our%20proposed%20framework%20has%0Aachieved%20superior%20performance%20in%20terms%20of%20both%20accuracy%20and%20efficiency%20in%20robot%0Ascene%20perception%20and%20autonomous%20robot%20navigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.17083v3&entry.124074799=Read"},
{"title": "Solving morphological analogies: from retrieval to generation", "author": "Esteban Marquer and Miguel Couceiro", "abstract": "  Analogical inference is a remarkable capability of human reasoning, and has\nbeen used to solve hard reasoning tasks. Analogy based reasoning (AR) has\ngained increasing interest from the artificial intelligence community and has\nshown its potential in multiple machine learning tasks such as classification,\ndecision making and recommendation with competitive results. We propose a deep\nlearning (DL) framework to address and tackle two key tasks in AR: analogy\ndetection and solving. The framework is thoroughly tested on the Siganalogies\ndataset of morphological analogical proportions (APs) between words, and shown\nto outperform symbolic approaches in many languages. Previous work have\nexplored the behavior of the Analogy Neural Network for classification (ANNc)\non analogy detection and of the Analogy Neural Network for retrieval (ANNr) on\nanalogy solving by retrieval, as well as the potential of an autoencoder (AE)\nfor analogy solving by generating the solution word. In this article we\nsummarize these findings and we extend them by combining ANNr and the AE\nembedding model, and checking the performance of ANNc as an retrieval method.\nThe combination of ANNr and AE outperforms the other approaches in almost all\ncases, and ANNc as a retrieval method achieves competitive or better\nperformance than 3CosMul. We conclude with general guidelines on using our\nframework to tackle APs with DL.\n", "link": "http://arxiv.org/abs/2303.18062v2", "date": "2024-04-17", "relevancy": 1.9018, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4901}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4671}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4642}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Solving%20morphological%20analogies%3A%20from%20retrieval%20to%20generation&body=Title%3A%20Solving%20morphological%20analogies%3A%20from%20retrieval%20to%20generation%0AAuthor%3A%20Esteban%20Marquer%20and%20Miguel%20Couceiro%0AAbstract%3A%20%20%20Analogical%20inference%20is%20a%20remarkable%20capability%20of%20human%20reasoning%2C%20and%20has%0Abeen%20used%20to%20solve%20hard%20reasoning%20tasks.%20Analogy%20based%20reasoning%20%28AR%29%20has%0Agained%20increasing%20interest%20from%20the%20artificial%20intelligence%20community%20and%20has%0Ashown%20its%20potential%20in%20multiple%20machine%20learning%20tasks%20such%20as%20classification%2C%0Adecision%20making%20and%20recommendation%20with%20competitive%20results.%20We%20propose%20a%20deep%0Alearning%20%28DL%29%20framework%20to%20address%20and%20tackle%20two%20key%20tasks%20in%20AR%3A%20analogy%0Adetection%20and%20solving.%20The%20framework%20is%20thoroughly%20tested%20on%20the%20Siganalogies%0Adataset%20of%20morphological%20analogical%20proportions%20%28APs%29%20between%20words%2C%20and%20shown%0Ato%20outperform%20symbolic%20approaches%20in%20many%20languages.%20Previous%20work%20have%0Aexplored%20the%20behavior%20of%20the%20Analogy%20Neural%20Network%20for%20classification%20%28ANNc%29%0Aon%20analogy%20detection%20and%20of%20the%20Analogy%20Neural%20Network%20for%20retrieval%20%28ANNr%29%20on%0Aanalogy%20solving%20by%20retrieval%2C%20as%20well%20as%20the%20potential%20of%20an%20autoencoder%20%28AE%29%0Afor%20analogy%20solving%20by%20generating%20the%20solution%20word.%20In%20this%20article%20we%0Asummarize%20these%20findings%20and%20we%20extend%20them%20by%20combining%20ANNr%20and%20the%20AE%0Aembedding%20model%2C%20and%20checking%20the%20performance%20of%20ANNc%20as%20an%20retrieval%20method.%0AThe%20combination%20of%20ANNr%20and%20AE%20outperforms%20the%20other%20approaches%20in%20almost%20all%0Acases%2C%20and%20ANNc%20as%20a%20retrieval%20method%20achieves%20competitive%20or%20better%0Aperformance%20than%203CosMul.%20We%20conclude%20with%20general%20guidelines%20on%20using%20our%0Aframework%20to%20tackle%20APs%20with%20DL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.18062v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20morphological%20analogies%3A%20from%20retrieval%20to%20generation&entry.906535625=Esteban%20Marquer%20and%20Miguel%20Couceiro&entry.1292438233=%20%20Analogical%20inference%20is%20a%20remarkable%20capability%20of%20human%20reasoning%2C%20and%20has%0Abeen%20used%20to%20solve%20hard%20reasoning%20tasks.%20Analogy%20based%20reasoning%20%28AR%29%20has%0Agained%20increasing%20interest%20from%20the%20artificial%20intelligence%20community%20and%20has%0Ashown%20its%20potential%20in%20multiple%20machine%20learning%20tasks%20such%20as%20classification%2C%0Adecision%20making%20and%20recommendation%20with%20competitive%20results.%20We%20propose%20a%20deep%0Alearning%20%28DL%29%20framework%20to%20address%20and%20tackle%20two%20key%20tasks%20in%20AR%3A%20analogy%0Adetection%20and%20solving.%20The%20framework%20is%20thoroughly%20tested%20on%20the%20Siganalogies%0Adataset%20of%20morphological%20analogical%20proportions%20%28APs%29%20between%20words%2C%20and%20shown%0Ato%20outperform%20symbolic%20approaches%20in%20many%20languages.%20Previous%20work%20have%0Aexplored%20the%20behavior%20of%20the%20Analogy%20Neural%20Network%20for%20classification%20%28ANNc%29%0Aon%20analogy%20detection%20and%20of%20the%20Analogy%20Neural%20Network%20for%20retrieval%20%28ANNr%29%20on%0Aanalogy%20solving%20by%20retrieval%2C%20as%20well%20as%20the%20potential%20of%20an%20autoencoder%20%28AE%29%0Afor%20analogy%20solving%20by%20generating%20the%20solution%20word.%20In%20this%20article%20we%0Asummarize%20these%20findings%20and%20we%20extend%20them%20by%20combining%20ANNr%20and%20the%20AE%0Aembedding%20model%2C%20and%20checking%20the%20performance%20of%20ANNc%20as%20an%20retrieval%20method.%0AThe%20combination%20of%20ANNr%20and%20AE%20outperforms%20the%20other%20approaches%20in%20almost%20all%0Acases%2C%20and%20ANNc%20as%20a%20retrieval%20method%20achieves%20competitive%20or%20better%0Aperformance%20than%203CosMul.%20We%20conclude%20with%20general%20guidelines%20on%20using%20our%0Aframework%20to%20tackle%20APs%20with%20DL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.18062v2&entry.124074799=Read"},
{"title": "EPIM: Efficient Processing-In-Memory Accelerators based on Epitome", "author": "Chenyu Wang and Zhen Dong and Daquan Zhou and Zhenhua Zhu and Yu Wang and Jiashi Feng and Kurt Keutzer", "abstract": "  The utilization of large-scale neural networks on Processing-In-Memory (PIM)\naccelerators encounters challenges due to constrained on-chip memory capacity.\nTo tackle this issue, current works explore model compression algorithms to\nreduce the size of Convolutional Neural Networks (CNNs). Most of these\nalgorithms either aim to represent neural operators with reduced-size\nparameters (e.g., quantization) or search for the best combinations of neural\noperators (e.g., neural architecture search). Designing neural operators to\nalign with PIM accelerators' specifications is an area that warrants further\nstudy. In this paper, we introduce the Epitome, a lightweight neural operator\noffering convolution-like functionality, to craft memory-efficient CNN\noperators for PIM accelerators (EPIM). On the software side, we evaluate\nepitomes' latency and energy on PIM accelerators and introduce a PIM-aware\nlayer-wise design method to enhance their hardware efficiency. We apply\nepitome-aware quantization to further reduce the size of epitomes. On the\nhardware side, we modify the datapath of current PIM accelerators to\naccommodate epitomes and implement a feature map reuse technique to reduce\ncomputation cost. Experimental results reveal that our 3-bit quantized\nEPIM-ResNet50 attains 71.59% top-1 accuracy on ImageNet, reducing crossbar\nareas by 30.65 times. EPIM surpasses the state-of-the-art pruning methods on\nPIM.\n", "link": "http://arxiv.org/abs/2311.07620v3", "date": "2024-04-17", "relevancy": 1.8998, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5098}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4719}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4641}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20EPIM%3A%20Efficient%20Processing-In-Memory%20Accelerators%20based%20on%20Epitome&body=Title%3A%20EPIM%3A%20Efficient%20Processing-In-Memory%20Accelerators%20based%20on%20Epitome%0AAuthor%3A%20Chenyu%20Wang%20and%20Zhen%20Dong%20and%20Daquan%20Zhou%20and%20Zhenhua%20Zhu%20and%20Yu%20Wang%20and%20Jiashi%20Feng%20and%20Kurt%20Keutzer%0AAbstract%3A%20%20%20The%20utilization%20of%20large-scale%20neural%20networks%20on%20Processing-In-Memory%20%28PIM%29%0Aaccelerators%20encounters%20challenges%20due%20to%20constrained%20on-chip%20memory%20capacity.%0ATo%20tackle%20this%20issue%2C%20current%20works%20explore%20model%20compression%20algorithms%20to%0Areduce%20the%20size%20of%20Convolutional%20Neural%20Networks%20%28CNNs%29.%20Most%20of%20these%0Aalgorithms%20either%20aim%20to%20represent%20neural%20operators%20with%20reduced-size%0Aparameters%20%28e.g.%2C%20quantization%29%20or%20search%20for%20the%20best%20combinations%20of%20neural%0Aoperators%20%28e.g.%2C%20neural%20architecture%20search%29.%20Designing%20neural%20operators%20to%0Aalign%20with%20PIM%20accelerators%27%20specifications%20is%20an%20area%20that%20warrants%20further%0Astudy.%20In%20this%20paper%2C%20we%20introduce%20the%20Epitome%2C%20a%20lightweight%20neural%20operator%0Aoffering%20convolution-like%20functionality%2C%20to%20craft%20memory-efficient%20CNN%0Aoperators%20for%20PIM%20accelerators%20%28EPIM%29.%20On%20the%20software%20side%2C%20we%20evaluate%0Aepitomes%27%20latency%20and%20energy%20on%20PIM%20accelerators%20and%20introduce%20a%20PIM-aware%0Alayer-wise%20design%20method%20to%20enhance%20their%20hardware%20efficiency.%20We%20apply%0Aepitome-aware%20quantization%20to%20further%20reduce%20the%20size%20of%20epitomes.%20On%20the%0Ahardware%20side%2C%20we%20modify%20the%20datapath%20of%20current%20PIM%20accelerators%20to%0Aaccommodate%20epitomes%20and%20implement%20a%20feature%20map%20reuse%20technique%20to%20reduce%0Acomputation%20cost.%20Experimental%20results%20reveal%20that%20our%203-bit%20quantized%0AEPIM-ResNet50%20attains%2071.59%25%20top-1%20accuracy%20on%20ImageNet%2C%20reducing%20crossbar%0Aareas%20by%2030.65%20times.%20EPIM%20surpasses%20the%20state-of-the-art%20pruning%20methods%20on%0APIM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.07620v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EPIM%3A%20Efficient%20Processing-In-Memory%20Accelerators%20based%20on%20Epitome&entry.906535625=Chenyu%20Wang%20and%20Zhen%20Dong%20and%20Daquan%20Zhou%20and%20Zhenhua%20Zhu%20and%20Yu%20Wang%20and%20Jiashi%20Feng%20and%20Kurt%20Keutzer&entry.1292438233=%20%20The%20utilization%20of%20large-scale%20neural%20networks%20on%20Processing-In-Memory%20%28PIM%29%0Aaccelerators%20encounters%20challenges%20due%20to%20constrained%20on-chip%20memory%20capacity.%0ATo%20tackle%20this%20issue%2C%20current%20works%20explore%20model%20compression%20algorithms%20to%0Areduce%20the%20size%20of%20Convolutional%20Neural%20Networks%20%28CNNs%29.%20Most%20of%20these%0Aalgorithms%20either%20aim%20to%20represent%20neural%20operators%20with%20reduced-size%0Aparameters%20%28e.g.%2C%20quantization%29%20or%20search%20for%20the%20best%20combinations%20of%20neural%0Aoperators%20%28e.g.%2C%20neural%20architecture%20search%29.%20Designing%20neural%20operators%20to%0Aalign%20with%20PIM%20accelerators%27%20specifications%20is%20an%20area%20that%20warrants%20further%0Astudy.%20In%20this%20paper%2C%20we%20introduce%20the%20Epitome%2C%20a%20lightweight%20neural%20operator%0Aoffering%20convolution-like%20functionality%2C%20to%20craft%20memory-efficient%20CNN%0Aoperators%20for%20PIM%20accelerators%20%28EPIM%29.%20On%20the%20software%20side%2C%20we%20evaluate%0Aepitomes%27%20latency%20and%20energy%20on%20PIM%20accelerators%20and%20introduce%20a%20PIM-aware%0Alayer-wise%20design%20method%20to%20enhance%20their%20hardware%20efficiency.%20We%20apply%0Aepitome-aware%20quantization%20to%20further%20reduce%20the%20size%20of%20epitomes.%20On%20the%0Ahardware%20side%2C%20we%20modify%20the%20datapath%20of%20current%20PIM%20accelerators%20to%0Aaccommodate%20epitomes%20and%20implement%20a%20feature%20map%20reuse%20technique%20to%20reduce%0Acomputation%20cost.%20Experimental%20results%20reveal%20that%20our%203-bit%20quantized%0AEPIM-ResNet50%20attains%2071.59%25%20top-1%20accuracy%20on%20ImageNet%2C%20reducing%20crossbar%0Aareas%20by%2030.65%20times.%20EPIM%20surpasses%20the%20state-of-the-art%20pruning%20methods%20on%0APIM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07620v3&entry.124074799=Read"},
{"title": "Provable Reward-Agnostic Preference-Based Reinforcement Learning", "author": "Wenhao Zhan and Masatoshi Uehara and Wen Sun and Jason D. Lee", "abstract": "  Preference-based Reinforcement Learning (PbRL) is a paradigm in which an RL\nagent learns to optimize a task using pair-wise preference-based feedback over\ntrajectories, rather than explicit reward signals. While PbRL has demonstrated\npractical success in fine-tuning language models, existing theoretical work\nfocuses on regret minimization and fails to capture most of the practical\nframeworks. In this study, we fill in such a gap between theoretical PbRL and\npractical algorithms by proposing a theoretical reward-agnostic PbRL framework\nwhere exploratory trajectories that enable accurate learning of hidden reward\nfunctions are acquired before collecting any human feedback. Theoretical\nanalysis demonstrates that our algorithm requires less human feedback for\nlearning the optimal policy under preference-based models with linear\nparameterization and unknown transitions, compared to the existing theoretical\nliterature. Specifically, our framework can incorporate linear and low-rank\nMDPs with efficient sample complexity. Additionally, we investigate\nreward-agnostic RL with action-based comparison feedback and introduce an\nefficient querying algorithm tailored to this scenario.\n", "link": "http://arxiv.org/abs/2305.18505v3", "date": "2024-04-17", "relevancy": 1.8976, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5338}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4656}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4595}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Provable%20Reward-Agnostic%20Preference-Based%20Reinforcement%20Learning&body=Title%3A%20Provable%20Reward-Agnostic%20Preference-Based%20Reinforcement%20Learning%0AAuthor%3A%20Wenhao%20Zhan%20and%20Masatoshi%20Uehara%20and%20Wen%20Sun%20and%20Jason%20D.%20Lee%0AAbstract%3A%20%20%20Preference-based%20Reinforcement%20Learning%20%28PbRL%29%20is%20a%20paradigm%20in%20which%20an%20RL%0Aagent%20learns%20to%20optimize%20a%20task%20using%20pair-wise%20preference-based%20feedback%20over%0Atrajectories%2C%20rather%20than%20explicit%20reward%20signals.%20While%20PbRL%20has%20demonstrated%0Apractical%20success%20in%20fine-tuning%20language%20models%2C%20existing%20theoretical%20work%0Afocuses%20on%20regret%20minimization%20and%20fails%20to%20capture%20most%20of%20the%20practical%0Aframeworks.%20In%20this%20study%2C%20we%20fill%20in%20such%20a%20gap%20between%20theoretical%20PbRL%20and%0Apractical%20algorithms%20by%20proposing%20a%20theoretical%20reward-agnostic%20PbRL%20framework%0Awhere%20exploratory%20trajectories%20that%20enable%20accurate%20learning%20of%20hidden%20reward%0Afunctions%20are%20acquired%20before%20collecting%20any%20human%20feedback.%20Theoretical%0Aanalysis%20demonstrates%20that%20our%20algorithm%20requires%20less%20human%20feedback%20for%0Alearning%20the%20optimal%20policy%20under%20preference-based%20models%20with%20linear%0Aparameterization%20and%20unknown%20transitions%2C%20compared%20to%20the%20existing%20theoretical%0Aliterature.%20Specifically%2C%20our%20framework%20can%20incorporate%20linear%20and%20low-rank%0AMDPs%20with%20efficient%20sample%20complexity.%20Additionally%2C%20we%20investigate%0Areward-agnostic%20RL%20with%20action-based%20comparison%20feedback%20and%20introduce%20an%0Aefficient%20querying%20algorithm%20tailored%20to%20this%20scenario.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.18505v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provable%20Reward-Agnostic%20Preference-Based%20Reinforcement%20Learning&entry.906535625=Wenhao%20Zhan%20and%20Masatoshi%20Uehara%20and%20Wen%20Sun%20and%20Jason%20D.%20Lee&entry.1292438233=%20%20Preference-based%20Reinforcement%20Learning%20%28PbRL%29%20is%20a%20paradigm%20in%20which%20an%20RL%0Aagent%20learns%20to%20optimize%20a%20task%20using%20pair-wise%20preference-based%20feedback%20over%0Atrajectories%2C%20rather%20than%20explicit%20reward%20signals.%20While%20PbRL%20has%20demonstrated%0Apractical%20success%20in%20fine-tuning%20language%20models%2C%20existing%20theoretical%20work%0Afocuses%20on%20regret%20minimization%20and%20fails%20to%20capture%20most%20of%20the%20practical%0Aframeworks.%20In%20this%20study%2C%20we%20fill%20in%20such%20a%20gap%20between%20theoretical%20PbRL%20and%0Apractical%20algorithms%20by%20proposing%20a%20theoretical%20reward-agnostic%20PbRL%20framework%0Awhere%20exploratory%20trajectories%20that%20enable%20accurate%20learning%20of%20hidden%20reward%0Afunctions%20are%20acquired%20before%20collecting%20any%20human%20feedback.%20Theoretical%0Aanalysis%20demonstrates%20that%20our%20algorithm%20requires%20less%20human%20feedback%20for%0Alearning%20the%20optimal%20policy%20under%20preference-based%20models%20with%20linear%0Aparameterization%20and%20unknown%20transitions%2C%20compared%20to%20the%20existing%20theoretical%0Aliterature.%20Specifically%2C%20our%20framework%20can%20incorporate%20linear%20and%20low-rank%0AMDPs%20with%20efficient%20sample%20complexity.%20Additionally%2C%20we%20investigate%0Areward-agnostic%20RL%20with%20action-based%20comparison%20feedback%20and%20introduce%20an%0Aefficient%20querying%20algorithm%20tailored%20to%20this%20scenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.18505v3&entry.124074799=Read"},
{"title": "Variational Bayesian Last Layers", "author": "James Harrison and John Willes and Jasper Snoek", "abstract": "  We introduce a deterministic variational formulation for training Bayesian\nlast layer neural networks. This yields a sampling-free, single-pass model and\nloss that effectively improves uncertainty estimation. Our variational Bayesian\nlast layer (VBLL) can be trained and evaluated with only quadratic complexity\nin last layer width, and is thus (nearly) computationally free to add to\nstandard architectures. We experimentally investigate VBLLs, and show that they\nimprove predictive accuracy, calibration, and out of distribution detection\nover baselines across both regression and classification. Finally, we\ninvestigate combining VBLL layers with variational Bayesian feature learning,\nyielding a lower variance collapsed variational inference method for Bayesian\nneural networks.\n", "link": "http://arxiv.org/abs/2404.11599v1", "date": "2024-04-17", "relevancy": 1.8973, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5064}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4758}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.46}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Variational%20Bayesian%20Last%20Layers&body=Title%3A%20Variational%20Bayesian%20Last%20Layers%0AAuthor%3A%20James%20Harrison%20and%20John%20Willes%20and%20Jasper%20Snoek%0AAbstract%3A%20%20%20We%20introduce%20a%20deterministic%20variational%20formulation%20for%20training%20Bayesian%0Alast%20layer%20neural%20networks.%20This%20yields%20a%20sampling-free%2C%20single-pass%20model%20and%0Aloss%20that%20effectively%20improves%20uncertainty%20estimation.%20Our%20variational%20Bayesian%0Alast%20layer%20%28VBLL%29%20can%20be%20trained%20and%20evaluated%20with%20only%20quadratic%20complexity%0Ain%20last%20layer%20width%2C%20and%20is%20thus%20%28nearly%29%20computationally%20free%20to%20add%20to%0Astandard%20architectures.%20We%20experimentally%20investigate%20VBLLs%2C%20and%20show%20that%20they%0Aimprove%20predictive%20accuracy%2C%20calibration%2C%20and%20out%20of%20distribution%20detection%0Aover%20baselines%20across%20both%20regression%20and%20classification.%20Finally%2C%20we%0Ainvestigate%20combining%20VBLL%20layers%20with%20variational%20Bayesian%20feature%20learning%2C%0Ayielding%20a%20lower%20variance%20collapsed%20variational%20inference%20method%20for%20Bayesian%0Aneural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11599v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Bayesian%20Last%20Layers&entry.906535625=James%20Harrison%20and%20John%20Willes%20and%20Jasper%20Snoek&entry.1292438233=%20%20We%20introduce%20a%20deterministic%20variational%20formulation%20for%20training%20Bayesian%0Alast%20layer%20neural%20networks.%20This%20yields%20a%20sampling-free%2C%20single-pass%20model%20and%0Aloss%20that%20effectively%20improves%20uncertainty%20estimation.%20Our%20variational%20Bayesian%0Alast%20layer%20%28VBLL%29%20can%20be%20trained%20and%20evaluated%20with%20only%20quadratic%20complexity%0Ain%20last%20layer%20width%2C%20and%20is%20thus%20%28nearly%29%20computationally%20free%20to%20add%20to%0Astandard%20architectures.%20We%20experimentally%20investigate%20VBLLs%2C%20and%20show%20that%20they%0Aimprove%20predictive%20accuracy%2C%20calibration%2C%20and%20out%20of%20distribution%20detection%0Aover%20baselines%20across%20both%20regression%20and%20classification.%20Finally%2C%20we%0Ainvestigate%20combining%20VBLL%20layers%20with%20variational%20Bayesian%20feature%20learning%2C%0Ayielding%20a%20lower%20variance%20collapsed%20variational%20inference%20method%20for%20Bayesian%0Aneural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11599v1&entry.124074799=Read"},
{"title": "Toward Understanding the Disagreement Problem in Neural Network Feature\n  Attribution", "author": "Niklas Koenen and Marvin N. Wright", "abstract": "  In recent years, neural networks have demonstrated their remarkable ability\nto discern intricate patterns and relationships from raw data. However,\nunderstanding the inner workings of these black box models remains challenging,\nyet crucial for high-stake decisions. Among the prominent approaches for\nexplaining these black boxes are feature attribution methods, which assign\nrelevance or contribution scores to each input variable for a model prediction.\nDespite the plethora of proposed techniques, ranging from gradient-based to\nbackpropagation-based methods, a significant debate persists about which method\nto use. Various evaluation metrics have been proposed to assess the\ntrustworthiness or robustness of their results. However, current research\nhighlights disagreement among state-of-the-art methods in their explanations.\nOur work addresses this confusion by investigating the explanations'\nfundamental and distributional behavior. Additionally, through a comprehensive\nsimulation study, we illustrate the impact of common scaling and encoding\ntechniques on the explanation quality, assess their efficacy across different\neffect sizes, and demonstrate the origin of inconsistency in rank-based\nevaluation metrics.\n", "link": "http://arxiv.org/abs/2404.11330v1", "date": "2024-04-17", "relevancy": 1.8932, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4801}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4728}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4711}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Toward%20Understanding%20the%20Disagreement%20Problem%20in%20Neural%20Network%20Feature%0A%20%20Attribution&body=Title%3A%20Toward%20Understanding%20the%20Disagreement%20Problem%20in%20Neural%20Network%20Feature%0A%20%20Attribution%0AAuthor%3A%20Niklas%20Koenen%20and%20Marvin%20N.%20Wright%0AAbstract%3A%20%20%20In%20recent%20years%2C%20neural%20networks%20have%20demonstrated%20their%20remarkable%20ability%0Ato%20discern%20intricate%20patterns%20and%20relationships%20from%20raw%20data.%20However%2C%0Aunderstanding%20the%20inner%20workings%20of%20these%20black%20box%20models%20remains%20challenging%2C%0Ayet%20crucial%20for%20high-stake%20decisions.%20Among%20the%20prominent%20approaches%20for%0Aexplaining%20these%20black%20boxes%20are%20feature%20attribution%20methods%2C%20which%20assign%0Arelevance%20or%20contribution%20scores%20to%20each%20input%20variable%20for%20a%20model%20prediction.%0ADespite%20the%20plethora%20of%20proposed%20techniques%2C%20ranging%20from%20gradient-based%20to%0Abackpropagation-based%20methods%2C%20a%20significant%20debate%20persists%20about%20which%20method%0Ato%20use.%20Various%20evaluation%20metrics%20have%20been%20proposed%20to%20assess%20the%0Atrustworthiness%20or%20robustness%20of%20their%20results.%20However%2C%20current%20research%0Ahighlights%20disagreement%20among%20state-of-the-art%20methods%20in%20their%20explanations.%0AOur%20work%20addresses%20this%20confusion%20by%20investigating%20the%20explanations%27%0Afundamental%20and%20distributional%20behavior.%20Additionally%2C%20through%20a%20comprehensive%0Asimulation%20study%2C%20we%20illustrate%20the%20impact%20of%20common%20scaling%20and%20encoding%0Atechniques%20on%20the%20explanation%20quality%2C%20assess%20their%20efficacy%20across%20different%0Aeffect%20sizes%2C%20and%20demonstrate%20the%20origin%20of%20inconsistency%20in%20rank-based%0Aevaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11330v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Understanding%20the%20Disagreement%20Problem%20in%20Neural%20Network%20Feature%0A%20%20Attribution&entry.906535625=Niklas%20Koenen%20and%20Marvin%20N.%20Wright&entry.1292438233=%20%20In%20recent%20years%2C%20neural%20networks%20have%20demonstrated%20their%20remarkable%20ability%0Ato%20discern%20intricate%20patterns%20and%20relationships%20from%20raw%20data.%20However%2C%0Aunderstanding%20the%20inner%20workings%20of%20these%20black%20box%20models%20remains%20challenging%2C%0Ayet%20crucial%20for%20high-stake%20decisions.%20Among%20the%20prominent%20approaches%20for%0Aexplaining%20these%20black%20boxes%20are%20feature%20attribution%20methods%2C%20which%20assign%0Arelevance%20or%20contribution%20scores%20to%20each%20input%20variable%20for%20a%20model%20prediction.%0ADespite%20the%20plethora%20of%20proposed%20techniques%2C%20ranging%20from%20gradient-based%20to%0Abackpropagation-based%20methods%2C%20a%20significant%20debate%20persists%20about%20which%20method%0Ato%20use.%20Various%20evaluation%20metrics%20have%20been%20proposed%20to%20assess%20the%0Atrustworthiness%20or%20robustness%20of%20their%20results.%20However%2C%20current%20research%0Ahighlights%20disagreement%20among%20state-of-the-art%20methods%20in%20their%20explanations.%0AOur%20work%20addresses%20this%20confusion%20by%20investigating%20the%20explanations%27%0Afundamental%20and%20distributional%20behavior.%20Additionally%2C%20through%20a%20comprehensive%0Asimulation%20study%2C%20we%20illustrate%20the%20impact%20of%20common%20scaling%20and%20encoding%0Atechniques%20on%20the%20explanation%20quality%2C%20assess%20their%20efficacy%20across%20different%0Aeffect%20sizes%2C%20and%20demonstrate%20the%20origin%20of%20inconsistency%20in%20rank-based%0Aevaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11330v1&entry.124074799=Read"},
{"title": "A Federated Learning Approach to Privacy Preserving Offensive Language\n  Identification", "author": "Marcos Zampieri and Damith Premasiri and Tharindu Ranasinghe", "abstract": "  The spread of various forms of offensive speech online is an important\nconcern in social media. While platforms have been investing heavily in ways of\ncoping with this problem, the question of privacy remains largely unaddressed.\nModels trained to detect offensive language on social media are trained and/or\nfine-tuned using large amounts of data often stored in centralized servers.\nSince most social media data originates from end users, we propose a privacy\npreserving decentralized architecture for identifying offensive language online\nby introducing Federated Learning (FL) in the context of offensive language\nidentification. FL is a decentralized architecture that allows multiple models\nto be trained locally without the need for data sharing hence preserving users'\nprivacy. We propose a model fusion approach to perform FL. We trained multiple\ndeep learning models on four publicly available English benchmark datasets\n(AHSD, HASOC, HateXplain, OLID) and evaluated their performance in detail. We\nalso present initial cross-lingual experiments in English and Spanish. We show\nthat the proposed model fusion approach outperforms baselines in all the\ndatasets while preserving privacy.\n", "link": "http://arxiv.org/abs/2404.11470v1", "date": "2024-04-17", "relevancy": 1.8863, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4852}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4745}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4632}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Federated%20Learning%20Approach%20to%20Privacy%20Preserving%20Offensive%20Language%0A%20%20Identification&body=Title%3A%20A%20Federated%20Learning%20Approach%20to%20Privacy%20Preserving%20Offensive%20Language%0A%20%20Identification%0AAuthor%3A%20Marcos%20Zampieri%20and%20Damith%20Premasiri%20and%20Tharindu%20Ranasinghe%0AAbstract%3A%20%20%20The%20spread%20of%20various%20forms%20of%20offensive%20speech%20online%20is%20an%20important%0Aconcern%20in%20social%20media.%20While%20platforms%20have%20been%20investing%20heavily%20in%20ways%20of%0Acoping%20with%20this%20problem%2C%20the%20question%20of%20privacy%20remains%20largely%20unaddressed.%0AModels%20trained%20to%20detect%20offensive%20language%20on%20social%20media%20are%20trained%20and/or%0Afine-tuned%20using%20large%20amounts%20of%20data%20often%20stored%20in%20centralized%20servers.%0ASince%20most%20social%20media%20data%20originates%20from%20end%20users%2C%20we%20propose%20a%20privacy%0Apreserving%20decentralized%20architecture%20for%20identifying%20offensive%20language%20online%0Aby%20introducing%20Federated%20Learning%20%28FL%29%20in%20the%20context%20of%20offensive%20language%0Aidentification.%20FL%20is%20a%20decentralized%20architecture%20that%20allows%20multiple%20models%0Ato%20be%20trained%20locally%20without%20the%20need%20for%20data%20sharing%20hence%20preserving%20users%27%0Aprivacy.%20We%20propose%20a%20model%20fusion%20approach%20to%20perform%20FL.%20We%20trained%20multiple%0Adeep%20learning%20models%20on%20four%20publicly%20available%20English%20benchmark%20datasets%0A%28AHSD%2C%20HASOC%2C%20HateXplain%2C%20OLID%29%20and%20evaluated%20their%20performance%20in%20detail.%20We%0Aalso%20present%20initial%20cross-lingual%20experiments%20in%20English%20and%20Spanish.%20We%20show%0Athat%20the%20proposed%20model%20fusion%20approach%20outperforms%20baselines%20in%20all%20the%0Adatasets%20while%20preserving%20privacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11470v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Federated%20Learning%20Approach%20to%20Privacy%20Preserving%20Offensive%20Language%0A%20%20Identification&entry.906535625=Marcos%20Zampieri%20and%20Damith%20Premasiri%20and%20Tharindu%20Ranasinghe&entry.1292438233=%20%20The%20spread%20of%20various%20forms%20of%20offensive%20speech%20online%20is%20an%20important%0Aconcern%20in%20social%20media.%20While%20platforms%20have%20been%20investing%20heavily%20in%20ways%20of%0Acoping%20with%20this%20problem%2C%20the%20question%20of%20privacy%20remains%20largely%20unaddressed.%0AModels%20trained%20to%20detect%20offensive%20language%20on%20social%20media%20are%20trained%20and/or%0Afine-tuned%20using%20large%20amounts%20of%20data%20often%20stored%20in%20centralized%20servers.%0ASince%20most%20social%20media%20data%20originates%20from%20end%20users%2C%20we%20propose%20a%20privacy%0Apreserving%20decentralized%20architecture%20for%20identifying%20offensive%20language%20online%0Aby%20introducing%20Federated%20Learning%20%28FL%29%20in%20the%20context%20of%20offensive%20language%0Aidentification.%20FL%20is%20a%20decentralized%20architecture%20that%20allows%20multiple%20models%0Ato%20be%20trained%20locally%20without%20the%20need%20for%20data%20sharing%20hence%20preserving%20users%27%0Aprivacy.%20We%20propose%20a%20model%20fusion%20approach%20to%20perform%20FL.%20We%20trained%20multiple%0Adeep%20learning%20models%20on%20four%20publicly%20available%20English%20benchmark%20datasets%0A%28AHSD%2C%20HASOC%2C%20HateXplain%2C%20OLID%29%20and%20evaluated%20their%20performance%20in%20detail.%20We%0Aalso%20present%20initial%20cross-lingual%20experiments%20in%20English%20and%20Spanish.%20We%20show%0Athat%20the%20proposed%20model%20fusion%20approach%20outperforms%20baselines%20in%20all%20the%0Adatasets%20while%20preserving%20privacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11470v1&entry.124074799=Read"},
{"title": "Reformatted Alignment", "author": "Run-Ze Fan and Xuefeng Li and Haoyang Zou and Junlong Li and Shwai He and Ethan Chern and Jiewen Hu and Pengfei Liu", "abstract": "  The quality of finetuning data is crucial for aligning large language models\n(LLMs) with human values. Current methods to improve data quality are either\nlabor-intensive or prone to factual errors caused by LLM hallucinations. This\npaper explores elevating the quality of existing instruction data to better\nalign with human values, introducing a simple and effective approach named\nReAlign, which reformats the responses of instruction data into a format that\nbetter aligns with pre-established criteria and the collated evidence. This\napproach minimizes human annotation, hallucination, and the difficulty in\nscaling, remaining orthogonal to existing alignment techniques. Experimentally,\nReAlign significantly boosts the general alignment ability, math reasoning,\nfactuality, and readability of the LLMs.\n  Encouragingly, without introducing any additional data or advanced training\ntechniques, and merely by reformatting the response, LLaMA-2-13B's mathematical\nreasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy.\nAdditionally, a mere 5% of ReAlign data yields a 67% boost in general alignment\nability measured by the Alpaca dataset. This work highlights the need for\nfurther research into the science and mechanistic interpretability of LLMs. We\nhave made the associated code and data publicly accessible to support future\nstudies at https://github.com/GAIR-NLP/ReAlign.\n", "link": "http://arxiv.org/abs/2402.12219v2", "date": "2024-04-17", "relevancy": 1.8855, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4834}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4805}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4574}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Reformatted%20Alignment&body=Title%3A%20Reformatted%20Alignment%0AAuthor%3A%20Run-Ze%20Fan%20and%20Xuefeng%20Li%20and%20Haoyang%20Zou%20and%20Junlong%20Li%20and%20Shwai%20He%20and%20Ethan%20Chern%20and%20Jiewen%20Hu%20and%20Pengfei%20Liu%0AAbstract%3A%20%20%20The%20quality%20of%20finetuning%20data%20is%20crucial%20for%20aligning%20large%20language%20models%0A%28LLMs%29%20with%20human%20values.%20Current%20methods%20to%20improve%20data%20quality%20are%20either%0Alabor-intensive%20or%20prone%20to%20factual%20errors%20caused%20by%20LLM%20hallucinations.%20This%0Apaper%20explores%20elevating%20the%20quality%20of%20existing%20instruction%20data%20to%20better%0Aalign%20with%20human%20values%2C%20introducing%20a%20simple%20and%20effective%20approach%20named%0AReAlign%2C%20which%20reformats%20the%20responses%20of%20instruction%20data%20into%20a%20format%20that%0Abetter%20aligns%20with%20pre-established%20criteria%20and%20the%20collated%20evidence.%20This%0Aapproach%20minimizes%20human%20annotation%2C%20hallucination%2C%20and%20the%20difficulty%20in%0Ascaling%2C%20remaining%20orthogonal%20to%20existing%20alignment%20techniques.%20Experimentally%2C%0AReAlign%20significantly%20boosts%20the%20general%20alignment%20ability%2C%20math%20reasoning%2C%0Afactuality%2C%20and%20readability%20of%20the%20LLMs.%0A%20%20Encouragingly%2C%20without%20introducing%20any%20additional%20data%20or%20advanced%20training%0Atechniques%2C%20and%20merely%20by%20reformatting%20the%20response%2C%20LLaMA-2-13B%27s%20mathematical%0Areasoning%20ability%20on%20GSM8K%20can%20be%20improved%20from%2046.77%25%20to%2056.63%25%20in%20accuracy.%0AAdditionally%2C%20a%20mere%205%25%20of%20ReAlign%20data%20yields%20a%2067%25%20boost%20in%20general%20alignment%0Aability%20measured%20by%20the%20Alpaca%20dataset.%20This%20work%20highlights%20the%20need%20for%0Afurther%20research%20into%20the%20science%20and%20mechanistic%20interpretability%20of%20LLMs.%20We%0Ahave%20made%20the%20associated%20code%20and%20data%20publicly%20accessible%20to%20support%20future%0Astudies%20at%20https%3A//github.com/GAIR-NLP/ReAlign.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12219v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reformatted%20Alignment&entry.906535625=Run-Ze%20Fan%20and%20Xuefeng%20Li%20and%20Haoyang%20Zou%20and%20Junlong%20Li%20and%20Shwai%20He%20and%20Ethan%20Chern%20and%20Jiewen%20Hu%20and%20Pengfei%20Liu&entry.1292438233=%20%20The%20quality%20of%20finetuning%20data%20is%20crucial%20for%20aligning%20large%20language%20models%0A%28LLMs%29%20with%20human%20values.%20Current%20methods%20to%20improve%20data%20quality%20are%20either%0Alabor-intensive%20or%20prone%20to%20factual%20errors%20caused%20by%20LLM%20hallucinations.%20This%0Apaper%20explores%20elevating%20the%20quality%20of%20existing%20instruction%20data%20to%20better%0Aalign%20with%20human%20values%2C%20introducing%20a%20simple%20and%20effective%20approach%20named%0AReAlign%2C%20which%20reformats%20the%20responses%20of%20instruction%20data%20into%20a%20format%20that%0Abetter%20aligns%20with%20pre-established%20criteria%20and%20the%20collated%20evidence.%20This%0Aapproach%20minimizes%20human%20annotation%2C%20hallucination%2C%20and%20the%20difficulty%20in%0Ascaling%2C%20remaining%20orthogonal%20to%20existing%20alignment%20techniques.%20Experimentally%2C%0AReAlign%20significantly%20boosts%20the%20general%20alignment%20ability%2C%20math%20reasoning%2C%0Afactuality%2C%20and%20readability%20of%20the%20LLMs.%0A%20%20Encouragingly%2C%20without%20introducing%20any%20additional%20data%20or%20advanced%20training%0Atechniques%2C%20and%20merely%20by%20reformatting%20the%20response%2C%20LLaMA-2-13B%27s%20mathematical%0Areasoning%20ability%20on%20GSM8K%20can%20be%20improved%20from%2046.77%25%20to%2056.63%25%20in%20accuracy.%0AAdditionally%2C%20a%20mere%205%25%20of%20ReAlign%20data%20yields%20a%2067%25%20boost%20in%20general%20alignment%0Aability%20measured%20by%20the%20Alpaca%20dataset.%20This%20work%20highlights%20the%20need%20for%0Afurther%20research%20into%20the%20science%20and%20mechanistic%20interpretability%20of%20LLMs.%20We%0Ahave%20made%20the%20associated%20code%20and%20data%20publicly%20accessible%20to%20support%20future%0Astudies%20at%20https%3A//github.com/GAIR-NLP/ReAlign.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12219v2&entry.124074799=Read"},
{"title": "Optical Image-to-Image Translation Using Denoising Diffusion Models:\n  Heterogeneous Change Detection as a Use Case", "author": "Jo\u00e3o Gabriel Vinholi and Marco Chini and Anis Amziane and Renato Machado and Danilo Silva and Patrick Matgen", "abstract": "  We introduce an innovative deep learning-based method that uses a denoising\ndiffusion-based model to translate low-resolution images to high-resolution\nones from different optical sensors while preserving the contents and avoiding\nundesired artifacts. The proposed method is trained and tested on a large and\ndiverse data set of paired Sentinel-II and Planet Dove images. We show that it\ncan solve serious image generation issues observed when the popular\nclassifier-free guided Denoising Diffusion Implicit Model (DDIM) framework is\nused in the task of Image-to-Image Translation of multi-sensor optical remote\nsensing images and that it can generate large images with highly consistent\npatches, both in colors and in features. Moreover, we demonstrate how our\nmethod improves heterogeneous change detection results in two urban areas:\nBeirut, Lebanon, and Austin, USA. Our contributions are: i) a new training and\ntesting algorithm based on denoising diffusion models for optical image\ntranslation; ii) a comprehensive image quality evaluation and ablation study;\niii) a comparison with the classifier-free guided DDIM framework; and iv)\nchange detection experiments on heterogeneous data.\n", "link": "http://arxiv.org/abs/2404.11243v1", "date": "2024-04-17", "relevancy": 1.872, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6374}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.625}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6081}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Optical%20Image-to-Image%20Translation%20Using%20Denoising%20Diffusion%20Models%3A%0A%20%20Heterogeneous%20Change%20Detection%20as%20a%20Use%20Case&body=Title%3A%20Optical%20Image-to-Image%20Translation%20Using%20Denoising%20Diffusion%20Models%3A%0A%20%20Heterogeneous%20Change%20Detection%20as%20a%20Use%20Case%0AAuthor%3A%20Jo%C3%A3o%20Gabriel%20Vinholi%20and%20Marco%20Chini%20and%20Anis%20Amziane%20and%20Renato%20Machado%20and%20Danilo%20Silva%20and%20Patrick%20Matgen%0AAbstract%3A%20%20%20We%20introduce%20an%20innovative%20deep%20learning-based%20method%20that%20uses%20a%20denoising%0Adiffusion-based%20model%20to%20translate%20low-resolution%20images%20to%20high-resolution%0Aones%20from%20different%20optical%20sensors%20while%20preserving%20the%20contents%20and%20avoiding%0Aundesired%20artifacts.%20The%20proposed%20method%20is%20trained%20and%20tested%20on%20a%20large%20and%0Adiverse%20data%20set%20of%20paired%20Sentinel-II%20and%20Planet%20Dove%20images.%20We%20show%20that%20it%0Acan%20solve%20serious%20image%20generation%20issues%20observed%20when%20the%20popular%0Aclassifier-free%20guided%20Denoising%20Diffusion%20Implicit%20Model%20%28DDIM%29%20framework%20is%0Aused%20in%20the%20task%20of%20Image-to-Image%20Translation%20of%20multi-sensor%20optical%20remote%0Asensing%20images%20and%20that%20it%20can%20generate%20large%20images%20with%20highly%20consistent%0Apatches%2C%20both%20in%20colors%20and%20in%20features.%20Moreover%2C%20we%20demonstrate%20how%20our%0Amethod%20improves%20heterogeneous%20change%20detection%20results%20in%20two%20urban%20areas%3A%0ABeirut%2C%20Lebanon%2C%20and%20Austin%2C%20USA.%20Our%20contributions%20are%3A%20i%29%20a%20new%20training%20and%0Atesting%20algorithm%20based%20on%20denoising%20diffusion%20models%20for%20optical%20image%0Atranslation%3B%20ii%29%20a%20comprehensive%20image%20quality%20evaluation%20and%20ablation%20study%3B%0Aiii%29%20a%20comparison%20with%20the%20classifier-free%20guided%20DDIM%20framework%3B%20and%20iv%29%0Achange%20detection%20experiments%20on%20heterogeneous%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11243v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optical%20Image-to-Image%20Translation%20Using%20Denoising%20Diffusion%20Models%3A%0A%20%20Heterogeneous%20Change%20Detection%20as%20a%20Use%20Case&entry.906535625=Jo%C3%A3o%20Gabriel%20Vinholi%20and%20Marco%20Chini%20and%20Anis%20Amziane%20and%20Renato%20Machado%20and%20Danilo%20Silva%20and%20Patrick%20Matgen&entry.1292438233=%20%20We%20introduce%20an%20innovative%20deep%20learning-based%20method%20that%20uses%20a%20denoising%0Adiffusion-based%20model%20to%20translate%20low-resolution%20images%20to%20high-resolution%0Aones%20from%20different%20optical%20sensors%20while%20preserving%20the%20contents%20and%20avoiding%0Aundesired%20artifacts.%20The%20proposed%20method%20is%20trained%20and%20tested%20on%20a%20large%20and%0Adiverse%20data%20set%20of%20paired%20Sentinel-II%20and%20Planet%20Dove%20images.%20We%20show%20that%20it%0Acan%20solve%20serious%20image%20generation%20issues%20observed%20when%20the%20popular%0Aclassifier-free%20guided%20Denoising%20Diffusion%20Implicit%20Model%20%28DDIM%29%20framework%20is%0Aused%20in%20the%20task%20of%20Image-to-Image%20Translation%20of%20multi-sensor%20optical%20remote%0Asensing%20images%20and%20that%20it%20can%20generate%20large%20images%20with%20highly%20consistent%0Apatches%2C%20both%20in%20colors%20and%20in%20features.%20Moreover%2C%20we%20demonstrate%20how%20our%0Amethod%20improves%20heterogeneous%20change%20detection%20results%20in%20two%20urban%20areas%3A%0ABeirut%2C%20Lebanon%2C%20and%20Austin%2C%20USA.%20Our%20contributions%20are%3A%20i%29%20a%20new%20training%20and%0Atesting%20algorithm%20based%20on%20denoising%20diffusion%20models%20for%20optical%20image%0Atranslation%3B%20ii%29%20a%20comprehensive%20image%20quality%20evaluation%20and%20ablation%20study%3B%0Aiii%29%20a%20comparison%20with%20the%20classifier-free%20guided%20DDIM%20framework%3B%20and%20iv%29%0Achange%20detection%20experiments%20on%20heterogeneous%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11243v1&entry.124074799=Read"},
{"title": "Video shutter angle estimation using optical flow and linear blur", "author": "David Korcak and Jiri Matas", "abstract": "  We present a method for estimating the shutter angle, a.k.a. exposure\nfraction - the ratio of the exposure time and the reciprocal of frame rate - of\nvideoclips containing motion. The approach exploits the relation of the\nexposure fraction, optical flow, and linear motion blur. Robustness is achieved\nby selecting image patches where both the optical flow and blur estimates are\nreliable, checking their consistency. The method was evaluated on the publicly\navailable Beam-Splitter Dataset with a range of exposure fractions from 0.015\nto 0.36. The best achieved mean absolute error of estimates was 0.039. We\nsuccessfully test the suitability of the method for a forensic application of\ndetection of video tampering by frame removal or insertion\n", "link": "http://arxiv.org/abs/2303.10247v2", "date": "2024-04-17", "relevancy": 1.8719, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4901}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4785}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4417}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Video%20shutter%20angle%20estimation%20using%20optical%20flow%20and%20linear%20blur&body=Title%3A%20Video%20shutter%20angle%20estimation%20using%20optical%20flow%20and%20linear%20blur%0AAuthor%3A%20David%20Korcak%20and%20Jiri%20Matas%0AAbstract%3A%20%20%20We%20present%20a%20method%20for%20estimating%20the%20shutter%20angle%2C%20a.k.a.%20exposure%0Afraction%20-%20the%20ratio%20of%20the%20exposure%20time%20and%20the%20reciprocal%20of%20frame%20rate%20-%20of%0Avideoclips%20containing%20motion.%20The%20approach%20exploits%20the%20relation%20of%20the%0Aexposure%20fraction%2C%20optical%20flow%2C%20and%20linear%20motion%20blur.%20Robustness%20is%20achieved%0Aby%20selecting%20image%20patches%20where%20both%20the%20optical%20flow%20and%20blur%20estimates%20are%0Areliable%2C%20checking%20their%20consistency.%20The%20method%20was%20evaluated%20on%20the%20publicly%0Aavailable%20Beam-Splitter%20Dataset%20with%20a%20range%20of%20exposure%20fractions%20from%200.015%0Ato%200.36.%20The%20best%20achieved%20mean%20absolute%20error%20of%20estimates%20was%200.039.%20We%0Asuccessfully%20test%20the%20suitability%20of%20the%20method%20for%20a%20forensic%20application%20of%0Adetection%20of%20video%20tampering%20by%20frame%20removal%20or%20insertion%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.10247v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20shutter%20angle%20estimation%20using%20optical%20flow%20and%20linear%20blur&entry.906535625=David%20Korcak%20and%20Jiri%20Matas&entry.1292438233=%20%20We%20present%20a%20method%20for%20estimating%20the%20shutter%20angle%2C%20a.k.a.%20exposure%0Afraction%20-%20the%20ratio%20of%20the%20exposure%20time%20and%20the%20reciprocal%20of%20frame%20rate%20-%20of%0Avideoclips%20containing%20motion.%20The%20approach%20exploits%20the%20relation%20of%20the%0Aexposure%20fraction%2C%20optical%20flow%2C%20and%20linear%20motion%20blur.%20Robustness%20is%20achieved%0Aby%20selecting%20image%20patches%20where%20both%20the%20optical%20flow%20and%20blur%20estimates%20are%0Areliable%2C%20checking%20their%20consistency.%20The%20method%20was%20evaluated%20on%20the%20publicly%0Aavailable%20Beam-Splitter%20Dataset%20with%20a%20range%20of%20exposure%20fractions%20from%200.015%0Ato%200.36.%20The%20best%20achieved%20mean%20absolute%20error%20of%20estimates%20was%200.039.%20We%0Asuccessfully%20test%20the%20suitability%20of%20the%20method%20for%20a%20forensic%20application%20of%0Adetection%20of%20video%20tampering%20by%20frame%20removal%20or%20insertion%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.10247v2&entry.124074799=Read"},
{"title": "Decentralized Personalized Federated Learning for Min-Max Problems", "author": "Ekaterina Borodich and Aleksandr Beznosikov and Abdurakhmon Sadiev and Vadim Sushko and Nikolay Savelyev and Martin Tak\u00e1\u010d and Alexander Gasnikov", "abstract": "  Personalized Federated Learning (PFL) has witnessed remarkable advancements,\nenabling the development of innovative machine learning applications that\npreserve the privacy of training data. However, existing theoretical research\nin this field has primarily focused on distributed optimization for\nminimization problems. This paper is the first to study PFL for saddle point\nproblems encompassing a broader range of optimization problems, that require\nmore than just solving minimization problems. In this work, we consider a\nrecently proposed PFL setting with the mixing objective function, an approach\ncombining the learning of a global model together with locally distributed\nlearners. Unlike most previous work, which considered only the centralized\nsetting, we work in a more general and decentralized setup that allows us to\ndesign and analyze more practical and federated ways to connect devices to the\nnetwork. We proposed new algorithms to address this problem and provide a\ntheoretical analysis of the smooth (strongly) convex-(strongly) concave saddle\npoint problems in stochastic and deterministic cases. Numerical experiments for\nbilinear problems and neural networks with adversarial noise demonstrate the\neffectiveness of the proposed methods.\n", "link": "http://arxiv.org/abs/2106.07289v6", "date": "2024-04-17", "relevancy": 1.8715, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4778}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4639}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.453}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Decentralized%20Personalized%20Federated%20Learning%20for%20Min-Max%20Problems&body=Title%3A%20Decentralized%20Personalized%20Federated%20Learning%20for%20Min-Max%20Problems%0AAuthor%3A%20Ekaterina%20Borodich%20and%20Aleksandr%20Beznosikov%20and%20Abdurakhmon%20Sadiev%20and%20Vadim%20Sushko%20and%20Nikolay%20Savelyev%20and%20Martin%20Tak%C3%A1%C4%8D%20and%20Alexander%20Gasnikov%0AAbstract%3A%20%20%20Personalized%20Federated%20Learning%20%28PFL%29%20has%20witnessed%20remarkable%20advancements%2C%0Aenabling%20the%20development%20of%20innovative%20machine%20learning%20applications%20that%0Apreserve%20the%20privacy%20of%20training%20data.%20However%2C%20existing%20theoretical%20research%0Ain%20this%20field%20has%20primarily%20focused%20on%20distributed%20optimization%20for%0Aminimization%20problems.%20This%20paper%20is%20the%20first%20to%20study%20PFL%20for%20saddle%20point%0Aproblems%20encompassing%20a%20broader%20range%20of%20optimization%20problems%2C%20that%20require%0Amore%20than%20just%20solving%20minimization%20problems.%20In%20this%20work%2C%20we%20consider%20a%0Arecently%20proposed%20PFL%20setting%20with%20the%20mixing%20objective%20function%2C%20an%20approach%0Acombining%20the%20learning%20of%20a%20global%20model%20together%20with%20locally%20distributed%0Alearners.%20Unlike%20most%20previous%20work%2C%20which%20considered%20only%20the%20centralized%0Asetting%2C%20we%20work%20in%20a%20more%20general%20and%20decentralized%20setup%20that%20allows%20us%20to%0Adesign%20and%20analyze%20more%20practical%20and%20federated%20ways%20to%20connect%20devices%20to%20the%0Anetwork.%20We%20proposed%20new%20algorithms%20to%20address%20this%20problem%20and%20provide%20a%0Atheoretical%20analysis%20of%20the%20smooth%20%28strongly%29%20convex-%28strongly%29%20concave%20saddle%0Apoint%20problems%20in%20stochastic%20and%20deterministic%20cases.%20Numerical%20experiments%20for%0Abilinear%20problems%20and%20neural%20networks%20with%20adversarial%20noise%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2106.07289v6", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20Personalized%20Federated%20Learning%20for%20Min-Max%20Problems&entry.906535625=Ekaterina%20Borodich%20and%20Aleksandr%20Beznosikov%20and%20Abdurakhmon%20Sadiev%20and%20Vadim%20Sushko%20and%20Nikolay%20Savelyev%20and%20Martin%20Tak%C3%A1%C4%8D%20and%20Alexander%20Gasnikov&entry.1292438233=%20%20Personalized%20Federated%20Learning%20%28PFL%29%20has%20witnessed%20remarkable%20advancements%2C%0Aenabling%20the%20development%20of%20innovative%20machine%20learning%20applications%20that%0Apreserve%20the%20privacy%20of%20training%20data.%20However%2C%20existing%20theoretical%20research%0Ain%20this%20field%20has%20primarily%20focused%20on%20distributed%20optimization%20for%0Aminimization%20problems.%20This%20paper%20is%20the%20first%20to%20study%20PFL%20for%20saddle%20point%0Aproblems%20encompassing%20a%20broader%20range%20of%20optimization%20problems%2C%20that%20require%0Amore%20than%20just%20solving%20minimization%20problems.%20In%20this%20work%2C%20we%20consider%20a%0Arecently%20proposed%20PFL%20setting%20with%20the%20mixing%20objective%20function%2C%20an%20approach%0Acombining%20the%20learning%20of%20a%20global%20model%20together%20with%20locally%20distributed%0Alearners.%20Unlike%20most%20previous%20work%2C%20which%20considered%20only%20the%20centralized%0Asetting%2C%20we%20work%20in%20a%20more%20general%20and%20decentralized%20setup%20that%20allows%20us%20to%0Adesign%20and%20analyze%20more%20practical%20and%20federated%20ways%20to%20connect%20devices%20to%20the%0Anetwork.%20We%20proposed%20new%20algorithms%20to%20address%20this%20problem%20and%20provide%20a%0Atheoretical%20analysis%20of%20the%20smooth%20%28strongly%29%20convex-%28strongly%29%20concave%20saddle%0Apoint%20problems%20in%20stochastic%20and%20deterministic%20cases.%20Numerical%20experiments%20for%0Abilinear%20problems%20and%20neural%20networks%20with%20adversarial%20noise%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2106.07289v6&entry.124074799=Read"},
{"title": "NTIRE 2024 Challenge on Short-form UGC Video Quality Assessment: Methods\n  and Results", "author": "Xin Li and Kun Yuan and Yajing Pei and Yiting Lu and Ming Sun and Chao Zhou and Zhibo Chen and Radu Timofte and Wei Sun and Haoning Wu and Zicheng Zhang and Jun Jia and Zhichao Zhang and Linhan Cao and Qiubo Chen and Xiongkuo Min and Weisi Lin and Guangtao Zhai and Jianhui Sun and Tianyi Wang and Lei Li and Han Kong and Wenxuan Wang and Bing Li and Cheng Luo and Haiqiang Wang and Xiangguang Chen and Wenhui Meng and Xiang Pan and Huiying Shi and Han Zhu and Xiaozhong Xu and Lei Sun and Zhenzhong Chen and Shan Liu and Fangyuan Kong and Haotian Fan and Yifang Xu and Haoran Xu and Mengduo Yang and Jie Zhou and Jiaze Li and Shijie Wen and Mai Xu and Da Li and Shunyu Yao and Jiazhi Du and Wangmeng Zuo and Zhibo Li and Shuai He and Anlong Ming and Huiyuan Fu and Huadong Ma and Yong Wu and Fie Xue and Guozhi Zhao and Lina Du and Jie Guo and Yu Zhang and Huimin Zheng and Junhao Chen and Yue Liu and Dulan Zhou and Kele Xu and Qisheng Xu and Tao Sun and Zhixiang Ding and Yuhang Hu", "abstract": "  This paper reviews the NTIRE 2024 Challenge on Shortform UGC Video Quality\nAssessment (S-UGC VQA), where various excellent solutions are submitted and\nevaluated on the collected dataset KVQ from popular short-form video platform,\ni.e., Kuaishou/Kwai Platform. The KVQ database is divided into three parts,\nincluding 2926 videos for training, 420 videos for validation, and 854 videos\nfor testing. The purpose is to build new benchmarks and advance the development\nof S-UGC VQA. The competition had 200 participants and 13 teams submitted valid\nsolutions for the final testing phase. The proposed solutions achieved\nstate-of-the-art performances for S-UGC VQA. The project can be found at\nhttps://github.com/lixinustc/KVQChallenge-CVPR-NTIRE2024.\n", "link": "http://arxiv.org/abs/2404.11313v1", "date": "2024-04-17", "relevancy": 1.8662, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4839}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4569}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4474}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NTIRE%202024%20Challenge%20on%20Short-form%20UGC%20Video%20Quality%20Assessment%3A%20Methods%0A%20%20and%20Results&body=Title%3A%20NTIRE%202024%20Challenge%20on%20Short-form%20UGC%20Video%20Quality%20Assessment%3A%20Methods%0A%20%20and%20Results%0AAuthor%3A%20Xin%20Li%20and%20Kun%20Yuan%20and%20Yajing%20Pei%20and%20Yiting%20Lu%20and%20Ming%20Sun%20and%20Chao%20Zhou%20and%20Zhibo%20Chen%20and%20Radu%20Timofte%20and%20Wei%20Sun%20and%20Haoning%20Wu%20and%20Zicheng%20Zhang%20and%20Jun%20Jia%20and%20Zhichao%20Zhang%20and%20Linhan%20Cao%20and%20Qiubo%20Chen%20and%20Xiongkuo%20Min%20and%20Weisi%20Lin%20and%20Guangtao%20Zhai%20and%20Jianhui%20Sun%20and%20Tianyi%20Wang%20and%20Lei%20Li%20and%20Han%20Kong%20and%20Wenxuan%20Wang%20and%20Bing%20Li%20and%20Cheng%20Luo%20and%20Haiqiang%20Wang%20and%20Xiangguang%20Chen%20and%20Wenhui%20Meng%20and%20Xiang%20Pan%20and%20Huiying%20Shi%20and%20Han%20Zhu%20and%20Xiaozhong%20Xu%20and%20Lei%20Sun%20and%20Zhenzhong%20Chen%20and%20Shan%20Liu%20and%20Fangyuan%20Kong%20and%20Haotian%20Fan%20and%20Yifang%20Xu%20and%20Haoran%20Xu%20and%20Mengduo%20Yang%20and%20Jie%20Zhou%20and%20Jiaze%20Li%20and%20Shijie%20Wen%20and%20Mai%20Xu%20and%20Da%20Li%20and%20Shunyu%20Yao%20and%20Jiazhi%20Du%20and%20Wangmeng%20Zuo%20and%20Zhibo%20Li%20and%20Shuai%20He%20and%20Anlong%20Ming%20and%20Huiyuan%20Fu%20and%20Huadong%20Ma%20and%20Yong%20Wu%20and%20Fie%20Xue%20and%20Guozhi%20Zhao%20and%20Lina%20Du%20and%20Jie%20Guo%20and%20Yu%20Zhang%20and%20Huimin%20Zheng%20and%20Junhao%20Chen%20and%20Yue%20Liu%20and%20Dulan%20Zhou%20and%20Kele%20Xu%20and%20Qisheng%20Xu%20and%20Tao%20Sun%20and%20Zhixiang%20Ding%20and%20Yuhang%20Hu%0AAbstract%3A%20%20%20This%20paper%20reviews%20the%20NTIRE%202024%20Challenge%20on%20Shortform%20UGC%20Video%20Quality%0AAssessment%20%28S-UGC%20VQA%29%2C%20where%20various%20excellent%20solutions%20are%20submitted%20and%0Aevaluated%20on%20the%20collected%20dataset%20KVQ%20from%20popular%20short-form%20video%20platform%2C%0Ai.e.%2C%20Kuaishou/Kwai%20Platform.%20The%20KVQ%20database%20is%20divided%20into%20three%20parts%2C%0Aincluding%202926%20videos%20for%20training%2C%20420%20videos%20for%20validation%2C%20and%20854%20videos%0Afor%20testing.%20The%20purpose%20is%20to%20build%20new%20benchmarks%20and%20advance%20the%20development%0Aof%20S-UGC%20VQA.%20The%20competition%20had%20200%20participants%20and%2013%20teams%20submitted%20valid%0Asolutions%20for%20the%20final%20testing%20phase.%20The%20proposed%20solutions%20achieved%0Astate-of-the-art%20performances%20for%20S-UGC%20VQA.%20The%20project%20can%20be%20found%20at%0Ahttps%3A//github.com/lixinustc/KVQChallenge-CVPR-NTIRE2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11313v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NTIRE%202024%20Challenge%20on%20Short-form%20UGC%20Video%20Quality%20Assessment%3A%20Methods%0A%20%20and%20Results&entry.906535625=Xin%20Li%20and%20Kun%20Yuan%20and%20Yajing%20Pei%20and%20Yiting%20Lu%20and%20Ming%20Sun%20and%20Chao%20Zhou%20and%20Zhibo%20Chen%20and%20Radu%20Timofte%20and%20Wei%20Sun%20and%20Haoning%20Wu%20and%20Zicheng%20Zhang%20and%20Jun%20Jia%20and%20Zhichao%20Zhang%20and%20Linhan%20Cao%20and%20Qiubo%20Chen%20and%20Xiongkuo%20Min%20and%20Weisi%20Lin%20and%20Guangtao%20Zhai%20and%20Jianhui%20Sun%20and%20Tianyi%20Wang%20and%20Lei%20Li%20and%20Han%20Kong%20and%20Wenxuan%20Wang%20and%20Bing%20Li%20and%20Cheng%20Luo%20and%20Haiqiang%20Wang%20and%20Xiangguang%20Chen%20and%20Wenhui%20Meng%20and%20Xiang%20Pan%20and%20Huiying%20Shi%20and%20Han%20Zhu%20and%20Xiaozhong%20Xu%20and%20Lei%20Sun%20and%20Zhenzhong%20Chen%20and%20Shan%20Liu%20and%20Fangyuan%20Kong%20and%20Haotian%20Fan%20and%20Yifang%20Xu%20and%20Haoran%20Xu%20and%20Mengduo%20Yang%20and%20Jie%20Zhou%20and%20Jiaze%20Li%20and%20Shijie%20Wen%20and%20Mai%20Xu%20and%20Da%20Li%20and%20Shunyu%20Yao%20and%20Jiazhi%20Du%20and%20Wangmeng%20Zuo%20and%20Zhibo%20Li%20and%20Shuai%20He%20and%20Anlong%20Ming%20and%20Huiyuan%20Fu%20and%20Huadong%20Ma%20and%20Yong%20Wu%20and%20Fie%20Xue%20and%20Guozhi%20Zhao%20and%20Lina%20Du%20and%20Jie%20Guo%20and%20Yu%20Zhang%20and%20Huimin%20Zheng%20and%20Junhao%20Chen%20and%20Yue%20Liu%20and%20Dulan%20Zhou%20and%20Kele%20Xu%20and%20Qisheng%20Xu%20and%20Tao%20Sun%20and%20Zhixiang%20Ding%20and%20Yuhang%20Hu&entry.1292438233=%20%20This%20paper%20reviews%20the%20NTIRE%202024%20Challenge%20on%20Shortform%20UGC%20Video%20Quality%0AAssessment%20%28S-UGC%20VQA%29%2C%20where%20various%20excellent%20solutions%20are%20submitted%20and%0Aevaluated%20on%20the%20collected%20dataset%20KVQ%20from%20popular%20short-form%20video%20platform%2C%0Ai.e.%2C%20Kuaishou/Kwai%20Platform.%20The%20KVQ%20database%20is%20divided%20into%20three%20parts%2C%0Aincluding%202926%20videos%20for%20training%2C%20420%20videos%20for%20validation%2C%20and%20854%20videos%0Afor%20testing.%20The%20purpose%20is%20to%20build%20new%20benchmarks%20and%20advance%20the%20development%0Aof%20S-UGC%20VQA.%20The%20competition%20had%20200%20participants%20and%2013%20teams%20submitted%20valid%0Asolutions%20for%20the%20final%20testing%20phase.%20The%20proposed%20solutions%20achieved%0Astate-of-the-art%20performances%20for%20S-UGC%20VQA.%20The%20project%20can%20be%20found%20at%0Ahttps%3A//github.com/lixinustc/KVQChallenge-CVPR-NTIRE2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11313v1&entry.124074799=Read"},
{"title": "Unifying Bias and Unfairness in Information Retrieval: A Survey of\n  Challenges and Opportunities with Large Language Models", "author": "Sunhao Dai and Chen Xu and Shicheng Xu and Liang Pang and Zhenhua Dong and Jun Xu", "abstract": "  With the rapid advancement of large language models (LLMs), information\nretrieval (IR) systems, such as search engines and recommender systems, have\nundergone a significant paradigm shift. This evolution, while heralding new\nopportunities, introduces emerging challenges, particularly in terms of biases\nand unfairness, which may threaten the information ecosystem. In this paper, we\npresent a comprehensive survey of existing works on emerging and pressing bias\nand unfairness issues in IR systems when the integration of LLMs. We first\nunify bias and unfairness issues as distribution mismatch problems, providing a\ngroundwork for categorizing various mitigation strategies through distribution\nalignment. Subsequently, we systematically delve into the specific bias and\nunfairness issues arising from three critical stages of LLMs integration into\nIR systems: data collection, model development, and result evaluation. In doing\nso, we meticulously review and analyze recent literature, focusing on the\ndefinitions, characteristics, and corresponding mitigation strategies\nassociated with these issues. Finally, we identify and highlight some open\nproblems and challenges for future work, aiming to inspire researchers and\nstakeholders in the IR field and beyond to better understand and mitigate bias\nand unfairness issues of IR in this LLM era. We also consistently maintain a\nGitHub repository for the relevant papers and resources in this rising\ndirection at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.\n", "link": "http://arxiv.org/abs/2404.11457v1", "date": "2024-04-17", "relevancy": 1.865, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4917}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4623}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.46}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unifying%20Bias%20and%20Unfairness%20in%20Information%20Retrieval%3A%20A%20Survey%20of%0A%20%20Challenges%20and%20Opportunities%20with%20Large%20Language%20Models&body=Title%3A%20Unifying%20Bias%20and%20Unfairness%20in%20Information%20Retrieval%3A%20A%20Survey%20of%0A%20%20Challenges%20and%20Opportunities%20with%20Large%20Language%20Models%0AAuthor%3A%20Sunhao%20Dai%20and%20Chen%20Xu%20and%20Shicheng%20Xu%20and%20Liang%20Pang%20and%20Zhenhua%20Dong%20and%20Jun%20Xu%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20of%20large%20language%20models%20%28LLMs%29%2C%20information%0Aretrieval%20%28IR%29%20systems%2C%20such%20as%20search%20engines%20and%20recommender%20systems%2C%20have%0Aundergone%20a%20significant%20paradigm%20shift.%20This%20evolution%2C%20while%20heralding%20new%0Aopportunities%2C%20introduces%20emerging%20challenges%2C%20particularly%20in%20terms%20of%20biases%0Aand%20unfairness%2C%20which%20may%20threaten%20the%20information%20ecosystem.%20In%20this%20paper%2C%20we%0Apresent%20a%20comprehensive%20survey%20of%20existing%20works%20on%20emerging%20and%20pressing%20bias%0Aand%20unfairness%20issues%20in%20IR%20systems%20when%20the%20integration%20of%20LLMs.%20We%20first%0Aunify%20bias%20and%20unfairness%20issues%20as%20distribution%20mismatch%20problems%2C%20providing%20a%0Agroundwork%20for%20categorizing%20various%20mitigation%20strategies%20through%20distribution%0Aalignment.%20Subsequently%2C%20we%20systematically%20delve%20into%20the%20specific%20bias%20and%0Aunfairness%20issues%20arising%20from%20three%20critical%20stages%20of%20LLMs%20integration%20into%0AIR%20systems%3A%20data%20collection%2C%20model%20development%2C%20and%20result%20evaluation.%20In%20doing%0Aso%2C%20we%20meticulously%20review%20and%20analyze%20recent%20literature%2C%20focusing%20on%20the%0Adefinitions%2C%20characteristics%2C%20and%20corresponding%20mitigation%20strategies%0Aassociated%20with%20these%20issues.%20Finally%2C%20we%20identify%20and%20highlight%20some%20open%0Aproblems%20and%20challenges%20for%20future%20work%2C%20aiming%20to%20inspire%20researchers%20and%0Astakeholders%20in%20the%20IR%20field%20and%20beyond%20to%20better%20understand%20and%20mitigate%20bias%0Aand%20unfairness%20issues%20of%20IR%20in%20this%20LLM%20era.%20We%20also%20consistently%20maintain%20a%0AGitHub%20repository%20for%20the%20relevant%20papers%20and%20resources%20in%20this%20rising%0Adirection%20at%20https%3A//github.com/KID-22/LLM-IR-Bias-Fairness-Survey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11457v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unifying%20Bias%20and%20Unfairness%20in%20Information%20Retrieval%3A%20A%20Survey%20of%0A%20%20Challenges%20and%20Opportunities%20with%20Large%20Language%20Models&entry.906535625=Sunhao%20Dai%20and%20Chen%20Xu%20and%20Shicheng%20Xu%20and%20Liang%20Pang%20and%20Zhenhua%20Dong%20and%20Jun%20Xu&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%20large%20language%20models%20%28LLMs%29%2C%20information%0Aretrieval%20%28IR%29%20systems%2C%20such%20as%20search%20engines%20and%20recommender%20systems%2C%20have%0Aundergone%20a%20significant%20paradigm%20shift.%20This%20evolution%2C%20while%20heralding%20new%0Aopportunities%2C%20introduces%20emerging%20challenges%2C%20particularly%20in%20terms%20of%20biases%0Aand%20unfairness%2C%20which%20may%20threaten%20the%20information%20ecosystem.%20In%20this%20paper%2C%20we%0Apresent%20a%20comprehensive%20survey%20of%20existing%20works%20on%20emerging%20and%20pressing%20bias%0Aand%20unfairness%20issues%20in%20IR%20systems%20when%20the%20integration%20of%20LLMs.%20We%20first%0Aunify%20bias%20and%20unfairness%20issues%20as%20distribution%20mismatch%20problems%2C%20providing%20a%0Agroundwork%20for%20categorizing%20various%20mitigation%20strategies%20through%20distribution%0Aalignment.%20Subsequently%2C%20we%20systematically%20delve%20into%20the%20specific%20bias%20and%0Aunfairness%20issues%20arising%20from%20three%20critical%20stages%20of%20LLMs%20integration%20into%0AIR%20systems%3A%20data%20collection%2C%20model%20development%2C%20and%20result%20evaluation.%20In%20doing%0Aso%2C%20we%20meticulously%20review%20and%20analyze%20recent%20literature%2C%20focusing%20on%20the%0Adefinitions%2C%20characteristics%2C%20and%20corresponding%20mitigation%20strategies%0Aassociated%20with%20these%20issues.%20Finally%2C%20we%20identify%20and%20highlight%20some%20open%0Aproblems%20and%20challenges%20for%20future%20work%2C%20aiming%20to%20inspire%20researchers%20and%0Astakeholders%20in%20the%20IR%20field%20and%20beyond%20to%20better%20understand%20and%20mitigate%20bias%0Aand%20unfairness%20issues%20of%20IR%20in%20this%20LLM%20era.%20We%20also%20consistently%20maintain%20a%0AGitHub%20repository%20for%20the%20relevant%20papers%20and%20resources%20in%20this%20rising%0Adirection%20at%20https%3A//github.com/KID-22/LLM-IR-Bias-Fairness-Survey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11457v1&entry.124074799=Read"},
{"title": "The Brain Tumor Sequence Registration (BraTS-Reg) Challenge:\n  Establishing Correspondence Between Pre-Operative and Follow-up MRI Scans of\n  Diffuse Glioma Patients", "author": "Bhakti Baheti and Satrajit Chakrabarty and Hamed Akbari and Michel Bilello and Benedikt Wiestler and Julian Schwarting and Evan Calabrese and Jeffrey Rudie and Syed Abidi and Mina Mousa and Javier Villanueva-Meyer and Brandon K. K. Fields and Florian Kofler and Russell Takeshi Shinohara and Juan Eugenio Iglesias and Tony C. W. Mok and Albert C. S. Chung and Marek Wodzinski and Artur Jurgas and Niccolo Marini and Manfredo Atzori and Henning Muller and Christoph Grobroehmer and Hanna Siebert and Lasse Hansen and Mattias P. Heinrich and Luca Canalini and Jan Klein and Annika Gerken and Stefan Heldmann and Alessa Hering and Horst K. Hahn and Mingyuan Meng and Lei Bi and Dagan Feng and Jinman Kim and Ramy A. Zeineldin and Mohamed E. Karar and Franziska Mathis-Ullrich and Oliver Burgert and Javid Abderezaei and Aymeric Pionteck and Agamdeep Chopra and Mehmet Kurt and Kewei Yan and Yonghong Yan and Zhe Tang and Jianqiang Ma and Sahar Almahfouz Nasser and Nikhil Cherian Kurian and Mohit Meena and Saqib Shamsi and Amit Sethi and Nicholas J. Tustison and Brian B. Avants and Philip Cook and James C. Gee and Lin Tian and Hastings Greer and Marc Niethammer and Andrew Hoopes and Malte Hoffmann and Adrian V. Dalca and Stergios Christodoulidis and Theo Estiene and Maria Vakalopoulou and Nikos Paragios and Daniel S. Marcus and Christos Davatzikos and Aristeidis Sotiras and Bjoern Menze and Spyridon Bakas and Diana Waldmannstetter", "abstract": "  Registration of longitudinal brain MRI scans containing pathologies is\nchallenging due to dramatic changes in tissue appearance. Although there has\nbeen progress in developing general-purpose medical image registration\ntechniques, they have not yet attained the requisite precision and reliability\nfor this task, highlighting its inherent complexity. Here we describe the Brain\nTumor Sequence Registration (BraTS-Reg) challenge, as the first public\nbenchmark environment for deformable registration algorithms focusing on\nestimating correspondences between pre-operative and follow-up scans of the\nsame patient diagnosed with a diffuse brain glioma. The BraTS-Reg data comprise\nde-identified multi-institutional multi-parametric MRI (mpMRI) scans, curated\nfor size and resolution according to a canonical anatomical template, and\ndivided into training, validation, and testing sets. Clinical experts annotated\nground truth (GT) landmark points of anatomical locations distinct across the\ntemporal domain. Quantitative evaluation and ranking were based on the Median\nEuclidean Error (MEE), Robustness, and the determinant of the Jacobian of the\ndisplacement field. The top-ranked methodologies yielded similar performance\nacross all evaluation metrics and shared several methodological commonalities,\nincluding pre-alignment, deep neural networks, inverse consistency analysis,\nand test-time instance optimization per-case basis as a post-processing step.\nThe top-ranked method attained the MEE at or below that of the inter-rater\nvariability for approximately 60% of the evaluated landmarks, underscoring the\nscope for further accuracy and robustness improvements, especially relative to\nhuman experts. The aim of BraTS-Reg is to continue to serve as an active\nresource for research, with the data and online evaluation tools accessible at\nhttps://bratsreg.github.io/.\n", "link": "http://arxiv.org/abs/2112.06979v2", "date": "2024-04-17", "relevancy": 1.8533, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4764}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4565}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4478}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Brain%20Tumor%20Sequence%20Registration%20%28BraTS-Reg%29%20Challenge%3A%0A%20%20Establishing%20Correspondence%20Between%20Pre-Operative%20and%20Follow-up%20MRI%20Scans%20of%0A%20%20Diffuse%20Glioma%20Patients&body=Title%3A%20The%20Brain%20Tumor%20Sequence%20Registration%20%28BraTS-Reg%29%20Challenge%3A%0A%20%20Establishing%20Correspondence%20Between%20Pre-Operative%20and%20Follow-up%20MRI%20Scans%20of%0A%20%20Diffuse%20Glioma%20Patients%0AAuthor%3A%20Bhakti%20Baheti%20and%20Satrajit%20Chakrabarty%20and%20Hamed%20Akbari%20and%20Michel%20Bilello%20and%20Benedikt%20Wiestler%20and%20Julian%20Schwarting%20and%20Evan%20Calabrese%20and%20Jeffrey%20Rudie%20and%20Syed%20Abidi%20and%20Mina%20Mousa%20and%20Javier%20Villanueva-Meyer%20and%20Brandon%20K.%20K.%20Fields%20and%20Florian%20Kofler%20and%20Russell%20Takeshi%20Shinohara%20and%20Juan%20Eugenio%20Iglesias%20and%20Tony%20C.%20W.%20Mok%20and%20Albert%20C.%20S.%20Chung%20and%20Marek%20Wodzinski%20and%20Artur%20Jurgas%20and%20Niccolo%20Marini%20and%20Manfredo%20Atzori%20and%20Henning%20Muller%20and%20Christoph%20Grobroehmer%20and%20Hanna%20Siebert%20and%20Lasse%20Hansen%20and%20Mattias%20P.%20Heinrich%20and%20Luca%20Canalini%20and%20Jan%20Klein%20and%20Annika%20Gerken%20and%20Stefan%20Heldmann%20and%20Alessa%20Hering%20and%20Horst%20K.%20Hahn%20and%20Mingyuan%20Meng%20and%20Lei%20Bi%20and%20Dagan%20Feng%20and%20Jinman%20Kim%20and%20Ramy%20A.%20Zeineldin%20and%20Mohamed%20E.%20Karar%20and%20Franziska%20Mathis-Ullrich%20and%20Oliver%20Burgert%20and%20Javid%20Abderezaei%20and%20Aymeric%20Pionteck%20and%20Agamdeep%20Chopra%20and%20Mehmet%20Kurt%20and%20Kewei%20Yan%20and%20Yonghong%20Yan%20and%20Zhe%20Tang%20and%20Jianqiang%20Ma%20and%20Sahar%20Almahfouz%20Nasser%20and%20Nikhil%20Cherian%20Kurian%20and%20Mohit%20Meena%20and%20Saqib%20Shamsi%20and%20Amit%20Sethi%20and%20Nicholas%20J.%20Tustison%20and%20Brian%20B.%20Avants%20and%20Philip%20Cook%20and%20James%20C.%20Gee%20and%20Lin%20Tian%20and%20Hastings%20Greer%20and%20Marc%20Niethammer%20and%20Andrew%20Hoopes%20and%20Malte%20Hoffmann%20and%20Adrian%20V.%20Dalca%20and%20Stergios%20Christodoulidis%20and%20Theo%20Estiene%20and%20Maria%20Vakalopoulou%20and%20Nikos%20Paragios%20and%20Daniel%20S.%20Marcus%20and%20Christos%20Davatzikos%20and%20Aristeidis%20Sotiras%20and%20Bjoern%20Menze%20and%20Spyridon%20Bakas%20and%20Diana%20Waldmannstetter%0AAbstract%3A%20%20%20Registration%20of%20longitudinal%20brain%20MRI%20scans%20containing%20pathologies%20is%0Achallenging%20due%20to%20dramatic%20changes%20in%20tissue%20appearance.%20Although%20there%20has%0Abeen%20progress%20in%20developing%20general-purpose%20medical%20image%20registration%0Atechniques%2C%20they%20have%20not%20yet%20attained%20the%20requisite%20precision%20and%20reliability%0Afor%20this%20task%2C%20highlighting%20its%20inherent%20complexity.%20Here%20we%20describe%20the%20Brain%0ATumor%20Sequence%20Registration%20%28BraTS-Reg%29%20challenge%2C%20as%20the%20first%20public%0Abenchmark%20environment%20for%20deformable%20registration%20algorithms%20focusing%20on%0Aestimating%20correspondences%20between%20pre-operative%20and%20follow-up%20scans%20of%20the%0Asame%20patient%20diagnosed%20with%20a%20diffuse%20brain%20glioma.%20The%20BraTS-Reg%20data%20comprise%0Ade-identified%20multi-institutional%20multi-parametric%20MRI%20%28mpMRI%29%20scans%2C%20curated%0Afor%20size%20and%20resolution%20according%20to%20a%20canonical%20anatomical%20template%2C%20and%0Adivided%20into%20training%2C%20validation%2C%20and%20testing%20sets.%20Clinical%20experts%20annotated%0Aground%20truth%20%28GT%29%20landmark%20points%20of%20anatomical%20locations%20distinct%20across%20the%0Atemporal%20domain.%20Quantitative%20evaluation%20and%20ranking%20were%20based%20on%20the%20Median%0AEuclidean%20Error%20%28MEE%29%2C%20Robustness%2C%20and%20the%20determinant%20of%20the%20Jacobian%20of%20the%0Adisplacement%20field.%20The%20top-ranked%20methodologies%20yielded%20similar%20performance%0Aacross%20all%20evaluation%20metrics%20and%20shared%20several%20methodological%20commonalities%2C%0Aincluding%20pre-alignment%2C%20deep%20neural%20networks%2C%20inverse%20consistency%20analysis%2C%0Aand%20test-time%20instance%20optimization%20per-case%20basis%20as%20a%20post-processing%20step.%0AThe%20top-ranked%20method%20attained%20the%20MEE%20at%20or%20below%20that%20of%20the%20inter-rater%0Avariability%20for%20approximately%2060%25%20of%20the%20evaluated%20landmarks%2C%20underscoring%20the%0Ascope%20for%20further%20accuracy%20and%20robustness%20improvements%2C%20especially%20relative%20to%0Ahuman%20experts.%20The%20aim%20of%20BraTS-Reg%20is%20to%20continue%20to%20serve%20as%20an%20active%0Aresource%20for%20research%2C%20with%20the%20data%20and%20online%20evaluation%20tools%20accessible%20at%0Ahttps%3A//bratsreg.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2112.06979v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Brain%20Tumor%20Sequence%20Registration%20%28BraTS-Reg%29%20Challenge%3A%0A%20%20Establishing%20Correspondence%20Between%20Pre-Operative%20and%20Follow-up%20MRI%20Scans%20of%0A%20%20Diffuse%20Glioma%20Patients&entry.906535625=Bhakti%20Baheti%20and%20Satrajit%20Chakrabarty%20and%20Hamed%20Akbari%20and%20Michel%20Bilello%20and%20Benedikt%20Wiestler%20and%20Julian%20Schwarting%20and%20Evan%20Calabrese%20and%20Jeffrey%20Rudie%20and%20Syed%20Abidi%20and%20Mina%20Mousa%20and%20Javier%20Villanueva-Meyer%20and%20Brandon%20K.%20K.%20Fields%20and%20Florian%20Kofler%20and%20Russell%20Takeshi%20Shinohara%20and%20Juan%20Eugenio%20Iglesias%20and%20Tony%20C.%20W.%20Mok%20and%20Albert%20C.%20S.%20Chung%20and%20Marek%20Wodzinski%20and%20Artur%20Jurgas%20and%20Niccolo%20Marini%20and%20Manfredo%20Atzori%20and%20Henning%20Muller%20and%20Christoph%20Grobroehmer%20and%20Hanna%20Siebert%20and%20Lasse%20Hansen%20and%20Mattias%20P.%20Heinrich%20and%20Luca%20Canalini%20and%20Jan%20Klein%20and%20Annika%20Gerken%20and%20Stefan%20Heldmann%20and%20Alessa%20Hering%20and%20Horst%20K.%20Hahn%20and%20Mingyuan%20Meng%20and%20Lei%20Bi%20and%20Dagan%20Feng%20and%20Jinman%20Kim%20and%20Ramy%20A.%20Zeineldin%20and%20Mohamed%20E.%20Karar%20and%20Franziska%20Mathis-Ullrich%20and%20Oliver%20Burgert%20and%20Javid%20Abderezaei%20and%20Aymeric%20Pionteck%20and%20Agamdeep%20Chopra%20and%20Mehmet%20Kurt%20and%20Kewei%20Yan%20and%20Yonghong%20Yan%20and%20Zhe%20Tang%20and%20Jianqiang%20Ma%20and%20Sahar%20Almahfouz%20Nasser%20and%20Nikhil%20Cherian%20Kurian%20and%20Mohit%20Meena%20and%20Saqib%20Shamsi%20and%20Amit%20Sethi%20and%20Nicholas%20J.%20Tustison%20and%20Brian%20B.%20Avants%20and%20Philip%20Cook%20and%20James%20C.%20Gee%20and%20Lin%20Tian%20and%20Hastings%20Greer%20and%20Marc%20Niethammer%20and%20Andrew%20Hoopes%20and%20Malte%20Hoffmann%20and%20Adrian%20V.%20Dalca%20and%20Stergios%20Christodoulidis%20and%20Theo%20Estiene%20and%20Maria%20Vakalopoulou%20and%20Nikos%20Paragios%20and%20Daniel%20S.%20Marcus%20and%20Christos%20Davatzikos%20and%20Aristeidis%20Sotiras%20and%20Bjoern%20Menze%20and%20Spyridon%20Bakas%20and%20Diana%20Waldmannstetter&entry.1292438233=%20%20Registration%20of%20longitudinal%20brain%20MRI%20scans%20containing%20pathologies%20is%0Achallenging%20due%20to%20dramatic%20changes%20in%20tissue%20appearance.%20Although%20there%20has%0Abeen%20progress%20in%20developing%20general-purpose%20medical%20image%20registration%0Atechniques%2C%20they%20have%20not%20yet%20attained%20the%20requisite%20precision%20and%20reliability%0Afor%20this%20task%2C%20highlighting%20its%20inherent%20complexity.%20Here%20we%20describe%20the%20Brain%0ATumor%20Sequence%20Registration%20%28BraTS-Reg%29%20challenge%2C%20as%20the%20first%20public%0Abenchmark%20environment%20for%20deformable%20registration%20algorithms%20focusing%20on%0Aestimating%20correspondences%20between%20pre-operative%20and%20follow-up%20scans%20of%20the%0Asame%20patient%20diagnosed%20with%20a%20diffuse%20brain%20glioma.%20The%20BraTS-Reg%20data%20comprise%0Ade-identified%20multi-institutional%20multi-parametric%20MRI%20%28mpMRI%29%20scans%2C%20curated%0Afor%20size%20and%20resolution%20according%20to%20a%20canonical%20anatomical%20template%2C%20and%0Adivided%20into%20training%2C%20validation%2C%20and%20testing%20sets.%20Clinical%20experts%20annotated%0Aground%20truth%20%28GT%29%20landmark%20points%20of%20anatomical%20locations%20distinct%20across%20the%0Atemporal%20domain.%20Quantitative%20evaluation%20and%20ranking%20were%20based%20on%20the%20Median%0AEuclidean%20Error%20%28MEE%29%2C%20Robustness%2C%20and%20the%20determinant%20of%20the%20Jacobian%20of%20the%0Adisplacement%20field.%20The%20top-ranked%20methodologies%20yielded%20similar%20performance%0Aacross%20all%20evaluation%20metrics%20and%20shared%20several%20methodological%20commonalities%2C%0Aincluding%20pre-alignment%2C%20deep%20neural%20networks%2C%20inverse%20consistency%20analysis%2C%0Aand%20test-time%20instance%20optimization%20per-case%20basis%20as%20a%20post-processing%20step.%0AThe%20top-ranked%20method%20attained%20the%20MEE%20at%20or%20below%20that%20of%20the%20inter-rater%0Avariability%20for%20approximately%2060%25%20of%20the%20evaluated%20landmarks%2C%20underscoring%20the%0Ascope%20for%20further%20accuracy%20and%20robustness%20improvements%2C%20especially%20relative%20to%0Ahuman%20experts.%20The%20aim%20of%20BraTS-Reg%20is%20to%20continue%20to%20serve%20as%20an%20active%0Aresource%20for%20research%2C%20with%20the%20data%20and%20online%20evaluation%20tools%20accessible%20at%0Ahttps%3A//bratsreg.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2112.06979v2&entry.124074799=Read"},
{"title": "Spiral Complete Coverage Path Planning Based on Conformal Slit Mapping\n  in Multi-connected Domains", "author": "Changqing Shen and Sihao Mao and Bingzhou Xu and Ziwei Wang and Xiaojian Zhang and Sijie Yan and Han Ding", "abstract": "  The generation of smoother and shorter spiral complete coverage paths in\nmulti-connected domains is a crucial research topic in path planning for\nrobotic cavity machining and other related fields. Traditional methods for\nspiral path planning in multi-connected domains typically incorporate a\nsubregion division procedure that leads to excessive subregion bridging,\nrequiring longer, more sharply turning, and unevenly spaced spirals to achieve\ncomplete coverage. To address this issue, this paper proposes a novel spiral\ncomplete coverage path planning method using conformal slit mapping. It takes\nadvantage of the fact that conformal slit mapping can transform multi-connected\ndomains into regular disks or annuluses without the need for subregion\ndivision. Firstly, a slit mapping calculation technique is proposed for\nsegmented cubic spline boundaries with corners. Secondly, a spiral path spacing\ncontrol method is developed based on the maximum inscribed circle radius\nbetween adjacent conformal slit mapping iso-parameters. Thirdly, the spiral\ncoverage path is derived by offsetting iso-parameters. Numerical experiments\nindicate that our method shares a comparable order of magnitude in computation\ntime with the traditional PDE-based spiral complete coverage path method, but\nit excels in optimizing total path length, smoothness, and spacing consistency.\nFinally, we performed experiments on cavity milling and dry runs to compare the\nnew method with the traditional PDE-based method in terms of machining duration\nand steering impact, respectively. The comparison reveals that, with both\nalgorithms achieving complete coverage, the new algorithm reduces machining\ntime and steering impact by 12.34% and 22.78%, respectively, compared with the\ntraditional PDE-based method.\n", "link": "http://arxiv.org/abs/2309.10655v2", "date": "2024-04-17", "relevancy": 1.8499, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4819}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4696}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4402}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Spiral%20Complete%20Coverage%20Path%20Planning%20Based%20on%20Conformal%20Slit%20Mapping%0A%20%20in%20Multi-connected%20Domains&body=Title%3A%20Spiral%20Complete%20Coverage%20Path%20Planning%20Based%20on%20Conformal%20Slit%20Mapping%0A%20%20in%20Multi-connected%20Domains%0AAuthor%3A%20Changqing%20Shen%20and%20Sihao%20Mao%20and%20Bingzhou%20Xu%20and%20Ziwei%20Wang%20and%20Xiaojian%20Zhang%20and%20Sijie%20Yan%20and%20Han%20Ding%0AAbstract%3A%20%20%20The%20generation%20of%20smoother%20and%20shorter%20spiral%20complete%20coverage%20paths%20in%0Amulti-connected%20domains%20is%20a%20crucial%20research%20topic%20in%20path%20planning%20for%0Arobotic%20cavity%20machining%20and%20other%20related%20fields.%20Traditional%20methods%20for%0Aspiral%20path%20planning%20in%20multi-connected%20domains%20typically%20incorporate%20a%0Asubregion%20division%20procedure%20that%20leads%20to%20excessive%20subregion%20bridging%2C%0Arequiring%20longer%2C%20more%20sharply%20turning%2C%20and%20unevenly%20spaced%20spirals%20to%20achieve%0Acomplete%20coverage.%20To%20address%20this%20issue%2C%20this%20paper%20proposes%20a%20novel%20spiral%0Acomplete%20coverage%20path%20planning%20method%20using%20conformal%20slit%20mapping.%20It%20takes%0Aadvantage%20of%20the%20fact%20that%20conformal%20slit%20mapping%20can%20transform%20multi-connected%0Adomains%20into%20regular%20disks%20or%20annuluses%20without%20the%20need%20for%20subregion%0Adivision.%20Firstly%2C%20a%20slit%20mapping%20calculation%20technique%20is%20proposed%20for%0Asegmented%20cubic%20spline%20boundaries%20with%20corners.%20Secondly%2C%20a%20spiral%20path%20spacing%0Acontrol%20method%20is%20developed%20based%20on%20the%20maximum%20inscribed%20circle%20radius%0Abetween%20adjacent%20conformal%20slit%20mapping%20iso-parameters.%20Thirdly%2C%20the%20spiral%0Acoverage%20path%20is%20derived%20by%20offsetting%20iso-parameters.%20Numerical%20experiments%0Aindicate%20that%20our%20method%20shares%20a%20comparable%20order%20of%20magnitude%20in%20computation%0Atime%20with%20the%20traditional%20PDE-based%20spiral%20complete%20coverage%20path%20method%2C%20but%0Ait%20excels%20in%20optimizing%20total%20path%20length%2C%20smoothness%2C%20and%20spacing%20consistency.%0AFinally%2C%20we%20performed%20experiments%20on%20cavity%20milling%20and%20dry%20runs%20to%20compare%20the%0Anew%20method%20with%20the%20traditional%20PDE-based%20method%20in%20terms%20of%20machining%20duration%0Aand%20steering%20impact%2C%20respectively.%20The%20comparison%20reveals%20that%2C%20with%20both%0Aalgorithms%20achieving%20complete%20coverage%2C%20the%20new%20algorithm%20reduces%20machining%0Atime%20and%20steering%20impact%20by%2012.34%25%20and%2022.78%25%2C%20respectively%2C%20compared%20with%20the%0Atraditional%20PDE-based%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.10655v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spiral%20Complete%20Coverage%20Path%20Planning%20Based%20on%20Conformal%20Slit%20Mapping%0A%20%20in%20Multi-connected%20Domains&entry.906535625=Changqing%20Shen%20and%20Sihao%20Mao%20and%20Bingzhou%20Xu%20and%20Ziwei%20Wang%20and%20Xiaojian%20Zhang%20and%20Sijie%20Yan%20and%20Han%20Ding&entry.1292438233=%20%20The%20generation%20of%20smoother%20and%20shorter%20spiral%20complete%20coverage%20paths%20in%0Amulti-connected%20domains%20is%20a%20crucial%20research%20topic%20in%20path%20planning%20for%0Arobotic%20cavity%20machining%20and%20other%20related%20fields.%20Traditional%20methods%20for%0Aspiral%20path%20planning%20in%20multi-connected%20domains%20typically%20incorporate%20a%0Asubregion%20division%20procedure%20that%20leads%20to%20excessive%20subregion%20bridging%2C%0Arequiring%20longer%2C%20more%20sharply%20turning%2C%20and%20unevenly%20spaced%20spirals%20to%20achieve%0Acomplete%20coverage.%20To%20address%20this%20issue%2C%20this%20paper%20proposes%20a%20novel%20spiral%0Acomplete%20coverage%20path%20planning%20method%20using%20conformal%20slit%20mapping.%20It%20takes%0Aadvantage%20of%20the%20fact%20that%20conformal%20slit%20mapping%20can%20transform%20multi-connected%0Adomains%20into%20regular%20disks%20or%20annuluses%20without%20the%20need%20for%20subregion%0Adivision.%20Firstly%2C%20a%20slit%20mapping%20calculation%20technique%20is%20proposed%20for%0Asegmented%20cubic%20spline%20boundaries%20with%20corners.%20Secondly%2C%20a%20spiral%20path%20spacing%0Acontrol%20method%20is%20developed%20based%20on%20the%20maximum%20inscribed%20circle%20radius%0Abetween%20adjacent%20conformal%20slit%20mapping%20iso-parameters.%20Thirdly%2C%20the%20spiral%0Acoverage%20path%20is%20derived%20by%20offsetting%20iso-parameters.%20Numerical%20experiments%0Aindicate%20that%20our%20method%20shares%20a%20comparable%20order%20of%20magnitude%20in%20computation%0Atime%20with%20the%20traditional%20PDE-based%20spiral%20complete%20coverage%20path%20method%2C%20but%0Ait%20excels%20in%20optimizing%20total%20path%20length%2C%20smoothness%2C%20and%20spacing%20consistency.%0AFinally%2C%20we%20performed%20experiments%20on%20cavity%20milling%20and%20dry%20runs%20to%20compare%20the%0Anew%20method%20with%20the%20traditional%20PDE-based%20method%20in%20terms%20of%20machining%20duration%0Aand%20steering%20impact%2C%20respectively.%20The%20comparison%20reveals%20that%2C%20with%20both%0Aalgorithms%20achieving%20complete%20coverage%2C%20the%20new%20algorithm%20reduces%20machining%0Atime%20and%20steering%20impact%20by%2012.34%25%20and%2022.78%25%2C%20respectively%2C%20compared%20with%20the%0Atraditional%20PDE-based%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.10655v2&entry.124074799=Read"},
{"title": "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models", "author": "Yougang Lyu and Lingyong Yan and Shuaiqiang Wang and Haibo Shi and Dawei Yin and Pengjie Ren and Zhumin Chen and Maarten de Rijke and Zhaochun Ren", "abstract": "  Despite their success at many natural language processing (NLP) tasks, large\nlanguage models still struggle to effectively leverage knowledge for\nknowledge-intensive tasks, manifesting limitations such as generating\nincomplete, non-factual, or illogical answers. These limitations stem from\ninadequate knowledge awareness of LLMs during vanilla fine-tuning. To address\nthese problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to\nimprove fine-grained and coarse-grained knowledge awareness of LLMs. We devise\na fine-grained knowledge augmentation stage to train LLMs to identify difficult\nfine-grained knowledge in answers. We also propose a coarse-grained knowledge\ncomparison stage to train LLMs to distinguish between reliable and unreliable\nknowledge, in three aspects: completeness, factuality, and logicality.\nExtensive experiments on both generic and medical question answering (QA)\ndatasets confirm the effectiveness of KnowTuning, through automatic and human\nevaluations, across various sizes of LLMs. We further verify that KnowTuning\ngenerates more facts with less factual error rate under fine-grained facts\nevaluation.\n", "link": "http://arxiv.org/abs/2402.11176v2", "date": "2024-04-17", "relevancy": 1.8487, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5055}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.462}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.445}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20KnowTuning%3A%20Knowledge-aware%20Fine-tuning%20for%20Large%20Language%20Models&body=Title%3A%20KnowTuning%3A%20Knowledge-aware%20Fine-tuning%20for%20Large%20Language%20Models%0AAuthor%3A%20Yougang%20Lyu%20and%20Lingyong%20Yan%20and%20Shuaiqiang%20Wang%20and%20Haibo%20Shi%20and%20Dawei%20Yin%20and%20Pengjie%20Ren%20and%20Zhumin%20Chen%20and%20Maarten%20de%20Rijke%20and%20Zhaochun%20Ren%0AAbstract%3A%20%20%20Despite%20their%20success%20at%20many%20natural%20language%20processing%20%28NLP%29%20tasks%2C%20large%0Alanguage%20models%20still%20struggle%20to%20effectively%20leverage%20knowledge%20for%0Aknowledge-intensive%20tasks%2C%20manifesting%20limitations%20such%20as%20generating%0Aincomplete%2C%20non-factual%2C%20or%20illogical%20answers.%20These%20limitations%20stem%20from%0Ainadequate%20knowledge%20awareness%20of%20LLMs%20during%20vanilla%20fine-tuning.%20To%20address%0Athese%20problems%2C%20we%20propose%20a%20knowledge-aware%20fine-tuning%20%28KnowTuning%29%20method%20to%0Aimprove%20fine-grained%20and%20coarse-grained%20knowledge%20awareness%20of%20LLMs.%20We%20devise%0Aa%20fine-grained%20knowledge%20augmentation%20stage%20to%20train%20LLMs%20to%20identify%20difficult%0Afine-grained%20knowledge%20in%20answers.%20We%20also%20propose%20a%20coarse-grained%20knowledge%0Acomparison%20stage%20to%20train%20LLMs%20to%20distinguish%20between%20reliable%20and%20unreliable%0Aknowledge%2C%20in%20three%20aspects%3A%20completeness%2C%20factuality%2C%20and%20logicality.%0AExtensive%20experiments%20on%20both%20generic%20and%20medical%20question%20answering%20%28QA%29%0Adatasets%20confirm%20the%20effectiveness%20of%20KnowTuning%2C%20through%20automatic%20and%20human%0Aevaluations%2C%20across%20various%20sizes%20of%20LLMs.%20We%20further%20verify%20that%20KnowTuning%0Agenerates%20more%20facts%20with%20less%20factual%20error%20rate%20under%20fine-grained%20facts%0Aevaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11176v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KnowTuning%3A%20Knowledge-aware%20Fine-tuning%20for%20Large%20Language%20Models&entry.906535625=Yougang%20Lyu%20and%20Lingyong%20Yan%20and%20Shuaiqiang%20Wang%20and%20Haibo%20Shi%20and%20Dawei%20Yin%20and%20Pengjie%20Ren%20and%20Zhumin%20Chen%20and%20Maarten%20de%20Rijke%20and%20Zhaochun%20Ren&entry.1292438233=%20%20Despite%20their%20success%20at%20many%20natural%20language%20processing%20%28NLP%29%20tasks%2C%20large%0Alanguage%20models%20still%20struggle%20to%20effectively%20leverage%20knowledge%20for%0Aknowledge-intensive%20tasks%2C%20manifesting%20limitations%20such%20as%20generating%0Aincomplete%2C%20non-factual%2C%20or%20illogical%20answers.%20These%20limitations%20stem%20from%0Ainadequate%20knowledge%20awareness%20of%20LLMs%20during%20vanilla%20fine-tuning.%20To%20address%0Athese%20problems%2C%20we%20propose%20a%20knowledge-aware%20fine-tuning%20%28KnowTuning%29%20method%20to%0Aimprove%20fine-grained%20and%20coarse-grained%20knowledge%20awareness%20of%20LLMs.%20We%20devise%0Aa%20fine-grained%20knowledge%20augmentation%20stage%20to%20train%20LLMs%20to%20identify%20difficult%0Afine-grained%20knowledge%20in%20answers.%20We%20also%20propose%20a%20coarse-grained%20knowledge%0Acomparison%20stage%20to%20train%20LLMs%20to%20distinguish%20between%20reliable%20and%20unreliable%0Aknowledge%2C%20in%20three%20aspects%3A%20completeness%2C%20factuality%2C%20and%20logicality.%0AExtensive%20experiments%20on%20both%20generic%20and%20medical%20question%20answering%20%28QA%29%0Adatasets%20confirm%20the%20effectiveness%20of%20KnowTuning%2C%20through%20automatic%20and%20human%0Aevaluations%2C%20across%20various%20sizes%20of%20LLMs.%20We%20further%20verify%20that%20KnowTuning%0Agenerates%20more%20facts%20with%20less%20factual%20error%20rate%20under%20fine-grained%20facts%0Aevaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11176v2&entry.124074799=Read"},
{"title": "Improving Composed Image Retrieval via Contrastive Learning with Scaling\n  Positives and Negatives", "author": "Zhangchi Feng and Richong Zhang and Zhijie Nie", "abstract": "  The Composed Image Retrieval (CIR) task aims to retrieve target images using\na composed query consisting of a reference image and a modified text. Advanced\nmethods often utilize contrastive learning as the optimization objective, which\nbenefits from adequate positive and negative examples. However, the triplet for\nCIR incurs high manual annotation costs, resulting in limited positive\nexamples. Furthermore, existing methods commonly use in-batch negative\nsampling, which reduces the negative number available for the model. To address\nthe problem of lack of positives, we propose a data generation method by\nleveraging a multi-modal large language model to construct triplets for CIR. To\nintroduce more negatives during fine-tuning, we design a two-stage fine-tuning\nframework for CIR, whose second stage introduces plenty of static\nrepresentations of negatives to optimize the representation space rapidly. The\nabove two improvements can be effectively stacked and designed to be\nplug-and-play, easily applied to existing CIR models without changing their\noriginal architectures. Extensive experiments and ablation analysis demonstrate\nthat our method effectively scales positives and negatives and achieves\nstate-of-the-art results on both FashionIQ and CIRR datasets. In addition, our\nmethods also perform well in zero-shot composed image retrieval, providing a\nnew CIR solution for the low-resources scenario.\n", "link": "http://arxiv.org/abs/2404.11317v1", "date": "2024-04-17", "relevancy": 1.6873, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5655}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5597}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5574}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20Composed%20Image%20Retrieval%20via%20Contrastive%20Learning%20with%20Scaling%0A%20%20Positives%20and%20Negatives&body=Title%3A%20Improving%20Composed%20Image%20Retrieval%20via%20Contrastive%20Learning%20with%20Scaling%0A%20%20Positives%20and%20Negatives%0AAuthor%3A%20Zhangchi%20Feng%20and%20Richong%20Zhang%20and%20Zhijie%20Nie%0AAbstract%3A%20%20%20The%20Composed%20Image%20Retrieval%20%28CIR%29%20task%20aims%20to%20retrieve%20target%20images%20using%0Aa%20composed%20query%20consisting%20of%20a%20reference%20image%20and%20a%20modified%20text.%20Advanced%0Amethods%20often%20utilize%20contrastive%20learning%20as%20the%20optimization%20objective%2C%20which%0Abenefits%20from%20adequate%20positive%20and%20negative%20examples.%20However%2C%20the%20triplet%20for%0ACIR%20incurs%20high%20manual%20annotation%20costs%2C%20resulting%20in%20limited%20positive%0Aexamples.%20Furthermore%2C%20existing%20methods%20commonly%20use%20in-batch%20negative%0Asampling%2C%20which%20reduces%20the%20negative%20number%20available%20for%20the%20model.%20To%20address%0Athe%20problem%20of%20lack%20of%20positives%2C%20we%20propose%20a%20data%20generation%20method%20by%0Aleveraging%20a%20multi-modal%20large%20language%20model%20to%20construct%20triplets%20for%20CIR.%20To%0Aintroduce%20more%20negatives%20during%20fine-tuning%2C%20we%20design%20a%20two-stage%20fine-tuning%0Aframework%20for%20CIR%2C%20whose%20second%20stage%20introduces%20plenty%20of%20static%0Arepresentations%20of%20negatives%20to%20optimize%20the%20representation%20space%20rapidly.%20The%0Aabove%20two%20improvements%20can%20be%20effectively%20stacked%20and%20designed%20to%20be%0Aplug-and-play%2C%20easily%20applied%20to%20existing%20CIR%20models%20without%20changing%20their%0Aoriginal%20architectures.%20Extensive%20experiments%20and%20ablation%20analysis%20demonstrate%0Athat%20our%20method%20effectively%20scales%20positives%20and%20negatives%20and%20achieves%0Astate-of-the-art%20results%20on%20both%20FashionIQ%20and%20CIRR%20datasets.%20In%20addition%2C%20our%0Amethods%20also%20perform%20well%20in%20zero-shot%20composed%20image%20retrieval%2C%20providing%20a%0Anew%20CIR%20solution%20for%20the%20low-resources%20scenario.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11317v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Composed%20Image%20Retrieval%20via%20Contrastive%20Learning%20with%20Scaling%0A%20%20Positives%20and%20Negatives&entry.906535625=Zhangchi%20Feng%20and%20Richong%20Zhang%20and%20Zhijie%20Nie&entry.1292438233=%20%20The%20Composed%20Image%20Retrieval%20%28CIR%29%20task%20aims%20to%20retrieve%20target%20images%20using%0Aa%20composed%20query%20consisting%20of%20a%20reference%20image%20and%20a%20modified%20text.%20Advanced%0Amethods%20often%20utilize%20contrastive%20learning%20as%20the%20optimization%20objective%2C%20which%0Abenefits%20from%20adequate%20positive%20and%20negative%20examples.%20However%2C%20the%20triplet%20for%0ACIR%20incurs%20high%20manual%20annotation%20costs%2C%20resulting%20in%20limited%20positive%0Aexamples.%20Furthermore%2C%20existing%20methods%20commonly%20use%20in-batch%20negative%0Asampling%2C%20which%20reduces%20the%20negative%20number%20available%20for%20the%20model.%20To%20address%0Athe%20problem%20of%20lack%20of%20positives%2C%20we%20propose%20a%20data%20generation%20method%20by%0Aleveraging%20a%20multi-modal%20large%20language%20model%20to%20construct%20triplets%20for%20CIR.%20To%0Aintroduce%20more%20negatives%20during%20fine-tuning%2C%20we%20design%20a%20two-stage%20fine-tuning%0Aframework%20for%20CIR%2C%20whose%20second%20stage%20introduces%20plenty%20of%20static%0Arepresentations%20of%20negatives%20to%20optimize%20the%20representation%20space%20rapidly.%20The%0Aabove%20two%20improvements%20can%20be%20effectively%20stacked%20and%20designed%20to%20be%0Aplug-and-play%2C%20easily%20applied%20to%20existing%20CIR%20models%20without%20changing%20their%0Aoriginal%20architectures.%20Extensive%20experiments%20and%20ablation%20analysis%20demonstrate%0Athat%20our%20method%20effectively%20scales%20positives%20and%20negatives%20and%20achieves%0Astate-of-the-art%20results%20on%20both%20FashionIQ%20and%20CIRR%20datasets.%20In%20addition%2C%20our%0Amethods%20also%20perform%20well%20in%20zero-shot%20composed%20image%20retrieval%2C%20providing%20a%0Anew%20CIR%20solution%20for%20the%20low-resources%20scenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11317v1&entry.124074799=Read"},
{"title": "Leveraging Foundation Models for Content-Based Medical Image Retrieval\n  in Radiology", "author": "Stefan Denner and David Zimmerer and Dimitrios Bounias and Markus Bujotzek and Shuhan Xiao and Lisa Kausch and Philipp Schader and Tobias Penzkofer and Paul F. J\u00e4ger and Klaus Maier-Hein", "abstract": "  Content-based image retrieval (CBIR) has the potential to significantly\nimprove diagnostic aid and medical research in radiology. Current CBIR systems\nface limitations due to their specialization to certain pathologies, limiting\ntheir utility. In response, we propose using vision foundation models as\npowerful and versatile off-the-shelf feature extractors for content-based\nmedical image retrieval. By benchmarking these models on a comprehensive\ndataset of 1.6 million 2D radiological images spanning four modalities and 161\npathologies, we identify weakly-supervised models as superior, achieving a P@1\nof up to 0.594. This performance not only competes with a specialized model but\ndoes so without the need for fine-tuning. Our analysis further explores the\nchallenges in retrieving pathological versus anatomical structures, indicating\nthat accurate retrieval of pathological features presents greater difficulty.\nDespite these challenges, our research underscores the vast potential of\nfoundation models for CBIR in radiology, proposing a shift towards versatile,\ngeneral-purpose medical image retrieval systems that do not require specific\ntuning.\n", "link": "http://arxiv.org/abs/2403.06567v3", "date": "2024-04-17", "relevancy": 1.4057, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4848}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4645}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4637}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Foundation%20Models%20for%20Content-Based%20Medical%20Image%20Retrieval%0A%20%20in%20Radiology&body=Title%3A%20Leveraging%20Foundation%20Models%20for%20Content-Based%20Medical%20Image%20Retrieval%0A%20%20in%20Radiology%0AAuthor%3A%20Stefan%20Denner%20and%20David%20Zimmerer%20and%20Dimitrios%20Bounias%20and%20Markus%20Bujotzek%20and%20Shuhan%20Xiao%20and%20Lisa%20Kausch%20and%20Philipp%20Schader%20and%20Tobias%20Penzkofer%20and%20Paul%20F.%20J%C3%A4ger%20and%20Klaus%20Maier-Hein%0AAbstract%3A%20%20%20Content-based%20image%20retrieval%20%28CBIR%29%20has%20the%20potential%20to%20significantly%0Aimprove%20diagnostic%20aid%20and%20medical%20research%20in%20radiology.%20Current%20CBIR%20systems%0Aface%20limitations%20due%20to%20their%20specialization%20to%20certain%20pathologies%2C%20limiting%0Atheir%20utility.%20In%20response%2C%20we%20propose%20using%20vision%20foundation%20models%20as%0Apowerful%20and%20versatile%20off-the-shelf%20feature%20extractors%20for%20content-based%0Amedical%20image%20retrieval.%20By%20benchmarking%20these%20models%20on%20a%20comprehensive%0Adataset%20of%201.6%20million%202D%20radiological%20images%20spanning%20four%20modalities%20and%20161%0Apathologies%2C%20we%20identify%20weakly-supervised%20models%20as%20superior%2C%20achieving%20a%20P%401%0Aof%20up%20to%200.594.%20This%20performance%20not%20only%20competes%20with%20a%20specialized%20model%20but%0Adoes%20so%20without%20the%20need%20for%20fine-tuning.%20Our%20analysis%20further%20explores%20the%0Achallenges%20in%20retrieving%20pathological%20versus%20anatomical%20structures%2C%20indicating%0Athat%20accurate%20retrieval%20of%20pathological%20features%20presents%20greater%20difficulty.%0ADespite%20these%20challenges%2C%20our%20research%20underscores%20the%20vast%20potential%20of%0Afoundation%20models%20for%20CBIR%20in%20radiology%2C%20proposing%20a%20shift%20towards%20versatile%2C%0Ageneral-purpose%20medical%20image%20retrieval%20systems%20that%20do%20not%20require%20specific%0Atuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06567v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Foundation%20Models%20for%20Content-Based%20Medical%20Image%20Retrieval%0A%20%20in%20Radiology&entry.906535625=Stefan%20Denner%20and%20David%20Zimmerer%20and%20Dimitrios%20Bounias%20and%20Markus%20Bujotzek%20and%20Shuhan%20Xiao%20and%20Lisa%20Kausch%20and%20Philipp%20Schader%20and%20Tobias%20Penzkofer%20and%20Paul%20F.%20J%C3%A4ger%20and%20Klaus%20Maier-Hein&entry.1292438233=%20%20Content-based%20image%20retrieval%20%28CBIR%29%20has%20the%20potential%20to%20significantly%0Aimprove%20diagnostic%20aid%20and%20medical%20research%20in%20radiology.%20Current%20CBIR%20systems%0Aface%20limitations%20due%20to%20their%20specialization%20to%20certain%20pathologies%2C%20limiting%0Atheir%20utility.%20In%20response%2C%20we%20propose%20using%20vision%20foundation%20models%20as%0Apowerful%20and%20versatile%20off-the-shelf%20feature%20extractors%20for%20content-based%0Amedical%20image%20retrieval.%20By%20benchmarking%20these%20models%20on%20a%20comprehensive%0Adataset%20of%201.6%20million%202D%20radiological%20images%20spanning%20four%20modalities%20and%20161%0Apathologies%2C%20we%20identify%20weakly-supervised%20models%20as%20superior%2C%20achieving%20a%20P%401%0Aof%20up%20to%200.594.%20This%20performance%20not%20only%20competes%20with%20a%20specialized%20model%20but%0Adoes%20so%20without%20the%20need%20for%20fine-tuning.%20Our%20analysis%20further%20explores%20the%0Achallenges%20in%20retrieving%20pathological%20versus%20anatomical%20structures%2C%20indicating%0Athat%20accurate%20retrieval%20of%20pathological%20features%20presents%20greater%20difficulty.%0ADespite%20these%20challenges%2C%20our%20research%20underscores%20the%20vast%20potential%20of%0Afoundation%20models%20for%20CBIR%20in%20radiology%2C%20proposing%20a%20shift%20towards%20versatile%2C%0Ageneral-purpose%20medical%20image%20retrieval%20systems%20that%20do%20not%20require%20specific%0Atuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06567v3&entry.124074799=Read"},
{"title": "Quantifying Multilingual Performance of Large Language Models Across\n  Languages", "author": "Zihao Li and Yucheng Shi and Zirui Liu and Fan Yang and Ninghao Liu and Mengnan Du", "abstract": "  The training process of Large Language Models (LLMs) requires extensive text\ncorpus. However, these data are often unevenly distributed in different\nlanguages. As a result, LLMs perform well on common languages, such as English,\nGerman, and French, but perform poorly on low-resource languages. However,\ncurrently there is no work to quantitatively measure the performance of LLMs in\nlow-resource languages. To fill this gap, we proposed the Language Ranker that\naims to benchmark and rank different languages according to the performance of\nLLMs on those languages. We employ the LLM's performance on the English corpus\nas a baseline to compare the performances of different languages and English.\nWe have the following three findings: 1. The performance rankings of different\nLLMs in all languages are roughly the same. 2. LLMs with different sizes have\nthe same partial order of performance. 3. There is a strong correlation between\nLlaMa2's performance in different languages and the proportion of the\npre-training corpus. These findings illustrate that the Language Ranker can be\nused as an indicator to measure the language performance of LLMs.\n", "link": "http://arxiv.org/abs/2404.11553v1", "date": "2024-04-17", "relevancy": 1.312, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4425}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.441}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.423}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Quantifying%20Multilingual%20Performance%20of%20Large%20Language%20Models%20Across%0A%20%20Languages&body=Title%3A%20Quantifying%20Multilingual%20Performance%20of%20Large%20Language%20Models%20Across%0A%20%20Languages%0AAuthor%3A%20Zihao%20Li%20and%20Yucheng%20Shi%20and%20Zirui%20Liu%20and%20Fan%20Yang%20and%20Ninghao%20Liu%20and%20Mengnan%20Du%0AAbstract%3A%20%20%20The%20training%20process%20of%20Large%20Language%20Models%20%28LLMs%29%20requires%20extensive%20text%0Acorpus.%20However%2C%20these%20data%20are%20often%20unevenly%20distributed%20in%20different%0Alanguages.%20As%20a%20result%2C%20LLMs%20perform%20well%20on%20common%20languages%2C%20such%20as%20English%2C%0AGerman%2C%20and%20French%2C%20but%20perform%20poorly%20on%20low-resource%20languages.%20However%2C%0Acurrently%20there%20is%20no%20work%20to%20quantitatively%20measure%20the%20performance%20of%20LLMs%20in%0Alow-resource%20languages.%20To%20fill%20this%20gap%2C%20we%20proposed%20the%20Language%20Ranker%20that%0Aaims%20to%20benchmark%20and%20rank%20different%20languages%20according%20to%20the%20performance%20of%0ALLMs%20on%20those%20languages.%20We%20employ%20the%20LLM%27s%20performance%20on%20the%20English%20corpus%0Aas%20a%20baseline%20to%20compare%20the%20performances%20of%20different%20languages%20and%20English.%0AWe%20have%20the%20following%20three%20findings%3A%201.%20The%20performance%20rankings%20of%20different%0ALLMs%20in%20all%20languages%20are%20roughly%20the%20same.%202.%20LLMs%20with%20different%20sizes%20have%0Athe%20same%20partial%20order%20of%20performance.%203.%20There%20is%20a%20strong%20correlation%20between%0ALlaMa2%27s%20performance%20in%20different%20languages%20and%20the%20proportion%20of%20the%0Apre-training%20corpus.%20These%20findings%20illustrate%20that%20the%20Language%20Ranker%20can%20be%0Aused%20as%20an%20indicator%20to%20measure%20the%20language%20performance%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11553v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20Multilingual%20Performance%20of%20Large%20Language%20Models%20Across%0A%20%20Languages&entry.906535625=Zihao%20Li%20and%20Yucheng%20Shi%20and%20Zirui%20Liu%20and%20Fan%20Yang%20and%20Ninghao%20Liu%20and%20Mengnan%20Du&entry.1292438233=%20%20The%20training%20process%20of%20Large%20Language%20Models%20%28LLMs%29%20requires%20extensive%20text%0Acorpus.%20However%2C%20these%20data%20are%20often%20unevenly%20distributed%20in%20different%0Alanguages.%20As%20a%20result%2C%20LLMs%20perform%20well%20on%20common%20languages%2C%20such%20as%20English%2C%0AGerman%2C%20and%20French%2C%20but%20perform%20poorly%20on%20low-resource%20languages.%20However%2C%0Acurrently%20there%20is%20no%20work%20to%20quantitatively%20measure%20the%20performance%20of%20LLMs%20in%0Alow-resource%20languages.%20To%20fill%20this%20gap%2C%20we%20proposed%20the%20Language%20Ranker%20that%0Aaims%20to%20benchmark%20and%20rank%20different%20languages%20according%20to%20the%20performance%20of%0ALLMs%20on%20those%20languages.%20We%20employ%20the%20LLM%27s%20performance%20on%20the%20English%20corpus%0Aas%20a%20baseline%20to%20compare%20the%20performances%20of%20different%20languages%20and%20English.%0AWe%20have%20the%20following%20three%20findings%3A%201.%20The%20performance%20rankings%20of%20different%0ALLMs%20in%20all%20languages%20are%20roughly%20the%20same.%202.%20LLMs%20with%20different%20sizes%20have%0Athe%20same%20partial%20order%20of%20performance.%203.%20There%20is%20a%20strong%20correlation%20between%0ALlaMa2%27s%20performance%20in%20different%20languages%20and%20the%20proportion%20of%20the%0Apre-training%20corpus.%20These%20findings%20illustrate%20that%20the%20Language%20Ranker%20can%20be%0Aused%20as%20an%20indicator%20to%20measure%20the%20language%20performance%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11553v1&entry.124074799=Read"},
{"title": "Embedding Privacy in Computational Social Science and Artificial\n  Intelligence Research", "author": "Keenan Jones and Fatima Zahrah and Jason R. C. Nurse", "abstract": "  Privacy is a human right. It ensures that individuals are free to engage in\ndiscussions, participate in groups, and form relationships online or offline\nwithout fear of their data being inappropriately harvested, analyzed, or\notherwise used to harm them. Preserving privacy has emerged as a critical\nfactor in research, particularly in the computational social science (CSS),\nartificial intelligence (AI) and data science domains, given their reliance on\nindividuals' data for novel insights. The increasing use of advanced\ncomputational models stands to exacerbate privacy concerns because, if\ninappropriately used, they can quickly infringe privacy rights and lead to\nadverse effects for individuals - especially vulnerable groups - and society.\nWe have already witnessed a host of privacy issues emerge with the advent of\nlarge language models (LLMs), such as ChatGPT, which further demonstrate the\nimportance of embedding privacy from the start. This article contributes to the\nfield by discussing the role of privacy and the primary issues that researchers\nworking in CSS, AI, data science and related domains are likely to face. It\nthen presents several key considerations for researchers to ensure participant\nprivacy is best preserved in their research design, data collection and use,\nanalysis, and dissemination of research results.\n", "link": "http://arxiv.org/abs/2404.11515v1", "date": "2024-04-17", "relevancy": 1.7053, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4407}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4275}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4194}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Embedding%20Privacy%20in%20Computational%20Social%20Science%20and%20Artificial%0A%20%20Intelligence%20Research&body=Title%3A%20Embedding%20Privacy%20in%20Computational%20Social%20Science%20and%20Artificial%0A%20%20Intelligence%20Research%0AAuthor%3A%20Keenan%20Jones%20and%20Fatima%20Zahrah%20and%20Jason%20R.%20C.%20Nurse%0AAbstract%3A%20%20%20Privacy%20is%20a%20human%20right.%20It%20ensures%20that%20individuals%20are%20free%20to%20engage%20in%0Adiscussions%2C%20participate%20in%20groups%2C%20and%20form%20relationships%20online%20or%20offline%0Awithout%20fear%20of%20their%20data%20being%20inappropriately%20harvested%2C%20analyzed%2C%20or%0Aotherwise%20used%20to%20harm%20them.%20Preserving%20privacy%20has%20emerged%20as%20a%20critical%0Afactor%20in%20research%2C%20particularly%20in%20the%20computational%20social%20science%20%28CSS%29%2C%0Aartificial%20intelligence%20%28AI%29%20and%20data%20science%20domains%2C%20given%20their%20reliance%20on%0Aindividuals%27%20data%20for%20novel%20insights.%20The%20increasing%20use%20of%20advanced%0Acomputational%20models%20stands%20to%20exacerbate%20privacy%20concerns%20because%2C%20if%0Ainappropriately%20used%2C%20they%20can%20quickly%20infringe%20privacy%20rights%20and%20lead%20to%0Aadverse%20effects%20for%20individuals%20-%20especially%20vulnerable%20groups%20-%20and%20society.%0AWe%20have%20already%20witnessed%20a%20host%20of%20privacy%20issues%20emerge%20with%20the%20advent%20of%0Alarge%20language%20models%20%28LLMs%29%2C%20such%20as%20ChatGPT%2C%20which%20further%20demonstrate%20the%0Aimportance%20of%20embedding%20privacy%20from%20the%20start.%20This%20article%20contributes%20to%20the%0Afield%20by%20discussing%20the%20role%20of%20privacy%20and%20the%20primary%20issues%20that%20researchers%0Aworking%20in%20CSS%2C%20AI%2C%20data%20science%20and%20related%20domains%20are%20likely%20to%20face.%20It%0Athen%20presents%20several%20key%20considerations%20for%20researchers%20to%20ensure%20participant%0Aprivacy%20is%20best%20preserved%20in%20their%20research%20design%2C%20data%20collection%20and%20use%2C%0Aanalysis%2C%20and%20dissemination%20of%20research%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11515v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedding%20Privacy%20in%20Computational%20Social%20Science%20and%20Artificial%0A%20%20Intelligence%20Research&entry.906535625=Keenan%20Jones%20and%20Fatima%20Zahrah%20and%20Jason%20R.%20C.%20Nurse&entry.1292438233=%20%20Privacy%20is%20a%20human%20right.%20It%20ensures%20that%20individuals%20are%20free%20to%20engage%20in%0Adiscussions%2C%20participate%20in%20groups%2C%20and%20form%20relationships%20online%20or%20offline%0Awithout%20fear%20of%20their%20data%20being%20inappropriately%20harvested%2C%20analyzed%2C%20or%0Aotherwise%20used%20to%20harm%20them.%20Preserving%20privacy%20has%20emerged%20as%20a%20critical%0Afactor%20in%20research%2C%20particularly%20in%20the%20computational%20social%20science%20%28CSS%29%2C%0Aartificial%20intelligence%20%28AI%29%20and%20data%20science%20domains%2C%20given%20their%20reliance%20on%0Aindividuals%27%20data%20for%20novel%20insights.%20The%20increasing%20use%20of%20advanced%0Acomputational%20models%20stands%20to%20exacerbate%20privacy%20concerns%20because%2C%20if%0Ainappropriately%20used%2C%20they%20can%20quickly%20infringe%20privacy%20rights%20and%20lead%20to%0Aadverse%20effects%20for%20individuals%20-%20especially%20vulnerable%20groups%20-%20and%20society.%0AWe%20have%20already%20witnessed%20a%20host%20of%20privacy%20issues%20emerge%20with%20the%20advent%20of%0Alarge%20language%20models%20%28LLMs%29%2C%20such%20as%20ChatGPT%2C%20which%20further%20demonstrate%20the%0Aimportance%20of%20embedding%20privacy%20from%20the%20start.%20This%20article%20contributes%20to%20the%0Afield%20by%20discussing%20the%20role%20of%20privacy%20and%20the%20primary%20issues%20that%20researchers%0Aworking%20in%20CSS%2C%20AI%2C%20data%20science%20and%20related%20domains%20are%20likely%20to%20face.%20It%0Athen%20presents%20several%20key%20considerations%20for%20researchers%20to%20ensure%20participant%0Aprivacy%20is%20best%20preserved%20in%20their%20research%20design%2C%20data%20collection%20and%20use%2C%0Aanalysis%2C%20and%20dissemination%20of%20research%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11515v1&entry.124074799=Read"},
{"title": "Towards Human Awareness in Robot Task Planning with Large Language\n  Models", "author": "Yuchen Liu and Luigi Palmieri and Sebastian Koch and Ilche Georgievski and Marco Aiello", "abstract": "  The recent breakthroughs in the research on Large Language Models (LLMs) have\ntriggered a transformation across several research domains. Notably, the\nintegration of LLMs has greatly enhanced performance in robot Task And Motion\nPlanning (TAMP). However, previous approaches often neglect the consideration\nof dynamic environments, i.e., the presence of dynamic objects such as humans.\nIn this paper, we propose a novel approach to address this gap by incorporating\nhuman awareness into LLM-based robot task planning. To obtain an effective\nrepresentation of the dynamic environment, our approach integrates humans'\ninformation into a hierarchical scene graph. To ensure the plan's\nexecutability, we leverage LLMs to ground the environmental topology and\nactionable knowledge into formal planning language. Most importantly, we use\nLLMs to predict future human activities and plan tasks for the robot\nconsidering the predictions. Our contribution facilitates the development of\nintegrating human awareness into LLM-driven robot task planning, and paves the\nway for proactive robot decision-making in dynamic environments.\n", "link": "http://arxiv.org/abs/2404.11267v1", "date": "2024-04-17", "relevancy": 1.6956, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6219}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5715}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.54}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Human%20Awareness%20in%20Robot%20Task%20Planning%20with%20Large%20Language%0A%20%20Models&body=Title%3A%20Towards%20Human%20Awareness%20in%20Robot%20Task%20Planning%20with%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Yuchen%20Liu%20and%20Luigi%20Palmieri%20and%20Sebastian%20Koch%20and%20Ilche%20Georgievski%20and%20Marco%20Aiello%0AAbstract%3A%20%20%20The%20recent%20breakthroughs%20in%20the%20research%20on%20Large%20Language%20Models%20%28LLMs%29%20have%0Atriggered%20a%20transformation%20across%20several%20research%20domains.%20Notably%2C%20the%0Aintegration%20of%20LLMs%20has%20greatly%20enhanced%20performance%20in%20robot%20Task%20And%20Motion%0APlanning%20%28TAMP%29.%20However%2C%20previous%20approaches%20often%20neglect%20the%20consideration%0Aof%20dynamic%20environments%2C%20i.e.%2C%20the%20presence%20of%20dynamic%20objects%20such%20as%20humans.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%20address%20this%20gap%20by%20incorporating%0Ahuman%20awareness%20into%20LLM-based%20robot%20task%20planning.%20To%20obtain%20an%20effective%0Arepresentation%20of%20the%20dynamic%20environment%2C%20our%20approach%20integrates%20humans%27%0Ainformation%20into%20a%20hierarchical%20scene%20graph.%20To%20ensure%20the%20plan%27s%0Aexecutability%2C%20we%20leverage%20LLMs%20to%20ground%20the%20environmental%20topology%20and%0Aactionable%20knowledge%20into%20formal%20planning%20language.%20Most%20importantly%2C%20we%20use%0ALLMs%20to%20predict%20future%20human%20activities%20and%20plan%20tasks%20for%20the%20robot%0Aconsidering%20the%20predictions.%20Our%20contribution%20facilitates%20the%20development%20of%0Aintegrating%20human%20awareness%20into%20LLM-driven%20robot%20task%20planning%2C%20and%20paves%20the%0Away%20for%20proactive%20robot%20decision-making%20in%20dynamic%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11267v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Human%20Awareness%20in%20Robot%20Task%20Planning%20with%20Large%20Language%0A%20%20Models&entry.906535625=Yuchen%20Liu%20and%20Luigi%20Palmieri%20and%20Sebastian%20Koch%20and%20Ilche%20Georgievski%20and%20Marco%20Aiello&entry.1292438233=%20%20The%20recent%20breakthroughs%20in%20the%20research%20on%20Large%20Language%20Models%20%28LLMs%29%20have%0Atriggered%20a%20transformation%20across%20several%20research%20domains.%20Notably%2C%20the%0Aintegration%20of%20LLMs%20has%20greatly%20enhanced%20performance%20in%20robot%20Task%20And%20Motion%0APlanning%20%28TAMP%29.%20However%2C%20previous%20approaches%20often%20neglect%20the%20consideration%0Aof%20dynamic%20environments%2C%20i.e.%2C%20the%20presence%20of%20dynamic%20objects%20such%20as%20humans.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%20address%20this%20gap%20by%20incorporating%0Ahuman%20awareness%20into%20LLM-based%20robot%20task%20planning.%20To%20obtain%20an%20effective%0Arepresentation%20of%20the%20dynamic%20environment%2C%20our%20approach%20integrates%20humans%27%0Ainformation%20into%20a%20hierarchical%20scene%20graph.%20To%20ensure%20the%20plan%27s%0Aexecutability%2C%20we%20leverage%20LLMs%20to%20ground%20the%20environmental%20topology%20and%0Aactionable%20knowledge%20into%20formal%20planning%20language.%20Most%20importantly%2C%20we%20use%0ALLMs%20to%20predict%20future%20human%20activities%20and%20plan%20tasks%20for%20the%20robot%0Aconsidering%20the%20predictions.%20Our%20contribution%20facilitates%20the%20development%20of%0Aintegrating%20human%20awareness%20into%20LLM-driven%20robot%20task%20planning%2C%20and%20paves%20the%0Away%20for%20proactive%20robot%20decision-making%20in%20dynamic%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11267v1&entry.124074799=Read"},
{"title": "What are human values, and how do we align AI to them?", "author": "Oliver Klingefjord and Ryan Lowe and Joe Edelman", "abstract": "  There is an emerging consensus that we need to align AI systems with human\nvalues (Gabriel, 2020; Ji et al., 2024), but it remains unclear how to apply\nthis to language models in practice. We split the problem of \"aligning to human\nvalues\" into three parts: first, eliciting values from people; second,\nreconciling those values into an alignment target for training ML models; and\nthird, actually training the model. In this paper, we focus on the first two\nparts, and ask the question: what are \"good\" ways to synthesize diverse human\ninputs about values into a target for aligning language models? To answer this\nquestion, we first define a set of 6 criteria that we believe must be satisfied\nfor an alignment target to shape model behavior in accordance with human\nvalues. We then propose a process for eliciting and reconciling values called\nMoral Graph Elicitation (MGE), which uses a large language model to interview\nparticipants about their values in particular contexts; our approach is\ninspired by the philosophy of values advanced by Taylor (1977), Chang (2004),\nand others. We trial MGE with a representative sample of 500 Americans, on 3\nintentionally divisive prompts (e.g. advice about abortion). Our results\ndemonstrate that MGE is promising for improving model alignment across all 6\ncriteria. For example, almost all participants (89.1%) felt well represented by\nthe process, and (89%) thought the final moral graph was fair, even if their\nvalue wasn't voted as the wisest. Our process often results in \"expert\" values\n(e.g. values from women who have solicited abortion advice) rising to the top\nof the moral graph, without defining who is considered an expert in advance.\n", "link": "http://arxiv.org/abs/2404.10636v2", "date": "2024-04-17", "relevancy": 1.1678, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3982}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3946}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3836}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20What%20are%20human%20values%2C%20and%20how%20do%20we%20align%20AI%20to%20them%3F&body=Title%3A%20What%20are%20human%20values%2C%20and%20how%20do%20we%20align%20AI%20to%20them%3F%0AAuthor%3A%20Oliver%20Klingefjord%20and%20Ryan%20Lowe%20and%20Joe%20Edelman%0AAbstract%3A%20%20%20There%20is%20an%20emerging%20consensus%20that%20we%20need%20to%20align%20AI%20systems%20with%20human%0Avalues%20%28Gabriel%2C%202020%3B%20Ji%20et%20al.%2C%202024%29%2C%20but%20it%20remains%20unclear%20how%20to%20apply%0Athis%20to%20language%20models%20in%20practice.%20We%20split%20the%20problem%20of%20%22aligning%20to%20human%0Avalues%22%20into%20three%20parts%3A%20first%2C%20eliciting%20values%20from%20people%3B%20second%2C%0Areconciling%20those%20values%20into%20an%20alignment%20target%20for%20training%20ML%20models%3B%20and%0Athird%2C%20actually%20training%20the%20model.%20In%20this%20paper%2C%20we%20focus%20on%20the%20first%20two%0Aparts%2C%20and%20ask%20the%20question%3A%20what%20are%20%22good%22%20ways%20to%20synthesize%20diverse%20human%0Ainputs%20about%20values%20into%20a%20target%20for%20aligning%20language%20models%3F%20To%20answer%20this%0Aquestion%2C%20we%20first%20define%20a%20set%20of%206%20criteria%20that%20we%20believe%20must%20be%20satisfied%0Afor%20an%20alignment%20target%20to%20shape%20model%20behavior%20in%20accordance%20with%20human%0Avalues.%20We%20then%20propose%20a%20process%20for%20eliciting%20and%20reconciling%20values%20called%0AMoral%20Graph%20Elicitation%20%28MGE%29%2C%20which%20uses%20a%20large%20language%20model%20to%20interview%0Aparticipants%20about%20their%20values%20in%20particular%20contexts%3B%20our%20approach%20is%0Ainspired%20by%20the%20philosophy%20of%20values%20advanced%20by%20Taylor%20%281977%29%2C%20Chang%20%282004%29%2C%0Aand%20others.%20We%20trial%20MGE%20with%20a%20representative%20sample%20of%20500%20Americans%2C%20on%203%0Aintentionally%20divisive%20prompts%20%28e.g.%20advice%20about%20abortion%29.%20Our%20results%0Ademonstrate%20that%20MGE%20is%20promising%20for%20improving%20model%20alignment%20across%20all%206%0Acriteria.%20For%20example%2C%20almost%20all%20participants%20%2889.1%25%29%20felt%20well%20represented%20by%0Athe%20process%2C%20and%20%2889%25%29%20thought%20the%20final%20moral%20graph%20was%20fair%2C%20even%20if%20their%0Avalue%20wasn%27t%20voted%20as%20the%20wisest.%20Our%20process%20often%20results%20in%20%22expert%22%20values%0A%28e.g.%20values%20from%20women%20who%20have%20solicited%20abortion%20advice%29%20rising%20to%20the%20top%0Aof%20the%20moral%20graph%2C%20without%20defining%20who%20is%20considered%20an%20expert%20in%20advance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10636v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20are%20human%20values%2C%20and%20how%20do%20we%20align%20AI%20to%20them%3F&entry.906535625=Oliver%20Klingefjord%20and%20Ryan%20Lowe%20and%20Joe%20Edelman&entry.1292438233=%20%20There%20is%20an%20emerging%20consensus%20that%20we%20need%20to%20align%20AI%20systems%20with%20human%0Avalues%20%28Gabriel%2C%202020%3B%20Ji%20et%20al.%2C%202024%29%2C%20but%20it%20remains%20unclear%20how%20to%20apply%0Athis%20to%20language%20models%20in%20practice.%20We%20split%20the%20problem%20of%20%22aligning%20to%20human%0Avalues%22%20into%20three%20parts%3A%20first%2C%20eliciting%20values%20from%20people%3B%20second%2C%0Areconciling%20those%20values%20into%20an%20alignment%20target%20for%20training%20ML%20models%3B%20and%0Athird%2C%20actually%20training%20the%20model.%20In%20this%20paper%2C%20we%20focus%20on%20the%20first%20two%0Aparts%2C%20and%20ask%20the%20question%3A%20what%20are%20%22good%22%20ways%20to%20synthesize%20diverse%20human%0Ainputs%20about%20values%20into%20a%20target%20for%20aligning%20language%20models%3F%20To%20answer%20this%0Aquestion%2C%20we%20first%20define%20a%20set%20of%206%20criteria%20that%20we%20believe%20must%20be%20satisfied%0Afor%20an%20alignment%20target%20to%20shape%20model%20behavior%20in%20accordance%20with%20human%0Avalues.%20We%20then%20propose%20a%20process%20for%20eliciting%20and%20reconciling%20values%20called%0AMoral%20Graph%20Elicitation%20%28MGE%29%2C%20which%20uses%20a%20large%20language%20model%20to%20interview%0Aparticipants%20about%20their%20values%20in%20particular%20contexts%3B%20our%20approach%20is%0Ainspired%20by%20the%20philosophy%20of%20values%20advanced%20by%20Taylor%20%281977%29%2C%20Chang%20%282004%29%2C%0Aand%20others.%20We%20trial%20MGE%20with%20a%20representative%20sample%20of%20500%20Americans%2C%20on%203%0Aintentionally%20divisive%20prompts%20%28e.g.%20advice%20about%20abortion%29.%20Our%20results%0Ademonstrate%20that%20MGE%20is%20promising%20for%20improving%20model%20alignment%20across%20all%206%0Acriteria.%20For%20example%2C%20almost%20all%20participants%20%2889.1%25%29%20felt%20well%20represented%20by%0Athe%20process%2C%20and%20%2889%25%29%20thought%20the%20final%20moral%20graph%20was%20fair%2C%20even%20if%20their%0Avalue%20wasn%27t%20voted%20as%20the%20wisest.%20Our%20process%20often%20results%20in%20%22expert%22%20values%0A%28e.g.%20values%20from%20women%20who%20have%20solicited%20abortion%20advice%29%20rising%20to%20the%20top%0Aof%20the%20moral%20graph%2C%20without%20defining%20who%20is%20considered%20an%20expert%20in%20advance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10636v2&entry.124074799=Read"},
{"title": "Influencer Backdoor Attack on Semantic Segmentation", "author": "Haoheng Lan and Jindong Gu and Philip Torr and Hengshuang Zhao", "abstract": "  When a small number of poisoned samples are injected into the training\ndataset of a deep neural network, the network can be induced to exhibit\nmalicious behavior during inferences, which poses potential threats to\nreal-world applications. While they have been intensively studied in\nclassification, backdoor attacks on semantic segmentation have been largely\noverlooked. Unlike classification, semantic segmentation aims to classify every\npixel within a given image. In this work, we explore backdoor attacks on\nsegmentation models to misclassify all pixels of a victim class by injecting a\nspecific trigger on non-victim pixels during inferences, which is dubbed\nInfluencer Backdoor Attack (IBA). IBA is expected to maintain the\nclassification accuracy of non-victim pixels and mislead classifications of all\nvictim pixels in every single inference and could be easily applied to\nreal-world scenes. Based on the context aggregation ability of segmentation\nmodels, we proposed a simple, yet effective, Nearest-Neighbor trigger injection\nstrategy. We also introduce an innovative Pixel Random Labeling strategy which\nmaintains optimal performance even when the trigger is placed far from the\nvictim pixels. Our extensive experiments reveal that current segmentation\nmodels do suffer from backdoor attacks, demonstrate IBA real-world\napplicability, and show that our proposed techniques can further increase\nattack performance.\n", "link": "http://arxiv.org/abs/2303.12054v5", "date": "2024-04-17", "relevancy": 1.4896, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5082}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4825}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4814}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Influencer%20Backdoor%20Attack%20on%20Semantic%20Segmentation&body=Title%3A%20Influencer%20Backdoor%20Attack%20on%20Semantic%20Segmentation%0AAuthor%3A%20Haoheng%20Lan%20and%20Jindong%20Gu%20and%20Philip%20Torr%20and%20Hengshuang%20Zhao%0AAbstract%3A%20%20%20When%20a%20small%20number%20of%20poisoned%20samples%20are%20injected%20into%20the%20training%0Adataset%20of%20a%20deep%20neural%20network%2C%20the%20network%20can%20be%20induced%20to%20exhibit%0Amalicious%20behavior%20during%20inferences%2C%20which%20poses%20potential%20threats%20to%0Areal-world%20applications.%20While%20they%20have%20been%20intensively%20studied%20in%0Aclassification%2C%20backdoor%20attacks%20on%20semantic%20segmentation%20have%20been%20largely%0Aoverlooked.%20Unlike%20classification%2C%20semantic%20segmentation%20aims%20to%20classify%20every%0Apixel%20within%20a%20given%20image.%20In%20this%20work%2C%20we%20explore%20backdoor%20attacks%20on%0Asegmentation%20models%20to%20misclassify%20all%20pixels%20of%20a%20victim%20class%20by%20injecting%20a%0Aspecific%20trigger%20on%20non-victim%20pixels%20during%20inferences%2C%20which%20is%20dubbed%0AInfluencer%20Backdoor%20Attack%20%28IBA%29.%20IBA%20is%20expected%20to%20maintain%20the%0Aclassification%20accuracy%20of%20non-victim%20pixels%20and%20mislead%20classifications%20of%20all%0Avictim%20pixels%20in%20every%20single%20inference%20and%20could%20be%20easily%20applied%20to%0Areal-world%20scenes.%20Based%20on%20the%20context%20aggregation%20ability%20of%20segmentation%0Amodels%2C%20we%20proposed%20a%20simple%2C%20yet%20effective%2C%20Nearest-Neighbor%20trigger%20injection%0Astrategy.%20We%20also%20introduce%20an%20innovative%20Pixel%20Random%20Labeling%20strategy%20which%0Amaintains%20optimal%20performance%20even%20when%20the%20trigger%20is%20placed%20far%20from%20the%0Avictim%20pixels.%20Our%20extensive%20experiments%20reveal%20that%20current%20segmentation%0Amodels%20do%20suffer%20from%20backdoor%20attacks%2C%20demonstrate%20IBA%20real-world%0Aapplicability%2C%20and%20show%20that%20our%20proposed%20techniques%20can%20further%20increase%0Aattack%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.12054v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Influencer%20Backdoor%20Attack%20on%20Semantic%20Segmentation&entry.906535625=Haoheng%20Lan%20and%20Jindong%20Gu%20and%20Philip%20Torr%20and%20Hengshuang%20Zhao&entry.1292438233=%20%20When%20a%20small%20number%20of%20poisoned%20samples%20are%20injected%20into%20the%20training%0Adataset%20of%20a%20deep%20neural%20network%2C%20the%20network%20can%20be%20induced%20to%20exhibit%0Amalicious%20behavior%20during%20inferences%2C%20which%20poses%20potential%20threats%20to%0Areal-world%20applications.%20While%20they%20have%20been%20intensively%20studied%20in%0Aclassification%2C%20backdoor%20attacks%20on%20semantic%20segmentation%20have%20been%20largely%0Aoverlooked.%20Unlike%20classification%2C%20semantic%20segmentation%20aims%20to%20classify%20every%0Apixel%20within%20a%20given%20image.%20In%20this%20work%2C%20we%20explore%20backdoor%20attacks%20on%0Asegmentation%20models%20to%20misclassify%20all%20pixels%20of%20a%20victim%20class%20by%20injecting%20a%0Aspecific%20trigger%20on%20non-victim%20pixels%20during%20inferences%2C%20which%20is%20dubbed%0AInfluencer%20Backdoor%20Attack%20%28IBA%29.%20IBA%20is%20expected%20to%20maintain%20the%0Aclassification%20accuracy%20of%20non-victim%20pixels%20and%20mislead%20classifications%20of%20all%0Avictim%20pixels%20in%20every%20single%20inference%20and%20could%20be%20easily%20applied%20to%0Areal-world%20scenes.%20Based%20on%20the%20context%20aggregation%20ability%20of%20segmentation%0Amodels%2C%20we%20proposed%20a%20simple%2C%20yet%20effective%2C%20Nearest-Neighbor%20trigger%20injection%0Astrategy.%20We%20also%20introduce%20an%20innovative%20Pixel%20Random%20Labeling%20strategy%20which%0Amaintains%20optimal%20performance%20even%20when%20the%20trigger%20is%20placed%20far%20from%20the%0Avictim%20pixels.%20Our%20extensive%20experiments%20reveal%20that%20current%20segmentation%0Amodels%20do%20suffer%20from%20backdoor%20attacks%2C%20demonstrate%20IBA%20real-world%0Aapplicability%2C%20and%20show%20that%20our%20proposed%20techniques%20can%20further%20increase%0Aattack%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.12054v5&entry.124074799=Read"},
{"title": "DeblurGS: Gaussian Splatting for Camera Motion Blur", "author": "Jeongtaek Oh and Jaeyoung Chung and Dongwoo Lee and Kyoung Mu Lee", "abstract": "  Although significant progress has been made in reconstructing sharp 3D scenes\nfrom motion-blurred images, a transition to real-world applications remains\nchallenging. The primary obstacle stems from the severe blur which leads to\ninaccuracies in the acquisition of initial camera poses through\nStructure-from-Motion, a critical aspect often overlooked by previous\napproaches. To address this challenge, we propose DeblurGS, a method to\noptimize sharp 3D Gaussian Splatting from motion-blurred images, even with the\nnoisy camera pose initialization. We restore a fine-grained sharp scene by\nleveraging the remarkable reconstruction capability of 3D Gaussian Splatting.\nOur approach estimates the 6-Degree-of-Freedom camera motion for each blurry\nobservation and synthesizes corresponding blurry renderings for the\noptimization process. Furthermore, we propose Gaussian Densification Annealing\nstrategy to prevent the generation of inaccurate Gaussians at erroneous\nlocations during the early training stages when camera motion is still\nimprecise. Comprehensive experiments demonstrate that our DeblurGS achieves\nstate-of-the-art performance in deblurring and novel view synthesis for\nreal-world and synthetic benchmark datasets, as well as field-captured blurry\nsmartphone videos.\n", "link": "http://arxiv.org/abs/2404.11358v1", "date": "2024-04-17", "relevancy": 1.5742, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5364}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5231}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4972}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DeblurGS%3A%20Gaussian%20Splatting%20for%20Camera%20Motion%20Blur&body=Title%3A%20DeblurGS%3A%20Gaussian%20Splatting%20for%20Camera%20Motion%20Blur%0AAuthor%3A%20Jeongtaek%20Oh%20and%20Jaeyoung%20Chung%20and%20Dongwoo%20Lee%20and%20Kyoung%20Mu%20Lee%0AAbstract%3A%20%20%20Although%20significant%20progress%20has%20been%20made%20in%20reconstructing%20sharp%203D%20scenes%0Afrom%20motion-blurred%20images%2C%20a%20transition%20to%20real-world%20applications%20remains%0Achallenging.%20The%20primary%20obstacle%20stems%20from%20the%20severe%20blur%20which%20leads%20to%0Ainaccuracies%20in%20the%20acquisition%20of%20initial%20camera%20poses%20through%0AStructure-from-Motion%2C%20a%20critical%20aspect%20often%20overlooked%20by%20previous%0Aapproaches.%20To%20address%20this%20challenge%2C%20we%20propose%20DeblurGS%2C%20a%20method%20to%0Aoptimize%20sharp%203D%20Gaussian%20Splatting%20from%20motion-blurred%20images%2C%20even%20with%20the%0Anoisy%20camera%20pose%20initialization.%20We%20restore%20a%20fine-grained%20sharp%20scene%20by%0Aleveraging%20the%20remarkable%20reconstruction%20capability%20of%203D%20Gaussian%20Splatting.%0AOur%20approach%20estimates%20the%206-Degree-of-Freedom%20camera%20motion%20for%20each%20blurry%0Aobservation%20and%20synthesizes%20corresponding%20blurry%20renderings%20for%20the%0Aoptimization%20process.%20Furthermore%2C%20we%20propose%20Gaussian%20Densification%20Annealing%0Astrategy%20to%20prevent%20the%20generation%20of%20inaccurate%20Gaussians%20at%20erroneous%0Alocations%20during%20the%20early%20training%20stages%20when%20camera%20motion%20is%20still%0Aimprecise.%20Comprehensive%20experiments%20demonstrate%20that%20our%20DeblurGS%20achieves%0Astate-of-the-art%20performance%20in%20deblurring%20and%20novel%20view%20synthesis%20for%0Areal-world%20and%20synthetic%20benchmark%20datasets%2C%20as%20well%20as%20field-captured%20blurry%0Asmartphone%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11358v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeblurGS%3A%20Gaussian%20Splatting%20for%20Camera%20Motion%20Blur&entry.906535625=Jeongtaek%20Oh%20and%20Jaeyoung%20Chung%20and%20Dongwoo%20Lee%20and%20Kyoung%20Mu%20Lee&entry.1292438233=%20%20Although%20significant%20progress%20has%20been%20made%20in%20reconstructing%20sharp%203D%20scenes%0Afrom%20motion-blurred%20images%2C%20a%20transition%20to%20real-world%20applications%20remains%0Achallenging.%20The%20primary%20obstacle%20stems%20from%20the%20severe%20blur%20which%20leads%20to%0Ainaccuracies%20in%20the%20acquisition%20of%20initial%20camera%20poses%20through%0AStructure-from-Motion%2C%20a%20critical%20aspect%20often%20overlooked%20by%20previous%0Aapproaches.%20To%20address%20this%20challenge%2C%20we%20propose%20DeblurGS%2C%20a%20method%20to%0Aoptimize%20sharp%203D%20Gaussian%20Splatting%20from%20motion-blurred%20images%2C%20even%20with%20the%0Anoisy%20camera%20pose%20initialization.%20We%20restore%20a%20fine-grained%20sharp%20scene%20by%0Aleveraging%20the%20remarkable%20reconstruction%20capability%20of%203D%20Gaussian%20Splatting.%0AOur%20approach%20estimates%20the%206-Degree-of-Freedom%20camera%20motion%20for%20each%20blurry%0Aobservation%20and%20synthesizes%20corresponding%20blurry%20renderings%20for%20the%0Aoptimization%20process.%20Furthermore%2C%20we%20propose%20Gaussian%20Densification%20Annealing%0Astrategy%20to%20prevent%20the%20generation%20of%20inaccurate%20Gaussians%20at%20erroneous%0Alocations%20during%20the%20early%20training%20stages%20when%20camera%20motion%20is%20still%0Aimprecise.%20Comprehensive%20experiments%20demonstrate%20that%20our%20DeblurGS%20achieves%0Astate-of-the-art%20performance%20in%20deblurring%20and%20novel%20view%20synthesis%20for%0Areal-world%20and%20synthetic%20benchmark%20datasets%2C%20as%20well%20as%20field-captured%20blurry%0Asmartphone%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11358v1&entry.124074799=Read"},
{"title": "AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation\n  Quality in Online Discussions Using LLMs", "author": "Maike Behrendt and Stefan Sylvius Wagner and Marc Ziegele and Lena Wilms and Anke Stoll and Dominique Heinbach and Stefan Harmeling", "abstract": "  Measuring the quality of contributions in political online discussions is\ncrucial in deliberation research and computer science. Research has identified\nvarious indicators to assess online discussion quality, and with deep learning\nadvancements, automating these measures has become feasible. While some studies\nfocus on analyzing specific quality indicators, a comprehensive quality score\nincorporating various deliberative aspects is often preferred. In this work, we\nintroduce AQuA, an additive score that calculates a unified deliberative\nquality score from multiple indices for each discussion post. Unlike other\nsingular scores, AQuA preserves information on the deliberative aspects present\nin comments, enhancing model transparency. We develop adapter models for 20\ndeliberative indices, and calculate correlation coefficients between experts'\nannotations and the perceived deliberativeness by non-experts to weigh the\nindividual indices into a single deliberative score. We demonstrate that the\nAQuA score can be computed easily from pre-trained adapters and aligns well\nwith annotations on other datasets that have not be seen during training. The\nanalysis of experts' vs. non-experts' annotations confirms theoretical findings\nin the social science literature.\n", "link": "http://arxiv.org/abs/2404.02761v3", "date": "2024-04-17", "relevancy": 1.7998, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.489}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.444}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4402}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AQuA%20--%20Combining%20Experts%27%20and%20Non-Experts%27%20Views%20To%20Assess%20Deliberation%0A%20%20Quality%20in%20Online%20Discussions%20Using%20LLMs&body=Title%3A%20AQuA%20--%20Combining%20Experts%27%20and%20Non-Experts%27%20Views%20To%20Assess%20Deliberation%0A%20%20Quality%20in%20Online%20Discussions%20Using%20LLMs%0AAuthor%3A%20Maike%20Behrendt%20and%20Stefan%20Sylvius%20Wagner%20and%20Marc%20Ziegele%20and%20Lena%20Wilms%20and%20Anke%20Stoll%20and%20Dominique%20Heinbach%20and%20Stefan%20Harmeling%0AAbstract%3A%20%20%20Measuring%20the%20quality%20of%20contributions%20in%20political%20online%20discussions%20is%0Acrucial%20in%20deliberation%20research%20and%20computer%20science.%20Research%20has%20identified%0Avarious%20indicators%20to%20assess%20online%20discussion%20quality%2C%20and%20with%20deep%20learning%0Aadvancements%2C%20automating%20these%20measures%20has%20become%20feasible.%20While%20some%20studies%0Afocus%20on%20analyzing%20specific%20quality%20indicators%2C%20a%20comprehensive%20quality%20score%0Aincorporating%20various%20deliberative%20aspects%20is%20often%20preferred.%20In%20this%20work%2C%20we%0Aintroduce%20AQuA%2C%20an%20additive%20score%20that%20calculates%20a%20unified%20deliberative%0Aquality%20score%20from%20multiple%20indices%20for%20each%20discussion%20post.%20Unlike%20other%0Asingular%20scores%2C%20AQuA%20preserves%20information%20on%20the%20deliberative%20aspects%20present%0Ain%20comments%2C%20enhancing%20model%20transparency.%20We%20develop%20adapter%20models%20for%2020%0Adeliberative%20indices%2C%20and%20calculate%20correlation%20coefficients%20between%20experts%27%0Aannotations%20and%20the%20perceived%20deliberativeness%20by%20non-experts%20to%20weigh%20the%0Aindividual%20indices%20into%20a%20single%20deliberative%20score.%20We%20demonstrate%20that%20the%0AAQuA%20score%20can%20be%20computed%20easily%20from%20pre-trained%20adapters%20and%20aligns%20well%0Awith%20annotations%20on%20other%20datasets%20that%20have%20not%20be%20seen%20during%20training.%20The%0Aanalysis%20of%20experts%27%20vs.%20non-experts%27%20annotations%20confirms%20theoretical%20findings%0Ain%20the%20social%20science%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02761v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AQuA%20--%20Combining%20Experts%27%20and%20Non-Experts%27%20Views%20To%20Assess%20Deliberation%0A%20%20Quality%20in%20Online%20Discussions%20Using%20LLMs&entry.906535625=Maike%20Behrendt%20and%20Stefan%20Sylvius%20Wagner%20and%20Marc%20Ziegele%20and%20Lena%20Wilms%20and%20Anke%20Stoll%20and%20Dominique%20Heinbach%20and%20Stefan%20Harmeling&entry.1292438233=%20%20Measuring%20the%20quality%20of%20contributions%20in%20political%20online%20discussions%20is%0Acrucial%20in%20deliberation%20research%20and%20computer%20science.%20Research%20has%20identified%0Avarious%20indicators%20to%20assess%20online%20discussion%20quality%2C%20and%20with%20deep%20learning%0Aadvancements%2C%20automating%20these%20measures%20has%20become%20feasible.%20While%20some%20studies%0Afocus%20on%20analyzing%20specific%20quality%20indicators%2C%20a%20comprehensive%20quality%20score%0Aincorporating%20various%20deliberative%20aspects%20is%20often%20preferred.%20In%20this%20work%2C%20we%0Aintroduce%20AQuA%2C%20an%20additive%20score%20that%20calculates%20a%20unified%20deliberative%0Aquality%20score%20from%20multiple%20indices%20for%20each%20discussion%20post.%20Unlike%20other%0Asingular%20scores%2C%20AQuA%20preserves%20information%20on%20the%20deliberative%20aspects%20present%0Ain%20comments%2C%20enhancing%20model%20transparency.%20We%20develop%20adapter%20models%20for%2020%0Adeliberative%20indices%2C%20and%20calculate%20correlation%20coefficients%20between%20experts%27%0Aannotations%20and%20the%20perceived%20deliberativeness%20by%20non-experts%20to%20weigh%20the%0Aindividual%20indices%20into%20a%20single%20deliberative%20score.%20We%20demonstrate%20that%20the%0AAQuA%20score%20can%20be%20computed%20easily%20from%20pre-trained%20adapters%20and%20aligns%20well%0Awith%20annotations%20on%20other%20datasets%20that%20have%20not%20be%20seen%20during%20training.%20The%0Aanalysis%20of%20experts%27%20vs.%20non-experts%27%20annotations%20confirms%20theoretical%20findings%0Ain%20the%20social%20science%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02761v3&entry.124074799=Read"},
{"title": "The Landscape of Emerging AI Agent Architectures for Reasoning,\n  Planning, and Tool Calling: A Survey", "author": "Tula Masterman and Sandi Besen and Mason Sawtell and Alex Chao", "abstract": "  This survey paper examines the recent advancements in AI agent\nimplementations, with a focus on their ability to achieve complex goals that\nrequire enhanced reasoning, planning, and tool execution capabilities. The\nprimary objectives of this work are to a) communicate the current capabilities\nand limitations of existing AI agent implementations, b) share insights gained\nfrom our observations of these systems in action, and c) suggest important\nconsiderations for future developments in AI agent design. We achieve this by\nproviding overviews of single-agent and multi-agent architectures, identifying\nkey patterns and divergences in design choices, and evaluating their overall\nimpact on accomplishing a provided goal. Our contribution outlines key themes\nwhen selecting an agentic architecture, the impact of leadership on agent\nsystems, agent communication styles, and key phases for planning, execution,\nand reflection that enable robust AI agent systems.\n", "link": "http://arxiv.org/abs/2404.11584v1", "date": "2024-04-17", "relevancy": 1.4901, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5404}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.488}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4827}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Landscape%20of%20Emerging%20AI%20Agent%20Architectures%20for%20Reasoning%2C%0A%20%20Planning%2C%20and%20Tool%20Calling%3A%20A%20Survey&body=Title%3A%20The%20Landscape%20of%20Emerging%20AI%20Agent%20Architectures%20for%20Reasoning%2C%0A%20%20Planning%2C%20and%20Tool%20Calling%3A%20A%20Survey%0AAuthor%3A%20Tula%20Masterman%20and%20Sandi%20Besen%20and%20Mason%20Sawtell%20and%20Alex%20Chao%0AAbstract%3A%20%20%20This%20survey%20paper%20examines%20the%20recent%20advancements%20in%20AI%20agent%0Aimplementations%2C%20with%20a%20focus%20on%20their%20ability%20to%20achieve%20complex%20goals%20that%0Arequire%20enhanced%20reasoning%2C%20planning%2C%20and%20tool%20execution%20capabilities.%20The%0Aprimary%20objectives%20of%20this%20work%20are%20to%20a%29%20communicate%20the%20current%20capabilities%0Aand%20limitations%20of%20existing%20AI%20agent%20implementations%2C%20b%29%20share%20insights%20gained%0Afrom%20our%20observations%20of%20these%20systems%20in%20action%2C%20and%20c%29%20suggest%20important%0Aconsiderations%20for%20future%20developments%20in%20AI%20agent%20design.%20We%20achieve%20this%20by%0Aproviding%20overviews%20of%20single-agent%20and%20multi-agent%20architectures%2C%20identifying%0Akey%20patterns%20and%20divergences%20in%20design%20choices%2C%20and%20evaluating%20their%20overall%0Aimpact%20on%20accomplishing%20a%20provided%20goal.%20Our%20contribution%20outlines%20key%20themes%0Awhen%20selecting%20an%20agentic%20architecture%2C%20the%20impact%20of%20leadership%20on%20agent%0Asystems%2C%20agent%20communication%20styles%2C%20and%20key%20phases%20for%20planning%2C%20execution%2C%0Aand%20reflection%20that%20enable%20robust%20AI%20agent%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11584v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Landscape%20of%20Emerging%20AI%20Agent%20Architectures%20for%20Reasoning%2C%0A%20%20Planning%2C%20and%20Tool%20Calling%3A%20A%20Survey&entry.906535625=Tula%20Masterman%20and%20Sandi%20Besen%20and%20Mason%20Sawtell%20and%20Alex%20Chao&entry.1292438233=%20%20This%20survey%20paper%20examines%20the%20recent%20advancements%20in%20AI%20agent%0Aimplementations%2C%20with%20a%20focus%20on%20their%20ability%20to%20achieve%20complex%20goals%20that%0Arequire%20enhanced%20reasoning%2C%20planning%2C%20and%20tool%20execution%20capabilities.%20The%0Aprimary%20objectives%20of%20this%20work%20are%20to%20a%29%20communicate%20the%20current%20capabilities%0Aand%20limitations%20of%20existing%20AI%20agent%20implementations%2C%20b%29%20share%20insights%20gained%0Afrom%20our%20observations%20of%20these%20systems%20in%20action%2C%20and%20c%29%20suggest%20important%0Aconsiderations%20for%20future%20developments%20in%20AI%20agent%20design.%20We%20achieve%20this%20by%0Aproviding%20overviews%20of%20single-agent%20and%20multi-agent%20architectures%2C%20identifying%0Akey%20patterns%20and%20divergences%20in%20design%20choices%2C%20and%20evaluating%20their%20overall%0Aimpact%20on%20accomplishing%20a%20provided%20goal.%20Our%20contribution%20outlines%20key%20themes%0Awhen%20selecting%20an%20agentic%20architecture%2C%20the%20impact%20of%20leadership%20on%20agent%0Asystems%2C%20agent%20communication%20styles%2C%20and%20key%20phases%20for%20planning%2C%20execution%2C%0Aand%20reflection%20that%20enable%20robust%20AI%20agent%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11584v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


