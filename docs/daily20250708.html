<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250706.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars", "author": "Yiming Zhong and Xiaolin Zhang and Ligang Liu and Yao Zhao and Yunchao Wei", "abstract": "  Similar to facial beautification in real life, 3D virtual avatars require\npersonalized customization to enhance their visual appeal, yet this area\nremains insufficiently explored. Although current 3D Gaussian editing methods\ncan be adapted for facial makeup purposes, these methods fail to meet the\nfundamental requirements for achieving realistic makeup effects: 1) ensuring a\nconsistent appearance during drivable expressions, 2) preserving the identity\nthroughout the makeup process, and 3) enabling precise control over fine\ndetails. To address these, we propose a specialized 3D makeup method named\nAvatarMakeup, leveraging a pretrained diffusion model to transfer makeup\npatterns from a single reference photo of any individual. We adopt a\ncoarse-to-fine idea to first maintain the consistent appearance and identity,\nand then to refine the details. In particular, the diffusion model is employed\nto generate makeup images as supervision. Due to the uncertainties in diffusion\nprocess, the generated images are inconsistent across different viewpoints and\nexpressions. Therefore, we propose a Coherent Duplication method to coarsely\napply makeup to the target while ensuring consistency across dynamic and\nmultiview effects. Coherent Duplication optimizes a global UV map by recoding\nthe averaged facial attributes among the generated makeup images. By querying\nthe global UV map, it easily synthesizes coherent makeup guidance from\narbitrary views and expressions to optimize the target avatar. Given the coarse\nmakeup avatar, we further enhance the makeup by incorporating a Refinement\nModule into the diffusion model to achieve high makeup quality. Experiments\ndemonstrate that AvatarMakeup achieves state-of-the-art makeup transfer quality\nand consistency throughout animation.\n", "link": "http://arxiv.org/abs/2507.02419v2", "date": "2025-07-07", "relevancy": 3.4122, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7015}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7015}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AvatarMakeup%3A%20Realistic%20Makeup%20Transfer%20for%203D%20Animatable%20Head%20Avatars&body=Title%3A%20AvatarMakeup%3A%20Realistic%20Makeup%20Transfer%20for%203D%20Animatable%20Head%20Avatars%0AAuthor%3A%20Yiming%20Zhong%20and%20Xiaolin%20Zhang%20and%20Ligang%20Liu%20and%20Yao%20Zhao%20and%20Yunchao%20Wei%0AAbstract%3A%20%20%20Similar%20to%20facial%20beautification%20in%20real%20life%2C%203D%20virtual%20avatars%20require%0Apersonalized%20customization%20to%20enhance%20their%20visual%20appeal%2C%20yet%20this%20area%0Aremains%20insufficiently%20explored.%20Although%20current%203D%20Gaussian%20editing%20methods%0Acan%20be%20adapted%20for%20facial%20makeup%20purposes%2C%20these%20methods%20fail%20to%20meet%20the%0Afundamental%20requirements%20for%20achieving%20realistic%20makeup%20effects%3A%201%29%20ensuring%20a%0Aconsistent%20appearance%20during%20drivable%20expressions%2C%202%29%20preserving%20the%20identity%0Athroughout%20the%20makeup%20process%2C%20and%203%29%20enabling%20precise%20control%20over%20fine%0Adetails.%20To%20address%20these%2C%20we%20propose%20a%20specialized%203D%20makeup%20method%20named%0AAvatarMakeup%2C%20leveraging%20a%20pretrained%20diffusion%20model%20to%20transfer%20makeup%0Apatterns%20from%20a%20single%20reference%20photo%20of%20any%20individual.%20We%20adopt%20a%0Acoarse-to-fine%20idea%20to%20first%20maintain%20the%20consistent%20appearance%20and%20identity%2C%0Aand%20then%20to%20refine%20the%20details.%20In%20particular%2C%20the%20diffusion%20model%20is%20employed%0Ato%20generate%20makeup%20images%20as%20supervision.%20Due%20to%20the%20uncertainties%20in%20diffusion%0Aprocess%2C%20the%20generated%20images%20are%20inconsistent%20across%20different%20viewpoints%20and%0Aexpressions.%20Therefore%2C%20we%20propose%20a%20Coherent%20Duplication%20method%20to%20coarsely%0Aapply%20makeup%20to%20the%20target%20while%20ensuring%20consistency%20across%20dynamic%20and%0Amultiview%20effects.%20Coherent%20Duplication%20optimizes%20a%20global%20UV%20map%20by%20recoding%0Athe%20averaged%20facial%20attributes%20among%20the%20generated%20makeup%20images.%20By%20querying%0Athe%20global%20UV%20map%2C%20it%20easily%20synthesizes%20coherent%20makeup%20guidance%20from%0Aarbitrary%20views%20and%20expressions%20to%20optimize%20the%20target%20avatar.%20Given%20the%20coarse%0Amakeup%20avatar%2C%20we%20further%20enhance%20the%20makeup%20by%20incorporating%20a%20Refinement%0AModule%20into%20the%20diffusion%20model%20to%20achieve%20high%20makeup%20quality.%20Experiments%0Ademonstrate%20that%20AvatarMakeup%20achieves%20state-of-the-art%20makeup%20transfer%20quality%0Aand%20consistency%20throughout%20animation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02419v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAvatarMakeup%253A%2520Realistic%2520Makeup%2520Transfer%2520for%25203D%2520Animatable%2520Head%2520Avatars%26entry.906535625%3DYiming%2520Zhong%2520and%2520Xiaolin%2520Zhang%2520and%2520Ligang%2520Liu%2520and%2520Yao%2520Zhao%2520and%2520Yunchao%2520Wei%26entry.1292438233%3D%2520%2520Similar%2520to%2520facial%2520beautification%2520in%2520real%2520life%252C%25203D%2520virtual%2520avatars%2520require%250Apersonalized%2520customization%2520to%2520enhance%2520their%2520visual%2520appeal%252C%2520yet%2520this%2520area%250Aremains%2520insufficiently%2520explored.%2520Although%2520current%25203D%2520Gaussian%2520editing%2520methods%250Acan%2520be%2520adapted%2520for%2520facial%2520makeup%2520purposes%252C%2520these%2520methods%2520fail%2520to%2520meet%2520the%250Afundamental%2520requirements%2520for%2520achieving%2520realistic%2520makeup%2520effects%253A%25201%2529%2520ensuring%2520a%250Aconsistent%2520appearance%2520during%2520drivable%2520expressions%252C%25202%2529%2520preserving%2520the%2520identity%250Athroughout%2520the%2520makeup%2520process%252C%2520and%25203%2529%2520enabling%2520precise%2520control%2520over%2520fine%250Adetails.%2520To%2520address%2520these%252C%2520we%2520propose%2520a%2520specialized%25203D%2520makeup%2520method%2520named%250AAvatarMakeup%252C%2520leveraging%2520a%2520pretrained%2520diffusion%2520model%2520to%2520transfer%2520makeup%250Apatterns%2520from%2520a%2520single%2520reference%2520photo%2520of%2520any%2520individual.%2520We%2520adopt%2520a%250Acoarse-to-fine%2520idea%2520to%2520first%2520maintain%2520the%2520consistent%2520appearance%2520and%2520identity%252C%250Aand%2520then%2520to%2520refine%2520the%2520details.%2520In%2520particular%252C%2520the%2520diffusion%2520model%2520is%2520employed%250Ato%2520generate%2520makeup%2520images%2520as%2520supervision.%2520Due%2520to%2520the%2520uncertainties%2520in%2520diffusion%250Aprocess%252C%2520the%2520generated%2520images%2520are%2520inconsistent%2520across%2520different%2520viewpoints%2520and%250Aexpressions.%2520Therefore%252C%2520we%2520propose%2520a%2520Coherent%2520Duplication%2520method%2520to%2520coarsely%250Aapply%2520makeup%2520to%2520the%2520target%2520while%2520ensuring%2520consistency%2520across%2520dynamic%2520and%250Amultiview%2520effects.%2520Coherent%2520Duplication%2520optimizes%2520a%2520global%2520UV%2520map%2520by%2520recoding%250Athe%2520averaged%2520facial%2520attributes%2520among%2520the%2520generated%2520makeup%2520images.%2520By%2520querying%250Athe%2520global%2520UV%2520map%252C%2520it%2520easily%2520synthesizes%2520coherent%2520makeup%2520guidance%2520from%250Aarbitrary%2520views%2520and%2520expressions%2520to%2520optimize%2520the%2520target%2520avatar.%2520Given%2520the%2520coarse%250Amakeup%2520avatar%252C%2520we%2520further%2520enhance%2520the%2520makeup%2520by%2520incorporating%2520a%2520Refinement%250AModule%2520into%2520the%2520diffusion%2520model%2520to%2520achieve%2520high%2520makeup%2520quality.%2520Experiments%250Ademonstrate%2520that%2520AvatarMakeup%2520achieves%2520state-of-the-art%2520makeup%2520transfer%2520quality%250Aand%2520consistency%2520throughout%2520animation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02419v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AvatarMakeup%3A%20Realistic%20Makeup%20Transfer%20for%203D%20Animatable%20Head%20Avatars&entry.906535625=Yiming%20Zhong%20and%20Xiaolin%20Zhang%20and%20Ligang%20Liu%20and%20Yao%20Zhao%20and%20Yunchao%20Wei&entry.1292438233=%20%20Similar%20to%20facial%20beautification%20in%20real%20life%2C%203D%20virtual%20avatars%20require%0Apersonalized%20customization%20to%20enhance%20their%20visual%20appeal%2C%20yet%20this%20area%0Aremains%20insufficiently%20explored.%20Although%20current%203D%20Gaussian%20editing%20methods%0Acan%20be%20adapted%20for%20facial%20makeup%20purposes%2C%20these%20methods%20fail%20to%20meet%20the%0Afundamental%20requirements%20for%20achieving%20realistic%20makeup%20effects%3A%201%29%20ensuring%20a%0Aconsistent%20appearance%20during%20drivable%20expressions%2C%202%29%20preserving%20the%20identity%0Athroughout%20the%20makeup%20process%2C%20and%203%29%20enabling%20precise%20control%20over%20fine%0Adetails.%20To%20address%20these%2C%20we%20propose%20a%20specialized%203D%20makeup%20method%20named%0AAvatarMakeup%2C%20leveraging%20a%20pretrained%20diffusion%20model%20to%20transfer%20makeup%0Apatterns%20from%20a%20single%20reference%20photo%20of%20any%20individual.%20We%20adopt%20a%0Acoarse-to-fine%20idea%20to%20first%20maintain%20the%20consistent%20appearance%20and%20identity%2C%0Aand%20then%20to%20refine%20the%20details.%20In%20particular%2C%20the%20diffusion%20model%20is%20employed%0Ato%20generate%20makeup%20images%20as%20supervision.%20Due%20to%20the%20uncertainties%20in%20diffusion%0Aprocess%2C%20the%20generated%20images%20are%20inconsistent%20across%20different%20viewpoints%20and%0Aexpressions.%20Therefore%2C%20we%20propose%20a%20Coherent%20Duplication%20method%20to%20coarsely%0Aapply%20makeup%20to%20the%20target%20while%20ensuring%20consistency%20across%20dynamic%20and%0Amultiview%20effects.%20Coherent%20Duplication%20optimizes%20a%20global%20UV%20map%20by%20recoding%0Athe%20averaged%20facial%20attributes%20among%20the%20generated%20makeup%20images.%20By%20querying%0Athe%20global%20UV%20map%2C%20it%20easily%20synthesizes%20coherent%20makeup%20guidance%20from%0Aarbitrary%20views%20and%20expressions%20to%20optimize%20the%20target%20avatar.%20Given%20the%20coarse%0Amakeup%20avatar%2C%20we%20further%20enhance%20the%20makeup%20by%20incorporating%20a%20Refinement%0AModule%20into%20the%20diffusion%20model%20to%20achieve%20high%20makeup%20quality.%20Experiments%0Ademonstrate%20that%20AvatarMakeup%20achieves%20state-of-the-art%20makeup%20transfer%20quality%0Aand%20consistency%20throughout%20animation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02419v2&entry.124074799=Read"},
{"title": "GaussRender: Learning 3D Occupancy with Gaussian Rendering", "author": "Lo\u00efck Chambon and Eloi Zablocki and Alexandre Boulch and Micka\u00ebl Chen and Matthieu Cord", "abstract": "  Understanding the 3D geometry and semantics of driving scenes is critical for\nsafe autonomous driving. Recent advances in 3D occupancy prediction have\nimproved scene representation but often suffer from visual inconsistencies,\nleading to floating artifacts and poor surface localization. Existing\nvoxel-wise losses (e.g., cross-entropy) fail to enforce visible geometric\ncoherence. In this paper, we propose GaussRender, a module that improves 3D\noccupancy learning by enforcing projective consistency. Our key idea is to\nproject both predicted and ground-truth 3D occupancy into 2D camera views,\nwhere we apply supervision. Our method penalizes 3D configurations that produce\ninconsistent 2D projections, thereby enforcing a more coherent 3D structure. To\nachieve this efficiently, we leverage differentiable rendering with Gaussian\nsplatting. GaussRender seamlessly integrates with existing architectures while\nmaintaining efficiency and requiring no inference-time modifications. Extensive\nevaluations on multiple benchmarks (SurroundOcc-nuScenes, Occ3D-nuScenes,\nSSCBench-KITTI360) demonstrate that GaussRender significantly improves\ngeometric fidelity across various 3D occupancy models (TPVFormer, SurroundOcc,\nSymphonies), achieving state-of-the-art results, particularly on\nsurface-sensitive metrics such as RayIoU. The code is open-sourced at\nhttps://github.com/valeoai/GaussRender.\n", "link": "http://arxiv.org/abs/2502.05040v3", "date": "2025-07-07", "relevancy": 3.3251, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6808}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6765}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussRender%3A%20Learning%203D%20Occupancy%20with%20Gaussian%20Rendering&body=Title%3A%20GaussRender%3A%20Learning%203D%20Occupancy%20with%20Gaussian%20Rendering%0AAuthor%3A%20Lo%C3%AFck%20Chambon%20and%20Eloi%20Zablocki%20and%20Alexandre%20Boulch%20and%20Micka%C3%ABl%20Chen%20and%20Matthieu%20Cord%0AAbstract%3A%20%20%20Understanding%20the%203D%20geometry%20and%20semantics%20of%20driving%20scenes%20is%20critical%20for%0Asafe%20autonomous%20driving.%20Recent%20advances%20in%203D%20occupancy%20prediction%20have%0Aimproved%20scene%20representation%20but%20often%20suffer%20from%20visual%20inconsistencies%2C%0Aleading%20to%20floating%20artifacts%20and%20poor%20surface%20localization.%20Existing%0Avoxel-wise%20losses%20%28e.g.%2C%20cross-entropy%29%20fail%20to%20enforce%20visible%20geometric%0Acoherence.%20In%20this%20paper%2C%20we%20propose%20GaussRender%2C%20a%20module%20that%20improves%203D%0Aoccupancy%20learning%20by%20enforcing%20projective%20consistency.%20Our%20key%20idea%20is%20to%0Aproject%20both%20predicted%20and%20ground-truth%203D%20occupancy%20into%202D%20camera%20views%2C%0Awhere%20we%20apply%20supervision.%20Our%20method%20penalizes%203D%20configurations%20that%20produce%0Ainconsistent%202D%20projections%2C%20thereby%20enforcing%20a%20more%20coherent%203D%20structure.%20To%0Aachieve%20this%20efficiently%2C%20we%20leverage%20differentiable%20rendering%20with%20Gaussian%0Asplatting.%20GaussRender%20seamlessly%20integrates%20with%20existing%20architectures%20while%0Amaintaining%20efficiency%20and%20requiring%20no%20inference-time%20modifications.%20Extensive%0Aevaluations%20on%20multiple%20benchmarks%20%28SurroundOcc-nuScenes%2C%20Occ3D-nuScenes%2C%0ASSCBench-KITTI360%29%20demonstrate%20that%20GaussRender%20significantly%20improves%0Ageometric%20fidelity%20across%20various%203D%20occupancy%20models%20%28TPVFormer%2C%20SurroundOcc%2C%0ASymphonies%29%2C%20achieving%20state-of-the-art%20results%2C%20particularly%20on%0Asurface-sensitive%20metrics%20such%20as%20RayIoU.%20The%20code%20is%20open-sourced%20at%0Ahttps%3A//github.com/valeoai/GaussRender.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05040v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussRender%253A%2520Learning%25203D%2520Occupancy%2520with%2520Gaussian%2520Rendering%26entry.906535625%3DLo%25C3%25AFck%2520Chambon%2520and%2520Eloi%2520Zablocki%2520and%2520Alexandre%2520Boulch%2520and%2520Micka%25C3%25ABl%2520Chen%2520and%2520Matthieu%2520Cord%26entry.1292438233%3D%2520%2520Understanding%2520the%25203D%2520geometry%2520and%2520semantics%2520of%2520driving%2520scenes%2520is%2520critical%2520for%250Asafe%2520autonomous%2520driving.%2520Recent%2520advances%2520in%25203D%2520occupancy%2520prediction%2520have%250Aimproved%2520scene%2520representation%2520but%2520often%2520suffer%2520from%2520visual%2520inconsistencies%252C%250Aleading%2520to%2520floating%2520artifacts%2520and%2520poor%2520surface%2520localization.%2520Existing%250Avoxel-wise%2520losses%2520%2528e.g.%252C%2520cross-entropy%2529%2520fail%2520to%2520enforce%2520visible%2520geometric%250Acoherence.%2520In%2520this%2520paper%252C%2520we%2520propose%2520GaussRender%252C%2520a%2520module%2520that%2520improves%25203D%250Aoccupancy%2520learning%2520by%2520enforcing%2520projective%2520consistency.%2520Our%2520key%2520idea%2520is%2520to%250Aproject%2520both%2520predicted%2520and%2520ground-truth%25203D%2520occupancy%2520into%25202D%2520camera%2520views%252C%250Awhere%2520we%2520apply%2520supervision.%2520Our%2520method%2520penalizes%25203D%2520configurations%2520that%2520produce%250Ainconsistent%25202D%2520projections%252C%2520thereby%2520enforcing%2520a%2520more%2520coherent%25203D%2520structure.%2520To%250Aachieve%2520this%2520efficiently%252C%2520we%2520leverage%2520differentiable%2520rendering%2520with%2520Gaussian%250Asplatting.%2520GaussRender%2520seamlessly%2520integrates%2520with%2520existing%2520architectures%2520while%250Amaintaining%2520efficiency%2520and%2520requiring%2520no%2520inference-time%2520modifications.%2520Extensive%250Aevaluations%2520on%2520multiple%2520benchmarks%2520%2528SurroundOcc-nuScenes%252C%2520Occ3D-nuScenes%252C%250ASSCBench-KITTI360%2529%2520demonstrate%2520that%2520GaussRender%2520significantly%2520improves%250Ageometric%2520fidelity%2520across%2520various%25203D%2520occupancy%2520models%2520%2528TPVFormer%252C%2520SurroundOcc%252C%250ASymphonies%2529%252C%2520achieving%2520state-of-the-art%2520results%252C%2520particularly%2520on%250Asurface-sensitive%2520metrics%2520such%2520as%2520RayIoU.%2520The%2520code%2520is%2520open-sourced%2520at%250Ahttps%253A//github.com/valeoai/GaussRender.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05040v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussRender%3A%20Learning%203D%20Occupancy%20with%20Gaussian%20Rendering&entry.906535625=Lo%C3%AFck%20Chambon%20and%20Eloi%20Zablocki%20and%20Alexandre%20Boulch%20and%20Micka%C3%ABl%20Chen%20and%20Matthieu%20Cord&entry.1292438233=%20%20Understanding%20the%203D%20geometry%20and%20semantics%20of%20driving%20scenes%20is%20critical%20for%0Asafe%20autonomous%20driving.%20Recent%20advances%20in%203D%20occupancy%20prediction%20have%0Aimproved%20scene%20representation%20but%20often%20suffer%20from%20visual%20inconsistencies%2C%0Aleading%20to%20floating%20artifacts%20and%20poor%20surface%20localization.%20Existing%0Avoxel-wise%20losses%20%28e.g.%2C%20cross-entropy%29%20fail%20to%20enforce%20visible%20geometric%0Acoherence.%20In%20this%20paper%2C%20we%20propose%20GaussRender%2C%20a%20module%20that%20improves%203D%0Aoccupancy%20learning%20by%20enforcing%20projective%20consistency.%20Our%20key%20idea%20is%20to%0Aproject%20both%20predicted%20and%20ground-truth%203D%20occupancy%20into%202D%20camera%20views%2C%0Awhere%20we%20apply%20supervision.%20Our%20method%20penalizes%203D%20configurations%20that%20produce%0Ainconsistent%202D%20projections%2C%20thereby%20enforcing%20a%20more%20coherent%203D%20structure.%20To%0Aachieve%20this%20efficiently%2C%20we%20leverage%20differentiable%20rendering%20with%20Gaussian%0Asplatting.%20GaussRender%20seamlessly%20integrates%20with%20existing%20architectures%20while%0Amaintaining%20efficiency%20and%20requiring%20no%20inference-time%20modifications.%20Extensive%0Aevaluations%20on%20multiple%20benchmarks%20%28SurroundOcc-nuScenes%2C%20Occ3D-nuScenes%2C%0ASSCBench-KITTI360%29%20demonstrate%20that%20GaussRender%20significantly%20improves%0Ageometric%20fidelity%20across%20various%203D%20occupancy%20models%20%28TPVFormer%2C%20SurroundOcc%2C%0ASymphonies%29%2C%20achieving%20state-of-the-art%20results%2C%20particularly%20on%0Asurface-sensitive%20metrics%20such%20as%20RayIoU.%20The%20code%20is%20open-sourced%20at%0Ahttps%3A//github.com/valeoai/GaussRender.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05040v3&entry.124074799=Read"},
{"title": "All in One: Visual-Description-Guided Unified Point Cloud Segmentation", "author": "Zongyan Han and Mohamed El Amine Boudjoghra and Jiahua Dong and Jinhong Wang and Rao Muhammad Anwer", "abstract": "  Unified segmentation of 3D point clouds is crucial for scene understanding,\nbut is hindered by its sparse structure, limited annotations, and the challenge\nof distinguishing fine-grained object classes in complex environments. Existing\nmethods often struggle to capture rich semantic and contextual information due\nto limited supervision and a lack of diverse multimodal cues, leading to\nsuboptimal differentiation of classes and instances. To address these\nchallenges, we propose VDG-Uni3DSeg, a novel framework that integrates\npre-trained vision-language models (e.g., CLIP) and large language models\n(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual\ndescriptions and reference images from the internet, our method incorporates\nrich multimodal cues, facilitating fine-grained class and instance separation.\nWe further design a Semantic-Visual Contrastive Loss to align point features\nwith multimodal queries and a Spatial Enhanced Module to model scene-wide\nrelationships efficiently. Operating within a closed-set paradigm that utilizes\nmultimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art\nresults in semantic, instance, and panoptic segmentation, offering a scalable\nand practical solution for 3D understanding. Our code is available at\nhttps://github.com/Hanzy1996/VDG-Uni3DSeg.\n", "link": "http://arxiv.org/abs/2507.05211v1", "date": "2025-07-07", "relevancy": 3.1749, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6448}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6448}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20All%20in%20One%3A%20Visual-Description-Guided%20Unified%20Point%20Cloud%20Segmentation&body=Title%3A%20All%20in%20One%3A%20Visual-Description-Guided%20Unified%20Point%20Cloud%20Segmentation%0AAuthor%3A%20Zongyan%20Han%20and%20Mohamed%20El%20Amine%20Boudjoghra%20and%20Jiahua%20Dong%20and%20Jinhong%20Wang%20and%20Rao%20Muhammad%20Anwer%0AAbstract%3A%20%20%20Unified%20segmentation%20of%203D%20point%20clouds%20is%20crucial%20for%20scene%20understanding%2C%0Abut%20is%20hindered%20by%20its%20sparse%20structure%2C%20limited%20annotations%2C%20and%20the%20challenge%0Aof%20distinguishing%20fine-grained%20object%20classes%20in%20complex%20environments.%20Existing%0Amethods%20often%20struggle%20to%20capture%20rich%20semantic%20and%20contextual%20information%20due%0Ato%20limited%20supervision%20and%20a%20lack%20of%20diverse%20multimodal%20cues%2C%20leading%20to%0Asuboptimal%20differentiation%20of%20classes%20and%20instances.%20To%20address%20these%0Achallenges%2C%20we%20propose%20VDG-Uni3DSeg%2C%20a%20novel%20framework%20that%20integrates%0Apre-trained%20vision-language%20models%20%28e.g.%2C%20CLIP%29%20and%20large%20language%20models%0A%28LLMs%29%20to%20enhance%203D%20segmentation.%20By%20leveraging%20LLM-generated%20textual%0Adescriptions%20and%20reference%20images%20from%20the%20internet%2C%20our%20method%20incorporates%0Arich%20multimodal%20cues%2C%20facilitating%20fine-grained%20class%20and%20instance%20separation.%0AWe%20further%20design%20a%20Semantic-Visual%20Contrastive%20Loss%20to%20align%20point%20features%0Awith%20multimodal%20queries%20and%20a%20Spatial%20Enhanced%20Module%20to%20model%20scene-wide%0Arelationships%20efficiently.%20Operating%20within%20a%20closed-set%20paradigm%20that%20utilizes%0Amultimodal%20knowledge%20generated%20offline%2C%20VDG-Uni3DSeg%20achieves%20state-of-the-art%0Aresults%20in%20semantic%2C%20instance%2C%20and%20panoptic%20segmentation%2C%20offering%20a%20scalable%0Aand%20practical%20solution%20for%203D%20understanding.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Hanzy1996/VDG-Uni3DSeg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAll%2520in%2520One%253A%2520Visual-Description-Guided%2520Unified%2520Point%2520Cloud%2520Segmentation%26entry.906535625%3DZongyan%2520Han%2520and%2520Mohamed%2520El%2520Amine%2520Boudjoghra%2520and%2520Jiahua%2520Dong%2520and%2520Jinhong%2520Wang%2520and%2520Rao%2520Muhammad%2520Anwer%26entry.1292438233%3D%2520%2520Unified%2520segmentation%2520of%25203D%2520point%2520clouds%2520is%2520crucial%2520for%2520scene%2520understanding%252C%250Abut%2520is%2520hindered%2520by%2520its%2520sparse%2520structure%252C%2520limited%2520annotations%252C%2520and%2520the%2520challenge%250Aof%2520distinguishing%2520fine-grained%2520object%2520classes%2520in%2520complex%2520environments.%2520Existing%250Amethods%2520often%2520struggle%2520to%2520capture%2520rich%2520semantic%2520and%2520contextual%2520information%2520due%250Ato%2520limited%2520supervision%2520and%2520a%2520lack%2520of%2520diverse%2520multimodal%2520cues%252C%2520leading%2520to%250Asuboptimal%2520differentiation%2520of%2520classes%2520and%2520instances.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520VDG-Uni3DSeg%252C%2520a%2520novel%2520framework%2520that%2520integrates%250Apre-trained%2520vision-language%2520models%2520%2528e.g.%252C%2520CLIP%2529%2520and%2520large%2520language%2520models%250A%2528LLMs%2529%2520to%2520enhance%25203D%2520segmentation.%2520By%2520leveraging%2520LLM-generated%2520textual%250Adescriptions%2520and%2520reference%2520images%2520from%2520the%2520internet%252C%2520our%2520method%2520incorporates%250Arich%2520multimodal%2520cues%252C%2520facilitating%2520fine-grained%2520class%2520and%2520instance%2520separation.%250AWe%2520further%2520design%2520a%2520Semantic-Visual%2520Contrastive%2520Loss%2520to%2520align%2520point%2520features%250Awith%2520multimodal%2520queries%2520and%2520a%2520Spatial%2520Enhanced%2520Module%2520to%2520model%2520scene-wide%250Arelationships%2520efficiently.%2520Operating%2520within%2520a%2520closed-set%2520paradigm%2520that%2520utilizes%250Amultimodal%2520knowledge%2520generated%2520offline%252C%2520VDG-Uni3DSeg%2520achieves%2520state-of-the-art%250Aresults%2520in%2520semantic%252C%2520instance%252C%2520and%2520panoptic%2520segmentation%252C%2520offering%2520a%2520scalable%250Aand%2520practical%2520solution%2520for%25203D%2520understanding.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Hanzy1996/VDG-Uni3DSeg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=All%20in%20One%3A%20Visual-Description-Guided%20Unified%20Point%20Cloud%20Segmentation&entry.906535625=Zongyan%20Han%20and%20Mohamed%20El%20Amine%20Boudjoghra%20and%20Jiahua%20Dong%20and%20Jinhong%20Wang%20and%20Rao%20Muhammad%20Anwer&entry.1292438233=%20%20Unified%20segmentation%20of%203D%20point%20clouds%20is%20crucial%20for%20scene%20understanding%2C%0Abut%20is%20hindered%20by%20its%20sparse%20structure%2C%20limited%20annotations%2C%20and%20the%20challenge%0Aof%20distinguishing%20fine-grained%20object%20classes%20in%20complex%20environments.%20Existing%0Amethods%20often%20struggle%20to%20capture%20rich%20semantic%20and%20contextual%20information%20due%0Ato%20limited%20supervision%20and%20a%20lack%20of%20diverse%20multimodal%20cues%2C%20leading%20to%0Asuboptimal%20differentiation%20of%20classes%20and%20instances.%20To%20address%20these%0Achallenges%2C%20we%20propose%20VDG-Uni3DSeg%2C%20a%20novel%20framework%20that%20integrates%0Apre-trained%20vision-language%20models%20%28e.g.%2C%20CLIP%29%20and%20large%20language%20models%0A%28LLMs%29%20to%20enhance%203D%20segmentation.%20By%20leveraging%20LLM-generated%20textual%0Adescriptions%20and%20reference%20images%20from%20the%20internet%2C%20our%20method%20incorporates%0Arich%20multimodal%20cues%2C%20facilitating%20fine-grained%20class%20and%20instance%20separation.%0AWe%20further%20design%20a%20Semantic-Visual%20Contrastive%20Loss%20to%20align%20point%20features%0Awith%20multimodal%20queries%20and%20a%20Spatial%20Enhanced%20Module%20to%20model%20scene-wide%0Arelationships%20efficiently.%20Operating%20within%20a%20closed-set%20paradigm%20that%20utilizes%0Amultimodal%20knowledge%20generated%20offline%2C%20VDG-Uni3DSeg%20achieves%20state-of-the-art%0Aresults%20in%20semantic%2C%20instance%2C%20and%20panoptic%20segmentation%2C%20offering%20a%20scalable%0Aand%20practical%20solution%20for%203D%20understanding.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Hanzy1996/VDG-Uni3DSeg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05211v1&entry.124074799=Read"},
{"title": "InterGSEdit: Interactive 3D Gaussian Splatting Editing with 3D\n  Geometry-Consistent Attention Prior", "author": "Minghao Wen and Shengjie Wu and Kangkan Wang and Dong Liang", "abstract": "  3D Gaussian Splatting based 3D editing has demonstrated impressive\nperformance in recent years. However, the multi-view editing often exhibits\nsignificant local inconsistency, especially in areas of non-rigid deformation,\nwhich lead to local artifacts, texture blurring, or semantic variations in\nedited 3D scenes. We also found that the existing editing methods, which rely\nentirely on text prompts make the editing process a \"one-shot deal\", making it\ndifficult for users to control the editing degree flexibly. In response to\nthese challenges, we present InterGSEdit, a novel framework for high-quality\n3DGS editing via interactively selecting key views with users' preferences. We\npropose a CLIP-based Semantic Consistency Selection (CSCS) strategy to\nadaptively screen a group of semantically consistent reference views for each\nuser-selected key view. Then, the cross-attention maps derived from the\nreference views are used in a weighted Gaussian Splatting unprojection to\nconstruct the 3D Geometry-Consistent Attention Prior ($GAP^{3D}$). We project\n$GAP^{3D}$ to obtain 3D-constrained attention, which are fused with 2D\ncross-attention via Attention Fusion Network (AFN). AFN employs an adaptive\nattention strategy that prioritizes 3D-constrained attention for geometric\nconsistency during early inference, and gradually prioritizes 2D\ncross-attention maps in diffusion for fine-grained features during the later\ninference. Extensive experiments demonstrate that InterGSEdit achieves\nstate-of-the-art performance, delivering consistent, high-fidelity 3DGS editing\nwith improved user experience.\n", "link": "http://arxiv.org/abs/2507.04961v1", "date": "2025-07-07", "relevancy": 3.1508, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6409}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6316}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InterGSEdit%3A%20Interactive%203D%20Gaussian%20Splatting%20Editing%20with%203D%0A%20%20Geometry-Consistent%20Attention%20Prior&body=Title%3A%20InterGSEdit%3A%20Interactive%203D%20Gaussian%20Splatting%20Editing%20with%203D%0A%20%20Geometry-Consistent%20Attention%20Prior%0AAuthor%3A%20Minghao%20Wen%20and%20Shengjie%20Wu%20and%20Kangkan%20Wang%20and%20Dong%20Liang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20based%203D%20editing%20has%20demonstrated%20impressive%0Aperformance%20in%20recent%20years.%20However%2C%20the%20multi-view%20editing%20often%20exhibits%0Asignificant%20local%20inconsistency%2C%20especially%20in%20areas%20of%20non-rigid%20deformation%2C%0Awhich%20lead%20to%20local%20artifacts%2C%20texture%20blurring%2C%20or%20semantic%20variations%20in%0Aedited%203D%20scenes.%20We%20also%20found%20that%20the%20existing%20editing%20methods%2C%20which%20rely%0Aentirely%20on%20text%20prompts%20make%20the%20editing%20process%20a%20%22one-shot%20deal%22%2C%20making%20it%0Adifficult%20for%20users%20to%20control%20the%20editing%20degree%20flexibly.%20In%20response%20to%0Athese%20challenges%2C%20we%20present%20InterGSEdit%2C%20a%20novel%20framework%20for%20high-quality%0A3DGS%20editing%20via%20interactively%20selecting%20key%20views%20with%20users%27%20preferences.%20We%0Apropose%20a%20CLIP-based%20Semantic%20Consistency%20Selection%20%28CSCS%29%20strategy%20to%0Aadaptively%20screen%20a%20group%20of%20semantically%20consistent%20reference%20views%20for%20each%0Auser-selected%20key%20view.%20Then%2C%20the%20cross-attention%20maps%20derived%20from%20the%0Areference%20views%20are%20used%20in%20a%20weighted%20Gaussian%20Splatting%20unprojection%20to%0Aconstruct%20the%203D%20Geometry-Consistent%20Attention%20Prior%20%28%24GAP%5E%7B3D%7D%24%29.%20We%20project%0A%24GAP%5E%7B3D%7D%24%20to%20obtain%203D-constrained%20attention%2C%20which%20are%20fused%20with%202D%0Across-attention%20via%20Attention%20Fusion%20Network%20%28AFN%29.%20AFN%20employs%20an%20adaptive%0Aattention%20strategy%20that%20prioritizes%203D-constrained%20attention%20for%20geometric%0Aconsistency%20during%20early%20inference%2C%20and%20gradually%20prioritizes%202D%0Across-attention%20maps%20in%20diffusion%20for%20fine-grained%20features%20during%20the%20later%0Ainference.%20Extensive%20experiments%20demonstrate%20that%20InterGSEdit%20achieves%0Astate-of-the-art%20performance%2C%20delivering%20consistent%2C%20high-fidelity%203DGS%20editing%0Awith%20improved%20user%20experience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04961v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterGSEdit%253A%2520Interactive%25203D%2520Gaussian%2520Splatting%2520Editing%2520with%25203D%250A%2520%2520Geometry-Consistent%2520Attention%2520Prior%26entry.906535625%3DMinghao%2520Wen%2520and%2520Shengjie%2520Wu%2520and%2520Kangkan%2520Wang%2520and%2520Dong%2520Liang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520based%25203D%2520editing%2520has%2520demonstrated%2520impressive%250Aperformance%2520in%2520recent%2520years.%2520However%252C%2520the%2520multi-view%2520editing%2520often%2520exhibits%250Asignificant%2520local%2520inconsistency%252C%2520especially%2520in%2520areas%2520of%2520non-rigid%2520deformation%252C%250Awhich%2520lead%2520to%2520local%2520artifacts%252C%2520texture%2520blurring%252C%2520or%2520semantic%2520variations%2520in%250Aedited%25203D%2520scenes.%2520We%2520also%2520found%2520that%2520the%2520existing%2520editing%2520methods%252C%2520which%2520rely%250Aentirely%2520on%2520text%2520prompts%2520make%2520the%2520editing%2520process%2520a%2520%2522one-shot%2520deal%2522%252C%2520making%2520it%250Adifficult%2520for%2520users%2520to%2520control%2520the%2520editing%2520degree%2520flexibly.%2520In%2520response%2520to%250Athese%2520challenges%252C%2520we%2520present%2520InterGSEdit%252C%2520a%2520novel%2520framework%2520for%2520high-quality%250A3DGS%2520editing%2520via%2520interactively%2520selecting%2520key%2520views%2520with%2520users%2527%2520preferences.%2520We%250Apropose%2520a%2520CLIP-based%2520Semantic%2520Consistency%2520Selection%2520%2528CSCS%2529%2520strategy%2520to%250Aadaptively%2520screen%2520a%2520group%2520of%2520semantically%2520consistent%2520reference%2520views%2520for%2520each%250Auser-selected%2520key%2520view.%2520Then%252C%2520the%2520cross-attention%2520maps%2520derived%2520from%2520the%250Areference%2520views%2520are%2520used%2520in%2520a%2520weighted%2520Gaussian%2520Splatting%2520unprojection%2520to%250Aconstruct%2520the%25203D%2520Geometry-Consistent%2520Attention%2520Prior%2520%2528%2524GAP%255E%257B3D%257D%2524%2529.%2520We%2520project%250A%2524GAP%255E%257B3D%257D%2524%2520to%2520obtain%25203D-constrained%2520attention%252C%2520which%2520are%2520fused%2520with%25202D%250Across-attention%2520via%2520Attention%2520Fusion%2520Network%2520%2528AFN%2529.%2520AFN%2520employs%2520an%2520adaptive%250Aattention%2520strategy%2520that%2520prioritizes%25203D-constrained%2520attention%2520for%2520geometric%250Aconsistency%2520during%2520early%2520inference%252C%2520and%2520gradually%2520prioritizes%25202D%250Across-attention%2520maps%2520in%2520diffusion%2520for%2520fine-grained%2520features%2520during%2520the%2520later%250Ainference.%2520Extensive%2520experiments%2520demonstrate%2520that%2520InterGSEdit%2520achieves%250Astate-of-the-art%2520performance%252C%2520delivering%2520consistent%252C%2520high-fidelity%25203DGS%2520editing%250Awith%2520improved%2520user%2520experience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04961v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InterGSEdit%3A%20Interactive%203D%20Gaussian%20Splatting%20Editing%20with%203D%0A%20%20Geometry-Consistent%20Attention%20Prior&entry.906535625=Minghao%20Wen%20and%20Shengjie%20Wu%20and%20Kangkan%20Wang%20and%20Dong%20Liang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20based%203D%20editing%20has%20demonstrated%20impressive%0Aperformance%20in%20recent%20years.%20However%2C%20the%20multi-view%20editing%20often%20exhibits%0Asignificant%20local%20inconsistency%2C%20especially%20in%20areas%20of%20non-rigid%20deformation%2C%0Awhich%20lead%20to%20local%20artifacts%2C%20texture%20blurring%2C%20or%20semantic%20variations%20in%0Aedited%203D%20scenes.%20We%20also%20found%20that%20the%20existing%20editing%20methods%2C%20which%20rely%0Aentirely%20on%20text%20prompts%20make%20the%20editing%20process%20a%20%22one-shot%20deal%22%2C%20making%20it%0Adifficult%20for%20users%20to%20control%20the%20editing%20degree%20flexibly.%20In%20response%20to%0Athese%20challenges%2C%20we%20present%20InterGSEdit%2C%20a%20novel%20framework%20for%20high-quality%0A3DGS%20editing%20via%20interactively%20selecting%20key%20views%20with%20users%27%20preferences.%20We%0Apropose%20a%20CLIP-based%20Semantic%20Consistency%20Selection%20%28CSCS%29%20strategy%20to%0Aadaptively%20screen%20a%20group%20of%20semantically%20consistent%20reference%20views%20for%20each%0Auser-selected%20key%20view.%20Then%2C%20the%20cross-attention%20maps%20derived%20from%20the%0Areference%20views%20are%20used%20in%20a%20weighted%20Gaussian%20Splatting%20unprojection%20to%0Aconstruct%20the%203D%20Geometry-Consistent%20Attention%20Prior%20%28%24GAP%5E%7B3D%7D%24%29.%20We%20project%0A%24GAP%5E%7B3D%7D%24%20to%20obtain%203D-constrained%20attention%2C%20which%20are%20fused%20with%202D%0Across-attention%20via%20Attention%20Fusion%20Network%20%28AFN%29.%20AFN%20employs%20an%20adaptive%0Aattention%20strategy%20that%20prioritizes%203D-constrained%20attention%20for%20geometric%0Aconsistency%20during%20early%20inference%2C%20and%20gradually%20prioritizes%202D%0Across-attention%20maps%20in%20diffusion%20for%20fine-grained%20features%20during%20the%20later%0Ainference.%20Extensive%20experiments%20demonstrate%20that%20InterGSEdit%20achieves%0Astate-of-the-art%20performance%2C%20delivering%20consistent%2C%20high-fidelity%203DGS%20editing%0Awith%20improved%20user%20experience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04961v1&entry.124074799=Read"},
{"title": "DynamicFace: High-Quality and Consistent Face Swapping for Image and\n  Video using Composable 3D Facial Priors", "author": "Runqi Wang and Yang Chen and Sijie Xu and Tianyao He and Wei Zhu and Dejia Song and Nemo Chen and Xu Tang and Yao Hu", "abstract": "  Face swapping transfers the identity of a source face to a target face while\nretaining the attributes like expression, pose, hair, and background of the\ntarget face. Advanced face swapping methods have achieved attractive results.\nHowever, these methods often inadvertently transfer identity information from\nthe target face, compromising expression-related details and accurate identity.\nWe propose a novel method DynamicFace that leverages the power of diffusion\nmodels and plug-and-play adaptive attention layers for image and video face\nswapping. First, we introduce four fine-grained facial conditions using 3D\nfacial priors. All conditions are designed to be disentangled from each other\nfor precise and unique control. Then, we adopt Face Former and ReferenceNet for\nhigh-level and detailed identity injection. Through experiments on the FF++\ndataset, we demonstrate that our method achieves state-of-the-art results in\nface swapping, showcasing superior image quality, identity preservation, and\nexpression accuracy. Our framework seamlessly adapts to both image and video\ndomains. Our code and results will be available on the project page:\nhttps://dynamic-face.github.io/\n", "link": "http://arxiv.org/abs/2501.08553v2", "date": "2025-07-07", "relevancy": 3.1368, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6501}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6304}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynamicFace%3A%20High-Quality%20and%20Consistent%20Face%20Swapping%20for%20Image%20and%0A%20%20Video%20using%20Composable%203D%20Facial%20Priors&body=Title%3A%20DynamicFace%3A%20High-Quality%20and%20Consistent%20Face%20Swapping%20for%20Image%20and%0A%20%20Video%20using%20Composable%203D%20Facial%20Priors%0AAuthor%3A%20Runqi%20Wang%20and%20Yang%20Chen%20and%20Sijie%20Xu%20and%20Tianyao%20He%20and%20Wei%20Zhu%20and%20Dejia%20Song%20and%20Nemo%20Chen%20and%20Xu%20Tang%20and%20Yao%20Hu%0AAbstract%3A%20%20%20Face%20swapping%20transfers%20the%20identity%20of%20a%20source%20face%20to%20a%20target%20face%20while%0Aretaining%20the%20attributes%20like%20expression%2C%20pose%2C%20hair%2C%20and%20background%20of%20the%0Atarget%20face.%20Advanced%20face%20swapping%20methods%20have%20achieved%20attractive%20results.%0AHowever%2C%20these%20methods%20often%20inadvertently%20transfer%20identity%20information%20from%0Athe%20target%20face%2C%20compromising%20expression-related%20details%20and%20accurate%20identity.%0AWe%20propose%20a%20novel%20method%20DynamicFace%20that%20leverages%20the%20power%20of%20diffusion%0Amodels%20and%20plug-and-play%20adaptive%20attention%20layers%20for%20image%20and%20video%20face%0Aswapping.%20First%2C%20we%20introduce%20four%20fine-grained%20facial%20conditions%20using%203D%0Afacial%20priors.%20All%20conditions%20are%20designed%20to%20be%20disentangled%20from%20each%20other%0Afor%20precise%20and%20unique%20control.%20Then%2C%20we%20adopt%20Face%20Former%20and%20ReferenceNet%20for%0Ahigh-level%20and%20detailed%20identity%20injection.%20Through%20experiments%20on%20the%20FF%2B%2B%0Adataset%2C%20we%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20results%20in%0Aface%20swapping%2C%20showcasing%20superior%20image%20quality%2C%20identity%20preservation%2C%20and%0Aexpression%20accuracy.%20Our%20framework%20seamlessly%20adapts%20to%20both%20image%20and%20video%0Adomains.%20Our%20code%20and%20results%20will%20be%20available%20on%20the%20project%20page%3A%0Ahttps%3A//dynamic-face.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08553v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamicFace%253A%2520High-Quality%2520and%2520Consistent%2520Face%2520Swapping%2520for%2520Image%2520and%250A%2520%2520Video%2520using%2520Composable%25203D%2520Facial%2520Priors%26entry.906535625%3DRunqi%2520Wang%2520and%2520Yang%2520Chen%2520and%2520Sijie%2520Xu%2520and%2520Tianyao%2520He%2520and%2520Wei%2520Zhu%2520and%2520Dejia%2520Song%2520and%2520Nemo%2520Chen%2520and%2520Xu%2520Tang%2520and%2520Yao%2520Hu%26entry.1292438233%3D%2520%2520Face%2520swapping%2520transfers%2520the%2520identity%2520of%2520a%2520source%2520face%2520to%2520a%2520target%2520face%2520while%250Aretaining%2520the%2520attributes%2520like%2520expression%252C%2520pose%252C%2520hair%252C%2520and%2520background%2520of%2520the%250Atarget%2520face.%2520Advanced%2520face%2520swapping%2520methods%2520have%2520achieved%2520attractive%2520results.%250AHowever%252C%2520these%2520methods%2520often%2520inadvertently%2520transfer%2520identity%2520information%2520from%250Athe%2520target%2520face%252C%2520compromising%2520expression-related%2520details%2520and%2520accurate%2520identity.%250AWe%2520propose%2520a%2520novel%2520method%2520DynamicFace%2520that%2520leverages%2520the%2520power%2520of%2520diffusion%250Amodels%2520and%2520plug-and-play%2520adaptive%2520attention%2520layers%2520for%2520image%2520and%2520video%2520face%250Aswapping.%2520First%252C%2520we%2520introduce%2520four%2520fine-grained%2520facial%2520conditions%2520using%25203D%250Afacial%2520priors.%2520All%2520conditions%2520are%2520designed%2520to%2520be%2520disentangled%2520from%2520each%2520other%250Afor%2520precise%2520and%2520unique%2520control.%2520Then%252C%2520we%2520adopt%2520Face%2520Former%2520and%2520ReferenceNet%2520for%250Ahigh-level%2520and%2520detailed%2520identity%2520injection.%2520Through%2520experiments%2520on%2520the%2520FF%252B%252B%250Adataset%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520results%2520in%250Aface%2520swapping%252C%2520showcasing%2520superior%2520image%2520quality%252C%2520identity%2520preservation%252C%2520and%250Aexpression%2520accuracy.%2520Our%2520framework%2520seamlessly%2520adapts%2520to%2520both%2520image%2520and%2520video%250Adomains.%2520Our%2520code%2520and%2520results%2520will%2520be%2520available%2520on%2520the%2520project%2520page%253A%250Ahttps%253A//dynamic-face.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08553v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynamicFace%3A%20High-Quality%20and%20Consistent%20Face%20Swapping%20for%20Image%20and%0A%20%20Video%20using%20Composable%203D%20Facial%20Priors&entry.906535625=Runqi%20Wang%20and%20Yang%20Chen%20and%20Sijie%20Xu%20and%20Tianyao%20He%20and%20Wei%20Zhu%20and%20Dejia%20Song%20and%20Nemo%20Chen%20and%20Xu%20Tang%20and%20Yao%20Hu&entry.1292438233=%20%20Face%20swapping%20transfers%20the%20identity%20of%20a%20source%20face%20to%20a%20target%20face%20while%0Aretaining%20the%20attributes%20like%20expression%2C%20pose%2C%20hair%2C%20and%20background%20of%20the%0Atarget%20face.%20Advanced%20face%20swapping%20methods%20have%20achieved%20attractive%20results.%0AHowever%2C%20these%20methods%20often%20inadvertently%20transfer%20identity%20information%20from%0Athe%20target%20face%2C%20compromising%20expression-related%20details%20and%20accurate%20identity.%0AWe%20propose%20a%20novel%20method%20DynamicFace%20that%20leverages%20the%20power%20of%20diffusion%0Amodels%20and%20plug-and-play%20adaptive%20attention%20layers%20for%20image%20and%20video%20face%0Aswapping.%20First%2C%20we%20introduce%20four%20fine-grained%20facial%20conditions%20using%203D%0Afacial%20priors.%20All%20conditions%20are%20designed%20to%20be%20disentangled%20from%20each%20other%0Afor%20precise%20and%20unique%20control.%20Then%2C%20we%20adopt%20Face%20Former%20and%20ReferenceNet%20for%0Ahigh-level%20and%20detailed%20identity%20injection.%20Through%20experiments%20on%20the%20FF%2B%2B%0Adataset%2C%20we%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20results%20in%0Aface%20swapping%2C%20showcasing%20superior%20image%20quality%2C%20identity%20preservation%2C%20and%0Aexpression%20accuracy.%20Our%20framework%20seamlessly%20adapts%20to%20both%20image%20and%20video%0Adomains.%20Our%20code%20and%20results%20will%20be%20available%20on%20the%20project%20page%3A%0Ahttps%3A//dynamic-face.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08553v2&entry.124074799=Read"},
{"title": "NavigScene: Bridging Local Perception and Global Navigation for\n  Beyond-Visual-Range Autonomous Driving", "author": "Qucheng Peng and Chen Bai and Guoxiang Zhang and Bo Xu and Xiaotong Liu and Xiaoyin Zheng and Chen Chen and Cheng Lu", "abstract": "  Autonomous driving systems have made significant advances in Q&A, perception,\nprediction, and planning based on local visual information, yet they struggle\nto incorporate broader navigational context that human drivers routinely\nutilize. We address this critical gap between local sensor data and global\nnavigation information by proposing NavigScene, an auxiliary navigation-guided\nnatural language dataset that simulates a human-like driving environment within\nautonomous driving systems. Moreover, we develop three complementary paradigms\nto leverage NavigScene: (1) Navigation-guided Reasoning, which enhances\nvision-language models by incorporating navigation context into the prompting\napproach; (2) Navigation-guided Preference Optimization, a reinforcement\nlearning method that extends Direct Preference Optimization to improve\nvision-language model responses by establishing preferences for\nnavigation-relevant summarized information; and (3) Navigation-guided\nVision-Language-Action model, which integrates navigation guidance and\nvision-language models with conventional driving models through feature fusion.\nExtensive experiments demonstrate that our approaches significantly improve\nperformance across perception, prediction, planning, and question-answering\ntasks by enabling reasoning capabilities beyond visual range and improving\ngeneralization to diverse driving scenarios. This work represents a significant\nstep toward more comprehensive autonomous driving systems capable of navigating\ncomplex, unfamiliar environments with greater reliability and safety.\n", "link": "http://arxiv.org/abs/2507.05227v1", "date": "2025-07-07", "relevancy": 3.0722, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6195}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NavigScene%3A%20Bridging%20Local%20Perception%20and%20Global%20Navigation%20for%0A%20%20Beyond-Visual-Range%20Autonomous%20Driving&body=Title%3A%20NavigScene%3A%20Bridging%20Local%20Perception%20and%20Global%20Navigation%20for%0A%20%20Beyond-Visual-Range%20Autonomous%20Driving%0AAuthor%3A%20Qucheng%20Peng%20and%20Chen%20Bai%20and%20Guoxiang%20Zhang%20and%20Bo%20Xu%20and%20Xiaotong%20Liu%20and%20Xiaoyin%20Zheng%20and%20Chen%20Chen%20and%20Cheng%20Lu%0AAbstract%3A%20%20%20Autonomous%20driving%20systems%20have%20made%20significant%20advances%20in%20Q%26A%2C%20perception%2C%0Aprediction%2C%20and%20planning%20based%20on%20local%20visual%20information%2C%20yet%20they%20struggle%0Ato%20incorporate%20broader%20navigational%20context%20that%20human%20drivers%20routinely%0Autilize.%20We%20address%20this%20critical%20gap%20between%20local%20sensor%20data%20and%20global%0Anavigation%20information%20by%20proposing%20NavigScene%2C%20an%20auxiliary%20navigation-guided%0Anatural%20language%20dataset%20that%20simulates%20a%20human-like%20driving%20environment%20within%0Aautonomous%20driving%20systems.%20Moreover%2C%20we%20develop%20three%20complementary%20paradigms%0Ato%20leverage%20NavigScene%3A%20%281%29%20Navigation-guided%20Reasoning%2C%20which%20enhances%0Avision-language%20models%20by%20incorporating%20navigation%20context%20into%20the%20prompting%0Aapproach%3B%20%282%29%20Navigation-guided%20Preference%20Optimization%2C%20a%20reinforcement%0Alearning%20method%20that%20extends%20Direct%20Preference%20Optimization%20to%20improve%0Avision-language%20model%20responses%20by%20establishing%20preferences%20for%0Anavigation-relevant%20summarized%20information%3B%20and%20%283%29%20Navigation-guided%0AVision-Language-Action%20model%2C%20which%20integrates%20navigation%20guidance%20and%0Avision-language%20models%20with%20conventional%20driving%20models%20through%20feature%20fusion.%0AExtensive%20experiments%20demonstrate%20that%20our%20approaches%20significantly%20improve%0Aperformance%20across%20perception%2C%20prediction%2C%20planning%2C%20and%20question-answering%0Atasks%20by%20enabling%20reasoning%20capabilities%20beyond%20visual%20range%20and%20improving%0Ageneralization%20to%20diverse%20driving%20scenarios.%20This%20work%20represents%20a%20significant%0Astep%20toward%20more%20comprehensive%20autonomous%20driving%20systems%20capable%20of%20navigating%0Acomplex%2C%20unfamiliar%20environments%20with%20greater%20reliability%20and%20safety.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavigScene%253A%2520Bridging%2520Local%2520Perception%2520and%2520Global%2520Navigation%2520for%250A%2520%2520Beyond-Visual-Range%2520Autonomous%2520Driving%26entry.906535625%3DQucheng%2520Peng%2520and%2520Chen%2520Bai%2520and%2520Guoxiang%2520Zhang%2520and%2520Bo%2520Xu%2520and%2520Xiaotong%2520Liu%2520and%2520Xiaoyin%2520Zheng%2520and%2520Chen%2520Chen%2520and%2520Cheng%2520Lu%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520systems%2520have%2520made%2520significant%2520advances%2520in%2520Q%2526A%252C%2520perception%252C%250Aprediction%252C%2520and%2520planning%2520based%2520on%2520local%2520visual%2520information%252C%2520yet%2520they%2520struggle%250Ato%2520incorporate%2520broader%2520navigational%2520context%2520that%2520human%2520drivers%2520routinely%250Autilize.%2520We%2520address%2520this%2520critical%2520gap%2520between%2520local%2520sensor%2520data%2520and%2520global%250Anavigation%2520information%2520by%2520proposing%2520NavigScene%252C%2520an%2520auxiliary%2520navigation-guided%250Anatural%2520language%2520dataset%2520that%2520simulates%2520a%2520human-like%2520driving%2520environment%2520within%250Aautonomous%2520driving%2520systems.%2520Moreover%252C%2520we%2520develop%2520three%2520complementary%2520paradigms%250Ato%2520leverage%2520NavigScene%253A%2520%25281%2529%2520Navigation-guided%2520Reasoning%252C%2520which%2520enhances%250Avision-language%2520models%2520by%2520incorporating%2520navigation%2520context%2520into%2520the%2520prompting%250Aapproach%253B%2520%25282%2529%2520Navigation-guided%2520Preference%2520Optimization%252C%2520a%2520reinforcement%250Alearning%2520method%2520that%2520extends%2520Direct%2520Preference%2520Optimization%2520to%2520improve%250Avision-language%2520model%2520responses%2520by%2520establishing%2520preferences%2520for%250Anavigation-relevant%2520summarized%2520information%253B%2520and%2520%25283%2529%2520Navigation-guided%250AVision-Language-Action%2520model%252C%2520which%2520integrates%2520navigation%2520guidance%2520and%250Avision-language%2520models%2520with%2520conventional%2520driving%2520models%2520through%2520feature%2520fusion.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520approaches%2520significantly%2520improve%250Aperformance%2520across%2520perception%252C%2520prediction%252C%2520planning%252C%2520and%2520question-answering%250Atasks%2520by%2520enabling%2520reasoning%2520capabilities%2520beyond%2520visual%2520range%2520and%2520improving%250Ageneralization%2520to%2520diverse%2520driving%2520scenarios.%2520This%2520work%2520represents%2520a%2520significant%250Astep%2520toward%2520more%2520comprehensive%2520autonomous%2520driving%2520systems%2520capable%2520of%2520navigating%250Acomplex%252C%2520unfamiliar%2520environments%2520with%2520greater%2520reliability%2520and%2520safety.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NavigScene%3A%20Bridging%20Local%20Perception%20and%20Global%20Navigation%20for%0A%20%20Beyond-Visual-Range%20Autonomous%20Driving&entry.906535625=Qucheng%20Peng%20and%20Chen%20Bai%20and%20Guoxiang%20Zhang%20and%20Bo%20Xu%20and%20Xiaotong%20Liu%20and%20Xiaoyin%20Zheng%20and%20Chen%20Chen%20and%20Cheng%20Lu&entry.1292438233=%20%20Autonomous%20driving%20systems%20have%20made%20significant%20advances%20in%20Q%26A%2C%20perception%2C%0Aprediction%2C%20and%20planning%20based%20on%20local%20visual%20information%2C%20yet%20they%20struggle%0Ato%20incorporate%20broader%20navigational%20context%20that%20human%20drivers%20routinely%0Autilize.%20We%20address%20this%20critical%20gap%20between%20local%20sensor%20data%20and%20global%0Anavigation%20information%20by%20proposing%20NavigScene%2C%20an%20auxiliary%20navigation-guided%0Anatural%20language%20dataset%20that%20simulates%20a%20human-like%20driving%20environment%20within%0Aautonomous%20driving%20systems.%20Moreover%2C%20we%20develop%20three%20complementary%20paradigms%0Ato%20leverage%20NavigScene%3A%20%281%29%20Navigation-guided%20Reasoning%2C%20which%20enhances%0Avision-language%20models%20by%20incorporating%20navigation%20context%20into%20the%20prompting%0Aapproach%3B%20%282%29%20Navigation-guided%20Preference%20Optimization%2C%20a%20reinforcement%0Alearning%20method%20that%20extends%20Direct%20Preference%20Optimization%20to%20improve%0Avision-language%20model%20responses%20by%20establishing%20preferences%20for%0Anavigation-relevant%20summarized%20information%3B%20and%20%283%29%20Navigation-guided%0AVision-Language-Action%20model%2C%20which%20integrates%20navigation%20guidance%20and%0Avision-language%20models%20with%20conventional%20driving%20models%20through%20feature%20fusion.%0AExtensive%20experiments%20demonstrate%20that%20our%20approaches%20significantly%20improve%0Aperformance%20across%20perception%2C%20prediction%2C%20planning%2C%20and%20question-answering%0Atasks%20by%20enabling%20reasoning%20capabilities%20beyond%20visual%20range%20and%20improving%0Ageneralization%20to%20diverse%20driving%20scenarios.%20This%20work%20represents%20a%20significant%0Astep%20toward%20more%20comprehensive%20autonomous%20driving%20systems%20capable%20of%20navigating%0Acomplex%2C%20unfamiliar%20environments%20with%20greater%20reliability%20and%20safety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05227v1&entry.124074799=Read"},
{"title": "Semantically Consistent Discrete Diffusion for 3D Biological Graph\n  Modeling", "author": "Chinmay Prabhakar and Suprosanna Shit and Tamaz Amiranashvili and Hongwei Bran Li and Bjoern Menze", "abstract": "  3D spatial graphs play a crucial role in biological and clinical research by\nmodeling anatomical networks such as blood vessels,neurons, and airways.\nHowever, generating 3D biological graphs while maintaining anatomical validity\nremains challenging, a key limitation of existing diffusion-based methods. In\nthis work, we propose a novel 3D biological graph generation method that\nadheres to structural and semantic plausibility conditions. We achieve this by\nusing a novel projection operator during sampling that stochastically fixes\ninconsistencies. Further, we adopt a superior edge-deletion-based noising\nprocedure suitable for sparse biological graphs. Our method demonstrates\nsuperior performance on two real-world datasets, human circle of Willis and\nlung airways, compared to previous approaches. Importantly, we demonstrate that\nthe generated samples significantly enhance downstream graph labeling\nperformance. Furthermore, we show that our generative model is a reasonable\nout-of-the-box link predictior.\n", "link": "http://arxiv.org/abs/2507.04856v1", "date": "2025-07-07", "relevancy": 3.025, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6287}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5932}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantically%20Consistent%20Discrete%20Diffusion%20for%203D%20Biological%20Graph%0A%20%20Modeling&body=Title%3A%20Semantically%20Consistent%20Discrete%20Diffusion%20for%203D%20Biological%20Graph%0A%20%20Modeling%0AAuthor%3A%20Chinmay%20Prabhakar%20and%20Suprosanna%20Shit%20and%20Tamaz%20Amiranashvili%20and%20Hongwei%20Bran%20Li%20and%20Bjoern%20Menze%0AAbstract%3A%20%20%203D%20spatial%20graphs%20play%20a%20crucial%20role%20in%20biological%20and%20clinical%20research%20by%0Amodeling%20anatomical%20networks%20such%20as%20blood%20vessels%2Cneurons%2C%20and%20airways.%0AHowever%2C%20generating%203D%20biological%20graphs%20while%20maintaining%20anatomical%20validity%0Aremains%20challenging%2C%20a%20key%20limitation%20of%20existing%20diffusion-based%20methods.%20In%0Athis%20work%2C%20we%20propose%20a%20novel%203D%20biological%20graph%20generation%20method%20that%0Aadheres%20to%20structural%20and%20semantic%20plausibility%20conditions.%20We%20achieve%20this%20by%0Ausing%20a%20novel%20projection%20operator%20during%20sampling%20that%20stochastically%20fixes%0Ainconsistencies.%20Further%2C%20we%20adopt%20a%20superior%20edge-deletion-based%20noising%0Aprocedure%20suitable%20for%20sparse%20biological%20graphs.%20Our%20method%20demonstrates%0Asuperior%20performance%20on%20two%20real-world%20datasets%2C%20human%20circle%20of%20Willis%20and%0Alung%20airways%2C%20compared%20to%20previous%20approaches.%20Importantly%2C%20we%20demonstrate%20that%0Athe%20generated%20samples%20significantly%20enhance%20downstream%20graph%20labeling%0Aperformance.%20Furthermore%2C%20we%20show%20that%20our%20generative%20model%20is%20a%20reasonable%0Aout-of-the-box%20link%20predictior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04856v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantically%2520Consistent%2520Discrete%2520Diffusion%2520for%25203D%2520Biological%2520Graph%250A%2520%2520Modeling%26entry.906535625%3DChinmay%2520Prabhakar%2520and%2520Suprosanna%2520Shit%2520and%2520Tamaz%2520Amiranashvili%2520and%2520Hongwei%2520Bran%2520Li%2520and%2520Bjoern%2520Menze%26entry.1292438233%3D%2520%25203D%2520spatial%2520graphs%2520play%2520a%2520crucial%2520role%2520in%2520biological%2520and%2520clinical%2520research%2520by%250Amodeling%2520anatomical%2520networks%2520such%2520as%2520blood%2520vessels%252Cneurons%252C%2520and%2520airways.%250AHowever%252C%2520generating%25203D%2520biological%2520graphs%2520while%2520maintaining%2520anatomical%2520validity%250Aremains%2520challenging%252C%2520a%2520key%2520limitation%2520of%2520existing%2520diffusion-based%2520methods.%2520In%250Athis%2520work%252C%2520we%2520propose%2520a%2520novel%25203D%2520biological%2520graph%2520generation%2520method%2520that%250Aadheres%2520to%2520structural%2520and%2520semantic%2520plausibility%2520conditions.%2520We%2520achieve%2520this%2520by%250Ausing%2520a%2520novel%2520projection%2520operator%2520during%2520sampling%2520that%2520stochastically%2520fixes%250Ainconsistencies.%2520Further%252C%2520we%2520adopt%2520a%2520superior%2520edge-deletion-based%2520noising%250Aprocedure%2520suitable%2520for%2520sparse%2520biological%2520graphs.%2520Our%2520method%2520demonstrates%250Asuperior%2520performance%2520on%2520two%2520real-world%2520datasets%252C%2520human%2520circle%2520of%2520Willis%2520and%250Alung%2520airways%252C%2520compared%2520to%2520previous%2520approaches.%2520Importantly%252C%2520we%2520demonstrate%2520that%250Athe%2520generated%2520samples%2520significantly%2520enhance%2520downstream%2520graph%2520labeling%250Aperformance.%2520Furthermore%252C%2520we%2520show%2520that%2520our%2520generative%2520model%2520is%2520a%2520reasonable%250Aout-of-the-box%2520link%2520predictior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04856v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantically%20Consistent%20Discrete%20Diffusion%20for%203D%20Biological%20Graph%0A%20%20Modeling&entry.906535625=Chinmay%20Prabhakar%20and%20Suprosanna%20Shit%20and%20Tamaz%20Amiranashvili%20and%20Hongwei%20Bran%20Li%20and%20Bjoern%20Menze&entry.1292438233=%20%203D%20spatial%20graphs%20play%20a%20crucial%20role%20in%20biological%20and%20clinical%20research%20by%0Amodeling%20anatomical%20networks%20such%20as%20blood%20vessels%2Cneurons%2C%20and%20airways.%0AHowever%2C%20generating%203D%20biological%20graphs%20while%20maintaining%20anatomical%20validity%0Aremains%20challenging%2C%20a%20key%20limitation%20of%20existing%20diffusion-based%20methods.%20In%0Athis%20work%2C%20we%20propose%20a%20novel%203D%20biological%20graph%20generation%20method%20that%0Aadheres%20to%20structural%20and%20semantic%20plausibility%20conditions.%20We%20achieve%20this%20by%0Ausing%20a%20novel%20projection%20operator%20during%20sampling%20that%20stochastically%20fixes%0Ainconsistencies.%20Further%2C%20we%20adopt%20a%20superior%20edge-deletion-based%20noising%0Aprocedure%20suitable%20for%20sparse%20biological%20graphs.%20Our%20method%20demonstrates%0Asuperior%20performance%20on%20two%20real-world%20datasets%2C%20human%20circle%20of%20Willis%20and%0Alung%20airways%2C%20compared%20to%20previous%20approaches.%20Importantly%2C%20we%20demonstrate%20that%0Athe%20generated%20samples%20significantly%20enhance%20downstream%20graph%20labeling%0Aperformance.%20Furthermore%2C%20we%20show%20that%20our%20generative%20model%20is%20a%20reasonable%0Aout-of-the-box%20link%20predictior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04856v1&entry.124074799=Read"},
{"title": "Beyond One Shot, Beyond One Perspective: Cross-View and Long-Horizon\n  Distillation for Better LiDAR Representations", "author": "Xiang Xu and Lingdong Kong and Song Wang and Chuanwei Zhou and Qingshan Liu", "abstract": "  LiDAR representation learning aims to extract rich structural and semantic\ninformation from large-scale, readily available datasets, reducing reliance on\ncostly human annotations. However, existing LiDAR representation strategies\noften overlook the inherent spatiotemporal cues in LiDAR sequences, limiting\ntheir effectiveness. In this work, we propose LiMA, a novel long-term\nimage-to-LiDAR Memory Aggregation framework that explicitly captures longer\nrange temporal correlations to enhance LiDAR representation learning. LiMA\ncomprises three key components: 1) a Cross-View Aggregation module that aligns\nand fuses overlapping regions across neighboring camera views, constructing a\nmore unified and redundancy-free memory bank; 2) a Long-Term Feature\nPropagation mechanism that efficiently aligns and integrates multi-frame image\nfeatures, reinforcing temporal coherence during LiDAR representation learning;\nand 3) a Cross-Sequence Memory Alignment strategy that enforces consistency\nacross driving sequences, improving generalization to unseen environments. LiMA\nmaintains high pretraining efficiency and incurs no additional computational\noverhead during downstream tasks. Extensive experiments on mainstream\nLiDAR-based perception benchmarks demonstrate that LiMA significantly improves\nboth LiDAR semantic segmentation and 3D object detection. We hope this work\ninspires more effective pretraining paradigms for autonomous driving. The code\nhas be made publicly accessible for future research.\n", "link": "http://arxiv.org/abs/2507.05260v1", "date": "2025-07-07", "relevancy": 2.9831, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.608}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6039}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20One%20Shot%2C%20Beyond%20One%20Perspective%3A%20Cross-View%20and%20Long-Horizon%0A%20%20Distillation%20for%20Better%20LiDAR%20Representations&body=Title%3A%20Beyond%20One%20Shot%2C%20Beyond%20One%20Perspective%3A%20Cross-View%20and%20Long-Horizon%0A%20%20Distillation%20for%20Better%20LiDAR%20Representations%0AAuthor%3A%20Xiang%20Xu%20and%20Lingdong%20Kong%20and%20Song%20Wang%20and%20Chuanwei%20Zhou%20and%20Qingshan%20Liu%0AAbstract%3A%20%20%20LiDAR%20representation%20learning%20aims%20to%20extract%20rich%20structural%20and%20semantic%0Ainformation%20from%20large-scale%2C%20readily%20available%20datasets%2C%20reducing%20reliance%20on%0Acostly%20human%20annotations.%20However%2C%20existing%20LiDAR%20representation%20strategies%0Aoften%20overlook%20the%20inherent%20spatiotemporal%20cues%20in%20LiDAR%20sequences%2C%20limiting%0Atheir%20effectiveness.%20In%20this%20work%2C%20we%20propose%20LiMA%2C%20a%20novel%20long-term%0Aimage-to-LiDAR%20Memory%20Aggregation%20framework%20that%20explicitly%20captures%20longer%0Arange%20temporal%20correlations%20to%20enhance%20LiDAR%20representation%20learning.%20LiMA%0Acomprises%20three%20key%20components%3A%201%29%20a%20Cross-View%20Aggregation%20module%20that%20aligns%0Aand%20fuses%20overlapping%20regions%20across%20neighboring%20camera%20views%2C%20constructing%20a%0Amore%20unified%20and%20redundancy-free%20memory%20bank%3B%202%29%20a%20Long-Term%20Feature%0APropagation%20mechanism%20that%20efficiently%20aligns%20and%20integrates%20multi-frame%20image%0Afeatures%2C%20reinforcing%20temporal%20coherence%20during%20LiDAR%20representation%20learning%3B%0Aand%203%29%20a%20Cross-Sequence%20Memory%20Alignment%20strategy%20that%20enforces%20consistency%0Aacross%20driving%20sequences%2C%20improving%20generalization%20to%20unseen%20environments.%20LiMA%0Amaintains%20high%20pretraining%20efficiency%20and%20incurs%20no%20additional%20computational%0Aoverhead%20during%20downstream%20tasks.%20Extensive%20experiments%20on%20mainstream%0ALiDAR-based%20perception%20benchmarks%20demonstrate%20that%20LiMA%20significantly%20improves%0Aboth%20LiDAR%20semantic%20segmentation%20and%203D%20object%20detection.%20We%20hope%20this%20work%0Ainspires%20more%20effective%20pretraining%20paradigms%20for%20autonomous%20driving.%20The%20code%0Ahas%20be%20made%20publicly%20accessible%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05260v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520One%2520Shot%252C%2520Beyond%2520One%2520Perspective%253A%2520Cross-View%2520and%2520Long-Horizon%250A%2520%2520Distillation%2520for%2520Better%2520LiDAR%2520Representations%26entry.906535625%3DXiang%2520Xu%2520and%2520Lingdong%2520Kong%2520and%2520Song%2520Wang%2520and%2520Chuanwei%2520Zhou%2520and%2520Qingshan%2520Liu%26entry.1292438233%3D%2520%2520LiDAR%2520representation%2520learning%2520aims%2520to%2520extract%2520rich%2520structural%2520and%2520semantic%250Ainformation%2520from%2520large-scale%252C%2520readily%2520available%2520datasets%252C%2520reducing%2520reliance%2520on%250Acostly%2520human%2520annotations.%2520However%252C%2520existing%2520LiDAR%2520representation%2520strategies%250Aoften%2520overlook%2520the%2520inherent%2520spatiotemporal%2520cues%2520in%2520LiDAR%2520sequences%252C%2520limiting%250Atheir%2520effectiveness.%2520In%2520this%2520work%252C%2520we%2520propose%2520LiMA%252C%2520a%2520novel%2520long-term%250Aimage-to-LiDAR%2520Memory%2520Aggregation%2520framework%2520that%2520explicitly%2520captures%2520longer%250Arange%2520temporal%2520correlations%2520to%2520enhance%2520LiDAR%2520representation%2520learning.%2520LiMA%250Acomprises%2520three%2520key%2520components%253A%25201%2529%2520a%2520Cross-View%2520Aggregation%2520module%2520that%2520aligns%250Aand%2520fuses%2520overlapping%2520regions%2520across%2520neighboring%2520camera%2520views%252C%2520constructing%2520a%250Amore%2520unified%2520and%2520redundancy-free%2520memory%2520bank%253B%25202%2529%2520a%2520Long-Term%2520Feature%250APropagation%2520mechanism%2520that%2520efficiently%2520aligns%2520and%2520integrates%2520multi-frame%2520image%250Afeatures%252C%2520reinforcing%2520temporal%2520coherence%2520during%2520LiDAR%2520representation%2520learning%253B%250Aand%25203%2529%2520a%2520Cross-Sequence%2520Memory%2520Alignment%2520strategy%2520that%2520enforces%2520consistency%250Aacross%2520driving%2520sequences%252C%2520improving%2520generalization%2520to%2520unseen%2520environments.%2520LiMA%250Amaintains%2520high%2520pretraining%2520efficiency%2520and%2520incurs%2520no%2520additional%2520computational%250Aoverhead%2520during%2520downstream%2520tasks.%2520Extensive%2520experiments%2520on%2520mainstream%250ALiDAR-based%2520perception%2520benchmarks%2520demonstrate%2520that%2520LiMA%2520significantly%2520improves%250Aboth%2520LiDAR%2520semantic%2520segmentation%2520and%25203D%2520object%2520detection.%2520We%2520hope%2520this%2520work%250Ainspires%2520more%2520effective%2520pretraining%2520paradigms%2520for%2520autonomous%2520driving.%2520The%2520code%250Ahas%2520be%2520made%2520publicly%2520accessible%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05260v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20One%20Shot%2C%20Beyond%20One%20Perspective%3A%20Cross-View%20and%20Long-Horizon%0A%20%20Distillation%20for%20Better%20LiDAR%20Representations&entry.906535625=Xiang%20Xu%20and%20Lingdong%20Kong%20and%20Song%20Wang%20and%20Chuanwei%20Zhou%20and%20Qingshan%20Liu&entry.1292438233=%20%20LiDAR%20representation%20learning%20aims%20to%20extract%20rich%20structural%20and%20semantic%0Ainformation%20from%20large-scale%2C%20readily%20available%20datasets%2C%20reducing%20reliance%20on%0Acostly%20human%20annotations.%20However%2C%20existing%20LiDAR%20representation%20strategies%0Aoften%20overlook%20the%20inherent%20spatiotemporal%20cues%20in%20LiDAR%20sequences%2C%20limiting%0Atheir%20effectiveness.%20In%20this%20work%2C%20we%20propose%20LiMA%2C%20a%20novel%20long-term%0Aimage-to-LiDAR%20Memory%20Aggregation%20framework%20that%20explicitly%20captures%20longer%0Arange%20temporal%20correlations%20to%20enhance%20LiDAR%20representation%20learning.%20LiMA%0Acomprises%20three%20key%20components%3A%201%29%20a%20Cross-View%20Aggregation%20module%20that%20aligns%0Aand%20fuses%20overlapping%20regions%20across%20neighboring%20camera%20views%2C%20constructing%20a%0Amore%20unified%20and%20redundancy-free%20memory%20bank%3B%202%29%20a%20Long-Term%20Feature%0APropagation%20mechanism%20that%20efficiently%20aligns%20and%20integrates%20multi-frame%20image%0Afeatures%2C%20reinforcing%20temporal%20coherence%20during%20LiDAR%20representation%20learning%3B%0Aand%203%29%20a%20Cross-Sequence%20Memory%20Alignment%20strategy%20that%20enforces%20consistency%0Aacross%20driving%20sequences%2C%20improving%20generalization%20to%20unseen%20environments.%20LiMA%0Amaintains%20high%20pretraining%20efficiency%20and%20incurs%20no%20additional%20computational%0Aoverhead%20during%20downstream%20tasks.%20Extensive%20experiments%20on%20mainstream%0ALiDAR-based%20perception%20benchmarks%20demonstrate%20that%20LiMA%20significantly%20improves%0Aboth%20LiDAR%20semantic%20segmentation%20and%203D%20object%20detection.%20We%20hope%20this%20work%0Ainspires%20more%20effective%20pretraining%20paradigms%20for%20autonomous%20driving.%20The%20code%0Ahas%20be%20made%20publicly%20accessible%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05260v1&entry.124074799=Read"},
{"title": "PEVLM: Parallel Encoding for Vision-Language Models", "author": "Letian Kang and Shixian Luo and Yiqiang Li and Xiaoyang Yu and Shenxuan Zhou and Yong Wu", "abstract": "  Vision-Language Models (VLMs) have demonstrated strong capabilities in\nmultimodal understanding and generation tasks. However, their application to\nlong video understanding remains hindered by the quadratic complexity of\nstandard attention mechanisms. In this work, we introduce \\textbf{PEVLM}, a\nfine-tuning-free parallel encoding method designed to enhance the prefilling\nefficiency of VLMs in long video scenarios. PEVLM partitions the input video\ninto context blocks with a shared sink block, while preserving sequential\nposition embeddings to align the attention weight distribution with that of\nFull-Attention. This design reduces attention complexity from $O((T \\times\nN)^2)$ to $O(T \\times N)$ where $T$ is the number of frames and $N$ the number\nof tokens per frame, without sacrificing accuracy. Extensive experiments across\nmultiple state-of-the-art models and benchmarks demonstrate that PEVLM\nconsistently outperforms existing parallel encoding approaches, achieving up to\n\\textbf{7.47x} speedup in attention computation and reducing end-to-end latency\nby \\textbf{40\\%}. Remarkably, PEVLM not only maintains high accuracy, but in\nsome settings even surpasses Full-Attention performance. Under strict latency\nconstraints, it achieves substantial gains, improving accuracy from\n\\textbf{23.26\\%} to \\textbf{61.03\\%}. These results underscore the\neffectiveness of PEVLM for low-latency, long-context video understanding,\nmaking it a promising solution for real-world applications.\n", "link": "http://arxiv.org/abs/2506.19651v2", "date": "2025-07-07", "relevancy": 2.9726, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6059}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6059}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PEVLM%3A%20Parallel%20Encoding%20for%20Vision-Language%20Models&body=Title%3A%20PEVLM%3A%20Parallel%20Encoding%20for%20Vision-Language%20Models%0AAuthor%3A%20Letian%20Kang%20and%20Shixian%20Luo%20and%20Yiqiang%20Li%20and%20Xiaoyang%20Yu%20and%20Shenxuan%20Zhou%20and%20Yong%20Wu%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20strong%20capabilities%20in%0Amultimodal%20understanding%20and%20generation%20tasks.%20However%2C%20their%20application%20to%0Along%20video%20understanding%20remains%20hindered%20by%20the%20quadratic%20complexity%20of%0Astandard%20attention%20mechanisms.%20In%20this%20work%2C%20we%20introduce%20%5Ctextbf%7BPEVLM%7D%2C%20a%0Afine-tuning-free%20parallel%20encoding%20method%20designed%20to%20enhance%20the%20prefilling%0Aefficiency%20of%20VLMs%20in%20long%20video%20scenarios.%20PEVLM%20partitions%20the%20input%20video%0Ainto%20context%20blocks%20with%20a%20shared%20sink%20block%2C%20while%20preserving%20sequential%0Aposition%20embeddings%20to%20align%20the%20attention%20weight%20distribution%20with%20that%20of%0AFull-Attention.%20This%20design%20reduces%20attention%20complexity%20from%20%24O%28%28T%20%5Ctimes%0AN%29%5E2%29%24%20to%20%24O%28T%20%5Ctimes%20N%29%24%20where%20%24T%24%20is%20the%20number%20of%20frames%20and%20%24N%24%20the%20number%0Aof%20tokens%20per%20frame%2C%20without%20sacrificing%20accuracy.%20Extensive%20experiments%20across%0Amultiple%20state-of-the-art%20models%20and%20benchmarks%20demonstrate%20that%20PEVLM%0Aconsistently%20outperforms%20existing%20parallel%20encoding%20approaches%2C%20achieving%20up%20to%0A%5Ctextbf%7B7.47x%7D%20speedup%20in%20attention%20computation%20and%20reducing%20end-to-end%20latency%0Aby%20%5Ctextbf%7B40%5C%25%7D.%20Remarkably%2C%20PEVLM%20not%20only%20maintains%20high%20accuracy%2C%20but%20in%0Asome%20settings%20even%20surpasses%20Full-Attention%20performance.%20Under%20strict%20latency%0Aconstraints%2C%20it%20achieves%20substantial%20gains%2C%20improving%20accuracy%20from%0A%5Ctextbf%7B23.26%5C%25%7D%20to%20%5Ctextbf%7B61.03%5C%25%7D.%20These%20results%20underscore%20the%0Aeffectiveness%20of%20PEVLM%20for%20low-latency%2C%20long-context%20video%20understanding%2C%0Amaking%20it%20a%20promising%20solution%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.19651v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPEVLM%253A%2520Parallel%2520Encoding%2520for%2520Vision-Language%2520Models%26entry.906535625%3DLetian%2520Kang%2520and%2520Shixian%2520Luo%2520and%2520Yiqiang%2520Li%2520and%2520Xiaoyang%2520Yu%2520and%2520Shenxuan%2520Zhou%2520and%2520Yong%2520Wu%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%2520strong%2520capabilities%2520in%250Amultimodal%2520understanding%2520and%2520generation%2520tasks.%2520However%252C%2520their%2520application%2520to%250Along%2520video%2520understanding%2520remains%2520hindered%2520by%2520the%2520quadratic%2520complexity%2520of%250Astandard%2520attention%2520mechanisms.%2520In%2520this%2520work%252C%2520we%2520introduce%2520%255Ctextbf%257BPEVLM%257D%252C%2520a%250Afine-tuning-free%2520parallel%2520encoding%2520method%2520designed%2520to%2520enhance%2520the%2520prefilling%250Aefficiency%2520of%2520VLMs%2520in%2520long%2520video%2520scenarios.%2520PEVLM%2520partitions%2520the%2520input%2520video%250Ainto%2520context%2520blocks%2520with%2520a%2520shared%2520sink%2520block%252C%2520while%2520preserving%2520sequential%250Aposition%2520embeddings%2520to%2520align%2520the%2520attention%2520weight%2520distribution%2520with%2520that%2520of%250AFull-Attention.%2520This%2520design%2520reduces%2520attention%2520complexity%2520from%2520%2524O%2528%2528T%2520%255Ctimes%250AN%2529%255E2%2529%2524%2520to%2520%2524O%2528T%2520%255Ctimes%2520N%2529%2524%2520where%2520%2524T%2524%2520is%2520the%2520number%2520of%2520frames%2520and%2520%2524N%2524%2520the%2520number%250Aof%2520tokens%2520per%2520frame%252C%2520without%2520sacrificing%2520accuracy.%2520Extensive%2520experiments%2520across%250Amultiple%2520state-of-the-art%2520models%2520and%2520benchmarks%2520demonstrate%2520that%2520PEVLM%250Aconsistently%2520outperforms%2520existing%2520parallel%2520encoding%2520approaches%252C%2520achieving%2520up%2520to%250A%255Ctextbf%257B7.47x%257D%2520speedup%2520in%2520attention%2520computation%2520and%2520reducing%2520end-to-end%2520latency%250Aby%2520%255Ctextbf%257B40%255C%2525%257D.%2520Remarkably%252C%2520PEVLM%2520not%2520only%2520maintains%2520high%2520accuracy%252C%2520but%2520in%250Asome%2520settings%2520even%2520surpasses%2520Full-Attention%2520performance.%2520Under%2520strict%2520latency%250Aconstraints%252C%2520it%2520achieves%2520substantial%2520gains%252C%2520improving%2520accuracy%2520from%250A%255Ctextbf%257B23.26%255C%2525%257D%2520to%2520%255Ctextbf%257B61.03%255C%2525%257D.%2520These%2520results%2520underscore%2520the%250Aeffectiveness%2520of%2520PEVLM%2520for%2520low-latency%252C%2520long-context%2520video%2520understanding%252C%250Amaking%2520it%2520a%2520promising%2520solution%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19651v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PEVLM%3A%20Parallel%20Encoding%20for%20Vision-Language%20Models&entry.906535625=Letian%20Kang%20and%20Shixian%20Luo%20and%20Yiqiang%20Li%20and%20Xiaoyang%20Yu%20and%20Shenxuan%20Zhou%20and%20Yong%20Wu&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20strong%20capabilities%20in%0Amultimodal%20understanding%20and%20generation%20tasks.%20However%2C%20their%20application%20to%0Along%20video%20understanding%20remains%20hindered%20by%20the%20quadratic%20complexity%20of%0Astandard%20attention%20mechanisms.%20In%20this%20work%2C%20we%20introduce%20%5Ctextbf%7BPEVLM%7D%2C%20a%0Afine-tuning-free%20parallel%20encoding%20method%20designed%20to%20enhance%20the%20prefilling%0Aefficiency%20of%20VLMs%20in%20long%20video%20scenarios.%20PEVLM%20partitions%20the%20input%20video%0Ainto%20context%20blocks%20with%20a%20shared%20sink%20block%2C%20while%20preserving%20sequential%0Aposition%20embeddings%20to%20align%20the%20attention%20weight%20distribution%20with%20that%20of%0AFull-Attention.%20This%20design%20reduces%20attention%20complexity%20from%20%24O%28%28T%20%5Ctimes%0AN%29%5E2%29%24%20to%20%24O%28T%20%5Ctimes%20N%29%24%20where%20%24T%24%20is%20the%20number%20of%20frames%20and%20%24N%24%20the%20number%0Aof%20tokens%20per%20frame%2C%20without%20sacrificing%20accuracy.%20Extensive%20experiments%20across%0Amultiple%20state-of-the-art%20models%20and%20benchmarks%20demonstrate%20that%20PEVLM%0Aconsistently%20outperforms%20existing%20parallel%20encoding%20approaches%2C%20achieving%20up%20to%0A%5Ctextbf%7B7.47x%7D%20speedup%20in%20attention%20computation%20and%20reducing%20end-to-end%20latency%0Aby%20%5Ctextbf%7B40%5C%25%7D.%20Remarkably%2C%20PEVLM%20not%20only%20maintains%20high%20accuracy%2C%20but%20in%0Asome%20settings%20even%20surpasses%20Full-Attention%20performance.%20Under%20strict%20latency%0Aconstraints%2C%20it%20achieves%20substantial%20gains%2C%20improving%20accuracy%20from%0A%5Ctextbf%7B23.26%5C%25%7D%20to%20%5Ctextbf%7B61.03%5C%25%7D.%20These%20results%20underscore%20the%0Aeffectiveness%20of%20PEVLM%20for%20low-latency%2C%20long-context%20video%20understanding%2C%0Amaking%20it%20a%20promising%20solution%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.19651v2&entry.124074799=Read"},
{"title": "SegmentDreamer: Towards High-fidelity Text-to-3D Synthesis with\n  Segmented Consistency Trajectory Distillation", "author": "Jiahao Zhu and Zixuan Chen and Guangcong Wang and Xiaohua Xie and Yi Zhou", "abstract": "  Recent advancements in text-to-3D generation improve the visual quality of\nScore Distillation Sampling (SDS) and its variants by directly connecting\nConsistency Distillation (CD) to score distillation. However, due to the\nimbalance between self-consistency and cross-consistency, these CD-based\nmethods inherently suffer from improper conditional guidance, leading to\nsub-optimal generation results. To address this issue, we present\nSegmentDreamer, a novel framework designed to fully unleash the potential of\nconsistency models for high-fidelity text-to-3D generation. Specifically, we\nreformulate SDS through the proposed Segmented Consistency Trajectory\nDistillation (SCTD), effectively mitigating the imbalance issues by explicitly\ndefining the relationship between self- and cross-consistency. Moreover, SCTD\npartitions the Probability Flow Ordinary Differential Equation (PF-ODE)\ntrajectory into multiple sub-trajectories and ensures consistency within each\nsegment, which can theoretically provide a significantly tighter upper bound on\ndistillation error. Additionally, we propose a distillation pipeline for a more\nswift and stable generation. Extensive experiments demonstrate that our\nSegmentDreamer outperforms state-of-the-art methods in visual quality, enabling\nhigh-fidelity 3D asset creation through 3D Gaussian Splatting (3DGS).\n", "link": "http://arxiv.org/abs/2507.05256v1", "date": "2025-07-07", "relevancy": 2.9067, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5983}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5729}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SegmentDreamer%3A%20Towards%20High-fidelity%20Text-to-3D%20Synthesis%20with%0A%20%20Segmented%20Consistency%20Trajectory%20Distillation&body=Title%3A%20SegmentDreamer%3A%20Towards%20High-fidelity%20Text-to-3D%20Synthesis%20with%0A%20%20Segmented%20Consistency%20Trajectory%20Distillation%0AAuthor%3A%20Jiahao%20Zhu%20and%20Zixuan%20Chen%20and%20Guangcong%20Wang%20and%20Xiaohua%20Xie%20and%20Yi%20Zhou%0AAbstract%3A%20%20%20Recent%20advancements%20in%20text-to-3D%20generation%20improve%20the%20visual%20quality%20of%0AScore%20Distillation%20Sampling%20%28SDS%29%20and%20its%20variants%20by%20directly%20connecting%0AConsistency%20Distillation%20%28CD%29%20to%20score%20distillation.%20However%2C%20due%20to%20the%0Aimbalance%20between%20self-consistency%20and%20cross-consistency%2C%20these%20CD-based%0Amethods%20inherently%20suffer%20from%20improper%20conditional%20guidance%2C%20leading%20to%0Asub-optimal%20generation%20results.%20To%20address%20this%20issue%2C%20we%20present%0ASegmentDreamer%2C%20a%20novel%20framework%20designed%20to%20fully%20unleash%20the%20potential%20of%0Aconsistency%20models%20for%20high-fidelity%20text-to-3D%20generation.%20Specifically%2C%20we%0Areformulate%20SDS%20through%20the%20proposed%20Segmented%20Consistency%20Trajectory%0ADistillation%20%28SCTD%29%2C%20effectively%20mitigating%20the%20imbalance%20issues%20by%20explicitly%0Adefining%20the%20relationship%20between%20self-%20and%20cross-consistency.%20Moreover%2C%20SCTD%0Apartitions%20the%20Probability%20Flow%20Ordinary%20Differential%20Equation%20%28PF-ODE%29%0Atrajectory%20into%20multiple%20sub-trajectories%20and%20ensures%20consistency%20within%20each%0Asegment%2C%20which%20can%20theoretically%20provide%20a%20significantly%20tighter%20upper%20bound%20on%0Adistillation%20error.%20Additionally%2C%20we%20propose%20a%20distillation%20pipeline%20for%20a%20more%0Aswift%20and%20stable%20generation.%20Extensive%20experiments%20demonstrate%20that%20our%0ASegmentDreamer%20outperforms%20state-of-the-art%20methods%20in%20visual%20quality%2C%20enabling%0Ahigh-fidelity%203D%20asset%20creation%20through%203D%20Gaussian%20Splatting%20%283DGS%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05256v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegmentDreamer%253A%2520Towards%2520High-fidelity%2520Text-to-3D%2520Synthesis%2520with%250A%2520%2520Segmented%2520Consistency%2520Trajectory%2520Distillation%26entry.906535625%3DJiahao%2520Zhu%2520and%2520Zixuan%2520Chen%2520and%2520Guangcong%2520Wang%2520and%2520Xiaohua%2520Xie%2520and%2520Yi%2520Zhou%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520text-to-3D%2520generation%2520improve%2520the%2520visual%2520quality%2520of%250AScore%2520Distillation%2520Sampling%2520%2528SDS%2529%2520and%2520its%2520variants%2520by%2520directly%2520connecting%250AConsistency%2520Distillation%2520%2528CD%2529%2520to%2520score%2520distillation.%2520However%252C%2520due%2520to%2520the%250Aimbalance%2520between%2520self-consistency%2520and%2520cross-consistency%252C%2520these%2520CD-based%250Amethods%2520inherently%2520suffer%2520from%2520improper%2520conditional%2520guidance%252C%2520leading%2520to%250Asub-optimal%2520generation%2520results.%2520To%2520address%2520this%2520issue%252C%2520we%2520present%250ASegmentDreamer%252C%2520a%2520novel%2520framework%2520designed%2520to%2520fully%2520unleash%2520the%2520potential%2520of%250Aconsistency%2520models%2520for%2520high-fidelity%2520text-to-3D%2520generation.%2520Specifically%252C%2520we%250Areformulate%2520SDS%2520through%2520the%2520proposed%2520Segmented%2520Consistency%2520Trajectory%250ADistillation%2520%2528SCTD%2529%252C%2520effectively%2520mitigating%2520the%2520imbalance%2520issues%2520by%2520explicitly%250Adefining%2520the%2520relationship%2520between%2520self-%2520and%2520cross-consistency.%2520Moreover%252C%2520SCTD%250Apartitions%2520the%2520Probability%2520Flow%2520Ordinary%2520Differential%2520Equation%2520%2528PF-ODE%2529%250Atrajectory%2520into%2520multiple%2520sub-trajectories%2520and%2520ensures%2520consistency%2520within%2520each%250Asegment%252C%2520which%2520can%2520theoretically%2520provide%2520a%2520significantly%2520tighter%2520upper%2520bound%2520on%250Adistillation%2520error.%2520Additionally%252C%2520we%2520propose%2520a%2520distillation%2520pipeline%2520for%2520a%2520more%250Aswift%2520and%2520stable%2520generation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250ASegmentDreamer%2520outperforms%2520state-of-the-art%2520methods%2520in%2520visual%2520quality%252C%2520enabling%250Ahigh-fidelity%25203D%2520asset%2520creation%2520through%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05256v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SegmentDreamer%3A%20Towards%20High-fidelity%20Text-to-3D%20Synthesis%20with%0A%20%20Segmented%20Consistency%20Trajectory%20Distillation&entry.906535625=Jiahao%20Zhu%20and%20Zixuan%20Chen%20and%20Guangcong%20Wang%20and%20Xiaohua%20Xie%20and%20Yi%20Zhou&entry.1292438233=%20%20Recent%20advancements%20in%20text-to-3D%20generation%20improve%20the%20visual%20quality%20of%0AScore%20Distillation%20Sampling%20%28SDS%29%20and%20its%20variants%20by%20directly%20connecting%0AConsistency%20Distillation%20%28CD%29%20to%20score%20distillation.%20However%2C%20due%20to%20the%0Aimbalance%20between%20self-consistency%20and%20cross-consistency%2C%20these%20CD-based%0Amethods%20inherently%20suffer%20from%20improper%20conditional%20guidance%2C%20leading%20to%0Asub-optimal%20generation%20results.%20To%20address%20this%20issue%2C%20we%20present%0ASegmentDreamer%2C%20a%20novel%20framework%20designed%20to%20fully%20unleash%20the%20potential%20of%0Aconsistency%20models%20for%20high-fidelity%20text-to-3D%20generation.%20Specifically%2C%20we%0Areformulate%20SDS%20through%20the%20proposed%20Segmented%20Consistency%20Trajectory%0ADistillation%20%28SCTD%29%2C%20effectively%20mitigating%20the%20imbalance%20issues%20by%20explicitly%0Adefining%20the%20relationship%20between%20self-%20and%20cross-consistency.%20Moreover%2C%20SCTD%0Apartitions%20the%20Probability%20Flow%20Ordinary%20Differential%20Equation%20%28PF-ODE%29%0Atrajectory%20into%20multiple%20sub-trajectories%20and%20ensures%20consistency%20within%20each%0Asegment%2C%20which%20can%20theoretically%20provide%20a%20significantly%20tighter%20upper%20bound%20on%0Adistillation%20error.%20Additionally%2C%20we%20propose%20a%20distillation%20pipeline%20for%20a%20more%0Aswift%20and%20stable%20generation.%20Extensive%20experiments%20demonstrate%20that%20our%0ASegmentDreamer%20outperforms%20state-of-the-art%20methods%20in%20visual%20quality%2C%20enabling%0Ahigh-fidelity%203D%20asset%20creation%20through%203D%20Gaussian%20Splatting%20%283DGS%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05256v1&entry.124074799=Read"},
{"title": "Spatial and Semantic Embedding Integration for Stereo Sound Event\n  Localization and Detection in Regular Videos", "author": "Davide Berghi and Philip J. B. Jackson", "abstract": "  This report presents our systems submitted to the audio-only and audio-visual\ntracks of the DCASE2025 Task 3 Challenge: Stereo Sound Event Localization and\nDetection (SELD) in Regular Video Content. SELD is a complex task that combines\ntemporal event classification with spatial localization, requiring reasoning\nacross spatial, temporal, and semantic dimensions. The last is arguably the\nmost challenging to model. Traditional SELD architectures rely on multichannel\ninput, which limits their ability to leverage large-scale pre-training due to\ndata constraints. To address this, we enhance standard SELD architectures with\nsemantic information by integrating pre-trained, contrastive language-aligned\nmodels: CLAP for audio and OWL-ViT for visual inputs. These embeddings are\nincorporated into a modified Conformer module tailored for multimodal fusion,\nwhich we refer to as the Cross-Modal Conformer. Additionally, we incorporate\nautocorrelation-based acoustic features to improve distance estimation. We\npre-train our models on curated synthetic audio and audio-visual datasets and\napply a left-right channel swapping augmentation to further increase the\ntraining data. Both our audio-only and audio-visual systems substantially\noutperform the challenge baselines on the development set, demonstrating the\neffectiveness of our strategy. Performance is further improved through model\nensembling and a visual post-processing step based on human keypoints. Future\nwork will investigate the contribution of each modality and explore\narchitectural variants to further enhance results.\n", "link": "http://arxiv.org/abs/2507.04845v1", "date": "2025-07-07", "relevancy": 2.9007, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5887}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5887}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial%20and%20Semantic%20Embedding%20Integration%20for%20Stereo%20Sound%20Event%0A%20%20Localization%20and%20Detection%20in%20Regular%20Videos&body=Title%3A%20Spatial%20and%20Semantic%20Embedding%20Integration%20for%20Stereo%20Sound%20Event%0A%20%20Localization%20and%20Detection%20in%20Regular%20Videos%0AAuthor%3A%20Davide%20Berghi%20and%20Philip%20J.%20B.%20Jackson%0AAbstract%3A%20%20%20This%20report%20presents%20our%20systems%20submitted%20to%20the%20audio-only%20and%20audio-visual%0Atracks%20of%20the%20DCASE2025%20Task%203%20Challenge%3A%20Stereo%20Sound%20Event%20Localization%20and%0ADetection%20%28SELD%29%20in%20Regular%20Video%20Content.%20SELD%20is%20a%20complex%20task%20that%20combines%0Atemporal%20event%20classification%20with%20spatial%20localization%2C%20requiring%20reasoning%0Aacross%20spatial%2C%20temporal%2C%20and%20semantic%20dimensions.%20The%20last%20is%20arguably%20the%0Amost%20challenging%20to%20model.%20Traditional%20SELD%20architectures%20rely%20on%20multichannel%0Ainput%2C%20which%20limits%20their%20ability%20to%20leverage%20large-scale%20pre-training%20due%20to%0Adata%20constraints.%20To%20address%20this%2C%20we%20enhance%20standard%20SELD%20architectures%20with%0Asemantic%20information%20by%20integrating%20pre-trained%2C%20contrastive%20language-aligned%0Amodels%3A%20CLAP%20for%20audio%20and%20OWL-ViT%20for%20visual%20inputs.%20These%20embeddings%20are%0Aincorporated%20into%20a%20modified%20Conformer%20module%20tailored%20for%20multimodal%20fusion%2C%0Awhich%20we%20refer%20to%20as%20the%20Cross-Modal%20Conformer.%20Additionally%2C%20we%20incorporate%0Aautocorrelation-based%20acoustic%20features%20to%20improve%20distance%20estimation.%20We%0Apre-train%20our%20models%20on%20curated%20synthetic%20audio%20and%20audio-visual%20datasets%20and%0Aapply%20a%20left-right%20channel%20swapping%20augmentation%20to%20further%20increase%20the%0Atraining%20data.%20Both%20our%20audio-only%20and%20audio-visual%20systems%20substantially%0Aoutperform%20the%20challenge%20baselines%20on%20the%20development%20set%2C%20demonstrating%20the%0Aeffectiveness%20of%20our%20strategy.%20Performance%20is%20further%20improved%20through%20model%0Aensembling%20and%20a%20visual%20post-processing%20step%20based%20on%20human%20keypoints.%20Future%0Awork%20will%20investigate%20the%20contribution%20of%20each%20modality%20and%20explore%0Aarchitectural%20variants%20to%20further%20enhance%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04845v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial%2520and%2520Semantic%2520Embedding%2520Integration%2520for%2520Stereo%2520Sound%2520Event%250A%2520%2520Localization%2520and%2520Detection%2520in%2520Regular%2520Videos%26entry.906535625%3DDavide%2520Berghi%2520and%2520Philip%2520J.%2520B.%2520Jackson%26entry.1292438233%3D%2520%2520This%2520report%2520presents%2520our%2520systems%2520submitted%2520to%2520the%2520audio-only%2520and%2520audio-visual%250Atracks%2520of%2520the%2520DCASE2025%2520Task%25203%2520Challenge%253A%2520Stereo%2520Sound%2520Event%2520Localization%2520and%250ADetection%2520%2528SELD%2529%2520in%2520Regular%2520Video%2520Content.%2520SELD%2520is%2520a%2520complex%2520task%2520that%2520combines%250Atemporal%2520event%2520classification%2520with%2520spatial%2520localization%252C%2520requiring%2520reasoning%250Aacross%2520spatial%252C%2520temporal%252C%2520and%2520semantic%2520dimensions.%2520The%2520last%2520is%2520arguably%2520the%250Amost%2520challenging%2520to%2520model.%2520Traditional%2520SELD%2520architectures%2520rely%2520on%2520multichannel%250Ainput%252C%2520which%2520limits%2520their%2520ability%2520to%2520leverage%2520large-scale%2520pre-training%2520due%2520to%250Adata%2520constraints.%2520To%2520address%2520this%252C%2520we%2520enhance%2520standard%2520SELD%2520architectures%2520with%250Asemantic%2520information%2520by%2520integrating%2520pre-trained%252C%2520contrastive%2520language-aligned%250Amodels%253A%2520CLAP%2520for%2520audio%2520and%2520OWL-ViT%2520for%2520visual%2520inputs.%2520These%2520embeddings%2520are%250Aincorporated%2520into%2520a%2520modified%2520Conformer%2520module%2520tailored%2520for%2520multimodal%2520fusion%252C%250Awhich%2520we%2520refer%2520to%2520as%2520the%2520Cross-Modal%2520Conformer.%2520Additionally%252C%2520we%2520incorporate%250Aautocorrelation-based%2520acoustic%2520features%2520to%2520improve%2520distance%2520estimation.%2520We%250Apre-train%2520our%2520models%2520on%2520curated%2520synthetic%2520audio%2520and%2520audio-visual%2520datasets%2520and%250Aapply%2520a%2520left-right%2520channel%2520swapping%2520augmentation%2520to%2520further%2520increase%2520the%250Atraining%2520data.%2520Both%2520our%2520audio-only%2520and%2520audio-visual%2520systems%2520substantially%250Aoutperform%2520the%2520challenge%2520baselines%2520on%2520the%2520development%2520set%252C%2520demonstrating%2520the%250Aeffectiveness%2520of%2520our%2520strategy.%2520Performance%2520is%2520further%2520improved%2520through%2520model%250Aensembling%2520and%2520a%2520visual%2520post-processing%2520step%2520based%2520on%2520human%2520keypoints.%2520Future%250Awork%2520will%2520investigate%2520the%2520contribution%2520of%2520each%2520modality%2520and%2520explore%250Aarchitectural%2520variants%2520to%2520further%2520enhance%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04845v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20and%20Semantic%20Embedding%20Integration%20for%20Stereo%20Sound%20Event%0A%20%20Localization%20and%20Detection%20in%20Regular%20Videos&entry.906535625=Davide%20Berghi%20and%20Philip%20J.%20B.%20Jackson&entry.1292438233=%20%20This%20report%20presents%20our%20systems%20submitted%20to%20the%20audio-only%20and%20audio-visual%0Atracks%20of%20the%20DCASE2025%20Task%203%20Challenge%3A%20Stereo%20Sound%20Event%20Localization%20and%0ADetection%20%28SELD%29%20in%20Regular%20Video%20Content.%20SELD%20is%20a%20complex%20task%20that%20combines%0Atemporal%20event%20classification%20with%20spatial%20localization%2C%20requiring%20reasoning%0Aacross%20spatial%2C%20temporal%2C%20and%20semantic%20dimensions.%20The%20last%20is%20arguably%20the%0Amost%20challenging%20to%20model.%20Traditional%20SELD%20architectures%20rely%20on%20multichannel%0Ainput%2C%20which%20limits%20their%20ability%20to%20leverage%20large-scale%20pre-training%20due%20to%0Adata%20constraints.%20To%20address%20this%2C%20we%20enhance%20standard%20SELD%20architectures%20with%0Asemantic%20information%20by%20integrating%20pre-trained%2C%20contrastive%20language-aligned%0Amodels%3A%20CLAP%20for%20audio%20and%20OWL-ViT%20for%20visual%20inputs.%20These%20embeddings%20are%0Aincorporated%20into%20a%20modified%20Conformer%20module%20tailored%20for%20multimodal%20fusion%2C%0Awhich%20we%20refer%20to%20as%20the%20Cross-Modal%20Conformer.%20Additionally%2C%20we%20incorporate%0Aautocorrelation-based%20acoustic%20features%20to%20improve%20distance%20estimation.%20We%0Apre-train%20our%20models%20on%20curated%20synthetic%20audio%20and%20audio-visual%20datasets%20and%0Aapply%20a%20left-right%20channel%20swapping%20augmentation%20to%20further%20increase%20the%0Atraining%20data.%20Both%20our%20audio-only%20and%20audio-visual%20systems%20substantially%0Aoutperform%20the%20challenge%20baselines%20on%20the%20development%20set%2C%20demonstrating%20the%0Aeffectiveness%20of%20our%20strategy.%20Performance%20is%20further%20improved%20through%20model%0Aensembling%20and%20a%20visual%20post-processing%20step%20based%20on%20human%20keypoints.%20Future%0Awork%20will%20investigate%20the%20contribution%20of%20each%20modality%20and%20explore%0Aarchitectural%20variants%20to%20further%20enhance%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04845v1&entry.124074799=Read"},
{"title": "Neuralocks: Real-Time Dynamic Neural Hair Simulation", "author": "Gene Wei-Chin Lin and Egor Larionov and Hsiao-yu Chen and Doug Roble and Tuur Stuyck", "abstract": "  Real-time hair simulation is a vital component in creating believable virtual\navatars, as it provides a sense of immersion and authenticity. The dynamic\nbehavior of hair, such as bouncing or swaying in response to character\nmovements like jumping or walking, plays a significant role in enhancing the\noverall realism and engagement of virtual experiences. Current methods for\nsimulating hair have been constrained by two primary approaches: highly\noptimized physics-based systems and neural methods. However, state-of-the-art\nneural techniques have been limited to quasi-static solutions, failing to\ncapture the dynamic behavior of hair. This paper introduces a novel neural\nmethod that breaks through these limitations, achieving efficient and stable\ndynamic hair simulation while outperforming existing approaches. We propose a\nfully self-supervised method which can be trained without any manual\nintervention or artist generated training data allowing the method to be\nintegrated with hair reconstruction methods to enable automatic end-to-end\nmethods for avatar reconstruction. Our approach harnesses the power of compact,\nmemory-efficient neural networks to simulate hair at the strand level, allowing\nfor the simulation of diverse hairstyles without excessive computational\nresources or memory requirements. We validate the effectiveness of our method\nthrough a variety of hairstyle examples, showcasing its potential for\nreal-world applications.\n", "link": "http://arxiv.org/abs/2507.05191v1", "date": "2025-07-07", "relevancy": 2.8937, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5815}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5773}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neuralocks%3A%20Real-Time%20Dynamic%20Neural%20Hair%20Simulation&body=Title%3A%20Neuralocks%3A%20Real-Time%20Dynamic%20Neural%20Hair%20Simulation%0AAuthor%3A%20Gene%20Wei-Chin%20Lin%20and%20Egor%20Larionov%20and%20Hsiao-yu%20Chen%20and%20Doug%20Roble%20and%20Tuur%20Stuyck%0AAbstract%3A%20%20%20Real-time%20hair%20simulation%20is%20a%20vital%20component%20in%20creating%20believable%20virtual%0Aavatars%2C%20as%20it%20provides%20a%20sense%20of%20immersion%20and%20authenticity.%20The%20dynamic%0Abehavior%20of%20hair%2C%20such%20as%20bouncing%20or%20swaying%20in%20response%20to%20character%0Amovements%20like%20jumping%20or%20walking%2C%20plays%20a%20significant%20role%20in%20enhancing%20the%0Aoverall%20realism%20and%20engagement%20of%20virtual%20experiences.%20Current%20methods%20for%0Asimulating%20hair%20have%20been%20constrained%20by%20two%20primary%20approaches%3A%20highly%0Aoptimized%20physics-based%20systems%20and%20neural%20methods.%20However%2C%20state-of-the-art%0Aneural%20techniques%20have%20been%20limited%20to%20quasi-static%20solutions%2C%20failing%20to%0Acapture%20the%20dynamic%20behavior%20of%20hair.%20This%20paper%20introduces%20a%20novel%20neural%0Amethod%20that%20breaks%20through%20these%20limitations%2C%20achieving%20efficient%20and%20stable%0Adynamic%20hair%20simulation%20while%20outperforming%20existing%20approaches.%20We%20propose%20a%0Afully%20self-supervised%20method%20which%20can%20be%20trained%20without%20any%20manual%0Aintervention%20or%20artist%20generated%20training%20data%20allowing%20the%20method%20to%20be%0Aintegrated%20with%20hair%20reconstruction%20methods%20to%20enable%20automatic%20end-to-end%0Amethods%20for%20avatar%20reconstruction.%20Our%20approach%20harnesses%20the%20power%20of%20compact%2C%0Amemory-efficient%20neural%20networks%20to%20simulate%20hair%20at%20the%20strand%20level%2C%20allowing%0Afor%20the%20simulation%20of%20diverse%20hairstyles%20without%20excessive%20computational%0Aresources%20or%20memory%20requirements.%20We%20validate%20the%20effectiveness%20of%20our%20method%0Athrough%20a%20variety%20of%20hairstyle%20examples%2C%20showcasing%20its%20potential%20for%0Areal-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuralocks%253A%2520Real-Time%2520Dynamic%2520Neural%2520Hair%2520Simulation%26entry.906535625%3DGene%2520Wei-Chin%2520Lin%2520and%2520Egor%2520Larionov%2520and%2520Hsiao-yu%2520Chen%2520and%2520Doug%2520Roble%2520and%2520Tuur%2520Stuyck%26entry.1292438233%3D%2520%2520Real-time%2520hair%2520simulation%2520is%2520a%2520vital%2520component%2520in%2520creating%2520believable%2520virtual%250Aavatars%252C%2520as%2520it%2520provides%2520a%2520sense%2520of%2520immersion%2520and%2520authenticity.%2520The%2520dynamic%250Abehavior%2520of%2520hair%252C%2520such%2520as%2520bouncing%2520or%2520swaying%2520in%2520response%2520to%2520character%250Amovements%2520like%2520jumping%2520or%2520walking%252C%2520plays%2520a%2520significant%2520role%2520in%2520enhancing%2520the%250Aoverall%2520realism%2520and%2520engagement%2520of%2520virtual%2520experiences.%2520Current%2520methods%2520for%250Asimulating%2520hair%2520have%2520been%2520constrained%2520by%2520two%2520primary%2520approaches%253A%2520highly%250Aoptimized%2520physics-based%2520systems%2520and%2520neural%2520methods.%2520However%252C%2520state-of-the-art%250Aneural%2520techniques%2520have%2520been%2520limited%2520to%2520quasi-static%2520solutions%252C%2520failing%2520to%250Acapture%2520the%2520dynamic%2520behavior%2520of%2520hair.%2520This%2520paper%2520introduces%2520a%2520novel%2520neural%250Amethod%2520that%2520breaks%2520through%2520these%2520limitations%252C%2520achieving%2520efficient%2520and%2520stable%250Adynamic%2520hair%2520simulation%2520while%2520outperforming%2520existing%2520approaches.%2520We%2520propose%2520a%250Afully%2520self-supervised%2520method%2520which%2520can%2520be%2520trained%2520without%2520any%2520manual%250Aintervention%2520or%2520artist%2520generated%2520training%2520data%2520allowing%2520the%2520method%2520to%2520be%250Aintegrated%2520with%2520hair%2520reconstruction%2520methods%2520to%2520enable%2520automatic%2520end-to-end%250Amethods%2520for%2520avatar%2520reconstruction.%2520Our%2520approach%2520harnesses%2520the%2520power%2520of%2520compact%252C%250Amemory-efficient%2520neural%2520networks%2520to%2520simulate%2520hair%2520at%2520the%2520strand%2520level%252C%2520allowing%250Afor%2520the%2520simulation%2520of%2520diverse%2520hairstyles%2520without%2520excessive%2520computational%250Aresources%2520or%2520memory%2520requirements.%2520We%2520validate%2520the%2520effectiveness%2520of%2520our%2520method%250Athrough%2520a%2520variety%2520of%2520hairstyle%2520examples%252C%2520showcasing%2520its%2520potential%2520for%250Areal-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neuralocks%3A%20Real-Time%20Dynamic%20Neural%20Hair%20Simulation&entry.906535625=Gene%20Wei-Chin%20Lin%20and%20Egor%20Larionov%20and%20Hsiao-yu%20Chen%20and%20Doug%20Roble%20and%20Tuur%20Stuyck&entry.1292438233=%20%20Real-time%20hair%20simulation%20is%20a%20vital%20component%20in%20creating%20believable%20virtual%0Aavatars%2C%20as%20it%20provides%20a%20sense%20of%20immersion%20and%20authenticity.%20The%20dynamic%0Abehavior%20of%20hair%2C%20such%20as%20bouncing%20or%20swaying%20in%20response%20to%20character%0Amovements%20like%20jumping%20or%20walking%2C%20plays%20a%20significant%20role%20in%20enhancing%20the%0Aoverall%20realism%20and%20engagement%20of%20virtual%20experiences.%20Current%20methods%20for%0Asimulating%20hair%20have%20been%20constrained%20by%20two%20primary%20approaches%3A%20highly%0Aoptimized%20physics-based%20systems%20and%20neural%20methods.%20However%2C%20state-of-the-art%0Aneural%20techniques%20have%20been%20limited%20to%20quasi-static%20solutions%2C%20failing%20to%0Acapture%20the%20dynamic%20behavior%20of%20hair.%20This%20paper%20introduces%20a%20novel%20neural%0Amethod%20that%20breaks%20through%20these%20limitations%2C%20achieving%20efficient%20and%20stable%0Adynamic%20hair%20simulation%20while%20outperforming%20existing%20approaches.%20We%20propose%20a%0Afully%20self-supervised%20method%20which%20can%20be%20trained%20without%20any%20manual%0Aintervention%20or%20artist%20generated%20training%20data%20allowing%20the%20method%20to%20be%0Aintegrated%20with%20hair%20reconstruction%20methods%20to%20enable%20automatic%20end-to-end%0Amethods%20for%20avatar%20reconstruction.%20Our%20approach%20harnesses%20the%20power%20of%20compact%2C%0Amemory-efficient%20neural%20networks%20to%20simulate%20hair%20at%20the%20strand%20level%2C%20allowing%0Afor%20the%20simulation%20of%20diverse%20hairstyles%20without%20excessive%20computational%0Aresources%20or%20memory%20requirements.%20We%20validate%20the%20effectiveness%20of%20our%20method%0Athrough%20a%20variety%20of%20hairstyle%20examples%2C%20showcasing%20its%20potential%20for%0Areal-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05191v1&entry.124074799=Read"},
{"title": "VERITAS: Verification and Explanation of Realness in Images for\n  Transparency in AI Systems", "author": "Aadi Srivastava and Vignesh Natarajkumar and Utkarsh Bheemanaboyna and Devisree Akashapu and Nagraj Gaonkar and Archit Joshi", "abstract": "  The widespread and rapid adoption of AI-generated content, created by models\nsuch as Generative Adversarial Networks (GANs) and Diffusion Models, has\nrevolutionized the digital media landscape by allowing efficient and creative\ncontent generation. However, these models also blur the difference between real\nimages and AI-generated synthetic images, raising concerns regarding content\nauthenticity and integrity. While many existing solutions to detect fake images\nfocus solely on classification and higher-resolution images, they often lack\ntransparency in their decision-making, making it difficult for users to\nunderstand why an image is classified as fake. In this paper, we present\nVERITAS, a comprehensive framework that not only accurately detects whether a\nsmall (32x32) image is AI-generated but also explains why it was classified\nthat way through artifact localization and semantic reasoning. VERITAS produces\nhuman-readable explanations that describe key artifacts in synthetic images. We\nshow that this architecture offers clear explanations of the basis of zero-shot\nsynthetic image detection tasks. Code and relevant prompts can be found at\nhttps://github.com/V-i-g-n-e-s-h-N/VERITAS .\n", "link": "http://arxiv.org/abs/2507.05146v1", "date": "2025-07-07", "relevancy": 2.8735, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5838}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5745}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VERITAS%3A%20Verification%20and%20Explanation%20of%20Realness%20in%20Images%20for%0A%20%20Transparency%20in%20AI%20Systems&body=Title%3A%20VERITAS%3A%20Verification%20and%20Explanation%20of%20Realness%20in%20Images%20for%0A%20%20Transparency%20in%20AI%20Systems%0AAuthor%3A%20Aadi%20Srivastava%20and%20Vignesh%20Natarajkumar%20and%20Utkarsh%20Bheemanaboyna%20and%20Devisree%20Akashapu%20and%20Nagraj%20Gaonkar%20and%20Archit%20Joshi%0AAbstract%3A%20%20%20The%20widespread%20and%20rapid%20adoption%20of%20AI-generated%20content%2C%20created%20by%20models%0Asuch%20as%20Generative%20Adversarial%20Networks%20%28GANs%29%20and%20Diffusion%20Models%2C%20has%0Arevolutionized%20the%20digital%20media%20landscape%20by%20allowing%20efficient%20and%20creative%0Acontent%20generation.%20However%2C%20these%20models%20also%20blur%20the%20difference%20between%20real%0Aimages%20and%20AI-generated%20synthetic%20images%2C%20raising%20concerns%20regarding%20content%0Aauthenticity%20and%20integrity.%20While%20many%20existing%20solutions%20to%20detect%20fake%20images%0Afocus%20solely%20on%20classification%20and%20higher-resolution%20images%2C%20they%20often%20lack%0Atransparency%20in%20their%20decision-making%2C%20making%20it%20difficult%20for%20users%20to%0Aunderstand%20why%20an%20image%20is%20classified%20as%20fake.%20In%20this%20paper%2C%20we%20present%0AVERITAS%2C%20a%20comprehensive%20framework%20that%20not%20only%20accurately%20detects%20whether%20a%0Asmall%20%2832x32%29%20image%20is%20AI-generated%20but%20also%20explains%20why%20it%20was%20classified%0Athat%20way%20through%20artifact%20localization%20and%20semantic%20reasoning.%20VERITAS%20produces%0Ahuman-readable%20explanations%20that%20describe%20key%20artifacts%20in%20synthetic%20images.%20We%0Ashow%20that%20this%20architecture%20offers%20clear%20explanations%20of%20the%20basis%20of%20zero-shot%0Asynthetic%20image%20detection%20tasks.%20Code%20and%20relevant%20prompts%20can%20be%20found%20at%0Ahttps%3A//github.com/V-i-g-n-e-s-h-N/VERITAS%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVERITAS%253A%2520Verification%2520and%2520Explanation%2520of%2520Realness%2520in%2520Images%2520for%250A%2520%2520Transparency%2520in%2520AI%2520Systems%26entry.906535625%3DAadi%2520Srivastava%2520and%2520Vignesh%2520Natarajkumar%2520and%2520Utkarsh%2520Bheemanaboyna%2520and%2520Devisree%2520Akashapu%2520and%2520Nagraj%2520Gaonkar%2520and%2520Archit%2520Joshi%26entry.1292438233%3D%2520%2520The%2520widespread%2520and%2520rapid%2520adoption%2520of%2520AI-generated%2520content%252C%2520created%2520by%2520models%250Asuch%2520as%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520and%2520Diffusion%2520Models%252C%2520has%250Arevolutionized%2520the%2520digital%2520media%2520landscape%2520by%2520allowing%2520efficient%2520and%2520creative%250Acontent%2520generation.%2520However%252C%2520these%2520models%2520also%2520blur%2520the%2520difference%2520between%2520real%250Aimages%2520and%2520AI-generated%2520synthetic%2520images%252C%2520raising%2520concerns%2520regarding%2520content%250Aauthenticity%2520and%2520integrity.%2520While%2520many%2520existing%2520solutions%2520to%2520detect%2520fake%2520images%250Afocus%2520solely%2520on%2520classification%2520and%2520higher-resolution%2520images%252C%2520they%2520often%2520lack%250Atransparency%2520in%2520their%2520decision-making%252C%2520making%2520it%2520difficult%2520for%2520users%2520to%250Aunderstand%2520why%2520an%2520image%2520is%2520classified%2520as%2520fake.%2520In%2520this%2520paper%252C%2520we%2520present%250AVERITAS%252C%2520a%2520comprehensive%2520framework%2520that%2520not%2520only%2520accurately%2520detects%2520whether%2520a%250Asmall%2520%252832x32%2529%2520image%2520is%2520AI-generated%2520but%2520also%2520explains%2520why%2520it%2520was%2520classified%250Athat%2520way%2520through%2520artifact%2520localization%2520and%2520semantic%2520reasoning.%2520VERITAS%2520produces%250Ahuman-readable%2520explanations%2520that%2520describe%2520key%2520artifacts%2520in%2520synthetic%2520images.%2520We%250Ashow%2520that%2520this%2520architecture%2520offers%2520clear%2520explanations%2520of%2520the%2520basis%2520of%2520zero-shot%250Asynthetic%2520image%2520detection%2520tasks.%2520Code%2520and%2520relevant%2520prompts%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/V-i-g-n-e-s-h-N/VERITAS%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VERITAS%3A%20Verification%20and%20Explanation%20of%20Realness%20in%20Images%20for%0A%20%20Transparency%20in%20AI%20Systems&entry.906535625=Aadi%20Srivastava%20and%20Vignesh%20Natarajkumar%20and%20Utkarsh%20Bheemanaboyna%20and%20Devisree%20Akashapu%20and%20Nagraj%20Gaonkar%20and%20Archit%20Joshi&entry.1292438233=%20%20The%20widespread%20and%20rapid%20adoption%20of%20AI-generated%20content%2C%20created%20by%20models%0Asuch%20as%20Generative%20Adversarial%20Networks%20%28GANs%29%20and%20Diffusion%20Models%2C%20has%0Arevolutionized%20the%20digital%20media%20landscape%20by%20allowing%20efficient%20and%20creative%0Acontent%20generation.%20However%2C%20these%20models%20also%20blur%20the%20difference%20between%20real%0Aimages%20and%20AI-generated%20synthetic%20images%2C%20raising%20concerns%20regarding%20content%0Aauthenticity%20and%20integrity.%20While%20many%20existing%20solutions%20to%20detect%20fake%20images%0Afocus%20solely%20on%20classification%20and%20higher-resolution%20images%2C%20they%20often%20lack%0Atransparency%20in%20their%20decision-making%2C%20making%20it%20difficult%20for%20users%20to%0Aunderstand%20why%20an%20image%20is%20classified%20as%20fake.%20In%20this%20paper%2C%20we%20present%0AVERITAS%2C%20a%20comprehensive%20framework%20that%20not%20only%20accurately%20detects%20whether%20a%0Asmall%20%2832x32%29%20image%20is%20AI-generated%20but%20also%20explains%20why%20it%20was%20classified%0Athat%20way%20through%20artifact%20localization%20and%20semantic%20reasoning.%20VERITAS%20produces%0Ahuman-readable%20explanations%20that%20describe%20key%20artifacts%20in%20synthetic%20images.%20We%0Ashow%20that%20this%20architecture%20offers%20clear%20explanations%20of%20the%20basis%20of%20zero-shot%0Asynthetic%20image%20detection%20tasks.%20Code%20and%20relevant%20prompts%20can%20be%20found%20at%0Ahttps%3A//github.com/V-i-g-n-e-s-h-N/VERITAS%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05146v1&entry.124074799=Read"},
{"title": "Spatio-Temporal LLM: Reasoning about Environments and Actions", "author": "Haozhen Zheng and Beitong Tian and Mingyuan Wu and Zhenggang Tang and Klara Nahrstedt and Alex Schwing", "abstract": "  Despite the significant recent progress of Multimodal Large Language Models\n(MLLMs), MLLMs still struggle to correctly answer prompts that require a\nholistic spatio-temporal understanding. Specifically, it is challenging to\naddress prompts that refer to 1) the entirety of an environment that an agent\nequipped with an MLLM can operate in; and simultaneously also refer to 2)\nrecent actions that just happened and are encoded in a video clip. However,\nsuch a holistic spatio-temporal understanding is important for agents operating\nin the real world. To address this issue, we first develop a framework to\ncollect a large-scale dataset. Using the collected \"Reasoning about\nEnvironments and Actions\" (REA) dataset, we show that recent methods indeed\nstruggle to correctly answer the prompts. To improve, we develop a\n\"spatio-temporal LLM\" (ST-LLM), a model equipped with projectors to improve\nboth spatial understanding of an environment and temporal understanding of\nrecent observations. On the collected REA data, we show that the proposed\nmethod significantly improves results compared to prior work. Code and data are\navailable at https://zoezheng126.github.io/STLLM-website/.\n", "link": "http://arxiv.org/abs/2507.05258v1", "date": "2025-07-07", "relevancy": 2.8272, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5688}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5638}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatio-Temporal%20LLM%3A%20Reasoning%20about%20Environments%20and%20Actions&body=Title%3A%20Spatio-Temporal%20LLM%3A%20Reasoning%20about%20Environments%20and%20Actions%0AAuthor%3A%20Haozhen%20Zheng%20and%20Beitong%20Tian%20and%20Mingyuan%20Wu%20and%20Zhenggang%20Tang%20and%20Klara%20Nahrstedt%20and%20Alex%20Schwing%0AAbstract%3A%20%20%20Despite%20the%20significant%20recent%20progress%20of%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%2C%20MLLMs%20still%20struggle%20to%20correctly%20answer%20prompts%20that%20require%20a%0Aholistic%20spatio-temporal%20understanding.%20Specifically%2C%20it%20is%20challenging%20to%0Aaddress%20prompts%20that%20refer%20to%201%29%20the%20entirety%20of%20an%20environment%20that%20an%20agent%0Aequipped%20with%20an%20MLLM%20can%20operate%20in%3B%20and%20simultaneously%20also%20refer%20to%202%29%0Arecent%20actions%20that%20just%20happened%20and%20are%20encoded%20in%20a%20video%20clip.%20However%2C%0Asuch%20a%20holistic%20spatio-temporal%20understanding%20is%20important%20for%20agents%20operating%0Ain%20the%20real%20world.%20To%20address%20this%20issue%2C%20we%20first%20develop%20a%20framework%20to%0Acollect%20a%20large-scale%20dataset.%20Using%20the%20collected%20%22Reasoning%20about%0AEnvironments%20and%20Actions%22%20%28REA%29%20dataset%2C%20we%20show%20that%20recent%20methods%20indeed%0Astruggle%20to%20correctly%20answer%20the%20prompts.%20To%20improve%2C%20we%20develop%20a%0A%22spatio-temporal%20LLM%22%20%28ST-LLM%29%2C%20a%20model%20equipped%20with%20projectors%20to%20improve%0Aboth%20spatial%20understanding%20of%20an%20environment%20and%20temporal%20understanding%20of%0Arecent%20observations.%20On%20the%20collected%20REA%20data%2C%20we%20show%20that%20the%20proposed%0Amethod%20significantly%20improves%20results%20compared%20to%20prior%20work.%20Code%20and%20data%20are%0Aavailable%20at%20https%3A//zoezheng126.github.io/STLLM-website/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatio-Temporal%2520LLM%253A%2520Reasoning%2520about%2520Environments%2520and%2520Actions%26entry.906535625%3DHaozhen%2520Zheng%2520and%2520Beitong%2520Tian%2520and%2520Mingyuan%2520Wu%2520and%2520Zhenggang%2520Tang%2520and%2520Klara%2520Nahrstedt%2520and%2520Alex%2520Schwing%26entry.1292438233%3D%2520%2520Despite%2520the%2520significant%2520recent%2520progress%2520of%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%252C%2520MLLMs%2520still%2520struggle%2520to%2520correctly%2520answer%2520prompts%2520that%2520require%2520a%250Aholistic%2520spatio-temporal%2520understanding.%2520Specifically%252C%2520it%2520is%2520challenging%2520to%250Aaddress%2520prompts%2520that%2520refer%2520to%25201%2529%2520the%2520entirety%2520of%2520an%2520environment%2520that%2520an%2520agent%250Aequipped%2520with%2520an%2520MLLM%2520can%2520operate%2520in%253B%2520and%2520simultaneously%2520also%2520refer%2520to%25202%2529%250Arecent%2520actions%2520that%2520just%2520happened%2520and%2520are%2520encoded%2520in%2520a%2520video%2520clip.%2520However%252C%250Asuch%2520a%2520holistic%2520spatio-temporal%2520understanding%2520is%2520important%2520for%2520agents%2520operating%250Ain%2520the%2520real%2520world.%2520To%2520address%2520this%2520issue%252C%2520we%2520first%2520develop%2520a%2520framework%2520to%250Acollect%2520a%2520large-scale%2520dataset.%2520Using%2520the%2520collected%2520%2522Reasoning%2520about%250AEnvironments%2520and%2520Actions%2522%2520%2528REA%2529%2520dataset%252C%2520we%2520show%2520that%2520recent%2520methods%2520indeed%250Astruggle%2520to%2520correctly%2520answer%2520the%2520prompts.%2520To%2520improve%252C%2520we%2520develop%2520a%250A%2522spatio-temporal%2520LLM%2522%2520%2528ST-LLM%2529%252C%2520a%2520model%2520equipped%2520with%2520projectors%2520to%2520improve%250Aboth%2520spatial%2520understanding%2520of%2520an%2520environment%2520and%2520temporal%2520understanding%2520of%250Arecent%2520observations.%2520On%2520the%2520collected%2520REA%2520data%252C%2520we%2520show%2520that%2520the%2520proposed%250Amethod%2520significantly%2520improves%2520results%2520compared%2520to%2520prior%2520work.%2520Code%2520and%2520data%2520are%250Aavailable%2520at%2520https%253A//zoezheng126.github.io/STLLM-website/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatio-Temporal%20LLM%3A%20Reasoning%20about%20Environments%20and%20Actions&entry.906535625=Haozhen%20Zheng%20and%20Beitong%20Tian%20and%20Mingyuan%20Wu%20and%20Zhenggang%20Tang%20and%20Klara%20Nahrstedt%20and%20Alex%20Schwing&entry.1292438233=%20%20Despite%20the%20significant%20recent%20progress%20of%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%2C%20MLLMs%20still%20struggle%20to%20correctly%20answer%20prompts%20that%20require%20a%0Aholistic%20spatio-temporal%20understanding.%20Specifically%2C%20it%20is%20challenging%20to%0Aaddress%20prompts%20that%20refer%20to%201%29%20the%20entirety%20of%20an%20environment%20that%20an%20agent%0Aequipped%20with%20an%20MLLM%20can%20operate%20in%3B%20and%20simultaneously%20also%20refer%20to%202%29%0Arecent%20actions%20that%20just%20happened%20and%20are%20encoded%20in%20a%20video%20clip.%20However%2C%0Asuch%20a%20holistic%20spatio-temporal%20understanding%20is%20important%20for%20agents%20operating%0Ain%20the%20real%20world.%20To%20address%20this%20issue%2C%20we%20first%20develop%20a%20framework%20to%0Acollect%20a%20large-scale%20dataset.%20Using%20the%20collected%20%22Reasoning%20about%0AEnvironments%20and%20Actions%22%20%28REA%29%20dataset%2C%20we%20show%20that%20recent%20methods%20indeed%0Astruggle%20to%20correctly%20answer%20the%20prompts.%20To%20improve%2C%20we%20develop%20a%0A%22spatio-temporal%20LLM%22%20%28ST-LLM%29%2C%20a%20model%20equipped%20with%20projectors%20to%20improve%0Aboth%20spatial%20understanding%20of%20an%20environment%20and%20temporal%20understanding%20of%0Arecent%20observations.%20On%20the%20collected%20REA%20data%2C%20we%20show%20that%20the%20proposed%0Amethod%20significantly%20improves%20results%20compared%20to%20prior%20work.%20Code%20and%20data%20are%0Aavailable%20at%20https%3A//zoezheng126.github.io/STLLM-website/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05258v1&entry.124074799=Read"},
{"title": "Holistic Tokenizer for Autoregressive Image Generation", "author": "Anlin Zheng and Haochen Wang and Yucheng Zhao and Weipeng Deng and Tiancai Wang and Xiangyu Zhang and Xiaojuan Qi", "abstract": "  The vanilla autoregressive image generation model generates visual tokens in\na step-by-step fashion, which limits the ability to capture holistic\nrelationships among token sequences. Moreover, most visual tokenizers map local\nimage patches into latent tokens, leading to limited global information. To\naddress this, we introduce \\textit{Hita}, a novel image tokenizer for\nautoregressive (AR) image generation. It introduces a holistic-to-local\ntokenization scheme with learnable holistic queries and local patch tokens.\nBesides, Hita incorporates two key strategies for improved alignment with the\nAR generation process: 1) it arranges a sequential structure with holistic\ntokens at the beginning followed by patch-level tokens while using causal\nattention to maintain awareness of previous tokens; and 2) before feeding the\nde-quantized tokens into the decoder, Hita adopts a lightweight fusion module\nto control information flow to prioritize holistic tokens. Extensive\nexperiments show that Hita accelerates the training speed of AR generators and\noutperforms those trained with vanilla tokenizers, achieving \\textbf{2.59 FID}\nand \\textbf{281.9 IS} on the ImageNet benchmark. A detailed analysis of the\nholistic representation highlights its ability to capture global image\nproperties such as textures, materials, and shapes. Additionally, Hita also\ndemonstrates effectiveness in zero-shot style transfer and image in-painting.\nThe code is available at\n\\href{https://github.com/CVMI-Lab/Hita}{https://github.com/CVMI-Lab/Hita}\n", "link": "http://arxiv.org/abs/2507.02358v2", "date": "2025-07-07", "relevancy": 2.7824, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6085}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5379}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Holistic%20Tokenizer%20for%20Autoregressive%20Image%20Generation&body=Title%3A%20Holistic%20Tokenizer%20for%20Autoregressive%20Image%20Generation%0AAuthor%3A%20Anlin%20Zheng%20and%20Haochen%20Wang%20and%20Yucheng%20Zhao%20and%20Weipeng%20Deng%20and%20Tiancai%20Wang%20and%20Xiangyu%20Zhang%20and%20Xiaojuan%20Qi%0AAbstract%3A%20%20%20The%20vanilla%20autoregressive%20image%20generation%20model%20generates%20visual%20tokens%20in%0Aa%20step-by-step%20fashion%2C%20which%20limits%20the%20ability%20to%20capture%20holistic%0Arelationships%20among%20token%20sequences.%20Moreover%2C%20most%20visual%20tokenizers%20map%20local%0Aimage%20patches%20into%20latent%20tokens%2C%20leading%20to%20limited%20global%20information.%20To%0Aaddress%20this%2C%20we%20introduce%20%5Ctextit%7BHita%7D%2C%20a%20novel%20image%20tokenizer%20for%0Aautoregressive%20%28AR%29%20image%20generation.%20It%20introduces%20a%20holistic-to-local%0Atokenization%20scheme%20with%20learnable%20holistic%20queries%20and%20local%20patch%20tokens.%0ABesides%2C%20Hita%20incorporates%20two%20key%20strategies%20for%20improved%20alignment%20with%20the%0AAR%20generation%20process%3A%201%29%20it%20arranges%20a%20sequential%20structure%20with%20holistic%0Atokens%20at%20the%20beginning%20followed%20by%20patch-level%20tokens%20while%20using%20causal%0Aattention%20to%20maintain%20awareness%20of%20previous%20tokens%3B%20and%202%29%20before%20feeding%20the%0Ade-quantized%20tokens%20into%20the%20decoder%2C%20Hita%20adopts%20a%20lightweight%20fusion%20module%0Ato%20control%20information%20flow%20to%20prioritize%20holistic%20tokens.%20Extensive%0Aexperiments%20show%20that%20Hita%20accelerates%20the%20training%20speed%20of%20AR%20generators%20and%0Aoutperforms%20those%20trained%20with%20vanilla%20tokenizers%2C%20achieving%20%5Ctextbf%7B2.59%20FID%7D%0Aand%20%5Ctextbf%7B281.9%20IS%7D%20on%20the%20ImageNet%20benchmark.%20A%20detailed%20analysis%20of%20the%0Aholistic%20representation%20highlights%20its%20ability%20to%20capture%20global%20image%0Aproperties%20such%20as%20textures%2C%20materials%2C%20and%20shapes.%20Additionally%2C%20Hita%20also%0Ademonstrates%20effectiveness%20in%20zero-shot%20style%20transfer%20and%20image%20in-painting.%0AThe%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/CVMI-Lab/Hita%7D%7Bhttps%3A//github.com/CVMI-Lab/Hita%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.02358v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHolistic%2520Tokenizer%2520for%2520Autoregressive%2520Image%2520Generation%26entry.906535625%3DAnlin%2520Zheng%2520and%2520Haochen%2520Wang%2520and%2520Yucheng%2520Zhao%2520and%2520Weipeng%2520Deng%2520and%2520Tiancai%2520Wang%2520and%2520Xiangyu%2520Zhang%2520and%2520Xiaojuan%2520Qi%26entry.1292438233%3D%2520%2520The%2520vanilla%2520autoregressive%2520image%2520generation%2520model%2520generates%2520visual%2520tokens%2520in%250Aa%2520step-by-step%2520fashion%252C%2520which%2520limits%2520the%2520ability%2520to%2520capture%2520holistic%250Arelationships%2520among%2520token%2520sequences.%2520Moreover%252C%2520most%2520visual%2520tokenizers%2520map%2520local%250Aimage%2520patches%2520into%2520latent%2520tokens%252C%2520leading%2520to%2520limited%2520global%2520information.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520%255Ctextit%257BHita%257D%252C%2520a%2520novel%2520image%2520tokenizer%2520for%250Aautoregressive%2520%2528AR%2529%2520image%2520generation.%2520It%2520introduces%2520a%2520holistic-to-local%250Atokenization%2520scheme%2520with%2520learnable%2520holistic%2520queries%2520and%2520local%2520patch%2520tokens.%250ABesides%252C%2520Hita%2520incorporates%2520two%2520key%2520strategies%2520for%2520improved%2520alignment%2520with%2520the%250AAR%2520generation%2520process%253A%25201%2529%2520it%2520arranges%2520a%2520sequential%2520structure%2520with%2520holistic%250Atokens%2520at%2520the%2520beginning%2520followed%2520by%2520patch-level%2520tokens%2520while%2520using%2520causal%250Aattention%2520to%2520maintain%2520awareness%2520of%2520previous%2520tokens%253B%2520and%25202%2529%2520before%2520feeding%2520the%250Ade-quantized%2520tokens%2520into%2520the%2520decoder%252C%2520Hita%2520adopts%2520a%2520lightweight%2520fusion%2520module%250Ato%2520control%2520information%2520flow%2520to%2520prioritize%2520holistic%2520tokens.%2520Extensive%250Aexperiments%2520show%2520that%2520Hita%2520accelerates%2520the%2520training%2520speed%2520of%2520AR%2520generators%2520and%250Aoutperforms%2520those%2520trained%2520with%2520vanilla%2520tokenizers%252C%2520achieving%2520%255Ctextbf%257B2.59%2520FID%257D%250Aand%2520%255Ctextbf%257B281.9%2520IS%257D%2520on%2520the%2520ImageNet%2520benchmark.%2520A%2520detailed%2520analysis%2520of%2520the%250Aholistic%2520representation%2520highlights%2520its%2520ability%2520to%2520capture%2520global%2520image%250Aproperties%2520such%2520as%2520textures%252C%2520materials%252C%2520and%2520shapes.%2520Additionally%252C%2520Hita%2520also%250Ademonstrates%2520effectiveness%2520in%2520zero-shot%2520style%2520transfer%2520and%2520image%2520in-painting.%250AThe%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/CVMI-Lab/Hita%257D%257Bhttps%253A//github.com/CVMI-Lab/Hita%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02358v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Holistic%20Tokenizer%20for%20Autoregressive%20Image%20Generation&entry.906535625=Anlin%20Zheng%20and%20Haochen%20Wang%20and%20Yucheng%20Zhao%20and%20Weipeng%20Deng%20and%20Tiancai%20Wang%20and%20Xiangyu%20Zhang%20and%20Xiaojuan%20Qi&entry.1292438233=%20%20The%20vanilla%20autoregressive%20image%20generation%20model%20generates%20visual%20tokens%20in%0Aa%20step-by-step%20fashion%2C%20which%20limits%20the%20ability%20to%20capture%20holistic%0Arelationships%20among%20token%20sequences.%20Moreover%2C%20most%20visual%20tokenizers%20map%20local%0Aimage%20patches%20into%20latent%20tokens%2C%20leading%20to%20limited%20global%20information.%20To%0Aaddress%20this%2C%20we%20introduce%20%5Ctextit%7BHita%7D%2C%20a%20novel%20image%20tokenizer%20for%0Aautoregressive%20%28AR%29%20image%20generation.%20It%20introduces%20a%20holistic-to-local%0Atokenization%20scheme%20with%20learnable%20holistic%20queries%20and%20local%20patch%20tokens.%0ABesides%2C%20Hita%20incorporates%20two%20key%20strategies%20for%20improved%20alignment%20with%20the%0AAR%20generation%20process%3A%201%29%20it%20arranges%20a%20sequential%20structure%20with%20holistic%0Atokens%20at%20the%20beginning%20followed%20by%20patch-level%20tokens%20while%20using%20causal%0Aattention%20to%20maintain%20awareness%20of%20previous%20tokens%3B%20and%202%29%20before%20feeding%20the%0Ade-quantized%20tokens%20into%20the%20decoder%2C%20Hita%20adopts%20a%20lightweight%20fusion%20module%0Ato%20control%20information%20flow%20to%20prioritize%20holistic%20tokens.%20Extensive%0Aexperiments%20show%20that%20Hita%20accelerates%20the%20training%20speed%20of%20AR%20generators%20and%0Aoutperforms%20those%20trained%20with%20vanilla%20tokenizers%2C%20achieving%20%5Ctextbf%7B2.59%20FID%7D%0Aand%20%5Ctextbf%7B281.9%20IS%7D%20on%20the%20ImageNet%20benchmark.%20A%20detailed%20analysis%20of%20the%0Aholistic%20representation%20highlights%20its%20ability%20to%20capture%20global%20image%0Aproperties%20such%20as%20textures%2C%20materials%2C%20and%20shapes.%20Additionally%2C%20Hita%20also%0Ademonstrates%20effectiveness%20in%20zero-shot%20style%20transfer%20and%20image%20in-painting.%0AThe%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/CVMI-Lab/Hita%7D%7Bhttps%3A//github.com/CVMI-Lab/Hita%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.02358v2&entry.124074799=Read"},
{"title": "INTER: Mitigating Hallucination in Large Vision-Language Models by\n  Interaction Guidance Sampling", "author": "Xin Dong and Shichao Dong and Jin Wang and Jing Huang and Li Zhou and Zenghui Sun and Lihua Jing and Jingsong Lan and Xiaoyong Zhu and Bo Zheng", "abstract": "  Hallucinations in large vision-language models (LVLMs) pose significant\nchallenges for real-world applications, as LVLMs may generate responses that\nappear plausible yet remain inconsistent with the associated visual content.\nThis issue rarely occurs in human cognition. We argue that this discrepancy\narises from humans' ability to effectively leverage multimodal interaction\ninformation in data samples. Specifically, humans typically first gather\nmultimodal information, analyze the interactions across modalities for\nunderstanding, and then express their understanding through language. Motivated\nby this observation, we conduct extensive experiments on popular LVLMs and\nobtained insights that surprisingly reveal human-like, though less pronounced,\ncognitive behavior of LVLMs on multimodal samples. Building on these findings,\nwe further propose \\textbf{INTER}: \\textbf{Inter}action Guidance Sampling, a\nnovel training-free algorithm that mitigate hallucinations without requiring\nadditional data. Specifically, INTER explicitly guides LVLMs to effectively\nreapply their understanding of multimodal interaction information when\ngenerating responses, thereby reducing potential hallucinations. On six\nbenchmarks including VQA and image captioning tasks, INTER achieves an average\nimprovement of up to 3.4\\% on five LVLMs compared to the state-of-the-art\ndecoding strategy. The code will be released when the paper is accepted.\n", "link": "http://arxiv.org/abs/2507.05056v1", "date": "2025-07-07", "relevancy": 2.738, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5483}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5473}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20INTER%3A%20Mitigating%20Hallucination%20in%20Large%20Vision-Language%20Models%20by%0A%20%20Interaction%20Guidance%20Sampling&body=Title%3A%20INTER%3A%20Mitigating%20Hallucination%20in%20Large%20Vision-Language%20Models%20by%0A%20%20Interaction%20Guidance%20Sampling%0AAuthor%3A%20Xin%20Dong%20and%20Shichao%20Dong%20and%20Jin%20Wang%20and%20Jing%20Huang%20and%20Li%20Zhou%20and%20Zenghui%20Sun%20and%20Lihua%20Jing%20and%20Jingsong%20Lan%20and%20Xiaoyong%20Zhu%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20Hallucinations%20in%20large%20vision-language%20models%20%28LVLMs%29%20pose%20significant%0Achallenges%20for%20real-world%20applications%2C%20as%20LVLMs%20may%20generate%20responses%20that%0Aappear%20plausible%20yet%20remain%20inconsistent%20with%20the%20associated%20visual%20content.%0AThis%20issue%20rarely%20occurs%20in%20human%20cognition.%20We%20argue%20that%20this%20discrepancy%0Aarises%20from%20humans%27%20ability%20to%20effectively%20leverage%20multimodal%20interaction%0Ainformation%20in%20data%20samples.%20Specifically%2C%20humans%20typically%20first%20gather%0Amultimodal%20information%2C%20analyze%20the%20interactions%20across%20modalities%20for%0Aunderstanding%2C%20and%20then%20express%20their%20understanding%20through%20language.%20Motivated%0Aby%20this%20observation%2C%20we%20conduct%20extensive%20experiments%20on%20popular%20LVLMs%20and%0Aobtained%20insights%20that%20surprisingly%20reveal%20human-like%2C%20though%20less%20pronounced%2C%0Acognitive%20behavior%20of%20LVLMs%20on%20multimodal%20samples.%20Building%20on%20these%20findings%2C%0Awe%20further%20propose%20%5Ctextbf%7BINTER%7D%3A%20%5Ctextbf%7BInter%7Daction%20Guidance%20Sampling%2C%20a%0Anovel%20training-free%20algorithm%20that%20mitigate%20hallucinations%20without%20requiring%0Aadditional%20data.%20Specifically%2C%20INTER%20explicitly%20guides%20LVLMs%20to%20effectively%0Areapply%20their%20understanding%20of%20multimodal%20interaction%20information%20when%0Agenerating%20responses%2C%20thereby%20reducing%20potential%20hallucinations.%20On%20six%0Abenchmarks%20including%20VQA%20and%20image%20captioning%20tasks%2C%20INTER%20achieves%20an%20average%0Aimprovement%20of%20up%20to%203.4%5C%25%20on%20five%20LVLMs%20compared%20to%20the%20state-of-the-art%0Adecoding%20strategy.%20The%20code%20will%20be%20released%20when%20the%20paper%20is%20accepted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05056v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DINTER%253A%2520Mitigating%2520Hallucination%2520in%2520Large%2520Vision-Language%2520Models%2520by%250A%2520%2520Interaction%2520Guidance%2520Sampling%26entry.906535625%3DXin%2520Dong%2520and%2520Shichao%2520Dong%2520and%2520Jin%2520Wang%2520and%2520Jing%2520Huang%2520and%2520Li%2520Zhou%2520and%2520Zenghui%2520Sun%2520and%2520Lihua%2520Jing%2520and%2520Jingsong%2520Lan%2520and%2520Xiaoyong%2520Zhu%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520Hallucinations%2520in%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520pose%2520significant%250Achallenges%2520for%2520real-world%2520applications%252C%2520as%2520LVLMs%2520may%2520generate%2520responses%2520that%250Aappear%2520plausible%2520yet%2520remain%2520inconsistent%2520with%2520the%2520associated%2520visual%2520content.%250AThis%2520issue%2520rarely%2520occurs%2520in%2520human%2520cognition.%2520We%2520argue%2520that%2520this%2520discrepancy%250Aarises%2520from%2520humans%2527%2520ability%2520to%2520effectively%2520leverage%2520multimodal%2520interaction%250Ainformation%2520in%2520data%2520samples.%2520Specifically%252C%2520humans%2520typically%2520first%2520gather%250Amultimodal%2520information%252C%2520analyze%2520the%2520interactions%2520across%2520modalities%2520for%250Aunderstanding%252C%2520and%2520then%2520express%2520their%2520understanding%2520through%2520language.%2520Motivated%250Aby%2520this%2520observation%252C%2520we%2520conduct%2520extensive%2520experiments%2520on%2520popular%2520LVLMs%2520and%250Aobtained%2520insights%2520that%2520surprisingly%2520reveal%2520human-like%252C%2520though%2520less%2520pronounced%252C%250Acognitive%2520behavior%2520of%2520LVLMs%2520on%2520multimodal%2520samples.%2520Building%2520on%2520these%2520findings%252C%250Awe%2520further%2520propose%2520%255Ctextbf%257BINTER%257D%253A%2520%255Ctextbf%257BInter%257Daction%2520Guidance%2520Sampling%252C%2520a%250Anovel%2520training-free%2520algorithm%2520that%2520mitigate%2520hallucinations%2520without%2520requiring%250Aadditional%2520data.%2520Specifically%252C%2520INTER%2520explicitly%2520guides%2520LVLMs%2520to%2520effectively%250Areapply%2520their%2520understanding%2520of%2520multimodal%2520interaction%2520information%2520when%250Agenerating%2520responses%252C%2520thereby%2520reducing%2520potential%2520hallucinations.%2520On%2520six%250Abenchmarks%2520including%2520VQA%2520and%2520image%2520captioning%2520tasks%252C%2520INTER%2520achieves%2520an%2520average%250Aimprovement%2520of%2520up%2520to%25203.4%255C%2525%2520on%2520five%2520LVLMs%2520compared%2520to%2520the%2520state-of-the-art%250Adecoding%2520strategy.%2520The%2520code%2520will%2520be%2520released%2520when%2520the%2520paper%2520is%2520accepted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05056v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=INTER%3A%20Mitigating%20Hallucination%20in%20Large%20Vision-Language%20Models%20by%0A%20%20Interaction%20Guidance%20Sampling&entry.906535625=Xin%20Dong%20and%20Shichao%20Dong%20and%20Jin%20Wang%20and%20Jing%20Huang%20and%20Li%20Zhou%20and%20Zenghui%20Sun%20and%20Lihua%20Jing%20and%20Jingsong%20Lan%20and%20Xiaoyong%20Zhu%20and%20Bo%20Zheng&entry.1292438233=%20%20Hallucinations%20in%20large%20vision-language%20models%20%28LVLMs%29%20pose%20significant%0Achallenges%20for%20real-world%20applications%2C%20as%20LVLMs%20may%20generate%20responses%20that%0Aappear%20plausible%20yet%20remain%20inconsistent%20with%20the%20associated%20visual%20content.%0AThis%20issue%20rarely%20occurs%20in%20human%20cognition.%20We%20argue%20that%20this%20discrepancy%0Aarises%20from%20humans%27%20ability%20to%20effectively%20leverage%20multimodal%20interaction%0Ainformation%20in%20data%20samples.%20Specifically%2C%20humans%20typically%20first%20gather%0Amultimodal%20information%2C%20analyze%20the%20interactions%20across%20modalities%20for%0Aunderstanding%2C%20and%20then%20express%20their%20understanding%20through%20language.%20Motivated%0Aby%20this%20observation%2C%20we%20conduct%20extensive%20experiments%20on%20popular%20LVLMs%20and%0Aobtained%20insights%20that%20surprisingly%20reveal%20human-like%2C%20though%20less%20pronounced%2C%0Acognitive%20behavior%20of%20LVLMs%20on%20multimodal%20samples.%20Building%20on%20these%20findings%2C%0Awe%20further%20propose%20%5Ctextbf%7BINTER%7D%3A%20%5Ctextbf%7BInter%7Daction%20Guidance%20Sampling%2C%20a%0Anovel%20training-free%20algorithm%20that%20mitigate%20hallucinations%20without%20requiring%0Aadditional%20data.%20Specifically%2C%20INTER%20explicitly%20guides%20LVLMs%20to%20effectively%0Areapply%20their%20understanding%20of%20multimodal%20interaction%20information%20when%0Agenerating%20responses%2C%20thereby%20reducing%20potential%20hallucinations.%20On%20six%0Abenchmarks%20including%20VQA%20and%20image%20captioning%20tasks%2C%20INTER%20achieves%20an%20average%0Aimprovement%20of%20up%20to%203.4%5C%25%20on%20five%20LVLMs%20compared%20to%20the%20state-of-the-art%0Adecoding%20strategy.%20The%20code%20will%20be%20released%20when%20the%20paper%20is%20accepted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05056v1&entry.124074799=Read"},
{"title": "RIPE: Reinforcement Learning on Unlabeled Image Pairs for Robust\n  Keypoint Extraction", "author": "Johannes K\u00fcnzel and Anna Hilsmann and Peter Eisert", "abstract": "  We introduce RIPE, an innovative reinforcement learning-based framework for\nweakly-supervised training of a keypoint extractor that excels in both\ndetection and description tasks. In contrast to conventional training regimes\nthat depend heavily on artificial transformations, pre-generated models, or 3D\ndata, RIPE requires only a binary label indicating whether paired images\nrepresent the same scene. This minimal supervision significantly expands the\npool of training data, enabling the creation of a highly generalized and robust\nkeypoint extractor.\n  RIPE utilizes the encoder's intermediate layers for the description of the\nkeypoints with a hyper-column approach to integrate information from different\nscales. Additionally, we propose an auxiliary loss to enhance the\ndiscriminative capability of the learned descriptors.\n  Comprehensive evaluations on standard benchmarks demonstrate that RIPE\nsimplifies data preparation while achieving competitive performance compared to\nstate-of-the-art techniques, marking a significant advancement in robust\nkeypoint extraction and description. To support further research, we have made\nour code publicly available at https://github.com/fraunhoferhhi/RIPE.\n", "link": "http://arxiv.org/abs/2507.04839v1", "date": "2025-07-07", "relevancy": 2.731, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.576}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5411}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RIPE%3A%20Reinforcement%20Learning%20on%20Unlabeled%20Image%20Pairs%20for%20Robust%0A%20%20Keypoint%20Extraction&body=Title%3A%20RIPE%3A%20Reinforcement%20Learning%20on%20Unlabeled%20Image%20Pairs%20for%20Robust%0A%20%20Keypoint%20Extraction%0AAuthor%3A%20Johannes%20K%C3%BCnzel%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert%0AAbstract%3A%20%20%20We%20introduce%20RIPE%2C%20an%20innovative%20reinforcement%20learning-based%20framework%20for%0Aweakly-supervised%20training%20of%20a%20keypoint%20extractor%20that%20excels%20in%20both%0Adetection%20and%20description%20tasks.%20In%20contrast%20to%20conventional%20training%20regimes%0Athat%20depend%20heavily%20on%20artificial%20transformations%2C%20pre-generated%20models%2C%20or%203D%0Adata%2C%20RIPE%20requires%20only%20a%20binary%20label%20indicating%20whether%20paired%20images%0Arepresent%20the%20same%20scene.%20This%20minimal%20supervision%20significantly%20expands%20the%0Apool%20of%20training%20data%2C%20enabling%20the%20creation%20of%20a%20highly%20generalized%20and%20robust%0Akeypoint%20extractor.%0A%20%20RIPE%20utilizes%20the%20encoder%27s%20intermediate%20layers%20for%20the%20description%20of%20the%0Akeypoints%20with%20a%20hyper-column%20approach%20to%20integrate%20information%20from%20different%0Ascales.%20Additionally%2C%20we%20propose%20an%20auxiliary%20loss%20to%20enhance%20the%0Adiscriminative%20capability%20of%20the%20learned%20descriptors.%0A%20%20Comprehensive%20evaluations%20on%20standard%20benchmarks%20demonstrate%20that%20RIPE%0Asimplifies%20data%20preparation%20while%20achieving%20competitive%20performance%20compared%20to%0Astate-of-the-art%20techniques%2C%20marking%20a%20significant%20advancement%20in%20robust%0Akeypoint%20extraction%20and%20description.%20To%20support%20further%20research%2C%20we%20have%20made%0Aour%20code%20publicly%20available%20at%20https%3A//github.com/fraunhoferhhi/RIPE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04839v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRIPE%253A%2520Reinforcement%2520Learning%2520on%2520Unlabeled%2520Image%2520Pairs%2520for%2520Robust%250A%2520%2520Keypoint%2520Extraction%26entry.906535625%3DJohannes%2520K%25C3%25BCnzel%2520and%2520Anna%2520Hilsmann%2520and%2520Peter%2520Eisert%26entry.1292438233%3D%2520%2520We%2520introduce%2520RIPE%252C%2520an%2520innovative%2520reinforcement%2520learning-based%2520framework%2520for%250Aweakly-supervised%2520training%2520of%2520a%2520keypoint%2520extractor%2520that%2520excels%2520in%2520both%250Adetection%2520and%2520description%2520tasks.%2520In%2520contrast%2520to%2520conventional%2520training%2520regimes%250Athat%2520depend%2520heavily%2520on%2520artificial%2520transformations%252C%2520pre-generated%2520models%252C%2520or%25203D%250Adata%252C%2520RIPE%2520requires%2520only%2520a%2520binary%2520label%2520indicating%2520whether%2520paired%2520images%250Arepresent%2520the%2520same%2520scene.%2520This%2520minimal%2520supervision%2520significantly%2520expands%2520the%250Apool%2520of%2520training%2520data%252C%2520enabling%2520the%2520creation%2520of%2520a%2520highly%2520generalized%2520and%2520robust%250Akeypoint%2520extractor.%250A%2520%2520RIPE%2520utilizes%2520the%2520encoder%2527s%2520intermediate%2520layers%2520for%2520the%2520description%2520of%2520the%250Akeypoints%2520with%2520a%2520hyper-column%2520approach%2520to%2520integrate%2520information%2520from%2520different%250Ascales.%2520Additionally%252C%2520we%2520propose%2520an%2520auxiliary%2520loss%2520to%2520enhance%2520the%250Adiscriminative%2520capability%2520of%2520the%2520learned%2520descriptors.%250A%2520%2520Comprehensive%2520evaluations%2520on%2520standard%2520benchmarks%2520demonstrate%2520that%2520RIPE%250Asimplifies%2520data%2520preparation%2520while%2520achieving%2520competitive%2520performance%2520compared%2520to%250Astate-of-the-art%2520techniques%252C%2520marking%2520a%2520significant%2520advancement%2520in%2520robust%250Akeypoint%2520extraction%2520and%2520description.%2520To%2520support%2520further%2520research%252C%2520we%2520have%2520made%250Aour%2520code%2520publicly%2520available%2520at%2520https%253A//github.com/fraunhoferhhi/RIPE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04839v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RIPE%3A%20Reinforcement%20Learning%20on%20Unlabeled%20Image%20Pairs%20for%20Robust%0A%20%20Keypoint%20Extraction&entry.906535625=Johannes%20K%C3%BCnzel%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert&entry.1292438233=%20%20We%20introduce%20RIPE%2C%20an%20innovative%20reinforcement%20learning-based%20framework%20for%0Aweakly-supervised%20training%20of%20a%20keypoint%20extractor%20that%20excels%20in%20both%0Adetection%20and%20description%20tasks.%20In%20contrast%20to%20conventional%20training%20regimes%0Athat%20depend%20heavily%20on%20artificial%20transformations%2C%20pre-generated%20models%2C%20or%203D%0Adata%2C%20RIPE%20requires%20only%20a%20binary%20label%20indicating%20whether%20paired%20images%0Arepresent%20the%20same%20scene.%20This%20minimal%20supervision%20significantly%20expands%20the%0Apool%20of%20training%20data%2C%20enabling%20the%20creation%20of%20a%20highly%20generalized%20and%20robust%0Akeypoint%20extractor.%0A%20%20RIPE%20utilizes%20the%20encoder%27s%20intermediate%20layers%20for%20the%20description%20of%20the%0Akeypoints%20with%20a%20hyper-column%20approach%20to%20integrate%20information%20from%20different%0Ascales.%20Additionally%2C%20we%20propose%20an%20auxiliary%20loss%20to%20enhance%20the%0Adiscriminative%20capability%20of%20the%20learned%20descriptors.%0A%20%20Comprehensive%20evaluations%20on%20standard%20benchmarks%20demonstrate%20that%20RIPE%0Asimplifies%20data%20preparation%20while%20achieving%20competitive%20performance%20compared%20to%0Astate-of-the-art%20techniques%2C%20marking%20a%20significant%20advancement%20in%20robust%0Akeypoint%20extraction%20and%20description.%20To%20support%20further%20research%2C%20we%20have%20made%0Aour%20code%20publicly%20available%20at%20https%3A//github.com/fraunhoferhhi/RIPE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04839v1&entry.124074799=Read"},
{"title": "Leveraging Self-Supervised Features for Efficient Flooded Region\n  Identification in UAV Aerial Images", "author": "Dibyabha Deb and Ujjwal Verma", "abstract": "  Identifying regions affected by disasters is a vital step in effectively\nmanaging and planning relief and rescue efforts. Unlike the traditional\napproaches of manually assessing post-disaster damage, analyzing images of\nUnmanned Aerial Vehicles (UAVs) offers an objective and reliable way to assess\nthe damage. In the past, segmentation techniques have been adopted to identify\npost-flood damage in UAV aerial images. However, most of these supervised\nlearning approaches rely on manually annotated datasets. Indeed, annotating\nimages is a time-consuming and error-prone task that requires domain expertise.\nThis work focuses on leveraging self-supervised features to accurately identify\nflooded regions in UAV aerial images. This work proposes two\nencoder-decoder-based segmentation approaches, which integrate the visual\nfeatures learned from DINOv2 with the traditional encoder backbone. This study\ninvestigates the generalization of self-supervised features for UAV aerial\nimages. Specifically, we evaluate the effectiveness of features from the DINOv2\nmodel, trained on non-aerial images, for segmenting aerial images, noting the\ndistinct perspectives between the two image types. Our results demonstrate that\nDINOv2's self-supervised pretraining on natural images generates transferable,\ngeneral-purpose visual features that streamline the development of aerial\nsegmentation workflows. By leveraging these features as a foundation, we\nsignificantly reduce reliance on labor-intensive manual annotation processes,\nenabling high-accuracy segmentation with limited labeled aerial data.\n", "link": "http://arxiv.org/abs/2507.04915v1", "date": "2025-07-07", "relevancy": 2.7139, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5607}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5338}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Self-Supervised%20Features%20for%20Efficient%20Flooded%20Region%0A%20%20Identification%20in%20UAV%20Aerial%20Images&body=Title%3A%20Leveraging%20Self-Supervised%20Features%20for%20Efficient%20Flooded%20Region%0A%20%20Identification%20in%20UAV%20Aerial%20Images%0AAuthor%3A%20Dibyabha%20Deb%20and%20Ujjwal%20Verma%0AAbstract%3A%20%20%20Identifying%20regions%20affected%20by%20disasters%20is%20a%20vital%20step%20in%20effectively%0Amanaging%20and%20planning%20relief%20and%20rescue%20efforts.%20Unlike%20the%20traditional%0Aapproaches%20of%20manually%20assessing%20post-disaster%20damage%2C%20analyzing%20images%20of%0AUnmanned%20Aerial%20Vehicles%20%28UAVs%29%20offers%20an%20objective%20and%20reliable%20way%20to%20assess%0Athe%20damage.%20In%20the%20past%2C%20segmentation%20techniques%20have%20been%20adopted%20to%20identify%0Apost-flood%20damage%20in%20UAV%20aerial%20images.%20However%2C%20most%20of%20these%20supervised%0Alearning%20approaches%20rely%20on%20manually%20annotated%20datasets.%20Indeed%2C%20annotating%0Aimages%20is%20a%20time-consuming%20and%20error-prone%20task%20that%20requires%20domain%20expertise.%0AThis%20work%20focuses%20on%20leveraging%20self-supervised%20features%20to%20accurately%20identify%0Aflooded%20regions%20in%20UAV%20aerial%20images.%20This%20work%20proposes%20two%0Aencoder-decoder-based%20segmentation%20approaches%2C%20which%20integrate%20the%20visual%0Afeatures%20learned%20from%20DINOv2%20with%20the%20traditional%20encoder%20backbone.%20This%20study%0Ainvestigates%20the%20generalization%20of%20self-supervised%20features%20for%20UAV%20aerial%0Aimages.%20Specifically%2C%20we%20evaluate%20the%20effectiveness%20of%20features%20from%20the%20DINOv2%0Amodel%2C%20trained%20on%20non-aerial%20images%2C%20for%20segmenting%20aerial%20images%2C%20noting%20the%0Adistinct%20perspectives%20between%20the%20two%20image%20types.%20Our%20results%20demonstrate%20that%0ADINOv2%27s%20self-supervised%20pretraining%20on%20natural%20images%20generates%20transferable%2C%0Ageneral-purpose%20visual%20features%20that%20streamline%20the%20development%20of%20aerial%0Asegmentation%20workflows.%20By%20leveraging%20these%20features%20as%20a%20foundation%2C%20we%0Asignificantly%20reduce%20reliance%20on%20labor-intensive%20manual%20annotation%20processes%2C%0Aenabling%20high-accuracy%20segmentation%20with%20limited%20labeled%20aerial%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Self-Supervised%2520Features%2520for%2520Efficient%2520Flooded%2520Region%250A%2520%2520Identification%2520in%2520UAV%2520Aerial%2520Images%26entry.906535625%3DDibyabha%2520Deb%2520and%2520Ujjwal%2520Verma%26entry.1292438233%3D%2520%2520Identifying%2520regions%2520affected%2520by%2520disasters%2520is%2520a%2520vital%2520step%2520in%2520effectively%250Amanaging%2520and%2520planning%2520relief%2520and%2520rescue%2520efforts.%2520Unlike%2520the%2520traditional%250Aapproaches%2520of%2520manually%2520assessing%2520post-disaster%2520damage%252C%2520analyzing%2520images%2520of%250AUnmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520offers%2520an%2520objective%2520and%2520reliable%2520way%2520to%2520assess%250Athe%2520damage.%2520In%2520the%2520past%252C%2520segmentation%2520techniques%2520have%2520been%2520adopted%2520to%2520identify%250Apost-flood%2520damage%2520in%2520UAV%2520aerial%2520images.%2520However%252C%2520most%2520of%2520these%2520supervised%250Alearning%2520approaches%2520rely%2520on%2520manually%2520annotated%2520datasets.%2520Indeed%252C%2520annotating%250Aimages%2520is%2520a%2520time-consuming%2520and%2520error-prone%2520task%2520that%2520requires%2520domain%2520expertise.%250AThis%2520work%2520focuses%2520on%2520leveraging%2520self-supervised%2520features%2520to%2520accurately%2520identify%250Aflooded%2520regions%2520in%2520UAV%2520aerial%2520images.%2520This%2520work%2520proposes%2520two%250Aencoder-decoder-based%2520segmentation%2520approaches%252C%2520which%2520integrate%2520the%2520visual%250Afeatures%2520learned%2520from%2520DINOv2%2520with%2520the%2520traditional%2520encoder%2520backbone.%2520This%2520study%250Ainvestigates%2520the%2520generalization%2520of%2520self-supervised%2520features%2520for%2520UAV%2520aerial%250Aimages.%2520Specifically%252C%2520we%2520evaluate%2520the%2520effectiveness%2520of%2520features%2520from%2520the%2520DINOv2%250Amodel%252C%2520trained%2520on%2520non-aerial%2520images%252C%2520for%2520segmenting%2520aerial%2520images%252C%2520noting%2520the%250Adistinct%2520perspectives%2520between%2520the%2520two%2520image%2520types.%2520Our%2520results%2520demonstrate%2520that%250ADINOv2%2527s%2520self-supervised%2520pretraining%2520on%2520natural%2520images%2520generates%2520transferable%252C%250Ageneral-purpose%2520visual%2520features%2520that%2520streamline%2520the%2520development%2520of%2520aerial%250Asegmentation%2520workflows.%2520By%2520leveraging%2520these%2520features%2520as%2520a%2520foundation%252C%2520we%250Asignificantly%2520reduce%2520reliance%2520on%2520labor-intensive%2520manual%2520annotation%2520processes%252C%250Aenabling%2520high-accuracy%2520segmentation%2520with%2520limited%2520labeled%2520aerial%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Self-Supervised%20Features%20for%20Efficient%20Flooded%20Region%0A%20%20Identification%20in%20UAV%20Aerial%20Images&entry.906535625=Dibyabha%20Deb%20and%20Ujjwal%20Verma&entry.1292438233=%20%20Identifying%20regions%20affected%20by%20disasters%20is%20a%20vital%20step%20in%20effectively%0Amanaging%20and%20planning%20relief%20and%20rescue%20efforts.%20Unlike%20the%20traditional%0Aapproaches%20of%20manually%20assessing%20post-disaster%20damage%2C%20analyzing%20images%20of%0AUnmanned%20Aerial%20Vehicles%20%28UAVs%29%20offers%20an%20objective%20and%20reliable%20way%20to%20assess%0Athe%20damage.%20In%20the%20past%2C%20segmentation%20techniques%20have%20been%20adopted%20to%20identify%0Apost-flood%20damage%20in%20UAV%20aerial%20images.%20However%2C%20most%20of%20these%20supervised%0Alearning%20approaches%20rely%20on%20manually%20annotated%20datasets.%20Indeed%2C%20annotating%0Aimages%20is%20a%20time-consuming%20and%20error-prone%20task%20that%20requires%20domain%20expertise.%0AThis%20work%20focuses%20on%20leveraging%20self-supervised%20features%20to%20accurately%20identify%0Aflooded%20regions%20in%20UAV%20aerial%20images.%20This%20work%20proposes%20two%0Aencoder-decoder-based%20segmentation%20approaches%2C%20which%20integrate%20the%20visual%0Afeatures%20learned%20from%20DINOv2%20with%20the%20traditional%20encoder%20backbone.%20This%20study%0Ainvestigates%20the%20generalization%20of%20self-supervised%20features%20for%20UAV%20aerial%0Aimages.%20Specifically%2C%20we%20evaluate%20the%20effectiveness%20of%20features%20from%20the%20DINOv2%0Amodel%2C%20trained%20on%20non-aerial%20images%2C%20for%20segmenting%20aerial%20images%2C%20noting%20the%0Adistinct%20perspectives%20between%20the%20two%20image%20types.%20Our%20results%20demonstrate%20that%0ADINOv2%27s%20self-supervised%20pretraining%20on%20natural%20images%20generates%20transferable%2C%0Ageneral-purpose%20visual%20features%20that%20streamline%20the%20development%20of%20aerial%0Asegmentation%20workflows.%20By%20leveraging%20these%20features%20as%20a%20foundation%2C%20we%0Asignificantly%20reduce%20reliance%20on%20labor-intensive%20manual%20annotation%20processes%2C%0Aenabling%20high-accuracy%20segmentation%20with%20limited%20labeled%20aerial%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04915v1&entry.124074799=Read"},
{"title": "ReLoop: \"Seeing Twice and Thinking Backwards\" via Closed-loop Training\n  to Mitigate Hallucinations in Multimodal understanding", "author": "Jianjiang Yang and Ziyan Huang and Yanshu Li", "abstract": "  While Multimodal Large Language Models (MLLMs) have achieved remarkable\nprogress in open-ended visual question answering, they remain vulnerable to\nhallucinations. These are outputs that contradict or misrepresent input\nsemantics, posing a critical challenge to the reliability and factual\nconsistency. Existing methods often rely on external verification or post-hoc\ncorrection, lacking an internal mechanism to validate outputs directly during\ntraining. To bridge this gap, we propose ReLoop, a unified closed-loop training\nframework that encourages multimodal consistency for cross-modal understanding\nin MLLMs. ReLoop adopts a ring-shaped structure that integrates three\ncomplementary consistency feedback mechanisms, obliging MLLMs to \"seeing twice\nand thinking backwards\". Specifically, ReLoop employs the frozen Consistency\nFeedback Plugin (CFP), comprising semantic reconstruction, visual description,\nand an attention supervision module for attention alignment. These components\ncollectively enforce semantic reversibility, visual consistency, and\ninterpretable attention, enabling the model to correct its outputs during\ntraining. Extensive evaluations and analyses demonstrate the effectiveness of\nReLoop in reducing hallucination rates across multiple benchmarks, establishing\na robust method for hallucination mitigation in MLLMs. We will release our\nsource code and data in the camera-ready version.\n", "link": "http://arxiv.org/abs/2507.04943v1", "date": "2025-07-07", "relevancy": 2.6827, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5285}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReLoop%3A%20%22Seeing%20Twice%20and%20Thinking%20Backwards%22%20via%20Closed-loop%20Training%0A%20%20to%20Mitigate%20Hallucinations%20in%20Multimodal%20understanding&body=Title%3A%20ReLoop%3A%20%22Seeing%20Twice%20and%20Thinking%20Backwards%22%20via%20Closed-loop%20Training%0A%20%20to%20Mitigate%20Hallucinations%20in%20Multimodal%20understanding%0AAuthor%3A%20Jianjiang%20Yang%20and%20Ziyan%20Huang%20and%20Yanshu%20Li%0AAbstract%3A%20%20%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%0Aprogress%20in%20open-ended%20visual%20question%20answering%2C%20they%20remain%20vulnerable%20to%0Ahallucinations.%20These%20are%20outputs%20that%20contradict%20or%20misrepresent%20input%0Asemantics%2C%20posing%20a%20critical%20challenge%20to%20the%20reliability%20and%20factual%0Aconsistency.%20Existing%20methods%20often%20rely%20on%20external%20verification%20or%20post-hoc%0Acorrection%2C%20lacking%20an%20internal%20mechanism%20to%20validate%20outputs%20directly%20during%0Atraining.%20To%20bridge%20this%20gap%2C%20we%20propose%20ReLoop%2C%20a%20unified%20closed-loop%20training%0Aframework%20that%20encourages%20multimodal%20consistency%20for%20cross-modal%20understanding%0Ain%20MLLMs.%20ReLoop%20adopts%20a%20ring-shaped%20structure%20that%20integrates%20three%0Acomplementary%20consistency%20feedback%20mechanisms%2C%20obliging%20MLLMs%20to%20%22seeing%20twice%0Aand%20thinking%20backwards%22.%20Specifically%2C%20ReLoop%20employs%20the%20frozen%20Consistency%0AFeedback%20Plugin%20%28CFP%29%2C%20comprising%20semantic%20reconstruction%2C%20visual%20description%2C%0Aand%20an%20attention%20supervision%20module%20for%20attention%20alignment.%20These%20components%0Acollectively%20enforce%20semantic%20reversibility%2C%20visual%20consistency%2C%20and%0Ainterpretable%20attention%2C%20enabling%20the%20model%20to%20correct%20its%20outputs%20during%0Atraining.%20Extensive%20evaluations%20and%20analyses%20demonstrate%20the%20effectiveness%20of%0AReLoop%20in%20reducing%20hallucination%20rates%20across%20multiple%20benchmarks%2C%20establishing%0Aa%20robust%20method%20for%20hallucination%20mitigation%20in%20MLLMs.%20We%20will%20release%20our%0Asource%20code%20and%20data%20in%20the%20camera-ready%20version.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReLoop%253A%2520%2522Seeing%2520Twice%2520and%2520Thinking%2520Backwards%2522%2520via%2520Closed-loop%2520Training%250A%2520%2520to%2520Mitigate%2520Hallucinations%2520in%2520Multimodal%2520understanding%26entry.906535625%3DJianjiang%2520Yang%2520and%2520Ziyan%2520Huang%2520and%2520Yanshu%2520Li%26entry.1292438233%3D%2520%2520While%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520achieved%2520remarkable%250Aprogress%2520in%2520open-ended%2520visual%2520question%2520answering%252C%2520they%2520remain%2520vulnerable%2520to%250Ahallucinations.%2520These%2520are%2520outputs%2520that%2520contradict%2520or%2520misrepresent%2520input%250Asemantics%252C%2520posing%2520a%2520critical%2520challenge%2520to%2520the%2520reliability%2520and%2520factual%250Aconsistency.%2520Existing%2520methods%2520often%2520rely%2520on%2520external%2520verification%2520or%2520post-hoc%250Acorrection%252C%2520lacking%2520an%2520internal%2520mechanism%2520to%2520validate%2520outputs%2520directly%2520during%250Atraining.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520ReLoop%252C%2520a%2520unified%2520closed-loop%2520training%250Aframework%2520that%2520encourages%2520multimodal%2520consistency%2520for%2520cross-modal%2520understanding%250Ain%2520MLLMs.%2520ReLoop%2520adopts%2520a%2520ring-shaped%2520structure%2520that%2520integrates%2520three%250Acomplementary%2520consistency%2520feedback%2520mechanisms%252C%2520obliging%2520MLLMs%2520to%2520%2522seeing%2520twice%250Aand%2520thinking%2520backwards%2522.%2520Specifically%252C%2520ReLoop%2520employs%2520the%2520frozen%2520Consistency%250AFeedback%2520Plugin%2520%2528CFP%2529%252C%2520comprising%2520semantic%2520reconstruction%252C%2520visual%2520description%252C%250Aand%2520an%2520attention%2520supervision%2520module%2520for%2520attention%2520alignment.%2520These%2520components%250Acollectively%2520enforce%2520semantic%2520reversibility%252C%2520visual%2520consistency%252C%2520and%250Ainterpretable%2520attention%252C%2520enabling%2520the%2520model%2520to%2520correct%2520its%2520outputs%2520during%250Atraining.%2520Extensive%2520evaluations%2520and%2520analyses%2520demonstrate%2520the%2520effectiveness%2520of%250AReLoop%2520in%2520reducing%2520hallucination%2520rates%2520across%2520multiple%2520benchmarks%252C%2520establishing%250Aa%2520robust%2520method%2520for%2520hallucination%2520mitigation%2520in%2520MLLMs.%2520We%2520will%2520release%2520our%250Asource%2520code%2520and%2520data%2520in%2520the%2520camera-ready%2520version.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReLoop%3A%20%22Seeing%20Twice%20and%20Thinking%20Backwards%22%20via%20Closed-loop%20Training%0A%20%20to%20Mitigate%20Hallucinations%20in%20Multimodal%20understanding&entry.906535625=Jianjiang%20Yang%20and%20Ziyan%20Huang%20and%20Yanshu%20Li&entry.1292438233=%20%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%0Aprogress%20in%20open-ended%20visual%20question%20answering%2C%20they%20remain%20vulnerable%20to%0Ahallucinations.%20These%20are%20outputs%20that%20contradict%20or%20misrepresent%20input%0Asemantics%2C%20posing%20a%20critical%20challenge%20to%20the%20reliability%20and%20factual%0Aconsistency.%20Existing%20methods%20often%20rely%20on%20external%20verification%20or%20post-hoc%0Acorrection%2C%20lacking%20an%20internal%20mechanism%20to%20validate%20outputs%20directly%20during%0Atraining.%20To%20bridge%20this%20gap%2C%20we%20propose%20ReLoop%2C%20a%20unified%20closed-loop%20training%0Aframework%20that%20encourages%20multimodal%20consistency%20for%20cross-modal%20understanding%0Ain%20MLLMs.%20ReLoop%20adopts%20a%20ring-shaped%20structure%20that%20integrates%20three%0Acomplementary%20consistency%20feedback%20mechanisms%2C%20obliging%20MLLMs%20to%20%22seeing%20twice%0Aand%20thinking%20backwards%22.%20Specifically%2C%20ReLoop%20employs%20the%20frozen%20Consistency%0AFeedback%20Plugin%20%28CFP%29%2C%20comprising%20semantic%20reconstruction%2C%20visual%20description%2C%0Aand%20an%20attention%20supervision%20module%20for%20attention%20alignment.%20These%20components%0Acollectively%20enforce%20semantic%20reversibility%2C%20visual%20consistency%2C%20and%0Ainterpretable%20attention%2C%20enabling%20the%20model%20to%20correct%20its%20outputs%20during%0Atraining.%20Extensive%20evaluations%20and%20analyses%20demonstrate%20the%20effectiveness%20of%0AReLoop%20in%20reducing%20hallucination%20rates%20across%20multiple%20benchmarks%2C%20establishing%0Aa%20robust%20method%20for%20hallucination%20mitigation%20in%20MLLMs.%20We%20will%20release%20our%0Asource%20code%20and%20data%20in%20the%20camera-ready%20version.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04943v1&entry.124074799=Read"},
{"title": "When Does Pruning Benefit Vision Representations?", "author": "Enrico Cassano and Riccardo Renzulli and Andrea Bragagnolo and Marco Grangetto", "abstract": "  Pruning is widely used to reduce the complexity of deep learning models, but\nits effects on interpretability and representation learning remain poorly\nunderstood. This paper investigates how pruning influences vision models across\nthree key dimensions: (i) interpretability, (ii) unsupervised object discovery,\nand (iii) alignment with human perception. We first analyze different vision\nnetwork architectures to examine how varying sparsity levels affect feature\nattribution interpretability methods. Additionally, we explore whether pruning\npromotes more succinct and structured representations, potentially improving\nunsupervised object discovery by discarding redundant information while\npreserving essential features. Finally, we assess whether pruning enhances the\nalignment between model representations and human perception, investigating\nwhether sparser models focus on more discriminative features similarly to\nhumans. Our findings also reveal the presence of sweet spots, where sparse\nmodels exhibit higher interpretability, downstream generalization and human\nalignment. However, these spots highly depend on the network architectures and\ntheir size in terms of trainable parameters. Our results suggest a complex\ninterplay between these three dimensions, highlighting the importance of\ninvestigating when and how pruning benefits vision representations.\n", "link": "http://arxiv.org/abs/2507.01722v2", "date": "2025-07-07", "relevancy": 2.6752, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5616}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Does%20Pruning%20Benefit%20Vision%20Representations%3F&body=Title%3A%20When%20Does%20Pruning%20Benefit%20Vision%20Representations%3F%0AAuthor%3A%20Enrico%20Cassano%20and%20Riccardo%20Renzulli%20and%20Andrea%20Bragagnolo%20and%20Marco%20Grangetto%0AAbstract%3A%20%20%20Pruning%20is%20widely%20used%20to%20reduce%20the%20complexity%20of%20deep%20learning%20models%2C%20but%0Aits%20effects%20on%20interpretability%20and%20representation%20learning%20remain%20poorly%0Aunderstood.%20This%20paper%20investigates%20how%20pruning%20influences%20vision%20models%20across%0Athree%20key%20dimensions%3A%20%28i%29%20interpretability%2C%20%28ii%29%20unsupervised%20object%20discovery%2C%0Aand%20%28iii%29%20alignment%20with%20human%20perception.%20We%20first%20analyze%20different%20vision%0Anetwork%20architectures%20to%20examine%20how%20varying%20sparsity%20levels%20affect%20feature%0Aattribution%20interpretability%20methods.%20Additionally%2C%20we%20explore%20whether%20pruning%0Apromotes%20more%20succinct%20and%20structured%20representations%2C%20potentially%20improving%0Aunsupervised%20object%20discovery%20by%20discarding%20redundant%20information%20while%0Apreserving%20essential%20features.%20Finally%2C%20we%20assess%20whether%20pruning%20enhances%20the%0Aalignment%20between%20model%20representations%20and%20human%20perception%2C%20investigating%0Awhether%20sparser%20models%20focus%20on%20more%20discriminative%20features%20similarly%20to%0Ahumans.%20Our%20findings%20also%20reveal%20the%20presence%20of%20sweet%20spots%2C%20where%20sparse%0Amodels%20exhibit%20higher%20interpretability%2C%20downstream%20generalization%20and%20human%0Aalignment.%20However%2C%20these%20spots%20highly%20depend%20on%20the%20network%20architectures%20and%0Atheir%20size%20in%20terms%20of%20trainable%20parameters.%20Our%20results%20suggest%20a%20complex%0Ainterplay%20between%20these%20three%20dimensions%2C%20highlighting%20the%20importance%20of%0Ainvestigating%20when%20and%20how%20pruning%20benefits%20vision%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01722v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Does%2520Pruning%2520Benefit%2520Vision%2520Representations%253F%26entry.906535625%3DEnrico%2520Cassano%2520and%2520Riccardo%2520Renzulli%2520and%2520Andrea%2520Bragagnolo%2520and%2520Marco%2520Grangetto%26entry.1292438233%3D%2520%2520Pruning%2520is%2520widely%2520used%2520to%2520reduce%2520the%2520complexity%2520of%2520deep%2520learning%2520models%252C%2520but%250Aits%2520effects%2520on%2520interpretability%2520and%2520representation%2520learning%2520remain%2520poorly%250Aunderstood.%2520This%2520paper%2520investigates%2520how%2520pruning%2520influences%2520vision%2520models%2520across%250Athree%2520key%2520dimensions%253A%2520%2528i%2529%2520interpretability%252C%2520%2528ii%2529%2520unsupervised%2520object%2520discovery%252C%250Aand%2520%2528iii%2529%2520alignment%2520with%2520human%2520perception.%2520We%2520first%2520analyze%2520different%2520vision%250Anetwork%2520architectures%2520to%2520examine%2520how%2520varying%2520sparsity%2520levels%2520affect%2520feature%250Aattribution%2520interpretability%2520methods.%2520Additionally%252C%2520we%2520explore%2520whether%2520pruning%250Apromotes%2520more%2520succinct%2520and%2520structured%2520representations%252C%2520potentially%2520improving%250Aunsupervised%2520object%2520discovery%2520by%2520discarding%2520redundant%2520information%2520while%250Apreserving%2520essential%2520features.%2520Finally%252C%2520we%2520assess%2520whether%2520pruning%2520enhances%2520the%250Aalignment%2520between%2520model%2520representations%2520and%2520human%2520perception%252C%2520investigating%250Awhether%2520sparser%2520models%2520focus%2520on%2520more%2520discriminative%2520features%2520similarly%2520to%250Ahumans.%2520Our%2520findings%2520also%2520reveal%2520the%2520presence%2520of%2520sweet%2520spots%252C%2520where%2520sparse%250Amodels%2520exhibit%2520higher%2520interpretability%252C%2520downstream%2520generalization%2520and%2520human%250Aalignment.%2520However%252C%2520these%2520spots%2520highly%2520depend%2520on%2520the%2520network%2520architectures%2520and%250Atheir%2520size%2520in%2520terms%2520of%2520trainable%2520parameters.%2520Our%2520results%2520suggest%2520a%2520complex%250Ainterplay%2520between%2520these%2520three%2520dimensions%252C%2520highlighting%2520the%2520importance%2520of%250Ainvestigating%2520when%2520and%2520how%2520pruning%2520benefits%2520vision%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01722v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Does%20Pruning%20Benefit%20Vision%20Representations%3F&entry.906535625=Enrico%20Cassano%20and%20Riccardo%20Renzulli%20and%20Andrea%20Bragagnolo%20and%20Marco%20Grangetto&entry.1292438233=%20%20Pruning%20is%20widely%20used%20to%20reduce%20the%20complexity%20of%20deep%20learning%20models%2C%20but%0Aits%20effects%20on%20interpretability%20and%20representation%20learning%20remain%20poorly%0Aunderstood.%20This%20paper%20investigates%20how%20pruning%20influences%20vision%20models%20across%0Athree%20key%20dimensions%3A%20%28i%29%20interpretability%2C%20%28ii%29%20unsupervised%20object%20discovery%2C%0Aand%20%28iii%29%20alignment%20with%20human%20perception.%20We%20first%20analyze%20different%20vision%0Anetwork%20architectures%20to%20examine%20how%20varying%20sparsity%20levels%20affect%20feature%0Aattribution%20interpretability%20methods.%20Additionally%2C%20we%20explore%20whether%20pruning%0Apromotes%20more%20succinct%20and%20structured%20representations%2C%20potentially%20improving%0Aunsupervised%20object%20discovery%20by%20discarding%20redundant%20information%20while%0Apreserving%20essential%20features.%20Finally%2C%20we%20assess%20whether%20pruning%20enhances%20the%0Aalignment%20between%20model%20representations%20and%20human%20perception%2C%20investigating%0Awhether%20sparser%20models%20focus%20on%20more%20discriminative%20features%20similarly%20to%0Ahumans.%20Our%20findings%20also%20reveal%20the%20presence%20of%20sweet%20spots%2C%20where%20sparse%0Amodels%20exhibit%20higher%20interpretability%2C%20downstream%20generalization%20and%20human%0Aalignment.%20However%2C%20these%20spots%20highly%20depend%20on%20the%20network%20architectures%20and%0Atheir%20size%20in%20terms%20of%20trainable%20parameters.%20Our%20results%20suggest%20a%20complex%0Ainterplay%20between%20these%20three%20dimensions%2C%20highlighting%20the%20importance%20of%0Ainvestigating%20when%20and%20how%20pruning%20benefits%20vision%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01722v2&entry.124074799=Read"},
{"title": "SV-DRR: High-Fidelity Novel View X-Ray Synthesis Using Diffusion Model", "author": "Chun Xie and Yuichi Yoshii and Itaru Kitahara", "abstract": "  X-ray imaging is a rapid and cost-effective tool for visualizing internal\nhuman anatomy. While multi-view X-ray imaging provides complementary\ninformation that enhances diagnosis, intervention, and education, acquiring\nimages from multiple angles increases radiation exposure and complicates\nclinical workflows. To address these challenges, we propose a novel\nview-conditioned diffusion model for synthesizing multi-view X-ray images from\na single view. Unlike prior methods, which are limited in angular range,\nresolution, and image quality, our approach leverages the Diffusion Transformer\nto preserve fine details and employs a weak-to-strong training strategy for\nstable high-resolution image generation. Experimental results demonstrate that\nour method generates higher-resolution outputs with improved control over\nviewing angles. This capability has significant implications not only for\nclinical applications but also for medical education and data extension,\nenabling the creation of diverse, high-quality datasets for training and\nanalysis. Our code is available at GitHub.\n", "link": "http://arxiv.org/abs/2507.05148v1", "date": "2025-07-07", "relevancy": 2.6601, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6707}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6707}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SV-DRR%3A%20High-Fidelity%20Novel%20View%20X-Ray%20Synthesis%20Using%20Diffusion%20Model&body=Title%3A%20SV-DRR%3A%20High-Fidelity%20Novel%20View%20X-Ray%20Synthesis%20Using%20Diffusion%20Model%0AAuthor%3A%20Chun%20Xie%20and%20Yuichi%20Yoshii%20and%20Itaru%20Kitahara%0AAbstract%3A%20%20%20X-ray%20imaging%20is%20a%20rapid%20and%20cost-effective%20tool%20for%20visualizing%20internal%0Ahuman%20anatomy.%20While%20multi-view%20X-ray%20imaging%20provides%20complementary%0Ainformation%20that%20enhances%20diagnosis%2C%20intervention%2C%20and%20education%2C%20acquiring%0Aimages%20from%20multiple%20angles%20increases%20radiation%20exposure%20and%20complicates%0Aclinical%20workflows.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%0Aview-conditioned%20diffusion%20model%20for%20synthesizing%20multi-view%20X-ray%20images%20from%0Aa%20single%20view.%20Unlike%20prior%20methods%2C%20which%20are%20limited%20in%20angular%20range%2C%0Aresolution%2C%20and%20image%20quality%2C%20our%20approach%20leverages%20the%20Diffusion%20Transformer%0Ato%20preserve%20fine%20details%20and%20employs%20a%20weak-to-strong%20training%20strategy%20for%0Astable%20high-resolution%20image%20generation.%20Experimental%20results%20demonstrate%20that%0Aour%20method%20generates%20higher-resolution%20outputs%20with%20improved%20control%20over%0Aviewing%20angles.%20This%20capability%20has%20significant%20implications%20not%20only%20for%0Aclinical%20applications%20but%20also%20for%20medical%20education%20and%20data%20extension%2C%0Aenabling%20the%20creation%20of%20diverse%2C%20high-quality%20datasets%20for%20training%20and%0Aanalysis.%20Our%20code%20is%20available%20at%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05148v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSV-DRR%253A%2520High-Fidelity%2520Novel%2520View%2520X-Ray%2520Synthesis%2520Using%2520Diffusion%2520Model%26entry.906535625%3DChun%2520Xie%2520and%2520Yuichi%2520Yoshii%2520and%2520Itaru%2520Kitahara%26entry.1292438233%3D%2520%2520X-ray%2520imaging%2520is%2520a%2520rapid%2520and%2520cost-effective%2520tool%2520for%2520visualizing%2520internal%250Ahuman%2520anatomy.%2520While%2520multi-view%2520X-ray%2520imaging%2520provides%2520complementary%250Ainformation%2520that%2520enhances%2520diagnosis%252C%2520intervention%252C%2520and%2520education%252C%2520acquiring%250Aimages%2520from%2520multiple%2520angles%2520increases%2520radiation%2520exposure%2520and%2520complicates%250Aclinical%2520workflows.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%250Aview-conditioned%2520diffusion%2520model%2520for%2520synthesizing%2520multi-view%2520X-ray%2520images%2520from%250Aa%2520single%2520view.%2520Unlike%2520prior%2520methods%252C%2520which%2520are%2520limited%2520in%2520angular%2520range%252C%250Aresolution%252C%2520and%2520image%2520quality%252C%2520our%2520approach%2520leverages%2520the%2520Diffusion%2520Transformer%250Ato%2520preserve%2520fine%2520details%2520and%2520employs%2520a%2520weak-to-strong%2520training%2520strategy%2520for%250Astable%2520high-resolution%2520image%2520generation.%2520Experimental%2520results%2520demonstrate%2520that%250Aour%2520method%2520generates%2520higher-resolution%2520outputs%2520with%2520improved%2520control%2520over%250Aviewing%2520angles.%2520This%2520capability%2520has%2520significant%2520implications%2520not%2520only%2520for%250Aclinical%2520applications%2520but%2520also%2520for%2520medical%2520education%2520and%2520data%2520extension%252C%250Aenabling%2520the%2520creation%2520of%2520diverse%252C%2520high-quality%2520datasets%2520for%2520training%2520and%250Aanalysis.%2520Our%2520code%2520is%2520available%2520at%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05148v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SV-DRR%3A%20High-Fidelity%20Novel%20View%20X-Ray%20Synthesis%20Using%20Diffusion%20Model&entry.906535625=Chun%20Xie%20and%20Yuichi%20Yoshii%20and%20Itaru%20Kitahara&entry.1292438233=%20%20X-ray%20imaging%20is%20a%20rapid%20and%20cost-effective%20tool%20for%20visualizing%20internal%0Ahuman%20anatomy.%20While%20multi-view%20X-ray%20imaging%20provides%20complementary%0Ainformation%20that%20enhances%20diagnosis%2C%20intervention%2C%20and%20education%2C%20acquiring%0Aimages%20from%20multiple%20angles%20increases%20radiation%20exposure%20and%20complicates%0Aclinical%20workflows.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%0Aview-conditioned%20diffusion%20model%20for%20synthesizing%20multi-view%20X-ray%20images%20from%0Aa%20single%20view.%20Unlike%20prior%20methods%2C%20which%20are%20limited%20in%20angular%20range%2C%0Aresolution%2C%20and%20image%20quality%2C%20our%20approach%20leverages%20the%20Diffusion%20Transformer%0Ato%20preserve%20fine%20details%20and%20employs%20a%20weak-to-strong%20training%20strategy%20for%0Astable%20high-resolution%20image%20generation.%20Experimental%20results%20demonstrate%20that%0Aour%20method%20generates%20higher-resolution%20outputs%20with%20improved%20control%20over%0Aviewing%20angles.%20This%20capability%20has%20significant%20implications%20not%20only%20for%0Aclinical%20applications%20but%20also%20for%20medical%20education%20and%20data%20extension%2C%0Aenabling%20the%20creation%20of%20diverse%2C%20high-quality%20datasets%20for%20training%20and%0Aanalysis.%20Our%20code%20is%20available%20at%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05148v1&entry.124074799=Read"},
{"title": "An Evaluation of Large Language Models on Text Summarization Tasks Using\n  Prompt Engineering Techniques", "author": "Walid Mohamed Aly and Taysir Hassan A. Soliman and Amr Mohamed AbdelAziz", "abstract": "  Large Language Models (LLMs) continue to advance natural language processing\nwith their ability to generate human-like text across a range of tasks. Despite\nthe remarkable success of LLMs in Natural Language Processing (NLP), their\nperformance in text summarization across various domains and datasets has not\nbeen comprehensively evaluated. At the same time, the ability to summarize text\neffectively without relying on extensive training data has become a crucial\nbottleneck. To address these issues, we present a systematic evaluation of six\nLLMs across four datasets: CNN/Daily Mail and NewsRoom (news), SAMSum (dialog),\nand ArXiv (scientific). By leveraging prompt engineering techniques including\nzero-shot and in-context learning, our study evaluates the performance using\nthe ROUGE and BERTScore metrics. In addition, a detailed analysis of inference\ntimes is conducted to better understand the trade-off between summarization\nquality and computational efficiency. For Long documents, introduce a\nsentence-based chunking strategy that enables LLMs with shorter context windows\nto summarize extended inputs in multiple stages. The findings reveal that while\nLLMs perform competitively on news and dialog tasks, their performance on long\nscientific documents improves significantly when aided by chunking strategies.\nIn addition, notable performance variations were observed based on model\nparameters, dataset properties, and prompt design. These results offer\nactionable insights into how different LLMs behave across task types,\ncontributing to ongoing research in efficient, instruction-based NLP systems.\n", "link": "http://arxiv.org/abs/2507.05123v1", "date": "2025-07-07", "relevancy": 2.6506, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5502}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5502}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Evaluation%20of%20Large%20Language%20Models%20on%20Text%20Summarization%20Tasks%20Using%0A%20%20Prompt%20Engineering%20Techniques&body=Title%3A%20An%20Evaluation%20of%20Large%20Language%20Models%20on%20Text%20Summarization%20Tasks%20Using%0A%20%20Prompt%20Engineering%20Techniques%0AAuthor%3A%20Walid%20Mohamed%20Aly%20and%20Taysir%20Hassan%20A.%20Soliman%20and%20Amr%20Mohamed%20AbdelAziz%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20continue%20to%20advance%20natural%20language%20processing%0Awith%20their%20ability%20to%20generate%20human-like%20text%20across%20a%20range%20of%20tasks.%20Despite%0Athe%20remarkable%20success%20of%20LLMs%20in%20Natural%20Language%20Processing%20%28NLP%29%2C%20their%0Aperformance%20in%20text%20summarization%20across%20various%20domains%20and%20datasets%20has%20not%0Abeen%20comprehensively%20evaluated.%20At%20the%20same%20time%2C%20the%20ability%20to%20summarize%20text%0Aeffectively%20without%20relying%20on%20extensive%20training%20data%20has%20become%20a%20crucial%0Abottleneck.%20To%20address%20these%20issues%2C%20we%20present%20a%20systematic%20evaluation%20of%20six%0ALLMs%20across%20four%20datasets%3A%20CNN/Daily%20Mail%20and%20NewsRoom%20%28news%29%2C%20SAMSum%20%28dialog%29%2C%0Aand%20ArXiv%20%28scientific%29.%20By%20leveraging%20prompt%20engineering%20techniques%20including%0Azero-shot%20and%20in-context%20learning%2C%20our%20study%20evaluates%20the%20performance%20using%0Athe%20ROUGE%20and%20BERTScore%20metrics.%20In%20addition%2C%20a%20detailed%20analysis%20of%20inference%0Atimes%20is%20conducted%20to%20better%20understand%20the%20trade-off%20between%20summarization%0Aquality%20and%20computational%20efficiency.%20For%20Long%20documents%2C%20introduce%20a%0Asentence-based%20chunking%20strategy%20that%20enables%20LLMs%20with%20shorter%20context%20windows%0Ato%20summarize%20extended%20inputs%20in%20multiple%20stages.%20The%20findings%20reveal%20that%20while%0ALLMs%20perform%20competitively%20on%20news%20and%20dialog%20tasks%2C%20their%20performance%20on%20long%0Ascientific%20documents%20improves%20significantly%20when%20aided%20by%20chunking%20strategies.%0AIn%20addition%2C%20notable%20performance%20variations%20were%20observed%20based%20on%20model%0Aparameters%2C%20dataset%20properties%2C%20and%20prompt%20design.%20These%20results%20offer%0Aactionable%20insights%20into%20how%20different%20LLMs%20behave%20across%20task%20types%2C%0Acontributing%20to%20ongoing%20research%20in%20efficient%2C%20instruction-based%20NLP%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05123v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Evaluation%2520of%2520Large%2520Language%2520Models%2520on%2520Text%2520Summarization%2520Tasks%2520Using%250A%2520%2520Prompt%2520Engineering%2520Techniques%26entry.906535625%3DWalid%2520Mohamed%2520Aly%2520and%2520Taysir%2520Hassan%2520A.%2520Soliman%2520and%2520Amr%2520Mohamed%2520AbdelAziz%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520continue%2520to%2520advance%2520natural%2520language%2520processing%250Awith%2520their%2520ability%2520to%2520generate%2520human-like%2520text%2520across%2520a%2520range%2520of%2520tasks.%2520Despite%250Athe%2520remarkable%2520success%2520of%2520LLMs%2520in%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%252C%2520their%250Aperformance%2520in%2520text%2520summarization%2520across%2520various%2520domains%2520and%2520datasets%2520has%2520not%250Abeen%2520comprehensively%2520evaluated.%2520At%2520the%2520same%2520time%252C%2520the%2520ability%2520to%2520summarize%2520text%250Aeffectively%2520without%2520relying%2520on%2520extensive%2520training%2520data%2520has%2520become%2520a%2520crucial%250Abottleneck.%2520To%2520address%2520these%2520issues%252C%2520we%2520present%2520a%2520systematic%2520evaluation%2520of%2520six%250ALLMs%2520across%2520four%2520datasets%253A%2520CNN/Daily%2520Mail%2520and%2520NewsRoom%2520%2528news%2529%252C%2520SAMSum%2520%2528dialog%2529%252C%250Aand%2520ArXiv%2520%2528scientific%2529.%2520By%2520leveraging%2520prompt%2520engineering%2520techniques%2520including%250Azero-shot%2520and%2520in-context%2520learning%252C%2520our%2520study%2520evaluates%2520the%2520performance%2520using%250Athe%2520ROUGE%2520and%2520BERTScore%2520metrics.%2520In%2520addition%252C%2520a%2520detailed%2520analysis%2520of%2520inference%250Atimes%2520is%2520conducted%2520to%2520better%2520understand%2520the%2520trade-off%2520between%2520summarization%250Aquality%2520and%2520computational%2520efficiency.%2520For%2520Long%2520documents%252C%2520introduce%2520a%250Asentence-based%2520chunking%2520strategy%2520that%2520enables%2520LLMs%2520with%2520shorter%2520context%2520windows%250Ato%2520summarize%2520extended%2520inputs%2520in%2520multiple%2520stages.%2520The%2520findings%2520reveal%2520that%2520while%250ALLMs%2520perform%2520competitively%2520on%2520news%2520and%2520dialog%2520tasks%252C%2520their%2520performance%2520on%2520long%250Ascientific%2520documents%2520improves%2520significantly%2520when%2520aided%2520by%2520chunking%2520strategies.%250AIn%2520addition%252C%2520notable%2520performance%2520variations%2520were%2520observed%2520based%2520on%2520model%250Aparameters%252C%2520dataset%2520properties%252C%2520and%2520prompt%2520design.%2520These%2520results%2520offer%250Aactionable%2520insights%2520into%2520how%2520different%2520LLMs%2520behave%2520across%2520task%2520types%252C%250Acontributing%2520to%2520ongoing%2520research%2520in%2520efficient%252C%2520instruction-based%2520NLP%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05123v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Evaluation%20of%20Large%20Language%20Models%20on%20Text%20Summarization%20Tasks%20Using%0A%20%20Prompt%20Engineering%20Techniques&entry.906535625=Walid%20Mohamed%20Aly%20and%20Taysir%20Hassan%20A.%20Soliman%20and%20Amr%20Mohamed%20AbdelAziz&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20continue%20to%20advance%20natural%20language%20processing%0Awith%20their%20ability%20to%20generate%20human-like%20text%20across%20a%20range%20of%20tasks.%20Despite%0Athe%20remarkable%20success%20of%20LLMs%20in%20Natural%20Language%20Processing%20%28NLP%29%2C%20their%0Aperformance%20in%20text%20summarization%20across%20various%20domains%20and%20datasets%20has%20not%0Abeen%20comprehensively%20evaluated.%20At%20the%20same%20time%2C%20the%20ability%20to%20summarize%20text%0Aeffectively%20without%20relying%20on%20extensive%20training%20data%20has%20become%20a%20crucial%0Abottleneck.%20To%20address%20these%20issues%2C%20we%20present%20a%20systematic%20evaluation%20of%20six%0ALLMs%20across%20four%20datasets%3A%20CNN/Daily%20Mail%20and%20NewsRoom%20%28news%29%2C%20SAMSum%20%28dialog%29%2C%0Aand%20ArXiv%20%28scientific%29.%20By%20leveraging%20prompt%20engineering%20techniques%20including%0Azero-shot%20and%20in-context%20learning%2C%20our%20study%20evaluates%20the%20performance%20using%0Athe%20ROUGE%20and%20BERTScore%20metrics.%20In%20addition%2C%20a%20detailed%20analysis%20of%20inference%0Atimes%20is%20conducted%20to%20better%20understand%20the%20trade-off%20between%20summarization%0Aquality%20and%20computational%20efficiency.%20For%20Long%20documents%2C%20introduce%20a%0Asentence-based%20chunking%20strategy%20that%20enables%20LLMs%20with%20shorter%20context%20windows%0Ato%20summarize%20extended%20inputs%20in%20multiple%20stages.%20The%20findings%20reveal%20that%20while%0ALLMs%20perform%20competitively%20on%20news%20and%20dialog%20tasks%2C%20their%20performance%20on%20long%0Ascientific%20documents%20improves%20significantly%20when%20aided%20by%20chunking%20strategies.%0AIn%20addition%2C%20notable%20performance%20variations%20were%20observed%20based%20on%20model%0Aparameters%2C%20dataset%20properties%2C%20and%20prompt%20design.%20These%20results%20offer%0Aactionable%20insights%20into%20how%20different%20LLMs%20behave%20across%20task%20types%2C%0Acontributing%20to%20ongoing%20research%20in%20efficient%2C%20instruction-based%20NLP%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05123v1&entry.124074799=Read"},
{"title": "AniCrafter: Customizing Realistic Human-Centric Animation via\n  Avatar-Background Conditioning in Video Diffusion Models", "author": "Muyao Niu and Mingdeng Cao and Yifan Zhan and Qingtian Zhu and Mingze Ma and Jiancheng Zhao and Yanhong Zeng and Zhihang Zhong and Xiao Sun and Yinqiang Zheng", "abstract": "  Recent advances in video diffusion models have significantly improved\ncharacter animation techniques. However, current approaches rely on basic\nstructural conditions such as DWPose or SMPL-X to animate character images,\nlimiting their effectiveness in open-domain scenarios with dynamic backgrounds\nor challenging human poses. In this paper, we introduce \\textbf{AniCrafter}, a\ndiffusion-based human-centric animation model that can seamlessly integrate and\nanimate a given character into open-domain dynamic backgrounds while following\ngiven human motion sequences. Built on cutting-edge Image-to-Video (I2V)\ndiffusion architectures, our model incorporates an innovative\n''avatar-background'' conditioning mechanism that reframes open-domain\nhuman-centric animation as a restoration task, enabling more stable and\nversatile animation outputs. Experimental results demonstrate the superior\nperformance of our method. Codes are available at\nhttps://github.com/MyNiuuu/AniCrafter.\n", "link": "http://arxiv.org/abs/2505.20255v2", "date": "2025-07-07", "relevancy": 2.6489, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.684}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6485}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AniCrafter%3A%20Customizing%20Realistic%20Human-Centric%20Animation%20via%0A%20%20Avatar-Background%20Conditioning%20in%20Video%20Diffusion%20Models&body=Title%3A%20AniCrafter%3A%20Customizing%20Realistic%20Human-Centric%20Animation%20via%0A%20%20Avatar-Background%20Conditioning%20in%20Video%20Diffusion%20Models%0AAuthor%3A%20Muyao%20Niu%20and%20Mingdeng%20Cao%20and%20Yifan%20Zhan%20and%20Qingtian%20Zhu%20and%20Mingze%20Ma%20and%20Jiancheng%20Zhao%20and%20Yanhong%20Zeng%20and%20Zhihang%20Zhong%20and%20Xiao%20Sun%20and%20Yinqiang%20Zheng%0AAbstract%3A%20%20%20Recent%20advances%20in%20video%20diffusion%20models%20have%20significantly%20improved%0Acharacter%20animation%20techniques.%20However%2C%20current%20approaches%20rely%20on%20basic%0Astructural%20conditions%20such%20as%20DWPose%20or%20SMPL-X%20to%20animate%20character%20images%2C%0Alimiting%20their%20effectiveness%20in%20open-domain%20scenarios%20with%20dynamic%20backgrounds%0Aor%20challenging%20human%20poses.%20In%20this%20paper%2C%20we%20introduce%20%5Ctextbf%7BAniCrafter%7D%2C%20a%0Adiffusion-based%20human-centric%20animation%20model%20that%20can%20seamlessly%20integrate%20and%0Aanimate%20a%20given%20character%20into%20open-domain%20dynamic%20backgrounds%20while%20following%0Agiven%20human%20motion%20sequences.%20Built%20on%20cutting-edge%20Image-to-Video%20%28I2V%29%0Adiffusion%20architectures%2C%20our%20model%20incorporates%20an%20innovative%0A%27%27avatar-background%27%27%20conditioning%20mechanism%20that%20reframes%20open-domain%0Ahuman-centric%20animation%20as%20a%20restoration%20task%2C%20enabling%20more%20stable%20and%0Aversatile%20animation%20outputs.%20Experimental%20results%20demonstrate%20the%20superior%0Aperformance%20of%20our%20method.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/MyNiuuu/AniCrafter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20255v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAniCrafter%253A%2520Customizing%2520Realistic%2520Human-Centric%2520Animation%2520via%250A%2520%2520Avatar-Background%2520Conditioning%2520in%2520Video%2520Diffusion%2520Models%26entry.906535625%3DMuyao%2520Niu%2520and%2520Mingdeng%2520Cao%2520and%2520Yifan%2520Zhan%2520and%2520Qingtian%2520Zhu%2520and%2520Mingze%2520Ma%2520and%2520Jiancheng%2520Zhao%2520and%2520Yanhong%2520Zeng%2520and%2520Zhihang%2520Zhong%2520and%2520Xiao%2520Sun%2520and%2520Yinqiang%2520Zheng%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520video%2520diffusion%2520models%2520have%2520significantly%2520improved%250Acharacter%2520animation%2520techniques.%2520However%252C%2520current%2520approaches%2520rely%2520on%2520basic%250Astructural%2520conditions%2520such%2520as%2520DWPose%2520or%2520SMPL-X%2520to%2520animate%2520character%2520images%252C%250Alimiting%2520their%2520effectiveness%2520in%2520open-domain%2520scenarios%2520with%2520dynamic%2520backgrounds%250Aor%2520challenging%2520human%2520poses.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520%255Ctextbf%257BAniCrafter%257D%252C%2520a%250Adiffusion-based%2520human-centric%2520animation%2520model%2520that%2520can%2520seamlessly%2520integrate%2520and%250Aanimate%2520a%2520given%2520character%2520into%2520open-domain%2520dynamic%2520backgrounds%2520while%2520following%250Agiven%2520human%2520motion%2520sequences.%2520Built%2520on%2520cutting-edge%2520Image-to-Video%2520%2528I2V%2529%250Adiffusion%2520architectures%252C%2520our%2520model%2520incorporates%2520an%2520innovative%250A%2527%2527avatar-background%2527%2527%2520conditioning%2520mechanism%2520that%2520reframes%2520open-domain%250Ahuman-centric%2520animation%2520as%2520a%2520restoration%2520task%252C%2520enabling%2520more%2520stable%2520and%250Aversatile%2520animation%2520outputs.%2520Experimental%2520results%2520demonstrate%2520the%2520superior%250Aperformance%2520of%2520our%2520method.%2520Codes%2520are%2520available%2520at%250Ahttps%253A//github.com/MyNiuuu/AniCrafter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20255v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AniCrafter%3A%20Customizing%20Realistic%20Human-Centric%20Animation%20via%0A%20%20Avatar-Background%20Conditioning%20in%20Video%20Diffusion%20Models&entry.906535625=Muyao%20Niu%20and%20Mingdeng%20Cao%20and%20Yifan%20Zhan%20and%20Qingtian%20Zhu%20and%20Mingze%20Ma%20and%20Jiancheng%20Zhao%20and%20Yanhong%20Zeng%20and%20Zhihang%20Zhong%20and%20Xiao%20Sun%20and%20Yinqiang%20Zheng&entry.1292438233=%20%20Recent%20advances%20in%20video%20diffusion%20models%20have%20significantly%20improved%0Acharacter%20animation%20techniques.%20However%2C%20current%20approaches%20rely%20on%20basic%0Astructural%20conditions%20such%20as%20DWPose%20or%20SMPL-X%20to%20animate%20character%20images%2C%0Alimiting%20their%20effectiveness%20in%20open-domain%20scenarios%20with%20dynamic%20backgrounds%0Aor%20challenging%20human%20poses.%20In%20this%20paper%2C%20we%20introduce%20%5Ctextbf%7BAniCrafter%7D%2C%20a%0Adiffusion-based%20human-centric%20animation%20model%20that%20can%20seamlessly%20integrate%20and%0Aanimate%20a%20given%20character%20into%20open-domain%20dynamic%20backgrounds%20while%20following%0Agiven%20human%20motion%20sequences.%20Built%20on%20cutting-edge%20Image-to-Video%20%28I2V%29%0Adiffusion%20architectures%2C%20our%20model%20incorporates%20an%20innovative%0A%27%27avatar-background%27%27%20conditioning%20mechanism%20that%20reframes%20open-domain%0Ahuman-centric%20animation%20as%20a%20restoration%20task%2C%20enabling%20more%20stable%20and%0Aversatile%20animation%20outputs.%20Experimental%20results%20demonstrate%20the%20superior%0Aperformance%20of%20our%20method.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/MyNiuuu/AniCrafter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20255v2&entry.124074799=Read"},
{"title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual\n  Retrieval", "author": "Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao", "abstract": "  We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding\nmodel that unifies text and image representations through a novel architecture\nsupporting both single-vector and multi-vector embeddings in the late\ninteraction style. The model incorporates task-specific Low-Rank Adaptation\n(LoRA) adapters to optimize performance across diverse retrieval scenarios,\nincluding query-document retrieval, semantic text similarity, and code search.\nComprehensive evaluations demonstrate that jina-embeddings-v4 achieves\nstate-of-the-art performance on both single-modal and cross-modal retrieval\ntasks, with particular strength in processing visually rich content such as\ntables, charts, diagrams, and mixed-media formats. To facilitate evaluation of\nthis capability, we also introduce Jina-VDR, a novel benchmark specifically\ndesigned for visually rich image retrieval.\n", "link": "http://arxiv.org/abs/2506.18902v3", "date": "2025-07-07", "relevancy": 2.6324, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5334}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20jina-embeddings-v4%3A%20Universal%20Embeddings%20for%20Multimodal%20Multilingual%0A%20%20Retrieval&body=Title%3A%20jina-embeddings-v4%3A%20Universal%20Embeddings%20for%20Multimodal%20Multilingual%0A%20%20Retrieval%0AAuthor%3A%20Michael%20G%C3%BCnther%20and%20Saba%20Sturua%20and%20Mohammad%20Kalim%20Akram%20and%20Isabelle%20Mohr%20and%20Andrei%20Ungureanu%20and%20Bo%20Wang%20and%20Sedigheh%20Eslami%20and%20Scott%20Martens%20and%20Maximilian%20Werk%20and%20Nan%20Wang%20and%20Han%20Xiao%0AAbstract%3A%20%20%20We%20introduce%20jina-embeddings-v4%2C%20a%203.8%20billion%20parameter%20multimodal%20embedding%0Amodel%20that%20unifies%20text%20and%20image%20representations%20through%20a%20novel%20architecture%0Asupporting%20both%20single-vector%20and%20multi-vector%20embeddings%20in%20the%20late%0Ainteraction%20style.%20The%20model%20incorporates%20task-specific%20Low-Rank%20Adaptation%0A%28LoRA%29%20adapters%20to%20optimize%20performance%20across%20diverse%20retrieval%20scenarios%2C%0Aincluding%20query-document%20retrieval%2C%20semantic%20text%20similarity%2C%20and%20code%20search.%0AComprehensive%20evaluations%20demonstrate%20that%20jina-embeddings-v4%20achieves%0Astate-of-the-art%20performance%20on%20both%20single-modal%20and%20cross-modal%20retrieval%0Atasks%2C%20with%20particular%20strength%20in%20processing%20visually%20rich%20content%20such%20as%0Atables%2C%20charts%2C%20diagrams%2C%20and%20mixed-media%20formats.%20To%20facilitate%20evaluation%20of%0Athis%20capability%2C%20we%20also%20introduce%20Jina-VDR%2C%20a%20novel%20benchmark%20specifically%0Adesigned%20for%20visually%20rich%20image%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.18902v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Djina-embeddings-v4%253A%2520Universal%2520Embeddings%2520for%2520Multimodal%2520Multilingual%250A%2520%2520Retrieval%26entry.906535625%3DMichael%2520G%25C3%25BCnther%2520and%2520Saba%2520Sturua%2520and%2520Mohammad%2520Kalim%2520Akram%2520and%2520Isabelle%2520Mohr%2520and%2520Andrei%2520Ungureanu%2520and%2520Bo%2520Wang%2520and%2520Sedigheh%2520Eslami%2520and%2520Scott%2520Martens%2520and%2520Maximilian%2520Werk%2520and%2520Nan%2520Wang%2520and%2520Han%2520Xiao%26entry.1292438233%3D%2520%2520We%2520introduce%2520jina-embeddings-v4%252C%2520a%25203.8%2520billion%2520parameter%2520multimodal%2520embedding%250Amodel%2520that%2520unifies%2520text%2520and%2520image%2520representations%2520through%2520a%2520novel%2520architecture%250Asupporting%2520both%2520single-vector%2520and%2520multi-vector%2520embeddings%2520in%2520the%2520late%250Ainteraction%2520style.%2520The%2520model%2520incorporates%2520task-specific%2520Low-Rank%2520Adaptation%250A%2528LoRA%2529%2520adapters%2520to%2520optimize%2520performance%2520across%2520diverse%2520retrieval%2520scenarios%252C%250Aincluding%2520query-document%2520retrieval%252C%2520semantic%2520text%2520similarity%252C%2520and%2520code%2520search.%250AComprehensive%2520evaluations%2520demonstrate%2520that%2520jina-embeddings-v4%2520achieves%250Astate-of-the-art%2520performance%2520on%2520both%2520single-modal%2520and%2520cross-modal%2520retrieval%250Atasks%252C%2520with%2520particular%2520strength%2520in%2520processing%2520visually%2520rich%2520content%2520such%2520as%250Atables%252C%2520charts%252C%2520diagrams%252C%2520and%2520mixed-media%2520formats.%2520To%2520facilitate%2520evaluation%2520of%250Athis%2520capability%252C%2520we%2520also%2520introduce%2520Jina-VDR%252C%2520a%2520novel%2520benchmark%2520specifically%250Adesigned%2520for%2520visually%2520rich%2520image%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.18902v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=jina-embeddings-v4%3A%20Universal%20Embeddings%20for%20Multimodal%20Multilingual%0A%20%20Retrieval&entry.906535625=Michael%20G%C3%BCnther%20and%20Saba%20Sturua%20and%20Mohammad%20Kalim%20Akram%20and%20Isabelle%20Mohr%20and%20Andrei%20Ungureanu%20and%20Bo%20Wang%20and%20Sedigheh%20Eslami%20and%20Scott%20Martens%20and%20Maximilian%20Werk%20and%20Nan%20Wang%20and%20Han%20Xiao&entry.1292438233=%20%20We%20introduce%20jina-embeddings-v4%2C%20a%203.8%20billion%20parameter%20multimodal%20embedding%0Amodel%20that%20unifies%20text%20and%20image%20representations%20through%20a%20novel%20architecture%0Asupporting%20both%20single-vector%20and%20multi-vector%20embeddings%20in%20the%20late%0Ainteraction%20style.%20The%20model%20incorporates%20task-specific%20Low-Rank%20Adaptation%0A%28LoRA%29%20adapters%20to%20optimize%20performance%20across%20diverse%20retrieval%20scenarios%2C%0Aincluding%20query-document%20retrieval%2C%20semantic%20text%20similarity%2C%20and%20code%20search.%0AComprehensive%20evaluations%20demonstrate%20that%20jina-embeddings-v4%20achieves%0Astate-of-the-art%20performance%20on%20both%20single-modal%20and%20cross-modal%20retrieval%0Atasks%2C%20with%20particular%20strength%20in%20processing%20visually%20rich%20content%20such%20as%0Atables%2C%20charts%2C%20diagrams%2C%20and%20mixed-media%20formats.%20To%20facilitate%20evaluation%20of%0Athis%20capability%2C%20we%20also%20introduce%20Jina-VDR%2C%20a%20novel%20benchmark%20specifically%0Adesigned%20for%20visually%20rich%20image%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.18902v3&entry.124074799=Read"},
{"title": "MoDiT: Learning Highly Consistent 3D Motion Coefficients with Diffusion\n  Transformer for Talking Head Generation", "author": "Yucheng Wang and Dan Xu", "abstract": "  Audio-driven talking head generation is critical for applications such as\nvirtual assistants, video games, and films, where natural lip movements are\nessential. Despite progress in this field, challenges remain in producing both\nconsistent and realistic facial animations. Existing methods, often based on\nGANs or UNet-based diffusion models, face three major limitations: (i) temporal\njittering caused by weak temporal constraints, resulting in frame\ninconsistencies; (ii) identity drift due to insufficient 3D information\nextraction, leading to poor preservation of facial identity; and (iii)\nunnatural blinking behavior due to inadequate modeling of realistic blink\ndynamics. To address these issues, we propose MoDiT, a novel framework that\ncombines the 3D Morphable Model (3DMM) with a Diffusion-based Transformer. Our\ncontributions include: (i) A hierarchical denoising strategy with revised\ntemporal attention and biased self/cross-attention mechanisms, enabling the\nmodel to refine lip synchronization and progressively enhance full-face\ncoherence, effectively mitigating temporal jittering. (ii) The integration of\n3DMM coefficients to provide explicit spatial constraints, ensuring accurate\n3D-informed optical flow prediction and improved lip synchronization using\nWav2Lip results, thereby preserving identity consistency. (iii) A refined\nblinking strategy to model natural eye movements, with smoother and more\nrealistic blinking behaviors.\n", "link": "http://arxiv.org/abs/2507.05092v1", "date": "2025-07-07", "relevancy": 2.6058, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7015}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.617}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoDiT%3A%20Learning%20Highly%20Consistent%203D%20Motion%20Coefficients%20with%20Diffusion%0A%20%20Transformer%20for%20Talking%20Head%20Generation&body=Title%3A%20MoDiT%3A%20Learning%20Highly%20Consistent%203D%20Motion%20Coefficients%20with%20Diffusion%0A%20%20Transformer%20for%20Talking%20Head%20Generation%0AAuthor%3A%20Yucheng%20Wang%20and%20Dan%20Xu%0AAbstract%3A%20%20%20Audio-driven%20talking%20head%20generation%20is%20critical%20for%20applications%20such%20as%0Avirtual%20assistants%2C%20video%20games%2C%20and%20films%2C%20where%20natural%20lip%20movements%20are%0Aessential.%20Despite%20progress%20in%20this%20field%2C%20challenges%20remain%20in%20producing%20both%0Aconsistent%20and%20realistic%20facial%20animations.%20Existing%20methods%2C%20often%20based%20on%0AGANs%20or%20UNet-based%20diffusion%20models%2C%20face%20three%20major%20limitations%3A%20%28i%29%20temporal%0Ajittering%20caused%20by%20weak%20temporal%20constraints%2C%20resulting%20in%20frame%0Ainconsistencies%3B%20%28ii%29%20identity%20drift%20due%20to%20insufficient%203D%20information%0Aextraction%2C%20leading%20to%20poor%20preservation%20of%20facial%20identity%3B%20and%20%28iii%29%0Aunnatural%20blinking%20behavior%20due%20to%20inadequate%20modeling%20of%20realistic%20blink%0Adynamics.%20To%20address%20these%20issues%2C%20we%20propose%20MoDiT%2C%20a%20novel%20framework%20that%0Acombines%20the%203D%20Morphable%20Model%20%283DMM%29%20with%20a%20Diffusion-based%20Transformer.%20Our%0Acontributions%20include%3A%20%28i%29%20A%20hierarchical%20denoising%20strategy%20with%20revised%0Atemporal%20attention%20and%20biased%20self/cross-attention%20mechanisms%2C%20enabling%20the%0Amodel%20to%20refine%20lip%20synchronization%20and%20progressively%20enhance%20full-face%0Acoherence%2C%20effectively%20mitigating%20temporal%20jittering.%20%28ii%29%20The%20integration%20of%0A3DMM%20coefficients%20to%20provide%20explicit%20spatial%20constraints%2C%20ensuring%20accurate%0A3D-informed%20optical%20flow%20prediction%20and%20improved%20lip%20synchronization%20using%0AWav2Lip%20results%2C%20thereby%20preserving%20identity%20consistency.%20%28iii%29%20A%20refined%0Ablinking%20strategy%20to%20model%20natural%20eye%20movements%2C%20with%20smoother%20and%20more%0Arealistic%20blinking%20behaviors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoDiT%253A%2520Learning%2520Highly%2520Consistent%25203D%2520Motion%2520Coefficients%2520with%2520Diffusion%250A%2520%2520Transformer%2520for%2520Talking%2520Head%2520Generation%26entry.906535625%3DYucheng%2520Wang%2520and%2520Dan%2520Xu%26entry.1292438233%3D%2520%2520Audio-driven%2520talking%2520head%2520generation%2520is%2520critical%2520for%2520applications%2520such%2520as%250Avirtual%2520assistants%252C%2520video%2520games%252C%2520and%2520films%252C%2520where%2520natural%2520lip%2520movements%2520are%250Aessential.%2520Despite%2520progress%2520in%2520this%2520field%252C%2520challenges%2520remain%2520in%2520producing%2520both%250Aconsistent%2520and%2520realistic%2520facial%2520animations.%2520Existing%2520methods%252C%2520often%2520based%2520on%250AGANs%2520or%2520UNet-based%2520diffusion%2520models%252C%2520face%2520three%2520major%2520limitations%253A%2520%2528i%2529%2520temporal%250Ajittering%2520caused%2520by%2520weak%2520temporal%2520constraints%252C%2520resulting%2520in%2520frame%250Ainconsistencies%253B%2520%2528ii%2529%2520identity%2520drift%2520due%2520to%2520insufficient%25203D%2520information%250Aextraction%252C%2520leading%2520to%2520poor%2520preservation%2520of%2520facial%2520identity%253B%2520and%2520%2528iii%2529%250Aunnatural%2520blinking%2520behavior%2520due%2520to%2520inadequate%2520modeling%2520of%2520realistic%2520blink%250Adynamics.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520MoDiT%252C%2520a%2520novel%2520framework%2520that%250Acombines%2520the%25203D%2520Morphable%2520Model%2520%25283DMM%2529%2520with%2520a%2520Diffusion-based%2520Transformer.%2520Our%250Acontributions%2520include%253A%2520%2528i%2529%2520A%2520hierarchical%2520denoising%2520strategy%2520with%2520revised%250Atemporal%2520attention%2520and%2520biased%2520self/cross-attention%2520mechanisms%252C%2520enabling%2520the%250Amodel%2520to%2520refine%2520lip%2520synchronization%2520and%2520progressively%2520enhance%2520full-face%250Acoherence%252C%2520effectively%2520mitigating%2520temporal%2520jittering.%2520%2528ii%2529%2520The%2520integration%2520of%250A3DMM%2520coefficients%2520to%2520provide%2520explicit%2520spatial%2520constraints%252C%2520ensuring%2520accurate%250A3D-informed%2520optical%2520flow%2520prediction%2520and%2520improved%2520lip%2520synchronization%2520using%250AWav2Lip%2520results%252C%2520thereby%2520preserving%2520identity%2520consistency.%2520%2528iii%2529%2520A%2520refined%250Ablinking%2520strategy%2520to%2520model%2520natural%2520eye%2520movements%252C%2520with%2520smoother%2520and%2520more%250Arealistic%2520blinking%2520behaviors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoDiT%3A%20Learning%20Highly%20Consistent%203D%20Motion%20Coefficients%20with%20Diffusion%0A%20%20Transformer%20for%20Talking%20Head%20Generation&entry.906535625=Yucheng%20Wang%20and%20Dan%20Xu&entry.1292438233=%20%20Audio-driven%20talking%20head%20generation%20is%20critical%20for%20applications%20such%20as%0Avirtual%20assistants%2C%20video%20games%2C%20and%20films%2C%20where%20natural%20lip%20movements%20are%0Aessential.%20Despite%20progress%20in%20this%20field%2C%20challenges%20remain%20in%20producing%20both%0Aconsistent%20and%20realistic%20facial%20animations.%20Existing%20methods%2C%20often%20based%20on%0AGANs%20or%20UNet-based%20diffusion%20models%2C%20face%20three%20major%20limitations%3A%20%28i%29%20temporal%0Ajittering%20caused%20by%20weak%20temporal%20constraints%2C%20resulting%20in%20frame%0Ainconsistencies%3B%20%28ii%29%20identity%20drift%20due%20to%20insufficient%203D%20information%0Aextraction%2C%20leading%20to%20poor%20preservation%20of%20facial%20identity%3B%20and%20%28iii%29%0Aunnatural%20blinking%20behavior%20due%20to%20inadequate%20modeling%20of%20realistic%20blink%0Adynamics.%20To%20address%20these%20issues%2C%20we%20propose%20MoDiT%2C%20a%20novel%20framework%20that%0Acombines%20the%203D%20Morphable%20Model%20%283DMM%29%20with%20a%20Diffusion-based%20Transformer.%20Our%0Acontributions%20include%3A%20%28i%29%20A%20hierarchical%20denoising%20strategy%20with%20revised%0Atemporal%20attention%20and%20biased%20self/cross-attention%20mechanisms%2C%20enabling%20the%0Amodel%20to%20refine%20lip%20synchronization%20and%20progressively%20enhance%20full-face%0Acoherence%2C%20effectively%20mitigating%20temporal%20jittering.%20%28ii%29%20The%20integration%20of%0A3DMM%20coefficients%20to%20provide%20explicit%20spatial%20constraints%2C%20ensuring%20accurate%0A3D-informed%20optical%20flow%20prediction%20and%20improved%20lip%20synchronization%20using%0AWav2Lip%20results%2C%20thereby%20preserving%20identity%20consistency.%20%28iii%29%20A%20refined%0Ablinking%20strategy%20to%20model%20natural%20eye%20movements%2C%20with%20smoother%20and%20more%0Arealistic%20blinking%20behaviors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05092v1&entry.124074799=Read"},
{"title": "Semantic Frame Interpolation", "author": "Yijia Hong and Jiangning Zhang and Ran Yi and Yuji Wang and Weijian Cao and Xiaobin Hu and Zhucun Xue and Yabiao Wang and Chengjie Wang and Lizhuang Ma", "abstract": "  Generating intermediate video content of varying lengths based on given first\nand last frames, along with text prompt information, offers significant\nresearch and application potential. However, traditional frame interpolation\ntasks primarily focus on scenarios with a small number of frames, no text\ncontrol, and minimal differences between the first and last frames. Recent\ncommunity developers have utilized large video models represented by Wan to\nendow frame-to-frame capabilities. However, these models can only generate a\nfixed number of frames and often fail to produce satisfactory results for\ncertain frame lengths, while this setting lacks a clear official definition and\na well-established benchmark. In this paper, we first propose a new practical\nSemantic Frame Interpolation (SFI) task from the perspective of academic\ndefinition, which covers the above two settings and supports inference at\nmultiple frame rates. To achieve this goal, we propose a novel SemFi model\nbuilding upon Wan2.1, which incorporates a Mixture-of-LoRA module to ensure the\ngeneration of high-consistency content that aligns with control conditions\nacross various frame length limitations. Furthermore, we propose SFI-300K, the\nfirst general-purpose dataset and benchmark specifically designed for SFI. To\nsupport this, we collect and process data from the perspective of SFI,\ncarefully designing evaluation metrics and methods to assess the model's\nperformance across multiple dimensions, encompassing image and video, and\nvarious aspects, including consistency and diversity. Through extensive\nexperiments on SFI-300K, we demonstrate that our method is particularly\nwell-suited to meet the requirements of the SFI task.\n", "link": "http://arxiv.org/abs/2507.05173v1", "date": "2025-07-07", "relevancy": 2.5845, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5285}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5113}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Frame%20Interpolation&body=Title%3A%20Semantic%20Frame%20Interpolation%0AAuthor%3A%20Yijia%20Hong%20and%20Jiangning%20Zhang%20and%20Ran%20Yi%20and%20Yuji%20Wang%20and%20Weijian%20Cao%20and%20Xiaobin%20Hu%20and%20Zhucun%20Xue%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Lizhuang%20Ma%0AAbstract%3A%20%20%20Generating%20intermediate%20video%20content%20of%20varying%20lengths%20based%20on%20given%20first%0Aand%20last%20frames%2C%20along%20with%20text%20prompt%20information%2C%20offers%20significant%0Aresearch%20and%20application%20potential.%20However%2C%20traditional%20frame%20interpolation%0Atasks%20primarily%20focus%20on%20scenarios%20with%20a%20small%20number%20of%20frames%2C%20no%20text%0Acontrol%2C%20and%20minimal%20differences%20between%20the%20first%20and%20last%20frames.%20Recent%0Acommunity%20developers%20have%20utilized%20large%20video%20models%20represented%20by%20Wan%20to%0Aendow%20frame-to-frame%20capabilities.%20However%2C%20these%20models%20can%20only%20generate%20a%0Afixed%20number%20of%20frames%20and%20often%20fail%20to%20produce%20satisfactory%20results%20for%0Acertain%20frame%20lengths%2C%20while%20this%20setting%20lacks%20a%20clear%20official%20definition%20and%0Aa%20well-established%20benchmark.%20In%20this%20paper%2C%20we%20first%20propose%20a%20new%20practical%0ASemantic%20Frame%20Interpolation%20%28SFI%29%20task%20from%20the%20perspective%20of%20academic%0Adefinition%2C%20which%20covers%20the%20above%20two%20settings%20and%20supports%20inference%20at%0Amultiple%20frame%20rates.%20To%20achieve%20this%20goal%2C%20we%20propose%20a%20novel%20SemFi%20model%0Abuilding%20upon%20Wan2.1%2C%20which%20incorporates%20a%20Mixture-of-LoRA%20module%20to%20ensure%20the%0Ageneration%20of%20high-consistency%20content%20that%20aligns%20with%20control%20conditions%0Aacross%20various%20frame%20length%20limitations.%20Furthermore%2C%20we%20propose%20SFI-300K%2C%20the%0Afirst%20general-purpose%20dataset%20and%20benchmark%20specifically%20designed%20for%20SFI.%20To%0Asupport%20this%2C%20we%20collect%20and%20process%20data%20from%20the%20perspective%20of%20SFI%2C%0Acarefully%20designing%20evaluation%20metrics%20and%20methods%20to%20assess%20the%20model%27s%0Aperformance%20across%20multiple%20dimensions%2C%20encompassing%20image%20and%20video%2C%20and%0Avarious%20aspects%2C%20including%20consistency%20and%20diversity.%20Through%20extensive%0Aexperiments%20on%20SFI-300K%2C%20we%20demonstrate%20that%20our%20method%20is%20particularly%0Awell-suited%20to%20meet%20the%20requirements%20of%20the%20SFI%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Frame%2520Interpolation%26entry.906535625%3DYijia%2520Hong%2520and%2520Jiangning%2520Zhang%2520and%2520Ran%2520Yi%2520and%2520Yuji%2520Wang%2520and%2520Weijian%2520Cao%2520and%2520Xiaobin%2520Hu%2520and%2520Zhucun%2520Xue%2520and%2520Yabiao%2520Wang%2520and%2520Chengjie%2520Wang%2520and%2520Lizhuang%2520Ma%26entry.1292438233%3D%2520%2520Generating%2520intermediate%2520video%2520content%2520of%2520varying%2520lengths%2520based%2520on%2520given%2520first%250Aand%2520last%2520frames%252C%2520along%2520with%2520text%2520prompt%2520information%252C%2520offers%2520significant%250Aresearch%2520and%2520application%2520potential.%2520However%252C%2520traditional%2520frame%2520interpolation%250Atasks%2520primarily%2520focus%2520on%2520scenarios%2520with%2520a%2520small%2520number%2520of%2520frames%252C%2520no%2520text%250Acontrol%252C%2520and%2520minimal%2520differences%2520between%2520the%2520first%2520and%2520last%2520frames.%2520Recent%250Acommunity%2520developers%2520have%2520utilized%2520large%2520video%2520models%2520represented%2520by%2520Wan%2520to%250Aendow%2520frame-to-frame%2520capabilities.%2520However%252C%2520these%2520models%2520can%2520only%2520generate%2520a%250Afixed%2520number%2520of%2520frames%2520and%2520often%2520fail%2520to%2520produce%2520satisfactory%2520results%2520for%250Acertain%2520frame%2520lengths%252C%2520while%2520this%2520setting%2520lacks%2520a%2520clear%2520official%2520definition%2520and%250Aa%2520well-established%2520benchmark.%2520In%2520this%2520paper%252C%2520we%2520first%2520propose%2520a%2520new%2520practical%250ASemantic%2520Frame%2520Interpolation%2520%2528SFI%2529%2520task%2520from%2520the%2520perspective%2520of%2520academic%250Adefinition%252C%2520which%2520covers%2520the%2520above%2520two%2520settings%2520and%2520supports%2520inference%2520at%250Amultiple%2520frame%2520rates.%2520To%2520achieve%2520this%2520goal%252C%2520we%2520propose%2520a%2520novel%2520SemFi%2520model%250Abuilding%2520upon%2520Wan2.1%252C%2520which%2520incorporates%2520a%2520Mixture-of-LoRA%2520module%2520to%2520ensure%2520the%250Ageneration%2520of%2520high-consistency%2520content%2520that%2520aligns%2520with%2520control%2520conditions%250Aacross%2520various%2520frame%2520length%2520limitations.%2520Furthermore%252C%2520we%2520propose%2520SFI-300K%252C%2520the%250Afirst%2520general-purpose%2520dataset%2520and%2520benchmark%2520specifically%2520designed%2520for%2520SFI.%2520To%250Asupport%2520this%252C%2520we%2520collect%2520and%2520process%2520data%2520from%2520the%2520perspective%2520of%2520SFI%252C%250Acarefully%2520designing%2520evaluation%2520metrics%2520and%2520methods%2520to%2520assess%2520the%2520model%2527s%250Aperformance%2520across%2520multiple%2520dimensions%252C%2520encompassing%2520image%2520and%2520video%252C%2520and%250Avarious%2520aspects%252C%2520including%2520consistency%2520and%2520diversity.%2520Through%2520extensive%250Aexperiments%2520on%2520SFI-300K%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520is%2520particularly%250Awell-suited%2520to%2520meet%2520the%2520requirements%2520of%2520the%2520SFI%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Frame%20Interpolation&entry.906535625=Yijia%20Hong%20and%20Jiangning%20Zhang%20and%20Ran%20Yi%20and%20Yuji%20Wang%20and%20Weijian%20Cao%20and%20Xiaobin%20Hu%20and%20Zhucun%20Xue%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Lizhuang%20Ma&entry.1292438233=%20%20Generating%20intermediate%20video%20content%20of%20varying%20lengths%20based%20on%20given%20first%0Aand%20last%20frames%2C%20along%20with%20text%20prompt%20information%2C%20offers%20significant%0Aresearch%20and%20application%20potential.%20However%2C%20traditional%20frame%20interpolation%0Atasks%20primarily%20focus%20on%20scenarios%20with%20a%20small%20number%20of%20frames%2C%20no%20text%0Acontrol%2C%20and%20minimal%20differences%20between%20the%20first%20and%20last%20frames.%20Recent%0Acommunity%20developers%20have%20utilized%20large%20video%20models%20represented%20by%20Wan%20to%0Aendow%20frame-to-frame%20capabilities.%20However%2C%20these%20models%20can%20only%20generate%20a%0Afixed%20number%20of%20frames%20and%20often%20fail%20to%20produce%20satisfactory%20results%20for%0Acertain%20frame%20lengths%2C%20while%20this%20setting%20lacks%20a%20clear%20official%20definition%20and%0Aa%20well-established%20benchmark.%20In%20this%20paper%2C%20we%20first%20propose%20a%20new%20practical%0ASemantic%20Frame%20Interpolation%20%28SFI%29%20task%20from%20the%20perspective%20of%20academic%0Adefinition%2C%20which%20covers%20the%20above%20two%20settings%20and%20supports%20inference%20at%0Amultiple%20frame%20rates.%20To%20achieve%20this%20goal%2C%20we%20propose%20a%20novel%20SemFi%20model%0Abuilding%20upon%20Wan2.1%2C%20which%20incorporates%20a%20Mixture-of-LoRA%20module%20to%20ensure%20the%0Ageneration%20of%20high-consistency%20content%20that%20aligns%20with%20control%20conditions%0Aacross%20various%20frame%20length%20limitations.%20Furthermore%2C%20we%20propose%20SFI-300K%2C%20the%0Afirst%20general-purpose%20dataset%20and%20benchmark%20specifically%20designed%20for%20SFI.%20To%0Asupport%20this%2C%20we%20collect%20and%20process%20data%20from%20the%20perspective%20of%20SFI%2C%0Acarefully%20designing%20evaluation%20metrics%20and%20methods%20to%20assess%20the%20model%27s%0Aperformance%20across%20multiple%20dimensions%2C%20encompassing%20image%20and%20video%2C%20and%0Avarious%20aspects%2C%20including%20consistency%20and%20diversity.%20Through%20extensive%0Aexperiments%20on%20SFI-300K%2C%20we%20demonstrate%20that%20our%20method%20is%20particularly%0Awell-suited%20to%20meet%20the%20requirements%20of%20the%20SFI%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05173v1&entry.124074799=Read"},
{"title": "Dynamic EventNeRF: Reconstructing General Dynamic Scenes from Multi-view\n  RGB and Event Streams", "author": "Viktor Rudnev and Gereon Fox and Mohamed Elgharib and Christian Theobalt and Vladislav Golyanik", "abstract": "  Volumetric reconstruction of dynamic scenes is an important problem in\ncomputer vision. It is especially challenging in poor lighting and with fast\nmotion. This is partly due to limitations of RGB cameras: To capture frames\nunder low lighting, the exposure time needs to be increased, which leads to\nmore motion blur. In contrast, event cameras, which record changes in pixel\nbrightness asynchronously, are much less dependent on lighting, making them\nmore suitable for recording fast motion. We hence propose the first method to\nspatiotemporally reconstruct a scene from sparse multi-view event streams and\nsparse RGB frames. We train a sequence of cross-faded time-conditioned NeRF\nmodels, one per short recording segment. The individual segments are supervised\nwith a set of event- and RGB-based losses and sparse-view regularisation. We\nassemble a real-world multi-view camera rig with six static event cameras\naround the object and record a benchmark multi-view event stream dataset of\nchallenging motions. Our work outperforms RGB-based baselines, producing\nstate-of-the-art results, and opens up the topic of multi-view event-based\nreconstruction as a new path for fast scene capture beyond RGB cameras. The\ncode and the data are released at https://4dqv.mpi-inf.mpg.de/DynEventNeRF/\n", "link": "http://arxiv.org/abs/2412.06770v3", "date": "2025-07-07", "relevancy": 2.539, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.646}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6429}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20EventNeRF%3A%20Reconstructing%20General%20Dynamic%20Scenes%20from%20Multi-view%0A%20%20RGB%20and%20Event%20Streams&body=Title%3A%20Dynamic%20EventNeRF%3A%20Reconstructing%20General%20Dynamic%20Scenes%20from%20Multi-view%0A%20%20RGB%20and%20Event%20Streams%0AAuthor%3A%20Viktor%20Rudnev%20and%20Gereon%20Fox%20and%20Mohamed%20Elgharib%20and%20Christian%20Theobalt%20and%20Vladislav%20Golyanik%0AAbstract%3A%20%20%20Volumetric%20reconstruction%20of%20dynamic%20scenes%20is%20an%20important%20problem%20in%0Acomputer%20vision.%20It%20is%20especially%20challenging%20in%20poor%20lighting%20and%20with%20fast%0Amotion.%20This%20is%20partly%20due%20to%20limitations%20of%20RGB%20cameras%3A%20To%20capture%20frames%0Aunder%20low%20lighting%2C%20the%20exposure%20time%20needs%20to%20be%20increased%2C%20which%20leads%20to%0Amore%20motion%20blur.%20In%20contrast%2C%20event%20cameras%2C%20which%20record%20changes%20in%20pixel%0Abrightness%20asynchronously%2C%20are%20much%20less%20dependent%20on%20lighting%2C%20making%20them%0Amore%20suitable%20for%20recording%20fast%20motion.%20We%20hence%20propose%20the%20first%20method%20to%0Aspatiotemporally%20reconstruct%20a%20scene%20from%20sparse%20multi-view%20event%20streams%20and%0Asparse%20RGB%20frames.%20We%20train%20a%20sequence%20of%20cross-faded%20time-conditioned%20NeRF%0Amodels%2C%20one%20per%20short%20recording%20segment.%20The%20individual%20segments%20are%20supervised%0Awith%20a%20set%20of%20event-%20and%20RGB-based%20losses%20and%20sparse-view%20regularisation.%20We%0Aassemble%20a%20real-world%20multi-view%20camera%20rig%20with%20six%20static%20event%20cameras%0Aaround%20the%20object%20and%20record%20a%20benchmark%20multi-view%20event%20stream%20dataset%20of%0Achallenging%20motions.%20Our%20work%20outperforms%20RGB-based%20baselines%2C%20producing%0Astate-of-the-art%20results%2C%20and%20opens%20up%20the%20topic%20of%20multi-view%20event-based%0Areconstruction%20as%20a%20new%20path%20for%20fast%20scene%20capture%20beyond%20RGB%20cameras.%20The%0Acode%20and%20the%20data%20are%20released%20at%20https%3A//4dqv.mpi-inf.mpg.de/DynEventNeRF/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06770v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520EventNeRF%253A%2520Reconstructing%2520General%2520Dynamic%2520Scenes%2520from%2520Multi-view%250A%2520%2520RGB%2520and%2520Event%2520Streams%26entry.906535625%3DViktor%2520Rudnev%2520and%2520Gereon%2520Fox%2520and%2520Mohamed%2520Elgharib%2520and%2520Christian%2520Theobalt%2520and%2520Vladislav%2520Golyanik%26entry.1292438233%3D%2520%2520Volumetric%2520reconstruction%2520of%2520dynamic%2520scenes%2520is%2520an%2520important%2520problem%2520in%250Acomputer%2520vision.%2520It%2520is%2520especially%2520challenging%2520in%2520poor%2520lighting%2520and%2520with%2520fast%250Amotion.%2520This%2520is%2520partly%2520due%2520to%2520limitations%2520of%2520RGB%2520cameras%253A%2520To%2520capture%2520frames%250Aunder%2520low%2520lighting%252C%2520the%2520exposure%2520time%2520needs%2520to%2520be%2520increased%252C%2520which%2520leads%2520to%250Amore%2520motion%2520blur.%2520In%2520contrast%252C%2520event%2520cameras%252C%2520which%2520record%2520changes%2520in%2520pixel%250Abrightness%2520asynchronously%252C%2520are%2520much%2520less%2520dependent%2520on%2520lighting%252C%2520making%2520them%250Amore%2520suitable%2520for%2520recording%2520fast%2520motion.%2520We%2520hence%2520propose%2520the%2520first%2520method%2520to%250Aspatiotemporally%2520reconstruct%2520a%2520scene%2520from%2520sparse%2520multi-view%2520event%2520streams%2520and%250Asparse%2520RGB%2520frames.%2520We%2520train%2520a%2520sequence%2520of%2520cross-faded%2520time-conditioned%2520NeRF%250Amodels%252C%2520one%2520per%2520short%2520recording%2520segment.%2520The%2520individual%2520segments%2520are%2520supervised%250Awith%2520a%2520set%2520of%2520event-%2520and%2520RGB-based%2520losses%2520and%2520sparse-view%2520regularisation.%2520We%250Aassemble%2520a%2520real-world%2520multi-view%2520camera%2520rig%2520with%2520six%2520static%2520event%2520cameras%250Aaround%2520the%2520object%2520and%2520record%2520a%2520benchmark%2520multi-view%2520event%2520stream%2520dataset%2520of%250Achallenging%2520motions.%2520Our%2520work%2520outperforms%2520RGB-based%2520baselines%252C%2520producing%250Astate-of-the-art%2520results%252C%2520and%2520opens%2520up%2520the%2520topic%2520of%2520multi-view%2520event-based%250Areconstruction%2520as%2520a%2520new%2520path%2520for%2520fast%2520scene%2520capture%2520beyond%2520RGB%2520cameras.%2520The%250Acode%2520and%2520the%2520data%2520are%2520released%2520at%2520https%253A//4dqv.mpi-inf.mpg.de/DynEventNeRF/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06770v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20EventNeRF%3A%20Reconstructing%20General%20Dynamic%20Scenes%20from%20Multi-view%0A%20%20RGB%20and%20Event%20Streams&entry.906535625=Viktor%20Rudnev%20and%20Gereon%20Fox%20and%20Mohamed%20Elgharib%20and%20Christian%20Theobalt%20and%20Vladislav%20Golyanik&entry.1292438233=%20%20Volumetric%20reconstruction%20of%20dynamic%20scenes%20is%20an%20important%20problem%20in%0Acomputer%20vision.%20It%20is%20especially%20challenging%20in%20poor%20lighting%20and%20with%20fast%0Amotion.%20This%20is%20partly%20due%20to%20limitations%20of%20RGB%20cameras%3A%20To%20capture%20frames%0Aunder%20low%20lighting%2C%20the%20exposure%20time%20needs%20to%20be%20increased%2C%20which%20leads%20to%0Amore%20motion%20blur.%20In%20contrast%2C%20event%20cameras%2C%20which%20record%20changes%20in%20pixel%0Abrightness%20asynchronously%2C%20are%20much%20less%20dependent%20on%20lighting%2C%20making%20them%0Amore%20suitable%20for%20recording%20fast%20motion.%20We%20hence%20propose%20the%20first%20method%20to%0Aspatiotemporally%20reconstruct%20a%20scene%20from%20sparse%20multi-view%20event%20streams%20and%0Asparse%20RGB%20frames.%20We%20train%20a%20sequence%20of%20cross-faded%20time-conditioned%20NeRF%0Amodels%2C%20one%20per%20short%20recording%20segment.%20The%20individual%20segments%20are%20supervised%0Awith%20a%20set%20of%20event-%20and%20RGB-based%20losses%20and%20sparse-view%20regularisation.%20We%0Aassemble%20a%20real-world%20multi-view%20camera%20rig%20with%20six%20static%20event%20cameras%0Aaround%20the%20object%20and%20record%20a%20benchmark%20multi-view%20event%20stream%20dataset%20of%0Achallenging%20motions.%20Our%20work%20outperforms%20RGB-based%20baselines%2C%20producing%0Astate-of-the-art%20results%2C%20and%20opens%20up%20the%20topic%20of%20multi-view%20event-based%0Areconstruction%20as%20a%20new%20path%20for%20fast%20scene%20capture%20beyond%20RGB%20cameras.%20The%0Acode%20and%20the%20data%20are%20released%20at%20https%3A//4dqv.mpi-inf.mpg.de/DynEventNeRF/%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06770v3&entry.124074799=Read"},
{"title": "HGNet: High-Order Spatial Awareness Hypergraph and Multi-Scale Context\n  Attention Network for Colorectal Polyp Detection", "author": "Xiaofang Liu and Lingling Sun and Xuqing Zhang and Yuannong Ye and Bin zhao", "abstract": "  Colorectal cancer (CRC) is closely linked to the malignant transformation of\ncolorectal polyps, making early detection essential. However, current models\nstruggle with detecting small lesions, accurately localizing boundaries, and\nproviding interpretable decisions. To address these issues, we propose HGNet,\nwhich integrates High-Order Spatial Awareness Hypergraph and Multi-Scale\nContext Attention. Key innovations include: (1) an Efficient Multi-Scale\nContext Attention (EMCA) module to enhance lesion feature representation and\nboundary modeling; (2) the deployment of a spatial hypergraph convolution\nmodule before the detection head to capture higher-order spatial relationships\nbetween nodes; (3) the application of transfer learning to address the scarcity\nof medical image data; and (4) Eigen Class Activation Map (Eigen-CAM) for\ndecision visualization. Experimental results show that HGNet achieves 94%\naccuracy, 90.6% recall, and 90% mAP@0.5, significantly improving small lesion\ndifferentiation and clinical interpretability. The source code will be made\npublicly available upon publication of this paper.\n", "link": "http://arxiv.org/abs/2507.04880v1", "date": "2025-07-07", "relevancy": 2.5254, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.51}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5086}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HGNet%3A%20High-Order%20Spatial%20Awareness%20Hypergraph%20and%20Multi-Scale%20Context%0A%20%20Attention%20Network%20for%20Colorectal%20Polyp%20Detection&body=Title%3A%20HGNet%3A%20High-Order%20Spatial%20Awareness%20Hypergraph%20and%20Multi-Scale%20Context%0A%20%20Attention%20Network%20for%20Colorectal%20Polyp%20Detection%0AAuthor%3A%20Xiaofang%20Liu%20and%20Lingling%20Sun%20and%20Xuqing%20Zhang%20and%20Yuannong%20Ye%20and%20Bin%20zhao%0AAbstract%3A%20%20%20Colorectal%20cancer%20%28CRC%29%20is%20closely%20linked%20to%20the%20malignant%20transformation%20of%0Acolorectal%20polyps%2C%20making%20early%20detection%20essential.%20However%2C%20current%20models%0Astruggle%20with%20detecting%20small%20lesions%2C%20accurately%20localizing%20boundaries%2C%20and%0Aproviding%20interpretable%20decisions.%20To%20address%20these%20issues%2C%20we%20propose%20HGNet%2C%0Awhich%20integrates%20High-Order%20Spatial%20Awareness%20Hypergraph%20and%20Multi-Scale%0AContext%20Attention.%20Key%20innovations%20include%3A%20%281%29%20an%20Efficient%20Multi-Scale%0AContext%20Attention%20%28EMCA%29%20module%20to%20enhance%20lesion%20feature%20representation%20and%0Aboundary%20modeling%3B%20%282%29%20the%20deployment%20of%20a%20spatial%20hypergraph%20convolution%0Amodule%20before%20the%20detection%20head%20to%20capture%20higher-order%20spatial%20relationships%0Abetween%20nodes%3B%20%283%29%20the%20application%20of%20transfer%20learning%20to%20address%20the%20scarcity%0Aof%20medical%20image%20data%3B%20and%20%284%29%20Eigen%20Class%20Activation%20Map%20%28Eigen-CAM%29%20for%0Adecision%20visualization.%20Experimental%20results%20show%20that%20HGNet%20achieves%2094%25%0Aaccuracy%2C%2090.6%25%20recall%2C%20and%2090%25%20mAP%400.5%2C%20significantly%20improving%20small%20lesion%0Adifferentiation%20and%20clinical%20interpretability.%20The%20source%20code%20will%20be%20made%0Apublicly%20available%20upon%20publication%20of%20this%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04880v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHGNet%253A%2520High-Order%2520Spatial%2520Awareness%2520Hypergraph%2520and%2520Multi-Scale%2520Context%250A%2520%2520Attention%2520Network%2520for%2520Colorectal%2520Polyp%2520Detection%26entry.906535625%3DXiaofang%2520Liu%2520and%2520Lingling%2520Sun%2520and%2520Xuqing%2520Zhang%2520and%2520Yuannong%2520Ye%2520and%2520Bin%2520zhao%26entry.1292438233%3D%2520%2520Colorectal%2520cancer%2520%2528CRC%2529%2520is%2520closely%2520linked%2520to%2520the%2520malignant%2520transformation%2520of%250Acolorectal%2520polyps%252C%2520making%2520early%2520detection%2520essential.%2520However%252C%2520current%2520models%250Astruggle%2520with%2520detecting%2520small%2520lesions%252C%2520accurately%2520localizing%2520boundaries%252C%2520and%250Aproviding%2520interpretable%2520decisions.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520HGNet%252C%250Awhich%2520integrates%2520High-Order%2520Spatial%2520Awareness%2520Hypergraph%2520and%2520Multi-Scale%250AContext%2520Attention.%2520Key%2520innovations%2520include%253A%2520%25281%2529%2520an%2520Efficient%2520Multi-Scale%250AContext%2520Attention%2520%2528EMCA%2529%2520module%2520to%2520enhance%2520lesion%2520feature%2520representation%2520and%250Aboundary%2520modeling%253B%2520%25282%2529%2520the%2520deployment%2520of%2520a%2520spatial%2520hypergraph%2520convolution%250Amodule%2520before%2520the%2520detection%2520head%2520to%2520capture%2520higher-order%2520spatial%2520relationships%250Abetween%2520nodes%253B%2520%25283%2529%2520the%2520application%2520of%2520transfer%2520learning%2520to%2520address%2520the%2520scarcity%250Aof%2520medical%2520image%2520data%253B%2520and%2520%25284%2529%2520Eigen%2520Class%2520Activation%2520Map%2520%2528Eigen-CAM%2529%2520for%250Adecision%2520visualization.%2520Experimental%2520results%2520show%2520that%2520HGNet%2520achieves%252094%2525%250Aaccuracy%252C%252090.6%2525%2520recall%252C%2520and%252090%2525%2520mAP%25400.5%252C%2520significantly%2520improving%2520small%2520lesion%250Adifferentiation%2520and%2520clinical%2520interpretability.%2520The%2520source%2520code%2520will%2520be%2520made%250Apublicly%2520available%2520upon%2520publication%2520of%2520this%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04880v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HGNet%3A%20High-Order%20Spatial%20Awareness%20Hypergraph%20and%20Multi-Scale%20Context%0A%20%20Attention%20Network%20for%20Colorectal%20Polyp%20Detection&entry.906535625=Xiaofang%20Liu%20and%20Lingling%20Sun%20and%20Xuqing%20Zhang%20and%20Yuannong%20Ye%20and%20Bin%20zhao&entry.1292438233=%20%20Colorectal%20cancer%20%28CRC%29%20is%20closely%20linked%20to%20the%20malignant%20transformation%20of%0Acolorectal%20polyps%2C%20making%20early%20detection%20essential.%20However%2C%20current%20models%0Astruggle%20with%20detecting%20small%20lesions%2C%20accurately%20localizing%20boundaries%2C%20and%0Aproviding%20interpretable%20decisions.%20To%20address%20these%20issues%2C%20we%20propose%20HGNet%2C%0Awhich%20integrates%20High-Order%20Spatial%20Awareness%20Hypergraph%20and%20Multi-Scale%0AContext%20Attention.%20Key%20innovations%20include%3A%20%281%29%20an%20Efficient%20Multi-Scale%0AContext%20Attention%20%28EMCA%29%20module%20to%20enhance%20lesion%20feature%20representation%20and%0Aboundary%20modeling%3B%20%282%29%20the%20deployment%20of%20a%20spatial%20hypergraph%20convolution%0Amodule%20before%20the%20detection%20head%20to%20capture%20higher-order%20spatial%20relationships%0Abetween%20nodes%3B%20%283%29%20the%20application%20of%20transfer%20learning%20to%20address%20the%20scarcity%0Aof%20medical%20image%20data%3B%20and%20%284%29%20Eigen%20Class%20Activation%20Map%20%28Eigen-CAM%29%20for%0Adecision%20visualization.%20Experimental%20results%20show%20that%20HGNet%20achieves%2094%25%0Aaccuracy%2C%2090.6%25%20recall%2C%20and%2090%25%20mAP%400.5%2C%20significantly%20improving%20small%20lesion%0Adifferentiation%20and%20clinical%20interpretability.%20The%20source%20code%20will%20be%20made%0Apublicly%20available%20upon%20publication%20of%20this%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04880v1&entry.124074799=Read"},
{"title": "Critiques of World Models", "author": "Eric Xing and Mingkai Deng and Jinyu Hou and Zhiting Hu", "abstract": "  World Model, the supposed algorithmic surrogate of the real-world environment\nwhich biological agents experience with and act upon, has been an emerging\ntopic in recent years because of the rising needs to develop virtual agents\nwith artificial (general) intelligence. There has been much debate on what a\nworld model really is, how to build it, how to use it, and how to evaluate it.\nIn this essay, starting from the imagination in the famed Sci-Fi classic Dune,\nand drawing inspiration from the concept of \"hypothetical thinking\" in\npsychology literature, we offer critiques of several schools of thoughts on\nworld modeling, and argue the primary goal of a world model to be simulating\nall actionable possibilities of the real world for purposeful reasoning and\nacting. Building on the critiques, we propose a new architecture for a\ngeneral-purpose world model, based on hierarchical, multi-level, and mixed\ncontinuous/discrete representations, and a generative and self-supervision\nlearning framework, with an outlook of a Physical, Agentic, and Nested (PAN)\nAGI system enabled by such a model.\n", "link": "http://arxiv.org/abs/2507.05169v1", "date": "2025-07-07", "relevancy": 2.4951, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4825}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Critiques%20of%20World%20Models&body=Title%3A%20Critiques%20of%20World%20Models%0AAuthor%3A%20Eric%20Xing%20and%20Mingkai%20Deng%20and%20Jinyu%20Hou%20and%20Zhiting%20Hu%0AAbstract%3A%20%20%20World%20Model%2C%20the%20supposed%20algorithmic%20surrogate%20of%20the%20real-world%20environment%0Awhich%20biological%20agents%20experience%20with%20and%20act%20upon%2C%20has%20been%20an%20emerging%0Atopic%20in%20recent%20years%20because%20of%20the%20rising%20needs%20to%20develop%20virtual%20agents%0Awith%20artificial%20%28general%29%20intelligence.%20There%20has%20been%20much%20debate%20on%20what%20a%0Aworld%20model%20really%20is%2C%20how%20to%20build%20it%2C%20how%20to%20use%20it%2C%20and%20how%20to%20evaluate%20it.%0AIn%20this%20essay%2C%20starting%20from%20the%20imagination%20in%20the%20famed%20Sci-Fi%20classic%20Dune%2C%0Aand%20drawing%20inspiration%20from%20the%20concept%20of%20%22hypothetical%20thinking%22%20in%0Apsychology%20literature%2C%20we%20offer%20critiques%20of%20several%20schools%20of%20thoughts%20on%0Aworld%20modeling%2C%20and%20argue%20the%20primary%20goal%20of%20a%20world%20model%20to%20be%20simulating%0Aall%20actionable%20possibilities%20of%20the%20real%20world%20for%20purposeful%20reasoning%20and%0Aacting.%20Building%20on%20the%20critiques%2C%20we%20propose%20a%20new%20architecture%20for%20a%0Ageneral-purpose%20world%20model%2C%20based%20on%20hierarchical%2C%20multi-level%2C%20and%20mixed%0Acontinuous/discrete%20representations%2C%20and%20a%20generative%20and%20self-supervision%0Alearning%20framework%2C%20with%20an%20outlook%20of%20a%20Physical%2C%20Agentic%2C%20and%20Nested%20%28PAN%29%0AAGI%20system%20enabled%20by%20such%20a%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05169v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCritiques%2520of%2520World%2520Models%26entry.906535625%3DEric%2520Xing%2520and%2520Mingkai%2520Deng%2520and%2520Jinyu%2520Hou%2520and%2520Zhiting%2520Hu%26entry.1292438233%3D%2520%2520World%2520Model%252C%2520the%2520supposed%2520algorithmic%2520surrogate%2520of%2520the%2520real-world%2520environment%250Awhich%2520biological%2520agents%2520experience%2520with%2520and%2520act%2520upon%252C%2520has%2520been%2520an%2520emerging%250Atopic%2520in%2520recent%2520years%2520because%2520of%2520the%2520rising%2520needs%2520to%2520develop%2520virtual%2520agents%250Awith%2520artificial%2520%2528general%2529%2520intelligence.%2520There%2520has%2520been%2520much%2520debate%2520on%2520what%2520a%250Aworld%2520model%2520really%2520is%252C%2520how%2520to%2520build%2520it%252C%2520how%2520to%2520use%2520it%252C%2520and%2520how%2520to%2520evaluate%2520it.%250AIn%2520this%2520essay%252C%2520starting%2520from%2520the%2520imagination%2520in%2520the%2520famed%2520Sci-Fi%2520classic%2520Dune%252C%250Aand%2520drawing%2520inspiration%2520from%2520the%2520concept%2520of%2520%2522hypothetical%2520thinking%2522%2520in%250Apsychology%2520literature%252C%2520we%2520offer%2520critiques%2520of%2520several%2520schools%2520of%2520thoughts%2520on%250Aworld%2520modeling%252C%2520and%2520argue%2520the%2520primary%2520goal%2520of%2520a%2520world%2520model%2520to%2520be%2520simulating%250Aall%2520actionable%2520possibilities%2520of%2520the%2520real%2520world%2520for%2520purposeful%2520reasoning%2520and%250Aacting.%2520Building%2520on%2520the%2520critiques%252C%2520we%2520propose%2520a%2520new%2520architecture%2520for%2520a%250Ageneral-purpose%2520world%2520model%252C%2520based%2520on%2520hierarchical%252C%2520multi-level%252C%2520and%2520mixed%250Acontinuous/discrete%2520representations%252C%2520and%2520a%2520generative%2520and%2520self-supervision%250Alearning%2520framework%252C%2520with%2520an%2520outlook%2520of%2520a%2520Physical%252C%2520Agentic%252C%2520and%2520Nested%2520%2528PAN%2529%250AAGI%2520system%2520enabled%2520by%2520such%2520a%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05169v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Critiques%20of%20World%20Models&entry.906535625=Eric%20Xing%20and%20Mingkai%20Deng%20and%20Jinyu%20Hou%20and%20Zhiting%20Hu&entry.1292438233=%20%20World%20Model%2C%20the%20supposed%20algorithmic%20surrogate%20of%20the%20real-world%20environment%0Awhich%20biological%20agents%20experience%20with%20and%20act%20upon%2C%20has%20been%20an%20emerging%0Atopic%20in%20recent%20years%20because%20of%20the%20rising%20needs%20to%20develop%20virtual%20agents%0Awith%20artificial%20%28general%29%20intelligence.%20There%20has%20been%20much%20debate%20on%20what%20a%0Aworld%20model%20really%20is%2C%20how%20to%20build%20it%2C%20how%20to%20use%20it%2C%20and%20how%20to%20evaluate%20it.%0AIn%20this%20essay%2C%20starting%20from%20the%20imagination%20in%20the%20famed%20Sci-Fi%20classic%20Dune%2C%0Aand%20drawing%20inspiration%20from%20the%20concept%20of%20%22hypothetical%20thinking%22%20in%0Apsychology%20literature%2C%20we%20offer%20critiques%20of%20several%20schools%20of%20thoughts%20on%0Aworld%20modeling%2C%20and%20argue%20the%20primary%20goal%20of%20a%20world%20model%20to%20be%20simulating%0Aall%20actionable%20possibilities%20of%20the%20real%20world%20for%20purposeful%20reasoning%20and%0Aacting.%20Building%20on%20the%20critiques%2C%20we%20propose%20a%20new%20architecture%20for%20a%0Ageneral-purpose%20world%20model%2C%20based%20on%20hierarchical%2C%20multi-level%2C%20and%20mixed%0Acontinuous/discrete%20representations%2C%20and%20a%20generative%20and%20self-supervision%0Alearning%20framework%2C%20with%20an%20outlook%20of%20a%20Physical%2C%20Agentic%2C%20and%20Nested%20%28PAN%29%0AAGI%20system%20enabled%20by%20such%20a%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05169v1&entry.124074799=Read"},
{"title": "4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous\n  Capture", "author": "Yutian Chen and Shi Guo and Tianshuo Yang and Lihe Ding and Xiuyuan Yu and Jinwei Gu and Tianfan Xue", "abstract": "  Reconstructing fast-dynamic scenes from multi-view videos is crucial for\nhigh-speed motion analysis and realistic 4D reconstruction. However, the\nmajority of 4D capture systems are limited to frame rates below 30 FPS (frames\nper second), and a direct 4D reconstruction of high-speed motion from low FPS\ninput may lead to undesirable results. In this work, we propose a high-speed 4D\ncapturing system only using low FPS cameras, through novel capturing and\nprocessing modules. On the capturing side, we propose an asynchronous capture\nscheme that increases the effective frame rate by staggering the start times of\ncameras. By grouping cameras and leveraging a base frame rate of 25 FPS, our\nmethod achieves an equivalent frame rate of 100-200 FPS without requiring\nspecialized high-speed cameras. On processing side, we also propose a novel\ngenerative model to fix artifacts caused by 4D sparse-view reconstruction, as\nasynchrony reduces the number of viewpoints at each timestamp. Specifically, we\npropose to train a video-diffusion-based artifact-fix model for sparse 4D\nreconstruction, which refines missing details, maintains temporal consistency,\nand improves overall reconstruction quality. Experimental results demonstrate\nthat our method significantly enhances high-speed 4D reconstruction compared to\nsynchronous capture.\n", "link": "http://arxiv.org/abs/2507.05163v1", "date": "2025-07-07", "relevancy": 2.459, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6348}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6007}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204DSloMo%3A%204D%20Reconstruction%20for%20High%20Speed%20Scene%20with%20Asynchronous%0A%20%20Capture&body=Title%3A%204DSloMo%3A%204D%20Reconstruction%20for%20High%20Speed%20Scene%20with%20Asynchronous%0A%20%20Capture%0AAuthor%3A%20Yutian%20Chen%20and%20Shi%20Guo%20and%20Tianshuo%20Yang%20and%20Lihe%20Ding%20and%20Xiuyuan%20Yu%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue%0AAbstract%3A%20%20%20Reconstructing%20fast-dynamic%20scenes%20from%20multi-view%20videos%20is%20crucial%20for%0Ahigh-speed%20motion%20analysis%20and%20realistic%204D%20reconstruction.%20However%2C%20the%0Amajority%20of%204D%20capture%20systems%20are%20limited%20to%20frame%20rates%20below%2030%20FPS%20%28frames%0Aper%20second%29%2C%20and%20a%20direct%204D%20reconstruction%20of%20high-speed%20motion%20from%20low%20FPS%0Ainput%20may%20lead%20to%20undesirable%20results.%20In%20this%20work%2C%20we%20propose%20a%20high-speed%204D%0Acapturing%20system%20only%20using%20low%20FPS%20cameras%2C%20through%20novel%20capturing%20and%0Aprocessing%20modules.%20On%20the%20capturing%20side%2C%20we%20propose%20an%20asynchronous%20capture%0Ascheme%20that%20increases%20the%20effective%20frame%20rate%20by%20staggering%20the%20start%20times%20of%0Acameras.%20By%20grouping%20cameras%20and%20leveraging%20a%20base%20frame%20rate%20of%2025%20FPS%2C%20our%0Amethod%20achieves%20an%20equivalent%20frame%20rate%20of%20100-200%20FPS%20without%20requiring%0Aspecialized%20high-speed%20cameras.%20On%20processing%20side%2C%20we%20also%20propose%20a%20novel%0Agenerative%20model%20to%20fix%20artifacts%20caused%20by%204D%20sparse-view%20reconstruction%2C%20as%0Aasynchrony%20reduces%20the%20number%20of%20viewpoints%20at%20each%20timestamp.%20Specifically%2C%20we%0Apropose%20to%20train%20a%20video-diffusion-based%20artifact-fix%20model%20for%20sparse%204D%0Areconstruction%2C%20which%20refines%20missing%20details%2C%20maintains%20temporal%20consistency%2C%0Aand%20improves%20overall%20reconstruction%20quality.%20Experimental%20results%20demonstrate%0Athat%20our%20method%20significantly%20enhances%20high-speed%204D%20reconstruction%20compared%20to%0Asynchronous%20capture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4DSloMo%253A%25204D%2520Reconstruction%2520for%2520High%2520Speed%2520Scene%2520with%2520Asynchronous%250A%2520%2520Capture%26entry.906535625%3DYutian%2520Chen%2520and%2520Shi%2520Guo%2520and%2520Tianshuo%2520Yang%2520and%2520Lihe%2520Ding%2520and%2520Xiuyuan%2520Yu%2520and%2520Jinwei%2520Gu%2520and%2520Tianfan%2520Xue%26entry.1292438233%3D%2520%2520Reconstructing%2520fast-dynamic%2520scenes%2520from%2520multi-view%2520videos%2520is%2520crucial%2520for%250Ahigh-speed%2520motion%2520analysis%2520and%2520realistic%25204D%2520reconstruction.%2520However%252C%2520the%250Amajority%2520of%25204D%2520capture%2520systems%2520are%2520limited%2520to%2520frame%2520rates%2520below%252030%2520FPS%2520%2528frames%250Aper%2520second%2529%252C%2520and%2520a%2520direct%25204D%2520reconstruction%2520of%2520high-speed%2520motion%2520from%2520low%2520FPS%250Ainput%2520may%2520lead%2520to%2520undesirable%2520results.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520high-speed%25204D%250Acapturing%2520system%2520only%2520using%2520low%2520FPS%2520cameras%252C%2520through%2520novel%2520capturing%2520and%250Aprocessing%2520modules.%2520On%2520the%2520capturing%2520side%252C%2520we%2520propose%2520an%2520asynchronous%2520capture%250Ascheme%2520that%2520increases%2520the%2520effective%2520frame%2520rate%2520by%2520staggering%2520the%2520start%2520times%2520of%250Acameras.%2520By%2520grouping%2520cameras%2520and%2520leveraging%2520a%2520base%2520frame%2520rate%2520of%252025%2520FPS%252C%2520our%250Amethod%2520achieves%2520an%2520equivalent%2520frame%2520rate%2520of%2520100-200%2520FPS%2520without%2520requiring%250Aspecialized%2520high-speed%2520cameras.%2520On%2520processing%2520side%252C%2520we%2520also%2520propose%2520a%2520novel%250Agenerative%2520model%2520to%2520fix%2520artifacts%2520caused%2520by%25204D%2520sparse-view%2520reconstruction%252C%2520as%250Aasynchrony%2520reduces%2520the%2520number%2520of%2520viewpoints%2520at%2520each%2520timestamp.%2520Specifically%252C%2520we%250Apropose%2520to%2520train%2520a%2520video-diffusion-based%2520artifact-fix%2520model%2520for%2520sparse%25204D%250Areconstruction%252C%2520which%2520refines%2520missing%2520details%252C%2520maintains%2520temporal%2520consistency%252C%250Aand%2520improves%2520overall%2520reconstruction%2520quality.%2520Experimental%2520results%2520demonstrate%250Athat%2520our%2520method%2520significantly%2520enhances%2520high-speed%25204D%2520reconstruction%2520compared%2520to%250Asynchronous%2520capture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4DSloMo%3A%204D%20Reconstruction%20for%20High%20Speed%20Scene%20with%20Asynchronous%0A%20%20Capture&entry.906535625=Yutian%20Chen%20and%20Shi%20Guo%20and%20Tianshuo%20Yang%20and%20Lihe%20Ding%20and%20Xiuyuan%20Yu%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue&entry.1292438233=%20%20Reconstructing%20fast-dynamic%20scenes%20from%20multi-view%20videos%20is%20crucial%20for%0Ahigh-speed%20motion%20analysis%20and%20realistic%204D%20reconstruction.%20However%2C%20the%0Amajority%20of%204D%20capture%20systems%20are%20limited%20to%20frame%20rates%20below%2030%20FPS%20%28frames%0Aper%20second%29%2C%20and%20a%20direct%204D%20reconstruction%20of%20high-speed%20motion%20from%20low%20FPS%0Ainput%20may%20lead%20to%20undesirable%20results.%20In%20this%20work%2C%20we%20propose%20a%20high-speed%204D%0Acapturing%20system%20only%20using%20low%20FPS%20cameras%2C%20through%20novel%20capturing%20and%0Aprocessing%20modules.%20On%20the%20capturing%20side%2C%20we%20propose%20an%20asynchronous%20capture%0Ascheme%20that%20increases%20the%20effective%20frame%20rate%20by%20staggering%20the%20start%20times%20of%0Acameras.%20By%20grouping%20cameras%20and%20leveraging%20a%20base%20frame%20rate%20of%2025%20FPS%2C%20our%0Amethod%20achieves%20an%20equivalent%20frame%20rate%20of%20100-200%20FPS%20without%20requiring%0Aspecialized%20high-speed%20cameras.%20On%20processing%20side%2C%20we%20also%20propose%20a%20novel%0Agenerative%20model%20to%20fix%20artifacts%20caused%20by%204D%20sparse-view%20reconstruction%2C%20as%0Aasynchrony%20reduces%20the%20number%20of%20viewpoints%20at%20each%20timestamp.%20Specifically%2C%20we%0Apropose%20to%20train%20a%20video-diffusion-based%20artifact-fix%20model%20for%20sparse%204D%0Areconstruction%2C%20which%20refines%20missing%20details%2C%20maintains%20temporal%20consistency%2C%0Aand%20improves%20overall%20reconstruction%20quality.%20Experimental%20results%20demonstrate%0Athat%20our%20method%20significantly%20enhances%20high-speed%204D%20reconstruction%20compared%20to%0Asynchronous%20capture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05163v1&entry.124074799=Read"},
{"title": "UniForm: A Unified Multi-Task Diffusion Transformer for Audio-Video\n  Generation", "author": "Lei Zhao and Linfeng Feng and Dongxu Ge and Rujin Chen and Fangqiu Yi and Chi Zhang and Xiao-Lei Zhang and Xuelong Li", "abstract": "  With the rise of diffusion models, audio-video generation has been\nrevolutionized. However, most existing methods rely on separate modules for\neach modality, with limited exploration of unified generative architectures. In\naddition, many are confined to a single task and small-scale datasets. To\novercome these limitations, we introduce UniForm, a unified multi-task\ndiffusion transformer that generates both audio and visual modalities in a\nshared latent space. By using a unified denoising network, UniForm captures the\ninherent correlations between sound and vision. Additionally, we propose\ntask-specific noise schemes and task tokens, enabling the model to support\nmultiple tasks with a single set of parameters, including video-to-audio,\naudio-to-video and text-to-audio-video generation. Furthermore, by leveraging\nlarge language models and a large-scale text-audio-video combined dataset,\nUniForm achieves greater generative diversity than prior approaches.\nExperiments show that UniForm achieves performance close to the\nstate-of-the-art single-task models across three generation tasks, with\ngenerated content that is not only highly aligned with real-world data\ndistributions but also enables more diverse and fine-grained generation.\n", "link": "http://arxiv.org/abs/2502.03897v5", "date": "2025-07-07", "relevancy": 2.4554, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6313}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.622}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniForm%3A%20A%20Unified%20Multi-Task%20Diffusion%20Transformer%20for%20Audio-Video%0A%20%20Generation&body=Title%3A%20UniForm%3A%20A%20Unified%20Multi-Task%20Diffusion%20Transformer%20for%20Audio-Video%0A%20%20Generation%0AAuthor%3A%20Lei%20Zhao%20and%20Linfeng%20Feng%20and%20Dongxu%20Ge%20and%20Rujin%20Chen%20and%20Fangqiu%20Yi%20and%20Chi%20Zhang%20and%20Xiao-Lei%20Zhang%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20With%20the%20rise%20of%20diffusion%20models%2C%20audio-video%20generation%20has%20been%0Arevolutionized.%20However%2C%20most%20existing%20methods%20rely%20on%20separate%20modules%20for%0Aeach%20modality%2C%20with%20limited%20exploration%20of%20unified%20generative%20architectures.%20In%0Aaddition%2C%20many%20are%20confined%20to%20a%20single%20task%20and%20small-scale%20datasets.%20To%0Aovercome%20these%20limitations%2C%20we%20introduce%20UniForm%2C%20a%20unified%20multi-task%0Adiffusion%20transformer%20that%20generates%20both%20audio%20and%20visual%20modalities%20in%20a%0Ashared%20latent%20space.%20By%20using%20a%20unified%20denoising%20network%2C%20UniForm%20captures%20the%0Ainherent%20correlations%20between%20sound%20and%20vision.%20Additionally%2C%20we%20propose%0Atask-specific%20noise%20schemes%20and%20task%20tokens%2C%20enabling%20the%20model%20to%20support%0Amultiple%20tasks%20with%20a%20single%20set%20of%20parameters%2C%20including%20video-to-audio%2C%0Aaudio-to-video%20and%20text-to-audio-video%20generation.%20Furthermore%2C%20by%20leveraging%0Alarge%20language%20models%20and%20a%20large-scale%20text-audio-video%20combined%20dataset%2C%0AUniForm%20achieves%20greater%20generative%20diversity%20than%20prior%20approaches.%0AExperiments%20show%20that%20UniForm%20achieves%20performance%20close%20to%20the%0Astate-of-the-art%20single-task%20models%20across%20three%20generation%20tasks%2C%20with%0Agenerated%20content%20that%20is%20not%20only%20highly%20aligned%20with%20real-world%20data%0Adistributions%20but%20also%20enables%20more%20diverse%20and%20fine-grained%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03897v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniForm%253A%2520A%2520Unified%2520Multi-Task%2520Diffusion%2520Transformer%2520for%2520Audio-Video%250A%2520%2520Generation%26entry.906535625%3DLei%2520Zhao%2520and%2520Linfeng%2520Feng%2520and%2520Dongxu%2520Ge%2520and%2520Rujin%2520Chen%2520and%2520Fangqiu%2520Yi%2520and%2520Chi%2520Zhang%2520and%2520Xiao-Lei%2520Zhang%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520With%2520the%2520rise%2520of%2520diffusion%2520models%252C%2520audio-video%2520generation%2520has%2520been%250Arevolutionized.%2520However%252C%2520most%2520existing%2520methods%2520rely%2520on%2520separate%2520modules%2520for%250Aeach%2520modality%252C%2520with%2520limited%2520exploration%2520of%2520unified%2520generative%2520architectures.%2520In%250Aaddition%252C%2520many%2520are%2520confined%2520to%2520a%2520single%2520task%2520and%2520small-scale%2520datasets.%2520To%250Aovercome%2520these%2520limitations%252C%2520we%2520introduce%2520UniForm%252C%2520a%2520unified%2520multi-task%250Adiffusion%2520transformer%2520that%2520generates%2520both%2520audio%2520and%2520visual%2520modalities%2520in%2520a%250Ashared%2520latent%2520space.%2520By%2520using%2520a%2520unified%2520denoising%2520network%252C%2520UniForm%2520captures%2520the%250Ainherent%2520correlations%2520between%2520sound%2520and%2520vision.%2520Additionally%252C%2520we%2520propose%250Atask-specific%2520noise%2520schemes%2520and%2520task%2520tokens%252C%2520enabling%2520the%2520model%2520to%2520support%250Amultiple%2520tasks%2520with%2520a%2520single%2520set%2520of%2520parameters%252C%2520including%2520video-to-audio%252C%250Aaudio-to-video%2520and%2520text-to-audio-video%2520generation.%2520Furthermore%252C%2520by%2520leveraging%250Alarge%2520language%2520models%2520and%2520a%2520large-scale%2520text-audio-video%2520combined%2520dataset%252C%250AUniForm%2520achieves%2520greater%2520generative%2520diversity%2520than%2520prior%2520approaches.%250AExperiments%2520show%2520that%2520UniForm%2520achieves%2520performance%2520close%2520to%2520the%250Astate-of-the-art%2520single-task%2520models%2520across%2520three%2520generation%2520tasks%252C%2520with%250Agenerated%2520content%2520that%2520is%2520not%2520only%2520highly%2520aligned%2520with%2520real-world%2520data%250Adistributions%2520but%2520also%2520enables%2520more%2520diverse%2520and%2520fine-grained%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03897v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniForm%3A%20A%20Unified%20Multi-Task%20Diffusion%20Transformer%20for%20Audio-Video%0A%20%20Generation&entry.906535625=Lei%20Zhao%20and%20Linfeng%20Feng%20and%20Dongxu%20Ge%20and%20Rujin%20Chen%20and%20Fangqiu%20Yi%20and%20Chi%20Zhang%20and%20Xiao-Lei%20Zhang%20and%20Xuelong%20Li&entry.1292438233=%20%20With%20the%20rise%20of%20diffusion%20models%2C%20audio-video%20generation%20has%20been%0Arevolutionized.%20However%2C%20most%20existing%20methods%20rely%20on%20separate%20modules%20for%0Aeach%20modality%2C%20with%20limited%20exploration%20of%20unified%20generative%20architectures.%20In%0Aaddition%2C%20many%20are%20confined%20to%20a%20single%20task%20and%20small-scale%20datasets.%20To%0Aovercome%20these%20limitations%2C%20we%20introduce%20UniForm%2C%20a%20unified%20multi-task%0Adiffusion%20transformer%20that%20generates%20both%20audio%20and%20visual%20modalities%20in%20a%0Ashared%20latent%20space.%20By%20using%20a%20unified%20denoising%20network%2C%20UniForm%20captures%20the%0Ainherent%20correlations%20between%20sound%20and%20vision.%20Additionally%2C%20we%20propose%0Atask-specific%20noise%20schemes%20and%20task%20tokens%2C%20enabling%20the%20model%20to%20support%0Amultiple%20tasks%20with%20a%20single%20set%20of%20parameters%2C%20including%20video-to-audio%2C%0Aaudio-to-video%20and%20text-to-audio-video%20generation.%20Furthermore%2C%20by%20leveraging%0Alarge%20language%20models%20and%20a%20large-scale%20text-audio-video%20combined%20dataset%2C%0AUniForm%20achieves%20greater%20generative%20diversity%20than%20prior%20approaches.%0AExperiments%20show%20that%20UniForm%20achieves%20performance%20close%20to%20the%0Astate-of-the-art%20single-task%20models%20across%20three%20generation%20tasks%2C%20with%0Agenerated%20content%20that%20is%20not%20only%20highly%20aligned%20with%20real-world%20data%0Adistributions%20but%20also%20enables%20more%20diverse%20and%20fine-grained%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03897v5&entry.124074799=Read"},
{"title": "Vecchia-Inducing-Points Full-Scale Approximations for Gaussian Processes", "author": "Tim Gyger and Reinhard Furrer and Fabio Sigrist", "abstract": "  Gaussian processes are flexible, probabilistic, non-parametric models widely\nused in machine learning and statistics. However, their scalability to large\ndata sets is limited by computational constraints. To overcome these\nchallenges, we propose Vecchia-inducing-points full-scale (VIF) approximations\ncombining the strengths of global inducing points and local Vecchia\napproximations. Vecchia approximations excel in settings with low-dimensional\ninputs and moderately smooth covariance functions, while inducing point methods\nare better suited to high-dimensional inputs and smoother covariance functions.\nOur VIF approach bridges these two regimes by using an efficient\ncorrelation-based neighbor-finding strategy for the Vecchia approximation of\nthe residual process, implemented via a modified cover tree algorithm. We\nfurther extend our framework to non-Gaussian likelihoods by introducing\niterative methods that substantially reduce computational costs for training\nand prediction by several orders of magnitudes compared to Cholesky-based\ncomputations when using a Laplace approximation. In particular, we propose and\ncompare novel preconditioners and provide theoretical convergence results.\nExtensive numerical experiments on simulated and real-world data sets show that\nVIF approximations are both computationally efficient as well as more accurate\nand numerically stable than state-of-the-art alternatives. All methods are\nimplemented in the open source C++ library GPBoost with high-level Python and R\ninterfaces.\n", "link": "http://arxiv.org/abs/2507.05064v1", "date": "2025-07-07", "relevancy": 2.4286, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5048}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4766}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vecchia-Inducing-Points%20Full-Scale%20Approximations%20for%20Gaussian%20Processes&body=Title%3A%20Vecchia-Inducing-Points%20Full-Scale%20Approximations%20for%20Gaussian%20Processes%0AAuthor%3A%20Tim%20Gyger%20and%20Reinhard%20Furrer%20and%20Fabio%20Sigrist%0AAbstract%3A%20%20%20Gaussian%20processes%20are%20flexible%2C%20probabilistic%2C%20non-parametric%20models%20widely%0Aused%20in%20machine%20learning%20and%20statistics.%20However%2C%20their%20scalability%20to%20large%0Adata%20sets%20is%20limited%20by%20computational%20constraints.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20Vecchia-inducing-points%20full-scale%20%28VIF%29%20approximations%0Acombining%20the%20strengths%20of%20global%20inducing%20points%20and%20local%20Vecchia%0Aapproximations.%20Vecchia%20approximations%20excel%20in%20settings%20with%20low-dimensional%0Ainputs%20and%20moderately%20smooth%20covariance%20functions%2C%20while%20inducing%20point%20methods%0Aare%20better%20suited%20to%20high-dimensional%20inputs%20and%20smoother%20covariance%20functions.%0AOur%20VIF%20approach%20bridges%20these%20two%20regimes%20by%20using%20an%20efficient%0Acorrelation-based%20neighbor-finding%20strategy%20for%20the%20Vecchia%20approximation%20of%0Athe%20residual%20process%2C%20implemented%20via%20a%20modified%20cover%20tree%20algorithm.%20We%0Afurther%20extend%20our%20framework%20to%20non-Gaussian%20likelihoods%20by%20introducing%0Aiterative%20methods%20that%20substantially%20reduce%20computational%20costs%20for%20training%0Aand%20prediction%20by%20several%20orders%20of%20magnitudes%20compared%20to%20Cholesky-based%0Acomputations%20when%20using%20a%20Laplace%20approximation.%20In%20particular%2C%20we%20propose%20and%0Acompare%20novel%20preconditioners%20and%20provide%20theoretical%20convergence%20results.%0AExtensive%20numerical%20experiments%20on%20simulated%20and%20real-world%20data%20sets%20show%20that%0AVIF%20approximations%20are%20both%20computationally%20efficient%20as%20well%20as%20more%20accurate%0Aand%20numerically%20stable%20than%20state-of-the-art%20alternatives.%20All%20methods%20are%0Aimplemented%20in%20the%20open%20source%20C%2B%2B%20library%20GPBoost%20with%20high-level%20Python%20and%20R%0Ainterfaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05064v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVecchia-Inducing-Points%2520Full-Scale%2520Approximations%2520for%2520Gaussian%2520Processes%26entry.906535625%3DTim%2520Gyger%2520and%2520Reinhard%2520Furrer%2520and%2520Fabio%2520Sigrist%26entry.1292438233%3D%2520%2520Gaussian%2520processes%2520are%2520flexible%252C%2520probabilistic%252C%2520non-parametric%2520models%2520widely%250Aused%2520in%2520machine%2520learning%2520and%2520statistics.%2520However%252C%2520their%2520scalability%2520to%2520large%250Adata%2520sets%2520is%2520limited%2520by%2520computational%2520constraints.%2520To%2520overcome%2520these%250Achallenges%252C%2520we%2520propose%2520Vecchia-inducing-points%2520full-scale%2520%2528VIF%2529%2520approximations%250Acombining%2520the%2520strengths%2520of%2520global%2520inducing%2520points%2520and%2520local%2520Vecchia%250Aapproximations.%2520Vecchia%2520approximations%2520excel%2520in%2520settings%2520with%2520low-dimensional%250Ainputs%2520and%2520moderately%2520smooth%2520covariance%2520functions%252C%2520while%2520inducing%2520point%2520methods%250Aare%2520better%2520suited%2520to%2520high-dimensional%2520inputs%2520and%2520smoother%2520covariance%2520functions.%250AOur%2520VIF%2520approach%2520bridges%2520these%2520two%2520regimes%2520by%2520using%2520an%2520efficient%250Acorrelation-based%2520neighbor-finding%2520strategy%2520for%2520the%2520Vecchia%2520approximation%2520of%250Athe%2520residual%2520process%252C%2520implemented%2520via%2520a%2520modified%2520cover%2520tree%2520algorithm.%2520We%250Afurther%2520extend%2520our%2520framework%2520to%2520non-Gaussian%2520likelihoods%2520by%2520introducing%250Aiterative%2520methods%2520that%2520substantially%2520reduce%2520computational%2520costs%2520for%2520training%250Aand%2520prediction%2520by%2520several%2520orders%2520of%2520magnitudes%2520compared%2520to%2520Cholesky-based%250Acomputations%2520when%2520using%2520a%2520Laplace%2520approximation.%2520In%2520particular%252C%2520we%2520propose%2520and%250Acompare%2520novel%2520preconditioners%2520and%2520provide%2520theoretical%2520convergence%2520results.%250AExtensive%2520numerical%2520experiments%2520on%2520simulated%2520and%2520real-world%2520data%2520sets%2520show%2520that%250AVIF%2520approximations%2520are%2520both%2520computationally%2520efficient%2520as%2520well%2520as%2520more%2520accurate%250Aand%2520numerically%2520stable%2520than%2520state-of-the-art%2520alternatives.%2520All%2520methods%2520are%250Aimplemented%2520in%2520the%2520open%2520source%2520C%252B%252B%2520library%2520GPBoost%2520with%2520high-level%2520Python%2520and%2520R%250Ainterfaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05064v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vecchia-Inducing-Points%20Full-Scale%20Approximations%20for%20Gaussian%20Processes&entry.906535625=Tim%20Gyger%20and%20Reinhard%20Furrer%20and%20Fabio%20Sigrist&entry.1292438233=%20%20Gaussian%20processes%20are%20flexible%2C%20probabilistic%2C%20non-parametric%20models%20widely%0Aused%20in%20machine%20learning%20and%20statistics.%20However%2C%20their%20scalability%20to%20large%0Adata%20sets%20is%20limited%20by%20computational%20constraints.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20Vecchia-inducing-points%20full-scale%20%28VIF%29%20approximations%0Acombining%20the%20strengths%20of%20global%20inducing%20points%20and%20local%20Vecchia%0Aapproximations.%20Vecchia%20approximations%20excel%20in%20settings%20with%20low-dimensional%0Ainputs%20and%20moderately%20smooth%20covariance%20functions%2C%20while%20inducing%20point%20methods%0Aare%20better%20suited%20to%20high-dimensional%20inputs%20and%20smoother%20covariance%20functions.%0AOur%20VIF%20approach%20bridges%20these%20two%20regimes%20by%20using%20an%20efficient%0Acorrelation-based%20neighbor-finding%20strategy%20for%20the%20Vecchia%20approximation%20of%0Athe%20residual%20process%2C%20implemented%20via%20a%20modified%20cover%20tree%20algorithm.%20We%0Afurther%20extend%20our%20framework%20to%20non-Gaussian%20likelihoods%20by%20introducing%0Aiterative%20methods%20that%20substantially%20reduce%20computational%20costs%20for%20training%0Aand%20prediction%20by%20several%20orders%20of%20magnitudes%20compared%20to%20Cholesky-based%0Acomputations%20when%20using%20a%20Laplace%20approximation.%20In%20particular%2C%20we%20propose%20and%0Acompare%20novel%20preconditioners%20and%20provide%20theoretical%20convergence%20results.%0AExtensive%20numerical%20experiments%20on%20simulated%20and%20real-world%20data%20sets%20show%20that%0AVIF%20approximations%20are%20both%20computationally%20efficient%20as%20well%20as%20more%20accurate%0Aand%20numerically%20stable%20than%20state-of-the-art%20alternatives.%20All%20methods%20are%0Aimplemented%20in%20the%20open%20source%20C%2B%2B%20library%20GPBoost%20with%20high-level%20Python%20and%20R%0Ainterfaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05064v1&entry.124074799=Read"},
{"title": "VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo\n  Alignment", "author": "Wenyan Cong and Hanqing Zhu and Kevin Wang and Jiahui Lei and Colton Stearns and Yuanhao Cai and Leonidas Guibas and Zhangyang Wang and Zhiwen Fan", "abstract": "  Efficiently reconstructing 3D scenes from monocular video remains a core\nchallenge in computer vision, vital for applications in virtual reality,\nrobotics, and scene understanding. Recently, frame-by-frame progressive\nreconstruction without camera poses is commonly adopted, incurring high\ncomputational overhead and compounding errors when scaling to longer videos. To\novercome these issues, we introduce VideoLifter, a novel video-to-3D pipeline\nthat leverages a local-to-global strategy on a fragment basis, achieving both\nextreme efficiency and SOTA quality. Locally, VideoLifter leverages learnable\n3D priors to register fragments, extracting essential information for\nsubsequent 3D Gaussian initialization with enforced inter-fragment consistency\nand optimized efficiency. Globally, it employs a tree-based hierarchical\nmerging method with key frame guidance for inter-fragment alignment, pairwise\nmerging with Gaussian point pruning, and subsequent joint optimization to\nensure global consistency while efficiently mitigating cumulative errors. This\napproach significantly accelerates the reconstruction process, reducing\ntraining time by over 82% while holding better visual quality than current SOTA\nmethods.\n", "link": "http://arxiv.org/abs/2501.01949v3", "date": "2025-07-07", "relevancy": 2.4168, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6173}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6127}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoLifter%3A%20Lifting%20Videos%20to%203D%20with%20Fast%20Hierarchical%20Stereo%0A%20%20Alignment&body=Title%3A%20VideoLifter%3A%20Lifting%20Videos%20to%203D%20with%20Fast%20Hierarchical%20Stereo%0A%20%20Alignment%0AAuthor%3A%20Wenyan%20Cong%20and%20Hanqing%20Zhu%20and%20Kevin%20Wang%20and%20Jiahui%20Lei%20and%20Colton%20Stearns%20and%20Yuanhao%20Cai%20and%20Leonidas%20Guibas%20and%20Zhangyang%20Wang%20and%20Zhiwen%20Fan%0AAbstract%3A%20%20%20Efficiently%20reconstructing%203D%20scenes%20from%20monocular%20video%20remains%20a%20core%0Achallenge%20in%20computer%20vision%2C%20vital%20for%20applications%20in%20virtual%20reality%2C%0Arobotics%2C%20and%20scene%20understanding.%20Recently%2C%20frame-by-frame%20progressive%0Areconstruction%20without%20camera%20poses%20is%20commonly%20adopted%2C%20incurring%20high%0Acomputational%20overhead%20and%20compounding%20errors%20when%20scaling%20to%20longer%20videos.%20To%0Aovercome%20these%20issues%2C%20we%20introduce%20VideoLifter%2C%20a%20novel%20video-to-3D%20pipeline%0Athat%20leverages%20a%20local-to-global%20strategy%20on%20a%20fragment%20basis%2C%20achieving%20both%0Aextreme%20efficiency%20and%20SOTA%20quality.%20Locally%2C%20VideoLifter%20leverages%20learnable%0A3D%20priors%20to%20register%20fragments%2C%20extracting%20essential%20information%20for%0Asubsequent%203D%20Gaussian%20initialization%20with%20enforced%20inter-fragment%20consistency%0Aand%20optimized%20efficiency.%20Globally%2C%20it%20employs%20a%20tree-based%20hierarchical%0Amerging%20method%20with%20key%20frame%20guidance%20for%20inter-fragment%20alignment%2C%20pairwise%0Amerging%20with%20Gaussian%20point%20pruning%2C%20and%20subsequent%20joint%20optimization%20to%0Aensure%20global%20consistency%20while%20efficiently%20mitigating%20cumulative%20errors.%20This%0Aapproach%20significantly%20accelerates%20the%20reconstruction%20process%2C%20reducing%0Atraining%20time%20by%20over%2082%25%20while%20holding%20better%20visual%20quality%20than%20current%20SOTA%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01949v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoLifter%253A%2520Lifting%2520Videos%2520to%25203D%2520with%2520Fast%2520Hierarchical%2520Stereo%250A%2520%2520Alignment%26entry.906535625%3DWenyan%2520Cong%2520and%2520Hanqing%2520Zhu%2520and%2520Kevin%2520Wang%2520and%2520Jiahui%2520Lei%2520and%2520Colton%2520Stearns%2520and%2520Yuanhao%2520Cai%2520and%2520Leonidas%2520Guibas%2520and%2520Zhangyang%2520Wang%2520and%2520Zhiwen%2520Fan%26entry.1292438233%3D%2520%2520Efficiently%2520reconstructing%25203D%2520scenes%2520from%2520monocular%2520video%2520remains%2520a%2520core%250Achallenge%2520in%2520computer%2520vision%252C%2520vital%2520for%2520applications%2520in%2520virtual%2520reality%252C%250Arobotics%252C%2520and%2520scene%2520understanding.%2520Recently%252C%2520frame-by-frame%2520progressive%250Areconstruction%2520without%2520camera%2520poses%2520is%2520commonly%2520adopted%252C%2520incurring%2520high%250Acomputational%2520overhead%2520and%2520compounding%2520errors%2520when%2520scaling%2520to%2520longer%2520videos.%2520To%250Aovercome%2520these%2520issues%252C%2520we%2520introduce%2520VideoLifter%252C%2520a%2520novel%2520video-to-3D%2520pipeline%250Athat%2520leverages%2520a%2520local-to-global%2520strategy%2520on%2520a%2520fragment%2520basis%252C%2520achieving%2520both%250Aextreme%2520efficiency%2520and%2520SOTA%2520quality.%2520Locally%252C%2520VideoLifter%2520leverages%2520learnable%250A3D%2520priors%2520to%2520register%2520fragments%252C%2520extracting%2520essential%2520information%2520for%250Asubsequent%25203D%2520Gaussian%2520initialization%2520with%2520enforced%2520inter-fragment%2520consistency%250Aand%2520optimized%2520efficiency.%2520Globally%252C%2520it%2520employs%2520a%2520tree-based%2520hierarchical%250Amerging%2520method%2520with%2520key%2520frame%2520guidance%2520for%2520inter-fragment%2520alignment%252C%2520pairwise%250Amerging%2520with%2520Gaussian%2520point%2520pruning%252C%2520and%2520subsequent%2520joint%2520optimization%2520to%250Aensure%2520global%2520consistency%2520while%2520efficiently%2520mitigating%2520cumulative%2520errors.%2520This%250Aapproach%2520significantly%2520accelerates%2520the%2520reconstruction%2520process%252C%2520reducing%250Atraining%2520time%2520by%2520over%252082%2525%2520while%2520holding%2520better%2520visual%2520quality%2520than%2520current%2520SOTA%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01949v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoLifter%3A%20Lifting%20Videos%20to%203D%20with%20Fast%20Hierarchical%20Stereo%0A%20%20Alignment&entry.906535625=Wenyan%20Cong%20and%20Hanqing%20Zhu%20and%20Kevin%20Wang%20and%20Jiahui%20Lei%20and%20Colton%20Stearns%20and%20Yuanhao%20Cai%20and%20Leonidas%20Guibas%20and%20Zhangyang%20Wang%20and%20Zhiwen%20Fan&entry.1292438233=%20%20Efficiently%20reconstructing%203D%20scenes%20from%20monocular%20video%20remains%20a%20core%0Achallenge%20in%20computer%20vision%2C%20vital%20for%20applications%20in%20virtual%20reality%2C%0Arobotics%2C%20and%20scene%20understanding.%20Recently%2C%20frame-by-frame%20progressive%0Areconstruction%20without%20camera%20poses%20is%20commonly%20adopted%2C%20incurring%20high%0Acomputational%20overhead%20and%20compounding%20errors%20when%20scaling%20to%20longer%20videos.%20To%0Aovercome%20these%20issues%2C%20we%20introduce%20VideoLifter%2C%20a%20novel%20video-to-3D%20pipeline%0Athat%20leverages%20a%20local-to-global%20strategy%20on%20a%20fragment%20basis%2C%20achieving%20both%0Aextreme%20efficiency%20and%20SOTA%20quality.%20Locally%2C%20VideoLifter%20leverages%20learnable%0A3D%20priors%20to%20register%20fragments%2C%20extracting%20essential%20information%20for%0Asubsequent%203D%20Gaussian%20initialization%20with%20enforced%20inter-fragment%20consistency%0Aand%20optimized%20efficiency.%20Globally%2C%20it%20employs%20a%20tree-based%20hierarchical%0Amerging%20method%20with%20key%20frame%20guidance%20for%20inter-fragment%20alignment%2C%20pairwise%0Amerging%20with%20Gaussian%20point%20pruning%2C%20and%20subsequent%20joint%20optimization%20to%0Aensure%20global%20consistency%20while%20efficiently%20mitigating%20cumulative%20errors.%20This%0Aapproach%20significantly%20accelerates%20the%20reconstruction%20process%2C%20reducing%0Atraining%20time%20by%20over%2082%25%20while%20holding%20better%20visual%20quality%20than%20current%20SOTA%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01949v3&entry.124074799=Read"},
{"title": "Rethinking Detecting Salient and Camouflaged Objects in Unconstrained\n  Scenes", "author": "Zhangjun Zhou and Yiping Li and Chunlin Zhong and Jianuo Huang and Jialun Pei and Hua Li and He Tang", "abstract": "  While the human visual system employs distinct mechanisms to perceive salient\nand camouflaged objects, existing models struggle to disentangle these tasks.\nSpecifically, salient object detection (SOD) models frequently misclassify\ncamouflaged objects as salient, while camouflaged object detection (COD) models\nconversely misinterpret salient objects as camouflaged. We hypothesize that\nthis can be attributed to two factors: (i) the specific annotation paradigm of\ncurrent SOD and COD datasets, and (ii) the lack of explicit attribute\nrelationship modeling in current models. Prevalent SOD/COD datasets enforce a\nmutual exclusivity constraint, assuming scenes contain either salient or\ncamouflaged objects, which poorly aligns with the real world. Furthermore,\ncurrent SOD/COD methods are primarily designed for these highly constrained\ndatasets and lack explicit modeling of the relationship between salient and\ncamouflaged objects. In this paper, to promote the development of unconstrained\nsalient and camouflaged object detection, we construct a large-scale dataset,\nUSC12K, which features comprehensive labels and four different scenes that\ncover all possible logical existence scenarios of both salient and camouflaged\nobjects. To explicitly model the relationship between salient and camouflaged\nobjects, we propose a model called USCNet, which introduces two distinct prompt\nquery mechanisms for modeling inter-sample and intra-sample attribute\nrelationships. Additionally, to assess the model's ability to distinguish\nbetween salient and camouflaged objects, we design an evaluation metric called\nCSCS. The proposed method achieves state-of-the-art performance across all\nscenes in various metrics. The code and dataset will be available at\nhttps://github.com/ssecv/USCNet.\n", "link": "http://arxiv.org/abs/2412.10943v3", "date": "2025-07-07", "relevancy": 2.4127, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6108}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6108}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Detecting%20Salient%20and%20Camouflaged%20Objects%20in%20Unconstrained%0A%20%20Scenes&body=Title%3A%20Rethinking%20Detecting%20Salient%20and%20Camouflaged%20Objects%20in%20Unconstrained%0A%20%20Scenes%0AAuthor%3A%20Zhangjun%20Zhou%20and%20Yiping%20Li%20and%20Chunlin%20Zhong%20and%20Jianuo%20Huang%20and%20Jialun%20Pei%20and%20Hua%20Li%20and%20He%20Tang%0AAbstract%3A%20%20%20While%20the%20human%20visual%20system%20employs%20distinct%20mechanisms%20to%20perceive%20salient%0Aand%20camouflaged%20objects%2C%20existing%20models%20struggle%20to%20disentangle%20these%20tasks.%0ASpecifically%2C%20salient%20object%20detection%20%28SOD%29%20models%20frequently%20misclassify%0Acamouflaged%20objects%20as%20salient%2C%20while%20camouflaged%20object%20detection%20%28COD%29%20models%0Aconversely%20misinterpret%20salient%20objects%20as%20camouflaged.%20We%20hypothesize%20that%0Athis%20can%20be%20attributed%20to%20two%20factors%3A%20%28i%29%20the%20specific%20annotation%20paradigm%20of%0Acurrent%20SOD%20and%20COD%20datasets%2C%20and%20%28ii%29%20the%20lack%20of%20explicit%20attribute%0Arelationship%20modeling%20in%20current%20models.%20Prevalent%20SOD/COD%20datasets%20enforce%20a%0Amutual%20exclusivity%20constraint%2C%20assuming%20scenes%20contain%20either%20salient%20or%0Acamouflaged%20objects%2C%20which%20poorly%20aligns%20with%20the%20real%20world.%20Furthermore%2C%0Acurrent%20SOD/COD%20methods%20are%20primarily%20designed%20for%20these%20highly%20constrained%0Adatasets%20and%20lack%20explicit%20modeling%20of%20the%20relationship%20between%20salient%20and%0Acamouflaged%20objects.%20In%20this%20paper%2C%20to%20promote%20the%20development%20of%20unconstrained%0Asalient%20and%20camouflaged%20object%20detection%2C%20we%20construct%20a%20large-scale%20dataset%2C%0AUSC12K%2C%20which%20features%20comprehensive%20labels%20and%20four%20different%20scenes%20that%0Acover%20all%20possible%20logical%20existence%20scenarios%20of%20both%20salient%20and%20camouflaged%0Aobjects.%20To%20explicitly%20model%20the%20relationship%20between%20salient%20and%20camouflaged%0Aobjects%2C%20we%20propose%20a%20model%20called%20USCNet%2C%20which%20introduces%20two%20distinct%20prompt%0Aquery%20mechanisms%20for%20modeling%20inter-sample%20and%20intra-sample%20attribute%0Arelationships.%20Additionally%2C%20to%20assess%20the%20model%27s%20ability%20to%20distinguish%0Abetween%20salient%20and%20camouflaged%20objects%2C%20we%20design%20an%20evaluation%20metric%20called%0ACSCS.%20The%20proposed%20method%20achieves%20state-of-the-art%20performance%20across%20all%0Ascenes%20in%20various%20metrics.%20The%20code%20and%20dataset%20will%20be%20available%20at%0Ahttps%3A//github.com/ssecv/USCNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10943v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Detecting%2520Salient%2520and%2520Camouflaged%2520Objects%2520in%2520Unconstrained%250A%2520%2520Scenes%26entry.906535625%3DZhangjun%2520Zhou%2520and%2520Yiping%2520Li%2520and%2520Chunlin%2520Zhong%2520and%2520Jianuo%2520Huang%2520and%2520Jialun%2520Pei%2520and%2520Hua%2520Li%2520and%2520He%2520Tang%26entry.1292438233%3D%2520%2520While%2520the%2520human%2520visual%2520system%2520employs%2520distinct%2520mechanisms%2520to%2520perceive%2520salient%250Aand%2520camouflaged%2520objects%252C%2520existing%2520models%2520struggle%2520to%2520disentangle%2520these%2520tasks.%250ASpecifically%252C%2520salient%2520object%2520detection%2520%2528SOD%2529%2520models%2520frequently%2520misclassify%250Acamouflaged%2520objects%2520as%2520salient%252C%2520while%2520camouflaged%2520object%2520detection%2520%2528COD%2529%2520models%250Aconversely%2520misinterpret%2520salient%2520objects%2520as%2520camouflaged.%2520We%2520hypothesize%2520that%250Athis%2520can%2520be%2520attributed%2520to%2520two%2520factors%253A%2520%2528i%2529%2520the%2520specific%2520annotation%2520paradigm%2520of%250Acurrent%2520SOD%2520and%2520COD%2520datasets%252C%2520and%2520%2528ii%2529%2520the%2520lack%2520of%2520explicit%2520attribute%250Arelationship%2520modeling%2520in%2520current%2520models.%2520Prevalent%2520SOD/COD%2520datasets%2520enforce%2520a%250Amutual%2520exclusivity%2520constraint%252C%2520assuming%2520scenes%2520contain%2520either%2520salient%2520or%250Acamouflaged%2520objects%252C%2520which%2520poorly%2520aligns%2520with%2520the%2520real%2520world.%2520Furthermore%252C%250Acurrent%2520SOD/COD%2520methods%2520are%2520primarily%2520designed%2520for%2520these%2520highly%2520constrained%250Adatasets%2520and%2520lack%2520explicit%2520modeling%2520of%2520the%2520relationship%2520between%2520salient%2520and%250Acamouflaged%2520objects.%2520In%2520this%2520paper%252C%2520to%2520promote%2520the%2520development%2520of%2520unconstrained%250Asalient%2520and%2520camouflaged%2520object%2520detection%252C%2520we%2520construct%2520a%2520large-scale%2520dataset%252C%250AUSC12K%252C%2520which%2520features%2520comprehensive%2520labels%2520and%2520four%2520different%2520scenes%2520that%250Acover%2520all%2520possible%2520logical%2520existence%2520scenarios%2520of%2520both%2520salient%2520and%2520camouflaged%250Aobjects.%2520To%2520explicitly%2520model%2520the%2520relationship%2520between%2520salient%2520and%2520camouflaged%250Aobjects%252C%2520we%2520propose%2520a%2520model%2520called%2520USCNet%252C%2520which%2520introduces%2520two%2520distinct%2520prompt%250Aquery%2520mechanisms%2520for%2520modeling%2520inter-sample%2520and%2520intra-sample%2520attribute%250Arelationships.%2520Additionally%252C%2520to%2520assess%2520the%2520model%2527s%2520ability%2520to%2520distinguish%250Abetween%2520salient%2520and%2520camouflaged%2520objects%252C%2520we%2520design%2520an%2520evaluation%2520metric%2520called%250ACSCS.%2520The%2520proposed%2520method%2520achieves%2520state-of-the-art%2520performance%2520across%2520all%250Ascenes%2520in%2520various%2520metrics.%2520The%2520code%2520and%2520dataset%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/ssecv/USCNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10943v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Detecting%20Salient%20and%20Camouflaged%20Objects%20in%20Unconstrained%0A%20%20Scenes&entry.906535625=Zhangjun%20Zhou%20and%20Yiping%20Li%20and%20Chunlin%20Zhong%20and%20Jianuo%20Huang%20and%20Jialun%20Pei%20and%20Hua%20Li%20and%20He%20Tang&entry.1292438233=%20%20While%20the%20human%20visual%20system%20employs%20distinct%20mechanisms%20to%20perceive%20salient%0Aand%20camouflaged%20objects%2C%20existing%20models%20struggle%20to%20disentangle%20these%20tasks.%0ASpecifically%2C%20salient%20object%20detection%20%28SOD%29%20models%20frequently%20misclassify%0Acamouflaged%20objects%20as%20salient%2C%20while%20camouflaged%20object%20detection%20%28COD%29%20models%0Aconversely%20misinterpret%20salient%20objects%20as%20camouflaged.%20We%20hypothesize%20that%0Athis%20can%20be%20attributed%20to%20two%20factors%3A%20%28i%29%20the%20specific%20annotation%20paradigm%20of%0Acurrent%20SOD%20and%20COD%20datasets%2C%20and%20%28ii%29%20the%20lack%20of%20explicit%20attribute%0Arelationship%20modeling%20in%20current%20models.%20Prevalent%20SOD/COD%20datasets%20enforce%20a%0Amutual%20exclusivity%20constraint%2C%20assuming%20scenes%20contain%20either%20salient%20or%0Acamouflaged%20objects%2C%20which%20poorly%20aligns%20with%20the%20real%20world.%20Furthermore%2C%0Acurrent%20SOD/COD%20methods%20are%20primarily%20designed%20for%20these%20highly%20constrained%0Adatasets%20and%20lack%20explicit%20modeling%20of%20the%20relationship%20between%20salient%20and%0Acamouflaged%20objects.%20In%20this%20paper%2C%20to%20promote%20the%20development%20of%20unconstrained%0Asalient%20and%20camouflaged%20object%20detection%2C%20we%20construct%20a%20large-scale%20dataset%2C%0AUSC12K%2C%20which%20features%20comprehensive%20labels%20and%20four%20different%20scenes%20that%0Acover%20all%20possible%20logical%20existence%20scenarios%20of%20both%20salient%20and%20camouflaged%0Aobjects.%20To%20explicitly%20model%20the%20relationship%20between%20salient%20and%20camouflaged%0Aobjects%2C%20we%20propose%20a%20model%20called%20USCNet%2C%20which%20introduces%20two%20distinct%20prompt%0Aquery%20mechanisms%20for%20modeling%20inter-sample%20and%20intra-sample%20attribute%0Arelationships.%20Additionally%2C%20to%20assess%20the%20model%27s%20ability%20to%20distinguish%0Abetween%20salient%20and%20camouflaged%20objects%2C%20we%20design%20an%20evaluation%20metric%20called%0ACSCS.%20The%20proposed%20method%20achieves%20state-of-the-art%20performance%20across%20all%0Ascenes%20in%20various%20metrics.%20The%20code%20and%20dataset%20will%20be%20available%20at%0Ahttps%3A//github.com/ssecv/USCNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10943v3&entry.124074799=Read"},
{"title": "Fast online node labeling with graph subsampling", "author": "Yushen Huang and Ertai Luo and Reza Babenezhad and Yifan Sun", "abstract": "  Large data applications rely on storing data in massive, sparse graphs with\nmillions to trillions of nodes. Graph-based methods, such as node prediction,\naim for computational efficiency regardless of graph size. Techniques like\nlocalized approximate personalized page rank (APPR) solve sparse linear systems\nwith complexity independent of graph size, but is in terms of the maximum node\ndegree, which can be much larger in practice than the average node degree for\nreal-world large graphs. In this paper, we consider an \\emph{online subsampled\nAPPR method}, where messages are intentionally dropped at random. We use tools\nfrom graph sparsifiers and matrix linear algebra to give approximation bounds\non the graph's spectral properties ($O(1/\\epsilon^2)$ edges), and node\nclassification performance (added $O(n\\epsilon)$ overhead).\n", "link": "http://arxiv.org/abs/2503.16755v2", "date": "2025-07-07", "relevancy": 2.4112, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4907}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4785}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20online%20node%20labeling%20with%20graph%20subsampling&body=Title%3A%20Fast%20online%20node%20labeling%20with%20graph%20subsampling%0AAuthor%3A%20Yushen%20Huang%20and%20Ertai%20Luo%20and%20Reza%20Babenezhad%20and%20Yifan%20Sun%0AAbstract%3A%20%20%20Large%20data%20applications%20rely%20on%20storing%20data%20in%20massive%2C%20sparse%20graphs%20with%0Amillions%20to%20trillions%20of%20nodes.%20Graph-based%20methods%2C%20such%20as%20node%20prediction%2C%0Aaim%20for%20computational%20efficiency%20regardless%20of%20graph%20size.%20Techniques%20like%0Alocalized%20approximate%20personalized%20page%20rank%20%28APPR%29%20solve%20sparse%20linear%20systems%0Awith%20complexity%20independent%20of%20graph%20size%2C%20but%20is%20in%20terms%20of%20the%20maximum%20node%0Adegree%2C%20which%20can%20be%20much%20larger%20in%20practice%20than%20the%20average%20node%20degree%20for%0Areal-world%20large%20graphs.%20In%20this%20paper%2C%20we%20consider%20an%20%5Cemph%7Bonline%20subsampled%0AAPPR%20method%7D%2C%20where%20messages%20are%20intentionally%20dropped%20at%20random.%20We%20use%20tools%0Afrom%20graph%20sparsifiers%20and%20matrix%20linear%20algebra%20to%20give%20approximation%20bounds%0Aon%20the%20graph%27s%20spectral%20properties%20%28%24O%281/%5Cepsilon%5E2%29%24%20edges%29%2C%20and%20node%0Aclassification%20performance%20%28added%20%24O%28n%5Cepsilon%29%24%20overhead%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.16755v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520online%2520node%2520labeling%2520with%2520graph%2520subsampling%26entry.906535625%3DYushen%2520Huang%2520and%2520Ertai%2520Luo%2520and%2520Reza%2520Babenezhad%2520and%2520Yifan%2520Sun%26entry.1292438233%3D%2520%2520Large%2520data%2520applications%2520rely%2520on%2520storing%2520data%2520in%2520massive%252C%2520sparse%2520graphs%2520with%250Amillions%2520to%2520trillions%2520of%2520nodes.%2520Graph-based%2520methods%252C%2520such%2520as%2520node%2520prediction%252C%250Aaim%2520for%2520computational%2520efficiency%2520regardless%2520of%2520graph%2520size.%2520Techniques%2520like%250Alocalized%2520approximate%2520personalized%2520page%2520rank%2520%2528APPR%2529%2520solve%2520sparse%2520linear%2520systems%250Awith%2520complexity%2520independent%2520of%2520graph%2520size%252C%2520but%2520is%2520in%2520terms%2520of%2520the%2520maximum%2520node%250Adegree%252C%2520which%2520can%2520be%2520much%2520larger%2520in%2520practice%2520than%2520the%2520average%2520node%2520degree%2520for%250Areal-world%2520large%2520graphs.%2520In%2520this%2520paper%252C%2520we%2520consider%2520an%2520%255Cemph%257Bonline%2520subsampled%250AAPPR%2520method%257D%252C%2520where%2520messages%2520are%2520intentionally%2520dropped%2520at%2520random.%2520We%2520use%2520tools%250Afrom%2520graph%2520sparsifiers%2520and%2520matrix%2520linear%2520algebra%2520to%2520give%2520approximation%2520bounds%250Aon%2520the%2520graph%2527s%2520spectral%2520properties%2520%2528%2524O%25281/%255Cepsilon%255E2%2529%2524%2520edges%2529%252C%2520and%2520node%250Aclassification%2520performance%2520%2528added%2520%2524O%2528n%255Cepsilon%2529%2524%2520overhead%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.16755v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20online%20node%20labeling%20with%20graph%20subsampling&entry.906535625=Yushen%20Huang%20and%20Ertai%20Luo%20and%20Reza%20Babenezhad%20and%20Yifan%20Sun&entry.1292438233=%20%20Large%20data%20applications%20rely%20on%20storing%20data%20in%20massive%2C%20sparse%20graphs%20with%0Amillions%20to%20trillions%20of%20nodes.%20Graph-based%20methods%2C%20such%20as%20node%20prediction%2C%0Aaim%20for%20computational%20efficiency%20regardless%20of%20graph%20size.%20Techniques%20like%0Alocalized%20approximate%20personalized%20page%20rank%20%28APPR%29%20solve%20sparse%20linear%20systems%0Awith%20complexity%20independent%20of%20graph%20size%2C%20but%20is%20in%20terms%20of%20the%20maximum%20node%0Adegree%2C%20which%20can%20be%20much%20larger%20in%20practice%20than%20the%20average%20node%20degree%20for%0Areal-world%20large%20graphs.%20In%20this%20paper%2C%20we%20consider%20an%20%5Cemph%7Bonline%20subsampled%0AAPPR%20method%7D%2C%20where%20messages%20are%20intentionally%20dropped%20at%20random.%20We%20use%20tools%0Afrom%20graph%20sparsifiers%20and%20matrix%20linear%20algebra%20to%20give%20approximation%20bounds%0Aon%20the%20graph%27s%20spectral%20properties%20%28%24O%281/%5Cepsilon%5E2%29%24%20edges%29%2C%20and%20node%0Aclassification%20performance%20%28added%20%24O%28n%5Cepsilon%29%24%20overhead%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.16755v2&entry.124074799=Read"},
{"title": "Riemannian Complex Hermit Positive Definite Convolution Network for\n  Polarimetric SAR Image Classification", "author": "Junfei Shi and Yuke Li and Mengmeng Nie and Fang Liu and Haiyan Jin and Junhuai Li and Weisi Lin", "abstract": "  Deep learning has been extensively utilized for PolSAR image classification.\nHowever, most existing methods transform the polarimetric covariance matrix\ninto a real- or complex-valued vector to comply with standard deep learning\nframeworks in Euclidean space. This approach overlooks the inherent structure\nof the covariance matrix, which is a complex Hermitian positive definite (HPD)\nmatrix residing in the Riemannian manifold. Vectorization disrupts the matrix\nstructure and misrepresents its geometric properties. To mitigate this\ndrawback, we propose HPDNet, a novel framework that directly processes HPD\nmatrices on the Riemannian manifold. The HPDnet fully considers the complex\nphase information by decomposing a complex HPD matrix into the real- and\nimaginarymatrices. The proposed HPDnet consists of several HPD mapping layers\nand rectifying layers, which can preserve the geometric structure of the data\nand transform them into a more separable manifold representation. Subsequently,\na complex LogEig layer is developed to project the manifold data into a tangent\nspace, ensuring that conventional Euclidean-based deep learning networks can be\napplied to further extract contextual features for classification. Furthermore,\nto optimize computational efficiency, we design a fast eigenvalue decomposition\nmethod for parallelized matrix processing. Experiments conducted on three\nreal-world PolSAR datasets demonstrate that the proposed method outperforms\nstate-of-the-art approaches, especially in heterogeneous regions.\n", "link": "http://arxiv.org/abs/2502.08137v2", "date": "2025-07-07", "relevancy": 2.4104, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.495}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4799}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Riemannian%20Complex%20Hermit%20Positive%20Definite%20Convolution%20Network%20for%0A%20%20Polarimetric%20SAR%20Image%20Classification&body=Title%3A%20Riemannian%20Complex%20Hermit%20Positive%20Definite%20Convolution%20Network%20for%0A%20%20Polarimetric%20SAR%20Image%20Classification%0AAuthor%3A%20Junfei%20Shi%20and%20Yuke%20Li%20and%20Mengmeng%20Nie%20and%20Fang%20Liu%20and%20Haiyan%20Jin%20and%20Junhuai%20Li%20and%20Weisi%20Lin%0AAbstract%3A%20%20%20Deep%20learning%20has%20been%20extensively%20utilized%20for%20PolSAR%20image%20classification.%0AHowever%2C%20most%20existing%20methods%20transform%20the%20polarimetric%20covariance%20matrix%0Ainto%20a%20real-%20or%20complex-valued%20vector%20to%20comply%20with%20standard%20deep%20learning%0Aframeworks%20in%20Euclidean%20space.%20This%20approach%20overlooks%20the%20inherent%20structure%0Aof%20the%20covariance%20matrix%2C%20which%20is%20a%20complex%20Hermitian%20positive%20definite%20%28HPD%29%0Amatrix%20residing%20in%20the%20Riemannian%20manifold.%20Vectorization%20disrupts%20the%20matrix%0Astructure%20and%20misrepresents%20its%20geometric%20properties.%20To%20mitigate%20this%0Adrawback%2C%20we%20propose%20HPDNet%2C%20a%20novel%20framework%20that%20directly%20processes%20HPD%0Amatrices%20on%20the%20Riemannian%20manifold.%20The%20HPDnet%20fully%20considers%20the%20complex%0Aphase%20information%20by%20decomposing%20a%20complex%20HPD%20matrix%20into%20the%20real-%20and%0Aimaginarymatrices.%20The%20proposed%20HPDnet%20consists%20of%20several%20HPD%20mapping%20layers%0Aand%20rectifying%20layers%2C%20which%20can%20preserve%20the%20geometric%20structure%20of%20the%20data%0Aand%20transform%20them%20into%20a%20more%20separable%20manifold%20representation.%20Subsequently%2C%0Aa%20complex%20LogEig%20layer%20is%20developed%20to%20project%20the%20manifold%20data%20into%20a%20tangent%0Aspace%2C%20ensuring%20that%20conventional%20Euclidean-based%20deep%20learning%20networks%20can%20be%0Aapplied%20to%20further%20extract%20contextual%20features%20for%20classification.%20Furthermore%2C%0Ato%20optimize%20computational%20efficiency%2C%20we%20design%20a%20fast%20eigenvalue%20decomposition%0Amethod%20for%20parallelized%20matrix%20processing.%20Experiments%20conducted%20on%20three%0Areal-world%20PolSAR%20datasets%20demonstrate%20that%20the%20proposed%20method%20outperforms%0Astate-of-the-art%20approaches%2C%20especially%20in%20heterogeneous%20regions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08137v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRiemannian%2520Complex%2520Hermit%2520Positive%2520Definite%2520Convolution%2520Network%2520for%250A%2520%2520Polarimetric%2520SAR%2520Image%2520Classification%26entry.906535625%3DJunfei%2520Shi%2520and%2520Yuke%2520Li%2520and%2520Mengmeng%2520Nie%2520and%2520Fang%2520Liu%2520and%2520Haiyan%2520Jin%2520and%2520Junhuai%2520Li%2520and%2520Weisi%2520Lin%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520been%2520extensively%2520utilized%2520for%2520PolSAR%2520image%2520classification.%250AHowever%252C%2520most%2520existing%2520methods%2520transform%2520the%2520polarimetric%2520covariance%2520matrix%250Ainto%2520a%2520real-%2520or%2520complex-valued%2520vector%2520to%2520comply%2520with%2520standard%2520deep%2520learning%250Aframeworks%2520in%2520Euclidean%2520space.%2520This%2520approach%2520overlooks%2520the%2520inherent%2520structure%250Aof%2520the%2520covariance%2520matrix%252C%2520which%2520is%2520a%2520complex%2520Hermitian%2520positive%2520definite%2520%2528HPD%2529%250Amatrix%2520residing%2520in%2520the%2520Riemannian%2520manifold.%2520Vectorization%2520disrupts%2520the%2520matrix%250Astructure%2520and%2520misrepresents%2520its%2520geometric%2520properties.%2520To%2520mitigate%2520this%250Adrawback%252C%2520we%2520propose%2520HPDNet%252C%2520a%2520novel%2520framework%2520that%2520directly%2520processes%2520HPD%250Amatrices%2520on%2520the%2520Riemannian%2520manifold.%2520The%2520HPDnet%2520fully%2520considers%2520the%2520complex%250Aphase%2520information%2520by%2520decomposing%2520a%2520complex%2520HPD%2520matrix%2520into%2520the%2520real-%2520and%250Aimaginarymatrices.%2520The%2520proposed%2520HPDnet%2520consists%2520of%2520several%2520HPD%2520mapping%2520layers%250Aand%2520rectifying%2520layers%252C%2520which%2520can%2520preserve%2520the%2520geometric%2520structure%2520of%2520the%2520data%250Aand%2520transform%2520them%2520into%2520a%2520more%2520separable%2520manifold%2520representation.%2520Subsequently%252C%250Aa%2520complex%2520LogEig%2520layer%2520is%2520developed%2520to%2520project%2520the%2520manifold%2520data%2520into%2520a%2520tangent%250Aspace%252C%2520ensuring%2520that%2520conventional%2520Euclidean-based%2520deep%2520learning%2520networks%2520can%2520be%250Aapplied%2520to%2520further%2520extract%2520contextual%2520features%2520for%2520classification.%2520Furthermore%252C%250Ato%2520optimize%2520computational%2520efficiency%252C%2520we%2520design%2520a%2520fast%2520eigenvalue%2520decomposition%250Amethod%2520for%2520parallelized%2520matrix%2520processing.%2520Experiments%2520conducted%2520on%2520three%250Areal-world%2520PolSAR%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520method%2520outperforms%250Astate-of-the-art%2520approaches%252C%2520especially%2520in%2520heterogeneous%2520regions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08137v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Riemannian%20Complex%20Hermit%20Positive%20Definite%20Convolution%20Network%20for%0A%20%20Polarimetric%20SAR%20Image%20Classification&entry.906535625=Junfei%20Shi%20and%20Yuke%20Li%20and%20Mengmeng%20Nie%20and%20Fang%20Liu%20and%20Haiyan%20Jin%20and%20Junhuai%20Li%20and%20Weisi%20Lin&entry.1292438233=%20%20Deep%20learning%20has%20been%20extensively%20utilized%20for%20PolSAR%20image%20classification.%0AHowever%2C%20most%20existing%20methods%20transform%20the%20polarimetric%20covariance%20matrix%0Ainto%20a%20real-%20or%20complex-valued%20vector%20to%20comply%20with%20standard%20deep%20learning%0Aframeworks%20in%20Euclidean%20space.%20This%20approach%20overlooks%20the%20inherent%20structure%0Aof%20the%20covariance%20matrix%2C%20which%20is%20a%20complex%20Hermitian%20positive%20definite%20%28HPD%29%0Amatrix%20residing%20in%20the%20Riemannian%20manifold.%20Vectorization%20disrupts%20the%20matrix%0Astructure%20and%20misrepresents%20its%20geometric%20properties.%20To%20mitigate%20this%0Adrawback%2C%20we%20propose%20HPDNet%2C%20a%20novel%20framework%20that%20directly%20processes%20HPD%0Amatrices%20on%20the%20Riemannian%20manifold.%20The%20HPDnet%20fully%20considers%20the%20complex%0Aphase%20information%20by%20decomposing%20a%20complex%20HPD%20matrix%20into%20the%20real-%20and%0Aimaginarymatrices.%20The%20proposed%20HPDnet%20consists%20of%20several%20HPD%20mapping%20layers%0Aand%20rectifying%20layers%2C%20which%20can%20preserve%20the%20geometric%20structure%20of%20the%20data%0Aand%20transform%20them%20into%20a%20more%20separable%20manifold%20representation.%20Subsequently%2C%0Aa%20complex%20LogEig%20layer%20is%20developed%20to%20project%20the%20manifold%20data%20into%20a%20tangent%0Aspace%2C%20ensuring%20that%20conventional%20Euclidean-based%20deep%20learning%20networks%20can%20be%0Aapplied%20to%20further%20extract%20contextual%20features%20for%20classification.%20Furthermore%2C%0Ato%20optimize%20computational%20efficiency%2C%20we%20design%20a%20fast%20eigenvalue%20decomposition%0Amethod%20for%20parallelized%20matrix%20processing.%20Experiments%20conducted%20on%20three%0Areal-world%20PolSAR%20datasets%20demonstrate%20that%20the%20proposed%20method%20outperforms%0Astate-of-the-art%20approaches%2C%20especially%20in%20heterogeneous%20regions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08137v2&entry.124074799=Read"},
{"title": "Multi-person Physics-based Pose Estimation for Combat Sports", "author": "Hossein Feiz and David Labb\u00e9 and Thomas Romeas and Jocelyn Faubert and Sheldon Andrews", "abstract": "  We propose a novel framework for accurate 3D human pose estimation in combat\nsports using sparse multi-camera setups. Our method integrates robust\nmulti-view 2D pose tracking via a transformer-based top-down approach,\nemploying epipolar geometry constraints and long-term video object segmentation\nfor consistent identity tracking across views. Initial 3D poses are obtained\nthrough weighted triangulation and spline smoothing, followed by kinematic\noptimization to refine pose accuracy. We further enhance pose realism and\nrobustness by introducing a multi-person physics-based trajectory optimization\nstep, effectively addressing challenges such as rapid motions, occlusions, and\nclose interactions. Experimental results on diverse datasets, including a new\nbenchmark of elite boxing footage, demonstrate state-of-the-art performance.\nAdditionally, we release comprehensive annotated video datasets to advance\nfuture research in multi-person pose estimation for combat sports.\n", "link": "http://arxiv.org/abs/2504.08175v3", "date": "2025-07-07", "relevancy": 2.4096, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6348}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5841}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-person%20Physics-based%20Pose%20Estimation%20for%20Combat%20Sports&body=Title%3A%20Multi-person%20Physics-based%20Pose%20Estimation%20for%20Combat%20Sports%0AAuthor%3A%20Hossein%20Feiz%20and%20David%20Labb%C3%A9%20and%20Thomas%20Romeas%20and%20Jocelyn%20Faubert%20and%20Sheldon%20Andrews%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20framework%20for%20accurate%203D%20human%20pose%20estimation%20in%20combat%0Asports%20using%20sparse%20multi-camera%20setups.%20Our%20method%20integrates%20robust%0Amulti-view%202D%20pose%20tracking%20via%20a%20transformer-based%20top-down%20approach%2C%0Aemploying%20epipolar%20geometry%20constraints%20and%20long-term%20video%20object%20segmentation%0Afor%20consistent%20identity%20tracking%20across%20views.%20Initial%203D%20poses%20are%20obtained%0Athrough%20weighted%20triangulation%20and%20spline%20smoothing%2C%20followed%20by%20kinematic%0Aoptimization%20to%20refine%20pose%20accuracy.%20We%20further%20enhance%20pose%20realism%20and%0Arobustness%20by%20introducing%20a%20multi-person%20physics-based%20trajectory%20optimization%0Astep%2C%20effectively%20addressing%20challenges%20such%20as%20rapid%20motions%2C%20occlusions%2C%20and%0Aclose%20interactions.%20Experimental%20results%20on%20diverse%20datasets%2C%20including%20a%20new%0Abenchmark%20of%20elite%20boxing%20footage%2C%20demonstrate%20state-of-the-art%20performance.%0AAdditionally%2C%20we%20release%20comprehensive%20annotated%20video%20datasets%20to%20advance%0Afuture%20research%20in%20multi-person%20pose%20estimation%20for%20combat%20sports.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.08175v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-person%2520Physics-based%2520Pose%2520Estimation%2520for%2520Combat%2520Sports%26entry.906535625%3DHossein%2520Feiz%2520and%2520David%2520Labb%25C3%25A9%2520and%2520Thomas%2520Romeas%2520and%2520Jocelyn%2520Faubert%2520and%2520Sheldon%2520Andrews%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520framework%2520for%2520accurate%25203D%2520human%2520pose%2520estimation%2520in%2520combat%250Asports%2520using%2520sparse%2520multi-camera%2520setups.%2520Our%2520method%2520integrates%2520robust%250Amulti-view%25202D%2520pose%2520tracking%2520via%2520a%2520transformer-based%2520top-down%2520approach%252C%250Aemploying%2520epipolar%2520geometry%2520constraints%2520and%2520long-term%2520video%2520object%2520segmentation%250Afor%2520consistent%2520identity%2520tracking%2520across%2520views.%2520Initial%25203D%2520poses%2520are%2520obtained%250Athrough%2520weighted%2520triangulation%2520and%2520spline%2520smoothing%252C%2520followed%2520by%2520kinematic%250Aoptimization%2520to%2520refine%2520pose%2520accuracy.%2520We%2520further%2520enhance%2520pose%2520realism%2520and%250Arobustness%2520by%2520introducing%2520a%2520multi-person%2520physics-based%2520trajectory%2520optimization%250Astep%252C%2520effectively%2520addressing%2520challenges%2520such%2520as%2520rapid%2520motions%252C%2520occlusions%252C%2520and%250Aclose%2520interactions.%2520Experimental%2520results%2520on%2520diverse%2520datasets%252C%2520including%2520a%2520new%250Abenchmark%2520of%2520elite%2520boxing%2520footage%252C%2520demonstrate%2520state-of-the-art%2520performance.%250AAdditionally%252C%2520we%2520release%2520comprehensive%2520annotated%2520video%2520datasets%2520to%2520advance%250Afuture%2520research%2520in%2520multi-person%2520pose%2520estimation%2520for%2520combat%2520sports.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.08175v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-person%20Physics-based%20Pose%20Estimation%20for%20Combat%20Sports&entry.906535625=Hossein%20Feiz%20and%20David%20Labb%C3%A9%20and%20Thomas%20Romeas%20and%20Jocelyn%20Faubert%20and%20Sheldon%20Andrews&entry.1292438233=%20%20We%20propose%20a%20novel%20framework%20for%20accurate%203D%20human%20pose%20estimation%20in%20combat%0Asports%20using%20sparse%20multi-camera%20setups.%20Our%20method%20integrates%20robust%0Amulti-view%202D%20pose%20tracking%20via%20a%20transformer-based%20top-down%20approach%2C%0Aemploying%20epipolar%20geometry%20constraints%20and%20long-term%20video%20object%20segmentation%0Afor%20consistent%20identity%20tracking%20across%20views.%20Initial%203D%20poses%20are%20obtained%0Athrough%20weighted%20triangulation%20and%20spline%20smoothing%2C%20followed%20by%20kinematic%0Aoptimization%20to%20refine%20pose%20accuracy.%20We%20further%20enhance%20pose%20realism%20and%0Arobustness%20by%20introducing%20a%20multi-person%20physics-based%20trajectory%20optimization%0Astep%2C%20effectively%20addressing%20challenges%20such%20as%20rapid%20motions%2C%20occlusions%2C%20and%0Aclose%20interactions.%20Experimental%20results%20on%20diverse%20datasets%2C%20including%20a%20new%0Abenchmark%20of%20elite%20boxing%20footage%2C%20demonstrate%20state-of-the-art%20performance.%0AAdditionally%2C%20we%20release%20comprehensive%20annotated%20video%20datasets%20to%20advance%0Afuture%20research%20in%20multi-person%20pose%20estimation%20for%20combat%20sports.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.08175v3&entry.124074799=Read"},
{"title": "Hear-Your-Click: Interactive Video-to-Audio Generation via Object-aware\n  Contrastive Audio-Visual Fine-tuning", "author": "Yingshan Liang and Keyu Fan and Zhicheng Du and Yiran Wang and Qingyang Shi and Xinyu Zhang and Jiasheng Lu and Peiwu Qin", "abstract": "  Video-to-audio (V2A) generation shows great potential in fields such as film\nproduction. Despite significant advances, current V2A methods, which rely on\nglobal video information, struggle with complex scenes and often fail to\ngenerate audio tailored to specific objects or regions in the videos. To\naddress these limitations, we introduce Hear-Your-Click, an interactive V2A\nframework that enables users to generate sounds for specific objects in the\nvideos by simply clicking on the frame. To achieve this, we propose\nObject-aware Contrastive Audio-Visual Fine-tuning (OCAV) with a Mask-guided\nVisual Encoder (MVE) to obtain object-level visual features aligned with\ncorresponding audio segments. Furthermore, we tailor two data augmentation\nstrategies: Random Video Stitching (RVS) and Mask-guided Loudness Modulation\n(MLM), aimed at enhancing the model's sensitivity to the segmented objects. To\neffectively measure the audio-visual correspondence, we design a new evaluation\nmetric, the CAV score, for evaluation. Extensive experiments demonstrate that\nour framework offers more precise control and improved generation performance\nacross various metrics. Project Page:\nhttps://github.com/SynapGrid/Hear-Your-Click\n", "link": "http://arxiv.org/abs/2507.04959v1", "date": "2025-07-07", "relevancy": 2.4088, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6064}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5996}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hear-Your-Click%3A%20Interactive%20Video-to-Audio%20Generation%20via%20Object-aware%0A%20%20Contrastive%20Audio-Visual%20Fine-tuning&body=Title%3A%20Hear-Your-Click%3A%20Interactive%20Video-to-Audio%20Generation%20via%20Object-aware%0A%20%20Contrastive%20Audio-Visual%20Fine-tuning%0AAuthor%3A%20Yingshan%20Liang%20and%20Keyu%20Fan%20and%20Zhicheng%20Du%20and%20Yiran%20Wang%20and%20Qingyang%20Shi%20and%20Xinyu%20Zhang%20and%20Jiasheng%20Lu%20and%20Peiwu%20Qin%0AAbstract%3A%20%20%20Video-to-audio%20%28V2A%29%20generation%20shows%20great%20potential%20in%20fields%20such%20as%20film%0Aproduction.%20Despite%20significant%20advances%2C%20current%20V2A%20methods%2C%20which%20rely%20on%0Aglobal%20video%20information%2C%20struggle%20with%20complex%20scenes%20and%20often%20fail%20to%0Agenerate%20audio%20tailored%20to%20specific%20objects%20or%20regions%20in%20the%20videos.%20To%0Aaddress%20these%20limitations%2C%20we%20introduce%20Hear-Your-Click%2C%20an%20interactive%20V2A%0Aframework%20that%20enables%20users%20to%20generate%20sounds%20for%20specific%20objects%20in%20the%0Avideos%20by%20simply%20clicking%20on%20the%20frame.%20To%20achieve%20this%2C%20we%20propose%0AObject-aware%20Contrastive%20Audio-Visual%20Fine-tuning%20%28OCAV%29%20with%20a%20Mask-guided%0AVisual%20Encoder%20%28MVE%29%20to%20obtain%20object-level%20visual%20features%20aligned%20with%0Acorresponding%20audio%20segments.%20Furthermore%2C%20we%20tailor%20two%20data%20augmentation%0Astrategies%3A%20Random%20Video%20Stitching%20%28RVS%29%20and%20Mask-guided%20Loudness%20Modulation%0A%28MLM%29%2C%20aimed%20at%20enhancing%20the%20model%27s%20sensitivity%20to%20the%20segmented%20objects.%20To%0Aeffectively%20measure%20the%20audio-visual%20correspondence%2C%20we%20design%20a%20new%20evaluation%0Ametric%2C%20the%20CAV%20score%2C%20for%20evaluation.%20Extensive%20experiments%20demonstrate%20that%0Aour%20framework%20offers%20more%20precise%20control%20and%20improved%20generation%20performance%0Aacross%20various%20metrics.%20Project%20Page%3A%0Ahttps%3A//github.com/SynapGrid/Hear-Your-Click%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHear-Your-Click%253A%2520Interactive%2520Video-to-Audio%2520Generation%2520via%2520Object-aware%250A%2520%2520Contrastive%2520Audio-Visual%2520Fine-tuning%26entry.906535625%3DYingshan%2520Liang%2520and%2520Keyu%2520Fan%2520and%2520Zhicheng%2520Du%2520and%2520Yiran%2520Wang%2520and%2520Qingyang%2520Shi%2520and%2520Xinyu%2520Zhang%2520and%2520Jiasheng%2520Lu%2520and%2520Peiwu%2520Qin%26entry.1292438233%3D%2520%2520Video-to-audio%2520%2528V2A%2529%2520generation%2520shows%2520great%2520potential%2520in%2520fields%2520such%2520as%2520film%250Aproduction.%2520Despite%2520significant%2520advances%252C%2520current%2520V2A%2520methods%252C%2520which%2520rely%2520on%250Aglobal%2520video%2520information%252C%2520struggle%2520with%2520complex%2520scenes%2520and%2520often%2520fail%2520to%250Agenerate%2520audio%2520tailored%2520to%2520specific%2520objects%2520or%2520regions%2520in%2520the%2520videos.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520introduce%2520Hear-Your-Click%252C%2520an%2520interactive%2520V2A%250Aframework%2520that%2520enables%2520users%2520to%2520generate%2520sounds%2520for%2520specific%2520objects%2520in%2520the%250Avideos%2520by%2520simply%2520clicking%2520on%2520the%2520frame.%2520To%2520achieve%2520this%252C%2520we%2520propose%250AObject-aware%2520Contrastive%2520Audio-Visual%2520Fine-tuning%2520%2528OCAV%2529%2520with%2520a%2520Mask-guided%250AVisual%2520Encoder%2520%2528MVE%2529%2520to%2520obtain%2520object-level%2520visual%2520features%2520aligned%2520with%250Acorresponding%2520audio%2520segments.%2520Furthermore%252C%2520we%2520tailor%2520two%2520data%2520augmentation%250Astrategies%253A%2520Random%2520Video%2520Stitching%2520%2528RVS%2529%2520and%2520Mask-guided%2520Loudness%2520Modulation%250A%2528MLM%2529%252C%2520aimed%2520at%2520enhancing%2520the%2520model%2527s%2520sensitivity%2520to%2520the%2520segmented%2520objects.%2520To%250Aeffectively%2520measure%2520the%2520audio-visual%2520correspondence%252C%2520we%2520design%2520a%2520new%2520evaluation%250Ametric%252C%2520the%2520CAV%2520score%252C%2520for%2520evaluation.%2520Extensive%2520experiments%2520demonstrate%2520that%250Aour%2520framework%2520offers%2520more%2520precise%2520control%2520and%2520improved%2520generation%2520performance%250Aacross%2520various%2520metrics.%2520Project%2520Page%253A%250Ahttps%253A//github.com/SynapGrid/Hear-Your-Click%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hear-Your-Click%3A%20Interactive%20Video-to-Audio%20Generation%20via%20Object-aware%0A%20%20Contrastive%20Audio-Visual%20Fine-tuning&entry.906535625=Yingshan%20Liang%20and%20Keyu%20Fan%20and%20Zhicheng%20Du%20and%20Yiran%20Wang%20and%20Qingyang%20Shi%20and%20Xinyu%20Zhang%20and%20Jiasheng%20Lu%20and%20Peiwu%20Qin&entry.1292438233=%20%20Video-to-audio%20%28V2A%29%20generation%20shows%20great%20potential%20in%20fields%20such%20as%20film%0Aproduction.%20Despite%20significant%20advances%2C%20current%20V2A%20methods%2C%20which%20rely%20on%0Aglobal%20video%20information%2C%20struggle%20with%20complex%20scenes%20and%20often%20fail%20to%0Agenerate%20audio%20tailored%20to%20specific%20objects%20or%20regions%20in%20the%20videos.%20To%0Aaddress%20these%20limitations%2C%20we%20introduce%20Hear-Your-Click%2C%20an%20interactive%20V2A%0Aframework%20that%20enables%20users%20to%20generate%20sounds%20for%20specific%20objects%20in%20the%0Avideos%20by%20simply%20clicking%20on%20the%20frame.%20To%20achieve%20this%2C%20we%20propose%0AObject-aware%20Contrastive%20Audio-Visual%20Fine-tuning%20%28OCAV%29%20with%20a%20Mask-guided%0AVisual%20Encoder%20%28MVE%29%20to%20obtain%20object-level%20visual%20features%20aligned%20with%0Acorresponding%20audio%20segments.%20Furthermore%2C%20we%20tailor%20two%20data%20augmentation%0Astrategies%3A%20Random%20Video%20Stitching%20%28RVS%29%20and%20Mask-guided%20Loudness%20Modulation%0A%28MLM%29%2C%20aimed%20at%20enhancing%20the%20model%27s%20sensitivity%20to%20the%20segmented%20objects.%20To%0Aeffectively%20measure%20the%20audio-visual%20correspondence%2C%20we%20design%20a%20new%20evaluation%0Ametric%2C%20the%20CAV%20score%2C%20for%20evaluation.%20Extensive%20experiments%20demonstrate%20that%0Aour%20framework%20offers%20more%20precise%20control%20and%20improved%20generation%20performance%0Aacross%20various%20metrics.%20Project%20Page%3A%0Ahttps%3A//github.com/SynapGrid/Hear-Your-Click%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04959v1&entry.124074799=Read"},
{"title": "Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for\n  Visual Reasoning", "author": "Yana Wei and Liang Zhao and Jianjian Sun and Kangheng Lin and Jisheng Yin and Jingcheng Hu and Yinmin Zhang and En Yu and Haoran Lv and Zejia Weng and Jia Wang and Chunrui Han and Yuang Peng and Qi Han and Zheng Ge and Xiangyu Zhang and Daxin Jiang and Vishal M. Patel", "abstract": "  The remarkable reasoning capability of large language models (LLMs) stems\nfrom cognitive behaviors that emerge through reinforcement with verifiable\nrewards. This work investigates how to transfer this principle to Multimodal\nLLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage\nparadigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning,\nfollowed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps,\nsurpassing all previous open-source efforts in scale. This pioneering work\nreveals three fundamental insights: 1) Behavior transfer emerges surprisingly\nearly in cold start due to linguistic mental imagery. 2) Cold start broadly\nmemorizes visual behaviors, while RL critically discerns and scales up\neffective patterns. 3) Transfer strategically favors high-utility behaviors\nsuch as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR),\nachieves state-of-the-art performance on a suite of reasoning benchmarks,\nincluding 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We\nrelease our model, data, and training dynamics to catalyze the development of\nmore capable, behavior-aligned multimodal reasoners.\n", "link": "http://arxiv.org/abs/2507.05255v1", "date": "2025-07-07", "relevancy": 2.4039, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6107}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6107}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open%20Vision%20Reasoner%3A%20Transferring%20Linguistic%20Cognitive%20Behavior%20for%0A%20%20Visual%20Reasoning&body=Title%3A%20Open%20Vision%20Reasoner%3A%20Transferring%20Linguistic%20Cognitive%20Behavior%20for%0A%20%20Visual%20Reasoning%0AAuthor%3A%20Yana%20Wei%20and%20Liang%20Zhao%20and%20Jianjian%20Sun%20and%20Kangheng%20Lin%20and%20Jisheng%20Yin%20and%20Jingcheng%20Hu%20and%20Yinmin%20Zhang%20and%20En%20Yu%20and%20Haoran%20Lv%20and%20Zejia%20Weng%20and%20Jia%20Wang%20and%20Chunrui%20Han%20and%20Yuang%20Peng%20and%20Qi%20Han%20and%20Zheng%20Ge%20and%20Xiangyu%20Zhang%20and%20Daxin%20Jiang%20and%20Vishal%20M.%20Patel%0AAbstract%3A%20%20%20The%20remarkable%20reasoning%20capability%20of%20large%20language%20models%20%28LLMs%29%20stems%0Afrom%20cognitive%20behaviors%20that%20emerge%20through%20reinforcement%20with%20verifiable%0Arewards.%20This%20work%20investigates%20how%20to%20transfer%20this%20principle%20to%20Multimodal%0ALLMs%20%28MLLMs%29%20to%20unlock%20advanced%20visual%20reasoning.%20We%20introduce%20a%20two-stage%0Aparadigm%20built%20on%20Qwen2.5-VL-7B%3A%20a%20massive%20linguistic%20cold-start%20fine-tuning%2C%0Afollowed%20by%20multimodal%20reinforcement%20learning%20%28RL%29%20spanning%20nearly%201%2C000%20steps%2C%0Asurpassing%20all%20previous%20open-source%20efforts%20in%20scale.%20This%20pioneering%20work%0Areveals%20three%20fundamental%20insights%3A%201%29%20Behavior%20transfer%20emerges%20surprisingly%0Aearly%20in%20cold%20start%20due%20to%20linguistic%20mental%20imagery.%202%29%20Cold%20start%20broadly%0Amemorizes%20visual%20behaviors%2C%20while%20RL%20critically%20discerns%20and%20scales%20up%0Aeffective%20patterns.%203%29%20Transfer%20strategically%20favors%20high-utility%20behaviors%0Asuch%20as%20visual%20reflection.%20Our%20resulting%20model%2C%20Open-Vision-Reasoner%20%28OVR%29%2C%0Aachieves%20state-of-the-art%20performance%20on%20a%20suite%20of%20reasoning%20benchmarks%2C%0Aincluding%2095.3%25%20on%20MATH500%2C%2051.8%25%20on%20MathVision%20and%2054.6%25%20on%20MathVerse.%20We%0Arelease%20our%20model%2C%20data%2C%20and%20training%20dynamics%20to%20catalyze%20the%20development%20of%0Amore%20capable%2C%20behavior-aligned%20multimodal%20reasoners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen%2520Vision%2520Reasoner%253A%2520Transferring%2520Linguistic%2520Cognitive%2520Behavior%2520for%250A%2520%2520Visual%2520Reasoning%26entry.906535625%3DYana%2520Wei%2520and%2520Liang%2520Zhao%2520and%2520Jianjian%2520Sun%2520and%2520Kangheng%2520Lin%2520and%2520Jisheng%2520Yin%2520and%2520Jingcheng%2520Hu%2520and%2520Yinmin%2520Zhang%2520and%2520En%2520Yu%2520and%2520Haoran%2520Lv%2520and%2520Zejia%2520Weng%2520and%2520Jia%2520Wang%2520and%2520Chunrui%2520Han%2520and%2520Yuang%2520Peng%2520and%2520Qi%2520Han%2520and%2520Zheng%2520Ge%2520and%2520Xiangyu%2520Zhang%2520and%2520Daxin%2520Jiang%2520and%2520Vishal%2520M.%2520Patel%26entry.1292438233%3D%2520%2520The%2520remarkable%2520reasoning%2520capability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520stems%250Afrom%2520cognitive%2520behaviors%2520that%2520emerge%2520through%2520reinforcement%2520with%2520verifiable%250Arewards.%2520This%2520work%2520investigates%2520how%2520to%2520transfer%2520this%2520principle%2520to%2520Multimodal%250ALLMs%2520%2528MLLMs%2529%2520to%2520unlock%2520advanced%2520visual%2520reasoning.%2520We%2520introduce%2520a%2520two-stage%250Aparadigm%2520built%2520on%2520Qwen2.5-VL-7B%253A%2520a%2520massive%2520linguistic%2520cold-start%2520fine-tuning%252C%250Afollowed%2520by%2520multimodal%2520reinforcement%2520learning%2520%2528RL%2529%2520spanning%2520nearly%25201%252C000%2520steps%252C%250Asurpassing%2520all%2520previous%2520open-source%2520efforts%2520in%2520scale.%2520This%2520pioneering%2520work%250Areveals%2520three%2520fundamental%2520insights%253A%25201%2529%2520Behavior%2520transfer%2520emerges%2520surprisingly%250Aearly%2520in%2520cold%2520start%2520due%2520to%2520linguistic%2520mental%2520imagery.%25202%2529%2520Cold%2520start%2520broadly%250Amemorizes%2520visual%2520behaviors%252C%2520while%2520RL%2520critically%2520discerns%2520and%2520scales%2520up%250Aeffective%2520patterns.%25203%2529%2520Transfer%2520strategically%2520favors%2520high-utility%2520behaviors%250Asuch%2520as%2520visual%2520reflection.%2520Our%2520resulting%2520model%252C%2520Open-Vision-Reasoner%2520%2528OVR%2529%252C%250Aachieves%2520state-of-the-art%2520performance%2520on%2520a%2520suite%2520of%2520reasoning%2520benchmarks%252C%250Aincluding%252095.3%2525%2520on%2520MATH500%252C%252051.8%2525%2520on%2520MathVision%2520and%252054.6%2525%2520on%2520MathVerse.%2520We%250Arelease%2520our%2520model%252C%2520data%252C%2520and%2520training%2520dynamics%2520to%2520catalyze%2520the%2520development%2520of%250Amore%2520capable%252C%2520behavior-aligned%2520multimodal%2520reasoners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open%20Vision%20Reasoner%3A%20Transferring%20Linguistic%20Cognitive%20Behavior%20for%0A%20%20Visual%20Reasoning&entry.906535625=Yana%20Wei%20and%20Liang%20Zhao%20and%20Jianjian%20Sun%20and%20Kangheng%20Lin%20and%20Jisheng%20Yin%20and%20Jingcheng%20Hu%20and%20Yinmin%20Zhang%20and%20En%20Yu%20and%20Haoran%20Lv%20and%20Zejia%20Weng%20and%20Jia%20Wang%20and%20Chunrui%20Han%20and%20Yuang%20Peng%20and%20Qi%20Han%20and%20Zheng%20Ge%20and%20Xiangyu%20Zhang%20and%20Daxin%20Jiang%20and%20Vishal%20M.%20Patel&entry.1292438233=%20%20The%20remarkable%20reasoning%20capability%20of%20large%20language%20models%20%28LLMs%29%20stems%0Afrom%20cognitive%20behaviors%20that%20emerge%20through%20reinforcement%20with%20verifiable%0Arewards.%20This%20work%20investigates%20how%20to%20transfer%20this%20principle%20to%20Multimodal%0ALLMs%20%28MLLMs%29%20to%20unlock%20advanced%20visual%20reasoning.%20We%20introduce%20a%20two-stage%0Aparadigm%20built%20on%20Qwen2.5-VL-7B%3A%20a%20massive%20linguistic%20cold-start%20fine-tuning%2C%0Afollowed%20by%20multimodal%20reinforcement%20learning%20%28RL%29%20spanning%20nearly%201%2C000%20steps%2C%0Asurpassing%20all%20previous%20open-source%20efforts%20in%20scale.%20This%20pioneering%20work%0Areveals%20three%20fundamental%20insights%3A%201%29%20Behavior%20transfer%20emerges%20surprisingly%0Aearly%20in%20cold%20start%20due%20to%20linguistic%20mental%20imagery.%202%29%20Cold%20start%20broadly%0Amemorizes%20visual%20behaviors%2C%20while%20RL%20critically%20discerns%20and%20scales%20up%0Aeffective%20patterns.%203%29%20Transfer%20strategically%20favors%20high-utility%20behaviors%0Asuch%20as%20visual%20reflection.%20Our%20resulting%20model%2C%20Open-Vision-Reasoner%20%28OVR%29%2C%0Aachieves%20state-of-the-art%20performance%20on%20a%20suite%20of%20reasoning%20benchmarks%2C%0Aincluding%2095.3%25%20on%20MATH500%2C%2051.8%25%20on%20MathVision%20and%2054.6%25%20on%20MathVerse.%20We%0Arelease%20our%20model%2C%20data%2C%20and%20training%20dynamics%20to%20catalyze%20the%20development%20of%0Amore%20capable%2C%20behavior-aligned%20multimodal%20reasoners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05255v1&entry.124074799=Read"},
{"title": "Occlusion-Aware Consistent Model Predictive Control for Robot Navigation\n  in Occluded Obstacle-Dense Environments", "author": "Minzhe Zheng and Lei Zheng and Jun Ma", "abstract": "  Ensuring safety and motion consistency for robot navigation in occluded,\nobstacle-dense environments is a critical challenge. In this context, this\nstudy presents an occlusion-aware Consistent Model Predictive Control (CMPC)\nstrategy. To account for the occluded obstacles, it incorporates adjustable\nrisk regions that represent their potential future locations. Subsequently,\ndynamic risk boundary constraints are developed online to ensure safety. The\nCMPC then constructs multiple locally optimal trajectory branches (each\ntailored to different risk regions) to balance between exploitation and\nexploration. A shared consensus trunk is generated to ensure smooth transitions\nbetween branches without significant velocity fluctuations, further preserving\nmotion consistency. To facilitate high computational efficiency and ensure\ncoordination across local trajectories, we use the alternating direction method\nof multipliers (ADMM) to decompose the CMPC into manageable sub-problems for\nparallel solving. The proposed strategy is validated through simulation and\nreal-world experiments on an Ackermann-steering robot platform. The results\ndemonstrate the effectiveness of the proposed CMPC strategy through comparisons\nwith baseline approaches in occluded, obstacle-dense environments.\n", "link": "http://arxiv.org/abs/2503.04563v2", "date": "2025-07-07", "relevancy": 2.4022, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.608}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6059}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5909}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Occlusion-Aware%20Consistent%20Model%20Predictive%20Control%20for%20Robot%20Navigation%0A%20%20in%20Occluded%20Obstacle-Dense%20Environments&body=Title%3A%20Occlusion-Aware%20Consistent%20Model%20Predictive%20Control%20for%20Robot%20Navigation%0A%20%20in%20Occluded%20Obstacle-Dense%20Environments%0AAuthor%3A%20Minzhe%20Zheng%20and%20Lei%20Zheng%20and%20Jun%20Ma%0AAbstract%3A%20%20%20Ensuring%20safety%20and%20motion%20consistency%20for%20robot%20navigation%20in%20occluded%2C%0Aobstacle-dense%20environments%20is%20a%20critical%20challenge.%20In%20this%20context%2C%20this%0Astudy%20presents%20an%20occlusion-aware%20Consistent%20Model%20Predictive%20Control%20%28CMPC%29%0Astrategy.%20To%20account%20for%20the%20occluded%20obstacles%2C%20it%20incorporates%20adjustable%0Arisk%20regions%20that%20represent%20their%20potential%20future%20locations.%20Subsequently%2C%0Adynamic%20risk%20boundary%20constraints%20are%20developed%20online%20to%20ensure%20safety.%20The%0ACMPC%20then%20constructs%20multiple%20locally%20optimal%20trajectory%20branches%20%28each%0Atailored%20to%20different%20risk%20regions%29%20to%20balance%20between%20exploitation%20and%0Aexploration.%20A%20shared%20consensus%20trunk%20is%20generated%20to%20ensure%20smooth%20transitions%0Abetween%20branches%20without%20significant%20velocity%20fluctuations%2C%20further%20preserving%0Amotion%20consistency.%20To%20facilitate%20high%20computational%20efficiency%20and%20ensure%0Acoordination%20across%20local%20trajectories%2C%20we%20use%20the%20alternating%20direction%20method%0Aof%20multipliers%20%28ADMM%29%20to%20decompose%20the%20CMPC%20into%20manageable%20sub-problems%20for%0Aparallel%20solving.%20The%20proposed%20strategy%20is%20validated%20through%20simulation%20and%0Areal-world%20experiments%20on%20an%20Ackermann-steering%20robot%20platform.%20The%20results%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20CMPC%20strategy%20through%20comparisons%0Awith%20baseline%20approaches%20in%20occluded%2C%20obstacle-dense%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.04563v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOcclusion-Aware%2520Consistent%2520Model%2520Predictive%2520Control%2520for%2520Robot%2520Navigation%250A%2520%2520in%2520Occluded%2520Obstacle-Dense%2520Environments%26entry.906535625%3DMinzhe%2520Zheng%2520and%2520Lei%2520Zheng%2520and%2520Jun%2520Ma%26entry.1292438233%3D%2520%2520Ensuring%2520safety%2520and%2520motion%2520consistency%2520for%2520robot%2520navigation%2520in%2520occluded%252C%250Aobstacle-dense%2520environments%2520is%2520a%2520critical%2520challenge.%2520In%2520this%2520context%252C%2520this%250Astudy%2520presents%2520an%2520occlusion-aware%2520Consistent%2520Model%2520Predictive%2520Control%2520%2528CMPC%2529%250Astrategy.%2520To%2520account%2520for%2520the%2520occluded%2520obstacles%252C%2520it%2520incorporates%2520adjustable%250Arisk%2520regions%2520that%2520represent%2520their%2520potential%2520future%2520locations.%2520Subsequently%252C%250Adynamic%2520risk%2520boundary%2520constraints%2520are%2520developed%2520online%2520to%2520ensure%2520safety.%2520The%250ACMPC%2520then%2520constructs%2520multiple%2520locally%2520optimal%2520trajectory%2520branches%2520%2528each%250Atailored%2520to%2520different%2520risk%2520regions%2529%2520to%2520balance%2520between%2520exploitation%2520and%250Aexploration.%2520A%2520shared%2520consensus%2520trunk%2520is%2520generated%2520to%2520ensure%2520smooth%2520transitions%250Abetween%2520branches%2520without%2520significant%2520velocity%2520fluctuations%252C%2520further%2520preserving%250Amotion%2520consistency.%2520To%2520facilitate%2520high%2520computational%2520efficiency%2520and%2520ensure%250Acoordination%2520across%2520local%2520trajectories%252C%2520we%2520use%2520the%2520alternating%2520direction%2520method%250Aof%2520multipliers%2520%2528ADMM%2529%2520to%2520decompose%2520the%2520CMPC%2520into%2520manageable%2520sub-problems%2520for%250Aparallel%2520solving.%2520The%2520proposed%2520strategy%2520is%2520validated%2520through%2520simulation%2520and%250Areal-world%2520experiments%2520on%2520an%2520Ackermann-steering%2520robot%2520platform.%2520The%2520results%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520CMPC%2520strategy%2520through%2520comparisons%250Awith%2520baseline%2520approaches%2520in%2520occluded%252C%2520obstacle-dense%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04563v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Occlusion-Aware%20Consistent%20Model%20Predictive%20Control%20for%20Robot%20Navigation%0A%20%20in%20Occluded%20Obstacle-Dense%20Environments&entry.906535625=Minzhe%20Zheng%20and%20Lei%20Zheng%20and%20Jun%20Ma&entry.1292438233=%20%20Ensuring%20safety%20and%20motion%20consistency%20for%20robot%20navigation%20in%20occluded%2C%0Aobstacle-dense%20environments%20is%20a%20critical%20challenge.%20In%20this%20context%2C%20this%0Astudy%20presents%20an%20occlusion-aware%20Consistent%20Model%20Predictive%20Control%20%28CMPC%29%0Astrategy.%20To%20account%20for%20the%20occluded%20obstacles%2C%20it%20incorporates%20adjustable%0Arisk%20regions%20that%20represent%20their%20potential%20future%20locations.%20Subsequently%2C%0Adynamic%20risk%20boundary%20constraints%20are%20developed%20online%20to%20ensure%20safety.%20The%0ACMPC%20then%20constructs%20multiple%20locally%20optimal%20trajectory%20branches%20%28each%0Atailored%20to%20different%20risk%20regions%29%20to%20balance%20between%20exploitation%20and%0Aexploration.%20A%20shared%20consensus%20trunk%20is%20generated%20to%20ensure%20smooth%20transitions%0Abetween%20branches%20without%20significant%20velocity%20fluctuations%2C%20further%20preserving%0Amotion%20consistency.%20To%20facilitate%20high%20computational%20efficiency%20and%20ensure%0Acoordination%20across%20local%20trajectories%2C%20we%20use%20the%20alternating%20direction%20method%0Aof%20multipliers%20%28ADMM%29%20to%20decompose%20the%20CMPC%20into%20manageable%20sub-problems%20for%0Aparallel%20solving.%20The%20proposed%20strategy%20is%20validated%20through%20simulation%20and%0Areal-world%20experiments%20on%20an%20Ackermann-steering%20robot%20platform.%20The%20results%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20CMPC%20strategy%20through%20comparisons%0Awith%20baseline%20approaches%20in%20occluded%2C%20obstacle-dense%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.04563v2&entry.124074799=Read"},
{"title": "BackFed: An Efficient & Standardized Benchmark Suite for Backdoor\n  Attacks in Federated Learning", "author": "Thinh Dao and Dung Thuy Nguyen and Khoa D Doan and Kok-Seng Wong", "abstract": "  Federated Learning (FL) systems are vulnerable to backdoor attacks, where\nadversaries train their local models on poisoned data and submit poisoned model\nupdates to compromise the global model. Despite numerous proposed attacks and\ndefenses, divergent experimental settings, implementation errors, and\nunrealistic assumptions hinder fair comparisons and valid conclusions about\ntheir effectiveness in real-world scenarios. To address this, we introduce\nBackFed - a comprehensive benchmark suite designed to standardize, streamline,\nand reliably evaluate backdoor attacks and defenses in FL, with a focus on\npractical constraints. Our benchmark offers key advantages through its\nmulti-processing implementation that significantly accelerates experimentation\nand the modular design that enables seamless integration of new methods via\nwell-defined APIs. With a standardized evaluation pipeline, we envision BackFed\nas a plug-and-play environment for researchers to comprehensively and reliably\nevaluate new attacks and defenses. Using BackFed, we conduct large-scale\nstudies of representative backdoor attacks and defenses across both Computer\nVision and Natural Language Processing tasks with diverse model architectures\nand experimental settings. Our experiments critically assess the performance of\nproposed attacks and defenses, revealing unknown limitations and modes of\nfailures under practical conditions. These empirical insights provide valuable\nguidance for the development of new methods and for enhancing the security of\nFL systems. Our framework is openly available at\nhttps://github.com/thinh-dao/BackFed.\n", "link": "http://arxiv.org/abs/2507.04903v1", "date": "2025-07-07", "relevancy": 2.3993, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4958}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4809}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BackFed%3A%20An%20Efficient%20%26%20Standardized%20Benchmark%20Suite%20for%20Backdoor%0A%20%20Attacks%20in%20Federated%20Learning&body=Title%3A%20BackFed%3A%20An%20Efficient%20%26%20Standardized%20Benchmark%20Suite%20for%20Backdoor%0A%20%20Attacks%20in%20Federated%20Learning%0AAuthor%3A%20Thinh%20Dao%20and%20Dung%20Thuy%20Nguyen%20and%20Khoa%20D%20Doan%20and%20Kok-Seng%20Wong%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20systems%20are%20vulnerable%20to%20backdoor%20attacks%2C%20where%0Aadversaries%20train%20their%20local%20models%20on%20poisoned%20data%20and%20submit%20poisoned%20model%0Aupdates%20to%20compromise%20the%20global%20model.%20Despite%20numerous%20proposed%20attacks%20and%0Adefenses%2C%20divergent%20experimental%20settings%2C%20implementation%20errors%2C%20and%0Aunrealistic%20assumptions%20hinder%20fair%20comparisons%20and%20valid%20conclusions%20about%0Atheir%20effectiveness%20in%20real-world%20scenarios.%20To%20address%20this%2C%20we%20introduce%0ABackFed%20-%20a%20comprehensive%20benchmark%20suite%20designed%20to%20standardize%2C%20streamline%2C%0Aand%20reliably%20evaluate%20backdoor%20attacks%20and%20defenses%20in%20FL%2C%20with%20a%20focus%20on%0Apractical%20constraints.%20Our%20benchmark%20offers%20key%20advantages%20through%20its%0Amulti-processing%20implementation%20that%20significantly%20accelerates%20experimentation%0Aand%20the%20modular%20design%20that%20enables%20seamless%20integration%20of%20new%20methods%20via%0Awell-defined%20APIs.%20With%20a%20standardized%20evaluation%20pipeline%2C%20we%20envision%20BackFed%0Aas%20a%20plug-and-play%20environment%20for%20researchers%20to%20comprehensively%20and%20reliably%0Aevaluate%20new%20attacks%20and%20defenses.%20Using%20BackFed%2C%20we%20conduct%20large-scale%0Astudies%20of%20representative%20backdoor%20attacks%20and%20defenses%20across%20both%20Computer%0AVision%20and%20Natural%20Language%20Processing%20tasks%20with%20diverse%20model%20architectures%0Aand%20experimental%20settings.%20Our%20experiments%20critically%20assess%20the%20performance%20of%0Aproposed%20attacks%20and%20defenses%2C%20revealing%20unknown%20limitations%20and%20modes%20of%0Afailures%20under%20practical%20conditions.%20These%20empirical%20insights%20provide%20valuable%0Aguidance%20for%20the%20development%20of%20new%20methods%20and%20for%20enhancing%20the%20security%20of%0AFL%20systems.%20Our%20framework%20is%20openly%20available%20at%0Ahttps%3A//github.com/thinh-dao/BackFed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04903v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackFed%253A%2520An%2520Efficient%2520%2526%2520Standardized%2520Benchmark%2520Suite%2520for%2520Backdoor%250A%2520%2520Attacks%2520in%2520Federated%2520Learning%26entry.906535625%3DThinh%2520Dao%2520and%2520Dung%2520Thuy%2520Nguyen%2520and%2520Khoa%2520D%2520Doan%2520and%2520Kok-Seng%2520Wong%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520systems%2520are%2520vulnerable%2520to%2520backdoor%2520attacks%252C%2520where%250Aadversaries%2520train%2520their%2520local%2520models%2520on%2520poisoned%2520data%2520and%2520submit%2520poisoned%2520model%250Aupdates%2520to%2520compromise%2520the%2520global%2520model.%2520Despite%2520numerous%2520proposed%2520attacks%2520and%250Adefenses%252C%2520divergent%2520experimental%2520settings%252C%2520implementation%2520errors%252C%2520and%250Aunrealistic%2520assumptions%2520hinder%2520fair%2520comparisons%2520and%2520valid%2520conclusions%2520about%250Atheir%2520effectiveness%2520in%2520real-world%2520scenarios.%2520To%2520address%2520this%252C%2520we%2520introduce%250ABackFed%2520-%2520a%2520comprehensive%2520benchmark%2520suite%2520designed%2520to%2520standardize%252C%2520streamline%252C%250Aand%2520reliably%2520evaluate%2520backdoor%2520attacks%2520and%2520defenses%2520in%2520FL%252C%2520with%2520a%2520focus%2520on%250Apractical%2520constraints.%2520Our%2520benchmark%2520offers%2520key%2520advantages%2520through%2520its%250Amulti-processing%2520implementation%2520that%2520significantly%2520accelerates%2520experimentation%250Aand%2520the%2520modular%2520design%2520that%2520enables%2520seamless%2520integration%2520of%2520new%2520methods%2520via%250Awell-defined%2520APIs.%2520With%2520a%2520standardized%2520evaluation%2520pipeline%252C%2520we%2520envision%2520BackFed%250Aas%2520a%2520plug-and-play%2520environment%2520for%2520researchers%2520to%2520comprehensively%2520and%2520reliably%250Aevaluate%2520new%2520attacks%2520and%2520defenses.%2520Using%2520BackFed%252C%2520we%2520conduct%2520large-scale%250Astudies%2520of%2520representative%2520backdoor%2520attacks%2520and%2520defenses%2520across%2520both%2520Computer%250AVision%2520and%2520Natural%2520Language%2520Processing%2520tasks%2520with%2520diverse%2520model%2520architectures%250Aand%2520experimental%2520settings.%2520Our%2520experiments%2520critically%2520assess%2520the%2520performance%2520of%250Aproposed%2520attacks%2520and%2520defenses%252C%2520revealing%2520unknown%2520limitations%2520and%2520modes%2520of%250Afailures%2520under%2520practical%2520conditions.%2520These%2520empirical%2520insights%2520provide%2520valuable%250Aguidance%2520for%2520the%2520development%2520of%2520new%2520methods%2520and%2520for%2520enhancing%2520the%2520security%2520of%250AFL%2520systems.%2520Our%2520framework%2520is%2520openly%2520available%2520at%250Ahttps%253A//github.com/thinh-dao/BackFed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04903v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BackFed%3A%20An%20Efficient%20%26%20Standardized%20Benchmark%20Suite%20for%20Backdoor%0A%20%20Attacks%20in%20Federated%20Learning&entry.906535625=Thinh%20Dao%20and%20Dung%20Thuy%20Nguyen%20and%20Khoa%20D%20Doan%20and%20Kok-Seng%20Wong&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20systems%20are%20vulnerable%20to%20backdoor%20attacks%2C%20where%0Aadversaries%20train%20their%20local%20models%20on%20poisoned%20data%20and%20submit%20poisoned%20model%0Aupdates%20to%20compromise%20the%20global%20model.%20Despite%20numerous%20proposed%20attacks%20and%0Adefenses%2C%20divergent%20experimental%20settings%2C%20implementation%20errors%2C%20and%0Aunrealistic%20assumptions%20hinder%20fair%20comparisons%20and%20valid%20conclusions%20about%0Atheir%20effectiveness%20in%20real-world%20scenarios.%20To%20address%20this%2C%20we%20introduce%0ABackFed%20-%20a%20comprehensive%20benchmark%20suite%20designed%20to%20standardize%2C%20streamline%2C%0Aand%20reliably%20evaluate%20backdoor%20attacks%20and%20defenses%20in%20FL%2C%20with%20a%20focus%20on%0Apractical%20constraints.%20Our%20benchmark%20offers%20key%20advantages%20through%20its%0Amulti-processing%20implementation%20that%20significantly%20accelerates%20experimentation%0Aand%20the%20modular%20design%20that%20enables%20seamless%20integration%20of%20new%20methods%20via%0Awell-defined%20APIs.%20With%20a%20standardized%20evaluation%20pipeline%2C%20we%20envision%20BackFed%0Aas%20a%20plug-and-play%20environment%20for%20researchers%20to%20comprehensively%20and%20reliably%0Aevaluate%20new%20attacks%20and%20defenses.%20Using%20BackFed%2C%20we%20conduct%20large-scale%0Astudies%20of%20representative%20backdoor%20attacks%20and%20defenses%20across%20both%20Computer%0AVision%20and%20Natural%20Language%20Processing%20tasks%20with%20diverse%20model%20architectures%0Aand%20experimental%20settings.%20Our%20experiments%20critically%20assess%20the%20performance%20of%0Aproposed%20attacks%20and%20defenses%2C%20revealing%20unknown%20limitations%20and%20modes%20of%0Afailures%20under%20practical%20conditions.%20These%20empirical%20insights%20provide%20valuable%0Aguidance%20for%20the%20development%20of%20new%20methods%20and%20for%20enhancing%20the%20security%20of%0AFL%20systems.%20Our%20framework%20is%20openly%20available%20at%0Ahttps%3A//github.com/thinh-dao/BackFed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04903v1&entry.124074799=Read"},
{"title": "NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed\n  Target Tracking in Unstructured GPS-Denied Environments", "author": "Alessandro Saviolo and Giuseppe Loianno", "abstract": "  Autonomous aerial target tracking in unstructured and GPS-denied environments\nremains a fundamental challenge in robotics. Many existing methods rely on\nmotion capture systems, pre-mapped scenes, or feature-based localization to\nensure safety and control, limiting their deployment in real-world conditions.\nWe introduce NOVA, a fully onboard, object-centric framework that enables\nrobust target tracking and collision-aware navigation using only a stereo\ncamera and an IMU. Rather than constructing a global map or relying on absolute\nlocalization, NOVA formulates perception, estimation, and control entirely in\nthe target's reference frame. A tightly integrated stack combines a lightweight\nobject detector with stereo depth completion, followed by histogram-based\nfiltering to infer robust target distances under occlusion and noise. These\nmeasurements feed a visual-inertial state estimator that recovers the full\n6-DoF pose of the robot relative to the target. A nonlinear model predictive\ncontroller (NMPC) plans dynamically feasible trajectories in the target frame.\nTo ensure safety, high-order control barrier functions are constructed online\nfrom a compact set of high-risk collision points extracted from depth, enabling\nreal-time obstacle avoidance without maps or dense representations. We validate\nNOVA across challenging real-world scenarios, including urban mazes, forest\ntrails, and repeated transitions through buildings with intermittent GPS loss\nand severe lighting changes that disrupt feature-based localization. Each\nexperiment is repeated multiple times under similar conditions to assess\nresilience, showing consistent and reliable performance. NOVA achieves agile\ntarget following at speeds exceeding 50 km/h. These results show that\nhigh-speed vision-based tracking is possible in the wild using only onboard\nsensing, with no reliance on external localization or environment assumptions.\n", "link": "http://arxiv.org/abs/2506.18689v2", "date": "2025-07-07", "relevancy": 2.375, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.607}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5905}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NOVA%3A%20Navigation%20via%20Object-Centric%20Visual%20Autonomy%20for%20High-Speed%0A%20%20Target%20Tracking%20in%20Unstructured%20GPS-Denied%20Environments&body=Title%3A%20NOVA%3A%20Navigation%20via%20Object-Centric%20Visual%20Autonomy%20for%20High-Speed%0A%20%20Target%20Tracking%20in%20Unstructured%20GPS-Denied%20Environments%0AAuthor%3A%20Alessandro%20Saviolo%20and%20Giuseppe%20Loianno%0AAbstract%3A%20%20%20Autonomous%20aerial%20target%20tracking%20in%20unstructured%20and%20GPS-denied%20environments%0Aremains%20a%20fundamental%20challenge%20in%20robotics.%20Many%20existing%20methods%20rely%20on%0Amotion%20capture%20systems%2C%20pre-mapped%20scenes%2C%20or%20feature-based%20localization%20to%0Aensure%20safety%20and%20control%2C%20limiting%20their%20deployment%20in%20real-world%20conditions.%0AWe%20introduce%20NOVA%2C%20a%20fully%20onboard%2C%20object-centric%20framework%20that%20enables%0Arobust%20target%20tracking%20and%20collision-aware%20navigation%20using%20only%20a%20stereo%0Acamera%20and%20an%20IMU.%20Rather%20than%20constructing%20a%20global%20map%20or%20relying%20on%20absolute%0Alocalization%2C%20NOVA%20formulates%20perception%2C%20estimation%2C%20and%20control%20entirely%20in%0Athe%20target%27s%20reference%20frame.%20A%20tightly%20integrated%20stack%20combines%20a%20lightweight%0Aobject%20detector%20with%20stereo%20depth%20completion%2C%20followed%20by%20histogram-based%0Afiltering%20to%20infer%20robust%20target%20distances%20under%20occlusion%20and%20noise.%20These%0Ameasurements%20feed%20a%20visual-inertial%20state%20estimator%20that%20recovers%20the%20full%0A6-DoF%20pose%20of%20the%20robot%20relative%20to%20the%20target.%20A%20nonlinear%20model%20predictive%0Acontroller%20%28NMPC%29%20plans%20dynamically%20feasible%20trajectories%20in%20the%20target%20frame.%0ATo%20ensure%20safety%2C%20high-order%20control%20barrier%20functions%20are%20constructed%20online%0Afrom%20a%20compact%20set%20of%20high-risk%20collision%20points%20extracted%20from%20depth%2C%20enabling%0Areal-time%20obstacle%20avoidance%20without%20maps%20or%20dense%20representations.%20We%20validate%0ANOVA%20across%20challenging%20real-world%20scenarios%2C%20including%20urban%20mazes%2C%20forest%0Atrails%2C%20and%20repeated%20transitions%20through%20buildings%20with%20intermittent%20GPS%20loss%0Aand%20severe%20lighting%20changes%20that%20disrupt%20feature-based%20localization.%20Each%0Aexperiment%20is%20repeated%20multiple%20times%20under%20similar%20conditions%20to%20assess%0Aresilience%2C%20showing%20consistent%20and%20reliable%20performance.%20NOVA%20achieves%20agile%0Atarget%20following%20at%20speeds%20exceeding%2050%20km/h.%20These%20results%20show%20that%0Ahigh-speed%20vision-based%20tracking%20is%20possible%20in%20the%20wild%20using%20only%20onboard%0Asensing%2C%20with%20no%20reliance%20on%20external%20localization%20or%20environment%20assumptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.18689v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNOVA%253A%2520Navigation%2520via%2520Object-Centric%2520Visual%2520Autonomy%2520for%2520High-Speed%250A%2520%2520Target%2520Tracking%2520in%2520Unstructured%2520GPS-Denied%2520Environments%26entry.906535625%3DAlessandro%2520Saviolo%2520and%2520Giuseppe%2520Loianno%26entry.1292438233%3D%2520%2520Autonomous%2520aerial%2520target%2520tracking%2520in%2520unstructured%2520and%2520GPS-denied%2520environments%250Aremains%2520a%2520fundamental%2520challenge%2520in%2520robotics.%2520Many%2520existing%2520methods%2520rely%2520on%250Amotion%2520capture%2520systems%252C%2520pre-mapped%2520scenes%252C%2520or%2520feature-based%2520localization%2520to%250Aensure%2520safety%2520and%2520control%252C%2520limiting%2520their%2520deployment%2520in%2520real-world%2520conditions.%250AWe%2520introduce%2520NOVA%252C%2520a%2520fully%2520onboard%252C%2520object-centric%2520framework%2520that%2520enables%250Arobust%2520target%2520tracking%2520and%2520collision-aware%2520navigation%2520using%2520only%2520a%2520stereo%250Acamera%2520and%2520an%2520IMU.%2520Rather%2520than%2520constructing%2520a%2520global%2520map%2520or%2520relying%2520on%2520absolute%250Alocalization%252C%2520NOVA%2520formulates%2520perception%252C%2520estimation%252C%2520and%2520control%2520entirely%2520in%250Athe%2520target%2527s%2520reference%2520frame.%2520A%2520tightly%2520integrated%2520stack%2520combines%2520a%2520lightweight%250Aobject%2520detector%2520with%2520stereo%2520depth%2520completion%252C%2520followed%2520by%2520histogram-based%250Afiltering%2520to%2520infer%2520robust%2520target%2520distances%2520under%2520occlusion%2520and%2520noise.%2520These%250Ameasurements%2520feed%2520a%2520visual-inertial%2520state%2520estimator%2520that%2520recovers%2520the%2520full%250A6-DoF%2520pose%2520of%2520the%2520robot%2520relative%2520to%2520the%2520target.%2520A%2520nonlinear%2520model%2520predictive%250Acontroller%2520%2528NMPC%2529%2520plans%2520dynamically%2520feasible%2520trajectories%2520in%2520the%2520target%2520frame.%250ATo%2520ensure%2520safety%252C%2520high-order%2520control%2520barrier%2520functions%2520are%2520constructed%2520online%250Afrom%2520a%2520compact%2520set%2520of%2520high-risk%2520collision%2520points%2520extracted%2520from%2520depth%252C%2520enabling%250Areal-time%2520obstacle%2520avoidance%2520without%2520maps%2520or%2520dense%2520representations.%2520We%2520validate%250ANOVA%2520across%2520challenging%2520real-world%2520scenarios%252C%2520including%2520urban%2520mazes%252C%2520forest%250Atrails%252C%2520and%2520repeated%2520transitions%2520through%2520buildings%2520with%2520intermittent%2520GPS%2520loss%250Aand%2520severe%2520lighting%2520changes%2520that%2520disrupt%2520feature-based%2520localization.%2520Each%250Aexperiment%2520is%2520repeated%2520multiple%2520times%2520under%2520similar%2520conditions%2520to%2520assess%250Aresilience%252C%2520showing%2520consistent%2520and%2520reliable%2520performance.%2520NOVA%2520achieves%2520agile%250Atarget%2520following%2520at%2520speeds%2520exceeding%252050%2520km/h.%2520These%2520results%2520show%2520that%250Ahigh-speed%2520vision-based%2520tracking%2520is%2520possible%2520in%2520the%2520wild%2520using%2520only%2520onboard%250Asensing%252C%2520with%2520no%2520reliance%2520on%2520external%2520localization%2520or%2520environment%2520assumptions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.18689v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NOVA%3A%20Navigation%20via%20Object-Centric%20Visual%20Autonomy%20for%20High-Speed%0A%20%20Target%20Tracking%20in%20Unstructured%20GPS-Denied%20Environments&entry.906535625=Alessandro%20Saviolo%20and%20Giuseppe%20Loianno&entry.1292438233=%20%20Autonomous%20aerial%20target%20tracking%20in%20unstructured%20and%20GPS-denied%20environments%0Aremains%20a%20fundamental%20challenge%20in%20robotics.%20Many%20existing%20methods%20rely%20on%0Amotion%20capture%20systems%2C%20pre-mapped%20scenes%2C%20or%20feature-based%20localization%20to%0Aensure%20safety%20and%20control%2C%20limiting%20their%20deployment%20in%20real-world%20conditions.%0AWe%20introduce%20NOVA%2C%20a%20fully%20onboard%2C%20object-centric%20framework%20that%20enables%0Arobust%20target%20tracking%20and%20collision-aware%20navigation%20using%20only%20a%20stereo%0Acamera%20and%20an%20IMU.%20Rather%20than%20constructing%20a%20global%20map%20or%20relying%20on%20absolute%0Alocalization%2C%20NOVA%20formulates%20perception%2C%20estimation%2C%20and%20control%20entirely%20in%0Athe%20target%27s%20reference%20frame.%20A%20tightly%20integrated%20stack%20combines%20a%20lightweight%0Aobject%20detector%20with%20stereo%20depth%20completion%2C%20followed%20by%20histogram-based%0Afiltering%20to%20infer%20robust%20target%20distances%20under%20occlusion%20and%20noise.%20These%0Ameasurements%20feed%20a%20visual-inertial%20state%20estimator%20that%20recovers%20the%20full%0A6-DoF%20pose%20of%20the%20robot%20relative%20to%20the%20target.%20A%20nonlinear%20model%20predictive%0Acontroller%20%28NMPC%29%20plans%20dynamically%20feasible%20trajectories%20in%20the%20target%20frame.%0ATo%20ensure%20safety%2C%20high-order%20control%20barrier%20functions%20are%20constructed%20online%0Afrom%20a%20compact%20set%20of%20high-risk%20collision%20points%20extracted%20from%20depth%2C%20enabling%0Areal-time%20obstacle%20avoidance%20without%20maps%20or%20dense%20representations.%20We%20validate%0ANOVA%20across%20challenging%20real-world%20scenarios%2C%20including%20urban%20mazes%2C%20forest%0Atrails%2C%20and%20repeated%20transitions%20through%20buildings%20with%20intermittent%20GPS%20loss%0Aand%20severe%20lighting%20changes%20that%20disrupt%20feature-based%20localization.%20Each%0Aexperiment%20is%20repeated%20multiple%20times%20under%20similar%20conditions%20to%20assess%0Aresilience%2C%20showing%20consistent%20and%20reliable%20performance.%20NOVA%20achieves%20agile%0Atarget%20following%20at%20speeds%20exceeding%2050%20km/h.%20These%20results%20show%20that%0Ahigh-speed%20vision-based%20tracking%20is%20possible%20in%20the%20wild%20using%20only%20onboard%0Asensing%2C%20with%20no%20reliance%20on%20external%20localization%20or%20environment%20assumptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.18689v2&entry.124074799=Read"},
{"title": "HV-MMBench: Benchmarking MLLMs for Human-Centric Video Understanding", "author": "Yuxuan Cai and Jiangning Zhang and Zhenye Gan and Qingdong He and Xiaobin Hu and Junwei Zhu and Yabiao Wang and Chengjie Wang and Zhucun Xue and Xinwei He and Xiang Bai", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated significant\nadvances in visual understanding tasks involving both images and videos.\nHowever, their capacity to comprehend human-centric video data remains\nunderexplored, primarily due to the absence of comprehensive and high-quality\nevaluation benchmarks. Existing human-centric benchmarks predominantly\nemphasize video generation quality and action recognition, while overlooking\nessential perceptual and cognitive abilities required in human-centered\nscenarios. Furthermore, they are often limited by single-question paradigms and\noverly simplistic evaluation metrics. To address above limitations, we propose\na modern HV-MMBench, a rigorously curated benchmark designed to provide a more\nholistic evaluation of MLLMs in human-centric video understanding. Compared to\nexisting human-centric video benchmarks, our work offers the following key\nfeatures: (1) Diverse evaluation dimensions: HV-MMBench encompasses 15 tasks,\nranging from basic attribute perception (e.g., age estimation, emotion\nrecognition) to advanced cognitive reasoning (e.g., social relationship\nprediction, intention prediction), enabling comprehensive assessment of model\ncapabilities; (2) Varied data types: The benchmark includes multiple-choice,\nfill-in-blank, true/false, and open-ended question formats, combined with\ndiverse evaluation metrics, to more accurately and robustly reflect model\nperformance; (3) Multi-domain video coverage: The benchmark spans 50 distinct\nvisual scenarios, enabling comprehensive evaluation across fine-grained scene\nvariations; (4) Temporal coverage: The benchmark covers videos from short-term\n(10 seconds) to long-term (up to 30min) durations, supporting systematic\nanalysis of models temporal reasoning abilities across diverse contextual\nlengths.\n", "link": "http://arxiv.org/abs/2507.04909v1", "date": "2025-07-07", "relevancy": 2.3664, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6029}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6029}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HV-MMBench%3A%20Benchmarking%20MLLMs%20for%20Human-Centric%20Video%20Understanding&body=Title%3A%20HV-MMBench%3A%20Benchmarking%20MLLMs%20for%20Human-Centric%20Video%20Understanding%0AAuthor%3A%20Yuxuan%20Cai%20and%20Jiangning%20Zhang%20and%20Zhenye%20Gan%20and%20Qingdong%20He%20and%20Xiaobin%20Hu%20and%20Junwei%20Zhu%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Zhucun%20Xue%20and%20Xinwei%20He%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20significant%0Aadvances%20in%20visual%20understanding%20tasks%20involving%20both%20images%20and%20videos.%0AHowever%2C%20their%20capacity%20to%20comprehend%20human-centric%20video%20data%20remains%0Aunderexplored%2C%20primarily%20due%20to%20the%20absence%20of%20comprehensive%20and%20high-quality%0Aevaluation%20benchmarks.%20Existing%20human-centric%20benchmarks%20predominantly%0Aemphasize%20video%20generation%20quality%20and%20action%20recognition%2C%20while%20overlooking%0Aessential%20perceptual%20and%20cognitive%20abilities%20required%20in%20human-centered%0Ascenarios.%20Furthermore%2C%20they%20are%20often%20limited%20by%20single-question%20paradigms%20and%0Aoverly%20simplistic%20evaluation%20metrics.%20To%20address%20above%20limitations%2C%20we%20propose%0Aa%20modern%20HV-MMBench%2C%20a%20rigorously%20curated%20benchmark%20designed%20to%20provide%20a%20more%0Aholistic%20evaluation%20of%20MLLMs%20in%20human-centric%20video%20understanding.%20Compared%20to%0Aexisting%20human-centric%20video%20benchmarks%2C%20our%20work%20offers%20the%20following%20key%0Afeatures%3A%20%281%29%20Diverse%20evaluation%20dimensions%3A%20HV-MMBench%20encompasses%2015%20tasks%2C%0Aranging%20from%20basic%20attribute%20perception%20%28e.g.%2C%20age%20estimation%2C%20emotion%0Arecognition%29%20to%20advanced%20cognitive%20reasoning%20%28e.g.%2C%20social%20relationship%0Aprediction%2C%20intention%20prediction%29%2C%20enabling%20comprehensive%20assessment%20of%20model%0Acapabilities%3B%20%282%29%20Varied%20data%20types%3A%20The%20benchmark%20includes%20multiple-choice%2C%0Afill-in-blank%2C%20true/false%2C%20and%20open-ended%20question%20formats%2C%20combined%20with%0Adiverse%20evaluation%20metrics%2C%20to%20more%20accurately%20and%20robustly%20reflect%20model%0Aperformance%3B%20%283%29%20Multi-domain%20video%20coverage%3A%20The%20benchmark%20spans%2050%20distinct%0Avisual%20scenarios%2C%20enabling%20comprehensive%20evaluation%20across%20fine-grained%20scene%0Avariations%3B%20%284%29%20Temporal%20coverage%3A%20The%20benchmark%20covers%20videos%20from%20short-term%0A%2810%20seconds%29%20to%20long-term%20%28up%20to%2030min%29%20durations%2C%20supporting%20systematic%0Aanalysis%20of%20models%20temporal%20reasoning%20abilities%20across%20diverse%20contextual%0Alengths.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHV-MMBench%253A%2520Benchmarking%2520MLLMs%2520for%2520Human-Centric%2520Video%2520Understanding%26entry.906535625%3DYuxuan%2520Cai%2520and%2520Jiangning%2520Zhang%2520and%2520Zhenye%2520Gan%2520and%2520Qingdong%2520He%2520and%2520Xiaobin%2520Hu%2520and%2520Junwei%2520Zhu%2520and%2520Yabiao%2520Wang%2520and%2520Chengjie%2520Wang%2520and%2520Zhucun%2520Xue%2520and%2520Xinwei%2520He%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520significant%250Aadvances%2520in%2520visual%2520understanding%2520tasks%2520involving%2520both%2520images%2520and%2520videos.%250AHowever%252C%2520their%2520capacity%2520to%2520comprehend%2520human-centric%2520video%2520data%2520remains%250Aunderexplored%252C%2520primarily%2520due%2520to%2520the%2520absence%2520of%2520comprehensive%2520and%2520high-quality%250Aevaluation%2520benchmarks.%2520Existing%2520human-centric%2520benchmarks%2520predominantly%250Aemphasize%2520video%2520generation%2520quality%2520and%2520action%2520recognition%252C%2520while%2520overlooking%250Aessential%2520perceptual%2520and%2520cognitive%2520abilities%2520required%2520in%2520human-centered%250Ascenarios.%2520Furthermore%252C%2520they%2520are%2520often%2520limited%2520by%2520single-question%2520paradigms%2520and%250Aoverly%2520simplistic%2520evaluation%2520metrics.%2520To%2520address%2520above%2520limitations%252C%2520we%2520propose%250Aa%2520modern%2520HV-MMBench%252C%2520a%2520rigorously%2520curated%2520benchmark%2520designed%2520to%2520provide%2520a%2520more%250Aholistic%2520evaluation%2520of%2520MLLMs%2520in%2520human-centric%2520video%2520understanding.%2520Compared%2520to%250Aexisting%2520human-centric%2520video%2520benchmarks%252C%2520our%2520work%2520offers%2520the%2520following%2520key%250Afeatures%253A%2520%25281%2529%2520Diverse%2520evaluation%2520dimensions%253A%2520HV-MMBench%2520encompasses%252015%2520tasks%252C%250Aranging%2520from%2520basic%2520attribute%2520perception%2520%2528e.g.%252C%2520age%2520estimation%252C%2520emotion%250Arecognition%2529%2520to%2520advanced%2520cognitive%2520reasoning%2520%2528e.g.%252C%2520social%2520relationship%250Aprediction%252C%2520intention%2520prediction%2529%252C%2520enabling%2520comprehensive%2520assessment%2520of%2520model%250Acapabilities%253B%2520%25282%2529%2520Varied%2520data%2520types%253A%2520The%2520benchmark%2520includes%2520multiple-choice%252C%250Afill-in-blank%252C%2520true/false%252C%2520and%2520open-ended%2520question%2520formats%252C%2520combined%2520with%250Adiverse%2520evaluation%2520metrics%252C%2520to%2520more%2520accurately%2520and%2520robustly%2520reflect%2520model%250Aperformance%253B%2520%25283%2529%2520Multi-domain%2520video%2520coverage%253A%2520The%2520benchmark%2520spans%252050%2520distinct%250Avisual%2520scenarios%252C%2520enabling%2520comprehensive%2520evaluation%2520across%2520fine-grained%2520scene%250Avariations%253B%2520%25284%2529%2520Temporal%2520coverage%253A%2520The%2520benchmark%2520covers%2520videos%2520from%2520short-term%250A%252810%2520seconds%2529%2520to%2520long-term%2520%2528up%2520to%252030min%2529%2520durations%252C%2520supporting%2520systematic%250Aanalysis%2520of%2520models%2520temporal%2520reasoning%2520abilities%2520across%2520diverse%2520contextual%250Alengths.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HV-MMBench%3A%20Benchmarking%20MLLMs%20for%20Human-Centric%20Video%20Understanding&entry.906535625=Yuxuan%20Cai%20and%20Jiangning%20Zhang%20and%20Zhenye%20Gan%20and%20Qingdong%20He%20and%20Xiaobin%20Hu%20and%20Junwei%20Zhu%20and%20Yabiao%20Wang%20and%20Chengjie%20Wang%20and%20Zhucun%20Xue%20and%20Xinwei%20He%20and%20Xiang%20Bai&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20significant%0Aadvances%20in%20visual%20understanding%20tasks%20involving%20both%20images%20and%20videos.%0AHowever%2C%20their%20capacity%20to%20comprehend%20human-centric%20video%20data%20remains%0Aunderexplored%2C%20primarily%20due%20to%20the%20absence%20of%20comprehensive%20and%20high-quality%0Aevaluation%20benchmarks.%20Existing%20human-centric%20benchmarks%20predominantly%0Aemphasize%20video%20generation%20quality%20and%20action%20recognition%2C%20while%20overlooking%0Aessential%20perceptual%20and%20cognitive%20abilities%20required%20in%20human-centered%0Ascenarios.%20Furthermore%2C%20they%20are%20often%20limited%20by%20single-question%20paradigms%20and%0Aoverly%20simplistic%20evaluation%20metrics.%20To%20address%20above%20limitations%2C%20we%20propose%0Aa%20modern%20HV-MMBench%2C%20a%20rigorously%20curated%20benchmark%20designed%20to%20provide%20a%20more%0Aholistic%20evaluation%20of%20MLLMs%20in%20human-centric%20video%20understanding.%20Compared%20to%0Aexisting%20human-centric%20video%20benchmarks%2C%20our%20work%20offers%20the%20following%20key%0Afeatures%3A%20%281%29%20Diverse%20evaluation%20dimensions%3A%20HV-MMBench%20encompasses%2015%20tasks%2C%0Aranging%20from%20basic%20attribute%20perception%20%28e.g.%2C%20age%20estimation%2C%20emotion%0Arecognition%29%20to%20advanced%20cognitive%20reasoning%20%28e.g.%2C%20social%20relationship%0Aprediction%2C%20intention%20prediction%29%2C%20enabling%20comprehensive%20assessment%20of%20model%0Acapabilities%3B%20%282%29%20Varied%20data%20types%3A%20The%20benchmark%20includes%20multiple-choice%2C%0Afill-in-blank%2C%20true/false%2C%20and%20open-ended%20question%20formats%2C%20combined%20with%0Adiverse%20evaluation%20metrics%2C%20to%20more%20accurately%20and%20robustly%20reflect%20model%0Aperformance%3B%20%283%29%20Multi-domain%20video%20coverage%3A%20The%20benchmark%20spans%2050%20distinct%0Avisual%20scenarios%2C%20enabling%20comprehensive%20evaluation%20across%20fine-grained%20scene%0Avariations%3B%20%284%29%20Temporal%20coverage%3A%20The%20benchmark%20covers%20videos%20from%20short-term%0A%2810%20seconds%29%20to%20long-term%20%28up%20to%2030min%29%20durations%2C%20supporting%20systematic%0Aanalysis%20of%20models%20temporal%20reasoning%20abilities%20across%20diverse%20contextual%0Alengths.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04909v1&entry.124074799=Read"},
{"title": "Piggyback Camera: Easy-to-Deploy Visual Surveillance by Mobile Sensing\n  on Commercial Robot Vacuums", "author": "Ryo Yonetani", "abstract": "  This paper presents Piggyback Camera, an easy-to-deploy system for visual\nsurveillance using commercial robot vacuums. Rather than requiring access to\ninternal robot systems, our approach mounts a smartphone equipped with a camera\nand Inertial Measurement Unit (IMU) on the robot, making it applicable to any\ncommercial robot without hardware modifications. The system estimates robot\nposes through neural inertial navigation and efficiently captures images at\nregular spatial intervals throughout the cleaning task. We develop a novel\ntest-time data augmentation method called Rotation-Augmented Ensemble (RAE) to\nmitigate domain gaps in neural inertial navigation. A loop closure method that\nexploits robot cleaning patterns further refines these estimated poses. We\ndemonstrate the system with an object mapping application that analyzes\ncaptured images to geo-localize objects in the environment. Experimental\nevaluation in retail environments shows that our approach achieves 0.83 m\nrelative pose error for robot localization and 0.97 m positional error for\nobject mapping of over 100 items.\n", "link": "http://arxiv.org/abs/2507.04910v1", "date": "2025-07-07", "relevancy": 2.3656, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6045}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5843}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Piggyback%20Camera%3A%20Easy-to-Deploy%20Visual%20Surveillance%20by%20Mobile%20Sensing%0A%20%20on%20Commercial%20Robot%20Vacuums&body=Title%3A%20Piggyback%20Camera%3A%20Easy-to-Deploy%20Visual%20Surveillance%20by%20Mobile%20Sensing%0A%20%20on%20Commercial%20Robot%20Vacuums%0AAuthor%3A%20Ryo%20Yonetani%0AAbstract%3A%20%20%20This%20paper%20presents%20Piggyback%20Camera%2C%20an%20easy-to-deploy%20system%20for%20visual%0Asurveillance%20using%20commercial%20robot%20vacuums.%20Rather%20than%20requiring%20access%20to%0Ainternal%20robot%20systems%2C%20our%20approach%20mounts%20a%20smartphone%20equipped%20with%20a%20camera%0Aand%20Inertial%20Measurement%20Unit%20%28IMU%29%20on%20the%20robot%2C%20making%20it%20applicable%20to%20any%0Acommercial%20robot%20without%20hardware%20modifications.%20The%20system%20estimates%20robot%0Aposes%20through%20neural%20inertial%20navigation%20and%20efficiently%20captures%20images%20at%0Aregular%20spatial%20intervals%20throughout%20the%20cleaning%20task.%20We%20develop%20a%20novel%0Atest-time%20data%20augmentation%20method%20called%20Rotation-Augmented%20Ensemble%20%28RAE%29%20to%0Amitigate%20domain%20gaps%20in%20neural%20inertial%20navigation.%20A%20loop%20closure%20method%20that%0Aexploits%20robot%20cleaning%20patterns%20further%20refines%20these%20estimated%20poses.%20We%0Ademonstrate%20the%20system%20with%20an%20object%20mapping%20application%20that%20analyzes%0Acaptured%20images%20to%20geo-localize%20objects%20in%20the%20environment.%20Experimental%0Aevaluation%20in%20retail%20environments%20shows%20that%20our%20approach%20achieves%200.83%20m%0Arelative%20pose%20error%20for%20robot%20localization%20and%200.97%20m%20positional%20error%20for%0Aobject%20mapping%20of%20over%20100%20items.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPiggyback%2520Camera%253A%2520Easy-to-Deploy%2520Visual%2520Surveillance%2520by%2520Mobile%2520Sensing%250A%2520%2520on%2520Commercial%2520Robot%2520Vacuums%26entry.906535625%3DRyo%2520Yonetani%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520Piggyback%2520Camera%252C%2520an%2520easy-to-deploy%2520system%2520for%2520visual%250Asurveillance%2520using%2520commercial%2520robot%2520vacuums.%2520Rather%2520than%2520requiring%2520access%2520to%250Ainternal%2520robot%2520systems%252C%2520our%2520approach%2520mounts%2520a%2520smartphone%2520equipped%2520with%2520a%2520camera%250Aand%2520Inertial%2520Measurement%2520Unit%2520%2528IMU%2529%2520on%2520the%2520robot%252C%2520making%2520it%2520applicable%2520to%2520any%250Acommercial%2520robot%2520without%2520hardware%2520modifications.%2520The%2520system%2520estimates%2520robot%250Aposes%2520through%2520neural%2520inertial%2520navigation%2520and%2520efficiently%2520captures%2520images%2520at%250Aregular%2520spatial%2520intervals%2520throughout%2520the%2520cleaning%2520task.%2520We%2520develop%2520a%2520novel%250Atest-time%2520data%2520augmentation%2520method%2520called%2520Rotation-Augmented%2520Ensemble%2520%2528RAE%2529%2520to%250Amitigate%2520domain%2520gaps%2520in%2520neural%2520inertial%2520navigation.%2520A%2520loop%2520closure%2520method%2520that%250Aexploits%2520robot%2520cleaning%2520patterns%2520further%2520refines%2520these%2520estimated%2520poses.%2520We%250Ademonstrate%2520the%2520system%2520with%2520an%2520object%2520mapping%2520application%2520that%2520analyzes%250Acaptured%2520images%2520to%2520geo-localize%2520objects%2520in%2520the%2520environment.%2520Experimental%250Aevaluation%2520in%2520retail%2520environments%2520shows%2520that%2520our%2520approach%2520achieves%25200.83%2520m%250Arelative%2520pose%2520error%2520for%2520robot%2520localization%2520and%25200.97%2520m%2520positional%2520error%2520for%250Aobject%2520mapping%2520of%2520over%2520100%2520items.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Piggyback%20Camera%3A%20Easy-to-Deploy%20Visual%20Surveillance%20by%20Mobile%20Sensing%0A%20%20on%20Commercial%20Robot%20Vacuums&entry.906535625=Ryo%20Yonetani&entry.1292438233=%20%20This%20paper%20presents%20Piggyback%20Camera%2C%20an%20easy-to-deploy%20system%20for%20visual%0Asurveillance%20using%20commercial%20robot%20vacuums.%20Rather%20than%20requiring%20access%20to%0Ainternal%20robot%20systems%2C%20our%20approach%20mounts%20a%20smartphone%20equipped%20with%20a%20camera%0Aand%20Inertial%20Measurement%20Unit%20%28IMU%29%20on%20the%20robot%2C%20making%20it%20applicable%20to%20any%0Acommercial%20robot%20without%20hardware%20modifications.%20The%20system%20estimates%20robot%0Aposes%20through%20neural%20inertial%20navigation%20and%20efficiently%20captures%20images%20at%0Aregular%20spatial%20intervals%20throughout%20the%20cleaning%20task.%20We%20develop%20a%20novel%0Atest-time%20data%20augmentation%20method%20called%20Rotation-Augmented%20Ensemble%20%28RAE%29%20to%0Amitigate%20domain%20gaps%20in%20neural%20inertial%20navigation.%20A%20loop%20closure%20method%20that%0Aexploits%20robot%20cleaning%20patterns%20further%20refines%20these%20estimated%20poses.%20We%0Ademonstrate%20the%20system%20with%20an%20object%20mapping%20application%20that%20analyzes%0Acaptured%20images%20to%20geo-localize%20objects%20in%20the%20environment.%20Experimental%0Aevaluation%20in%20retail%20environments%20shows%20that%20our%20approach%20achieves%200.83%20m%0Arelative%20pose%20error%20for%20robot%20localization%20and%200.97%20m%20positional%20error%20for%0Aobject%20mapping%20of%20over%20100%20items.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04910v1&entry.124074799=Read"},
{"title": "Estimating Object Physical Properties from RGB-D Vision and Depth Robot\n  Sensors Using Deep Learning", "author": "Ricardo Cardoso and Plinio Moreno", "abstract": "  Inertial mass plays a crucial role in robotic applications such as object\ngrasping, manipulation, and simulation, providing a strong prior for planning\nand control. Accurately estimating an object's mass before interaction can\nsignificantly enhance the performance of various robotic tasks. However, mass\nestimation using only vision sensors is a relatively underexplored area. This\npaper proposes a novel approach combining sparse point-cloud data from depth\nimages with RGB images to estimate the mass of objects. We evaluate a range of\npoint-cloud processing architectures, alongside RGB-only methods. To overcome\nthe limited availability of training data, we create a synthetic dataset using\nShapeNetSem 3D models, simulating RGBD images via a Kinect camera. This\nsynthetic data is used to train an image generation model for estimating dense\ndepth maps, which we then use to augment an existing dataset of images paired\nwith mass values. Our approach significantly outperforms existing benchmarks\nacross all evaluated metrics. The data generation\n(https://github.com/RavineWindteer/ShapenetSem-to-RGBD) as well as the training\nof the depth estimator (https://github.com/RavineWindteer/GLPDepth-Edited) and\nthe mass estimator (https://github.com/RavineWindteer/Depth-mass-estimator) are\navailable online.\n", "link": "http://arxiv.org/abs/2507.05029v1", "date": "2025-07-07", "relevancy": 2.3597, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6784}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5722}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20Object%20Physical%20Properties%20from%20RGB-D%20Vision%20and%20Depth%20Robot%0A%20%20Sensors%20Using%20Deep%20Learning&body=Title%3A%20Estimating%20Object%20Physical%20Properties%20from%20RGB-D%20Vision%20and%20Depth%20Robot%0A%20%20Sensors%20Using%20Deep%20Learning%0AAuthor%3A%20Ricardo%20Cardoso%20and%20Plinio%20Moreno%0AAbstract%3A%20%20%20Inertial%20mass%20plays%20a%20crucial%20role%20in%20robotic%20applications%20such%20as%20object%0Agrasping%2C%20manipulation%2C%20and%20simulation%2C%20providing%20a%20strong%20prior%20for%20planning%0Aand%20control.%20Accurately%20estimating%20an%20object%27s%20mass%20before%20interaction%20can%0Asignificantly%20enhance%20the%20performance%20of%20various%20robotic%20tasks.%20However%2C%20mass%0Aestimation%20using%20only%20vision%20sensors%20is%20a%20relatively%20underexplored%20area.%20This%0Apaper%20proposes%20a%20novel%20approach%20combining%20sparse%20point-cloud%20data%20from%20depth%0Aimages%20with%20RGB%20images%20to%20estimate%20the%20mass%20of%20objects.%20We%20evaluate%20a%20range%20of%0Apoint-cloud%20processing%20architectures%2C%20alongside%20RGB-only%20methods.%20To%20overcome%0Athe%20limited%20availability%20of%20training%20data%2C%20we%20create%20a%20synthetic%20dataset%20using%0AShapeNetSem%203D%20models%2C%20simulating%20RGBD%20images%20via%20a%20Kinect%20camera.%20This%0Asynthetic%20data%20is%20used%20to%20train%20an%20image%20generation%20model%20for%20estimating%20dense%0Adepth%20maps%2C%20which%20we%20then%20use%20to%20augment%20an%20existing%20dataset%20of%20images%20paired%0Awith%20mass%20values.%20Our%20approach%20significantly%20outperforms%20existing%20benchmarks%0Aacross%20all%20evaluated%20metrics.%20The%20data%20generation%0A%28https%3A//github.com/RavineWindteer/ShapenetSem-to-RGBD%29%20as%20well%20as%20the%20training%0Aof%20the%20depth%20estimator%20%28https%3A//github.com/RavineWindteer/GLPDepth-Edited%29%20and%0Athe%20mass%20estimator%20%28https%3A//github.com/RavineWindteer/Depth-mass-estimator%29%20are%0Aavailable%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05029v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520Object%2520Physical%2520Properties%2520from%2520RGB-D%2520Vision%2520and%2520Depth%2520Robot%250A%2520%2520Sensors%2520Using%2520Deep%2520Learning%26entry.906535625%3DRicardo%2520Cardoso%2520and%2520Plinio%2520Moreno%26entry.1292438233%3D%2520%2520Inertial%2520mass%2520plays%2520a%2520crucial%2520role%2520in%2520robotic%2520applications%2520such%2520as%2520object%250Agrasping%252C%2520manipulation%252C%2520and%2520simulation%252C%2520providing%2520a%2520strong%2520prior%2520for%2520planning%250Aand%2520control.%2520Accurately%2520estimating%2520an%2520object%2527s%2520mass%2520before%2520interaction%2520can%250Asignificantly%2520enhance%2520the%2520performance%2520of%2520various%2520robotic%2520tasks.%2520However%252C%2520mass%250Aestimation%2520using%2520only%2520vision%2520sensors%2520is%2520a%2520relatively%2520underexplored%2520area.%2520This%250Apaper%2520proposes%2520a%2520novel%2520approach%2520combining%2520sparse%2520point-cloud%2520data%2520from%2520depth%250Aimages%2520with%2520RGB%2520images%2520to%2520estimate%2520the%2520mass%2520of%2520objects.%2520We%2520evaluate%2520a%2520range%2520of%250Apoint-cloud%2520processing%2520architectures%252C%2520alongside%2520RGB-only%2520methods.%2520To%2520overcome%250Athe%2520limited%2520availability%2520of%2520training%2520data%252C%2520we%2520create%2520a%2520synthetic%2520dataset%2520using%250AShapeNetSem%25203D%2520models%252C%2520simulating%2520RGBD%2520images%2520via%2520a%2520Kinect%2520camera.%2520This%250Asynthetic%2520data%2520is%2520used%2520to%2520train%2520an%2520image%2520generation%2520model%2520for%2520estimating%2520dense%250Adepth%2520maps%252C%2520which%2520we%2520then%2520use%2520to%2520augment%2520an%2520existing%2520dataset%2520of%2520images%2520paired%250Awith%2520mass%2520values.%2520Our%2520approach%2520significantly%2520outperforms%2520existing%2520benchmarks%250Aacross%2520all%2520evaluated%2520metrics.%2520The%2520data%2520generation%250A%2528https%253A//github.com/RavineWindteer/ShapenetSem-to-RGBD%2529%2520as%2520well%2520as%2520the%2520training%250Aof%2520the%2520depth%2520estimator%2520%2528https%253A//github.com/RavineWindteer/GLPDepth-Edited%2529%2520and%250Athe%2520mass%2520estimator%2520%2528https%253A//github.com/RavineWindteer/Depth-mass-estimator%2529%2520are%250Aavailable%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05029v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20Object%20Physical%20Properties%20from%20RGB-D%20Vision%20and%20Depth%20Robot%0A%20%20Sensors%20Using%20Deep%20Learning&entry.906535625=Ricardo%20Cardoso%20and%20Plinio%20Moreno&entry.1292438233=%20%20Inertial%20mass%20plays%20a%20crucial%20role%20in%20robotic%20applications%20such%20as%20object%0Agrasping%2C%20manipulation%2C%20and%20simulation%2C%20providing%20a%20strong%20prior%20for%20planning%0Aand%20control.%20Accurately%20estimating%20an%20object%27s%20mass%20before%20interaction%20can%0Asignificantly%20enhance%20the%20performance%20of%20various%20robotic%20tasks.%20However%2C%20mass%0Aestimation%20using%20only%20vision%20sensors%20is%20a%20relatively%20underexplored%20area.%20This%0Apaper%20proposes%20a%20novel%20approach%20combining%20sparse%20point-cloud%20data%20from%20depth%0Aimages%20with%20RGB%20images%20to%20estimate%20the%20mass%20of%20objects.%20We%20evaluate%20a%20range%20of%0Apoint-cloud%20processing%20architectures%2C%20alongside%20RGB-only%20methods.%20To%20overcome%0Athe%20limited%20availability%20of%20training%20data%2C%20we%20create%20a%20synthetic%20dataset%20using%0AShapeNetSem%203D%20models%2C%20simulating%20RGBD%20images%20via%20a%20Kinect%20camera.%20This%0Asynthetic%20data%20is%20used%20to%20train%20an%20image%20generation%20model%20for%20estimating%20dense%0Adepth%20maps%2C%20which%20we%20then%20use%20to%20augment%20an%20existing%20dataset%20of%20images%20paired%0Awith%20mass%20values.%20Our%20approach%20significantly%20outperforms%20existing%20benchmarks%0Aacross%20all%20evaluated%20metrics.%20The%20data%20generation%0A%28https%3A//github.com/RavineWindteer/ShapenetSem-to-RGBD%29%20as%20well%20as%20the%20training%0Aof%20the%20depth%20estimator%20%28https%3A//github.com/RavineWindteer/GLPDepth-Edited%29%20and%0Athe%20mass%20estimator%20%28https%3A//github.com/RavineWindteer/Depth-mass-estimator%29%20are%0Aavailable%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05029v1&entry.124074799=Read"},
{"title": "OpenS2S: Advancing Open-Source End-to-End Empathetic Large Speech\n  Language Model", "author": "Chen Wang and Tianyu Peng and Wen Yang and Yinan Bai and Guangfu Wang and Jun Lin and Lanpeng Jia and Lingxiang Wu and Jinqiao Wang and Chengqing Zong and Jiajun Zhang", "abstract": "  Empathetic interaction is a cornerstone of human-machine communication, due\nto the need for understanding speech enriched with paralinguistic cues and\ngenerating emotional and expressive responses. However, the most powerful\nempathetic LSLMs are increasingly closed off, leaving the crucial details about\nthe architecture, data and development opaque to researchers. Given the\ncritical need for transparent research into the LSLMs and empathetic behavior,\nwe present OpenS2S, a fully open-source, transparent and end-to-end LSLM\ndesigned to enable empathetic speech interactions. Based on our empathetic\nspeech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved\ndecoding architecture to achieve low-latency speech generation. To facilitate\nend-to-end training, OpenS2S incorporates an automated data construction\npipeline that synthesizes diverse, high-quality empathetic speech dialogues at\nlow cost. By leveraging large language models to generate empathetic content\nand controllable text-to-speech systems to introduce speaker and emotional\nvariation, we construct a scalable training corpus with rich paralinguistic\ndiversity and minimal human supervision. We release the fully open-source\nOpenS2S model, including the dataset, model weights, pre-training and\nfine-tuning codes, to empower the broader research community and accelerate\ninnovation in empathetic speech systems. The project webpage can be accessed at\nhttps://casia-lm.github.io/OpenS2S\n", "link": "http://arxiv.org/abs/2507.05177v1", "date": "2025-07-07", "relevancy": 2.3574, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4834}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4834}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenS2S%3A%20Advancing%20Open-Source%20End-to-End%20Empathetic%20Large%20Speech%0A%20%20Language%20Model&body=Title%3A%20OpenS2S%3A%20Advancing%20Open-Source%20End-to-End%20Empathetic%20Large%20Speech%0A%20%20Language%20Model%0AAuthor%3A%20Chen%20Wang%20and%20Tianyu%20Peng%20and%20Wen%20Yang%20and%20Yinan%20Bai%20and%20Guangfu%20Wang%20and%20Jun%20Lin%20and%20Lanpeng%20Jia%20and%20Lingxiang%20Wu%20and%20Jinqiao%20Wang%20and%20Chengqing%20Zong%20and%20Jiajun%20Zhang%0AAbstract%3A%20%20%20Empathetic%20interaction%20is%20a%20cornerstone%20of%20human-machine%20communication%2C%20due%0Ato%20the%20need%20for%20understanding%20speech%20enriched%20with%20paralinguistic%20cues%20and%0Agenerating%20emotional%20and%20expressive%20responses.%20However%2C%20the%20most%20powerful%0Aempathetic%20LSLMs%20are%20increasingly%20closed%20off%2C%20leaving%20the%20crucial%20details%20about%0Athe%20architecture%2C%20data%20and%20development%20opaque%20to%20researchers.%20Given%20the%0Acritical%20need%20for%20transparent%20research%20into%20the%20LSLMs%20and%20empathetic%20behavior%2C%0Awe%20present%20OpenS2S%2C%20a%20fully%20open-source%2C%20transparent%20and%20end-to-end%20LSLM%0Adesigned%20to%20enable%20empathetic%20speech%20interactions.%20Based%20on%20our%20empathetic%0Aspeech-to-text%20model%20BLSP-Emo%2C%20OpenS2S%20further%20employs%20a%20streaming%20interleaved%0Adecoding%20architecture%20to%20achieve%20low-latency%20speech%20generation.%20To%20facilitate%0Aend-to-end%20training%2C%20OpenS2S%20incorporates%20an%20automated%20data%20construction%0Apipeline%20that%20synthesizes%20diverse%2C%20high-quality%20empathetic%20speech%20dialogues%20at%0Alow%20cost.%20By%20leveraging%20large%20language%20models%20to%20generate%20empathetic%20content%0Aand%20controllable%20text-to-speech%20systems%20to%20introduce%20speaker%20and%20emotional%0Avariation%2C%20we%20construct%20a%20scalable%20training%20corpus%20with%20rich%20paralinguistic%0Adiversity%20and%20minimal%20human%20supervision.%20We%20release%20the%20fully%20open-source%0AOpenS2S%20model%2C%20including%20the%20dataset%2C%20model%20weights%2C%20pre-training%20and%0Afine-tuning%20codes%2C%20to%20empower%20the%20broader%20research%20community%20and%20accelerate%0Ainnovation%20in%20empathetic%20speech%20systems.%20The%20project%20webpage%20can%20be%20accessed%20at%0Ahttps%3A//casia-lm.github.io/OpenS2S%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenS2S%253A%2520Advancing%2520Open-Source%2520End-to-End%2520Empathetic%2520Large%2520Speech%250A%2520%2520Language%2520Model%26entry.906535625%3DChen%2520Wang%2520and%2520Tianyu%2520Peng%2520and%2520Wen%2520Yang%2520and%2520Yinan%2520Bai%2520and%2520Guangfu%2520Wang%2520and%2520Jun%2520Lin%2520and%2520Lanpeng%2520Jia%2520and%2520Lingxiang%2520Wu%2520and%2520Jinqiao%2520Wang%2520and%2520Chengqing%2520Zong%2520and%2520Jiajun%2520Zhang%26entry.1292438233%3D%2520%2520Empathetic%2520interaction%2520is%2520a%2520cornerstone%2520of%2520human-machine%2520communication%252C%2520due%250Ato%2520the%2520need%2520for%2520understanding%2520speech%2520enriched%2520with%2520paralinguistic%2520cues%2520and%250Agenerating%2520emotional%2520and%2520expressive%2520responses.%2520However%252C%2520the%2520most%2520powerful%250Aempathetic%2520LSLMs%2520are%2520increasingly%2520closed%2520off%252C%2520leaving%2520the%2520crucial%2520details%2520about%250Athe%2520architecture%252C%2520data%2520and%2520development%2520opaque%2520to%2520researchers.%2520Given%2520the%250Acritical%2520need%2520for%2520transparent%2520research%2520into%2520the%2520LSLMs%2520and%2520empathetic%2520behavior%252C%250Awe%2520present%2520OpenS2S%252C%2520a%2520fully%2520open-source%252C%2520transparent%2520and%2520end-to-end%2520LSLM%250Adesigned%2520to%2520enable%2520empathetic%2520speech%2520interactions.%2520Based%2520on%2520our%2520empathetic%250Aspeech-to-text%2520model%2520BLSP-Emo%252C%2520OpenS2S%2520further%2520employs%2520a%2520streaming%2520interleaved%250Adecoding%2520architecture%2520to%2520achieve%2520low-latency%2520speech%2520generation.%2520To%2520facilitate%250Aend-to-end%2520training%252C%2520OpenS2S%2520incorporates%2520an%2520automated%2520data%2520construction%250Apipeline%2520that%2520synthesizes%2520diverse%252C%2520high-quality%2520empathetic%2520speech%2520dialogues%2520at%250Alow%2520cost.%2520By%2520leveraging%2520large%2520language%2520models%2520to%2520generate%2520empathetic%2520content%250Aand%2520controllable%2520text-to-speech%2520systems%2520to%2520introduce%2520speaker%2520and%2520emotional%250Avariation%252C%2520we%2520construct%2520a%2520scalable%2520training%2520corpus%2520with%2520rich%2520paralinguistic%250Adiversity%2520and%2520minimal%2520human%2520supervision.%2520We%2520release%2520the%2520fully%2520open-source%250AOpenS2S%2520model%252C%2520including%2520the%2520dataset%252C%2520model%2520weights%252C%2520pre-training%2520and%250Afine-tuning%2520codes%252C%2520to%2520empower%2520the%2520broader%2520research%2520community%2520and%2520accelerate%250Ainnovation%2520in%2520empathetic%2520speech%2520systems.%2520The%2520project%2520webpage%2520can%2520be%2520accessed%2520at%250Ahttps%253A//casia-lm.github.io/OpenS2S%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenS2S%3A%20Advancing%20Open-Source%20End-to-End%20Empathetic%20Large%20Speech%0A%20%20Language%20Model&entry.906535625=Chen%20Wang%20and%20Tianyu%20Peng%20and%20Wen%20Yang%20and%20Yinan%20Bai%20and%20Guangfu%20Wang%20and%20Jun%20Lin%20and%20Lanpeng%20Jia%20and%20Lingxiang%20Wu%20and%20Jinqiao%20Wang%20and%20Chengqing%20Zong%20and%20Jiajun%20Zhang&entry.1292438233=%20%20Empathetic%20interaction%20is%20a%20cornerstone%20of%20human-machine%20communication%2C%20due%0Ato%20the%20need%20for%20understanding%20speech%20enriched%20with%20paralinguistic%20cues%20and%0Agenerating%20emotional%20and%20expressive%20responses.%20However%2C%20the%20most%20powerful%0Aempathetic%20LSLMs%20are%20increasingly%20closed%20off%2C%20leaving%20the%20crucial%20details%20about%0Athe%20architecture%2C%20data%20and%20development%20opaque%20to%20researchers.%20Given%20the%0Acritical%20need%20for%20transparent%20research%20into%20the%20LSLMs%20and%20empathetic%20behavior%2C%0Awe%20present%20OpenS2S%2C%20a%20fully%20open-source%2C%20transparent%20and%20end-to-end%20LSLM%0Adesigned%20to%20enable%20empathetic%20speech%20interactions.%20Based%20on%20our%20empathetic%0Aspeech-to-text%20model%20BLSP-Emo%2C%20OpenS2S%20further%20employs%20a%20streaming%20interleaved%0Adecoding%20architecture%20to%20achieve%20low-latency%20speech%20generation.%20To%20facilitate%0Aend-to-end%20training%2C%20OpenS2S%20incorporates%20an%20automated%20data%20construction%0Apipeline%20that%20synthesizes%20diverse%2C%20high-quality%20empathetic%20speech%20dialogues%20at%0Alow%20cost.%20By%20leveraging%20large%20language%20models%20to%20generate%20empathetic%20content%0Aand%20controllable%20text-to-speech%20systems%20to%20introduce%20speaker%20and%20emotional%0Avariation%2C%20we%20construct%20a%20scalable%20training%20corpus%20with%20rich%20paralinguistic%0Adiversity%20and%20minimal%20human%20supervision.%20We%20release%20the%20fully%20open-source%0AOpenS2S%20model%2C%20including%20the%20dataset%2C%20model%20weights%2C%20pre-training%20and%0Afine-tuning%20codes%2C%20to%20empower%20the%20broader%20research%20community%20and%20accelerate%0Ainnovation%20in%20empathetic%20speech%20systems.%20The%20project%20webpage%20can%20be%20accessed%20at%0Ahttps%3A//casia-lm.github.io/OpenS2S%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05177v1&entry.124074799=Read"},
{"title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling", "author": "Meng Wei and Chenyang Wan and Xiqian Yu and Tai Wang and Yuqiang Yang and Xiaohan Mao and Chenming Zhu and Wenzhe Cai and Hanqing Wang and Yilun Chen and Xihui Liu and Jiangmiao Pang", "abstract": "  Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}.\n", "link": "http://arxiv.org/abs/2507.05240v1", "date": "2025-07-07", "relevancy": 2.3353, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5955}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5955}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StreamVLN%3A%20Streaming%20Vision-and-Language%20Navigation%20via%20SlowFast%20Context%0A%20%20Modeling&body=Title%3A%20StreamVLN%3A%20Streaming%20Vision-and-Language%20Navigation%20via%20SlowFast%20Context%0A%20%20Modeling%0AAuthor%3A%20Meng%20Wei%20and%20Chenyang%20Wan%20and%20Xiqian%20Yu%20and%20Tai%20Wang%20and%20Yuqiang%20Yang%20and%20Xiaohan%20Mao%20and%20Chenming%20Zhu%20and%20Wenzhe%20Cai%20and%20Hanqing%20Wang%20and%20Yilun%20Chen%20and%20Xihui%20Liu%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Vision-and-Language%20Navigation%20%28VLN%29%20in%20real-world%20settings%20requires%20agents%0Ato%20process%20continuous%20visual%20streams%20and%20generate%20actions%20with%20low%20latency%0Agrounded%20in%20language%20instructions.%20While%20Video-based%20Large%20Language%20Models%0A%28Video-LLMs%29%20have%20driven%20recent%20progress%2C%20current%20VLN%20methods%20based%20on%0AVideo-LLM%20often%20face%20trade-offs%20among%20fine-grained%20visual%20understanding%2C%0Along-term%20context%20modeling%20and%20computational%20efficiency.%20We%20introduce%0AStreamVLN%2C%20a%20streaming%20VLN%20framework%20that%20employs%20a%20hybrid%20slow-fast%20context%0Amodeling%20strategy%20to%20support%20multi-modal%20reasoning%20over%20interleaved%20vision%2C%0Alanguage%20and%20action%20inputs.%20The%20fast-streaming%20dialogue%20context%20facilitates%0Aresponsive%20action%20generation%20through%20a%20sliding-window%20of%20active%20dialogues%2C%0Awhile%20the%20slow-updating%20memory%20context%20compresses%20historical%20visual%20states%0Ausing%20a%203D-aware%20token%20pruning%20strategy.%20With%20this%20slow-fast%20design%2C%20StreamVLN%0Aachieves%20coherent%20multi-turn%20dialogue%20through%20efficient%20KV%20cache%20reuse%2C%0Asupporting%20long%20video%20streams%20with%20bounded%20context%20size%20and%20inference%20cost.%0AExperiments%20on%20VLN-CE%20benchmarks%20demonstrate%20state-of-the-art%20performance%20with%0Astable%20low%20latency%2C%20ensuring%20robustness%20and%20efficiency%20in%20real-world%0Adeployment.%20The%20project%20page%20is%3A%0A%5Chref%7Bhttps%3A//streamvln.github.io/%7D%7Bhttps%3A//streamvln.github.io/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreamVLN%253A%2520Streaming%2520Vision-and-Language%2520Navigation%2520via%2520SlowFast%2520Context%250A%2520%2520Modeling%26entry.906535625%3DMeng%2520Wei%2520and%2520Chenyang%2520Wan%2520and%2520Xiqian%2520Yu%2520and%2520Tai%2520Wang%2520and%2520Yuqiang%2520Yang%2520and%2520Xiaohan%2520Mao%2520and%2520Chenming%2520Zhu%2520and%2520Wenzhe%2520Cai%2520and%2520Hanqing%2520Wang%2520and%2520Yilun%2520Chen%2520and%2520Xihui%2520Liu%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520Vision-and-Language%2520Navigation%2520%2528VLN%2529%2520in%2520real-world%2520settings%2520requires%2520agents%250Ato%2520process%2520continuous%2520visual%2520streams%2520and%2520generate%2520actions%2520with%2520low%2520latency%250Agrounded%2520in%2520language%2520instructions.%2520While%2520Video-based%2520Large%2520Language%2520Models%250A%2528Video-LLMs%2529%2520have%2520driven%2520recent%2520progress%252C%2520current%2520VLN%2520methods%2520based%2520on%250AVideo-LLM%2520often%2520face%2520trade-offs%2520among%2520fine-grained%2520visual%2520understanding%252C%250Along-term%2520context%2520modeling%2520and%2520computational%2520efficiency.%2520We%2520introduce%250AStreamVLN%252C%2520a%2520streaming%2520VLN%2520framework%2520that%2520employs%2520a%2520hybrid%2520slow-fast%2520context%250Amodeling%2520strategy%2520to%2520support%2520multi-modal%2520reasoning%2520over%2520interleaved%2520vision%252C%250Alanguage%2520and%2520action%2520inputs.%2520The%2520fast-streaming%2520dialogue%2520context%2520facilitates%250Aresponsive%2520action%2520generation%2520through%2520a%2520sliding-window%2520of%2520active%2520dialogues%252C%250Awhile%2520the%2520slow-updating%2520memory%2520context%2520compresses%2520historical%2520visual%2520states%250Ausing%2520a%25203D-aware%2520token%2520pruning%2520strategy.%2520With%2520this%2520slow-fast%2520design%252C%2520StreamVLN%250Aachieves%2520coherent%2520multi-turn%2520dialogue%2520through%2520efficient%2520KV%2520cache%2520reuse%252C%250Asupporting%2520long%2520video%2520streams%2520with%2520bounded%2520context%2520size%2520and%2520inference%2520cost.%250AExperiments%2520on%2520VLN-CE%2520benchmarks%2520demonstrate%2520state-of-the-art%2520performance%2520with%250Astable%2520low%2520latency%252C%2520ensuring%2520robustness%2520and%2520efficiency%2520in%2520real-world%250Adeployment.%2520The%2520project%2520page%2520is%253A%250A%255Chref%257Bhttps%253A//streamvln.github.io/%257D%257Bhttps%253A//streamvln.github.io/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StreamVLN%3A%20Streaming%20Vision-and-Language%20Navigation%20via%20SlowFast%20Context%0A%20%20Modeling&entry.906535625=Meng%20Wei%20and%20Chenyang%20Wan%20and%20Xiqian%20Yu%20and%20Tai%20Wang%20and%20Yuqiang%20Yang%20and%20Xiaohan%20Mao%20and%20Chenming%20Zhu%20and%20Wenzhe%20Cai%20and%20Hanqing%20Wang%20and%20Yilun%20Chen%20and%20Xihui%20Liu%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Vision-and-Language%20Navigation%20%28VLN%29%20in%20real-world%20settings%20requires%20agents%0Ato%20process%20continuous%20visual%20streams%20and%20generate%20actions%20with%20low%20latency%0Agrounded%20in%20language%20instructions.%20While%20Video-based%20Large%20Language%20Models%0A%28Video-LLMs%29%20have%20driven%20recent%20progress%2C%20current%20VLN%20methods%20based%20on%0AVideo-LLM%20often%20face%20trade-offs%20among%20fine-grained%20visual%20understanding%2C%0Along-term%20context%20modeling%20and%20computational%20efficiency.%20We%20introduce%0AStreamVLN%2C%20a%20streaming%20VLN%20framework%20that%20employs%20a%20hybrid%20slow-fast%20context%0Amodeling%20strategy%20to%20support%20multi-modal%20reasoning%20over%20interleaved%20vision%2C%0Alanguage%20and%20action%20inputs.%20The%20fast-streaming%20dialogue%20context%20facilitates%0Aresponsive%20action%20generation%20through%20a%20sliding-window%20of%20active%20dialogues%2C%0Awhile%20the%20slow-updating%20memory%20context%20compresses%20historical%20visual%20states%0Ausing%20a%203D-aware%20token%20pruning%20strategy.%20With%20this%20slow-fast%20design%2C%20StreamVLN%0Aachieves%20coherent%20multi-turn%20dialogue%20through%20efficient%20KV%20cache%20reuse%2C%0Asupporting%20long%20video%20streams%20with%20bounded%20context%20size%20and%20inference%20cost.%0AExperiments%20on%20VLN-CE%20benchmarks%20demonstrate%20state-of-the-art%20performance%20with%0Astable%20low%20latency%2C%20ensuring%20robustness%20and%20efficiency%20in%20real-world%0Adeployment.%20The%20project%20page%20is%3A%0A%5Chref%7Bhttps%3A//streamvln.github.io/%7D%7Bhttps%3A//streamvln.github.io/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05240v1&entry.124074799=Read"},
{"title": "EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via\n  Embodied World Modeling", "author": "Boyuan Wang and Xinpan Meng and Xiaofeng Wang and Zheng Zhu and Angen Ye and Yang Wang and Zhiqin Yang and Chaojun Ni and Guan Huang and Xingang Wang", "abstract": "  The rapid advancement of Embodied AI has led to an increasing demand for\nlarge-scale, high-quality real-world data. However, collecting such embodied\ndata remains costly and inefficient. As a result, simulation environments have\nbecome a crucial surrogate for training robot policies. Yet, the significant\nReal2Sim2Real gap remains a critical bottleneck, particularly in terms of\nphysical dynamics and visual appearance. To address this challenge, we propose\nEmbodieDreamer, a novel framework that reduces the Real2Sim2Real gap from both\nthe physics and appearance perspectives. Specifically, we propose PhysAligner,\na differentiable physics module designed to reduce the Real2Sim physical gap.\nIt jointly optimizes robot-specific parameters such as control gains and\nfriction coefficients to better align simulated dynamics with real-world\nobservations. In addition, we introduce VisAligner, which incorporates a\nconditional video diffusion model to bridge the Sim2Real appearance gap by\ntranslating low-fidelity simulated renderings into photorealistic videos\nconditioned on simulation states, enabling high-fidelity visual transfer.\nExtensive experiments validate the effectiveness of EmbodieDreamer. The\nproposed PhysAligner reduces physical parameter estimation error by 3.74%\ncompared to simulated annealing methods while improving optimization speed by\n89.91\\%. Moreover, training robot policies in the generated photorealistic\nenvironment leads to a 29.17% improvement in the average task success rate\nacross real-world tasks after reinforcement learning. Code, model and data will\nbe publicly available.\n", "link": "http://arxiv.org/abs/2507.05198v1", "date": "2025-07-07", "relevancy": 2.3294, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5963}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5816}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmbodieDreamer%3A%20Advancing%20Real2Sim2Real%20Transfer%20for%20Policy%20Training%20via%0A%20%20Embodied%20World%20Modeling&body=Title%3A%20EmbodieDreamer%3A%20Advancing%20Real2Sim2Real%20Transfer%20for%20Policy%20Training%20via%0A%20%20Embodied%20World%20Modeling%0AAuthor%3A%20Boyuan%20Wang%20and%20Xinpan%20Meng%20and%20Xiaofeng%20Wang%20and%20Zheng%20Zhu%20and%20Angen%20Ye%20and%20Yang%20Wang%20and%20Zhiqin%20Yang%20and%20Chaojun%20Ni%20and%20Guan%20Huang%20and%20Xingang%20Wang%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20Embodied%20AI%20has%20led%20to%20an%20increasing%20demand%20for%0Alarge-scale%2C%20high-quality%20real-world%20data.%20However%2C%20collecting%20such%20embodied%0Adata%20remains%20costly%20and%20inefficient.%20As%20a%20result%2C%20simulation%20environments%20have%0Abecome%20a%20crucial%20surrogate%20for%20training%20robot%20policies.%20Yet%2C%20the%20significant%0AReal2Sim2Real%20gap%20remains%20a%20critical%20bottleneck%2C%20particularly%20in%20terms%20of%0Aphysical%20dynamics%20and%20visual%20appearance.%20To%20address%20this%20challenge%2C%20we%20propose%0AEmbodieDreamer%2C%20a%20novel%20framework%20that%20reduces%20the%20Real2Sim2Real%20gap%20from%20both%0Athe%20physics%20and%20appearance%20perspectives.%20Specifically%2C%20we%20propose%20PhysAligner%2C%0Aa%20differentiable%20physics%20module%20designed%20to%20reduce%20the%20Real2Sim%20physical%20gap.%0AIt%20jointly%20optimizes%20robot-specific%20parameters%20such%20as%20control%20gains%20and%0Afriction%20coefficients%20to%20better%20align%20simulated%20dynamics%20with%20real-world%0Aobservations.%20In%20addition%2C%20we%20introduce%20VisAligner%2C%20which%20incorporates%20a%0Aconditional%20video%20diffusion%20model%20to%20bridge%20the%20Sim2Real%20appearance%20gap%20by%0Atranslating%20low-fidelity%20simulated%20renderings%20into%20photorealistic%20videos%0Aconditioned%20on%20simulation%20states%2C%20enabling%20high-fidelity%20visual%20transfer.%0AExtensive%20experiments%20validate%20the%20effectiveness%20of%20EmbodieDreamer.%20The%0Aproposed%20PhysAligner%20reduces%20physical%20parameter%20estimation%20error%20by%203.74%25%0Acompared%20to%20simulated%20annealing%20methods%20while%20improving%20optimization%20speed%20by%0A89.91%5C%25.%20Moreover%2C%20training%20robot%20policies%20in%20the%20generated%20photorealistic%0Aenvironment%20leads%20to%20a%2029.17%25%20improvement%20in%20the%20average%20task%20success%20rate%0Aacross%20real-world%20tasks%20after%20reinforcement%20learning.%20Code%2C%20model%20and%20data%20will%0Abe%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbodieDreamer%253A%2520Advancing%2520Real2Sim2Real%2520Transfer%2520for%2520Policy%2520Training%2520via%250A%2520%2520Embodied%2520World%2520Modeling%26entry.906535625%3DBoyuan%2520Wang%2520and%2520Xinpan%2520Meng%2520and%2520Xiaofeng%2520Wang%2520and%2520Zheng%2520Zhu%2520and%2520Angen%2520Ye%2520and%2520Yang%2520Wang%2520and%2520Zhiqin%2520Yang%2520and%2520Chaojun%2520Ni%2520and%2520Guan%2520Huang%2520and%2520Xingang%2520Wang%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520Embodied%2520AI%2520has%2520led%2520to%2520an%2520increasing%2520demand%2520for%250Alarge-scale%252C%2520high-quality%2520real-world%2520data.%2520However%252C%2520collecting%2520such%2520embodied%250Adata%2520remains%2520costly%2520and%2520inefficient.%2520As%2520a%2520result%252C%2520simulation%2520environments%2520have%250Abecome%2520a%2520crucial%2520surrogate%2520for%2520training%2520robot%2520policies.%2520Yet%252C%2520the%2520significant%250AReal2Sim2Real%2520gap%2520remains%2520a%2520critical%2520bottleneck%252C%2520particularly%2520in%2520terms%2520of%250Aphysical%2520dynamics%2520and%2520visual%2520appearance.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%250AEmbodieDreamer%252C%2520a%2520novel%2520framework%2520that%2520reduces%2520the%2520Real2Sim2Real%2520gap%2520from%2520both%250Athe%2520physics%2520and%2520appearance%2520perspectives.%2520Specifically%252C%2520we%2520propose%2520PhysAligner%252C%250Aa%2520differentiable%2520physics%2520module%2520designed%2520to%2520reduce%2520the%2520Real2Sim%2520physical%2520gap.%250AIt%2520jointly%2520optimizes%2520robot-specific%2520parameters%2520such%2520as%2520control%2520gains%2520and%250Afriction%2520coefficients%2520to%2520better%2520align%2520simulated%2520dynamics%2520with%2520real-world%250Aobservations.%2520In%2520addition%252C%2520we%2520introduce%2520VisAligner%252C%2520which%2520incorporates%2520a%250Aconditional%2520video%2520diffusion%2520model%2520to%2520bridge%2520the%2520Sim2Real%2520appearance%2520gap%2520by%250Atranslating%2520low-fidelity%2520simulated%2520renderings%2520into%2520photorealistic%2520videos%250Aconditioned%2520on%2520simulation%2520states%252C%2520enabling%2520high-fidelity%2520visual%2520transfer.%250AExtensive%2520experiments%2520validate%2520the%2520effectiveness%2520of%2520EmbodieDreamer.%2520The%250Aproposed%2520PhysAligner%2520reduces%2520physical%2520parameter%2520estimation%2520error%2520by%25203.74%2525%250Acompared%2520to%2520simulated%2520annealing%2520methods%2520while%2520improving%2520optimization%2520speed%2520by%250A89.91%255C%2525.%2520Moreover%252C%2520training%2520robot%2520policies%2520in%2520the%2520generated%2520photorealistic%250Aenvironment%2520leads%2520to%2520a%252029.17%2525%2520improvement%2520in%2520the%2520average%2520task%2520success%2520rate%250Aacross%2520real-world%2520tasks%2520after%2520reinforcement%2520learning.%2520Code%252C%2520model%2520and%2520data%2520will%250Abe%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmbodieDreamer%3A%20Advancing%20Real2Sim2Real%20Transfer%20for%20Policy%20Training%20via%0A%20%20Embodied%20World%20Modeling&entry.906535625=Boyuan%20Wang%20and%20Xinpan%20Meng%20and%20Xiaofeng%20Wang%20and%20Zheng%20Zhu%20and%20Angen%20Ye%20and%20Yang%20Wang%20and%20Zhiqin%20Yang%20and%20Chaojun%20Ni%20and%20Guan%20Huang%20and%20Xingang%20Wang&entry.1292438233=%20%20The%20rapid%20advancement%20of%20Embodied%20AI%20has%20led%20to%20an%20increasing%20demand%20for%0Alarge-scale%2C%20high-quality%20real-world%20data.%20However%2C%20collecting%20such%20embodied%0Adata%20remains%20costly%20and%20inefficient.%20As%20a%20result%2C%20simulation%20environments%20have%0Abecome%20a%20crucial%20surrogate%20for%20training%20robot%20policies.%20Yet%2C%20the%20significant%0AReal2Sim2Real%20gap%20remains%20a%20critical%20bottleneck%2C%20particularly%20in%20terms%20of%0Aphysical%20dynamics%20and%20visual%20appearance.%20To%20address%20this%20challenge%2C%20we%20propose%0AEmbodieDreamer%2C%20a%20novel%20framework%20that%20reduces%20the%20Real2Sim2Real%20gap%20from%20both%0Athe%20physics%20and%20appearance%20perspectives.%20Specifically%2C%20we%20propose%20PhysAligner%2C%0Aa%20differentiable%20physics%20module%20designed%20to%20reduce%20the%20Real2Sim%20physical%20gap.%0AIt%20jointly%20optimizes%20robot-specific%20parameters%20such%20as%20control%20gains%20and%0Afriction%20coefficients%20to%20better%20align%20simulated%20dynamics%20with%20real-world%0Aobservations.%20In%20addition%2C%20we%20introduce%20VisAligner%2C%20which%20incorporates%20a%0Aconditional%20video%20diffusion%20model%20to%20bridge%20the%20Sim2Real%20appearance%20gap%20by%0Atranslating%20low-fidelity%20simulated%20renderings%20into%20photorealistic%20videos%0Aconditioned%20on%20simulation%20states%2C%20enabling%20high-fidelity%20visual%20transfer.%0AExtensive%20experiments%20validate%20the%20effectiveness%20of%20EmbodieDreamer.%20The%0Aproposed%20PhysAligner%20reduces%20physical%20parameter%20estimation%20error%20by%203.74%25%0Acompared%20to%20simulated%20annealing%20methods%20while%20improving%20optimization%20speed%20by%0A89.91%5C%25.%20Moreover%2C%20training%20robot%20policies%20in%20the%20generated%20photorealistic%0Aenvironment%20leads%20to%20a%2029.17%25%20improvement%20in%20the%20average%20task%20success%20rate%0Aacross%20real-world%20tasks%20after%20reinforcement%20learning.%20Code%2C%20model%20and%20data%20will%0Abe%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05198v1&entry.124074799=Read"},
{"title": "EXPOTION: Facial Expression and Motion Control for Multimodal Music\n  Generation", "author": "Fathinah Izzati and Xinyue Li and Gus Xia", "abstract": "  We propose Expotion (Facial Expression and Motion Control for Multimodal\nMusic Generation), a generative model leveraging multimodal visual controls -\nspecifically, human facial expressions and upper-body motion - as well as text\nprompts to produce expressive and temporally accurate music. We adopt\nparameter-efficient fine-tuning (PEFT) on the pretrained text-to-music\ngeneration model, enabling fine-grained adaptation to the multimodal controls\nusing a small dataset. To ensure precise synchronization between video and\nmusic, we introduce a temporal smoothing strategy to align multiple modalities.\nExperiments demonstrate that integrating visual features alongside textual\ndescriptions enhances the overall quality of generated music in terms of\nmusicality, creativity, beat-tempo consistency, temporal alignment with the\nvideo, and text adherence, surpassing both proposed baselines and existing\nstate-of-the-art video-to-music generation models. Additionally, we introduce a\nnovel dataset consisting of 7 hours of synchronized video recordings capturing\nexpressive facial and upper-body gestures aligned with corresponding music,\nproviding significant potential for future research in multimodal and\ninteractive music generation.\n", "link": "http://arxiv.org/abs/2507.04955v1", "date": "2025-07-07", "relevancy": 2.3291, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5839}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5813}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EXPOTION%3A%20Facial%20Expression%20and%20Motion%20Control%20for%20Multimodal%20Music%0A%20%20Generation&body=Title%3A%20EXPOTION%3A%20Facial%20Expression%20and%20Motion%20Control%20for%20Multimodal%20Music%0A%20%20Generation%0AAuthor%3A%20Fathinah%20Izzati%20and%20Xinyue%20Li%20and%20Gus%20Xia%0AAbstract%3A%20%20%20We%20propose%20Expotion%20%28Facial%20Expression%20and%20Motion%20Control%20for%20Multimodal%0AMusic%20Generation%29%2C%20a%20generative%20model%20leveraging%20multimodal%20visual%20controls%20-%0Aspecifically%2C%20human%20facial%20expressions%20and%20upper-body%20motion%20-%20as%20well%20as%20text%0Aprompts%20to%20produce%20expressive%20and%20temporally%20accurate%20music.%20We%20adopt%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20on%20the%20pretrained%20text-to-music%0Ageneration%20model%2C%20enabling%20fine-grained%20adaptation%20to%20the%20multimodal%20controls%0Ausing%20a%20small%20dataset.%20To%20ensure%20precise%20synchronization%20between%20video%20and%0Amusic%2C%20we%20introduce%20a%20temporal%20smoothing%20strategy%20to%20align%20multiple%20modalities.%0AExperiments%20demonstrate%20that%20integrating%20visual%20features%20alongside%20textual%0Adescriptions%20enhances%20the%20overall%20quality%20of%20generated%20music%20in%20terms%20of%0Amusicality%2C%20creativity%2C%20beat-tempo%20consistency%2C%20temporal%20alignment%20with%20the%0Avideo%2C%20and%20text%20adherence%2C%20surpassing%20both%20proposed%20baselines%20and%20existing%0Astate-of-the-art%20video-to-music%20generation%20models.%20Additionally%2C%20we%20introduce%20a%0Anovel%20dataset%20consisting%20of%207%20hours%20of%20synchronized%20video%20recordings%20capturing%0Aexpressive%20facial%20and%20upper-body%20gestures%20aligned%20with%20corresponding%20music%2C%0Aproviding%20significant%20potential%20for%20future%20research%20in%20multimodal%20and%0Ainteractive%20music%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEXPOTION%253A%2520Facial%2520Expression%2520and%2520Motion%2520Control%2520for%2520Multimodal%2520Music%250A%2520%2520Generation%26entry.906535625%3DFathinah%2520Izzati%2520and%2520Xinyue%2520Li%2520and%2520Gus%2520Xia%26entry.1292438233%3D%2520%2520We%2520propose%2520Expotion%2520%2528Facial%2520Expression%2520and%2520Motion%2520Control%2520for%2520Multimodal%250AMusic%2520Generation%2529%252C%2520a%2520generative%2520model%2520leveraging%2520multimodal%2520visual%2520controls%2520-%250Aspecifically%252C%2520human%2520facial%2520expressions%2520and%2520upper-body%2520motion%2520-%2520as%2520well%2520as%2520text%250Aprompts%2520to%2520produce%2520expressive%2520and%2520temporally%2520accurate%2520music.%2520We%2520adopt%250Aparameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520on%2520the%2520pretrained%2520text-to-music%250Ageneration%2520model%252C%2520enabling%2520fine-grained%2520adaptation%2520to%2520the%2520multimodal%2520controls%250Ausing%2520a%2520small%2520dataset.%2520To%2520ensure%2520precise%2520synchronization%2520between%2520video%2520and%250Amusic%252C%2520we%2520introduce%2520a%2520temporal%2520smoothing%2520strategy%2520to%2520align%2520multiple%2520modalities.%250AExperiments%2520demonstrate%2520that%2520integrating%2520visual%2520features%2520alongside%2520textual%250Adescriptions%2520enhances%2520the%2520overall%2520quality%2520of%2520generated%2520music%2520in%2520terms%2520of%250Amusicality%252C%2520creativity%252C%2520beat-tempo%2520consistency%252C%2520temporal%2520alignment%2520with%2520the%250Avideo%252C%2520and%2520text%2520adherence%252C%2520surpassing%2520both%2520proposed%2520baselines%2520and%2520existing%250Astate-of-the-art%2520video-to-music%2520generation%2520models.%2520Additionally%252C%2520we%2520introduce%2520a%250Anovel%2520dataset%2520consisting%2520of%25207%2520hours%2520of%2520synchronized%2520video%2520recordings%2520capturing%250Aexpressive%2520facial%2520and%2520upper-body%2520gestures%2520aligned%2520with%2520corresponding%2520music%252C%250Aproviding%2520significant%2520potential%2520for%2520future%2520research%2520in%2520multimodal%2520and%250Ainteractive%2520music%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EXPOTION%3A%20Facial%20Expression%20and%20Motion%20Control%20for%20Multimodal%20Music%0A%20%20Generation&entry.906535625=Fathinah%20Izzati%20and%20Xinyue%20Li%20and%20Gus%20Xia&entry.1292438233=%20%20We%20propose%20Expotion%20%28Facial%20Expression%20and%20Motion%20Control%20for%20Multimodal%0AMusic%20Generation%29%2C%20a%20generative%20model%20leveraging%20multimodal%20visual%20controls%20-%0Aspecifically%2C%20human%20facial%20expressions%20and%20upper-body%20motion%20-%20as%20well%20as%20text%0Aprompts%20to%20produce%20expressive%20and%20temporally%20accurate%20music.%20We%20adopt%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20on%20the%20pretrained%20text-to-music%0Ageneration%20model%2C%20enabling%20fine-grained%20adaptation%20to%20the%20multimodal%20controls%0Ausing%20a%20small%20dataset.%20To%20ensure%20precise%20synchronization%20between%20video%20and%0Amusic%2C%20we%20introduce%20a%20temporal%20smoothing%20strategy%20to%20align%20multiple%20modalities.%0AExperiments%20demonstrate%20that%20integrating%20visual%20features%20alongside%20textual%0Adescriptions%20enhances%20the%20overall%20quality%20of%20generated%20music%20in%20terms%20of%0Amusicality%2C%20creativity%2C%20beat-tempo%20consistency%2C%20temporal%20alignment%20with%20the%0Avideo%2C%20and%20text%20adherence%2C%20surpassing%20both%20proposed%20baselines%20and%20existing%0Astate-of-the-art%20video-to-music%20generation%20models.%20Additionally%2C%20we%20introduce%20a%0Anovel%20dataset%20consisting%20of%207%20hours%20of%20synchronized%20video%20recordings%20capturing%0Aexpressive%20facial%20and%20upper-body%20gestures%20aligned%20with%20corresponding%20music%2C%0Aproviding%20significant%20potential%20for%20future%20research%20in%20multimodal%20and%0Ainteractive%20music%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04955v1&entry.124074799=Read"},
{"title": "Interpretable Mnemonic Generation for Kanji Learning via\n  Expectation-Maximization", "author": "Jaewook Lee and Alexander Scarlatos and Andrew Lan", "abstract": "  Learning Japanese vocabulary is a challenge for learners from Roman alphabet\nbackgrounds due to script differences. Japanese combines syllabaries like\nhiragana with kanji, which are logographic characters of Chinese origin. Kanji\nare also complicated due to their complexity and volume. Keyword mnemonics are\na common strategy to aid memorization, often using the compositional structure\nof kanji to form vivid associations. Despite recent efforts to use large\nlanguage models (LLMs) to assist learners, existing methods for LLM-based\nkeyword mnemonic generation function as a black box, offering limited\ninterpretability. We propose a generative framework that explicitly models the\nmnemonic construction process as driven by a set of common rules, and learn\nthem using a novel Expectation-Maximization-type algorithm. Trained on\nlearner-authored mnemonics from an online platform, our method learns latent\nstructures and compositional rules, enabling interpretable and systematic\nmnemonics generation. Experiments show that our method performs well in the\ncold-start setting for new learners while providing insight into the mechanisms\nbehind effective mnemonic creation.\n", "link": "http://arxiv.org/abs/2507.05137v1", "date": "2025-07-07", "relevancy": 2.3126, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4803}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4572}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Mnemonic%20Generation%20for%20Kanji%20Learning%20via%0A%20%20Expectation-Maximization&body=Title%3A%20Interpretable%20Mnemonic%20Generation%20for%20Kanji%20Learning%20via%0A%20%20Expectation-Maximization%0AAuthor%3A%20Jaewook%20Lee%20and%20Alexander%20Scarlatos%20and%20Andrew%20Lan%0AAbstract%3A%20%20%20Learning%20Japanese%20vocabulary%20is%20a%20challenge%20for%20learners%20from%20Roman%20alphabet%0Abackgrounds%20due%20to%20script%20differences.%20Japanese%20combines%20syllabaries%20like%0Ahiragana%20with%20kanji%2C%20which%20are%20logographic%20characters%20of%20Chinese%20origin.%20Kanji%0Aare%20also%20complicated%20due%20to%20their%20complexity%20and%20volume.%20Keyword%20mnemonics%20are%0Aa%20common%20strategy%20to%20aid%20memorization%2C%20often%20using%20the%20compositional%20structure%0Aof%20kanji%20to%20form%20vivid%20associations.%20Despite%20recent%20efforts%20to%20use%20large%0Alanguage%20models%20%28LLMs%29%20to%20assist%20learners%2C%20existing%20methods%20for%20LLM-based%0Akeyword%20mnemonic%20generation%20function%20as%20a%20black%20box%2C%20offering%20limited%0Ainterpretability.%20We%20propose%20a%20generative%20framework%20that%20explicitly%20models%20the%0Amnemonic%20construction%20process%20as%20driven%20by%20a%20set%20of%20common%20rules%2C%20and%20learn%0Athem%20using%20a%20novel%20Expectation-Maximization-type%20algorithm.%20Trained%20on%0Alearner-authored%20mnemonics%20from%20an%20online%20platform%2C%20our%20method%20learns%20latent%0Astructures%20and%20compositional%20rules%2C%20enabling%20interpretable%20and%20systematic%0Amnemonics%20generation.%20Experiments%20show%20that%20our%20method%20performs%20well%20in%20the%0Acold-start%20setting%20for%20new%20learners%20while%20providing%20insight%20into%20the%20mechanisms%0Abehind%20effective%20mnemonic%20creation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Mnemonic%2520Generation%2520for%2520Kanji%2520Learning%2520via%250A%2520%2520Expectation-Maximization%26entry.906535625%3DJaewook%2520Lee%2520and%2520Alexander%2520Scarlatos%2520and%2520Andrew%2520Lan%26entry.1292438233%3D%2520%2520Learning%2520Japanese%2520vocabulary%2520is%2520a%2520challenge%2520for%2520learners%2520from%2520Roman%2520alphabet%250Abackgrounds%2520due%2520to%2520script%2520differences.%2520Japanese%2520combines%2520syllabaries%2520like%250Ahiragana%2520with%2520kanji%252C%2520which%2520are%2520logographic%2520characters%2520of%2520Chinese%2520origin.%2520Kanji%250Aare%2520also%2520complicated%2520due%2520to%2520their%2520complexity%2520and%2520volume.%2520Keyword%2520mnemonics%2520are%250Aa%2520common%2520strategy%2520to%2520aid%2520memorization%252C%2520often%2520using%2520the%2520compositional%2520structure%250Aof%2520kanji%2520to%2520form%2520vivid%2520associations.%2520Despite%2520recent%2520efforts%2520to%2520use%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520to%2520assist%2520learners%252C%2520existing%2520methods%2520for%2520LLM-based%250Akeyword%2520mnemonic%2520generation%2520function%2520as%2520a%2520black%2520box%252C%2520offering%2520limited%250Ainterpretability.%2520We%2520propose%2520a%2520generative%2520framework%2520that%2520explicitly%2520models%2520the%250Amnemonic%2520construction%2520process%2520as%2520driven%2520by%2520a%2520set%2520of%2520common%2520rules%252C%2520and%2520learn%250Athem%2520using%2520a%2520novel%2520Expectation-Maximization-type%2520algorithm.%2520Trained%2520on%250Alearner-authored%2520mnemonics%2520from%2520an%2520online%2520platform%252C%2520our%2520method%2520learns%2520latent%250Astructures%2520and%2520compositional%2520rules%252C%2520enabling%2520interpretable%2520and%2520systematic%250Amnemonics%2520generation.%2520Experiments%2520show%2520that%2520our%2520method%2520performs%2520well%2520in%2520the%250Acold-start%2520setting%2520for%2520new%2520learners%2520while%2520providing%2520insight%2520into%2520the%2520mechanisms%250Abehind%2520effective%2520mnemonic%2520creation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Mnemonic%20Generation%20for%20Kanji%20Learning%20via%0A%20%20Expectation-Maximization&entry.906535625=Jaewook%20Lee%20and%20Alexander%20Scarlatos%20and%20Andrew%20Lan&entry.1292438233=%20%20Learning%20Japanese%20vocabulary%20is%20a%20challenge%20for%20learners%20from%20Roman%20alphabet%0Abackgrounds%20due%20to%20script%20differences.%20Japanese%20combines%20syllabaries%20like%0Ahiragana%20with%20kanji%2C%20which%20are%20logographic%20characters%20of%20Chinese%20origin.%20Kanji%0Aare%20also%20complicated%20due%20to%20their%20complexity%20and%20volume.%20Keyword%20mnemonics%20are%0Aa%20common%20strategy%20to%20aid%20memorization%2C%20often%20using%20the%20compositional%20structure%0Aof%20kanji%20to%20form%20vivid%20associations.%20Despite%20recent%20efforts%20to%20use%20large%0Alanguage%20models%20%28LLMs%29%20to%20assist%20learners%2C%20existing%20methods%20for%20LLM-based%0Akeyword%20mnemonic%20generation%20function%20as%20a%20black%20box%2C%20offering%20limited%0Ainterpretability.%20We%20propose%20a%20generative%20framework%20that%20explicitly%20models%20the%0Amnemonic%20construction%20process%20as%20driven%20by%20a%20set%20of%20common%20rules%2C%20and%20learn%0Athem%20using%20a%20novel%20Expectation-Maximization-type%20algorithm.%20Trained%20on%0Alearner-authored%20mnemonics%20from%20an%20online%20platform%2C%20our%20method%20learns%20latent%0Astructures%20and%20compositional%20rules%2C%20enabling%20interpretable%20and%20systematic%0Amnemonics%20generation.%20Experiments%20show%20that%20our%20method%20performs%20well%20in%20the%0Acold-start%20setting%20for%20new%20learners%20while%20providing%20insight%20into%20the%20mechanisms%0Abehind%20effective%20mnemonic%20creation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05137v1&entry.124074799=Read"},
{"title": "The Case for Instance-Optimized LLMs in OLAP Databases", "author": "Bardia Mohammadi and Laurent Bindschaedler", "abstract": "  Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications.\n", "link": "http://arxiv.org/abs/2507.04967v1", "date": "2025-07-07", "relevancy": 2.2617, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4615}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4615}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Case%20for%20Instance-Optimized%20LLMs%20in%20OLAP%20Databases&body=Title%3A%20The%20Case%20for%20Instance-Optimized%20LLMs%20in%20OLAP%20Databases%0AAuthor%3A%20Bardia%20Mohammadi%20and%20Laurent%20Bindschaedler%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20can%20enhance%20analytics%20systems%20with%20powerful%20data%0Asummarization%2C%20cleaning%2C%20and%20semantic%20transformation%20capabilities.%20However%2C%0Adeploying%20LLMs%20at%20scale%20--%20processing%20millions%20to%20billions%20of%20rows%20--%20remains%0Aprohibitively%20expensive%20in%20computation%20and%20memory.%20We%20present%20IOLM-DB%2C%20a%20novel%0Asystem%20that%20makes%20LLM-enhanced%20database%20queries%20practical%20through%0Aquery-specific%20model%20optimization.%20Instead%20of%20using%20general-purpose%20LLMs%2C%0AIOLM-DB%20generates%20lightweight%2C%20specialized%20models%20tailored%20to%20each%20query%27s%0Aspecific%20needs%20using%20representative%20data%20samples.%20IOLM-DB%20reduces%20model%0Afootprints%20by%20up%20to%2076%25%20and%20increases%20throughput%20by%20up%20to%203.31%24%5Ctimes%24%20while%0Amaintaining%20accuracy%20through%20aggressive%20compression%20techniques%2C%20including%0Aquantization%2C%20sparsification%2C%20and%20structural%20pruning.%20We%20further%20show%20how%20our%0Aapproach%20enables%20higher%20parallelism%20on%20existing%20hardware%20and%20seamlessly%0Asupports%20caching%20and%20batching%20strategies%20to%20reduce%20overheads.%20Our%20prototype%0Ademonstrates%20that%20leveraging%20LLM%20queries%20inside%20analytics%20systems%20is%20feasible%0Aat%20scale%2C%20opening%20new%20possibilities%20for%20future%20OLAP%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Case%2520for%2520Instance-Optimized%2520LLMs%2520in%2520OLAP%2520Databases%26entry.906535625%3DBardia%2520Mohammadi%2520and%2520Laurent%2520Bindschaedler%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520enhance%2520analytics%2520systems%2520with%2520powerful%2520data%250Asummarization%252C%2520cleaning%252C%2520and%2520semantic%2520transformation%2520capabilities.%2520However%252C%250Adeploying%2520LLMs%2520at%2520scale%2520--%2520processing%2520millions%2520to%2520billions%2520of%2520rows%2520--%2520remains%250Aprohibitively%2520expensive%2520in%2520computation%2520and%2520memory.%2520We%2520present%2520IOLM-DB%252C%2520a%2520novel%250Asystem%2520that%2520makes%2520LLM-enhanced%2520database%2520queries%2520practical%2520through%250Aquery-specific%2520model%2520optimization.%2520Instead%2520of%2520using%2520general-purpose%2520LLMs%252C%250AIOLM-DB%2520generates%2520lightweight%252C%2520specialized%2520models%2520tailored%2520to%2520each%2520query%2527s%250Aspecific%2520needs%2520using%2520representative%2520data%2520samples.%2520IOLM-DB%2520reduces%2520model%250Afootprints%2520by%2520up%2520to%252076%2525%2520and%2520increases%2520throughput%2520by%2520up%2520to%25203.31%2524%255Ctimes%2524%2520while%250Amaintaining%2520accuracy%2520through%2520aggressive%2520compression%2520techniques%252C%2520including%250Aquantization%252C%2520sparsification%252C%2520and%2520structural%2520pruning.%2520We%2520further%2520show%2520how%2520our%250Aapproach%2520enables%2520higher%2520parallelism%2520on%2520existing%2520hardware%2520and%2520seamlessly%250Asupports%2520caching%2520and%2520batching%2520strategies%2520to%2520reduce%2520overheads.%2520Our%2520prototype%250Ademonstrates%2520that%2520leveraging%2520LLM%2520queries%2520inside%2520analytics%2520systems%2520is%2520feasible%250Aat%2520scale%252C%2520opening%2520new%2520possibilities%2520for%2520future%2520OLAP%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Case%20for%20Instance-Optimized%20LLMs%20in%20OLAP%20Databases&entry.906535625=Bardia%20Mohammadi%20and%20Laurent%20Bindschaedler&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20can%20enhance%20analytics%20systems%20with%20powerful%20data%0Asummarization%2C%20cleaning%2C%20and%20semantic%20transformation%20capabilities.%20However%2C%0Adeploying%20LLMs%20at%20scale%20--%20processing%20millions%20to%20billions%20of%20rows%20--%20remains%0Aprohibitively%20expensive%20in%20computation%20and%20memory.%20We%20present%20IOLM-DB%2C%20a%20novel%0Asystem%20that%20makes%20LLM-enhanced%20database%20queries%20practical%20through%0Aquery-specific%20model%20optimization.%20Instead%20of%20using%20general-purpose%20LLMs%2C%0AIOLM-DB%20generates%20lightweight%2C%20specialized%20models%20tailored%20to%20each%20query%27s%0Aspecific%20needs%20using%20representative%20data%20samples.%20IOLM-DB%20reduces%20model%0Afootprints%20by%20up%20to%2076%25%20and%20increases%20throughput%20by%20up%20to%203.31%24%5Ctimes%24%20while%0Amaintaining%20accuracy%20through%20aggressive%20compression%20techniques%2C%20including%0Aquantization%2C%20sparsification%2C%20and%20structural%20pruning.%20We%20further%20show%20how%20our%0Aapproach%20enables%20higher%20parallelism%20on%20existing%20hardware%20and%20seamlessly%0Asupports%20caching%20and%20batching%20strategies%20to%20reduce%20overheads.%20Our%20prototype%0Ademonstrates%20that%20leveraging%20LLM%20queries%20inside%20analytics%20systems%20is%20feasible%0Aat%20scale%2C%20opening%20new%20possibilities%20for%20future%20OLAP%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04967v1&entry.124074799=Read"},
{"title": "Robust Incomplete-Modality Alignment for Ophthalmic Disease Grading and\n  Diagnosis via Labeled Optimal Transport", "author": "Qinkai Yu and Jianyang Xie and Yitian Zhao and Cheng Chen and Lijun Zhang and Liming Chen and Jun Cheng and Lu Liu and Yalin Zheng and Yanda Meng", "abstract": "  Multimodal ophthalmic imaging-based diagnosis integrates color fundus image\nwith optical coherence tomography (OCT) to provide a comprehensive view of\nocular pathologies. However, the uneven global distribution of healthcare\nresources often results in real-world clinical scenarios encountering\nincomplete multimodal data, which significantly compromises diagnostic\naccuracy. Existing commonly used pipelines, such as modality imputation and\ndistillation methods, face notable limitations: 1)Imputation methods struggle\nwith accurately reconstructing key lesion features, since OCT lesions are\nlocalized, while fundus images vary in style. 2)distillation methods rely\nheavily on fully paired multimodal training data. To address these challenges,\nwe propose a novel multimodal alignment and fusion framework capable of\nrobustly handling missing modalities in the task of ophthalmic diagnostics. By\nconsidering the distinctive feature characteristics of OCT and fundus images,\nwe emphasize the alignment of semantic features within the same category and\nexplicitly learn soft matching between modalities, allowing the missing\nmodality to utilize existing modality information, achieving robust cross-modal\nfeature alignment under the missing modality. Specifically, we leverage the\nOptimal Transport for multi-scale modality feature alignment: class-wise\nalignment through predicted class prototypes and feature-wise alignment via\ncross-modal shared feature transport. Furthermore, we propose an asymmetric\nfusion strategy that effectively exploits the distinct characteristics of OCT\nand fundus modalities. Extensive evaluations on three large ophthalmic\nmultimodal datasets demonstrate our model's superior performance under various\nmodality-incomplete scenarios, achieving Sota performance in both complete\nmodality and inter-modality incompleteness conditions. Code is available at\nhttps://github.com/Qinkaiyu/RIMA\n", "link": "http://arxiv.org/abs/2507.04999v1", "date": "2025-07-07", "relevancy": 2.251, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5874}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5473}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Incomplete-Modality%20Alignment%20for%20Ophthalmic%20Disease%20Grading%20and%0A%20%20Diagnosis%20via%20Labeled%20Optimal%20Transport&body=Title%3A%20Robust%20Incomplete-Modality%20Alignment%20for%20Ophthalmic%20Disease%20Grading%20and%0A%20%20Diagnosis%20via%20Labeled%20Optimal%20Transport%0AAuthor%3A%20Qinkai%20Yu%20and%20Jianyang%20Xie%20and%20Yitian%20Zhao%20and%20Cheng%20Chen%20and%20Lijun%20Zhang%20and%20Liming%20Chen%20and%20Jun%20Cheng%20and%20Lu%20Liu%20and%20Yalin%20Zheng%20and%20Yanda%20Meng%0AAbstract%3A%20%20%20Multimodal%20ophthalmic%20imaging-based%20diagnosis%20integrates%20color%20fundus%20image%0Awith%20optical%20coherence%20tomography%20%28OCT%29%20to%20provide%20a%20comprehensive%20view%20of%0Aocular%20pathologies.%20However%2C%20the%20uneven%20global%20distribution%20of%20healthcare%0Aresources%20often%20results%20in%20real-world%20clinical%20scenarios%20encountering%0Aincomplete%20multimodal%20data%2C%20which%20significantly%20compromises%20diagnostic%0Aaccuracy.%20Existing%20commonly%20used%20pipelines%2C%20such%20as%20modality%20imputation%20and%0Adistillation%20methods%2C%20face%20notable%20limitations%3A%201%29Imputation%20methods%20struggle%0Awith%20accurately%20reconstructing%20key%20lesion%20features%2C%20since%20OCT%20lesions%20are%0Alocalized%2C%20while%20fundus%20images%20vary%20in%20style.%202%29distillation%20methods%20rely%0Aheavily%20on%20fully%20paired%20multimodal%20training%20data.%20To%20address%20these%20challenges%2C%0Awe%20propose%20a%20novel%20multimodal%20alignment%20and%20fusion%20framework%20capable%20of%0Arobustly%20handling%20missing%20modalities%20in%20the%20task%20of%20ophthalmic%20diagnostics.%20By%0Aconsidering%20the%20distinctive%20feature%20characteristics%20of%20OCT%20and%20fundus%20images%2C%0Awe%20emphasize%20the%20alignment%20of%20semantic%20features%20within%20the%20same%20category%20and%0Aexplicitly%20learn%20soft%20matching%20between%20modalities%2C%20allowing%20the%20missing%0Amodality%20to%20utilize%20existing%20modality%20information%2C%20achieving%20robust%20cross-modal%0Afeature%20alignment%20under%20the%20missing%20modality.%20Specifically%2C%20we%20leverage%20the%0AOptimal%20Transport%20for%20multi-scale%20modality%20feature%20alignment%3A%20class-wise%0Aalignment%20through%20predicted%20class%20prototypes%20and%20feature-wise%20alignment%20via%0Across-modal%20shared%20feature%20transport.%20Furthermore%2C%20we%20propose%20an%20asymmetric%0Afusion%20strategy%20that%20effectively%20exploits%20the%20distinct%20characteristics%20of%20OCT%0Aand%20fundus%20modalities.%20Extensive%20evaluations%20on%20three%20large%20ophthalmic%0Amultimodal%20datasets%20demonstrate%20our%20model%27s%20superior%20performance%20under%20various%0Amodality-incomplete%20scenarios%2C%20achieving%20Sota%20performance%20in%20both%20complete%0Amodality%20and%20inter-modality%20incompleteness%20conditions.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Qinkaiyu/RIMA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Incomplete-Modality%2520Alignment%2520for%2520Ophthalmic%2520Disease%2520Grading%2520and%250A%2520%2520Diagnosis%2520via%2520Labeled%2520Optimal%2520Transport%26entry.906535625%3DQinkai%2520Yu%2520and%2520Jianyang%2520Xie%2520and%2520Yitian%2520Zhao%2520and%2520Cheng%2520Chen%2520and%2520Lijun%2520Zhang%2520and%2520Liming%2520Chen%2520and%2520Jun%2520Cheng%2520and%2520Lu%2520Liu%2520and%2520Yalin%2520Zheng%2520and%2520Yanda%2520Meng%26entry.1292438233%3D%2520%2520Multimodal%2520ophthalmic%2520imaging-based%2520diagnosis%2520integrates%2520color%2520fundus%2520image%250Awith%2520optical%2520coherence%2520tomography%2520%2528OCT%2529%2520to%2520provide%2520a%2520comprehensive%2520view%2520of%250Aocular%2520pathologies.%2520However%252C%2520the%2520uneven%2520global%2520distribution%2520of%2520healthcare%250Aresources%2520often%2520results%2520in%2520real-world%2520clinical%2520scenarios%2520encountering%250Aincomplete%2520multimodal%2520data%252C%2520which%2520significantly%2520compromises%2520diagnostic%250Aaccuracy.%2520Existing%2520commonly%2520used%2520pipelines%252C%2520such%2520as%2520modality%2520imputation%2520and%250Adistillation%2520methods%252C%2520face%2520notable%2520limitations%253A%25201%2529Imputation%2520methods%2520struggle%250Awith%2520accurately%2520reconstructing%2520key%2520lesion%2520features%252C%2520since%2520OCT%2520lesions%2520are%250Alocalized%252C%2520while%2520fundus%2520images%2520vary%2520in%2520style.%25202%2529distillation%2520methods%2520rely%250Aheavily%2520on%2520fully%2520paired%2520multimodal%2520training%2520data.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520propose%2520a%2520novel%2520multimodal%2520alignment%2520and%2520fusion%2520framework%2520capable%2520of%250Arobustly%2520handling%2520missing%2520modalities%2520in%2520the%2520task%2520of%2520ophthalmic%2520diagnostics.%2520By%250Aconsidering%2520the%2520distinctive%2520feature%2520characteristics%2520of%2520OCT%2520and%2520fundus%2520images%252C%250Awe%2520emphasize%2520the%2520alignment%2520of%2520semantic%2520features%2520within%2520the%2520same%2520category%2520and%250Aexplicitly%2520learn%2520soft%2520matching%2520between%2520modalities%252C%2520allowing%2520the%2520missing%250Amodality%2520to%2520utilize%2520existing%2520modality%2520information%252C%2520achieving%2520robust%2520cross-modal%250Afeature%2520alignment%2520under%2520the%2520missing%2520modality.%2520Specifically%252C%2520we%2520leverage%2520the%250AOptimal%2520Transport%2520for%2520multi-scale%2520modality%2520feature%2520alignment%253A%2520class-wise%250Aalignment%2520through%2520predicted%2520class%2520prototypes%2520and%2520feature-wise%2520alignment%2520via%250Across-modal%2520shared%2520feature%2520transport.%2520Furthermore%252C%2520we%2520propose%2520an%2520asymmetric%250Afusion%2520strategy%2520that%2520effectively%2520exploits%2520the%2520distinct%2520characteristics%2520of%2520OCT%250Aand%2520fundus%2520modalities.%2520Extensive%2520evaluations%2520on%2520three%2520large%2520ophthalmic%250Amultimodal%2520datasets%2520demonstrate%2520our%2520model%2527s%2520superior%2520performance%2520under%2520various%250Amodality-incomplete%2520scenarios%252C%2520achieving%2520Sota%2520performance%2520in%2520both%2520complete%250Amodality%2520and%2520inter-modality%2520incompleteness%2520conditions.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/Qinkaiyu/RIMA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Incomplete-Modality%20Alignment%20for%20Ophthalmic%20Disease%20Grading%20and%0A%20%20Diagnosis%20via%20Labeled%20Optimal%20Transport&entry.906535625=Qinkai%20Yu%20and%20Jianyang%20Xie%20and%20Yitian%20Zhao%20and%20Cheng%20Chen%20and%20Lijun%20Zhang%20and%20Liming%20Chen%20and%20Jun%20Cheng%20and%20Lu%20Liu%20and%20Yalin%20Zheng%20and%20Yanda%20Meng&entry.1292438233=%20%20Multimodal%20ophthalmic%20imaging-based%20diagnosis%20integrates%20color%20fundus%20image%0Awith%20optical%20coherence%20tomography%20%28OCT%29%20to%20provide%20a%20comprehensive%20view%20of%0Aocular%20pathologies.%20However%2C%20the%20uneven%20global%20distribution%20of%20healthcare%0Aresources%20often%20results%20in%20real-world%20clinical%20scenarios%20encountering%0Aincomplete%20multimodal%20data%2C%20which%20significantly%20compromises%20diagnostic%0Aaccuracy.%20Existing%20commonly%20used%20pipelines%2C%20such%20as%20modality%20imputation%20and%0Adistillation%20methods%2C%20face%20notable%20limitations%3A%201%29Imputation%20methods%20struggle%0Awith%20accurately%20reconstructing%20key%20lesion%20features%2C%20since%20OCT%20lesions%20are%0Alocalized%2C%20while%20fundus%20images%20vary%20in%20style.%202%29distillation%20methods%20rely%0Aheavily%20on%20fully%20paired%20multimodal%20training%20data.%20To%20address%20these%20challenges%2C%0Awe%20propose%20a%20novel%20multimodal%20alignment%20and%20fusion%20framework%20capable%20of%0Arobustly%20handling%20missing%20modalities%20in%20the%20task%20of%20ophthalmic%20diagnostics.%20By%0Aconsidering%20the%20distinctive%20feature%20characteristics%20of%20OCT%20and%20fundus%20images%2C%0Awe%20emphasize%20the%20alignment%20of%20semantic%20features%20within%20the%20same%20category%20and%0Aexplicitly%20learn%20soft%20matching%20between%20modalities%2C%20allowing%20the%20missing%0Amodality%20to%20utilize%20existing%20modality%20information%2C%20achieving%20robust%20cross-modal%0Afeature%20alignment%20under%20the%20missing%20modality.%20Specifically%2C%20we%20leverage%20the%0AOptimal%20Transport%20for%20multi-scale%20modality%20feature%20alignment%3A%20class-wise%0Aalignment%20through%20predicted%20class%20prototypes%20and%20feature-wise%20alignment%20via%0Across-modal%20shared%20feature%20transport.%20Furthermore%2C%20we%20propose%20an%20asymmetric%0Afusion%20strategy%20that%20effectively%20exploits%20the%20distinct%20characteristics%20of%20OCT%0Aand%20fundus%20modalities.%20Extensive%20evaluations%20on%20three%20large%20ophthalmic%0Amultimodal%20datasets%20demonstrate%20our%20model%27s%20superior%20performance%20under%20various%0Amodality-incomplete%20scenarios%2C%20achieving%20Sota%20performance%20in%20both%20complete%0Amodality%20and%20inter-modality%20incompleteness%20conditions.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Qinkaiyu/RIMA%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04999v1&entry.124074799=Read"},
{"title": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen\n  Visual Unicode Representations", "author": "A. Bochkov", "abstract": "  Understanding the locus of semantic representation in large language models\n(LLMs) is crucial for interpretability and architectural innovation. The\ndominant paradigm posits that trainable input embeddings serve as foundational\n\"meaning vectors.\" This paper challenges that view. We construct Transformer\nmodels where the embedding layer is entirely frozen, with vectors derived not\nfrom data, but from the visual structure of Unicode glyphs. These non-semantic,\nprecomputed visual embeddings are fixed throughout training. Our method is\ncompatible with any tokenizer, including a novel Unicode-centric tokenizer we\nintroduce to ensure universal text coverage. Despite the absence of trainable,\nsemantically initialized embeddings, our models converge, generate coherent\ntext, and, critically, outperform architecturally identical models with\ntrainable embeddings on the MMLU reasoning benchmark. We attribute this to\n\"representational interference\" in conventional models, where the embedding\nlayer is burdened with learning both structural and semantic features. Our\nresults indicate that high-level semantics are not inherent to input embeddings\nbut are an emergent property of the Transformer's compositional architecture\nand data scale. This reframes the role of embeddings from meaning containers to\nstructural primitives. We release all code and models to foster further\nresearch.\n", "link": "http://arxiv.org/abs/2507.04886v1", "date": "2025-07-07", "relevancy": 2.2429, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5614}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5614}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergent%20Semantics%20Beyond%20Token%20Embeddings%3A%20Transformer%20LMs%20with%20Frozen%0A%20%20Visual%20Unicode%20Representations&body=Title%3A%20Emergent%20Semantics%20Beyond%20Token%20Embeddings%3A%20Transformer%20LMs%20with%20Frozen%0A%20%20Visual%20Unicode%20Representations%0AAuthor%3A%20A.%20Bochkov%0AAbstract%3A%20%20%20Understanding%20the%20locus%20of%20semantic%20representation%20in%20large%20language%20models%0A%28LLMs%29%20is%20crucial%20for%20interpretability%20and%20architectural%20innovation.%20The%0Adominant%20paradigm%20posits%20that%20trainable%20input%20embeddings%20serve%20as%20foundational%0A%22meaning%20vectors.%22%20This%20paper%20challenges%20that%20view.%20We%20construct%20Transformer%0Amodels%20where%20the%20embedding%20layer%20is%20entirely%20frozen%2C%20with%20vectors%20derived%20not%0Afrom%20data%2C%20but%20from%20the%20visual%20structure%20of%20Unicode%20glyphs.%20These%20non-semantic%2C%0Aprecomputed%20visual%20embeddings%20are%20fixed%20throughout%20training.%20Our%20method%20is%0Acompatible%20with%20any%20tokenizer%2C%20including%20a%20novel%20Unicode-centric%20tokenizer%20we%0Aintroduce%20to%20ensure%20universal%20text%20coverage.%20Despite%20the%20absence%20of%20trainable%2C%0Asemantically%20initialized%20embeddings%2C%20our%20models%20converge%2C%20generate%20coherent%0Atext%2C%20and%2C%20critically%2C%20outperform%20architecturally%20identical%20models%20with%0Atrainable%20embeddings%20on%20the%20MMLU%20reasoning%20benchmark.%20We%20attribute%20this%20to%0A%22representational%20interference%22%20in%20conventional%20models%2C%20where%20the%20embedding%0Alayer%20is%20burdened%20with%20learning%20both%20structural%20and%20semantic%20features.%20Our%0Aresults%20indicate%20that%20high-level%20semantics%20are%20not%20inherent%20to%20input%20embeddings%0Abut%20are%20an%20emergent%20property%20of%20the%20Transformer%27s%20compositional%20architecture%0Aand%20data%20scale.%20This%20reframes%20the%20role%20of%20embeddings%20from%20meaning%20containers%20to%0Astructural%20primitives.%20We%20release%20all%20code%20and%20models%20to%20foster%20further%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04886v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergent%2520Semantics%2520Beyond%2520Token%2520Embeddings%253A%2520Transformer%2520LMs%2520with%2520Frozen%250A%2520%2520Visual%2520Unicode%2520Representations%26entry.906535625%3DA.%2520Bochkov%26entry.1292438233%3D%2520%2520Understanding%2520the%2520locus%2520of%2520semantic%2520representation%2520in%2520large%2520language%2520models%250A%2528LLMs%2529%2520is%2520crucial%2520for%2520interpretability%2520and%2520architectural%2520innovation.%2520The%250Adominant%2520paradigm%2520posits%2520that%2520trainable%2520input%2520embeddings%2520serve%2520as%2520foundational%250A%2522meaning%2520vectors.%2522%2520This%2520paper%2520challenges%2520that%2520view.%2520We%2520construct%2520Transformer%250Amodels%2520where%2520the%2520embedding%2520layer%2520is%2520entirely%2520frozen%252C%2520with%2520vectors%2520derived%2520not%250Afrom%2520data%252C%2520but%2520from%2520the%2520visual%2520structure%2520of%2520Unicode%2520glyphs.%2520These%2520non-semantic%252C%250Aprecomputed%2520visual%2520embeddings%2520are%2520fixed%2520throughout%2520training.%2520Our%2520method%2520is%250Acompatible%2520with%2520any%2520tokenizer%252C%2520including%2520a%2520novel%2520Unicode-centric%2520tokenizer%2520we%250Aintroduce%2520to%2520ensure%2520universal%2520text%2520coverage.%2520Despite%2520the%2520absence%2520of%2520trainable%252C%250Asemantically%2520initialized%2520embeddings%252C%2520our%2520models%2520converge%252C%2520generate%2520coherent%250Atext%252C%2520and%252C%2520critically%252C%2520outperform%2520architecturally%2520identical%2520models%2520with%250Atrainable%2520embeddings%2520on%2520the%2520MMLU%2520reasoning%2520benchmark.%2520We%2520attribute%2520this%2520to%250A%2522representational%2520interference%2522%2520in%2520conventional%2520models%252C%2520where%2520the%2520embedding%250Alayer%2520is%2520burdened%2520with%2520learning%2520both%2520structural%2520and%2520semantic%2520features.%2520Our%250Aresults%2520indicate%2520that%2520high-level%2520semantics%2520are%2520not%2520inherent%2520to%2520input%2520embeddings%250Abut%2520are%2520an%2520emergent%2520property%2520of%2520the%2520Transformer%2527s%2520compositional%2520architecture%250Aand%2520data%2520scale.%2520This%2520reframes%2520the%2520role%2520of%2520embeddings%2520from%2520meaning%2520containers%2520to%250Astructural%2520primitives.%2520We%2520release%2520all%2520code%2520and%2520models%2520to%2520foster%2520further%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04886v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergent%20Semantics%20Beyond%20Token%20Embeddings%3A%20Transformer%20LMs%20with%20Frozen%0A%20%20Visual%20Unicode%20Representations&entry.906535625=A.%20Bochkov&entry.1292438233=%20%20Understanding%20the%20locus%20of%20semantic%20representation%20in%20large%20language%20models%0A%28LLMs%29%20is%20crucial%20for%20interpretability%20and%20architectural%20innovation.%20The%0Adominant%20paradigm%20posits%20that%20trainable%20input%20embeddings%20serve%20as%20foundational%0A%22meaning%20vectors.%22%20This%20paper%20challenges%20that%20view.%20We%20construct%20Transformer%0Amodels%20where%20the%20embedding%20layer%20is%20entirely%20frozen%2C%20with%20vectors%20derived%20not%0Afrom%20data%2C%20but%20from%20the%20visual%20structure%20of%20Unicode%20glyphs.%20These%20non-semantic%2C%0Aprecomputed%20visual%20embeddings%20are%20fixed%20throughout%20training.%20Our%20method%20is%0Acompatible%20with%20any%20tokenizer%2C%20including%20a%20novel%20Unicode-centric%20tokenizer%20we%0Aintroduce%20to%20ensure%20universal%20text%20coverage.%20Despite%20the%20absence%20of%20trainable%2C%0Asemantically%20initialized%20embeddings%2C%20our%20models%20converge%2C%20generate%20coherent%0Atext%2C%20and%2C%20critically%2C%20outperform%20architecturally%20identical%20models%20with%0Atrainable%20embeddings%20on%20the%20MMLU%20reasoning%20benchmark.%20We%20attribute%20this%20to%0A%22representational%20interference%22%20in%20conventional%20models%2C%20where%20the%20embedding%0Alayer%20is%20burdened%20with%20learning%20both%20structural%20and%20semantic%20features.%20Our%0Aresults%20indicate%20that%20high-level%20semantics%20are%20not%20inherent%20to%20input%20embeddings%0Abut%20are%20an%20emergent%20property%20of%20the%20Transformer%27s%20compositional%20architecture%0Aand%20data%20scale.%20This%20reframes%20the%20role%20of%20embeddings%20from%20meaning%20containers%20to%0Astructural%20primitives.%20We%20release%20all%20code%20and%20models%20to%20foster%20further%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04886v1&entry.124074799=Read"},
{"title": "Differential Attention for Multimodal Crisis Event Analysis", "author": "Nusrat Munia and Junfeng Zhu and Olfa Nasraoui and Abdullah-Al-Zubaer Imran", "abstract": "  Social networks can be a valuable source of information during crisis events.\nIn particular, users can post a stream of multimodal data that can be critical\nfor real-time humanitarian response. However, effectively extracting meaningful\ninformation from this large and noisy data stream and effectively integrating\nheterogeneous data remains a formidable challenge. In this work, we explore\nvision language models (VLMs) and advanced fusion strategies to enhance the\nclassification of crisis data in three different tasks. We incorporate\nLLaVA-generated text to improve text-image alignment. Additionally, we leverage\nContrastive Language-Image Pretraining (CLIP)-based vision and text embeddings,\nwhich, without task-specific fine-tuning, outperform traditional models. To\nfurther refine multimodal fusion, we employ Guided Cross Attention (Guided CA)\nand combine it with the Differential Attention mechanism to enhance feature\nalignment by emphasizing critical information while filtering out irrelevant\ncontent. Our results show that while Differential Attention improves\nclassification performance, Guided CA remains highly effective in aligning\nmultimodal features. Extensive experiments on the CrisisMMD benchmark data set\ndemonstrate that the combination of pretrained VLMs, enriched textual\ndescriptions, and adaptive fusion strategies consistently outperforms\nstate-of-the-art models in classification accuracy, contributing to more\nreliable and interpretable models for three different tasks that are crucial\nfor disaster response. Our code is available at\nhttps://github.com/Munia03/Multimodal_Crisis_Event.\n", "link": "http://arxiv.org/abs/2507.05165v1", "date": "2025-07-07", "relevancy": 2.2414, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5766}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5737}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differential%20Attention%20for%20Multimodal%20Crisis%20Event%20Analysis&body=Title%3A%20Differential%20Attention%20for%20Multimodal%20Crisis%20Event%20Analysis%0AAuthor%3A%20Nusrat%20Munia%20and%20Junfeng%20Zhu%20and%20Olfa%20Nasraoui%20and%20Abdullah-Al-Zubaer%20Imran%0AAbstract%3A%20%20%20Social%20networks%20can%20be%20a%20valuable%20source%20of%20information%20during%20crisis%20events.%0AIn%20particular%2C%20users%20can%20post%20a%20stream%20of%20multimodal%20data%20that%20can%20be%20critical%0Afor%20real-time%20humanitarian%20response.%20However%2C%20effectively%20extracting%20meaningful%0Ainformation%20from%20this%20large%20and%20noisy%20data%20stream%20and%20effectively%20integrating%0Aheterogeneous%20data%20remains%20a%20formidable%20challenge.%20In%20this%20work%2C%20we%20explore%0Avision%20language%20models%20%28VLMs%29%20and%20advanced%20fusion%20strategies%20to%20enhance%20the%0Aclassification%20of%20crisis%20data%20in%20three%20different%20tasks.%20We%20incorporate%0ALLaVA-generated%20text%20to%20improve%20text-image%20alignment.%20Additionally%2C%20we%20leverage%0AContrastive%20Language-Image%20Pretraining%20%28CLIP%29-based%20vision%20and%20text%20embeddings%2C%0Awhich%2C%20without%20task-specific%20fine-tuning%2C%20outperform%20traditional%20models.%20To%0Afurther%20refine%20multimodal%20fusion%2C%20we%20employ%20Guided%20Cross%20Attention%20%28Guided%20CA%29%0Aand%20combine%20it%20with%20the%20Differential%20Attention%20mechanism%20to%20enhance%20feature%0Aalignment%20by%20emphasizing%20critical%20information%20while%20filtering%20out%20irrelevant%0Acontent.%20Our%20results%20show%20that%20while%20Differential%20Attention%20improves%0Aclassification%20performance%2C%20Guided%20CA%20remains%20highly%20effective%20in%20aligning%0Amultimodal%20features.%20Extensive%20experiments%20on%20the%20CrisisMMD%20benchmark%20data%20set%0Ademonstrate%20that%20the%20combination%20of%20pretrained%20VLMs%2C%20enriched%20textual%0Adescriptions%2C%20and%20adaptive%20fusion%20strategies%20consistently%20outperforms%0Astate-of-the-art%20models%20in%20classification%20accuracy%2C%20contributing%20to%20more%0Areliable%20and%20interpretable%20models%20for%20three%20different%20tasks%20that%20are%20crucial%0Afor%20disaster%20response.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Munia03/Multimodal_Crisis_Event.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferential%2520Attention%2520for%2520Multimodal%2520Crisis%2520Event%2520Analysis%26entry.906535625%3DNusrat%2520Munia%2520and%2520Junfeng%2520Zhu%2520and%2520Olfa%2520Nasraoui%2520and%2520Abdullah-Al-Zubaer%2520Imran%26entry.1292438233%3D%2520%2520Social%2520networks%2520can%2520be%2520a%2520valuable%2520source%2520of%2520information%2520during%2520crisis%2520events.%250AIn%2520particular%252C%2520users%2520can%2520post%2520a%2520stream%2520of%2520multimodal%2520data%2520that%2520can%2520be%2520critical%250Afor%2520real-time%2520humanitarian%2520response.%2520However%252C%2520effectively%2520extracting%2520meaningful%250Ainformation%2520from%2520this%2520large%2520and%2520noisy%2520data%2520stream%2520and%2520effectively%2520integrating%250Aheterogeneous%2520data%2520remains%2520a%2520formidable%2520challenge.%2520In%2520this%2520work%252C%2520we%2520explore%250Avision%2520language%2520models%2520%2528VLMs%2529%2520and%2520advanced%2520fusion%2520strategies%2520to%2520enhance%2520the%250Aclassification%2520of%2520crisis%2520data%2520in%2520three%2520different%2520tasks.%2520We%2520incorporate%250ALLaVA-generated%2520text%2520to%2520improve%2520text-image%2520alignment.%2520Additionally%252C%2520we%2520leverage%250AContrastive%2520Language-Image%2520Pretraining%2520%2528CLIP%2529-based%2520vision%2520and%2520text%2520embeddings%252C%250Awhich%252C%2520without%2520task-specific%2520fine-tuning%252C%2520outperform%2520traditional%2520models.%2520To%250Afurther%2520refine%2520multimodal%2520fusion%252C%2520we%2520employ%2520Guided%2520Cross%2520Attention%2520%2528Guided%2520CA%2529%250Aand%2520combine%2520it%2520with%2520the%2520Differential%2520Attention%2520mechanism%2520to%2520enhance%2520feature%250Aalignment%2520by%2520emphasizing%2520critical%2520information%2520while%2520filtering%2520out%2520irrelevant%250Acontent.%2520Our%2520results%2520show%2520that%2520while%2520Differential%2520Attention%2520improves%250Aclassification%2520performance%252C%2520Guided%2520CA%2520remains%2520highly%2520effective%2520in%2520aligning%250Amultimodal%2520features.%2520Extensive%2520experiments%2520on%2520the%2520CrisisMMD%2520benchmark%2520data%2520set%250Ademonstrate%2520that%2520the%2520combination%2520of%2520pretrained%2520VLMs%252C%2520enriched%2520textual%250Adescriptions%252C%2520and%2520adaptive%2520fusion%2520strategies%2520consistently%2520outperforms%250Astate-of-the-art%2520models%2520in%2520classification%2520accuracy%252C%2520contributing%2520to%2520more%250Areliable%2520and%2520interpretable%2520models%2520for%2520three%2520different%2520tasks%2520that%2520are%2520crucial%250Afor%2520disaster%2520response.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Munia03/Multimodal_Crisis_Event.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differential%20Attention%20for%20Multimodal%20Crisis%20Event%20Analysis&entry.906535625=Nusrat%20Munia%20and%20Junfeng%20Zhu%20and%20Olfa%20Nasraoui%20and%20Abdullah-Al-Zubaer%20Imran&entry.1292438233=%20%20Social%20networks%20can%20be%20a%20valuable%20source%20of%20information%20during%20crisis%20events.%0AIn%20particular%2C%20users%20can%20post%20a%20stream%20of%20multimodal%20data%20that%20can%20be%20critical%0Afor%20real-time%20humanitarian%20response.%20However%2C%20effectively%20extracting%20meaningful%0Ainformation%20from%20this%20large%20and%20noisy%20data%20stream%20and%20effectively%20integrating%0Aheterogeneous%20data%20remains%20a%20formidable%20challenge.%20In%20this%20work%2C%20we%20explore%0Avision%20language%20models%20%28VLMs%29%20and%20advanced%20fusion%20strategies%20to%20enhance%20the%0Aclassification%20of%20crisis%20data%20in%20three%20different%20tasks.%20We%20incorporate%0ALLaVA-generated%20text%20to%20improve%20text-image%20alignment.%20Additionally%2C%20we%20leverage%0AContrastive%20Language-Image%20Pretraining%20%28CLIP%29-based%20vision%20and%20text%20embeddings%2C%0Awhich%2C%20without%20task-specific%20fine-tuning%2C%20outperform%20traditional%20models.%20To%0Afurther%20refine%20multimodal%20fusion%2C%20we%20employ%20Guided%20Cross%20Attention%20%28Guided%20CA%29%0Aand%20combine%20it%20with%20the%20Differential%20Attention%20mechanism%20to%20enhance%20feature%0Aalignment%20by%20emphasizing%20critical%20information%20while%20filtering%20out%20irrelevant%0Acontent.%20Our%20results%20show%20that%20while%20Differential%20Attention%20improves%0Aclassification%20performance%2C%20Guided%20CA%20remains%20highly%20effective%20in%20aligning%0Amultimodal%20features.%20Extensive%20experiments%20on%20the%20CrisisMMD%20benchmark%20data%20set%0Ademonstrate%20that%20the%20combination%20of%20pretrained%20VLMs%2C%20enriched%20textual%0Adescriptions%2C%20and%20adaptive%20fusion%20strategies%20consistently%20outperforms%0Astate-of-the-art%20models%20in%20classification%20accuracy%2C%20contributing%20to%20more%0Areliable%20and%20interpretable%20models%20for%20three%20different%20tasks%20that%20are%20crucial%0Afor%20disaster%20response.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Munia03/Multimodal_Crisis_Event.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05165v1&entry.124074799=Read"},
{"title": "Beyond Simple Edits: X-Planner for Complex Instruction-Based Image\n  Editing", "author": "Chun-Hsiao Yeh and Yilin Wang and Nanxuan Zhao and Richard Zhang and Yuheng Li and Yi Ma and Krishna Kumar Singh", "abstract": "  Recent diffusion-based image editing methods have significantly advanced\ntext-guided tasks but often struggle to interpret complex, indirect\ninstructions. Moreover, current models frequently suffer from poor identity\npreservation, unintended edits, or rely heavily on manual masks. To address\nthese challenges, we introduce X-Planner, a Multimodal Large Language Model\n(MLLM)-based planning system that effectively bridges user intent with editing\nmodel capabilities. X-Planner employs chain-of-thought reasoning to\nsystematically decompose complex instructions into simpler, clear\nsub-instructions. For each sub-instruction, X-Planner automatically generates\nprecise edit types and segmentation masks, eliminating manual intervention and\nensuring localized, identity-preserving edits. Additionally, we propose a novel\nautomated pipeline for generating large-scale data to train X-Planner which\nachieves state-of-the-art results on both existing benchmarks and our newly\nintroduced complex editing benchmark.\n", "link": "http://arxiv.org/abs/2507.05259v1", "date": "2025-07-07", "relevancy": 2.236, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5665}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Simple%20Edits%3A%20X-Planner%20for%20Complex%20Instruction-Based%20Image%0A%20%20Editing&body=Title%3A%20Beyond%20Simple%20Edits%3A%20X-Planner%20for%20Complex%20Instruction-Based%20Image%0A%20%20Editing%0AAuthor%3A%20Chun-Hsiao%20Yeh%20and%20Yilin%20Wang%20and%20Nanxuan%20Zhao%20and%20Richard%20Zhang%20and%20Yuheng%20Li%20and%20Yi%20Ma%20and%20Krishna%20Kumar%20Singh%0AAbstract%3A%20%20%20Recent%20diffusion-based%20image%20editing%20methods%20have%20significantly%20advanced%0Atext-guided%20tasks%20but%20often%20struggle%20to%20interpret%20complex%2C%20indirect%0Ainstructions.%20Moreover%2C%20current%20models%20frequently%20suffer%20from%20poor%20identity%0Apreservation%2C%20unintended%20edits%2C%20or%20rely%20heavily%20on%20manual%20masks.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20X-Planner%2C%20a%20Multimodal%20Large%20Language%20Model%0A%28MLLM%29-based%20planning%20system%20that%20effectively%20bridges%20user%20intent%20with%20editing%0Amodel%20capabilities.%20X-Planner%20employs%20chain-of-thought%20reasoning%20to%0Asystematically%20decompose%20complex%20instructions%20into%20simpler%2C%20clear%0Asub-instructions.%20For%20each%20sub-instruction%2C%20X-Planner%20automatically%20generates%0Aprecise%20edit%20types%20and%20segmentation%20masks%2C%20eliminating%20manual%20intervention%20and%0Aensuring%20localized%2C%20identity-preserving%20edits.%20Additionally%2C%20we%20propose%20a%20novel%0Aautomated%20pipeline%20for%20generating%20large-scale%20data%20to%20train%20X-Planner%20which%0Aachieves%20state-of-the-art%20results%20on%20both%20existing%20benchmarks%20and%20our%20newly%0Aintroduced%20complex%20editing%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Simple%2520Edits%253A%2520X-Planner%2520for%2520Complex%2520Instruction-Based%2520Image%250A%2520%2520Editing%26entry.906535625%3DChun-Hsiao%2520Yeh%2520and%2520Yilin%2520Wang%2520and%2520Nanxuan%2520Zhao%2520and%2520Richard%2520Zhang%2520and%2520Yuheng%2520Li%2520and%2520Yi%2520Ma%2520and%2520Krishna%2520Kumar%2520Singh%26entry.1292438233%3D%2520%2520Recent%2520diffusion-based%2520image%2520editing%2520methods%2520have%2520significantly%2520advanced%250Atext-guided%2520tasks%2520but%2520often%2520struggle%2520to%2520interpret%2520complex%252C%2520indirect%250Ainstructions.%2520Moreover%252C%2520current%2520models%2520frequently%2520suffer%2520from%2520poor%2520identity%250Apreservation%252C%2520unintended%2520edits%252C%2520or%2520rely%2520heavily%2520on%2520manual%2520masks.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520introduce%2520X-Planner%252C%2520a%2520Multimodal%2520Large%2520Language%2520Model%250A%2528MLLM%2529-based%2520planning%2520system%2520that%2520effectively%2520bridges%2520user%2520intent%2520with%2520editing%250Amodel%2520capabilities.%2520X-Planner%2520employs%2520chain-of-thought%2520reasoning%2520to%250Asystematically%2520decompose%2520complex%2520instructions%2520into%2520simpler%252C%2520clear%250Asub-instructions.%2520For%2520each%2520sub-instruction%252C%2520X-Planner%2520automatically%2520generates%250Aprecise%2520edit%2520types%2520and%2520segmentation%2520masks%252C%2520eliminating%2520manual%2520intervention%2520and%250Aensuring%2520localized%252C%2520identity-preserving%2520edits.%2520Additionally%252C%2520we%2520propose%2520a%2520novel%250Aautomated%2520pipeline%2520for%2520generating%2520large-scale%2520data%2520to%2520train%2520X-Planner%2520which%250Aachieves%2520state-of-the-art%2520results%2520on%2520both%2520existing%2520benchmarks%2520and%2520our%2520newly%250Aintroduced%2520complex%2520editing%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Simple%20Edits%3A%20X-Planner%20for%20Complex%20Instruction-Based%20Image%0A%20%20Editing&entry.906535625=Chun-Hsiao%20Yeh%20and%20Yilin%20Wang%20and%20Nanxuan%20Zhao%20and%20Richard%20Zhang%20and%20Yuheng%20Li%20and%20Yi%20Ma%20and%20Krishna%20Kumar%20Singh&entry.1292438233=%20%20Recent%20diffusion-based%20image%20editing%20methods%20have%20significantly%20advanced%0Atext-guided%20tasks%20but%20often%20struggle%20to%20interpret%20complex%2C%20indirect%0Ainstructions.%20Moreover%2C%20current%20models%20frequently%20suffer%20from%20poor%20identity%0Apreservation%2C%20unintended%20edits%2C%20or%20rely%20heavily%20on%20manual%20masks.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20X-Planner%2C%20a%20Multimodal%20Large%20Language%20Model%0A%28MLLM%29-based%20planning%20system%20that%20effectively%20bridges%20user%20intent%20with%20editing%0Amodel%20capabilities.%20X-Planner%20employs%20chain-of-thought%20reasoning%20to%0Asystematically%20decompose%20complex%20instructions%20into%20simpler%2C%20clear%0Asub-instructions.%20For%20each%20sub-instruction%2C%20X-Planner%20automatically%20generates%0Aprecise%20edit%20types%20and%20segmentation%20masks%2C%20eliminating%20manual%20intervention%20and%0Aensuring%20localized%2C%20identity-preserving%20edits.%20Additionally%2C%20we%20propose%20a%20novel%0Aautomated%20pipeline%20for%20generating%20large-scale%20data%20to%20train%20X-Planner%20which%0Aachieves%20state-of-the-art%20results%20on%20both%20existing%20benchmarks%20and%20our%20newly%0Aintroduced%20complex%20editing%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05259v1&entry.124074799=Read"},
{"title": "Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent\n  Collaboration", "author": "Benjamin Li and Shuyang Shi and Lucia Romero and Huao Li and Yaqi Xie and Woojun Kim and Stefanos Nikolaidis and Michael Lewis and Katia Sycara and Simon Stepputtis", "abstract": "  In collaborative tasks, being able to adapt to your teammates is a necessary\nrequirement for success. When teammates are heterogeneous, such as in\nhuman-agent teams, agents need to be able to observe, recognize, and adapt to\ntheir human partners in real time. This becomes particularly challenging in\ntasks with time pressure and complex strategic spaces where the dynamics can\nchange rapidly. In this work, we introduce TALENTS, a strategy-conditioned\ncooperator framework that learns to represent, categorize, and adapt to a range\nof partner strategies, enabling ad-hoc teamwork. Our approach utilizes a\nvariational autoencoder to learn a latent strategy space from trajectory data.\nThis latent space represents the underlying strategies that agents employ.\nSubsequently, the system identifies different types of strategy by clustering\nthe data. Finally, a cooperator agent is trained to generate partners for each\ntype of strategy, conditioned on these clusters. In order to adapt to\npreviously unseen partners, we leverage a fixed-share regret minimization\nalgorithm that infers and adjusts the estimated partner strategy dynamically.\nWe assess our approach in a customized version of the Overcooked environment,\nposing a challenging cooperative cooking task that demands strong coordination\nacross a wide range of possible strategies. Using an online user study, we show\nthat our agent outperforms current baselines when working with unfamiliar human\npartners.\n", "link": "http://arxiv.org/abs/2507.05244v1", "date": "2025-07-07", "relevancy": 2.2326, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5612}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5582}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20Latent%20Partner%20Strategies%20for%20Adaptive%20Zero-Shot%20Human-Agent%0A%20%20Collaboration&body=Title%3A%20Modeling%20Latent%20Partner%20Strategies%20for%20Adaptive%20Zero-Shot%20Human-Agent%0A%20%20Collaboration%0AAuthor%3A%20Benjamin%20Li%20and%20Shuyang%20Shi%20and%20Lucia%20Romero%20and%20Huao%20Li%20and%20Yaqi%20Xie%20and%20Woojun%20Kim%20and%20Stefanos%20Nikolaidis%20and%20Michael%20Lewis%20and%20Katia%20Sycara%20and%20Simon%20Stepputtis%0AAbstract%3A%20%20%20In%20collaborative%20tasks%2C%20being%20able%20to%20adapt%20to%20your%20teammates%20is%20a%20necessary%0Arequirement%20for%20success.%20When%20teammates%20are%20heterogeneous%2C%20such%20as%20in%0Ahuman-agent%20teams%2C%20agents%20need%20to%20be%20able%20to%20observe%2C%20recognize%2C%20and%20adapt%20to%0Atheir%20human%20partners%20in%20real%20time.%20This%20becomes%20particularly%20challenging%20in%0Atasks%20with%20time%20pressure%20and%20complex%20strategic%20spaces%20where%20the%20dynamics%20can%0Achange%20rapidly.%20In%20this%20work%2C%20we%20introduce%20TALENTS%2C%20a%20strategy-conditioned%0Acooperator%20framework%20that%20learns%20to%20represent%2C%20categorize%2C%20and%20adapt%20to%20a%20range%0Aof%20partner%20strategies%2C%20enabling%20ad-hoc%20teamwork.%20Our%20approach%20utilizes%20a%0Avariational%20autoencoder%20to%20learn%20a%20latent%20strategy%20space%20from%20trajectory%20data.%0AThis%20latent%20space%20represents%20the%20underlying%20strategies%20that%20agents%20employ.%0ASubsequently%2C%20the%20system%20identifies%20different%20types%20of%20strategy%20by%20clustering%0Athe%20data.%20Finally%2C%20a%20cooperator%20agent%20is%20trained%20to%20generate%20partners%20for%20each%0Atype%20of%20strategy%2C%20conditioned%20on%20these%20clusters.%20In%20order%20to%20adapt%20to%0Apreviously%20unseen%20partners%2C%20we%20leverage%20a%20fixed-share%20regret%20minimization%0Aalgorithm%20that%20infers%20and%20adjusts%20the%20estimated%20partner%20strategy%20dynamically.%0AWe%20assess%20our%20approach%20in%20a%20customized%20version%20of%20the%20Overcooked%20environment%2C%0Aposing%20a%20challenging%20cooperative%20cooking%20task%20that%20demands%20strong%20coordination%0Aacross%20a%20wide%20range%20of%20possible%20strategies.%20Using%20an%20online%20user%20study%2C%20we%20show%0Athat%20our%20agent%20outperforms%20current%20baselines%20when%20working%20with%20unfamiliar%20human%0Apartners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520Latent%2520Partner%2520Strategies%2520for%2520Adaptive%2520Zero-Shot%2520Human-Agent%250A%2520%2520Collaboration%26entry.906535625%3DBenjamin%2520Li%2520and%2520Shuyang%2520Shi%2520and%2520Lucia%2520Romero%2520and%2520Huao%2520Li%2520and%2520Yaqi%2520Xie%2520and%2520Woojun%2520Kim%2520and%2520Stefanos%2520Nikolaidis%2520and%2520Michael%2520Lewis%2520and%2520Katia%2520Sycara%2520and%2520Simon%2520Stepputtis%26entry.1292438233%3D%2520%2520In%2520collaborative%2520tasks%252C%2520being%2520able%2520to%2520adapt%2520to%2520your%2520teammates%2520is%2520a%2520necessary%250Arequirement%2520for%2520success.%2520When%2520teammates%2520are%2520heterogeneous%252C%2520such%2520as%2520in%250Ahuman-agent%2520teams%252C%2520agents%2520need%2520to%2520be%2520able%2520to%2520observe%252C%2520recognize%252C%2520and%2520adapt%2520to%250Atheir%2520human%2520partners%2520in%2520real%2520time.%2520This%2520becomes%2520particularly%2520challenging%2520in%250Atasks%2520with%2520time%2520pressure%2520and%2520complex%2520strategic%2520spaces%2520where%2520the%2520dynamics%2520can%250Achange%2520rapidly.%2520In%2520this%2520work%252C%2520we%2520introduce%2520TALENTS%252C%2520a%2520strategy-conditioned%250Acooperator%2520framework%2520that%2520learns%2520to%2520represent%252C%2520categorize%252C%2520and%2520adapt%2520to%2520a%2520range%250Aof%2520partner%2520strategies%252C%2520enabling%2520ad-hoc%2520teamwork.%2520Our%2520approach%2520utilizes%2520a%250Avariational%2520autoencoder%2520to%2520learn%2520a%2520latent%2520strategy%2520space%2520from%2520trajectory%2520data.%250AThis%2520latent%2520space%2520represents%2520the%2520underlying%2520strategies%2520that%2520agents%2520employ.%250ASubsequently%252C%2520the%2520system%2520identifies%2520different%2520types%2520of%2520strategy%2520by%2520clustering%250Athe%2520data.%2520Finally%252C%2520a%2520cooperator%2520agent%2520is%2520trained%2520to%2520generate%2520partners%2520for%2520each%250Atype%2520of%2520strategy%252C%2520conditioned%2520on%2520these%2520clusters.%2520In%2520order%2520to%2520adapt%2520to%250Apreviously%2520unseen%2520partners%252C%2520we%2520leverage%2520a%2520fixed-share%2520regret%2520minimization%250Aalgorithm%2520that%2520infers%2520and%2520adjusts%2520the%2520estimated%2520partner%2520strategy%2520dynamically.%250AWe%2520assess%2520our%2520approach%2520in%2520a%2520customized%2520version%2520of%2520the%2520Overcooked%2520environment%252C%250Aposing%2520a%2520challenging%2520cooperative%2520cooking%2520task%2520that%2520demands%2520strong%2520coordination%250Aacross%2520a%2520wide%2520range%2520of%2520possible%2520strategies.%2520Using%2520an%2520online%2520user%2520study%252C%2520we%2520show%250Athat%2520our%2520agent%2520outperforms%2520current%2520baselines%2520when%2520working%2520with%2520unfamiliar%2520human%250Apartners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Latent%20Partner%20Strategies%20for%20Adaptive%20Zero-Shot%20Human-Agent%0A%20%20Collaboration&entry.906535625=Benjamin%20Li%20and%20Shuyang%20Shi%20and%20Lucia%20Romero%20and%20Huao%20Li%20and%20Yaqi%20Xie%20and%20Woojun%20Kim%20and%20Stefanos%20Nikolaidis%20and%20Michael%20Lewis%20and%20Katia%20Sycara%20and%20Simon%20Stepputtis&entry.1292438233=%20%20In%20collaborative%20tasks%2C%20being%20able%20to%20adapt%20to%20your%20teammates%20is%20a%20necessary%0Arequirement%20for%20success.%20When%20teammates%20are%20heterogeneous%2C%20such%20as%20in%0Ahuman-agent%20teams%2C%20agents%20need%20to%20be%20able%20to%20observe%2C%20recognize%2C%20and%20adapt%20to%0Atheir%20human%20partners%20in%20real%20time.%20This%20becomes%20particularly%20challenging%20in%0Atasks%20with%20time%20pressure%20and%20complex%20strategic%20spaces%20where%20the%20dynamics%20can%0Achange%20rapidly.%20In%20this%20work%2C%20we%20introduce%20TALENTS%2C%20a%20strategy-conditioned%0Acooperator%20framework%20that%20learns%20to%20represent%2C%20categorize%2C%20and%20adapt%20to%20a%20range%0Aof%20partner%20strategies%2C%20enabling%20ad-hoc%20teamwork.%20Our%20approach%20utilizes%20a%0Avariational%20autoencoder%20to%20learn%20a%20latent%20strategy%20space%20from%20trajectory%20data.%0AThis%20latent%20space%20represents%20the%20underlying%20strategies%20that%20agents%20employ.%0ASubsequently%2C%20the%20system%20identifies%20different%20types%20of%20strategy%20by%20clustering%0Athe%20data.%20Finally%2C%20a%20cooperator%20agent%20is%20trained%20to%20generate%20partners%20for%20each%0Atype%20of%20strategy%2C%20conditioned%20on%20these%20clusters.%20In%20order%20to%20adapt%20to%0Apreviously%20unseen%20partners%2C%20we%20leverage%20a%20fixed-share%20regret%20minimization%0Aalgorithm%20that%20infers%20and%20adjusts%20the%20estimated%20partner%20strategy%20dynamically.%0AWe%20assess%20our%20approach%20in%20a%20customized%20version%20of%20the%20Overcooked%20environment%2C%0Aposing%20a%20challenging%20cooperative%20cooking%20task%20that%20demands%20strong%20coordination%0Aacross%20a%20wide%20range%20of%20possible%20strategies.%20Using%20an%20online%20user%20study%2C%20we%20show%0Athat%20our%20agent%20outperforms%20current%20baselines%20when%20working%20with%20unfamiliar%20human%0Apartners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05244v1&entry.124074799=Read"},
{"title": "Distribution-dependent Generalization Bounds for Tuning Linear\n  Regression Across Tasks", "author": "Maria-Florina Balcan and Saumya Goyal and Dravyansh Sharma", "abstract": "  Modern regression problems often involve high-dimensional data and a careful\ntuning of the regularization hyperparameters is crucial to avoid overly complex\nmodels that may overfit the training data while guaranteeing desirable\nproperties like effective variable selection. We study the recently introduced\ndirection of tuning regularization hyperparameters in linear regression across\nmultiple related tasks. We obtain distribution-dependent bounds on the\ngeneralization error for the validation loss when tuning the L1 and L2\ncoefficients, including ridge, lasso and the elastic net. In contrast, prior\nwork develops bounds that apply uniformly to all distributions, but such bounds\nnecessarily degrade with feature dimension, d. While these bounds are shown to\nbe tight for worst-case distributions, our bounds improve with the \"niceness\"\nof the data distribution. Concretely, we show that under additional assumptions\nthat instances within each task are i.i.d. draws from broad well-studied\nclasses of distributions including sub-Gaussians, our generalization bounds do\nnot get worse with increasing d, and are much sharper than prior work for very\nlarge d. We also extend our results to a generalization of ridge regression,\nwhere we achieve tighter bounds that take into account an estimate of the mean\nof the ground truth distribution.\n", "link": "http://arxiv.org/abs/2507.05084v1", "date": "2025-07-07", "relevancy": 2.2304, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4637}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4495}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distribution-dependent%20Generalization%20Bounds%20for%20Tuning%20Linear%0A%20%20Regression%20Across%20Tasks&body=Title%3A%20Distribution-dependent%20Generalization%20Bounds%20for%20Tuning%20Linear%0A%20%20Regression%20Across%20Tasks%0AAuthor%3A%20Maria-Florina%20Balcan%20and%20Saumya%20Goyal%20and%20Dravyansh%20Sharma%0AAbstract%3A%20%20%20Modern%20regression%20problems%20often%20involve%20high-dimensional%20data%20and%20a%20careful%0Atuning%20of%20the%20regularization%20hyperparameters%20is%20crucial%20to%20avoid%20overly%20complex%0Amodels%20that%20may%20overfit%20the%20training%20data%20while%20guaranteeing%20desirable%0Aproperties%20like%20effective%20variable%20selection.%20We%20study%20the%20recently%20introduced%0Adirection%20of%20tuning%20regularization%20hyperparameters%20in%20linear%20regression%20across%0Amultiple%20related%20tasks.%20We%20obtain%20distribution-dependent%20bounds%20on%20the%0Ageneralization%20error%20for%20the%20validation%20loss%20when%20tuning%20the%20L1%20and%20L2%0Acoefficients%2C%20including%20ridge%2C%20lasso%20and%20the%20elastic%20net.%20In%20contrast%2C%20prior%0Awork%20develops%20bounds%20that%20apply%20uniformly%20to%20all%20distributions%2C%20but%20such%20bounds%0Anecessarily%20degrade%20with%20feature%20dimension%2C%20d.%20While%20these%20bounds%20are%20shown%20to%0Abe%20tight%20for%20worst-case%20distributions%2C%20our%20bounds%20improve%20with%20the%20%22niceness%22%0Aof%20the%20data%20distribution.%20Concretely%2C%20we%20show%20that%20under%20additional%20assumptions%0Athat%20instances%20within%20each%20task%20are%20i.i.d.%20draws%20from%20broad%20well-studied%0Aclasses%20of%20distributions%20including%20sub-Gaussians%2C%20our%20generalization%20bounds%20do%0Anot%20get%20worse%20with%20increasing%20d%2C%20and%20are%20much%20sharper%20than%20prior%20work%20for%20very%0Alarge%20d.%20We%20also%20extend%20our%20results%20to%20a%20generalization%20of%20ridge%20regression%2C%0Awhere%20we%20achieve%20tighter%20bounds%20that%20take%20into%20account%20an%20estimate%20of%20the%20mean%0Aof%20the%20ground%20truth%20distribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistribution-dependent%2520Generalization%2520Bounds%2520for%2520Tuning%2520Linear%250A%2520%2520Regression%2520Across%2520Tasks%26entry.906535625%3DMaria-Florina%2520Balcan%2520and%2520Saumya%2520Goyal%2520and%2520Dravyansh%2520Sharma%26entry.1292438233%3D%2520%2520Modern%2520regression%2520problems%2520often%2520involve%2520high-dimensional%2520data%2520and%2520a%2520careful%250Atuning%2520of%2520the%2520regularization%2520hyperparameters%2520is%2520crucial%2520to%2520avoid%2520overly%2520complex%250Amodels%2520that%2520may%2520overfit%2520the%2520training%2520data%2520while%2520guaranteeing%2520desirable%250Aproperties%2520like%2520effective%2520variable%2520selection.%2520We%2520study%2520the%2520recently%2520introduced%250Adirection%2520of%2520tuning%2520regularization%2520hyperparameters%2520in%2520linear%2520regression%2520across%250Amultiple%2520related%2520tasks.%2520We%2520obtain%2520distribution-dependent%2520bounds%2520on%2520the%250Ageneralization%2520error%2520for%2520the%2520validation%2520loss%2520when%2520tuning%2520the%2520L1%2520and%2520L2%250Acoefficients%252C%2520including%2520ridge%252C%2520lasso%2520and%2520the%2520elastic%2520net.%2520In%2520contrast%252C%2520prior%250Awork%2520develops%2520bounds%2520that%2520apply%2520uniformly%2520to%2520all%2520distributions%252C%2520but%2520such%2520bounds%250Anecessarily%2520degrade%2520with%2520feature%2520dimension%252C%2520d.%2520While%2520these%2520bounds%2520are%2520shown%2520to%250Abe%2520tight%2520for%2520worst-case%2520distributions%252C%2520our%2520bounds%2520improve%2520with%2520the%2520%2522niceness%2522%250Aof%2520the%2520data%2520distribution.%2520Concretely%252C%2520we%2520show%2520that%2520under%2520additional%2520assumptions%250Athat%2520instances%2520within%2520each%2520task%2520are%2520i.i.d.%2520draws%2520from%2520broad%2520well-studied%250Aclasses%2520of%2520distributions%2520including%2520sub-Gaussians%252C%2520our%2520generalization%2520bounds%2520do%250Anot%2520get%2520worse%2520with%2520increasing%2520d%252C%2520and%2520are%2520much%2520sharper%2520than%2520prior%2520work%2520for%2520very%250Alarge%2520d.%2520We%2520also%2520extend%2520our%2520results%2520to%2520a%2520generalization%2520of%2520ridge%2520regression%252C%250Awhere%2520we%2520achieve%2520tighter%2520bounds%2520that%2520take%2520into%2520account%2520an%2520estimate%2520of%2520the%2520mean%250Aof%2520the%2520ground%2520truth%2520distribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distribution-dependent%20Generalization%20Bounds%20for%20Tuning%20Linear%0A%20%20Regression%20Across%20Tasks&entry.906535625=Maria-Florina%20Balcan%20and%20Saumya%20Goyal%20and%20Dravyansh%20Sharma&entry.1292438233=%20%20Modern%20regression%20problems%20often%20involve%20high-dimensional%20data%20and%20a%20careful%0Atuning%20of%20the%20regularization%20hyperparameters%20is%20crucial%20to%20avoid%20overly%20complex%0Amodels%20that%20may%20overfit%20the%20training%20data%20while%20guaranteeing%20desirable%0Aproperties%20like%20effective%20variable%20selection.%20We%20study%20the%20recently%20introduced%0Adirection%20of%20tuning%20regularization%20hyperparameters%20in%20linear%20regression%20across%0Amultiple%20related%20tasks.%20We%20obtain%20distribution-dependent%20bounds%20on%20the%0Ageneralization%20error%20for%20the%20validation%20loss%20when%20tuning%20the%20L1%20and%20L2%0Acoefficients%2C%20including%20ridge%2C%20lasso%20and%20the%20elastic%20net.%20In%20contrast%2C%20prior%0Awork%20develops%20bounds%20that%20apply%20uniformly%20to%20all%20distributions%2C%20but%20such%20bounds%0Anecessarily%20degrade%20with%20feature%20dimension%2C%20d.%20While%20these%20bounds%20are%20shown%20to%0Abe%20tight%20for%20worst-case%20distributions%2C%20our%20bounds%20improve%20with%20the%20%22niceness%22%0Aof%20the%20data%20distribution.%20Concretely%2C%20we%20show%20that%20under%20additional%20assumptions%0Athat%20instances%20within%20each%20task%20are%20i.i.d.%20draws%20from%20broad%20well-studied%0Aclasses%20of%20distributions%20including%20sub-Gaussians%2C%20our%20generalization%20bounds%20do%0Anot%20get%20worse%20with%20increasing%20d%2C%20and%20are%20much%20sharper%20than%20prior%20work%20for%20very%0Alarge%20d.%20We%20also%20extend%20our%20results%20to%20a%20generalization%20of%20ridge%20regression%2C%0Awhere%20we%20achieve%20tighter%20bounds%20that%20take%20into%20account%20an%20estimate%20of%20the%20mean%0Aof%20the%20ground%20truth%20distribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05084v1&entry.124074799=Read"},
{"title": "VOTE: Vision-Language-Action Optimization with Trajectory Ensemble\n  Voting", "author": "Juyi Lin and Amir Taherin and Arash Akbari and Arman Akbari and Lei Lu and Guangyu Chen and Taskin Padir and Xiaomeng Yang and Weiwei Chen and Yiqian Li and Xue Lin and David Kaeli and Pu Zhao and Yanzhi Wang", "abstract": "  Recent large-scale Vision Language Action (VLA) models have shown superior\nperformance in robotic manipulation tasks guided by natural language. However,\ntheir generalization remains limited when applied to novel objects or\nunfamiliar environments that lie outside the training distribution. To address\nthis, many existing approaches integrate additional components such as depth\nestimation, segmentation, or even diffusion to improve generalization, at the\ncost of adding significant computation overhead, resulting in low efficiency.\nThis motivates the exploration of efficient action prediction methods, which\nare independent of additional high-level visual representations or diffusion\ntechniques. In this work, we propose VOTE, an efficient and general framework\nfor the optimization and acceleration of VLA models. In details, we propose a\nnovel tokenizer-free fine-tuning approach for parallel accurate action\nprediction, which reduces computational overhead and accelerates inference\nspeed. Additionally, we adopt an ensemble voting strategy for the action\nsampling, which significantly improves model performance and enhances\ngeneralization. Experimental results show that our method achieves\nstate-of-the-art performance with 35$\\times$ faster inference and 145 Hz\nthroughput. All the details and codes will be open-sourced.\n", "link": "http://arxiv.org/abs/2507.05116v1", "date": "2025-07-07", "relevancy": 2.2294, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5587}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5587}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VOTE%3A%20Vision-Language-Action%20Optimization%20with%20Trajectory%20Ensemble%0A%20%20Voting&body=Title%3A%20VOTE%3A%20Vision-Language-Action%20Optimization%20with%20Trajectory%20Ensemble%0A%20%20Voting%0AAuthor%3A%20Juyi%20Lin%20and%20Amir%20Taherin%20and%20Arash%20Akbari%20and%20Arman%20Akbari%20and%20Lei%20Lu%20and%20Guangyu%20Chen%20and%20Taskin%20Padir%20and%20Xiaomeng%20Yang%20and%20Weiwei%20Chen%20and%20Yiqian%20Li%20and%20Xue%20Lin%20and%20David%20Kaeli%20and%20Pu%20Zhao%20and%20Yanzhi%20Wang%0AAbstract%3A%20%20%20Recent%20large-scale%20Vision%20Language%20Action%20%28VLA%29%20models%20have%20shown%20superior%0Aperformance%20in%20robotic%20manipulation%20tasks%20guided%20by%20natural%20language.%20However%2C%0Atheir%20generalization%20remains%20limited%20when%20applied%20to%20novel%20objects%20or%0Aunfamiliar%20environments%20that%20lie%20outside%20the%20training%20distribution.%20To%20address%0Athis%2C%20many%20existing%20approaches%20integrate%20additional%20components%20such%20as%20depth%0Aestimation%2C%20segmentation%2C%20or%20even%20diffusion%20to%20improve%20generalization%2C%20at%20the%0Acost%20of%20adding%20significant%20computation%20overhead%2C%20resulting%20in%20low%20efficiency.%0AThis%20motivates%20the%20exploration%20of%20efficient%20action%20prediction%20methods%2C%20which%0Aare%20independent%20of%20additional%20high-level%20visual%20representations%20or%20diffusion%0Atechniques.%20In%20this%20work%2C%20we%20propose%20VOTE%2C%20an%20efficient%20and%20general%20framework%0Afor%20the%20optimization%20and%20acceleration%20of%20VLA%20models.%20In%20details%2C%20we%20propose%20a%0Anovel%20tokenizer-free%20fine-tuning%20approach%20for%20parallel%20accurate%20action%0Aprediction%2C%20which%20reduces%20computational%20overhead%20and%20accelerates%20inference%0Aspeed.%20Additionally%2C%20we%20adopt%20an%20ensemble%20voting%20strategy%20for%20the%20action%0Asampling%2C%20which%20significantly%20improves%20model%20performance%20and%20enhances%0Ageneralization.%20Experimental%20results%20show%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20with%2035%24%5Ctimes%24%20faster%20inference%20and%20145%20Hz%0Athroughput.%20All%20the%20details%20and%20codes%20will%20be%20open-sourced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05116v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVOTE%253A%2520Vision-Language-Action%2520Optimization%2520with%2520Trajectory%2520Ensemble%250A%2520%2520Voting%26entry.906535625%3DJuyi%2520Lin%2520and%2520Amir%2520Taherin%2520and%2520Arash%2520Akbari%2520and%2520Arman%2520Akbari%2520and%2520Lei%2520Lu%2520and%2520Guangyu%2520Chen%2520and%2520Taskin%2520Padir%2520and%2520Xiaomeng%2520Yang%2520and%2520Weiwei%2520Chen%2520and%2520Yiqian%2520Li%2520and%2520Xue%2520Lin%2520and%2520David%2520Kaeli%2520and%2520Pu%2520Zhao%2520and%2520Yanzhi%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520large-scale%2520Vision%2520Language%2520Action%2520%2528VLA%2529%2520models%2520have%2520shown%2520superior%250Aperformance%2520in%2520robotic%2520manipulation%2520tasks%2520guided%2520by%2520natural%2520language.%2520However%252C%250Atheir%2520generalization%2520remains%2520limited%2520when%2520applied%2520to%2520novel%2520objects%2520or%250Aunfamiliar%2520environments%2520that%2520lie%2520outside%2520the%2520training%2520distribution.%2520To%2520address%250Athis%252C%2520many%2520existing%2520approaches%2520integrate%2520additional%2520components%2520such%2520as%2520depth%250Aestimation%252C%2520segmentation%252C%2520or%2520even%2520diffusion%2520to%2520improve%2520generalization%252C%2520at%2520the%250Acost%2520of%2520adding%2520significant%2520computation%2520overhead%252C%2520resulting%2520in%2520low%2520efficiency.%250AThis%2520motivates%2520the%2520exploration%2520of%2520efficient%2520action%2520prediction%2520methods%252C%2520which%250Aare%2520independent%2520of%2520additional%2520high-level%2520visual%2520representations%2520or%2520diffusion%250Atechniques.%2520In%2520this%2520work%252C%2520we%2520propose%2520VOTE%252C%2520an%2520efficient%2520and%2520general%2520framework%250Afor%2520the%2520optimization%2520and%2520acceleration%2520of%2520VLA%2520models.%2520In%2520details%252C%2520we%2520propose%2520a%250Anovel%2520tokenizer-free%2520fine-tuning%2520approach%2520for%2520parallel%2520accurate%2520action%250Aprediction%252C%2520which%2520reduces%2520computational%2520overhead%2520and%2520accelerates%2520inference%250Aspeed.%2520Additionally%252C%2520we%2520adopt%2520an%2520ensemble%2520voting%2520strategy%2520for%2520the%2520action%250Asampling%252C%2520which%2520significantly%2520improves%2520model%2520performance%2520and%2520enhances%250Ageneralization.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520performance%2520with%252035%2524%255Ctimes%2524%2520faster%2520inference%2520and%2520145%2520Hz%250Athroughput.%2520All%2520the%2520details%2520and%2520codes%2520will%2520be%2520open-sourced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05116v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VOTE%3A%20Vision-Language-Action%20Optimization%20with%20Trajectory%20Ensemble%0A%20%20Voting&entry.906535625=Juyi%20Lin%20and%20Amir%20Taherin%20and%20Arash%20Akbari%20and%20Arman%20Akbari%20and%20Lei%20Lu%20and%20Guangyu%20Chen%20and%20Taskin%20Padir%20and%20Xiaomeng%20Yang%20and%20Weiwei%20Chen%20and%20Yiqian%20Li%20and%20Xue%20Lin%20and%20David%20Kaeli%20and%20Pu%20Zhao%20and%20Yanzhi%20Wang&entry.1292438233=%20%20Recent%20large-scale%20Vision%20Language%20Action%20%28VLA%29%20models%20have%20shown%20superior%0Aperformance%20in%20robotic%20manipulation%20tasks%20guided%20by%20natural%20language.%20However%2C%0Atheir%20generalization%20remains%20limited%20when%20applied%20to%20novel%20objects%20or%0Aunfamiliar%20environments%20that%20lie%20outside%20the%20training%20distribution.%20To%20address%0Athis%2C%20many%20existing%20approaches%20integrate%20additional%20components%20such%20as%20depth%0Aestimation%2C%20segmentation%2C%20or%20even%20diffusion%20to%20improve%20generalization%2C%20at%20the%0Acost%20of%20adding%20significant%20computation%20overhead%2C%20resulting%20in%20low%20efficiency.%0AThis%20motivates%20the%20exploration%20of%20efficient%20action%20prediction%20methods%2C%20which%0Aare%20independent%20of%20additional%20high-level%20visual%20representations%20or%20diffusion%0Atechniques.%20In%20this%20work%2C%20we%20propose%20VOTE%2C%20an%20efficient%20and%20general%20framework%0Afor%20the%20optimization%20and%20acceleration%20of%20VLA%20models.%20In%20details%2C%20we%20propose%20a%0Anovel%20tokenizer-free%20fine-tuning%20approach%20for%20parallel%20accurate%20action%0Aprediction%2C%20which%20reduces%20computational%20overhead%20and%20accelerates%20inference%0Aspeed.%20Additionally%2C%20we%20adopt%20an%20ensemble%20voting%20strategy%20for%20the%20action%0Asampling%2C%20which%20significantly%20improves%20model%20performance%20and%20enhances%0Ageneralization.%20Experimental%20results%20show%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20with%2035%24%5Ctimes%24%20faster%20inference%20and%20145%20Hz%0Athroughput.%20All%20the%20details%20and%20codes%20will%20be%20open-sourced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05116v1&entry.124074799=Read"},
{"title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study", "author": "Yuqi Zhu and Yi Zhong and Jintian Zhang and Ziheng Zhang and Shuofei Qiao and Yujie Luo and Lun Du and Da Zheng and Huajun Chen and Ningyu Zhang", "abstract": "  Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities.\n", "link": "http://arxiv.org/abs/2506.19794v2", "date": "2025-07-07", "relevancy": 2.2272, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5721}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5721}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Do%20Open-Source%20LLMs%20Struggle%20with%20Data%20Analysis%3F%20A%20Systematic%0A%20%20Empirical%20Study&body=Title%3A%20Why%20Do%20Open-Source%20LLMs%20Struggle%20with%20Data%20Analysis%3F%20A%20Systematic%0A%20%20Empirical%20Study%0AAuthor%3A%20Yuqi%20Zhu%20and%20Yi%20Zhong%20and%20Jintian%20Zhang%20and%20Ziheng%20Zhang%20and%20Shuofei%20Qiao%20and%20Yujie%20Luo%20and%20Lun%20Du%20and%20Da%20Zheng%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20hold%20promise%20in%20automating%20data%20analysis%20tasks%2C%0Ayet%20open-source%20models%20face%20significant%20limitations%20in%20these%20kinds%20of%0Areasoning-intensive%20scenarios.%20In%20this%20work%2C%20we%20investigate%20strategies%20to%0Aenhance%20the%20data%20analysis%20capabilities%20of%20open-source%20LLMs.%20By%20curating%20a%20seed%0Adataset%20of%20diverse%2C%20realistic%20scenarios%2C%20we%20evaluate%20models%20across%20three%0Adimensions%3A%20data%20understanding%2C%20code%20generation%2C%20and%20strategic%20planning.%20Our%0Aanalysis%20reveals%20three%20key%20findings%3A%20%281%29%20Strategic%20planning%20quality%20serves%20as%0Athe%20primary%20determinant%20of%20model%20performance%3B%20%282%29%20Interaction%20design%20and%20task%0Acomplexity%20significantly%20influence%20reasoning%20capabilities%3B%20%283%29%20Data%20quality%0Ademonstrates%20a%20greater%20impact%20than%20diversity%20in%20achieving%20optimal%20performance.%0AWe%20leverage%20these%20insights%20to%20develop%20a%20data%20synthesis%20methodology%2C%0Ademonstrating%20significant%20improvements%20in%20open-source%20LLMs%27%20analytical%0Areasoning%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.19794v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Do%2520Open-Source%2520LLMs%2520Struggle%2520with%2520Data%2520Analysis%253F%2520A%2520Systematic%250A%2520%2520Empirical%2520Study%26entry.906535625%3DYuqi%2520Zhu%2520and%2520Yi%2520Zhong%2520and%2520Jintian%2520Zhang%2520and%2520Ziheng%2520Zhang%2520and%2520Shuofei%2520Qiao%2520and%2520Yujie%2520Luo%2520and%2520Lun%2520Du%2520and%2520Da%2520Zheng%2520and%2520Huajun%2520Chen%2520and%2520Ningyu%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520hold%2520promise%2520in%2520automating%2520data%2520analysis%2520tasks%252C%250Ayet%2520open-source%2520models%2520face%2520significant%2520limitations%2520in%2520these%2520kinds%2520of%250Areasoning-intensive%2520scenarios.%2520In%2520this%2520work%252C%2520we%2520investigate%2520strategies%2520to%250Aenhance%2520the%2520data%2520analysis%2520capabilities%2520of%2520open-source%2520LLMs.%2520By%2520curating%2520a%2520seed%250Adataset%2520of%2520diverse%252C%2520realistic%2520scenarios%252C%2520we%2520evaluate%2520models%2520across%2520three%250Adimensions%253A%2520data%2520understanding%252C%2520code%2520generation%252C%2520and%2520strategic%2520planning.%2520Our%250Aanalysis%2520reveals%2520three%2520key%2520findings%253A%2520%25281%2529%2520Strategic%2520planning%2520quality%2520serves%2520as%250Athe%2520primary%2520determinant%2520of%2520model%2520performance%253B%2520%25282%2529%2520Interaction%2520design%2520and%2520task%250Acomplexity%2520significantly%2520influence%2520reasoning%2520capabilities%253B%2520%25283%2529%2520Data%2520quality%250Ademonstrates%2520a%2520greater%2520impact%2520than%2520diversity%2520in%2520achieving%2520optimal%2520performance.%250AWe%2520leverage%2520these%2520insights%2520to%2520develop%2520a%2520data%2520synthesis%2520methodology%252C%250Ademonstrating%2520significant%2520improvements%2520in%2520open-source%2520LLMs%2527%2520analytical%250Areasoning%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19794v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Do%20Open-Source%20LLMs%20Struggle%20with%20Data%20Analysis%3F%20A%20Systematic%0A%20%20Empirical%20Study&entry.906535625=Yuqi%20Zhu%20and%20Yi%20Zhong%20and%20Jintian%20Zhang%20and%20Ziheng%20Zhang%20and%20Shuofei%20Qiao%20and%20Yujie%20Luo%20and%20Lun%20Du%20and%20Da%20Zheng%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20hold%20promise%20in%20automating%20data%20analysis%20tasks%2C%0Ayet%20open-source%20models%20face%20significant%20limitations%20in%20these%20kinds%20of%0Areasoning-intensive%20scenarios.%20In%20this%20work%2C%20we%20investigate%20strategies%20to%0Aenhance%20the%20data%20analysis%20capabilities%20of%20open-source%20LLMs.%20By%20curating%20a%20seed%0Adataset%20of%20diverse%2C%20realistic%20scenarios%2C%20we%20evaluate%20models%20across%20three%0Adimensions%3A%20data%20understanding%2C%20code%20generation%2C%20and%20strategic%20planning.%20Our%0Aanalysis%20reveals%20three%20key%20findings%3A%20%281%29%20Strategic%20planning%20quality%20serves%20as%0Athe%20primary%20determinant%20of%20model%20performance%3B%20%282%29%20Interaction%20design%20and%20task%0Acomplexity%20significantly%20influence%20reasoning%20capabilities%3B%20%283%29%20Data%20quality%0Ademonstrates%20a%20greater%20impact%20than%20diversity%20in%20achieving%20optimal%20performance.%0AWe%20leverage%20these%20insights%20to%20develop%20a%20data%20synthesis%20methodology%2C%0Ademonstrating%20significant%20improvements%20in%20open-source%20LLMs%27%20analytical%0Areasoning%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.19794v2&entry.124074799=Read"},
{"title": "Latent Motion Profiling for Annotation-free Cardiac Phase Detection in\n  Adult and Fetal Echocardiography Videos", "author": "Yingyu Yang and Qianye Yang and Kangning Cui and Can Peng and Elena D'Alberti and Netzahualcoyotl Hernandez-Cruz and Olga Patey and Aris T. Papageorghiou and J. Alison Noble", "abstract": "  The identification of cardiac phase is an essential step for analysis and\ndiagnosis of cardiac function. Automatic methods, especially data-driven\nmethods for cardiac phase detection, typically require extensive annotations,\nwhich is time-consuming and labor-intensive. In this paper, we present an\nunsupervised framework for end-diastole (ED) and end-systole (ES) detection\nthrough self-supervised learning of latent cardiac motion trajectories from\n4-chamber-view echocardiography videos. Our method eliminates the need for\nmanual annotations, including ED and ES indices, segmentation, or volumetric\nmeasurements, by training a reconstruction model to encode interpretable\nspatiotemporal motion patterns. Evaluated on the EchoNet-Dynamic benchmark, the\napproach achieves mean absolute error (MAE) of 3 frames (58.3 ms) for ED and 2\nframes (38.8 ms) for ES detection, matching state-of-the-art supervised\nmethods. Extended to fetal echocardiography, the model demonstrates robust\nperformance with MAE 1.46 frames (20.7 ms) for ED and 1.74 frames (25.3 ms) for\nES, despite the fact that the fetal heart model is built using non-standardized\nheart views due to fetal heart positioning variability. Our results demonstrate\nthe potential of the proposed latent motion trajectory strategy for cardiac\nphase detection in adult and fetal echocardiography. This work advances\nunsupervised cardiac motion analysis, offering a scalable solution for clinical\npopulations lacking annotated data. Code will be released at\nhttps://github.com/YingyuYyy/CardiacPhase.\n", "link": "http://arxiv.org/abs/2507.05154v1", "date": "2025-07-07", "relevancy": 2.2214, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5671}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5517}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Motion%20Profiling%20for%20Annotation-free%20Cardiac%20Phase%20Detection%20in%0A%20%20Adult%20and%20Fetal%20Echocardiography%20Videos&body=Title%3A%20Latent%20Motion%20Profiling%20for%20Annotation-free%20Cardiac%20Phase%20Detection%20in%0A%20%20Adult%20and%20Fetal%20Echocardiography%20Videos%0AAuthor%3A%20Yingyu%20Yang%20and%20Qianye%20Yang%20and%20Kangning%20Cui%20and%20Can%20Peng%20and%20Elena%20D%27Alberti%20and%20Netzahualcoyotl%20Hernandez-Cruz%20and%20Olga%20Patey%20and%20Aris%20T.%20Papageorghiou%20and%20J.%20Alison%20Noble%0AAbstract%3A%20%20%20The%20identification%20of%20cardiac%20phase%20is%20an%20essential%20step%20for%20analysis%20and%0Adiagnosis%20of%20cardiac%20function.%20Automatic%20methods%2C%20especially%20data-driven%0Amethods%20for%20cardiac%20phase%20detection%2C%20typically%20require%20extensive%20annotations%2C%0Awhich%20is%20time-consuming%20and%20labor-intensive.%20In%20this%20paper%2C%20we%20present%20an%0Aunsupervised%20framework%20for%20end-diastole%20%28ED%29%20and%20end-systole%20%28ES%29%20detection%0Athrough%20self-supervised%20learning%20of%20latent%20cardiac%20motion%20trajectories%20from%0A4-chamber-view%20echocardiography%20videos.%20Our%20method%20eliminates%20the%20need%20for%0Amanual%20annotations%2C%20including%20ED%20and%20ES%20indices%2C%20segmentation%2C%20or%20volumetric%0Ameasurements%2C%20by%20training%20a%20reconstruction%20model%20to%20encode%20interpretable%0Aspatiotemporal%20motion%20patterns.%20Evaluated%20on%20the%20EchoNet-Dynamic%20benchmark%2C%20the%0Aapproach%20achieves%20mean%20absolute%20error%20%28MAE%29%20of%203%20frames%20%2858.3%20ms%29%20for%20ED%20and%202%0Aframes%20%2838.8%20ms%29%20for%20ES%20detection%2C%20matching%20state-of-the-art%20supervised%0Amethods.%20Extended%20to%20fetal%20echocardiography%2C%20the%20model%20demonstrates%20robust%0Aperformance%20with%20MAE%201.46%20frames%20%2820.7%20ms%29%20for%20ED%20and%201.74%20frames%20%2825.3%20ms%29%20for%0AES%2C%20despite%20the%20fact%20that%20the%20fetal%20heart%20model%20is%20built%20using%20non-standardized%0Aheart%20views%20due%20to%20fetal%20heart%20positioning%20variability.%20Our%20results%20demonstrate%0Athe%20potential%20of%20the%20proposed%20latent%20motion%20trajectory%20strategy%20for%20cardiac%0Aphase%20detection%20in%20adult%20and%20fetal%20echocardiography.%20This%20work%20advances%0Aunsupervised%20cardiac%20motion%20analysis%2C%20offering%20a%20scalable%20solution%20for%20clinical%0Apopulations%20lacking%20annotated%20data.%20Code%20will%20be%20released%20at%0Ahttps%3A//github.com/YingyuYyy/CardiacPhase.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Motion%2520Profiling%2520for%2520Annotation-free%2520Cardiac%2520Phase%2520Detection%2520in%250A%2520%2520Adult%2520and%2520Fetal%2520Echocardiography%2520Videos%26entry.906535625%3DYingyu%2520Yang%2520and%2520Qianye%2520Yang%2520and%2520Kangning%2520Cui%2520and%2520Can%2520Peng%2520and%2520Elena%2520D%2527Alberti%2520and%2520Netzahualcoyotl%2520Hernandez-Cruz%2520and%2520Olga%2520Patey%2520and%2520Aris%2520T.%2520Papageorghiou%2520and%2520J.%2520Alison%2520Noble%26entry.1292438233%3D%2520%2520The%2520identification%2520of%2520cardiac%2520phase%2520is%2520an%2520essential%2520step%2520for%2520analysis%2520and%250Adiagnosis%2520of%2520cardiac%2520function.%2520Automatic%2520methods%252C%2520especially%2520data-driven%250Amethods%2520for%2520cardiac%2520phase%2520detection%252C%2520typically%2520require%2520extensive%2520annotations%252C%250Awhich%2520is%2520time-consuming%2520and%2520labor-intensive.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%250Aunsupervised%2520framework%2520for%2520end-diastole%2520%2528ED%2529%2520and%2520end-systole%2520%2528ES%2529%2520detection%250Athrough%2520self-supervised%2520learning%2520of%2520latent%2520cardiac%2520motion%2520trajectories%2520from%250A4-chamber-view%2520echocardiography%2520videos.%2520Our%2520method%2520eliminates%2520the%2520need%2520for%250Amanual%2520annotations%252C%2520including%2520ED%2520and%2520ES%2520indices%252C%2520segmentation%252C%2520or%2520volumetric%250Ameasurements%252C%2520by%2520training%2520a%2520reconstruction%2520model%2520to%2520encode%2520interpretable%250Aspatiotemporal%2520motion%2520patterns.%2520Evaluated%2520on%2520the%2520EchoNet-Dynamic%2520benchmark%252C%2520the%250Aapproach%2520achieves%2520mean%2520absolute%2520error%2520%2528MAE%2529%2520of%25203%2520frames%2520%252858.3%2520ms%2529%2520for%2520ED%2520and%25202%250Aframes%2520%252838.8%2520ms%2529%2520for%2520ES%2520detection%252C%2520matching%2520state-of-the-art%2520supervised%250Amethods.%2520Extended%2520to%2520fetal%2520echocardiography%252C%2520the%2520model%2520demonstrates%2520robust%250Aperformance%2520with%2520MAE%25201.46%2520frames%2520%252820.7%2520ms%2529%2520for%2520ED%2520and%25201.74%2520frames%2520%252825.3%2520ms%2529%2520for%250AES%252C%2520despite%2520the%2520fact%2520that%2520the%2520fetal%2520heart%2520model%2520is%2520built%2520using%2520non-standardized%250Aheart%2520views%2520due%2520to%2520fetal%2520heart%2520positioning%2520variability.%2520Our%2520results%2520demonstrate%250Athe%2520potential%2520of%2520the%2520proposed%2520latent%2520motion%2520trajectory%2520strategy%2520for%2520cardiac%250Aphase%2520detection%2520in%2520adult%2520and%2520fetal%2520echocardiography.%2520This%2520work%2520advances%250Aunsupervised%2520cardiac%2520motion%2520analysis%252C%2520offering%2520a%2520scalable%2520solution%2520for%2520clinical%250Apopulations%2520lacking%2520annotated%2520data.%2520Code%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/YingyuYyy/CardiacPhase.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Motion%20Profiling%20for%20Annotation-free%20Cardiac%20Phase%20Detection%20in%0A%20%20Adult%20and%20Fetal%20Echocardiography%20Videos&entry.906535625=Yingyu%20Yang%20and%20Qianye%20Yang%20and%20Kangning%20Cui%20and%20Can%20Peng%20and%20Elena%20D%27Alberti%20and%20Netzahualcoyotl%20Hernandez-Cruz%20and%20Olga%20Patey%20and%20Aris%20T.%20Papageorghiou%20and%20J.%20Alison%20Noble&entry.1292438233=%20%20The%20identification%20of%20cardiac%20phase%20is%20an%20essential%20step%20for%20analysis%20and%0Adiagnosis%20of%20cardiac%20function.%20Automatic%20methods%2C%20especially%20data-driven%0Amethods%20for%20cardiac%20phase%20detection%2C%20typically%20require%20extensive%20annotations%2C%0Awhich%20is%20time-consuming%20and%20labor-intensive.%20In%20this%20paper%2C%20we%20present%20an%0Aunsupervised%20framework%20for%20end-diastole%20%28ED%29%20and%20end-systole%20%28ES%29%20detection%0Athrough%20self-supervised%20learning%20of%20latent%20cardiac%20motion%20trajectories%20from%0A4-chamber-view%20echocardiography%20videos.%20Our%20method%20eliminates%20the%20need%20for%0Amanual%20annotations%2C%20including%20ED%20and%20ES%20indices%2C%20segmentation%2C%20or%20volumetric%0Ameasurements%2C%20by%20training%20a%20reconstruction%20model%20to%20encode%20interpretable%0Aspatiotemporal%20motion%20patterns.%20Evaluated%20on%20the%20EchoNet-Dynamic%20benchmark%2C%20the%0Aapproach%20achieves%20mean%20absolute%20error%20%28MAE%29%20of%203%20frames%20%2858.3%20ms%29%20for%20ED%20and%202%0Aframes%20%2838.8%20ms%29%20for%20ES%20detection%2C%20matching%20state-of-the-art%20supervised%0Amethods.%20Extended%20to%20fetal%20echocardiography%2C%20the%20model%20demonstrates%20robust%0Aperformance%20with%20MAE%201.46%20frames%20%2820.7%20ms%29%20for%20ED%20and%201.74%20frames%20%2825.3%20ms%29%20for%0AES%2C%20despite%20the%20fact%20that%20the%20fetal%20heart%20model%20is%20built%20using%20non-standardized%0Aheart%20views%20due%20to%20fetal%20heart%20positioning%20variability.%20Our%20results%20demonstrate%0Athe%20potential%20of%20the%20proposed%20latent%20motion%20trajectory%20strategy%20for%20cardiac%0Aphase%20detection%20in%20adult%20and%20fetal%20echocardiography.%20This%20work%20advances%0Aunsupervised%20cardiac%20motion%20analysis%2C%20offering%20a%20scalable%20solution%20for%20clinical%0Apopulations%20lacking%20annotated%20data.%20Code%20will%20be%20released%20at%0Ahttps%3A//github.com/YingyuYyy/CardiacPhase.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05154v1&entry.124074799=Read"},
{"title": "Discrete Diffusion Trajectory Alignment via Stepwise Decomposition", "author": "Jiaqi Han and Austin Wang and Minkai Xu and Wenda Chu and Meihua Dang and Yisong Yue and Stefano Ermon", "abstract": "  Discrete diffusion models have demonstrated great promise in modeling various\nsequence data, ranging from human language to biological sequences. Inspired by\nthe success of RL in language models, there is growing interest in further\nimproving the models by alignment with a certain reward. In this work, we\npropose a novel preference optimization method for masked discrete diffusion\nmodels through a principled diffusion trajectory alignment. Instead of applying\nthe reward on the final output and backpropagating the gradient to the entire\ndiscrete denoising process, we decompose the problem into a set of stepwise\nalignment objectives. This framework enables efficient diffusion optimization,\nis compatible with arbitrary reward functions, and importantly, guarantees an\nequivalent optimal solution under additive factorization of the trajectory\nreward. Experiments across multiple domains including DNA sequence design,\nprotein inverse folding, and language modeling consistently demonstrate the\nsuperiority of our approach. Notably, it achieves an up to 12\\% improvement\nover the most competitive RL-based baseline in terms of predicted activity on\nDNA sequence design, and further improves the GSM8K score from 78.6 to 80.7 on\nLLaDA-8B-Instruct for language modeling.\n", "link": "http://arxiv.org/abs/2507.04832v1", "date": "2025-07-07", "relevancy": 2.2207, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5687}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5548}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discrete%20Diffusion%20Trajectory%20Alignment%20via%20Stepwise%20Decomposition&body=Title%3A%20Discrete%20Diffusion%20Trajectory%20Alignment%20via%20Stepwise%20Decomposition%0AAuthor%3A%20Jiaqi%20Han%20and%20Austin%20Wang%20and%20Minkai%20Xu%20and%20Wenda%20Chu%20and%20Meihua%20Dang%20and%20Yisong%20Yue%20and%20Stefano%20Ermon%0AAbstract%3A%20%20%20Discrete%20diffusion%20models%20have%20demonstrated%20great%20promise%20in%20modeling%20various%0Asequence%20data%2C%20ranging%20from%20human%20language%20to%20biological%20sequences.%20Inspired%20by%0Athe%20success%20of%20RL%20in%20language%20models%2C%20there%20is%20growing%20interest%20in%20further%0Aimproving%20the%20models%20by%20alignment%20with%20a%20certain%20reward.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20preference%20optimization%20method%20for%20masked%20discrete%20diffusion%0Amodels%20through%20a%20principled%20diffusion%20trajectory%20alignment.%20Instead%20of%20applying%0Athe%20reward%20on%20the%20final%20output%20and%20backpropagating%20the%20gradient%20to%20the%20entire%0Adiscrete%20denoising%20process%2C%20we%20decompose%20the%20problem%20into%20a%20set%20of%20stepwise%0Aalignment%20objectives.%20This%20framework%20enables%20efficient%20diffusion%20optimization%2C%0Ais%20compatible%20with%20arbitrary%20reward%20functions%2C%20and%20importantly%2C%20guarantees%20an%0Aequivalent%20optimal%20solution%20under%20additive%20factorization%20of%20the%20trajectory%0Areward.%20Experiments%20across%20multiple%20domains%20including%20DNA%20sequence%20design%2C%0Aprotein%20inverse%20folding%2C%20and%20language%20modeling%20consistently%20demonstrate%20the%0Asuperiority%20of%20our%20approach.%20Notably%2C%20it%20achieves%20an%20up%20to%2012%5C%25%20improvement%0Aover%20the%20most%20competitive%20RL-based%20baseline%20in%20terms%20of%20predicted%20activity%20on%0ADNA%20sequence%20design%2C%20and%20further%20improves%20the%20GSM8K%20score%20from%2078.6%20to%2080.7%20on%0ALLaDA-8B-Instruct%20for%20language%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscrete%2520Diffusion%2520Trajectory%2520Alignment%2520via%2520Stepwise%2520Decomposition%26entry.906535625%3DJiaqi%2520Han%2520and%2520Austin%2520Wang%2520and%2520Minkai%2520Xu%2520and%2520Wenda%2520Chu%2520and%2520Meihua%2520Dang%2520and%2520Yisong%2520Yue%2520and%2520Stefano%2520Ermon%26entry.1292438233%3D%2520%2520Discrete%2520diffusion%2520models%2520have%2520demonstrated%2520great%2520promise%2520in%2520modeling%2520various%250Asequence%2520data%252C%2520ranging%2520from%2520human%2520language%2520to%2520biological%2520sequences.%2520Inspired%2520by%250Athe%2520success%2520of%2520RL%2520in%2520language%2520models%252C%2520there%2520is%2520growing%2520interest%2520in%2520further%250Aimproving%2520the%2520models%2520by%2520alignment%2520with%2520a%2520certain%2520reward.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520preference%2520optimization%2520method%2520for%2520masked%2520discrete%2520diffusion%250Amodels%2520through%2520a%2520principled%2520diffusion%2520trajectory%2520alignment.%2520Instead%2520of%2520applying%250Athe%2520reward%2520on%2520the%2520final%2520output%2520and%2520backpropagating%2520the%2520gradient%2520to%2520the%2520entire%250Adiscrete%2520denoising%2520process%252C%2520we%2520decompose%2520the%2520problem%2520into%2520a%2520set%2520of%2520stepwise%250Aalignment%2520objectives.%2520This%2520framework%2520enables%2520efficient%2520diffusion%2520optimization%252C%250Ais%2520compatible%2520with%2520arbitrary%2520reward%2520functions%252C%2520and%2520importantly%252C%2520guarantees%2520an%250Aequivalent%2520optimal%2520solution%2520under%2520additive%2520factorization%2520of%2520the%2520trajectory%250Areward.%2520Experiments%2520across%2520multiple%2520domains%2520including%2520DNA%2520sequence%2520design%252C%250Aprotein%2520inverse%2520folding%252C%2520and%2520language%2520modeling%2520consistently%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520approach.%2520Notably%252C%2520it%2520achieves%2520an%2520up%2520to%252012%255C%2525%2520improvement%250Aover%2520the%2520most%2520competitive%2520RL-based%2520baseline%2520in%2520terms%2520of%2520predicted%2520activity%2520on%250ADNA%2520sequence%2520design%252C%2520and%2520further%2520improves%2520the%2520GSM8K%2520score%2520from%252078.6%2520to%252080.7%2520on%250ALLaDA-8B-Instruct%2520for%2520language%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discrete%20Diffusion%20Trajectory%20Alignment%20via%20Stepwise%20Decomposition&entry.906535625=Jiaqi%20Han%20and%20Austin%20Wang%20and%20Minkai%20Xu%20and%20Wenda%20Chu%20and%20Meihua%20Dang%20and%20Yisong%20Yue%20and%20Stefano%20Ermon&entry.1292438233=%20%20Discrete%20diffusion%20models%20have%20demonstrated%20great%20promise%20in%20modeling%20various%0Asequence%20data%2C%20ranging%20from%20human%20language%20to%20biological%20sequences.%20Inspired%20by%0Athe%20success%20of%20RL%20in%20language%20models%2C%20there%20is%20growing%20interest%20in%20further%0Aimproving%20the%20models%20by%20alignment%20with%20a%20certain%20reward.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20preference%20optimization%20method%20for%20masked%20discrete%20diffusion%0Amodels%20through%20a%20principled%20diffusion%20trajectory%20alignment.%20Instead%20of%20applying%0Athe%20reward%20on%20the%20final%20output%20and%20backpropagating%20the%20gradient%20to%20the%20entire%0Adiscrete%20denoising%20process%2C%20we%20decompose%20the%20problem%20into%20a%20set%20of%20stepwise%0Aalignment%20objectives.%20This%20framework%20enables%20efficient%20diffusion%20optimization%2C%0Ais%20compatible%20with%20arbitrary%20reward%20functions%2C%20and%20importantly%2C%20guarantees%20an%0Aequivalent%20optimal%20solution%20under%20additive%20factorization%20of%20the%20trajectory%0Areward.%20Experiments%20across%20multiple%20domains%20including%20DNA%20sequence%20design%2C%0Aprotein%20inverse%20folding%2C%20and%20language%20modeling%20consistently%20demonstrate%20the%0Asuperiority%20of%20our%20approach.%20Notably%2C%20it%20achieves%20an%20up%20to%2012%5C%25%20improvement%0Aover%20the%20most%20competitive%20RL-based%20baseline%20in%20terms%20of%20predicted%20activity%20on%0ADNA%20sequence%20design%2C%20and%20further%20improves%20the%20GSM8K%20score%20from%2078.6%20to%2080.7%20on%0ALLaDA-8B-Instruct%20for%20language%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04832v1&entry.124074799=Read"},
{"title": "Music Boomerang: Reusing Diffusion Models for Data Augmentation and\n  Audio Manipulation", "author": "Alexander Fichtinger and Jan Schl\u00fcter and Gerhard Widmer", "abstract": "  Generative models of music audio are typically used to generate output based\nsolely on a text prompt or melody. Boomerang sampling, recently proposed for\nthe image domain, allows generating output close to an existing example, using\nany pretrained diffusion model. In this work, we explore its application in the\naudio domain as a tool for data augmentation or content manipulation.\nSpecifically, implementing Boomerang sampling for Stable Audio Open, we augment\ntraining data for a state-of-the-art beat tracker, and attempt to replace\nmusical instruments in recordings. Our results show that the rhythmic structure\nof existing examples is mostly preserved, that it improves performance of the\nbeat tracker, but only in scenarios of limited training data, and that it can\naccomplish text-based instrument replacement on monophonic inputs. We publish\nour implementation to invite experiments on data augmentation in other tasks\nand explore further applications.\n", "link": "http://arxiv.org/abs/2507.04864v1", "date": "2025-07-07", "relevancy": 2.21, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.563}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.555}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Music%20Boomerang%3A%20Reusing%20Diffusion%20Models%20for%20Data%20Augmentation%20and%0A%20%20Audio%20Manipulation&body=Title%3A%20Music%20Boomerang%3A%20Reusing%20Diffusion%20Models%20for%20Data%20Augmentation%20and%0A%20%20Audio%20Manipulation%0AAuthor%3A%20Alexander%20Fichtinger%20and%20Jan%20Schl%C3%BCter%20and%20Gerhard%20Widmer%0AAbstract%3A%20%20%20Generative%20models%20of%20music%20audio%20are%20typically%20used%20to%20generate%20output%20based%0Asolely%20on%20a%20text%20prompt%20or%20melody.%20Boomerang%20sampling%2C%20recently%20proposed%20for%0Athe%20image%20domain%2C%20allows%20generating%20output%20close%20to%20an%20existing%20example%2C%20using%0Aany%20pretrained%20diffusion%20model.%20In%20this%20work%2C%20we%20explore%20its%20application%20in%20the%0Aaudio%20domain%20as%20a%20tool%20for%20data%20augmentation%20or%20content%20manipulation.%0ASpecifically%2C%20implementing%20Boomerang%20sampling%20for%20Stable%20Audio%20Open%2C%20we%20augment%0Atraining%20data%20for%20a%20state-of-the-art%20beat%20tracker%2C%20and%20attempt%20to%20replace%0Amusical%20instruments%20in%20recordings.%20Our%20results%20show%20that%20the%20rhythmic%20structure%0Aof%20existing%20examples%20is%20mostly%20preserved%2C%20that%20it%20improves%20performance%20of%20the%0Abeat%20tracker%2C%20but%20only%20in%20scenarios%20of%20limited%20training%20data%2C%20and%20that%20it%20can%0Aaccomplish%20text-based%20instrument%20replacement%20on%20monophonic%20inputs.%20We%20publish%0Aour%20implementation%20to%20invite%20experiments%20on%20data%20augmentation%20in%20other%20tasks%0Aand%20explore%20further%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMusic%2520Boomerang%253A%2520Reusing%2520Diffusion%2520Models%2520for%2520Data%2520Augmentation%2520and%250A%2520%2520Audio%2520Manipulation%26entry.906535625%3DAlexander%2520Fichtinger%2520and%2520Jan%2520Schl%25C3%25BCter%2520and%2520Gerhard%2520Widmer%26entry.1292438233%3D%2520%2520Generative%2520models%2520of%2520music%2520audio%2520are%2520typically%2520used%2520to%2520generate%2520output%2520based%250Asolely%2520on%2520a%2520text%2520prompt%2520or%2520melody.%2520Boomerang%2520sampling%252C%2520recently%2520proposed%2520for%250Athe%2520image%2520domain%252C%2520allows%2520generating%2520output%2520close%2520to%2520an%2520existing%2520example%252C%2520using%250Aany%2520pretrained%2520diffusion%2520model.%2520In%2520this%2520work%252C%2520we%2520explore%2520its%2520application%2520in%2520the%250Aaudio%2520domain%2520as%2520a%2520tool%2520for%2520data%2520augmentation%2520or%2520content%2520manipulation.%250ASpecifically%252C%2520implementing%2520Boomerang%2520sampling%2520for%2520Stable%2520Audio%2520Open%252C%2520we%2520augment%250Atraining%2520data%2520for%2520a%2520state-of-the-art%2520beat%2520tracker%252C%2520and%2520attempt%2520to%2520replace%250Amusical%2520instruments%2520in%2520recordings.%2520Our%2520results%2520show%2520that%2520the%2520rhythmic%2520structure%250Aof%2520existing%2520examples%2520is%2520mostly%2520preserved%252C%2520that%2520it%2520improves%2520performance%2520of%2520the%250Abeat%2520tracker%252C%2520but%2520only%2520in%2520scenarios%2520of%2520limited%2520training%2520data%252C%2520and%2520that%2520it%2520can%250Aaccomplish%2520text-based%2520instrument%2520replacement%2520on%2520monophonic%2520inputs.%2520We%2520publish%250Aour%2520implementation%2520to%2520invite%2520experiments%2520on%2520data%2520augmentation%2520in%2520other%2520tasks%250Aand%2520explore%2520further%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Music%20Boomerang%3A%20Reusing%20Diffusion%20Models%20for%20Data%20Augmentation%20and%0A%20%20Audio%20Manipulation&entry.906535625=Alexander%20Fichtinger%20and%20Jan%20Schl%C3%BCter%20and%20Gerhard%20Widmer&entry.1292438233=%20%20Generative%20models%20of%20music%20audio%20are%20typically%20used%20to%20generate%20output%20based%0Asolely%20on%20a%20text%20prompt%20or%20melody.%20Boomerang%20sampling%2C%20recently%20proposed%20for%0Athe%20image%20domain%2C%20allows%20generating%20output%20close%20to%20an%20existing%20example%2C%20using%0Aany%20pretrained%20diffusion%20model.%20In%20this%20work%2C%20we%20explore%20its%20application%20in%20the%0Aaudio%20domain%20as%20a%20tool%20for%20data%20augmentation%20or%20content%20manipulation.%0ASpecifically%2C%20implementing%20Boomerang%20sampling%20for%20Stable%20Audio%20Open%2C%20we%20augment%0Atraining%20data%20for%20a%20state-of-the-art%20beat%20tracker%2C%20and%20attempt%20to%20replace%0Amusical%20instruments%20in%20recordings.%20Our%20results%20show%20that%20the%20rhythmic%20structure%0Aof%20existing%20examples%20is%20mostly%20preserved%2C%20that%20it%20improves%20performance%20of%20the%0Abeat%20tracker%2C%20but%20only%20in%20scenarios%20of%20limited%20training%20data%2C%20and%20that%20it%20can%0Aaccomplish%20text-based%20instrument%20replacement%20on%20monophonic%20inputs.%20We%20publish%0Aour%20implementation%20to%20invite%20experiments%20on%20data%20augmentation%20in%20other%20tasks%0Aand%20explore%20further%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04864v1&entry.124074799=Read"},
{"title": "Train-before-Test Harmonizes Language Model Rankings", "author": "Guanhua Zhang and Ricardo Dominguez-Olmedo and Moritz Hardt", "abstract": "  Existing language model benchmarks provide contradictory model rankings, even\nfor benchmarks that aim to capture similar skills. This dilemma of conflicting\nrankings hampers model selection, clouds model comparisons, and adds confusion\nto a growing ecosystem of competing models. Recent work attributed ranking\ndisagreement to the phenomenon of training on the test task: As released,\ndifferent models exhibit a different level of preparation for any given test\ntask. A candidate solution to the problem is train-before-test: Give each model\nthe same benchmark-specific finetuning before evaluation. Our primary\ncontribution is a broad empirical evaluation of train-before-test across 24\nbenchmarks and 61 models. We show that train-before-test significantly improves\nranking agreement consistently across all benchmarks. Whereas rankings have\nlittle external validity to start with, they enjoy a significant degree of\nexternal validity when applying train-before-test: Model rankings transfer\ngracefully from one benchmark to the other. Even within the same model family,\ntrain-before-test reduces strong ranking disagreement to near-perfect\nagreement. In addition, train-before-test reduces the model-score matrix to\nessentially rank one, revealing new insights into the latent factors of\nbenchmark performance. Our work supports the recommendation to make\ntrain-before-test a default component of LLM benchmarking.\n", "link": "http://arxiv.org/abs/2507.05195v1", "date": "2025-07-07", "relevancy": 2.1993, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4471}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4471}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Train-before-Test%20Harmonizes%20Language%20Model%20Rankings&body=Title%3A%20Train-before-Test%20Harmonizes%20Language%20Model%20Rankings%0AAuthor%3A%20Guanhua%20Zhang%20and%20Ricardo%20Dominguez-Olmedo%20and%20Moritz%20Hardt%0AAbstract%3A%20%20%20Existing%20language%20model%20benchmarks%20provide%20contradictory%20model%20rankings%2C%20even%0Afor%20benchmarks%20that%20aim%20to%20capture%20similar%20skills.%20This%20dilemma%20of%20conflicting%0Arankings%20hampers%20model%20selection%2C%20clouds%20model%20comparisons%2C%20and%20adds%20confusion%0Ato%20a%20growing%20ecosystem%20of%20competing%20models.%20Recent%20work%20attributed%20ranking%0Adisagreement%20to%20the%20phenomenon%20of%20training%20on%20the%20test%20task%3A%20As%20released%2C%0Adifferent%20models%20exhibit%20a%20different%20level%20of%20preparation%20for%20any%20given%20test%0Atask.%20A%20candidate%20solution%20to%20the%20problem%20is%20train-before-test%3A%20Give%20each%20model%0Athe%20same%20benchmark-specific%20finetuning%20before%20evaluation.%20Our%20primary%0Acontribution%20is%20a%20broad%20empirical%20evaluation%20of%20train-before-test%20across%2024%0Abenchmarks%20and%2061%20models.%20We%20show%20that%20train-before-test%20significantly%20improves%0Aranking%20agreement%20consistently%20across%20all%20benchmarks.%20Whereas%20rankings%20have%0Alittle%20external%20validity%20to%20start%20with%2C%20they%20enjoy%20a%20significant%20degree%20of%0Aexternal%20validity%20when%20applying%20train-before-test%3A%20Model%20rankings%20transfer%0Agracefully%20from%20one%20benchmark%20to%20the%20other.%20Even%20within%20the%20same%20model%20family%2C%0Atrain-before-test%20reduces%20strong%20ranking%20disagreement%20to%20near-perfect%0Aagreement.%20In%20addition%2C%20train-before-test%20reduces%20the%20model-score%20matrix%20to%0Aessentially%20rank%20one%2C%20revealing%20new%20insights%20into%20the%20latent%20factors%20of%0Abenchmark%20performance.%20Our%20work%20supports%20the%20recommendation%20to%20make%0Atrain-before-test%20a%20default%20component%20of%20LLM%20benchmarking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrain-before-Test%2520Harmonizes%2520Language%2520Model%2520Rankings%26entry.906535625%3DGuanhua%2520Zhang%2520and%2520Ricardo%2520Dominguez-Olmedo%2520and%2520Moritz%2520Hardt%26entry.1292438233%3D%2520%2520Existing%2520language%2520model%2520benchmarks%2520provide%2520contradictory%2520model%2520rankings%252C%2520even%250Afor%2520benchmarks%2520that%2520aim%2520to%2520capture%2520similar%2520skills.%2520This%2520dilemma%2520of%2520conflicting%250Arankings%2520hampers%2520model%2520selection%252C%2520clouds%2520model%2520comparisons%252C%2520and%2520adds%2520confusion%250Ato%2520a%2520growing%2520ecosystem%2520of%2520competing%2520models.%2520Recent%2520work%2520attributed%2520ranking%250Adisagreement%2520to%2520the%2520phenomenon%2520of%2520training%2520on%2520the%2520test%2520task%253A%2520As%2520released%252C%250Adifferent%2520models%2520exhibit%2520a%2520different%2520level%2520of%2520preparation%2520for%2520any%2520given%2520test%250Atask.%2520A%2520candidate%2520solution%2520to%2520the%2520problem%2520is%2520train-before-test%253A%2520Give%2520each%2520model%250Athe%2520same%2520benchmark-specific%2520finetuning%2520before%2520evaluation.%2520Our%2520primary%250Acontribution%2520is%2520a%2520broad%2520empirical%2520evaluation%2520of%2520train-before-test%2520across%252024%250Abenchmarks%2520and%252061%2520models.%2520We%2520show%2520that%2520train-before-test%2520significantly%2520improves%250Aranking%2520agreement%2520consistently%2520across%2520all%2520benchmarks.%2520Whereas%2520rankings%2520have%250Alittle%2520external%2520validity%2520to%2520start%2520with%252C%2520they%2520enjoy%2520a%2520significant%2520degree%2520of%250Aexternal%2520validity%2520when%2520applying%2520train-before-test%253A%2520Model%2520rankings%2520transfer%250Agracefully%2520from%2520one%2520benchmark%2520to%2520the%2520other.%2520Even%2520within%2520the%2520same%2520model%2520family%252C%250Atrain-before-test%2520reduces%2520strong%2520ranking%2520disagreement%2520to%2520near-perfect%250Aagreement.%2520In%2520addition%252C%2520train-before-test%2520reduces%2520the%2520model-score%2520matrix%2520to%250Aessentially%2520rank%2520one%252C%2520revealing%2520new%2520insights%2520into%2520the%2520latent%2520factors%2520of%250Abenchmark%2520performance.%2520Our%2520work%2520supports%2520the%2520recommendation%2520to%2520make%250Atrain-before-test%2520a%2520default%2520component%2520of%2520LLM%2520benchmarking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Train-before-Test%20Harmonizes%20Language%20Model%20Rankings&entry.906535625=Guanhua%20Zhang%20and%20Ricardo%20Dominguez-Olmedo%20and%20Moritz%20Hardt&entry.1292438233=%20%20Existing%20language%20model%20benchmarks%20provide%20contradictory%20model%20rankings%2C%20even%0Afor%20benchmarks%20that%20aim%20to%20capture%20similar%20skills.%20This%20dilemma%20of%20conflicting%0Arankings%20hampers%20model%20selection%2C%20clouds%20model%20comparisons%2C%20and%20adds%20confusion%0Ato%20a%20growing%20ecosystem%20of%20competing%20models.%20Recent%20work%20attributed%20ranking%0Adisagreement%20to%20the%20phenomenon%20of%20training%20on%20the%20test%20task%3A%20As%20released%2C%0Adifferent%20models%20exhibit%20a%20different%20level%20of%20preparation%20for%20any%20given%20test%0Atask.%20A%20candidate%20solution%20to%20the%20problem%20is%20train-before-test%3A%20Give%20each%20model%0Athe%20same%20benchmark-specific%20finetuning%20before%20evaluation.%20Our%20primary%0Acontribution%20is%20a%20broad%20empirical%20evaluation%20of%20train-before-test%20across%2024%0Abenchmarks%20and%2061%20models.%20We%20show%20that%20train-before-test%20significantly%20improves%0Aranking%20agreement%20consistently%20across%20all%20benchmarks.%20Whereas%20rankings%20have%0Alittle%20external%20validity%20to%20start%20with%2C%20they%20enjoy%20a%20significant%20degree%20of%0Aexternal%20validity%20when%20applying%20train-before-test%3A%20Model%20rankings%20transfer%0Agracefully%20from%20one%20benchmark%20to%20the%20other.%20Even%20within%20the%20same%20model%20family%2C%0Atrain-before-test%20reduces%20strong%20ranking%20disagreement%20to%20near-perfect%0Aagreement.%20In%20addition%2C%20train-before-test%20reduces%20the%20model-score%20matrix%20to%0Aessentially%20rank%20one%2C%20revealing%20new%20insights%20into%20the%20latent%20factors%20of%0Abenchmark%20performance.%20Our%20work%20supports%20the%20recommendation%20to%20make%0Atrain-before-test%20a%20default%20component%20of%20LLM%20benchmarking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05195v1&entry.124074799=Read"},
{"title": "LVM4CSI: Enabling Direct Application of Pre-Trained Large Vision Models\n  for Wireless Channel Tasks", "author": "Jiajia Guo and Peiwen Jiang and Chao-Kai Wen and Shi Jin and Jun Zhang", "abstract": "  Accurate channel state information (CSI) is critical to the performance of\nwireless communication systems, especially with the increasing scale and\ncomplexity introduced by 5G and future 6G technologies. While artificial\nintelligence (AI) offers a promising approach to CSI acquisition and\nutilization, existing methods largely depend on task-specific neural networks\n(NNs) that require expert-driven design and large training datasets, limiting\ntheir generalizability and practicality. To address these challenges, we\npropose LVM4CSI, a general and efficient framework that leverages the\nstructural similarity between CSI and computer vision (CV) data to directly\napply large vision models (LVMs) pre-trained on extensive CV datasets to\nwireless tasks without any fine-tuning, in contrast to large language\nmodel-based methods that generally necessitate fine-tuning. LVM4CSI maps CSI\ntasks to analogous CV tasks, transforms complex-valued CSI into visual formats\ncompatible with LVMs, and integrates lightweight trainable layers to adapt\nextracted features to specific communication objectives. We validate LVM4CSI\nthrough three representative case studies, including channel estimation, human\nactivity recognition, and user localization. Results demonstrate that LVM4CSI\nachieves comparable or superior performance to task-specific NNs, including an\nimprovement exceeding 9.61 dB in channel estimation and approximately 40%\nreduction in localization error. Furthermore, it significantly reduces the\nnumber of trainable parameters and eliminates the need for task-specific NN\ndesign.\n", "link": "http://arxiv.org/abs/2507.05121v1", "date": "2025-07-07", "relevancy": 2.1984, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5507}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5507}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LVM4CSI%3A%20Enabling%20Direct%20Application%20of%20Pre-Trained%20Large%20Vision%20Models%0A%20%20for%20Wireless%20Channel%20Tasks&body=Title%3A%20LVM4CSI%3A%20Enabling%20Direct%20Application%20of%20Pre-Trained%20Large%20Vision%20Models%0A%20%20for%20Wireless%20Channel%20Tasks%0AAuthor%3A%20Jiajia%20Guo%20and%20Peiwen%20Jiang%20and%20Chao-Kai%20Wen%20and%20Shi%20Jin%20and%20Jun%20Zhang%0AAbstract%3A%20%20%20Accurate%20channel%20state%20information%20%28CSI%29%20is%20critical%20to%20the%20performance%20of%0Awireless%20communication%20systems%2C%20especially%20with%20the%20increasing%20scale%20and%0Acomplexity%20introduced%20by%205G%20and%20future%206G%20technologies.%20While%20artificial%0Aintelligence%20%28AI%29%20offers%20a%20promising%20approach%20to%20CSI%20acquisition%20and%0Autilization%2C%20existing%20methods%20largely%20depend%20on%20task-specific%20neural%20networks%0A%28NNs%29%20that%20require%20expert-driven%20design%20and%20large%20training%20datasets%2C%20limiting%0Atheir%20generalizability%20and%20practicality.%20To%20address%20these%20challenges%2C%20we%0Apropose%20LVM4CSI%2C%20a%20general%20and%20efficient%20framework%20that%20leverages%20the%0Astructural%20similarity%20between%20CSI%20and%20computer%20vision%20%28CV%29%20data%20to%20directly%0Aapply%20large%20vision%20models%20%28LVMs%29%20pre-trained%20on%20extensive%20CV%20datasets%20to%0Awireless%20tasks%20without%20any%20fine-tuning%2C%20in%20contrast%20to%20large%20language%0Amodel-based%20methods%20that%20generally%20necessitate%20fine-tuning.%20LVM4CSI%20maps%20CSI%0Atasks%20to%20analogous%20CV%20tasks%2C%20transforms%20complex-valued%20CSI%20into%20visual%20formats%0Acompatible%20with%20LVMs%2C%20and%20integrates%20lightweight%20trainable%20layers%20to%20adapt%0Aextracted%20features%20to%20specific%20communication%20objectives.%20We%20validate%20LVM4CSI%0Athrough%20three%20representative%20case%20studies%2C%20including%20channel%20estimation%2C%20human%0Aactivity%20recognition%2C%20and%20user%20localization.%20Results%20demonstrate%20that%20LVM4CSI%0Aachieves%20comparable%20or%20superior%20performance%20to%20task-specific%20NNs%2C%20including%20an%0Aimprovement%20exceeding%209.61%20dB%20in%20channel%20estimation%20and%20approximately%2040%25%0Areduction%20in%20localization%20error.%20Furthermore%2C%20it%20significantly%20reduces%20the%0Anumber%20of%20trainable%20parameters%20and%20eliminates%20the%20need%20for%20task-specific%20NN%0Adesign.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05121v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLVM4CSI%253A%2520Enabling%2520Direct%2520Application%2520of%2520Pre-Trained%2520Large%2520Vision%2520Models%250A%2520%2520for%2520Wireless%2520Channel%2520Tasks%26entry.906535625%3DJiajia%2520Guo%2520and%2520Peiwen%2520Jiang%2520and%2520Chao-Kai%2520Wen%2520and%2520Shi%2520Jin%2520and%2520Jun%2520Zhang%26entry.1292438233%3D%2520%2520Accurate%2520channel%2520state%2520information%2520%2528CSI%2529%2520is%2520critical%2520to%2520the%2520performance%2520of%250Awireless%2520communication%2520systems%252C%2520especially%2520with%2520the%2520increasing%2520scale%2520and%250Acomplexity%2520introduced%2520by%25205G%2520and%2520future%25206G%2520technologies.%2520While%2520artificial%250Aintelligence%2520%2528AI%2529%2520offers%2520a%2520promising%2520approach%2520to%2520CSI%2520acquisition%2520and%250Autilization%252C%2520existing%2520methods%2520largely%2520depend%2520on%2520task-specific%2520neural%2520networks%250A%2528NNs%2529%2520that%2520require%2520expert-driven%2520design%2520and%2520large%2520training%2520datasets%252C%2520limiting%250Atheir%2520generalizability%2520and%2520practicality.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520LVM4CSI%252C%2520a%2520general%2520and%2520efficient%2520framework%2520that%2520leverages%2520the%250Astructural%2520similarity%2520between%2520CSI%2520and%2520computer%2520vision%2520%2528CV%2529%2520data%2520to%2520directly%250Aapply%2520large%2520vision%2520models%2520%2528LVMs%2529%2520pre-trained%2520on%2520extensive%2520CV%2520datasets%2520to%250Awireless%2520tasks%2520without%2520any%2520fine-tuning%252C%2520in%2520contrast%2520to%2520large%2520language%250Amodel-based%2520methods%2520that%2520generally%2520necessitate%2520fine-tuning.%2520LVM4CSI%2520maps%2520CSI%250Atasks%2520to%2520analogous%2520CV%2520tasks%252C%2520transforms%2520complex-valued%2520CSI%2520into%2520visual%2520formats%250Acompatible%2520with%2520LVMs%252C%2520and%2520integrates%2520lightweight%2520trainable%2520layers%2520to%2520adapt%250Aextracted%2520features%2520to%2520specific%2520communication%2520objectives.%2520We%2520validate%2520LVM4CSI%250Athrough%2520three%2520representative%2520case%2520studies%252C%2520including%2520channel%2520estimation%252C%2520human%250Aactivity%2520recognition%252C%2520and%2520user%2520localization.%2520Results%2520demonstrate%2520that%2520LVM4CSI%250Aachieves%2520comparable%2520or%2520superior%2520performance%2520to%2520task-specific%2520NNs%252C%2520including%2520an%250Aimprovement%2520exceeding%25209.61%2520dB%2520in%2520channel%2520estimation%2520and%2520approximately%252040%2525%250Areduction%2520in%2520localization%2520error.%2520Furthermore%252C%2520it%2520significantly%2520reduces%2520the%250Anumber%2520of%2520trainable%2520parameters%2520and%2520eliminates%2520the%2520need%2520for%2520task-specific%2520NN%250Adesign.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05121v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LVM4CSI%3A%20Enabling%20Direct%20Application%20of%20Pre-Trained%20Large%20Vision%20Models%0A%20%20for%20Wireless%20Channel%20Tasks&entry.906535625=Jiajia%20Guo%20and%20Peiwen%20Jiang%20and%20Chao-Kai%20Wen%20and%20Shi%20Jin%20and%20Jun%20Zhang&entry.1292438233=%20%20Accurate%20channel%20state%20information%20%28CSI%29%20is%20critical%20to%20the%20performance%20of%0Awireless%20communication%20systems%2C%20especially%20with%20the%20increasing%20scale%20and%0Acomplexity%20introduced%20by%205G%20and%20future%206G%20technologies.%20While%20artificial%0Aintelligence%20%28AI%29%20offers%20a%20promising%20approach%20to%20CSI%20acquisition%20and%0Autilization%2C%20existing%20methods%20largely%20depend%20on%20task-specific%20neural%20networks%0A%28NNs%29%20that%20require%20expert-driven%20design%20and%20large%20training%20datasets%2C%20limiting%0Atheir%20generalizability%20and%20practicality.%20To%20address%20these%20challenges%2C%20we%0Apropose%20LVM4CSI%2C%20a%20general%20and%20efficient%20framework%20that%20leverages%20the%0Astructural%20similarity%20between%20CSI%20and%20computer%20vision%20%28CV%29%20data%20to%20directly%0Aapply%20large%20vision%20models%20%28LVMs%29%20pre-trained%20on%20extensive%20CV%20datasets%20to%0Awireless%20tasks%20without%20any%20fine-tuning%2C%20in%20contrast%20to%20large%20language%0Amodel-based%20methods%20that%20generally%20necessitate%20fine-tuning.%20LVM4CSI%20maps%20CSI%0Atasks%20to%20analogous%20CV%20tasks%2C%20transforms%20complex-valued%20CSI%20into%20visual%20formats%0Acompatible%20with%20LVMs%2C%20and%20integrates%20lightweight%20trainable%20layers%20to%20adapt%0Aextracted%20features%20to%20specific%20communication%20objectives.%20We%20validate%20LVM4CSI%0Athrough%20three%20representative%20case%20studies%2C%20including%20channel%20estimation%2C%20human%0Aactivity%20recognition%2C%20and%20user%20localization.%20Results%20demonstrate%20that%20LVM4CSI%0Aachieves%20comparable%20or%20superior%20performance%20to%20task-specific%20NNs%2C%20including%20an%0Aimprovement%20exceeding%209.61%20dB%20in%20channel%20estimation%20and%20approximately%2040%25%0Areduction%20in%20localization%20error.%20Furthermore%2C%20it%20significantly%20reduces%20the%0Anumber%20of%20trainable%20parameters%20and%20eliminates%20the%20need%20for%20task-specific%20NN%0Adesign.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05121v1&entry.124074799=Read"},
{"title": "Random weights of DNNs and emergence of fixed points", "author": "L. Berlyand and O. Krupchytskyi and V. Slavin", "abstract": "  This paper is concerned with a special class of deep neural networks (DNNs)\nwhere the input and the output vectors have the same dimension. Such DNNs are\nwidely used in applications, e.g., autoencoders. The training of such networks\ncan be characterized by their fixed points (FPs). We are concerned with the\ndependence of the FPs number and their stability on the distribution of\nrandomly initialized DNNs' weight matrices. Specifically, we consider the\ni.i.d. random weights with heavy and light-tail distributions. Our objectives\nare twofold. First, the dependence of FPs number and stability of FPs on the\ntype of the distribution tail. Second, the dependence of the number of FPs on\nthe DNNs' architecture. We perform extensive simulations and show that for\nlight tails (e.g., Gaussian), which are typically used for initialization, a\nsingle stable FP exists for broad types of architectures. In contrast, for\nheavy tail distributions (e.g., Cauchy), which typically appear in trained\nDNNs, a number of FPs emerge. We further observe that these FPs are stable\nattractors and their basins of attraction partition the domain of input\nvectors. Finally, we observe an intriguing non-monotone dependence of the\nnumber of fixed points $Q(L)$ on the DNNs' depth $L$. The above results were\nfirst obtained for untrained DNNs with two types of distributions at\ninitialization and then verified by considering DNNs in which the heavy tail\ndistributions arise in training.\n", "link": "http://arxiv.org/abs/2501.04182v2", "date": "2025-07-07", "relevancy": 2.1968, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4652}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.437}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Random%20weights%20of%20DNNs%20and%20emergence%20of%20fixed%20points&body=Title%3A%20Random%20weights%20of%20DNNs%20and%20emergence%20of%20fixed%20points%0AAuthor%3A%20L.%20Berlyand%20and%20O.%20Krupchytskyi%20and%20V.%20Slavin%0AAbstract%3A%20%20%20This%20paper%20is%20concerned%20with%20a%20special%20class%20of%20deep%20neural%20networks%20%28DNNs%29%0Awhere%20the%20input%20and%20the%20output%20vectors%20have%20the%20same%20dimension.%20Such%20DNNs%20are%0Awidely%20used%20in%20applications%2C%20e.g.%2C%20autoencoders.%20The%20training%20of%20such%20networks%0Acan%20be%20characterized%20by%20their%20fixed%20points%20%28FPs%29.%20We%20are%20concerned%20with%20the%0Adependence%20of%20the%20FPs%20number%20and%20their%20stability%20on%20the%20distribution%20of%0Arandomly%20initialized%20DNNs%27%20weight%20matrices.%20Specifically%2C%20we%20consider%20the%0Ai.i.d.%20random%20weights%20with%20heavy%20and%20light-tail%20distributions.%20Our%20objectives%0Aare%20twofold.%20First%2C%20the%20dependence%20of%20FPs%20number%20and%20stability%20of%20FPs%20on%20the%0Atype%20of%20the%20distribution%20tail.%20Second%2C%20the%20dependence%20of%20the%20number%20of%20FPs%20on%0Athe%20DNNs%27%20architecture.%20We%20perform%20extensive%20simulations%20and%20show%20that%20for%0Alight%20tails%20%28e.g.%2C%20Gaussian%29%2C%20which%20are%20typically%20used%20for%20initialization%2C%20a%0Asingle%20stable%20FP%20exists%20for%20broad%20types%20of%20architectures.%20In%20contrast%2C%20for%0Aheavy%20tail%20distributions%20%28e.g.%2C%20Cauchy%29%2C%20which%20typically%20appear%20in%20trained%0ADNNs%2C%20a%20number%20of%20FPs%20emerge.%20We%20further%20observe%20that%20these%20FPs%20are%20stable%0Aattractors%20and%20their%20basins%20of%20attraction%20partition%20the%20domain%20of%20input%0Avectors.%20Finally%2C%20we%20observe%20an%20intriguing%20non-monotone%20dependence%20of%20the%0Anumber%20of%20fixed%20points%20%24Q%28L%29%24%20on%20the%20DNNs%27%20depth%20%24L%24.%20The%20above%20results%20were%0Afirst%20obtained%20for%20untrained%20DNNs%20with%20two%20types%20of%20distributions%20at%0Ainitialization%20and%20then%20verified%20by%20considering%20DNNs%20in%20which%20the%20heavy%20tail%0Adistributions%20arise%20in%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04182v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandom%2520weights%2520of%2520DNNs%2520and%2520emergence%2520of%2520fixed%2520points%26entry.906535625%3DL.%2520Berlyand%2520and%2520O.%2520Krupchytskyi%2520and%2520V.%2520Slavin%26entry.1292438233%3D%2520%2520This%2520paper%2520is%2520concerned%2520with%2520a%2520special%2520class%2520of%2520deep%2520neural%2520networks%2520%2528DNNs%2529%250Awhere%2520the%2520input%2520and%2520the%2520output%2520vectors%2520have%2520the%2520same%2520dimension.%2520Such%2520DNNs%2520are%250Awidely%2520used%2520in%2520applications%252C%2520e.g.%252C%2520autoencoders.%2520The%2520training%2520of%2520such%2520networks%250Acan%2520be%2520characterized%2520by%2520their%2520fixed%2520points%2520%2528FPs%2529.%2520We%2520are%2520concerned%2520with%2520the%250Adependence%2520of%2520the%2520FPs%2520number%2520and%2520their%2520stability%2520on%2520the%2520distribution%2520of%250Arandomly%2520initialized%2520DNNs%2527%2520weight%2520matrices.%2520Specifically%252C%2520we%2520consider%2520the%250Ai.i.d.%2520random%2520weights%2520with%2520heavy%2520and%2520light-tail%2520distributions.%2520Our%2520objectives%250Aare%2520twofold.%2520First%252C%2520the%2520dependence%2520of%2520FPs%2520number%2520and%2520stability%2520of%2520FPs%2520on%2520the%250Atype%2520of%2520the%2520distribution%2520tail.%2520Second%252C%2520the%2520dependence%2520of%2520the%2520number%2520of%2520FPs%2520on%250Athe%2520DNNs%2527%2520architecture.%2520We%2520perform%2520extensive%2520simulations%2520and%2520show%2520that%2520for%250Alight%2520tails%2520%2528e.g.%252C%2520Gaussian%2529%252C%2520which%2520are%2520typically%2520used%2520for%2520initialization%252C%2520a%250Asingle%2520stable%2520FP%2520exists%2520for%2520broad%2520types%2520of%2520architectures.%2520In%2520contrast%252C%2520for%250Aheavy%2520tail%2520distributions%2520%2528e.g.%252C%2520Cauchy%2529%252C%2520which%2520typically%2520appear%2520in%2520trained%250ADNNs%252C%2520a%2520number%2520of%2520FPs%2520emerge.%2520We%2520further%2520observe%2520that%2520these%2520FPs%2520are%2520stable%250Aattractors%2520and%2520their%2520basins%2520of%2520attraction%2520partition%2520the%2520domain%2520of%2520input%250Avectors.%2520Finally%252C%2520we%2520observe%2520an%2520intriguing%2520non-monotone%2520dependence%2520of%2520the%250Anumber%2520of%2520fixed%2520points%2520%2524Q%2528L%2529%2524%2520on%2520the%2520DNNs%2527%2520depth%2520%2524L%2524.%2520The%2520above%2520results%2520were%250Afirst%2520obtained%2520for%2520untrained%2520DNNs%2520with%2520two%2520types%2520of%2520distributions%2520at%250Ainitialization%2520and%2520then%2520verified%2520by%2520considering%2520DNNs%2520in%2520which%2520the%2520heavy%2520tail%250Adistributions%2520arise%2520in%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04182v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Random%20weights%20of%20DNNs%20and%20emergence%20of%20fixed%20points&entry.906535625=L.%20Berlyand%20and%20O.%20Krupchytskyi%20and%20V.%20Slavin&entry.1292438233=%20%20This%20paper%20is%20concerned%20with%20a%20special%20class%20of%20deep%20neural%20networks%20%28DNNs%29%0Awhere%20the%20input%20and%20the%20output%20vectors%20have%20the%20same%20dimension.%20Such%20DNNs%20are%0Awidely%20used%20in%20applications%2C%20e.g.%2C%20autoencoders.%20The%20training%20of%20such%20networks%0Acan%20be%20characterized%20by%20their%20fixed%20points%20%28FPs%29.%20We%20are%20concerned%20with%20the%0Adependence%20of%20the%20FPs%20number%20and%20their%20stability%20on%20the%20distribution%20of%0Arandomly%20initialized%20DNNs%27%20weight%20matrices.%20Specifically%2C%20we%20consider%20the%0Ai.i.d.%20random%20weights%20with%20heavy%20and%20light-tail%20distributions.%20Our%20objectives%0Aare%20twofold.%20First%2C%20the%20dependence%20of%20FPs%20number%20and%20stability%20of%20FPs%20on%20the%0Atype%20of%20the%20distribution%20tail.%20Second%2C%20the%20dependence%20of%20the%20number%20of%20FPs%20on%0Athe%20DNNs%27%20architecture.%20We%20perform%20extensive%20simulations%20and%20show%20that%20for%0Alight%20tails%20%28e.g.%2C%20Gaussian%29%2C%20which%20are%20typically%20used%20for%20initialization%2C%20a%0Asingle%20stable%20FP%20exists%20for%20broad%20types%20of%20architectures.%20In%20contrast%2C%20for%0Aheavy%20tail%20distributions%20%28e.g.%2C%20Cauchy%29%2C%20which%20typically%20appear%20in%20trained%0ADNNs%2C%20a%20number%20of%20FPs%20emerge.%20We%20further%20observe%20that%20these%20FPs%20are%20stable%0Aattractors%20and%20their%20basins%20of%20attraction%20partition%20the%20domain%20of%20input%0Avectors.%20Finally%2C%20we%20observe%20an%20intriguing%20non-monotone%20dependence%20of%20the%0Anumber%20of%20fixed%20points%20%24Q%28L%29%24%20on%20the%20DNNs%27%20depth%20%24L%24.%20The%20above%20results%20were%0Afirst%20obtained%20for%20untrained%20DNNs%20with%20two%20types%20of%20distributions%20at%0Ainitialization%20and%20then%20verified%20by%20considering%20DNNs%20in%20which%20the%20heavy%20tail%0Adistributions%20arise%20in%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04182v2&entry.124074799=Read"},
{"title": "ICAS: Detecting Training Data from Autoregressive Image Generative\n  Models", "author": "Hongyao Yu and Yixiang Qiu and Yiheng Yang and Hao Fang and Tianqu Zhuang and Jiaxin Hong and Bin Chen and Hao Wu and Shu-Tao Xia", "abstract": "  Autoregressive image generation has witnessed rapid advancements, with\nprominent models such as scale-wise visual auto-regression pushing the\nboundaries of visual synthesis. However, these developments also raise\nsignificant concerns regarding data privacy and copyright. In response,\ntraining data detection has emerged as a critical task for identifying\nunauthorized data usage in model training. To better understand the\nvulnerability of autoregressive image generative models to such detection, we\nconduct the first study applying membership inference to this domain. Our\napproach comprises two key components: implicit classification and an adaptive\nscore aggregation strategy. First, we compute the implicit token-wise\nclassification score within the query image. Then we propose an adaptive score\naggregation strategy to acquire a final score, which places greater emphasis on\nthe tokens with lower scores. A higher final score indicates that the sample is\nmore likely to be involved in the training set. To validate the effectiveness\nof our method, we adapt existing detection algorithms originally designed for\nLLMs to visual autoregressive models. Extensive experiments demonstrate the\nsuperiority of our method in both class-conditional and text-to-image\nscenarios. Moreover, our approach exhibits strong robustness and generalization\nunder various data transformations. Furthermore, sufficient experiments suggest\ntwo novel key findings: (1) A linear scaling law on membership inference,\nexposing the vulnerability of large foundation models. (2) Training data from\nscale-wise visual autoregressive models is easier to detect than other\nautoregressive paradigms.Our code is available at\nhttps://github.com/Chrisqcwx/ImageAR-MIA.\n", "link": "http://arxiv.org/abs/2507.05068v1", "date": "2025-07-07", "relevancy": 2.1954, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5651}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5395}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ICAS%3A%20Detecting%20Training%20Data%20from%20Autoregressive%20Image%20Generative%0A%20%20Models&body=Title%3A%20ICAS%3A%20Detecting%20Training%20Data%20from%20Autoregressive%20Image%20Generative%0A%20%20Models%0AAuthor%3A%20Hongyao%20Yu%20and%20Yixiang%20Qiu%20and%20Yiheng%20Yang%20and%20Hao%20Fang%20and%20Tianqu%20Zhuang%20and%20Jiaxin%20Hong%20and%20Bin%20Chen%20and%20Hao%20Wu%20and%20Shu-Tao%20Xia%0AAbstract%3A%20%20%20Autoregressive%20image%20generation%20has%20witnessed%20rapid%20advancements%2C%20with%0Aprominent%20models%20such%20as%20scale-wise%20visual%20auto-regression%20pushing%20the%0Aboundaries%20of%20visual%20synthesis.%20However%2C%20these%20developments%20also%20raise%0Asignificant%20concerns%20regarding%20data%20privacy%20and%20copyright.%20In%20response%2C%0Atraining%20data%20detection%20has%20emerged%20as%20a%20critical%20task%20for%20identifying%0Aunauthorized%20data%20usage%20in%20model%20training.%20To%20better%20understand%20the%0Avulnerability%20of%20autoregressive%20image%20generative%20models%20to%20such%20detection%2C%20we%0Aconduct%20the%20first%20study%20applying%20membership%20inference%20to%20this%20domain.%20Our%0Aapproach%20comprises%20two%20key%20components%3A%20implicit%20classification%20and%20an%20adaptive%0Ascore%20aggregation%20strategy.%20First%2C%20we%20compute%20the%20implicit%20token-wise%0Aclassification%20score%20within%20the%20query%20image.%20Then%20we%20propose%20an%20adaptive%20score%0Aaggregation%20strategy%20to%20acquire%20a%20final%20score%2C%20which%20places%20greater%20emphasis%20on%0Athe%20tokens%20with%20lower%20scores.%20A%20higher%20final%20score%20indicates%20that%20the%20sample%20is%0Amore%20likely%20to%20be%20involved%20in%20the%20training%20set.%20To%20validate%20the%20effectiveness%0Aof%20our%20method%2C%20we%20adapt%20existing%20detection%20algorithms%20originally%20designed%20for%0ALLMs%20to%20visual%20autoregressive%20models.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20method%20in%20both%20class-conditional%20and%20text-to-image%0Ascenarios.%20Moreover%2C%20our%20approach%20exhibits%20strong%20robustness%20and%20generalization%0Aunder%20various%20data%20transformations.%20Furthermore%2C%20sufficient%20experiments%20suggest%0Atwo%20novel%20key%20findings%3A%20%281%29%20A%20linear%20scaling%20law%20on%20membership%20inference%2C%0Aexposing%20the%20vulnerability%20of%20large%20foundation%20models.%20%282%29%20Training%20data%20from%0Ascale-wise%20visual%20autoregressive%20models%20is%20easier%20to%20detect%20than%20other%0Aautoregressive%20paradigms.Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Chrisqcwx/ImageAR-MIA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DICAS%253A%2520Detecting%2520Training%2520Data%2520from%2520Autoregressive%2520Image%2520Generative%250A%2520%2520Models%26entry.906535625%3DHongyao%2520Yu%2520and%2520Yixiang%2520Qiu%2520and%2520Yiheng%2520Yang%2520and%2520Hao%2520Fang%2520and%2520Tianqu%2520Zhuang%2520and%2520Jiaxin%2520Hong%2520and%2520Bin%2520Chen%2520and%2520Hao%2520Wu%2520and%2520Shu-Tao%2520Xia%26entry.1292438233%3D%2520%2520Autoregressive%2520image%2520generation%2520has%2520witnessed%2520rapid%2520advancements%252C%2520with%250Aprominent%2520models%2520such%2520as%2520scale-wise%2520visual%2520auto-regression%2520pushing%2520the%250Aboundaries%2520of%2520visual%2520synthesis.%2520However%252C%2520these%2520developments%2520also%2520raise%250Asignificant%2520concerns%2520regarding%2520data%2520privacy%2520and%2520copyright.%2520In%2520response%252C%250Atraining%2520data%2520detection%2520has%2520emerged%2520as%2520a%2520critical%2520task%2520for%2520identifying%250Aunauthorized%2520data%2520usage%2520in%2520model%2520training.%2520To%2520better%2520understand%2520the%250Avulnerability%2520of%2520autoregressive%2520image%2520generative%2520models%2520to%2520such%2520detection%252C%2520we%250Aconduct%2520the%2520first%2520study%2520applying%2520membership%2520inference%2520to%2520this%2520domain.%2520Our%250Aapproach%2520comprises%2520two%2520key%2520components%253A%2520implicit%2520classification%2520and%2520an%2520adaptive%250Ascore%2520aggregation%2520strategy.%2520First%252C%2520we%2520compute%2520the%2520implicit%2520token-wise%250Aclassification%2520score%2520within%2520the%2520query%2520image.%2520Then%2520we%2520propose%2520an%2520adaptive%2520score%250Aaggregation%2520strategy%2520to%2520acquire%2520a%2520final%2520score%252C%2520which%2520places%2520greater%2520emphasis%2520on%250Athe%2520tokens%2520with%2520lower%2520scores.%2520A%2520higher%2520final%2520score%2520indicates%2520that%2520the%2520sample%2520is%250Amore%2520likely%2520to%2520be%2520involved%2520in%2520the%2520training%2520set.%2520To%2520validate%2520the%2520effectiveness%250Aof%2520our%2520method%252C%2520we%2520adapt%2520existing%2520detection%2520algorithms%2520originally%2520designed%2520for%250ALLMs%2520to%2520visual%2520autoregressive%2520models.%2520Extensive%2520experiments%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520method%2520in%2520both%2520class-conditional%2520and%2520text-to-image%250Ascenarios.%2520Moreover%252C%2520our%2520approach%2520exhibits%2520strong%2520robustness%2520and%2520generalization%250Aunder%2520various%2520data%2520transformations.%2520Furthermore%252C%2520sufficient%2520experiments%2520suggest%250Atwo%2520novel%2520key%2520findings%253A%2520%25281%2529%2520A%2520linear%2520scaling%2520law%2520on%2520membership%2520inference%252C%250Aexposing%2520the%2520vulnerability%2520of%2520large%2520foundation%2520models.%2520%25282%2529%2520Training%2520data%2520from%250Ascale-wise%2520visual%2520autoregressive%2520models%2520is%2520easier%2520to%2520detect%2520than%2520other%250Aautoregressive%2520paradigms.Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Chrisqcwx/ImageAR-MIA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ICAS%3A%20Detecting%20Training%20Data%20from%20Autoregressive%20Image%20Generative%0A%20%20Models&entry.906535625=Hongyao%20Yu%20and%20Yixiang%20Qiu%20and%20Yiheng%20Yang%20and%20Hao%20Fang%20and%20Tianqu%20Zhuang%20and%20Jiaxin%20Hong%20and%20Bin%20Chen%20and%20Hao%20Wu%20and%20Shu-Tao%20Xia&entry.1292438233=%20%20Autoregressive%20image%20generation%20has%20witnessed%20rapid%20advancements%2C%20with%0Aprominent%20models%20such%20as%20scale-wise%20visual%20auto-regression%20pushing%20the%0Aboundaries%20of%20visual%20synthesis.%20However%2C%20these%20developments%20also%20raise%0Asignificant%20concerns%20regarding%20data%20privacy%20and%20copyright.%20In%20response%2C%0Atraining%20data%20detection%20has%20emerged%20as%20a%20critical%20task%20for%20identifying%0Aunauthorized%20data%20usage%20in%20model%20training.%20To%20better%20understand%20the%0Avulnerability%20of%20autoregressive%20image%20generative%20models%20to%20such%20detection%2C%20we%0Aconduct%20the%20first%20study%20applying%20membership%20inference%20to%20this%20domain.%20Our%0Aapproach%20comprises%20two%20key%20components%3A%20implicit%20classification%20and%20an%20adaptive%0Ascore%20aggregation%20strategy.%20First%2C%20we%20compute%20the%20implicit%20token-wise%0Aclassification%20score%20within%20the%20query%20image.%20Then%20we%20propose%20an%20adaptive%20score%0Aaggregation%20strategy%20to%20acquire%20a%20final%20score%2C%20which%20places%20greater%20emphasis%20on%0Athe%20tokens%20with%20lower%20scores.%20A%20higher%20final%20score%20indicates%20that%20the%20sample%20is%0Amore%20likely%20to%20be%20involved%20in%20the%20training%20set.%20To%20validate%20the%20effectiveness%0Aof%20our%20method%2C%20we%20adapt%20existing%20detection%20algorithms%20originally%20designed%20for%0ALLMs%20to%20visual%20autoregressive%20models.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20method%20in%20both%20class-conditional%20and%20text-to-image%0Ascenarios.%20Moreover%2C%20our%20approach%20exhibits%20strong%20robustness%20and%20generalization%0Aunder%20various%20data%20transformations.%20Furthermore%2C%20sufficient%20experiments%20suggest%0Atwo%20novel%20key%20findings%3A%20%281%29%20A%20linear%20scaling%20law%20on%20membership%20inference%2C%0Aexposing%20the%20vulnerability%20of%20large%20foundation%20models.%20%282%29%20Training%20data%20from%0Ascale-wise%20visual%20autoregressive%20models%20is%20easier%20to%20detect%20than%20other%0Aautoregressive%20paradigms.Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Chrisqcwx/ImageAR-MIA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05068v1&entry.124074799=Read"},
{"title": "SEE-2-SOUND: Zero-Shot Spatial Environment-to-Spatial Sound", "author": "Rishit Dagli and Shivesh Prakash and Robert Wu and Houman Khosravani", "abstract": "  Generating combined visual and auditory sensory experiences is critical for\nthe consumption of immersive content. Recent advances in neural generative\nmodels have enabled the creation of high-resolution content across multiple\nmodalities such as images, text, speech, and videos. Despite these successes,\nthere remains a significant gap in the generation of high-quality spatial audio\nthat complements generated visual content. Furthermore, current audio\ngeneration models excel in either generating natural audio or speech or music\nbut fall short in integrating spatial audio cues necessary for immersive\nexperiences. In this work, we introduce SEE-2-SOUND, a zero-shot approach that\ndecomposes the task into (1) identifying visual regions of interest; (2)\nlocating these elements in 3D space; (3) generating mono-audio for each; and\n(4) integrating them into spatial audio. Using our framework, we demonstrate\ncompelling results for generating spatial audio for high-quality videos,\nimages, and dynamic images from the internet, as well as media generated by\nlearned approaches.\n", "link": "http://arxiv.org/abs/2406.06612v2", "date": "2025-07-07", "relevancy": 2.1869, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEE-2-SOUND%3A%20Zero-Shot%20Spatial%20Environment-to-Spatial%20Sound&body=Title%3A%20SEE-2-SOUND%3A%20Zero-Shot%20Spatial%20Environment-to-Spatial%20Sound%0AAuthor%3A%20Rishit%20Dagli%20and%20Shivesh%20Prakash%20and%20Robert%20Wu%20and%20Houman%20Khosravani%0AAbstract%3A%20%20%20Generating%20combined%20visual%20and%20auditory%20sensory%20experiences%20is%20critical%20for%0Athe%20consumption%20of%20immersive%20content.%20Recent%20advances%20in%20neural%20generative%0Amodels%20have%20enabled%20the%20creation%20of%20high-resolution%20content%20across%20multiple%0Amodalities%20such%20as%20images%2C%20text%2C%20speech%2C%20and%20videos.%20Despite%20these%20successes%2C%0Athere%20remains%20a%20significant%20gap%20in%20the%20generation%20of%20high-quality%20spatial%20audio%0Athat%20complements%20generated%20visual%20content.%20Furthermore%2C%20current%20audio%0Ageneration%20models%20excel%20in%20either%20generating%20natural%20audio%20or%20speech%20or%20music%0Abut%20fall%20short%20in%20integrating%20spatial%20audio%20cues%20necessary%20for%20immersive%0Aexperiences.%20In%20this%20work%2C%20we%20introduce%20SEE-2-SOUND%2C%20a%20zero-shot%20approach%20that%0Adecomposes%20the%20task%20into%20%281%29%20identifying%20visual%20regions%20of%20interest%3B%20%282%29%0Alocating%20these%20elements%20in%203D%20space%3B%20%283%29%20generating%20mono-audio%20for%20each%3B%20and%0A%284%29%20integrating%20them%20into%20spatial%20audio.%20Using%20our%20framework%2C%20we%20demonstrate%0Acompelling%20results%20for%20generating%20spatial%20audio%20for%20high-quality%20videos%2C%0Aimages%2C%20and%20dynamic%20images%20from%20the%20internet%2C%20as%20well%20as%20media%20generated%20by%0Alearned%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06612v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEE-2-SOUND%253A%2520Zero-Shot%2520Spatial%2520Environment-to-Spatial%2520Sound%26entry.906535625%3DRishit%2520Dagli%2520and%2520Shivesh%2520Prakash%2520and%2520Robert%2520Wu%2520and%2520Houman%2520Khosravani%26entry.1292438233%3D%2520%2520Generating%2520combined%2520visual%2520and%2520auditory%2520sensory%2520experiences%2520is%2520critical%2520for%250Athe%2520consumption%2520of%2520immersive%2520content.%2520Recent%2520advances%2520in%2520neural%2520generative%250Amodels%2520have%2520enabled%2520the%2520creation%2520of%2520high-resolution%2520content%2520across%2520multiple%250Amodalities%2520such%2520as%2520images%252C%2520text%252C%2520speech%252C%2520and%2520videos.%2520Despite%2520these%2520successes%252C%250Athere%2520remains%2520a%2520significant%2520gap%2520in%2520the%2520generation%2520of%2520high-quality%2520spatial%2520audio%250Athat%2520complements%2520generated%2520visual%2520content.%2520Furthermore%252C%2520current%2520audio%250Ageneration%2520models%2520excel%2520in%2520either%2520generating%2520natural%2520audio%2520or%2520speech%2520or%2520music%250Abut%2520fall%2520short%2520in%2520integrating%2520spatial%2520audio%2520cues%2520necessary%2520for%2520immersive%250Aexperiences.%2520In%2520this%2520work%252C%2520we%2520introduce%2520SEE-2-SOUND%252C%2520a%2520zero-shot%2520approach%2520that%250Adecomposes%2520the%2520task%2520into%2520%25281%2529%2520identifying%2520visual%2520regions%2520of%2520interest%253B%2520%25282%2529%250Alocating%2520these%2520elements%2520in%25203D%2520space%253B%2520%25283%2529%2520generating%2520mono-audio%2520for%2520each%253B%2520and%250A%25284%2529%2520integrating%2520them%2520into%2520spatial%2520audio.%2520Using%2520our%2520framework%252C%2520we%2520demonstrate%250Acompelling%2520results%2520for%2520generating%2520spatial%2520audio%2520for%2520high-quality%2520videos%252C%250Aimages%252C%2520and%2520dynamic%2520images%2520from%2520the%2520internet%252C%2520as%2520well%2520as%2520media%2520generated%2520by%250Alearned%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06612v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEE-2-SOUND%3A%20Zero-Shot%20Spatial%20Environment-to-Spatial%20Sound&entry.906535625=Rishit%20Dagli%20and%20Shivesh%20Prakash%20and%20Robert%20Wu%20and%20Houman%20Khosravani&entry.1292438233=%20%20Generating%20combined%20visual%20and%20auditory%20sensory%20experiences%20is%20critical%20for%0Athe%20consumption%20of%20immersive%20content.%20Recent%20advances%20in%20neural%20generative%0Amodels%20have%20enabled%20the%20creation%20of%20high-resolution%20content%20across%20multiple%0Amodalities%20such%20as%20images%2C%20text%2C%20speech%2C%20and%20videos.%20Despite%20these%20successes%2C%0Athere%20remains%20a%20significant%20gap%20in%20the%20generation%20of%20high-quality%20spatial%20audio%0Athat%20complements%20generated%20visual%20content.%20Furthermore%2C%20current%20audio%0Ageneration%20models%20excel%20in%20either%20generating%20natural%20audio%20or%20speech%20or%20music%0Abut%20fall%20short%20in%20integrating%20spatial%20audio%20cues%20necessary%20for%20immersive%0Aexperiences.%20In%20this%20work%2C%20we%20introduce%20SEE-2-SOUND%2C%20a%20zero-shot%20approach%20that%0Adecomposes%20the%20task%20into%20%281%29%20identifying%20visual%20regions%20of%20interest%3B%20%282%29%0Alocating%20these%20elements%20in%203D%20space%3B%20%283%29%20generating%20mono-audio%20for%20each%3B%20and%0A%284%29%20integrating%20them%20into%20spatial%20audio.%20Using%20our%20framework%2C%20we%20demonstrate%0Acompelling%20results%20for%20generating%20spatial%20audio%20for%20high-quality%20videos%2C%0Aimages%2C%20and%20dynamic%20images%20from%20the%20internet%2C%20as%20well%20as%20media%20generated%20by%0Alearned%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06612v2&entry.124074799=Read"},
{"title": "LAID: Lightweight AI-Generated Image Detection in Spatial and Spectral\n  Domains", "author": "Nicholas Chivaran and Jianbing Ni", "abstract": "  The recent proliferation of photorealistic AI-generated images (AIGI) has\nraised urgent concerns about their potential misuse, particularly on social\nmedia platforms. Current state-of-the-art AIGI detection methods typically rely\non large, deep neural architectures, creating significant computational\nbarriers to real-time, large-scale deployment on platforms like social media.\nTo challenge this reliance on computationally intensive models, we introduce\nLAID, the first framework -- to our knowledge -- that benchmarks and evaluates\nthe detection performance and efficiency of off-the-shelf lightweight neural\nnetworks. In this framework, we comprehensively train and evaluate selected\nmodels on a representative subset of the GenImage dataset across spatial,\nspectral, and fusion image domains. Our results demonstrate that lightweight\nmodels can achieve competitive accuracy, even under adversarial conditions,\nwhile incurring substantially lower memory and computation costs compared to\ncurrent state-of-the-art methods. This study offers valuable insight into the\ntrade-off between efficiency and performance in AIGI detection and lays a\nfoundation for the development of practical, scalable, and trustworthy\ndetection systems. The source code of LAID can be found at:\nhttps://github.com/nchivar/LAID.\n", "link": "http://arxiv.org/abs/2507.05162v1", "date": "2025-07-07", "relevancy": 2.1811, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5628}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5448}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAID%3A%20Lightweight%20AI-Generated%20Image%20Detection%20in%20Spatial%20and%20Spectral%0A%20%20Domains&body=Title%3A%20LAID%3A%20Lightweight%20AI-Generated%20Image%20Detection%20in%20Spatial%20and%20Spectral%0A%20%20Domains%0AAuthor%3A%20Nicholas%20Chivaran%20and%20Jianbing%20Ni%0AAbstract%3A%20%20%20The%20recent%20proliferation%20of%20photorealistic%20AI-generated%20images%20%28AIGI%29%20has%0Araised%20urgent%20concerns%20about%20their%20potential%20misuse%2C%20particularly%20on%20social%0Amedia%20platforms.%20Current%20state-of-the-art%20AIGI%20detection%20methods%20typically%20rely%0Aon%20large%2C%20deep%20neural%20architectures%2C%20creating%20significant%20computational%0Abarriers%20to%20real-time%2C%20large-scale%20deployment%20on%20platforms%20like%20social%20media.%0ATo%20challenge%20this%20reliance%20on%20computationally%20intensive%20models%2C%20we%20introduce%0ALAID%2C%20the%20first%20framework%20--%20to%20our%20knowledge%20--%20that%20benchmarks%20and%20evaluates%0Athe%20detection%20performance%20and%20efficiency%20of%20off-the-shelf%20lightweight%20neural%0Anetworks.%20In%20this%20framework%2C%20we%20comprehensively%20train%20and%20evaluate%20selected%0Amodels%20on%20a%20representative%20subset%20of%20the%20GenImage%20dataset%20across%20spatial%2C%0Aspectral%2C%20and%20fusion%20image%20domains.%20Our%20results%20demonstrate%20that%20lightweight%0Amodels%20can%20achieve%20competitive%20accuracy%2C%20even%20under%20adversarial%20conditions%2C%0Awhile%20incurring%20substantially%20lower%20memory%20and%20computation%20costs%20compared%20to%0Acurrent%20state-of-the-art%20methods.%20This%20study%20offers%20valuable%20insight%20into%20the%0Atrade-off%20between%20efficiency%20and%20performance%20in%20AIGI%20detection%20and%20lays%20a%0Afoundation%20for%20the%20development%20of%20practical%2C%20scalable%2C%20and%20trustworthy%0Adetection%20systems.%20The%20source%20code%20of%20LAID%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/nchivar/LAID.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05162v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAID%253A%2520Lightweight%2520AI-Generated%2520Image%2520Detection%2520in%2520Spatial%2520and%2520Spectral%250A%2520%2520Domains%26entry.906535625%3DNicholas%2520Chivaran%2520and%2520Jianbing%2520Ni%26entry.1292438233%3D%2520%2520The%2520recent%2520proliferation%2520of%2520photorealistic%2520AI-generated%2520images%2520%2528AIGI%2529%2520has%250Araised%2520urgent%2520concerns%2520about%2520their%2520potential%2520misuse%252C%2520particularly%2520on%2520social%250Amedia%2520platforms.%2520Current%2520state-of-the-art%2520AIGI%2520detection%2520methods%2520typically%2520rely%250Aon%2520large%252C%2520deep%2520neural%2520architectures%252C%2520creating%2520significant%2520computational%250Abarriers%2520to%2520real-time%252C%2520large-scale%2520deployment%2520on%2520platforms%2520like%2520social%2520media.%250ATo%2520challenge%2520this%2520reliance%2520on%2520computationally%2520intensive%2520models%252C%2520we%2520introduce%250ALAID%252C%2520the%2520first%2520framework%2520--%2520to%2520our%2520knowledge%2520--%2520that%2520benchmarks%2520and%2520evaluates%250Athe%2520detection%2520performance%2520and%2520efficiency%2520of%2520off-the-shelf%2520lightweight%2520neural%250Anetworks.%2520In%2520this%2520framework%252C%2520we%2520comprehensively%2520train%2520and%2520evaluate%2520selected%250Amodels%2520on%2520a%2520representative%2520subset%2520of%2520the%2520GenImage%2520dataset%2520across%2520spatial%252C%250Aspectral%252C%2520and%2520fusion%2520image%2520domains.%2520Our%2520results%2520demonstrate%2520that%2520lightweight%250Amodels%2520can%2520achieve%2520competitive%2520accuracy%252C%2520even%2520under%2520adversarial%2520conditions%252C%250Awhile%2520incurring%2520substantially%2520lower%2520memory%2520and%2520computation%2520costs%2520compared%2520to%250Acurrent%2520state-of-the-art%2520methods.%2520This%2520study%2520offers%2520valuable%2520insight%2520into%2520the%250Atrade-off%2520between%2520efficiency%2520and%2520performance%2520in%2520AIGI%2520detection%2520and%2520lays%2520a%250Afoundation%2520for%2520the%2520development%2520of%2520practical%252C%2520scalable%252C%2520and%2520trustworthy%250Adetection%2520systems.%2520The%2520source%2520code%2520of%2520LAID%2520can%2520be%2520found%2520at%253A%250Ahttps%253A//github.com/nchivar/LAID.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05162v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAID%3A%20Lightweight%20AI-Generated%20Image%20Detection%20in%20Spatial%20and%20Spectral%0A%20%20Domains&entry.906535625=Nicholas%20Chivaran%20and%20Jianbing%20Ni&entry.1292438233=%20%20The%20recent%20proliferation%20of%20photorealistic%20AI-generated%20images%20%28AIGI%29%20has%0Araised%20urgent%20concerns%20about%20their%20potential%20misuse%2C%20particularly%20on%20social%0Amedia%20platforms.%20Current%20state-of-the-art%20AIGI%20detection%20methods%20typically%20rely%0Aon%20large%2C%20deep%20neural%20architectures%2C%20creating%20significant%20computational%0Abarriers%20to%20real-time%2C%20large-scale%20deployment%20on%20platforms%20like%20social%20media.%0ATo%20challenge%20this%20reliance%20on%20computationally%20intensive%20models%2C%20we%20introduce%0ALAID%2C%20the%20first%20framework%20--%20to%20our%20knowledge%20--%20that%20benchmarks%20and%20evaluates%0Athe%20detection%20performance%20and%20efficiency%20of%20off-the-shelf%20lightweight%20neural%0Anetworks.%20In%20this%20framework%2C%20we%20comprehensively%20train%20and%20evaluate%20selected%0Amodels%20on%20a%20representative%20subset%20of%20the%20GenImage%20dataset%20across%20spatial%2C%0Aspectral%2C%20and%20fusion%20image%20domains.%20Our%20results%20demonstrate%20that%20lightweight%0Amodels%20can%20achieve%20competitive%20accuracy%2C%20even%20under%20adversarial%20conditions%2C%0Awhile%20incurring%20substantially%20lower%20memory%20and%20computation%20costs%20compared%20to%0Acurrent%20state-of-the-art%20methods.%20This%20study%20offers%20valuable%20insight%20into%20the%0Atrade-off%20between%20efficiency%20and%20performance%20in%20AIGI%20detection%20and%20lays%20a%0Afoundation%20for%20the%20development%20of%20practical%2C%20scalable%2C%20and%20trustworthy%0Adetection%20systems.%20The%20source%20code%20of%20LAID%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/nchivar/LAID.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05162v1&entry.124074799=Read"},
{"title": "Boosting Temporal Sentence Grounding via Causal Inference", "author": "Kefan Tang and Lihuo He and Jisheng Dang and Xinbo Gao", "abstract": "  Temporal Sentence Grounding (TSG) aims to identify relevant moments in an\nuntrimmed video that semantically correspond to a given textual query. Despite\nexisting studies having made substantial progress, they often overlook the\nissue of spurious correlations between video and textual queries. These\nspurious correlations arise from two primary factors: (1) inherent biases in\nthe textual data, such as frequent co-occurrences of specific verbs or phrases,\nand (2) the model's tendency to overfit to salient or repetitive patterns in\nvideo content. Such biases mislead the model into associating textual cues with\nincorrect visual moments, resulting in unreliable predictions and poor\ngeneralization to out-of-distribution examples. To overcome these limitations,\nwe propose a novel TSG framework, causal intervention and counterfactual\nreasoning that utilizes causal inference to eliminate spurious correlations and\nenhance the model's robustness. Specifically, we first formulate the TSG task\nfrom a causal perspective with a structural causal model. Then, to address\nunobserved confounders reflecting textual biases toward specific verbs or\nphrases, a textual causal intervention is proposed, utilizing do-calculus to\nestimate the causal effects. Furthermore, visual counterfactual reasoning is\nperformed by constructing a counterfactual scenario that focuses solely on\nvideo features, excluding the query and fused multi-modal features. This allows\nus to debias the model by isolating and removing the influence of the video\nfrom the overall effect. Experiments on public datasets demonstrate the\nsuperiority of the proposed method. The code is available at\nhttps://github.com/Tangkfan/CICR.\n", "link": "http://arxiv.org/abs/2507.04958v1", "date": "2025-07-07", "relevancy": 2.1794, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5502}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5474}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Temporal%20Sentence%20Grounding%20via%20Causal%20Inference&body=Title%3A%20Boosting%20Temporal%20Sentence%20Grounding%20via%20Causal%20Inference%0AAuthor%3A%20Kefan%20Tang%20and%20Lihuo%20He%20and%20Jisheng%20Dang%20and%20Xinbo%20Gao%0AAbstract%3A%20%20%20Temporal%20Sentence%20Grounding%20%28TSG%29%20aims%20to%20identify%20relevant%20moments%20in%20an%0Auntrimmed%20video%20that%20semantically%20correspond%20to%20a%20given%20textual%20query.%20Despite%0Aexisting%20studies%20having%20made%20substantial%20progress%2C%20they%20often%20overlook%20the%0Aissue%20of%20spurious%20correlations%20between%20video%20and%20textual%20queries.%20These%0Aspurious%20correlations%20arise%20from%20two%20primary%20factors%3A%20%281%29%20inherent%20biases%20in%0Athe%20textual%20data%2C%20such%20as%20frequent%20co-occurrences%20of%20specific%20verbs%20or%20phrases%2C%0Aand%20%282%29%20the%20model%27s%20tendency%20to%20overfit%20to%20salient%20or%20repetitive%20patterns%20in%0Avideo%20content.%20Such%20biases%20mislead%20the%20model%20into%20associating%20textual%20cues%20with%0Aincorrect%20visual%20moments%2C%20resulting%20in%20unreliable%20predictions%20and%20poor%0Ageneralization%20to%20out-of-distribution%20examples.%20To%20overcome%20these%20limitations%2C%0Awe%20propose%20a%20novel%20TSG%20framework%2C%20causal%20intervention%20and%20counterfactual%0Areasoning%20that%20utilizes%20causal%20inference%20to%20eliminate%20spurious%20correlations%20and%0Aenhance%20the%20model%27s%20robustness.%20Specifically%2C%20we%20first%20formulate%20the%20TSG%20task%0Afrom%20a%20causal%20perspective%20with%20a%20structural%20causal%20model.%20Then%2C%20to%20address%0Aunobserved%20confounders%20reflecting%20textual%20biases%20toward%20specific%20verbs%20or%0Aphrases%2C%20a%20textual%20causal%20intervention%20is%20proposed%2C%20utilizing%20do-calculus%20to%0Aestimate%20the%20causal%20effects.%20Furthermore%2C%20visual%20counterfactual%20reasoning%20is%0Aperformed%20by%20constructing%20a%20counterfactual%20scenario%20that%20focuses%20solely%20on%0Avideo%20features%2C%20excluding%20the%20query%20and%20fused%20multi-modal%20features.%20This%20allows%0Aus%20to%20debias%20the%20model%20by%20isolating%20and%20removing%20the%20influence%20of%20the%20video%0Afrom%20the%20overall%20effect.%20Experiments%20on%20public%20datasets%20demonstrate%20the%0Asuperiority%20of%20the%20proposed%20method.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Tangkfan/CICR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Temporal%2520Sentence%2520Grounding%2520via%2520Causal%2520Inference%26entry.906535625%3DKefan%2520Tang%2520and%2520Lihuo%2520He%2520and%2520Jisheng%2520Dang%2520and%2520Xinbo%2520Gao%26entry.1292438233%3D%2520%2520Temporal%2520Sentence%2520Grounding%2520%2528TSG%2529%2520aims%2520to%2520identify%2520relevant%2520moments%2520in%2520an%250Auntrimmed%2520video%2520that%2520semantically%2520correspond%2520to%2520a%2520given%2520textual%2520query.%2520Despite%250Aexisting%2520studies%2520having%2520made%2520substantial%2520progress%252C%2520they%2520often%2520overlook%2520the%250Aissue%2520of%2520spurious%2520correlations%2520between%2520video%2520and%2520textual%2520queries.%2520These%250Aspurious%2520correlations%2520arise%2520from%2520two%2520primary%2520factors%253A%2520%25281%2529%2520inherent%2520biases%2520in%250Athe%2520textual%2520data%252C%2520such%2520as%2520frequent%2520co-occurrences%2520of%2520specific%2520verbs%2520or%2520phrases%252C%250Aand%2520%25282%2529%2520the%2520model%2527s%2520tendency%2520to%2520overfit%2520to%2520salient%2520or%2520repetitive%2520patterns%2520in%250Avideo%2520content.%2520Such%2520biases%2520mislead%2520the%2520model%2520into%2520associating%2520textual%2520cues%2520with%250Aincorrect%2520visual%2520moments%252C%2520resulting%2520in%2520unreliable%2520predictions%2520and%2520poor%250Ageneralization%2520to%2520out-of-distribution%2520examples.%2520To%2520overcome%2520these%2520limitations%252C%250Awe%2520propose%2520a%2520novel%2520TSG%2520framework%252C%2520causal%2520intervention%2520and%2520counterfactual%250Areasoning%2520that%2520utilizes%2520causal%2520inference%2520to%2520eliminate%2520spurious%2520correlations%2520and%250Aenhance%2520the%2520model%2527s%2520robustness.%2520Specifically%252C%2520we%2520first%2520formulate%2520the%2520TSG%2520task%250Afrom%2520a%2520causal%2520perspective%2520with%2520a%2520structural%2520causal%2520model.%2520Then%252C%2520to%2520address%250Aunobserved%2520confounders%2520reflecting%2520textual%2520biases%2520toward%2520specific%2520verbs%2520or%250Aphrases%252C%2520a%2520textual%2520causal%2520intervention%2520is%2520proposed%252C%2520utilizing%2520do-calculus%2520to%250Aestimate%2520the%2520causal%2520effects.%2520Furthermore%252C%2520visual%2520counterfactual%2520reasoning%2520is%250Aperformed%2520by%2520constructing%2520a%2520counterfactual%2520scenario%2520that%2520focuses%2520solely%2520on%250Avideo%2520features%252C%2520excluding%2520the%2520query%2520and%2520fused%2520multi-modal%2520features.%2520This%2520allows%250Aus%2520to%2520debias%2520the%2520model%2520by%2520isolating%2520and%2520removing%2520the%2520influence%2520of%2520the%2520video%250Afrom%2520the%2520overall%2520effect.%2520Experiments%2520on%2520public%2520datasets%2520demonstrate%2520the%250Asuperiority%2520of%2520the%2520proposed%2520method.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Tangkfan/CICR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Temporal%20Sentence%20Grounding%20via%20Causal%20Inference&entry.906535625=Kefan%20Tang%20and%20Lihuo%20He%20and%20Jisheng%20Dang%20and%20Xinbo%20Gao&entry.1292438233=%20%20Temporal%20Sentence%20Grounding%20%28TSG%29%20aims%20to%20identify%20relevant%20moments%20in%20an%0Auntrimmed%20video%20that%20semantically%20correspond%20to%20a%20given%20textual%20query.%20Despite%0Aexisting%20studies%20having%20made%20substantial%20progress%2C%20they%20often%20overlook%20the%0Aissue%20of%20spurious%20correlations%20between%20video%20and%20textual%20queries.%20These%0Aspurious%20correlations%20arise%20from%20two%20primary%20factors%3A%20%281%29%20inherent%20biases%20in%0Athe%20textual%20data%2C%20such%20as%20frequent%20co-occurrences%20of%20specific%20verbs%20or%20phrases%2C%0Aand%20%282%29%20the%20model%27s%20tendency%20to%20overfit%20to%20salient%20or%20repetitive%20patterns%20in%0Avideo%20content.%20Such%20biases%20mislead%20the%20model%20into%20associating%20textual%20cues%20with%0Aincorrect%20visual%20moments%2C%20resulting%20in%20unreliable%20predictions%20and%20poor%0Ageneralization%20to%20out-of-distribution%20examples.%20To%20overcome%20these%20limitations%2C%0Awe%20propose%20a%20novel%20TSG%20framework%2C%20causal%20intervention%20and%20counterfactual%0Areasoning%20that%20utilizes%20causal%20inference%20to%20eliminate%20spurious%20correlations%20and%0Aenhance%20the%20model%27s%20robustness.%20Specifically%2C%20we%20first%20formulate%20the%20TSG%20task%0Afrom%20a%20causal%20perspective%20with%20a%20structural%20causal%20model.%20Then%2C%20to%20address%0Aunobserved%20confounders%20reflecting%20textual%20biases%20toward%20specific%20verbs%20or%0Aphrases%2C%20a%20textual%20causal%20intervention%20is%20proposed%2C%20utilizing%20do-calculus%20to%0Aestimate%20the%20causal%20effects.%20Furthermore%2C%20visual%20counterfactual%20reasoning%20is%0Aperformed%20by%20constructing%20a%20counterfactual%20scenario%20that%20focuses%20solely%20on%0Avideo%20features%2C%20excluding%20the%20query%20and%20fused%20multi-modal%20features.%20This%20allows%0Aus%20to%20debias%20the%20model%20by%20isolating%20and%20removing%20the%20influence%20of%20the%20video%0Afrom%20the%20overall%20effect.%20Experiments%20on%20public%20datasets%20demonstrate%20the%0Asuperiority%20of%20the%20proposed%20method.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Tangkfan/CICR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04958v1&entry.124074799=Read"},
{"title": "CTA: Cross-Task Alignment for Better Test Time Training", "author": "Samuel Barbeau and Pedram Fekri and David Osowiechi and Ali Bahri and Moslem YazdanpanahMasih Aminbeidokhti and Christian Desrosiers", "abstract": "  Deep learning models have demonstrated exceptional performance across a wide\nrange of computer vision tasks. However, their performance often degrades\nsignificantly when faced with distribution shifts, such as domain or dataset\nchanges. Test-Time Training (TTT) has emerged as an effective method to enhance\nmodel robustness by incorporating an auxiliary unsupervised task during\ntraining and leveraging it for model updates at test time. In this work, we\nintroduce CTA (Cross-Task Alignment), a novel approach for improving TTT.\nUnlike existing TTT methods, CTA does not require a specialized model\narchitecture and instead takes inspiration from the success of multi-modal\ncontrastive learning to align a supervised encoder with a self-supervised one.\nThis process enforces alignment between the learned representations of both\nmodels, thereby mitigating the risk of gradient interference, preserving the\nintrinsic robustness of self-supervised learning and enabling more semantically\nmeaningful updates at test-time. Experimental results demonstrate substantial\nimprovements in robustness and generalization over the state-of-the-art on\nseveral benchmark datasets.\n", "link": "http://arxiv.org/abs/2507.05221v1", "date": "2025-07-07", "relevancy": 2.1767, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5804}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5194}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CTA%3A%20Cross-Task%20Alignment%20for%20Better%20Test%20Time%20Training&body=Title%3A%20CTA%3A%20Cross-Task%20Alignment%20for%20Better%20Test%20Time%20Training%0AAuthor%3A%20Samuel%20Barbeau%20and%20Pedram%20Fekri%20and%20David%20Osowiechi%20and%20Ali%20Bahri%20and%20Moslem%20YazdanpanahMasih%20Aminbeidokhti%20and%20Christian%20Desrosiers%0AAbstract%3A%20%20%20Deep%20learning%20models%20have%20demonstrated%20exceptional%20performance%20across%20a%20wide%0Arange%20of%20computer%20vision%20tasks.%20However%2C%20their%20performance%20often%20degrades%0Asignificantly%20when%20faced%20with%20distribution%20shifts%2C%20such%20as%20domain%20or%20dataset%0Achanges.%20Test-Time%20Training%20%28TTT%29%20has%20emerged%20as%20an%20effective%20method%20to%20enhance%0Amodel%20robustness%20by%20incorporating%20an%20auxiliary%20unsupervised%20task%20during%0Atraining%20and%20leveraging%20it%20for%20model%20updates%20at%20test%20time.%20In%20this%20work%2C%20we%0Aintroduce%20CTA%20%28Cross-Task%20Alignment%29%2C%20a%20novel%20approach%20for%20improving%20TTT.%0AUnlike%20existing%20TTT%20methods%2C%20CTA%20does%20not%20require%20a%20specialized%20model%0Aarchitecture%20and%20instead%20takes%20inspiration%20from%20the%20success%20of%20multi-modal%0Acontrastive%20learning%20to%20align%20a%20supervised%20encoder%20with%20a%20self-supervised%20one.%0AThis%20process%20enforces%20alignment%20between%20the%20learned%20representations%20of%20both%0Amodels%2C%20thereby%20mitigating%20the%20risk%20of%20gradient%20interference%2C%20preserving%20the%0Aintrinsic%20robustness%20of%20self-supervised%20learning%20and%20enabling%20more%20semantically%0Ameaningful%20updates%20at%20test-time.%20Experimental%20results%20demonstrate%20substantial%0Aimprovements%20in%20robustness%20and%20generalization%20over%20the%20state-of-the-art%20on%0Aseveral%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCTA%253A%2520Cross-Task%2520Alignment%2520for%2520Better%2520Test%2520Time%2520Training%26entry.906535625%3DSamuel%2520Barbeau%2520and%2520Pedram%2520Fekri%2520and%2520David%2520Osowiechi%2520and%2520Ali%2520Bahri%2520and%2520Moslem%2520YazdanpanahMasih%2520Aminbeidokhti%2520and%2520Christian%2520Desrosiers%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520have%2520demonstrated%2520exceptional%2520performance%2520across%2520a%2520wide%250Arange%2520of%2520computer%2520vision%2520tasks.%2520However%252C%2520their%2520performance%2520often%2520degrades%250Asignificantly%2520when%2520faced%2520with%2520distribution%2520shifts%252C%2520such%2520as%2520domain%2520or%2520dataset%250Achanges.%2520Test-Time%2520Training%2520%2528TTT%2529%2520has%2520emerged%2520as%2520an%2520effective%2520method%2520to%2520enhance%250Amodel%2520robustness%2520by%2520incorporating%2520an%2520auxiliary%2520unsupervised%2520task%2520during%250Atraining%2520and%2520leveraging%2520it%2520for%2520model%2520updates%2520at%2520test%2520time.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520CTA%2520%2528Cross-Task%2520Alignment%2529%252C%2520a%2520novel%2520approach%2520for%2520improving%2520TTT.%250AUnlike%2520existing%2520TTT%2520methods%252C%2520CTA%2520does%2520not%2520require%2520a%2520specialized%2520model%250Aarchitecture%2520and%2520instead%2520takes%2520inspiration%2520from%2520the%2520success%2520of%2520multi-modal%250Acontrastive%2520learning%2520to%2520align%2520a%2520supervised%2520encoder%2520with%2520a%2520self-supervised%2520one.%250AThis%2520process%2520enforces%2520alignment%2520between%2520the%2520learned%2520representations%2520of%2520both%250Amodels%252C%2520thereby%2520mitigating%2520the%2520risk%2520of%2520gradient%2520interference%252C%2520preserving%2520the%250Aintrinsic%2520robustness%2520of%2520self-supervised%2520learning%2520and%2520enabling%2520more%2520semantically%250Ameaningful%2520updates%2520at%2520test-time.%2520Experimental%2520results%2520demonstrate%2520substantial%250Aimprovements%2520in%2520robustness%2520and%2520generalization%2520over%2520the%2520state-of-the-art%2520on%250Aseveral%2520benchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CTA%3A%20Cross-Task%20Alignment%20for%20Better%20Test%20Time%20Training&entry.906535625=Samuel%20Barbeau%20and%20Pedram%20Fekri%20and%20David%20Osowiechi%20and%20Ali%20Bahri%20and%20Moslem%20YazdanpanahMasih%20Aminbeidokhti%20and%20Christian%20Desrosiers&entry.1292438233=%20%20Deep%20learning%20models%20have%20demonstrated%20exceptional%20performance%20across%20a%20wide%0Arange%20of%20computer%20vision%20tasks.%20However%2C%20their%20performance%20often%20degrades%0Asignificantly%20when%20faced%20with%20distribution%20shifts%2C%20such%20as%20domain%20or%20dataset%0Achanges.%20Test-Time%20Training%20%28TTT%29%20has%20emerged%20as%20an%20effective%20method%20to%20enhance%0Amodel%20robustness%20by%20incorporating%20an%20auxiliary%20unsupervised%20task%20during%0Atraining%20and%20leveraging%20it%20for%20model%20updates%20at%20test%20time.%20In%20this%20work%2C%20we%0Aintroduce%20CTA%20%28Cross-Task%20Alignment%29%2C%20a%20novel%20approach%20for%20improving%20TTT.%0AUnlike%20existing%20TTT%20methods%2C%20CTA%20does%20not%20require%20a%20specialized%20model%0Aarchitecture%20and%20instead%20takes%20inspiration%20from%20the%20success%20of%20multi-modal%0Acontrastive%20learning%20to%20align%20a%20supervised%20encoder%20with%20a%20self-supervised%20one.%0AThis%20process%20enforces%20alignment%20between%20the%20learned%20representations%20of%20both%0Amodels%2C%20thereby%20mitigating%20the%20risk%20of%20gradient%20interference%2C%20preserving%20the%0Aintrinsic%20robustness%20of%20self-supervised%20learning%20and%20enabling%20more%20semantically%0Ameaningful%20updates%20at%20test-time.%20Experimental%20results%20demonstrate%20substantial%0Aimprovements%20in%20robustness%20and%20generalization%20over%20the%20state-of-the-art%20on%0Aseveral%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05221v1&entry.124074799=Read"},
{"title": "Open-Set Gait Recognition from Sparse mmWave Radar Point Clouds", "author": "Riccardo Mazzieri and Jacopo Pegoraro and Michele Rossi", "abstract": "  The adoption of Millimeter-Wave (mmWave) radar devices for human sensing,\nparticularly gait recognition, has recently gathered significant attention due\nto their efficiency, resilience to environmental conditions, and\nprivacy-preserving nature. In this work, we tackle the challenging problem of\nOpen-set Gait Recognition (OSGR) from sparse mmWave radar point clouds. Unlike\nmost existing research, which assumes a closed-set scenario, our work considers\nthe more realistic open-set case, where unknown subjects might be present at\ninference time, and should be correctly recognized by the system. Point clouds\nare well-suited for edge computing applications with resource constraints, but\nare more significantly affected by noise and random fluctuations than other\nrepresentations, like the more common micro-Doppler signature. This is the\nfirst work addressing open-set gait recognition with sparse point cloud data.\nTo do so, we propose a novel neural network architecture that combines\nsupervised classification with unsupervised reconstruction of the point clouds,\ncreating a robust, rich, and highly regularized latent space of gait features.\nTo detect unknown subjects at inference time, we introduce a probabilistic\nnovelty detection algorithm that leverages the structured latent space and\noffers a tunable trade-off between inference speed and prediction accuracy.\nAlong with this paper, we release mmGait10, an original human gait dataset\nfeaturing over five hours of measurements from ten subjects, under varied\nwalking modalities. Extensive experimental results show that our solution\nattains F1-Score improvements by 24% over state-of-the-art methods adapted for\npoint clouds, on average, and across multiple openness levels.\n", "link": "http://arxiv.org/abs/2503.07435v4", "date": "2025-07-07", "relevancy": 2.173, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5697}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5495}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-Set%20Gait%20Recognition%20from%20Sparse%20mmWave%20Radar%20Point%20Clouds&body=Title%3A%20Open-Set%20Gait%20Recognition%20from%20Sparse%20mmWave%20Radar%20Point%20Clouds%0AAuthor%3A%20Riccardo%20Mazzieri%20and%20Jacopo%20Pegoraro%20and%20Michele%20Rossi%0AAbstract%3A%20%20%20The%20adoption%20of%20Millimeter-Wave%20%28mmWave%29%20radar%20devices%20for%20human%20sensing%2C%0Aparticularly%20gait%20recognition%2C%20has%20recently%20gathered%20significant%20attention%20due%0Ato%20their%20efficiency%2C%20resilience%20to%20environmental%20conditions%2C%20and%0Aprivacy-preserving%20nature.%20In%20this%20work%2C%20we%20tackle%20the%20challenging%20problem%20of%0AOpen-set%20Gait%20Recognition%20%28OSGR%29%20from%20sparse%20mmWave%20radar%20point%20clouds.%20Unlike%0Amost%20existing%20research%2C%20which%20assumes%20a%20closed-set%20scenario%2C%20our%20work%20considers%0Athe%20more%20realistic%20open-set%20case%2C%20where%20unknown%20subjects%20might%20be%20present%20at%0Ainference%20time%2C%20and%20should%20be%20correctly%20recognized%20by%20the%20system.%20Point%20clouds%0Aare%20well-suited%20for%20edge%20computing%20applications%20with%20resource%20constraints%2C%20but%0Aare%20more%20significantly%20affected%20by%20noise%20and%20random%20fluctuations%20than%20other%0Arepresentations%2C%20like%20the%20more%20common%20micro-Doppler%20signature.%20This%20is%20the%0Afirst%20work%20addressing%20open-set%20gait%20recognition%20with%20sparse%20point%20cloud%20data.%0ATo%20do%20so%2C%20we%20propose%20a%20novel%20neural%20network%20architecture%20that%20combines%0Asupervised%20classification%20with%20unsupervised%20reconstruction%20of%20the%20point%20clouds%2C%0Acreating%20a%20robust%2C%20rich%2C%20and%20highly%20regularized%20latent%20space%20of%20gait%20features.%0ATo%20detect%20unknown%20subjects%20at%20inference%20time%2C%20we%20introduce%20a%20probabilistic%0Anovelty%20detection%20algorithm%20that%20leverages%20the%20structured%20latent%20space%20and%0Aoffers%20a%20tunable%20trade-off%20between%20inference%20speed%20and%20prediction%20accuracy.%0AAlong%20with%20this%20paper%2C%20we%20release%20mmGait10%2C%20an%20original%20human%20gait%20dataset%0Afeaturing%20over%20five%20hours%20of%20measurements%20from%20ten%20subjects%2C%20under%20varied%0Awalking%20modalities.%20Extensive%20experimental%20results%20show%20that%20our%20solution%0Aattains%20F1-Score%20improvements%20by%2024%25%20over%20state-of-the-art%20methods%20adapted%20for%0Apoint%20clouds%2C%20on%20average%2C%20and%20across%20multiple%20openness%20levels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.07435v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-Set%2520Gait%2520Recognition%2520from%2520Sparse%2520mmWave%2520Radar%2520Point%2520Clouds%26entry.906535625%3DRiccardo%2520Mazzieri%2520and%2520Jacopo%2520Pegoraro%2520and%2520Michele%2520Rossi%26entry.1292438233%3D%2520%2520The%2520adoption%2520of%2520Millimeter-Wave%2520%2528mmWave%2529%2520radar%2520devices%2520for%2520human%2520sensing%252C%250Aparticularly%2520gait%2520recognition%252C%2520has%2520recently%2520gathered%2520significant%2520attention%2520due%250Ato%2520their%2520efficiency%252C%2520resilience%2520to%2520environmental%2520conditions%252C%2520and%250Aprivacy-preserving%2520nature.%2520In%2520this%2520work%252C%2520we%2520tackle%2520the%2520challenging%2520problem%2520of%250AOpen-set%2520Gait%2520Recognition%2520%2528OSGR%2529%2520from%2520sparse%2520mmWave%2520radar%2520point%2520clouds.%2520Unlike%250Amost%2520existing%2520research%252C%2520which%2520assumes%2520a%2520closed-set%2520scenario%252C%2520our%2520work%2520considers%250Athe%2520more%2520realistic%2520open-set%2520case%252C%2520where%2520unknown%2520subjects%2520might%2520be%2520present%2520at%250Ainference%2520time%252C%2520and%2520should%2520be%2520correctly%2520recognized%2520by%2520the%2520system.%2520Point%2520clouds%250Aare%2520well-suited%2520for%2520edge%2520computing%2520applications%2520with%2520resource%2520constraints%252C%2520but%250Aare%2520more%2520significantly%2520affected%2520by%2520noise%2520and%2520random%2520fluctuations%2520than%2520other%250Arepresentations%252C%2520like%2520the%2520more%2520common%2520micro-Doppler%2520signature.%2520This%2520is%2520the%250Afirst%2520work%2520addressing%2520open-set%2520gait%2520recognition%2520with%2520sparse%2520point%2520cloud%2520data.%250ATo%2520do%2520so%252C%2520we%2520propose%2520a%2520novel%2520neural%2520network%2520architecture%2520that%2520combines%250Asupervised%2520classification%2520with%2520unsupervised%2520reconstruction%2520of%2520the%2520point%2520clouds%252C%250Acreating%2520a%2520robust%252C%2520rich%252C%2520and%2520highly%2520regularized%2520latent%2520space%2520of%2520gait%2520features.%250ATo%2520detect%2520unknown%2520subjects%2520at%2520inference%2520time%252C%2520we%2520introduce%2520a%2520probabilistic%250Anovelty%2520detection%2520algorithm%2520that%2520leverages%2520the%2520structured%2520latent%2520space%2520and%250Aoffers%2520a%2520tunable%2520trade-off%2520between%2520inference%2520speed%2520and%2520prediction%2520accuracy.%250AAlong%2520with%2520this%2520paper%252C%2520we%2520release%2520mmGait10%252C%2520an%2520original%2520human%2520gait%2520dataset%250Afeaturing%2520over%2520five%2520hours%2520of%2520measurements%2520from%2520ten%2520subjects%252C%2520under%2520varied%250Awalking%2520modalities.%2520Extensive%2520experimental%2520results%2520show%2520that%2520our%2520solution%250Aattains%2520F1-Score%2520improvements%2520by%252024%2525%2520over%2520state-of-the-art%2520methods%2520adapted%2520for%250Apoint%2520clouds%252C%2520on%2520average%252C%2520and%2520across%2520multiple%2520openness%2520levels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07435v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Set%20Gait%20Recognition%20from%20Sparse%20mmWave%20Radar%20Point%20Clouds&entry.906535625=Riccardo%20Mazzieri%20and%20Jacopo%20Pegoraro%20and%20Michele%20Rossi&entry.1292438233=%20%20The%20adoption%20of%20Millimeter-Wave%20%28mmWave%29%20radar%20devices%20for%20human%20sensing%2C%0Aparticularly%20gait%20recognition%2C%20has%20recently%20gathered%20significant%20attention%20due%0Ato%20their%20efficiency%2C%20resilience%20to%20environmental%20conditions%2C%20and%0Aprivacy-preserving%20nature.%20In%20this%20work%2C%20we%20tackle%20the%20challenging%20problem%20of%0AOpen-set%20Gait%20Recognition%20%28OSGR%29%20from%20sparse%20mmWave%20radar%20point%20clouds.%20Unlike%0Amost%20existing%20research%2C%20which%20assumes%20a%20closed-set%20scenario%2C%20our%20work%20considers%0Athe%20more%20realistic%20open-set%20case%2C%20where%20unknown%20subjects%20might%20be%20present%20at%0Ainference%20time%2C%20and%20should%20be%20correctly%20recognized%20by%20the%20system.%20Point%20clouds%0Aare%20well-suited%20for%20edge%20computing%20applications%20with%20resource%20constraints%2C%20but%0Aare%20more%20significantly%20affected%20by%20noise%20and%20random%20fluctuations%20than%20other%0Arepresentations%2C%20like%20the%20more%20common%20micro-Doppler%20signature.%20This%20is%20the%0Afirst%20work%20addressing%20open-set%20gait%20recognition%20with%20sparse%20point%20cloud%20data.%0ATo%20do%20so%2C%20we%20propose%20a%20novel%20neural%20network%20architecture%20that%20combines%0Asupervised%20classification%20with%20unsupervised%20reconstruction%20of%20the%20point%20clouds%2C%0Acreating%20a%20robust%2C%20rich%2C%20and%20highly%20regularized%20latent%20space%20of%20gait%20features.%0ATo%20detect%20unknown%20subjects%20at%20inference%20time%2C%20we%20introduce%20a%20probabilistic%0Anovelty%20detection%20algorithm%20that%20leverages%20the%20structured%20latent%20space%20and%0Aoffers%20a%20tunable%20trade-off%20between%20inference%20speed%20and%20prediction%20accuracy.%0AAlong%20with%20this%20paper%2C%20we%20release%20mmGait10%2C%20an%20original%20human%20gait%20dataset%0Afeaturing%20over%20five%20hours%20of%20measurements%20from%20ten%20subjects%2C%20under%20varied%0Awalking%20modalities.%20Extensive%20experimental%20results%20show%20that%20our%20solution%0Aattains%20F1-Score%20improvements%20by%2024%25%20over%20state-of-the-art%20methods%20adapted%20for%0Apoint%20clouds%2C%20on%20average%2C%20and%20across%20multiple%20openness%20levels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.07435v4&entry.124074799=Read"},
{"title": "Relative Overfitting and Accept-Reject Framework", "author": "Yanxin Liu and Yunqi Zhang", "abstract": "  The scaling of Large Language Models (LLMs) currently faces significant\nchallenges. Model assembly is widely considered a promising solution to break\nthrough these performance bottlenecks. However, current ensembling methods are\nprimarily guided by the statistical expectation that combining multiple models\nover large samples will lead to performance gains. We propose an ensemble\nframework that transitions from such stochastic, sample-dependent methods to a\nregular, controllable approach based on fine-grained model segmentation. This\nregularity governs how models are segmented to ensure performance improvement,\nhow the magnitude of this improvement varies with model selection, and what\nfactors determine its theoretical maximum. To formalize this pattern, we\nintroduce the concept of'relative overfitting,' which is derived from the\nperformance discrepancies between constituent models and builds a bridge\nbetween ensemble outcomes and the inherent attributes of these models. We\ndetail the patterns of this framework within the domain of NLP and briefly\ndescribe its extensibility to other fields, such as computer vision (CV) and AI\nfor science. Our approach was validated using both custom-built and pre-trained\nmainstream models across diverse benchmarks, including language modeling,\nlong-context tasks, and question-answering (QA). The results indicate that the\nensemble rules we proposed are generally effective and that we provide a\nrigorous proof of these rules in certain experimental scenarios. The proposed\nframework offers a new perspective for understanding ensemble theory and\nprovides a systematic approach to addressing the performance bottlenecks of\nLLMs.\n", "link": "http://arxiv.org/abs/2505.07783v4", "date": "2025-07-07", "relevancy": 2.1704, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5462}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5462}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relative%20Overfitting%20and%20Accept-Reject%20Framework&body=Title%3A%20Relative%20Overfitting%20and%20Accept-Reject%20Framework%0AAuthor%3A%20Yanxin%20Liu%20and%20Yunqi%20Zhang%0AAbstract%3A%20%20%20The%20scaling%20of%20Large%20Language%20Models%20%28LLMs%29%20currently%20faces%20significant%0Achallenges.%20Model%20assembly%20is%20widely%20considered%20a%20promising%20solution%20to%20break%0Athrough%20these%20performance%20bottlenecks.%20However%2C%20current%20ensembling%20methods%20are%0Aprimarily%20guided%20by%20the%20statistical%20expectation%20that%20combining%20multiple%20models%0Aover%20large%20samples%20will%20lead%20to%20performance%20gains.%20We%20propose%20an%20ensemble%0Aframework%20that%20transitions%20from%20such%20stochastic%2C%20sample-dependent%20methods%20to%20a%0Aregular%2C%20controllable%20approach%20based%20on%20fine-grained%20model%20segmentation.%20This%0Aregularity%20governs%20how%20models%20are%20segmented%20to%20ensure%20performance%20improvement%2C%0Ahow%20the%20magnitude%20of%20this%20improvement%20varies%20with%20model%20selection%2C%20and%20what%0Afactors%20determine%20its%20theoretical%20maximum.%20To%20formalize%20this%20pattern%2C%20we%0Aintroduce%20the%20concept%20of%27relative%20overfitting%2C%27%20which%20is%20derived%20from%20the%0Aperformance%20discrepancies%20between%20constituent%20models%20and%20builds%20a%20bridge%0Abetween%20ensemble%20outcomes%20and%20the%20inherent%20attributes%20of%20these%20models.%20We%0Adetail%20the%20patterns%20of%20this%20framework%20within%20the%20domain%20of%20NLP%20and%20briefly%0Adescribe%20its%20extensibility%20to%20other%20fields%2C%20such%20as%20computer%20vision%20%28CV%29%20and%20AI%0Afor%20science.%20Our%20approach%20was%20validated%20using%20both%20custom-built%20and%20pre-trained%0Amainstream%20models%20across%20diverse%20benchmarks%2C%20including%20language%20modeling%2C%0Along-context%20tasks%2C%20and%20question-answering%20%28QA%29.%20The%20results%20indicate%20that%20the%0Aensemble%20rules%20we%20proposed%20are%20generally%20effective%20and%20that%20we%20provide%20a%0Arigorous%20proof%20of%20these%20rules%20in%20certain%20experimental%20scenarios.%20The%20proposed%0Aframework%20offers%20a%20new%20perspective%20for%20understanding%20ensemble%20theory%20and%0Aprovides%20a%20systematic%20approach%20to%20addressing%20the%20performance%20bottlenecks%20of%0ALLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07783v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelative%2520Overfitting%2520and%2520Accept-Reject%2520Framework%26entry.906535625%3DYanxin%2520Liu%2520and%2520Yunqi%2520Zhang%26entry.1292438233%3D%2520%2520The%2520scaling%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520currently%2520faces%2520significant%250Achallenges.%2520Model%2520assembly%2520is%2520widely%2520considered%2520a%2520promising%2520solution%2520to%2520break%250Athrough%2520these%2520performance%2520bottlenecks.%2520However%252C%2520current%2520ensembling%2520methods%2520are%250Aprimarily%2520guided%2520by%2520the%2520statistical%2520expectation%2520that%2520combining%2520multiple%2520models%250Aover%2520large%2520samples%2520will%2520lead%2520to%2520performance%2520gains.%2520We%2520propose%2520an%2520ensemble%250Aframework%2520that%2520transitions%2520from%2520such%2520stochastic%252C%2520sample-dependent%2520methods%2520to%2520a%250Aregular%252C%2520controllable%2520approach%2520based%2520on%2520fine-grained%2520model%2520segmentation.%2520This%250Aregularity%2520governs%2520how%2520models%2520are%2520segmented%2520to%2520ensure%2520performance%2520improvement%252C%250Ahow%2520the%2520magnitude%2520of%2520this%2520improvement%2520varies%2520with%2520model%2520selection%252C%2520and%2520what%250Afactors%2520determine%2520its%2520theoretical%2520maximum.%2520To%2520formalize%2520this%2520pattern%252C%2520we%250Aintroduce%2520the%2520concept%2520of%2527relative%2520overfitting%252C%2527%2520which%2520is%2520derived%2520from%2520the%250Aperformance%2520discrepancies%2520between%2520constituent%2520models%2520and%2520builds%2520a%2520bridge%250Abetween%2520ensemble%2520outcomes%2520and%2520the%2520inherent%2520attributes%2520of%2520these%2520models.%2520We%250Adetail%2520the%2520patterns%2520of%2520this%2520framework%2520within%2520the%2520domain%2520of%2520NLP%2520and%2520briefly%250Adescribe%2520its%2520extensibility%2520to%2520other%2520fields%252C%2520such%2520as%2520computer%2520vision%2520%2528CV%2529%2520and%2520AI%250Afor%2520science.%2520Our%2520approach%2520was%2520validated%2520using%2520both%2520custom-built%2520and%2520pre-trained%250Amainstream%2520models%2520across%2520diverse%2520benchmarks%252C%2520including%2520language%2520modeling%252C%250Along-context%2520tasks%252C%2520and%2520question-answering%2520%2528QA%2529.%2520The%2520results%2520indicate%2520that%2520the%250Aensemble%2520rules%2520we%2520proposed%2520are%2520generally%2520effective%2520and%2520that%2520we%2520provide%2520a%250Arigorous%2520proof%2520of%2520these%2520rules%2520in%2520certain%2520experimental%2520scenarios.%2520The%2520proposed%250Aframework%2520offers%2520a%2520new%2520perspective%2520for%2520understanding%2520ensemble%2520theory%2520and%250Aprovides%2520a%2520systematic%2520approach%2520to%2520addressing%2520the%2520performance%2520bottlenecks%2520of%250ALLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07783v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relative%20Overfitting%20and%20Accept-Reject%20Framework&entry.906535625=Yanxin%20Liu%20and%20Yunqi%20Zhang&entry.1292438233=%20%20The%20scaling%20of%20Large%20Language%20Models%20%28LLMs%29%20currently%20faces%20significant%0Achallenges.%20Model%20assembly%20is%20widely%20considered%20a%20promising%20solution%20to%20break%0Athrough%20these%20performance%20bottlenecks.%20However%2C%20current%20ensembling%20methods%20are%0Aprimarily%20guided%20by%20the%20statistical%20expectation%20that%20combining%20multiple%20models%0Aover%20large%20samples%20will%20lead%20to%20performance%20gains.%20We%20propose%20an%20ensemble%0Aframework%20that%20transitions%20from%20such%20stochastic%2C%20sample-dependent%20methods%20to%20a%0Aregular%2C%20controllable%20approach%20based%20on%20fine-grained%20model%20segmentation.%20This%0Aregularity%20governs%20how%20models%20are%20segmented%20to%20ensure%20performance%20improvement%2C%0Ahow%20the%20magnitude%20of%20this%20improvement%20varies%20with%20model%20selection%2C%20and%20what%0Afactors%20determine%20its%20theoretical%20maximum.%20To%20formalize%20this%20pattern%2C%20we%0Aintroduce%20the%20concept%20of%27relative%20overfitting%2C%27%20which%20is%20derived%20from%20the%0Aperformance%20discrepancies%20between%20constituent%20models%20and%20builds%20a%20bridge%0Abetween%20ensemble%20outcomes%20and%20the%20inherent%20attributes%20of%20these%20models.%20We%0Adetail%20the%20patterns%20of%20this%20framework%20within%20the%20domain%20of%20NLP%20and%20briefly%0Adescribe%20its%20extensibility%20to%20other%20fields%2C%20such%20as%20computer%20vision%20%28CV%29%20and%20AI%0Afor%20science.%20Our%20approach%20was%20validated%20using%20both%20custom-built%20and%20pre-trained%0Amainstream%20models%20across%20diverse%20benchmarks%2C%20including%20language%20modeling%2C%0Along-context%20tasks%2C%20and%20question-answering%20%28QA%29.%20The%20results%20indicate%20that%20the%0Aensemble%20rules%20we%20proposed%20are%20generally%20effective%20and%20that%20we%20provide%20a%0Arigorous%20proof%20of%20these%20rules%20in%20certain%20experimental%20scenarios.%20The%20proposed%0Aframework%20offers%20a%20new%20perspective%20for%20understanding%20ensemble%20theory%20and%0Aprovides%20a%20systematic%20approach%20to%20addressing%20the%20performance%20bottlenecks%20of%0ALLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07783v4&entry.124074799=Read"},
{"title": "UNSURF: Uncertainty Quantification for Cortical Surface Reconstruction\n  of Clinical Brain MRIs", "author": "Raghav Mehta and Karthik Gopinath and Ben Glocker and Juan Eugenio Iglesias", "abstract": "  We propose UNSURF, a novel uncertainty measure for cortical surface\nreconstruction of clinical brain MRI scans of any orientation, resolution, and\ncontrast. It relies on the discrepancy between predicted voxel-wise signed\ndistance functions (SDFs) and the actual SDFs of the fitted surfaces. Our\nexperiments on real clinical scans show that traditional uncertainty measures,\nsuch as voxel-wise Monte Carlo variance, are not suitable for modeling the\nuncertainty of surface placement. Our results demonstrate that UNSURF estimates\ncorrelate well with the ground truth errors and: \\textit{(i)}~enable effective\nautomated quality control of surface reconstructions at the subject-, parcel-,\nmesh node-level; and \\textit{(ii)}~improve performance on a downstream\nAlzheimer's disease classification task.\n", "link": "http://arxiv.org/abs/2506.00498v2", "date": "2025-07-07", "relevancy": 2.1465, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5748}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5425}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UNSURF%3A%20Uncertainty%20Quantification%20for%20Cortical%20Surface%20Reconstruction%0A%20%20of%20Clinical%20Brain%20MRIs&body=Title%3A%20UNSURF%3A%20Uncertainty%20Quantification%20for%20Cortical%20Surface%20Reconstruction%0A%20%20of%20Clinical%20Brain%20MRIs%0AAuthor%3A%20Raghav%20Mehta%20and%20Karthik%20Gopinath%20and%20Ben%20Glocker%20and%20Juan%20Eugenio%20Iglesias%0AAbstract%3A%20%20%20We%20propose%20UNSURF%2C%20a%20novel%20uncertainty%20measure%20for%20cortical%20surface%0Areconstruction%20of%20clinical%20brain%20MRI%20scans%20of%20any%20orientation%2C%20resolution%2C%20and%0Acontrast.%20It%20relies%20on%20the%20discrepancy%20between%20predicted%20voxel-wise%20signed%0Adistance%20functions%20%28SDFs%29%20and%20the%20actual%20SDFs%20of%20the%20fitted%20surfaces.%20Our%0Aexperiments%20on%20real%20clinical%20scans%20show%20that%20traditional%20uncertainty%20measures%2C%0Asuch%20as%20voxel-wise%20Monte%20Carlo%20variance%2C%20are%20not%20suitable%20for%20modeling%20the%0Auncertainty%20of%20surface%20placement.%20Our%20results%20demonstrate%20that%20UNSURF%20estimates%0Acorrelate%20well%20with%20the%20ground%20truth%20errors%20and%3A%20%5Ctextit%7B%28i%29%7D~enable%20effective%0Aautomated%20quality%20control%20of%20surface%20reconstructions%20at%20the%20subject-%2C%20parcel-%2C%0Amesh%20node-level%3B%20and%20%5Ctextit%7B%28ii%29%7D~improve%20performance%20on%20a%20downstream%0AAlzheimer%27s%20disease%20classification%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00498v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUNSURF%253A%2520Uncertainty%2520Quantification%2520for%2520Cortical%2520Surface%2520Reconstruction%250A%2520%2520of%2520Clinical%2520Brain%2520MRIs%26entry.906535625%3DRaghav%2520Mehta%2520and%2520Karthik%2520Gopinath%2520and%2520Ben%2520Glocker%2520and%2520Juan%2520Eugenio%2520Iglesias%26entry.1292438233%3D%2520%2520We%2520propose%2520UNSURF%252C%2520a%2520novel%2520uncertainty%2520measure%2520for%2520cortical%2520surface%250Areconstruction%2520of%2520clinical%2520brain%2520MRI%2520scans%2520of%2520any%2520orientation%252C%2520resolution%252C%2520and%250Acontrast.%2520It%2520relies%2520on%2520the%2520discrepancy%2520between%2520predicted%2520voxel-wise%2520signed%250Adistance%2520functions%2520%2528SDFs%2529%2520and%2520the%2520actual%2520SDFs%2520of%2520the%2520fitted%2520surfaces.%2520Our%250Aexperiments%2520on%2520real%2520clinical%2520scans%2520show%2520that%2520traditional%2520uncertainty%2520measures%252C%250Asuch%2520as%2520voxel-wise%2520Monte%2520Carlo%2520variance%252C%2520are%2520not%2520suitable%2520for%2520modeling%2520the%250Auncertainty%2520of%2520surface%2520placement.%2520Our%2520results%2520demonstrate%2520that%2520UNSURF%2520estimates%250Acorrelate%2520well%2520with%2520the%2520ground%2520truth%2520errors%2520and%253A%2520%255Ctextit%257B%2528i%2529%257D~enable%2520effective%250Aautomated%2520quality%2520control%2520of%2520surface%2520reconstructions%2520at%2520the%2520subject-%252C%2520parcel-%252C%250Amesh%2520node-level%253B%2520and%2520%255Ctextit%257B%2528ii%2529%257D~improve%2520performance%2520on%2520a%2520downstream%250AAlzheimer%2527s%2520disease%2520classification%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00498v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UNSURF%3A%20Uncertainty%20Quantification%20for%20Cortical%20Surface%20Reconstruction%0A%20%20of%20Clinical%20Brain%20MRIs&entry.906535625=Raghav%20Mehta%20and%20Karthik%20Gopinath%20and%20Ben%20Glocker%20and%20Juan%20Eugenio%20Iglesias&entry.1292438233=%20%20We%20propose%20UNSURF%2C%20a%20novel%20uncertainty%20measure%20for%20cortical%20surface%0Areconstruction%20of%20clinical%20brain%20MRI%20scans%20of%20any%20orientation%2C%20resolution%2C%20and%0Acontrast.%20It%20relies%20on%20the%20discrepancy%20between%20predicted%20voxel-wise%20signed%0Adistance%20functions%20%28SDFs%29%20and%20the%20actual%20SDFs%20of%20the%20fitted%20surfaces.%20Our%0Aexperiments%20on%20real%20clinical%20scans%20show%20that%20traditional%20uncertainty%20measures%2C%0Asuch%20as%20voxel-wise%20Monte%20Carlo%20variance%2C%20are%20not%20suitable%20for%20modeling%20the%0Auncertainty%20of%20surface%20placement.%20Our%20results%20demonstrate%20that%20UNSURF%20estimates%0Acorrelate%20well%20with%20the%20ground%20truth%20errors%20and%3A%20%5Ctextit%7B%28i%29%7D~enable%20effective%0Aautomated%20quality%20control%20of%20surface%20reconstructions%20at%20the%20subject-%2C%20parcel-%2C%0Amesh%20node-level%3B%20and%20%5Ctextit%7B%28ii%29%7D~improve%20performance%20on%20a%20downstream%0AAlzheimer%27s%20disease%20classification%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00498v2&entry.124074799=Read"},
{"title": "A Dynamical Systems Perspective on the Analysis of Neural Networks", "author": "Dennis Chemnitz and Maximilian Engel and Christian Kuehn and Sara-Viola Kuntz", "abstract": "  In this chapter, we utilize dynamical systems to analyze several aspects of\nmachine learning algorithms. As an expository contribution we demonstrate how\nto re-formulate a wide variety of challenges from deep neural networks,\n(stochastic) gradient descent, and related topics into dynamical statements. We\nalso tackle three concrete challenges. First, we consider the process of\ninformation propagation through a neural network, i.e., we study the\ninput-output map for different architectures. We explain the universal\nembedding property for augmented neural ODEs representing arbitrary functions\nof given regularity, the classification of multilayer perceptrons and neural\nODEs in terms of suitable function classes, and the memory-dependence in neural\ndelay equations. Second, we consider the training aspect of neural networks\ndynamically. We describe a dynamical systems perspective on gradient descent\nand study stability for overdetermined problems. We then extend this analysis\nto the overparameterized setting and describe the edge of stability phenomenon,\nalso in the context of possible explanations for implicit bias. For stochastic\ngradient descent, we present stability results for the overparameterized\nsetting via Lyapunov exponents of interpolation solutions. Third, we explain\nseveral results regarding mean-field limits of neural networks. We describe a\nresult that extends existing techniques to heterogeneous neural networks\ninvolving graph limits via digraph measures. This shows how large classes of\nneural networks naturally fall within the framework of Kuramoto-type models on\ngraphs and their large-graph limits. Finally, we point out that similar\nstrategies to use dynamics to study explainable and reliable AI can also be\napplied to settings such as generative models or fundamental issues in gradient\ntraining methods, such as backpropagation or vanishing/exploding gradients.\n", "link": "http://arxiv.org/abs/2507.05164v1", "date": "2025-07-07", "relevancy": 2.1222, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.573}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5096}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Dynamical%20Systems%20Perspective%20on%20the%20Analysis%20of%20Neural%20Networks&body=Title%3A%20A%20Dynamical%20Systems%20Perspective%20on%20the%20Analysis%20of%20Neural%20Networks%0AAuthor%3A%20Dennis%20Chemnitz%20and%20Maximilian%20Engel%20and%20Christian%20Kuehn%20and%20Sara-Viola%20Kuntz%0AAbstract%3A%20%20%20In%20this%20chapter%2C%20we%20utilize%20dynamical%20systems%20to%20analyze%20several%20aspects%20of%0Amachine%20learning%20algorithms.%20As%20an%20expository%20contribution%20we%20demonstrate%20how%0Ato%20re-formulate%20a%20wide%20variety%20of%20challenges%20from%20deep%20neural%20networks%2C%0A%28stochastic%29%20gradient%20descent%2C%20and%20related%20topics%20into%20dynamical%20statements.%20We%0Aalso%20tackle%20three%20concrete%20challenges.%20First%2C%20we%20consider%20the%20process%20of%0Ainformation%20propagation%20through%20a%20neural%20network%2C%20i.e.%2C%20we%20study%20the%0Ainput-output%20map%20for%20different%20architectures.%20We%20explain%20the%20universal%0Aembedding%20property%20for%20augmented%20neural%20ODEs%20representing%20arbitrary%20functions%0Aof%20given%20regularity%2C%20the%20classification%20of%20multilayer%20perceptrons%20and%20neural%0AODEs%20in%20terms%20of%20suitable%20function%20classes%2C%20and%20the%20memory-dependence%20in%20neural%0Adelay%20equations.%20Second%2C%20we%20consider%20the%20training%20aspect%20of%20neural%20networks%0Adynamically.%20We%20describe%20a%20dynamical%20systems%20perspective%20on%20gradient%20descent%0Aand%20study%20stability%20for%20overdetermined%20problems.%20We%20then%20extend%20this%20analysis%0Ato%20the%20overparameterized%20setting%20and%20describe%20the%20edge%20of%20stability%20phenomenon%2C%0Aalso%20in%20the%20context%20of%20possible%20explanations%20for%20implicit%20bias.%20For%20stochastic%0Agradient%20descent%2C%20we%20present%20stability%20results%20for%20the%20overparameterized%0Asetting%20via%20Lyapunov%20exponents%20of%20interpolation%20solutions.%20Third%2C%20we%20explain%0Aseveral%20results%20regarding%20mean-field%20limits%20of%20neural%20networks.%20We%20describe%20a%0Aresult%20that%20extends%20existing%20techniques%20to%20heterogeneous%20neural%20networks%0Ainvolving%20graph%20limits%20via%20digraph%20measures.%20This%20shows%20how%20large%20classes%20of%0Aneural%20networks%20naturally%20fall%20within%20the%20framework%20of%20Kuramoto-type%20models%20on%0Agraphs%20and%20their%20large-graph%20limits.%20Finally%2C%20we%20point%20out%20that%20similar%0Astrategies%20to%20use%20dynamics%20to%20study%20explainable%20and%20reliable%20AI%20can%20also%20be%0Aapplied%20to%20settings%20such%20as%20generative%20models%20or%20fundamental%20issues%20in%20gradient%0Atraining%20methods%2C%20such%20as%20backpropagation%20or%20vanishing/exploding%20gradients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Dynamical%2520Systems%2520Perspective%2520on%2520the%2520Analysis%2520of%2520Neural%2520Networks%26entry.906535625%3DDennis%2520Chemnitz%2520and%2520Maximilian%2520Engel%2520and%2520Christian%2520Kuehn%2520and%2520Sara-Viola%2520Kuntz%26entry.1292438233%3D%2520%2520In%2520this%2520chapter%252C%2520we%2520utilize%2520dynamical%2520systems%2520to%2520analyze%2520several%2520aspects%2520of%250Amachine%2520learning%2520algorithms.%2520As%2520an%2520expository%2520contribution%2520we%2520demonstrate%2520how%250Ato%2520re-formulate%2520a%2520wide%2520variety%2520of%2520challenges%2520from%2520deep%2520neural%2520networks%252C%250A%2528stochastic%2529%2520gradient%2520descent%252C%2520and%2520related%2520topics%2520into%2520dynamical%2520statements.%2520We%250Aalso%2520tackle%2520three%2520concrete%2520challenges.%2520First%252C%2520we%2520consider%2520the%2520process%2520of%250Ainformation%2520propagation%2520through%2520a%2520neural%2520network%252C%2520i.e.%252C%2520we%2520study%2520the%250Ainput-output%2520map%2520for%2520different%2520architectures.%2520We%2520explain%2520the%2520universal%250Aembedding%2520property%2520for%2520augmented%2520neural%2520ODEs%2520representing%2520arbitrary%2520functions%250Aof%2520given%2520regularity%252C%2520the%2520classification%2520of%2520multilayer%2520perceptrons%2520and%2520neural%250AODEs%2520in%2520terms%2520of%2520suitable%2520function%2520classes%252C%2520and%2520the%2520memory-dependence%2520in%2520neural%250Adelay%2520equations.%2520Second%252C%2520we%2520consider%2520the%2520training%2520aspect%2520of%2520neural%2520networks%250Adynamically.%2520We%2520describe%2520a%2520dynamical%2520systems%2520perspective%2520on%2520gradient%2520descent%250Aand%2520study%2520stability%2520for%2520overdetermined%2520problems.%2520We%2520then%2520extend%2520this%2520analysis%250Ato%2520the%2520overparameterized%2520setting%2520and%2520describe%2520the%2520edge%2520of%2520stability%2520phenomenon%252C%250Aalso%2520in%2520the%2520context%2520of%2520possible%2520explanations%2520for%2520implicit%2520bias.%2520For%2520stochastic%250Agradient%2520descent%252C%2520we%2520present%2520stability%2520results%2520for%2520the%2520overparameterized%250Asetting%2520via%2520Lyapunov%2520exponents%2520of%2520interpolation%2520solutions.%2520Third%252C%2520we%2520explain%250Aseveral%2520results%2520regarding%2520mean-field%2520limits%2520of%2520neural%2520networks.%2520We%2520describe%2520a%250Aresult%2520that%2520extends%2520existing%2520techniques%2520to%2520heterogeneous%2520neural%2520networks%250Ainvolving%2520graph%2520limits%2520via%2520digraph%2520measures.%2520This%2520shows%2520how%2520large%2520classes%2520of%250Aneural%2520networks%2520naturally%2520fall%2520within%2520the%2520framework%2520of%2520Kuramoto-type%2520models%2520on%250Agraphs%2520and%2520their%2520large-graph%2520limits.%2520Finally%252C%2520we%2520point%2520out%2520that%2520similar%250Astrategies%2520to%2520use%2520dynamics%2520to%2520study%2520explainable%2520and%2520reliable%2520AI%2520can%2520also%2520be%250Aapplied%2520to%2520settings%2520such%2520as%2520generative%2520models%2520or%2520fundamental%2520issues%2520in%2520gradient%250Atraining%2520methods%252C%2520such%2520as%2520backpropagation%2520or%2520vanishing/exploding%2520gradients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Dynamical%20Systems%20Perspective%20on%20the%20Analysis%20of%20Neural%20Networks&entry.906535625=Dennis%20Chemnitz%20and%20Maximilian%20Engel%20and%20Christian%20Kuehn%20and%20Sara-Viola%20Kuntz&entry.1292438233=%20%20In%20this%20chapter%2C%20we%20utilize%20dynamical%20systems%20to%20analyze%20several%20aspects%20of%0Amachine%20learning%20algorithms.%20As%20an%20expository%20contribution%20we%20demonstrate%20how%0Ato%20re-formulate%20a%20wide%20variety%20of%20challenges%20from%20deep%20neural%20networks%2C%0A%28stochastic%29%20gradient%20descent%2C%20and%20related%20topics%20into%20dynamical%20statements.%20We%0Aalso%20tackle%20three%20concrete%20challenges.%20First%2C%20we%20consider%20the%20process%20of%0Ainformation%20propagation%20through%20a%20neural%20network%2C%20i.e.%2C%20we%20study%20the%0Ainput-output%20map%20for%20different%20architectures.%20We%20explain%20the%20universal%0Aembedding%20property%20for%20augmented%20neural%20ODEs%20representing%20arbitrary%20functions%0Aof%20given%20regularity%2C%20the%20classification%20of%20multilayer%20perceptrons%20and%20neural%0AODEs%20in%20terms%20of%20suitable%20function%20classes%2C%20and%20the%20memory-dependence%20in%20neural%0Adelay%20equations.%20Second%2C%20we%20consider%20the%20training%20aspect%20of%20neural%20networks%0Adynamically.%20We%20describe%20a%20dynamical%20systems%20perspective%20on%20gradient%20descent%0Aand%20study%20stability%20for%20overdetermined%20problems.%20We%20then%20extend%20this%20analysis%0Ato%20the%20overparameterized%20setting%20and%20describe%20the%20edge%20of%20stability%20phenomenon%2C%0Aalso%20in%20the%20context%20of%20possible%20explanations%20for%20implicit%20bias.%20For%20stochastic%0Agradient%20descent%2C%20we%20present%20stability%20results%20for%20the%20overparameterized%0Asetting%20via%20Lyapunov%20exponents%20of%20interpolation%20solutions.%20Third%2C%20we%20explain%0Aseveral%20results%20regarding%20mean-field%20limits%20of%20neural%20networks.%20We%20describe%20a%0Aresult%20that%20extends%20existing%20techniques%20to%20heterogeneous%20neural%20networks%0Ainvolving%20graph%20limits%20via%20digraph%20measures.%20This%20shows%20how%20large%20classes%20of%0Aneural%20networks%20naturally%20fall%20within%20the%20framework%20of%20Kuramoto-type%20models%20on%0Agraphs%20and%20their%20large-graph%20limits.%20Finally%2C%20we%20point%20out%20that%20similar%0Astrategies%20to%20use%20dynamics%20to%20study%20explainable%20and%20reliable%20AI%20can%20also%20be%0Aapplied%20to%20settings%20such%20as%20generative%20models%20or%20fundamental%20issues%20in%20gradient%0Atraining%20methods%2C%20such%20as%20backpropagation%20or%20vanishing/exploding%20gradients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05164v1&entry.124074799=Read"},
{"title": "Judging the Judges: Can Large Vision-Language Models Fairly Evaluate\n  Chart Comprehension and Reasoning?", "author": "Md Tahmid Rahman Laskar and Mohammed Saidul Islam and Ridwan Mahbub and Ahmed Masry and Mizanur Rahman and Amran Bhuiyan and Mir Tafseer Nayeem and Shafiq Joty and Enamul Hoque and Jimmy Huang", "abstract": "  Charts are ubiquitous as they help people understand and reason with data.\nRecently, various downstream tasks, such as chart question answering,\nchart2text, and fact-checking, have emerged. Large Vision-Language Models\n(LVLMs) show promise in tackling these tasks, but their evaluation is costly\nand time-consuming, limiting real-world deployment. While using LVLMs as judges\nto assess the chart comprehension capabilities of other LVLMs could streamline\nevaluation processes, challenges like proprietary datasets, restricted access\nto powerful models, and evaluation costs hinder their adoption in industrial\nsettings. To this end, we present a comprehensive evaluation of 13 open-source\nLVLMs as judges for diverse chart comprehension and reasoning tasks. We design\nboth pairwise and pointwise evaluation tasks covering criteria like factual\ncorrectness, informativeness, and relevancy. Additionally, we analyze LVLM\njudges based on format adherence, positional consistency, length bias, and\ninstruction-following. We focus on cost-effective LVLMs (<10B parameters)\nsuitable for both research and commercial use, following a standardized\nevaluation protocol and rubric to measure the LVLM judge's accuracy.\nExperimental results reveal notable variability: while some open LVLM judges\nachieve GPT-4-level evaluation performance (about 80% agreement with GPT-4\njudgments), others struggle (below ~10% agreement). Our findings highlight that\nstate-of-the-art open-source LVLMs can serve as cost-effective automatic\nevaluators for chart-related tasks, though biases such as positional preference\nand length bias persist.\n", "link": "http://arxiv.org/abs/2505.08468v2", "date": "2025-07-07", "relevancy": 2.12, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5405}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Judging%20the%20Judges%3A%20Can%20Large%20Vision-Language%20Models%20Fairly%20Evaluate%0A%20%20Chart%20Comprehension%20and%20Reasoning%3F&body=Title%3A%20Judging%20the%20Judges%3A%20Can%20Large%20Vision-Language%20Models%20Fairly%20Evaluate%0A%20%20Chart%20Comprehension%20and%20Reasoning%3F%0AAuthor%3A%20Md%20Tahmid%20Rahman%20Laskar%20and%20Mohammed%20Saidul%20Islam%20and%20Ridwan%20Mahbub%20and%20Ahmed%20Masry%20and%20Mizanur%20Rahman%20and%20Amran%20Bhuiyan%20and%20Mir%20Tafseer%20Nayeem%20and%20Shafiq%20Joty%20and%20Enamul%20Hoque%20and%20Jimmy%20Huang%0AAbstract%3A%20%20%20Charts%20are%20ubiquitous%20as%20they%20help%20people%20understand%20and%20reason%20with%20data.%0ARecently%2C%20various%20downstream%20tasks%2C%20such%20as%20chart%20question%20answering%2C%0Achart2text%2C%20and%20fact-checking%2C%20have%20emerged.%20Large%20Vision-Language%20Models%0A%28LVLMs%29%20show%20promise%20in%20tackling%20these%20tasks%2C%20but%20their%20evaluation%20is%20costly%0Aand%20time-consuming%2C%20limiting%20real-world%20deployment.%20While%20using%20LVLMs%20as%20judges%0Ato%20assess%20the%20chart%20comprehension%20capabilities%20of%20other%20LVLMs%20could%20streamline%0Aevaluation%20processes%2C%20challenges%20like%20proprietary%20datasets%2C%20restricted%20access%0Ato%20powerful%20models%2C%20and%20evaluation%20costs%20hinder%20their%20adoption%20in%20industrial%0Asettings.%20To%20this%20end%2C%20we%20present%20a%20comprehensive%20evaluation%20of%2013%20open-source%0ALVLMs%20as%20judges%20for%20diverse%20chart%20comprehension%20and%20reasoning%20tasks.%20We%20design%0Aboth%20pairwise%20and%20pointwise%20evaluation%20tasks%20covering%20criteria%20like%20factual%0Acorrectness%2C%20informativeness%2C%20and%20relevancy.%20Additionally%2C%20we%20analyze%20LVLM%0Ajudges%20based%20on%20format%20adherence%2C%20positional%20consistency%2C%20length%20bias%2C%20and%0Ainstruction-following.%20We%20focus%20on%20cost-effective%20LVLMs%20%28%3C10B%20parameters%29%0Asuitable%20for%20both%20research%20and%20commercial%20use%2C%20following%20a%20standardized%0Aevaluation%20protocol%20and%20rubric%20to%20measure%20the%20LVLM%20judge%27s%20accuracy.%0AExperimental%20results%20reveal%20notable%20variability%3A%20while%20some%20open%20LVLM%20judges%0Aachieve%20GPT-4-level%20evaluation%20performance%20%28about%2080%25%20agreement%20with%20GPT-4%0Ajudgments%29%2C%20others%20struggle%20%28below%20~10%25%20agreement%29.%20Our%20findings%20highlight%20that%0Astate-of-the-art%20open-source%20LVLMs%20can%20serve%20as%20cost-effective%20automatic%0Aevaluators%20for%20chart-related%20tasks%2C%20though%20biases%20such%20as%20positional%20preference%0Aand%20length%20bias%20persist.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08468v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJudging%2520the%2520Judges%253A%2520Can%2520Large%2520Vision-Language%2520Models%2520Fairly%2520Evaluate%250A%2520%2520Chart%2520Comprehension%2520and%2520Reasoning%253F%26entry.906535625%3DMd%2520Tahmid%2520Rahman%2520Laskar%2520and%2520Mohammed%2520Saidul%2520Islam%2520and%2520Ridwan%2520Mahbub%2520and%2520Ahmed%2520Masry%2520and%2520Mizanur%2520Rahman%2520and%2520Amran%2520Bhuiyan%2520and%2520Mir%2520Tafseer%2520Nayeem%2520and%2520Shafiq%2520Joty%2520and%2520Enamul%2520Hoque%2520and%2520Jimmy%2520Huang%26entry.1292438233%3D%2520%2520Charts%2520are%2520ubiquitous%2520as%2520they%2520help%2520people%2520understand%2520and%2520reason%2520with%2520data.%250ARecently%252C%2520various%2520downstream%2520tasks%252C%2520such%2520as%2520chart%2520question%2520answering%252C%250Achart2text%252C%2520and%2520fact-checking%252C%2520have%2520emerged.%2520Large%2520Vision-Language%2520Models%250A%2528LVLMs%2529%2520show%2520promise%2520in%2520tackling%2520these%2520tasks%252C%2520but%2520their%2520evaluation%2520is%2520costly%250Aand%2520time-consuming%252C%2520limiting%2520real-world%2520deployment.%2520While%2520using%2520LVLMs%2520as%2520judges%250Ato%2520assess%2520the%2520chart%2520comprehension%2520capabilities%2520of%2520other%2520LVLMs%2520could%2520streamline%250Aevaluation%2520processes%252C%2520challenges%2520like%2520proprietary%2520datasets%252C%2520restricted%2520access%250Ato%2520powerful%2520models%252C%2520and%2520evaluation%2520costs%2520hinder%2520their%2520adoption%2520in%2520industrial%250Asettings.%2520To%2520this%2520end%252C%2520we%2520present%2520a%2520comprehensive%2520evaluation%2520of%252013%2520open-source%250ALVLMs%2520as%2520judges%2520for%2520diverse%2520chart%2520comprehension%2520and%2520reasoning%2520tasks.%2520We%2520design%250Aboth%2520pairwise%2520and%2520pointwise%2520evaluation%2520tasks%2520covering%2520criteria%2520like%2520factual%250Acorrectness%252C%2520informativeness%252C%2520and%2520relevancy.%2520Additionally%252C%2520we%2520analyze%2520LVLM%250Ajudges%2520based%2520on%2520format%2520adherence%252C%2520positional%2520consistency%252C%2520length%2520bias%252C%2520and%250Ainstruction-following.%2520We%2520focus%2520on%2520cost-effective%2520LVLMs%2520%2528%253C10B%2520parameters%2529%250Asuitable%2520for%2520both%2520research%2520and%2520commercial%2520use%252C%2520following%2520a%2520standardized%250Aevaluation%2520protocol%2520and%2520rubric%2520to%2520measure%2520the%2520LVLM%2520judge%2527s%2520accuracy.%250AExperimental%2520results%2520reveal%2520notable%2520variability%253A%2520while%2520some%2520open%2520LVLM%2520judges%250Aachieve%2520GPT-4-level%2520evaluation%2520performance%2520%2528about%252080%2525%2520agreement%2520with%2520GPT-4%250Ajudgments%2529%252C%2520others%2520struggle%2520%2528below%2520~10%2525%2520agreement%2529.%2520Our%2520findings%2520highlight%2520that%250Astate-of-the-art%2520open-source%2520LVLMs%2520can%2520serve%2520as%2520cost-effective%2520automatic%250Aevaluators%2520for%2520chart-related%2520tasks%252C%2520though%2520biases%2520such%2520as%2520positional%2520preference%250Aand%2520length%2520bias%2520persist.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08468v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Judging%20the%20Judges%3A%20Can%20Large%20Vision-Language%20Models%20Fairly%20Evaluate%0A%20%20Chart%20Comprehension%20and%20Reasoning%3F&entry.906535625=Md%20Tahmid%20Rahman%20Laskar%20and%20Mohammed%20Saidul%20Islam%20and%20Ridwan%20Mahbub%20and%20Ahmed%20Masry%20and%20Mizanur%20Rahman%20and%20Amran%20Bhuiyan%20and%20Mir%20Tafseer%20Nayeem%20and%20Shafiq%20Joty%20and%20Enamul%20Hoque%20and%20Jimmy%20Huang&entry.1292438233=%20%20Charts%20are%20ubiquitous%20as%20they%20help%20people%20understand%20and%20reason%20with%20data.%0ARecently%2C%20various%20downstream%20tasks%2C%20such%20as%20chart%20question%20answering%2C%0Achart2text%2C%20and%20fact-checking%2C%20have%20emerged.%20Large%20Vision-Language%20Models%0A%28LVLMs%29%20show%20promise%20in%20tackling%20these%20tasks%2C%20but%20their%20evaluation%20is%20costly%0Aand%20time-consuming%2C%20limiting%20real-world%20deployment.%20While%20using%20LVLMs%20as%20judges%0Ato%20assess%20the%20chart%20comprehension%20capabilities%20of%20other%20LVLMs%20could%20streamline%0Aevaluation%20processes%2C%20challenges%20like%20proprietary%20datasets%2C%20restricted%20access%0Ato%20powerful%20models%2C%20and%20evaluation%20costs%20hinder%20their%20adoption%20in%20industrial%0Asettings.%20To%20this%20end%2C%20we%20present%20a%20comprehensive%20evaluation%20of%2013%20open-source%0ALVLMs%20as%20judges%20for%20diverse%20chart%20comprehension%20and%20reasoning%20tasks.%20We%20design%0Aboth%20pairwise%20and%20pointwise%20evaluation%20tasks%20covering%20criteria%20like%20factual%0Acorrectness%2C%20informativeness%2C%20and%20relevancy.%20Additionally%2C%20we%20analyze%20LVLM%0Ajudges%20based%20on%20format%20adherence%2C%20positional%20consistency%2C%20length%20bias%2C%20and%0Ainstruction-following.%20We%20focus%20on%20cost-effective%20LVLMs%20%28%3C10B%20parameters%29%0Asuitable%20for%20both%20research%20and%20commercial%20use%2C%20following%20a%20standardized%0Aevaluation%20protocol%20and%20rubric%20to%20measure%20the%20LVLM%20judge%27s%20accuracy.%0AExperimental%20results%20reveal%20notable%20variability%3A%20while%20some%20open%20LVLM%20judges%0Aachieve%20GPT-4-level%20evaluation%20performance%20%28about%2080%25%20agreement%20with%20GPT-4%0Ajudgments%29%2C%20others%20struggle%20%28below%20~10%25%20agreement%29.%20Our%20findings%20highlight%20that%0Astate-of-the-art%20open-source%20LVLMs%20can%20serve%20as%20cost-effective%20automatic%0Aevaluators%20for%20chart-related%20tasks%2C%20though%20biases%20such%20as%20positional%20preference%0Aand%20length%20bias%20persist.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08468v2&entry.124074799=Read"},
{"title": "Transfer Attack for Bad and Good: Explain and Boost Adversarial\n  Transferability across Multimodal Large Language Models", "author": "Hao Cheng and Erjia Xiao and Jiayan Yang and Jinhao Duan and Yichi Wang and Jiahang Cao and Qiang Zhang and Le Yang and Kaidi Xu and Jindong Gu and Renjing Xu", "abstract": "  Multimodal Large Language Models (MLLMs) demonstrate exceptional performance\nin cross-modality interaction, yet they also suffer adversarial\nvulnerabilities. In particular, the transferability of adversarial examples\nremains an ongoing challenge. In this paper, we specifically analyze the\nmanifestation of adversarial transferability among MLLMs and identify the key\nfactors that influence this characteristic. We discover that the\ntransferability of MLLMs exists in cross-LLM scenarios with the same vision\nencoder and indicate \\underline{\\textit{two key Factors}} that may influence\ntransferability. We provide two semantic-level data augmentation methods,\nAdding Image Patch (AIP) and Typography Augment Transferability Method (TATM),\nwhich boost the transferability of adversarial examples across MLLMs. To\nexplore the potential impact in the real world, we utilize two tasks that can\nhave both negative and positive societal impacts: \\ding{182} Harmful Content\nInsertion and \\ding{183} Information Protection.\n", "link": "http://arxiv.org/abs/2405.20090v4", "date": "2025-07-07", "relevancy": 2.12, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5641}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5107}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transfer%20Attack%20for%20Bad%20and%20Good%3A%20Explain%20and%20Boost%20Adversarial%0A%20%20Transferability%20across%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Transfer%20Attack%20for%20Bad%20and%20Good%3A%20Explain%20and%20Boost%20Adversarial%0A%20%20Transferability%20across%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Hao%20Cheng%20and%20Erjia%20Xiao%20and%20Jiayan%20Yang%20and%20Jinhao%20Duan%20and%20Yichi%20Wang%20and%20Jiahang%20Cao%20and%20Qiang%20Zhang%20and%20Le%20Yang%20and%20Kaidi%20Xu%20and%20Jindong%20Gu%20and%20Renjing%20Xu%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20demonstrate%20exceptional%20performance%0Ain%20cross-modality%20interaction%2C%20yet%20they%20also%20suffer%20adversarial%0Avulnerabilities.%20In%20particular%2C%20the%20transferability%20of%20adversarial%20examples%0Aremains%20an%20ongoing%20challenge.%20In%20this%20paper%2C%20we%20specifically%20analyze%20the%0Amanifestation%20of%20adversarial%20transferability%20among%20MLLMs%20and%20identify%20the%20key%0Afactors%20that%20influence%20this%20characteristic.%20We%20discover%20that%20the%0Atransferability%20of%20MLLMs%20exists%20in%20cross-LLM%20scenarios%20with%20the%20same%20vision%0Aencoder%20and%20indicate%20%5Cunderline%7B%5Ctextit%7Btwo%20key%20Factors%7D%7D%20that%20may%20influence%0Atransferability.%20We%20provide%20two%20semantic-level%20data%20augmentation%20methods%2C%0AAdding%20Image%20Patch%20%28AIP%29%20and%20Typography%20Augment%20Transferability%20Method%20%28TATM%29%2C%0Awhich%20boost%20the%20transferability%20of%20adversarial%20examples%20across%20MLLMs.%20To%0Aexplore%20the%20potential%20impact%20in%20the%20real%20world%2C%20we%20utilize%20two%20tasks%20that%20can%0Ahave%20both%20negative%20and%20positive%20societal%20impacts%3A%20%5Cding%7B182%7D%20Harmful%20Content%0AInsertion%20and%20%5Cding%7B183%7D%20Information%20Protection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20090v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransfer%2520Attack%2520for%2520Bad%2520and%2520Good%253A%2520Explain%2520and%2520Boost%2520Adversarial%250A%2520%2520Transferability%2520across%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DHao%2520Cheng%2520and%2520Erjia%2520Xiao%2520and%2520Jiayan%2520Yang%2520and%2520Jinhao%2520Duan%2520and%2520Yichi%2520Wang%2520and%2520Jiahang%2520Cao%2520and%2520Qiang%2520Zhang%2520and%2520Le%2520Yang%2520and%2520Kaidi%2520Xu%2520and%2520Jindong%2520Gu%2520and%2520Renjing%2520Xu%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520demonstrate%2520exceptional%2520performance%250Ain%2520cross-modality%2520interaction%252C%2520yet%2520they%2520also%2520suffer%2520adversarial%250Avulnerabilities.%2520In%2520particular%252C%2520the%2520transferability%2520of%2520adversarial%2520examples%250Aremains%2520an%2520ongoing%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520specifically%2520analyze%2520the%250Amanifestation%2520of%2520adversarial%2520transferability%2520among%2520MLLMs%2520and%2520identify%2520the%2520key%250Afactors%2520that%2520influence%2520this%2520characteristic.%2520We%2520discover%2520that%2520the%250Atransferability%2520of%2520MLLMs%2520exists%2520in%2520cross-LLM%2520scenarios%2520with%2520the%2520same%2520vision%250Aencoder%2520and%2520indicate%2520%255Cunderline%257B%255Ctextit%257Btwo%2520key%2520Factors%257D%257D%2520that%2520may%2520influence%250Atransferability.%2520We%2520provide%2520two%2520semantic-level%2520data%2520augmentation%2520methods%252C%250AAdding%2520Image%2520Patch%2520%2528AIP%2529%2520and%2520Typography%2520Augment%2520Transferability%2520Method%2520%2528TATM%2529%252C%250Awhich%2520boost%2520the%2520transferability%2520of%2520adversarial%2520examples%2520across%2520MLLMs.%2520To%250Aexplore%2520the%2520potential%2520impact%2520in%2520the%2520real%2520world%252C%2520we%2520utilize%2520two%2520tasks%2520that%2520can%250Ahave%2520both%2520negative%2520and%2520positive%2520societal%2520impacts%253A%2520%255Cding%257B182%257D%2520Harmful%2520Content%250AInsertion%2520and%2520%255Cding%257B183%257D%2520Information%2520Protection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20090v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transfer%20Attack%20for%20Bad%20and%20Good%3A%20Explain%20and%20Boost%20Adversarial%0A%20%20Transferability%20across%20Multimodal%20Large%20Language%20Models&entry.906535625=Hao%20Cheng%20and%20Erjia%20Xiao%20and%20Jiayan%20Yang%20and%20Jinhao%20Duan%20and%20Yichi%20Wang%20and%20Jiahang%20Cao%20and%20Qiang%20Zhang%20and%20Le%20Yang%20and%20Kaidi%20Xu%20and%20Jindong%20Gu%20and%20Renjing%20Xu&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20demonstrate%20exceptional%20performance%0Ain%20cross-modality%20interaction%2C%20yet%20they%20also%20suffer%20adversarial%0Avulnerabilities.%20In%20particular%2C%20the%20transferability%20of%20adversarial%20examples%0Aremains%20an%20ongoing%20challenge.%20In%20this%20paper%2C%20we%20specifically%20analyze%20the%0Amanifestation%20of%20adversarial%20transferability%20among%20MLLMs%20and%20identify%20the%20key%0Afactors%20that%20influence%20this%20characteristic.%20We%20discover%20that%20the%0Atransferability%20of%20MLLMs%20exists%20in%20cross-LLM%20scenarios%20with%20the%20same%20vision%0Aencoder%20and%20indicate%20%5Cunderline%7B%5Ctextit%7Btwo%20key%20Factors%7D%7D%20that%20may%20influence%0Atransferability.%20We%20provide%20two%20semantic-level%20data%20augmentation%20methods%2C%0AAdding%20Image%20Patch%20%28AIP%29%20and%20Typography%20Augment%20Transferability%20Method%20%28TATM%29%2C%0Awhich%20boost%20the%20transferability%20of%20adversarial%20examples%20across%20MLLMs.%20To%0Aexplore%20the%20potential%20impact%20in%20the%20real%20world%2C%20we%20utilize%20two%20tasks%20that%20can%0Ahave%20both%20negative%20and%20positive%20societal%20impacts%3A%20%5Cding%7B182%7D%20Harmful%20Content%0AInsertion%20and%20%5Cding%7B183%7D%20Information%20Protection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20090v4&entry.124074799=Read"},
{"title": "Can Video LLMs Refuse to Answer? Alignment for Answerability in Video\n  Large Language Models", "author": "Eunseop Yoon and Hee Suk Yoon and Mark A. Hasegawa-Johnson and Chang D. Yoo", "abstract": "  In the broader context of deep learning, Multimodal Large Language Models\nhave achieved significant breakthroughs by leveraging powerful Large Language\nModels as a backbone to align different modalities into the language space. A\nprime exemplification is the development of Video Large Language Models\n(Video-LLMs). While numerous advancements have been proposed to enhance the\nvideo understanding capabilities of these models, they are predominantly\ntrained on questions generated directly from video content. However, in\nreal-world scenarios, users often pose questions that extend beyond the\ninformational scope of the video, highlighting the need for Video-LLMs to\nassess the relevance of the question. We demonstrate that even the\nbest-performing Video-LLMs fail to reject unfit questions-not necessarily due\nto a lack of video understanding, but because they have not been trained to\nidentify and refuse such questions. To address this limitation, we propose\nalignment for answerability, a framework that equips Video-LLMs with the\nability to evaluate the relevance of a question based on the input video and\nappropriately decline to answer when the question exceeds the scope of the\nvideo, as well as an evaluation framework with a comprehensive set of metrics\ndesigned to measure model behavior before and after alignment. Furthermore, we\npresent a pipeline for creating a dataset specifically tailored for alignment\nfor answerability, leveraging existing video-description paired datasets.\n", "link": "http://arxiv.org/abs/2507.04976v1", "date": "2025-07-07", "relevancy": 2.1164, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Video%20LLMs%20Refuse%20to%20Answer%3F%20Alignment%20for%20Answerability%20in%20Video%0A%20%20Large%20Language%20Models&body=Title%3A%20Can%20Video%20LLMs%20Refuse%20to%20Answer%3F%20Alignment%20for%20Answerability%20in%20Video%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Eunseop%20Yoon%20and%20Hee%20Suk%20Yoon%20and%20Mark%20A.%20Hasegawa-Johnson%20and%20Chang%20D.%20Yoo%0AAbstract%3A%20%20%20In%20the%20broader%20context%20of%20deep%20learning%2C%20Multimodal%20Large%20Language%20Models%0Ahave%20achieved%20significant%20breakthroughs%20by%20leveraging%20powerful%20Large%20Language%0AModels%20as%20a%20backbone%20to%20align%20different%20modalities%20into%20the%20language%20space.%20A%0Aprime%20exemplification%20is%20the%20development%20of%20Video%20Large%20Language%20Models%0A%28Video-LLMs%29.%20While%20numerous%20advancements%20have%20been%20proposed%20to%20enhance%20the%0Avideo%20understanding%20capabilities%20of%20these%20models%2C%20they%20are%20predominantly%0Atrained%20on%20questions%20generated%20directly%20from%20video%20content.%20However%2C%20in%0Areal-world%20scenarios%2C%20users%20often%20pose%20questions%20that%20extend%20beyond%20the%0Ainformational%20scope%20of%20the%20video%2C%20highlighting%20the%20need%20for%20Video-LLMs%20to%0Aassess%20the%20relevance%20of%20the%20question.%20We%20demonstrate%20that%20even%20the%0Abest-performing%20Video-LLMs%20fail%20to%20reject%20unfit%20questions-not%20necessarily%20due%0Ato%20a%20lack%20of%20video%20understanding%2C%20but%20because%20they%20have%20not%20been%20trained%20to%0Aidentify%20and%20refuse%20such%20questions.%20To%20address%20this%20limitation%2C%20we%20propose%0Aalignment%20for%20answerability%2C%20a%20framework%20that%20equips%20Video-LLMs%20with%20the%0Aability%20to%20evaluate%20the%20relevance%20of%20a%20question%20based%20on%20the%20input%20video%20and%0Aappropriately%20decline%20to%20answer%20when%20the%20question%20exceeds%20the%20scope%20of%20the%0Avideo%2C%20as%20well%20as%20an%20evaluation%20framework%20with%20a%20comprehensive%20set%20of%20metrics%0Adesigned%20to%20measure%20model%20behavior%20before%20and%20after%20alignment.%20Furthermore%2C%20we%0Apresent%20a%20pipeline%20for%20creating%20a%20dataset%20specifically%20tailored%20for%20alignment%0Afor%20answerability%2C%20leveraging%20existing%20video-description%20paired%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Video%2520LLMs%2520Refuse%2520to%2520Answer%253F%2520Alignment%2520for%2520Answerability%2520in%2520Video%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DEunseop%2520Yoon%2520and%2520Hee%2520Suk%2520Yoon%2520and%2520Mark%2520A.%2520Hasegawa-Johnson%2520and%2520Chang%2520D.%2520Yoo%26entry.1292438233%3D%2520%2520In%2520the%2520broader%2520context%2520of%2520deep%2520learning%252C%2520Multimodal%2520Large%2520Language%2520Models%250Ahave%2520achieved%2520significant%2520breakthroughs%2520by%2520leveraging%2520powerful%2520Large%2520Language%250AModels%2520as%2520a%2520backbone%2520to%2520align%2520different%2520modalities%2520into%2520the%2520language%2520space.%2520A%250Aprime%2520exemplification%2520is%2520the%2520development%2520of%2520Video%2520Large%2520Language%2520Models%250A%2528Video-LLMs%2529.%2520While%2520numerous%2520advancements%2520have%2520been%2520proposed%2520to%2520enhance%2520the%250Avideo%2520understanding%2520capabilities%2520of%2520these%2520models%252C%2520they%2520are%2520predominantly%250Atrained%2520on%2520questions%2520generated%2520directly%2520from%2520video%2520content.%2520However%252C%2520in%250Areal-world%2520scenarios%252C%2520users%2520often%2520pose%2520questions%2520that%2520extend%2520beyond%2520the%250Ainformational%2520scope%2520of%2520the%2520video%252C%2520highlighting%2520the%2520need%2520for%2520Video-LLMs%2520to%250Aassess%2520the%2520relevance%2520of%2520the%2520question.%2520We%2520demonstrate%2520that%2520even%2520the%250Abest-performing%2520Video-LLMs%2520fail%2520to%2520reject%2520unfit%2520questions-not%2520necessarily%2520due%250Ato%2520a%2520lack%2520of%2520video%2520understanding%252C%2520but%2520because%2520they%2520have%2520not%2520been%2520trained%2520to%250Aidentify%2520and%2520refuse%2520such%2520questions.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%250Aalignment%2520for%2520answerability%252C%2520a%2520framework%2520that%2520equips%2520Video-LLMs%2520with%2520the%250Aability%2520to%2520evaluate%2520the%2520relevance%2520of%2520a%2520question%2520based%2520on%2520the%2520input%2520video%2520and%250Aappropriately%2520decline%2520to%2520answer%2520when%2520the%2520question%2520exceeds%2520the%2520scope%2520of%2520the%250Avideo%252C%2520as%2520well%2520as%2520an%2520evaluation%2520framework%2520with%2520a%2520comprehensive%2520set%2520of%2520metrics%250Adesigned%2520to%2520measure%2520model%2520behavior%2520before%2520and%2520after%2520alignment.%2520Furthermore%252C%2520we%250Apresent%2520a%2520pipeline%2520for%2520creating%2520a%2520dataset%2520specifically%2520tailored%2520for%2520alignment%250Afor%2520answerability%252C%2520leveraging%2520existing%2520video-description%2520paired%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Video%20LLMs%20Refuse%20to%20Answer%3F%20Alignment%20for%20Answerability%20in%20Video%0A%20%20Large%20Language%20Models&entry.906535625=Eunseop%20Yoon%20and%20Hee%20Suk%20Yoon%20and%20Mark%20A.%20Hasegawa-Johnson%20and%20Chang%20D.%20Yoo&entry.1292438233=%20%20In%20the%20broader%20context%20of%20deep%20learning%2C%20Multimodal%20Large%20Language%20Models%0Ahave%20achieved%20significant%20breakthroughs%20by%20leveraging%20powerful%20Large%20Language%0AModels%20as%20a%20backbone%20to%20align%20different%20modalities%20into%20the%20language%20space.%20A%0Aprime%20exemplification%20is%20the%20development%20of%20Video%20Large%20Language%20Models%0A%28Video-LLMs%29.%20While%20numerous%20advancements%20have%20been%20proposed%20to%20enhance%20the%0Avideo%20understanding%20capabilities%20of%20these%20models%2C%20they%20are%20predominantly%0Atrained%20on%20questions%20generated%20directly%20from%20video%20content.%20However%2C%20in%0Areal-world%20scenarios%2C%20users%20often%20pose%20questions%20that%20extend%20beyond%20the%0Ainformational%20scope%20of%20the%20video%2C%20highlighting%20the%20need%20for%20Video-LLMs%20to%0Aassess%20the%20relevance%20of%20the%20question.%20We%20demonstrate%20that%20even%20the%0Abest-performing%20Video-LLMs%20fail%20to%20reject%20unfit%20questions-not%20necessarily%20due%0Ato%20a%20lack%20of%20video%20understanding%2C%20but%20because%20they%20have%20not%20been%20trained%20to%0Aidentify%20and%20refuse%20such%20questions.%20To%20address%20this%20limitation%2C%20we%20propose%0Aalignment%20for%20answerability%2C%20a%20framework%20that%20equips%20Video-LLMs%20with%20the%0Aability%20to%20evaluate%20the%20relevance%20of%20a%20question%20based%20on%20the%20input%20video%20and%0Aappropriately%20decline%20to%20answer%20when%20the%20question%20exceeds%20the%20scope%20of%20the%0Avideo%2C%20as%20well%20as%20an%20evaluation%20framework%20with%20a%20comprehensive%20set%20of%20metrics%0Adesigned%20to%20measure%20model%20behavior%20before%20and%20after%20alignment.%20Furthermore%2C%20we%0Apresent%20a%20pipeline%20for%20creating%20a%20dataset%20specifically%20tailored%20for%20alignment%0Afor%20answerability%2C%20leveraging%20existing%20video-description%20paired%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04976v1&entry.124074799=Read"},
{"title": "MedGemma Technical Report", "author": "Andrew Sellergren and Sahar Kazemzadeh and Tiam Jaroensri and Atilla Kiraly and Madeleine Traverse and Timo Kohlberger and Shawn Xu and Fayaz Jamil and C\u00edan Hughes and Charles Lau and Justin Chen and Fereshteh Mahvar and Liron Yatziv and Tiffany Chen and Bram Sterling and Stefanie Anna Baby and Susanna Maria Baby and Jeremy Lai and Samuel Schmidgall and Lu Yang and Kejia Chen and Per Bjornsson and Shashir Reddy and Ryan Brush and Kenneth Philbrick and Howard Hu and Howard Yang and Richa Tiwari and Sunny Jansen and Preeti Singh and Yun Liu and Shekoofeh Azizi and Aishwarya Kamath and Johan Ferret and Shreya Pathak and Nino Vieillard and Ramona Merhej and Sarah Perrin and Tatiana Matejovicova and Alexandre Ram\u00e9 and Morgane Riviere and Louis Rouillard and Thomas Mesnard and Geoffrey Cideron and Jean-bastien Grill and Sabela Ramos and Edouard Yvinec and Michelle Casbon and Elena Buchatskaya and Jean-Baptiste Alayrac and  Dmitry and  Lepikhin and Vlad Feinberg and Sebastian Borgeaud and Alek Andreev and Cassidy Hardin and Robert Dadashi and L\u00e9onard Hussenot and Armand Joulin and Olivier Bachem and Yossi Matias and Katherine Chou and Avinatan Hassidim and Kavi Goel and Clement Farabet and Joelle Barral and Tris Warkentin and Jonathon Shlens and David Fleet and Victor Cotruta and Omar Sanseviero and Gus Martins and Phoebe Kirk and Anand Rao and Shravya Shetty and David F. Steiner and Can Kirmizibayrak and Rory Pilgrim and Daniel Golden and Lin Yang", "abstract": "  Artificial intelligence (AI) has significant potential in healthcare\napplications, but its training and deployment faces challenges due to\nhealthcare's diverse data, complex tasks, and the need to preserve privacy.\nFoundation models that perform well on medical tasks and require less\ntask-specific tuning data are critical to accelerate the development of\nhealthcare AI applications. We introduce MedGemma, a collection of medical\nvision-language foundation models based on Gemma 3 4B and 27B. MedGemma\ndemonstrates advanced medical understanding and reasoning on images and text,\nsignificantly exceeding the performance of similar-sized generative models and\napproaching the performance of task-specific models, while maintaining the\ngeneral capabilities of the Gemma 3 base models. For out-of-distribution tasks,\nMedGemma achieves 2.6-10% improvement on medical multimodal question answering,\n15.5-18.1% improvement on chest X-ray finding classification, and 10.8%\nimprovement on agentic evaluations compared to the base models. Fine-tuning\nMedGemma further improves performance in subdomains, reducing errors in\nelectronic health record information retrieval by 50% and reaching comparable\nperformance to existing specialized state-of-the-art methods for pneumothorax\nclassification and histopathology patch classification. We additionally\nintroduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.\nMedSigLIP powers the visual understanding capabilities of MedGemma and as an\nencoder achieves comparable or better performance than specialized medical\nimage encoders. Taken together, the MedGemma collection provides a strong\nfoundation of medical image and text capabilities, with potential to\nsignificantly accelerate medical research and development of downstream\napplications. The MedGemma collection, including tutorials and model weights,\ncan be found at https://goo.gle/medgemma.\n", "link": "http://arxiv.org/abs/2507.05201v1", "date": "2025-07-07", "relevancy": 2.1159, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5438}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MedGemma%20Technical%20Report&body=Title%3A%20MedGemma%20Technical%20Report%0AAuthor%3A%20Andrew%20Sellergren%20and%20Sahar%20Kazemzadeh%20and%20Tiam%20Jaroensri%20and%20Atilla%20Kiraly%20and%20Madeleine%20Traverse%20and%20Timo%20Kohlberger%20and%20Shawn%20Xu%20and%20Fayaz%20Jamil%20and%20C%C3%ADan%20Hughes%20and%20Charles%20Lau%20and%20Justin%20Chen%20and%20Fereshteh%20Mahvar%20and%20Liron%20Yatziv%20and%20Tiffany%20Chen%20and%20Bram%20Sterling%20and%20Stefanie%20Anna%20Baby%20and%20Susanna%20Maria%20Baby%20and%20Jeremy%20Lai%20and%20Samuel%20Schmidgall%20and%20Lu%20Yang%20and%20Kejia%20Chen%20and%20Per%20Bjornsson%20and%20Shashir%20Reddy%20and%20Ryan%20Brush%20and%20Kenneth%20Philbrick%20and%20Howard%20Hu%20and%20Howard%20Yang%20and%20Richa%20Tiwari%20and%20Sunny%20Jansen%20and%20Preeti%20Singh%20and%20Yun%20Liu%20and%20Shekoofeh%20Azizi%20and%20Aishwarya%20Kamath%20and%20Johan%20Ferret%20and%20Shreya%20Pathak%20and%20Nino%20Vieillard%20and%20Ramona%20Merhej%20and%20Sarah%20Perrin%20and%20Tatiana%20Matejovicova%20and%20Alexandre%20Ram%C3%A9%20and%20Morgane%20Riviere%20and%20Louis%20Rouillard%20and%20Thomas%20Mesnard%20and%20Geoffrey%20Cideron%20and%20Jean-bastien%20Grill%20and%20Sabela%20Ramos%20and%20Edouard%20Yvinec%20and%20Michelle%20Casbon%20and%20Elena%20Buchatskaya%20and%20Jean-Baptiste%20Alayrac%20and%20%20Dmitry%20and%20%20Lepikhin%20and%20Vlad%20Feinberg%20and%20Sebastian%20Borgeaud%20and%20Alek%20Andreev%20and%20Cassidy%20Hardin%20and%20Robert%20Dadashi%20and%20L%C3%A9onard%20Hussenot%20and%20Armand%20Joulin%20and%20Olivier%20Bachem%20and%20Yossi%20Matias%20and%20Katherine%20Chou%20and%20Avinatan%20Hassidim%20and%20Kavi%20Goel%20and%20Clement%20Farabet%20and%20Joelle%20Barral%20and%20Tris%20Warkentin%20and%20Jonathon%20Shlens%20and%20David%20Fleet%20and%20Victor%20Cotruta%20and%20Omar%20Sanseviero%20and%20Gus%20Martins%20and%20Phoebe%20Kirk%20and%20Anand%20Rao%20and%20Shravya%20Shetty%20and%20David%20F.%20Steiner%20and%20Can%20Kirmizibayrak%20and%20Rory%20Pilgrim%20and%20Daniel%20Golden%20and%20Lin%20Yang%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29%20has%20significant%20potential%20in%20healthcare%0Aapplications%2C%20but%20its%20training%20and%20deployment%20faces%20challenges%20due%20to%0Ahealthcare%27s%20diverse%20data%2C%20complex%20tasks%2C%20and%20the%20need%20to%20preserve%20privacy.%0AFoundation%20models%20that%20perform%20well%20on%20medical%20tasks%20and%20require%20less%0Atask-specific%20tuning%20data%20are%20critical%20to%20accelerate%20the%20development%20of%0Ahealthcare%20AI%20applications.%20We%20introduce%20MedGemma%2C%20a%20collection%20of%20medical%0Avision-language%20foundation%20models%20based%20on%20Gemma%203%204B%20and%2027B.%20MedGemma%0Ademonstrates%20advanced%20medical%20understanding%20and%20reasoning%20on%20images%20and%20text%2C%0Asignificantly%20exceeding%20the%20performance%20of%20similar-sized%20generative%20models%20and%0Aapproaching%20the%20performance%20of%20task-specific%20models%2C%20while%20maintaining%20the%0Ageneral%20capabilities%20of%20the%20Gemma%203%20base%20models.%20For%20out-of-distribution%20tasks%2C%0AMedGemma%20achieves%202.6-10%25%20improvement%20on%20medical%20multimodal%20question%20answering%2C%0A15.5-18.1%25%20improvement%20on%20chest%20X-ray%20finding%20classification%2C%20and%2010.8%25%0Aimprovement%20on%20agentic%20evaluations%20compared%20to%20the%20base%20models.%20Fine-tuning%0AMedGemma%20further%20improves%20performance%20in%20subdomains%2C%20reducing%20errors%20in%0Aelectronic%20health%20record%20information%20retrieval%20by%2050%25%20and%20reaching%20comparable%0Aperformance%20to%20existing%20specialized%20state-of-the-art%20methods%20for%20pneumothorax%0Aclassification%20and%20histopathology%20patch%20classification.%20We%20additionally%0Aintroduce%20MedSigLIP%2C%20a%20medically-tuned%20vision%20encoder%20derived%20from%20SigLIP.%0AMedSigLIP%20powers%20the%20visual%20understanding%20capabilities%20of%20MedGemma%20and%20as%20an%0Aencoder%20achieves%20comparable%20or%20better%20performance%20than%20specialized%20medical%0Aimage%20encoders.%20Taken%20together%2C%20the%20MedGemma%20collection%20provides%20a%20strong%0Afoundation%20of%20medical%20image%20and%20text%20capabilities%2C%20with%20potential%20to%0Asignificantly%20accelerate%20medical%20research%20and%20development%20of%20downstream%0Aapplications.%20The%20MedGemma%20collection%2C%20including%20tutorials%20and%20model%20weights%2C%0Acan%20be%20found%20at%20https%3A//goo.gle/medgemma.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05201v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedGemma%2520Technical%2520Report%26entry.906535625%3DAndrew%2520Sellergren%2520and%2520Sahar%2520Kazemzadeh%2520and%2520Tiam%2520Jaroensri%2520and%2520Atilla%2520Kiraly%2520and%2520Madeleine%2520Traverse%2520and%2520Timo%2520Kohlberger%2520and%2520Shawn%2520Xu%2520and%2520Fayaz%2520Jamil%2520and%2520C%25C3%25ADan%2520Hughes%2520and%2520Charles%2520Lau%2520and%2520Justin%2520Chen%2520and%2520Fereshteh%2520Mahvar%2520and%2520Liron%2520Yatziv%2520and%2520Tiffany%2520Chen%2520and%2520Bram%2520Sterling%2520and%2520Stefanie%2520Anna%2520Baby%2520and%2520Susanna%2520Maria%2520Baby%2520and%2520Jeremy%2520Lai%2520and%2520Samuel%2520Schmidgall%2520and%2520Lu%2520Yang%2520and%2520Kejia%2520Chen%2520and%2520Per%2520Bjornsson%2520and%2520Shashir%2520Reddy%2520and%2520Ryan%2520Brush%2520and%2520Kenneth%2520Philbrick%2520and%2520Howard%2520Hu%2520and%2520Howard%2520Yang%2520and%2520Richa%2520Tiwari%2520and%2520Sunny%2520Jansen%2520and%2520Preeti%2520Singh%2520and%2520Yun%2520Liu%2520and%2520Shekoofeh%2520Azizi%2520and%2520Aishwarya%2520Kamath%2520and%2520Johan%2520Ferret%2520and%2520Shreya%2520Pathak%2520and%2520Nino%2520Vieillard%2520and%2520Ramona%2520Merhej%2520and%2520Sarah%2520Perrin%2520and%2520Tatiana%2520Matejovicova%2520and%2520Alexandre%2520Ram%25C3%25A9%2520and%2520Morgane%2520Riviere%2520and%2520Louis%2520Rouillard%2520and%2520Thomas%2520Mesnard%2520and%2520Geoffrey%2520Cideron%2520and%2520Jean-bastien%2520Grill%2520and%2520Sabela%2520Ramos%2520and%2520Edouard%2520Yvinec%2520and%2520Michelle%2520Casbon%2520and%2520Elena%2520Buchatskaya%2520and%2520Jean-Baptiste%2520Alayrac%2520and%2520%2520Dmitry%2520and%2520%2520Lepikhin%2520and%2520Vlad%2520Feinberg%2520and%2520Sebastian%2520Borgeaud%2520and%2520Alek%2520Andreev%2520and%2520Cassidy%2520Hardin%2520and%2520Robert%2520Dadashi%2520and%2520L%25C3%25A9onard%2520Hussenot%2520and%2520Armand%2520Joulin%2520and%2520Olivier%2520Bachem%2520and%2520Yossi%2520Matias%2520and%2520Katherine%2520Chou%2520and%2520Avinatan%2520Hassidim%2520and%2520Kavi%2520Goel%2520and%2520Clement%2520Farabet%2520and%2520Joelle%2520Barral%2520and%2520Tris%2520Warkentin%2520and%2520Jonathon%2520Shlens%2520and%2520David%2520Fleet%2520and%2520Victor%2520Cotruta%2520and%2520Omar%2520Sanseviero%2520and%2520Gus%2520Martins%2520and%2520Phoebe%2520Kirk%2520and%2520Anand%2520Rao%2520and%2520Shravya%2520Shetty%2520and%2520David%2520F.%2520Steiner%2520and%2520Can%2520Kirmizibayrak%2520and%2520Rory%2520Pilgrim%2520and%2520Daniel%2520Golden%2520and%2520Lin%2520Yang%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529%2520has%2520significant%2520potential%2520in%2520healthcare%250Aapplications%252C%2520but%2520its%2520training%2520and%2520deployment%2520faces%2520challenges%2520due%2520to%250Ahealthcare%2527s%2520diverse%2520data%252C%2520complex%2520tasks%252C%2520and%2520the%2520need%2520to%2520preserve%2520privacy.%250AFoundation%2520models%2520that%2520perform%2520well%2520on%2520medical%2520tasks%2520and%2520require%2520less%250Atask-specific%2520tuning%2520data%2520are%2520critical%2520to%2520accelerate%2520the%2520development%2520of%250Ahealthcare%2520AI%2520applications.%2520We%2520introduce%2520MedGemma%252C%2520a%2520collection%2520of%2520medical%250Avision-language%2520foundation%2520models%2520based%2520on%2520Gemma%25203%25204B%2520and%252027B.%2520MedGemma%250Ademonstrates%2520advanced%2520medical%2520understanding%2520and%2520reasoning%2520on%2520images%2520and%2520text%252C%250Asignificantly%2520exceeding%2520the%2520performance%2520of%2520similar-sized%2520generative%2520models%2520and%250Aapproaching%2520the%2520performance%2520of%2520task-specific%2520models%252C%2520while%2520maintaining%2520the%250Ageneral%2520capabilities%2520of%2520the%2520Gemma%25203%2520base%2520models.%2520For%2520out-of-distribution%2520tasks%252C%250AMedGemma%2520achieves%25202.6-10%2525%2520improvement%2520on%2520medical%2520multimodal%2520question%2520answering%252C%250A15.5-18.1%2525%2520improvement%2520on%2520chest%2520X-ray%2520finding%2520classification%252C%2520and%252010.8%2525%250Aimprovement%2520on%2520agentic%2520evaluations%2520compared%2520to%2520the%2520base%2520models.%2520Fine-tuning%250AMedGemma%2520further%2520improves%2520performance%2520in%2520subdomains%252C%2520reducing%2520errors%2520in%250Aelectronic%2520health%2520record%2520information%2520retrieval%2520by%252050%2525%2520and%2520reaching%2520comparable%250Aperformance%2520to%2520existing%2520specialized%2520state-of-the-art%2520methods%2520for%2520pneumothorax%250Aclassification%2520and%2520histopathology%2520patch%2520classification.%2520We%2520additionally%250Aintroduce%2520MedSigLIP%252C%2520a%2520medically-tuned%2520vision%2520encoder%2520derived%2520from%2520SigLIP.%250AMedSigLIP%2520powers%2520the%2520visual%2520understanding%2520capabilities%2520of%2520MedGemma%2520and%2520as%2520an%250Aencoder%2520achieves%2520comparable%2520or%2520better%2520performance%2520than%2520specialized%2520medical%250Aimage%2520encoders.%2520Taken%2520together%252C%2520the%2520MedGemma%2520collection%2520provides%2520a%2520strong%250Afoundation%2520of%2520medical%2520image%2520and%2520text%2520capabilities%252C%2520with%2520potential%2520to%250Asignificantly%2520accelerate%2520medical%2520research%2520and%2520development%2520of%2520downstream%250Aapplications.%2520The%2520MedGemma%2520collection%252C%2520including%2520tutorials%2520and%2520model%2520weights%252C%250Acan%2520be%2520found%2520at%2520https%253A//goo.gle/medgemma.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05201v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedGemma%20Technical%20Report&entry.906535625=Andrew%20Sellergren%20and%20Sahar%20Kazemzadeh%20and%20Tiam%20Jaroensri%20and%20Atilla%20Kiraly%20and%20Madeleine%20Traverse%20and%20Timo%20Kohlberger%20and%20Shawn%20Xu%20and%20Fayaz%20Jamil%20and%20C%C3%ADan%20Hughes%20and%20Charles%20Lau%20and%20Justin%20Chen%20and%20Fereshteh%20Mahvar%20and%20Liron%20Yatziv%20and%20Tiffany%20Chen%20and%20Bram%20Sterling%20and%20Stefanie%20Anna%20Baby%20and%20Susanna%20Maria%20Baby%20and%20Jeremy%20Lai%20and%20Samuel%20Schmidgall%20and%20Lu%20Yang%20and%20Kejia%20Chen%20and%20Per%20Bjornsson%20and%20Shashir%20Reddy%20and%20Ryan%20Brush%20and%20Kenneth%20Philbrick%20and%20Howard%20Hu%20and%20Howard%20Yang%20and%20Richa%20Tiwari%20and%20Sunny%20Jansen%20and%20Preeti%20Singh%20and%20Yun%20Liu%20and%20Shekoofeh%20Azizi%20and%20Aishwarya%20Kamath%20and%20Johan%20Ferret%20and%20Shreya%20Pathak%20and%20Nino%20Vieillard%20and%20Ramona%20Merhej%20and%20Sarah%20Perrin%20and%20Tatiana%20Matejovicova%20and%20Alexandre%20Ram%C3%A9%20and%20Morgane%20Riviere%20and%20Louis%20Rouillard%20and%20Thomas%20Mesnard%20and%20Geoffrey%20Cideron%20and%20Jean-bastien%20Grill%20and%20Sabela%20Ramos%20and%20Edouard%20Yvinec%20and%20Michelle%20Casbon%20and%20Elena%20Buchatskaya%20and%20Jean-Baptiste%20Alayrac%20and%20%20Dmitry%20and%20%20Lepikhin%20and%20Vlad%20Feinberg%20and%20Sebastian%20Borgeaud%20and%20Alek%20Andreev%20and%20Cassidy%20Hardin%20and%20Robert%20Dadashi%20and%20L%C3%A9onard%20Hussenot%20and%20Armand%20Joulin%20and%20Olivier%20Bachem%20and%20Yossi%20Matias%20and%20Katherine%20Chou%20and%20Avinatan%20Hassidim%20and%20Kavi%20Goel%20and%20Clement%20Farabet%20and%20Joelle%20Barral%20and%20Tris%20Warkentin%20and%20Jonathon%20Shlens%20and%20David%20Fleet%20and%20Victor%20Cotruta%20and%20Omar%20Sanseviero%20and%20Gus%20Martins%20and%20Phoebe%20Kirk%20and%20Anand%20Rao%20and%20Shravya%20Shetty%20and%20David%20F.%20Steiner%20and%20Can%20Kirmizibayrak%20and%20Rory%20Pilgrim%20and%20Daniel%20Golden%20and%20Lin%20Yang&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29%20has%20significant%20potential%20in%20healthcare%0Aapplications%2C%20but%20its%20training%20and%20deployment%20faces%20challenges%20due%20to%0Ahealthcare%27s%20diverse%20data%2C%20complex%20tasks%2C%20and%20the%20need%20to%20preserve%20privacy.%0AFoundation%20models%20that%20perform%20well%20on%20medical%20tasks%20and%20require%20less%0Atask-specific%20tuning%20data%20are%20critical%20to%20accelerate%20the%20development%20of%0Ahealthcare%20AI%20applications.%20We%20introduce%20MedGemma%2C%20a%20collection%20of%20medical%0Avision-language%20foundation%20models%20based%20on%20Gemma%203%204B%20and%2027B.%20MedGemma%0Ademonstrates%20advanced%20medical%20understanding%20and%20reasoning%20on%20images%20and%20text%2C%0Asignificantly%20exceeding%20the%20performance%20of%20similar-sized%20generative%20models%20and%0Aapproaching%20the%20performance%20of%20task-specific%20models%2C%20while%20maintaining%20the%0Ageneral%20capabilities%20of%20the%20Gemma%203%20base%20models.%20For%20out-of-distribution%20tasks%2C%0AMedGemma%20achieves%202.6-10%25%20improvement%20on%20medical%20multimodal%20question%20answering%2C%0A15.5-18.1%25%20improvement%20on%20chest%20X-ray%20finding%20classification%2C%20and%2010.8%25%0Aimprovement%20on%20agentic%20evaluations%20compared%20to%20the%20base%20models.%20Fine-tuning%0AMedGemma%20further%20improves%20performance%20in%20subdomains%2C%20reducing%20errors%20in%0Aelectronic%20health%20record%20information%20retrieval%20by%2050%25%20and%20reaching%20comparable%0Aperformance%20to%20existing%20specialized%20state-of-the-art%20methods%20for%20pneumothorax%0Aclassification%20and%20histopathology%20patch%20classification.%20We%20additionally%0Aintroduce%20MedSigLIP%2C%20a%20medically-tuned%20vision%20encoder%20derived%20from%20SigLIP.%0AMedSigLIP%20powers%20the%20visual%20understanding%20capabilities%20of%20MedGemma%20and%20as%20an%0Aencoder%20achieves%20comparable%20or%20better%20performance%20than%20specialized%20medical%0Aimage%20encoders.%20Taken%20together%2C%20the%20MedGemma%20collection%20provides%20a%20strong%0Afoundation%20of%20medical%20image%20and%20text%20capabilities%2C%20with%20potential%20to%0Asignificantly%20accelerate%20medical%20research%20and%20development%20of%20downstream%0Aapplications.%20The%20MedGemma%20collection%2C%20including%20tutorials%20and%20model%20weights%2C%0Acan%20be%20found%20at%20https%3A//goo.gle/medgemma.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05201v1&entry.124074799=Read"},
{"title": "ConBatch-BAL: Batch Bayesian Active Learning under Budget Constraints", "author": "Pablo G. Morato and Charalampos P. Andriotis and Seyran Khademi", "abstract": "  Varying annotation costs among data points and budget constraints can hinder\nthe adoption of active learning strategies in real-world applications. This\nwork introduces two Bayesian active learning strategies for batch acquisition\nunder constraints (ConBatch-BAL), one based on dynamic thresholding and one\nfollowing greedy acquisition. Both select samples using uncertainty metrics\ncomputed via Bayesian neural networks. The dynamic thresholding strategy\nredistributes the budget across the batch, while the greedy one selects the\ntop-ranked sample at each step, limited by the remaining budget. Focusing on\nscenarios with costly data annotation and geospatial constraints, we also\nrelease two new real-world datasets containing geolocated aerial images of\nbuildings, annotated with energy efficiency or typology classes. The\nConBatch-BAL strategies are benchmarked against a random acquisition baseline\non these datasets under various budget and cost scenarios. The results show\nthat the developed ConBatch-BAL strategies can reduce active learning\niterations and data acquisition costs in real-world settings, and even\noutperform the unconstrained baseline solutions.\n", "link": "http://arxiv.org/abs/2507.04929v1", "date": "2025-07-07", "relevancy": 2.1095, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5407}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5314}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConBatch-BAL%3A%20Batch%20Bayesian%20Active%20Learning%20under%20Budget%20Constraints&body=Title%3A%20ConBatch-BAL%3A%20Batch%20Bayesian%20Active%20Learning%20under%20Budget%20Constraints%0AAuthor%3A%20Pablo%20G.%20Morato%20and%20Charalampos%20P.%20Andriotis%20and%20Seyran%20Khademi%0AAbstract%3A%20%20%20Varying%20annotation%20costs%20among%20data%20points%20and%20budget%20constraints%20can%20hinder%0Athe%20adoption%20of%20active%20learning%20strategies%20in%20real-world%20applications.%20This%0Awork%20introduces%20two%20Bayesian%20active%20learning%20strategies%20for%20batch%20acquisition%0Aunder%20constraints%20%28ConBatch-BAL%29%2C%20one%20based%20on%20dynamic%20thresholding%20and%20one%0Afollowing%20greedy%20acquisition.%20Both%20select%20samples%20using%20uncertainty%20metrics%0Acomputed%20via%20Bayesian%20neural%20networks.%20The%20dynamic%20thresholding%20strategy%0Aredistributes%20the%20budget%20across%20the%20batch%2C%20while%20the%20greedy%20one%20selects%20the%0Atop-ranked%20sample%20at%20each%20step%2C%20limited%20by%20the%20remaining%20budget.%20Focusing%20on%0Ascenarios%20with%20costly%20data%20annotation%20and%20geospatial%20constraints%2C%20we%20also%0Arelease%20two%20new%20real-world%20datasets%20containing%20geolocated%20aerial%20images%20of%0Abuildings%2C%20annotated%20with%20energy%20efficiency%20or%20typology%20classes.%20The%0AConBatch-BAL%20strategies%20are%20benchmarked%20against%20a%20random%20acquisition%20baseline%0Aon%20these%20datasets%20under%20various%20budget%20and%20cost%20scenarios.%20The%20results%20show%0Athat%20the%20developed%20ConBatch-BAL%20strategies%20can%20reduce%20active%20learning%0Aiterations%20and%20data%20acquisition%20costs%20in%20real-world%20settings%2C%20and%20even%0Aoutperform%20the%20unconstrained%20baseline%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04929v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConBatch-BAL%253A%2520Batch%2520Bayesian%2520Active%2520Learning%2520under%2520Budget%2520Constraints%26entry.906535625%3DPablo%2520G.%2520Morato%2520and%2520Charalampos%2520P.%2520Andriotis%2520and%2520Seyran%2520Khademi%26entry.1292438233%3D%2520%2520Varying%2520annotation%2520costs%2520among%2520data%2520points%2520and%2520budget%2520constraints%2520can%2520hinder%250Athe%2520adoption%2520of%2520active%2520learning%2520strategies%2520in%2520real-world%2520applications.%2520This%250Awork%2520introduces%2520two%2520Bayesian%2520active%2520learning%2520strategies%2520for%2520batch%2520acquisition%250Aunder%2520constraints%2520%2528ConBatch-BAL%2529%252C%2520one%2520based%2520on%2520dynamic%2520thresholding%2520and%2520one%250Afollowing%2520greedy%2520acquisition.%2520Both%2520select%2520samples%2520using%2520uncertainty%2520metrics%250Acomputed%2520via%2520Bayesian%2520neural%2520networks.%2520The%2520dynamic%2520thresholding%2520strategy%250Aredistributes%2520the%2520budget%2520across%2520the%2520batch%252C%2520while%2520the%2520greedy%2520one%2520selects%2520the%250Atop-ranked%2520sample%2520at%2520each%2520step%252C%2520limited%2520by%2520the%2520remaining%2520budget.%2520Focusing%2520on%250Ascenarios%2520with%2520costly%2520data%2520annotation%2520and%2520geospatial%2520constraints%252C%2520we%2520also%250Arelease%2520two%2520new%2520real-world%2520datasets%2520containing%2520geolocated%2520aerial%2520images%2520of%250Abuildings%252C%2520annotated%2520with%2520energy%2520efficiency%2520or%2520typology%2520classes.%2520The%250AConBatch-BAL%2520strategies%2520are%2520benchmarked%2520against%2520a%2520random%2520acquisition%2520baseline%250Aon%2520these%2520datasets%2520under%2520various%2520budget%2520and%2520cost%2520scenarios.%2520The%2520results%2520show%250Athat%2520the%2520developed%2520ConBatch-BAL%2520strategies%2520can%2520reduce%2520active%2520learning%250Aiterations%2520and%2520data%2520acquisition%2520costs%2520in%2520real-world%2520settings%252C%2520and%2520even%250Aoutperform%2520the%2520unconstrained%2520baseline%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04929v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConBatch-BAL%3A%20Batch%20Bayesian%20Active%20Learning%20under%20Budget%20Constraints&entry.906535625=Pablo%20G.%20Morato%20and%20Charalampos%20P.%20Andriotis%20and%20Seyran%20Khademi&entry.1292438233=%20%20Varying%20annotation%20costs%20among%20data%20points%20and%20budget%20constraints%20can%20hinder%0Athe%20adoption%20of%20active%20learning%20strategies%20in%20real-world%20applications.%20This%0Awork%20introduces%20two%20Bayesian%20active%20learning%20strategies%20for%20batch%20acquisition%0Aunder%20constraints%20%28ConBatch-BAL%29%2C%20one%20based%20on%20dynamic%20thresholding%20and%20one%0Afollowing%20greedy%20acquisition.%20Both%20select%20samples%20using%20uncertainty%20metrics%0Acomputed%20via%20Bayesian%20neural%20networks.%20The%20dynamic%20thresholding%20strategy%0Aredistributes%20the%20budget%20across%20the%20batch%2C%20while%20the%20greedy%20one%20selects%20the%0Atop-ranked%20sample%20at%20each%20step%2C%20limited%20by%20the%20remaining%20budget.%20Focusing%20on%0Ascenarios%20with%20costly%20data%20annotation%20and%20geospatial%20constraints%2C%20we%20also%0Arelease%20two%20new%20real-world%20datasets%20containing%20geolocated%20aerial%20images%20of%0Abuildings%2C%20annotated%20with%20energy%20efficiency%20or%20typology%20classes.%20The%0AConBatch-BAL%20strategies%20are%20benchmarked%20against%20a%20random%20acquisition%20baseline%0Aon%20these%20datasets%20under%20various%20budget%20and%20cost%20scenarios.%20The%20results%20show%0Athat%20the%20developed%20ConBatch-BAL%20strategies%20can%20reduce%20active%20learning%0Aiterations%20and%20data%20acquisition%20costs%20in%20real-world%20settings%2C%20and%20even%0Aoutperform%20the%20unconstrained%20baseline%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04929v1&entry.124074799=Read"},
{"title": "Beyond Features: How Dataset Design Influences Multi-Agent Trajectory\n  Prediction Performance", "author": "Tobias Demmler and Jakob H\u00e4ringer and Andreas Tamke and Thao Dang and Alexander Hegai and Lars Mikelsons", "abstract": "  Accurate trajectory prediction is critical for safe autonomous navigation,\nyet the impact of dataset design on model performance remains understudied.\nThis work systematically examines how feature selection, cross-dataset\ntransfer, and geographic diversity influence trajectory prediction accuracy in\nmulti-agent settings. We evaluate a state-of-the-art model using our novel L4\nMotion Forecasting dataset based on our own data recordings in Germany and the\nUS. This includes enhanced map and agent features. We compare our dataset to\nthe US-centric Argoverse 2 benchmark. First, we find that incorporating\nsupplementary map and agent features unique to our dataset, yields no\nmeasurable improvement over baseline features, demonstrating that modern\narchitectures do not need extensive feature sets for optimal performance. The\nlimited features of public datasets are sufficient to capture convoluted\ninteractions without added complexity. Second, we perform cross-dataset\nexperiments to evaluate how effective domain knowledge can be transferred\nbetween datasets. Third, we group our dataset by country and check the\nknowledge transfer between different driving cultures.\n", "link": "http://arxiv.org/abs/2507.05098v1", "date": "2025-07-07", "relevancy": 2.1054, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5886}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5146}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Features%3A%20How%20Dataset%20Design%20Influences%20Multi-Agent%20Trajectory%0A%20%20Prediction%20Performance&body=Title%3A%20Beyond%20Features%3A%20How%20Dataset%20Design%20Influences%20Multi-Agent%20Trajectory%0A%20%20Prediction%20Performance%0AAuthor%3A%20Tobias%20Demmler%20and%20Jakob%20H%C3%A4ringer%20and%20Andreas%20Tamke%20and%20Thao%20Dang%20and%20Alexander%20Hegai%20and%20Lars%20Mikelsons%0AAbstract%3A%20%20%20Accurate%20trajectory%20prediction%20is%20critical%20for%20safe%20autonomous%20navigation%2C%0Ayet%20the%20impact%20of%20dataset%20design%20on%20model%20performance%20remains%20understudied.%0AThis%20work%20systematically%20examines%20how%20feature%20selection%2C%20cross-dataset%0Atransfer%2C%20and%20geographic%20diversity%20influence%20trajectory%20prediction%20accuracy%20in%0Amulti-agent%20settings.%20We%20evaluate%20a%20state-of-the-art%20model%20using%20our%20novel%20L4%0AMotion%20Forecasting%20dataset%20based%20on%20our%20own%20data%20recordings%20in%20Germany%20and%20the%0AUS.%20This%20includes%20enhanced%20map%20and%20agent%20features.%20We%20compare%20our%20dataset%20to%0Athe%20US-centric%20Argoverse%202%20benchmark.%20First%2C%20we%20find%20that%20incorporating%0Asupplementary%20map%20and%20agent%20features%20unique%20to%20our%20dataset%2C%20yields%20no%0Ameasurable%20improvement%20over%20baseline%20features%2C%20demonstrating%20that%20modern%0Aarchitectures%20do%20not%20need%20extensive%20feature%20sets%20for%20optimal%20performance.%20The%0Alimited%20features%20of%20public%20datasets%20are%20sufficient%20to%20capture%20convoluted%0Ainteractions%20without%20added%20complexity.%20Second%2C%20we%20perform%20cross-dataset%0Aexperiments%20to%20evaluate%20how%20effective%20domain%20knowledge%20can%20be%20transferred%0Abetween%20datasets.%20Third%2C%20we%20group%20our%20dataset%20by%20country%20and%20check%20the%0Aknowledge%20transfer%20between%20different%20driving%20cultures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05098v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Features%253A%2520How%2520Dataset%2520Design%2520Influences%2520Multi-Agent%2520Trajectory%250A%2520%2520Prediction%2520Performance%26entry.906535625%3DTobias%2520Demmler%2520and%2520Jakob%2520H%25C3%25A4ringer%2520and%2520Andreas%2520Tamke%2520and%2520Thao%2520Dang%2520and%2520Alexander%2520Hegai%2520and%2520Lars%2520Mikelsons%26entry.1292438233%3D%2520%2520Accurate%2520trajectory%2520prediction%2520is%2520critical%2520for%2520safe%2520autonomous%2520navigation%252C%250Ayet%2520the%2520impact%2520of%2520dataset%2520design%2520on%2520model%2520performance%2520remains%2520understudied.%250AThis%2520work%2520systematically%2520examines%2520how%2520feature%2520selection%252C%2520cross-dataset%250Atransfer%252C%2520and%2520geographic%2520diversity%2520influence%2520trajectory%2520prediction%2520accuracy%2520in%250Amulti-agent%2520settings.%2520We%2520evaluate%2520a%2520state-of-the-art%2520model%2520using%2520our%2520novel%2520L4%250AMotion%2520Forecasting%2520dataset%2520based%2520on%2520our%2520own%2520data%2520recordings%2520in%2520Germany%2520and%2520the%250AUS.%2520This%2520includes%2520enhanced%2520map%2520and%2520agent%2520features.%2520We%2520compare%2520our%2520dataset%2520to%250Athe%2520US-centric%2520Argoverse%25202%2520benchmark.%2520First%252C%2520we%2520find%2520that%2520incorporating%250Asupplementary%2520map%2520and%2520agent%2520features%2520unique%2520to%2520our%2520dataset%252C%2520yields%2520no%250Ameasurable%2520improvement%2520over%2520baseline%2520features%252C%2520demonstrating%2520that%2520modern%250Aarchitectures%2520do%2520not%2520need%2520extensive%2520feature%2520sets%2520for%2520optimal%2520performance.%2520The%250Alimited%2520features%2520of%2520public%2520datasets%2520are%2520sufficient%2520to%2520capture%2520convoluted%250Ainteractions%2520without%2520added%2520complexity.%2520Second%252C%2520we%2520perform%2520cross-dataset%250Aexperiments%2520to%2520evaluate%2520how%2520effective%2520domain%2520knowledge%2520can%2520be%2520transferred%250Abetween%2520datasets.%2520Third%252C%2520we%2520group%2520our%2520dataset%2520by%2520country%2520and%2520check%2520the%250Aknowledge%2520transfer%2520between%2520different%2520driving%2520cultures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05098v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Features%3A%20How%20Dataset%20Design%20Influences%20Multi-Agent%20Trajectory%0A%20%20Prediction%20Performance&entry.906535625=Tobias%20Demmler%20and%20Jakob%20H%C3%A4ringer%20and%20Andreas%20Tamke%20and%20Thao%20Dang%20and%20Alexander%20Hegai%20and%20Lars%20Mikelsons&entry.1292438233=%20%20Accurate%20trajectory%20prediction%20is%20critical%20for%20safe%20autonomous%20navigation%2C%0Ayet%20the%20impact%20of%20dataset%20design%20on%20model%20performance%20remains%20understudied.%0AThis%20work%20systematically%20examines%20how%20feature%20selection%2C%20cross-dataset%0Atransfer%2C%20and%20geographic%20diversity%20influence%20trajectory%20prediction%20accuracy%20in%0Amulti-agent%20settings.%20We%20evaluate%20a%20state-of-the-art%20model%20using%20our%20novel%20L4%0AMotion%20Forecasting%20dataset%20based%20on%20our%20own%20data%20recordings%20in%20Germany%20and%20the%0AUS.%20This%20includes%20enhanced%20map%20and%20agent%20features.%20We%20compare%20our%20dataset%20to%0Athe%20US-centric%20Argoverse%202%20benchmark.%20First%2C%20we%20find%20that%20incorporating%0Asupplementary%20map%20and%20agent%20features%20unique%20to%20our%20dataset%2C%20yields%20no%0Ameasurable%20improvement%20over%20baseline%20features%2C%20demonstrating%20that%20modern%0Aarchitectures%20do%20not%20need%20extensive%20feature%20sets%20for%20optimal%20performance.%20The%0Alimited%20features%20of%20public%20datasets%20are%20sufficient%20to%20capture%20convoluted%0Ainteractions%20without%20added%20complexity.%20Second%2C%20we%20perform%20cross-dataset%0Aexperiments%20to%20evaluate%20how%20effective%20domain%20knowledge%20can%20be%20transferred%0Abetween%20datasets.%20Third%2C%20we%20group%20our%20dataset%20by%20country%20and%20check%20the%0Aknowledge%20transfer%20between%20different%20driving%20cultures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05098v1&entry.124074799=Read"},
{"title": "Computation-Aware Gaussian Processes: Model Selection And Linear-Time\n  Inference", "author": "Jonathan Wenger and Kaiwen Wu and Philipp Hennig and Jacob R. Gardner and Geoff Pleiss and John P. Cunningham", "abstract": "  Model selection in Gaussian processes scales prohibitively with the size of\nthe training dataset, both in time and memory. While many approximations exist,\nall incur inevitable approximation error. Recent work accounts for this error\nin the form of computational uncertainty, which enables -- at the cost of\nquadratic complexity -- an explicit tradeoff between computation and precision.\nHere we extend this development to model selection, which requires significant\nenhancements to the existing approach, including linear-time scaling in the\nsize of the dataset. We propose a novel training loss for hyperparameter\noptimization and demonstrate empirically that the resulting method can\noutperform SGPR, CGGP and SVGP, state-of-the-art methods for GP model\nselection, on medium to large-scale datasets. Our experiments show that model\nselection for computation-aware GPs trained on 1.8 million data points can be\ndone within a few hours on a single GPU. As a result of this work, Gaussian\nprocesses can be trained on large-scale datasets without significantly\ncompromising their ability to quantify uncertainty -- a fundamental\nprerequisite for optimal decision-making.\n", "link": "http://arxiv.org/abs/2411.01036v2", "date": "2025-07-07", "relevancy": 2.097, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5281}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5246}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computation-Aware%20Gaussian%20Processes%3A%20Model%20Selection%20And%20Linear-Time%0A%20%20Inference&body=Title%3A%20Computation-Aware%20Gaussian%20Processes%3A%20Model%20Selection%20And%20Linear-Time%0A%20%20Inference%0AAuthor%3A%20Jonathan%20Wenger%20and%20Kaiwen%20Wu%20and%20Philipp%20Hennig%20and%20Jacob%20R.%20Gardner%20and%20Geoff%20Pleiss%20and%20John%20P.%20Cunningham%0AAbstract%3A%20%20%20Model%20selection%20in%20Gaussian%20processes%20scales%20prohibitively%20with%20the%20size%20of%0Athe%20training%20dataset%2C%20both%20in%20time%20and%20memory.%20While%20many%20approximations%20exist%2C%0Aall%20incur%20inevitable%20approximation%20error.%20Recent%20work%20accounts%20for%20this%20error%0Ain%20the%20form%20of%20computational%20uncertainty%2C%20which%20enables%20--%20at%20the%20cost%20of%0Aquadratic%20complexity%20--%20an%20explicit%20tradeoff%20between%20computation%20and%20precision.%0AHere%20we%20extend%20this%20development%20to%20model%20selection%2C%20which%20requires%20significant%0Aenhancements%20to%20the%20existing%20approach%2C%20including%20linear-time%20scaling%20in%20the%0Asize%20of%20the%20dataset.%20We%20propose%20a%20novel%20training%20loss%20for%20hyperparameter%0Aoptimization%20and%20demonstrate%20empirically%20that%20the%20resulting%20method%20can%0Aoutperform%20SGPR%2C%20CGGP%20and%20SVGP%2C%20state-of-the-art%20methods%20for%20GP%20model%0Aselection%2C%20on%20medium%20to%20large-scale%20datasets.%20Our%20experiments%20show%20that%20model%0Aselection%20for%20computation-aware%20GPs%20trained%20on%201.8%20million%20data%20points%20can%20be%0Adone%20within%20a%20few%20hours%20on%20a%20single%20GPU.%20As%20a%20result%20of%20this%20work%2C%20Gaussian%0Aprocesses%20can%20be%20trained%20on%20large-scale%20datasets%20without%20significantly%0Acompromising%20their%20ability%20to%20quantify%20uncertainty%20--%20a%20fundamental%0Aprerequisite%20for%20optimal%20decision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01036v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputation-Aware%2520Gaussian%2520Processes%253A%2520Model%2520Selection%2520And%2520Linear-Time%250A%2520%2520Inference%26entry.906535625%3DJonathan%2520Wenger%2520and%2520Kaiwen%2520Wu%2520and%2520Philipp%2520Hennig%2520and%2520Jacob%2520R.%2520Gardner%2520and%2520Geoff%2520Pleiss%2520and%2520John%2520P.%2520Cunningham%26entry.1292438233%3D%2520%2520Model%2520selection%2520in%2520Gaussian%2520processes%2520scales%2520prohibitively%2520with%2520the%2520size%2520of%250Athe%2520training%2520dataset%252C%2520both%2520in%2520time%2520and%2520memory.%2520While%2520many%2520approximations%2520exist%252C%250Aall%2520incur%2520inevitable%2520approximation%2520error.%2520Recent%2520work%2520accounts%2520for%2520this%2520error%250Ain%2520the%2520form%2520of%2520computational%2520uncertainty%252C%2520which%2520enables%2520--%2520at%2520the%2520cost%2520of%250Aquadratic%2520complexity%2520--%2520an%2520explicit%2520tradeoff%2520between%2520computation%2520and%2520precision.%250AHere%2520we%2520extend%2520this%2520development%2520to%2520model%2520selection%252C%2520which%2520requires%2520significant%250Aenhancements%2520to%2520the%2520existing%2520approach%252C%2520including%2520linear-time%2520scaling%2520in%2520the%250Asize%2520of%2520the%2520dataset.%2520We%2520propose%2520a%2520novel%2520training%2520loss%2520for%2520hyperparameter%250Aoptimization%2520and%2520demonstrate%2520empirically%2520that%2520the%2520resulting%2520method%2520can%250Aoutperform%2520SGPR%252C%2520CGGP%2520and%2520SVGP%252C%2520state-of-the-art%2520methods%2520for%2520GP%2520model%250Aselection%252C%2520on%2520medium%2520to%2520large-scale%2520datasets.%2520Our%2520experiments%2520show%2520that%2520model%250Aselection%2520for%2520computation-aware%2520GPs%2520trained%2520on%25201.8%2520million%2520data%2520points%2520can%2520be%250Adone%2520within%2520a%2520few%2520hours%2520on%2520a%2520single%2520GPU.%2520As%2520a%2520result%2520of%2520this%2520work%252C%2520Gaussian%250Aprocesses%2520can%2520be%2520trained%2520on%2520large-scale%2520datasets%2520without%2520significantly%250Acompromising%2520their%2520ability%2520to%2520quantify%2520uncertainty%2520--%2520a%2520fundamental%250Aprerequisite%2520for%2520optimal%2520decision-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01036v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computation-Aware%20Gaussian%20Processes%3A%20Model%20Selection%20And%20Linear-Time%0A%20%20Inference&entry.906535625=Jonathan%20Wenger%20and%20Kaiwen%20Wu%20and%20Philipp%20Hennig%20and%20Jacob%20R.%20Gardner%20and%20Geoff%20Pleiss%20and%20John%20P.%20Cunningham&entry.1292438233=%20%20Model%20selection%20in%20Gaussian%20processes%20scales%20prohibitively%20with%20the%20size%20of%0Athe%20training%20dataset%2C%20both%20in%20time%20and%20memory.%20While%20many%20approximations%20exist%2C%0Aall%20incur%20inevitable%20approximation%20error.%20Recent%20work%20accounts%20for%20this%20error%0Ain%20the%20form%20of%20computational%20uncertainty%2C%20which%20enables%20--%20at%20the%20cost%20of%0Aquadratic%20complexity%20--%20an%20explicit%20tradeoff%20between%20computation%20and%20precision.%0AHere%20we%20extend%20this%20development%20to%20model%20selection%2C%20which%20requires%20significant%0Aenhancements%20to%20the%20existing%20approach%2C%20including%20linear-time%20scaling%20in%20the%0Asize%20of%20the%20dataset.%20We%20propose%20a%20novel%20training%20loss%20for%20hyperparameter%0Aoptimization%20and%20demonstrate%20empirically%20that%20the%20resulting%20method%20can%0Aoutperform%20SGPR%2C%20CGGP%20and%20SVGP%2C%20state-of-the-art%20methods%20for%20GP%20model%0Aselection%2C%20on%20medium%20to%20large-scale%20datasets.%20Our%20experiments%20show%20that%20model%0Aselection%20for%20computation-aware%20GPs%20trained%20on%201.8%20million%20data%20points%20can%20be%0Adone%20within%20a%20few%20hours%20on%20a%20single%20GPU.%20As%20a%20result%20of%20this%20work%2C%20Gaussian%0Aprocesses%20can%20be%20trained%20on%20large-scale%20datasets%20without%20significantly%0Acompromising%20their%20ability%20to%20quantify%20uncertainty%20--%20a%20fundamental%0Aprerequisite%20for%20optimal%20decision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01036v2&entry.124074799=Read"},
{"title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I.\n  X-Master as Foundation: Can We Lead on Humanity's Last Exam?", "author": "Jingyi Chai and Shuo Tang and Rui Ye and Yuwen Du and Xinyu Zhu and Mengcheng Zhou and Yanfeng Wang and Weinan E and Siheng Chen", "abstract": "  The rapid advancements of AI agents have ignited the long-held ambition of\nleveraging them to accelerate scientific discovery. Achieving this goal\nrequires a deep understanding of the frontiers of human knowledge. As such,\nHumanity's Last Exam (HLE) provides an exceptionally challenging touchstone for\nevaluating scientific AI agents. In this work, we aim to construct the\nfoundational architecture for general-purpose agents and validate the\ncapabilities through leading performance on HLE. To achieve this, we introduce\nX-Master, a tool-augmented reasoning agent designed to emulate human\nresearchers by interacting flexibly with external tools during its reasoning\nprocess. This agent, guided by the conceptualization of code as an interaction\nlanguage, can flexibly leverage built-in Python libraries and our customized\ntools to augment the reasoning. We further scale its capabilities through\nX-Masters, a scattered-and-stacked agentic workflow that systematically\nenhances breadth and depth of reasoning. Our open-source solution, X-Masters,\nsets a new state-of-the-art record on HLE with a score of 32.1%, surpassing\nOpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to\nexceed the 30% threshold. This work allows us to gain a deeper understanding of\ncomplex task-solving and accumulates valuable experience that can inform future\nadvancements, guiding subsequent model training.\n", "link": "http://arxiv.org/abs/2507.05241v1", "date": "2025-07-07", "relevancy": 2.0952, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5518}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5307}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SciMaster%3A%20Towards%20General-Purpose%20Scientific%20AI%20Agents%2C%20Part%20I.%0A%20%20X-Master%20as%20Foundation%3A%20Can%20We%20Lead%20on%20Humanity%27s%20Last%20Exam%3F&body=Title%3A%20SciMaster%3A%20Towards%20General-Purpose%20Scientific%20AI%20Agents%2C%20Part%20I.%0A%20%20X-Master%20as%20Foundation%3A%20Can%20We%20Lead%20on%20Humanity%27s%20Last%20Exam%3F%0AAuthor%3A%20Jingyi%20Chai%20and%20Shuo%20Tang%20and%20Rui%20Ye%20and%20Yuwen%20Du%20and%20Xinyu%20Zhu%20and%20Mengcheng%20Zhou%20and%20Yanfeng%20Wang%20and%20Weinan%20E%20and%20Siheng%20Chen%0AAbstract%3A%20%20%20The%20rapid%20advancements%20of%20AI%20agents%20have%20ignited%20the%20long-held%20ambition%20of%0Aleveraging%20them%20to%20accelerate%20scientific%20discovery.%20Achieving%20this%20goal%0Arequires%20a%20deep%20understanding%20of%20the%20frontiers%20of%20human%20knowledge.%20As%20such%2C%0AHumanity%27s%20Last%20Exam%20%28HLE%29%20provides%20an%20exceptionally%20challenging%20touchstone%20for%0Aevaluating%20scientific%20AI%20agents.%20In%20this%20work%2C%20we%20aim%20to%20construct%20the%0Afoundational%20architecture%20for%20general-purpose%20agents%20and%20validate%20the%0Acapabilities%20through%20leading%20performance%20on%20HLE.%20To%20achieve%20this%2C%20we%20introduce%0AX-Master%2C%20a%20tool-augmented%20reasoning%20agent%20designed%20to%20emulate%20human%0Aresearchers%20by%20interacting%20flexibly%20with%20external%20tools%20during%20its%20reasoning%0Aprocess.%20This%20agent%2C%20guided%20by%20the%20conceptualization%20of%20code%20as%20an%20interaction%0Alanguage%2C%20can%20flexibly%20leverage%20built-in%20Python%20libraries%20and%20our%20customized%0Atools%20to%20augment%20the%20reasoning.%20We%20further%20scale%20its%20capabilities%20through%0AX-Masters%2C%20a%20scattered-and-stacked%20agentic%20workflow%20that%20systematically%0Aenhances%20breadth%20and%20depth%20of%20reasoning.%20Our%20open-source%20solution%2C%20X-Masters%2C%0Asets%20a%20new%20state-of-the-art%20record%20on%20HLE%20with%20a%20score%20of%2032.1%25%2C%20surpassing%0AOpenAI%27s%20and%20Google%27s%20Deep%20Research%20%2826.6%25%20and%2026.9%25%29%20and%20becoming%20the%20first%20to%0Aexceed%20the%2030%25%20threshold.%20This%20work%20allows%20us%20to%20gain%20a%20deeper%20understanding%20of%0Acomplex%20task-solving%20and%20accumulates%20valuable%20experience%20that%20can%20inform%20future%0Aadvancements%2C%20guiding%20subsequent%20model%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05241v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSciMaster%253A%2520Towards%2520General-Purpose%2520Scientific%2520AI%2520Agents%252C%2520Part%2520I.%250A%2520%2520X-Master%2520as%2520Foundation%253A%2520Can%2520We%2520Lead%2520on%2520Humanity%2527s%2520Last%2520Exam%253F%26entry.906535625%3DJingyi%2520Chai%2520and%2520Shuo%2520Tang%2520and%2520Rui%2520Ye%2520and%2520Yuwen%2520Du%2520and%2520Xinyu%2520Zhu%2520and%2520Mengcheng%2520Zhou%2520and%2520Yanfeng%2520Wang%2520and%2520Weinan%2520E%2520and%2520Siheng%2520Chen%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancements%2520of%2520AI%2520agents%2520have%2520ignited%2520the%2520long-held%2520ambition%2520of%250Aleveraging%2520them%2520to%2520accelerate%2520scientific%2520discovery.%2520Achieving%2520this%2520goal%250Arequires%2520a%2520deep%2520understanding%2520of%2520the%2520frontiers%2520of%2520human%2520knowledge.%2520As%2520such%252C%250AHumanity%2527s%2520Last%2520Exam%2520%2528HLE%2529%2520provides%2520an%2520exceptionally%2520challenging%2520touchstone%2520for%250Aevaluating%2520scientific%2520AI%2520agents.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520construct%2520the%250Afoundational%2520architecture%2520for%2520general-purpose%2520agents%2520and%2520validate%2520the%250Acapabilities%2520through%2520leading%2520performance%2520on%2520HLE.%2520To%2520achieve%2520this%252C%2520we%2520introduce%250AX-Master%252C%2520a%2520tool-augmented%2520reasoning%2520agent%2520designed%2520to%2520emulate%2520human%250Aresearchers%2520by%2520interacting%2520flexibly%2520with%2520external%2520tools%2520during%2520its%2520reasoning%250Aprocess.%2520This%2520agent%252C%2520guided%2520by%2520the%2520conceptualization%2520of%2520code%2520as%2520an%2520interaction%250Alanguage%252C%2520can%2520flexibly%2520leverage%2520built-in%2520Python%2520libraries%2520and%2520our%2520customized%250Atools%2520to%2520augment%2520the%2520reasoning.%2520We%2520further%2520scale%2520its%2520capabilities%2520through%250AX-Masters%252C%2520a%2520scattered-and-stacked%2520agentic%2520workflow%2520that%2520systematically%250Aenhances%2520breadth%2520and%2520depth%2520of%2520reasoning.%2520Our%2520open-source%2520solution%252C%2520X-Masters%252C%250Asets%2520a%2520new%2520state-of-the-art%2520record%2520on%2520HLE%2520with%2520a%2520score%2520of%252032.1%2525%252C%2520surpassing%250AOpenAI%2527s%2520and%2520Google%2527s%2520Deep%2520Research%2520%252826.6%2525%2520and%252026.9%2525%2529%2520and%2520becoming%2520the%2520first%2520to%250Aexceed%2520the%252030%2525%2520threshold.%2520This%2520work%2520allows%2520us%2520to%2520gain%2520a%2520deeper%2520understanding%2520of%250Acomplex%2520task-solving%2520and%2520accumulates%2520valuable%2520experience%2520that%2520can%2520inform%2520future%250Aadvancements%252C%2520guiding%2520subsequent%2520model%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05241v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SciMaster%3A%20Towards%20General-Purpose%20Scientific%20AI%20Agents%2C%20Part%20I.%0A%20%20X-Master%20as%20Foundation%3A%20Can%20We%20Lead%20on%20Humanity%27s%20Last%20Exam%3F&entry.906535625=Jingyi%20Chai%20and%20Shuo%20Tang%20and%20Rui%20Ye%20and%20Yuwen%20Du%20and%20Xinyu%20Zhu%20and%20Mengcheng%20Zhou%20and%20Yanfeng%20Wang%20and%20Weinan%20E%20and%20Siheng%20Chen&entry.1292438233=%20%20The%20rapid%20advancements%20of%20AI%20agents%20have%20ignited%20the%20long-held%20ambition%20of%0Aleveraging%20them%20to%20accelerate%20scientific%20discovery.%20Achieving%20this%20goal%0Arequires%20a%20deep%20understanding%20of%20the%20frontiers%20of%20human%20knowledge.%20As%20such%2C%0AHumanity%27s%20Last%20Exam%20%28HLE%29%20provides%20an%20exceptionally%20challenging%20touchstone%20for%0Aevaluating%20scientific%20AI%20agents.%20In%20this%20work%2C%20we%20aim%20to%20construct%20the%0Afoundational%20architecture%20for%20general-purpose%20agents%20and%20validate%20the%0Acapabilities%20through%20leading%20performance%20on%20HLE.%20To%20achieve%20this%2C%20we%20introduce%0AX-Master%2C%20a%20tool-augmented%20reasoning%20agent%20designed%20to%20emulate%20human%0Aresearchers%20by%20interacting%20flexibly%20with%20external%20tools%20during%20its%20reasoning%0Aprocess.%20This%20agent%2C%20guided%20by%20the%20conceptualization%20of%20code%20as%20an%20interaction%0Alanguage%2C%20can%20flexibly%20leverage%20built-in%20Python%20libraries%20and%20our%20customized%0Atools%20to%20augment%20the%20reasoning.%20We%20further%20scale%20its%20capabilities%20through%0AX-Masters%2C%20a%20scattered-and-stacked%20agentic%20workflow%20that%20systematically%0Aenhances%20breadth%20and%20depth%20of%20reasoning.%20Our%20open-source%20solution%2C%20X-Masters%2C%0Asets%20a%20new%20state-of-the-art%20record%20on%20HLE%20with%20a%20score%20of%2032.1%25%2C%20surpassing%0AOpenAI%27s%20and%20Google%27s%20Deep%20Research%20%2826.6%25%20and%2026.9%25%29%20and%20becoming%20the%20first%20to%0Aexceed%20the%2030%25%20threshold.%20This%20work%20allows%20us%20to%20gain%20a%20deeper%20understanding%20of%0Acomplex%20task-solving%20and%20accumulates%20valuable%20experience%20that%20can%20inform%20future%0Aadvancements%2C%20guiding%20subsequent%20model%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05241v1&entry.124074799=Read"},
{"title": "LaCoOT: Layer Collapse through Optimal Transport", "author": "Victor Qu\u00e9tu and Nour Hezbri and Enzo Tartaglione", "abstract": "  Although deep neural networks are well-known for their outstanding\nperformance in tackling complex tasks, their hunger for computational resources\nremains a significant hurdle, posing energy-consumption issues and restricting\ntheir deployment on resource-constrained devices, preventing their widespread\nadoption. In this paper, we present an optimal transport-based method to reduce\nthe depth of over-parametrized deep neural networks, alleviating their\ncomputational burden. More specifically, we propose a new regularization\nstrategy based on the Max-Sliced Wasserstein distance to minimize the distance\nbetween the intermediate feature distributions in the neural network. We show\nthat minimizing this distance enables the complete removal of intermediate\nlayers in the network, achieving better performance/depth trade-off compared to\nexisting techniques. We assess the effectiveness of our method on traditional\nimage classification setups and extend it to generative image models. Our code\nis available at https://github.com/VGCQ/LaCoOT.\n", "link": "http://arxiv.org/abs/2406.08933v2", "date": "2025-07-07", "relevancy": 2.0888, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5402}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5215}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaCoOT%3A%20Layer%20Collapse%20through%20Optimal%20Transport&body=Title%3A%20LaCoOT%3A%20Layer%20Collapse%20through%20Optimal%20Transport%0AAuthor%3A%20Victor%20Qu%C3%A9tu%20and%20Nour%20Hezbri%20and%20Enzo%20Tartaglione%0AAbstract%3A%20%20%20Although%20deep%20neural%20networks%20are%20well-known%20for%20their%20outstanding%0Aperformance%20in%20tackling%20complex%20tasks%2C%20their%20hunger%20for%20computational%20resources%0Aremains%20a%20significant%20hurdle%2C%20posing%20energy-consumption%20issues%20and%20restricting%0Atheir%20deployment%20on%20resource-constrained%20devices%2C%20preventing%20their%20widespread%0Aadoption.%20In%20this%20paper%2C%20we%20present%20an%20optimal%20transport-based%20method%20to%20reduce%0Athe%20depth%20of%20over-parametrized%20deep%20neural%20networks%2C%20alleviating%20their%0Acomputational%20burden.%20More%20specifically%2C%20we%20propose%20a%20new%20regularization%0Astrategy%20based%20on%20the%20Max-Sliced%20Wasserstein%20distance%20to%20minimize%20the%20distance%0Abetween%20the%20intermediate%20feature%20distributions%20in%20the%20neural%20network.%20We%20show%0Athat%20minimizing%20this%20distance%20enables%20the%20complete%20removal%20of%20intermediate%0Alayers%20in%20the%20network%2C%20achieving%20better%20performance/depth%20trade-off%20compared%20to%0Aexisting%20techniques.%20We%20assess%20the%20effectiveness%20of%20our%20method%20on%20traditional%0Aimage%20classification%20setups%20and%20extend%20it%20to%20generative%20image%20models.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/VGCQ/LaCoOT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08933v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaCoOT%253A%2520Layer%2520Collapse%2520through%2520Optimal%2520Transport%26entry.906535625%3DVictor%2520Qu%25C3%25A9tu%2520and%2520Nour%2520Hezbri%2520and%2520Enzo%2520Tartaglione%26entry.1292438233%3D%2520%2520Although%2520deep%2520neural%2520networks%2520are%2520well-known%2520for%2520their%2520outstanding%250Aperformance%2520in%2520tackling%2520complex%2520tasks%252C%2520their%2520hunger%2520for%2520computational%2520resources%250Aremains%2520a%2520significant%2520hurdle%252C%2520posing%2520energy-consumption%2520issues%2520and%2520restricting%250Atheir%2520deployment%2520on%2520resource-constrained%2520devices%252C%2520preventing%2520their%2520widespread%250Aadoption.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520optimal%2520transport-based%2520method%2520to%2520reduce%250Athe%2520depth%2520of%2520over-parametrized%2520deep%2520neural%2520networks%252C%2520alleviating%2520their%250Acomputational%2520burden.%2520More%2520specifically%252C%2520we%2520propose%2520a%2520new%2520regularization%250Astrategy%2520based%2520on%2520the%2520Max-Sliced%2520Wasserstein%2520distance%2520to%2520minimize%2520the%2520distance%250Abetween%2520the%2520intermediate%2520feature%2520distributions%2520in%2520the%2520neural%2520network.%2520We%2520show%250Athat%2520minimizing%2520this%2520distance%2520enables%2520the%2520complete%2520removal%2520of%2520intermediate%250Alayers%2520in%2520the%2520network%252C%2520achieving%2520better%2520performance/depth%2520trade-off%2520compared%2520to%250Aexisting%2520techniques.%2520We%2520assess%2520the%2520effectiveness%2520of%2520our%2520method%2520on%2520traditional%250Aimage%2520classification%2520setups%2520and%2520extend%2520it%2520to%2520generative%2520image%2520models.%2520Our%2520code%250Ais%2520available%2520at%2520https%253A//github.com/VGCQ/LaCoOT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08933v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaCoOT%3A%20Layer%20Collapse%20through%20Optimal%20Transport&entry.906535625=Victor%20Qu%C3%A9tu%20and%20Nour%20Hezbri%20and%20Enzo%20Tartaglione&entry.1292438233=%20%20Although%20deep%20neural%20networks%20are%20well-known%20for%20their%20outstanding%0Aperformance%20in%20tackling%20complex%20tasks%2C%20their%20hunger%20for%20computational%20resources%0Aremains%20a%20significant%20hurdle%2C%20posing%20energy-consumption%20issues%20and%20restricting%0Atheir%20deployment%20on%20resource-constrained%20devices%2C%20preventing%20their%20widespread%0Aadoption.%20In%20this%20paper%2C%20we%20present%20an%20optimal%20transport-based%20method%20to%20reduce%0Athe%20depth%20of%20over-parametrized%20deep%20neural%20networks%2C%20alleviating%20their%0Acomputational%20burden.%20More%20specifically%2C%20we%20propose%20a%20new%20regularization%0Astrategy%20based%20on%20the%20Max-Sliced%20Wasserstein%20distance%20to%20minimize%20the%20distance%0Abetween%20the%20intermediate%20feature%20distributions%20in%20the%20neural%20network.%20We%20show%0Athat%20minimizing%20this%20distance%20enables%20the%20complete%20removal%20of%20intermediate%0Alayers%20in%20the%20network%2C%20achieving%20better%20performance/depth%20trade-off%20compared%20to%0Aexisting%20techniques.%20We%20assess%20the%20effectiveness%20of%20our%20method%20on%20traditional%0Aimage%20classification%20setups%20and%20extend%20it%20to%20generative%20image%20models.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/VGCQ/LaCoOT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08933v2&entry.124074799=Read"},
{"title": "A Generative Diffusion Model for Amorphous Materials", "author": "Kai Yang and Daniel Schwalbe-Koda", "abstract": "  Generative models show great promise for the inverse design of molecules and\ninorganic crystals, but remain largely ineffective within more complex\nstructures such as amorphous materials. Here, we present a diffusion model that\nreliably generates amorphous structures up to 1000 times faster than\nconventional simulations across processing conditions, compositions, and data\nsources. Generated structures recovered the short- and medium-range order,\nsampling diversity, and macroscopic properties of silica glass, as validated by\nsimulations and an information-theoretical strategy. Conditional generation\nallowed sampling large structures at low cooling rates of 10$^{-2}$ K/ps to\nuncover a ductile-to-brittle transition and mesoporous silica structures.\nExtension to metallic glassy systems accurately reproduced local structures and\nproperties from both computational and experimental datasets, demonstrating how\nsynthetic data can be generated from characterization results. Our methods\nprovide a roadmap for the design and simulation of amorphous materials\npreviously inaccessible to computational methods.\n", "link": "http://arxiv.org/abs/2507.05024v1", "date": "2025-07-07", "relevancy": 2.0822, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5214}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5214}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Generative%20Diffusion%20Model%20for%20Amorphous%20Materials&body=Title%3A%20A%20Generative%20Diffusion%20Model%20for%20Amorphous%20Materials%0AAuthor%3A%20Kai%20Yang%20and%20Daniel%20Schwalbe-Koda%0AAbstract%3A%20%20%20Generative%20models%20show%20great%20promise%20for%20the%20inverse%20design%20of%20molecules%20and%0Ainorganic%20crystals%2C%20but%20remain%20largely%20ineffective%20within%20more%20complex%0Astructures%20such%20as%20amorphous%20materials.%20Here%2C%20we%20present%20a%20diffusion%20model%20that%0Areliably%20generates%20amorphous%20structures%20up%20to%201000%20times%20faster%20than%0Aconventional%20simulations%20across%20processing%20conditions%2C%20compositions%2C%20and%20data%0Asources.%20Generated%20structures%20recovered%20the%20short-%20and%20medium-range%20order%2C%0Asampling%20diversity%2C%20and%20macroscopic%20properties%20of%20silica%20glass%2C%20as%20validated%20by%0Asimulations%20and%20an%20information-theoretical%20strategy.%20Conditional%20generation%0Aallowed%20sampling%20large%20structures%20at%20low%20cooling%20rates%20of%2010%24%5E%7B-2%7D%24%20K/ps%20to%0Auncover%20a%20ductile-to-brittle%20transition%20and%20mesoporous%20silica%20structures.%0AExtension%20to%20metallic%20glassy%20systems%20accurately%20reproduced%20local%20structures%20and%0Aproperties%20from%20both%20computational%20and%20experimental%20datasets%2C%20demonstrating%20how%0Asynthetic%20data%20can%20be%20generated%20from%20characterization%20results.%20Our%20methods%0Aprovide%20a%20roadmap%20for%20the%20design%20and%20simulation%20of%20amorphous%20materials%0Apreviously%20inaccessible%20to%20computational%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05024v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Generative%2520Diffusion%2520Model%2520for%2520Amorphous%2520Materials%26entry.906535625%3DKai%2520Yang%2520and%2520Daniel%2520Schwalbe-Koda%26entry.1292438233%3D%2520%2520Generative%2520models%2520show%2520great%2520promise%2520for%2520the%2520inverse%2520design%2520of%2520molecules%2520and%250Ainorganic%2520crystals%252C%2520but%2520remain%2520largely%2520ineffective%2520within%2520more%2520complex%250Astructures%2520such%2520as%2520amorphous%2520materials.%2520Here%252C%2520we%2520present%2520a%2520diffusion%2520model%2520that%250Areliably%2520generates%2520amorphous%2520structures%2520up%2520to%25201000%2520times%2520faster%2520than%250Aconventional%2520simulations%2520across%2520processing%2520conditions%252C%2520compositions%252C%2520and%2520data%250Asources.%2520Generated%2520structures%2520recovered%2520the%2520short-%2520and%2520medium-range%2520order%252C%250Asampling%2520diversity%252C%2520and%2520macroscopic%2520properties%2520of%2520silica%2520glass%252C%2520as%2520validated%2520by%250Asimulations%2520and%2520an%2520information-theoretical%2520strategy.%2520Conditional%2520generation%250Aallowed%2520sampling%2520large%2520structures%2520at%2520low%2520cooling%2520rates%2520of%252010%2524%255E%257B-2%257D%2524%2520K/ps%2520to%250Auncover%2520a%2520ductile-to-brittle%2520transition%2520and%2520mesoporous%2520silica%2520structures.%250AExtension%2520to%2520metallic%2520glassy%2520systems%2520accurately%2520reproduced%2520local%2520structures%2520and%250Aproperties%2520from%2520both%2520computational%2520and%2520experimental%2520datasets%252C%2520demonstrating%2520how%250Asynthetic%2520data%2520can%2520be%2520generated%2520from%2520characterization%2520results.%2520Our%2520methods%250Aprovide%2520a%2520roadmap%2520for%2520the%2520design%2520and%2520simulation%2520of%2520amorphous%2520materials%250Apreviously%2520inaccessible%2520to%2520computational%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05024v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Generative%20Diffusion%20Model%20for%20Amorphous%20Materials&entry.906535625=Kai%20Yang%20and%20Daniel%20Schwalbe-Koda&entry.1292438233=%20%20Generative%20models%20show%20great%20promise%20for%20the%20inverse%20design%20of%20molecules%20and%0Ainorganic%20crystals%2C%20but%20remain%20largely%20ineffective%20within%20more%20complex%0Astructures%20such%20as%20amorphous%20materials.%20Here%2C%20we%20present%20a%20diffusion%20model%20that%0Areliably%20generates%20amorphous%20structures%20up%20to%201000%20times%20faster%20than%0Aconventional%20simulations%20across%20processing%20conditions%2C%20compositions%2C%20and%20data%0Asources.%20Generated%20structures%20recovered%20the%20short-%20and%20medium-range%20order%2C%0Asampling%20diversity%2C%20and%20macroscopic%20properties%20of%20silica%20glass%2C%20as%20validated%20by%0Asimulations%20and%20an%20information-theoretical%20strategy.%20Conditional%20generation%0Aallowed%20sampling%20large%20structures%20at%20low%20cooling%20rates%20of%2010%24%5E%7B-2%7D%24%20K/ps%20to%0Auncover%20a%20ductile-to-brittle%20transition%20and%20mesoporous%20silica%20structures.%0AExtension%20to%20metallic%20glassy%20systems%20accurately%20reproduced%20local%20structures%20and%0Aproperties%20from%20both%20computational%20and%20experimental%20datasets%2C%20demonstrating%20how%0Asynthetic%20data%20can%20be%20generated%20from%20characterization%20results.%20Our%20methods%0Aprovide%20a%20roadmap%20for%20the%20design%20and%20simulation%20of%20amorphous%20materials%0Apreviously%20inaccessible%20to%20computational%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05024v1&entry.124074799=Read"},
{"title": "$\\varphi$-Adapt: A Physics-Informed Adaptation Learning Approach to 2D\n  Quantum Material Discovery", "author": "Hoang-Quan Nguyen and Xuan Bac Nguyen and Sankalp Pandey and Tim Faltermeier and Nicholas Borys and Hugh Churchill and Khoa Luu", "abstract": "  Characterizing quantum flakes is a critical step in quantum hardware\nengineering because the quality of these flakes directly influences qubit\nperformance. Although computer vision methods for identifying two-dimensional\nquantum flakes have emerged, they still face significant challenges in\nestimating flake thickness. These challenges include limited data, poor\ngeneralization, sensitivity to domain shifts, and a lack of physical\ninterpretability. In this paper, we introduce one of the first Physics-informed\nAdaptation Learning approaches to overcome these obstacles. We focus on two\nmain issues, i.e., data scarcity and generalization. First, we propose a new\nsynthetic data generation framework that produces diverse quantum flake samples\nacross various materials and configurations, reducing the need for\ntime-consuming manual collection. Second, we present $\\varphi$-Adapt, a\nphysics-informed adaptation method that bridges the performance gap between\nmodels trained on synthetic data and those deployed in real-world settings.\nExperimental results show that our approach achieves state-of-the-art\nperformance on multiple benchmarks, outperforming existing methods. Our\nproposed approach advances the integration of physics-based modeling and domain\nadaptation. It also addresses a critical gap in leveraging synthesized data for\nreal-world 2D material analysis, offering impactful tools for deep learning and\nmaterials science communities.\n", "link": "http://arxiv.org/abs/2507.05184v1", "date": "2025-07-07", "relevancy": 2.0817, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5368}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5323}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%5Cvarphi%24-Adapt%3A%20A%20Physics-Informed%20Adaptation%20Learning%20Approach%20to%202D%0A%20%20Quantum%20Material%20Discovery&body=Title%3A%20%24%5Cvarphi%24-Adapt%3A%20A%20Physics-Informed%20Adaptation%20Learning%20Approach%20to%202D%0A%20%20Quantum%20Material%20Discovery%0AAuthor%3A%20Hoang-Quan%20Nguyen%20and%20Xuan%20Bac%20Nguyen%20and%20Sankalp%20Pandey%20and%20Tim%20Faltermeier%20and%20Nicholas%20Borys%20and%20Hugh%20Churchill%20and%20Khoa%20Luu%0AAbstract%3A%20%20%20Characterizing%20quantum%20flakes%20is%20a%20critical%20step%20in%20quantum%20hardware%0Aengineering%20because%20the%20quality%20of%20these%20flakes%20directly%20influences%20qubit%0Aperformance.%20Although%20computer%20vision%20methods%20for%20identifying%20two-dimensional%0Aquantum%20flakes%20have%20emerged%2C%20they%20still%20face%20significant%20challenges%20in%0Aestimating%20flake%20thickness.%20These%20challenges%20include%20limited%20data%2C%20poor%0Ageneralization%2C%20sensitivity%20to%20domain%20shifts%2C%20and%20a%20lack%20of%20physical%0Ainterpretability.%20In%20this%20paper%2C%20we%20introduce%20one%20of%20the%20first%20Physics-informed%0AAdaptation%20Learning%20approaches%20to%20overcome%20these%20obstacles.%20We%20focus%20on%20two%0Amain%20issues%2C%20i.e.%2C%20data%20scarcity%20and%20generalization.%20First%2C%20we%20propose%20a%20new%0Asynthetic%20data%20generation%20framework%20that%20produces%20diverse%20quantum%20flake%20samples%0Aacross%20various%20materials%20and%20configurations%2C%20reducing%20the%20need%20for%0Atime-consuming%20manual%20collection.%20Second%2C%20we%20present%20%24%5Cvarphi%24-Adapt%2C%20a%0Aphysics-informed%20adaptation%20method%20that%20bridges%20the%20performance%20gap%20between%0Amodels%20trained%20on%20synthetic%20data%20and%20those%20deployed%20in%20real-world%20settings.%0AExperimental%20results%20show%20that%20our%20approach%20achieves%20state-of-the-art%0Aperformance%20on%20multiple%20benchmarks%2C%20outperforming%20existing%20methods.%20Our%0Aproposed%20approach%20advances%20the%20integration%20of%20physics-based%20modeling%20and%20domain%0Aadaptation.%20It%20also%20addresses%20a%20critical%20gap%20in%20leveraging%20synthesized%20data%20for%0Areal-world%202D%20material%20analysis%2C%20offering%20impactful%20tools%20for%20deep%20learning%20and%0Amaterials%20science%20communities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05184v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%255Cvarphi%2524-Adapt%253A%2520A%2520Physics-Informed%2520Adaptation%2520Learning%2520Approach%2520to%25202D%250A%2520%2520Quantum%2520Material%2520Discovery%26entry.906535625%3DHoang-Quan%2520Nguyen%2520and%2520Xuan%2520Bac%2520Nguyen%2520and%2520Sankalp%2520Pandey%2520and%2520Tim%2520Faltermeier%2520and%2520Nicholas%2520Borys%2520and%2520Hugh%2520Churchill%2520and%2520Khoa%2520Luu%26entry.1292438233%3D%2520%2520Characterizing%2520quantum%2520flakes%2520is%2520a%2520critical%2520step%2520in%2520quantum%2520hardware%250Aengineering%2520because%2520the%2520quality%2520of%2520these%2520flakes%2520directly%2520influences%2520qubit%250Aperformance.%2520Although%2520computer%2520vision%2520methods%2520for%2520identifying%2520two-dimensional%250Aquantum%2520flakes%2520have%2520emerged%252C%2520they%2520still%2520face%2520significant%2520challenges%2520in%250Aestimating%2520flake%2520thickness.%2520These%2520challenges%2520include%2520limited%2520data%252C%2520poor%250Ageneralization%252C%2520sensitivity%2520to%2520domain%2520shifts%252C%2520and%2520a%2520lack%2520of%2520physical%250Ainterpretability.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520one%2520of%2520the%2520first%2520Physics-informed%250AAdaptation%2520Learning%2520approaches%2520to%2520overcome%2520these%2520obstacles.%2520We%2520focus%2520on%2520two%250Amain%2520issues%252C%2520i.e.%252C%2520data%2520scarcity%2520and%2520generalization.%2520First%252C%2520we%2520propose%2520a%2520new%250Asynthetic%2520data%2520generation%2520framework%2520that%2520produces%2520diverse%2520quantum%2520flake%2520samples%250Aacross%2520various%2520materials%2520and%2520configurations%252C%2520reducing%2520the%2520need%2520for%250Atime-consuming%2520manual%2520collection.%2520Second%252C%2520we%2520present%2520%2524%255Cvarphi%2524-Adapt%252C%2520a%250Aphysics-informed%2520adaptation%2520method%2520that%2520bridges%2520the%2520performance%2520gap%2520between%250Amodels%2520trained%2520on%2520synthetic%2520data%2520and%2520those%2520deployed%2520in%2520real-world%2520settings.%250AExperimental%2520results%2520show%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520multiple%2520benchmarks%252C%2520outperforming%2520existing%2520methods.%2520Our%250Aproposed%2520approach%2520advances%2520the%2520integration%2520of%2520physics-based%2520modeling%2520and%2520domain%250Aadaptation.%2520It%2520also%2520addresses%2520a%2520critical%2520gap%2520in%2520leveraging%2520synthesized%2520data%2520for%250Areal-world%25202D%2520material%2520analysis%252C%2520offering%2520impactful%2520tools%2520for%2520deep%2520learning%2520and%250Amaterials%2520science%2520communities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05184v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%5Cvarphi%24-Adapt%3A%20A%20Physics-Informed%20Adaptation%20Learning%20Approach%20to%202D%0A%20%20Quantum%20Material%20Discovery&entry.906535625=Hoang-Quan%20Nguyen%20and%20Xuan%20Bac%20Nguyen%20and%20Sankalp%20Pandey%20and%20Tim%20Faltermeier%20and%20Nicholas%20Borys%20and%20Hugh%20Churchill%20and%20Khoa%20Luu&entry.1292438233=%20%20Characterizing%20quantum%20flakes%20is%20a%20critical%20step%20in%20quantum%20hardware%0Aengineering%20because%20the%20quality%20of%20these%20flakes%20directly%20influences%20qubit%0Aperformance.%20Although%20computer%20vision%20methods%20for%20identifying%20two-dimensional%0Aquantum%20flakes%20have%20emerged%2C%20they%20still%20face%20significant%20challenges%20in%0Aestimating%20flake%20thickness.%20These%20challenges%20include%20limited%20data%2C%20poor%0Ageneralization%2C%20sensitivity%20to%20domain%20shifts%2C%20and%20a%20lack%20of%20physical%0Ainterpretability.%20In%20this%20paper%2C%20we%20introduce%20one%20of%20the%20first%20Physics-informed%0AAdaptation%20Learning%20approaches%20to%20overcome%20these%20obstacles.%20We%20focus%20on%20two%0Amain%20issues%2C%20i.e.%2C%20data%20scarcity%20and%20generalization.%20First%2C%20we%20propose%20a%20new%0Asynthetic%20data%20generation%20framework%20that%20produces%20diverse%20quantum%20flake%20samples%0Aacross%20various%20materials%20and%20configurations%2C%20reducing%20the%20need%20for%0Atime-consuming%20manual%20collection.%20Second%2C%20we%20present%20%24%5Cvarphi%24-Adapt%2C%20a%0Aphysics-informed%20adaptation%20method%20that%20bridges%20the%20performance%20gap%20between%0Amodels%20trained%20on%20synthetic%20data%20and%20those%20deployed%20in%20real-world%20settings.%0AExperimental%20results%20show%20that%20our%20approach%20achieves%20state-of-the-art%0Aperformance%20on%20multiple%20benchmarks%2C%20outperforming%20existing%20methods.%20Our%0Aproposed%20approach%20advances%20the%20integration%20of%20physics-based%20modeling%20and%20domain%0Aadaptation.%20It%20also%20addresses%20a%20critical%20gap%20in%20leveraging%20synthesized%20data%20for%0Areal-world%202D%20material%20analysis%2C%20offering%20impactful%20tools%20for%20deep%20learning%20and%0Amaterials%20science%20communities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05184v1&entry.124074799=Read"},
{"title": "Meta-Learning Transformers to Improve In-Context Generalization", "author": "Lorenzo Braccaioli and Anna Vettoruzzo and Prabhant Singh and Joaquin Vanschoren and Mohamed-Rafik Bouguelia and Nicola Conci", "abstract": "  In-context learning enables transformer models to generalize to new tasks\nbased solely on input prompts, without any need for weight updates. However,\nexisting training paradigms typically rely on large, unstructured datasets that\nare costly to store, difficult to evaluate for quality and balance, and pose\nprivacy and ethical concerns due to the inclusion of sensitive information.\nMotivated by these limitations and risks, we propose an alternative training\nstrategy where we leverage a collection of multiple, small-scale, and\ndomain-specific datasets. We empirically demonstrate that the increased quality\nand diversity of such data improve the generalization abilities of in-context\nlearners beyond their training domain, while achieving comparable performance\nwith models trained on a single large-scale dataset. We investigate this\nparadigm by leveraging meta-learning to train an in-context learner on the\nMeta-Album collection under several settings. Firstly, we show the performance\nin a controlled environment, where the test domain is completely excluded from\nthe training knowledge. Secondly, we explore the robustness of these models to\nforgetting in a continual scenario where the information is accessible for a\nlimited time. Finally, we explore the more challenging unsupervised scenario.\nOur findings demonstrate that transformers still generalize for in-context\nprediction when trained on a curated dataset collection while offering\nadvantages in modularity and replaceability.\n", "link": "http://arxiv.org/abs/2507.05019v1", "date": "2025-07-07", "relevancy": 2.0744, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.522}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5168}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-Learning%20Transformers%20to%20Improve%20In-Context%20Generalization&body=Title%3A%20Meta-Learning%20Transformers%20to%20Improve%20In-Context%20Generalization%0AAuthor%3A%20Lorenzo%20Braccaioli%20and%20Anna%20Vettoruzzo%20and%20Prabhant%20Singh%20and%20Joaquin%20Vanschoren%20and%20Mohamed-Rafik%20Bouguelia%20and%20Nicola%20Conci%0AAbstract%3A%20%20%20In-context%20learning%20enables%20transformer%20models%20to%20generalize%20to%20new%20tasks%0Abased%20solely%20on%20input%20prompts%2C%20without%20any%20need%20for%20weight%20updates.%20However%2C%0Aexisting%20training%20paradigms%20typically%20rely%20on%20large%2C%20unstructured%20datasets%20that%0Aare%20costly%20to%20store%2C%20difficult%20to%20evaluate%20for%20quality%20and%20balance%2C%20and%20pose%0Aprivacy%20and%20ethical%20concerns%20due%20to%20the%20inclusion%20of%20sensitive%20information.%0AMotivated%20by%20these%20limitations%20and%20risks%2C%20we%20propose%20an%20alternative%20training%0Astrategy%20where%20we%20leverage%20a%20collection%20of%20multiple%2C%20small-scale%2C%20and%0Adomain-specific%20datasets.%20We%20empirically%20demonstrate%20that%20the%20increased%20quality%0Aand%20diversity%20of%20such%20data%20improve%20the%20generalization%20abilities%20of%20in-context%0Alearners%20beyond%20their%20training%20domain%2C%20while%20achieving%20comparable%20performance%0Awith%20models%20trained%20on%20a%20single%20large-scale%20dataset.%20We%20investigate%20this%0Aparadigm%20by%20leveraging%20meta-learning%20to%20train%20an%20in-context%20learner%20on%20the%0AMeta-Album%20collection%20under%20several%20settings.%20Firstly%2C%20we%20show%20the%20performance%0Ain%20a%20controlled%20environment%2C%20where%20the%20test%20domain%20is%20completely%20excluded%20from%0Athe%20training%20knowledge.%20Secondly%2C%20we%20explore%20the%20robustness%20of%20these%20models%20to%0Aforgetting%20in%20a%20continual%20scenario%20where%20the%20information%20is%20accessible%20for%20a%0Alimited%20time.%20Finally%2C%20we%20explore%20the%20more%20challenging%20unsupervised%20scenario.%0AOur%20findings%20demonstrate%20that%20transformers%20still%20generalize%20for%20in-context%0Aprediction%20when%20trained%20on%20a%20curated%20dataset%20collection%20while%20offering%0Aadvantages%20in%20modularity%20and%20replaceability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-Learning%2520Transformers%2520to%2520Improve%2520In-Context%2520Generalization%26entry.906535625%3DLorenzo%2520Braccaioli%2520and%2520Anna%2520Vettoruzzo%2520and%2520Prabhant%2520Singh%2520and%2520Joaquin%2520Vanschoren%2520and%2520Mohamed-Rafik%2520Bouguelia%2520and%2520Nicola%2520Conci%26entry.1292438233%3D%2520%2520In-context%2520learning%2520enables%2520transformer%2520models%2520to%2520generalize%2520to%2520new%2520tasks%250Abased%2520solely%2520on%2520input%2520prompts%252C%2520without%2520any%2520need%2520for%2520weight%2520updates.%2520However%252C%250Aexisting%2520training%2520paradigms%2520typically%2520rely%2520on%2520large%252C%2520unstructured%2520datasets%2520that%250Aare%2520costly%2520to%2520store%252C%2520difficult%2520to%2520evaluate%2520for%2520quality%2520and%2520balance%252C%2520and%2520pose%250Aprivacy%2520and%2520ethical%2520concerns%2520due%2520to%2520the%2520inclusion%2520of%2520sensitive%2520information.%250AMotivated%2520by%2520these%2520limitations%2520and%2520risks%252C%2520we%2520propose%2520an%2520alternative%2520training%250Astrategy%2520where%2520we%2520leverage%2520a%2520collection%2520of%2520multiple%252C%2520small-scale%252C%2520and%250Adomain-specific%2520datasets.%2520We%2520empirically%2520demonstrate%2520that%2520the%2520increased%2520quality%250Aand%2520diversity%2520of%2520such%2520data%2520improve%2520the%2520generalization%2520abilities%2520of%2520in-context%250Alearners%2520beyond%2520their%2520training%2520domain%252C%2520while%2520achieving%2520comparable%2520performance%250Awith%2520models%2520trained%2520on%2520a%2520single%2520large-scale%2520dataset.%2520We%2520investigate%2520this%250Aparadigm%2520by%2520leveraging%2520meta-learning%2520to%2520train%2520an%2520in-context%2520learner%2520on%2520the%250AMeta-Album%2520collection%2520under%2520several%2520settings.%2520Firstly%252C%2520we%2520show%2520the%2520performance%250Ain%2520a%2520controlled%2520environment%252C%2520where%2520the%2520test%2520domain%2520is%2520completely%2520excluded%2520from%250Athe%2520training%2520knowledge.%2520Secondly%252C%2520we%2520explore%2520the%2520robustness%2520of%2520these%2520models%2520to%250Aforgetting%2520in%2520a%2520continual%2520scenario%2520where%2520the%2520information%2520is%2520accessible%2520for%2520a%250Alimited%2520time.%2520Finally%252C%2520we%2520explore%2520the%2520more%2520challenging%2520unsupervised%2520scenario.%250AOur%2520findings%2520demonstrate%2520that%2520transformers%2520still%2520generalize%2520for%2520in-context%250Aprediction%2520when%2520trained%2520on%2520a%2520curated%2520dataset%2520collection%2520while%2520offering%250Aadvantages%2520in%2520modularity%2520and%2520replaceability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-Learning%20Transformers%20to%20Improve%20In-Context%20Generalization&entry.906535625=Lorenzo%20Braccaioli%20and%20Anna%20Vettoruzzo%20and%20Prabhant%20Singh%20and%20Joaquin%20Vanschoren%20and%20Mohamed-Rafik%20Bouguelia%20and%20Nicola%20Conci&entry.1292438233=%20%20In-context%20learning%20enables%20transformer%20models%20to%20generalize%20to%20new%20tasks%0Abased%20solely%20on%20input%20prompts%2C%20without%20any%20need%20for%20weight%20updates.%20However%2C%0Aexisting%20training%20paradigms%20typically%20rely%20on%20large%2C%20unstructured%20datasets%20that%0Aare%20costly%20to%20store%2C%20difficult%20to%20evaluate%20for%20quality%20and%20balance%2C%20and%20pose%0Aprivacy%20and%20ethical%20concerns%20due%20to%20the%20inclusion%20of%20sensitive%20information.%0AMotivated%20by%20these%20limitations%20and%20risks%2C%20we%20propose%20an%20alternative%20training%0Astrategy%20where%20we%20leverage%20a%20collection%20of%20multiple%2C%20small-scale%2C%20and%0Adomain-specific%20datasets.%20We%20empirically%20demonstrate%20that%20the%20increased%20quality%0Aand%20diversity%20of%20such%20data%20improve%20the%20generalization%20abilities%20of%20in-context%0Alearners%20beyond%20their%20training%20domain%2C%20while%20achieving%20comparable%20performance%0Awith%20models%20trained%20on%20a%20single%20large-scale%20dataset.%20We%20investigate%20this%0Aparadigm%20by%20leveraging%20meta-learning%20to%20train%20an%20in-context%20learner%20on%20the%0AMeta-Album%20collection%20under%20several%20settings.%20Firstly%2C%20we%20show%20the%20performance%0Ain%20a%20controlled%20environment%2C%20where%20the%20test%20domain%20is%20completely%20excluded%20from%0Athe%20training%20knowledge.%20Secondly%2C%20we%20explore%20the%20robustness%20of%20these%20models%20to%0Aforgetting%20in%20a%20continual%20scenario%20where%20the%20information%20is%20accessible%20for%20a%0Alimited%20time.%20Finally%2C%20we%20explore%20the%20more%20challenging%20unsupervised%20scenario.%0AOur%20findings%20demonstrate%20that%20transformers%20still%20generalize%20for%20in-context%0Aprediction%20when%20trained%20on%20a%20curated%20dataset%20collection%20while%20offering%0Aadvantages%20in%20modularity%20and%20replaceability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05019v1&entry.124074799=Read"},
{"title": "Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning", "author": "Seungho Baek and Taegeon Park and Jongchan Park and Seungjun Oh and Yusung Kim", "abstract": "  Existing offline hierarchical reinforcement learning methods rely on\nhigh-level policy learning to generate subgoal sequences. However, their\nefficiency degrades as task horizons increase, and they lack effective\nstrategies for stitching useful state transitions across different\ntrajectories. We propose Graph-Assisted Stitching (GAS), a novel framework that\nformulates subgoal selection as a graph search problem rather than learning an\nexplicit high-level policy. By embedding states into a Temporal Distance\nRepresentation (TDR) space, GAS clusters semantically similar states from\ndifferent trajectories into unified graph nodes, enabling efficient transition\nstitching. A shortest-path algorithm is then applied to select subgoal\nsequences within the graph, while a low-level policy learns to reach the\nsubgoals. To improve graph quality, we introduce the Temporal Efficiency (TE)\nmetric, which filters out noisy or inefficient transition states, significantly\nenhancing task performance. GAS outperforms prior offline HRL methods across\nlocomotion, navigation, and manipulation tasks. Notably, in the most\nstitching-critical task, it achieves a score of 88.3, dramatically surpassing\nthe previous state-of-the-art score of 1.0. Our source code is available at:\nhttps://github.com/qortmdgh4141/GAS.\n", "link": "http://arxiv.org/abs/2506.07744v3", "date": "2025-07-07", "relevancy": 2.0738, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5312}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5178}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-Assisted%20Stitching%20for%20Offline%20Hierarchical%20Reinforcement%20Learning&body=Title%3A%20Graph-Assisted%20Stitching%20for%20Offline%20Hierarchical%20Reinforcement%20Learning%0AAuthor%3A%20Seungho%20Baek%20and%20Taegeon%20Park%20and%20Jongchan%20Park%20and%20Seungjun%20Oh%20and%20Yusung%20Kim%0AAbstract%3A%20%20%20Existing%20offline%20hierarchical%20reinforcement%20learning%20methods%20rely%20on%0Ahigh-level%20policy%20learning%20to%20generate%20subgoal%20sequences.%20However%2C%20their%0Aefficiency%20degrades%20as%20task%20horizons%20increase%2C%20and%20they%20lack%20effective%0Astrategies%20for%20stitching%20useful%20state%20transitions%20across%20different%0Atrajectories.%20We%20propose%20Graph-Assisted%20Stitching%20%28GAS%29%2C%20a%20novel%20framework%20that%0Aformulates%20subgoal%20selection%20as%20a%20graph%20search%20problem%20rather%20than%20learning%20an%0Aexplicit%20high-level%20policy.%20By%20embedding%20states%20into%20a%20Temporal%20Distance%0ARepresentation%20%28TDR%29%20space%2C%20GAS%20clusters%20semantically%20similar%20states%20from%0Adifferent%20trajectories%20into%20unified%20graph%20nodes%2C%20enabling%20efficient%20transition%0Astitching.%20A%20shortest-path%20algorithm%20is%20then%20applied%20to%20select%20subgoal%0Asequences%20within%20the%20graph%2C%20while%20a%20low-level%20policy%20learns%20to%20reach%20the%0Asubgoals.%20To%20improve%20graph%20quality%2C%20we%20introduce%20the%20Temporal%20Efficiency%20%28TE%29%0Ametric%2C%20which%20filters%20out%20noisy%20or%20inefficient%20transition%20states%2C%20significantly%0Aenhancing%20task%20performance.%20GAS%20outperforms%20prior%20offline%20HRL%20methods%20across%0Alocomotion%2C%20navigation%2C%20and%20manipulation%20tasks.%20Notably%2C%20in%20the%20most%0Astitching-critical%20task%2C%20it%20achieves%20a%20score%20of%2088.3%2C%20dramatically%20surpassing%0Athe%20previous%20state-of-the-art%20score%20of%201.0.%20Our%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/qortmdgh4141/GAS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07744v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-Assisted%2520Stitching%2520for%2520Offline%2520Hierarchical%2520Reinforcement%2520Learning%26entry.906535625%3DSeungho%2520Baek%2520and%2520Taegeon%2520Park%2520and%2520Jongchan%2520Park%2520and%2520Seungjun%2520Oh%2520and%2520Yusung%2520Kim%26entry.1292438233%3D%2520%2520Existing%2520offline%2520hierarchical%2520reinforcement%2520learning%2520methods%2520rely%2520on%250Ahigh-level%2520policy%2520learning%2520to%2520generate%2520subgoal%2520sequences.%2520However%252C%2520their%250Aefficiency%2520degrades%2520as%2520task%2520horizons%2520increase%252C%2520and%2520they%2520lack%2520effective%250Astrategies%2520for%2520stitching%2520useful%2520state%2520transitions%2520across%2520different%250Atrajectories.%2520We%2520propose%2520Graph-Assisted%2520Stitching%2520%2528GAS%2529%252C%2520a%2520novel%2520framework%2520that%250Aformulates%2520subgoal%2520selection%2520as%2520a%2520graph%2520search%2520problem%2520rather%2520than%2520learning%2520an%250Aexplicit%2520high-level%2520policy.%2520By%2520embedding%2520states%2520into%2520a%2520Temporal%2520Distance%250ARepresentation%2520%2528TDR%2529%2520space%252C%2520GAS%2520clusters%2520semantically%2520similar%2520states%2520from%250Adifferent%2520trajectories%2520into%2520unified%2520graph%2520nodes%252C%2520enabling%2520efficient%2520transition%250Astitching.%2520A%2520shortest-path%2520algorithm%2520is%2520then%2520applied%2520to%2520select%2520subgoal%250Asequences%2520within%2520the%2520graph%252C%2520while%2520a%2520low-level%2520policy%2520learns%2520to%2520reach%2520the%250Asubgoals.%2520To%2520improve%2520graph%2520quality%252C%2520we%2520introduce%2520the%2520Temporal%2520Efficiency%2520%2528TE%2529%250Ametric%252C%2520which%2520filters%2520out%2520noisy%2520or%2520inefficient%2520transition%2520states%252C%2520significantly%250Aenhancing%2520task%2520performance.%2520GAS%2520outperforms%2520prior%2520offline%2520HRL%2520methods%2520across%250Alocomotion%252C%2520navigation%252C%2520and%2520manipulation%2520tasks.%2520Notably%252C%2520in%2520the%2520most%250Astitching-critical%2520task%252C%2520it%2520achieves%2520a%2520score%2520of%252088.3%252C%2520dramatically%2520surpassing%250Athe%2520previous%2520state-of-the-art%2520score%2520of%25201.0.%2520Our%2520source%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/qortmdgh4141/GAS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07744v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-Assisted%20Stitching%20for%20Offline%20Hierarchical%20Reinforcement%20Learning&entry.906535625=Seungho%20Baek%20and%20Taegeon%20Park%20and%20Jongchan%20Park%20and%20Seungjun%20Oh%20and%20Yusung%20Kim&entry.1292438233=%20%20Existing%20offline%20hierarchical%20reinforcement%20learning%20methods%20rely%20on%0Ahigh-level%20policy%20learning%20to%20generate%20subgoal%20sequences.%20However%2C%20their%0Aefficiency%20degrades%20as%20task%20horizons%20increase%2C%20and%20they%20lack%20effective%0Astrategies%20for%20stitching%20useful%20state%20transitions%20across%20different%0Atrajectories.%20We%20propose%20Graph-Assisted%20Stitching%20%28GAS%29%2C%20a%20novel%20framework%20that%0Aformulates%20subgoal%20selection%20as%20a%20graph%20search%20problem%20rather%20than%20learning%20an%0Aexplicit%20high-level%20policy.%20By%20embedding%20states%20into%20a%20Temporal%20Distance%0ARepresentation%20%28TDR%29%20space%2C%20GAS%20clusters%20semantically%20similar%20states%20from%0Adifferent%20trajectories%20into%20unified%20graph%20nodes%2C%20enabling%20efficient%20transition%0Astitching.%20A%20shortest-path%20algorithm%20is%20then%20applied%20to%20select%20subgoal%0Asequences%20within%20the%20graph%2C%20while%20a%20low-level%20policy%20learns%20to%20reach%20the%0Asubgoals.%20To%20improve%20graph%20quality%2C%20we%20introduce%20the%20Temporal%20Efficiency%20%28TE%29%0Ametric%2C%20which%20filters%20out%20noisy%20or%20inefficient%20transition%20states%2C%20significantly%0Aenhancing%20task%20performance.%20GAS%20outperforms%20prior%20offline%20HRL%20methods%20across%0Alocomotion%2C%20navigation%2C%20and%20manipulation%20tasks.%20Notably%2C%20in%20the%20most%0Astitching-critical%20task%2C%20it%20achieves%20a%20score%20of%2088.3%2C%20dramatically%20surpassing%0Athe%20previous%20state-of-the-art%20score%20of%201.0.%20Our%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/qortmdgh4141/GAS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07744v3&entry.124074799=Read"},
{"title": "Scalable Multi-Task Learning for Particle Collision Event Reconstruction\n  with Heterogeneous Graph Neural Networks", "author": "William Sutcliffe and Marta Calvi and Simone Capelli and Jonas Eschle and Juli\u00e1n Garc\u00eda Pardi\u00f1as and Abhijit Mathad and Azusa Uzuki and Nicola Serra", "abstract": "  The growing luminosity frontier at the Large Hadron Collider is challenging\nthe reconstruction and analysis of particle collision events. Increased\nparticle multiplicities are straining latency and storage requirements at the\ndata acquisition stage, while new complications are emerging, including higher\nbackground levels and more frequent particle vertex misassociations. This in\nturn necessitates the development of more holistic and scalable reconstruction\nmethods that take advantage of recent advances in machine learning. We propose\na novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique\nrepresentations for diverse particle collision relationships and integrated\ngraph pruning layers for scalability. Trained with a multi-task paradigm in an\nenvironment mimicking the LHCb experiment, this HGNN significantly improves\nbeauty hadron reconstruction performance. Notably, it concurrently performs\nparticle vertex association and graph pruning within a single framework. We\nquantify reconstruction and pruning performance, demonstrate enhanced inference\ntime scaling with event complexity, and mitigate potential performance loss\nusing a weighted message passing scheme.\n", "link": "http://arxiv.org/abs/2504.21844v2", "date": "2025-07-07", "relevancy": 2.0715, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5381}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5181}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Multi-Task%20Learning%20for%20Particle%20Collision%20Event%20Reconstruction%0A%20%20with%20Heterogeneous%20Graph%20Neural%20Networks&body=Title%3A%20Scalable%20Multi-Task%20Learning%20for%20Particle%20Collision%20Event%20Reconstruction%0A%20%20with%20Heterogeneous%20Graph%20Neural%20Networks%0AAuthor%3A%20William%20Sutcliffe%20and%20Marta%20Calvi%20and%20Simone%20Capelli%20and%20Jonas%20Eschle%20and%20Juli%C3%A1n%20Garc%C3%ADa%20Pardi%C3%B1as%20and%20Abhijit%20Mathad%20and%20Azusa%20Uzuki%20and%20Nicola%20Serra%0AAbstract%3A%20%20%20The%20growing%20luminosity%20frontier%20at%20the%20Large%20Hadron%20Collider%20is%20challenging%0Athe%20reconstruction%20and%20analysis%20of%20particle%20collision%20events.%20Increased%0Aparticle%20multiplicities%20are%20straining%20latency%20and%20storage%20requirements%20at%20the%0Adata%20acquisition%20stage%2C%20while%20new%20complications%20are%20emerging%2C%20including%20higher%0Abackground%20levels%20and%20more%20frequent%20particle%20vertex%20misassociations.%20This%20in%0Aturn%20necessitates%20the%20development%20of%20more%20holistic%20and%20scalable%20reconstruction%0Amethods%20that%20take%20advantage%20of%20recent%20advances%20in%20machine%20learning.%20We%20propose%0Aa%20novel%20Heterogeneous%20Graph%20Neural%20Network%20%28HGNN%29%20architecture%20featuring%20unique%0Arepresentations%20for%20diverse%20particle%20collision%20relationships%20and%20integrated%0Agraph%20pruning%20layers%20for%20scalability.%20Trained%20with%20a%20multi-task%20paradigm%20in%20an%0Aenvironment%20mimicking%20the%20LHCb%20experiment%2C%20this%20HGNN%20significantly%20improves%0Abeauty%20hadron%20reconstruction%20performance.%20Notably%2C%20it%20concurrently%20performs%0Aparticle%20vertex%20association%20and%20graph%20pruning%20within%20a%20single%20framework.%20We%0Aquantify%20reconstruction%20and%20pruning%20performance%2C%20demonstrate%20enhanced%20inference%0Atime%20scaling%20with%20event%20complexity%2C%20and%20mitigate%20potential%20performance%20loss%0Ausing%20a%20weighted%20message%20passing%20scheme.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.21844v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Multi-Task%2520Learning%2520for%2520Particle%2520Collision%2520Event%2520Reconstruction%250A%2520%2520with%2520Heterogeneous%2520Graph%2520Neural%2520Networks%26entry.906535625%3DWilliam%2520Sutcliffe%2520and%2520Marta%2520Calvi%2520and%2520Simone%2520Capelli%2520and%2520Jonas%2520Eschle%2520and%2520Juli%25C3%25A1n%2520Garc%25C3%25ADa%2520Pardi%25C3%25B1as%2520and%2520Abhijit%2520Mathad%2520and%2520Azusa%2520Uzuki%2520and%2520Nicola%2520Serra%26entry.1292438233%3D%2520%2520The%2520growing%2520luminosity%2520frontier%2520at%2520the%2520Large%2520Hadron%2520Collider%2520is%2520challenging%250Athe%2520reconstruction%2520and%2520analysis%2520of%2520particle%2520collision%2520events.%2520Increased%250Aparticle%2520multiplicities%2520are%2520straining%2520latency%2520and%2520storage%2520requirements%2520at%2520the%250Adata%2520acquisition%2520stage%252C%2520while%2520new%2520complications%2520are%2520emerging%252C%2520including%2520higher%250Abackground%2520levels%2520and%2520more%2520frequent%2520particle%2520vertex%2520misassociations.%2520This%2520in%250Aturn%2520necessitates%2520the%2520development%2520of%2520more%2520holistic%2520and%2520scalable%2520reconstruction%250Amethods%2520that%2520take%2520advantage%2520of%2520recent%2520advances%2520in%2520machine%2520learning.%2520We%2520propose%250Aa%2520novel%2520Heterogeneous%2520Graph%2520Neural%2520Network%2520%2528HGNN%2529%2520architecture%2520featuring%2520unique%250Arepresentations%2520for%2520diverse%2520particle%2520collision%2520relationships%2520and%2520integrated%250Agraph%2520pruning%2520layers%2520for%2520scalability.%2520Trained%2520with%2520a%2520multi-task%2520paradigm%2520in%2520an%250Aenvironment%2520mimicking%2520the%2520LHCb%2520experiment%252C%2520this%2520HGNN%2520significantly%2520improves%250Abeauty%2520hadron%2520reconstruction%2520performance.%2520Notably%252C%2520it%2520concurrently%2520performs%250Aparticle%2520vertex%2520association%2520and%2520graph%2520pruning%2520within%2520a%2520single%2520framework.%2520We%250Aquantify%2520reconstruction%2520and%2520pruning%2520performance%252C%2520demonstrate%2520enhanced%2520inference%250Atime%2520scaling%2520with%2520event%2520complexity%252C%2520and%2520mitigate%2520potential%2520performance%2520loss%250Ausing%2520a%2520weighted%2520message%2520passing%2520scheme.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.21844v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Multi-Task%20Learning%20for%20Particle%20Collision%20Event%20Reconstruction%0A%20%20with%20Heterogeneous%20Graph%20Neural%20Networks&entry.906535625=William%20Sutcliffe%20and%20Marta%20Calvi%20and%20Simone%20Capelli%20and%20Jonas%20Eschle%20and%20Juli%C3%A1n%20Garc%C3%ADa%20Pardi%C3%B1as%20and%20Abhijit%20Mathad%20and%20Azusa%20Uzuki%20and%20Nicola%20Serra&entry.1292438233=%20%20The%20growing%20luminosity%20frontier%20at%20the%20Large%20Hadron%20Collider%20is%20challenging%0Athe%20reconstruction%20and%20analysis%20of%20particle%20collision%20events.%20Increased%0Aparticle%20multiplicities%20are%20straining%20latency%20and%20storage%20requirements%20at%20the%0Adata%20acquisition%20stage%2C%20while%20new%20complications%20are%20emerging%2C%20including%20higher%0Abackground%20levels%20and%20more%20frequent%20particle%20vertex%20misassociations.%20This%20in%0Aturn%20necessitates%20the%20development%20of%20more%20holistic%20and%20scalable%20reconstruction%0Amethods%20that%20take%20advantage%20of%20recent%20advances%20in%20machine%20learning.%20We%20propose%0Aa%20novel%20Heterogeneous%20Graph%20Neural%20Network%20%28HGNN%29%20architecture%20featuring%20unique%0Arepresentations%20for%20diverse%20particle%20collision%20relationships%20and%20integrated%0Agraph%20pruning%20layers%20for%20scalability.%20Trained%20with%20a%20multi-task%20paradigm%20in%20an%0Aenvironment%20mimicking%20the%20LHCb%20experiment%2C%20this%20HGNN%20significantly%20improves%0Abeauty%20hadron%20reconstruction%20performance.%20Notably%2C%20it%20concurrently%20performs%0Aparticle%20vertex%20association%20and%20graph%20pruning%20within%20a%20single%20framework.%20We%0Aquantify%20reconstruction%20and%20pruning%20performance%2C%20demonstrate%20enhanced%20inference%0Atime%20scaling%20with%20event%20complexity%2C%20and%20mitigate%20potential%20performance%20loss%0Ausing%20a%20weighted%20message%20passing%20scheme.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.21844v2&entry.124074799=Read"},
{"title": "EyeTrAES: Fine-grained, Low-Latency Eye Tracking via Adaptive Event\n  Slicing", "author": "Argha Sen and Nuwan Bandara and Ila Gokarn and Thivya Kandappu and Archan Misra", "abstract": "  Eye-tracking technology has gained significant attention in recent years due\nto its wide range of applications in human-computer interaction, virtual and\naugmented reality, and wearable health. Traditional RGB camera-based\neye-tracking systems often struggle with poor temporal resolution and\ncomputational constraints, limiting their effectiveness in capturing rapid eye\nmovements. To address these limitations, we propose EyeTrAES, a novel approach\nusing neuromorphic event cameras for high-fidelity tracking of natural\npupillary movement that shows significant kinematic variance. One of EyeTrAES's\nhighlights is the use of a novel adaptive windowing/slicing algorithm that\nensures just the right amount of descriptive asynchronous event data\naccumulation within an event frame, across a wide range of eye movement\npatterns. EyeTrAES then applies lightweight image processing functions over\naccumulated event frames from just a single eye to perform pupil segmentation\nand tracking. We show that these methods boost pupil tracking fidelity by 6+%,\nachieving IoU~=92%, while incurring at least 3x lower latency than competing\npure event-based eye tracking alternatives [38]. We additionally demonstrate\nthat the microscopic pupillary motion captured by EyeTrAES exhibits distinctive\nvariations across individuals and can thus serve as a biometric fingerprint.\nFor robust user authentication, we train a lightweight per-user Random Forest\nclassifier using a novel feature vector of short-term pupillary kinematics,\ncomprising a sliding window of pupil (location, velocity, acceleration)\ntriples. Experimental studies with two different datasets demonstrate that the\nEyeTrAES-based authentication technique can simultaneously achieve high\nauthentication accuracy (~=0.82) and low processing latency (~=12ms), and\nsignificantly outperform multiple state-of-the-art competitive baselines.\n", "link": "http://arxiv.org/abs/2409.18813v2", "date": "2025-07-07", "relevancy": 2.0698, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5371}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5174}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EyeTrAES%3A%20Fine-grained%2C%20Low-Latency%20Eye%20Tracking%20via%20Adaptive%20Event%0A%20%20Slicing&body=Title%3A%20EyeTrAES%3A%20Fine-grained%2C%20Low-Latency%20Eye%20Tracking%20via%20Adaptive%20Event%0A%20%20Slicing%0AAuthor%3A%20Argha%20Sen%20and%20Nuwan%20Bandara%20and%20Ila%20Gokarn%20and%20Thivya%20Kandappu%20and%20Archan%20Misra%0AAbstract%3A%20%20%20Eye-tracking%20technology%20has%20gained%20significant%20attention%20in%20recent%20years%20due%0Ato%20its%20wide%20range%20of%20applications%20in%20human-computer%20interaction%2C%20virtual%20and%0Aaugmented%20reality%2C%20and%20wearable%20health.%20Traditional%20RGB%20camera-based%0Aeye-tracking%20systems%20often%20struggle%20with%20poor%20temporal%20resolution%20and%0Acomputational%20constraints%2C%20limiting%20their%20effectiveness%20in%20capturing%20rapid%20eye%0Amovements.%20To%20address%20these%20limitations%2C%20we%20propose%20EyeTrAES%2C%20a%20novel%20approach%0Ausing%20neuromorphic%20event%20cameras%20for%20high-fidelity%20tracking%20of%20natural%0Apupillary%20movement%20that%20shows%20significant%20kinematic%20variance.%20One%20of%20EyeTrAES%27s%0Ahighlights%20is%20the%20use%20of%20a%20novel%20adaptive%20windowing/slicing%20algorithm%20that%0Aensures%20just%20the%20right%20amount%20of%20descriptive%20asynchronous%20event%20data%0Aaccumulation%20within%20an%20event%20frame%2C%20across%20a%20wide%20range%20of%20eye%20movement%0Apatterns.%20EyeTrAES%20then%20applies%20lightweight%20image%20processing%20functions%20over%0Aaccumulated%20event%20frames%20from%20just%20a%20single%20eye%20to%20perform%20pupil%20segmentation%0Aand%20tracking.%20We%20show%20that%20these%20methods%20boost%20pupil%20tracking%20fidelity%20by%206%2B%25%2C%0Aachieving%20IoU~%3D92%25%2C%20while%20incurring%20at%20least%203x%20lower%20latency%20than%20competing%0Apure%20event-based%20eye%20tracking%20alternatives%20%5B38%5D.%20We%20additionally%20demonstrate%0Athat%20the%20microscopic%20pupillary%20motion%20captured%20by%20EyeTrAES%20exhibits%20distinctive%0Avariations%20across%20individuals%20and%20can%20thus%20serve%20as%20a%20biometric%20fingerprint.%0AFor%20robust%20user%20authentication%2C%20we%20train%20a%20lightweight%20per-user%20Random%20Forest%0Aclassifier%20using%20a%20novel%20feature%20vector%20of%20short-term%20pupillary%20kinematics%2C%0Acomprising%20a%20sliding%20window%20of%20pupil%20%28location%2C%20velocity%2C%20acceleration%29%0Atriples.%20Experimental%20studies%20with%20two%20different%20datasets%20demonstrate%20that%20the%0AEyeTrAES-based%20authentication%20technique%20can%20simultaneously%20achieve%20high%0Aauthentication%20accuracy%20%28~%3D0.82%29%20and%20low%20processing%20latency%20%28~%3D12ms%29%2C%20and%0Asignificantly%20outperform%20multiple%20state-of-the-art%20competitive%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18813v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEyeTrAES%253A%2520Fine-grained%252C%2520Low-Latency%2520Eye%2520Tracking%2520via%2520Adaptive%2520Event%250A%2520%2520Slicing%26entry.906535625%3DArgha%2520Sen%2520and%2520Nuwan%2520Bandara%2520and%2520Ila%2520Gokarn%2520and%2520Thivya%2520Kandappu%2520and%2520Archan%2520Misra%26entry.1292438233%3D%2520%2520Eye-tracking%2520technology%2520has%2520gained%2520significant%2520attention%2520in%2520recent%2520years%2520due%250Ato%2520its%2520wide%2520range%2520of%2520applications%2520in%2520human-computer%2520interaction%252C%2520virtual%2520and%250Aaugmented%2520reality%252C%2520and%2520wearable%2520health.%2520Traditional%2520RGB%2520camera-based%250Aeye-tracking%2520systems%2520often%2520struggle%2520with%2520poor%2520temporal%2520resolution%2520and%250Acomputational%2520constraints%252C%2520limiting%2520their%2520effectiveness%2520in%2520capturing%2520rapid%2520eye%250Amovements.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520EyeTrAES%252C%2520a%2520novel%2520approach%250Ausing%2520neuromorphic%2520event%2520cameras%2520for%2520high-fidelity%2520tracking%2520of%2520natural%250Apupillary%2520movement%2520that%2520shows%2520significant%2520kinematic%2520variance.%2520One%2520of%2520EyeTrAES%2527s%250Ahighlights%2520is%2520the%2520use%2520of%2520a%2520novel%2520adaptive%2520windowing/slicing%2520algorithm%2520that%250Aensures%2520just%2520the%2520right%2520amount%2520of%2520descriptive%2520asynchronous%2520event%2520data%250Aaccumulation%2520within%2520an%2520event%2520frame%252C%2520across%2520a%2520wide%2520range%2520of%2520eye%2520movement%250Apatterns.%2520EyeTrAES%2520then%2520applies%2520lightweight%2520image%2520processing%2520functions%2520over%250Aaccumulated%2520event%2520frames%2520from%2520just%2520a%2520single%2520eye%2520to%2520perform%2520pupil%2520segmentation%250Aand%2520tracking.%2520We%2520show%2520that%2520these%2520methods%2520boost%2520pupil%2520tracking%2520fidelity%2520by%25206%252B%2525%252C%250Aachieving%2520IoU~%253D92%2525%252C%2520while%2520incurring%2520at%2520least%25203x%2520lower%2520latency%2520than%2520competing%250Apure%2520event-based%2520eye%2520tracking%2520alternatives%2520%255B38%255D.%2520We%2520additionally%2520demonstrate%250Athat%2520the%2520microscopic%2520pupillary%2520motion%2520captured%2520by%2520EyeTrAES%2520exhibits%2520distinctive%250Avariations%2520across%2520individuals%2520and%2520can%2520thus%2520serve%2520as%2520a%2520biometric%2520fingerprint.%250AFor%2520robust%2520user%2520authentication%252C%2520we%2520train%2520a%2520lightweight%2520per-user%2520Random%2520Forest%250Aclassifier%2520using%2520a%2520novel%2520feature%2520vector%2520of%2520short-term%2520pupillary%2520kinematics%252C%250Acomprising%2520a%2520sliding%2520window%2520of%2520pupil%2520%2528location%252C%2520velocity%252C%2520acceleration%2529%250Atriples.%2520Experimental%2520studies%2520with%2520two%2520different%2520datasets%2520demonstrate%2520that%2520the%250AEyeTrAES-based%2520authentication%2520technique%2520can%2520simultaneously%2520achieve%2520high%250Aauthentication%2520accuracy%2520%2528~%253D0.82%2529%2520and%2520low%2520processing%2520latency%2520%2528~%253D12ms%2529%252C%2520and%250Asignificantly%2520outperform%2520multiple%2520state-of-the-art%2520competitive%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18813v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EyeTrAES%3A%20Fine-grained%2C%20Low-Latency%20Eye%20Tracking%20via%20Adaptive%20Event%0A%20%20Slicing&entry.906535625=Argha%20Sen%20and%20Nuwan%20Bandara%20and%20Ila%20Gokarn%20and%20Thivya%20Kandappu%20and%20Archan%20Misra&entry.1292438233=%20%20Eye-tracking%20technology%20has%20gained%20significant%20attention%20in%20recent%20years%20due%0Ato%20its%20wide%20range%20of%20applications%20in%20human-computer%20interaction%2C%20virtual%20and%0Aaugmented%20reality%2C%20and%20wearable%20health.%20Traditional%20RGB%20camera-based%0Aeye-tracking%20systems%20often%20struggle%20with%20poor%20temporal%20resolution%20and%0Acomputational%20constraints%2C%20limiting%20their%20effectiveness%20in%20capturing%20rapid%20eye%0Amovements.%20To%20address%20these%20limitations%2C%20we%20propose%20EyeTrAES%2C%20a%20novel%20approach%0Ausing%20neuromorphic%20event%20cameras%20for%20high-fidelity%20tracking%20of%20natural%0Apupillary%20movement%20that%20shows%20significant%20kinematic%20variance.%20One%20of%20EyeTrAES%27s%0Ahighlights%20is%20the%20use%20of%20a%20novel%20adaptive%20windowing/slicing%20algorithm%20that%0Aensures%20just%20the%20right%20amount%20of%20descriptive%20asynchronous%20event%20data%0Aaccumulation%20within%20an%20event%20frame%2C%20across%20a%20wide%20range%20of%20eye%20movement%0Apatterns.%20EyeTrAES%20then%20applies%20lightweight%20image%20processing%20functions%20over%0Aaccumulated%20event%20frames%20from%20just%20a%20single%20eye%20to%20perform%20pupil%20segmentation%0Aand%20tracking.%20We%20show%20that%20these%20methods%20boost%20pupil%20tracking%20fidelity%20by%206%2B%25%2C%0Aachieving%20IoU~%3D92%25%2C%20while%20incurring%20at%20least%203x%20lower%20latency%20than%20competing%0Apure%20event-based%20eye%20tracking%20alternatives%20%5B38%5D.%20We%20additionally%20demonstrate%0Athat%20the%20microscopic%20pupillary%20motion%20captured%20by%20EyeTrAES%20exhibits%20distinctive%0Avariations%20across%20individuals%20and%20can%20thus%20serve%20as%20a%20biometric%20fingerprint.%0AFor%20robust%20user%20authentication%2C%20we%20train%20a%20lightweight%20per-user%20Random%20Forest%0Aclassifier%20using%20a%20novel%20feature%20vector%20of%20short-term%20pupillary%20kinematics%2C%0Acomprising%20a%20sliding%20window%20of%20pupil%20%28location%2C%20velocity%2C%20acceleration%29%0Atriples.%20Experimental%20studies%20with%20two%20different%20datasets%20demonstrate%20that%20the%0AEyeTrAES-based%20authentication%20technique%20can%20simultaneously%20achieve%20high%0Aauthentication%20accuracy%20%28~%3D0.82%29%20and%20low%20processing%20latency%20%28~%3D12ms%29%2C%20and%0Asignificantly%20outperform%20multiple%20state-of-the-art%20competitive%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18813v2&entry.124074799=Read"},
{"title": "EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement\n  Learning Framework", "author": "Chen Wang and Lai Wei and Yanzhi Zhang and Chenyang Shao and Zedong Dan and Weiran Huang and Yue Wang and Yuzhi Zhang", "abstract": "  Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), an efficient variant of PPO that lowers RL's\ncomputational cost, still faces limited exploration, low sample efficiency and\ninstability, constraining its performance on complex reasoning tasks. To\naddress these limitations, we introduce EFRame, an Exploration-Filter-Replay\nframework that systematically augments GRPO along three critical dimensions.\nEFRame performs additional rollouts to explore high-quality trajectories,\napplies online filtering to eliminate low-quality samples that introduce noise\nand variance, and leverages experience replay to repeatedly exploit rare but\ninformative samples. EFRame establishes a complete and stable learning cycle,\nguiding the model through a structured transition from exploration to\nconvergence. Our experiments across a variety of reasoning benchmarks\ndemonstrate that EFRame not only improves the robustness and efficiency of\ntraining, but also enables access to deeper reasoning capabilities that remain\nunattainable under vanilla GRPO. Furthermore, EFRame not only enables\nfine-grained categorization of training samples for deeper insight into their\ncontributions, but also introduces an efficient and precise mechanism for\nentropy control, which is critical for balancing exploration and convergence in\nRL training. Our code is available at https://github.com/597358816/EFRame.\n", "link": "http://arxiv.org/abs/2506.22200v3", "date": "2025-07-07", "relevancy": 2.0569, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5099}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EFRame%3A%20Deeper%20Reasoning%20via%20Exploration-Filter-Replay%20Reinforcement%0A%20%20Learning%20Framework&body=Title%3A%20EFRame%3A%20Deeper%20Reasoning%20via%20Exploration-Filter-Replay%20Reinforcement%0A%20%20Learning%20Framework%0AAuthor%3A%20Chen%20Wang%20and%20Lai%20Wei%20and%20Yanzhi%20Zhang%20and%20Chenyang%20Shao%20and%20Zedong%20Dan%20and%20Weiran%20Huang%20and%20Yue%20Wang%20and%20Yuzhi%20Zhang%0AAbstract%3A%20%20%20Recent%20advances%20in%20reinforcement%20learning%20%28RL%29%20have%20significantly%20enhanced%0Athe%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29.%20Group%20Relative%0APolicy%20Optimization%20%28GRPO%29%2C%20an%20efficient%20variant%20of%20PPO%20that%20lowers%20RL%27s%0Acomputational%20cost%2C%20still%20faces%20limited%20exploration%2C%20low%20sample%20efficiency%20and%0Ainstability%2C%20constraining%20its%20performance%20on%20complex%20reasoning%20tasks.%20To%0Aaddress%20these%20limitations%2C%20we%20introduce%20EFRame%2C%20an%20Exploration-Filter-Replay%0Aframework%20that%20systematically%20augments%20GRPO%20along%20three%20critical%20dimensions.%0AEFRame%20performs%20additional%20rollouts%20to%20explore%20high-quality%20trajectories%2C%0Aapplies%20online%20filtering%20to%20eliminate%20low-quality%20samples%20that%20introduce%20noise%0Aand%20variance%2C%20and%20leverages%20experience%20replay%20to%20repeatedly%20exploit%20rare%20but%0Ainformative%20samples.%20EFRame%20establishes%20a%20complete%20and%20stable%20learning%20cycle%2C%0Aguiding%20the%20model%20through%20a%20structured%20transition%20from%20exploration%20to%0Aconvergence.%20Our%20experiments%20across%20a%20variety%20of%20reasoning%20benchmarks%0Ademonstrate%20that%20EFRame%20not%20only%20improves%20the%20robustness%20and%20efficiency%20of%0Atraining%2C%20but%20also%20enables%20access%20to%20deeper%20reasoning%20capabilities%20that%20remain%0Aunattainable%20under%20vanilla%20GRPO.%20Furthermore%2C%20EFRame%20not%20only%20enables%0Afine-grained%20categorization%20of%20training%20samples%20for%20deeper%20insight%20into%20their%0Acontributions%2C%20but%20also%20introduces%20an%20efficient%20and%20precise%20mechanism%20for%0Aentropy%20control%2C%20which%20is%20critical%20for%20balancing%20exploration%20and%20convergence%20in%0ARL%20training.%20Our%20code%20is%20available%20at%20https%3A//github.com/597358816/EFRame.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.22200v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEFRame%253A%2520Deeper%2520Reasoning%2520via%2520Exploration-Filter-Replay%2520Reinforcement%250A%2520%2520Learning%2520Framework%26entry.906535625%3DChen%2520Wang%2520and%2520Lai%2520Wei%2520and%2520Yanzhi%2520Zhang%2520and%2520Chenyang%2520Shao%2520and%2520Zedong%2520Dan%2520and%2520Weiran%2520Huang%2520and%2520Yue%2520Wang%2520and%2520Yuzhi%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520reinforcement%2520learning%2520%2528RL%2529%2520have%2520significantly%2520enhanced%250Athe%2520reasoning%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Group%2520Relative%250APolicy%2520Optimization%2520%2528GRPO%2529%252C%2520an%2520efficient%2520variant%2520of%2520PPO%2520that%2520lowers%2520RL%2527s%250Acomputational%2520cost%252C%2520still%2520faces%2520limited%2520exploration%252C%2520low%2520sample%2520efficiency%2520and%250Ainstability%252C%2520constraining%2520its%2520performance%2520on%2520complex%2520reasoning%2520tasks.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520introduce%2520EFRame%252C%2520an%2520Exploration-Filter-Replay%250Aframework%2520that%2520systematically%2520augments%2520GRPO%2520along%2520three%2520critical%2520dimensions.%250AEFRame%2520performs%2520additional%2520rollouts%2520to%2520explore%2520high-quality%2520trajectories%252C%250Aapplies%2520online%2520filtering%2520to%2520eliminate%2520low-quality%2520samples%2520that%2520introduce%2520noise%250Aand%2520variance%252C%2520and%2520leverages%2520experience%2520replay%2520to%2520repeatedly%2520exploit%2520rare%2520but%250Ainformative%2520samples.%2520EFRame%2520establishes%2520a%2520complete%2520and%2520stable%2520learning%2520cycle%252C%250Aguiding%2520the%2520model%2520through%2520a%2520structured%2520transition%2520from%2520exploration%2520to%250Aconvergence.%2520Our%2520experiments%2520across%2520a%2520variety%2520of%2520reasoning%2520benchmarks%250Ademonstrate%2520that%2520EFRame%2520not%2520only%2520improves%2520the%2520robustness%2520and%2520efficiency%2520of%250Atraining%252C%2520but%2520also%2520enables%2520access%2520to%2520deeper%2520reasoning%2520capabilities%2520that%2520remain%250Aunattainable%2520under%2520vanilla%2520GRPO.%2520Furthermore%252C%2520EFRame%2520not%2520only%2520enables%250Afine-grained%2520categorization%2520of%2520training%2520samples%2520for%2520deeper%2520insight%2520into%2520their%250Acontributions%252C%2520but%2520also%2520introduces%2520an%2520efficient%2520and%2520precise%2520mechanism%2520for%250Aentropy%2520control%252C%2520which%2520is%2520critical%2520for%2520balancing%2520exploration%2520and%2520convergence%2520in%250ARL%2520training.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/597358816/EFRame.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.22200v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EFRame%3A%20Deeper%20Reasoning%20via%20Exploration-Filter-Replay%20Reinforcement%0A%20%20Learning%20Framework&entry.906535625=Chen%20Wang%20and%20Lai%20Wei%20and%20Yanzhi%20Zhang%20and%20Chenyang%20Shao%20and%20Zedong%20Dan%20and%20Weiran%20Huang%20and%20Yue%20Wang%20and%20Yuzhi%20Zhang&entry.1292438233=%20%20Recent%20advances%20in%20reinforcement%20learning%20%28RL%29%20have%20significantly%20enhanced%0Athe%20reasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29.%20Group%20Relative%0APolicy%20Optimization%20%28GRPO%29%2C%20an%20efficient%20variant%20of%20PPO%20that%20lowers%20RL%27s%0Acomputational%20cost%2C%20still%20faces%20limited%20exploration%2C%20low%20sample%20efficiency%20and%0Ainstability%2C%20constraining%20its%20performance%20on%20complex%20reasoning%20tasks.%20To%0Aaddress%20these%20limitations%2C%20we%20introduce%20EFRame%2C%20an%20Exploration-Filter-Replay%0Aframework%20that%20systematically%20augments%20GRPO%20along%20three%20critical%20dimensions.%0AEFRame%20performs%20additional%20rollouts%20to%20explore%20high-quality%20trajectories%2C%0Aapplies%20online%20filtering%20to%20eliminate%20low-quality%20samples%20that%20introduce%20noise%0Aand%20variance%2C%20and%20leverages%20experience%20replay%20to%20repeatedly%20exploit%20rare%20but%0Ainformative%20samples.%20EFRame%20establishes%20a%20complete%20and%20stable%20learning%20cycle%2C%0Aguiding%20the%20model%20through%20a%20structured%20transition%20from%20exploration%20to%0Aconvergence.%20Our%20experiments%20across%20a%20variety%20of%20reasoning%20benchmarks%0Ademonstrate%20that%20EFRame%20not%20only%20improves%20the%20robustness%20and%20efficiency%20of%0Atraining%2C%20but%20also%20enables%20access%20to%20deeper%20reasoning%20capabilities%20that%20remain%0Aunattainable%20under%20vanilla%20GRPO.%20Furthermore%2C%20EFRame%20not%20only%20enables%0Afine-grained%20categorization%20of%20training%20samples%20for%20deeper%20insight%20into%20their%0Acontributions%2C%20but%20also%20introduces%20an%20efficient%20and%20precise%20mechanism%20for%0Aentropy%20control%2C%20which%20is%20critical%20for%20balancing%20exploration%20and%20convergence%20in%0ARL%20training.%20Our%20code%20is%20available%20at%20https%3A//github.com/597358816/EFRame.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.22200v3&entry.124074799=Read"},
{"title": "Efficient SAR Vessel Detection for FPGA-Based On-Satellite Sensing", "author": "Colin Laganier and Liam Fletcher and Elim Kwan and Richard Walters and Victoria Nockles", "abstract": "  Rapid analysis of satellite data is vital for many remote sensing\napplications, from disaster response to environmental monitoring, but is\nbecoming harder to achieve with the increasing volumes of data generated by\nmodern satellites. On-satellite machine learning (ML) offers a potential\nsolution, by reducing latency associated with transmission of these large data\nvolumes to ground stations, but state-of-the-art models are often too large or\npower-hungry for satellite deployment. Vessel detection using Synthetic\nAperture Radar (SAR) is a critical time-sensitive task for maritime security\nthat exemplifies this challenge. SAR vessel detection has previously been\ndemonstrated only by ML models that either are too large for satellite\ndeployment, have not been developed for sufficiently low-power hardware, or\nhave only been developed and tested on small SAR datasets that do not\nsufficiently represent the real-world task. Here we address this issue by\ndeveloping and deploying a new efficient and highly performant SAR vessel\ndetection model, using a customised YOLOv8 architecture specifically optimized\nfor FPGA-based processing within common satellite power constraints (<10W). We\ntrain and evaluate our model on the largest and most diverse open SAR vessel\ndataset, xView3-SAR, and deploy it on a Kria KV260 MPSoC. We show that our\nFPGA-based model has detection and classification performance only ~2% and 3%\nlower than values from state-of-the-art GPU-based models, despite being two to\nthree orders of magnitude smaller in size. This work demonstrates small yet\nhighly performant ML models for time-critical SAR analysis, paving the way for\nmore autonomous, responsive, and scalable Earth observation systems.\n", "link": "http://arxiv.org/abs/2507.04842v1", "date": "2025-07-07", "relevancy": 1.4854, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5116}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5049}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20SAR%20Vessel%20Detection%20for%20FPGA-Based%20On-Satellite%20Sensing&body=Title%3A%20Efficient%20SAR%20Vessel%20Detection%20for%20FPGA-Based%20On-Satellite%20Sensing%0AAuthor%3A%20Colin%20Laganier%20and%20Liam%20Fletcher%20and%20Elim%20Kwan%20and%20Richard%20Walters%20and%20Victoria%20Nockles%0AAbstract%3A%20%20%20Rapid%20analysis%20of%20satellite%20data%20is%20vital%20for%20many%20remote%20sensing%0Aapplications%2C%20from%20disaster%20response%20to%20environmental%20monitoring%2C%20but%20is%0Abecoming%20harder%20to%20achieve%20with%20the%20increasing%20volumes%20of%20data%20generated%20by%0Amodern%20satellites.%20On-satellite%20machine%20learning%20%28ML%29%20offers%20a%20potential%0Asolution%2C%20by%20reducing%20latency%20associated%20with%20transmission%20of%20these%20large%20data%0Avolumes%20to%20ground%20stations%2C%20but%20state-of-the-art%20models%20are%20often%20too%20large%20or%0Apower-hungry%20for%20satellite%20deployment.%20Vessel%20detection%20using%20Synthetic%0AAperture%20Radar%20%28SAR%29%20is%20a%20critical%20time-sensitive%20task%20for%20maritime%20security%0Athat%20exemplifies%20this%20challenge.%20SAR%20vessel%20detection%20has%20previously%20been%0Ademonstrated%20only%20by%20ML%20models%20that%20either%20are%20too%20large%20for%20satellite%0Adeployment%2C%20have%20not%20been%20developed%20for%20sufficiently%20low-power%20hardware%2C%20or%0Ahave%20only%20been%20developed%20and%20tested%20on%20small%20SAR%20datasets%20that%20do%20not%0Asufficiently%20represent%20the%20real-world%20task.%20Here%20we%20address%20this%20issue%20by%0Adeveloping%20and%20deploying%20a%20new%20efficient%20and%20highly%20performant%20SAR%20vessel%0Adetection%20model%2C%20using%20a%20customised%20YOLOv8%20architecture%20specifically%20optimized%0Afor%20FPGA-based%20processing%20within%20common%20satellite%20power%20constraints%20%28%3C10W%29.%20We%0Atrain%20and%20evaluate%20our%20model%20on%20the%20largest%20and%20most%20diverse%20open%20SAR%20vessel%0Adataset%2C%20xView3-SAR%2C%20and%20deploy%20it%20on%20a%20Kria%20KV260%20MPSoC.%20We%20show%20that%20our%0AFPGA-based%20model%20has%20detection%20and%20classification%20performance%20only%20~2%25%20and%203%25%0Alower%20than%20values%20from%20state-of-the-art%20GPU-based%20models%2C%20despite%20being%20two%20to%0Athree%20orders%20of%20magnitude%20smaller%20in%20size.%20This%20work%20demonstrates%20small%20yet%0Ahighly%20performant%20ML%20models%20for%20time-critical%20SAR%20analysis%2C%20paving%20the%20way%20for%0Amore%20autonomous%2C%20responsive%2C%20and%20scalable%20Earth%20observation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520SAR%2520Vessel%2520Detection%2520for%2520FPGA-Based%2520On-Satellite%2520Sensing%26entry.906535625%3DColin%2520Laganier%2520and%2520Liam%2520Fletcher%2520and%2520Elim%2520Kwan%2520and%2520Richard%2520Walters%2520and%2520Victoria%2520Nockles%26entry.1292438233%3D%2520%2520Rapid%2520analysis%2520of%2520satellite%2520data%2520is%2520vital%2520for%2520many%2520remote%2520sensing%250Aapplications%252C%2520from%2520disaster%2520response%2520to%2520environmental%2520monitoring%252C%2520but%2520is%250Abecoming%2520harder%2520to%2520achieve%2520with%2520the%2520increasing%2520volumes%2520of%2520data%2520generated%2520by%250Amodern%2520satellites.%2520On-satellite%2520machine%2520learning%2520%2528ML%2529%2520offers%2520a%2520potential%250Asolution%252C%2520by%2520reducing%2520latency%2520associated%2520with%2520transmission%2520of%2520these%2520large%2520data%250Avolumes%2520to%2520ground%2520stations%252C%2520but%2520state-of-the-art%2520models%2520are%2520often%2520too%2520large%2520or%250Apower-hungry%2520for%2520satellite%2520deployment.%2520Vessel%2520detection%2520using%2520Synthetic%250AAperture%2520Radar%2520%2528SAR%2529%2520is%2520a%2520critical%2520time-sensitive%2520task%2520for%2520maritime%2520security%250Athat%2520exemplifies%2520this%2520challenge.%2520SAR%2520vessel%2520detection%2520has%2520previously%2520been%250Ademonstrated%2520only%2520by%2520ML%2520models%2520that%2520either%2520are%2520too%2520large%2520for%2520satellite%250Adeployment%252C%2520have%2520not%2520been%2520developed%2520for%2520sufficiently%2520low-power%2520hardware%252C%2520or%250Ahave%2520only%2520been%2520developed%2520and%2520tested%2520on%2520small%2520SAR%2520datasets%2520that%2520do%2520not%250Asufficiently%2520represent%2520the%2520real-world%2520task.%2520Here%2520we%2520address%2520this%2520issue%2520by%250Adeveloping%2520and%2520deploying%2520a%2520new%2520efficient%2520and%2520highly%2520performant%2520SAR%2520vessel%250Adetection%2520model%252C%2520using%2520a%2520customised%2520YOLOv8%2520architecture%2520specifically%2520optimized%250Afor%2520FPGA-based%2520processing%2520within%2520common%2520satellite%2520power%2520constraints%2520%2528%253C10W%2529.%2520We%250Atrain%2520and%2520evaluate%2520our%2520model%2520on%2520the%2520largest%2520and%2520most%2520diverse%2520open%2520SAR%2520vessel%250Adataset%252C%2520xView3-SAR%252C%2520and%2520deploy%2520it%2520on%2520a%2520Kria%2520KV260%2520MPSoC.%2520We%2520show%2520that%2520our%250AFPGA-based%2520model%2520has%2520detection%2520and%2520classification%2520performance%2520only%2520~2%2525%2520and%25203%2525%250Alower%2520than%2520values%2520from%2520state-of-the-art%2520GPU-based%2520models%252C%2520despite%2520being%2520two%2520to%250Athree%2520orders%2520of%2520magnitude%2520smaller%2520in%2520size.%2520This%2520work%2520demonstrates%2520small%2520yet%250Ahighly%2520performant%2520ML%2520models%2520for%2520time-critical%2520SAR%2520analysis%252C%2520paving%2520the%2520way%2520for%250Amore%2520autonomous%252C%2520responsive%252C%2520and%2520scalable%2520Earth%2520observation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20SAR%20Vessel%20Detection%20for%20FPGA-Based%20On-Satellite%20Sensing&entry.906535625=Colin%20Laganier%20and%20Liam%20Fletcher%20and%20Elim%20Kwan%20and%20Richard%20Walters%20and%20Victoria%20Nockles&entry.1292438233=%20%20Rapid%20analysis%20of%20satellite%20data%20is%20vital%20for%20many%20remote%20sensing%0Aapplications%2C%20from%20disaster%20response%20to%20environmental%20monitoring%2C%20but%20is%0Abecoming%20harder%20to%20achieve%20with%20the%20increasing%20volumes%20of%20data%20generated%20by%0Amodern%20satellites.%20On-satellite%20machine%20learning%20%28ML%29%20offers%20a%20potential%0Asolution%2C%20by%20reducing%20latency%20associated%20with%20transmission%20of%20these%20large%20data%0Avolumes%20to%20ground%20stations%2C%20but%20state-of-the-art%20models%20are%20often%20too%20large%20or%0Apower-hungry%20for%20satellite%20deployment.%20Vessel%20detection%20using%20Synthetic%0AAperture%20Radar%20%28SAR%29%20is%20a%20critical%20time-sensitive%20task%20for%20maritime%20security%0Athat%20exemplifies%20this%20challenge.%20SAR%20vessel%20detection%20has%20previously%20been%0Ademonstrated%20only%20by%20ML%20models%20that%20either%20are%20too%20large%20for%20satellite%0Adeployment%2C%20have%20not%20been%20developed%20for%20sufficiently%20low-power%20hardware%2C%20or%0Ahave%20only%20been%20developed%20and%20tested%20on%20small%20SAR%20datasets%20that%20do%20not%0Asufficiently%20represent%20the%20real-world%20task.%20Here%20we%20address%20this%20issue%20by%0Adeveloping%20and%20deploying%20a%20new%20efficient%20and%20highly%20performant%20SAR%20vessel%0Adetection%20model%2C%20using%20a%20customised%20YOLOv8%20architecture%20specifically%20optimized%0Afor%20FPGA-based%20processing%20within%20common%20satellite%20power%20constraints%20%28%3C10W%29.%20We%0Atrain%20and%20evaluate%20our%20model%20on%20the%20largest%20and%20most%20diverse%20open%20SAR%20vessel%0Adataset%2C%20xView3-SAR%2C%20and%20deploy%20it%20on%20a%20Kria%20KV260%20MPSoC.%20We%20show%20that%20our%0AFPGA-based%20model%20has%20detection%20and%20classification%20performance%20only%20~2%25%20and%203%25%0Alower%20than%20values%20from%20state-of-the-art%20GPU-based%20models%2C%20despite%20being%20two%20to%0Athree%20orders%20of%20magnitude%20smaller%20in%20size.%20This%20work%20demonstrates%20small%20yet%0Ahighly%20performant%20ML%20models%20for%20time-critical%20SAR%20analysis%2C%20paving%20the%20way%20for%0Amore%20autonomous%2C%20responsive%2C%20and%20scalable%20Earth%20observation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04842v1&entry.124074799=Read"},
{"title": "When Chain of Thought is Necessary, Language Models Struggle to Evade\n  Monitors", "author": "Scott Emmons and Erik Jenner and David K. Elson and Rif A. Saurous and Senthooran Rajamanoharan and Heng Chen and Irhum Shafkat and Rohin Shah", "abstract": "  While chain-of-thought (CoT) monitoring is an appealing AI safety defense,\nrecent work on \"unfaithfulness\" has cast doubt on its reliability. These\nfindings highlight an important failure mode, particularly when CoT acts as a\npost-hoc rationalization in applications like auditing for bias. However, for\nthe distinct problem of runtime monitoring to prevent severe harm, we argue the\nkey property is not faithfulness but monitorability. To this end, we introduce\na conceptual framework distinguishing CoT-as-rationalization from\nCoT-as-computation. We expect that certain classes of severe harm will require\ncomplex, multi-step reasoning that necessitates CoT-as-computation. Replicating\nthe experimental setups of prior work, we increase the difficulty of the bad\nbehavior to enforce this necessity condition; this forces the model to expose\nits reasoning, making it monitorable. We then present methodology guidelines to\nstress-test CoT monitoring against deliberate evasion. Applying these\nguidelines, we find that models can learn to obscure their intentions, but only\nwhen given significant help, such as detailed human-written strategies or\niterative optimization against the monitor. We conclude that, while not\ninfallible, CoT monitoring offers a substantial layer of defense that requires\nactive protection and continued stress-testing.\n", "link": "http://arxiv.org/abs/2507.05246v1", "date": "2025-07-07", "relevancy": 1.765, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4558}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4383}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Chain%20of%20Thought%20is%20Necessary%2C%20Language%20Models%20Struggle%20to%20Evade%0A%20%20Monitors&body=Title%3A%20When%20Chain%20of%20Thought%20is%20Necessary%2C%20Language%20Models%20Struggle%20to%20Evade%0A%20%20Monitors%0AAuthor%3A%20Scott%20Emmons%20and%20Erik%20Jenner%20and%20David%20K.%20Elson%20and%20Rif%20A.%20Saurous%20and%20Senthooran%20Rajamanoharan%20and%20Heng%20Chen%20and%20Irhum%20Shafkat%20and%20Rohin%20Shah%0AAbstract%3A%20%20%20While%20chain-of-thought%20%28CoT%29%20monitoring%20is%20an%20appealing%20AI%20safety%20defense%2C%0Arecent%20work%20on%20%22unfaithfulness%22%20has%20cast%20doubt%20on%20its%20reliability.%20These%0Afindings%20highlight%20an%20important%20failure%20mode%2C%20particularly%20when%20CoT%20acts%20as%20a%0Apost-hoc%20rationalization%20in%20applications%20like%20auditing%20for%20bias.%20However%2C%20for%0Athe%20distinct%20problem%20of%20runtime%20monitoring%20to%20prevent%20severe%20harm%2C%20we%20argue%20the%0Akey%20property%20is%20not%20faithfulness%20but%20monitorability.%20To%20this%20end%2C%20we%20introduce%0Aa%20conceptual%20framework%20distinguishing%20CoT-as-rationalization%20from%0ACoT-as-computation.%20We%20expect%20that%20certain%20classes%20of%20severe%20harm%20will%20require%0Acomplex%2C%20multi-step%20reasoning%20that%20necessitates%20CoT-as-computation.%20Replicating%0Athe%20experimental%20setups%20of%20prior%20work%2C%20we%20increase%20the%20difficulty%20of%20the%20bad%0Abehavior%20to%20enforce%20this%20necessity%20condition%3B%20this%20forces%20the%20model%20to%20expose%0Aits%20reasoning%2C%20making%20it%20monitorable.%20We%20then%20present%20methodology%20guidelines%20to%0Astress-test%20CoT%20monitoring%20against%20deliberate%20evasion.%20Applying%20these%0Aguidelines%2C%20we%20find%20that%20models%20can%20learn%20to%20obscure%20their%20intentions%2C%20but%20only%0Awhen%20given%20significant%20help%2C%20such%20as%20detailed%20human-written%20strategies%20or%0Aiterative%20optimization%20against%20the%20monitor.%20We%20conclude%20that%2C%20while%20not%0Ainfallible%2C%20CoT%20monitoring%20offers%20a%20substantial%20layer%20of%20defense%20that%20requires%0Aactive%20protection%20and%20continued%20stress-testing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Chain%2520of%2520Thought%2520is%2520Necessary%252C%2520Language%2520Models%2520Struggle%2520to%2520Evade%250A%2520%2520Monitors%26entry.906535625%3DScott%2520Emmons%2520and%2520Erik%2520Jenner%2520and%2520David%2520K.%2520Elson%2520and%2520Rif%2520A.%2520Saurous%2520and%2520Senthooran%2520Rajamanoharan%2520and%2520Heng%2520Chen%2520and%2520Irhum%2520Shafkat%2520and%2520Rohin%2520Shah%26entry.1292438233%3D%2520%2520While%2520chain-of-thought%2520%2528CoT%2529%2520monitoring%2520is%2520an%2520appealing%2520AI%2520safety%2520defense%252C%250Arecent%2520work%2520on%2520%2522unfaithfulness%2522%2520has%2520cast%2520doubt%2520on%2520its%2520reliability.%2520These%250Afindings%2520highlight%2520an%2520important%2520failure%2520mode%252C%2520particularly%2520when%2520CoT%2520acts%2520as%2520a%250Apost-hoc%2520rationalization%2520in%2520applications%2520like%2520auditing%2520for%2520bias.%2520However%252C%2520for%250Athe%2520distinct%2520problem%2520of%2520runtime%2520monitoring%2520to%2520prevent%2520severe%2520harm%252C%2520we%2520argue%2520the%250Akey%2520property%2520is%2520not%2520faithfulness%2520but%2520monitorability.%2520To%2520this%2520end%252C%2520we%2520introduce%250Aa%2520conceptual%2520framework%2520distinguishing%2520CoT-as-rationalization%2520from%250ACoT-as-computation.%2520We%2520expect%2520that%2520certain%2520classes%2520of%2520severe%2520harm%2520will%2520require%250Acomplex%252C%2520multi-step%2520reasoning%2520that%2520necessitates%2520CoT-as-computation.%2520Replicating%250Athe%2520experimental%2520setups%2520of%2520prior%2520work%252C%2520we%2520increase%2520the%2520difficulty%2520of%2520the%2520bad%250Abehavior%2520to%2520enforce%2520this%2520necessity%2520condition%253B%2520this%2520forces%2520the%2520model%2520to%2520expose%250Aits%2520reasoning%252C%2520making%2520it%2520monitorable.%2520We%2520then%2520present%2520methodology%2520guidelines%2520to%250Astress-test%2520CoT%2520monitoring%2520against%2520deliberate%2520evasion.%2520Applying%2520these%250Aguidelines%252C%2520we%2520find%2520that%2520models%2520can%2520learn%2520to%2520obscure%2520their%2520intentions%252C%2520but%2520only%250Awhen%2520given%2520significant%2520help%252C%2520such%2520as%2520detailed%2520human-written%2520strategies%2520or%250Aiterative%2520optimization%2520against%2520the%2520monitor.%2520We%2520conclude%2520that%252C%2520while%2520not%250Ainfallible%252C%2520CoT%2520monitoring%2520offers%2520a%2520substantial%2520layer%2520of%2520defense%2520that%2520requires%250Aactive%2520protection%2520and%2520continued%2520stress-testing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Chain%20of%20Thought%20is%20Necessary%2C%20Language%20Models%20Struggle%20to%20Evade%0A%20%20Monitors&entry.906535625=Scott%20Emmons%20and%20Erik%20Jenner%20and%20David%20K.%20Elson%20and%20Rif%20A.%20Saurous%20and%20Senthooran%20Rajamanoharan%20and%20Heng%20Chen%20and%20Irhum%20Shafkat%20and%20Rohin%20Shah&entry.1292438233=%20%20While%20chain-of-thought%20%28CoT%29%20monitoring%20is%20an%20appealing%20AI%20safety%20defense%2C%0Arecent%20work%20on%20%22unfaithfulness%22%20has%20cast%20doubt%20on%20its%20reliability.%20These%0Afindings%20highlight%20an%20important%20failure%20mode%2C%20particularly%20when%20CoT%20acts%20as%20a%0Apost-hoc%20rationalization%20in%20applications%20like%20auditing%20for%20bias.%20However%2C%20for%0Athe%20distinct%20problem%20of%20runtime%20monitoring%20to%20prevent%20severe%20harm%2C%20we%20argue%20the%0Akey%20property%20is%20not%20faithfulness%20but%20monitorability.%20To%20this%20end%2C%20we%20introduce%0Aa%20conceptual%20framework%20distinguishing%20CoT-as-rationalization%20from%0ACoT-as-computation.%20We%20expect%20that%20certain%20classes%20of%20severe%20harm%20will%20require%0Acomplex%2C%20multi-step%20reasoning%20that%20necessitates%20CoT-as-computation.%20Replicating%0Athe%20experimental%20setups%20of%20prior%20work%2C%20we%20increase%20the%20difficulty%20of%20the%20bad%0Abehavior%20to%20enforce%20this%20necessity%20condition%3B%20this%20forces%20the%20model%20to%20expose%0Aits%20reasoning%2C%20making%20it%20monitorable.%20We%20then%20present%20methodology%20guidelines%20to%0Astress-test%20CoT%20monitoring%20against%20deliberate%20evasion.%20Applying%20these%0Aguidelines%2C%20we%20find%20that%20models%20can%20learn%20to%20obscure%20their%20intentions%2C%20but%20only%0Awhen%20given%20significant%20help%2C%20such%20as%20detailed%20human-written%20strategies%20or%0Aiterative%20optimization%20against%20the%20monitor.%20We%20conclude%20that%2C%20while%20not%0Ainfallible%2C%20CoT%20monitoring%20offers%20a%20substantial%20layer%20of%20defense%20that%20requires%0Aactive%20protection%20and%20continued%20stress-testing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05246v1&entry.124074799=Read"},
{"title": "Object-centric Denoising Diffusion Models for Physical Reasoning", "author": "Moritz Lange and Raphael C. Engelhardt and Wolfgang Konen and Andrew Melnik and Laurenz Wiskott", "abstract": "  Reasoning about the trajectories of multiple, interacting objects is integral\nto physical reasoning tasks in machine learning. This involves conditions\nimposed on the objects at different time steps, for instance initial states or\ndesired goal states. Existing approaches in physical reasoning generally rely\non autoregressive modeling, which can only be conditioned on initial states,\nbut not on later states. In fields such as planning for reinforcement learning,\nsimilar challenges are being addressed with denoising diffusion models. In this\nwork, we propose an object-centric denoising diffusion model architecture for\nphysical reasoning that is translation equivariant over time, permutation\nequivariant over objects, and can be conditioned on arbitrary time steps for\narbitrary objects. We demonstrate how this model can solve tasks with multiple\nconditions and examine its performance when changing object numbers and\ntrajectory lengths during inference.\n", "link": "http://arxiv.org/abs/2507.04920v1", "date": "2025-07-07", "relevancy": 1.148, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6045}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5645}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object-centric%20Denoising%20Diffusion%20Models%20for%20Physical%20Reasoning&body=Title%3A%20Object-centric%20Denoising%20Diffusion%20Models%20for%20Physical%20Reasoning%0AAuthor%3A%20Moritz%20Lange%20and%20Raphael%20C.%20Engelhardt%20and%20Wolfgang%20Konen%20and%20Andrew%20Melnik%20and%20Laurenz%20Wiskott%0AAbstract%3A%20%20%20Reasoning%20about%20the%20trajectories%20of%20multiple%2C%20interacting%20objects%20is%20integral%0Ato%20physical%20reasoning%20tasks%20in%20machine%20learning.%20This%20involves%20conditions%0Aimposed%20on%20the%20objects%20at%20different%20time%20steps%2C%20for%20instance%20initial%20states%20or%0Adesired%20goal%20states.%20Existing%20approaches%20in%20physical%20reasoning%20generally%20rely%0Aon%20autoregressive%20modeling%2C%20which%20can%20only%20be%20conditioned%20on%20initial%20states%2C%0Abut%20not%20on%20later%20states.%20In%20fields%20such%20as%20planning%20for%20reinforcement%20learning%2C%0Asimilar%20challenges%20are%20being%20addressed%20with%20denoising%20diffusion%20models.%20In%20this%0Awork%2C%20we%20propose%20an%20object-centric%20denoising%20diffusion%20model%20architecture%20for%0Aphysical%20reasoning%20that%20is%20translation%20equivariant%20over%20time%2C%20permutation%0Aequivariant%20over%20objects%2C%20and%20can%20be%20conditioned%20on%20arbitrary%20time%20steps%20for%0Aarbitrary%20objects.%20We%20demonstrate%20how%20this%20model%20can%20solve%20tasks%20with%20multiple%0Aconditions%20and%20examine%20its%20performance%20when%20changing%20object%20numbers%20and%0Atrajectory%20lengths%20during%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject-centric%2520Denoising%2520Diffusion%2520Models%2520for%2520Physical%2520Reasoning%26entry.906535625%3DMoritz%2520Lange%2520and%2520Raphael%2520C.%2520Engelhardt%2520and%2520Wolfgang%2520Konen%2520and%2520Andrew%2520Melnik%2520and%2520Laurenz%2520Wiskott%26entry.1292438233%3D%2520%2520Reasoning%2520about%2520the%2520trajectories%2520of%2520multiple%252C%2520interacting%2520objects%2520is%2520integral%250Ato%2520physical%2520reasoning%2520tasks%2520in%2520machine%2520learning.%2520This%2520involves%2520conditions%250Aimposed%2520on%2520the%2520objects%2520at%2520different%2520time%2520steps%252C%2520for%2520instance%2520initial%2520states%2520or%250Adesired%2520goal%2520states.%2520Existing%2520approaches%2520in%2520physical%2520reasoning%2520generally%2520rely%250Aon%2520autoregressive%2520modeling%252C%2520which%2520can%2520only%2520be%2520conditioned%2520on%2520initial%2520states%252C%250Abut%2520not%2520on%2520later%2520states.%2520In%2520fields%2520such%2520as%2520planning%2520for%2520reinforcement%2520learning%252C%250Asimilar%2520challenges%2520are%2520being%2520addressed%2520with%2520denoising%2520diffusion%2520models.%2520In%2520this%250Awork%252C%2520we%2520propose%2520an%2520object-centric%2520denoising%2520diffusion%2520model%2520architecture%2520for%250Aphysical%2520reasoning%2520that%2520is%2520translation%2520equivariant%2520over%2520time%252C%2520permutation%250Aequivariant%2520over%2520objects%252C%2520and%2520can%2520be%2520conditioned%2520on%2520arbitrary%2520time%2520steps%2520for%250Aarbitrary%2520objects.%2520We%2520demonstrate%2520how%2520this%2520model%2520can%2520solve%2520tasks%2520with%2520multiple%250Aconditions%2520and%2520examine%2520its%2520performance%2520when%2520changing%2520object%2520numbers%2520and%250Atrajectory%2520lengths%2520during%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object-centric%20Denoising%20Diffusion%20Models%20for%20Physical%20Reasoning&entry.906535625=Moritz%20Lange%20and%20Raphael%20C.%20Engelhardt%20and%20Wolfgang%20Konen%20and%20Andrew%20Melnik%20and%20Laurenz%20Wiskott&entry.1292438233=%20%20Reasoning%20about%20the%20trajectories%20of%20multiple%2C%20interacting%20objects%20is%20integral%0Ato%20physical%20reasoning%20tasks%20in%20machine%20learning.%20This%20involves%20conditions%0Aimposed%20on%20the%20objects%20at%20different%20time%20steps%2C%20for%20instance%20initial%20states%20or%0Adesired%20goal%20states.%20Existing%20approaches%20in%20physical%20reasoning%20generally%20rely%0Aon%20autoregressive%20modeling%2C%20which%20can%20only%20be%20conditioned%20on%20initial%20states%2C%0Abut%20not%20on%20later%20states.%20In%20fields%20such%20as%20planning%20for%20reinforcement%20learning%2C%0Asimilar%20challenges%20are%20being%20addressed%20with%20denoising%20diffusion%20models.%20In%20this%0Awork%2C%20we%20propose%20an%20object-centric%20denoising%20diffusion%20model%20architecture%20for%0Aphysical%20reasoning%20that%20is%20translation%20equivariant%20over%20time%2C%20permutation%0Aequivariant%20over%20objects%2C%20and%20can%20be%20conditioned%20on%20arbitrary%20time%20steps%20for%0Aarbitrary%20objects.%20We%20demonstrate%20how%20this%20model%20can%20solve%20tasks%20with%20multiple%0Aconditions%20and%20examine%20its%20performance%20when%20changing%20object%20numbers%20and%0Atrajectory%20lengths%20during%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04920v1&entry.124074799=Read"},
{"title": "Gradient Purification: Defense Against Poisoning Attack in Decentralized\n  Federated Learning", "author": "Bin Li and Xiaoye Miao and Yan Zhang and Jianwei Yin", "abstract": "  Decentralized federated learning (DFL) is inherently vulnerable to data\npoisoning attacks, as malicious clients can transmit manipulated gradients to\nneighboring clients. Existing defense methods either reject suspicious\ngradients per iteration or restart DFL aggregation after excluding all\nmalicious clients. They all neglect the potential benefits that may exist\nwithin contributions from malicious clients. In this paper, we propose a novel\ngradient purification defense, termed GPD, to defend against data poisoning\nattacks in DFL. It aims to separately mitigate the harm in gradients and retain\nbenefits embedded in model weights, thereby enhancing overall model accuracy.\nFor each benign client in GPD, a recording variable is designed to track\nhistorically aggregated gradients from one of its neighbors. It allows benign\nclients to precisely detect malicious neighbors and mitigate all aggregated\nmalicious gradients at once. Upon mitigation, benign clients optimize model\nweights using purified gradients. This optimization not only retains previously\nbeneficial components from malicious clients but also exploits canonical\ncontributions from benign clients. We analyze the convergence of GPD, as well\nas its ability to harvest high accuracy. Extensive experiments demonstrate\nthat, GPD is capable of mitigating data poisoning attacks under both iid and\nnon-iid data distributions. It also significantly outperforms state-of-the-art\ndefense methods in terms of model accuracy.\n", "link": "http://arxiv.org/abs/2501.04453v3", "date": "2025-07-07", "relevancy": 1.8604, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4688}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4627}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Purification%3A%20Defense%20Against%20Poisoning%20Attack%20in%20Decentralized%0A%20%20Federated%20Learning&body=Title%3A%20Gradient%20Purification%3A%20Defense%20Against%20Poisoning%20Attack%20in%20Decentralized%0A%20%20Federated%20Learning%0AAuthor%3A%20Bin%20Li%20and%20Xiaoye%20Miao%20and%20Yan%20Zhang%20and%20Jianwei%20Yin%0AAbstract%3A%20%20%20Decentralized%20federated%20learning%20%28DFL%29%20is%20inherently%20vulnerable%20to%20data%0Apoisoning%20attacks%2C%20as%20malicious%20clients%20can%20transmit%20manipulated%20gradients%20to%0Aneighboring%20clients.%20Existing%20defense%20methods%20either%20reject%20suspicious%0Agradients%20per%20iteration%20or%20restart%20DFL%20aggregation%20after%20excluding%20all%0Amalicious%20clients.%20They%20all%20neglect%20the%20potential%20benefits%20that%20may%20exist%0Awithin%20contributions%20from%20malicious%20clients.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Agradient%20purification%20defense%2C%20termed%20GPD%2C%20to%20defend%20against%20data%20poisoning%0Aattacks%20in%20DFL.%20It%20aims%20to%20separately%20mitigate%20the%20harm%20in%20gradients%20and%20retain%0Abenefits%20embedded%20in%20model%20weights%2C%20thereby%20enhancing%20overall%20model%20accuracy.%0AFor%20each%20benign%20client%20in%20GPD%2C%20a%20recording%20variable%20is%20designed%20to%20track%0Ahistorically%20aggregated%20gradients%20from%20one%20of%20its%20neighbors.%20It%20allows%20benign%0Aclients%20to%20precisely%20detect%20malicious%20neighbors%20and%20mitigate%20all%20aggregated%0Amalicious%20gradients%20at%20once.%20Upon%20mitigation%2C%20benign%20clients%20optimize%20model%0Aweights%20using%20purified%20gradients.%20This%20optimization%20not%20only%20retains%20previously%0Abeneficial%20components%20from%20malicious%20clients%20but%20also%20exploits%20canonical%0Acontributions%20from%20benign%20clients.%20We%20analyze%20the%20convergence%20of%20GPD%2C%20as%20well%0Aas%20its%20ability%20to%20harvest%20high%20accuracy.%20Extensive%20experiments%20demonstrate%0Athat%2C%20GPD%20is%20capable%20of%20mitigating%20data%20poisoning%20attacks%20under%20both%20iid%20and%0Anon-iid%20data%20distributions.%20It%20also%20significantly%20outperforms%20state-of-the-art%0Adefense%20methods%20in%20terms%20of%20model%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04453v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Purification%253A%2520Defense%2520Against%2520Poisoning%2520Attack%2520in%2520Decentralized%250A%2520%2520Federated%2520Learning%26entry.906535625%3DBin%2520Li%2520and%2520Xiaoye%2520Miao%2520and%2520Yan%2520Zhang%2520and%2520Jianwei%2520Yin%26entry.1292438233%3D%2520%2520Decentralized%2520federated%2520learning%2520%2528DFL%2529%2520is%2520inherently%2520vulnerable%2520to%2520data%250Apoisoning%2520attacks%252C%2520as%2520malicious%2520clients%2520can%2520transmit%2520manipulated%2520gradients%2520to%250Aneighboring%2520clients.%2520Existing%2520defense%2520methods%2520either%2520reject%2520suspicious%250Agradients%2520per%2520iteration%2520or%2520restart%2520DFL%2520aggregation%2520after%2520excluding%2520all%250Amalicious%2520clients.%2520They%2520all%2520neglect%2520the%2520potential%2520benefits%2520that%2520may%2520exist%250Awithin%2520contributions%2520from%2520malicious%2520clients.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Agradient%2520purification%2520defense%252C%2520termed%2520GPD%252C%2520to%2520defend%2520against%2520data%2520poisoning%250Aattacks%2520in%2520DFL.%2520It%2520aims%2520to%2520separately%2520mitigate%2520the%2520harm%2520in%2520gradients%2520and%2520retain%250Abenefits%2520embedded%2520in%2520model%2520weights%252C%2520thereby%2520enhancing%2520overall%2520model%2520accuracy.%250AFor%2520each%2520benign%2520client%2520in%2520GPD%252C%2520a%2520recording%2520variable%2520is%2520designed%2520to%2520track%250Ahistorically%2520aggregated%2520gradients%2520from%2520one%2520of%2520its%2520neighbors.%2520It%2520allows%2520benign%250Aclients%2520to%2520precisely%2520detect%2520malicious%2520neighbors%2520and%2520mitigate%2520all%2520aggregated%250Amalicious%2520gradients%2520at%2520once.%2520Upon%2520mitigation%252C%2520benign%2520clients%2520optimize%2520model%250Aweights%2520using%2520purified%2520gradients.%2520This%2520optimization%2520not%2520only%2520retains%2520previously%250Abeneficial%2520components%2520from%2520malicious%2520clients%2520but%2520also%2520exploits%2520canonical%250Acontributions%2520from%2520benign%2520clients.%2520We%2520analyze%2520the%2520convergence%2520of%2520GPD%252C%2520as%2520well%250Aas%2520its%2520ability%2520to%2520harvest%2520high%2520accuracy.%2520Extensive%2520experiments%2520demonstrate%250Athat%252C%2520GPD%2520is%2520capable%2520of%2520mitigating%2520data%2520poisoning%2520attacks%2520under%2520both%2520iid%2520and%250Anon-iid%2520data%2520distributions.%2520It%2520also%2520significantly%2520outperforms%2520state-of-the-art%250Adefense%2520methods%2520in%2520terms%2520of%2520model%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04453v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Purification%3A%20Defense%20Against%20Poisoning%20Attack%20in%20Decentralized%0A%20%20Federated%20Learning&entry.906535625=Bin%20Li%20and%20Xiaoye%20Miao%20and%20Yan%20Zhang%20and%20Jianwei%20Yin&entry.1292438233=%20%20Decentralized%20federated%20learning%20%28DFL%29%20is%20inherently%20vulnerable%20to%20data%0Apoisoning%20attacks%2C%20as%20malicious%20clients%20can%20transmit%20manipulated%20gradients%20to%0Aneighboring%20clients.%20Existing%20defense%20methods%20either%20reject%20suspicious%0Agradients%20per%20iteration%20or%20restart%20DFL%20aggregation%20after%20excluding%20all%0Amalicious%20clients.%20They%20all%20neglect%20the%20potential%20benefits%20that%20may%20exist%0Awithin%20contributions%20from%20malicious%20clients.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Agradient%20purification%20defense%2C%20termed%20GPD%2C%20to%20defend%20against%20data%20poisoning%0Aattacks%20in%20DFL.%20It%20aims%20to%20separately%20mitigate%20the%20harm%20in%20gradients%20and%20retain%0Abenefits%20embedded%20in%20model%20weights%2C%20thereby%20enhancing%20overall%20model%20accuracy.%0AFor%20each%20benign%20client%20in%20GPD%2C%20a%20recording%20variable%20is%20designed%20to%20track%0Ahistorically%20aggregated%20gradients%20from%20one%20of%20its%20neighbors.%20It%20allows%20benign%0Aclients%20to%20precisely%20detect%20malicious%20neighbors%20and%20mitigate%20all%20aggregated%0Amalicious%20gradients%20at%20once.%20Upon%20mitigation%2C%20benign%20clients%20optimize%20model%0Aweights%20using%20purified%20gradients.%20This%20optimization%20not%20only%20retains%20previously%0Abeneficial%20components%20from%20malicious%20clients%20but%20also%20exploits%20canonical%0Acontributions%20from%20benign%20clients.%20We%20analyze%20the%20convergence%20of%20GPD%2C%20as%20well%0Aas%20its%20ability%20to%20harvest%20high%20accuracy.%20Extensive%20experiments%20demonstrate%0Athat%2C%20GPD%20is%20capable%20of%20mitigating%20data%20poisoning%20attacks%20under%20both%20iid%20and%0Anon-iid%20data%20distributions.%20It%20also%20significantly%20outperforms%20state-of-the-art%0Adefense%20methods%20in%20terms%20of%20model%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04453v3&entry.124074799=Read"},
{"title": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility\n  Systems", "author": "Jiangbo Yu", "abstract": "  Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are defined as systems capable of perceiving their\nenvironment and executing preprogrammed tasks independently of external input.\nHowever, both research and real-world deployments increasingly showcase\nvehicles that demonstrate behaviors beyond this definition (including the SAE\nlevels 1 to 6), such as interaction with humans and machines, goal adaptation,\ncontextual reasoning, external tool use, and long-term planning, particularly\nwith the integration of large language models (LLMs) and agentic AI systems.\nThese developments reveal a conceptual gap between technical autonomy and the\nbroader cognitive and social capabilities needed for future human-centered\nmobility systems. To address this, we introduce the concept of agentic vehicles\n(AgVs), referring to vehicles that integrate agentic AI to reason, adapt, and\ninteract within complex environments. This paper presents a systems-level\nframework to characterize AgVs, focusing on their cognitive and communicative\nlayers and differentiating them from conventional AuVs. It synthesizes relevant\nadvances in agentic AI, robotics, multi-agent systems, and human-machine\ninteraction, and highlights how agentic AI, through high-level reasoning and\ntool use, can function not merely as computational tools but as interactive\nagents embedded in mobility ecosystems. The paper concludes by identifying key\nchallenges in the development and governance of AgVs, including safety,\nreal-time control, public acceptance, ethical alignment, and regulatory\nframeworks.\n", "link": "http://arxiv.org/abs/2507.04996v1", "date": "2025-07-07", "relevancy": 1.6103, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5639}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5063}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Autonomy%20to%20Agency%3A%20Agentic%20Vehicles%20for%20Human-Centered%20Mobility%0A%20%20Systems&body=Title%3A%20From%20Autonomy%20to%20Agency%3A%20Agentic%20Vehicles%20for%20Human-Centered%20Mobility%0A%20%20Systems%0AAuthor%3A%20Jiangbo%20Yu%0AAbstract%3A%20%20%20Autonomy%2C%20from%20the%20Greek%20autos%20%28self%29%20and%20nomos%20%28law%29%2C%20refers%20to%20the%20capacity%0Ato%20operate%20according%20to%20internal%20rules%20without%20external%20control.%20Accordingly%2C%0Aautonomous%20vehicles%20%28AuVs%29%20are%20defined%20as%20systems%20capable%20of%20perceiving%20their%0Aenvironment%20and%20executing%20preprogrammed%20tasks%20independently%20of%20external%20input.%0AHowever%2C%20both%20research%20and%20real-world%20deployments%20increasingly%20showcase%0Avehicles%20that%20demonstrate%20behaviors%20beyond%20this%20definition%20%28including%20the%20SAE%0Alevels%201%20to%206%29%2C%20such%20as%20interaction%20with%20humans%20and%20machines%2C%20goal%20adaptation%2C%0Acontextual%20reasoning%2C%20external%20tool%20use%2C%20and%20long-term%20planning%2C%20particularly%0Awith%20the%20integration%20of%20large%20language%20models%20%28LLMs%29%20and%20agentic%20AI%20systems.%0AThese%20developments%20reveal%20a%20conceptual%20gap%20between%20technical%20autonomy%20and%20the%0Abroader%20cognitive%20and%20social%20capabilities%20needed%20for%20future%20human-centered%0Amobility%20systems.%20To%20address%20this%2C%20we%20introduce%20the%20concept%20of%20agentic%20vehicles%0A%28AgVs%29%2C%20referring%20to%20vehicles%20that%20integrate%20agentic%20AI%20to%20reason%2C%20adapt%2C%20and%0Ainteract%20within%20complex%20environments.%20This%20paper%20presents%20a%20systems-level%0Aframework%20to%20characterize%20AgVs%2C%20focusing%20on%20their%20cognitive%20and%20communicative%0Alayers%20and%20differentiating%20them%20from%20conventional%20AuVs.%20It%20synthesizes%20relevant%0Aadvances%20in%20agentic%20AI%2C%20robotics%2C%20multi-agent%20systems%2C%20and%20human-machine%0Ainteraction%2C%20and%20highlights%20how%20agentic%20AI%2C%20through%20high-level%20reasoning%20and%0Atool%20use%2C%20can%20function%20not%20merely%20as%20computational%20tools%20but%20as%20interactive%0Aagents%20embedded%20in%20mobility%20ecosystems.%20The%20paper%20concludes%20by%20identifying%20key%0Achallenges%20in%20the%20development%20and%20governance%20of%20AgVs%2C%20including%20safety%2C%0Areal-time%20control%2C%20public%20acceptance%2C%20ethical%20alignment%2C%20and%20regulatory%0Aframeworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Autonomy%2520to%2520Agency%253A%2520Agentic%2520Vehicles%2520for%2520Human-Centered%2520Mobility%250A%2520%2520Systems%26entry.906535625%3DJiangbo%2520Yu%26entry.1292438233%3D%2520%2520Autonomy%252C%2520from%2520the%2520Greek%2520autos%2520%2528self%2529%2520and%2520nomos%2520%2528law%2529%252C%2520refers%2520to%2520the%2520capacity%250Ato%2520operate%2520according%2520to%2520internal%2520rules%2520without%2520external%2520control.%2520Accordingly%252C%250Aautonomous%2520vehicles%2520%2528AuVs%2529%2520are%2520defined%2520as%2520systems%2520capable%2520of%2520perceiving%2520their%250Aenvironment%2520and%2520executing%2520preprogrammed%2520tasks%2520independently%2520of%2520external%2520input.%250AHowever%252C%2520both%2520research%2520and%2520real-world%2520deployments%2520increasingly%2520showcase%250Avehicles%2520that%2520demonstrate%2520behaviors%2520beyond%2520this%2520definition%2520%2528including%2520the%2520SAE%250Alevels%25201%2520to%25206%2529%252C%2520such%2520as%2520interaction%2520with%2520humans%2520and%2520machines%252C%2520goal%2520adaptation%252C%250Acontextual%2520reasoning%252C%2520external%2520tool%2520use%252C%2520and%2520long-term%2520planning%252C%2520particularly%250Awith%2520the%2520integration%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520agentic%2520AI%2520systems.%250AThese%2520developments%2520reveal%2520a%2520conceptual%2520gap%2520between%2520technical%2520autonomy%2520and%2520the%250Abroader%2520cognitive%2520and%2520social%2520capabilities%2520needed%2520for%2520future%2520human-centered%250Amobility%2520systems.%2520To%2520address%2520this%252C%2520we%2520introduce%2520the%2520concept%2520of%2520agentic%2520vehicles%250A%2528AgVs%2529%252C%2520referring%2520to%2520vehicles%2520that%2520integrate%2520agentic%2520AI%2520to%2520reason%252C%2520adapt%252C%2520and%250Ainteract%2520within%2520complex%2520environments.%2520This%2520paper%2520presents%2520a%2520systems-level%250Aframework%2520to%2520characterize%2520AgVs%252C%2520focusing%2520on%2520their%2520cognitive%2520and%2520communicative%250Alayers%2520and%2520differentiating%2520them%2520from%2520conventional%2520AuVs.%2520It%2520synthesizes%2520relevant%250Aadvances%2520in%2520agentic%2520AI%252C%2520robotics%252C%2520multi-agent%2520systems%252C%2520and%2520human-machine%250Ainteraction%252C%2520and%2520highlights%2520how%2520agentic%2520AI%252C%2520through%2520high-level%2520reasoning%2520and%250Atool%2520use%252C%2520can%2520function%2520not%2520merely%2520as%2520computational%2520tools%2520but%2520as%2520interactive%250Aagents%2520embedded%2520in%2520mobility%2520ecosystems.%2520The%2520paper%2520concludes%2520by%2520identifying%2520key%250Achallenges%2520in%2520the%2520development%2520and%2520governance%2520of%2520AgVs%252C%2520including%2520safety%252C%250Areal-time%2520control%252C%2520public%2520acceptance%252C%2520ethical%2520alignment%252C%2520and%2520regulatory%250Aframeworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Autonomy%20to%20Agency%3A%20Agentic%20Vehicles%20for%20Human-Centered%20Mobility%0A%20%20Systems&entry.906535625=Jiangbo%20Yu&entry.1292438233=%20%20Autonomy%2C%20from%20the%20Greek%20autos%20%28self%29%20and%20nomos%20%28law%29%2C%20refers%20to%20the%20capacity%0Ato%20operate%20according%20to%20internal%20rules%20without%20external%20control.%20Accordingly%2C%0Aautonomous%20vehicles%20%28AuVs%29%20are%20defined%20as%20systems%20capable%20of%20perceiving%20their%0Aenvironment%20and%20executing%20preprogrammed%20tasks%20independently%20of%20external%20input.%0AHowever%2C%20both%20research%20and%20real-world%20deployments%20increasingly%20showcase%0Avehicles%20that%20demonstrate%20behaviors%20beyond%20this%20definition%20%28including%20the%20SAE%0Alevels%201%20to%206%29%2C%20such%20as%20interaction%20with%20humans%20and%20machines%2C%20goal%20adaptation%2C%0Acontextual%20reasoning%2C%20external%20tool%20use%2C%20and%20long-term%20planning%2C%20particularly%0Awith%20the%20integration%20of%20large%20language%20models%20%28LLMs%29%20and%20agentic%20AI%20systems.%0AThese%20developments%20reveal%20a%20conceptual%20gap%20between%20technical%20autonomy%20and%20the%0Abroader%20cognitive%20and%20social%20capabilities%20needed%20for%20future%20human-centered%0Amobility%20systems.%20To%20address%20this%2C%20we%20introduce%20the%20concept%20of%20agentic%20vehicles%0A%28AgVs%29%2C%20referring%20to%20vehicles%20that%20integrate%20agentic%20AI%20to%20reason%2C%20adapt%2C%20and%0Ainteract%20within%20complex%20environments.%20This%20paper%20presents%20a%20systems-level%0Aframework%20to%20characterize%20AgVs%2C%20focusing%20on%20their%20cognitive%20and%20communicative%0Alayers%20and%20differentiating%20them%20from%20conventional%20AuVs.%20It%20synthesizes%20relevant%0Aadvances%20in%20agentic%20AI%2C%20robotics%2C%20multi-agent%20systems%2C%20and%20human-machine%0Ainteraction%2C%20and%20highlights%20how%20agentic%20AI%2C%20through%20high-level%20reasoning%20and%0Atool%20use%2C%20can%20function%20not%20merely%20as%20computational%20tools%20but%20as%20interactive%0Aagents%20embedded%20in%20mobility%20ecosystems.%20The%20paper%20concludes%20by%20identifying%20key%0Achallenges%20in%20the%20development%20and%20governance%20of%20AgVs%2C%20including%20safety%2C%0Areal-time%20control%2C%20public%20acceptance%2C%20ethical%20alignment%2C%20and%20regulatory%0Aframeworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04996v1&entry.124074799=Read"},
{"title": "Effects of Unplanned Incoming Flights on Airport Relief Processes after\n  a Major Natural Disaster", "author": "Luka Van de Sype and Matthieu Vert and Alexei Sharpanskykh and Seyed Sahand Mohammadi Ziabari", "abstract": "  The severity of natural disasters is increasing every year, impacting many\npeople's lives. During the response phase of disasters, airports are important\nhubs where relief aid arrives and people need to be evacuated. However, the\nairport often forms a bottleneck in these relief operations due to the sudden\nneed for increased capacity. Limited research has been done on the operational\nside of airport disaster management. Experts identify the main problems as,\nfirst, the asymmetry of information between the airport and incoming flights,\nand second, the lack of resources. The goal of this research is to understand\nthe effects of incomplete knowledge of incoming flights with different resource\nallocation strategies on the performance of cargo handling operations at an\nairport after a natural disaster. An agent-based model is created, implementing\nrealistic offloading strategies with different degrees of information\nuncertainty. Model calibration and verification are performed with experts in\nthe field. The model performance is measured by the average turnaround time,\nwhich is divided into offloading time, boarding time, and cumulative waiting\ntimes. The results show that the effects of one unplanned aircraft are\nnegligible. However, all waiting times increase with more arriving unplanned\naircraft.\n", "link": "http://arxiv.org/abs/2507.05150v1", "date": "2025-07-07", "relevancy": 1.0688, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3693}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.343}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Effects%20of%20Unplanned%20Incoming%20Flights%20on%20Airport%20Relief%20Processes%20after%0A%20%20a%20Major%20Natural%20Disaster&body=Title%3A%20Effects%20of%20Unplanned%20Incoming%20Flights%20on%20Airport%20Relief%20Processes%20after%0A%20%20a%20Major%20Natural%20Disaster%0AAuthor%3A%20Luka%20Van%20de%20Sype%20and%20Matthieu%20Vert%20and%20Alexei%20Sharpanskykh%20and%20Seyed%20Sahand%20Mohammadi%20Ziabari%0AAbstract%3A%20%20%20The%20severity%20of%20natural%20disasters%20is%20increasing%20every%20year%2C%20impacting%20many%0Apeople%27s%20lives.%20During%20the%20response%20phase%20of%20disasters%2C%20airports%20are%20important%0Ahubs%20where%20relief%20aid%20arrives%20and%20people%20need%20to%20be%20evacuated.%20However%2C%20the%0Aairport%20often%20forms%20a%20bottleneck%20in%20these%20relief%20operations%20due%20to%20the%20sudden%0Aneed%20for%20increased%20capacity.%20Limited%20research%20has%20been%20done%20on%20the%20operational%0Aside%20of%20airport%20disaster%20management.%20Experts%20identify%20the%20main%20problems%20as%2C%0Afirst%2C%20the%20asymmetry%20of%20information%20between%20the%20airport%20and%20incoming%20flights%2C%0Aand%20second%2C%20the%20lack%20of%20resources.%20The%20goal%20of%20this%20research%20is%20to%20understand%0Athe%20effects%20of%20incomplete%20knowledge%20of%20incoming%20flights%20with%20different%20resource%0Aallocation%20strategies%20on%20the%20performance%20of%20cargo%20handling%20operations%20at%20an%0Aairport%20after%20a%20natural%20disaster.%20An%20agent-based%20model%20is%20created%2C%20implementing%0Arealistic%20offloading%20strategies%20with%20different%20degrees%20of%20information%0Auncertainty.%20Model%20calibration%20and%20verification%20are%20performed%20with%20experts%20in%0Athe%20field.%20The%20model%20performance%20is%20measured%20by%20the%20average%20turnaround%20time%2C%0Awhich%20is%20divided%20into%20offloading%20time%2C%20boarding%20time%2C%20and%20cumulative%20waiting%0Atimes.%20The%20results%20show%20that%20the%20effects%20of%20one%20unplanned%20aircraft%20are%0Anegligible.%20However%2C%20all%20waiting%20times%20increase%20with%20more%20arriving%20unplanned%0Aaircraft.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffects%2520of%2520Unplanned%2520Incoming%2520Flights%2520on%2520Airport%2520Relief%2520Processes%2520after%250A%2520%2520a%2520Major%2520Natural%2520Disaster%26entry.906535625%3DLuka%2520Van%2520de%2520Sype%2520and%2520Matthieu%2520Vert%2520and%2520Alexei%2520Sharpanskykh%2520and%2520Seyed%2520Sahand%2520Mohammadi%2520Ziabari%26entry.1292438233%3D%2520%2520The%2520severity%2520of%2520natural%2520disasters%2520is%2520increasing%2520every%2520year%252C%2520impacting%2520many%250Apeople%2527s%2520lives.%2520During%2520the%2520response%2520phase%2520of%2520disasters%252C%2520airports%2520are%2520important%250Ahubs%2520where%2520relief%2520aid%2520arrives%2520and%2520people%2520need%2520to%2520be%2520evacuated.%2520However%252C%2520the%250Aairport%2520often%2520forms%2520a%2520bottleneck%2520in%2520these%2520relief%2520operations%2520due%2520to%2520the%2520sudden%250Aneed%2520for%2520increased%2520capacity.%2520Limited%2520research%2520has%2520been%2520done%2520on%2520the%2520operational%250Aside%2520of%2520airport%2520disaster%2520management.%2520Experts%2520identify%2520the%2520main%2520problems%2520as%252C%250Afirst%252C%2520the%2520asymmetry%2520of%2520information%2520between%2520the%2520airport%2520and%2520incoming%2520flights%252C%250Aand%2520second%252C%2520the%2520lack%2520of%2520resources.%2520The%2520goal%2520of%2520this%2520research%2520is%2520to%2520understand%250Athe%2520effects%2520of%2520incomplete%2520knowledge%2520of%2520incoming%2520flights%2520with%2520different%2520resource%250Aallocation%2520strategies%2520on%2520the%2520performance%2520of%2520cargo%2520handling%2520operations%2520at%2520an%250Aairport%2520after%2520a%2520natural%2520disaster.%2520An%2520agent-based%2520model%2520is%2520created%252C%2520implementing%250Arealistic%2520offloading%2520strategies%2520with%2520different%2520degrees%2520of%2520information%250Auncertainty.%2520Model%2520calibration%2520and%2520verification%2520are%2520performed%2520with%2520experts%2520in%250Athe%2520field.%2520The%2520model%2520performance%2520is%2520measured%2520by%2520the%2520average%2520turnaround%2520time%252C%250Awhich%2520is%2520divided%2520into%2520offloading%2520time%252C%2520boarding%2520time%252C%2520and%2520cumulative%2520waiting%250Atimes.%2520The%2520results%2520show%2520that%2520the%2520effects%2520of%2520one%2520unplanned%2520aircraft%2520are%250Anegligible.%2520However%252C%2520all%2520waiting%2520times%2520increase%2520with%2520more%2520arriving%2520unplanned%250Aaircraft.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effects%20of%20Unplanned%20Incoming%20Flights%20on%20Airport%20Relief%20Processes%20after%0A%20%20a%20Major%20Natural%20Disaster&entry.906535625=Luka%20Van%20de%20Sype%20and%20Matthieu%20Vert%20and%20Alexei%20Sharpanskykh%20and%20Seyed%20Sahand%20Mohammadi%20Ziabari&entry.1292438233=%20%20The%20severity%20of%20natural%20disasters%20is%20increasing%20every%20year%2C%20impacting%20many%0Apeople%27s%20lives.%20During%20the%20response%20phase%20of%20disasters%2C%20airports%20are%20important%0Ahubs%20where%20relief%20aid%20arrives%20and%20people%20need%20to%20be%20evacuated.%20However%2C%20the%0Aairport%20often%20forms%20a%20bottleneck%20in%20these%20relief%20operations%20due%20to%20the%20sudden%0Aneed%20for%20increased%20capacity.%20Limited%20research%20has%20been%20done%20on%20the%20operational%0Aside%20of%20airport%20disaster%20management.%20Experts%20identify%20the%20main%20problems%20as%2C%0Afirst%2C%20the%20asymmetry%20of%20information%20between%20the%20airport%20and%20incoming%20flights%2C%0Aand%20second%2C%20the%20lack%20of%20resources.%20The%20goal%20of%20this%20research%20is%20to%20understand%0Athe%20effects%20of%20incomplete%20knowledge%20of%20incoming%20flights%20with%20different%20resource%0Aallocation%20strategies%20on%20the%20performance%20of%20cargo%20handling%20operations%20at%20an%0Aairport%20after%20a%20natural%20disaster.%20An%20agent-based%20model%20is%20created%2C%20implementing%0Arealistic%20offloading%20strategies%20with%20different%20degrees%20of%20information%0Auncertainty.%20Model%20calibration%20and%20verification%20are%20performed%20with%20experts%20in%0Athe%20field.%20The%20model%20performance%20is%20measured%20by%20the%20average%20turnaround%20time%2C%0Awhich%20is%20divided%20into%20offloading%20time%2C%20boarding%20time%2C%20and%20cumulative%20waiting%0Atimes.%20The%20results%20show%20that%20the%20effects%20of%20one%20unplanned%20aircraft%20are%0Anegligible.%20However%2C%20all%20waiting%20times%20increase%20with%20more%20arriving%20unplanned%0Aaircraft.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05150v1&entry.124074799=Read"},
{"title": "AI for the Routine, Humans for the Complex: Accuracy-Driven Data\n  Labelling with Mixed Integer Linear Programming", "author": "Mohammad Hossein Amini and Mehrdad Sabetzadeh and Shiva Nejati", "abstract": "  The scarcity of accurately labelled data remains a major challenge in deep\nlearning (DL). Many DL approaches rely on semi-supervised methods, which focus\non constructing large datasets that require only a minimal amount of\nhuman-labelled data. Since DL training algorithms can tolerate moderate label\nnoise, it has generally been acceptable for the accuracy of labels in large\ntraining datasets to fall well short of a perfect 100%. However, when it comes\nto testing DL models, achieving high label accuracy-as close to 100% as\npossible-is paramount for reliable verification. In this article, we introduce\nOPAL, a human-assisted labelling method that can be configured to target a\ndesired accuracy level while minimizing the manual effort required for\nlabelling. The main contribution of OPAL is a mixed-integer linear programming\n(MILP) formulation that minimizes labelling effort subject to a specified\naccuracy target. We evaluate OPAL for two tasks in the context of testing\nvision systems: automatic labelling of test data and automated validation of\ntest data. Our evaluation, based on more than 2500 experiments performed on\nseven datasets, comparing OPAL with eight baseline methods, shows that OPAL,\nrelying on its MILP formulation, achieves an average accuracy of 98.8%, just\n1.2% below perfect accuracy, while cutting manual labelling by more than half.\nFurther, OPAL significantly outperforms automated labelling baselines in\nlabelling accuracy across all seven datasets, with large effect sizes, when all\nmethods are provided with the same manual-labelling budget. For automated\ntest-input validation, on average, OPAL reduces manual effort by 28.8% while\nachieving 4.5% higher accuracy than the SOTA validation baselines. Finally, we\nshow that augmenting OPAL with an active learning loop leads to an additional\n4.5% reduction in required manual labelling, without compromising accuracy.\n", "link": "http://arxiv.org/abs/2507.04990v1", "date": "2025-07-07", "relevancy": 1.6192, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5772}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5317}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20for%20the%20Routine%2C%20Humans%20for%20the%20Complex%3A%20Accuracy-Driven%20Data%0A%20%20Labelling%20with%20Mixed%20Integer%20Linear%20Programming&body=Title%3A%20AI%20for%20the%20Routine%2C%20Humans%20for%20the%20Complex%3A%20Accuracy-Driven%20Data%0A%20%20Labelling%20with%20Mixed%20Integer%20Linear%20Programming%0AAuthor%3A%20Mohammad%20Hossein%20Amini%20and%20Mehrdad%20Sabetzadeh%20and%20Shiva%20Nejati%0AAbstract%3A%20%20%20The%20scarcity%20of%20accurately%20labelled%20data%20remains%20a%20major%20challenge%20in%20deep%0Alearning%20%28DL%29.%20Many%20DL%20approaches%20rely%20on%20semi-supervised%20methods%2C%20which%20focus%0Aon%20constructing%20large%20datasets%20that%20require%20only%20a%20minimal%20amount%20of%0Ahuman-labelled%20data.%20Since%20DL%20training%20algorithms%20can%20tolerate%20moderate%20label%0Anoise%2C%20it%20has%20generally%20been%20acceptable%20for%20the%20accuracy%20of%20labels%20in%20large%0Atraining%20datasets%20to%20fall%20well%20short%20of%20a%20perfect%20100%25.%20However%2C%20when%20it%20comes%0Ato%20testing%20DL%20models%2C%20achieving%20high%20label%20accuracy-as%20close%20to%20100%25%20as%0Apossible-is%20paramount%20for%20reliable%20verification.%20In%20this%20article%2C%20we%20introduce%0AOPAL%2C%20a%20human-assisted%20labelling%20method%20that%20can%20be%20configured%20to%20target%20a%0Adesired%20accuracy%20level%20while%20minimizing%20the%20manual%20effort%20required%20for%0Alabelling.%20The%20main%20contribution%20of%20OPAL%20is%20a%20mixed-integer%20linear%20programming%0A%28MILP%29%20formulation%20that%20minimizes%20labelling%20effort%20subject%20to%20a%20specified%0Aaccuracy%20target.%20We%20evaluate%20OPAL%20for%20two%20tasks%20in%20the%20context%20of%20testing%0Avision%20systems%3A%20automatic%20labelling%20of%20test%20data%20and%20automated%20validation%20of%0Atest%20data.%20Our%20evaluation%2C%20based%20on%20more%20than%202500%20experiments%20performed%20on%0Aseven%20datasets%2C%20comparing%20OPAL%20with%20eight%20baseline%20methods%2C%20shows%20that%20OPAL%2C%0Arelying%20on%20its%20MILP%20formulation%2C%20achieves%20an%20average%20accuracy%20of%2098.8%25%2C%20just%0A1.2%25%20below%20perfect%20accuracy%2C%20while%20cutting%20manual%20labelling%20by%20more%20than%20half.%0AFurther%2C%20OPAL%20significantly%20outperforms%20automated%20labelling%20baselines%20in%0Alabelling%20accuracy%20across%20all%20seven%20datasets%2C%20with%20large%20effect%20sizes%2C%20when%20all%0Amethods%20are%20provided%20with%20the%20same%20manual-labelling%20budget.%20For%20automated%0Atest-input%20validation%2C%20on%20average%2C%20OPAL%20reduces%20manual%20effort%20by%2028.8%25%20while%0Aachieving%204.5%25%20higher%20accuracy%20than%20the%20SOTA%20validation%20baselines.%20Finally%2C%20we%0Ashow%20that%20augmenting%20OPAL%20with%20an%20active%20learning%20loop%20leads%20to%20an%20additional%0A4.5%25%20reduction%20in%20required%20manual%20labelling%2C%20without%20compromising%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520for%2520the%2520Routine%252C%2520Humans%2520for%2520the%2520Complex%253A%2520Accuracy-Driven%2520Data%250A%2520%2520Labelling%2520with%2520Mixed%2520Integer%2520Linear%2520Programming%26entry.906535625%3DMohammad%2520Hossein%2520Amini%2520and%2520Mehrdad%2520Sabetzadeh%2520and%2520Shiva%2520Nejati%26entry.1292438233%3D%2520%2520The%2520scarcity%2520of%2520accurately%2520labelled%2520data%2520remains%2520a%2520major%2520challenge%2520in%2520deep%250Alearning%2520%2528DL%2529.%2520Many%2520DL%2520approaches%2520rely%2520on%2520semi-supervised%2520methods%252C%2520which%2520focus%250Aon%2520constructing%2520large%2520datasets%2520that%2520require%2520only%2520a%2520minimal%2520amount%2520of%250Ahuman-labelled%2520data.%2520Since%2520DL%2520training%2520algorithms%2520can%2520tolerate%2520moderate%2520label%250Anoise%252C%2520it%2520has%2520generally%2520been%2520acceptable%2520for%2520the%2520accuracy%2520of%2520labels%2520in%2520large%250Atraining%2520datasets%2520to%2520fall%2520well%2520short%2520of%2520a%2520perfect%2520100%2525.%2520However%252C%2520when%2520it%2520comes%250Ato%2520testing%2520DL%2520models%252C%2520achieving%2520high%2520label%2520accuracy-as%2520close%2520to%2520100%2525%2520as%250Apossible-is%2520paramount%2520for%2520reliable%2520verification.%2520In%2520this%2520article%252C%2520we%2520introduce%250AOPAL%252C%2520a%2520human-assisted%2520labelling%2520method%2520that%2520can%2520be%2520configured%2520to%2520target%2520a%250Adesired%2520accuracy%2520level%2520while%2520minimizing%2520the%2520manual%2520effort%2520required%2520for%250Alabelling.%2520The%2520main%2520contribution%2520of%2520OPAL%2520is%2520a%2520mixed-integer%2520linear%2520programming%250A%2528MILP%2529%2520formulation%2520that%2520minimizes%2520labelling%2520effort%2520subject%2520to%2520a%2520specified%250Aaccuracy%2520target.%2520We%2520evaluate%2520OPAL%2520for%2520two%2520tasks%2520in%2520the%2520context%2520of%2520testing%250Avision%2520systems%253A%2520automatic%2520labelling%2520of%2520test%2520data%2520and%2520automated%2520validation%2520of%250Atest%2520data.%2520Our%2520evaluation%252C%2520based%2520on%2520more%2520than%25202500%2520experiments%2520performed%2520on%250Aseven%2520datasets%252C%2520comparing%2520OPAL%2520with%2520eight%2520baseline%2520methods%252C%2520shows%2520that%2520OPAL%252C%250Arelying%2520on%2520its%2520MILP%2520formulation%252C%2520achieves%2520an%2520average%2520accuracy%2520of%252098.8%2525%252C%2520just%250A1.2%2525%2520below%2520perfect%2520accuracy%252C%2520while%2520cutting%2520manual%2520labelling%2520by%2520more%2520than%2520half.%250AFurther%252C%2520OPAL%2520significantly%2520outperforms%2520automated%2520labelling%2520baselines%2520in%250Alabelling%2520accuracy%2520across%2520all%2520seven%2520datasets%252C%2520with%2520large%2520effect%2520sizes%252C%2520when%2520all%250Amethods%2520are%2520provided%2520with%2520the%2520same%2520manual-labelling%2520budget.%2520For%2520automated%250Atest-input%2520validation%252C%2520on%2520average%252C%2520OPAL%2520reduces%2520manual%2520effort%2520by%252028.8%2525%2520while%250Aachieving%25204.5%2525%2520higher%2520accuracy%2520than%2520the%2520SOTA%2520validation%2520baselines.%2520Finally%252C%2520we%250Ashow%2520that%2520augmenting%2520OPAL%2520with%2520an%2520active%2520learning%2520loop%2520leads%2520to%2520an%2520additional%250A4.5%2525%2520reduction%2520in%2520required%2520manual%2520labelling%252C%2520without%2520compromising%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20for%20the%20Routine%2C%20Humans%20for%20the%20Complex%3A%20Accuracy-Driven%20Data%0A%20%20Labelling%20with%20Mixed%20Integer%20Linear%20Programming&entry.906535625=Mohammad%20Hossein%20Amini%20and%20Mehrdad%20Sabetzadeh%20and%20Shiva%20Nejati&entry.1292438233=%20%20The%20scarcity%20of%20accurately%20labelled%20data%20remains%20a%20major%20challenge%20in%20deep%0Alearning%20%28DL%29.%20Many%20DL%20approaches%20rely%20on%20semi-supervised%20methods%2C%20which%20focus%0Aon%20constructing%20large%20datasets%20that%20require%20only%20a%20minimal%20amount%20of%0Ahuman-labelled%20data.%20Since%20DL%20training%20algorithms%20can%20tolerate%20moderate%20label%0Anoise%2C%20it%20has%20generally%20been%20acceptable%20for%20the%20accuracy%20of%20labels%20in%20large%0Atraining%20datasets%20to%20fall%20well%20short%20of%20a%20perfect%20100%25.%20However%2C%20when%20it%20comes%0Ato%20testing%20DL%20models%2C%20achieving%20high%20label%20accuracy-as%20close%20to%20100%25%20as%0Apossible-is%20paramount%20for%20reliable%20verification.%20In%20this%20article%2C%20we%20introduce%0AOPAL%2C%20a%20human-assisted%20labelling%20method%20that%20can%20be%20configured%20to%20target%20a%0Adesired%20accuracy%20level%20while%20minimizing%20the%20manual%20effort%20required%20for%0Alabelling.%20The%20main%20contribution%20of%20OPAL%20is%20a%20mixed-integer%20linear%20programming%0A%28MILP%29%20formulation%20that%20minimizes%20labelling%20effort%20subject%20to%20a%20specified%0Aaccuracy%20target.%20We%20evaluate%20OPAL%20for%20two%20tasks%20in%20the%20context%20of%20testing%0Avision%20systems%3A%20automatic%20labelling%20of%20test%20data%20and%20automated%20validation%20of%0Atest%20data.%20Our%20evaluation%2C%20based%20on%20more%20than%202500%20experiments%20performed%20on%0Aseven%20datasets%2C%20comparing%20OPAL%20with%20eight%20baseline%20methods%2C%20shows%20that%20OPAL%2C%0Arelying%20on%20its%20MILP%20formulation%2C%20achieves%20an%20average%20accuracy%20of%2098.8%25%2C%20just%0A1.2%25%20below%20perfect%20accuracy%2C%20while%20cutting%20manual%20labelling%20by%20more%20than%20half.%0AFurther%2C%20OPAL%20significantly%20outperforms%20automated%20labelling%20baselines%20in%0Alabelling%20accuracy%20across%20all%20seven%20datasets%2C%20with%20large%20effect%20sizes%2C%20when%20all%0Amethods%20are%20provided%20with%20the%20same%20manual-labelling%20budget.%20For%20automated%0Atest-input%20validation%2C%20on%20average%2C%20OPAL%20reduces%20manual%20effort%20by%2028.8%25%20while%0Aachieving%204.5%25%20higher%20accuracy%20than%20the%20SOTA%20validation%20baselines.%20Finally%2C%20we%0Ashow%20that%20augmenting%20OPAL%20with%20an%20active%20learning%20loop%20leads%20to%20an%20additional%0A4.5%25%20reduction%20in%20required%20manual%20labelling%2C%20without%20compromising%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04990v1&entry.124074799=Read"},
{"title": "Fairness and Sparsity within Rashomon sets: Enumeration-Free Exploration\n  and Characterization", "author": "Lucas Langlade and Julien Ferry and Gabriel Laberge and Thibaut Vidal", "abstract": "  We introduce an enumeration-free method based on mathematical programming to\nprecisely characterize various properties such as fairness or sparsity within\nthe set of \"good models\", known as Rashomon set. This approach is generically\napplicable to any hypothesis class, provided that a mathematical formulation of\nthe model learning task exists. It offers a structured framework to define the\nnotion of business necessity and evaluate how fairness can be improved or\ndegraded towards a specific protected group, while remaining within the\nRashomon set and maintaining any desired sparsity level.\n  We apply our approach to two hypothesis classes: scoring systems and decision\ndiagrams, leveraging recent mathematical programming formulations for training\nsuch models. As seen in our experiments, the method comprehensively and\ncertifiably quantifies trade-offs between predictive performance, sparsity, and\nfairness. We observe that a wide range of fairness values are attainable,\nranging from highly favorable to significantly unfavorable for a protected\ngroup, while staying within less than 1% of the best possible training accuracy\nfor the hypothesis class. Additionally, we observe that sparsity constraints\nlimit these trade-offs and may disproportionately harm specific subgroups. As\nwe evidenced, thoroughly characterizing the tensions between these key aspects\nis critical for an informed and accountable selection of models.\n", "link": "http://arxiv.org/abs/2502.05286v2", "date": "2025-07-07", "relevancy": 1.4215, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4992}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.476}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fairness%20and%20Sparsity%20within%20Rashomon%20sets%3A%20Enumeration-Free%20Exploration%0A%20%20and%20Characterization&body=Title%3A%20Fairness%20and%20Sparsity%20within%20Rashomon%20sets%3A%20Enumeration-Free%20Exploration%0A%20%20and%20Characterization%0AAuthor%3A%20Lucas%20Langlade%20and%20Julien%20Ferry%20and%20Gabriel%20Laberge%20and%20Thibaut%20Vidal%0AAbstract%3A%20%20%20We%20introduce%20an%20enumeration-free%20method%20based%20on%20mathematical%20programming%20to%0Aprecisely%20characterize%20various%20properties%20such%20as%20fairness%20or%20sparsity%20within%0Athe%20set%20of%20%22good%20models%22%2C%20known%20as%20Rashomon%20set.%20This%20approach%20is%20generically%0Aapplicable%20to%20any%20hypothesis%20class%2C%20provided%20that%20a%20mathematical%20formulation%20of%0Athe%20model%20learning%20task%20exists.%20It%20offers%20a%20structured%20framework%20to%20define%20the%0Anotion%20of%20business%20necessity%20and%20evaluate%20how%20fairness%20can%20be%20improved%20or%0Adegraded%20towards%20a%20specific%20protected%20group%2C%20while%20remaining%20within%20the%0ARashomon%20set%20and%20maintaining%20any%20desired%20sparsity%20level.%0A%20%20We%20apply%20our%20approach%20to%20two%20hypothesis%20classes%3A%20scoring%20systems%20and%20decision%0Adiagrams%2C%20leveraging%20recent%20mathematical%20programming%20formulations%20for%20training%0Asuch%20models.%20As%20seen%20in%20our%20experiments%2C%20the%20method%20comprehensively%20and%0Acertifiably%20quantifies%20trade-offs%20between%20predictive%20performance%2C%20sparsity%2C%20and%0Afairness.%20We%20observe%20that%20a%20wide%20range%20of%20fairness%20values%20are%20attainable%2C%0Aranging%20from%20highly%20favorable%20to%20significantly%20unfavorable%20for%20a%20protected%0Agroup%2C%20while%20staying%20within%20less%20than%201%25%20of%20the%20best%20possible%20training%20accuracy%0Afor%20the%20hypothesis%20class.%20Additionally%2C%20we%20observe%20that%20sparsity%20constraints%0Alimit%20these%20trade-offs%20and%20may%20disproportionately%20harm%20specific%20subgroups.%20As%0Awe%20evidenced%2C%20thoroughly%20characterizing%20the%20tensions%20between%20these%20key%20aspects%0Ais%20critical%20for%20an%20informed%20and%20accountable%20selection%20of%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05286v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairness%2520and%2520Sparsity%2520within%2520Rashomon%2520sets%253A%2520Enumeration-Free%2520Exploration%250A%2520%2520and%2520Characterization%26entry.906535625%3DLucas%2520Langlade%2520and%2520Julien%2520Ferry%2520and%2520Gabriel%2520Laberge%2520and%2520Thibaut%2520Vidal%26entry.1292438233%3D%2520%2520We%2520introduce%2520an%2520enumeration-free%2520method%2520based%2520on%2520mathematical%2520programming%2520to%250Aprecisely%2520characterize%2520various%2520properties%2520such%2520as%2520fairness%2520or%2520sparsity%2520within%250Athe%2520set%2520of%2520%2522good%2520models%2522%252C%2520known%2520as%2520Rashomon%2520set.%2520This%2520approach%2520is%2520generically%250Aapplicable%2520to%2520any%2520hypothesis%2520class%252C%2520provided%2520that%2520a%2520mathematical%2520formulation%2520of%250Athe%2520model%2520learning%2520task%2520exists.%2520It%2520offers%2520a%2520structured%2520framework%2520to%2520define%2520the%250Anotion%2520of%2520business%2520necessity%2520and%2520evaluate%2520how%2520fairness%2520can%2520be%2520improved%2520or%250Adegraded%2520towards%2520a%2520specific%2520protected%2520group%252C%2520while%2520remaining%2520within%2520the%250ARashomon%2520set%2520and%2520maintaining%2520any%2520desired%2520sparsity%2520level.%250A%2520%2520We%2520apply%2520our%2520approach%2520to%2520two%2520hypothesis%2520classes%253A%2520scoring%2520systems%2520and%2520decision%250Adiagrams%252C%2520leveraging%2520recent%2520mathematical%2520programming%2520formulations%2520for%2520training%250Asuch%2520models.%2520As%2520seen%2520in%2520our%2520experiments%252C%2520the%2520method%2520comprehensively%2520and%250Acertifiably%2520quantifies%2520trade-offs%2520between%2520predictive%2520performance%252C%2520sparsity%252C%2520and%250Afairness.%2520We%2520observe%2520that%2520a%2520wide%2520range%2520of%2520fairness%2520values%2520are%2520attainable%252C%250Aranging%2520from%2520highly%2520favorable%2520to%2520significantly%2520unfavorable%2520for%2520a%2520protected%250Agroup%252C%2520while%2520staying%2520within%2520less%2520than%25201%2525%2520of%2520the%2520best%2520possible%2520training%2520accuracy%250Afor%2520the%2520hypothesis%2520class.%2520Additionally%252C%2520we%2520observe%2520that%2520sparsity%2520constraints%250Alimit%2520these%2520trade-offs%2520and%2520may%2520disproportionately%2520harm%2520specific%2520subgroups.%2520As%250Awe%2520evidenced%252C%2520thoroughly%2520characterizing%2520the%2520tensions%2520between%2520these%2520key%2520aspects%250Ais%2520critical%2520for%2520an%2520informed%2520and%2520accountable%2520selection%2520of%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05286v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fairness%20and%20Sparsity%20within%20Rashomon%20sets%3A%20Enumeration-Free%20Exploration%0A%20%20and%20Characterization&entry.906535625=Lucas%20Langlade%20and%20Julien%20Ferry%20and%20Gabriel%20Laberge%20and%20Thibaut%20Vidal&entry.1292438233=%20%20We%20introduce%20an%20enumeration-free%20method%20based%20on%20mathematical%20programming%20to%0Aprecisely%20characterize%20various%20properties%20such%20as%20fairness%20or%20sparsity%20within%0Athe%20set%20of%20%22good%20models%22%2C%20known%20as%20Rashomon%20set.%20This%20approach%20is%20generically%0Aapplicable%20to%20any%20hypothesis%20class%2C%20provided%20that%20a%20mathematical%20formulation%20of%0Athe%20model%20learning%20task%20exists.%20It%20offers%20a%20structured%20framework%20to%20define%20the%0Anotion%20of%20business%20necessity%20and%20evaluate%20how%20fairness%20can%20be%20improved%20or%0Adegraded%20towards%20a%20specific%20protected%20group%2C%20while%20remaining%20within%20the%0ARashomon%20set%20and%20maintaining%20any%20desired%20sparsity%20level.%0A%20%20We%20apply%20our%20approach%20to%20two%20hypothesis%20classes%3A%20scoring%20systems%20and%20decision%0Adiagrams%2C%20leveraging%20recent%20mathematical%20programming%20formulations%20for%20training%0Asuch%20models.%20As%20seen%20in%20our%20experiments%2C%20the%20method%20comprehensively%20and%0Acertifiably%20quantifies%20trade-offs%20between%20predictive%20performance%2C%20sparsity%2C%20and%0Afairness.%20We%20observe%20that%20a%20wide%20range%20of%20fairness%20values%20are%20attainable%2C%0Aranging%20from%20highly%20favorable%20to%20significantly%20unfavorable%20for%20a%20protected%0Agroup%2C%20while%20staying%20within%20less%20than%201%25%20of%20the%20best%20possible%20training%20accuracy%0Afor%20the%20hypothesis%20class.%20Additionally%2C%20we%20observe%20that%20sparsity%20constraints%0Alimit%20these%20trade-offs%20and%20may%20disproportionately%20harm%20specific%20subgroups.%20As%0Awe%20evidenced%2C%20thoroughly%20characterizing%20the%20tensions%20between%20these%20key%20aspects%0Ais%20critical%20for%20an%20informed%20and%20accountable%20selection%20of%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05286v2&entry.124074799=Read"},
{"title": "Active Stereo in the Wild through Virtual Pattern Projection", "author": "Luca Bartolomei and Matteo Poggi and Fabio Tosi and Andrea Conti and Stefano Mattoccia", "abstract": "  This paper presents a novel general-purpose guided stereo paradigm that\nmimics the active stereo principle by replacing the unreliable physical pattern\nprojector with a depth sensor. It works by projecting virtual patterns\nconsistent with the scene geometry onto the left and right images acquired by a\nconventional stereo camera, using the sparse hints obtained from a depth\nsensor, to facilitate the visual correspondence. Purposely, any depth sensing\ndevice can be seamlessly plugged into our framework, enabling the deployment of\na virtual active stereo setup in any possible environment and overcoming the\nsevere limitations of physical pattern projection, such as the limited working\nrange and environmental conditions. Exhaustive experiments on indoor and\noutdoor datasets featuring both long and close range, including those providing\nraw, unfiltered depth hints from off-the-shelf depth sensors, highlight the\neffectiveness of our approach in notably boosting the robustness and accuracy\nof algorithms and deep stereo without any code modification and even without\nre-training. Additionally, we assess the performance of our strategy on active\nstereo evaluation datasets with conventional pattern projection. Indeed, in all\nthese scenarios, our virtual pattern projection paradigm achieves\nstate-of-the-art performance. The source code is available at:\nhttps://github.com/bartn8/vppstereo.\n", "link": "http://arxiv.org/abs/2406.04345v2", "date": "2025-07-07", "relevancy": 1.6442, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5548}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5523}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Stereo%20in%20the%20Wild%20through%20Virtual%20Pattern%20Projection&body=Title%3A%20Active%20Stereo%20in%20the%20Wild%20through%20Virtual%20Pattern%20Projection%0AAuthor%3A%20Luca%20Bartolomei%20and%20Matteo%20Poggi%20and%20Fabio%20Tosi%20and%20Andrea%20Conti%20and%20Stefano%20Mattoccia%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20general-purpose%20guided%20stereo%20paradigm%20that%0Amimics%20the%20active%20stereo%20principle%20by%20replacing%20the%20unreliable%20physical%20pattern%0Aprojector%20with%20a%20depth%20sensor.%20It%20works%20by%20projecting%20virtual%20patterns%0Aconsistent%20with%20the%20scene%20geometry%20onto%20the%20left%20and%20right%20images%20acquired%20by%20a%0Aconventional%20stereo%20camera%2C%20using%20the%20sparse%20hints%20obtained%20from%20a%20depth%0Asensor%2C%20to%20facilitate%20the%20visual%20correspondence.%20Purposely%2C%20any%20depth%20sensing%0Adevice%20can%20be%20seamlessly%20plugged%20into%20our%20framework%2C%20enabling%20the%20deployment%20of%0Aa%20virtual%20active%20stereo%20setup%20in%20any%20possible%20environment%20and%20overcoming%20the%0Asevere%20limitations%20of%20physical%20pattern%20projection%2C%20such%20as%20the%20limited%20working%0Arange%20and%20environmental%20conditions.%20Exhaustive%20experiments%20on%20indoor%20and%0Aoutdoor%20datasets%20featuring%20both%20long%20and%20close%20range%2C%20including%20those%20providing%0Araw%2C%20unfiltered%20depth%20hints%20from%20off-the-shelf%20depth%20sensors%2C%20highlight%20the%0Aeffectiveness%20of%20our%20approach%20in%20notably%20boosting%20the%20robustness%20and%20accuracy%0Aof%20algorithms%20and%20deep%20stereo%20without%20any%20code%20modification%20and%20even%20without%0Are-training.%20Additionally%2C%20we%20assess%20the%20performance%20of%20our%20strategy%20on%20active%0Astereo%20evaluation%20datasets%20with%20conventional%20pattern%20projection.%20Indeed%2C%20in%20all%0Athese%20scenarios%2C%20our%20virtual%20pattern%20projection%20paradigm%20achieves%0Astate-of-the-art%20performance.%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/bartn8/vppstereo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04345v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Stereo%2520in%2520the%2520Wild%2520through%2520Virtual%2520Pattern%2520Projection%26entry.906535625%3DLuca%2520Bartolomei%2520and%2520Matteo%2520Poggi%2520and%2520Fabio%2520Tosi%2520and%2520Andrea%2520Conti%2520and%2520Stefano%2520Mattoccia%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520general-purpose%2520guided%2520stereo%2520paradigm%2520that%250Amimics%2520the%2520active%2520stereo%2520principle%2520by%2520replacing%2520the%2520unreliable%2520physical%2520pattern%250Aprojector%2520with%2520a%2520depth%2520sensor.%2520It%2520works%2520by%2520projecting%2520virtual%2520patterns%250Aconsistent%2520with%2520the%2520scene%2520geometry%2520onto%2520the%2520left%2520and%2520right%2520images%2520acquired%2520by%2520a%250Aconventional%2520stereo%2520camera%252C%2520using%2520the%2520sparse%2520hints%2520obtained%2520from%2520a%2520depth%250Asensor%252C%2520to%2520facilitate%2520the%2520visual%2520correspondence.%2520Purposely%252C%2520any%2520depth%2520sensing%250Adevice%2520can%2520be%2520seamlessly%2520plugged%2520into%2520our%2520framework%252C%2520enabling%2520the%2520deployment%2520of%250Aa%2520virtual%2520active%2520stereo%2520setup%2520in%2520any%2520possible%2520environment%2520and%2520overcoming%2520the%250Asevere%2520limitations%2520of%2520physical%2520pattern%2520projection%252C%2520such%2520as%2520the%2520limited%2520working%250Arange%2520and%2520environmental%2520conditions.%2520Exhaustive%2520experiments%2520on%2520indoor%2520and%250Aoutdoor%2520datasets%2520featuring%2520both%2520long%2520and%2520close%2520range%252C%2520including%2520those%2520providing%250Araw%252C%2520unfiltered%2520depth%2520hints%2520from%2520off-the-shelf%2520depth%2520sensors%252C%2520highlight%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520in%2520notably%2520boosting%2520the%2520robustness%2520and%2520accuracy%250Aof%2520algorithms%2520and%2520deep%2520stereo%2520without%2520any%2520code%2520modification%2520and%2520even%2520without%250Are-training.%2520Additionally%252C%2520we%2520assess%2520the%2520performance%2520of%2520our%2520strategy%2520on%2520active%250Astereo%2520evaluation%2520datasets%2520with%2520conventional%2520pattern%2520projection.%2520Indeed%252C%2520in%2520all%250Athese%2520scenarios%252C%2520our%2520virtual%2520pattern%2520projection%2520paradigm%2520achieves%250Astate-of-the-art%2520performance.%2520The%2520source%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/bartn8/vppstereo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04345v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Stereo%20in%20the%20Wild%20through%20Virtual%20Pattern%20Projection&entry.906535625=Luca%20Bartolomei%20and%20Matteo%20Poggi%20and%20Fabio%20Tosi%20and%20Andrea%20Conti%20and%20Stefano%20Mattoccia&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20general-purpose%20guided%20stereo%20paradigm%20that%0Amimics%20the%20active%20stereo%20principle%20by%20replacing%20the%20unreliable%20physical%20pattern%0Aprojector%20with%20a%20depth%20sensor.%20It%20works%20by%20projecting%20virtual%20patterns%0Aconsistent%20with%20the%20scene%20geometry%20onto%20the%20left%20and%20right%20images%20acquired%20by%20a%0Aconventional%20stereo%20camera%2C%20using%20the%20sparse%20hints%20obtained%20from%20a%20depth%0Asensor%2C%20to%20facilitate%20the%20visual%20correspondence.%20Purposely%2C%20any%20depth%20sensing%0Adevice%20can%20be%20seamlessly%20plugged%20into%20our%20framework%2C%20enabling%20the%20deployment%20of%0Aa%20virtual%20active%20stereo%20setup%20in%20any%20possible%20environment%20and%20overcoming%20the%0Asevere%20limitations%20of%20physical%20pattern%20projection%2C%20such%20as%20the%20limited%20working%0Arange%20and%20environmental%20conditions.%20Exhaustive%20experiments%20on%20indoor%20and%0Aoutdoor%20datasets%20featuring%20both%20long%20and%20close%20range%2C%20including%20those%20providing%0Araw%2C%20unfiltered%20depth%20hints%20from%20off-the-shelf%20depth%20sensors%2C%20highlight%20the%0Aeffectiveness%20of%20our%20approach%20in%20notably%20boosting%20the%20robustness%20and%20accuracy%0Aof%20algorithms%20and%20deep%20stereo%20without%20any%20code%20modification%20and%20even%20without%0Are-training.%20Additionally%2C%20we%20assess%20the%20performance%20of%20our%20strategy%20on%20active%0Astereo%20evaluation%20datasets%20with%20conventional%20pattern%20projection.%20Indeed%2C%20in%20all%0Athese%20scenarios%2C%20our%20virtual%20pattern%20projection%20paradigm%20achieves%0Astate-of-the-art%20performance.%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/bartn8/vppstereo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04345v2&entry.124074799=Read"},
{"title": "A Framework for Synthetic Audio Conversations Generation using Large\n  Language Models", "author": "Kaung Myat Kyaw and Jonathan Hoyin Chan", "abstract": "  In this paper, we introduce ConversaSynth, a framework designed to generate\nsynthetic conversation audio using large language models (LLMs) with multiple\npersona settings. The framework first creates diverse and coherent text-based\ndialogues across various topics, which are then converted into audio using\ntext-to-speech (TTS) systems. Our experiments demonstrate that ConversaSynth\neffectively generates highquality synthetic audio datasets, which can\nsignificantly enhance the training and evaluation of models for audio tagging,\naudio classification, and multi-speaker speech recognition. The results\nindicate that the synthetic datasets generated by ConversaSynth exhibit\nsubstantial diversity and realism, making them suitable for developing robust,\nadaptable audio-based AI systems.\n", "link": "http://arxiv.org/abs/2409.00946v3", "date": "2025-07-07", "relevancy": 1.9814, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5091}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4956}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Framework%20for%20Synthetic%20Audio%20Conversations%20Generation%20using%20Large%0A%20%20Language%20Models&body=Title%3A%20A%20Framework%20for%20Synthetic%20Audio%20Conversations%20Generation%20using%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Kaung%20Myat%20Kyaw%20and%20Jonathan%20Hoyin%20Chan%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20ConversaSynth%2C%20a%20framework%20designed%20to%20generate%0Asynthetic%20conversation%20audio%20using%20large%20language%20models%20%28LLMs%29%20with%20multiple%0Apersona%20settings.%20The%20framework%20first%20creates%20diverse%20and%20coherent%20text-based%0Adialogues%20across%20various%20topics%2C%20which%20are%20then%20converted%20into%20audio%20using%0Atext-to-speech%20%28TTS%29%20systems.%20Our%20experiments%20demonstrate%20that%20ConversaSynth%0Aeffectively%20generates%20highquality%20synthetic%20audio%20datasets%2C%20which%20can%0Asignificantly%20enhance%20the%20training%20and%20evaluation%20of%20models%20for%20audio%20tagging%2C%0Aaudio%20classification%2C%20and%20multi-speaker%20speech%20recognition.%20The%20results%0Aindicate%20that%20the%20synthetic%20datasets%20generated%20by%20ConversaSynth%20exhibit%0Asubstantial%20diversity%20and%20realism%2C%20making%20them%20suitable%20for%20developing%20robust%2C%0Aadaptable%20audio-based%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00946v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Framework%2520for%2520Synthetic%2520Audio%2520Conversations%2520Generation%2520using%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DKaung%2520Myat%2520Kyaw%2520and%2520Jonathan%2520Hoyin%2520Chan%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520ConversaSynth%252C%2520a%2520framework%2520designed%2520to%2520generate%250Asynthetic%2520conversation%2520audio%2520using%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520multiple%250Apersona%2520settings.%2520The%2520framework%2520first%2520creates%2520diverse%2520and%2520coherent%2520text-based%250Adialogues%2520across%2520various%2520topics%252C%2520which%2520are%2520then%2520converted%2520into%2520audio%2520using%250Atext-to-speech%2520%2528TTS%2529%2520systems.%2520Our%2520experiments%2520demonstrate%2520that%2520ConversaSynth%250Aeffectively%2520generates%2520highquality%2520synthetic%2520audio%2520datasets%252C%2520which%2520can%250Asignificantly%2520enhance%2520the%2520training%2520and%2520evaluation%2520of%2520models%2520for%2520audio%2520tagging%252C%250Aaudio%2520classification%252C%2520and%2520multi-speaker%2520speech%2520recognition.%2520The%2520results%250Aindicate%2520that%2520the%2520synthetic%2520datasets%2520generated%2520by%2520ConversaSynth%2520exhibit%250Asubstantial%2520diversity%2520and%2520realism%252C%2520making%2520them%2520suitable%2520for%2520developing%2520robust%252C%250Aadaptable%2520audio-based%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00946v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Framework%20for%20Synthetic%20Audio%20Conversations%20Generation%20using%20Large%0A%20%20Language%20Models&entry.906535625=Kaung%20Myat%20Kyaw%20and%20Jonathan%20Hoyin%20Chan&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20ConversaSynth%2C%20a%20framework%20designed%20to%20generate%0Asynthetic%20conversation%20audio%20using%20large%20language%20models%20%28LLMs%29%20with%20multiple%0Apersona%20settings.%20The%20framework%20first%20creates%20diverse%20and%20coherent%20text-based%0Adialogues%20across%20various%20topics%2C%20which%20are%20then%20converted%20into%20audio%20using%0Atext-to-speech%20%28TTS%29%20systems.%20Our%20experiments%20demonstrate%20that%20ConversaSynth%0Aeffectively%20generates%20highquality%20synthetic%20audio%20datasets%2C%20which%20can%0Asignificantly%20enhance%20the%20training%20and%20evaluation%20of%20models%20for%20audio%20tagging%2C%0Aaudio%20classification%2C%20and%20multi-speaker%20speech%20recognition.%20The%20results%0Aindicate%20that%20the%20synthetic%20datasets%20generated%20by%20ConversaSynth%20exhibit%0Asubstantial%20diversity%20and%20realism%2C%20making%20them%20suitable%20for%20developing%20robust%2C%0Aadaptable%20audio-based%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00946v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


