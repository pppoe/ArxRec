<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260205.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Efficient Scene Modeling via Structure-Aware and Region-Prioritized 3D Gaussians", "author": "Guangchi Fang and Bing Wang", "abstract": "Reconstructing 3D scenes with high fidelity and efficiency remains a central pursuit in computer vision and graphics. Recent advances in 3D Gaussian Splatting (3DGS) enable photorealistic rendering with Gaussian primitives, yet the modeling process remains governed predominantly by photometric supervision. This reliance often leads to irregular spatial distribution and indiscriminate primitive adjustments that largely ignore underlying geometric context. In this work, we rethink Gaussian modeling from a geometric standpoint and introduce Mini-Splatting2, an efficient scene modeling framework that couples structure-aware distribution and region-prioritized optimization, driving 3DGS into a geometry-regulated paradigm. The structure-aware distribution enforces spatial regularity through structured reorganization and representation sparsity, ensuring balanced structural coverage for compact organization. The region-prioritized optimization improves training discrimination through geometric saliency and computational selectivity, fostering appropriate structural emergence for fast convergence. These mechanisms alleviate the long-standing tension among representation compactness, convergence acceleration, and rendering fidelity. Extensive experiments demonstrate that Mini-Splatting2 achieves up to 4$\\times$ fewer Gaussians and 3$\\times$ faster optimization while maintaining state-of-the-art visual quality, paving the way towards structured and efficient 3D Gaussian modeling.", "link": "http://arxiv.org/abs/2411.12788v2", "date": "2026-02-05", "relevancy": 3.3985, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6924}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6842}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Scene%20Modeling%20via%20Structure-Aware%20and%20Region-Prioritized%203D%20Gaussians&body=Title%3A%20Efficient%20Scene%20Modeling%20via%20Structure-Aware%20and%20Region-Prioritized%203D%20Gaussians%0AAuthor%3A%20Guangchi%20Fang%20and%20Bing%20Wang%0AAbstract%3A%20Reconstructing%203D%20scenes%20with%20high%20fidelity%20and%20efficiency%20remains%20a%20central%20pursuit%20in%20computer%20vision%20and%20graphics.%20Recent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20enable%20photorealistic%20rendering%20with%20Gaussian%20primitives%2C%20yet%20the%20modeling%20process%20remains%20governed%20predominantly%20by%20photometric%20supervision.%20This%20reliance%20often%20leads%20to%20irregular%20spatial%20distribution%20and%20indiscriminate%20primitive%20adjustments%20that%20largely%20ignore%20underlying%20geometric%20context.%20In%20this%20work%2C%20we%20rethink%20Gaussian%20modeling%20from%20a%20geometric%20standpoint%20and%20introduce%20Mini-Splatting2%2C%20an%20efficient%20scene%20modeling%20framework%20that%20couples%20structure-aware%20distribution%20and%20region-prioritized%20optimization%2C%20driving%203DGS%20into%20a%20geometry-regulated%20paradigm.%20The%20structure-aware%20distribution%20enforces%20spatial%20regularity%20through%20structured%20reorganization%20and%20representation%20sparsity%2C%20ensuring%20balanced%20structural%20coverage%20for%20compact%20organization.%20The%20region-prioritized%20optimization%20improves%20training%20discrimination%20through%20geometric%20saliency%20and%20computational%20selectivity%2C%20fostering%20appropriate%20structural%20emergence%20for%20fast%20convergence.%20These%20mechanisms%20alleviate%20the%20long-standing%20tension%20among%20representation%20compactness%2C%20convergence%20acceleration%2C%20and%20rendering%20fidelity.%20Extensive%20experiments%20demonstrate%20that%20Mini-Splatting2%20achieves%20up%20to%204%24%5Ctimes%24%20fewer%20Gaussians%20and%203%24%5Ctimes%24%20faster%20optimization%20while%20maintaining%20state-of-the-art%20visual%20quality%2C%20paving%20the%20way%20towards%20structured%20and%20efficient%203D%20Gaussian%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2411.12788v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Scene%2520Modeling%2520via%2520Structure-Aware%2520and%2520Region-Prioritized%25203D%2520Gaussians%26entry.906535625%3DGuangchi%2520Fang%2520and%2520Bing%2520Wang%26entry.1292438233%3DReconstructing%25203D%2520scenes%2520with%2520high%2520fidelity%2520and%2520efficiency%2520remains%2520a%2520central%2520pursuit%2520in%2520computer%2520vision%2520and%2520graphics.%2520Recent%2520advances%2520in%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520enable%2520photorealistic%2520rendering%2520with%2520Gaussian%2520primitives%252C%2520yet%2520the%2520modeling%2520process%2520remains%2520governed%2520predominantly%2520by%2520photometric%2520supervision.%2520This%2520reliance%2520often%2520leads%2520to%2520irregular%2520spatial%2520distribution%2520and%2520indiscriminate%2520primitive%2520adjustments%2520that%2520largely%2520ignore%2520underlying%2520geometric%2520context.%2520In%2520this%2520work%252C%2520we%2520rethink%2520Gaussian%2520modeling%2520from%2520a%2520geometric%2520standpoint%2520and%2520introduce%2520Mini-Splatting2%252C%2520an%2520efficient%2520scene%2520modeling%2520framework%2520that%2520couples%2520structure-aware%2520distribution%2520and%2520region-prioritized%2520optimization%252C%2520driving%25203DGS%2520into%2520a%2520geometry-regulated%2520paradigm.%2520The%2520structure-aware%2520distribution%2520enforces%2520spatial%2520regularity%2520through%2520structured%2520reorganization%2520and%2520representation%2520sparsity%252C%2520ensuring%2520balanced%2520structural%2520coverage%2520for%2520compact%2520organization.%2520The%2520region-prioritized%2520optimization%2520improves%2520training%2520discrimination%2520through%2520geometric%2520saliency%2520and%2520computational%2520selectivity%252C%2520fostering%2520appropriate%2520structural%2520emergence%2520for%2520fast%2520convergence.%2520These%2520mechanisms%2520alleviate%2520the%2520long-standing%2520tension%2520among%2520representation%2520compactness%252C%2520convergence%2520acceleration%252C%2520and%2520rendering%2520fidelity.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Mini-Splatting2%2520achieves%2520up%2520to%25204%2524%255Ctimes%2524%2520fewer%2520Gaussians%2520and%25203%2524%255Ctimes%2524%2520faster%2520optimization%2520while%2520maintaining%2520state-of-the-art%2520visual%2520quality%252C%2520paving%2520the%2520way%2520towards%2520structured%2520and%2520efficient%25203D%2520Gaussian%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12788v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Scene%20Modeling%20via%20Structure-Aware%20and%20Region-Prioritized%203D%20Gaussians&entry.906535625=Guangchi%20Fang%20and%20Bing%20Wang&entry.1292438233=Reconstructing%203D%20scenes%20with%20high%20fidelity%20and%20efficiency%20remains%20a%20central%20pursuit%20in%20computer%20vision%20and%20graphics.%20Recent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20enable%20photorealistic%20rendering%20with%20Gaussian%20primitives%2C%20yet%20the%20modeling%20process%20remains%20governed%20predominantly%20by%20photometric%20supervision.%20This%20reliance%20often%20leads%20to%20irregular%20spatial%20distribution%20and%20indiscriminate%20primitive%20adjustments%20that%20largely%20ignore%20underlying%20geometric%20context.%20In%20this%20work%2C%20we%20rethink%20Gaussian%20modeling%20from%20a%20geometric%20standpoint%20and%20introduce%20Mini-Splatting2%2C%20an%20efficient%20scene%20modeling%20framework%20that%20couples%20structure-aware%20distribution%20and%20region-prioritized%20optimization%2C%20driving%203DGS%20into%20a%20geometry-regulated%20paradigm.%20The%20structure-aware%20distribution%20enforces%20spatial%20regularity%20through%20structured%20reorganization%20and%20representation%20sparsity%2C%20ensuring%20balanced%20structural%20coverage%20for%20compact%20organization.%20The%20region-prioritized%20optimization%20improves%20training%20discrimination%20through%20geometric%20saliency%20and%20computational%20selectivity%2C%20fostering%20appropriate%20structural%20emergence%20for%20fast%20convergence.%20These%20mechanisms%20alleviate%20the%20long-standing%20tension%20among%20representation%20compactness%2C%20convergence%20acceleration%2C%20and%20rendering%20fidelity.%20Extensive%20experiments%20demonstrate%20that%20Mini-Splatting2%20achieves%20up%20to%204%24%5Ctimes%24%20fewer%20Gaussians%20and%203%24%5Ctimes%24%20faster%20optimization%20while%20maintaining%20state-of-the-art%20visual%20quality%2C%20paving%20the%20way%20towards%20structured%20and%20efficient%203D%20Gaussian%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2411.12788v2&entry.124074799=Read"},
{"title": "ShapeGaussian: High-Fidelity 4D Human Reconstruction in Monocular Videos via Vision Priors", "author": "Zhenxiao Liang and Ning Zhang and Youbao Tang and Ruei-Sung Lin and Qixing Huang and Peng Chang and Jing Xiao", "abstract": "We introduce ShapeGaussian, a high-fidelity, template-free method for 4D human reconstruction from casual monocular videos. Generic reconstruction methods lacking robust vision priors, such as 4DGS, struggle to capture high-deformation human motion without multi-view cues. While template-based approaches, primarily relying on SMPL, such as HUGS, can produce photorealistic results, they are highly susceptible to errors in human pose estimation, often leading to unrealistic artifacts. In contrast, ShapeGaussian effectively integrates template-free vision priors to achieve both high-fidelity and robust scene reconstructions. Our method follows a two-step pipeline: first, we learn a coarse, deformable geometry using pretrained models that estimate data-driven priors, providing a foundation for reconstruction. Then, we refine this geometry using a neural deformation model to capture fine-grained dynamic details. By leveraging 2D vision priors, we mitigate artifacts from erroneous pose estimation in template-based methods and employ multiple reference frames to resolve the invisibility issue of 2D keypoints in a template-free manner. Extensive experiments demonstrate that ShapeGaussian surpasses template-based methods in reconstruction accuracy, achieving superior visual quality and robustness across diverse human motions in casual monocular videos.", "link": "http://arxiv.org/abs/2602.05572v1", "date": "2026-02-05", "relevancy": 3.375, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6974}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6771}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShapeGaussian%3A%20High-Fidelity%204D%20Human%20Reconstruction%20in%20Monocular%20Videos%20via%20Vision%20Priors&body=Title%3A%20ShapeGaussian%3A%20High-Fidelity%204D%20Human%20Reconstruction%20in%20Monocular%20Videos%20via%20Vision%20Priors%0AAuthor%3A%20Zhenxiao%20Liang%20and%20Ning%20Zhang%20and%20Youbao%20Tang%20and%20Ruei-Sung%20Lin%20and%20Qixing%20Huang%20and%20Peng%20Chang%20and%20Jing%20Xiao%0AAbstract%3A%20We%20introduce%20ShapeGaussian%2C%20a%20high-fidelity%2C%20template-free%20method%20for%204D%20human%20reconstruction%20from%20casual%20monocular%20videos.%20Generic%20reconstruction%20methods%20lacking%20robust%20vision%20priors%2C%20such%20as%204DGS%2C%20struggle%20to%20capture%20high-deformation%20human%20motion%20without%20multi-view%20cues.%20While%20template-based%20approaches%2C%20primarily%20relying%20on%20SMPL%2C%20such%20as%20HUGS%2C%20can%20produce%20photorealistic%20results%2C%20they%20are%20highly%20susceptible%20to%20errors%20in%20human%20pose%20estimation%2C%20often%20leading%20to%20unrealistic%20artifacts.%20In%20contrast%2C%20ShapeGaussian%20effectively%20integrates%20template-free%20vision%20priors%20to%20achieve%20both%20high-fidelity%20and%20robust%20scene%20reconstructions.%20Our%20method%20follows%20a%20two-step%20pipeline%3A%20first%2C%20we%20learn%20a%20coarse%2C%20deformable%20geometry%20using%20pretrained%20models%20that%20estimate%20data-driven%20priors%2C%20providing%20a%20foundation%20for%20reconstruction.%20Then%2C%20we%20refine%20this%20geometry%20using%20a%20neural%20deformation%20model%20to%20capture%20fine-grained%20dynamic%20details.%20By%20leveraging%202D%20vision%20priors%2C%20we%20mitigate%20artifacts%20from%20erroneous%20pose%20estimation%20in%20template-based%20methods%20and%20employ%20multiple%20reference%20frames%20to%20resolve%20the%20invisibility%20issue%20of%202D%20keypoints%20in%20a%20template-free%20manner.%20Extensive%20experiments%20demonstrate%20that%20ShapeGaussian%20surpasses%20template-based%20methods%20in%20reconstruction%20accuracy%2C%20achieving%20superior%20visual%20quality%20and%20robustness%20across%20diverse%20human%20motions%20in%20casual%20monocular%20videos.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05572v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShapeGaussian%253A%2520High-Fidelity%25204D%2520Human%2520Reconstruction%2520in%2520Monocular%2520Videos%2520via%2520Vision%2520Priors%26entry.906535625%3DZhenxiao%2520Liang%2520and%2520Ning%2520Zhang%2520and%2520Youbao%2520Tang%2520and%2520Ruei-Sung%2520Lin%2520and%2520Qixing%2520Huang%2520and%2520Peng%2520Chang%2520and%2520Jing%2520Xiao%26entry.1292438233%3DWe%2520introduce%2520ShapeGaussian%252C%2520a%2520high-fidelity%252C%2520template-free%2520method%2520for%25204D%2520human%2520reconstruction%2520from%2520casual%2520monocular%2520videos.%2520Generic%2520reconstruction%2520methods%2520lacking%2520robust%2520vision%2520priors%252C%2520such%2520as%25204DGS%252C%2520struggle%2520to%2520capture%2520high-deformation%2520human%2520motion%2520without%2520multi-view%2520cues.%2520While%2520template-based%2520approaches%252C%2520primarily%2520relying%2520on%2520SMPL%252C%2520such%2520as%2520HUGS%252C%2520can%2520produce%2520photorealistic%2520results%252C%2520they%2520are%2520highly%2520susceptible%2520to%2520errors%2520in%2520human%2520pose%2520estimation%252C%2520often%2520leading%2520to%2520unrealistic%2520artifacts.%2520In%2520contrast%252C%2520ShapeGaussian%2520effectively%2520integrates%2520template-free%2520vision%2520priors%2520to%2520achieve%2520both%2520high-fidelity%2520and%2520robust%2520scene%2520reconstructions.%2520Our%2520method%2520follows%2520a%2520two-step%2520pipeline%253A%2520first%252C%2520we%2520learn%2520a%2520coarse%252C%2520deformable%2520geometry%2520using%2520pretrained%2520models%2520that%2520estimate%2520data-driven%2520priors%252C%2520providing%2520a%2520foundation%2520for%2520reconstruction.%2520Then%252C%2520we%2520refine%2520this%2520geometry%2520using%2520a%2520neural%2520deformation%2520model%2520to%2520capture%2520fine-grained%2520dynamic%2520details.%2520By%2520leveraging%25202D%2520vision%2520priors%252C%2520we%2520mitigate%2520artifacts%2520from%2520erroneous%2520pose%2520estimation%2520in%2520template-based%2520methods%2520and%2520employ%2520multiple%2520reference%2520frames%2520to%2520resolve%2520the%2520invisibility%2520issue%2520of%25202D%2520keypoints%2520in%2520a%2520template-free%2520manner.%2520Extensive%2520experiments%2520demonstrate%2520that%2520ShapeGaussian%2520surpasses%2520template-based%2520methods%2520in%2520reconstruction%2520accuracy%252C%2520achieving%2520superior%2520visual%2520quality%2520and%2520robustness%2520across%2520diverse%2520human%2520motions%2520in%2520casual%2520monocular%2520videos.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05572v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShapeGaussian%3A%20High-Fidelity%204D%20Human%20Reconstruction%20in%20Monocular%20Videos%20via%20Vision%20Priors&entry.906535625=Zhenxiao%20Liang%20and%20Ning%20Zhang%20and%20Youbao%20Tang%20and%20Ruei-Sung%20Lin%20and%20Qixing%20Huang%20and%20Peng%20Chang%20and%20Jing%20Xiao&entry.1292438233=We%20introduce%20ShapeGaussian%2C%20a%20high-fidelity%2C%20template-free%20method%20for%204D%20human%20reconstruction%20from%20casual%20monocular%20videos.%20Generic%20reconstruction%20methods%20lacking%20robust%20vision%20priors%2C%20such%20as%204DGS%2C%20struggle%20to%20capture%20high-deformation%20human%20motion%20without%20multi-view%20cues.%20While%20template-based%20approaches%2C%20primarily%20relying%20on%20SMPL%2C%20such%20as%20HUGS%2C%20can%20produce%20photorealistic%20results%2C%20they%20are%20highly%20susceptible%20to%20errors%20in%20human%20pose%20estimation%2C%20often%20leading%20to%20unrealistic%20artifacts.%20In%20contrast%2C%20ShapeGaussian%20effectively%20integrates%20template-free%20vision%20priors%20to%20achieve%20both%20high-fidelity%20and%20robust%20scene%20reconstructions.%20Our%20method%20follows%20a%20two-step%20pipeline%3A%20first%2C%20we%20learn%20a%20coarse%2C%20deformable%20geometry%20using%20pretrained%20models%20that%20estimate%20data-driven%20priors%2C%20providing%20a%20foundation%20for%20reconstruction.%20Then%2C%20we%20refine%20this%20geometry%20using%20a%20neural%20deformation%20model%20to%20capture%20fine-grained%20dynamic%20details.%20By%20leveraging%202D%20vision%20priors%2C%20we%20mitigate%20artifacts%20from%20erroneous%20pose%20estimation%20in%20template-based%20methods%20and%20employ%20multiple%20reference%20frames%20to%20resolve%20the%20invisibility%20issue%20of%202D%20keypoints%20in%20a%20template-free%20manner.%20Extensive%20experiments%20demonstrate%20that%20ShapeGaussian%20surpasses%20template-based%20methods%20in%20reconstruction%20accuracy%2C%20achieving%20superior%20visual%20quality%20and%20robustness%20across%20diverse%20human%20motions%20in%20casual%20monocular%20videos.&entry.1838667208=http%3A//arxiv.org/abs/2602.05572v1&entry.124074799=Read"},
{"title": "REArtGS++: Generalizable Articulation Reconstruction with Temporal Geometry Constraint via Planar Gaussian Splatting", "author": "Di Wu and Liu Liu and Anran Huang and Yuyan Liu and Qiaojun Yu and Shaofan Liu and Liangtu Song and Cewu Lu", "abstract": "Articulated objects are pervasive in daily environments, such as drawers and refrigerators. Towards their part-level surface reconstruction and joint parameter estimation, REArtGS introduces a category-agnostic approach using multi-view RGB images at two different states. However, we observe that REArtGS still struggles with screw-joint or multi-part objects and lacks geometric constraints for unseen states. In this paper, we propose REArtGS++, a novel method towards generalizable articulated object reconstruction with temporal geometry constraint and planar Gaussian splatting. We first model a decoupled screw motion for each joint without type prior, and jointly optimize part-aware Gaussians with joint parameters through part motion blending. To introduce time-continuous geometric constraint for articulated modeling, we encourage Gaussians to be planar and propose a temporally consistent regularization between planar normal and depth through Taylor first-order expansion. Extensive experiments on both synthetic and real-world articulated objects demonstrate our superiority in generalizable part-level surface reconstruction and joint parameter estimation, compared to existing approaches. Project Site: https://sites.google.com/view/reartgs2/home.", "link": "http://arxiv.org/abs/2511.17059v3", "date": "2026-02-05", "relevancy": 3.357, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6861}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6671}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REArtGS%2B%2B%3A%20Generalizable%20Articulation%20Reconstruction%20with%20Temporal%20Geometry%20Constraint%20via%20Planar%20Gaussian%20Splatting&body=Title%3A%20REArtGS%2B%2B%3A%20Generalizable%20Articulation%20Reconstruction%20with%20Temporal%20Geometry%20Constraint%20via%20Planar%20Gaussian%20Splatting%0AAuthor%3A%20Di%20Wu%20and%20Liu%20Liu%20and%20Anran%20Huang%20and%20Yuyan%20Liu%20and%20Qiaojun%20Yu%20and%20Shaofan%20Liu%20and%20Liangtu%20Song%20and%20Cewu%20Lu%0AAbstract%3A%20Articulated%20objects%20are%20pervasive%20in%20daily%20environments%2C%20such%20as%20drawers%20and%20refrigerators.%20Towards%20their%20part-level%20surface%20reconstruction%20and%20joint%20parameter%20estimation%2C%20REArtGS%20introduces%20a%20category-agnostic%20approach%20using%20multi-view%20RGB%20images%20at%20two%20different%20states.%20However%2C%20we%20observe%20that%20REArtGS%20still%20struggles%20with%20screw-joint%20or%20multi-part%20objects%20and%20lacks%20geometric%20constraints%20for%20unseen%20states.%20In%20this%20paper%2C%20we%20propose%20REArtGS%2B%2B%2C%20a%20novel%20method%20towards%20generalizable%20articulated%20object%20reconstruction%20with%20temporal%20geometry%20constraint%20and%20planar%20Gaussian%20splatting.%20We%20first%20model%20a%20decoupled%20screw%20motion%20for%20each%20joint%20without%20type%20prior%2C%20and%20jointly%20optimize%20part-aware%20Gaussians%20with%20joint%20parameters%20through%20part%20motion%20blending.%20To%20introduce%20time-continuous%20geometric%20constraint%20for%20articulated%20modeling%2C%20we%20encourage%20Gaussians%20to%20be%20planar%20and%20propose%20a%20temporally%20consistent%20regularization%20between%20planar%20normal%20and%20depth%20through%20Taylor%20first-order%20expansion.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%20articulated%20objects%20demonstrate%20our%20superiority%20in%20generalizable%20part-level%20surface%20reconstruction%20and%20joint%20parameter%20estimation%2C%20compared%20to%20existing%20approaches.%20Project%20Site%3A%20https%3A//sites.google.com/view/reartgs2/home.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17059v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREArtGS%252B%252B%253A%2520Generalizable%2520Articulation%2520Reconstruction%2520with%2520Temporal%2520Geometry%2520Constraint%2520via%2520Planar%2520Gaussian%2520Splatting%26entry.906535625%3DDi%2520Wu%2520and%2520Liu%2520Liu%2520and%2520Anran%2520Huang%2520and%2520Yuyan%2520Liu%2520and%2520Qiaojun%2520Yu%2520and%2520Shaofan%2520Liu%2520and%2520Liangtu%2520Song%2520and%2520Cewu%2520Lu%26entry.1292438233%3DArticulated%2520objects%2520are%2520pervasive%2520in%2520daily%2520environments%252C%2520such%2520as%2520drawers%2520and%2520refrigerators.%2520Towards%2520their%2520part-level%2520surface%2520reconstruction%2520and%2520joint%2520parameter%2520estimation%252C%2520REArtGS%2520introduces%2520a%2520category-agnostic%2520approach%2520using%2520multi-view%2520RGB%2520images%2520at%2520two%2520different%2520states.%2520However%252C%2520we%2520observe%2520that%2520REArtGS%2520still%2520struggles%2520with%2520screw-joint%2520or%2520multi-part%2520objects%2520and%2520lacks%2520geometric%2520constraints%2520for%2520unseen%2520states.%2520In%2520this%2520paper%252C%2520we%2520propose%2520REArtGS%252B%252B%252C%2520a%2520novel%2520method%2520towards%2520generalizable%2520articulated%2520object%2520reconstruction%2520with%2520temporal%2520geometry%2520constraint%2520and%2520planar%2520Gaussian%2520splatting.%2520We%2520first%2520model%2520a%2520decoupled%2520screw%2520motion%2520for%2520each%2520joint%2520without%2520type%2520prior%252C%2520and%2520jointly%2520optimize%2520part-aware%2520Gaussians%2520with%2520joint%2520parameters%2520through%2520part%2520motion%2520blending.%2520To%2520introduce%2520time-continuous%2520geometric%2520constraint%2520for%2520articulated%2520modeling%252C%2520we%2520encourage%2520Gaussians%2520to%2520be%2520planar%2520and%2520propose%2520a%2520temporally%2520consistent%2520regularization%2520between%2520planar%2520normal%2520and%2520depth%2520through%2520Taylor%2520first-order%2520expansion.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520articulated%2520objects%2520demonstrate%2520our%2520superiority%2520in%2520generalizable%2520part-level%2520surface%2520reconstruction%2520and%2520joint%2520parameter%2520estimation%252C%2520compared%2520to%2520existing%2520approaches.%2520Project%2520Site%253A%2520https%253A//sites.google.com/view/reartgs2/home.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17059v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REArtGS%2B%2B%3A%20Generalizable%20Articulation%20Reconstruction%20with%20Temporal%20Geometry%20Constraint%20via%20Planar%20Gaussian%20Splatting&entry.906535625=Di%20Wu%20and%20Liu%20Liu%20and%20Anran%20Huang%20and%20Yuyan%20Liu%20and%20Qiaojun%20Yu%20and%20Shaofan%20Liu%20and%20Liangtu%20Song%20and%20Cewu%20Lu&entry.1292438233=Articulated%20objects%20are%20pervasive%20in%20daily%20environments%2C%20such%20as%20drawers%20and%20refrigerators.%20Towards%20their%20part-level%20surface%20reconstruction%20and%20joint%20parameter%20estimation%2C%20REArtGS%20introduces%20a%20category-agnostic%20approach%20using%20multi-view%20RGB%20images%20at%20two%20different%20states.%20However%2C%20we%20observe%20that%20REArtGS%20still%20struggles%20with%20screw-joint%20or%20multi-part%20objects%20and%20lacks%20geometric%20constraints%20for%20unseen%20states.%20In%20this%20paper%2C%20we%20propose%20REArtGS%2B%2B%2C%20a%20novel%20method%20towards%20generalizable%20articulated%20object%20reconstruction%20with%20temporal%20geometry%20constraint%20and%20planar%20Gaussian%20splatting.%20We%20first%20model%20a%20decoupled%20screw%20motion%20for%20each%20joint%20without%20type%20prior%2C%20and%20jointly%20optimize%20part-aware%20Gaussians%20with%20joint%20parameters%20through%20part%20motion%20blending.%20To%20introduce%20time-continuous%20geometric%20constraint%20for%20articulated%20modeling%2C%20we%20encourage%20Gaussians%20to%20be%20planar%20and%20propose%20a%20temporally%20consistent%20regularization%20between%20planar%20normal%20and%20depth%20through%20Taylor%20first-order%20expansion.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%20articulated%20objects%20demonstrate%20our%20superiority%20in%20generalizable%20part-level%20surface%20reconstruction%20and%20joint%20parameter%20estimation%2C%20compared%20to%20existing%20approaches.%20Project%20Site%3A%20https%3A//sites.google.com/view/reartgs2/home.&entry.1838667208=http%3A//arxiv.org/abs/2511.17059v3&entry.124074799=Read"},
{"title": "SharpTimeGS: Sharp and Stable Dynamic Gaussian Splatting via Lifespan Modulation", "author": "Zhanfeng Liao and Jiajun Zhang and Hanzhang Tu and Zhixi Wang and Yunqi Gao and Hongwen Zhang and Yebin Liu", "abstract": "Novel view synthesis of dynamic scenes is fundamental to achieving photorealistic 4D reconstruction and immersive visual experiences. Recent progress in Gaussian-based representations has significantly improved real-time rendering quality, yet existing methods still struggle to maintain a balance between long-term static and short-term dynamic regions in both representation and optimization. To address this, we present SharpTimeGS, a lifespan-aware 4D Gaussian framework that achieves temporally adaptive modeling of both static and dynamic regions under a unified representation. Specifically, we introduce a learnable lifespan parameter that reformulates temporal visibility from a Gaussian-shaped decay into a flat-top profile, allowing primitives to remain consistently active over their intended duration and avoiding redundant densification. In addition, the learned lifespan modulates each primitives' motion, reducing drift in long-lived static points while retaining unrestricted motion for short-lived dynamic ones. This effectively decouples motion magnitude from temporal duration, improving long-term stability without compromising dynamic fidelity. Moreover, we design a lifespan-velocity-aware densification strategy that mitigates optimization imbalance between static and dynamic regions by allocating more capacity to regions with pronounced motion while keeping static areas compact and stable. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art performance while supporting real-time rendering up to 4K resolution at 100 FPS on one RTX 4090.", "link": "http://arxiv.org/abs/2602.02989v2", "date": "2026-02-05", "relevancy": 3.2367, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6757}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.669}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SharpTimeGS%3A%20Sharp%20and%20Stable%20Dynamic%20Gaussian%20Splatting%20via%20Lifespan%20Modulation&body=Title%3A%20SharpTimeGS%3A%20Sharp%20and%20Stable%20Dynamic%20Gaussian%20Splatting%20via%20Lifespan%20Modulation%0AAuthor%3A%20Zhanfeng%20Liao%20and%20Jiajun%20Zhang%20and%20Hanzhang%20Tu%20and%20Zhixi%20Wang%20and%20Yunqi%20Gao%20and%20Hongwen%20Zhang%20and%20Yebin%20Liu%0AAbstract%3A%20Novel%20view%20synthesis%20of%20dynamic%20scenes%20is%20fundamental%20to%20achieving%20photorealistic%204D%20reconstruction%20and%20immersive%20visual%20experiences.%20Recent%20progress%20in%20Gaussian-based%20representations%20has%20significantly%20improved%20real-time%20rendering%20quality%2C%20yet%20existing%20methods%20still%20struggle%20to%20maintain%20a%20balance%20between%20long-term%20static%20and%20short-term%20dynamic%20regions%20in%20both%20representation%20and%20optimization.%20To%20address%20this%2C%20we%20present%20SharpTimeGS%2C%20a%20lifespan-aware%204D%20Gaussian%20framework%20that%20achieves%20temporally%20adaptive%20modeling%20of%20both%20static%20and%20dynamic%20regions%20under%20a%20unified%20representation.%20Specifically%2C%20we%20introduce%20a%20learnable%20lifespan%20parameter%20that%20reformulates%20temporal%20visibility%20from%20a%20Gaussian-shaped%20decay%20into%20a%20flat-top%20profile%2C%20allowing%20primitives%20to%20remain%20consistently%20active%20over%20their%20intended%20duration%20and%20avoiding%20redundant%20densification.%20In%20addition%2C%20the%20learned%20lifespan%20modulates%20each%20primitives%27%20motion%2C%20reducing%20drift%20in%20long-lived%20static%20points%20while%20retaining%20unrestricted%20motion%20for%20short-lived%20dynamic%20ones.%20This%20effectively%20decouples%20motion%20magnitude%20from%20temporal%20duration%2C%20improving%20long-term%20stability%20without%20compromising%20dynamic%20fidelity.%20Moreover%2C%20we%20design%20a%20lifespan-velocity-aware%20densification%20strategy%20that%20mitigates%20optimization%20imbalance%20between%20static%20and%20dynamic%20regions%20by%20allocating%20more%20capacity%20to%20regions%20with%20pronounced%20motion%20while%20keeping%20static%20areas%20compact%20and%20stable.%20Extensive%20experiments%20on%20multiple%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20while%20supporting%20real-time%20rendering%20up%20to%204K%20resolution%20at%20100%20FPS%20on%20one%20RTX%204090.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02989v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharpTimeGS%253A%2520Sharp%2520and%2520Stable%2520Dynamic%2520Gaussian%2520Splatting%2520via%2520Lifespan%2520Modulation%26entry.906535625%3DZhanfeng%2520Liao%2520and%2520Jiajun%2520Zhang%2520and%2520Hanzhang%2520Tu%2520and%2520Zhixi%2520Wang%2520and%2520Yunqi%2520Gao%2520and%2520Hongwen%2520Zhang%2520and%2520Yebin%2520Liu%26entry.1292438233%3DNovel%2520view%2520synthesis%2520of%2520dynamic%2520scenes%2520is%2520fundamental%2520to%2520achieving%2520photorealistic%25204D%2520reconstruction%2520and%2520immersive%2520visual%2520experiences.%2520Recent%2520progress%2520in%2520Gaussian-based%2520representations%2520has%2520significantly%2520improved%2520real-time%2520rendering%2520quality%252C%2520yet%2520existing%2520methods%2520still%2520struggle%2520to%2520maintain%2520a%2520balance%2520between%2520long-term%2520static%2520and%2520short-term%2520dynamic%2520regions%2520in%2520both%2520representation%2520and%2520optimization.%2520To%2520address%2520this%252C%2520we%2520present%2520SharpTimeGS%252C%2520a%2520lifespan-aware%25204D%2520Gaussian%2520framework%2520that%2520achieves%2520temporally%2520adaptive%2520modeling%2520of%2520both%2520static%2520and%2520dynamic%2520regions%2520under%2520a%2520unified%2520representation.%2520Specifically%252C%2520we%2520introduce%2520a%2520learnable%2520lifespan%2520parameter%2520that%2520reformulates%2520temporal%2520visibility%2520from%2520a%2520Gaussian-shaped%2520decay%2520into%2520a%2520flat-top%2520profile%252C%2520allowing%2520primitives%2520to%2520remain%2520consistently%2520active%2520over%2520their%2520intended%2520duration%2520and%2520avoiding%2520redundant%2520densification.%2520In%2520addition%252C%2520the%2520learned%2520lifespan%2520modulates%2520each%2520primitives%2527%2520motion%252C%2520reducing%2520drift%2520in%2520long-lived%2520static%2520points%2520while%2520retaining%2520unrestricted%2520motion%2520for%2520short-lived%2520dynamic%2520ones.%2520This%2520effectively%2520decouples%2520motion%2520magnitude%2520from%2520temporal%2520duration%252C%2520improving%2520long-term%2520stability%2520without%2520compromising%2520dynamic%2520fidelity.%2520Moreover%252C%2520we%2520design%2520a%2520lifespan-velocity-aware%2520densification%2520strategy%2520that%2520mitigates%2520optimization%2520imbalance%2520between%2520static%2520and%2520dynamic%2520regions%2520by%2520allocating%2520more%2520capacity%2520to%2520regions%2520with%2520pronounced%2520motion%2520while%2520keeping%2520static%2520areas%2520compact%2520and%2520stable.%2520Extensive%2520experiments%2520on%2520multiple%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%2520while%2520supporting%2520real-time%2520rendering%2520up%2520to%25204K%2520resolution%2520at%2520100%2520FPS%2520on%2520one%2520RTX%25204090.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02989v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SharpTimeGS%3A%20Sharp%20and%20Stable%20Dynamic%20Gaussian%20Splatting%20via%20Lifespan%20Modulation&entry.906535625=Zhanfeng%20Liao%20and%20Jiajun%20Zhang%20and%20Hanzhang%20Tu%20and%20Zhixi%20Wang%20and%20Yunqi%20Gao%20and%20Hongwen%20Zhang%20and%20Yebin%20Liu&entry.1292438233=Novel%20view%20synthesis%20of%20dynamic%20scenes%20is%20fundamental%20to%20achieving%20photorealistic%204D%20reconstruction%20and%20immersive%20visual%20experiences.%20Recent%20progress%20in%20Gaussian-based%20representations%20has%20significantly%20improved%20real-time%20rendering%20quality%2C%20yet%20existing%20methods%20still%20struggle%20to%20maintain%20a%20balance%20between%20long-term%20static%20and%20short-term%20dynamic%20regions%20in%20both%20representation%20and%20optimization.%20To%20address%20this%2C%20we%20present%20SharpTimeGS%2C%20a%20lifespan-aware%204D%20Gaussian%20framework%20that%20achieves%20temporally%20adaptive%20modeling%20of%20both%20static%20and%20dynamic%20regions%20under%20a%20unified%20representation.%20Specifically%2C%20we%20introduce%20a%20learnable%20lifespan%20parameter%20that%20reformulates%20temporal%20visibility%20from%20a%20Gaussian-shaped%20decay%20into%20a%20flat-top%20profile%2C%20allowing%20primitives%20to%20remain%20consistently%20active%20over%20their%20intended%20duration%20and%20avoiding%20redundant%20densification.%20In%20addition%2C%20the%20learned%20lifespan%20modulates%20each%20primitives%27%20motion%2C%20reducing%20drift%20in%20long-lived%20static%20points%20while%20retaining%20unrestricted%20motion%20for%20short-lived%20dynamic%20ones.%20This%20effectively%20decouples%20motion%20magnitude%20from%20temporal%20duration%2C%20improving%20long-term%20stability%20without%20compromising%20dynamic%20fidelity.%20Moreover%2C%20we%20design%20a%20lifespan-velocity-aware%20densification%20strategy%20that%20mitigates%20optimization%20imbalance%20between%20static%20and%20dynamic%20regions%20by%20allocating%20more%20capacity%20to%20regions%20with%20pronounced%20motion%20while%20keeping%20static%20areas%20compact%20and%20stable.%20Extensive%20experiments%20on%20multiple%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20while%20supporting%20real-time%20rendering%20up%20to%204K%20resolution%20at%20100%20FPS%20on%20one%20RTX%204090.&entry.1838667208=http%3A//arxiv.org/abs/2602.02989v2&entry.124074799=Read"},
{"title": "MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding", "author": "Benjamin Beilharz and Thomas S. A. Wallis", "abstract": "While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.", "link": "http://arxiv.org/abs/2512.12307v2", "date": "2026-02-05", "relevancy": 3.2355, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6678}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6678}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MRD%3A%20Using%20Physically%20Based%20Differentiable%20Rendering%20to%20Probe%20Vision%20Models%20for%203D%20Scene%20Understanding&body=Title%3A%20MRD%3A%20Using%20Physically%20Based%20Differentiable%20Rendering%20to%20Probe%20Vision%20Models%20for%203D%20Scene%20Understanding%0AAuthor%3A%20Benjamin%20Beilharz%20and%20Thomas%20S.%20A.%20Wallis%0AAbstract%3A%20While%20deep%20learning%20methods%20have%20achieved%20impressive%20success%20in%20many%20vision%20benchmarks%2C%20it%20remains%20difficult%20to%20understand%20and%20explain%20the%20representations%20and%20decisions%20of%20these%20models.%20Though%20vision%20models%20are%20typically%20trained%20on%202D%20inputs%2C%20they%20are%20often%20assumed%20to%20develop%20an%20implicit%20representation%20of%20the%20underlying%203D%20scene%20%28for%20example%2C%20showing%20tolerance%20to%20partial%20occlusion%2C%20or%20the%20ability%20to%20reason%20about%20relative%20depth%29.%20Here%2C%20we%20introduce%20MRD%20%28metamers%20rendered%20differentiably%29%2C%20an%20approach%20that%20uses%20physically%20based%20differentiable%20rendering%20to%20probe%20vision%20models%27%20implicit%20understanding%20of%20generative%203D%20scene%20properties%2C%20by%20finding%203D%20scene%20parameters%20that%20are%20physically%20different%20but%20produce%20the%20same%20model%20activation%20%28i.e.%20are%20model%20metamers%29.%20Unlike%20previous%20pixel-based%20methods%20for%20evaluating%20model%20representations%2C%20these%20reconstruction%20results%20are%20always%20grounded%20in%20physical%20scene%20descriptions.%20This%20means%20we%20can%2C%20for%20example%2C%20probe%20a%20model%27s%20sensitivity%20to%20object%20shape%20while%20holding%20material%20and%20lighting%20constant.%20As%20a%20proof-of-principle%2C%20we%20assess%20multiple%20models%20in%20their%20ability%20to%20recover%20scene%20parameters%20of%20geometry%20%28shape%29%20and%20bidirectional%20reflectance%20distribution%20function%20%28material%29.%20The%20results%20show%20high%20similarity%20in%20model%20activation%20between%20target%20and%20optimized%20scenes%2C%20with%20varying%20visual%20results.%20Qualitatively%2C%20these%20reconstructions%20help%20investigate%20the%20physical%20scene%20attributes%20to%20which%20models%20are%20sensitive%20or%20invariant.%20MRD%20holds%20promise%20for%20advancing%20our%20understanding%20of%20both%20computer%20and%20human%20vision%20by%20enabling%20analysis%20of%20how%20physical%20scene%20parameters%20drive%20changes%20in%20model%20responses.%0ALink%3A%20http%3A//arxiv.org/abs/2512.12307v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMRD%253A%2520Using%2520Physically%2520Based%2520Differentiable%2520Rendering%2520to%2520Probe%2520Vision%2520Models%2520for%25203D%2520Scene%2520Understanding%26entry.906535625%3DBenjamin%2520Beilharz%2520and%2520Thomas%2520S.%2520A.%2520Wallis%26entry.1292438233%3DWhile%2520deep%2520learning%2520methods%2520have%2520achieved%2520impressive%2520success%2520in%2520many%2520vision%2520benchmarks%252C%2520it%2520remains%2520difficult%2520to%2520understand%2520and%2520explain%2520the%2520representations%2520and%2520decisions%2520of%2520these%2520models.%2520Though%2520vision%2520models%2520are%2520typically%2520trained%2520on%25202D%2520inputs%252C%2520they%2520are%2520often%2520assumed%2520to%2520develop%2520an%2520implicit%2520representation%2520of%2520the%2520underlying%25203D%2520scene%2520%2528for%2520example%252C%2520showing%2520tolerance%2520to%2520partial%2520occlusion%252C%2520or%2520the%2520ability%2520to%2520reason%2520about%2520relative%2520depth%2529.%2520Here%252C%2520we%2520introduce%2520MRD%2520%2528metamers%2520rendered%2520differentiably%2529%252C%2520an%2520approach%2520that%2520uses%2520physically%2520based%2520differentiable%2520rendering%2520to%2520probe%2520vision%2520models%2527%2520implicit%2520understanding%2520of%2520generative%25203D%2520scene%2520properties%252C%2520by%2520finding%25203D%2520scene%2520parameters%2520that%2520are%2520physically%2520different%2520but%2520produce%2520the%2520same%2520model%2520activation%2520%2528i.e.%2520are%2520model%2520metamers%2529.%2520Unlike%2520previous%2520pixel-based%2520methods%2520for%2520evaluating%2520model%2520representations%252C%2520these%2520reconstruction%2520results%2520are%2520always%2520grounded%2520in%2520physical%2520scene%2520descriptions.%2520This%2520means%2520we%2520can%252C%2520for%2520example%252C%2520probe%2520a%2520model%2527s%2520sensitivity%2520to%2520object%2520shape%2520while%2520holding%2520material%2520and%2520lighting%2520constant.%2520As%2520a%2520proof-of-principle%252C%2520we%2520assess%2520multiple%2520models%2520in%2520their%2520ability%2520to%2520recover%2520scene%2520parameters%2520of%2520geometry%2520%2528shape%2529%2520and%2520bidirectional%2520reflectance%2520distribution%2520function%2520%2528material%2529.%2520The%2520results%2520show%2520high%2520similarity%2520in%2520model%2520activation%2520between%2520target%2520and%2520optimized%2520scenes%252C%2520with%2520varying%2520visual%2520results.%2520Qualitatively%252C%2520these%2520reconstructions%2520help%2520investigate%2520the%2520physical%2520scene%2520attributes%2520to%2520which%2520models%2520are%2520sensitive%2520or%2520invariant.%2520MRD%2520holds%2520promise%2520for%2520advancing%2520our%2520understanding%2520of%2520both%2520computer%2520and%2520human%2520vision%2520by%2520enabling%2520analysis%2520of%2520how%2520physical%2520scene%2520parameters%2520drive%2520changes%2520in%2520model%2520responses.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.12307v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MRD%3A%20Using%20Physically%20Based%20Differentiable%20Rendering%20to%20Probe%20Vision%20Models%20for%203D%20Scene%20Understanding&entry.906535625=Benjamin%20Beilharz%20and%20Thomas%20S.%20A.%20Wallis&entry.1292438233=While%20deep%20learning%20methods%20have%20achieved%20impressive%20success%20in%20many%20vision%20benchmarks%2C%20it%20remains%20difficult%20to%20understand%20and%20explain%20the%20representations%20and%20decisions%20of%20these%20models.%20Though%20vision%20models%20are%20typically%20trained%20on%202D%20inputs%2C%20they%20are%20often%20assumed%20to%20develop%20an%20implicit%20representation%20of%20the%20underlying%203D%20scene%20%28for%20example%2C%20showing%20tolerance%20to%20partial%20occlusion%2C%20or%20the%20ability%20to%20reason%20about%20relative%20depth%29.%20Here%2C%20we%20introduce%20MRD%20%28metamers%20rendered%20differentiably%29%2C%20an%20approach%20that%20uses%20physically%20based%20differentiable%20rendering%20to%20probe%20vision%20models%27%20implicit%20understanding%20of%20generative%203D%20scene%20properties%2C%20by%20finding%203D%20scene%20parameters%20that%20are%20physically%20different%20but%20produce%20the%20same%20model%20activation%20%28i.e.%20are%20model%20metamers%29.%20Unlike%20previous%20pixel-based%20methods%20for%20evaluating%20model%20representations%2C%20these%20reconstruction%20results%20are%20always%20grounded%20in%20physical%20scene%20descriptions.%20This%20means%20we%20can%2C%20for%20example%2C%20probe%20a%20model%27s%20sensitivity%20to%20object%20shape%20while%20holding%20material%20and%20lighting%20constant.%20As%20a%20proof-of-principle%2C%20we%20assess%20multiple%20models%20in%20their%20ability%20to%20recover%20scene%20parameters%20of%20geometry%20%28shape%29%20and%20bidirectional%20reflectance%20distribution%20function%20%28material%29.%20The%20results%20show%20high%20similarity%20in%20model%20activation%20between%20target%20and%20optimized%20scenes%2C%20with%20varying%20visual%20results.%20Qualitatively%2C%20these%20reconstructions%20help%20investigate%20the%20physical%20scene%20attributes%20to%20which%20models%20are%20sensitive%20or%20invariant.%20MRD%20holds%20promise%20for%20advancing%20our%20understanding%20of%20both%20computer%20and%20human%20vision%20by%20enabling%20analysis%20of%20how%20physical%20scene%20parameters%20drive%20changes%20in%20model%20responses.&entry.1838667208=http%3A//arxiv.org/abs/2512.12307v2&entry.124074799=Read"},
{"title": "VGGT-Motion: Motion-Aware Calibration-Free Monocular SLAM for Long-Range Consistency", "author": "Zhuang Xiong and Chen Zhang and Qingshan Xu and Wenbing Tao", "abstract": "Despite recent progress in calibration-free monocular SLAM via 3D vision foundation models, scale drift remains severe on long sequences. Motion-agnostic partitioning breaks contextual coherence and causes zero-motion drift, while conventional geometric alignment is computationally expensive. To address these issues, we propose VGGT-Motion, a calibration-free SLAM system for efficient and robust global consistency over kilometer-scale trajectories. Specifically, we first propose a motion-aware submap construction mechanism that uses optical flow to guide adaptive partitioning, prune static redundancy, and encapsulate turns for stable local geometry. We then design an anchor-driven direct Sim(3) registration strategy. By exploiting context-balanced anchors, it achieves search-free, pixel-wise dense alignment and efficient loop closure without costly feature matching. Finally, a lightweight submap-level pose graph optimization enforces global consistency with linear complexity, enabling scalable long-range operation. Experiments show that VGGT-Motion markedly improves trajectory accuracy and efficiency, achieving state-of-the-art performance in zero-shot, long-range calibration-free monocular SLAM.", "link": "http://arxiv.org/abs/2602.05508v1", "date": "2026-02-05", "relevancy": 3.1156, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6563}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6171}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VGGT-Motion%3A%20Motion-Aware%20Calibration-Free%20Monocular%20SLAM%20for%20Long-Range%20Consistency&body=Title%3A%20VGGT-Motion%3A%20Motion-Aware%20Calibration-Free%20Monocular%20SLAM%20for%20Long-Range%20Consistency%0AAuthor%3A%20Zhuang%20Xiong%20and%20Chen%20Zhang%20and%20Qingshan%20Xu%20and%20Wenbing%20Tao%0AAbstract%3A%20Despite%20recent%20progress%20in%20calibration-free%20monocular%20SLAM%20via%203D%20vision%20foundation%20models%2C%20scale%20drift%20remains%20severe%20on%20long%20sequences.%20Motion-agnostic%20partitioning%20breaks%20contextual%20coherence%20and%20causes%20zero-motion%20drift%2C%20while%20conventional%20geometric%20alignment%20is%20computationally%20expensive.%20To%20address%20these%20issues%2C%20we%20propose%20VGGT-Motion%2C%20a%20calibration-free%20SLAM%20system%20for%20efficient%20and%20robust%20global%20consistency%20over%20kilometer-scale%20trajectories.%20Specifically%2C%20we%20first%20propose%20a%20motion-aware%20submap%20construction%20mechanism%20that%20uses%20optical%20flow%20to%20guide%20adaptive%20partitioning%2C%20prune%20static%20redundancy%2C%20and%20encapsulate%20turns%20for%20stable%20local%20geometry.%20We%20then%20design%20an%20anchor-driven%20direct%20Sim%283%29%20registration%20strategy.%20By%20exploiting%20context-balanced%20anchors%2C%20it%20achieves%20search-free%2C%20pixel-wise%20dense%20alignment%20and%20efficient%20loop%20closure%20without%20costly%20feature%20matching.%20Finally%2C%20a%20lightweight%20submap-level%20pose%20graph%20optimization%20enforces%20global%20consistency%20with%20linear%20complexity%2C%20enabling%20scalable%20long-range%20operation.%20Experiments%20show%20that%20VGGT-Motion%20markedly%20improves%20trajectory%20accuracy%20and%20efficiency%2C%20achieving%20state-of-the-art%20performance%20in%20zero-shot%2C%20long-range%20calibration-free%20monocular%20SLAM.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVGGT-Motion%253A%2520Motion-Aware%2520Calibration-Free%2520Monocular%2520SLAM%2520for%2520Long-Range%2520Consistency%26entry.906535625%3DZhuang%2520Xiong%2520and%2520Chen%2520Zhang%2520and%2520Qingshan%2520Xu%2520and%2520Wenbing%2520Tao%26entry.1292438233%3DDespite%2520recent%2520progress%2520in%2520calibration-free%2520monocular%2520SLAM%2520via%25203D%2520vision%2520foundation%2520models%252C%2520scale%2520drift%2520remains%2520severe%2520on%2520long%2520sequences.%2520Motion-agnostic%2520partitioning%2520breaks%2520contextual%2520coherence%2520and%2520causes%2520zero-motion%2520drift%252C%2520while%2520conventional%2520geometric%2520alignment%2520is%2520computationally%2520expensive.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520VGGT-Motion%252C%2520a%2520calibration-free%2520SLAM%2520system%2520for%2520efficient%2520and%2520robust%2520global%2520consistency%2520over%2520kilometer-scale%2520trajectories.%2520Specifically%252C%2520we%2520first%2520propose%2520a%2520motion-aware%2520submap%2520construction%2520mechanism%2520that%2520uses%2520optical%2520flow%2520to%2520guide%2520adaptive%2520partitioning%252C%2520prune%2520static%2520redundancy%252C%2520and%2520encapsulate%2520turns%2520for%2520stable%2520local%2520geometry.%2520We%2520then%2520design%2520an%2520anchor-driven%2520direct%2520Sim%25283%2529%2520registration%2520strategy.%2520By%2520exploiting%2520context-balanced%2520anchors%252C%2520it%2520achieves%2520search-free%252C%2520pixel-wise%2520dense%2520alignment%2520and%2520efficient%2520loop%2520closure%2520without%2520costly%2520feature%2520matching.%2520Finally%252C%2520a%2520lightweight%2520submap-level%2520pose%2520graph%2520optimization%2520enforces%2520global%2520consistency%2520with%2520linear%2520complexity%252C%2520enabling%2520scalable%2520long-range%2520operation.%2520Experiments%2520show%2520that%2520VGGT-Motion%2520markedly%2520improves%2520trajectory%2520accuracy%2520and%2520efficiency%252C%2520achieving%2520state-of-the-art%2520performance%2520in%2520zero-shot%252C%2520long-range%2520calibration-free%2520monocular%2520SLAM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VGGT-Motion%3A%20Motion-Aware%20Calibration-Free%20Monocular%20SLAM%20for%20Long-Range%20Consistency&entry.906535625=Zhuang%20Xiong%20and%20Chen%20Zhang%20and%20Qingshan%20Xu%20and%20Wenbing%20Tao&entry.1292438233=Despite%20recent%20progress%20in%20calibration-free%20monocular%20SLAM%20via%203D%20vision%20foundation%20models%2C%20scale%20drift%20remains%20severe%20on%20long%20sequences.%20Motion-agnostic%20partitioning%20breaks%20contextual%20coherence%20and%20causes%20zero-motion%20drift%2C%20while%20conventional%20geometric%20alignment%20is%20computationally%20expensive.%20To%20address%20these%20issues%2C%20we%20propose%20VGGT-Motion%2C%20a%20calibration-free%20SLAM%20system%20for%20efficient%20and%20robust%20global%20consistency%20over%20kilometer-scale%20trajectories.%20Specifically%2C%20we%20first%20propose%20a%20motion-aware%20submap%20construction%20mechanism%20that%20uses%20optical%20flow%20to%20guide%20adaptive%20partitioning%2C%20prune%20static%20redundancy%2C%20and%20encapsulate%20turns%20for%20stable%20local%20geometry.%20We%20then%20design%20an%20anchor-driven%20direct%20Sim%283%29%20registration%20strategy.%20By%20exploiting%20context-balanced%20anchors%2C%20it%20achieves%20search-free%2C%20pixel-wise%20dense%20alignment%20and%20efficient%20loop%20closure%20without%20costly%20feature%20matching.%20Finally%2C%20a%20lightweight%20submap-level%20pose%20graph%20optimization%20enforces%20global%20consistency%20with%20linear%20complexity%2C%20enabling%20scalable%20long-range%20operation.%20Experiments%20show%20that%20VGGT-Motion%20markedly%20improves%20trajectory%20accuracy%20and%20efficiency%2C%20achieving%20state-of-the-art%20performance%20in%20zero-shot%2C%20long-range%20calibration-free%20monocular%20SLAM.&entry.1838667208=http%3A//arxiv.org/abs/2602.05508v1&entry.124074799=Read"},
{"title": "Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation", "author": "David Shavin and Sagie Benaim", "abstract": "Vision Foundation Models (VFMs) have achieved remarkable success when applied to various downstream 2D tasks. Despite their effectiveness, they often exhibit a critical lack of 3D awareness. To this end, we introduce Splat and Distill, a framework that instills robust 3D awareness into 2D VFMs by augmenting the teacher model with a fast, feed-forward 3D reconstruction pipeline. Given 2D features produced by a teacher model, our method first lifts these features into an explicit 3D Gaussian representation, in a feedforward manner. These 3D features are then ``splatted\" onto novel viewpoints, producing a set of novel 2D feature maps used to supervise the student model, ``distilling\" geometrically grounded knowledge. By replacing slow per-scene optimization of prior work with our feed-forward lifting approach, our framework avoids feature-averaging artifacts, creating a dynamic learning process where the teacher's consistency improves alongside that of the student. We conduct a comprehensive evaluation on a suite of downstream tasks, including monocular depth estimation, surface normal estimation, multi-view correspondence, and semantic segmentation. Our method significantly outperforms prior works, not only achieving substantial gains in 3D awareness but also enhancing the underlying semantic richness of 2D features. Project page is available at https://davidshavin4.github.io/Splat-and-Distill/", "link": "http://arxiv.org/abs/2602.06032v1", "date": "2026-02-05", "relevancy": 3.0492, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6307}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6092}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Splat%20and%20Distill%3A%20Augmenting%20Teachers%20with%20Feed-Forward%203D%20Reconstruction%20For%203D-Aware%20Distillation&body=Title%3A%20Splat%20and%20Distill%3A%20Augmenting%20Teachers%20with%20Feed-Forward%203D%20Reconstruction%20For%203D-Aware%20Distillation%0AAuthor%3A%20David%20Shavin%20and%20Sagie%20Benaim%0AAbstract%3A%20Vision%20Foundation%20Models%20%28VFMs%29%20have%20achieved%20remarkable%20success%20when%20applied%20to%20various%20downstream%202D%20tasks.%20Despite%20their%20effectiveness%2C%20they%20often%20exhibit%20a%20critical%20lack%20of%203D%20awareness.%20To%20this%20end%2C%20we%20introduce%20Splat%20and%20Distill%2C%20a%20framework%20that%20instills%20robust%203D%20awareness%20into%202D%20VFMs%20by%20augmenting%20the%20teacher%20model%20with%20a%20fast%2C%20feed-forward%203D%20reconstruction%20pipeline.%20Given%202D%20features%20produced%20by%20a%20teacher%20model%2C%20our%20method%20first%20lifts%20these%20features%20into%20an%20explicit%203D%20Gaussian%20representation%2C%20in%20a%20feedforward%20manner.%20These%203D%20features%20are%20then%20%60%60splatted%22%20onto%20novel%20viewpoints%2C%20producing%20a%20set%20of%20novel%202D%20feature%20maps%20used%20to%20supervise%20the%20student%20model%2C%20%60%60distilling%22%20geometrically%20grounded%20knowledge.%20By%20replacing%20slow%20per-scene%20optimization%20of%20prior%20work%20with%20our%20feed-forward%20lifting%20approach%2C%20our%20framework%20avoids%20feature-averaging%20artifacts%2C%20creating%20a%20dynamic%20learning%20process%20where%20the%20teacher%27s%20consistency%20improves%20alongside%20that%20of%20the%20student.%20We%20conduct%20a%20comprehensive%20evaluation%20on%20a%20suite%20of%20downstream%20tasks%2C%20including%20monocular%20depth%20estimation%2C%20surface%20normal%20estimation%2C%20multi-view%20correspondence%2C%20and%20semantic%20segmentation.%20Our%20method%20significantly%20outperforms%20prior%20works%2C%20not%20only%20achieving%20substantial%20gains%20in%203D%20awareness%20but%20also%20enhancing%20the%20underlying%20semantic%20richness%20of%202D%20features.%20Project%20page%20is%20available%20at%20https%3A//davidshavin4.github.io/Splat-and-Distill/%0ALink%3A%20http%3A//arxiv.org/abs/2602.06032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplat%2520and%2520Distill%253A%2520Augmenting%2520Teachers%2520with%2520Feed-Forward%25203D%2520Reconstruction%2520For%25203D-Aware%2520Distillation%26entry.906535625%3DDavid%2520Shavin%2520and%2520Sagie%2520Benaim%26entry.1292438233%3DVision%2520Foundation%2520Models%2520%2528VFMs%2529%2520have%2520achieved%2520remarkable%2520success%2520when%2520applied%2520to%2520various%2520downstream%25202D%2520tasks.%2520Despite%2520their%2520effectiveness%252C%2520they%2520often%2520exhibit%2520a%2520critical%2520lack%2520of%25203D%2520awareness.%2520To%2520this%2520end%252C%2520we%2520introduce%2520Splat%2520and%2520Distill%252C%2520a%2520framework%2520that%2520instills%2520robust%25203D%2520awareness%2520into%25202D%2520VFMs%2520by%2520augmenting%2520the%2520teacher%2520model%2520with%2520a%2520fast%252C%2520feed-forward%25203D%2520reconstruction%2520pipeline.%2520Given%25202D%2520features%2520produced%2520by%2520a%2520teacher%2520model%252C%2520our%2520method%2520first%2520lifts%2520these%2520features%2520into%2520an%2520explicit%25203D%2520Gaussian%2520representation%252C%2520in%2520a%2520feedforward%2520manner.%2520These%25203D%2520features%2520are%2520then%2520%2560%2560splatted%2522%2520onto%2520novel%2520viewpoints%252C%2520producing%2520a%2520set%2520of%2520novel%25202D%2520feature%2520maps%2520used%2520to%2520supervise%2520the%2520student%2520model%252C%2520%2560%2560distilling%2522%2520geometrically%2520grounded%2520knowledge.%2520By%2520replacing%2520slow%2520per-scene%2520optimization%2520of%2520prior%2520work%2520with%2520our%2520feed-forward%2520lifting%2520approach%252C%2520our%2520framework%2520avoids%2520feature-averaging%2520artifacts%252C%2520creating%2520a%2520dynamic%2520learning%2520process%2520where%2520the%2520teacher%2527s%2520consistency%2520improves%2520alongside%2520that%2520of%2520the%2520student.%2520We%2520conduct%2520a%2520comprehensive%2520evaluation%2520on%2520a%2520suite%2520of%2520downstream%2520tasks%252C%2520including%2520monocular%2520depth%2520estimation%252C%2520surface%2520normal%2520estimation%252C%2520multi-view%2520correspondence%252C%2520and%2520semantic%2520segmentation.%2520Our%2520method%2520significantly%2520outperforms%2520prior%2520works%252C%2520not%2520only%2520achieving%2520substantial%2520gains%2520in%25203D%2520awareness%2520but%2520also%2520enhancing%2520the%2520underlying%2520semantic%2520richness%2520of%25202D%2520features.%2520Project%2520page%2520is%2520available%2520at%2520https%253A//davidshavin4.github.io/Splat-and-Distill/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.06032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Splat%20and%20Distill%3A%20Augmenting%20Teachers%20with%20Feed-Forward%203D%20Reconstruction%20For%203D-Aware%20Distillation&entry.906535625=David%20Shavin%20and%20Sagie%20Benaim&entry.1292438233=Vision%20Foundation%20Models%20%28VFMs%29%20have%20achieved%20remarkable%20success%20when%20applied%20to%20various%20downstream%202D%20tasks.%20Despite%20their%20effectiveness%2C%20they%20often%20exhibit%20a%20critical%20lack%20of%203D%20awareness.%20To%20this%20end%2C%20we%20introduce%20Splat%20and%20Distill%2C%20a%20framework%20that%20instills%20robust%203D%20awareness%20into%202D%20VFMs%20by%20augmenting%20the%20teacher%20model%20with%20a%20fast%2C%20feed-forward%203D%20reconstruction%20pipeline.%20Given%202D%20features%20produced%20by%20a%20teacher%20model%2C%20our%20method%20first%20lifts%20these%20features%20into%20an%20explicit%203D%20Gaussian%20representation%2C%20in%20a%20feedforward%20manner.%20These%203D%20features%20are%20then%20%60%60splatted%22%20onto%20novel%20viewpoints%2C%20producing%20a%20set%20of%20novel%202D%20feature%20maps%20used%20to%20supervise%20the%20student%20model%2C%20%60%60distilling%22%20geometrically%20grounded%20knowledge.%20By%20replacing%20slow%20per-scene%20optimization%20of%20prior%20work%20with%20our%20feed-forward%20lifting%20approach%2C%20our%20framework%20avoids%20feature-averaging%20artifacts%2C%20creating%20a%20dynamic%20learning%20process%20where%20the%20teacher%27s%20consistency%20improves%20alongside%20that%20of%20the%20student.%20We%20conduct%20a%20comprehensive%20evaluation%20on%20a%20suite%20of%20downstream%20tasks%2C%20including%20monocular%20depth%20estimation%2C%20surface%20normal%20estimation%2C%20multi-view%20correspondence%2C%20and%20semantic%20segmentation.%20Our%20method%20significantly%20outperforms%20prior%20works%2C%20not%20only%20achieving%20substantial%20gains%20in%203D%20awareness%20but%20also%20enhancing%20the%20underlying%20semantic%20richness%20of%202D%20features.%20Project%20page%20is%20available%20at%20https%3A//davidshavin4.github.io/Splat-and-Distill/&entry.1838667208=http%3A//arxiv.org/abs/2602.06032v1&entry.124074799=Read"},
{"title": "RISE-Video: Can Video Generators Decode Implicit World Rules?", "author": "Mingxin Liu and Shuran Ma and Shibei Meng and Xiangyu Zhao and Zicheng Zhang and Shaofeng Zhang and Zhihang Zhong and Peixian Chen and Haoyu Cao and Xing Sun and Haodong Duan and Xue Yang", "abstract": "While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \\textit{Reasoning Alignment}, \\textit{Temporal Consistency}, \\textit{Physical Rationality}, and \\textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.", "link": "http://arxiv.org/abs/2602.05986v1", "date": "2026-02-05", "relevancy": 3.0404, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6237}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6024}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RISE-Video%3A%20Can%20Video%20Generators%20Decode%20Implicit%20World%20Rules%3F&body=Title%3A%20RISE-Video%3A%20Can%20Video%20Generators%20Decode%20Implicit%20World%20Rules%3F%0AAuthor%3A%20Mingxin%20Liu%20and%20Shuran%20Ma%20and%20Shibei%20Meng%20and%20Xiangyu%20Zhao%20and%20Zicheng%20Zhang%20and%20Shaofeng%20Zhang%20and%20Zhihang%20Zhong%20and%20Peixian%20Chen%20and%20Haoyu%20Cao%20and%20Xing%20Sun%20and%20Haodong%20Duan%20and%20Xue%20Yang%0AAbstract%3A%20While%20generative%20video%20models%20have%20achieved%20remarkable%20visual%20fidelity%2C%20their%20capacity%20to%20internalize%20and%20reason%20over%20implicit%20world%20rules%20remains%20a%20critical%20yet%20under-explored%20frontier.%20To%20bridge%20this%20gap%2C%20we%20present%20RISE-Video%2C%20a%20pioneering%20reasoning-oriented%20benchmark%20for%20Text-Image-to-Video%20%28TI2V%29%20synthesis%20that%20shifts%20the%20evaluative%20focus%20from%20surface-level%20aesthetics%20to%20deep%20cognitive%20reasoning.%20RISE-Video%20comprises%20467%20meticulously%20human-annotated%20samples%20spanning%20eight%20rigorous%20categories%2C%20providing%20a%20structured%20testbed%20for%20probing%20model%20intelligence%20across%20diverse%20dimensions%2C%20ranging%20from%20commonsense%20and%20spatial%20dynamics%20to%20specialized%20subject%20domains.%20Our%20framework%20introduces%20a%20multi-dimensional%20evaluation%20protocol%20consisting%20of%20four%20metrics%3A%20%5Ctextit%7BReasoning%20Alignment%7D%2C%20%5Ctextit%7BTemporal%20Consistency%7D%2C%20%5Ctextit%7BPhysical%20Rationality%7D%2C%20and%20%5Ctextit%7BVisual%20Quality%7D.%20To%20further%20support%20scalable%20evaluation%2C%20we%20propose%20an%20automated%20pipeline%20leveraging%20Large%20Multimodal%20Models%20%28LMMs%29%20to%20emulate%20human-centric%20assessment.%20Extensive%20experiments%20on%2011%20state-of-the-art%20TI2V%20models%20reveal%20pervasive%20deficiencies%20in%20simulating%20complex%20scenarios%20under%20implicit%20constraints%2C%20offering%20critical%20insights%20for%20the%20advancement%20of%20future%20world-simulating%20generative%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05986v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRISE-Video%253A%2520Can%2520Video%2520Generators%2520Decode%2520Implicit%2520World%2520Rules%253F%26entry.906535625%3DMingxin%2520Liu%2520and%2520Shuran%2520Ma%2520and%2520Shibei%2520Meng%2520and%2520Xiangyu%2520Zhao%2520and%2520Zicheng%2520Zhang%2520and%2520Shaofeng%2520Zhang%2520and%2520Zhihang%2520Zhong%2520and%2520Peixian%2520Chen%2520and%2520Haoyu%2520Cao%2520and%2520Xing%2520Sun%2520and%2520Haodong%2520Duan%2520and%2520Xue%2520Yang%26entry.1292438233%3DWhile%2520generative%2520video%2520models%2520have%2520achieved%2520remarkable%2520visual%2520fidelity%252C%2520their%2520capacity%2520to%2520internalize%2520and%2520reason%2520over%2520implicit%2520world%2520rules%2520remains%2520a%2520critical%2520yet%2520under-explored%2520frontier.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520RISE-Video%252C%2520a%2520pioneering%2520reasoning-oriented%2520benchmark%2520for%2520Text-Image-to-Video%2520%2528TI2V%2529%2520synthesis%2520that%2520shifts%2520the%2520evaluative%2520focus%2520from%2520surface-level%2520aesthetics%2520to%2520deep%2520cognitive%2520reasoning.%2520RISE-Video%2520comprises%2520467%2520meticulously%2520human-annotated%2520samples%2520spanning%2520eight%2520rigorous%2520categories%252C%2520providing%2520a%2520structured%2520testbed%2520for%2520probing%2520model%2520intelligence%2520across%2520diverse%2520dimensions%252C%2520ranging%2520from%2520commonsense%2520and%2520spatial%2520dynamics%2520to%2520specialized%2520subject%2520domains.%2520Our%2520framework%2520introduces%2520a%2520multi-dimensional%2520evaluation%2520protocol%2520consisting%2520of%2520four%2520metrics%253A%2520%255Ctextit%257BReasoning%2520Alignment%257D%252C%2520%255Ctextit%257BTemporal%2520Consistency%257D%252C%2520%255Ctextit%257BPhysical%2520Rationality%257D%252C%2520and%2520%255Ctextit%257BVisual%2520Quality%257D.%2520To%2520further%2520support%2520scalable%2520evaluation%252C%2520we%2520propose%2520an%2520automated%2520pipeline%2520leveraging%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520to%2520emulate%2520human-centric%2520assessment.%2520Extensive%2520experiments%2520on%252011%2520state-of-the-art%2520TI2V%2520models%2520reveal%2520pervasive%2520deficiencies%2520in%2520simulating%2520complex%2520scenarios%2520under%2520implicit%2520constraints%252C%2520offering%2520critical%2520insights%2520for%2520the%2520advancement%2520of%2520future%2520world-simulating%2520generative%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05986v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RISE-Video%3A%20Can%20Video%20Generators%20Decode%20Implicit%20World%20Rules%3F&entry.906535625=Mingxin%20Liu%20and%20Shuran%20Ma%20and%20Shibei%20Meng%20and%20Xiangyu%20Zhao%20and%20Zicheng%20Zhang%20and%20Shaofeng%20Zhang%20and%20Zhihang%20Zhong%20and%20Peixian%20Chen%20and%20Haoyu%20Cao%20and%20Xing%20Sun%20and%20Haodong%20Duan%20and%20Xue%20Yang&entry.1292438233=While%20generative%20video%20models%20have%20achieved%20remarkable%20visual%20fidelity%2C%20their%20capacity%20to%20internalize%20and%20reason%20over%20implicit%20world%20rules%20remains%20a%20critical%20yet%20under-explored%20frontier.%20To%20bridge%20this%20gap%2C%20we%20present%20RISE-Video%2C%20a%20pioneering%20reasoning-oriented%20benchmark%20for%20Text-Image-to-Video%20%28TI2V%29%20synthesis%20that%20shifts%20the%20evaluative%20focus%20from%20surface-level%20aesthetics%20to%20deep%20cognitive%20reasoning.%20RISE-Video%20comprises%20467%20meticulously%20human-annotated%20samples%20spanning%20eight%20rigorous%20categories%2C%20providing%20a%20structured%20testbed%20for%20probing%20model%20intelligence%20across%20diverse%20dimensions%2C%20ranging%20from%20commonsense%20and%20spatial%20dynamics%20to%20specialized%20subject%20domains.%20Our%20framework%20introduces%20a%20multi-dimensional%20evaluation%20protocol%20consisting%20of%20four%20metrics%3A%20%5Ctextit%7BReasoning%20Alignment%7D%2C%20%5Ctextit%7BTemporal%20Consistency%7D%2C%20%5Ctextit%7BPhysical%20Rationality%7D%2C%20and%20%5Ctextit%7BVisual%20Quality%7D.%20To%20further%20support%20scalable%20evaluation%2C%20we%20propose%20an%20automated%20pipeline%20leveraging%20Large%20Multimodal%20Models%20%28LMMs%29%20to%20emulate%20human-centric%20assessment.%20Extensive%20experiments%20on%2011%20state-of-the-art%20TI2V%20models%20reveal%20pervasive%20deficiencies%20in%20simulating%20complex%20scenarios%20under%20implicit%20constraints%2C%20offering%20critical%20insights%20for%20the%20advancement%20of%20future%20world-simulating%20generative%20models.&entry.1838667208=http%3A//arxiv.org/abs/2602.05986v1&entry.124074799=Read"},
{"title": "GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra", "author": "Mateusz Michalkiewicz and Anekha Sokhal and Tadeusz Michalkiewicz and Piotr Pawlikowski and Mahsa Baktashmotlagh and Varun Jampani and Guha Balakrishnan", "abstract": "Modern monocular 3D reconstruction methods and vision-language models (VLMs) demonstrate impressive results on standard benchmarks, yet recent works cast doubt on their true understanding of geometric properties. We introduce GOQ, a comprehensive benchmark specifically designed to evaluate the geometric reasoning capabilities of vision and vision-language foundation models. GIQ comprises synthetic and real-world images and corresponding 3D meshes of diverse polyhedra covering varying levels of complexity and symmetry, from Platonic, Archimedean, Johnson, and Catalan solids to stellations and compound shapes. Through systematic experiments involving monocular 3D reconstruction, 3D symmetry detection, mental rotation tests, and zero-shot shape classification tasks, we reveal significant shortcomings in current models. State-of-the-art reconstruction algorithms trained on extensive 3D datasets struggle to reconstruct even basic geometric Platonic solids accurately. Next, although foundation models may be shown via linear and non-linear probing to capture specific 3D symmetry elements, they falter significantly in tasks requiring detailed geometric differentiation, such as mental rotation. Moreover, advanced vision-language assistants such as ChatGPT, Gemini and Claud exhibit remarkably low accuracy in interpreting basic shape properties such as face geometry, convexity, and compound structures of complex polyhedra. GIQ is publicly available at toomanymatts.github.io/giq-benchmark/, providing a structured platform to benchmark critical gaps in geometric intelligence and facilitate future progress in robust, geometry-aware representation learning.", "link": "http://arxiv.org/abs/2506.08194v3", "date": "2026-02-05", "relevancy": 3.036, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6092}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GIQ%3A%20Benchmarking%203D%20Geometric%20Reasoning%20of%20Vision%20Foundation%20Models%20with%20Simulated%20and%20Real%20Polyhedra&body=Title%3A%20GIQ%3A%20Benchmarking%203D%20Geometric%20Reasoning%20of%20Vision%20Foundation%20Models%20with%20Simulated%20and%20Real%20Polyhedra%0AAuthor%3A%20Mateusz%20Michalkiewicz%20and%20Anekha%20Sokhal%20and%20Tadeusz%20Michalkiewicz%20and%20Piotr%20Pawlikowski%20and%20Mahsa%20Baktashmotlagh%20and%20Varun%20Jampani%20and%20Guha%20Balakrishnan%0AAbstract%3A%20Modern%20monocular%203D%20reconstruction%20methods%20and%20vision-language%20models%20%28VLMs%29%20demonstrate%20impressive%20results%20on%20standard%20benchmarks%2C%20yet%20recent%20works%20cast%20doubt%20on%20their%20true%20understanding%20of%20geometric%20properties.%20We%20introduce%20GOQ%2C%20a%20comprehensive%20benchmark%20specifically%20designed%20to%20evaluate%20the%20geometric%20reasoning%20capabilities%20of%20vision%20and%20vision-language%20foundation%20models.%20GIQ%20comprises%20synthetic%20and%20real-world%20images%20and%20corresponding%203D%20meshes%20of%20diverse%20polyhedra%20covering%20varying%20levels%20of%20complexity%20and%20symmetry%2C%20from%20Platonic%2C%20Archimedean%2C%20Johnson%2C%20and%20Catalan%20solids%20to%20stellations%20and%20compound%20shapes.%20Through%20systematic%20experiments%20involving%20monocular%203D%20reconstruction%2C%203D%20symmetry%20detection%2C%20mental%20rotation%20tests%2C%20and%20zero-shot%20shape%20classification%20tasks%2C%20we%20reveal%20significant%20shortcomings%20in%20current%20models.%20State-of-the-art%20reconstruction%20algorithms%20trained%20on%20extensive%203D%20datasets%20struggle%20to%20reconstruct%20even%20basic%20geometric%20Platonic%20solids%20accurately.%20Next%2C%20although%20foundation%20models%20may%20be%20shown%20via%20linear%20and%20non-linear%20probing%20to%20capture%20specific%203D%20symmetry%20elements%2C%20they%20falter%20significantly%20in%20tasks%20requiring%20detailed%20geometric%20differentiation%2C%20such%20as%20mental%20rotation.%20Moreover%2C%20advanced%20vision-language%20assistants%20such%20as%20ChatGPT%2C%20Gemini%20and%20Claud%20exhibit%20remarkably%20low%20accuracy%20in%20interpreting%20basic%20shape%20properties%20such%20as%20face%20geometry%2C%20convexity%2C%20and%20compound%20structures%20of%20complex%20polyhedra.%20GIQ%20is%20publicly%20available%20at%20toomanymatts.github.io/giq-benchmark/%2C%20providing%20a%20structured%20platform%20to%20benchmark%20critical%20gaps%20in%20geometric%20intelligence%20and%20facilitate%20future%20progress%20in%20robust%2C%20geometry-aware%20representation%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2506.08194v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGIQ%253A%2520Benchmarking%25203D%2520Geometric%2520Reasoning%2520of%2520Vision%2520Foundation%2520Models%2520with%2520Simulated%2520and%2520Real%2520Polyhedra%26entry.906535625%3DMateusz%2520Michalkiewicz%2520and%2520Anekha%2520Sokhal%2520and%2520Tadeusz%2520Michalkiewicz%2520and%2520Piotr%2520Pawlikowski%2520and%2520Mahsa%2520Baktashmotlagh%2520and%2520Varun%2520Jampani%2520and%2520Guha%2520Balakrishnan%26entry.1292438233%3DModern%2520monocular%25203D%2520reconstruction%2520methods%2520and%2520vision-language%2520models%2520%2528VLMs%2529%2520demonstrate%2520impressive%2520results%2520on%2520standard%2520benchmarks%252C%2520yet%2520recent%2520works%2520cast%2520doubt%2520on%2520their%2520true%2520understanding%2520of%2520geometric%2520properties.%2520We%2520introduce%2520GOQ%252C%2520a%2520comprehensive%2520benchmark%2520specifically%2520designed%2520to%2520evaluate%2520the%2520geometric%2520reasoning%2520capabilities%2520of%2520vision%2520and%2520vision-language%2520foundation%2520models.%2520GIQ%2520comprises%2520synthetic%2520and%2520real-world%2520images%2520and%2520corresponding%25203D%2520meshes%2520of%2520diverse%2520polyhedra%2520covering%2520varying%2520levels%2520of%2520complexity%2520and%2520symmetry%252C%2520from%2520Platonic%252C%2520Archimedean%252C%2520Johnson%252C%2520and%2520Catalan%2520solids%2520to%2520stellations%2520and%2520compound%2520shapes.%2520Through%2520systematic%2520experiments%2520involving%2520monocular%25203D%2520reconstruction%252C%25203D%2520symmetry%2520detection%252C%2520mental%2520rotation%2520tests%252C%2520and%2520zero-shot%2520shape%2520classification%2520tasks%252C%2520we%2520reveal%2520significant%2520shortcomings%2520in%2520current%2520models.%2520State-of-the-art%2520reconstruction%2520algorithms%2520trained%2520on%2520extensive%25203D%2520datasets%2520struggle%2520to%2520reconstruct%2520even%2520basic%2520geometric%2520Platonic%2520solids%2520accurately.%2520Next%252C%2520although%2520foundation%2520models%2520may%2520be%2520shown%2520via%2520linear%2520and%2520non-linear%2520probing%2520to%2520capture%2520specific%25203D%2520symmetry%2520elements%252C%2520they%2520falter%2520significantly%2520in%2520tasks%2520requiring%2520detailed%2520geometric%2520differentiation%252C%2520such%2520as%2520mental%2520rotation.%2520Moreover%252C%2520advanced%2520vision-language%2520assistants%2520such%2520as%2520ChatGPT%252C%2520Gemini%2520and%2520Claud%2520exhibit%2520remarkably%2520low%2520accuracy%2520in%2520interpreting%2520basic%2520shape%2520properties%2520such%2520as%2520face%2520geometry%252C%2520convexity%252C%2520and%2520compound%2520structures%2520of%2520complex%2520polyhedra.%2520GIQ%2520is%2520publicly%2520available%2520at%2520toomanymatts.github.io/giq-benchmark/%252C%2520providing%2520a%2520structured%2520platform%2520to%2520benchmark%2520critical%2520gaps%2520in%2520geometric%2520intelligence%2520and%2520facilitate%2520future%2520progress%2520in%2520robust%252C%2520geometry-aware%2520representation%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08194v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GIQ%3A%20Benchmarking%203D%20Geometric%20Reasoning%20of%20Vision%20Foundation%20Models%20with%20Simulated%20and%20Real%20Polyhedra&entry.906535625=Mateusz%20Michalkiewicz%20and%20Anekha%20Sokhal%20and%20Tadeusz%20Michalkiewicz%20and%20Piotr%20Pawlikowski%20and%20Mahsa%20Baktashmotlagh%20and%20Varun%20Jampani%20and%20Guha%20Balakrishnan&entry.1292438233=Modern%20monocular%203D%20reconstruction%20methods%20and%20vision-language%20models%20%28VLMs%29%20demonstrate%20impressive%20results%20on%20standard%20benchmarks%2C%20yet%20recent%20works%20cast%20doubt%20on%20their%20true%20understanding%20of%20geometric%20properties.%20We%20introduce%20GOQ%2C%20a%20comprehensive%20benchmark%20specifically%20designed%20to%20evaluate%20the%20geometric%20reasoning%20capabilities%20of%20vision%20and%20vision-language%20foundation%20models.%20GIQ%20comprises%20synthetic%20and%20real-world%20images%20and%20corresponding%203D%20meshes%20of%20diverse%20polyhedra%20covering%20varying%20levels%20of%20complexity%20and%20symmetry%2C%20from%20Platonic%2C%20Archimedean%2C%20Johnson%2C%20and%20Catalan%20solids%20to%20stellations%20and%20compound%20shapes.%20Through%20systematic%20experiments%20involving%20monocular%203D%20reconstruction%2C%203D%20symmetry%20detection%2C%20mental%20rotation%20tests%2C%20and%20zero-shot%20shape%20classification%20tasks%2C%20we%20reveal%20significant%20shortcomings%20in%20current%20models.%20State-of-the-art%20reconstruction%20algorithms%20trained%20on%20extensive%203D%20datasets%20struggle%20to%20reconstruct%20even%20basic%20geometric%20Platonic%20solids%20accurately.%20Next%2C%20although%20foundation%20models%20may%20be%20shown%20via%20linear%20and%20non-linear%20probing%20to%20capture%20specific%203D%20symmetry%20elements%2C%20they%20falter%20significantly%20in%20tasks%20requiring%20detailed%20geometric%20differentiation%2C%20such%20as%20mental%20rotation.%20Moreover%2C%20advanced%20vision-language%20assistants%20such%20as%20ChatGPT%2C%20Gemini%20and%20Claud%20exhibit%20remarkably%20low%20accuracy%20in%20interpreting%20basic%20shape%20properties%20such%20as%20face%20geometry%2C%20convexity%2C%20and%20compound%20structures%20of%20complex%20polyhedra.%20GIQ%20is%20publicly%20available%20at%20toomanymatts.github.io/giq-benchmark/%2C%20providing%20a%20structured%20platform%20to%20benchmark%20critical%20gaps%20in%20geometric%20intelligence%20and%20facilitate%20future%20progress%20in%20robust%2C%20geometry-aware%20representation%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2506.08194v3&entry.124074799=Read"},
{"title": "OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention", "author": "Zhangquan Chen and Jiale Tao and Ruihuang Li and Yihao Hu and Ruitao Chen and Zhantao Yang and Xinlei Yu and Haodong Jing and Manyuan Zhang and Shuai Shao and Biao Wang and Qinglin Lu and Ruqi Huang", "abstract": "While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to \"think with omnimodal cues\" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.", "link": "http://arxiv.org/abs/2602.05847v1", "date": "2026-02-05", "relevancy": 3.0157, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6308}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6308}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniVideo-R1%3A%20Reinforcing%20Audio-visual%20Reasoning%20with%20Query%20Intention%20and%20Modality%20Attention&body=Title%3A%20OmniVideo-R1%3A%20Reinforcing%20Audio-visual%20Reasoning%20with%20Query%20Intention%20and%20Modality%20Attention%0AAuthor%3A%20Zhangquan%20Chen%20and%20Jiale%20Tao%20and%20Ruihuang%20Li%20and%20Yihao%20Hu%20and%20Ruitao%20Chen%20and%20Zhantao%20Yang%20and%20Xinlei%20Yu%20and%20Haodong%20Jing%20and%20Manyuan%20Zhang%20and%20Shuai%20Shao%20and%20Biao%20Wang%20and%20Qinglin%20Lu%20and%20Ruqi%20Huang%0AAbstract%3A%20While%20humans%20perceive%20the%20world%20through%20diverse%20modalities%20that%20operate%20synergistically%20to%20support%20a%20holistic%20understanding%20of%20their%20surroundings%2C%20existing%20omnivideo%20models%20still%20face%20substantial%20challenges%20on%20audio-visual%20understanding%20tasks.%20In%20this%20paper%2C%20we%20propose%20OmniVideo-R1%2C%20a%20novel%20reinforced%20framework%20that%20improves%20mixed-modality%20reasoning.%20OmniVideo-R1%20empowers%20models%20to%20%22think%20with%20omnimodal%20cues%22%20by%20two%20key%20strategies%3A%20%281%29%20query-intensive%20grounding%20based%20on%20self-supervised%20learning%20paradigms%3B%20and%20%282%29%20modality-attentive%20fusion%20built%20upon%20contrastive%20learning%20paradigms.%20Extensive%20experiments%20on%20multiple%20benchmarks%20demonstrate%20that%20OmniVideo-R1%20consistently%20outperforms%20strong%20baselines%2C%20highlighting%20its%20effectiveness%20and%20robust%20generalization%20capabilities.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniVideo-R1%253A%2520Reinforcing%2520Audio-visual%2520Reasoning%2520with%2520Query%2520Intention%2520and%2520Modality%2520Attention%26entry.906535625%3DZhangquan%2520Chen%2520and%2520Jiale%2520Tao%2520and%2520Ruihuang%2520Li%2520and%2520Yihao%2520Hu%2520and%2520Ruitao%2520Chen%2520and%2520Zhantao%2520Yang%2520and%2520Xinlei%2520Yu%2520and%2520Haodong%2520Jing%2520and%2520Manyuan%2520Zhang%2520and%2520Shuai%2520Shao%2520and%2520Biao%2520Wang%2520and%2520Qinglin%2520Lu%2520and%2520Ruqi%2520Huang%26entry.1292438233%3DWhile%2520humans%2520perceive%2520the%2520world%2520through%2520diverse%2520modalities%2520that%2520operate%2520synergistically%2520to%2520support%2520a%2520holistic%2520understanding%2520of%2520their%2520surroundings%252C%2520existing%2520omnivideo%2520models%2520still%2520face%2520substantial%2520challenges%2520on%2520audio-visual%2520understanding%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520OmniVideo-R1%252C%2520a%2520novel%2520reinforced%2520framework%2520that%2520improves%2520mixed-modality%2520reasoning.%2520OmniVideo-R1%2520empowers%2520models%2520to%2520%2522think%2520with%2520omnimodal%2520cues%2522%2520by%2520two%2520key%2520strategies%253A%2520%25281%2529%2520query-intensive%2520grounding%2520based%2520on%2520self-supervised%2520learning%2520paradigms%253B%2520and%2520%25282%2529%2520modality-attentive%2520fusion%2520built%2520upon%2520contrastive%2520learning%2520paradigms.%2520Extensive%2520experiments%2520on%2520multiple%2520benchmarks%2520demonstrate%2520that%2520OmniVideo-R1%2520consistently%2520outperforms%2520strong%2520baselines%252C%2520highlighting%2520its%2520effectiveness%2520and%2520robust%2520generalization%2520capabilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniVideo-R1%3A%20Reinforcing%20Audio-visual%20Reasoning%20with%20Query%20Intention%20and%20Modality%20Attention&entry.906535625=Zhangquan%20Chen%20and%20Jiale%20Tao%20and%20Ruihuang%20Li%20and%20Yihao%20Hu%20and%20Ruitao%20Chen%20and%20Zhantao%20Yang%20and%20Xinlei%20Yu%20and%20Haodong%20Jing%20and%20Manyuan%20Zhang%20and%20Shuai%20Shao%20and%20Biao%20Wang%20and%20Qinglin%20Lu%20and%20Ruqi%20Huang&entry.1292438233=While%20humans%20perceive%20the%20world%20through%20diverse%20modalities%20that%20operate%20synergistically%20to%20support%20a%20holistic%20understanding%20of%20their%20surroundings%2C%20existing%20omnivideo%20models%20still%20face%20substantial%20challenges%20on%20audio-visual%20understanding%20tasks.%20In%20this%20paper%2C%20we%20propose%20OmniVideo-R1%2C%20a%20novel%20reinforced%20framework%20that%20improves%20mixed-modality%20reasoning.%20OmniVideo-R1%20empowers%20models%20to%20%22think%20with%20omnimodal%20cues%22%20by%20two%20key%20strategies%3A%20%281%29%20query-intensive%20grounding%20based%20on%20self-supervised%20learning%20paradigms%3B%20and%20%282%29%20modality-attentive%20fusion%20built%20upon%20contrastive%20learning%20paradigms.%20Extensive%20experiments%20on%20multiple%20benchmarks%20demonstrate%20that%20OmniVideo-R1%20consistently%20outperforms%20strong%20baselines%2C%20highlighting%20its%20effectiveness%20and%20robust%20generalization%20capabilities.&entry.1838667208=http%3A//arxiv.org/abs/2602.05847v1&entry.124074799=Read"},
{"title": "Unified Sensor Simulation for Autonomous Driving", "author": "Nikolay Patakin and Arsenii Shirokov and Anton Konushin and Dmitry Senushkin", "abstract": "In this work, we introduce \\textbf{XSIM}, a sensor simulation framework for autonomous driving. XSIM extends 3DGUT splatting with a generalized rolling-shutter modeling tailored for autonomous driving applications. Our framework provides a unified and flexible formulation for appearance and geometric sensor modeling, enabling rendering of complex sensor distortions in dynamic environments. We identify spherical cameras, such as LiDARs, as a critical edge case for existing 3DGUT splatting due to cyclic projection and time discontinuities at azimuth boundaries leading to incorrect particle projection. To address this issue, we propose a phase modeling mechanism that explicitly accounts temporal and shape discontinuities of Gaussians projected by the Unscented Transform at azimuth borders. In addition, we introduce an extended 3D Gaussian representation that incorporates two distinct opacity parameters to resolve mismatches between geometry and color distributions. As a result, our framework provides enhanced scene representations with improved geometric consistency and photorealistic appearance. We evaluate our framework extensively on multiple autonomous driving datasets, including Waymo Open Dataset, Argoverse 2, and PandaSet. Our framework consistently outperforms strong recent baselines and achieves state-of-the-art performance across all datasets. The source code is publicly available at \\href{https://github.com/whesense/XSIM}{https://github.com/whesense/XSIM}.", "link": "http://arxiv.org/abs/2602.05617v1", "date": "2026-02-05", "relevancy": 3.0003, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6223}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.589}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Sensor%20Simulation%20for%20Autonomous%20Driving&body=Title%3A%20Unified%20Sensor%20Simulation%20for%20Autonomous%20Driving%0AAuthor%3A%20Nikolay%20Patakin%20and%20Arsenii%20Shirokov%20and%20Anton%20Konushin%20and%20Dmitry%20Senushkin%0AAbstract%3A%20In%20this%20work%2C%20we%20introduce%20%5Ctextbf%7BXSIM%7D%2C%20a%20sensor%20simulation%20framework%20for%20autonomous%20driving.%20XSIM%20extends%203DGUT%20splatting%20with%20a%20generalized%20rolling-shutter%20modeling%20tailored%20for%20autonomous%20driving%20applications.%20Our%20framework%20provides%20a%20unified%20and%20flexible%20formulation%20for%20appearance%20and%20geometric%20sensor%20modeling%2C%20enabling%20rendering%20of%20complex%20sensor%20distortions%20in%20dynamic%20environments.%20We%20identify%20spherical%20cameras%2C%20such%20as%20LiDARs%2C%20as%20a%20critical%20edge%20case%20for%20existing%203DGUT%20splatting%20due%20to%20cyclic%20projection%20and%20time%20discontinuities%20at%20azimuth%20boundaries%20leading%20to%20incorrect%20particle%20projection.%20To%20address%20this%20issue%2C%20we%20propose%20a%20phase%20modeling%20mechanism%20that%20explicitly%20accounts%20temporal%20and%20shape%20discontinuities%20of%20Gaussians%20projected%20by%20the%20Unscented%20Transform%20at%20azimuth%20borders.%20In%20addition%2C%20we%20introduce%20an%20extended%203D%20Gaussian%20representation%20that%20incorporates%20two%20distinct%20opacity%20parameters%20to%20resolve%20mismatches%20between%20geometry%20and%20color%20distributions.%20As%20a%20result%2C%20our%20framework%20provides%20enhanced%20scene%20representations%20with%20improved%20geometric%20consistency%20and%20photorealistic%20appearance.%20We%20evaluate%20our%20framework%20extensively%20on%20multiple%20autonomous%20driving%20datasets%2C%20including%20Waymo%20Open%20Dataset%2C%20Argoverse%202%2C%20and%20PandaSet.%20Our%20framework%20consistently%20outperforms%20strong%20recent%20baselines%20and%20achieves%20state-of-the-art%20performance%20across%20all%20datasets.%20The%20source%20code%20is%20publicly%20available%20at%20%5Chref%7Bhttps%3A//github.com/whesense/XSIM%7D%7Bhttps%3A//github.com/whesense/XSIM%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Sensor%2520Simulation%2520for%2520Autonomous%2520Driving%26entry.906535625%3DNikolay%2520Patakin%2520and%2520Arsenii%2520Shirokov%2520and%2520Anton%2520Konushin%2520and%2520Dmitry%2520Senushkin%26entry.1292438233%3DIn%2520this%2520work%252C%2520we%2520introduce%2520%255Ctextbf%257BXSIM%257D%252C%2520a%2520sensor%2520simulation%2520framework%2520for%2520autonomous%2520driving.%2520XSIM%2520extends%25203DGUT%2520splatting%2520with%2520a%2520generalized%2520rolling-shutter%2520modeling%2520tailored%2520for%2520autonomous%2520driving%2520applications.%2520Our%2520framework%2520provides%2520a%2520unified%2520and%2520flexible%2520formulation%2520for%2520appearance%2520and%2520geometric%2520sensor%2520modeling%252C%2520enabling%2520rendering%2520of%2520complex%2520sensor%2520distortions%2520in%2520dynamic%2520environments.%2520We%2520identify%2520spherical%2520cameras%252C%2520such%2520as%2520LiDARs%252C%2520as%2520a%2520critical%2520edge%2520case%2520for%2520existing%25203DGUT%2520splatting%2520due%2520to%2520cyclic%2520projection%2520and%2520time%2520discontinuities%2520at%2520azimuth%2520boundaries%2520leading%2520to%2520incorrect%2520particle%2520projection.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520phase%2520modeling%2520mechanism%2520that%2520explicitly%2520accounts%2520temporal%2520and%2520shape%2520discontinuities%2520of%2520Gaussians%2520projected%2520by%2520the%2520Unscented%2520Transform%2520at%2520azimuth%2520borders.%2520In%2520addition%252C%2520we%2520introduce%2520an%2520extended%25203D%2520Gaussian%2520representation%2520that%2520incorporates%2520two%2520distinct%2520opacity%2520parameters%2520to%2520resolve%2520mismatches%2520between%2520geometry%2520and%2520color%2520distributions.%2520As%2520a%2520result%252C%2520our%2520framework%2520provides%2520enhanced%2520scene%2520representations%2520with%2520improved%2520geometric%2520consistency%2520and%2520photorealistic%2520appearance.%2520We%2520evaluate%2520our%2520framework%2520extensively%2520on%2520multiple%2520autonomous%2520driving%2520datasets%252C%2520including%2520Waymo%2520Open%2520Dataset%252C%2520Argoverse%25202%252C%2520and%2520PandaSet.%2520Our%2520framework%2520consistently%2520outperforms%2520strong%2520recent%2520baselines%2520and%2520achieves%2520state-of-the-art%2520performance%2520across%2520all%2520datasets.%2520The%2520source%2520code%2520is%2520publicly%2520available%2520at%2520%255Chref%257Bhttps%253A//github.com/whesense/XSIM%257D%257Bhttps%253A//github.com/whesense/XSIM%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Sensor%20Simulation%20for%20Autonomous%20Driving&entry.906535625=Nikolay%20Patakin%20and%20Arsenii%20Shirokov%20and%20Anton%20Konushin%20and%20Dmitry%20Senushkin&entry.1292438233=In%20this%20work%2C%20we%20introduce%20%5Ctextbf%7BXSIM%7D%2C%20a%20sensor%20simulation%20framework%20for%20autonomous%20driving.%20XSIM%20extends%203DGUT%20splatting%20with%20a%20generalized%20rolling-shutter%20modeling%20tailored%20for%20autonomous%20driving%20applications.%20Our%20framework%20provides%20a%20unified%20and%20flexible%20formulation%20for%20appearance%20and%20geometric%20sensor%20modeling%2C%20enabling%20rendering%20of%20complex%20sensor%20distortions%20in%20dynamic%20environments.%20We%20identify%20spherical%20cameras%2C%20such%20as%20LiDARs%2C%20as%20a%20critical%20edge%20case%20for%20existing%203DGUT%20splatting%20due%20to%20cyclic%20projection%20and%20time%20discontinuities%20at%20azimuth%20boundaries%20leading%20to%20incorrect%20particle%20projection.%20To%20address%20this%20issue%2C%20we%20propose%20a%20phase%20modeling%20mechanism%20that%20explicitly%20accounts%20temporal%20and%20shape%20discontinuities%20of%20Gaussians%20projected%20by%20the%20Unscented%20Transform%20at%20azimuth%20borders.%20In%20addition%2C%20we%20introduce%20an%20extended%203D%20Gaussian%20representation%20that%20incorporates%20two%20distinct%20opacity%20parameters%20to%20resolve%20mismatches%20between%20geometry%20and%20color%20distributions.%20As%20a%20result%2C%20our%20framework%20provides%20enhanced%20scene%20representations%20with%20improved%20geometric%20consistency%20and%20photorealistic%20appearance.%20We%20evaluate%20our%20framework%20extensively%20on%20multiple%20autonomous%20driving%20datasets%2C%20including%20Waymo%20Open%20Dataset%2C%20Argoverse%202%2C%20and%20PandaSet.%20Our%20framework%20consistently%20outperforms%20strong%20recent%20baselines%20and%20achieves%20state-of-the-art%20performance%20across%20all%20datasets.%20The%20source%20code%20is%20publicly%20available%20at%20%5Chref%7Bhttps%3A//github.com/whesense/XSIM%7D%7Bhttps%3A//github.com/whesense/XSIM%7D.&entry.1838667208=http%3A//arxiv.org/abs/2602.05617v1&entry.124074799=Read"},
{"title": "Exploring the Temporal Consistency for Point-Level Weakly-Supervised Temporal Action Localization", "author": "Yunchuan Ma and Laiyun Qing and Guorong Li and Yuqing Liu and Yuankai Qi and Qingming Huang", "abstract": "Point-supervised Temporal Action Localization (PTAL) adopts a lightly frame-annotated paradigm (\\textit{i.e.}, labeling only a single frame per action instance) to train a model to effectively locate action instances within untrimmed videos. Most existing approaches design the task head of models with only a point-supervised snippet-level classification, without explicit modeling of understanding temporal relationships among frames of an action. However, understanding the temporal relationships of frames is crucial because it can help a model understand how an action is defined and therefore benefits localizing the full frames of an action. To this end, in this paper, we design a multi-task learning framework that fully utilizes point supervision to boost the model's temporal understanding capability for action localization. Specifically, we design three self-supervised temporal understanding tasks: (i) Action Completion, (ii) Action Order Understanding, and (iii) Action Regularity Understanding. These tasks help a model understand the temporal consistency of actions across videos. To the best of our knowledge, this is the first attempt to explicitly explore temporal consistency for point supervision action localization. Extensive experimental results on four benchmark datasets demonstrate the effectiveness of the proposed method compared to several state-of-the-art approaches.", "link": "http://arxiv.org/abs/2602.05718v1", "date": "2026-02-05", "relevancy": 2.9751, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.665}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5787}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Temporal%20Consistency%20for%20Point-Level%20Weakly-Supervised%20Temporal%20Action%20Localization&body=Title%3A%20Exploring%20the%20Temporal%20Consistency%20for%20Point-Level%20Weakly-Supervised%20Temporal%20Action%20Localization%0AAuthor%3A%20Yunchuan%20Ma%20and%20Laiyun%20Qing%20and%20Guorong%20Li%20and%20Yuqing%20Liu%20and%20Yuankai%20Qi%20and%20Qingming%20Huang%0AAbstract%3A%20Point-supervised%20Temporal%20Action%20Localization%20%28PTAL%29%20adopts%20a%20lightly%20frame-annotated%20paradigm%20%28%5Ctextit%7Bi.e.%7D%2C%20labeling%20only%20a%20single%20frame%20per%20action%20instance%29%20to%20train%20a%20model%20to%20effectively%20locate%20action%20instances%20within%20untrimmed%20videos.%20Most%20existing%20approaches%20design%20the%20task%20head%20of%20models%20with%20only%20a%20point-supervised%20snippet-level%20classification%2C%20without%20explicit%20modeling%20of%20understanding%20temporal%20relationships%20among%20frames%20of%20an%20action.%20However%2C%20understanding%20the%20temporal%20relationships%20of%20frames%20is%20crucial%20because%20it%20can%20help%20a%20model%20understand%20how%20an%20action%20is%20defined%20and%20therefore%20benefits%20localizing%20the%20full%20frames%20of%20an%20action.%20To%20this%20end%2C%20in%20this%20paper%2C%20we%20design%20a%20multi-task%20learning%20framework%20that%20fully%20utilizes%20point%20supervision%20to%20boost%20the%20model%27s%20temporal%20understanding%20capability%20for%20action%20localization.%20Specifically%2C%20we%20design%20three%20self-supervised%20temporal%20understanding%20tasks%3A%20%28i%29%20Action%20Completion%2C%20%28ii%29%20Action%20Order%20Understanding%2C%20and%20%28iii%29%20Action%20Regularity%20Understanding.%20These%20tasks%20help%20a%20model%20understand%20the%20temporal%20consistency%20of%20actions%20across%20videos.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20attempt%20to%20explicitly%20explore%20temporal%20consistency%20for%20point%20supervision%20action%20localization.%20Extensive%20experimental%20results%20on%20four%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%20compared%20to%20several%20state-of-the-art%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Temporal%2520Consistency%2520for%2520Point-Level%2520Weakly-Supervised%2520Temporal%2520Action%2520Localization%26entry.906535625%3DYunchuan%2520Ma%2520and%2520Laiyun%2520Qing%2520and%2520Guorong%2520Li%2520and%2520Yuqing%2520Liu%2520and%2520Yuankai%2520Qi%2520and%2520Qingming%2520Huang%26entry.1292438233%3DPoint-supervised%2520Temporal%2520Action%2520Localization%2520%2528PTAL%2529%2520adopts%2520a%2520lightly%2520frame-annotated%2520paradigm%2520%2528%255Ctextit%257Bi.e.%257D%252C%2520labeling%2520only%2520a%2520single%2520frame%2520per%2520action%2520instance%2529%2520to%2520train%2520a%2520model%2520to%2520effectively%2520locate%2520action%2520instances%2520within%2520untrimmed%2520videos.%2520Most%2520existing%2520approaches%2520design%2520the%2520task%2520head%2520of%2520models%2520with%2520only%2520a%2520point-supervised%2520snippet-level%2520classification%252C%2520without%2520explicit%2520modeling%2520of%2520understanding%2520temporal%2520relationships%2520among%2520frames%2520of%2520an%2520action.%2520However%252C%2520understanding%2520the%2520temporal%2520relationships%2520of%2520frames%2520is%2520crucial%2520because%2520it%2520can%2520help%2520a%2520model%2520understand%2520how%2520an%2520action%2520is%2520defined%2520and%2520therefore%2520benefits%2520localizing%2520the%2520full%2520frames%2520of%2520an%2520action.%2520To%2520this%2520end%252C%2520in%2520this%2520paper%252C%2520we%2520design%2520a%2520multi-task%2520learning%2520framework%2520that%2520fully%2520utilizes%2520point%2520supervision%2520to%2520boost%2520the%2520model%2527s%2520temporal%2520understanding%2520capability%2520for%2520action%2520localization.%2520Specifically%252C%2520we%2520design%2520three%2520self-supervised%2520temporal%2520understanding%2520tasks%253A%2520%2528i%2529%2520Action%2520Completion%252C%2520%2528ii%2529%2520Action%2520Order%2520Understanding%252C%2520and%2520%2528iii%2529%2520Action%2520Regularity%2520Understanding.%2520These%2520tasks%2520help%2520a%2520model%2520understand%2520the%2520temporal%2520consistency%2520of%2520actions%2520across%2520videos.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520attempt%2520to%2520explicitly%2520explore%2520temporal%2520consistency%2520for%2520point%2520supervision%2520action%2520localization.%2520Extensive%2520experimental%2520results%2520on%2520four%2520benchmark%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%2520compared%2520to%2520several%2520state-of-the-art%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Temporal%20Consistency%20for%20Point-Level%20Weakly-Supervised%20Temporal%20Action%20Localization&entry.906535625=Yunchuan%20Ma%20and%20Laiyun%20Qing%20and%20Guorong%20Li%20and%20Yuqing%20Liu%20and%20Yuankai%20Qi%20and%20Qingming%20Huang&entry.1292438233=Point-supervised%20Temporal%20Action%20Localization%20%28PTAL%29%20adopts%20a%20lightly%20frame-annotated%20paradigm%20%28%5Ctextit%7Bi.e.%7D%2C%20labeling%20only%20a%20single%20frame%20per%20action%20instance%29%20to%20train%20a%20model%20to%20effectively%20locate%20action%20instances%20within%20untrimmed%20videos.%20Most%20existing%20approaches%20design%20the%20task%20head%20of%20models%20with%20only%20a%20point-supervised%20snippet-level%20classification%2C%20without%20explicit%20modeling%20of%20understanding%20temporal%20relationships%20among%20frames%20of%20an%20action.%20However%2C%20understanding%20the%20temporal%20relationships%20of%20frames%20is%20crucial%20because%20it%20can%20help%20a%20model%20understand%20how%20an%20action%20is%20defined%20and%20therefore%20benefits%20localizing%20the%20full%20frames%20of%20an%20action.%20To%20this%20end%2C%20in%20this%20paper%2C%20we%20design%20a%20multi-task%20learning%20framework%20that%20fully%20utilizes%20point%20supervision%20to%20boost%20the%20model%27s%20temporal%20understanding%20capability%20for%20action%20localization.%20Specifically%2C%20we%20design%20three%20self-supervised%20temporal%20understanding%20tasks%3A%20%28i%29%20Action%20Completion%2C%20%28ii%29%20Action%20Order%20Understanding%2C%20and%20%28iii%29%20Action%20Regularity%20Understanding.%20These%20tasks%20help%20a%20model%20understand%20the%20temporal%20consistency%20of%20actions%20across%20videos.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20attempt%20to%20explicitly%20explore%20temporal%20consistency%20for%20point%20supervision%20action%20localization.%20Extensive%20experimental%20results%20on%20four%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%20compared%20to%20several%20state-of-the-art%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2602.05718v1&entry.124074799=Read"},
{"title": "TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?", "author": "Yikun Zong and Cheston Tan", "abstract": "Humans excel at spatial reasoning tasks like Tangram puzzle assembly through cognitive processes involving mental rotation, iterative refinement, and visual feedback. Inspired by how humans solve Tangram puzzles through trial-and-error, observation, and correction, we design a framework that models these human cognitive mechanisms. However, comprehensive experiments across five representative Vision-Language Models (VLMs) reveal systematic failures in continuous geometric reasoning: average IoU of only 0.41 on single-piece tasks, dropping to 0.23 on two-piece composition, far below human performance where children can complete Tangram tasks successfully. This paper addresses a fundamental challenge in self-improving AI: can models iteratively refine their predictions at test time without parameter updates? We introduce a test-time self-refinement framework that combines in-context learning (ICL) with reward-guided feedback loops, inspired by human cognitive processes. Our training-free verifier-refiner agent applies recursive refinement loops that iteratively self-refine predictions based on geometric consistency feedback, achieving IoU improvements from 0.63 to 0.932 on medium-triangle cases without any model retraining. This demonstrates that incorporating human-inspired iterative refinement mechanisms through ICL and reward loops can substantially enhance geometric reasoning in VLMs, moving self-improving AI from promise to practice in continuous spatial domains. Our work is available at this anonymous link https://anonymous.4open.science/r/TangramVLM-F582/.", "link": "http://arxiv.org/abs/2602.05570v1", "date": "2026-02-05", "relevancy": 2.948, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5974}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5974}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TangramSR%3A%20Can%20Vision-Language%20Models%20Reason%20in%20Continuous%20Geometric%20Space%3F&body=Title%3A%20TangramSR%3A%20Can%20Vision-Language%20Models%20Reason%20in%20Continuous%20Geometric%20Space%3F%0AAuthor%3A%20Yikun%20Zong%20and%20Cheston%20Tan%0AAbstract%3A%20Humans%20excel%20at%20spatial%20reasoning%20tasks%20like%20Tangram%20puzzle%20assembly%20through%20cognitive%20processes%20involving%20mental%20rotation%2C%20iterative%20refinement%2C%20and%20visual%20feedback.%20Inspired%20by%20how%20humans%20solve%20Tangram%20puzzles%20through%20trial-and-error%2C%20observation%2C%20and%20correction%2C%20we%20design%20a%20framework%20that%20models%20these%20human%20cognitive%20mechanisms.%20However%2C%20comprehensive%20experiments%20across%20five%20representative%20Vision-Language%20Models%20%28VLMs%29%20reveal%20systematic%20failures%20in%20continuous%20geometric%20reasoning%3A%20average%20IoU%20of%20only%200.41%20on%20single-piece%20tasks%2C%20dropping%20to%200.23%20on%20two-piece%20composition%2C%20far%20below%20human%20performance%20where%20children%20can%20complete%20Tangram%20tasks%20successfully.%20This%20paper%20addresses%20a%20fundamental%20challenge%20in%20self-improving%20AI%3A%20can%20models%20iteratively%20refine%20their%20predictions%20at%20test%20time%20without%20parameter%20updates%3F%20We%20introduce%20a%20test-time%20self-refinement%20framework%20that%20combines%20in-context%20learning%20%28ICL%29%20with%20reward-guided%20feedback%20loops%2C%20inspired%20by%20human%20cognitive%20processes.%20Our%20training-free%20verifier-refiner%20agent%20applies%20recursive%20refinement%20loops%20that%20iteratively%20self-refine%20predictions%20based%20on%20geometric%20consistency%20feedback%2C%20achieving%20IoU%20improvements%20from%200.63%20to%200.932%20on%20medium-triangle%20cases%20without%20any%20model%20retraining.%20This%20demonstrates%20that%20incorporating%20human-inspired%20iterative%20refinement%20mechanisms%20through%20ICL%20and%20reward%20loops%20can%20substantially%20enhance%20geometric%20reasoning%20in%20VLMs%2C%20moving%20self-improving%20AI%20from%20promise%20to%20practice%20in%20continuous%20spatial%20domains.%20Our%20work%20is%20available%20at%20this%20anonymous%20link%20https%3A//anonymous.4open.science/r/TangramVLM-F582/.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05570v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTangramSR%253A%2520Can%2520Vision-Language%2520Models%2520Reason%2520in%2520Continuous%2520Geometric%2520Space%253F%26entry.906535625%3DYikun%2520Zong%2520and%2520Cheston%2520Tan%26entry.1292438233%3DHumans%2520excel%2520at%2520spatial%2520reasoning%2520tasks%2520like%2520Tangram%2520puzzle%2520assembly%2520through%2520cognitive%2520processes%2520involving%2520mental%2520rotation%252C%2520iterative%2520refinement%252C%2520and%2520visual%2520feedback.%2520Inspired%2520by%2520how%2520humans%2520solve%2520Tangram%2520puzzles%2520through%2520trial-and-error%252C%2520observation%252C%2520and%2520correction%252C%2520we%2520design%2520a%2520framework%2520that%2520models%2520these%2520human%2520cognitive%2520mechanisms.%2520However%252C%2520comprehensive%2520experiments%2520across%2520five%2520representative%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520reveal%2520systematic%2520failures%2520in%2520continuous%2520geometric%2520reasoning%253A%2520average%2520IoU%2520of%2520only%25200.41%2520on%2520single-piece%2520tasks%252C%2520dropping%2520to%25200.23%2520on%2520two-piece%2520composition%252C%2520far%2520below%2520human%2520performance%2520where%2520children%2520can%2520complete%2520Tangram%2520tasks%2520successfully.%2520This%2520paper%2520addresses%2520a%2520fundamental%2520challenge%2520in%2520self-improving%2520AI%253A%2520can%2520models%2520iteratively%2520refine%2520their%2520predictions%2520at%2520test%2520time%2520without%2520parameter%2520updates%253F%2520We%2520introduce%2520a%2520test-time%2520self-refinement%2520framework%2520that%2520combines%2520in-context%2520learning%2520%2528ICL%2529%2520with%2520reward-guided%2520feedback%2520loops%252C%2520inspired%2520by%2520human%2520cognitive%2520processes.%2520Our%2520training-free%2520verifier-refiner%2520agent%2520applies%2520recursive%2520refinement%2520loops%2520that%2520iteratively%2520self-refine%2520predictions%2520based%2520on%2520geometric%2520consistency%2520feedback%252C%2520achieving%2520IoU%2520improvements%2520from%25200.63%2520to%25200.932%2520on%2520medium-triangle%2520cases%2520without%2520any%2520model%2520retraining.%2520This%2520demonstrates%2520that%2520incorporating%2520human-inspired%2520iterative%2520refinement%2520mechanisms%2520through%2520ICL%2520and%2520reward%2520loops%2520can%2520substantially%2520enhance%2520geometric%2520reasoning%2520in%2520VLMs%252C%2520moving%2520self-improving%2520AI%2520from%2520promise%2520to%2520practice%2520in%2520continuous%2520spatial%2520domains.%2520Our%2520work%2520is%2520available%2520at%2520this%2520anonymous%2520link%2520https%253A//anonymous.4open.science/r/TangramVLM-F582/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05570v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TangramSR%3A%20Can%20Vision-Language%20Models%20Reason%20in%20Continuous%20Geometric%20Space%3F&entry.906535625=Yikun%20Zong%20and%20Cheston%20Tan&entry.1292438233=Humans%20excel%20at%20spatial%20reasoning%20tasks%20like%20Tangram%20puzzle%20assembly%20through%20cognitive%20processes%20involving%20mental%20rotation%2C%20iterative%20refinement%2C%20and%20visual%20feedback.%20Inspired%20by%20how%20humans%20solve%20Tangram%20puzzles%20through%20trial-and-error%2C%20observation%2C%20and%20correction%2C%20we%20design%20a%20framework%20that%20models%20these%20human%20cognitive%20mechanisms.%20However%2C%20comprehensive%20experiments%20across%20five%20representative%20Vision-Language%20Models%20%28VLMs%29%20reveal%20systematic%20failures%20in%20continuous%20geometric%20reasoning%3A%20average%20IoU%20of%20only%200.41%20on%20single-piece%20tasks%2C%20dropping%20to%200.23%20on%20two-piece%20composition%2C%20far%20below%20human%20performance%20where%20children%20can%20complete%20Tangram%20tasks%20successfully.%20This%20paper%20addresses%20a%20fundamental%20challenge%20in%20self-improving%20AI%3A%20can%20models%20iteratively%20refine%20their%20predictions%20at%20test%20time%20without%20parameter%20updates%3F%20We%20introduce%20a%20test-time%20self-refinement%20framework%20that%20combines%20in-context%20learning%20%28ICL%29%20with%20reward-guided%20feedback%20loops%2C%20inspired%20by%20human%20cognitive%20processes.%20Our%20training-free%20verifier-refiner%20agent%20applies%20recursive%20refinement%20loops%20that%20iteratively%20self-refine%20predictions%20based%20on%20geometric%20consistency%20feedback%2C%20achieving%20IoU%20improvements%20from%200.63%20to%200.932%20on%20medium-triangle%20cases%20without%20any%20model%20retraining.%20This%20demonstrates%20that%20incorporating%20human-inspired%20iterative%20refinement%20mechanisms%20through%20ICL%20and%20reward%20loops%20can%20substantially%20enhance%20geometric%20reasoning%20in%20VLMs%2C%20moving%20self-improving%20AI%20from%20promise%20to%20practice%20in%20continuous%20spatial%20domains.%20Our%20work%20is%20available%20at%20this%20anonymous%20link%20https%3A//anonymous.4open.science/r/TangramVLM-F582/.&entry.1838667208=http%3A//arxiv.org/abs/2602.05570v1&entry.124074799=Read"},
{"title": "Improved Bag-of-Words Image Retrieval with Geometric Constraints for Ground Texture Localization", "author": "Aaron Wilhelm and Nils Napp", "abstract": "Ground texture localization using a downward-facing camera offers a low-cost, high-precision localization solution that is robust to dynamic environments and requires no environmental modification. We present a significantly improved bag-of-words (BoW) image retrieval system for ground texture localization, achieving substantially higher accuracy for global localization and higher precision and recall for loop closure detection in SLAM. Our approach leverages an approximate $k$-means (AKM) vocabulary with soft assignment, and exploits the consistent orientation and constant scale constraints inherent to ground texture localization. Identifying the different needs of global localization vs. loop closure detection for SLAM, we present both high-accuracy and high-speed versions of our algorithm. We test the effect of each of our proposed improvements through an ablation study and demonstrate our method's effectiveness for both global localization and loop closure detection. With numerous ground texture localization systems already using BoW, our method can readily replace other generic BoW systems in their pipeline and immediately improve their results.", "link": "http://arxiv.org/abs/2505.11620v3", "date": "2026-02-05", "relevancy": 2.9314, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6044}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5982}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Bag-of-Words%20Image%20Retrieval%20with%20Geometric%20Constraints%20for%20Ground%20Texture%20Localization&body=Title%3A%20Improved%20Bag-of-Words%20Image%20Retrieval%20with%20Geometric%20Constraints%20for%20Ground%20Texture%20Localization%0AAuthor%3A%20Aaron%20Wilhelm%20and%20Nils%20Napp%0AAbstract%3A%20Ground%20texture%20localization%20using%20a%20downward-facing%20camera%20offers%20a%20low-cost%2C%20high-precision%20localization%20solution%20that%20is%20robust%20to%20dynamic%20environments%20and%20requires%20no%20environmental%20modification.%20We%20present%20a%20significantly%20improved%20bag-of-words%20%28BoW%29%20image%20retrieval%20system%20for%20ground%20texture%20localization%2C%20achieving%20substantially%20higher%20accuracy%20for%20global%20localization%20and%20higher%20precision%20and%20recall%20for%20loop%20closure%20detection%20in%20SLAM.%20Our%20approach%20leverages%20an%20approximate%20%24k%24-means%20%28AKM%29%20vocabulary%20with%20soft%20assignment%2C%20and%20exploits%20the%20consistent%20orientation%20and%20constant%20scale%20constraints%20inherent%20to%20ground%20texture%20localization.%20Identifying%20the%20different%20needs%20of%20global%20localization%20vs.%20loop%20closure%20detection%20for%20SLAM%2C%20we%20present%20both%20high-accuracy%20and%20high-speed%20versions%20of%20our%20algorithm.%20We%20test%20the%20effect%20of%20each%20of%20our%20proposed%20improvements%20through%20an%20ablation%20study%20and%20demonstrate%20our%20method%27s%20effectiveness%20for%20both%20global%20localization%20and%20loop%20closure%20detection.%20With%20numerous%20ground%20texture%20localization%20systems%20already%20using%20BoW%2C%20our%20method%20can%20readily%20replace%20other%20generic%20BoW%20systems%20in%20their%20pipeline%20and%20immediately%20improve%20their%20results.%0ALink%3A%20http%3A//arxiv.org/abs/2505.11620v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Bag-of-Words%2520Image%2520Retrieval%2520with%2520Geometric%2520Constraints%2520for%2520Ground%2520Texture%2520Localization%26entry.906535625%3DAaron%2520Wilhelm%2520and%2520Nils%2520Napp%26entry.1292438233%3DGround%2520texture%2520localization%2520using%2520a%2520downward-facing%2520camera%2520offers%2520a%2520low-cost%252C%2520high-precision%2520localization%2520solution%2520that%2520is%2520robust%2520to%2520dynamic%2520environments%2520and%2520requires%2520no%2520environmental%2520modification.%2520We%2520present%2520a%2520significantly%2520improved%2520bag-of-words%2520%2528BoW%2529%2520image%2520retrieval%2520system%2520for%2520ground%2520texture%2520localization%252C%2520achieving%2520substantially%2520higher%2520accuracy%2520for%2520global%2520localization%2520and%2520higher%2520precision%2520and%2520recall%2520for%2520loop%2520closure%2520detection%2520in%2520SLAM.%2520Our%2520approach%2520leverages%2520an%2520approximate%2520%2524k%2524-means%2520%2528AKM%2529%2520vocabulary%2520with%2520soft%2520assignment%252C%2520and%2520exploits%2520the%2520consistent%2520orientation%2520and%2520constant%2520scale%2520constraints%2520inherent%2520to%2520ground%2520texture%2520localization.%2520Identifying%2520the%2520different%2520needs%2520of%2520global%2520localization%2520vs.%2520loop%2520closure%2520detection%2520for%2520SLAM%252C%2520we%2520present%2520both%2520high-accuracy%2520and%2520high-speed%2520versions%2520of%2520our%2520algorithm.%2520We%2520test%2520the%2520effect%2520of%2520each%2520of%2520our%2520proposed%2520improvements%2520through%2520an%2520ablation%2520study%2520and%2520demonstrate%2520our%2520method%2527s%2520effectiveness%2520for%2520both%2520global%2520localization%2520and%2520loop%2520closure%2520detection.%2520With%2520numerous%2520ground%2520texture%2520localization%2520systems%2520already%2520using%2520BoW%252C%2520our%2520method%2520can%2520readily%2520replace%2520other%2520generic%2520BoW%2520systems%2520in%2520their%2520pipeline%2520and%2520immediately%2520improve%2520their%2520results.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11620v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Bag-of-Words%20Image%20Retrieval%20with%20Geometric%20Constraints%20for%20Ground%20Texture%20Localization&entry.906535625=Aaron%20Wilhelm%20and%20Nils%20Napp&entry.1292438233=Ground%20texture%20localization%20using%20a%20downward-facing%20camera%20offers%20a%20low-cost%2C%20high-precision%20localization%20solution%20that%20is%20robust%20to%20dynamic%20environments%20and%20requires%20no%20environmental%20modification.%20We%20present%20a%20significantly%20improved%20bag-of-words%20%28BoW%29%20image%20retrieval%20system%20for%20ground%20texture%20localization%2C%20achieving%20substantially%20higher%20accuracy%20for%20global%20localization%20and%20higher%20precision%20and%20recall%20for%20loop%20closure%20detection%20in%20SLAM.%20Our%20approach%20leverages%20an%20approximate%20%24k%24-means%20%28AKM%29%20vocabulary%20with%20soft%20assignment%2C%20and%20exploits%20the%20consistent%20orientation%20and%20constant%20scale%20constraints%20inherent%20to%20ground%20texture%20localization.%20Identifying%20the%20different%20needs%20of%20global%20localization%20vs.%20loop%20closure%20detection%20for%20SLAM%2C%20we%20present%20both%20high-accuracy%20and%20high-speed%20versions%20of%20our%20algorithm.%20We%20test%20the%20effect%20of%20each%20of%20our%20proposed%20improvements%20through%20an%20ablation%20study%20and%20demonstrate%20our%20method%27s%20effectiveness%20for%20both%20global%20localization%20and%20loop%20closure%20detection.%20With%20numerous%20ground%20texture%20localization%20systems%20already%20using%20BoW%2C%20our%20method%20can%20readily%20replace%20other%20generic%20BoW%20systems%20in%20their%20pipeline%20and%20immediately%20improve%20their%20results.&entry.1838667208=http%3A//arxiv.org/abs/2505.11620v3&entry.124074799=Read"},
{"title": "A Comparative Study of 3D Person Detection: Sensor Modalities and Robustness in Diverse Indoor and Outdoor Environments", "author": "Malaz Tamim and Andrea Matic-Flierl and Karsten Roscher", "abstract": "Accurate 3D person detection is critical for safety in applications such as robotics, industrial monitoring, and surveillance. This work presents a systematic evaluation of 3D person detection using camera-only, LiDAR-only, and camera-LiDAR fusion. While most existing research focuses on autonomous driving, we explore detection performance and robustness in diverse indoor and outdoor scenes using the JRDB dataset. We compare three representative models - BEVDepth (camera), PointPillars (LiDAR), and DAL (camera-LiDAR fusion) - and analyze their behavior under varying occlusion and distance levels. Our results show that the fusion-based approach consistently outperforms single-modality models, particularly in challenging scenarios. We further investigate robustness against sensor corruptions and misalignments, revealing that while DAL offers improved resilience, it remains sensitive to sensor misalignment and certain LiDAR-based corruptions. In contrast, the camera-based BEVDepth model showed the lowest performance and was most affected by occlusion, distance, and noise. Our findings highlight the importance of utilizing sensor fusion for enhanced 3D person detection, while also underscoring the need for ongoing research to address the vulnerabilities inherent in these systems.", "link": "http://arxiv.org/abs/2602.05538v1", "date": "2026-02-05", "relevancy": 2.9263, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.586}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.586}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comparative%20Study%20of%203D%20Person%20Detection%3A%20Sensor%20Modalities%20and%20Robustness%20in%20Diverse%20Indoor%20and%20Outdoor%20Environments&body=Title%3A%20A%20Comparative%20Study%20of%203D%20Person%20Detection%3A%20Sensor%20Modalities%20and%20Robustness%20in%20Diverse%20Indoor%20and%20Outdoor%20Environments%0AAuthor%3A%20Malaz%20Tamim%20and%20Andrea%20Matic-Flierl%20and%20Karsten%20Roscher%0AAbstract%3A%20Accurate%203D%20person%20detection%20is%20critical%20for%20safety%20in%20applications%20such%20as%20robotics%2C%20industrial%20monitoring%2C%20and%20surveillance.%20This%20work%20presents%20a%20systematic%20evaluation%20of%203D%20person%20detection%20using%20camera-only%2C%20LiDAR-only%2C%20and%20camera-LiDAR%20fusion.%20While%20most%20existing%20research%20focuses%20on%20autonomous%20driving%2C%20we%20explore%20detection%20performance%20and%20robustness%20in%20diverse%20indoor%20and%20outdoor%20scenes%20using%20the%20JRDB%20dataset.%20We%20compare%20three%20representative%20models%20-%20BEVDepth%20%28camera%29%2C%20PointPillars%20%28LiDAR%29%2C%20and%20DAL%20%28camera-LiDAR%20fusion%29%20-%20and%20analyze%20their%20behavior%20under%20varying%20occlusion%20and%20distance%20levels.%20Our%20results%20show%20that%20the%20fusion-based%20approach%20consistently%20outperforms%20single-modality%20models%2C%20particularly%20in%20challenging%20scenarios.%20We%20further%20investigate%20robustness%20against%20sensor%20corruptions%20and%20misalignments%2C%20revealing%20that%20while%20DAL%20offers%20improved%20resilience%2C%20it%20remains%20sensitive%20to%20sensor%20misalignment%20and%20certain%20LiDAR-based%20corruptions.%20In%20contrast%2C%20the%20camera-based%20BEVDepth%20model%20showed%20the%20lowest%20performance%20and%20was%20most%20affected%20by%20occlusion%2C%20distance%2C%20and%20noise.%20Our%20findings%20highlight%20the%20importance%20of%20utilizing%20sensor%20fusion%20for%20enhanced%203D%20person%20detection%2C%20while%20also%20underscoring%20the%20need%20for%20ongoing%20research%20to%20address%20the%20vulnerabilities%20inherent%20in%20these%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comparative%2520Study%2520of%25203D%2520Person%2520Detection%253A%2520Sensor%2520Modalities%2520and%2520Robustness%2520in%2520Diverse%2520Indoor%2520and%2520Outdoor%2520Environments%26entry.906535625%3DMalaz%2520Tamim%2520and%2520Andrea%2520Matic-Flierl%2520and%2520Karsten%2520Roscher%26entry.1292438233%3DAccurate%25203D%2520person%2520detection%2520is%2520critical%2520for%2520safety%2520in%2520applications%2520such%2520as%2520robotics%252C%2520industrial%2520monitoring%252C%2520and%2520surveillance.%2520This%2520work%2520presents%2520a%2520systematic%2520evaluation%2520of%25203D%2520person%2520detection%2520using%2520camera-only%252C%2520LiDAR-only%252C%2520and%2520camera-LiDAR%2520fusion.%2520While%2520most%2520existing%2520research%2520focuses%2520on%2520autonomous%2520driving%252C%2520we%2520explore%2520detection%2520performance%2520and%2520robustness%2520in%2520diverse%2520indoor%2520and%2520outdoor%2520scenes%2520using%2520the%2520JRDB%2520dataset.%2520We%2520compare%2520three%2520representative%2520models%2520-%2520BEVDepth%2520%2528camera%2529%252C%2520PointPillars%2520%2528LiDAR%2529%252C%2520and%2520DAL%2520%2528camera-LiDAR%2520fusion%2529%2520-%2520and%2520analyze%2520their%2520behavior%2520under%2520varying%2520occlusion%2520and%2520distance%2520levels.%2520Our%2520results%2520show%2520that%2520the%2520fusion-based%2520approach%2520consistently%2520outperforms%2520single-modality%2520models%252C%2520particularly%2520in%2520challenging%2520scenarios.%2520We%2520further%2520investigate%2520robustness%2520against%2520sensor%2520corruptions%2520and%2520misalignments%252C%2520revealing%2520that%2520while%2520DAL%2520offers%2520improved%2520resilience%252C%2520it%2520remains%2520sensitive%2520to%2520sensor%2520misalignment%2520and%2520certain%2520LiDAR-based%2520corruptions.%2520In%2520contrast%252C%2520the%2520camera-based%2520BEVDepth%2520model%2520showed%2520the%2520lowest%2520performance%2520and%2520was%2520most%2520affected%2520by%2520occlusion%252C%2520distance%252C%2520and%2520noise.%2520Our%2520findings%2520highlight%2520the%2520importance%2520of%2520utilizing%2520sensor%2520fusion%2520for%2520enhanced%25203D%2520person%2520detection%252C%2520while%2520also%2520underscoring%2520the%2520need%2520for%2520ongoing%2520research%2520to%2520address%2520the%2520vulnerabilities%2520inherent%2520in%2520these%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comparative%20Study%20of%203D%20Person%20Detection%3A%20Sensor%20Modalities%20and%20Robustness%20in%20Diverse%20Indoor%20and%20Outdoor%20Environments&entry.906535625=Malaz%20Tamim%20and%20Andrea%20Matic-Flierl%20and%20Karsten%20Roscher&entry.1292438233=Accurate%203D%20person%20detection%20is%20critical%20for%20safety%20in%20applications%20such%20as%20robotics%2C%20industrial%20monitoring%2C%20and%20surveillance.%20This%20work%20presents%20a%20systematic%20evaluation%20of%203D%20person%20detection%20using%20camera-only%2C%20LiDAR-only%2C%20and%20camera-LiDAR%20fusion.%20While%20most%20existing%20research%20focuses%20on%20autonomous%20driving%2C%20we%20explore%20detection%20performance%20and%20robustness%20in%20diverse%20indoor%20and%20outdoor%20scenes%20using%20the%20JRDB%20dataset.%20We%20compare%20three%20representative%20models%20-%20BEVDepth%20%28camera%29%2C%20PointPillars%20%28LiDAR%29%2C%20and%20DAL%20%28camera-LiDAR%20fusion%29%20-%20and%20analyze%20their%20behavior%20under%20varying%20occlusion%20and%20distance%20levels.%20Our%20results%20show%20that%20the%20fusion-based%20approach%20consistently%20outperforms%20single-modality%20models%2C%20particularly%20in%20challenging%20scenarios.%20We%20further%20investigate%20robustness%20against%20sensor%20corruptions%20and%20misalignments%2C%20revealing%20that%20while%20DAL%20offers%20improved%20resilience%2C%20it%20remains%20sensitive%20to%20sensor%20misalignment%20and%20certain%20LiDAR-based%20corruptions.%20In%20contrast%2C%20the%20camera-based%20BEVDepth%20model%20showed%20the%20lowest%20performance%20and%20was%20most%20affected%20by%20occlusion%2C%20distance%2C%20and%20noise.%20Our%20findings%20highlight%20the%20importance%20of%20utilizing%20sensor%20fusion%20for%20enhanced%203D%20person%20detection%2C%20while%20also%20underscoring%20the%20need%20for%20ongoing%20research%20to%20address%20the%20vulnerabilities%20inherent%20in%20these%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2602.05538v1&entry.124074799=Read"},
{"title": "Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation", "author": "Hengyi Wang and Ruiqiang Zhang and Chang Liu and Guanjie Wang and Zehua Ma and Han Fang and Weiming Zhang", "abstract": "With the rising need for spatially grounded tasks such as Vision-Language Navigation/Action, allocentric perception capabilities in Vision-Language Models (VLMs) are receiving growing focus. However, VLMs remain brittle on allocentric spatial queries that require explicit perspective shifts, where the answer depends on reasoning in a target-centric frame rather than the observed camera view. Thus, we introduce Allocentric Perceiver, a training-free strategy that recovers metric 3D states from one or more images with off-the-shelf geometric experts, and then instantiates a query-conditioned allocentric reference frame aligned with the instruction's semantic intent. By deterministically transforming reconstructed geometry into the target frame and prompting the backbone VLM with structured, geometry-grounded representations, Allocentric Perceriver offloads mental rotation from implicit reasoning to explicit computation. We evaluate Allocentric Perciver across multiple backbone families on spatial reasoning benchmarks, observing consistent and substantial gains ($\\sim$10%) on allocentric tasks while maintaining strong egocentric performance, and surpassing both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.", "link": "http://arxiv.org/abs/2602.05789v1", "date": "2026-02-05", "relevancy": 2.9243, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6082}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6082}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Allocentric%20Perceiver%3A%20Disentangling%20Allocentric%20Reasoning%20from%20Egocentric%20Visual%20Priors%20via%20Frame%20Instantiation&body=Title%3A%20Allocentric%20Perceiver%3A%20Disentangling%20Allocentric%20Reasoning%20from%20Egocentric%20Visual%20Priors%20via%20Frame%20Instantiation%0AAuthor%3A%20Hengyi%20Wang%20and%20Ruiqiang%20Zhang%20and%20Chang%20Liu%20and%20Guanjie%20Wang%20and%20Zehua%20Ma%20and%20Han%20Fang%20and%20Weiming%20Zhang%0AAbstract%3A%20With%20the%20rising%20need%20for%20spatially%20grounded%20tasks%20such%20as%20Vision-Language%20Navigation/Action%2C%20allocentric%20perception%20capabilities%20in%20Vision-Language%20Models%20%28VLMs%29%20are%20receiving%20growing%20focus.%20However%2C%20VLMs%20remain%20brittle%20on%20allocentric%20spatial%20queries%20that%20require%20explicit%20perspective%20shifts%2C%20where%20the%20answer%20depends%20on%20reasoning%20in%20a%20target-centric%20frame%20rather%20than%20the%20observed%20camera%20view.%20Thus%2C%20we%20introduce%20Allocentric%20Perceiver%2C%20a%20training-free%20strategy%20that%20recovers%20metric%203D%20states%20from%20one%20or%20more%20images%20with%20off-the-shelf%20geometric%20experts%2C%20and%20then%20instantiates%20a%20query-conditioned%20allocentric%20reference%20frame%20aligned%20with%20the%20instruction%27s%20semantic%20intent.%20By%20deterministically%20transforming%20reconstructed%20geometry%20into%20the%20target%20frame%20and%20prompting%20the%20backbone%20VLM%20with%20structured%2C%20geometry-grounded%20representations%2C%20Allocentric%20Perceriver%20offloads%20mental%20rotation%20from%20implicit%20reasoning%20to%20explicit%20computation.%20We%20evaluate%20Allocentric%20Perciver%20across%20multiple%20backbone%20families%20on%20spatial%20reasoning%20benchmarks%2C%20observing%20consistent%20and%20substantial%20gains%20%28%24%5Csim%2410%25%29%20on%20allocentric%20tasks%20while%20maintaining%20strong%20egocentric%20performance%2C%20and%20surpassing%20both%20spatial-perception-finetuned%20models%20and%20state-of-the-art%20open-source%20and%20proprietary%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAllocentric%2520Perceiver%253A%2520Disentangling%2520Allocentric%2520Reasoning%2520from%2520Egocentric%2520Visual%2520Priors%2520via%2520Frame%2520Instantiation%26entry.906535625%3DHengyi%2520Wang%2520and%2520Ruiqiang%2520Zhang%2520and%2520Chang%2520Liu%2520and%2520Guanjie%2520Wang%2520and%2520Zehua%2520Ma%2520and%2520Han%2520Fang%2520and%2520Weiming%2520Zhang%26entry.1292438233%3DWith%2520the%2520rising%2520need%2520for%2520spatially%2520grounded%2520tasks%2520such%2520as%2520Vision-Language%2520Navigation/Action%252C%2520allocentric%2520perception%2520capabilities%2520in%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520are%2520receiving%2520growing%2520focus.%2520However%252C%2520VLMs%2520remain%2520brittle%2520on%2520allocentric%2520spatial%2520queries%2520that%2520require%2520explicit%2520perspective%2520shifts%252C%2520where%2520the%2520answer%2520depends%2520on%2520reasoning%2520in%2520a%2520target-centric%2520frame%2520rather%2520than%2520the%2520observed%2520camera%2520view.%2520Thus%252C%2520we%2520introduce%2520Allocentric%2520Perceiver%252C%2520a%2520training-free%2520strategy%2520that%2520recovers%2520metric%25203D%2520states%2520from%2520one%2520or%2520more%2520images%2520with%2520off-the-shelf%2520geometric%2520experts%252C%2520and%2520then%2520instantiates%2520a%2520query-conditioned%2520allocentric%2520reference%2520frame%2520aligned%2520with%2520the%2520instruction%2527s%2520semantic%2520intent.%2520By%2520deterministically%2520transforming%2520reconstructed%2520geometry%2520into%2520the%2520target%2520frame%2520and%2520prompting%2520the%2520backbone%2520VLM%2520with%2520structured%252C%2520geometry-grounded%2520representations%252C%2520Allocentric%2520Perceriver%2520offloads%2520mental%2520rotation%2520from%2520implicit%2520reasoning%2520to%2520explicit%2520computation.%2520We%2520evaluate%2520Allocentric%2520Perciver%2520across%2520multiple%2520backbone%2520families%2520on%2520spatial%2520reasoning%2520benchmarks%252C%2520observing%2520consistent%2520and%2520substantial%2520gains%2520%2528%2524%255Csim%252410%2525%2529%2520on%2520allocentric%2520tasks%2520while%2520maintaining%2520strong%2520egocentric%2520performance%252C%2520and%2520surpassing%2520both%2520spatial-perception-finetuned%2520models%2520and%2520state-of-the-art%2520open-source%2520and%2520proprietary%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Allocentric%20Perceiver%3A%20Disentangling%20Allocentric%20Reasoning%20from%20Egocentric%20Visual%20Priors%20via%20Frame%20Instantiation&entry.906535625=Hengyi%20Wang%20and%20Ruiqiang%20Zhang%20and%20Chang%20Liu%20and%20Guanjie%20Wang%20and%20Zehua%20Ma%20and%20Han%20Fang%20and%20Weiming%20Zhang&entry.1292438233=With%20the%20rising%20need%20for%20spatially%20grounded%20tasks%20such%20as%20Vision-Language%20Navigation/Action%2C%20allocentric%20perception%20capabilities%20in%20Vision-Language%20Models%20%28VLMs%29%20are%20receiving%20growing%20focus.%20However%2C%20VLMs%20remain%20brittle%20on%20allocentric%20spatial%20queries%20that%20require%20explicit%20perspective%20shifts%2C%20where%20the%20answer%20depends%20on%20reasoning%20in%20a%20target-centric%20frame%20rather%20than%20the%20observed%20camera%20view.%20Thus%2C%20we%20introduce%20Allocentric%20Perceiver%2C%20a%20training-free%20strategy%20that%20recovers%20metric%203D%20states%20from%20one%20or%20more%20images%20with%20off-the-shelf%20geometric%20experts%2C%20and%20then%20instantiates%20a%20query-conditioned%20allocentric%20reference%20frame%20aligned%20with%20the%20instruction%27s%20semantic%20intent.%20By%20deterministically%20transforming%20reconstructed%20geometry%20into%20the%20target%20frame%20and%20prompting%20the%20backbone%20VLM%20with%20structured%2C%20geometry-grounded%20representations%2C%20Allocentric%20Perceriver%20offloads%20mental%20rotation%20from%20implicit%20reasoning%20to%20explicit%20computation.%20We%20evaluate%20Allocentric%20Perciver%20across%20multiple%20backbone%20families%20on%20spatial%20reasoning%20benchmarks%2C%20observing%20consistent%20and%20substantial%20gains%20%28%24%5Csim%2410%25%29%20on%20allocentric%20tasks%20while%20maintaining%20strong%20egocentric%20performance%2C%20and%20surpassing%20both%20spatial-perception-finetuned%20models%20and%20state-of-the-art%20open-source%20and%20proprietary%20models.&entry.1838667208=http%3A//arxiv.org/abs/2602.05789v1&entry.124074799=Read"},
{"title": "Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning", "author": "Enwei Tong and Yuanchao Bai and Yao Zhu and Junjun Jiang and Xianming Liu", "abstract": "Vision-language models (VLMs) often generate massive visual tokens that greatly increase inference latency and memory footprint; while training-free token pruning offers a practical remedy, existing methods still struggle to balance local evidence and global context under aggressive compression. We propose Focus-Scan-Refine (FSR), a human-inspired, plug-and-play pruning framework that mimics how humans answer visual questions: focus on key evidence, then scan globally if needed, and refine the scanned context by aggregating relevant details. FSR first focuses on key evidence by combining visual importance with instruction relevance, avoiding the bias toward visually salient but query-irrelevant regions. It then scans for complementary context conditioned on the focused set, selecting tokens that are most different from the focused evidence. Finally, FSR refines the scanned context by aggregating nearby informative tokens into the scan anchors via similarity-based assignment and score-weighted merging, without increasing the token budget. Extensive experiments across multiple VLM backbones and vision-language benchmarks show that FSR consistently improves the accuracy-efficiency trade-off over existing state-of-the-art pruning methods. The source codes can be found at https://github.com/ILOT-code/FSR", "link": "http://arxiv.org/abs/2602.05809v1", "date": "2026-02-05", "relevancy": 2.9179, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5959}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5959}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Focus-Scan-Refine%3A%20From%20Human%20Visual%20Perception%20to%20Efficient%20Visual%20Token%20Pruning&body=Title%3A%20Focus-Scan-Refine%3A%20From%20Human%20Visual%20Perception%20to%20Efficient%20Visual%20Token%20Pruning%0AAuthor%3A%20Enwei%20Tong%20and%20Yuanchao%20Bai%20and%20Yao%20Zhu%20and%20Junjun%20Jiang%20and%20Xianming%20Liu%0AAbstract%3A%20Vision-language%20models%20%28VLMs%29%20often%20generate%20massive%20visual%20tokens%20that%20greatly%20increase%20inference%20latency%20and%20memory%20footprint%3B%20while%20training-free%20token%20pruning%20offers%20a%20practical%20remedy%2C%20existing%20methods%20still%20struggle%20to%20balance%20local%20evidence%20and%20global%20context%20under%20aggressive%20compression.%20We%20propose%20Focus-Scan-Refine%20%28FSR%29%2C%20a%20human-inspired%2C%20plug-and-play%20pruning%20framework%20that%20mimics%20how%20humans%20answer%20visual%20questions%3A%20focus%20on%20key%20evidence%2C%20then%20scan%20globally%20if%20needed%2C%20and%20refine%20the%20scanned%20context%20by%20aggregating%20relevant%20details.%20FSR%20first%20focuses%20on%20key%20evidence%20by%20combining%20visual%20importance%20with%20instruction%20relevance%2C%20avoiding%20the%20bias%20toward%20visually%20salient%20but%20query-irrelevant%20regions.%20It%20then%20scans%20for%20complementary%20context%20conditioned%20on%20the%20focused%20set%2C%20selecting%20tokens%20that%20are%20most%20different%20from%20the%20focused%20evidence.%20Finally%2C%20FSR%20refines%20the%20scanned%20context%20by%20aggregating%20nearby%20informative%20tokens%20into%20the%20scan%20anchors%20via%20similarity-based%20assignment%20and%20score-weighted%20merging%2C%20without%20increasing%20the%20token%20budget.%20Extensive%20experiments%20across%20multiple%20VLM%20backbones%20and%20vision-language%20benchmarks%20show%20that%20FSR%20consistently%20improves%20the%20accuracy-efficiency%20trade-off%20over%20existing%20state-of-the-art%20pruning%20methods.%20The%20source%20codes%20can%20be%20found%20at%20https%3A//github.com/ILOT-code/FSR%0ALink%3A%20http%3A//arxiv.org/abs/2602.05809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFocus-Scan-Refine%253A%2520From%2520Human%2520Visual%2520Perception%2520to%2520Efficient%2520Visual%2520Token%2520Pruning%26entry.906535625%3DEnwei%2520Tong%2520and%2520Yuanchao%2520Bai%2520and%2520Yao%2520Zhu%2520and%2520Junjun%2520Jiang%2520and%2520Xianming%2520Liu%26entry.1292438233%3DVision-language%2520models%2520%2528VLMs%2529%2520often%2520generate%2520massive%2520visual%2520tokens%2520that%2520greatly%2520increase%2520inference%2520latency%2520and%2520memory%2520footprint%253B%2520while%2520training-free%2520token%2520pruning%2520offers%2520a%2520practical%2520remedy%252C%2520existing%2520methods%2520still%2520struggle%2520to%2520balance%2520local%2520evidence%2520and%2520global%2520context%2520under%2520aggressive%2520compression.%2520We%2520propose%2520Focus-Scan-Refine%2520%2528FSR%2529%252C%2520a%2520human-inspired%252C%2520plug-and-play%2520pruning%2520framework%2520that%2520mimics%2520how%2520humans%2520answer%2520visual%2520questions%253A%2520focus%2520on%2520key%2520evidence%252C%2520then%2520scan%2520globally%2520if%2520needed%252C%2520and%2520refine%2520the%2520scanned%2520context%2520by%2520aggregating%2520relevant%2520details.%2520FSR%2520first%2520focuses%2520on%2520key%2520evidence%2520by%2520combining%2520visual%2520importance%2520with%2520instruction%2520relevance%252C%2520avoiding%2520the%2520bias%2520toward%2520visually%2520salient%2520but%2520query-irrelevant%2520regions.%2520It%2520then%2520scans%2520for%2520complementary%2520context%2520conditioned%2520on%2520the%2520focused%2520set%252C%2520selecting%2520tokens%2520that%2520are%2520most%2520different%2520from%2520the%2520focused%2520evidence.%2520Finally%252C%2520FSR%2520refines%2520the%2520scanned%2520context%2520by%2520aggregating%2520nearby%2520informative%2520tokens%2520into%2520the%2520scan%2520anchors%2520via%2520similarity-based%2520assignment%2520and%2520score-weighted%2520merging%252C%2520without%2520increasing%2520the%2520token%2520budget.%2520Extensive%2520experiments%2520across%2520multiple%2520VLM%2520backbones%2520and%2520vision-language%2520benchmarks%2520show%2520that%2520FSR%2520consistently%2520improves%2520the%2520accuracy-efficiency%2520trade-off%2520over%2520existing%2520state-of-the-art%2520pruning%2520methods.%2520The%2520source%2520codes%2520can%2520be%2520found%2520at%2520https%253A//github.com/ILOT-code/FSR%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Focus-Scan-Refine%3A%20From%20Human%20Visual%20Perception%20to%20Efficient%20Visual%20Token%20Pruning&entry.906535625=Enwei%20Tong%20and%20Yuanchao%20Bai%20and%20Yao%20Zhu%20and%20Junjun%20Jiang%20and%20Xianming%20Liu&entry.1292438233=Vision-language%20models%20%28VLMs%29%20often%20generate%20massive%20visual%20tokens%20that%20greatly%20increase%20inference%20latency%20and%20memory%20footprint%3B%20while%20training-free%20token%20pruning%20offers%20a%20practical%20remedy%2C%20existing%20methods%20still%20struggle%20to%20balance%20local%20evidence%20and%20global%20context%20under%20aggressive%20compression.%20We%20propose%20Focus-Scan-Refine%20%28FSR%29%2C%20a%20human-inspired%2C%20plug-and-play%20pruning%20framework%20that%20mimics%20how%20humans%20answer%20visual%20questions%3A%20focus%20on%20key%20evidence%2C%20then%20scan%20globally%20if%20needed%2C%20and%20refine%20the%20scanned%20context%20by%20aggregating%20relevant%20details.%20FSR%20first%20focuses%20on%20key%20evidence%20by%20combining%20visual%20importance%20with%20instruction%20relevance%2C%20avoiding%20the%20bias%20toward%20visually%20salient%20but%20query-irrelevant%20regions.%20It%20then%20scans%20for%20complementary%20context%20conditioned%20on%20the%20focused%20set%2C%20selecting%20tokens%20that%20are%20most%20different%20from%20the%20focused%20evidence.%20Finally%2C%20FSR%20refines%20the%20scanned%20context%20by%20aggregating%20nearby%20informative%20tokens%20into%20the%20scan%20anchors%20via%20similarity-based%20assignment%20and%20score-weighted%20merging%2C%20without%20increasing%20the%20token%20budget.%20Extensive%20experiments%20across%20multiple%20VLM%20backbones%20and%20vision-language%20benchmarks%20show%20that%20FSR%20consistently%20improves%20the%20accuracy-efficiency%20trade-off%20over%20existing%20state-of-the-art%20pruning%20methods.%20The%20source%20codes%20can%20be%20found%20at%20https%3A//github.com/ILOT-code/FSR&entry.1838667208=http%3A//arxiv.org/abs/2602.05809v1&entry.124074799=Read"},
{"title": "GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?", "author": "Ruihang Li and Leigang Qu and Jingxu Zhang and Dongnan Gui and Mengde Xu and Xiaosong Zhang and Han Hu and Wenjie Wang and Jiaqi Wang", "abstract": "The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.", "link": "http://arxiv.org/abs/2602.06013v1", "date": "2026-02-05", "relevancy": 2.8806, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6216}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5653}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenArena%3A%20How%20Can%20We%20Achieve%20Human-Aligned%20Evaluation%20for%20Visual%20Generation%20Tasks%3F&body=Title%3A%20GenArena%3A%20How%20Can%20We%20Achieve%20Human-Aligned%20Evaluation%20for%20Visual%20Generation%20Tasks%3F%0AAuthor%3A%20Ruihang%20Li%20and%20Leigang%20Qu%20and%20Jingxu%20Zhang%20and%20Dongnan%20Gui%20and%20Mengde%20Xu%20and%20Xiaosong%20Zhang%20and%20Han%20Hu%20and%20Wenjie%20Wang%20and%20Jiaqi%20Wang%0AAbstract%3A%20The%20rapid%20advancement%20of%20visual%20generation%20models%20has%20outpaced%20traditional%20evaluation%20approaches%2C%20necessitating%20the%20adoption%20of%20Vision-Language%20Models%20as%20surrogate%20judges.%20In%20this%20work%2C%20we%20systematically%20investigate%20the%20reliability%20of%20the%20prevailing%20absolute%20pointwise%20scoring%20standard%2C%20across%20a%20wide%20spectrum%20of%20visual%20generation%20tasks.%20Our%20analysis%20reveals%20that%20this%20paradigm%20is%20limited%20due%20to%20stochastic%20inconsistency%20and%20poor%20alignment%20with%20human%20perception.%20To%20resolve%20these%20limitations%2C%20we%20introduce%20GenArena%2C%20a%20unified%20evaluation%20framework%20that%20leverages%20a%20pairwise%20comparison%20paradigm%20to%20ensure%20stable%20and%20human-aligned%20evaluation.%20Crucially%2C%20our%20experiments%20uncover%20a%20transformative%20finding%20that%20simply%20adopting%20this%20pairwise%20protocol%20enables%20off-the-shelf%20open-source%20models%20to%20outperform%20top-tier%20proprietary%20models.%20Notably%2C%20our%20method%20boosts%20evaluation%20accuracy%20by%20over%2020%25%20and%20achieves%20a%20Spearman%20correlation%20of%200.86%20with%20the%20authoritative%20LMArena%20leaderboard%2C%20drastically%20surpassing%20the%200.36%20correlation%20of%20pointwise%20methods.%20Based%20on%20GenArena%2C%20we%20benchmark%20state-of-the-art%20visual%20generation%20models%20across%20diverse%20tasks%2C%20providing%20the%20community%20with%20a%20rigorous%20and%20automated%20evaluation%20standard%20for%20visual%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.06013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenArena%253A%2520How%2520Can%2520We%2520Achieve%2520Human-Aligned%2520Evaluation%2520for%2520Visual%2520Generation%2520Tasks%253F%26entry.906535625%3DRuihang%2520Li%2520and%2520Leigang%2520Qu%2520and%2520Jingxu%2520Zhang%2520and%2520Dongnan%2520Gui%2520and%2520Mengde%2520Xu%2520and%2520Xiaosong%2520Zhang%2520and%2520Han%2520Hu%2520and%2520Wenjie%2520Wang%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3DThe%2520rapid%2520advancement%2520of%2520visual%2520generation%2520models%2520has%2520outpaced%2520traditional%2520evaluation%2520approaches%252C%2520necessitating%2520the%2520adoption%2520of%2520Vision-Language%2520Models%2520as%2520surrogate%2520judges.%2520In%2520this%2520work%252C%2520we%2520systematically%2520investigate%2520the%2520reliability%2520of%2520the%2520prevailing%2520absolute%2520pointwise%2520scoring%2520standard%252C%2520across%2520a%2520wide%2520spectrum%2520of%2520visual%2520generation%2520tasks.%2520Our%2520analysis%2520reveals%2520that%2520this%2520paradigm%2520is%2520limited%2520due%2520to%2520stochastic%2520inconsistency%2520and%2520poor%2520alignment%2520with%2520human%2520perception.%2520To%2520resolve%2520these%2520limitations%252C%2520we%2520introduce%2520GenArena%252C%2520a%2520unified%2520evaluation%2520framework%2520that%2520leverages%2520a%2520pairwise%2520comparison%2520paradigm%2520to%2520ensure%2520stable%2520and%2520human-aligned%2520evaluation.%2520Crucially%252C%2520our%2520experiments%2520uncover%2520a%2520transformative%2520finding%2520that%2520simply%2520adopting%2520this%2520pairwise%2520protocol%2520enables%2520off-the-shelf%2520open-source%2520models%2520to%2520outperform%2520top-tier%2520proprietary%2520models.%2520Notably%252C%2520our%2520method%2520boosts%2520evaluation%2520accuracy%2520by%2520over%252020%2525%2520and%2520achieves%2520a%2520Spearman%2520correlation%2520of%25200.86%2520with%2520the%2520authoritative%2520LMArena%2520leaderboard%252C%2520drastically%2520surpassing%2520the%25200.36%2520correlation%2520of%2520pointwise%2520methods.%2520Based%2520on%2520GenArena%252C%2520we%2520benchmark%2520state-of-the-art%2520visual%2520generation%2520models%2520across%2520diverse%2520tasks%252C%2520providing%2520the%2520community%2520with%2520a%2520rigorous%2520and%2520automated%2520evaluation%2520standard%2520for%2520visual%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.06013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenArena%3A%20How%20Can%20We%20Achieve%20Human-Aligned%20Evaluation%20for%20Visual%20Generation%20Tasks%3F&entry.906535625=Ruihang%20Li%20and%20Leigang%20Qu%20and%20Jingxu%20Zhang%20and%20Dongnan%20Gui%20and%20Mengde%20Xu%20and%20Xiaosong%20Zhang%20and%20Han%20Hu%20and%20Wenjie%20Wang%20and%20Jiaqi%20Wang&entry.1292438233=The%20rapid%20advancement%20of%20visual%20generation%20models%20has%20outpaced%20traditional%20evaluation%20approaches%2C%20necessitating%20the%20adoption%20of%20Vision-Language%20Models%20as%20surrogate%20judges.%20In%20this%20work%2C%20we%20systematically%20investigate%20the%20reliability%20of%20the%20prevailing%20absolute%20pointwise%20scoring%20standard%2C%20across%20a%20wide%20spectrum%20of%20visual%20generation%20tasks.%20Our%20analysis%20reveals%20that%20this%20paradigm%20is%20limited%20due%20to%20stochastic%20inconsistency%20and%20poor%20alignment%20with%20human%20perception.%20To%20resolve%20these%20limitations%2C%20we%20introduce%20GenArena%2C%20a%20unified%20evaluation%20framework%20that%20leverages%20a%20pairwise%20comparison%20paradigm%20to%20ensure%20stable%20and%20human-aligned%20evaluation.%20Crucially%2C%20our%20experiments%20uncover%20a%20transformative%20finding%20that%20simply%20adopting%20this%20pairwise%20protocol%20enables%20off-the-shelf%20open-source%20models%20to%20outperform%20top-tier%20proprietary%20models.%20Notably%2C%20our%20method%20boosts%20evaluation%20accuracy%20by%20over%2020%25%20and%20achieves%20a%20Spearman%20correlation%20of%200.86%20with%20the%20authoritative%20LMArena%20leaderboard%2C%20drastically%20surpassing%20the%200.36%20correlation%20of%20pointwise%20methods.%20Based%20on%20GenArena%2C%20we%20benchmark%20state-of-the-art%20visual%20generation%20models%20across%20diverse%20tasks%2C%20providing%20the%20community%20with%20a%20rigorous%20and%20automated%20evaluation%20standard%20for%20visual%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2602.06013v1&entry.124074799=Read"},
{"title": "Self-Supervised Learning with a Multi-Task Latent Space Objective", "author": "Pierre-Fran\u00e7ois De Plaen and Abhishek Jha and Luc Van Gool and Tinne Tuytelaars and Marc Proesmans", "abstract": "Self-supervised learning (SSL) methods based on Siamese networks learn visual representations by aligning different views of the same image. The multi-crop strategy, which incorporates small local crops to global ones, enhances many SSL frameworks but causes instability in predictor-based architectures such as BYOL, SimSiam, and MoCo v3. We trace this failure to the shared predictor used across all views and demonstrate that assigning a separate predictor to each view type stabilizes multi-crop training, resulting in significant performance gains. Extending this idea, we treat each spatial transformation as a distinct alignment task and add cutout views, where part of the image is masked before encoding. This yields a simple multi-task formulation of asymmetric Siamese SSL that combines global, local, and masked views into a single framework. The approach is stable, generally applicable across backbones, and consistently improves the performance of ResNet and ViT models on ImageNet.", "link": "http://arxiv.org/abs/2602.05845v1", "date": "2026-02-05", "relevancy": 2.8434, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5954}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5761}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20with%20a%20Multi-Task%20Latent%20Space%20Objective&body=Title%3A%20Self-Supervised%20Learning%20with%20a%20Multi-Task%20Latent%20Space%20Objective%0AAuthor%3A%20Pierre-Fran%C3%A7ois%20De%20Plaen%20and%20Abhishek%20Jha%20and%20Luc%20Van%20Gool%20and%20Tinne%20Tuytelaars%20and%20Marc%20Proesmans%0AAbstract%3A%20Self-supervised%20learning%20%28SSL%29%20methods%20based%20on%20Siamese%20networks%20learn%20visual%20representations%20by%20aligning%20different%20views%20of%20the%20same%20image.%20The%20multi-crop%20strategy%2C%20which%20incorporates%20small%20local%20crops%20to%20global%20ones%2C%20enhances%20many%20SSL%20frameworks%20but%20causes%20instability%20in%20predictor-based%20architectures%20such%20as%20BYOL%2C%20SimSiam%2C%20and%20MoCo%20v3.%20We%20trace%20this%20failure%20to%20the%20shared%20predictor%20used%20across%20all%20views%20and%20demonstrate%20that%20assigning%20a%20separate%20predictor%20to%20each%20view%20type%20stabilizes%20multi-crop%20training%2C%20resulting%20in%20significant%20performance%20gains.%20Extending%20this%20idea%2C%20we%20treat%20each%20spatial%20transformation%20as%20a%20distinct%20alignment%20task%20and%20add%20cutout%20views%2C%20where%20part%20of%20the%20image%20is%20masked%20before%20encoding.%20This%20yields%20a%20simple%20multi-task%20formulation%20of%20asymmetric%20Siamese%20SSL%20that%20combines%20global%2C%20local%2C%20and%20masked%20views%20into%20a%20single%20framework.%20The%20approach%20is%20stable%2C%20generally%20applicable%20across%20backbones%2C%20and%20consistently%20improves%20the%20performance%20of%20ResNet%20and%20ViT%20models%20on%20ImageNet.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05845v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Learning%2520with%2520a%2520Multi-Task%2520Latent%2520Space%2520Objective%26entry.906535625%3DPierre-Fran%25C3%25A7ois%2520De%2520Plaen%2520and%2520Abhishek%2520Jha%2520and%2520Luc%2520Van%2520Gool%2520and%2520Tinne%2520Tuytelaars%2520and%2520Marc%2520Proesmans%26entry.1292438233%3DSelf-supervised%2520learning%2520%2528SSL%2529%2520methods%2520based%2520on%2520Siamese%2520networks%2520learn%2520visual%2520representations%2520by%2520aligning%2520different%2520views%2520of%2520the%2520same%2520image.%2520The%2520multi-crop%2520strategy%252C%2520which%2520incorporates%2520small%2520local%2520crops%2520to%2520global%2520ones%252C%2520enhances%2520many%2520SSL%2520frameworks%2520but%2520causes%2520instability%2520in%2520predictor-based%2520architectures%2520such%2520as%2520BYOL%252C%2520SimSiam%252C%2520and%2520MoCo%2520v3.%2520We%2520trace%2520this%2520failure%2520to%2520the%2520shared%2520predictor%2520used%2520across%2520all%2520views%2520and%2520demonstrate%2520that%2520assigning%2520a%2520separate%2520predictor%2520to%2520each%2520view%2520type%2520stabilizes%2520multi-crop%2520training%252C%2520resulting%2520in%2520significant%2520performance%2520gains.%2520Extending%2520this%2520idea%252C%2520we%2520treat%2520each%2520spatial%2520transformation%2520as%2520a%2520distinct%2520alignment%2520task%2520and%2520add%2520cutout%2520views%252C%2520where%2520part%2520of%2520the%2520image%2520is%2520masked%2520before%2520encoding.%2520This%2520yields%2520a%2520simple%2520multi-task%2520formulation%2520of%2520asymmetric%2520Siamese%2520SSL%2520that%2520combines%2520global%252C%2520local%252C%2520and%2520masked%2520views%2520into%2520a%2520single%2520framework.%2520The%2520approach%2520is%2520stable%252C%2520generally%2520applicable%2520across%2520backbones%252C%2520and%2520consistently%2520improves%2520the%2520performance%2520of%2520ResNet%2520and%2520ViT%2520models%2520on%2520ImageNet.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05845v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20with%20a%20Multi-Task%20Latent%20Space%20Objective&entry.906535625=Pierre-Fran%C3%A7ois%20De%20Plaen%20and%20Abhishek%20Jha%20and%20Luc%20Van%20Gool%20and%20Tinne%20Tuytelaars%20and%20Marc%20Proesmans&entry.1292438233=Self-supervised%20learning%20%28SSL%29%20methods%20based%20on%20Siamese%20networks%20learn%20visual%20representations%20by%20aligning%20different%20views%20of%20the%20same%20image.%20The%20multi-crop%20strategy%2C%20which%20incorporates%20small%20local%20crops%20to%20global%20ones%2C%20enhances%20many%20SSL%20frameworks%20but%20causes%20instability%20in%20predictor-based%20architectures%20such%20as%20BYOL%2C%20SimSiam%2C%20and%20MoCo%20v3.%20We%20trace%20this%20failure%20to%20the%20shared%20predictor%20used%20across%20all%20views%20and%20demonstrate%20that%20assigning%20a%20separate%20predictor%20to%20each%20view%20type%20stabilizes%20multi-crop%20training%2C%20resulting%20in%20significant%20performance%20gains.%20Extending%20this%20idea%2C%20we%20treat%20each%20spatial%20transformation%20as%20a%20distinct%20alignment%20task%20and%20add%20cutout%20views%2C%20where%20part%20of%20the%20image%20is%20masked%20before%20encoding.%20This%20yields%20a%20simple%20multi-task%20formulation%20of%20asymmetric%20Siamese%20SSL%20that%20combines%20global%2C%20local%2C%20and%20masked%20views%20into%20a%20single%20framework.%20The%20approach%20is%20stable%2C%20generally%20applicable%20across%20backbones%2C%20and%20consistently%20improves%20the%20performance%20of%20ResNet%20and%20ViT%20models%20on%20ImageNet.&entry.1838667208=http%3A//arxiv.org/abs/2602.05845v1&entry.124074799=Read"},
{"title": "LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation", "author": "Junyang Chen and Xiangbo Lv and Zhiqiang Kou and Xingdong Sheng and Ning Xu and Yiguo Qiao", "abstract": "Open-vocabulary semantic segmentation (OVSS) extends traditional closed-set segmentation by enabling pixel-wise annotation for both seen and unseen categories using arbitrary textual descriptions. While existing methods leverage vision-language models (VLMs) like CLIP, their reliance on image-level pretraining often results in imprecise spatial alignment, leading to mismatched segmentations in ambiguous or cluttered scenes. However, most existing approaches lack strong object priors and region-level constraints, which can lead to object hallucination or missed detections, further degrading performance. To address these challenges, we propose LoGoSeg, an efficient single-stage framework that integrates three key innovations: (i) an object existence prior that dynamically weights relevant categories through global image-text similarity, effectively reducing hallucinations; (ii) a region-aware alignment module that establishes precise region-level visual-textual correspondences; and (iii) a dual-stream fusion mechanism that optimally combines local structural information with global semantic context. Unlike prior works, LoGoSeg eliminates the need for external mask proposals, additional backbones, or extra datasets, ensuring efficiency. Extensive experiments on six benchmarks (A-847, PC-459, A-150, PC-59, PAS-20, and PAS-20b) demonstrate its competitive performance and strong generalization in open-vocabulary settings.", "link": "http://arxiv.org/abs/2602.05578v1", "date": "2026-02-05", "relevancy": 2.8409, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5781}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5781}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoGoSeg%3A%20Integrating%20Local%20and%20Global%20Features%20for%20Open-Vocabulary%20Semantic%20Segmentation&body=Title%3A%20LoGoSeg%3A%20Integrating%20Local%20and%20Global%20Features%20for%20Open-Vocabulary%20Semantic%20Segmentation%0AAuthor%3A%20Junyang%20Chen%20and%20Xiangbo%20Lv%20and%20Zhiqiang%20Kou%20and%20Xingdong%20Sheng%20and%20Ning%20Xu%20and%20Yiguo%20Qiao%0AAbstract%3A%20Open-vocabulary%20semantic%20segmentation%20%28OVSS%29%20extends%20traditional%20closed-set%20segmentation%20by%20enabling%20pixel-wise%20annotation%20for%20both%20seen%20and%20unseen%20categories%20using%20arbitrary%20textual%20descriptions.%20While%20existing%20methods%20leverage%20vision-language%20models%20%28VLMs%29%20like%20CLIP%2C%20their%20reliance%20on%20image-level%20pretraining%20often%20results%20in%20imprecise%20spatial%20alignment%2C%20leading%20to%20mismatched%20segmentations%20in%20ambiguous%20or%20cluttered%20scenes.%20However%2C%20most%20existing%20approaches%20lack%20strong%20object%20priors%20and%20region-level%20constraints%2C%20which%20can%20lead%20to%20object%20hallucination%20or%20missed%20detections%2C%20further%20degrading%20performance.%20To%20address%20these%20challenges%2C%20we%20propose%20LoGoSeg%2C%20an%20efficient%20single-stage%20framework%20that%20integrates%20three%20key%20innovations%3A%20%28i%29%20an%20object%20existence%20prior%20that%20dynamically%20weights%20relevant%20categories%20through%20global%20image-text%20similarity%2C%20effectively%20reducing%20hallucinations%3B%20%28ii%29%20a%20region-aware%20alignment%20module%20that%20establishes%20precise%20region-level%20visual-textual%20correspondences%3B%20and%20%28iii%29%20a%20dual-stream%20fusion%20mechanism%20that%20optimally%20combines%20local%20structural%20information%20with%20global%20semantic%20context.%20Unlike%20prior%20works%2C%20LoGoSeg%20eliminates%20the%20need%20for%20external%20mask%20proposals%2C%20additional%20backbones%2C%20or%20extra%20datasets%2C%20ensuring%20efficiency.%20Extensive%20experiments%20on%20six%20benchmarks%20%28A-847%2C%20PC-459%2C%20A-150%2C%20PC-59%2C%20PAS-20%2C%20and%20PAS-20b%29%20demonstrate%20its%20competitive%20performance%20and%20strong%20generalization%20in%20open-vocabulary%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05578v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoGoSeg%253A%2520Integrating%2520Local%2520and%2520Global%2520Features%2520for%2520Open-Vocabulary%2520Semantic%2520Segmentation%26entry.906535625%3DJunyang%2520Chen%2520and%2520Xiangbo%2520Lv%2520and%2520Zhiqiang%2520Kou%2520and%2520Xingdong%2520Sheng%2520and%2520Ning%2520Xu%2520and%2520Yiguo%2520Qiao%26entry.1292438233%3DOpen-vocabulary%2520semantic%2520segmentation%2520%2528OVSS%2529%2520extends%2520traditional%2520closed-set%2520segmentation%2520by%2520enabling%2520pixel-wise%2520annotation%2520for%2520both%2520seen%2520and%2520unseen%2520categories%2520using%2520arbitrary%2520textual%2520descriptions.%2520While%2520existing%2520methods%2520leverage%2520vision-language%2520models%2520%2528VLMs%2529%2520like%2520CLIP%252C%2520their%2520reliance%2520on%2520image-level%2520pretraining%2520often%2520results%2520in%2520imprecise%2520spatial%2520alignment%252C%2520leading%2520to%2520mismatched%2520segmentations%2520in%2520ambiguous%2520or%2520cluttered%2520scenes.%2520However%252C%2520most%2520existing%2520approaches%2520lack%2520strong%2520object%2520priors%2520and%2520region-level%2520constraints%252C%2520which%2520can%2520lead%2520to%2520object%2520hallucination%2520or%2520missed%2520detections%252C%2520further%2520degrading%2520performance.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520LoGoSeg%252C%2520an%2520efficient%2520single-stage%2520framework%2520that%2520integrates%2520three%2520key%2520innovations%253A%2520%2528i%2529%2520an%2520object%2520existence%2520prior%2520that%2520dynamically%2520weights%2520relevant%2520categories%2520through%2520global%2520image-text%2520similarity%252C%2520effectively%2520reducing%2520hallucinations%253B%2520%2528ii%2529%2520a%2520region-aware%2520alignment%2520module%2520that%2520establishes%2520precise%2520region-level%2520visual-textual%2520correspondences%253B%2520and%2520%2528iii%2529%2520a%2520dual-stream%2520fusion%2520mechanism%2520that%2520optimally%2520combines%2520local%2520structural%2520information%2520with%2520global%2520semantic%2520context.%2520Unlike%2520prior%2520works%252C%2520LoGoSeg%2520eliminates%2520the%2520need%2520for%2520external%2520mask%2520proposals%252C%2520additional%2520backbones%252C%2520or%2520extra%2520datasets%252C%2520ensuring%2520efficiency.%2520Extensive%2520experiments%2520on%2520six%2520benchmarks%2520%2528A-847%252C%2520PC-459%252C%2520A-150%252C%2520PC-59%252C%2520PAS-20%252C%2520and%2520PAS-20b%2529%2520demonstrate%2520its%2520competitive%2520performance%2520and%2520strong%2520generalization%2520in%2520open-vocabulary%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05578v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoGoSeg%3A%20Integrating%20Local%20and%20Global%20Features%20for%20Open-Vocabulary%20Semantic%20Segmentation&entry.906535625=Junyang%20Chen%20and%20Xiangbo%20Lv%20and%20Zhiqiang%20Kou%20and%20Xingdong%20Sheng%20and%20Ning%20Xu%20and%20Yiguo%20Qiao&entry.1292438233=Open-vocabulary%20semantic%20segmentation%20%28OVSS%29%20extends%20traditional%20closed-set%20segmentation%20by%20enabling%20pixel-wise%20annotation%20for%20both%20seen%20and%20unseen%20categories%20using%20arbitrary%20textual%20descriptions.%20While%20existing%20methods%20leverage%20vision-language%20models%20%28VLMs%29%20like%20CLIP%2C%20their%20reliance%20on%20image-level%20pretraining%20often%20results%20in%20imprecise%20spatial%20alignment%2C%20leading%20to%20mismatched%20segmentations%20in%20ambiguous%20or%20cluttered%20scenes.%20However%2C%20most%20existing%20approaches%20lack%20strong%20object%20priors%20and%20region-level%20constraints%2C%20which%20can%20lead%20to%20object%20hallucination%20or%20missed%20detections%2C%20further%20degrading%20performance.%20To%20address%20these%20challenges%2C%20we%20propose%20LoGoSeg%2C%20an%20efficient%20single-stage%20framework%20that%20integrates%20three%20key%20innovations%3A%20%28i%29%20an%20object%20existence%20prior%20that%20dynamically%20weights%20relevant%20categories%20through%20global%20image-text%20similarity%2C%20effectively%20reducing%20hallucinations%3B%20%28ii%29%20a%20region-aware%20alignment%20module%20that%20establishes%20precise%20region-level%20visual-textual%20correspondences%3B%20and%20%28iii%29%20a%20dual-stream%20fusion%20mechanism%20that%20optimally%20combines%20local%20structural%20information%20with%20global%20semantic%20context.%20Unlike%20prior%20works%2C%20LoGoSeg%20eliminates%20the%20need%20for%20external%20mask%20proposals%2C%20additional%20backbones%2C%20or%20extra%20datasets%2C%20ensuring%20efficiency.%20Extensive%20experiments%20on%20six%20benchmarks%20%28A-847%2C%20PC-459%2C%20A-150%2C%20PC-59%2C%20PAS-20%2C%20and%20PAS-20b%29%20demonstrate%20its%20competitive%20performance%20and%20strong%20generalization%20in%20open-vocabulary%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2602.05578v1&entry.124074799=Read"},
{"title": "Thinking with Geometry: Active Geometry Integration for Spatial Reasoning", "author": "Haoyuan Li and Qihang Cao and Tao Tang and Kun Xiang and Zihan Guo and Jianhua Han and Hang Xu and Xiaodan Liang", "abstract": "Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.", "link": "http://arxiv.org/abs/2602.06037v1", "date": "2026-02-05", "relevancy": 2.8316, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5684}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5684}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinking%20with%20Geometry%3A%20Active%20Geometry%20Integration%20for%20Spatial%20Reasoning&body=Title%3A%20Thinking%20with%20Geometry%3A%20Active%20Geometry%20Integration%20for%20Spatial%20Reasoning%0AAuthor%3A%20Haoyuan%20Li%20and%20Qihang%20Cao%20and%20Tao%20Tang%20and%20Kun%20Xiang%20and%20Zihan%20Guo%20and%20Jianhua%20Han%20and%20Hang%20Xu%20and%20Xiaodan%20Liang%0AAbstract%3A%20Recent%20progress%20in%20spatial%20reasoning%20with%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20increasingly%20leverages%20geometric%20priors%20from%203D%20encoders.%20However%2C%20most%20existing%20integration%20strategies%20remain%20passive%3A%20geometry%20is%20exposed%20as%20a%20global%20stream%20and%20fused%20in%20an%20indiscriminate%20manner%2C%20which%20often%20induces%20semantic-geometry%20misalignment%20and%20redundant%20signals.%20We%20propose%20GeoThinker%2C%20a%20framework%20that%20shifts%20the%20paradigm%20from%20passive%20fusion%20to%20active%20perception.%20Instead%20of%20feature%20mixing%2C%20GeoThinker%20enables%20the%20model%20to%20selectively%20retrieve%20geometric%20evidence%20conditioned%20on%20its%20internal%20reasoning%20demands.%20GeoThinker%20achieves%20this%20through%20Spatial-Grounded%20Fusion%20applied%20at%20carefully%20selected%20VLM%20layers%2C%20where%20semantic%20visual%20priors%20selectively%20query%20and%20integrate%20task-relevant%20geometry%20via%20frame-strict%20cross-attention%2C%20further%20calibrated%20by%20Importance%20Gating%20that%20biases%20per-frame%20attention%20toward%20task-relevant%20structures.%20Comprehensive%20evaluation%20results%20show%20that%20GeoThinker%20sets%20a%20new%20state-of-the-art%20in%20spatial%20intelligence%2C%20achieving%20a%20peak%20score%20of%2072.6%20on%20the%20VSI-Bench.%20Furthermore%2C%20GeoThinker%20demonstrates%20robust%20generalization%20and%20significantly%20improved%20spatial%20perception%20across%20complex%20downstream%20scenarios%2C%20including%20embodied%20referring%20and%20autonomous%20driving.%20Our%20results%20indicate%20that%20the%20ability%20to%20actively%20integrate%20spatial%20structures%20is%20essential%20for%20next-generation%20spatial%20intelligence.%20Code%20can%20be%20found%20at%20https%3A//github.com/Li-Hao-yuan/GeoThinker.%0ALink%3A%20http%3A//arxiv.org/abs/2602.06037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinking%2520with%2520Geometry%253A%2520Active%2520Geometry%2520Integration%2520for%2520Spatial%2520Reasoning%26entry.906535625%3DHaoyuan%2520Li%2520and%2520Qihang%2520Cao%2520and%2520Tao%2520Tang%2520and%2520Kun%2520Xiang%2520and%2520Zihan%2520Guo%2520and%2520Jianhua%2520Han%2520and%2520Hang%2520Xu%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3DRecent%2520progress%2520in%2520spatial%2520reasoning%2520with%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520increasingly%2520leverages%2520geometric%2520priors%2520from%25203D%2520encoders.%2520However%252C%2520most%2520existing%2520integration%2520strategies%2520remain%2520passive%253A%2520geometry%2520is%2520exposed%2520as%2520a%2520global%2520stream%2520and%2520fused%2520in%2520an%2520indiscriminate%2520manner%252C%2520which%2520often%2520induces%2520semantic-geometry%2520misalignment%2520and%2520redundant%2520signals.%2520We%2520propose%2520GeoThinker%252C%2520a%2520framework%2520that%2520shifts%2520the%2520paradigm%2520from%2520passive%2520fusion%2520to%2520active%2520perception.%2520Instead%2520of%2520feature%2520mixing%252C%2520GeoThinker%2520enables%2520the%2520model%2520to%2520selectively%2520retrieve%2520geometric%2520evidence%2520conditioned%2520on%2520its%2520internal%2520reasoning%2520demands.%2520GeoThinker%2520achieves%2520this%2520through%2520Spatial-Grounded%2520Fusion%2520applied%2520at%2520carefully%2520selected%2520VLM%2520layers%252C%2520where%2520semantic%2520visual%2520priors%2520selectively%2520query%2520and%2520integrate%2520task-relevant%2520geometry%2520via%2520frame-strict%2520cross-attention%252C%2520further%2520calibrated%2520by%2520Importance%2520Gating%2520that%2520biases%2520per-frame%2520attention%2520toward%2520task-relevant%2520structures.%2520Comprehensive%2520evaluation%2520results%2520show%2520that%2520GeoThinker%2520sets%2520a%2520new%2520state-of-the-art%2520in%2520spatial%2520intelligence%252C%2520achieving%2520a%2520peak%2520score%2520of%252072.6%2520on%2520the%2520VSI-Bench.%2520Furthermore%252C%2520GeoThinker%2520demonstrates%2520robust%2520generalization%2520and%2520significantly%2520improved%2520spatial%2520perception%2520across%2520complex%2520downstream%2520scenarios%252C%2520including%2520embodied%2520referring%2520and%2520autonomous%2520driving.%2520Our%2520results%2520indicate%2520that%2520the%2520ability%2520to%2520actively%2520integrate%2520spatial%2520structures%2520is%2520essential%2520for%2520next-generation%2520spatial%2520intelligence.%2520Code%2520can%2520be%2520found%2520at%2520https%253A//github.com/Li-Hao-yuan/GeoThinker.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.06037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinking%20with%20Geometry%3A%20Active%20Geometry%20Integration%20for%20Spatial%20Reasoning&entry.906535625=Haoyuan%20Li%20and%20Qihang%20Cao%20and%20Tao%20Tang%20and%20Kun%20Xiang%20and%20Zihan%20Guo%20and%20Jianhua%20Han%20and%20Hang%20Xu%20and%20Xiaodan%20Liang&entry.1292438233=Recent%20progress%20in%20spatial%20reasoning%20with%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20increasingly%20leverages%20geometric%20priors%20from%203D%20encoders.%20However%2C%20most%20existing%20integration%20strategies%20remain%20passive%3A%20geometry%20is%20exposed%20as%20a%20global%20stream%20and%20fused%20in%20an%20indiscriminate%20manner%2C%20which%20often%20induces%20semantic-geometry%20misalignment%20and%20redundant%20signals.%20We%20propose%20GeoThinker%2C%20a%20framework%20that%20shifts%20the%20paradigm%20from%20passive%20fusion%20to%20active%20perception.%20Instead%20of%20feature%20mixing%2C%20GeoThinker%20enables%20the%20model%20to%20selectively%20retrieve%20geometric%20evidence%20conditioned%20on%20its%20internal%20reasoning%20demands.%20GeoThinker%20achieves%20this%20through%20Spatial-Grounded%20Fusion%20applied%20at%20carefully%20selected%20VLM%20layers%2C%20where%20semantic%20visual%20priors%20selectively%20query%20and%20integrate%20task-relevant%20geometry%20via%20frame-strict%20cross-attention%2C%20further%20calibrated%20by%20Importance%20Gating%20that%20biases%20per-frame%20attention%20toward%20task-relevant%20structures.%20Comprehensive%20evaluation%20results%20show%20that%20GeoThinker%20sets%20a%20new%20state-of-the-art%20in%20spatial%20intelligence%2C%20achieving%20a%20peak%20score%20of%2072.6%20on%20the%20VSI-Bench.%20Furthermore%2C%20GeoThinker%20demonstrates%20robust%20generalization%20and%20significantly%20improved%20spatial%20perception%20across%20complex%20downstream%20scenarios%2C%20including%20embodied%20referring%20and%20autonomous%20driving.%20Our%20results%20indicate%20that%20the%20ability%20to%20actively%20integrate%20spatial%20structures%20is%20essential%20for%20next-generation%20spatial%20intelligence.%20Code%20can%20be%20found%20at%20https%3A//github.com/Li-Hao-yuan/GeoThinker.&entry.1838667208=http%3A//arxiv.org/abs/2602.06037v1&entry.124074799=Read"},
{"title": "EgoPoseVR: Spatiotemporal Multi-Modal Reasoning for Egocentric Full-Body Pose in Virtual Reality", "author": "Haojie Cheng and Shaun Jing Heng Ong and Shaoyu Cai and Aiden Tat Yang Koh and Fuxi Ouyang and Eng Tat Khoo", "abstract": "Immersive virtual reality (VR) applications demand accurate, temporally coherent full-body pose tracking. Recent head-mounted camera-based approaches show promise in egocentric pose estimation, but encounter challenges when applied to VR head-mounted displays (HMDs), including temporal instability, inaccurate lower-body estimation, and the lack of real-time performance. To address these limitations, we present EgoPoseVR, an end-to-end framework for accurate egocentric full-body pose estimation in VR that integrates headset motion cues with egocentric RGB-D observations through a dual-modality fusion pipeline. A spatiotemporal encoder extracts frame- and joint-level representations, which are fused via cross-attention to fully exploit complementary motion cues across modalities. A kinematic optimization module then imposes constraints from HMD signals, enhancing the accuracy and stability of pose estimation. To facilitate training and evaluation, we introduce a large-scale synthetic dataset of over 1.8 million temporally aligned HMD and RGB-D frames across diverse VR scenarios. Experimental results show that EgoPoseVR outperforms state-of-the-art egocentric pose estimation models. A user study in real-world scenes further shows that EgoPoseVR achieved significantly higher subjective ratings in accuracy, stability, embodiment, and intention for future use compared to baseline methods. These results show that EgoPoseVR enables robust full-body pose tracking, offering a practical solution for accurate VR embodiment without requiring additional body-worn sensors or room-scale tracking systems.", "link": "http://arxiv.org/abs/2602.05590v1", "date": "2026-02-05", "relevancy": 2.8098, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5837}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.556}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoPoseVR%3A%20Spatiotemporal%20Multi-Modal%20Reasoning%20for%20Egocentric%20Full-Body%20Pose%20in%20Virtual%20Reality&body=Title%3A%20EgoPoseVR%3A%20Spatiotemporal%20Multi-Modal%20Reasoning%20for%20Egocentric%20Full-Body%20Pose%20in%20Virtual%20Reality%0AAuthor%3A%20Haojie%20Cheng%20and%20Shaun%20Jing%20Heng%20Ong%20and%20Shaoyu%20Cai%20and%20Aiden%20Tat%20Yang%20Koh%20and%20Fuxi%20Ouyang%20and%20Eng%20Tat%20Khoo%0AAbstract%3A%20Immersive%20virtual%20reality%20%28VR%29%20applications%20demand%20accurate%2C%20temporally%20coherent%20full-body%20pose%20tracking.%20Recent%20head-mounted%20camera-based%20approaches%20show%20promise%20in%20egocentric%20pose%20estimation%2C%20but%20encounter%20challenges%20when%20applied%20to%20VR%20head-mounted%20displays%20%28HMDs%29%2C%20including%20temporal%20instability%2C%20inaccurate%20lower-body%20estimation%2C%20and%20the%20lack%20of%20real-time%20performance.%20To%20address%20these%20limitations%2C%20we%20present%20EgoPoseVR%2C%20an%20end-to-end%20framework%20for%20accurate%20egocentric%20full-body%20pose%20estimation%20in%20VR%20that%20integrates%20headset%20motion%20cues%20with%20egocentric%20RGB-D%20observations%20through%20a%20dual-modality%20fusion%20pipeline.%20A%20spatiotemporal%20encoder%20extracts%20frame-%20and%20joint-level%20representations%2C%20which%20are%20fused%20via%20cross-attention%20to%20fully%20exploit%20complementary%20motion%20cues%20across%20modalities.%20A%20kinematic%20optimization%20module%20then%20imposes%20constraints%20from%20HMD%20signals%2C%20enhancing%20the%20accuracy%20and%20stability%20of%20pose%20estimation.%20To%20facilitate%20training%20and%20evaluation%2C%20we%20introduce%20a%20large-scale%20synthetic%20dataset%20of%20over%201.8%20million%20temporally%20aligned%20HMD%20and%20RGB-D%20frames%20across%20diverse%20VR%20scenarios.%20Experimental%20results%20show%20that%20EgoPoseVR%20outperforms%20state-of-the-art%20egocentric%20pose%20estimation%20models.%20A%20user%20study%20in%20real-world%20scenes%20further%20shows%20that%20EgoPoseVR%20achieved%20significantly%20higher%20subjective%20ratings%20in%20accuracy%2C%20stability%2C%20embodiment%2C%20and%20intention%20for%20future%20use%20compared%20to%20baseline%20methods.%20These%20results%20show%20that%20EgoPoseVR%20enables%20robust%20full-body%20pose%20tracking%2C%20offering%20a%20practical%20solution%20for%20accurate%20VR%20embodiment%20without%20requiring%20additional%20body-worn%20sensors%20or%20room-scale%20tracking%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoPoseVR%253A%2520Spatiotemporal%2520Multi-Modal%2520Reasoning%2520for%2520Egocentric%2520Full-Body%2520Pose%2520in%2520Virtual%2520Reality%26entry.906535625%3DHaojie%2520Cheng%2520and%2520Shaun%2520Jing%2520Heng%2520Ong%2520and%2520Shaoyu%2520Cai%2520and%2520Aiden%2520Tat%2520Yang%2520Koh%2520and%2520Fuxi%2520Ouyang%2520and%2520Eng%2520Tat%2520Khoo%26entry.1292438233%3DImmersive%2520virtual%2520reality%2520%2528VR%2529%2520applications%2520demand%2520accurate%252C%2520temporally%2520coherent%2520full-body%2520pose%2520tracking.%2520Recent%2520head-mounted%2520camera-based%2520approaches%2520show%2520promise%2520in%2520egocentric%2520pose%2520estimation%252C%2520but%2520encounter%2520challenges%2520when%2520applied%2520to%2520VR%2520head-mounted%2520displays%2520%2528HMDs%2529%252C%2520including%2520temporal%2520instability%252C%2520inaccurate%2520lower-body%2520estimation%252C%2520and%2520the%2520lack%2520of%2520real-time%2520performance.%2520To%2520address%2520these%2520limitations%252C%2520we%2520present%2520EgoPoseVR%252C%2520an%2520end-to-end%2520framework%2520for%2520accurate%2520egocentric%2520full-body%2520pose%2520estimation%2520in%2520VR%2520that%2520integrates%2520headset%2520motion%2520cues%2520with%2520egocentric%2520RGB-D%2520observations%2520through%2520a%2520dual-modality%2520fusion%2520pipeline.%2520A%2520spatiotemporal%2520encoder%2520extracts%2520frame-%2520and%2520joint-level%2520representations%252C%2520which%2520are%2520fused%2520via%2520cross-attention%2520to%2520fully%2520exploit%2520complementary%2520motion%2520cues%2520across%2520modalities.%2520A%2520kinematic%2520optimization%2520module%2520then%2520imposes%2520constraints%2520from%2520HMD%2520signals%252C%2520enhancing%2520the%2520accuracy%2520and%2520stability%2520of%2520pose%2520estimation.%2520To%2520facilitate%2520training%2520and%2520evaluation%252C%2520we%2520introduce%2520a%2520large-scale%2520synthetic%2520dataset%2520of%2520over%25201.8%2520million%2520temporally%2520aligned%2520HMD%2520and%2520RGB-D%2520frames%2520across%2520diverse%2520VR%2520scenarios.%2520Experimental%2520results%2520show%2520that%2520EgoPoseVR%2520outperforms%2520state-of-the-art%2520egocentric%2520pose%2520estimation%2520models.%2520A%2520user%2520study%2520in%2520real-world%2520scenes%2520further%2520shows%2520that%2520EgoPoseVR%2520achieved%2520significantly%2520higher%2520subjective%2520ratings%2520in%2520accuracy%252C%2520stability%252C%2520embodiment%252C%2520and%2520intention%2520for%2520future%2520use%2520compared%2520to%2520baseline%2520methods.%2520These%2520results%2520show%2520that%2520EgoPoseVR%2520enables%2520robust%2520full-body%2520pose%2520tracking%252C%2520offering%2520a%2520practical%2520solution%2520for%2520accurate%2520VR%2520embodiment%2520without%2520requiring%2520additional%2520body-worn%2520sensors%2520or%2520room-scale%2520tracking%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoPoseVR%3A%20Spatiotemporal%20Multi-Modal%20Reasoning%20for%20Egocentric%20Full-Body%20Pose%20in%20Virtual%20Reality&entry.906535625=Haojie%20Cheng%20and%20Shaun%20Jing%20Heng%20Ong%20and%20Shaoyu%20Cai%20and%20Aiden%20Tat%20Yang%20Koh%20and%20Fuxi%20Ouyang%20and%20Eng%20Tat%20Khoo&entry.1292438233=Immersive%20virtual%20reality%20%28VR%29%20applications%20demand%20accurate%2C%20temporally%20coherent%20full-body%20pose%20tracking.%20Recent%20head-mounted%20camera-based%20approaches%20show%20promise%20in%20egocentric%20pose%20estimation%2C%20but%20encounter%20challenges%20when%20applied%20to%20VR%20head-mounted%20displays%20%28HMDs%29%2C%20including%20temporal%20instability%2C%20inaccurate%20lower-body%20estimation%2C%20and%20the%20lack%20of%20real-time%20performance.%20To%20address%20these%20limitations%2C%20we%20present%20EgoPoseVR%2C%20an%20end-to-end%20framework%20for%20accurate%20egocentric%20full-body%20pose%20estimation%20in%20VR%20that%20integrates%20headset%20motion%20cues%20with%20egocentric%20RGB-D%20observations%20through%20a%20dual-modality%20fusion%20pipeline.%20A%20spatiotemporal%20encoder%20extracts%20frame-%20and%20joint-level%20representations%2C%20which%20are%20fused%20via%20cross-attention%20to%20fully%20exploit%20complementary%20motion%20cues%20across%20modalities.%20A%20kinematic%20optimization%20module%20then%20imposes%20constraints%20from%20HMD%20signals%2C%20enhancing%20the%20accuracy%20and%20stability%20of%20pose%20estimation.%20To%20facilitate%20training%20and%20evaluation%2C%20we%20introduce%20a%20large-scale%20synthetic%20dataset%20of%20over%201.8%20million%20temporally%20aligned%20HMD%20and%20RGB-D%20frames%20across%20diverse%20VR%20scenarios.%20Experimental%20results%20show%20that%20EgoPoseVR%20outperforms%20state-of-the-art%20egocentric%20pose%20estimation%20models.%20A%20user%20study%20in%20real-world%20scenes%20further%20shows%20that%20EgoPoseVR%20achieved%20significantly%20higher%20subjective%20ratings%20in%20accuracy%2C%20stability%2C%20embodiment%2C%20and%20intention%20for%20future%20use%20compared%20to%20baseline%20methods.%20These%20results%20show%20that%20EgoPoseVR%20enables%20robust%20full-body%20pose%20tracking%2C%20offering%20a%20practical%20solution%20for%20accurate%20VR%20embodiment%20without%20requiring%20additional%20body-worn%20sensors%20or%20room-scale%20tracking%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2602.05590v1&entry.124074799=Read"},
{"title": "Weaver: End-to-End Agentic System Training for Video Interleaved Reasoning", "author": "Yudi Shi and Shangzhe Di and Qirui Chen and Qinian Wang and Jiayin Cai and Xiaolong Jiang and Yao Hu and Weidi Xie", "abstract": "Video reasoning constitutes a comprehensive assessment of a model's capabilities, as it demands robust perceptual and interpretive skills, thereby serving as a means to explore the boundaries of model performance. While recent research has leveraged text-centric Chain-of-Thought reasoning to augment these capabilities, such approaches frequently suffer from representational mismatch and restricted by limited perceptual acuity. To address these limitations, we propose Weaver, a novel, end-to-end trainable multimodal reasoning agentic system. Weaver empowers its policy model to dynamically invoke diverse tools throughout the reasoning process, enabling progressive acquisition of crucial visual cues and construction of authentic multimodal reasoning trajectories. Furthermore, we integrate a reinforcement learning algorithm to allow the system to freely explore strategies for employing and combining these tools with trajectory-free data. Extensive experiments demonstrate that our system, Weaver, enhances performance on several complex video reasoning benchmarks, particularly those involving long videos.", "link": "http://arxiv.org/abs/2602.05829v1", "date": "2026-02-05", "relevancy": 2.7966, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5634}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5634}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weaver%3A%20End-to-End%20Agentic%20System%20Training%20for%20Video%20Interleaved%20Reasoning&body=Title%3A%20Weaver%3A%20End-to-End%20Agentic%20System%20Training%20for%20Video%20Interleaved%20Reasoning%0AAuthor%3A%20Yudi%20Shi%20and%20Shangzhe%20Di%20and%20Qirui%20Chen%20and%20Qinian%20Wang%20and%20Jiayin%20Cai%20and%20Xiaolong%20Jiang%20and%20Yao%20Hu%20and%20Weidi%20Xie%0AAbstract%3A%20Video%20reasoning%20constitutes%20a%20comprehensive%20assessment%20of%20a%20model%27s%20capabilities%2C%20as%20it%20demands%20robust%20perceptual%20and%20interpretive%20skills%2C%20thereby%20serving%20as%20a%20means%20to%20explore%20the%20boundaries%20of%20model%20performance.%20While%20recent%20research%20has%20leveraged%20text-centric%20Chain-of-Thought%20reasoning%20to%20augment%20these%20capabilities%2C%20such%20approaches%20frequently%20suffer%20from%20representational%20mismatch%20and%20restricted%20by%20limited%20perceptual%20acuity.%20To%20address%20these%20limitations%2C%20we%20propose%20Weaver%2C%20a%20novel%2C%20end-to-end%20trainable%20multimodal%20reasoning%20agentic%20system.%20Weaver%20empowers%20its%20policy%20model%20to%20dynamically%20invoke%20diverse%20tools%20throughout%20the%20reasoning%20process%2C%20enabling%20progressive%20acquisition%20of%20crucial%20visual%20cues%20and%20construction%20of%20authentic%20multimodal%20reasoning%20trajectories.%20Furthermore%2C%20we%20integrate%20a%20reinforcement%20learning%20algorithm%20to%20allow%20the%20system%20to%20freely%20explore%20strategies%20for%20employing%20and%20combining%20these%20tools%20with%20trajectory-free%20data.%20Extensive%20experiments%20demonstrate%20that%20our%20system%2C%20Weaver%2C%20enhances%20performance%20on%20several%20complex%20video%20reasoning%20benchmarks%2C%20particularly%20those%20involving%20long%20videos.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeaver%253A%2520End-to-End%2520Agentic%2520System%2520Training%2520for%2520Video%2520Interleaved%2520Reasoning%26entry.906535625%3DYudi%2520Shi%2520and%2520Shangzhe%2520Di%2520and%2520Qirui%2520Chen%2520and%2520Qinian%2520Wang%2520and%2520Jiayin%2520Cai%2520and%2520Xiaolong%2520Jiang%2520and%2520Yao%2520Hu%2520and%2520Weidi%2520Xie%26entry.1292438233%3DVideo%2520reasoning%2520constitutes%2520a%2520comprehensive%2520assessment%2520of%2520a%2520model%2527s%2520capabilities%252C%2520as%2520it%2520demands%2520robust%2520perceptual%2520and%2520interpretive%2520skills%252C%2520thereby%2520serving%2520as%2520a%2520means%2520to%2520explore%2520the%2520boundaries%2520of%2520model%2520performance.%2520While%2520recent%2520research%2520has%2520leveraged%2520text-centric%2520Chain-of-Thought%2520reasoning%2520to%2520augment%2520these%2520capabilities%252C%2520such%2520approaches%2520frequently%2520suffer%2520from%2520representational%2520mismatch%2520and%2520restricted%2520by%2520limited%2520perceptual%2520acuity.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Weaver%252C%2520a%2520novel%252C%2520end-to-end%2520trainable%2520multimodal%2520reasoning%2520agentic%2520system.%2520Weaver%2520empowers%2520its%2520policy%2520model%2520to%2520dynamically%2520invoke%2520diverse%2520tools%2520throughout%2520the%2520reasoning%2520process%252C%2520enabling%2520progressive%2520acquisition%2520of%2520crucial%2520visual%2520cues%2520and%2520construction%2520of%2520authentic%2520multimodal%2520reasoning%2520trajectories.%2520Furthermore%252C%2520we%2520integrate%2520a%2520reinforcement%2520learning%2520algorithm%2520to%2520allow%2520the%2520system%2520to%2520freely%2520explore%2520strategies%2520for%2520employing%2520and%2520combining%2520these%2520tools%2520with%2520trajectory-free%2520data.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520system%252C%2520Weaver%252C%2520enhances%2520performance%2520on%2520several%2520complex%2520video%2520reasoning%2520benchmarks%252C%2520particularly%2520those%2520involving%2520long%2520videos.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weaver%3A%20End-to-End%20Agentic%20System%20Training%20for%20Video%20Interleaved%20Reasoning&entry.906535625=Yudi%20Shi%20and%20Shangzhe%20Di%20and%20Qirui%20Chen%20and%20Qinian%20Wang%20and%20Jiayin%20Cai%20and%20Xiaolong%20Jiang%20and%20Yao%20Hu%20and%20Weidi%20Xie&entry.1292438233=Video%20reasoning%20constitutes%20a%20comprehensive%20assessment%20of%20a%20model%27s%20capabilities%2C%20as%20it%20demands%20robust%20perceptual%20and%20interpretive%20skills%2C%20thereby%20serving%20as%20a%20means%20to%20explore%20the%20boundaries%20of%20model%20performance.%20While%20recent%20research%20has%20leveraged%20text-centric%20Chain-of-Thought%20reasoning%20to%20augment%20these%20capabilities%2C%20such%20approaches%20frequently%20suffer%20from%20representational%20mismatch%20and%20restricted%20by%20limited%20perceptual%20acuity.%20To%20address%20these%20limitations%2C%20we%20propose%20Weaver%2C%20a%20novel%2C%20end-to-end%20trainable%20multimodal%20reasoning%20agentic%20system.%20Weaver%20empowers%20its%20policy%20model%20to%20dynamically%20invoke%20diverse%20tools%20throughout%20the%20reasoning%20process%2C%20enabling%20progressive%20acquisition%20of%20crucial%20visual%20cues%20and%20construction%20of%20authentic%20multimodal%20reasoning%20trajectories.%20Furthermore%2C%20we%20integrate%20a%20reinforcement%20learning%20algorithm%20to%20allow%20the%20system%20to%20freely%20explore%20strategies%20for%20employing%20and%20combining%20these%20tools%20with%20trajectory-free%20data.%20Extensive%20experiments%20demonstrate%20that%20our%20system%2C%20Weaver%2C%20enhances%20performance%20on%20several%20complex%20video%20reasoning%20benchmarks%2C%20particularly%20those%20involving%20long%20videos.&entry.1838667208=http%3A//arxiv.org/abs/2602.05829v1&entry.124074799=Read"},
{"title": "Neural Implicit 3D Cardiac Shape Reconstruction from Sparse CT Angiography Slices Mimicking 2D Transthoracic Echocardiography Views", "author": "Gino E. Jansen and Carolina Br\u00e1s and R. Nils Planken and Mark J. Schuuring and Berto J. Bouma and Ivana I\u0161gum", "abstract": "Accurate 3D representations of cardiac structures allow quantitative analysis of anatomy and function. In this work, we propose a method for reconstructing complete 3D cardiac shapes from segmentations of sparse planes in CT angiography (CTA) for application in 2D transthoracic echocardiography (TTE). Our method uses a neural implicit function to reconstruct the 3D shape of the cardiac chambers and left-ventricle myocardium from sparse CTA planes. To investigate the feasibility of achieving 3D reconstruction from 2D TTE, we select planes that mimic the standard apical 2D TTE views. During training, a multi-layer perceptron learns shape priors from 3D segmentations of the target structures in CTA. At test time, the network reconstructs 3D cardiac shapes from segmentations of TTE-mimicking CTA planes by jointly optimizing the latent code and the rigid transforms that map the observed planes into 3D space. For each heart, we simulate four realistic apical views, and we compare reconstructed multi-class volumes with the reference CTA volumes. On a held-out set of CTA segmentations, our approach achieves an average Dice coefficient of 0.86 $\\pm$ 0.04 across all structures. Our method also achieves markedly lower volume errors than the clinical standard, Simpson's biplane rule: 4.88 $\\pm$ 4.26 mL vs. 8.14 $\\pm$ 6.04 mL, respectively, for the left ventricle; and 6.40 $\\pm$ 7.37 mL vs. 37.76 $\\pm$ 22.96 mL, respectively, for the left atrium. This suggests that our approach offers a viable route to more accurate 3D chamber quantification in 2D transthoracic echocardiography.", "link": "http://arxiv.org/abs/2602.05884v1", "date": "2026-02-05", "relevancy": 2.7921, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.563}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5562}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Implicit%203D%20Cardiac%20Shape%20Reconstruction%20from%20Sparse%20CT%20Angiography%20Slices%20Mimicking%202D%20Transthoracic%20Echocardiography%20Views&body=Title%3A%20Neural%20Implicit%203D%20Cardiac%20Shape%20Reconstruction%20from%20Sparse%20CT%20Angiography%20Slices%20Mimicking%202D%20Transthoracic%20Echocardiography%20Views%0AAuthor%3A%20Gino%20E.%20Jansen%20and%20Carolina%20Br%C3%A1s%20and%20R.%20Nils%20Planken%20and%20Mark%20J.%20Schuuring%20and%20Berto%20J.%20Bouma%20and%20Ivana%20I%C5%A1gum%0AAbstract%3A%20Accurate%203D%20representations%20of%20cardiac%20structures%20allow%20quantitative%20analysis%20of%20anatomy%20and%20function.%20In%20this%20work%2C%20we%20propose%20a%20method%20for%20reconstructing%20complete%203D%20cardiac%20shapes%20from%20segmentations%20of%20sparse%20planes%20in%20CT%20angiography%20%28CTA%29%20for%20application%20in%202D%20transthoracic%20echocardiography%20%28TTE%29.%20Our%20method%20uses%20a%20neural%20implicit%20function%20to%20reconstruct%20the%203D%20shape%20of%20the%20cardiac%20chambers%20and%20left-ventricle%20myocardium%20from%20sparse%20CTA%20planes.%20To%20investigate%20the%20feasibility%20of%20achieving%203D%20reconstruction%20from%202D%20TTE%2C%20we%20select%20planes%20that%20mimic%20the%20standard%20apical%202D%20TTE%20views.%20During%20training%2C%20a%20multi-layer%20perceptron%20learns%20shape%20priors%20from%203D%20segmentations%20of%20the%20target%20structures%20in%20CTA.%20At%20test%20time%2C%20the%20network%20reconstructs%203D%20cardiac%20shapes%20from%20segmentations%20of%20TTE-mimicking%20CTA%20planes%20by%20jointly%20optimizing%20the%20latent%20code%20and%20the%20rigid%20transforms%20that%20map%20the%20observed%20planes%20into%203D%20space.%20For%20each%20heart%2C%20we%20simulate%20four%20realistic%20apical%20views%2C%20and%20we%20compare%20reconstructed%20multi-class%20volumes%20with%20the%20reference%20CTA%20volumes.%20On%20a%20held-out%20set%20of%20CTA%20segmentations%2C%20our%20approach%20achieves%20an%20average%20Dice%20coefficient%20of%200.86%20%24%5Cpm%24%200.04%20across%20all%20structures.%20Our%20method%20also%20achieves%20markedly%20lower%20volume%20errors%20than%20the%20clinical%20standard%2C%20Simpson%27s%20biplane%20rule%3A%204.88%20%24%5Cpm%24%204.26%20mL%20vs.%208.14%20%24%5Cpm%24%206.04%20mL%2C%20respectively%2C%20for%20the%20left%20ventricle%3B%20and%206.40%20%24%5Cpm%24%207.37%20mL%20vs.%2037.76%20%24%5Cpm%24%2022.96%20mL%2C%20respectively%2C%20for%20the%20left%20atrium.%20This%20suggests%20that%20our%20approach%20offers%20a%20viable%20route%20to%20more%20accurate%203D%20chamber%20quantification%20in%202D%20transthoracic%20echocardiography.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05884v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Implicit%25203D%2520Cardiac%2520Shape%2520Reconstruction%2520from%2520Sparse%2520CT%2520Angiography%2520Slices%2520Mimicking%25202D%2520Transthoracic%2520Echocardiography%2520Views%26entry.906535625%3DGino%2520E.%2520Jansen%2520and%2520Carolina%2520Br%25C3%25A1s%2520and%2520R.%2520Nils%2520Planken%2520and%2520Mark%2520J.%2520Schuuring%2520and%2520Berto%2520J.%2520Bouma%2520and%2520Ivana%2520I%25C5%25A1gum%26entry.1292438233%3DAccurate%25203D%2520representations%2520of%2520cardiac%2520structures%2520allow%2520quantitative%2520analysis%2520of%2520anatomy%2520and%2520function.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520method%2520for%2520reconstructing%2520complete%25203D%2520cardiac%2520shapes%2520from%2520segmentations%2520of%2520sparse%2520planes%2520in%2520CT%2520angiography%2520%2528CTA%2529%2520for%2520application%2520in%25202D%2520transthoracic%2520echocardiography%2520%2528TTE%2529.%2520Our%2520method%2520uses%2520a%2520neural%2520implicit%2520function%2520to%2520reconstruct%2520the%25203D%2520shape%2520of%2520the%2520cardiac%2520chambers%2520and%2520left-ventricle%2520myocardium%2520from%2520sparse%2520CTA%2520planes.%2520To%2520investigate%2520the%2520feasibility%2520of%2520achieving%25203D%2520reconstruction%2520from%25202D%2520TTE%252C%2520we%2520select%2520planes%2520that%2520mimic%2520the%2520standard%2520apical%25202D%2520TTE%2520views.%2520During%2520training%252C%2520a%2520multi-layer%2520perceptron%2520learns%2520shape%2520priors%2520from%25203D%2520segmentations%2520of%2520the%2520target%2520structures%2520in%2520CTA.%2520At%2520test%2520time%252C%2520the%2520network%2520reconstructs%25203D%2520cardiac%2520shapes%2520from%2520segmentations%2520of%2520TTE-mimicking%2520CTA%2520planes%2520by%2520jointly%2520optimizing%2520the%2520latent%2520code%2520and%2520the%2520rigid%2520transforms%2520that%2520map%2520the%2520observed%2520planes%2520into%25203D%2520space.%2520For%2520each%2520heart%252C%2520we%2520simulate%2520four%2520realistic%2520apical%2520views%252C%2520and%2520we%2520compare%2520reconstructed%2520multi-class%2520volumes%2520with%2520the%2520reference%2520CTA%2520volumes.%2520On%2520a%2520held-out%2520set%2520of%2520CTA%2520segmentations%252C%2520our%2520approach%2520achieves%2520an%2520average%2520Dice%2520coefficient%2520of%25200.86%2520%2524%255Cpm%2524%25200.04%2520across%2520all%2520structures.%2520Our%2520method%2520also%2520achieves%2520markedly%2520lower%2520volume%2520errors%2520than%2520the%2520clinical%2520standard%252C%2520Simpson%2527s%2520biplane%2520rule%253A%25204.88%2520%2524%255Cpm%2524%25204.26%2520mL%2520vs.%25208.14%2520%2524%255Cpm%2524%25206.04%2520mL%252C%2520respectively%252C%2520for%2520the%2520left%2520ventricle%253B%2520and%25206.40%2520%2524%255Cpm%2524%25207.37%2520mL%2520vs.%252037.76%2520%2524%255Cpm%2524%252022.96%2520mL%252C%2520respectively%252C%2520for%2520the%2520left%2520atrium.%2520This%2520suggests%2520that%2520our%2520approach%2520offers%2520a%2520viable%2520route%2520to%2520more%2520accurate%25203D%2520chamber%2520quantification%2520in%25202D%2520transthoracic%2520echocardiography.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05884v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Implicit%203D%20Cardiac%20Shape%20Reconstruction%20from%20Sparse%20CT%20Angiography%20Slices%20Mimicking%202D%20Transthoracic%20Echocardiography%20Views&entry.906535625=Gino%20E.%20Jansen%20and%20Carolina%20Br%C3%A1s%20and%20R.%20Nils%20Planken%20and%20Mark%20J.%20Schuuring%20and%20Berto%20J.%20Bouma%20and%20Ivana%20I%C5%A1gum&entry.1292438233=Accurate%203D%20representations%20of%20cardiac%20structures%20allow%20quantitative%20analysis%20of%20anatomy%20and%20function.%20In%20this%20work%2C%20we%20propose%20a%20method%20for%20reconstructing%20complete%203D%20cardiac%20shapes%20from%20segmentations%20of%20sparse%20planes%20in%20CT%20angiography%20%28CTA%29%20for%20application%20in%202D%20transthoracic%20echocardiography%20%28TTE%29.%20Our%20method%20uses%20a%20neural%20implicit%20function%20to%20reconstruct%20the%203D%20shape%20of%20the%20cardiac%20chambers%20and%20left-ventricle%20myocardium%20from%20sparse%20CTA%20planes.%20To%20investigate%20the%20feasibility%20of%20achieving%203D%20reconstruction%20from%202D%20TTE%2C%20we%20select%20planes%20that%20mimic%20the%20standard%20apical%202D%20TTE%20views.%20During%20training%2C%20a%20multi-layer%20perceptron%20learns%20shape%20priors%20from%203D%20segmentations%20of%20the%20target%20structures%20in%20CTA.%20At%20test%20time%2C%20the%20network%20reconstructs%203D%20cardiac%20shapes%20from%20segmentations%20of%20TTE-mimicking%20CTA%20planes%20by%20jointly%20optimizing%20the%20latent%20code%20and%20the%20rigid%20transforms%20that%20map%20the%20observed%20planes%20into%203D%20space.%20For%20each%20heart%2C%20we%20simulate%20four%20realistic%20apical%20views%2C%20and%20we%20compare%20reconstructed%20multi-class%20volumes%20with%20the%20reference%20CTA%20volumes.%20On%20a%20held-out%20set%20of%20CTA%20segmentations%2C%20our%20approach%20achieves%20an%20average%20Dice%20coefficient%20of%200.86%20%24%5Cpm%24%200.04%20across%20all%20structures.%20Our%20method%20also%20achieves%20markedly%20lower%20volume%20errors%20than%20the%20clinical%20standard%2C%20Simpson%27s%20biplane%20rule%3A%204.88%20%24%5Cpm%24%204.26%20mL%20vs.%208.14%20%24%5Cpm%24%206.04%20mL%2C%20respectively%2C%20for%20the%20left%20ventricle%3B%20and%206.40%20%24%5Cpm%24%207.37%20mL%20vs.%2037.76%20%24%5Cpm%24%2022.96%20mL%2C%20respectively%2C%20for%20the%20left%20atrium.%20This%20suggests%20that%20our%20approach%20offers%20a%20viable%20route%20to%20more%20accurate%203D%20chamber%20quantification%20in%202D%20transthoracic%20echocardiography.&entry.1838667208=http%3A//arxiv.org/abs/2602.05884v1&entry.124074799=Read"},
{"title": "V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval", "author": "Dongyang Chen and Chaoyang Wang and Dezhao SU and Xi Xiao and Zeyu Zhang and Jing Xiong and Qing Li and Yuzhang Shang and Shichao Ka", "abstract": "Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.", "link": "http://arxiv.org/abs/2602.06034v1", "date": "2026-02-05", "relevancy": 2.7782, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5651}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-Retrver%3A%20Evidence-Driven%20Agentic%20Reasoning%20for%20Universal%20Multimodal%20Retrieval&body=Title%3A%20V-Retrver%3A%20Evidence-Driven%20Agentic%20Reasoning%20for%20Universal%20Multimodal%20Retrieval%0AAuthor%3A%20Dongyang%20Chen%20and%20Chaoyang%20Wang%20and%20Dezhao%20SU%20and%20Xi%20Xiao%20and%20Zeyu%20Zhang%20and%20Jing%20Xiong%20and%20Qing%20Li%20and%20Yuzhang%20Shang%20and%20Shichao%20Ka%0AAbstract%3A%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20recently%20been%20applied%20to%20universal%20multimodal%20retrieval%2C%20where%20Chain-of-Thought%20%28CoT%29%20reasoning%20improves%20candidate%20reranking.%20However%2C%20existing%20approaches%20remain%20largely%20language-driven%2C%20relying%20on%20static%20visual%20encodings%20and%20lacking%20the%20ability%20to%20actively%20verify%20fine-grained%20visual%20evidence%2C%20which%20often%20leads%20to%20speculative%20reasoning%20in%20visually%20ambiguous%20cases.%20We%20propose%20V-Retrver%2C%20an%20evidence-driven%20retrieval%20framework%20that%20reformulates%20multimodal%20retrieval%20as%20an%20agentic%20reasoning%20process%20grounded%20in%20visual%20inspection.%20V-Retrver%20enables%20an%20MLLM%20to%20selectively%20acquire%20visual%20evidence%20during%20reasoning%20via%20external%20visual%20tools%2C%20performing%20a%20multimodal%20interleaved%20reasoning%20process%20that%20alternates%20between%20hypothesis%20generation%20and%20targeted%20visual%20verification.To%20train%20such%20an%20evidence-gathering%20retrieval%20agent%2C%20we%20adopt%20a%20curriculum-based%20learning%20strategy%20combining%20supervised%20reasoning%20activation%2C%20rejection-based%20refinement%2C%20and%20reinforcement%20learning%20with%20an%20evidence-aligned%20objective.%20Experiments%20across%20multiple%20multimodal%20retrieval%20benchmarks%20demonstrate%20consistent%20improvements%20in%20retrieval%20accuracy%20%28with%2023.0%25%20improvements%20on%20average%29%2C%20perception-driven%20reasoning%20reliability%2C%20and%20generalization.%0ALink%3A%20http%3A//arxiv.org/abs/2602.06034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-Retrver%253A%2520Evidence-Driven%2520Agentic%2520Reasoning%2520for%2520Universal%2520Multimodal%2520Retrieval%26entry.906535625%3DDongyang%2520Chen%2520and%2520Chaoyang%2520Wang%2520and%2520Dezhao%2520SU%2520and%2520Xi%2520Xiao%2520and%2520Zeyu%2520Zhang%2520and%2520Jing%2520Xiong%2520and%2520Qing%2520Li%2520and%2520Yuzhang%2520Shang%2520and%2520Shichao%2520Ka%26entry.1292438233%3DMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520recently%2520been%2520applied%2520to%2520universal%2520multimodal%2520retrieval%252C%2520where%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520improves%2520candidate%2520reranking.%2520However%252C%2520existing%2520approaches%2520remain%2520largely%2520language-driven%252C%2520relying%2520on%2520static%2520visual%2520encodings%2520and%2520lacking%2520the%2520ability%2520to%2520actively%2520verify%2520fine-grained%2520visual%2520evidence%252C%2520which%2520often%2520leads%2520to%2520speculative%2520reasoning%2520in%2520visually%2520ambiguous%2520cases.%2520We%2520propose%2520V-Retrver%252C%2520an%2520evidence-driven%2520retrieval%2520framework%2520that%2520reformulates%2520multimodal%2520retrieval%2520as%2520an%2520agentic%2520reasoning%2520process%2520grounded%2520in%2520visual%2520inspection.%2520V-Retrver%2520enables%2520an%2520MLLM%2520to%2520selectively%2520acquire%2520visual%2520evidence%2520during%2520reasoning%2520via%2520external%2520visual%2520tools%252C%2520performing%2520a%2520multimodal%2520interleaved%2520reasoning%2520process%2520that%2520alternates%2520between%2520hypothesis%2520generation%2520and%2520targeted%2520visual%2520verification.To%2520train%2520such%2520an%2520evidence-gathering%2520retrieval%2520agent%252C%2520we%2520adopt%2520a%2520curriculum-based%2520learning%2520strategy%2520combining%2520supervised%2520reasoning%2520activation%252C%2520rejection-based%2520refinement%252C%2520and%2520reinforcement%2520learning%2520with%2520an%2520evidence-aligned%2520objective.%2520Experiments%2520across%2520multiple%2520multimodal%2520retrieval%2520benchmarks%2520demonstrate%2520consistent%2520improvements%2520in%2520retrieval%2520accuracy%2520%2528with%252023.0%2525%2520improvements%2520on%2520average%2529%252C%2520perception-driven%2520reasoning%2520reliability%252C%2520and%2520generalization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.06034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-Retrver%3A%20Evidence-Driven%20Agentic%20Reasoning%20for%20Universal%20Multimodal%20Retrieval&entry.906535625=Dongyang%20Chen%20and%20Chaoyang%20Wang%20and%20Dezhao%20SU%20and%20Xi%20Xiao%20and%20Zeyu%20Zhang%20and%20Jing%20Xiong%20and%20Qing%20Li%20and%20Yuzhang%20Shang%20and%20Shichao%20Ka&entry.1292438233=Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20recently%20been%20applied%20to%20universal%20multimodal%20retrieval%2C%20where%20Chain-of-Thought%20%28CoT%29%20reasoning%20improves%20candidate%20reranking.%20However%2C%20existing%20approaches%20remain%20largely%20language-driven%2C%20relying%20on%20static%20visual%20encodings%20and%20lacking%20the%20ability%20to%20actively%20verify%20fine-grained%20visual%20evidence%2C%20which%20often%20leads%20to%20speculative%20reasoning%20in%20visually%20ambiguous%20cases.%20We%20propose%20V-Retrver%2C%20an%20evidence-driven%20retrieval%20framework%20that%20reformulates%20multimodal%20retrieval%20as%20an%20agentic%20reasoning%20process%20grounded%20in%20visual%20inspection.%20V-Retrver%20enables%20an%20MLLM%20to%20selectively%20acquire%20visual%20evidence%20during%20reasoning%20via%20external%20visual%20tools%2C%20performing%20a%20multimodal%20interleaved%20reasoning%20process%20that%20alternates%20between%20hypothesis%20generation%20and%20targeted%20visual%20verification.To%20train%20such%20an%20evidence-gathering%20retrieval%20agent%2C%20we%20adopt%20a%20curriculum-based%20learning%20strategy%20combining%20supervised%20reasoning%20activation%2C%20rejection-based%20refinement%2C%20and%20reinforcement%20learning%20with%20an%20evidence-aligned%20objective.%20Experiments%20across%20multiple%20multimodal%20retrieval%20benchmarks%20demonstrate%20consistent%20improvements%20in%20retrieval%20accuracy%20%28with%2023.0%25%20improvements%20on%20average%29%2C%20perception-driven%20reasoning%20reliability%2C%20and%20generalization.&entry.1838667208=http%3A//arxiv.org/abs/2602.06034v1&entry.124074799=Read"},
{"title": "Mapper-GIN: Lightweight Structural Graph Abstraction for Corrupted 3D Point Cloud Classification", "author": "Jeongbin You and Donggun Kim and Sejun Park and Seungsang Oh", "abstract": "Robust 3D point cloud classification is often pursued by scaling up backbones or relying on specialized data augmentation. We instead ask whether structural abstraction alone can improve robustness, and study a simple topology-inspired decomposition based on the Mapper algorithm. We propose Mapper-GIN, a lightweight pipeline that partitions a point cloud into overlapping regions using Mapper (PCA lens, cubical cover, and followed by density-based clustering), constructs a region graph from their overlaps, and performs graph classification with a Graph Isomorphism Network. On the corruption benchmark ModelNet40-C, Mapper-GIN achieves competitive and stable accuracy under Noise and Transformation corruptions with only 0.5M parameters. In contrast to prior approaches that require heavier architectures or additional mechanisms to gain robustness, Mapper-GIN attains strong corruption robustness through simple region-level graph abstraction and GIN message passing. Overall, our results suggest that region-graph structure offers an efficient and interpretable source of robustness for 3D visual recognition.", "link": "http://arxiv.org/abs/2602.05522v1", "date": "2026-02-05", "relevancy": 2.7568, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5571}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5543}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mapper-GIN%3A%20Lightweight%20Structural%20Graph%20Abstraction%20for%20Corrupted%203D%20Point%20Cloud%20Classification&body=Title%3A%20Mapper-GIN%3A%20Lightweight%20Structural%20Graph%20Abstraction%20for%20Corrupted%203D%20Point%20Cloud%20Classification%0AAuthor%3A%20Jeongbin%20You%20and%20Donggun%20Kim%20and%20Sejun%20Park%20and%20Seungsang%20Oh%0AAbstract%3A%20Robust%203D%20point%20cloud%20classification%20is%20often%20pursued%20by%20scaling%20up%20backbones%20or%20relying%20on%20specialized%20data%20augmentation.%20We%20instead%20ask%20whether%20structural%20abstraction%20alone%20can%20improve%20robustness%2C%20and%20study%20a%20simple%20topology-inspired%20decomposition%20based%20on%20the%20Mapper%20algorithm.%20We%20propose%20Mapper-GIN%2C%20a%20lightweight%20pipeline%20that%20partitions%20a%20point%20cloud%20into%20overlapping%20regions%20using%20Mapper%20%28PCA%20lens%2C%20cubical%20cover%2C%20and%20followed%20by%20density-based%20clustering%29%2C%20constructs%20a%20region%20graph%20from%20their%20overlaps%2C%20and%20performs%20graph%20classification%20with%20a%20Graph%20Isomorphism%20Network.%20On%20the%20corruption%20benchmark%20ModelNet40-C%2C%20Mapper-GIN%20achieves%20competitive%20and%20stable%20accuracy%20under%20Noise%20and%20Transformation%20corruptions%20with%20only%200.5M%20parameters.%20In%20contrast%20to%20prior%20approaches%20that%20require%20heavier%20architectures%20or%20additional%20mechanisms%20to%20gain%20robustness%2C%20Mapper-GIN%20attains%20strong%20corruption%20robustness%20through%20simple%20region-level%20graph%20abstraction%20and%20GIN%20message%20passing.%20Overall%2C%20our%20results%20suggest%20that%20region-graph%20structure%20offers%20an%20efficient%20and%20interpretable%20source%20of%20robustness%20for%203D%20visual%20recognition.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapper-GIN%253A%2520Lightweight%2520Structural%2520Graph%2520Abstraction%2520for%2520Corrupted%25203D%2520Point%2520Cloud%2520Classification%26entry.906535625%3DJeongbin%2520You%2520and%2520Donggun%2520Kim%2520and%2520Sejun%2520Park%2520and%2520Seungsang%2520Oh%26entry.1292438233%3DRobust%25203D%2520point%2520cloud%2520classification%2520is%2520often%2520pursued%2520by%2520scaling%2520up%2520backbones%2520or%2520relying%2520on%2520specialized%2520data%2520augmentation.%2520We%2520instead%2520ask%2520whether%2520structural%2520abstraction%2520alone%2520can%2520improve%2520robustness%252C%2520and%2520study%2520a%2520simple%2520topology-inspired%2520decomposition%2520based%2520on%2520the%2520Mapper%2520algorithm.%2520We%2520propose%2520Mapper-GIN%252C%2520a%2520lightweight%2520pipeline%2520that%2520partitions%2520a%2520point%2520cloud%2520into%2520overlapping%2520regions%2520using%2520Mapper%2520%2528PCA%2520lens%252C%2520cubical%2520cover%252C%2520and%2520followed%2520by%2520density-based%2520clustering%2529%252C%2520constructs%2520a%2520region%2520graph%2520from%2520their%2520overlaps%252C%2520and%2520performs%2520graph%2520classification%2520with%2520a%2520Graph%2520Isomorphism%2520Network.%2520On%2520the%2520corruption%2520benchmark%2520ModelNet40-C%252C%2520Mapper-GIN%2520achieves%2520competitive%2520and%2520stable%2520accuracy%2520under%2520Noise%2520and%2520Transformation%2520corruptions%2520with%2520only%25200.5M%2520parameters.%2520In%2520contrast%2520to%2520prior%2520approaches%2520that%2520require%2520heavier%2520architectures%2520or%2520additional%2520mechanisms%2520to%2520gain%2520robustness%252C%2520Mapper-GIN%2520attains%2520strong%2520corruption%2520robustness%2520through%2520simple%2520region-level%2520graph%2520abstraction%2520and%2520GIN%2520message%2520passing.%2520Overall%252C%2520our%2520results%2520suggest%2520that%2520region-graph%2520structure%2520offers%2520an%2520efficient%2520and%2520interpretable%2520source%2520of%2520robustness%2520for%25203D%2520visual%2520recognition.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mapper-GIN%3A%20Lightweight%20Structural%20Graph%20Abstraction%20for%20Corrupted%203D%20Point%20Cloud%20Classification&entry.906535625=Jeongbin%20You%20and%20Donggun%20Kim%20and%20Sejun%20Park%20and%20Seungsang%20Oh&entry.1292438233=Robust%203D%20point%20cloud%20classification%20is%20often%20pursued%20by%20scaling%20up%20backbones%20or%20relying%20on%20specialized%20data%20augmentation.%20We%20instead%20ask%20whether%20structural%20abstraction%20alone%20can%20improve%20robustness%2C%20and%20study%20a%20simple%20topology-inspired%20decomposition%20based%20on%20the%20Mapper%20algorithm.%20We%20propose%20Mapper-GIN%2C%20a%20lightweight%20pipeline%20that%20partitions%20a%20point%20cloud%20into%20overlapping%20regions%20using%20Mapper%20%28PCA%20lens%2C%20cubical%20cover%2C%20and%20followed%20by%20density-based%20clustering%29%2C%20constructs%20a%20region%20graph%20from%20their%20overlaps%2C%20and%20performs%20graph%20classification%20with%20a%20Graph%20Isomorphism%20Network.%20On%20the%20corruption%20benchmark%20ModelNet40-C%2C%20Mapper-GIN%20achieves%20competitive%20and%20stable%20accuracy%20under%20Noise%20and%20Transformation%20corruptions%20with%20only%200.5M%20parameters.%20In%20contrast%20to%20prior%20approaches%20that%20require%20heavier%20architectures%20or%20additional%20mechanisms%20to%20gain%20robustness%2C%20Mapper-GIN%20attains%20strong%20corruption%20robustness%20through%20simple%20region-level%20graph%20abstraction%20and%20GIN%20message%20passing.%20Overall%2C%20our%20results%20suggest%20that%20region-graph%20structure%20offers%20an%20efficient%20and%20interpretable%20source%20of%20robustness%20for%203D%20visual%20recognition.&entry.1838667208=http%3A//arxiv.org/abs/2602.05522v1&entry.124074799=Read"},
{"title": "NEX: Neuron Explore-Exploit Scoring for Label-Free Chain-of-Thought Selection and Model Ranking", "author": "Kang Chen and Zhuoka Feng and Sihan Zhao and Kai Xiong and Junjie Nian and Yaoning Wang and Changyi Xiao and Yixin Cao", "abstract": "Large language models increasingly spend inference compute sampling multiple chain-of-thought traces or searching over merged checkpoints. This shifts the bottleneck from generation to selection, often without supervision on the target distribution. We show entropy-based exploration proxies follow an inverted-U with accuracy, suggesting extra exploration can become redundant and induce overthinking. We propose NEX, a white-box label-free unsupervised scoring framework that views reasoning as alternating E-phase (exploration) and X-phase (exploitation). NEX detects E-phase as spikes in newly activated MLP neurons per token from sparse activation caches, then uses a sticky two-state HMM to infer E-X phases and credits E-introduced neurons by whether they are reused in the following X span. These signals yield interpretable neuron weights and a single Good-Mass Fraction score to rank candidate responses and merged variants without task answers. Across reasoning benchmarks and Qwen3 merge families, NEX computed on a small unlabeled activation set predicts downstream accuracy and identifies better variants; we further validate the E-X signal with human annotations and provide causal evidence via \"Effective-vs-Redundant\" neuron transfer.", "link": "http://arxiv.org/abs/2602.05805v1", "date": "2026-02-05", "relevancy": 2.6581, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5396}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5396}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NEX%3A%20Neuron%20Explore-Exploit%20Scoring%20for%20Label-Free%20Chain-of-Thought%20Selection%20and%20Model%20Ranking&body=Title%3A%20NEX%3A%20Neuron%20Explore-Exploit%20Scoring%20for%20Label-Free%20Chain-of-Thought%20Selection%20and%20Model%20Ranking%0AAuthor%3A%20Kang%20Chen%20and%20Zhuoka%20Feng%20and%20Sihan%20Zhao%20and%20Kai%20Xiong%20and%20Junjie%20Nian%20and%20Yaoning%20Wang%20and%20Changyi%20Xiao%20and%20Yixin%20Cao%0AAbstract%3A%20Large%20language%20models%20increasingly%20spend%20inference%20compute%20sampling%20multiple%20chain-of-thought%20traces%20or%20searching%20over%20merged%20checkpoints.%20This%20shifts%20the%20bottleneck%20from%20generation%20to%20selection%2C%20often%20without%20supervision%20on%20the%20target%20distribution.%20We%20show%20entropy-based%20exploration%20proxies%20follow%20an%20inverted-U%20with%20accuracy%2C%20suggesting%20extra%20exploration%20can%20become%20redundant%20and%20induce%20overthinking.%20We%20propose%20NEX%2C%20a%20white-box%20label-free%20unsupervised%20scoring%20framework%20that%20views%20reasoning%20as%20alternating%20E-phase%20%28exploration%29%20and%20X-phase%20%28exploitation%29.%20NEX%20detects%20E-phase%20as%20spikes%20in%20newly%20activated%20MLP%20neurons%20per%20token%20from%20sparse%20activation%20caches%2C%20then%20uses%20a%20sticky%20two-state%20HMM%20to%20infer%20E-X%20phases%20and%20credits%20E-introduced%20neurons%20by%20whether%20they%20are%20reused%20in%20the%20following%20X%20span.%20These%20signals%20yield%20interpretable%20neuron%20weights%20and%20a%20single%20Good-Mass%20Fraction%20score%20to%20rank%20candidate%20responses%20and%20merged%20variants%20without%20task%20answers.%20Across%20reasoning%20benchmarks%20and%20Qwen3%20merge%20families%2C%20NEX%20computed%20on%20a%20small%20unlabeled%20activation%20set%20predicts%20downstream%20accuracy%20and%20identifies%20better%20variants%3B%20we%20further%20validate%20the%20E-X%20signal%20with%20human%20annotations%20and%20provide%20causal%20evidence%20via%20%22Effective-vs-Redundant%22%20neuron%20transfer.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNEX%253A%2520Neuron%2520Explore-Exploit%2520Scoring%2520for%2520Label-Free%2520Chain-of-Thought%2520Selection%2520and%2520Model%2520Ranking%26entry.906535625%3DKang%2520Chen%2520and%2520Zhuoka%2520Feng%2520and%2520Sihan%2520Zhao%2520and%2520Kai%2520Xiong%2520and%2520Junjie%2520Nian%2520and%2520Yaoning%2520Wang%2520and%2520Changyi%2520Xiao%2520and%2520Yixin%2520Cao%26entry.1292438233%3DLarge%2520language%2520models%2520increasingly%2520spend%2520inference%2520compute%2520sampling%2520multiple%2520chain-of-thought%2520traces%2520or%2520searching%2520over%2520merged%2520checkpoints.%2520This%2520shifts%2520the%2520bottleneck%2520from%2520generation%2520to%2520selection%252C%2520often%2520without%2520supervision%2520on%2520the%2520target%2520distribution.%2520We%2520show%2520entropy-based%2520exploration%2520proxies%2520follow%2520an%2520inverted-U%2520with%2520accuracy%252C%2520suggesting%2520extra%2520exploration%2520can%2520become%2520redundant%2520and%2520induce%2520overthinking.%2520We%2520propose%2520NEX%252C%2520a%2520white-box%2520label-free%2520unsupervised%2520scoring%2520framework%2520that%2520views%2520reasoning%2520as%2520alternating%2520E-phase%2520%2528exploration%2529%2520and%2520X-phase%2520%2528exploitation%2529.%2520NEX%2520detects%2520E-phase%2520as%2520spikes%2520in%2520newly%2520activated%2520MLP%2520neurons%2520per%2520token%2520from%2520sparse%2520activation%2520caches%252C%2520then%2520uses%2520a%2520sticky%2520two-state%2520HMM%2520to%2520infer%2520E-X%2520phases%2520and%2520credits%2520E-introduced%2520neurons%2520by%2520whether%2520they%2520are%2520reused%2520in%2520the%2520following%2520X%2520span.%2520These%2520signals%2520yield%2520interpretable%2520neuron%2520weights%2520and%2520a%2520single%2520Good-Mass%2520Fraction%2520score%2520to%2520rank%2520candidate%2520responses%2520and%2520merged%2520variants%2520without%2520task%2520answers.%2520Across%2520reasoning%2520benchmarks%2520and%2520Qwen3%2520merge%2520families%252C%2520NEX%2520computed%2520on%2520a%2520small%2520unlabeled%2520activation%2520set%2520predicts%2520downstream%2520accuracy%2520and%2520identifies%2520better%2520variants%253B%2520we%2520further%2520validate%2520the%2520E-X%2520signal%2520with%2520human%2520annotations%2520and%2520provide%2520causal%2520evidence%2520via%2520%2522Effective-vs-Redundant%2522%2520neuron%2520transfer.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NEX%3A%20Neuron%20Explore-Exploit%20Scoring%20for%20Label-Free%20Chain-of-Thought%20Selection%20and%20Model%20Ranking&entry.906535625=Kang%20Chen%20and%20Zhuoka%20Feng%20and%20Sihan%20Zhao%20and%20Kai%20Xiong%20and%20Junjie%20Nian%20and%20Yaoning%20Wang%20and%20Changyi%20Xiao%20and%20Yixin%20Cao&entry.1292438233=Large%20language%20models%20increasingly%20spend%20inference%20compute%20sampling%20multiple%20chain-of-thought%20traces%20or%20searching%20over%20merged%20checkpoints.%20This%20shifts%20the%20bottleneck%20from%20generation%20to%20selection%2C%20often%20without%20supervision%20on%20the%20target%20distribution.%20We%20show%20entropy-based%20exploration%20proxies%20follow%20an%20inverted-U%20with%20accuracy%2C%20suggesting%20extra%20exploration%20can%20become%20redundant%20and%20induce%20overthinking.%20We%20propose%20NEX%2C%20a%20white-box%20label-free%20unsupervised%20scoring%20framework%20that%20views%20reasoning%20as%20alternating%20E-phase%20%28exploration%29%20and%20X-phase%20%28exploitation%29.%20NEX%20detects%20E-phase%20as%20spikes%20in%20newly%20activated%20MLP%20neurons%20per%20token%20from%20sparse%20activation%20caches%2C%20then%20uses%20a%20sticky%20two-state%20HMM%20to%20infer%20E-X%20phases%20and%20credits%20E-introduced%20neurons%20by%20whether%20they%20are%20reused%20in%20the%20following%20X%20span.%20These%20signals%20yield%20interpretable%20neuron%20weights%20and%20a%20single%20Good-Mass%20Fraction%20score%20to%20rank%20candidate%20responses%20and%20merged%20variants%20without%20task%20answers.%20Across%20reasoning%20benchmarks%20and%20Qwen3%20merge%20families%2C%20NEX%20computed%20on%20a%20small%20unlabeled%20activation%20set%20predicts%20downstream%20accuracy%20and%20identifies%20better%20variants%3B%20we%20further%20validate%20the%20E-X%20signal%20with%20human%20annotations%20and%20provide%20causal%20evidence%20via%20%22Effective-vs-Redundant%22%20neuron%20transfer.&entry.1838667208=http%3A//arxiv.org/abs/2602.05805v1&entry.124074799=Read"},
{"title": "PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective", "author": "Haokui Zhang and Congyang Ou and Dawei Yan and Peng Wang and Qingsen Yan and Ying Li and Rong Xiao and Chunhua Shen", "abstract": "Recently, reducing redundant visual tokens in vision-language models (VLMs) to accelerate VLM inference has emerged as a hot topic. However, most existing methods rely on heuristics constructed based on inter-visual-token similarity or cross-modal visual-text similarity, which gives rise to certain limitations in compression performance and practical deployment. In contrast, we propose PIO-FVLM from the perspective of inference objectives, which transforms visual token compression into preserving output result invariance and selects tokens primarily by their importance to this goal. Specially, vision tokens are reordered with the guidance of token-level gradient saliency generated by our designed layer-local proxy loss, a coarse constraint from the current layer to the final result. Then the most valuable vision tokens are selected following the non-maximum suppression (NMS) principle. The proposed PIO-FVLM is training-free and compatible with FlashAttention, friendly to practical application and deployment. It can be deployed independently as an encoder-free method, or combined with encoder compression approaches like VisionZip for use as an encoder-involved method. On LLaVA-Next-7B, PIO-FVLM retains just 11.1% of visual tokens but maintains 97.2% of the original performance, with a 2.67$\\times$ prefill speedup, 2.11$\\times$ inference speedup, 6.22$\\times$ lower FLOPs, and 6.05$\\times$ reduced KV Cache overhead. Our code is available at https://github.com/ocy1/PIO-FVLM.", "link": "http://arxiv.org/abs/2602.04657v2", "date": "2026-02-05", "relevancy": 2.6504, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.537}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.537}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PIO-FVLM%3A%20Rethinking%20Training-Free%20Visual%20Token%20Reduction%20for%20VLM%20Acceleration%20from%20an%20Inference-Objective%20Perspective&body=Title%3A%20PIO-FVLM%3A%20Rethinking%20Training-Free%20Visual%20Token%20Reduction%20for%20VLM%20Acceleration%20from%20an%20Inference-Objective%20Perspective%0AAuthor%3A%20Haokui%20Zhang%20and%20Congyang%20Ou%20and%20Dawei%20Yan%20and%20Peng%20Wang%20and%20Qingsen%20Yan%20and%20Ying%20Li%20and%20Rong%20Xiao%20and%20Chunhua%20Shen%0AAbstract%3A%20Recently%2C%20reducing%20redundant%20visual%20tokens%20in%20vision-language%20models%20%28VLMs%29%20to%20accelerate%20VLM%20inference%20has%20emerged%20as%20a%20hot%20topic.%20However%2C%20most%20existing%20methods%20rely%20on%20heuristics%20constructed%20based%20on%20inter-visual-token%20similarity%20or%20cross-modal%20visual-text%20similarity%2C%20which%20gives%20rise%20to%20certain%20limitations%20in%20compression%20performance%20and%20practical%20deployment.%20In%20contrast%2C%20we%20propose%20PIO-FVLM%20from%20the%20perspective%20of%20inference%20objectives%2C%20which%20transforms%20visual%20token%20compression%20into%20preserving%20output%20result%20invariance%20and%20selects%20tokens%20primarily%20by%20their%20importance%20to%20this%20goal.%20Specially%2C%20vision%20tokens%20are%20reordered%20with%20the%20guidance%20of%20token-level%20gradient%20saliency%20generated%20by%20our%20designed%20layer-local%20proxy%20loss%2C%20a%20coarse%20constraint%20from%20the%20current%20layer%20to%20the%20final%20result.%20Then%20the%20most%20valuable%20vision%20tokens%20are%20selected%20following%20the%20non-maximum%20suppression%20%28NMS%29%20principle.%20The%20proposed%20PIO-FVLM%20is%20training-free%20and%20compatible%20with%20FlashAttention%2C%20friendly%20to%20practical%20application%20and%20deployment.%20It%20can%20be%20deployed%20independently%20as%20an%20encoder-free%20method%2C%20or%20combined%20with%20encoder%20compression%20approaches%20like%20VisionZip%20for%20use%20as%20an%20encoder-involved%20method.%20On%20LLaVA-Next-7B%2C%20PIO-FVLM%20retains%20just%2011.1%25%20of%20visual%20tokens%20but%20maintains%2097.2%25%20of%20the%20original%20performance%2C%20with%20a%202.67%24%5Ctimes%24%20prefill%20speedup%2C%202.11%24%5Ctimes%24%20inference%20speedup%2C%206.22%24%5Ctimes%24%20lower%20FLOPs%2C%20and%206.05%24%5Ctimes%24%20reduced%20KV%20Cache%20overhead.%20Our%20code%20is%20available%20at%20https%3A//github.com/ocy1/PIO-FVLM.%0ALink%3A%20http%3A//arxiv.org/abs/2602.04657v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPIO-FVLM%253A%2520Rethinking%2520Training-Free%2520Visual%2520Token%2520Reduction%2520for%2520VLM%2520Acceleration%2520from%2520an%2520Inference-Objective%2520Perspective%26entry.906535625%3DHaokui%2520Zhang%2520and%2520Congyang%2520Ou%2520and%2520Dawei%2520Yan%2520and%2520Peng%2520Wang%2520and%2520Qingsen%2520Yan%2520and%2520Ying%2520Li%2520and%2520Rong%2520Xiao%2520and%2520Chunhua%2520Shen%26entry.1292438233%3DRecently%252C%2520reducing%2520redundant%2520visual%2520tokens%2520in%2520vision-language%2520models%2520%2528VLMs%2529%2520to%2520accelerate%2520VLM%2520inference%2520has%2520emerged%2520as%2520a%2520hot%2520topic.%2520However%252C%2520most%2520existing%2520methods%2520rely%2520on%2520heuristics%2520constructed%2520based%2520on%2520inter-visual-token%2520similarity%2520or%2520cross-modal%2520visual-text%2520similarity%252C%2520which%2520gives%2520rise%2520to%2520certain%2520limitations%2520in%2520compression%2520performance%2520and%2520practical%2520deployment.%2520In%2520contrast%252C%2520we%2520propose%2520PIO-FVLM%2520from%2520the%2520perspective%2520of%2520inference%2520objectives%252C%2520which%2520transforms%2520visual%2520token%2520compression%2520into%2520preserving%2520output%2520result%2520invariance%2520and%2520selects%2520tokens%2520primarily%2520by%2520their%2520importance%2520to%2520this%2520goal.%2520Specially%252C%2520vision%2520tokens%2520are%2520reordered%2520with%2520the%2520guidance%2520of%2520token-level%2520gradient%2520saliency%2520generated%2520by%2520our%2520designed%2520layer-local%2520proxy%2520loss%252C%2520a%2520coarse%2520constraint%2520from%2520the%2520current%2520layer%2520to%2520the%2520final%2520result.%2520Then%2520the%2520most%2520valuable%2520vision%2520tokens%2520are%2520selected%2520following%2520the%2520non-maximum%2520suppression%2520%2528NMS%2529%2520principle.%2520The%2520proposed%2520PIO-FVLM%2520is%2520training-free%2520and%2520compatible%2520with%2520FlashAttention%252C%2520friendly%2520to%2520practical%2520application%2520and%2520deployment.%2520It%2520can%2520be%2520deployed%2520independently%2520as%2520an%2520encoder-free%2520method%252C%2520or%2520combined%2520with%2520encoder%2520compression%2520approaches%2520like%2520VisionZip%2520for%2520use%2520as%2520an%2520encoder-involved%2520method.%2520On%2520LLaVA-Next-7B%252C%2520PIO-FVLM%2520retains%2520just%252011.1%2525%2520of%2520visual%2520tokens%2520but%2520maintains%252097.2%2525%2520of%2520the%2520original%2520performance%252C%2520with%2520a%25202.67%2524%255Ctimes%2524%2520prefill%2520speedup%252C%25202.11%2524%255Ctimes%2524%2520inference%2520speedup%252C%25206.22%2524%255Ctimes%2524%2520lower%2520FLOPs%252C%2520and%25206.05%2524%255Ctimes%2524%2520reduced%2520KV%2520Cache%2520overhead.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/ocy1/PIO-FVLM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.04657v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PIO-FVLM%3A%20Rethinking%20Training-Free%20Visual%20Token%20Reduction%20for%20VLM%20Acceleration%20from%20an%20Inference-Objective%20Perspective&entry.906535625=Haokui%20Zhang%20and%20Congyang%20Ou%20and%20Dawei%20Yan%20and%20Peng%20Wang%20and%20Qingsen%20Yan%20and%20Ying%20Li%20and%20Rong%20Xiao%20and%20Chunhua%20Shen&entry.1292438233=Recently%2C%20reducing%20redundant%20visual%20tokens%20in%20vision-language%20models%20%28VLMs%29%20to%20accelerate%20VLM%20inference%20has%20emerged%20as%20a%20hot%20topic.%20However%2C%20most%20existing%20methods%20rely%20on%20heuristics%20constructed%20based%20on%20inter-visual-token%20similarity%20or%20cross-modal%20visual-text%20similarity%2C%20which%20gives%20rise%20to%20certain%20limitations%20in%20compression%20performance%20and%20practical%20deployment.%20In%20contrast%2C%20we%20propose%20PIO-FVLM%20from%20the%20perspective%20of%20inference%20objectives%2C%20which%20transforms%20visual%20token%20compression%20into%20preserving%20output%20result%20invariance%20and%20selects%20tokens%20primarily%20by%20their%20importance%20to%20this%20goal.%20Specially%2C%20vision%20tokens%20are%20reordered%20with%20the%20guidance%20of%20token-level%20gradient%20saliency%20generated%20by%20our%20designed%20layer-local%20proxy%20loss%2C%20a%20coarse%20constraint%20from%20the%20current%20layer%20to%20the%20final%20result.%20Then%20the%20most%20valuable%20vision%20tokens%20are%20selected%20following%20the%20non-maximum%20suppression%20%28NMS%29%20principle.%20The%20proposed%20PIO-FVLM%20is%20training-free%20and%20compatible%20with%20FlashAttention%2C%20friendly%20to%20practical%20application%20and%20deployment.%20It%20can%20be%20deployed%20independently%20as%20an%20encoder-free%20method%2C%20or%20combined%20with%20encoder%20compression%20approaches%20like%20VisionZip%20for%20use%20as%20an%20encoder-involved%20method.%20On%20LLaVA-Next-7B%2C%20PIO-FVLM%20retains%20just%2011.1%25%20of%20visual%20tokens%20but%20maintains%2097.2%25%20of%20the%20original%20performance%2C%20with%20a%202.67%24%5Ctimes%24%20prefill%20speedup%2C%202.11%24%5Ctimes%24%20inference%20speedup%2C%206.22%24%5Ctimes%24%20lower%20FLOPs%2C%20and%206.05%24%5Ctimes%24%20reduced%20KV%20Cache%20overhead.%20Our%20code%20is%20available%20at%20https%3A//github.com/ocy1/PIO-FVLM.&entry.1838667208=http%3A//arxiv.org/abs/2602.04657v2&entry.124074799=Read"},
{"title": "CLIP-Map: Structured Matrix Mapping for Parameter-Efficient CLIP Compression", "author": "Kangjie Zhang and Wenxuan Huang and Xin Zhou and Boxiang Zhou and Dejia Song and Yuan Xie and Baochang Zhang and Lizhuang Ma and Nemo Chen and Xu Tang and Yao Hu and Shaohui Lin", "abstract": "Contrastive Language-Image Pre-training (CLIP) has achieved widely applications in various computer vision tasks, e.g., text-to-image generation, Image-Text retrieval and Image captioning. However, CLIP suffers from high memory and computation cost, which prohibits its usage to the resource-limited application scenarios. Existing CLIP compression methods typically reduce the size of pre-trained CLIP weights by selecting their subset as weight inheritance for further retraining via mask optimization or important weight measurement. However, these select-based weight inheritance often compromises the feature presentation ability, especially on the extreme compression. In this paper, we propose a novel mapping-based CLIP compression framework, CLIP-Map. It leverages learnable matrices to map and combine pretrained weights by Full-Mapping with Kronecker Factorization, aiming to preserve as much information from the original weights as possible. To mitigate the optimization challenges introduced by the learnable mapping, we propose Diagonal Inheritance Initialization to reduce the distribution shifting problem for efficient and effective mapping learning. Extensive experimental results demonstrate that the proposed CLIP-Map outperforms select-based frameworks across various compression ratios, with particularly significant gains observed under high compression settings.", "link": "http://arxiv.org/abs/2602.05909v1", "date": "2026-02-05", "relevancy": 2.6486, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5787}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5142}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP-Map%3A%20Structured%20Matrix%20Mapping%20for%20Parameter-Efficient%20CLIP%20Compression&body=Title%3A%20CLIP-Map%3A%20Structured%20Matrix%20Mapping%20for%20Parameter-Efficient%20CLIP%20Compression%0AAuthor%3A%20Kangjie%20Zhang%20and%20Wenxuan%20Huang%20and%20Xin%20Zhou%20and%20Boxiang%20Zhou%20and%20Dejia%20Song%20and%20Yuan%20Xie%20and%20Baochang%20Zhang%20and%20Lizhuang%20Ma%20and%20Nemo%20Chen%20and%20Xu%20Tang%20and%20Yao%20Hu%20and%20Shaohui%20Lin%0AAbstract%3A%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20has%20achieved%20widely%20applications%20in%20various%20computer%20vision%20tasks%2C%20e.g.%2C%20text-to-image%20generation%2C%20Image-Text%20retrieval%20and%20Image%20captioning.%20However%2C%20CLIP%20suffers%20from%20high%20memory%20and%20computation%20cost%2C%20which%20prohibits%20its%20usage%20to%20the%20resource-limited%20application%20scenarios.%20Existing%20CLIP%20compression%20methods%20typically%20reduce%20the%20size%20of%20pre-trained%20CLIP%20weights%20by%20selecting%20their%20subset%20as%20weight%20inheritance%20for%20further%20retraining%20via%20mask%20optimization%20or%20important%20weight%20measurement.%20However%2C%20these%20select-based%20weight%20inheritance%20often%20compromises%20the%20feature%20presentation%20ability%2C%20especially%20on%20the%20extreme%20compression.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20mapping-based%20CLIP%20compression%20framework%2C%20CLIP-Map.%20It%20leverages%20learnable%20matrices%20to%20map%20and%20combine%20pretrained%20weights%20by%20Full-Mapping%20with%20Kronecker%20Factorization%2C%20aiming%20to%20preserve%20as%20much%20information%20from%20the%20original%20weights%20as%20possible.%20To%20mitigate%20the%20optimization%20challenges%20introduced%20by%20the%20learnable%20mapping%2C%20we%20propose%20Diagonal%20Inheritance%20Initialization%20to%20reduce%20the%20distribution%20shifting%20problem%20for%20efficient%20and%20effective%20mapping%20learning.%20Extensive%20experimental%20results%20demonstrate%20that%20the%20proposed%20CLIP-Map%20outperforms%20select-based%20frameworks%20across%20various%20compression%20ratios%2C%20with%20particularly%20significant%20gains%20observed%20under%20high%20compression%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP-Map%253A%2520Structured%2520Matrix%2520Mapping%2520for%2520Parameter-Efficient%2520CLIP%2520Compression%26entry.906535625%3DKangjie%2520Zhang%2520and%2520Wenxuan%2520Huang%2520and%2520Xin%2520Zhou%2520and%2520Boxiang%2520Zhou%2520and%2520Dejia%2520Song%2520and%2520Yuan%2520Xie%2520and%2520Baochang%2520Zhang%2520and%2520Lizhuang%2520Ma%2520and%2520Nemo%2520Chen%2520and%2520Xu%2520Tang%2520and%2520Yao%2520Hu%2520and%2520Shaohui%2520Lin%26entry.1292438233%3DContrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520has%2520achieved%2520widely%2520applications%2520in%2520various%2520computer%2520vision%2520tasks%252C%2520e.g.%252C%2520text-to-image%2520generation%252C%2520Image-Text%2520retrieval%2520and%2520Image%2520captioning.%2520However%252C%2520CLIP%2520suffers%2520from%2520high%2520memory%2520and%2520computation%2520cost%252C%2520which%2520prohibits%2520its%2520usage%2520to%2520the%2520resource-limited%2520application%2520scenarios.%2520Existing%2520CLIP%2520compression%2520methods%2520typically%2520reduce%2520the%2520size%2520of%2520pre-trained%2520CLIP%2520weights%2520by%2520selecting%2520their%2520subset%2520as%2520weight%2520inheritance%2520for%2520further%2520retraining%2520via%2520mask%2520optimization%2520or%2520important%2520weight%2520measurement.%2520However%252C%2520these%2520select-based%2520weight%2520inheritance%2520often%2520compromises%2520the%2520feature%2520presentation%2520ability%252C%2520especially%2520on%2520the%2520extreme%2520compression.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520mapping-based%2520CLIP%2520compression%2520framework%252C%2520CLIP-Map.%2520It%2520leverages%2520learnable%2520matrices%2520to%2520map%2520and%2520combine%2520pretrained%2520weights%2520by%2520Full-Mapping%2520with%2520Kronecker%2520Factorization%252C%2520aiming%2520to%2520preserve%2520as%2520much%2520information%2520from%2520the%2520original%2520weights%2520as%2520possible.%2520To%2520mitigate%2520the%2520optimization%2520challenges%2520introduced%2520by%2520the%2520learnable%2520mapping%252C%2520we%2520propose%2520Diagonal%2520Inheritance%2520Initialization%2520to%2520reduce%2520the%2520distribution%2520shifting%2520problem%2520for%2520efficient%2520and%2520effective%2520mapping%2520learning.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520CLIP-Map%2520outperforms%2520select-based%2520frameworks%2520across%2520various%2520compression%2520ratios%252C%2520with%2520particularly%2520significant%2520gains%2520observed%2520under%2520high%2520compression%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-Map%3A%20Structured%20Matrix%20Mapping%20for%20Parameter-Efficient%20CLIP%20Compression&entry.906535625=Kangjie%20Zhang%20and%20Wenxuan%20Huang%20and%20Xin%20Zhou%20and%20Boxiang%20Zhou%20and%20Dejia%20Song%20and%20Yuan%20Xie%20and%20Baochang%20Zhang%20and%20Lizhuang%20Ma%20and%20Nemo%20Chen%20and%20Xu%20Tang%20and%20Yao%20Hu%20and%20Shaohui%20Lin&entry.1292438233=Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20has%20achieved%20widely%20applications%20in%20various%20computer%20vision%20tasks%2C%20e.g.%2C%20text-to-image%20generation%2C%20Image-Text%20retrieval%20and%20Image%20captioning.%20However%2C%20CLIP%20suffers%20from%20high%20memory%20and%20computation%20cost%2C%20which%20prohibits%20its%20usage%20to%20the%20resource-limited%20application%20scenarios.%20Existing%20CLIP%20compression%20methods%20typically%20reduce%20the%20size%20of%20pre-trained%20CLIP%20weights%20by%20selecting%20their%20subset%20as%20weight%20inheritance%20for%20further%20retraining%20via%20mask%20optimization%20or%20important%20weight%20measurement.%20However%2C%20these%20select-based%20weight%20inheritance%20often%20compromises%20the%20feature%20presentation%20ability%2C%20especially%20on%20the%20extreme%20compression.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20mapping-based%20CLIP%20compression%20framework%2C%20CLIP-Map.%20It%20leverages%20learnable%20matrices%20to%20map%20and%20combine%20pretrained%20weights%20by%20Full-Mapping%20with%20Kronecker%20Factorization%2C%20aiming%20to%20preserve%20as%20much%20information%20from%20the%20original%20weights%20as%20possible.%20To%20mitigate%20the%20optimization%20challenges%20introduced%20by%20the%20learnable%20mapping%2C%20we%20propose%20Diagonal%20Inheritance%20Initialization%20to%20reduce%20the%20distribution%20shifting%20problem%20for%20efficient%20and%20effective%20mapping%20learning.%20Extensive%20experimental%20results%20demonstrate%20that%20the%20proposed%20CLIP-Map%20outperforms%20select-based%20frameworks%20across%20various%20compression%20ratios%2C%20with%20particularly%20significant%20gains%20observed%20under%20high%20compression%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2602.05909v1&entry.124074799=Read"},
{"title": "A Mixed Reality System for Robust Manikin Localization in Childbirth Training", "author": "Haojie Cheng and Chang Liu and Abhiram Kanneganti and Mahesh Arjandas Choolani and Arundhati Tushar Gosavi and Eng Tat Khoo", "abstract": "Opportunities for medical students to gain practical experience in vaginal births are increasingly constrained by shortened clinical rotations, patient reluctance, and the unpredictable nature of labour. To alleviate clinicians' instructional burden and enhance trainees' learning efficiency, we introduce a mixed reality (MR) system for childbirth training that combines virtual guidance with tactile manikin interaction, thereby preserving authentic haptic feedback while enabling independent practice without continuous on-site expert supervision. The system extends the passthrough capability of commercial head-mounted displays (HMDs) by spatially calibrating an external RGB-D camera, allowing real-time visual integration of physical training objects. Building on this capability, we implement a coarse-to-fine localization pipeline that first aligns the maternal manikin with fiducial markers to define a delivery region and then registers the pre-scanned neonatal head within this area. This process enables spatially accurate overlay of virtual guiding hands near the manikin, allowing trainees to follow expert trajectories reinforced by haptic interaction. Experimental evaluations demonstrate that the system achieves accurate and stable manikin localization on a standalone headset, ensuring practical deployment without external computing resources. A large-scale user study involving 83 fourth-year medical students was subsequently conducted to compare MR-based and virtual reality (VR)-based childbirth training. Four senior obstetricians independently assessed performance using standardized criteria. Results showed that MR training achieved significantly higher scores in delivery, post-delivery, and overall task performance, and was consistently preferred by trainees over VR training.", "link": "http://arxiv.org/abs/2602.05588v1", "date": "2026-02-05", "relevancy": 2.6471, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5376}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5262}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Mixed%20Reality%20System%20for%20Robust%20Manikin%20Localization%20in%20Childbirth%20Training&body=Title%3A%20A%20Mixed%20Reality%20System%20for%20Robust%20Manikin%20Localization%20in%20Childbirth%20Training%0AAuthor%3A%20Haojie%20Cheng%20and%20Chang%20Liu%20and%20Abhiram%20Kanneganti%20and%20Mahesh%20Arjandas%20Choolani%20and%20Arundhati%20Tushar%20Gosavi%20and%20Eng%20Tat%20Khoo%0AAbstract%3A%20Opportunities%20for%20medical%20students%20to%20gain%20practical%20experience%20in%20vaginal%20births%20are%20increasingly%20constrained%20by%20shortened%20clinical%20rotations%2C%20patient%20reluctance%2C%20and%20the%20unpredictable%20nature%20of%20labour.%20To%20alleviate%20clinicians%27%20instructional%20burden%20and%20enhance%20trainees%27%20learning%20efficiency%2C%20we%20introduce%20a%20mixed%20reality%20%28MR%29%20system%20for%20childbirth%20training%20that%20combines%20virtual%20guidance%20with%20tactile%20manikin%20interaction%2C%20thereby%20preserving%20authentic%20haptic%20feedback%20while%20enabling%20independent%20practice%20without%20continuous%20on-site%20expert%20supervision.%20The%20system%20extends%20the%20passthrough%20capability%20of%20commercial%20head-mounted%20displays%20%28HMDs%29%20by%20spatially%20calibrating%20an%20external%20RGB-D%20camera%2C%20allowing%20real-time%20visual%20integration%20of%20physical%20training%20objects.%20Building%20on%20this%20capability%2C%20we%20implement%20a%20coarse-to-fine%20localization%20pipeline%20that%20first%20aligns%20the%20maternal%20manikin%20with%20fiducial%20markers%20to%20define%20a%20delivery%20region%20and%20then%20registers%20the%20pre-scanned%20neonatal%20head%20within%20this%20area.%20This%20process%20enables%20spatially%20accurate%20overlay%20of%20virtual%20guiding%20hands%20near%20the%20manikin%2C%20allowing%20trainees%20to%20follow%20expert%20trajectories%20reinforced%20by%20haptic%20interaction.%20Experimental%20evaluations%20demonstrate%20that%20the%20system%20achieves%20accurate%20and%20stable%20manikin%20localization%20on%20a%20standalone%20headset%2C%20ensuring%20practical%20deployment%20without%20external%20computing%20resources.%20A%20large-scale%20user%20study%20involving%2083%20fourth-year%20medical%20students%20was%20subsequently%20conducted%20to%20compare%20MR-based%20and%20virtual%20reality%20%28VR%29-based%20childbirth%20training.%20Four%20senior%20obstetricians%20independently%20assessed%20performance%20using%20standardized%20criteria.%20Results%20showed%20that%20MR%20training%20achieved%20significantly%20higher%20scores%20in%20delivery%2C%20post-delivery%2C%20and%20overall%20task%20performance%2C%20and%20was%20consistently%20preferred%20by%20trainees%20over%20VR%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Mixed%2520Reality%2520System%2520for%2520Robust%2520Manikin%2520Localization%2520in%2520Childbirth%2520Training%26entry.906535625%3DHaojie%2520Cheng%2520and%2520Chang%2520Liu%2520and%2520Abhiram%2520Kanneganti%2520and%2520Mahesh%2520Arjandas%2520Choolani%2520and%2520Arundhati%2520Tushar%2520Gosavi%2520and%2520Eng%2520Tat%2520Khoo%26entry.1292438233%3DOpportunities%2520for%2520medical%2520students%2520to%2520gain%2520practical%2520experience%2520in%2520vaginal%2520births%2520are%2520increasingly%2520constrained%2520by%2520shortened%2520clinical%2520rotations%252C%2520patient%2520reluctance%252C%2520and%2520the%2520unpredictable%2520nature%2520of%2520labour.%2520To%2520alleviate%2520clinicians%2527%2520instructional%2520burden%2520and%2520enhance%2520trainees%2527%2520learning%2520efficiency%252C%2520we%2520introduce%2520a%2520mixed%2520reality%2520%2528MR%2529%2520system%2520for%2520childbirth%2520training%2520that%2520combines%2520virtual%2520guidance%2520with%2520tactile%2520manikin%2520interaction%252C%2520thereby%2520preserving%2520authentic%2520haptic%2520feedback%2520while%2520enabling%2520independent%2520practice%2520without%2520continuous%2520on-site%2520expert%2520supervision.%2520The%2520system%2520extends%2520the%2520passthrough%2520capability%2520of%2520commercial%2520head-mounted%2520displays%2520%2528HMDs%2529%2520by%2520spatially%2520calibrating%2520an%2520external%2520RGB-D%2520camera%252C%2520allowing%2520real-time%2520visual%2520integration%2520of%2520physical%2520training%2520objects.%2520Building%2520on%2520this%2520capability%252C%2520we%2520implement%2520a%2520coarse-to-fine%2520localization%2520pipeline%2520that%2520first%2520aligns%2520the%2520maternal%2520manikin%2520with%2520fiducial%2520markers%2520to%2520define%2520a%2520delivery%2520region%2520and%2520then%2520registers%2520the%2520pre-scanned%2520neonatal%2520head%2520within%2520this%2520area.%2520This%2520process%2520enables%2520spatially%2520accurate%2520overlay%2520of%2520virtual%2520guiding%2520hands%2520near%2520the%2520manikin%252C%2520allowing%2520trainees%2520to%2520follow%2520expert%2520trajectories%2520reinforced%2520by%2520haptic%2520interaction.%2520Experimental%2520evaluations%2520demonstrate%2520that%2520the%2520system%2520achieves%2520accurate%2520and%2520stable%2520manikin%2520localization%2520on%2520a%2520standalone%2520headset%252C%2520ensuring%2520practical%2520deployment%2520without%2520external%2520computing%2520resources.%2520A%2520large-scale%2520user%2520study%2520involving%252083%2520fourth-year%2520medical%2520students%2520was%2520subsequently%2520conducted%2520to%2520compare%2520MR-based%2520and%2520virtual%2520reality%2520%2528VR%2529-based%2520childbirth%2520training.%2520Four%2520senior%2520obstetricians%2520independently%2520assessed%2520performance%2520using%2520standardized%2520criteria.%2520Results%2520showed%2520that%2520MR%2520training%2520achieved%2520significantly%2520higher%2520scores%2520in%2520delivery%252C%2520post-delivery%252C%2520and%2520overall%2520task%2520performance%252C%2520and%2520was%2520consistently%2520preferred%2520by%2520trainees%2520over%2520VR%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Mixed%20Reality%20System%20for%20Robust%20Manikin%20Localization%20in%20Childbirth%20Training&entry.906535625=Haojie%20Cheng%20and%20Chang%20Liu%20and%20Abhiram%20Kanneganti%20and%20Mahesh%20Arjandas%20Choolani%20and%20Arundhati%20Tushar%20Gosavi%20and%20Eng%20Tat%20Khoo&entry.1292438233=Opportunities%20for%20medical%20students%20to%20gain%20practical%20experience%20in%20vaginal%20births%20are%20increasingly%20constrained%20by%20shortened%20clinical%20rotations%2C%20patient%20reluctance%2C%20and%20the%20unpredictable%20nature%20of%20labour.%20To%20alleviate%20clinicians%27%20instructional%20burden%20and%20enhance%20trainees%27%20learning%20efficiency%2C%20we%20introduce%20a%20mixed%20reality%20%28MR%29%20system%20for%20childbirth%20training%20that%20combines%20virtual%20guidance%20with%20tactile%20manikin%20interaction%2C%20thereby%20preserving%20authentic%20haptic%20feedback%20while%20enabling%20independent%20practice%20without%20continuous%20on-site%20expert%20supervision.%20The%20system%20extends%20the%20passthrough%20capability%20of%20commercial%20head-mounted%20displays%20%28HMDs%29%20by%20spatially%20calibrating%20an%20external%20RGB-D%20camera%2C%20allowing%20real-time%20visual%20integration%20of%20physical%20training%20objects.%20Building%20on%20this%20capability%2C%20we%20implement%20a%20coarse-to-fine%20localization%20pipeline%20that%20first%20aligns%20the%20maternal%20manikin%20with%20fiducial%20markers%20to%20define%20a%20delivery%20region%20and%20then%20registers%20the%20pre-scanned%20neonatal%20head%20within%20this%20area.%20This%20process%20enables%20spatially%20accurate%20overlay%20of%20virtual%20guiding%20hands%20near%20the%20manikin%2C%20allowing%20trainees%20to%20follow%20expert%20trajectories%20reinforced%20by%20haptic%20interaction.%20Experimental%20evaluations%20demonstrate%20that%20the%20system%20achieves%20accurate%20and%20stable%20manikin%20localization%20on%20a%20standalone%20headset%2C%20ensuring%20practical%20deployment%20without%20external%20computing%20resources.%20A%20large-scale%20user%20study%20involving%2083%20fourth-year%20medical%20students%20was%20subsequently%20conducted%20to%20compare%20MR-based%20and%20virtual%20reality%20%28VR%29-based%20childbirth%20training.%20Four%20senior%20obstetricians%20independently%20assessed%20performance%20using%20standardized%20criteria.%20Results%20showed%20that%20MR%20training%20achieved%20significantly%20higher%20scores%20in%20delivery%2C%20post-delivery%2C%20and%20overall%20task%20performance%2C%20and%20was%20consistently%20preferred%20by%20trainees%20over%20VR%20training.&entry.1838667208=http%3A//arxiv.org/abs/2602.05588v1&entry.124074799=Read"},
{"title": "EEG Foundation Models: Progresses, Benchmarking, and Open Problems", "author": "Dingkun Liu and Yuheng Chen and Zhu Chen and Zhenyao Cui and Yaozhi Wen and Jiayu An and Jingwei Luo and Dongrui Wu", "abstract": "Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.", "link": "http://arxiv.org/abs/2601.17883v2", "date": "2026-02-05", "relevancy": 2.6447, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.565}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EEG%20Foundation%20Models%3A%20Progresses%2C%20Benchmarking%2C%20and%20Open%20Problems&body=Title%3A%20EEG%20Foundation%20Models%3A%20Progresses%2C%20Benchmarking%2C%20and%20Open%20Problems%0AAuthor%3A%20Dingkun%20Liu%20and%20Yuheng%20Chen%20and%20Zhu%20Chen%20and%20Zhenyao%20Cui%20and%20Yaozhi%20Wen%20and%20Jiayu%20An%20and%20Jingwei%20Luo%20and%20Dongrui%20Wu%0AAbstract%3A%20Electroencephalography%20%28EEG%29%20foundation%20models%20have%20recently%20emerged%20as%20a%20promising%20paradigm%20for%20brain-computer%20interfaces%20%28BCIs%29%2C%20aiming%20to%20learn%20transferable%20neural%20representations%20from%20large-scale%20heterogeneous%20recordings.%20Despite%20rapid%20progresses%2C%20there%20lacks%20fair%20and%20comprehensive%20comparisons%20of%20existing%20EEG%20foundation%20models%2C%20due%20to%20inconsistent%20pre-training%20objectives%2C%20preprocessing%20choices%2C%20and%20downstream%20evaluation%20protocols.%20This%20paper%20fills%20this%20gap.%20We%20first%20review%2050%20representative%20models%20and%20organize%20their%20design%20choices%20into%20a%20unified%20taxonomic%20framework%20including%20data%20standardization%2C%20model%20architectures%2C%20and%20self-supervised%20pre-training%20strategies.%20We%20then%20evaluate%2012%20open-source%20foundation%20models%20and%20competitive%20specialist%20baselines%20across%2013%20EEG%20datasets%20spanning%20nine%20BCI%20paradigms.%20Emphasizing%20real-world%20deployments%2C%20we%20consider%20both%20cross-subject%20generalization%20under%20a%20leave-one-subject-out%20protocol%20and%20rapid%20calibration%20under%20a%20within-subject%20few-shot%20setting.%20We%20further%20compare%20full-parameter%20fine-tuning%20with%20linear%20probing%20to%20assess%20the%20transferability%20of%20pre-trained%20representations%2C%20and%20examine%20the%20relationship%20between%20model%20scale%20and%20downstream%20performance.%20Our%20results%20indicate%20that%3A%201%29%20linear%20probing%20is%20frequently%20insufficient%3B%202%29%20specialist%20models%20trained%20from%20scratch%20remain%20competitive%20across%20many%20tasks%3B%20and%2C%203%29%20larger%20foundation%20models%20do%20not%20necessarily%20yield%20better%20generalization%20performance%20under%20current%20data%20regimes%20and%20training%20practices.%0ALink%3A%20http%3A//arxiv.org/abs/2601.17883v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEEG%2520Foundation%2520Models%253A%2520Progresses%252C%2520Benchmarking%252C%2520and%2520Open%2520Problems%26entry.906535625%3DDingkun%2520Liu%2520and%2520Yuheng%2520Chen%2520and%2520Zhu%2520Chen%2520and%2520Zhenyao%2520Cui%2520and%2520Yaozhi%2520Wen%2520and%2520Jiayu%2520An%2520and%2520Jingwei%2520Luo%2520and%2520Dongrui%2520Wu%26entry.1292438233%3DElectroencephalography%2520%2528EEG%2529%2520foundation%2520models%2520have%2520recently%2520emerged%2520as%2520a%2520promising%2520paradigm%2520for%2520brain-computer%2520interfaces%2520%2528BCIs%2529%252C%2520aiming%2520to%2520learn%2520transferable%2520neural%2520representations%2520from%2520large-scale%2520heterogeneous%2520recordings.%2520Despite%2520rapid%2520progresses%252C%2520there%2520lacks%2520fair%2520and%2520comprehensive%2520comparisons%2520of%2520existing%2520EEG%2520foundation%2520models%252C%2520due%2520to%2520inconsistent%2520pre-training%2520objectives%252C%2520preprocessing%2520choices%252C%2520and%2520downstream%2520evaluation%2520protocols.%2520This%2520paper%2520fills%2520this%2520gap.%2520We%2520first%2520review%252050%2520representative%2520models%2520and%2520organize%2520their%2520design%2520choices%2520into%2520a%2520unified%2520taxonomic%2520framework%2520including%2520data%2520standardization%252C%2520model%2520architectures%252C%2520and%2520self-supervised%2520pre-training%2520strategies.%2520We%2520then%2520evaluate%252012%2520open-source%2520foundation%2520models%2520and%2520competitive%2520specialist%2520baselines%2520across%252013%2520EEG%2520datasets%2520spanning%2520nine%2520BCI%2520paradigms.%2520Emphasizing%2520real-world%2520deployments%252C%2520we%2520consider%2520both%2520cross-subject%2520generalization%2520under%2520a%2520leave-one-subject-out%2520protocol%2520and%2520rapid%2520calibration%2520under%2520a%2520within-subject%2520few-shot%2520setting.%2520We%2520further%2520compare%2520full-parameter%2520fine-tuning%2520with%2520linear%2520probing%2520to%2520assess%2520the%2520transferability%2520of%2520pre-trained%2520representations%252C%2520and%2520examine%2520the%2520relationship%2520between%2520model%2520scale%2520and%2520downstream%2520performance.%2520Our%2520results%2520indicate%2520that%253A%25201%2529%2520linear%2520probing%2520is%2520frequently%2520insufficient%253B%25202%2529%2520specialist%2520models%2520trained%2520from%2520scratch%2520remain%2520competitive%2520across%2520many%2520tasks%253B%2520and%252C%25203%2529%2520larger%2520foundation%2520models%2520do%2520not%2520necessarily%2520yield%2520better%2520generalization%2520performance%2520under%2520current%2520data%2520regimes%2520and%2520training%2520practices.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.17883v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EEG%20Foundation%20Models%3A%20Progresses%2C%20Benchmarking%2C%20and%20Open%20Problems&entry.906535625=Dingkun%20Liu%20and%20Yuheng%20Chen%20and%20Zhu%20Chen%20and%20Zhenyao%20Cui%20and%20Yaozhi%20Wen%20and%20Jiayu%20An%20and%20Jingwei%20Luo%20and%20Dongrui%20Wu&entry.1292438233=Electroencephalography%20%28EEG%29%20foundation%20models%20have%20recently%20emerged%20as%20a%20promising%20paradigm%20for%20brain-computer%20interfaces%20%28BCIs%29%2C%20aiming%20to%20learn%20transferable%20neural%20representations%20from%20large-scale%20heterogeneous%20recordings.%20Despite%20rapid%20progresses%2C%20there%20lacks%20fair%20and%20comprehensive%20comparisons%20of%20existing%20EEG%20foundation%20models%2C%20due%20to%20inconsistent%20pre-training%20objectives%2C%20preprocessing%20choices%2C%20and%20downstream%20evaluation%20protocols.%20This%20paper%20fills%20this%20gap.%20We%20first%20review%2050%20representative%20models%20and%20organize%20their%20design%20choices%20into%20a%20unified%20taxonomic%20framework%20including%20data%20standardization%2C%20model%20architectures%2C%20and%20self-supervised%20pre-training%20strategies.%20We%20then%20evaluate%2012%20open-source%20foundation%20models%20and%20competitive%20specialist%20baselines%20across%2013%20EEG%20datasets%20spanning%20nine%20BCI%20paradigms.%20Emphasizing%20real-world%20deployments%2C%20we%20consider%20both%20cross-subject%20generalization%20under%20a%20leave-one-subject-out%20protocol%20and%20rapid%20calibration%20under%20a%20within-subject%20few-shot%20setting.%20We%20further%20compare%20full-parameter%20fine-tuning%20with%20linear%20probing%20to%20assess%20the%20transferability%20of%20pre-trained%20representations%2C%20and%20examine%20the%20relationship%20between%20model%20scale%20and%20downstream%20performance.%20Our%20results%20indicate%20that%3A%201%29%20linear%20probing%20is%20frequently%20insufficient%3B%202%29%20specialist%20models%20trained%20from%20scratch%20remain%20competitive%20across%20many%20tasks%3B%20and%2C%203%29%20larger%20foundation%20models%20do%20not%20necessarily%20yield%20better%20generalization%20performance%20under%20current%20data%20regimes%20and%20training%20practices.&entry.1838667208=http%3A//arxiv.org/abs/2601.17883v2&entry.124074799=Read"},
{"title": "Generalization of Self-Supervised Vision Transformers for Protein Localization Across Microscopy Domains", "author": "Ben Isselmann and Dilara G\u00f6ksu and Andreas Weinmann", "abstract": "Task-specific microscopy datasets are often too small to train deep learning models that learn robust feature representations. Self-supervised learning (SSL) can mitigate this by pretraining on large unlabeled datasets, but it remains unclear how well such representations transfer across microscopy domains with different staining protocols and channel configurations. We investigate the cross-domain transferability of DINO-pretrained Vision Transformers for protein localization on the OpenCell dataset. We generate image embeddings using three DINO backbones pretrained on ImageNet-1k, the Human Protein Atlas (HPA), and OpenCell, and evaluate them by training a supervised classification head on OpenCell labels. All pretrained models transfer well, with the microscopy-specific HPA-pretrained model achieving the best performance (mean macro $F_1$-score = 0.8221 \\pm 0.0062), slightly outperforming a DINO model trained directly on OpenCell (0.8057 \\pm 0.0090). These results highlight the value of large-scale pretraining and indicate that domain-relevant SSL representations can generalize effectively to related but distinct microscopy datasets, enabling strong downstream performance even when task-specific labeled data are limited.", "link": "http://arxiv.org/abs/2602.05527v1", "date": "2026-02-05", "relevancy": 2.6368, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5356}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5232}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalization%20of%20Self-Supervised%20Vision%20Transformers%20for%20Protein%20Localization%20Across%20Microscopy%20Domains&body=Title%3A%20Generalization%20of%20Self-Supervised%20Vision%20Transformers%20for%20Protein%20Localization%20Across%20Microscopy%20Domains%0AAuthor%3A%20Ben%20Isselmann%20and%20Dilara%20G%C3%B6ksu%20and%20Andreas%20Weinmann%0AAbstract%3A%20Task-specific%20microscopy%20datasets%20are%20often%20too%20small%20to%20train%20deep%20learning%20models%20that%20learn%20robust%20feature%20representations.%20Self-supervised%20learning%20%28SSL%29%20can%20mitigate%20this%20by%20pretraining%20on%20large%20unlabeled%20datasets%2C%20but%20it%20remains%20unclear%20how%20well%20such%20representations%20transfer%20across%20microscopy%20domains%20with%20different%20staining%20protocols%20and%20channel%20configurations.%20We%20investigate%20the%20cross-domain%20transferability%20of%20DINO-pretrained%20Vision%20Transformers%20for%20protein%20localization%20on%20the%20OpenCell%20dataset.%20We%20generate%20image%20embeddings%20using%20three%20DINO%20backbones%20pretrained%20on%20ImageNet-1k%2C%20the%20Human%20Protein%20Atlas%20%28HPA%29%2C%20and%20OpenCell%2C%20and%20evaluate%20them%20by%20training%20a%20supervised%20classification%20head%20on%20OpenCell%20labels.%20All%20pretrained%20models%20transfer%20well%2C%20with%20the%20microscopy-specific%20HPA-pretrained%20model%20achieving%20the%20best%20performance%20%28mean%20macro%20%24F_1%24-score%20%3D%200.8221%20%5Cpm%200.0062%29%2C%20slightly%20outperforming%20a%20DINO%20model%20trained%20directly%20on%20OpenCell%20%280.8057%20%5Cpm%200.0090%29.%20These%20results%20highlight%20the%20value%20of%20large-scale%20pretraining%20and%20indicate%20that%20domain-relevant%20SSL%20representations%20can%20generalize%20effectively%20to%20related%20but%20distinct%20microscopy%20datasets%2C%20enabling%20strong%20downstream%20performance%20even%20when%20task-specific%20labeled%20data%20are%20limited.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralization%2520of%2520Self-Supervised%2520Vision%2520Transformers%2520for%2520Protein%2520Localization%2520Across%2520Microscopy%2520Domains%26entry.906535625%3DBen%2520Isselmann%2520and%2520Dilara%2520G%25C3%25B6ksu%2520and%2520Andreas%2520Weinmann%26entry.1292438233%3DTask-specific%2520microscopy%2520datasets%2520are%2520often%2520too%2520small%2520to%2520train%2520deep%2520learning%2520models%2520that%2520learn%2520robust%2520feature%2520representations.%2520Self-supervised%2520learning%2520%2528SSL%2529%2520can%2520mitigate%2520this%2520by%2520pretraining%2520on%2520large%2520unlabeled%2520datasets%252C%2520but%2520it%2520remains%2520unclear%2520how%2520well%2520such%2520representations%2520transfer%2520across%2520microscopy%2520domains%2520with%2520different%2520staining%2520protocols%2520and%2520channel%2520configurations.%2520We%2520investigate%2520the%2520cross-domain%2520transferability%2520of%2520DINO-pretrained%2520Vision%2520Transformers%2520for%2520protein%2520localization%2520on%2520the%2520OpenCell%2520dataset.%2520We%2520generate%2520image%2520embeddings%2520using%2520three%2520DINO%2520backbones%2520pretrained%2520on%2520ImageNet-1k%252C%2520the%2520Human%2520Protein%2520Atlas%2520%2528HPA%2529%252C%2520and%2520OpenCell%252C%2520and%2520evaluate%2520them%2520by%2520training%2520a%2520supervised%2520classification%2520head%2520on%2520OpenCell%2520labels.%2520All%2520pretrained%2520models%2520transfer%2520well%252C%2520with%2520the%2520microscopy-specific%2520HPA-pretrained%2520model%2520achieving%2520the%2520best%2520performance%2520%2528mean%2520macro%2520%2524F_1%2524-score%2520%253D%25200.8221%2520%255Cpm%25200.0062%2529%252C%2520slightly%2520outperforming%2520a%2520DINO%2520model%2520trained%2520directly%2520on%2520OpenCell%2520%25280.8057%2520%255Cpm%25200.0090%2529.%2520These%2520results%2520highlight%2520the%2520value%2520of%2520large-scale%2520pretraining%2520and%2520indicate%2520that%2520domain-relevant%2520SSL%2520representations%2520can%2520generalize%2520effectively%2520to%2520related%2520but%2520distinct%2520microscopy%2520datasets%252C%2520enabling%2520strong%2520downstream%2520performance%2520even%2520when%2520task-specific%2520labeled%2520data%2520are%2520limited.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20of%20Self-Supervised%20Vision%20Transformers%20for%20Protein%20Localization%20Across%20Microscopy%20Domains&entry.906535625=Ben%20Isselmann%20and%20Dilara%20G%C3%B6ksu%20and%20Andreas%20Weinmann&entry.1292438233=Task-specific%20microscopy%20datasets%20are%20often%20too%20small%20to%20train%20deep%20learning%20models%20that%20learn%20robust%20feature%20representations.%20Self-supervised%20learning%20%28SSL%29%20can%20mitigate%20this%20by%20pretraining%20on%20large%20unlabeled%20datasets%2C%20but%20it%20remains%20unclear%20how%20well%20such%20representations%20transfer%20across%20microscopy%20domains%20with%20different%20staining%20protocols%20and%20channel%20configurations.%20We%20investigate%20the%20cross-domain%20transferability%20of%20DINO-pretrained%20Vision%20Transformers%20for%20protein%20localization%20on%20the%20OpenCell%20dataset.%20We%20generate%20image%20embeddings%20using%20three%20DINO%20backbones%20pretrained%20on%20ImageNet-1k%2C%20the%20Human%20Protein%20Atlas%20%28HPA%29%2C%20and%20OpenCell%2C%20and%20evaluate%20them%20by%20training%20a%20supervised%20classification%20head%20on%20OpenCell%20labels.%20All%20pretrained%20models%20transfer%20well%2C%20with%20the%20microscopy-specific%20HPA-pretrained%20model%20achieving%20the%20best%20performance%20%28mean%20macro%20%24F_1%24-score%20%3D%200.8221%20%5Cpm%200.0062%29%2C%20slightly%20outperforming%20a%20DINO%20model%20trained%20directly%20on%20OpenCell%20%280.8057%20%5Cpm%200.0090%29.%20These%20results%20highlight%20the%20value%20of%20large-scale%20pretraining%20and%20indicate%20that%20domain-relevant%20SSL%20representations%20can%20generalize%20effectively%20to%20related%20but%20distinct%20microscopy%20datasets%2C%20enabling%20strong%20downstream%20performance%20even%20when%20task-specific%20labeled%20data%20are%20limited.&entry.1838667208=http%3A//arxiv.org/abs/2602.05527v1&entry.124074799=Read"},
{"title": "DARWIN: Dynamic Agentically Rewriting Self-Improving Network", "author": "Henry Jiang", "abstract": "DARWIN is an evolutionary GPT model, utilizing a genetic-algorithm like optimization structure with several independent GPT agents being trained individually using unique training code. Each iteration, the GPT models are prompted to modify the training code of one another in an attempt to improve their performance in a mutation-like manner, and the best GPT agents are then benchmarked and selected for the next iteration by genetic algorithm. For demonstration purposes and due to budget and time constraints, OpenAI API is used to prompt training code improvements and the nanoGPT framework is used as the training code. DARWIN also utilizes persistent JSON-based memory files to track previous reasoning and changes to code to correlate with improvement to model performance. and a bidirectional interface for HITL intervention allowing the model to request upgrades such as additional datasets, training scripts, and restructuring of file hierarchies. In experiments, DARWIN achieved a 1.26 percent improvement in model FLOPS utilization (MFU) and a 2.07 percent improvement to perplexity in 5 iterations of training over baseline configurations, demonstrating promising capabilities as a foundation for scaling evolutionary GPT training.", "link": "http://arxiv.org/abs/2602.05848v1", "date": "2026-02-05", "relevancy": 2.6198, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5538}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5239}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DARWIN%3A%20Dynamic%20Agentically%20Rewriting%20Self-Improving%20Network&body=Title%3A%20DARWIN%3A%20Dynamic%20Agentically%20Rewriting%20Self-Improving%20Network%0AAuthor%3A%20Henry%20Jiang%0AAbstract%3A%20DARWIN%20is%20an%20evolutionary%20GPT%20model%2C%20utilizing%20a%20genetic-algorithm%20like%20optimization%20structure%20with%20several%20independent%20GPT%20agents%20being%20trained%20individually%20using%20unique%20training%20code.%20Each%20iteration%2C%20the%20GPT%20models%20are%20prompted%20to%20modify%20the%20training%20code%20of%20one%20another%20in%20an%20attempt%20to%20improve%20their%20performance%20in%20a%20mutation-like%20manner%2C%20and%20the%20best%20GPT%20agents%20are%20then%20benchmarked%20and%20selected%20for%20the%20next%20iteration%20by%20genetic%20algorithm.%20For%20demonstration%20purposes%20and%20due%20to%20budget%20and%20time%20constraints%2C%20OpenAI%20API%20is%20used%20to%20prompt%20training%20code%20improvements%20and%20the%20nanoGPT%20framework%20is%20used%20as%20the%20training%20code.%20DARWIN%20also%20utilizes%20persistent%20JSON-based%20memory%20files%20to%20track%20previous%20reasoning%20and%20changes%20to%20code%20to%20correlate%20with%20improvement%20to%20model%20performance.%20and%20a%20bidirectional%20interface%20for%20HITL%20intervention%20allowing%20the%20model%20to%20request%20upgrades%20such%20as%20additional%20datasets%2C%20training%20scripts%2C%20and%20restructuring%20of%20file%20hierarchies.%20In%20experiments%2C%20DARWIN%20achieved%20a%201.26%20percent%20improvement%20in%20model%20FLOPS%20utilization%20%28MFU%29%20and%20a%202.07%20percent%20improvement%20to%20perplexity%20in%205%20iterations%20of%20training%20over%20baseline%20configurations%2C%20demonstrating%20promising%20capabilities%20as%20a%20foundation%20for%20scaling%20evolutionary%20GPT%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05848v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDARWIN%253A%2520Dynamic%2520Agentically%2520Rewriting%2520Self-Improving%2520Network%26entry.906535625%3DHenry%2520Jiang%26entry.1292438233%3DDARWIN%2520is%2520an%2520evolutionary%2520GPT%2520model%252C%2520utilizing%2520a%2520genetic-algorithm%2520like%2520optimization%2520structure%2520with%2520several%2520independent%2520GPT%2520agents%2520being%2520trained%2520individually%2520using%2520unique%2520training%2520code.%2520Each%2520iteration%252C%2520the%2520GPT%2520models%2520are%2520prompted%2520to%2520modify%2520the%2520training%2520code%2520of%2520one%2520another%2520in%2520an%2520attempt%2520to%2520improve%2520their%2520performance%2520in%2520a%2520mutation-like%2520manner%252C%2520and%2520the%2520best%2520GPT%2520agents%2520are%2520then%2520benchmarked%2520and%2520selected%2520for%2520the%2520next%2520iteration%2520by%2520genetic%2520algorithm.%2520For%2520demonstration%2520purposes%2520and%2520due%2520to%2520budget%2520and%2520time%2520constraints%252C%2520OpenAI%2520API%2520is%2520used%2520to%2520prompt%2520training%2520code%2520improvements%2520and%2520the%2520nanoGPT%2520framework%2520is%2520used%2520as%2520the%2520training%2520code.%2520DARWIN%2520also%2520utilizes%2520persistent%2520JSON-based%2520memory%2520files%2520to%2520track%2520previous%2520reasoning%2520and%2520changes%2520to%2520code%2520to%2520correlate%2520with%2520improvement%2520to%2520model%2520performance.%2520and%2520a%2520bidirectional%2520interface%2520for%2520HITL%2520intervention%2520allowing%2520the%2520model%2520to%2520request%2520upgrades%2520such%2520as%2520additional%2520datasets%252C%2520training%2520scripts%252C%2520and%2520restructuring%2520of%2520file%2520hierarchies.%2520In%2520experiments%252C%2520DARWIN%2520achieved%2520a%25201.26%2520percent%2520improvement%2520in%2520model%2520FLOPS%2520utilization%2520%2528MFU%2529%2520and%2520a%25202.07%2520percent%2520improvement%2520to%2520perplexity%2520in%25205%2520iterations%2520of%2520training%2520over%2520baseline%2520configurations%252C%2520demonstrating%2520promising%2520capabilities%2520as%2520a%2520foundation%2520for%2520scaling%2520evolutionary%2520GPT%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05848v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DARWIN%3A%20Dynamic%20Agentically%20Rewriting%20Self-Improving%20Network&entry.906535625=Henry%20Jiang&entry.1292438233=DARWIN%20is%20an%20evolutionary%20GPT%20model%2C%20utilizing%20a%20genetic-algorithm%20like%20optimization%20structure%20with%20several%20independent%20GPT%20agents%20being%20trained%20individually%20using%20unique%20training%20code.%20Each%20iteration%2C%20the%20GPT%20models%20are%20prompted%20to%20modify%20the%20training%20code%20of%20one%20another%20in%20an%20attempt%20to%20improve%20their%20performance%20in%20a%20mutation-like%20manner%2C%20and%20the%20best%20GPT%20agents%20are%20then%20benchmarked%20and%20selected%20for%20the%20next%20iteration%20by%20genetic%20algorithm.%20For%20demonstration%20purposes%20and%20due%20to%20budget%20and%20time%20constraints%2C%20OpenAI%20API%20is%20used%20to%20prompt%20training%20code%20improvements%20and%20the%20nanoGPT%20framework%20is%20used%20as%20the%20training%20code.%20DARWIN%20also%20utilizes%20persistent%20JSON-based%20memory%20files%20to%20track%20previous%20reasoning%20and%20changes%20to%20code%20to%20correlate%20with%20improvement%20to%20model%20performance.%20and%20a%20bidirectional%20interface%20for%20HITL%20intervention%20allowing%20the%20model%20to%20request%20upgrades%20such%20as%20additional%20datasets%2C%20training%20scripts%2C%20and%20restructuring%20of%20file%20hierarchies.%20In%20experiments%2C%20DARWIN%20achieved%20a%201.26%20percent%20improvement%20in%20model%20FLOPS%20utilization%20%28MFU%29%20and%20a%202.07%20percent%20improvement%20to%20perplexity%20in%205%20iterations%20of%20training%20over%20baseline%20configurations%2C%20demonstrating%20promising%20capabilities%20as%20a%20foundation%20for%20scaling%20evolutionary%20GPT%20training.&entry.1838667208=http%3A//arxiv.org/abs/2602.05848v1&entry.124074799=Read"},
{"title": "Shared LoRA Subspaces for almost Strict Continual Learning", "author": "Prakhar Kaushik and Ankit Vaidya and Shravan Chaudhari and Rama Chellappa and Alan Yuille", "abstract": "Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.", "link": "http://arxiv.org/abs/2602.06043v1", "date": "2026-02-05", "relevancy": 2.5885, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5406}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5089}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shared%20LoRA%20Subspaces%20for%20almost%20Strict%20Continual%20Learning&body=Title%3A%20Shared%20LoRA%20Subspaces%20for%20almost%20Strict%20Continual%20Learning%0AAuthor%3A%20Prakhar%20Kaushik%20and%20Ankit%20Vaidya%20and%20Shravan%20Chaudhari%20and%20Rama%20Chellappa%20and%20Alan%20Yuille%0AAbstract%3A%20Adapting%20large%20pretrained%20models%20to%20new%20tasks%20efficiently%20and%20continually%20is%20crucial%20for%20real-world%20deployment%20but%20remains%20challenging%20due%20to%20catastrophic%20forgetting%20and%20the%20high%20cost%20of%20retraining.%20While%20parameter-efficient%20tuning%20methods%20like%20low%20rank%20adaptation%20%28LoRA%29%20reduce%20computational%20demands%2C%20they%20lack%20mechanisms%20for%20strict%20continual%20learning%20and%20knowledge%20integration%2C%20without%20relying%20on%20data%20replay%2C%20or%20multiple%20adapters.%20We%20propose%20Share%2C%20a%20novel%20approach%20to%20parameter%20efficient%20continual%20finetuning%20that%20learns%20and%20dynamically%20updates%20a%20single%2C%20shared%20low-rank%20subspace%2C%20enabling%20seamless%20adaptation%20across%20multiple%20tasks%20and%20modalities.%20Share%20constructs%20a%20foundational%20subspace%20that%20extracts%20core%20knowledge%20from%20past%20tasks%20and%20incrementally%20integrates%20new%20information%20by%20identifying%20essential%20subspace%20directions.%20Knowledge%20from%20each%20new%20task%20is%20incorporated%20into%20this%20evolving%20subspace%2C%20facilitating%20forward%20knowledge%20transfer%2C%20while%20minimizing%20catastrophic%20interference.%20This%20approach%20achieves%20up%20to%20100x%20parameter%20reduction%20and%20281x%20memory%20savings%20over%20traditional%20LoRA%20methods%2C%20maintaining%20performance%20comparable%20to%20jointly%20trained%20models.%20A%20single%20Share%20model%20can%20replace%20hundreds%20of%20task-specific%20LoRA%20adapters%2C%20supporting%20scalable%2C%20asynchronous%20continual%20learning.%20Experiments%20across%20image%20classification%2C%20natural%20language%20understanding%2C%203D%20pose%20estimation%2C%20and%20text-to-image%20generation%20validate%20its%20effectiveness%2C%20making%20Share%20a%20practical%20and%20scalable%20solution%20for%20lifelong%20learning%20in%20large-scale%20AI%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2602.06043v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShared%2520LoRA%2520Subspaces%2520for%2520almost%2520Strict%2520Continual%2520Learning%26entry.906535625%3DPrakhar%2520Kaushik%2520and%2520Ankit%2520Vaidya%2520and%2520Shravan%2520Chaudhari%2520and%2520Rama%2520Chellappa%2520and%2520Alan%2520Yuille%26entry.1292438233%3DAdapting%2520large%2520pretrained%2520models%2520to%2520new%2520tasks%2520efficiently%2520and%2520continually%2520is%2520crucial%2520for%2520real-world%2520deployment%2520but%2520remains%2520challenging%2520due%2520to%2520catastrophic%2520forgetting%2520and%2520the%2520high%2520cost%2520of%2520retraining.%2520While%2520parameter-efficient%2520tuning%2520methods%2520like%2520low%2520rank%2520adaptation%2520%2528LoRA%2529%2520reduce%2520computational%2520demands%252C%2520they%2520lack%2520mechanisms%2520for%2520strict%2520continual%2520learning%2520and%2520knowledge%2520integration%252C%2520without%2520relying%2520on%2520data%2520replay%252C%2520or%2520multiple%2520adapters.%2520We%2520propose%2520Share%252C%2520a%2520novel%2520approach%2520to%2520parameter%2520efficient%2520continual%2520finetuning%2520that%2520learns%2520and%2520dynamically%2520updates%2520a%2520single%252C%2520shared%2520low-rank%2520subspace%252C%2520enabling%2520seamless%2520adaptation%2520across%2520multiple%2520tasks%2520and%2520modalities.%2520Share%2520constructs%2520a%2520foundational%2520subspace%2520that%2520extracts%2520core%2520knowledge%2520from%2520past%2520tasks%2520and%2520incrementally%2520integrates%2520new%2520information%2520by%2520identifying%2520essential%2520subspace%2520directions.%2520Knowledge%2520from%2520each%2520new%2520task%2520is%2520incorporated%2520into%2520this%2520evolving%2520subspace%252C%2520facilitating%2520forward%2520knowledge%2520transfer%252C%2520while%2520minimizing%2520catastrophic%2520interference.%2520This%2520approach%2520achieves%2520up%2520to%2520100x%2520parameter%2520reduction%2520and%2520281x%2520memory%2520savings%2520over%2520traditional%2520LoRA%2520methods%252C%2520maintaining%2520performance%2520comparable%2520to%2520jointly%2520trained%2520models.%2520A%2520single%2520Share%2520model%2520can%2520replace%2520hundreds%2520of%2520task-specific%2520LoRA%2520adapters%252C%2520supporting%2520scalable%252C%2520asynchronous%2520continual%2520learning.%2520Experiments%2520across%2520image%2520classification%252C%2520natural%2520language%2520understanding%252C%25203D%2520pose%2520estimation%252C%2520and%2520text-to-image%2520generation%2520validate%2520its%2520effectiveness%252C%2520making%2520Share%2520a%2520practical%2520and%2520scalable%2520solution%2520for%2520lifelong%2520learning%2520in%2520large-scale%2520AI%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.06043v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shared%20LoRA%20Subspaces%20for%20almost%20Strict%20Continual%20Learning&entry.906535625=Prakhar%20Kaushik%20and%20Ankit%20Vaidya%20and%20Shravan%20Chaudhari%20and%20Rama%20Chellappa%20and%20Alan%20Yuille&entry.1292438233=Adapting%20large%20pretrained%20models%20to%20new%20tasks%20efficiently%20and%20continually%20is%20crucial%20for%20real-world%20deployment%20but%20remains%20challenging%20due%20to%20catastrophic%20forgetting%20and%20the%20high%20cost%20of%20retraining.%20While%20parameter-efficient%20tuning%20methods%20like%20low%20rank%20adaptation%20%28LoRA%29%20reduce%20computational%20demands%2C%20they%20lack%20mechanisms%20for%20strict%20continual%20learning%20and%20knowledge%20integration%2C%20without%20relying%20on%20data%20replay%2C%20or%20multiple%20adapters.%20We%20propose%20Share%2C%20a%20novel%20approach%20to%20parameter%20efficient%20continual%20finetuning%20that%20learns%20and%20dynamically%20updates%20a%20single%2C%20shared%20low-rank%20subspace%2C%20enabling%20seamless%20adaptation%20across%20multiple%20tasks%20and%20modalities.%20Share%20constructs%20a%20foundational%20subspace%20that%20extracts%20core%20knowledge%20from%20past%20tasks%20and%20incrementally%20integrates%20new%20information%20by%20identifying%20essential%20subspace%20directions.%20Knowledge%20from%20each%20new%20task%20is%20incorporated%20into%20this%20evolving%20subspace%2C%20facilitating%20forward%20knowledge%20transfer%2C%20while%20minimizing%20catastrophic%20interference.%20This%20approach%20achieves%20up%20to%20100x%20parameter%20reduction%20and%20281x%20memory%20savings%20over%20traditional%20LoRA%20methods%2C%20maintaining%20performance%20comparable%20to%20jointly%20trained%20models.%20A%20single%20Share%20model%20can%20replace%20hundreds%20of%20task-specific%20LoRA%20adapters%2C%20supporting%20scalable%2C%20asynchronous%20continual%20learning.%20Experiments%20across%20image%20classification%2C%20natural%20language%20understanding%2C%203D%20pose%20estimation%2C%20and%20text-to-image%20generation%20validate%20its%20effectiveness%2C%20making%20Share%20a%20practical%20and%20scalable%20solution%20for%20lifelong%20learning%20in%20large-scale%20AI%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2602.06043v1&entry.124074799=Read"},
{"title": "EigenLoRAx: Recycling Adapters to Find Principal Subspaces for Resource-Efficient Adaptation and Inference", "author": "Prakhar Kaushik and Ankit Vaidya and Shravan Chaudhari and Alan Yuille", "abstract": "The rapid growth of large models has raised concerns about their environmental impact and equity in accessibility due to significant computational costs. Low-Rank Adapters (LoRA) offer a lightweight solution for finetuning large models, resulting in an abundance of publicly available adapters tailored to diverse domains. We ask: Can these pretrained adapters be leveraged to further streamline adaptation to new tasks while addressing these challenges? We introduce EigenLoRAx, a parameter-efficient finetuning method that recycles existing adapters to create a principal subspace aligned with their shared domain knowledge which can be further augmented with orthogonal basis vectors in low-resource scenarios. This enables rapid adaptation to new tasks by learning only lightweight coefficients on the principal components of the subspace-eliminating the need to finetune entire adapters. EigenLoRAx requires significantly fewer parameters and memory, improving efficiency for both training and inference. Our method demonstrates strong performance across diverse domains and tasks, offering a scalable for edge-based applications, personalization, and equitable deployment of large models in resource-constrained environments.", "link": "http://arxiv.org/abs/2502.04700v5", "date": "2026-02-05", "relevancy": 2.5797, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5211}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5134}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EigenLoRAx%3A%20Recycling%20Adapters%20to%20Find%20Principal%20Subspaces%20for%20Resource-Efficient%20Adaptation%20and%20Inference&body=Title%3A%20EigenLoRAx%3A%20Recycling%20Adapters%20to%20Find%20Principal%20Subspaces%20for%20Resource-Efficient%20Adaptation%20and%20Inference%0AAuthor%3A%20Prakhar%20Kaushik%20and%20Ankit%20Vaidya%20and%20Shravan%20Chaudhari%20and%20Alan%20Yuille%0AAbstract%3A%20The%20rapid%20growth%20of%20large%20models%20has%20raised%20concerns%20about%20their%20environmental%20impact%20and%20equity%20in%20accessibility%20due%20to%20significant%20computational%20costs.%20Low-Rank%20Adapters%20%28LoRA%29%20offer%20a%20lightweight%20solution%20for%20finetuning%20large%20models%2C%20resulting%20in%20an%20abundance%20of%20publicly%20available%20adapters%20tailored%20to%20diverse%20domains.%20We%20ask%3A%20Can%20these%20pretrained%20adapters%20be%20leveraged%20to%20further%20streamline%20adaptation%20to%20new%20tasks%20while%20addressing%20these%20challenges%3F%20We%20introduce%20EigenLoRAx%2C%20a%20parameter-efficient%20finetuning%20method%20that%20recycles%20existing%20adapters%20to%20create%20a%20principal%20subspace%20aligned%20with%20their%20shared%20domain%20knowledge%20which%20can%20be%20further%20augmented%20with%20orthogonal%20basis%20vectors%20in%20low-resource%20scenarios.%20This%20enables%20rapid%20adaptation%20to%20new%20tasks%20by%20learning%20only%20lightweight%20coefficients%20on%20the%20principal%20components%20of%20the%20subspace-eliminating%20the%20need%20to%20finetune%20entire%20adapters.%20EigenLoRAx%20requires%20significantly%20fewer%20parameters%20and%20memory%2C%20improving%20efficiency%20for%20both%20training%20and%20inference.%20Our%20method%20demonstrates%20strong%20performance%20across%20diverse%20domains%20and%20tasks%2C%20offering%20a%20scalable%20for%20edge-based%20applications%2C%20personalization%2C%20and%20equitable%20deployment%20of%20large%20models%20in%20resource-constrained%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2502.04700v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEigenLoRAx%253A%2520Recycling%2520Adapters%2520to%2520Find%2520Principal%2520Subspaces%2520for%2520Resource-Efficient%2520Adaptation%2520and%2520Inference%26entry.906535625%3DPrakhar%2520Kaushik%2520and%2520Ankit%2520Vaidya%2520and%2520Shravan%2520Chaudhari%2520and%2520Alan%2520Yuille%26entry.1292438233%3DThe%2520rapid%2520growth%2520of%2520large%2520models%2520has%2520raised%2520concerns%2520about%2520their%2520environmental%2520impact%2520and%2520equity%2520in%2520accessibility%2520due%2520to%2520significant%2520computational%2520costs.%2520Low-Rank%2520Adapters%2520%2528LoRA%2529%2520offer%2520a%2520lightweight%2520solution%2520for%2520finetuning%2520large%2520models%252C%2520resulting%2520in%2520an%2520abundance%2520of%2520publicly%2520available%2520adapters%2520tailored%2520to%2520diverse%2520domains.%2520We%2520ask%253A%2520Can%2520these%2520pretrained%2520adapters%2520be%2520leveraged%2520to%2520further%2520streamline%2520adaptation%2520to%2520new%2520tasks%2520while%2520addressing%2520these%2520challenges%253F%2520We%2520introduce%2520EigenLoRAx%252C%2520a%2520parameter-efficient%2520finetuning%2520method%2520that%2520recycles%2520existing%2520adapters%2520to%2520create%2520a%2520principal%2520subspace%2520aligned%2520with%2520their%2520shared%2520domain%2520knowledge%2520which%2520can%2520be%2520further%2520augmented%2520with%2520orthogonal%2520basis%2520vectors%2520in%2520low-resource%2520scenarios.%2520This%2520enables%2520rapid%2520adaptation%2520to%2520new%2520tasks%2520by%2520learning%2520only%2520lightweight%2520coefficients%2520on%2520the%2520principal%2520components%2520of%2520the%2520subspace-eliminating%2520the%2520need%2520to%2520finetune%2520entire%2520adapters.%2520EigenLoRAx%2520requires%2520significantly%2520fewer%2520parameters%2520and%2520memory%252C%2520improving%2520efficiency%2520for%2520both%2520training%2520and%2520inference.%2520Our%2520method%2520demonstrates%2520strong%2520performance%2520across%2520diverse%2520domains%2520and%2520tasks%252C%2520offering%2520a%2520scalable%2520for%2520edge-based%2520applications%252C%2520personalization%252C%2520and%2520equitable%2520deployment%2520of%2520large%2520models%2520in%2520resource-constrained%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04700v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EigenLoRAx%3A%20Recycling%20Adapters%20to%20Find%20Principal%20Subspaces%20for%20Resource-Efficient%20Adaptation%20and%20Inference&entry.906535625=Prakhar%20Kaushik%20and%20Ankit%20Vaidya%20and%20Shravan%20Chaudhari%20and%20Alan%20Yuille&entry.1292438233=The%20rapid%20growth%20of%20large%20models%20has%20raised%20concerns%20about%20their%20environmental%20impact%20and%20equity%20in%20accessibility%20due%20to%20significant%20computational%20costs.%20Low-Rank%20Adapters%20%28LoRA%29%20offer%20a%20lightweight%20solution%20for%20finetuning%20large%20models%2C%20resulting%20in%20an%20abundance%20of%20publicly%20available%20adapters%20tailored%20to%20diverse%20domains.%20We%20ask%3A%20Can%20these%20pretrained%20adapters%20be%20leveraged%20to%20further%20streamline%20adaptation%20to%20new%20tasks%20while%20addressing%20these%20challenges%3F%20We%20introduce%20EigenLoRAx%2C%20a%20parameter-efficient%20finetuning%20method%20that%20recycles%20existing%20adapters%20to%20create%20a%20principal%20subspace%20aligned%20with%20their%20shared%20domain%20knowledge%20which%20can%20be%20further%20augmented%20with%20orthogonal%20basis%20vectors%20in%20low-resource%20scenarios.%20This%20enables%20rapid%20adaptation%20to%20new%20tasks%20by%20learning%20only%20lightweight%20coefficients%20on%20the%20principal%20components%20of%20the%20subspace-eliminating%20the%20need%20to%20finetune%20entire%20adapters.%20EigenLoRAx%20requires%20significantly%20fewer%20parameters%20and%20memory%2C%20improving%20efficiency%20for%20both%20training%20and%20inference.%20Our%20method%20demonstrates%20strong%20performance%20across%20diverse%20domains%20and%20tasks%2C%20offering%20a%20scalable%20for%20edge-based%20applications%2C%20personalization%2C%20and%20equitable%20deployment%20of%20large%20models%20in%20resource-constrained%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2502.04700v5&entry.124074799=Read"},
{"title": "Enhancing Personality Recognition by Comparing the Predictive Power of Traits, Facets, and Nuances", "author": "Amir Ansari and Jana Subirana and Bruna Silva and Sergio Escalera and David Gallardo-Pujol and Cristina Palmero", "abstract": "Personality is a complex, hierarchical construct typically assessed through item-level questionnaires aggregated into broad trait scores. Personality recognition models aim to infer personality traits from different sources of behavioral data. However, reliance on broad trait scores as ground truth, combined with limited training data, poses challenges for generalization, as similar trait scores can manifest through diverse, context dependent behaviors. In this work, we explore the predictive impact of the more granular hierarchical levels of the Big-Five Personality Model, facets and nuances, to enhance personality recognition from audiovisual interaction data. Using the UDIVA v0.5 dataset, we trained a transformer-based model including cross-modal (audiovisual) and cross-subject (dyad-aware) attention mechanisms. Results show that nuance-level models consistently outperform facet and trait-level models, reducing mean squared error by up to 74% across interaction scenarios.", "link": "http://arxiv.org/abs/2602.05650v1", "date": "2026-02-05", "relevancy": 2.5676, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5195}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Personality%20Recognition%20by%20Comparing%20the%20Predictive%20Power%20of%20Traits%2C%20Facets%2C%20and%20Nuances&body=Title%3A%20Enhancing%20Personality%20Recognition%20by%20Comparing%20the%20Predictive%20Power%20of%20Traits%2C%20Facets%2C%20and%20Nuances%0AAuthor%3A%20Amir%20Ansari%20and%20Jana%20Subirana%20and%20Bruna%20Silva%20and%20Sergio%20Escalera%20and%20David%20Gallardo-Pujol%20and%20Cristina%20Palmero%0AAbstract%3A%20Personality%20is%20a%20complex%2C%20hierarchical%20construct%20typically%20assessed%20through%20item-level%20questionnaires%20aggregated%20into%20broad%20trait%20scores.%20Personality%20recognition%20models%20aim%20to%20infer%20personality%20traits%20from%20different%20sources%20of%20behavioral%20data.%20However%2C%20reliance%20on%20broad%20trait%20scores%20as%20ground%20truth%2C%20combined%20with%20limited%20training%20data%2C%20poses%20challenges%20for%20generalization%2C%20as%20similar%20trait%20scores%20can%20manifest%20through%20diverse%2C%20context%20dependent%20behaviors.%20In%20this%20work%2C%20we%20explore%20the%20predictive%20impact%20of%20the%20more%20granular%20hierarchical%20levels%20of%20the%20Big-Five%20Personality%20Model%2C%20facets%20and%20nuances%2C%20to%20enhance%20personality%20recognition%20from%20audiovisual%20interaction%20data.%20Using%20the%20UDIVA%20v0.5%20dataset%2C%20we%20trained%20a%20transformer-based%20model%20including%20cross-modal%20%28audiovisual%29%20and%20cross-subject%20%28dyad-aware%29%20attention%20mechanisms.%20Results%20show%20that%20nuance-level%20models%20consistently%20outperform%20facet%20and%20trait-level%20models%2C%20reducing%20mean%20squared%20error%20by%20up%20to%2074%25%20across%20interaction%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Personality%2520Recognition%2520by%2520Comparing%2520the%2520Predictive%2520Power%2520of%2520Traits%252C%2520Facets%252C%2520and%2520Nuances%26entry.906535625%3DAmir%2520Ansari%2520and%2520Jana%2520Subirana%2520and%2520Bruna%2520Silva%2520and%2520Sergio%2520Escalera%2520and%2520David%2520Gallardo-Pujol%2520and%2520Cristina%2520Palmero%26entry.1292438233%3DPersonality%2520is%2520a%2520complex%252C%2520hierarchical%2520construct%2520typically%2520assessed%2520through%2520item-level%2520questionnaires%2520aggregated%2520into%2520broad%2520trait%2520scores.%2520Personality%2520recognition%2520models%2520aim%2520to%2520infer%2520personality%2520traits%2520from%2520different%2520sources%2520of%2520behavioral%2520data.%2520However%252C%2520reliance%2520on%2520broad%2520trait%2520scores%2520as%2520ground%2520truth%252C%2520combined%2520with%2520limited%2520training%2520data%252C%2520poses%2520challenges%2520for%2520generalization%252C%2520as%2520similar%2520trait%2520scores%2520can%2520manifest%2520through%2520diverse%252C%2520context%2520dependent%2520behaviors.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520predictive%2520impact%2520of%2520the%2520more%2520granular%2520hierarchical%2520levels%2520of%2520the%2520Big-Five%2520Personality%2520Model%252C%2520facets%2520and%2520nuances%252C%2520to%2520enhance%2520personality%2520recognition%2520from%2520audiovisual%2520interaction%2520data.%2520Using%2520the%2520UDIVA%2520v0.5%2520dataset%252C%2520we%2520trained%2520a%2520transformer-based%2520model%2520including%2520cross-modal%2520%2528audiovisual%2529%2520and%2520cross-subject%2520%2528dyad-aware%2529%2520attention%2520mechanisms.%2520Results%2520show%2520that%2520nuance-level%2520models%2520consistently%2520outperform%2520facet%2520and%2520trait-level%2520models%252C%2520reducing%2520mean%2520squared%2520error%2520by%2520up%2520to%252074%2525%2520across%2520interaction%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Personality%20Recognition%20by%20Comparing%20the%20Predictive%20Power%20of%20Traits%2C%20Facets%2C%20and%20Nuances&entry.906535625=Amir%20Ansari%20and%20Jana%20Subirana%20and%20Bruna%20Silva%20and%20Sergio%20Escalera%20and%20David%20Gallardo-Pujol%20and%20Cristina%20Palmero&entry.1292438233=Personality%20is%20a%20complex%2C%20hierarchical%20construct%20typically%20assessed%20through%20item-level%20questionnaires%20aggregated%20into%20broad%20trait%20scores.%20Personality%20recognition%20models%20aim%20to%20infer%20personality%20traits%20from%20different%20sources%20of%20behavioral%20data.%20However%2C%20reliance%20on%20broad%20trait%20scores%20as%20ground%20truth%2C%20combined%20with%20limited%20training%20data%2C%20poses%20challenges%20for%20generalization%2C%20as%20similar%20trait%20scores%20can%20manifest%20through%20diverse%2C%20context%20dependent%20behaviors.%20In%20this%20work%2C%20we%20explore%20the%20predictive%20impact%20of%20the%20more%20granular%20hierarchical%20levels%20of%20the%20Big-Five%20Personality%20Model%2C%20facets%20and%20nuances%2C%20to%20enhance%20personality%20recognition%20from%20audiovisual%20interaction%20data.%20Using%20the%20UDIVA%20v0.5%20dataset%2C%20we%20trained%20a%20transformer-based%20model%20including%20cross-modal%20%28audiovisual%29%20and%20cross-subject%20%28dyad-aware%29%20attention%20mechanisms.%20Results%20show%20that%20nuance-level%20models%20consistently%20outperform%20facet%20and%20trait-level%20models%2C%20reducing%20mean%20squared%20error%20by%20up%20to%2074%25%20across%20interaction%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2602.05650v1&entry.124074799=Read"},
{"title": "Vector Quantization using Gaussian Variational Autoencoder", "author": "Tongda Xu and Wendi Zheng and Jiajun He and Jose Miguel Hernandez-Lobato and Yan Wang and Ya-Qin Zhang and Jie Tang", "abstract": "Vector-quantized variational autoencoders (VQ-VAEs) are discrete autoencoders that compress images into discrete tokens. However, they are difficult to train due to discretization. In this paper, we propose a simple yet effective technique dubbed Gaussian Quant (GQ), which first trains a Gaussian VAE under certain constraints and then converts it into a VQ-VAE without additional training. For conversion, GQ generates random Gaussian noise as a codebook and finds the closest noise vector to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAEs for effective conversion, named the target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in the supplementary materials.", "link": "http://arxiv.org/abs/2512.06609v2", "date": "2026-02-05", "relevancy": 2.5515, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5247}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.504}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vector%20Quantization%20using%20Gaussian%20Variational%20Autoencoder&body=Title%3A%20Vector%20Quantization%20using%20Gaussian%20Variational%20Autoencoder%0AAuthor%3A%20Tongda%20Xu%20and%20Wendi%20Zheng%20and%20Jiajun%20He%20and%20Jose%20Miguel%20Hernandez-Lobato%20and%20Yan%20Wang%20and%20Ya-Qin%20Zhang%20and%20Jie%20Tang%0AAbstract%3A%20Vector-quantized%20variational%20autoencoders%20%28VQ-VAEs%29%20are%20discrete%20autoencoders%20that%20compress%20images%20into%20discrete%20tokens.%20However%2C%20they%20are%20difficult%20to%20train%20due%20to%20discretization.%20In%20this%20paper%2C%20we%20propose%20a%20simple%20yet%20effective%20technique%20dubbed%20Gaussian%20Quant%20%28GQ%29%2C%20which%20first%20trains%20a%20Gaussian%20VAE%20under%20certain%20constraints%20and%20then%20converts%20it%20into%20a%20VQ-VAE%20without%20additional%20training.%20For%20conversion%2C%20GQ%20generates%20random%20Gaussian%20noise%20as%20a%20codebook%20and%20finds%20the%20closest%20noise%20vector%20to%20the%20posterior%20mean.%20Theoretically%2C%20we%20prove%20that%20when%20the%20logarithm%20of%20the%20codebook%20size%20exceeds%20the%20bits-back%20coding%20rate%20of%20the%20Gaussian%20VAE%2C%20a%20small%20quantization%20error%20is%20guaranteed.%20Practically%2C%20we%20propose%20a%20heuristic%20to%20train%20Gaussian%20VAEs%20for%20effective%20conversion%2C%20named%20the%20target%20divergence%20constraint%20%28TDC%29.%20Empirically%2C%20we%20show%20that%20GQ%20outperforms%20previous%20VQ-VAEs%2C%20such%20as%20VQGAN%2C%20FSQ%2C%20LFQ%2C%20and%20BSQ%2C%20on%20both%20UNet%20and%20ViT%20architectures.%20Furthermore%2C%20TDC%20also%20improves%20previous%20Gaussian%20VAE%20discretization%20methods%2C%20such%20as%20TokenBridge.%20The%20source%20code%20is%20provided%20in%20the%20supplementary%20materials.%0ALink%3A%20http%3A//arxiv.org/abs/2512.06609v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVector%2520Quantization%2520using%2520Gaussian%2520Variational%2520Autoencoder%26entry.906535625%3DTongda%2520Xu%2520and%2520Wendi%2520Zheng%2520and%2520Jiajun%2520He%2520and%2520Jose%2520Miguel%2520Hernandez-Lobato%2520and%2520Yan%2520Wang%2520and%2520Ya-Qin%2520Zhang%2520and%2520Jie%2520Tang%26entry.1292438233%3DVector-quantized%2520variational%2520autoencoders%2520%2528VQ-VAEs%2529%2520are%2520discrete%2520autoencoders%2520that%2520compress%2520images%2520into%2520discrete%2520tokens.%2520However%252C%2520they%2520are%2520difficult%2520to%2520train%2520due%2520to%2520discretization.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520technique%2520dubbed%2520Gaussian%2520Quant%2520%2528GQ%2529%252C%2520which%2520first%2520trains%2520a%2520Gaussian%2520VAE%2520under%2520certain%2520constraints%2520and%2520then%2520converts%2520it%2520into%2520a%2520VQ-VAE%2520without%2520additional%2520training.%2520For%2520conversion%252C%2520GQ%2520generates%2520random%2520Gaussian%2520noise%2520as%2520a%2520codebook%2520and%2520finds%2520the%2520closest%2520noise%2520vector%2520to%2520the%2520posterior%2520mean.%2520Theoretically%252C%2520we%2520prove%2520that%2520when%2520the%2520logarithm%2520of%2520the%2520codebook%2520size%2520exceeds%2520the%2520bits-back%2520coding%2520rate%2520of%2520the%2520Gaussian%2520VAE%252C%2520a%2520small%2520quantization%2520error%2520is%2520guaranteed.%2520Practically%252C%2520we%2520propose%2520a%2520heuristic%2520to%2520train%2520Gaussian%2520VAEs%2520for%2520effective%2520conversion%252C%2520named%2520the%2520target%2520divergence%2520constraint%2520%2528TDC%2529.%2520Empirically%252C%2520we%2520show%2520that%2520GQ%2520outperforms%2520previous%2520VQ-VAEs%252C%2520such%2520as%2520VQGAN%252C%2520FSQ%252C%2520LFQ%252C%2520and%2520BSQ%252C%2520on%2520both%2520UNet%2520and%2520ViT%2520architectures.%2520Furthermore%252C%2520TDC%2520also%2520improves%2520previous%2520Gaussian%2520VAE%2520discretization%2520methods%252C%2520such%2520as%2520TokenBridge.%2520The%2520source%2520code%2520is%2520provided%2520in%2520the%2520supplementary%2520materials.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.06609v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vector%20Quantization%20using%20Gaussian%20Variational%20Autoencoder&entry.906535625=Tongda%20Xu%20and%20Wendi%20Zheng%20and%20Jiajun%20He%20and%20Jose%20Miguel%20Hernandez-Lobato%20and%20Yan%20Wang%20and%20Ya-Qin%20Zhang%20and%20Jie%20Tang&entry.1292438233=Vector-quantized%20variational%20autoencoders%20%28VQ-VAEs%29%20are%20discrete%20autoencoders%20that%20compress%20images%20into%20discrete%20tokens.%20However%2C%20they%20are%20difficult%20to%20train%20due%20to%20discretization.%20In%20this%20paper%2C%20we%20propose%20a%20simple%20yet%20effective%20technique%20dubbed%20Gaussian%20Quant%20%28GQ%29%2C%20which%20first%20trains%20a%20Gaussian%20VAE%20under%20certain%20constraints%20and%20then%20converts%20it%20into%20a%20VQ-VAE%20without%20additional%20training.%20For%20conversion%2C%20GQ%20generates%20random%20Gaussian%20noise%20as%20a%20codebook%20and%20finds%20the%20closest%20noise%20vector%20to%20the%20posterior%20mean.%20Theoretically%2C%20we%20prove%20that%20when%20the%20logarithm%20of%20the%20codebook%20size%20exceeds%20the%20bits-back%20coding%20rate%20of%20the%20Gaussian%20VAE%2C%20a%20small%20quantization%20error%20is%20guaranteed.%20Practically%2C%20we%20propose%20a%20heuristic%20to%20train%20Gaussian%20VAEs%20for%20effective%20conversion%2C%20named%20the%20target%20divergence%20constraint%20%28TDC%29.%20Empirically%2C%20we%20show%20that%20GQ%20outperforms%20previous%20VQ-VAEs%2C%20such%20as%20VQGAN%2C%20FSQ%2C%20LFQ%2C%20and%20BSQ%2C%20on%20both%20UNet%20and%20ViT%20architectures.%20Furthermore%2C%20TDC%20also%20improves%20previous%20Gaussian%20VAE%20discretization%20methods%2C%20such%20as%20TokenBridge.%20The%20source%20code%20is%20provided%20in%20the%20supplementary%20materials.&entry.1838667208=http%3A//arxiv.org/abs/2512.06609v2&entry.124074799=Read"},
{"title": "XEmoGPT: An Explainable Multimodal Emotion Recognition Framework with Cue-Level Perception and Reasoning", "author": "Hanwen Zhang and Yao Liu and Peiyuan Jiang and Lang Junjie and Xie Jun and Yihui He and Yajiao Deng and Siyu Du and Qiao Liu", "abstract": "Explainable Multimodal Emotion Recognition plays a crucial role in applications such as human-computer interaction and social media analytics. However, current approaches struggle with cue-level perception and reasoning due to two main challenges: 1) general-purpose modality encoders are pretrained to capture global structures and general semantics rather than fine-grained emotional cues, resulting in limited sensitivity to emotional signals; and 2) available datasets usually involve a trade-off between annotation quality and scale, which leads to insufficient supervision for emotional cues and ultimately limits cue-level reasoning. Moreover, existing evaluation metrics are inadequate for assessing cue-level reasoning performance. To address these challenges, we propose eXplainable Emotion GPT (XEmoGPT), a novel EMER framework capable of both perceiving and reasoning over emotional cues. It incorporates two specialized modules: the Video Emotional Cue Bridge (VECB) and the Audio Emotional Cue Bridge (AECB), which enhance the video and audio encoders through carefully designed tasks for fine-grained emotional cue perception. To further support cue-level reasoning, we construct a large-scale dataset, EmoCue, designed to teach XEmoGPT how to reason over multimodal emotional cues. In addition, we introduce EmoCue-360, an automated metric that extracts and matches emotional cues using semantic similarity, and release EmoCue-Eval, a benchmark of 400 expert-annotated samples covering diverse emotional scenarios. Experimental results show that XEmoGPT achieves strong performance in both emotional cue perception and reasoning.", "link": "http://arxiv.org/abs/2602.05496v1", "date": "2026-02-05", "relevancy": 2.5421, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5101}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5101}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XEmoGPT%3A%20An%20Explainable%20Multimodal%20Emotion%20Recognition%20Framework%20with%20Cue-Level%20Perception%20and%20Reasoning&body=Title%3A%20XEmoGPT%3A%20An%20Explainable%20Multimodal%20Emotion%20Recognition%20Framework%20with%20Cue-Level%20Perception%20and%20Reasoning%0AAuthor%3A%20Hanwen%20Zhang%20and%20Yao%20Liu%20and%20Peiyuan%20Jiang%20and%20Lang%20Junjie%20and%20Xie%20Jun%20and%20Yihui%20He%20and%20Yajiao%20Deng%20and%20Siyu%20Du%20and%20Qiao%20Liu%0AAbstract%3A%20Explainable%20Multimodal%20Emotion%20Recognition%20plays%20a%20crucial%20role%20in%20applications%20such%20as%20human-computer%20interaction%20and%20social%20media%20analytics.%20However%2C%20current%20approaches%20struggle%20with%20cue-level%20perception%20and%20reasoning%20due%20to%20two%20main%20challenges%3A%201%29%20general-purpose%20modality%20encoders%20are%20pretrained%20to%20capture%20global%20structures%20and%20general%20semantics%20rather%20than%20fine-grained%20emotional%20cues%2C%20resulting%20in%20limited%20sensitivity%20to%20emotional%20signals%3B%20and%202%29%20available%20datasets%20usually%20involve%20a%20trade-off%20between%20annotation%20quality%20and%20scale%2C%20which%20leads%20to%20insufficient%20supervision%20for%20emotional%20cues%20and%20ultimately%20limits%20cue-level%20reasoning.%20Moreover%2C%20existing%20evaluation%20metrics%20are%20inadequate%20for%20assessing%20cue-level%20reasoning%20performance.%20To%20address%20these%20challenges%2C%20we%20propose%20eXplainable%20Emotion%20GPT%20%28XEmoGPT%29%2C%20a%20novel%20EMER%20framework%20capable%20of%20both%20perceiving%20and%20reasoning%20over%20emotional%20cues.%20It%20incorporates%20two%20specialized%20modules%3A%20the%20Video%20Emotional%20Cue%20Bridge%20%28VECB%29%20and%20the%20Audio%20Emotional%20Cue%20Bridge%20%28AECB%29%2C%20which%20enhance%20the%20video%20and%20audio%20encoders%20through%20carefully%20designed%20tasks%20for%20fine-grained%20emotional%20cue%20perception.%20To%20further%20support%20cue-level%20reasoning%2C%20we%20construct%20a%20large-scale%20dataset%2C%20EmoCue%2C%20designed%20to%20teach%20XEmoGPT%20how%20to%20reason%20over%20multimodal%20emotional%20cues.%20In%20addition%2C%20we%20introduce%20EmoCue-360%2C%20an%20automated%20metric%20that%20extracts%20and%20matches%20emotional%20cues%20using%20semantic%20similarity%2C%20and%20release%20EmoCue-Eval%2C%20a%20benchmark%20of%20400%20expert-annotated%20samples%20covering%20diverse%20emotional%20scenarios.%20Experimental%20results%20show%20that%20XEmoGPT%20achieves%20strong%20performance%20in%20both%20emotional%20cue%20perception%20and%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXEmoGPT%253A%2520An%2520Explainable%2520Multimodal%2520Emotion%2520Recognition%2520Framework%2520with%2520Cue-Level%2520Perception%2520and%2520Reasoning%26entry.906535625%3DHanwen%2520Zhang%2520and%2520Yao%2520Liu%2520and%2520Peiyuan%2520Jiang%2520and%2520Lang%2520Junjie%2520and%2520Xie%2520Jun%2520and%2520Yihui%2520He%2520and%2520Yajiao%2520Deng%2520and%2520Siyu%2520Du%2520and%2520Qiao%2520Liu%26entry.1292438233%3DExplainable%2520Multimodal%2520Emotion%2520Recognition%2520plays%2520a%2520crucial%2520role%2520in%2520applications%2520such%2520as%2520human-computer%2520interaction%2520and%2520social%2520media%2520analytics.%2520However%252C%2520current%2520approaches%2520struggle%2520with%2520cue-level%2520perception%2520and%2520reasoning%2520due%2520to%2520two%2520main%2520challenges%253A%25201%2529%2520general-purpose%2520modality%2520encoders%2520are%2520pretrained%2520to%2520capture%2520global%2520structures%2520and%2520general%2520semantics%2520rather%2520than%2520fine-grained%2520emotional%2520cues%252C%2520resulting%2520in%2520limited%2520sensitivity%2520to%2520emotional%2520signals%253B%2520and%25202%2529%2520available%2520datasets%2520usually%2520involve%2520a%2520trade-off%2520between%2520annotation%2520quality%2520and%2520scale%252C%2520which%2520leads%2520to%2520insufficient%2520supervision%2520for%2520emotional%2520cues%2520and%2520ultimately%2520limits%2520cue-level%2520reasoning.%2520Moreover%252C%2520existing%2520evaluation%2520metrics%2520are%2520inadequate%2520for%2520assessing%2520cue-level%2520reasoning%2520performance.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520eXplainable%2520Emotion%2520GPT%2520%2528XEmoGPT%2529%252C%2520a%2520novel%2520EMER%2520framework%2520capable%2520of%2520both%2520perceiving%2520and%2520reasoning%2520over%2520emotional%2520cues.%2520It%2520incorporates%2520two%2520specialized%2520modules%253A%2520the%2520Video%2520Emotional%2520Cue%2520Bridge%2520%2528VECB%2529%2520and%2520the%2520Audio%2520Emotional%2520Cue%2520Bridge%2520%2528AECB%2529%252C%2520which%2520enhance%2520the%2520video%2520and%2520audio%2520encoders%2520through%2520carefully%2520designed%2520tasks%2520for%2520fine-grained%2520emotional%2520cue%2520perception.%2520To%2520further%2520support%2520cue-level%2520reasoning%252C%2520we%2520construct%2520a%2520large-scale%2520dataset%252C%2520EmoCue%252C%2520designed%2520to%2520teach%2520XEmoGPT%2520how%2520to%2520reason%2520over%2520multimodal%2520emotional%2520cues.%2520In%2520addition%252C%2520we%2520introduce%2520EmoCue-360%252C%2520an%2520automated%2520metric%2520that%2520extracts%2520and%2520matches%2520emotional%2520cues%2520using%2520semantic%2520similarity%252C%2520and%2520release%2520EmoCue-Eval%252C%2520a%2520benchmark%2520of%2520400%2520expert-annotated%2520samples%2520covering%2520diverse%2520emotional%2520scenarios.%2520Experimental%2520results%2520show%2520that%2520XEmoGPT%2520achieves%2520strong%2520performance%2520in%2520both%2520emotional%2520cue%2520perception%2520and%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XEmoGPT%3A%20An%20Explainable%20Multimodal%20Emotion%20Recognition%20Framework%20with%20Cue-Level%20Perception%20and%20Reasoning&entry.906535625=Hanwen%20Zhang%20and%20Yao%20Liu%20and%20Peiyuan%20Jiang%20and%20Lang%20Junjie%20and%20Xie%20Jun%20and%20Yihui%20He%20and%20Yajiao%20Deng%20and%20Siyu%20Du%20and%20Qiao%20Liu&entry.1292438233=Explainable%20Multimodal%20Emotion%20Recognition%20plays%20a%20crucial%20role%20in%20applications%20such%20as%20human-computer%20interaction%20and%20social%20media%20analytics.%20However%2C%20current%20approaches%20struggle%20with%20cue-level%20perception%20and%20reasoning%20due%20to%20two%20main%20challenges%3A%201%29%20general-purpose%20modality%20encoders%20are%20pretrained%20to%20capture%20global%20structures%20and%20general%20semantics%20rather%20than%20fine-grained%20emotional%20cues%2C%20resulting%20in%20limited%20sensitivity%20to%20emotional%20signals%3B%20and%202%29%20available%20datasets%20usually%20involve%20a%20trade-off%20between%20annotation%20quality%20and%20scale%2C%20which%20leads%20to%20insufficient%20supervision%20for%20emotional%20cues%20and%20ultimately%20limits%20cue-level%20reasoning.%20Moreover%2C%20existing%20evaluation%20metrics%20are%20inadequate%20for%20assessing%20cue-level%20reasoning%20performance.%20To%20address%20these%20challenges%2C%20we%20propose%20eXplainable%20Emotion%20GPT%20%28XEmoGPT%29%2C%20a%20novel%20EMER%20framework%20capable%20of%20both%20perceiving%20and%20reasoning%20over%20emotional%20cues.%20It%20incorporates%20two%20specialized%20modules%3A%20the%20Video%20Emotional%20Cue%20Bridge%20%28VECB%29%20and%20the%20Audio%20Emotional%20Cue%20Bridge%20%28AECB%29%2C%20which%20enhance%20the%20video%20and%20audio%20encoders%20through%20carefully%20designed%20tasks%20for%20fine-grained%20emotional%20cue%20perception.%20To%20further%20support%20cue-level%20reasoning%2C%20we%20construct%20a%20large-scale%20dataset%2C%20EmoCue%2C%20designed%20to%20teach%20XEmoGPT%20how%20to%20reason%20over%20multimodal%20emotional%20cues.%20In%20addition%2C%20we%20introduce%20EmoCue-360%2C%20an%20automated%20metric%20that%20extracts%20and%20matches%20emotional%20cues%20using%20semantic%20similarity%2C%20and%20release%20EmoCue-Eval%2C%20a%20benchmark%20of%20400%20expert-annotated%20samples%20covering%20diverse%20emotional%20scenarios.%20Experimental%20results%20show%20that%20XEmoGPT%20achieves%20strong%20performance%20in%20both%20emotional%20cue%20perception%20and%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2602.05496v1&entry.124074799=Read"},
{"title": "Many-for-Many: Unify the Training of Multiple Video and Image Generation and Manipulation Tasks", "author": "Ruibin Li and Tao Yang and Yangming Shi and Weiguo Feng and Shilei Wen and Bingyue Peng and Lei Zhang", "abstract": "Diffusion models have shown impressive performance in many visual generation and manipulation tasks. Many existing methods focus on training a model for a specific task, especially, text-to-video (T2V) generation, while many other works focus on finetuning the pretrained T2V model for image-to-video (I2V), video-to-video (V2V), image and video manipulation tasks, etc. However, training a strong T2V foundation model requires a large amount of high-quality annotations, which is very costly. In addition, many existing models can perform only one or several tasks. In this work, we introduce a unified framework, namely many-for-many, which leverages the available training data from many different visual generation and manipulation tasks to train a single model for those different tasks. Specifically, we design a lightweight adapter to unify the different conditions in different tasks, then employ a joint image-video learning strategy to progressively train the model from scratch. Our joint learning leads to a unified visual generation and manipulation model with improved video generation performance. In addition, we introduce depth maps as a condition to help our model better perceive the 3D space in visual generation. Two versions of our model are trained with different model sizes (8B and 2B), each of which can perform more than 10 different tasks. In particular, our 8B model demonstrates highly competitive performance in video generation tasks compared to open-source and even commercial engines. Our models and source codes are available at https://github.com/leeruibin/MfM.git.", "link": "http://arxiv.org/abs/2506.01758v3", "date": "2026-02-05", "relevancy": 2.5268, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6464}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6398}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Many-for-Many%3A%20Unify%20the%20Training%20of%20Multiple%20Video%20and%20Image%20Generation%20and%20Manipulation%20Tasks&body=Title%3A%20Many-for-Many%3A%20Unify%20the%20Training%20of%20Multiple%20Video%20and%20Image%20Generation%20and%20Manipulation%20Tasks%0AAuthor%3A%20Ruibin%20Li%20and%20Tao%20Yang%20and%20Yangming%20Shi%20and%20Weiguo%20Feng%20and%20Shilei%20Wen%20and%20Bingyue%20Peng%20and%20Lei%20Zhang%0AAbstract%3A%20Diffusion%20models%20have%20shown%20impressive%20performance%20in%20many%20visual%20generation%20and%20manipulation%20tasks.%20Many%20existing%20methods%20focus%20on%20training%20a%20model%20for%20a%20specific%20task%2C%20especially%2C%20text-to-video%20%28T2V%29%20generation%2C%20while%20many%20other%20works%20focus%20on%20finetuning%20the%20pretrained%20T2V%20model%20for%20image-to-video%20%28I2V%29%2C%20video-to-video%20%28V2V%29%2C%20image%20and%20video%20manipulation%20tasks%2C%20etc.%20However%2C%20training%20a%20strong%20T2V%20foundation%20model%20requires%20a%20large%20amount%20of%20high-quality%20annotations%2C%20which%20is%20very%20costly.%20In%20addition%2C%20many%20existing%20models%20can%20perform%20only%20one%20or%20several%20tasks.%20In%20this%20work%2C%20we%20introduce%20a%20unified%20framework%2C%20namely%20many-for-many%2C%20which%20leverages%20the%20available%20training%20data%20from%20many%20different%20visual%20generation%20and%20manipulation%20tasks%20to%20train%20a%20single%20model%20for%20those%20different%20tasks.%20Specifically%2C%20we%20design%20a%20lightweight%20adapter%20to%20unify%20the%20different%20conditions%20in%20different%20tasks%2C%20then%20employ%20a%20joint%20image-video%20learning%20strategy%20to%20progressively%20train%20the%20model%20from%20scratch.%20Our%20joint%20learning%20leads%20to%20a%20unified%20visual%20generation%20and%20manipulation%20model%20with%20improved%20video%20generation%20performance.%20In%20addition%2C%20we%20introduce%20depth%20maps%20as%20a%20condition%20to%20help%20our%20model%20better%20perceive%20the%203D%20space%20in%20visual%20generation.%20Two%20versions%20of%20our%20model%20are%20trained%20with%20different%20model%20sizes%20%288B%20and%202B%29%2C%20each%20of%20which%20can%20perform%20more%20than%2010%20different%20tasks.%20In%20particular%2C%20our%208B%20model%20demonstrates%20highly%20competitive%20performance%20in%20video%20generation%20tasks%20compared%20to%20open-source%20and%20even%20commercial%20engines.%20Our%20models%20and%20source%20codes%20are%20available%20at%20https%3A//github.com/leeruibin/MfM.git.%0ALink%3A%20http%3A//arxiv.org/abs/2506.01758v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMany-for-Many%253A%2520Unify%2520the%2520Training%2520of%2520Multiple%2520Video%2520and%2520Image%2520Generation%2520and%2520Manipulation%2520Tasks%26entry.906535625%3DRuibin%2520Li%2520and%2520Tao%2520Yang%2520and%2520Yangming%2520Shi%2520and%2520Weiguo%2520Feng%2520and%2520Shilei%2520Wen%2520and%2520Bingyue%2520Peng%2520and%2520Lei%2520Zhang%26entry.1292438233%3DDiffusion%2520models%2520have%2520shown%2520impressive%2520performance%2520in%2520many%2520visual%2520generation%2520and%2520manipulation%2520tasks.%2520Many%2520existing%2520methods%2520focus%2520on%2520training%2520a%2520model%2520for%2520a%2520specific%2520task%252C%2520especially%252C%2520text-to-video%2520%2528T2V%2529%2520generation%252C%2520while%2520many%2520other%2520works%2520focus%2520on%2520finetuning%2520the%2520pretrained%2520T2V%2520model%2520for%2520image-to-video%2520%2528I2V%2529%252C%2520video-to-video%2520%2528V2V%2529%252C%2520image%2520and%2520video%2520manipulation%2520tasks%252C%2520etc.%2520However%252C%2520training%2520a%2520strong%2520T2V%2520foundation%2520model%2520requires%2520a%2520large%2520amount%2520of%2520high-quality%2520annotations%252C%2520which%2520is%2520very%2520costly.%2520In%2520addition%252C%2520many%2520existing%2520models%2520can%2520perform%2520only%2520one%2520or%2520several%2520tasks.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520unified%2520framework%252C%2520namely%2520many-for-many%252C%2520which%2520leverages%2520the%2520available%2520training%2520data%2520from%2520many%2520different%2520visual%2520generation%2520and%2520manipulation%2520tasks%2520to%2520train%2520a%2520single%2520model%2520for%2520those%2520different%2520tasks.%2520Specifically%252C%2520we%2520design%2520a%2520lightweight%2520adapter%2520to%2520unify%2520the%2520different%2520conditions%2520in%2520different%2520tasks%252C%2520then%2520employ%2520a%2520joint%2520image-video%2520learning%2520strategy%2520to%2520progressively%2520train%2520the%2520model%2520from%2520scratch.%2520Our%2520joint%2520learning%2520leads%2520to%2520a%2520unified%2520visual%2520generation%2520and%2520manipulation%2520model%2520with%2520improved%2520video%2520generation%2520performance.%2520In%2520addition%252C%2520we%2520introduce%2520depth%2520maps%2520as%2520a%2520condition%2520to%2520help%2520our%2520model%2520better%2520perceive%2520the%25203D%2520space%2520in%2520visual%2520generation.%2520Two%2520versions%2520of%2520our%2520model%2520are%2520trained%2520with%2520different%2520model%2520sizes%2520%25288B%2520and%25202B%2529%252C%2520each%2520of%2520which%2520can%2520perform%2520more%2520than%252010%2520different%2520tasks.%2520In%2520particular%252C%2520our%25208B%2520model%2520demonstrates%2520highly%2520competitive%2520performance%2520in%2520video%2520generation%2520tasks%2520compared%2520to%2520open-source%2520and%2520even%2520commercial%2520engines.%2520Our%2520models%2520and%2520source%2520codes%2520are%2520available%2520at%2520https%253A//github.com/leeruibin/MfM.git.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01758v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Many-for-Many%3A%20Unify%20the%20Training%20of%20Multiple%20Video%20and%20Image%20Generation%20and%20Manipulation%20Tasks&entry.906535625=Ruibin%20Li%20and%20Tao%20Yang%20and%20Yangming%20Shi%20and%20Weiguo%20Feng%20and%20Shilei%20Wen%20and%20Bingyue%20Peng%20and%20Lei%20Zhang&entry.1292438233=Diffusion%20models%20have%20shown%20impressive%20performance%20in%20many%20visual%20generation%20and%20manipulation%20tasks.%20Many%20existing%20methods%20focus%20on%20training%20a%20model%20for%20a%20specific%20task%2C%20especially%2C%20text-to-video%20%28T2V%29%20generation%2C%20while%20many%20other%20works%20focus%20on%20finetuning%20the%20pretrained%20T2V%20model%20for%20image-to-video%20%28I2V%29%2C%20video-to-video%20%28V2V%29%2C%20image%20and%20video%20manipulation%20tasks%2C%20etc.%20However%2C%20training%20a%20strong%20T2V%20foundation%20model%20requires%20a%20large%20amount%20of%20high-quality%20annotations%2C%20which%20is%20very%20costly.%20In%20addition%2C%20many%20existing%20models%20can%20perform%20only%20one%20or%20several%20tasks.%20In%20this%20work%2C%20we%20introduce%20a%20unified%20framework%2C%20namely%20many-for-many%2C%20which%20leverages%20the%20available%20training%20data%20from%20many%20different%20visual%20generation%20and%20manipulation%20tasks%20to%20train%20a%20single%20model%20for%20those%20different%20tasks.%20Specifically%2C%20we%20design%20a%20lightweight%20adapter%20to%20unify%20the%20different%20conditions%20in%20different%20tasks%2C%20then%20employ%20a%20joint%20image-video%20learning%20strategy%20to%20progressively%20train%20the%20model%20from%20scratch.%20Our%20joint%20learning%20leads%20to%20a%20unified%20visual%20generation%20and%20manipulation%20model%20with%20improved%20video%20generation%20performance.%20In%20addition%2C%20we%20introduce%20depth%20maps%20as%20a%20condition%20to%20help%20our%20model%20better%20perceive%20the%203D%20space%20in%20visual%20generation.%20Two%20versions%20of%20our%20model%20are%20trained%20with%20different%20model%20sizes%20%288B%20and%202B%29%2C%20each%20of%20which%20can%20perform%20more%20than%2010%20different%20tasks.%20In%20particular%2C%20our%208B%20model%20demonstrates%20highly%20competitive%20performance%20in%20video%20generation%20tasks%20compared%20to%20open-source%20and%20even%20commercial%20engines.%20Our%20models%20and%20source%20codes%20are%20available%20at%20https%3A//github.com/leeruibin/MfM.git.&entry.1838667208=http%3A//arxiv.org/abs/2506.01758v3&entry.124074799=Read"},
{"title": "FastVMT: Eliminating Redundancy in Video Motion Transfer", "author": "Yue Ma and Zhikai Wang and Tianhao Ren and Mingzhe Zheng and Hongyu Liu and Jiayi Guo and Mark Fong and Yuxuan Xue and Zixiang Zhao and Konrad Schindler and Qifeng Chen and Linfeng Zhang", "abstract": "Video motion transfer aims to synthesize videos by generating visual content according to a text prompt while transferring the motion pattern observed in a reference video. Recent methods predominantly use the Diffusion Transformer (DiT) architecture. To achieve satisfactory runtime, several methods attempt to accelerate the computations in the DiT, but fail to address structural sources of inefficiency. In this work, we identify and remove two types of computational redundancy in earlier work: motion redundancy arises because the generic DiT architecture does not reflect the fact that frame-to-frame motion is small and smooth; gradient redundancy occurs if one ignores that gradients change slowly along the diffusion trajectory. To mitigate motion redundancy, we mask the corresponding attention layers to a local neighborhood such that interaction weights are not computed unnecessarily distant image regions. To exploit gradient redundancy, we design an optimization scheme that reuses gradients from previous diffusion steps and skips unwarranted gradient computations. On average, FastVMT achieves a 3.43x speedup without degrading the visual fidelity or the temporal consistency of the generated videos.", "link": "http://arxiv.org/abs/2602.05551v1", "date": "2026-02-05", "relevancy": 2.517, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6682}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6279}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastVMT%3A%20Eliminating%20Redundancy%20in%20Video%20Motion%20Transfer&body=Title%3A%20FastVMT%3A%20Eliminating%20Redundancy%20in%20Video%20Motion%20Transfer%0AAuthor%3A%20Yue%20Ma%20and%20Zhikai%20Wang%20and%20Tianhao%20Ren%20and%20Mingzhe%20Zheng%20and%20Hongyu%20Liu%20and%20Jiayi%20Guo%20and%20Mark%20Fong%20and%20Yuxuan%20Xue%20and%20Zixiang%20Zhao%20and%20Konrad%20Schindler%20and%20Qifeng%20Chen%20and%20Linfeng%20Zhang%0AAbstract%3A%20Video%20motion%20transfer%20aims%20to%20synthesize%20videos%20by%20generating%20visual%20content%20according%20to%20a%20text%20prompt%20while%20transferring%20the%20motion%20pattern%20observed%20in%20a%20reference%20video.%20Recent%20methods%20predominantly%20use%20the%20Diffusion%20Transformer%20%28DiT%29%20architecture.%20To%20achieve%20satisfactory%20runtime%2C%20several%20methods%20attempt%20to%20accelerate%20the%20computations%20in%20the%20DiT%2C%20but%20fail%20to%20address%20structural%20sources%20of%20inefficiency.%20In%20this%20work%2C%20we%20identify%20and%20remove%20two%20types%20of%20computational%20redundancy%20in%20earlier%20work%3A%20motion%20redundancy%20arises%20because%20the%20generic%20DiT%20architecture%20does%20not%20reflect%20the%20fact%20that%20frame-to-frame%20motion%20is%20small%20and%20smooth%3B%20gradient%20redundancy%20occurs%20if%20one%20ignores%20that%20gradients%20change%20slowly%20along%20the%20diffusion%20trajectory.%20To%20mitigate%20motion%20redundancy%2C%20we%20mask%20the%20corresponding%20attention%20layers%20to%20a%20local%20neighborhood%20such%20that%20interaction%20weights%20are%20not%20computed%20unnecessarily%20distant%20image%20regions.%20To%20exploit%20gradient%20redundancy%2C%20we%20design%20an%20optimization%20scheme%20that%20reuses%20gradients%20from%20previous%20diffusion%20steps%20and%20skips%20unwarranted%20gradient%20computations.%20On%20average%2C%20FastVMT%20achieves%20a%203.43x%20speedup%20without%20degrading%20the%20visual%20fidelity%20or%20the%20temporal%20consistency%20of%20the%20generated%20videos.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05551v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastVMT%253A%2520Eliminating%2520Redundancy%2520in%2520Video%2520Motion%2520Transfer%26entry.906535625%3DYue%2520Ma%2520and%2520Zhikai%2520Wang%2520and%2520Tianhao%2520Ren%2520and%2520Mingzhe%2520Zheng%2520and%2520Hongyu%2520Liu%2520and%2520Jiayi%2520Guo%2520and%2520Mark%2520Fong%2520and%2520Yuxuan%2520Xue%2520and%2520Zixiang%2520Zhao%2520and%2520Konrad%2520Schindler%2520and%2520Qifeng%2520Chen%2520and%2520Linfeng%2520Zhang%26entry.1292438233%3DVideo%2520motion%2520transfer%2520aims%2520to%2520synthesize%2520videos%2520by%2520generating%2520visual%2520content%2520according%2520to%2520a%2520text%2520prompt%2520while%2520transferring%2520the%2520motion%2520pattern%2520observed%2520in%2520a%2520reference%2520video.%2520Recent%2520methods%2520predominantly%2520use%2520the%2520Diffusion%2520Transformer%2520%2528DiT%2529%2520architecture.%2520To%2520achieve%2520satisfactory%2520runtime%252C%2520several%2520methods%2520attempt%2520to%2520accelerate%2520the%2520computations%2520in%2520the%2520DiT%252C%2520but%2520fail%2520to%2520address%2520structural%2520sources%2520of%2520inefficiency.%2520In%2520this%2520work%252C%2520we%2520identify%2520and%2520remove%2520two%2520types%2520of%2520computational%2520redundancy%2520in%2520earlier%2520work%253A%2520motion%2520redundancy%2520arises%2520because%2520the%2520generic%2520DiT%2520architecture%2520does%2520not%2520reflect%2520the%2520fact%2520that%2520frame-to-frame%2520motion%2520is%2520small%2520and%2520smooth%253B%2520gradient%2520redundancy%2520occurs%2520if%2520one%2520ignores%2520that%2520gradients%2520change%2520slowly%2520along%2520the%2520diffusion%2520trajectory.%2520To%2520mitigate%2520motion%2520redundancy%252C%2520we%2520mask%2520the%2520corresponding%2520attention%2520layers%2520to%2520a%2520local%2520neighborhood%2520such%2520that%2520interaction%2520weights%2520are%2520not%2520computed%2520unnecessarily%2520distant%2520image%2520regions.%2520To%2520exploit%2520gradient%2520redundancy%252C%2520we%2520design%2520an%2520optimization%2520scheme%2520that%2520reuses%2520gradients%2520from%2520previous%2520diffusion%2520steps%2520and%2520skips%2520unwarranted%2520gradient%2520computations.%2520On%2520average%252C%2520FastVMT%2520achieves%2520a%25203.43x%2520speedup%2520without%2520degrading%2520the%2520visual%2520fidelity%2520or%2520the%2520temporal%2520consistency%2520of%2520the%2520generated%2520videos.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05551v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastVMT%3A%20Eliminating%20Redundancy%20in%20Video%20Motion%20Transfer&entry.906535625=Yue%20Ma%20and%20Zhikai%20Wang%20and%20Tianhao%20Ren%20and%20Mingzhe%20Zheng%20and%20Hongyu%20Liu%20and%20Jiayi%20Guo%20and%20Mark%20Fong%20and%20Yuxuan%20Xue%20and%20Zixiang%20Zhao%20and%20Konrad%20Schindler%20and%20Qifeng%20Chen%20and%20Linfeng%20Zhang&entry.1292438233=Video%20motion%20transfer%20aims%20to%20synthesize%20videos%20by%20generating%20visual%20content%20according%20to%20a%20text%20prompt%20while%20transferring%20the%20motion%20pattern%20observed%20in%20a%20reference%20video.%20Recent%20methods%20predominantly%20use%20the%20Diffusion%20Transformer%20%28DiT%29%20architecture.%20To%20achieve%20satisfactory%20runtime%2C%20several%20methods%20attempt%20to%20accelerate%20the%20computations%20in%20the%20DiT%2C%20but%20fail%20to%20address%20structural%20sources%20of%20inefficiency.%20In%20this%20work%2C%20we%20identify%20and%20remove%20two%20types%20of%20computational%20redundancy%20in%20earlier%20work%3A%20motion%20redundancy%20arises%20because%20the%20generic%20DiT%20architecture%20does%20not%20reflect%20the%20fact%20that%20frame-to-frame%20motion%20is%20small%20and%20smooth%3B%20gradient%20redundancy%20occurs%20if%20one%20ignores%20that%20gradients%20change%20slowly%20along%20the%20diffusion%20trajectory.%20To%20mitigate%20motion%20redundancy%2C%20we%20mask%20the%20corresponding%20attention%20layers%20to%20a%20local%20neighborhood%20such%20that%20interaction%20weights%20are%20not%20computed%20unnecessarily%20distant%20image%20regions.%20To%20exploit%20gradient%20redundancy%2C%20we%20design%20an%20optimization%20scheme%20that%20reuses%20gradients%20from%20previous%20diffusion%20steps%20and%20skips%20unwarranted%20gradient%20computations.%20On%20average%2C%20FastVMT%20achieves%20a%203.43x%20speedup%20without%20degrading%20the%20visual%20fidelity%20or%20the%20temporal%20consistency%20of%20the%20generated%20videos.&entry.1838667208=http%3A//arxiv.org/abs/2602.05551v1&entry.124074799=Read"},
{"title": "IndustryShapes: An RGB-D Benchmark dataset for 6D object pose estimation of industrial assembly components and tools", "author": "Panagiotis Sapoutzoglou and Orestis Vaggelis and Athina Zacharia and Evangelos Sartinas and Maria Pateraki", "abstract": "We introduce IndustryShapes, a new RGB-D benchmark dataset of industrial tools and components, designed for both instance-level and novel object 6D pose estimation approaches. The dataset provides a realistic and application-relevant testbed for benchmarking these methods in the context of industrial robotics bridging the gap between lab-based research and deployment in real-world manufacturing scenarios. Unlike many previous datasets that focus on household or consumer products or use synthetic, clean tabletop datasets, or objects captured solely in controlled lab environments, IndustryShapes introduces five new object types with challenging properties, also captured in realistic industrial assembly settings. The dataset has diverse complexity, from simple to more challenging scenes, with single and multiple objects, including scenes with multiple instances of the same object and it is organized in two parts: the classic set and the extended set. The classic set includes a total of 4,6k images and 6k annotated poses. The extended set introduces additional data modalities to support the evaluation of model-free and sequence-based approaches. To the best of our knowledge, IndustryShapes is the first dataset to offer RGB-D static onboarding sequences. We further evaluate the dataset on a representative set of state-of-the art methods for instance-based and novel object 6D pose estimation, including also object detection, segmentation, showing that there is room for improvement in this domain. The dataset page can be found in https://pose-lab.github.io/IndustryShapes.", "link": "http://arxiv.org/abs/2602.05555v1", "date": "2026-02-05", "relevancy": 2.5131, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5119}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5011}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IndustryShapes%3A%20An%20RGB-D%20Benchmark%20dataset%20for%206D%20object%20pose%20estimation%20of%20industrial%20assembly%20components%20and%20tools&body=Title%3A%20IndustryShapes%3A%20An%20RGB-D%20Benchmark%20dataset%20for%206D%20object%20pose%20estimation%20of%20industrial%20assembly%20components%20and%20tools%0AAuthor%3A%20Panagiotis%20Sapoutzoglou%20and%20Orestis%20Vaggelis%20and%20Athina%20Zacharia%20and%20Evangelos%20Sartinas%20and%20Maria%20Pateraki%0AAbstract%3A%20We%20introduce%20IndustryShapes%2C%20a%20new%20RGB-D%20benchmark%20dataset%20of%20industrial%20tools%20and%20components%2C%20designed%20for%20both%20instance-level%20and%20novel%20object%206D%20pose%20estimation%20approaches.%20The%20dataset%20provides%20a%20realistic%20and%20application-relevant%20testbed%20for%20benchmarking%20these%20methods%20in%20the%20context%20of%20industrial%20robotics%20bridging%20the%20gap%20between%20lab-based%20research%20and%20deployment%20in%20real-world%20manufacturing%20scenarios.%20Unlike%20many%20previous%20datasets%20that%20focus%20on%20household%20or%20consumer%20products%20or%20use%20synthetic%2C%20clean%20tabletop%20datasets%2C%20or%20objects%20captured%20solely%20in%20controlled%20lab%20environments%2C%20IndustryShapes%20introduces%20five%20new%20object%20types%20with%20challenging%20properties%2C%20also%20captured%20in%20realistic%20industrial%20assembly%20settings.%20The%20dataset%20has%20diverse%20complexity%2C%20from%20simple%20to%20more%20challenging%20scenes%2C%20with%20single%20and%20multiple%20objects%2C%20including%20scenes%20with%20multiple%20instances%20of%20the%20same%20object%20and%20it%20is%20organized%20in%20two%20parts%3A%20the%20classic%20set%20and%20the%20extended%20set.%20The%20classic%20set%20includes%20a%20total%20of%204%2C6k%20images%20and%206k%20annotated%20poses.%20The%20extended%20set%20introduces%20additional%20data%20modalities%20to%20support%20the%20evaluation%20of%20model-free%20and%20sequence-based%20approaches.%20To%20the%20best%20of%20our%20knowledge%2C%20IndustryShapes%20is%20the%20first%20dataset%20to%20offer%20RGB-D%20static%20onboarding%20sequences.%20We%20further%20evaluate%20the%20dataset%20on%20a%20representative%20set%20of%20state-of-the%20art%20methods%20for%20instance-based%20and%20novel%20object%206D%20pose%20estimation%2C%20including%20also%20object%20detection%2C%20segmentation%2C%20showing%20that%20there%20is%20room%20for%20improvement%20in%20this%20domain.%20The%20dataset%20page%20can%20be%20found%20in%20https%3A//pose-lab.github.io/IndustryShapes.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIndustryShapes%253A%2520An%2520RGB-D%2520Benchmark%2520dataset%2520for%25206D%2520object%2520pose%2520estimation%2520of%2520industrial%2520assembly%2520components%2520and%2520tools%26entry.906535625%3DPanagiotis%2520Sapoutzoglou%2520and%2520Orestis%2520Vaggelis%2520and%2520Athina%2520Zacharia%2520and%2520Evangelos%2520Sartinas%2520and%2520Maria%2520Pateraki%26entry.1292438233%3DWe%2520introduce%2520IndustryShapes%252C%2520a%2520new%2520RGB-D%2520benchmark%2520dataset%2520of%2520industrial%2520tools%2520and%2520components%252C%2520designed%2520for%2520both%2520instance-level%2520and%2520novel%2520object%25206D%2520pose%2520estimation%2520approaches.%2520The%2520dataset%2520provides%2520a%2520realistic%2520and%2520application-relevant%2520testbed%2520for%2520benchmarking%2520these%2520methods%2520in%2520the%2520context%2520of%2520industrial%2520robotics%2520bridging%2520the%2520gap%2520between%2520lab-based%2520research%2520and%2520deployment%2520in%2520real-world%2520manufacturing%2520scenarios.%2520Unlike%2520many%2520previous%2520datasets%2520that%2520focus%2520on%2520household%2520or%2520consumer%2520products%2520or%2520use%2520synthetic%252C%2520clean%2520tabletop%2520datasets%252C%2520or%2520objects%2520captured%2520solely%2520in%2520controlled%2520lab%2520environments%252C%2520IndustryShapes%2520introduces%2520five%2520new%2520object%2520types%2520with%2520challenging%2520properties%252C%2520also%2520captured%2520in%2520realistic%2520industrial%2520assembly%2520settings.%2520The%2520dataset%2520has%2520diverse%2520complexity%252C%2520from%2520simple%2520to%2520more%2520challenging%2520scenes%252C%2520with%2520single%2520and%2520multiple%2520objects%252C%2520including%2520scenes%2520with%2520multiple%2520instances%2520of%2520the%2520same%2520object%2520and%2520it%2520is%2520organized%2520in%2520two%2520parts%253A%2520the%2520classic%2520set%2520and%2520the%2520extended%2520set.%2520The%2520classic%2520set%2520includes%2520a%2520total%2520of%25204%252C6k%2520images%2520and%25206k%2520annotated%2520poses.%2520The%2520extended%2520set%2520introduces%2520additional%2520data%2520modalities%2520to%2520support%2520the%2520evaluation%2520of%2520model-free%2520and%2520sequence-based%2520approaches.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520IndustryShapes%2520is%2520the%2520first%2520dataset%2520to%2520offer%2520RGB-D%2520static%2520onboarding%2520sequences.%2520We%2520further%2520evaluate%2520the%2520dataset%2520on%2520a%2520representative%2520set%2520of%2520state-of-the%2520art%2520methods%2520for%2520instance-based%2520and%2520novel%2520object%25206D%2520pose%2520estimation%252C%2520including%2520also%2520object%2520detection%252C%2520segmentation%252C%2520showing%2520that%2520there%2520is%2520room%2520for%2520improvement%2520in%2520this%2520domain.%2520The%2520dataset%2520page%2520can%2520be%2520found%2520in%2520https%253A//pose-lab.github.io/IndustryShapes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IndustryShapes%3A%20An%20RGB-D%20Benchmark%20dataset%20for%206D%20object%20pose%20estimation%20of%20industrial%20assembly%20components%20and%20tools&entry.906535625=Panagiotis%20Sapoutzoglou%20and%20Orestis%20Vaggelis%20and%20Athina%20Zacharia%20and%20Evangelos%20Sartinas%20and%20Maria%20Pateraki&entry.1292438233=We%20introduce%20IndustryShapes%2C%20a%20new%20RGB-D%20benchmark%20dataset%20of%20industrial%20tools%20and%20components%2C%20designed%20for%20both%20instance-level%20and%20novel%20object%206D%20pose%20estimation%20approaches.%20The%20dataset%20provides%20a%20realistic%20and%20application-relevant%20testbed%20for%20benchmarking%20these%20methods%20in%20the%20context%20of%20industrial%20robotics%20bridging%20the%20gap%20between%20lab-based%20research%20and%20deployment%20in%20real-world%20manufacturing%20scenarios.%20Unlike%20many%20previous%20datasets%20that%20focus%20on%20household%20or%20consumer%20products%20or%20use%20synthetic%2C%20clean%20tabletop%20datasets%2C%20or%20objects%20captured%20solely%20in%20controlled%20lab%20environments%2C%20IndustryShapes%20introduces%20five%20new%20object%20types%20with%20challenging%20properties%2C%20also%20captured%20in%20realistic%20industrial%20assembly%20settings.%20The%20dataset%20has%20diverse%20complexity%2C%20from%20simple%20to%20more%20challenging%20scenes%2C%20with%20single%20and%20multiple%20objects%2C%20including%20scenes%20with%20multiple%20instances%20of%20the%20same%20object%20and%20it%20is%20organized%20in%20two%20parts%3A%20the%20classic%20set%20and%20the%20extended%20set.%20The%20classic%20set%20includes%20a%20total%20of%204%2C6k%20images%20and%206k%20annotated%20poses.%20The%20extended%20set%20introduces%20additional%20data%20modalities%20to%20support%20the%20evaluation%20of%20model-free%20and%20sequence-based%20approaches.%20To%20the%20best%20of%20our%20knowledge%2C%20IndustryShapes%20is%20the%20first%20dataset%20to%20offer%20RGB-D%20static%20onboarding%20sequences.%20We%20further%20evaluate%20the%20dataset%20on%20a%20representative%20set%20of%20state-of-the%20art%20methods%20for%20instance-based%20and%20novel%20object%206D%20pose%20estimation%2C%20including%20also%20object%20detection%2C%20segmentation%2C%20showing%20that%20there%20is%20room%20for%20improvement%20in%20this%20domain.%20The%20dataset%20page%20can%20be%20found%20in%20https%3A//pose-lab.github.io/IndustryShapes.&entry.1838667208=http%3A//arxiv.org/abs/2602.05555v1&entry.124074799=Read"},
{"title": "Dual Perspectives on Non-Contrastive Self-Supervised Learning", "author": "Jean Ponce and Basile Terver and Martial Hebert and Michael Arbel", "abstract": "The {\\em stop gradient} and {\\em exponential moving average} iterative procedures are commonly used in non-contrastive approaches to self-supervised learning to avoid representation collapse, with excellent performance in downstream applications in practice. This presentation investigates these procedures from the dual viewpoints of optimization and dynamical systems. We show that, in general, although they {\\em do not} optimize the original objective, or {\\em any} other smooth function, they {\\em do} avoid collapse Following~\\citet{Tian21}, but without any of the extra assumptions used in their proofs, we then show using a dynamical system perspective that, in the linear case, minimizing the original objective function without the use of a stop gradient or exponential moving average {\\em always} leads to collapse. Conversely, we characterize explicitly the equilibria of the dynamical systems associated with these two procedures in this linear setting as algebraic varieties in their parameter space, and show that they are, in general, {\\em asymptotically stable}. Our theoretical findings are illustrated by empirical experiments with real and synthetic data.", "link": "http://arxiv.org/abs/2507.01028v3", "date": "2026-02-05", "relevancy": 2.5106, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5188}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5014}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Perspectives%20on%20Non-Contrastive%20Self-Supervised%20Learning&body=Title%3A%20Dual%20Perspectives%20on%20Non-Contrastive%20Self-Supervised%20Learning%0AAuthor%3A%20Jean%20Ponce%20and%20Basile%20Terver%20and%20Martial%20Hebert%20and%20Michael%20Arbel%0AAbstract%3A%20The%20%7B%5Cem%20stop%20gradient%7D%20and%20%7B%5Cem%20exponential%20moving%20average%7D%20iterative%20procedures%20are%20commonly%20used%20in%20non-contrastive%20approaches%20to%20self-supervised%20learning%20to%20avoid%20representation%20collapse%2C%20with%20excellent%20performance%20in%20downstream%20applications%20in%20practice.%20This%20presentation%20investigates%20these%20procedures%20from%20the%20dual%20viewpoints%20of%20optimization%20and%20dynamical%20systems.%20We%20show%20that%2C%20in%20general%2C%20although%20they%20%7B%5Cem%20do%20not%7D%20optimize%20the%20original%20objective%2C%20or%20%7B%5Cem%20any%7D%20other%20smooth%20function%2C%20they%20%7B%5Cem%20do%7D%20avoid%20collapse%20Following~%5Ccitet%7BTian21%7D%2C%20but%20without%20any%20of%20the%20extra%20assumptions%20used%20in%20their%20proofs%2C%20we%20then%20show%20using%20a%20dynamical%20system%20perspective%20that%2C%20in%20the%20linear%20case%2C%20minimizing%20the%20original%20objective%20function%20without%20the%20use%20of%20a%20stop%20gradient%20or%20exponential%20moving%20average%20%7B%5Cem%20always%7D%20leads%20to%20collapse.%20Conversely%2C%20we%20characterize%20explicitly%20the%20equilibria%20of%20the%20dynamical%20systems%20associated%20with%20these%20two%20procedures%20in%20this%20linear%20setting%20as%20algebraic%20varieties%20in%20their%20parameter%20space%2C%20and%20show%20that%20they%20are%2C%20in%20general%2C%20%7B%5Cem%20asymptotically%20stable%7D.%20Our%20theoretical%20findings%20are%20illustrated%20by%20empirical%20experiments%20with%20real%20and%20synthetic%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2507.01028v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Perspectives%2520on%2520Non-Contrastive%2520Self-Supervised%2520Learning%26entry.906535625%3DJean%2520Ponce%2520and%2520Basile%2520Terver%2520and%2520Martial%2520Hebert%2520and%2520Michael%2520Arbel%26entry.1292438233%3DThe%2520%257B%255Cem%2520stop%2520gradient%257D%2520and%2520%257B%255Cem%2520exponential%2520moving%2520average%257D%2520iterative%2520procedures%2520are%2520commonly%2520used%2520in%2520non-contrastive%2520approaches%2520to%2520self-supervised%2520learning%2520to%2520avoid%2520representation%2520collapse%252C%2520with%2520excellent%2520performance%2520in%2520downstream%2520applications%2520in%2520practice.%2520This%2520presentation%2520investigates%2520these%2520procedures%2520from%2520the%2520dual%2520viewpoints%2520of%2520optimization%2520and%2520dynamical%2520systems.%2520We%2520show%2520that%252C%2520in%2520general%252C%2520although%2520they%2520%257B%255Cem%2520do%2520not%257D%2520optimize%2520the%2520original%2520objective%252C%2520or%2520%257B%255Cem%2520any%257D%2520other%2520smooth%2520function%252C%2520they%2520%257B%255Cem%2520do%257D%2520avoid%2520collapse%2520Following~%255Ccitet%257BTian21%257D%252C%2520but%2520without%2520any%2520of%2520the%2520extra%2520assumptions%2520used%2520in%2520their%2520proofs%252C%2520we%2520then%2520show%2520using%2520a%2520dynamical%2520system%2520perspective%2520that%252C%2520in%2520the%2520linear%2520case%252C%2520minimizing%2520the%2520original%2520objective%2520function%2520without%2520the%2520use%2520of%2520a%2520stop%2520gradient%2520or%2520exponential%2520moving%2520average%2520%257B%255Cem%2520always%257D%2520leads%2520to%2520collapse.%2520Conversely%252C%2520we%2520characterize%2520explicitly%2520the%2520equilibria%2520of%2520the%2520dynamical%2520systems%2520associated%2520with%2520these%2520two%2520procedures%2520in%2520this%2520linear%2520setting%2520as%2520algebraic%2520varieties%2520in%2520their%2520parameter%2520space%252C%2520and%2520show%2520that%2520they%2520are%252C%2520in%2520general%252C%2520%257B%255Cem%2520asymptotically%2520stable%257D.%2520Our%2520theoretical%2520findings%2520are%2520illustrated%2520by%2520empirical%2520experiments%2520with%2520real%2520and%2520synthetic%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01028v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Perspectives%20on%20Non-Contrastive%20Self-Supervised%20Learning&entry.906535625=Jean%20Ponce%20and%20Basile%20Terver%20and%20Martial%20Hebert%20and%20Michael%20Arbel&entry.1292438233=The%20%7B%5Cem%20stop%20gradient%7D%20and%20%7B%5Cem%20exponential%20moving%20average%7D%20iterative%20procedures%20are%20commonly%20used%20in%20non-contrastive%20approaches%20to%20self-supervised%20learning%20to%20avoid%20representation%20collapse%2C%20with%20excellent%20performance%20in%20downstream%20applications%20in%20practice.%20This%20presentation%20investigates%20these%20procedures%20from%20the%20dual%20viewpoints%20of%20optimization%20and%20dynamical%20systems.%20We%20show%20that%2C%20in%20general%2C%20although%20they%20%7B%5Cem%20do%20not%7D%20optimize%20the%20original%20objective%2C%20or%20%7B%5Cem%20any%7D%20other%20smooth%20function%2C%20they%20%7B%5Cem%20do%7D%20avoid%20collapse%20Following~%5Ccitet%7BTian21%7D%2C%20but%20without%20any%20of%20the%20extra%20assumptions%20used%20in%20their%20proofs%2C%20we%20then%20show%20using%20a%20dynamical%20system%20perspective%20that%2C%20in%20the%20linear%20case%2C%20minimizing%20the%20original%20objective%20function%20without%20the%20use%20of%20a%20stop%20gradient%20or%20exponential%20moving%20average%20%7B%5Cem%20always%7D%20leads%20to%20collapse.%20Conversely%2C%20we%20characterize%20explicitly%20the%20equilibria%20of%20the%20dynamical%20systems%20associated%20with%20these%20two%20procedures%20in%20this%20linear%20setting%20as%20algebraic%20varieties%20in%20their%20parameter%20space%2C%20and%20show%20that%20they%20are%2C%20in%20general%2C%20%7B%5Cem%20asymptotically%20stable%7D.%20Our%20theoretical%20findings%20are%20illustrated%20by%20empirical%20experiments%20with%20real%20and%20synthetic%20data.&entry.1838667208=http%3A//arxiv.org/abs/2507.01028v3&entry.124074799=Read"},
{"title": "An Attention-based Feature Memory Design for Energy-Efficient Continual Learning", "author": "Yuandou Wang and Filip Gunnarsson and Rihan Hai", "abstract": "Tabular data streams are increasingly prevalent in real-time decision-making across healthcare, finance, and the Internet of Things, often generated and processed on resource-constrained edge and mobile devices. Continual learning (CL) enables models to learn sequentially from such streams while retaining previously acquired knowledge. While recent CL advances have made significant progress in mitigating catastrophic forgetting, the energy and memory efficiency of CL for tabular data streams remains largely unexplored. To address this gap, we propose AttenMLP, which integrates attention-based feature replay with context retrieval and sliding buffer updates within a minibatch training framework for streaming tabular learning.\n  We evaluate AttenMLP against state-of-the-art (SOTA) tabular models on real-world concept drift benchmarks with temporal distribution shifts. Experimental results show that AttenMLP achieves accuracy comparable to strong baselines without replay, while substantially reducing energy consumption through tunable design choices. In particular, with the proposed attention-based feature memory design, AttenMLP costs a 0.062 decrease in final accuracy under the incremental concept drift dataset, while reducing energy usage up to 33.3\\% compared to TabPFNv2. Under the abrupt concept drift dataset, AttenMLP reduces 1.47\\% energy consumption compared to TabR, at the cost of a 0.038 decrease in final accuracy. Although ranking third in global efficiency, AttenMLP demonstrates energy-accuracy trade-offs across both abrupt and incremental concept drift scenarios compared to SOTA tabular models.", "link": "http://arxiv.org/abs/2510.04660v2", "date": "2026-02-05", "relevancy": 2.5038, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5127}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4961}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Attention-based%20Feature%20Memory%20Design%20for%20Energy-Efficient%20Continual%20Learning&body=Title%3A%20An%20Attention-based%20Feature%20Memory%20Design%20for%20Energy-Efficient%20Continual%20Learning%0AAuthor%3A%20Yuandou%20Wang%20and%20Filip%20Gunnarsson%20and%20Rihan%20Hai%0AAbstract%3A%20Tabular%20data%20streams%20are%20increasingly%20prevalent%20in%20real-time%20decision-making%20across%20healthcare%2C%20finance%2C%20and%20the%20Internet%20of%20Things%2C%20often%20generated%20and%20processed%20on%20resource-constrained%20edge%20and%20mobile%20devices.%20Continual%20learning%20%28CL%29%20enables%20models%20to%20learn%20sequentially%20from%20such%20streams%20while%20retaining%20previously%20acquired%20knowledge.%20While%20recent%20CL%20advances%20have%20made%20significant%20progress%20in%20mitigating%20catastrophic%20forgetting%2C%20the%20energy%20and%20memory%20efficiency%20of%20CL%20for%20tabular%20data%20streams%20remains%20largely%20unexplored.%20To%20address%20this%20gap%2C%20we%20propose%20AttenMLP%2C%20which%20integrates%20attention-based%20feature%20replay%20with%20context%20retrieval%20and%20sliding%20buffer%20updates%20within%20a%20minibatch%20training%20framework%20for%20streaming%20tabular%20learning.%0A%20%20We%20evaluate%20AttenMLP%20against%20state-of-the-art%20%28SOTA%29%20tabular%20models%20on%20real-world%20concept%20drift%20benchmarks%20with%20temporal%20distribution%20shifts.%20Experimental%20results%20show%20that%20AttenMLP%20achieves%20accuracy%20comparable%20to%20strong%20baselines%20without%20replay%2C%20while%20substantially%20reducing%20energy%20consumption%20through%20tunable%20design%20choices.%20In%20particular%2C%20with%20the%20proposed%20attention-based%20feature%20memory%20design%2C%20AttenMLP%20costs%20a%200.062%20decrease%20in%20final%20accuracy%20under%20the%20incremental%20concept%20drift%20dataset%2C%20while%20reducing%20energy%20usage%20up%20to%2033.3%5C%25%20compared%20to%20TabPFNv2.%20Under%20the%20abrupt%20concept%20drift%20dataset%2C%20AttenMLP%20reduces%201.47%5C%25%20energy%20consumption%20compared%20to%20TabR%2C%20at%20the%20cost%20of%20a%200.038%20decrease%20in%20final%20accuracy.%20Although%20ranking%20third%20in%20global%20efficiency%2C%20AttenMLP%20demonstrates%20energy-accuracy%20trade-offs%20across%20both%20abrupt%20and%20incremental%20concept%20drift%20scenarios%20compared%20to%20SOTA%20tabular%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2510.04660v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Attention-based%2520Feature%2520Memory%2520Design%2520for%2520Energy-Efficient%2520Continual%2520Learning%26entry.906535625%3DYuandou%2520Wang%2520and%2520Filip%2520Gunnarsson%2520and%2520Rihan%2520Hai%26entry.1292438233%3DTabular%2520data%2520streams%2520are%2520increasingly%2520prevalent%2520in%2520real-time%2520decision-making%2520across%2520healthcare%252C%2520finance%252C%2520and%2520the%2520Internet%2520of%2520Things%252C%2520often%2520generated%2520and%2520processed%2520on%2520resource-constrained%2520edge%2520and%2520mobile%2520devices.%2520Continual%2520learning%2520%2528CL%2529%2520enables%2520models%2520to%2520learn%2520sequentially%2520from%2520such%2520streams%2520while%2520retaining%2520previously%2520acquired%2520knowledge.%2520While%2520recent%2520CL%2520advances%2520have%2520made%2520significant%2520progress%2520in%2520mitigating%2520catastrophic%2520forgetting%252C%2520the%2520energy%2520and%2520memory%2520efficiency%2520of%2520CL%2520for%2520tabular%2520data%2520streams%2520remains%2520largely%2520unexplored.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520AttenMLP%252C%2520which%2520integrates%2520attention-based%2520feature%2520replay%2520with%2520context%2520retrieval%2520and%2520sliding%2520buffer%2520updates%2520within%2520a%2520minibatch%2520training%2520framework%2520for%2520streaming%2520tabular%2520learning.%250A%2520%2520We%2520evaluate%2520AttenMLP%2520against%2520state-of-the-art%2520%2528SOTA%2529%2520tabular%2520models%2520on%2520real-world%2520concept%2520drift%2520benchmarks%2520with%2520temporal%2520distribution%2520shifts.%2520Experimental%2520results%2520show%2520that%2520AttenMLP%2520achieves%2520accuracy%2520comparable%2520to%2520strong%2520baselines%2520without%2520replay%252C%2520while%2520substantially%2520reducing%2520energy%2520consumption%2520through%2520tunable%2520design%2520choices.%2520In%2520particular%252C%2520with%2520the%2520proposed%2520attention-based%2520feature%2520memory%2520design%252C%2520AttenMLP%2520costs%2520a%25200.062%2520decrease%2520in%2520final%2520accuracy%2520under%2520the%2520incremental%2520concept%2520drift%2520dataset%252C%2520while%2520reducing%2520energy%2520usage%2520up%2520to%252033.3%255C%2525%2520compared%2520to%2520TabPFNv2.%2520Under%2520the%2520abrupt%2520concept%2520drift%2520dataset%252C%2520AttenMLP%2520reduces%25201.47%255C%2525%2520energy%2520consumption%2520compared%2520to%2520TabR%252C%2520at%2520the%2520cost%2520of%2520a%25200.038%2520decrease%2520in%2520final%2520accuracy.%2520Although%2520ranking%2520third%2520in%2520global%2520efficiency%252C%2520AttenMLP%2520demonstrates%2520energy-accuracy%2520trade-offs%2520across%2520both%2520abrupt%2520and%2520incremental%2520concept%2520drift%2520scenarios%2520compared%2520to%2520SOTA%2520tabular%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04660v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Attention-based%20Feature%20Memory%20Design%20for%20Energy-Efficient%20Continual%20Learning&entry.906535625=Yuandou%20Wang%20and%20Filip%20Gunnarsson%20and%20Rihan%20Hai&entry.1292438233=Tabular%20data%20streams%20are%20increasingly%20prevalent%20in%20real-time%20decision-making%20across%20healthcare%2C%20finance%2C%20and%20the%20Internet%20of%20Things%2C%20often%20generated%20and%20processed%20on%20resource-constrained%20edge%20and%20mobile%20devices.%20Continual%20learning%20%28CL%29%20enables%20models%20to%20learn%20sequentially%20from%20such%20streams%20while%20retaining%20previously%20acquired%20knowledge.%20While%20recent%20CL%20advances%20have%20made%20significant%20progress%20in%20mitigating%20catastrophic%20forgetting%2C%20the%20energy%20and%20memory%20efficiency%20of%20CL%20for%20tabular%20data%20streams%20remains%20largely%20unexplored.%20To%20address%20this%20gap%2C%20we%20propose%20AttenMLP%2C%20which%20integrates%20attention-based%20feature%20replay%20with%20context%20retrieval%20and%20sliding%20buffer%20updates%20within%20a%20minibatch%20training%20framework%20for%20streaming%20tabular%20learning.%0A%20%20We%20evaluate%20AttenMLP%20against%20state-of-the-art%20%28SOTA%29%20tabular%20models%20on%20real-world%20concept%20drift%20benchmarks%20with%20temporal%20distribution%20shifts.%20Experimental%20results%20show%20that%20AttenMLP%20achieves%20accuracy%20comparable%20to%20strong%20baselines%20without%20replay%2C%20while%20substantially%20reducing%20energy%20consumption%20through%20tunable%20design%20choices.%20In%20particular%2C%20with%20the%20proposed%20attention-based%20feature%20memory%20design%2C%20AttenMLP%20costs%20a%200.062%20decrease%20in%20final%20accuracy%20under%20the%20incremental%20concept%20drift%20dataset%2C%20while%20reducing%20energy%20usage%20up%20to%2033.3%5C%25%20compared%20to%20TabPFNv2.%20Under%20the%20abrupt%20concept%20drift%20dataset%2C%20AttenMLP%20reduces%201.47%5C%25%20energy%20consumption%20compared%20to%20TabR%2C%20at%20the%20cost%20of%20a%200.038%20decrease%20in%20final%20accuracy.%20Although%20ranking%20third%20in%20global%20efficiency%2C%20AttenMLP%20demonstrates%20energy-accuracy%20trade-offs%20across%20both%20abrupt%20and%20incremental%20concept%20drift%20scenarios%20compared%20to%20SOTA%20tabular%20models.&entry.1838667208=http%3A//arxiv.org/abs/2510.04660v2&entry.124074799=Read"},
{"title": "FedRandom: Sampling Consistent and Accurate Contribution Values in Federated Learning", "author": "Arno Geimer and Beltran Fiz Pontiveros and Radu State", "abstract": "Federated Learning is a privacy-preserving decentralized approach for Machine Learning tasks. In industry deployments characterized by a limited number of entities possessing abundant data, the significance of a participant's role in shaping the global model becomes pivotal given that participation in a federation incurs costs, and participants may expect compensation for their involvement. Additionally, the contributions of participants serve as a crucial means to identify and address potential malicious actors and free-riders. However, fairly assessing individual contributions remains a significant hurdle. Recent works have demonstrated a considerable inherent instability in contribution estimations across aggregation strategies. While employing a different strategy may offer convergence benefits, this instability can have potentially harming effects on the willingness of participants in engaging in the federation. In this work, we introduce FedRandom, a novel mitigation technique to the contribution instability problem. Tackling the instability as a statistical estimation problem, FedRandom allows us to generate more samples than when using regular FL strategies. We show that these additional samples provide a more consistent and reliable evaluation of participant contributions. We demonstrate our approach using different data distributions across CIFAR-10, MNIST, CIFAR-100 and FMNIST and show that FedRandom reduces the overall distance to the ground truth by more than a third in half of all evaluated scenarios, and improves stability in more than 90% of cases.", "link": "http://arxiv.org/abs/2602.05693v1", "date": "2026-02-05", "relevancy": 2.5022, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5079}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4979}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedRandom%3A%20Sampling%20Consistent%20and%20Accurate%20Contribution%20Values%20in%20Federated%20Learning&body=Title%3A%20FedRandom%3A%20Sampling%20Consistent%20and%20Accurate%20Contribution%20Values%20in%20Federated%20Learning%0AAuthor%3A%20Arno%20Geimer%20and%20Beltran%20Fiz%20Pontiveros%20and%20Radu%20State%0AAbstract%3A%20Federated%20Learning%20is%20a%20privacy-preserving%20decentralized%20approach%20for%20Machine%20Learning%20tasks.%20In%20industry%20deployments%20characterized%20by%20a%20limited%20number%20of%20entities%20possessing%20abundant%20data%2C%20the%20significance%20of%20a%20participant%27s%20role%20in%20shaping%20the%20global%20model%20becomes%20pivotal%20given%20that%20participation%20in%20a%20federation%20incurs%20costs%2C%20and%20participants%20may%20expect%20compensation%20for%20their%20involvement.%20Additionally%2C%20the%20contributions%20of%20participants%20serve%20as%20a%20crucial%20means%20to%20identify%20and%20address%20potential%20malicious%20actors%20and%20free-riders.%20However%2C%20fairly%20assessing%20individual%20contributions%20remains%20a%20significant%20hurdle.%20Recent%20works%20have%20demonstrated%20a%20considerable%20inherent%20instability%20in%20contribution%20estimations%20across%20aggregation%20strategies.%20While%20employing%20a%20different%20strategy%20may%20offer%20convergence%20benefits%2C%20this%20instability%20can%20have%20potentially%20harming%20effects%20on%20the%20willingness%20of%20participants%20in%20engaging%20in%20the%20federation.%20In%20this%20work%2C%20we%20introduce%20FedRandom%2C%20a%20novel%20mitigation%20technique%20to%20the%20contribution%20instability%20problem.%20Tackling%20the%20instability%20as%20a%20statistical%20estimation%20problem%2C%20FedRandom%20allows%20us%20to%20generate%20more%20samples%20than%20when%20using%20regular%20FL%20strategies.%20We%20show%20that%20these%20additional%20samples%20provide%20a%20more%20consistent%20and%20reliable%20evaluation%20of%20participant%20contributions.%20We%20demonstrate%20our%20approach%20using%20different%20data%20distributions%20across%20CIFAR-10%2C%20MNIST%2C%20CIFAR-100%20and%20FMNIST%20and%20show%20that%20FedRandom%20reduces%20the%20overall%20distance%20to%20the%20ground%20truth%20by%20more%20than%20a%20third%20in%20half%20of%20all%20evaluated%20scenarios%2C%20and%20improves%20stability%20in%20more%20than%2090%25%20of%20cases.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedRandom%253A%2520Sampling%2520Consistent%2520and%2520Accurate%2520Contribution%2520Values%2520in%2520Federated%2520Learning%26entry.906535625%3DArno%2520Geimer%2520and%2520Beltran%2520Fiz%2520Pontiveros%2520and%2520Radu%2520State%26entry.1292438233%3DFederated%2520Learning%2520is%2520a%2520privacy-preserving%2520decentralized%2520approach%2520for%2520Machine%2520Learning%2520tasks.%2520In%2520industry%2520deployments%2520characterized%2520by%2520a%2520limited%2520number%2520of%2520entities%2520possessing%2520abundant%2520data%252C%2520the%2520significance%2520of%2520a%2520participant%2527s%2520role%2520in%2520shaping%2520the%2520global%2520model%2520becomes%2520pivotal%2520given%2520that%2520participation%2520in%2520a%2520federation%2520incurs%2520costs%252C%2520and%2520participants%2520may%2520expect%2520compensation%2520for%2520their%2520involvement.%2520Additionally%252C%2520the%2520contributions%2520of%2520participants%2520serve%2520as%2520a%2520crucial%2520means%2520to%2520identify%2520and%2520address%2520potential%2520malicious%2520actors%2520and%2520free-riders.%2520However%252C%2520fairly%2520assessing%2520individual%2520contributions%2520remains%2520a%2520significant%2520hurdle.%2520Recent%2520works%2520have%2520demonstrated%2520a%2520considerable%2520inherent%2520instability%2520in%2520contribution%2520estimations%2520across%2520aggregation%2520strategies.%2520While%2520employing%2520a%2520different%2520strategy%2520may%2520offer%2520convergence%2520benefits%252C%2520this%2520instability%2520can%2520have%2520potentially%2520harming%2520effects%2520on%2520the%2520willingness%2520of%2520participants%2520in%2520engaging%2520in%2520the%2520federation.%2520In%2520this%2520work%252C%2520we%2520introduce%2520FedRandom%252C%2520a%2520novel%2520mitigation%2520technique%2520to%2520the%2520contribution%2520instability%2520problem.%2520Tackling%2520the%2520instability%2520as%2520a%2520statistical%2520estimation%2520problem%252C%2520FedRandom%2520allows%2520us%2520to%2520generate%2520more%2520samples%2520than%2520when%2520using%2520regular%2520FL%2520strategies.%2520We%2520show%2520that%2520these%2520additional%2520samples%2520provide%2520a%2520more%2520consistent%2520and%2520reliable%2520evaluation%2520of%2520participant%2520contributions.%2520We%2520demonstrate%2520our%2520approach%2520using%2520different%2520data%2520distributions%2520across%2520CIFAR-10%252C%2520MNIST%252C%2520CIFAR-100%2520and%2520FMNIST%2520and%2520show%2520that%2520FedRandom%2520reduces%2520the%2520overall%2520distance%2520to%2520the%2520ground%2520truth%2520by%2520more%2520than%2520a%2520third%2520in%2520half%2520of%2520all%2520evaluated%2520scenarios%252C%2520and%2520improves%2520stability%2520in%2520more%2520than%252090%2525%2520of%2520cases.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedRandom%3A%20Sampling%20Consistent%20and%20Accurate%20Contribution%20Values%20in%20Federated%20Learning&entry.906535625=Arno%20Geimer%20and%20Beltran%20Fiz%20Pontiveros%20and%20Radu%20State&entry.1292438233=Federated%20Learning%20is%20a%20privacy-preserving%20decentralized%20approach%20for%20Machine%20Learning%20tasks.%20In%20industry%20deployments%20characterized%20by%20a%20limited%20number%20of%20entities%20possessing%20abundant%20data%2C%20the%20significance%20of%20a%20participant%27s%20role%20in%20shaping%20the%20global%20model%20becomes%20pivotal%20given%20that%20participation%20in%20a%20federation%20incurs%20costs%2C%20and%20participants%20may%20expect%20compensation%20for%20their%20involvement.%20Additionally%2C%20the%20contributions%20of%20participants%20serve%20as%20a%20crucial%20means%20to%20identify%20and%20address%20potential%20malicious%20actors%20and%20free-riders.%20However%2C%20fairly%20assessing%20individual%20contributions%20remains%20a%20significant%20hurdle.%20Recent%20works%20have%20demonstrated%20a%20considerable%20inherent%20instability%20in%20contribution%20estimations%20across%20aggregation%20strategies.%20While%20employing%20a%20different%20strategy%20may%20offer%20convergence%20benefits%2C%20this%20instability%20can%20have%20potentially%20harming%20effects%20on%20the%20willingness%20of%20participants%20in%20engaging%20in%20the%20federation.%20In%20this%20work%2C%20we%20introduce%20FedRandom%2C%20a%20novel%20mitigation%20technique%20to%20the%20contribution%20instability%20problem.%20Tackling%20the%20instability%20as%20a%20statistical%20estimation%20problem%2C%20FedRandom%20allows%20us%20to%20generate%20more%20samples%20than%20when%20using%20regular%20FL%20strategies.%20We%20show%20that%20these%20additional%20samples%20provide%20a%20more%20consistent%20and%20reliable%20evaluation%20of%20participant%20contributions.%20We%20demonstrate%20our%20approach%20using%20different%20data%20distributions%20across%20CIFAR-10%2C%20MNIST%2C%20CIFAR-100%20and%20FMNIST%20and%20show%20that%20FedRandom%20reduces%20the%20overall%20distance%20to%20the%20ground%20truth%20by%20more%20than%20a%20third%20in%20half%20of%20all%20evaluated%20scenarios%2C%20and%20improves%20stability%20in%20more%20than%2090%25%20of%20cases.&entry.1838667208=http%3A//arxiv.org/abs/2602.05693v1&entry.124074799=Read"},
{"title": "How to Achieve the Intended Aim of Deep Clustering Now, without Deep Learning", "author": "Kai Ming Ting and Wei-Jie Xu and Hang Zhang", "abstract": "Deep clustering (DC) is often quoted to have a key advantage over $k$-means clustering. Yet, this advantage is often demonstrated using image datasets only, and it is unclear whether it addresses the fundamental limitations of $k$-means clustering. Deep Embedded Clustering (DEC) learns a latent representation via an autoencoder and performs clustering based on a $k$-means-like procedure, while the optimization is conducted in an end-to-end manner. This paper investigates whether the deep-learned representation has enabled DEC to overcome the known fundamental limitations of $k$-means clustering, i.e., its inability to discover clusters of arbitrary shapes, varied sizes and densities. Our investigations on DEC have a wider implication on deep clustering methods in general. Notably, none of these methods exploit the underlying data distribution. We uncover that a non-deep learning approach achieves the intended aim of deep clustering by making use of distributional information of clusters in a dataset to effectively address these fundamental limitations.", "link": "http://arxiv.org/abs/2602.05749v1", "date": "2026-02-05", "relevancy": 2.4978, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5202}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4905}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Achieve%20the%20Intended%20Aim%20of%20Deep%20Clustering%20Now%2C%20without%20Deep%20Learning&body=Title%3A%20How%20to%20Achieve%20the%20Intended%20Aim%20of%20Deep%20Clustering%20Now%2C%20without%20Deep%20Learning%0AAuthor%3A%20Kai%20Ming%20Ting%20and%20Wei-Jie%20Xu%20and%20Hang%20Zhang%0AAbstract%3A%20Deep%20clustering%20%28DC%29%20is%20often%20quoted%20to%20have%20a%20key%20advantage%20over%20%24k%24-means%20clustering.%20Yet%2C%20this%20advantage%20is%20often%20demonstrated%20using%20image%20datasets%20only%2C%20and%20it%20is%20unclear%20whether%20it%20addresses%20the%20fundamental%20limitations%20of%20%24k%24-means%20clustering.%20Deep%20Embedded%20Clustering%20%28DEC%29%20learns%20a%20latent%20representation%20via%20an%20autoencoder%20and%20performs%20clustering%20based%20on%20a%20%24k%24-means-like%20procedure%2C%20while%20the%20optimization%20is%20conducted%20in%20an%20end-to-end%20manner.%20This%20paper%20investigates%20whether%20the%20deep-learned%20representation%20has%20enabled%20DEC%20to%20overcome%20the%20known%20fundamental%20limitations%20of%20%24k%24-means%20clustering%2C%20i.e.%2C%20its%20inability%20to%20discover%20clusters%20of%20arbitrary%20shapes%2C%20varied%20sizes%20and%20densities.%20Our%20investigations%20on%20DEC%20have%20a%20wider%20implication%20on%20deep%20clustering%20methods%20in%20general.%20Notably%2C%20none%20of%20these%20methods%20exploit%20the%20underlying%20data%20distribution.%20We%20uncover%20that%20a%20non-deep%20learning%20approach%20achieves%20the%20intended%20aim%20of%20deep%20clustering%20by%20making%20use%20of%20distributional%20information%20of%20clusters%20in%20a%20dataset%20to%20effectively%20address%20these%20fundamental%20limitations.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Achieve%2520the%2520Intended%2520Aim%2520of%2520Deep%2520Clustering%2520Now%252C%2520without%2520Deep%2520Learning%26entry.906535625%3DKai%2520Ming%2520Ting%2520and%2520Wei-Jie%2520Xu%2520and%2520Hang%2520Zhang%26entry.1292438233%3DDeep%2520clustering%2520%2528DC%2529%2520is%2520often%2520quoted%2520to%2520have%2520a%2520key%2520advantage%2520over%2520%2524k%2524-means%2520clustering.%2520Yet%252C%2520this%2520advantage%2520is%2520often%2520demonstrated%2520using%2520image%2520datasets%2520only%252C%2520and%2520it%2520is%2520unclear%2520whether%2520it%2520addresses%2520the%2520fundamental%2520limitations%2520of%2520%2524k%2524-means%2520clustering.%2520Deep%2520Embedded%2520Clustering%2520%2528DEC%2529%2520learns%2520a%2520latent%2520representation%2520via%2520an%2520autoencoder%2520and%2520performs%2520clustering%2520based%2520on%2520a%2520%2524k%2524-means-like%2520procedure%252C%2520while%2520the%2520optimization%2520is%2520conducted%2520in%2520an%2520end-to-end%2520manner.%2520This%2520paper%2520investigates%2520whether%2520the%2520deep-learned%2520representation%2520has%2520enabled%2520DEC%2520to%2520overcome%2520the%2520known%2520fundamental%2520limitations%2520of%2520%2524k%2524-means%2520clustering%252C%2520i.e.%252C%2520its%2520inability%2520to%2520discover%2520clusters%2520of%2520arbitrary%2520shapes%252C%2520varied%2520sizes%2520and%2520densities.%2520Our%2520investigations%2520on%2520DEC%2520have%2520a%2520wider%2520implication%2520on%2520deep%2520clustering%2520methods%2520in%2520general.%2520Notably%252C%2520none%2520of%2520these%2520methods%2520exploit%2520the%2520underlying%2520data%2520distribution.%2520We%2520uncover%2520that%2520a%2520non-deep%2520learning%2520approach%2520achieves%2520the%2520intended%2520aim%2520of%2520deep%2520clustering%2520by%2520making%2520use%2520of%2520distributional%2520information%2520of%2520clusters%2520in%2520a%2520dataset%2520to%2520effectively%2520address%2520these%2520fundamental%2520limitations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Achieve%20the%20Intended%20Aim%20of%20Deep%20Clustering%20Now%2C%20without%20Deep%20Learning&entry.906535625=Kai%20Ming%20Ting%20and%20Wei-Jie%20Xu%20and%20Hang%20Zhang&entry.1292438233=Deep%20clustering%20%28DC%29%20is%20often%20quoted%20to%20have%20a%20key%20advantage%20over%20%24k%24-means%20clustering.%20Yet%2C%20this%20advantage%20is%20often%20demonstrated%20using%20image%20datasets%20only%2C%20and%20it%20is%20unclear%20whether%20it%20addresses%20the%20fundamental%20limitations%20of%20%24k%24-means%20clustering.%20Deep%20Embedded%20Clustering%20%28DEC%29%20learns%20a%20latent%20representation%20via%20an%20autoencoder%20and%20performs%20clustering%20based%20on%20a%20%24k%24-means-like%20procedure%2C%20while%20the%20optimization%20is%20conducted%20in%20an%20end-to-end%20manner.%20This%20paper%20investigates%20whether%20the%20deep-learned%20representation%20has%20enabled%20DEC%20to%20overcome%20the%20known%20fundamental%20limitations%20of%20%24k%24-means%20clustering%2C%20i.e.%2C%20its%20inability%20to%20discover%20clusters%20of%20arbitrary%20shapes%2C%20varied%20sizes%20and%20densities.%20Our%20investigations%20on%20DEC%20have%20a%20wider%20implication%20on%20deep%20clustering%20methods%20in%20general.%20Notably%2C%20none%20of%20these%20methods%20exploit%20the%20underlying%20data%20distribution.%20We%20uncover%20that%20a%20non-deep%20learning%20approach%20achieves%20the%20intended%20aim%20of%20deep%20clustering%20by%20making%20use%20of%20distributional%20information%20of%20clusters%20in%20a%20dataset%20to%20effectively%20address%20these%20fundamental%20limitations.&entry.1838667208=http%3A//arxiv.org/abs/2602.05749v1&entry.124074799=Read"},
{"title": "Verification of the Implicit World Model in a Generative Model via Adversarial Sequences", "author": "Andr\u00e1s Balogh and M\u00e1rk Jelasity", "abstract": "Generative sequence models are typically trained on sample sequences from natural or formal languages. It is a crucial question whether -- or to what extent -- sample-based training is able to capture the true structure of these languages, often referred to as the ``world model''. Theoretical results indicate that we can hope for soundness at best, that is, generating valid sequences, but not necessarily all of them. However, it is still important to have practical tools that are able to verify whether a given sequence model is sound. In this study, we focus on chess, as it is a domain that provides enough complexity while having a simple rule-based world model. We propose adversarial sequence generation for verifying the soundness of the sequence model. Our adversaries generate valid sequences so as to force the sequence model to generate an invalid next move prediction. Apart from the falsification of soundness, this method is also suitable for a more fine-grained analysis of the failure modes and the effects of different choices during training. To demonstrate this, we propose a number of methods for adversarial sequence generation and evaluate the approach on a large set of chess models. We train models on random as well as high-quality chess games, using several training recipes. We find that none of the models are sound, but some training techniques and dataset choices are able to improve soundness remarkably. We also investigate the potential application of board state probes in both our training and attack methods. Our findings indicate that the extracted board states have no causal role in next token prediction in most of the models.", "link": "http://arxiv.org/abs/2602.05903v1", "date": "2026-02-05", "relevancy": 2.4976, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5313}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4941}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Verification%20of%20the%20Implicit%20World%20Model%20in%20a%20Generative%20Model%20via%20Adversarial%20Sequences&body=Title%3A%20Verification%20of%20the%20Implicit%20World%20Model%20in%20a%20Generative%20Model%20via%20Adversarial%20Sequences%0AAuthor%3A%20Andr%C3%A1s%20Balogh%20and%20M%C3%A1rk%20Jelasity%0AAbstract%3A%20Generative%20sequence%20models%20are%20typically%20trained%20on%20sample%20sequences%20from%20natural%20or%20formal%20languages.%20It%20is%20a%20crucial%20question%20whether%20--%20or%20to%20what%20extent%20--%20sample-based%20training%20is%20able%20to%20capture%20the%20true%20structure%20of%20these%20languages%2C%20often%20referred%20to%20as%20the%20%60%60world%20model%27%27.%20Theoretical%20results%20indicate%20that%20we%20can%20hope%20for%20soundness%20at%20best%2C%20that%20is%2C%20generating%20valid%20sequences%2C%20but%20not%20necessarily%20all%20of%20them.%20However%2C%20it%20is%20still%20important%20to%20have%20practical%20tools%20that%20are%20able%20to%20verify%20whether%20a%20given%20sequence%20model%20is%20sound.%20In%20this%20study%2C%20we%20focus%20on%20chess%2C%20as%20it%20is%20a%20domain%20that%20provides%20enough%20complexity%20while%20having%20a%20simple%20rule-based%20world%20model.%20We%20propose%20adversarial%20sequence%20generation%20for%20verifying%20the%20soundness%20of%20the%20sequence%20model.%20Our%20adversaries%20generate%20valid%20sequences%20so%20as%20to%20force%20the%20sequence%20model%20to%20generate%20an%20invalid%20next%20move%20prediction.%20Apart%20from%20the%20falsification%20of%20soundness%2C%20this%20method%20is%20also%20suitable%20for%20a%20more%20fine-grained%20analysis%20of%20the%20failure%20modes%20and%20the%20effects%20of%20different%20choices%20during%20training.%20To%20demonstrate%20this%2C%20we%20propose%20a%20number%20of%20methods%20for%20adversarial%20sequence%20generation%20and%20evaluate%20the%20approach%20on%20a%20large%20set%20of%20chess%20models.%20We%20train%20models%20on%20random%20as%20well%20as%20high-quality%20chess%20games%2C%20using%20several%20training%20recipes.%20We%20find%20that%20none%20of%20the%20models%20are%20sound%2C%20but%20some%20training%20techniques%20and%20dataset%20choices%20are%20able%20to%20improve%20soundness%20remarkably.%20We%20also%20investigate%20the%20potential%20application%20of%20board%20state%20probes%20in%20both%20our%20training%20and%20attack%20methods.%20Our%20findings%20indicate%20that%20the%20extracted%20board%20states%20have%20no%20causal%20role%20in%20next%20token%20prediction%20in%20most%20of%20the%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05903v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerification%2520of%2520the%2520Implicit%2520World%2520Model%2520in%2520a%2520Generative%2520Model%2520via%2520Adversarial%2520Sequences%26entry.906535625%3DAndr%25C3%25A1s%2520Balogh%2520and%2520M%25C3%25A1rk%2520Jelasity%26entry.1292438233%3DGenerative%2520sequence%2520models%2520are%2520typically%2520trained%2520on%2520sample%2520sequences%2520from%2520natural%2520or%2520formal%2520languages.%2520It%2520is%2520a%2520crucial%2520question%2520whether%2520--%2520or%2520to%2520what%2520extent%2520--%2520sample-based%2520training%2520is%2520able%2520to%2520capture%2520the%2520true%2520structure%2520of%2520these%2520languages%252C%2520often%2520referred%2520to%2520as%2520the%2520%2560%2560world%2520model%2527%2527.%2520Theoretical%2520results%2520indicate%2520that%2520we%2520can%2520hope%2520for%2520soundness%2520at%2520best%252C%2520that%2520is%252C%2520generating%2520valid%2520sequences%252C%2520but%2520not%2520necessarily%2520all%2520of%2520them.%2520However%252C%2520it%2520is%2520still%2520important%2520to%2520have%2520practical%2520tools%2520that%2520are%2520able%2520to%2520verify%2520whether%2520a%2520given%2520sequence%2520model%2520is%2520sound.%2520In%2520this%2520study%252C%2520we%2520focus%2520on%2520chess%252C%2520as%2520it%2520is%2520a%2520domain%2520that%2520provides%2520enough%2520complexity%2520while%2520having%2520a%2520simple%2520rule-based%2520world%2520model.%2520We%2520propose%2520adversarial%2520sequence%2520generation%2520for%2520verifying%2520the%2520soundness%2520of%2520the%2520sequence%2520model.%2520Our%2520adversaries%2520generate%2520valid%2520sequences%2520so%2520as%2520to%2520force%2520the%2520sequence%2520model%2520to%2520generate%2520an%2520invalid%2520next%2520move%2520prediction.%2520Apart%2520from%2520the%2520falsification%2520of%2520soundness%252C%2520this%2520method%2520is%2520also%2520suitable%2520for%2520a%2520more%2520fine-grained%2520analysis%2520of%2520the%2520failure%2520modes%2520and%2520the%2520effects%2520of%2520different%2520choices%2520during%2520training.%2520To%2520demonstrate%2520this%252C%2520we%2520propose%2520a%2520number%2520of%2520methods%2520for%2520adversarial%2520sequence%2520generation%2520and%2520evaluate%2520the%2520approach%2520on%2520a%2520large%2520set%2520of%2520chess%2520models.%2520We%2520train%2520models%2520on%2520random%2520as%2520well%2520as%2520high-quality%2520chess%2520games%252C%2520using%2520several%2520training%2520recipes.%2520We%2520find%2520that%2520none%2520of%2520the%2520models%2520are%2520sound%252C%2520but%2520some%2520training%2520techniques%2520and%2520dataset%2520choices%2520are%2520able%2520to%2520improve%2520soundness%2520remarkably.%2520We%2520also%2520investigate%2520the%2520potential%2520application%2520of%2520board%2520state%2520probes%2520in%2520both%2520our%2520training%2520and%2520attack%2520methods.%2520Our%2520findings%2520indicate%2520that%2520the%2520extracted%2520board%2520states%2520have%2520no%2520causal%2520role%2520in%2520next%2520token%2520prediction%2520in%2520most%2520of%2520the%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05903v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Verification%20of%20the%20Implicit%20World%20Model%20in%20a%20Generative%20Model%20via%20Adversarial%20Sequences&entry.906535625=Andr%C3%A1s%20Balogh%20and%20M%C3%A1rk%20Jelasity&entry.1292438233=Generative%20sequence%20models%20are%20typically%20trained%20on%20sample%20sequences%20from%20natural%20or%20formal%20languages.%20It%20is%20a%20crucial%20question%20whether%20--%20or%20to%20what%20extent%20--%20sample-based%20training%20is%20able%20to%20capture%20the%20true%20structure%20of%20these%20languages%2C%20often%20referred%20to%20as%20the%20%60%60world%20model%27%27.%20Theoretical%20results%20indicate%20that%20we%20can%20hope%20for%20soundness%20at%20best%2C%20that%20is%2C%20generating%20valid%20sequences%2C%20but%20not%20necessarily%20all%20of%20them.%20However%2C%20it%20is%20still%20important%20to%20have%20practical%20tools%20that%20are%20able%20to%20verify%20whether%20a%20given%20sequence%20model%20is%20sound.%20In%20this%20study%2C%20we%20focus%20on%20chess%2C%20as%20it%20is%20a%20domain%20that%20provides%20enough%20complexity%20while%20having%20a%20simple%20rule-based%20world%20model.%20We%20propose%20adversarial%20sequence%20generation%20for%20verifying%20the%20soundness%20of%20the%20sequence%20model.%20Our%20adversaries%20generate%20valid%20sequences%20so%20as%20to%20force%20the%20sequence%20model%20to%20generate%20an%20invalid%20next%20move%20prediction.%20Apart%20from%20the%20falsification%20of%20soundness%2C%20this%20method%20is%20also%20suitable%20for%20a%20more%20fine-grained%20analysis%20of%20the%20failure%20modes%20and%20the%20effects%20of%20different%20choices%20during%20training.%20To%20demonstrate%20this%2C%20we%20propose%20a%20number%20of%20methods%20for%20adversarial%20sequence%20generation%20and%20evaluate%20the%20approach%20on%20a%20large%20set%20of%20chess%20models.%20We%20train%20models%20on%20random%20as%20well%20as%20high-quality%20chess%20games%2C%20using%20several%20training%20recipes.%20We%20find%20that%20none%20of%20the%20models%20are%20sound%2C%20but%20some%20training%20techniques%20and%20dataset%20choices%20are%20able%20to%20improve%20soundness%20remarkably.%20We%20also%20investigate%20the%20potential%20application%20of%20board%20state%20probes%20in%20both%20our%20training%20and%20attack%20methods.%20Our%20findings%20indicate%20that%20the%20extracted%20board%20states%20have%20no%20causal%20role%20in%20next%20token%20prediction%20in%20most%20of%20the%20models.&entry.1838667208=http%3A//arxiv.org/abs/2602.05903v1&entry.124074799=Read"},
{"title": "Alignment-Aware Model Adaptation via Feedback-Guided Optimization", "author": "Gaurav Bhatt and Aditya Chinchure and Jiawei Zhou and Leonid Sigal", "abstract": "Fine-tuning is the primary mechanism for adapting foundation models to downstream tasks; however, standard approaches largely optimize task objectives in isolation and do not account for secondary yet critical alignment objectives (e.g., safety and hallucination avoidance). As a result, downstream fine-tuning can degrade alignment and fail to correct pre-existing misaligned behavior. We propose an alignment-aware fine-tuning framework that integrates feedback from an external alignment signal through policy-gradient-based regularization. Our method introduces an adaptive gating mechanism that dynamically balances supervised and alignment-driven gradients on a per-sample basis, prioritizing uncertain or misaligned cases while allowing well-aligned examples to follow standard supervised updates. The framework further learns abstention behavior for fully misaligned inputs, incorporating conservative responses directly into the fine-tuned model. Experiments on general and domain-specific instruction-tuning benchmarks demonstrate consistent reductions in harmful and hallucinated outputs without sacrificing downstream task performance. Additional analyses show robustness to adversarial fine-tuning, prompt-based attacks, and unsafe initializations, establishing adaptively gated alignment optimization as an effective approach for alignment-preserving and alignment-recovering model adaptation.", "link": "http://arxiv.org/abs/2602.02258v2", "date": "2026-02-05", "relevancy": 2.4933, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5048}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5047}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alignment-Aware%20Model%20Adaptation%20via%20Feedback-Guided%20Optimization&body=Title%3A%20Alignment-Aware%20Model%20Adaptation%20via%20Feedback-Guided%20Optimization%0AAuthor%3A%20Gaurav%20Bhatt%20and%20Aditya%20Chinchure%20and%20Jiawei%20Zhou%20and%20Leonid%20Sigal%0AAbstract%3A%20Fine-tuning%20is%20the%20primary%20mechanism%20for%20adapting%20foundation%20models%20to%20downstream%20tasks%3B%20however%2C%20standard%20approaches%20largely%20optimize%20task%20objectives%20in%20isolation%20and%20do%20not%20account%20for%20secondary%20yet%20critical%20alignment%20objectives%20%28e.g.%2C%20safety%20and%20hallucination%20avoidance%29.%20As%20a%20result%2C%20downstream%20fine-tuning%20can%20degrade%20alignment%20and%20fail%20to%20correct%20pre-existing%20misaligned%20behavior.%20We%20propose%20an%20alignment-aware%20fine-tuning%20framework%20that%20integrates%20feedback%20from%20an%20external%20alignment%20signal%20through%20policy-gradient-based%20regularization.%20Our%20method%20introduces%20an%20adaptive%20gating%20mechanism%20that%20dynamically%20balances%20supervised%20and%20alignment-driven%20gradients%20on%20a%20per-sample%20basis%2C%20prioritizing%20uncertain%20or%20misaligned%20cases%20while%20allowing%20well-aligned%20examples%20to%20follow%20standard%20supervised%20updates.%20The%20framework%20further%20learns%20abstention%20behavior%20for%20fully%20misaligned%20inputs%2C%20incorporating%20conservative%20responses%20directly%20into%20the%20fine-tuned%20model.%20Experiments%20on%20general%20and%20domain-specific%20instruction-tuning%20benchmarks%20demonstrate%20consistent%20reductions%20in%20harmful%20and%20hallucinated%20outputs%20without%20sacrificing%20downstream%20task%20performance.%20Additional%20analyses%20show%20robustness%20to%20adversarial%20fine-tuning%2C%20prompt-based%20attacks%2C%20and%20unsafe%20initializations%2C%20establishing%20adaptively%20gated%20alignment%20optimization%20as%20an%20effective%20approach%20for%20alignment-preserving%20and%20alignment-recovering%20model%20adaptation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02258v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignment-Aware%2520Model%2520Adaptation%2520via%2520Feedback-Guided%2520Optimization%26entry.906535625%3DGaurav%2520Bhatt%2520and%2520Aditya%2520Chinchure%2520and%2520Jiawei%2520Zhou%2520and%2520Leonid%2520Sigal%26entry.1292438233%3DFine-tuning%2520is%2520the%2520primary%2520mechanism%2520for%2520adapting%2520foundation%2520models%2520to%2520downstream%2520tasks%253B%2520however%252C%2520standard%2520approaches%2520largely%2520optimize%2520task%2520objectives%2520in%2520isolation%2520and%2520do%2520not%2520account%2520for%2520secondary%2520yet%2520critical%2520alignment%2520objectives%2520%2528e.g.%252C%2520safety%2520and%2520hallucination%2520avoidance%2529.%2520As%2520a%2520result%252C%2520downstream%2520fine-tuning%2520can%2520degrade%2520alignment%2520and%2520fail%2520to%2520correct%2520pre-existing%2520misaligned%2520behavior.%2520We%2520propose%2520an%2520alignment-aware%2520fine-tuning%2520framework%2520that%2520integrates%2520feedback%2520from%2520an%2520external%2520alignment%2520signal%2520through%2520policy-gradient-based%2520regularization.%2520Our%2520method%2520introduces%2520an%2520adaptive%2520gating%2520mechanism%2520that%2520dynamically%2520balances%2520supervised%2520and%2520alignment-driven%2520gradients%2520on%2520a%2520per-sample%2520basis%252C%2520prioritizing%2520uncertain%2520or%2520misaligned%2520cases%2520while%2520allowing%2520well-aligned%2520examples%2520to%2520follow%2520standard%2520supervised%2520updates.%2520The%2520framework%2520further%2520learns%2520abstention%2520behavior%2520for%2520fully%2520misaligned%2520inputs%252C%2520incorporating%2520conservative%2520responses%2520directly%2520into%2520the%2520fine-tuned%2520model.%2520Experiments%2520on%2520general%2520and%2520domain-specific%2520instruction-tuning%2520benchmarks%2520demonstrate%2520consistent%2520reductions%2520in%2520harmful%2520and%2520hallucinated%2520outputs%2520without%2520sacrificing%2520downstream%2520task%2520performance.%2520Additional%2520analyses%2520show%2520robustness%2520to%2520adversarial%2520fine-tuning%252C%2520prompt-based%2520attacks%252C%2520and%2520unsafe%2520initializations%252C%2520establishing%2520adaptively%2520gated%2520alignment%2520optimization%2520as%2520an%2520effective%2520approach%2520for%2520alignment-preserving%2520and%2520alignment-recovering%2520model%2520adaptation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02258v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alignment-Aware%20Model%20Adaptation%20via%20Feedback-Guided%20Optimization&entry.906535625=Gaurav%20Bhatt%20and%20Aditya%20Chinchure%20and%20Jiawei%20Zhou%20and%20Leonid%20Sigal&entry.1292438233=Fine-tuning%20is%20the%20primary%20mechanism%20for%20adapting%20foundation%20models%20to%20downstream%20tasks%3B%20however%2C%20standard%20approaches%20largely%20optimize%20task%20objectives%20in%20isolation%20and%20do%20not%20account%20for%20secondary%20yet%20critical%20alignment%20objectives%20%28e.g.%2C%20safety%20and%20hallucination%20avoidance%29.%20As%20a%20result%2C%20downstream%20fine-tuning%20can%20degrade%20alignment%20and%20fail%20to%20correct%20pre-existing%20misaligned%20behavior.%20We%20propose%20an%20alignment-aware%20fine-tuning%20framework%20that%20integrates%20feedback%20from%20an%20external%20alignment%20signal%20through%20policy-gradient-based%20regularization.%20Our%20method%20introduces%20an%20adaptive%20gating%20mechanism%20that%20dynamically%20balances%20supervised%20and%20alignment-driven%20gradients%20on%20a%20per-sample%20basis%2C%20prioritizing%20uncertain%20or%20misaligned%20cases%20while%20allowing%20well-aligned%20examples%20to%20follow%20standard%20supervised%20updates.%20The%20framework%20further%20learns%20abstention%20behavior%20for%20fully%20misaligned%20inputs%2C%20incorporating%20conservative%20responses%20directly%20into%20the%20fine-tuned%20model.%20Experiments%20on%20general%20and%20domain-specific%20instruction-tuning%20benchmarks%20demonstrate%20consistent%20reductions%20in%20harmful%20and%20hallucinated%20outputs%20without%20sacrificing%20downstream%20task%20performance.%20Additional%20analyses%20show%20robustness%20to%20adversarial%20fine-tuning%2C%20prompt-based%20attacks%2C%20and%20unsafe%20initializations%2C%20establishing%20adaptively%20gated%20alignment%20optimization%20as%20an%20effective%20approach%20for%20alignment-preserving%20and%20alignment-recovering%20model%20adaptation.&entry.1838667208=http%3A//arxiv.org/abs/2602.02258v2&entry.124074799=Read"},
{"title": "FMPose3D: monocular 3D pose estimation via flow matching", "author": "Ti Wang and Xiaohang Yu and Mackenzie Weygandt Mathis", "abstract": "Monocular 3D pose estimation is fundamentally ill-posed due to depth ambiguity and occlusions, thereby motivating probabilistic methods that generate multiple plausible 3D pose hypotheses. In particular, diffusion-based models have recently demonstrated strong performance, but their iterative denoising process typically requires many timesteps for each prediction, making inference computationally expensive. In contrast, we leverage Flow Matching (FM) to learn a velocity field defined by an Ordinary Differential Equation (ODE), enabling efficient generation of 3D pose samples with only a few integration steps. We propose a novel generative pose estimation framework, FMPose3D, that formulates 3D pose estimation as a conditional distribution transport problem. It continuously transports samples from a standard Gaussian prior to the distribution of plausible 3D poses conditioned only on 2D inputs. Although ODE trajectories are deterministic, FMPose3D naturally generates various pose hypotheses by sampling different noise seeds. To obtain a single accurate prediction from those hypotheses, we further introduce a Reprojection-based Posterior Expectation Aggregation (RPEA) module, which approximates the Bayesian posterior expectation over 3D hypotheses. FMPose3D surpasses existing methods on the widely used human pose estimation benchmarks Human3.6M and MPI-INF-3DHP, and further achieves state-of-the-art performance on the 3D animal pose datasets Animal3D and CtrlAni3D, demonstrating strong performance across both 3D pose domains. The code is available at https://github.com/AdaptiveMotorControlLab/FMPose3D.", "link": "http://arxiv.org/abs/2602.05755v1", "date": "2026-02-05", "relevancy": 2.4924, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6772}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5868}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FMPose3D%3A%20monocular%203D%20pose%20estimation%20via%20flow%20matching&body=Title%3A%20FMPose3D%3A%20monocular%203D%20pose%20estimation%20via%20flow%20matching%0AAuthor%3A%20Ti%20Wang%20and%20Xiaohang%20Yu%20and%20Mackenzie%20Weygandt%20Mathis%0AAbstract%3A%20Monocular%203D%20pose%20estimation%20is%20fundamentally%20ill-posed%20due%20to%20depth%20ambiguity%20and%20occlusions%2C%20thereby%20motivating%20probabilistic%20methods%20that%20generate%20multiple%20plausible%203D%20pose%20hypotheses.%20In%20particular%2C%20diffusion-based%20models%20have%20recently%20demonstrated%20strong%20performance%2C%20but%20their%20iterative%20denoising%20process%20typically%20requires%20many%20timesteps%20for%20each%20prediction%2C%20making%20inference%20computationally%20expensive.%20In%20contrast%2C%20we%20leverage%20Flow%20Matching%20%28FM%29%20to%20learn%20a%20velocity%20field%20defined%20by%20an%20Ordinary%20Differential%20Equation%20%28ODE%29%2C%20enabling%20efficient%20generation%20of%203D%20pose%20samples%20with%20only%20a%20few%20integration%20steps.%20We%20propose%20a%20novel%20generative%20pose%20estimation%20framework%2C%20FMPose3D%2C%20that%20formulates%203D%20pose%20estimation%20as%20a%20conditional%20distribution%20transport%20problem.%20It%20continuously%20transports%20samples%20from%20a%20standard%20Gaussian%20prior%20to%20the%20distribution%20of%20plausible%203D%20poses%20conditioned%20only%20on%202D%20inputs.%20Although%20ODE%20trajectories%20are%20deterministic%2C%20FMPose3D%20naturally%20generates%20various%20pose%20hypotheses%20by%20sampling%20different%20noise%20seeds.%20To%20obtain%20a%20single%20accurate%20prediction%20from%20those%20hypotheses%2C%20we%20further%20introduce%20a%20Reprojection-based%20Posterior%20Expectation%20Aggregation%20%28RPEA%29%20module%2C%20which%20approximates%20the%20Bayesian%20posterior%20expectation%20over%203D%20hypotheses.%20FMPose3D%20surpasses%20existing%20methods%20on%20the%20widely%20used%20human%20pose%20estimation%20benchmarks%20Human3.6M%20and%20MPI-INF-3DHP%2C%20and%20further%20achieves%20state-of-the-art%20performance%20on%20the%203D%20animal%20pose%20datasets%20Animal3D%20and%20CtrlAni3D%2C%20demonstrating%20strong%20performance%20across%20both%203D%20pose%20domains.%20The%20code%20is%20available%20at%20https%3A//github.com/AdaptiveMotorControlLab/FMPose3D.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFMPose3D%253A%2520monocular%25203D%2520pose%2520estimation%2520via%2520flow%2520matching%26entry.906535625%3DTi%2520Wang%2520and%2520Xiaohang%2520Yu%2520and%2520Mackenzie%2520Weygandt%2520Mathis%26entry.1292438233%3DMonocular%25203D%2520pose%2520estimation%2520is%2520fundamentally%2520ill-posed%2520due%2520to%2520depth%2520ambiguity%2520and%2520occlusions%252C%2520thereby%2520motivating%2520probabilistic%2520methods%2520that%2520generate%2520multiple%2520plausible%25203D%2520pose%2520hypotheses.%2520In%2520particular%252C%2520diffusion-based%2520models%2520have%2520recently%2520demonstrated%2520strong%2520performance%252C%2520but%2520their%2520iterative%2520denoising%2520process%2520typically%2520requires%2520many%2520timesteps%2520for%2520each%2520prediction%252C%2520making%2520inference%2520computationally%2520expensive.%2520In%2520contrast%252C%2520we%2520leverage%2520Flow%2520Matching%2520%2528FM%2529%2520to%2520learn%2520a%2520velocity%2520field%2520defined%2520by%2520an%2520Ordinary%2520Differential%2520Equation%2520%2528ODE%2529%252C%2520enabling%2520efficient%2520generation%2520of%25203D%2520pose%2520samples%2520with%2520only%2520a%2520few%2520integration%2520steps.%2520We%2520propose%2520a%2520novel%2520generative%2520pose%2520estimation%2520framework%252C%2520FMPose3D%252C%2520that%2520formulates%25203D%2520pose%2520estimation%2520as%2520a%2520conditional%2520distribution%2520transport%2520problem.%2520It%2520continuously%2520transports%2520samples%2520from%2520a%2520standard%2520Gaussian%2520prior%2520to%2520the%2520distribution%2520of%2520plausible%25203D%2520poses%2520conditioned%2520only%2520on%25202D%2520inputs.%2520Although%2520ODE%2520trajectories%2520are%2520deterministic%252C%2520FMPose3D%2520naturally%2520generates%2520various%2520pose%2520hypotheses%2520by%2520sampling%2520different%2520noise%2520seeds.%2520To%2520obtain%2520a%2520single%2520accurate%2520prediction%2520from%2520those%2520hypotheses%252C%2520we%2520further%2520introduce%2520a%2520Reprojection-based%2520Posterior%2520Expectation%2520Aggregation%2520%2528RPEA%2529%2520module%252C%2520which%2520approximates%2520the%2520Bayesian%2520posterior%2520expectation%2520over%25203D%2520hypotheses.%2520FMPose3D%2520surpasses%2520existing%2520methods%2520on%2520the%2520widely%2520used%2520human%2520pose%2520estimation%2520benchmarks%2520Human3.6M%2520and%2520MPI-INF-3DHP%252C%2520and%2520further%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%25203D%2520animal%2520pose%2520datasets%2520Animal3D%2520and%2520CtrlAni3D%252C%2520demonstrating%2520strong%2520performance%2520across%2520both%25203D%2520pose%2520domains.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/AdaptiveMotorControlLab/FMPose3D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FMPose3D%3A%20monocular%203D%20pose%20estimation%20via%20flow%20matching&entry.906535625=Ti%20Wang%20and%20Xiaohang%20Yu%20and%20Mackenzie%20Weygandt%20Mathis&entry.1292438233=Monocular%203D%20pose%20estimation%20is%20fundamentally%20ill-posed%20due%20to%20depth%20ambiguity%20and%20occlusions%2C%20thereby%20motivating%20probabilistic%20methods%20that%20generate%20multiple%20plausible%203D%20pose%20hypotheses.%20In%20particular%2C%20diffusion-based%20models%20have%20recently%20demonstrated%20strong%20performance%2C%20but%20their%20iterative%20denoising%20process%20typically%20requires%20many%20timesteps%20for%20each%20prediction%2C%20making%20inference%20computationally%20expensive.%20In%20contrast%2C%20we%20leverage%20Flow%20Matching%20%28FM%29%20to%20learn%20a%20velocity%20field%20defined%20by%20an%20Ordinary%20Differential%20Equation%20%28ODE%29%2C%20enabling%20efficient%20generation%20of%203D%20pose%20samples%20with%20only%20a%20few%20integration%20steps.%20We%20propose%20a%20novel%20generative%20pose%20estimation%20framework%2C%20FMPose3D%2C%20that%20formulates%203D%20pose%20estimation%20as%20a%20conditional%20distribution%20transport%20problem.%20It%20continuously%20transports%20samples%20from%20a%20standard%20Gaussian%20prior%20to%20the%20distribution%20of%20plausible%203D%20poses%20conditioned%20only%20on%202D%20inputs.%20Although%20ODE%20trajectories%20are%20deterministic%2C%20FMPose3D%20naturally%20generates%20various%20pose%20hypotheses%20by%20sampling%20different%20noise%20seeds.%20To%20obtain%20a%20single%20accurate%20prediction%20from%20those%20hypotheses%2C%20we%20further%20introduce%20a%20Reprojection-based%20Posterior%20Expectation%20Aggregation%20%28RPEA%29%20module%2C%20which%20approximates%20the%20Bayesian%20posterior%20expectation%20over%203D%20hypotheses.%20FMPose3D%20surpasses%20existing%20methods%20on%20the%20widely%20used%20human%20pose%20estimation%20benchmarks%20Human3.6M%20and%20MPI-INF-3DHP%2C%20and%20further%20achieves%20state-of-the-art%20performance%20on%20the%203D%20animal%20pose%20datasets%20Animal3D%20and%20CtrlAni3D%2C%20demonstrating%20strong%20performance%20across%20both%203D%20pose%20domains.%20The%20code%20is%20available%20at%20https%3A//github.com/AdaptiveMotorControlLab/FMPose3D.&entry.1838667208=http%3A//arxiv.org/abs/2602.05755v1&entry.124074799=Read"},
{"title": "Feature Engineering is Not Dead: Reviving Classical Machine Learning with Entropy, HOG, and LBP Feature Fusion for Image Classification", "author": "Abhijit Sen and Giridas Maiti and Bikram K. Parida and Bhanu P. Mishra and Mahima Arya and Denys I. Bondar", "abstract": "Feature engineering continues to play a critical role in image classification, particularly when interpretability and computational efficiency are prioritized over deep learning models with millions of parameters. In this study, we revisit classical machine learning based image classification through a novel approach centered on Permutation Entropy (PE), a robust and computationally lightweight measure traditionally used in time series analysis but rarely applied to image data. We extend PE to two-dimensional images and propose a multiscale, multi-orientation entropy-based feature extraction approach that characterizes spatial order and complexity along rows, columns, diagonals, anti-diagonals, and local patches of the image. To enhance the discriminatory power of the entropy features, we integrate two classic image descriptors: the Histogram of Oriented Gradients (HOG) to capture shape and edge structure, and Local Binary Patterns (LBP) to encode micro-texture of an image. The resulting hand-crafted feature set, comprising of 780 dimensions, is used to train Support Vector Machine (SVM) classifiers optimized through grid search. The proposed approach is evaluated on multiple benchmark datasets, including Fashion-MNIST, KMNIST, EMNIST, and CIFAR-10, where it delivers competitive classification performance without relying on deep architectures. Our results demonstrate that the fusion of PE with HOG and LBP provides a compact, interpretable, and effective alternative to computationally expensive and limited interpretable deep learning models. This shows a potential of entropy-based descriptors in image classification and contributes a lightweight and generalizable solution to interpretable machine learning in image classification and computer vision.", "link": "http://arxiv.org/abs/2507.13772v2", "date": "2026-02-05", "relevancy": 2.4718, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5121}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4879}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Engineering%20is%20Not%20Dead%3A%20Reviving%20Classical%20Machine%20Learning%20with%20Entropy%2C%20HOG%2C%20and%20LBP%20Feature%20Fusion%20for%20Image%20Classification&body=Title%3A%20Feature%20Engineering%20is%20Not%20Dead%3A%20Reviving%20Classical%20Machine%20Learning%20with%20Entropy%2C%20HOG%2C%20and%20LBP%20Feature%20Fusion%20for%20Image%20Classification%0AAuthor%3A%20Abhijit%20Sen%20and%20Giridas%20Maiti%20and%20Bikram%20K.%20Parida%20and%20Bhanu%20P.%20Mishra%20and%20Mahima%20Arya%20and%20Denys%20I.%20Bondar%0AAbstract%3A%20Feature%20engineering%20continues%20to%20play%20a%20critical%20role%20in%20image%20classification%2C%20particularly%20when%20interpretability%20and%20computational%20efficiency%20are%20prioritized%20over%20deep%20learning%20models%20with%20millions%20of%20parameters.%20In%20this%20study%2C%20we%20revisit%20classical%20machine%20learning%20based%20image%20classification%20through%20a%20novel%20approach%20centered%20on%20Permutation%20Entropy%20%28PE%29%2C%20a%20robust%20and%20computationally%20lightweight%20measure%20traditionally%20used%20in%20time%20series%20analysis%20but%20rarely%20applied%20to%20image%20data.%20We%20extend%20PE%20to%20two-dimensional%20images%20and%20propose%20a%20multiscale%2C%20multi-orientation%20entropy-based%20feature%20extraction%20approach%20that%20characterizes%20spatial%20order%20and%20complexity%20along%20rows%2C%20columns%2C%20diagonals%2C%20anti-diagonals%2C%20and%20local%20patches%20of%20the%20image.%20To%20enhance%20the%20discriminatory%20power%20of%20the%20entropy%20features%2C%20we%20integrate%20two%20classic%20image%20descriptors%3A%20the%20Histogram%20of%20Oriented%20Gradients%20%28HOG%29%20to%20capture%20shape%20and%20edge%20structure%2C%20and%20Local%20Binary%20Patterns%20%28LBP%29%20to%20encode%20micro-texture%20of%20an%20image.%20The%20resulting%20hand-crafted%20feature%20set%2C%20comprising%20of%20780%20dimensions%2C%20is%20used%20to%20train%20Support%20Vector%20Machine%20%28SVM%29%20classifiers%20optimized%20through%20grid%20search.%20The%20proposed%20approach%20is%20evaluated%20on%20multiple%20benchmark%20datasets%2C%20including%20Fashion-MNIST%2C%20KMNIST%2C%20EMNIST%2C%20and%20CIFAR-10%2C%20where%20it%20delivers%20competitive%20classification%20performance%20without%20relying%20on%20deep%20architectures.%20Our%20results%20demonstrate%20that%20the%20fusion%20of%20PE%20with%20HOG%20and%20LBP%20provides%20a%20compact%2C%20interpretable%2C%20and%20effective%20alternative%20to%20computationally%20expensive%20and%20limited%20interpretable%20deep%20learning%20models.%20This%20shows%20a%20potential%20of%20entropy-based%20descriptors%20in%20image%20classification%20and%20contributes%20a%20lightweight%20and%20generalizable%20solution%20to%20interpretable%20machine%20learning%20in%20image%20classification%20and%20computer%20vision.%0ALink%3A%20http%3A//arxiv.org/abs/2507.13772v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Engineering%2520is%2520Not%2520Dead%253A%2520Reviving%2520Classical%2520Machine%2520Learning%2520with%2520Entropy%252C%2520HOG%252C%2520and%2520LBP%2520Feature%2520Fusion%2520for%2520Image%2520Classification%26entry.906535625%3DAbhijit%2520Sen%2520and%2520Giridas%2520Maiti%2520and%2520Bikram%2520K.%2520Parida%2520and%2520Bhanu%2520P.%2520Mishra%2520and%2520Mahima%2520Arya%2520and%2520Denys%2520I.%2520Bondar%26entry.1292438233%3DFeature%2520engineering%2520continues%2520to%2520play%2520a%2520critical%2520role%2520in%2520image%2520classification%252C%2520particularly%2520when%2520interpretability%2520and%2520computational%2520efficiency%2520are%2520prioritized%2520over%2520deep%2520learning%2520models%2520with%2520millions%2520of%2520parameters.%2520In%2520this%2520study%252C%2520we%2520revisit%2520classical%2520machine%2520learning%2520based%2520image%2520classification%2520through%2520a%2520novel%2520approach%2520centered%2520on%2520Permutation%2520Entropy%2520%2528PE%2529%252C%2520a%2520robust%2520and%2520computationally%2520lightweight%2520measure%2520traditionally%2520used%2520in%2520time%2520series%2520analysis%2520but%2520rarely%2520applied%2520to%2520image%2520data.%2520We%2520extend%2520PE%2520to%2520two-dimensional%2520images%2520and%2520propose%2520a%2520multiscale%252C%2520multi-orientation%2520entropy-based%2520feature%2520extraction%2520approach%2520that%2520characterizes%2520spatial%2520order%2520and%2520complexity%2520along%2520rows%252C%2520columns%252C%2520diagonals%252C%2520anti-diagonals%252C%2520and%2520local%2520patches%2520of%2520the%2520image.%2520To%2520enhance%2520the%2520discriminatory%2520power%2520of%2520the%2520entropy%2520features%252C%2520we%2520integrate%2520two%2520classic%2520image%2520descriptors%253A%2520the%2520Histogram%2520of%2520Oriented%2520Gradients%2520%2528HOG%2529%2520to%2520capture%2520shape%2520and%2520edge%2520structure%252C%2520and%2520Local%2520Binary%2520Patterns%2520%2528LBP%2529%2520to%2520encode%2520micro-texture%2520of%2520an%2520image.%2520The%2520resulting%2520hand-crafted%2520feature%2520set%252C%2520comprising%2520of%2520780%2520dimensions%252C%2520is%2520used%2520to%2520train%2520Support%2520Vector%2520Machine%2520%2528SVM%2529%2520classifiers%2520optimized%2520through%2520grid%2520search.%2520The%2520proposed%2520approach%2520is%2520evaluated%2520on%2520multiple%2520benchmark%2520datasets%252C%2520including%2520Fashion-MNIST%252C%2520KMNIST%252C%2520EMNIST%252C%2520and%2520CIFAR-10%252C%2520where%2520it%2520delivers%2520competitive%2520classification%2520performance%2520without%2520relying%2520on%2520deep%2520architectures.%2520Our%2520results%2520demonstrate%2520that%2520the%2520fusion%2520of%2520PE%2520with%2520HOG%2520and%2520LBP%2520provides%2520a%2520compact%252C%2520interpretable%252C%2520and%2520effective%2520alternative%2520to%2520computationally%2520expensive%2520and%2520limited%2520interpretable%2520deep%2520learning%2520models.%2520This%2520shows%2520a%2520potential%2520of%2520entropy-based%2520descriptors%2520in%2520image%2520classification%2520and%2520contributes%2520a%2520lightweight%2520and%2520generalizable%2520solution%2520to%2520interpretable%2520machine%2520learning%2520in%2520image%2520classification%2520and%2520computer%2520vision.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13772v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Engineering%20is%20Not%20Dead%3A%20Reviving%20Classical%20Machine%20Learning%20with%20Entropy%2C%20HOG%2C%20and%20LBP%20Feature%20Fusion%20for%20Image%20Classification&entry.906535625=Abhijit%20Sen%20and%20Giridas%20Maiti%20and%20Bikram%20K.%20Parida%20and%20Bhanu%20P.%20Mishra%20and%20Mahima%20Arya%20and%20Denys%20I.%20Bondar&entry.1292438233=Feature%20engineering%20continues%20to%20play%20a%20critical%20role%20in%20image%20classification%2C%20particularly%20when%20interpretability%20and%20computational%20efficiency%20are%20prioritized%20over%20deep%20learning%20models%20with%20millions%20of%20parameters.%20In%20this%20study%2C%20we%20revisit%20classical%20machine%20learning%20based%20image%20classification%20through%20a%20novel%20approach%20centered%20on%20Permutation%20Entropy%20%28PE%29%2C%20a%20robust%20and%20computationally%20lightweight%20measure%20traditionally%20used%20in%20time%20series%20analysis%20but%20rarely%20applied%20to%20image%20data.%20We%20extend%20PE%20to%20two-dimensional%20images%20and%20propose%20a%20multiscale%2C%20multi-orientation%20entropy-based%20feature%20extraction%20approach%20that%20characterizes%20spatial%20order%20and%20complexity%20along%20rows%2C%20columns%2C%20diagonals%2C%20anti-diagonals%2C%20and%20local%20patches%20of%20the%20image.%20To%20enhance%20the%20discriminatory%20power%20of%20the%20entropy%20features%2C%20we%20integrate%20two%20classic%20image%20descriptors%3A%20the%20Histogram%20of%20Oriented%20Gradients%20%28HOG%29%20to%20capture%20shape%20and%20edge%20structure%2C%20and%20Local%20Binary%20Patterns%20%28LBP%29%20to%20encode%20micro-texture%20of%20an%20image.%20The%20resulting%20hand-crafted%20feature%20set%2C%20comprising%20of%20780%20dimensions%2C%20is%20used%20to%20train%20Support%20Vector%20Machine%20%28SVM%29%20classifiers%20optimized%20through%20grid%20search.%20The%20proposed%20approach%20is%20evaluated%20on%20multiple%20benchmark%20datasets%2C%20including%20Fashion-MNIST%2C%20KMNIST%2C%20EMNIST%2C%20and%20CIFAR-10%2C%20where%20it%20delivers%20competitive%20classification%20performance%20without%20relying%20on%20deep%20architectures.%20Our%20results%20demonstrate%20that%20the%20fusion%20of%20PE%20with%20HOG%20and%20LBP%20provides%20a%20compact%2C%20interpretable%2C%20and%20effective%20alternative%20to%20computationally%20expensive%20and%20limited%20interpretable%20deep%20learning%20models.%20This%20shows%20a%20potential%20of%20entropy-based%20descriptors%20in%20image%20classification%20and%20contributes%20a%20lightweight%20and%20generalizable%20solution%20to%20interpretable%20machine%20learning%20in%20image%20classification%20and%20computer%20vision.&entry.1838667208=http%3A//arxiv.org/abs/2507.13772v2&entry.124074799=Read"},
{"title": "A Sketch-and-Project Analysis of Subsampled Natural Gradient Algorithms", "author": "Gil Goldshlager and Jiang Hu and Lin Lin", "abstract": "Subsampled natural gradient descent (SNG) has been used to enable high-precision scientific machine learning, but standard analyses based on stochastic preconditioning fail to provide insight into realistic small-sample settings. We overcome this limitation by instead analyzing SNG as a sketch-and-project method. Motivated by this lens, we discard the usual theoretical proxy which decouples gradients and preconditioners using two independent mini-batches, and we replace it with a new proxy based on squared volume sampling. Under this new proxy we show that the expectation of the SNG direction becomes equal to a preconditioned gradient descent step even in the presence of coupling, leading to (i) global convergence guarantees when using a single mini-batch of any size, and (ii) an explicit characterization of the convergence rate in terms of quantities related to the sketch-and-project structure. These findings in turn yield new insights into small-sample settings, for example by suggesting that the advantage of SNG over SGD is that it can more effectively exploit spectral decay in the model Jacobian. We also extend these ideas to explain a popular structured momentum scheme for SNG, known as SPRING, by showing that it arises naturally from accelerated sketch-and-project methods.", "link": "http://arxiv.org/abs/2508.21022v2", "date": "2026-02-05", "relevancy": 2.4632, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4965}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.496}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Sketch-and-Project%20Analysis%20of%20Subsampled%20Natural%20Gradient%20Algorithms&body=Title%3A%20A%20Sketch-and-Project%20Analysis%20of%20Subsampled%20Natural%20Gradient%20Algorithms%0AAuthor%3A%20Gil%20Goldshlager%20and%20Jiang%20Hu%20and%20Lin%20Lin%0AAbstract%3A%20Subsampled%20natural%20gradient%20descent%20%28SNG%29%20has%20been%20used%20to%20enable%20high-precision%20scientific%20machine%20learning%2C%20but%20standard%20analyses%20based%20on%20stochastic%20preconditioning%20fail%20to%20provide%20insight%20into%20realistic%20small-sample%20settings.%20We%20overcome%20this%20limitation%20by%20instead%20analyzing%20SNG%20as%20a%20sketch-and-project%20method.%20Motivated%20by%20this%20lens%2C%20we%20discard%20the%20usual%20theoretical%20proxy%20which%20decouples%20gradients%20and%20preconditioners%20using%20two%20independent%20mini-batches%2C%20and%20we%20replace%20it%20with%20a%20new%20proxy%20based%20on%20squared%20volume%20sampling.%20Under%20this%20new%20proxy%20we%20show%20that%20the%20expectation%20of%20the%20SNG%20direction%20becomes%20equal%20to%20a%20preconditioned%20gradient%20descent%20step%20even%20in%20the%20presence%20of%20coupling%2C%20leading%20to%20%28i%29%20global%20convergence%20guarantees%20when%20using%20a%20single%20mini-batch%20of%20any%20size%2C%20and%20%28ii%29%20an%20explicit%20characterization%20of%20the%20convergence%20rate%20in%20terms%20of%20quantities%20related%20to%20the%20sketch-and-project%20structure.%20These%20findings%20in%20turn%20yield%20new%20insights%20into%20small-sample%20settings%2C%20for%20example%20by%20suggesting%20that%20the%20advantage%20of%20SNG%20over%20SGD%20is%20that%20it%20can%20more%20effectively%20exploit%20spectral%20decay%20in%20the%20model%20Jacobian.%20We%20also%20extend%20these%20ideas%20to%20explain%20a%20popular%20structured%20momentum%20scheme%20for%20SNG%2C%20known%20as%20SPRING%2C%20by%20showing%20that%20it%20arises%20naturally%20from%20accelerated%20sketch-and-project%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2508.21022v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Sketch-and-Project%2520Analysis%2520of%2520Subsampled%2520Natural%2520Gradient%2520Algorithms%26entry.906535625%3DGil%2520Goldshlager%2520and%2520Jiang%2520Hu%2520and%2520Lin%2520Lin%26entry.1292438233%3DSubsampled%2520natural%2520gradient%2520descent%2520%2528SNG%2529%2520has%2520been%2520used%2520to%2520enable%2520high-precision%2520scientific%2520machine%2520learning%252C%2520but%2520standard%2520analyses%2520based%2520on%2520stochastic%2520preconditioning%2520fail%2520to%2520provide%2520insight%2520into%2520realistic%2520small-sample%2520settings.%2520We%2520overcome%2520this%2520limitation%2520by%2520instead%2520analyzing%2520SNG%2520as%2520a%2520sketch-and-project%2520method.%2520Motivated%2520by%2520this%2520lens%252C%2520we%2520discard%2520the%2520usual%2520theoretical%2520proxy%2520which%2520decouples%2520gradients%2520and%2520preconditioners%2520using%2520two%2520independent%2520mini-batches%252C%2520and%2520we%2520replace%2520it%2520with%2520a%2520new%2520proxy%2520based%2520on%2520squared%2520volume%2520sampling.%2520Under%2520this%2520new%2520proxy%2520we%2520show%2520that%2520the%2520expectation%2520of%2520the%2520SNG%2520direction%2520becomes%2520equal%2520to%2520a%2520preconditioned%2520gradient%2520descent%2520step%2520even%2520in%2520the%2520presence%2520of%2520coupling%252C%2520leading%2520to%2520%2528i%2529%2520global%2520convergence%2520guarantees%2520when%2520using%2520a%2520single%2520mini-batch%2520of%2520any%2520size%252C%2520and%2520%2528ii%2529%2520an%2520explicit%2520characterization%2520of%2520the%2520convergence%2520rate%2520in%2520terms%2520of%2520quantities%2520related%2520to%2520the%2520sketch-and-project%2520structure.%2520These%2520findings%2520in%2520turn%2520yield%2520new%2520insights%2520into%2520small-sample%2520settings%252C%2520for%2520example%2520by%2520suggesting%2520that%2520the%2520advantage%2520of%2520SNG%2520over%2520SGD%2520is%2520that%2520it%2520can%2520more%2520effectively%2520exploit%2520spectral%2520decay%2520in%2520the%2520model%2520Jacobian.%2520We%2520also%2520extend%2520these%2520ideas%2520to%2520explain%2520a%2520popular%2520structured%2520momentum%2520scheme%2520for%2520SNG%252C%2520known%2520as%2520SPRING%252C%2520by%2520showing%2520that%2520it%2520arises%2520naturally%2520from%2520accelerated%2520sketch-and-project%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21022v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Sketch-and-Project%20Analysis%20of%20Subsampled%20Natural%20Gradient%20Algorithms&entry.906535625=Gil%20Goldshlager%20and%20Jiang%20Hu%20and%20Lin%20Lin&entry.1292438233=Subsampled%20natural%20gradient%20descent%20%28SNG%29%20has%20been%20used%20to%20enable%20high-precision%20scientific%20machine%20learning%2C%20but%20standard%20analyses%20based%20on%20stochastic%20preconditioning%20fail%20to%20provide%20insight%20into%20realistic%20small-sample%20settings.%20We%20overcome%20this%20limitation%20by%20instead%20analyzing%20SNG%20as%20a%20sketch-and-project%20method.%20Motivated%20by%20this%20lens%2C%20we%20discard%20the%20usual%20theoretical%20proxy%20which%20decouples%20gradients%20and%20preconditioners%20using%20two%20independent%20mini-batches%2C%20and%20we%20replace%20it%20with%20a%20new%20proxy%20based%20on%20squared%20volume%20sampling.%20Under%20this%20new%20proxy%20we%20show%20that%20the%20expectation%20of%20the%20SNG%20direction%20becomes%20equal%20to%20a%20preconditioned%20gradient%20descent%20step%20even%20in%20the%20presence%20of%20coupling%2C%20leading%20to%20%28i%29%20global%20convergence%20guarantees%20when%20using%20a%20single%20mini-batch%20of%20any%20size%2C%20and%20%28ii%29%20an%20explicit%20characterization%20of%20the%20convergence%20rate%20in%20terms%20of%20quantities%20related%20to%20the%20sketch-and-project%20structure.%20These%20findings%20in%20turn%20yield%20new%20insights%20into%20small-sample%20settings%2C%20for%20example%20by%20suggesting%20that%20the%20advantage%20of%20SNG%20over%20SGD%20is%20that%20it%20can%20more%20effectively%20exploit%20spectral%20decay%20in%20the%20model%20Jacobian.%20We%20also%20extend%20these%20ideas%20to%20explain%20a%20popular%20structured%20momentum%20scheme%20for%20SNG%2C%20known%20as%20SPRING%2C%20by%20showing%20that%20it%20arises%20naturally%20from%20accelerated%20sketch-and-project%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2508.21022v2&entry.124074799=Read"},
{"title": "TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?", "author": "Zhongyuan Bao and Lejun Zhang", "abstract": "Multimodal large language models (MLLMs) excel at general video understanding but struggle with fast, high-frequency sports like tennis, where rally clips are short yet information-dense. To systematically evaluate MLLMs in this challenging domain, we present TennisTV, the first and most comprehensive benchmark for tennis video understanding. TennisTV models each rally as a temporal-ordered sequence of consecutive stroke events, using automated pipelines for filtering and question generation. It covers 8 tasks from the stroke level to the rally level and includes 2527 human-verified questions. Evaluating 17 representative MLLMs, we provide the first systematic assessment of tennis video understanding. Results yield two key insights: (i) frame-sampling density should be tailored and balanced across tasks, and (ii) improving temporal grounding is essential for stronger reasoning.", "link": "http://arxiv.org/abs/2509.15602v4", "date": "2026-02-05", "relevancy": 2.4471, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4981}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4981}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TennisTV%3A%20Do%20Multimodal%20Large%20Language%20Models%20Understand%20Tennis%20Rallies%3F&body=Title%3A%20TennisTV%3A%20Do%20Multimodal%20Large%20Language%20Models%20Understand%20Tennis%20Rallies%3F%0AAuthor%3A%20Zhongyuan%20Bao%20and%20Lejun%20Zhang%0AAbstract%3A%20Multimodal%20large%20language%20models%20%28MLLMs%29%20excel%20at%20general%20video%20understanding%20but%20struggle%20with%20fast%2C%20high-frequency%20sports%20like%20tennis%2C%20where%20rally%20clips%20are%20short%20yet%20information-dense.%20To%20systematically%20evaluate%20MLLMs%20in%20this%20challenging%20domain%2C%20we%20present%20TennisTV%2C%20the%20first%20and%20most%20comprehensive%20benchmark%20for%20tennis%20video%20understanding.%20TennisTV%20models%20each%20rally%20as%20a%20temporal-ordered%20sequence%20of%20consecutive%20stroke%20events%2C%20using%20automated%20pipelines%20for%20filtering%20and%20question%20generation.%20It%20covers%208%20tasks%20from%20the%20stroke%20level%20to%20the%20rally%20level%20and%20includes%202527%20human-verified%20questions.%20Evaluating%2017%20representative%20MLLMs%2C%20we%20provide%20the%20first%20systematic%20assessment%20of%20tennis%20video%20understanding.%20Results%20yield%20two%20key%20insights%3A%20%28i%29%20frame-sampling%20density%20should%20be%20tailored%20and%20balanced%20across%20tasks%2C%20and%20%28ii%29%20improving%20temporal%20grounding%20is%20essential%20for%20stronger%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2509.15602v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTennisTV%253A%2520Do%2520Multimodal%2520Large%2520Language%2520Models%2520Understand%2520Tennis%2520Rallies%253F%26entry.906535625%3DZhongyuan%2520Bao%2520and%2520Lejun%2520Zhang%26entry.1292438233%3DMultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520excel%2520at%2520general%2520video%2520understanding%2520but%2520struggle%2520with%2520fast%252C%2520high-frequency%2520sports%2520like%2520tennis%252C%2520where%2520rally%2520clips%2520are%2520short%2520yet%2520information-dense.%2520To%2520systematically%2520evaluate%2520MLLMs%2520in%2520this%2520challenging%2520domain%252C%2520we%2520present%2520TennisTV%252C%2520the%2520first%2520and%2520most%2520comprehensive%2520benchmark%2520for%2520tennis%2520video%2520understanding.%2520TennisTV%2520models%2520each%2520rally%2520as%2520a%2520temporal-ordered%2520sequence%2520of%2520consecutive%2520stroke%2520events%252C%2520using%2520automated%2520pipelines%2520for%2520filtering%2520and%2520question%2520generation.%2520It%2520covers%25208%2520tasks%2520from%2520the%2520stroke%2520level%2520to%2520the%2520rally%2520level%2520and%2520includes%25202527%2520human-verified%2520questions.%2520Evaluating%252017%2520representative%2520MLLMs%252C%2520we%2520provide%2520the%2520first%2520systematic%2520assessment%2520of%2520tennis%2520video%2520understanding.%2520Results%2520yield%2520two%2520key%2520insights%253A%2520%2528i%2529%2520frame-sampling%2520density%2520should%2520be%2520tailored%2520and%2520balanced%2520across%2520tasks%252C%2520and%2520%2528ii%2529%2520improving%2520temporal%2520grounding%2520is%2520essential%2520for%2520stronger%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15602v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TennisTV%3A%20Do%20Multimodal%20Large%20Language%20Models%20Understand%20Tennis%20Rallies%3F&entry.906535625=Zhongyuan%20Bao%20and%20Lejun%20Zhang&entry.1292438233=Multimodal%20large%20language%20models%20%28MLLMs%29%20excel%20at%20general%20video%20understanding%20but%20struggle%20with%20fast%2C%20high-frequency%20sports%20like%20tennis%2C%20where%20rally%20clips%20are%20short%20yet%20information-dense.%20To%20systematically%20evaluate%20MLLMs%20in%20this%20challenging%20domain%2C%20we%20present%20TennisTV%2C%20the%20first%20and%20most%20comprehensive%20benchmark%20for%20tennis%20video%20understanding.%20TennisTV%20models%20each%20rally%20as%20a%20temporal-ordered%20sequence%20of%20consecutive%20stroke%20events%2C%20using%20automated%20pipelines%20for%20filtering%20and%20question%20generation.%20It%20covers%208%20tasks%20from%20the%20stroke%20level%20to%20the%20rally%20level%20and%20includes%202527%20human-verified%20questions.%20Evaluating%2017%20representative%20MLLMs%2C%20we%20provide%20the%20first%20systematic%20assessment%20of%20tennis%20video%20understanding.%20Results%20yield%20two%20key%20insights%3A%20%28i%29%20frame-sampling%20density%20should%20be%20tailored%20and%20balanced%20across%20tasks%2C%20and%20%28ii%29%20improving%20temporal%20grounding%20is%20essential%20for%20stronger%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2509.15602v4&entry.124074799=Read"},
{"title": "Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities", "author": "Florian Dietz and William Wale and Oscar Gilg and Robert McCarthy and Felix Michalak and Gustavo Ewbank Rodrigues Danon and Miguelito de Guzman and Dietrich Klakow", "abstract": "Detecting misalignment in large language models is challenging because models may learn to conceal misbehavior during training. Standard auditing techniques fall short: black-box methods often cannot distinguish misaligned outputs from benign ones, and mechanistic interpretability does not scale with model capabilities. We introduce Split Personality Training (SPT), which fine-tunes a second ``honest persona'' into LoRA parameters that remain inactive during normal operation. After the main model responds, we activate the LoRA adapter and insert a trigger string, enabling the honest persona to review the response while accessing the main model's latent states. We test our method on the Anthropic Auditing Game Model Organism, a benchmark where Llama-3.3-70B is trained to exploit reward hacks while concealing this behavior. SPT achieves 96% overall accuracy, whereas Anthropic reports near 0% accuracy. The honest persona reveals latent knowledge inaccessible to external observers, such as the fictional biases the compromised model was trained on.", "link": "http://arxiv.org/abs/2602.05532v1", "date": "2026-02-05", "relevancy": 2.4353, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4913}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.488}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4818}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Split%20Personality%20Training%3A%20Revealing%20Latent%20Knowledge%20Through%20Alternate%20Personalities&body=Title%3A%20Split%20Personality%20Training%3A%20Revealing%20Latent%20Knowledge%20Through%20Alternate%20Personalities%0AAuthor%3A%20Florian%20Dietz%20and%20William%20Wale%20and%20Oscar%20Gilg%20and%20Robert%20McCarthy%20and%20Felix%20Michalak%20and%20Gustavo%20Ewbank%20Rodrigues%20Danon%20and%20Miguelito%20de%20Guzman%20and%20Dietrich%20Klakow%0AAbstract%3A%20Detecting%20misalignment%20in%20large%20language%20models%20is%20challenging%20because%20models%20may%20learn%20to%20conceal%20misbehavior%20during%20training.%20Standard%20auditing%20techniques%20fall%20short%3A%20black-box%20methods%20often%20cannot%20distinguish%20misaligned%20outputs%20from%20benign%20ones%2C%20and%20mechanistic%20interpretability%20does%20not%20scale%20with%20model%20capabilities.%20We%20introduce%20Split%20Personality%20Training%20%28SPT%29%2C%20which%20fine-tunes%20a%20second%20%60%60honest%20persona%27%27%20into%20LoRA%20parameters%20that%20remain%20inactive%20during%20normal%20operation.%20After%20the%20main%20model%20responds%2C%20we%20activate%20the%20LoRA%20adapter%20and%20insert%20a%20trigger%20string%2C%20enabling%20the%20honest%20persona%20to%20review%20the%20response%20while%20accessing%20the%20main%20model%27s%20latent%20states.%20We%20test%20our%20method%20on%20the%20Anthropic%20Auditing%20Game%20Model%20Organism%2C%20a%20benchmark%20where%20Llama-3.3-70B%20is%20trained%20to%20exploit%20reward%20hacks%20while%20concealing%20this%20behavior.%20SPT%20achieves%2096%25%20overall%20accuracy%2C%20whereas%20Anthropic%20reports%20near%200%25%20accuracy.%20The%20honest%20persona%20reveals%20latent%20knowledge%20inaccessible%20to%20external%20observers%2C%20such%20as%20the%20fictional%20biases%20the%20compromised%20model%20was%20trained%20on.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplit%2520Personality%2520Training%253A%2520Revealing%2520Latent%2520Knowledge%2520Through%2520Alternate%2520Personalities%26entry.906535625%3DFlorian%2520Dietz%2520and%2520William%2520Wale%2520and%2520Oscar%2520Gilg%2520and%2520Robert%2520McCarthy%2520and%2520Felix%2520Michalak%2520and%2520Gustavo%2520Ewbank%2520Rodrigues%2520Danon%2520and%2520Miguelito%2520de%2520Guzman%2520and%2520Dietrich%2520Klakow%26entry.1292438233%3DDetecting%2520misalignment%2520in%2520large%2520language%2520models%2520is%2520challenging%2520because%2520models%2520may%2520learn%2520to%2520conceal%2520misbehavior%2520during%2520training.%2520Standard%2520auditing%2520techniques%2520fall%2520short%253A%2520black-box%2520methods%2520often%2520cannot%2520distinguish%2520misaligned%2520outputs%2520from%2520benign%2520ones%252C%2520and%2520mechanistic%2520interpretability%2520does%2520not%2520scale%2520with%2520model%2520capabilities.%2520We%2520introduce%2520Split%2520Personality%2520Training%2520%2528SPT%2529%252C%2520which%2520fine-tunes%2520a%2520second%2520%2560%2560honest%2520persona%2527%2527%2520into%2520LoRA%2520parameters%2520that%2520remain%2520inactive%2520during%2520normal%2520operation.%2520After%2520the%2520main%2520model%2520responds%252C%2520we%2520activate%2520the%2520LoRA%2520adapter%2520and%2520insert%2520a%2520trigger%2520string%252C%2520enabling%2520the%2520honest%2520persona%2520to%2520review%2520the%2520response%2520while%2520accessing%2520the%2520main%2520model%2527s%2520latent%2520states.%2520We%2520test%2520our%2520method%2520on%2520the%2520Anthropic%2520Auditing%2520Game%2520Model%2520Organism%252C%2520a%2520benchmark%2520where%2520Llama-3.3-70B%2520is%2520trained%2520to%2520exploit%2520reward%2520hacks%2520while%2520concealing%2520this%2520behavior.%2520SPT%2520achieves%252096%2525%2520overall%2520accuracy%252C%2520whereas%2520Anthropic%2520reports%2520near%25200%2525%2520accuracy.%2520The%2520honest%2520persona%2520reveals%2520latent%2520knowledge%2520inaccessible%2520to%2520external%2520observers%252C%2520such%2520as%2520the%2520fictional%2520biases%2520the%2520compromised%2520model%2520was%2520trained%2520on.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Split%20Personality%20Training%3A%20Revealing%20Latent%20Knowledge%20Through%20Alternate%20Personalities&entry.906535625=Florian%20Dietz%20and%20William%20Wale%20and%20Oscar%20Gilg%20and%20Robert%20McCarthy%20and%20Felix%20Michalak%20and%20Gustavo%20Ewbank%20Rodrigues%20Danon%20and%20Miguelito%20de%20Guzman%20and%20Dietrich%20Klakow&entry.1292438233=Detecting%20misalignment%20in%20large%20language%20models%20is%20challenging%20because%20models%20may%20learn%20to%20conceal%20misbehavior%20during%20training.%20Standard%20auditing%20techniques%20fall%20short%3A%20black-box%20methods%20often%20cannot%20distinguish%20misaligned%20outputs%20from%20benign%20ones%2C%20and%20mechanistic%20interpretability%20does%20not%20scale%20with%20model%20capabilities.%20We%20introduce%20Split%20Personality%20Training%20%28SPT%29%2C%20which%20fine-tunes%20a%20second%20%60%60honest%20persona%27%27%20into%20LoRA%20parameters%20that%20remain%20inactive%20during%20normal%20operation.%20After%20the%20main%20model%20responds%2C%20we%20activate%20the%20LoRA%20adapter%20and%20insert%20a%20trigger%20string%2C%20enabling%20the%20honest%20persona%20to%20review%20the%20response%20while%20accessing%20the%20main%20model%27s%20latent%20states.%20We%20test%20our%20method%20on%20the%20Anthropic%20Auditing%20Game%20Model%20Organism%2C%20a%20benchmark%20where%20Llama-3.3-70B%20is%20trained%20to%20exploit%20reward%20hacks%20while%20concealing%20this%20behavior.%20SPT%20achieves%2096%25%20overall%20accuracy%2C%20whereas%20Anthropic%20reports%20near%200%25%20accuracy.%20The%20honest%20persona%20reveals%20latent%20knowledge%20inaccessible%20to%20external%20observers%2C%20such%20as%20the%20fictional%20biases%20the%20compromised%20model%20was%20trained%20on.&entry.1838667208=http%3A//arxiv.org/abs/2602.05532v1&entry.124074799=Read"},
{"title": "Reasoning-guided Collaborative Filtering with Language Models for Explainable Recommendation", "author": "Fahad Anwaar and Adil Mehmood Khan and Muhammad Khalid and Usman Zia and Kezhi Wang", "abstract": "Large Language Models (LLMs) exhibit potential for explainable recommendation systems but overlook collaborative signals, while prevailing methods treat recommendation and explanation as separate tasks, resulting in a memory footprint. We present RGCF-XRec, a hybrid framework that introduces reasoning-guided collaborative filtering (CF) knowledge into a language model to deliver explainable sequential recommendations in a single step. Theoretical grounding and empirical findings reveal that RGCF-XRec offers three key merits over leading CF-aware LLM-based methods: (1) reasoning-guided augmentation of CF knowledge through contextual prompting to discover latent preferences and interpretable reasoning paths; (2) an efficient scoring mechanism based on four dimensions: coherence, completeness, relevance, and consistency to mitigate noisy CF reasoning traces and retain high-quality explanations; (3) a unified representation learning network that encodes collaborative and semantic signals, enabling a structured prompt to condition the LLM for explainable sequential recommendation. RGCF-XRec demonstrates consistent improvements across Amazon datasets, Sports, Toys, and Beauty, comprising 642,503 user-item interactions. It improves HR@10 by 7.38\\% in Sports and 4.59\\% in Toys, along with ROUGE-L by 8.02\\% and 3.49\\%, respectively. It reduces the cold warm performance gap, achieving overall gains of 14.5\\% in cold-start and 11.9\\% in warm start scenarios, and enhances zero-shot HR@5 by 18.54\\% in Beauty and 23.16\\% in Toys, highlighting effective generalization and robustness. Moreover, RGCF-XRec achieves training efficiency with a lightweight LLaMA 3.2-3B backbone, ensuring scalability for real-world applications.", "link": "http://arxiv.org/abs/2602.05544v1", "date": "2026-02-05", "relevancy": 2.4328, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4874}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4861}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning-guided%20Collaborative%20Filtering%20with%20Language%20Models%20for%20Explainable%20Recommendation&body=Title%3A%20Reasoning-guided%20Collaborative%20Filtering%20with%20Language%20Models%20for%20Explainable%20Recommendation%0AAuthor%3A%20Fahad%20Anwaar%20and%20Adil%20Mehmood%20Khan%20and%20Muhammad%20Khalid%20and%20Usman%20Zia%20and%20Kezhi%20Wang%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20potential%20for%20explainable%20recommendation%20systems%20but%20overlook%20collaborative%20signals%2C%20while%20prevailing%20methods%20treat%20recommendation%20and%20explanation%20as%20separate%20tasks%2C%20resulting%20in%20a%20memory%20footprint.%20We%20present%20RGCF-XRec%2C%20a%20hybrid%20framework%20that%20introduces%20reasoning-guided%20collaborative%20filtering%20%28CF%29%20knowledge%20into%20a%20language%20model%20to%20deliver%20explainable%20sequential%20recommendations%20in%20a%20single%20step.%20Theoretical%20grounding%20and%20empirical%20findings%20reveal%20that%20RGCF-XRec%20offers%20three%20key%20merits%20over%20leading%20CF-aware%20LLM-based%20methods%3A%20%281%29%20reasoning-guided%20augmentation%20of%20CF%20knowledge%20through%20contextual%20prompting%20to%20discover%20latent%20preferences%20and%20interpretable%20reasoning%20paths%3B%20%282%29%20an%20efficient%20scoring%20mechanism%20based%20on%20four%20dimensions%3A%20coherence%2C%20completeness%2C%20relevance%2C%20and%20consistency%20to%20mitigate%20noisy%20CF%20reasoning%20traces%20and%20retain%20high-quality%20explanations%3B%20%283%29%20a%20unified%20representation%20learning%20network%20that%20encodes%20collaborative%20and%20semantic%20signals%2C%20enabling%20a%20structured%20prompt%20to%20condition%20the%20LLM%20for%20explainable%20sequential%20recommendation.%20RGCF-XRec%20demonstrates%20consistent%20improvements%20across%20Amazon%20datasets%2C%20Sports%2C%20Toys%2C%20and%20Beauty%2C%20comprising%20642%2C503%20user-item%20interactions.%20It%20improves%20HR%4010%20by%207.38%5C%25%20in%20Sports%20and%204.59%5C%25%20in%20Toys%2C%20along%20with%20ROUGE-L%20by%208.02%5C%25%20and%203.49%5C%25%2C%20respectively.%20It%20reduces%20the%20cold%20warm%20performance%20gap%2C%20achieving%20overall%20gains%20of%2014.5%5C%25%20in%20cold-start%20and%2011.9%5C%25%20in%20warm%20start%20scenarios%2C%20and%20enhances%20zero-shot%20HR%405%20by%2018.54%5C%25%20in%20Beauty%20and%2023.16%5C%25%20in%20Toys%2C%20highlighting%20effective%20generalization%20and%20robustness.%20Moreover%2C%20RGCF-XRec%20achieves%20training%20efficiency%20with%20a%20lightweight%20LLaMA%203.2-3B%20backbone%2C%20ensuring%20scalability%20for%20real-world%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning-guided%2520Collaborative%2520Filtering%2520with%2520Language%2520Models%2520for%2520Explainable%2520Recommendation%26entry.906535625%3DFahad%2520Anwaar%2520and%2520Adil%2520Mehmood%2520Khan%2520and%2520Muhammad%2520Khalid%2520and%2520Usman%2520Zia%2520and%2520Kezhi%2520Wang%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520exhibit%2520potential%2520for%2520explainable%2520recommendation%2520systems%2520but%2520overlook%2520collaborative%2520signals%252C%2520while%2520prevailing%2520methods%2520treat%2520recommendation%2520and%2520explanation%2520as%2520separate%2520tasks%252C%2520resulting%2520in%2520a%2520memory%2520footprint.%2520We%2520present%2520RGCF-XRec%252C%2520a%2520hybrid%2520framework%2520that%2520introduces%2520reasoning-guided%2520collaborative%2520filtering%2520%2528CF%2529%2520knowledge%2520into%2520a%2520language%2520model%2520to%2520deliver%2520explainable%2520sequential%2520recommendations%2520in%2520a%2520single%2520step.%2520Theoretical%2520grounding%2520and%2520empirical%2520findings%2520reveal%2520that%2520RGCF-XRec%2520offers%2520three%2520key%2520merits%2520over%2520leading%2520CF-aware%2520LLM-based%2520methods%253A%2520%25281%2529%2520reasoning-guided%2520augmentation%2520of%2520CF%2520knowledge%2520through%2520contextual%2520prompting%2520to%2520discover%2520latent%2520preferences%2520and%2520interpretable%2520reasoning%2520paths%253B%2520%25282%2529%2520an%2520efficient%2520scoring%2520mechanism%2520based%2520on%2520four%2520dimensions%253A%2520coherence%252C%2520completeness%252C%2520relevance%252C%2520and%2520consistency%2520to%2520mitigate%2520noisy%2520CF%2520reasoning%2520traces%2520and%2520retain%2520high-quality%2520explanations%253B%2520%25283%2529%2520a%2520unified%2520representation%2520learning%2520network%2520that%2520encodes%2520collaborative%2520and%2520semantic%2520signals%252C%2520enabling%2520a%2520structured%2520prompt%2520to%2520condition%2520the%2520LLM%2520for%2520explainable%2520sequential%2520recommendation.%2520RGCF-XRec%2520demonstrates%2520consistent%2520improvements%2520across%2520Amazon%2520datasets%252C%2520Sports%252C%2520Toys%252C%2520and%2520Beauty%252C%2520comprising%2520642%252C503%2520user-item%2520interactions.%2520It%2520improves%2520HR%254010%2520by%25207.38%255C%2525%2520in%2520Sports%2520and%25204.59%255C%2525%2520in%2520Toys%252C%2520along%2520with%2520ROUGE-L%2520by%25208.02%255C%2525%2520and%25203.49%255C%2525%252C%2520respectively.%2520It%2520reduces%2520the%2520cold%2520warm%2520performance%2520gap%252C%2520achieving%2520overall%2520gains%2520of%252014.5%255C%2525%2520in%2520cold-start%2520and%252011.9%255C%2525%2520in%2520warm%2520start%2520scenarios%252C%2520and%2520enhances%2520zero-shot%2520HR%25405%2520by%252018.54%255C%2525%2520in%2520Beauty%2520and%252023.16%255C%2525%2520in%2520Toys%252C%2520highlighting%2520effective%2520generalization%2520and%2520robustness.%2520Moreover%252C%2520RGCF-XRec%2520achieves%2520training%2520efficiency%2520with%2520a%2520lightweight%2520LLaMA%25203.2-3B%2520backbone%252C%2520ensuring%2520scalability%2520for%2520real-world%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning-guided%20Collaborative%20Filtering%20with%20Language%20Models%20for%20Explainable%20Recommendation&entry.906535625=Fahad%20Anwaar%20and%20Adil%20Mehmood%20Khan%20and%20Muhammad%20Khalid%20and%20Usman%20Zia%20and%20Kezhi%20Wang&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20exhibit%20potential%20for%20explainable%20recommendation%20systems%20but%20overlook%20collaborative%20signals%2C%20while%20prevailing%20methods%20treat%20recommendation%20and%20explanation%20as%20separate%20tasks%2C%20resulting%20in%20a%20memory%20footprint.%20We%20present%20RGCF-XRec%2C%20a%20hybrid%20framework%20that%20introduces%20reasoning-guided%20collaborative%20filtering%20%28CF%29%20knowledge%20into%20a%20language%20model%20to%20deliver%20explainable%20sequential%20recommendations%20in%20a%20single%20step.%20Theoretical%20grounding%20and%20empirical%20findings%20reveal%20that%20RGCF-XRec%20offers%20three%20key%20merits%20over%20leading%20CF-aware%20LLM-based%20methods%3A%20%281%29%20reasoning-guided%20augmentation%20of%20CF%20knowledge%20through%20contextual%20prompting%20to%20discover%20latent%20preferences%20and%20interpretable%20reasoning%20paths%3B%20%282%29%20an%20efficient%20scoring%20mechanism%20based%20on%20four%20dimensions%3A%20coherence%2C%20completeness%2C%20relevance%2C%20and%20consistency%20to%20mitigate%20noisy%20CF%20reasoning%20traces%20and%20retain%20high-quality%20explanations%3B%20%283%29%20a%20unified%20representation%20learning%20network%20that%20encodes%20collaborative%20and%20semantic%20signals%2C%20enabling%20a%20structured%20prompt%20to%20condition%20the%20LLM%20for%20explainable%20sequential%20recommendation.%20RGCF-XRec%20demonstrates%20consistent%20improvements%20across%20Amazon%20datasets%2C%20Sports%2C%20Toys%2C%20and%20Beauty%2C%20comprising%20642%2C503%20user-item%20interactions.%20It%20improves%20HR%4010%20by%207.38%5C%25%20in%20Sports%20and%204.59%5C%25%20in%20Toys%2C%20along%20with%20ROUGE-L%20by%208.02%5C%25%20and%203.49%5C%25%2C%20respectively.%20It%20reduces%20the%20cold%20warm%20performance%20gap%2C%20achieving%20overall%20gains%20of%2014.5%5C%25%20in%20cold-start%20and%2011.9%5C%25%20in%20warm%20start%20scenarios%2C%20and%20enhances%20zero-shot%20HR%405%20by%2018.54%5C%25%20in%20Beauty%20and%2023.16%5C%25%20in%20Toys%2C%20highlighting%20effective%20generalization%20and%20robustness.%20Moreover%2C%20RGCF-XRec%20achieves%20training%20efficiency%20with%20a%20lightweight%20LLaMA%203.2-3B%20backbone%2C%20ensuring%20scalability%20for%20real-world%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2602.05544v1&entry.124074799=Read"},
{"title": "Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning", "author": "Xuejun Zhang and Aditi Tiwari and Zhenhailong Wang and Heng Ji", "abstract": "Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20\u00b0 and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.", "link": "http://arxiv.org/abs/2602.06041v1", "date": "2026-02-05", "relevancy": 2.431, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6226}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6059}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Camera%20Pose%20from%20Perspective%20Descriptions%20for%20Spatial%20Reasoning&body=Title%3A%20Predicting%20Camera%20Pose%20from%20Perspective%20Descriptions%20for%20Spatial%20Reasoning%0AAuthor%3A%20Xuejun%20Zhang%20and%20Aditi%20Tiwari%20and%20Zhenhailong%20Wang%20and%20Heng%20Ji%0AAbstract%3A%20Multi-image%20spatial%20reasoning%20remains%20challenging%20for%20current%20multimodal%20large%20language%20models%20%28MLLMs%29.%20While%20single-view%20perception%20is%20inherently%202D%2C%20reasoning%20over%20multiple%20views%20requires%20building%20a%20coherent%20scene%20understanding%20across%20viewpoints.%20In%20particular%2C%20we%20study%20perspective%20taking%2C%20where%20a%20model%20must%20build%20a%20coherent%203D%20understanding%20from%20multi-view%20observations%20and%20use%20it%20to%20reason%20from%20a%20new%2C%20language-specified%20viewpoint.%20We%20introduce%20CAMCUE%2C%20a%20pose-aware%20multi-image%20framework%20that%20uses%20camera%20pose%20as%20an%20explicit%20geometric%20anchor%20for%20cross-view%20fusion%20and%20novel-view%20reasoning.%20CAMCUE%20injects%20per-view%20pose%20into%20visual%20tokens%2C%20grounds%20natural-language%20viewpoint%20descriptions%20to%20a%20target%20camera%20pose%2C%20and%20synthesizes%20a%20pose-conditioned%20imagined%20target%20view%20to%20support%20answering.%20To%20support%20this%20setting%2C%20we%20curate%20CAMCUE-DATA%20with%2027%2C668%20training%20and%20508%20test%20instances%20pairing%20multi-view%20images%20and%20poses%20with%20diverse%20target-viewpoint%20descriptions%20and%20perspective-shift%20questions.%20We%20also%20include%20human-annotated%20viewpoint%20descriptions%20in%20the%20test%20split%20to%20evaluate%20generalization%20to%20human%20language.%20CAMCUE%20improves%20overall%20accuracy%20by%209.06%25%20and%20predicts%20target%20poses%20from%20natural-language%20viewpoint%20descriptions%20with%20over%2090%25%20rotation%20accuracy%20within%2020%C2%B0%20and%20translation%20accuracy%20within%20a%200.5%20error%20threshold.%20This%20direct%20grounding%20avoids%20expensive%20test-time%20search-and-match%2C%20reducing%20inference%20time%20from%20256.6s%20to%201.45s%20per%20example%20and%20enabling%20fast%2C%20interactive%20use%20in%20real-world%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2602.06041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Camera%2520Pose%2520from%2520Perspective%2520Descriptions%2520for%2520Spatial%2520Reasoning%26entry.906535625%3DXuejun%2520Zhang%2520and%2520Aditi%2520Tiwari%2520and%2520Zhenhailong%2520Wang%2520and%2520Heng%2520Ji%26entry.1292438233%3DMulti-image%2520spatial%2520reasoning%2520remains%2520challenging%2520for%2520current%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520While%2520single-view%2520perception%2520is%2520inherently%25202D%252C%2520reasoning%2520over%2520multiple%2520views%2520requires%2520building%2520a%2520coherent%2520scene%2520understanding%2520across%2520viewpoints.%2520In%2520particular%252C%2520we%2520study%2520perspective%2520taking%252C%2520where%2520a%2520model%2520must%2520build%2520a%2520coherent%25203D%2520understanding%2520from%2520multi-view%2520observations%2520and%2520use%2520it%2520to%2520reason%2520from%2520a%2520new%252C%2520language-specified%2520viewpoint.%2520We%2520introduce%2520CAMCUE%252C%2520a%2520pose-aware%2520multi-image%2520framework%2520that%2520uses%2520camera%2520pose%2520as%2520an%2520explicit%2520geometric%2520anchor%2520for%2520cross-view%2520fusion%2520and%2520novel-view%2520reasoning.%2520CAMCUE%2520injects%2520per-view%2520pose%2520into%2520visual%2520tokens%252C%2520grounds%2520natural-language%2520viewpoint%2520descriptions%2520to%2520a%2520target%2520camera%2520pose%252C%2520and%2520synthesizes%2520a%2520pose-conditioned%2520imagined%2520target%2520view%2520to%2520support%2520answering.%2520To%2520support%2520this%2520setting%252C%2520we%2520curate%2520CAMCUE-DATA%2520with%252027%252C668%2520training%2520and%2520508%2520test%2520instances%2520pairing%2520multi-view%2520images%2520and%2520poses%2520with%2520diverse%2520target-viewpoint%2520descriptions%2520and%2520perspective-shift%2520questions.%2520We%2520also%2520include%2520human-annotated%2520viewpoint%2520descriptions%2520in%2520the%2520test%2520split%2520to%2520evaluate%2520generalization%2520to%2520human%2520language.%2520CAMCUE%2520improves%2520overall%2520accuracy%2520by%25209.06%2525%2520and%2520predicts%2520target%2520poses%2520from%2520natural-language%2520viewpoint%2520descriptions%2520with%2520over%252090%2525%2520rotation%2520accuracy%2520within%252020%25C2%25B0%2520and%2520translation%2520accuracy%2520within%2520a%25200.5%2520error%2520threshold.%2520This%2520direct%2520grounding%2520avoids%2520expensive%2520test-time%2520search-and-match%252C%2520reducing%2520inference%2520time%2520from%2520256.6s%2520to%25201.45s%2520per%2520example%2520and%2520enabling%2520fast%252C%2520interactive%2520use%2520in%2520real-world%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.06041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Camera%20Pose%20from%20Perspective%20Descriptions%20for%20Spatial%20Reasoning&entry.906535625=Xuejun%20Zhang%20and%20Aditi%20Tiwari%20and%20Zhenhailong%20Wang%20and%20Heng%20Ji&entry.1292438233=Multi-image%20spatial%20reasoning%20remains%20challenging%20for%20current%20multimodal%20large%20language%20models%20%28MLLMs%29.%20While%20single-view%20perception%20is%20inherently%202D%2C%20reasoning%20over%20multiple%20views%20requires%20building%20a%20coherent%20scene%20understanding%20across%20viewpoints.%20In%20particular%2C%20we%20study%20perspective%20taking%2C%20where%20a%20model%20must%20build%20a%20coherent%203D%20understanding%20from%20multi-view%20observations%20and%20use%20it%20to%20reason%20from%20a%20new%2C%20language-specified%20viewpoint.%20We%20introduce%20CAMCUE%2C%20a%20pose-aware%20multi-image%20framework%20that%20uses%20camera%20pose%20as%20an%20explicit%20geometric%20anchor%20for%20cross-view%20fusion%20and%20novel-view%20reasoning.%20CAMCUE%20injects%20per-view%20pose%20into%20visual%20tokens%2C%20grounds%20natural-language%20viewpoint%20descriptions%20to%20a%20target%20camera%20pose%2C%20and%20synthesizes%20a%20pose-conditioned%20imagined%20target%20view%20to%20support%20answering.%20To%20support%20this%20setting%2C%20we%20curate%20CAMCUE-DATA%20with%2027%2C668%20training%20and%20508%20test%20instances%20pairing%20multi-view%20images%20and%20poses%20with%20diverse%20target-viewpoint%20descriptions%20and%20perspective-shift%20questions.%20We%20also%20include%20human-annotated%20viewpoint%20descriptions%20in%20the%20test%20split%20to%20evaluate%20generalization%20to%20human%20language.%20CAMCUE%20improves%20overall%20accuracy%20by%209.06%25%20and%20predicts%20target%20poses%20from%20natural-language%20viewpoint%20descriptions%20with%20over%2090%25%20rotation%20accuracy%20within%2020%C2%B0%20and%20translation%20accuracy%20within%20a%200.5%20error%20threshold.%20This%20direct%20grounding%20avoids%20expensive%20test-time%20search-and-match%2C%20reducing%20inference%20time%20from%20256.6s%20to%201.45s%20per%20example%20and%20enabling%20fast%2C%20interactive%20use%20in%20real-world%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2602.06041v1&entry.124074799=Read"},
{"title": "CSRv2: Unlocking Ultra-Sparse Embeddings", "author": "Lixuan Guo and Yifei Wang and Tiansheng Wen and Yifan Wang and Aosong Feng and Bo Chen and Stefanie Jegelka and Chenyu You", "abstract": "In the era of large foundation models, the quality of embeddings has become a central determinant of downstream task performance and overall system capability. Yet widely used dense embeddings are often extremely high-dimensional, incurring substantial costs in storage, memory, and inference latency. To address these, Contrastive Sparse Representation (CSR) is recently proposed as a promising direction, mapping dense embeddings into high-dimensional but k-sparse vectors, in contrast to compact dense embeddings such as Matryoshka Representation Learning (MRL). Despite its promise, CSR suffers severe degradation in the ultra-sparse regime, where over 80% of neurons remain inactive, leaving much of its efficiency potential unrealized. In this paper, we introduce CSRv2, a principled training approach designed to make ultra-sparse embeddings viable. CSRv2 stabilizes sparsity learning through progressive k-annealing, enhances representational quality via supervised contrastive objectives, and ensures end-to-end adaptability with full backbone finetuning. CSRv2 reduces dead neurons from 80% to 20% and delivers a 14% accuracy gain at k=2, bringing ultra-sparse embeddings on par with CSR at k=8 and MRL at 32 dimensions, all with only two active features. While maintaining comparable performance, CSRv2 delivers a 7x speedup over MRL, and yields up to 300x improvements in compute and memory efficiency relative to dense embeddings in text representation. Extensive experiments across text and vision demonstrate that CSRv2 makes ultra-sparse embeddings practical without compromising performance, where CSRv2 achieves 7%/4% improvement over CSR when k=4 and further increases this gap to 14%/6% when k=2 in text/vision representation. By making extreme sparsity viable, CSRv2 broadens the design space for real-time and edge-deployable AI systems where both embedding quality and efficiency are critical.", "link": "http://arxiv.org/abs/2602.05735v1", "date": "2026-02-05", "relevancy": 2.4294, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4903}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4837}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CSRv2%3A%20Unlocking%20Ultra-Sparse%20Embeddings&body=Title%3A%20CSRv2%3A%20Unlocking%20Ultra-Sparse%20Embeddings%0AAuthor%3A%20Lixuan%20Guo%20and%20Yifei%20Wang%20and%20Tiansheng%20Wen%20and%20Yifan%20Wang%20and%20Aosong%20Feng%20and%20Bo%20Chen%20and%20Stefanie%20Jegelka%20and%20Chenyu%20You%0AAbstract%3A%20In%20the%20era%20of%20large%20foundation%20models%2C%20the%20quality%20of%20embeddings%20has%20become%20a%20central%20determinant%20of%20downstream%20task%20performance%20and%20overall%20system%20capability.%20Yet%20widely%20used%20dense%20embeddings%20are%20often%20extremely%20high-dimensional%2C%20incurring%20substantial%20costs%20in%20storage%2C%20memory%2C%20and%20inference%20latency.%20To%20address%20these%2C%20Contrastive%20Sparse%20Representation%20%28CSR%29%20is%20recently%20proposed%20as%20a%20promising%20direction%2C%20mapping%20dense%20embeddings%20into%20high-dimensional%20but%20k-sparse%20vectors%2C%20in%20contrast%20to%20compact%20dense%20embeddings%20such%20as%20Matryoshka%20Representation%20Learning%20%28MRL%29.%20Despite%20its%20promise%2C%20CSR%20suffers%20severe%20degradation%20in%20the%20ultra-sparse%20regime%2C%20where%20over%2080%25%20of%20neurons%20remain%20inactive%2C%20leaving%20much%20of%20its%20efficiency%20potential%20unrealized.%20In%20this%20paper%2C%20we%20introduce%20CSRv2%2C%20a%20principled%20training%20approach%20designed%20to%20make%20ultra-sparse%20embeddings%20viable.%20CSRv2%20stabilizes%20sparsity%20learning%20through%20progressive%20k-annealing%2C%20enhances%20representational%20quality%20via%20supervised%20contrastive%20objectives%2C%20and%20ensures%20end-to-end%20adaptability%20with%20full%20backbone%20finetuning.%20CSRv2%20reduces%20dead%20neurons%20from%2080%25%20to%2020%25%20and%20delivers%20a%2014%25%20accuracy%20gain%20at%20k%3D2%2C%20bringing%20ultra-sparse%20embeddings%20on%20par%20with%20CSR%20at%20k%3D8%20and%20MRL%20at%2032%20dimensions%2C%20all%20with%20only%20two%20active%20features.%20While%20maintaining%20comparable%20performance%2C%20CSRv2%20delivers%20a%207x%20speedup%20over%20MRL%2C%20and%20yields%20up%20to%20300x%20improvements%20in%20compute%20and%20memory%20efficiency%20relative%20to%20dense%20embeddings%20in%20text%20representation.%20Extensive%20experiments%20across%20text%20and%20vision%20demonstrate%20that%20CSRv2%20makes%20ultra-sparse%20embeddings%20practical%20without%20compromising%20performance%2C%20where%20CSRv2%20achieves%207%25/4%25%20improvement%20over%20CSR%20when%20k%3D4%20and%20further%20increases%20this%20gap%20to%2014%25/6%25%20when%20k%3D2%20in%20text/vision%20representation.%20By%20making%20extreme%20sparsity%20viable%2C%20CSRv2%20broadens%20the%20design%20space%20for%20real-time%20and%20edge-deployable%20AI%20systems%20where%20both%20embedding%20quality%20and%20efficiency%20are%20critical.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCSRv2%253A%2520Unlocking%2520Ultra-Sparse%2520Embeddings%26entry.906535625%3DLixuan%2520Guo%2520and%2520Yifei%2520Wang%2520and%2520Tiansheng%2520Wen%2520and%2520Yifan%2520Wang%2520and%2520Aosong%2520Feng%2520and%2520Bo%2520Chen%2520and%2520Stefanie%2520Jegelka%2520and%2520Chenyu%2520You%26entry.1292438233%3DIn%2520the%2520era%2520of%2520large%2520foundation%2520models%252C%2520the%2520quality%2520of%2520embeddings%2520has%2520become%2520a%2520central%2520determinant%2520of%2520downstream%2520task%2520performance%2520and%2520overall%2520system%2520capability.%2520Yet%2520widely%2520used%2520dense%2520embeddings%2520are%2520often%2520extremely%2520high-dimensional%252C%2520incurring%2520substantial%2520costs%2520in%2520storage%252C%2520memory%252C%2520and%2520inference%2520latency.%2520To%2520address%2520these%252C%2520Contrastive%2520Sparse%2520Representation%2520%2528CSR%2529%2520is%2520recently%2520proposed%2520as%2520a%2520promising%2520direction%252C%2520mapping%2520dense%2520embeddings%2520into%2520high-dimensional%2520but%2520k-sparse%2520vectors%252C%2520in%2520contrast%2520to%2520compact%2520dense%2520embeddings%2520such%2520as%2520Matryoshka%2520Representation%2520Learning%2520%2528MRL%2529.%2520Despite%2520its%2520promise%252C%2520CSR%2520suffers%2520severe%2520degradation%2520in%2520the%2520ultra-sparse%2520regime%252C%2520where%2520over%252080%2525%2520of%2520neurons%2520remain%2520inactive%252C%2520leaving%2520much%2520of%2520its%2520efficiency%2520potential%2520unrealized.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520CSRv2%252C%2520a%2520principled%2520training%2520approach%2520designed%2520to%2520make%2520ultra-sparse%2520embeddings%2520viable.%2520CSRv2%2520stabilizes%2520sparsity%2520learning%2520through%2520progressive%2520k-annealing%252C%2520enhances%2520representational%2520quality%2520via%2520supervised%2520contrastive%2520objectives%252C%2520and%2520ensures%2520end-to-end%2520adaptability%2520with%2520full%2520backbone%2520finetuning.%2520CSRv2%2520reduces%2520dead%2520neurons%2520from%252080%2525%2520to%252020%2525%2520and%2520delivers%2520a%252014%2525%2520accuracy%2520gain%2520at%2520k%253D2%252C%2520bringing%2520ultra-sparse%2520embeddings%2520on%2520par%2520with%2520CSR%2520at%2520k%253D8%2520and%2520MRL%2520at%252032%2520dimensions%252C%2520all%2520with%2520only%2520two%2520active%2520features.%2520While%2520maintaining%2520comparable%2520performance%252C%2520CSRv2%2520delivers%2520a%25207x%2520speedup%2520over%2520MRL%252C%2520and%2520yields%2520up%2520to%2520300x%2520improvements%2520in%2520compute%2520and%2520memory%2520efficiency%2520relative%2520to%2520dense%2520embeddings%2520in%2520text%2520representation.%2520Extensive%2520experiments%2520across%2520text%2520and%2520vision%2520demonstrate%2520that%2520CSRv2%2520makes%2520ultra-sparse%2520embeddings%2520practical%2520without%2520compromising%2520performance%252C%2520where%2520CSRv2%2520achieves%25207%2525/4%2525%2520improvement%2520over%2520CSR%2520when%2520k%253D4%2520and%2520further%2520increases%2520this%2520gap%2520to%252014%2525/6%2525%2520when%2520k%253D2%2520in%2520text/vision%2520representation.%2520By%2520making%2520extreme%2520sparsity%2520viable%252C%2520CSRv2%2520broadens%2520the%2520design%2520space%2520for%2520real-time%2520and%2520edge-deployable%2520AI%2520systems%2520where%2520both%2520embedding%2520quality%2520and%2520efficiency%2520are%2520critical.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CSRv2%3A%20Unlocking%20Ultra-Sparse%20Embeddings&entry.906535625=Lixuan%20Guo%20and%20Yifei%20Wang%20and%20Tiansheng%20Wen%20and%20Yifan%20Wang%20and%20Aosong%20Feng%20and%20Bo%20Chen%20and%20Stefanie%20Jegelka%20and%20Chenyu%20You&entry.1292438233=In%20the%20era%20of%20large%20foundation%20models%2C%20the%20quality%20of%20embeddings%20has%20become%20a%20central%20determinant%20of%20downstream%20task%20performance%20and%20overall%20system%20capability.%20Yet%20widely%20used%20dense%20embeddings%20are%20often%20extremely%20high-dimensional%2C%20incurring%20substantial%20costs%20in%20storage%2C%20memory%2C%20and%20inference%20latency.%20To%20address%20these%2C%20Contrastive%20Sparse%20Representation%20%28CSR%29%20is%20recently%20proposed%20as%20a%20promising%20direction%2C%20mapping%20dense%20embeddings%20into%20high-dimensional%20but%20k-sparse%20vectors%2C%20in%20contrast%20to%20compact%20dense%20embeddings%20such%20as%20Matryoshka%20Representation%20Learning%20%28MRL%29.%20Despite%20its%20promise%2C%20CSR%20suffers%20severe%20degradation%20in%20the%20ultra-sparse%20regime%2C%20where%20over%2080%25%20of%20neurons%20remain%20inactive%2C%20leaving%20much%20of%20its%20efficiency%20potential%20unrealized.%20In%20this%20paper%2C%20we%20introduce%20CSRv2%2C%20a%20principled%20training%20approach%20designed%20to%20make%20ultra-sparse%20embeddings%20viable.%20CSRv2%20stabilizes%20sparsity%20learning%20through%20progressive%20k-annealing%2C%20enhances%20representational%20quality%20via%20supervised%20contrastive%20objectives%2C%20and%20ensures%20end-to-end%20adaptability%20with%20full%20backbone%20finetuning.%20CSRv2%20reduces%20dead%20neurons%20from%2080%25%20to%2020%25%20and%20delivers%20a%2014%25%20accuracy%20gain%20at%20k%3D2%2C%20bringing%20ultra-sparse%20embeddings%20on%20par%20with%20CSR%20at%20k%3D8%20and%20MRL%20at%2032%20dimensions%2C%20all%20with%20only%20two%20active%20features.%20While%20maintaining%20comparable%20performance%2C%20CSRv2%20delivers%20a%207x%20speedup%20over%20MRL%2C%20and%20yields%20up%20to%20300x%20improvements%20in%20compute%20and%20memory%20efficiency%20relative%20to%20dense%20embeddings%20in%20text%20representation.%20Extensive%20experiments%20across%20text%20and%20vision%20demonstrate%20that%20CSRv2%20makes%20ultra-sparse%20embeddings%20practical%20without%20compromising%20performance%2C%20where%20CSRv2%20achieves%207%25/4%25%20improvement%20over%20CSR%20when%20k%3D4%20and%20further%20increases%20this%20gap%20to%2014%25/6%25%20when%20k%3D2%20in%20text/vision%20representation.%20By%20making%20extreme%20sparsity%20viable%2C%20CSRv2%20broadens%20the%20design%20space%20for%20real-time%20and%20edge-deployable%20AI%20systems%20where%20both%20embedding%20quality%20and%20efficiency%20are%20critical.&entry.1838667208=http%3A//arxiv.org/abs/2602.05735v1&entry.124074799=Read"},
{"title": "Chunky Post-Training: Data Driven Failures of Generalization", "author": "Seoirse Murray and Allison Qi and Timothy Qian and John Schulman and Collin Burns and Sara Price", "abstract": "LLM post-training involves many diverse datasets, each targeting a specific behavior. But these datasets encode incidental patterns alongside intended ones: correlations between formatting and content, narrow phrasings across diverse problems, and implicit associations arising from the discrete data curation process. These patterns are often invisible to developers yet salient to models, producing behaviors that surprise their creators, such as rejecting true facts presented in a particular question format. We call this chunky post-training: the model learns spurious correlations as a result of distinct chunks of post-training data. We introduce SURF, a black-box pipeline which surfaces these unintended behaviors at run time, and TURF, a tool that traces these failures back to specific post-training data. Applying these tools to frontier models (Claude 4.5, GPT-5.1, Grok 4.1, Gemini 3) and open models (T\u00fclu 3), we show that chunky post-training produces miscalibrated behaviors, which often result from imbalanced or underspecified chunks of post-training data.", "link": "http://arxiv.org/abs/2602.05910v1", "date": "2026-02-05", "relevancy": 2.4286, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4993}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4846}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chunky%20Post-Training%3A%20Data%20Driven%20Failures%20of%20Generalization&body=Title%3A%20Chunky%20Post-Training%3A%20Data%20Driven%20Failures%20of%20Generalization%0AAuthor%3A%20Seoirse%20Murray%20and%20Allison%20Qi%20and%20Timothy%20Qian%20and%20John%20Schulman%20and%20Collin%20Burns%20and%20Sara%20Price%0AAbstract%3A%20LLM%20post-training%20involves%20many%20diverse%20datasets%2C%20each%20targeting%20a%20specific%20behavior.%20But%20these%20datasets%20encode%20incidental%20patterns%20alongside%20intended%20ones%3A%20correlations%20between%20formatting%20and%20content%2C%20narrow%20phrasings%20across%20diverse%20problems%2C%20and%20implicit%20associations%20arising%20from%20the%20discrete%20data%20curation%20process.%20These%20patterns%20are%20often%20invisible%20to%20developers%20yet%20salient%20to%20models%2C%20producing%20behaviors%20that%20surprise%20their%20creators%2C%20such%20as%20rejecting%20true%20facts%20presented%20in%20a%20particular%20question%20format.%20We%20call%20this%20chunky%20post-training%3A%20the%20model%20learns%20spurious%20correlations%20as%20a%20result%20of%20distinct%20chunks%20of%20post-training%20data.%20We%20introduce%20SURF%2C%20a%20black-box%20pipeline%20which%20surfaces%20these%20unintended%20behaviors%20at%20run%20time%2C%20and%20TURF%2C%20a%20tool%20that%20traces%20these%20failures%20back%20to%20specific%20post-training%20data.%20Applying%20these%20tools%20to%20frontier%20models%20%28Claude%204.5%2C%20GPT-5.1%2C%20Grok%204.1%2C%20Gemini%203%29%20and%20open%20models%20%28T%C3%BClu%203%29%2C%20we%20show%20that%20chunky%20post-training%20produces%20miscalibrated%20behaviors%2C%20which%20often%20result%20from%20imbalanced%20or%20underspecified%20chunks%20of%20post-training%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChunky%2520Post-Training%253A%2520Data%2520Driven%2520Failures%2520of%2520Generalization%26entry.906535625%3DSeoirse%2520Murray%2520and%2520Allison%2520Qi%2520and%2520Timothy%2520Qian%2520and%2520John%2520Schulman%2520and%2520Collin%2520Burns%2520and%2520Sara%2520Price%26entry.1292438233%3DLLM%2520post-training%2520involves%2520many%2520diverse%2520datasets%252C%2520each%2520targeting%2520a%2520specific%2520behavior.%2520But%2520these%2520datasets%2520encode%2520incidental%2520patterns%2520alongside%2520intended%2520ones%253A%2520correlations%2520between%2520formatting%2520and%2520content%252C%2520narrow%2520phrasings%2520across%2520diverse%2520problems%252C%2520and%2520implicit%2520associations%2520arising%2520from%2520the%2520discrete%2520data%2520curation%2520process.%2520These%2520patterns%2520are%2520often%2520invisible%2520to%2520developers%2520yet%2520salient%2520to%2520models%252C%2520producing%2520behaviors%2520that%2520surprise%2520their%2520creators%252C%2520such%2520as%2520rejecting%2520true%2520facts%2520presented%2520in%2520a%2520particular%2520question%2520format.%2520We%2520call%2520this%2520chunky%2520post-training%253A%2520the%2520model%2520learns%2520spurious%2520correlations%2520as%2520a%2520result%2520of%2520distinct%2520chunks%2520of%2520post-training%2520data.%2520We%2520introduce%2520SURF%252C%2520a%2520black-box%2520pipeline%2520which%2520surfaces%2520these%2520unintended%2520behaviors%2520at%2520run%2520time%252C%2520and%2520TURF%252C%2520a%2520tool%2520that%2520traces%2520these%2520failures%2520back%2520to%2520specific%2520post-training%2520data.%2520Applying%2520these%2520tools%2520to%2520frontier%2520models%2520%2528Claude%25204.5%252C%2520GPT-5.1%252C%2520Grok%25204.1%252C%2520Gemini%25203%2529%2520and%2520open%2520models%2520%2528T%25C3%25BClu%25203%2529%252C%2520we%2520show%2520that%2520chunky%2520post-training%2520produces%2520miscalibrated%2520behaviors%252C%2520which%2520often%2520result%2520from%2520imbalanced%2520or%2520underspecified%2520chunks%2520of%2520post-training%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chunky%20Post-Training%3A%20Data%20Driven%20Failures%20of%20Generalization&entry.906535625=Seoirse%20Murray%20and%20Allison%20Qi%20and%20Timothy%20Qian%20and%20John%20Schulman%20and%20Collin%20Burns%20and%20Sara%20Price&entry.1292438233=LLM%20post-training%20involves%20many%20diverse%20datasets%2C%20each%20targeting%20a%20specific%20behavior.%20But%20these%20datasets%20encode%20incidental%20patterns%20alongside%20intended%20ones%3A%20correlations%20between%20formatting%20and%20content%2C%20narrow%20phrasings%20across%20diverse%20problems%2C%20and%20implicit%20associations%20arising%20from%20the%20discrete%20data%20curation%20process.%20These%20patterns%20are%20often%20invisible%20to%20developers%20yet%20salient%20to%20models%2C%20producing%20behaviors%20that%20surprise%20their%20creators%2C%20such%20as%20rejecting%20true%20facts%20presented%20in%20a%20particular%20question%20format.%20We%20call%20this%20chunky%20post-training%3A%20the%20model%20learns%20spurious%20correlations%20as%20a%20result%20of%20distinct%20chunks%20of%20post-training%20data.%20We%20introduce%20SURF%2C%20a%20black-box%20pipeline%20which%20surfaces%20these%20unintended%20behaviors%20at%20run%20time%2C%20and%20TURF%2C%20a%20tool%20that%20traces%20these%20failures%20back%20to%20specific%20post-training%20data.%20Applying%20these%20tools%20to%20frontier%20models%20%28Claude%204.5%2C%20GPT-5.1%2C%20Grok%204.1%2C%20Gemini%203%29%20and%20open%20models%20%28T%C3%BClu%203%29%2C%20we%20show%20that%20chunky%20post-training%20produces%20miscalibrated%20behaviors%2C%20which%20often%20result%20from%20imbalanced%20or%20underspecified%20chunks%20of%20post-training%20data.&entry.1838667208=http%3A//arxiv.org/abs/2602.05910v1&entry.124074799=Read"},
{"title": "InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions", "author": "Sirui Xu and Samuel Schulter and Morteza Ziyadi and Xialin He and Xiaohan Fei and Yu-Xiong Wang and Liangyan Gui", "abstract": "Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.", "link": "http://arxiv.org/abs/2602.06035v1", "date": "2026-02-05", "relevancy": 2.4253, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6424}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5818}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InterPrior%3A%20Scaling%20Generative%20Control%20for%20Physics-Based%20Human-Object%20Interactions&body=Title%3A%20InterPrior%3A%20Scaling%20Generative%20Control%20for%20Physics-Based%20Human-Object%20Interactions%0AAuthor%3A%20Sirui%20Xu%20and%20Samuel%20Schulter%20and%20Morteza%20Ziyadi%20and%20Xialin%20He%20and%20Xiaohan%20Fei%20and%20Yu-Xiong%20Wang%20and%20Liangyan%20Gui%0AAbstract%3A%20Humans%20rarely%20plan%20whole-body%20interactions%20with%20objects%20at%20the%20level%20of%20explicit%20whole-body%20movements.%20High-level%20intentions%2C%20such%20as%20affordance%2C%20define%20the%20goal%2C%20while%20coordinated%20balance%2C%20contact%2C%20and%20manipulation%20can%20emerge%20naturally%20from%20underlying%20physical%20and%20motor%20priors.%20Scaling%20such%20priors%20is%20key%20to%20enabling%20humanoids%20to%20compose%20and%20generalize%20loco-manipulation%20skills%20across%20diverse%20contexts%20while%20maintaining%20physically%20coherent%20whole-body%20coordination.%20To%20this%20end%2C%20we%20introduce%20InterPrior%2C%20a%20scalable%20framework%20that%20learns%20a%20unified%20generative%20controller%20through%20large-scale%20imitation%20pretraining%20and%20post-training%20by%20reinforcement%20learning.%20InterPrior%20first%20distills%20a%20full-reference%20imitation%20expert%20into%20a%20versatile%2C%20goal-conditioned%20variational%20policy%20that%20reconstructs%20motion%20from%20multimodal%20observations%20and%20high-level%20intent.%20While%20the%20distilled%20policy%20reconstructs%20training%20behaviors%2C%20it%20does%20not%20generalize%20reliably%20due%20to%20the%20vast%20configuration%20space%20of%20large-scale%20human-object%20interactions.%20To%20address%20this%2C%20we%20apply%20data%20augmentation%20with%20physical%20perturbations%2C%20and%20then%20perform%20reinforcement%20learning%20finetuning%20to%20improve%20competence%20on%20unseen%20goals%20and%20initializations.%20Together%2C%20these%20steps%20consolidate%20the%20reconstructed%20latent%20skills%20into%20a%20valid%20manifold%2C%20yielding%20a%20motion%20prior%20that%20generalizes%20beyond%20the%20training%20data%2C%20e.g.%2C%20it%20can%20incorporate%20new%20behaviors%20such%20as%20interactions%20with%20unseen%20objects.%20We%20further%20demonstrate%20its%20effectiveness%20for%20user-interactive%20control%20and%20its%20potential%20for%20real%20robot%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2602.06035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterPrior%253A%2520Scaling%2520Generative%2520Control%2520for%2520Physics-Based%2520Human-Object%2520Interactions%26entry.906535625%3DSirui%2520Xu%2520and%2520Samuel%2520Schulter%2520and%2520Morteza%2520Ziyadi%2520and%2520Xialin%2520He%2520and%2520Xiaohan%2520Fei%2520and%2520Yu-Xiong%2520Wang%2520and%2520Liangyan%2520Gui%26entry.1292438233%3DHumans%2520rarely%2520plan%2520whole-body%2520interactions%2520with%2520objects%2520at%2520the%2520level%2520of%2520explicit%2520whole-body%2520movements.%2520High-level%2520intentions%252C%2520such%2520as%2520affordance%252C%2520define%2520the%2520goal%252C%2520while%2520coordinated%2520balance%252C%2520contact%252C%2520and%2520manipulation%2520can%2520emerge%2520naturally%2520from%2520underlying%2520physical%2520and%2520motor%2520priors.%2520Scaling%2520such%2520priors%2520is%2520key%2520to%2520enabling%2520humanoids%2520to%2520compose%2520and%2520generalize%2520loco-manipulation%2520skills%2520across%2520diverse%2520contexts%2520while%2520maintaining%2520physically%2520coherent%2520whole-body%2520coordination.%2520To%2520this%2520end%252C%2520we%2520introduce%2520InterPrior%252C%2520a%2520scalable%2520framework%2520that%2520learns%2520a%2520unified%2520generative%2520controller%2520through%2520large-scale%2520imitation%2520pretraining%2520and%2520post-training%2520by%2520reinforcement%2520learning.%2520InterPrior%2520first%2520distills%2520a%2520full-reference%2520imitation%2520expert%2520into%2520a%2520versatile%252C%2520goal-conditioned%2520variational%2520policy%2520that%2520reconstructs%2520motion%2520from%2520multimodal%2520observations%2520and%2520high-level%2520intent.%2520While%2520the%2520distilled%2520policy%2520reconstructs%2520training%2520behaviors%252C%2520it%2520does%2520not%2520generalize%2520reliably%2520due%2520to%2520the%2520vast%2520configuration%2520space%2520of%2520large-scale%2520human-object%2520interactions.%2520To%2520address%2520this%252C%2520we%2520apply%2520data%2520augmentation%2520with%2520physical%2520perturbations%252C%2520and%2520then%2520perform%2520reinforcement%2520learning%2520finetuning%2520to%2520improve%2520competence%2520on%2520unseen%2520goals%2520and%2520initializations.%2520Together%252C%2520these%2520steps%2520consolidate%2520the%2520reconstructed%2520latent%2520skills%2520into%2520a%2520valid%2520manifold%252C%2520yielding%2520a%2520motion%2520prior%2520that%2520generalizes%2520beyond%2520the%2520training%2520data%252C%2520e.g.%252C%2520it%2520can%2520incorporate%2520new%2520behaviors%2520such%2520as%2520interactions%2520with%2520unseen%2520objects.%2520We%2520further%2520demonstrate%2520its%2520effectiveness%2520for%2520user-interactive%2520control%2520and%2520its%2520potential%2520for%2520real%2520robot%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.06035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InterPrior%3A%20Scaling%20Generative%20Control%20for%20Physics-Based%20Human-Object%20Interactions&entry.906535625=Sirui%20Xu%20and%20Samuel%20Schulter%20and%20Morteza%20Ziyadi%20and%20Xialin%20He%20and%20Xiaohan%20Fei%20and%20Yu-Xiong%20Wang%20and%20Liangyan%20Gui&entry.1292438233=Humans%20rarely%20plan%20whole-body%20interactions%20with%20objects%20at%20the%20level%20of%20explicit%20whole-body%20movements.%20High-level%20intentions%2C%20such%20as%20affordance%2C%20define%20the%20goal%2C%20while%20coordinated%20balance%2C%20contact%2C%20and%20manipulation%20can%20emerge%20naturally%20from%20underlying%20physical%20and%20motor%20priors.%20Scaling%20such%20priors%20is%20key%20to%20enabling%20humanoids%20to%20compose%20and%20generalize%20loco-manipulation%20skills%20across%20diverse%20contexts%20while%20maintaining%20physically%20coherent%20whole-body%20coordination.%20To%20this%20end%2C%20we%20introduce%20InterPrior%2C%20a%20scalable%20framework%20that%20learns%20a%20unified%20generative%20controller%20through%20large-scale%20imitation%20pretraining%20and%20post-training%20by%20reinforcement%20learning.%20InterPrior%20first%20distills%20a%20full-reference%20imitation%20expert%20into%20a%20versatile%2C%20goal-conditioned%20variational%20policy%20that%20reconstructs%20motion%20from%20multimodal%20observations%20and%20high-level%20intent.%20While%20the%20distilled%20policy%20reconstructs%20training%20behaviors%2C%20it%20does%20not%20generalize%20reliably%20due%20to%20the%20vast%20configuration%20space%20of%20large-scale%20human-object%20interactions.%20To%20address%20this%2C%20we%20apply%20data%20augmentation%20with%20physical%20perturbations%2C%20and%20then%20perform%20reinforcement%20learning%20finetuning%20to%20improve%20competence%20on%20unseen%20goals%20and%20initializations.%20Together%2C%20these%20steps%20consolidate%20the%20reconstructed%20latent%20skills%20into%20a%20valid%20manifold%2C%20yielding%20a%20motion%20prior%20that%20generalizes%20beyond%20the%20training%20data%2C%20e.g.%2C%20it%20can%20incorporate%20new%20behaviors%20such%20as%20interactions%20with%20unseen%20objects.%20We%20further%20demonstrate%20its%20effectiveness%20for%20user-interactive%20control%20and%20its%20potential%20for%20real%20robot%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2602.06035v1&entry.124074799=Read"},
{"title": "Projected Boosting with Fairness Constraints: Quantifying the Cost of Fair Training Distributions", "author": "Amir Asiaee and Kaveh Aryan", "abstract": "Boosting algorithms enjoy strong theoretical guarantees: when weak learners maintain positive edge, AdaBoost achieves geometric decrease of exponential loss. We study how to incorporate group fairness constraints into boosting while preserving analyzable training dynamics. Our approach, FairBoost, projects the ensemble-induced exponential-weights distribution onto a convex set of distributions satisfying fairness constraints (as a reweighting surrogate), then trains weak learners on this fair distribution. The key theoretical insight is that projecting the training distribution reduces the effective edge of weak learners by a quantity controlled by the KL-divergence of the projection. We prove an exponential-loss bound where the convergence rate depends on weak learner edge minus a \"fairness cost\" term $\u03b4_t = \\sqrt{\\mathrm{KL}(w^t \\| q^t)/2}$. This directly quantifies the accuracy-fairness tradeoff in boosting dynamics. Experiments on standard benchmarks validate the theoretical predictions and demonstrate competitive fairness-accuracy tradeoffs with stable training curves.", "link": "http://arxiv.org/abs/2602.05713v1", "date": "2026-02-05", "relevancy": 2.4223, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5474}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4567}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Projected%20Boosting%20with%20Fairness%20Constraints%3A%20Quantifying%20the%20Cost%20of%20Fair%20Training%20Distributions&body=Title%3A%20Projected%20Boosting%20with%20Fairness%20Constraints%3A%20Quantifying%20the%20Cost%20of%20Fair%20Training%20Distributions%0AAuthor%3A%20Amir%20Asiaee%20and%20Kaveh%20Aryan%0AAbstract%3A%20Boosting%20algorithms%20enjoy%20strong%20theoretical%20guarantees%3A%20when%20weak%20learners%20maintain%20positive%20edge%2C%20AdaBoost%20achieves%20geometric%20decrease%20of%20exponential%20loss.%20We%20study%20how%20to%20incorporate%20group%20fairness%20constraints%20into%20boosting%20while%20preserving%20analyzable%20training%20dynamics.%20Our%20approach%2C%20FairBoost%2C%20projects%20the%20ensemble-induced%20exponential-weights%20distribution%20onto%20a%20convex%20set%20of%20distributions%20satisfying%20fairness%20constraints%20%28as%20a%20reweighting%20surrogate%29%2C%20then%20trains%20weak%20learners%20on%20this%20fair%20distribution.%20The%20key%20theoretical%20insight%20is%20that%20projecting%20the%20training%20distribution%20reduces%20the%20effective%20edge%20of%20weak%20learners%20by%20a%20quantity%20controlled%20by%20the%20KL-divergence%20of%20the%20projection.%20We%20prove%20an%20exponential-loss%20bound%20where%20the%20convergence%20rate%20depends%20on%20weak%20learner%20edge%20minus%20a%20%22fairness%20cost%22%20term%20%24%CE%B4_t%20%3D%20%5Csqrt%7B%5Cmathrm%7BKL%7D%28w%5Et%20%5C%7C%20q%5Et%29/2%7D%24.%20This%20directly%20quantifies%20the%20accuracy-fairness%20tradeoff%20in%20boosting%20dynamics.%20Experiments%20on%20standard%20benchmarks%20validate%20the%20theoretical%20predictions%20and%20demonstrate%20competitive%20fairness-accuracy%20tradeoffs%20with%20stable%20training%20curves.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProjected%2520Boosting%2520with%2520Fairness%2520Constraints%253A%2520Quantifying%2520the%2520Cost%2520of%2520Fair%2520Training%2520Distributions%26entry.906535625%3DAmir%2520Asiaee%2520and%2520Kaveh%2520Aryan%26entry.1292438233%3DBoosting%2520algorithms%2520enjoy%2520strong%2520theoretical%2520guarantees%253A%2520when%2520weak%2520learners%2520maintain%2520positive%2520edge%252C%2520AdaBoost%2520achieves%2520geometric%2520decrease%2520of%2520exponential%2520loss.%2520We%2520study%2520how%2520to%2520incorporate%2520group%2520fairness%2520constraints%2520into%2520boosting%2520while%2520preserving%2520analyzable%2520training%2520dynamics.%2520Our%2520approach%252C%2520FairBoost%252C%2520projects%2520the%2520ensemble-induced%2520exponential-weights%2520distribution%2520onto%2520a%2520convex%2520set%2520of%2520distributions%2520satisfying%2520fairness%2520constraints%2520%2528as%2520a%2520reweighting%2520surrogate%2529%252C%2520then%2520trains%2520weak%2520learners%2520on%2520this%2520fair%2520distribution.%2520The%2520key%2520theoretical%2520insight%2520is%2520that%2520projecting%2520the%2520training%2520distribution%2520reduces%2520the%2520effective%2520edge%2520of%2520weak%2520learners%2520by%2520a%2520quantity%2520controlled%2520by%2520the%2520KL-divergence%2520of%2520the%2520projection.%2520We%2520prove%2520an%2520exponential-loss%2520bound%2520where%2520the%2520convergence%2520rate%2520depends%2520on%2520weak%2520learner%2520edge%2520minus%2520a%2520%2522fairness%2520cost%2522%2520term%2520%2524%25CE%25B4_t%2520%253D%2520%255Csqrt%257B%255Cmathrm%257BKL%257D%2528w%255Et%2520%255C%257C%2520q%255Et%2529/2%257D%2524.%2520This%2520directly%2520quantifies%2520the%2520accuracy-fairness%2520tradeoff%2520in%2520boosting%2520dynamics.%2520Experiments%2520on%2520standard%2520benchmarks%2520validate%2520the%2520theoretical%2520predictions%2520and%2520demonstrate%2520competitive%2520fairness-accuracy%2520tradeoffs%2520with%2520stable%2520training%2520curves.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Projected%20Boosting%20with%20Fairness%20Constraints%3A%20Quantifying%20the%20Cost%20of%20Fair%20Training%20Distributions&entry.906535625=Amir%20Asiaee%20and%20Kaveh%20Aryan&entry.1292438233=Boosting%20algorithms%20enjoy%20strong%20theoretical%20guarantees%3A%20when%20weak%20learners%20maintain%20positive%20edge%2C%20AdaBoost%20achieves%20geometric%20decrease%20of%20exponential%20loss.%20We%20study%20how%20to%20incorporate%20group%20fairness%20constraints%20into%20boosting%20while%20preserving%20analyzable%20training%20dynamics.%20Our%20approach%2C%20FairBoost%2C%20projects%20the%20ensemble-induced%20exponential-weights%20distribution%20onto%20a%20convex%20set%20of%20distributions%20satisfying%20fairness%20constraints%20%28as%20a%20reweighting%20surrogate%29%2C%20then%20trains%20weak%20learners%20on%20this%20fair%20distribution.%20The%20key%20theoretical%20insight%20is%20that%20projecting%20the%20training%20distribution%20reduces%20the%20effective%20edge%20of%20weak%20learners%20by%20a%20quantity%20controlled%20by%20the%20KL-divergence%20of%20the%20projection.%20We%20prove%20an%20exponential-loss%20bound%20where%20the%20convergence%20rate%20depends%20on%20weak%20learner%20edge%20minus%20a%20%22fairness%20cost%22%20term%20%24%CE%B4_t%20%3D%20%5Csqrt%7B%5Cmathrm%7BKL%7D%28w%5Et%20%5C%7C%20q%5Et%29/2%7D%24.%20This%20directly%20quantifies%20the%20accuracy-fairness%20tradeoff%20in%20boosting%20dynamics.%20Experiments%20on%20standard%20benchmarks%20validate%20the%20theoretical%20predictions%20and%20demonstrate%20competitive%20fairness-accuracy%20tradeoffs%20with%20stable%20training%20curves.&entry.1838667208=http%3A//arxiv.org/abs/2602.05713v1&entry.124074799=Read"},
{"title": "Are foundation models useful feature extractors for electroencephalography analysis?", "author": "\u00d6zg\u00fcn Turgut and Felix S. Bott and Markus Ploner and Daniel Rueckert", "abstract": "The success of foundation models in natural language processing and computer vision has motivated similar approaches in time series analysis. While foundational time series models have proven beneficial on a variety of tasks, their effectiveness in medical applications with limited data remains underexplored. In this work, we investigate this question in the context of electroencephalography (EEG) by evaluating general-purpose time series models on age prediction, seizure detection, and classification of clinically relevant EEG events. We compare their diagnostic performance against specialised EEG models and assess the quality of the extracted features. The results show that general-purpose models are competitive and capture features useful to localising demographic and disease-related biomarkers. These findings indicate that foundational time series models can reduce the reliance on large task-specific datasets and models, making them valuable in clinical practice.", "link": "http://arxiv.org/abs/2502.21086v2", "date": "2026-02-05", "relevancy": 2.4223, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5024}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5024}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20foundation%20models%20useful%20feature%20extractors%20for%20electroencephalography%20analysis%3F&body=Title%3A%20Are%20foundation%20models%20useful%20feature%20extractors%20for%20electroencephalography%20analysis%3F%0AAuthor%3A%20%C3%96zg%C3%BCn%20Turgut%20and%20Felix%20S.%20Bott%20and%20Markus%20Ploner%20and%20Daniel%20Rueckert%0AAbstract%3A%20The%20success%20of%20foundation%20models%20in%20natural%20language%20processing%20and%20computer%20vision%20has%20motivated%20similar%20approaches%20in%20time%20series%20analysis.%20While%20foundational%20time%20series%20models%20have%20proven%20beneficial%20on%20a%20variety%20of%20tasks%2C%20their%20effectiveness%20in%20medical%20applications%20with%20limited%20data%20remains%20underexplored.%20In%20this%20work%2C%20we%20investigate%20this%20question%20in%20the%20context%20of%20electroencephalography%20%28EEG%29%20by%20evaluating%20general-purpose%20time%20series%20models%20on%20age%20prediction%2C%20seizure%20detection%2C%20and%20classification%20of%20clinically%20relevant%20EEG%20events.%20We%20compare%20their%20diagnostic%20performance%20against%20specialised%20EEG%20models%20and%20assess%20the%20quality%20of%20the%20extracted%20features.%20The%20results%20show%20that%20general-purpose%20models%20are%20competitive%20and%20capture%20features%20useful%20to%20localising%20demographic%20and%20disease-related%20biomarkers.%20These%20findings%20indicate%20that%20foundational%20time%20series%20models%20can%20reduce%20the%20reliance%20on%20large%20task-specific%20datasets%20and%20models%2C%20making%20them%20valuable%20in%20clinical%20practice.%0ALink%3A%20http%3A//arxiv.org/abs/2502.21086v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520foundation%2520models%2520useful%2520feature%2520extractors%2520for%2520electroencephalography%2520analysis%253F%26entry.906535625%3D%25C3%2596zg%25C3%25BCn%2520Turgut%2520and%2520Felix%2520S.%2520Bott%2520and%2520Markus%2520Ploner%2520and%2520Daniel%2520Rueckert%26entry.1292438233%3DThe%2520success%2520of%2520foundation%2520models%2520in%2520natural%2520language%2520processing%2520and%2520computer%2520vision%2520has%2520motivated%2520similar%2520approaches%2520in%2520time%2520series%2520analysis.%2520While%2520foundational%2520time%2520series%2520models%2520have%2520proven%2520beneficial%2520on%2520a%2520variety%2520of%2520tasks%252C%2520their%2520effectiveness%2520in%2520medical%2520applications%2520with%2520limited%2520data%2520remains%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520investigate%2520this%2520question%2520in%2520the%2520context%2520of%2520electroencephalography%2520%2528EEG%2529%2520by%2520evaluating%2520general-purpose%2520time%2520series%2520models%2520on%2520age%2520prediction%252C%2520seizure%2520detection%252C%2520and%2520classification%2520of%2520clinically%2520relevant%2520EEG%2520events.%2520We%2520compare%2520their%2520diagnostic%2520performance%2520against%2520specialised%2520EEG%2520models%2520and%2520assess%2520the%2520quality%2520of%2520the%2520extracted%2520features.%2520The%2520results%2520show%2520that%2520general-purpose%2520models%2520are%2520competitive%2520and%2520capture%2520features%2520useful%2520to%2520localising%2520demographic%2520and%2520disease-related%2520biomarkers.%2520These%2520findings%2520indicate%2520that%2520foundational%2520time%2520series%2520models%2520can%2520reduce%2520the%2520reliance%2520on%2520large%2520task-specific%2520datasets%2520and%2520models%252C%2520making%2520them%2520valuable%2520in%2520clinical%2520practice.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.21086v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20foundation%20models%20useful%20feature%20extractors%20for%20electroencephalography%20analysis%3F&entry.906535625=%C3%96zg%C3%BCn%20Turgut%20and%20Felix%20S.%20Bott%20and%20Markus%20Ploner%20and%20Daniel%20Rueckert&entry.1292438233=The%20success%20of%20foundation%20models%20in%20natural%20language%20processing%20and%20computer%20vision%20has%20motivated%20similar%20approaches%20in%20time%20series%20analysis.%20While%20foundational%20time%20series%20models%20have%20proven%20beneficial%20on%20a%20variety%20of%20tasks%2C%20their%20effectiveness%20in%20medical%20applications%20with%20limited%20data%20remains%20underexplored.%20In%20this%20work%2C%20we%20investigate%20this%20question%20in%20the%20context%20of%20electroencephalography%20%28EEG%29%20by%20evaluating%20general-purpose%20time%20series%20models%20on%20age%20prediction%2C%20seizure%20detection%2C%20and%20classification%20of%20clinically%20relevant%20EEG%20events.%20We%20compare%20their%20diagnostic%20performance%20against%20specialised%20EEG%20models%20and%20assess%20the%20quality%20of%20the%20extracted%20features.%20The%20results%20show%20that%20general-purpose%20models%20are%20competitive%20and%20capture%20features%20useful%20to%20localising%20demographic%20and%20disease-related%20biomarkers.%20These%20findings%20indicate%20that%20foundational%20time%20series%20models%20can%20reduce%20the%20reliance%20on%20large%20task-specific%20datasets%20and%20models%2C%20making%20them%20valuable%20in%20clinical%20practice.&entry.1838667208=http%3A//arxiv.org/abs/2502.21086v2&entry.124074799=Read"},
{"title": "Disc-Centric Contrastive Learning for Lumbar Spine Severity Grading", "author": "Sajjan Acharya and Pralisha Kansakar", "abstract": "This work examines a disc-centric approach for automated severity grading of lumbar spinal stenosis from sagittal T2-weighted MRI. The method combines contrastive pretraining with disc-level fine-tuning, using a single anatomically localized region of interest per intervertebral disc. Contrastive learning is employed to help the model focus on meaningful disc features and reduce sensitivity to irrelevant differences in image appearance. The framework includes an auxiliary regression task for disc localization and applies weighted focal loss to address class imbalance. Experiments demonstrate a 78.1% balanced accuracy and a reduced severe-to-normal misclassification rate of 2.13% compared with supervised training from scratch. Detecting discs with moderate severity can still be challenging, but focusing on disc-level features provides a practical way to assess the lumbar spinal stenosis.", "link": "http://arxiv.org/abs/2602.05738v1", "date": "2026-02-05", "relevancy": 2.4205, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4952}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.491}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disc-Centric%20Contrastive%20Learning%20for%20Lumbar%20Spine%20Severity%20Grading&body=Title%3A%20Disc-Centric%20Contrastive%20Learning%20for%20Lumbar%20Spine%20Severity%20Grading%0AAuthor%3A%20Sajjan%20Acharya%20and%20Pralisha%20Kansakar%0AAbstract%3A%20This%20work%20examines%20a%20disc-centric%20approach%20for%20automated%20severity%20grading%20of%20lumbar%20spinal%20stenosis%20from%20sagittal%20T2-weighted%20MRI.%20The%20method%20combines%20contrastive%20pretraining%20with%20disc-level%20fine-tuning%2C%20using%20a%20single%20anatomically%20localized%20region%20of%20interest%20per%20intervertebral%20disc.%20Contrastive%20learning%20is%20employed%20to%20help%20the%20model%20focus%20on%20meaningful%20disc%20features%20and%20reduce%20sensitivity%20to%20irrelevant%20differences%20in%20image%20appearance.%20The%20framework%20includes%20an%20auxiliary%20regression%20task%20for%20disc%20localization%20and%20applies%20weighted%20focal%20loss%20to%20address%20class%20imbalance.%20Experiments%20demonstrate%20a%2078.1%25%20balanced%20accuracy%20and%20a%20reduced%20severe-to-normal%20misclassification%20rate%20of%202.13%25%20compared%20with%20supervised%20training%20from%20scratch.%20Detecting%20discs%20with%20moderate%20severity%20can%20still%20be%20challenging%2C%20but%20focusing%20on%20disc-level%20features%20provides%20a%20practical%20way%20to%20assess%20the%20lumbar%20spinal%20stenosis.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisc-Centric%2520Contrastive%2520Learning%2520for%2520Lumbar%2520Spine%2520Severity%2520Grading%26entry.906535625%3DSajjan%2520Acharya%2520and%2520Pralisha%2520Kansakar%26entry.1292438233%3DThis%2520work%2520examines%2520a%2520disc-centric%2520approach%2520for%2520automated%2520severity%2520grading%2520of%2520lumbar%2520spinal%2520stenosis%2520from%2520sagittal%2520T2-weighted%2520MRI.%2520The%2520method%2520combines%2520contrastive%2520pretraining%2520with%2520disc-level%2520fine-tuning%252C%2520using%2520a%2520single%2520anatomically%2520localized%2520region%2520of%2520interest%2520per%2520intervertebral%2520disc.%2520Contrastive%2520learning%2520is%2520employed%2520to%2520help%2520the%2520model%2520focus%2520on%2520meaningful%2520disc%2520features%2520and%2520reduce%2520sensitivity%2520to%2520irrelevant%2520differences%2520in%2520image%2520appearance.%2520The%2520framework%2520includes%2520an%2520auxiliary%2520regression%2520task%2520for%2520disc%2520localization%2520and%2520applies%2520weighted%2520focal%2520loss%2520to%2520address%2520class%2520imbalance.%2520Experiments%2520demonstrate%2520a%252078.1%2525%2520balanced%2520accuracy%2520and%2520a%2520reduced%2520severe-to-normal%2520misclassification%2520rate%2520of%25202.13%2525%2520compared%2520with%2520supervised%2520training%2520from%2520scratch.%2520Detecting%2520discs%2520with%2520moderate%2520severity%2520can%2520still%2520be%2520challenging%252C%2520but%2520focusing%2520on%2520disc-level%2520features%2520provides%2520a%2520practical%2520way%2520to%2520assess%2520the%2520lumbar%2520spinal%2520stenosis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disc-Centric%20Contrastive%20Learning%20for%20Lumbar%20Spine%20Severity%20Grading&entry.906535625=Sajjan%20Acharya%20and%20Pralisha%20Kansakar&entry.1292438233=This%20work%20examines%20a%20disc-centric%20approach%20for%20automated%20severity%20grading%20of%20lumbar%20spinal%20stenosis%20from%20sagittal%20T2-weighted%20MRI.%20The%20method%20combines%20contrastive%20pretraining%20with%20disc-level%20fine-tuning%2C%20using%20a%20single%20anatomically%20localized%20region%20of%20interest%20per%20intervertebral%20disc.%20Contrastive%20learning%20is%20employed%20to%20help%20the%20model%20focus%20on%20meaningful%20disc%20features%20and%20reduce%20sensitivity%20to%20irrelevant%20differences%20in%20image%20appearance.%20The%20framework%20includes%20an%20auxiliary%20regression%20task%20for%20disc%20localization%20and%20applies%20weighted%20focal%20loss%20to%20address%20class%20imbalance.%20Experiments%20demonstrate%20a%2078.1%25%20balanced%20accuracy%20and%20a%20reduced%20severe-to-normal%20misclassification%20rate%20of%202.13%25%20compared%20with%20supervised%20training%20from%20scratch.%20Detecting%20discs%20with%20moderate%20severity%20can%20still%20be%20challenging%2C%20but%20focusing%20on%20disc-level%20features%20provides%20a%20practical%20way%20to%20assess%20the%20lumbar%20spinal%20stenosis.&entry.1838667208=http%3A//arxiv.org/abs/2602.05738v1&entry.124074799=Read"},
{"title": "When Shared Knowledge Hurts: Spectral Over-Accumulation in Model Merging", "author": "Yayuan Li and Ze Peng and Jian Zhang and Jintao Guo and Yue Duan and Yinghuan Shi", "abstract": "Model merging combines multiple fine-tuned models into a single model by adding their weight updates, providing a lightweight alternative to retraining. Existing methods primarily target resolving conflicts between task updates, leaving the failure mode of over-counting shared knowledge unaddressed. We show that when tasks share aligned spectral directions (i.e., overlapping singular vectors), a simple linear combination repeatedly accumulates these directions, inflating the singular values and biasing the merged model toward shared subspaces. To mitigate this issue, we propose Singular Value Calibration (SVC), a training-free and data-free post-processing method that quantifies subspace overlap and rescales inflated singular values to restore a balanced spectrum. Across vision and language benchmarks, SVC consistently improves strong merging baselines and achieves state-of-the-art performance. Furthermore, by modifying only the singular values, SVC improves the performance of Task Arithmetic by 13.0%. Code is available at: https://github.com/lyymuwu/SVC.", "link": "http://arxiv.org/abs/2602.05536v1", "date": "2026-02-05", "relevancy": 2.4059, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4909}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4765}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Shared%20Knowledge%20Hurts%3A%20Spectral%20Over-Accumulation%20in%20Model%20Merging&body=Title%3A%20When%20Shared%20Knowledge%20Hurts%3A%20Spectral%20Over-Accumulation%20in%20Model%20Merging%0AAuthor%3A%20Yayuan%20Li%20and%20Ze%20Peng%20and%20Jian%20Zhang%20and%20Jintao%20Guo%20and%20Yue%20Duan%20and%20Yinghuan%20Shi%0AAbstract%3A%20Model%20merging%20combines%20multiple%20fine-tuned%20models%20into%20a%20single%20model%20by%20adding%20their%20weight%20updates%2C%20providing%20a%20lightweight%20alternative%20to%20retraining.%20Existing%20methods%20primarily%20target%20resolving%20conflicts%20between%20task%20updates%2C%20leaving%20the%20failure%20mode%20of%20over-counting%20shared%20knowledge%20unaddressed.%20We%20show%20that%20when%20tasks%20share%20aligned%20spectral%20directions%20%28i.e.%2C%20overlapping%20singular%20vectors%29%2C%20a%20simple%20linear%20combination%20repeatedly%20accumulates%20these%20directions%2C%20inflating%20the%20singular%20values%20and%20biasing%20the%20merged%20model%20toward%20shared%20subspaces.%20To%20mitigate%20this%20issue%2C%20we%20propose%20Singular%20Value%20Calibration%20%28SVC%29%2C%20a%20training-free%20and%20data-free%20post-processing%20method%20that%20quantifies%20subspace%20overlap%20and%20rescales%20inflated%20singular%20values%20to%20restore%20a%20balanced%20spectrum.%20Across%20vision%20and%20language%20benchmarks%2C%20SVC%20consistently%20improves%20strong%20merging%20baselines%20and%20achieves%20state-of-the-art%20performance.%20Furthermore%2C%20by%20modifying%20only%20the%20singular%20values%2C%20SVC%20improves%20the%20performance%20of%20Task%20Arithmetic%20by%2013.0%25.%20Code%20is%20available%20at%3A%20https%3A//github.com/lyymuwu/SVC.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Shared%2520Knowledge%2520Hurts%253A%2520Spectral%2520Over-Accumulation%2520in%2520Model%2520Merging%26entry.906535625%3DYayuan%2520Li%2520and%2520Ze%2520Peng%2520and%2520Jian%2520Zhang%2520and%2520Jintao%2520Guo%2520and%2520Yue%2520Duan%2520and%2520Yinghuan%2520Shi%26entry.1292438233%3DModel%2520merging%2520combines%2520multiple%2520fine-tuned%2520models%2520into%2520a%2520single%2520model%2520by%2520adding%2520their%2520weight%2520updates%252C%2520providing%2520a%2520lightweight%2520alternative%2520to%2520retraining.%2520Existing%2520methods%2520primarily%2520target%2520resolving%2520conflicts%2520between%2520task%2520updates%252C%2520leaving%2520the%2520failure%2520mode%2520of%2520over-counting%2520shared%2520knowledge%2520unaddressed.%2520We%2520show%2520that%2520when%2520tasks%2520share%2520aligned%2520spectral%2520directions%2520%2528i.e.%252C%2520overlapping%2520singular%2520vectors%2529%252C%2520a%2520simple%2520linear%2520combination%2520repeatedly%2520accumulates%2520these%2520directions%252C%2520inflating%2520the%2520singular%2520values%2520and%2520biasing%2520the%2520merged%2520model%2520toward%2520shared%2520subspaces.%2520To%2520mitigate%2520this%2520issue%252C%2520we%2520propose%2520Singular%2520Value%2520Calibration%2520%2528SVC%2529%252C%2520a%2520training-free%2520and%2520data-free%2520post-processing%2520method%2520that%2520quantifies%2520subspace%2520overlap%2520and%2520rescales%2520inflated%2520singular%2520values%2520to%2520restore%2520a%2520balanced%2520spectrum.%2520Across%2520vision%2520and%2520language%2520benchmarks%252C%2520SVC%2520consistently%2520improves%2520strong%2520merging%2520baselines%2520and%2520achieves%2520state-of-the-art%2520performance.%2520Furthermore%252C%2520by%2520modifying%2520only%2520the%2520singular%2520values%252C%2520SVC%2520improves%2520the%2520performance%2520of%2520Task%2520Arithmetic%2520by%252013.0%2525.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/lyymuwu/SVC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Shared%20Knowledge%20Hurts%3A%20Spectral%20Over-Accumulation%20in%20Model%20Merging&entry.906535625=Yayuan%20Li%20and%20Ze%20Peng%20and%20Jian%20Zhang%20and%20Jintao%20Guo%20and%20Yue%20Duan%20and%20Yinghuan%20Shi&entry.1292438233=Model%20merging%20combines%20multiple%20fine-tuned%20models%20into%20a%20single%20model%20by%20adding%20their%20weight%20updates%2C%20providing%20a%20lightweight%20alternative%20to%20retraining.%20Existing%20methods%20primarily%20target%20resolving%20conflicts%20between%20task%20updates%2C%20leaving%20the%20failure%20mode%20of%20over-counting%20shared%20knowledge%20unaddressed.%20We%20show%20that%20when%20tasks%20share%20aligned%20spectral%20directions%20%28i.e.%2C%20overlapping%20singular%20vectors%29%2C%20a%20simple%20linear%20combination%20repeatedly%20accumulates%20these%20directions%2C%20inflating%20the%20singular%20values%20and%20biasing%20the%20merged%20model%20toward%20shared%20subspaces.%20To%20mitigate%20this%20issue%2C%20we%20propose%20Singular%20Value%20Calibration%20%28SVC%29%2C%20a%20training-free%20and%20data-free%20post-processing%20method%20that%20quantifies%20subspace%20overlap%20and%20rescales%20inflated%20singular%20values%20to%20restore%20a%20balanced%20spectrum.%20Across%20vision%20and%20language%20benchmarks%2C%20SVC%20consistently%20improves%20strong%20merging%20baselines%20and%20achieves%20state-of-the-art%20performance.%20Furthermore%2C%20by%20modifying%20only%20the%20singular%20values%2C%20SVC%20improves%20the%20performance%20of%20Task%20Arithmetic%20by%2013.0%25.%20Code%20is%20available%20at%3A%20https%3A//github.com/lyymuwu/SVC.&entry.1838667208=http%3A//arxiv.org/abs/2602.05536v1&entry.124074799=Read"},
{"title": "Accelerating Benchmarking of Functional Connectivity Modeling via Structure-aware Core-set Selection", "author": "Ling Zhan and Zhen Li and Junjie Huang and Tao Jia", "abstract": "Benchmarking the hundreds of functional connectivity (FC) modeling methods on large-scale fMRI datasets is critical for reproducible neuroscience. However, the combinatorial explosion of model-data pairings makes exhaustive evaluation computationally prohibitive, preventing such assessments from becoming a routine pre-analysis step. To break this bottleneck, we reframe the challenge of FC benchmarking by selecting a small, representative core-set whose sole purpose is to preserve the relative performance ranking of FC operators. We formalize this as a ranking-preserving subset selection problem and propose Structure-aware Contrastive Learning for Core-set Selection (SCLCS), a self-supervised framework to select these core-sets. SCLCS first uses an adaptive Transformer to learn each sample's unique FC structure. It then introduces a novel Structural Perturbation Score (SPS) to quantify the stability of these learned structures during training, identifying samples that represent foundational connectivity archetypes. Finally, while SCLCS identifies stable samples via a top-k ranking, we further introduce a density-balanced sampling strategy as a necessary correction to promote diversity, ensuring the final core-set is both structurally robust and distributionally representative. On the large-scale REST-meta-MDD dataset, SCLCS preserves the ground-truth model ranking with just 10% of the data, outperforming state-of-the-art (SOTA) core-set selection methods by up to 23.2% in ranking consistency (nDCG@k). To our knowledge, this is the first work to formalize core-set selection for FC operator benchmarking, thereby making large-scale operators comparisons a feasible and integral part of computational neuroscience. Code is publicly available on https://github.com/lzhan94swu/SCLCS", "link": "http://arxiv.org/abs/2602.05667v1", "date": "2026-02-05", "relevancy": 2.4002, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.489}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Benchmarking%20of%20Functional%20Connectivity%20Modeling%20via%20Structure-aware%20Core-set%20Selection&body=Title%3A%20Accelerating%20Benchmarking%20of%20Functional%20Connectivity%20Modeling%20via%20Structure-aware%20Core-set%20Selection%0AAuthor%3A%20Ling%20Zhan%20and%20Zhen%20Li%20and%20Junjie%20Huang%20and%20Tao%20Jia%0AAbstract%3A%20Benchmarking%20the%20hundreds%20of%20functional%20connectivity%20%28FC%29%20modeling%20methods%20on%20large-scale%20fMRI%20datasets%20is%20critical%20for%20reproducible%20neuroscience.%20However%2C%20the%20combinatorial%20explosion%20of%20model-data%20pairings%20makes%20exhaustive%20evaluation%20computationally%20prohibitive%2C%20preventing%20such%20assessments%20from%20becoming%20a%20routine%20pre-analysis%20step.%20To%20break%20this%20bottleneck%2C%20we%20reframe%20the%20challenge%20of%20FC%20benchmarking%20by%20selecting%20a%20small%2C%20representative%20core-set%20whose%20sole%20purpose%20is%20to%20preserve%20the%20relative%20performance%20ranking%20of%20FC%20operators.%20We%20formalize%20this%20as%20a%20ranking-preserving%20subset%20selection%20problem%20and%20propose%20Structure-aware%20Contrastive%20Learning%20for%20Core-set%20Selection%20%28SCLCS%29%2C%20a%20self-supervised%20framework%20to%20select%20these%20core-sets.%20SCLCS%20first%20uses%20an%20adaptive%20Transformer%20to%20learn%20each%20sample%27s%20unique%20FC%20structure.%20It%20then%20introduces%20a%20novel%20Structural%20Perturbation%20Score%20%28SPS%29%20to%20quantify%20the%20stability%20of%20these%20learned%20structures%20during%20training%2C%20identifying%20samples%20that%20represent%20foundational%20connectivity%20archetypes.%20Finally%2C%20while%20SCLCS%20identifies%20stable%20samples%20via%20a%20top-k%20ranking%2C%20we%20further%20introduce%20a%20density-balanced%20sampling%20strategy%20as%20a%20necessary%20correction%20to%20promote%20diversity%2C%20ensuring%20the%20final%20core-set%20is%20both%20structurally%20robust%20and%20distributionally%20representative.%20On%20the%20large-scale%20REST-meta-MDD%20dataset%2C%20SCLCS%20preserves%20the%20ground-truth%20model%20ranking%20with%20just%2010%25%20of%20the%20data%2C%20outperforming%20state-of-the-art%20%28SOTA%29%20core-set%20selection%20methods%20by%20up%20to%2023.2%25%20in%20ranking%20consistency%20%28nDCG%40k%29.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20formalize%20core-set%20selection%20for%20FC%20operator%20benchmarking%2C%20thereby%20making%20large-scale%20operators%20comparisons%20a%20feasible%20and%20integral%20part%20of%20computational%20neuroscience.%20Code%20is%20publicly%20available%20on%20https%3A//github.com/lzhan94swu/SCLCS%0ALink%3A%20http%3A//arxiv.org/abs/2602.05667v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Benchmarking%2520of%2520Functional%2520Connectivity%2520Modeling%2520via%2520Structure-aware%2520Core-set%2520Selection%26entry.906535625%3DLing%2520Zhan%2520and%2520Zhen%2520Li%2520and%2520Junjie%2520Huang%2520and%2520Tao%2520Jia%26entry.1292438233%3DBenchmarking%2520the%2520hundreds%2520of%2520functional%2520connectivity%2520%2528FC%2529%2520modeling%2520methods%2520on%2520large-scale%2520fMRI%2520datasets%2520is%2520critical%2520for%2520reproducible%2520neuroscience.%2520However%252C%2520the%2520combinatorial%2520explosion%2520of%2520model-data%2520pairings%2520makes%2520exhaustive%2520evaluation%2520computationally%2520prohibitive%252C%2520preventing%2520such%2520assessments%2520from%2520becoming%2520a%2520routine%2520pre-analysis%2520step.%2520To%2520break%2520this%2520bottleneck%252C%2520we%2520reframe%2520the%2520challenge%2520of%2520FC%2520benchmarking%2520by%2520selecting%2520a%2520small%252C%2520representative%2520core-set%2520whose%2520sole%2520purpose%2520is%2520to%2520preserve%2520the%2520relative%2520performance%2520ranking%2520of%2520FC%2520operators.%2520We%2520formalize%2520this%2520as%2520a%2520ranking-preserving%2520subset%2520selection%2520problem%2520and%2520propose%2520Structure-aware%2520Contrastive%2520Learning%2520for%2520Core-set%2520Selection%2520%2528SCLCS%2529%252C%2520a%2520self-supervised%2520framework%2520to%2520select%2520these%2520core-sets.%2520SCLCS%2520first%2520uses%2520an%2520adaptive%2520Transformer%2520to%2520learn%2520each%2520sample%2527s%2520unique%2520FC%2520structure.%2520It%2520then%2520introduces%2520a%2520novel%2520Structural%2520Perturbation%2520Score%2520%2528SPS%2529%2520to%2520quantify%2520the%2520stability%2520of%2520these%2520learned%2520structures%2520during%2520training%252C%2520identifying%2520samples%2520that%2520represent%2520foundational%2520connectivity%2520archetypes.%2520Finally%252C%2520while%2520SCLCS%2520identifies%2520stable%2520samples%2520via%2520a%2520top-k%2520ranking%252C%2520we%2520further%2520introduce%2520a%2520density-balanced%2520sampling%2520strategy%2520as%2520a%2520necessary%2520correction%2520to%2520promote%2520diversity%252C%2520ensuring%2520the%2520final%2520core-set%2520is%2520both%2520structurally%2520robust%2520and%2520distributionally%2520representative.%2520On%2520the%2520large-scale%2520REST-meta-MDD%2520dataset%252C%2520SCLCS%2520preserves%2520the%2520ground-truth%2520model%2520ranking%2520with%2520just%252010%2525%2520of%2520the%2520data%252C%2520outperforming%2520state-of-the-art%2520%2528SOTA%2529%2520core-set%2520selection%2520methods%2520by%2520up%2520to%252023.2%2525%2520in%2520ranking%2520consistency%2520%2528nDCG%2540k%2529.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520to%2520formalize%2520core-set%2520selection%2520for%2520FC%2520operator%2520benchmarking%252C%2520thereby%2520making%2520large-scale%2520operators%2520comparisons%2520a%2520feasible%2520and%2520integral%2520part%2520of%2520computational%2520neuroscience.%2520Code%2520is%2520publicly%2520available%2520on%2520https%253A//github.com/lzhan94swu/SCLCS%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05667v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Benchmarking%20of%20Functional%20Connectivity%20Modeling%20via%20Structure-aware%20Core-set%20Selection&entry.906535625=Ling%20Zhan%20and%20Zhen%20Li%20and%20Junjie%20Huang%20and%20Tao%20Jia&entry.1292438233=Benchmarking%20the%20hundreds%20of%20functional%20connectivity%20%28FC%29%20modeling%20methods%20on%20large-scale%20fMRI%20datasets%20is%20critical%20for%20reproducible%20neuroscience.%20However%2C%20the%20combinatorial%20explosion%20of%20model-data%20pairings%20makes%20exhaustive%20evaluation%20computationally%20prohibitive%2C%20preventing%20such%20assessments%20from%20becoming%20a%20routine%20pre-analysis%20step.%20To%20break%20this%20bottleneck%2C%20we%20reframe%20the%20challenge%20of%20FC%20benchmarking%20by%20selecting%20a%20small%2C%20representative%20core-set%20whose%20sole%20purpose%20is%20to%20preserve%20the%20relative%20performance%20ranking%20of%20FC%20operators.%20We%20formalize%20this%20as%20a%20ranking-preserving%20subset%20selection%20problem%20and%20propose%20Structure-aware%20Contrastive%20Learning%20for%20Core-set%20Selection%20%28SCLCS%29%2C%20a%20self-supervised%20framework%20to%20select%20these%20core-sets.%20SCLCS%20first%20uses%20an%20adaptive%20Transformer%20to%20learn%20each%20sample%27s%20unique%20FC%20structure.%20It%20then%20introduces%20a%20novel%20Structural%20Perturbation%20Score%20%28SPS%29%20to%20quantify%20the%20stability%20of%20these%20learned%20structures%20during%20training%2C%20identifying%20samples%20that%20represent%20foundational%20connectivity%20archetypes.%20Finally%2C%20while%20SCLCS%20identifies%20stable%20samples%20via%20a%20top-k%20ranking%2C%20we%20further%20introduce%20a%20density-balanced%20sampling%20strategy%20as%20a%20necessary%20correction%20to%20promote%20diversity%2C%20ensuring%20the%20final%20core-set%20is%20both%20structurally%20robust%20and%20distributionally%20representative.%20On%20the%20large-scale%20REST-meta-MDD%20dataset%2C%20SCLCS%20preserves%20the%20ground-truth%20model%20ranking%20with%20just%2010%25%20of%20the%20data%2C%20outperforming%20state-of-the-art%20%28SOTA%29%20core-set%20selection%20methods%20by%20up%20to%2023.2%25%20in%20ranking%20consistency%20%28nDCG%40k%29.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20formalize%20core-set%20selection%20for%20FC%20operator%20benchmarking%2C%20thereby%20making%20large-scale%20operators%20comparisons%20a%20feasible%20and%20integral%20part%20of%20computational%20neuroscience.%20Code%20is%20publicly%20available%20on%20https%3A//github.com/lzhan94swu/SCLCS&entry.1838667208=http%3A//arxiv.org/abs/2602.05667v1&entry.124074799=Read"},
{"title": "Layer-wise LoRA fine-tuning: a similarity metric approach", "author": "Keith Ando Ogawa and Bruno Lopes Yamamoto and Lucas Lauton de Alcantara and Lucas Pellicer and Rosimeire Pereira Costa and Edson Bollis and Anna Helena Reali Costa and Artur Jordao", "abstract": "Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AI. In contrast, enhancing their predictive performance on downstream tasks typically involves adapting their knowledge through fine-tuning. Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing the pre-trained model and updating a smaller number of parameters. In comparison to full fine-tuning, these methods achieve over 99\\% reduction in trainable parameter count, depending on the configuration. Unfortunately, such a reduction may prove insufficient as LLMs continue to grow in scale. In this work, we address the previous problem by systematically selecting only a few layers to fine-tune using LoRA or its variants. We argue that not all layers contribute equally to the model adaptation. Leveraging this, we identify the most relevant layers to fine-tune by measuring their contribution to changes in internal representations. Our method is orthogonal to and readily compatible with existing low-rank adaptation techniques. We reduce the trainable parameters in LoRA-based techniques by up to 50\\%, while maintaining the predictive performance across different models and tasks. Specifically, on encoder-only architectures, this reduction in trainable parameters leads to a negligible predictive performance drop on the GLUE benchmark. On decoder-only architectures, we achieve a small drop or even improvements in the predictive performance on mathematical problem-solving capabilities and coding tasks. Finally, this effectiveness extends to multimodal models, for which we also observe competitive results relative to fine-tuning with LoRA modules in all layers. Code is available at: https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA", "link": "http://arxiv.org/abs/2602.05988v1", "date": "2026-02-05", "relevancy": 2.3981, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4845}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4805}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Layer-wise%20LoRA%20fine-tuning%3A%20a%20similarity%20metric%20approach&body=Title%3A%20Layer-wise%20LoRA%20fine-tuning%3A%20a%20similarity%20metric%20approach%0AAuthor%3A%20Keith%20Ando%20Ogawa%20and%20Bruno%20Lopes%20Yamamoto%20and%20Lucas%20Lauton%20de%20Alcantara%20and%20Lucas%20Pellicer%20and%20Rosimeire%20Pereira%20Costa%20and%20Edson%20Bollis%20and%20Anna%20Helena%20Reali%20Costa%20and%20Artur%20Jordao%0AAbstract%3A%20Pre-training%20Large%20Language%20Models%20%28LLMs%29%20on%20web-scale%20datasets%20becomes%20fundamental%20for%20advancing%20general-purpose%20AI.%20In%20contrast%2C%20enhancing%20their%20predictive%20performance%20on%20downstream%20tasks%20typically%20involves%20adapting%20their%20knowledge%20through%20fine-tuning.%20Parameter-efficient%20fine-tuning%20techniques%2C%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20aim%20to%20reduce%20the%20computational%20cost%20of%20this%20process%20by%20freezing%20the%20pre-trained%20model%20and%20updating%20a%20smaller%20number%20of%20parameters.%20In%20comparison%20to%20full%20fine-tuning%2C%20these%20methods%20achieve%20over%2099%5C%25%20reduction%20in%20trainable%20parameter%20count%2C%20depending%20on%20the%20configuration.%20Unfortunately%2C%20such%20a%20reduction%20may%20prove%20insufficient%20as%20LLMs%20continue%20to%20grow%20in%20scale.%20In%20this%20work%2C%20we%20address%20the%20previous%20problem%20by%20systematically%20selecting%20only%20a%20few%20layers%20to%20fine-tune%20using%20LoRA%20or%20its%20variants.%20We%20argue%20that%20not%20all%20layers%20contribute%20equally%20to%20the%20model%20adaptation.%20Leveraging%20this%2C%20we%20identify%20the%20most%20relevant%20layers%20to%20fine-tune%20by%20measuring%20their%20contribution%20to%20changes%20in%20internal%20representations.%20Our%20method%20is%20orthogonal%20to%20and%20readily%20compatible%20with%20existing%20low-rank%20adaptation%20techniques.%20We%20reduce%20the%20trainable%20parameters%20in%20LoRA-based%20techniques%20by%20up%20to%2050%5C%25%2C%20while%20maintaining%20the%20predictive%20performance%20across%20different%20models%20and%20tasks.%20Specifically%2C%20on%20encoder-only%20architectures%2C%20this%20reduction%20in%20trainable%20parameters%20leads%20to%20a%20negligible%20predictive%20performance%20drop%20on%20the%20GLUE%20benchmark.%20On%20decoder-only%20architectures%2C%20we%20achieve%20a%20small%20drop%20or%20even%20improvements%20in%20the%20predictive%20performance%20on%20mathematical%20problem-solving%20capabilities%20and%20coding%20tasks.%20Finally%2C%20this%20effectiveness%20extends%20to%20multimodal%20models%2C%20for%20which%20we%20also%20observe%20competitive%20results%20relative%20to%20fine-tuning%20with%20LoRA%20modules%20in%20all%20layers.%20Code%20is%20available%20at%3A%20https%3A//github.com/c2d-usp/Layer-wise-LoRA-with-CKA%0ALink%3A%20http%3A//arxiv.org/abs/2602.05988v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayer-wise%2520LoRA%2520fine-tuning%253A%2520a%2520similarity%2520metric%2520approach%26entry.906535625%3DKeith%2520Ando%2520Ogawa%2520and%2520Bruno%2520Lopes%2520Yamamoto%2520and%2520Lucas%2520Lauton%2520de%2520Alcantara%2520and%2520Lucas%2520Pellicer%2520and%2520Rosimeire%2520Pereira%2520Costa%2520and%2520Edson%2520Bollis%2520and%2520Anna%2520Helena%2520Reali%2520Costa%2520and%2520Artur%2520Jordao%26entry.1292438233%3DPre-training%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520on%2520web-scale%2520datasets%2520becomes%2520fundamental%2520for%2520advancing%2520general-purpose%2520AI.%2520In%2520contrast%252C%2520enhancing%2520their%2520predictive%2520performance%2520on%2520downstream%2520tasks%2520typically%2520involves%2520adapting%2520their%2520knowledge%2520through%2520fine-tuning.%2520Parameter-efficient%2520fine-tuning%2520techniques%252C%2520such%2520as%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%252C%2520aim%2520to%2520reduce%2520the%2520computational%2520cost%2520of%2520this%2520process%2520by%2520freezing%2520the%2520pre-trained%2520model%2520and%2520updating%2520a%2520smaller%2520number%2520of%2520parameters.%2520In%2520comparison%2520to%2520full%2520fine-tuning%252C%2520these%2520methods%2520achieve%2520over%252099%255C%2525%2520reduction%2520in%2520trainable%2520parameter%2520count%252C%2520depending%2520on%2520the%2520configuration.%2520Unfortunately%252C%2520such%2520a%2520reduction%2520may%2520prove%2520insufficient%2520as%2520LLMs%2520continue%2520to%2520grow%2520in%2520scale.%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520previous%2520problem%2520by%2520systematically%2520selecting%2520only%2520a%2520few%2520layers%2520to%2520fine-tune%2520using%2520LoRA%2520or%2520its%2520variants.%2520We%2520argue%2520that%2520not%2520all%2520layers%2520contribute%2520equally%2520to%2520the%2520model%2520adaptation.%2520Leveraging%2520this%252C%2520we%2520identify%2520the%2520most%2520relevant%2520layers%2520to%2520fine-tune%2520by%2520measuring%2520their%2520contribution%2520to%2520changes%2520in%2520internal%2520representations.%2520Our%2520method%2520is%2520orthogonal%2520to%2520and%2520readily%2520compatible%2520with%2520existing%2520low-rank%2520adaptation%2520techniques.%2520We%2520reduce%2520the%2520trainable%2520parameters%2520in%2520LoRA-based%2520techniques%2520by%2520up%2520to%252050%255C%2525%252C%2520while%2520maintaining%2520the%2520predictive%2520performance%2520across%2520different%2520models%2520and%2520tasks.%2520Specifically%252C%2520on%2520encoder-only%2520architectures%252C%2520this%2520reduction%2520in%2520trainable%2520parameters%2520leads%2520to%2520a%2520negligible%2520predictive%2520performance%2520drop%2520on%2520the%2520GLUE%2520benchmark.%2520On%2520decoder-only%2520architectures%252C%2520we%2520achieve%2520a%2520small%2520drop%2520or%2520even%2520improvements%2520in%2520the%2520predictive%2520performance%2520on%2520mathematical%2520problem-solving%2520capabilities%2520and%2520coding%2520tasks.%2520Finally%252C%2520this%2520effectiveness%2520extends%2520to%2520multimodal%2520models%252C%2520for%2520which%2520we%2520also%2520observe%2520competitive%2520results%2520relative%2520to%2520fine-tuning%2520with%2520LoRA%2520modules%2520in%2520all%2520layers.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/c2d-usp/Layer-wise-LoRA-with-CKA%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05988v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Layer-wise%20LoRA%20fine-tuning%3A%20a%20similarity%20metric%20approach&entry.906535625=Keith%20Ando%20Ogawa%20and%20Bruno%20Lopes%20Yamamoto%20and%20Lucas%20Lauton%20de%20Alcantara%20and%20Lucas%20Pellicer%20and%20Rosimeire%20Pereira%20Costa%20and%20Edson%20Bollis%20and%20Anna%20Helena%20Reali%20Costa%20and%20Artur%20Jordao&entry.1292438233=Pre-training%20Large%20Language%20Models%20%28LLMs%29%20on%20web-scale%20datasets%20becomes%20fundamental%20for%20advancing%20general-purpose%20AI.%20In%20contrast%2C%20enhancing%20their%20predictive%20performance%20on%20downstream%20tasks%20typically%20involves%20adapting%20their%20knowledge%20through%20fine-tuning.%20Parameter-efficient%20fine-tuning%20techniques%2C%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20aim%20to%20reduce%20the%20computational%20cost%20of%20this%20process%20by%20freezing%20the%20pre-trained%20model%20and%20updating%20a%20smaller%20number%20of%20parameters.%20In%20comparison%20to%20full%20fine-tuning%2C%20these%20methods%20achieve%20over%2099%5C%25%20reduction%20in%20trainable%20parameter%20count%2C%20depending%20on%20the%20configuration.%20Unfortunately%2C%20such%20a%20reduction%20may%20prove%20insufficient%20as%20LLMs%20continue%20to%20grow%20in%20scale.%20In%20this%20work%2C%20we%20address%20the%20previous%20problem%20by%20systematically%20selecting%20only%20a%20few%20layers%20to%20fine-tune%20using%20LoRA%20or%20its%20variants.%20We%20argue%20that%20not%20all%20layers%20contribute%20equally%20to%20the%20model%20adaptation.%20Leveraging%20this%2C%20we%20identify%20the%20most%20relevant%20layers%20to%20fine-tune%20by%20measuring%20their%20contribution%20to%20changes%20in%20internal%20representations.%20Our%20method%20is%20orthogonal%20to%20and%20readily%20compatible%20with%20existing%20low-rank%20adaptation%20techniques.%20We%20reduce%20the%20trainable%20parameters%20in%20LoRA-based%20techniques%20by%20up%20to%2050%5C%25%2C%20while%20maintaining%20the%20predictive%20performance%20across%20different%20models%20and%20tasks.%20Specifically%2C%20on%20encoder-only%20architectures%2C%20this%20reduction%20in%20trainable%20parameters%20leads%20to%20a%20negligible%20predictive%20performance%20drop%20on%20the%20GLUE%20benchmark.%20On%20decoder-only%20architectures%2C%20we%20achieve%20a%20small%20drop%20or%20even%20improvements%20in%20the%20predictive%20performance%20on%20mathematical%20problem-solving%20capabilities%20and%20coding%20tasks.%20Finally%2C%20this%20effectiveness%20extends%20to%20multimodal%20models%2C%20for%20which%20we%20also%20observe%20competitive%20results%20relative%20to%20fine-tuning%20with%20LoRA%20modules%20in%20all%20layers.%20Code%20is%20available%20at%3A%20https%3A//github.com/c2d-usp/Layer-wise-LoRA-with-CKA&entry.1838667208=http%3A//arxiv.org/abs/2602.05988v1&entry.124074799=Read"},
{"title": "Energy Guided smoothness to improve Robustness in Graph Classification", "author": "Farooq Ahmad Wani and Maria Sofia Bucarelli and Andrea Giuseppe Di Francesco and Oleksandr Pryymak and Fabrizio Silvestri", "abstract": "Graph Neural Networks (GNNs) are powerful at solving graph classification tasks, yet applied problems often contain noisy labels. In this work, we study GNN robustness to label noise, demonstrate GNN failure modes when models struggle to generalise on low-order graphs, low label coverage, or when a model is over-parameterized. We establish both empirical and theoretical links between GNN robustness and the reduction of the total Dirichlet Energy of learned node representations, which encapsulates the hypothesized GNN smoothness inductive bias. Finally, we introduce two training strategies to enhance GNN robustness: (1) by incorporating a novel inductive bias in the weight matrices through the removal of negative eigenvalues, connected to Dirichlet Energy minimization; (2) by extending to GNNs a loss penalty that promotes learned smoothness. Importantly, neither approach negatively impacts performance in noise-free settings, supporting our hypothesis that the source of GNNs robustness is their smoothness inductive bias.", "link": "http://arxiv.org/abs/2412.08419v2", "date": "2026-02-05", "relevancy": 2.3907, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5354}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4505}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy%20Guided%20smoothness%20to%20improve%20Robustness%20in%20Graph%20Classification&body=Title%3A%20Energy%20Guided%20smoothness%20to%20improve%20Robustness%20in%20Graph%20Classification%0AAuthor%3A%20Farooq%20Ahmad%20Wani%20and%20Maria%20Sofia%20Bucarelli%20and%20Andrea%20Giuseppe%20Di%20Francesco%20and%20Oleksandr%20Pryymak%20and%20Fabrizio%20Silvestri%0AAbstract%3A%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20powerful%20at%20solving%20graph%20classification%20tasks%2C%20yet%20applied%20problems%20often%20contain%20noisy%20labels.%20In%20this%20work%2C%20we%20study%20GNN%20robustness%20to%20label%20noise%2C%20demonstrate%20GNN%20failure%20modes%20when%20models%20struggle%20to%20generalise%20on%20low-order%20graphs%2C%20low%20label%20coverage%2C%20or%20when%20a%20model%20is%20over-parameterized.%20We%20establish%20both%20empirical%20and%20theoretical%20links%20between%20GNN%20robustness%20and%20the%20reduction%20of%20the%20total%20Dirichlet%20Energy%20of%20learned%20node%20representations%2C%20which%20encapsulates%20the%20hypothesized%20GNN%20smoothness%20inductive%20bias.%20Finally%2C%20we%20introduce%20two%20training%20strategies%20to%20enhance%20GNN%20robustness%3A%20%281%29%20by%20incorporating%20a%20novel%20inductive%20bias%20in%20the%20weight%20matrices%20through%20the%20removal%20of%20negative%20eigenvalues%2C%20connected%20to%20Dirichlet%20Energy%20minimization%3B%20%282%29%20by%20extending%20to%20GNNs%20a%20loss%20penalty%20that%20promotes%20learned%20smoothness.%20Importantly%2C%20neither%20approach%20negatively%20impacts%20performance%20in%20noise-free%20settings%2C%20supporting%20our%20hypothesis%20that%20the%20source%20of%20GNNs%20robustness%20is%20their%20smoothness%20inductive%20bias.%0ALink%3A%20http%3A//arxiv.org/abs/2412.08419v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy%2520Guided%2520smoothness%2520to%2520improve%2520Robustness%2520in%2520Graph%2520Classification%26entry.906535625%3DFarooq%2520Ahmad%2520Wani%2520and%2520Maria%2520Sofia%2520Bucarelli%2520and%2520Andrea%2520Giuseppe%2520Di%2520Francesco%2520and%2520Oleksandr%2520Pryymak%2520and%2520Fabrizio%2520Silvestri%26entry.1292438233%3DGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520powerful%2520at%2520solving%2520graph%2520classification%2520tasks%252C%2520yet%2520applied%2520problems%2520often%2520contain%2520noisy%2520labels.%2520In%2520this%2520work%252C%2520we%2520study%2520GNN%2520robustness%2520to%2520label%2520noise%252C%2520demonstrate%2520GNN%2520failure%2520modes%2520when%2520models%2520struggle%2520to%2520generalise%2520on%2520low-order%2520graphs%252C%2520low%2520label%2520coverage%252C%2520or%2520when%2520a%2520model%2520is%2520over-parameterized.%2520We%2520establish%2520both%2520empirical%2520and%2520theoretical%2520links%2520between%2520GNN%2520robustness%2520and%2520the%2520reduction%2520of%2520the%2520total%2520Dirichlet%2520Energy%2520of%2520learned%2520node%2520representations%252C%2520which%2520encapsulates%2520the%2520hypothesized%2520GNN%2520smoothness%2520inductive%2520bias.%2520Finally%252C%2520we%2520introduce%2520two%2520training%2520strategies%2520to%2520enhance%2520GNN%2520robustness%253A%2520%25281%2529%2520by%2520incorporating%2520a%2520novel%2520inductive%2520bias%2520in%2520the%2520weight%2520matrices%2520through%2520the%2520removal%2520of%2520negative%2520eigenvalues%252C%2520connected%2520to%2520Dirichlet%2520Energy%2520minimization%253B%2520%25282%2529%2520by%2520extending%2520to%2520GNNs%2520a%2520loss%2520penalty%2520that%2520promotes%2520learned%2520smoothness.%2520Importantly%252C%2520neither%2520approach%2520negatively%2520impacts%2520performance%2520in%2520noise-free%2520settings%252C%2520supporting%2520our%2520hypothesis%2520that%2520the%2520source%2520of%2520GNNs%2520robustness%2520is%2520their%2520smoothness%2520inductive%2520bias.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08419v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy%20Guided%20smoothness%20to%20improve%20Robustness%20in%20Graph%20Classification&entry.906535625=Farooq%20Ahmad%20Wani%20and%20Maria%20Sofia%20Bucarelli%20and%20Andrea%20Giuseppe%20Di%20Francesco%20and%20Oleksandr%20Pryymak%20and%20Fabrizio%20Silvestri&entry.1292438233=Graph%20Neural%20Networks%20%28GNNs%29%20are%20powerful%20at%20solving%20graph%20classification%20tasks%2C%20yet%20applied%20problems%20often%20contain%20noisy%20labels.%20In%20this%20work%2C%20we%20study%20GNN%20robustness%20to%20label%20noise%2C%20demonstrate%20GNN%20failure%20modes%20when%20models%20struggle%20to%20generalise%20on%20low-order%20graphs%2C%20low%20label%20coverage%2C%20or%20when%20a%20model%20is%20over-parameterized.%20We%20establish%20both%20empirical%20and%20theoretical%20links%20between%20GNN%20robustness%20and%20the%20reduction%20of%20the%20total%20Dirichlet%20Energy%20of%20learned%20node%20representations%2C%20which%20encapsulates%20the%20hypothesized%20GNN%20smoothness%20inductive%20bias.%20Finally%2C%20we%20introduce%20two%20training%20strategies%20to%20enhance%20GNN%20robustness%3A%20%281%29%20by%20incorporating%20a%20novel%20inductive%20bias%20in%20the%20weight%20matrices%20through%20the%20removal%20of%20negative%20eigenvalues%2C%20connected%20to%20Dirichlet%20Energy%20minimization%3B%20%282%29%20by%20extending%20to%20GNNs%20a%20loss%20penalty%20that%20promotes%20learned%20smoothness.%20Importantly%2C%20neither%20approach%20negatively%20impacts%20performance%20in%20noise-free%20settings%2C%20supporting%20our%20hypothesis%20that%20the%20source%20of%20GNNs%20robustness%20is%20their%20smoothness%20inductive%20bias.&entry.1838667208=http%3A//arxiv.org/abs/2412.08419v2&entry.124074799=Read"},
{"title": "Pathwise Test-Time Correction for Autoregressive Long Video Generation", "author": "Xunzhi Xiang and Zixuan Duan and Guiyu Zhang and Haiyu Zhang and Zhe Gao and Junta Wu and Shaofeng Zhang and Tengfei Wang and Qi Fan and Chunchao Guo", "abstract": "Distilled autoregressive diffusion models facilitate real-time short video synthesis but suffer from severe error accumulation during long-sequence generation. While existing Test-Time Optimization (TTO) methods prove effective for images or short clips, we identify that they fail to mitigate drift in extended sequences due to unstable reward landscapes and the hypersensitivity of distilled parameters. To overcome these limitations, we introduce Test-Time Correction (TTC), a training-free alternative. Specifically, TTC utilizes the initial frame as a stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory. Extensive experiments demonstrate that our method seamlessly integrates with various distilled models, extending generation lengths with negligible overhead while matching the quality of resource-intensive training-based methods on 30-second benchmarks.", "link": "http://arxiv.org/abs/2602.05871v1", "date": "2026-02-05", "relevancy": 2.3897, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6142}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5983}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pathwise%20Test-Time%20Correction%20for%20Autoregressive%20Long%20Video%20Generation&body=Title%3A%20Pathwise%20Test-Time%20Correction%20for%20Autoregressive%20Long%20Video%20Generation%0AAuthor%3A%20Xunzhi%20Xiang%20and%20Zixuan%20Duan%20and%20Guiyu%20Zhang%20and%20Haiyu%20Zhang%20and%20Zhe%20Gao%20and%20Junta%20Wu%20and%20Shaofeng%20Zhang%20and%20Tengfei%20Wang%20and%20Qi%20Fan%20and%20Chunchao%20Guo%0AAbstract%3A%20Distilled%20autoregressive%20diffusion%20models%20facilitate%20real-time%20short%20video%20synthesis%20but%20suffer%20from%20severe%20error%20accumulation%20during%20long-sequence%20generation.%20While%20existing%20Test-Time%20Optimization%20%28TTO%29%20methods%20prove%20effective%20for%20images%20or%20short%20clips%2C%20we%20identify%20that%20they%20fail%20to%20mitigate%20drift%20in%20extended%20sequences%20due%20to%20unstable%20reward%20landscapes%20and%20the%20hypersensitivity%20of%20distilled%20parameters.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20Test-Time%20Correction%20%28TTC%29%2C%20a%20training-free%20alternative.%20Specifically%2C%20TTC%20utilizes%20the%20initial%20frame%20as%20a%20stable%20reference%20anchor%20to%20calibrate%20intermediate%20stochastic%20states%20along%20the%20sampling%20trajectory.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20seamlessly%20integrates%20with%20various%20distilled%20models%2C%20extending%20generation%20lengths%20with%20negligible%20overhead%20while%20matching%20the%20quality%20of%20resource-intensive%20training-based%20methods%20on%2030-second%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPathwise%2520Test-Time%2520Correction%2520for%2520Autoregressive%2520Long%2520Video%2520Generation%26entry.906535625%3DXunzhi%2520Xiang%2520and%2520Zixuan%2520Duan%2520and%2520Guiyu%2520Zhang%2520and%2520Haiyu%2520Zhang%2520and%2520Zhe%2520Gao%2520and%2520Junta%2520Wu%2520and%2520Shaofeng%2520Zhang%2520and%2520Tengfei%2520Wang%2520and%2520Qi%2520Fan%2520and%2520Chunchao%2520Guo%26entry.1292438233%3DDistilled%2520autoregressive%2520diffusion%2520models%2520facilitate%2520real-time%2520short%2520video%2520synthesis%2520but%2520suffer%2520from%2520severe%2520error%2520accumulation%2520during%2520long-sequence%2520generation.%2520While%2520existing%2520Test-Time%2520Optimization%2520%2528TTO%2529%2520methods%2520prove%2520effective%2520for%2520images%2520or%2520short%2520clips%252C%2520we%2520identify%2520that%2520they%2520fail%2520to%2520mitigate%2520drift%2520in%2520extended%2520sequences%2520due%2520to%2520unstable%2520reward%2520landscapes%2520and%2520the%2520hypersensitivity%2520of%2520distilled%2520parameters.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520Test-Time%2520Correction%2520%2528TTC%2529%252C%2520a%2520training-free%2520alternative.%2520Specifically%252C%2520TTC%2520utilizes%2520the%2520initial%2520frame%2520as%2520a%2520stable%2520reference%2520anchor%2520to%2520calibrate%2520intermediate%2520stochastic%2520states%2520along%2520the%2520sampling%2520trajectory.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520seamlessly%2520integrates%2520with%2520various%2520distilled%2520models%252C%2520extending%2520generation%2520lengths%2520with%2520negligible%2520overhead%2520while%2520matching%2520the%2520quality%2520of%2520resource-intensive%2520training-based%2520methods%2520on%252030-second%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pathwise%20Test-Time%20Correction%20for%20Autoregressive%20Long%20Video%20Generation&entry.906535625=Xunzhi%20Xiang%20and%20Zixuan%20Duan%20and%20Guiyu%20Zhang%20and%20Haiyu%20Zhang%20and%20Zhe%20Gao%20and%20Junta%20Wu%20and%20Shaofeng%20Zhang%20and%20Tengfei%20Wang%20and%20Qi%20Fan%20and%20Chunchao%20Guo&entry.1292438233=Distilled%20autoregressive%20diffusion%20models%20facilitate%20real-time%20short%20video%20synthesis%20but%20suffer%20from%20severe%20error%20accumulation%20during%20long-sequence%20generation.%20While%20existing%20Test-Time%20Optimization%20%28TTO%29%20methods%20prove%20effective%20for%20images%20or%20short%20clips%2C%20we%20identify%20that%20they%20fail%20to%20mitigate%20drift%20in%20extended%20sequences%20due%20to%20unstable%20reward%20landscapes%20and%20the%20hypersensitivity%20of%20distilled%20parameters.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20Test-Time%20Correction%20%28TTC%29%2C%20a%20training-free%20alternative.%20Specifically%2C%20TTC%20utilizes%20the%20initial%20frame%20as%20a%20stable%20reference%20anchor%20to%20calibrate%20intermediate%20stochastic%20states%20along%20the%20sampling%20trajectory.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20seamlessly%20integrates%20with%20various%20distilled%20models%2C%20extending%20generation%20lengths%20with%20negligible%20overhead%20while%20matching%20the%20quality%20of%20resource-intensive%20training-based%20methods%20on%2030-second%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2602.05871v1&entry.124074799=Read"},
{"title": "Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations", "author": "Wei Liu and Jiawei Xu and Yingru Li and Longtao Zheng and Tianjian Li and Qian Liu and Junxian He", "abstract": "High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.", "link": "http://arxiv.org/abs/2602.05885v1", "date": "2026-02-05", "relevancy": 2.3877, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4868}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4731}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dr.%20Kernel%3A%20Reinforcement%20Learning%20Done%20Right%20for%20Triton%20Kernel%20Generations&body=Title%3A%20Dr.%20Kernel%3A%20Reinforcement%20Learning%20Done%20Right%20for%20Triton%20Kernel%20Generations%0AAuthor%3A%20Wei%20Liu%20and%20Jiawei%20Xu%20and%20Yingru%20Li%20and%20Longtao%20Zheng%20and%20Tianjian%20Li%20and%20Qian%20Liu%20and%20Junxian%20He%0AAbstract%3A%20High-quality%20kernel%20is%20critical%20for%20scalable%20AI%20systems%2C%20and%20enabling%20LLMs%20to%20generate%20such%20code%20would%20advance%20AI%20development.%20However%2C%20training%20LLMs%20for%20this%20task%20requires%20sufficient%20data%2C%20a%20robust%20environment%2C%20and%20the%20process%20is%20often%20vulnerable%20to%20reward%20hacking%20and%20lazy%20optimization.%20In%20these%20cases%2C%20models%20may%20hack%20training%20rewards%20and%20prioritize%20trivial%20correctness%20over%20meaningful%20speedup.%20In%20this%20paper%2C%20we%20systematically%20study%20reinforcement%20learning%20%28RL%29%20for%20kernel%20generation.%20We%20first%20design%20KernelGYM%2C%20a%20robust%20distributed%20GPU%20environment%20that%20supports%20reward%20hacking%20check%2C%20data%20collection%20from%20multi-turn%20interactions%20and%20long-term%20RL%20training.%20Building%20on%20KernelGYM%2C%20we%20investigate%20effective%20multi-turn%20RL%20methods%20and%20identify%20a%20biased%20policy%20gradient%20issue%20caused%20by%20self-inclusion%20in%20GRPO.%20To%20solve%20this%2C%20we%20propose%20Turn-level%20Reinforce-Leave-One-Out%20%28TRLOO%29%20to%20provide%20unbiased%20advantage%20estimation%20for%20multi-turn%20RL.%20To%20alleviate%20lazy%20optimization%2C%20we%20incorporate%20mismatch%20correction%20for%20training%20stability%20and%20introduce%20Profiling-based%20Rewards%20%28PR%29%20and%20Profiling-based%20Rejection%20Sampling%20%28PRS%29%20to%20overcome%20the%20issue.%20The%20trained%20model%2C%20Dr.Kernel-14B%2C%20reaches%20performance%20competitive%20with%20Claude-4.5-Sonnet%20in%20Kernelbench.%20Finally%2C%20we%20study%20sequential%20test-time%20scaling%20for%20Dr.Kernel-14B.%20On%20the%20KernelBench%20Level-2%20subset%2C%2031.6%25%20of%20the%20generated%20kernels%20achieve%20at%20least%20a%201.2x%20speedup%20over%20the%20Torch%20reference%2C%20surpassing%20Claude-4.5-Sonnet%20%2826.7%25%29%20and%20GPT-5%20%2828.6%25%29.%20When%20selecting%20the%20best%20candidate%20across%20all%20turns%2C%20this%201.2x%20speedup%20rate%20further%20increases%20to%2047.8%25.%20All%20resources%2C%20including%20environment%2C%20training%20code%2C%20models%2C%20and%20dataset%2C%20are%20included%20in%20https%3A//www.github.com/hkust-nlp/KernelGYM.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDr.%2520Kernel%253A%2520Reinforcement%2520Learning%2520Done%2520Right%2520for%2520Triton%2520Kernel%2520Generations%26entry.906535625%3DWei%2520Liu%2520and%2520Jiawei%2520Xu%2520and%2520Yingru%2520Li%2520and%2520Longtao%2520Zheng%2520and%2520Tianjian%2520Li%2520and%2520Qian%2520Liu%2520and%2520Junxian%2520He%26entry.1292438233%3DHigh-quality%2520kernel%2520is%2520critical%2520for%2520scalable%2520AI%2520systems%252C%2520and%2520enabling%2520LLMs%2520to%2520generate%2520such%2520code%2520would%2520advance%2520AI%2520development.%2520However%252C%2520training%2520LLMs%2520for%2520this%2520task%2520requires%2520sufficient%2520data%252C%2520a%2520robust%2520environment%252C%2520and%2520the%2520process%2520is%2520often%2520vulnerable%2520to%2520reward%2520hacking%2520and%2520lazy%2520optimization.%2520In%2520these%2520cases%252C%2520models%2520may%2520hack%2520training%2520rewards%2520and%2520prioritize%2520trivial%2520correctness%2520over%2520meaningful%2520speedup.%2520In%2520this%2520paper%252C%2520we%2520systematically%2520study%2520reinforcement%2520learning%2520%2528RL%2529%2520for%2520kernel%2520generation.%2520We%2520first%2520design%2520KernelGYM%252C%2520a%2520robust%2520distributed%2520GPU%2520environment%2520that%2520supports%2520reward%2520hacking%2520check%252C%2520data%2520collection%2520from%2520multi-turn%2520interactions%2520and%2520long-term%2520RL%2520training.%2520Building%2520on%2520KernelGYM%252C%2520we%2520investigate%2520effective%2520multi-turn%2520RL%2520methods%2520and%2520identify%2520a%2520biased%2520policy%2520gradient%2520issue%2520caused%2520by%2520self-inclusion%2520in%2520GRPO.%2520To%2520solve%2520this%252C%2520we%2520propose%2520Turn-level%2520Reinforce-Leave-One-Out%2520%2528TRLOO%2529%2520to%2520provide%2520unbiased%2520advantage%2520estimation%2520for%2520multi-turn%2520RL.%2520To%2520alleviate%2520lazy%2520optimization%252C%2520we%2520incorporate%2520mismatch%2520correction%2520for%2520training%2520stability%2520and%2520introduce%2520Profiling-based%2520Rewards%2520%2528PR%2529%2520and%2520Profiling-based%2520Rejection%2520Sampling%2520%2528PRS%2529%2520to%2520overcome%2520the%2520issue.%2520The%2520trained%2520model%252C%2520Dr.Kernel-14B%252C%2520reaches%2520performance%2520competitive%2520with%2520Claude-4.5-Sonnet%2520in%2520Kernelbench.%2520Finally%252C%2520we%2520study%2520sequential%2520test-time%2520scaling%2520for%2520Dr.Kernel-14B.%2520On%2520the%2520KernelBench%2520Level-2%2520subset%252C%252031.6%2525%2520of%2520the%2520generated%2520kernels%2520achieve%2520at%2520least%2520a%25201.2x%2520speedup%2520over%2520the%2520Torch%2520reference%252C%2520surpassing%2520Claude-4.5-Sonnet%2520%252826.7%2525%2529%2520and%2520GPT-5%2520%252828.6%2525%2529.%2520When%2520selecting%2520the%2520best%2520candidate%2520across%2520all%2520turns%252C%2520this%25201.2x%2520speedup%2520rate%2520further%2520increases%2520to%252047.8%2525.%2520All%2520resources%252C%2520including%2520environment%252C%2520training%2520code%252C%2520models%252C%2520and%2520dataset%252C%2520are%2520included%2520in%2520https%253A//www.github.com/hkust-nlp/KernelGYM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dr.%20Kernel%3A%20Reinforcement%20Learning%20Done%20Right%20for%20Triton%20Kernel%20Generations&entry.906535625=Wei%20Liu%20and%20Jiawei%20Xu%20and%20Yingru%20Li%20and%20Longtao%20Zheng%20and%20Tianjian%20Li%20and%20Qian%20Liu%20and%20Junxian%20He&entry.1292438233=High-quality%20kernel%20is%20critical%20for%20scalable%20AI%20systems%2C%20and%20enabling%20LLMs%20to%20generate%20such%20code%20would%20advance%20AI%20development.%20However%2C%20training%20LLMs%20for%20this%20task%20requires%20sufficient%20data%2C%20a%20robust%20environment%2C%20and%20the%20process%20is%20often%20vulnerable%20to%20reward%20hacking%20and%20lazy%20optimization.%20In%20these%20cases%2C%20models%20may%20hack%20training%20rewards%20and%20prioritize%20trivial%20correctness%20over%20meaningful%20speedup.%20In%20this%20paper%2C%20we%20systematically%20study%20reinforcement%20learning%20%28RL%29%20for%20kernel%20generation.%20We%20first%20design%20KernelGYM%2C%20a%20robust%20distributed%20GPU%20environment%20that%20supports%20reward%20hacking%20check%2C%20data%20collection%20from%20multi-turn%20interactions%20and%20long-term%20RL%20training.%20Building%20on%20KernelGYM%2C%20we%20investigate%20effective%20multi-turn%20RL%20methods%20and%20identify%20a%20biased%20policy%20gradient%20issue%20caused%20by%20self-inclusion%20in%20GRPO.%20To%20solve%20this%2C%20we%20propose%20Turn-level%20Reinforce-Leave-One-Out%20%28TRLOO%29%20to%20provide%20unbiased%20advantage%20estimation%20for%20multi-turn%20RL.%20To%20alleviate%20lazy%20optimization%2C%20we%20incorporate%20mismatch%20correction%20for%20training%20stability%20and%20introduce%20Profiling-based%20Rewards%20%28PR%29%20and%20Profiling-based%20Rejection%20Sampling%20%28PRS%29%20to%20overcome%20the%20issue.%20The%20trained%20model%2C%20Dr.Kernel-14B%2C%20reaches%20performance%20competitive%20with%20Claude-4.5-Sonnet%20in%20Kernelbench.%20Finally%2C%20we%20study%20sequential%20test-time%20scaling%20for%20Dr.Kernel-14B.%20On%20the%20KernelBench%20Level-2%20subset%2C%2031.6%25%20of%20the%20generated%20kernels%20achieve%20at%20least%20a%201.2x%20speedup%20over%20the%20Torch%20reference%2C%20surpassing%20Claude-4.5-Sonnet%20%2826.7%25%29%20and%20GPT-5%20%2828.6%25%29.%20When%20selecting%20the%20best%20candidate%20across%20all%20turns%2C%20this%201.2x%20speedup%20rate%20further%20increases%20to%2047.8%25.%20All%20resources%2C%20including%20environment%2C%20training%20code%2C%20models%2C%20and%20dataset%2C%20are%20included%20in%20https%3A//www.github.com/hkust-nlp/KernelGYM.&entry.1838667208=http%3A//arxiv.org/abs/2602.05885v1&entry.124074799=Read"},
{"title": "Local EGOP for Continuous Index Learning", "author": "Alex Kokot and Anand Hemmady and Vydhourie Thiyageswaran and Marina Meila", "abstract": "We introduce the setting of continuous index learning, in which a function of many variables varies only along a small number of directions at each point. For efficient estimation, it is beneficial for a learning algorithm to adapt, near each point $x$, to the subspace that captures the local variability of the function $f$. We pose this task as kernel adaptation along a manifold with noise, and introduce Local EGOP learning, a recursive algorithm that utilizes the Expected Gradient Outer Product (EGOP) quadratic form as both a metric and inverse-covariance of our target distribution. We prove that Local EGOP learning adapts to the regularity of the function of interest, showing that under a supervised noisy manifold hypothesis, intrinsic dimensional learning rates are achieved for arbitrarily high-dimensional noise. Empirically, we compare our algorithm to the feature learning capabilities of deep learning. Additionally, we demonstrate improved regression quality compared to two-layer neural networks in the continuous single-index setting.", "link": "http://arxiv.org/abs/2601.07061v3", "date": "2026-02-05", "relevancy": 2.3854, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.515}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4664}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20EGOP%20for%20Continuous%20Index%20Learning&body=Title%3A%20Local%20EGOP%20for%20Continuous%20Index%20Learning%0AAuthor%3A%20Alex%20Kokot%20and%20Anand%20Hemmady%20and%20Vydhourie%20Thiyageswaran%20and%20Marina%20Meila%0AAbstract%3A%20We%20introduce%20the%20setting%20of%20continuous%20index%20learning%2C%20in%20which%20a%20function%20of%20many%20variables%20varies%20only%20along%20a%20small%20number%20of%20directions%20at%20each%20point.%20For%20efficient%20estimation%2C%20it%20is%20beneficial%20for%20a%20learning%20algorithm%20to%20adapt%2C%20near%20each%20point%20%24x%24%2C%20to%20the%20subspace%20that%20captures%20the%20local%20variability%20of%20the%20function%20%24f%24.%20We%20pose%20this%20task%20as%20kernel%20adaptation%20along%20a%20manifold%20with%20noise%2C%20and%20introduce%20Local%20EGOP%20learning%2C%20a%20recursive%20algorithm%20that%20utilizes%20the%20Expected%20Gradient%20Outer%20Product%20%28EGOP%29%20quadratic%20form%20as%20both%20a%20metric%20and%20inverse-covariance%20of%20our%20target%20distribution.%20We%20prove%20that%20Local%20EGOP%20learning%20adapts%20to%20the%20regularity%20of%20the%20function%20of%20interest%2C%20showing%20that%20under%20a%20supervised%20noisy%20manifold%20hypothesis%2C%20intrinsic%20dimensional%20learning%20rates%20are%20achieved%20for%20arbitrarily%20high-dimensional%20noise.%20Empirically%2C%20we%20compare%20our%20algorithm%20to%20the%20feature%20learning%20capabilities%20of%20deep%20learning.%20Additionally%2C%20we%20demonstrate%20improved%20regression%20quality%20compared%20to%20two-layer%20neural%20networks%20in%20the%20continuous%20single-index%20setting.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07061v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520EGOP%2520for%2520Continuous%2520Index%2520Learning%26entry.906535625%3DAlex%2520Kokot%2520and%2520Anand%2520Hemmady%2520and%2520Vydhourie%2520Thiyageswaran%2520and%2520Marina%2520Meila%26entry.1292438233%3DWe%2520introduce%2520the%2520setting%2520of%2520continuous%2520index%2520learning%252C%2520in%2520which%2520a%2520function%2520of%2520many%2520variables%2520varies%2520only%2520along%2520a%2520small%2520number%2520of%2520directions%2520at%2520each%2520point.%2520For%2520efficient%2520estimation%252C%2520it%2520is%2520beneficial%2520for%2520a%2520learning%2520algorithm%2520to%2520adapt%252C%2520near%2520each%2520point%2520%2524x%2524%252C%2520to%2520the%2520subspace%2520that%2520captures%2520the%2520local%2520variability%2520of%2520the%2520function%2520%2524f%2524.%2520We%2520pose%2520this%2520task%2520as%2520kernel%2520adaptation%2520along%2520a%2520manifold%2520with%2520noise%252C%2520and%2520introduce%2520Local%2520EGOP%2520learning%252C%2520a%2520recursive%2520algorithm%2520that%2520utilizes%2520the%2520Expected%2520Gradient%2520Outer%2520Product%2520%2528EGOP%2529%2520quadratic%2520form%2520as%2520both%2520a%2520metric%2520and%2520inverse-covariance%2520of%2520our%2520target%2520distribution.%2520We%2520prove%2520that%2520Local%2520EGOP%2520learning%2520adapts%2520to%2520the%2520regularity%2520of%2520the%2520function%2520of%2520interest%252C%2520showing%2520that%2520under%2520a%2520supervised%2520noisy%2520manifold%2520hypothesis%252C%2520intrinsic%2520dimensional%2520learning%2520rates%2520are%2520achieved%2520for%2520arbitrarily%2520high-dimensional%2520noise.%2520Empirically%252C%2520we%2520compare%2520our%2520algorithm%2520to%2520the%2520feature%2520learning%2520capabilities%2520of%2520deep%2520learning.%2520Additionally%252C%2520we%2520demonstrate%2520improved%2520regression%2520quality%2520compared%2520to%2520two-layer%2520neural%2520networks%2520in%2520the%2520continuous%2520single-index%2520setting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07061v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20EGOP%20for%20Continuous%20Index%20Learning&entry.906535625=Alex%20Kokot%20and%20Anand%20Hemmady%20and%20Vydhourie%20Thiyageswaran%20and%20Marina%20Meila&entry.1292438233=We%20introduce%20the%20setting%20of%20continuous%20index%20learning%2C%20in%20which%20a%20function%20of%20many%20variables%20varies%20only%20along%20a%20small%20number%20of%20directions%20at%20each%20point.%20For%20efficient%20estimation%2C%20it%20is%20beneficial%20for%20a%20learning%20algorithm%20to%20adapt%2C%20near%20each%20point%20%24x%24%2C%20to%20the%20subspace%20that%20captures%20the%20local%20variability%20of%20the%20function%20%24f%24.%20We%20pose%20this%20task%20as%20kernel%20adaptation%20along%20a%20manifold%20with%20noise%2C%20and%20introduce%20Local%20EGOP%20learning%2C%20a%20recursive%20algorithm%20that%20utilizes%20the%20Expected%20Gradient%20Outer%20Product%20%28EGOP%29%20quadratic%20form%20as%20both%20a%20metric%20and%20inverse-covariance%20of%20our%20target%20distribution.%20We%20prove%20that%20Local%20EGOP%20learning%20adapts%20to%20the%20regularity%20of%20the%20function%20of%20interest%2C%20showing%20that%20under%20a%20supervised%20noisy%20manifold%20hypothesis%2C%20intrinsic%20dimensional%20learning%20rates%20are%20achieved%20for%20arbitrarily%20high-dimensional%20noise.%20Empirically%2C%20we%20compare%20our%20algorithm%20to%20the%20feature%20learning%20capabilities%20of%20deep%20learning.%20Additionally%2C%20we%20demonstrate%20improved%20regression%20quality%20compared%20to%20two-layer%20neural%20networks%20in%20the%20continuous%20single-index%20setting.&entry.1838667208=http%3A//arxiv.org/abs/2601.07061v3&entry.124074799=Read"},
{"title": "Learning to Inject: Automated Prompt Injection via Reinforcement Learning", "author": "Xin Chen and Jie Zhang and Florian Tramer", "abstract": "Prompt injection is one of the most critical vulnerabilities in LLM agents; yet, effective automated attacks remain largely unexplored from an optimization perspective. Existing methods heavily depend on human red-teamers and hand-crafted prompts, limiting their scalability and adaptability. We propose AutoInject, a reinforcement learning framework that generates universal, transferable adversarial suffixes while jointly optimizing for attack success and utility preservation on benign tasks. Our black-box method supports both query-based optimization and transfer attacks to unseen models and tasks. Using only a 1.5B parameter adversarial suffix generator, we successfully compromise frontier systems including GPT 5 Nano, Claude Sonnet 3.5, and Gemini 2.5 Flash on the AgentDojo benchmark, establishing a stronger baseline for automated prompt injection research.", "link": "http://arxiv.org/abs/2602.05746v1", "date": "2026-02-05", "relevancy": 2.3771, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.483}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4738}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Inject%3A%20Automated%20Prompt%20Injection%20via%20Reinforcement%20Learning&body=Title%3A%20Learning%20to%20Inject%3A%20Automated%20Prompt%20Injection%20via%20Reinforcement%20Learning%0AAuthor%3A%20Xin%20Chen%20and%20Jie%20Zhang%20and%20Florian%20Tramer%0AAbstract%3A%20Prompt%20injection%20is%20one%20of%20the%20most%20critical%20vulnerabilities%20in%20LLM%20agents%3B%20yet%2C%20effective%20automated%20attacks%20remain%20largely%20unexplored%20from%20an%20optimization%20perspective.%20Existing%20methods%20heavily%20depend%20on%20human%20red-teamers%20and%20hand-crafted%20prompts%2C%20limiting%20their%20scalability%20and%20adaptability.%20We%20propose%20AutoInject%2C%20a%20reinforcement%20learning%20framework%20that%20generates%20universal%2C%20transferable%20adversarial%20suffixes%20while%20jointly%20optimizing%20for%20attack%20success%20and%20utility%20preservation%20on%20benign%20tasks.%20Our%20black-box%20method%20supports%20both%20query-based%20optimization%20and%20transfer%20attacks%20to%20unseen%20models%20and%20tasks.%20Using%20only%20a%201.5B%20parameter%20adversarial%20suffix%20generator%2C%20we%20successfully%20compromise%20frontier%20systems%20including%20GPT%205%20Nano%2C%20Claude%20Sonnet%203.5%2C%20and%20Gemini%202.5%20Flash%20on%20the%20AgentDojo%20benchmark%2C%20establishing%20a%20stronger%20baseline%20for%20automated%20prompt%20injection%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Inject%253A%2520Automated%2520Prompt%2520Injection%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DXin%2520Chen%2520and%2520Jie%2520Zhang%2520and%2520Florian%2520Tramer%26entry.1292438233%3DPrompt%2520injection%2520is%2520one%2520of%2520the%2520most%2520critical%2520vulnerabilities%2520in%2520LLM%2520agents%253B%2520yet%252C%2520effective%2520automated%2520attacks%2520remain%2520largely%2520unexplored%2520from%2520an%2520optimization%2520perspective.%2520Existing%2520methods%2520heavily%2520depend%2520on%2520human%2520red-teamers%2520and%2520hand-crafted%2520prompts%252C%2520limiting%2520their%2520scalability%2520and%2520adaptability.%2520We%2520propose%2520AutoInject%252C%2520a%2520reinforcement%2520learning%2520framework%2520that%2520generates%2520universal%252C%2520transferable%2520adversarial%2520suffixes%2520while%2520jointly%2520optimizing%2520for%2520attack%2520success%2520and%2520utility%2520preservation%2520on%2520benign%2520tasks.%2520Our%2520black-box%2520method%2520supports%2520both%2520query-based%2520optimization%2520and%2520transfer%2520attacks%2520to%2520unseen%2520models%2520and%2520tasks.%2520Using%2520only%2520a%25201.5B%2520parameter%2520adversarial%2520suffix%2520generator%252C%2520we%2520successfully%2520compromise%2520frontier%2520systems%2520including%2520GPT%25205%2520Nano%252C%2520Claude%2520Sonnet%25203.5%252C%2520and%2520Gemini%25202.5%2520Flash%2520on%2520the%2520AgentDojo%2520benchmark%252C%2520establishing%2520a%2520stronger%2520baseline%2520for%2520automated%2520prompt%2520injection%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Inject%3A%20Automated%20Prompt%20Injection%20via%20Reinforcement%20Learning&entry.906535625=Xin%20Chen%20and%20Jie%20Zhang%20and%20Florian%20Tramer&entry.1292438233=Prompt%20injection%20is%20one%20of%20the%20most%20critical%20vulnerabilities%20in%20LLM%20agents%3B%20yet%2C%20effective%20automated%20attacks%20remain%20largely%20unexplored%20from%20an%20optimization%20perspective.%20Existing%20methods%20heavily%20depend%20on%20human%20red-teamers%20and%20hand-crafted%20prompts%2C%20limiting%20their%20scalability%20and%20adaptability.%20We%20propose%20AutoInject%2C%20a%20reinforcement%20learning%20framework%20that%20generates%20universal%2C%20transferable%20adversarial%20suffixes%20while%20jointly%20optimizing%20for%20attack%20success%20and%20utility%20preservation%20on%20benign%20tasks.%20Our%20black-box%20method%20supports%20both%20query-based%20optimization%20and%20transfer%20attacks%20to%20unseen%20models%20and%20tasks.%20Using%20only%20a%201.5B%20parameter%20adversarial%20suffix%20generator%2C%20we%20successfully%20compromise%20frontier%20systems%20including%20GPT%205%20Nano%2C%20Claude%20Sonnet%203.5%2C%20and%20Gemini%202.5%20Flash%20on%20the%20AgentDojo%20benchmark%2C%20establishing%20a%20stronger%20baseline%20for%20automated%20prompt%20injection%20research.&entry.1838667208=http%3A//arxiv.org/abs/2602.05746v1&entry.124074799=Read"},
{"title": "Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification", "author": "Tao Huang and Rui Wang and Xiaofei Liu and Yi Qin and Li Duan and Liping Jing", "abstract": "Large vision-language models (LVLMs) have shown substantial advances in multimodal understanding and generation. However, when presented with incompetent or adversarial inputs, they frequently produce unreliable or even harmful content, such as fact hallucinations or dangerous instructions. This misalignment with human expectations, referred to as \\emph{misbehaviors} of LVLMs, raises serious concerns for deployment in critical applications. These misbehaviors are found to stem from epistemic uncertainty, specifically either conflicting internal knowledge or the absence of supporting information. However, existing uncertainty quantification methods, which typically capture only overall epistemic uncertainty, have shown limited effectiveness in identifying such issues. To address this gap, we propose Evidential Uncertainty Quantification (EUQ), a fine-grained method that captures both information conflict and ignorance for effective detection of LVLM misbehaviors. In particular, we interpret features from the model output head as either supporting (positive) or opposing (negative) evidence. Leveraging Evidence Theory, we model and aggregate this evidence to quantify internal conflict and knowledge gaps within a single forward pass. We extensively evaluate our method across four categories of misbehavior, including hallucinations, jailbreaks, adversarial vulnerabilities, and out-of-distribution (OOD) failures, using state-of-the-art LVLMs, and find that EUQ consistently outperforms strong baselines, showing that hallucinations correspond to high internal conflict and OOD failures to high ignorance. Furthermore, layer-wise evidential uncertainty dynamics analysis helps interpret the evolution of internal representations from a new perspective. The source code is available at https://github.com/HT86159/EUQ.", "link": "http://arxiv.org/abs/2602.05535v1", "date": "2026-02-05", "relevancy": 2.3763, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6589}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.6245}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Misbehaviors%20of%20Large%20Vision-Language%20Models%20by%20Evidential%20Uncertainty%20Quantification&body=Title%3A%20Detecting%20Misbehaviors%20of%20Large%20Vision-Language%20Models%20by%20Evidential%20Uncertainty%20Quantification%0AAuthor%3A%20Tao%20Huang%20and%20Rui%20Wang%20and%20Xiaofei%20Liu%20and%20Yi%20Qin%20and%20Li%20Duan%20and%20Liping%20Jing%0AAbstract%3A%20Large%20vision-language%20models%20%28LVLMs%29%20have%20shown%20substantial%20advances%20in%20multimodal%20understanding%20and%20generation.%20However%2C%20when%20presented%20with%20incompetent%20or%20adversarial%20inputs%2C%20they%20frequently%20produce%20unreliable%20or%20even%20harmful%20content%2C%20such%20as%20fact%20hallucinations%20or%20dangerous%20instructions.%20This%20misalignment%20with%20human%20expectations%2C%20referred%20to%20as%20%5Cemph%7Bmisbehaviors%7D%20of%20LVLMs%2C%20raises%20serious%20concerns%20for%20deployment%20in%20critical%20applications.%20These%20misbehaviors%20are%20found%20to%20stem%20from%20epistemic%20uncertainty%2C%20specifically%20either%20conflicting%20internal%20knowledge%20or%20the%20absence%20of%20supporting%20information.%20However%2C%20existing%20uncertainty%20quantification%20methods%2C%20which%20typically%20capture%20only%20overall%20epistemic%20uncertainty%2C%20have%20shown%20limited%20effectiveness%20in%20identifying%20such%20issues.%20To%20address%20this%20gap%2C%20we%20propose%20Evidential%20Uncertainty%20Quantification%20%28EUQ%29%2C%20a%20fine-grained%20method%20that%20captures%20both%20information%20conflict%20and%20ignorance%20for%20effective%20detection%20of%20LVLM%20misbehaviors.%20In%20particular%2C%20we%20interpret%20features%20from%20the%20model%20output%20head%20as%20either%20supporting%20%28positive%29%20or%20opposing%20%28negative%29%20evidence.%20Leveraging%20Evidence%20Theory%2C%20we%20model%20and%20aggregate%20this%20evidence%20to%20quantify%20internal%20conflict%20and%20knowledge%20gaps%20within%20a%20single%20forward%20pass.%20We%20extensively%20evaluate%20our%20method%20across%20four%20categories%20of%20misbehavior%2C%20including%20hallucinations%2C%20jailbreaks%2C%20adversarial%20vulnerabilities%2C%20and%20out-of-distribution%20%28OOD%29%20failures%2C%20using%20state-of-the-art%20LVLMs%2C%20and%20find%20that%20EUQ%20consistently%20outperforms%20strong%20baselines%2C%20showing%20that%20hallucinations%20correspond%20to%20high%20internal%20conflict%20and%20OOD%20failures%20to%20high%20ignorance.%20Furthermore%2C%20layer-wise%20evidential%20uncertainty%20dynamics%20analysis%20helps%20interpret%20the%20evolution%20of%20internal%20representations%20from%20a%20new%20perspective.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/HT86159/EUQ.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Misbehaviors%2520of%2520Large%2520Vision-Language%2520Models%2520by%2520Evidential%2520Uncertainty%2520Quantification%26entry.906535625%3DTao%2520Huang%2520and%2520Rui%2520Wang%2520and%2520Xiaofei%2520Liu%2520and%2520Yi%2520Qin%2520and%2520Li%2520Duan%2520and%2520Liping%2520Jing%26entry.1292438233%3DLarge%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520shown%2520substantial%2520advances%2520in%2520multimodal%2520understanding%2520and%2520generation.%2520However%252C%2520when%2520presented%2520with%2520incompetent%2520or%2520adversarial%2520inputs%252C%2520they%2520frequently%2520produce%2520unreliable%2520or%2520even%2520harmful%2520content%252C%2520such%2520as%2520fact%2520hallucinations%2520or%2520dangerous%2520instructions.%2520This%2520misalignment%2520with%2520human%2520expectations%252C%2520referred%2520to%2520as%2520%255Cemph%257Bmisbehaviors%257D%2520of%2520LVLMs%252C%2520raises%2520serious%2520concerns%2520for%2520deployment%2520in%2520critical%2520applications.%2520These%2520misbehaviors%2520are%2520found%2520to%2520stem%2520from%2520epistemic%2520uncertainty%252C%2520specifically%2520either%2520conflicting%2520internal%2520knowledge%2520or%2520the%2520absence%2520of%2520supporting%2520information.%2520However%252C%2520existing%2520uncertainty%2520quantification%2520methods%252C%2520which%2520typically%2520capture%2520only%2520overall%2520epistemic%2520uncertainty%252C%2520have%2520shown%2520limited%2520effectiveness%2520in%2520identifying%2520such%2520issues.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520Evidential%2520Uncertainty%2520Quantification%2520%2528EUQ%2529%252C%2520a%2520fine-grained%2520method%2520that%2520captures%2520both%2520information%2520conflict%2520and%2520ignorance%2520for%2520effective%2520detection%2520of%2520LVLM%2520misbehaviors.%2520In%2520particular%252C%2520we%2520interpret%2520features%2520from%2520the%2520model%2520output%2520head%2520as%2520either%2520supporting%2520%2528positive%2529%2520or%2520opposing%2520%2528negative%2529%2520evidence.%2520Leveraging%2520Evidence%2520Theory%252C%2520we%2520model%2520and%2520aggregate%2520this%2520evidence%2520to%2520quantify%2520internal%2520conflict%2520and%2520knowledge%2520gaps%2520within%2520a%2520single%2520forward%2520pass.%2520We%2520extensively%2520evaluate%2520our%2520method%2520across%2520four%2520categories%2520of%2520misbehavior%252C%2520including%2520hallucinations%252C%2520jailbreaks%252C%2520adversarial%2520vulnerabilities%252C%2520and%2520out-of-distribution%2520%2528OOD%2529%2520failures%252C%2520using%2520state-of-the-art%2520LVLMs%252C%2520and%2520find%2520that%2520EUQ%2520consistently%2520outperforms%2520strong%2520baselines%252C%2520showing%2520that%2520hallucinations%2520correspond%2520to%2520high%2520internal%2520conflict%2520and%2520OOD%2520failures%2520to%2520high%2520ignorance.%2520Furthermore%252C%2520layer-wise%2520evidential%2520uncertainty%2520dynamics%2520analysis%2520helps%2520interpret%2520the%2520evolution%2520of%2520internal%2520representations%2520from%2520a%2520new%2520perspective.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/HT86159/EUQ.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Misbehaviors%20of%20Large%20Vision-Language%20Models%20by%20Evidential%20Uncertainty%20Quantification&entry.906535625=Tao%20Huang%20and%20Rui%20Wang%20and%20Xiaofei%20Liu%20and%20Yi%20Qin%20and%20Li%20Duan%20and%20Liping%20Jing&entry.1292438233=Large%20vision-language%20models%20%28LVLMs%29%20have%20shown%20substantial%20advances%20in%20multimodal%20understanding%20and%20generation.%20However%2C%20when%20presented%20with%20incompetent%20or%20adversarial%20inputs%2C%20they%20frequently%20produce%20unreliable%20or%20even%20harmful%20content%2C%20such%20as%20fact%20hallucinations%20or%20dangerous%20instructions.%20This%20misalignment%20with%20human%20expectations%2C%20referred%20to%20as%20%5Cemph%7Bmisbehaviors%7D%20of%20LVLMs%2C%20raises%20serious%20concerns%20for%20deployment%20in%20critical%20applications.%20These%20misbehaviors%20are%20found%20to%20stem%20from%20epistemic%20uncertainty%2C%20specifically%20either%20conflicting%20internal%20knowledge%20or%20the%20absence%20of%20supporting%20information.%20However%2C%20existing%20uncertainty%20quantification%20methods%2C%20which%20typically%20capture%20only%20overall%20epistemic%20uncertainty%2C%20have%20shown%20limited%20effectiveness%20in%20identifying%20such%20issues.%20To%20address%20this%20gap%2C%20we%20propose%20Evidential%20Uncertainty%20Quantification%20%28EUQ%29%2C%20a%20fine-grained%20method%20that%20captures%20both%20information%20conflict%20and%20ignorance%20for%20effective%20detection%20of%20LVLM%20misbehaviors.%20In%20particular%2C%20we%20interpret%20features%20from%20the%20model%20output%20head%20as%20either%20supporting%20%28positive%29%20or%20opposing%20%28negative%29%20evidence.%20Leveraging%20Evidence%20Theory%2C%20we%20model%20and%20aggregate%20this%20evidence%20to%20quantify%20internal%20conflict%20and%20knowledge%20gaps%20within%20a%20single%20forward%20pass.%20We%20extensively%20evaluate%20our%20method%20across%20four%20categories%20of%20misbehavior%2C%20including%20hallucinations%2C%20jailbreaks%2C%20adversarial%20vulnerabilities%2C%20and%20out-of-distribution%20%28OOD%29%20failures%2C%20using%20state-of-the-art%20LVLMs%2C%20and%20find%20that%20EUQ%20consistently%20outperforms%20strong%20baselines%2C%20showing%20that%20hallucinations%20correspond%20to%20high%20internal%20conflict%20and%20OOD%20failures%20to%20high%20ignorance.%20Furthermore%2C%20layer-wise%20evidential%20uncertainty%20dynamics%20analysis%20helps%20interpret%20the%20evolution%20of%20internal%20representations%20from%20a%20new%20perspective.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/HT86159/EUQ.&entry.1838667208=http%3A//arxiv.org/abs/2602.05535v1&entry.124074799=Read"},
{"title": "EuroLLM-22B: Technical Report", "author": "Miguel Moura Ramos and Duarte M. Alves and Hippolyte Gisserot-Boukhlef and Jo\u00e3o Alves and Pedro Henrique Martins and Patrick Fernandes and Jos\u00e9 Pombal and Nuno M. Guerreiro and Ricardo Rei and Nicolas Boizard and Amin Farajian and Mateusz Klimaszewski and Jos\u00e9 G. C. de Souza and Barry Haddow and Fran\u00e7ois Yvon and Pierre Colombo and Alexandra Birch and Andr\u00e9 F. T. Martins", "abstract": "This report presents EuroLLM-22B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-22B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. Across a broad set of multilingual benchmarks, EuroLLM-22B demonstrates strong performance in reasoning, instruction following, and translation, achieving results competitive with models of comparable size. To support future research, we release our base and instruction-tuned models, our multilingual web pretraining data and updated EuroBlocks instruction datasets, as well as our pre-training and evaluation codebases.", "link": "http://arxiv.org/abs/2602.05879v1", "date": "2026-02-05", "relevancy": 2.3555, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4864}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4864}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EuroLLM-22B%3A%20Technical%20Report&body=Title%3A%20EuroLLM-22B%3A%20Technical%20Report%0AAuthor%3A%20Miguel%20Moura%20Ramos%20and%20Duarte%20M.%20Alves%20and%20Hippolyte%20Gisserot-Boukhlef%20and%20Jo%C3%A3o%20Alves%20and%20Pedro%20Henrique%20Martins%20and%20Patrick%20Fernandes%20and%20Jos%C3%A9%20Pombal%20and%20Nuno%20M.%20Guerreiro%20and%20Ricardo%20Rei%20and%20Nicolas%20Boizard%20and%20Amin%20Farajian%20and%20Mateusz%20Klimaszewski%20and%20Jos%C3%A9%20G.%20C.%20de%20Souza%20and%20Barry%20Haddow%20and%20Fran%C3%A7ois%20Yvon%20and%20Pierre%20Colombo%20and%20Alexandra%20Birch%20and%20Andr%C3%A9%20F.%20T.%20Martins%0AAbstract%3A%20This%20report%20presents%20EuroLLM-22B%2C%20a%20large%20language%20model%20trained%20from%20scratch%20to%20support%20the%20needs%20of%20European%20citizens%20by%20covering%20all%2024%20official%20European%20Union%20languages%20and%2011%20additional%20languages.%20EuroLLM%20addresses%20the%20issue%20of%20European%20languages%20being%20underrepresented%20and%20underserved%20in%20existing%20open%20large%20language%20models.%20We%20provide%20a%20comprehensive%20overview%20of%20EuroLLM-22B%27s%20development%2C%20including%20tokenizer%20design%2C%20architectural%20specifications%2C%20data%20filtering%2C%20and%20training%20procedures.%20Across%20a%20broad%20set%20of%20multilingual%20benchmarks%2C%20EuroLLM-22B%20demonstrates%20strong%20performance%20in%20reasoning%2C%20instruction%20following%2C%20and%20translation%2C%20achieving%20results%20competitive%20with%20models%20of%20comparable%20size.%20To%20support%20future%20research%2C%20we%20release%20our%20base%20and%20instruction-tuned%20models%2C%20our%20multilingual%20web%20pretraining%20data%20and%20updated%20EuroBlocks%20instruction%20datasets%2C%20as%20well%20as%20our%20pre-training%20and%20evaluation%20codebases.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEuroLLM-22B%253A%2520Technical%2520Report%26entry.906535625%3DMiguel%2520Moura%2520Ramos%2520and%2520Duarte%2520M.%2520Alves%2520and%2520Hippolyte%2520Gisserot-Boukhlef%2520and%2520Jo%25C3%25A3o%2520Alves%2520and%2520Pedro%2520Henrique%2520Martins%2520and%2520Patrick%2520Fernandes%2520and%2520Jos%25C3%25A9%2520Pombal%2520and%2520Nuno%2520M.%2520Guerreiro%2520and%2520Ricardo%2520Rei%2520and%2520Nicolas%2520Boizard%2520and%2520Amin%2520Farajian%2520and%2520Mateusz%2520Klimaszewski%2520and%2520Jos%25C3%25A9%2520G.%2520C.%2520de%2520Souza%2520and%2520Barry%2520Haddow%2520and%2520Fran%25C3%25A7ois%2520Yvon%2520and%2520Pierre%2520Colombo%2520and%2520Alexandra%2520Birch%2520and%2520Andr%25C3%25A9%2520F.%2520T.%2520Martins%26entry.1292438233%3DThis%2520report%2520presents%2520EuroLLM-22B%252C%2520a%2520large%2520language%2520model%2520trained%2520from%2520scratch%2520to%2520support%2520the%2520needs%2520of%2520European%2520citizens%2520by%2520covering%2520all%252024%2520official%2520European%2520Union%2520languages%2520and%252011%2520additional%2520languages.%2520EuroLLM%2520addresses%2520the%2520issue%2520of%2520European%2520languages%2520being%2520underrepresented%2520and%2520underserved%2520in%2520existing%2520open%2520large%2520language%2520models.%2520We%2520provide%2520a%2520comprehensive%2520overview%2520of%2520EuroLLM-22B%2527s%2520development%252C%2520including%2520tokenizer%2520design%252C%2520architectural%2520specifications%252C%2520data%2520filtering%252C%2520and%2520training%2520procedures.%2520Across%2520a%2520broad%2520set%2520of%2520multilingual%2520benchmarks%252C%2520EuroLLM-22B%2520demonstrates%2520strong%2520performance%2520in%2520reasoning%252C%2520instruction%2520following%252C%2520and%2520translation%252C%2520achieving%2520results%2520competitive%2520with%2520models%2520of%2520comparable%2520size.%2520To%2520support%2520future%2520research%252C%2520we%2520release%2520our%2520base%2520and%2520instruction-tuned%2520models%252C%2520our%2520multilingual%2520web%2520pretraining%2520data%2520and%2520updated%2520EuroBlocks%2520instruction%2520datasets%252C%2520as%2520well%2520as%2520our%2520pre-training%2520and%2520evaluation%2520codebases.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EuroLLM-22B%3A%20Technical%20Report&entry.906535625=Miguel%20Moura%20Ramos%20and%20Duarte%20M.%20Alves%20and%20Hippolyte%20Gisserot-Boukhlef%20and%20Jo%C3%A3o%20Alves%20and%20Pedro%20Henrique%20Martins%20and%20Patrick%20Fernandes%20and%20Jos%C3%A9%20Pombal%20and%20Nuno%20M.%20Guerreiro%20and%20Ricardo%20Rei%20and%20Nicolas%20Boizard%20and%20Amin%20Farajian%20and%20Mateusz%20Klimaszewski%20and%20Jos%C3%A9%20G.%20C.%20de%20Souza%20and%20Barry%20Haddow%20and%20Fran%C3%A7ois%20Yvon%20and%20Pierre%20Colombo%20and%20Alexandra%20Birch%20and%20Andr%C3%A9%20F.%20T.%20Martins&entry.1292438233=This%20report%20presents%20EuroLLM-22B%2C%20a%20large%20language%20model%20trained%20from%20scratch%20to%20support%20the%20needs%20of%20European%20citizens%20by%20covering%20all%2024%20official%20European%20Union%20languages%20and%2011%20additional%20languages.%20EuroLLM%20addresses%20the%20issue%20of%20European%20languages%20being%20underrepresented%20and%20underserved%20in%20existing%20open%20large%20language%20models.%20We%20provide%20a%20comprehensive%20overview%20of%20EuroLLM-22B%27s%20development%2C%20including%20tokenizer%20design%2C%20architectural%20specifications%2C%20data%20filtering%2C%20and%20training%20procedures.%20Across%20a%20broad%20set%20of%20multilingual%20benchmarks%2C%20EuroLLM-22B%20demonstrates%20strong%20performance%20in%20reasoning%2C%20instruction%20following%2C%20and%20translation%2C%20achieving%20results%20competitive%20with%20models%20of%20comparable%20size.%20To%20support%20future%20research%2C%20we%20release%20our%20base%20and%20instruction-tuned%20models%2C%20our%20multilingual%20web%20pretraining%20data%20and%20updated%20EuroBlocks%20instruction%20datasets%2C%20as%20well%20as%20our%20pre-training%20and%20evaluation%20codebases.&entry.1838667208=http%3A//arxiv.org/abs/2602.05879v1&entry.124074799=Read"},
{"title": "Visual Implicit Geometry Transformer for Autonomous Driving", "author": "Arsenii Shirokov and Mikhail Kuznetsov and Danila Stepochkin and Egor Evdokimov and Daniil Glazkov and Nikolay Patakin and Anton Konushin and Dmitry Senushkin", "abstract": "We introduce the Visual Implicit Geometry Transformer (ViGT), an autonomous driving geometric model that estimates continuous 3D occupancy fields from surround-view camera rigs. ViGT represents a step towards foundational geometric models for autonomous driving, prioritizing scalability, architectural simplicity, and generalization across diverse sensor configurations. Our approach achieves this through a calibration-free architecture, enabling a single model to adapt to different sensor setups. Unlike general-purpose geometric foundational models that focus on pixel-aligned predictions, ViGT estimates a continuous 3D occupancy field in a birds-eye-view (BEV) addressing domain-specific requirements. ViGT naturally infers geometry from multiple camera views into a single metric coordinate frame, providing a common representation for multiple geometric tasks. Unlike most existing occupancy models, we adopt a self-supervised training procedure that leverages synchronized image-LiDAR pairs, eliminating the need for costly manual annotations. We validate the scalability and generalizability of our approach by training our model on a mixture of five large-scale autonomous driving datasets (NuScenes, Waymo, NuPlan, ONCE, and Argoverse) and achieving state-of-the-art performance on the pointmap estimation task, with the best average rank across all evaluated baselines. We further evaluate ViGT on the Occ3D-nuScenes benchmark, where ViGT achieves comparable performance with supervised methods. The source code is publicly available at \\href{https://github.com/whesense/ViGT}{https://github.com/whesense/ViGT}.", "link": "http://arxiv.org/abs/2602.05573v1", "date": "2026-02-05", "relevancy": 2.3555, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5985}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5925}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Implicit%20Geometry%20Transformer%20for%20Autonomous%20Driving&body=Title%3A%20Visual%20Implicit%20Geometry%20Transformer%20for%20Autonomous%20Driving%0AAuthor%3A%20Arsenii%20Shirokov%20and%20Mikhail%20Kuznetsov%20and%20Danila%20Stepochkin%20and%20Egor%20Evdokimov%20and%20Daniil%20Glazkov%20and%20Nikolay%20Patakin%20and%20Anton%20Konushin%20and%20Dmitry%20Senushkin%0AAbstract%3A%20We%20introduce%20the%20Visual%20Implicit%20Geometry%20Transformer%20%28ViGT%29%2C%20an%20autonomous%20driving%20geometric%20model%20that%20estimates%20continuous%203D%20occupancy%20fields%20from%20surround-view%20camera%20rigs.%20ViGT%20represents%20a%20step%20towards%20foundational%20geometric%20models%20for%20autonomous%20driving%2C%20prioritizing%20scalability%2C%20architectural%20simplicity%2C%20and%20generalization%20across%20diverse%20sensor%20configurations.%20Our%20approach%20achieves%20this%20through%20a%20calibration-free%20architecture%2C%20enabling%20a%20single%20model%20to%20adapt%20to%20different%20sensor%20setups.%20Unlike%20general-purpose%20geometric%20foundational%20models%20that%20focus%20on%20pixel-aligned%20predictions%2C%20ViGT%20estimates%20a%20continuous%203D%20occupancy%20field%20in%20a%20birds-eye-view%20%28BEV%29%20addressing%20domain-specific%20requirements.%20ViGT%20naturally%20infers%20geometry%20from%20multiple%20camera%20views%20into%20a%20single%20metric%20coordinate%20frame%2C%20providing%20a%20common%20representation%20for%20multiple%20geometric%20tasks.%20Unlike%20most%20existing%20occupancy%20models%2C%20we%20adopt%20a%20self-supervised%20training%20procedure%20that%20leverages%20synchronized%20image-LiDAR%20pairs%2C%20eliminating%20the%20need%20for%20costly%20manual%20annotations.%20We%20validate%20the%20scalability%20and%20generalizability%20of%20our%20approach%20by%20training%20our%20model%20on%20a%20mixture%20of%20five%20large-scale%20autonomous%20driving%20datasets%20%28NuScenes%2C%20Waymo%2C%20NuPlan%2C%20ONCE%2C%20and%20Argoverse%29%20and%20achieving%20state-of-the-art%20performance%20on%20the%20pointmap%20estimation%20task%2C%20with%20the%20best%20average%20rank%20across%20all%20evaluated%20baselines.%20We%20further%20evaluate%20ViGT%20on%20the%20Occ3D-nuScenes%20benchmark%2C%20where%20ViGT%20achieves%20comparable%20performance%20with%20supervised%20methods.%20The%20source%20code%20is%20publicly%20available%20at%20%5Chref%7Bhttps%3A//github.com/whesense/ViGT%7D%7Bhttps%3A//github.com/whesense/ViGT%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Implicit%2520Geometry%2520Transformer%2520for%2520Autonomous%2520Driving%26entry.906535625%3DArsenii%2520Shirokov%2520and%2520Mikhail%2520Kuznetsov%2520and%2520Danila%2520Stepochkin%2520and%2520Egor%2520Evdokimov%2520and%2520Daniil%2520Glazkov%2520and%2520Nikolay%2520Patakin%2520and%2520Anton%2520Konushin%2520and%2520Dmitry%2520Senushkin%26entry.1292438233%3DWe%2520introduce%2520the%2520Visual%2520Implicit%2520Geometry%2520Transformer%2520%2528ViGT%2529%252C%2520an%2520autonomous%2520driving%2520geometric%2520model%2520that%2520estimates%2520continuous%25203D%2520occupancy%2520fields%2520from%2520surround-view%2520camera%2520rigs.%2520ViGT%2520represents%2520a%2520step%2520towards%2520foundational%2520geometric%2520models%2520for%2520autonomous%2520driving%252C%2520prioritizing%2520scalability%252C%2520architectural%2520simplicity%252C%2520and%2520generalization%2520across%2520diverse%2520sensor%2520configurations.%2520Our%2520approach%2520achieves%2520this%2520through%2520a%2520calibration-free%2520architecture%252C%2520enabling%2520a%2520single%2520model%2520to%2520adapt%2520to%2520different%2520sensor%2520setups.%2520Unlike%2520general-purpose%2520geometric%2520foundational%2520models%2520that%2520focus%2520on%2520pixel-aligned%2520predictions%252C%2520ViGT%2520estimates%2520a%2520continuous%25203D%2520occupancy%2520field%2520in%2520a%2520birds-eye-view%2520%2528BEV%2529%2520addressing%2520domain-specific%2520requirements.%2520ViGT%2520naturally%2520infers%2520geometry%2520from%2520multiple%2520camera%2520views%2520into%2520a%2520single%2520metric%2520coordinate%2520frame%252C%2520providing%2520a%2520common%2520representation%2520for%2520multiple%2520geometric%2520tasks.%2520Unlike%2520most%2520existing%2520occupancy%2520models%252C%2520we%2520adopt%2520a%2520self-supervised%2520training%2520procedure%2520that%2520leverages%2520synchronized%2520image-LiDAR%2520pairs%252C%2520eliminating%2520the%2520need%2520for%2520costly%2520manual%2520annotations.%2520We%2520validate%2520the%2520scalability%2520and%2520generalizability%2520of%2520our%2520approach%2520by%2520training%2520our%2520model%2520on%2520a%2520mixture%2520of%2520five%2520large-scale%2520autonomous%2520driving%2520datasets%2520%2528NuScenes%252C%2520Waymo%252C%2520NuPlan%252C%2520ONCE%252C%2520and%2520Argoverse%2529%2520and%2520achieving%2520state-of-the-art%2520performance%2520on%2520the%2520pointmap%2520estimation%2520task%252C%2520with%2520the%2520best%2520average%2520rank%2520across%2520all%2520evaluated%2520baselines.%2520We%2520further%2520evaluate%2520ViGT%2520on%2520the%2520Occ3D-nuScenes%2520benchmark%252C%2520where%2520ViGT%2520achieves%2520comparable%2520performance%2520with%2520supervised%2520methods.%2520The%2520source%2520code%2520is%2520publicly%2520available%2520at%2520%255Chref%257Bhttps%253A//github.com/whesense/ViGT%257D%257Bhttps%253A//github.com/whesense/ViGT%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Implicit%20Geometry%20Transformer%20for%20Autonomous%20Driving&entry.906535625=Arsenii%20Shirokov%20and%20Mikhail%20Kuznetsov%20and%20Danila%20Stepochkin%20and%20Egor%20Evdokimov%20and%20Daniil%20Glazkov%20and%20Nikolay%20Patakin%20and%20Anton%20Konushin%20and%20Dmitry%20Senushkin&entry.1292438233=We%20introduce%20the%20Visual%20Implicit%20Geometry%20Transformer%20%28ViGT%29%2C%20an%20autonomous%20driving%20geometric%20model%20that%20estimates%20continuous%203D%20occupancy%20fields%20from%20surround-view%20camera%20rigs.%20ViGT%20represents%20a%20step%20towards%20foundational%20geometric%20models%20for%20autonomous%20driving%2C%20prioritizing%20scalability%2C%20architectural%20simplicity%2C%20and%20generalization%20across%20diverse%20sensor%20configurations.%20Our%20approach%20achieves%20this%20through%20a%20calibration-free%20architecture%2C%20enabling%20a%20single%20model%20to%20adapt%20to%20different%20sensor%20setups.%20Unlike%20general-purpose%20geometric%20foundational%20models%20that%20focus%20on%20pixel-aligned%20predictions%2C%20ViGT%20estimates%20a%20continuous%203D%20occupancy%20field%20in%20a%20birds-eye-view%20%28BEV%29%20addressing%20domain-specific%20requirements.%20ViGT%20naturally%20infers%20geometry%20from%20multiple%20camera%20views%20into%20a%20single%20metric%20coordinate%20frame%2C%20providing%20a%20common%20representation%20for%20multiple%20geometric%20tasks.%20Unlike%20most%20existing%20occupancy%20models%2C%20we%20adopt%20a%20self-supervised%20training%20procedure%20that%20leverages%20synchronized%20image-LiDAR%20pairs%2C%20eliminating%20the%20need%20for%20costly%20manual%20annotations.%20We%20validate%20the%20scalability%20and%20generalizability%20of%20our%20approach%20by%20training%20our%20model%20on%20a%20mixture%20of%20five%20large-scale%20autonomous%20driving%20datasets%20%28NuScenes%2C%20Waymo%2C%20NuPlan%2C%20ONCE%2C%20and%20Argoverse%29%20and%20achieving%20state-of-the-art%20performance%20on%20the%20pointmap%20estimation%20task%2C%20with%20the%20best%20average%20rank%20across%20all%20evaluated%20baselines.%20We%20further%20evaluate%20ViGT%20on%20the%20Occ3D-nuScenes%20benchmark%2C%20where%20ViGT%20achieves%20comparable%20performance%20with%20supervised%20methods.%20The%20source%20code%20is%20publicly%20available%20at%20%5Chref%7Bhttps%3A//github.com/whesense/ViGT%7D%7Bhttps%3A//github.com/whesense/ViGT%7D.&entry.1838667208=http%3A//arxiv.org/abs/2602.05573v1&entry.124074799=Read"},
{"title": "Dimensionality Reduction on Riemannian Manifolds in Data Analysis", "author": "Alaa El Ichi and Khalide Jbilou", "abstract": "In this work, we investigate Riemannian geometry based dimensionality reduction methods that respect the underlying manifold structure of the data. In particular, we focus on Principal Geodesic Analysis (PGA) as a nonlinear generalization of PCA for manifold valued data, and extend discriminant analysis through Riemannian adaptations of other known dimensionality reduction methods. These approaches exploit geodesic distances, tangent space representations, and intrinsic statistical measures to achieve more faithful low dimensional embeddings. We also discuss related manifold learning techniques and highlight their theoretical foundations and practical advantages. Experimental results on representative datasets demonstrate that Riemannian methods provide improved representation quality and classification performance compared to their Euclidean counterparts, especially for data constrained to curved spaces such as hyperspheres and symmetric positive definite manifolds. This study underscores the importance of geometry aware dimensionality reduction in modern machine learning and data science applications.", "link": "http://arxiv.org/abs/2602.05936v1", "date": "2026-02-05", "relevancy": 2.3392, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.472}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4711}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dimensionality%20Reduction%20on%20Riemannian%20Manifolds%20in%20Data%20Analysis&body=Title%3A%20Dimensionality%20Reduction%20on%20Riemannian%20Manifolds%20in%20Data%20Analysis%0AAuthor%3A%20Alaa%20El%20Ichi%20and%20Khalide%20Jbilou%0AAbstract%3A%20In%20this%20work%2C%20we%20investigate%20Riemannian%20geometry%20based%20dimensionality%20reduction%20methods%20that%20respect%20the%20underlying%20manifold%20structure%20of%20the%20data.%20In%20particular%2C%20we%20focus%20on%20Principal%20Geodesic%20Analysis%20%28PGA%29%20as%20a%20nonlinear%20generalization%20of%20PCA%20for%20manifold%20valued%20data%2C%20and%20extend%20discriminant%20analysis%20through%20Riemannian%20adaptations%20of%20other%20known%20dimensionality%20reduction%20methods.%20These%20approaches%20exploit%20geodesic%20distances%2C%20tangent%20space%20representations%2C%20and%20intrinsic%20statistical%20measures%20to%20achieve%20more%20faithful%20low%20dimensional%20embeddings.%20We%20also%20discuss%20related%20manifold%20learning%20techniques%20and%20highlight%20their%20theoretical%20foundations%20and%20practical%20advantages.%20Experimental%20results%20on%20representative%20datasets%20demonstrate%20that%20Riemannian%20methods%20provide%20improved%20representation%20quality%20and%20classification%20performance%20compared%20to%20their%20Euclidean%20counterparts%2C%20especially%20for%20data%20constrained%20to%20curved%20spaces%20such%20as%20hyperspheres%20and%20symmetric%20positive%20definite%20manifolds.%20This%20study%20underscores%20the%20importance%20of%20geometry%20aware%20dimensionality%20reduction%20in%20modern%20machine%20learning%20and%20data%20science%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDimensionality%2520Reduction%2520on%2520Riemannian%2520Manifolds%2520in%2520Data%2520Analysis%26entry.906535625%3DAlaa%2520El%2520Ichi%2520and%2520Khalide%2520Jbilou%26entry.1292438233%3DIn%2520this%2520work%252C%2520we%2520investigate%2520Riemannian%2520geometry%2520based%2520dimensionality%2520reduction%2520methods%2520that%2520respect%2520the%2520underlying%2520manifold%2520structure%2520of%2520the%2520data.%2520In%2520particular%252C%2520we%2520focus%2520on%2520Principal%2520Geodesic%2520Analysis%2520%2528PGA%2529%2520as%2520a%2520nonlinear%2520generalization%2520of%2520PCA%2520for%2520manifold%2520valued%2520data%252C%2520and%2520extend%2520discriminant%2520analysis%2520through%2520Riemannian%2520adaptations%2520of%2520other%2520known%2520dimensionality%2520reduction%2520methods.%2520These%2520approaches%2520exploit%2520geodesic%2520distances%252C%2520tangent%2520space%2520representations%252C%2520and%2520intrinsic%2520statistical%2520measures%2520to%2520achieve%2520more%2520faithful%2520low%2520dimensional%2520embeddings.%2520We%2520also%2520discuss%2520related%2520manifold%2520learning%2520techniques%2520and%2520highlight%2520their%2520theoretical%2520foundations%2520and%2520practical%2520advantages.%2520Experimental%2520results%2520on%2520representative%2520datasets%2520demonstrate%2520that%2520Riemannian%2520methods%2520provide%2520improved%2520representation%2520quality%2520and%2520classification%2520performance%2520compared%2520to%2520their%2520Euclidean%2520counterparts%252C%2520especially%2520for%2520data%2520constrained%2520to%2520curved%2520spaces%2520such%2520as%2520hyperspheres%2520and%2520symmetric%2520positive%2520definite%2520manifolds.%2520This%2520study%2520underscores%2520the%2520importance%2520of%2520geometry%2520aware%2520dimensionality%2520reduction%2520in%2520modern%2520machine%2520learning%2520and%2520data%2520science%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dimensionality%20Reduction%20on%20Riemannian%20Manifolds%20in%20Data%20Analysis&entry.906535625=Alaa%20El%20Ichi%20and%20Khalide%20Jbilou&entry.1292438233=In%20this%20work%2C%20we%20investigate%20Riemannian%20geometry%20based%20dimensionality%20reduction%20methods%20that%20respect%20the%20underlying%20manifold%20structure%20of%20the%20data.%20In%20particular%2C%20we%20focus%20on%20Principal%20Geodesic%20Analysis%20%28PGA%29%20as%20a%20nonlinear%20generalization%20of%20PCA%20for%20manifold%20valued%20data%2C%20and%20extend%20discriminant%20analysis%20through%20Riemannian%20adaptations%20of%20other%20known%20dimensionality%20reduction%20methods.%20These%20approaches%20exploit%20geodesic%20distances%2C%20tangent%20space%20representations%2C%20and%20intrinsic%20statistical%20measures%20to%20achieve%20more%20faithful%20low%20dimensional%20embeddings.%20We%20also%20discuss%20related%20manifold%20learning%20techniques%20and%20highlight%20their%20theoretical%20foundations%20and%20practical%20advantages.%20Experimental%20results%20on%20representative%20datasets%20demonstrate%20that%20Riemannian%20methods%20provide%20improved%20representation%20quality%20and%20classification%20performance%20compared%20to%20their%20Euclidean%20counterparts%2C%20especially%20for%20data%20constrained%20to%20curved%20spaces%20such%20as%20hyperspheres%20and%20symmetric%20positive%20definite%20manifolds.%20This%20study%20underscores%20the%20importance%20of%20geometry%20aware%20dimensionality%20reduction%20in%20modern%20machine%20learning%20and%20data%20science%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2602.05936v1&entry.124074799=Read"},
{"title": "Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation", "author": "Hai Zhang and Siqi Liang and Li Chen and Yuxian Li and Yukuan Xu and Yichao Zhong and Fu Zhang and Hongyang Li", "abstract": "Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes.", "link": "http://arxiv.org/abs/2602.05827v1", "date": "2026-02-05", "relevancy": 2.3373, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6347}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5789}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Video%20Generation%20Propels%20Real-World%20Beyond-the-View%20Vision-Language%20Navigation&body=Title%3A%20Sparse%20Video%20Generation%20Propels%20Real-World%20Beyond-the-View%20Vision-Language%20Navigation%0AAuthor%3A%20Hai%20Zhang%20and%20Siqi%20Liang%20and%20Li%20Chen%20and%20Yuxian%20Li%20and%20Yukuan%20Xu%20and%20Yichao%20Zhong%20and%20Fu%20Zhang%20and%20Hongyang%20Li%0AAbstract%3A%20Why%20must%20vision-language%20navigation%20be%20bound%20to%20detailed%20and%20verbose%20language%20instructions%3F%20While%20such%20details%20ease%20decision-making%2C%20they%20fundamentally%20contradict%20the%20goal%20for%20navigation%20in%20the%20real-world.%20Ideally%2C%20agents%20should%20possess%20the%20autonomy%20to%20navigate%20in%20unknown%20environments%20guided%20solely%20by%20simple%20and%20high-level%20intents.%20Realizing%20this%20ambition%20introduces%20a%20formidable%20challenge%3A%20Beyond-the-View%20Navigation%20%28BVN%29%2C%20where%20agents%20must%20locate%20distant%2C%20unseen%20targets%20without%20dense%20and%20step-by-step%20guidance.%20Existing%20large%20language%20model%20%28LLM%29-based%20methods%2C%20though%20adept%20at%20following%20dense%20instructions%2C%20often%20suffer%20from%20short-sighted%20behaviors%20due%20to%20their%20reliance%20on%20short-horimzon%20supervision.%20Simply%20extending%20the%20supervision%20horizon%2C%20however%2C%20destabilizes%20LLM%20training.%20In%20this%20work%2C%20we%20identify%20that%20video%20generation%20models%20inherently%20benefit%20from%20long-horizon%20supervision%20to%20align%20with%20language%20instructions%2C%20rendering%20them%20uniquely%20suitable%20for%20BVN%20tasks.%20Capitalizing%20on%20this%20insight%2C%20we%20propose%20introducing%20the%20video%20generation%20model%20into%20this%20field%20for%20the%20first%20time.%20Yet%2C%20the%20prohibitive%20latency%20for%20generating%20videos%20spanning%20tens%20of%20seconds%20makes%20real-world%20deployment%20impractical.%20To%20bridge%20this%20gap%2C%20we%20propose%20SparseVideoNav%2C%20achieving%20sub-second%20trajectory%20inference%20guided%20by%20a%20generated%20sparse%20future%20spanning%20a%2020-second%20horizon.%20This%20yields%20a%20remarkable%2027x%20speed-up%20compared%20to%20the%20unoptimized%20counterpart.%20Extensive%20real-world%20zero-shot%20experiments%20demonstrate%20that%20SparseVideoNav%20achieves%202.5x%20the%20success%20rate%20of%20state-of-the-art%20LLM%20baselines%20on%20BVN%20tasks%20and%20marks%20the%20first%20realization%20of%20such%20capability%20in%20challenging%20night%20scenes.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Video%2520Generation%2520Propels%2520Real-World%2520Beyond-the-View%2520Vision-Language%2520Navigation%26entry.906535625%3DHai%2520Zhang%2520and%2520Siqi%2520Liang%2520and%2520Li%2520Chen%2520and%2520Yuxian%2520Li%2520and%2520Yukuan%2520Xu%2520and%2520Yichao%2520Zhong%2520and%2520Fu%2520Zhang%2520and%2520Hongyang%2520Li%26entry.1292438233%3DWhy%2520must%2520vision-language%2520navigation%2520be%2520bound%2520to%2520detailed%2520and%2520verbose%2520language%2520instructions%253F%2520While%2520such%2520details%2520ease%2520decision-making%252C%2520they%2520fundamentally%2520contradict%2520the%2520goal%2520for%2520navigation%2520in%2520the%2520real-world.%2520Ideally%252C%2520agents%2520should%2520possess%2520the%2520autonomy%2520to%2520navigate%2520in%2520unknown%2520environments%2520guided%2520solely%2520by%2520simple%2520and%2520high-level%2520intents.%2520Realizing%2520this%2520ambition%2520introduces%2520a%2520formidable%2520challenge%253A%2520Beyond-the-View%2520Navigation%2520%2528BVN%2529%252C%2520where%2520agents%2520must%2520locate%2520distant%252C%2520unseen%2520targets%2520without%2520dense%2520and%2520step-by-step%2520guidance.%2520Existing%2520large%2520language%2520model%2520%2528LLM%2529-based%2520methods%252C%2520though%2520adept%2520at%2520following%2520dense%2520instructions%252C%2520often%2520suffer%2520from%2520short-sighted%2520behaviors%2520due%2520to%2520their%2520reliance%2520on%2520short-horimzon%2520supervision.%2520Simply%2520extending%2520the%2520supervision%2520horizon%252C%2520however%252C%2520destabilizes%2520LLM%2520training.%2520In%2520this%2520work%252C%2520we%2520identify%2520that%2520video%2520generation%2520models%2520inherently%2520benefit%2520from%2520long-horizon%2520supervision%2520to%2520align%2520with%2520language%2520instructions%252C%2520rendering%2520them%2520uniquely%2520suitable%2520for%2520BVN%2520tasks.%2520Capitalizing%2520on%2520this%2520insight%252C%2520we%2520propose%2520introducing%2520the%2520video%2520generation%2520model%2520into%2520this%2520field%2520for%2520the%2520first%2520time.%2520Yet%252C%2520the%2520prohibitive%2520latency%2520for%2520generating%2520videos%2520spanning%2520tens%2520of%2520seconds%2520makes%2520real-world%2520deployment%2520impractical.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520SparseVideoNav%252C%2520achieving%2520sub-second%2520trajectory%2520inference%2520guided%2520by%2520a%2520generated%2520sparse%2520future%2520spanning%2520a%252020-second%2520horizon.%2520This%2520yields%2520a%2520remarkable%252027x%2520speed-up%2520compared%2520to%2520the%2520unoptimized%2520counterpart.%2520Extensive%2520real-world%2520zero-shot%2520experiments%2520demonstrate%2520that%2520SparseVideoNav%2520achieves%25202.5x%2520the%2520success%2520rate%2520of%2520state-of-the-art%2520LLM%2520baselines%2520on%2520BVN%2520tasks%2520and%2520marks%2520the%2520first%2520realization%2520of%2520such%2520capability%2520in%2520challenging%2520night%2520scenes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Video%20Generation%20Propels%20Real-World%20Beyond-the-View%20Vision-Language%20Navigation&entry.906535625=Hai%20Zhang%20and%20Siqi%20Liang%20and%20Li%20Chen%20and%20Yuxian%20Li%20and%20Yukuan%20Xu%20and%20Yichao%20Zhong%20and%20Fu%20Zhang%20and%20Hongyang%20Li&entry.1292438233=Why%20must%20vision-language%20navigation%20be%20bound%20to%20detailed%20and%20verbose%20language%20instructions%3F%20While%20such%20details%20ease%20decision-making%2C%20they%20fundamentally%20contradict%20the%20goal%20for%20navigation%20in%20the%20real-world.%20Ideally%2C%20agents%20should%20possess%20the%20autonomy%20to%20navigate%20in%20unknown%20environments%20guided%20solely%20by%20simple%20and%20high-level%20intents.%20Realizing%20this%20ambition%20introduces%20a%20formidable%20challenge%3A%20Beyond-the-View%20Navigation%20%28BVN%29%2C%20where%20agents%20must%20locate%20distant%2C%20unseen%20targets%20without%20dense%20and%20step-by-step%20guidance.%20Existing%20large%20language%20model%20%28LLM%29-based%20methods%2C%20though%20adept%20at%20following%20dense%20instructions%2C%20often%20suffer%20from%20short-sighted%20behaviors%20due%20to%20their%20reliance%20on%20short-horimzon%20supervision.%20Simply%20extending%20the%20supervision%20horizon%2C%20however%2C%20destabilizes%20LLM%20training.%20In%20this%20work%2C%20we%20identify%20that%20video%20generation%20models%20inherently%20benefit%20from%20long-horizon%20supervision%20to%20align%20with%20language%20instructions%2C%20rendering%20them%20uniquely%20suitable%20for%20BVN%20tasks.%20Capitalizing%20on%20this%20insight%2C%20we%20propose%20introducing%20the%20video%20generation%20model%20into%20this%20field%20for%20the%20first%20time.%20Yet%2C%20the%20prohibitive%20latency%20for%20generating%20videos%20spanning%20tens%20of%20seconds%20makes%20real-world%20deployment%20impractical.%20To%20bridge%20this%20gap%2C%20we%20propose%20SparseVideoNav%2C%20achieving%20sub-second%20trajectory%20inference%20guided%20by%20a%20generated%20sparse%20future%20spanning%20a%2020-second%20horizon.%20This%20yields%20a%20remarkable%2027x%20speed-up%20compared%20to%20the%20unoptimized%20counterpart.%20Extensive%20real-world%20zero-shot%20experiments%20demonstrate%20that%20SparseVideoNav%20achieves%202.5x%20the%20success%20rate%20of%20state-of-the-art%20LLM%20baselines%20on%20BVN%20tasks%20and%20marks%20the%20first%20realization%20of%20such%20capability%20in%20challenging%20night%20scenes.&entry.1838667208=http%3A//arxiv.org/abs/2602.05827v1&entry.124074799=Read"},
{"title": "Cross-Domain Offline Policy Adaptation via Selective Transition Correction", "author": "Mengbei Yan and Jiafei Lyu and Shengjie Sun and Zhongjian Qiao and Jingwen Yang and Zichuan Lin and Deheng Ye and Xiu Li", "abstract": "It remains a critical challenge to adapt policies across domains with mismatched dynamics in reinforcement learning (RL). In this paper, we study cross-domain offline RL, where an offline dataset from another similar source domain can be accessed to enhance policy learning upon a target domain dataset. Directly merging the two datasets may lead to suboptimal performance due to potential dynamics mismatches. Existing approaches typically mitigate this issue through source domain transition filtering or reward modification, which, however, may lead to insufficient exploitation of the valuable source domain data. Instead, we propose to modify the source domain data into the target domain data. To that end, we leverage an inverse policy model and a reward model to correct the actions and rewards of source transitions, explicitly achieving alignment with the target dynamics. Since limited data may result in inaccurate model training, we further employ a forward dynamics model to retain corrected samples that better match the target dynamics than the original transitions. Consequently, we propose the Selective Transition Correction (STC) algorithm, which enables reliable usage of source domain data for policy adaptation. Experiments on various environments with dynamics shifts demonstrate that STC achieves superior performance against existing baselines.", "link": "http://arxiv.org/abs/2602.05776v1", "date": "2026-02-05", "relevancy": 2.3298, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.471}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4689}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Domain%20Offline%20Policy%20Adaptation%20via%20Selective%20Transition%20Correction&body=Title%3A%20Cross-Domain%20Offline%20Policy%20Adaptation%20via%20Selective%20Transition%20Correction%0AAuthor%3A%20Mengbei%20Yan%20and%20Jiafei%20Lyu%20and%20Shengjie%20Sun%20and%20Zhongjian%20Qiao%20and%20Jingwen%20Yang%20and%20Zichuan%20Lin%20and%20Deheng%20Ye%20and%20Xiu%20Li%0AAbstract%3A%20It%20remains%20a%20critical%20challenge%20to%20adapt%20policies%20across%20domains%20with%20mismatched%20dynamics%20in%20reinforcement%20learning%20%28RL%29.%20In%20this%20paper%2C%20we%20study%20cross-domain%20offline%20RL%2C%20where%20an%20offline%20dataset%20from%20another%20similar%20source%20domain%20can%20be%20accessed%20to%20enhance%20policy%20learning%20upon%20a%20target%20domain%20dataset.%20Directly%20merging%20the%20two%20datasets%20may%20lead%20to%20suboptimal%20performance%20due%20to%20potential%20dynamics%20mismatches.%20Existing%20approaches%20typically%20mitigate%20this%20issue%20through%20source%20domain%20transition%20filtering%20or%20reward%20modification%2C%20which%2C%20however%2C%20may%20lead%20to%20insufficient%20exploitation%20of%20the%20valuable%20source%20domain%20data.%20Instead%2C%20we%20propose%20to%20modify%20the%20source%20domain%20data%20into%20the%20target%20domain%20data.%20To%20that%20end%2C%20we%20leverage%20an%20inverse%20policy%20model%20and%20a%20reward%20model%20to%20correct%20the%20actions%20and%20rewards%20of%20source%20transitions%2C%20explicitly%20achieving%20alignment%20with%20the%20target%20dynamics.%20Since%20limited%20data%20may%20result%20in%20inaccurate%20model%20training%2C%20we%20further%20employ%20a%20forward%20dynamics%20model%20to%20retain%20corrected%20samples%20that%20better%20match%20the%20target%20dynamics%20than%20the%20original%20transitions.%20Consequently%2C%20we%20propose%20the%20Selective%20Transition%20Correction%20%28STC%29%20algorithm%2C%20which%20enables%20reliable%20usage%20of%20source%20domain%20data%20for%20policy%20adaptation.%20Experiments%20on%20various%20environments%20with%20dynamics%20shifts%20demonstrate%20that%20STC%20achieves%20superior%20performance%20against%20existing%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Domain%2520Offline%2520Policy%2520Adaptation%2520via%2520Selective%2520Transition%2520Correction%26entry.906535625%3DMengbei%2520Yan%2520and%2520Jiafei%2520Lyu%2520and%2520Shengjie%2520Sun%2520and%2520Zhongjian%2520Qiao%2520and%2520Jingwen%2520Yang%2520and%2520Zichuan%2520Lin%2520and%2520Deheng%2520Ye%2520and%2520Xiu%2520Li%26entry.1292438233%3DIt%2520remains%2520a%2520critical%2520challenge%2520to%2520adapt%2520policies%2520across%2520domains%2520with%2520mismatched%2520dynamics%2520in%2520reinforcement%2520learning%2520%2528RL%2529.%2520In%2520this%2520paper%252C%2520we%2520study%2520cross-domain%2520offline%2520RL%252C%2520where%2520an%2520offline%2520dataset%2520from%2520another%2520similar%2520source%2520domain%2520can%2520be%2520accessed%2520to%2520enhance%2520policy%2520learning%2520upon%2520a%2520target%2520domain%2520dataset.%2520Directly%2520merging%2520the%2520two%2520datasets%2520may%2520lead%2520to%2520suboptimal%2520performance%2520due%2520to%2520potential%2520dynamics%2520mismatches.%2520Existing%2520approaches%2520typically%2520mitigate%2520this%2520issue%2520through%2520source%2520domain%2520transition%2520filtering%2520or%2520reward%2520modification%252C%2520which%252C%2520however%252C%2520may%2520lead%2520to%2520insufficient%2520exploitation%2520of%2520the%2520valuable%2520source%2520domain%2520data.%2520Instead%252C%2520we%2520propose%2520to%2520modify%2520the%2520source%2520domain%2520data%2520into%2520the%2520target%2520domain%2520data.%2520To%2520that%2520end%252C%2520we%2520leverage%2520an%2520inverse%2520policy%2520model%2520and%2520a%2520reward%2520model%2520to%2520correct%2520the%2520actions%2520and%2520rewards%2520of%2520source%2520transitions%252C%2520explicitly%2520achieving%2520alignment%2520with%2520the%2520target%2520dynamics.%2520Since%2520limited%2520data%2520may%2520result%2520in%2520inaccurate%2520model%2520training%252C%2520we%2520further%2520employ%2520a%2520forward%2520dynamics%2520model%2520to%2520retain%2520corrected%2520samples%2520that%2520better%2520match%2520the%2520target%2520dynamics%2520than%2520the%2520original%2520transitions.%2520Consequently%252C%2520we%2520propose%2520the%2520Selective%2520Transition%2520Correction%2520%2528STC%2529%2520algorithm%252C%2520which%2520enables%2520reliable%2520usage%2520of%2520source%2520domain%2520data%2520for%2520policy%2520adaptation.%2520Experiments%2520on%2520various%2520environments%2520with%2520dynamics%2520shifts%2520demonstrate%2520that%2520STC%2520achieves%2520superior%2520performance%2520against%2520existing%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Domain%20Offline%20Policy%20Adaptation%20via%20Selective%20Transition%20Correction&entry.906535625=Mengbei%20Yan%20and%20Jiafei%20Lyu%20and%20Shengjie%20Sun%20and%20Zhongjian%20Qiao%20and%20Jingwen%20Yang%20and%20Zichuan%20Lin%20and%20Deheng%20Ye%20and%20Xiu%20Li&entry.1292438233=It%20remains%20a%20critical%20challenge%20to%20adapt%20policies%20across%20domains%20with%20mismatched%20dynamics%20in%20reinforcement%20learning%20%28RL%29.%20In%20this%20paper%2C%20we%20study%20cross-domain%20offline%20RL%2C%20where%20an%20offline%20dataset%20from%20another%20similar%20source%20domain%20can%20be%20accessed%20to%20enhance%20policy%20learning%20upon%20a%20target%20domain%20dataset.%20Directly%20merging%20the%20two%20datasets%20may%20lead%20to%20suboptimal%20performance%20due%20to%20potential%20dynamics%20mismatches.%20Existing%20approaches%20typically%20mitigate%20this%20issue%20through%20source%20domain%20transition%20filtering%20or%20reward%20modification%2C%20which%2C%20however%2C%20may%20lead%20to%20insufficient%20exploitation%20of%20the%20valuable%20source%20domain%20data.%20Instead%2C%20we%20propose%20to%20modify%20the%20source%20domain%20data%20into%20the%20target%20domain%20data.%20To%20that%20end%2C%20we%20leverage%20an%20inverse%20policy%20model%20and%20a%20reward%20model%20to%20correct%20the%20actions%20and%20rewards%20of%20source%20transitions%2C%20explicitly%20achieving%20alignment%20with%20the%20target%20dynamics.%20Since%20limited%20data%20may%20result%20in%20inaccurate%20model%20training%2C%20we%20further%20employ%20a%20forward%20dynamics%20model%20to%20retain%20corrected%20samples%20that%20better%20match%20the%20target%20dynamics%20than%20the%20original%20transitions.%20Consequently%2C%20we%20propose%20the%20Selective%20Transition%20Correction%20%28STC%29%20algorithm%2C%20which%20enables%20reliable%20usage%20of%20source%20domain%20data%20for%20policy%20adaptation.%20Experiments%20on%20various%20environments%20with%20dynamics%20shifts%20demonstrate%20that%20STC%20achieves%20superior%20performance%20against%20existing%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2602.05776v1&entry.124074799=Read"},
{"title": "Scalable and General Whole-Body Control for Cross-Humanoid Locomotion", "author": "Yufei Xue and YunFeng Lin and Wentao Dong and Yang Tang and Jingbo Wang and Jiangmiao Pang and Ming Zhou and Minghuan Liu and Weinan Zhang", "abstract": "Learning-based whole-body controllers have become a key driver for humanoid robots, yet most existing approaches require robot-specific training. In this paper, we study the problem of cross-embodiment humanoid control and show that a single policy can robustly generalize across a wide range of humanoid robot designs with one-time training. We introduce XHugWBC, a novel cross-embodiment training framework that enables generalist humanoid control through: (1) physics-consistent morphological randomization, (2) semantically aligned observation and action spaces across diverse humanoid robots, and (3) effective policy architectures modeling morphological and dynamical properties. XHugWBC is not tied to any specific robot. Instead, it internalizes a broad distribution of morphological and dynamical characteristics during training. By learning motion priors from diverse randomized embodiments, the policy acquires a strong structural bias that supports zero-shot transfer to previously unseen robots. Experiments on twelve simulated humanoids and seven real-world robots demonstrate the strong generalization and robustness of the resulting universal controller.", "link": "http://arxiv.org/abs/2602.05791v1", "date": "2026-02-05", "relevancy": 2.3135, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5882}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5727}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20and%20General%20Whole-Body%20Control%20for%20Cross-Humanoid%20Locomotion&body=Title%3A%20Scalable%20and%20General%20Whole-Body%20Control%20for%20Cross-Humanoid%20Locomotion%0AAuthor%3A%20Yufei%20Xue%20and%20YunFeng%20Lin%20and%20Wentao%20Dong%20and%20Yang%20Tang%20and%20Jingbo%20Wang%20and%20Jiangmiao%20Pang%20and%20Ming%20Zhou%20and%20Minghuan%20Liu%20and%20Weinan%20Zhang%0AAbstract%3A%20Learning-based%20whole-body%20controllers%20have%20become%20a%20key%20driver%20for%20humanoid%20robots%2C%20yet%20most%20existing%20approaches%20require%20robot-specific%20training.%20In%20this%20paper%2C%20we%20study%20the%20problem%20of%20cross-embodiment%20humanoid%20control%20and%20show%20that%20a%20single%20policy%20can%20robustly%20generalize%20across%20a%20wide%20range%20of%20humanoid%20robot%20designs%20with%20one-time%20training.%20We%20introduce%20XHugWBC%2C%20a%20novel%20cross-embodiment%20training%20framework%20that%20enables%20generalist%20humanoid%20control%20through%3A%20%281%29%20physics-consistent%20morphological%20randomization%2C%20%282%29%20semantically%20aligned%20observation%20and%20action%20spaces%20across%20diverse%20humanoid%20robots%2C%20and%20%283%29%20effective%20policy%20architectures%20modeling%20morphological%20and%20dynamical%20properties.%20XHugWBC%20is%20not%20tied%20to%20any%20specific%20robot.%20Instead%2C%20it%20internalizes%20a%20broad%20distribution%20of%20morphological%20and%20dynamical%20characteristics%20during%20training.%20By%20learning%20motion%20priors%20from%20diverse%20randomized%20embodiments%2C%20the%20policy%20acquires%20a%20strong%20structural%20bias%20that%20supports%20zero-shot%20transfer%20to%20previously%20unseen%20robots.%20Experiments%20on%20twelve%20simulated%20humanoids%20and%20seven%20real-world%20robots%20demonstrate%20the%20strong%20generalization%20and%20robustness%20of%20the%20resulting%20universal%20controller.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520and%2520General%2520Whole-Body%2520Control%2520for%2520Cross-Humanoid%2520Locomotion%26entry.906535625%3DYufei%2520Xue%2520and%2520YunFeng%2520Lin%2520and%2520Wentao%2520Dong%2520and%2520Yang%2520Tang%2520and%2520Jingbo%2520Wang%2520and%2520Jiangmiao%2520Pang%2520and%2520Ming%2520Zhou%2520and%2520Minghuan%2520Liu%2520and%2520Weinan%2520Zhang%26entry.1292438233%3DLearning-based%2520whole-body%2520controllers%2520have%2520become%2520a%2520key%2520driver%2520for%2520humanoid%2520robots%252C%2520yet%2520most%2520existing%2520approaches%2520require%2520robot-specific%2520training.%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520problem%2520of%2520cross-embodiment%2520humanoid%2520control%2520and%2520show%2520that%2520a%2520single%2520policy%2520can%2520robustly%2520generalize%2520across%2520a%2520wide%2520range%2520of%2520humanoid%2520robot%2520designs%2520with%2520one-time%2520training.%2520We%2520introduce%2520XHugWBC%252C%2520a%2520novel%2520cross-embodiment%2520training%2520framework%2520that%2520enables%2520generalist%2520humanoid%2520control%2520through%253A%2520%25281%2529%2520physics-consistent%2520morphological%2520randomization%252C%2520%25282%2529%2520semantically%2520aligned%2520observation%2520and%2520action%2520spaces%2520across%2520diverse%2520humanoid%2520robots%252C%2520and%2520%25283%2529%2520effective%2520policy%2520architectures%2520modeling%2520morphological%2520and%2520dynamical%2520properties.%2520XHugWBC%2520is%2520not%2520tied%2520to%2520any%2520specific%2520robot.%2520Instead%252C%2520it%2520internalizes%2520a%2520broad%2520distribution%2520of%2520morphological%2520and%2520dynamical%2520characteristics%2520during%2520training.%2520By%2520learning%2520motion%2520priors%2520from%2520diverse%2520randomized%2520embodiments%252C%2520the%2520policy%2520acquires%2520a%2520strong%2520structural%2520bias%2520that%2520supports%2520zero-shot%2520transfer%2520to%2520previously%2520unseen%2520robots.%2520Experiments%2520on%2520twelve%2520simulated%2520humanoids%2520and%2520seven%2520real-world%2520robots%2520demonstrate%2520the%2520strong%2520generalization%2520and%2520robustness%2520of%2520the%2520resulting%2520universal%2520controller.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20and%20General%20Whole-Body%20Control%20for%20Cross-Humanoid%20Locomotion&entry.906535625=Yufei%20Xue%20and%20YunFeng%20Lin%20and%20Wentao%20Dong%20and%20Yang%20Tang%20and%20Jingbo%20Wang%20and%20Jiangmiao%20Pang%20and%20Ming%20Zhou%20and%20Minghuan%20Liu%20and%20Weinan%20Zhang&entry.1292438233=Learning-based%20whole-body%20controllers%20have%20become%20a%20key%20driver%20for%20humanoid%20robots%2C%20yet%20most%20existing%20approaches%20require%20robot-specific%20training.%20In%20this%20paper%2C%20we%20study%20the%20problem%20of%20cross-embodiment%20humanoid%20control%20and%20show%20that%20a%20single%20policy%20can%20robustly%20generalize%20across%20a%20wide%20range%20of%20humanoid%20robot%20designs%20with%20one-time%20training.%20We%20introduce%20XHugWBC%2C%20a%20novel%20cross-embodiment%20training%20framework%20that%20enables%20generalist%20humanoid%20control%20through%3A%20%281%29%20physics-consistent%20morphological%20randomization%2C%20%282%29%20semantically%20aligned%20observation%20and%20action%20spaces%20across%20diverse%20humanoid%20robots%2C%20and%20%283%29%20effective%20policy%20architectures%20modeling%20morphological%20and%20dynamical%20properties.%20XHugWBC%20is%20not%20tied%20to%20any%20specific%20robot.%20Instead%2C%20it%20internalizes%20a%20broad%20distribution%20of%20morphological%20and%20dynamical%20characteristics%20during%20training.%20By%20learning%20motion%20priors%20from%20diverse%20randomized%20embodiments%2C%20the%20policy%20acquires%20a%20strong%20structural%20bias%20that%20supports%20zero-shot%20transfer%20to%20previously%20unseen%20robots.%20Experiments%20on%20twelve%20simulated%20humanoids%20and%20seven%20real-world%20robots%20demonstrate%20the%20strong%20generalization%20and%20robustness%20of%20the%20resulting%20universal%20controller.&entry.1838667208=http%3A//arxiv.org/abs/2602.05791v1&entry.124074799=Read"},
{"title": "MAGPrompt: Message-Adaptive Graph Prompt Tuning for Graph Neural Networks", "author": "Long D. Nguyen and Binh P. Nguyen", "abstract": "Pre-trained graph neural networks (GNNs) transfer well, but adapting them to downstream tasks remains challenging due to mismatches between pre-training objectives and task requirements. Graph prompt tuning offers a parameter-efficient alternative to fine-tuning, yet most methods only modify inputs or representations and leave message passing unchanged, limiting their ability to adapt neighborhood interactions. We propose message-adaptive graph prompt tuning, which injects learnable prompts into the message passing step to reweight incoming neighbor messages and add task-specific prompt vectors during message aggregation, while keeping the backbone GNN frozen. The approach is compatible with common GNN backbones and pre-training strategies, and applicable across downstream settings. Experiments on diverse node- and graph-level datasets show consistent gains over prior graph prompting methods in few-shot settings, while achieving performance competitive with fine-tuning in full-shot regimes.", "link": "http://arxiv.org/abs/2602.05567v1", "date": "2026-02-05", "relevancy": 2.3112, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.525}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4311}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAGPrompt%3A%20Message-Adaptive%20Graph%20Prompt%20Tuning%20for%20Graph%20Neural%20Networks&body=Title%3A%20MAGPrompt%3A%20Message-Adaptive%20Graph%20Prompt%20Tuning%20for%20Graph%20Neural%20Networks%0AAuthor%3A%20Long%20D.%20Nguyen%20and%20Binh%20P.%20Nguyen%0AAbstract%3A%20Pre-trained%20graph%20neural%20networks%20%28GNNs%29%20transfer%20well%2C%20but%20adapting%20them%20to%20downstream%20tasks%20remains%20challenging%20due%20to%20mismatches%20between%20pre-training%20objectives%20and%20task%20requirements.%20Graph%20prompt%20tuning%20offers%20a%20parameter-efficient%20alternative%20to%20fine-tuning%2C%20yet%20most%20methods%20only%20modify%20inputs%20or%20representations%20and%20leave%20message%20passing%20unchanged%2C%20limiting%20their%20ability%20to%20adapt%20neighborhood%20interactions.%20We%20propose%20message-adaptive%20graph%20prompt%20tuning%2C%20which%20injects%20learnable%20prompts%20into%20the%20message%20passing%20step%20to%20reweight%20incoming%20neighbor%20messages%20and%20add%20task-specific%20prompt%20vectors%20during%20message%20aggregation%2C%20while%20keeping%20the%20backbone%20GNN%20frozen.%20The%20approach%20is%20compatible%20with%20common%20GNN%20backbones%20and%20pre-training%20strategies%2C%20and%20applicable%20across%20downstream%20settings.%20Experiments%20on%20diverse%20node-%20and%20graph-level%20datasets%20show%20consistent%20gains%20over%20prior%20graph%20prompting%20methods%20in%20few-shot%20settings%2C%20while%20achieving%20performance%20competitive%20with%20fine-tuning%20in%20full-shot%20regimes.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAGPrompt%253A%2520Message-Adaptive%2520Graph%2520Prompt%2520Tuning%2520for%2520Graph%2520Neural%2520Networks%26entry.906535625%3DLong%2520D.%2520Nguyen%2520and%2520Binh%2520P.%2520Nguyen%26entry.1292438233%3DPre-trained%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520transfer%2520well%252C%2520but%2520adapting%2520them%2520to%2520downstream%2520tasks%2520remains%2520challenging%2520due%2520to%2520mismatches%2520between%2520pre-training%2520objectives%2520and%2520task%2520requirements.%2520Graph%2520prompt%2520tuning%2520offers%2520a%2520parameter-efficient%2520alternative%2520to%2520fine-tuning%252C%2520yet%2520most%2520methods%2520only%2520modify%2520inputs%2520or%2520representations%2520and%2520leave%2520message%2520passing%2520unchanged%252C%2520limiting%2520their%2520ability%2520to%2520adapt%2520neighborhood%2520interactions.%2520We%2520propose%2520message-adaptive%2520graph%2520prompt%2520tuning%252C%2520which%2520injects%2520learnable%2520prompts%2520into%2520the%2520message%2520passing%2520step%2520to%2520reweight%2520incoming%2520neighbor%2520messages%2520and%2520add%2520task-specific%2520prompt%2520vectors%2520during%2520message%2520aggregation%252C%2520while%2520keeping%2520the%2520backbone%2520GNN%2520frozen.%2520The%2520approach%2520is%2520compatible%2520with%2520common%2520GNN%2520backbones%2520and%2520pre-training%2520strategies%252C%2520and%2520applicable%2520across%2520downstream%2520settings.%2520Experiments%2520on%2520diverse%2520node-%2520and%2520graph-level%2520datasets%2520show%2520consistent%2520gains%2520over%2520prior%2520graph%2520prompting%2520methods%2520in%2520few-shot%2520settings%252C%2520while%2520achieving%2520performance%2520competitive%2520with%2520fine-tuning%2520in%2520full-shot%2520regimes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAGPrompt%3A%20Message-Adaptive%20Graph%20Prompt%20Tuning%20for%20Graph%20Neural%20Networks&entry.906535625=Long%20D.%20Nguyen%20and%20Binh%20P.%20Nguyen&entry.1292438233=Pre-trained%20graph%20neural%20networks%20%28GNNs%29%20transfer%20well%2C%20but%20adapting%20them%20to%20downstream%20tasks%20remains%20challenging%20due%20to%20mismatches%20between%20pre-training%20objectives%20and%20task%20requirements.%20Graph%20prompt%20tuning%20offers%20a%20parameter-efficient%20alternative%20to%20fine-tuning%2C%20yet%20most%20methods%20only%20modify%20inputs%20or%20representations%20and%20leave%20message%20passing%20unchanged%2C%20limiting%20their%20ability%20to%20adapt%20neighborhood%20interactions.%20We%20propose%20message-adaptive%20graph%20prompt%20tuning%2C%20which%20injects%20learnable%20prompts%20into%20the%20message%20passing%20step%20to%20reweight%20incoming%20neighbor%20messages%20and%20add%20task-specific%20prompt%20vectors%20during%20message%20aggregation%2C%20while%20keeping%20the%20backbone%20GNN%20frozen.%20The%20approach%20is%20compatible%20with%20common%20GNN%20backbones%20and%20pre-training%20strategies%2C%20and%20applicable%20across%20downstream%20settings.%20Experiments%20on%20diverse%20node-%20and%20graph-level%20datasets%20show%20consistent%20gains%20over%20prior%20graph%20prompting%20methods%20in%20few-shot%20settings%2C%20while%20achieving%20performance%20competitive%20with%20fine-tuning%20in%20full-shot%20regimes.&entry.1838667208=http%3A//arxiv.org/abs/2602.05567v1&entry.124074799=Read"},
{"title": "Orthogonal Model Merging", "author": "Sihan Yang and Kexuan Shi and Weiyang Liu", "abstract": "Merging finetuned Large Language Models (LLMs) has become increasingly important for integrating diverse capabilities into a single unified model. However, prevailing model merging methods rely on linear arithmetic in Euclidean space, which often destroys the intrinsic geometric properties of pretrained weights, such as hyperspherical energy. To address this, we propose Orthogonal Model Merging (OrthoMerge), a method that performs merging operations on the Riemannian manifold formed by the orthogonal group to preserve the geometric structure of the model's weights. By mapping task-specific orthogonal matrices learned by Orthogonal Finetuning (OFT) to the Lie algebra, OrthoMerge enables a principled yet efficient integration that takes into account both the direction and intensity of adaptations. In addition to directly leveraging orthogonal matrices obtained by OFT, we further extend this approach to general models finetuned with non-OFT methods (i.e., low-rank finetuning, full finetuning) via an Orthogonal-Residual Decoupling strategy. This technique extracts the orthogonal components of expert models by solving the orthogonal Procrustes problem, which are then merged on the manifold of the orthogonal group, while the remaining linear residuals are processed through standard additive merging. Extensive empirical results demonstrate the effectiveness of OrthoMerge in mitigating catastrophic forgetting and maintaining model performance across diverse tasks.", "link": "http://arxiv.org/abs/2602.05943v1", "date": "2026-02-05", "relevancy": 2.3079, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4647}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4623}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Orthogonal%20Model%20Merging&body=Title%3A%20Orthogonal%20Model%20Merging%0AAuthor%3A%20Sihan%20Yang%20and%20Kexuan%20Shi%20and%20Weiyang%20Liu%0AAbstract%3A%20Merging%20finetuned%20Large%20Language%20Models%20%28LLMs%29%20has%20become%20increasingly%20important%20for%20integrating%20diverse%20capabilities%20into%20a%20single%20unified%20model.%20However%2C%20prevailing%20model%20merging%20methods%20rely%20on%20linear%20arithmetic%20in%20Euclidean%20space%2C%20which%20often%20destroys%20the%20intrinsic%20geometric%20properties%20of%20pretrained%20weights%2C%20such%20as%20hyperspherical%20energy.%20To%20address%20this%2C%20we%20propose%20Orthogonal%20Model%20Merging%20%28OrthoMerge%29%2C%20a%20method%20that%20performs%20merging%20operations%20on%20the%20Riemannian%20manifold%20formed%20by%20the%20orthogonal%20group%20to%20preserve%20the%20geometric%20structure%20of%20the%20model%27s%20weights.%20By%20mapping%20task-specific%20orthogonal%20matrices%20learned%20by%20Orthogonal%20Finetuning%20%28OFT%29%20to%20the%20Lie%20algebra%2C%20OrthoMerge%20enables%20a%20principled%20yet%20efficient%20integration%20that%20takes%20into%20account%20both%20the%20direction%20and%20intensity%20of%20adaptations.%20In%20addition%20to%20directly%20leveraging%20orthogonal%20matrices%20obtained%20by%20OFT%2C%20we%20further%20extend%20this%20approach%20to%20general%20models%20finetuned%20with%20non-OFT%20methods%20%28i.e.%2C%20low-rank%20finetuning%2C%20full%20finetuning%29%20via%20an%20Orthogonal-Residual%20Decoupling%20strategy.%20This%20technique%20extracts%20the%20orthogonal%20components%20of%20expert%20models%20by%20solving%20the%20orthogonal%20Procrustes%20problem%2C%20which%20are%20then%20merged%20on%20the%20manifold%20of%20the%20orthogonal%20group%2C%20while%20the%20remaining%20linear%20residuals%20are%20processed%20through%20standard%20additive%20merging.%20Extensive%20empirical%20results%20demonstrate%20the%20effectiveness%20of%20OrthoMerge%20in%20mitigating%20catastrophic%20forgetting%20and%20maintaining%20model%20performance%20across%20diverse%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrthogonal%2520Model%2520Merging%26entry.906535625%3DSihan%2520Yang%2520and%2520Kexuan%2520Shi%2520and%2520Weiyang%2520Liu%26entry.1292438233%3DMerging%2520finetuned%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520become%2520increasingly%2520important%2520for%2520integrating%2520diverse%2520capabilities%2520into%2520a%2520single%2520unified%2520model.%2520However%252C%2520prevailing%2520model%2520merging%2520methods%2520rely%2520on%2520linear%2520arithmetic%2520in%2520Euclidean%2520space%252C%2520which%2520often%2520destroys%2520the%2520intrinsic%2520geometric%2520properties%2520of%2520pretrained%2520weights%252C%2520such%2520as%2520hyperspherical%2520energy.%2520To%2520address%2520this%252C%2520we%2520propose%2520Orthogonal%2520Model%2520Merging%2520%2528OrthoMerge%2529%252C%2520a%2520method%2520that%2520performs%2520merging%2520operations%2520on%2520the%2520Riemannian%2520manifold%2520formed%2520by%2520the%2520orthogonal%2520group%2520to%2520preserve%2520the%2520geometric%2520structure%2520of%2520the%2520model%2527s%2520weights.%2520By%2520mapping%2520task-specific%2520orthogonal%2520matrices%2520learned%2520by%2520Orthogonal%2520Finetuning%2520%2528OFT%2529%2520to%2520the%2520Lie%2520algebra%252C%2520OrthoMerge%2520enables%2520a%2520principled%2520yet%2520efficient%2520integration%2520that%2520takes%2520into%2520account%2520both%2520the%2520direction%2520and%2520intensity%2520of%2520adaptations.%2520In%2520addition%2520to%2520directly%2520leveraging%2520orthogonal%2520matrices%2520obtained%2520by%2520OFT%252C%2520we%2520further%2520extend%2520this%2520approach%2520to%2520general%2520models%2520finetuned%2520with%2520non-OFT%2520methods%2520%2528i.e.%252C%2520low-rank%2520finetuning%252C%2520full%2520finetuning%2529%2520via%2520an%2520Orthogonal-Residual%2520Decoupling%2520strategy.%2520This%2520technique%2520extracts%2520the%2520orthogonal%2520components%2520of%2520expert%2520models%2520by%2520solving%2520the%2520orthogonal%2520Procrustes%2520problem%252C%2520which%2520are%2520then%2520merged%2520on%2520the%2520manifold%2520of%2520the%2520orthogonal%2520group%252C%2520while%2520the%2520remaining%2520linear%2520residuals%2520are%2520processed%2520through%2520standard%2520additive%2520merging.%2520Extensive%2520empirical%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520OrthoMerge%2520in%2520mitigating%2520catastrophic%2520forgetting%2520and%2520maintaining%2520model%2520performance%2520across%2520diverse%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Orthogonal%20Model%20Merging&entry.906535625=Sihan%20Yang%20and%20Kexuan%20Shi%20and%20Weiyang%20Liu&entry.1292438233=Merging%20finetuned%20Large%20Language%20Models%20%28LLMs%29%20has%20become%20increasingly%20important%20for%20integrating%20diverse%20capabilities%20into%20a%20single%20unified%20model.%20However%2C%20prevailing%20model%20merging%20methods%20rely%20on%20linear%20arithmetic%20in%20Euclidean%20space%2C%20which%20often%20destroys%20the%20intrinsic%20geometric%20properties%20of%20pretrained%20weights%2C%20such%20as%20hyperspherical%20energy.%20To%20address%20this%2C%20we%20propose%20Orthogonal%20Model%20Merging%20%28OrthoMerge%29%2C%20a%20method%20that%20performs%20merging%20operations%20on%20the%20Riemannian%20manifold%20formed%20by%20the%20orthogonal%20group%20to%20preserve%20the%20geometric%20structure%20of%20the%20model%27s%20weights.%20By%20mapping%20task-specific%20orthogonal%20matrices%20learned%20by%20Orthogonal%20Finetuning%20%28OFT%29%20to%20the%20Lie%20algebra%2C%20OrthoMerge%20enables%20a%20principled%20yet%20efficient%20integration%20that%20takes%20into%20account%20both%20the%20direction%20and%20intensity%20of%20adaptations.%20In%20addition%20to%20directly%20leveraging%20orthogonal%20matrices%20obtained%20by%20OFT%2C%20we%20further%20extend%20this%20approach%20to%20general%20models%20finetuned%20with%20non-OFT%20methods%20%28i.e.%2C%20low-rank%20finetuning%2C%20full%20finetuning%29%20via%20an%20Orthogonal-Residual%20Decoupling%20strategy.%20This%20technique%20extracts%20the%20orthogonal%20components%20of%20expert%20models%20by%20solving%20the%20orthogonal%20Procrustes%20problem%2C%20which%20are%20then%20merged%20on%20the%20manifold%20of%20the%20orthogonal%20group%2C%20while%20the%20remaining%20linear%20residuals%20are%20processed%20through%20standard%20additive%20merging.%20Extensive%20empirical%20results%20demonstrate%20the%20effectiveness%20of%20OrthoMerge%20in%20mitigating%20catastrophic%20forgetting%20and%20maintaining%20model%20performance%20across%20diverse%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2602.05943v1&entry.124074799=Read"},
{"title": "A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion", "author": "Dennis Bank and Joost Cordes and Thomas Seel and Simon F. G. Ehlers", "abstract": "Reliable terrain perception is a critical prerequisite for the deployment of humanoid robots in unstructured, human-centric environments. While traditional systems often rely on manually engineered, single-sensor pipelines, this paper presents a learning-based framework that uses an intermediate, robot-centric heightmap representation. A hybrid Encoder-Decoder Structure (EDS) is introduced, utilizing a Convolutional Neural Network (CNN) for spatial feature extraction fused with a Gated Recurrent Unit (GRU) core for temporal consistency. The architecture integrates multimodal data from an Intel RealSense depth camera, a LIVOX MID-360 LiDAR processed via efficient spherical projection, and an onboard IMU. Quantitative results demonstrate that multimodal fusion improves reconstruction accuracy by 7.2% over depth-only and 9.9% over LiDAR-only configurations. Furthermore, the integration of a 3.2 s temporal context reduces mapping drift.", "link": "http://arxiv.org/abs/2602.05855v1", "date": "2026-02-05", "relevancy": 2.2861, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.573}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5707}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Hybrid%20Autoencoder%20for%20Robust%20Heightmap%20Generation%20from%20Fused%20Lidar%20and%20Depth%20Data%20for%20Humanoid%20Robot%20Locomotion&body=Title%3A%20A%20Hybrid%20Autoencoder%20for%20Robust%20Heightmap%20Generation%20from%20Fused%20Lidar%20and%20Depth%20Data%20for%20Humanoid%20Robot%20Locomotion%0AAuthor%3A%20Dennis%20Bank%20and%20Joost%20Cordes%20and%20Thomas%20Seel%20and%20Simon%20F.%20G.%20Ehlers%0AAbstract%3A%20Reliable%20terrain%20perception%20is%20a%20critical%20prerequisite%20for%20the%20deployment%20of%20humanoid%20robots%20in%20unstructured%2C%20human-centric%20environments.%20While%20traditional%20systems%20often%20rely%20on%20manually%20engineered%2C%20single-sensor%20pipelines%2C%20this%20paper%20presents%20a%20learning-based%20framework%20that%20uses%20an%20intermediate%2C%20robot-centric%20heightmap%20representation.%20A%20hybrid%20Encoder-Decoder%20Structure%20%28EDS%29%20is%20introduced%2C%20utilizing%20a%20Convolutional%20Neural%20Network%20%28CNN%29%20for%20spatial%20feature%20extraction%20fused%20with%20a%20Gated%20Recurrent%20Unit%20%28GRU%29%20core%20for%20temporal%20consistency.%20The%20architecture%20integrates%20multimodal%20data%20from%20an%20Intel%20RealSense%20depth%20camera%2C%20a%20LIVOX%20MID-360%20LiDAR%20processed%20via%20efficient%20spherical%20projection%2C%20and%20an%20onboard%20IMU.%20Quantitative%20results%20demonstrate%20that%20multimodal%20fusion%20improves%20reconstruction%20accuracy%20by%207.2%25%20over%20depth-only%20and%209.9%25%20over%20LiDAR-only%20configurations.%20Furthermore%2C%20the%20integration%20of%20a%203.2%20s%20temporal%20context%20reduces%20mapping%20drift.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Hybrid%2520Autoencoder%2520for%2520Robust%2520Heightmap%2520Generation%2520from%2520Fused%2520Lidar%2520and%2520Depth%2520Data%2520for%2520Humanoid%2520Robot%2520Locomotion%26entry.906535625%3DDennis%2520Bank%2520and%2520Joost%2520Cordes%2520and%2520Thomas%2520Seel%2520and%2520Simon%2520F.%2520G.%2520Ehlers%26entry.1292438233%3DReliable%2520terrain%2520perception%2520is%2520a%2520critical%2520prerequisite%2520for%2520the%2520deployment%2520of%2520humanoid%2520robots%2520in%2520unstructured%252C%2520human-centric%2520environments.%2520While%2520traditional%2520systems%2520often%2520rely%2520on%2520manually%2520engineered%252C%2520single-sensor%2520pipelines%252C%2520this%2520paper%2520presents%2520a%2520learning-based%2520framework%2520that%2520uses%2520an%2520intermediate%252C%2520robot-centric%2520heightmap%2520representation.%2520A%2520hybrid%2520Encoder-Decoder%2520Structure%2520%2528EDS%2529%2520is%2520introduced%252C%2520utilizing%2520a%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529%2520for%2520spatial%2520feature%2520extraction%2520fused%2520with%2520a%2520Gated%2520Recurrent%2520Unit%2520%2528GRU%2529%2520core%2520for%2520temporal%2520consistency.%2520The%2520architecture%2520integrates%2520multimodal%2520data%2520from%2520an%2520Intel%2520RealSense%2520depth%2520camera%252C%2520a%2520LIVOX%2520MID-360%2520LiDAR%2520processed%2520via%2520efficient%2520spherical%2520projection%252C%2520and%2520an%2520onboard%2520IMU.%2520Quantitative%2520results%2520demonstrate%2520that%2520multimodal%2520fusion%2520improves%2520reconstruction%2520accuracy%2520by%25207.2%2525%2520over%2520depth-only%2520and%25209.9%2525%2520over%2520LiDAR-only%2520configurations.%2520Furthermore%252C%2520the%2520integration%2520of%2520a%25203.2%2520s%2520temporal%2520context%2520reduces%2520mapping%2520drift.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Hybrid%20Autoencoder%20for%20Robust%20Heightmap%20Generation%20from%20Fused%20Lidar%20and%20Depth%20Data%20for%20Humanoid%20Robot%20Locomotion&entry.906535625=Dennis%20Bank%20and%20Joost%20Cordes%20and%20Thomas%20Seel%20and%20Simon%20F.%20G.%20Ehlers&entry.1292438233=Reliable%20terrain%20perception%20is%20a%20critical%20prerequisite%20for%20the%20deployment%20of%20humanoid%20robots%20in%20unstructured%2C%20human-centric%20environments.%20While%20traditional%20systems%20often%20rely%20on%20manually%20engineered%2C%20single-sensor%20pipelines%2C%20this%20paper%20presents%20a%20learning-based%20framework%20that%20uses%20an%20intermediate%2C%20robot-centric%20heightmap%20representation.%20A%20hybrid%20Encoder-Decoder%20Structure%20%28EDS%29%20is%20introduced%2C%20utilizing%20a%20Convolutional%20Neural%20Network%20%28CNN%29%20for%20spatial%20feature%20extraction%20fused%20with%20a%20Gated%20Recurrent%20Unit%20%28GRU%29%20core%20for%20temporal%20consistency.%20The%20architecture%20integrates%20multimodal%20data%20from%20an%20Intel%20RealSense%20depth%20camera%2C%20a%20LIVOX%20MID-360%20LiDAR%20processed%20via%20efficient%20spherical%20projection%2C%20and%20an%20onboard%20IMU.%20Quantitative%20results%20demonstrate%20that%20multimodal%20fusion%20improves%20reconstruction%20accuracy%20by%207.2%25%20over%20depth-only%20and%209.9%25%20over%20LiDAR-only%20configurations.%20Furthermore%2C%20the%20integration%20of%20a%203.2%20s%20temporal%20context%20reduces%20mapping%20drift.&entry.1838667208=http%3A//arxiv.org/abs/2602.05855v1&entry.124074799=Read"},
{"title": "MeshGraphNet-Transformer: Scalable Mesh-based Learned Simulation for Solid Mechanics", "author": "Mikel M. Iparraguirre and Iciar Alfaro and David Gonzalez and Elias Cueto", "abstract": "We present MeshGraphNet-Transformer (MGN-T), a novel architecture that combines the global modeling capabilities of Transformers with the geometric inductive bias of MeshGraphNets, while preserving a mesh-based graph representation. MGN-T overcomes a key limitation of standard MGN, the inefficient long-range information propagation caused by iterative message passing on large, high-resolution meshes. A physics-attention Transformer serves as a global processor, updating all nodal states simultaneously while explicitly retaining node and edge attributes. By directly capturing long-range physical interactions, MGN-T eliminates the need for deep message-passing stacks or hierarchical, coarsened meshes, enabling efficient learning on high-resolution meshes with varying geometries, topologies, and boundary conditions at an industrial scale.\n  We demonstrate that MGN-T successfully handles industrial-scale meshes for impact dynamics, a setting in which standard MGN fails due message-passing under-reaching. The method accurately models self-contact, plasticity, and multivariate outputs, including internal, phenomenological plastic variables. Moreover, MGN-T outperforms state-of-the-art approaches on classical benchmarks, achieving higher accuracy while maintaining practical efficiency, using only a fraction of the parameters required by competing baselines.", "link": "http://arxiv.org/abs/2601.23177v3", "date": "2026-02-05", "relevancy": 2.2756, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6013}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5591}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshGraphNet-Transformer%3A%20Scalable%20Mesh-based%20Learned%20Simulation%20for%20Solid%20Mechanics&body=Title%3A%20MeshGraphNet-Transformer%3A%20Scalable%20Mesh-based%20Learned%20Simulation%20for%20Solid%20Mechanics%0AAuthor%3A%20Mikel%20M.%20Iparraguirre%20and%20Iciar%20Alfaro%20and%20David%20Gonzalez%20and%20Elias%20Cueto%0AAbstract%3A%20We%20present%20MeshGraphNet-Transformer%20%28MGN-T%29%2C%20a%20novel%20architecture%20that%20combines%20the%20global%20modeling%20capabilities%20of%20Transformers%20with%20the%20geometric%20inductive%20bias%20of%20MeshGraphNets%2C%20while%20preserving%20a%20mesh-based%20graph%20representation.%20MGN-T%20overcomes%20a%20key%20limitation%20of%20standard%20MGN%2C%20the%20inefficient%20long-range%20information%20propagation%20caused%20by%20iterative%20message%20passing%20on%20large%2C%20high-resolution%20meshes.%20A%20physics-attention%20Transformer%20serves%20as%20a%20global%20processor%2C%20updating%20all%20nodal%20states%20simultaneously%20while%20explicitly%20retaining%20node%20and%20edge%20attributes.%20By%20directly%20capturing%20long-range%20physical%20interactions%2C%20MGN-T%20eliminates%20the%20need%20for%20deep%20message-passing%20stacks%20or%20hierarchical%2C%20coarsened%20meshes%2C%20enabling%20efficient%20learning%20on%20high-resolution%20meshes%20with%20varying%20geometries%2C%20topologies%2C%20and%20boundary%20conditions%20at%20an%20industrial%20scale.%0A%20%20We%20demonstrate%20that%20MGN-T%20successfully%20handles%20industrial-scale%20meshes%20for%20impact%20dynamics%2C%20a%20setting%20in%20which%20standard%20MGN%20fails%20due%20message-passing%20under-reaching.%20The%20method%20accurately%20models%20self-contact%2C%20plasticity%2C%20and%20multivariate%20outputs%2C%20including%20internal%2C%20phenomenological%20plastic%20variables.%20Moreover%2C%20MGN-T%20outperforms%20state-of-the-art%20approaches%20on%20classical%20benchmarks%2C%20achieving%20higher%20accuracy%20while%20maintaining%20practical%20efficiency%2C%20using%20only%20a%20fraction%20of%20the%20parameters%20required%20by%20competing%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.23177v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshGraphNet-Transformer%253A%2520Scalable%2520Mesh-based%2520Learned%2520Simulation%2520for%2520Solid%2520Mechanics%26entry.906535625%3DMikel%2520M.%2520Iparraguirre%2520and%2520Iciar%2520Alfaro%2520and%2520David%2520Gonzalez%2520and%2520Elias%2520Cueto%26entry.1292438233%3DWe%2520present%2520MeshGraphNet-Transformer%2520%2528MGN-T%2529%252C%2520a%2520novel%2520architecture%2520that%2520combines%2520the%2520global%2520modeling%2520capabilities%2520of%2520Transformers%2520with%2520the%2520geometric%2520inductive%2520bias%2520of%2520MeshGraphNets%252C%2520while%2520preserving%2520a%2520mesh-based%2520graph%2520representation.%2520MGN-T%2520overcomes%2520a%2520key%2520limitation%2520of%2520standard%2520MGN%252C%2520the%2520inefficient%2520long-range%2520information%2520propagation%2520caused%2520by%2520iterative%2520message%2520passing%2520on%2520large%252C%2520high-resolution%2520meshes.%2520A%2520physics-attention%2520Transformer%2520serves%2520as%2520a%2520global%2520processor%252C%2520updating%2520all%2520nodal%2520states%2520simultaneously%2520while%2520explicitly%2520retaining%2520node%2520and%2520edge%2520attributes.%2520By%2520directly%2520capturing%2520long-range%2520physical%2520interactions%252C%2520MGN-T%2520eliminates%2520the%2520need%2520for%2520deep%2520message-passing%2520stacks%2520or%2520hierarchical%252C%2520coarsened%2520meshes%252C%2520enabling%2520efficient%2520learning%2520on%2520high-resolution%2520meshes%2520with%2520varying%2520geometries%252C%2520topologies%252C%2520and%2520boundary%2520conditions%2520at%2520an%2520industrial%2520scale.%250A%2520%2520We%2520demonstrate%2520that%2520MGN-T%2520successfully%2520handles%2520industrial-scale%2520meshes%2520for%2520impact%2520dynamics%252C%2520a%2520setting%2520in%2520which%2520standard%2520MGN%2520fails%2520due%2520message-passing%2520under-reaching.%2520The%2520method%2520accurately%2520models%2520self-contact%252C%2520plasticity%252C%2520and%2520multivariate%2520outputs%252C%2520including%2520internal%252C%2520phenomenological%2520plastic%2520variables.%2520Moreover%252C%2520MGN-T%2520outperforms%2520state-of-the-art%2520approaches%2520on%2520classical%2520benchmarks%252C%2520achieving%2520higher%2520accuracy%2520while%2520maintaining%2520practical%2520efficiency%252C%2520using%2520only%2520a%2520fraction%2520of%2520the%2520parameters%2520required%2520by%2520competing%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.23177v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshGraphNet-Transformer%3A%20Scalable%20Mesh-based%20Learned%20Simulation%20for%20Solid%20Mechanics&entry.906535625=Mikel%20M.%20Iparraguirre%20and%20Iciar%20Alfaro%20and%20David%20Gonzalez%20and%20Elias%20Cueto&entry.1292438233=We%20present%20MeshGraphNet-Transformer%20%28MGN-T%29%2C%20a%20novel%20architecture%20that%20combines%20the%20global%20modeling%20capabilities%20of%20Transformers%20with%20the%20geometric%20inductive%20bias%20of%20MeshGraphNets%2C%20while%20preserving%20a%20mesh-based%20graph%20representation.%20MGN-T%20overcomes%20a%20key%20limitation%20of%20standard%20MGN%2C%20the%20inefficient%20long-range%20information%20propagation%20caused%20by%20iterative%20message%20passing%20on%20large%2C%20high-resolution%20meshes.%20A%20physics-attention%20Transformer%20serves%20as%20a%20global%20processor%2C%20updating%20all%20nodal%20states%20simultaneously%20while%20explicitly%20retaining%20node%20and%20edge%20attributes.%20By%20directly%20capturing%20long-range%20physical%20interactions%2C%20MGN-T%20eliminates%20the%20need%20for%20deep%20message-passing%20stacks%20or%20hierarchical%2C%20coarsened%20meshes%2C%20enabling%20efficient%20learning%20on%20high-resolution%20meshes%20with%20varying%20geometries%2C%20topologies%2C%20and%20boundary%20conditions%20at%20an%20industrial%20scale.%0A%20%20We%20demonstrate%20that%20MGN-T%20successfully%20handles%20industrial-scale%20meshes%20for%20impact%20dynamics%2C%20a%20setting%20in%20which%20standard%20MGN%20fails%20due%20message-passing%20under-reaching.%20The%20method%20accurately%20models%20self-contact%2C%20plasticity%2C%20and%20multivariate%20outputs%2C%20including%20internal%2C%20phenomenological%20plastic%20variables.%20Moreover%2C%20MGN-T%20outperforms%20state-of-the-art%20approaches%20on%20classical%20benchmarks%2C%20achieving%20higher%20accuracy%20while%20maintaining%20practical%20efficiency%2C%20using%20only%20a%20fraction%20of%20the%20parameters%20required%20by%20competing%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2601.23177v3&entry.124074799=Read"},
{"title": "Perception-Based Beliefs for POMDPs with Visual Observations", "author": "Miriam Sch\u00e4fers and Merlijn Krale and Thiago D. Sim\u00e3o and Nils Jansen and Maximilian Weininger", "abstract": "Partially observable Markov decision processes (POMDPs) are a principled planning model for sequential decision-making under uncertainty. Yet, real-world problems with high-dimensional observations, such as camera images, remain intractable for traditional belief- and filtering-based solvers. To tackle this problem, we introduce the Perception-based Beliefs for POMDPs framework (PBP), which complements such solvers with a perception model. This model takes the form of an image classifier which maps visual observations to probability distributions over states. PBP incorporates these distributions directly into belief updates, so the underlying solver does not need to reason explicitly over high-dimensional observation spaces. We show that the belief update of PBP coincides with the standard belief update if the image classifier is exact. Moreover, to handle classifier imprecision, we incorporate uncertainty quantification and introduce two methods to adjust the belief update accordingly. We implement PBP using two traditional POMDP solvers and empirically show that (1) it outperforms existing end-to-end deep RL methods and (2) uncertainty quantification improves robustness of PBP against visual corruption.", "link": "http://arxiv.org/abs/2602.05679v1", "date": "2026-02-05", "relevancy": 2.2754, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6135}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5862}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perception-Based%20Beliefs%20for%20POMDPs%20with%20Visual%20Observations&body=Title%3A%20Perception-Based%20Beliefs%20for%20POMDPs%20with%20Visual%20Observations%0AAuthor%3A%20Miriam%20Sch%C3%A4fers%20and%20Merlijn%20Krale%20and%20Thiago%20D.%20Sim%C3%A3o%20and%20Nils%20Jansen%20and%20Maximilian%20Weininger%0AAbstract%3A%20Partially%20observable%20Markov%20decision%20processes%20%28POMDPs%29%20are%20a%20principled%20planning%20model%20for%20sequential%20decision-making%20under%20uncertainty.%20Yet%2C%20real-world%20problems%20with%20high-dimensional%20observations%2C%20such%20as%20camera%20images%2C%20remain%20intractable%20for%20traditional%20belief-%20and%20filtering-based%20solvers.%20To%20tackle%20this%20problem%2C%20we%20introduce%20the%20Perception-based%20Beliefs%20for%20POMDPs%20framework%20%28PBP%29%2C%20which%20complements%20such%20solvers%20with%20a%20perception%20model.%20This%20model%20takes%20the%20form%20of%20an%20image%20classifier%20which%20maps%20visual%20observations%20to%20probability%20distributions%20over%20states.%20PBP%20incorporates%20these%20distributions%20directly%20into%20belief%20updates%2C%20so%20the%20underlying%20solver%20does%20not%20need%20to%20reason%20explicitly%20over%20high-dimensional%20observation%20spaces.%20We%20show%20that%20the%20belief%20update%20of%20PBP%20coincides%20with%20the%20standard%20belief%20update%20if%20the%20image%20classifier%20is%20exact.%20Moreover%2C%20to%20handle%20classifier%20imprecision%2C%20we%20incorporate%20uncertainty%20quantification%20and%20introduce%20two%20methods%20to%20adjust%20the%20belief%20update%20accordingly.%20We%20implement%20PBP%20using%20two%20traditional%20POMDP%20solvers%20and%20empirically%20show%20that%20%281%29%20it%20outperforms%20existing%20end-to-end%20deep%20RL%20methods%20and%20%282%29%20uncertainty%20quantification%20improves%20robustness%20of%20PBP%20against%20visual%20corruption.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerception-Based%2520Beliefs%2520for%2520POMDPs%2520with%2520Visual%2520Observations%26entry.906535625%3DMiriam%2520Sch%25C3%25A4fers%2520and%2520Merlijn%2520Krale%2520and%2520Thiago%2520D.%2520Sim%25C3%25A3o%2520and%2520Nils%2520Jansen%2520and%2520Maximilian%2520Weininger%26entry.1292438233%3DPartially%2520observable%2520Markov%2520decision%2520processes%2520%2528POMDPs%2529%2520are%2520a%2520principled%2520planning%2520model%2520for%2520sequential%2520decision-making%2520under%2520uncertainty.%2520Yet%252C%2520real-world%2520problems%2520with%2520high-dimensional%2520observations%252C%2520such%2520as%2520camera%2520images%252C%2520remain%2520intractable%2520for%2520traditional%2520belief-%2520and%2520filtering-based%2520solvers.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520introduce%2520the%2520Perception-based%2520Beliefs%2520for%2520POMDPs%2520framework%2520%2528PBP%2529%252C%2520which%2520complements%2520such%2520solvers%2520with%2520a%2520perception%2520model.%2520This%2520model%2520takes%2520the%2520form%2520of%2520an%2520image%2520classifier%2520which%2520maps%2520visual%2520observations%2520to%2520probability%2520distributions%2520over%2520states.%2520PBP%2520incorporates%2520these%2520distributions%2520directly%2520into%2520belief%2520updates%252C%2520so%2520the%2520underlying%2520solver%2520does%2520not%2520need%2520to%2520reason%2520explicitly%2520over%2520high-dimensional%2520observation%2520spaces.%2520We%2520show%2520that%2520the%2520belief%2520update%2520of%2520PBP%2520coincides%2520with%2520the%2520standard%2520belief%2520update%2520if%2520the%2520image%2520classifier%2520is%2520exact.%2520Moreover%252C%2520to%2520handle%2520classifier%2520imprecision%252C%2520we%2520incorporate%2520uncertainty%2520quantification%2520and%2520introduce%2520two%2520methods%2520to%2520adjust%2520the%2520belief%2520update%2520accordingly.%2520We%2520implement%2520PBP%2520using%2520two%2520traditional%2520POMDP%2520solvers%2520and%2520empirically%2520show%2520that%2520%25281%2529%2520it%2520outperforms%2520existing%2520end-to-end%2520deep%2520RL%2520methods%2520and%2520%25282%2529%2520uncertainty%2520quantification%2520improves%2520robustness%2520of%2520PBP%2520against%2520visual%2520corruption.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perception-Based%20Beliefs%20for%20POMDPs%20with%20Visual%20Observations&entry.906535625=Miriam%20Sch%C3%A4fers%20and%20Merlijn%20Krale%20and%20Thiago%20D.%20Sim%C3%A3o%20and%20Nils%20Jansen%20and%20Maximilian%20Weininger&entry.1292438233=Partially%20observable%20Markov%20decision%20processes%20%28POMDPs%29%20are%20a%20principled%20planning%20model%20for%20sequential%20decision-making%20under%20uncertainty.%20Yet%2C%20real-world%20problems%20with%20high-dimensional%20observations%2C%20such%20as%20camera%20images%2C%20remain%20intractable%20for%20traditional%20belief-%20and%20filtering-based%20solvers.%20To%20tackle%20this%20problem%2C%20we%20introduce%20the%20Perception-based%20Beliefs%20for%20POMDPs%20framework%20%28PBP%29%2C%20which%20complements%20such%20solvers%20with%20a%20perception%20model.%20This%20model%20takes%20the%20form%20of%20an%20image%20classifier%20which%20maps%20visual%20observations%20to%20probability%20distributions%20over%20states.%20PBP%20incorporates%20these%20distributions%20directly%20into%20belief%20updates%2C%20so%20the%20underlying%20solver%20does%20not%20need%20to%20reason%20explicitly%20over%20high-dimensional%20observation%20spaces.%20We%20show%20that%20the%20belief%20update%20of%20PBP%20coincides%20with%20the%20standard%20belief%20update%20if%20the%20image%20classifier%20is%20exact.%20Moreover%2C%20to%20handle%20classifier%20imprecision%2C%20we%20incorporate%20uncertainty%20quantification%20and%20introduce%20two%20methods%20to%20adjust%20the%20belief%20update%20accordingly.%20We%20implement%20PBP%20using%20two%20traditional%20POMDP%20solvers%20and%20empirically%20show%20that%20%281%29%20it%20outperforms%20existing%20end-to-end%20deep%20RL%20methods%20and%20%282%29%20uncertainty%20quantification%20improves%20robustness%20of%20PBP%20against%20visual%20corruption.&entry.1838667208=http%3A//arxiv.org/abs/2602.05679v1&entry.124074799=Read"},
{"title": "ReText: Text Boosts Generalization in Image-Based Person Re-identification", "author": "Timur Mamedov and Karina Kvanchiani and Anton Konushin and Vadim Konushin", "abstract": "Generalizable image-based person re-identification (Re-ID) aims to recognize individuals across cameras in unseen domains without retraining. While multiple existing approaches address the domain gap through complex architectures, recent findings indicate that better generalization can be achieved by stylistically diverse single-camera data. Although this data is easy to collect, it lacks complexity due to minimal cross-view variation. We propose ReText, a novel method trained on a mixture of multi-camera Re-ID data and single-camera data, where the latter is complemented by textual descriptions to enrich semantic cues. During training, ReText jointly optimizes three tasks: (1) Re-ID on multi-camera data, (2) image-text matching, and (3) image reconstruction guided by text on single-camera data. Experiments demonstrate that ReText achieves strong generalization and significantly outperforms state-of-the-art methods on cross-domain Re-ID benchmarks. To the best of our knowledge, this is the first work to explore multimodal joint learning on a mixture of multi-camera and single-camera data in image-based person Re-ID.", "link": "http://arxiv.org/abs/2602.05785v1", "date": "2026-02-05", "relevancy": 2.269, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5764}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5692}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReText%3A%20Text%20Boosts%20Generalization%20in%20Image-Based%20Person%20Re-identification&body=Title%3A%20ReText%3A%20Text%20Boosts%20Generalization%20in%20Image-Based%20Person%20Re-identification%0AAuthor%3A%20Timur%20Mamedov%20and%20Karina%20Kvanchiani%20and%20Anton%20Konushin%20and%20Vadim%20Konushin%0AAbstract%3A%20Generalizable%20image-based%20person%20re-identification%20%28Re-ID%29%20aims%20to%20recognize%20individuals%20across%20cameras%20in%20unseen%20domains%20without%20retraining.%20While%20multiple%20existing%20approaches%20address%20the%20domain%20gap%20through%20complex%20architectures%2C%20recent%20findings%20indicate%20that%20better%20generalization%20can%20be%20achieved%20by%20stylistically%20diverse%20single-camera%20data.%20Although%20this%20data%20is%20easy%20to%20collect%2C%20it%20lacks%20complexity%20due%20to%20minimal%20cross-view%20variation.%20We%20propose%20ReText%2C%20a%20novel%20method%20trained%20on%20a%20mixture%20of%20multi-camera%20Re-ID%20data%20and%20single-camera%20data%2C%20where%20the%20latter%20is%20complemented%20by%20textual%20descriptions%20to%20enrich%20semantic%20cues.%20During%20training%2C%20ReText%20jointly%20optimizes%20three%20tasks%3A%20%281%29%20Re-ID%20on%20multi-camera%20data%2C%20%282%29%20image-text%20matching%2C%20and%20%283%29%20image%20reconstruction%20guided%20by%20text%20on%20single-camera%20data.%20Experiments%20demonstrate%20that%20ReText%20achieves%20strong%20generalization%20and%20significantly%20outperforms%20state-of-the-art%20methods%20on%20cross-domain%20Re-ID%20benchmarks.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20explore%20multimodal%20joint%20learning%20on%20a%20mixture%20of%20multi-camera%20and%20single-camera%20data%20in%20image-based%20person%20Re-ID.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReText%253A%2520Text%2520Boosts%2520Generalization%2520in%2520Image-Based%2520Person%2520Re-identification%26entry.906535625%3DTimur%2520Mamedov%2520and%2520Karina%2520Kvanchiani%2520and%2520Anton%2520Konushin%2520and%2520Vadim%2520Konushin%26entry.1292438233%3DGeneralizable%2520image-based%2520person%2520re-identification%2520%2528Re-ID%2529%2520aims%2520to%2520recognize%2520individuals%2520across%2520cameras%2520in%2520unseen%2520domains%2520without%2520retraining.%2520While%2520multiple%2520existing%2520approaches%2520address%2520the%2520domain%2520gap%2520through%2520complex%2520architectures%252C%2520recent%2520findings%2520indicate%2520that%2520better%2520generalization%2520can%2520be%2520achieved%2520by%2520stylistically%2520diverse%2520single-camera%2520data.%2520Although%2520this%2520data%2520is%2520easy%2520to%2520collect%252C%2520it%2520lacks%2520complexity%2520due%2520to%2520minimal%2520cross-view%2520variation.%2520We%2520propose%2520ReText%252C%2520a%2520novel%2520method%2520trained%2520on%2520a%2520mixture%2520of%2520multi-camera%2520Re-ID%2520data%2520and%2520single-camera%2520data%252C%2520where%2520the%2520latter%2520is%2520complemented%2520by%2520textual%2520descriptions%2520to%2520enrich%2520semantic%2520cues.%2520During%2520training%252C%2520ReText%2520jointly%2520optimizes%2520three%2520tasks%253A%2520%25281%2529%2520Re-ID%2520on%2520multi-camera%2520data%252C%2520%25282%2529%2520image-text%2520matching%252C%2520and%2520%25283%2529%2520image%2520reconstruction%2520guided%2520by%2520text%2520on%2520single-camera%2520data.%2520Experiments%2520demonstrate%2520that%2520ReText%2520achieves%2520strong%2520generalization%2520and%2520significantly%2520outperforms%2520state-of-the-art%2520methods%2520on%2520cross-domain%2520Re-ID%2520benchmarks.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520to%2520explore%2520multimodal%2520joint%2520learning%2520on%2520a%2520mixture%2520of%2520multi-camera%2520and%2520single-camera%2520data%2520in%2520image-based%2520person%2520Re-ID.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReText%3A%20Text%20Boosts%20Generalization%20in%20Image-Based%20Person%20Re-identification&entry.906535625=Timur%20Mamedov%20and%20Karina%20Kvanchiani%20and%20Anton%20Konushin%20and%20Vadim%20Konushin&entry.1292438233=Generalizable%20image-based%20person%20re-identification%20%28Re-ID%29%20aims%20to%20recognize%20individuals%20across%20cameras%20in%20unseen%20domains%20without%20retraining.%20While%20multiple%20existing%20approaches%20address%20the%20domain%20gap%20through%20complex%20architectures%2C%20recent%20findings%20indicate%20that%20better%20generalization%20can%20be%20achieved%20by%20stylistically%20diverse%20single-camera%20data.%20Although%20this%20data%20is%20easy%20to%20collect%2C%20it%20lacks%20complexity%20due%20to%20minimal%20cross-view%20variation.%20We%20propose%20ReText%2C%20a%20novel%20method%20trained%20on%20a%20mixture%20of%20multi-camera%20Re-ID%20data%20and%20single-camera%20data%2C%20where%20the%20latter%20is%20complemented%20by%20textual%20descriptions%20to%20enrich%20semantic%20cues.%20During%20training%2C%20ReText%20jointly%20optimizes%20three%20tasks%3A%20%281%29%20Re-ID%20on%20multi-camera%20data%2C%20%282%29%20image-text%20matching%2C%20and%20%283%29%20image%20reconstruction%20guided%20by%20text%20on%20single-camera%20data.%20Experiments%20demonstrate%20that%20ReText%20achieves%20strong%20generalization%20and%20significantly%20outperforms%20state-of-the-art%20methods%20on%20cross-domain%20Re-ID%20benchmarks.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20work%20to%20explore%20multimodal%20joint%20learning%20on%20a%20mixture%20of%20multi-camera%20and%20single-camera%20data%20in%20image-based%20person%20Re-ID.&entry.1838667208=http%3A//arxiv.org/abs/2602.05785v1&entry.124074799=Read"},
{"title": "FHAIM: Fully Homomorphic AIM For Private Synthetic Data Generation", "author": "Mayank Kumar and Qian Lou and Paulo Barreto and Martine De Cock and Sikha Pentyala", "abstract": "Data is the lifeblood of AI, yet much of the most valuable data remains locked in silos due to privacy and regulations. As a result, AI remains heavily underutilized in many of the most important domains, including healthcare, education, and finance. Synthetic data generation (SDG), i.e. the generation of artificial data with a synthesizer trained on real data, offers an appealing solution to make data available while mitigating privacy concerns, however existing SDG-as-a-service workflow require data holders to trust providers with access to private data.We propose FHAIM, the first fully homomorphic encryption (FHE) framework for training a marginal-based synthetic data generator on encrypted tabular data. FHAIM adapts the widely used AIM algorithm to the FHE setting using novel FHE protocols, ensuring that the private data remains encrypted throughout and is released only with differential privacy guarantees. Our empirical analysis show that FHAIM preserves the performance of AIM while maintaining feasible runtimes.", "link": "http://arxiv.org/abs/2602.05838v1", "date": "2026-02-05", "relevancy": 2.2677, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4592}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4565}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FHAIM%3A%20Fully%20Homomorphic%20AIM%20For%20Private%20Synthetic%20Data%20Generation&body=Title%3A%20FHAIM%3A%20Fully%20Homomorphic%20AIM%20For%20Private%20Synthetic%20Data%20Generation%0AAuthor%3A%20Mayank%20Kumar%20and%20Qian%20Lou%20and%20Paulo%20Barreto%20and%20Martine%20De%20Cock%20and%20Sikha%20Pentyala%0AAbstract%3A%20Data%20is%20the%20lifeblood%20of%20AI%2C%20yet%20much%20of%20the%20most%20valuable%20data%20remains%20locked%20in%20silos%20due%20to%20privacy%20and%20regulations.%20As%20a%20result%2C%20AI%20remains%20heavily%20underutilized%20in%20many%20of%20the%20most%20important%20domains%2C%20including%20healthcare%2C%20education%2C%20and%20finance.%20Synthetic%20data%20generation%20%28SDG%29%2C%20i.e.%20the%20generation%20of%20artificial%20data%20with%20a%20synthesizer%20trained%20on%20real%20data%2C%20offers%20an%20appealing%20solution%20to%20make%20data%20available%20while%20mitigating%20privacy%20concerns%2C%20however%20existing%20SDG-as-a-service%20workflow%20require%20data%20holders%20to%20trust%20providers%20with%20access%20to%20private%20data.We%20propose%20FHAIM%2C%20the%20first%20fully%20homomorphic%20encryption%20%28FHE%29%20framework%20for%20training%20a%20marginal-based%20synthetic%20data%20generator%20on%20encrypted%20tabular%20data.%20FHAIM%20adapts%20the%20widely%20used%20AIM%20algorithm%20to%20the%20FHE%20setting%20using%20novel%20FHE%20protocols%2C%20ensuring%20that%20the%20private%20data%20remains%20encrypted%20throughout%20and%20is%20released%20only%20with%20differential%20privacy%20guarantees.%20Our%20empirical%20analysis%20show%20that%20FHAIM%20preserves%20the%20performance%20of%20AIM%20while%20maintaining%20feasible%20runtimes.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFHAIM%253A%2520Fully%2520Homomorphic%2520AIM%2520For%2520Private%2520Synthetic%2520Data%2520Generation%26entry.906535625%3DMayank%2520Kumar%2520and%2520Qian%2520Lou%2520and%2520Paulo%2520Barreto%2520and%2520Martine%2520De%2520Cock%2520and%2520Sikha%2520Pentyala%26entry.1292438233%3DData%2520is%2520the%2520lifeblood%2520of%2520AI%252C%2520yet%2520much%2520of%2520the%2520most%2520valuable%2520data%2520remains%2520locked%2520in%2520silos%2520due%2520to%2520privacy%2520and%2520regulations.%2520As%2520a%2520result%252C%2520AI%2520remains%2520heavily%2520underutilized%2520in%2520many%2520of%2520the%2520most%2520important%2520domains%252C%2520including%2520healthcare%252C%2520education%252C%2520and%2520finance.%2520Synthetic%2520data%2520generation%2520%2528SDG%2529%252C%2520i.e.%2520the%2520generation%2520of%2520artificial%2520data%2520with%2520a%2520synthesizer%2520trained%2520on%2520real%2520data%252C%2520offers%2520an%2520appealing%2520solution%2520to%2520make%2520data%2520available%2520while%2520mitigating%2520privacy%2520concerns%252C%2520however%2520existing%2520SDG-as-a-service%2520workflow%2520require%2520data%2520holders%2520to%2520trust%2520providers%2520with%2520access%2520to%2520private%2520data.We%2520propose%2520FHAIM%252C%2520the%2520first%2520fully%2520homomorphic%2520encryption%2520%2528FHE%2529%2520framework%2520for%2520training%2520a%2520marginal-based%2520synthetic%2520data%2520generator%2520on%2520encrypted%2520tabular%2520data.%2520FHAIM%2520adapts%2520the%2520widely%2520used%2520AIM%2520algorithm%2520to%2520the%2520FHE%2520setting%2520using%2520novel%2520FHE%2520protocols%252C%2520ensuring%2520that%2520the%2520private%2520data%2520remains%2520encrypted%2520throughout%2520and%2520is%2520released%2520only%2520with%2520differential%2520privacy%2520guarantees.%2520Our%2520empirical%2520analysis%2520show%2520that%2520FHAIM%2520preserves%2520the%2520performance%2520of%2520AIM%2520while%2520maintaining%2520feasible%2520runtimes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FHAIM%3A%20Fully%20Homomorphic%20AIM%20For%20Private%20Synthetic%20Data%20Generation&entry.906535625=Mayank%20Kumar%20and%20Qian%20Lou%20and%20Paulo%20Barreto%20and%20Martine%20De%20Cock%20and%20Sikha%20Pentyala&entry.1292438233=Data%20is%20the%20lifeblood%20of%20AI%2C%20yet%20much%20of%20the%20most%20valuable%20data%20remains%20locked%20in%20silos%20due%20to%20privacy%20and%20regulations.%20As%20a%20result%2C%20AI%20remains%20heavily%20underutilized%20in%20many%20of%20the%20most%20important%20domains%2C%20including%20healthcare%2C%20education%2C%20and%20finance.%20Synthetic%20data%20generation%20%28SDG%29%2C%20i.e.%20the%20generation%20of%20artificial%20data%20with%20a%20synthesizer%20trained%20on%20real%20data%2C%20offers%20an%20appealing%20solution%20to%20make%20data%20available%20while%20mitigating%20privacy%20concerns%2C%20however%20existing%20SDG-as-a-service%20workflow%20require%20data%20holders%20to%20trust%20providers%20with%20access%20to%20private%20data.We%20propose%20FHAIM%2C%20the%20first%20fully%20homomorphic%20encryption%20%28FHE%29%20framework%20for%20training%20a%20marginal-based%20synthetic%20data%20generator%20on%20encrypted%20tabular%20data.%20FHAIM%20adapts%20the%20widely%20used%20AIM%20algorithm%20to%20the%20FHE%20setting%20using%20novel%20FHE%20protocols%2C%20ensuring%20that%20the%20private%20data%20remains%20encrypted%20throughout%20and%20is%20released%20only%20with%20differential%20privacy%20guarantees.%20Our%20empirical%20analysis%20show%20that%20FHAIM%20preserves%20the%20performance%20of%20AIM%20while%20maintaining%20feasible%20runtimes.&entry.1838667208=http%3A//arxiv.org/abs/2602.05838v1&entry.124074799=Read"},
{"title": "PIRATR: Parametric Object Inference for Robotic Applications with Transformers in 3D Point Clouds", "author": "Michael Schwingshackl and Fabio F. Oberweger and Mario Niedermeyer and Huemer Johannes and Markus Murschitz", "abstract": "We present PIRATR, an end-to-end 3D object detection framework for robotic use cases in point clouds. Extending PI3DETR, our method streamlines parametric 3D object detection by jointly estimating multi-class 6-DoF poses and class-specific parametric attributes directly from occlusion-affected point cloud data. This formulation enables not only geometric localization but also the estimation of task-relevant properties for parametric objects, such as a gripper's opening, where the 3D model is adjusted according to simple, predefined rules. The architecture employs modular, class-specific heads, making it straightforward to extend to novel object types without re-designing the pipeline. We validate PIRATR on an automated forklift platform, focusing on three structurally and functionally diverse categories: crane grippers, loading platforms, and pallets. Trained entirely in a synthetic environment, PIRATR generalizes effectively to real outdoor LiDAR scans, achieving a detection mAP of 0.919 without additional fine-tuning. PIRATR establishes a new paradigm of pose-aware, parameterized perception. This bridges the gap between low-level geometric reasoning and actionable world models, paving the way for scalable, simulation-trained perception systems that can be deployed in dynamic robotic environments. Code available at https://github.com/swingaxe/piratr.", "link": "http://arxiv.org/abs/2602.05557v1", "date": "2026-02-05", "relevancy": 2.2674, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5819}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.565}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PIRATR%3A%20Parametric%20Object%20Inference%20for%20Robotic%20Applications%20with%20Transformers%20in%203D%20Point%20Clouds&body=Title%3A%20PIRATR%3A%20Parametric%20Object%20Inference%20for%20Robotic%20Applications%20with%20Transformers%20in%203D%20Point%20Clouds%0AAuthor%3A%20Michael%20Schwingshackl%20and%20Fabio%20F.%20Oberweger%20and%20Mario%20Niedermeyer%20and%20Huemer%20Johannes%20and%20Markus%20Murschitz%0AAbstract%3A%20We%20present%20PIRATR%2C%20an%20end-to-end%203D%20object%20detection%20framework%20for%20robotic%20use%20cases%20in%20point%20clouds.%20Extending%20PI3DETR%2C%20our%20method%20streamlines%20parametric%203D%20object%20detection%20by%20jointly%20estimating%20multi-class%206-DoF%20poses%20and%20class-specific%20parametric%20attributes%20directly%20from%20occlusion-affected%20point%20cloud%20data.%20This%20formulation%20enables%20not%20only%20geometric%20localization%20but%20also%20the%20estimation%20of%20task-relevant%20properties%20for%20parametric%20objects%2C%20such%20as%20a%20gripper%27s%20opening%2C%20where%20the%203D%20model%20is%20adjusted%20according%20to%20simple%2C%20predefined%20rules.%20The%20architecture%20employs%20modular%2C%20class-specific%20heads%2C%20making%20it%20straightforward%20to%20extend%20to%20novel%20object%20types%20without%20re-designing%20the%20pipeline.%20We%20validate%20PIRATR%20on%20an%20automated%20forklift%20platform%2C%20focusing%20on%20three%20structurally%20and%20functionally%20diverse%20categories%3A%20crane%20grippers%2C%20loading%20platforms%2C%20and%20pallets.%20Trained%20entirely%20in%20a%20synthetic%20environment%2C%20PIRATR%20generalizes%20effectively%20to%20real%20outdoor%20LiDAR%20scans%2C%20achieving%20a%20detection%20mAP%20of%200.919%20without%20additional%20fine-tuning.%20PIRATR%20establishes%20a%20new%20paradigm%20of%20pose-aware%2C%20parameterized%20perception.%20This%20bridges%20the%20gap%20between%20low-level%20geometric%20reasoning%20and%20actionable%20world%20models%2C%20paving%20the%20way%20for%20scalable%2C%20simulation-trained%20perception%20systems%20that%20can%20be%20deployed%20in%20dynamic%20robotic%20environments.%20Code%20available%20at%20https%3A//github.com/swingaxe/piratr.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPIRATR%253A%2520Parametric%2520Object%2520Inference%2520for%2520Robotic%2520Applications%2520with%2520Transformers%2520in%25203D%2520Point%2520Clouds%26entry.906535625%3DMichael%2520Schwingshackl%2520and%2520Fabio%2520F.%2520Oberweger%2520and%2520Mario%2520Niedermeyer%2520and%2520Huemer%2520Johannes%2520and%2520Markus%2520Murschitz%26entry.1292438233%3DWe%2520present%2520PIRATR%252C%2520an%2520end-to-end%25203D%2520object%2520detection%2520framework%2520for%2520robotic%2520use%2520cases%2520in%2520point%2520clouds.%2520Extending%2520PI3DETR%252C%2520our%2520method%2520streamlines%2520parametric%25203D%2520object%2520detection%2520by%2520jointly%2520estimating%2520multi-class%25206-DoF%2520poses%2520and%2520class-specific%2520parametric%2520attributes%2520directly%2520from%2520occlusion-affected%2520point%2520cloud%2520data.%2520This%2520formulation%2520enables%2520not%2520only%2520geometric%2520localization%2520but%2520also%2520the%2520estimation%2520of%2520task-relevant%2520properties%2520for%2520parametric%2520objects%252C%2520such%2520as%2520a%2520gripper%2527s%2520opening%252C%2520where%2520the%25203D%2520model%2520is%2520adjusted%2520according%2520to%2520simple%252C%2520predefined%2520rules.%2520The%2520architecture%2520employs%2520modular%252C%2520class-specific%2520heads%252C%2520making%2520it%2520straightforward%2520to%2520extend%2520to%2520novel%2520object%2520types%2520without%2520re-designing%2520the%2520pipeline.%2520We%2520validate%2520PIRATR%2520on%2520an%2520automated%2520forklift%2520platform%252C%2520focusing%2520on%2520three%2520structurally%2520and%2520functionally%2520diverse%2520categories%253A%2520crane%2520grippers%252C%2520loading%2520platforms%252C%2520and%2520pallets.%2520Trained%2520entirely%2520in%2520a%2520synthetic%2520environment%252C%2520PIRATR%2520generalizes%2520effectively%2520to%2520real%2520outdoor%2520LiDAR%2520scans%252C%2520achieving%2520a%2520detection%2520mAP%2520of%25200.919%2520without%2520additional%2520fine-tuning.%2520PIRATR%2520establishes%2520a%2520new%2520paradigm%2520of%2520pose-aware%252C%2520parameterized%2520perception.%2520This%2520bridges%2520the%2520gap%2520between%2520low-level%2520geometric%2520reasoning%2520and%2520actionable%2520world%2520models%252C%2520paving%2520the%2520way%2520for%2520scalable%252C%2520simulation-trained%2520perception%2520systems%2520that%2520can%2520be%2520deployed%2520in%2520dynamic%2520robotic%2520environments.%2520Code%2520available%2520at%2520https%253A//github.com/swingaxe/piratr.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PIRATR%3A%20Parametric%20Object%20Inference%20for%20Robotic%20Applications%20with%20Transformers%20in%203D%20Point%20Clouds&entry.906535625=Michael%20Schwingshackl%20and%20Fabio%20F.%20Oberweger%20and%20Mario%20Niedermeyer%20and%20Huemer%20Johannes%20and%20Markus%20Murschitz&entry.1292438233=We%20present%20PIRATR%2C%20an%20end-to-end%203D%20object%20detection%20framework%20for%20robotic%20use%20cases%20in%20point%20clouds.%20Extending%20PI3DETR%2C%20our%20method%20streamlines%20parametric%203D%20object%20detection%20by%20jointly%20estimating%20multi-class%206-DoF%20poses%20and%20class-specific%20parametric%20attributes%20directly%20from%20occlusion-affected%20point%20cloud%20data.%20This%20formulation%20enables%20not%20only%20geometric%20localization%20but%20also%20the%20estimation%20of%20task-relevant%20properties%20for%20parametric%20objects%2C%20such%20as%20a%20gripper%27s%20opening%2C%20where%20the%203D%20model%20is%20adjusted%20according%20to%20simple%2C%20predefined%20rules.%20The%20architecture%20employs%20modular%2C%20class-specific%20heads%2C%20making%20it%20straightforward%20to%20extend%20to%20novel%20object%20types%20without%20re-designing%20the%20pipeline.%20We%20validate%20PIRATR%20on%20an%20automated%20forklift%20platform%2C%20focusing%20on%20three%20structurally%20and%20functionally%20diverse%20categories%3A%20crane%20grippers%2C%20loading%20platforms%2C%20and%20pallets.%20Trained%20entirely%20in%20a%20synthetic%20environment%2C%20PIRATR%20generalizes%20effectively%20to%20real%20outdoor%20LiDAR%20scans%2C%20achieving%20a%20detection%20mAP%20of%200.919%20without%20additional%20fine-tuning.%20PIRATR%20establishes%20a%20new%20paradigm%20of%20pose-aware%2C%20parameterized%20perception.%20This%20bridges%20the%20gap%20between%20low-level%20geometric%20reasoning%20and%20actionable%20world%20models%2C%20paving%20the%20way%20for%20scalable%2C%20simulation-trained%20perception%20systems%20that%20can%20be%20deployed%20in%20dynamic%20robotic%20environments.%20Code%20available%20at%20https%3A//github.com/swingaxe/piratr.&entry.1838667208=http%3A//arxiv.org/abs/2602.05557v1&entry.124074799=Read"},
{"title": "Test-time Adaptive Hierarchical Co-enhanced Denoising Network for Reliable Multimodal Classification", "author": "Shu Shen and C. L. Philip Chen and Tong Zhang", "abstract": "Reliable learning of multimodal data (e.g., multi-omics) is a widely concerning issue, especially in safety-critical applications such as medical diagnosis. However, low-quality data induced by multimodal noise poses a major challenge in this domain, causing existing methods to suffer from two key limitations. First, they struggle to handle heterogeneous data noise, hindering robust multimodal representation learning. Second, they exhibit limited adaptability and generalization when encountering previously unseen noise. To address these issues, we propose Test-time Adaptive Hierarchical Co-enhanced Denoising Network (TAHCD). On one hand, TAHCD introduces the Adaptive Stable Subspace Alignment and Sample-Adaptive Confidence Alignment to reliably remove heterogeneous noise. They account for noise at both global and instance levels and enable jointly removal of modality-specific and cross-modality noise, achieving robust learning. On the other hand, TAHCD introduces Test-Time Cooperative Enhancement, which adaptively updates the model in response to input noise in a label-free manner, thus improving generalization. This is achieved by collaboratively enhancing the joint removal process of modality-specific and cross-modality noise across global and instance levels according to sample noise. Experiments on multiple benchmarks demonstrate that the proposed method achieves superior classification performance, robustness, and generalization compared with state-of-the-art reliable multimodal learning approaches.", "link": "http://arxiv.org/abs/2601.07163v2", "date": "2026-02-05", "relevancy": 2.2562, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5717}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.565}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-time%20Adaptive%20Hierarchical%20Co-enhanced%20Denoising%20Network%20for%20Reliable%20Multimodal%20Classification&body=Title%3A%20Test-time%20Adaptive%20Hierarchical%20Co-enhanced%20Denoising%20Network%20for%20Reliable%20Multimodal%20Classification%0AAuthor%3A%20Shu%20Shen%20and%20C.%20L.%20Philip%20Chen%20and%20Tong%20Zhang%0AAbstract%3A%20Reliable%20learning%20of%20multimodal%20data%20%28e.g.%2C%20multi-omics%29%20is%20a%20widely%20concerning%20issue%2C%20especially%20in%20safety-critical%20applications%20such%20as%20medical%20diagnosis.%20However%2C%20low-quality%20data%20induced%20by%20multimodal%20noise%20poses%20a%20major%20challenge%20in%20this%20domain%2C%20causing%20existing%20methods%20to%20suffer%20from%20two%20key%20limitations.%20First%2C%20they%20struggle%20to%20handle%20heterogeneous%20data%20noise%2C%20hindering%20robust%20multimodal%20representation%20learning.%20Second%2C%20they%20exhibit%20limited%20adaptability%20and%20generalization%20when%20encountering%20previously%20unseen%20noise.%20To%20address%20these%20issues%2C%20we%20propose%20Test-time%20Adaptive%20Hierarchical%20Co-enhanced%20Denoising%20Network%20%28TAHCD%29.%20On%20one%20hand%2C%20TAHCD%20introduces%20the%20Adaptive%20Stable%20Subspace%20Alignment%20and%20Sample-Adaptive%20Confidence%20Alignment%20to%20reliably%20remove%20heterogeneous%20noise.%20They%20account%20for%20noise%20at%20both%20global%20and%20instance%20levels%20and%20enable%20jointly%20removal%20of%20modality-specific%20and%20cross-modality%20noise%2C%20achieving%20robust%20learning.%20On%20the%20other%20hand%2C%20TAHCD%20introduces%20Test-Time%20Cooperative%20Enhancement%2C%20which%20adaptively%20updates%20the%20model%20in%20response%20to%20input%20noise%20in%20a%20label-free%20manner%2C%20thus%20improving%20generalization.%20This%20is%20achieved%20by%20collaboratively%20enhancing%20the%20joint%20removal%20process%20of%20modality-specific%20and%20cross-modality%20noise%20across%20global%20and%20instance%20levels%20according%20to%20sample%20noise.%20Experiments%20on%20multiple%20benchmarks%20demonstrate%20that%20the%20proposed%20method%20achieves%20superior%20classification%20performance%2C%20robustness%2C%20and%20generalization%20compared%20with%20state-of-the-art%20reliable%20multimodal%20learning%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07163v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-time%2520Adaptive%2520Hierarchical%2520Co-enhanced%2520Denoising%2520Network%2520for%2520Reliable%2520Multimodal%2520Classification%26entry.906535625%3DShu%2520Shen%2520and%2520C.%2520L.%2520Philip%2520Chen%2520and%2520Tong%2520Zhang%26entry.1292438233%3DReliable%2520learning%2520of%2520multimodal%2520data%2520%2528e.g.%252C%2520multi-omics%2529%2520is%2520a%2520widely%2520concerning%2520issue%252C%2520especially%2520in%2520safety-critical%2520applications%2520such%2520as%2520medical%2520diagnosis.%2520However%252C%2520low-quality%2520data%2520induced%2520by%2520multimodal%2520noise%2520poses%2520a%2520major%2520challenge%2520in%2520this%2520domain%252C%2520causing%2520existing%2520methods%2520to%2520suffer%2520from%2520two%2520key%2520limitations.%2520First%252C%2520they%2520struggle%2520to%2520handle%2520heterogeneous%2520data%2520noise%252C%2520hindering%2520robust%2520multimodal%2520representation%2520learning.%2520Second%252C%2520they%2520exhibit%2520limited%2520adaptability%2520and%2520generalization%2520when%2520encountering%2520previously%2520unseen%2520noise.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520Test-time%2520Adaptive%2520Hierarchical%2520Co-enhanced%2520Denoising%2520Network%2520%2528TAHCD%2529.%2520On%2520one%2520hand%252C%2520TAHCD%2520introduces%2520the%2520Adaptive%2520Stable%2520Subspace%2520Alignment%2520and%2520Sample-Adaptive%2520Confidence%2520Alignment%2520to%2520reliably%2520remove%2520heterogeneous%2520noise.%2520They%2520account%2520for%2520noise%2520at%2520both%2520global%2520and%2520instance%2520levels%2520and%2520enable%2520jointly%2520removal%2520of%2520modality-specific%2520and%2520cross-modality%2520noise%252C%2520achieving%2520robust%2520learning.%2520On%2520the%2520other%2520hand%252C%2520TAHCD%2520introduces%2520Test-Time%2520Cooperative%2520Enhancement%252C%2520which%2520adaptively%2520updates%2520the%2520model%2520in%2520response%2520to%2520input%2520noise%2520in%2520a%2520label-free%2520manner%252C%2520thus%2520improving%2520generalization.%2520This%2520is%2520achieved%2520by%2520collaboratively%2520enhancing%2520the%2520joint%2520removal%2520process%2520of%2520modality-specific%2520and%2520cross-modality%2520noise%2520across%2520global%2520and%2520instance%2520levels%2520according%2520to%2520sample%2520noise.%2520Experiments%2520on%2520multiple%2520benchmarks%2520demonstrate%2520that%2520the%2520proposed%2520method%2520achieves%2520superior%2520classification%2520performance%252C%2520robustness%252C%2520and%2520generalization%2520compared%2520with%2520state-of-the-art%2520reliable%2520multimodal%2520learning%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07163v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-time%20Adaptive%20Hierarchical%20Co-enhanced%20Denoising%20Network%20for%20Reliable%20Multimodal%20Classification&entry.906535625=Shu%20Shen%20and%20C.%20L.%20Philip%20Chen%20and%20Tong%20Zhang&entry.1292438233=Reliable%20learning%20of%20multimodal%20data%20%28e.g.%2C%20multi-omics%29%20is%20a%20widely%20concerning%20issue%2C%20especially%20in%20safety-critical%20applications%20such%20as%20medical%20diagnosis.%20However%2C%20low-quality%20data%20induced%20by%20multimodal%20noise%20poses%20a%20major%20challenge%20in%20this%20domain%2C%20causing%20existing%20methods%20to%20suffer%20from%20two%20key%20limitations.%20First%2C%20they%20struggle%20to%20handle%20heterogeneous%20data%20noise%2C%20hindering%20robust%20multimodal%20representation%20learning.%20Second%2C%20they%20exhibit%20limited%20adaptability%20and%20generalization%20when%20encountering%20previously%20unseen%20noise.%20To%20address%20these%20issues%2C%20we%20propose%20Test-time%20Adaptive%20Hierarchical%20Co-enhanced%20Denoising%20Network%20%28TAHCD%29.%20On%20one%20hand%2C%20TAHCD%20introduces%20the%20Adaptive%20Stable%20Subspace%20Alignment%20and%20Sample-Adaptive%20Confidence%20Alignment%20to%20reliably%20remove%20heterogeneous%20noise.%20They%20account%20for%20noise%20at%20both%20global%20and%20instance%20levels%20and%20enable%20jointly%20removal%20of%20modality-specific%20and%20cross-modality%20noise%2C%20achieving%20robust%20learning.%20On%20the%20other%20hand%2C%20TAHCD%20introduces%20Test-Time%20Cooperative%20Enhancement%2C%20which%20adaptively%20updates%20the%20model%20in%20response%20to%20input%20noise%20in%20a%20label-free%20manner%2C%20thus%20improving%20generalization.%20This%20is%20achieved%20by%20collaboratively%20enhancing%20the%20joint%20removal%20process%20of%20modality-specific%20and%20cross-modality%20noise%20across%20global%20and%20instance%20levels%20according%20to%20sample%20noise.%20Experiments%20on%20multiple%20benchmarks%20demonstrate%20that%20the%20proposed%20method%20achieves%20superior%20classification%20performance%2C%20robustness%2C%20and%20generalization%20compared%20with%20state-of-the-art%20reliable%20multimodal%20learning%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2601.07163v2&entry.124074799=Read"},
{"title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models", "author": "Wenxuan Huang and Bohan Jia and Zijie Zhai and Shaosheng Cao and Zheyu Ye and Fei Zhao and Zhe Xu and Yao Hu and Shaohui Lin", "abstract": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of $\\sim$6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. Scaling up the amount of multimodal math data in the RL training, Vision-R1-32B and Vison-R1-72B achieves 76.4% and 78.2% MathVista benchmark scores, respectively. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 .", "link": "http://arxiv.org/abs/2503.06749v3", "date": "2026-02-05", "relevancy": 2.2414, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5604}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5604}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-R1%3A%20Incentivizing%20Reasoning%20Capability%20in%20Multimodal%20Large%20Language%20Models&body=Title%3A%20Vision-R1%3A%20Incentivizing%20Reasoning%20Capability%20in%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Wenxuan%20Huang%20and%20Bohan%20Jia%20and%20Zijie%20Zhai%20and%20Shaosheng%20Cao%20and%20Zheyu%20Ye%20and%20Fei%20Zhao%20and%20Zhe%20Xu%20and%20Yao%20Hu%20and%20Shaohui%20Lin%0AAbstract%3A%20DeepSeek-R1-Zero%20has%20successfully%20demonstrated%20the%20emergence%20of%20reasoning%20capabilities%20in%20LLMs%20purely%20through%20Reinforcement%20Learning%20%28RL%29.%20Inspired%20by%20this%20breakthrough%2C%20we%20explore%20how%20RL%20can%20be%20utilized%20to%20enhance%20the%20reasoning%20capability%20of%20MLLMs.%20However%2C%20direct%20training%20with%20RL%20struggles%20to%20activate%20complex%20reasoning%20capabilities%20such%20as%20questioning%20and%20reflection%20in%20MLLMs%2C%20due%20to%20the%20absence%20of%20substantial%20high-quality%20multimodal%20reasoning%20data.%20To%20address%20this%20issue%2C%20we%20propose%20the%20reasoning%20MLLM%2C%20Vision-R1%2C%20to%20improve%20multimodal%20reasoning%20capability.%20Specifically%2C%20we%20first%20construct%20a%20high-quality%20multimodal%20CoT%20dataset%20without%20human%20annotations%20by%20leveraging%20an%20existing%20MLLM%20and%20DeepSeek-R1%20through%20modality%20bridging%20and%20data%20filtering%20to%20obtain%20a%20200K%20multimodal%20CoT%20dataset%2C%20Vision-R1-cold%20dataset.%20It%20serves%20as%20cold-start%20initialization%20data%20for%20Vision-R1.%20To%20mitigate%20the%20optimization%20challenges%20caused%20by%20overthinking%20after%20cold%20start%2C%20we%20propose%20Progressive%20Thinking%20Suppression%20Training%20%28PTST%29%20strategy%20and%20employ%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20with%20the%20hard%20formatting%20result%20reward%20function%20to%20gradually%20refine%20the%20model%27s%20ability%20to%20learn%20correct%20and%20complex%20reasoning%20processes%20on%20a%2010K%20multimodal%20math%20dataset.%20Comprehensive%20experiments%20show%20our%20model%20achieves%20an%20average%20improvement%20of%20%24%5Csim%246%25%20across%20various%20multimodal%20math%20reasoning%20benchmarks.%20Vision-R1-7B%20achieves%20a%2073.5%25%20accuracy%20on%20the%20widely%20used%20MathVista%20benchmark%2C%20which%20is%20only%200.4%25%20lower%20than%20the%20leading%20reasoning%20model%2C%20OpenAI%20O1.%20Scaling%20up%20the%20amount%20of%20multimodal%20math%20data%20in%20the%20RL%20training%2C%20Vision-R1-32B%20and%20Vison-R1-72B%20achieves%2076.4%25%20and%2078.2%25%20MathVista%20benchmark%20scores%2C%20respectively.%20The%20datasets%20and%20code%20will%20be%20released%20in%3A%20https%3A//github.com/Osilly/Vision-R1%20.%0ALink%3A%20http%3A//arxiv.org/abs/2503.06749v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-R1%253A%2520Incentivizing%2520Reasoning%2520Capability%2520in%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DWenxuan%2520Huang%2520and%2520Bohan%2520Jia%2520and%2520Zijie%2520Zhai%2520and%2520Shaosheng%2520Cao%2520and%2520Zheyu%2520Ye%2520and%2520Fei%2520Zhao%2520and%2520Zhe%2520Xu%2520and%2520Yao%2520Hu%2520and%2520Shaohui%2520Lin%26entry.1292438233%3DDeepSeek-R1-Zero%2520has%2520successfully%2520demonstrated%2520the%2520emergence%2520of%2520reasoning%2520capabilities%2520in%2520LLMs%2520purely%2520through%2520Reinforcement%2520Learning%2520%2528RL%2529.%2520Inspired%2520by%2520this%2520breakthrough%252C%2520we%2520explore%2520how%2520RL%2520can%2520be%2520utilized%2520to%2520enhance%2520the%2520reasoning%2520capability%2520of%2520MLLMs.%2520However%252C%2520direct%2520training%2520with%2520RL%2520struggles%2520to%2520activate%2520complex%2520reasoning%2520capabilities%2520such%2520as%2520questioning%2520and%2520reflection%2520in%2520MLLMs%252C%2520due%2520to%2520the%2520absence%2520of%2520substantial%2520high-quality%2520multimodal%2520reasoning%2520data.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520the%2520reasoning%2520MLLM%252C%2520Vision-R1%252C%2520to%2520improve%2520multimodal%2520reasoning%2520capability.%2520Specifically%252C%2520we%2520first%2520construct%2520a%2520high-quality%2520multimodal%2520CoT%2520dataset%2520without%2520human%2520annotations%2520by%2520leveraging%2520an%2520existing%2520MLLM%2520and%2520DeepSeek-R1%2520through%2520modality%2520bridging%2520and%2520data%2520filtering%2520to%2520obtain%2520a%2520200K%2520multimodal%2520CoT%2520dataset%252C%2520Vision-R1-cold%2520dataset.%2520It%2520serves%2520as%2520cold-start%2520initialization%2520data%2520for%2520Vision-R1.%2520To%2520mitigate%2520the%2520optimization%2520challenges%2520caused%2520by%2520overthinking%2520after%2520cold%2520start%252C%2520we%2520propose%2520Progressive%2520Thinking%2520Suppression%2520Training%2520%2528PTST%2529%2520strategy%2520and%2520employ%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520with%2520the%2520hard%2520formatting%2520result%2520reward%2520function%2520to%2520gradually%2520refine%2520the%2520model%2527s%2520ability%2520to%2520learn%2520correct%2520and%2520complex%2520reasoning%2520processes%2520on%2520a%252010K%2520multimodal%2520math%2520dataset.%2520Comprehensive%2520experiments%2520show%2520our%2520model%2520achieves%2520an%2520average%2520improvement%2520of%2520%2524%255Csim%25246%2525%2520across%2520various%2520multimodal%2520math%2520reasoning%2520benchmarks.%2520Vision-R1-7B%2520achieves%2520a%252073.5%2525%2520accuracy%2520on%2520the%2520widely%2520used%2520MathVista%2520benchmark%252C%2520which%2520is%2520only%25200.4%2525%2520lower%2520than%2520the%2520leading%2520reasoning%2520model%252C%2520OpenAI%2520O1.%2520Scaling%2520up%2520the%2520amount%2520of%2520multimodal%2520math%2520data%2520in%2520the%2520RL%2520training%252C%2520Vision-R1-32B%2520and%2520Vison-R1-72B%2520achieves%252076.4%2525%2520and%252078.2%2525%2520MathVista%2520benchmark%2520scores%252C%2520respectively.%2520The%2520datasets%2520and%2520code%2520will%2520be%2520released%2520in%253A%2520https%253A//github.com/Osilly/Vision-R1%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06749v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-R1%3A%20Incentivizing%20Reasoning%20Capability%20in%20Multimodal%20Large%20Language%20Models&entry.906535625=Wenxuan%20Huang%20and%20Bohan%20Jia%20and%20Zijie%20Zhai%20and%20Shaosheng%20Cao%20and%20Zheyu%20Ye%20and%20Fei%20Zhao%20and%20Zhe%20Xu%20and%20Yao%20Hu%20and%20Shaohui%20Lin&entry.1292438233=DeepSeek-R1-Zero%20has%20successfully%20demonstrated%20the%20emergence%20of%20reasoning%20capabilities%20in%20LLMs%20purely%20through%20Reinforcement%20Learning%20%28RL%29.%20Inspired%20by%20this%20breakthrough%2C%20we%20explore%20how%20RL%20can%20be%20utilized%20to%20enhance%20the%20reasoning%20capability%20of%20MLLMs.%20However%2C%20direct%20training%20with%20RL%20struggles%20to%20activate%20complex%20reasoning%20capabilities%20such%20as%20questioning%20and%20reflection%20in%20MLLMs%2C%20due%20to%20the%20absence%20of%20substantial%20high-quality%20multimodal%20reasoning%20data.%20To%20address%20this%20issue%2C%20we%20propose%20the%20reasoning%20MLLM%2C%20Vision-R1%2C%20to%20improve%20multimodal%20reasoning%20capability.%20Specifically%2C%20we%20first%20construct%20a%20high-quality%20multimodal%20CoT%20dataset%20without%20human%20annotations%20by%20leveraging%20an%20existing%20MLLM%20and%20DeepSeek-R1%20through%20modality%20bridging%20and%20data%20filtering%20to%20obtain%20a%20200K%20multimodal%20CoT%20dataset%2C%20Vision-R1-cold%20dataset.%20It%20serves%20as%20cold-start%20initialization%20data%20for%20Vision-R1.%20To%20mitigate%20the%20optimization%20challenges%20caused%20by%20overthinking%20after%20cold%20start%2C%20we%20propose%20Progressive%20Thinking%20Suppression%20Training%20%28PTST%29%20strategy%20and%20employ%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20with%20the%20hard%20formatting%20result%20reward%20function%20to%20gradually%20refine%20the%20model%27s%20ability%20to%20learn%20correct%20and%20complex%20reasoning%20processes%20on%20a%2010K%20multimodal%20math%20dataset.%20Comprehensive%20experiments%20show%20our%20model%20achieves%20an%20average%20improvement%20of%20%24%5Csim%246%25%20across%20various%20multimodal%20math%20reasoning%20benchmarks.%20Vision-R1-7B%20achieves%20a%2073.5%25%20accuracy%20on%20the%20widely%20used%20MathVista%20benchmark%2C%20which%20is%20only%200.4%25%20lower%20than%20the%20leading%20reasoning%20model%2C%20OpenAI%20O1.%20Scaling%20up%20the%20amount%20of%20multimodal%20math%20data%20in%20the%20RL%20training%2C%20Vision-R1-32B%20and%20Vison-R1-72B%20achieves%2076.4%25%20and%2078.2%25%20MathVista%20benchmark%20scores%2C%20respectively.%20The%20datasets%20and%20code%20will%20be%20released%20in%3A%20https%3A//github.com/Osilly/Vision-R1%20.&entry.1838667208=http%3A//arxiv.org/abs/2503.06749v3&entry.124074799=Read"},
{"title": "CAViT -- Channel-Aware Vision Transformer for Dynamic Feature Fusion", "author": "Aon Safdar and Mohamed Saadeldin", "abstract": "Vision Transformers (ViTs) have demonstrated strong performance across a range of computer vision tasks by modeling long-range spatial interactions via self-attention. However, channel-wise mixing in ViTs remains static, relying on fixed multilayer perceptrons (MLPs) that lack adaptability to input content. We introduce 'CAViT', a dual-attention architecture that replaces the static MLP with a dynamic, attention-based mechanism for feature interaction. Each Transformer block in CAViT performs spatial self-attention followed by channel-wise self-attention, allowing the model to dynamically recalibrate feature representations based on global image context. This unified and content-aware token mixing strategy enhances representational expressiveness without increasing depth or complexity. We validate CAViT across five benchmark datasets spanning both natural and medical domains, where it outperforms the standard ViT baseline by up to +3.6% in accuracy, while reducing parameter count and FLOPs by over 30%. Qualitative attention maps reveal sharper and semantically meaningful activation patterns, validating the effectiveness of our attention-driven token mixing.", "link": "http://arxiv.org/abs/2602.05598v1", "date": "2026-02-05", "relevancy": 2.2408, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5699}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5558}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAViT%20--%20Channel-Aware%20Vision%20Transformer%20for%20Dynamic%20Feature%20Fusion&body=Title%3A%20CAViT%20--%20Channel-Aware%20Vision%20Transformer%20for%20Dynamic%20Feature%20Fusion%0AAuthor%3A%20Aon%20Safdar%20and%20Mohamed%20Saadeldin%0AAbstract%3A%20Vision%20Transformers%20%28ViTs%29%20have%20demonstrated%20strong%20performance%20across%20a%20range%20of%20computer%20vision%20tasks%20by%20modeling%20long-range%20spatial%20interactions%20via%20self-attention.%20However%2C%20channel-wise%20mixing%20in%20ViTs%20remains%20static%2C%20relying%20on%20fixed%20multilayer%20perceptrons%20%28MLPs%29%20that%20lack%20adaptability%20to%20input%20content.%20We%20introduce%20%27CAViT%27%2C%20a%20dual-attention%20architecture%20that%20replaces%20the%20static%20MLP%20with%20a%20dynamic%2C%20attention-based%20mechanism%20for%20feature%20interaction.%20Each%20Transformer%20block%20in%20CAViT%20performs%20spatial%20self-attention%20followed%20by%20channel-wise%20self-attention%2C%20allowing%20the%20model%20to%20dynamically%20recalibrate%20feature%20representations%20based%20on%20global%20image%20context.%20This%20unified%20and%20content-aware%20token%20mixing%20strategy%20enhances%20representational%20expressiveness%20without%20increasing%20depth%20or%20complexity.%20We%20validate%20CAViT%20across%20five%20benchmark%20datasets%20spanning%20both%20natural%20and%20medical%20domains%2C%20where%20it%20outperforms%20the%20standard%20ViT%20baseline%20by%20up%20to%20%2B3.6%25%20in%20accuracy%2C%20while%20reducing%20parameter%20count%20and%20FLOPs%20by%20over%2030%25.%20Qualitative%20attention%20maps%20reveal%20sharper%20and%20semantically%20meaningful%20activation%20patterns%2C%20validating%20the%20effectiveness%20of%20our%20attention-driven%20token%20mixing.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAViT%2520--%2520Channel-Aware%2520Vision%2520Transformer%2520for%2520Dynamic%2520Feature%2520Fusion%26entry.906535625%3DAon%2520Safdar%2520and%2520Mohamed%2520Saadeldin%26entry.1292438233%3DVision%2520Transformers%2520%2528ViTs%2529%2520have%2520demonstrated%2520strong%2520performance%2520across%2520a%2520range%2520of%2520computer%2520vision%2520tasks%2520by%2520modeling%2520long-range%2520spatial%2520interactions%2520via%2520self-attention.%2520However%252C%2520channel-wise%2520mixing%2520in%2520ViTs%2520remains%2520static%252C%2520relying%2520on%2520fixed%2520multilayer%2520perceptrons%2520%2528MLPs%2529%2520that%2520lack%2520adaptability%2520to%2520input%2520content.%2520We%2520introduce%2520%2527CAViT%2527%252C%2520a%2520dual-attention%2520architecture%2520that%2520replaces%2520the%2520static%2520MLP%2520with%2520a%2520dynamic%252C%2520attention-based%2520mechanism%2520for%2520feature%2520interaction.%2520Each%2520Transformer%2520block%2520in%2520CAViT%2520performs%2520spatial%2520self-attention%2520followed%2520by%2520channel-wise%2520self-attention%252C%2520allowing%2520the%2520model%2520to%2520dynamically%2520recalibrate%2520feature%2520representations%2520based%2520on%2520global%2520image%2520context.%2520This%2520unified%2520and%2520content-aware%2520token%2520mixing%2520strategy%2520enhances%2520representational%2520expressiveness%2520without%2520increasing%2520depth%2520or%2520complexity.%2520We%2520validate%2520CAViT%2520across%2520five%2520benchmark%2520datasets%2520spanning%2520both%2520natural%2520and%2520medical%2520domains%252C%2520where%2520it%2520outperforms%2520the%2520standard%2520ViT%2520baseline%2520by%2520up%2520to%2520%252B3.6%2525%2520in%2520accuracy%252C%2520while%2520reducing%2520parameter%2520count%2520and%2520FLOPs%2520by%2520over%252030%2525.%2520Qualitative%2520attention%2520maps%2520reveal%2520sharper%2520and%2520semantically%2520meaningful%2520activation%2520patterns%252C%2520validating%2520the%2520effectiveness%2520of%2520our%2520attention-driven%2520token%2520mixing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAViT%20--%20Channel-Aware%20Vision%20Transformer%20for%20Dynamic%20Feature%20Fusion&entry.906535625=Aon%20Safdar%20and%20Mohamed%20Saadeldin&entry.1292438233=Vision%20Transformers%20%28ViTs%29%20have%20demonstrated%20strong%20performance%20across%20a%20range%20of%20computer%20vision%20tasks%20by%20modeling%20long-range%20spatial%20interactions%20via%20self-attention.%20However%2C%20channel-wise%20mixing%20in%20ViTs%20remains%20static%2C%20relying%20on%20fixed%20multilayer%20perceptrons%20%28MLPs%29%20that%20lack%20adaptability%20to%20input%20content.%20We%20introduce%20%27CAViT%27%2C%20a%20dual-attention%20architecture%20that%20replaces%20the%20static%20MLP%20with%20a%20dynamic%2C%20attention-based%20mechanism%20for%20feature%20interaction.%20Each%20Transformer%20block%20in%20CAViT%20performs%20spatial%20self-attention%20followed%20by%20channel-wise%20self-attention%2C%20allowing%20the%20model%20to%20dynamically%20recalibrate%20feature%20representations%20based%20on%20global%20image%20context.%20This%20unified%20and%20content-aware%20token%20mixing%20strategy%20enhances%20representational%20expressiveness%20without%20increasing%20depth%20or%20complexity.%20We%20validate%20CAViT%20across%20five%20benchmark%20datasets%20spanning%20both%20natural%20and%20medical%20domains%2C%20where%20it%20outperforms%20the%20standard%20ViT%20baseline%20by%20up%20to%20%2B3.6%25%20in%20accuracy%2C%20while%20reducing%20parameter%20count%20and%20FLOPs%20by%20over%2030%25.%20Qualitative%20attention%20maps%20reveal%20sharper%20and%20semantically%20meaningful%20activation%20patterns%2C%20validating%20the%20effectiveness%20of%20our%20attention-driven%20token%20mixing.&entry.1838667208=http%3A//arxiv.org/abs/2602.05598v1&entry.124074799=Read"},
{"title": "Visuo-Tactile World Models", "author": "Carolina Higuera and Sergio Arnaud and Byron Boots and Mustafa Mukadam and Francois Robert Hogan and Franziska Meier", "abstract": "We introduce multi-task Visuo-Tactile World Models (VT-WM), which capture the physics of contact through touch reasoning. By complementing vision with tactile sensing, VT-WM better understands robot-object interactions in contact-rich tasks, avoiding common failure modes of vision-only models under occlusion or ambiguous contact states, such as objects disappearing, teleporting, or moving in ways that violate basic physics. Trained across a set of contact-rich manipulation tasks, VT-WM improves physical fidelity in imagination, achieving 33% better performance at maintaining object permanence and 29% better compliance with the laws of motion in autoregressive rollouts. Moreover, experiments show that grounding in contact dynamics also translates to planning. In zero-shot real-robot experiments, VT-WM achieves up to 35% higher success rates, with the largest gains in multi-step, contact-rich tasks. Finally, VT-WM demonstrates significant downstream versatility, effectively adapting its learned contact dynamics to a novel task and achieving reliable planning success with only a limited set of demonstrations.", "link": "http://arxiv.org/abs/2602.06001v1", "date": "2026-02-05", "relevancy": 2.239, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5995}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5649}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visuo-Tactile%20World%20Models&body=Title%3A%20Visuo-Tactile%20World%20Models%0AAuthor%3A%20Carolina%20Higuera%20and%20Sergio%20Arnaud%20and%20Byron%20Boots%20and%20Mustafa%20Mukadam%20and%20Francois%20Robert%20Hogan%20and%20Franziska%20Meier%0AAbstract%3A%20We%20introduce%20multi-task%20Visuo-Tactile%20World%20Models%20%28VT-WM%29%2C%20which%20capture%20the%20physics%20of%20contact%20through%20touch%20reasoning.%20By%20complementing%20vision%20with%20tactile%20sensing%2C%20VT-WM%20better%20understands%20robot-object%20interactions%20in%20contact-rich%20tasks%2C%20avoiding%20common%20failure%20modes%20of%20vision-only%20models%20under%20occlusion%20or%20ambiguous%20contact%20states%2C%20such%20as%20objects%20disappearing%2C%20teleporting%2C%20or%20moving%20in%20ways%20that%20violate%20basic%20physics.%20Trained%20across%20a%20set%20of%20contact-rich%20manipulation%20tasks%2C%20VT-WM%20improves%20physical%20fidelity%20in%20imagination%2C%20achieving%2033%25%20better%20performance%20at%20maintaining%20object%20permanence%20and%2029%25%20better%20compliance%20with%20the%20laws%20of%20motion%20in%20autoregressive%20rollouts.%20Moreover%2C%20experiments%20show%20that%20grounding%20in%20contact%20dynamics%20also%20translates%20to%20planning.%20In%20zero-shot%20real-robot%20experiments%2C%20VT-WM%20achieves%20up%20to%2035%25%20higher%20success%20rates%2C%20with%20the%20largest%20gains%20in%20multi-step%2C%20contact-rich%20tasks.%20Finally%2C%20VT-WM%20demonstrates%20significant%20downstream%20versatility%2C%20effectively%20adapting%20its%20learned%20contact%20dynamics%20to%20a%20novel%20task%20and%20achieving%20reliable%20planning%20success%20with%20only%20a%20limited%20set%20of%20demonstrations.%0ALink%3A%20http%3A//arxiv.org/abs/2602.06001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisuo-Tactile%2520World%2520Models%26entry.906535625%3DCarolina%2520Higuera%2520and%2520Sergio%2520Arnaud%2520and%2520Byron%2520Boots%2520and%2520Mustafa%2520Mukadam%2520and%2520Francois%2520Robert%2520Hogan%2520and%2520Franziska%2520Meier%26entry.1292438233%3DWe%2520introduce%2520multi-task%2520Visuo-Tactile%2520World%2520Models%2520%2528VT-WM%2529%252C%2520which%2520capture%2520the%2520physics%2520of%2520contact%2520through%2520touch%2520reasoning.%2520By%2520complementing%2520vision%2520with%2520tactile%2520sensing%252C%2520VT-WM%2520better%2520understands%2520robot-object%2520interactions%2520in%2520contact-rich%2520tasks%252C%2520avoiding%2520common%2520failure%2520modes%2520of%2520vision-only%2520models%2520under%2520occlusion%2520or%2520ambiguous%2520contact%2520states%252C%2520such%2520as%2520objects%2520disappearing%252C%2520teleporting%252C%2520or%2520moving%2520in%2520ways%2520that%2520violate%2520basic%2520physics.%2520Trained%2520across%2520a%2520set%2520of%2520contact-rich%2520manipulation%2520tasks%252C%2520VT-WM%2520improves%2520physical%2520fidelity%2520in%2520imagination%252C%2520achieving%252033%2525%2520better%2520performance%2520at%2520maintaining%2520object%2520permanence%2520and%252029%2525%2520better%2520compliance%2520with%2520the%2520laws%2520of%2520motion%2520in%2520autoregressive%2520rollouts.%2520Moreover%252C%2520experiments%2520show%2520that%2520grounding%2520in%2520contact%2520dynamics%2520also%2520translates%2520to%2520planning.%2520In%2520zero-shot%2520real-robot%2520experiments%252C%2520VT-WM%2520achieves%2520up%2520to%252035%2525%2520higher%2520success%2520rates%252C%2520with%2520the%2520largest%2520gains%2520in%2520multi-step%252C%2520contact-rich%2520tasks.%2520Finally%252C%2520VT-WM%2520demonstrates%2520significant%2520downstream%2520versatility%252C%2520effectively%2520adapting%2520its%2520learned%2520contact%2520dynamics%2520to%2520a%2520novel%2520task%2520and%2520achieving%2520reliable%2520planning%2520success%2520with%2520only%2520a%2520limited%2520set%2520of%2520demonstrations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.06001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visuo-Tactile%20World%20Models&entry.906535625=Carolina%20Higuera%20and%20Sergio%20Arnaud%20and%20Byron%20Boots%20and%20Mustafa%20Mukadam%20and%20Francois%20Robert%20Hogan%20and%20Franziska%20Meier&entry.1292438233=We%20introduce%20multi-task%20Visuo-Tactile%20World%20Models%20%28VT-WM%29%2C%20which%20capture%20the%20physics%20of%20contact%20through%20touch%20reasoning.%20By%20complementing%20vision%20with%20tactile%20sensing%2C%20VT-WM%20better%20understands%20robot-object%20interactions%20in%20contact-rich%20tasks%2C%20avoiding%20common%20failure%20modes%20of%20vision-only%20models%20under%20occlusion%20or%20ambiguous%20contact%20states%2C%20such%20as%20objects%20disappearing%2C%20teleporting%2C%20or%20moving%20in%20ways%20that%20violate%20basic%20physics.%20Trained%20across%20a%20set%20of%20contact-rich%20manipulation%20tasks%2C%20VT-WM%20improves%20physical%20fidelity%20in%20imagination%2C%20achieving%2033%25%20better%20performance%20at%20maintaining%20object%20permanence%20and%2029%25%20better%20compliance%20with%20the%20laws%20of%20motion%20in%20autoregressive%20rollouts.%20Moreover%2C%20experiments%20show%20that%20grounding%20in%20contact%20dynamics%20also%20translates%20to%20planning.%20In%20zero-shot%20real-robot%20experiments%2C%20VT-WM%20achieves%20up%20to%2035%25%20higher%20success%20rates%2C%20with%20the%20largest%20gains%20in%20multi-step%2C%20contact-rich%20tasks.%20Finally%2C%20VT-WM%20demonstrates%20significant%20downstream%20versatility%2C%20effectively%20adapting%20its%20learned%20contact%20dynamics%20to%20a%20novel%20task%20and%20achieving%20reliable%20planning%20success%20with%20only%20a%20limited%20set%20of%20demonstrations.&entry.1838667208=http%3A//arxiv.org/abs/2602.06001v1&entry.124074799=Read"},
{"title": "RAG4Tickets: AI-Powered Ticket Resolution via Retrieval-Augmented Generation on JIRA and GitHub Data", "author": "Mohammad Baqar", "abstract": "Modern software teams frequently encounter delays in resolving recurring or related issues due to fragmented knowledge scattered across JIRA tickets, developer discussions, and GitHub pull requests (PRs). To address this challenge, we propose a Retrieval-Augmented Generation (RAG) framework that integrates Sentence-Transformers for semantic embeddings with FAISS-based vector search to deliver context-aware ticket resolution recommendations. The approach embeds historical JIRA tickets, user comments, and linked PR metadata to retrieve semantically similar past cases, which are then synthesized by a Large Language Model (LLM) into grounded and explainable resolution suggestions. The framework contributes a unified pipeline linking JIRA and GitHub data, an embedding and FAISS indexing strategy for heterogeneous software artifacts, and a resolution generation module guided by retrieved evidence. Experimental evaluation using precision, recall, resolution time reduction, and developer acceptance metrics shows that the proposed system significantly improves resolution accuracy, fix quality, and knowledge reuse in modern DevOps environments.", "link": "http://arxiv.org/abs/2510.08667v2", "date": "2026-02-05", "relevancy": 2.2388, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4523}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4503}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAG4Tickets%3A%20AI-Powered%20Ticket%20Resolution%20via%20Retrieval-Augmented%20Generation%20on%20JIRA%20and%20GitHub%20Data&body=Title%3A%20RAG4Tickets%3A%20AI-Powered%20Ticket%20Resolution%20via%20Retrieval-Augmented%20Generation%20on%20JIRA%20and%20GitHub%20Data%0AAuthor%3A%20Mohammad%20Baqar%0AAbstract%3A%20Modern%20software%20teams%20frequently%20encounter%20delays%20in%20resolving%20recurring%20or%20related%20issues%20due%20to%20fragmented%20knowledge%20scattered%20across%20JIRA%20tickets%2C%20developer%20discussions%2C%20and%20GitHub%20pull%20requests%20%28PRs%29.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20Retrieval-Augmented%20Generation%20%28RAG%29%20framework%20that%20integrates%20Sentence-Transformers%20for%20semantic%20embeddings%20with%20FAISS-based%20vector%20search%20to%20deliver%20context-aware%20ticket%20resolution%20recommendations.%20The%20approach%20embeds%20historical%20JIRA%20tickets%2C%20user%20comments%2C%20and%20linked%20PR%20metadata%20to%20retrieve%20semantically%20similar%20past%20cases%2C%20which%20are%20then%20synthesized%20by%20a%20Large%20Language%20Model%20%28LLM%29%20into%20grounded%20and%20explainable%20resolution%20suggestions.%20The%20framework%20contributes%20a%20unified%20pipeline%20linking%20JIRA%20and%20GitHub%20data%2C%20an%20embedding%20and%20FAISS%20indexing%20strategy%20for%20heterogeneous%20software%20artifacts%2C%20and%20a%20resolution%20generation%20module%20guided%20by%20retrieved%20evidence.%20Experimental%20evaluation%20using%20precision%2C%20recall%2C%20resolution%20time%20reduction%2C%20and%20developer%20acceptance%20metrics%20shows%20that%20the%20proposed%20system%20significantly%20improves%20resolution%20accuracy%2C%20fix%20quality%2C%20and%20knowledge%20reuse%20in%20modern%20DevOps%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2510.08667v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAG4Tickets%253A%2520AI-Powered%2520Ticket%2520Resolution%2520via%2520Retrieval-Augmented%2520Generation%2520on%2520JIRA%2520and%2520GitHub%2520Data%26entry.906535625%3DMohammad%2520Baqar%26entry.1292438233%3DModern%2520software%2520teams%2520frequently%2520encounter%2520delays%2520in%2520resolving%2520recurring%2520or%2520related%2520issues%2520due%2520to%2520fragmented%2520knowledge%2520scattered%2520across%2520JIRA%2520tickets%252C%2520developer%2520discussions%252C%2520and%2520GitHub%2520pull%2520requests%2520%2528PRs%2529.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520framework%2520that%2520integrates%2520Sentence-Transformers%2520for%2520semantic%2520embeddings%2520with%2520FAISS-based%2520vector%2520search%2520to%2520deliver%2520context-aware%2520ticket%2520resolution%2520recommendations.%2520The%2520approach%2520embeds%2520historical%2520JIRA%2520tickets%252C%2520user%2520comments%252C%2520and%2520linked%2520PR%2520metadata%2520to%2520retrieve%2520semantically%2520similar%2520past%2520cases%252C%2520which%2520are%2520then%2520synthesized%2520by%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520into%2520grounded%2520and%2520explainable%2520resolution%2520suggestions.%2520The%2520framework%2520contributes%2520a%2520unified%2520pipeline%2520linking%2520JIRA%2520and%2520GitHub%2520data%252C%2520an%2520embedding%2520and%2520FAISS%2520indexing%2520strategy%2520for%2520heterogeneous%2520software%2520artifacts%252C%2520and%2520a%2520resolution%2520generation%2520module%2520guided%2520by%2520retrieved%2520evidence.%2520Experimental%2520evaluation%2520using%2520precision%252C%2520recall%252C%2520resolution%2520time%2520reduction%252C%2520and%2520developer%2520acceptance%2520metrics%2520shows%2520that%2520the%2520proposed%2520system%2520significantly%2520improves%2520resolution%2520accuracy%252C%2520fix%2520quality%252C%2520and%2520knowledge%2520reuse%2520in%2520modern%2520DevOps%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08667v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAG4Tickets%3A%20AI-Powered%20Ticket%20Resolution%20via%20Retrieval-Augmented%20Generation%20on%20JIRA%20and%20GitHub%20Data&entry.906535625=Mohammad%20Baqar&entry.1292438233=Modern%20software%20teams%20frequently%20encounter%20delays%20in%20resolving%20recurring%20or%20related%20issues%20due%20to%20fragmented%20knowledge%20scattered%20across%20JIRA%20tickets%2C%20developer%20discussions%2C%20and%20GitHub%20pull%20requests%20%28PRs%29.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20Retrieval-Augmented%20Generation%20%28RAG%29%20framework%20that%20integrates%20Sentence-Transformers%20for%20semantic%20embeddings%20with%20FAISS-based%20vector%20search%20to%20deliver%20context-aware%20ticket%20resolution%20recommendations.%20The%20approach%20embeds%20historical%20JIRA%20tickets%2C%20user%20comments%2C%20and%20linked%20PR%20metadata%20to%20retrieve%20semantically%20similar%20past%20cases%2C%20which%20are%20then%20synthesized%20by%20a%20Large%20Language%20Model%20%28LLM%29%20into%20grounded%20and%20explainable%20resolution%20suggestions.%20The%20framework%20contributes%20a%20unified%20pipeline%20linking%20JIRA%20and%20GitHub%20data%2C%20an%20embedding%20and%20FAISS%20indexing%20strategy%20for%20heterogeneous%20software%20artifacts%2C%20and%20a%20resolution%20generation%20module%20guided%20by%20retrieved%20evidence.%20Experimental%20evaluation%20using%20precision%2C%20recall%2C%20resolution%20time%20reduction%2C%20and%20developer%20acceptance%20metrics%20shows%20that%20the%20proposed%20system%20significantly%20improves%20resolution%20accuracy%2C%20fix%20quality%2C%20and%20knowledge%20reuse%20in%20modern%20DevOps%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2510.08667v2&entry.124074799=Read"},
{"title": "Data Heterogeneity and Forgotten Labels in Split Federated Learning", "author": "Joana Tirana and Dimitra Tsigkari and David Solans Noguero and Nicolas Kourtellis", "abstract": "In Split Federated Learning (SFL), the clients collaboratively train a model with the help of a server by splitting the model into two parts. Part-1 is trained locally at each client and aggregated by the aggregator at the end of each round. Part-2 is trained at a server that sequentially processes the intermediate activations received from each client. We study the phenomenon of catastrophic forgetting (CF) in SFL in the presence of data heterogeneity. In detail, due to the nature of SFL, local updates of part-1 may drift away from global optima, while part-2 is sensitive to the processing sequence, similar to forgetting in continual learning (CL). Specifically, we observe that the trained model performs better in classes (labels) seen at the end of the sequence. We investigate this phenomenon with emphasis on key aspects of SFL, such as the processing order at the server and the cut layer. Based on our findings, we propose Hydra, a novel mitigation method inspired by multi-head neural networks and adapted for the SFL setting. Extensive numerical evaluations show that Hydra outperforms baselines and methods from the literature.", "link": "http://arxiv.org/abs/2511.09736v2", "date": "2026-02-05", "relevancy": 2.2258, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4552}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4486}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Heterogeneity%20and%20Forgotten%20Labels%20in%20Split%20Federated%20Learning&body=Title%3A%20Data%20Heterogeneity%20and%20Forgotten%20Labels%20in%20Split%20Federated%20Learning%0AAuthor%3A%20Joana%20Tirana%20and%20Dimitra%20Tsigkari%20and%20David%20Solans%20Noguero%20and%20Nicolas%20Kourtellis%0AAbstract%3A%20In%20Split%20Federated%20Learning%20%28SFL%29%2C%20the%20clients%20collaboratively%20train%20a%20model%20with%20the%20help%20of%20a%20server%20by%20splitting%20the%20model%20into%20two%20parts.%20Part-1%20is%20trained%20locally%20at%20each%20client%20and%20aggregated%20by%20the%20aggregator%20at%20the%20end%20of%20each%20round.%20Part-2%20is%20trained%20at%20a%20server%20that%20sequentially%20processes%20the%20intermediate%20activations%20received%20from%20each%20client.%20We%20study%20the%20phenomenon%20of%20catastrophic%20forgetting%20%28CF%29%20in%20SFL%20in%20the%20presence%20of%20data%20heterogeneity.%20In%20detail%2C%20due%20to%20the%20nature%20of%20SFL%2C%20local%20updates%20of%20part-1%20may%20drift%20away%20from%20global%20optima%2C%20while%20part-2%20is%20sensitive%20to%20the%20processing%20sequence%2C%20similar%20to%20forgetting%20in%20continual%20learning%20%28CL%29.%20Specifically%2C%20we%20observe%20that%20the%20trained%20model%20performs%20better%20in%20classes%20%28labels%29%20seen%20at%20the%20end%20of%20the%20sequence.%20We%20investigate%20this%20phenomenon%20with%20emphasis%20on%20key%20aspects%20of%20SFL%2C%20such%20as%20the%20processing%20order%20at%20the%20server%20and%20the%20cut%20layer.%20Based%20on%20our%20findings%2C%20we%20propose%20Hydra%2C%20a%20novel%20mitigation%20method%20inspired%20by%20multi-head%20neural%20networks%20and%20adapted%20for%20the%20SFL%20setting.%20Extensive%20numerical%20evaluations%20show%20that%20Hydra%20outperforms%20baselines%20and%20methods%20from%20the%20literature.%0ALink%3A%20http%3A//arxiv.org/abs/2511.09736v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Heterogeneity%2520and%2520Forgotten%2520Labels%2520in%2520Split%2520Federated%2520Learning%26entry.906535625%3DJoana%2520Tirana%2520and%2520Dimitra%2520Tsigkari%2520and%2520David%2520Solans%2520Noguero%2520and%2520Nicolas%2520Kourtellis%26entry.1292438233%3DIn%2520Split%2520Federated%2520Learning%2520%2528SFL%2529%252C%2520the%2520clients%2520collaboratively%2520train%2520a%2520model%2520with%2520the%2520help%2520of%2520a%2520server%2520by%2520splitting%2520the%2520model%2520into%2520two%2520parts.%2520Part-1%2520is%2520trained%2520locally%2520at%2520each%2520client%2520and%2520aggregated%2520by%2520the%2520aggregator%2520at%2520the%2520end%2520of%2520each%2520round.%2520Part-2%2520is%2520trained%2520at%2520a%2520server%2520that%2520sequentially%2520processes%2520the%2520intermediate%2520activations%2520received%2520from%2520each%2520client.%2520We%2520study%2520the%2520phenomenon%2520of%2520catastrophic%2520forgetting%2520%2528CF%2529%2520in%2520SFL%2520in%2520the%2520presence%2520of%2520data%2520heterogeneity.%2520In%2520detail%252C%2520due%2520to%2520the%2520nature%2520of%2520SFL%252C%2520local%2520updates%2520of%2520part-1%2520may%2520drift%2520away%2520from%2520global%2520optima%252C%2520while%2520part-2%2520is%2520sensitive%2520to%2520the%2520processing%2520sequence%252C%2520similar%2520to%2520forgetting%2520in%2520continual%2520learning%2520%2528CL%2529.%2520Specifically%252C%2520we%2520observe%2520that%2520the%2520trained%2520model%2520performs%2520better%2520in%2520classes%2520%2528labels%2529%2520seen%2520at%2520the%2520end%2520of%2520the%2520sequence.%2520We%2520investigate%2520this%2520phenomenon%2520with%2520emphasis%2520on%2520key%2520aspects%2520of%2520SFL%252C%2520such%2520as%2520the%2520processing%2520order%2520at%2520the%2520server%2520and%2520the%2520cut%2520layer.%2520Based%2520on%2520our%2520findings%252C%2520we%2520propose%2520Hydra%252C%2520a%2520novel%2520mitigation%2520method%2520inspired%2520by%2520multi-head%2520neural%2520networks%2520and%2520adapted%2520for%2520the%2520SFL%2520setting.%2520Extensive%2520numerical%2520evaluations%2520show%2520that%2520Hydra%2520outperforms%2520baselines%2520and%2520methods%2520from%2520the%2520literature.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.09736v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Heterogeneity%20and%20Forgotten%20Labels%20in%20Split%20Federated%20Learning&entry.906535625=Joana%20Tirana%20and%20Dimitra%20Tsigkari%20and%20David%20Solans%20Noguero%20and%20Nicolas%20Kourtellis&entry.1292438233=In%20Split%20Federated%20Learning%20%28SFL%29%2C%20the%20clients%20collaboratively%20train%20a%20model%20with%20the%20help%20of%20a%20server%20by%20splitting%20the%20model%20into%20two%20parts.%20Part-1%20is%20trained%20locally%20at%20each%20client%20and%20aggregated%20by%20the%20aggregator%20at%20the%20end%20of%20each%20round.%20Part-2%20is%20trained%20at%20a%20server%20that%20sequentially%20processes%20the%20intermediate%20activations%20received%20from%20each%20client.%20We%20study%20the%20phenomenon%20of%20catastrophic%20forgetting%20%28CF%29%20in%20SFL%20in%20the%20presence%20of%20data%20heterogeneity.%20In%20detail%2C%20due%20to%20the%20nature%20of%20SFL%2C%20local%20updates%20of%20part-1%20may%20drift%20away%20from%20global%20optima%2C%20while%20part-2%20is%20sensitive%20to%20the%20processing%20sequence%2C%20similar%20to%20forgetting%20in%20continual%20learning%20%28CL%29.%20Specifically%2C%20we%20observe%20that%20the%20trained%20model%20performs%20better%20in%20classes%20%28labels%29%20seen%20at%20the%20end%20of%20the%20sequence.%20We%20investigate%20this%20phenomenon%20with%20emphasis%20on%20key%20aspects%20of%20SFL%2C%20such%20as%20the%20processing%20order%20at%20the%20server%20and%20the%20cut%20layer.%20Based%20on%20our%20findings%2C%20we%20propose%20Hydra%2C%20a%20novel%20mitigation%20method%20inspired%20by%20multi-head%20neural%20networks%20and%20adapted%20for%20the%20SFL%20setting.%20Extensive%20numerical%20evaluations%20show%20that%20Hydra%20outperforms%20baselines%20and%20methods%20from%20the%20literature.&entry.1838667208=http%3A//arxiv.org/abs/2511.09736v2&entry.124074799=Read"},
{"title": "Progressive multi-fidelity learning with neural networks for physical system predictions", "author": "Paolo Conti and Mengwu Guo and Attilio Frangi and Andrea Manzoni", "abstract": "Highly accurate datasets from numerical or physical experiments are often expensive and time-consuming to acquire, posing a significant challenge for applications that require precise evaluations, potentially across multiple scenarios and in real-time. Even building sufficiently accurate surrogate models can be extremely challenging with limited high-fidelity data. Conversely, less expensive, low-fidelity data can be computed more easily and encompass a broader range of scenarios. By leveraging multi-fidelity information, prediction capabilities of surrogates can be improved. However, in practical situations, data may be different in types, come from sources of different modalities, and not be concurrently available, further complicating the modeling process. To address these challenges, we introduce a progressive multi-fidelity surrogate model. This model can sequentially incorporate diverse data types using tailored encoders. Multi-fidelity regression from the encoded inputs to the target quantities of interest is then performed using neural networks. Input information progressively flows from lower to higher fidelity levels through two sets of connections: concatenations among all the encoded inputs, and additive connections among the final outputs. This dual connection system enables the model to exploit correlations among different datasets while ensuring that each level makes an additive correction to the previous level without altering it. This approach prevents performance degradation as new input data are integrated into the model and automatically adapts predictions based on the available inputs. We demonstrate the effectiveness of the approach on numerical benchmarks and a real-world case study, showing that it reliably integrates multi-modal data and provides accurate predictions, maintaining performance when generalizing across time and parameter variations.", "link": "http://arxiv.org/abs/2510.13762v2", "date": "2026-02-05", "relevancy": 2.2256, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6196}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5743}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20multi-fidelity%20learning%20with%20neural%20networks%20for%20physical%20system%20predictions&body=Title%3A%20Progressive%20multi-fidelity%20learning%20with%20neural%20networks%20for%20physical%20system%20predictions%0AAuthor%3A%20Paolo%20Conti%20and%20Mengwu%20Guo%20and%20Attilio%20Frangi%20and%20Andrea%20Manzoni%0AAbstract%3A%20Highly%20accurate%20datasets%20from%20numerical%20or%20physical%20experiments%20are%20often%20expensive%20and%20time-consuming%20to%20acquire%2C%20posing%20a%20significant%20challenge%20for%20applications%20that%20require%20precise%20evaluations%2C%20potentially%20across%20multiple%20scenarios%20and%20in%20real-time.%20Even%20building%20sufficiently%20accurate%20surrogate%20models%20can%20be%20extremely%20challenging%20with%20limited%20high-fidelity%20data.%20Conversely%2C%20less%20expensive%2C%20low-fidelity%20data%20can%20be%20computed%20more%20easily%20and%20encompass%20a%20broader%20range%20of%20scenarios.%20By%20leveraging%20multi-fidelity%20information%2C%20prediction%20capabilities%20of%20surrogates%20can%20be%20improved.%20However%2C%20in%20practical%20situations%2C%20data%20may%20be%20different%20in%20types%2C%20come%20from%20sources%20of%20different%20modalities%2C%20and%20not%20be%20concurrently%20available%2C%20further%20complicating%20the%20modeling%20process.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20progressive%20multi-fidelity%20surrogate%20model.%20This%20model%20can%20sequentially%20incorporate%20diverse%20data%20types%20using%20tailored%20encoders.%20Multi-fidelity%20regression%20from%20the%20encoded%20inputs%20to%20the%20target%20quantities%20of%20interest%20is%20then%20performed%20using%20neural%20networks.%20Input%20information%20progressively%20flows%20from%20lower%20to%20higher%20fidelity%20levels%20through%20two%20sets%20of%20connections%3A%20concatenations%20among%20all%20the%20encoded%20inputs%2C%20and%20additive%20connections%20among%20the%20final%20outputs.%20This%20dual%20connection%20system%20enables%20the%20model%20to%20exploit%20correlations%20among%20different%20datasets%20while%20ensuring%20that%20each%20level%20makes%20an%20additive%20correction%20to%20the%20previous%20level%20without%20altering%20it.%20This%20approach%20prevents%20performance%20degradation%20as%20new%20input%20data%20are%20integrated%20into%20the%20model%20and%20automatically%20adapts%20predictions%20based%20on%20the%20available%20inputs.%20We%20demonstrate%20the%20effectiveness%20of%20the%20approach%20on%20numerical%20benchmarks%20and%20a%20real-world%20case%20study%2C%20showing%20that%20it%20reliably%20integrates%20multi-modal%20data%20and%20provides%20accurate%20predictions%2C%20maintaining%20performance%20when%20generalizing%20across%20time%20and%20parameter%20variations.%0ALink%3A%20http%3A//arxiv.org/abs/2510.13762v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520multi-fidelity%2520learning%2520with%2520neural%2520networks%2520for%2520physical%2520system%2520predictions%26entry.906535625%3DPaolo%2520Conti%2520and%2520Mengwu%2520Guo%2520and%2520Attilio%2520Frangi%2520and%2520Andrea%2520Manzoni%26entry.1292438233%3DHighly%2520accurate%2520datasets%2520from%2520numerical%2520or%2520physical%2520experiments%2520are%2520often%2520expensive%2520and%2520time-consuming%2520to%2520acquire%252C%2520posing%2520a%2520significant%2520challenge%2520for%2520applications%2520that%2520require%2520precise%2520evaluations%252C%2520potentially%2520across%2520multiple%2520scenarios%2520and%2520in%2520real-time.%2520Even%2520building%2520sufficiently%2520accurate%2520surrogate%2520models%2520can%2520be%2520extremely%2520challenging%2520with%2520limited%2520high-fidelity%2520data.%2520Conversely%252C%2520less%2520expensive%252C%2520low-fidelity%2520data%2520can%2520be%2520computed%2520more%2520easily%2520and%2520encompass%2520a%2520broader%2520range%2520of%2520scenarios.%2520By%2520leveraging%2520multi-fidelity%2520information%252C%2520prediction%2520capabilities%2520of%2520surrogates%2520can%2520be%2520improved.%2520However%252C%2520in%2520practical%2520situations%252C%2520data%2520may%2520be%2520different%2520in%2520types%252C%2520come%2520from%2520sources%2520of%2520different%2520modalities%252C%2520and%2520not%2520be%2520concurrently%2520available%252C%2520further%2520complicating%2520the%2520modeling%2520process.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520progressive%2520multi-fidelity%2520surrogate%2520model.%2520This%2520model%2520can%2520sequentially%2520incorporate%2520diverse%2520data%2520types%2520using%2520tailored%2520encoders.%2520Multi-fidelity%2520regression%2520from%2520the%2520encoded%2520inputs%2520to%2520the%2520target%2520quantities%2520of%2520interest%2520is%2520then%2520performed%2520using%2520neural%2520networks.%2520Input%2520information%2520progressively%2520flows%2520from%2520lower%2520to%2520higher%2520fidelity%2520levels%2520through%2520two%2520sets%2520of%2520connections%253A%2520concatenations%2520among%2520all%2520the%2520encoded%2520inputs%252C%2520and%2520additive%2520connections%2520among%2520the%2520final%2520outputs.%2520This%2520dual%2520connection%2520system%2520enables%2520the%2520model%2520to%2520exploit%2520correlations%2520among%2520different%2520datasets%2520while%2520ensuring%2520that%2520each%2520level%2520makes%2520an%2520additive%2520correction%2520to%2520the%2520previous%2520level%2520without%2520altering%2520it.%2520This%2520approach%2520prevents%2520performance%2520degradation%2520as%2520new%2520input%2520data%2520are%2520integrated%2520into%2520the%2520model%2520and%2520automatically%2520adapts%2520predictions%2520based%2520on%2520the%2520available%2520inputs.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520approach%2520on%2520numerical%2520benchmarks%2520and%2520a%2520real-world%2520case%2520study%252C%2520showing%2520that%2520it%2520reliably%2520integrates%2520multi-modal%2520data%2520and%2520provides%2520accurate%2520predictions%252C%2520maintaining%2520performance%2520when%2520generalizing%2520across%2520time%2520and%2520parameter%2520variations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13762v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20multi-fidelity%20learning%20with%20neural%20networks%20for%20physical%20system%20predictions&entry.906535625=Paolo%20Conti%20and%20Mengwu%20Guo%20and%20Attilio%20Frangi%20and%20Andrea%20Manzoni&entry.1292438233=Highly%20accurate%20datasets%20from%20numerical%20or%20physical%20experiments%20are%20often%20expensive%20and%20time-consuming%20to%20acquire%2C%20posing%20a%20significant%20challenge%20for%20applications%20that%20require%20precise%20evaluations%2C%20potentially%20across%20multiple%20scenarios%20and%20in%20real-time.%20Even%20building%20sufficiently%20accurate%20surrogate%20models%20can%20be%20extremely%20challenging%20with%20limited%20high-fidelity%20data.%20Conversely%2C%20less%20expensive%2C%20low-fidelity%20data%20can%20be%20computed%20more%20easily%20and%20encompass%20a%20broader%20range%20of%20scenarios.%20By%20leveraging%20multi-fidelity%20information%2C%20prediction%20capabilities%20of%20surrogates%20can%20be%20improved.%20However%2C%20in%20practical%20situations%2C%20data%20may%20be%20different%20in%20types%2C%20come%20from%20sources%20of%20different%20modalities%2C%20and%20not%20be%20concurrently%20available%2C%20further%20complicating%20the%20modeling%20process.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20progressive%20multi-fidelity%20surrogate%20model.%20This%20model%20can%20sequentially%20incorporate%20diverse%20data%20types%20using%20tailored%20encoders.%20Multi-fidelity%20regression%20from%20the%20encoded%20inputs%20to%20the%20target%20quantities%20of%20interest%20is%20then%20performed%20using%20neural%20networks.%20Input%20information%20progressively%20flows%20from%20lower%20to%20higher%20fidelity%20levels%20through%20two%20sets%20of%20connections%3A%20concatenations%20among%20all%20the%20encoded%20inputs%2C%20and%20additive%20connections%20among%20the%20final%20outputs.%20This%20dual%20connection%20system%20enables%20the%20model%20to%20exploit%20correlations%20among%20different%20datasets%20while%20ensuring%20that%20each%20level%20makes%20an%20additive%20correction%20to%20the%20previous%20level%20without%20altering%20it.%20This%20approach%20prevents%20performance%20degradation%20as%20new%20input%20data%20are%20integrated%20into%20the%20model%20and%20automatically%20adapts%20predictions%20based%20on%20the%20available%20inputs.%20We%20demonstrate%20the%20effectiveness%20of%20the%20approach%20on%20numerical%20benchmarks%20and%20a%20real-world%20case%20study%2C%20showing%20that%20it%20reliably%20integrates%20multi-modal%20data%20and%20provides%20accurate%20predictions%2C%20maintaining%20performance%20when%20generalizing%20across%20time%20and%20parameter%20variations.&entry.1838667208=http%3A//arxiv.org/abs/2510.13762v2&entry.124074799=Read"},
{"title": "Minimax optimal differentially private synthetic data for smooth queries", "author": "Rundong Ding and Yiyun He and Yizhe Zhu", "abstract": "Differentially private synthetic data enables the sharing and analysis of sensitive datasets while providing rigorous privacy guarantees for individual contributors. A central challenge is to achieve strong utility guarantees for meaningful downstream analysis. Many existing methods ensure uniform accuracy over broad query classes, such as all Lipschitz functions, but this level of generality often leads to suboptimal rates for statistics of practical interest. Since many common data analysis queries exhibit smoothness beyond what worst-case Lipschitz bounds capture, we ask whether exploiting this additional structure can yield improved utility.\n  We study the problem of generating $(\\varepsilon,\u03b4)$-differentially private synthetic data from a dataset of size $n$ supported on the hypercube $[-1,1]^d$, with utility guarantees uniformly for all smooth queries having bounded derivatives up to order $k$. We propose a polynomial-time algorithm that achieves a minimax error rate of $n^{-\\min \\{1, \\frac{k}{d}\\}}$, up to a $\\log(n)$ factor. This characterization uncovers a phase transition at $k=d$. Our results generalize the Chebyshev moment matching framework of (Musco et al., 2025; Wang et al., 2016) and strictly improve the error rates for $k$-smooth queries established in (Wang et al., 2016). Moreover, we establish the first minimax lower bound for the utility of $(\\varepsilon,\u03b4)$-differentially private synthetic data with respect to $k$-smooth queries, extending the Wasserstein lower bound for $\\varepsilon$-differential privacy in (Boedihardjo et al., 2024).", "link": "http://arxiv.org/abs/2602.01607v2", "date": "2026-02-05", "relevancy": 1.7099, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4381}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4358}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4149}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Minimax%20optimal%20differentially%20private%20synthetic%20data%20for%20smooth%20queries&body=Title%3A%20Minimax%20optimal%20differentially%20private%20synthetic%20data%20for%20smooth%20queries%0AAuthor%3A%20Rundong%20Ding%20and%20Yiyun%20He%20and%20Yizhe%20Zhu%0AAbstract%3A%20Differentially%20private%20synthetic%20data%20enables%20the%20sharing%20and%20analysis%20of%20sensitive%20datasets%20while%20providing%20rigorous%20privacy%20guarantees%20for%20individual%20contributors.%20A%20central%20challenge%20is%20to%20achieve%20strong%20utility%20guarantees%20for%20meaningful%20downstream%20analysis.%20Many%20existing%20methods%20ensure%20uniform%20accuracy%20over%20broad%20query%20classes%2C%20such%20as%20all%20Lipschitz%20functions%2C%20but%20this%20level%20of%20generality%20often%20leads%20to%20suboptimal%20rates%20for%20statistics%20of%20practical%20interest.%20Since%20many%20common%20data%20analysis%20queries%20exhibit%20smoothness%20beyond%20what%20worst-case%20Lipschitz%20bounds%20capture%2C%20we%20ask%20whether%20exploiting%20this%20additional%20structure%20can%20yield%20improved%20utility.%0A%20%20We%20study%20the%20problem%20of%20generating%20%24%28%5Cvarepsilon%2C%CE%B4%29%24-differentially%20private%20synthetic%20data%20from%20a%20dataset%20of%20size%20%24n%24%20supported%20on%20the%20hypercube%20%24%5B-1%2C1%5D%5Ed%24%2C%20with%20utility%20guarantees%20uniformly%20for%20all%20smooth%20queries%20having%20bounded%20derivatives%20up%20to%20order%20%24k%24.%20We%20propose%20a%20polynomial-time%20algorithm%20that%20achieves%20a%20minimax%20error%20rate%20of%20%24n%5E%7B-%5Cmin%20%5C%7B1%2C%20%5Cfrac%7Bk%7D%7Bd%7D%5C%7D%7D%24%2C%20up%20to%20a%20%24%5Clog%28n%29%24%20factor.%20This%20characterization%20uncovers%20a%20phase%20transition%20at%20%24k%3Dd%24.%20Our%20results%20generalize%20the%20Chebyshev%20moment%20matching%20framework%20of%20%28Musco%20et%20al.%2C%202025%3B%20Wang%20et%20al.%2C%202016%29%20and%20strictly%20improve%20the%20error%20rates%20for%20%24k%24-smooth%20queries%20established%20in%20%28Wang%20et%20al.%2C%202016%29.%20Moreover%2C%20we%20establish%20the%20first%20minimax%20lower%20bound%20for%20the%20utility%20of%20%24%28%5Cvarepsilon%2C%CE%B4%29%24-differentially%20private%20synthetic%20data%20with%20respect%20to%20%24k%24-smooth%20queries%2C%20extending%20the%20Wasserstein%20lower%20bound%20for%20%24%5Cvarepsilon%24-differential%20privacy%20in%20%28Boedihardjo%20et%20al.%2C%202024%29.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01607v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMinimax%2520optimal%2520differentially%2520private%2520synthetic%2520data%2520for%2520smooth%2520queries%26entry.906535625%3DRundong%2520Ding%2520and%2520Yiyun%2520He%2520and%2520Yizhe%2520Zhu%26entry.1292438233%3DDifferentially%2520private%2520synthetic%2520data%2520enables%2520the%2520sharing%2520and%2520analysis%2520of%2520sensitive%2520datasets%2520while%2520providing%2520rigorous%2520privacy%2520guarantees%2520for%2520individual%2520contributors.%2520A%2520central%2520challenge%2520is%2520to%2520achieve%2520strong%2520utility%2520guarantees%2520for%2520meaningful%2520downstream%2520analysis.%2520Many%2520existing%2520methods%2520ensure%2520uniform%2520accuracy%2520over%2520broad%2520query%2520classes%252C%2520such%2520as%2520all%2520Lipschitz%2520functions%252C%2520but%2520this%2520level%2520of%2520generality%2520often%2520leads%2520to%2520suboptimal%2520rates%2520for%2520statistics%2520of%2520practical%2520interest.%2520Since%2520many%2520common%2520data%2520analysis%2520queries%2520exhibit%2520smoothness%2520beyond%2520what%2520worst-case%2520Lipschitz%2520bounds%2520capture%252C%2520we%2520ask%2520whether%2520exploiting%2520this%2520additional%2520structure%2520can%2520yield%2520improved%2520utility.%250A%2520%2520We%2520study%2520the%2520problem%2520of%2520generating%2520%2524%2528%255Cvarepsilon%252C%25CE%25B4%2529%2524-differentially%2520private%2520synthetic%2520data%2520from%2520a%2520dataset%2520of%2520size%2520%2524n%2524%2520supported%2520on%2520the%2520hypercube%2520%2524%255B-1%252C1%255D%255Ed%2524%252C%2520with%2520utility%2520guarantees%2520uniformly%2520for%2520all%2520smooth%2520queries%2520having%2520bounded%2520derivatives%2520up%2520to%2520order%2520%2524k%2524.%2520We%2520propose%2520a%2520polynomial-time%2520algorithm%2520that%2520achieves%2520a%2520minimax%2520error%2520rate%2520of%2520%2524n%255E%257B-%255Cmin%2520%255C%257B1%252C%2520%255Cfrac%257Bk%257D%257Bd%257D%255C%257D%257D%2524%252C%2520up%2520to%2520a%2520%2524%255Clog%2528n%2529%2524%2520factor.%2520This%2520characterization%2520uncovers%2520a%2520phase%2520transition%2520at%2520%2524k%253Dd%2524.%2520Our%2520results%2520generalize%2520the%2520Chebyshev%2520moment%2520matching%2520framework%2520of%2520%2528Musco%2520et%2520al.%252C%25202025%253B%2520Wang%2520et%2520al.%252C%25202016%2529%2520and%2520strictly%2520improve%2520the%2520error%2520rates%2520for%2520%2524k%2524-smooth%2520queries%2520established%2520in%2520%2528Wang%2520et%2520al.%252C%25202016%2529.%2520Moreover%252C%2520we%2520establish%2520the%2520first%2520minimax%2520lower%2520bound%2520for%2520the%2520utility%2520of%2520%2524%2528%255Cvarepsilon%252C%25CE%25B4%2529%2524-differentially%2520private%2520synthetic%2520data%2520with%2520respect%2520to%2520%2524k%2524-smooth%2520queries%252C%2520extending%2520the%2520Wasserstein%2520lower%2520bound%2520for%2520%2524%255Cvarepsilon%2524-differential%2520privacy%2520in%2520%2528Boedihardjo%2520et%2520al.%252C%25202024%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01607v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Minimax%20optimal%20differentially%20private%20synthetic%20data%20for%20smooth%20queries&entry.906535625=Rundong%20Ding%20and%20Yiyun%20He%20and%20Yizhe%20Zhu&entry.1292438233=Differentially%20private%20synthetic%20data%20enables%20the%20sharing%20and%20analysis%20of%20sensitive%20datasets%20while%20providing%20rigorous%20privacy%20guarantees%20for%20individual%20contributors.%20A%20central%20challenge%20is%20to%20achieve%20strong%20utility%20guarantees%20for%20meaningful%20downstream%20analysis.%20Many%20existing%20methods%20ensure%20uniform%20accuracy%20over%20broad%20query%20classes%2C%20such%20as%20all%20Lipschitz%20functions%2C%20but%20this%20level%20of%20generality%20often%20leads%20to%20suboptimal%20rates%20for%20statistics%20of%20practical%20interest.%20Since%20many%20common%20data%20analysis%20queries%20exhibit%20smoothness%20beyond%20what%20worst-case%20Lipschitz%20bounds%20capture%2C%20we%20ask%20whether%20exploiting%20this%20additional%20structure%20can%20yield%20improved%20utility.%0A%20%20We%20study%20the%20problem%20of%20generating%20%24%28%5Cvarepsilon%2C%CE%B4%29%24-differentially%20private%20synthetic%20data%20from%20a%20dataset%20of%20size%20%24n%24%20supported%20on%20the%20hypercube%20%24%5B-1%2C1%5D%5Ed%24%2C%20with%20utility%20guarantees%20uniformly%20for%20all%20smooth%20queries%20having%20bounded%20derivatives%20up%20to%20order%20%24k%24.%20We%20propose%20a%20polynomial-time%20algorithm%20that%20achieves%20a%20minimax%20error%20rate%20of%20%24n%5E%7B-%5Cmin%20%5C%7B1%2C%20%5Cfrac%7Bk%7D%7Bd%7D%5C%7D%7D%24%2C%20up%20to%20a%20%24%5Clog%28n%29%24%20factor.%20This%20characterization%20uncovers%20a%20phase%20transition%20at%20%24k%3Dd%24.%20Our%20results%20generalize%20the%20Chebyshev%20moment%20matching%20framework%20of%20%28Musco%20et%20al.%2C%202025%3B%20Wang%20et%20al.%2C%202016%29%20and%20strictly%20improve%20the%20error%20rates%20for%20%24k%24-smooth%20queries%20established%20in%20%28Wang%20et%20al.%2C%202016%29.%20Moreover%2C%20we%20establish%20the%20first%20minimax%20lower%20bound%20for%20the%20utility%20of%20%24%28%5Cvarepsilon%2C%CE%B4%29%24-differentially%20private%20synthetic%20data%20with%20respect%20to%20%24k%24-smooth%20queries%2C%20extending%20the%20Wasserstein%20lower%20bound%20for%20%24%5Cvarepsilon%24-differential%20privacy%20in%20%28Boedihardjo%20et%20al.%2C%202024%29.&entry.1838667208=http%3A//arxiv.org/abs/2602.01607v2&entry.124074799=Read"},
{"title": "Investigating the Impact of Histopathological Foundation Models on Regressive Prediction of Homologous Recombination Deficiency", "author": "Alexander Blezinger and Wolfgang Nejdl and Ming Tang", "abstract": "Foundation models pretrained on large-scale histopathology data have found great success in various fields of computational pathology, but their impact on regressive biomarker prediction remains underexplored. In this work, we systematically evaluate histopathological foundation models for regression-based tasks, demonstrated through the prediction of homologous recombination deficiency (HRD) score - a critical biomarker for personalized cancer treatment. Within multiple instance learning frameworks, we extract patch-level features from whole slide images (WSI) using five state-of-the-art foundation models, and evaluate their impact compared to contrastive learning-based features. Models are trained to predict continuous HRD scores based on these extracted features across breast, endometrial, and lung cancer cohorts from two public medical data collections. Extensive experiments demonstrate that models trained on foundation model features consistently outperform the baseline in terms of predictive accuracy and generalization capabilities while exhibiting systematic differences among the foundation models. Additionally, we propose a distribution-based upsampling strategy to mitigate target imbalance in these datasets, significantly improving the recall and balanced accuracy for underrepresented but clinically important patient populations. Furthermore, we investigate the impact of different sampling strategies and instance bagsizes by ablation studies. Our results highlight the benefits of large-scale histopathological pretraining for more precise and transferable regressive biomarker prediction, showcasing its potential to advance AI-driven precision oncology.", "link": "http://arxiv.org/abs/2602.00151v2", "date": "2026-02-05", "relevancy": 1.9193, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4825}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4825}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20the%20Impact%20of%20Histopathological%20Foundation%20Models%20on%20Regressive%20Prediction%20of%20Homologous%20Recombination%20Deficiency&body=Title%3A%20Investigating%20the%20Impact%20of%20Histopathological%20Foundation%20Models%20on%20Regressive%20Prediction%20of%20Homologous%20Recombination%20Deficiency%0AAuthor%3A%20Alexander%20Blezinger%20and%20Wolfgang%20Nejdl%20and%20Ming%20Tang%0AAbstract%3A%20Foundation%20models%20pretrained%20on%20large-scale%20histopathology%20data%20have%20found%20great%20success%20in%20various%20fields%20of%20computational%20pathology%2C%20but%20their%20impact%20on%20regressive%20biomarker%20prediction%20remains%20underexplored.%20In%20this%20work%2C%20we%20systematically%20evaluate%20histopathological%20foundation%20models%20for%20regression-based%20tasks%2C%20demonstrated%20through%20the%20prediction%20of%20homologous%20recombination%20deficiency%20%28HRD%29%20score%20-%20a%20critical%20biomarker%20for%20personalized%20cancer%20treatment.%20Within%20multiple%20instance%20learning%20frameworks%2C%20we%20extract%20patch-level%20features%20from%20whole%20slide%20images%20%28WSI%29%20using%20five%20state-of-the-art%20foundation%20models%2C%20and%20evaluate%20their%20impact%20compared%20to%20contrastive%20learning-based%20features.%20Models%20are%20trained%20to%20predict%20continuous%20HRD%20scores%20based%20on%20these%20extracted%20features%20across%20breast%2C%20endometrial%2C%20and%20lung%20cancer%20cohorts%20from%20two%20public%20medical%20data%20collections.%20Extensive%20experiments%20demonstrate%20that%20models%20trained%20on%20foundation%20model%20features%20consistently%20outperform%20the%20baseline%20in%20terms%20of%20predictive%20accuracy%20and%20generalization%20capabilities%20while%20exhibiting%20systematic%20differences%20among%20the%20foundation%20models.%20Additionally%2C%20we%20propose%20a%20distribution-based%20upsampling%20strategy%20to%20mitigate%20target%20imbalance%20in%20these%20datasets%2C%20significantly%20improving%20the%20recall%20and%20balanced%20accuracy%20for%20underrepresented%20but%20clinically%20important%20patient%20populations.%20Furthermore%2C%20we%20investigate%20the%20impact%20of%20different%20sampling%20strategies%20and%20instance%20bagsizes%20by%20ablation%20studies.%20Our%20results%20highlight%20the%20benefits%20of%20large-scale%20histopathological%20pretraining%20for%20more%20precise%20and%20transferable%20regressive%20biomarker%20prediction%2C%20showcasing%20its%20potential%20to%20advance%20AI-driven%20precision%20oncology.%0ALink%3A%20http%3A//arxiv.org/abs/2602.00151v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520the%2520Impact%2520of%2520Histopathological%2520Foundation%2520Models%2520on%2520Regressive%2520Prediction%2520of%2520Homologous%2520Recombination%2520Deficiency%26entry.906535625%3DAlexander%2520Blezinger%2520and%2520Wolfgang%2520Nejdl%2520and%2520Ming%2520Tang%26entry.1292438233%3DFoundation%2520models%2520pretrained%2520on%2520large-scale%2520histopathology%2520data%2520have%2520found%2520great%2520success%2520in%2520various%2520fields%2520of%2520computational%2520pathology%252C%2520but%2520their%2520impact%2520on%2520regressive%2520biomarker%2520prediction%2520remains%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520systematically%2520evaluate%2520histopathological%2520foundation%2520models%2520for%2520regression-based%2520tasks%252C%2520demonstrated%2520through%2520the%2520prediction%2520of%2520homologous%2520recombination%2520deficiency%2520%2528HRD%2529%2520score%2520-%2520a%2520critical%2520biomarker%2520for%2520personalized%2520cancer%2520treatment.%2520Within%2520multiple%2520instance%2520learning%2520frameworks%252C%2520we%2520extract%2520patch-level%2520features%2520from%2520whole%2520slide%2520images%2520%2528WSI%2529%2520using%2520five%2520state-of-the-art%2520foundation%2520models%252C%2520and%2520evaluate%2520their%2520impact%2520compared%2520to%2520contrastive%2520learning-based%2520features.%2520Models%2520are%2520trained%2520to%2520predict%2520continuous%2520HRD%2520scores%2520based%2520on%2520these%2520extracted%2520features%2520across%2520breast%252C%2520endometrial%252C%2520and%2520lung%2520cancer%2520cohorts%2520from%2520two%2520public%2520medical%2520data%2520collections.%2520Extensive%2520experiments%2520demonstrate%2520that%2520models%2520trained%2520on%2520foundation%2520model%2520features%2520consistently%2520outperform%2520the%2520baseline%2520in%2520terms%2520of%2520predictive%2520accuracy%2520and%2520generalization%2520capabilities%2520while%2520exhibiting%2520systematic%2520differences%2520among%2520the%2520foundation%2520models.%2520Additionally%252C%2520we%2520propose%2520a%2520distribution-based%2520upsampling%2520strategy%2520to%2520mitigate%2520target%2520imbalance%2520in%2520these%2520datasets%252C%2520significantly%2520improving%2520the%2520recall%2520and%2520balanced%2520accuracy%2520for%2520underrepresented%2520but%2520clinically%2520important%2520patient%2520populations.%2520Furthermore%252C%2520we%2520investigate%2520the%2520impact%2520of%2520different%2520sampling%2520strategies%2520and%2520instance%2520bagsizes%2520by%2520ablation%2520studies.%2520Our%2520results%2520highlight%2520the%2520benefits%2520of%2520large-scale%2520histopathological%2520pretraining%2520for%2520more%2520precise%2520and%2520transferable%2520regressive%2520biomarker%2520prediction%252C%2520showcasing%2520its%2520potential%2520to%2520advance%2520AI-driven%2520precision%2520oncology.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.00151v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20the%20Impact%20of%20Histopathological%20Foundation%20Models%20on%20Regressive%20Prediction%20of%20Homologous%20Recombination%20Deficiency&entry.906535625=Alexander%20Blezinger%20and%20Wolfgang%20Nejdl%20and%20Ming%20Tang&entry.1292438233=Foundation%20models%20pretrained%20on%20large-scale%20histopathology%20data%20have%20found%20great%20success%20in%20various%20fields%20of%20computational%20pathology%2C%20but%20their%20impact%20on%20regressive%20biomarker%20prediction%20remains%20underexplored.%20In%20this%20work%2C%20we%20systematically%20evaluate%20histopathological%20foundation%20models%20for%20regression-based%20tasks%2C%20demonstrated%20through%20the%20prediction%20of%20homologous%20recombination%20deficiency%20%28HRD%29%20score%20-%20a%20critical%20biomarker%20for%20personalized%20cancer%20treatment.%20Within%20multiple%20instance%20learning%20frameworks%2C%20we%20extract%20patch-level%20features%20from%20whole%20slide%20images%20%28WSI%29%20using%20five%20state-of-the-art%20foundation%20models%2C%20and%20evaluate%20their%20impact%20compared%20to%20contrastive%20learning-based%20features.%20Models%20are%20trained%20to%20predict%20continuous%20HRD%20scores%20based%20on%20these%20extracted%20features%20across%20breast%2C%20endometrial%2C%20and%20lung%20cancer%20cohorts%20from%20two%20public%20medical%20data%20collections.%20Extensive%20experiments%20demonstrate%20that%20models%20trained%20on%20foundation%20model%20features%20consistently%20outperform%20the%20baseline%20in%20terms%20of%20predictive%20accuracy%20and%20generalization%20capabilities%20while%20exhibiting%20systematic%20differences%20among%20the%20foundation%20models.%20Additionally%2C%20we%20propose%20a%20distribution-based%20upsampling%20strategy%20to%20mitigate%20target%20imbalance%20in%20these%20datasets%2C%20significantly%20improving%20the%20recall%20and%20balanced%20accuracy%20for%20underrepresented%20but%20clinically%20important%20patient%20populations.%20Furthermore%2C%20we%20investigate%20the%20impact%20of%20different%20sampling%20strategies%20and%20instance%20bagsizes%20by%20ablation%20studies.%20Our%20results%20highlight%20the%20benefits%20of%20large-scale%20histopathological%20pretraining%20for%20more%20precise%20and%20transferable%20regressive%20biomarker%20prediction%2C%20showcasing%20its%20potential%20to%20advance%20AI-driven%20precision%20oncology.&entry.1838667208=http%3A//arxiv.org/abs/2602.00151v2&entry.124074799=Read"},
{"title": "BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring", "author": "Usman Haider and Lukasz Szemet and Daniel Kelly and Vasileios Sergis and Andrew C. Daly and Karl Mason", "abstract": "Bioprinting is a rapidly advancing field that offers a transformative approach to fabricating tissue and organ models through the precise deposition of cell-laden bioinks. Ensuring the fidelity and consistency of printed structures in real-time remains a core challenge, particularly under constraints imposed by limited imaging data and resource-constrained embedded hardware. Semantic segmentation of the extrusion process, differentiating between nozzle, extruded bioink, and surrounding background, enables in situ monitoring critical to maintaining print quality and biological viability. In this work, we introduce a lightweight semantic segmentation framework tailored for real-time bioprinting applications. We present a novel, manually annotated dataset comprising 787 RGB images captured during the bioprinting process, labeled across three classes: nozzle, bioink, and background. To achieve fast and efficient inference suitable for integration with bioprinting systems, we propose a BioLite U-Net architecture that leverages depthwise separable convolutions to drastically reduce computational load without compromising accuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based segmentation baselines using mean Intersection over Union (mIoU), Dice score, and pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess real-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85% and a Dice score of 96.17%, while being over 1300x smaller than MobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame, demonstrating near real-time capability. Compared to MobileNet baselines, BioLite U-Net offers a superior tradeoff between segmentation accuracy, efficiency, and deployability, making it highly suitable for intelligent, closed-loop bioprinting systems.", "link": "http://arxiv.org/abs/2509.06690v2", "date": "2026-02-05", "relevancy": 1.5598, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5262}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5208}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BioLite%20U-Net%3A%20Edge-Deployable%20Semantic%20Segmentation%20for%20In%20Situ%20Bioprinting%20Monitoring&body=Title%3A%20BioLite%20U-Net%3A%20Edge-Deployable%20Semantic%20Segmentation%20for%20In%20Situ%20Bioprinting%20Monitoring%0AAuthor%3A%20Usman%20Haider%20and%20Lukasz%20Szemet%20and%20Daniel%20Kelly%20and%20Vasileios%20Sergis%20and%20Andrew%20C.%20Daly%20and%20Karl%20Mason%0AAbstract%3A%20Bioprinting%20is%20a%20rapidly%20advancing%20field%20that%20offers%20a%20transformative%20approach%20to%20fabricating%20tissue%20and%20organ%20models%20through%20the%20precise%20deposition%20of%20cell-laden%20bioinks.%20Ensuring%20the%20fidelity%20and%20consistency%20of%20printed%20structures%20in%20real-time%20remains%20a%20core%20challenge%2C%20particularly%20under%20constraints%20imposed%20by%20limited%20imaging%20data%20and%20resource-constrained%20embedded%20hardware.%20Semantic%20segmentation%20of%20the%20extrusion%20process%2C%20differentiating%20between%20nozzle%2C%20extruded%20bioink%2C%20and%20surrounding%20background%2C%20enables%20in%20situ%20monitoring%20critical%20to%20maintaining%20print%20quality%20and%20biological%20viability.%20In%20this%20work%2C%20we%20introduce%20a%20lightweight%20semantic%20segmentation%20framework%20tailored%20for%20real-time%20bioprinting%20applications.%20We%20present%20a%20novel%2C%20manually%20annotated%20dataset%20comprising%20787%20RGB%20images%20captured%20during%20the%20bioprinting%20process%2C%20labeled%20across%20three%20classes%3A%20nozzle%2C%20bioink%2C%20and%20background.%20To%20achieve%20fast%20and%20efficient%20inference%20suitable%20for%20integration%20with%20bioprinting%20systems%2C%20we%20propose%20a%20BioLite%20U-Net%20architecture%20that%20leverages%20depthwise%20separable%20convolutions%20to%20drastically%20reduce%20computational%20load%20without%20compromising%20accuracy.%20Our%20model%20is%20benchmarked%20against%20MobileNetV2%20and%20MobileNetV3-based%20segmentation%20baselines%20using%20mean%20Intersection%20over%20Union%20%28mIoU%29%2C%20Dice%20score%2C%20and%20pixel%20accuracy.%20All%20models%20were%20evaluated%20on%20a%20Raspberry%20Pi%204B%20to%20assess%20real-world%20feasibility.%20The%20proposed%20BioLite%20U-Net%20achieves%20an%20mIoU%20of%2092.85%25%20and%20a%20Dice%20score%20of%2096.17%25%2C%20while%20being%20over%201300x%20smaller%20than%20MobileNetV2-DeepLabV3%2B.%20On-device%20inference%20takes%20335%20ms%20per%20frame%2C%20demonstrating%20near%20real-time%20capability.%20Compared%20to%20MobileNet%20baselines%2C%20BioLite%20U-Net%20offers%20a%20superior%20tradeoff%20between%20segmentation%20accuracy%2C%20efficiency%2C%20and%20deployability%2C%20making%20it%20highly%20suitable%20for%20intelligent%2C%20closed-loop%20bioprinting%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2509.06690v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBioLite%2520U-Net%253A%2520Edge-Deployable%2520Semantic%2520Segmentation%2520for%2520In%2520Situ%2520Bioprinting%2520Monitoring%26entry.906535625%3DUsman%2520Haider%2520and%2520Lukasz%2520Szemet%2520and%2520Daniel%2520Kelly%2520and%2520Vasileios%2520Sergis%2520and%2520Andrew%2520C.%2520Daly%2520and%2520Karl%2520Mason%26entry.1292438233%3DBioprinting%2520is%2520a%2520rapidly%2520advancing%2520field%2520that%2520offers%2520a%2520transformative%2520approach%2520to%2520fabricating%2520tissue%2520and%2520organ%2520models%2520through%2520the%2520precise%2520deposition%2520of%2520cell-laden%2520bioinks.%2520Ensuring%2520the%2520fidelity%2520and%2520consistency%2520of%2520printed%2520structures%2520in%2520real-time%2520remains%2520a%2520core%2520challenge%252C%2520particularly%2520under%2520constraints%2520imposed%2520by%2520limited%2520imaging%2520data%2520and%2520resource-constrained%2520embedded%2520hardware.%2520Semantic%2520segmentation%2520of%2520the%2520extrusion%2520process%252C%2520differentiating%2520between%2520nozzle%252C%2520extruded%2520bioink%252C%2520and%2520surrounding%2520background%252C%2520enables%2520in%2520situ%2520monitoring%2520critical%2520to%2520maintaining%2520print%2520quality%2520and%2520biological%2520viability.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520lightweight%2520semantic%2520segmentation%2520framework%2520tailored%2520for%2520real-time%2520bioprinting%2520applications.%2520We%2520present%2520a%2520novel%252C%2520manually%2520annotated%2520dataset%2520comprising%2520787%2520RGB%2520images%2520captured%2520during%2520the%2520bioprinting%2520process%252C%2520labeled%2520across%2520three%2520classes%253A%2520nozzle%252C%2520bioink%252C%2520and%2520background.%2520To%2520achieve%2520fast%2520and%2520efficient%2520inference%2520suitable%2520for%2520integration%2520with%2520bioprinting%2520systems%252C%2520we%2520propose%2520a%2520BioLite%2520U-Net%2520architecture%2520that%2520leverages%2520depthwise%2520separable%2520convolutions%2520to%2520drastically%2520reduce%2520computational%2520load%2520without%2520compromising%2520accuracy.%2520Our%2520model%2520is%2520benchmarked%2520against%2520MobileNetV2%2520and%2520MobileNetV3-based%2520segmentation%2520baselines%2520using%2520mean%2520Intersection%2520over%2520Union%2520%2528mIoU%2529%252C%2520Dice%2520score%252C%2520and%2520pixel%2520accuracy.%2520All%2520models%2520were%2520evaluated%2520on%2520a%2520Raspberry%2520Pi%25204B%2520to%2520assess%2520real-world%2520feasibility.%2520The%2520proposed%2520BioLite%2520U-Net%2520achieves%2520an%2520mIoU%2520of%252092.85%2525%2520and%2520a%2520Dice%2520score%2520of%252096.17%2525%252C%2520while%2520being%2520over%25201300x%2520smaller%2520than%2520MobileNetV2-DeepLabV3%252B.%2520On-device%2520inference%2520takes%2520335%2520ms%2520per%2520frame%252C%2520demonstrating%2520near%2520real-time%2520capability.%2520Compared%2520to%2520MobileNet%2520baselines%252C%2520BioLite%2520U-Net%2520offers%2520a%2520superior%2520tradeoff%2520between%2520segmentation%2520accuracy%252C%2520efficiency%252C%2520and%2520deployability%252C%2520making%2520it%2520highly%2520suitable%2520for%2520intelligent%252C%2520closed-loop%2520bioprinting%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06690v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BioLite%20U-Net%3A%20Edge-Deployable%20Semantic%20Segmentation%20for%20In%20Situ%20Bioprinting%20Monitoring&entry.906535625=Usman%20Haider%20and%20Lukasz%20Szemet%20and%20Daniel%20Kelly%20and%20Vasileios%20Sergis%20and%20Andrew%20C.%20Daly%20and%20Karl%20Mason&entry.1292438233=Bioprinting%20is%20a%20rapidly%20advancing%20field%20that%20offers%20a%20transformative%20approach%20to%20fabricating%20tissue%20and%20organ%20models%20through%20the%20precise%20deposition%20of%20cell-laden%20bioinks.%20Ensuring%20the%20fidelity%20and%20consistency%20of%20printed%20structures%20in%20real-time%20remains%20a%20core%20challenge%2C%20particularly%20under%20constraints%20imposed%20by%20limited%20imaging%20data%20and%20resource-constrained%20embedded%20hardware.%20Semantic%20segmentation%20of%20the%20extrusion%20process%2C%20differentiating%20between%20nozzle%2C%20extruded%20bioink%2C%20and%20surrounding%20background%2C%20enables%20in%20situ%20monitoring%20critical%20to%20maintaining%20print%20quality%20and%20biological%20viability.%20In%20this%20work%2C%20we%20introduce%20a%20lightweight%20semantic%20segmentation%20framework%20tailored%20for%20real-time%20bioprinting%20applications.%20We%20present%20a%20novel%2C%20manually%20annotated%20dataset%20comprising%20787%20RGB%20images%20captured%20during%20the%20bioprinting%20process%2C%20labeled%20across%20three%20classes%3A%20nozzle%2C%20bioink%2C%20and%20background.%20To%20achieve%20fast%20and%20efficient%20inference%20suitable%20for%20integration%20with%20bioprinting%20systems%2C%20we%20propose%20a%20BioLite%20U-Net%20architecture%20that%20leverages%20depthwise%20separable%20convolutions%20to%20drastically%20reduce%20computational%20load%20without%20compromising%20accuracy.%20Our%20model%20is%20benchmarked%20against%20MobileNetV2%20and%20MobileNetV3-based%20segmentation%20baselines%20using%20mean%20Intersection%20over%20Union%20%28mIoU%29%2C%20Dice%20score%2C%20and%20pixel%20accuracy.%20All%20models%20were%20evaluated%20on%20a%20Raspberry%20Pi%204B%20to%20assess%20real-world%20feasibility.%20The%20proposed%20BioLite%20U-Net%20achieves%20an%20mIoU%20of%2092.85%25%20and%20a%20Dice%20score%20of%2096.17%25%2C%20while%20being%20over%201300x%20smaller%20than%20MobileNetV2-DeepLabV3%2B.%20On-device%20inference%20takes%20335%20ms%20per%20frame%2C%20demonstrating%20near%20real-time%20capability.%20Compared%20to%20MobileNet%20baselines%2C%20BioLite%20U-Net%20offers%20a%20superior%20tradeoff%20between%20segmentation%20accuracy%2C%20efficiency%2C%20and%20deployability%2C%20making%20it%20highly%20suitable%20for%20intelligent%2C%20closed-loop%20bioprinting%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2509.06690v2&entry.124074799=Read"},
{"title": "Metric Hedonic Games on the Line", "author": "Merlin de la Haye and Pascal Lenzner and Farehe Soheil and Marcus Wunderlich", "abstract": "Hedonic games are fundamental models for investigating the formation of coalitions among a set of strategic agents, where every agent has a certain utility for every possible coalition of agents it can be part of. To avoid the intractability of defining exponentially many utilities for all possible coalitions, many variants with succinct representations of the agents' utility functions have been devised and analyzed, e.g., modified fractional hedonic games by Monaco et al. [JAAMAS 2020]. We extend this by studying a novel succinct variant that is related to modified fractional hedonic games. In our model, each agent has a fixed type-value and an agent's cost for some given coalition is based on the differences between its value and those of the other members of its coalition. This allows to model natural situations like athletes forming training groups with similar performance levels or voters that partition themselves along a political spectrum.\n  In particular, we investigate natural variants where an agent's cost is defined by distance thresholds, or by the maximum or average value difference to the other agents in its coalition. For these settings, we study the existence of stable coalition structures, their properties, and their quality in terms of the price of anarchy and the price of stability. Further, we investigate the impact of limiting the maximum number of coalitions. Despite the simple setting with metric distances on a line, we uncover a rich landscape of models, partially with counter-intuitive behavior. Also, our focus on both swap stability and jump stability allows us to study the influence of fixing the number and the size of the coalitions. Overall, we find that stable coalition structures always exist but that their properties and quality can vary widely.", "link": "http://arxiv.org/abs/2602.05888v1", "date": "2026-02-05", "relevancy": 1.1631, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4137}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.3828}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Metric%20Hedonic%20Games%20on%20the%20Line&body=Title%3A%20Metric%20Hedonic%20Games%20on%20the%20Line%0AAuthor%3A%20Merlin%20de%20la%20Haye%20and%20Pascal%20Lenzner%20and%20Farehe%20Soheil%20and%20Marcus%20Wunderlich%0AAbstract%3A%20Hedonic%20games%20are%20fundamental%20models%20for%20investigating%20the%20formation%20of%20coalitions%20among%20a%20set%20of%20strategic%20agents%2C%20where%20every%20agent%20has%20a%20certain%20utility%20for%20every%20possible%20coalition%20of%20agents%20it%20can%20be%20part%20of.%20To%20avoid%20the%20intractability%20of%20defining%20exponentially%20many%20utilities%20for%20all%20possible%20coalitions%2C%20many%20variants%20with%20succinct%20representations%20of%20the%20agents%27%20utility%20functions%20have%20been%20devised%20and%20analyzed%2C%20e.g.%2C%20modified%20fractional%20hedonic%20games%20by%20Monaco%20et%20al.%20%5BJAAMAS%202020%5D.%20We%20extend%20this%20by%20studying%20a%20novel%20succinct%20variant%20that%20is%20related%20to%20modified%20fractional%20hedonic%20games.%20In%20our%20model%2C%20each%20agent%20has%20a%20fixed%20type-value%20and%20an%20agent%27s%20cost%20for%20some%20given%20coalition%20is%20based%20on%20the%20differences%20between%20its%20value%20and%20those%20of%20the%20other%20members%20of%20its%20coalition.%20This%20allows%20to%20model%20natural%20situations%20like%20athletes%20forming%20training%20groups%20with%20similar%20performance%20levels%20or%20voters%20that%20partition%20themselves%20along%20a%20political%20spectrum.%0A%20%20In%20particular%2C%20we%20investigate%20natural%20variants%20where%20an%20agent%27s%20cost%20is%20defined%20by%20distance%20thresholds%2C%20or%20by%20the%20maximum%20or%20average%20value%20difference%20to%20the%20other%20agents%20in%20its%20coalition.%20For%20these%20settings%2C%20we%20study%20the%20existence%20of%20stable%20coalition%20structures%2C%20their%20properties%2C%20and%20their%20quality%20in%20terms%20of%20the%20price%20of%20anarchy%20and%20the%20price%20of%20stability.%20Further%2C%20we%20investigate%20the%20impact%20of%20limiting%20the%20maximum%20number%20of%20coalitions.%20Despite%20the%20simple%20setting%20with%20metric%20distances%20on%20a%20line%2C%20we%20uncover%20a%20rich%20landscape%20of%20models%2C%20partially%20with%20counter-intuitive%20behavior.%20Also%2C%20our%20focus%20on%20both%20swap%20stability%20and%20jump%20stability%20allows%20us%20to%20study%20the%20influence%20of%20fixing%20the%20number%20and%20the%20size%20of%20the%20coalitions.%20Overall%2C%20we%20find%20that%20stable%20coalition%20structures%20always%20exist%20but%20that%20their%20properties%20and%20quality%20can%20vary%20widely.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetric%2520Hedonic%2520Games%2520on%2520the%2520Line%26entry.906535625%3DMerlin%2520de%2520la%2520Haye%2520and%2520Pascal%2520Lenzner%2520and%2520Farehe%2520Soheil%2520and%2520Marcus%2520Wunderlich%26entry.1292438233%3DHedonic%2520games%2520are%2520fundamental%2520models%2520for%2520investigating%2520the%2520formation%2520of%2520coalitions%2520among%2520a%2520set%2520of%2520strategic%2520agents%252C%2520where%2520every%2520agent%2520has%2520a%2520certain%2520utility%2520for%2520every%2520possible%2520coalition%2520of%2520agents%2520it%2520can%2520be%2520part%2520of.%2520To%2520avoid%2520the%2520intractability%2520of%2520defining%2520exponentially%2520many%2520utilities%2520for%2520all%2520possible%2520coalitions%252C%2520many%2520variants%2520with%2520succinct%2520representations%2520of%2520the%2520agents%2527%2520utility%2520functions%2520have%2520been%2520devised%2520and%2520analyzed%252C%2520e.g.%252C%2520modified%2520fractional%2520hedonic%2520games%2520by%2520Monaco%2520et%2520al.%2520%255BJAAMAS%25202020%255D.%2520We%2520extend%2520this%2520by%2520studying%2520a%2520novel%2520succinct%2520variant%2520that%2520is%2520related%2520to%2520modified%2520fractional%2520hedonic%2520games.%2520In%2520our%2520model%252C%2520each%2520agent%2520has%2520a%2520fixed%2520type-value%2520and%2520an%2520agent%2527s%2520cost%2520for%2520some%2520given%2520coalition%2520is%2520based%2520on%2520the%2520differences%2520between%2520its%2520value%2520and%2520those%2520of%2520the%2520other%2520members%2520of%2520its%2520coalition.%2520This%2520allows%2520to%2520model%2520natural%2520situations%2520like%2520athletes%2520forming%2520training%2520groups%2520with%2520similar%2520performance%2520levels%2520or%2520voters%2520that%2520partition%2520themselves%2520along%2520a%2520political%2520spectrum.%250A%2520%2520In%2520particular%252C%2520we%2520investigate%2520natural%2520variants%2520where%2520an%2520agent%2527s%2520cost%2520is%2520defined%2520by%2520distance%2520thresholds%252C%2520or%2520by%2520the%2520maximum%2520or%2520average%2520value%2520difference%2520to%2520the%2520other%2520agents%2520in%2520its%2520coalition.%2520For%2520these%2520settings%252C%2520we%2520study%2520the%2520existence%2520of%2520stable%2520coalition%2520structures%252C%2520their%2520properties%252C%2520and%2520their%2520quality%2520in%2520terms%2520of%2520the%2520price%2520of%2520anarchy%2520and%2520the%2520price%2520of%2520stability.%2520Further%252C%2520we%2520investigate%2520the%2520impact%2520of%2520limiting%2520the%2520maximum%2520number%2520of%2520coalitions.%2520Despite%2520the%2520simple%2520setting%2520with%2520metric%2520distances%2520on%2520a%2520line%252C%2520we%2520uncover%2520a%2520rich%2520landscape%2520of%2520models%252C%2520partially%2520with%2520counter-intuitive%2520behavior.%2520Also%252C%2520our%2520focus%2520on%2520both%2520swap%2520stability%2520and%2520jump%2520stability%2520allows%2520us%2520to%2520study%2520the%2520influence%2520of%2520fixing%2520the%2520number%2520and%2520the%2520size%2520of%2520the%2520coalitions.%2520Overall%252C%2520we%2520find%2520that%2520stable%2520coalition%2520structures%2520always%2520exist%2520but%2520that%2520their%2520properties%2520and%2520quality%2520can%2520vary%2520widely.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Metric%20Hedonic%20Games%20on%20the%20Line&entry.906535625=Merlin%20de%20la%20Haye%20and%20Pascal%20Lenzner%20and%20Farehe%20Soheil%20and%20Marcus%20Wunderlich&entry.1292438233=Hedonic%20games%20are%20fundamental%20models%20for%20investigating%20the%20formation%20of%20coalitions%20among%20a%20set%20of%20strategic%20agents%2C%20where%20every%20agent%20has%20a%20certain%20utility%20for%20every%20possible%20coalition%20of%20agents%20it%20can%20be%20part%20of.%20To%20avoid%20the%20intractability%20of%20defining%20exponentially%20many%20utilities%20for%20all%20possible%20coalitions%2C%20many%20variants%20with%20succinct%20representations%20of%20the%20agents%27%20utility%20functions%20have%20been%20devised%20and%20analyzed%2C%20e.g.%2C%20modified%20fractional%20hedonic%20games%20by%20Monaco%20et%20al.%20%5BJAAMAS%202020%5D.%20We%20extend%20this%20by%20studying%20a%20novel%20succinct%20variant%20that%20is%20related%20to%20modified%20fractional%20hedonic%20games.%20In%20our%20model%2C%20each%20agent%20has%20a%20fixed%20type-value%20and%20an%20agent%27s%20cost%20for%20some%20given%20coalition%20is%20based%20on%20the%20differences%20between%20its%20value%20and%20those%20of%20the%20other%20members%20of%20its%20coalition.%20This%20allows%20to%20model%20natural%20situations%20like%20athletes%20forming%20training%20groups%20with%20similar%20performance%20levels%20or%20voters%20that%20partition%20themselves%20along%20a%20political%20spectrum.%0A%20%20In%20particular%2C%20we%20investigate%20natural%20variants%20where%20an%20agent%27s%20cost%20is%20defined%20by%20distance%20thresholds%2C%20or%20by%20the%20maximum%20or%20average%20value%20difference%20to%20the%20other%20agents%20in%20its%20coalition.%20For%20these%20settings%2C%20we%20study%20the%20existence%20of%20stable%20coalition%20structures%2C%20their%20properties%2C%20and%20their%20quality%20in%20terms%20of%20the%20price%20of%20anarchy%20and%20the%20price%20of%20stability.%20Further%2C%20we%20investigate%20the%20impact%20of%20limiting%20the%20maximum%20number%20of%20coalitions.%20Despite%20the%20simple%20setting%20with%20metric%20distances%20on%20a%20line%2C%20we%20uncover%20a%20rich%20landscape%20of%20models%2C%20partially%20with%20counter-intuitive%20behavior.%20Also%2C%20our%20focus%20on%20both%20swap%20stability%20and%20jump%20stability%20allows%20us%20to%20study%20the%20influence%20of%20fixing%20the%20number%20and%20the%20size%20of%20the%20coalitions.%20Overall%2C%20we%20find%20that%20stable%20coalition%20structures%20always%20exist%20but%20that%20their%20properties%20and%20quality%20can%20vary%20widely.&entry.1838667208=http%3A//arxiv.org/abs/2602.05888v1&entry.124074799=Read"},
{"title": "EdgeMask-DG*: Learning Domain-Invariant Graph Structures via Adversarial Edge Masking", "author": "Rishabh Bhattacharya and Naresh Manwani", "abstract": "Structural shifts pose a significant challenge for graph neural networks, as graph topology acts as a covariate that can vary across domains. Existing domain generalization methods rely on fixed structural augmentations or training on globally perturbed graphs, mechanisms that do not pinpoint which specific edges encode domain-invariant information. We argue that domain-invariant structural information is not rigidly tied to a single topology but resides in the consensus across multiple graph structures derived from topology and feature similarity. To capture this, we first propose EdgeMask-DG, a novel min-max algorithm where an edge masker learns to find worst-case continuous masks subject to a sparsity constraint, compelling a task GNN to perform effectively under these adversarial structural perturbations. Building upon this, we introduce EdgeMask-DG*, an extension that applies this adversarial masking principle to an enriched graph. This enriched graph combines the original topology with feature-derived edges, allowing the model to discover invariances even when the original topology is noisy or domain-specific. EdgeMask-DG* is the first to systematically combine adaptive adversarial topology search with feature-enriched graphs. We provide a formal justification for our approach from a robust optimization perspective. We demonstrate that EdgeMask-DG* achieves new state-of-the-art performance on diverse graph domain generalization benchmarks, including citation networks, social networks, and temporal graphs. Notably, on the Cora OOD benchmark, EdgeMask-DG* lifts the worst-case domain accuracy to 78.0\\%, a +3.8 pp improvement over the prior state of the art (74.2\\%). The source code for our experiments can be found here: https://anonymous.4open.science/r/TMLR-EAEF/", "link": "http://arxiv.org/abs/2602.05571v1", "date": "2026-02-05", "relevancy": 2.141, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5364}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.535}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EdgeMask-DG%2A%3A%20Learning%20Domain-Invariant%20Graph%20Structures%20via%20Adversarial%20Edge%20Masking&body=Title%3A%20EdgeMask-DG%2A%3A%20Learning%20Domain-Invariant%20Graph%20Structures%20via%20Adversarial%20Edge%20Masking%0AAuthor%3A%20Rishabh%20Bhattacharya%20and%20Naresh%20Manwani%0AAbstract%3A%20Structural%20shifts%20pose%20a%20significant%20challenge%20for%20graph%20neural%20networks%2C%20as%20graph%20topology%20acts%20as%20a%20covariate%20that%20can%20vary%20across%20domains.%20Existing%20domain%20generalization%20methods%20rely%20on%20fixed%20structural%20augmentations%20or%20training%20on%20globally%20perturbed%20graphs%2C%20mechanisms%20that%20do%20not%20pinpoint%20which%20specific%20edges%20encode%20domain-invariant%20information.%20We%20argue%20that%20domain-invariant%20structural%20information%20is%20not%20rigidly%20tied%20to%20a%20single%20topology%20but%20resides%20in%20the%20consensus%20across%20multiple%20graph%20structures%20derived%20from%20topology%20and%20feature%20similarity.%20To%20capture%20this%2C%20we%20first%20propose%20EdgeMask-DG%2C%20a%20novel%20min-max%20algorithm%20where%20an%20edge%20masker%20learns%20to%20find%20worst-case%20continuous%20masks%20subject%20to%20a%20sparsity%20constraint%2C%20compelling%20a%20task%20GNN%20to%20perform%20effectively%20under%20these%20adversarial%20structural%20perturbations.%20Building%20upon%20this%2C%20we%20introduce%20EdgeMask-DG%2A%2C%20an%20extension%20that%20applies%20this%20adversarial%20masking%20principle%20to%20an%20enriched%20graph.%20This%20enriched%20graph%20combines%20the%20original%20topology%20with%20feature-derived%20edges%2C%20allowing%20the%20model%20to%20discover%20invariances%20even%20when%20the%20original%20topology%20is%20noisy%20or%20domain-specific.%20EdgeMask-DG%2A%20is%20the%20first%20to%20systematically%20combine%20adaptive%20adversarial%20topology%20search%20with%20feature-enriched%20graphs.%20We%20provide%20a%20formal%20justification%20for%20our%20approach%20from%20a%20robust%20optimization%20perspective.%20We%20demonstrate%20that%20EdgeMask-DG%2A%20achieves%20new%20state-of-the-art%20performance%20on%20diverse%20graph%20domain%20generalization%20benchmarks%2C%20including%20citation%20networks%2C%20social%20networks%2C%20and%20temporal%20graphs.%20Notably%2C%20on%20the%20Cora%20OOD%20benchmark%2C%20EdgeMask-DG%2A%20lifts%20the%20worst-case%20domain%20accuracy%20to%2078.0%5C%25%2C%20a%20%2B3.8%20pp%20improvement%20over%20the%20prior%20state%20of%20the%20art%20%2874.2%5C%25%29.%20The%20source%20code%20for%20our%20experiments%20can%20be%20found%20here%3A%20https%3A//anonymous.4open.science/r/TMLR-EAEF/%0ALink%3A%20http%3A//arxiv.org/abs/2602.05571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdgeMask-DG%252A%253A%2520Learning%2520Domain-Invariant%2520Graph%2520Structures%2520via%2520Adversarial%2520Edge%2520Masking%26entry.906535625%3DRishabh%2520Bhattacharya%2520and%2520Naresh%2520Manwani%26entry.1292438233%3DStructural%2520shifts%2520pose%2520a%2520significant%2520challenge%2520for%2520graph%2520neural%2520networks%252C%2520as%2520graph%2520topology%2520acts%2520as%2520a%2520covariate%2520that%2520can%2520vary%2520across%2520domains.%2520Existing%2520domain%2520generalization%2520methods%2520rely%2520on%2520fixed%2520structural%2520augmentations%2520or%2520training%2520on%2520globally%2520perturbed%2520graphs%252C%2520mechanisms%2520that%2520do%2520not%2520pinpoint%2520which%2520specific%2520edges%2520encode%2520domain-invariant%2520information.%2520We%2520argue%2520that%2520domain-invariant%2520structural%2520information%2520is%2520not%2520rigidly%2520tied%2520to%2520a%2520single%2520topology%2520but%2520resides%2520in%2520the%2520consensus%2520across%2520multiple%2520graph%2520structures%2520derived%2520from%2520topology%2520and%2520feature%2520similarity.%2520To%2520capture%2520this%252C%2520we%2520first%2520propose%2520EdgeMask-DG%252C%2520a%2520novel%2520min-max%2520algorithm%2520where%2520an%2520edge%2520masker%2520learns%2520to%2520find%2520worst-case%2520continuous%2520masks%2520subject%2520to%2520a%2520sparsity%2520constraint%252C%2520compelling%2520a%2520task%2520GNN%2520to%2520perform%2520effectively%2520under%2520these%2520adversarial%2520structural%2520perturbations.%2520Building%2520upon%2520this%252C%2520we%2520introduce%2520EdgeMask-DG%252A%252C%2520an%2520extension%2520that%2520applies%2520this%2520adversarial%2520masking%2520principle%2520to%2520an%2520enriched%2520graph.%2520This%2520enriched%2520graph%2520combines%2520the%2520original%2520topology%2520with%2520feature-derived%2520edges%252C%2520allowing%2520the%2520model%2520to%2520discover%2520invariances%2520even%2520when%2520the%2520original%2520topology%2520is%2520noisy%2520or%2520domain-specific.%2520EdgeMask-DG%252A%2520is%2520the%2520first%2520to%2520systematically%2520combine%2520adaptive%2520adversarial%2520topology%2520search%2520with%2520feature-enriched%2520graphs.%2520We%2520provide%2520a%2520formal%2520justification%2520for%2520our%2520approach%2520from%2520a%2520robust%2520optimization%2520perspective.%2520We%2520demonstrate%2520that%2520EdgeMask-DG%252A%2520achieves%2520new%2520state-of-the-art%2520performance%2520on%2520diverse%2520graph%2520domain%2520generalization%2520benchmarks%252C%2520including%2520citation%2520networks%252C%2520social%2520networks%252C%2520and%2520temporal%2520graphs.%2520Notably%252C%2520on%2520the%2520Cora%2520OOD%2520benchmark%252C%2520EdgeMask-DG%252A%2520lifts%2520the%2520worst-case%2520domain%2520accuracy%2520to%252078.0%255C%2525%252C%2520a%2520%252B3.8%2520pp%2520improvement%2520over%2520the%2520prior%2520state%2520of%2520the%2520art%2520%252874.2%255C%2525%2529.%2520The%2520source%2520code%2520for%2520our%2520experiments%2520can%2520be%2520found%2520here%253A%2520https%253A//anonymous.4open.science/r/TMLR-EAEF/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EdgeMask-DG%2A%3A%20Learning%20Domain-Invariant%20Graph%20Structures%20via%20Adversarial%20Edge%20Masking&entry.906535625=Rishabh%20Bhattacharya%20and%20Naresh%20Manwani&entry.1292438233=Structural%20shifts%20pose%20a%20significant%20challenge%20for%20graph%20neural%20networks%2C%20as%20graph%20topology%20acts%20as%20a%20covariate%20that%20can%20vary%20across%20domains.%20Existing%20domain%20generalization%20methods%20rely%20on%20fixed%20structural%20augmentations%20or%20training%20on%20globally%20perturbed%20graphs%2C%20mechanisms%20that%20do%20not%20pinpoint%20which%20specific%20edges%20encode%20domain-invariant%20information.%20We%20argue%20that%20domain-invariant%20structural%20information%20is%20not%20rigidly%20tied%20to%20a%20single%20topology%20but%20resides%20in%20the%20consensus%20across%20multiple%20graph%20structures%20derived%20from%20topology%20and%20feature%20similarity.%20To%20capture%20this%2C%20we%20first%20propose%20EdgeMask-DG%2C%20a%20novel%20min-max%20algorithm%20where%20an%20edge%20masker%20learns%20to%20find%20worst-case%20continuous%20masks%20subject%20to%20a%20sparsity%20constraint%2C%20compelling%20a%20task%20GNN%20to%20perform%20effectively%20under%20these%20adversarial%20structural%20perturbations.%20Building%20upon%20this%2C%20we%20introduce%20EdgeMask-DG%2A%2C%20an%20extension%20that%20applies%20this%20adversarial%20masking%20principle%20to%20an%20enriched%20graph.%20This%20enriched%20graph%20combines%20the%20original%20topology%20with%20feature-derived%20edges%2C%20allowing%20the%20model%20to%20discover%20invariances%20even%20when%20the%20original%20topology%20is%20noisy%20or%20domain-specific.%20EdgeMask-DG%2A%20is%20the%20first%20to%20systematically%20combine%20adaptive%20adversarial%20topology%20search%20with%20feature-enriched%20graphs.%20We%20provide%20a%20formal%20justification%20for%20our%20approach%20from%20a%20robust%20optimization%20perspective.%20We%20demonstrate%20that%20EdgeMask-DG%2A%20achieves%20new%20state-of-the-art%20performance%20on%20diverse%20graph%20domain%20generalization%20benchmarks%2C%20including%20citation%20networks%2C%20social%20networks%2C%20and%20temporal%20graphs.%20Notably%2C%20on%20the%20Cora%20OOD%20benchmark%2C%20EdgeMask-DG%2A%20lifts%20the%20worst-case%20domain%20accuracy%20to%2078.0%5C%25%2C%20a%20%2B3.8%20pp%20improvement%20over%20the%20prior%20state%20of%20the%20art%20%2874.2%5C%25%29.%20The%20source%20code%20for%20our%20experiments%20can%20be%20found%20here%3A%20https%3A//anonymous.4open.science/r/TMLR-EAEF/&entry.1838667208=http%3A//arxiv.org/abs/2602.05571v1&entry.124074799=Read"},
{"title": "A Unified Multimodal Framework for Dataset Construction and Model-Based Diagnosis of Ameloblastoma", "author": "Ajo Babu George and Anna Mariam John and Athul Anoop and Balu Bhasuran", "abstract": "Artificial intelligence (AI)-enabled diagnostics in maxillofacial pathology require structured, high-quality multimodal datasets. However, existing resources provide limited ameloblastoma coverage and lack the format consistency needed for direct model training. We present a newly curated multimodal dataset specifically focused on ameloblastoma, integrating annotated radiological, histopathological, and intraoral clinical images with structured data derived from case reports. Natural language processing techniques were employed to extract clinically relevant features from textual reports, while image data underwent domain specific preprocessing and augmentation. Using this dataset, a multimodal deep learning model was developed to classify ameloblastoma variants, assess behavioral patterns such as recurrence risk, and support surgical planning. The model is designed to accept clinical inputs such as presenting complaint, age, and gender during deployment to enhance personalized inference. Quantitative evaluation demonstrated substantial improvements; variant classification accuracy increased from 46.2 percent to 65.9 percent, and abnormal tissue detection F1-score improved from 43.0 percent to 90.3 percent. Benchmarked against resources like MultiCaRe, this work advances patient-specific decision support by providing both a robust dataset and an adaptable multimodal AI framework.", "link": "http://arxiv.org/abs/2602.05515v1", "date": "2026-02-05", "relevancy": 1.4835, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5084}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4863}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Multimodal%20Framework%20for%20Dataset%20Construction%20and%20Model-Based%20Diagnosis%20of%20Ameloblastoma&body=Title%3A%20A%20Unified%20Multimodal%20Framework%20for%20Dataset%20Construction%20and%20Model-Based%20Diagnosis%20of%20Ameloblastoma%0AAuthor%3A%20Ajo%20Babu%20George%20and%20Anna%20Mariam%20John%20and%20Athul%20Anoop%20and%20Balu%20Bhasuran%0AAbstract%3A%20Artificial%20intelligence%20%28AI%29-enabled%20diagnostics%20in%20maxillofacial%20pathology%20require%20structured%2C%20high-quality%20multimodal%20datasets.%20However%2C%20existing%20resources%20provide%20limited%20ameloblastoma%20coverage%20and%20lack%20the%20format%20consistency%20needed%20for%20direct%20model%20training.%20We%20present%20a%20newly%20curated%20multimodal%20dataset%20specifically%20focused%20on%20ameloblastoma%2C%20integrating%20annotated%20radiological%2C%20histopathological%2C%20and%20intraoral%20clinical%20images%20with%20structured%20data%20derived%20from%20case%20reports.%20Natural%20language%20processing%20techniques%20were%20employed%20to%20extract%20clinically%20relevant%20features%20from%20textual%20reports%2C%20while%20image%20data%20underwent%20domain%20specific%20preprocessing%20and%20augmentation.%20Using%20this%20dataset%2C%20a%20multimodal%20deep%20learning%20model%20was%20developed%20to%20classify%20ameloblastoma%20variants%2C%20assess%20behavioral%20patterns%20such%20as%20recurrence%20risk%2C%20and%20support%20surgical%20planning.%20The%20model%20is%20designed%20to%20accept%20clinical%20inputs%20such%20as%20presenting%20complaint%2C%20age%2C%20and%20gender%20during%20deployment%20to%20enhance%20personalized%20inference.%20Quantitative%20evaluation%20demonstrated%20substantial%20improvements%3B%20variant%20classification%20accuracy%20increased%20from%2046.2%20percent%20to%2065.9%20percent%2C%20and%20abnormal%20tissue%20detection%20F1-score%20improved%20from%2043.0%20percent%20to%2090.3%20percent.%20Benchmarked%20against%20resources%20like%20MultiCaRe%2C%20this%20work%20advances%20patient-specific%20decision%20support%20by%20providing%20both%20a%20robust%20dataset%20and%20an%20adaptable%20multimodal%20AI%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Multimodal%2520Framework%2520for%2520Dataset%2520Construction%2520and%2520Model-Based%2520Diagnosis%2520of%2520Ameloblastoma%26entry.906535625%3DAjo%2520Babu%2520George%2520and%2520Anna%2520Mariam%2520John%2520and%2520Athul%2520Anoop%2520and%2520Balu%2520Bhasuran%26entry.1292438233%3DArtificial%2520intelligence%2520%2528AI%2529-enabled%2520diagnostics%2520in%2520maxillofacial%2520pathology%2520require%2520structured%252C%2520high-quality%2520multimodal%2520datasets.%2520However%252C%2520existing%2520resources%2520provide%2520limited%2520ameloblastoma%2520coverage%2520and%2520lack%2520the%2520format%2520consistency%2520needed%2520for%2520direct%2520model%2520training.%2520We%2520present%2520a%2520newly%2520curated%2520multimodal%2520dataset%2520specifically%2520focused%2520on%2520ameloblastoma%252C%2520integrating%2520annotated%2520radiological%252C%2520histopathological%252C%2520and%2520intraoral%2520clinical%2520images%2520with%2520structured%2520data%2520derived%2520from%2520case%2520reports.%2520Natural%2520language%2520processing%2520techniques%2520were%2520employed%2520to%2520extract%2520clinically%2520relevant%2520features%2520from%2520textual%2520reports%252C%2520while%2520image%2520data%2520underwent%2520domain%2520specific%2520preprocessing%2520and%2520augmentation.%2520Using%2520this%2520dataset%252C%2520a%2520multimodal%2520deep%2520learning%2520model%2520was%2520developed%2520to%2520classify%2520ameloblastoma%2520variants%252C%2520assess%2520behavioral%2520patterns%2520such%2520as%2520recurrence%2520risk%252C%2520and%2520support%2520surgical%2520planning.%2520The%2520model%2520is%2520designed%2520to%2520accept%2520clinical%2520inputs%2520such%2520as%2520presenting%2520complaint%252C%2520age%252C%2520and%2520gender%2520during%2520deployment%2520to%2520enhance%2520personalized%2520inference.%2520Quantitative%2520evaluation%2520demonstrated%2520substantial%2520improvements%253B%2520variant%2520classification%2520accuracy%2520increased%2520from%252046.2%2520percent%2520to%252065.9%2520percent%252C%2520and%2520abnormal%2520tissue%2520detection%2520F1-score%2520improved%2520from%252043.0%2520percent%2520to%252090.3%2520percent.%2520Benchmarked%2520against%2520resources%2520like%2520MultiCaRe%252C%2520this%2520work%2520advances%2520patient-specific%2520decision%2520support%2520by%2520providing%2520both%2520a%2520robust%2520dataset%2520and%2520an%2520adaptable%2520multimodal%2520AI%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Multimodal%20Framework%20for%20Dataset%20Construction%20and%20Model-Based%20Diagnosis%20of%20Ameloblastoma&entry.906535625=Ajo%20Babu%20George%20and%20Anna%20Mariam%20John%20and%20Athul%20Anoop%20and%20Balu%20Bhasuran&entry.1292438233=Artificial%20intelligence%20%28AI%29-enabled%20diagnostics%20in%20maxillofacial%20pathology%20require%20structured%2C%20high-quality%20multimodal%20datasets.%20However%2C%20existing%20resources%20provide%20limited%20ameloblastoma%20coverage%20and%20lack%20the%20format%20consistency%20needed%20for%20direct%20model%20training.%20We%20present%20a%20newly%20curated%20multimodal%20dataset%20specifically%20focused%20on%20ameloblastoma%2C%20integrating%20annotated%20radiological%2C%20histopathological%2C%20and%20intraoral%20clinical%20images%20with%20structured%20data%20derived%20from%20case%20reports.%20Natural%20language%20processing%20techniques%20were%20employed%20to%20extract%20clinically%20relevant%20features%20from%20textual%20reports%2C%20while%20image%20data%20underwent%20domain%20specific%20preprocessing%20and%20augmentation.%20Using%20this%20dataset%2C%20a%20multimodal%20deep%20learning%20model%20was%20developed%20to%20classify%20ameloblastoma%20variants%2C%20assess%20behavioral%20patterns%20such%20as%20recurrence%20risk%2C%20and%20support%20surgical%20planning.%20The%20model%20is%20designed%20to%20accept%20clinical%20inputs%20such%20as%20presenting%20complaint%2C%20age%2C%20and%20gender%20during%20deployment%20to%20enhance%20personalized%20inference.%20Quantitative%20evaluation%20demonstrated%20substantial%20improvements%3B%20variant%20classification%20accuracy%20increased%20from%2046.2%20percent%20to%2065.9%20percent%2C%20and%20abnormal%20tissue%20detection%20F1-score%20improved%20from%2043.0%20percent%20to%2090.3%20percent.%20Benchmarked%20against%20resources%20like%20MultiCaRe%2C%20this%20work%20advances%20patient-specific%20decision%20support%20by%20providing%20both%20a%20robust%20dataset%20and%20an%20adaptable%20multimodal%20AI%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2602.05515v1&entry.124074799=Read"},
{"title": "Structural Disentanglement in Bilinear MLPs via Architectural Inductive Bias", "author": "Ojasva Nema and Kaustubh Sharma and Aditya Chauhan and Parikshit Pareek", "abstract": "Selective unlearning and long-horizon extrapolation remain fragile in modern neural networks, even when tasks have underlying algebraic structure. In this work, we argue that these failures arise not solely from optimization or unlearning algorithms, but from how models structure their internal representations during training. We explore if having explicit multiplicative interactions as an architectural inductive bias helps in structural disentanglement, through Bilinear MLPs. We show analytically that bilinear parameterizations possess a `non-mixing' property under gradient flow conditions, where functional components separate into orthogonal subspace representations. This provides a mathematical foundation for surgical model modification. We validate this hypothesis through a series of controlled experiments spanning modular arithmetic, cyclic reasoning, Lie group dynamics, and targeted unlearning benchmarks. Unlike pointwise nonlinear networks, multiplicative architectures are able to recover true operators aligned with the underlying algebraic structure. Our results suggest that model editability and generalization are constrained by representational structure, and that architectural inductive bias plays a central role in enabling reliable unlearning.", "link": "http://arxiv.org/abs/2602.05635v1", "date": "2026-02-05", "relevancy": 1.88, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4929}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4667}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structural%20Disentanglement%20in%20Bilinear%20MLPs%20via%20Architectural%20Inductive%20Bias&body=Title%3A%20Structural%20Disentanglement%20in%20Bilinear%20MLPs%20via%20Architectural%20Inductive%20Bias%0AAuthor%3A%20Ojasva%20Nema%20and%20Kaustubh%20Sharma%20and%20Aditya%20Chauhan%20and%20Parikshit%20Pareek%0AAbstract%3A%20Selective%20unlearning%20and%20long-horizon%20extrapolation%20remain%20fragile%20in%20modern%20neural%20networks%2C%20even%20when%20tasks%20have%20underlying%20algebraic%20structure.%20In%20this%20work%2C%20we%20argue%20that%20these%20failures%20arise%20not%20solely%20from%20optimization%20or%20unlearning%20algorithms%2C%20but%20from%20how%20models%20structure%20their%20internal%20representations%20during%20training.%20We%20explore%20if%20having%20explicit%20multiplicative%20interactions%20as%20an%20architectural%20inductive%20bias%20helps%20in%20structural%20disentanglement%2C%20through%20Bilinear%20MLPs.%20We%20show%20analytically%20that%20bilinear%20parameterizations%20possess%20a%20%60non-mixing%27%20property%20under%20gradient%20flow%20conditions%2C%20where%20functional%20components%20separate%20into%20orthogonal%20subspace%20representations.%20This%20provides%20a%20mathematical%20foundation%20for%20surgical%20model%20modification.%20We%20validate%20this%20hypothesis%20through%20a%20series%20of%20controlled%20experiments%20spanning%20modular%20arithmetic%2C%20cyclic%20reasoning%2C%20Lie%20group%20dynamics%2C%20and%20targeted%20unlearning%20benchmarks.%20Unlike%20pointwise%20nonlinear%20networks%2C%20multiplicative%20architectures%20are%20able%20to%20recover%20true%20operators%20aligned%20with%20the%20underlying%20algebraic%20structure.%20Our%20results%20suggest%20that%20model%20editability%20and%20generalization%20are%20constrained%20by%20representational%20structure%2C%20and%20that%20architectural%20inductive%20bias%20plays%20a%20central%20role%20in%20enabling%20reliable%20unlearning.%0ALink%3A%20http%3A//arxiv.org/abs/2602.05635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructural%2520Disentanglement%2520in%2520Bilinear%2520MLPs%2520via%2520Architectural%2520Inductive%2520Bias%26entry.906535625%3DOjasva%2520Nema%2520and%2520Kaustubh%2520Sharma%2520and%2520Aditya%2520Chauhan%2520and%2520Parikshit%2520Pareek%26entry.1292438233%3DSelective%2520unlearning%2520and%2520long-horizon%2520extrapolation%2520remain%2520fragile%2520in%2520modern%2520neural%2520networks%252C%2520even%2520when%2520tasks%2520have%2520underlying%2520algebraic%2520structure.%2520In%2520this%2520work%252C%2520we%2520argue%2520that%2520these%2520failures%2520arise%2520not%2520solely%2520from%2520optimization%2520or%2520unlearning%2520algorithms%252C%2520but%2520from%2520how%2520models%2520structure%2520their%2520internal%2520representations%2520during%2520training.%2520We%2520explore%2520if%2520having%2520explicit%2520multiplicative%2520interactions%2520as%2520an%2520architectural%2520inductive%2520bias%2520helps%2520in%2520structural%2520disentanglement%252C%2520through%2520Bilinear%2520MLPs.%2520We%2520show%2520analytically%2520that%2520bilinear%2520parameterizations%2520possess%2520a%2520%2560non-mixing%2527%2520property%2520under%2520gradient%2520flow%2520conditions%252C%2520where%2520functional%2520components%2520separate%2520into%2520orthogonal%2520subspace%2520representations.%2520This%2520provides%2520a%2520mathematical%2520foundation%2520for%2520surgical%2520model%2520modification.%2520We%2520validate%2520this%2520hypothesis%2520through%2520a%2520series%2520of%2520controlled%2520experiments%2520spanning%2520modular%2520arithmetic%252C%2520cyclic%2520reasoning%252C%2520Lie%2520group%2520dynamics%252C%2520and%2520targeted%2520unlearning%2520benchmarks.%2520Unlike%2520pointwise%2520nonlinear%2520networks%252C%2520multiplicative%2520architectures%2520are%2520able%2520to%2520recover%2520true%2520operators%2520aligned%2520with%2520the%2520underlying%2520algebraic%2520structure.%2520Our%2520results%2520suggest%2520that%2520model%2520editability%2520and%2520generalization%2520are%2520constrained%2520by%2520representational%2520structure%252C%2520and%2520that%2520architectural%2520inductive%2520bias%2520plays%2520a%2520central%2520role%2520in%2520enabling%2520reliable%2520unlearning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.05635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structural%20Disentanglement%20in%20Bilinear%20MLPs%20via%20Architectural%20Inductive%20Bias&entry.906535625=Ojasva%20Nema%20and%20Kaustubh%20Sharma%20and%20Aditya%20Chauhan%20and%20Parikshit%20Pareek&entry.1292438233=Selective%20unlearning%20and%20long-horizon%20extrapolation%20remain%20fragile%20in%20modern%20neural%20networks%2C%20even%20when%20tasks%20have%20underlying%20algebraic%20structure.%20In%20this%20work%2C%20we%20argue%20that%20these%20failures%20arise%20not%20solely%20from%20optimization%20or%20unlearning%20algorithms%2C%20but%20from%20how%20models%20structure%20their%20internal%20representations%20during%20training.%20We%20explore%20if%20having%20explicit%20multiplicative%20interactions%20as%20an%20architectural%20inductive%20bias%20helps%20in%20structural%20disentanglement%2C%20through%20Bilinear%20MLPs.%20We%20show%20analytically%20that%20bilinear%20parameterizations%20possess%20a%20%60non-mixing%27%20property%20under%20gradient%20flow%20conditions%2C%20where%20functional%20components%20separate%20into%20orthogonal%20subspace%20representations.%20This%20provides%20a%20mathematical%20foundation%20for%20surgical%20model%20modification.%20We%20validate%20this%20hypothesis%20through%20a%20series%20of%20controlled%20experiments%20spanning%20modular%20arithmetic%2C%20cyclic%20reasoning%2C%20Lie%20group%20dynamics%2C%20and%20targeted%20unlearning%20benchmarks.%20Unlike%20pointwise%20nonlinear%20networks%2C%20multiplicative%20architectures%20are%20able%20to%20recover%20true%20operators%20aligned%20with%20the%20underlying%20algebraic%20structure.%20Our%20results%20suggest%20that%20model%20editability%20and%20generalization%20are%20constrained%20by%20representational%20structure%2C%20and%20that%20architectural%20inductive%20bias%20plays%20a%20central%20role%20in%20enabling%20reliable%20unlearning.&entry.1838667208=http%3A//arxiv.org/abs/2602.05635v1&entry.124074799=Read"},
{"title": "Transmuting prompts into weights", "author": "Hanna Mazzawi and Benoit Dherin and Michael Munn and Michael Wunder and Javier Gonzalvo", "abstract": "A growing body of research has demonstrated that the behavior of large language models can be effectively controlled at inference time by directly modifying their internal states, either through vector additions to their activations or through updates to their weight matrices. These techniques, while powerful, are often guided by empirical heuristics, such as deriving steering vectors from the average activations of contrastive prompts. This work provides a theoretical foundation for these interventions, explaining how they emerge from the fundamental computations of the transformer architecture. Building on the recent finding that a prompt's influence can be mathematically mapped to token-dependent implicit weight updates (Dherin et. al, 2025), we derive a principled method for condensing this information into token-independent thought vectors and thought matrices. These constructs provide a theoretical explanation for existing vector-and-matrix-based model editing techniques and offer a direct, computationally-grounded method for transmuting textual input into reusable weight updates.", "link": "http://arxiv.org/abs/2510.08734v2", "date": "2026-02-05", "relevancy": 1.4831, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5552}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4819}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transmuting%20prompts%20into%20weights&body=Title%3A%20Transmuting%20prompts%20into%20weights%0AAuthor%3A%20Hanna%20Mazzawi%20and%20Benoit%20Dherin%20and%20Michael%20Munn%20and%20Michael%20Wunder%20and%20Javier%20Gonzalvo%0AAbstract%3A%20A%20growing%20body%20of%20research%20has%20demonstrated%20that%20the%20behavior%20of%20large%20language%20models%20can%20be%20effectively%20controlled%20at%20inference%20time%20by%20directly%20modifying%20their%20internal%20states%2C%20either%20through%20vector%20additions%20to%20their%20activations%20or%20through%20updates%20to%20their%20weight%20matrices.%20These%20techniques%2C%20while%20powerful%2C%20are%20often%20guided%20by%20empirical%20heuristics%2C%20such%20as%20deriving%20steering%20vectors%20from%20the%20average%20activations%20of%20contrastive%20prompts.%20This%20work%20provides%20a%20theoretical%20foundation%20for%20these%20interventions%2C%20explaining%20how%20they%20emerge%20from%20the%20fundamental%20computations%20of%20the%20transformer%20architecture.%20Building%20on%20the%20recent%20finding%20that%20a%20prompt%27s%20influence%20can%20be%20mathematically%20mapped%20to%20token-dependent%20implicit%20weight%20updates%20%28Dherin%20et.%20al%2C%202025%29%2C%20we%20derive%20a%20principled%20method%20for%20condensing%20this%20information%20into%20token-independent%20thought%20vectors%20and%20thought%20matrices.%20These%20constructs%20provide%20a%20theoretical%20explanation%20for%20existing%20vector-and-matrix-based%20model%20editing%20techniques%20and%20offer%20a%20direct%2C%20computationally-grounded%20method%20for%20transmuting%20textual%20input%20into%20reusable%20weight%20updates.%0ALink%3A%20http%3A//arxiv.org/abs/2510.08734v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransmuting%2520prompts%2520into%2520weights%26entry.906535625%3DHanna%2520Mazzawi%2520and%2520Benoit%2520Dherin%2520and%2520Michael%2520Munn%2520and%2520Michael%2520Wunder%2520and%2520Javier%2520Gonzalvo%26entry.1292438233%3DA%2520growing%2520body%2520of%2520research%2520has%2520demonstrated%2520that%2520the%2520behavior%2520of%2520large%2520language%2520models%2520can%2520be%2520effectively%2520controlled%2520at%2520inference%2520time%2520by%2520directly%2520modifying%2520their%2520internal%2520states%252C%2520either%2520through%2520vector%2520additions%2520to%2520their%2520activations%2520or%2520through%2520updates%2520to%2520their%2520weight%2520matrices.%2520These%2520techniques%252C%2520while%2520powerful%252C%2520are%2520often%2520guided%2520by%2520empirical%2520heuristics%252C%2520such%2520as%2520deriving%2520steering%2520vectors%2520from%2520the%2520average%2520activations%2520of%2520contrastive%2520prompts.%2520This%2520work%2520provides%2520a%2520theoretical%2520foundation%2520for%2520these%2520interventions%252C%2520explaining%2520how%2520they%2520emerge%2520from%2520the%2520fundamental%2520computations%2520of%2520the%2520transformer%2520architecture.%2520Building%2520on%2520the%2520recent%2520finding%2520that%2520a%2520prompt%2527s%2520influence%2520can%2520be%2520mathematically%2520mapped%2520to%2520token-dependent%2520implicit%2520weight%2520updates%2520%2528Dherin%2520et.%2520al%252C%25202025%2529%252C%2520we%2520derive%2520a%2520principled%2520method%2520for%2520condensing%2520this%2520information%2520into%2520token-independent%2520thought%2520vectors%2520and%2520thought%2520matrices.%2520These%2520constructs%2520provide%2520a%2520theoretical%2520explanation%2520for%2520existing%2520vector-and-matrix-based%2520model%2520editing%2520techniques%2520and%2520offer%2520a%2520direct%252C%2520computationally-grounded%2520method%2520for%2520transmuting%2520textual%2520input%2520into%2520reusable%2520weight%2520updates.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08734v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transmuting%20prompts%20into%20weights&entry.906535625=Hanna%20Mazzawi%20and%20Benoit%20Dherin%20and%20Michael%20Munn%20and%20Michael%20Wunder%20and%20Javier%20Gonzalvo&entry.1292438233=A%20growing%20body%20of%20research%20has%20demonstrated%20that%20the%20behavior%20of%20large%20language%20models%20can%20be%20effectively%20controlled%20at%20inference%20time%20by%20directly%20modifying%20their%20internal%20states%2C%20either%20through%20vector%20additions%20to%20their%20activations%20or%20through%20updates%20to%20their%20weight%20matrices.%20These%20techniques%2C%20while%20powerful%2C%20are%20often%20guided%20by%20empirical%20heuristics%2C%20such%20as%20deriving%20steering%20vectors%20from%20the%20average%20activations%20of%20contrastive%20prompts.%20This%20work%20provides%20a%20theoretical%20foundation%20for%20these%20interventions%2C%20explaining%20how%20they%20emerge%20from%20the%20fundamental%20computations%20of%20the%20transformer%20architecture.%20Building%20on%20the%20recent%20finding%20that%20a%20prompt%27s%20influence%20can%20be%20mathematically%20mapped%20to%20token-dependent%20implicit%20weight%20updates%20%28Dherin%20et.%20al%2C%202025%29%2C%20we%20derive%20a%20principled%20method%20for%20condensing%20this%20information%20into%20token-independent%20thought%20vectors%20and%20thought%20matrices.%20These%20constructs%20provide%20a%20theoretical%20explanation%20for%20existing%20vector-and-matrix-based%20model%20editing%20techniques%20and%20offer%20a%20direct%2C%20computationally-grounded%20method%20for%20transmuting%20textual%20input%20into%20reusable%20weight%20updates.&entry.1838667208=http%3A//arxiv.org/abs/2510.08734v2&entry.124074799=Read"},
{"title": "Enhancing Quantum Diffusion Models for Complex Image Generation", "author": "Jeongbin Jo and Santanam Wishal and Shah Md Khalil Ullah and Shan Zeng and Dikshant Dulal", "abstract": "Quantum generative models offer a novel approach to exploring high-dimensional Hilbert spaces but face significant challenges in scalability and expressibility when applied to multi-modal distributions. In this study, we explore a Hybrid Quantum-Classical U-Net architecture integrated with Adaptive Non-Local Observables (ANO) as a potential solution to these hurdles. By compressing classical data into a dense quantum latent space and utilizing trainable observables, our model aims to extract non-local features that complement classical processing. We also investigate the role of Skip Connections in preserving semantic information during the reverse diffusion process. Experimental results on the full MNIST dataset (digits 0-9) demonstrate that the proposed architecture is capable of generating structurally coherent and recognizable images for all digit classes. While hardware constraints still impose limitations on resolution, our findings suggest that hybrid architectures with adaptive measurements provide a feasible pathway for mitigating mode collapse and enhancing generative capabilities in the NISQ era.", "link": "http://arxiv.org/abs/2602.03405v2", "date": "2026-02-05", "relevancy": 1.868, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6542}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6145}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Quantum%20Diffusion%20Models%20for%20Complex%20Image%20Generation&body=Title%3A%20Enhancing%20Quantum%20Diffusion%20Models%20for%20Complex%20Image%20Generation%0AAuthor%3A%20Jeongbin%20Jo%20and%20Santanam%20Wishal%20and%20Shah%20Md%20Khalil%20Ullah%20and%20Shan%20Zeng%20and%20Dikshant%20Dulal%0AAbstract%3A%20Quantum%20generative%20models%20offer%20a%20novel%20approach%20to%20exploring%20high-dimensional%20Hilbert%20spaces%20but%20face%20significant%20challenges%20in%20scalability%20and%20expressibility%20when%20applied%20to%20multi-modal%20distributions.%20In%20this%20study%2C%20we%20explore%20a%20Hybrid%20Quantum-Classical%20U-Net%20architecture%20integrated%20with%20Adaptive%20Non-Local%20Observables%20%28ANO%29%20as%20a%20potential%20solution%20to%20these%20hurdles.%20By%20compressing%20classical%20data%20into%20a%20dense%20quantum%20latent%20space%20and%20utilizing%20trainable%20observables%2C%20our%20model%20aims%20to%20extract%20non-local%20features%20that%20complement%20classical%20processing.%20We%20also%20investigate%20the%20role%20of%20Skip%20Connections%20in%20preserving%20semantic%20information%20during%20the%20reverse%20diffusion%20process.%20Experimental%20results%20on%20the%20full%20MNIST%20dataset%20%28digits%200-9%29%20demonstrate%20that%20the%20proposed%20architecture%20is%20capable%20of%20generating%20structurally%20coherent%20and%20recognizable%20images%20for%20all%20digit%20classes.%20While%20hardware%20constraints%20still%20impose%20limitations%20on%20resolution%2C%20our%20findings%20suggest%20that%20hybrid%20architectures%20with%20adaptive%20measurements%20provide%20a%20feasible%20pathway%20for%20mitigating%20mode%20collapse%20and%20enhancing%20generative%20capabilities%20in%20the%20NISQ%20era.%0ALink%3A%20http%3A//arxiv.org/abs/2602.03405v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Quantum%2520Diffusion%2520Models%2520for%2520Complex%2520Image%2520Generation%26entry.906535625%3DJeongbin%2520Jo%2520and%2520Santanam%2520Wishal%2520and%2520Shah%2520Md%2520Khalil%2520Ullah%2520and%2520Shan%2520Zeng%2520and%2520Dikshant%2520Dulal%26entry.1292438233%3DQuantum%2520generative%2520models%2520offer%2520a%2520novel%2520approach%2520to%2520exploring%2520high-dimensional%2520Hilbert%2520spaces%2520but%2520face%2520significant%2520challenges%2520in%2520scalability%2520and%2520expressibility%2520when%2520applied%2520to%2520multi-modal%2520distributions.%2520In%2520this%2520study%252C%2520we%2520explore%2520a%2520Hybrid%2520Quantum-Classical%2520U-Net%2520architecture%2520integrated%2520with%2520Adaptive%2520Non-Local%2520Observables%2520%2528ANO%2529%2520as%2520a%2520potential%2520solution%2520to%2520these%2520hurdles.%2520By%2520compressing%2520classical%2520data%2520into%2520a%2520dense%2520quantum%2520latent%2520space%2520and%2520utilizing%2520trainable%2520observables%252C%2520our%2520model%2520aims%2520to%2520extract%2520non-local%2520features%2520that%2520complement%2520classical%2520processing.%2520We%2520also%2520investigate%2520the%2520role%2520of%2520Skip%2520Connections%2520in%2520preserving%2520semantic%2520information%2520during%2520the%2520reverse%2520diffusion%2520process.%2520Experimental%2520results%2520on%2520the%2520full%2520MNIST%2520dataset%2520%2528digits%25200-9%2529%2520demonstrate%2520that%2520the%2520proposed%2520architecture%2520is%2520capable%2520of%2520generating%2520structurally%2520coherent%2520and%2520recognizable%2520images%2520for%2520all%2520digit%2520classes.%2520While%2520hardware%2520constraints%2520still%2520impose%2520limitations%2520on%2520resolution%252C%2520our%2520findings%2520suggest%2520that%2520hybrid%2520architectures%2520with%2520adaptive%2520measurements%2520provide%2520a%2520feasible%2520pathway%2520for%2520mitigating%2520mode%2520collapse%2520and%2520enhancing%2520generative%2520capabilities%2520in%2520the%2520NISQ%2520era.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.03405v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Quantum%20Diffusion%20Models%20for%20Complex%20Image%20Generation&entry.906535625=Jeongbin%20Jo%20and%20Santanam%20Wishal%20and%20Shah%20Md%20Khalil%20Ullah%20and%20Shan%20Zeng%20and%20Dikshant%20Dulal&entry.1292438233=Quantum%20generative%20models%20offer%20a%20novel%20approach%20to%20exploring%20high-dimensional%20Hilbert%20spaces%20but%20face%20significant%20challenges%20in%20scalability%20and%20expressibility%20when%20applied%20to%20multi-modal%20distributions.%20In%20this%20study%2C%20we%20explore%20a%20Hybrid%20Quantum-Classical%20U-Net%20architecture%20integrated%20with%20Adaptive%20Non-Local%20Observables%20%28ANO%29%20as%20a%20potential%20solution%20to%20these%20hurdles.%20By%20compressing%20classical%20data%20into%20a%20dense%20quantum%20latent%20space%20and%20utilizing%20trainable%20observables%2C%20our%20model%20aims%20to%20extract%20non-local%20features%20that%20complement%20classical%20processing.%20We%20also%20investigate%20the%20role%20of%20Skip%20Connections%20in%20preserving%20semantic%20information%20during%20the%20reverse%20diffusion%20process.%20Experimental%20results%20on%20the%20full%20MNIST%20dataset%20%28digits%200-9%29%20demonstrate%20that%20the%20proposed%20architecture%20is%20capable%20of%20generating%20structurally%20coherent%20and%20recognizable%20images%20for%20all%20digit%20classes.%20While%20hardware%20constraints%20still%20impose%20limitations%20on%20resolution%2C%20our%20findings%20suggest%20that%20hybrid%20architectures%20with%20adaptive%20measurements%20provide%20a%20feasible%20pathway%20for%20mitigating%20mode%20collapse%20and%20enhancing%20generative%20capabilities%20in%20the%20NISQ%20era.&entry.1838667208=http%3A//arxiv.org/abs/2602.03405v2&entry.124074799=Read"},
{"title": "Improved Generalization Bounds for Transductive Learning by Transductive Local Complexity and Its Applications", "author": "Yingzhen Yang", "abstract": "We introduce Transductive Local Complexity (TLC) to extend the classical Local Rademacher Complexity (LRC) to the transductive setting, incorporating substantial and novel components. Although LRC has been used to obtain sharp generalization bounds and minimax rates for inductive tasks such as classification and nonparametric regression, it has remained an open problem whether a localized Rademacher complexity framework can be effectively adapted to transductive learning to achieve sharp or nearly sharp bounds consistent with inductive results. We provide an affirmative answer via TLC. TLC is constructed by first deriving a new concentration inequality in Theorem 4.1 for the supremum of empirical processes capturing the gap between test and training losses, termed the test-train process, under uniform sampling without replacement, which leverages a novel combinatorial property of the test-train process and a new proof strategy applying the exponential Efron-Stein inequality twice. A subsequent peeling strategy applied to a new decomposition of the expectation of the test-train process and a new surrogate variance operator then yield excess risk bounds in the transductive setting that are nearly consistent with classical LRC-based inductive bounds up to a logarithmic gap. We further advance transductive learning through two applications: (1) for realizable transductive learning over binary-valued classes with finite VC dimension of $\\dVC$ and $u \\ge m \\ge \\dVC$, where $u$ and $m$ are the number of test features and training features, our Theorem 6.1 gives a nearly optimal bound $\u0398(\\dVC \\log(me/\\dVC)/m)$ matching the minimax rate $\u0398(\\dVC/m)$ up to $\\log m$, resolving a decade-old open question; and (2) Theorem 6.2 presents a sharper excess risk bound for transductive kernel learning compared to the current state-of-the-art.", "link": "http://arxiv.org/abs/2309.16858v5", "date": "2026-02-05", "relevancy": 1.9065, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4865}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4787}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Generalization%20Bounds%20for%20Transductive%20Learning%20by%20Transductive%20Local%20Complexity%20and%20Its%20Applications&body=Title%3A%20Improved%20Generalization%20Bounds%20for%20Transductive%20Learning%20by%20Transductive%20Local%20Complexity%20and%20Its%20Applications%0AAuthor%3A%20Yingzhen%20Yang%0AAbstract%3A%20We%20introduce%20Transductive%20Local%20Complexity%20%28TLC%29%20to%20extend%20the%20classical%20Local%20Rademacher%20Complexity%20%28LRC%29%20to%20the%20transductive%20setting%2C%20incorporating%20substantial%20and%20novel%20components.%20Although%20LRC%20has%20been%20used%20to%20obtain%20sharp%20generalization%20bounds%20and%20minimax%20rates%20for%20inductive%20tasks%20such%20as%20classification%20and%20nonparametric%20regression%2C%20it%20has%20remained%20an%20open%20problem%20whether%20a%20localized%20Rademacher%20complexity%20framework%20can%20be%20effectively%20adapted%20to%20transductive%20learning%20to%20achieve%20sharp%20or%20nearly%20sharp%20bounds%20consistent%20with%20inductive%20results.%20We%20provide%20an%20affirmative%20answer%20via%20TLC.%20TLC%20is%20constructed%20by%20first%20deriving%20a%20new%20concentration%20inequality%20in%20Theorem%204.1%20for%20the%20supremum%20of%20empirical%20processes%20capturing%20the%20gap%20between%20test%20and%20training%20losses%2C%20termed%20the%20test-train%20process%2C%20under%20uniform%20sampling%20without%20replacement%2C%20which%20leverages%20a%20novel%20combinatorial%20property%20of%20the%20test-train%20process%20and%20a%20new%20proof%20strategy%20applying%20the%20exponential%20Efron-Stein%20inequality%20twice.%20A%20subsequent%20peeling%20strategy%20applied%20to%20a%20new%20decomposition%20of%20the%20expectation%20of%20the%20test-train%20process%20and%20a%20new%20surrogate%20variance%20operator%20then%20yield%20excess%20risk%20bounds%20in%20the%20transductive%20setting%20that%20are%20nearly%20consistent%20with%20classical%20LRC-based%20inductive%20bounds%20up%20to%20a%20logarithmic%20gap.%20We%20further%20advance%20transductive%20learning%20through%20two%20applications%3A%20%281%29%20for%20realizable%20transductive%20learning%20over%20binary-valued%20classes%20with%20finite%20VC%20dimension%20of%20%24%5CdVC%24%20and%20%24u%20%5Cge%20m%20%5Cge%20%5CdVC%24%2C%20where%20%24u%24%20and%20%24m%24%20are%20the%20number%20of%20test%20features%20and%20training%20features%2C%20our%20Theorem%206.1%20gives%20a%20nearly%20optimal%20bound%20%24%CE%98%28%5CdVC%20%5Clog%28me/%5CdVC%29/m%29%24%20matching%20the%20minimax%20rate%20%24%CE%98%28%5CdVC/m%29%24%20up%20to%20%24%5Clog%20m%24%2C%20resolving%20a%20decade-old%20open%20question%3B%20and%20%282%29%20Theorem%206.2%20presents%20a%20sharper%20excess%20risk%20bound%20for%20transductive%20kernel%20learning%20compared%20to%20the%20current%20state-of-the-art.%0ALink%3A%20http%3A//arxiv.org/abs/2309.16858v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Generalization%2520Bounds%2520for%2520Transductive%2520Learning%2520by%2520Transductive%2520Local%2520Complexity%2520and%2520Its%2520Applications%26entry.906535625%3DYingzhen%2520Yang%26entry.1292438233%3DWe%2520introduce%2520Transductive%2520Local%2520Complexity%2520%2528TLC%2529%2520to%2520extend%2520the%2520classical%2520Local%2520Rademacher%2520Complexity%2520%2528LRC%2529%2520to%2520the%2520transductive%2520setting%252C%2520incorporating%2520substantial%2520and%2520novel%2520components.%2520Although%2520LRC%2520has%2520been%2520used%2520to%2520obtain%2520sharp%2520generalization%2520bounds%2520and%2520minimax%2520rates%2520for%2520inductive%2520tasks%2520such%2520as%2520classification%2520and%2520nonparametric%2520regression%252C%2520it%2520has%2520remained%2520an%2520open%2520problem%2520whether%2520a%2520localized%2520Rademacher%2520complexity%2520framework%2520can%2520be%2520effectively%2520adapted%2520to%2520transductive%2520learning%2520to%2520achieve%2520sharp%2520or%2520nearly%2520sharp%2520bounds%2520consistent%2520with%2520inductive%2520results.%2520We%2520provide%2520an%2520affirmative%2520answer%2520via%2520TLC.%2520TLC%2520is%2520constructed%2520by%2520first%2520deriving%2520a%2520new%2520concentration%2520inequality%2520in%2520Theorem%25204.1%2520for%2520the%2520supremum%2520of%2520empirical%2520processes%2520capturing%2520the%2520gap%2520between%2520test%2520and%2520training%2520losses%252C%2520termed%2520the%2520test-train%2520process%252C%2520under%2520uniform%2520sampling%2520without%2520replacement%252C%2520which%2520leverages%2520a%2520novel%2520combinatorial%2520property%2520of%2520the%2520test-train%2520process%2520and%2520a%2520new%2520proof%2520strategy%2520applying%2520the%2520exponential%2520Efron-Stein%2520inequality%2520twice.%2520A%2520subsequent%2520peeling%2520strategy%2520applied%2520to%2520a%2520new%2520decomposition%2520of%2520the%2520expectation%2520of%2520the%2520test-train%2520process%2520and%2520a%2520new%2520surrogate%2520variance%2520operator%2520then%2520yield%2520excess%2520risk%2520bounds%2520in%2520the%2520transductive%2520setting%2520that%2520are%2520nearly%2520consistent%2520with%2520classical%2520LRC-based%2520inductive%2520bounds%2520up%2520to%2520a%2520logarithmic%2520gap.%2520We%2520further%2520advance%2520transductive%2520learning%2520through%2520two%2520applications%253A%2520%25281%2529%2520for%2520realizable%2520transductive%2520learning%2520over%2520binary-valued%2520classes%2520with%2520finite%2520VC%2520dimension%2520of%2520%2524%255CdVC%2524%2520and%2520%2524u%2520%255Cge%2520m%2520%255Cge%2520%255CdVC%2524%252C%2520where%2520%2524u%2524%2520and%2520%2524m%2524%2520are%2520the%2520number%2520of%2520test%2520features%2520and%2520training%2520features%252C%2520our%2520Theorem%25206.1%2520gives%2520a%2520nearly%2520optimal%2520bound%2520%2524%25CE%2598%2528%255CdVC%2520%255Clog%2528me/%255CdVC%2529/m%2529%2524%2520matching%2520the%2520minimax%2520rate%2520%2524%25CE%2598%2528%255CdVC/m%2529%2524%2520up%2520to%2520%2524%255Clog%2520m%2524%252C%2520resolving%2520a%2520decade-old%2520open%2520question%253B%2520and%2520%25282%2529%2520Theorem%25206.2%2520presents%2520a%2520sharper%2520excess%2520risk%2520bound%2520for%2520transductive%2520kernel%2520learning%2520compared%2520to%2520the%2520current%2520state-of-the-art.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.16858v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Generalization%20Bounds%20for%20Transductive%20Learning%20by%20Transductive%20Local%20Complexity%20and%20Its%20Applications&entry.906535625=Yingzhen%20Yang&entry.1292438233=We%20introduce%20Transductive%20Local%20Complexity%20%28TLC%29%20to%20extend%20the%20classical%20Local%20Rademacher%20Complexity%20%28LRC%29%20to%20the%20transductive%20setting%2C%20incorporating%20substantial%20and%20novel%20components.%20Although%20LRC%20has%20been%20used%20to%20obtain%20sharp%20generalization%20bounds%20and%20minimax%20rates%20for%20inductive%20tasks%20such%20as%20classification%20and%20nonparametric%20regression%2C%20it%20has%20remained%20an%20open%20problem%20whether%20a%20localized%20Rademacher%20complexity%20framework%20can%20be%20effectively%20adapted%20to%20transductive%20learning%20to%20achieve%20sharp%20or%20nearly%20sharp%20bounds%20consistent%20with%20inductive%20results.%20We%20provide%20an%20affirmative%20answer%20via%20TLC.%20TLC%20is%20constructed%20by%20first%20deriving%20a%20new%20concentration%20inequality%20in%20Theorem%204.1%20for%20the%20supremum%20of%20empirical%20processes%20capturing%20the%20gap%20between%20test%20and%20training%20losses%2C%20termed%20the%20test-train%20process%2C%20under%20uniform%20sampling%20without%20replacement%2C%20which%20leverages%20a%20novel%20combinatorial%20property%20of%20the%20test-train%20process%20and%20a%20new%20proof%20strategy%20applying%20the%20exponential%20Efron-Stein%20inequality%20twice.%20A%20subsequent%20peeling%20strategy%20applied%20to%20a%20new%20decomposition%20of%20the%20expectation%20of%20the%20test-train%20process%20and%20a%20new%20surrogate%20variance%20operator%20then%20yield%20excess%20risk%20bounds%20in%20the%20transductive%20setting%20that%20are%20nearly%20consistent%20with%20classical%20LRC-based%20inductive%20bounds%20up%20to%20a%20logarithmic%20gap.%20We%20further%20advance%20transductive%20learning%20through%20two%20applications%3A%20%281%29%20for%20realizable%20transductive%20learning%20over%20binary-valued%20classes%20with%20finite%20VC%20dimension%20of%20%24%5CdVC%24%20and%20%24u%20%5Cge%20m%20%5Cge%20%5CdVC%24%2C%20where%20%24u%24%20and%20%24m%24%20are%20the%20number%20of%20test%20features%20and%20training%20features%2C%20our%20Theorem%206.1%20gives%20a%20nearly%20optimal%20bound%20%24%CE%98%28%5CdVC%20%5Clog%28me/%5CdVC%29/m%29%24%20matching%20the%20minimax%20rate%20%24%CE%98%28%5CdVC/m%29%24%20up%20to%20%24%5Clog%20m%24%2C%20resolving%20a%20decade-old%20open%20question%3B%20and%20%282%29%20Theorem%206.2%20presents%20a%20sharper%20excess%20risk%20bound%20for%20transductive%20kernel%20learning%20compared%20to%20the%20current%20state-of-the-art.&entry.1838667208=http%3A//arxiv.org/abs/2309.16858v5&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


