<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250225.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View\n  Gaussian Optimization", "author": "Onat \u015eahin and Mohammad Altillawi and George Eskandar and Carlos Carbone and Ziyuan Liu", "abstract": "  Recent advances in diffusion models have significantly improved 3D\ngeneration, enabling the use of assets generated from an image for embodied AI\nsimulations. However, the one-to-many nature of the image-to-3D problem limits\ntheir use due to inconsistent content and quality across views. Previous models\noptimize a 3D model by sampling views from a view-conditioned diffusion prior,\nbut diffusion models cannot guarantee view consistency. Instead, we present\nConsistentDreamer, where we first generate a set of fixed multi-view prior\nimages and sample random views between them with another diffusion model\nthrough a score distillation sampling (SDS) loss. Thereby, we limit the\ndiscrepancies between the views guided by the SDS loss and ensure a consistent\nrough shape. In each iteration, we also use our generated multi-view prior\nimages for fine-detail reconstruction. To balance between the rough shape and\nthe fine-detail optimizations, we introduce dynamic task-dependent weights\nbased on homoscedastic uncertainty, updated automatically in each iteration.\nAdditionally, we employ opacity, depth distortion, and normal alignment losses\nto refine the surface for mesh extraction. Our method ensures better view\nconsistency and visual quality compared to the state-of-the-art.\n", "link": "http://arxiv.org/abs/2502.09278v3", "date": "2025-02-25", "relevancy": 3.3748, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6905}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6672}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConsistentDreamer%3A%20View-Consistent%20Meshes%20Through%20Balanced%20Multi-View%0A%20%20Gaussian%20Optimization&body=Title%3A%20ConsistentDreamer%3A%20View-Consistent%20Meshes%20Through%20Balanced%20Multi-View%0A%20%20Gaussian%20Optimization%0AAuthor%3A%20Onat%20%C5%9Eahin%20and%20Mohammad%20Altillawi%20and%20George%20Eskandar%20and%20Carlos%20Carbone%20and%20Ziyuan%20Liu%0AAbstract%3A%20%20%20Recent%20advances%20in%20diffusion%20models%20have%20significantly%20improved%203D%0Ageneration%2C%20enabling%20the%20use%20of%20assets%20generated%20from%20an%20image%20for%20embodied%20AI%0Asimulations.%20However%2C%20the%20one-to-many%20nature%20of%20the%20image-to-3D%20problem%20limits%0Atheir%20use%20due%20to%20inconsistent%20content%20and%20quality%20across%20views.%20Previous%20models%0Aoptimize%20a%203D%20model%20by%20sampling%20views%20from%20a%20view-conditioned%20diffusion%20prior%2C%0Abut%20diffusion%20models%20cannot%20guarantee%20view%20consistency.%20Instead%2C%20we%20present%0AConsistentDreamer%2C%20where%20we%20first%20generate%20a%20set%20of%20fixed%20multi-view%20prior%0Aimages%20and%20sample%20random%20views%20between%20them%20with%20another%20diffusion%20model%0Athrough%20a%20score%20distillation%20sampling%20%28SDS%29%20loss.%20Thereby%2C%20we%20limit%20the%0Adiscrepancies%20between%20the%20views%20guided%20by%20the%20SDS%20loss%20and%20ensure%20a%20consistent%0Arough%20shape.%20In%20each%20iteration%2C%20we%20also%20use%20our%20generated%20multi-view%20prior%0Aimages%20for%20fine-detail%20reconstruction.%20To%20balance%20between%20the%20rough%20shape%20and%0Athe%20fine-detail%20optimizations%2C%20we%20introduce%20dynamic%20task-dependent%20weights%0Abased%20on%20homoscedastic%20uncertainty%2C%20updated%20automatically%20in%20each%20iteration.%0AAdditionally%2C%20we%20employ%20opacity%2C%20depth%20distortion%2C%20and%20normal%20alignment%20losses%0Ato%20refine%20the%20surface%20for%20mesh%20extraction.%20Our%20method%20ensures%20better%20view%0Aconsistency%20and%20visual%20quality%20compared%20to%20the%20state-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09278v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistentDreamer%253A%2520View-Consistent%2520Meshes%2520Through%2520Balanced%2520Multi-View%250A%2520%2520Gaussian%2520Optimization%26entry.906535625%3DOnat%2520%25C5%259Eahin%2520and%2520Mohammad%2520Altillawi%2520and%2520George%2520Eskandar%2520and%2520Carlos%2520Carbone%2520and%2520Ziyuan%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520diffusion%2520models%2520have%2520significantly%2520improved%25203D%250Ageneration%252C%2520enabling%2520the%2520use%2520of%2520assets%2520generated%2520from%2520an%2520image%2520for%2520embodied%2520AI%250Asimulations.%2520However%252C%2520the%2520one-to-many%2520nature%2520of%2520the%2520image-to-3D%2520problem%2520limits%250Atheir%2520use%2520due%2520to%2520inconsistent%2520content%2520and%2520quality%2520across%2520views.%2520Previous%2520models%250Aoptimize%2520a%25203D%2520model%2520by%2520sampling%2520views%2520from%2520a%2520view-conditioned%2520diffusion%2520prior%252C%250Abut%2520diffusion%2520models%2520cannot%2520guarantee%2520view%2520consistency.%2520Instead%252C%2520we%2520present%250AConsistentDreamer%252C%2520where%2520we%2520first%2520generate%2520a%2520set%2520of%2520fixed%2520multi-view%2520prior%250Aimages%2520and%2520sample%2520random%2520views%2520between%2520them%2520with%2520another%2520diffusion%2520model%250Athrough%2520a%2520score%2520distillation%2520sampling%2520%2528SDS%2529%2520loss.%2520Thereby%252C%2520we%2520limit%2520the%250Adiscrepancies%2520between%2520the%2520views%2520guided%2520by%2520the%2520SDS%2520loss%2520and%2520ensure%2520a%2520consistent%250Arough%2520shape.%2520In%2520each%2520iteration%252C%2520we%2520also%2520use%2520our%2520generated%2520multi-view%2520prior%250Aimages%2520for%2520fine-detail%2520reconstruction.%2520To%2520balance%2520between%2520the%2520rough%2520shape%2520and%250Athe%2520fine-detail%2520optimizations%252C%2520we%2520introduce%2520dynamic%2520task-dependent%2520weights%250Abased%2520on%2520homoscedastic%2520uncertainty%252C%2520updated%2520automatically%2520in%2520each%2520iteration.%250AAdditionally%252C%2520we%2520employ%2520opacity%252C%2520depth%2520distortion%252C%2520and%2520normal%2520alignment%2520losses%250Ato%2520refine%2520the%2520surface%2520for%2520mesh%2520extraction.%2520Our%2520method%2520ensures%2520better%2520view%250Aconsistency%2520and%2520visual%2520quality%2520compared%2520to%2520the%2520state-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09278v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConsistentDreamer%3A%20View-Consistent%20Meshes%20Through%20Balanced%20Multi-View%0A%20%20Gaussian%20Optimization&entry.906535625=Onat%20%C5%9Eahin%20and%20Mohammad%20Altillawi%20and%20George%20Eskandar%20and%20Carlos%20Carbone%20and%20Ziyuan%20Liu&entry.1292438233=%20%20Recent%20advances%20in%20diffusion%20models%20have%20significantly%20improved%203D%0Ageneration%2C%20enabling%20the%20use%20of%20assets%20generated%20from%20an%20image%20for%20embodied%20AI%0Asimulations.%20However%2C%20the%20one-to-many%20nature%20of%20the%20image-to-3D%20problem%20limits%0Atheir%20use%20due%20to%20inconsistent%20content%20and%20quality%20across%20views.%20Previous%20models%0Aoptimize%20a%203D%20model%20by%20sampling%20views%20from%20a%20view-conditioned%20diffusion%20prior%2C%0Abut%20diffusion%20models%20cannot%20guarantee%20view%20consistency.%20Instead%2C%20we%20present%0AConsistentDreamer%2C%20where%20we%20first%20generate%20a%20set%20of%20fixed%20multi-view%20prior%0Aimages%20and%20sample%20random%20views%20between%20them%20with%20another%20diffusion%20model%0Athrough%20a%20score%20distillation%20sampling%20%28SDS%29%20loss.%20Thereby%2C%20we%20limit%20the%0Adiscrepancies%20between%20the%20views%20guided%20by%20the%20SDS%20loss%20and%20ensure%20a%20consistent%0Arough%20shape.%20In%20each%20iteration%2C%20we%20also%20use%20our%20generated%20multi-view%20prior%0Aimages%20for%20fine-detail%20reconstruction.%20To%20balance%20between%20the%20rough%20shape%20and%0Athe%20fine-detail%20optimizations%2C%20we%20introduce%20dynamic%20task-dependent%20weights%0Abased%20on%20homoscedastic%20uncertainty%2C%20updated%20automatically%20in%20each%20iteration.%0AAdditionally%2C%20we%20employ%20opacity%2C%20depth%20distortion%2C%20and%20normal%20alignment%20losses%0Ato%20refine%20the%20surface%20for%20mesh%20extraction.%20Our%20method%20ensures%20better%20view%0Aconsistency%20and%20visual%20quality%20compared%20to%20the%20state-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09278v3&entry.124074799=Read"},
{"title": "Synthesizing Consistent Novel Views via 3D Epipolar Attention without\n  Re-Training", "author": "Botao Ye and Sifei Liu and Xueting Li and Marc Pollefeys and Ming-Hsuan Yang", "abstract": "  Large diffusion models demonstrate remarkable zero-shot capabilities in novel\nview synthesis from a single image. However, these models often face challenges\nin maintaining consistency across novel and reference views. A crucial factor\nleading to this issue is the limited utilization of contextual information from\nreference views. Specifically, when there is an overlap in the viewing frustum\nbetween two views, it is essential to ensure that the corresponding regions\nmaintain consistency in both geometry and appearance. This observation leads to\na simple yet effective approach, where we propose to use epipolar geometry to\nlocate and retrieve overlapping information from the input view. This\ninformation is then incorporated into the generation of target views,\neliminating the need for training or fine-tuning, as the process requires no\nlearnable parameters. Furthermore, to enhance the overall consistency of\ngenerated views, we extend the utilization of epipolar attention to a\nmulti-view setting, allowing retrieval of overlapping information from the\ninput view and other target views. Qualitative and quantitative experimental\nresults demonstrate the effectiveness of our method in significantly improving\nthe consistency of synthesized views without the need for any fine-tuning.\nMoreover, This enhancement also boosts the performance of downstream\napplications such as 3D reconstruction. The code is available at\nhttps://github.com/botaoye/ConsisSyn.\n", "link": "http://arxiv.org/abs/2502.18219v1", "date": "2025-02-25", "relevancy": 3.3035, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6717}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6717}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthesizing%20Consistent%20Novel%20Views%20via%203D%20Epipolar%20Attention%20without%0A%20%20Re-Training&body=Title%3A%20Synthesizing%20Consistent%20Novel%20Views%20via%203D%20Epipolar%20Attention%20without%0A%20%20Re-Training%0AAuthor%3A%20Botao%20Ye%20and%20Sifei%20Liu%20and%20Xueting%20Li%20and%20Marc%20Pollefeys%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20Large%20diffusion%20models%20demonstrate%20remarkable%20zero-shot%20capabilities%20in%20novel%0Aview%20synthesis%20from%20a%20single%20image.%20However%2C%20these%20models%20often%20face%20challenges%0Ain%20maintaining%20consistency%20across%20novel%20and%20reference%20views.%20A%20crucial%20factor%0Aleading%20to%20this%20issue%20is%20the%20limited%20utilization%20of%20contextual%20information%20from%0Areference%20views.%20Specifically%2C%20when%20there%20is%20an%20overlap%20in%20the%20viewing%20frustum%0Abetween%20two%20views%2C%20it%20is%20essential%20to%20ensure%20that%20the%20corresponding%20regions%0Amaintain%20consistency%20in%20both%20geometry%20and%20appearance.%20This%20observation%20leads%20to%0Aa%20simple%20yet%20effective%20approach%2C%20where%20we%20propose%20to%20use%20epipolar%20geometry%20to%0Alocate%20and%20retrieve%20overlapping%20information%20from%20the%20input%20view.%20This%0Ainformation%20is%20then%20incorporated%20into%20the%20generation%20of%20target%20views%2C%0Aeliminating%20the%20need%20for%20training%20or%20fine-tuning%2C%20as%20the%20process%20requires%20no%0Alearnable%20parameters.%20Furthermore%2C%20to%20enhance%20the%20overall%20consistency%20of%0Agenerated%20views%2C%20we%20extend%20the%20utilization%20of%20epipolar%20attention%20to%20a%0Amulti-view%20setting%2C%20allowing%20retrieval%20of%20overlapping%20information%20from%20the%0Ainput%20view%20and%20other%20target%20views.%20Qualitative%20and%20quantitative%20experimental%0Aresults%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%20significantly%20improving%0Athe%20consistency%20of%20synthesized%20views%20without%20the%20need%20for%20any%20fine-tuning.%0AMoreover%2C%20This%20enhancement%20also%20boosts%20the%20performance%20of%20downstream%0Aapplications%20such%20as%203D%20reconstruction.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/botaoye/ConsisSyn.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18219v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthesizing%2520Consistent%2520Novel%2520Views%2520via%25203D%2520Epipolar%2520Attention%2520without%250A%2520%2520Re-Training%26entry.906535625%3DBotao%2520Ye%2520and%2520Sifei%2520Liu%2520and%2520Xueting%2520Li%2520and%2520Marc%2520Pollefeys%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520Large%2520diffusion%2520models%2520demonstrate%2520remarkable%2520zero-shot%2520capabilities%2520in%2520novel%250Aview%2520synthesis%2520from%2520a%2520single%2520image.%2520However%252C%2520these%2520models%2520often%2520face%2520challenges%250Ain%2520maintaining%2520consistency%2520across%2520novel%2520and%2520reference%2520views.%2520A%2520crucial%2520factor%250Aleading%2520to%2520this%2520issue%2520is%2520the%2520limited%2520utilization%2520of%2520contextual%2520information%2520from%250Areference%2520views.%2520Specifically%252C%2520when%2520there%2520is%2520an%2520overlap%2520in%2520the%2520viewing%2520frustum%250Abetween%2520two%2520views%252C%2520it%2520is%2520essential%2520to%2520ensure%2520that%2520the%2520corresponding%2520regions%250Amaintain%2520consistency%2520in%2520both%2520geometry%2520and%2520appearance.%2520This%2520observation%2520leads%2520to%250Aa%2520simple%2520yet%2520effective%2520approach%252C%2520where%2520we%2520propose%2520to%2520use%2520epipolar%2520geometry%2520to%250Alocate%2520and%2520retrieve%2520overlapping%2520information%2520from%2520the%2520input%2520view.%2520This%250Ainformation%2520is%2520then%2520incorporated%2520into%2520the%2520generation%2520of%2520target%2520views%252C%250Aeliminating%2520the%2520need%2520for%2520training%2520or%2520fine-tuning%252C%2520as%2520the%2520process%2520requires%2520no%250Alearnable%2520parameters.%2520Furthermore%252C%2520to%2520enhance%2520the%2520overall%2520consistency%2520of%250Agenerated%2520views%252C%2520we%2520extend%2520the%2520utilization%2520of%2520epipolar%2520attention%2520to%2520a%250Amulti-view%2520setting%252C%2520allowing%2520retrieval%2520of%2520overlapping%2520information%2520from%2520the%250Ainput%2520view%2520and%2520other%2520target%2520views.%2520Qualitative%2520and%2520quantitative%2520experimental%250Aresults%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520significantly%2520improving%250Athe%2520consistency%2520of%2520synthesized%2520views%2520without%2520the%2520need%2520for%2520any%2520fine-tuning.%250AMoreover%252C%2520This%2520enhancement%2520also%2520boosts%2520the%2520performance%2520of%2520downstream%250Aapplications%2520such%2520as%25203D%2520reconstruction.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/botaoye/ConsisSyn.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18219v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthesizing%20Consistent%20Novel%20Views%20via%203D%20Epipolar%20Attention%20without%0A%20%20Re-Training&entry.906535625=Botao%20Ye%20and%20Sifei%20Liu%20and%20Xueting%20Li%20and%20Marc%20Pollefeys%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20Large%20diffusion%20models%20demonstrate%20remarkable%20zero-shot%20capabilities%20in%20novel%0Aview%20synthesis%20from%20a%20single%20image.%20However%2C%20these%20models%20often%20face%20challenges%0Ain%20maintaining%20consistency%20across%20novel%20and%20reference%20views.%20A%20crucial%20factor%0Aleading%20to%20this%20issue%20is%20the%20limited%20utilization%20of%20contextual%20information%20from%0Areference%20views.%20Specifically%2C%20when%20there%20is%20an%20overlap%20in%20the%20viewing%20frustum%0Abetween%20two%20views%2C%20it%20is%20essential%20to%20ensure%20that%20the%20corresponding%20regions%0Amaintain%20consistency%20in%20both%20geometry%20and%20appearance.%20This%20observation%20leads%20to%0Aa%20simple%20yet%20effective%20approach%2C%20where%20we%20propose%20to%20use%20epipolar%20geometry%20to%0Alocate%20and%20retrieve%20overlapping%20information%20from%20the%20input%20view.%20This%0Ainformation%20is%20then%20incorporated%20into%20the%20generation%20of%20target%20views%2C%0Aeliminating%20the%20need%20for%20training%20or%20fine-tuning%2C%20as%20the%20process%20requires%20no%0Alearnable%20parameters.%20Furthermore%2C%20to%20enhance%20the%20overall%20consistency%20of%0Agenerated%20views%2C%20we%20extend%20the%20utilization%20of%20epipolar%20attention%20to%20a%0Amulti-view%20setting%2C%20allowing%20retrieval%20of%20overlapping%20information%20from%20the%0Ainput%20view%20and%20other%20target%20views.%20Qualitative%20and%20quantitative%20experimental%0Aresults%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%20significantly%20improving%0Athe%20consistency%20of%20synthesized%20views%20without%20the%20need%20for%20any%20fine-tuning.%0AMoreover%2C%20This%20enhancement%20also%20boosts%20the%20performance%20of%20downstream%0Aapplications%20such%20as%203D%20reconstruction.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/botaoye/ConsisSyn.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18219v1&entry.124074799=Read"},
{"title": "Joint Reconstruction of Spatially-Coherent and Realistic Clothed Humans\n  and Objects from a Single Image", "author": "Ayushi Dutta and Marco Pesavento and Marco Volino and Adrian Hilton and Armin Mustafa", "abstract": "  Recent advances in human shape learning have focused on achieving accurate\nhuman reconstruction from single-view images. However, in the real world,\nhumans share space with other objects. Reconstructing images with humans and\nobjects is challenging due to the occlusions and lack of 3D spatial awareness,\nwhich leads to depth ambiguity in the reconstruction. Existing methods in\nmonocular human-object reconstruction fail to capture intricate details of\nclothed human bodies and object surfaces due to their template-based nature. In\nthis paper, we jointly reconstruct clothed humans and objects in a spatially\ncoherent manner from single-view images, while addressing human-object\nocclusions. A novel attention-based neural implicit model is proposed that\nleverages image pixel alignment to retrieve high-quality details, and\nincorporates semantic features extracted from the human-object pose to enable\n3D spatial awareness. A generative diffusion model is used to handle\nhuman-object occlusions. For training and evaluation, we introduce a synthetic\ndataset with rendered scenes of inter-occluded 3D human scans and diverse\nobjects. Extensive evaluation on both synthetic and real datasets demonstrates\nthe superior quality of proposed human-object reconstructions over competitive\nmethods.\n", "link": "http://arxiv.org/abs/2502.18150v1", "date": "2025-02-25", "relevancy": 3.1458, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6576}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.63}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Reconstruction%20of%20Spatially-Coherent%20and%20Realistic%20Clothed%20Humans%0A%20%20and%20Objects%20from%20a%20Single%20Image&body=Title%3A%20Joint%20Reconstruction%20of%20Spatially-Coherent%20and%20Realistic%20Clothed%20Humans%0A%20%20and%20Objects%20from%20a%20Single%20Image%0AAuthor%3A%20Ayushi%20Dutta%20and%20Marco%20Pesavento%20and%20Marco%20Volino%20and%20Adrian%20Hilton%20and%20Armin%20Mustafa%0AAbstract%3A%20%20%20Recent%20advances%20in%20human%20shape%20learning%20have%20focused%20on%20achieving%20accurate%0Ahuman%20reconstruction%20from%20single-view%20images.%20However%2C%20in%20the%20real%20world%2C%0Ahumans%20share%20space%20with%20other%20objects.%20Reconstructing%20images%20with%20humans%20and%0Aobjects%20is%20challenging%20due%20to%20the%20occlusions%20and%20lack%20of%203D%20spatial%20awareness%2C%0Awhich%20leads%20to%20depth%20ambiguity%20in%20the%20reconstruction.%20Existing%20methods%20in%0Amonocular%20human-object%20reconstruction%20fail%20to%20capture%20intricate%20details%20of%0Aclothed%20human%20bodies%20and%20object%20surfaces%20due%20to%20their%20template-based%20nature.%20In%0Athis%20paper%2C%20we%20jointly%20reconstruct%20clothed%20humans%20and%20objects%20in%20a%20spatially%0Acoherent%20manner%20from%20single-view%20images%2C%20while%20addressing%20human-object%0Aocclusions.%20A%20novel%20attention-based%20neural%20implicit%20model%20is%20proposed%20that%0Aleverages%20image%20pixel%20alignment%20to%20retrieve%20high-quality%20details%2C%20and%0Aincorporates%20semantic%20features%20extracted%20from%20the%20human-object%20pose%20to%20enable%0A3D%20spatial%20awareness.%20A%20generative%20diffusion%20model%20is%20used%20to%20handle%0Ahuman-object%20occlusions.%20For%20training%20and%20evaluation%2C%20we%20introduce%20a%20synthetic%0Adataset%20with%20rendered%20scenes%20of%20inter-occluded%203D%20human%20scans%20and%20diverse%0Aobjects.%20Extensive%20evaluation%20on%20both%20synthetic%20and%20real%20datasets%20demonstrates%0Athe%20superior%20quality%20of%20proposed%20human-object%20reconstructions%20over%20competitive%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Reconstruction%2520of%2520Spatially-Coherent%2520and%2520Realistic%2520Clothed%2520Humans%250A%2520%2520and%2520Objects%2520from%2520a%2520Single%2520Image%26entry.906535625%3DAyushi%2520Dutta%2520and%2520Marco%2520Pesavento%2520and%2520Marco%2520Volino%2520and%2520Adrian%2520Hilton%2520and%2520Armin%2520Mustafa%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520human%2520shape%2520learning%2520have%2520focused%2520on%2520achieving%2520accurate%250Ahuman%2520reconstruction%2520from%2520single-view%2520images.%2520However%252C%2520in%2520the%2520real%2520world%252C%250Ahumans%2520share%2520space%2520with%2520other%2520objects.%2520Reconstructing%2520images%2520with%2520humans%2520and%250Aobjects%2520is%2520challenging%2520due%2520to%2520the%2520occlusions%2520and%2520lack%2520of%25203D%2520spatial%2520awareness%252C%250Awhich%2520leads%2520to%2520depth%2520ambiguity%2520in%2520the%2520reconstruction.%2520Existing%2520methods%2520in%250Amonocular%2520human-object%2520reconstruction%2520fail%2520to%2520capture%2520intricate%2520details%2520of%250Aclothed%2520human%2520bodies%2520and%2520object%2520surfaces%2520due%2520to%2520their%2520template-based%2520nature.%2520In%250Athis%2520paper%252C%2520we%2520jointly%2520reconstruct%2520clothed%2520humans%2520and%2520objects%2520in%2520a%2520spatially%250Acoherent%2520manner%2520from%2520single-view%2520images%252C%2520while%2520addressing%2520human-object%250Aocclusions.%2520A%2520novel%2520attention-based%2520neural%2520implicit%2520model%2520is%2520proposed%2520that%250Aleverages%2520image%2520pixel%2520alignment%2520to%2520retrieve%2520high-quality%2520details%252C%2520and%250Aincorporates%2520semantic%2520features%2520extracted%2520from%2520the%2520human-object%2520pose%2520to%2520enable%250A3D%2520spatial%2520awareness.%2520A%2520generative%2520diffusion%2520model%2520is%2520used%2520to%2520handle%250Ahuman-object%2520occlusions.%2520For%2520training%2520and%2520evaluation%252C%2520we%2520introduce%2520a%2520synthetic%250Adataset%2520with%2520rendered%2520scenes%2520of%2520inter-occluded%25203D%2520human%2520scans%2520and%2520diverse%250Aobjects.%2520Extensive%2520evaluation%2520on%2520both%2520synthetic%2520and%2520real%2520datasets%2520demonstrates%250Athe%2520superior%2520quality%2520of%2520proposed%2520human-object%2520reconstructions%2520over%2520competitive%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Reconstruction%20of%20Spatially-Coherent%20and%20Realistic%20Clothed%20Humans%0A%20%20and%20Objects%20from%20a%20Single%20Image&entry.906535625=Ayushi%20Dutta%20and%20Marco%20Pesavento%20and%20Marco%20Volino%20and%20Adrian%20Hilton%20and%20Armin%20Mustafa&entry.1292438233=%20%20Recent%20advances%20in%20human%20shape%20learning%20have%20focused%20on%20achieving%20accurate%0Ahuman%20reconstruction%20from%20single-view%20images.%20However%2C%20in%20the%20real%20world%2C%0Ahumans%20share%20space%20with%20other%20objects.%20Reconstructing%20images%20with%20humans%20and%0Aobjects%20is%20challenging%20due%20to%20the%20occlusions%20and%20lack%20of%203D%20spatial%20awareness%2C%0Awhich%20leads%20to%20depth%20ambiguity%20in%20the%20reconstruction.%20Existing%20methods%20in%0Amonocular%20human-object%20reconstruction%20fail%20to%20capture%20intricate%20details%20of%0Aclothed%20human%20bodies%20and%20object%20surfaces%20due%20to%20their%20template-based%20nature.%20In%0Athis%20paper%2C%20we%20jointly%20reconstruct%20clothed%20humans%20and%20objects%20in%20a%20spatially%0Acoherent%20manner%20from%20single-view%20images%2C%20while%20addressing%20human-object%0Aocclusions.%20A%20novel%20attention-based%20neural%20implicit%20model%20is%20proposed%20that%0Aleverages%20image%20pixel%20alignment%20to%20retrieve%20high-quality%20details%2C%20and%0Aincorporates%20semantic%20features%20extracted%20from%20the%20human-object%20pose%20to%20enable%0A3D%20spatial%20awareness.%20A%20generative%20diffusion%20model%20is%20used%20to%20handle%0Ahuman-object%20occlusions.%20For%20training%20and%20evaluation%2C%20we%20introduce%20a%20synthetic%0Adataset%20with%20rendered%20scenes%20of%20inter-occluded%203D%20human%20scans%20and%20diverse%0Aobjects.%20Extensive%20evaluation%20on%20both%20synthetic%20and%20real%20datasets%20demonstrates%0Athe%20superior%20quality%20of%20proposed%20human-object%20reconstructions%20over%20competitive%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18150v1&entry.124074799=Read"},
{"title": "GHOST 2.0: generative high-fidelity one shot transfer of heads", "author": "Alexander Groshev and Anastasiia Iashchenko and Pavel Paramonov and Denis Dimitrov and Andrey Kuznetsov", "abstract": "  While the task of face swapping has recently gained attention in the research\ncommunity, a related problem of head swapping remains largely unexplored. In\naddition to skin color transfer, head swap poses extra challenges, such as the\nneed to preserve structural information of the whole head during synthesis and\ninpaint gaps between swapped head and background. In this paper, we address\nthese concerns with GHOST 2.0, which consists of two problem-specific modules.\nFirst, we introduce enhanced Aligner model for head reenactment, which\npreserves identity information at multiple scales and is robust to extreme pose\nvariations. Secondly, we use a Blender module that seamlessly integrates the\nreenacted head into the target background by transferring skin color and\ninpainting mismatched regions. Both modules outperform the baselines on the\ncorresponding tasks, allowing to achieve state of the art results in head\nswapping. We also tackle complex cases, such as large difference in hair styles\nof source and target.\n", "link": "http://arxiv.org/abs/2502.18417v1", "date": "2025-02-25", "relevancy": 2.9689, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.603}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.603}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GHOST%202.0%3A%20generative%20high-fidelity%20one%20shot%20transfer%20of%20heads&body=Title%3A%20GHOST%202.0%3A%20generative%20high-fidelity%20one%20shot%20transfer%20of%20heads%0AAuthor%3A%20Alexander%20Groshev%20and%20Anastasiia%20Iashchenko%20and%20Pavel%20Paramonov%20and%20Denis%20Dimitrov%20and%20Andrey%20Kuznetsov%0AAbstract%3A%20%20%20While%20the%20task%20of%20face%20swapping%20has%20recently%20gained%20attention%20in%20the%20research%0Acommunity%2C%20a%20related%20problem%20of%20head%20swapping%20remains%20largely%20unexplored.%20In%0Aaddition%20to%20skin%20color%20transfer%2C%20head%20swap%20poses%20extra%20challenges%2C%20such%20as%20the%0Aneed%20to%20preserve%20structural%20information%20of%20the%20whole%20head%20during%20synthesis%20and%0Ainpaint%20gaps%20between%20swapped%20head%20and%20background.%20In%20this%20paper%2C%20we%20address%0Athese%20concerns%20with%20GHOST%202.0%2C%20which%20consists%20of%20two%20problem-specific%20modules.%0AFirst%2C%20we%20introduce%20enhanced%20Aligner%20model%20for%20head%20reenactment%2C%20which%0Apreserves%20identity%20information%20at%20multiple%20scales%20and%20is%20robust%20to%20extreme%20pose%0Avariations.%20Secondly%2C%20we%20use%20a%20Blender%20module%20that%20seamlessly%20integrates%20the%0Areenacted%20head%20into%20the%20target%20background%20by%20transferring%20skin%20color%20and%0Ainpainting%20mismatched%20regions.%20Both%20modules%20outperform%20the%20baselines%20on%20the%0Acorresponding%20tasks%2C%20allowing%20to%20achieve%20state%20of%20the%20art%20results%20in%20head%0Aswapping.%20We%20also%20tackle%20complex%20cases%2C%20such%20as%20large%20difference%20in%20hair%20styles%0Aof%20source%20and%20target.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGHOST%25202.0%253A%2520generative%2520high-fidelity%2520one%2520shot%2520transfer%2520of%2520heads%26entry.906535625%3DAlexander%2520Groshev%2520and%2520Anastasiia%2520Iashchenko%2520and%2520Pavel%2520Paramonov%2520and%2520Denis%2520Dimitrov%2520and%2520Andrey%2520Kuznetsov%26entry.1292438233%3D%2520%2520While%2520the%2520task%2520of%2520face%2520swapping%2520has%2520recently%2520gained%2520attention%2520in%2520the%2520research%250Acommunity%252C%2520a%2520related%2520problem%2520of%2520head%2520swapping%2520remains%2520largely%2520unexplored.%2520In%250Aaddition%2520to%2520skin%2520color%2520transfer%252C%2520head%2520swap%2520poses%2520extra%2520challenges%252C%2520such%2520as%2520the%250Aneed%2520to%2520preserve%2520structural%2520information%2520of%2520the%2520whole%2520head%2520during%2520synthesis%2520and%250Ainpaint%2520gaps%2520between%2520swapped%2520head%2520and%2520background.%2520In%2520this%2520paper%252C%2520we%2520address%250Athese%2520concerns%2520with%2520GHOST%25202.0%252C%2520which%2520consists%2520of%2520two%2520problem-specific%2520modules.%250AFirst%252C%2520we%2520introduce%2520enhanced%2520Aligner%2520model%2520for%2520head%2520reenactment%252C%2520which%250Apreserves%2520identity%2520information%2520at%2520multiple%2520scales%2520and%2520is%2520robust%2520to%2520extreme%2520pose%250Avariations.%2520Secondly%252C%2520we%2520use%2520a%2520Blender%2520module%2520that%2520seamlessly%2520integrates%2520the%250Areenacted%2520head%2520into%2520the%2520target%2520background%2520by%2520transferring%2520skin%2520color%2520and%250Ainpainting%2520mismatched%2520regions.%2520Both%2520modules%2520outperform%2520the%2520baselines%2520on%2520the%250Acorresponding%2520tasks%252C%2520allowing%2520to%2520achieve%2520state%2520of%2520the%2520art%2520results%2520in%2520head%250Aswapping.%2520We%2520also%2520tackle%2520complex%2520cases%252C%2520such%2520as%2520large%2520difference%2520in%2520hair%2520styles%250Aof%2520source%2520and%2520target.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GHOST%202.0%3A%20generative%20high-fidelity%20one%20shot%20transfer%20of%20heads&entry.906535625=Alexander%20Groshev%20and%20Anastasiia%20Iashchenko%20and%20Pavel%20Paramonov%20and%20Denis%20Dimitrov%20and%20Andrey%20Kuznetsov&entry.1292438233=%20%20While%20the%20task%20of%20face%20swapping%20has%20recently%20gained%20attention%20in%20the%20research%0Acommunity%2C%20a%20related%20problem%20of%20head%20swapping%20remains%20largely%20unexplored.%20In%0Aaddition%20to%20skin%20color%20transfer%2C%20head%20swap%20poses%20extra%20challenges%2C%20such%20as%20the%0Aneed%20to%20preserve%20structural%20information%20of%20the%20whole%20head%20during%20synthesis%20and%0Ainpaint%20gaps%20between%20swapped%20head%20and%20background.%20In%20this%20paper%2C%20we%20address%0Athese%20concerns%20with%20GHOST%202.0%2C%20which%20consists%20of%20two%20problem-specific%20modules.%0AFirst%2C%20we%20introduce%20enhanced%20Aligner%20model%20for%20head%20reenactment%2C%20which%0Apreserves%20identity%20information%20at%20multiple%20scales%20and%20is%20robust%20to%20extreme%20pose%0Avariations.%20Secondly%2C%20we%20use%20a%20Blender%20module%20that%20seamlessly%20integrates%20the%0Areenacted%20head%20into%20the%20target%20background%20by%20transferring%20skin%20color%20and%0Ainpainting%20mismatched%20regions.%20Both%20modules%20outperform%20the%20baselines%20on%20the%0Acorresponding%20tasks%2C%20allowing%20to%20achieve%20state%20of%20the%20art%20results%20in%20head%0Aswapping.%20We%20also%20tackle%20complex%20cases%2C%20such%20as%20large%20difference%20in%20hair%20styles%0Aof%20source%20and%20target.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18417v1&entry.124074799=Read"},
{"title": "Multimodality Helps Few-shot 3D Point Cloud Semantic Segmentation", "author": "Zhaochong An and Guolei Sun and Yun Liu and Runjia Li and Min Wu and Ming-Ming Cheng and Ender Konukoglu and Serge Belongie", "abstract": "  Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models to\nsegment novel categories with minimal annotated support samples. While existing\nFS-PCS methods have shown promise, they primarily focus on unimodal point cloud\ninputs, overlooking the potential benefits of leveraging multimodal\ninformation. In this paper, we address this gap by introducing a multimodal\nFS-PCS setup, utilizing textual labels and the potentially available 2D image\nmodality. Under this easy-to-achieve setup, we present the MultiModal Few-Shot\nSegNet (MM-FSS), a model effectively harnessing complementary information from\nmultiple modalities. MM-FSS employs a shared backbone with two heads to extract\nintermodal and unimodal visual features, and a pretrained text encoder to\ngenerate text embeddings. To fully exploit the multimodal information, we\npropose a Multimodal Correlation Fusion (MCF) module to generate multimodal\ncorrelations, and a Multimodal Semantic Fusion (MSF) module to refine the\ncorrelations using text-aware semantic guidance. Additionally, we propose a\nsimple yet effective Test-time Adaptive Cross-modal Calibration (TACC)\ntechnique to mitigate training bias, further improving generalization.\nExperimental results on S3DIS and ScanNet datasets demonstrate significant\nperformance improvements achieved by our method. The efficacy of our approach\nindicates the benefits of leveraging commonly-ignored free modalities for\nFS-PCS, providing valuable insights for future research. The code is available\nat https://github.com/ZhaochongAn/Multimodality-3D-Few-Shot\n", "link": "http://arxiv.org/abs/2410.22489v3", "date": "2025-02-25", "relevancy": 2.9405, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6178}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.576}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodality%20Helps%20Few-shot%203D%20Point%20Cloud%20Semantic%20Segmentation&body=Title%3A%20Multimodality%20Helps%20Few-shot%203D%20Point%20Cloud%20Semantic%20Segmentation%0AAuthor%3A%20Zhaochong%20An%20and%20Guolei%20Sun%20and%20Yun%20Liu%20and%20Runjia%20Li%20and%20Min%20Wu%20and%20Ming-Ming%20Cheng%20and%20Ender%20Konukoglu%20and%20Serge%20Belongie%0AAbstract%3A%20%20%20Few-shot%203D%20point%20cloud%20segmentation%20%28FS-PCS%29%20aims%20at%20generalizing%20models%20to%0Asegment%20novel%20categories%20with%20minimal%20annotated%20support%20samples.%20While%20existing%0AFS-PCS%20methods%20have%20shown%20promise%2C%20they%20primarily%20focus%20on%20unimodal%20point%20cloud%0Ainputs%2C%20overlooking%20the%20potential%20benefits%20of%20leveraging%20multimodal%0Ainformation.%20In%20this%20paper%2C%20we%20address%20this%20gap%20by%20introducing%20a%20multimodal%0AFS-PCS%20setup%2C%20utilizing%20textual%20labels%20and%20the%20potentially%20available%202D%20image%0Amodality.%20Under%20this%20easy-to-achieve%20setup%2C%20we%20present%20the%20MultiModal%20Few-Shot%0ASegNet%20%28MM-FSS%29%2C%20a%20model%20effectively%20harnessing%20complementary%20information%20from%0Amultiple%20modalities.%20MM-FSS%20employs%20a%20shared%20backbone%20with%20two%20heads%20to%20extract%0Aintermodal%20and%20unimodal%20visual%20features%2C%20and%20a%20pretrained%20text%20encoder%20to%0Agenerate%20text%20embeddings.%20To%20fully%20exploit%20the%20multimodal%20information%2C%20we%0Apropose%20a%20Multimodal%20Correlation%20Fusion%20%28MCF%29%20module%20to%20generate%20multimodal%0Acorrelations%2C%20and%20a%20Multimodal%20Semantic%20Fusion%20%28MSF%29%20module%20to%20refine%20the%0Acorrelations%20using%20text-aware%20semantic%20guidance.%20Additionally%2C%20we%20propose%20a%0Asimple%20yet%20effective%20Test-time%20Adaptive%20Cross-modal%20Calibration%20%28TACC%29%0Atechnique%20to%20mitigate%20training%20bias%2C%20further%20improving%20generalization.%0AExperimental%20results%20on%20S3DIS%20and%20ScanNet%20datasets%20demonstrate%20significant%0Aperformance%20improvements%20achieved%20by%20our%20method.%20The%20efficacy%20of%20our%20approach%0Aindicates%20the%20benefits%20of%20leveraging%20commonly-ignored%20free%20modalities%20for%0AFS-PCS%2C%20providing%20valuable%20insights%20for%20future%20research.%20The%20code%20is%20available%0Aat%20https%3A//github.com/ZhaochongAn/Multimodality-3D-Few-Shot%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22489v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodality%2520Helps%2520Few-shot%25203D%2520Point%2520Cloud%2520Semantic%2520Segmentation%26entry.906535625%3DZhaochong%2520An%2520and%2520Guolei%2520Sun%2520and%2520Yun%2520Liu%2520and%2520Runjia%2520Li%2520and%2520Min%2520Wu%2520and%2520Ming-Ming%2520Cheng%2520and%2520Ender%2520Konukoglu%2520and%2520Serge%2520Belongie%26entry.1292438233%3D%2520%2520Few-shot%25203D%2520point%2520cloud%2520segmentation%2520%2528FS-PCS%2529%2520aims%2520at%2520generalizing%2520models%2520to%250Asegment%2520novel%2520categories%2520with%2520minimal%2520annotated%2520support%2520samples.%2520While%2520existing%250AFS-PCS%2520methods%2520have%2520shown%2520promise%252C%2520they%2520primarily%2520focus%2520on%2520unimodal%2520point%2520cloud%250Ainputs%252C%2520overlooking%2520the%2520potential%2520benefits%2520of%2520leveraging%2520multimodal%250Ainformation.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520gap%2520by%2520introducing%2520a%2520multimodal%250AFS-PCS%2520setup%252C%2520utilizing%2520textual%2520labels%2520and%2520the%2520potentially%2520available%25202D%2520image%250Amodality.%2520Under%2520this%2520easy-to-achieve%2520setup%252C%2520we%2520present%2520the%2520MultiModal%2520Few-Shot%250ASegNet%2520%2528MM-FSS%2529%252C%2520a%2520model%2520effectively%2520harnessing%2520complementary%2520information%2520from%250Amultiple%2520modalities.%2520MM-FSS%2520employs%2520a%2520shared%2520backbone%2520with%2520two%2520heads%2520to%2520extract%250Aintermodal%2520and%2520unimodal%2520visual%2520features%252C%2520and%2520a%2520pretrained%2520text%2520encoder%2520to%250Agenerate%2520text%2520embeddings.%2520To%2520fully%2520exploit%2520the%2520multimodal%2520information%252C%2520we%250Apropose%2520a%2520Multimodal%2520Correlation%2520Fusion%2520%2528MCF%2529%2520module%2520to%2520generate%2520multimodal%250Acorrelations%252C%2520and%2520a%2520Multimodal%2520Semantic%2520Fusion%2520%2528MSF%2529%2520module%2520to%2520refine%2520the%250Acorrelations%2520using%2520text-aware%2520semantic%2520guidance.%2520Additionally%252C%2520we%2520propose%2520a%250Asimple%2520yet%2520effective%2520Test-time%2520Adaptive%2520Cross-modal%2520Calibration%2520%2528TACC%2529%250Atechnique%2520to%2520mitigate%2520training%2520bias%252C%2520further%2520improving%2520generalization.%250AExperimental%2520results%2520on%2520S3DIS%2520and%2520ScanNet%2520datasets%2520demonstrate%2520significant%250Aperformance%2520improvements%2520achieved%2520by%2520our%2520method.%2520The%2520efficacy%2520of%2520our%2520approach%250Aindicates%2520the%2520benefits%2520of%2520leveraging%2520commonly-ignored%2520free%2520modalities%2520for%250AFS-PCS%252C%2520providing%2520valuable%2520insights%2520for%2520future%2520research.%2520The%2520code%2520is%2520available%250Aat%2520https%253A//github.com/ZhaochongAn/Multimodality-3D-Few-Shot%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22489v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodality%20Helps%20Few-shot%203D%20Point%20Cloud%20Semantic%20Segmentation&entry.906535625=Zhaochong%20An%20and%20Guolei%20Sun%20and%20Yun%20Liu%20and%20Runjia%20Li%20and%20Min%20Wu%20and%20Ming-Ming%20Cheng%20and%20Ender%20Konukoglu%20and%20Serge%20Belongie&entry.1292438233=%20%20Few-shot%203D%20point%20cloud%20segmentation%20%28FS-PCS%29%20aims%20at%20generalizing%20models%20to%0Asegment%20novel%20categories%20with%20minimal%20annotated%20support%20samples.%20While%20existing%0AFS-PCS%20methods%20have%20shown%20promise%2C%20they%20primarily%20focus%20on%20unimodal%20point%20cloud%0Ainputs%2C%20overlooking%20the%20potential%20benefits%20of%20leveraging%20multimodal%0Ainformation.%20In%20this%20paper%2C%20we%20address%20this%20gap%20by%20introducing%20a%20multimodal%0AFS-PCS%20setup%2C%20utilizing%20textual%20labels%20and%20the%20potentially%20available%202D%20image%0Amodality.%20Under%20this%20easy-to-achieve%20setup%2C%20we%20present%20the%20MultiModal%20Few-Shot%0ASegNet%20%28MM-FSS%29%2C%20a%20model%20effectively%20harnessing%20complementary%20information%20from%0Amultiple%20modalities.%20MM-FSS%20employs%20a%20shared%20backbone%20with%20two%20heads%20to%20extract%0Aintermodal%20and%20unimodal%20visual%20features%2C%20and%20a%20pretrained%20text%20encoder%20to%0Agenerate%20text%20embeddings.%20To%20fully%20exploit%20the%20multimodal%20information%2C%20we%0Apropose%20a%20Multimodal%20Correlation%20Fusion%20%28MCF%29%20module%20to%20generate%20multimodal%0Acorrelations%2C%20and%20a%20Multimodal%20Semantic%20Fusion%20%28MSF%29%20module%20to%20refine%20the%0Acorrelations%20using%20text-aware%20semantic%20guidance.%20Additionally%2C%20we%20propose%20a%0Asimple%20yet%20effective%20Test-time%20Adaptive%20Cross-modal%20Calibration%20%28TACC%29%0Atechnique%20to%20mitigate%20training%20bias%2C%20further%20improving%20generalization.%0AExperimental%20results%20on%20S3DIS%20and%20ScanNet%20datasets%20demonstrate%20significant%0Aperformance%20improvements%20achieved%20by%20our%20method.%20The%20efficacy%20of%20our%20approach%0Aindicates%20the%20benefits%20of%20leveraging%20commonly-ignored%20free%20modalities%20for%0AFS-PCS%2C%20providing%20valuable%20insights%20for%20future%20research.%20The%20code%20is%20available%0Aat%20https%3A//github.com/ZhaochongAn/Multimodality-3D-Few-Shot%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22489v3&entry.124074799=Read"},
{"title": "GCDance: Genre-Controlled 3D Full Body Dance Generation Driven By Music", "author": "Xinran Liu and Xu Dong and Diptesh Kanojia and Wenwu Wang and Zhenhua Feng", "abstract": "  Generating high-quality full-body dance sequences from music is a challenging\ntask as it requires strict adherence to genre-specific choreography. Moreover,\nthe generated sequences must be both physically realistic and precisely\nsynchronized with the beats and rhythm of the music. To overcome these\nchallenges, we propose GCDance, a classifier-free diffusion framework for\ngenerating genre-specific dance motions conditioned on both music and textual\nprompts. Specifically, our approach extracts music features by combining\nhigh-level pre-trained music foundation model features with hand-crafted\nfeatures for multi-granularity feature fusion. To achieve genre\ncontrollability, we leverage CLIP to efficiently embed genre-based textual\nprompt representations at each time step within our dance generation pipeline.\nOur GCDance framework can generate diverse dance styles from the same piece of\nmusic while ensuring coherence with the rhythm and melody of the music.\nExtensive experimental results obtained on the FineDance dataset demonstrate\nthat GCDance significantly outperforms the existing state-of-the-art\napproaches, which also achieve competitive results on the AIST++ dataset. Our\nablation and inference time analysis demonstrate that GCDance provides an\neffective solution for high-quality music-driven dance generation.\n", "link": "http://arxiv.org/abs/2502.18309v1", "date": "2025-02-25", "relevancy": 2.9155, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6148}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5824}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GCDance%3A%20Genre-Controlled%203D%20Full%20Body%20Dance%20Generation%20Driven%20By%20Music&body=Title%3A%20GCDance%3A%20Genre-Controlled%203D%20Full%20Body%20Dance%20Generation%20Driven%20By%20Music%0AAuthor%3A%20Xinran%20Liu%20and%20Xu%20Dong%20and%20Diptesh%20Kanojia%20and%20Wenwu%20Wang%20and%20Zhenhua%20Feng%0AAbstract%3A%20%20%20Generating%20high-quality%20full-body%20dance%20sequences%20from%20music%20is%20a%20challenging%0Atask%20as%20it%20requires%20strict%20adherence%20to%20genre-specific%20choreography.%20Moreover%2C%0Athe%20generated%20sequences%20must%20be%20both%20physically%20realistic%20and%20precisely%0Asynchronized%20with%20the%20beats%20and%20rhythm%20of%20the%20music.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20GCDance%2C%20a%20classifier-free%20diffusion%20framework%20for%0Agenerating%20genre-specific%20dance%20motions%20conditioned%20on%20both%20music%20and%20textual%0Aprompts.%20Specifically%2C%20our%20approach%20extracts%20music%20features%20by%20combining%0Ahigh-level%20pre-trained%20music%20foundation%20model%20features%20with%20hand-crafted%0Afeatures%20for%20multi-granularity%20feature%20fusion.%20To%20achieve%20genre%0Acontrollability%2C%20we%20leverage%20CLIP%20to%20efficiently%20embed%20genre-based%20textual%0Aprompt%20representations%20at%20each%20time%20step%20within%20our%20dance%20generation%20pipeline.%0AOur%20GCDance%20framework%20can%20generate%20diverse%20dance%20styles%20from%20the%20same%20piece%20of%0Amusic%20while%20ensuring%20coherence%20with%20the%20rhythm%20and%20melody%20of%20the%20music.%0AExtensive%20experimental%20results%20obtained%20on%20the%20FineDance%20dataset%20demonstrate%0Athat%20GCDance%20significantly%20outperforms%20the%20existing%20state-of-the-art%0Aapproaches%2C%20which%20also%20achieve%20competitive%20results%20on%20the%20AIST%2B%2B%20dataset.%20Our%0Aablation%20and%20inference%20time%20analysis%20demonstrate%20that%20GCDance%20provides%20an%0Aeffective%20solution%20for%20high-quality%20music-driven%20dance%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGCDance%253A%2520Genre-Controlled%25203D%2520Full%2520Body%2520Dance%2520Generation%2520Driven%2520By%2520Music%26entry.906535625%3DXinran%2520Liu%2520and%2520Xu%2520Dong%2520and%2520Diptesh%2520Kanojia%2520and%2520Wenwu%2520Wang%2520and%2520Zhenhua%2520Feng%26entry.1292438233%3D%2520%2520Generating%2520high-quality%2520full-body%2520dance%2520sequences%2520from%2520music%2520is%2520a%2520challenging%250Atask%2520as%2520it%2520requires%2520strict%2520adherence%2520to%2520genre-specific%2520choreography.%2520Moreover%252C%250Athe%2520generated%2520sequences%2520must%2520be%2520both%2520physically%2520realistic%2520and%2520precisely%250Asynchronized%2520with%2520the%2520beats%2520and%2520rhythm%2520of%2520the%2520music.%2520To%2520overcome%2520these%250Achallenges%252C%2520we%2520propose%2520GCDance%252C%2520a%2520classifier-free%2520diffusion%2520framework%2520for%250Agenerating%2520genre-specific%2520dance%2520motions%2520conditioned%2520on%2520both%2520music%2520and%2520textual%250Aprompts.%2520Specifically%252C%2520our%2520approach%2520extracts%2520music%2520features%2520by%2520combining%250Ahigh-level%2520pre-trained%2520music%2520foundation%2520model%2520features%2520with%2520hand-crafted%250Afeatures%2520for%2520multi-granularity%2520feature%2520fusion.%2520To%2520achieve%2520genre%250Acontrollability%252C%2520we%2520leverage%2520CLIP%2520to%2520efficiently%2520embed%2520genre-based%2520textual%250Aprompt%2520representations%2520at%2520each%2520time%2520step%2520within%2520our%2520dance%2520generation%2520pipeline.%250AOur%2520GCDance%2520framework%2520can%2520generate%2520diverse%2520dance%2520styles%2520from%2520the%2520same%2520piece%2520of%250Amusic%2520while%2520ensuring%2520coherence%2520with%2520the%2520rhythm%2520and%2520melody%2520of%2520the%2520music.%250AExtensive%2520experimental%2520results%2520obtained%2520on%2520the%2520FineDance%2520dataset%2520demonstrate%250Athat%2520GCDance%2520significantly%2520outperforms%2520the%2520existing%2520state-of-the-art%250Aapproaches%252C%2520which%2520also%2520achieve%2520competitive%2520results%2520on%2520the%2520AIST%252B%252B%2520dataset.%2520Our%250Aablation%2520and%2520inference%2520time%2520analysis%2520demonstrate%2520that%2520GCDance%2520provides%2520an%250Aeffective%2520solution%2520for%2520high-quality%2520music-driven%2520dance%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GCDance%3A%20Genre-Controlled%203D%20Full%20Body%20Dance%20Generation%20Driven%20By%20Music&entry.906535625=Xinran%20Liu%20and%20Xu%20Dong%20and%20Diptesh%20Kanojia%20and%20Wenwu%20Wang%20and%20Zhenhua%20Feng&entry.1292438233=%20%20Generating%20high-quality%20full-body%20dance%20sequences%20from%20music%20is%20a%20challenging%0Atask%20as%20it%20requires%20strict%20adherence%20to%20genre-specific%20choreography.%20Moreover%2C%0Athe%20generated%20sequences%20must%20be%20both%20physically%20realistic%20and%20precisely%0Asynchronized%20with%20the%20beats%20and%20rhythm%20of%20the%20music.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20GCDance%2C%20a%20classifier-free%20diffusion%20framework%20for%0Agenerating%20genre-specific%20dance%20motions%20conditioned%20on%20both%20music%20and%20textual%0Aprompts.%20Specifically%2C%20our%20approach%20extracts%20music%20features%20by%20combining%0Ahigh-level%20pre-trained%20music%20foundation%20model%20features%20with%20hand-crafted%0Afeatures%20for%20multi-granularity%20feature%20fusion.%20To%20achieve%20genre%0Acontrollability%2C%20we%20leverage%20CLIP%20to%20efficiently%20embed%20genre-based%20textual%0Aprompt%20representations%20at%20each%20time%20step%20within%20our%20dance%20generation%20pipeline.%0AOur%20GCDance%20framework%20can%20generate%20diverse%20dance%20styles%20from%20the%20same%20piece%20of%0Amusic%20while%20ensuring%20coherence%20with%20the%20rhythm%20and%20melody%20of%20the%20music.%0AExtensive%20experimental%20results%20obtained%20on%20the%20FineDance%20dataset%20demonstrate%0Athat%20GCDance%20significantly%20outperforms%20the%20existing%20state-of-the-art%0Aapproaches%2C%20which%20also%20achieve%20competitive%20results%20on%20the%20AIST%2B%2B%20dataset.%20Our%0Aablation%20and%20inference%20time%20analysis%20demonstrate%20that%20GCDance%20provides%20an%0Aeffective%20solution%20for%20high-quality%20music-driven%20dance%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18309v1&entry.124074799=Read"},
{"title": "Escaping The Big Data Paradigm in Self-Supervised Representation\n  Learning", "author": "Carlos V\u00e9lez Garc\u00eda and Miguel Cazorla and Jorge Pomares", "abstract": "  The reliance on large-scale datasets and extensive computational resources\nhas become a major barrier to advancing representation learning in vision,\nespecially in data-scarce domains. In this paper, we address the critical\nquestion: Can we escape the big data paradigm in self-supervised representation\nlearning from images? We introduce SCOTT (Sparse Convolutional Tokenizer for\nTransformers), a shallow tokenization architecture that is compatible with\nMasked Image Modeling (MIM) tasks. SCOTT injects convolutional inductive biases\ninto Vision Transformers (ViTs), enhancing their efficacy in small-scale data\nregimes. Alongside, we propose to train on a Joint-Embedding Predictive\nArchitecture within a MIM framework (MIM-JEPA), operating in latent\nrepresentation space to capture more semantic features. Our approach enables\nViTs to be trained from scratch on datasets orders of magnitude smaller than\ntraditionally required --without relying on massive external datasets for\npretraining. We validate our method on three small-size, standard-resoultion,\nfine-grained datasets: Oxford Flowers-102, Oxford IIIT Pets-37, and\nImageNet-100. Despite the challenges of limited data and high intra-class\nsimilarity, frozen SCOTT models pretrained with MIM-JEPA significantly\noutperform fully supervised methods and achieve competitive results with SOTA\napproaches that rely on large-scale pretraining, complex image augmentations\nand bigger model sizes. By demonstrating that robust off-the-shelf\nrepresentations can be learned with limited data, compute, and model sizes, our\nwork paves the way for computer applications in resource constrained\nenvironments such as medical imaging or robotics. Our findings challenge the\nprevailing notion that vast amounts of data are indispensable for effective\nrepresentation learning in vision, offering a new pathway toward more\naccessible and inclusive advancements in the field.\n", "link": "http://arxiv.org/abs/2502.18056v1", "date": "2025-02-25", "relevancy": 2.9118, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6066}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5759}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Escaping%20The%20Big%20Data%20Paradigm%20in%20Self-Supervised%20Representation%0A%20%20Learning&body=Title%3A%20Escaping%20The%20Big%20Data%20Paradigm%20in%20Self-Supervised%20Representation%0A%20%20Learning%0AAuthor%3A%20Carlos%20V%C3%A9lez%20Garc%C3%ADa%20and%20Miguel%20Cazorla%20and%20Jorge%20Pomares%0AAbstract%3A%20%20%20The%20reliance%20on%20large-scale%20datasets%20and%20extensive%20computational%20resources%0Ahas%20become%20a%20major%20barrier%20to%20advancing%20representation%20learning%20in%20vision%2C%0Aespecially%20in%20data-scarce%20domains.%20In%20this%20paper%2C%20we%20address%20the%20critical%0Aquestion%3A%20Can%20we%20escape%20the%20big%20data%20paradigm%20in%20self-supervised%20representation%0Alearning%20from%20images%3F%20We%20introduce%20SCOTT%20%28Sparse%20Convolutional%20Tokenizer%20for%0ATransformers%29%2C%20a%20shallow%20tokenization%20architecture%20that%20is%20compatible%20with%0AMasked%20Image%20Modeling%20%28MIM%29%20tasks.%20SCOTT%20injects%20convolutional%20inductive%20biases%0Ainto%20Vision%20Transformers%20%28ViTs%29%2C%20enhancing%20their%20efficacy%20in%20small-scale%20data%0Aregimes.%20Alongside%2C%20we%20propose%20to%20train%20on%20a%20Joint-Embedding%20Predictive%0AArchitecture%20within%20a%20MIM%20framework%20%28MIM-JEPA%29%2C%20operating%20in%20latent%0Arepresentation%20space%20to%20capture%20more%20semantic%20features.%20Our%20approach%20enables%0AViTs%20to%20be%20trained%20from%20scratch%20on%20datasets%20orders%20of%20magnitude%20smaller%20than%0Atraditionally%20required%20--without%20relying%20on%20massive%20external%20datasets%20for%0Apretraining.%20We%20validate%20our%20method%20on%20three%20small-size%2C%20standard-resoultion%2C%0Afine-grained%20datasets%3A%20Oxford%20Flowers-102%2C%20Oxford%20IIIT%20Pets-37%2C%20and%0AImageNet-100.%20Despite%20the%20challenges%20of%20limited%20data%20and%20high%20intra-class%0Asimilarity%2C%20frozen%20SCOTT%20models%20pretrained%20with%20MIM-JEPA%20significantly%0Aoutperform%20fully%20supervised%20methods%20and%20achieve%20competitive%20results%20with%20SOTA%0Aapproaches%20that%20rely%20on%20large-scale%20pretraining%2C%20complex%20image%20augmentations%0Aand%20bigger%20model%20sizes.%20By%20demonstrating%20that%20robust%20off-the-shelf%0Arepresentations%20can%20be%20learned%20with%20limited%20data%2C%20compute%2C%20and%20model%20sizes%2C%20our%0Awork%20paves%20the%20way%20for%20computer%20applications%20in%20resource%20constrained%0Aenvironments%20such%20as%20medical%20imaging%20or%20robotics.%20Our%20findings%20challenge%20the%0Aprevailing%20notion%20that%20vast%20amounts%20of%20data%20are%20indispensable%20for%20effective%0Arepresentation%20learning%20in%20vision%2C%20offering%20a%20new%20pathway%20toward%20more%0Aaccessible%20and%20inclusive%20advancements%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18056v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEscaping%2520The%2520Big%2520Data%2520Paradigm%2520in%2520Self-Supervised%2520Representation%250A%2520%2520Learning%26entry.906535625%3DCarlos%2520V%25C3%25A9lez%2520Garc%25C3%25ADa%2520and%2520Miguel%2520Cazorla%2520and%2520Jorge%2520Pomares%26entry.1292438233%3D%2520%2520The%2520reliance%2520on%2520large-scale%2520datasets%2520and%2520extensive%2520computational%2520resources%250Ahas%2520become%2520a%2520major%2520barrier%2520to%2520advancing%2520representation%2520learning%2520in%2520vision%252C%250Aespecially%2520in%2520data-scarce%2520domains.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520critical%250Aquestion%253A%2520Can%2520we%2520escape%2520the%2520big%2520data%2520paradigm%2520in%2520self-supervised%2520representation%250Alearning%2520from%2520images%253F%2520We%2520introduce%2520SCOTT%2520%2528Sparse%2520Convolutional%2520Tokenizer%2520for%250ATransformers%2529%252C%2520a%2520shallow%2520tokenization%2520architecture%2520that%2520is%2520compatible%2520with%250AMasked%2520Image%2520Modeling%2520%2528MIM%2529%2520tasks.%2520SCOTT%2520injects%2520convolutional%2520inductive%2520biases%250Ainto%2520Vision%2520Transformers%2520%2528ViTs%2529%252C%2520enhancing%2520their%2520efficacy%2520in%2520small-scale%2520data%250Aregimes.%2520Alongside%252C%2520we%2520propose%2520to%2520train%2520on%2520a%2520Joint-Embedding%2520Predictive%250AArchitecture%2520within%2520a%2520MIM%2520framework%2520%2528MIM-JEPA%2529%252C%2520operating%2520in%2520latent%250Arepresentation%2520space%2520to%2520capture%2520more%2520semantic%2520features.%2520Our%2520approach%2520enables%250AViTs%2520to%2520be%2520trained%2520from%2520scratch%2520on%2520datasets%2520orders%2520of%2520magnitude%2520smaller%2520than%250Atraditionally%2520required%2520--without%2520relying%2520on%2520massive%2520external%2520datasets%2520for%250Apretraining.%2520We%2520validate%2520our%2520method%2520on%2520three%2520small-size%252C%2520standard-resoultion%252C%250Afine-grained%2520datasets%253A%2520Oxford%2520Flowers-102%252C%2520Oxford%2520IIIT%2520Pets-37%252C%2520and%250AImageNet-100.%2520Despite%2520the%2520challenges%2520of%2520limited%2520data%2520and%2520high%2520intra-class%250Asimilarity%252C%2520frozen%2520SCOTT%2520models%2520pretrained%2520with%2520MIM-JEPA%2520significantly%250Aoutperform%2520fully%2520supervised%2520methods%2520and%2520achieve%2520competitive%2520results%2520with%2520SOTA%250Aapproaches%2520that%2520rely%2520on%2520large-scale%2520pretraining%252C%2520complex%2520image%2520augmentations%250Aand%2520bigger%2520model%2520sizes.%2520By%2520demonstrating%2520that%2520robust%2520off-the-shelf%250Arepresentations%2520can%2520be%2520learned%2520with%2520limited%2520data%252C%2520compute%252C%2520and%2520model%2520sizes%252C%2520our%250Awork%2520paves%2520the%2520way%2520for%2520computer%2520applications%2520in%2520resource%2520constrained%250Aenvironments%2520such%2520as%2520medical%2520imaging%2520or%2520robotics.%2520Our%2520findings%2520challenge%2520the%250Aprevailing%2520notion%2520that%2520vast%2520amounts%2520of%2520data%2520are%2520indispensable%2520for%2520effective%250Arepresentation%2520learning%2520in%2520vision%252C%2520offering%2520a%2520new%2520pathway%2520toward%2520more%250Aaccessible%2520and%2520inclusive%2520advancements%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18056v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Escaping%20The%20Big%20Data%20Paradigm%20in%20Self-Supervised%20Representation%0A%20%20Learning&entry.906535625=Carlos%20V%C3%A9lez%20Garc%C3%ADa%20and%20Miguel%20Cazorla%20and%20Jorge%20Pomares&entry.1292438233=%20%20The%20reliance%20on%20large-scale%20datasets%20and%20extensive%20computational%20resources%0Ahas%20become%20a%20major%20barrier%20to%20advancing%20representation%20learning%20in%20vision%2C%0Aespecially%20in%20data-scarce%20domains.%20In%20this%20paper%2C%20we%20address%20the%20critical%0Aquestion%3A%20Can%20we%20escape%20the%20big%20data%20paradigm%20in%20self-supervised%20representation%0Alearning%20from%20images%3F%20We%20introduce%20SCOTT%20%28Sparse%20Convolutional%20Tokenizer%20for%0ATransformers%29%2C%20a%20shallow%20tokenization%20architecture%20that%20is%20compatible%20with%0AMasked%20Image%20Modeling%20%28MIM%29%20tasks.%20SCOTT%20injects%20convolutional%20inductive%20biases%0Ainto%20Vision%20Transformers%20%28ViTs%29%2C%20enhancing%20their%20efficacy%20in%20small-scale%20data%0Aregimes.%20Alongside%2C%20we%20propose%20to%20train%20on%20a%20Joint-Embedding%20Predictive%0AArchitecture%20within%20a%20MIM%20framework%20%28MIM-JEPA%29%2C%20operating%20in%20latent%0Arepresentation%20space%20to%20capture%20more%20semantic%20features.%20Our%20approach%20enables%0AViTs%20to%20be%20trained%20from%20scratch%20on%20datasets%20orders%20of%20magnitude%20smaller%20than%0Atraditionally%20required%20--without%20relying%20on%20massive%20external%20datasets%20for%0Apretraining.%20We%20validate%20our%20method%20on%20three%20small-size%2C%20standard-resoultion%2C%0Afine-grained%20datasets%3A%20Oxford%20Flowers-102%2C%20Oxford%20IIIT%20Pets-37%2C%20and%0AImageNet-100.%20Despite%20the%20challenges%20of%20limited%20data%20and%20high%20intra-class%0Asimilarity%2C%20frozen%20SCOTT%20models%20pretrained%20with%20MIM-JEPA%20significantly%0Aoutperform%20fully%20supervised%20methods%20and%20achieve%20competitive%20results%20with%20SOTA%0Aapproaches%20that%20rely%20on%20large-scale%20pretraining%2C%20complex%20image%20augmentations%0Aand%20bigger%20model%20sizes.%20By%20demonstrating%20that%20robust%20off-the-shelf%0Arepresentations%20can%20be%20learned%20with%20limited%20data%2C%20compute%2C%20and%20model%20sizes%2C%20our%0Awork%20paves%20the%20way%20for%20computer%20applications%20in%20resource%20constrained%0Aenvironments%20such%20as%20medical%20imaging%20or%20robotics.%20Our%20findings%20challenge%20the%0Aprevailing%20notion%20that%20vast%20amounts%20of%20data%20are%20indispensable%20for%20effective%0Arepresentation%20learning%20in%20vision%2C%20offering%20a%20new%20pathway%20toward%20more%0Aaccessible%20and%20inclusive%20advancements%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18056v1&entry.124074799=Read"},
{"title": "VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver\n  Attention Fusion", "author": "Pei Liu and Haipeng Liu and Haichao Liu and Xin Liu and Jinxin Ni and Jun Ma", "abstract": "  Human drivers adeptly navigate complex scenarios by utilizing rich\nattentional semantics, but the current autonomous systems struggle to replicate\nthis ability, as they often lose critical semantic information when converting\n2D observations into 3D space. In this sense, it hinders their effective\ndeployment in dynamic and complex environments. Leveraging the superior scene\nunderstanding and reasoning abilities of Vision-Language Models (VLMs), we\npropose VLM-E2E, a novel framework that uses the VLMs to enhance training by\nproviding attentional cues. Our method integrates textual representations into\nBird's-Eye-View (BEV) features for semantic supervision, which enables the\nmodel to learn richer feature representations that explicitly capture the\ndriver's attentional semantics. By focusing on attentional semantics, VLM-E2E\nbetter aligns with human-like driving behavior, which is critical for\nnavigating dynamic and complex environments. Furthermore, we introduce a\nBEV-Text learnable weighted fusion strategy to address the issue of modality\nimportance imbalance in fusing multimodal information. This approach\ndynamically balances the contributions of BEV and text features, ensuring that\nthe complementary information from visual and textual modality is effectively\nutilized. By explicitly addressing the imbalance in multimodal fusion, our\nmethod facilitates a more holistic and robust representation of driving\nenvironments. We evaluate VLM-E2E on the nuScenes dataset and demonstrate its\nsuperiority over state-of-the-art approaches, showcasing significant\nimprovements in performance.\n", "link": "http://arxiv.org/abs/2502.18042v1", "date": "2025-02-25", "relevancy": 2.9091, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5908}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5908}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLM-E2E%3A%20Enhancing%20End-to-End%20Autonomous%20Driving%20with%20Multimodal%20Driver%0A%20%20Attention%20Fusion&body=Title%3A%20VLM-E2E%3A%20Enhancing%20End-to-End%20Autonomous%20Driving%20with%20Multimodal%20Driver%0A%20%20Attention%20Fusion%0AAuthor%3A%20Pei%20Liu%20and%20Haipeng%20Liu%20and%20Haichao%20Liu%20and%20Xin%20Liu%20and%20Jinxin%20Ni%20and%20Jun%20Ma%0AAbstract%3A%20%20%20Human%20drivers%20adeptly%20navigate%20complex%20scenarios%20by%20utilizing%20rich%0Aattentional%20semantics%2C%20but%20the%20current%20autonomous%20systems%20struggle%20to%20replicate%0Athis%20ability%2C%20as%20they%20often%20lose%20critical%20semantic%20information%20when%20converting%0A2D%20observations%20into%203D%20space.%20In%20this%20sense%2C%20it%20hinders%20their%20effective%0Adeployment%20in%20dynamic%20and%20complex%20environments.%20Leveraging%20the%20superior%20scene%0Aunderstanding%20and%20reasoning%20abilities%20of%20Vision-Language%20Models%20%28VLMs%29%2C%20we%0Apropose%20VLM-E2E%2C%20a%20novel%20framework%20that%20uses%20the%20VLMs%20to%20enhance%20training%20by%0Aproviding%20attentional%20cues.%20Our%20method%20integrates%20textual%20representations%20into%0ABird%27s-Eye-View%20%28BEV%29%20features%20for%20semantic%20supervision%2C%20which%20enables%20the%0Amodel%20to%20learn%20richer%20feature%20representations%20that%20explicitly%20capture%20the%0Adriver%27s%20attentional%20semantics.%20By%20focusing%20on%20attentional%20semantics%2C%20VLM-E2E%0Abetter%20aligns%20with%20human-like%20driving%20behavior%2C%20which%20is%20critical%20for%0Anavigating%20dynamic%20and%20complex%20environments.%20Furthermore%2C%20we%20introduce%20a%0ABEV-Text%20learnable%20weighted%20fusion%20strategy%20to%20address%20the%20issue%20of%20modality%0Aimportance%20imbalance%20in%20fusing%20multimodal%20information.%20This%20approach%0Adynamically%20balances%20the%20contributions%20of%20BEV%20and%20text%20features%2C%20ensuring%20that%0Athe%20complementary%20information%20from%20visual%20and%20textual%20modality%20is%20effectively%0Autilized.%20By%20explicitly%20addressing%20the%20imbalance%20in%20multimodal%20fusion%2C%20our%0Amethod%20facilitates%20a%20more%20holistic%20and%20robust%20representation%20of%20driving%0Aenvironments.%20We%20evaluate%20VLM-E2E%20on%20the%20nuScenes%20dataset%20and%20demonstrate%20its%0Asuperiority%20over%20state-of-the-art%20approaches%2C%20showcasing%20significant%0Aimprovements%20in%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLM-E2E%253A%2520Enhancing%2520End-to-End%2520Autonomous%2520Driving%2520with%2520Multimodal%2520Driver%250A%2520%2520Attention%2520Fusion%26entry.906535625%3DPei%2520Liu%2520and%2520Haipeng%2520Liu%2520and%2520Haichao%2520Liu%2520and%2520Xin%2520Liu%2520and%2520Jinxin%2520Ni%2520and%2520Jun%2520Ma%26entry.1292438233%3D%2520%2520Human%2520drivers%2520adeptly%2520navigate%2520complex%2520scenarios%2520by%2520utilizing%2520rich%250Aattentional%2520semantics%252C%2520but%2520the%2520current%2520autonomous%2520systems%2520struggle%2520to%2520replicate%250Athis%2520ability%252C%2520as%2520they%2520often%2520lose%2520critical%2520semantic%2520information%2520when%2520converting%250A2D%2520observations%2520into%25203D%2520space.%2520In%2520this%2520sense%252C%2520it%2520hinders%2520their%2520effective%250Adeployment%2520in%2520dynamic%2520and%2520complex%2520environments.%2520Leveraging%2520the%2520superior%2520scene%250Aunderstanding%2520and%2520reasoning%2520abilities%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520we%250Apropose%2520VLM-E2E%252C%2520a%2520novel%2520framework%2520that%2520uses%2520the%2520VLMs%2520to%2520enhance%2520training%2520by%250Aproviding%2520attentional%2520cues.%2520Our%2520method%2520integrates%2520textual%2520representations%2520into%250ABird%2527s-Eye-View%2520%2528BEV%2529%2520features%2520for%2520semantic%2520supervision%252C%2520which%2520enables%2520the%250Amodel%2520to%2520learn%2520richer%2520feature%2520representations%2520that%2520explicitly%2520capture%2520the%250Adriver%2527s%2520attentional%2520semantics.%2520By%2520focusing%2520on%2520attentional%2520semantics%252C%2520VLM-E2E%250Abetter%2520aligns%2520with%2520human-like%2520driving%2520behavior%252C%2520which%2520is%2520critical%2520for%250Anavigating%2520dynamic%2520and%2520complex%2520environments.%2520Furthermore%252C%2520we%2520introduce%2520a%250ABEV-Text%2520learnable%2520weighted%2520fusion%2520strategy%2520to%2520address%2520the%2520issue%2520of%2520modality%250Aimportance%2520imbalance%2520in%2520fusing%2520multimodal%2520information.%2520This%2520approach%250Adynamically%2520balances%2520the%2520contributions%2520of%2520BEV%2520and%2520text%2520features%252C%2520ensuring%2520that%250Athe%2520complementary%2520information%2520from%2520visual%2520and%2520textual%2520modality%2520is%2520effectively%250Autilized.%2520By%2520explicitly%2520addressing%2520the%2520imbalance%2520in%2520multimodal%2520fusion%252C%2520our%250Amethod%2520facilitates%2520a%2520more%2520holistic%2520and%2520robust%2520representation%2520of%2520driving%250Aenvironments.%2520We%2520evaluate%2520VLM-E2E%2520on%2520the%2520nuScenes%2520dataset%2520and%2520demonstrate%2520its%250Asuperiority%2520over%2520state-of-the-art%2520approaches%252C%2520showcasing%2520significant%250Aimprovements%2520in%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLM-E2E%3A%20Enhancing%20End-to-End%20Autonomous%20Driving%20with%20Multimodal%20Driver%0A%20%20Attention%20Fusion&entry.906535625=Pei%20Liu%20and%20Haipeng%20Liu%20and%20Haichao%20Liu%20and%20Xin%20Liu%20and%20Jinxin%20Ni%20and%20Jun%20Ma&entry.1292438233=%20%20Human%20drivers%20adeptly%20navigate%20complex%20scenarios%20by%20utilizing%20rich%0Aattentional%20semantics%2C%20but%20the%20current%20autonomous%20systems%20struggle%20to%20replicate%0Athis%20ability%2C%20as%20they%20often%20lose%20critical%20semantic%20information%20when%20converting%0A2D%20observations%20into%203D%20space.%20In%20this%20sense%2C%20it%20hinders%20their%20effective%0Adeployment%20in%20dynamic%20and%20complex%20environments.%20Leveraging%20the%20superior%20scene%0Aunderstanding%20and%20reasoning%20abilities%20of%20Vision-Language%20Models%20%28VLMs%29%2C%20we%0Apropose%20VLM-E2E%2C%20a%20novel%20framework%20that%20uses%20the%20VLMs%20to%20enhance%20training%20by%0Aproviding%20attentional%20cues.%20Our%20method%20integrates%20textual%20representations%20into%0ABird%27s-Eye-View%20%28BEV%29%20features%20for%20semantic%20supervision%2C%20which%20enables%20the%0Amodel%20to%20learn%20richer%20feature%20representations%20that%20explicitly%20capture%20the%0Adriver%27s%20attentional%20semantics.%20By%20focusing%20on%20attentional%20semantics%2C%20VLM-E2E%0Abetter%20aligns%20with%20human-like%20driving%20behavior%2C%20which%20is%20critical%20for%0Anavigating%20dynamic%20and%20complex%20environments.%20Furthermore%2C%20we%20introduce%20a%0ABEV-Text%20learnable%20weighted%20fusion%20strategy%20to%20address%20the%20issue%20of%20modality%0Aimportance%20imbalance%20in%20fusing%20multimodal%20information.%20This%20approach%0Adynamically%20balances%20the%20contributions%20of%20BEV%20and%20text%20features%2C%20ensuring%20that%0Athe%20complementary%20information%20from%20visual%20and%20textual%20modality%20is%20effectively%0Autilized.%20By%20explicitly%20addressing%20the%20imbalance%20in%20multimodal%20fusion%2C%20our%0Amethod%20facilitates%20a%20more%20holistic%20and%20robust%20representation%20of%20driving%0Aenvironments.%20We%20evaluate%20VLM-E2E%20on%20the%20nuScenes%20dataset%20and%20demonstrate%20its%0Asuperiority%20over%20state-of-the-art%20approaches%2C%20showcasing%20significant%0Aimprovements%20in%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18042v1&entry.124074799=Read"},
{"title": "EgoSim: An Egocentric Multi-view Simulator and Real Dataset for\n  Body-worn Cameras during Motion and Activity", "author": "Dominik Hollidt and Paul Streli and Jiaxi Jiang and Yasaman Haghighi and Changlin Qian and Xintong Liu and Christian Holz", "abstract": "  Research on egocentric tasks in computer vision has mostly focused on\nhead-mounted cameras, such as fisheye cameras or embedded cameras inside\nimmersive headsets. We argue that the increasing miniaturization of optical\nsensors will lead to the prolific integration of cameras into many more\nbody-worn devices at various locations. This will bring fresh perspectives to\nestablished tasks in computer vision and benefit key areas such as human motion\ntracking, body pose estimation, or action recognition -- particularly for the\nlower body, which is typically occluded.\n  In this paper, we introduce EgoSim, a novel simulator of body-worn cameras\nthat generates realistic egocentric renderings from multiple perspectives\nacross a wearer's body. A key feature of EgoSim is its use of real motion\ncapture data to render motion artifacts, which are especially noticeable with\narm- or leg-worn cameras. In addition, we introduce MultiEgoView, a dataset of\negocentric footage from six body-worn cameras and ground-truth full-body 3D\nposes during several activities: 119 hours of data are derived from AMASS\nmotion sequences in four high-fidelity virtual environments, which we augment\nwith 5 hours of real-world motion data from 13 participants using six GoPro\ncameras and 3D body pose references from an Xsens motion capture suit.\n  We demonstrate EgoSim's effectiveness by training an end-to-end video-only 3D\npose estimation network. Analyzing its domain gap, we show that our dataset and\nsimulator substantially aid training for inference on real-world data.\n  EgoSim code & MultiEgoView dataset: https://siplab.org/projects/EgoSim\n", "link": "http://arxiv.org/abs/2502.18373v1", "date": "2025-02-25", "relevancy": 2.8568, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5799}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5779}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoSim%3A%20An%20Egocentric%20Multi-view%20Simulator%20and%20Real%20Dataset%20for%0A%20%20Body-worn%20Cameras%20during%20Motion%20and%20Activity&body=Title%3A%20EgoSim%3A%20An%20Egocentric%20Multi-view%20Simulator%20and%20Real%20Dataset%20for%0A%20%20Body-worn%20Cameras%20during%20Motion%20and%20Activity%0AAuthor%3A%20Dominik%20Hollidt%20and%20Paul%20Streli%20and%20Jiaxi%20Jiang%20and%20Yasaman%20Haghighi%20and%20Changlin%20Qian%20and%20Xintong%20Liu%20and%20Christian%20Holz%0AAbstract%3A%20%20%20Research%20on%20egocentric%20tasks%20in%20computer%20vision%20has%20mostly%20focused%20on%0Ahead-mounted%20cameras%2C%20such%20as%20fisheye%20cameras%20or%20embedded%20cameras%20inside%0Aimmersive%20headsets.%20We%20argue%20that%20the%20increasing%20miniaturization%20of%20optical%0Asensors%20will%20lead%20to%20the%20prolific%20integration%20of%20cameras%20into%20many%20more%0Abody-worn%20devices%20at%20various%20locations.%20This%20will%20bring%20fresh%20perspectives%20to%0Aestablished%20tasks%20in%20computer%20vision%20and%20benefit%20key%20areas%20such%20as%20human%20motion%0Atracking%2C%20body%20pose%20estimation%2C%20or%20action%20recognition%20--%20particularly%20for%20the%0Alower%20body%2C%20which%20is%20typically%20occluded.%0A%20%20In%20this%20paper%2C%20we%20introduce%20EgoSim%2C%20a%20novel%20simulator%20of%20body-worn%20cameras%0Athat%20generates%20realistic%20egocentric%20renderings%20from%20multiple%20perspectives%0Aacross%20a%20wearer%27s%20body.%20A%20key%20feature%20of%20EgoSim%20is%20its%20use%20of%20real%20motion%0Acapture%20data%20to%20render%20motion%20artifacts%2C%20which%20are%20especially%20noticeable%20with%0Aarm-%20or%20leg-worn%20cameras.%20In%20addition%2C%20we%20introduce%20MultiEgoView%2C%20a%20dataset%20of%0Aegocentric%20footage%20from%20six%20body-worn%20cameras%20and%20ground-truth%20full-body%203D%0Aposes%20during%20several%20activities%3A%20119%20hours%20of%20data%20are%20derived%20from%20AMASS%0Amotion%20sequences%20in%20four%20high-fidelity%20virtual%20environments%2C%20which%20we%20augment%0Awith%205%20hours%20of%20real-world%20motion%20data%20from%2013%20participants%20using%20six%20GoPro%0Acameras%20and%203D%20body%20pose%20references%20from%20an%20Xsens%20motion%20capture%20suit.%0A%20%20We%20demonstrate%20EgoSim%27s%20effectiveness%20by%20training%20an%20end-to-end%20video-only%203D%0Apose%20estimation%20network.%20Analyzing%20its%20domain%20gap%2C%20we%20show%20that%20our%20dataset%20and%0Asimulator%20substantially%20aid%20training%20for%20inference%20on%20real-world%20data.%0A%20%20EgoSim%20code%20%26%20MultiEgoView%20dataset%3A%20https%3A//siplab.org/projects/EgoSim%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18373v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoSim%253A%2520An%2520Egocentric%2520Multi-view%2520Simulator%2520and%2520Real%2520Dataset%2520for%250A%2520%2520Body-worn%2520Cameras%2520during%2520Motion%2520and%2520Activity%26entry.906535625%3DDominik%2520Hollidt%2520and%2520Paul%2520Streli%2520and%2520Jiaxi%2520Jiang%2520and%2520Yasaman%2520Haghighi%2520and%2520Changlin%2520Qian%2520and%2520Xintong%2520Liu%2520and%2520Christian%2520Holz%26entry.1292438233%3D%2520%2520Research%2520on%2520egocentric%2520tasks%2520in%2520computer%2520vision%2520has%2520mostly%2520focused%2520on%250Ahead-mounted%2520cameras%252C%2520such%2520as%2520fisheye%2520cameras%2520or%2520embedded%2520cameras%2520inside%250Aimmersive%2520headsets.%2520We%2520argue%2520that%2520the%2520increasing%2520miniaturization%2520of%2520optical%250Asensors%2520will%2520lead%2520to%2520the%2520prolific%2520integration%2520of%2520cameras%2520into%2520many%2520more%250Abody-worn%2520devices%2520at%2520various%2520locations.%2520This%2520will%2520bring%2520fresh%2520perspectives%2520to%250Aestablished%2520tasks%2520in%2520computer%2520vision%2520and%2520benefit%2520key%2520areas%2520such%2520as%2520human%2520motion%250Atracking%252C%2520body%2520pose%2520estimation%252C%2520or%2520action%2520recognition%2520--%2520particularly%2520for%2520the%250Alower%2520body%252C%2520which%2520is%2520typically%2520occluded.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520EgoSim%252C%2520a%2520novel%2520simulator%2520of%2520body-worn%2520cameras%250Athat%2520generates%2520realistic%2520egocentric%2520renderings%2520from%2520multiple%2520perspectives%250Aacross%2520a%2520wearer%2527s%2520body.%2520A%2520key%2520feature%2520of%2520EgoSim%2520is%2520its%2520use%2520of%2520real%2520motion%250Acapture%2520data%2520to%2520render%2520motion%2520artifacts%252C%2520which%2520are%2520especially%2520noticeable%2520with%250Aarm-%2520or%2520leg-worn%2520cameras.%2520In%2520addition%252C%2520we%2520introduce%2520MultiEgoView%252C%2520a%2520dataset%2520of%250Aegocentric%2520footage%2520from%2520six%2520body-worn%2520cameras%2520and%2520ground-truth%2520full-body%25203D%250Aposes%2520during%2520several%2520activities%253A%2520119%2520hours%2520of%2520data%2520are%2520derived%2520from%2520AMASS%250Amotion%2520sequences%2520in%2520four%2520high-fidelity%2520virtual%2520environments%252C%2520which%2520we%2520augment%250Awith%25205%2520hours%2520of%2520real-world%2520motion%2520data%2520from%252013%2520participants%2520using%2520six%2520GoPro%250Acameras%2520and%25203D%2520body%2520pose%2520references%2520from%2520an%2520Xsens%2520motion%2520capture%2520suit.%250A%2520%2520We%2520demonstrate%2520EgoSim%2527s%2520effectiveness%2520by%2520training%2520an%2520end-to-end%2520video-only%25203D%250Apose%2520estimation%2520network.%2520Analyzing%2520its%2520domain%2520gap%252C%2520we%2520show%2520that%2520our%2520dataset%2520and%250Asimulator%2520substantially%2520aid%2520training%2520for%2520inference%2520on%2520real-world%2520data.%250A%2520%2520EgoSim%2520code%2520%2526%2520MultiEgoView%2520dataset%253A%2520https%253A//siplab.org/projects/EgoSim%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18373v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoSim%3A%20An%20Egocentric%20Multi-view%20Simulator%20and%20Real%20Dataset%20for%0A%20%20Body-worn%20Cameras%20during%20Motion%20and%20Activity&entry.906535625=Dominik%20Hollidt%20and%20Paul%20Streli%20and%20Jiaxi%20Jiang%20and%20Yasaman%20Haghighi%20and%20Changlin%20Qian%20and%20Xintong%20Liu%20and%20Christian%20Holz&entry.1292438233=%20%20Research%20on%20egocentric%20tasks%20in%20computer%20vision%20has%20mostly%20focused%20on%0Ahead-mounted%20cameras%2C%20such%20as%20fisheye%20cameras%20or%20embedded%20cameras%20inside%0Aimmersive%20headsets.%20We%20argue%20that%20the%20increasing%20miniaturization%20of%20optical%0Asensors%20will%20lead%20to%20the%20prolific%20integration%20of%20cameras%20into%20many%20more%0Abody-worn%20devices%20at%20various%20locations.%20This%20will%20bring%20fresh%20perspectives%20to%0Aestablished%20tasks%20in%20computer%20vision%20and%20benefit%20key%20areas%20such%20as%20human%20motion%0Atracking%2C%20body%20pose%20estimation%2C%20or%20action%20recognition%20--%20particularly%20for%20the%0Alower%20body%2C%20which%20is%20typically%20occluded.%0A%20%20In%20this%20paper%2C%20we%20introduce%20EgoSim%2C%20a%20novel%20simulator%20of%20body-worn%20cameras%0Athat%20generates%20realistic%20egocentric%20renderings%20from%20multiple%20perspectives%0Aacross%20a%20wearer%27s%20body.%20A%20key%20feature%20of%20EgoSim%20is%20its%20use%20of%20real%20motion%0Acapture%20data%20to%20render%20motion%20artifacts%2C%20which%20are%20especially%20noticeable%20with%0Aarm-%20or%20leg-worn%20cameras.%20In%20addition%2C%20we%20introduce%20MultiEgoView%2C%20a%20dataset%20of%0Aegocentric%20footage%20from%20six%20body-worn%20cameras%20and%20ground-truth%20full-body%203D%0Aposes%20during%20several%20activities%3A%20119%20hours%20of%20data%20are%20derived%20from%20AMASS%0Amotion%20sequences%20in%20four%20high-fidelity%20virtual%20environments%2C%20which%20we%20augment%0Awith%205%20hours%20of%20real-world%20motion%20data%20from%2013%20participants%20using%20six%20GoPro%0Acameras%20and%203D%20body%20pose%20references%20from%20an%20Xsens%20motion%20capture%20suit.%0A%20%20We%20demonstrate%20EgoSim%27s%20effectiveness%20by%20training%20an%20end-to-end%20video-only%203D%0Apose%20estimation%20network.%20Analyzing%20its%20domain%20gap%2C%20we%20show%20that%20our%20dataset%20and%0Asimulator%20substantially%20aid%20training%20for%20inference%20on%20real-world%20data.%0A%20%20EgoSim%20code%20%26%20MultiEgoView%20dataset%3A%20https%3A//siplab.org/projects/EgoSim%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18373v1&entry.124074799=Read"},
{"title": "Learning Structure-Supporting Dependencies via Keypoint Interactive\n  Transformer for General Mammal Pose Estimation", "author": "Tianyang Xu and Jiyong Rao and Xiaoning Song and Zhenhua Feng and Xiao-Jun Wu", "abstract": "  General mammal pose estimation is an important and challenging task in\ncomputer vision, which is essential for understanding mammal behaviour in\nreal-world applications. However, existing studies are at their preliminary\nresearch stage, which focus on addressing the problem for only a few specific\nmammal species. In principle, from specific to general mammal pose estimation,\nthe biggest issue is how to address the huge appearance and pose variances for\ndifferent species. We argue that given appearance context, instance-level prior\nand the structural relation among keypoints can serve as complementary\nevidence. To this end, we propose a Keypoint Interactive Transformer (KIT) to\nlearn instance-level structure-supporting dependencies for general mammal pose\nestimation. Specifically, our KITPose consists of two coupled components. The\nfirst component is to extract keypoint features and generate body part prompts.\nThe features are supervised by a dedicated generalised heatmap regression loss\n(GHRL). Instead of introducing external visual/text prompts, we devise\nkeypoints clustering to generate body part biases, aligning them with image\ncontext to generate corresponding instance-level prompts. Second, we propose a\nnovel interactive transformer that takes feature slices as input tokens without\nperforming spatial splitting. In addition, to enhance the capability of the KIT\nmodel, we design an adaptive weight strategy to address the imbalance issue\namong different keypoints.\n", "link": "http://arxiv.org/abs/2502.18214v1", "date": "2025-02-25", "relevancy": 2.8506, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5792}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5672}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Structure-Supporting%20Dependencies%20via%20Keypoint%20Interactive%0A%20%20Transformer%20for%20General%20Mammal%20Pose%20Estimation&body=Title%3A%20Learning%20Structure-Supporting%20Dependencies%20via%20Keypoint%20Interactive%0A%20%20Transformer%20for%20General%20Mammal%20Pose%20Estimation%0AAuthor%3A%20Tianyang%20Xu%20and%20Jiyong%20Rao%20and%20Xiaoning%20Song%20and%20Zhenhua%20Feng%20and%20Xiao-Jun%20Wu%0AAbstract%3A%20%20%20General%20mammal%20pose%20estimation%20is%20an%20important%20and%20challenging%20task%20in%0Acomputer%20vision%2C%20which%20is%20essential%20for%20understanding%20mammal%20behaviour%20in%0Areal-world%20applications.%20However%2C%20existing%20studies%20are%20at%20their%20preliminary%0Aresearch%20stage%2C%20which%20focus%20on%20addressing%20the%20problem%20for%20only%20a%20few%20specific%0Amammal%20species.%20In%20principle%2C%20from%20specific%20to%20general%20mammal%20pose%20estimation%2C%0Athe%20biggest%20issue%20is%20how%20to%20address%20the%20huge%20appearance%20and%20pose%20variances%20for%0Adifferent%20species.%20We%20argue%20that%20given%20appearance%20context%2C%20instance-level%20prior%0Aand%20the%20structural%20relation%20among%20keypoints%20can%20serve%20as%20complementary%0Aevidence.%20To%20this%20end%2C%20we%20propose%20a%20Keypoint%20Interactive%20Transformer%20%28KIT%29%20to%0Alearn%20instance-level%20structure-supporting%20dependencies%20for%20general%20mammal%20pose%0Aestimation.%20Specifically%2C%20our%20KITPose%20consists%20of%20two%20coupled%20components.%20The%0Afirst%20component%20is%20to%20extract%20keypoint%20features%20and%20generate%20body%20part%20prompts.%0AThe%20features%20are%20supervised%20by%20a%20dedicated%20generalised%20heatmap%20regression%20loss%0A%28GHRL%29.%20Instead%20of%20introducing%20external%20visual/text%20prompts%2C%20we%20devise%0Akeypoints%20clustering%20to%20generate%20body%20part%20biases%2C%20aligning%20them%20with%20image%0Acontext%20to%20generate%20corresponding%20instance-level%20prompts.%20Second%2C%20we%20propose%20a%0Anovel%20interactive%20transformer%20that%20takes%20feature%20slices%20as%20input%20tokens%20without%0Aperforming%20spatial%20splitting.%20In%20addition%2C%20to%20enhance%20the%20capability%20of%20the%20KIT%0Amodel%2C%20we%20design%20an%20adaptive%20weight%20strategy%20to%20address%20the%20imbalance%20issue%0Aamong%20different%20keypoints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Structure-Supporting%2520Dependencies%2520via%2520Keypoint%2520Interactive%250A%2520%2520Transformer%2520for%2520General%2520Mammal%2520Pose%2520Estimation%26entry.906535625%3DTianyang%2520Xu%2520and%2520Jiyong%2520Rao%2520and%2520Xiaoning%2520Song%2520and%2520Zhenhua%2520Feng%2520and%2520Xiao-Jun%2520Wu%26entry.1292438233%3D%2520%2520General%2520mammal%2520pose%2520estimation%2520is%2520an%2520important%2520and%2520challenging%2520task%2520in%250Acomputer%2520vision%252C%2520which%2520is%2520essential%2520for%2520understanding%2520mammal%2520behaviour%2520in%250Areal-world%2520applications.%2520However%252C%2520existing%2520studies%2520are%2520at%2520their%2520preliminary%250Aresearch%2520stage%252C%2520which%2520focus%2520on%2520addressing%2520the%2520problem%2520for%2520only%2520a%2520few%2520specific%250Amammal%2520species.%2520In%2520principle%252C%2520from%2520specific%2520to%2520general%2520mammal%2520pose%2520estimation%252C%250Athe%2520biggest%2520issue%2520is%2520how%2520to%2520address%2520the%2520huge%2520appearance%2520and%2520pose%2520variances%2520for%250Adifferent%2520species.%2520We%2520argue%2520that%2520given%2520appearance%2520context%252C%2520instance-level%2520prior%250Aand%2520the%2520structural%2520relation%2520among%2520keypoints%2520can%2520serve%2520as%2520complementary%250Aevidence.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520Keypoint%2520Interactive%2520Transformer%2520%2528KIT%2529%2520to%250Alearn%2520instance-level%2520structure-supporting%2520dependencies%2520for%2520general%2520mammal%2520pose%250Aestimation.%2520Specifically%252C%2520our%2520KITPose%2520consists%2520of%2520two%2520coupled%2520components.%2520The%250Afirst%2520component%2520is%2520to%2520extract%2520keypoint%2520features%2520and%2520generate%2520body%2520part%2520prompts.%250AThe%2520features%2520are%2520supervised%2520by%2520a%2520dedicated%2520generalised%2520heatmap%2520regression%2520loss%250A%2528GHRL%2529.%2520Instead%2520of%2520introducing%2520external%2520visual/text%2520prompts%252C%2520we%2520devise%250Akeypoints%2520clustering%2520to%2520generate%2520body%2520part%2520biases%252C%2520aligning%2520them%2520with%2520image%250Acontext%2520to%2520generate%2520corresponding%2520instance-level%2520prompts.%2520Second%252C%2520we%2520propose%2520a%250Anovel%2520interactive%2520transformer%2520that%2520takes%2520feature%2520slices%2520as%2520input%2520tokens%2520without%250Aperforming%2520spatial%2520splitting.%2520In%2520addition%252C%2520to%2520enhance%2520the%2520capability%2520of%2520the%2520KIT%250Amodel%252C%2520we%2520design%2520an%2520adaptive%2520weight%2520strategy%2520to%2520address%2520the%2520imbalance%2520issue%250Aamong%2520different%2520keypoints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Structure-Supporting%20Dependencies%20via%20Keypoint%20Interactive%0A%20%20Transformer%20for%20General%20Mammal%20Pose%20Estimation&entry.906535625=Tianyang%20Xu%20and%20Jiyong%20Rao%20and%20Xiaoning%20Song%20and%20Zhenhua%20Feng%20and%20Xiao-Jun%20Wu&entry.1292438233=%20%20General%20mammal%20pose%20estimation%20is%20an%20important%20and%20challenging%20task%20in%0Acomputer%20vision%2C%20which%20is%20essential%20for%20understanding%20mammal%20behaviour%20in%0Areal-world%20applications.%20However%2C%20existing%20studies%20are%20at%20their%20preliminary%0Aresearch%20stage%2C%20which%20focus%20on%20addressing%20the%20problem%20for%20only%20a%20few%20specific%0Amammal%20species.%20In%20principle%2C%20from%20specific%20to%20general%20mammal%20pose%20estimation%2C%0Athe%20biggest%20issue%20is%20how%20to%20address%20the%20huge%20appearance%20and%20pose%20variances%20for%0Adifferent%20species.%20We%20argue%20that%20given%20appearance%20context%2C%20instance-level%20prior%0Aand%20the%20structural%20relation%20among%20keypoints%20can%20serve%20as%20complementary%0Aevidence.%20To%20this%20end%2C%20we%20propose%20a%20Keypoint%20Interactive%20Transformer%20%28KIT%29%20to%0Alearn%20instance-level%20structure-supporting%20dependencies%20for%20general%20mammal%20pose%0Aestimation.%20Specifically%2C%20our%20KITPose%20consists%20of%20two%20coupled%20components.%20The%0Afirst%20component%20is%20to%20extract%20keypoint%20features%20and%20generate%20body%20part%20prompts.%0AThe%20features%20are%20supervised%20by%20a%20dedicated%20generalised%20heatmap%20regression%20loss%0A%28GHRL%29.%20Instead%20of%20introducing%20external%20visual/text%20prompts%2C%20we%20devise%0Akeypoints%20clustering%20to%20generate%20body%20part%20biases%2C%20aligning%20them%20with%20image%0Acontext%20to%20generate%20corresponding%20instance-level%20prompts.%20Second%2C%20we%20propose%20a%0Anovel%20interactive%20transformer%20that%20takes%20feature%20slices%20as%20input%20tokens%20without%0Aperforming%20spatial%20splitting.%20In%20addition%2C%20to%20enhance%20the%20capability%20of%20the%20KIT%0Amodel%2C%20we%20design%20an%20adaptive%20weight%20strategy%20to%20address%20the%20imbalance%20issue%0Aamong%20different%20keypoints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18214v1&entry.124074799=Read"},
{"title": "OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial\n  Vision-Language Navigation", "author": "Yunpeng Gao and Chenhui Li and Zhongrui You and Junli Liu and Zhen Li and Pengan Chen and Qizhi Chen and Zhonghan Tang and Liansheng Wang and Penghui Yang and Yiwen Tang and Yuhang Tang and Shuai Liang and Songyi Zhu and Ziqin Xiong and Yifei Su and Xinyi Ye and Jianan Li and Yan Ding and Dong Wang and Zhigang Wang and Bin Zhao and Xuelong Li", "abstract": "  Vision-Language Navigation (VLN) aims to guide agents through an environment\nby leveraging both language instructions and visual cues, playing a pivotal\nrole in embodied AI. Indoor VLN has been extensively studied, whereas outdoor\naerial VLN remains underexplored. The potential reason is that outdoor aerial\nview encompasses vast areas, making data collection more challenging, which\nresults in a lack of benchmarks. To address this problem, we propose OpenFly, a\nplatform comprising a versatile toolchain and large-scale benchmark for aerial\nVLN. Firstly, we develop a highly automated toolchain for data collection,\nenabling automatic point cloud acquisition, scene semantic segmentation, flight\ntrajectory creation, and instruction generation. Secondly, based on the\ntoolchain, we construct a large-scale aerial VLN dataset with 100k\ntrajectories, covering diverse heights and lengths across 18 scenes. The\ncorresponding visual data are generated using various rendering engines and\nadvanced techniques, including Unreal Engine, GTA V, Google Earth, and 3D\nGaussian Splatting (3D GS). All data exhibit high visual quality. Particularly,\n3D GS supports real-to-sim rendering, further enhancing the realism of the\ndataset. Thirdly, we propose OpenFly-Agent, a keyframe-aware VLN model, which\ntakes language instructions, current observations, and historical keyframes as\ninput, and outputs flight actions directly. Extensive analyses and experiments\nare conducted, showcasing the superiority of our OpenFly platform and\nOpenFly-Agent. The toolchain, dataset, and codes will be open-sourced.\n", "link": "http://arxiv.org/abs/2502.18041v1", "date": "2025-02-25", "relevancy": 2.8495, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5782}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5782}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenFly%3A%20A%20Versatile%20Toolchain%20and%20Large-scale%20Benchmark%20for%20Aerial%0A%20%20Vision-Language%20Navigation&body=Title%3A%20OpenFly%3A%20A%20Versatile%20Toolchain%20and%20Large-scale%20Benchmark%20for%20Aerial%0A%20%20Vision-Language%20Navigation%0AAuthor%3A%20Yunpeng%20Gao%20and%20Chenhui%20Li%20and%20Zhongrui%20You%20and%20Junli%20Liu%20and%20Zhen%20Li%20and%20Pengan%20Chen%20and%20Qizhi%20Chen%20and%20Zhonghan%20Tang%20and%20Liansheng%20Wang%20and%20Penghui%20Yang%20and%20Yiwen%20Tang%20and%20Yuhang%20Tang%20and%20Shuai%20Liang%20and%20Songyi%20Zhu%20and%20Ziqin%20Xiong%20and%20Yifei%20Su%20and%20Xinyi%20Ye%20and%20Jianan%20Li%20and%20Yan%20Ding%20and%20Dong%20Wang%20and%20Zhigang%20Wang%20and%20Bin%20Zhao%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Vision-Language%20Navigation%20%28VLN%29%20aims%20to%20guide%20agents%20through%20an%20environment%0Aby%20leveraging%20both%20language%20instructions%20and%20visual%20cues%2C%20playing%20a%20pivotal%0Arole%20in%20embodied%20AI.%20Indoor%20VLN%20has%20been%20extensively%20studied%2C%20whereas%20outdoor%0Aaerial%20VLN%20remains%20underexplored.%20The%20potential%20reason%20is%20that%20outdoor%20aerial%0Aview%20encompasses%20vast%20areas%2C%20making%20data%20collection%20more%20challenging%2C%20which%0Aresults%20in%20a%20lack%20of%20benchmarks.%20To%20address%20this%20problem%2C%20we%20propose%20OpenFly%2C%20a%0Aplatform%20comprising%20a%20versatile%20toolchain%20and%20large-scale%20benchmark%20for%20aerial%0AVLN.%20Firstly%2C%20we%20develop%20a%20highly%20automated%20toolchain%20for%20data%20collection%2C%0Aenabling%20automatic%20point%20cloud%20acquisition%2C%20scene%20semantic%20segmentation%2C%20flight%0Atrajectory%20creation%2C%20and%20instruction%20generation.%20Secondly%2C%20based%20on%20the%0Atoolchain%2C%20we%20construct%20a%20large-scale%20aerial%20VLN%20dataset%20with%20100k%0Atrajectories%2C%20covering%20diverse%20heights%20and%20lengths%20across%2018%20scenes.%20The%0Acorresponding%20visual%20data%20are%20generated%20using%20various%20rendering%20engines%20and%0Aadvanced%20techniques%2C%20including%20Unreal%20Engine%2C%20GTA%20V%2C%20Google%20Earth%2C%20and%203D%0AGaussian%20Splatting%20%283D%20GS%29.%20All%20data%20exhibit%20high%20visual%20quality.%20Particularly%2C%0A3D%20GS%20supports%20real-to-sim%20rendering%2C%20further%20enhancing%20the%20realism%20of%20the%0Adataset.%20Thirdly%2C%20we%20propose%20OpenFly-Agent%2C%20a%20keyframe-aware%20VLN%20model%2C%20which%0Atakes%20language%20instructions%2C%20current%20observations%2C%20and%20historical%20keyframes%20as%0Ainput%2C%20and%20outputs%20flight%20actions%20directly.%20Extensive%20analyses%20and%20experiments%0Aare%20conducted%2C%20showcasing%20the%20superiority%20of%20our%20OpenFly%20platform%20and%0AOpenFly-Agent.%20The%20toolchain%2C%20dataset%2C%20and%20codes%20will%20be%20open-sourced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenFly%253A%2520A%2520Versatile%2520Toolchain%2520and%2520Large-scale%2520Benchmark%2520for%2520Aerial%250A%2520%2520Vision-Language%2520Navigation%26entry.906535625%3DYunpeng%2520Gao%2520and%2520Chenhui%2520Li%2520and%2520Zhongrui%2520You%2520and%2520Junli%2520Liu%2520and%2520Zhen%2520Li%2520and%2520Pengan%2520Chen%2520and%2520Qizhi%2520Chen%2520and%2520Zhonghan%2520Tang%2520and%2520Liansheng%2520Wang%2520and%2520Penghui%2520Yang%2520and%2520Yiwen%2520Tang%2520and%2520Yuhang%2520Tang%2520and%2520Shuai%2520Liang%2520and%2520Songyi%2520Zhu%2520and%2520Ziqin%2520Xiong%2520and%2520Yifei%2520Su%2520and%2520Xinyi%2520Ye%2520and%2520Jianan%2520Li%2520and%2520Yan%2520Ding%2520and%2520Dong%2520Wang%2520and%2520Zhigang%2520Wang%2520and%2520Bin%2520Zhao%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520Vision-Language%2520Navigation%2520%2528VLN%2529%2520aims%2520to%2520guide%2520agents%2520through%2520an%2520environment%250Aby%2520leveraging%2520both%2520language%2520instructions%2520and%2520visual%2520cues%252C%2520playing%2520a%2520pivotal%250Arole%2520in%2520embodied%2520AI.%2520Indoor%2520VLN%2520has%2520been%2520extensively%2520studied%252C%2520whereas%2520outdoor%250Aaerial%2520VLN%2520remains%2520underexplored.%2520The%2520potential%2520reason%2520is%2520that%2520outdoor%2520aerial%250Aview%2520encompasses%2520vast%2520areas%252C%2520making%2520data%2520collection%2520more%2520challenging%252C%2520which%250Aresults%2520in%2520a%2520lack%2520of%2520benchmarks.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520OpenFly%252C%2520a%250Aplatform%2520comprising%2520a%2520versatile%2520toolchain%2520and%2520large-scale%2520benchmark%2520for%2520aerial%250AVLN.%2520Firstly%252C%2520we%2520develop%2520a%2520highly%2520automated%2520toolchain%2520for%2520data%2520collection%252C%250Aenabling%2520automatic%2520point%2520cloud%2520acquisition%252C%2520scene%2520semantic%2520segmentation%252C%2520flight%250Atrajectory%2520creation%252C%2520and%2520instruction%2520generation.%2520Secondly%252C%2520based%2520on%2520the%250Atoolchain%252C%2520we%2520construct%2520a%2520large-scale%2520aerial%2520VLN%2520dataset%2520with%2520100k%250Atrajectories%252C%2520covering%2520diverse%2520heights%2520and%2520lengths%2520across%252018%2520scenes.%2520The%250Acorresponding%2520visual%2520data%2520are%2520generated%2520using%2520various%2520rendering%2520engines%2520and%250Aadvanced%2520techniques%252C%2520including%2520Unreal%2520Engine%252C%2520GTA%2520V%252C%2520Google%2520Earth%252C%2520and%25203D%250AGaussian%2520Splatting%2520%25283D%2520GS%2529.%2520All%2520data%2520exhibit%2520high%2520visual%2520quality.%2520Particularly%252C%250A3D%2520GS%2520supports%2520real-to-sim%2520rendering%252C%2520further%2520enhancing%2520the%2520realism%2520of%2520the%250Adataset.%2520Thirdly%252C%2520we%2520propose%2520OpenFly-Agent%252C%2520a%2520keyframe-aware%2520VLN%2520model%252C%2520which%250Atakes%2520language%2520instructions%252C%2520current%2520observations%252C%2520and%2520historical%2520keyframes%2520as%250Ainput%252C%2520and%2520outputs%2520flight%2520actions%2520directly.%2520Extensive%2520analyses%2520and%2520experiments%250Aare%2520conducted%252C%2520showcasing%2520the%2520superiority%2520of%2520our%2520OpenFly%2520platform%2520and%250AOpenFly-Agent.%2520The%2520toolchain%252C%2520dataset%252C%2520and%2520codes%2520will%2520be%2520open-sourced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenFly%3A%20A%20Versatile%20Toolchain%20and%20Large-scale%20Benchmark%20for%20Aerial%0A%20%20Vision-Language%20Navigation&entry.906535625=Yunpeng%20Gao%20and%20Chenhui%20Li%20and%20Zhongrui%20You%20and%20Junli%20Liu%20and%20Zhen%20Li%20and%20Pengan%20Chen%20and%20Qizhi%20Chen%20and%20Zhonghan%20Tang%20and%20Liansheng%20Wang%20and%20Penghui%20Yang%20and%20Yiwen%20Tang%20and%20Yuhang%20Tang%20and%20Shuai%20Liang%20and%20Songyi%20Zhu%20and%20Ziqin%20Xiong%20and%20Yifei%20Su%20and%20Xinyi%20Ye%20and%20Jianan%20Li%20and%20Yan%20Ding%20and%20Dong%20Wang%20and%20Zhigang%20Wang%20and%20Bin%20Zhao%20and%20Xuelong%20Li&entry.1292438233=%20%20Vision-Language%20Navigation%20%28VLN%29%20aims%20to%20guide%20agents%20through%20an%20environment%0Aby%20leveraging%20both%20language%20instructions%20and%20visual%20cues%2C%20playing%20a%20pivotal%0Arole%20in%20embodied%20AI.%20Indoor%20VLN%20has%20been%20extensively%20studied%2C%20whereas%20outdoor%0Aaerial%20VLN%20remains%20underexplored.%20The%20potential%20reason%20is%20that%20outdoor%20aerial%0Aview%20encompasses%20vast%20areas%2C%20making%20data%20collection%20more%20challenging%2C%20which%0Aresults%20in%20a%20lack%20of%20benchmarks.%20To%20address%20this%20problem%2C%20we%20propose%20OpenFly%2C%20a%0Aplatform%20comprising%20a%20versatile%20toolchain%20and%20large-scale%20benchmark%20for%20aerial%0AVLN.%20Firstly%2C%20we%20develop%20a%20highly%20automated%20toolchain%20for%20data%20collection%2C%0Aenabling%20automatic%20point%20cloud%20acquisition%2C%20scene%20semantic%20segmentation%2C%20flight%0Atrajectory%20creation%2C%20and%20instruction%20generation.%20Secondly%2C%20based%20on%20the%0Atoolchain%2C%20we%20construct%20a%20large-scale%20aerial%20VLN%20dataset%20with%20100k%0Atrajectories%2C%20covering%20diverse%20heights%20and%20lengths%20across%2018%20scenes.%20The%0Acorresponding%20visual%20data%20are%20generated%20using%20various%20rendering%20engines%20and%0Aadvanced%20techniques%2C%20including%20Unreal%20Engine%2C%20GTA%20V%2C%20Google%20Earth%2C%20and%203D%0AGaussian%20Splatting%20%283D%20GS%29.%20All%20data%20exhibit%20high%20visual%20quality.%20Particularly%2C%0A3D%20GS%20supports%20real-to-sim%20rendering%2C%20further%20enhancing%20the%20realism%20of%20the%0Adataset.%20Thirdly%2C%20we%20propose%20OpenFly-Agent%2C%20a%20keyframe-aware%20VLN%20model%2C%20which%0Atakes%20language%20instructions%2C%20current%20observations%2C%20and%20historical%20keyframes%20as%0Ainput%2C%20and%20outputs%20flight%20actions%20directly.%20Extensive%20analyses%20and%20experiments%0Aare%20conducted%2C%20showcasing%20the%20superiority%20of%20our%20OpenFly%20platform%20and%0AOpenFly-Agent.%20The%20toolchain%2C%20dataset%2C%20and%20codes%20will%20be%20open-sourced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18041v1&entry.124074799=Read"},
{"title": "Progressive Local Alignment for Medical Multimodal Pre-training", "author": "Huimin Yan and Xian Yang and Liang Bai and Jiye Liang", "abstract": "  Local alignment between medical images and text is essential for accurate\ndiagnosis, though it remains challenging due to the absence of natural local\npairings and the limitations of rigid region recognition methods. Traditional\napproaches rely on hard boundaries, which introduce uncertainty, whereas\nmedical imaging demands flexible soft region recognition to handle irregular\nstructures. To overcome these challenges, we propose the Progressive Local\nAlignment Network (PLAN), which designs a novel contrastive learning-based\napproach for local alignment to establish meaningful word-pixel relationships\nand introduces a progressive learning strategy to iteratively refine these\nrelationships, enhancing alignment precision and robustness. By combining these\ntechniques, PLAN effectively improves soft region recognition while suppressing\nnoise interference. Extensive experiments on multiple medical datasets\ndemonstrate that PLAN surpasses state-of-the-art methods in phrase grounding,\nimage-text retrieval, object detection, and zero-shot classification, setting a\nnew benchmark for medical image-text alignment.\n", "link": "http://arxiv.org/abs/2502.18047v1", "date": "2025-02-25", "relevancy": 2.7672, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5775}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5633}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20Local%20Alignment%20for%20Medical%20Multimodal%20Pre-training&body=Title%3A%20Progressive%20Local%20Alignment%20for%20Medical%20Multimodal%20Pre-training%0AAuthor%3A%20Huimin%20Yan%20and%20Xian%20Yang%20and%20Liang%20Bai%20and%20Jiye%20Liang%0AAbstract%3A%20%20%20Local%20alignment%20between%20medical%20images%20and%20text%20is%20essential%20for%20accurate%0Adiagnosis%2C%20though%20it%20remains%20challenging%20due%20to%20the%20absence%20of%20natural%20local%0Apairings%20and%20the%20limitations%20of%20rigid%20region%20recognition%20methods.%20Traditional%0Aapproaches%20rely%20on%20hard%20boundaries%2C%20which%20introduce%20uncertainty%2C%20whereas%0Amedical%20imaging%20demands%20flexible%20soft%20region%20recognition%20to%20handle%20irregular%0Astructures.%20To%20overcome%20these%20challenges%2C%20we%20propose%20the%20Progressive%20Local%0AAlignment%20Network%20%28PLAN%29%2C%20which%20designs%20a%20novel%20contrastive%20learning-based%0Aapproach%20for%20local%20alignment%20to%20establish%20meaningful%20word-pixel%20relationships%0Aand%20introduces%20a%20progressive%20learning%20strategy%20to%20iteratively%20refine%20these%0Arelationships%2C%20enhancing%20alignment%20precision%20and%20robustness.%20By%20combining%20these%0Atechniques%2C%20PLAN%20effectively%20improves%20soft%20region%20recognition%20while%20suppressing%0Anoise%20interference.%20Extensive%20experiments%20on%20multiple%20medical%20datasets%0Ademonstrate%20that%20PLAN%20surpasses%20state-of-the-art%20methods%20in%20phrase%20grounding%2C%0Aimage-text%20retrieval%2C%20object%20detection%2C%20and%20zero-shot%20classification%2C%20setting%20a%0Anew%20benchmark%20for%20medical%20image-text%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520Local%2520Alignment%2520for%2520Medical%2520Multimodal%2520Pre-training%26entry.906535625%3DHuimin%2520Yan%2520and%2520Xian%2520Yang%2520and%2520Liang%2520Bai%2520and%2520Jiye%2520Liang%26entry.1292438233%3D%2520%2520Local%2520alignment%2520between%2520medical%2520images%2520and%2520text%2520is%2520essential%2520for%2520accurate%250Adiagnosis%252C%2520though%2520it%2520remains%2520challenging%2520due%2520to%2520the%2520absence%2520of%2520natural%2520local%250Apairings%2520and%2520the%2520limitations%2520of%2520rigid%2520region%2520recognition%2520methods.%2520Traditional%250Aapproaches%2520rely%2520on%2520hard%2520boundaries%252C%2520which%2520introduce%2520uncertainty%252C%2520whereas%250Amedical%2520imaging%2520demands%2520flexible%2520soft%2520region%2520recognition%2520to%2520handle%2520irregular%250Astructures.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520the%2520Progressive%2520Local%250AAlignment%2520Network%2520%2528PLAN%2529%252C%2520which%2520designs%2520a%2520novel%2520contrastive%2520learning-based%250Aapproach%2520for%2520local%2520alignment%2520to%2520establish%2520meaningful%2520word-pixel%2520relationships%250Aand%2520introduces%2520a%2520progressive%2520learning%2520strategy%2520to%2520iteratively%2520refine%2520these%250Arelationships%252C%2520enhancing%2520alignment%2520precision%2520and%2520robustness.%2520By%2520combining%2520these%250Atechniques%252C%2520PLAN%2520effectively%2520improves%2520soft%2520region%2520recognition%2520while%2520suppressing%250Anoise%2520interference.%2520Extensive%2520experiments%2520on%2520multiple%2520medical%2520datasets%250Ademonstrate%2520that%2520PLAN%2520surpasses%2520state-of-the-art%2520methods%2520in%2520phrase%2520grounding%252C%250Aimage-text%2520retrieval%252C%2520object%2520detection%252C%2520and%2520zero-shot%2520classification%252C%2520setting%2520a%250Anew%2520benchmark%2520for%2520medical%2520image-text%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Local%20Alignment%20for%20Medical%20Multimodal%20Pre-training&entry.906535625=Huimin%20Yan%20and%20Xian%20Yang%20and%20Liang%20Bai%20and%20Jiye%20Liang&entry.1292438233=%20%20Local%20alignment%20between%20medical%20images%20and%20text%20is%20essential%20for%20accurate%0Adiagnosis%2C%20though%20it%20remains%20challenging%20due%20to%20the%20absence%20of%20natural%20local%0Apairings%20and%20the%20limitations%20of%20rigid%20region%20recognition%20methods.%20Traditional%0Aapproaches%20rely%20on%20hard%20boundaries%2C%20which%20introduce%20uncertainty%2C%20whereas%0Amedical%20imaging%20demands%20flexible%20soft%20region%20recognition%20to%20handle%20irregular%0Astructures.%20To%20overcome%20these%20challenges%2C%20we%20propose%20the%20Progressive%20Local%0AAlignment%20Network%20%28PLAN%29%2C%20which%20designs%20a%20novel%20contrastive%20learning-based%0Aapproach%20for%20local%20alignment%20to%20establish%20meaningful%20word-pixel%20relationships%0Aand%20introduces%20a%20progressive%20learning%20strategy%20to%20iteratively%20refine%20these%0Arelationships%2C%20enhancing%20alignment%20precision%20and%20robustness.%20By%20combining%20these%0Atechniques%2C%20PLAN%20effectively%20improves%20soft%20region%20recognition%20while%20suppressing%0Anoise%20interference.%20Extensive%20experiments%20on%20multiple%20medical%20datasets%0Ademonstrate%20that%20PLAN%20surpasses%20state-of-the-art%20methods%20in%20phrase%20grounding%2C%0Aimage-text%20retrieval%2C%20object%20detection%2C%20and%20zero-shot%20classification%2C%20setting%20a%0Anew%20benchmark%20for%20medical%20image-text%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18047v1&entry.124074799=Read"},
{"title": "OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference", "author": "Xiangyu Zhao and Shengyuan Ding and Zicheng Zhang and Haian Huang and Maosong Cao and Weiyun Wang and Jiaqi Wang and Xinyu Fang and Wenhai Wang and Guangtao Zhai and Haodong Duan and Hua Yang and Kai Chen", "abstract": "  Recent advancements in open-source multi-modal large language models (MLLMs)\nhave primarily focused on enhancing foundational capabilities, leaving a\nsignificant gap in human preference alignment. This paper introduces\nOmniAlign-V, a comprehensive dataset of 200K high-quality training samples\nfeaturing diverse images, complex questions, and varied response formats to\nimprove MLLMs' alignment with human preferences. We also present MM-AlignBench,\na human-annotated benchmark specifically designed to evaluate MLLMs' alignment\nwith human values. Experimental results show that finetuning MLLMs with\nOmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference\nOptimization (DPO), significantly enhances human preference alignment while\nmaintaining or enhancing performance on standard VQA benchmarks, preserving\ntheir fundamental capabilities. Our datasets, benchmark, code and checkpoints\nhave been released at https://github.com/PhoenixZ810/OmniAlign-V.\n", "link": "http://arxiv.org/abs/2502.18411v1", "date": "2025-02-25", "relevancy": 2.7611, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5686}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniAlign-V%3A%20Towards%20Enhanced%20Alignment%20of%20MLLMs%20with%20Human%20Preference&body=Title%3A%20OmniAlign-V%3A%20Towards%20Enhanced%20Alignment%20of%20MLLMs%20with%20Human%20Preference%0AAuthor%3A%20Xiangyu%20Zhao%20and%20Shengyuan%20Ding%20and%20Zicheng%20Zhang%20and%20Haian%20Huang%20and%20Maosong%20Cao%20and%20Weiyun%20Wang%20and%20Jiaqi%20Wang%20and%20Xinyu%20Fang%20and%20Wenhai%20Wang%20and%20Guangtao%20Zhai%20and%20Haodong%20Duan%20and%20Hua%20Yang%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Recent%20advancements%20in%20open-source%20multi-modal%20large%20language%20models%20%28MLLMs%29%0Ahave%20primarily%20focused%20on%20enhancing%20foundational%20capabilities%2C%20leaving%20a%0Asignificant%20gap%20in%20human%20preference%20alignment.%20This%20paper%20introduces%0AOmniAlign-V%2C%20a%20comprehensive%20dataset%20of%20200K%20high-quality%20training%20samples%0Afeaturing%20diverse%20images%2C%20complex%20questions%2C%20and%20varied%20response%20formats%20to%0Aimprove%20MLLMs%27%20alignment%20with%20human%20preferences.%20We%20also%20present%20MM-AlignBench%2C%0Aa%20human-annotated%20benchmark%20specifically%20designed%20to%20evaluate%20MLLMs%27%20alignment%0Awith%20human%20values.%20Experimental%20results%20show%20that%20finetuning%20MLLMs%20with%0AOmniAlign-V%2C%20using%20Supervised%20Fine-Tuning%20%28SFT%29%20or%20Direct%20Preference%0AOptimization%20%28DPO%29%2C%20significantly%20enhances%20human%20preference%20alignment%20while%0Amaintaining%20or%20enhancing%20performance%20on%20standard%20VQA%20benchmarks%2C%20preserving%0Atheir%20fundamental%20capabilities.%20Our%20datasets%2C%20benchmark%2C%20code%20and%20checkpoints%0Ahave%20been%20released%20at%20https%3A//github.com/PhoenixZ810/OmniAlign-V.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniAlign-V%253A%2520Towards%2520Enhanced%2520Alignment%2520of%2520MLLMs%2520with%2520Human%2520Preference%26entry.906535625%3DXiangyu%2520Zhao%2520and%2520Shengyuan%2520Ding%2520and%2520Zicheng%2520Zhang%2520and%2520Haian%2520Huang%2520and%2520Maosong%2520Cao%2520and%2520Weiyun%2520Wang%2520and%2520Jiaqi%2520Wang%2520and%2520Xinyu%2520Fang%2520and%2520Wenhai%2520Wang%2520and%2520Guangtao%2520Zhai%2520and%2520Haodong%2520Duan%2520and%2520Hua%2520Yang%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520open-source%2520multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%250Ahave%2520primarily%2520focused%2520on%2520enhancing%2520foundational%2520capabilities%252C%2520leaving%2520a%250Asignificant%2520gap%2520in%2520human%2520preference%2520alignment.%2520This%2520paper%2520introduces%250AOmniAlign-V%252C%2520a%2520comprehensive%2520dataset%2520of%2520200K%2520high-quality%2520training%2520samples%250Afeaturing%2520diverse%2520images%252C%2520complex%2520questions%252C%2520and%2520varied%2520response%2520formats%2520to%250Aimprove%2520MLLMs%2527%2520alignment%2520with%2520human%2520preferences.%2520We%2520also%2520present%2520MM-AlignBench%252C%250Aa%2520human-annotated%2520benchmark%2520specifically%2520designed%2520to%2520evaluate%2520MLLMs%2527%2520alignment%250Awith%2520human%2520values.%2520Experimental%2520results%2520show%2520that%2520finetuning%2520MLLMs%2520with%250AOmniAlign-V%252C%2520using%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520or%2520Direct%2520Preference%250AOptimization%2520%2528DPO%2529%252C%2520significantly%2520enhances%2520human%2520preference%2520alignment%2520while%250Amaintaining%2520or%2520enhancing%2520performance%2520on%2520standard%2520VQA%2520benchmarks%252C%2520preserving%250Atheir%2520fundamental%2520capabilities.%2520Our%2520datasets%252C%2520benchmark%252C%2520code%2520and%2520checkpoints%250Ahave%2520been%2520released%2520at%2520https%253A//github.com/PhoenixZ810/OmniAlign-V.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniAlign-V%3A%20Towards%20Enhanced%20Alignment%20of%20MLLMs%20with%20Human%20Preference&entry.906535625=Xiangyu%20Zhao%20and%20Shengyuan%20Ding%20and%20Zicheng%20Zhang%20and%20Haian%20Huang%20and%20Maosong%20Cao%20and%20Weiyun%20Wang%20and%20Jiaqi%20Wang%20and%20Xinyu%20Fang%20and%20Wenhai%20Wang%20and%20Guangtao%20Zhai%20and%20Haodong%20Duan%20and%20Hua%20Yang%20and%20Kai%20Chen&entry.1292438233=%20%20Recent%20advancements%20in%20open-source%20multi-modal%20large%20language%20models%20%28MLLMs%29%0Ahave%20primarily%20focused%20on%20enhancing%20foundational%20capabilities%2C%20leaving%20a%0Asignificant%20gap%20in%20human%20preference%20alignment.%20This%20paper%20introduces%0AOmniAlign-V%2C%20a%20comprehensive%20dataset%20of%20200K%20high-quality%20training%20samples%0Afeaturing%20diverse%20images%2C%20complex%20questions%2C%20and%20varied%20response%20formats%20to%0Aimprove%20MLLMs%27%20alignment%20with%20human%20preferences.%20We%20also%20present%20MM-AlignBench%2C%0Aa%20human-annotated%20benchmark%20specifically%20designed%20to%20evaluate%20MLLMs%27%20alignment%0Awith%20human%20values.%20Experimental%20results%20show%20that%20finetuning%20MLLMs%20with%0AOmniAlign-V%2C%20using%20Supervised%20Fine-Tuning%20%28SFT%29%20or%20Direct%20Preference%0AOptimization%20%28DPO%29%2C%20significantly%20enhances%20human%20preference%20alignment%20while%0Amaintaining%20or%20enhancing%20performance%20on%20standard%20VQA%20benchmarks%2C%20preserving%0Atheir%20fundamental%20capabilities.%20Our%20datasets%2C%20benchmark%2C%20code%20and%20checkpoints%0Ahave%20been%20released%20at%20https%3A//github.com/PhoenixZ810/OmniAlign-V.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18411v1&entry.124074799=Read"},
{"title": "Problem Solved? Information Extraction Design Space for Layout-Rich\n  Documents using LLMs", "author": "Gaye Colakoglu and G\u00fcrkan Solmaz and Jonathan F\u00fcrst", "abstract": "  This paper defines and explores the design space for information extraction\n(IE) from layout-rich documents using large language models (LLMs). The three\ncore challenges of layout-aware IE with LLMs are 1) data structuring, 2) model\nengagement, and 3) output refinement. Our study delves into the sub-problems\nwithin these core challenges, such as input representation, chunking,\nprompting, and selection of LLMs and multimodal models. It examines the\noutcomes of different design choices through a new layout-aware IE test suite,\nbenchmarking against the state-of-art (SoA) model LayoutLMv3. The results show\nthat the configuration from one-factor-at-a-time (OFAT) trial achieves\nnear-optimal results with 14.1 points F1-score gain from the baseline model,\nwhile full factorial exploration yields only a slightly higher 15.1 points gain\nat around 36x greater token usage. We demonstrate that well-configured\ngeneral-purpose LLMs can match the performance of specialized models, providing\na cost-effective alternative. Our test-suite is freely available at\nhttps://github.com/gayecolakoglu/LayIE-LLM.\n", "link": "http://arxiv.org/abs/2502.18179v1", "date": "2025-02-25", "relevancy": 2.7269, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5829}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Problem%20Solved%3F%20Information%20Extraction%20Design%20Space%20for%20Layout-Rich%0A%20%20Documents%20using%20LLMs&body=Title%3A%20Problem%20Solved%3F%20Information%20Extraction%20Design%20Space%20for%20Layout-Rich%0A%20%20Documents%20using%20LLMs%0AAuthor%3A%20Gaye%20Colakoglu%20and%20G%C3%BCrkan%20Solmaz%20and%20Jonathan%20F%C3%BCrst%0AAbstract%3A%20%20%20This%20paper%20defines%20and%20explores%20the%20design%20space%20for%20information%20extraction%0A%28IE%29%20from%20layout-rich%20documents%20using%20large%20language%20models%20%28LLMs%29.%20The%20three%0Acore%20challenges%20of%20layout-aware%20IE%20with%20LLMs%20are%201%29%20data%20structuring%2C%202%29%20model%0Aengagement%2C%20and%203%29%20output%20refinement.%20Our%20study%20delves%20into%20the%20sub-problems%0Awithin%20these%20core%20challenges%2C%20such%20as%20input%20representation%2C%20chunking%2C%0Aprompting%2C%20and%20selection%20of%20LLMs%20and%20multimodal%20models.%20It%20examines%20the%0Aoutcomes%20of%20different%20design%20choices%20through%20a%20new%20layout-aware%20IE%20test%20suite%2C%0Abenchmarking%20against%20the%20state-of-art%20%28SoA%29%20model%20LayoutLMv3.%20The%20results%20show%0Athat%20the%20configuration%20from%20one-factor-at-a-time%20%28OFAT%29%20trial%20achieves%0Anear-optimal%20results%20with%2014.1%20points%20F1-score%20gain%20from%20the%20baseline%20model%2C%0Awhile%20full%20factorial%20exploration%20yields%20only%20a%20slightly%20higher%2015.1%20points%20gain%0Aat%20around%2036x%20greater%20token%20usage.%20We%20demonstrate%20that%20well-configured%0Ageneral-purpose%20LLMs%20can%20match%20the%20performance%20of%20specialized%20models%2C%20providing%0Aa%20cost-effective%20alternative.%20Our%20test-suite%20is%20freely%20available%20at%0Ahttps%3A//github.com/gayecolakoglu/LayIE-LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProblem%2520Solved%253F%2520Information%2520Extraction%2520Design%2520Space%2520for%2520Layout-Rich%250A%2520%2520Documents%2520using%2520LLMs%26entry.906535625%3DGaye%2520Colakoglu%2520and%2520G%25C3%25BCrkan%2520Solmaz%2520and%2520Jonathan%2520F%25C3%25BCrst%26entry.1292438233%3D%2520%2520This%2520paper%2520defines%2520and%2520explores%2520the%2520design%2520space%2520for%2520information%2520extraction%250A%2528IE%2529%2520from%2520layout-rich%2520documents%2520using%2520large%2520language%2520models%2520%2528LLMs%2529.%2520The%2520three%250Acore%2520challenges%2520of%2520layout-aware%2520IE%2520with%2520LLMs%2520are%25201%2529%2520data%2520structuring%252C%25202%2529%2520model%250Aengagement%252C%2520and%25203%2529%2520output%2520refinement.%2520Our%2520study%2520delves%2520into%2520the%2520sub-problems%250Awithin%2520these%2520core%2520challenges%252C%2520such%2520as%2520input%2520representation%252C%2520chunking%252C%250Aprompting%252C%2520and%2520selection%2520of%2520LLMs%2520and%2520multimodal%2520models.%2520It%2520examines%2520the%250Aoutcomes%2520of%2520different%2520design%2520choices%2520through%2520a%2520new%2520layout-aware%2520IE%2520test%2520suite%252C%250Abenchmarking%2520against%2520the%2520state-of-art%2520%2528SoA%2529%2520model%2520LayoutLMv3.%2520The%2520results%2520show%250Athat%2520the%2520configuration%2520from%2520one-factor-at-a-time%2520%2528OFAT%2529%2520trial%2520achieves%250Anear-optimal%2520results%2520with%252014.1%2520points%2520F1-score%2520gain%2520from%2520the%2520baseline%2520model%252C%250Awhile%2520full%2520factorial%2520exploration%2520yields%2520only%2520a%2520slightly%2520higher%252015.1%2520points%2520gain%250Aat%2520around%252036x%2520greater%2520token%2520usage.%2520We%2520demonstrate%2520that%2520well-configured%250Ageneral-purpose%2520LLMs%2520can%2520match%2520the%2520performance%2520of%2520specialized%2520models%252C%2520providing%250Aa%2520cost-effective%2520alternative.%2520Our%2520test-suite%2520is%2520freely%2520available%2520at%250Ahttps%253A//github.com/gayecolakoglu/LayIE-LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Problem%20Solved%3F%20Information%20Extraction%20Design%20Space%20for%20Layout-Rich%0A%20%20Documents%20using%20LLMs&entry.906535625=Gaye%20Colakoglu%20and%20G%C3%BCrkan%20Solmaz%20and%20Jonathan%20F%C3%BCrst&entry.1292438233=%20%20This%20paper%20defines%20and%20explores%20the%20design%20space%20for%20information%20extraction%0A%28IE%29%20from%20layout-rich%20documents%20using%20large%20language%20models%20%28LLMs%29.%20The%20three%0Acore%20challenges%20of%20layout-aware%20IE%20with%20LLMs%20are%201%29%20data%20structuring%2C%202%29%20model%0Aengagement%2C%20and%203%29%20output%20refinement.%20Our%20study%20delves%20into%20the%20sub-problems%0Awithin%20these%20core%20challenges%2C%20such%20as%20input%20representation%2C%20chunking%2C%0Aprompting%2C%20and%20selection%20of%20LLMs%20and%20multimodal%20models.%20It%20examines%20the%0Aoutcomes%20of%20different%20design%20choices%20through%20a%20new%20layout-aware%20IE%20test%20suite%2C%0Abenchmarking%20against%20the%20state-of-art%20%28SoA%29%20model%20LayoutLMv3.%20The%20results%20show%0Athat%20the%20configuration%20from%20one-factor-at-a-time%20%28OFAT%29%20trial%20achieves%0Anear-optimal%20results%20with%2014.1%20points%20F1-score%20gain%20from%20the%20baseline%20model%2C%0Awhile%20full%20factorial%20exploration%20yields%20only%20a%20slightly%20higher%2015.1%20points%20gain%0Aat%20around%2036x%20greater%20token%20usage.%20We%20demonstrate%20that%20well-configured%0Ageneral-purpose%20LLMs%20can%20match%20the%20performance%20of%20specialized%20models%2C%20providing%0Aa%20cost-effective%20alternative.%20Our%20test-suite%20is%20freely%20available%20at%0Ahttps%3A//github.com/gayecolakoglu/LayIE-LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18179v1&entry.124074799=Read"},
{"title": "MegaLoc: One Retrieval to Place Them All", "author": "Gabriele Berton and Carlo Masone", "abstract": "  Retrieving images from the same location as a given query is an important\ncomponent of multiple computer vision tasks, like Visual Place Recognition,\nLandmark Retrieval, Visual Localization, 3D reconstruction, and SLAM. However,\nexisting solutions are built to specifically work for one of these tasks, and\nare known to fail when the requirements slightly change or when they meet\nout-of-distribution data. In this paper we combine a variety of existing\nmethods, training techniques, and datasets to train a retrieval model, called\nMegaLoc, that is performant on multiple tasks. We find that MegaLoc (1)\nachieves state of the art on a large number of Visual Place Recognition\ndatasets, (2) impressive results on common Landmark Retrieval datasets, and (3)\nsets a new state of the art for Visual Localization on the LaMAR datasets,\nwhere we only changed the retrieval method to the existing localization\npipeline. The code for MegaLoc is available at\nhttps://github.com/gmberton/MegaLoc\n", "link": "http://arxiv.org/abs/2502.17237v2", "date": "2025-02-25", "relevancy": 2.7266, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5766}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5579}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MegaLoc%3A%20One%20Retrieval%20to%20Place%20Them%20All&body=Title%3A%20MegaLoc%3A%20One%20Retrieval%20to%20Place%20Them%20All%0AAuthor%3A%20Gabriele%20Berton%20and%20Carlo%20Masone%0AAbstract%3A%20%20%20Retrieving%20images%20from%20the%20same%20location%20as%20a%20given%20query%20is%20an%20important%0Acomponent%20of%20multiple%20computer%20vision%20tasks%2C%20like%20Visual%20Place%20Recognition%2C%0ALandmark%20Retrieval%2C%20Visual%20Localization%2C%203D%20reconstruction%2C%20and%20SLAM.%20However%2C%0Aexisting%20solutions%20are%20built%20to%20specifically%20work%20for%20one%20of%20these%20tasks%2C%20and%0Aare%20known%20to%20fail%20when%20the%20requirements%20slightly%20change%20or%20when%20they%20meet%0Aout-of-distribution%20data.%20In%20this%20paper%20we%20combine%20a%20variety%20of%20existing%0Amethods%2C%20training%20techniques%2C%20and%20datasets%20to%20train%20a%20retrieval%20model%2C%20called%0AMegaLoc%2C%20that%20is%20performant%20on%20multiple%20tasks.%20We%20find%20that%20MegaLoc%20%281%29%0Aachieves%20state%20of%20the%20art%20on%20a%20large%20number%20of%20Visual%20Place%20Recognition%0Adatasets%2C%20%282%29%20impressive%20results%20on%20common%20Landmark%20Retrieval%20datasets%2C%20and%20%283%29%0Asets%20a%20new%20state%20of%20the%20art%20for%20Visual%20Localization%20on%20the%20LaMAR%20datasets%2C%0Awhere%20we%20only%20changed%20the%20retrieval%20method%20to%20the%20existing%20localization%0Apipeline.%20The%20code%20for%20MegaLoc%20is%20available%20at%0Ahttps%3A//github.com/gmberton/MegaLoc%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17237v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMegaLoc%253A%2520One%2520Retrieval%2520to%2520Place%2520Them%2520All%26entry.906535625%3DGabriele%2520Berton%2520and%2520Carlo%2520Masone%26entry.1292438233%3D%2520%2520Retrieving%2520images%2520from%2520the%2520same%2520location%2520as%2520a%2520given%2520query%2520is%2520an%2520important%250Acomponent%2520of%2520multiple%2520computer%2520vision%2520tasks%252C%2520like%2520Visual%2520Place%2520Recognition%252C%250ALandmark%2520Retrieval%252C%2520Visual%2520Localization%252C%25203D%2520reconstruction%252C%2520and%2520SLAM.%2520However%252C%250Aexisting%2520solutions%2520are%2520built%2520to%2520specifically%2520work%2520for%2520one%2520of%2520these%2520tasks%252C%2520and%250Aare%2520known%2520to%2520fail%2520when%2520the%2520requirements%2520slightly%2520change%2520or%2520when%2520they%2520meet%250Aout-of-distribution%2520data.%2520In%2520this%2520paper%2520we%2520combine%2520a%2520variety%2520of%2520existing%250Amethods%252C%2520training%2520techniques%252C%2520and%2520datasets%2520to%2520train%2520a%2520retrieval%2520model%252C%2520called%250AMegaLoc%252C%2520that%2520is%2520performant%2520on%2520multiple%2520tasks.%2520We%2520find%2520that%2520MegaLoc%2520%25281%2529%250Aachieves%2520state%2520of%2520the%2520art%2520on%2520a%2520large%2520number%2520of%2520Visual%2520Place%2520Recognition%250Adatasets%252C%2520%25282%2529%2520impressive%2520results%2520on%2520common%2520Landmark%2520Retrieval%2520datasets%252C%2520and%2520%25283%2529%250Asets%2520a%2520new%2520state%2520of%2520the%2520art%2520for%2520Visual%2520Localization%2520on%2520the%2520LaMAR%2520datasets%252C%250Awhere%2520we%2520only%2520changed%2520the%2520retrieval%2520method%2520to%2520the%2520existing%2520localization%250Apipeline.%2520The%2520code%2520for%2520MegaLoc%2520is%2520available%2520at%250Ahttps%253A//github.com/gmberton/MegaLoc%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17237v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MegaLoc%3A%20One%20Retrieval%20to%20Place%20Them%20All&entry.906535625=Gabriele%20Berton%20and%20Carlo%20Masone&entry.1292438233=%20%20Retrieving%20images%20from%20the%20same%20location%20as%20a%20given%20query%20is%20an%20important%0Acomponent%20of%20multiple%20computer%20vision%20tasks%2C%20like%20Visual%20Place%20Recognition%2C%0ALandmark%20Retrieval%2C%20Visual%20Localization%2C%203D%20reconstruction%2C%20and%20SLAM.%20However%2C%0Aexisting%20solutions%20are%20built%20to%20specifically%20work%20for%20one%20of%20these%20tasks%2C%20and%0Aare%20known%20to%20fail%20when%20the%20requirements%20slightly%20change%20or%20when%20they%20meet%0Aout-of-distribution%20data.%20In%20this%20paper%20we%20combine%20a%20variety%20of%20existing%0Amethods%2C%20training%20techniques%2C%20and%20datasets%20to%20train%20a%20retrieval%20model%2C%20called%0AMegaLoc%2C%20that%20is%20performant%20on%20multiple%20tasks.%20We%20find%20that%20MegaLoc%20%281%29%0Aachieves%20state%20of%20the%20art%20on%20a%20large%20number%20of%20Visual%20Place%20Recognition%0Adatasets%2C%20%282%29%20impressive%20results%20on%20common%20Landmark%20Retrieval%20datasets%2C%20and%20%283%29%0Asets%20a%20new%20state%20of%20the%20art%20for%20Visual%20Localization%20on%20the%20LaMAR%20datasets%2C%0Awhere%20we%20only%20changed%20the%20retrieval%20method%20to%20the%20existing%20localization%0Apipeline.%20The%20code%20for%20MegaLoc%20is%20available%20at%0Ahttps%3A//github.com/gmberton/MegaLoc%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17237v2&entry.124074799=Read"},
{"title": "MR-STGN: Multi-Residual Spatio Temporal Graph Network Using Attention\n  Fusion for Patient Action Assessment", "author": "Youssef Mourchid and Rim Slama", "abstract": "  Accurate assessment of patient actions plays a crucial role in healthcare as\nit contributes significantly to disease progression monitoring and treatment\neffectiveness. However, traditional approaches to assess patient actions often\nrely on manual observation and scoring, which are subjective and\ntime-consuming. In this paper, we propose an automated approach for patient\naction assessment using a Multi-Residual Spatio Temporal Graph Network\n(MR-STGN) that incorporates both angular and positional 3D skeletons. The\nMR-STGN is specifically designed to capture the spatio-temporal dynamics of\npatient actions. It achieves this by integrating information from multiple\nresidual layers, with each layer extracting features at distinct levels of\nabstraction. Furthermore, we integrate an attention fusion mechanism into the\nnetwork, which facilitates the adaptive weighting of various features. This\nempowers the model to concentrate on the most pertinent aspects of the\npatient's movements, offering precise instructions regarding specific body\nparts or movements that require attention. Ablation studies are conducted to\nanalyze the impact of individual components within the proposed model. We\nevaluate our model on the UI-PRMD dataset demonstrating its performance in\naccurately predicting real-time patient action scores, surpassing\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2312.13509v2", "date": "2025-02-25", "relevancy": 2.7176, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5644}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5342}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MR-STGN%3A%20Multi-Residual%20Spatio%20Temporal%20Graph%20Network%20Using%20Attention%0A%20%20Fusion%20for%20Patient%20Action%20Assessment&body=Title%3A%20MR-STGN%3A%20Multi-Residual%20Spatio%20Temporal%20Graph%20Network%20Using%20Attention%0A%20%20Fusion%20for%20Patient%20Action%20Assessment%0AAuthor%3A%20Youssef%20Mourchid%20and%20Rim%20Slama%0AAbstract%3A%20%20%20Accurate%20assessment%20of%20patient%20actions%20plays%20a%20crucial%20role%20in%20healthcare%20as%0Ait%20contributes%20significantly%20to%20disease%20progression%20monitoring%20and%20treatment%0Aeffectiveness.%20However%2C%20traditional%20approaches%20to%20assess%20patient%20actions%20often%0Arely%20on%20manual%20observation%20and%20scoring%2C%20which%20are%20subjective%20and%0Atime-consuming.%20In%20this%20paper%2C%20we%20propose%20an%20automated%20approach%20for%20patient%0Aaction%20assessment%20using%20a%20Multi-Residual%20Spatio%20Temporal%20Graph%20Network%0A%28MR-STGN%29%20that%20incorporates%20both%20angular%20and%20positional%203D%20skeletons.%20The%0AMR-STGN%20is%20specifically%20designed%20to%20capture%20the%20spatio-temporal%20dynamics%20of%0Apatient%20actions.%20It%20achieves%20this%20by%20integrating%20information%20from%20multiple%0Aresidual%20layers%2C%20with%20each%20layer%20extracting%20features%20at%20distinct%20levels%20of%0Aabstraction.%20Furthermore%2C%20we%20integrate%20an%20attention%20fusion%20mechanism%20into%20the%0Anetwork%2C%20which%20facilitates%20the%20adaptive%20weighting%20of%20various%20features.%20This%0Aempowers%20the%20model%20to%20concentrate%20on%20the%20most%20pertinent%20aspects%20of%20the%0Apatient%27s%20movements%2C%20offering%20precise%20instructions%20regarding%20specific%20body%0Aparts%20or%20movements%20that%20require%20attention.%20Ablation%20studies%20are%20conducted%20to%0Aanalyze%20the%20impact%20of%20individual%20components%20within%20the%20proposed%20model.%20We%0Aevaluate%20our%20model%20on%20the%20UI-PRMD%20dataset%20demonstrating%20its%20performance%20in%0Aaccurately%20predicting%20real-time%20patient%20action%20scores%2C%20surpassing%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13509v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMR-STGN%253A%2520Multi-Residual%2520Spatio%2520Temporal%2520Graph%2520Network%2520Using%2520Attention%250A%2520%2520Fusion%2520for%2520Patient%2520Action%2520Assessment%26entry.906535625%3DYoussef%2520Mourchid%2520and%2520Rim%2520Slama%26entry.1292438233%3D%2520%2520Accurate%2520assessment%2520of%2520patient%2520actions%2520plays%2520a%2520crucial%2520role%2520in%2520healthcare%2520as%250Ait%2520contributes%2520significantly%2520to%2520disease%2520progression%2520monitoring%2520and%2520treatment%250Aeffectiveness.%2520However%252C%2520traditional%2520approaches%2520to%2520assess%2520patient%2520actions%2520often%250Arely%2520on%2520manual%2520observation%2520and%2520scoring%252C%2520which%2520are%2520subjective%2520and%250Atime-consuming.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520automated%2520approach%2520for%2520patient%250Aaction%2520assessment%2520using%2520a%2520Multi-Residual%2520Spatio%2520Temporal%2520Graph%2520Network%250A%2528MR-STGN%2529%2520that%2520incorporates%2520both%2520angular%2520and%2520positional%25203D%2520skeletons.%2520The%250AMR-STGN%2520is%2520specifically%2520designed%2520to%2520capture%2520the%2520spatio-temporal%2520dynamics%2520of%250Apatient%2520actions.%2520It%2520achieves%2520this%2520by%2520integrating%2520information%2520from%2520multiple%250Aresidual%2520layers%252C%2520with%2520each%2520layer%2520extracting%2520features%2520at%2520distinct%2520levels%2520of%250Aabstraction.%2520Furthermore%252C%2520we%2520integrate%2520an%2520attention%2520fusion%2520mechanism%2520into%2520the%250Anetwork%252C%2520which%2520facilitates%2520the%2520adaptive%2520weighting%2520of%2520various%2520features.%2520This%250Aempowers%2520the%2520model%2520to%2520concentrate%2520on%2520the%2520most%2520pertinent%2520aspects%2520of%2520the%250Apatient%2527s%2520movements%252C%2520offering%2520precise%2520instructions%2520regarding%2520specific%2520body%250Aparts%2520or%2520movements%2520that%2520require%2520attention.%2520Ablation%2520studies%2520are%2520conducted%2520to%250Aanalyze%2520the%2520impact%2520of%2520individual%2520components%2520within%2520the%2520proposed%2520model.%2520We%250Aevaluate%2520our%2520model%2520on%2520the%2520UI-PRMD%2520dataset%2520demonstrating%2520its%2520performance%2520in%250Aaccurately%2520predicting%2520real-time%2520patient%2520action%2520scores%252C%2520surpassing%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13509v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MR-STGN%3A%20Multi-Residual%20Spatio%20Temporal%20Graph%20Network%20Using%20Attention%0A%20%20Fusion%20for%20Patient%20Action%20Assessment&entry.906535625=Youssef%20Mourchid%20and%20Rim%20Slama&entry.1292438233=%20%20Accurate%20assessment%20of%20patient%20actions%20plays%20a%20crucial%20role%20in%20healthcare%20as%0Ait%20contributes%20significantly%20to%20disease%20progression%20monitoring%20and%20treatment%0Aeffectiveness.%20However%2C%20traditional%20approaches%20to%20assess%20patient%20actions%20often%0Arely%20on%20manual%20observation%20and%20scoring%2C%20which%20are%20subjective%20and%0Atime-consuming.%20In%20this%20paper%2C%20we%20propose%20an%20automated%20approach%20for%20patient%0Aaction%20assessment%20using%20a%20Multi-Residual%20Spatio%20Temporal%20Graph%20Network%0A%28MR-STGN%29%20that%20incorporates%20both%20angular%20and%20positional%203D%20skeletons.%20The%0AMR-STGN%20is%20specifically%20designed%20to%20capture%20the%20spatio-temporal%20dynamics%20of%0Apatient%20actions.%20It%20achieves%20this%20by%20integrating%20information%20from%20multiple%0Aresidual%20layers%2C%20with%20each%20layer%20extracting%20features%20at%20distinct%20levels%20of%0Aabstraction.%20Furthermore%2C%20we%20integrate%20an%20attention%20fusion%20mechanism%20into%20the%0Anetwork%2C%20which%20facilitates%20the%20adaptive%20weighting%20of%20various%20features.%20This%0Aempowers%20the%20model%20to%20concentrate%20on%20the%20most%20pertinent%20aspects%20of%20the%0Apatient%27s%20movements%2C%20offering%20precise%20instructions%20regarding%20specific%20body%0Aparts%20or%20movements%20that%20require%20attention.%20Ablation%20studies%20are%20conducted%20to%0Aanalyze%20the%20impact%20of%20individual%20components%20within%20the%20proposed%20model.%20We%0Aevaluate%20our%20model%20on%20the%20UI-PRMD%20dataset%20demonstrating%20its%20performance%20in%0Aaccurately%20predicting%20real-time%20patient%20action%20scores%2C%20surpassing%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13509v2&entry.124074799=Read"},
{"title": "Stealthy Backdoor Attack in Self-Supervised Learning Vision Encoders for\n  Large Vision Language Models", "author": "Zhaoyi Liu and Huan Zhang", "abstract": "  Self-supervised learning (SSL) vision encoders learn high-quality image\nrepresentations and thus have become a vital part of developing vision modality\nof large vision language models (LVLMs). Due to the high cost of training such\nencoders, pre-trained encoders are widely shared and deployed into many LVLMs,\nwhich are security-critical or bear societal significance. Under this practical\nscenario, we reveal a new backdoor threat that significant visual\nhallucinations can be induced into these LVLMs by merely compromising vision\nencoders. Because of the sharing and reuse of these encoders, many downstream\nLVLMs may inherit backdoor behaviors from encoders, leading to widespread\nbackdoors. In this work, we propose BadVision, the first method to exploit this\nvulnerability in SSL vision encoders for LVLMs with novel trigger optimization\nand backdoor learning techniques. We evaluate BadVision on two types of SSL\nencoders and LVLMs across eight benchmarks. We show that BadVision effectively\ndrives the LVLMs to attacker-chosen hallucination with over 99% attack success\nrate, causing a 77.6% relative visual understanding error while maintaining the\nstealthiness. SoTA backdoor detection methods cannot detect our attack\neffectively.\n", "link": "http://arxiv.org/abs/2502.18290v1", "date": "2025-02-25", "relevancy": 2.6848, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5454}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5454}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stealthy%20Backdoor%20Attack%20in%20Self-Supervised%20Learning%20Vision%20Encoders%20for%0A%20%20Large%20Vision%20Language%20Models&body=Title%3A%20Stealthy%20Backdoor%20Attack%20in%20Self-Supervised%20Learning%20Vision%20Encoders%20for%0A%20%20Large%20Vision%20Language%20Models%0AAuthor%3A%20Zhaoyi%20Liu%20and%20Huan%20Zhang%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20vision%20encoders%20learn%20high-quality%20image%0Arepresentations%20and%20thus%20have%20become%20a%20vital%20part%20of%20developing%20vision%20modality%0Aof%20large%20vision%20language%20models%20%28LVLMs%29.%20Due%20to%20the%20high%20cost%20of%20training%20such%0Aencoders%2C%20pre-trained%20encoders%20are%20widely%20shared%20and%20deployed%20into%20many%20LVLMs%2C%0Awhich%20are%20security-critical%20or%20bear%20societal%20significance.%20Under%20this%20practical%0Ascenario%2C%20we%20reveal%20a%20new%20backdoor%20threat%20that%20significant%20visual%0Ahallucinations%20can%20be%20induced%20into%20these%20LVLMs%20by%20merely%20compromising%20vision%0Aencoders.%20Because%20of%20the%20sharing%20and%20reuse%20of%20these%20encoders%2C%20many%20downstream%0ALVLMs%20may%20inherit%20backdoor%20behaviors%20from%20encoders%2C%20leading%20to%20widespread%0Abackdoors.%20In%20this%20work%2C%20we%20propose%20BadVision%2C%20the%20first%20method%20to%20exploit%20this%0Avulnerability%20in%20SSL%20vision%20encoders%20for%20LVLMs%20with%20novel%20trigger%20optimization%0Aand%20backdoor%20learning%20techniques.%20We%20evaluate%20BadVision%20on%20two%20types%20of%20SSL%0Aencoders%20and%20LVLMs%20across%20eight%20benchmarks.%20We%20show%20that%20BadVision%20effectively%0Adrives%20the%20LVLMs%20to%20attacker-chosen%20hallucination%20with%20over%2099%25%20attack%20success%0Arate%2C%20causing%20a%2077.6%25%20relative%20visual%20understanding%20error%20while%20maintaining%20the%0Astealthiness.%20SoTA%20backdoor%20detection%20methods%20cannot%20detect%20our%20attack%0Aeffectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18290v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStealthy%2520Backdoor%2520Attack%2520in%2520Self-Supervised%2520Learning%2520Vision%2520Encoders%2520for%250A%2520%2520Large%2520Vision%2520Language%2520Models%26entry.906535625%3DZhaoyi%2520Liu%2520and%2520Huan%2520Zhang%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520vision%2520encoders%2520learn%2520high-quality%2520image%250Arepresentations%2520and%2520thus%2520have%2520become%2520a%2520vital%2520part%2520of%2520developing%2520vision%2520modality%250Aof%2520large%2520vision%2520language%2520models%2520%2528LVLMs%2529.%2520Due%2520to%2520the%2520high%2520cost%2520of%2520training%2520such%250Aencoders%252C%2520pre-trained%2520encoders%2520are%2520widely%2520shared%2520and%2520deployed%2520into%2520many%2520LVLMs%252C%250Awhich%2520are%2520security-critical%2520or%2520bear%2520societal%2520significance.%2520Under%2520this%2520practical%250Ascenario%252C%2520we%2520reveal%2520a%2520new%2520backdoor%2520threat%2520that%2520significant%2520visual%250Ahallucinations%2520can%2520be%2520induced%2520into%2520these%2520LVLMs%2520by%2520merely%2520compromising%2520vision%250Aencoders.%2520Because%2520of%2520the%2520sharing%2520and%2520reuse%2520of%2520these%2520encoders%252C%2520many%2520downstream%250ALVLMs%2520may%2520inherit%2520backdoor%2520behaviors%2520from%2520encoders%252C%2520leading%2520to%2520widespread%250Abackdoors.%2520In%2520this%2520work%252C%2520we%2520propose%2520BadVision%252C%2520the%2520first%2520method%2520to%2520exploit%2520this%250Avulnerability%2520in%2520SSL%2520vision%2520encoders%2520for%2520LVLMs%2520with%2520novel%2520trigger%2520optimization%250Aand%2520backdoor%2520learning%2520techniques.%2520We%2520evaluate%2520BadVision%2520on%2520two%2520types%2520of%2520SSL%250Aencoders%2520and%2520LVLMs%2520across%2520eight%2520benchmarks.%2520We%2520show%2520that%2520BadVision%2520effectively%250Adrives%2520the%2520LVLMs%2520to%2520attacker-chosen%2520hallucination%2520with%2520over%252099%2525%2520attack%2520success%250Arate%252C%2520causing%2520a%252077.6%2525%2520relative%2520visual%2520understanding%2520error%2520while%2520maintaining%2520the%250Astealthiness.%2520SoTA%2520backdoor%2520detection%2520methods%2520cannot%2520detect%2520our%2520attack%250Aeffectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18290v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stealthy%20Backdoor%20Attack%20in%20Self-Supervised%20Learning%20Vision%20Encoders%20for%0A%20%20Large%20Vision%20Language%20Models&entry.906535625=Zhaoyi%20Liu%20and%20Huan%20Zhang&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20vision%20encoders%20learn%20high-quality%20image%0Arepresentations%20and%20thus%20have%20become%20a%20vital%20part%20of%20developing%20vision%20modality%0Aof%20large%20vision%20language%20models%20%28LVLMs%29.%20Due%20to%20the%20high%20cost%20of%20training%20such%0Aencoders%2C%20pre-trained%20encoders%20are%20widely%20shared%20and%20deployed%20into%20many%20LVLMs%2C%0Awhich%20are%20security-critical%20or%20bear%20societal%20significance.%20Under%20this%20practical%0Ascenario%2C%20we%20reveal%20a%20new%20backdoor%20threat%20that%20significant%20visual%0Ahallucinations%20can%20be%20induced%20into%20these%20LVLMs%20by%20merely%20compromising%20vision%0Aencoders.%20Because%20of%20the%20sharing%20and%20reuse%20of%20these%20encoders%2C%20many%20downstream%0ALVLMs%20may%20inherit%20backdoor%20behaviors%20from%20encoders%2C%20leading%20to%20widespread%0Abackdoors.%20In%20this%20work%2C%20we%20propose%20BadVision%2C%20the%20first%20method%20to%20exploit%20this%0Avulnerability%20in%20SSL%20vision%20encoders%20for%20LVLMs%20with%20novel%20trigger%20optimization%0Aand%20backdoor%20learning%20techniques.%20We%20evaluate%20BadVision%20on%20two%20types%20of%20SSL%0Aencoders%20and%20LVLMs%20across%20eight%20benchmarks.%20We%20show%20that%20BadVision%20effectively%0Adrives%20the%20LVLMs%20to%20attacker-chosen%20hallucination%20with%20over%2099%25%20attack%20success%0Arate%2C%20causing%20a%2077.6%25%20relative%20visual%20understanding%20error%20while%20maintaining%20the%0Astealthiness.%20SoTA%20backdoor%20detection%20methods%20cannot%20detect%20our%20attack%0Aeffectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18290v1&entry.124074799=Read"},
{"title": "SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation", "author": "Koichi Namekata and Sherwin Bahmani and Ziyi Wu and Yash Kant and Igor Gilitschenski and David B. Lindell", "abstract": "  Methods for image-to-video generation have achieved impressive,\nphoto-realistic quality. However, adjusting specific elements in generated\nvideos, such as object motion or camera movement, is often a tedious process of\ntrial and error, e.g., involving re-generating videos with different random\nseeds. Recent techniques address this issue by fine-tuning a pre-trained model\nto follow conditioning signals, such as bounding boxes or point trajectories.\nYet, this fine-tuning procedure can be computationally expensive, and it\nrequires datasets with annotated object motion, which can be difficult to\nprocure. In this work, we introduce SG-I2V, a framework for controllable\nimage-to-video generation that is self-guided$\\unicode{x2013}$offering\nzero-shot control by relying solely on the knowledge present in a pre-trained\nimage-to-video diffusion model without the need for fine-tuning or external\nknowledge. Our zero-shot method outperforms unsupervised baselines while\nsignificantly narrowing down the performance gap with supervised models in\nterms of visual quality and motion fidelity. Additional details and video\nresults are available on our project page:\nhttps://kmcode1.github.io/Projects/SG-I2V\n", "link": "http://arxiv.org/abs/2411.04989v3", "date": "2025-02-25", "relevancy": 2.6352, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.662}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6576}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SG-I2V%3A%20Self-Guided%20Trajectory%20Control%20in%20Image-to-Video%20Generation&body=Title%3A%20SG-I2V%3A%20Self-Guided%20Trajectory%20Control%20in%20Image-to-Video%20Generation%0AAuthor%3A%20Koichi%20Namekata%20and%20Sherwin%20Bahmani%20and%20Ziyi%20Wu%20and%20Yash%20Kant%20and%20Igor%20Gilitschenski%20and%20David%20B.%20Lindell%0AAbstract%3A%20%20%20Methods%20for%20image-to-video%20generation%20have%20achieved%20impressive%2C%0Aphoto-realistic%20quality.%20However%2C%20adjusting%20specific%20elements%20in%20generated%0Avideos%2C%20such%20as%20object%20motion%20or%20camera%20movement%2C%20is%20often%20a%20tedious%20process%20of%0Atrial%20and%20error%2C%20e.g.%2C%20involving%20re-generating%20videos%20with%20different%20random%0Aseeds.%20Recent%20techniques%20address%20this%20issue%20by%20fine-tuning%20a%20pre-trained%20model%0Ato%20follow%20conditioning%20signals%2C%20such%20as%20bounding%20boxes%20or%20point%20trajectories.%0AYet%2C%20this%20fine-tuning%20procedure%20can%20be%20computationally%20expensive%2C%20and%20it%0Arequires%20datasets%20with%20annotated%20object%20motion%2C%20which%20can%20be%20difficult%20to%0Aprocure.%20In%20this%20work%2C%20we%20introduce%20SG-I2V%2C%20a%20framework%20for%20controllable%0Aimage-to-video%20generation%20that%20is%20self-guided%24%5Cunicode%7Bx2013%7D%24offering%0Azero-shot%20control%20by%20relying%20solely%20on%20the%20knowledge%20present%20in%20a%20pre-trained%0Aimage-to-video%20diffusion%20model%20without%20the%20need%20for%20fine-tuning%20or%20external%0Aknowledge.%20Our%20zero-shot%20method%20outperforms%20unsupervised%20baselines%20while%0Asignificantly%20narrowing%20down%20the%20performance%20gap%20with%20supervised%20models%20in%0Aterms%20of%20visual%20quality%20and%20motion%20fidelity.%20Additional%20details%20and%20video%0Aresults%20are%20available%20on%20our%20project%20page%3A%0Ahttps%3A//kmcode1.github.io/Projects/SG-I2V%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04989v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSG-I2V%253A%2520Self-Guided%2520Trajectory%2520Control%2520in%2520Image-to-Video%2520Generation%26entry.906535625%3DKoichi%2520Namekata%2520and%2520Sherwin%2520Bahmani%2520and%2520Ziyi%2520Wu%2520and%2520Yash%2520Kant%2520and%2520Igor%2520Gilitschenski%2520and%2520David%2520B.%2520Lindell%26entry.1292438233%3D%2520%2520Methods%2520for%2520image-to-video%2520generation%2520have%2520achieved%2520impressive%252C%250Aphoto-realistic%2520quality.%2520However%252C%2520adjusting%2520specific%2520elements%2520in%2520generated%250Avideos%252C%2520such%2520as%2520object%2520motion%2520or%2520camera%2520movement%252C%2520is%2520often%2520a%2520tedious%2520process%2520of%250Atrial%2520and%2520error%252C%2520e.g.%252C%2520involving%2520re-generating%2520videos%2520with%2520different%2520random%250Aseeds.%2520Recent%2520techniques%2520address%2520this%2520issue%2520by%2520fine-tuning%2520a%2520pre-trained%2520model%250Ato%2520follow%2520conditioning%2520signals%252C%2520such%2520as%2520bounding%2520boxes%2520or%2520point%2520trajectories.%250AYet%252C%2520this%2520fine-tuning%2520procedure%2520can%2520be%2520computationally%2520expensive%252C%2520and%2520it%250Arequires%2520datasets%2520with%2520annotated%2520object%2520motion%252C%2520which%2520can%2520be%2520difficult%2520to%250Aprocure.%2520In%2520this%2520work%252C%2520we%2520introduce%2520SG-I2V%252C%2520a%2520framework%2520for%2520controllable%250Aimage-to-video%2520generation%2520that%2520is%2520self-guided%2524%255Cunicode%257Bx2013%257D%2524offering%250Azero-shot%2520control%2520by%2520relying%2520solely%2520on%2520the%2520knowledge%2520present%2520in%2520a%2520pre-trained%250Aimage-to-video%2520diffusion%2520model%2520without%2520the%2520need%2520for%2520fine-tuning%2520or%2520external%250Aknowledge.%2520Our%2520zero-shot%2520method%2520outperforms%2520unsupervised%2520baselines%2520while%250Asignificantly%2520narrowing%2520down%2520the%2520performance%2520gap%2520with%2520supervised%2520models%2520in%250Aterms%2520of%2520visual%2520quality%2520and%2520motion%2520fidelity.%2520Additional%2520details%2520and%2520video%250Aresults%2520are%2520available%2520on%2520our%2520project%2520page%253A%250Ahttps%253A//kmcode1.github.io/Projects/SG-I2V%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04989v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SG-I2V%3A%20Self-Guided%20Trajectory%20Control%20in%20Image-to-Video%20Generation&entry.906535625=Koichi%20Namekata%20and%20Sherwin%20Bahmani%20and%20Ziyi%20Wu%20and%20Yash%20Kant%20and%20Igor%20Gilitschenski%20and%20David%20B.%20Lindell&entry.1292438233=%20%20Methods%20for%20image-to-video%20generation%20have%20achieved%20impressive%2C%0Aphoto-realistic%20quality.%20However%2C%20adjusting%20specific%20elements%20in%20generated%0Avideos%2C%20such%20as%20object%20motion%20or%20camera%20movement%2C%20is%20often%20a%20tedious%20process%20of%0Atrial%20and%20error%2C%20e.g.%2C%20involving%20re-generating%20videos%20with%20different%20random%0Aseeds.%20Recent%20techniques%20address%20this%20issue%20by%20fine-tuning%20a%20pre-trained%20model%0Ato%20follow%20conditioning%20signals%2C%20such%20as%20bounding%20boxes%20or%20point%20trajectories.%0AYet%2C%20this%20fine-tuning%20procedure%20can%20be%20computationally%20expensive%2C%20and%20it%0Arequires%20datasets%20with%20annotated%20object%20motion%2C%20which%20can%20be%20difficult%20to%0Aprocure.%20In%20this%20work%2C%20we%20introduce%20SG-I2V%2C%20a%20framework%20for%20controllable%0Aimage-to-video%20generation%20that%20is%20self-guided%24%5Cunicode%7Bx2013%7D%24offering%0Azero-shot%20control%20by%20relying%20solely%20on%20the%20knowledge%20present%20in%20a%20pre-trained%0Aimage-to-video%20diffusion%20model%20without%20the%20need%20for%20fine-tuning%20or%20external%0Aknowledge.%20Our%20zero-shot%20method%20outperforms%20unsupervised%20baselines%20while%0Asignificantly%20narrowing%20down%20the%20performance%20gap%20with%20supervised%20models%20in%0Aterms%20of%20visual%20quality%20and%20motion%20fidelity.%20Additional%20details%20and%20video%0Aresults%20are%20available%20on%20our%20project%20page%3A%0Ahttps%3A//kmcode1.github.io/Projects/SG-I2V%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04989v3&entry.124074799=Read"},
{"title": "Examining the Threat Landscape: Foundation Models and Model Stealing", "author": "Ankita Raj and Deepankar Varma and Chetan Arora", "abstract": "  Foundation models (FMs) for computer vision learn rich and robust\nrepresentations, enabling their adaptation to task/domain-specific deployments\nwith little to no fine-tuning. However, we posit that the very same strength\ncan make applications based on FMs vulnerable to model stealing attacks.\nThrough empirical analysis, we reveal that models fine-tuned from FMs harbor\nheightened susceptibility to model stealing, compared to conventional vision\narchitectures like ResNets. We hypothesize that this behavior is due to the\ncomprehensive encoding of visual patterns and features learned by FMs during\npre-training, which are accessible to both the attacker and the victim. We\nreport that an attacker is able to obtain 94.28% agreement (matched predictions\nwith victim) for a Vision Transformer based victim model (ViT-L/16) trained on\nCIFAR-10 dataset, compared to only 73.20% agreement for a ResNet-18 victim,\nwhen using ViT-L/16 as the thief model. We arguably show, for the first time,\nthat utilizing FMs for downstream tasks may not be the best choice for\ndeployment in commercial APIs due to their susceptibility to model theft. We\nthereby alert model owners towards the associated security risks, and highlight\nthe need for robust security measures to safeguard such models against theft.\nCode is available at https://github.com/rajankita/foundation_model_stealing.\n", "link": "http://arxiv.org/abs/2502.18077v1", "date": "2025-02-25", "relevancy": 2.6334, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5427}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5427}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Examining%20the%20Threat%20Landscape%3A%20Foundation%20Models%20and%20Model%20Stealing&body=Title%3A%20Examining%20the%20Threat%20Landscape%3A%20Foundation%20Models%20and%20Model%20Stealing%0AAuthor%3A%20Ankita%20Raj%20and%20Deepankar%20Varma%20and%20Chetan%20Arora%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20for%20computer%20vision%20learn%20rich%20and%20robust%0Arepresentations%2C%20enabling%20their%20adaptation%20to%20task/domain-specific%20deployments%0Awith%20little%20to%20no%20fine-tuning.%20However%2C%20we%20posit%20that%20the%20very%20same%20strength%0Acan%20make%20applications%20based%20on%20FMs%20vulnerable%20to%20model%20stealing%20attacks.%0AThrough%20empirical%20analysis%2C%20we%20reveal%20that%20models%20fine-tuned%20from%20FMs%20harbor%0Aheightened%20susceptibility%20to%20model%20stealing%2C%20compared%20to%20conventional%20vision%0Aarchitectures%20like%20ResNets.%20We%20hypothesize%20that%20this%20behavior%20is%20due%20to%20the%0Acomprehensive%20encoding%20of%20visual%20patterns%20and%20features%20learned%20by%20FMs%20during%0Apre-training%2C%20which%20are%20accessible%20to%20both%20the%20attacker%20and%20the%20victim.%20We%0Areport%20that%20an%20attacker%20is%20able%20to%20obtain%2094.28%25%20agreement%20%28matched%20predictions%0Awith%20victim%29%20for%20a%20Vision%20Transformer%20based%20victim%20model%20%28ViT-L/16%29%20trained%20on%0ACIFAR-10%20dataset%2C%20compared%20to%20only%2073.20%25%20agreement%20for%20a%20ResNet-18%20victim%2C%0Awhen%20using%20ViT-L/16%20as%20the%20thief%20model.%20We%20arguably%20show%2C%20for%20the%20first%20time%2C%0Athat%20utilizing%20FMs%20for%20downstream%20tasks%20may%20not%20be%20the%20best%20choice%20for%0Adeployment%20in%20commercial%20APIs%20due%20to%20their%20susceptibility%20to%20model%20theft.%20We%0Athereby%20alert%20model%20owners%20towards%20the%20associated%20security%20risks%2C%20and%20highlight%0Athe%20need%20for%20robust%20security%20measures%20to%20safeguard%20such%20models%20against%20theft.%0ACode%20is%20available%20at%20https%3A//github.com/rajankita/foundation_model_stealing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExamining%2520the%2520Threat%2520Landscape%253A%2520Foundation%2520Models%2520and%2520Model%2520Stealing%26entry.906535625%3DAnkita%2520Raj%2520and%2520Deepankar%2520Varma%2520and%2520Chetan%2520Arora%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520for%2520computer%2520vision%2520learn%2520rich%2520and%2520robust%250Arepresentations%252C%2520enabling%2520their%2520adaptation%2520to%2520task/domain-specific%2520deployments%250Awith%2520little%2520to%2520no%2520fine-tuning.%2520However%252C%2520we%2520posit%2520that%2520the%2520very%2520same%2520strength%250Acan%2520make%2520applications%2520based%2520on%2520FMs%2520vulnerable%2520to%2520model%2520stealing%2520attacks.%250AThrough%2520empirical%2520analysis%252C%2520we%2520reveal%2520that%2520models%2520fine-tuned%2520from%2520FMs%2520harbor%250Aheightened%2520susceptibility%2520to%2520model%2520stealing%252C%2520compared%2520to%2520conventional%2520vision%250Aarchitectures%2520like%2520ResNets.%2520We%2520hypothesize%2520that%2520this%2520behavior%2520is%2520due%2520to%2520the%250Acomprehensive%2520encoding%2520of%2520visual%2520patterns%2520and%2520features%2520learned%2520by%2520FMs%2520during%250Apre-training%252C%2520which%2520are%2520accessible%2520to%2520both%2520the%2520attacker%2520and%2520the%2520victim.%2520We%250Areport%2520that%2520an%2520attacker%2520is%2520able%2520to%2520obtain%252094.28%2525%2520agreement%2520%2528matched%2520predictions%250Awith%2520victim%2529%2520for%2520a%2520Vision%2520Transformer%2520based%2520victim%2520model%2520%2528ViT-L/16%2529%2520trained%2520on%250ACIFAR-10%2520dataset%252C%2520compared%2520to%2520only%252073.20%2525%2520agreement%2520for%2520a%2520ResNet-18%2520victim%252C%250Awhen%2520using%2520ViT-L/16%2520as%2520the%2520thief%2520model.%2520We%2520arguably%2520show%252C%2520for%2520the%2520first%2520time%252C%250Athat%2520utilizing%2520FMs%2520for%2520downstream%2520tasks%2520may%2520not%2520be%2520the%2520best%2520choice%2520for%250Adeployment%2520in%2520commercial%2520APIs%2520due%2520to%2520their%2520susceptibility%2520to%2520model%2520theft.%2520We%250Athereby%2520alert%2520model%2520owners%2520towards%2520the%2520associated%2520security%2520risks%252C%2520and%2520highlight%250Athe%2520need%2520for%2520robust%2520security%2520measures%2520to%2520safeguard%2520such%2520models%2520against%2520theft.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/rajankita/foundation_model_stealing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Examining%20the%20Threat%20Landscape%3A%20Foundation%20Models%20and%20Model%20Stealing&entry.906535625=Ankita%20Raj%20and%20Deepankar%20Varma%20and%20Chetan%20Arora&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20for%20computer%20vision%20learn%20rich%20and%20robust%0Arepresentations%2C%20enabling%20their%20adaptation%20to%20task/domain-specific%20deployments%0Awith%20little%20to%20no%20fine-tuning.%20However%2C%20we%20posit%20that%20the%20very%20same%20strength%0Acan%20make%20applications%20based%20on%20FMs%20vulnerable%20to%20model%20stealing%20attacks.%0AThrough%20empirical%20analysis%2C%20we%20reveal%20that%20models%20fine-tuned%20from%20FMs%20harbor%0Aheightened%20susceptibility%20to%20model%20stealing%2C%20compared%20to%20conventional%20vision%0Aarchitectures%20like%20ResNets.%20We%20hypothesize%20that%20this%20behavior%20is%20due%20to%20the%0Acomprehensive%20encoding%20of%20visual%20patterns%20and%20features%20learned%20by%20FMs%20during%0Apre-training%2C%20which%20are%20accessible%20to%20both%20the%20attacker%20and%20the%20victim.%20We%0Areport%20that%20an%20attacker%20is%20able%20to%20obtain%2094.28%25%20agreement%20%28matched%20predictions%0Awith%20victim%29%20for%20a%20Vision%20Transformer%20based%20victim%20model%20%28ViT-L/16%29%20trained%20on%0ACIFAR-10%20dataset%2C%20compared%20to%20only%2073.20%25%20agreement%20for%20a%20ResNet-18%20victim%2C%0Awhen%20using%20ViT-L/16%20as%20the%20thief%20model.%20We%20arguably%20show%2C%20for%20the%20first%20time%2C%0Athat%20utilizing%20FMs%20for%20downstream%20tasks%20may%20not%20be%20the%20best%20choice%20for%0Adeployment%20in%20commercial%20APIs%20due%20to%20their%20susceptibility%20to%20model%20theft.%20We%0Athereby%20alert%20model%20owners%20towards%20the%20associated%20security%20risks%2C%20and%20highlight%0Athe%20need%20for%20robust%20security%20measures%20to%20safeguard%20such%20models%20against%20theft.%0ACode%20is%20available%20at%20https%3A//github.com/rajankita/foundation_model_stealing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18077v1&entry.124074799=Read"},
{"title": "MoFM: A Large-Scale Human Motion Foundation Model", "author": "Mohammadreza Baharani and Ghazal Alinezhad Noghre and Armin Danesh Pazho and Gabriel Maldonado and Hamed Tabkhi", "abstract": "  Foundation Models (FM) have increasingly drawn the attention of researchers\ndue to their scalability and generalization across diverse tasks. Inspired by\nthe success of FMs and the principles that have driven advancements in Large\nLanguage Models (LLMs), we introduce MoFM as a novel Motion Foundation Model.\nMoFM is designed for the semantic understanding of complex human motions in\nboth time and space. To facilitate large-scale training, MotionBook, a\ncomprehensive human motion dictionary of discretized motions is designed and\nemployed. MotionBook utilizes Thermal Cubes to capture spatio-temporal motion\nheatmaps, applying principles from discrete variational models to encode human\nmovements into discrete units for a more efficient and scalable representation.\nMoFM, trained on a large corpus of motion data, provides a foundational\nbackbone adaptable to diverse downstream tasks, supporting paradigms such as\none-shot, unsupervised, and supervised tasks. This versatility makes MoFM\nwell-suited for a wide range of motion-based applications.\n", "link": "http://arxiv.org/abs/2502.05432v2", "date": "2025-02-25", "relevancy": 2.6215, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5303}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5213}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoFM%3A%20A%20Large-Scale%20Human%20Motion%20Foundation%20Model&body=Title%3A%20MoFM%3A%20A%20Large-Scale%20Human%20Motion%20Foundation%20Model%0AAuthor%3A%20Mohammadreza%20Baharani%20and%20Ghazal%20Alinezhad%20Noghre%20and%20Armin%20Danesh%20Pazho%20and%20Gabriel%20Maldonado%20and%20Hamed%20Tabkhi%0AAbstract%3A%20%20%20Foundation%20Models%20%28FM%29%20have%20increasingly%20drawn%20the%20attention%20of%20researchers%0Adue%20to%20their%20scalability%20and%20generalization%20across%20diverse%20tasks.%20Inspired%20by%0Athe%20success%20of%20FMs%20and%20the%20principles%20that%20have%20driven%20advancements%20in%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20we%20introduce%20MoFM%20as%20a%20novel%20Motion%20Foundation%20Model.%0AMoFM%20is%20designed%20for%20the%20semantic%20understanding%20of%20complex%20human%20motions%20in%0Aboth%20time%20and%20space.%20To%20facilitate%20large-scale%20training%2C%20MotionBook%2C%20a%0Acomprehensive%20human%20motion%20dictionary%20of%20discretized%20motions%20is%20designed%20and%0Aemployed.%20MotionBook%20utilizes%20Thermal%20Cubes%20to%20capture%20spatio-temporal%20motion%0Aheatmaps%2C%20applying%20principles%20from%20discrete%20variational%20models%20to%20encode%20human%0Amovements%20into%20discrete%20units%20for%20a%20more%20efficient%20and%20scalable%20representation.%0AMoFM%2C%20trained%20on%20a%20large%20corpus%20of%20motion%20data%2C%20provides%20a%20foundational%0Abackbone%20adaptable%20to%20diverse%20downstream%20tasks%2C%20supporting%20paradigms%20such%20as%0Aone-shot%2C%20unsupervised%2C%20and%20supervised%20tasks.%20This%20versatility%20makes%20MoFM%0Awell-suited%20for%20a%20wide%20range%20of%20motion-based%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05432v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoFM%253A%2520A%2520Large-Scale%2520Human%2520Motion%2520Foundation%2520Model%26entry.906535625%3DMohammadreza%2520Baharani%2520and%2520Ghazal%2520Alinezhad%2520Noghre%2520and%2520Armin%2520Danesh%2520Pazho%2520and%2520Gabriel%2520Maldonado%2520and%2520Hamed%2520Tabkhi%26entry.1292438233%3D%2520%2520Foundation%2520Models%2520%2528FM%2529%2520have%2520increasingly%2520drawn%2520the%2520attention%2520of%2520researchers%250Adue%2520to%2520their%2520scalability%2520and%2520generalization%2520across%2520diverse%2520tasks.%2520Inspired%2520by%250Athe%2520success%2520of%2520FMs%2520and%2520the%2520principles%2520that%2520have%2520driven%2520advancements%2520in%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%252C%2520we%2520introduce%2520MoFM%2520as%2520a%2520novel%2520Motion%2520Foundation%2520Model.%250AMoFM%2520is%2520designed%2520for%2520the%2520semantic%2520understanding%2520of%2520complex%2520human%2520motions%2520in%250Aboth%2520time%2520and%2520space.%2520To%2520facilitate%2520large-scale%2520training%252C%2520MotionBook%252C%2520a%250Acomprehensive%2520human%2520motion%2520dictionary%2520of%2520discretized%2520motions%2520is%2520designed%2520and%250Aemployed.%2520MotionBook%2520utilizes%2520Thermal%2520Cubes%2520to%2520capture%2520spatio-temporal%2520motion%250Aheatmaps%252C%2520applying%2520principles%2520from%2520discrete%2520variational%2520models%2520to%2520encode%2520human%250Amovements%2520into%2520discrete%2520units%2520for%2520a%2520more%2520efficient%2520and%2520scalable%2520representation.%250AMoFM%252C%2520trained%2520on%2520a%2520large%2520corpus%2520of%2520motion%2520data%252C%2520provides%2520a%2520foundational%250Abackbone%2520adaptable%2520to%2520diverse%2520downstream%2520tasks%252C%2520supporting%2520paradigms%2520such%2520as%250Aone-shot%252C%2520unsupervised%252C%2520and%2520supervised%2520tasks.%2520This%2520versatility%2520makes%2520MoFM%250Awell-suited%2520for%2520a%2520wide%2520range%2520of%2520motion-based%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05432v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoFM%3A%20A%20Large-Scale%20Human%20Motion%20Foundation%20Model&entry.906535625=Mohammadreza%20Baharani%20and%20Ghazal%20Alinezhad%20Noghre%20and%20Armin%20Danesh%20Pazho%20and%20Gabriel%20Maldonado%20and%20Hamed%20Tabkhi&entry.1292438233=%20%20Foundation%20Models%20%28FM%29%20have%20increasingly%20drawn%20the%20attention%20of%20researchers%0Adue%20to%20their%20scalability%20and%20generalization%20across%20diverse%20tasks.%20Inspired%20by%0Athe%20success%20of%20FMs%20and%20the%20principles%20that%20have%20driven%20advancements%20in%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20we%20introduce%20MoFM%20as%20a%20novel%20Motion%20Foundation%20Model.%0AMoFM%20is%20designed%20for%20the%20semantic%20understanding%20of%20complex%20human%20motions%20in%0Aboth%20time%20and%20space.%20To%20facilitate%20large-scale%20training%2C%20MotionBook%2C%20a%0Acomprehensive%20human%20motion%20dictionary%20of%20discretized%20motions%20is%20designed%20and%0Aemployed.%20MotionBook%20utilizes%20Thermal%20Cubes%20to%20capture%20spatio-temporal%20motion%0Aheatmaps%2C%20applying%20principles%20from%20discrete%20variational%20models%20to%20encode%20human%0Amovements%20into%20discrete%20units%20for%20a%20more%20efficient%20and%20scalable%20representation.%0AMoFM%2C%20trained%20on%20a%20large%20corpus%20of%20motion%20data%2C%20provides%20a%20foundational%0Abackbone%20adaptable%20to%20diverse%20downstream%20tasks%2C%20supporting%20paradigms%20such%20as%0Aone-shot%2C%20unsupervised%2C%20and%20supervised%20tasks.%20This%20versatility%20makes%20MoFM%0Awell-suited%20for%20a%20wide%20range%20of%20motion-based%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05432v2&entry.124074799=Read"},
{"title": "Learn Your Reference Model for Real Good Alignment", "author": "Alexey Gorbatovski and Boris Shaposhnikov and Alexey Malakhov and Nikita Surnachev and Yaroslav Aksenov and Ian Maksimov and Nikita Balagansky and Daniil Gavrilov", "abstract": "  Despite the fact that offline methods for Large Language Models (LLMs)\nalignment do not require a direct reward model, they remain susceptible to\noveroptimization. This issue arises when the trained model deviates excessively\nfrom the reference policy, leading to a decrease in sample quality. We propose\na new paradigm of offline alignment methods, called Trust Region (including\nvariants TR-DPO, TR-IPO, TR-KTO), which dynamically updates the reference\npolicy throughout the training process. Our results show that TR alignment\nmethods effectively mitigate overoptimization, enabling models to maintain\nstrong performance even when substantially deviating from the initial reference\npolicy. We demonstrate the efficacy of these approaches not only through toy\nexamples that exhibit reduced overoptimization, but also through direct,\nside-by-side comparisons in specific tasks such as helpful and harmless\ndialogue, as well as summarization, where they surpass conventional methods.\nAdditionally, we report significant improvements in general-purpose assistant\nsetups with the Llama3 model on the AlpacaEval 2 and Arena-Hard benchmarks,\nhighlighting the advantages of Trust Region methods over classical approaches.\n", "link": "http://arxiv.org/abs/2404.09656v4", "date": "2025-02-25", "relevancy": 2.6204, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5181}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learn%20Your%20Reference%20Model%20for%20Real%20Good%20Alignment&body=Title%3A%20Learn%20Your%20Reference%20Model%20for%20Real%20Good%20Alignment%0AAuthor%3A%20Alexey%20Gorbatovski%20and%20Boris%20Shaposhnikov%20and%20Alexey%20Malakhov%20and%20Nikita%20Surnachev%20and%20Yaroslav%20Aksenov%20and%20Ian%20Maksimov%20and%20Nikita%20Balagansky%20and%20Daniil%20Gavrilov%0AAbstract%3A%20%20%20Despite%20the%20fact%20that%20offline%20methods%20for%20Large%20Language%20Models%20%28LLMs%29%0Aalignment%20do%20not%20require%20a%20direct%20reward%20model%2C%20they%20remain%20susceptible%20to%0Aoveroptimization.%20This%20issue%20arises%20when%20the%20trained%20model%20deviates%20excessively%0Afrom%20the%20reference%20policy%2C%20leading%20to%20a%20decrease%20in%20sample%20quality.%20We%20propose%0Aa%20new%20paradigm%20of%20offline%20alignment%20methods%2C%20called%20Trust%20Region%20%28including%0Avariants%20TR-DPO%2C%20TR-IPO%2C%20TR-KTO%29%2C%20which%20dynamically%20updates%20the%20reference%0Apolicy%20throughout%20the%20training%20process.%20Our%20results%20show%20that%20TR%20alignment%0Amethods%20effectively%20mitigate%20overoptimization%2C%20enabling%20models%20to%20maintain%0Astrong%20performance%20even%20when%20substantially%20deviating%20from%20the%20initial%20reference%0Apolicy.%20We%20demonstrate%20the%20efficacy%20of%20these%20approaches%20not%20only%20through%20toy%0Aexamples%20that%20exhibit%20reduced%20overoptimization%2C%20but%20also%20through%20direct%2C%0Aside-by-side%20comparisons%20in%20specific%20tasks%20such%20as%20helpful%20and%20harmless%0Adialogue%2C%20as%20well%20as%20summarization%2C%20where%20they%20surpass%20conventional%20methods.%0AAdditionally%2C%20we%20report%20significant%20improvements%20in%20general-purpose%20assistant%0Asetups%20with%20the%20Llama3%20model%20on%20the%20AlpacaEval%202%20and%20Arena-Hard%20benchmarks%2C%0Ahighlighting%20the%20advantages%20of%20Trust%20Region%20methods%20over%20classical%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09656v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearn%2520Your%2520Reference%2520Model%2520for%2520Real%2520Good%2520Alignment%26entry.906535625%3DAlexey%2520Gorbatovski%2520and%2520Boris%2520Shaposhnikov%2520and%2520Alexey%2520Malakhov%2520and%2520Nikita%2520Surnachev%2520and%2520Yaroslav%2520Aksenov%2520and%2520Ian%2520Maksimov%2520and%2520Nikita%2520Balagansky%2520and%2520Daniil%2520Gavrilov%26entry.1292438233%3D%2520%2520Despite%2520the%2520fact%2520that%2520offline%2520methods%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Aalignment%2520do%2520not%2520require%2520a%2520direct%2520reward%2520model%252C%2520they%2520remain%2520susceptible%2520to%250Aoveroptimization.%2520This%2520issue%2520arises%2520when%2520the%2520trained%2520model%2520deviates%2520excessively%250Afrom%2520the%2520reference%2520policy%252C%2520leading%2520to%2520a%2520decrease%2520in%2520sample%2520quality.%2520We%2520propose%250Aa%2520new%2520paradigm%2520of%2520offline%2520alignment%2520methods%252C%2520called%2520Trust%2520Region%2520%2528including%250Avariants%2520TR-DPO%252C%2520TR-IPO%252C%2520TR-KTO%2529%252C%2520which%2520dynamically%2520updates%2520the%2520reference%250Apolicy%2520throughout%2520the%2520training%2520process.%2520Our%2520results%2520show%2520that%2520TR%2520alignment%250Amethods%2520effectively%2520mitigate%2520overoptimization%252C%2520enabling%2520models%2520to%2520maintain%250Astrong%2520performance%2520even%2520when%2520substantially%2520deviating%2520from%2520the%2520initial%2520reference%250Apolicy.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520these%2520approaches%2520not%2520only%2520through%2520toy%250Aexamples%2520that%2520exhibit%2520reduced%2520overoptimization%252C%2520but%2520also%2520through%2520direct%252C%250Aside-by-side%2520comparisons%2520in%2520specific%2520tasks%2520such%2520as%2520helpful%2520and%2520harmless%250Adialogue%252C%2520as%2520well%2520as%2520summarization%252C%2520where%2520they%2520surpass%2520conventional%2520methods.%250AAdditionally%252C%2520we%2520report%2520significant%2520improvements%2520in%2520general-purpose%2520assistant%250Asetups%2520with%2520the%2520Llama3%2520model%2520on%2520the%2520AlpacaEval%25202%2520and%2520Arena-Hard%2520benchmarks%252C%250Ahighlighting%2520the%2520advantages%2520of%2520Trust%2520Region%2520methods%2520over%2520classical%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09656v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn%20Your%20Reference%20Model%20for%20Real%20Good%20Alignment&entry.906535625=Alexey%20Gorbatovski%20and%20Boris%20Shaposhnikov%20and%20Alexey%20Malakhov%20and%20Nikita%20Surnachev%20and%20Yaroslav%20Aksenov%20and%20Ian%20Maksimov%20and%20Nikita%20Balagansky%20and%20Daniil%20Gavrilov&entry.1292438233=%20%20Despite%20the%20fact%20that%20offline%20methods%20for%20Large%20Language%20Models%20%28LLMs%29%0Aalignment%20do%20not%20require%20a%20direct%20reward%20model%2C%20they%20remain%20susceptible%20to%0Aoveroptimization.%20This%20issue%20arises%20when%20the%20trained%20model%20deviates%20excessively%0Afrom%20the%20reference%20policy%2C%20leading%20to%20a%20decrease%20in%20sample%20quality.%20We%20propose%0Aa%20new%20paradigm%20of%20offline%20alignment%20methods%2C%20called%20Trust%20Region%20%28including%0Avariants%20TR-DPO%2C%20TR-IPO%2C%20TR-KTO%29%2C%20which%20dynamically%20updates%20the%20reference%0Apolicy%20throughout%20the%20training%20process.%20Our%20results%20show%20that%20TR%20alignment%0Amethods%20effectively%20mitigate%20overoptimization%2C%20enabling%20models%20to%20maintain%0Astrong%20performance%20even%20when%20substantially%20deviating%20from%20the%20initial%20reference%0Apolicy.%20We%20demonstrate%20the%20efficacy%20of%20these%20approaches%20not%20only%20through%20toy%0Aexamples%20that%20exhibit%20reduced%20overoptimization%2C%20but%20also%20through%20direct%2C%0Aside-by-side%20comparisons%20in%20specific%20tasks%20such%20as%20helpful%20and%20harmless%0Adialogue%2C%20as%20well%20as%20summarization%2C%20where%20they%20surpass%20conventional%20methods.%0AAdditionally%2C%20we%20report%20significant%20improvements%20in%20general-purpose%20assistant%0Asetups%20with%20the%20Llama3%20model%20on%20the%20AlpacaEval%202%20and%20Arena-Hard%20benchmarks%2C%0Ahighlighting%20the%20advantages%20of%20Trust%20Region%20methods%20over%20classical%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09656v4&entry.124074799=Read"},
{"title": "VideoShield: Regulating Diffusion-based Video Generation Models via\n  Watermarking", "author": "Runyi Hu and Jie Zhang and Yiming Li and Jiwei Li and Qing Guo and Han Qiu and Tianwei Zhang", "abstract": "  Artificial Intelligence Generated Content (AIGC) has advanced significantly,\nparticularly with the development of video generation models such as\ntext-to-video (T2V) models and image-to-video (I2V) models. However, like other\nAIGC types, video generation requires robust content control. A common approach\nis to embed watermarks, but most research has focused on images, with limited\nattention given to videos. Traditional methods, which embed watermarks\nframe-by-frame in a post-processing manner, often degrade video quality. In\nthis paper, we propose VideoShield, a novel watermarking framework specifically\ndesigned for popular diffusion-based video generation models. Unlike\npost-processing methods, VideoShield embeds watermarks directly during video\ngeneration, eliminating the need for additional training. To ensure video\nintegrity, we introduce a tamper localization feature that can detect changes\nboth temporally (across frames) and spatially (within individual frames). Our\nmethod maps watermark bits to template bits, which are then used to generate\nwatermarked noise during the denoising process. Using DDIM Inversion, we can\nreverse the video to its original watermarked noise, enabling straightforward\nwatermark extraction. Additionally, template bits allow precise detection for\npotential temporal and spatial modification. Extensive experiments across\nvarious video models (both T2V and I2V models) demonstrate that our method\neffectively extracts watermarks and detects tamper without compromising video\nquality. Furthermore, we show that this approach is applicable to image\ngeneration models, enabling tamper detection in generated images as well. Codes\nand models are available at https://github.com/hurunyi/VideoShield.\n", "link": "http://arxiv.org/abs/2501.14195v2", "date": "2025-02-25", "relevancy": 2.6156, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6785}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6399}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoShield%3A%20Regulating%20Diffusion-based%20Video%20Generation%20Models%20via%0A%20%20Watermarking&body=Title%3A%20VideoShield%3A%20Regulating%20Diffusion-based%20Video%20Generation%20Models%20via%0A%20%20Watermarking%0AAuthor%3A%20Runyi%20Hu%20and%20Jie%20Zhang%20and%20Yiming%20Li%20and%20Jiwei%20Li%20and%20Qing%20Guo%20and%20Han%20Qiu%20and%20Tianwei%20Zhang%0AAbstract%3A%20%20%20Artificial%20Intelligence%20Generated%20Content%20%28AIGC%29%20has%20advanced%20significantly%2C%0Aparticularly%20with%20the%20development%20of%20video%20generation%20models%20such%20as%0Atext-to-video%20%28T2V%29%20models%20and%20image-to-video%20%28I2V%29%20models.%20However%2C%20like%20other%0AAIGC%20types%2C%20video%20generation%20requires%20robust%20content%20control.%20A%20common%20approach%0Ais%20to%20embed%20watermarks%2C%20but%20most%20research%20has%20focused%20on%20images%2C%20with%20limited%0Aattention%20given%20to%20videos.%20Traditional%20methods%2C%20which%20embed%20watermarks%0Aframe-by-frame%20in%20a%20post-processing%20manner%2C%20often%20degrade%20video%20quality.%20In%0Athis%20paper%2C%20we%20propose%20VideoShield%2C%20a%20novel%20watermarking%20framework%20specifically%0Adesigned%20for%20popular%20diffusion-based%20video%20generation%20models.%20Unlike%0Apost-processing%20methods%2C%20VideoShield%20embeds%20watermarks%20directly%20during%20video%0Ageneration%2C%20eliminating%20the%20need%20for%20additional%20training.%20To%20ensure%20video%0Aintegrity%2C%20we%20introduce%20a%20tamper%20localization%20feature%20that%20can%20detect%20changes%0Aboth%20temporally%20%28across%20frames%29%20and%20spatially%20%28within%20individual%20frames%29.%20Our%0Amethod%20maps%20watermark%20bits%20to%20template%20bits%2C%20which%20are%20then%20used%20to%20generate%0Awatermarked%20noise%20during%20the%20denoising%20process.%20Using%20DDIM%20Inversion%2C%20we%20can%0Areverse%20the%20video%20to%20its%20original%20watermarked%20noise%2C%20enabling%20straightforward%0Awatermark%20extraction.%20Additionally%2C%20template%20bits%20allow%20precise%20detection%20for%0Apotential%20temporal%20and%20spatial%20modification.%20Extensive%20experiments%20across%0Avarious%20video%20models%20%28both%20T2V%20and%20I2V%20models%29%20demonstrate%20that%20our%20method%0Aeffectively%20extracts%20watermarks%20and%20detects%20tamper%20without%20compromising%20video%0Aquality.%20Furthermore%2C%20we%20show%20that%20this%20approach%20is%20applicable%20to%20image%0Ageneration%20models%2C%20enabling%20tamper%20detection%20in%20generated%20images%20as%20well.%20Codes%0Aand%20models%20are%20available%20at%20https%3A//github.com/hurunyi/VideoShield.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14195v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoShield%253A%2520Regulating%2520Diffusion-based%2520Video%2520Generation%2520Models%2520via%250A%2520%2520Watermarking%26entry.906535625%3DRunyi%2520Hu%2520and%2520Jie%2520Zhang%2520and%2520Yiming%2520Li%2520and%2520Jiwei%2520Li%2520and%2520Qing%2520Guo%2520and%2520Han%2520Qiu%2520and%2520Tianwei%2520Zhang%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence%2520Generated%2520Content%2520%2528AIGC%2529%2520has%2520advanced%2520significantly%252C%250Aparticularly%2520with%2520the%2520development%2520of%2520video%2520generation%2520models%2520such%2520as%250Atext-to-video%2520%2528T2V%2529%2520models%2520and%2520image-to-video%2520%2528I2V%2529%2520models.%2520However%252C%2520like%2520other%250AAIGC%2520types%252C%2520video%2520generation%2520requires%2520robust%2520content%2520control.%2520A%2520common%2520approach%250Ais%2520to%2520embed%2520watermarks%252C%2520but%2520most%2520research%2520has%2520focused%2520on%2520images%252C%2520with%2520limited%250Aattention%2520given%2520to%2520videos.%2520Traditional%2520methods%252C%2520which%2520embed%2520watermarks%250Aframe-by-frame%2520in%2520a%2520post-processing%2520manner%252C%2520often%2520degrade%2520video%2520quality.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520VideoShield%252C%2520a%2520novel%2520watermarking%2520framework%2520specifically%250Adesigned%2520for%2520popular%2520diffusion-based%2520video%2520generation%2520models.%2520Unlike%250Apost-processing%2520methods%252C%2520VideoShield%2520embeds%2520watermarks%2520directly%2520during%2520video%250Ageneration%252C%2520eliminating%2520the%2520need%2520for%2520additional%2520training.%2520To%2520ensure%2520video%250Aintegrity%252C%2520we%2520introduce%2520a%2520tamper%2520localization%2520feature%2520that%2520can%2520detect%2520changes%250Aboth%2520temporally%2520%2528across%2520frames%2529%2520and%2520spatially%2520%2528within%2520individual%2520frames%2529.%2520Our%250Amethod%2520maps%2520watermark%2520bits%2520to%2520template%2520bits%252C%2520which%2520are%2520then%2520used%2520to%2520generate%250Awatermarked%2520noise%2520during%2520the%2520denoising%2520process.%2520Using%2520DDIM%2520Inversion%252C%2520we%2520can%250Areverse%2520the%2520video%2520to%2520its%2520original%2520watermarked%2520noise%252C%2520enabling%2520straightforward%250Awatermark%2520extraction.%2520Additionally%252C%2520template%2520bits%2520allow%2520precise%2520detection%2520for%250Apotential%2520temporal%2520and%2520spatial%2520modification.%2520Extensive%2520experiments%2520across%250Avarious%2520video%2520models%2520%2528both%2520T2V%2520and%2520I2V%2520models%2529%2520demonstrate%2520that%2520our%2520method%250Aeffectively%2520extracts%2520watermarks%2520and%2520detects%2520tamper%2520without%2520compromising%2520video%250Aquality.%2520Furthermore%252C%2520we%2520show%2520that%2520this%2520approach%2520is%2520applicable%2520to%2520image%250Ageneration%2520models%252C%2520enabling%2520tamper%2520detection%2520in%2520generated%2520images%2520as%2520well.%2520Codes%250Aand%2520models%2520are%2520available%2520at%2520https%253A//github.com/hurunyi/VideoShield.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14195v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoShield%3A%20Regulating%20Diffusion-based%20Video%20Generation%20Models%20via%0A%20%20Watermarking&entry.906535625=Runyi%20Hu%20and%20Jie%20Zhang%20and%20Yiming%20Li%20and%20Jiwei%20Li%20and%20Qing%20Guo%20and%20Han%20Qiu%20and%20Tianwei%20Zhang&entry.1292438233=%20%20Artificial%20Intelligence%20Generated%20Content%20%28AIGC%29%20has%20advanced%20significantly%2C%0Aparticularly%20with%20the%20development%20of%20video%20generation%20models%20such%20as%0Atext-to-video%20%28T2V%29%20models%20and%20image-to-video%20%28I2V%29%20models.%20However%2C%20like%20other%0AAIGC%20types%2C%20video%20generation%20requires%20robust%20content%20control.%20A%20common%20approach%0Ais%20to%20embed%20watermarks%2C%20but%20most%20research%20has%20focused%20on%20images%2C%20with%20limited%0Aattention%20given%20to%20videos.%20Traditional%20methods%2C%20which%20embed%20watermarks%0Aframe-by-frame%20in%20a%20post-processing%20manner%2C%20often%20degrade%20video%20quality.%20In%0Athis%20paper%2C%20we%20propose%20VideoShield%2C%20a%20novel%20watermarking%20framework%20specifically%0Adesigned%20for%20popular%20diffusion-based%20video%20generation%20models.%20Unlike%0Apost-processing%20methods%2C%20VideoShield%20embeds%20watermarks%20directly%20during%20video%0Ageneration%2C%20eliminating%20the%20need%20for%20additional%20training.%20To%20ensure%20video%0Aintegrity%2C%20we%20introduce%20a%20tamper%20localization%20feature%20that%20can%20detect%20changes%0Aboth%20temporally%20%28across%20frames%29%20and%20spatially%20%28within%20individual%20frames%29.%20Our%0Amethod%20maps%20watermark%20bits%20to%20template%20bits%2C%20which%20are%20then%20used%20to%20generate%0Awatermarked%20noise%20during%20the%20denoising%20process.%20Using%20DDIM%20Inversion%2C%20we%20can%0Areverse%20the%20video%20to%20its%20original%20watermarked%20noise%2C%20enabling%20straightforward%0Awatermark%20extraction.%20Additionally%2C%20template%20bits%20allow%20precise%20detection%20for%0Apotential%20temporal%20and%20spatial%20modification.%20Extensive%20experiments%20across%0Avarious%20video%20models%20%28both%20T2V%20and%20I2V%20models%29%20demonstrate%20that%20our%20method%0Aeffectively%20extracts%20watermarks%20and%20detects%20tamper%20without%20compromising%20video%0Aquality.%20Furthermore%2C%20we%20show%20that%20this%20approach%20is%20applicable%20to%20image%0Ageneration%20models%2C%20enabling%20tamper%20detection%20in%20generated%20images%20as%20well.%20Codes%0Aand%20models%20are%20available%20at%20https%3A//github.com/hurunyi/VideoShield.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14195v2&entry.124074799=Read"},
{"title": "From Vision to Sound: Advancing Audio Anomaly Detection with\n  Vision-Based Algorithms", "author": "Manuel Barusco and Francesco Borsatti and Davide Dalle Pezze and Francesco Paissan and Elisabetta Farella and Gian Antonio Susto", "abstract": "  Recent advances in Visual Anomaly Detection (VAD) have introduced\nsophisticated algorithms leveraging embeddings generated by pre-trained feature\nextractors. Inspired by these developments, we investigate the adaptation of\nsuch algorithms to the audio domain to address the problem of Audio Anomaly\nDetection (AAD). Unlike most existing AAD methods, which primarily classify\nanomalous samples, our approach introduces fine-grained temporal-frequency\nlocalization of anomalies within the spectrogram, significantly improving\nexplainability. This capability enables a more precise understanding of where\nand when anomalies occur, making the results more actionable for end users. We\nevaluate our approach on industrial and environmental benchmarks, demonstrating\nthe effectiveness of VAD techniques in detecting anomalies in audio signals.\nMoreover, they improve explainability by enabling localized anomaly\nidentification, making audio anomaly detection systems more interpretable and\npractical.\n", "link": "http://arxiv.org/abs/2502.18328v1", "date": "2025-02-25", "relevancy": 2.6013, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5298}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5155}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Vision%20to%20Sound%3A%20Advancing%20Audio%20Anomaly%20Detection%20with%0A%20%20Vision-Based%20Algorithms&body=Title%3A%20From%20Vision%20to%20Sound%3A%20Advancing%20Audio%20Anomaly%20Detection%20with%0A%20%20Vision-Based%20Algorithms%0AAuthor%3A%20Manuel%20Barusco%20and%20Francesco%20Borsatti%20and%20Davide%20Dalle%20Pezze%20and%20Francesco%20Paissan%20and%20Elisabetta%20Farella%20and%20Gian%20Antonio%20Susto%0AAbstract%3A%20%20%20Recent%20advances%20in%20Visual%20Anomaly%20Detection%20%28VAD%29%20have%20introduced%0Asophisticated%20algorithms%20leveraging%20embeddings%20generated%20by%20pre-trained%20feature%0Aextractors.%20Inspired%20by%20these%20developments%2C%20we%20investigate%20the%20adaptation%20of%0Asuch%20algorithms%20to%20the%20audio%20domain%20to%20address%20the%20problem%20of%20Audio%20Anomaly%0ADetection%20%28AAD%29.%20Unlike%20most%20existing%20AAD%20methods%2C%20which%20primarily%20classify%0Aanomalous%20samples%2C%20our%20approach%20introduces%20fine-grained%20temporal-frequency%0Alocalization%20of%20anomalies%20within%20the%20spectrogram%2C%20significantly%20improving%0Aexplainability.%20This%20capability%20enables%20a%20more%20precise%20understanding%20of%20where%0Aand%20when%20anomalies%20occur%2C%20making%20the%20results%20more%20actionable%20for%20end%20users.%20We%0Aevaluate%20our%20approach%20on%20industrial%20and%20environmental%20benchmarks%2C%20demonstrating%0Athe%20effectiveness%20of%20VAD%20techniques%20in%20detecting%20anomalies%20in%20audio%20signals.%0AMoreover%2C%20they%20improve%20explainability%20by%20enabling%20localized%20anomaly%0Aidentification%2C%20making%20audio%20anomaly%20detection%20systems%20more%20interpretable%20and%0Apractical.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18328v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Vision%2520to%2520Sound%253A%2520Advancing%2520Audio%2520Anomaly%2520Detection%2520with%250A%2520%2520Vision-Based%2520Algorithms%26entry.906535625%3DManuel%2520Barusco%2520and%2520Francesco%2520Borsatti%2520and%2520Davide%2520Dalle%2520Pezze%2520and%2520Francesco%2520Paissan%2520and%2520Elisabetta%2520Farella%2520and%2520Gian%2520Antonio%2520Susto%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Visual%2520Anomaly%2520Detection%2520%2528VAD%2529%2520have%2520introduced%250Asophisticated%2520algorithms%2520leveraging%2520embeddings%2520generated%2520by%2520pre-trained%2520feature%250Aextractors.%2520Inspired%2520by%2520these%2520developments%252C%2520we%2520investigate%2520the%2520adaptation%2520of%250Asuch%2520algorithms%2520to%2520the%2520audio%2520domain%2520to%2520address%2520the%2520problem%2520of%2520Audio%2520Anomaly%250ADetection%2520%2528AAD%2529.%2520Unlike%2520most%2520existing%2520AAD%2520methods%252C%2520which%2520primarily%2520classify%250Aanomalous%2520samples%252C%2520our%2520approach%2520introduces%2520fine-grained%2520temporal-frequency%250Alocalization%2520of%2520anomalies%2520within%2520the%2520spectrogram%252C%2520significantly%2520improving%250Aexplainability.%2520This%2520capability%2520enables%2520a%2520more%2520precise%2520understanding%2520of%2520where%250Aand%2520when%2520anomalies%2520occur%252C%2520making%2520the%2520results%2520more%2520actionable%2520for%2520end%2520users.%2520We%250Aevaluate%2520our%2520approach%2520on%2520industrial%2520and%2520environmental%2520benchmarks%252C%2520demonstrating%250Athe%2520effectiveness%2520of%2520VAD%2520techniques%2520in%2520detecting%2520anomalies%2520in%2520audio%2520signals.%250AMoreover%252C%2520they%2520improve%2520explainability%2520by%2520enabling%2520localized%2520anomaly%250Aidentification%252C%2520making%2520audio%2520anomaly%2520detection%2520systems%2520more%2520interpretable%2520and%250Apractical.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18328v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Vision%20to%20Sound%3A%20Advancing%20Audio%20Anomaly%20Detection%20with%0A%20%20Vision-Based%20Algorithms&entry.906535625=Manuel%20Barusco%20and%20Francesco%20Borsatti%20and%20Davide%20Dalle%20Pezze%20and%20Francesco%20Paissan%20and%20Elisabetta%20Farella%20and%20Gian%20Antonio%20Susto&entry.1292438233=%20%20Recent%20advances%20in%20Visual%20Anomaly%20Detection%20%28VAD%29%20have%20introduced%0Asophisticated%20algorithms%20leveraging%20embeddings%20generated%20by%20pre-trained%20feature%0Aextractors.%20Inspired%20by%20these%20developments%2C%20we%20investigate%20the%20adaptation%20of%0Asuch%20algorithms%20to%20the%20audio%20domain%20to%20address%20the%20problem%20of%20Audio%20Anomaly%0ADetection%20%28AAD%29.%20Unlike%20most%20existing%20AAD%20methods%2C%20which%20primarily%20classify%0Aanomalous%20samples%2C%20our%20approach%20introduces%20fine-grained%20temporal-frequency%0Alocalization%20of%20anomalies%20within%20the%20spectrogram%2C%20significantly%20improving%0Aexplainability.%20This%20capability%20enables%20a%20more%20precise%20understanding%20of%20where%0Aand%20when%20anomalies%20occur%2C%20making%20the%20results%20more%20actionable%20for%20end%20users.%20We%0Aevaluate%20our%20approach%20on%20industrial%20and%20environmental%20benchmarks%2C%20demonstrating%0Athe%20effectiveness%20of%20VAD%20techniques%20in%20detecting%20anomalies%20in%20audio%20signals.%0AMoreover%2C%20they%20improve%20explainability%20by%20enabling%20localized%20anomaly%0Aidentification%2C%20making%20audio%20anomaly%20detection%20systems%20more%20interpretable%20and%0Apractical.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18328v1&entry.124074799=Read"},
{"title": "TinySubNets: An efficient and low capacity continual learning strategy", "author": "Marcin Pietro\u0144 and Kamil Faber and Dominik \u017burek and Roberto Corizzo", "abstract": "  Continual Learning (CL) is a highly relevant setting gaining traction in\nrecent machine learning research. Among CL works, architectural and hybrid\nstrategies are particularly effective due to their potential to adapt the model\narchitecture as new tasks are presented. However, many existing solutions do\nnot efficiently exploit model sparsity, and are prone to capacity saturation\ndue to their inefficient use of available weights, which limits the number of\nlearnable tasks. In this paper, we propose TinySubNets (TSN), a novel\narchitectural CL strategy that addresses the issues through the unique\ncombination of pruning with different sparsity levels, adaptive quantization,\nand weight sharing. Pruning identifies a subset of weights that preserve model\nperformance, making less relevant weights available for future tasks. Adaptive\nquantization allows a single weight to be separated into multiple parts which\ncan be assigned to different tasks. Weight sharing between tasks boosts the\nexploitation of capacity and task similarity, allowing for the identification\nof a better trade-off between model accuracy and capacity. These features allow\nTSN to efficiently leverage the available capacity, enhance knowledge transfer,\nand reduce computational resource consumption. Experimental results involving\ncommon benchmark CL datasets and scenarios show that our proposed strategy\nachieves better results in terms of accuracy than existing state-of-the-art CL\nstrategies. Moreover, our strategy is shown to provide a significantly improved\nmodel capacity exploitation. Code released at:\nhttps://github.com/lifelonglab/tinysubnets.\n", "link": "http://arxiv.org/abs/2412.10869v2", "date": "2025-02-25", "relevancy": 2.5983, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5518}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5068}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TinySubNets%3A%20An%20efficient%20and%20low%20capacity%20continual%20learning%20strategy&body=Title%3A%20TinySubNets%3A%20An%20efficient%20and%20low%20capacity%20continual%20learning%20strategy%0AAuthor%3A%20Marcin%20Pietro%C5%84%20and%20Kamil%20Faber%20and%20Dominik%20%C5%BBurek%20and%20Roberto%20Corizzo%0AAbstract%3A%20%20%20Continual%20Learning%20%28CL%29%20is%20a%20highly%20relevant%20setting%20gaining%20traction%20in%0Arecent%20machine%20learning%20research.%20Among%20CL%20works%2C%20architectural%20and%20hybrid%0Astrategies%20are%20particularly%20effective%20due%20to%20their%20potential%20to%20adapt%20the%20model%0Aarchitecture%20as%20new%20tasks%20are%20presented.%20However%2C%20many%20existing%20solutions%20do%0Anot%20efficiently%20exploit%20model%20sparsity%2C%20and%20are%20prone%20to%20capacity%20saturation%0Adue%20to%20their%20inefficient%20use%20of%20available%20weights%2C%20which%20limits%20the%20number%20of%0Alearnable%20tasks.%20In%20this%20paper%2C%20we%20propose%20TinySubNets%20%28TSN%29%2C%20a%20novel%0Aarchitectural%20CL%20strategy%20that%20addresses%20the%20issues%20through%20the%20unique%0Acombination%20of%20pruning%20with%20different%20sparsity%20levels%2C%20adaptive%20quantization%2C%0Aand%20weight%20sharing.%20Pruning%20identifies%20a%20subset%20of%20weights%20that%20preserve%20model%0Aperformance%2C%20making%20less%20relevant%20weights%20available%20for%20future%20tasks.%20Adaptive%0Aquantization%20allows%20a%20single%20weight%20to%20be%20separated%20into%20multiple%20parts%20which%0Acan%20be%20assigned%20to%20different%20tasks.%20Weight%20sharing%20between%20tasks%20boosts%20the%0Aexploitation%20of%20capacity%20and%20task%20similarity%2C%20allowing%20for%20the%20identification%0Aof%20a%20better%20trade-off%20between%20model%20accuracy%20and%20capacity.%20These%20features%20allow%0ATSN%20to%20efficiently%20leverage%20the%20available%20capacity%2C%20enhance%20knowledge%20transfer%2C%0Aand%20reduce%20computational%20resource%20consumption.%20Experimental%20results%20involving%0Acommon%20benchmark%20CL%20datasets%20and%20scenarios%20show%20that%20our%20proposed%20strategy%0Aachieves%20better%20results%20in%20terms%20of%20accuracy%20than%20existing%20state-of-the-art%20CL%0Astrategies.%20Moreover%2C%20our%20strategy%20is%20shown%20to%20provide%20a%20significantly%20improved%0Amodel%20capacity%20exploitation.%20Code%20released%20at%3A%0Ahttps%3A//github.com/lifelonglab/tinysubnets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10869v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTinySubNets%253A%2520An%2520efficient%2520and%2520low%2520capacity%2520continual%2520learning%2520strategy%26entry.906535625%3DMarcin%2520Pietro%25C5%2584%2520and%2520Kamil%2520Faber%2520and%2520Dominik%2520%25C5%25BBurek%2520and%2520Roberto%2520Corizzo%26entry.1292438233%3D%2520%2520Continual%2520Learning%2520%2528CL%2529%2520is%2520a%2520highly%2520relevant%2520setting%2520gaining%2520traction%2520in%250Arecent%2520machine%2520learning%2520research.%2520Among%2520CL%2520works%252C%2520architectural%2520and%2520hybrid%250Astrategies%2520are%2520particularly%2520effective%2520due%2520to%2520their%2520potential%2520to%2520adapt%2520the%2520model%250Aarchitecture%2520as%2520new%2520tasks%2520are%2520presented.%2520However%252C%2520many%2520existing%2520solutions%2520do%250Anot%2520efficiently%2520exploit%2520model%2520sparsity%252C%2520and%2520are%2520prone%2520to%2520capacity%2520saturation%250Adue%2520to%2520their%2520inefficient%2520use%2520of%2520available%2520weights%252C%2520which%2520limits%2520the%2520number%2520of%250Alearnable%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520TinySubNets%2520%2528TSN%2529%252C%2520a%2520novel%250Aarchitectural%2520CL%2520strategy%2520that%2520addresses%2520the%2520issues%2520through%2520the%2520unique%250Acombination%2520of%2520pruning%2520with%2520different%2520sparsity%2520levels%252C%2520adaptive%2520quantization%252C%250Aand%2520weight%2520sharing.%2520Pruning%2520identifies%2520a%2520subset%2520of%2520weights%2520that%2520preserve%2520model%250Aperformance%252C%2520making%2520less%2520relevant%2520weights%2520available%2520for%2520future%2520tasks.%2520Adaptive%250Aquantization%2520allows%2520a%2520single%2520weight%2520to%2520be%2520separated%2520into%2520multiple%2520parts%2520which%250Acan%2520be%2520assigned%2520to%2520different%2520tasks.%2520Weight%2520sharing%2520between%2520tasks%2520boosts%2520the%250Aexploitation%2520of%2520capacity%2520and%2520task%2520similarity%252C%2520allowing%2520for%2520the%2520identification%250Aof%2520a%2520better%2520trade-off%2520between%2520model%2520accuracy%2520and%2520capacity.%2520These%2520features%2520allow%250ATSN%2520to%2520efficiently%2520leverage%2520the%2520available%2520capacity%252C%2520enhance%2520knowledge%2520transfer%252C%250Aand%2520reduce%2520computational%2520resource%2520consumption.%2520Experimental%2520results%2520involving%250Acommon%2520benchmark%2520CL%2520datasets%2520and%2520scenarios%2520show%2520that%2520our%2520proposed%2520strategy%250Aachieves%2520better%2520results%2520in%2520terms%2520of%2520accuracy%2520than%2520existing%2520state-of-the-art%2520CL%250Astrategies.%2520Moreover%252C%2520our%2520strategy%2520is%2520shown%2520to%2520provide%2520a%2520significantly%2520improved%250Amodel%2520capacity%2520exploitation.%2520Code%2520released%2520at%253A%250Ahttps%253A//github.com/lifelonglab/tinysubnets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10869v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TinySubNets%3A%20An%20efficient%20and%20low%20capacity%20continual%20learning%20strategy&entry.906535625=Marcin%20Pietro%C5%84%20and%20Kamil%20Faber%20and%20Dominik%20%C5%BBurek%20and%20Roberto%20Corizzo&entry.1292438233=%20%20Continual%20Learning%20%28CL%29%20is%20a%20highly%20relevant%20setting%20gaining%20traction%20in%0Arecent%20machine%20learning%20research.%20Among%20CL%20works%2C%20architectural%20and%20hybrid%0Astrategies%20are%20particularly%20effective%20due%20to%20their%20potential%20to%20adapt%20the%20model%0Aarchitecture%20as%20new%20tasks%20are%20presented.%20However%2C%20many%20existing%20solutions%20do%0Anot%20efficiently%20exploit%20model%20sparsity%2C%20and%20are%20prone%20to%20capacity%20saturation%0Adue%20to%20their%20inefficient%20use%20of%20available%20weights%2C%20which%20limits%20the%20number%20of%0Alearnable%20tasks.%20In%20this%20paper%2C%20we%20propose%20TinySubNets%20%28TSN%29%2C%20a%20novel%0Aarchitectural%20CL%20strategy%20that%20addresses%20the%20issues%20through%20the%20unique%0Acombination%20of%20pruning%20with%20different%20sparsity%20levels%2C%20adaptive%20quantization%2C%0Aand%20weight%20sharing.%20Pruning%20identifies%20a%20subset%20of%20weights%20that%20preserve%20model%0Aperformance%2C%20making%20less%20relevant%20weights%20available%20for%20future%20tasks.%20Adaptive%0Aquantization%20allows%20a%20single%20weight%20to%20be%20separated%20into%20multiple%20parts%20which%0Acan%20be%20assigned%20to%20different%20tasks.%20Weight%20sharing%20between%20tasks%20boosts%20the%0Aexploitation%20of%20capacity%20and%20task%20similarity%2C%20allowing%20for%20the%20identification%0Aof%20a%20better%20trade-off%20between%20model%20accuracy%20and%20capacity.%20These%20features%20allow%0ATSN%20to%20efficiently%20leverage%20the%20available%20capacity%2C%20enhance%20knowledge%20transfer%2C%0Aand%20reduce%20computational%20resource%20consumption.%20Experimental%20results%20involving%0Acommon%20benchmark%20CL%20datasets%20and%20scenarios%20show%20that%20our%20proposed%20strategy%0Aachieves%20better%20results%20in%20terms%20of%20accuracy%20than%20existing%20state-of-the-art%20CL%0Astrategies.%20Moreover%2C%20our%20strategy%20is%20shown%20to%20provide%20a%20significantly%20improved%0Amodel%20capacity%20exploitation.%20Code%20released%20at%3A%0Ahttps%3A//github.com/lifelonglab/tinysubnets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10869v2&entry.124074799=Read"},
{"title": "Implicit In-context Learning", "author": "Zhuowei Li and Zihao Xu and Ligong Han and Yunhe Gao and Song Wen and Di Liu and Hao Wang and Dimitris N. Metaxas", "abstract": "  In-context Learning (ICL) empowers large language models (LLMs) to swiftly\nadapt to unseen tasks at inference-time by prefixing a few demonstration\nexamples before queries. Despite its versatility, ICL incurs substantial\ncomputational and memory overheads compared to zero-shot learning and is\nsensitive to the selection and order of demonstration examples. In this work,\nwe introduce Implicit In-context Learning (I2CL), an innovative paradigm that\nreduces the inference cost of ICL to that of zero-shot learning with minimal\ninformation loss. I2CL operates by first generating a condensed vector\nrepresentation, namely a context vector, extracted from the demonstration\nexamples. It then conducts an inference-time intervention through injecting a\nlinear combination of the context vector and query activations back into the\nmodel's residual streams. Empirical evaluation on nine real-world tasks across\nthree model architectures demonstrates that I2CL achieves few-shot level\nperformance at zero-shot inference cost, and it exhibits robustness against\nvariations in demonstration examples. Furthermore, I2CL facilitates a novel\nrepresentation of task-ids, enhancing task similarity detection and fostering\neffective transfer learning. We also perform a comprehensive analysis and\nablation study on I2CL, offering deeper insights into its internal mechanisms.\nCode is available at https://github.com/LzVv123456/I2CL.\n", "link": "http://arxiv.org/abs/2405.14660v2", "date": "2025-02-25", "relevancy": 2.5835, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5507}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4997}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20In-context%20Learning&body=Title%3A%20Implicit%20In-context%20Learning%0AAuthor%3A%20Zhuowei%20Li%20and%20Zihao%20Xu%20and%20Ligong%20Han%20and%20Yunhe%20Gao%20and%20Song%20Wen%20and%20Di%20Liu%20and%20Hao%20Wang%20and%20Dimitris%20N.%20Metaxas%0AAbstract%3A%20%20%20In-context%20Learning%20%28ICL%29%20empowers%20large%20language%20models%20%28LLMs%29%20to%20swiftly%0Aadapt%20to%20unseen%20tasks%20at%20inference-time%20by%20prefixing%20a%20few%20demonstration%0Aexamples%20before%20queries.%20Despite%20its%20versatility%2C%20ICL%20incurs%20substantial%0Acomputational%20and%20memory%20overheads%20compared%20to%20zero-shot%20learning%20and%20is%0Asensitive%20to%20the%20selection%20and%20order%20of%20demonstration%20examples.%20In%20this%20work%2C%0Awe%20introduce%20Implicit%20In-context%20Learning%20%28I2CL%29%2C%20an%20innovative%20paradigm%20that%0Areduces%20the%20inference%20cost%20of%20ICL%20to%20that%20of%20zero-shot%20learning%20with%20minimal%0Ainformation%20loss.%20I2CL%20operates%20by%20first%20generating%20a%20condensed%20vector%0Arepresentation%2C%20namely%20a%20context%20vector%2C%20extracted%20from%20the%20demonstration%0Aexamples.%20It%20then%20conducts%20an%20inference-time%20intervention%20through%20injecting%20a%0Alinear%20combination%20of%20the%20context%20vector%20and%20query%20activations%20back%20into%20the%0Amodel%27s%20residual%20streams.%20Empirical%20evaluation%20on%20nine%20real-world%20tasks%20across%0Athree%20model%20architectures%20demonstrates%20that%20I2CL%20achieves%20few-shot%20level%0Aperformance%20at%20zero-shot%20inference%20cost%2C%20and%20it%20exhibits%20robustness%20against%0Avariations%20in%20demonstration%20examples.%20Furthermore%2C%20I2CL%20facilitates%20a%20novel%0Arepresentation%20of%20task-ids%2C%20enhancing%20task%20similarity%20detection%20and%20fostering%0Aeffective%20transfer%20learning.%20We%20also%20perform%20a%20comprehensive%20analysis%20and%0Aablation%20study%20on%20I2CL%2C%20offering%20deeper%20insights%20into%20its%20internal%20mechanisms.%0ACode%20is%20available%20at%20https%3A//github.com/LzVv123456/I2CL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14660v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520In-context%2520Learning%26entry.906535625%3DZhuowei%2520Li%2520and%2520Zihao%2520Xu%2520and%2520Ligong%2520Han%2520and%2520Yunhe%2520Gao%2520and%2520Song%2520Wen%2520and%2520Di%2520Liu%2520and%2520Hao%2520Wang%2520and%2520Dimitris%2520N.%2520Metaxas%26entry.1292438233%3D%2520%2520In-context%2520Learning%2520%2528ICL%2529%2520empowers%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520swiftly%250Aadapt%2520to%2520unseen%2520tasks%2520at%2520inference-time%2520by%2520prefixing%2520a%2520few%2520demonstration%250Aexamples%2520before%2520queries.%2520Despite%2520its%2520versatility%252C%2520ICL%2520incurs%2520substantial%250Acomputational%2520and%2520memory%2520overheads%2520compared%2520to%2520zero-shot%2520learning%2520and%2520is%250Asensitive%2520to%2520the%2520selection%2520and%2520order%2520of%2520demonstration%2520examples.%2520In%2520this%2520work%252C%250Awe%2520introduce%2520Implicit%2520In-context%2520Learning%2520%2528I2CL%2529%252C%2520an%2520innovative%2520paradigm%2520that%250Areduces%2520the%2520inference%2520cost%2520of%2520ICL%2520to%2520that%2520of%2520zero-shot%2520learning%2520with%2520minimal%250Ainformation%2520loss.%2520I2CL%2520operates%2520by%2520first%2520generating%2520a%2520condensed%2520vector%250Arepresentation%252C%2520namely%2520a%2520context%2520vector%252C%2520extracted%2520from%2520the%2520demonstration%250Aexamples.%2520It%2520then%2520conducts%2520an%2520inference-time%2520intervention%2520through%2520injecting%2520a%250Alinear%2520combination%2520of%2520the%2520context%2520vector%2520and%2520query%2520activations%2520back%2520into%2520the%250Amodel%2527s%2520residual%2520streams.%2520Empirical%2520evaluation%2520on%2520nine%2520real-world%2520tasks%2520across%250Athree%2520model%2520architectures%2520demonstrates%2520that%2520I2CL%2520achieves%2520few-shot%2520level%250Aperformance%2520at%2520zero-shot%2520inference%2520cost%252C%2520and%2520it%2520exhibits%2520robustness%2520against%250Avariations%2520in%2520demonstration%2520examples.%2520Furthermore%252C%2520I2CL%2520facilitates%2520a%2520novel%250Arepresentation%2520of%2520task-ids%252C%2520enhancing%2520task%2520similarity%2520detection%2520and%2520fostering%250Aeffective%2520transfer%2520learning.%2520We%2520also%2520perform%2520a%2520comprehensive%2520analysis%2520and%250Aablation%2520study%2520on%2520I2CL%252C%2520offering%2520deeper%2520insights%2520into%2520its%2520internal%2520mechanisms.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/LzVv123456/I2CL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14660v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20In-context%20Learning&entry.906535625=Zhuowei%20Li%20and%20Zihao%20Xu%20and%20Ligong%20Han%20and%20Yunhe%20Gao%20and%20Song%20Wen%20and%20Di%20Liu%20and%20Hao%20Wang%20and%20Dimitris%20N.%20Metaxas&entry.1292438233=%20%20In-context%20Learning%20%28ICL%29%20empowers%20large%20language%20models%20%28LLMs%29%20to%20swiftly%0Aadapt%20to%20unseen%20tasks%20at%20inference-time%20by%20prefixing%20a%20few%20demonstration%0Aexamples%20before%20queries.%20Despite%20its%20versatility%2C%20ICL%20incurs%20substantial%0Acomputational%20and%20memory%20overheads%20compared%20to%20zero-shot%20learning%20and%20is%0Asensitive%20to%20the%20selection%20and%20order%20of%20demonstration%20examples.%20In%20this%20work%2C%0Awe%20introduce%20Implicit%20In-context%20Learning%20%28I2CL%29%2C%20an%20innovative%20paradigm%20that%0Areduces%20the%20inference%20cost%20of%20ICL%20to%20that%20of%20zero-shot%20learning%20with%20minimal%0Ainformation%20loss.%20I2CL%20operates%20by%20first%20generating%20a%20condensed%20vector%0Arepresentation%2C%20namely%20a%20context%20vector%2C%20extracted%20from%20the%20demonstration%0Aexamples.%20It%20then%20conducts%20an%20inference-time%20intervention%20through%20injecting%20a%0Alinear%20combination%20of%20the%20context%20vector%20and%20query%20activations%20back%20into%20the%0Amodel%27s%20residual%20streams.%20Empirical%20evaluation%20on%20nine%20real-world%20tasks%20across%0Athree%20model%20architectures%20demonstrates%20that%20I2CL%20achieves%20few-shot%20level%0Aperformance%20at%20zero-shot%20inference%20cost%2C%20and%20it%20exhibits%20robustness%20against%0Avariations%20in%20demonstration%20examples.%20Furthermore%2C%20I2CL%20facilitates%20a%20novel%0Arepresentation%20of%20task-ids%2C%20enhancing%20task%20similarity%20detection%20and%20fostering%0Aeffective%20transfer%20learning.%20We%20also%20perform%20a%20comprehensive%20analysis%20and%0Aablation%20study%20on%20I2CL%2C%20offering%20deeper%20insights%20into%20its%20internal%20mechanisms.%0ACode%20is%20available%20at%20https%3A//github.com/LzVv123456/I2CL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14660v2&entry.124074799=Read"},
{"title": "DIS-CO: Discovering Copyrighted Content in VLMs Training Data", "author": "Andr\u00e9 V. Duarte and Xuandong Zhao and Arlindo L. Oliveira and Lei Li", "abstract": "  How can we verify whether copyrighted content was used to train a large\nvision-language model (VLM) without direct access to its training data?\nMotivated by the hypothesis that a VLM is able to recognize images from its\ntraining corpus, we propose DIS-CO, a novel approach to infer the inclusion of\ncopyrighted content during the model's development. By repeatedly querying a\nVLM with specific frames from targeted copyrighted material, DIS-CO extracts\nthe content's identity through free-form text completions. To assess its\neffectiveness, we introduce MovieTection, a benchmark comprising 14,000 frames\npaired with detailed captions, drawn from films released both before and after\na model's training cutoff. Our results show that DIS-CO significantly improves\ndetection performance, nearly doubling the average AUC of the best prior method\non models with logits available. Our findings also highlight a broader concern:\nall tested models appear to have been exposed to some extent to copyrighted\ncontent. Our code and data are available at\nhttps://github.com/avduarte333/DIS-CO\n", "link": "http://arxiv.org/abs/2502.17358v2", "date": "2025-02-25", "relevancy": 2.5817, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5186}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5152}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIS-CO%3A%20Discovering%20Copyrighted%20Content%20in%20VLMs%20Training%20Data&body=Title%3A%20DIS-CO%3A%20Discovering%20Copyrighted%20Content%20in%20VLMs%20Training%20Data%0AAuthor%3A%20Andr%C3%A9%20V.%20Duarte%20and%20Xuandong%20Zhao%20and%20Arlindo%20L.%20Oliveira%20and%20Lei%20Li%0AAbstract%3A%20%20%20How%20can%20we%20verify%20whether%20copyrighted%20content%20was%20used%20to%20train%20a%20large%0Avision-language%20model%20%28VLM%29%20without%20direct%20access%20to%20its%20training%20data%3F%0AMotivated%20by%20the%20hypothesis%20that%20a%20VLM%20is%20able%20to%20recognize%20images%20from%20its%0Atraining%20corpus%2C%20we%20propose%20DIS-CO%2C%20a%20novel%20approach%20to%20infer%20the%20inclusion%20of%0Acopyrighted%20content%20during%20the%20model%27s%20development.%20By%20repeatedly%20querying%20a%0AVLM%20with%20specific%20frames%20from%20targeted%20copyrighted%20material%2C%20DIS-CO%20extracts%0Athe%20content%27s%20identity%20through%20free-form%20text%20completions.%20To%20assess%20its%0Aeffectiveness%2C%20we%20introduce%20MovieTection%2C%20a%20benchmark%20comprising%2014%2C000%20frames%0Apaired%20with%20detailed%20captions%2C%20drawn%20from%20films%20released%20both%20before%20and%20after%0Aa%20model%27s%20training%20cutoff.%20Our%20results%20show%20that%20DIS-CO%20significantly%20improves%0Adetection%20performance%2C%20nearly%20doubling%20the%20average%20AUC%20of%20the%20best%20prior%20method%0Aon%20models%20with%20logits%20available.%20Our%20findings%20also%20highlight%20a%20broader%20concern%3A%0Aall%20tested%20models%20appear%20to%20have%20been%20exposed%20to%20some%20extent%20to%20copyrighted%0Acontent.%20Our%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/avduarte333/DIS-CO%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17358v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIS-CO%253A%2520Discovering%2520Copyrighted%2520Content%2520in%2520VLMs%2520Training%2520Data%26entry.906535625%3DAndr%25C3%25A9%2520V.%2520Duarte%2520and%2520Xuandong%2520Zhao%2520and%2520Arlindo%2520L.%2520Oliveira%2520and%2520Lei%2520Li%26entry.1292438233%3D%2520%2520How%2520can%2520we%2520verify%2520whether%2520copyrighted%2520content%2520was%2520used%2520to%2520train%2520a%2520large%250Avision-language%2520model%2520%2528VLM%2529%2520without%2520direct%2520access%2520to%2520its%2520training%2520data%253F%250AMotivated%2520by%2520the%2520hypothesis%2520that%2520a%2520VLM%2520is%2520able%2520to%2520recognize%2520images%2520from%2520its%250Atraining%2520corpus%252C%2520we%2520propose%2520DIS-CO%252C%2520a%2520novel%2520approach%2520to%2520infer%2520the%2520inclusion%2520of%250Acopyrighted%2520content%2520during%2520the%2520model%2527s%2520development.%2520By%2520repeatedly%2520querying%2520a%250AVLM%2520with%2520specific%2520frames%2520from%2520targeted%2520copyrighted%2520material%252C%2520DIS-CO%2520extracts%250Athe%2520content%2527s%2520identity%2520through%2520free-form%2520text%2520completions.%2520To%2520assess%2520its%250Aeffectiveness%252C%2520we%2520introduce%2520MovieTection%252C%2520a%2520benchmark%2520comprising%252014%252C000%2520frames%250Apaired%2520with%2520detailed%2520captions%252C%2520drawn%2520from%2520films%2520released%2520both%2520before%2520and%2520after%250Aa%2520model%2527s%2520training%2520cutoff.%2520Our%2520results%2520show%2520that%2520DIS-CO%2520significantly%2520improves%250Adetection%2520performance%252C%2520nearly%2520doubling%2520the%2520average%2520AUC%2520of%2520the%2520best%2520prior%2520method%250Aon%2520models%2520with%2520logits%2520available.%2520Our%2520findings%2520also%2520highlight%2520a%2520broader%2520concern%253A%250Aall%2520tested%2520models%2520appear%2520to%2520have%2520been%2520exposed%2520to%2520some%2520extent%2520to%2520copyrighted%250Acontent.%2520Our%2520code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/avduarte333/DIS-CO%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17358v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIS-CO%3A%20Discovering%20Copyrighted%20Content%20in%20VLMs%20Training%20Data&entry.906535625=Andr%C3%A9%20V.%20Duarte%20and%20Xuandong%20Zhao%20and%20Arlindo%20L.%20Oliveira%20and%20Lei%20Li&entry.1292438233=%20%20How%20can%20we%20verify%20whether%20copyrighted%20content%20was%20used%20to%20train%20a%20large%0Avision-language%20model%20%28VLM%29%20without%20direct%20access%20to%20its%20training%20data%3F%0AMotivated%20by%20the%20hypothesis%20that%20a%20VLM%20is%20able%20to%20recognize%20images%20from%20its%0Atraining%20corpus%2C%20we%20propose%20DIS-CO%2C%20a%20novel%20approach%20to%20infer%20the%20inclusion%20of%0Acopyrighted%20content%20during%20the%20model%27s%20development.%20By%20repeatedly%20querying%20a%0AVLM%20with%20specific%20frames%20from%20targeted%20copyrighted%20material%2C%20DIS-CO%20extracts%0Athe%20content%27s%20identity%20through%20free-form%20text%20completions.%20To%20assess%20its%0Aeffectiveness%2C%20we%20introduce%20MovieTection%2C%20a%20benchmark%20comprising%2014%2C000%20frames%0Apaired%20with%20detailed%20captions%2C%20drawn%20from%20films%20released%20both%20before%20and%20after%0Aa%20model%27s%20training%20cutoff.%20Our%20results%20show%20that%20DIS-CO%20significantly%20improves%0Adetection%20performance%2C%20nearly%20doubling%20the%20average%20AUC%20of%20the%20best%20prior%20method%0Aon%20models%20with%20logits%20available.%20Our%20findings%20also%20highlight%20a%20broader%20concern%3A%0Aall%20tested%20models%20appear%20to%20have%20been%20exposed%20to%20some%20extent%20to%20copyrighted%0Acontent.%20Our%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/avduarte333/DIS-CO%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17358v2&entry.124074799=Read"},
{"title": "VesselSAM: Leveraging SAM for Aortic Vessel Segmentation with LoRA and\n  Atrous Attention", "author": "Adnan Iltaf and Rayan Merghani Ahmed and Bin Li and Shoujun Zhou", "abstract": "  Medical image segmentation is crucial for clinical diagnosis and treatment\nplanning, particularly for complex anatomical structures like vessels. In this\nwork, we propose VesselSAM, a modified version of the Segmentation Anything\nModel (SAM), specifically designed for aortic vessel segmentation. VesselSAM\nincorporates AtrousLoRA, a novel module that combines Atrous Attention with\nLow-Rank Adaptation (LoRA), to improve segmentation performance. Atrous\nAttention enables the model to capture multi-scale contextual information,\npreserving both fine local details and broader global context. At the same\ntime, LoRA facilitates efficient fine-tuning of the frozen SAM image encoder,\nreducing the number of trainable parameters and ensuring computational\nefficiency. We evaluate VesselSAM on two challenging datasets: the Aortic\nVessel Tree (AVT) dataset and the Type-B Aortic Dissection (TBAD) dataset.\nVesselSAM achieves state-of-the-art performance with DSC scores of 93.50\\%,\n93.25\\%, 93.02\\%, and 93.26\\% across multiple medical centers. Our results\ndemonstrate that VesselSAM delivers high segmentation accuracy while\nsignificantly reducing computational overhead compared to existing large-scale\nmodels. This development paves the way for enhanced AI-based aortic vessel\nsegmentation in clinical environments. The code and models will be released at\nhttps://github.com/Adnan-CAS/AtrousLora.\n", "link": "http://arxiv.org/abs/2502.18185v1", "date": "2025-02-25", "relevancy": 2.5485, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5476}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5018}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VesselSAM%3A%20Leveraging%20SAM%20for%20Aortic%20Vessel%20Segmentation%20with%20LoRA%20and%0A%20%20Atrous%20Attention&body=Title%3A%20VesselSAM%3A%20Leveraging%20SAM%20for%20Aortic%20Vessel%20Segmentation%20with%20LoRA%20and%0A%20%20Atrous%20Attention%0AAuthor%3A%20Adnan%20Iltaf%20and%20Rayan%20Merghani%20Ahmed%20and%20Bin%20Li%20and%20Shoujun%20Zhou%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20is%20crucial%20for%20clinical%20diagnosis%20and%20treatment%0Aplanning%2C%20particularly%20for%20complex%20anatomical%20structures%20like%20vessels.%20In%20this%0Awork%2C%20we%20propose%20VesselSAM%2C%20a%20modified%20version%20of%20the%20Segmentation%20Anything%0AModel%20%28SAM%29%2C%20specifically%20designed%20for%20aortic%20vessel%20segmentation.%20VesselSAM%0Aincorporates%20AtrousLoRA%2C%20a%20novel%20module%20that%20combines%20Atrous%20Attention%20with%0ALow-Rank%20Adaptation%20%28LoRA%29%2C%20to%20improve%20segmentation%20performance.%20Atrous%0AAttention%20enables%20the%20model%20to%20capture%20multi-scale%20contextual%20information%2C%0Apreserving%20both%20fine%20local%20details%20and%20broader%20global%20context.%20At%20the%20same%0Atime%2C%20LoRA%20facilitates%20efficient%20fine-tuning%20of%20the%20frozen%20SAM%20image%20encoder%2C%0Areducing%20the%20number%20of%20trainable%20parameters%20and%20ensuring%20computational%0Aefficiency.%20We%20evaluate%20VesselSAM%20on%20two%20challenging%20datasets%3A%20the%20Aortic%0AVessel%20Tree%20%28AVT%29%20dataset%20and%20the%20Type-B%20Aortic%20Dissection%20%28TBAD%29%20dataset.%0AVesselSAM%20achieves%20state-of-the-art%20performance%20with%20DSC%20scores%20of%2093.50%5C%25%2C%0A93.25%5C%25%2C%2093.02%5C%25%2C%20and%2093.26%5C%25%20across%20multiple%20medical%20centers.%20Our%20results%0Ademonstrate%20that%20VesselSAM%20delivers%20high%20segmentation%20accuracy%20while%0Asignificantly%20reducing%20computational%20overhead%20compared%20to%20existing%20large-scale%0Amodels.%20This%20development%20paves%20the%20way%20for%20enhanced%20AI-based%20aortic%20vessel%0Asegmentation%20in%20clinical%20environments.%20The%20code%20and%20models%20will%20be%20released%20at%0Ahttps%3A//github.com/Adnan-CAS/AtrousLora.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVesselSAM%253A%2520Leveraging%2520SAM%2520for%2520Aortic%2520Vessel%2520Segmentation%2520with%2520LoRA%2520and%250A%2520%2520Atrous%2520Attention%26entry.906535625%3DAdnan%2520Iltaf%2520and%2520Rayan%2520Merghani%2520Ahmed%2520and%2520Bin%2520Li%2520and%2520Shoujun%2520Zhou%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520is%2520crucial%2520for%2520clinical%2520diagnosis%2520and%2520treatment%250Aplanning%252C%2520particularly%2520for%2520complex%2520anatomical%2520structures%2520like%2520vessels.%2520In%2520this%250Awork%252C%2520we%2520propose%2520VesselSAM%252C%2520a%2520modified%2520version%2520of%2520the%2520Segmentation%2520Anything%250AModel%2520%2528SAM%2529%252C%2520specifically%2520designed%2520for%2520aortic%2520vessel%2520segmentation.%2520VesselSAM%250Aincorporates%2520AtrousLoRA%252C%2520a%2520novel%2520module%2520that%2520combines%2520Atrous%2520Attention%2520with%250ALow-Rank%2520Adaptation%2520%2528LoRA%2529%252C%2520to%2520improve%2520segmentation%2520performance.%2520Atrous%250AAttention%2520enables%2520the%2520model%2520to%2520capture%2520multi-scale%2520contextual%2520information%252C%250Apreserving%2520both%2520fine%2520local%2520details%2520and%2520broader%2520global%2520context.%2520At%2520the%2520same%250Atime%252C%2520LoRA%2520facilitates%2520efficient%2520fine-tuning%2520of%2520the%2520frozen%2520SAM%2520image%2520encoder%252C%250Areducing%2520the%2520number%2520of%2520trainable%2520parameters%2520and%2520ensuring%2520computational%250Aefficiency.%2520We%2520evaluate%2520VesselSAM%2520on%2520two%2520challenging%2520datasets%253A%2520the%2520Aortic%250AVessel%2520Tree%2520%2528AVT%2529%2520dataset%2520and%2520the%2520Type-B%2520Aortic%2520Dissection%2520%2528TBAD%2529%2520dataset.%250AVesselSAM%2520achieves%2520state-of-the-art%2520performance%2520with%2520DSC%2520scores%2520of%252093.50%255C%2525%252C%250A93.25%255C%2525%252C%252093.02%255C%2525%252C%2520and%252093.26%255C%2525%2520across%2520multiple%2520medical%2520centers.%2520Our%2520results%250Ademonstrate%2520that%2520VesselSAM%2520delivers%2520high%2520segmentation%2520accuracy%2520while%250Asignificantly%2520reducing%2520computational%2520overhead%2520compared%2520to%2520existing%2520large-scale%250Amodels.%2520This%2520development%2520paves%2520the%2520way%2520for%2520enhanced%2520AI-based%2520aortic%2520vessel%250Asegmentation%2520in%2520clinical%2520environments.%2520The%2520code%2520and%2520models%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/Adnan-CAS/AtrousLora.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VesselSAM%3A%20Leveraging%20SAM%20for%20Aortic%20Vessel%20Segmentation%20with%20LoRA%20and%0A%20%20Atrous%20Attention&entry.906535625=Adnan%20Iltaf%20and%20Rayan%20Merghani%20Ahmed%20and%20Bin%20Li%20and%20Shoujun%20Zhou&entry.1292438233=%20%20Medical%20image%20segmentation%20is%20crucial%20for%20clinical%20diagnosis%20and%20treatment%0Aplanning%2C%20particularly%20for%20complex%20anatomical%20structures%20like%20vessels.%20In%20this%0Awork%2C%20we%20propose%20VesselSAM%2C%20a%20modified%20version%20of%20the%20Segmentation%20Anything%0AModel%20%28SAM%29%2C%20specifically%20designed%20for%20aortic%20vessel%20segmentation.%20VesselSAM%0Aincorporates%20AtrousLoRA%2C%20a%20novel%20module%20that%20combines%20Atrous%20Attention%20with%0ALow-Rank%20Adaptation%20%28LoRA%29%2C%20to%20improve%20segmentation%20performance.%20Atrous%0AAttention%20enables%20the%20model%20to%20capture%20multi-scale%20contextual%20information%2C%0Apreserving%20both%20fine%20local%20details%20and%20broader%20global%20context.%20At%20the%20same%0Atime%2C%20LoRA%20facilitates%20efficient%20fine-tuning%20of%20the%20frozen%20SAM%20image%20encoder%2C%0Areducing%20the%20number%20of%20trainable%20parameters%20and%20ensuring%20computational%0Aefficiency.%20We%20evaluate%20VesselSAM%20on%20two%20challenging%20datasets%3A%20the%20Aortic%0AVessel%20Tree%20%28AVT%29%20dataset%20and%20the%20Type-B%20Aortic%20Dissection%20%28TBAD%29%20dataset.%0AVesselSAM%20achieves%20state-of-the-art%20performance%20with%20DSC%20scores%20of%2093.50%5C%25%2C%0A93.25%5C%25%2C%2093.02%5C%25%2C%20and%2093.26%5C%25%20across%20multiple%20medical%20centers.%20Our%20results%0Ademonstrate%20that%20VesselSAM%20delivers%20high%20segmentation%20accuracy%20while%0Asignificantly%20reducing%20computational%20overhead%20compared%20to%20existing%20large-scale%0Amodels.%20This%20development%20paves%20the%20way%20for%20enhanced%20AI-based%20aortic%20vessel%0Asegmentation%20in%20clinical%20environments.%20The%20code%20and%20models%20will%20be%20released%20at%0Ahttps%3A//github.com/Adnan-CAS/AtrousLora.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18185v1&entry.124074799=Read"},
{"title": "ARLON: Boosting Diffusion Transformers with Autoregressive Models for\n  Long Video Generation", "author": "Zongyi Li and Shujie Hu and Shujie Liu and Long Zhou and Jeongsoo Choi and Lingwei Meng and Xun Guo and Jinyu Li and Hefei Ling and Furu Wei", "abstract": "  Text-to-video models have recently undergone rapid and substantial\nadvancements. Nevertheless, due to limitations in data and computational\nresources, achieving efficient generation of long videos with rich motion\ndynamics remains a significant challenge. To generate high-quality, dynamic,\nand temporally consistent long videos, this paper presents ARLON, a novel\nframework that boosts diffusion Transformers with autoregressive models for\nlong video generation, by integrating the coarse spatial and long-range\ntemporal information provided by the AR model to guide the DiT model.\nSpecifically, ARLON incorporates several key innovations: 1) A latent Vector\nQuantized Variational Autoencoder (VQ-VAE) compresses the input latent space of\nthe DiT model into compact visual tokens, bridging the AR and DiT models and\nbalancing the learning complexity and information density; 2) An adaptive\nnorm-based semantic injection module integrates the coarse discrete visual\nunits from the AR model into the DiT model, ensuring effective guidance during\nvideo generation; 3) To enhance the tolerance capability of noise introduced\nfrom the AR inference, the DiT model is trained with coarser visual latent\ntokens incorporated with an uncertainty sampling module. Experimental results\ndemonstrate that ARLON significantly outperforms the baseline OpenSora-V1.2 on\neight out of eleven metrics selected from VBench, with notable improvements in\ndynamic degree and aesthetic quality, while delivering competitive results on\nthe remaining three and simultaneously accelerating the generation process. In\naddition, ARLON achieves state-of-the-art performance in long video generation.\nDetailed analyses of the improvements in inference efficiency are presented,\nalongside a practical application that demonstrates the generation of long\nvideos using progressive text prompts. See demos of ARLON at\nhttp://aka.ms/arlon.\n", "link": "http://arxiv.org/abs/2410.20502v2", "date": "2025-02-25", "relevancy": 2.5435, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6654}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6344}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARLON%3A%20Boosting%20Diffusion%20Transformers%20with%20Autoregressive%20Models%20for%0A%20%20Long%20Video%20Generation&body=Title%3A%20ARLON%3A%20Boosting%20Diffusion%20Transformers%20with%20Autoregressive%20Models%20for%0A%20%20Long%20Video%20Generation%0AAuthor%3A%20Zongyi%20Li%20and%20Shujie%20Hu%20and%20Shujie%20Liu%20and%20Long%20Zhou%20and%20Jeongsoo%20Choi%20and%20Lingwei%20Meng%20and%20Xun%20Guo%20and%20Jinyu%20Li%20and%20Hefei%20Ling%20and%20Furu%20Wei%0AAbstract%3A%20%20%20Text-to-video%20models%20have%20recently%20undergone%20rapid%20and%20substantial%0Aadvancements.%20Nevertheless%2C%20due%20to%20limitations%20in%20data%20and%20computational%0Aresources%2C%20achieving%20efficient%20generation%20of%20long%20videos%20with%20rich%20motion%0Adynamics%20remains%20a%20significant%20challenge.%20To%20generate%20high-quality%2C%20dynamic%2C%0Aand%20temporally%20consistent%20long%20videos%2C%20this%20paper%20presents%20ARLON%2C%20a%20novel%0Aframework%20that%20boosts%20diffusion%20Transformers%20with%20autoregressive%20models%20for%0Along%20video%20generation%2C%20by%20integrating%20the%20coarse%20spatial%20and%20long-range%0Atemporal%20information%20provided%20by%20the%20AR%20model%20to%20guide%20the%20DiT%20model.%0ASpecifically%2C%20ARLON%20incorporates%20several%20key%20innovations%3A%201%29%20A%20latent%20Vector%0AQuantized%20Variational%20Autoencoder%20%28VQ-VAE%29%20compresses%20the%20input%20latent%20space%20of%0Athe%20DiT%20model%20into%20compact%20visual%20tokens%2C%20bridging%20the%20AR%20and%20DiT%20models%20and%0Abalancing%20the%20learning%20complexity%20and%20information%20density%3B%202%29%20An%20adaptive%0Anorm-based%20semantic%20injection%20module%20integrates%20the%20coarse%20discrete%20visual%0Aunits%20from%20the%20AR%20model%20into%20the%20DiT%20model%2C%20ensuring%20effective%20guidance%20during%0Avideo%20generation%3B%203%29%20To%20enhance%20the%20tolerance%20capability%20of%20noise%20introduced%0Afrom%20the%20AR%20inference%2C%20the%20DiT%20model%20is%20trained%20with%20coarser%20visual%20latent%0Atokens%20incorporated%20with%20an%20uncertainty%20sampling%20module.%20Experimental%20results%0Ademonstrate%20that%20ARLON%20significantly%20outperforms%20the%20baseline%20OpenSora-V1.2%20on%0Aeight%20out%20of%20eleven%20metrics%20selected%20from%20VBench%2C%20with%20notable%20improvements%20in%0Adynamic%20degree%20and%20aesthetic%20quality%2C%20while%20delivering%20competitive%20results%20on%0Athe%20remaining%20three%20and%20simultaneously%20accelerating%20the%20generation%20process.%20In%0Aaddition%2C%20ARLON%20achieves%20state-of-the-art%20performance%20in%20long%20video%20generation.%0ADetailed%20analyses%20of%20the%20improvements%20in%20inference%20efficiency%20are%20presented%2C%0Aalongside%20a%20practical%20application%20that%20demonstrates%20the%20generation%20of%20long%0Avideos%20using%20progressive%20text%20prompts.%20See%20demos%20of%20ARLON%20at%0Ahttp%3A//aka.ms/arlon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20502v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARLON%253A%2520Boosting%2520Diffusion%2520Transformers%2520with%2520Autoregressive%2520Models%2520for%250A%2520%2520Long%2520Video%2520Generation%26entry.906535625%3DZongyi%2520Li%2520and%2520Shujie%2520Hu%2520and%2520Shujie%2520Liu%2520and%2520Long%2520Zhou%2520and%2520Jeongsoo%2520Choi%2520and%2520Lingwei%2520Meng%2520and%2520Xun%2520Guo%2520and%2520Jinyu%2520Li%2520and%2520Hefei%2520Ling%2520and%2520Furu%2520Wei%26entry.1292438233%3D%2520%2520Text-to-video%2520models%2520have%2520recently%2520undergone%2520rapid%2520and%2520substantial%250Aadvancements.%2520Nevertheless%252C%2520due%2520to%2520limitations%2520in%2520data%2520and%2520computational%250Aresources%252C%2520achieving%2520efficient%2520generation%2520of%2520long%2520videos%2520with%2520rich%2520motion%250Adynamics%2520remains%2520a%2520significant%2520challenge.%2520To%2520generate%2520high-quality%252C%2520dynamic%252C%250Aand%2520temporally%2520consistent%2520long%2520videos%252C%2520this%2520paper%2520presents%2520ARLON%252C%2520a%2520novel%250Aframework%2520that%2520boosts%2520diffusion%2520Transformers%2520with%2520autoregressive%2520models%2520for%250Along%2520video%2520generation%252C%2520by%2520integrating%2520the%2520coarse%2520spatial%2520and%2520long-range%250Atemporal%2520information%2520provided%2520by%2520the%2520AR%2520model%2520to%2520guide%2520the%2520DiT%2520model.%250ASpecifically%252C%2520ARLON%2520incorporates%2520several%2520key%2520innovations%253A%25201%2529%2520A%2520latent%2520Vector%250AQuantized%2520Variational%2520Autoencoder%2520%2528VQ-VAE%2529%2520compresses%2520the%2520input%2520latent%2520space%2520of%250Athe%2520DiT%2520model%2520into%2520compact%2520visual%2520tokens%252C%2520bridging%2520the%2520AR%2520and%2520DiT%2520models%2520and%250Abalancing%2520the%2520learning%2520complexity%2520and%2520information%2520density%253B%25202%2529%2520An%2520adaptive%250Anorm-based%2520semantic%2520injection%2520module%2520integrates%2520the%2520coarse%2520discrete%2520visual%250Aunits%2520from%2520the%2520AR%2520model%2520into%2520the%2520DiT%2520model%252C%2520ensuring%2520effective%2520guidance%2520during%250Avideo%2520generation%253B%25203%2529%2520To%2520enhance%2520the%2520tolerance%2520capability%2520of%2520noise%2520introduced%250Afrom%2520the%2520AR%2520inference%252C%2520the%2520DiT%2520model%2520is%2520trained%2520with%2520coarser%2520visual%2520latent%250Atokens%2520incorporated%2520with%2520an%2520uncertainty%2520sampling%2520module.%2520Experimental%2520results%250Ademonstrate%2520that%2520ARLON%2520significantly%2520outperforms%2520the%2520baseline%2520OpenSora-V1.2%2520on%250Aeight%2520out%2520of%2520eleven%2520metrics%2520selected%2520from%2520VBench%252C%2520with%2520notable%2520improvements%2520in%250Adynamic%2520degree%2520and%2520aesthetic%2520quality%252C%2520while%2520delivering%2520competitive%2520results%2520on%250Athe%2520remaining%2520three%2520and%2520simultaneously%2520accelerating%2520the%2520generation%2520process.%2520In%250Aaddition%252C%2520ARLON%2520achieves%2520state-of-the-art%2520performance%2520in%2520long%2520video%2520generation.%250ADetailed%2520analyses%2520of%2520the%2520improvements%2520in%2520inference%2520efficiency%2520are%2520presented%252C%250Aalongside%2520a%2520practical%2520application%2520that%2520demonstrates%2520the%2520generation%2520of%2520long%250Avideos%2520using%2520progressive%2520text%2520prompts.%2520See%2520demos%2520of%2520ARLON%2520at%250Ahttp%253A//aka.ms/arlon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20502v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARLON%3A%20Boosting%20Diffusion%20Transformers%20with%20Autoregressive%20Models%20for%0A%20%20Long%20Video%20Generation&entry.906535625=Zongyi%20Li%20and%20Shujie%20Hu%20and%20Shujie%20Liu%20and%20Long%20Zhou%20and%20Jeongsoo%20Choi%20and%20Lingwei%20Meng%20and%20Xun%20Guo%20and%20Jinyu%20Li%20and%20Hefei%20Ling%20and%20Furu%20Wei&entry.1292438233=%20%20Text-to-video%20models%20have%20recently%20undergone%20rapid%20and%20substantial%0Aadvancements.%20Nevertheless%2C%20due%20to%20limitations%20in%20data%20and%20computational%0Aresources%2C%20achieving%20efficient%20generation%20of%20long%20videos%20with%20rich%20motion%0Adynamics%20remains%20a%20significant%20challenge.%20To%20generate%20high-quality%2C%20dynamic%2C%0Aand%20temporally%20consistent%20long%20videos%2C%20this%20paper%20presents%20ARLON%2C%20a%20novel%0Aframework%20that%20boosts%20diffusion%20Transformers%20with%20autoregressive%20models%20for%0Along%20video%20generation%2C%20by%20integrating%20the%20coarse%20spatial%20and%20long-range%0Atemporal%20information%20provided%20by%20the%20AR%20model%20to%20guide%20the%20DiT%20model.%0ASpecifically%2C%20ARLON%20incorporates%20several%20key%20innovations%3A%201%29%20A%20latent%20Vector%0AQuantized%20Variational%20Autoencoder%20%28VQ-VAE%29%20compresses%20the%20input%20latent%20space%20of%0Athe%20DiT%20model%20into%20compact%20visual%20tokens%2C%20bridging%20the%20AR%20and%20DiT%20models%20and%0Abalancing%20the%20learning%20complexity%20and%20information%20density%3B%202%29%20An%20adaptive%0Anorm-based%20semantic%20injection%20module%20integrates%20the%20coarse%20discrete%20visual%0Aunits%20from%20the%20AR%20model%20into%20the%20DiT%20model%2C%20ensuring%20effective%20guidance%20during%0Avideo%20generation%3B%203%29%20To%20enhance%20the%20tolerance%20capability%20of%20noise%20introduced%0Afrom%20the%20AR%20inference%2C%20the%20DiT%20model%20is%20trained%20with%20coarser%20visual%20latent%0Atokens%20incorporated%20with%20an%20uncertainty%20sampling%20module.%20Experimental%20results%0Ademonstrate%20that%20ARLON%20significantly%20outperforms%20the%20baseline%20OpenSora-V1.2%20on%0Aeight%20out%20of%20eleven%20metrics%20selected%20from%20VBench%2C%20with%20notable%20improvements%20in%0Adynamic%20degree%20and%20aesthetic%20quality%2C%20while%20delivering%20competitive%20results%20on%0Athe%20remaining%20three%20and%20simultaneously%20accelerating%20the%20generation%20process.%20In%0Aaddition%2C%20ARLON%20achieves%20state-of-the-art%20performance%20in%20long%20video%20generation.%0ADetailed%20analyses%20of%20the%20improvements%20in%20inference%20efficiency%20are%20presented%2C%0Aalongside%20a%20practical%20application%20that%20demonstrates%20the%20generation%20of%20long%0Avideos%20using%20progressive%20text%20prompts.%20See%20demos%20of%20ARLON%20at%0Ahttp%3A//aka.ms/arlon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20502v2&entry.124074799=Read"},
{"title": "WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio\n  Language Modeling", "author": "Shengpeng Ji and Ziyue Jiang and Wen Wang and Yifu Chen and Minghui Fang and Jialong Zuo and Qian Yang and Xize Cheng and Zehan Wang and Ruiqi Li and Ziang Zhang and Xiaoda Yang and Rongjie Huang and Yidi Jiang and Qian Chen and Siqi Zheng and Zhou Zhao", "abstract": "  Language models have been effectively applied to modeling natural signals,\nsuch as images, video, speech, and audio. A crucial component of these models\nis the codec tokenizer, which compresses high-dimensional natural signals into\nlower-dimensional discrete tokens. In this paper, we introduce WavTokenizer,\nwhich offers several advantages over previous SOTA acoustic codec models in the\naudio domain: 1)extreme compression. By compressing the layers of quantizers\nand the temporal dimension of the discrete codec, one-second audio of 24kHz\nsampling rate requires only a single quantizer with 40 or 75 tokens. 2)improved\nsubjective quality. Despite the reduced number of tokens, WavTokenizer achieves\nstate-of-the-art reconstruction quality with outstanding UTMOS scores and\ninherently contains richer semantic information. Specifically, we achieve these\nresults by designing a broader VQ space, extended contextual windows, and\nimproved attention networks, as well as introducing a powerful multi-scale\ndiscriminator and an inverse Fourier transform structure. We conducted\nextensive reconstruction experiments in the domains of speech, audio, and\nmusic. WavTokenizer exhibited strong performance across various objective and\nsubjective metrics compared to state-of-the-art models. We also tested semantic\ninformation, VQ utilization, and adaptability to generative models.\nComprehensive ablation studies confirm the necessity of each module in\nWavTokenizer. The related code, demos, and pre-trained models are available at\nhttps://github.com/jishengpeng/WavTokenizer.\n", "link": "http://arxiv.org/abs/2408.16532v3", "date": "2025-02-25", "relevancy": 2.5376, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5313}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4956}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WavTokenizer%3A%20an%20Efficient%20Acoustic%20Discrete%20Codec%20Tokenizer%20for%20Audio%0A%20%20Language%20Modeling&body=Title%3A%20WavTokenizer%3A%20an%20Efficient%20Acoustic%20Discrete%20Codec%20Tokenizer%20for%20Audio%0A%20%20Language%20Modeling%0AAuthor%3A%20Shengpeng%20Ji%20and%20Ziyue%20Jiang%20and%20Wen%20Wang%20and%20Yifu%20Chen%20and%20Minghui%20Fang%20and%20Jialong%20Zuo%20and%20Qian%20Yang%20and%20Xize%20Cheng%20and%20Zehan%20Wang%20and%20Ruiqi%20Li%20and%20Ziang%20Zhang%20and%20Xiaoda%20Yang%20and%20Rongjie%20Huang%20and%20Yidi%20Jiang%20and%20Qian%20Chen%20and%20Siqi%20Zheng%20and%20Zhou%20Zhao%0AAbstract%3A%20%20%20Language%20models%20have%20been%20effectively%20applied%20to%20modeling%20natural%20signals%2C%0Asuch%20as%20images%2C%20video%2C%20speech%2C%20and%20audio.%20A%20crucial%20component%20of%20these%20models%0Ais%20the%20codec%20tokenizer%2C%20which%20compresses%20high-dimensional%20natural%20signals%20into%0Alower-dimensional%20discrete%20tokens.%20In%20this%20paper%2C%20we%20introduce%20WavTokenizer%2C%0Awhich%20offers%20several%20advantages%20over%20previous%20SOTA%20acoustic%20codec%20models%20in%20the%0Aaudio%20domain%3A%201%29extreme%20compression.%20By%20compressing%20the%20layers%20of%20quantizers%0Aand%20the%20temporal%20dimension%20of%20the%20discrete%20codec%2C%20one-second%20audio%20of%2024kHz%0Asampling%20rate%20requires%20only%20a%20single%20quantizer%20with%2040%20or%2075%20tokens.%202%29improved%0Asubjective%20quality.%20Despite%20the%20reduced%20number%20of%20tokens%2C%20WavTokenizer%20achieves%0Astate-of-the-art%20reconstruction%20quality%20with%20outstanding%20UTMOS%20scores%20and%0Ainherently%20contains%20richer%20semantic%20information.%20Specifically%2C%20we%20achieve%20these%0Aresults%20by%20designing%20a%20broader%20VQ%20space%2C%20extended%20contextual%20windows%2C%20and%0Aimproved%20attention%20networks%2C%20as%20well%20as%20introducing%20a%20powerful%20multi-scale%0Adiscriminator%20and%20an%20inverse%20Fourier%20transform%20structure.%20We%20conducted%0Aextensive%20reconstruction%20experiments%20in%20the%20domains%20of%20speech%2C%20audio%2C%20and%0Amusic.%20WavTokenizer%20exhibited%20strong%20performance%20across%20various%20objective%20and%0Asubjective%20metrics%20compared%20to%20state-of-the-art%20models.%20We%20also%20tested%20semantic%0Ainformation%2C%20VQ%20utilization%2C%20and%20adaptability%20to%20generative%20models.%0AComprehensive%20ablation%20studies%20confirm%20the%20necessity%20of%20each%20module%20in%0AWavTokenizer.%20The%20related%20code%2C%20demos%2C%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/jishengpeng/WavTokenizer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16532v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWavTokenizer%253A%2520an%2520Efficient%2520Acoustic%2520Discrete%2520Codec%2520Tokenizer%2520for%2520Audio%250A%2520%2520Language%2520Modeling%26entry.906535625%3DShengpeng%2520Ji%2520and%2520Ziyue%2520Jiang%2520and%2520Wen%2520Wang%2520and%2520Yifu%2520Chen%2520and%2520Minghui%2520Fang%2520and%2520Jialong%2520Zuo%2520and%2520Qian%2520Yang%2520and%2520Xize%2520Cheng%2520and%2520Zehan%2520Wang%2520and%2520Ruiqi%2520Li%2520and%2520Ziang%2520Zhang%2520and%2520Xiaoda%2520Yang%2520and%2520Rongjie%2520Huang%2520and%2520Yidi%2520Jiang%2520and%2520Qian%2520Chen%2520and%2520Siqi%2520Zheng%2520and%2520Zhou%2520Zhao%26entry.1292438233%3D%2520%2520Language%2520models%2520have%2520been%2520effectively%2520applied%2520to%2520modeling%2520natural%2520signals%252C%250Asuch%2520as%2520images%252C%2520video%252C%2520speech%252C%2520and%2520audio.%2520A%2520crucial%2520component%2520of%2520these%2520models%250Ais%2520the%2520codec%2520tokenizer%252C%2520which%2520compresses%2520high-dimensional%2520natural%2520signals%2520into%250Alower-dimensional%2520discrete%2520tokens.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520WavTokenizer%252C%250Awhich%2520offers%2520several%2520advantages%2520over%2520previous%2520SOTA%2520acoustic%2520codec%2520models%2520in%2520the%250Aaudio%2520domain%253A%25201%2529extreme%2520compression.%2520By%2520compressing%2520the%2520layers%2520of%2520quantizers%250Aand%2520the%2520temporal%2520dimension%2520of%2520the%2520discrete%2520codec%252C%2520one-second%2520audio%2520of%252024kHz%250Asampling%2520rate%2520requires%2520only%2520a%2520single%2520quantizer%2520with%252040%2520or%252075%2520tokens.%25202%2529improved%250Asubjective%2520quality.%2520Despite%2520the%2520reduced%2520number%2520of%2520tokens%252C%2520WavTokenizer%2520achieves%250Astate-of-the-art%2520reconstruction%2520quality%2520with%2520outstanding%2520UTMOS%2520scores%2520and%250Ainherently%2520contains%2520richer%2520semantic%2520information.%2520Specifically%252C%2520we%2520achieve%2520these%250Aresults%2520by%2520designing%2520a%2520broader%2520VQ%2520space%252C%2520extended%2520contextual%2520windows%252C%2520and%250Aimproved%2520attention%2520networks%252C%2520as%2520well%2520as%2520introducing%2520a%2520powerful%2520multi-scale%250Adiscriminator%2520and%2520an%2520inverse%2520Fourier%2520transform%2520structure.%2520We%2520conducted%250Aextensive%2520reconstruction%2520experiments%2520in%2520the%2520domains%2520of%2520speech%252C%2520audio%252C%2520and%250Amusic.%2520WavTokenizer%2520exhibited%2520strong%2520performance%2520across%2520various%2520objective%2520and%250Asubjective%2520metrics%2520compared%2520to%2520state-of-the-art%2520models.%2520We%2520also%2520tested%2520semantic%250Ainformation%252C%2520VQ%2520utilization%252C%2520and%2520adaptability%2520to%2520generative%2520models.%250AComprehensive%2520ablation%2520studies%2520confirm%2520the%2520necessity%2520of%2520each%2520module%2520in%250AWavTokenizer.%2520The%2520related%2520code%252C%2520demos%252C%2520and%2520pre-trained%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/jishengpeng/WavTokenizer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16532v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WavTokenizer%3A%20an%20Efficient%20Acoustic%20Discrete%20Codec%20Tokenizer%20for%20Audio%0A%20%20Language%20Modeling&entry.906535625=Shengpeng%20Ji%20and%20Ziyue%20Jiang%20and%20Wen%20Wang%20and%20Yifu%20Chen%20and%20Minghui%20Fang%20and%20Jialong%20Zuo%20and%20Qian%20Yang%20and%20Xize%20Cheng%20and%20Zehan%20Wang%20and%20Ruiqi%20Li%20and%20Ziang%20Zhang%20and%20Xiaoda%20Yang%20and%20Rongjie%20Huang%20and%20Yidi%20Jiang%20and%20Qian%20Chen%20and%20Siqi%20Zheng%20and%20Zhou%20Zhao&entry.1292438233=%20%20Language%20models%20have%20been%20effectively%20applied%20to%20modeling%20natural%20signals%2C%0Asuch%20as%20images%2C%20video%2C%20speech%2C%20and%20audio.%20A%20crucial%20component%20of%20these%20models%0Ais%20the%20codec%20tokenizer%2C%20which%20compresses%20high-dimensional%20natural%20signals%20into%0Alower-dimensional%20discrete%20tokens.%20In%20this%20paper%2C%20we%20introduce%20WavTokenizer%2C%0Awhich%20offers%20several%20advantages%20over%20previous%20SOTA%20acoustic%20codec%20models%20in%20the%0Aaudio%20domain%3A%201%29extreme%20compression.%20By%20compressing%20the%20layers%20of%20quantizers%0Aand%20the%20temporal%20dimension%20of%20the%20discrete%20codec%2C%20one-second%20audio%20of%2024kHz%0Asampling%20rate%20requires%20only%20a%20single%20quantizer%20with%2040%20or%2075%20tokens.%202%29improved%0Asubjective%20quality.%20Despite%20the%20reduced%20number%20of%20tokens%2C%20WavTokenizer%20achieves%0Astate-of-the-art%20reconstruction%20quality%20with%20outstanding%20UTMOS%20scores%20and%0Ainherently%20contains%20richer%20semantic%20information.%20Specifically%2C%20we%20achieve%20these%0Aresults%20by%20designing%20a%20broader%20VQ%20space%2C%20extended%20contextual%20windows%2C%20and%0Aimproved%20attention%20networks%2C%20as%20well%20as%20introducing%20a%20powerful%20multi-scale%0Adiscriminator%20and%20an%20inverse%20Fourier%20transform%20structure.%20We%20conducted%0Aextensive%20reconstruction%20experiments%20in%20the%20domains%20of%20speech%2C%20audio%2C%20and%0Amusic.%20WavTokenizer%20exhibited%20strong%20performance%20across%20various%20objective%20and%0Asubjective%20metrics%20compared%20to%20state-of-the-art%20models.%20We%20also%20tested%20semantic%0Ainformation%2C%20VQ%20utilization%2C%20and%20adaptability%20to%20generative%20models.%0AComprehensive%20ablation%20studies%20confirm%20the%20necessity%20of%20each%20module%20in%0AWavTokenizer.%20The%20related%20code%2C%20demos%2C%20and%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/jishengpeng/WavTokenizer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16532v3&entry.124074799=Read"},
{"title": "Personalized Topology-Informed Localization of Standard 12-Lead ECG\n  Electrode Placement from Incomplete Cardiac MRIs for Efficient Cardiac\n  Digital Twins", "author": "Lei Li and Hannah Smith and Yilin Lyu and Julia Camps and Shuang Qian and Blanca Rodriguez and Abhirup Banerjee and Vicente Grau", "abstract": "  Cardiac digital twins (CDTs) offer personalized in-silico cardiac\nrepresentations for the inference of multi-scale properties tied to cardiac\nmechanisms. The creation of CDTs requires precise information about the\nelectrode position on the torso, especially for the personalized\nelectrocardiogram (ECG) calibration. However, current studies commonly rely on\nadditional acquisition of torso imaging and manual/semi-automatic methods for\nECG electrode localization. In this study, we propose a novel and efficient\ntopology-informed model to fully automatically extract personalized ECG\nstandard electrode locations from 2D clinically standard cardiac MRIs.\nSpecifically, we obtain the sparse torso contours from the cardiac MRIs and\nthen localize the standard electrodes of 12-lead ECG from the contours. Cardiac\nMRIs aim at imaging of the heart instead of the torso, leading to incomplete\ntorso geometry within the imaging. To tackle the missing topology, we\nincorporate the electrodes as a subset of the keypoints, which can be\nexplicitly aligned with the 3D torso topology. The experimental results\ndemonstrate that the proposed model outperforms the time-consuming conventional\nmodel projection-based method in terms of accuracy (Euclidean distance: $1.24\n\\pm 0.293$ cm vs. $1.48 \\pm 0.362$ cm) and efficiency ($2$~s vs.\n$30$-$35$~min). We further demonstrate the effectiveness of using the detected\nelectrodes for in-silico ECG simulation, highlighting their potential for\ncreating accurate and efficient CDT models. The code is available at\nhttps://github.com/lileitech/12lead_ECG_electrode_localizer.\n", "link": "http://arxiv.org/abs/2408.13945v2", "date": "2025-02-25", "relevancy": 2.5335, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5232}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5199}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Topology-Informed%20Localization%20of%20Standard%2012-Lead%20ECG%0A%20%20Electrode%20Placement%20from%20Incomplete%20Cardiac%20MRIs%20for%20Efficient%20Cardiac%0A%20%20Digital%20Twins&body=Title%3A%20Personalized%20Topology-Informed%20Localization%20of%20Standard%2012-Lead%20ECG%0A%20%20Electrode%20Placement%20from%20Incomplete%20Cardiac%20MRIs%20for%20Efficient%20Cardiac%0A%20%20Digital%20Twins%0AAuthor%3A%20Lei%20Li%20and%20Hannah%20Smith%20and%20Yilin%20Lyu%20and%20Julia%20Camps%20and%20Shuang%20Qian%20and%20Blanca%20Rodriguez%20and%20Abhirup%20Banerjee%20and%20Vicente%20Grau%0AAbstract%3A%20%20%20Cardiac%20digital%20twins%20%28CDTs%29%20offer%20personalized%20in-silico%20cardiac%0Arepresentations%20for%20the%20inference%20of%20multi-scale%20properties%20tied%20to%20cardiac%0Amechanisms.%20The%20creation%20of%20CDTs%20requires%20precise%20information%20about%20the%0Aelectrode%20position%20on%20the%20torso%2C%20especially%20for%20the%20personalized%0Aelectrocardiogram%20%28ECG%29%20calibration.%20However%2C%20current%20studies%20commonly%20rely%20on%0Aadditional%20acquisition%20of%20torso%20imaging%20and%20manual/semi-automatic%20methods%20for%0AECG%20electrode%20localization.%20In%20this%20study%2C%20we%20propose%20a%20novel%20and%20efficient%0Atopology-informed%20model%20to%20fully%20automatically%20extract%20personalized%20ECG%0Astandard%20electrode%20locations%20from%202D%20clinically%20standard%20cardiac%20MRIs.%0ASpecifically%2C%20we%20obtain%20the%20sparse%20torso%20contours%20from%20the%20cardiac%20MRIs%20and%0Athen%20localize%20the%20standard%20electrodes%20of%2012-lead%20ECG%20from%20the%20contours.%20Cardiac%0AMRIs%20aim%20at%20imaging%20of%20the%20heart%20instead%20of%20the%20torso%2C%20leading%20to%20incomplete%0Atorso%20geometry%20within%20the%20imaging.%20To%20tackle%20the%20missing%20topology%2C%20we%0Aincorporate%20the%20electrodes%20as%20a%20subset%20of%20the%20keypoints%2C%20which%20can%20be%0Aexplicitly%20aligned%20with%20the%203D%20torso%20topology.%20The%20experimental%20results%0Ademonstrate%20that%20the%20proposed%20model%20outperforms%20the%20time-consuming%20conventional%0Amodel%20projection-based%20method%20in%20terms%20of%20accuracy%20%28Euclidean%20distance%3A%20%241.24%0A%5Cpm%200.293%24%20cm%20vs.%20%241.48%20%5Cpm%200.362%24%20cm%29%20and%20efficiency%20%28%242%24~s%20vs.%0A%2430%24-%2435%24~min%29.%20We%20further%20demonstrate%20the%20effectiveness%20of%20using%20the%20detected%0Aelectrodes%20for%20in-silico%20ECG%20simulation%2C%20highlighting%20their%20potential%20for%0Acreating%20accurate%20and%20efficient%20CDT%20models.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/lileitech/12lead_ECG_electrode_localizer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13945v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Topology-Informed%2520Localization%2520of%2520Standard%252012-Lead%2520ECG%250A%2520%2520Electrode%2520Placement%2520from%2520Incomplete%2520Cardiac%2520MRIs%2520for%2520Efficient%2520Cardiac%250A%2520%2520Digital%2520Twins%26entry.906535625%3DLei%2520Li%2520and%2520Hannah%2520Smith%2520and%2520Yilin%2520Lyu%2520and%2520Julia%2520Camps%2520and%2520Shuang%2520Qian%2520and%2520Blanca%2520Rodriguez%2520and%2520Abhirup%2520Banerjee%2520and%2520Vicente%2520Grau%26entry.1292438233%3D%2520%2520Cardiac%2520digital%2520twins%2520%2528CDTs%2529%2520offer%2520personalized%2520in-silico%2520cardiac%250Arepresentations%2520for%2520the%2520inference%2520of%2520multi-scale%2520properties%2520tied%2520to%2520cardiac%250Amechanisms.%2520The%2520creation%2520of%2520CDTs%2520requires%2520precise%2520information%2520about%2520the%250Aelectrode%2520position%2520on%2520the%2520torso%252C%2520especially%2520for%2520the%2520personalized%250Aelectrocardiogram%2520%2528ECG%2529%2520calibration.%2520However%252C%2520current%2520studies%2520commonly%2520rely%2520on%250Aadditional%2520acquisition%2520of%2520torso%2520imaging%2520and%2520manual/semi-automatic%2520methods%2520for%250AECG%2520electrode%2520localization.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%2520and%2520efficient%250Atopology-informed%2520model%2520to%2520fully%2520automatically%2520extract%2520personalized%2520ECG%250Astandard%2520electrode%2520locations%2520from%25202D%2520clinically%2520standard%2520cardiac%2520MRIs.%250ASpecifically%252C%2520we%2520obtain%2520the%2520sparse%2520torso%2520contours%2520from%2520the%2520cardiac%2520MRIs%2520and%250Athen%2520localize%2520the%2520standard%2520electrodes%2520of%252012-lead%2520ECG%2520from%2520the%2520contours.%2520Cardiac%250AMRIs%2520aim%2520at%2520imaging%2520of%2520the%2520heart%2520instead%2520of%2520the%2520torso%252C%2520leading%2520to%2520incomplete%250Atorso%2520geometry%2520within%2520the%2520imaging.%2520To%2520tackle%2520the%2520missing%2520topology%252C%2520we%250Aincorporate%2520the%2520electrodes%2520as%2520a%2520subset%2520of%2520the%2520keypoints%252C%2520which%2520can%2520be%250Aexplicitly%2520aligned%2520with%2520the%25203D%2520torso%2520topology.%2520The%2520experimental%2520results%250Ademonstrate%2520that%2520the%2520proposed%2520model%2520outperforms%2520the%2520time-consuming%2520conventional%250Amodel%2520projection-based%2520method%2520in%2520terms%2520of%2520accuracy%2520%2528Euclidean%2520distance%253A%2520%25241.24%250A%255Cpm%25200.293%2524%2520cm%2520vs.%2520%25241.48%2520%255Cpm%25200.362%2524%2520cm%2529%2520and%2520efficiency%2520%2528%25242%2524~s%2520vs.%250A%252430%2524-%252435%2524~min%2529.%2520We%2520further%2520demonstrate%2520the%2520effectiveness%2520of%2520using%2520the%2520detected%250Aelectrodes%2520for%2520in-silico%2520ECG%2520simulation%252C%2520highlighting%2520their%2520potential%2520for%250Acreating%2520accurate%2520and%2520efficient%2520CDT%2520models.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/lileitech/12lead_ECG_electrode_localizer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13945v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Topology-Informed%20Localization%20of%20Standard%2012-Lead%20ECG%0A%20%20Electrode%20Placement%20from%20Incomplete%20Cardiac%20MRIs%20for%20Efficient%20Cardiac%0A%20%20Digital%20Twins&entry.906535625=Lei%20Li%20and%20Hannah%20Smith%20and%20Yilin%20Lyu%20and%20Julia%20Camps%20and%20Shuang%20Qian%20and%20Blanca%20Rodriguez%20and%20Abhirup%20Banerjee%20and%20Vicente%20Grau&entry.1292438233=%20%20Cardiac%20digital%20twins%20%28CDTs%29%20offer%20personalized%20in-silico%20cardiac%0Arepresentations%20for%20the%20inference%20of%20multi-scale%20properties%20tied%20to%20cardiac%0Amechanisms.%20The%20creation%20of%20CDTs%20requires%20precise%20information%20about%20the%0Aelectrode%20position%20on%20the%20torso%2C%20especially%20for%20the%20personalized%0Aelectrocardiogram%20%28ECG%29%20calibration.%20However%2C%20current%20studies%20commonly%20rely%20on%0Aadditional%20acquisition%20of%20torso%20imaging%20and%20manual/semi-automatic%20methods%20for%0AECG%20electrode%20localization.%20In%20this%20study%2C%20we%20propose%20a%20novel%20and%20efficient%0Atopology-informed%20model%20to%20fully%20automatically%20extract%20personalized%20ECG%0Astandard%20electrode%20locations%20from%202D%20clinically%20standard%20cardiac%20MRIs.%0ASpecifically%2C%20we%20obtain%20the%20sparse%20torso%20contours%20from%20the%20cardiac%20MRIs%20and%0Athen%20localize%20the%20standard%20electrodes%20of%2012-lead%20ECG%20from%20the%20contours.%20Cardiac%0AMRIs%20aim%20at%20imaging%20of%20the%20heart%20instead%20of%20the%20torso%2C%20leading%20to%20incomplete%0Atorso%20geometry%20within%20the%20imaging.%20To%20tackle%20the%20missing%20topology%2C%20we%0Aincorporate%20the%20electrodes%20as%20a%20subset%20of%20the%20keypoints%2C%20which%20can%20be%0Aexplicitly%20aligned%20with%20the%203D%20torso%20topology.%20The%20experimental%20results%0Ademonstrate%20that%20the%20proposed%20model%20outperforms%20the%20time-consuming%20conventional%0Amodel%20projection-based%20method%20in%20terms%20of%20accuracy%20%28Euclidean%20distance%3A%20%241.24%0A%5Cpm%200.293%24%20cm%20vs.%20%241.48%20%5Cpm%200.362%24%20cm%29%20and%20efficiency%20%28%242%24~s%20vs.%0A%2430%24-%2435%24~min%29.%20We%20further%20demonstrate%20the%20effectiveness%20of%20using%20the%20detected%0Aelectrodes%20for%20in-silico%20ECG%20simulation%2C%20highlighting%20their%20potential%20for%0Acreating%20accurate%20and%20efficient%20CDT%20models.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/lileitech/12lead_ECG_electrode_localizer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13945v2&entry.124074799=Read"},
{"title": "How Far are LLMs from Real Search? A Comprehensive Study on Efficiency,\n  Completeness, and Inherent Capabilities", "author": "Minhua Lin and Hui Liu and Xianfeng Tang and Jingying Zeng and Zhenwei Dai and Chen Luo and Zheng Li and Xiang Zhang and Qi He and Suhang Wang", "abstract": "  Search plays a fundamental role in problem-solving across various domains,\nwith most real-world decision-making problems being solvable through systematic\nsearch. Drawing inspiration from recent discussions on search and learning, we\nsystematically explore the complementary relationship between search and Large\nLanguage Models (LLMs) from three perspectives. First, we analyze how learning\ncan enhance search efficiency and propose Search via Learning (SeaL), a\nframework that leverages LLMs for effective and efficient search. Second, we\nfurther extend SeaL to SeaL-C to ensure rigorous completeness during search.\nOur evaluation across three real-world planning tasks demonstrates that SeaL\nachieves near-perfect accuracy while reducing search spaces by up to 99.1%\ncompared to traditional approaches. Finally, we explore how far LLMs are from\nreal search by investigating whether they can develop search capabilities\nindependently. Our analysis reveals that while current LLMs struggle with\nefficient search in complex problems, incorporating systematic search\nstrategies significantly enhances their problem-solving capabilities. These\nfindings not only validate the effectiveness of our approach but also highlight\nthe need for improving LLMs' search abilities for real-world applications.\n", "link": "http://arxiv.org/abs/2502.18387v1", "date": "2025-02-25", "relevancy": 2.5122, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5221}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5221}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Far%20are%20LLMs%20from%20Real%20Search%3F%20A%20Comprehensive%20Study%20on%20Efficiency%2C%0A%20%20Completeness%2C%20and%20Inherent%20Capabilities&body=Title%3A%20How%20Far%20are%20LLMs%20from%20Real%20Search%3F%20A%20Comprehensive%20Study%20on%20Efficiency%2C%0A%20%20Completeness%2C%20and%20Inherent%20Capabilities%0AAuthor%3A%20Minhua%20Lin%20and%20Hui%20Liu%20and%20Xianfeng%20Tang%20and%20Jingying%20Zeng%20and%20Zhenwei%20Dai%20and%20Chen%20Luo%20and%20Zheng%20Li%20and%20Xiang%20Zhang%20and%20Qi%20He%20and%20Suhang%20Wang%0AAbstract%3A%20%20%20Search%20plays%20a%20fundamental%20role%20in%20problem-solving%20across%20various%20domains%2C%0Awith%20most%20real-world%20decision-making%20problems%20being%20solvable%20through%20systematic%0Asearch.%20Drawing%20inspiration%20from%20recent%20discussions%20on%20search%20and%20learning%2C%20we%0Asystematically%20explore%20the%20complementary%20relationship%20between%20search%20and%20Large%0ALanguage%20Models%20%28LLMs%29%20from%20three%20perspectives.%20First%2C%20we%20analyze%20how%20learning%0Acan%20enhance%20search%20efficiency%20and%20propose%20Search%20via%20Learning%20%28SeaL%29%2C%20a%0Aframework%20that%20leverages%20LLMs%20for%20effective%20and%20efficient%20search.%20Second%2C%20we%0Afurther%20extend%20SeaL%20to%20SeaL-C%20to%20ensure%20rigorous%20completeness%20during%20search.%0AOur%20evaluation%20across%20three%20real-world%20planning%20tasks%20demonstrates%20that%20SeaL%0Aachieves%20near-perfect%20accuracy%20while%20reducing%20search%20spaces%20by%20up%20to%2099.1%25%0Acompared%20to%20traditional%20approaches.%20Finally%2C%20we%20explore%20how%20far%20LLMs%20are%20from%0Areal%20search%20by%20investigating%20whether%20they%20can%20develop%20search%20capabilities%0Aindependently.%20Our%20analysis%20reveals%20that%20while%20current%20LLMs%20struggle%20with%0Aefficient%20search%20in%20complex%20problems%2C%20incorporating%20systematic%20search%0Astrategies%20significantly%20enhances%20their%20problem-solving%20capabilities.%20These%0Afindings%20not%20only%20validate%20the%20effectiveness%20of%20our%20approach%20but%20also%20highlight%0Athe%20need%20for%20improving%20LLMs%27%20search%20abilities%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Far%2520are%2520LLMs%2520from%2520Real%2520Search%253F%2520A%2520Comprehensive%2520Study%2520on%2520Efficiency%252C%250A%2520%2520Completeness%252C%2520and%2520Inherent%2520Capabilities%26entry.906535625%3DMinhua%2520Lin%2520and%2520Hui%2520Liu%2520and%2520Xianfeng%2520Tang%2520and%2520Jingying%2520Zeng%2520and%2520Zhenwei%2520Dai%2520and%2520Chen%2520Luo%2520and%2520Zheng%2520Li%2520and%2520Xiang%2520Zhang%2520and%2520Qi%2520He%2520and%2520Suhang%2520Wang%26entry.1292438233%3D%2520%2520Search%2520plays%2520a%2520fundamental%2520role%2520in%2520problem-solving%2520across%2520various%2520domains%252C%250Awith%2520most%2520real-world%2520decision-making%2520problems%2520being%2520solvable%2520through%2520systematic%250Asearch.%2520Drawing%2520inspiration%2520from%2520recent%2520discussions%2520on%2520search%2520and%2520learning%252C%2520we%250Asystematically%2520explore%2520the%2520complementary%2520relationship%2520between%2520search%2520and%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520from%2520three%2520perspectives.%2520First%252C%2520we%2520analyze%2520how%2520learning%250Acan%2520enhance%2520search%2520efficiency%2520and%2520propose%2520Search%2520via%2520Learning%2520%2528SeaL%2529%252C%2520a%250Aframework%2520that%2520leverages%2520LLMs%2520for%2520effective%2520and%2520efficient%2520search.%2520Second%252C%2520we%250Afurther%2520extend%2520SeaL%2520to%2520SeaL-C%2520to%2520ensure%2520rigorous%2520completeness%2520during%2520search.%250AOur%2520evaluation%2520across%2520three%2520real-world%2520planning%2520tasks%2520demonstrates%2520that%2520SeaL%250Aachieves%2520near-perfect%2520accuracy%2520while%2520reducing%2520search%2520spaces%2520by%2520up%2520to%252099.1%2525%250Acompared%2520to%2520traditional%2520approaches.%2520Finally%252C%2520we%2520explore%2520how%2520far%2520LLMs%2520are%2520from%250Areal%2520search%2520by%2520investigating%2520whether%2520they%2520can%2520develop%2520search%2520capabilities%250Aindependently.%2520Our%2520analysis%2520reveals%2520that%2520while%2520current%2520LLMs%2520struggle%2520with%250Aefficient%2520search%2520in%2520complex%2520problems%252C%2520incorporating%2520systematic%2520search%250Astrategies%2520significantly%2520enhances%2520their%2520problem-solving%2520capabilities.%2520These%250Afindings%2520not%2520only%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%2520but%2520also%2520highlight%250Athe%2520need%2520for%2520improving%2520LLMs%2527%2520search%2520abilities%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Far%20are%20LLMs%20from%20Real%20Search%3F%20A%20Comprehensive%20Study%20on%20Efficiency%2C%0A%20%20Completeness%2C%20and%20Inherent%20Capabilities&entry.906535625=Minhua%20Lin%20and%20Hui%20Liu%20and%20Xianfeng%20Tang%20and%20Jingying%20Zeng%20and%20Zhenwei%20Dai%20and%20Chen%20Luo%20and%20Zheng%20Li%20and%20Xiang%20Zhang%20and%20Qi%20He%20and%20Suhang%20Wang&entry.1292438233=%20%20Search%20plays%20a%20fundamental%20role%20in%20problem-solving%20across%20various%20domains%2C%0Awith%20most%20real-world%20decision-making%20problems%20being%20solvable%20through%20systematic%0Asearch.%20Drawing%20inspiration%20from%20recent%20discussions%20on%20search%20and%20learning%2C%20we%0Asystematically%20explore%20the%20complementary%20relationship%20between%20search%20and%20Large%0ALanguage%20Models%20%28LLMs%29%20from%20three%20perspectives.%20First%2C%20we%20analyze%20how%20learning%0Acan%20enhance%20search%20efficiency%20and%20propose%20Search%20via%20Learning%20%28SeaL%29%2C%20a%0Aframework%20that%20leverages%20LLMs%20for%20effective%20and%20efficient%20search.%20Second%2C%20we%0Afurther%20extend%20SeaL%20to%20SeaL-C%20to%20ensure%20rigorous%20completeness%20during%20search.%0AOur%20evaluation%20across%20three%20real-world%20planning%20tasks%20demonstrates%20that%20SeaL%0Aachieves%20near-perfect%20accuracy%20while%20reducing%20search%20spaces%20by%20up%20to%2099.1%25%0Acompared%20to%20traditional%20approaches.%20Finally%2C%20we%20explore%20how%20far%20LLMs%20are%20from%0Areal%20search%20by%20investigating%20whether%20they%20can%20develop%20search%20capabilities%0Aindependently.%20Our%20analysis%20reveals%20that%20while%20current%20LLMs%20struggle%20with%0Aefficient%20search%20in%20complex%20problems%2C%20incorporating%20systematic%20search%0Astrategies%20significantly%20enhances%20their%20problem-solving%20capabilities.%20These%0Afindings%20not%20only%20validate%20the%20effectiveness%20of%20our%20approach%20but%20also%20highlight%0Athe%20need%20for%20improving%20LLMs%27%20search%20abilities%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18387v1&entry.124074799=Read"},
{"title": "QuantMoE-Bench: Examining Post-Training Quantization for\n  Mixture-of-Experts", "author": "Pingzhi Li and Xiaolong Jin and Zhen Tan and Yu Cheng and Tianlong Chen", "abstract": "  Mixture-of-Experts (MoE) is a promising way to scale up the learning capacity\nof large language models. It increases the number of parameters while keeping\nFLOPs nearly constant during inference through sparse activation. Yet, it still\nsuffers from significant memory overheads due to the vast parameter size,\nnecessitating model compression techniques. Post-training quantization offers a\npowerful approach for model compression. Existing methods adopt a fixed\nquantization precision for the entire MoE model. This rigid setup can lead to\nsuboptimal performance, without considering the inherent sparse structure. For\nexample, MoE's sparse routing mechanism leads to different activation patterns,\nwhere shared experts are accessed by all tokens while token-conditioned experts\nare selectively activated. This activation disparity suggests different\nquantization requirements, with consistently activated shared experts\npotentially needing higher precision to maintain model quality. In this paper,\nwe study a fine-grained precision setup for MoE quantization. We explore MoE\nstructure-aware quantization heuristics, ranging from coarse (e.g., MoE layers)\nto fine granularity (e.g., linear layers). Our investigations reveal critical\nprinciples, where different MoE structures require varying numbers of bits for\neffective quantization. Conclusions are supported by extensive benchmarking\nacross two representative MoE models and six tasks including commonsense\nreasoning and natural language understanding. We further show that an MoE\nquantized in a fined-grained mixed precision achieved state-of-the-art 65.35%\nperformance on average compared to the baseline 64.30% (i.e., GPTQ). Moreover,\nbased on the findings, we introduce novel data-driven techniques for optimizing\nbit allocation in MoE quantization, including the outlier-aware linear layer\nscorer and MoE block importance predictor.\n", "link": "http://arxiv.org/abs/2406.08155v2", "date": "2025-02-25", "relevancy": 2.4967, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5005}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5005}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QuantMoE-Bench%3A%20Examining%20Post-Training%20Quantization%20for%0A%20%20Mixture-of-Experts&body=Title%3A%20QuantMoE-Bench%3A%20Examining%20Post-Training%20Quantization%20for%0A%20%20Mixture-of-Experts%0AAuthor%3A%20Pingzhi%20Li%20and%20Xiaolong%20Jin%20and%20Zhen%20Tan%20and%20Yu%20Cheng%20and%20Tianlong%20Chen%0AAbstract%3A%20%20%20Mixture-of-Experts%20%28MoE%29%20is%20a%20promising%20way%20to%20scale%20up%20the%20learning%20capacity%0Aof%20large%20language%20models.%20It%20increases%20the%20number%20of%20parameters%20while%20keeping%0AFLOPs%20nearly%20constant%20during%20inference%20through%20sparse%20activation.%20Yet%2C%20it%20still%0Asuffers%20from%20significant%20memory%20overheads%20due%20to%20the%20vast%20parameter%20size%2C%0Anecessitating%20model%20compression%20techniques.%20Post-training%20quantization%20offers%20a%0Apowerful%20approach%20for%20model%20compression.%20Existing%20methods%20adopt%20a%20fixed%0Aquantization%20precision%20for%20the%20entire%20MoE%20model.%20This%20rigid%20setup%20can%20lead%20to%0Asuboptimal%20performance%2C%20without%20considering%20the%20inherent%20sparse%20structure.%20For%0Aexample%2C%20MoE%27s%20sparse%20routing%20mechanism%20leads%20to%20different%20activation%20patterns%2C%0Awhere%20shared%20experts%20are%20accessed%20by%20all%20tokens%20while%20token-conditioned%20experts%0Aare%20selectively%20activated.%20This%20activation%20disparity%20suggests%20different%0Aquantization%20requirements%2C%20with%20consistently%20activated%20shared%20experts%0Apotentially%20needing%20higher%20precision%20to%20maintain%20model%20quality.%20In%20this%20paper%2C%0Awe%20study%20a%20fine-grained%20precision%20setup%20for%20MoE%20quantization.%20We%20explore%20MoE%0Astructure-aware%20quantization%20heuristics%2C%20ranging%20from%20coarse%20%28e.g.%2C%20MoE%20layers%29%0Ato%20fine%20granularity%20%28e.g.%2C%20linear%20layers%29.%20Our%20investigations%20reveal%20critical%0Aprinciples%2C%20where%20different%20MoE%20structures%20require%20varying%20numbers%20of%20bits%20for%0Aeffective%20quantization.%20Conclusions%20are%20supported%20by%20extensive%20benchmarking%0Aacross%20two%20representative%20MoE%20models%20and%20six%20tasks%20including%20commonsense%0Areasoning%20and%20natural%20language%20understanding.%20We%20further%20show%20that%20an%20MoE%0Aquantized%20in%20a%20fined-grained%20mixed%20precision%20achieved%20state-of-the-art%2065.35%25%0Aperformance%20on%20average%20compared%20to%20the%20baseline%2064.30%25%20%28i.e.%2C%20GPTQ%29.%20Moreover%2C%0Abased%20on%20the%20findings%2C%20we%20introduce%20novel%20data-driven%20techniques%20for%20optimizing%0Abit%20allocation%20in%20MoE%20quantization%2C%20including%20the%20outlier-aware%20linear%20layer%0Ascorer%20and%20MoE%20block%20importance%20predictor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08155v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantMoE-Bench%253A%2520Examining%2520Post-Training%2520Quantization%2520for%250A%2520%2520Mixture-of-Experts%26entry.906535625%3DPingzhi%2520Li%2520and%2520Xiaolong%2520Jin%2520and%2520Zhen%2520Tan%2520and%2520Yu%2520Cheng%2520and%2520Tianlong%2520Chen%26entry.1292438233%3D%2520%2520Mixture-of-Experts%2520%2528MoE%2529%2520is%2520a%2520promising%2520way%2520to%2520scale%2520up%2520the%2520learning%2520capacity%250Aof%2520large%2520language%2520models.%2520It%2520increases%2520the%2520number%2520of%2520parameters%2520while%2520keeping%250AFLOPs%2520nearly%2520constant%2520during%2520inference%2520through%2520sparse%2520activation.%2520Yet%252C%2520it%2520still%250Asuffers%2520from%2520significant%2520memory%2520overheads%2520due%2520to%2520the%2520vast%2520parameter%2520size%252C%250Anecessitating%2520model%2520compression%2520techniques.%2520Post-training%2520quantization%2520offers%2520a%250Apowerful%2520approach%2520for%2520model%2520compression.%2520Existing%2520methods%2520adopt%2520a%2520fixed%250Aquantization%2520precision%2520for%2520the%2520entire%2520MoE%2520model.%2520This%2520rigid%2520setup%2520can%2520lead%2520to%250Asuboptimal%2520performance%252C%2520without%2520considering%2520the%2520inherent%2520sparse%2520structure.%2520For%250Aexample%252C%2520MoE%2527s%2520sparse%2520routing%2520mechanism%2520leads%2520to%2520different%2520activation%2520patterns%252C%250Awhere%2520shared%2520experts%2520are%2520accessed%2520by%2520all%2520tokens%2520while%2520token-conditioned%2520experts%250Aare%2520selectively%2520activated.%2520This%2520activation%2520disparity%2520suggests%2520different%250Aquantization%2520requirements%252C%2520with%2520consistently%2520activated%2520shared%2520experts%250Apotentially%2520needing%2520higher%2520precision%2520to%2520maintain%2520model%2520quality.%2520In%2520this%2520paper%252C%250Awe%2520study%2520a%2520fine-grained%2520precision%2520setup%2520for%2520MoE%2520quantization.%2520We%2520explore%2520MoE%250Astructure-aware%2520quantization%2520heuristics%252C%2520ranging%2520from%2520coarse%2520%2528e.g.%252C%2520MoE%2520layers%2529%250Ato%2520fine%2520granularity%2520%2528e.g.%252C%2520linear%2520layers%2529.%2520Our%2520investigations%2520reveal%2520critical%250Aprinciples%252C%2520where%2520different%2520MoE%2520structures%2520require%2520varying%2520numbers%2520of%2520bits%2520for%250Aeffective%2520quantization.%2520Conclusions%2520are%2520supported%2520by%2520extensive%2520benchmarking%250Aacross%2520two%2520representative%2520MoE%2520models%2520and%2520six%2520tasks%2520including%2520commonsense%250Areasoning%2520and%2520natural%2520language%2520understanding.%2520We%2520further%2520show%2520that%2520an%2520MoE%250Aquantized%2520in%2520a%2520fined-grained%2520mixed%2520precision%2520achieved%2520state-of-the-art%252065.35%2525%250Aperformance%2520on%2520average%2520compared%2520to%2520the%2520baseline%252064.30%2525%2520%2528i.e.%252C%2520GPTQ%2529.%2520Moreover%252C%250Abased%2520on%2520the%2520findings%252C%2520we%2520introduce%2520novel%2520data-driven%2520techniques%2520for%2520optimizing%250Abit%2520allocation%2520in%2520MoE%2520quantization%252C%2520including%2520the%2520outlier-aware%2520linear%2520layer%250Ascorer%2520and%2520MoE%2520block%2520importance%2520predictor.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08155v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QuantMoE-Bench%3A%20Examining%20Post-Training%20Quantization%20for%0A%20%20Mixture-of-Experts&entry.906535625=Pingzhi%20Li%20and%20Xiaolong%20Jin%20and%20Zhen%20Tan%20and%20Yu%20Cheng%20and%20Tianlong%20Chen&entry.1292438233=%20%20Mixture-of-Experts%20%28MoE%29%20is%20a%20promising%20way%20to%20scale%20up%20the%20learning%20capacity%0Aof%20large%20language%20models.%20It%20increases%20the%20number%20of%20parameters%20while%20keeping%0AFLOPs%20nearly%20constant%20during%20inference%20through%20sparse%20activation.%20Yet%2C%20it%20still%0Asuffers%20from%20significant%20memory%20overheads%20due%20to%20the%20vast%20parameter%20size%2C%0Anecessitating%20model%20compression%20techniques.%20Post-training%20quantization%20offers%20a%0Apowerful%20approach%20for%20model%20compression.%20Existing%20methods%20adopt%20a%20fixed%0Aquantization%20precision%20for%20the%20entire%20MoE%20model.%20This%20rigid%20setup%20can%20lead%20to%0Asuboptimal%20performance%2C%20without%20considering%20the%20inherent%20sparse%20structure.%20For%0Aexample%2C%20MoE%27s%20sparse%20routing%20mechanism%20leads%20to%20different%20activation%20patterns%2C%0Awhere%20shared%20experts%20are%20accessed%20by%20all%20tokens%20while%20token-conditioned%20experts%0Aare%20selectively%20activated.%20This%20activation%20disparity%20suggests%20different%0Aquantization%20requirements%2C%20with%20consistently%20activated%20shared%20experts%0Apotentially%20needing%20higher%20precision%20to%20maintain%20model%20quality.%20In%20this%20paper%2C%0Awe%20study%20a%20fine-grained%20precision%20setup%20for%20MoE%20quantization.%20We%20explore%20MoE%0Astructure-aware%20quantization%20heuristics%2C%20ranging%20from%20coarse%20%28e.g.%2C%20MoE%20layers%29%0Ato%20fine%20granularity%20%28e.g.%2C%20linear%20layers%29.%20Our%20investigations%20reveal%20critical%0Aprinciples%2C%20where%20different%20MoE%20structures%20require%20varying%20numbers%20of%20bits%20for%0Aeffective%20quantization.%20Conclusions%20are%20supported%20by%20extensive%20benchmarking%0Aacross%20two%20representative%20MoE%20models%20and%20six%20tasks%20including%20commonsense%0Areasoning%20and%20natural%20language%20understanding.%20We%20further%20show%20that%20an%20MoE%0Aquantized%20in%20a%20fined-grained%20mixed%20precision%20achieved%20state-of-the-art%2065.35%25%0Aperformance%20on%20average%20compared%20to%20the%20baseline%2064.30%25%20%28i.e.%2C%20GPTQ%29.%20Moreover%2C%0Abased%20on%20the%20findings%2C%20we%20introduce%20novel%20data-driven%20techniques%20for%20optimizing%0Abit%20allocation%20in%20MoE%20quantization%2C%20including%20the%20outlier-aware%20linear%20layer%0Ascorer%20and%20MoE%20block%20importance%20predictor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08155v2&entry.124074799=Read"},
{"title": "How Does Code Pretraining Affect Language Model Task Performance?", "author": "Jackson Petty and Sjoerd van Steenkiste and Tal Linzen", "abstract": "  Large language models are increasingly trained on corpora containing both\nnatural language and non-linguistic data like source code. Aside from aiding\nprogramming-related tasks, anecdotal evidence suggests that including code in\npretraining corpora may improve performance on other, unrelated tasks, yet to\ndate no work has been able to establish a causal connection by controlling\nbetween language and code data. Here we do just this. We pretrain language\nmodels on datasets which interleave natural language and code in two different\nsettings: additive, in which the total volume of data seen during pretraining\nis held constant; and competitive, in which the volume of language data is held\nconstant. We study how the pretraining mixture affects performance on (a) a\ndiverse collection of tasks included in the BigBench benchmark, and (b)\ncompositionality, measured by generalization accuracy on semantic parsing and\nsyntactic transformations. We find that pretraining on higher proportions of\ncode improves performance on compositional tasks involving structured output\n(like semantic parsing), and mathematics. Conversely, increase code mixture can\nharm performance on other tasks, including on tasks that requires sensitivity\nto linguistic structure such as syntax or morphology, and tasks measuring\nreal-world knowledge.\n", "link": "http://arxiv.org/abs/2409.04556v2", "date": "2025-02-25", "relevancy": 2.4727, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.506}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.506}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Does%20Code%20Pretraining%20Affect%20Language%20Model%20Task%20Performance%3F&body=Title%3A%20How%20Does%20Code%20Pretraining%20Affect%20Language%20Model%20Task%20Performance%3F%0AAuthor%3A%20Jackson%20Petty%20and%20Sjoerd%20van%20Steenkiste%20and%20Tal%20Linzen%0AAbstract%3A%20%20%20Large%20language%20models%20are%20increasingly%20trained%20on%20corpora%20containing%20both%0Anatural%20language%20and%20non-linguistic%20data%20like%20source%20code.%20Aside%20from%20aiding%0Aprogramming-related%20tasks%2C%20anecdotal%20evidence%20suggests%20that%20including%20code%20in%0Apretraining%20corpora%20may%20improve%20performance%20on%20other%2C%20unrelated%20tasks%2C%20yet%20to%0Adate%20no%20work%20has%20been%20able%20to%20establish%20a%20causal%20connection%20by%20controlling%0Abetween%20language%20and%20code%20data.%20Here%20we%20do%20just%20this.%20We%20pretrain%20language%0Amodels%20on%20datasets%20which%20interleave%20natural%20language%20and%20code%20in%20two%20different%0Asettings%3A%20additive%2C%20in%20which%20the%20total%20volume%20of%20data%20seen%20during%20pretraining%0Ais%20held%20constant%3B%20and%20competitive%2C%20in%20which%20the%20volume%20of%20language%20data%20is%20held%0Aconstant.%20We%20study%20how%20the%20pretraining%20mixture%20affects%20performance%20on%20%28a%29%20a%0Adiverse%20collection%20of%20tasks%20included%20in%20the%20BigBench%20benchmark%2C%20and%20%28b%29%0Acompositionality%2C%20measured%20by%20generalization%20accuracy%20on%20semantic%20parsing%20and%0Asyntactic%20transformations.%20We%20find%20that%20pretraining%20on%20higher%20proportions%20of%0Acode%20improves%20performance%20on%20compositional%20tasks%20involving%20structured%20output%0A%28like%20semantic%20parsing%29%2C%20and%20mathematics.%20Conversely%2C%20increase%20code%20mixture%20can%0Aharm%20performance%20on%20other%20tasks%2C%20including%20on%20tasks%20that%20requires%20sensitivity%0Ato%20linguistic%20structure%20such%20as%20syntax%20or%20morphology%2C%20and%20tasks%20measuring%0Areal-world%20knowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04556v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Does%2520Code%2520Pretraining%2520Affect%2520Language%2520Model%2520Task%2520Performance%253F%26entry.906535625%3DJackson%2520Petty%2520and%2520Sjoerd%2520van%2520Steenkiste%2520and%2520Tal%2520Linzen%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520are%2520increasingly%2520trained%2520on%2520corpora%2520containing%2520both%250Anatural%2520language%2520and%2520non-linguistic%2520data%2520like%2520source%2520code.%2520Aside%2520from%2520aiding%250Aprogramming-related%2520tasks%252C%2520anecdotal%2520evidence%2520suggests%2520that%2520including%2520code%2520in%250Apretraining%2520corpora%2520may%2520improve%2520performance%2520on%2520other%252C%2520unrelated%2520tasks%252C%2520yet%2520to%250Adate%2520no%2520work%2520has%2520been%2520able%2520to%2520establish%2520a%2520causal%2520connection%2520by%2520controlling%250Abetween%2520language%2520and%2520code%2520data.%2520Here%2520we%2520do%2520just%2520this.%2520We%2520pretrain%2520language%250Amodels%2520on%2520datasets%2520which%2520interleave%2520natural%2520language%2520and%2520code%2520in%2520two%2520different%250Asettings%253A%2520additive%252C%2520in%2520which%2520the%2520total%2520volume%2520of%2520data%2520seen%2520during%2520pretraining%250Ais%2520held%2520constant%253B%2520and%2520competitive%252C%2520in%2520which%2520the%2520volume%2520of%2520language%2520data%2520is%2520held%250Aconstant.%2520We%2520study%2520how%2520the%2520pretraining%2520mixture%2520affects%2520performance%2520on%2520%2528a%2529%2520a%250Adiverse%2520collection%2520of%2520tasks%2520included%2520in%2520the%2520BigBench%2520benchmark%252C%2520and%2520%2528b%2529%250Acompositionality%252C%2520measured%2520by%2520generalization%2520accuracy%2520on%2520semantic%2520parsing%2520and%250Asyntactic%2520transformations.%2520We%2520find%2520that%2520pretraining%2520on%2520higher%2520proportions%2520of%250Acode%2520improves%2520performance%2520on%2520compositional%2520tasks%2520involving%2520structured%2520output%250A%2528like%2520semantic%2520parsing%2529%252C%2520and%2520mathematics.%2520Conversely%252C%2520increase%2520code%2520mixture%2520can%250Aharm%2520performance%2520on%2520other%2520tasks%252C%2520including%2520on%2520tasks%2520that%2520requires%2520sensitivity%250Ato%2520linguistic%2520structure%2520such%2520as%2520syntax%2520or%2520morphology%252C%2520and%2520tasks%2520measuring%250Areal-world%2520knowledge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04556v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Does%20Code%20Pretraining%20Affect%20Language%20Model%20Task%20Performance%3F&entry.906535625=Jackson%20Petty%20and%20Sjoerd%20van%20Steenkiste%20and%20Tal%20Linzen&entry.1292438233=%20%20Large%20language%20models%20are%20increasingly%20trained%20on%20corpora%20containing%20both%0Anatural%20language%20and%20non-linguistic%20data%20like%20source%20code.%20Aside%20from%20aiding%0Aprogramming-related%20tasks%2C%20anecdotal%20evidence%20suggests%20that%20including%20code%20in%0Apretraining%20corpora%20may%20improve%20performance%20on%20other%2C%20unrelated%20tasks%2C%20yet%20to%0Adate%20no%20work%20has%20been%20able%20to%20establish%20a%20causal%20connection%20by%20controlling%0Abetween%20language%20and%20code%20data.%20Here%20we%20do%20just%20this.%20We%20pretrain%20language%0Amodels%20on%20datasets%20which%20interleave%20natural%20language%20and%20code%20in%20two%20different%0Asettings%3A%20additive%2C%20in%20which%20the%20total%20volume%20of%20data%20seen%20during%20pretraining%0Ais%20held%20constant%3B%20and%20competitive%2C%20in%20which%20the%20volume%20of%20language%20data%20is%20held%0Aconstant.%20We%20study%20how%20the%20pretraining%20mixture%20affects%20performance%20on%20%28a%29%20a%0Adiverse%20collection%20of%20tasks%20included%20in%20the%20BigBench%20benchmark%2C%20and%20%28b%29%0Acompositionality%2C%20measured%20by%20generalization%20accuracy%20on%20semantic%20parsing%20and%0Asyntactic%20transformations.%20We%20find%20that%20pretraining%20on%20higher%20proportions%20of%0Acode%20improves%20performance%20on%20compositional%20tasks%20involving%20structured%20output%0A%28like%20semantic%20parsing%29%2C%20and%20mathematics.%20Conversely%2C%20increase%20code%20mixture%20can%0Aharm%20performance%20on%20other%20tasks%2C%20including%20on%20tasks%20that%20requires%20sensitivity%0Ato%20linguistic%20structure%20such%20as%20syntax%20or%20morphology%2C%20and%20tasks%20measuring%0Areal-world%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04556v2&entry.124074799=Read"},
{"title": "Spatial Context-based Self-Supervised Learning for Handwritten Text\n  Recognition", "author": "Carlos Penarrubia and Carlos Garrido-Munoz and Jose J. Valero-Mas and Jorge Calvo-Zaragoza", "abstract": "  Handwritten Text Recognition (HTR) is a relevant problem in computer vision,\nand implies unique challenges owing to its inherent variability and the rich\ncontextualization required for its interpretation. Despite the success of\nSelf-Supervised Learning (SSL) in computer vision, its application to HTR has\nbeen rather scattered, leaving key SSL methodologies unexplored. This work\nfocuses on one of them, namely Spatial Context-based SSL. We investigate how\nthis family of approaches can be adapted and optimized for HTR and propose new\nworkflows that leverage the unique features of handwritten text. Our\nexperiments demonstrate that the methods considered lead to advancements in the\nstate-of-the-art of SSL for HTR in a number of benchmark cases.\n", "link": "http://arxiv.org/abs/2404.11585v2", "date": "2025-02-25", "relevancy": 2.4703, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5465}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4696}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial%20Context-based%20Self-Supervised%20Learning%20for%20Handwritten%20Text%0A%20%20Recognition&body=Title%3A%20Spatial%20Context-based%20Self-Supervised%20Learning%20for%20Handwritten%20Text%0A%20%20Recognition%0AAuthor%3A%20Carlos%20Penarrubia%20and%20Carlos%20Garrido-Munoz%20and%20Jose%20J.%20Valero-Mas%20and%20Jorge%20Calvo-Zaragoza%0AAbstract%3A%20%20%20Handwritten%20Text%20Recognition%20%28HTR%29%20is%20a%20relevant%20problem%20in%20computer%20vision%2C%0Aand%20implies%20unique%20challenges%20owing%20to%20its%20inherent%20variability%20and%20the%20rich%0Acontextualization%20required%20for%20its%20interpretation.%20Despite%20the%20success%20of%0ASelf-Supervised%20Learning%20%28SSL%29%20in%20computer%20vision%2C%20its%20application%20to%20HTR%20has%0Abeen%20rather%20scattered%2C%20leaving%20key%20SSL%20methodologies%20unexplored.%20This%20work%0Afocuses%20on%20one%20of%20them%2C%20namely%20Spatial%20Context-based%20SSL.%20We%20investigate%20how%0Athis%20family%20of%20approaches%20can%20be%20adapted%20and%20optimized%20for%20HTR%20and%20propose%20new%0Aworkflows%20that%20leverage%20the%20unique%20features%20of%20handwritten%20text.%20Our%0Aexperiments%20demonstrate%20that%20the%20methods%20considered%20lead%20to%20advancements%20in%20the%0Astate-of-the-art%20of%20SSL%20for%20HTR%20in%20a%20number%20of%20benchmark%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11585v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial%2520Context-based%2520Self-Supervised%2520Learning%2520for%2520Handwritten%2520Text%250A%2520%2520Recognition%26entry.906535625%3DCarlos%2520Penarrubia%2520and%2520Carlos%2520Garrido-Munoz%2520and%2520Jose%2520J.%2520Valero-Mas%2520and%2520Jorge%2520Calvo-Zaragoza%26entry.1292438233%3D%2520%2520Handwritten%2520Text%2520Recognition%2520%2528HTR%2529%2520is%2520a%2520relevant%2520problem%2520in%2520computer%2520vision%252C%250Aand%2520implies%2520unique%2520challenges%2520owing%2520to%2520its%2520inherent%2520variability%2520and%2520the%2520rich%250Acontextualization%2520required%2520for%2520its%2520interpretation.%2520Despite%2520the%2520success%2520of%250ASelf-Supervised%2520Learning%2520%2528SSL%2529%2520in%2520computer%2520vision%252C%2520its%2520application%2520to%2520HTR%2520has%250Abeen%2520rather%2520scattered%252C%2520leaving%2520key%2520SSL%2520methodologies%2520unexplored.%2520This%2520work%250Afocuses%2520on%2520one%2520of%2520them%252C%2520namely%2520Spatial%2520Context-based%2520SSL.%2520We%2520investigate%2520how%250Athis%2520family%2520of%2520approaches%2520can%2520be%2520adapted%2520and%2520optimized%2520for%2520HTR%2520and%2520propose%2520new%250Aworkflows%2520that%2520leverage%2520the%2520unique%2520features%2520of%2520handwritten%2520text.%2520Our%250Aexperiments%2520demonstrate%2520that%2520the%2520methods%2520considered%2520lead%2520to%2520advancements%2520in%2520the%250Astate-of-the-art%2520of%2520SSL%2520for%2520HTR%2520in%2520a%2520number%2520of%2520benchmark%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11585v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20Context-based%20Self-Supervised%20Learning%20for%20Handwritten%20Text%0A%20%20Recognition&entry.906535625=Carlos%20Penarrubia%20and%20Carlos%20Garrido-Munoz%20and%20Jose%20J.%20Valero-Mas%20and%20Jorge%20Calvo-Zaragoza&entry.1292438233=%20%20Handwritten%20Text%20Recognition%20%28HTR%29%20is%20a%20relevant%20problem%20in%20computer%20vision%2C%0Aand%20implies%20unique%20challenges%20owing%20to%20its%20inherent%20variability%20and%20the%20rich%0Acontextualization%20required%20for%20its%20interpretation.%20Despite%20the%20success%20of%0ASelf-Supervised%20Learning%20%28SSL%29%20in%20computer%20vision%2C%20its%20application%20to%20HTR%20has%0Abeen%20rather%20scattered%2C%20leaving%20key%20SSL%20methodologies%20unexplored.%20This%20work%0Afocuses%20on%20one%20of%20them%2C%20namely%20Spatial%20Context-based%20SSL.%20We%20investigate%20how%0Athis%20family%20of%20approaches%20can%20be%20adapted%20and%20optimized%20for%20HTR%20and%20propose%20new%0Aworkflows%20that%20leverage%20the%20unique%20features%20of%20handwritten%20text.%20Our%0Aexperiments%20demonstrate%20that%20the%20methods%20considered%20lead%20to%20advancements%20in%20the%0Astate-of-the-art%20of%20SSL%20for%20HTR%20in%20a%20number%20of%20benchmark%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11585v2&entry.124074799=Read"},
{"title": "Rethinking Layer Removal: A Hybrid Pruning Framework Combining Layer\n  Removal and Singular Value Selection for Efficient LLM Compression", "author": "Kainan Liu and Yong Zhang and Ning Cheng and Zhitao Li and Shaojun Wang and Jing Xiao", "abstract": "  Layer removal is an effective technique for compressing large language models\n(LLMs) by reducing redundancy and improving inference efficiency. However,\nindiscriminate pruning disrupts representation stability, leading to\nperformance degradation. We propose GRASP (Gradient-based Retention of Adaptive\nSingular Parameters), which preserves representation-critical singular values\nto mitigate these effects. Unlike direct layer removal, GRASP leverages\ngradient-based attribution on a syntax- and semantics-rich dataset to guide the\nselection of representation-critical singular values. By selectively applying\nsingular value decomposition (SVD) to affected layers, GRASP achieves efficient\ncompression while maintaining representation stability with minimal overhead.\nExperiments across multiple LLMs show that GRASP consistently outperforms\nexisting compression methods in perplexity and downstream task performance.\n", "link": "http://arxiv.org/abs/2501.00339v2", "date": "2025-02-25", "relevancy": 2.4543, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5178}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4774}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Layer%20Removal%3A%20A%20Hybrid%20Pruning%20Framework%20Combining%20Layer%0A%20%20Removal%20and%20Singular%20Value%20Selection%20for%20Efficient%20LLM%20Compression&body=Title%3A%20Rethinking%20Layer%20Removal%3A%20A%20Hybrid%20Pruning%20Framework%20Combining%20Layer%0A%20%20Removal%20and%20Singular%20Value%20Selection%20for%20Efficient%20LLM%20Compression%0AAuthor%3A%20Kainan%20Liu%20and%20Yong%20Zhang%20and%20Ning%20Cheng%20and%20Zhitao%20Li%20and%20Shaojun%20Wang%20and%20Jing%20Xiao%0AAbstract%3A%20%20%20Layer%20removal%20is%20an%20effective%20technique%20for%20compressing%20large%20language%20models%0A%28LLMs%29%20by%20reducing%20redundancy%20and%20improving%20inference%20efficiency.%20However%2C%0Aindiscriminate%20pruning%20disrupts%20representation%20stability%2C%20leading%20to%0Aperformance%20degradation.%20We%20propose%20GRASP%20%28Gradient-based%20Retention%20of%20Adaptive%0ASingular%20Parameters%29%2C%20which%20preserves%20representation-critical%20singular%20values%0Ato%20mitigate%20these%20effects.%20Unlike%20direct%20layer%20removal%2C%20GRASP%20leverages%0Agradient-based%20attribution%20on%20a%20syntax-%20and%20semantics-rich%20dataset%20to%20guide%20the%0Aselection%20of%20representation-critical%20singular%20values.%20By%20selectively%20applying%0Asingular%20value%20decomposition%20%28SVD%29%20to%20affected%20layers%2C%20GRASP%20achieves%20efficient%0Acompression%20while%20maintaining%20representation%20stability%20with%20minimal%20overhead.%0AExperiments%20across%20multiple%20LLMs%20show%20that%20GRASP%20consistently%20outperforms%0Aexisting%20compression%20methods%20in%20perplexity%20and%20downstream%20task%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.00339v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Layer%2520Removal%253A%2520A%2520Hybrid%2520Pruning%2520Framework%2520Combining%2520Layer%250A%2520%2520Removal%2520and%2520Singular%2520Value%2520Selection%2520for%2520Efficient%2520LLM%2520Compression%26entry.906535625%3DKainan%2520Liu%2520and%2520Yong%2520Zhang%2520and%2520Ning%2520Cheng%2520and%2520Zhitao%2520Li%2520and%2520Shaojun%2520Wang%2520and%2520Jing%2520Xiao%26entry.1292438233%3D%2520%2520Layer%2520removal%2520is%2520an%2520effective%2520technique%2520for%2520compressing%2520large%2520language%2520models%250A%2528LLMs%2529%2520by%2520reducing%2520redundancy%2520and%2520improving%2520inference%2520efficiency.%2520However%252C%250Aindiscriminate%2520pruning%2520disrupts%2520representation%2520stability%252C%2520leading%2520to%250Aperformance%2520degradation.%2520We%2520propose%2520GRASP%2520%2528Gradient-based%2520Retention%2520of%2520Adaptive%250ASingular%2520Parameters%2529%252C%2520which%2520preserves%2520representation-critical%2520singular%2520values%250Ato%2520mitigate%2520these%2520effects.%2520Unlike%2520direct%2520layer%2520removal%252C%2520GRASP%2520leverages%250Agradient-based%2520attribution%2520on%2520a%2520syntax-%2520and%2520semantics-rich%2520dataset%2520to%2520guide%2520the%250Aselection%2520of%2520representation-critical%2520singular%2520values.%2520By%2520selectively%2520applying%250Asingular%2520value%2520decomposition%2520%2528SVD%2529%2520to%2520affected%2520layers%252C%2520GRASP%2520achieves%2520efficient%250Acompression%2520while%2520maintaining%2520representation%2520stability%2520with%2520minimal%2520overhead.%250AExperiments%2520across%2520multiple%2520LLMs%2520show%2520that%2520GRASP%2520consistently%2520outperforms%250Aexisting%2520compression%2520methods%2520in%2520perplexity%2520and%2520downstream%2520task%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.00339v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Layer%20Removal%3A%20A%20Hybrid%20Pruning%20Framework%20Combining%20Layer%0A%20%20Removal%20and%20Singular%20Value%20Selection%20for%20Efficient%20LLM%20Compression&entry.906535625=Kainan%20Liu%20and%20Yong%20Zhang%20and%20Ning%20Cheng%20and%20Zhitao%20Li%20and%20Shaojun%20Wang%20and%20Jing%20Xiao&entry.1292438233=%20%20Layer%20removal%20is%20an%20effective%20technique%20for%20compressing%20large%20language%20models%0A%28LLMs%29%20by%20reducing%20redundancy%20and%20improving%20inference%20efficiency.%20However%2C%0Aindiscriminate%20pruning%20disrupts%20representation%20stability%2C%20leading%20to%0Aperformance%20degradation.%20We%20propose%20GRASP%20%28Gradient-based%20Retention%20of%20Adaptive%0ASingular%20Parameters%29%2C%20which%20preserves%20representation-critical%20singular%20values%0Ato%20mitigate%20these%20effects.%20Unlike%20direct%20layer%20removal%2C%20GRASP%20leverages%0Agradient-based%20attribution%20on%20a%20syntax-%20and%20semantics-rich%20dataset%20to%20guide%20the%0Aselection%20of%20representation-critical%20singular%20values.%20By%20selectively%20applying%0Asingular%20value%20decomposition%20%28SVD%29%20to%20affected%20layers%2C%20GRASP%20achieves%20efficient%0Acompression%20while%20maintaining%20representation%20stability%20with%20minimal%20overhead.%0AExperiments%20across%20multiple%20LLMs%20show%20that%20GRASP%20consistently%20outperforms%0Aexisting%20compression%20methods%20in%20perplexity%20and%20downstream%20task%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.00339v2&entry.124074799=Read"},
{"title": "As Good As A Coin Toss: Human detection of AI-generated images, videos,\n  audio, and audiovisual stimuli", "author": "Di Cooke and Abigail Edwards and Sophia Barkoff and Kathryn Kelly", "abstract": "  Despite advancements in technology led synthetic media authentication and\nrecent government efforts to address the threats posed by maliciously employed\nsynthetic content via the mechanisms of law or through more public education,\none of the current principal defenses against weaponized synthetic media\ncontinues to be the ability of the targeted individual to visually or\nauditorily recognize AI-generated content when they encounter it. However, as\nthe realism of synthetic media continues to rapidly improve, it is vital to\nhave an accurate understanding of just how susceptible people currently are to\npotentially being misled by convincing but false AI generated content. We\nconducted a perceptual study with 1276 participants to assess how capable\npeople were at distinguishing between authentic and synthetic images, audio,\nvideo, and audiovisual media. We find that on average, people struggled to\ndistinguish between synthetic and authentic media, with the mean detection\nperformance close to a chance level performance of 50%. We also find that\naccuracy rates worsen when the stimuli contain any degree of synthetic content,\nfeatures foreign languages, and the media type is a single modality. People are\nalso less accurate at identifying synthetic images when they feature human\nfaces, and when audiovisual stimuli have heterogeneous authenticity. Finally,\nwe find that higher degrees of prior knowledgeability about synthetic media\ndoes not significantly impact detection accuracy rates, but age does, with\nolder individuals performing worse than their younger counterparts.\nCollectively, these results highlight that it is no longer feasible to rely on\nthe perceptual capabilities of people to protect themselves against the growing\nthreat of weaponized synthetic media, and that the need for alternative\ncountermeasures is more critical than ever before.\n", "link": "http://arxiv.org/abs/2403.16760v4", "date": "2025-02-25", "relevancy": 2.4523, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5114}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4952}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20As%20Good%20As%20A%20Coin%20Toss%3A%20Human%20detection%20of%20AI-generated%20images%2C%20videos%2C%0A%20%20audio%2C%20and%20audiovisual%20stimuli&body=Title%3A%20As%20Good%20As%20A%20Coin%20Toss%3A%20Human%20detection%20of%20AI-generated%20images%2C%20videos%2C%0A%20%20audio%2C%20and%20audiovisual%20stimuli%0AAuthor%3A%20Di%20Cooke%20and%20Abigail%20Edwards%20and%20Sophia%20Barkoff%20and%20Kathryn%20Kelly%0AAbstract%3A%20%20%20Despite%20advancements%20in%20technology%20led%20synthetic%20media%20authentication%20and%0Arecent%20government%20efforts%20to%20address%20the%20threats%20posed%20by%20maliciously%20employed%0Asynthetic%20content%20via%20the%20mechanisms%20of%20law%20or%20through%20more%20public%20education%2C%0Aone%20of%20the%20current%20principal%20defenses%20against%20weaponized%20synthetic%20media%0Acontinues%20to%20be%20the%20ability%20of%20the%20targeted%20individual%20to%20visually%20or%0Aauditorily%20recognize%20AI-generated%20content%20when%20they%20encounter%20it.%20However%2C%20as%0Athe%20realism%20of%20synthetic%20media%20continues%20to%20rapidly%20improve%2C%20it%20is%20vital%20to%0Ahave%20an%20accurate%20understanding%20of%20just%20how%20susceptible%20people%20currently%20are%20to%0Apotentially%20being%20misled%20by%20convincing%20but%20false%20AI%20generated%20content.%20We%0Aconducted%20a%20perceptual%20study%20with%201276%20participants%20to%20assess%20how%20capable%0Apeople%20were%20at%20distinguishing%20between%20authentic%20and%20synthetic%20images%2C%20audio%2C%0Avideo%2C%20and%20audiovisual%20media.%20We%20find%20that%20on%20average%2C%20people%20struggled%20to%0Adistinguish%20between%20synthetic%20and%20authentic%20media%2C%20with%20the%20mean%20detection%0Aperformance%20close%20to%20a%20chance%20level%20performance%20of%2050%25.%20We%20also%20find%20that%0Aaccuracy%20rates%20worsen%20when%20the%20stimuli%20contain%20any%20degree%20of%20synthetic%20content%2C%0Afeatures%20foreign%20languages%2C%20and%20the%20media%20type%20is%20a%20single%20modality.%20People%20are%0Aalso%20less%20accurate%20at%20identifying%20synthetic%20images%20when%20they%20feature%20human%0Afaces%2C%20and%20when%20audiovisual%20stimuli%20have%20heterogeneous%20authenticity.%20Finally%2C%0Awe%20find%20that%20higher%20degrees%20of%20prior%20knowledgeability%20about%20synthetic%20media%0Adoes%20not%20significantly%20impact%20detection%20accuracy%20rates%2C%20but%20age%20does%2C%20with%0Aolder%20individuals%20performing%20worse%20than%20their%20younger%20counterparts.%0ACollectively%2C%20these%20results%20highlight%20that%20it%20is%20no%20longer%20feasible%20to%20rely%20on%0Athe%20perceptual%20capabilities%20of%20people%20to%20protect%20themselves%20against%20the%20growing%0Athreat%20of%20weaponized%20synthetic%20media%2C%20and%20that%20the%20need%20for%20alternative%0Acountermeasures%20is%20more%20critical%20than%20ever%20before.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16760v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAs%2520Good%2520As%2520A%2520Coin%2520Toss%253A%2520Human%2520detection%2520of%2520AI-generated%2520images%252C%2520videos%252C%250A%2520%2520audio%252C%2520and%2520audiovisual%2520stimuli%26entry.906535625%3DDi%2520Cooke%2520and%2520Abigail%2520Edwards%2520and%2520Sophia%2520Barkoff%2520and%2520Kathryn%2520Kelly%26entry.1292438233%3D%2520%2520Despite%2520advancements%2520in%2520technology%2520led%2520synthetic%2520media%2520authentication%2520and%250Arecent%2520government%2520efforts%2520to%2520address%2520the%2520threats%2520posed%2520by%2520maliciously%2520employed%250Asynthetic%2520content%2520via%2520the%2520mechanisms%2520of%2520law%2520or%2520through%2520more%2520public%2520education%252C%250Aone%2520of%2520the%2520current%2520principal%2520defenses%2520against%2520weaponized%2520synthetic%2520media%250Acontinues%2520to%2520be%2520the%2520ability%2520of%2520the%2520targeted%2520individual%2520to%2520visually%2520or%250Aauditorily%2520recognize%2520AI-generated%2520content%2520when%2520they%2520encounter%2520it.%2520However%252C%2520as%250Athe%2520realism%2520of%2520synthetic%2520media%2520continues%2520to%2520rapidly%2520improve%252C%2520it%2520is%2520vital%2520to%250Ahave%2520an%2520accurate%2520understanding%2520of%2520just%2520how%2520susceptible%2520people%2520currently%2520are%2520to%250Apotentially%2520being%2520misled%2520by%2520convincing%2520but%2520false%2520AI%2520generated%2520content.%2520We%250Aconducted%2520a%2520perceptual%2520study%2520with%25201276%2520participants%2520to%2520assess%2520how%2520capable%250Apeople%2520were%2520at%2520distinguishing%2520between%2520authentic%2520and%2520synthetic%2520images%252C%2520audio%252C%250Avideo%252C%2520and%2520audiovisual%2520media.%2520We%2520find%2520that%2520on%2520average%252C%2520people%2520struggled%2520to%250Adistinguish%2520between%2520synthetic%2520and%2520authentic%2520media%252C%2520with%2520the%2520mean%2520detection%250Aperformance%2520close%2520to%2520a%2520chance%2520level%2520performance%2520of%252050%2525.%2520We%2520also%2520find%2520that%250Aaccuracy%2520rates%2520worsen%2520when%2520the%2520stimuli%2520contain%2520any%2520degree%2520of%2520synthetic%2520content%252C%250Afeatures%2520foreign%2520languages%252C%2520and%2520the%2520media%2520type%2520is%2520a%2520single%2520modality.%2520People%2520are%250Aalso%2520less%2520accurate%2520at%2520identifying%2520synthetic%2520images%2520when%2520they%2520feature%2520human%250Afaces%252C%2520and%2520when%2520audiovisual%2520stimuli%2520have%2520heterogeneous%2520authenticity.%2520Finally%252C%250Awe%2520find%2520that%2520higher%2520degrees%2520of%2520prior%2520knowledgeability%2520about%2520synthetic%2520media%250Adoes%2520not%2520significantly%2520impact%2520detection%2520accuracy%2520rates%252C%2520but%2520age%2520does%252C%2520with%250Aolder%2520individuals%2520performing%2520worse%2520than%2520their%2520younger%2520counterparts.%250ACollectively%252C%2520these%2520results%2520highlight%2520that%2520it%2520is%2520no%2520longer%2520feasible%2520to%2520rely%2520on%250Athe%2520perceptual%2520capabilities%2520of%2520people%2520to%2520protect%2520themselves%2520against%2520the%2520growing%250Athreat%2520of%2520weaponized%2520synthetic%2520media%252C%2520and%2520that%2520the%2520need%2520for%2520alternative%250Acountermeasures%2520is%2520more%2520critical%2520than%2520ever%2520before.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16760v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=As%20Good%20As%20A%20Coin%20Toss%3A%20Human%20detection%20of%20AI-generated%20images%2C%20videos%2C%0A%20%20audio%2C%20and%20audiovisual%20stimuli&entry.906535625=Di%20Cooke%20and%20Abigail%20Edwards%20and%20Sophia%20Barkoff%20and%20Kathryn%20Kelly&entry.1292438233=%20%20Despite%20advancements%20in%20technology%20led%20synthetic%20media%20authentication%20and%0Arecent%20government%20efforts%20to%20address%20the%20threats%20posed%20by%20maliciously%20employed%0Asynthetic%20content%20via%20the%20mechanisms%20of%20law%20or%20through%20more%20public%20education%2C%0Aone%20of%20the%20current%20principal%20defenses%20against%20weaponized%20synthetic%20media%0Acontinues%20to%20be%20the%20ability%20of%20the%20targeted%20individual%20to%20visually%20or%0Aauditorily%20recognize%20AI-generated%20content%20when%20they%20encounter%20it.%20However%2C%20as%0Athe%20realism%20of%20synthetic%20media%20continues%20to%20rapidly%20improve%2C%20it%20is%20vital%20to%0Ahave%20an%20accurate%20understanding%20of%20just%20how%20susceptible%20people%20currently%20are%20to%0Apotentially%20being%20misled%20by%20convincing%20but%20false%20AI%20generated%20content.%20We%0Aconducted%20a%20perceptual%20study%20with%201276%20participants%20to%20assess%20how%20capable%0Apeople%20were%20at%20distinguishing%20between%20authentic%20and%20synthetic%20images%2C%20audio%2C%0Avideo%2C%20and%20audiovisual%20media.%20We%20find%20that%20on%20average%2C%20people%20struggled%20to%0Adistinguish%20between%20synthetic%20and%20authentic%20media%2C%20with%20the%20mean%20detection%0Aperformance%20close%20to%20a%20chance%20level%20performance%20of%2050%25.%20We%20also%20find%20that%0Aaccuracy%20rates%20worsen%20when%20the%20stimuli%20contain%20any%20degree%20of%20synthetic%20content%2C%0Afeatures%20foreign%20languages%2C%20and%20the%20media%20type%20is%20a%20single%20modality.%20People%20are%0Aalso%20less%20accurate%20at%20identifying%20synthetic%20images%20when%20they%20feature%20human%0Afaces%2C%20and%20when%20audiovisual%20stimuli%20have%20heterogeneous%20authenticity.%20Finally%2C%0Awe%20find%20that%20higher%20degrees%20of%20prior%20knowledgeability%20about%20synthetic%20media%0Adoes%20not%20significantly%20impact%20detection%20accuracy%20rates%2C%20but%20age%20does%2C%20with%0Aolder%20individuals%20performing%20worse%20than%20their%20younger%20counterparts.%0ACollectively%2C%20these%20results%20highlight%20that%20it%20is%20no%20longer%20feasible%20to%20rely%20on%0Athe%20perceptual%20capabilities%20of%20people%20to%20protect%20themselves%20against%20the%20growing%0Athreat%20of%20weaponized%20synthetic%20media%2C%20and%20that%20the%20need%20for%20alternative%0Acountermeasures%20is%20more%20critical%20than%20ever%20before.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16760v4&entry.124074799=Read"},
{"title": "Leveraging 2D Masked Reconstruction for Domain Adaptation of 3D Pose\n  Estimation", "author": "Hansoo Park and Chanwoo Kim and Jihyeon Kim and Hoseong Cho and Nhat Nguyen Bao Truong and Taehwan Kim and Seungryul Baek", "abstract": "  RGB-based 3D pose estimation methods have been successful with the\ndevelopment of deep learning and the emergence of high-quality 3D pose\ndatasets. However, most existing methods do not operate well for testing images\nwhose distribution is far from that of training data. However, most existing\nmethods do not operate well for testing images whose distribution is far from\nthat of training data. This problem might be alleviated by involving diverse\ndata during training, however it is non-trivial to collect such diverse data\nwith corresponding labels (i.e. 3D pose). In this paper, we introduced an\nunsupervised domain adaptation framework for 3D pose estimation that utilizes\nthe unlabeled data in addition to labeled data via masked image modeling (MIM)\nframework. Foreground-centric reconstruction and attention regularization are\nfurther proposed to increase the effectiveness of unlabeled data usage.\nExperiments are conducted on the various datasets in human and hand pose\nestimation tasks, especially using the cross-domain scenario. We demonstrated\nthe effectiveness of ours by achieving the state-of-the-art accuracy on all\ndatasets.\n", "link": "http://arxiv.org/abs/2501.08408v2", "date": "2025-02-25", "relevancy": 2.4506, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6174}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6116}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%202D%20Masked%20Reconstruction%20for%20Domain%20Adaptation%20of%203D%20Pose%0A%20%20Estimation&body=Title%3A%20Leveraging%202D%20Masked%20Reconstruction%20for%20Domain%20Adaptation%20of%203D%20Pose%0A%20%20Estimation%0AAuthor%3A%20Hansoo%20Park%20and%20Chanwoo%20Kim%20and%20Jihyeon%20Kim%20and%20Hoseong%20Cho%20and%20Nhat%20Nguyen%20Bao%20Truong%20and%20Taehwan%20Kim%20and%20Seungryul%20Baek%0AAbstract%3A%20%20%20RGB-based%203D%20pose%20estimation%20methods%20have%20been%20successful%20with%20the%0Adevelopment%20of%20deep%20learning%20and%20the%20emergence%20of%20high-quality%203D%20pose%0Adatasets.%20However%2C%20most%20existing%20methods%20do%20not%20operate%20well%20for%20testing%20images%0Awhose%20distribution%20is%20far%20from%20that%20of%20training%20data.%20However%2C%20most%20existing%0Amethods%20do%20not%20operate%20well%20for%20testing%20images%20whose%20distribution%20is%20far%20from%0Athat%20of%20training%20data.%20This%20problem%20might%20be%20alleviated%20by%20involving%20diverse%0Adata%20during%20training%2C%20however%20it%20is%20non-trivial%20to%20collect%20such%20diverse%20data%0Awith%20corresponding%20labels%20%28i.e.%203D%20pose%29.%20In%20this%20paper%2C%20we%20introduced%20an%0Aunsupervised%20domain%20adaptation%20framework%20for%203D%20pose%20estimation%20that%20utilizes%0Athe%20unlabeled%20data%20in%20addition%20to%20labeled%20data%20via%20masked%20image%20modeling%20%28MIM%29%0Aframework.%20Foreground-centric%20reconstruction%20and%20attention%20regularization%20are%0Afurther%20proposed%20to%20increase%20the%20effectiveness%20of%20unlabeled%20data%20usage.%0AExperiments%20are%20conducted%20on%20the%20various%20datasets%20in%20human%20and%20hand%20pose%0Aestimation%20tasks%2C%20especially%20using%20the%20cross-domain%20scenario.%20We%20demonstrated%0Athe%20effectiveness%20of%20ours%20by%20achieving%20the%20state-of-the-art%20accuracy%20on%20all%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08408v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%25202D%2520Masked%2520Reconstruction%2520for%2520Domain%2520Adaptation%2520of%25203D%2520Pose%250A%2520%2520Estimation%26entry.906535625%3DHansoo%2520Park%2520and%2520Chanwoo%2520Kim%2520and%2520Jihyeon%2520Kim%2520and%2520Hoseong%2520Cho%2520and%2520Nhat%2520Nguyen%2520Bao%2520Truong%2520and%2520Taehwan%2520Kim%2520and%2520Seungryul%2520Baek%26entry.1292438233%3D%2520%2520RGB-based%25203D%2520pose%2520estimation%2520methods%2520have%2520been%2520successful%2520with%2520the%250Adevelopment%2520of%2520deep%2520learning%2520and%2520the%2520emergence%2520of%2520high-quality%25203D%2520pose%250Adatasets.%2520However%252C%2520most%2520existing%2520methods%2520do%2520not%2520operate%2520well%2520for%2520testing%2520images%250Awhose%2520distribution%2520is%2520far%2520from%2520that%2520of%2520training%2520data.%2520However%252C%2520most%2520existing%250Amethods%2520do%2520not%2520operate%2520well%2520for%2520testing%2520images%2520whose%2520distribution%2520is%2520far%2520from%250Athat%2520of%2520training%2520data.%2520This%2520problem%2520might%2520be%2520alleviated%2520by%2520involving%2520diverse%250Adata%2520during%2520training%252C%2520however%2520it%2520is%2520non-trivial%2520to%2520collect%2520such%2520diverse%2520data%250Awith%2520corresponding%2520labels%2520%2528i.e.%25203D%2520pose%2529.%2520In%2520this%2520paper%252C%2520we%2520introduced%2520an%250Aunsupervised%2520domain%2520adaptation%2520framework%2520for%25203D%2520pose%2520estimation%2520that%2520utilizes%250Athe%2520unlabeled%2520data%2520in%2520addition%2520to%2520labeled%2520data%2520via%2520masked%2520image%2520modeling%2520%2528MIM%2529%250Aframework.%2520Foreground-centric%2520reconstruction%2520and%2520attention%2520regularization%2520are%250Afurther%2520proposed%2520to%2520increase%2520the%2520effectiveness%2520of%2520unlabeled%2520data%2520usage.%250AExperiments%2520are%2520conducted%2520on%2520the%2520various%2520datasets%2520in%2520human%2520and%2520hand%2520pose%250Aestimation%2520tasks%252C%2520especially%2520using%2520the%2520cross-domain%2520scenario.%2520We%2520demonstrated%250Athe%2520effectiveness%2520of%2520ours%2520by%2520achieving%2520the%2520state-of-the-art%2520accuracy%2520on%2520all%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08408v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%202D%20Masked%20Reconstruction%20for%20Domain%20Adaptation%20of%203D%20Pose%0A%20%20Estimation&entry.906535625=Hansoo%20Park%20and%20Chanwoo%20Kim%20and%20Jihyeon%20Kim%20and%20Hoseong%20Cho%20and%20Nhat%20Nguyen%20Bao%20Truong%20and%20Taehwan%20Kim%20and%20Seungryul%20Baek&entry.1292438233=%20%20RGB-based%203D%20pose%20estimation%20methods%20have%20been%20successful%20with%20the%0Adevelopment%20of%20deep%20learning%20and%20the%20emergence%20of%20high-quality%203D%20pose%0Adatasets.%20However%2C%20most%20existing%20methods%20do%20not%20operate%20well%20for%20testing%20images%0Awhose%20distribution%20is%20far%20from%20that%20of%20training%20data.%20However%2C%20most%20existing%0Amethods%20do%20not%20operate%20well%20for%20testing%20images%20whose%20distribution%20is%20far%20from%0Athat%20of%20training%20data.%20This%20problem%20might%20be%20alleviated%20by%20involving%20diverse%0Adata%20during%20training%2C%20however%20it%20is%20non-trivial%20to%20collect%20such%20diverse%20data%0Awith%20corresponding%20labels%20%28i.e.%203D%20pose%29.%20In%20this%20paper%2C%20we%20introduced%20an%0Aunsupervised%20domain%20adaptation%20framework%20for%203D%20pose%20estimation%20that%20utilizes%0Athe%20unlabeled%20data%20in%20addition%20to%20labeled%20data%20via%20masked%20image%20modeling%20%28MIM%29%0Aframework.%20Foreground-centric%20reconstruction%20and%20attention%20regularization%20are%0Afurther%20proposed%20to%20increase%20the%20effectiveness%20of%20unlabeled%20data%20usage.%0AExperiments%20are%20conducted%20on%20the%20various%20datasets%20in%20human%20and%20hand%20pose%0Aestimation%20tasks%2C%20especially%20using%20the%20cross-domain%20scenario.%20We%20demonstrated%0Athe%20effectiveness%20of%20ours%20by%20achieving%20the%20state-of-the-art%20accuracy%20on%20all%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08408v2&entry.124074799=Read"},
{"title": "Generative Psycho-Lexical Approach for Constructing Value Systems in\n  Large Language Models", "author": "Haoran Ye and Tianze Zhang and Yuhang Xie and Liyuan Zhang and Yuanyi Ren and Xin Zhang and Guojie Song", "abstract": "  Values are core drivers of individual and collective perception, cognition,\nand behavior. Value systems, such as Schwartz's Theory of Basic Human Values,\ndelineate the hierarchy and interplay among these values, enabling\ncross-disciplinary investigations into decision-making and societal dynamics.\nRecently, the rise of Large Language Models (LLMs) has raised concerns\nregarding their elusive intrinsic values. Despite growing efforts in\nevaluating, understanding, and aligning LLM values, a psychologically grounded\nLLM value system remains underexplored. This study addresses the gap by\nintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,\nadaptable, and theoretically informed method for constructing value systems.\nLeveraging GPLA, we propose a psychologically grounded five-factor value system\ntailored for LLMs. For systematic validation, we present three benchmarking\ntasks that integrate psychological principles with cutting-edge AI priorities.\nOur results reveal that the proposed value system meets standard psychological\ncriteria, better captures LLM values, improves LLM safety prediction, and\nenhances LLM alignment, when compared to the canonical Schwartz's values.\n", "link": "http://arxiv.org/abs/2502.02444v3", "date": "2025-02-25", "relevancy": 2.4467, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Psycho-Lexical%20Approach%20for%20Constructing%20Value%20Systems%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20Generative%20Psycho-Lexical%20Approach%20for%20Constructing%20Value%20Systems%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Haoran%20Ye%20and%20Tianze%20Zhang%20and%20Yuhang%20Xie%20and%20Liyuan%20Zhang%20and%20Yuanyi%20Ren%20and%20Xin%20Zhang%20and%20Guojie%20Song%0AAbstract%3A%20%20%20Values%20are%20core%20drivers%20of%20individual%20and%20collective%20perception%2C%20cognition%2C%0Aand%20behavior.%20Value%20systems%2C%20such%20as%20Schwartz%27s%20Theory%20of%20Basic%20Human%20Values%2C%0Adelineate%20the%20hierarchy%20and%20interplay%20among%20these%20values%2C%20enabling%0Across-disciplinary%20investigations%20into%20decision-making%20and%20societal%20dynamics.%0ARecently%2C%20the%20rise%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20raised%20concerns%0Aregarding%20their%20elusive%20intrinsic%20values.%20Despite%20growing%20efforts%20in%0Aevaluating%2C%20understanding%2C%20and%20aligning%20LLM%20values%2C%20a%20psychologically%20grounded%0ALLM%20value%20system%20remains%20underexplored.%20This%20study%20addresses%20the%20gap%20by%0Aintroducing%20the%20Generative%20Psycho-Lexical%20Approach%20%28GPLA%29%2C%20a%20scalable%2C%0Aadaptable%2C%20and%20theoretically%20informed%20method%20for%20constructing%20value%20systems.%0ALeveraging%20GPLA%2C%20we%20propose%20a%20psychologically%20grounded%20five-factor%20value%20system%0Atailored%20for%20LLMs.%20For%20systematic%20validation%2C%20we%20present%20three%20benchmarking%0Atasks%20that%20integrate%20psychological%20principles%20with%20cutting-edge%20AI%20priorities.%0AOur%20results%20reveal%20that%20the%20proposed%20value%20system%20meets%20standard%20psychological%0Acriteria%2C%20better%20captures%20LLM%20values%2C%20improves%20LLM%20safety%20prediction%2C%20and%0Aenhances%20LLM%20alignment%2C%20when%20compared%20to%20the%20canonical%20Schwartz%27s%20values.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02444v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Psycho-Lexical%2520Approach%2520for%2520Constructing%2520Value%2520Systems%2520in%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DHaoran%2520Ye%2520and%2520Tianze%2520Zhang%2520and%2520Yuhang%2520Xie%2520and%2520Liyuan%2520Zhang%2520and%2520Yuanyi%2520Ren%2520and%2520Xin%2520Zhang%2520and%2520Guojie%2520Song%26entry.1292438233%3D%2520%2520Values%2520are%2520core%2520drivers%2520of%2520individual%2520and%2520collective%2520perception%252C%2520cognition%252C%250Aand%2520behavior.%2520Value%2520systems%252C%2520such%2520as%2520Schwartz%2527s%2520Theory%2520of%2520Basic%2520Human%2520Values%252C%250Adelineate%2520the%2520hierarchy%2520and%2520interplay%2520among%2520these%2520values%252C%2520enabling%250Across-disciplinary%2520investigations%2520into%2520decision-making%2520and%2520societal%2520dynamics.%250ARecently%252C%2520the%2520rise%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520raised%2520concerns%250Aregarding%2520their%2520elusive%2520intrinsic%2520values.%2520Despite%2520growing%2520efforts%2520in%250Aevaluating%252C%2520understanding%252C%2520and%2520aligning%2520LLM%2520values%252C%2520a%2520psychologically%2520grounded%250ALLM%2520value%2520system%2520remains%2520underexplored.%2520This%2520study%2520addresses%2520the%2520gap%2520by%250Aintroducing%2520the%2520Generative%2520Psycho-Lexical%2520Approach%2520%2528GPLA%2529%252C%2520a%2520scalable%252C%250Aadaptable%252C%2520and%2520theoretically%2520informed%2520method%2520for%2520constructing%2520value%2520systems.%250ALeveraging%2520GPLA%252C%2520we%2520propose%2520a%2520psychologically%2520grounded%2520five-factor%2520value%2520system%250Atailored%2520for%2520LLMs.%2520For%2520systematic%2520validation%252C%2520we%2520present%2520three%2520benchmarking%250Atasks%2520that%2520integrate%2520psychological%2520principles%2520with%2520cutting-edge%2520AI%2520priorities.%250AOur%2520results%2520reveal%2520that%2520the%2520proposed%2520value%2520system%2520meets%2520standard%2520psychological%250Acriteria%252C%2520better%2520captures%2520LLM%2520values%252C%2520improves%2520LLM%2520safety%2520prediction%252C%2520and%250Aenhances%2520LLM%2520alignment%252C%2520when%2520compared%2520to%2520the%2520canonical%2520Schwartz%2527s%2520values.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02444v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Psycho-Lexical%20Approach%20for%20Constructing%20Value%20Systems%20in%0A%20%20Large%20Language%20Models&entry.906535625=Haoran%20Ye%20and%20Tianze%20Zhang%20and%20Yuhang%20Xie%20and%20Liyuan%20Zhang%20and%20Yuanyi%20Ren%20and%20Xin%20Zhang%20and%20Guojie%20Song&entry.1292438233=%20%20Values%20are%20core%20drivers%20of%20individual%20and%20collective%20perception%2C%20cognition%2C%0Aand%20behavior.%20Value%20systems%2C%20such%20as%20Schwartz%27s%20Theory%20of%20Basic%20Human%20Values%2C%0Adelineate%20the%20hierarchy%20and%20interplay%20among%20these%20values%2C%20enabling%0Across-disciplinary%20investigations%20into%20decision-making%20and%20societal%20dynamics.%0ARecently%2C%20the%20rise%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20raised%20concerns%0Aregarding%20their%20elusive%20intrinsic%20values.%20Despite%20growing%20efforts%20in%0Aevaluating%2C%20understanding%2C%20and%20aligning%20LLM%20values%2C%20a%20psychologically%20grounded%0ALLM%20value%20system%20remains%20underexplored.%20This%20study%20addresses%20the%20gap%20by%0Aintroducing%20the%20Generative%20Psycho-Lexical%20Approach%20%28GPLA%29%2C%20a%20scalable%2C%0Aadaptable%2C%20and%20theoretically%20informed%20method%20for%20constructing%20value%20systems.%0ALeveraging%20GPLA%2C%20we%20propose%20a%20psychologically%20grounded%20five-factor%20value%20system%0Atailored%20for%20LLMs.%20For%20systematic%20validation%2C%20we%20present%20three%20benchmarking%0Atasks%20that%20integrate%20psychological%20principles%20with%20cutting-edge%20AI%20priorities.%0AOur%20results%20reveal%20that%20the%20proposed%20value%20system%20meets%20standard%20psychological%0Acriteria%2C%20better%20captures%20LLM%20values%2C%20improves%20LLM%20safety%20prediction%2C%20and%0Aenhances%20LLM%20alignment%2C%20when%20compared%20to%20the%20canonical%20Schwartz%27s%20values.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02444v3&entry.124074799=Read"},
{"title": "Structural Alignment Improves Graph Test-Time Adaptation", "author": "Hans Hao-Hsun Hsu and Shikun Liu and Han Zhao and Pan Li", "abstract": "  Graph-based learning has achieved remarkable success in domains ranging from\nrecommendation to fraud detection and particle physics by effectively capturing\nunderlying interaction patterns. However, it often struggles to generalize when\ndistribution shifts occur, particularly those involving changes in network\nconnectivity or interaction patterns. Existing approaches designed to mitigate\nsuch shifts typically require retraining with full access to source data,\nrendering them infeasible under strict computational or privacy constraints. To\naddress this limitation, we propose a test-time structural alignment (TSA)\nalgorithm for Graph Test-Time Adaptation (GTTA), a novel method that aligns\ngraph structures during inference without revisiting the source domain. Built\nupon a theoretically grounded treatment of graph data distribution shifts, TSA\nintegrates three key strategies: an uncertainty-aware neighborhood weighting\nthat accommodates structure shifts, an adaptive balancing of self-node and\nneighborhood-aggregated representations driven by node representations'\nsignal-to-noise ratio, and a decision boundary refinement that corrects\nremaining label and feature shifts. Extensive experiments on synthetic and\nreal-world datasets demonstrate that TSA can consistently outperform both\nnon-graph TTA methods and state-of-the-art GTTA baselines.\n", "link": "http://arxiv.org/abs/2502.18334v1", "date": "2025-02-25", "relevancy": 2.441, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4975}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4851}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structural%20Alignment%20Improves%20Graph%20Test-Time%20Adaptation&body=Title%3A%20Structural%20Alignment%20Improves%20Graph%20Test-Time%20Adaptation%0AAuthor%3A%20Hans%20Hao-Hsun%20Hsu%20and%20Shikun%20Liu%20and%20Han%20Zhao%20and%20Pan%20Li%0AAbstract%3A%20%20%20Graph-based%20learning%20has%20achieved%20remarkable%20success%20in%20domains%20ranging%20from%0Arecommendation%20to%20fraud%20detection%20and%20particle%20physics%20by%20effectively%20capturing%0Aunderlying%20interaction%20patterns.%20However%2C%20it%20often%20struggles%20to%20generalize%20when%0Adistribution%20shifts%20occur%2C%20particularly%20those%20involving%20changes%20in%20network%0Aconnectivity%20or%20interaction%20patterns.%20Existing%20approaches%20designed%20to%20mitigate%0Asuch%20shifts%20typically%20require%20retraining%20with%20full%20access%20to%20source%20data%2C%0Arendering%20them%20infeasible%20under%20strict%20computational%20or%20privacy%20constraints.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20a%20test-time%20structural%20alignment%20%28TSA%29%0Aalgorithm%20for%20Graph%20Test-Time%20Adaptation%20%28GTTA%29%2C%20a%20novel%20method%20that%20aligns%0Agraph%20structures%20during%20inference%20without%20revisiting%20the%20source%20domain.%20Built%0Aupon%20a%20theoretically%20grounded%20treatment%20of%20graph%20data%20distribution%20shifts%2C%20TSA%0Aintegrates%20three%20key%20strategies%3A%20an%20uncertainty-aware%20neighborhood%20weighting%0Athat%20accommodates%20structure%20shifts%2C%20an%20adaptive%20balancing%20of%20self-node%20and%0Aneighborhood-aggregated%20representations%20driven%20by%20node%20representations%27%0Asignal-to-noise%20ratio%2C%20and%20a%20decision%20boundary%20refinement%20that%20corrects%0Aremaining%20label%20and%20feature%20shifts.%20Extensive%20experiments%20on%20synthetic%20and%0Areal-world%20datasets%20demonstrate%20that%20TSA%20can%20consistently%20outperform%20both%0Anon-graph%20TTA%20methods%20and%20state-of-the-art%20GTTA%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructural%2520Alignment%2520Improves%2520Graph%2520Test-Time%2520Adaptation%26entry.906535625%3DHans%2520Hao-Hsun%2520Hsu%2520and%2520Shikun%2520Liu%2520and%2520Han%2520Zhao%2520and%2520Pan%2520Li%26entry.1292438233%3D%2520%2520Graph-based%2520learning%2520has%2520achieved%2520remarkable%2520success%2520in%2520domains%2520ranging%2520from%250Arecommendation%2520to%2520fraud%2520detection%2520and%2520particle%2520physics%2520by%2520effectively%2520capturing%250Aunderlying%2520interaction%2520patterns.%2520However%252C%2520it%2520often%2520struggles%2520to%2520generalize%2520when%250Adistribution%2520shifts%2520occur%252C%2520particularly%2520those%2520involving%2520changes%2520in%2520network%250Aconnectivity%2520or%2520interaction%2520patterns.%2520Existing%2520approaches%2520designed%2520to%2520mitigate%250Asuch%2520shifts%2520typically%2520require%2520retraining%2520with%2520full%2520access%2520to%2520source%2520data%252C%250Arendering%2520them%2520infeasible%2520under%2520strict%2520computational%2520or%2520privacy%2520constraints.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520propose%2520a%2520test-time%2520structural%2520alignment%2520%2528TSA%2529%250Aalgorithm%2520for%2520Graph%2520Test-Time%2520Adaptation%2520%2528GTTA%2529%252C%2520a%2520novel%2520method%2520that%2520aligns%250Agraph%2520structures%2520during%2520inference%2520without%2520revisiting%2520the%2520source%2520domain.%2520Built%250Aupon%2520a%2520theoretically%2520grounded%2520treatment%2520of%2520graph%2520data%2520distribution%2520shifts%252C%2520TSA%250Aintegrates%2520three%2520key%2520strategies%253A%2520an%2520uncertainty-aware%2520neighborhood%2520weighting%250Athat%2520accommodates%2520structure%2520shifts%252C%2520an%2520adaptive%2520balancing%2520of%2520self-node%2520and%250Aneighborhood-aggregated%2520representations%2520driven%2520by%2520node%2520representations%2527%250Asignal-to-noise%2520ratio%252C%2520and%2520a%2520decision%2520boundary%2520refinement%2520that%2520corrects%250Aremaining%2520label%2520and%2520feature%2520shifts.%2520Extensive%2520experiments%2520on%2520synthetic%2520and%250Areal-world%2520datasets%2520demonstrate%2520that%2520TSA%2520can%2520consistently%2520outperform%2520both%250Anon-graph%2520TTA%2520methods%2520and%2520state-of-the-art%2520GTTA%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structural%20Alignment%20Improves%20Graph%20Test-Time%20Adaptation&entry.906535625=Hans%20Hao-Hsun%20Hsu%20and%20Shikun%20Liu%20and%20Han%20Zhao%20and%20Pan%20Li&entry.1292438233=%20%20Graph-based%20learning%20has%20achieved%20remarkable%20success%20in%20domains%20ranging%20from%0Arecommendation%20to%20fraud%20detection%20and%20particle%20physics%20by%20effectively%20capturing%0Aunderlying%20interaction%20patterns.%20However%2C%20it%20often%20struggles%20to%20generalize%20when%0Adistribution%20shifts%20occur%2C%20particularly%20those%20involving%20changes%20in%20network%0Aconnectivity%20or%20interaction%20patterns.%20Existing%20approaches%20designed%20to%20mitigate%0Asuch%20shifts%20typically%20require%20retraining%20with%20full%20access%20to%20source%20data%2C%0Arendering%20them%20infeasible%20under%20strict%20computational%20or%20privacy%20constraints.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20a%20test-time%20structural%20alignment%20%28TSA%29%0Aalgorithm%20for%20Graph%20Test-Time%20Adaptation%20%28GTTA%29%2C%20a%20novel%20method%20that%20aligns%0Agraph%20structures%20during%20inference%20without%20revisiting%20the%20source%20domain.%20Built%0Aupon%20a%20theoretically%20grounded%20treatment%20of%20graph%20data%20distribution%20shifts%2C%20TSA%0Aintegrates%20three%20key%20strategies%3A%20an%20uncertainty-aware%20neighborhood%20weighting%0Athat%20accommodates%20structure%20shifts%2C%20an%20adaptive%20balancing%20of%20self-node%20and%0Aneighborhood-aggregated%20representations%20driven%20by%20node%20representations%27%0Asignal-to-noise%20ratio%2C%20and%20a%20decision%20boundary%20refinement%20that%20corrects%0Aremaining%20label%20and%20feature%20shifts.%20Extensive%20experiments%20on%20synthetic%20and%0Areal-world%20datasets%20demonstrate%20that%20TSA%20can%20consistently%20outperform%20both%0Anon-graph%20TTA%20methods%20and%20state-of-the-art%20GTTA%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18334v1&entry.124074799=Read"},
{"title": "A Self-Explainable Heterogeneous GNN for Relational Deep Learning", "author": "Francesco Ferrini and Antonio Longa and Andrea Passerini and Manfred Jaeger", "abstract": "  Recently, significant attention has been given to the idea of viewing\nrelational databases as heterogeneous graphs, enabling the application of graph\nneural network (GNN) technology for predictive tasks. However, existing GNN\nmethods struggle with the complexity of the heterogeneous graphs induced by\ndatabases with numerous tables and relations. Traditional approaches either\nconsider all possible relational meta-paths, thus failing to scale with the\nnumber of relations, or rely on domain experts to identify relevant meta-paths.\nA recent solution does manage to learn informative meta-paths without expert\nsupervision, but assumes that a node's class depends solely on the existence of\na meta-path occurrence. In this work, we present a self-explainable\nheterogeneous GNN for relational data, that supports models in which class\nmembership depends on aggregate information obtained from multiple occurrences\nof a meta-path. Experimental results show that in the context of relational\ndatabases, our approach effectively identifies informative meta-paths that\nfaithfully capture the model's reasoning mechanisms. It significantly\noutperforms existing methods in both synthetic and real-world scenario.\n", "link": "http://arxiv.org/abs/2412.00521v2", "date": "2025-02-25", "relevancy": 2.4282, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.494}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4932}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Self-Explainable%20Heterogeneous%20GNN%20for%20Relational%20Deep%20Learning&body=Title%3A%20A%20Self-Explainable%20Heterogeneous%20GNN%20for%20Relational%20Deep%20Learning%0AAuthor%3A%20Francesco%20Ferrini%20and%20Antonio%20Longa%20and%20Andrea%20Passerini%20and%20Manfred%20Jaeger%0AAbstract%3A%20%20%20Recently%2C%20significant%20attention%20has%20been%20given%20to%20the%20idea%20of%20viewing%0Arelational%20databases%20as%20heterogeneous%20graphs%2C%20enabling%20the%20application%20of%20graph%0Aneural%20network%20%28GNN%29%20technology%20for%20predictive%20tasks.%20However%2C%20existing%20GNN%0Amethods%20struggle%20with%20the%20complexity%20of%20the%20heterogeneous%20graphs%20induced%20by%0Adatabases%20with%20numerous%20tables%20and%20relations.%20Traditional%20approaches%20either%0Aconsider%20all%20possible%20relational%20meta-paths%2C%20thus%20failing%20to%20scale%20with%20the%0Anumber%20of%20relations%2C%20or%20rely%20on%20domain%20experts%20to%20identify%20relevant%20meta-paths.%0AA%20recent%20solution%20does%20manage%20to%20learn%20informative%20meta-paths%20without%20expert%0Asupervision%2C%20but%20assumes%20that%20a%20node%27s%20class%20depends%20solely%20on%20the%20existence%20of%0Aa%20meta-path%20occurrence.%20In%20this%20work%2C%20we%20present%20a%20self-explainable%0Aheterogeneous%20GNN%20for%20relational%20data%2C%20that%20supports%20models%20in%20which%20class%0Amembership%20depends%20on%20aggregate%20information%20obtained%20from%20multiple%20occurrences%0Aof%20a%20meta-path.%20Experimental%20results%20show%20that%20in%20the%20context%20of%20relational%0Adatabases%2C%20our%20approach%20effectively%20identifies%20informative%20meta-paths%20that%0Afaithfully%20capture%20the%20model%27s%20reasoning%20mechanisms.%20It%20significantly%0Aoutperforms%20existing%20methods%20in%20both%20synthetic%20and%20real-world%20scenario.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.00521v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Self-Explainable%2520Heterogeneous%2520GNN%2520for%2520Relational%2520Deep%2520Learning%26entry.906535625%3DFrancesco%2520Ferrini%2520and%2520Antonio%2520Longa%2520and%2520Andrea%2520Passerini%2520and%2520Manfred%2520Jaeger%26entry.1292438233%3D%2520%2520Recently%252C%2520significant%2520attention%2520has%2520been%2520given%2520to%2520the%2520idea%2520of%2520viewing%250Arelational%2520databases%2520as%2520heterogeneous%2520graphs%252C%2520enabling%2520the%2520application%2520of%2520graph%250Aneural%2520network%2520%2528GNN%2529%2520technology%2520for%2520predictive%2520tasks.%2520However%252C%2520existing%2520GNN%250Amethods%2520struggle%2520with%2520the%2520complexity%2520of%2520the%2520heterogeneous%2520graphs%2520induced%2520by%250Adatabases%2520with%2520numerous%2520tables%2520and%2520relations.%2520Traditional%2520approaches%2520either%250Aconsider%2520all%2520possible%2520relational%2520meta-paths%252C%2520thus%2520failing%2520to%2520scale%2520with%2520the%250Anumber%2520of%2520relations%252C%2520or%2520rely%2520on%2520domain%2520experts%2520to%2520identify%2520relevant%2520meta-paths.%250AA%2520recent%2520solution%2520does%2520manage%2520to%2520learn%2520informative%2520meta-paths%2520without%2520expert%250Asupervision%252C%2520but%2520assumes%2520that%2520a%2520node%2527s%2520class%2520depends%2520solely%2520on%2520the%2520existence%2520of%250Aa%2520meta-path%2520occurrence.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520self-explainable%250Aheterogeneous%2520GNN%2520for%2520relational%2520data%252C%2520that%2520supports%2520models%2520in%2520which%2520class%250Amembership%2520depends%2520on%2520aggregate%2520information%2520obtained%2520from%2520multiple%2520occurrences%250Aof%2520a%2520meta-path.%2520Experimental%2520results%2520show%2520that%2520in%2520the%2520context%2520of%2520relational%250Adatabases%252C%2520our%2520approach%2520effectively%2520identifies%2520informative%2520meta-paths%2520that%250Afaithfully%2520capture%2520the%2520model%2527s%2520reasoning%2520mechanisms.%2520It%2520significantly%250Aoutperforms%2520existing%2520methods%2520in%2520both%2520synthetic%2520and%2520real-world%2520scenario.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.00521v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Self-Explainable%20Heterogeneous%20GNN%20for%20Relational%20Deep%20Learning&entry.906535625=Francesco%20Ferrini%20and%20Antonio%20Longa%20and%20Andrea%20Passerini%20and%20Manfred%20Jaeger&entry.1292438233=%20%20Recently%2C%20significant%20attention%20has%20been%20given%20to%20the%20idea%20of%20viewing%0Arelational%20databases%20as%20heterogeneous%20graphs%2C%20enabling%20the%20application%20of%20graph%0Aneural%20network%20%28GNN%29%20technology%20for%20predictive%20tasks.%20However%2C%20existing%20GNN%0Amethods%20struggle%20with%20the%20complexity%20of%20the%20heterogeneous%20graphs%20induced%20by%0Adatabases%20with%20numerous%20tables%20and%20relations.%20Traditional%20approaches%20either%0Aconsider%20all%20possible%20relational%20meta-paths%2C%20thus%20failing%20to%20scale%20with%20the%0Anumber%20of%20relations%2C%20or%20rely%20on%20domain%20experts%20to%20identify%20relevant%20meta-paths.%0AA%20recent%20solution%20does%20manage%20to%20learn%20informative%20meta-paths%20without%20expert%0Asupervision%2C%20but%20assumes%20that%20a%20node%27s%20class%20depends%20solely%20on%20the%20existence%20of%0Aa%20meta-path%20occurrence.%20In%20this%20work%2C%20we%20present%20a%20self-explainable%0Aheterogeneous%20GNN%20for%20relational%20data%2C%20that%20supports%20models%20in%20which%20class%0Amembership%20depends%20on%20aggregate%20information%20obtained%20from%20multiple%20occurrences%0Aof%20a%20meta-path.%20Experimental%20results%20show%20that%20in%20the%20context%20of%20relational%0Adatabases%2C%20our%20approach%20effectively%20identifies%20informative%20meta-paths%20that%0Afaithfully%20capture%20the%20model%27s%20reasoning%20mechanisms.%20It%20significantly%0Aoutperforms%20existing%20methods%20in%20both%20synthetic%20and%20real-world%20scenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.00521v2&entry.124074799=Read"},
{"title": "Guiding Through Complexity: What Makes Good Supervision for Hard Math\n  Reasoning Tasks?", "author": "Xuan He and Da Yin and Nanyun Peng", "abstract": "  How can \"weak teacher models\" such as average human annotators or existing AI\nsystems, effectively supervise LLMs to improve performance on hard reasoning\ntasks, especially those that challenge and requires expertise or daily practice\nfrom the teacher models? In this paper, we seek for empirical answers to this\nquestion by investigating various data-driven strategies that offer supervision\ndata at different quality levels upon tasks of varying complexity. Two\nintuitive strategies emerge for teacher models to provide supervision during\nalignment training: 1) using lower-quality supervision from complete tasks that\nmatch the difficulty of the target reasoning tasks, and 2) leveraging\nhigher-quality supervision from easier subtasks that are less challenging.\nInterestingly, we find that even when the outcome error rate for hard task\nsupervision is high (e.g., 90\\%), training on such data can outperform\nperfectly correct supervision of easier subtasks on multiple hard math\nbenchmarks. We further identify a more critical factor influencing training\nperformance: step-wise error rates, which indicate the severity of errors in\nsolutions. Specifically, training on hard task supervision with the same\noutcome error rates but disparate step-wise error rates can lead to a 30\\%\naccuracy gap on MATH benchmark. Our results also reveal that supplementing hard\ntask supervision with the corresponding subtask supervision can yield notable\nperformance improvements than simply combining rephrased hard full task\nsupervision, suggesting new avenues for data augmentation. Data and code are\nreleased at https://github.com/hexuan21/Weak-to-Strong.\n", "link": "http://arxiv.org/abs/2410.20533v3", "date": "2025-02-25", "relevancy": 2.4139, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4887}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4887}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guiding%20Through%20Complexity%3A%20What%20Makes%20Good%20Supervision%20for%20Hard%20Math%0A%20%20Reasoning%20Tasks%3F&body=Title%3A%20Guiding%20Through%20Complexity%3A%20What%20Makes%20Good%20Supervision%20for%20Hard%20Math%0A%20%20Reasoning%20Tasks%3F%0AAuthor%3A%20Xuan%20He%20and%20Da%20Yin%20and%20Nanyun%20Peng%0AAbstract%3A%20%20%20How%20can%20%22weak%20teacher%20models%22%20such%20as%20average%20human%20annotators%20or%20existing%20AI%0Asystems%2C%20effectively%20supervise%20LLMs%20to%20improve%20performance%20on%20hard%20reasoning%0Atasks%2C%20especially%20those%20that%20challenge%20and%20requires%20expertise%20or%20daily%20practice%0Afrom%20the%20teacher%20models%3F%20In%20this%20paper%2C%20we%20seek%20for%20empirical%20answers%20to%20this%0Aquestion%20by%20investigating%20various%20data-driven%20strategies%20that%20offer%20supervision%0Adata%20at%20different%20quality%20levels%20upon%20tasks%20of%20varying%20complexity.%20Two%0Aintuitive%20strategies%20emerge%20for%20teacher%20models%20to%20provide%20supervision%20during%0Aalignment%20training%3A%201%29%20using%20lower-quality%20supervision%20from%20complete%20tasks%20that%0Amatch%20the%20difficulty%20of%20the%20target%20reasoning%20tasks%2C%20and%202%29%20leveraging%0Ahigher-quality%20supervision%20from%20easier%20subtasks%20that%20are%20less%20challenging.%0AInterestingly%2C%20we%20find%20that%20even%20when%20the%20outcome%20error%20rate%20for%20hard%20task%0Asupervision%20is%20high%20%28e.g.%2C%2090%5C%25%29%2C%20training%20on%20such%20data%20can%20outperform%0Aperfectly%20correct%20supervision%20of%20easier%20subtasks%20on%20multiple%20hard%20math%0Abenchmarks.%20We%20further%20identify%20a%20more%20critical%20factor%20influencing%20training%0Aperformance%3A%20step-wise%20error%20rates%2C%20which%20indicate%20the%20severity%20of%20errors%20in%0Asolutions.%20Specifically%2C%20training%20on%20hard%20task%20supervision%20with%20the%20same%0Aoutcome%20error%20rates%20but%20disparate%20step-wise%20error%20rates%20can%20lead%20to%20a%2030%5C%25%0Aaccuracy%20gap%20on%20MATH%20benchmark.%20Our%20results%20also%20reveal%20that%20supplementing%20hard%0Atask%20supervision%20with%20the%20corresponding%20subtask%20supervision%20can%20yield%20notable%0Aperformance%20improvements%20than%20simply%20combining%20rephrased%20hard%20full%20task%0Asupervision%2C%20suggesting%20new%20avenues%20for%20data%20augmentation.%20Data%20and%20code%20are%0Areleased%20at%20https%3A//github.com/hexuan21/Weak-to-Strong.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20533v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuiding%2520Through%2520Complexity%253A%2520What%2520Makes%2520Good%2520Supervision%2520for%2520Hard%2520Math%250A%2520%2520Reasoning%2520Tasks%253F%26entry.906535625%3DXuan%2520He%2520and%2520Da%2520Yin%2520and%2520Nanyun%2520Peng%26entry.1292438233%3D%2520%2520How%2520can%2520%2522weak%2520teacher%2520models%2522%2520such%2520as%2520average%2520human%2520annotators%2520or%2520existing%2520AI%250Asystems%252C%2520effectively%2520supervise%2520LLMs%2520to%2520improve%2520performance%2520on%2520hard%2520reasoning%250Atasks%252C%2520especially%2520those%2520that%2520challenge%2520and%2520requires%2520expertise%2520or%2520daily%2520practice%250Afrom%2520the%2520teacher%2520models%253F%2520In%2520this%2520paper%252C%2520we%2520seek%2520for%2520empirical%2520answers%2520to%2520this%250Aquestion%2520by%2520investigating%2520various%2520data-driven%2520strategies%2520that%2520offer%2520supervision%250Adata%2520at%2520different%2520quality%2520levels%2520upon%2520tasks%2520of%2520varying%2520complexity.%2520Two%250Aintuitive%2520strategies%2520emerge%2520for%2520teacher%2520models%2520to%2520provide%2520supervision%2520during%250Aalignment%2520training%253A%25201%2529%2520using%2520lower-quality%2520supervision%2520from%2520complete%2520tasks%2520that%250Amatch%2520the%2520difficulty%2520of%2520the%2520target%2520reasoning%2520tasks%252C%2520and%25202%2529%2520leveraging%250Ahigher-quality%2520supervision%2520from%2520easier%2520subtasks%2520that%2520are%2520less%2520challenging.%250AInterestingly%252C%2520we%2520find%2520that%2520even%2520when%2520the%2520outcome%2520error%2520rate%2520for%2520hard%2520task%250Asupervision%2520is%2520high%2520%2528e.g.%252C%252090%255C%2525%2529%252C%2520training%2520on%2520such%2520data%2520can%2520outperform%250Aperfectly%2520correct%2520supervision%2520of%2520easier%2520subtasks%2520on%2520multiple%2520hard%2520math%250Abenchmarks.%2520We%2520further%2520identify%2520a%2520more%2520critical%2520factor%2520influencing%2520training%250Aperformance%253A%2520step-wise%2520error%2520rates%252C%2520which%2520indicate%2520the%2520severity%2520of%2520errors%2520in%250Asolutions.%2520Specifically%252C%2520training%2520on%2520hard%2520task%2520supervision%2520with%2520the%2520same%250Aoutcome%2520error%2520rates%2520but%2520disparate%2520step-wise%2520error%2520rates%2520can%2520lead%2520to%2520a%252030%255C%2525%250Aaccuracy%2520gap%2520on%2520MATH%2520benchmark.%2520Our%2520results%2520also%2520reveal%2520that%2520supplementing%2520hard%250Atask%2520supervision%2520with%2520the%2520corresponding%2520subtask%2520supervision%2520can%2520yield%2520notable%250Aperformance%2520improvements%2520than%2520simply%2520combining%2520rephrased%2520hard%2520full%2520task%250Asupervision%252C%2520suggesting%2520new%2520avenues%2520for%2520data%2520augmentation.%2520Data%2520and%2520code%2520are%250Areleased%2520at%2520https%253A//github.com/hexuan21/Weak-to-Strong.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20533v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guiding%20Through%20Complexity%3A%20What%20Makes%20Good%20Supervision%20for%20Hard%20Math%0A%20%20Reasoning%20Tasks%3F&entry.906535625=Xuan%20He%20and%20Da%20Yin%20and%20Nanyun%20Peng&entry.1292438233=%20%20How%20can%20%22weak%20teacher%20models%22%20such%20as%20average%20human%20annotators%20or%20existing%20AI%0Asystems%2C%20effectively%20supervise%20LLMs%20to%20improve%20performance%20on%20hard%20reasoning%0Atasks%2C%20especially%20those%20that%20challenge%20and%20requires%20expertise%20or%20daily%20practice%0Afrom%20the%20teacher%20models%3F%20In%20this%20paper%2C%20we%20seek%20for%20empirical%20answers%20to%20this%0Aquestion%20by%20investigating%20various%20data-driven%20strategies%20that%20offer%20supervision%0Adata%20at%20different%20quality%20levels%20upon%20tasks%20of%20varying%20complexity.%20Two%0Aintuitive%20strategies%20emerge%20for%20teacher%20models%20to%20provide%20supervision%20during%0Aalignment%20training%3A%201%29%20using%20lower-quality%20supervision%20from%20complete%20tasks%20that%0Amatch%20the%20difficulty%20of%20the%20target%20reasoning%20tasks%2C%20and%202%29%20leveraging%0Ahigher-quality%20supervision%20from%20easier%20subtasks%20that%20are%20less%20challenging.%0AInterestingly%2C%20we%20find%20that%20even%20when%20the%20outcome%20error%20rate%20for%20hard%20task%0Asupervision%20is%20high%20%28e.g.%2C%2090%5C%25%29%2C%20training%20on%20such%20data%20can%20outperform%0Aperfectly%20correct%20supervision%20of%20easier%20subtasks%20on%20multiple%20hard%20math%0Abenchmarks.%20We%20further%20identify%20a%20more%20critical%20factor%20influencing%20training%0Aperformance%3A%20step-wise%20error%20rates%2C%20which%20indicate%20the%20severity%20of%20errors%20in%0Asolutions.%20Specifically%2C%20training%20on%20hard%20task%20supervision%20with%20the%20same%0Aoutcome%20error%20rates%20but%20disparate%20step-wise%20error%20rates%20can%20lead%20to%20a%2030%5C%25%0Aaccuracy%20gap%20on%20MATH%20benchmark.%20Our%20results%20also%20reveal%20that%20supplementing%20hard%0Atask%20supervision%20with%20the%20corresponding%20subtask%20supervision%20can%20yield%20notable%0Aperformance%20improvements%20than%20simply%20combining%20rephrased%20hard%20full%20task%0Asupervision%2C%20suggesting%20new%20avenues%20for%20data%20augmentation.%20Data%20and%20code%20are%0Areleased%20at%20https%3A//github.com/hexuan21/Weak-to-Strong.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20533v3&entry.124074799=Read"},
{"title": "DeepCircuitX: A Comprehensive Repository-Level Dataset for RTL Code\n  Understanding, Generation, and PPA Analysis", "author": "Zeju Li and Changran Xu and Zhengyuan Shi and Zedong Peng and Yi Liu and Yunhao Zhou and Lingfeng Zhou and Chengyu Ma and Jianyuan Zhong and Xi Wang and Jieru Zhao and Zhufei Chu and Xiaoyan Yang and Qiang Xu", "abstract": "  This paper introduces DeepCircuitX, a comprehensive repository-level dataset\ndesigned to advance RTL (Register Transfer Level) code understanding,\ngeneration, and power-performance-area (PPA) analysis. Unlike existing datasets\nthat are limited to either file-level RTL code or physical layout data,\nDeepCircuitX provides a holistic, multilevel resource that spans repository,\nfile, module, and block-level RTL code. This structure enables more nuanced\ntraining and evaluation of large language models (LLMs) for RTL-specific tasks.\nDeepCircuitX is enriched with Chain of Thought (CoT) annotations, offering\ndetailed descriptions of functionality and structure at multiple levels. These\nannotations enhance its utility for a wide range of tasks, including RTL code\nunderstanding, generation, and completion. Additionally, the dataset includes\nsynthesized netlists and PPA metrics, facilitating early-stage design\nexploration and enabling accurate PPA prediction directly from RTL code. We\ndemonstrate the dataset's effectiveness on various LLMs finetuned with our\ndataset and confirm the quality with human evaluations. Our results highlight\nDeepCircuitX as a critical resource for advancing RTL-focused machine learning\napplications in hardware design automation.Our data is available at\nhttps://zeju.gitbook.io/lcm-team.\n", "link": "http://arxiv.org/abs/2502.18297v1", "date": "2025-02-25", "relevancy": 2.4105, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4839}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4812}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepCircuitX%3A%20A%20Comprehensive%20Repository-Level%20Dataset%20for%20RTL%20Code%0A%20%20Understanding%2C%20Generation%2C%20and%20PPA%20Analysis&body=Title%3A%20DeepCircuitX%3A%20A%20Comprehensive%20Repository-Level%20Dataset%20for%20RTL%20Code%0A%20%20Understanding%2C%20Generation%2C%20and%20PPA%20Analysis%0AAuthor%3A%20Zeju%20Li%20and%20Changran%20Xu%20and%20Zhengyuan%20Shi%20and%20Zedong%20Peng%20and%20Yi%20Liu%20and%20Yunhao%20Zhou%20and%20Lingfeng%20Zhou%20and%20Chengyu%20Ma%20and%20Jianyuan%20Zhong%20and%20Xi%20Wang%20and%20Jieru%20Zhao%20and%20Zhufei%20Chu%20and%20Xiaoyan%20Yang%20and%20Qiang%20Xu%0AAbstract%3A%20%20%20This%20paper%20introduces%20DeepCircuitX%2C%20a%20comprehensive%20repository-level%20dataset%0Adesigned%20to%20advance%20RTL%20%28Register%20Transfer%20Level%29%20code%20understanding%2C%0Ageneration%2C%20and%20power-performance-area%20%28PPA%29%20analysis.%20Unlike%20existing%20datasets%0Athat%20are%20limited%20to%20either%20file-level%20RTL%20code%20or%20physical%20layout%20data%2C%0ADeepCircuitX%20provides%20a%20holistic%2C%20multilevel%20resource%20that%20spans%20repository%2C%0Afile%2C%20module%2C%20and%20block-level%20RTL%20code.%20This%20structure%20enables%20more%20nuanced%0Atraining%20and%20evaluation%20of%20large%20language%20models%20%28LLMs%29%20for%20RTL-specific%20tasks.%0ADeepCircuitX%20is%20enriched%20with%20Chain%20of%20Thought%20%28CoT%29%20annotations%2C%20offering%0Adetailed%20descriptions%20of%20functionality%20and%20structure%20at%20multiple%20levels.%20These%0Aannotations%20enhance%20its%20utility%20for%20a%20wide%20range%20of%20tasks%2C%20including%20RTL%20code%0Aunderstanding%2C%20generation%2C%20and%20completion.%20Additionally%2C%20the%20dataset%20includes%0Asynthesized%20netlists%20and%20PPA%20metrics%2C%20facilitating%20early-stage%20design%0Aexploration%20and%20enabling%20accurate%20PPA%20prediction%20directly%20from%20RTL%20code.%20We%0Ademonstrate%20the%20dataset%27s%20effectiveness%20on%20various%20LLMs%20finetuned%20with%20our%0Adataset%20and%20confirm%20the%20quality%20with%20human%20evaluations.%20Our%20results%20highlight%0ADeepCircuitX%20as%20a%20critical%20resource%20for%20advancing%20RTL-focused%20machine%20learning%0Aapplications%20in%20hardware%20design%20automation.Our%20data%20is%20available%20at%0Ahttps%3A//zeju.gitbook.io/lcm-team.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepCircuitX%253A%2520A%2520Comprehensive%2520Repository-Level%2520Dataset%2520for%2520RTL%2520Code%250A%2520%2520Understanding%252C%2520Generation%252C%2520and%2520PPA%2520Analysis%26entry.906535625%3DZeju%2520Li%2520and%2520Changran%2520Xu%2520and%2520Zhengyuan%2520Shi%2520and%2520Zedong%2520Peng%2520and%2520Yi%2520Liu%2520and%2520Yunhao%2520Zhou%2520and%2520Lingfeng%2520Zhou%2520and%2520Chengyu%2520Ma%2520and%2520Jianyuan%2520Zhong%2520and%2520Xi%2520Wang%2520and%2520Jieru%2520Zhao%2520and%2520Zhufei%2520Chu%2520and%2520Xiaoyan%2520Yang%2520and%2520Qiang%2520Xu%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520DeepCircuitX%252C%2520a%2520comprehensive%2520repository-level%2520dataset%250Adesigned%2520to%2520advance%2520RTL%2520%2528Register%2520Transfer%2520Level%2529%2520code%2520understanding%252C%250Ageneration%252C%2520and%2520power-performance-area%2520%2528PPA%2529%2520analysis.%2520Unlike%2520existing%2520datasets%250Athat%2520are%2520limited%2520to%2520either%2520file-level%2520RTL%2520code%2520or%2520physical%2520layout%2520data%252C%250ADeepCircuitX%2520provides%2520a%2520holistic%252C%2520multilevel%2520resource%2520that%2520spans%2520repository%252C%250Afile%252C%2520module%252C%2520and%2520block-level%2520RTL%2520code.%2520This%2520structure%2520enables%2520more%2520nuanced%250Atraining%2520and%2520evaluation%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520RTL-specific%2520tasks.%250ADeepCircuitX%2520is%2520enriched%2520with%2520Chain%2520of%2520Thought%2520%2528CoT%2529%2520annotations%252C%2520offering%250Adetailed%2520descriptions%2520of%2520functionality%2520and%2520structure%2520at%2520multiple%2520levels.%2520These%250Aannotations%2520enhance%2520its%2520utility%2520for%2520a%2520wide%2520range%2520of%2520tasks%252C%2520including%2520RTL%2520code%250Aunderstanding%252C%2520generation%252C%2520and%2520completion.%2520Additionally%252C%2520the%2520dataset%2520includes%250Asynthesized%2520netlists%2520and%2520PPA%2520metrics%252C%2520facilitating%2520early-stage%2520design%250Aexploration%2520and%2520enabling%2520accurate%2520PPA%2520prediction%2520directly%2520from%2520RTL%2520code.%2520We%250Ademonstrate%2520the%2520dataset%2527s%2520effectiveness%2520on%2520various%2520LLMs%2520finetuned%2520with%2520our%250Adataset%2520and%2520confirm%2520the%2520quality%2520with%2520human%2520evaluations.%2520Our%2520results%2520highlight%250ADeepCircuitX%2520as%2520a%2520critical%2520resource%2520for%2520advancing%2520RTL-focused%2520machine%2520learning%250Aapplications%2520in%2520hardware%2520design%2520automation.Our%2520data%2520is%2520available%2520at%250Ahttps%253A//zeju.gitbook.io/lcm-team.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepCircuitX%3A%20A%20Comprehensive%20Repository-Level%20Dataset%20for%20RTL%20Code%0A%20%20Understanding%2C%20Generation%2C%20and%20PPA%20Analysis&entry.906535625=Zeju%20Li%20and%20Changran%20Xu%20and%20Zhengyuan%20Shi%20and%20Zedong%20Peng%20and%20Yi%20Liu%20and%20Yunhao%20Zhou%20and%20Lingfeng%20Zhou%20and%20Chengyu%20Ma%20and%20Jianyuan%20Zhong%20and%20Xi%20Wang%20and%20Jieru%20Zhao%20and%20Zhufei%20Chu%20and%20Xiaoyan%20Yang%20and%20Qiang%20Xu&entry.1292438233=%20%20This%20paper%20introduces%20DeepCircuitX%2C%20a%20comprehensive%20repository-level%20dataset%0Adesigned%20to%20advance%20RTL%20%28Register%20Transfer%20Level%29%20code%20understanding%2C%0Ageneration%2C%20and%20power-performance-area%20%28PPA%29%20analysis.%20Unlike%20existing%20datasets%0Athat%20are%20limited%20to%20either%20file-level%20RTL%20code%20or%20physical%20layout%20data%2C%0ADeepCircuitX%20provides%20a%20holistic%2C%20multilevel%20resource%20that%20spans%20repository%2C%0Afile%2C%20module%2C%20and%20block-level%20RTL%20code.%20This%20structure%20enables%20more%20nuanced%0Atraining%20and%20evaluation%20of%20large%20language%20models%20%28LLMs%29%20for%20RTL-specific%20tasks.%0ADeepCircuitX%20is%20enriched%20with%20Chain%20of%20Thought%20%28CoT%29%20annotations%2C%20offering%0Adetailed%20descriptions%20of%20functionality%20and%20structure%20at%20multiple%20levels.%20These%0Aannotations%20enhance%20its%20utility%20for%20a%20wide%20range%20of%20tasks%2C%20including%20RTL%20code%0Aunderstanding%2C%20generation%2C%20and%20completion.%20Additionally%2C%20the%20dataset%20includes%0Asynthesized%20netlists%20and%20PPA%20metrics%2C%20facilitating%20early-stage%20design%0Aexploration%20and%20enabling%20accurate%20PPA%20prediction%20directly%20from%20RTL%20code.%20We%0Ademonstrate%20the%20dataset%27s%20effectiveness%20on%20various%20LLMs%20finetuned%20with%20our%0Adataset%20and%20confirm%20the%20quality%20with%20human%20evaluations.%20Our%20results%20highlight%0ADeepCircuitX%20as%20a%20critical%20resource%20for%20advancing%20RTL-focused%20machine%20learning%0Aapplications%20in%20hardware%20design%20automation.Our%20data%20is%20available%20at%0Ahttps%3A//zeju.gitbook.io/lcm-team.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18297v1&entry.124074799=Read"},
{"title": "Graph Augmentation for Cross Graph Domain Generalization", "author": "Guanzi Chen and Jiying Zhang and Yang Li", "abstract": "  Cross-graph node classification, utilizing the abundant labeled nodes from\none graph to help classify unlabeled nodes in another graph, can be viewed as a\ndomain generalization problem of graph neural networks (GNNs) due to the\nstructure shift commonly appearing among various graphs. Nevertheless, current\nendeavors for cross-graph node classification mainly focus on model training.\nData augmentation approaches, a simple and easy-to-implement domain\ngeneralization technique, remain under-explored. In this paper, we develop a\nnew graph structure augmentation for the crossgraph domain generalization\nproblem. Specifically, low-weight edgedropping is applied to remove potential\nnoise edges that may hinder the generalization ability of GNNs, stimulating the\nGNNs to capture the essential invariant information underlying different\nstructures. Meanwhile, clustering-based edge-adding is proposed to generate\ninvariant structures based on the node features from the same distribution.\nConsequently, with these augmentation techniques, the GNNs can maintain the\ndomain invariant structure information that can improve the generalization\nability. The experiments on out-ofdistribution citation network datasets verify\nour method achieves state-of-the-art performance among conventional\naugmentations.\n", "link": "http://arxiv.org/abs/2502.18188v1", "date": "2025-02-25", "relevancy": 2.3869, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4838}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4811}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Augmentation%20for%20Cross%20Graph%20Domain%20Generalization&body=Title%3A%20Graph%20Augmentation%20for%20Cross%20Graph%20Domain%20Generalization%0AAuthor%3A%20Guanzi%20Chen%20and%20Jiying%20Zhang%20and%20Yang%20Li%0AAbstract%3A%20%20%20Cross-graph%20node%20classification%2C%20utilizing%20the%20abundant%20labeled%20nodes%20from%0Aone%20graph%20to%20help%20classify%20unlabeled%20nodes%20in%20another%20graph%2C%20can%20be%20viewed%20as%20a%0Adomain%20generalization%20problem%20of%20graph%20neural%20networks%20%28GNNs%29%20due%20to%20the%0Astructure%20shift%20commonly%20appearing%20among%20various%20graphs.%20Nevertheless%2C%20current%0Aendeavors%20for%20cross-graph%20node%20classification%20mainly%20focus%20on%20model%20training.%0AData%20augmentation%20approaches%2C%20a%20simple%20and%20easy-to-implement%20domain%0Ageneralization%20technique%2C%20remain%20under-explored.%20In%20this%20paper%2C%20we%20develop%20a%0Anew%20graph%20structure%20augmentation%20for%20the%20crossgraph%20domain%20generalization%0Aproblem.%20Specifically%2C%20low-weight%20edgedropping%20is%20applied%20to%20remove%20potential%0Anoise%20edges%20that%20may%20hinder%20the%20generalization%20ability%20of%20GNNs%2C%20stimulating%20the%0AGNNs%20to%20capture%20the%20essential%20invariant%20information%20underlying%20different%0Astructures.%20Meanwhile%2C%20clustering-based%20edge-adding%20is%20proposed%20to%20generate%0Ainvariant%20structures%20based%20on%20the%20node%20features%20from%20the%20same%20distribution.%0AConsequently%2C%20with%20these%20augmentation%20techniques%2C%20the%20GNNs%20can%20maintain%20the%0Adomain%20invariant%20structure%20information%20that%20can%20improve%20the%20generalization%0Aability.%20The%20experiments%20on%20out-ofdistribution%20citation%20network%20datasets%20verify%0Aour%20method%20achieves%20state-of-the-art%20performance%20among%20conventional%0Aaugmentations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Augmentation%2520for%2520Cross%2520Graph%2520Domain%2520Generalization%26entry.906535625%3DGuanzi%2520Chen%2520and%2520Jiying%2520Zhang%2520and%2520Yang%2520Li%26entry.1292438233%3D%2520%2520Cross-graph%2520node%2520classification%252C%2520utilizing%2520the%2520abundant%2520labeled%2520nodes%2520from%250Aone%2520graph%2520to%2520help%2520classify%2520unlabeled%2520nodes%2520in%2520another%2520graph%252C%2520can%2520be%2520viewed%2520as%2520a%250Adomain%2520generalization%2520problem%2520of%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520due%2520to%2520the%250Astructure%2520shift%2520commonly%2520appearing%2520among%2520various%2520graphs.%2520Nevertheless%252C%2520current%250Aendeavors%2520for%2520cross-graph%2520node%2520classification%2520mainly%2520focus%2520on%2520model%2520training.%250AData%2520augmentation%2520approaches%252C%2520a%2520simple%2520and%2520easy-to-implement%2520domain%250Ageneralization%2520technique%252C%2520remain%2520under-explored.%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%250Anew%2520graph%2520structure%2520augmentation%2520for%2520the%2520crossgraph%2520domain%2520generalization%250Aproblem.%2520Specifically%252C%2520low-weight%2520edgedropping%2520is%2520applied%2520to%2520remove%2520potential%250Anoise%2520edges%2520that%2520may%2520hinder%2520the%2520generalization%2520ability%2520of%2520GNNs%252C%2520stimulating%2520the%250AGNNs%2520to%2520capture%2520the%2520essential%2520invariant%2520information%2520underlying%2520different%250Astructures.%2520Meanwhile%252C%2520clustering-based%2520edge-adding%2520is%2520proposed%2520to%2520generate%250Ainvariant%2520structures%2520based%2520on%2520the%2520node%2520features%2520from%2520the%2520same%2520distribution.%250AConsequently%252C%2520with%2520these%2520augmentation%2520techniques%252C%2520the%2520GNNs%2520can%2520maintain%2520the%250Adomain%2520invariant%2520structure%2520information%2520that%2520can%2520improve%2520the%2520generalization%250Aability.%2520The%2520experiments%2520on%2520out-ofdistribution%2520citation%2520network%2520datasets%2520verify%250Aour%2520method%2520achieves%2520state-of-the-art%2520performance%2520among%2520conventional%250Aaugmentations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Augmentation%20for%20Cross%20Graph%20Domain%20Generalization&entry.906535625=Guanzi%20Chen%20and%20Jiying%20Zhang%20and%20Yang%20Li&entry.1292438233=%20%20Cross-graph%20node%20classification%2C%20utilizing%20the%20abundant%20labeled%20nodes%20from%0Aone%20graph%20to%20help%20classify%20unlabeled%20nodes%20in%20another%20graph%2C%20can%20be%20viewed%20as%20a%0Adomain%20generalization%20problem%20of%20graph%20neural%20networks%20%28GNNs%29%20due%20to%20the%0Astructure%20shift%20commonly%20appearing%20among%20various%20graphs.%20Nevertheless%2C%20current%0Aendeavors%20for%20cross-graph%20node%20classification%20mainly%20focus%20on%20model%20training.%0AData%20augmentation%20approaches%2C%20a%20simple%20and%20easy-to-implement%20domain%0Ageneralization%20technique%2C%20remain%20under-explored.%20In%20this%20paper%2C%20we%20develop%20a%0Anew%20graph%20structure%20augmentation%20for%20the%20crossgraph%20domain%20generalization%0Aproblem.%20Specifically%2C%20low-weight%20edgedropping%20is%20applied%20to%20remove%20potential%0Anoise%20edges%20that%20may%20hinder%20the%20generalization%20ability%20of%20GNNs%2C%20stimulating%20the%0AGNNs%20to%20capture%20the%20essential%20invariant%20information%20underlying%20different%0Astructures.%20Meanwhile%2C%20clustering-based%20edge-adding%20is%20proposed%20to%20generate%0Ainvariant%20structures%20based%20on%20the%20node%20features%20from%20the%20same%20distribution.%0AConsequently%2C%20with%20these%20augmentation%20techniques%2C%20the%20GNNs%20can%20maintain%20the%0Adomain%20invariant%20structure%20information%20that%20can%20improve%20the%20generalization%0Aability.%20The%20experiments%20on%20out-ofdistribution%20citation%20network%20datasets%20verify%0Aour%20method%20achieves%20state-of-the-art%20performance%20among%20conventional%0Aaugmentations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18188v1&entry.124074799=Read"},
{"title": "CRESSim-MPM: A Material Point Method Library for Surgical Soft Body\n  Simulation with Cutting and Suturing", "author": "Yafei Ou and Mahdi Tavakoli", "abstract": "  A number of recent studies have focused on developing surgical simulation\nplatforms to train machine learning (ML) agents or models with synthetic data\nfor surgical assistance. While existing platforms excel at tasks such as rigid\nbody manipulation and soft body deformation, they struggle to simulate more\ncomplex soft body behaviors like cutting and suturing. A key challenge lies in\nmodeling soft body fracture and splitting using the finite-element method\n(FEM), which is the predominant approach in current platforms. Additionally,\nthe two-way suture needle/thread contact inside a soft body is further\ncomplicated when using FEM. In this work, we use the material point method\n(MPM) for such challenging simulations and propose new rigid geometries and\nsoft-rigid contact methods specifically designed for them. We introduce\nCRESSim-MPM, a GPU-accelerated MPM library that integrates multiple MPM solvers\nand incorporates surgical geometries for cutting and suturing, serving as a\nspecialized physics engine for surgical applications. It is further integrated\ninto Unity, requiring minimal modifications to existing projects for soft body\nsimulation. We demonstrate the simulator's capabilities in real-time simulation\nof cutting and suturing on soft tissue and provide an initial performance\nevaluation of different MPM solvers when simulating varying numbers of\nparticles.\n", "link": "http://arxiv.org/abs/2502.18437v1", "date": "2025-02-25", "relevancy": 2.3805, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4806}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4783}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CRESSim-MPM%3A%20A%20Material%20Point%20Method%20Library%20for%20Surgical%20Soft%20Body%0A%20%20Simulation%20with%20Cutting%20and%20Suturing&body=Title%3A%20CRESSim-MPM%3A%20A%20Material%20Point%20Method%20Library%20for%20Surgical%20Soft%20Body%0A%20%20Simulation%20with%20Cutting%20and%20Suturing%0AAuthor%3A%20Yafei%20Ou%20and%20Mahdi%20Tavakoli%0AAbstract%3A%20%20%20A%20number%20of%20recent%20studies%20have%20focused%20on%20developing%20surgical%20simulation%0Aplatforms%20to%20train%20machine%20learning%20%28ML%29%20agents%20or%20models%20with%20synthetic%20data%0Afor%20surgical%20assistance.%20While%20existing%20platforms%20excel%20at%20tasks%20such%20as%20rigid%0Abody%20manipulation%20and%20soft%20body%20deformation%2C%20they%20struggle%20to%20simulate%20more%0Acomplex%20soft%20body%20behaviors%20like%20cutting%20and%20suturing.%20A%20key%20challenge%20lies%20in%0Amodeling%20soft%20body%20fracture%20and%20splitting%20using%20the%20finite-element%20method%0A%28FEM%29%2C%20which%20is%20the%20predominant%20approach%20in%20current%20platforms.%20Additionally%2C%0Athe%20two-way%20suture%20needle/thread%20contact%20inside%20a%20soft%20body%20is%20further%0Acomplicated%20when%20using%20FEM.%20In%20this%20work%2C%20we%20use%20the%20material%20point%20method%0A%28MPM%29%20for%20such%20challenging%20simulations%20and%20propose%20new%20rigid%20geometries%20and%0Asoft-rigid%20contact%20methods%20specifically%20designed%20for%20them.%20We%20introduce%0ACRESSim-MPM%2C%20a%20GPU-accelerated%20MPM%20library%20that%20integrates%20multiple%20MPM%20solvers%0Aand%20incorporates%20surgical%20geometries%20for%20cutting%20and%20suturing%2C%20serving%20as%20a%0Aspecialized%20physics%20engine%20for%20surgical%20applications.%20It%20is%20further%20integrated%0Ainto%20Unity%2C%20requiring%20minimal%20modifications%20to%20existing%20projects%20for%20soft%20body%0Asimulation.%20We%20demonstrate%20the%20simulator%27s%20capabilities%20in%20real-time%20simulation%0Aof%20cutting%20and%20suturing%20on%20soft%20tissue%20and%20provide%20an%20initial%20performance%0Aevaluation%20of%20different%20MPM%20solvers%20when%20simulating%20varying%20numbers%20of%0Aparticles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCRESSim-MPM%253A%2520A%2520Material%2520Point%2520Method%2520Library%2520for%2520Surgical%2520Soft%2520Body%250A%2520%2520Simulation%2520with%2520Cutting%2520and%2520Suturing%26entry.906535625%3DYafei%2520Ou%2520and%2520Mahdi%2520Tavakoli%26entry.1292438233%3D%2520%2520A%2520number%2520of%2520recent%2520studies%2520have%2520focused%2520on%2520developing%2520surgical%2520simulation%250Aplatforms%2520to%2520train%2520machine%2520learning%2520%2528ML%2529%2520agents%2520or%2520models%2520with%2520synthetic%2520data%250Afor%2520surgical%2520assistance.%2520While%2520existing%2520platforms%2520excel%2520at%2520tasks%2520such%2520as%2520rigid%250Abody%2520manipulation%2520and%2520soft%2520body%2520deformation%252C%2520they%2520struggle%2520to%2520simulate%2520more%250Acomplex%2520soft%2520body%2520behaviors%2520like%2520cutting%2520and%2520suturing.%2520A%2520key%2520challenge%2520lies%2520in%250Amodeling%2520soft%2520body%2520fracture%2520and%2520splitting%2520using%2520the%2520finite-element%2520method%250A%2528FEM%2529%252C%2520which%2520is%2520the%2520predominant%2520approach%2520in%2520current%2520platforms.%2520Additionally%252C%250Athe%2520two-way%2520suture%2520needle/thread%2520contact%2520inside%2520a%2520soft%2520body%2520is%2520further%250Acomplicated%2520when%2520using%2520FEM.%2520In%2520this%2520work%252C%2520we%2520use%2520the%2520material%2520point%2520method%250A%2528MPM%2529%2520for%2520such%2520challenging%2520simulations%2520and%2520propose%2520new%2520rigid%2520geometries%2520and%250Asoft-rigid%2520contact%2520methods%2520specifically%2520designed%2520for%2520them.%2520We%2520introduce%250ACRESSim-MPM%252C%2520a%2520GPU-accelerated%2520MPM%2520library%2520that%2520integrates%2520multiple%2520MPM%2520solvers%250Aand%2520incorporates%2520surgical%2520geometries%2520for%2520cutting%2520and%2520suturing%252C%2520serving%2520as%2520a%250Aspecialized%2520physics%2520engine%2520for%2520surgical%2520applications.%2520It%2520is%2520further%2520integrated%250Ainto%2520Unity%252C%2520requiring%2520minimal%2520modifications%2520to%2520existing%2520projects%2520for%2520soft%2520body%250Asimulation.%2520We%2520demonstrate%2520the%2520simulator%2527s%2520capabilities%2520in%2520real-time%2520simulation%250Aof%2520cutting%2520and%2520suturing%2520on%2520soft%2520tissue%2520and%2520provide%2520an%2520initial%2520performance%250Aevaluation%2520of%2520different%2520MPM%2520solvers%2520when%2520simulating%2520varying%2520numbers%2520of%250Aparticles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CRESSim-MPM%3A%20A%20Material%20Point%20Method%20Library%20for%20Surgical%20Soft%20Body%0A%20%20Simulation%20with%20Cutting%20and%20Suturing&entry.906535625=Yafei%20Ou%20and%20Mahdi%20Tavakoli&entry.1292438233=%20%20A%20number%20of%20recent%20studies%20have%20focused%20on%20developing%20surgical%20simulation%0Aplatforms%20to%20train%20machine%20learning%20%28ML%29%20agents%20or%20models%20with%20synthetic%20data%0Afor%20surgical%20assistance.%20While%20existing%20platforms%20excel%20at%20tasks%20such%20as%20rigid%0Abody%20manipulation%20and%20soft%20body%20deformation%2C%20they%20struggle%20to%20simulate%20more%0Acomplex%20soft%20body%20behaviors%20like%20cutting%20and%20suturing.%20A%20key%20challenge%20lies%20in%0Amodeling%20soft%20body%20fracture%20and%20splitting%20using%20the%20finite-element%20method%0A%28FEM%29%2C%20which%20is%20the%20predominant%20approach%20in%20current%20platforms.%20Additionally%2C%0Athe%20two-way%20suture%20needle/thread%20contact%20inside%20a%20soft%20body%20is%20further%0Acomplicated%20when%20using%20FEM.%20In%20this%20work%2C%20we%20use%20the%20material%20point%20method%0A%28MPM%29%20for%20such%20challenging%20simulations%20and%20propose%20new%20rigid%20geometries%20and%0Asoft-rigid%20contact%20methods%20specifically%20designed%20for%20them.%20We%20introduce%0ACRESSim-MPM%2C%20a%20GPU-accelerated%20MPM%20library%20that%20integrates%20multiple%20MPM%20solvers%0Aand%20incorporates%20surgical%20geometries%20for%20cutting%20and%20suturing%2C%20serving%20as%20a%0Aspecialized%20physics%20engine%20for%20surgical%20applications.%20It%20is%20further%20integrated%0Ainto%20Unity%2C%20requiring%20minimal%20modifications%20to%20existing%20projects%20for%20soft%20body%0Asimulation.%20We%20demonstrate%20the%20simulator%27s%20capabilities%20in%20real-time%20simulation%0Aof%20cutting%20and%20suturing%20on%20soft%20tissue%20and%20provide%20an%20initial%20performance%0Aevaluation%20of%20different%20MPM%20solvers%20when%20simulating%20varying%20numbers%20of%0Aparticles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18437v1&entry.124074799=Read"},
{"title": "SGFormer: Spherical Geometry Transformer for 360 Depth Estimation", "author": "Junsong Zhang and Zisong Chen and Chunyu Lin and Lang Nie and Zhijie Shen and Kang Liao and Junda Huang and Yao Zhao", "abstract": "  Panoramic distortion poses a significant challenge in 360 depth estimation,\nparticularly pronounced at the north and south poles. Existing methods either\nadopt a bi-projection fusion strategy to remove distortions or model long-range\ndependencies to capture global structures, which can result in either unclear\nstructure or insufficient local perception. In this paper, we propose a\nspherical geometry transformer, named SGFormer, to address the above issues,\nwith an innovative step to integrate spherical geometric priors into vision\ntransformers. To this end, we retarget the transformer decoder to a spherical\nprior decoder (termed SPDecoder), which endeavors to uphold the integrity of\nspherical structures during decoding. Concretely, we leverage bipolar\nre-projection, circular rotation, and curve local embedding to preserve the\nspherical characteristics of equidistortion, continuity, and surface distance,\nrespectively. Furthermore, we present a query-based global conditional position\nembedding to compensate for spatial structure at varying resolutions. It not\nonly boosts the global perception of spatial position but also sharpens the\ndepth structure across different patches. Finally, we conduct extensive\nexperiments on popular benchmarks, demonstrating our superiority over\nstate-of-the-art solutions.\n", "link": "http://arxiv.org/abs/2404.14979v3", "date": "2025-02-25", "relevancy": 2.3796, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6162}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5833}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SGFormer%3A%20Spherical%20Geometry%20Transformer%20for%20360%20Depth%20Estimation&body=Title%3A%20SGFormer%3A%20Spherical%20Geometry%20Transformer%20for%20360%20Depth%20Estimation%0AAuthor%3A%20Junsong%20Zhang%20and%20Zisong%20Chen%20and%20Chunyu%20Lin%20and%20Lang%20Nie%20and%20Zhijie%20Shen%20and%20Kang%20Liao%20and%20Junda%20Huang%20and%20Yao%20Zhao%0AAbstract%3A%20%20%20Panoramic%20distortion%20poses%20a%20significant%20challenge%20in%20360%20depth%20estimation%2C%0Aparticularly%20pronounced%20at%20the%20north%20and%20south%20poles.%20Existing%20methods%20either%0Aadopt%20a%20bi-projection%20fusion%20strategy%20to%20remove%20distortions%20or%20model%20long-range%0Adependencies%20to%20capture%20global%20structures%2C%20which%20can%20result%20in%20either%20unclear%0Astructure%20or%20insufficient%20local%20perception.%20In%20this%20paper%2C%20we%20propose%20a%0Aspherical%20geometry%20transformer%2C%20named%20SGFormer%2C%20to%20address%20the%20above%20issues%2C%0Awith%20an%20innovative%20step%20to%20integrate%20spherical%20geometric%20priors%20into%20vision%0Atransformers.%20To%20this%20end%2C%20we%20retarget%20the%20transformer%20decoder%20to%20a%20spherical%0Aprior%20decoder%20%28termed%20SPDecoder%29%2C%20which%20endeavors%20to%20uphold%20the%20integrity%20of%0Aspherical%20structures%20during%20decoding.%20Concretely%2C%20we%20leverage%20bipolar%0Are-projection%2C%20circular%20rotation%2C%20and%20curve%20local%20embedding%20to%20preserve%20the%0Aspherical%20characteristics%20of%20equidistortion%2C%20continuity%2C%20and%20surface%20distance%2C%0Arespectively.%20Furthermore%2C%20we%20present%20a%20query-based%20global%20conditional%20position%0Aembedding%20to%20compensate%20for%20spatial%20structure%20at%20varying%20resolutions.%20It%20not%0Aonly%20boosts%20the%20global%20perception%20of%20spatial%20position%20but%20also%20sharpens%20the%0Adepth%20structure%20across%20different%20patches.%20Finally%2C%20we%20conduct%20extensive%0Aexperiments%20on%20popular%20benchmarks%2C%20demonstrating%20our%20superiority%20over%0Astate-of-the-art%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14979v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSGFormer%253A%2520Spherical%2520Geometry%2520Transformer%2520for%2520360%2520Depth%2520Estimation%26entry.906535625%3DJunsong%2520Zhang%2520and%2520Zisong%2520Chen%2520and%2520Chunyu%2520Lin%2520and%2520Lang%2520Nie%2520and%2520Zhijie%2520Shen%2520and%2520Kang%2520Liao%2520and%2520Junda%2520Huang%2520and%2520Yao%2520Zhao%26entry.1292438233%3D%2520%2520Panoramic%2520distortion%2520poses%2520a%2520significant%2520challenge%2520in%2520360%2520depth%2520estimation%252C%250Aparticularly%2520pronounced%2520at%2520the%2520north%2520and%2520south%2520poles.%2520Existing%2520methods%2520either%250Aadopt%2520a%2520bi-projection%2520fusion%2520strategy%2520to%2520remove%2520distortions%2520or%2520model%2520long-range%250Adependencies%2520to%2520capture%2520global%2520structures%252C%2520which%2520can%2520result%2520in%2520either%2520unclear%250Astructure%2520or%2520insufficient%2520local%2520perception.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Aspherical%2520geometry%2520transformer%252C%2520named%2520SGFormer%252C%2520to%2520address%2520the%2520above%2520issues%252C%250Awith%2520an%2520innovative%2520step%2520to%2520integrate%2520spherical%2520geometric%2520priors%2520into%2520vision%250Atransformers.%2520To%2520this%2520end%252C%2520we%2520retarget%2520the%2520transformer%2520decoder%2520to%2520a%2520spherical%250Aprior%2520decoder%2520%2528termed%2520SPDecoder%2529%252C%2520which%2520endeavors%2520to%2520uphold%2520the%2520integrity%2520of%250Aspherical%2520structures%2520during%2520decoding.%2520Concretely%252C%2520we%2520leverage%2520bipolar%250Are-projection%252C%2520circular%2520rotation%252C%2520and%2520curve%2520local%2520embedding%2520to%2520preserve%2520the%250Aspherical%2520characteristics%2520of%2520equidistortion%252C%2520continuity%252C%2520and%2520surface%2520distance%252C%250Arespectively.%2520Furthermore%252C%2520we%2520present%2520a%2520query-based%2520global%2520conditional%2520position%250Aembedding%2520to%2520compensate%2520for%2520spatial%2520structure%2520at%2520varying%2520resolutions.%2520It%2520not%250Aonly%2520boosts%2520the%2520global%2520perception%2520of%2520spatial%2520position%2520but%2520also%2520sharpens%2520the%250Adepth%2520structure%2520across%2520different%2520patches.%2520Finally%252C%2520we%2520conduct%2520extensive%250Aexperiments%2520on%2520popular%2520benchmarks%252C%2520demonstrating%2520our%2520superiority%2520over%250Astate-of-the-art%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14979v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SGFormer%3A%20Spherical%20Geometry%20Transformer%20for%20360%20Depth%20Estimation&entry.906535625=Junsong%20Zhang%20and%20Zisong%20Chen%20and%20Chunyu%20Lin%20and%20Lang%20Nie%20and%20Zhijie%20Shen%20and%20Kang%20Liao%20and%20Junda%20Huang%20and%20Yao%20Zhao&entry.1292438233=%20%20Panoramic%20distortion%20poses%20a%20significant%20challenge%20in%20360%20depth%20estimation%2C%0Aparticularly%20pronounced%20at%20the%20north%20and%20south%20poles.%20Existing%20methods%20either%0Aadopt%20a%20bi-projection%20fusion%20strategy%20to%20remove%20distortions%20or%20model%20long-range%0Adependencies%20to%20capture%20global%20structures%2C%20which%20can%20result%20in%20either%20unclear%0Astructure%20or%20insufficient%20local%20perception.%20In%20this%20paper%2C%20we%20propose%20a%0Aspherical%20geometry%20transformer%2C%20named%20SGFormer%2C%20to%20address%20the%20above%20issues%2C%0Awith%20an%20innovative%20step%20to%20integrate%20spherical%20geometric%20priors%20into%20vision%0Atransformers.%20To%20this%20end%2C%20we%20retarget%20the%20transformer%20decoder%20to%20a%20spherical%0Aprior%20decoder%20%28termed%20SPDecoder%29%2C%20which%20endeavors%20to%20uphold%20the%20integrity%20of%0Aspherical%20structures%20during%20decoding.%20Concretely%2C%20we%20leverage%20bipolar%0Are-projection%2C%20circular%20rotation%2C%20and%20curve%20local%20embedding%20to%20preserve%20the%0Aspherical%20characteristics%20of%20equidistortion%2C%20continuity%2C%20and%20surface%20distance%2C%0Arespectively.%20Furthermore%2C%20we%20present%20a%20query-based%20global%20conditional%20position%0Aembedding%20to%20compensate%20for%20spatial%20structure%20at%20varying%20resolutions.%20It%20not%0Aonly%20boosts%20the%20global%20perception%20of%20spatial%20position%20but%20also%20sharpens%20the%0Adepth%20structure%20across%20different%20patches.%20Finally%2C%20we%20conduct%20extensive%0Aexperiments%20on%20popular%20benchmarks%2C%20demonstrating%20our%20superiority%20over%0Astate-of-the-art%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14979v3&entry.124074799=Read"},
{"title": "NYT-Connections: A Deceptively Simple Text Classification Task that\n  Stumps System-1 Thinkers", "author": "Angel Yahir Loredo Lopez and Tyler McDonald and Ali Emami", "abstract": "  Large Language Models (LLMs) have shown impressive performance on various\nbenchmarks, yet their ability to engage in deliberate reasoning remains\nquestionable. We present NYT-Connections, a collection of 358 simple word\nclassification puzzles derived from the New York Times Connections game. This\nbenchmark is designed to penalize quick, intuitive \"System 1\" thinking,\nisolating fundamental reasoning skills. We evaluated six recent LLMs, a simple\nmachine learning heuristic, and humans across three configurations:\nsingle-attempt, multiple attempts without hints, and multiple attempts with\ncontextual hints. Our findings reveal a significant performance gap: even\ntop-performing LLMs like GPT-4 fall short of human performance by nearly 30%.\nNotably, advanced prompting techniques such as Chain-of-Thought and\nSelf-Consistency show diminishing returns as task difficulty increases.\nNYT-Connections uniquely combines linguistic isolation, resistance to intuitive\nshortcuts, and regular updates to mitigate data leakage, offering a novel tool\nfor assessing LLM reasoning capabilities.\n", "link": "http://arxiv.org/abs/2412.01621v3", "date": "2025-02-25", "relevancy": 2.3793, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4881}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4881}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NYT-Connections%3A%20A%20Deceptively%20Simple%20Text%20Classification%20Task%20that%0A%20%20Stumps%20System-1%20Thinkers&body=Title%3A%20NYT-Connections%3A%20A%20Deceptively%20Simple%20Text%20Classification%20Task%20that%0A%20%20Stumps%20System-1%20Thinkers%0AAuthor%3A%20Angel%20Yahir%20Loredo%20Lopez%20and%20Tyler%20McDonald%20and%20Ali%20Emami%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20impressive%20performance%20on%20various%0Abenchmarks%2C%20yet%20their%20ability%20to%20engage%20in%20deliberate%20reasoning%20remains%0Aquestionable.%20We%20present%20NYT-Connections%2C%20a%20collection%20of%20358%20simple%20word%0Aclassification%20puzzles%20derived%20from%20the%20New%20York%20Times%20Connections%20game.%20This%0Abenchmark%20is%20designed%20to%20penalize%20quick%2C%20intuitive%20%22System%201%22%20thinking%2C%0Aisolating%20fundamental%20reasoning%20skills.%20We%20evaluated%20six%20recent%20LLMs%2C%20a%20simple%0Amachine%20learning%20heuristic%2C%20and%20humans%20across%20three%20configurations%3A%0Asingle-attempt%2C%20multiple%20attempts%20without%20hints%2C%20and%20multiple%20attempts%20with%0Acontextual%20hints.%20Our%20findings%20reveal%20a%20significant%20performance%20gap%3A%20even%0Atop-performing%20LLMs%20like%20GPT-4%20fall%20short%20of%20human%20performance%20by%20nearly%2030%25.%0ANotably%2C%20advanced%20prompting%20techniques%20such%20as%20Chain-of-Thought%20and%0ASelf-Consistency%20show%20diminishing%20returns%20as%20task%20difficulty%20increases.%0ANYT-Connections%20uniquely%20combines%20linguistic%20isolation%2C%20resistance%20to%20intuitive%0Ashortcuts%2C%20and%20regular%20updates%20to%20mitigate%20data%20leakage%2C%20offering%20a%20novel%20tool%0Afor%20assessing%20LLM%20reasoning%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01621v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNYT-Connections%253A%2520A%2520Deceptively%2520Simple%2520Text%2520Classification%2520Task%2520that%250A%2520%2520Stumps%2520System-1%2520Thinkers%26entry.906535625%3DAngel%2520Yahir%2520Loredo%2520Lopez%2520and%2520Tyler%2520McDonald%2520and%2520Ali%2520Emami%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520impressive%2520performance%2520on%2520various%250Abenchmarks%252C%2520yet%2520their%2520ability%2520to%2520engage%2520in%2520deliberate%2520reasoning%2520remains%250Aquestionable.%2520We%2520present%2520NYT-Connections%252C%2520a%2520collection%2520of%2520358%2520simple%2520word%250Aclassification%2520puzzles%2520derived%2520from%2520the%2520New%2520York%2520Times%2520Connections%2520game.%2520This%250Abenchmark%2520is%2520designed%2520to%2520penalize%2520quick%252C%2520intuitive%2520%2522System%25201%2522%2520thinking%252C%250Aisolating%2520fundamental%2520reasoning%2520skills.%2520We%2520evaluated%2520six%2520recent%2520LLMs%252C%2520a%2520simple%250Amachine%2520learning%2520heuristic%252C%2520and%2520humans%2520across%2520three%2520configurations%253A%250Asingle-attempt%252C%2520multiple%2520attempts%2520without%2520hints%252C%2520and%2520multiple%2520attempts%2520with%250Acontextual%2520hints.%2520Our%2520findings%2520reveal%2520a%2520significant%2520performance%2520gap%253A%2520even%250Atop-performing%2520LLMs%2520like%2520GPT-4%2520fall%2520short%2520of%2520human%2520performance%2520by%2520nearly%252030%2525.%250ANotably%252C%2520advanced%2520prompting%2520techniques%2520such%2520as%2520Chain-of-Thought%2520and%250ASelf-Consistency%2520show%2520diminishing%2520returns%2520as%2520task%2520difficulty%2520increases.%250ANYT-Connections%2520uniquely%2520combines%2520linguistic%2520isolation%252C%2520resistance%2520to%2520intuitive%250Ashortcuts%252C%2520and%2520regular%2520updates%2520to%2520mitigate%2520data%2520leakage%252C%2520offering%2520a%2520novel%2520tool%250Afor%2520assessing%2520LLM%2520reasoning%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01621v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NYT-Connections%3A%20A%20Deceptively%20Simple%20Text%20Classification%20Task%20that%0A%20%20Stumps%20System-1%20Thinkers&entry.906535625=Angel%20Yahir%20Loredo%20Lopez%20and%20Tyler%20McDonald%20and%20Ali%20Emami&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20impressive%20performance%20on%20various%0Abenchmarks%2C%20yet%20their%20ability%20to%20engage%20in%20deliberate%20reasoning%20remains%0Aquestionable.%20We%20present%20NYT-Connections%2C%20a%20collection%20of%20358%20simple%20word%0Aclassification%20puzzles%20derived%20from%20the%20New%20York%20Times%20Connections%20game.%20This%0Abenchmark%20is%20designed%20to%20penalize%20quick%2C%20intuitive%20%22System%201%22%20thinking%2C%0Aisolating%20fundamental%20reasoning%20skills.%20We%20evaluated%20six%20recent%20LLMs%2C%20a%20simple%0Amachine%20learning%20heuristic%2C%20and%20humans%20across%20three%20configurations%3A%0Asingle-attempt%2C%20multiple%20attempts%20without%20hints%2C%20and%20multiple%20attempts%20with%0Acontextual%20hints.%20Our%20findings%20reveal%20a%20significant%20performance%20gap%3A%20even%0Atop-performing%20LLMs%20like%20GPT-4%20fall%20short%20of%20human%20performance%20by%20nearly%2030%25.%0ANotably%2C%20advanced%20prompting%20techniques%20such%20as%20Chain-of-Thought%20and%0ASelf-Consistency%20show%20diminishing%20returns%20as%20task%20difficulty%20increases.%0ANYT-Connections%20uniquely%20combines%20linguistic%20isolation%2C%20resistance%20to%20intuitive%0Ashortcuts%2C%20and%20regular%20updates%20to%20mitigate%20data%20leakage%2C%20offering%20a%20novel%20tool%0Afor%20assessing%20LLM%20reasoning%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01621v3&entry.124074799=Read"},
{"title": "Evolution 6.0: Evolving Robotic Capabilities Through Generative Design", "author": "Muhammad Haris Khan and Artyom Myshlyaev and Artem Lykov and Miguel Altamirano Cabrera and Dzmitry Tsetserukou", "abstract": "  We propose a new concept, Evolution 6.0, which represents the evolution of\nrobotics driven by Generative AI. When a robot lacks the necessary tools to\naccomplish a task requested by a human, it autonomously designs the required\ninstruments and learns how to use them to achieve the goal. Evolution 6.0 is an\nautonomous robotic system powered by Vision-Language Models (VLMs),\nVision-Language Action (VLA) models, and Text-to-3D generative models for tool\ndesign and task execution. The system comprises two key modules: the Tool\nGeneration Module, which fabricates task-specific tools from visual and textual\ndata, and the Action Generation Module, which converts natural language\ninstructions into robotic actions. It integrates QwenVLM for environmental\nunderstanding, OpenVLA for task execution, and Llama-Mesh for 3D tool\ngeneration. Evaluation results demonstrate a 90% success rate for tool\ngeneration with a 10-second inference time, and action generation achieving\n83.5% in physical and visual generalization, 70% in motion generalization, and\n37% in semantic generalization. Future improvements will focus on bimanual\nmanipulation, expanded task capabilities, and enhanced environmental\ninterpretation to improve real-world adaptability.\n", "link": "http://arxiv.org/abs/2502.17034v2", "date": "2025-02-25", "relevancy": 2.3465, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5998}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5855}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evolution%206.0%3A%20Evolving%20Robotic%20Capabilities%20Through%20Generative%20Design&body=Title%3A%20Evolution%206.0%3A%20Evolving%20Robotic%20Capabilities%20Through%20Generative%20Design%0AAuthor%3A%20Muhammad%20Haris%20Khan%20and%20Artyom%20Myshlyaev%20and%20Artem%20Lykov%20and%20Miguel%20Altamirano%20Cabrera%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20We%20propose%20a%20new%20concept%2C%20Evolution%206.0%2C%20which%20represents%20the%20evolution%20of%0Arobotics%20driven%20by%20Generative%20AI.%20When%20a%20robot%20lacks%20the%20necessary%20tools%20to%0Aaccomplish%20a%20task%20requested%20by%20a%20human%2C%20it%20autonomously%20designs%20the%20required%0Ainstruments%20and%20learns%20how%20to%20use%20them%20to%20achieve%20the%20goal.%20Evolution%206.0%20is%20an%0Aautonomous%20robotic%20system%20powered%20by%20Vision-Language%20Models%20%28VLMs%29%2C%0AVision-Language%20Action%20%28VLA%29%20models%2C%20and%20Text-to-3D%20generative%20models%20for%20tool%0Adesign%20and%20task%20execution.%20The%20system%20comprises%20two%20key%20modules%3A%20the%20Tool%0AGeneration%20Module%2C%20which%20fabricates%20task-specific%20tools%20from%20visual%20and%20textual%0Adata%2C%20and%20the%20Action%20Generation%20Module%2C%20which%20converts%20natural%20language%0Ainstructions%20into%20robotic%20actions.%20It%20integrates%20QwenVLM%20for%20environmental%0Aunderstanding%2C%20OpenVLA%20for%20task%20execution%2C%20and%20Llama-Mesh%20for%203D%20tool%0Ageneration.%20Evaluation%20results%20demonstrate%20a%2090%25%20success%20rate%20for%20tool%0Ageneration%20with%20a%2010-second%20inference%20time%2C%20and%20action%20generation%20achieving%0A83.5%25%20in%20physical%20and%20visual%20generalization%2C%2070%25%20in%20motion%20generalization%2C%20and%0A37%25%20in%20semantic%20generalization.%20Future%20improvements%20will%20focus%20on%20bimanual%0Amanipulation%2C%20expanded%20task%20capabilities%2C%20and%20enhanced%20environmental%0Ainterpretation%20to%20improve%20real-world%20adaptability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17034v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvolution%25206.0%253A%2520Evolving%2520Robotic%2520Capabilities%2520Through%2520Generative%2520Design%26entry.906535625%3DMuhammad%2520Haris%2520Khan%2520and%2520Artyom%2520Myshlyaev%2520and%2520Artem%2520Lykov%2520and%2520Miguel%2520Altamirano%2520Cabrera%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520concept%252C%2520Evolution%25206.0%252C%2520which%2520represents%2520the%2520evolution%2520of%250Arobotics%2520driven%2520by%2520Generative%2520AI.%2520When%2520a%2520robot%2520lacks%2520the%2520necessary%2520tools%2520to%250Aaccomplish%2520a%2520task%2520requested%2520by%2520a%2520human%252C%2520it%2520autonomously%2520designs%2520the%2520required%250Ainstruments%2520and%2520learns%2520how%2520to%2520use%2520them%2520to%2520achieve%2520the%2520goal.%2520Evolution%25206.0%2520is%2520an%250Aautonomous%2520robotic%2520system%2520powered%2520by%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%250AVision-Language%2520Action%2520%2528VLA%2529%2520models%252C%2520and%2520Text-to-3D%2520generative%2520models%2520for%2520tool%250Adesign%2520and%2520task%2520execution.%2520The%2520system%2520comprises%2520two%2520key%2520modules%253A%2520the%2520Tool%250AGeneration%2520Module%252C%2520which%2520fabricates%2520task-specific%2520tools%2520from%2520visual%2520and%2520textual%250Adata%252C%2520and%2520the%2520Action%2520Generation%2520Module%252C%2520which%2520converts%2520natural%2520language%250Ainstructions%2520into%2520robotic%2520actions.%2520It%2520integrates%2520QwenVLM%2520for%2520environmental%250Aunderstanding%252C%2520OpenVLA%2520for%2520task%2520execution%252C%2520and%2520Llama-Mesh%2520for%25203D%2520tool%250Ageneration.%2520Evaluation%2520results%2520demonstrate%2520a%252090%2525%2520success%2520rate%2520for%2520tool%250Ageneration%2520with%2520a%252010-second%2520inference%2520time%252C%2520and%2520action%2520generation%2520achieving%250A83.5%2525%2520in%2520physical%2520and%2520visual%2520generalization%252C%252070%2525%2520in%2520motion%2520generalization%252C%2520and%250A37%2525%2520in%2520semantic%2520generalization.%2520Future%2520improvements%2520will%2520focus%2520on%2520bimanual%250Amanipulation%252C%2520expanded%2520task%2520capabilities%252C%2520and%2520enhanced%2520environmental%250Ainterpretation%2520to%2520improve%2520real-world%2520adaptability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17034v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evolution%206.0%3A%20Evolving%20Robotic%20Capabilities%20Through%20Generative%20Design&entry.906535625=Muhammad%20Haris%20Khan%20and%20Artyom%20Myshlyaev%20and%20Artem%20Lykov%20and%20Miguel%20Altamirano%20Cabrera%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20We%20propose%20a%20new%20concept%2C%20Evolution%206.0%2C%20which%20represents%20the%20evolution%20of%0Arobotics%20driven%20by%20Generative%20AI.%20When%20a%20robot%20lacks%20the%20necessary%20tools%20to%0Aaccomplish%20a%20task%20requested%20by%20a%20human%2C%20it%20autonomously%20designs%20the%20required%0Ainstruments%20and%20learns%20how%20to%20use%20them%20to%20achieve%20the%20goal.%20Evolution%206.0%20is%20an%0Aautonomous%20robotic%20system%20powered%20by%20Vision-Language%20Models%20%28VLMs%29%2C%0AVision-Language%20Action%20%28VLA%29%20models%2C%20and%20Text-to-3D%20generative%20models%20for%20tool%0Adesign%20and%20task%20execution.%20The%20system%20comprises%20two%20key%20modules%3A%20the%20Tool%0AGeneration%20Module%2C%20which%20fabricates%20task-specific%20tools%20from%20visual%20and%20textual%0Adata%2C%20and%20the%20Action%20Generation%20Module%2C%20which%20converts%20natural%20language%0Ainstructions%20into%20robotic%20actions.%20It%20integrates%20QwenVLM%20for%20environmental%0Aunderstanding%2C%20OpenVLA%20for%20task%20execution%2C%20and%20Llama-Mesh%20for%203D%20tool%0Ageneration.%20Evaluation%20results%20demonstrate%20a%2090%25%20success%20rate%20for%20tool%0Ageneration%20with%20a%2010-second%20inference%20time%2C%20and%20action%20generation%20achieving%0A83.5%25%20in%20physical%20and%20visual%20generalization%2C%2070%25%20in%20motion%20generalization%2C%20and%0A37%25%20in%20semantic%20generalization.%20Future%20improvements%20will%20focus%20on%20bimanual%0Amanipulation%2C%20expanded%20task%20capabilities%2C%20and%20enhanced%20environmental%0Ainterpretation%20to%20improve%20real-world%20adaptability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17034v2&entry.124074799=Read"},
{"title": "Jacobian Sparse Autoencoders: Sparsify Computations, Not Just\n  Activations", "author": "Lucy Farnik and Tim Lawson and Conor Houghton and Laurence Aitchison", "abstract": "  Sparse autoencoders (SAEs) have been successfully used to discover sparse and\nhuman-interpretable representations of the latent activations of LLMs. However,\nwe would ultimately like to understand the computations performed by LLMs and\nnot just their representations. The extent to which SAEs can help us understand\ncomputations is unclear because they are not designed to \"sparsify\"\ncomputations in any sense, only latent activations. To solve this, we propose\nJacobian SAEs (JSAEs), which yield not only sparsity in the input and output\nactivations of a given model component but also sparsity in the computation\n(formally, the Jacobian) connecting them. With a na\\\"ive implementation, the\nJacobians in LLMs would be computationally intractable due to their size. One\nkey technical contribution is thus finding an efficient way of computing\nJacobians in this setup. We find that JSAEs extract a relatively large degree\nof computational sparsity while preserving downstream LLM performance\napproximately as well as traditional SAEs. We also show that Jacobians are a\nreasonable proxy for computational sparsity because MLPs are approximately\nlinear when rewritten in the JSAE basis. Lastly, we show that JSAEs achieve a\ngreater degree of computational sparsity on pre-trained LLMs than on the\nequivalent randomized LLM. This shows that the sparsity of the computational\ngraph appears to be a property that LLMs learn through training, and suggests\nthat JSAEs might be more suitable for understanding learned transformer\ncomputations than standard SAEs.\n", "link": "http://arxiv.org/abs/2502.18147v1", "date": "2025-02-25", "relevancy": 2.3387, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4866}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4673}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jacobian%20Sparse%20Autoencoders%3A%20Sparsify%20Computations%2C%20Not%20Just%0A%20%20Activations&body=Title%3A%20Jacobian%20Sparse%20Autoencoders%3A%20Sparsify%20Computations%2C%20Not%20Just%0A%20%20Activations%0AAuthor%3A%20Lucy%20Farnik%20and%20Tim%20Lawson%20and%20Conor%20Houghton%20and%20Laurence%20Aitchison%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20have%20been%20successfully%20used%20to%20discover%20sparse%20and%0Ahuman-interpretable%20representations%20of%20the%20latent%20activations%20of%20LLMs.%20However%2C%0Awe%20would%20ultimately%20like%20to%20understand%20the%20computations%20performed%20by%20LLMs%20and%0Anot%20just%20their%20representations.%20The%20extent%20to%20which%20SAEs%20can%20help%20us%20understand%0Acomputations%20is%20unclear%20because%20they%20are%20not%20designed%20to%20%22sparsify%22%0Acomputations%20in%20any%20sense%2C%20only%20latent%20activations.%20To%20solve%20this%2C%20we%20propose%0AJacobian%20SAEs%20%28JSAEs%29%2C%20which%20yield%20not%20only%20sparsity%20in%20the%20input%20and%20output%0Aactivations%20of%20a%20given%20model%20component%20but%20also%20sparsity%20in%20the%20computation%0A%28formally%2C%20the%20Jacobian%29%20connecting%20them.%20With%20a%20na%5C%22ive%20implementation%2C%20the%0AJacobians%20in%20LLMs%20would%20be%20computationally%20intractable%20due%20to%20their%20size.%20One%0Akey%20technical%20contribution%20is%20thus%20finding%20an%20efficient%20way%20of%20computing%0AJacobians%20in%20this%20setup.%20We%20find%20that%20JSAEs%20extract%20a%20relatively%20large%20degree%0Aof%20computational%20sparsity%20while%20preserving%20downstream%20LLM%20performance%0Aapproximately%20as%20well%20as%20traditional%20SAEs.%20We%20also%20show%20that%20Jacobians%20are%20a%0Areasonable%20proxy%20for%20computational%20sparsity%20because%20MLPs%20are%20approximately%0Alinear%20when%20rewritten%20in%20the%20JSAE%20basis.%20Lastly%2C%20we%20show%20that%20JSAEs%20achieve%20a%0Agreater%20degree%20of%20computational%20sparsity%20on%20pre-trained%20LLMs%20than%20on%20the%0Aequivalent%20randomized%20LLM.%20This%20shows%20that%20the%20sparsity%20of%20the%20computational%0Agraph%20appears%20to%20be%20a%20property%20that%20LLMs%20learn%20through%20training%2C%20and%20suggests%0Athat%20JSAEs%20might%20be%20more%20suitable%20for%20understanding%20learned%20transformer%0Acomputations%20than%20standard%20SAEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJacobian%2520Sparse%2520Autoencoders%253A%2520Sparsify%2520Computations%252C%2520Not%2520Just%250A%2520%2520Activations%26entry.906535625%3DLucy%2520Farnik%2520and%2520Tim%2520Lawson%2520and%2520Conor%2520Houghton%2520and%2520Laurence%2520Aitchison%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520have%2520been%2520successfully%2520used%2520to%2520discover%2520sparse%2520and%250Ahuman-interpretable%2520representations%2520of%2520the%2520latent%2520activations%2520of%2520LLMs.%2520However%252C%250Awe%2520would%2520ultimately%2520like%2520to%2520understand%2520the%2520computations%2520performed%2520by%2520LLMs%2520and%250Anot%2520just%2520their%2520representations.%2520The%2520extent%2520to%2520which%2520SAEs%2520can%2520help%2520us%2520understand%250Acomputations%2520is%2520unclear%2520because%2520they%2520are%2520not%2520designed%2520to%2520%2522sparsify%2522%250Acomputations%2520in%2520any%2520sense%252C%2520only%2520latent%2520activations.%2520To%2520solve%2520this%252C%2520we%2520propose%250AJacobian%2520SAEs%2520%2528JSAEs%2529%252C%2520which%2520yield%2520not%2520only%2520sparsity%2520in%2520the%2520input%2520and%2520output%250Aactivations%2520of%2520a%2520given%2520model%2520component%2520but%2520also%2520sparsity%2520in%2520the%2520computation%250A%2528formally%252C%2520the%2520Jacobian%2529%2520connecting%2520them.%2520With%2520a%2520na%255C%2522ive%2520implementation%252C%2520the%250AJacobians%2520in%2520LLMs%2520would%2520be%2520computationally%2520intractable%2520due%2520to%2520their%2520size.%2520One%250Akey%2520technical%2520contribution%2520is%2520thus%2520finding%2520an%2520efficient%2520way%2520of%2520computing%250AJacobians%2520in%2520this%2520setup.%2520We%2520find%2520that%2520JSAEs%2520extract%2520a%2520relatively%2520large%2520degree%250Aof%2520computational%2520sparsity%2520while%2520preserving%2520downstream%2520LLM%2520performance%250Aapproximately%2520as%2520well%2520as%2520traditional%2520SAEs.%2520We%2520also%2520show%2520that%2520Jacobians%2520are%2520a%250Areasonable%2520proxy%2520for%2520computational%2520sparsity%2520because%2520MLPs%2520are%2520approximately%250Alinear%2520when%2520rewritten%2520in%2520the%2520JSAE%2520basis.%2520Lastly%252C%2520we%2520show%2520that%2520JSAEs%2520achieve%2520a%250Agreater%2520degree%2520of%2520computational%2520sparsity%2520on%2520pre-trained%2520LLMs%2520than%2520on%2520the%250Aequivalent%2520randomized%2520LLM.%2520This%2520shows%2520that%2520the%2520sparsity%2520of%2520the%2520computational%250Agraph%2520appears%2520to%2520be%2520a%2520property%2520that%2520LLMs%2520learn%2520through%2520training%252C%2520and%2520suggests%250Athat%2520JSAEs%2520might%2520be%2520more%2520suitable%2520for%2520understanding%2520learned%2520transformer%250Acomputations%2520than%2520standard%2520SAEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jacobian%20Sparse%20Autoencoders%3A%20Sparsify%20Computations%2C%20Not%20Just%0A%20%20Activations&entry.906535625=Lucy%20Farnik%20and%20Tim%20Lawson%20and%20Conor%20Houghton%20and%20Laurence%20Aitchison&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20have%20been%20successfully%20used%20to%20discover%20sparse%20and%0Ahuman-interpretable%20representations%20of%20the%20latent%20activations%20of%20LLMs.%20However%2C%0Awe%20would%20ultimately%20like%20to%20understand%20the%20computations%20performed%20by%20LLMs%20and%0Anot%20just%20their%20representations.%20The%20extent%20to%20which%20SAEs%20can%20help%20us%20understand%0Acomputations%20is%20unclear%20because%20they%20are%20not%20designed%20to%20%22sparsify%22%0Acomputations%20in%20any%20sense%2C%20only%20latent%20activations.%20To%20solve%20this%2C%20we%20propose%0AJacobian%20SAEs%20%28JSAEs%29%2C%20which%20yield%20not%20only%20sparsity%20in%20the%20input%20and%20output%0Aactivations%20of%20a%20given%20model%20component%20but%20also%20sparsity%20in%20the%20computation%0A%28formally%2C%20the%20Jacobian%29%20connecting%20them.%20With%20a%20na%5C%22ive%20implementation%2C%20the%0AJacobians%20in%20LLMs%20would%20be%20computationally%20intractable%20due%20to%20their%20size.%20One%0Akey%20technical%20contribution%20is%20thus%20finding%20an%20efficient%20way%20of%20computing%0AJacobians%20in%20this%20setup.%20We%20find%20that%20JSAEs%20extract%20a%20relatively%20large%20degree%0Aof%20computational%20sparsity%20while%20preserving%20downstream%20LLM%20performance%0Aapproximately%20as%20well%20as%20traditional%20SAEs.%20We%20also%20show%20that%20Jacobians%20are%20a%0Areasonable%20proxy%20for%20computational%20sparsity%20because%20MLPs%20are%20approximately%0Alinear%20when%20rewritten%20in%20the%20JSAE%20basis.%20Lastly%2C%20we%20show%20that%20JSAEs%20achieve%20a%0Agreater%20degree%20of%20computational%20sparsity%20on%20pre-trained%20LLMs%20than%20on%20the%0Aequivalent%20randomized%20LLM.%20This%20shows%20that%20the%20sparsity%20of%20the%20computational%0Agraph%20appears%20to%20be%20a%20property%20that%20LLMs%20learn%20through%20training%2C%20and%20suggests%0Athat%20JSAEs%20might%20be%20more%20suitable%20for%20understanding%20learned%20transformer%0Acomputations%20than%20standard%20SAEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18147v1&entry.124074799=Read"},
{"title": "IG-CFAT: An Improved GAN-Based Framework for Effectively Exploiting\n  Transformers in Real-World Image Super-Resolution", "author": "Alireza Aghelan and Ali Amiryan and Abolfazl Zarghani and Modjtaba Rouhani", "abstract": "  In the field of single image super-resolution (SISR), transformer-based\nmodels, have demonstrated significant advancements. However, the potential and\nefficiency of these models in applied fields such as real-world image\nsuper-resolution have been less noticed and there are substantial opportunities\nfor improvement. Recently, composite fusion attention transformer (CFAT),\noutperformed previous state-of-the-art (SOTA) models in classic image\nsuper-resolution. In this paper, we propose a novel GAN-based framework by\nincorporating the CFAT model to effectively exploit the performance of\ntransformers in real-world image super-resolution. In our proposed approach, we\nintegrate a semantic-aware discriminator to reconstruct fine details more\naccurately and employ an adaptive degradation model to better simulate\nreal-world degradations. Moreover, we introduce a new combination of loss\nfunctions by adding wavelet loss to loss functions of GAN-based models to\nbetter recover high-frequency details. Empirical results demonstrate that\nIG-CFAT significantly outperforms existing SOTA models in both quantitative and\nqualitative metrics. Our proposed model revolutionizes the field of real-world\nimage super-resolution and demonstrates substantially better performance in\nrecovering fine details and generating realistic textures. The introduction of\nIG-CFAT offers a robust and adaptable solution for real-world image\nsuper-resolution tasks.\n", "link": "http://arxiv.org/abs/2406.13815v4", "date": "2025-02-25", "relevancy": 2.3367, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.649}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5739}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IG-CFAT%3A%20An%20Improved%20GAN-Based%20Framework%20for%20Effectively%20Exploiting%0A%20%20Transformers%20in%20Real-World%20Image%20Super-Resolution&body=Title%3A%20IG-CFAT%3A%20An%20Improved%20GAN-Based%20Framework%20for%20Effectively%20Exploiting%0A%20%20Transformers%20in%20Real-World%20Image%20Super-Resolution%0AAuthor%3A%20Alireza%20Aghelan%20and%20Ali%20Amiryan%20and%20Abolfazl%20Zarghani%20and%20Modjtaba%20Rouhani%0AAbstract%3A%20%20%20In%20the%20field%20of%20single%20image%20super-resolution%20%28SISR%29%2C%20transformer-based%0Amodels%2C%20have%20demonstrated%20significant%20advancements.%20However%2C%20the%20potential%20and%0Aefficiency%20of%20these%20models%20in%20applied%20fields%20such%20as%20real-world%20image%0Asuper-resolution%20have%20been%20less%20noticed%20and%20there%20are%20substantial%20opportunities%0Afor%20improvement.%20Recently%2C%20composite%20fusion%20attention%20transformer%20%28CFAT%29%2C%0Aoutperformed%20previous%20state-of-the-art%20%28SOTA%29%20models%20in%20classic%20image%0Asuper-resolution.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20GAN-based%20framework%20by%0Aincorporating%20the%20CFAT%20model%20to%20effectively%20exploit%20the%20performance%20of%0Atransformers%20in%20real-world%20image%20super-resolution.%20In%20our%20proposed%20approach%2C%20we%0Aintegrate%20a%20semantic-aware%20discriminator%20to%20reconstruct%20fine%20details%20more%0Aaccurately%20and%20employ%20an%20adaptive%20degradation%20model%20to%20better%20simulate%0Areal-world%20degradations.%20Moreover%2C%20we%20introduce%20a%20new%20combination%20of%20loss%0Afunctions%20by%20adding%20wavelet%20loss%20to%20loss%20functions%20of%20GAN-based%20models%20to%0Abetter%20recover%20high-frequency%20details.%20Empirical%20results%20demonstrate%20that%0AIG-CFAT%20significantly%20outperforms%20existing%20SOTA%20models%20in%20both%20quantitative%20and%0Aqualitative%20metrics.%20Our%20proposed%20model%20revolutionizes%20the%20field%20of%20real-world%0Aimage%20super-resolution%20and%20demonstrates%20substantially%20better%20performance%20in%0Arecovering%20fine%20details%20and%20generating%20realistic%20textures.%20The%20introduction%20of%0AIG-CFAT%20offers%20a%20robust%20and%20adaptable%20solution%20for%20real-world%20image%0Asuper-resolution%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13815v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIG-CFAT%253A%2520An%2520Improved%2520GAN-Based%2520Framework%2520for%2520Effectively%2520Exploiting%250A%2520%2520Transformers%2520in%2520Real-World%2520Image%2520Super-Resolution%26entry.906535625%3DAlireza%2520Aghelan%2520and%2520Ali%2520Amiryan%2520and%2520Abolfazl%2520Zarghani%2520and%2520Modjtaba%2520Rouhani%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520single%2520image%2520super-resolution%2520%2528SISR%2529%252C%2520transformer-based%250Amodels%252C%2520have%2520demonstrated%2520significant%2520advancements.%2520However%252C%2520the%2520potential%2520and%250Aefficiency%2520of%2520these%2520models%2520in%2520applied%2520fields%2520such%2520as%2520real-world%2520image%250Asuper-resolution%2520have%2520been%2520less%2520noticed%2520and%2520there%2520are%2520substantial%2520opportunities%250Afor%2520improvement.%2520Recently%252C%2520composite%2520fusion%2520attention%2520transformer%2520%2528CFAT%2529%252C%250Aoutperformed%2520previous%2520state-of-the-art%2520%2528SOTA%2529%2520models%2520in%2520classic%2520image%250Asuper-resolution.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520GAN-based%2520framework%2520by%250Aincorporating%2520the%2520CFAT%2520model%2520to%2520effectively%2520exploit%2520the%2520performance%2520of%250Atransformers%2520in%2520real-world%2520image%2520super-resolution.%2520In%2520our%2520proposed%2520approach%252C%2520we%250Aintegrate%2520a%2520semantic-aware%2520discriminator%2520to%2520reconstruct%2520fine%2520details%2520more%250Aaccurately%2520and%2520employ%2520an%2520adaptive%2520degradation%2520model%2520to%2520better%2520simulate%250Areal-world%2520degradations.%2520Moreover%252C%2520we%2520introduce%2520a%2520new%2520combination%2520of%2520loss%250Afunctions%2520by%2520adding%2520wavelet%2520loss%2520to%2520loss%2520functions%2520of%2520GAN-based%2520models%2520to%250Abetter%2520recover%2520high-frequency%2520details.%2520Empirical%2520results%2520demonstrate%2520that%250AIG-CFAT%2520significantly%2520outperforms%2520existing%2520SOTA%2520models%2520in%2520both%2520quantitative%2520and%250Aqualitative%2520metrics.%2520Our%2520proposed%2520model%2520revolutionizes%2520the%2520field%2520of%2520real-world%250Aimage%2520super-resolution%2520and%2520demonstrates%2520substantially%2520better%2520performance%2520in%250Arecovering%2520fine%2520details%2520and%2520generating%2520realistic%2520textures.%2520The%2520introduction%2520of%250AIG-CFAT%2520offers%2520a%2520robust%2520and%2520adaptable%2520solution%2520for%2520real-world%2520image%250Asuper-resolution%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13815v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IG-CFAT%3A%20An%20Improved%20GAN-Based%20Framework%20for%20Effectively%20Exploiting%0A%20%20Transformers%20in%20Real-World%20Image%20Super-Resolution&entry.906535625=Alireza%20Aghelan%20and%20Ali%20Amiryan%20and%20Abolfazl%20Zarghani%20and%20Modjtaba%20Rouhani&entry.1292438233=%20%20In%20the%20field%20of%20single%20image%20super-resolution%20%28SISR%29%2C%20transformer-based%0Amodels%2C%20have%20demonstrated%20significant%20advancements.%20However%2C%20the%20potential%20and%0Aefficiency%20of%20these%20models%20in%20applied%20fields%20such%20as%20real-world%20image%0Asuper-resolution%20have%20been%20less%20noticed%20and%20there%20are%20substantial%20opportunities%0Afor%20improvement.%20Recently%2C%20composite%20fusion%20attention%20transformer%20%28CFAT%29%2C%0Aoutperformed%20previous%20state-of-the-art%20%28SOTA%29%20models%20in%20classic%20image%0Asuper-resolution.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20GAN-based%20framework%20by%0Aincorporating%20the%20CFAT%20model%20to%20effectively%20exploit%20the%20performance%20of%0Atransformers%20in%20real-world%20image%20super-resolution.%20In%20our%20proposed%20approach%2C%20we%0Aintegrate%20a%20semantic-aware%20discriminator%20to%20reconstruct%20fine%20details%20more%0Aaccurately%20and%20employ%20an%20adaptive%20degradation%20model%20to%20better%20simulate%0Areal-world%20degradations.%20Moreover%2C%20we%20introduce%20a%20new%20combination%20of%20loss%0Afunctions%20by%20adding%20wavelet%20loss%20to%20loss%20functions%20of%20GAN-based%20models%20to%0Abetter%20recover%20high-frequency%20details.%20Empirical%20results%20demonstrate%20that%0AIG-CFAT%20significantly%20outperforms%20existing%20SOTA%20models%20in%20both%20quantitative%20and%0Aqualitative%20metrics.%20Our%20proposed%20model%20revolutionizes%20the%20field%20of%20real-world%0Aimage%20super-resolution%20and%20demonstrates%20substantially%20better%20performance%20in%0Arecovering%20fine%20details%20and%20generating%20realistic%20textures.%20The%20introduction%20of%0AIG-CFAT%20offers%20a%20robust%20and%20adaptable%20solution%20for%20real-world%20image%0Asuper-resolution%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13815v4&entry.124074799=Read"},
{"title": "K-LoRA: Unlocking Training-Free Fusion of Any Subject and Style LoRAs", "author": "Ziheng Ouyang and Zhen Li and Qibin Hou", "abstract": "  Recent studies have explored combining different LoRAs to jointly generate\nlearned style and content. However, existing methods either fail to effectively\npreserve both the original subject and style simultaneously or require\nadditional training. In this paper, we argue that the intrinsic properties of\nLoRA can effectively guide diffusion models in merging learned subject and\nstyle. Building on this insight, we propose K-LoRA, a simple yet effective\ntraining-free LoRA fusion approach. In each attention layer, K-LoRA compares\nthe Top-K elements in each LoRA to be fused, determining which LoRA to select\nfor optimal fusion. This selection mechanism ensures that the most\nrepresentative features of both subject and style are retained during the\nfusion process, effectively balancing their contributions. Experimental results\ndemonstrate that the proposed method effectively integrates the subject and\nstyle information learned by the original LoRAs, outperforming state-of-the-art\ntraining-based approaches in both qualitative and quantitative results.\n", "link": "http://arxiv.org/abs/2502.18461v1", "date": "2025-02-25", "relevancy": 2.3243, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4818}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4567}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20K-LoRA%3A%20Unlocking%20Training-Free%20Fusion%20of%20Any%20Subject%20and%20Style%20LoRAs&body=Title%3A%20K-LoRA%3A%20Unlocking%20Training-Free%20Fusion%20of%20Any%20Subject%20and%20Style%20LoRAs%0AAuthor%3A%20Ziheng%20Ouyang%20and%20Zhen%20Li%20and%20Qibin%20Hou%0AAbstract%3A%20%20%20Recent%20studies%20have%20explored%20combining%20different%20LoRAs%20to%20jointly%20generate%0Alearned%20style%20and%20content.%20However%2C%20existing%20methods%20either%20fail%20to%20effectively%0Apreserve%20both%20the%20original%20subject%20and%20style%20simultaneously%20or%20require%0Aadditional%20training.%20In%20this%20paper%2C%20we%20argue%20that%20the%20intrinsic%20properties%20of%0ALoRA%20can%20effectively%20guide%20diffusion%20models%20in%20merging%20learned%20subject%20and%0Astyle.%20Building%20on%20this%20insight%2C%20we%20propose%20K-LoRA%2C%20a%20simple%20yet%20effective%0Atraining-free%20LoRA%20fusion%20approach.%20In%20each%20attention%20layer%2C%20K-LoRA%20compares%0Athe%20Top-K%20elements%20in%20each%20LoRA%20to%20be%20fused%2C%20determining%20which%20LoRA%20to%20select%0Afor%20optimal%20fusion.%20This%20selection%20mechanism%20ensures%20that%20the%20most%0Arepresentative%20features%20of%20both%20subject%20and%20style%20are%20retained%20during%20the%0Afusion%20process%2C%20effectively%20balancing%20their%20contributions.%20Experimental%20results%0Ademonstrate%20that%20the%20proposed%20method%20effectively%20integrates%20the%20subject%20and%0Astyle%20information%20learned%20by%20the%20original%20LoRAs%2C%20outperforming%20state-of-the-art%0Atraining-based%20approaches%20in%20both%20qualitative%20and%20quantitative%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DK-LoRA%253A%2520Unlocking%2520Training-Free%2520Fusion%2520of%2520Any%2520Subject%2520and%2520Style%2520LoRAs%26entry.906535625%3DZiheng%2520Ouyang%2520and%2520Zhen%2520Li%2520and%2520Qibin%2520Hou%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520explored%2520combining%2520different%2520LoRAs%2520to%2520jointly%2520generate%250Alearned%2520style%2520and%2520content.%2520However%252C%2520existing%2520methods%2520either%2520fail%2520to%2520effectively%250Apreserve%2520both%2520the%2520original%2520subject%2520and%2520style%2520simultaneously%2520or%2520require%250Aadditional%2520training.%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%2520the%2520intrinsic%2520properties%2520of%250ALoRA%2520can%2520effectively%2520guide%2520diffusion%2520models%2520in%2520merging%2520learned%2520subject%2520and%250Astyle.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520K-LoRA%252C%2520a%2520simple%2520yet%2520effective%250Atraining-free%2520LoRA%2520fusion%2520approach.%2520In%2520each%2520attention%2520layer%252C%2520K-LoRA%2520compares%250Athe%2520Top-K%2520elements%2520in%2520each%2520LoRA%2520to%2520be%2520fused%252C%2520determining%2520which%2520LoRA%2520to%2520select%250Afor%2520optimal%2520fusion.%2520This%2520selection%2520mechanism%2520ensures%2520that%2520the%2520most%250Arepresentative%2520features%2520of%2520both%2520subject%2520and%2520style%2520are%2520retained%2520during%2520the%250Afusion%2520process%252C%2520effectively%2520balancing%2520their%2520contributions.%2520Experimental%2520results%250Ademonstrate%2520that%2520the%2520proposed%2520method%2520effectively%2520integrates%2520the%2520subject%2520and%250Astyle%2520information%2520learned%2520by%2520the%2520original%2520LoRAs%252C%2520outperforming%2520state-of-the-art%250Atraining-based%2520approaches%2520in%2520both%2520qualitative%2520and%2520quantitative%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=K-LoRA%3A%20Unlocking%20Training-Free%20Fusion%20of%20Any%20Subject%20and%20Style%20LoRAs&entry.906535625=Ziheng%20Ouyang%20and%20Zhen%20Li%20and%20Qibin%20Hou&entry.1292438233=%20%20Recent%20studies%20have%20explored%20combining%20different%20LoRAs%20to%20jointly%20generate%0Alearned%20style%20and%20content.%20However%2C%20existing%20methods%20either%20fail%20to%20effectively%0Apreserve%20both%20the%20original%20subject%20and%20style%20simultaneously%20or%20require%0Aadditional%20training.%20In%20this%20paper%2C%20we%20argue%20that%20the%20intrinsic%20properties%20of%0ALoRA%20can%20effectively%20guide%20diffusion%20models%20in%20merging%20learned%20subject%20and%0Astyle.%20Building%20on%20this%20insight%2C%20we%20propose%20K-LoRA%2C%20a%20simple%20yet%20effective%0Atraining-free%20LoRA%20fusion%20approach.%20In%20each%20attention%20layer%2C%20K-LoRA%20compares%0Athe%20Top-K%20elements%20in%20each%20LoRA%20to%20be%20fused%2C%20determining%20which%20LoRA%20to%20select%0Afor%20optimal%20fusion.%20This%20selection%20mechanism%20ensures%20that%20the%20most%0Arepresentative%20features%20of%20both%20subject%20and%20style%20are%20retained%20during%20the%0Afusion%20process%2C%20effectively%20balancing%20their%20contributions.%20Experimental%20results%0Ademonstrate%20that%20the%20proposed%20method%20effectively%20integrates%20the%20subject%20and%0Astyle%20information%20learned%20by%20the%20original%20LoRAs%2C%20outperforming%20state-of-the-art%0Atraining-based%20approaches%20in%20both%20qualitative%20and%20quantitative%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18461v1&entry.124074799=Read"},
{"title": "SASSHA: Sharpness-aware Adaptive Second-order Optimization with Stable\n  Hessian Approximation", "author": "Dahun Shin and Dongyeop Lee and Jinseok Chung and Namhoon Lee", "abstract": "  Approximate second-order optimization methods often exhibit poorer\ngeneralization compared to first-order approaches. In this work, we look into\nthis issue through the lens of the loss landscape and find that existing\nsecond-order methods tend to converge to sharper minima compared to SGD. In\nresponse, we propose Sassha, a novel second-order method designed to enhance\ngeneralization by explicitly reducing sharpness of the solution, while\nstabilizing the computation of approximate Hessians along the optimization\ntrajectory. In fact, this sharpness minimization scheme is crafted also to\naccommodate lazy Hessian updates, so as to secure efficiency besides flatness.\nTo validate its effectiveness, we conduct a wide range of standard deep\nlearning experiments where Sassha demonstrates its outstanding generalization\nperformance that is comparable to, and mostly better than, other methods. We\nprovide a comprehensive set of analyses including convergence, robustness,\nstability, efficiency, and cost.\n", "link": "http://arxiv.org/abs/2502.18153v1", "date": "2025-02-25", "relevancy": 2.2972, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.467}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.463}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SASSHA%3A%20Sharpness-aware%20Adaptive%20Second-order%20Optimization%20with%20Stable%0A%20%20Hessian%20Approximation&body=Title%3A%20SASSHA%3A%20Sharpness-aware%20Adaptive%20Second-order%20Optimization%20with%20Stable%0A%20%20Hessian%20Approximation%0AAuthor%3A%20Dahun%20Shin%20and%20Dongyeop%20Lee%20and%20Jinseok%20Chung%20and%20Namhoon%20Lee%0AAbstract%3A%20%20%20Approximate%20second-order%20optimization%20methods%20often%20exhibit%20poorer%0Ageneralization%20compared%20to%20first-order%20approaches.%20In%20this%20work%2C%20we%20look%20into%0Athis%20issue%20through%20the%20lens%20of%20the%20loss%20landscape%20and%20find%20that%20existing%0Asecond-order%20methods%20tend%20to%20converge%20to%20sharper%20minima%20compared%20to%20SGD.%20In%0Aresponse%2C%20we%20propose%20Sassha%2C%20a%20novel%20second-order%20method%20designed%20to%20enhance%0Ageneralization%20by%20explicitly%20reducing%20sharpness%20of%20the%20solution%2C%20while%0Astabilizing%20the%20computation%20of%20approximate%20Hessians%20along%20the%20optimization%0Atrajectory.%20In%20fact%2C%20this%20sharpness%20minimization%20scheme%20is%20crafted%20also%20to%0Aaccommodate%20lazy%20Hessian%20updates%2C%20so%20as%20to%20secure%20efficiency%20besides%20flatness.%0ATo%20validate%20its%20effectiveness%2C%20we%20conduct%20a%20wide%20range%20of%20standard%20deep%0Alearning%20experiments%20where%20Sassha%20demonstrates%20its%20outstanding%20generalization%0Aperformance%20that%20is%20comparable%20to%2C%20and%20mostly%20better%20than%2C%20other%20methods.%20We%0Aprovide%20a%20comprehensive%20set%20of%20analyses%20including%20convergence%2C%20robustness%2C%0Astability%2C%20efficiency%2C%20and%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18153v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSASSHA%253A%2520Sharpness-aware%2520Adaptive%2520Second-order%2520Optimization%2520with%2520Stable%250A%2520%2520Hessian%2520Approximation%26entry.906535625%3DDahun%2520Shin%2520and%2520Dongyeop%2520Lee%2520and%2520Jinseok%2520Chung%2520and%2520Namhoon%2520Lee%26entry.1292438233%3D%2520%2520Approximate%2520second-order%2520optimization%2520methods%2520often%2520exhibit%2520poorer%250Ageneralization%2520compared%2520to%2520first-order%2520approaches.%2520In%2520this%2520work%252C%2520we%2520look%2520into%250Athis%2520issue%2520through%2520the%2520lens%2520of%2520the%2520loss%2520landscape%2520and%2520find%2520that%2520existing%250Asecond-order%2520methods%2520tend%2520to%2520converge%2520to%2520sharper%2520minima%2520compared%2520to%2520SGD.%2520In%250Aresponse%252C%2520we%2520propose%2520Sassha%252C%2520a%2520novel%2520second-order%2520method%2520designed%2520to%2520enhance%250Ageneralization%2520by%2520explicitly%2520reducing%2520sharpness%2520of%2520the%2520solution%252C%2520while%250Astabilizing%2520the%2520computation%2520of%2520approximate%2520Hessians%2520along%2520the%2520optimization%250Atrajectory.%2520In%2520fact%252C%2520this%2520sharpness%2520minimization%2520scheme%2520is%2520crafted%2520also%2520to%250Aaccommodate%2520lazy%2520Hessian%2520updates%252C%2520so%2520as%2520to%2520secure%2520efficiency%2520besides%2520flatness.%250ATo%2520validate%2520its%2520effectiveness%252C%2520we%2520conduct%2520a%2520wide%2520range%2520of%2520standard%2520deep%250Alearning%2520experiments%2520where%2520Sassha%2520demonstrates%2520its%2520outstanding%2520generalization%250Aperformance%2520that%2520is%2520comparable%2520to%252C%2520and%2520mostly%2520better%2520than%252C%2520other%2520methods.%2520We%250Aprovide%2520a%2520comprehensive%2520set%2520of%2520analyses%2520including%2520convergence%252C%2520robustness%252C%250Astability%252C%2520efficiency%252C%2520and%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18153v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SASSHA%3A%20Sharpness-aware%20Adaptive%20Second-order%20Optimization%20with%20Stable%0A%20%20Hessian%20Approximation&entry.906535625=Dahun%20Shin%20and%20Dongyeop%20Lee%20and%20Jinseok%20Chung%20and%20Namhoon%20Lee&entry.1292438233=%20%20Approximate%20second-order%20optimization%20methods%20often%20exhibit%20poorer%0Ageneralization%20compared%20to%20first-order%20approaches.%20In%20this%20work%2C%20we%20look%20into%0Athis%20issue%20through%20the%20lens%20of%20the%20loss%20landscape%20and%20find%20that%20existing%0Asecond-order%20methods%20tend%20to%20converge%20to%20sharper%20minima%20compared%20to%20SGD.%20In%0Aresponse%2C%20we%20propose%20Sassha%2C%20a%20novel%20second-order%20method%20designed%20to%20enhance%0Ageneralization%20by%20explicitly%20reducing%20sharpness%20of%20the%20solution%2C%20while%0Astabilizing%20the%20computation%20of%20approximate%20Hessians%20along%20the%20optimization%0Atrajectory.%20In%20fact%2C%20this%20sharpness%20minimization%20scheme%20is%20crafted%20also%20to%0Aaccommodate%20lazy%20Hessian%20updates%2C%20so%20as%20to%20secure%20efficiency%20besides%20flatness.%0ATo%20validate%20its%20effectiveness%2C%20we%20conduct%20a%20wide%20range%20of%20standard%20deep%0Alearning%20experiments%20where%20Sassha%20demonstrates%20its%20outstanding%20generalization%0Aperformance%20that%20is%20comparable%20to%2C%20and%20mostly%20better%20than%2C%20other%20methods.%20We%0Aprovide%20a%20comprehensive%20set%20of%20analyses%20including%20convergence%2C%20robustness%2C%0Astability%2C%20efficiency%2C%20and%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18153v1&entry.124074799=Read"},
{"title": "Bayesian Optimization for Controlled Image Editing via LLMs", "author": "Chengkun Cai and Haoliang Liu and Xu Zhao and Zhongyu Jiang and Tianfang Zhang and Zongkai Wu and Jenq-Neng Hwang and Serge Belongie and Lei Li", "abstract": "  In the rapidly evolving field of image generation, achieving precise control\nover generated content and maintaining semantic consistency remain significant\nlimitations, particularly concerning grounding techniques and the necessity for\nmodel fine-tuning. To address these challenges, we propose BayesGenie, an\noff-the-shelf approach that integrates Large Language Models (LLMs) with\nBayesian Optimization to facilitate precise and user-friendly image editing.\nOur method enables users to modify images through natural language descriptions\nwithout manual area marking, while preserving the original image's semantic\nintegrity. Unlike existing techniques that require extensive pre-training or\nfine-tuning, our approach demonstrates remarkable adaptability across various\nLLMs through its model-agnostic design. BayesGenie employs an adapted Bayesian\noptimization strategy to automatically refine the inference process parameters,\nachieving high-precision image editing with minimal user intervention. Through\nextensive experiments across diverse scenarios, we demonstrate that our\nframework significantly outperforms existing methods in both editing accuracy\nand semantic preservation, as validated using different LLMs including Claude3\nand GPT-4.\n", "link": "http://arxiv.org/abs/2502.18116v1", "date": "2025-02-25", "relevancy": 2.2861, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5828}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5742}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Optimization%20for%20Controlled%20Image%20Editing%20via%20LLMs&body=Title%3A%20Bayesian%20Optimization%20for%20Controlled%20Image%20Editing%20via%20LLMs%0AAuthor%3A%20Chengkun%20Cai%20and%20Haoliang%20Liu%20and%20Xu%20Zhao%20and%20Zhongyu%20Jiang%20and%20Tianfang%20Zhang%20and%20Zongkai%20Wu%20and%20Jenq-Neng%20Hwang%20and%20Serge%20Belongie%20and%20Lei%20Li%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20field%20of%20image%20generation%2C%20achieving%20precise%20control%0Aover%20generated%20content%20and%20maintaining%20semantic%20consistency%20remain%20significant%0Alimitations%2C%20particularly%20concerning%20grounding%20techniques%20and%20the%20necessity%20for%0Amodel%20fine-tuning.%20To%20address%20these%20challenges%2C%20we%20propose%20BayesGenie%2C%20an%0Aoff-the-shelf%20approach%20that%20integrates%20Large%20Language%20Models%20%28LLMs%29%20with%0ABayesian%20Optimization%20to%20facilitate%20precise%20and%20user-friendly%20image%20editing.%0AOur%20method%20enables%20users%20to%20modify%20images%20through%20natural%20language%20descriptions%0Awithout%20manual%20area%20marking%2C%20while%20preserving%20the%20original%20image%27s%20semantic%0Aintegrity.%20Unlike%20existing%20techniques%20that%20require%20extensive%20pre-training%20or%0Afine-tuning%2C%20our%20approach%20demonstrates%20remarkable%20adaptability%20across%20various%0ALLMs%20through%20its%20model-agnostic%20design.%20BayesGenie%20employs%20an%20adapted%20Bayesian%0Aoptimization%20strategy%20to%20automatically%20refine%20the%20inference%20process%20parameters%2C%0Aachieving%20high-precision%20image%20editing%20with%20minimal%20user%20intervention.%20Through%0Aextensive%20experiments%20across%20diverse%20scenarios%2C%20we%20demonstrate%20that%20our%0Aframework%20significantly%20outperforms%20existing%20methods%20in%20both%20editing%20accuracy%0Aand%20semantic%20preservation%2C%20as%20validated%20using%20different%20LLMs%20including%20Claude3%0Aand%20GPT-4.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18116v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Optimization%2520for%2520Controlled%2520Image%2520Editing%2520via%2520LLMs%26entry.906535625%3DChengkun%2520Cai%2520and%2520Haoliang%2520Liu%2520and%2520Xu%2520Zhao%2520and%2520Zhongyu%2520Jiang%2520and%2520Tianfang%2520Zhang%2520and%2520Zongkai%2520Wu%2520and%2520Jenq-Neng%2520Hwang%2520and%2520Serge%2520Belongie%2520and%2520Lei%2520Li%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520field%2520of%2520image%2520generation%252C%2520achieving%2520precise%2520control%250Aover%2520generated%2520content%2520and%2520maintaining%2520semantic%2520consistency%2520remain%2520significant%250Alimitations%252C%2520particularly%2520concerning%2520grounding%2520techniques%2520and%2520the%2520necessity%2520for%250Amodel%2520fine-tuning.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520BayesGenie%252C%2520an%250Aoff-the-shelf%2520approach%2520that%2520integrates%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%250ABayesian%2520Optimization%2520to%2520facilitate%2520precise%2520and%2520user-friendly%2520image%2520editing.%250AOur%2520method%2520enables%2520users%2520to%2520modify%2520images%2520through%2520natural%2520language%2520descriptions%250Awithout%2520manual%2520area%2520marking%252C%2520while%2520preserving%2520the%2520original%2520image%2527s%2520semantic%250Aintegrity.%2520Unlike%2520existing%2520techniques%2520that%2520require%2520extensive%2520pre-training%2520or%250Afine-tuning%252C%2520our%2520approach%2520demonstrates%2520remarkable%2520adaptability%2520across%2520various%250ALLMs%2520through%2520its%2520model-agnostic%2520design.%2520BayesGenie%2520employs%2520an%2520adapted%2520Bayesian%250Aoptimization%2520strategy%2520to%2520automatically%2520refine%2520the%2520inference%2520process%2520parameters%252C%250Aachieving%2520high-precision%2520image%2520editing%2520with%2520minimal%2520user%2520intervention.%2520Through%250Aextensive%2520experiments%2520across%2520diverse%2520scenarios%252C%2520we%2520demonstrate%2520that%2520our%250Aframework%2520significantly%2520outperforms%2520existing%2520methods%2520in%2520both%2520editing%2520accuracy%250Aand%2520semantic%2520preservation%252C%2520as%2520validated%2520using%2520different%2520LLMs%2520including%2520Claude3%250Aand%2520GPT-4.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18116v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Optimization%20for%20Controlled%20Image%20Editing%20via%20LLMs&entry.906535625=Chengkun%20Cai%20and%20Haoliang%20Liu%20and%20Xu%20Zhao%20and%20Zhongyu%20Jiang%20and%20Tianfang%20Zhang%20and%20Zongkai%20Wu%20and%20Jenq-Neng%20Hwang%20and%20Serge%20Belongie%20and%20Lei%20Li&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20field%20of%20image%20generation%2C%20achieving%20precise%20control%0Aover%20generated%20content%20and%20maintaining%20semantic%20consistency%20remain%20significant%0Alimitations%2C%20particularly%20concerning%20grounding%20techniques%20and%20the%20necessity%20for%0Amodel%20fine-tuning.%20To%20address%20these%20challenges%2C%20we%20propose%20BayesGenie%2C%20an%0Aoff-the-shelf%20approach%20that%20integrates%20Large%20Language%20Models%20%28LLMs%29%20with%0ABayesian%20Optimization%20to%20facilitate%20precise%20and%20user-friendly%20image%20editing.%0AOur%20method%20enables%20users%20to%20modify%20images%20through%20natural%20language%20descriptions%0Awithout%20manual%20area%20marking%2C%20while%20preserving%20the%20original%20image%27s%20semantic%0Aintegrity.%20Unlike%20existing%20techniques%20that%20require%20extensive%20pre-training%20or%0Afine-tuning%2C%20our%20approach%20demonstrates%20remarkable%20adaptability%20across%20various%0ALLMs%20through%20its%20model-agnostic%20design.%20BayesGenie%20employs%20an%20adapted%20Bayesian%0Aoptimization%20strategy%20to%20automatically%20refine%20the%20inference%20process%20parameters%2C%0Aachieving%20high-precision%20image%20editing%20with%20minimal%20user%20intervention.%20Through%0Aextensive%20experiments%20across%20diverse%20scenarios%2C%20we%20demonstrate%20that%20our%0Aframework%20significantly%20outperforms%20existing%20methods%20in%20both%20editing%20accuracy%0Aand%20semantic%20preservation%2C%20as%20validated%20using%20different%20LLMs%20including%20Claude3%0Aand%20GPT-4.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18116v1&entry.124074799=Read"},
{"title": "Dynamic Attention-Guided Context Decoding for Mitigating Context\n  Faithfulness Hallucinations in Large Language Models", "author": "Yanwen Huang and Yong Zhang and Ning Cheng and Zhitao Li and Shaojun Wang and Jing Xiao", "abstract": "  Large language models (LLMs) often exhibit Context Faithfulness\nHallucinations, where outputs deviate from retrieved information due to\nincomplete context integration. Our analysis reveals a strong correlation\nbetween token-level uncertainty and hallucinations. We hypothesize that\nattention mechanisms inherently encode context utilization signals, supported\nby probing analysis. Based on these insights, we propose Dynamic\nAttention-Guided Context Decoding (DAGCD), a lightweight framework that\nleverages attention distributions and uncertainty signals in a single-pass\ndecoding. Experiments on open-book QA datasets demonstrate DAGCD's\neffectiveness, yielding significant improvements in faithfulness and robustness\nwhile preserving computational efficiency.\n", "link": "http://arxiv.org/abs/2501.01059v2", "date": "2025-02-25", "relevancy": 2.2842, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5744}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5744}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Attention-Guided%20Context%20Decoding%20for%20Mitigating%20Context%0A%20%20Faithfulness%20Hallucinations%20in%20Large%20Language%20Models&body=Title%3A%20Dynamic%20Attention-Guided%20Context%20Decoding%20for%20Mitigating%20Context%0A%20%20Faithfulness%20Hallucinations%20in%20Large%20Language%20Models%0AAuthor%3A%20Yanwen%20Huang%20and%20Yong%20Zhang%20and%20Ning%20Cheng%20and%20Zhitao%20Li%20and%20Shaojun%20Wang%20and%20Jing%20Xiao%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20often%20exhibit%20Context%20Faithfulness%0AHallucinations%2C%20where%20outputs%20deviate%20from%20retrieved%20information%20due%20to%0Aincomplete%20context%20integration.%20Our%20analysis%20reveals%20a%20strong%20correlation%0Abetween%20token-level%20uncertainty%20and%20hallucinations.%20We%20hypothesize%20that%0Aattention%20mechanisms%20inherently%20encode%20context%20utilization%20signals%2C%20supported%0Aby%20probing%20analysis.%20Based%20on%20these%20insights%2C%20we%20propose%20Dynamic%0AAttention-Guided%20Context%20Decoding%20%28DAGCD%29%2C%20a%20lightweight%20framework%20that%0Aleverages%20attention%20distributions%20and%20uncertainty%20signals%20in%20a%20single-pass%0Adecoding.%20Experiments%20on%20open-book%20QA%20datasets%20demonstrate%20DAGCD%27s%0Aeffectiveness%2C%20yielding%20significant%20improvements%20in%20faithfulness%20and%20robustness%0Awhile%20preserving%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01059v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Attention-Guided%2520Context%2520Decoding%2520for%2520Mitigating%2520Context%250A%2520%2520Faithfulness%2520Hallucinations%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DYanwen%2520Huang%2520and%2520Yong%2520Zhang%2520and%2520Ning%2520Cheng%2520and%2520Zhitao%2520Li%2520and%2520Shaojun%2520Wang%2520and%2520Jing%2520Xiao%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520often%2520exhibit%2520Context%2520Faithfulness%250AHallucinations%252C%2520where%2520outputs%2520deviate%2520from%2520retrieved%2520information%2520due%2520to%250Aincomplete%2520context%2520integration.%2520Our%2520analysis%2520reveals%2520a%2520strong%2520correlation%250Abetween%2520token-level%2520uncertainty%2520and%2520hallucinations.%2520We%2520hypothesize%2520that%250Aattention%2520mechanisms%2520inherently%2520encode%2520context%2520utilization%2520signals%252C%2520supported%250Aby%2520probing%2520analysis.%2520Based%2520on%2520these%2520insights%252C%2520we%2520propose%2520Dynamic%250AAttention-Guided%2520Context%2520Decoding%2520%2528DAGCD%2529%252C%2520a%2520lightweight%2520framework%2520that%250Aleverages%2520attention%2520distributions%2520and%2520uncertainty%2520signals%2520in%2520a%2520single-pass%250Adecoding.%2520Experiments%2520on%2520open-book%2520QA%2520datasets%2520demonstrate%2520DAGCD%2527s%250Aeffectiveness%252C%2520yielding%2520significant%2520improvements%2520in%2520faithfulness%2520and%2520robustness%250Awhile%2520preserving%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01059v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Attention-Guided%20Context%20Decoding%20for%20Mitigating%20Context%0A%20%20Faithfulness%20Hallucinations%20in%20Large%20Language%20Models&entry.906535625=Yanwen%20Huang%20and%20Yong%20Zhang%20and%20Ning%20Cheng%20and%20Zhitao%20Li%20and%20Shaojun%20Wang%20and%20Jing%20Xiao&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20often%20exhibit%20Context%20Faithfulness%0AHallucinations%2C%20where%20outputs%20deviate%20from%20retrieved%20information%20due%20to%0Aincomplete%20context%20integration.%20Our%20analysis%20reveals%20a%20strong%20correlation%0Abetween%20token-level%20uncertainty%20and%20hallucinations.%20We%20hypothesize%20that%0Aattention%20mechanisms%20inherently%20encode%20context%20utilization%20signals%2C%20supported%0Aby%20probing%20analysis.%20Based%20on%20these%20insights%2C%20we%20propose%20Dynamic%0AAttention-Guided%20Context%20Decoding%20%28DAGCD%29%2C%20a%20lightweight%20framework%20that%0Aleverages%20attention%20distributions%20and%20uncertainty%20signals%20in%20a%20single-pass%0Adecoding.%20Experiments%20on%20open-book%20QA%20datasets%20demonstrate%20DAGCD%27s%0Aeffectiveness%2C%20yielding%20significant%20improvements%20in%20faithfulness%20and%20robustness%0Awhile%20preserving%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01059v2&entry.124074799=Read"},
{"title": "Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning", "author": "Zenan Li and Zhaoyu Li and Wen Tang and Xian Zhang and Yuan Yao and Xujie Si and Fan Yang and Kaiyu Yang and Xiaoxing Ma", "abstract": "  Large language models (LLMs) can prove mathematical theorems formally by\ngenerating proof steps (\\textit{a.k.a.} tactics) within a proof system.\nHowever, the space of possible tactics is vast and complex, while the available\ntraining data for formal proofs is limited, posing a significant challenge to\nLLM-based tactic generation. To address this, we introduce a neuro-symbolic\ntactic generator that synergizes the mathematical intuition learned by LLMs\nwith domain-specific insights encoded by symbolic methods. The key aspect of\nthis integration is identifying which parts of mathematical reasoning are best\nsuited to LLMs and which to symbolic methods. While the high-level idea of\nneuro-symbolic integration is broadly applicable to various mathematical\nproblems, in this paper, we focus specifically on Olympiad inequalities\n(Figure~1). We analyze how humans solve these problems and distill the\ntechniques into two types of tactics: (1) scaling, handled by symbolic methods,\nand (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with\nLLMs to prune and rank the proof goals for efficient proof search. We evaluate\nour framework on 161 challenging inequalities from multiple mathematics\ncompetitions, achieving state-of-the-art performance and significantly\noutperforming existing LLM and symbolic approaches without requiring additional\ntraining data.\n", "link": "http://arxiv.org/abs/2502.13834v2", "date": "2025-02-25", "relevancy": 2.2839, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4649}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4649}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proving%20Olympiad%20Inequalities%20by%20Synergizing%20LLMs%20and%20Symbolic%20Reasoning&body=Title%3A%20Proving%20Olympiad%20Inequalities%20by%20Synergizing%20LLMs%20and%20Symbolic%20Reasoning%0AAuthor%3A%20Zenan%20Li%20and%20Zhaoyu%20Li%20and%20Wen%20Tang%20and%20Xian%20Zhang%20and%20Yuan%20Yao%20and%20Xujie%20Si%20and%20Fan%20Yang%20and%20Kaiyu%20Yang%20and%20Xiaoxing%20Ma%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20can%20prove%20mathematical%20theorems%20formally%20by%0Agenerating%20proof%20steps%20%28%5Ctextit%7Ba.k.a.%7D%20tactics%29%20within%20a%20proof%20system.%0AHowever%2C%20the%20space%20of%20possible%20tactics%20is%20vast%20and%20complex%2C%20while%20the%20available%0Atraining%20data%20for%20formal%20proofs%20is%20limited%2C%20posing%20a%20significant%20challenge%20to%0ALLM-based%20tactic%20generation.%20To%20address%20this%2C%20we%20introduce%20a%20neuro-symbolic%0Atactic%20generator%20that%20synergizes%20the%20mathematical%20intuition%20learned%20by%20LLMs%0Awith%20domain-specific%20insights%20encoded%20by%20symbolic%20methods.%20The%20key%20aspect%20of%0Athis%20integration%20is%20identifying%20which%20parts%20of%20mathematical%20reasoning%20are%20best%0Asuited%20to%20LLMs%20and%20which%20to%20symbolic%20methods.%20While%20the%20high-level%20idea%20of%0Aneuro-symbolic%20integration%20is%20broadly%20applicable%20to%20various%20mathematical%0Aproblems%2C%20in%20this%20paper%2C%20we%20focus%20specifically%20on%20Olympiad%20inequalities%0A%28Figure~1%29.%20We%20analyze%20how%20humans%20solve%20these%20problems%20and%20distill%20the%0Atechniques%20into%20two%20types%20of%20tactics%3A%20%281%29%20scaling%2C%20handled%20by%20symbolic%20methods%2C%0Aand%20%282%29%20rewriting%2C%20handled%20by%20LLMs.%20In%20addition%2C%20we%20combine%20symbolic%20tools%20with%0ALLMs%20to%20prune%20and%20rank%20the%20proof%20goals%20for%20efficient%20proof%20search.%20We%20evaluate%0Aour%20framework%20on%20161%20challenging%20inequalities%20from%20multiple%20mathematics%0Acompetitions%2C%20achieving%20state-of-the-art%20performance%20and%20significantly%0Aoutperforming%20existing%20LLM%20and%20symbolic%20approaches%20without%20requiring%20additional%0Atraining%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13834v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProving%2520Olympiad%2520Inequalities%2520by%2520Synergizing%2520LLMs%2520and%2520Symbolic%2520Reasoning%26entry.906535625%3DZenan%2520Li%2520and%2520Zhaoyu%2520Li%2520and%2520Wen%2520Tang%2520and%2520Xian%2520Zhang%2520and%2520Yuan%2520Yao%2520and%2520Xujie%2520Si%2520and%2520Fan%2520Yang%2520and%2520Kaiyu%2520Yang%2520and%2520Xiaoxing%2520Ma%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520can%2520prove%2520mathematical%2520theorems%2520formally%2520by%250Agenerating%2520proof%2520steps%2520%2528%255Ctextit%257Ba.k.a.%257D%2520tactics%2529%2520within%2520a%2520proof%2520system.%250AHowever%252C%2520the%2520space%2520of%2520possible%2520tactics%2520is%2520vast%2520and%2520complex%252C%2520while%2520the%2520available%250Atraining%2520data%2520for%2520formal%2520proofs%2520is%2520limited%252C%2520posing%2520a%2520significant%2520challenge%2520to%250ALLM-based%2520tactic%2520generation.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520neuro-symbolic%250Atactic%2520generator%2520that%2520synergizes%2520the%2520mathematical%2520intuition%2520learned%2520by%2520LLMs%250Awith%2520domain-specific%2520insights%2520encoded%2520by%2520symbolic%2520methods.%2520The%2520key%2520aspect%2520of%250Athis%2520integration%2520is%2520identifying%2520which%2520parts%2520of%2520mathematical%2520reasoning%2520are%2520best%250Asuited%2520to%2520LLMs%2520and%2520which%2520to%2520symbolic%2520methods.%2520While%2520the%2520high-level%2520idea%2520of%250Aneuro-symbolic%2520integration%2520is%2520broadly%2520applicable%2520to%2520various%2520mathematical%250Aproblems%252C%2520in%2520this%2520paper%252C%2520we%2520focus%2520specifically%2520on%2520Olympiad%2520inequalities%250A%2528Figure~1%2529.%2520We%2520analyze%2520how%2520humans%2520solve%2520these%2520problems%2520and%2520distill%2520the%250Atechniques%2520into%2520two%2520types%2520of%2520tactics%253A%2520%25281%2529%2520scaling%252C%2520handled%2520by%2520symbolic%2520methods%252C%250Aand%2520%25282%2529%2520rewriting%252C%2520handled%2520by%2520LLMs.%2520In%2520addition%252C%2520we%2520combine%2520symbolic%2520tools%2520with%250ALLMs%2520to%2520prune%2520and%2520rank%2520the%2520proof%2520goals%2520for%2520efficient%2520proof%2520search.%2520We%2520evaluate%250Aour%2520framework%2520on%2520161%2520challenging%2520inequalities%2520from%2520multiple%2520mathematics%250Acompetitions%252C%2520achieving%2520state-of-the-art%2520performance%2520and%2520significantly%250Aoutperforming%2520existing%2520LLM%2520and%2520symbolic%2520approaches%2520without%2520requiring%2520additional%250Atraining%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13834v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proving%20Olympiad%20Inequalities%20by%20Synergizing%20LLMs%20and%20Symbolic%20Reasoning&entry.906535625=Zenan%20Li%20and%20Zhaoyu%20Li%20and%20Wen%20Tang%20and%20Xian%20Zhang%20and%20Yuan%20Yao%20and%20Xujie%20Si%20and%20Fan%20Yang%20and%20Kaiyu%20Yang%20and%20Xiaoxing%20Ma&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20can%20prove%20mathematical%20theorems%20formally%20by%0Agenerating%20proof%20steps%20%28%5Ctextit%7Ba.k.a.%7D%20tactics%29%20within%20a%20proof%20system.%0AHowever%2C%20the%20space%20of%20possible%20tactics%20is%20vast%20and%20complex%2C%20while%20the%20available%0Atraining%20data%20for%20formal%20proofs%20is%20limited%2C%20posing%20a%20significant%20challenge%20to%0ALLM-based%20tactic%20generation.%20To%20address%20this%2C%20we%20introduce%20a%20neuro-symbolic%0Atactic%20generator%20that%20synergizes%20the%20mathematical%20intuition%20learned%20by%20LLMs%0Awith%20domain-specific%20insights%20encoded%20by%20symbolic%20methods.%20The%20key%20aspect%20of%0Athis%20integration%20is%20identifying%20which%20parts%20of%20mathematical%20reasoning%20are%20best%0Asuited%20to%20LLMs%20and%20which%20to%20symbolic%20methods.%20While%20the%20high-level%20idea%20of%0Aneuro-symbolic%20integration%20is%20broadly%20applicable%20to%20various%20mathematical%0Aproblems%2C%20in%20this%20paper%2C%20we%20focus%20specifically%20on%20Olympiad%20inequalities%0A%28Figure~1%29.%20We%20analyze%20how%20humans%20solve%20these%20problems%20and%20distill%20the%0Atechniques%20into%20two%20types%20of%20tactics%3A%20%281%29%20scaling%2C%20handled%20by%20symbolic%20methods%2C%0Aand%20%282%29%20rewriting%2C%20handled%20by%20LLMs.%20In%20addition%2C%20we%20combine%20symbolic%20tools%20with%0ALLMs%20to%20prune%20and%20rank%20the%20proof%20goals%20for%20efficient%20proof%20search.%20We%20evaluate%0Aour%20framework%20on%20161%20challenging%20inequalities%20from%20multiple%20mathematics%0Acompetitions%2C%20achieving%20state-of-the-art%20performance%20and%20significantly%0Aoutperforming%20existing%20LLM%20and%20symbolic%20approaches%20without%20requiring%20additional%0Atraining%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13834v2&entry.124074799=Read"},
{"title": "D-STGCNT: A Dense Spatio-Temporal Graph Conv-GRU Network based on\n  transformer for assessment of patient physical rehabilitation", "author": "Youssef Mourchid and Rim Slama", "abstract": "  This paper tackles the challenge of automatically assessing physical\nrehabilitation exercises for patients who perform the exercises without\nclinician supervision. The objective is to provide a quality score to ensure\ncorrect performance and achieve desired results. To achieve this goal, a new\ngraph-based model, the Dense Spatio-Temporal Graph Conv-GRU Network with\nTransformer, is introduced. This model combines a modified version of STGCN and\ntransformer architectures for efficient handling of spatio-temporal data. The\nkey idea is to consider skeleton data respecting its non-linear structure as a\ngraph and detecting joints playing the main role in each rehabilitation\nexercise. Dense connections and GRU mechanisms are used to rapidly process\nlarge 3D skeleton inputs and effectively model temporal dynamics. The\ntransformer encoder's attention mechanism focuses on relevant parts of the\ninput sequence, making it useful for evaluating rehabilitation exercises. The\nevaluation of our proposed approach on the KIMORE and UI-PRMD datasets\nhighlighted its potential, surpassing state-of-the-art methods in terms of\naccuracy and computational time. This resulted in faster and more accurate\nlearning and assessment of rehabilitation exercises. Additionally, our model\nprovides valuable feedback through qualitative illustrations, effectively\nhighlighting the significance of joints in specific exercises.\n", "link": "http://arxiv.org/abs/2401.06150v2", "date": "2025-02-25", "relevancy": 2.2782, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5863}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5627}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D-STGCNT%3A%20A%20Dense%20Spatio-Temporal%20Graph%20Conv-GRU%20Network%20based%20on%0A%20%20transformer%20for%20assessment%20of%20patient%20physical%20rehabilitation&body=Title%3A%20D-STGCNT%3A%20A%20Dense%20Spatio-Temporal%20Graph%20Conv-GRU%20Network%20based%20on%0A%20%20transformer%20for%20assessment%20of%20patient%20physical%20rehabilitation%0AAuthor%3A%20Youssef%20Mourchid%20and%20Rim%20Slama%0AAbstract%3A%20%20%20This%20paper%20tackles%20the%20challenge%20of%20automatically%20assessing%20physical%0Arehabilitation%20exercises%20for%20patients%20who%20perform%20the%20exercises%20without%0Aclinician%20supervision.%20The%20objective%20is%20to%20provide%20a%20quality%20score%20to%20ensure%0Acorrect%20performance%20and%20achieve%20desired%20results.%20To%20achieve%20this%20goal%2C%20a%20new%0Agraph-based%20model%2C%20the%20Dense%20Spatio-Temporal%20Graph%20Conv-GRU%20Network%20with%0ATransformer%2C%20is%20introduced.%20This%20model%20combines%20a%20modified%20version%20of%20STGCN%20and%0Atransformer%20architectures%20for%20efficient%20handling%20of%20spatio-temporal%20data.%20The%0Akey%20idea%20is%20to%20consider%20skeleton%20data%20respecting%20its%20non-linear%20structure%20as%20a%0Agraph%20and%20detecting%20joints%20playing%20the%20main%20role%20in%20each%20rehabilitation%0Aexercise.%20Dense%20connections%20and%20GRU%20mechanisms%20are%20used%20to%20rapidly%20process%0Alarge%203D%20skeleton%20inputs%20and%20effectively%20model%20temporal%20dynamics.%20The%0Atransformer%20encoder%27s%20attention%20mechanism%20focuses%20on%20relevant%20parts%20of%20the%0Ainput%20sequence%2C%20making%20it%20useful%20for%20evaluating%20rehabilitation%20exercises.%20The%0Aevaluation%20of%20our%20proposed%20approach%20on%20the%20KIMORE%20and%20UI-PRMD%20datasets%0Ahighlighted%20its%20potential%2C%20surpassing%20state-of-the-art%20methods%20in%20terms%20of%0Aaccuracy%20and%20computational%20time.%20This%20resulted%20in%20faster%20and%20more%20accurate%0Alearning%20and%20assessment%20of%20rehabilitation%20exercises.%20Additionally%2C%20our%20model%0Aprovides%20valuable%20feedback%20through%20qualitative%20illustrations%2C%20effectively%0Ahighlighting%20the%20significance%20of%20joints%20in%20specific%20exercises.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.06150v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD-STGCNT%253A%2520A%2520Dense%2520Spatio-Temporal%2520Graph%2520Conv-GRU%2520Network%2520based%2520on%250A%2520%2520transformer%2520for%2520assessment%2520of%2520patient%2520physical%2520rehabilitation%26entry.906535625%3DYoussef%2520Mourchid%2520and%2520Rim%2520Slama%26entry.1292438233%3D%2520%2520This%2520paper%2520tackles%2520the%2520challenge%2520of%2520automatically%2520assessing%2520physical%250Arehabilitation%2520exercises%2520for%2520patients%2520who%2520perform%2520the%2520exercises%2520without%250Aclinician%2520supervision.%2520The%2520objective%2520is%2520to%2520provide%2520a%2520quality%2520score%2520to%2520ensure%250Acorrect%2520performance%2520and%2520achieve%2520desired%2520results.%2520To%2520achieve%2520this%2520goal%252C%2520a%2520new%250Agraph-based%2520model%252C%2520the%2520Dense%2520Spatio-Temporal%2520Graph%2520Conv-GRU%2520Network%2520with%250ATransformer%252C%2520is%2520introduced.%2520This%2520model%2520combines%2520a%2520modified%2520version%2520of%2520STGCN%2520and%250Atransformer%2520architectures%2520for%2520efficient%2520handling%2520of%2520spatio-temporal%2520data.%2520The%250Akey%2520idea%2520is%2520to%2520consider%2520skeleton%2520data%2520respecting%2520its%2520non-linear%2520structure%2520as%2520a%250Agraph%2520and%2520detecting%2520joints%2520playing%2520the%2520main%2520role%2520in%2520each%2520rehabilitation%250Aexercise.%2520Dense%2520connections%2520and%2520GRU%2520mechanisms%2520are%2520used%2520to%2520rapidly%2520process%250Alarge%25203D%2520skeleton%2520inputs%2520and%2520effectively%2520model%2520temporal%2520dynamics.%2520The%250Atransformer%2520encoder%2527s%2520attention%2520mechanism%2520focuses%2520on%2520relevant%2520parts%2520of%2520the%250Ainput%2520sequence%252C%2520making%2520it%2520useful%2520for%2520evaluating%2520rehabilitation%2520exercises.%2520The%250Aevaluation%2520of%2520our%2520proposed%2520approach%2520on%2520the%2520KIMORE%2520and%2520UI-PRMD%2520datasets%250Ahighlighted%2520its%2520potential%252C%2520surpassing%2520state-of-the-art%2520methods%2520in%2520terms%2520of%250Aaccuracy%2520and%2520computational%2520time.%2520This%2520resulted%2520in%2520faster%2520and%2520more%2520accurate%250Alearning%2520and%2520assessment%2520of%2520rehabilitation%2520exercises.%2520Additionally%252C%2520our%2520model%250Aprovides%2520valuable%2520feedback%2520through%2520qualitative%2520illustrations%252C%2520effectively%250Ahighlighting%2520the%2520significance%2520of%2520joints%2520in%2520specific%2520exercises.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.06150v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D-STGCNT%3A%20A%20Dense%20Spatio-Temporal%20Graph%20Conv-GRU%20Network%20based%20on%0A%20%20transformer%20for%20assessment%20of%20patient%20physical%20rehabilitation&entry.906535625=Youssef%20Mourchid%20and%20Rim%20Slama&entry.1292438233=%20%20This%20paper%20tackles%20the%20challenge%20of%20automatically%20assessing%20physical%0Arehabilitation%20exercises%20for%20patients%20who%20perform%20the%20exercises%20without%0Aclinician%20supervision.%20The%20objective%20is%20to%20provide%20a%20quality%20score%20to%20ensure%0Acorrect%20performance%20and%20achieve%20desired%20results.%20To%20achieve%20this%20goal%2C%20a%20new%0Agraph-based%20model%2C%20the%20Dense%20Spatio-Temporal%20Graph%20Conv-GRU%20Network%20with%0ATransformer%2C%20is%20introduced.%20This%20model%20combines%20a%20modified%20version%20of%20STGCN%20and%0Atransformer%20architectures%20for%20efficient%20handling%20of%20spatio-temporal%20data.%20The%0Akey%20idea%20is%20to%20consider%20skeleton%20data%20respecting%20its%20non-linear%20structure%20as%20a%0Agraph%20and%20detecting%20joints%20playing%20the%20main%20role%20in%20each%20rehabilitation%0Aexercise.%20Dense%20connections%20and%20GRU%20mechanisms%20are%20used%20to%20rapidly%20process%0Alarge%203D%20skeleton%20inputs%20and%20effectively%20model%20temporal%20dynamics.%20The%0Atransformer%20encoder%27s%20attention%20mechanism%20focuses%20on%20relevant%20parts%20of%20the%0Ainput%20sequence%2C%20making%20it%20useful%20for%20evaluating%20rehabilitation%20exercises.%20The%0Aevaluation%20of%20our%20proposed%20approach%20on%20the%20KIMORE%20and%20UI-PRMD%20datasets%0Ahighlighted%20its%20potential%2C%20surpassing%20state-of-the-art%20methods%20in%20terms%20of%0Aaccuracy%20and%20computational%20time.%20This%20resulted%20in%20faster%20and%20more%20accurate%0Alearning%20and%20assessment%20of%20rehabilitation%20exercises.%20Additionally%2C%20our%20model%0Aprovides%20valuable%20feedback%20through%20qualitative%20illustrations%2C%20effectively%0Ahighlighting%20the%20significance%20of%20joints%20in%20specific%20exercises.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06150v2&entry.124074799=Read"},
{"title": "Stackelberg Game Preference Optimization for Data-Efficient Alignment of\n  Language Models", "author": "Xu Chu and Zhixin Zhang and Tianyu Jia and Yujie Jin", "abstract": "  Aligning language models with human preferences is critical for real-world\ndeployment, but existing methods often require large amounts of high-quality\nhuman annotations. Aiming at a data-efficient alignment method, we propose\nStackelberg Game Preference Optimization (SGPO), a framework that models\nalignment as a two-player Stackelberg game, where a policy (leader) optimizes\nagainst a worst-case preference distribution (follower) within an\n$\\epsilon$-Wasserstein ball, ensuring robustness to (self-)annotation noise and\ndistribution shifts. SGPO guarantees $O(\\epsilon)$-bounded regret, unlike\nDirect Preference Optimization (DPO), which suffers from linear regret growth\nin the distribution mismatch. We instantiate SGPO with the Stackelberg\nSelf-Annotated Preference Optimization (SSAPO) algorithm, which iteratively\nself-annotates preferences and adversarially reweights synthetic annotated\npreferences. Using only 2K seed preferences, from the UltraFeedback dataset,\ni.e., 1/30 of human labels in the dataset, our method achieves 35.82% GPT-4\nwin-rate with Mistral-7B and 40.12% with Llama3-8B-Instruct within three rounds\nof SSAPO.\n", "link": "http://arxiv.org/abs/2502.18099v1", "date": "2025-02-25", "relevancy": 2.2688, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4558}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4535}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stackelberg%20Game%20Preference%20Optimization%20for%20Data-Efficient%20Alignment%20of%0A%20%20Language%20Models&body=Title%3A%20Stackelberg%20Game%20Preference%20Optimization%20for%20Data-Efficient%20Alignment%20of%0A%20%20Language%20Models%0AAuthor%3A%20Xu%20Chu%20and%20Zhixin%20Zhang%20and%20Tianyu%20Jia%20and%20Yujie%20Jin%0AAbstract%3A%20%20%20Aligning%20language%20models%20with%20human%20preferences%20is%20critical%20for%20real-world%0Adeployment%2C%20but%20existing%20methods%20often%20require%20large%20amounts%20of%20high-quality%0Ahuman%20annotations.%20Aiming%20at%20a%20data-efficient%20alignment%20method%2C%20we%20propose%0AStackelberg%20Game%20Preference%20Optimization%20%28SGPO%29%2C%20a%20framework%20that%20models%0Aalignment%20as%20a%20two-player%20Stackelberg%20game%2C%20where%20a%20policy%20%28leader%29%20optimizes%0Aagainst%20a%20worst-case%20preference%20distribution%20%28follower%29%20within%20an%0A%24%5Cepsilon%24-Wasserstein%20ball%2C%20ensuring%20robustness%20to%20%28self-%29annotation%20noise%20and%0Adistribution%20shifts.%20SGPO%20guarantees%20%24O%28%5Cepsilon%29%24-bounded%20regret%2C%20unlike%0ADirect%20Preference%20Optimization%20%28DPO%29%2C%20which%20suffers%20from%20linear%20regret%20growth%0Ain%20the%20distribution%20mismatch.%20We%20instantiate%20SGPO%20with%20the%20Stackelberg%0ASelf-Annotated%20Preference%20Optimization%20%28SSAPO%29%20algorithm%2C%20which%20iteratively%0Aself-annotates%20preferences%20and%20adversarially%20reweights%20synthetic%20annotated%0Apreferences.%20Using%20only%202K%20seed%20preferences%2C%20from%20the%20UltraFeedback%20dataset%2C%0Ai.e.%2C%201/30%20of%20human%20labels%20in%20the%20dataset%2C%20our%20method%20achieves%2035.82%25%20GPT-4%0Awin-rate%20with%20Mistral-7B%20and%2040.12%25%20with%20Llama3-8B-Instruct%20within%20three%20rounds%0Aof%20SSAPO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18099v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStackelberg%2520Game%2520Preference%2520Optimization%2520for%2520Data-Efficient%2520Alignment%2520of%250A%2520%2520Language%2520Models%26entry.906535625%3DXu%2520Chu%2520and%2520Zhixin%2520Zhang%2520and%2520Tianyu%2520Jia%2520and%2520Yujie%2520Jin%26entry.1292438233%3D%2520%2520Aligning%2520language%2520models%2520with%2520human%2520preferences%2520is%2520critical%2520for%2520real-world%250Adeployment%252C%2520but%2520existing%2520methods%2520often%2520require%2520large%2520amounts%2520of%2520high-quality%250Ahuman%2520annotations.%2520Aiming%2520at%2520a%2520data-efficient%2520alignment%2520method%252C%2520we%2520propose%250AStackelberg%2520Game%2520Preference%2520Optimization%2520%2528SGPO%2529%252C%2520a%2520framework%2520that%2520models%250Aalignment%2520as%2520a%2520two-player%2520Stackelberg%2520game%252C%2520where%2520a%2520policy%2520%2528leader%2529%2520optimizes%250Aagainst%2520a%2520worst-case%2520preference%2520distribution%2520%2528follower%2529%2520within%2520an%250A%2524%255Cepsilon%2524-Wasserstein%2520ball%252C%2520ensuring%2520robustness%2520to%2520%2528self-%2529annotation%2520noise%2520and%250Adistribution%2520shifts.%2520SGPO%2520guarantees%2520%2524O%2528%255Cepsilon%2529%2524-bounded%2520regret%252C%2520unlike%250ADirect%2520Preference%2520Optimization%2520%2528DPO%2529%252C%2520which%2520suffers%2520from%2520linear%2520regret%2520growth%250Ain%2520the%2520distribution%2520mismatch.%2520We%2520instantiate%2520SGPO%2520with%2520the%2520Stackelberg%250ASelf-Annotated%2520Preference%2520Optimization%2520%2528SSAPO%2529%2520algorithm%252C%2520which%2520iteratively%250Aself-annotates%2520preferences%2520and%2520adversarially%2520reweights%2520synthetic%2520annotated%250Apreferences.%2520Using%2520only%25202K%2520seed%2520preferences%252C%2520from%2520the%2520UltraFeedback%2520dataset%252C%250Ai.e.%252C%25201/30%2520of%2520human%2520labels%2520in%2520the%2520dataset%252C%2520our%2520method%2520achieves%252035.82%2525%2520GPT-4%250Awin-rate%2520with%2520Mistral-7B%2520and%252040.12%2525%2520with%2520Llama3-8B-Instruct%2520within%2520three%2520rounds%250Aof%2520SSAPO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18099v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stackelberg%20Game%20Preference%20Optimization%20for%20Data-Efficient%20Alignment%20of%0A%20%20Language%20Models&entry.906535625=Xu%20Chu%20and%20Zhixin%20Zhang%20and%20Tianyu%20Jia%20and%20Yujie%20Jin&entry.1292438233=%20%20Aligning%20language%20models%20with%20human%20preferences%20is%20critical%20for%20real-world%0Adeployment%2C%20but%20existing%20methods%20often%20require%20large%20amounts%20of%20high-quality%0Ahuman%20annotations.%20Aiming%20at%20a%20data-efficient%20alignment%20method%2C%20we%20propose%0AStackelberg%20Game%20Preference%20Optimization%20%28SGPO%29%2C%20a%20framework%20that%20models%0Aalignment%20as%20a%20two-player%20Stackelberg%20game%2C%20where%20a%20policy%20%28leader%29%20optimizes%0Aagainst%20a%20worst-case%20preference%20distribution%20%28follower%29%20within%20an%0A%24%5Cepsilon%24-Wasserstein%20ball%2C%20ensuring%20robustness%20to%20%28self-%29annotation%20noise%20and%0Adistribution%20shifts.%20SGPO%20guarantees%20%24O%28%5Cepsilon%29%24-bounded%20regret%2C%20unlike%0ADirect%20Preference%20Optimization%20%28DPO%29%2C%20which%20suffers%20from%20linear%20regret%20growth%0Ain%20the%20distribution%20mismatch.%20We%20instantiate%20SGPO%20with%20the%20Stackelberg%0ASelf-Annotated%20Preference%20Optimization%20%28SSAPO%29%20algorithm%2C%20which%20iteratively%0Aself-annotates%20preferences%20and%20adversarially%20reweights%20synthetic%20annotated%0Apreferences.%20Using%20only%202K%20seed%20preferences%2C%20from%20the%20UltraFeedback%20dataset%2C%0Ai.e.%2C%201/30%20of%20human%20labels%20in%20the%20dataset%2C%20our%20method%20achieves%2035.82%25%20GPT-4%0Awin-rate%20with%20Mistral-7B%20and%2040.12%25%20with%20Llama3-8B-Instruct%20within%20three%20rounds%0Aof%20SSAPO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18099v1&entry.124074799=Read"},
{"title": "UASTrack: A Unified Adaptive Selection Framework with\n  Modality-Customization in Single Object Tracking", "author": "He Wang and Tianyang Xu and Zhangyong Tang and Xiao-Jun Wu and Josef Kittler", "abstract": "  Multi-modal tracking is essential in single-object tracking (SOT), as\ndifferent sensor types contribute unique capabilities to overcome challenges\ncaused by variations in object appearance. However, existing unified RGB-X\ntrackers (X represents depth, event, or thermal modality) either rely on the\ntask-specific training strategy for individual RGB-X image pairs or fail to\naddress the critical importance of modality-adaptive perception in real-world\napplications. In this work, we propose UASTrack, a unified adaptive selection\nframework that facilitates both model and parameter unification, as well as\nadaptive modality discrimination across various multi-modal tracking tasks. To\nachieve modality-adaptive perception in joint RGB-X pairs, we design a\nDiscriminative Auto-Selector (DAS) capable of identifying modality labels,\nthereby distinguishing the data distributions of auxiliary modalities.\nFurthermore, we propose a Task-Customized Optimization Adapter (TCOA) tailored\nto various modalities in the latent space. This strategy effectively filters\nnoise redundancy and mitigates background interference based on the specific\ncharacteristics of each modality. Extensive comparisons conducted on five\nbenchmarks including LasHeR, GTOT, RGBT234, VisEvent, and DepthTrack, covering\nRGB-T, RGB-E, and RGB-D tracking scenarios, demonstrate our innovative approach\nachieves comparative performance by introducing only additional training\nparameters of 1.87M and flops of 1.95G. The code will be available at\nhttps://github.com/wanghe/UASTrack.\n", "link": "http://arxiv.org/abs/2502.18220v1", "date": "2025-02-25", "relevancy": 2.2643, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5845}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5583}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UASTrack%3A%20A%20Unified%20Adaptive%20Selection%20Framework%20with%0A%20%20Modality-Customization%20in%20Single%20Object%20Tracking&body=Title%3A%20UASTrack%3A%20A%20Unified%20Adaptive%20Selection%20Framework%20with%0A%20%20Modality-Customization%20in%20Single%20Object%20Tracking%0AAuthor%3A%20He%20Wang%20and%20Tianyang%20Xu%20and%20Zhangyong%20Tang%20and%20Xiao-Jun%20Wu%20and%20Josef%20Kittler%0AAbstract%3A%20%20%20Multi-modal%20tracking%20is%20essential%20in%20single-object%20tracking%20%28SOT%29%2C%20as%0Adifferent%20sensor%20types%20contribute%20unique%20capabilities%20to%20overcome%20challenges%0Acaused%20by%20variations%20in%20object%20appearance.%20However%2C%20existing%20unified%20RGB-X%0Atrackers%20%28X%20represents%20depth%2C%20event%2C%20or%20thermal%20modality%29%20either%20rely%20on%20the%0Atask-specific%20training%20strategy%20for%20individual%20RGB-X%20image%20pairs%20or%20fail%20to%0Aaddress%20the%20critical%20importance%20of%20modality-adaptive%20perception%20in%20real-world%0Aapplications.%20In%20this%20work%2C%20we%20propose%20UASTrack%2C%20a%20unified%20adaptive%20selection%0Aframework%20that%20facilitates%20both%20model%20and%20parameter%20unification%2C%20as%20well%20as%0Aadaptive%20modality%20discrimination%20across%20various%20multi-modal%20tracking%20tasks.%20To%0Aachieve%20modality-adaptive%20perception%20in%20joint%20RGB-X%20pairs%2C%20we%20design%20a%0ADiscriminative%20Auto-Selector%20%28DAS%29%20capable%20of%20identifying%20modality%20labels%2C%0Athereby%20distinguishing%20the%20data%20distributions%20of%20auxiliary%20modalities.%0AFurthermore%2C%20we%20propose%20a%20Task-Customized%20Optimization%20Adapter%20%28TCOA%29%20tailored%0Ato%20various%20modalities%20in%20the%20latent%20space.%20This%20strategy%20effectively%20filters%0Anoise%20redundancy%20and%20mitigates%20background%20interference%20based%20on%20the%20specific%0Acharacteristics%20of%20each%20modality.%20Extensive%20comparisons%20conducted%20on%20five%0Abenchmarks%20including%20LasHeR%2C%20GTOT%2C%20RGBT234%2C%20VisEvent%2C%20and%20DepthTrack%2C%20covering%0ARGB-T%2C%20RGB-E%2C%20and%20RGB-D%20tracking%20scenarios%2C%20demonstrate%20our%20innovative%20approach%0Aachieves%20comparative%20performance%20by%20introducing%20only%20additional%20training%0Aparameters%20of%201.87M%20and%20flops%20of%201.95G.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/wanghe/UASTrack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUASTrack%253A%2520A%2520Unified%2520Adaptive%2520Selection%2520Framework%2520with%250A%2520%2520Modality-Customization%2520in%2520Single%2520Object%2520Tracking%26entry.906535625%3DHe%2520Wang%2520and%2520Tianyang%2520Xu%2520and%2520Zhangyong%2520Tang%2520and%2520Xiao-Jun%2520Wu%2520and%2520Josef%2520Kittler%26entry.1292438233%3D%2520%2520Multi-modal%2520tracking%2520is%2520essential%2520in%2520single-object%2520tracking%2520%2528SOT%2529%252C%2520as%250Adifferent%2520sensor%2520types%2520contribute%2520unique%2520capabilities%2520to%2520overcome%2520challenges%250Acaused%2520by%2520variations%2520in%2520object%2520appearance.%2520However%252C%2520existing%2520unified%2520RGB-X%250Atrackers%2520%2528X%2520represents%2520depth%252C%2520event%252C%2520or%2520thermal%2520modality%2529%2520either%2520rely%2520on%2520the%250Atask-specific%2520training%2520strategy%2520for%2520individual%2520RGB-X%2520image%2520pairs%2520or%2520fail%2520to%250Aaddress%2520the%2520critical%2520importance%2520of%2520modality-adaptive%2520perception%2520in%2520real-world%250Aapplications.%2520In%2520this%2520work%252C%2520we%2520propose%2520UASTrack%252C%2520a%2520unified%2520adaptive%2520selection%250Aframework%2520that%2520facilitates%2520both%2520model%2520and%2520parameter%2520unification%252C%2520as%2520well%2520as%250Aadaptive%2520modality%2520discrimination%2520across%2520various%2520multi-modal%2520tracking%2520tasks.%2520To%250Aachieve%2520modality-adaptive%2520perception%2520in%2520joint%2520RGB-X%2520pairs%252C%2520we%2520design%2520a%250ADiscriminative%2520Auto-Selector%2520%2528DAS%2529%2520capable%2520of%2520identifying%2520modality%2520labels%252C%250Athereby%2520distinguishing%2520the%2520data%2520distributions%2520of%2520auxiliary%2520modalities.%250AFurthermore%252C%2520we%2520propose%2520a%2520Task-Customized%2520Optimization%2520Adapter%2520%2528TCOA%2529%2520tailored%250Ato%2520various%2520modalities%2520in%2520the%2520latent%2520space.%2520This%2520strategy%2520effectively%2520filters%250Anoise%2520redundancy%2520and%2520mitigates%2520background%2520interference%2520based%2520on%2520the%2520specific%250Acharacteristics%2520of%2520each%2520modality.%2520Extensive%2520comparisons%2520conducted%2520on%2520five%250Abenchmarks%2520including%2520LasHeR%252C%2520GTOT%252C%2520RGBT234%252C%2520VisEvent%252C%2520and%2520DepthTrack%252C%2520covering%250ARGB-T%252C%2520RGB-E%252C%2520and%2520RGB-D%2520tracking%2520scenarios%252C%2520demonstrate%2520our%2520innovative%2520approach%250Aachieves%2520comparative%2520performance%2520by%2520introducing%2520only%2520additional%2520training%250Aparameters%2520of%25201.87M%2520and%2520flops%2520of%25201.95G.%2520The%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/wanghe/UASTrack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UASTrack%3A%20A%20Unified%20Adaptive%20Selection%20Framework%20with%0A%20%20Modality-Customization%20in%20Single%20Object%20Tracking&entry.906535625=He%20Wang%20and%20Tianyang%20Xu%20and%20Zhangyong%20Tang%20and%20Xiao-Jun%20Wu%20and%20Josef%20Kittler&entry.1292438233=%20%20Multi-modal%20tracking%20is%20essential%20in%20single-object%20tracking%20%28SOT%29%2C%20as%0Adifferent%20sensor%20types%20contribute%20unique%20capabilities%20to%20overcome%20challenges%0Acaused%20by%20variations%20in%20object%20appearance.%20However%2C%20existing%20unified%20RGB-X%0Atrackers%20%28X%20represents%20depth%2C%20event%2C%20or%20thermal%20modality%29%20either%20rely%20on%20the%0Atask-specific%20training%20strategy%20for%20individual%20RGB-X%20image%20pairs%20or%20fail%20to%0Aaddress%20the%20critical%20importance%20of%20modality-adaptive%20perception%20in%20real-world%0Aapplications.%20In%20this%20work%2C%20we%20propose%20UASTrack%2C%20a%20unified%20adaptive%20selection%0Aframework%20that%20facilitates%20both%20model%20and%20parameter%20unification%2C%20as%20well%20as%0Aadaptive%20modality%20discrimination%20across%20various%20multi-modal%20tracking%20tasks.%20To%0Aachieve%20modality-adaptive%20perception%20in%20joint%20RGB-X%20pairs%2C%20we%20design%20a%0ADiscriminative%20Auto-Selector%20%28DAS%29%20capable%20of%20identifying%20modality%20labels%2C%0Athereby%20distinguishing%20the%20data%20distributions%20of%20auxiliary%20modalities.%0AFurthermore%2C%20we%20propose%20a%20Task-Customized%20Optimization%20Adapter%20%28TCOA%29%20tailored%0Ato%20various%20modalities%20in%20the%20latent%20space.%20This%20strategy%20effectively%20filters%0Anoise%20redundancy%20and%20mitigates%20background%20interference%20based%20on%20the%20specific%0Acharacteristics%20of%20each%20modality.%20Extensive%20comparisons%20conducted%20on%20five%0Abenchmarks%20including%20LasHeR%2C%20GTOT%2C%20RGBT234%2C%20VisEvent%2C%20and%20DepthTrack%2C%20covering%0ARGB-T%2C%20RGB-E%2C%20and%20RGB-D%20tracking%20scenarios%2C%20demonstrate%20our%20innovative%20approach%0Aachieves%20comparative%20performance%20by%20introducing%20only%20additional%20training%0Aparameters%20of%201.87M%20and%20flops%20of%201.95G.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/wanghe/UASTrack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18220v1&entry.124074799=Read"},
{"title": "SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference", "author": "Jintao Zhang and Chendong Xiang and Haofeng Huang and Jia Wei and Haocheng Xi and Jun Zhu and Jianfei Chen", "abstract": "  An efficient attention implementation is essential for large models due to\nits quadratic time complexity. Fortunately, attention commonly exhibits\nsparsity, i.e., many values in the attention map are near zero, allowing for\nthe omission of corresponding computations. Many studies have utilized the\nsparse pattern to accelerate attention. However, most existing works focus on\noptimizing attention within specific models by exploiting certain sparse\npatterns of the attention map. A universal sparse attention that guarantees\nboth the speedup and end-to-end performance of diverse models remains elusive.\nIn this paper, we propose SpargeAttn, a universal sparse and quantized\nattention for any model. Our method uses a two-stage online filter: in the\nfirst stage, we rapidly and accurately predict the attention map, enabling the\nskip of some matrix multiplications in attention. In the second stage, we\ndesign an online softmax-aware filter that incurs no extra overhead and further\nskips some matrix multiplications. Experiments show that our method\nsignificantly accelerates diverse models, including language, image, and video\ngeneration, without sacrificing end-to-end metrics. The codes are available at\nhttps://github.com/thu-ml/SpargeAttn.\n", "link": "http://arxiv.org/abs/2502.18137v1", "date": "2025-02-25", "relevancy": 2.2511, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6192}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5257}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpargeAttn%3A%20Accurate%20Sparse%20Attention%20Accelerating%20Any%20Model%20Inference&body=Title%3A%20SpargeAttn%3A%20Accurate%20Sparse%20Attention%20Accelerating%20Any%20Model%20Inference%0AAuthor%3A%20Jintao%20Zhang%20and%20Chendong%20Xiang%20and%20Haofeng%20Huang%20and%20Jia%20Wei%20and%20Haocheng%20Xi%20and%20Jun%20Zhu%20and%20Jianfei%20Chen%0AAbstract%3A%20%20%20An%20efficient%20attention%20implementation%20is%20essential%20for%20large%20models%20due%20to%0Aits%20quadratic%20time%20complexity.%20Fortunately%2C%20attention%20commonly%20exhibits%0Asparsity%2C%20i.e.%2C%20many%20values%20in%20the%20attention%20map%20are%20near%20zero%2C%20allowing%20for%0Athe%20omission%20of%20corresponding%20computations.%20Many%20studies%20have%20utilized%20the%0Asparse%20pattern%20to%20accelerate%20attention.%20However%2C%20most%20existing%20works%20focus%20on%0Aoptimizing%20attention%20within%20specific%20models%20by%20exploiting%20certain%20sparse%0Apatterns%20of%20the%20attention%20map.%20A%20universal%20sparse%20attention%20that%20guarantees%0Aboth%20the%20speedup%20and%20end-to-end%20performance%20of%20diverse%20models%20remains%20elusive.%0AIn%20this%20paper%2C%20we%20propose%20SpargeAttn%2C%20a%20universal%20sparse%20and%20quantized%0Aattention%20for%20any%20model.%20Our%20method%20uses%20a%20two-stage%20online%20filter%3A%20in%20the%0Afirst%20stage%2C%20we%20rapidly%20and%20accurately%20predict%20the%20attention%20map%2C%20enabling%20the%0Askip%20of%20some%20matrix%20multiplications%20in%20attention.%20In%20the%20second%20stage%2C%20we%0Adesign%20an%20online%20softmax-aware%20filter%20that%20incurs%20no%20extra%20overhead%20and%20further%0Askips%20some%20matrix%20multiplications.%20Experiments%20show%20that%20our%20method%0Asignificantly%20accelerates%20diverse%20models%2C%20including%20language%2C%20image%2C%20and%20video%0Ageneration%2C%20without%20sacrificing%20end-to-end%20metrics.%20The%20codes%20are%20available%20at%0Ahttps%3A//github.com/thu-ml/SpargeAttn.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpargeAttn%253A%2520Accurate%2520Sparse%2520Attention%2520Accelerating%2520Any%2520Model%2520Inference%26entry.906535625%3DJintao%2520Zhang%2520and%2520Chendong%2520Xiang%2520and%2520Haofeng%2520Huang%2520and%2520Jia%2520Wei%2520and%2520Haocheng%2520Xi%2520and%2520Jun%2520Zhu%2520and%2520Jianfei%2520Chen%26entry.1292438233%3D%2520%2520An%2520efficient%2520attention%2520implementation%2520is%2520essential%2520for%2520large%2520models%2520due%2520to%250Aits%2520quadratic%2520time%2520complexity.%2520Fortunately%252C%2520attention%2520commonly%2520exhibits%250Asparsity%252C%2520i.e.%252C%2520many%2520values%2520in%2520the%2520attention%2520map%2520are%2520near%2520zero%252C%2520allowing%2520for%250Athe%2520omission%2520of%2520corresponding%2520computations.%2520Many%2520studies%2520have%2520utilized%2520the%250Asparse%2520pattern%2520to%2520accelerate%2520attention.%2520However%252C%2520most%2520existing%2520works%2520focus%2520on%250Aoptimizing%2520attention%2520within%2520specific%2520models%2520by%2520exploiting%2520certain%2520sparse%250Apatterns%2520of%2520the%2520attention%2520map.%2520A%2520universal%2520sparse%2520attention%2520that%2520guarantees%250Aboth%2520the%2520speedup%2520and%2520end-to-end%2520performance%2520of%2520diverse%2520models%2520remains%2520elusive.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520SpargeAttn%252C%2520a%2520universal%2520sparse%2520and%2520quantized%250Aattention%2520for%2520any%2520model.%2520Our%2520method%2520uses%2520a%2520two-stage%2520online%2520filter%253A%2520in%2520the%250Afirst%2520stage%252C%2520we%2520rapidly%2520and%2520accurately%2520predict%2520the%2520attention%2520map%252C%2520enabling%2520the%250Askip%2520of%2520some%2520matrix%2520multiplications%2520in%2520attention.%2520In%2520the%2520second%2520stage%252C%2520we%250Adesign%2520an%2520online%2520softmax-aware%2520filter%2520that%2520incurs%2520no%2520extra%2520overhead%2520and%2520further%250Askips%2520some%2520matrix%2520multiplications.%2520Experiments%2520show%2520that%2520our%2520method%250Asignificantly%2520accelerates%2520diverse%2520models%252C%2520including%2520language%252C%2520image%252C%2520and%2520video%250Ageneration%252C%2520without%2520sacrificing%2520end-to-end%2520metrics.%2520The%2520codes%2520are%2520available%2520at%250Ahttps%253A//github.com/thu-ml/SpargeAttn.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpargeAttn%3A%20Accurate%20Sparse%20Attention%20Accelerating%20Any%20Model%20Inference&entry.906535625=Jintao%20Zhang%20and%20Chendong%20Xiang%20and%20Haofeng%20Huang%20and%20Jia%20Wei%20and%20Haocheng%20Xi%20and%20Jun%20Zhu%20and%20Jianfei%20Chen&entry.1292438233=%20%20An%20efficient%20attention%20implementation%20is%20essential%20for%20large%20models%20due%20to%0Aits%20quadratic%20time%20complexity.%20Fortunately%2C%20attention%20commonly%20exhibits%0Asparsity%2C%20i.e.%2C%20many%20values%20in%20the%20attention%20map%20are%20near%20zero%2C%20allowing%20for%0Athe%20omission%20of%20corresponding%20computations.%20Many%20studies%20have%20utilized%20the%0Asparse%20pattern%20to%20accelerate%20attention.%20However%2C%20most%20existing%20works%20focus%20on%0Aoptimizing%20attention%20within%20specific%20models%20by%20exploiting%20certain%20sparse%0Apatterns%20of%20the%20attention%20map.%20A%20universal%20sparse%20attention%20that%20guarantees%0Aboth%20the%20speedup%20and%20end-to-end%20performance%20of%20diverse%20models%20remains%20elusive.%0AIn%20this%20paper%2C%20we%20propose%20SpargeAttn%2C%20a%20universal%20sparse%20and%20quantized%0Aattention%20for%20any%20model.%20Our%20method%20uses%20a%20two-stage%20online%20filter%3A%20in%20the%0Afirst%20stage%2C%20we%20rapidly%20and%20accurately%20predict%20the%20attention%20map%2C%20enabling%20the%0Askip%20of%20some%20matrix%20multiplications%20in%20attention.%20In%20the%20second%20stage%2C%20we%0Adesign%20an%20online%20softmax-aware%20filter%20that%20incurs%20no%20extra%20overhead%20and%20further%0Askips%20some%20matrix%20multiplications.%20Experiments%20show%20that%20our%20method%0Asignificantly%20accelerates%20diverse%20models%2C%20including%20language%2C%20image%2C%20and%20video%0Ageneration%2C%20without%20sacrificing%20end-to-end%20metrics.%20The%20codes%20are%20available%20at%0Ahttps%3A//github.com/thu-ml/SpargeAttn.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18137v1&entry.124074799=Read"},
{"title": "Collaboration of Large Language Models and Small Recommendation Models\n  for Device-Cloud Recommendation", "author": "Zheqi Lv and Tianyu Zhan and Wenjie Wang and Xinyu Lin and Shengyu Zhang and Wenqiao Zhang and Jiwei Li and Kun Kuang and Fei Wu", "abstract": "  Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising\nresearch direction that has demonstrated exceptional performance in this field.\nHowever, its inability to capture real-time user preferences greatly limits the\npractical application of LLM4Rec because (i) LLMs are costly to train and infer\nfrequently, and (ii) LLMs struggle to access real-time data (its large number\nof parameters poses an obstacle to deployment on devices). Fortunately, small\nrecommendation models (SRMs) can effectively supplement these shortcomings of\nLLM4Rec diagrams by consuming minimal resources for frequent training and\ninference, and by conveniently accessing real-time data on devices.\n  In light of this, we designed the Device-Cloud LLM-SRM Collaborative\nRecommendation Framework (LSC4Rec) under a device-cloud collaboration setting.\nLSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the\nbenefits of cloud and edge computing, achieving a complementary synergy. We\nenhance the practicability of LSC4Rec by designing three strategies:\ncollaborative training, collaborative inference, and intelligent request.\nDuring training, LLM generates candidate lists to enhance the ranking ability\nof SRM in collaborative scenarios and enables SRM to update adaptively to\ncapture real-time user interests. During inference, LLM and SRM are deployed on\nthe cloud and on the device, respectively. LLM generates candidate lists and\ninitial ranking results based on user behavior, and SRM get reranking results\nbased on the candidate list, with final results integrating both LLM's and\nSRM's scores. The device determines whether a new candidate list is needed by\ncomparing the consistency of the LLM's and SRM's sorted lists. Our\ncomprehensive and extensive experimental analysis validates the effectiveness\nof each strategy in LSC4Rec.\n", "link": "http://arxiv.org/abs/2501.05647v2", "date": "2025-02-25", "relevancy": 2.249, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4495}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaboration%20of%20Large%20Language%20Models%20and%20Small%20Recommendation%20Models%0A%20%20for%20Device-Cloud%20Recommendation&body=Title%3A%20Collaboration%20of%20Large%20Language%20Models%20and%20Small%20Recommendation%20Models%0A%20%20for%20Device-Cloud%20Recommendation%0AAuthor%3A%20Zheqi%20Lv%20and%20Tianyu%20Zhan%20and%20Wenjie%20Wang%20and%20Xinyu%20Lin%20and%20Shengyu%20Zhang%20and%20Wenqiao%20Zhang%20and%20Jiwei%20Li%20and%20Kun%20Kuang%20and%20Fei%20Wu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20for%20Recommendation%20%28LLM4Rec%29%20is%20a%20promising%0Aresearch%20direction%20that%20has%20demonstrated%20exceptional%20performance%20in%20this%20field.%0AHowever%2C%20its%20inability%20to%20capture%20real-time%20user%20preferences%20greatly%20limits%20the%0Apractical%20application%20of%20LLM4Rec%20because%20%28i%29%20LLMs%20are%20costly%20to%20train%20and%20infer%0Afrequently%2C%20and%20%28ii%29%20LLMs%20struggle%20to%20access%20real-time%20data%20%28its%20large%20number%0Aof%20parameters%20poses%20an%20obstacle%20to%20deployment%20on%20devices%29.%20Fortunately%2C%20small%0Arecommendation%20models%20%28SRMs%29%20can%20effectively%20supplement%20these%20shortcomings%20of%0ALLM4Rec%20diagrams%20by%20consuming%20minimal%20resources%20for%20frequent%20training%20and%0Ainference%2C%20and%20by%20conveniently%20accessing%20real-time%20data%20on%20devices.%0A%20%20In%20light%20of%20this%2C%20we%20designed%20the%20Device-Cloud%20LLM-SRM%20Collaborative%0ARecommendation%20Framework%20%28LSC4Rec%29%20under%20a%20device-cloud%20collaboration%20setting.%0ALSC4Rec%20aims%20to%20integrate%20the%20advantages%20of%20both%20LLMs%20and%20SRMs%2C%20as%20well%20as%20the%0Abenefits%20of%20cloud%20and%20edge%20computing%2C%20achieving%20a%20complementary%20synergy.%20We%0Aenhance%20the%20practicability%20of%20LSC4Rec%20by%20designing%20three%20strategies%3A%0Acollaborative%20training%2C%20collaborative%20inference%2C%20and%20intelligent%20request.%0ADuring%20training%2C%20LLM%20generates%20candidate%20lists%20to%20enhance%20the%20ranking%20ability%0Aof%20SRM%20in%20collaborative%20scenarios%20and%20enables%20SRM%20to%20update%20adaptively%20to%0Acapture%20real-time%20user%20interests.%20During%20inference%2C%20LLM%20and%20SRM%20are%20deployed%20on%0Athe%20cloud%20and%20on%20the%20device%2C%20respectively.%20LLM%20generates%20candidate%20lists%20and%0Ainitial%20ranking%20results%20based%20on%20user%20behavior%2C%20and%20SRM%20get%20reranking%20results%0Abased%20on%20the%20candidate%20list%2C%20with%20final%20results%20integrating%20both%20LLM%27s%20and%0ASRM%27s%20scores.%20The%20device%20determines%20whether%20a%20new%20candidate%20list%20is%20needed%20by%0Acomparing%20the%20consistency%20of%20the%20LLM%27s%20and%20SRM%27s%20sorted%20lists.%20Our%0Acomprehensive%20and%20extensive%20experimental%20analysis%20validates%20the%20effectiveness%0Aof%20each%20strategy%20in%20LSC4Rec.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05647v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaboration%2520of%2520Large%2520Language%2520Models%2520and%2520Small%2520Recommendation%2520Models%250A%2520%2520for%2520Device-Cloud%2520Recommendation%26entry.906535625%3DZheqi%2520Lv%2520and%2520Tianyu%2520Zhan%2520and%2520Wenjie%2520Wang%2520and%2520Xinyu%2520Lin%2520and%2520Shengyu%2520Zhang%2520and%2520Wenqiao%2520Zhang%2520and%2520Jiwei%2520Li%2520and%2520Kun%2520Kuang%2520and%2520Fei%2520Wu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520Recommendation%2520%2528LLM4Rec%2529%2520is%2520a%2520promising%250Aresearch%2520direction%2520that%2520has%2520demonstrated%2520exceptional%2520performance%2520in%2520this%2520field.%250AHowever%252C%2520its%2520inability%2520to%2520capture%2520real-time%2520user%2520preferences%2520greatly%2520limits%2520the%250Apractical%2520application%2520of%2520LLM4Rec%2520because%2520%2528i%2529%2520LLMs%2520are%2520costly%2520to%2520train%2520and%2520infer%250Afrequently%252C%2520and%2520%2528ii%2529%2520LLMs%2520struggle%2520to%2520access%2520real-time%2520data%2520%2528its%2520large%2520number%250Aof%2520parameters%2520poses%2520an%2520obstacle%2520to%2520deployment%2520on%2520devices%2529.%2520Fortunately%252C%2520small%250Arecommendation%2520models%2520%2528SRMs%2529%2520can%2520effectively%2520supplement%2520these%2520shortcomings%2520of%250ALLM4Rec%2520diagrams%2520by%2520consuming%2520minimal%2520resources%2520for%2520frequent%2520training%2520and%250Ainference%252C%2520and%2520by%2520conveniently%2520accessing%2520real-time%2520data%2520on%2520devices.%250A%2520%2520In%2520light%2520of%2520this%252C%2520we%2520designed%2520the%2520Device-Cloud%2520LLM-SRM%2520Collaborative%250ARecommendation%2520Framework%2520%2528LSC4Rec%2529%2520under%2520a%2520device-cloud%2520collaboration%2520setting.%250ALSC4Rec%2520aims%2520to%2520integrate%2520the%2520advantages%2520of%2520both%2520LLMs%2520and%2520SRMs%252C%2520as%2520well%2520as%2520the%250Abenefits%2520of%2520cloud%2520and%2520edge%2520computing%252C%2520achieving%2520a%2520complementary%2520synergy.%2520We%250Aenhance%2520the%2520practicability%2520of%2520LSC4Rec%2520by%2520designing%2520three%2520strategies%253A%250Acollaborative%2520training%252C%2520collaborative%2520inference%252C%2520and%2520intelligent%2520request.%250ADuring%2520training%252C%2520LLM%2520generates%2520candidate%2520lists%2520to%2520enhance%2520the%2520ranking%2520ability%250Aof%2520SRM%2520in%2520collaborative%2520scenarios%2520and%2520enables%2520SRM%2520to%2520update%2520adaptively%2520to%250Acapture%2520real-time%2520user%2520interests.%2520During%2520inference%252C%2520LLM%2520and%2520SRM%2520are%2520deployed%2520on%250Athe%2520cloud%2520and%2520on%2520the%2520device%252C%2520respectively.%2520LLM%2520generates%2520candidate%2520lists%2520and%250Ainitial%2520ranking%2520results%2520based%2520on%2520user%2520behavior%252C%2520and%2520SRM%2520get%2520reranking%2520results%250Abased%2520on%2520the%2520candidate%2520list%252C%2520with%2520final%2520results%2520integrating%2520both%2520LLM%2527s%2520and%250ASRM%2527s%2520scores.%2520The%2520device%2520determines%2520whether%2520a%2520new%2520candidate%2520list%2520is%2520needed%2520by%250Acomparing%2520the%2520consistency%2520of%2520the%2520LLM%2527s%2520and%2520SRM%2527s%2520sorted%2520lists.%2520Our%250Acomprehensive%2520and%2520extensive%2520experimental%2520analysis%2520validates%2520the%2520effectiveness%250Aof%2520each%2520strategy%2520in%2520LSC4Rec.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05647v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaboration%20of%20Large%20Language%20Models%20and%20Small%20Recommendation%20Models%0A%20%20for%20Device-Cloud%20Recommendation&entry.906535625=Zheqi%20Lv%20and%20Tianyu%20Zhan%20and%20Wenjie%20Wang%20and%20Xinyu%20Lin%20and%20Shengyu%20Zhang%20and%20Wenqiao%20Zhang%20and%20Jiwei%20Li%20and%20Kun%20Kuang%20and%20Fei%20Wu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20for%20Recommendation%20%28LLM4Rec%29%20is%20a%20promising%0Aresearch%20direction%20that%20has%20demonstrated%20exceptional%20performance%20in%20this%20field.%0AHowever%2C%20its%20inability%20to%20capture%20real-time%20user%20preferences%20greatly%20limits%20the%0Apractical%20application%20of%20LLM4Rec%20because%20%28i%29%20LLMs%20are%20costly%20to%20train%20and%20infer%0Afrequently%2C%20and%20%28ii%29%20LLMs%20struggle%20to%20access%20real-time%20data%20%28its%20large%20number%0Aof%20parameters%20poses%20an%20obstacle%20to%20deployment%20on%20devices%29.%20Fortunately%2C%20small%0Arecommendation%20models%20%28SRMs%29%20can%20effectively%20supplement%20these%20shortcomings%20of%0ALLM4Rec%20diagrams%20by%20consuming%20minimal%20resources%20for%20frequent%20training%20and%0Ainference%2C%20and%20by%20conveniently%20accessing%20real-time%20data%20on%20devices.%0A%20%20In%20light%20of%20this%2C%20we%20designed%20the%20Device-Cloud%20LLM-SRM%20Collaborative%0ARecommendation%20Framework%20%28LSC4Rec%29%20under%20a%20device-cloud%20collaboration%20setting.%0ALSC4Rec%20aims%20to%20integrate%20the%20advantages%20of%20both%20LLMs%20and%20SRMs%2C%20as%20well%20as%20the%0Abenefits%20of%20cloud%20and%20edge%20computing%2C%20achieving%20a%20complementary%20synergy.%20We%0Aenhance%20the%20practicability%20of%20LSC4Rec%20by%20designing%20three%20strategies%3A%0Acollaborative%20training%2C%20collaborative%20inference%2C%20and%20intelligent%20request.%0ADuring%20training%2C%20LLM%20generates%20candidate%20lists%20to%20enhance%20the%20ranking%20ability%0Aof%20SRM%20in%20collaborative%20scenarios%20and%20enables%20SRM%20to%20update%20adaptively%20to%0Acapture%20real-time%20user%20interests.%20During%20inference%2C%20LLM%20and%20SRM%20are%20deployed%20on%0Athe%20cloud%20and%20on%20the%20device%2C%20respectively.%20LLM%20generates%20candidate%20lists%20and%0Ainitial%20ranking%20results%20based%20on%20user%20behavior%2C%20and%20SRM%20get%20reranking%20results%0Abased%20on%20the%20candidate%20list%2C%20with%20final%20results%20integrating%20both%20LLM%27s%20and%0ASRM%27s%20scores.%20The%20device%20determines%20whether%20a%20new%20candidate%20list%20is%20needed%20by%0Acomparing%20the%20consistency%20of%20the%20LLM%27s%20and%20SRM%27s%20sorted%20lists.%20Our%0Acomprehensive%20and%20extensive%20experimental%20analysis%20validates%20the%20effectiveness%0Aof%20each%20strategy%20in%20LSC4Rec.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05647v2&entry.124074799=Read"},
{"title": "Training Consistency Models with Variational Noise Coupling", "author": "Gianluigi Silvestri and Luca Ambrogioni and Chieh-Hsin Lai and Yuhta Takida and Yuki Mitsufuji", "abstract": "  Consistency Training (CT) has recently emerged as a promising alternative to\ndiffusion models, achieving competitive performance in image generation tasks.\nHowever, non-distillation consistency training often suffers from high variance\nand instability, and analyzing and improving its training dynamics is an active\narea of research. In this work, we propose a novel CT training approach based\non the Flow Matching framework. Our main contribution is a trained\nnoise-coupling scheme inspired by the architecture of Variational Autoencoders\n(VAE). By training a data-dependent noise emission model implemented as an\nencoder architecture, our method can indirectly learn the geometry of the\nnoise-to-data mapping, which is instead fixed by the choice of the forward\nprocess in classical CT. Empirical results across diverse image datasets show\nsignificant generative improvements, with our model outperforming baselines and\nachieving the state-of-the-art (SoTA) non-distillation CT FID on CIFAR-10, and\nattaining FID on par with SoTA on ImageNet at $64 \\times 64$ resolution in\n2-step generation. Our code is available at https://github.com/sony/vct .\n", "link": "http://arxiv.org/abs/2502.18197v1", "date": "2025-02-25", "relevancy": 2.242, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5866}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5646}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Consistency%20Models%20with%20Variational%20Noise%20Coupling&body=Title%3A%20Training%20Consistency%20Models%20with%20Variational%20Noise%20Coupling%0AAuthor%3A%20Gianluigi%20Silvestri%20and%20Luca%20Ambrogioni%20and%20Chieh-Hsin%20Lai%20and%20Yuhta%20Takida%20and%20Yuki%20Mitsufuji%0AAbstract%3A%20%20%20Consistency%20Training%20%28CT%29%20has%20recently%20emerged%20as%20a%20promising%20alternative%20to%0Adiffusion%20models%2C%20achieving%20competitive%20performance%20in%20image%20generation%20tasks.%0AHowever%2C%20non-distillation%20consistency%20training%20often%20suffers%20from%20high%20variance%0Aand%20instability%2C%20and%20analyzing%20and%20improving%20its%20training%20dynamics%20is%20an%20active%0Aarea%20of%20research.%20In%20this%20work%2C%20we%20propose%20a%20novel%20CT%20training%20approach%20based%0Aon%20the%20Flow%20Matching%20framework.%20Our%20main%20contribution%20is%20a%20trained%0Anoise-coupling%20scheme%20inspired%20by%20the%20architecture%20of%20Variational%20Autoencoders%0A%28VAE%29.%20By%20training%20a%20data-dependent%20noise%20emission%20model%20implemented%20as%20an%0Aencoder%20architecture%2C%20our%20method%20can%20indirectly%20learn%20the%20geometry%20of%20the%0Anoise-to-data%20mapping%2C%20which%20is%20instead%20fixed%20by%20the%20choice%20of%20the%20forward%0Aprocess%20in%20classical%20CT.%20Empirical%20results%20across%20diverse%20image%20datasets%20show%0Asignificant%20generative%20improvements%2C%20with%20our%20model%20outperforming%20baselines%20and%0Aachieving%20the%20state-of-the-art%20%28SoTA%29%20non-distillation%20CT%20FID%20on%20CIFAR-10%2C%20and%0Aattaining%20FID%20on%20par%20with%20SoTA%20on%20ImageNet%20at%20%2464%20%5Ctimes%2064%24%20resolution%20in%0A2-step%20generation.%20Our%20code%20is%20available%20at%20https%3A//github.com/sony/vct%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18197v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Consistency%2520Models%2520with%2520Variational%2520Noise%2520Coupling%26entry.906535625%3DGianluigi%2520Silvestri%2520and%2520Luca%2520Ambrogioni%2520and%2520Chieh-Hsin%2520Lai%2520and%2520Yuhta%2520Takida%2520and%2520Yuki%2520Mitsufuji%26entry.1292438233%3D%2520%2520Consistency%2520Training%2520%2528CT%2529%2520has%2520recently%2520emerged%2520as%2520a%2520promising%2520alternative%2520to%250Adiffusion%2520models%252C%2520achieving%2520competitive%2520performance%2520in%2520image%2520generation%2520tasks.%250AHowever%252C%2520non-distillation%2520consistency%2520training%2520often%2520suffers%2520from%2520high%2520variance%250Aand%2520instability%252C%2520and%2520analyzing%2520and%2520improving%2520its%2520training%2520dynamics%2520is%2520an%2520active%250Aarea%2520of%2520research.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520CT%2520training%2520approach%2520based%250Aon%2520the%2520Flow%2520Matching%2520framework.%2520Our%2520main%2520contribution%2520is%2520a%2520trained%250Anoise-coupling%2520scheme%2520inspired%2520by%2520the%2520architecture%2520of%2520Variational%2520Autoencoders%250A%2528VAE%2529.%2520By%2520training%2520a%2520data-dependent%2520noise%2520emission%2520model%2520implemented%2520as%2520an%250Aencoder%2520architecture%252C%2520our%2520method%2520can%2520indirectly%2520learn%2520the%2520geometry%2520of%2520the%250Anoise-to-data%2520mapping%252C%2520which%2520is%2520instead%2520fixed%2520by%2520the%2520choice%2520of%2520the%2520forward%250Aprocess%2520in%2520classical%2520CT.%2520Empirical%2520results%2520across%2520diverse%2520image%2520datasets%2520show%250Asignificant%2520generative%2520improvements%252C%2520with%2520our%2520model%2520outperforming%2520baselines%2520and%250Aachieving%2520the%2520state-of-the-art%2520%2528SoTA%2529%2520non-distillation%2520CT%2520FID%2520on%2520CIFAR-10%252C%2520and%250Aattaining%2520FID%2520on%2520par%2520with%2520SoTA%2520on%2520ImageNet%2520at%2520%252464%2520%255Ctimes%252064%2524%2520resolution%2520in%250A2-step%2520generation.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/sony/vct%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18197v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Consistency%20Models%20with%20Variational%20Noise%20Coupling&entry.906535625=Gianluigi%20Silvestri%20and%20Luca%20Ambrogioni%20and%20Chieh-Hsin%20Lai%20and%20Yuhta%20Takida%20and%20Yuki%20Mitsufuji&entry.1292438233=%20%20Consistency%20Training%20%28CT%29%20has%20recently%20emerged%20as%20a%20promising%20alternative%20to%0Adiffusion%20models%2C%20achieving%20competitive%20performance%20in%20image%20generation%20tasks.%0AHowever%2C%20non-distillation%20consistency%20training%20often%20suffers%20from%20high%20variance%0Aand%20instability%2C%20and%20analyzing%20and%20improving%20its%20training%20dynamics%20is%20an%20active%0Aarea%20of%20research.%20In%20this%20work%2C%20we%20propose%20a%20novel%20CT%20training%20approach%20based%0Aon%20the%20Flow%20Matching%20framework.%20Our%20main%20contribution%20is%20a%20trained%0Anoise-coupling%20scheme%20inspired%20by%20the%20architecture%20of%20Variational%20Autoencoders%0A%28VAE%29.%20By%20training%20a%20data-dependent%20noise%20emission%20model%20implemented%20as%20an%0Aencoder%20architecture%2C%20our%20method%20can%20indirectly%20learn%20the%20geometry%20of%20the%0Anoise-to-data%20mapping%2C%20which%20is%20instead%20fixed%20by%20the%20choice%20of%20the%20forward%0Aprocess%20in%20classical%20CT.%20Empirical%20results%20across%20diverse%20image%20datasets%20show%0Asignificant%20generative%20improvements%2C%20with%20our%20model%20outperforming%20baselines%20and%0Aachieving%20the%20state-of-the-art%20%28SoTA%29%20non-distillation%20CT%20FID%20on%20CIFAR-10%2C%20and%0Aattaining%20FID%20on%20par%20with%20SoTA%20on%20ImageNet%20at%20%2464%20%5Ctimes%2064%24%20resolution%20in%0A2-step%20generation.%20Our%20code%20is%20available%20at%20https%3A//github.com/sony/vct%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18197v1&entry.124074799=Read"},
{"title": "Generalization error bound for denoising score matching under relaxed\n  manifold assumption", "author": "Konstantin Yakovlev and Nikita Puchkin", "abstract": "  We examine theoretical properties of the denoising score matching estimate.\nWe model the density of observations with a nonparametric Gaussian mixture. We\nsignificantly relax the standard manifold assumption allowing the samples step\naway from the manifold. At the same time, we are still able to leverage a nice\ndistribution structure. We derive non-asymptotic bounds on the approximation\nand generalization errors of the denoising score matching estimate. The rates\nof convergence are determined by the intrinsic dimension. Furthermore, our\nbounds remain valid even if we allow the ambient dimension grow polynomially\nwith the sample size.\n", "link": "http://arxiv.org/abs/2502.13662v2", "date": "2025-02-25", "relevancy": 2.2201, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4549}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4463}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalization%20error%20bound%20for%20denoising%20score%20matching%20under%20relaxed%0A%20%20manifold%20assumption&body=Title%3A%20Generalization%20error%20bound%20for%20denoising%20score%20matching%20under%20relaxed%0A%20%20manifold%20assumption%0AAuthor%3A%20Konstantin%20Yakovlev%20and%20Nikita%20Puchkin%0AAbstract%3A%20%20%20We%20examine%20theoretical%20properties%20of%20the%20denoising%20score%20matching%20estimate.%0AWe%20model%20the%20density%20of%20observations%20with%20a%20nonparametric%20Gaussian%20mixture.%20We%0Asignificantly%20relax%20the%20standard%20manifold%20assumption%20allowing%20the%20samples%20step%0Aaway%20from%20the%20manifold.%20At%20the%20same%20time%2C%20we%20are%20still%20able%20to%20leverage%20a%20nice%0Adistribution%20structure.%20We%20derive%20non-asymptotic%20bounds%20on%20the%20approximation%0Aand%20generalization%20errors%20of%20the%20denoising%20score%20matching%20estimate.%20The%20rates%0Aof%20convergence%20are%20determined%20by%20the%20intrinsic%20dimension.%20Furthermore%2C%20our%0Abounds%20remain%20valid%20even%20if%20we%20allow%20the%20ambient%20dimension%20grow%20polynomially%0Awith%20the%20sample%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13662v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralization%2520error%2520bound%2520for%2520denoising%2520score%2520matching%2520under%2520relaxed%250A%2520%2520manifold%2520assumption%26entry.906535625%3DKonstantin%2520Yakovlev%2520and%2520Nikita%2520Puchkin%26entry.1292438233%3D%2520%2520We%2520examine%2520theoretical%2520properties%2520of%2520the%2520denoising%2520score%2520matching%2520estimate.%250AWe%2520model%2520the%2520density%2520of%2520observations%2520with%2520a%2520nonparametric%2520Gaussian%2520mixture.%2520We%250Asignificantly%2520relax%2520the%2520standard%2520manifold%2520assumption%2520allowing%2520the%2520samples%2520step%250Aaway%2520from%2520the%2520manifold.%2520At%2520the%2520same%2520time%252C%2520we%2520are%2520still%2520able%2520to%2520leverage%2520a%2520nice%250Adistribution%2520structure.%2520We%2520derive%2520non-asymptotic%2520bounds%2520on%2520the%2520approximation%250Aand%2520generalization%2520errors%2520of%2520the%2520denoising%2520score%2520matching%2520estimate.%2520The%2520rates%250Aof%2520convergence%2520are%2520determined%2520by%2520the%2520intrinsic%2520dimension.%2520Furthermore%252C%2520our%250Abounds%2520remain%2520valid%2520even%2520if%2520we%2520allow%2520the%2520ambient%2520dimension%2520grow%2520polynomially%250Awith%2520the%2520sample%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13662v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20error%20bound%20for%20denoising%20score%20matching%20under%20relaxed%0A%20%20manifold%20assumption&entry.906535625=Konstantin%20Yakovlev%20and%20Nikita%20Puchkin&entry.1292438233=%20%20We%20examine%20theoretical%20properties%20of%20the%20denoising%20score%20matching%20estimate.%0AWe%20model%20the%20density%20of%20observations%20with%20a%20nonparametric%20Gaussian%20mixture.%20We%0Asignificantly%20relax%20the%20standard%20manifold%20assumption%20allowing%20the%20samples%20step%0Aaway%20from%20the%20manifold.%20At%20the%20same%20time%2C%20we%20are%20still%20able%20to%20leverage%20a%20nice%0Adistribution%20structure.%20We%20derive%20non-asymptotic%20bounds%20on%20the%20approximation%0Aand%20generalization%20errors%20of%20the%20denoising%20score%20matching%20estimate.%20The%20rates%0Aof%20convergence%20are%20determined%20by%20the%20intrinsic%20dimension.%20Furthermore%2C%20our%0Abounds%20remain%20valid%20even%20if%20we%20allow%20the%20ambient%20dimension%20grow%20polynomially%0Awith%20the%20sample%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13662v2&entry.124074799=Read"},
{"title": "Sewer Image Super-Resolution with Depth Priors and Its Lightweight\n  Network", "author": "Gang Pan and Chen Wang and Zhijie Sui and Shuai Guo and Yaozhi Lv and Honglie Li and Di Sun and Zixia Xia", "abstract": "  The Quick-view (QV) technique serves as a primary method for detecting\ndefects within sewerage systems. However, the effectiveness of QV is impeded by\nthe limited visual range of its hardware, resulting in suboptimal image quality\nfor distant portions of the sewer network. Image super-resolution is an\neffective way to improve image quality and has been applied in a variety of\nscenes. However, research on super-resolution for sewer images remains\nconsiderably unexplored. In response, this study leverages the inherent depth\nrelationships present within QV images and introduces a novel Depth-guided,\nReference-based Super-Resolution framework denoted as DSRNet. It comprises two\ncore components: a depth extraction module and a depth information matching\nmodule (DMM). DSRNet utilizes the adjacent frames of the low-resolution image\nas reference images and helps them recover texture information based on the\ncorrelation. By combining these modules, the integration of depth priors\nsignificantly enhances both visual quality and performance benchmarks. Besides,\nin pursuit of computational efficiency and compactness, a super-resolution\nknowledge distillation model based on an attention mechanism is introduced.\nThis mechanism facilitates the acquisition of feature similarity between a more\ncomplex teacher model and a streamlined student model, with the latter being a\nlightweight version of DSRNet. Experimental results demonstrate that DSRNet\nsignificantly improves PSNR and SSIM compared with other methods. This study\nalso conducts experiments on sewer defect semantic segmentation, object\ndetection, and classification on the Pipe dataset and Sewer-ML dataset.\nExperiments show that the method can improve the performance of low-resolution\nsewer images in these tasks.\n", "link": "http://arxiv.org/abs/2407.19271v3", "date": "2025-02-25", "relevancy": 2.2168, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sewer%20Image%20Super-Resolution%20with%20Depth%20Priors%20and%20Its%20Lightweight%0A%20%20Network&body=Title%3A%20Sewer%20Image%20Super-Resolution%20with%20Depth%20Priors%20and%20Its%20Lightweight%0A%20%20Network%0AAuthor%3A%20Gang%20Pan%20and%20Chen%20Wang%20and%20Zhijie%20Sui%20and%20Shuai%20Guo%20and%20Yaozhi%20Lv%20and%20Honglie%20Li%20and%20Di%20Sun%20and%20Zixia%20Xia%0AAbstract%3A%20%20%20The%20Quick-view%20%28QV%29%20technique%20serves%20as%20a%20primary%20method%20for%20detecting%0Adefects%20within%20sewerage%20systems.%20However%2C%20the%20effectiveness%20of%20QV%20is%20impeded%20by%0Athe%20limited%20visual%20range%20of%20its%20hardware%2C%20resulting%20in%20suboptimal%20image%20quality%0Afor%20distant%20portions%20of%20the%20sewer%20network.%20Image%20super-resolution%20is%20an%0Aeffective%20way%20to%20improve%20image%20quality%20and%20has%20been%20applied%20in%20a%20variety%20of%0Ascenes.%20However%2C%20research%20on%20super-resolution%20for%20sewer%20images%20remains%0Aconsiderably%20unexplored.%20In%20response%2C%20this%20study%20leverages%20the%20inherent%20depth%0Arelationships%20present%20within%20QV%20images%20and%20introduces%20a%20novel%20Depth-guided%2C%0AReference-based%20Super-Resolution%20framework%20denoted%20as%20DSRNet.%20It%20comprises%20two%0Acore%20components%3A%20a%20depth%20extraction%20module%20and%20a%20depth%20information%20matching%0Amodule%20%28DMM%29.%20DSRNet%20utilizes%20the%20adjacent%20frames%20of%20the%20low-resolution%20image%0Aas%20reference%20images%20and%20helps%20them%20recover%20texture%20information%20based%20on%20the%0Acorrelation.%20By%20combining%20these%20modules%2C%20the%20integration%20of%20depth%20priors%0Asignificantly%20enhances%20both%20visual%20quality%20and%20performance%20benchmarks.%20Besides%2C%0Ain%20pursuit%20of%20computational%20efficiency%20and%20compactness%2C%20a%20super-resolution%0Aknowledge%20distillation%20model%20based%20on%20an%20attention%20mechanism%20is%20introduced.%0AThis%20mechanism%20facilitates%20the%20acquisition%20of%20feature%20similarity%20between%20a%20more%0Acomplex%20teacher%20model%20and%20a%20streamlined%20student%20model%2C%20with%20the%20latter%20being%20a%0Alightweight%20version%20of%20DSRNet.%20Experimental%20results%20demonstrate%20that%20DSRNet%0Asignificantly%20improves%20PSNR%20and%20SSIM%20compared%20with%20other%20methods.%20This%20study%0Aalso%20conducts%20experiments%20on%20sewer%20defect%20semantic%20segmentation%2C%20object%0Adetection%2C%20and%20classification%20on%20the%20Pipe%20dataset%20and%20Sewer-ML%20dataset.%0AExperiments%20show%20that%20the%20method%20can%20improve%20the%20performance%20of%20low-resolution%0Asewer%20images%20in%20these%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19271v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSewer%2520Image%2520Super-Resolution%2520with%2520Depth%2520Priors%2520and%2520Its%2520Lightweight%250A%2520%2520Network%26entry.906535625%3DGang%2520Pan%2520and%2520Chen%2520Wang%2520and%2520Zhijie%2520Sui%2520and%2520Shuai%2520Guo%2520and%2520Yaozhi%2520Lv%2520and%2520Honglie%2520Li%2520and%2520Di%2520Sun%2520and%2520Zixia%2520Xia%26entry.1292438233%3D%2520%2520The%2520Quick-view%2520%2528QV%2529%2520technique%2520serves%2520as%2520a%2520primary%2520method%2520for%2520detecting%250Adefects%2520within%2520sewerage%2520systems.%2520However%252C%2520the%2520effectiveness%2520of%2520QV%2520is%2520impeded%2520by%250Athe%2520limited%2520visual%2520range%2520of%2520its%2520hardware%252C%2520resulting%2520in%2520suboptimal%2520image%2520quality%250Afor%2520distant%2520portions%2520of%2520the%2520sewer%2520network.%2520Image%2520super-resolution%2520is%2520an%250Aeffective%2520way%2520to%2520improve%2520image%2520quality%2520and%2520has%2520been%2520applied%2520in%2520a%2520variety%2520of%250Ascenes.%2520However%252C%2520research%2520on%2520super-resolution%2520for%2520sewer%2520images%2520remains%250Aconsiderably%2520unexplored.%2520In%2520response%252C%2520this%2520study%2520leverages%2520the%2520inherent%2520depth%250Arelationships%2520present%2520within%2520QV%2520images%2520and%2520introduces%2520a%2520novel%2520Depth-guided%252C%250AReference-based%2520Super-Resolution%2520framework%2520denoted%2520as%2520DSRNet.%2520It%2520comprises%2520two%250Acore%2520components%253A%2520a%2520depth%2520extraction%2520module%2520and%2520a%2520depth%2520information%2520matching%250Amodule%2520%2528DMM%2529.%2520DSRNet%2520utilizes%2520the%2520adjacent%2520frames%2520of%2520the%2520low-resolution%2520image%250Aas%2520reference%2520images%2520and%2520helps%2520them%2520recover%2520texture%2520information%2520based%2520on%2520the%250Acorrelation.%2520By%2520combining%2520these%2520modules%252C%2520the%2520integration%2520of%2520depth%2520priors%250Asignificantly%2520enhances%2520both%2520visual%2520quality%2520and%2520performance%2520benchmarks.%2520Besides%252C%250Ain%2520pursuit%2520of%2520computational%2520efficiency%2520and%2520compactness%252C%2520a%2520super-resolution%250Aknowledge%2520distillation%2520model%2520based%2520on%2520an%2520attention%2520mechanism%2520is%2520introduced.%250AThis%2520mechanism%2520facilitates%2520the%2520acquisition%2520of%2520feature%2520similarity%2520between%2520a%2520more%250Acomplex%2520teacher%2520model%2520and%2520a%2520streamlined%2520student%2520model%252C%2520with%2520the%2520latter%2520being%2520a%250Alightweight%2520version%2520of%2520DSRNet.%2520Experimental%2520results%2520demonstrate%2520that%2520DSRNet%250Asignificantly%2520improves%2520PSNR%2520and%2520SSIM%2520compared%2520with%2520other%2520methods.%2520This%2520study%250Aalso%2520conducts%2520experiments%2520on%2520sewer%2520defect%2520semantic%2520segmentation%252C%2520object%250Adetection%252C%2520and%2520classification%2520on%2520the%2520Pipe%2520dataset%2520and%2520Sewer-ML%2520dataset.%250AExperiments%2520show%2520that%2520the%2520method%2520can%2520improve%2520the%2520performance%2520of%2520low-resolution%250Asewer%2520images%2520in%2520these%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19271v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sewer%20Image%20Super-Resolution%20with%20Depth%20Priors%20and%20Its%20Lightweight%0A%20%20Network&entry.906535625=Gang%20Pan%20and%20Chen%20Wang%20and%20Zhijie%20Sui%20and%20Shuai%20Guo%20and%20Yaozhi%20Lv%20and%20Honglie%20Li%20and%20Di%20Sun%20and%20Zixia%20Xia&entry.1292438233=%20%20The%20Quick-view%20%28QV%29%20technique%20serves%20as%20a%20primary%20method%20for%20detecting%0Adefects%20within%20sewerage%20systems.%20However%2C%20the%20effectiveness%20of%20QV%20is%20impeded%20by%0Athe%20limited%20visual%20range%20of%20its%20hardware%2C%20resulting%20in%20suboptimal%20image%20quality%0Afor%20distant%20portions%20of%20the%20sewer%20network.%20Image%20super-resolution%20is%20an%0Aeffective%20way%20to%20improve%20image%20quality%20and%20has%20been%20applied%20in%20a%20variety%20of%0Ascenes.%20However%2C%20research%20on%20super-resolution%20for%20sewer%20images%20remains%0Aconsiderably%20unexplored.%20In%20response%2C%20this%20study%20leverages%20the%20inherent%20depth%0Arelationships%20present%20within%20QV%20images%20and%20introduces%20a%20novel%20Depth-guided%2C%0AReference-based%20Super-Resolution%20framework%20denoted%20as%20DSRNet.%20It%20comprises%20two%0Acore%20components%3A%20a%20depth%20extraction%20module%20and%20a%20depth%20information%20matching%0Amodule%20%28DMM%29.%20DSRNet%20utilizes%20the%20adjacent%20frames%20of%20the%20low-resolution%20image%0Aas%20reference%20images%20and%20helps%20them%20recover%20texture%20information%20based%20on%20the%0Acorrelation.%20By%20combining%20these%20modules%2C%20the%20integration%20of%20depth%20priors%0Asignificantly%20enhances%20both%20visual%20quality%20and%20performance%20benchmarks.%20Besides%2C%0Ain%20pursuit%20of%20computational%20efficiency%20and%20compactness%2C%20a%20super-resolution%0Aknowledge%20distillation%20model%20based%20on%20an%20attention%20mechanism%20is%20introduced.%0AThis%20mechanism%20facilitates%20the%20acquisition%20of%20feature%20similarity%20between%20a%20more%0Acomplex%20teacher%20model%20and%20a%20streamlined%20student%20model%2C%20with%20the%20latter%20being%20a%0Alightweight%20version%20of%20DSRNet.%20Experimental%20results%20demonstrate%20that%20DSRNet%0Asignificantly%20improves%20PSNR%20and%20SSIM%20compared%20with%20other%20methods.%20This%20study%0Aalso%20conducts%20experiments%20on%20sewer%20defect%20semantic%20segmentation%2C%20object%0Adetection%2C%20and%20classification%20on%20the%20Pipe%20dataset%20and%20Sewer-ML%20dataset.%0AExperiments%20show%20that%20the%20method%20can%20improve%20the%20performance%20of%20low-resolution%0Asewer%20images%20in%20these%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19271v3&entry.124074799=Read"},
{"title": "Signature Isolation Forest", "author": "Marta Campi and Guillaume Staerman and Gareth W. Peters and Tomoko Matsui", "abstract": "  Functional Isolation Forest (FIF) is a recent state-of-the-art Anomaly\nDetection (AD) algorithm designed for functional data. It relies on a tree\npartition procedure where an abnormality score is computed by projecting each\ncurve observation on a drawn dictionary through a linear inner product. Such\nlinear inner product and the dictionary are a priori choices that highly\ninfluence the algorithm's performances and might lead to unreliable results,\nparticularly with complex datasets. This work addresses these challenges by\nintroducing \\textit{Signature Isolation Forest}, a novel AD algorithm class\nleveraging the rough path theory's signature transform. Our objective is to\nremove the constraints imposed by FIF through the proposition of two algorithms\nwhich specifically target the linearity of the FIF inner product and the choice\nof the dictionary. We provide several numerical experiments, including a\nreal-world applications benchmark showing the relevance of our methods.\n", "link": "http://arxiv.org/abs/2403.04405v4", "date": "2025-02-25", "relevancy": 2.2, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4418}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4391}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Signature%20Isolation%20Forest&body=Title%3A%20Signature%20Isolation%20Forest%0AAuthor%3A%20Marta%20Campi%20and%20Guillaume%20Staerman%20and%20Gareth%20W.%20Peters%20and%20Tomoko%20Matsui%0AAbstract%3A%20%20%20Functional%20Isolation%20Forest%20%28FIF%29%20is%20a%20recent%20state-of-the-art%20Anomaly%0ADetection%20%28AD%29%20algorithm%20designed%20for%20functional%20data.%20It%20relies%20on%20a%20tree%0Apartition%20procedure%20where%20an%20abnormality%20score%20is%20computed%20by%20projecting%20each%0Acurve%20observation%20on%20a%20drawn%20dictionary%20through%20a%20linear%20inner%20product.%20Such%0Alinear%20inner%20product%20and%20the%20dictionary%20are%20a%20priori%20choices%20that%20highly%0Ainfluence%20the%20algorithm%27s%20performances%20and%20might%20lead%20to%20unreliable%20results%2C%0Aparticularly%20with%20complex%20datasets.%20This%20work%20addresses%20these%20challenges%20by%0Aintroducing%20%5Ctextit%7BSignature%20Isolation%20Forest%7D%2C%20a%20novel%20AD%20algorithm%20class%0Aleveraging%20the%20rough%20path%20theory%27s%20signature%20transform.%20Our%20objective%20is%20to%0Aremove%20the%20constraints%20imposed%20by%20FIF%20through%20the%20proposition%20of%20two%20algorithms%0Awhich%20specifically%20target%20the%20linearity%20of%20the%20FIF%20inner%20product%20and%20the%20choice%0Aof%20the%20dictionary.%20We%20provide%20several%20numerical%20experiments%2C%20including%20a%0Areal-world%20applications%20benchmark%20showing%20the%20relevance%20of%20our%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04405v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSignature%2520Isolation%2520Forest%26entry.906535625%3DMarta%2520Campi%2520and%2520Guillaume%2520Staerman%2520and%2520Gareth%2520W.%2520Peters%2520and%2520Tomoko%2520Matsui%26entry.1292438233%3D%2520%2520Functional%2520Isolation%2520Forest%2520%2528FIF%2529%2520is%2520a%2520recent%2520state-of-the-art%2520Anomaly%250ADetection%2520%2528AD%2529%2520algorithm%2520designed%2520for%2520functional%2520data.%2520It%2520relies%2520on%2520a%2520tree%250Apartition%2520procedure%2520where%2520an%2520abnormality%2520score%2520is%2520computed%2520by%2520projecting%2520each%250Acurve%2520observation%2520on%2520a%2520drawn%2520dictionary%2520through%2520a%2520linear%2520inner%2520product.%2520Such%250Alinear%2520inner%2520product%2520and%2520the%2520dictionary%2520are%2520a%2520priori%2520choices%2520that%2520highly%250Ainfluence%2520the%2520algorithm%2527s%2520performances%2520and%2520might%2520lead%2520to%2520unreliable%2520results%252C%250Aparticularly%2520with%2520complex%2520datasets.%2520This%2520work%2520addresses%2520these%2520challenges%2520by%250Aintroducing%2520%255Ctextit%257BSignature%2520Isolation%2520Forest%257D%252C%2520a%2520novel%2520AD%2520algorithm%2520class%250Aleveraging%2520the%2520rough%2520path%2520theory%2527s%2520signature%2520transform.%2520Our%2520objective%2520is%2520to%250Aremove%2520the%2520constraints%2520imposed%2520by%2520FIF%2520through%2520the%2520proposition%2520of%2520two%2520algorithms%250Awhich%2520specifically%2520target%2520the%2520linearity%2520of%2520the%2520FIF%2520inner%2520product%2520and%2520the%2520choice%250Aof%2520the%2520dictionary.%2520We%2520provide%2520several%2520numerical%2520experiments%252C%2520including%2520a%250Areal-world%2520applications%2520benchmark%2520showing%2520the%2520relevance%2520of%2520our%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04405v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Signature%20Isolation%20Forest&entry.906535625=Marta%20Campi%20and%20Guillaume%20Staerman%20and%20Gareth%20W.%20Peters%20and%20Tomoko%20Matsui&entry.1292438233=%20%20Functional%20Isolation%20Forest%20%28FIF%29%20is%20a%20recent%20state-of-the-art%20Anomaly%0ADetection%20%28AD%29%20algorithm%20designed%20for%20functional%20data.%20It%20relies%20on%20a%20tree%0Apartition%20procedure%20where%20an%20abnormality%20score%20is%20computed%20by%20projecting%20each%0Acurve%20observation%20on%20a%20drawn%20dictionary%20through%20a%20linear%20inner%20product.%20Such%0Alinear%20inner%20product%20and%20the%20dictionary%20are%20a%20priori%20choices%20that%20highly%0Ainfluence%20the%20algorithm%27s%20performances%20and%20might%20lead%20to%20unreliable%20results%2C%0Aparticularly%20with%20complex%20datasets.%20This%20work%20addresses%20these%20challenges%20by%0Aintroducing%20%5Ctextit%7BSignature%20Isolation%20Forest%7D%2C%20a%20novel%20AD%20algorithm%20class%0Aleveraging%20the%20rough%20path%20theory%27s%20signature%20transform.%20Our%20objective%20is%20to%0Aremove%20the%20constraints%20imposed%20by%20FIF%20through%20the%20proposition%20of%20two%20algorithms%0Awhich%20specifically%20target%20the%20linearity%20of%20the%20FIF%20inner%20product%20and%20the%20choice%0Aof%20the%20dictionary.%20We%20provide%20several%20numerical%20experiments%2C%20including%20a%0Areal-world%20applications%20benchmark%20showing%20the%20relevance%20of%20our%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04405v4&entry.124074799=Read"},
{"title": "Self-Supervised Data Generation for Precision Agriculture: Blending\n  Simulated Environments with Real Imagery", "author": "Leonardo Saraceni and Ionut Marian Motoi and Daniele Nardi and Thomas Alessandro Ciarfuglia", "abstract": "  In precision agriculture, the scarcity of labeled data and significant\ncovariate shifts pose unique challenges for training machine learning models.\nThis scarcity is particularly problematic due to the dynamic nature of the\nenvironment and the evolving appearance of agricultural subjects as living\nthings. We propose a novel system for generating realistic synthetic data to\naddress these challenges. Utilizing a vineyard simulator based on the Unity\nengine, our system employs a cut-and-paste technique with geometrical\nconsistency considerations to produce accurate photo-realistic images and\nlabels from synthetic environments to train detection algorithms. This approach\ngenerates diverse data samples across various viewpoints and lighting\nconditions. We demonstrate considerable performance improvements in training a\nstate-of-the-art detector by applying our method to table grapes cultivation.\nThe combination of techniques can be easily automated, an increasingly\nimportant consideration for adoption in agricultural practice.\n", "link": "http://arxiv.org/abs/2502.18320v1", "date": "2025-02-25", "relevancy": 2.1975, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5568}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5509}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Data%20Generation%20for%20Precision%20Agriculture%3A%20Blending%0A%20%20Simulated%20Environments%20with%20Real%20Imagery&body=Title%3A%20Self-Supervised%20Data%20Generation%20for%20Precision%20Agriculture%3A%20Blending%0A%20%20Simulated%20Environments%20with%20Real%20Imagery%0AAuthor%3A%20Leonardo%20Saraceni%20and%20Ionut%20Marian%20Motoi%20and%20Daniele%20Nardi%20and%20Thomas%20Alessandro%20Ciarfuglia%0AAbstract%3A%20%20%20In%20precision%20agriculture%2C%20the%20scarcity%20of%20labeled%20data%20and%20significant%0Acovariate%20shifts%20pose%20unique%20challenges%20for%20training%20machine%20learning%20models.%0AThis%20scarcity%20is%20particularly%20problematic%20due%20to%20the%20dynamic%20nature%20of%20the%0Aenvironment%20and%20the%20evolving%20appearance%20of%20agricultural%20subjects%20as%20living%0Athings.%20We%20propose%20a%20novel%20system%20for%20generating%20realistic%20synthetic%20data%20to%0Aaddress%20these%20challenges.%20Utilizing%20a%20vineyard%20simulator%20based%20on%20the%20Unity%0Aengine%2C%20our%20system%20employs%20a%20cut-and-paste%20technique%20with%20geometrical%0Aconsistency%20considerations%20to%20produce%20accurate%20photo-realistic%20images%20and%0Alabels%20from%20synthetic%20environments%20to%20train%20detection%20algorithms.%20This%20approach%0Agenerates%20diverse%20data%20samples%20across%20various%20viewpoints%20and%20lighting%0Aconditions.%20We%20demonstrate%20considerable%20performance%20improvements%20in%20training%20a%0Astate-of-the-art%20detector%20by%20applying%20our%20method%20to%20table%20grapes%20cultivation.%0AThe%20combination%20of%20techniques%20can%20be%20easily%20automated%2C%20an%20increasingly%0Aimportant%20consideration%20for%20adoption%20in%20agricultural%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Data%2520Generation%2520for%2520Precision%2520Agriculture%253A%2520Blending%250A%2520%2520Simulated%2520Environments%2520with%2520Real%2520Imagery%26entry.906535625%3DLeonardo%2520Saraceni%2520and%2520Ionut%2520Marian%2520Motoi%2520and%2520Daniele%2520Nardi%2520and%2520Thomas%2520Alessandro%2520Ciarfuglia%26entry.1292438233%3D%2520%2520In%2520precision%2520agriculture%252C%2520the%2520scarcity%2520of%2520labeled%2520data%2520and%2520significant%250Acovariate%2520shifts%2520pose%2520unique%2520challenges%2520for%2520training%2520machine%2520learning%2520models.%250AThis%2520scarcity%2520is%2520particularly%2520problematic%2520due%2520to%2520the%2520dynamic%2520nature%2520of%2520the%250Aenvironment%2520and%2520the%2520evolving%2520appearance%2520of%2520agricultural%2520subjects%2520as%2520living%250Athings.%2520We%2520propose%2520a%2520novel%2520system%2520for%2520generating%2520realistic%2520synthetic%2520data%2520to%250Aaddress%2520these%2520challenges.%2520Utilizing%2520a%2520vineyard%2520simulator%2520based%2520on%2520the%2520Unity%250Aengine%252C%2520our%2520system%2520employs%2520a%2520cut-and-paste%2520technique%2520with%2520geometrical%250Aconsistency%2520considerations%2520to%2520produce%2520accurate%2520photo-realistic%2520images%2520and%250Alabels%2520from%2520synthetic%2520environments%2520to%2520train%2520detection%2520algorithms.%2520This%2520approach%250Agenerates%2520diverse%2520data%2520samples%2520across%2520various%2520viewpoints%2520and%2520lighting%250Aconditions.%2520We%2520demonstrate%2520considerable%2520performance%2520improvements%2520in%2520training%2520a%250Astate-of-the-art%2520detector%2520by%2520applying%2520our%2520method%2520to%2520table%2520grapes%2520cultivation.%250AThe%2520combination%2520of%2520techniques%2520can%2520be%2520easily%2520automated%252C%2520an%2520increasingly%250Aimportant%2520consideration%2520for%2520adoption%2520in%2520agricultural%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Data%20Generation%20for%20Precision%20Agriculture%3A%20Blending%0A%20%20Simulated%20Environments%20with%20Real%20Imagery&entry.906535625=Leonardo%20Saraceni%20and%20Ionut%20Marian%20Motoi%20and%20Daniele%20Nardi%20and%20Thomas%20Alessandro%20Ciarfuglia&entry.1292438233=%20%20In%20precision%20agriculture%2C%20the%20scarcity%20of%20labeled%20data%20and%20significant%0Acovariate%20shifts%20pose%20unique%20challenges%20for%20training%20machine%20learning%20models.%0AThis%20scarcity%20is%20particularly%20problematic%20due%20to%20the%20dynamic%20nature%20of%20the%0Aenvironment%20and%20the%20evolving%20appearance%20of%20agricultural%20subjects%20as%20living%0Athings.%20We%20propose%20a%20novel%20system%20for%20generating%20realistic%20synthetic%20data%20to%0Aaddress%20these%20challenges.%20Utilizing%20a%20vineyard%20simulator%20based%20on%20the%20Unity%0Aengine%2C%20our%20system%20employs%20a%20cut-and-paste%20technique%20with%20geometrical%0Aconsistency%20considerations%20to%20produce%20accurate%20photo-realistic%20images%20and%0Alabels%20from%20synthetic%20environments%20to%20train%20detection%20algorithms.%20This%20approach%0Agenerates%20diverse%20data%20samples%20across%20various%20viewpoints%20and%20lighting%0Aconditions.%20We%20demonstrate%20considerable%20performance%20improvements%20in%20training%20a%0Astate-of-the-art%20detector%20by%20applying%20our%20method%20to%20table%20grapes%20cultivation.%0AThe%20combination%20of%20techniques%20can%20be%20easily%20automated%2C%20an%20increasingly%0Aimportant%20consideration%20for%20adoption%20in%20agricultural%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18320v1&entry.124074799=Read"},
{"title": "Small Language Models: Survey, Measurements, and Insights", "author": "Zhenyan Lu and Xiang Li and Dongqi Cai and Rongjie Yi and Fangming Liu and Xiwen Zhang and Nicholas D. Lane and Mengwei Xu", "abstract": "  Small language models (SLMs), despite their widespread adoption in modern\nsmart devices, have received significantly less academic attention compared to\ntheir large language model (LLM) counterparts, which are predominantly deployed\nin data centers and cloud environments. While researchers continue to improve\nthe capabilities of LLMs in the pursuit of artificial general intelligence, SLM\nresearch aims to make machine intelligence more accessible, affordable, and\nefficient for everyday tasks. Focusing on transformer-based, decoder-only\nlanguage models with 100M-5B parameters, we survey 70 state-of-the-art\nopen-source SLMs, analyzing their technical innovations across three axes:\narchitectures, training datasets, and training algorithms. In addition, we\nevaluate their capabilities in various domains, including commonsense\nreasoning, in-context learning, mathematics, and coding. To gain further\ninsight into their on-device runtime costs, we benchmark their inference\nlatency and memory footprints. Through in-depth analysis of our benchmarking\ndata, we offer valuable insights to advance research in this field.\n", "link": "http://arxiv.org/abs/2409.15790v2", "date": "2025-02-25", "relevancy": 2.1919, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5539}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5539}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Small%20Language%20Models%3A%20Survey%2C%20Measurements%2C%20and%20Insights&body=Title%3A%20Small%20Language%20Models%3A%20Survey%2C%20Measurements%2C%20and%20Insights%0AAuthor%3A%20Zhenyan%20Lu%20and%20Xiang%20Li%20and%20Dongqi%20Cai%20and%20Rongjie%20Yi%20and%20Fangming%20Liu%20and%20Xiwen%20Zhang%20and%20Nicholas%20D.%20Lane%20and%20Mengwei%20Xu%0AAbstract%3A%20%20%20Small%20language%20models%20%28SLMs%29%2C%20despite%20their%20widespread%20adoption%20in%20modern%0Asmart%20devices%2C%20have%20received%20significantly%20less%20academic%20attention%20compared%20to%0Atheir%20large%20language%20model%20%28LLM%29%20counterparts%2C%20which%20are%20predominantly%20deployed%0Ain%20data%20centers%20and%20cloud%20environments.%20While%20researchers%20continue%20to%20improve%0Athe%20capabilities%20of%20LLMs%20in%20the%20pursuit%20of%20artificial%20general%20intelligence%2C%20SLM%0Aresearch%20aims%20to%20make%20machine%20intelligence%20more%20accessible%2C%20affordable%2C%20and%0Aefficient%20for%20everyday%20tasks.%20Focusing%20on%20transformer-based%2C%20decoder-only%0Alanguage%20models%20with%20100M-5B%20parameters%2C%20we%20survey%2070%20state-of-the-art%0Aopen-source%20SLMs%2C%20analyzing%20their%20technical%20innovations%20across%20three%20axes%3A%0Aarchitectures%2C%20training%20datasets%2C%20and%20training%20algorithms.%20In%20addition%2C%20we%0Aevaluate%20their%20capabilities%20in%20various%20domains%2C%20including%20commonsense%0Areasoning%2C%20in-context%20learning%2C%20mathematics%2C%20and%20coding.%20To%20gain%20further%0Ainsight%20into%20their%20on-device%20runtime%20costs%2C%20we%20benchmark%20their%20inference%0Alatency%20and%20memory%20footprints.%20Through%20in-depth%20analysis%20of%20our%20benchmarking%0Adata%2C%20we%20offer%20valuable%20insights%20to%20advance%20research%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15790v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmall%2520Language%2520Models%253A%2520Survey%252C%2520Measurements%252C%2520and%2520Insights%26entry.906535625%3DZhenyan%2520Lu%2520and%2520Xiang%2520Li%2520and%2520Dongqi%2520Cai%2520and%2520Rongjie%2520Yi%2520and%2520Fangming%2520Liu%2520and%2520Xiwen%2520Zhang%2520and%2520Nicholas%2520D.%2520Lane%2520and%2520Mengwei%2520Xu%26entry.1292438233%3D%2520%2520Small%2520language%2520models%2520%2528SLMs%2529%252C%2520despite%2520their%2520widespread%2520adoption%2520in%2520modern%250Asmart%2520devices%252C%2520have%2520received%2520significantly%2520less%2520academic%2520attention%2520compared%2520to%250Atheir%2520large%2520language%2520model%2520%2528LLM%2529%2520counterparts%252C%2520which%2520are%2520predominantly%2520deployed%250Ain%2520data%2520centers%2520and%2520cloud%2520environments.%2520While%2520researchers%2520continue%2520to%2520improve%250Athe%2520capabilities%2520of%2520LLMs%2520in%2520the%2520pursuit%2520of%2520artificial%2520general%2520intelligence%252C%2520SLM%250Aresearch%2520aims%2520to%2520make%2520machine%2520intelligence%2520more%2520accessible%252C%2520affordable%252C%2520and%250Aefficient%2520for%2520everyday%2520tasks.%2520Focusing%2520on%2520transformer-based%252C%2520decoder-only%250Alanguage%2520models%2520with%2520100M-5B%2520parameters%252C%2520we%2520survey%252070%2520state-of-the-art%250Aopen-source%2520SLMs%252C%2520analyzing%2520their%2520technical%2520innovations%2520across%2520three%2520axes%253A%250Aarchitectures%252C%2520training%2520datasets%252C%2520and%2520training%2520algorithms.%2520In%2520addition%252C%2520we%250Aevaluate%2520their%2520capabilities%2520in%2520various%2520domains%252C%2520including%2520commonsense%250Areasoning%252C%2520in-context%2520learning%252C%2520mathematics%252C%2520and%2520coding.%2520To%2520gain%2520further%250Ainsight%2520into%2520their%2520on-device%2520runtime%2520costs%252C%2520we%2520benchmark%2520their%2520inference%250Alatency%2520and%2520memory%2520footprints.%2520Through%2520in-depth%2520analysis%2520of%2520our%2520benchmarking%250Adata%252C%2520we%2520offer%2520valuable%2520insights%2520to%2520advance%2520research%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15790v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Small%20Language%20Models%3A%20Survey%2C%20Measurements%2C%20and%20Insights&entry.906535625=Zhenyan%20Lu%20and%20Xiang%20Li%20and%20Dongqi%20Cai%20and%20Rongjie%20Yi%20and%20Fangming%20Liu%20and%20Xiwen%20Zhang%20and%20Nicholas%20D.%20Lane%20and%20Mengwei%20Xu&entry.1292438233=%20%20Small%20language%20models%20%28SLMs%29%2C%20despite%20their%20widespread%20adoption%20in%20modern%0Asmart%20devices%2C%20have%20received%20significantly%20less%20academic%20attention%20compared%20to%0Atheir%20large%20language%20model%20%28LLM%29%20counterparts%2C%20which%20are%20predominantly%20deployed%0Ain%20data%20centers%20and%20cloud%20environments.%20While%20researchers%20continue%20to%20improve%0Athe%20capabilities%20of%20LLMs%20in%20the%20pursuit%20of%20artificial%20general%20intelligence%2C%20SLM%0Aresearch%20aims%20to%20make%20machine%20intelligence%20more%20accessible%2C%20affordable%2C%20and%0Aefficient%20for%20everyday%20tasks.%20Focusing%20on%20transformer-based%2C%20decoder-only%0Alanguage%20models%20with%20100M-5B%20parameters%2C%20we%20survey%2070%20state-of-the-art%0Aopen-source%20SLMs%2C%20analyzing%20their%20technical%20innovations%20across%20three%20axes%3A%0Aarchitectures%2C%20training%20datasets%2C%20and%20training%20algorithms.%20In%20addition%2C%20we%0Aevaluate%20their%20capabilities%20in%20various%20domains%2C%20including%20commonsense%0Areasoning%2C%20in-context%20learning%2C%20mathematics%2C%20and%20coding.%20To%20gain%20further%0Ainsight%20into%20their%20on-device%20runtime%20costs%2C%20we%20benchmark%20their%20inference%0Alatency%20and%20memory%20footprints.%20Through%20in-depth%20analysis%20of%20our%20benchmarking%0Adata%2C%20we%20offer%20valuable%20insights%20to%20advance%20research%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15790v2&entry.124074799=Read"},
{"title": "Neural Network Graph Similarity Computation Based on Graph Fusion", "author": "Zenghui Chang and Yiqiao Zhang and Hong Cai Chen", "abstract": "  Graph similarity learning, crucial for tasks such as graph classification and\nsimilarity search, focuses on measuring the similarity between two\ngraph-structured entities. The core challenge in this field is effectively\nmanaging the interactions between graphs. Traditional methods often entail\nseparate, redundant computations for each graph pair, leading to unnecessary\ncomplexity. This paper revolutionizes the approach by introducing a parallel\ngraph interaction method called graph fusion. By merging the node sequences of\ngraph pairs into a single large graph, our method leverages a global attention\nmechanism to facilitate interaction computations and to harvest cross-graph\ninsights. We further assess the similarity between graph pairs at two distinct\nlevels-graph-level and node-level-introducing two innovative, yet\nstraightforward, similarity computation algorithms. Extensive testing across\nfive public datasets shows that our model not only outperforms leading baseline\nmodels in graph-to-graph classification and regression tasks but also sets a\nnew benchmark for performance and efficiency. The code for this paper is\nopen-source and available at https://github.com/LLiRarry/GFM-code.git\n", "link": "http://arxiv.org/abs/2502.18291v1", "date": "2025-02-25", "relevancy": 2.1901, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4432}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4361}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Network%20Graph%20Similarity%20Computation%20Based%20on%20Graph%20Fusion&body=Title%3A%20Neural%20Network%20Graph%20Similarity%20Computation%20Based%20on%20Graph%20Fusion%0AAuthor%3A%20Zenghui%20Chang%20and%20Yiqiao%20Zhang%20and%20Hong%20Cai%20Chen%0AAbstract%3A%20%20%20Graph%20similarity%20learning%2C%20crucial%20for%20tasks%20such%20as%20graph%20classification%20and%0Asimilarity%20search%2C%20focuses%20on%20measuring%20the%20similarity%20between%20two%0Agraph-structured%20entities.%20The%20core%20challenge%20in%20this%20field%20is%20effectively%0Amanaging%20the%20interactions%20between%20graphs.%20Traditional%20methods%20often%20entail%0Aseparate%2C%20redundant%20computations%20for%20each%20graph%20pair%2C%20leading%20to%20unnecessary%0Acomplexity.%20This%20paper%20revolutionizes%20the%20approach%20by%20introducing%20a%20parallel%0Agraph%20interaction%20method%20called%20graph%20fusion.%20By%20merging%20the%20node%20sequences%20of%0Agraph%20pairs%20into%20a%20single%20large%20graph%2C%20our%20method%20leverages%20a%20global%20attention%0Amechanism%20to%20facilitate%20interaction%20computations%20and%20to%20harvest%20cross-graph%0Ainsights.%20We%20further%20assess%20the%20similarity%20between%20graph%20pairs%20at%20two%20distinct%0Alevels-graph-level%20and%20node-level-introducing%20two%20innovative%2C%20yet%0Astraightforward%2C%20similarity%20computation%20algorithms.%20Extensive%20testing%20across%0Afive%20public%20datasets%20shows%20that%20our%20model%20not%20only%20outperforms%20leading%20baseline%0Amodels%20in%20graph-to-graph%20classification%20and%20regression%20tasks%20but%20also%20sets%20a%0Anew%20benchmark%20for%20performance%20and%20efficiency.%20The%20code%20for%20this%20paper%20is%0Aopen-source%20and%20available%20at%20https%3A//github.com/LLiRarry/GFM-code.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Network%2520Graph%2520Similarity%2520Computation%2520Based%2520on%2520Graph%2520Fusion%26entry.906535625%3DZenghui%2520Chang%2520and%2520Yiqiao%2520Zhang%2520and%2520Hong%2520Cai%2520Chen%26entry.1292438233%3D%2520%2520Graph%2520similarity%2520learning%252C%2520crucial%2520for%2520tasks%2520such%2520as%2520graph%2520classification%2520and%250Asimilarity%2520search%252C%2520focuses%2520on%2520measuring%2520the%2520similarity%2520between%2520two%250Agraph-structured%2520entities.%2520The%2520core%2520challenge%2520in%2520this%2520field%2520is%2520effectively%250Amanaging%2520the%2520interactions%2520between%2520graphs.%2520Traditional%2520methods%2520often%2520entail%250Aseparate%252C%2520redundant%2520computations%2520for%2520each%2520graph%2520pair%252C%2520leading%2520to%2520unnecessary%250Acomplexity.%2520This%2520paper%2520revolutionizes%2520the%2520approach%2520by%2520introducing%2520a%2520parallel%250Agraph%2520interaction%2520method%2520called%2520graph%2520fusion.%2520By%2520merging%2520the%2520node%2520sequences%2520of%250Agraph%2520pairs%2520into%2520a%2520single%2520large%2520graph%252C%2520our%2520method%2520leverages%2520a%2520global%2520attention%250Amechanism%2520to%2520facilitate%2520interaction%2520computations%2520and%2520to%2520harvest%2520cross-graph%250Ainsights.%2520We%2520further%2520assess%2520the%2520similarity%2520between%2520graph%2520pairs%2520at%2520two%2520distinct%250Alevels-graph-level%2520and%2520node-level-introducing%2520two%2520innovative%252C%2520yet%250Astraightforward%252C%2520similarity%2520computation%2520algorithms.%2520Extensive%2520testing%2520across%250Afive%2520public%2520datasets%2520shows%2520that%2520our%2520model%2520not%2520only%2520outperforms%2520leading%2520baseline%250Amodels%2520in%2520graph-to-graph%2520classification%2520and%2520regression%2520tasks%2520but%2520also%2520sets%2520a%250Anew%2520benchmark%2520for%2520performance%2520and%2520efficiency.%2520The%2520code%2520for%2520this%2520paper%2520is%250Aopen-source%2520and%2520available%2520at%2520https%253A//github.com/LLiRarry/GFM-code.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Network%20Graph%20Similarity%20Computation%20Based%20on%20Graph%20Fusion&entry.906535625=Zenghui%20Chang%20and%20Yiqiao%20Zhang%20and%20Hong%20Cai%20Chen&entry.1292438233=%20%20Graph%20similarity%20learning%2C%20crucial%20for%20tasks%20such%20as%20graph%20classification%20and%0Asimilarity%20search%2C%20focuses%20on%20measuring%20the%20similarity%20between%20two%0Agraph-structured%20entities.%20The%20core%20challenge%20in%20this%20field%20is%20effectively%0Amanaging%20the%20interactions%20between%20graphs.%20Traditional%20methods%20often%20entail%0Aseparate%2C%20redundant%20computations%20for%20each%20graph%20pair%2C%20leading%20to%20unnecessary%0Acomplexity.%20This%20paper%20revolutionizes%20the%20approach%20by%20introducing%20a%20parallel%0Agraph%20interaction%20method%20called%20graph%20fusion.%20By%20merging%20the%20node%20sequences%20of%0Agraph%20pairs%20into%20a%20single%20large%20graph%2C%20our%20method%20leverages%20a%20global%20attention%0Amechanism%20to%20facilitate%20interaction%20computations%20and%20to%20harvest%20cross-graph%0Ainsights.%20We%20further%20assess%20the%20similarity%20between%20graph%20pairs%20at%20two%20distinct%0Alevels-graph-level%20and%20node-level-introducing%20two%20innovative%2C%20yet%0Astraightforward%2C%20similarity%20computation%20algorithms.%20Extensive%20testing%20across%0Afive%20public%20datasets%20shows%20that%20our%20model%20not%20only%20outperforms%20leading%20baseline%0Amodels%20in%20graph-to-graph%20classification%20and%20regression%20tasks%20but%20also%20sets%20a%0Anew%20benchmark%20for%20performance%20and%20efficiency.%20The%20code%20for%20this%20paper%20is%0Aopen-source%20and%20available%20at%20https%3A//github.com/LLiRarry/GFM-code.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18291v1&entry.124074799=Read"},
{"title": "HAIFAI: Human-AI Interaction for Mental Face Reconstruction", "author": "Florian Strohm and Mihai B\u00e2ce and Andreas Bulling", "abstract": "  We present HAIFAI - a novel two-stage system where humans and AI interact to\ntackle the challenging task of reconstructing a visual representation of a face\nthat exists only in a person's mind. In the first stage, users iteratively rank\nimages our reconstruction system presents based on their resemblance to a\nmental image. These rankings, in turn, allow the system to extract relevant\nimage features, fuse them into a unified feature vector, and use a generative\nmodel to produce an initial reconstruction of the mental image. The second\nstage leverages an existing face editing method, allowing users to manually\nrefine and further improve this reconstruction using an easy-to-use slider\ninterface for face shape manipulation. To avoid the need for tedious human data\ncollection for training the reconstruction system, we introduce a computational\nuser model of human ranking behaviour. For this, we collected a small face\nranking dataset through an online crowd-sourcing study containing data from 275\nparticipants. We evaluate HAIFAI and an ablated version in a 12-participant\nuser study and demonstrate that our approach outperforms the previous state of\nthe art regarding reconstruction quality, usability, perceived workload, and\nreconstruction speed. We further validate the reconstructions in a subsequent\nface ranking study with 18 participants and show that HAIFAI achieves a new\nstate-of-the-art identification rate of 60.6%. These findings represent a\nsignificant advancement towards developing new interactive intelligent systems\ncapable of reliably and effortlessly reconstructing a user's mental image.\n", "link": "http://arxiv.org/abs/2412.06323v2", "date": "2025-02-25", "relevancy": 2.1884, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.555}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5438}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAIFAI%3A%20Human-AI%20Interaction%20for%20Mental%20Face%20Reconstruction&body=Title%3A%20HAIFAI%3A%20Human-AI%20Interaction%20for%20Mental%20Face%20Reconstruction%0AAuthor%3A%20Florian%20Strohm%20and%20Mihai%20B%C3%A2ce%20and%20Andreas%20Bulling%0AAbstract%3A%20%20%20We%20present%20HAIFAI%20-%20a%20novel%20two-stage%20system%20where%20humans%20and%20AI%20interact%20to%0Atackle%20the%20challenging%20task%20of%20reconstructing%20a%20visual%20representation%20of%20a%20face%0Athat%20exists%20only%20in%20a%20person%27s%20mind.%20In%20the%20first%20stage%2C%20users%20iteratively%20rank%0Aimages%20our%20reconstruction%20system%20presents%20based%20on%20their%20resemblance%20to%20a%0Amental%20image.%20These%20rankings%2C%20in%20turn%2C%20allow%20the%20system%20to%20extract%20relevant%0Aimage%20features%2C%20fuse%20them%20into%20a%20unified%20feature%20vector%2C%20and%20use%20a%20generative%0Amodel%20to%20produce%20an%20initial%20reconstruction%20of%20the%20mental%20image.%20The%20second%0Astage%20leverages%20an%20existing%20face%20editing%20method%2C%20allowing%20users%20to%20manually%0Arefine%20and%20further%20improve%20this%20reconstruction%20using%20an%20easy-to-use%20slider%0Ainterface%20for%20face%20shape%20manipulation.%20To%20avoid%20the%20need%20for%20tedious%20human%20data%0Acollection%20for%20training%20the%20reconstruction%20system%2C%20we%20introduce%20a%20computational%0Auser%20model%20of%20human%20ranking%20behaviour.%20For%20this%2C%20we%20collected%20a%20small%20face%0Aranking%20dataset%20through%20an%20online%20crowd-sourcing%20study%20containing%20data%20from%20275%0Aparticipants.%20We%20evaluate%20HAIFAI%20and%20an%20ablated%20version%20in%20a%2012-participant%0Auser%20study%20and%20demonstrate%20that%20our%20approach%20outperforms%20the%20previous%20state%20of%0Athe%20art%20regarding%20reconstruction%20quality%2C%20usability%2C%20perceived%20workload%2C%20and%0Areconstruction%20speed.%20We%20further%20validate%20the%20reconstructions%20in%20a%20subsequent%0Aface%20ranking%20study%20with%2018%20participants%20and%20show%20that%20HAIFAI%20achieves%20a%20new%0Astate-of-the-art%20identification%20rate%20of%2060.6%25.%20These%20findings%20represent%20a%0Asignificant%20advancement%20towards%20developing%20new%20interactive%20intelligent%20systems%0Acapable%20of%20reliably%20and%20effortlessly%20reconstructing%20a%20user%27s%20mental%20image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06323v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAIFAI%253A%2520Human-AI%2520Interaction%2520for%2520Mental%2520Face%2520Reconstruction%26entry.906535625%3DFlorian%2520Strohm%2520and%2520Mihai%2520B%25C3%25A2ce%2520and%2520Andreas%2520Bulling%26entry.1292438233%3D%2520%2520We%2520present%2520HAIFAI%2520-%2520a%2520novel%2520two-stage%2520system%2520where%2520humans%2520and%2520AI%2520interact%2520to%250Atackle%2520the%2520challenging%2520task%2520of%2520reconstructing%2520a%2520visual%2520representation%2520of%2520a%2520face%250Athat%2520exists%2520only%2520in%2520a%2520person%2527s%2520mind.%2520In%2520the%2520first%2520stage%252C%2520users%2520iteratively%2520rank%250Aimages%2520our%2520reconstruction%2520system%2520presents%2520based%2520on%2520their%2520resemblance%2520to%2520a%250Amental%2520image.%2520These%2520rankings%252C%2520in%2520turn%252C%2520allow%2520the%2520system%2520to%2520extract%2520relevant%250Aimage%2520features%252C%2520fuse%2520them%2520into%2520a%2520unified%2520feature%2520vector%252C%2520and%2520use%2520a%2520generative%250Amodel%2520to%2520produce%2520an%2520initial%2520reconstruction%2520of%2520the%2520mental%2520image.%2520The%2520second%250Astage%2520leverages%2520an%2520existing%2520face%2520editing%2520method%252C%2520allowing%2520users%2520to%2520manually%250Arefine%2520and%2520further%2520improve%2520this%2520reconstruction%2520using%2520an%2520easy-to-use%2520slider%250Ainterface%2520for%2520face%2520shape%2520manipulation.%2520To%2520avoid%2520the%2520need%2520for%2520tedious%2520human%2520data%250Acollection%2520for%2520training%2520the%2520reconstruction%2520system%252C%2520we%2520introduce%2520a%2520computational%250Auser%2520model%2520of%2520human%2520ranking%2520behaviour.%2520For%2520this%252C%2520we%2520collected%2520a%2520small%2520face%250Aranking%2520dataset%2520through%2520an%2520online%2520crowd-sourcing%2520study%2520containing%2520data%2520from%2520275%250Aparticipants.%2520We%2520evaluate%2520HAIFAI%2520and%2520an%2520ablated%2520version%2520in%2520a%252012-participant%250Auser%2520study%2520and%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520the%2520previous%2520state%2520of%250Athe%2520art%2520regarding%2520reconstruction%2520quality%252C%2520usability%252C%2520perceived%2520workload%252C%2520and%250Areconstruction%2520speed.%2520We%2520further%2520validate%2520the%2520reconstructions%2520in%2520a%2520subsequent%250Aface%2520ranking%2520study%2520with%252018%2520participants%2520and%2520show%2520that%2520HAIFAI%2520achieves%2520a%2520new%250Astate-of-the-art%2520identification%2520rate%2520of%252060.6%2525.%2520These%2520findings%2520represent%2520a%250Asignificant%2520advancement%2520towards%2520developing%2520new%2520interactive%2520intelligent%2520systems%250Acapable%2520of%2520reliably%2520and%2520effortlessly%2520reconstructing%2520a%2520user%2527s%2520mental%2520image.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06323v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAIFAI%3A%20Human-AI%20Interaction%20for%20Mental%20Face%20Reconstruction&entry.906535625=Florian%20Strohm%20and%20Mihai%20B%C3%A2ce%20and%20Andreas%20Bulling&entry.1292438233=%20%20We%20present%20HAIFAI%20-%20a%20novel%20two-stage%20system%20where%20humans%20and%20AI%20interact%20to%0Atackle%20the%20challenging%20task%20of%20reconstructing%20a%20visual%20representation%20of%20a%20face%0Athat%20exists%20only%20in%20a%20person%27s%20mind.%20In%20the%20first%20stage%2C%20users%20iteratively%20rank%0Aimages%20our%20reconstruction%20system%20presents%20based%20on%20their%20resemblance%20to%20a%0Amental%20image.%20These%20rankings%2C%20in%20turn%2C%20allow%20the%20system%20to%20extract%20relevant%0Aimage%20features%2C%20fuse%20them%20into%20a%20unified%20feature%20vector%2C%20and%20use%20a%20generative%0Amodel%20to%20produce%20an%20initial%20reconstruction%20of%20the%20mental%20image.%20The%20second%0Astage%20leverages%20an%20existing%20face%20editing%20method%2C%20allowing%20users%20to%20manually%0Arefine%20and%20further%20improve%20this%20reconstruction%20using%20an%20easy-to-use%20slider%0Ainterface%20for%20face%20shape%20manipulation.%20To%20avoid%20the%20need%20for%20tedious%20human%20data%0Acollection%20for%20training%20the%20reconstruction%20system%2C%20we%20introduce%20a%20computational%0Auser%20model%20of%20human%20ranking%20behaviour.%20For%20this%2C%20we%20collected%20a%20small%20face%0Aranking%20dataset%20through%20an%20online%20crowd-sourcing%20study%20containing%20data%20from%20275%0Aparticipants.%20We%20evaluate%20HAIFAI%20and%20an%20ablated%20version%20in%20a%2012-participant%0Auser%20study%20and%20demonstrate%20that%20our%20approach%20outperforms%20the%20previous%20state%20of%0Athe%20art%20regarding%20reconstruction%20quality%2C%20usability%2C%20perceived%20workload%2C%20and%0Areconstruction%20speed.%20We%20further%20validate%20the%20reconstructions%20in%20a%20subsequent%0Aface%20ranking%20study%20with%2018%20participants%20and%20show%20that%20HAIFAI%20achieves%20a%20new%0Astate-of-the-art%20identification%20rate%20of%2060.6%25.%20These%20findings%20represent%20a%0Asignificant%20advancement%20towards%20developing%20new%20interactive%20intelligent%20systems%0Acapable%20of%20reliably%20and%20effortlessly%20reconstructing%20a%20user%27s%20mental%20image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06323v2&entry.124074799=Read"},
{"title": "Controlling dynamics of stochastic systems with deep reinforcement\n  learning", "author": "Ruslan Mukhamadiarov", "abstract": "  A properly designed controller can help improve the quality of experimental\nmeasurements or force a dynamical system to follow a completely new\ntime-evolution path. Recent developments in deep reinforcement learning have\nmade steep advances toward designing effective control schemes for fairly\ncomplex systems. However, a general simulation scheme that employs deep\nreinforcement learning for exerting control in stochastic systems is yet to be\nestablished. In this paper, we attempt to further bridge a gap between control\ntheory and deep reinforcement learning by proposing a simulation algorithm that\nallows achieving control of the dynamics of stochastic systems through the use\nof trained artificial neural networks. Specifically, we use agent-based\nsimulations where the neural network plays the role of the controller that\ndrives local state-to-state transitions. We demonstrate the workflow and the\neffectiveness of the proposed control methods by considering the following two\nstochastic processes: particle coalescence on a lattice and a totally\nasymmetric exclusion process.\n", "link": "http://arxiv.org/abs/2502.18111v1", "date": "2025-02-25", "relevancy": 2.1876, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5882}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5223}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controlling%20dynamics%20of%20stochastic%20systems%20with%20deep%20reinforcement%0A%20%20learning&body=Title%3A%20Controlling%20dynamics%20of%20stochastic%20systems%20with%20deep%20reinforcement%0A%20%20learning%0AAuthor%3A%20Ruslan%20Mukhamadiarov%0AAbstract%3A%20%20%20A%20properly%20designed%20controller%20can%20help%20improve%20the%20quality%20of%20experimental%0Ameasurements%20or%20force%20a%20dynamical%20system%20to%20follow%20a%20completely%20new%0Atime-evolution%20path.%20Recent%20developments%20in%20deep%20reinforcement%20learning%20have%0Amade%20steep%20advances%20toward%20designing%20effective%20control%20schemes%20for%20fairly%0Acomplex%20systems.%20However%2C%20a%20general%20simulation%20scheme%20that%20employs%20deep%0Areinforcement%20learning%20for%20exerting%20control%20in%20stochastic%20systems%20is%20yet%20to%20be%0Aestablished.%20In%20this%20paper%2C%20we%20attempt%20to%20further%20bridge%20a%20gap%20between%20control%0Atheory%20and%20deep%20reinforcement%20learning%20by%20proposing%20a%20simulation%20algorithm%20that%0Aallows%20achieving%20control%20of%20the%20dynamics%20of%20stochastic%20systems%20through%20the%20use%0Aof%20trained%20artificial%20neural%20networks.%20Specifically%2C%20we%20use%20agent-based%0Asimulations%20where%20the%20neural%20network%20plays%20the%20role%20of%20the%20controller%20that%0Adrives%20local%20state-to-state%20transitions.%20We%20demonstrate%20the%20workflow%20and%20the%0Aeffectiveness%20of%20the%20proposed%20control%20methods%20by%20considering%20the%20following%20two%0Astochastic%20processes%3A%20particle%20coalescence%20on%20a%20lattice%20and%20a%20totally%0Aasymmetric%20exclusion%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControlling%2520dynamics%2520of%2520stochastic%2520systems%2520with%2520deep%2520reinforcement%250A%2520%2520learning%26entry.906535625%3DRuslan%2520Mukhamadiarov%26entry.1292438233%3D%2520%2520A%2520properly%2520designed%2520controller%2520can%2520help%2520improve%2520the%2520quality%2520of%2520experimental%250Ameasurements%2520or%2520force%2520a%2520dynamical%2520system%2520to%2520follow%2520a%2520completely%2520new%250Atime-evolution%2520path.%2520Recent%2520developments%2520in%2520deep%2520reinforcement%2520learning%2520have%250Amade%2520steep%2520advances%2520toward%2520designing%2520effective%2520control%2520schemes%2520for%2520fairly%250Acomplex%2520systems.%2520However%252C%2520a%2520general%2520simulation%2520scheme%2520that%2520employs%2520deep%250Areinforcement%2520learning%2520for%2520exerting%2520control%2520in%2520stochastic%2520systems%2520is%2520yet%2520to%2520be%250Aestablished.%2520In%2520this%2520paper%252C%2520we%2520attempt%2520to%2520further%2520bridge%2520a%2520gap%2520between%2520control%250Atheory%2520and%2520deep%2520reinforcement%2520learning%2520by%2520proposing%2520a%2520simulation%2520algorithm%2520that%250Aallows%2520achieving%2520control%2520of%2520the%2520dynamics%2520of%2520stochastic%2520systems%2520through%2520the%2520use%250Aof%2520trained%2520artificial%2520neural%2520networks.%2520Specifically%252C%2520we%2520use%2520agent-based%250Asimulations%2520where%2520the%2520neural%2520network%2520plays%2520the%2520role%2520of%2520the%2520controller%2520that%250Adrives%2520local%2520state-to-state%2520transitions.%2520We%2520demonstrate%2520the%2520workflow%2520and%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520control%2520methods%2520by%2520considering%2520the%2520following%2520two%250Astochastic%2520processes%253A%2520particle%2520coalescence%2520on%2520a%2520lattice%2520and%2520a%2520totally%250Aasymmetric%2520exclusion%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controlling%20dynamics%20of%20stochastic%20systems%20with%20deep%20reinforcement%0A%20%20learning&entry.906535625=Ruslan%20Mukhamadiarov&entry.1292438233=%20%20A%20properly%20designed%20controller%20can%20help%20improve%20the%20quality%20of%20experimental%0Ameasurements%20or%20force%20a%20dynamical%20system%20to%20follow%20a%20completely%20new%0Atime-evolution%20path.%20Recent%20developments%20in%20deep%20reinforcement%20learning%20have%0Amade%20steep%20advances%20toward%20designing%20effective%20control%20schemes%20for%20fairly%0Acomplex%20systems.%20However%2C%20a%20general%20simulation%20scheme%20that%20employs%20deep%0Areinforcement%20learning%20for%20exerting%20control%20in%20stochastic%20systems%20is%20yet%20to%20be%0Aestablished.%20In%20this%20paper%2C%20we%20attempt%20to%20further%20bridge%20a%20gap%20between%20control%0Atheory%20and%20deep%20reinforcement%20learning%20by%20proposing%20a%20simulation%20algorithm%20that%0Aallows%20achieving%20control%20of%20the%20dynamics%20of%20stochastic%20systems%20through%20the%20use%0Aof%20trained%20artificial%20neural%20networks.%20Specifically%2C%20we%20use%20agent-based%0Asimulations%20where%20the%20neural%20network%20plays%20the%20role%20of%20the%20controller%20that%0Adrives%20local%20state-to-state%20transitions.%20We%20demonstrate%20the%20workflow%20and%20the%0Aeffectiveness%20of%20the%20proposed%20control%20methods%20by%20considering%20the%20following%20two%0Astochastic%20processes%3A%20particle%20coalescence%20on%20a%20lattice%20and%20a%20totally%0Aasymmetric%20exclusion%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18111v1&entry.124074799=Read"},
{"title": "DBR: Divergence-Based Regularization for Debiasing Natural Language\n  Understanding Models", "author": "Zihao Li and Ruixiang Tang and Lu Cheng and Shuaiqiang Wang and Dawei Yin and Mengnan Du", "abstract": "  Pre-trained language models (PLMs) have achieved impressive results on\nvarious natural language processing tasks. However, recent research has\nrevealed that these models often rely on superficial features and shortcuts\ninstead of developing a genuine understanding of language, especially for\nnatural language understanding (NLU) tasks. Consequently, the models struggle\nto generalize to out-of-domain data. In this work, we propose Divergence Based\nRegularization (DBR) to mitigate this shortcut learning behavior. Our method\nmeasures the divergence between the output distributions for original examples\nand examples where shortcut tokens have been masked. This process prevents the\nmodel's predictions from being overly influenced by shortcut features or\nbiases. We evaluate our model on three NLU tasks and find that it improves\nout-of-domain performance with little loss of in-domain accuracy. Our results\ndemonstrate that reducing the reliance on shortcuts and superficial features\ncan enhance the generalization ability of large pre-trained language models.\n", "link": "http://arxiv.org/abs/2502.18353v1", "date": "2025-02-25", "relevancy": 2.1786, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5482}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5482}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DBR%3A%20Divergence-Based%20Regularization%20for%20Debiasing%20Natural%20Language%0A%20%20Understanding%20Models&body=Title%3A%20DBR%3A%20Divergence-Based%20Regularization%20for%20Debiasing%20Natural%20Language%0A%20%20Understanding%20Models%0AAuthor%3A%20Zihao%20Li%20and%20Ruixiang%20Tang%20and%20Lu%20Cheng%20and%20Shuaiqiang%20Wang%20and%20Dawei%20Yin%20and%20Mengnan%20Du%0AAbstract%3A%20%20%20Pre-trained%20language%20models%20%28PLMs%29%20have%20achieved%20impressive%20results%20on%0Avarious%20natural%20language%20processing%20tasks.%20However%2C%20recent%20research%20has%0Arevealed%20that%20these%20models%20often%20rely%20on%20superficial%20features%20and%20shortcuts%0Ainstead%20of%20developing%20a%20genuine%20understanding%20of%20language%2C%20especially%20for%0Anatural%20language%20understanding%20%28NLU%29%20tasks.%20Consequently%2C%20the%20models%20struggle%0Ato%20generalize%20to%20out-of-domain%20data.%20In%20this%20work%2C%20we%20propose%20Divergence%20Based%0ARegularization%20%28DBR%29%20to%20mitigate%20this%20shortcut%20learning%20behavior.%20Our%20method%0Ameasures%20the%20divergence%20between%20the%20output%20distributions%20for%20original%20examples%0Aand%20examples%20where%20shortcut%20tokens%20have%20been%20masked.%20This%20process%20prevents%20the%0Amodel%27s%20predictions%20from%20being%20overly%20influenced%20by%20shortcut%20features%20or%0Abiases.%20We%20evaluate%20our%20model%20on%20three%20NLU%20tasks%20and%20find%20that%20it%20improves%0Aout-of-domain%20performance%20with%20little%20loss%20of%20in-domain%20accuracy.%20Our%20results%0Ademonstrate%20that%20reducing%20the%20reliance%20on%20shortcuts%20and%20superficial%20features%0Acan%20enhance%20the%20generalization%20ability%20of%20large%20pre-trained%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDBR%253A%2520Divergence-Based%2520Regularization%2520for%2520Debiasing%2520Natural%2520Language%250A%2520%2520Understanding%2520Models%26entry.906535625%3DZihao%2520Li%2520and%2520Ruixiang%2520Tang%2520and%2520Lu%2520Cheng%2520and%2520Shuaiqiang%2520Wang%2520and%2520Dawei%2520Yin%2520and%2520Mengnan%2520Du%26entry.1292438233%3D%2520%2520Pre-trained%2520language%2520models%2520%2528PLMs%2529%2520have%2520achieved%2520impressive%2520results%2520on%250Avarious%2520natural%2520language%2520processing%2520tasks.%2520However%252C%2520recent%2520research%2520has%250Arevealed%2520that%2520these%2520models%2520often%2520rely%2520on%2520superficial%2520features%2520and%2520shortcuts%250Ainstead%2520of%2520developing%2520a%2520genuine%2520understanding%2520of%2520language%252C%2520especially%2520for%250Anatural%2520language%2520understanding%2520%2528NLU%2529%2520tasks.%2520Consequently%252C%2520the%2520models%2520struggle%250Ato%2520generalize%2520to%2520out-of-domain%2520data.%2520In%2520this%2520work%252C%2520we%2520propose%2520Divergence%2520Based%250ARegularization%2520%2528DBR%2529%2520to%2520mitigate%2520this%2520shortcut%2520learning%2520behavior.%2520Our%2520method%250Ameasures%2520the%2520divergence%2520between%2520the%2520output%2520distributions%2520for%2520original%2520examples%250Aand%2520examples%2520where%2520shortcut%2520tokens%2520have%2520been%2520masked.%2520This%2520process%2520prevents%2520the%250Amodel%2527s%2520predictions%2520from%2520being%2520overly%2520influenced%2520by%2520shortcut%2520features%2520or%250Abiases.%2520We%2520evaluate%2520our%2520model%2520on%2520three%2520NLU%2520tasks%2520and%2520find%2520that%2520it%2520improves%250Aout-of-domain%2520performance%2520with%2520little%2520loss%2520of%2520in-domain%2520accuracy.%2520Our%2520results%250Ademonstrate%2520that%2520reducing%2520the%2520reliance%2520on%2520shortcuts%2520and%2520superficial%2520features%250Acan%2520enhance%2520the%2520generalization%2520ability%2520of%2520large%2520pre-trained%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DBR%3A%20Divergence-Based%20Regularization%20for%20Debiasing%20Natural%20Language%0A%20%20Understanding%20Models&entry.906535625=Zihao%20Li%20and%20Ruixiang%20Tang%20and%20Lu%20Cheng%20and%20Shuaiqiang%20Wang%20and%20Dawei%20Yin%20and%20Mengnan%20Du&entry.1292438233=%20%20Pre-trained%20language%20models%20%28PLMs%29%20have%20achieved%20impressive%20results%20on%0Avarious%20natural%20language%20processing%20tasks.%20However%2C%20recent%20research%20has%0Arevealed%20that%20these%20models%20often%20rely%20on%20superficial%20features%20and%20shortcuts%0Ainstead%20of%20developing%20a%20genuine%20understanding%20of%20language%2C%20especially%20for%0Anatural%20language%20understanding%20%28NLU%29%20tasks.%20Consequently%2C%20the%20models%20struggle%0Ato%20generalize%20to%20out-of-domain%20data.%20In%20this%20work%2C%20we%20propose%20Divergence%20Based%0ARegularization%20%28DBR%29%20to%20mitigate%20this%20shortcut%20learning%20behavior.%20Our%20method%0Ameasures%20the%20divergence%20between%20the%20output%20distributions%20for%20original%20examples%0Aand%20examples%20where%20shortcut%20tokens%20have%20been%20masked.%20This%20process%20prevents%20the%0Amodel%27s%20predictions%20from%20being%20overly%20influenced%20by%20shortcut%20features%20or%0Abiases.%20We%20evaluate%20our%20model%20on%20three%20NLU%20tasks%20and%20find%20that%20it%20improves%0Aout-of-domain%20performance%20with%20little%20loss%20of%20in-domain%20accuracy.%20Our%20results%0Ademonstrate%20that%20reducing%20the%20reliance%20on%20shortcuts%20and%20superficial%20features%0Acan%20enhance%20the%20generalization%20ability%20of%20large%20pre-trained%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18353v1&entry.124074799=Read"},
{"title": "Both Ears Wide Open: Towards Language-Driven Spatial Audio Generation", "author": "Peiwen Sun and Sitong Cheng and Xiangtai Li and Zhen Ye and Huadai Liu and Honggang Zhang and Wei Xue and Yike Guo", "abstract": "  Recently, diffusion models have achieved great success in mono-channel audio\ngeneration. However, when it comes to stereo audio generation, the soundscapes\noften have a complex scene of multiple objects and directions. Controlling\nstereo audio with spatial contexts remains challenging due to high data costs\nand unstable generative models. To the best of our knowledge, this work\nrepresents the first attempt to address these issues. We first construct a\nlarge-scale, simulation-based, and GPT-assisted dataset, BEWO-1M, with abundant\nsoundscapes and descriptions even including moving and multiple sources. Beyond\ntext modality, we have also acquired a set of images and rationally paired\nstereo audios through retrieval to advance multimodal generation. Existing\naudio generation models tend to generate rather random and indistinct spatial\naudio. To provide accurate guidance for Latent Diffusion Models, we introduce\nthe SpatialSonic model utilizing spatial-aware encoders and azimuth state\nmatrices to reveal reasonable spatial guidance. By leveraging spatial guidance,\nour model not only achieves the objective of generating immersive and\ncontrollable spatial audio from text but also extends to other modalities as\nthe pioneer attempt. Finally, under fair settings, we conduct subjective and\nobjective evaluations on simulated and real-world data to compare our approach\nwith prevailing methods. The results demonstrate the effectiveness of our\nmethod, highlighting its capability to generate spatial audio that adheres to\nphysical rules.\n", "link": "http://arxiv.org/abs/2410.10676v2", "date": "2025-02-25", "relevancy": 2.1753, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5357}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Both%20Ears%20Wide%20Open%3A%20Towards%20Language-Driven%20Spatial%20Audio%20Generation&body=Title%3A%20Both%20Ears%20Wide%20Open%3A%20Towards%20Language-Driven%20Spatial%20Audio%20Generation%0AAuthor%3A%20Peiwen%20Sun%20and%20Sitong%20Cheng%20and%20Xiangtai%20Li%20and%20Zhen%20Ye%20and%20Huadai%20Liu%20and%20Honggang%20Zhang%20and%20Wei%20Xue%20and%20Yike%20Guo%0AAbstract%3A%20%20%20Recently%2C%20diffusion%20models%20have%20achieved%20great%20success%20in%20mono-channel%20audio%0Ageneration.%20However%2C%20when%20it%20comes%20to%20stereo%20audio%20generation%2C%20the%20soundscapes%0Aoften%20have%20a%20complex%20scene%20of%20multiple%20objects%20and%20directions.%20Controlling%0Astereo%20audio%20with%20spatial%20contexts%20remains%20challenging%20due%20to%20high%20data%20costs%0Aand%20unstable%20generative%20models.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%0Arepresents%20the%20first%20attempt%20to%20address%20these%20issues.%20We%20first%20construct%20a%0Alarge-scale%2C%20simulation-based%2C%20and%20GPT-assisted%20dataset%2C%20BEWO-1M%2C%20with%20abundant%0Asoundscapes%20and%20descriptions%20even%20including%20moving%20and%20multiple%20sources.%20Beyond%0Atext%20modality%2C%20we%20have%20also%20acquired%20a%20set%20of%20images%20and%20rationally%20paired%0Astereo%20audios%20through%20retrieval%20to%20advance%20multimodal%20generation.%20Existing%0Aaudio%20generation%20models%20tend%20to%20generate%20rather%20random%20and%20indistinct%20spatial%0Aaudio.%20To%20provide%20accurate%20guidance%20for%20Latent%20Diffusion%20Models%2C%20we%20introduce%0Athe%20SpatialSonic%20model%20utilizing%20spatial-aware%20encoders%20and%20azimuth%20state%0Amatrices%20to%20reveal%20reasonable%20spatial%20guidance.%20By%20leveraging%20spatial%20guidance%2C%0Aour%20model%20not%20only%20achieves%20the%20objective%20of%20generating%20immersive%20and%0Acontrollable%20spatial%20audio%20from%20text%20but%20also%20extends%20to%20other%20modalities%20as%0Athe%20pioneer%20attempt.%20Finally%2C%20under%20fair%20settings%2C%20we%20conduct%20subjective%20and%0Aobjective%20evaluations%20on%20simulated%20and%20real-world%20data%20to%20compare%20our%20approach%0Awith%20prevailing%20methods.%20The%20results%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%2C%20highlighting%20its%20capability%20to%20generate%20spatial%20audio%20that%20adheres%20to%0Aphysical%20rules.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10676v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoth%2520Ears%2520Wide%2520Open%253A%2520Towards%2520Language-Driven%2520Spatial%2520Audio%2520Generation%26entry.906535625%3DPeiwen%2520Sun%2520and%2520Sitong%2520Cheng%2520and%2520Xiangtai%2520Li%2520and%2520Zhen%2520Ye%2520and%2520Huadai%2520Liu%2520and%2520Honggang%2520Zhang%2520and%2520Wei%2520Xue%2520and%2520Yike%2520Guo%26entry.1292438233%3D%2520%2520Recently%252C%2520diffusion%2520models%2520have%2520achieved%2520great%2520success%2520in%2520mono-channel%2520audio%250Ageneration.%2520However%252C%2520when%2520it%2520comes%2520to%2520stereo%2520audio%2520generation%252C%2520the%2520soundscapes%250Aoften%2520have%2520a%2520complex%2520scene%2520of%2520multiple%2520objects%2520and%2520directions.%2520Controlling%250Astereo%2520audio%2520with%2520spatial%2520contexts%2520remains%2520challenging%2520due%2520to%2520high%2520data%2520costs%250Aand%2520unstable%2520generative%2520models.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520work%250Arepresents%2520the%2520first%2520attempt%2520to%2520address%2520these%2520issues.%2520We%2520first%2520construct%2520a%250Alarge-scale%252C%2520simulation-based%252C%2520and%2520GPT-assisted%2520dataset%252C%2520BEWO-1M%252C%2520with%2520abundant%250Asoundscapes%2520and%2520descriptions%2520even%2520including%2520moving%2520and%2520multiple%2520sources.%2520Beyond%250Atext%2520modality%252C%2520we%2520have%2520also%2520acquired%2520a%2520set%2520of%2520images%2520and%2520rationally%2520paired%250Astereo%2520audios%2520through%2520retrieval%2520to%2520advance%2520multimodal%2520generation.%2520Existing%250Aaudio%2520generation%2520models%2520tend%2520to%2520generate%2520rather%2520random%2520and%2520indistinct%2520spatial%250Aaudio.%2520To%2520provide%2520accurate%2520guidance%2520for%2520Latent%2520Diffusion%2520Models%252C%2520we%2520introduce%250Athe%2520SpatialSonic%2520model%2520utilizing%2520spatial-aware%2520encoders%2520and%2520azimuth%2520state%250Amatrices%2520to%2520reveal%2520reasonable%2520spatial%2520guidance.%2520By%2520leveraging%2520spatial%2520guidance%252C%250Aour%2520model%2520not%2520only%2520achieves%2520the%2520objective%2520of%2520generating%2520immersive%2520and%250Acontrollable%2520spatial%2520audio%2520from%2520text%2520but%2520also%2520extends%2520to%2520other%2520modalities%2520as%250Athe%2520pioneer%2520attempt.%2520Finally%252C%2520under%2520fair%2520settings%252C%2520we%2520conduct%2520subjective%2520and%250Aobjective%2520evaluations%2520on%2520simulated%2520and%2520real-world%2520data%2520to%2520compare%2520our%2520approach%250Awith%2520prevailing%2520methods.%2520The%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Amethod%252C%2520highlighting%2520its%2520capability%2520to%2520generate%2520spatial%2520audio%2520that%2520adheres%2520to%250Aphysical%2520rules.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10676v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Both%20Ears%20Wide%20Open%3A%20Towards%20Language-Driven%20Spatial%20Audio%20Generation&entry.906535625=Peiwen%20Sun%20and%20Sitong%20Cheng%20and%20Xiangtai%20Li%20and%20Zhen%20Ye%20and%20Huadai%20Liu%20and%20Honggang%20Zhang%20and%20Wei%20Xue%20and%20Yike%20Guo&entry.1292438233=%20%20Recently%2C%20diffusion%20models%20have%20achieved%20great%20success%20in%20mono-channel%20audio%0Ageneration.%20However%2C%20when%20it%20comes%20to%20stereo%20audio%20generation%2C%20the%20soundscapes%0Aoften%20have%20a%20complex%20scene%20of%20multiple%20objects%20and%20directions.%20Controlling%0Astereo%20audio%20with%20spatial%20contexts%20remains%20challenging%20due%20to%20high%20data%20costs%0Aand%20unstable%20generative%20models.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%0Arepresents%20the%20first%20attempt%20to%20address%20these%20issues.%20We%20first%20construct%20a%0Alarge-scale%2C%20simulation-based%2C%20and%20GPT-assisted%20dataset%2C%20BEWO-1M%2C%20with%20abundant%0Asoundscapes%20and%20descriptions%20even%20including%20moving%20and%20multiple%20sources.%20Beyond%0Atext%20modality%2C%20we%20have%20also%20acquired%20a%20set%20of%20images%20and%20rationally%20paired%0Astereo%20audios%20through%20retrieval%20to%20advance%20multimodal%20generation.%20Existing%0Aaudio%20generation%20models%20tend%20to%20generate%20rather%20random%20and%20indistinct%20spatial%0Aaudio.%20To%20provide%20accurate%20guidance%20for%20Latent%20Diffusion%20Models%2C%20we%20introduce%0Athe%20SpatialSonic%20model%20utilizing%20spatial-aware%20encoders%20and%20azimuth%20state%0Amatrices%20to%20reveal%20reasonable%20spatial%20guidance.%20By%20leveraging%20spatial%20guidance%2C%0Aour%20model%20not%20only%20achieves%20the%20objective%20of%20generating%20immersive%20and%0Acontrollable%20spatial%20audio%20from%20text%20but%20also%20extends%20to%20other%20modalities%20as%0Athe%20pioneer%20attempt.%20Finally%2C%20under%20fair%20settings%2C%20we%20conduct%20subjective%20and%0Aobjective%20evaluations%20on%20simulated%20and%20real-world%20data%20to%20compare%20our%20approach%0Awith%20prevailing%20methods.%20The%20results%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%2C%20highlighting%20its%20capability%20to%20generate%20spatial%20audio%20that%20adheres%20to%0Aphysical%20rules.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10676v2&entry.124074799=Read"},
{"title": "Extreme Rotation Estimation in the Wild", "author": "Hana Bezalel and Dotan Ankri and Ruojin Cai and Hadar Averbuch-Elor", "abstract": "  We present a technique and benchmark dataset for estimating the relative 3D\norientation between a pair of Internet images captured in an extreme setting,\nwhere the images have limited or non-overlapping field of views. Prior work\ntargeting extreme rotation estimation assume constrained 3D environments and\nemulate perspective images by cropping regions from panoramic views. However,\nreal images captured in the wild are highly diverse, exhibiting variation in\nboth appearance and camera intrinsics. In this work, we propose a\nTransformer-based method for estimating relative rotations in extreme\nreal-world settings, and contribute the ExtremeLandmarkPairs dataset, assembled\nfrom scene-level Internet photo collections. Our evaluation demonstrates that\nour approach succeeds in estimating the relative rotations in a wide variety of\nextreme-view Internet image pairs, outperforming various baselines, including\ndedicated rotation estimation techniques and contemporary 3D reconstruction\nmethods.\n", "link": "http://arxiv.org/abs/2411.07096v3", "date": "2025-02-25", "relevancy": 2.175, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5609}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5461}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extreme%20Rotation%20Estimation%20in%20the%20Wild&body=Title%3A%20Extreme%20Rotation%20Estimation%20in%20the%20Wild%0AAuthor%3A%20Hana%20Bezalel%20and%20Dotan%20Ankri%20and%20Ruojin%20Cai%20and%20Hadar%20Averbuch-Elor%0AAbstract%3A%20%20%20We%20present%20a%20technique%20and%20benchmark%20dataset%20for%20estimating%20the%20relative%203D%0Aorientation%20between%20a%20pair%20of%20Internet%20images%20captured%20in%20an%20extreme%20setting%2C%0Awhere%20the%20images%20have%20limited%20or%20non-overlapping%20field%20of%20views.%20Prior%20work%0Atargeting%20extreme%20rotation%20estimation%20assume%20constrained%203D%20environments%20and%0Aemulate%20perspective%20images%20by%20cropping%20regions%20from%20panoramic%20views.%20However%2C%0Areal%20images%20captured%20in%20the%20wild%20are%20highly%20diverse%2C%20exhibiting%20variation%20in%0Aboth%20appearance%20and%20camera%20intrinsics.%20In%20this%20work%2C%20we%20propose%20a%0ATransformer-based%20method%20for%20estimating%20relative%20rotations%20in%20extreme%0Areal-world%20settings%2C%20and%20contribute%20the%20ExtremeLandmarkPairs%20dataset%2C%20assembled%0Afrom%20scene-level%20Internet%20photo%20collections.%20Our%20evaluation%20demonstrates%20that%0Aour%20approach%20succeeds%20in%20estimating%20the%20relative%20rotations%20in%20a%20wide%20variety%20of%0Aextreme-view%20Internet%20image%20pairs%2C%20outperforming%20various%20baselines%2C%20including%0Adedicated%20rotation%20estimation%20techniques%20and%20contemporary%203D%20reconstruction%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07096v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtreme%2520Rotation%2520Estimation%2520in%2520the%2520Wild%26entry.906535625%3DHana%2520Bezalel%2520and%2520Dotan%2520Ankri%2520and%2520Ruojin%2520Cai%2520and%2520Hadar%2520Averbuch-Elor%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520technique%2520and%2520benchmark%2520dataset%2520for%2520estimating%2520the%2520relative%25203D%250Aorientation%2520between%2520a%2520pair%2520of%2520Internet%2520images%2520captured%2520in%2520an%2520extreme%2520setting%252C%250Awhere%2520the%2520images%2520have%2520limited%2520or%2520non-overlapping%2520field%2520of%2520views.%2520Prior%2520work%250Atargeting%2520extreme%2520rotation%2520estimation%2520assume%2520constrained%25203D%2520environments%2520and%250Aemulate%2520perspective%2520images%2520by%2520cropping%2520regions%2520from%2520panoramic%2520views.%2520However%252C%250Areal%2520images%2520captured%2520in%2520the%2520wild%2520are%2520highly%2520diverse%252C%2520exhibiting%2520variation%2520in%250Aboth%2520appearance%2520and%2520camera%2520intrinsics.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250ATransformer-based%2520method%2520for%2520estimating%2520relative%2520rotations%2520in%2520extreme%250Areal-world%2520settings%252C%2520and%2520contribute%2520the%2520ExtremeLandmarkPairs%2520dataset%252C%2520assembled%250Afrom%2520scene-level%2520Internet%2520photo%2520collections.%2520Our%2520evaluation%2520demonstrates%2520that%250Aour%2520approach%2520succeeds%2520in%2520estimating%2520the%2520relative%2520rotations%2520in%2520a%2520wide%2520variety%2520of%250Aextreme-view%2520Internet%2520image%2520pairs%252C%2520outperforming%2520various%2520baselines%252C%2520including%250Adedicated%2520rotation%2520estimation%2520techniques%2520and%2520contemporary%25203D%2520reconstruction%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07096v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extreme%20Rotation%20Estimation%20in%20the%20Wild&entry.906535625=Hana%20Bezalel%20and%20Dotan%20Ankri%20and%20Ruojin%20Cai%20and%20Hadar%20Averbuch-Elor&entry.1292438233=%20%20We%20present%20a%20technique%20and%20benchmark%20dataset%20for%20estimating%20the%20relative%203D%0Aorientation%20between%20a%20pair%20of%20Internet%20images%20captured%20in%20an%20extreme%20setting%2C%0Awhere%20the%20images%20have%20limited%20or%20non-overlapping%20field%20of%20views.%20Prior%20work%0Atargeting%20extreme%20rotation%20estimation%20assume%20constrained%203D%20environments%20and%0Aemulate%20perspective%20images%20by%20cropping%20regions%20from%20panoramic%20views.%20However%2C%0Areal%20images%20captured%20in%20the%20wild%20are%20highly%20diverse%2C%20exhibiting%20variation%20in%0Aboth%20appearance%20and%20camera%20intrinsics.%20In%20this%20work%2C%20we%20propose%20a%0ATransformer-based%20method%20for%20estimating%20relative%20rotations%20in%20extreme%0Areal-world%20settings%2C%20and%20contribute%20the%20ExtremeLandmarkPairs%20dataset%2C%20assembled%0Afrom%20scene-level%20Internet%20photo%20collections.%20Our%20evaluation%20demonstrates%20that%0Aour%20approach%20succeeds%20in%20estimating%20the%20relative%20rotations%20in%20a%20wide%20variety%20of%0Aextreme-view%20Internet%20image%20pairs%2C%20outperforming%20various%20baselines%2C%20including%0Adedicated%20rotation%20estimation%20techniques%20and%20contemporary%203D%20reconstruction%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07096v3&entry.124074799=Read"},
{"title": "Multi-Perspective Data Augmentation for Few-shot Object Detection", "author": "Anh-Khoa Nguyen Vu and Quoc-Truong Truong and Vinh-Tiep Nguyen and Thanh Duc Ngo and Thanh-Toan Do and Tam V. Nguyen", "abstract": "  Recent few-shot object detection (FSOD) methods have focused on augmenting\nsynthetic samples for novel classes, show promising results to the rise of\ndiffusion models. However, the diversity of such datasets is often limited in\nrepresentativeness because they lack awareness of typical and hard samples,\nespecially in the context of foreground and background relationships. To tackle\nthis issue, we propose a Multi-Perspective Data Augmentation (MPAD) framework.\nIn terms of foreground-foreground relationships, we propose in-context learning\nfor object synthesis (ICOS) with bounding box adjustments to enhance the detail\nand spatial information of synthetic samples. Inspired by the large margin\nprinciple, support samples play a vital role in defining class boundaries.\nTherefore, we design a Harmonic Prompt Aggregation Scheduler (HPAS) to mix\nprompt embeddings at each time step of the generation process in diffusion\nmodels, producing hard novel samples. For foreground-background relationships,\nwe introduce a Background Proposal method (BAP) to sample typical and hard\nbackgrounds. Extensive experiments on multiple FSOD benchmarks demonstrate the\neffectiveness of our approach. Our framework significantly outperforms\ntraditional methods, achieving an average increase of $17.5\\%$ in nAP50 over\nthe baseline on PASCAL VOC. Code is available at\nhttps://github.com/nvakhoa/MPAD.\n", "link": "http://arxiv.org/abs/2502.18195v1", "date": "2025-02-25", "relevancy": 2.1747, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5822}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.539}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Perspective%20Data%20Augmentation%20for%20Few-shot%20Object%20Detection&body=Title%3A%20Multi-Perspective%20Data%20Augmentation%20for%20Few-shot%20Object%20Detection%0AAuthor%3A%20Anh-Khoa%20Nguyen%20Vu%20and%20Quoc-Truong%20Truong%20and%20Vinh-Tiep%20Nguyen%20and%20Thanh%20Duc%20Ngo%20and%20Thanh-Toan%20Do%20and%20Tam%20V.%20Nguyen%0AAbstract%3A%20%20%20Recent%20few-shot%20object%20detection%20%28FSOD%29%20methods%20have%20focused%20on%20augmenting%0Asynthetic%20samples%20for%20novel%20classes%2C%20show%20promising%20results%20to%20the%20rise%20of%0Adiffusion%20models.%20However%2C%20the%20diversity%20of%20such%20datasets%20is%20often%20limited%20in%0Arepresentativeness%20because%20they%20lack%20awareness%20of%20typical%20and%20hard%20samples%2C%0Aespecially%20in%20the%20context%20of%20foreground%20and%20background%20relationships.%20To%20tackle%0Athis%20issue%2C%20we%20propose%20a%20Multi-Perspective%20Data%20Augmentation%20%28MPAD%29%20framework.%0AIn%20terms%20of%20foreground-foreground%20relationships%2C%20we%20propose%20in-context%20learning%0Afor%20object%20synthesis%20%28ICOS%29%20with%20bounding%20box%20adjustments%20to%20enhance%20the%20detail%0Aand%20spatial%20information%20of%20synthetic%20samples.%20Inspired%20by%20the%20large%20margin%0Aprinciple%2C%20support%20samples%20play%20a%20vital%20role%20in%20defining%20class%20boundaries.%0ATherefore%2C%20we%20design%20a%20Harmonic%20Prompt%20Aggregation%20Scheduler%20%28HPAS%29%20to%20mix%0Aprompt%20embeddings%20at%20each%20time%20step%20of%20the%20generation%20process%20in%20diffusion%0Amodels%2C%20producing%20hard%20novel%20samples.%20For%20foreground-background%20relationships%2C%0Awe%20introduce%20a%20Background%20Proposal%20method%20%28BAP%29%20to%20sample%20typical%20and%20hard%0Abackgrounds.%20Extensive%20experiments%20on%20multiple%20FSOD%20benchmarks%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach.%20Our%20framework%20significantly%20outperforms%0Atraditional%20methods%2C%20achieving%20an%20average%20increase%20of%20%2417.5%5C%25%24%20in%20nAP50%20over%0Athe%20baseline%20on%20PASCAL%20VOC.%20Code%20is%20available%20at%0Ahttps%3A//github.com/nvakhoa/MPAD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Perspective%2520Data%2520Augmentation%2520for%2520Few-shot%2520Object%2520Detection%26entry.906535625%3DAnh-Khoa%2520Nguyen%2520Vu%2520and%2520Quoc-Truong%2520Truong%2520and%2520Vinh-Tiep%2520Nguyen%2520and%2520Thanh%2520Duc%2520Ngo%2520and%2520Thanh-Toan%2520Do%2520and%2520Tam%2520V.%2520Nguyen%26entry.1292438233%3D%2520%2520Recent%2520few-shot%2520object%2520detection%2520%2528FSOD%2529%2520methods%2520have%2520focused%2520on%2520augmenting%250Asynthetic%2520samples%2520for%2520novel%2520classes%252C%2520show%2520promising%2520results%2520to%2520the%2520rise%2520of%250Adiffusion%2520models.%2520However%252C%2520the%2520diversity%2520of%2520such%2520datasets%2520is%2520often%2520limited%2520in%250Arepresentativeness%2520because%2520they%2520lack%2520awareness%2520of%2520typical%2520and%2520hard%2520samples%252C%250Aespecially%2520in%2520the%2520context%2520of%2520foreground%2520and%2520background%2520relationships.%2520To%2520tackle%250Athis%2520issue%252C%2520we%2520propose%2520a%2520Multi-Perspective%2520Data%2520Augmentation%2520%2528MPAD%2529%2520framework.%250AIn%2520terms%2520of%2520foreground-foreground%2520relationships%252C%2520we%2520propose%2520in-context%2520learning%250Afor%2520object%2520synthesis%2520%2528ICOS%2529%2520with%2520bounding%2520box%2520adjustments%2520to%2520enhance%2520the%2520detail%250Aand%2520spatial%2520information%2520of%2520synthetic%2520samples.%2520Inspired%2520by%2520the%2520large%2520margin%250Aprinciple%252C%2520support%2520samples%2520play%2520a%2520vital%2520role%2520in%2520defining%2520class%2520boundaries.%250ATherefore%252C%2520we%2520design%2520a%2520Harmonic%2520Prompt%2520Aggregation%2520Scheduler%2520%2528HPAS%2529%2520to%2520mix%250Aprompt%2520embeddings%2520at%2520each%2520time%2520step%2520of%2520the%2520generation%2520process%2520in%2520diffusion%250Amodels%252C%2520producing%2520hard%2520novel%2520samples.%2520For%2520foreground-background%2520relationships%252C%250Awe%2520introduce%2520a%2520Background%2520Proposal%2520method%2520%2528BAP%2529%2520to%2520sample%2520typical%2520and%2520hard%250Abackgrounds.%2520Extensive%2520experiments%2520on%2520multiple%2520FSOD%2520benchmarks%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520approach.%2520Our%2520framework%2520significantly%2520outperforms%250Atraditional%2520methods%252C%2520achieving%2520an%2520average%2520increase%2520of%2520%252417.5%255C%2525%2524%2520in%2520nAP50%2520over%250Athe%2520baseline%2520on%2520PASCAL%2520VOC.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/nvakhoa/MPAD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Perspective%20Data%20Augmentation%20for%20Few-shot%20Object%20Detection&entry.906535625=Anh-Khoa%20Nguyen%20Vu%20and%20Quoc-Truong%20Truong%20and%20Vinh-Tiep%20Nguyen%20and%20Thanh%20Duc%20Ngo%20and%20Thanh-Toan%20Do%20and%20Tam%20V.%20Nguyen&entry.1292438233=%20%20Recent%20few-shot%20object%20detection%20%28FSOD%29%20methods%20have%20focused%20on%20augmenting%0Asynthetic%20samples%20for%20novel%20classes%2C%20show%20promising%20results%20to%20the%20rise%20of%0Adiffusion%20models.%20However%2C%20the%20diversity%20of%20such%20datasets%20is%20often%20limited%20in%0Arepresentativeness%20because%20they%20lack%20awareness%20of%20typical%20and%20hard%20samples%2C%0Aespecially%20in%20the%20context%20of%20foreground%20and%20background%20relationships.%20To%20tackle%0Athis%20issue%2C%20we%20propose%20a%20Multi-Perspective%20Data%20Augmentation%20%28MPAD%29%20framework.%0AIn%20terms%20of%20foreground-foreground%20relationships%2C%20we%20propose%20in-context%20learning%0Afor%20object%20synthesis%20%28ICOS%29%20with%20bounding%20box%20adjustments%20to%20enhance%20the%20detail%0Aand%20spatial%20information%20of%20synthetic%20samples.%20Inspired%20by%20the%20large%20margin%0Aprinciple%2C%20support%20samples%20play%20a%20vital%20role%20in%20defining%20class%20boundaries.%0ATherefore%2C%20we%20design%20a%20Harmonic%20Prompt%20Aggregation%20Scheduler%20%28HPAS%29%20to%20mix%0Aprompt%20embeddings%20at%20each%20time%20step%20of%20the%20generation%20process%20in%20diffusion%0Amodels%2C%20producing%20hard%20novel%20samples.%20For%20foreground-background%20relationships%2C%0Awe%20introduce%20a%20Background%20Proposal%20method%20%28BAP%29%20to%20sample%20typical%20and%20hard%0Abackgrounds.%20Extensive%20experiments%20on%20multiple%20FSOD%20benchmarks%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach.%20Our%20framework%20significantly%20outperforms%0Atraditional%20methods%2C%20achieving%20an%20average%20increase%20of%20%2417.5%5C%25%24%20in%20nAP50%20over%0Athe%20baseline%20on%20PASCAL%20VOC.%20Code%20is%20available%20at%0Ahttps%3A//github.com/nvakhoa/MPAD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18195v1&entry.124074799=Read"},
{"title": "The FFT Strikes Back: An Efficient Alternative to Self-Attention", "author": "Jacob Fein-Ashley", "abstract": "  Conventional self-attention mechanisms incur quadratic complexity, limiting\ntheir scalability on long sequences. We introduce FFTNet, an adaptive spectral\nfiltering framework that leverages the Fast Fourier Transform (FFT) to achieve\nglobal token mixing in $\\mathcal{O}(n\\log n)$ time. By transforming inputs into\nthe frequency domain, FFTNet exploits the orthogonality and energy preservation\nguaranteed by Parseval's theorem to capture long-range dependencies\nefficiently. A learnable spectral filter and modReLU activation dynamically\nemphasize salient frequency components, providing a rigorous and adaptive\nalternative to traditional self-attention. Experiments on the Long Range Arena\nand ImageNet benchmarks validate our theoretical insights and demonstrate\nsuperior performance over fixed Fourier and standard attention models.\n", "link": "http://arxiv.org/abs/2502.18394v1", "date": "2025-02-25", "relevancy": 2.1738, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.593}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5143}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20FFT%20Strikes%20Back%3A%20An%20Efficient%20Alternative%20to%20Self-Attention&body=Title%3A%20The%20FFT%20Strikes%20Back%3A%20An%20Efficient%20Alternative%20to%20Self-Attention%0AAuthor%3A%20Jacob%20Fein-Ashley%0AAbstract%3A%20%20%20Conventional%20self-attention%20mechanisms%20incur%20quadratic%20complexity%2C%20limiting%0Atheir%20scalability%20on%20long%20sequences.%20We%20introduce%20FFTNet%2C%20an%20adaptive%20spectral%0Afiltering%20framework%20that%20leverages%20the%20Fast%20Fourier%20Transform%20%28FFT%29%20to%20achieve%0Aglobal%20token%20mixing%20in%20%24%5Cmathcal%7BO%7D%28n%5Clog%20n%29%24%20time.%20By%20transforming%20inputs%20into%0Athe%20frequency%20domain%2C%20FFTNet%20exploits%20the%20orthogonality%20and%20energy%20preservation%0Aguaranteed%20by%20Parseval%27s%20theorem%20to%20capture%20long-range%20dependencies%0Aefficiently.%20A%20learnable%20spectral%20filter%20and%20modReLU%20activation%20dynamically%0Aemphasize%20salient%20frequency%20components%2C%20providing%20a%20rigorous%20and%20adaptive%0Aalternative%20to%20traditional%20self-attention.%20Experiments%20on%20the%20Long%20Range%20Arena%0Aand%20ImageNet%20benchmarks%20validate%20our%20theoretical%20insights%20and%20demonstrate%0Asuperior%20performance%20over%20fixed%20Fourier%20and%20standard%20attention%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520FFT%2520Strikes%2520Back%253A%2520An%2520Efficient%2520Alternative%2520to%2520Self-Attention%26entry.906535625%3DJacob%2520Fein-Ashley%26entry.1292438233%3D%2520%2520Conventional%2520self-attention%2520mechanisms%2520incur%2520quadratic%2520complexity%252C%2520limiting%250Atheir%2520scalability%2520on%2520long%2520sequences.%2520We%2520introduce%2520FFTNet%252C%2520an%2520adaptive%2520spectral%250Afiltering%2520framework%2520that%2520leverages%2520the%2520Fast%2520Fourier%2520Transform%2520%2528FFT%2529%2520to%2520achieve%250Aglobal%2520token%2520mixing%2520in%2520%2524%255Cmathcal%257BO%257D%2528n%255Clog%2520n%2529%2524%2520time.%2520By%2520transforming%2520inputs%2520into%250Athe%2520frequency%2520domain%252C%2520FFTNet%2520exploits%2520the%2520orthogonality%2520and%2520energy%2520preservation%250Aguaranteed%2520by%2520Parseval%2527s%2520theorem%2520to%2520capture%2520long-range%2520dependencies%250Aefficiently.%2520A%2520learnable%2520spectral%2520filter%2520and%2520modReLU%2520activation%2520dynamically%250Aemphasize%2520salient%2520frequency%2520components%252C%2520providing%2520a%2520rigorous%2520and%2520adaptive%250Aalternative%2520to%2520traditional%2520self-attention.%2520Experiments%2520on%2520the%2520Long%2520Range%2520Arena%250Aand%2520ImageNet%2520benchmarks%2520validate%2520our%2520theoretical%2520insights%2520and%2520demonstrate%250Asuperior%2520performance%2520over%2520fixed%2520Fourier%2520and%2520standard%2520attention%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20FFT%20Strikes%20Back%3A%20An%20Efficient%20Alternative%20to%20Self-Attention&entry.906535625=Jacob%20Fein-Ashley&entry.1292438233=%20%20Conventional%20self-attention%20mechanisms%20incur%20quadratic%20complexity%2C%20limiting%0Atheir%20scalability%20on%20long%20sequences.%20We%20introduce%20FFTNet%2C%20an%20adaptive%20spectral%0Afiltering%20framework%20that%20leverages%20the%20Fast%20Fourier%20Transform%20%28FFT%29%20to%20achieve%0Aglobal%20token%20mixing%20in%20%24%5Cmathcal%7BO%7D%28n%5Clog%20n%29%24%20time.%20By%20transforming%20inputs%20into%0Athe%20frequency%20domain%2C%20FFTNet%20exploits%20the%20orthogonality%20and%20energy%20preservation%0Aguaranteed%20by%20Parseval%27s%20theorem%20to%20capture%20long-range%20dependencies%0Aefficiently.%20A%20learnable%20spectral%20filter%20and%20modReLU%20activation%20dynamically%0Aemphasize%20salient%20frequency%20components%2C%20providing%20a%20rigorous%20and%20adaptive%0Aalternative%20to%20traditional%20self-attention.%20Experiments%20on%20the%20Long%20Range%20Arena%0Aand%20ImageNet%20benchmarks%20validate%20our%20theoretical%20insights%20and%20demonstrate%0Asuperior%20performance%20over%20fixed%20Fourier%20and%20standard%20attention%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18394v1&entry.124074799=Read"},
{"title": "Enhancing DNA Foundation Models to Address Masking Inefficiencies", "author": "Monireh Safari and Pablo Millan Arias and Scott C. Lowe and Lila Kari and Angel X. Chang and Graham W. Taylor", "abstract": "  Masked language modelling (MLM) as a pretraining objective has been widely\nadopted in genomic sequence modelling. While pretrained models can successfully\nserve as encoders for various downstream tasks, the distribution shift between\npretraining and inference detrimentally impacts performance, as the pretraining\ntask is to map [MASK] tokens to predictions, yet the [MASK] is absent during\ndownstream applications. This means the encoder does not prioritize its\nencodings of non-[MASK] tokens, and expends parameters and compute on work only\nrelevant to the MLM task, despite this being irrelevant at deployment time. In\nthis work, we propose a modified encoder-decoder architecture based on the\nmasked autoencoder framework, designed to address this inefficiency within a\nBERT-based transformer. We empirically show that the resulting mismatch is\nparticularly detrimental in genomic pipelines where models are often used for\nfeature extraction without fine-tuning. We evaluate our approach on the\nBIOSCAN-5M dataset, comprising over 2 million unique DNA barcodes. We achieve\nsubstantial performance gains in both closed-world and open-world\nclassification tasks when compared against causal models and bidirectional\narchitectures pretrained with MLM tasks.\n", "link": "http://arxiv.org/abs/2502.18405v1", "date": "2025-02-25", "relevancy": 2.1595, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5468}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5468}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20DNA%20Foundation%20Models%20to%20Address%20Masking%20Inefficiencies&body=Title%3A%20Enhancing%20DNA%20Foundation%20Models%20to%20Address%20Masking%20Inefficiencies%0AAuthor%3A%20Monireh%20Safari%20and%20Pablo%20Millan%20Arias%20and%20Scott%20C.%20Lowe%20and%20Lila%20Kari%20and%20Angel%20X.%20Chang%20and%20Graham%20W.%20Taylor%0AAbstract%3A%20%20%20Masked%20language%20modelling%20%28MLM%29%20as%20a%20pretraining%20objective%20has%20been%20widely%0Aadopted%20in%20genomic%20sequence%20modelling.%20While%20pretrained%20models%20can%20successfully%0Aserve%20as%20encoders%20for%20various%20downstream%20tasks%2C%20the%20distribution%20shift%20between%0Apretraining%20and%20inference%20detrimentally%20impacts%20performance%2C%20as%20the%20pretraining%0Atask%20is%20to%20map%20%5BMASK%5D%20tokens%20to%20predictions%2C%20yet%20the%20%5BMASK%5D%20is%20absent%20during%0Adownstream%20applications.%20This%20means%20the%20encoder%20does%20not%20prioritize%20its%0Aencodings%20of%20non-%5BMASK%5D%20tokens%2C%20and%20expends%20parameters%20and%20compute%20on%20work%20only%0Arelevant%20to%20the%20MLM%20task%2C%20despite%20this%20being%20irrelevant%20at%20deployment%20time.%20In%0Athis%20work%2C%20we%20propose%20a%20modified%20encoder-decoder%20architecture%20based%20on%20the%0Amasked%20autoencoder%20framework%2C%20designed%20to%20address%20this%20inefficiency%20within%20a%0ABERT-based%20transformer.%20We%20empirically%20show%20that%20the%20resulting%20mismatch%20is%0Aparticularly%20detrimental%20in%20genomic%20pipelines%20where%20models%20are%20often%20used%20for%0Afeature%20extraction%20without%20fine-tuning.%20We%20evaluate%20our%20approach%20on%20the%0ABIOSCAN-5M%20dataset%2C%20comprising%20over%202%20million%20unique%20DNA%20barcodes.%20We%20achieve%0Asubstantial%20performance%20gains%20in%20both%20closed-world%20and%20open-world%0Aclassification%20tasks%20when%20compared%20against%20causal%20models%20and%20bidirectional%0Aarchitectures%20pretrained%20with%20MLM%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18405v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520DNA%2520Foundation%2520Models%2520to%2520Address%2520Masking%2520Inefficiencies%26entry.906535625%3DMonireh%2520Safari%2520and%2520Pablo%2520Millan%2520Arias%2520and%2520Scott%2520C.%2520Lowe%2520and%2520Lila%2520Kari%2520and%2520Angel%2520X.%2520Chang%2520and%2520Graham%2520W.%2520Taylor%26entry.1292438233%3D%2520%2520Masked%2520language%2520modelling%2520%2528MLM%2529%2520as%2520a%2520pretraining%2520objective%2520has%2520been%2520widely%250Aadopted%2520in%2520genomic%2520sequence%2520modelling.%2520While%2520pretrained%2520models%2520can%2520successfully%250Aserve%2520as%2520encoders%2520for%2520various%2520downstream%2520tasks%252C%2520the%2520distribution%2520shift%2520between%250Apretraining%2520and%2520inference%2520detrimentally%2520impacts%2520performance%252C%2520as%2520the%2520pretraining%250Atask%2520is%2520to%2520map%2520%255BMASK%255D%2520tokens%2520to%2520predictions%252C%2520yet%2520the%2520%255BMASK%255D%2520is%2520absent%2520during%250Adownstream%2520applications.%2520This%2520means%2520the%2520encoder%2520does%2520not%2520prioritize%2520its%250Aencodings%2520of%2520non-%255BMASK%255D%2520tokens%252C%2520and%2520expends%2520parameters%2520and%2520compute%2520on%2520work%2520only%250Arelevant%2520to%2520the%2520MLM%2520task%252C%2520despite%2520this%2520being%2520irrelevant%2520at%2520deployment%2520time.%2520In%250Athis%2520work%252C%2520we%2520propose%2520a%2520modified%2520encoder-decoder%2520architecture%2520based%2520on%2520the%250Amasked%2520autoencoder%2520framework%252C%2520designed%2520to%2520address%2520this%2520inefficiency%2520within%2520a%250ABERT-based%2520transformer.%2520We%2520empirically%2520show%2520that%2520the%2520resulting%2520mismatch%2520is%250Aparticularly%2520detrimental%2520in%2520genomic%2520pipelines%2520where%2520models%2520are%2520often%2520used%2520for%250Afeature%2520extraction%2520without%2520fine-tuning.%2520We%2520evaluate%2520our%2520approach%2520on%2520the%250ABIOSCAN-5M%2520dataset%252C%2520comprising%2520over%25202%2520million%2520unique%2520DNA%2520barcodes.%2520We%2520achieve%250Asubstantial%2520performance%2520gains%2520in%2520both%2520closed-world%2520and%2520open-world%250Aclassification%2520tasks%2520when%2520compared%2520against%2520causal%2520models%2520and%2520bidirectional%250Aarchitectures%2520pretrained%2520with%2520MLM%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18405v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20DNA%20Foundation%20Models%20to%20Address%20Masking%20Inefficiencies&entry.906535625=Monireh%20Safari%20and%20Pablo%20Millan%20Arias%20and%20Scott%20C.%20Lowe%20and%20Lila%20Kari%20and%20Angel%20X.%20Chang%20and%20Graham%20W.%20Taylor&entry.1292438233=%20%20Masked%20language%20modelling%20%28MLM%29%20as%20a%20pretraining%20objective%20has%20been%20widely%0Aadopted%20in%20genomic%20sequence%20modelling.%20While%20pretrained%20models%20can%20successfully%0Aserve%20as%20encoders%20for%20various%20downstream%20tasks%2C%20the%20distribution%20shift%20between%0Apretraining%20and%20inference%20detrimentally%20impacts%20performance%2C%20as%20the%20pretraining%0Atask%20is%20to%20map%20%5BMASK%5D%20tokens%20to%20predictions%2C%20yet%20the%20%5BMASK%5D%20is%20absent%20during%0Adownstream%20applications.%20This%20means%20the%20encoder%20does%20not%20prioritize%20its%0Aencodings%20of%20non-%5BMASK%5D%20tokens%2C%20and%20expends%20parameters%20and%20compute%20on%20work%20only%0Arelevant%20to%20the%20MLM%20task%2C%20despite%20this%20being%20irrelevant%20at%20deployment%20time.%20In%0Athis%20work%2C%20we%20propose%20a%20modified%20encoder-decoder%20architecture%20based%20on%20the%0Amasked%20autoencoder%20framework%2C%20designed%20to%20address%20this%20inefficiency%20within%20a%0ABERT-based%20transformer.%20We%20empirically%20show%20that%20the%20resulting%20mismatch%20is%0Aparticularly%20detrimental%20in%20genomic%20pipelines%20where%20models%20are%20often%20used%20for%0Afeature%20extraction%20without%20fine-tuning.%20We%20evaluate%20our%20approach%20on%20the%0ABIOSCAN-5M%20dataset%2C%20comprising%20over%202%20million%20unique%20DNA%20barcodes.%20We%20achieve%0Asubstantial%20performance%20gains%20in%20both%20closed-world%20and%20open-world%0Aclassification%20tasks%20when%20compared%20against%20causal%20models%20and%20bidirectional%0Aarchitectures%20pretrained%20with%20MLM%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18405v1&entry.124074799=Read"},
{"title": "From System 1 to System 2: A Survey of Reasoning Large Language Models", "author": "Zhong-Zhi Li and Duzhen Zhang and Ming-Liang Zhang and Jiaxin Zhang and Zengyan Liu and Yuxuan Yao and Haotian Xu and Junhao Zheng and Pei-Jie Wang and Xiuyi Chen and Yingying Zhang and Fei Yin and Jiahua Dong and Zhijiang Guo and Le Song and Cheng-Lin Liu", "abstract": "  Achieving human-level intelligence requires refining the transition from the\nfast, intuitive System 1 to the slower, more deliberate System 2 reasoning.\nWhile System 1 excels in quick, heuristic decisions, System 2 relies on logical\nreasoning for more accurate judgments and reduced biases. Foundational Large\nLanguage Models (LLMs) excel at fast decision-making but lack the depth for\ncomplex reasoning, as they have not yet fully embraced the step-by-step\nanalysis characteristic of true System 2 thinking. Recently, reasoning LLMs\nlike OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level\nperformance in fields such as mathematics and coding, closely mimicking the\ndeliberate reasoning of System 2 and showcasing human-like cognitive abilities.\nThis survey begins with a brief overview of the progress in foundational LLMs\nand the early development of System 2 technologies, exploring how their\ncombination has paved the way for reasoning LLMs. Next, we discuss how to\nconstruct reasoning LLMs, analyzing their features, the core methods enabling\nadvanced reasoning, and the evolution of various reasoning LLMs. Additionally,\nwe provide an overview of reasoning benchmarks, offering an in-depth comparison\nof the performance of representative reasoning LLMs. Finally, we explore\npromising directions for advancing reasoning LLMs and maintain a real-time\n\\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub\nRepository} to track the latest developments. We hope this survey will serve as\na valuable resource to inspire innovation and drive progress in this rapidly\nevolving field.\n", "link": "http://arxiv.org/abs/2502.17419v2", "date": "2025-02-25", "relevancy": 2.1563, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5498}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20System%201%20to%20System%202%3A%20A%20Survey%20of%20Reasoning%20Large%20Language%20Models&body=Title%3A%20From%20System%201%20to%20System%202%3A%20A%20Survey%20of%20Reasoning%20Large%20Language%20Models%0AAuthor%3A%20Zhong-Zhi%20Li%20and%20Duzhen%20Zhang%20and%20Ming-Liang%20Zhang%20and%20Jiaxin%20Zhang%20and%20Zengyan%20Liu%20and%20Yuxuan%20Yao%20and%20Haotian%20Xu%20and%20Junhao%20Zheng%20and%20Pei-Jie%20Wang%20and%20Xiuyi%20Chen%20and%20Yingying%20Zhang%20and%20Fei%20Yin%20and%20Jiahua%20Dong%20and%20Zhijiang%20Guo%20and%20Le%20Song%20and%20Cheng-Lin%20Liu%0AAbstract%3A%20%20%20Achieving%20human-level%20intelligence%20requires%20refining%20the%20transition%20from%20the%0Afast%2C%20intuitive%20System%201%20to%20the%20slower%2C%20more%20deliberate%20System%202%20reasoning.%0AWhile%20System%201%20excels%20in%20quick%2C%20heuristic%20decisions%2C%20System%202%20relies%20on%20logical%0Areasoning%20for%20more%20accurate%20judgments%20and%20reduced%20biases.%20Foundational%20Large%0ALanguage%20Models%20%28LLMs%29%20excel%20at%20fast%20decision-making%20but%20lack%20the%20depth%20for%0Acomplex%20reasoning%2C%20as%20they%20have%20not%20yet%20fully%20embraced%20the%20step-by-step%0Aanalysis%20characteristic%20of%20true%20System%202%20thinking.%20Recently%2C%20reasoning%20LLMs%0Alike%20OpenAI%27s%20o1/o3%20and%20DeepSeek%27s%20R1%20have%20demonstrated%20expert-level%0Aperformance%20in%20fields%20such%20as%20mathematics%20and%20coding%2C%20closely%20mimicking%20the%0Adeliberate%20reasoning%20of%20System%202%20and%20showcasing%20human-like%20cognitive%20abilities.%0AThis%20survey%20begins%20with%20a%20brief%20overview%20of%20the%20progress%20in%20foundational%20LLMs%0Aand%20the%20early%20development%20of%20System%202%20technologies%2C%20exploring%20how%20their%0Acombination%20has%20paved%20the%20way%20for%20reasoning%20LLMs.%20Next%2C%20we%20discuss%20how%20to%0Aconstruct%20reasoning%20LLMs%2C%20analyzing%20their%20features%2C%20the%20core%20methods%20enabling%0Aadvanced%20reasoning%2C%20and%20the%20evolution%20of%20various%20reasoning%20LLMs.%20Additionally%2C%0Awe%20provide%20an%20overview%20of%20reasoning%20benchmarks%2C%20offering%20an%20in-depth%20comparison%0Aof%20the%20performance%20of%20representative%20reasoning%20LLMs.%20Finally%2C%20we%20explore%0Apromising%20directions%20for%20advancing%20reasoning%20LLMs%20and%20maintain%20a%20real-time%0A%5Chref%7Bhttps%3A//github.com/zzli2022/Awesome-Slow-Reason-System%7D%7BGitHub%0ARepository%7D%20to%20track%20the%20latest%20developments.%20We%20hope%20this%20survey%20will%20serve%20as%0Aa%20valuable%20resource%20to%20inspire%20innovation%20and%20drive%20progress%20in%20this%20rapidly%0Aevolving%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17419v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520System%25201%2520to%2520System%25202%253A%2520A%2520Survey%2520of%2520Reasoning%2520Large%2520Language%2520Models%26entry.906535625%3DZhong-Zhi%2520Li%2520and%2520Duzhen%2520Zhang%2520and%2520Ming-Liang%2520Zhang%2520and%2520Jiaxin%2520Zhang%2520and%2520Zengyan%2520Liu%2520and%2520Yuxuan%2520Yao%2520and%2520Haotian%2520Xu%2520and%2520Junhao%2520Zheng%2520and%2520Pei-Jie%2520Wang%2520and%2520Xiuyi%2520Chen%2520and%2520Yingying%2520Zhang%2520and%2520Fei%2520Yin%2520and%2520Jiahua%2520Dong%2520and%2520Zhijiang%2520Guo%2520and%2520Le%2520Song%2520and%2520Cheng-Lin%2520Liu%26entry.1292438233%3D%2520%2520Achieving%2520human-level%2520intelligence%2520requires%2520refining%2520the%2520transition%2520from%2520the%250Afast%252C%2520intuitive%2520System%25201%2520to%2520the%2520slower%252C%2520more%2520deliberate%2520System%25202%2520reasoning.%250AWhile%2520System%25201%2520excels%2520in%2520quick%252C%2520heuristic%2520decisions%252C%2520System%25202%2520relies%2520on%2520logical%250Areasoning%2520for%2520more%2520accurate%2520judgments%2520and%2520reduced%2520biases.%2520Foundational%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520excel%2520at%2520fast%2520decision-making%2520but%2520lack%2520the%2520depth%2520for%250Acomplex%2520reasoning%252C%2520as%2520they%2520have%2520not%2520yet%2520fully%2520embraced%2520the%2520step-by-step%250Aanalysis%2520characteristic%2520of%2520true%2520System%25202%2520thinking.%2520Recently%252C%2520reasoning%2520LLMs%250Alike%2520OpenAI%2527s%2520o1/o3%2520and%2520DeepSeek%2527s%2520R1%2520have%2520demonstrated%2520expert-level%250Aperformance%2520in%2520fields%2520such%2520as%2520mathematics%2520and%2520coding%252C%2520closely%2520mimicking%2520the%250Adeliberate%2520reasoning%2520of%2520System%25202%2520and%2520showcasing%2520human-like%2520cognitive%2520abilities.%250AThis%2520survey%2520begins%2520with%2520a%2520brief%2520overview%2520of%2520the%2520progress%2520in%2520foundational%2520LLMs%250Aand%2520the%2520early%2520development%2520of%2520System%25202%2520technologies%252C%2520exploring%2520how%2520their%250Acombination%2520has%2520paved%2520the%2520way%2520for%2520reasoning%2520LLMs.%2520Next%252C%2520we%2520discuss%2520how%2520to%250Aconstruct%2520reasoning%2520LLMs%252C%2520analyzing%2520their%2520features%252C%2520the%2520core%2520methods%2520enabling%250Aadvanced%2520reasoning%252C%2520and%2520the%2520evolution%2520of%2520various%2520reasoning%2520LLMs.%2520Additionally%252C%250Awe%2520provide%2520an%2520overview%2520of%2520reasoning%2520benchmarks%252C%2520offering%2520an%2520in-depth%2520comparison%250Aof%2520the%2520performance%2520of%2520representative%2520reasoning%2520LLMs.%2520Finally%252C%2520we%2520explore%250Apromising%2520directions%2520for%2520advancing%2520reasoning%2520LLMs%2520and%2520maintain%2520a%2520real-time%250A%255Chref%257Bhttps%253A//github.com/zzli2022/Awesome-Slow-Reason-System%257D%257BGitHub%250ARepository%257D%2520to%2520track%2520the%2520latest%2520developments.%2520We%2520hope%2520this%2520survey%2520will%2520serve%2520as%250Aa%2520valuable%2520resource%2520to%2520inspire%2520innovation%2520and%2520drive%2520progress%2520in%2520this%2520rapidly%250Aevolving%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17419v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20System%201%20to%20System%202%3A%20A%20Survey%20of%20Reasoning%20Large%20Language%20Models&entry.906535625=Zhong-Zhi%20Li%20and%20Duzhen%20Zhang%20and%20Ming-Liang%20Zhang%20and%20Jiaxin%20Zhang%20and%20Zengyan%20Liu%20and%20Yuxuan%20Yao%20and%20Haotian%20Xu%20and%20Junhao%20Zheng%20and%20Pei-Jie%20Wang%20and%20Xiuyi%20Chen%20and%20Yingying%20Zhang%20and%20Fei%20Yin%20and%20Jiahua%20Dong%20and%20Zhijiang%20Guo%20and%20Le%20Song%20and%20Cheng-Lin%20Liu&entry.1292438233=%20%20Achieving%20human-level%20intelligence%20requires%20refining%20the%20transition%20from%20the%0Afast%2C%20intuitive%20System%201%20to%20the%20slower%2C%20more%20deliberate%20System%202%20reasoning.%0AWhile%20System%201%20excels%20in%20quick%2C%20heuristic%20decisions%2C%20System%202%20relies%20on%20logical%0Areasoning%20for%20more%20accurate%20judgments%20and%20reduced%20biases.%20Foundational%20Large%0ALanguage%20Models%20%28LLMs%29%20excel%20at%20fast%20decision-making%20but%20lack%20the%20depth%20for%0Acomplex%20reasoning%2C%20as%20they%20have%20not%20yet%20fully%20embraced%20the%20step-by-step%0Aanalysis%20characteristic%20of%20true%20System%202%20thinking.%20Recently%2C%20reasoning%20LLMs%0Alike%20OpenAI%27s%20o1/o3%20and%20DeepSeek%27s%20R1%20have%20demonstrated%20expert-level%0Aperformance%20in%20fields%20such%20as%20mathematics%20and%20coding%2C%20closely%20mimicking%20the%0Adeliberate%20reasoning%20of%20System%202%20and%20showcasing%20human-like%20cognitive%20abilities.%0AThis%20survey%20begins%20with%20a%20brief%20overview%20of%20the%20progress%20in%20foundational%20LLMs%0Aand%20the%20early%20development%20of%20System%202%20technologies%2C%20exploring%20how%20their%0Acombination%20has%20paved%20the%20way%20for%20reasoning%20LLMs.%20Next%2C%20we%20discuss%20how%20to%0Aconstruct%20reasoning%20LLMs%2C%20analyzing%20their%20features%2C%20the%20core%20methods%20enabling%0Aadvanced%20reasoning%2C%20and%20the%20evolution%20of%20various%20reasoning%20LLMs.%20Additionally%2C%0Awe%20provide%20an%20overview%20of%20reasoning%20benchmarks%2C%20offering%20an%20in-depth%20comparison%0Aof%20the%20performance%20of%20representative%20reasoning%20LLMs.%20Finally%2C%20we%20explore%0Apromising%20directions%20for%20advancing%20reasoning%20LLMs%20and%20maintain%20a%20real-time%0A%5Chref%7Bhttps%3A//github.com/zzli2022/Awesome-Slow-Reason-System%7D%7BGitHub%0ARepository%7D%20to%20track%20the%20latest%20developments.%20We%20hope%20this%20survey%20will%20serve%20as%0Aa%20valuable%20resource%20to%20inspire%20innovation%20and%20drive%20progress%20in%20this%20rapidly%0Aevolving%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17419v2&entry.124074799=Read"},
{"title": "FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in\n  Object-Based Common Sense Reasoning for Disaster Response", "author": "Mollie Shichman and Claire Bonial and Austin Blodgett and Taylor Hudson and Francis Ferraro and Rachel Rudinger", "abstract": "  Large Language Models (LLMs) have the potential for substantial common sense\nreasoning. However, these capabilities are often emergent in larger models.\nThis means smaller models that can be run locally are less helpful and capable\nwith respect to certain reasoning tasks. To meet our problem space\nrequirements, we fine-tune smaller LLMs to disaster domains, as these domains\ninvolve complex and low-frequency physical common sense knowledge. We introduce\na pipeline to create Field Ready Instruction Decoding Agent (FRIDA) models,\nwhere domain experts and linguists combine their knowledge to make high-quality\nseed data that is used to generate synthetic data for fine-tuning. We create a\nset of 130 seed instructions for synthetic generation, a synthetic dataset of\n25000 instructions, and 119 evaluation instructions relating to both general\nand earthquake-specific object affordances. We fine-tune several LLaMa and\nMistral instruction-tuned models and find that FRIDA models outperform their\nbase models at a variety of sizes. We then run an ablation study to understand\nwhich kinds of synthetic data most affect performance and find that training\nphysical state and object function common sense knowledge alone improves over\nFRIDA models trained on all data. We conclude that the FRIDA pipeline is\ncapable of instilling general common sense, but needs to be augmented with\ninformation retrieval for specific domain knowledge.\n", "link": "http://arxiv.org/abs/2502.18452v1", "date": "2025-02-25", "relevancy": 2.156, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5435}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5435}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRIDA%20to%20the%20Rescue%21%20Analyzing%20Synthetic%20Data%20Effectiveness%20in%0A%20%20Object-Based%20Common%20Sense%20Reasoning%20for%20Disaster%20Response&body=Title%3A%20FRIDA%20to%20the%20Rescue%21%20Analyzing%20Synthetic%20Data%20Effectiveness%20in%0A%20%20Object-Based%20Common%20Sense%20Reasoning%20for%20Disaster%20Response%0AAuthor%3A%20Mollie%20Shichman%20and%20Claire%20Bonial%20and%20Austin%20Blodgett%20and%20Taylor%20Hudson%20and%20Francis%20Ferraro%20and%20Rachel%20Rudinger%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20the%20potential%20for%20substantial%20common%20sense%0Areasoning.%20However%2C%20these%20capabilities%20are%20often%20emergent%20in%20larger%20models.%0AThis%20means%20smaller%20models%20that%20can%20be%20run%20locally%20are%20less%20helpful%20and%20capable%0Awith%20respect%20to%20certain%20reasoning%20tasks.%20To%20meet%20our%20problem%20space%0Arequirements%2C%20we%20fine-tune%20smaller%20LLMs%20to%20disaster%20domains%2C%20as%20these%20domains%0Ainvolve%20complex%20and%20low-frequency%20physical%20common%20sense%20knowledge.%20We%20introduce%0Aa%20pipeline%20to%20create%20Field%20Ready%20Instruction%20Decoding%20Agent%20%28FRIDA%29%20models%2C%0Awhere%20domain%20experts%20and%20linguists%20combine%20their%20knowledge%20to%20make%20high-quality%0Aseed%20data%20that%20is%20used%20to%20generate%20synthetic%20data%20for%20fine-tuning.%20We%20create%20a%0Aset%20of%20130%20seed%20instructions%20for%20synthetic%20generation%2C%20a%20synthetic%20dataset%20of%0A25000%20instructions%2C%20and%20119%20evaluation%20instructions%20relating%20to%20both%20general%0Aand%20earthquake-specific%20object%20affordances.%20We%20fine-tune%20several%20LLaMa%20and%0AMistral%20instruction-tuned%20models%20and%20find%20that%20FRIDA%20models%20outperform%20their%0Abase%20models%20at%20a%20variety%20of%20sizes.%20We%20then%20run%20an%20ablation%20study%20to%20understand%0Awhich%20kinds%20of%20synthetic%20data%20most%20affect%20performance%20and%20find%20that%20training%0Aphysical%20state%20and%20object%20function%20common%20sense%20knowledge%20alone%20improves%20over%0AFRIDA%20models%20trained%20on%20all%20data.%20We%20conclude%20that%20the%20FRIDA%20pipeline%20is%0Acapable%20of%20instilling%20general%20common%20sense%2C%20but%20needs%20to%20be%20augmented%20with%0Ainformation%20retrieval%20for%20specific%20domain%20knowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18452v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRIDA%2520to%2520the%2520Rescue%2521%2520Analyzing%2520Synthetic%2520Data%2520Effectiveness%2520in%250A%2520%2520Object-Based%2520Common%2520Sense%2520Reasoning%2520for%2520Disaster%2520Response%26entry.906535625%3DMollie%2520Shichman%2520and%2520Claire%2520Bonial%2520and%2520Austin%2520Blodgett%2520and%2520Taylor%2520Hudson%2520and%2520Francis%2520Ferraro%2520and%2520Rachel%2520Rudinger%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520the%2520potential%2520for%2520substantial%2520common%2520sense%250Areasoning.%2520However%252C%2520these%2520capabilities%2520are%2520often%2520emergent%2520in%2520larger%2520models.%250AThis%2520means%2520smaller%2520models%2520that%2520can%2520be%2520run%2520locally%2520are%2520less%2520helpful%2520and%2520capable%250Awith%2520respect%2520to%2520certain%2520reasoning%2520tasks.%2520To%2520meet%2520our%2520problem%2520space%250Arequirements%252C%2520we%2520fine-tune%2520smaller%2520LLMs%2520to%2520disaster%2520domains%252C%2520as%2520these%2520domains%250Ainvolve%2520complex%2520and%2520low-frequency%2520physical%2520common%2520sense%2520knowledge.%2520We%2520introduce%250Aa%2520pipeline%2520to%2520create%2520Field%2520Ready%2520Instruction%2520Decoding%2520Agent%2520%2528FRIDA%2529%2520models%252C%250Awhere%2520domain%2520experts%2520and%2520linguists%2520combine%2520their%2520knowledge%2520to%2520make%2520high-quality%250Aseed%2520data%2520that%2520is%2520used%2520to%2520generate%2520synthetic%2520data%2520for%2520fine-tuning.%2520We%2520create%2520a%250Aset%2520of%2520130%2520seed%2520instructions%2520for%2520synthetic%2520generation%252C%2520a%2520synthetic%2520dataset%2520of%250A25000%2520instructions%252C%2520and%2520119%2520evaluation%2520instructions%2520relating%2520to%2520both%2520general%250Aand%2520earthquake-specific%2520object%2520affordances.%2520We%2520fine-tune%2520several%2520LLaMa%2520and%250AMistral%2520instruction-tuned%2520models%2520and%2520find%2520that%2520FRIDA%2520models%2520outperform%2520their%250Abase%2520models%2520at%2520a%2520variety%2520of%2520sizes.%2520We%2520then%2520run%2520an%2520ablation%2520study%2520to%2520understand%250Awhich%2520kinds%2520of%2520synthetic%2520data%2520most%2520affect%2520performance%2520and%2520find%2520that%2520training%250Aphysical%2520state%2520and%2520object%2520function%2520common%2520sense%2520knowledge%2520alone%2520improves%2520over%250AFRIDA%2520models%2520trained%2520on%2520all%2520data.%2520We%2520conclude%2520that%2520the%2520FRIDA%2520pipeline%2520is%250Acapable%2520of%2520instilling%2520general%2520common%2520sense%252C%2520but%2520needs%2520to%2520be%2520augmented%2520with%250Ainformation%2520retrieval%2520for%2520specific%2520domain%2520knowledge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18452v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRIDA%20to%20the%20Rescue%21%20Analyzing%20Synthetic%20Data%20Effectiveness%20in%0A%20%20Object-Based%20Common%20Sense%20Reasoning%20for%20Disaster%20Response&entry.906535625=Mollie%20Shichman%20and%20Claire%20Bonial%20and%20Austin%20Blodgett%20and%20Taylor%20Hudson%20and%20Francis%20Ferraro%20and%20Rachel%20Rudinger&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20the%20potential%20for%20substantial%20common%20sense%0Areasoning.%20However%2C%20these%20capabilities%20are%20often%20emergent%20in%20larger%20models.%0AThis%20means%20smaller%20models%20that%20can%20be%20run%20locally%20are%20less%20helpful%20and%20capable%0Awith%20respect%20to%20certain%20reasoning%20tasks.%20To%20meet%20our%20problem%20space%0Arequirements%2C%20we%20fine-tune%20smaller%20LLMs%20to%20disaster%20domains%2C%20as%20these%20domains%0Ainvolve%20complex%20and%20low-frequency%20physical%20common%20sense%20knowledge.%20We%20introduce%0Aa%20pipeline%20to%20create%20Field%20Ready%20Instruction%20Decoding%20Agent%20%28FRIDA%29%20models%2C%0Awhere%20domain%20experts%20and%20linguists%20combine%20their%20knowledge%20to%20make%20high-quality%0Aseed%20data%20that%20is%20used%20to%20generate%20synthetic%20data%20for%20fine-tuning.%20We%20create%20a%0Aset%20of%20130%20seed%20instructions%20for%20synthetic%20generation%2C%20a%20synthetic%20dataset%20of%0A25000%20instructions%2C%20and%20119%20evaluation%20instructions%20relating%20to%20both%20general%0Aand%20earthquake-specific%20object%20affordances.%20We%20fine-tune%20several%20LLaMa%20and%0AMistral%20instruction-tuned%20models%20and%20find%20that%20FRIDA%20models%20outperform%20their%0Abase%20models%20at%20a%20variety%20of%20sizes.%20We%20then%20run%20an%20ablation%20study%20to%20understand%0Awhich%20kinds%20of%20synthetic%20data%20most%20affect%20performance%20and%20find%20that%20training%0Aphysical%20state%20and%20object%20function%20common%20sense%20knowledge%20alone%20improves%20over%0AFRIDA%20models%20trained%20on%20all%20data.%20We%20conclude%20that%20the%20FRIDA%20pipeline%20is%0Acapable%20of%20instilling%20general%20common%20sense%2C%20but%20needs%20to%20be%20augmented%20with%0Ainformation%20retrieval%20for%20specific%20domain%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18452v1&entry.124074799=Read"},
{"title": "SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention\n  and Low-Rank Adaptation in Large Language Models", "author": "Zhang Yuxuan and Li Ruizhe", "abstract": "  With the rapid development of large language models (LLMs), fully fine-tuning\n(FT) these models has become increasingly impractical due to the high\ncomputational demands. Additionally, FT can lead to catastrophic forgetting. As\nan alternative, Low-Rank Adaptation (LoRA) has been proposed, which fine-tunes\nonly a small subset of parameters, achieving similar performance to FT while\nsignificantly reducing resource requirements. However, since LoRA inherits FT's\ndesign, the issue of catastrophic forgetting remains.\n  To address these challenges, we propose SECURA: Sigmoid-Enhanced CUR\nDecomposition LoRA, a novel parameter-efficient fine-tuning (PEFT) variant that\nmitigates catastrophic forgetting while improving fine-tuning performance. Our\nmethod introduces a new normalization technique, SigNorm, to enhance parameter\nretention and overall performance.\n  SECURA has been evaluated on a variety of tasks, including mathematical\nproblem-solving (GSM8K), challenging question-answering (CNNDM), translation\n(NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results\nshow that SECURA achieves an average fine-tuning improvement of 3.59% across\nfour multiple-choice question (MCQ) tasks and a 2.51% improvement across five\nquestion-answering (QA) tasks on models such as Gemma2 2b, Qwen2 1.5b, Qwen 2\n7b, Llama3 8b, and Llama3.1 8b, compared to DoRA. Moreover, SECURA demonstrates\nsuperior knowledge retention capabilities, maintaining more than 70% accuracy\non basic LLM knowledge across 16 continual learning tests, outperforming\nExperience Replay (ER), Sequential Learning (SEQ), EWC, I-LoRA, and CUR-LoRA.\n", "link": "http://arxiv.org/abs/2502.18168v1", "date": "2025-02-25", "relevancy": 2.1306, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5331}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5331}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SECURA%3A%20Sigmoid-Enhanced%20CUR%20Decomposition%20with%20Uninterrupted%20Retention%0A%20%20and%20Low-Rank%20Adaptation%20in%20Large%20Language%20Models&body=Title%3A%20SECURA%3A%20Sigmoid-Enhanced%20CUR%20Decomposition%20with%20Uninterrupted%20Retention%0A%20%20and%20Low-Rank%20Adaptation%20in%20Large%20Language%20Models%0AAuthor%3A%20Zhang%20Yuxuan%20and%20Li%20Ruizhe%0AAbstract%3A%20%20%20With%20the%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%2C%20fully%20fine-tuning%0A%28FT%29%20these%20models%20has%20become%20increasingly%20impractical%20due%20to%20the%20high%0Acomputational%20demands.%20Additionally%2C%20FT%20can%20lead%20to%20catastrophic%20forgetting.%20As%0Aan%20alternative%2C%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20been%20proposed%2C%20which%20fine-tunes%0Aonly%20a%20small%20subset%20of%20parameters%2C%20achieving%20similar%20performance%20to%20FT%20while%0Asignificantly%20reducing%20resource%20requirements.%20However%2C%20since%20LoRA%20inherits%20FT%27s%0Adesign%2C%20the%20issue%20of%20catastrophic%20forgetting%20remains.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20SECURA%3A%20Sigmoid-Enhanced%20CUR%0ADecomposition%20LoRA%2C%20a%20novel%20parameter-efficient%20fine-tuning%20%28PEFT%29%20variant%20that%0Amitigates%20catastrophic%20forgetting%20while%20improving%20fine-tuning%20performance.%20Our%0Amethod%20introduces%20a%20new%20normalization%20technique%2C%20SigNorm%2C%20to%20enhance%20parameter%0Aretention%20and%20overall%20performance.%0A%20%20SECURA%20has%20been%20evaluated%20on%20a%20variety%20of%20tasks%2C%20including%20mathematical%0Aproblem-solving%20%28GSM8K%29%2C%20challenging%20question-answering%20%28CNNDM%29%2C%20translation%0A%28NewsDE%29%2C%20and%20complex%20multiple-choice%20reasoning%20%28LogiQA%29.%20Experimental%20results%0Ashow%20that%20SECURA%20achieves%20an%20average%20fine-tuning%20improvement%20of%203.59%25%20across%0Afour%20multiple-choice%20question%20%28MCQ%29%20tasks%20and%20a%202.51%25%20improvement%20across%20five%0Aquestion-answering%20%28QA%29%20tasks%20on%20models%20such%20as%20Gemma2%202b%2C%20Qwen2%201.5b%2C%20Qwen%202%0A7b%2C%20Llama3%208b%2C%20and%20Llama3.1%208b%2C%20compared%20to%20DoRA.%20Moreover%2C%20SECURA%20demonstrates%0Asuperior%20knowledge%20retention%20capabilities%2C%20maintaining%20more%20than%2070%25%20accuracy%0Aon%20basic%20LLM%20knowledge%20across%2016%20continual%20learning%20tests%2C%20outperforming%0AExperience%20Replay%20%28ER%29%2C%20Sequential%20Learning%20%28SEQ%29%2C%20EWC%2C%20I-LoRA%2C%20and%20CUR-LoRA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSECURA%253A%2520Sigmoid-Enhanced%2520CUR%2520Decomposition%2520with%2520Uninterrupted%2520Retention%250A%2520%2520and%2520Low-Rank%2520Adaptation%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DZhang%2520Yuxuan%2520and%2520Li%2520Ruizhe%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520development%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520fully%2520fine-tuning%250A%2528FT%2529%2520these%2520models%2520has%2520become%2520increasingly%2520impractical%2520due%2520to%2520the%2520high%250Acomputational%2520demands.%2520Additionally%252C%2520FT%2520can%2520lead%2520to%2520catastrophic%2520forgetting.%2520As%250Aan%2520alternative%252C%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520has%2520been%2520proposed%252C%2520which%2520fine-tunes%250Aonly%2520a%2520small%2520subset%2520of%2520parameters%252C%2520achieving%2520similar%2520performance%2520to%2520FT%2520while%250Asignificantly%2520reducing%2520resource%2520requirements.%2520However%252C%2520since%2520LoRA%2520inherits%2520FT%2527s%250Adesign%252C%2520the%2520issue%2520of%2520catastrophic%2520forgetting%2520remains.%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520SECURA%253A%2520Sigmoid-Enhanced%2520CUR%250ADecomposition%2520LoRA%252C%2520a%2520novel%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520variant%2520that%250Amitigates%2520catastrophic%2520forgetting%2520while%2520improving%2520fine-tuning%2520performance.%2520Our%250Amethod%2520introduces%2520a%2520new%2520normalization%2520technique%252C%2520SigNorm%252C%2520to%2520enhance%2520parameter%250Aretention%2520and%2520overall%2520performance.%250A%2520%2520SECURA%2520has%2520been%2520evaluated%2520on%2520a%2520variety%2520of%2520tasks%252C%2520including%2520mathematical%250Aproblem-solving%2520%2528GSM8K%2529%252C%2520challenging%2520question-answering%2520%2528CNNDM%2529%252C%2520translation%250A%2528NewsDE%2529%252C%2520and%2520complex%2520multiple-choice%2520reasoning%2520%2528LogiQA%2529.%2520Experimental%2520results%250Ashow%2520that%2520SECURA%2520achieves%2520an%2520average%2520fine-tuning%2520improvement%2520of%25203.59%2525%2520across%250Afour%2520multiple-choice%2520question%2520%2528MCQ%2529%2520tasks%2520and%2520a%25202.51%2525%2520improvement%2520across%2520five%250Aquestion-answering%2520%2528QA%2529%2520tasks%2520on%2520models%2520such%2520as%2520Gemma2%25202b%252C%2520Qwen2%25201.5b%252C%2520Qwen%25202%250A7b%252C%2520Llama3%25208b%252C%2520and%2520Llama3.1%25208b%252C%2520compared%2520to%2520DoRA.%2520Moreover%252C%2520SECURA%2520demonstrates%250Asuperior%2520knowledge%2520retention%2520capabilities%252C%2520maintaining%2520more%2520than%252070%2525%2520accuracy%250Aon%2520basic%2520LLM%2520knowledge%2520across%252016%2520continual%2520learning%2520tests%252C%2520outperforming%250AExperience%2520Replay%2520%2528ER%2529%252C%2520Sequential%2520Learning%2520%2528SEQ%2529%252C%2520EWC%252C%2520I-LoRA%252C%2520and%2520CUR-LoRA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SECURA%3A%20Sigmoid-Enhanced%20CUR%20Decomposition%20with%20Uninterrupted%20Retention%0A%20%20and%20Low-Rank%20Adaptation%20in%20Large%20Language%20Models&entry.906535625=Zhang%20Yuxuan%20and%20Li%20Ruizhe&entry.1292438233=%20%20With%20the%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%2C%20fully%20fine-tuning%0A%28FT%29%20these%20models%20has%20become%20increasingly%20impractical%20due%20to%20the%20high%0Acomputational%20demands.%20Additionally%2C%20FT%20can%20lead%20to%20catastrophic%20forgetting.%20As%0Aan%20alternative%2C%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20been%20proposed%2C%20which%20fine-tunes%0Aonly%20a%20small%20subset%20of%20parameters%2C%20achieving%20similar%20performance%20to%20FT%20while%0Asignificantly%20reducing%20resource%20requirements.%20However%2C%20since%20LoRA%20inherits%20FT%27s%0Adesign%2C%20the%20issue%20of%20catastrophic%20forgetting%20remains.%0A%20%20To%20address%20these%20challenges%2C%20we%20propose%20SECURA%3A%20Sigmoid-Enhanced%20CUR%0ADecomposition%20LoRA%2C%20a%20novel%20parameter-efficient%20fine-tuning%20%28PEFT%29%20variant%20that%0Amitigates%20catastrophic%20forgetting%20while%20improving%20fine-tuning%20performance.%20Our%0Amethod%20introduces%20a%20new%20normalization%20technique%2C%20SigNorm%2C%20to%20enhance%20parameter%0Aretention%20and%20overall%20performance.%0A%20%20SECURA%20has%20been%20evaluated%20on%20a%20variety%20of%20tasks%2C%20including%20mathematical%0Aproblem-solving%20%28GSM8K%29%2C%20challenging%20question-answering%20%28CNNDM%29%2C%20translation%0A%28NewsDE%29%2C%20and%20complex%20multiple-choice%20reasoning%20%28LogiQA%29.%20Experimental%20results%0Ashow%20that%20SECURA%20achieves%20an%20average%20fine-tuning%20improvement%20of%203.59%25%20across%0Afour%20multiple-choice%20question%20%28MCQ%29%20tasks%20and%20a%202.51%25%20improvement%20across%20five%0Aquestion-answering%20%28QA%29%20tasks%20on%20models%20such%20as%20Gemma2%202b%2C%20Qwen2%201.5b%2C%20Qwen%202%0A7b%2C%20Llama3%208b%2C%20and%20Llama3.1%208b%2C%20compared%20to%20DoRA.%20Moreover%2C%20SECURA%20demonstrates%0Asuperior%20knowledge%20retention%20capabilities%2C%20maintaining%20more%20than%2070%25%20accuracy%0Aon%20basic%20LLM%20knowledge%20across%2016%20continual%20learning%20tests%2C%20outperforming%0AExperience%20Replay%20%28ER%29%2C%20Sequential%20Learning%20%28SEQ%29%2C%20EWC%2C%20I-LoRA%2C%20and%20CUR-LoRA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18168v1&entry.124074799=Read"},
{"title": "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language\n  Models with Reinforcement Learning", "author": "Chanwoo Park and Seungju Han and Xingzhi Guo and Asuman Ozdaglar and Kaiqing Zhang and Joo-Kyung Kim", "abstract": "  Leveraging multiple large language models (LLMs) to build collaborative\nmulti-agentic workflows has demonstrated significant potential. However, most\nprevious studies focus on prompting the out-of-the-box LLMs, relying on their\ninnate capability for collaboration, which may not improve LLMs' performance as\nshown recently. In this paper, we introduce a new post-training paradigm MAPoRL\n(Multi-Agent Post-co-training for collaborative LLMs with Reinforcement\nLearning), to explicitly elicit the collaborative behaviors and further unleash\nthe power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first\ngenerate their own responses independently and engage in a multi-turn\ndiscussion to collaboratively improve the final answer. In the end, a MAPoRL\nverifier evaluates both the answer and the discussion, by assigning a score\nthat verifies the correctness of the answer, while adding incentives to\nencourage corrective and persuasive discussions. The score serves as the\nco-training reward, and is then maximized through multi-agent RL. Unlike\nexisting LLM post-training paradigms, MAPoRL advocates the co-training of\nmultiple LLMs together using RL for better generalization. Accompanied by\nanalytical insights, our experiments demonstrate that training individual LLMs\nalone is insufficient to induce effective collaboration. In contrast,\nmulti-agent co-training can boost the collaboration performance across\nbenchmarks, with generalization to unseen domains.\n", "link": "http://arxiv.org/abs/2502.18439v1", "date": "2025-02-25", "relevancy": 2.1288, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5592}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.52}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAPoRL%3A%20Multi-Agent%20Post-Co-Training%20for%20Collaborative%20Large%20Language%0A%20%20Models%20with%20Reinforcement%20Learning&body=Title%3A%20MAPoRL%3A%20Multi-Agent%20Post-Co-Training%20for%20Collaborative%20Large%20Language%0A%20%20Models%20with%20Reinforcement%20Learning%0AAuthor%3A%20Chanwoo%20Park%20and%20Seungju%20Han%20and%20Xingzhi%20Guo%20and%20Asuman%20Ozdaglar%20and%20Kaiqing%20Zhang%20and%20Joo-Kyung%20Kim%0AAbstract%3A%20%20%20Leveraging%20multiple%20large%20language%20models%20%28LLMs%29%20to%20build%20collaborative%0Amulti-agentic%20workflows%20has%20demonstrated%20significant%20potential.%20However%2C%20most%0Aprevious%20studies%20focus%20on%20prompting%20the%20out-of-the-box%20LLMs%2C%20relying%20on%20their%0Ainnate%20capability%20for%20collaboration%2C%20which%20may%20not%20improve%20LLMs%27%20performance%20as%0Ashown%20recently.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20post-training%20paradigm%20MAPoRL%0A%28Multi-Agent%20Post-co-training%20for%20collaborative%20LLMs%20with%20Reinforcement%0ALearning%29%2C%20to%20explicitly%20elicit%20the%20collaborative%20behaviors%20and%20further%20unleash%0Athe%20power%20of%20multi-agentic%20LLM%20frameworks.%20In%20MAPoRL%2C%20multiple%20LLMs%20first%0Agenerate%20their%20own%20responses%20independently%20and%20engage%20in%20a%20multi-turn%0Adiscussion%20to%20collaboratively%20improve%20the%20final%20answer.%20In%20the%20end%2C%20a%20MAPoRL%0Averifier%20evaluates%20both%20the%20answer%20and%20the%20discussion%2C%20by%20assigning%20a%20score%0Athat%20verifies%20the%20correctness%20of%20the%20answer%2C%20while%20adding%20incentives%20to%0Aencourage%20corrective%20and%20persuasive%20discussions.%20The%20score%20serves%20as%20the%0Aco-training%20reward%2C%20and%20is%20then%20maximized%20through%20multi-agent%20RL.%20Unlike%0Aexisting%20LLM%20post-training%20paradigms%2C%20MAPoRL%20advocates%20the%20co-training%20of%0Amultiple%20LLMs%20together%20using%20RL%20for%20better%20generalization.%20Accompanied%20by%0Aanalytical%20insights%2C%20our%20experiments%20demonstrate%20that%20training%20individual%20LLMs%0Aalone%20is%20insufficient%20to%20induce%20effective%20collaboration.%20In%20contrast%2C%0Amulti-agent%20co-training%20can%20boost%20the%20collaboration%20performance%20across%0Abenchmarks%2C%20with%20generalization%20to%20unseen%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAPoRL%253A%2520Multi-Agent%2520Post-Co-Training%2520for%2520Collaborative%2520Large%2520Language%250A%2520%2520Models%2520with%2520Reinforcement%2520Learning%26entry.906535625%3DChanwoo%2520Park%2520and%2520Seungju%2520Han%2520and%2520Xingzhi%2520Guo%2520and%2520Asuman%2520Ozdaglar%2520and%2520Kaiqing%2520Zhang%2520and%2520Joo-Kyung%2520Kim%26entry.1292438233%3D%2520%2520Leveraging%2520multiple%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520build%2520collaborative%250Amulti-agentic%2520workflows%2520has%2520demonstrated%2520significant%2520potential.%2520However%252C%2520most%250Aprevious%2520studies%2520focus%2520on%2520prompting%2520the%2520out-of-the-box%2520LLMs%252C%2520relying%2520on%2520their%250Ainnate%2520capability%2520for%2520collaboration%252C%2520which%2520may%2520not%2520improve%2520LLMs%2527%2520performance%2520as%250Ashown%2520recently.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520post-training%2520paradigm%2520MAPoRL%250A%2528Multi-Agent%2520Post-co-training%2520for%2520collaborative%2520LLMs%2520with%2520Reinforcement%250ALearning%2529%252C%2520to%2520explicitly%2520elicit%2520the%2520collaborative%2520behaviors%2520and%2520further%2520unleash%250Athe%2520power%2520of%2520multi-agentic%2520LLM%2520frameworks.%2520In%2520MAPoRL%252C%2520multiple%2520LLMs%2520first%250Agenerate%2520their%2520own%2520responses%2520independently%2520and%2520engage%2520in%2520a%2520multi-turn%250Adiscussion%2520to%2520collaboratively%2520improve%2520the%2520final%2520answer.%2520In%2520the%2520end%252C%2520a%2520MAPoRL%250Averifier%2520evaluates%2520both%2520the%2520answer%2520and%2520the%2520discussion%252C%2520by%2520assigning%2520a%2520score%250Athat%2520verifies%2520the%2520correctness%2520of%2520the%2520answer%252C%2520while%2520adding%2520incentives%2520to%250Aencourage%2520corrective%2520and%2520persuasive%2520discussions.%2520The%2520score%2520serves%2520as%2520the%250Aco-training%2520reward%252C%2520and%2520is%2520then%2520maximized%2520through%2520multi-agent%2520RL.%2520Unlike%250Aexisting%2520LLM%2520post-training%2520paradigms%252C%2520MAPoRL%2520advocates%2520the%2520co-training%2520of%250Amultiple%2520LLMs%2520together%2520using%2520RL%2520for%2520better%2520generalization.%2520Accompanied%2520by%250Aanalytical%2520insights%252C%2520our%2520experiments%2520demonstrate%2520that%2520training%2520individual%2520LLMs%250Aalone%2520is%2520insufficient%2520to%2520induce%2520effective%2520collaboration.%2520In%2520contrast%252C%250Amulti-agent%2520co-training%2520can%2520boost%2520the%2520collaboration%2520performance%2520across%250Abenchmarks%252C%2520with%2520generalization%2520to%2520unseen%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAPoRL%3A%20Multi-Agent%20Post-Co-Training%20for%20Collaborative%20Large%20Language%0A%20%20Models%20with%20Reinforcement%20Learning&entry.906535625=Chanwoo%20Park%20and%20Seungju%20Han%20and%20Xingzhi%20Guo%20and%20Asuman%20Ozdaglar%20and%20Kaiqing%20Zhang%20and%20Joo-Kyung%20Kim&entry.1292438233=%20%20Leveraging%20multiple%20large%20language%20models%20%28LLMs%29%20to%20build%20collaborative%0Amulti-agentic%20workflows%20has%20demonstrated%20significant%20potential.%20However%2C%20most%0Aprevious%20studies%20focus%20on%20prompting%20the%20out-of-the-box%20LLMs%2C%20relying%20on%20their%0Ainnate%20capability%20for%20collaboration%2C%20which%20may%20not%20improve%20LLMs%27%20performance%20as%0Ashown%20recently.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20post-training%20paradigm%20MAPoRL%0A%28Multi-Agent%20Post-co-training%20for%20collaborative%20LLMs%20with%20Reinforcement%0ALearning%29%2C%20to%20explicitly%20elicit%20the%20collaborative%20behaviors%20and%20further%20unleash%0Athe%20power%20of%20multi-agentic%20LLM%20frameworks.%20In%20MAPoRL%2C%20multiple%20LLMs%20first%0Agenerate%20their%20own%20responses%20independently%20and%20engage%20in%20a%20multi-turn%0Adiscussion%20to%20collaboratively%20improve%20the%20final%20answer.%20In%20the%20end%2C%20a%20MAPoRL%0Averifier%20evaluates%20both%20the%20answer%20and%20the%20discussion%2C%20by%20assigning%20a%20score%0Athat%20verifies%20the%20correctness%20of%20the%20answer%2C%20while%20adding%20incentives%20to%0Aencourage%20corrective%20and%20persuasive%20discussions.%20The%20score%20serves%20as%20the%0Aco-training%20reward%2C%20and%20is%20then%20maximized%20through%20multi-agent%20RL.%20Unlike%0Aexisting%20LLM%20post-training%20paradigms%2C%20MAPoRL%20advocates%20the%20co-training%20of%0Amultiple%20LLMs%20together%20using%20RL%20for%20better%20generalization.%20Accompanied%20by%0Aanalytical%20insights%2C%20our%20experiments%20demonstrate%20that%20training%20individual%20LLMs%0Aalone%20is%20insufficient%20to%20induce%20effective%20collaboration.%20In%20contrast%2C%0Amulti-agent%20co-training%20can%20boost%20the%20collaboration%20performance%20across%0Abenchmarks%2C%20with%20generalization%20to%20unseen%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18439v1&entry.124074799=Read"},
{"title": "Quantifying the Capability Boundary of DeepSeek Models: An\n  Application-Driven Performance Analysis", "author": "Kaikai Zhao and Zhaoxiang Liu and Xuejiao Lei and Ning Wang and Zhenhong Long and Jiaojiao Zhao and Zipeng Wang and Peijun Yang and Minjie Hua and Chaoyang Ma and Wen Liu and Kai Wang and Shiguo Lian", "abstract": "  DeepSeek-R1, known for its low training cost and exceptional reasoning\ncapabilities, has achieved state-of-the-art performance on various benchmarks.\nHowever, detailed evaluations from the perspective of real-world applications\nare lacking, making it challenging for users to select the most suitable\nDeepSeek models for their specific needs. To address this gap, we evaluate the\nDeepSeek-V3, DeepSeek-R1, DeepSeek-R1-Distill-Qwen series,\nDeepSeek-R1-Distill-Llama series, and their corresponding 4-bit quantized\nmodels on the enhanced A-Eval benchmark, A-Eval-2.0. By comparing original\ninstruction-tuned models with their distilled counterparts, we analyze how\nreasoning enhancements impact performance across diverse practical tasks. Our\nresults show that reasoning-enhanced models, while generally powerful, do not\nuniversally outperform across all tasks, with performance gains varying\nsignificantly across tasks and models. To further assist users in model\nselection, we quantify the capability boundary of DeepSeek models through\nperformance tier classifications and intuitive line charts. Specific examples\nprovide actionable insights to help users select and deploy the most\ncost-effective DeepSeek models, ensuring optimal performance and resource\nefficiency in real-world applications. It should be noted that, despite our\nefforts to establish a comprehensive, objective, and authoritative evaluation\nbenchmark, the selection of test samples, characteristics of data distribution,\nand the setting of evaluation criteria may inevitably introduce certain biases\ninto the evaluation results. We will continuously optimize the evaluation\nbenchmarks and periodically update this paper to provide more comprehensive and\naccurate evaluation results. Please refer to the latest version of the paper\nfor the most recent results and conclusions.\n", "link": "http://arxiv.org/abs/2502.11164v3", "date": "2025-02-25", "relevancy": 2.1269, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5384}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5384}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20the%20Capability%20Boundary%20of%20DeepSeek%20Models%3A%20An%0A%20%20Application-Driven%20Performance%20Analysis&body=Title%3A%20Quantifying%20the%20Capability%20Boundary%20of%20DeepSeek%20Models%3A%20An%0A%20%20Application-Driven%20Performance%20Analysis%0AAuthor%3A%20Kaikai%20Zhao%20and%20Zhaoxiang%20Liu%20and%20Xuejiao%20Lei%20and%20Ning%20Wang%20and%20Zhenhong%20Long%20and%20Jiaojiao%20Zhao%20and%20Zipeng%20Wang%20and%20Peijun%20Yang%20and%20Minjie%20Hua%20and%20Chaoyang%20Ma%20and%20Wen%20Liu%20and%20Kai%20Wang%20and%20Shiguo%20Lian%0AAbstract%3A%20%20%20DeepSeek-R1%2C%20known%20for%20its%20low%20training%20cost%20and%20exceptional%20reasoning%0Acapabilities%2C%20has%20achieved%20state-of-the-art%20performance%20on%20various%20benchmarks.%0AHowever%2C%20detailed%20evaluations%20from%20the%20perspective%20of%20real-world%20applications%0Aare%20lacking%2C%20making%20it%20challenging%20for%20users%20to%20select%20the%20most%20suitable%0ADeepSeek%20models%20for%20their%20specific%20needs.%20To%20address%20this%20gap%2C%20we%20evaluate%20the%0ADeepSeek-V3%2C%20DeepSeek-R1%2C%20DeepSeek-R1-Distill-Qwen%20series%2C%0ADeepSeek-R1-Distill-Llama%20series%2C%20and%20their%20corresponding%204-bit%20quantized%0Amodels%20on%20the%20enhanced%20A-Eval%20benchmark%2C%20A-Eval-2.0.%20By%20comparing%20original%0Ainstruction-tuned%20models%20with%20their%20distilled%20counterparts%2C%20we%20analyze%20how%0Areasoning%20enhancements%20impact%20performance%20across%20diverse%20practical%20tasks.%20Our%0Aresults%20show%20that%20reasoning-enhanced%20models%2C%20while%20generally%20powerful%2C%20do%20not%0Auniversally%20outperform%20across%20all%20tasks%2C%20with%20performance%20gains%20varying%0Asignificantly%20across%20tasks%20and%20models.%20To%20further%20assist%20users%20in%20model%0Aselection%2C%20we%20quantify%20the%20capability%20boundary%20of%20DeepSeek%20models%20through%0Aperformance%20tier%20classifications%20and%20intuitive%20line%20charts.%20Specific%20examples%0Aprovide%20actionable%20insights%20to%20help%20users%20select%20and%20deploy%20the%20most%0Acost-effective%20DeepSeek%20models%2C%20ensuring%20optimal%20performance%20and%20resource%0Aefficiency%20in%20real-world%20applications.%20It%20should%20be%20noted%20that%2C%20despite%20our%0Aefforts%20to%20establish%20a%20comprehensive%2C%20objective%2C%20and%20authoritative%20evaluation%0Abenchmark%2C%20the%20selection%20of%20test%20samples%2C%20characteristics%20of%20data%20distribution%2C%0Aand%20the%20setting%20of%20evaluation%20criteria%20may%20inevitably%20introduce%20certain%20biases%0Ainto%20the%20evaluation%20results.%20We%20will%20continuously%20optimize%20the%20evaluation%0Abenchmarks%20and%20periodically%20update%20this%20paper%20to%20provide%20more%20comprehensive%20and%0Aaccurate%20evaluation%20results.%20Please%20refer%20to%20the%20latest%20version%20of%20the%20paper%0Afor%20the%20most%20recent%20results%20and%20conclusions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11164v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520the%2520Capability%2520Boundary%2520of%2520DeepSeek%2520Models%253A%2520An%250A%2520%2520Application-Driven%2520Performance%2520Analysis%26entry.906535625%3DKaikai%2520Zhao%2520and%2520Zhaoxiang%2520Liu%2520and%2520Xuejiao%2520Lei%2520and%2520Ning%2520Wang%2520and%2520Zhenhong%2520Long%2520and%2520Jiaojiao%2520Zhao%2520and%2520Zipeng%2520Wang%2520and%2520Peijun%2520Yang%2520and%2520Minjie%2520Hua%2520and%2520Chaoyang%2520Ma%2520and%2520Wen%2520Liu%2520and%2520Kai%2520Wang%2520and%2520Shiguo%2520Lian%26entry.1292438233%3D%2520%2520DeepSeek-R1%252C%2520known%2520for%2520its%2520low%2520training%2520cost%2520and%2520exceptional%2520reasoning%250Acapabilities%252C%2520has%2520achieved%2520state-of-the-art%2520performance%2520on%2520various%2520benchmarks.%250AHowever%252C%2520detailed%2520evaluations%2520from%2520the%2520perspective%2520of%2520real-world%2520applications%250Aare%2520lacking%252C%2520making%2520it%2520challenging%2520for%2520users%2520to%2520select%2520the%2520most%2520suitable%250ADeepSeek%2520models%2520for%2520their%2520specific%2520needs.%2520To%2520address%2520this%2520gap%252C%2520we%2520evaluate%2520the%250ADeepSeek-V3%252C%2520DeepSeek-R1%252C%2520DeepSeek-R1-Distill-Qwen%2520series%252C%250ADeepSeek-R1-Distill-Llama%2520series%252C%2520and%2520their%2520corresponding%25204-bit%2520quantized%250Amodels%2520on%2520the%2520enhanced%2520A-Eval%2520benchmark%252C%2520A-Eval-2.0.%2520By%2520comparing%2520original%250Ainstruction-tuned%2520models%2520with%2520their%2520distilled%2520counterparts%252C%2520we%2520analyze%2520how%250Areasoning%2520enhancements%2520impact%2520performance%2520across%2520diverse%2520practical%2520tasks.%2520Our%250Aresults%2520show%2520that%2520reasoning-enhanced%2520models%252C%2520while%2520generally%2520powerful%252C%2520do%2520not%250Auniversally%2520outperform%2520across%2520all%2520tasks%252C%2520with%2520performance%2520gains%2520varying%250Asignificantly%2520across%2520tasks%2520and%2520models.%2520To%2520further%2520assist%2520users%2520in%2520model%250Aselection%252C%2520we%2520quantify%2520the%2520capability%2520boundary%2520of%2520DeepSeek%2520models%2520through%250Aperformance%2520tier%2520classifications%2520and%2520intuitive%2520line%2520charts.%2520Specific%2520examples%250Aprovide%2520actionable%2520insights%2520to%2520help%2520users%2520select%2520and%2520deploy%2520the%2520most%250Acost-effective%2520DeepSeek%2520models%252C%2520ensuring%2520optimal%2520performance%2520and%2520resource%250Aefficiency%2520in%2520real-world%2520applications.%2520It%2520should%2520be%2520noted%2520that%252C%2520despite%2520our%250Aefforts%2520to%2520establish%2520a%2520comprehensive%252C%2520objective%252C%2520and%2520authoritative%2520evaluation%250Abenchmark%252C%2520the%2520selection%2520of%2520test%2520samples%252C%2520characteristics%2520of%2520data%2520distribution%252C%250Aand%2520the%2520setting%2520of%2520evaluation%2520criteria%2520may%2520inevitably%2520introduce%2520certain%2520biases%250Ainto%2520the%2520evaluation%2520results.%2520We%2520will%2520continuously%2520optimize%2520the%2520evaluation%250Abenchmarks%2520and%2520periodically%2520update%2520this%2520paper%2520to%2520provide%2520more%2520comprehensive%2520and%250Aaccurate%2520evaluation%2520results.%2520Please%2520refer%2520to%2520the%2520latest%2520version%2520of%2520the%2520paper%250Afor%2520the%2520most%2520recent%2520results%2520and%2520conclusions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11164v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20the%20Capability%20Boundary%20of%20DeepSeek%20Models%3A%20An%0A%20%20Application-Driven%20Performance%20Analysis&entry.906535625=Kaikai%20Zhao%20and%20Zhaoxiang%20Liu%20and%20Xuejiao%20Lei%20and%20Ning%20Wang%20and%20Zhenhong%20Long%20and%20Jiaojiao%20Zhao%20and%20Zipeng%20Wang%20and%20Peijun%20Yang%20and%20Minjie%20Hua%20and%20Chaoyang%20Ma%20and%20Wen%20Liu%20and%20Kai%20Wang%20and%20Shiguo%20Lian&entry.1292438233=%20%20DeepSeek-R1%2C%20known%20for%20its%20low%20training%20cost%20and%20exceptional%20reasoning%0Acapabilities%2C%20has%20achieved%20state-of-the-art%20performance%20on%20various%20benchmarks.%0AHowever%2C%20detailed%20evaluations%20from%20the%20perspective%20of%20real-world%20applications%0Aare%20lacking%2C%20making%20it%20challenging%20for%20users%20to%20select%20the%20most%20suitable%0ADeepSeek%20models%20for%20their%20specific%20needs.%20To%20address%20this%20gap%2C%20we%20evaluate%20the%0ADeepSeek-V3%2C%20DeepSeek-R1%2C%20DeepSeek-R1-Distill-Qwen%20series%2C%0ADeepSeek-R1-Distill-Llama%20series%2C%20and%20their%20corresponding%204-bit%20quantized%0Amodels%20on%20the%20enhanced%20A-Eval%20benchmark%2C%20A-Eval-2.0.%20By%20comparing%20original%0Ainstruction-tuned%20models%20with%20their%20distilled%20counterparts%2C%20we%20analyze%20how%0Areasoning%20enhancements%20impact%20performance%20across%20diverse%20practical%20tasks.%20Our%0Aresults%20show%20that%20reasoning-enhanced%20models%2C%20while%20generally%20powerful%2C%20do%20not%0Auniversally%20outperform%20across%20all%20tasks%2C%20with%20performance%20gains%20varying%0Asignificantly%20across%20tasks%20and%20models.%20To%20further%20assist%20users%20in%20model%0Aselection%2C%20we%20quantify%20the%20capability%20boundary%20of%20DeepSeek%20models%20through%0Aperformance%20tier%20classifications%20and%20intuitive%20line%20charts.%20Specific%20examples%0Aprovide%20actionable%20insights%20to%20help%20users%20select%20and%20deploy%20the%20most%0Acost-effective%20DeepSeek%20models%2C%20ensuring%20optimal%20performance%20and%20resource%0Aefficiency%20in%20real-world%20applications.%20It%20should%20be%20noted%20that%2C%20despite%20our%0Aefforts%20to%20establish%20a%20comprehensive%2C%20objective%2C%20and%20authoritative%20evaluation%0Abenchmark%2C%20the%20selection%20of%20test%20samples%2C%20characteristics%20of%20data%20distribution%2C%0Aand%20the%20setting%20of%20evaluation%20criteria%20may%20inevitably%20introduce%20certain%20biases%0Ainto%20the%20evaluation%20results.%20We%20will%20continuously%20optimize%20the%20evaluation%0Abenchmarks%20and%20periodically%20update%20this%20paper%20to%20provide%20more%20comprehensive%20and%0Aaccurate%20evaluation%20results.%20Please%20refer%20to%20the%20latest%20version%20of%20the%20paper%0Afor%20the%20most%20recent%20results%20and%20conclusions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11164v3&entry.124074799=Read"},
{"title": "S$^4$ST: A Strong, Self-transferable, faSt, and Simple Scale\n  Transformation for Transferable Targeted Attack", "author": "Yongxiang Liu and Bowen Peng and Li Liu and Xiang Li", "abstract": "  Transferable Targeted Attacks (TTAs), which aim to deceive black-box models\ninto predicting specific erroneous labels, face significant challenges due to\nsevere overfitting to surrogate models. Although modifying image features to\ngenerate robust semantic patterns of the target class is a promising approach,\nexisting methods heavily rely on large-scale additional data. This dependence\nundermines the fair evaluation of TTA threats, potentially leading to a false\nsense of security or unnecessary overreactions. In this paper, we introduce two\nblind measures, surrogate self-alignment and self-transferability, to analyze\nthe effectiveness and correlations of basic transformations, to enhance\ndata-free attacks under strict black-box constraints. Our findings challenge\nconventional assumptions: (1) Attacking simple scaling transformations uniquely\nenhances targeted transferability, outperforming other basic transformations\nand rivaling leading complex methods; (2) Geometric and color transformations\nexhibit high internal redundancy despite weak inter-category correlations.\nThese insights drive the design and tuning of S4ST (Strong, Self-transferable,\nfaSt, Simple Scale Transformation), which integrates dimensionally consistent\nscaling, complementary low-redundancy transformations, and block-wise\noperations. Extensive experiments on the ImageNet-Compatible dataset\ndemonstrate that S4ST achieves a 77.7% average targeted success rate (tSuc),\nsurpassing existing transformations (+17.2% over H-Aug with only 26%\ncomputational time) and SOTA TTA solutions (+6.2% over SASD-WS with 1.2M\nsamples for post-training). Notably, it attains 69.6% and 55.3% average tSuc\nagainst three commercial APIs and vision-language models, respectively. This\nwork establishes a new SOTA for TTAs, highlights their potential threats, and\ncalls for a reevaluation of the data dependency in achieving targeted\ntransferability.\n", "link": "http://arxiv.org/abs/2410.13891v2", "date": "2025-02-25", "relevancy": 2.119, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5351}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5345}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S%24%5E4%24ST%3A%20A%20Strong%2C%20Self-transferable%2C%20faSt%2C%20and%20Simple%20Scale%0A%20%20Transformation%20for%20Transferable%20Targeted%20Attack&body=Title%3A%20S%24%5E4%24ST%3A%20A%20Strong%2C%20Self-transferable%2C%20faSt%2C%20and%20Simple%20Scale%0A%20%20Transformation%20for%20Transferable%20Targeted%20Attack%0AAuthor%3A%20Yongxiang%20Liu%20and%20Bowen%20Peng%20and%20Li%20Liu%20and%20Xiang%20Li%0AAbstract%3A%20%20%20Transferable%20Targeted%20Attacks%20%28TTAs%29%2C%20which%20aim%20to%20deceive%20black-box%20models%0Ainto%20predicting%20specific%20erroneous%20labels%2C%20face%20significant%20challenges%20due%20to%0Asevere%20overfitting%20to%20surrogate%20models.%20Although%20modifying%20image%20features%20to%0Agenerate%20robust%20semantic%20patterns%20of%20the%20target%20class%20is%20a%20promising%20approach%2C%0Aexisting%20methods%20heavily%20rely%20on%20large-scale%20additional%20data.%20This%20dependence%0Aundermines%20the%20fair%20evaluation%20of%20TTA%20threats%2C%20potentially%20leading%20to%20a%20false%0Asense%20of%20security%20or%20unnecessary%20overreactions.%20In%20this%20paper%2C%20we%20introduce%20two%0Ablind%20measures%2C%20surrogate%20self-alignment%20and%20self-transferability%2C%20to%20analyze%0Athe%20effectiveness%20and%20correlations%20of%20basic%20transformations%2C%20to%20enhance%0Adata-free%20attacks%20under%20strict%20black-box%20constraints.%20Our%20findings%20challenge%0Aconventional%20assumptions%3A%20%281%29%20Attacking%20simple%20scaling%20transformations%20uniquely%0Aenhances%20targeted%20transferability%2C%20outperforming%20other%20basic%20transformations%0Aand%20rivaling%20leading%20complex%20methods%3B%20%282%29%20Geometric%20and%20color%20transformations%0Aexhibit%20high%20internal%20redundancy%20despite%20weak%20inter-category%20correlations.%0AThese%20insights%20drive%20the%20design%20and%20tuning%20of%20S4ST%20%28Strong%2C%20Self-transferable%2C%0AfaSt%2C%20Simple%20Scale%20Transformation%29%2C%20which%20integrates%20dimensionally%20consistent%0Ascaling%2C%20complementary%20low-redundancy%20transformations%2C%20and%20block-wise%0Aoperations.%20Extensive%20experiments%20on%20the%20ImageNet-Compatible%20dataset%0Ademonstrate%20that%20S4ST%20achieves%20a%2077.7%25%20average%20targeted%20success%20rate%20%28tSuc%29%2C%0Asurpassing%20existing%20transformations%20%28%2B17.2%25%20over%20H-Aug%20with%20only%2026%25%0Acomputational%20time%29%20and%20SOTA%20TTA%20solutions%20%28%2B6.2%25%20over%20SASD-WS%20with%201.2M%0Asamples%20for%20post-training%29.%20Notably%2C%20it%20attains%2069.6%25%20and%2055.3%25%20average%20tSuc%0Aagainst%20three%20commercial%20APIs%20and%20vision-language%20models%2C%20respectively.%20This%0Awork%20establishes%20a%20new%20SOTA%20for%20TTAs%2C%20highlights%20their%20potential%20threats%2C%20and%0Acalls%20for%20a%20reevaluation%20of%20the%20data%20dependency%20in%20achieving%20targeted%0Atransferability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13891v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS%2524%255E4%2524ST%253A%2520A%2520Strong%252C%2520Self-transferable%252C%2520faSt%252C%2520and%2520Simple%2520Scale%250A%2520%2520Transformation%2520for%2520Transferable%2520Targeted%2520Attack%26entry.906535625%3DYongxiang%2520Liu%2520and%2520Bowen%2520Peng%2520and%2520Li%2520Liu%2520and%2520Xiang%2520Li%26entry.1292438233%3D%2520%2520Transferable%2520Targeted%2520Attacks%2520%2528TTAs%2529%252C%2520which%2520aim%2520to%2520deceive%2520black-box%2520models%250Ainto%2520predicting%2520specific%2520erroneous%2520labels%252C%2520face%2520significant%2520challenges%2520due%2520to%250Asevere%2520overfitting%2520to%2520surrogate%2520models.%2520Although%2520modifying%2520image%2520features%2520to%250Agenerate%2520robust%2520semantic%2520patterns%2520of%2520the%2520target%2520class%2520is%2520a%2520promising%2520approach%252C%250Aexisting%2520methods%2520heavily%2520rely%2520on%2520large-scale%2520additional%2520data.%2520This%2520dependence%250Aundermines%2520the%2520fair%2520evaluation%2520of%2520TTA%2520threats%252C%2520potentially%2520leading%2520to%2520a%2520false%250Asense%2520of%2520security%2520or%2520unnecessary%2520overreactions.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520two%250Ablind%2520measures%252C%2520surrogate%2520self-alignment%2520and%2520self-transferability%252C%2520to%2520analyze%250Athe%2520effectiveness%2520and%2520correlations%2520of%2520basic%2520transformations%252C%2520to%2520enhance%250Adata-free%2520attacks%2520under%2520strict%2520black-box%2520constraints.%2520Our%2520findings%2520challenge%250Aconventional%2520assumptions%253A%2520%25281%2529%2520Attacking%2520simple%2520scaling%2520transformations%2520uniquely%250Aenhances%2520targeted%2520transferability%252C%2520outperforming%2520other%2520basic%2520transformations%250Aand%2520rivaling%2520leading%2520complex%2520methods%253B%2520%25282%2529%2520Geometric%2520and%2520color%2520transformations%250Aexhibit%2520high%2520internal%2520redundancy%2520despite%2520weak%2520inter-category%2520correlations.%250AThese%2520insights%2520drive%2520the%2520design%2520and%2520tuning%2520of%2520S4ST%2520%2528Strong%252C%2520Self-transferable%252C%250AfaSt%252C%2520Simple%2520Scale%2520Transformation%2529%252C%2520which%2520integrates%2520dimensionally%2520consistent%250Ascaling%252C%2520complementary%2520low-redundancy%2520transformations%252C%2520and%2520block-wise%250Aoperations.%2520Extensive%2520experiments%2520on%2520the%2520ImageNet-Compatible%2520dataset%250Ademonstrate%2520that%2520S4ST%2520achieves%2520a%252077.7%2525%2520average%2520targeted%2520success%2520rate%2520%2528tSuc%2529%252C%250Asurpassing%2520existing%2520transformations%2520%2528%252B17.2%2525%2520over%2520H-Aug%2520with%2520only%252026%2525%250Acomputational%2520time%2529%2520and%2520SOTA%2520TTA%2520solutions%2520%2528%252B6.2%2525%2520over%2520SASD-WS%2520with%25201.2M%250Asamples%2520for%2520post-training%2529.%2520Notably%252C%2520it%2520attains%252069.6%2525%2520and%252055.3%2525%2520average%2520tSuc%250Aagainst%2520three%2520commercial%2520APIs%2520and%2520vision-language%2520models%252C%2520respectively.%2520This%250Awork%2520establishes%2520a%2520new%2520SOTA%2520for%2520TTAs%252C%2520highlights%2520their%2520potential%2520threats%252C%2520and%250Acalls%2520for%2520a%2520reevaluation%2520of%2520the%2520data%2520dependency%2520in%2520achieving%2520targeted%250Atransferability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13891v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S%24%5E4%24ST%3A%20A%20Strong%2C%20Self-transferable%2C%20faSt%2C%20and%20Simple%20Scale%0A%20%20Transformation%20for%20Transferable%20Targeted%20Attack&entry.906535625=Yongxiang%20Liu%20and%20Bowen%20Peng%20and%20Li%20Liu%20and%20Xiang%20Li&entry.1292438233=%20%20Transferable%20Targeted%20Attacks%20%28TTAs%29%2C%20which%20aim%20to%20deceive%20black-box%20models%0Ainto%20predicting%20specific%20erroneous%20labels%2C%20face%20significant%20challenges%20due%20to%0Asevere%20overfitting%20to%20surrogate%20models.%20Although%20modifying%20image%20features%20to%0Agenerate%20robust%20semantic%20patterns%20of%20the%20target%20class%20is%20a%20promising%20approach%2C%0Aexisting%20methods%20heavily%20rely%20on%20large-scale%20additional%20data.%20This%20dependence%0Aundermines%20the%20fair%20evaluation%20of%20TTA%20threats%2C%20potentially%20leading%20to%20a%20false%0Asense%20of%20security%20or%20unnecessary%20overreactions.%20In%20this%20paper%2C%20we%20introduce%20two%0Ablind%20measures%2C%20surrogate%20self-alignment%20and%20self-transferability%2C%20to%20analyze%0Athe%20effectiveness%20and%20correlations%20of%20basic%20transformations%2C%20to%20enhance%0Adata-free%20attacks%20under%20strict%20black-box%20constraints.%20Our%20findings%20challenge%0Aconventional%20assumptions%3A%20%281%29%20Attacking%20simple%20scaling%20transformations%20uniquely%0Aenhances%20targeted%20transferability%2C%20outperforming%20other%20basic%20transformations%0Aand%20rivaling%20leading%20complex%20methods%3B%20%282%29%20Geometric%20and%20color%20transformations%0Aexhibit%20high%20internal%20redundancy%20despite%20weak%20inter-category%20correlations.%0AThese%20insights%20drive%20the%20design%20and%20tuning%20of%20S4ST%20%28Strong%2C%20Self-transferable%2C%0AfaSt%2C%20Simple%20Scale%20Transformation%29%2C%20which%20integrates%20dimensionally%20consistent%0Ascaling%2C%20complementary%20low-redundancy%20transformations%2C%20and%20block-wise%0Aoperations.%20Extensive%20experiments%20on%20the%20ImageNet-Compatible%20dataset%0Ademonstrate%20that%20S4ST%20achieves%20a%2077.7%25%20average%20targeted%20success%20rate%20%28tSuc%29%2C%0Asurpassing%20existing%20transformations%20%28%2B17.2%25%20over%20H-Aug%20with%20only%2026%25%0Acomputational%20time%29%20and%20SOTA%20TTA%20solutions%20%28%2B6.2%25%20over%20SASD-WS%20with%201.2M%0Asamples%20for%20post-training%29.%20Notably%2C%20it%20attains%2069.6%25%20and%2055.3%25%20average%20tSuc%0Aagainst%20three%20commercial%20APIs%20and%20vision-language%20models%2C%20respectively.%20This%0Awork%20establishes%20a%20new%20SOTA%20for%20TTAs%2C%20highlights%20their%20potential%20threats%2C%20and%0Acalls%20for%20a%20reevaluation%20of%20the%20data%20dependency%20in%20achieving%20targeted%0Atransferability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13891v2&entry.124074799=Read"},
{"title": "Accelerated Training on Low-Power Edge Devices", "author": "Mohamed Aboelenien Ahmed and Kilian Pfeiffer and Heba Khdr and Osama Abboud and Ramin Khalili and J\u00f6rg Henkel", "abstract": "  Training on edge devices poses several challenges as these devices are\ngenerally resource-constrained, especially in terms of power. State-of-the-art\ntechniques at the device level reduce the GPU frequency to enforce power\nconstraints, leading to a significant increase in training time. To accelerate\ntraining, we propose to jointly adjust the system and application parameters\n(in our case, the GPU frequency and the batch size of the training task) while\nadhering to the power constraints on devices. We introduce a novel cross-layer\nmethodology that combines predictions of batch size efficiency and device\nprofiling to achieve the desired optimization. Our evaluation on real hardware\nshows that our method outperforms the current baselines that depend on state of\nthe art techniques, reducing the training time by $2.4\\times$ with results very\nclose to optimal. Our measurements also indicate a substantial reduction in the\noverall energy used for the training process. These gains are achieved without\nreduction in the performance of the trained model.\n", "link": "http://arxiv.org/abs/2502.18323v1", "date": "2025-02-25", "relevancy": 2.1145, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5559}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5343}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerated%20Training%20on%20Low-Power%20Edge%20Devices&body=Title%3A%20Accelerated%20Training%20on%20Low-Power%20Edge%20Devices%0AAuthor%3A%20Mohamed%20Aboelenien%20Ahmed%20and%20Kilian%20Pfeiffer%20and%20Heba%20Khdr%20and%20Osama%20Abboud%20and%20Ramin%20Khalili%20and%20J%C3%B6rg%20Henkel%0AAbstract%3A%20%20%20Training%20on%20edge%20devices%20poses%20several%20challenges%20as%20these%20devices%20are%0Agenerally%20resource-constrained%2C%20especially%20in%20terms%20of%20power.%20State-of-the-art%0Atechniques%20at%20the%20device%20level%20reduce%20the%20GPU%20frequency%20to%20enforce%20power%0Aconstraints%2C%20leading%20to%20a%20significant%20increase%20in%20training%20time.%20To%20accelerate%0Atraining%2C%20we%20propose%20to%20jointly%20adjust%20the%20system%20and%20application%20parameters%0A%28in%20our%20case%2C%20the%20GPU%20frequency%20and%20the%20batch%20size%20of%20the%20training%20task%29%20while%0Aadhering%20to%20the%20power%20constraints%20on%20devices.%20We%20introduce%20a%20novel%20cross-layer%0Amethodology%20that%20combines%20predictions%20of%20batch%20size%20efficiency%20and%20device%0Aprofiling%20to%20achieve%20the%20desired%20optimization.%20Our%20evaluation%20on%20real%20hardware%0Ashows%20that%20our%20method%20outperforms%20the%20current%20baselines%20that%20depend%20on%20state%20of%0Athe%20art%20techniques%2C%20reducing%20the%20training%20time%20by%20%242.4%5Ctimes%24%20with%20results%20very%0Aclose%20to%20optimal.%20Our%20measurements%20also%20indicate%20a%20substantial%20reduction%20in%20the%0Aoverall%20energy%20used%20for%20the%20training%20process.%20These%20gains%20are%20achieved%20without%0Areduction%20in%20the%20performance%20of%20the%20trained%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerated%2520Training%2520on%2520Low-Power%2520Edge%2520Devices%26entry.906535625%3DMohamed%2520Aboelenien%2520Ahmed%2520and%2520Kilian%2520Pfeiffer%2520and%2520Heba%2520Khdr%2520and%2520Osama%2520Abboud%2520and%2520Ramin%2520Khalili%2520and%2520J%25C3%25B6rg%2520Henkel%26entry.1292438233%3D%2520%2520Training%2520on%2520edge%2520devices%2520poses%2520several%2520challenges%2520as%2520these%2520devices%2520are%250Agenerally%2520resource-constrained%252C%2520especially%2520in%2520terms%2520of%2520power.%2520State-of-the-art%250Atechniques%2520at%2520the%2520device%2520level%2520reduce%2520the%2520GPU%2520frequency%2520to%2520enforce%2520power%250Aconstraints%252C%2520leading%2520to%2520a%2520significant%2520increase%2520in%2520training%2520time.%2520To%2520accelerate%250Atraining%252C%2520we%2520propose%2520to%2520jointly%2520adjust%2520the%2520system%2520and%2520application%2520parameters%250A%2528in%2520our%2520case%252C%2520the%2520GPU%2520frequency%2520and%2520the%2520batch%2520size%2520of%2520the%2520training%2520task%2529%2520while%250Aadhering%2520to%2520the%2520power%2520constraints%2520on%2520devices.%2520We%2520introduce%2520a%2520novel%2520cross-layer%250Amethodology%2520that%2520combines%2520predictions%2520of%2520batch%2520size%2520efficiency%2520and%2520device%250Aprofiling%2520to%2520achieve%2520the%2520desired%2520optimization.%2520Our%2520evaluation%2520on%2520real%2520hardware%250Ashows%2520that%2520our%2520method%2520outperforms%2520the%2520current%2520baselines%2520that%2520depend%2520on%2520state%2520of%250Athe%2520art%2520techniques%252C%2520reducing%2520the%2520training%2520time%2520by%2520%25242.4%255Ctimes%2524%2520with%2520results%2520very%250Aclose%2520to%2520optimal.%2520Our%2520measurements%2520also%2520indicate%2520a%2520substantial%2520reduction%2520in%2520the%250Aoverall%2520energy%2520used%2520for%2520the%2520training%2520process.%2520These%2520gains%2520are%2520achieved%2520without%250Areduction%2520in%2520the%2520performance%2520of%2520the%2520trained%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerated%20Training%20on%20Low-Power%20Edge%20Devices&entry.906535625=Mohamed%20Aboelenien%20Ahmed%20and%20Kilian%20Pfeiffer%20and%20Heba%20Khdr%20and%20Osama%20Abboud%20and%20Ramin%20Khalili%20and%20J%C3%B6rg%20Henkel&entry.1292438233=%20%20Training%20on%20edge%20devices%20poses%20several%20challenges%20as%20these%20devices%20are%0Agenerally%20resource-constrained%2C%20especially%20in%20terms%20of%20power.%20State-of-the-art%0Atechniques%20at%20the%20device%20level%20reduce%20the%20GPU%20frequency%20to%20enforce%20power%0Aconstraints%2C%20leading%20to%20a%20significant%20increase%20in%20training%20time.%20To%20accelerate%0Atraining%2C%20we%20propose%20to%20jointly%20adjust%20the%20system%20and%20application%20parameters%0A%28in%20our%20case%2C%20the%20GPU%20frequency%20and%20the%20batch%20size%20of%20the%20training%20task%29%20while%0Aadhering%20to%20the%20power%20constraints%20on%20devices.%20We%20introduce%20a%20novel%20cross-layer%0Amethodology%20that%20combines%20predictions%20of%20batch%20size%20efficiency%20and%20device%0Aprofiling%20to%20achieve%20the%20desired%20optimization.%20Our%20evaluation%20on%20real%20hardware%0Ashows%20that%20our%20method%20outperforms%20the%20current%20baselines%20that%20depend%20on%20state%20of%0Athe%20art%20techniques%2C%20reducing%20the%20training%20time%20by%20%242.4%5Ctimes%24%20with%20results%20very%0Aclose%20to%20optimal.%20Our%20measurements%20also%20indicate%20a%20substantial%20reduction%20in%20the%0Aoverall%20energy%20used%20for%20the%20training%20process.%20These%20gains%20are%20achieved%20without%0Areduction%20in%20the%20performance%20of%20the%20trained%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18323v1&entry.124074799=Read"},
{"title": "CLIPure: Purification in Latent Space via CLIP for Adversarially Robust\n  Zero-Shot Classification", "author": "Mingkun Zhang and Keping Bi and Wei Chen and Jiafeng Guo and Xueqi Cheng", "abstract": "  In this paper, we aim to build an adversarially robust zero-shot image\nclassifier. We ground our work on CLIP, a vision-language pre-trained encoder\nmodel that can perform zero-shot classification by matching an image with text\nprompts ``a photo of a <class-name>.''. Purification is the path we choose\nsince it does not require adversarial training on specific attack types and\nthus can cope with any foreseen attacks. We then formulate purification risk as\nthe KL divergence between the joint distributions of the purification process\nof denoising the adversarial samples and the attack process of adding\nperturbations to benign samples, through bidirectional Stochastic Differential\nEquations (SDEs). The final derived results inspire us to explore purification\nin the multi-modal latent space of CLIP. We propose two variants for our\nCLIPure approach: CLIPure-Diff which models the likelihood of images' latent\nvectors with the DiffusionPrior module in DaLLE-2 (modeling the generation\nprocess of CLIP's latent vectors), and CLIPure-Cos which models the likelihood\nwith the cosine similarity between the embeddings of an image and ``a photo of\na.''. As far as we know, CLIPure is the first purification method in\nmulti-modal latent space and CLIPure-Cos is the first purification method that\nis not based on generative models, which substantially improves defense\nefficiency. We conducted extensive experiments on CIFAR-10, ImageNet, and 13\ndatasets that previous CLIP-based defense methods used for evaluating zero-shot\nclassification robustness. Results show that CLIPure boosts the SOTA robustness\nby a large margin, e.g., from 71.7% to 91.1% on CIFAR10, from 59.6% to 72.6% on\nImageNet, and 108% relative improvements of average robustness on the 13\ndatasets over previous SOTA. The code is available at\nhttps://github.com/TMLResearchGroup-CAS/CLIPure.\n", "link": "http://arxiv.org/abs/2502.18176v1", "date": "2025-02-25", "relevancy": 2.1108, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5464}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5162}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIPure%3A%20Purification%20in%20Latent%20Space%20via%20CLIP%20for%20Adversarially%20Robust%0A%20%20Zero-Shot%20Classification&body=Title%3A%20CLIPure%3A%20Purification%20in%20Latent%20Space%20via%20CLIP%20for%20Adversarially%20Robust%0A%20%20Zero-Shot%20Classification%0AAuthor%3A%20Mingkun%20Zhang%20and%20Keping%20Bi%20and%20Wei%20Chen%20and%20Jiafeng%20Guo%20and%20Xueqi%20Cheng%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20aim%20to%20build%20an%20adversarially%20robust%20zero-shot%20image%0Aclassifier.%20We%20ground%20our%20work%20on%20CLIP%2C%20a%20vision-language%20pre-trained%20encoder%0Amodel%20that%20can%20perform%20zero-shot%20classification%20by%20matching%20an%20image%20with%20text%0Aprompts%20%60%60a%20photo%20of%20a%20%3Cclass-name%3E.%27%27.%20Purification%20is%20the%20path%20we%20choose%0Asince%20it%20does%20not%20require%20adversarial%20training%20on%20specific%20attack%20types%20and%0Athus%20can%20cope%20with%20any%20foreseen%20attacks.%20We%20then%20formulate%20purification%20risk%20as%0Athe%20KL%20divergence%20between%20the%20joint%20distributions%20of%20the%20purification%20process%0Aof%20denoising%20the%20adversarial%20samples%20and%20the%20attack%20process%20of%20adding%0Aperturbations%20to%20benign%20samples%2C%20through%20bidirectional%20Stochastic%20Differential%0AEquations%20%28SDEs%29.%20The%20final%20derived%20results%20inspire%20us%20to%20explore%20purification%0Ain%20the%20multi-modal%20latent%20space%20of%20CLIP.%20We%20propose%20two%20variants%20for%20our%0ACLIPure%20approach%3A%20CLIPure-Diff%20which%20models%20the%20likelihood%20of%20images%27%20latent%0Avectors%20with%20the%20DiffusionPrior%20module%20in%20DaLLE-2%20%28modeling%20the%20generation%0Aprocess%20of%20CLIP%27s%20latent%20vectors%29%2C%20and%20CLIPure-Cos%20which%20models%20the%20likelihood%0Awith%20the%20cosine%20similarity%20between%20the%20embeddings%20of%20an%20image%20and%20%60%60a%20photo%20of%0Aa.%27%27.%20As%20far%20as%20we%20know%2C%20CLIPure%20is%20the%20first%20purification%20method%20in%0Amulti-modal%20latent%20space%20and%20CLIPure-Cos%20is%20the%20first%20purification%20method%20that%0Ais%20not%20based%20on%20generative%20models%2C%20which%20substantially%20improves%20defense%0Aefficiency.%20We%20conducted%20extensive%20experiments%20on%20CIFAR-10%2C%20ImageNet%2C%20and%2013%0Adatasets%20that%20previous%20CLIP-based%20defense%20methods%20used%20for%20evaluating%20zero-shot%0Aclassification%20robustness.%20Results%20show%20that%20CLIPure%20boosts%20the%20SOTA%20robustness%0Aby%20a%20large%20margin%2C%20e.g.%2C%20from%2071.7%25%20to%2091.1%25%20on%20CIFAR10%2C%20from%2059.6%25%20to%2072.6%25%20on%0AImageNet%2C%20and%20108%25%20relative%20improvements%20of%20average%20robustness%20on%20the%2013%0Adatasets%20over%20previous%20SOTA.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/TMLResearchGroup-CAS/CLIPure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18176v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIPure%253A%2520Purification%2520in%2520Latent%2520Space%2520via%2520CLIP%2520for%2520Adversarially%2520Robust%250A%2520%2520Zero-Shot%2520Classification%26entry.906535625%3DMingkun%2520Zhang%2520and%2520Keping%2520Bi%2520and%2520Wei%2520Chen%2520and%2520Jiafeng%2520Guo%2520and%2520Xueqi%2520Cheng%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520build%2520an%2520adversarially%2520robust%2520zero-shot%2520image%250Aclassifier.%2520We%2520ground%2520our%2520work%2520on%2520CLIP%252C%2520a%2520vision-language%2520pre-trained%2520encoder%250Amodel%2520that%2520can%2520perform%2520zero-shot%2520classification%2520by%2520matching%2520an%2520image%2520with%2520text%250Aprompts%2520%2560%2560a%2520photo%2520of%2520a%2520%253Cclass-name%253E.%2527%2527.%2520Purification%2520is%2520the%2520path%2520we%2520choose%250Asince%2520it%2520does%2520not%2520require%2520adversarial%2520training%2520on%2520specific%2520attack%2520types%2520and%250Athus%2520can%2520cope%2520with%2520any%2520foreseen%2520attacks.%2520We%2520then%2520formulate%2520purification%2520risk%2520as%250Athe%2520KL%2520divergence%2520between%2520the%2520joint%2520distributions%2520of%2520the%2520purification%2520process%250Aof%2520denoising%2520the%2520adversarial%2520samples%2520and%2520the%2520attack%2520process%2520of%2520adding%250Aperturbations%2520to%2520benign%2520samples%252C%2520through%2520bidirectional%2520Stochastic%2520Differential%250AEquations%2520%2528SDEs%2529.%2520The%2520final%2520derived%2520results%2520inspire%2520us%2520to%2520explore%2520purification%250Ain%2520the%2520multi-modal%2520latent%2520space%2520of%2520CLIP.%2520We%2520propose%2520two%2520variants%2520for%2520our%250ACLIPure%2520approach%253A%2520CLIPure-Diff%2520which%2520models%2520the%2520likelihood%2520of%2520images%2527%2520latent%250Avectors%2520with%2520the%2520DiffusionPrior%2520module%2520in%2520DaLLE-2%2520%2528modeling%2520the%2520generation%250Aprocess%2520of%2520CLIP%2527s%2520latent%2520vectors%2529%252C%2520and%2520CLIPure-Cos%2520which%2520models%2520the%2520likelihood%250Awith%2520the%2520cosine%2520similarity%2520between%2520the%2520embeddings%2520of%2520an%2520image%2520and%2520%2560%2560a%2520photo%2520of%250Aa.%2527%2527.%2520As%2520far%2520as%2520we%2520know%252C%2520CLIPure%2520is%2520the%2520first%2520purification%2520method%2520in%250Amulti-modal%2520latent%2520space%2520and%2520CLIPure-Cos%2520is%2520the%2520first%2520purification%2520method%2520that%250Ais%2520not%2520based%2520on%2520generative%2520models%252C%2520which%2520substantially%2520improves%2520defense%250Aefficiency.%2520We%2520conducted%2520extensive%2520experiments%2520on%2520CIFAR-10%252C%2520ImageNet%252C%2520and%252013%250Adatasets%2520that%2520previous%2520CLIP-based%2520defense%2520methods%2520used%2520for%2520evaluating%2520zero-shot%250Aclassification%2520robustness.%2520Results%2520show%2520that%2520CLIPure%2520boosts%2520the%2520SOTA%2520robustness%250Aby%2520a%2520large%2520margin%252C%2520e.g.%252C%2520from%252071.7%2525%2520to%252091.1%2525%2520on%2520CIFAR10%252C%2520from%252059.6%2525%2520to%252072.6%2525%2520on%250AImageNet%252C%2520and%2520108%2525%2520relative%2520improvements%2520of%2520average%2520robustness%2520on%2520the%252013%250Adatasets%2520over%2520previous%2520SOTA.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/TMLResearchGroup-CAS/CLIPure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18176v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIPure%3A%20Purification%20in%20Latent%20Space%20via%20CLIP%20for%20Adversarially%20Robust%0A%20%20Zero-Shot%20Classification&entry.906535625=Mingkun%20Zhang%20and%20Keping%20Bi%20and%20Wei%20Chen%20and%20Jiafeng%20Guo%20and%20Xueqi%20Cheng&entry.1292438233=%20%20In%20this%20paper%2C%20we%20aim%20to%20build%20an%20adversarially%20robust%20zero-shot%20image%0Aclassifier.%20We%20ground%20our%20work%20on%20CLIP%2C%20a%20vision-language%20pre-trained%20encoder%0Amodel%20that%20can%20perform%20zero-shot%20classification%20by%20matching%20an%20image%20with%20text%0Aprompts%20%60%60a%20photo%20of%20a%20%3Cclass-name%3E.%27%27.%20Purification%20is%20the%20path%20we%20choose%0Asince%20it%20does%20not%20require%20adversarial%20training%20on%20specific%20attack%20types%20and%0Athus%20can%20cope%20with%20any%20foreseen%20attacks.%20We%20then%20formulate%20purification%20risk%20as%0Athe%20KL%20divergence%20between%20the%20joint%20distributions%20of%20the%20purification%20process%0Aof%20denoising%20the%20adversarial%20samples%20and%20the%20attack%20process%20of%20adding%0Aperturbations%20to%20benign%20samples%2C%20through%20bidirectional%20Stochastic%20Differential%0AEquations%20%28SDEs%29.%20The%20final%20derived%20results%20inspire%20us%20to%20explore%20purification%0Ain%20the%20multi-modal%20latent%20space%20of%20CLIP.%20We%20propose%20two%20variants%20for%20our%0ACLIPure%20approach%3A%20CLIPure-Diff%20which%20models%20the%20likelihood%20of%20images%27%20latent%0Avectors%20with%20the%20DiffusionPrior%20module%20in%20DaLLE-2%20%28modeling%20the%20generation%0Aprocess%20of%20CLIP%27s%20latent%20vectors%29%2C%20and%20CLIPure-Cos%20which%20models%20the%20likelihood%0Awith%20the%20cosine%20similarity%20between%20the%20embeddings%20of%20an%20image%20and%20%60%60a%20photo%20of%0Aa.%27%27.%20As%20far%20as%20we%20know%2C%20CLIPure%20is%20the%20first%20purification%20method%20in%0Amulti-modal%20latent%20space%20and%20CLIPure-Cos%20is%20the%20first%20purification%20method%20that%0Ais%20not%20based%20on%20generative%20models%2C%20which%20substantially%20improves%20defense%0Aefficiency.%20We%20conducted%20extensive%20experiments%20on%20CIFAR-10%2C%20ImageNet%2C%20and%2013%0Adatasets%20that%20previous%20CLIP-based%20defense%20methods%20used%20for%20evaluating%20zero-shot%0Aclassification%20robustness.%20Results%20show%20that%20CLIPure%20boosts%20the%20SOTA%20robustness%0Aby%20a%20large%20margin%2C%20e.g.%2C%20from%2071.7%25%20to%2091.1%25%20on%20CIFAR10%2C%20from%2059.6%25%20to%2072.6%25%20on%0AImageNet%2C%20and%20108%25%20relative%20improvements%20of%20average%20robustness%20on%20the%2013%0Adatasets%20over%20previous%20SOTA.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/TMLResearchGroup-CAS/CLIPure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18176v1&entry.124074799=Read"},
{"title": "EU-Nets: Enhanced, Explainable and Parsimonious U-Nets", "author": "B. Sun and P. Li\u00f2", "abstract": "  In this study, we propose MHEX+, a framework adaptable to any U-Net\narchitecture. Built upon MHEX+, we introduce novel U-Net variants, EU-Nets,\nwhich enhance explainability and uncertainty estimation, addressing the\nlimitations of traditional U-Net models while improving performance and\nstability. A key innovation is the Equivalent Convolutional Kernel, which\nunifies consecutive convolutional layers, boosting interpretability. For\nuncertainty estimation, we propose the collaboration gradient approach,\nmeasuring gradient consistency across decoder layers. Notably, EU-Nets achieve\nan average accuracy improvement of 1.389\\% and a variance reduction of 0.83\\%\nacross all networks and datasets in our experiments, requiring fewer than 0.1M\nparameters.\n", "link": "http://arxiv.org/abs/2502.18122v1", "date": "2025-02-25", "relevancy": 2.1097, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5557}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5389}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EU-Nets%3A%20Enhanced%2C%20Explainable%20and%20Parsimonious%20U-Nets&body=Title%3A%20EU-Nets%3A%20Enhanced%2C%20Explainable%20and%20Parsimonious%20U-Nets%0AAuthor%3A%20B.%20Sun%20and%20P.%20Li%C3%B2%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20propose%20MHEX%2B%2C%20a%20framework%20adaptable%20to%20any%20U-Net%0Aarchitecture.%20Built%20upon%20MHEX%2B%2C%20we%20introduce%20novel%20U-Net%20variants%2C%20EU-Nets%2C%0Awhich%20enhance%20explainability%20and%20uncertainty%20estimation%2C%20addressing%20the%0Alimitations%20of%20traditional%20U-Net%20models%20while%20improving%20performance%20and%0Astability.%20A%20key%20innovation%20is%20the%20Equivalent%20Convolutional%20Kernel%2C%20which%0Aunifies%20consecutive%20convolutional%20layers%2C%20boosting%20interpretability.%20For%0Auncertainty%20estimation%2C%20we%20propose%20the%20collaboration%20gradient%20approach%2C%0Ameasuring%20gradient%20consistency%20across%20decoder%20layers.%20Notably%2C%20EU-Nets%20achieve%0Aan%20average%20accuracy%20improvement%20of%201.389%5C%25%20and%20a%20variance%20reduction%20of%200.83%5C%25%0Aacross%20all%20networks%20and%20datasets%20in%20our%20experiments%2C%20requiring%20fewer%20than%200.1M%0Aparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEU-Nets%253A%2520Enhanced%252C%2520Explainable%2520and%2520Parsimonious%2520U-Nets%26entry.906535625%3DB.%2520Sun%2520and%2520P.%2520Li%25C3%25B2%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520propose%2520MHEX%252B%252C%2520a%2520framework%2520adaptable%2520to%2520any%2520U-Net%250Aarchitecture.%2520Built%2520upon%2520MHEX%252B%252C%2520we%2520introduce%2520novel%2520U-Net%2520variants%252C%2520EU-Nets%252C%250Awhich%2520enhance%2520explainability%2520and%2520uncertainty%2520estimation%252C%2520addressing%2520the%250Alimitations%2520of%2520traditional%2520U-Net%2520models%2520while%2520improving%2520performance%2520and%250Astability.%2520A%2520key%2520innovation%2520is%2520the%2520Equivalent%2520Convolutional%2520Kernel%252C%2520which%250Aunifies%2520consecutive%2520convolutional%2520layers%252C%2520boosting%2520interpretability.%2520For%250Auncertainty%2520estimation%252C%2520we%2520propose%2520the%2520collaboration%2520gradient%2520approach%252C%250Ameasuring%2520gradient%2520consistency%2520across%2520decoder%2520layers.%2520Notably%252C%2520EU-Nets%2520achieve%250Aan%2520average%2520accuracy%2520improvement%2520of%25201.389%255C%2525%2520and%2520a%2520variance%2520reduction%2520of%25200.83%255C%2525%250Aacross%2520all%2520networks%2520and%2520datasets%2520in%2520our%2520experiments%252C%2520requiring%2520fewer%2520than%25200.1M%250Aparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EU-Nets%3A%20Enhanced%2C%20Explainable%20and%20Parsimonious%20U-Nets&entry.906535625=B.%20Sun%20and%20P.%20Li%C3%B2&entry.1292438233=%20%20In%20this%20study%2C%20we%20propose%20MHEX%2B%2C%20a%20framework%20adaptable%20to%20any%20U-Net%0Aarchitecture.%20Built%20upon%20MHEX%2B%2C%20we%20introduce%20novel%20U-Net%20variants%2C%20EU-Nets%2C%0Awhich%20enhance%20explainability%20and%20uncertainty%20estimation%2C%20addressing%20the%0Alimitations%20of%20traditional%20U-Net%20models%20while%20improving%20performance%20and%0Astability.%20A%20key%20innovation%20is%20the%20Equivalent%20Convolutional%20Kernel%2C%20which%0Aunifies%20consecutive%20convolutional%20layers%2C%20boosting%20interpretability.%20For%0Auncertainty%20estimation%2C%20we%20propose%20the%20collaboration%20gradient%20approach%2C%0Ameasuring%20gradient%20consistency%20across%20decoder%20layers.%20Notably%2C%20EU-Nets%20achieve%0Aan%20average%20accuracy%20improvement%20of%201.389%5C%25%20and%20a%20variance%20reduction%20of%200.83%5C%25%0Aacross%20all%20networks%20and%20datasets%20in%20our%20experiments%2C%20requiring%20fewer%20than%200.1M%0Aparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18122v1&entry.124074799=Read"},
{"title": "ToMCAT: Theory-of-Mind for Cooperative Agents in Teams via Multiagent\n  Diffusion Policies", "author": "Pedro Sequeira and Vidyasagar Sadhu and Melinda Gervasio", "abstract": "  In this paper we present ToMCAT (Theory-of-Mind for Cooperative Agents in\nTeams), a new framework for generating ToM-conditioned trajectories. It\ncombines a meta-learning mechanism, that performs ToM reasoning over teammates'\nunderlying goals and future behavior, with a multiagent denoising-diffusion\nmodel, that generates plans for an agent and its teammates conditioned on both\nthe agent's goals and its teammates' characteristics, as computed via ToM. We\nimplemented an online planning system that dynamically samples new trajectories\n(replans) from the diffusion model whenever it detects a divergence between a\npreviously generated plan and the current state of the world. We conducted\nseveral experiments using ToMCAT in a simulated cooking domain. Our results\nhighlight the importance of the dynamic replanning mechanism in reducing the\nusage of resources without sacrificing team performance. We also show that\nrecent observations about the world and teammates' behavior collected by an\nagent over the course of an episode combined with ToM inferences are crucial to\ngenerate team-aware plans for dynamic adaptation to teammates, especially when\nno prior information is provided about them.\n", "link": "http://arxiv.org/abs/2502.18438v1", "date": "2025-02-25", "relevancy": 2.1005, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.545}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5211}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ToMCAT%3A%20Theory-of-Mind%20for%20Cooperative%20Agents%20in%20Teams%20via%20Multiagent%0A%20%20Diffusion%20Policies&body=Title%3A%20ToMCAT%3A%20Theory-of-Mind%20for%20Cooperative%20Agents%20in%20Teams%20via%20Multiagent%0A%20%20Diffusion%20Policies%0AAuthor%3A%20Pedro%20Sequeira%20and%20Vidyasagar%20Sadhu%20and%20Melinda%20Gervasio%0AAbstract%3A%20%20%20In%20this%20paper%20we%20present%20ToMCAT%20%28Theory-of-Mind%20for%20Cooperative%20Agents%20in%0ATeams%29%2C%20a%20new%20framework%20for%20generating%20ToM-conditioned%20trajectories.%20It%0Acombines%20a%20meta-learning%20mechanism%2C%20that%20performs%20ToM%20reasoning%20over%20teammates%27%0Aunderlying%20goals%20and%20future%20behavior%2C%20with%20a%20multiagent%20denoising-diffusion%0Amodel%2C%20that%20generates%20plans%20for%20an%20agent%20and%20its%20teammates%20conditioned%20on%20both%0Athe%20agent%27s%20goals%20and%20its%20teammates%27%20characteristics%2C%20as%20computed%20via%20ToM.%20We%0Aimplemented%20an%20online%20planning%20system%20that%20dynamically%20samples%20new%20trajectories%0A%28replans%29%20from%20the%20diffusion%20model%20whenever%20it%20detects%20a%20divergence%20between%20a%0Apreviously%20generated%20plan%20and%20the%20current%20state%20of%20the%20world.%20We%20conducted%0Aseveral%20experiments%20using%20ToMCAT%20in%20a%20simulated%20cooking%20domain.%20Our%20results%0Ahighlight%20the%20importance%20of%20the%20dynamic%20replanning%20mechanism%20in%20reducing%20the%0Ausage%20of%20resources%20without%20sacrificing%20team%20performance.%20We%20also%20show%20that%0Arecent%20observations%20about%20the%20world%20and%20teammates%27%20behavior%20collected%20by%20an%0Aagent%20over%20the%20course%20of%20an%20episode%20combined%20with%20ToM%20inferences%20are%20crucial%20to%0Agenerate%20team-aware%20plans%20for%20dynamic%20adaptation%20to%20teammates%2C%20especially%20when%0Ano%20prior%20information%20is%20provided%20about%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToMCAT%253A%2520Theory-of-Mind%2520for%2520Cooperative%2520Agents%2520in%2520Teams%2520via%2520Multiagent%250A%2520%2520Diffusion%2520Policies%26entry.906535625%3DPedro%2520Sequeira%2520and%2520Vidyasagar%2520Sadhu%2520and%2520Melinda%2520Gervasio%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520present%2520ToMCAT%2520%2528Theory-of-Mind%2520for%2520Cooperative%2520Agents%2520in%250ATeams%2529%252C%2520a%2520new%2520framework%2520for%2520generating%2520ToM-conditioned%2520trajectories.%2520It%250Acombines%2520a%2520meta-learning%2520mechanism%252C%2520that%2520performs%2520ToM%2520reasoning%2520over%2520teammates%2527%250Aunderlying%2520goals%2520and%2520future%2520behavior%252C%2520with%2520a%2520multiagent%2520denoising-diffusion%250Amodel%252C%2520that%2520generates%2520plans%2520for%2520an%2520agent%2520and%2520its%2520teammates%2520conditioned%2520on%2520both%250Athe%2520agent%2527s%2520goals%2520and%2520its%2520teammates%2527%2520characteristics%252C%2520as%2520computed%2520via%2520ToM.%2520We%250Aimplemented%2520an%2520online%2520planning%2520system%2520that%2520dynamically%2520samples%2520new%2520trajectories%250A%2528replans%2529%2520from%2520the%2520diffusion%2520model%2520whenever%2520it%2520detects%2520a%2520divergence%2520between%2520a%250Apreviously%2520generated%2520plan%2520and%2520the%2520current%2520state%2520of%2520the%2520world.%2520We%2520conducted%250Aseveral%2520experiments%2520using%2520ToMCAT%2520in%2520a%2520simulated%2520cooking%2520domain.%2520Our%2520results%250Ahighlight%2520the%2520importance%2520of%2520the%2520dynamic%2520replanning%2520mechanism%2520in%2520reducing%2520the%250Ausage%2520of%2520resources%2520without%2520sacrificing%2520team%2520performance.%2520We%2520also%2520show%2520that%250Arecent%2520observations%2520about%2520the%2520world%2520and%2520teammates%2527%2520behavior%2520collected%2520by%2520an%250Aagent%2520over%2520the%2520course%2520of%2520an%2520episode%2520combined%2520with%2520ToM%2520inferences%2520are%2520crucial%2520to%250Agenerate%2520team-aware%2520plans%2520for%2520dynamic%2520adaptation%2520to%2520teammates%252C%2520especially%2520when%250Ano%2520prior%2520information%2520is%2520provided%2520about%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToMCAT%3A%20Theory-of-Mind%20for%20Cooperative%20Agents%20in%20Teams%20via%20Multiagent%0A%20%20Diffusion%20Policies&entry.906535625=Pedro%20Sequeira%20and%20Vidyasagar%20Sadhu%20and%20Melinda%20Gervasio&entry.1292438233=%20%20In%20this%20paper%20we%20present%20ToMCAT%20%28Theory-of-Mind%20for%20Cooperative%20Agents%20in%0ATeams%29%2C%20a%20new%20framework%20for%20generating%20ToM-conditioned%20trajectories.%20It%0Acombines%20a%20meta-learning%20mechanism%2C%20that%20performs%20ToM%20reasoning%20over%20teammates%27%0Aunderlying%20goals%20and%20future%20behavior%2C%20with%20a%20multiagent%20denoising-diffusion%0Amodel%2C%20that%20generates%20plans%20for%20an%20agent%20and%20its%20teammates%20conditioned%20on%20both%0Athe%20agent%27s%20goals%20and%20its%20teammates%27%20characteristics%2C%20as%20computed%20via%20ToM.%20We%0Aimplemented%20an%20online%20planning%20system%20that%20dynamically%20samples%20new%20trajectories%0A%28replans%29%20from%20the%20diffusion%20model%20whenever%20it%20detects%20a%20divergence%20between%20a%0Apreviously%20generated%20plan%20and%20the%20current%20state%20of%20the%20world.%20We%20conducted%0Aseveral%20experiments%20using%20ToMCAT%20in%20a%20simulated%20cooking%20domain.%20Our%20results%0Ahighlight%20the%20importance%20of%20the%20dynamic%20replanning%20mechanism%20in%20reducing%20the%0Ausage%20of%20resources%20without%20sacrificing%20team%20performance.%20We%20also%20show%20that%0Arecent%20observations%20about%20the%20world%20and%20teammates%27%20behavior%20collected%20by%20an%0Aagent%20over%20the%20course%20of%20an%20episode%20combined%20with%20ToM%20inferences%20are%20crucial%20to%0Agenerate%20team-aware%20plans%20for%20dynamic%20adaptation%20to%20teammates%2C%20especially%20when%0Ano%20prior%20information%20is%20provided%20about%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18438v1&entry.124074799=Read"},
{"title": "GLEAN: Generalized Category Discovery with Diverse and Quality-Enhanced\n  LLM Feedback", "author": "Henry Peng Zou and Siffi Singh and Yi Nian and Jianfeng He and Jason Cai and Saab Mansour and Hang Su", "abstract": "  Generalized Category Discovery (GCD) is a practical and challenging\nopen-world task that aims to recognize both known and novel categories in\nunlabeled data using limited labeled data from known categories. Due to the\nlack of supervision, previous GCD methods face significant challenges, such as\ndifficulty in rectifying errors for confusing instances, and inability to\neffectively uncover and leverage the semantic meanings of discovered clusters.\nTherefore, additional annotations are usually required for real-world\napplicability. However, human annotation is extremely costly and inefficient.\nTo address these issues, we propose GLEAN, a unified framework for generalized\ncategory discovery that actively learns from diverse and quality-enhanced LLM\nfeedback. Our approach leverages three different types of LLM feedback to: (1)\nimprove instance-level contrastive features, (2) generate category\ndescriptions, and (3) align uncertain instances with LLM-selected category\ndescriptions. Extensive experiments demonstrate the superior performance of\n\\MethodName over state-of-the-art models across diverse datasets, metrics, and\nsupervision settings. Our code is available at\nhttps://github.com/amazon-science/Glean.\n", "link": "http://arxiv.org/abs/2502.18414v1", "date": "2025-02-25", "relevancy": 2.0976, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5409}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5303}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLEAN%3A%20Generalized%20Category%20Discovery%20with%20Diverse%20and%20Quality-Enhanced%0A%20%20LLM%20Feedback&body=Title%3A%20GLEAN%3A%20Generalized%20Category%20Discovery%20with%20Diverse%20and%20Quality-Enhanced%0A%20%20LLM%20Feedback%0AAuthor%3A%20Henry%20Peng%20Zou%20and%20Siffi%20Singh%20and%20Yi%20Nian%20and%20Jianfeng%20He%20and%20Jason%20Cai%20and%20Saab%20Mansour%20and%20Hang%20Su%0AAbstract%3A%20%20%20Generalized%20Category%20Discovery%20%28GCD%29%20is%20a%20practical%20and%20challenging%0Aopen-world%20task%20that%20aims%20to%20recognize%20both%20known%20and%20novel%20categories%20in%0Aunlabeled%20data%20using%20limited%20labeled%20data%20from%20known%20categories.%20Due%20to%20the%0Alack%20of%20supervision%2C%20previous%20GCD%20methods%20face%20significant%20challenges%2C%20such%20as%0Adifficulty%20in%20rectifying%20errors%20for%20confusing%20instances%2C%20and%20inability%20to%0Aeffectively%20uncover%20and%20leverage%20the%20semantic%20meanings%20of%20discovered%20clusters.%0ATherefore%2C%20additional%20annotations%20are%20usually%20required%20for%20real-world%0Aapplicability.%20However%2C%20human%20annotation%20is%20extremely%20costly%20and%20inefficient.%0ATo%20address%20these%20issues%2C%20we%20propose%20GLEAN%2C%20a%20unified%20framework%20for%20generalized%0Acategory%20discovery%20that%20actively%20learns%20from%20diverse%20and%20quality-enhanced%20LLM%0Afeedback.%20Our%20approach%20leverages%20three%20different%20types%20of%20LLM%20feedback%20to%3A%20%281%29%0Aimprove%20instance-level%20contrastive%20features%2C%20%282%29%20generate%20category%0Adescriptions%2C%20and%20%283%29%20align%20uncertain%20instances%20with%20LLM-selected%20category%0Adescriptions.%20Extensive%20experiments%20demonstrate%20the%20superior%20performance%20of%0A%5CMethodName%20over%20state-of-the-art%20models%20across%20diverse%20datasets%2C%20metrics%2C%20and%0Asupervision%20settings.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/amazon-science/Glean.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLEAN%253A%2520Generalized%2520Category%2520Discovery%2520with%2520Diverse%2520and%2520Quality-Enhanced%250A%2520%2520LLM%2520Feedback%26entry.906535625%3DHenry%2520Peng%2520Zou%2520and%2520Siffi%2520Singh%2520and%2520Yi%2520Nian%2520and%2520Jianfeng%2520He%2520and%2520Jason%2520Cai%2520and%2520Saab%2520Mansour%2520and%2520Hang%2520Su%26entry.1292438233%3D%2520%2520Generalized%2520Category%2520Discovery%2520%2528GCD%2529%2520is%2520a%2520practical%2520and%2520challenging%250Aopen-world%2520task%2520that%2520aims%2520to%2520recognize%2520both%2520known%2520and%2520novel%2520categories%2520in%250Aunlabeled%2520data%2520using%2520limited%2520labeled%2520data%2520from%2520known%2520categories.%2520Due%2520to%2520the%250Alack%2520of%2520supervision%252C%2520previous%2520GCD%2520methods%2520face%2520significant%2520challenges%252C%2520such%2520as%250Adifficulty%2520in%2520rectifying%2520errors%2520for%2520confusing%2520instances%252C%2520and%2520inability%2520to%250Aeffectively%2520uncover%2520and%2520leverage%2520the%2520semantic%2520meanings%2520of%2520discovered%2520clusters.%250ATherefore%252C%2520additional%2520annotations%2520are%2520usually%2520required%2520for%2520real-world%250Aapplicability.%2520However%252C%2520human%2520annotation%2520is%2520extremely%2520costly%2520and%2520inefficient.%250ATo%2520address%2520these%2520issues%252C%2520we%2520propose%2520GLEAN%252C%2520a%2520unified%2520framework%2520for%2520generalized%250Acategory%2520discovery%2520that%2520actively%2520learns%2520from%2520diverse%2520and%2520quality-enhanced%2520LLM%250Afeedback.%2520Our%2520approach%2520leverages%2520three%2520different%2520types%2520of%2520LLM%2520feedback%2520to%253A%2520%25281%2529%250Aimprove%2520instance-level%2520contrastive%2520features%252C%2520%25282%2529%2520generate%2520category%250Adescriptions%252C%2520and%2520%25283%2529%2520align%2520uncertain%2520instances%2520with%2520LLM-selected%2520category%250Adescriptions.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superior%2520performance%2520of%250A%255CMethodName%2520over%2520state-of-the-art%2520models%2520across%2520diverse%2520datasets%252C%2520metrics%252C%2520and%250Asupervision%2520settings.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/amazon-science/Glean.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLEAN%3A%20Generalized%20Category%20Discovery%20with%20Diverse%20and%20Quality-Enhanced%0A%20%20LLM%20Feedback&entry.906535625=Henry%20Peng%20Zou%20and%20Siffi%20Singh%20and%20Yi%20Nian%20and%20Jianfeng%20He%20and%20Jason%20Cai%20and%20Saab%20Mansour%20and%20Hang%20Su&entry.1292438233=%20%20Generalized%20Category%20Discovery%20%28GCD%29%20is%20a%20practical%20and%20challenging%0Aopen-world%20task%20that%20aims%20to%20recognize%20both%20known%20and%20novel%20categories%20in%0Aunlabeled%20data%20using%20limited%20labeled%20data%20from%20known%20categories.%20Due%20to%20the%0Alack%20of%20supervision%2C%20previous%20GCD%20methods%20face%20significant%20challenges%2C%20such%20as%0Adifficulty%20in%20rectifying%20errors%20for%20confusing%20instances%2C%20and%20inability%20to%0Aeffectively%20uncover%20and%20leverage%20the%20semantic%20meanings%20of%20discovered%20clusters.%0ATherefore%2C%20additional%20annotations%20are%20usually%20required%20for%20real-world%0Aapplicability.%20However%2C%20human%20annotation%20is%20extremely%20costly%20and%20inefficient.%0ATo%20address%20these%20issues%2C%20we%20propose%20GLEAN%2C%20a%20unified%20framework%20for%20generalized%0Acategory%20discovery%20that%20actively%20learns%20from%20diverse%20and%20quality-enhanced%20LLM%0Afeedback.%20Our%20approach%20leverages%20three%20different%20types%20of%20LLM%20feedback%20to%3A%20%281%29%0Aimprove%20instance-level%20contrastive%20features%2C%20%282%29%20generate%20category%0Adescriptions%2C%20and%20%283%29%20align%20uncertain%20instances%20with%20LLM-selected%20category%0Adescriptions.%20Extensive%20experiments%20demonstrate%20the%20superior%20performance%20of%0A%5CMethodName%20over%20state-of-the-art%20models%20across%20diverse%20datasets%2C%20metrics%2C%20and%0Asupervision%20settings.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/amazon-science/Glean.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18414v1&entry.124074799=Read"},
{"title": "Adaptive Segment-level Reward: Bridging the Gap Between Action and\n  Reward Space in Alignment", "author": "Yanshi Li and Shaopan Xiong and Gengru Chen and Xiaoyang Li and Yijia Luo and Xingyuan Bu and Yingshui Tan and Wenbo Su and Bo Zheng", "abstract": "  Reinforcement Learning (RL) has proven highly effective in aligning Large\nLanguage Models (LLMs) with human preferences. Typical RL methods optimize\nunder an overall sequence reward, which can lead to a suboptimal learning\nprocess. This reflects a key credit assignment problem: identifying which\ntokens to reinforce or suppress. To rectify these shortcomings, step-wise and\ntoken-wise methods have been proposed. However, step-wise methods rely on\npunctuation segmentation and still cannot accurately identify the key tokens.\nThe token-level approach is too fine-grained, attending to many unimportant\ntokens and thus introducing a large amount of noise. To assign more accurate\nrewards to different tokens, improving credit assignment, we propose the\n\"Adaptive Segment-wise Reward\" method. We employ semantic meaning, rather than\npunctuation, to adaptively delineate segments. Experiments demonstrate that our\nmethod can be integrated into various training methods. Compared to training\nmethods \\textit{without} our approach, our method improves the success rate on\nadversarial samples by 10\\%, and achieves a 1.3\\% improvement on evaluation\nbenchmarks such as MMLU, GSM8K, HumanEval, etc.\n", "link": "http://arxiv.org/abs/2411.00809v3", "date": "2025-02-25", "relevancy": 2.0927, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.538}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5191}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Segment-level%20Reward%3A%20Bridging%20the%20Gap%20Between%20Action%20and%0A%20%20Reward%20Space%20in%20Alignment&body=Title%3A%20Adaptive%20Segment-level%20Reward%3A%20Bridging%20the%20Gap%20Between%20Action%20and%0A%20%20Reward%20Space%20in%20Alignment%0AAuthor%3A%20Yanshi%20Li%20and%20Shaopan%20Xiong%20and%20Gengru%20Chen%20and%20Xiaoyang%20Li%20and%20Yijia%20Luo%20and%20Xingyuan%20Bu%20and%20Yingshui%20Tan%20and%20Wenbo%20Su%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20Reinforcement%20Learning%20%28RL%29%20has%20proven%20highly%20effective%20in%20aligning%20Large%0ALanguage%20Models%20%28LLMs%29%20with%20human%20preferences.%20Typical%20RL%20methods%20optimize%0Aunder%20an%20overall%20sequence%20reward%2C%20which%20can%20lead%20to%20a%20suboptimal%20learning%0Aprocess.%20This%20reflects%20a%20key%20credit%20assignment%20problem%3A%20identifying%20which%0Atokens%20to%20reinforce%20or%20suppress.%20To%20rectify%20these%20shortcomings%2C%20step-wise%20and%0Atoken-wise%20methods%20have%20been%20proposed.%20However%2C%20step-wise%20methods%20rely%20on%0Apunctuation%20segmentation%20and%20still%20cannot%20accurately%20identify%20the%20key%20tokens.%0AThe%20token-level%20approach%20is%20too%20fine-grained%2C%20attending%20to%20many%20unimportant%0Atokens%20and%20thus%20introducing%20a%20large%20amount%20of%20noise.%20To%20assign%20more%20accurate%0Arewards%20to%20different%20tokens%2C%20improving%20credit%20assignment%2C%20we%20propose%20the%0A%22Adaptive%20Segment-wise%20Reward%22%20method.%20We%20employ%20semantic%20meaning%2C%20rather%20than%0Apunctuation%2C%20to%20adaptively%20delineate%20segments.%20Experiments%20demonstrate%20that%20our%0Amethod%20can%20be%20integrated%20into%20various%20training%20methods.%20Compared%20to%20training%0Amethods%20%5Ctextit%7Bwithout%7D%20our%20approach%2C%20our%20method%20improves%20the%20success%20rate%20on%0Aadversarial%20samples%20by%2010%5C%25%2C%20and%20achieves%20a%201.3%5C%25%20improvement%20on%20evaluation%0Abenchmarks%20such%20as%20MMLU%2C%20GSM8K%2C%20HumanEval%2C%20etc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00809v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Segment-level%2520Reward%253A%2520Bridging%2520the%2520Gap%2520Between%2520Action%2520and%250A%2520%2520Reward%2520Space%2520in%2520Alignment%26entry.906535625%3DYanshi%2520Li%2520and%2520Shaopan%2520Xiong%2520and%2520Gengru%2520Chen%2520and%2520Xiaoyang%2520Li%2520and%2520Yijia%2520Luo%2520and%2520Xingyuan%2520Bu%2520and%2520Yingshui%2520Tan%2520and%2520Wenbo%2520Su%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520%2528RL%2529%2520has%2520proven%2520highly%2520effective%2520in%2520aligning%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520with%2520human%2520preferences.%2520Typical%2520RL%2520methods%2520optimize%250Aunder%2520an%2520overall%2520sequence%2520reward%252C%2520which%2520can%2520lead%2520to%2520a%2520suboptimal%2520learning%250Aprocess.%2520This%2520reflects%2520a%2520key%2520credit%2520assignment%2520problem%253A%2520identifying%2520which%250Atokens%2520to%2520reinforce%2520or%2520suppress.%2520To%2520rectify%2520these%2520shortcomings%252C%2520step-wise%2520and%250Atoken-wise%2520methods%2520have%2520been%2520proposed.%2520However%252C%2520step-wise%2520methods%2520rely%2520on%250Apunctuation%2520segmentation%2520and%2520still%2520cannot%2520accurately%2520identify%2520the%2520key%2520tokens.%250AThe%2520token-level%2520approach%2520is%2520too%2520fine-grained%252C%2520attending%2520to%2520many%2520unimportant%250Atokens%2520and%2520thus%2520introducing%2520a%2520large%2520amount%2520of%2520noise.%2520To%2520assign%2520more%2520accurate%250Arewards%2520to%2520different%2520tokens%252C%2520improving%2520credit%2520assignment%252C%2520we%2520propose%2520the%250A%2522Adaptive%2520Segment-wise%2520Reward%2522%2520method.%2520We%2520employ%2520semantic%2520meaning%252C%2520rather%2520than%250Apunctuation%252C%2520to%2520adaptively%2520delineate%2520segments.%2520Experiments%2520demonstrate%2520that%2520our%250Amethod%2520can%2520be%2520integrated%2520into%2520various%2520training%2520methods.%2520Compared%2520to%2520training%250Amethods%2520%255Ctextit%257Bwithout%257D%2520our%2520approach%252C%2520our%2520method%2520improves%2520the%2520success%2520rate%2520on%250Aadversarial%2520samples%2520by%252010%255C%2525%252C%2520and%2520achieves%2520a%25201.3%255C%2525%2520improvement%2520on%2520evaluation%250Abenchmarks%2520such%2520as%2520MMLU%252C%2520GSM8K%252C%2520HumanEval%252C%2520etc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00809v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Segment-level%20Reward%3A%20Bridging%20the%20Gap%20Between%20Action%20and%0A%20%20Reward%20Space%20in%20Alignment&entry.906535625=Yanshi%20Li%20and%20Shaopan%20Xiong%20and%20Gengru%20Chen%20and%20Xiaoyang%20Li%20and%20Yijia%20Luo%20and%20Xingyuan%20Bu%20and%20Yingshui%20Tan%20and%20Wenbo%20Su%20and%20Bo%20Zheng&entry.1292438233=%20%20Reinforcement%20Learning%20%28RL%29%20has%20proven%20highly%20effective%20in%20aligning%20Large%0ALanguage%20Models%20%28LLMs%29%20with%20human%20preferences.%20Typical%20RL%20methods%20optimize%0Aunder%20an%20overall%20sequence%20reward%2C%20which%20can%20lead%20to%20a%20suboptimal%20learning%0Aprocess.%20This%20reflects%20a%20key%20credit%20assignment%20problem%3A%20identifying%20which%0Atokens%20to%20reinforce%20or%20suppress.%20To%20rectify%20these%20shortcomings%2C%20step-wise%20and%0Atoken-wise%20methods%20have%20been%20proposed.%20However%2C%20step-wise%20methods%20rely%20on%0Apunctuation%20segmentation%20and%20still%20cannot%20accurately%20identify%20the%20key%20tokens.%0AThe%20token-level%20approach%20is%20too%20fine-grained%2C%20attending%20to%20many%20unimportant%0Atokens%20and%20thus%20introducing%20a%20large%20amount%20of%20noise.%20To%20assign%20more%20accurate%0Arewards%20to%20different%20tokens%2C%20improving%20credit%20assignment%2C%20we%20propose%20the%0A%22Adaptive%20Segment-wise%20Reward%22%20method.%20We%20employ%20semantic%20meaning%2C%20rather%20than%0Apunctuation%2C%20to%20adaptively%20delineate%20segments.%20Experiments%20demonstrate%20that%20our%0Amethod%20can%20be%20integrated%20into%20various%20training%20methods.%20Compared%20to%20training%0Amethods%20%5Ctextit%7Bwithout%7D%20our%20approach%2C%20our%20method%20improves%20the%20success%20rate%20on%0Aadversarial%20samples%20by%2010%5C%25%2C%20and%20achieves%20a%201.3%5C%25%20improvement%20on%20evaluation%0Abenchmarks%20such%20as%20MMLU%2C%20GSM8K%2C%20HumanEval%2C%20etc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00809v3&entry.124074799=Read"},
{"title": "FwNet-ECA: Facilitating Window Attention with Global Receptive Fields\n  through Fourier Filtering Operations", "author": "Shengtian Mian and Ya Wang and Nannan Gu and Yuping Wang and Xiaoqing Li", "abstract": "  Windowed attention mechanisms were introduced to mitigate the issue of\nexcessive computation inherent in global attention mechanisms. However, In this\npaper, we present FwNet-ECA, a novel method that utilizes Fourier transforms\npaired with learnable weight matrices to enhance the spectral features of\nimages. This strategy facilitates inter-window connectivity, thereby maximizing\nthe receptive field. Additionally, we incorporate the Efficient Channel\nAttention (ECA) module to improve communication between different channels.\nInstead of relying on physically shifted windows, our approach leverages\nfrequency domain enhancement to implicitly bridge information across spatial\nregions. We validate our model on the iCartoonFace dataset and conduct\ndownstream tasks on ImageNet, demonstrating that our model achieves lower\nparameter counts and computational overheads compared to shifted window\napproaches, while maintaining competitive accuracy. This work offers a more\nefficient and effective alternative for leveraging attention mechanisms in\nvisual processing tasks, alleviating the challenges associated with windowed\nattention models. Code is available at https://github.com/qingxiaoli/FwNet-ECA.\n", "link": "http://arxiv.org/abs/2502.18094v1", "date": "2025-02-25", "relevancy": 2.0894, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5361}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5179}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FwNet-ECA%3A%20Facilitating%20Window%20Attention%20with%20Global%20Receptive%20Fields%0A%20%20through%20Fourier%20Filtering%20Operations&body=Title%3A%20FwNet-ECA%3A%20Facilitating%20Window%20Attention%20with%20Global%20Receptive%20Fields%0A%20%20through%20Fourier%20Filtering%20Operations%0AAuthor%3A%20Shengtian%20Mian%20and%20Ya%20Wang%20and%20Nannan%20Gu%20and%20Yuping%20Wang%20and%20Xiaoqing%20Li%0AAbstract%3A%20%20%20Windowed%20attention%20mechanisms%20were%20introduced%20to%20mitigate%20the%20issue%20of%0Aexcessive%20computation%20inherent%20in%20global%20attention%20mechanisms.%20However%2C%20In%20this%0Apaper%2C%20we%20present%20FwNet-ECA%2C%20a%20novel%20method%20that%20utilizes%20Fourier%20transforms%0Apaired%20with%20learnable%20weight%20matrices%20to%20enhance%20the%20spectral%20features%20of%0Aimages.%20This%20strategy%20facilitates%20inter-window%20connectivity%2C%20thereby%20maximizing%0Athe%20receptive%20field.%20Additionally%2C%20we%20incorporate%20the%20Efficient%20Channel%0AAttention%20%28ECA%29%20module%20to%20improve%20communication%20between%20different%20channels.%0AInstead%20of%20relying%20on%20physically%20shifted%20windows%2C%20our%20approach%20leverages%0Afrequency%20domain%20enhancement%20to%20implicitly%20bridge%20information%20across%20spatial%0Aregions.%20We%20validate%20our%20model%20on%20the%20iCartoonFace%20dataset%20and%20conduct%0Adownstream%20tasks%20on%20ImageNet%2C%20demonstrating%20that%20our%20model%20achieves%20lower%0Aparameter%20counts%20and%20computational%20overheads%20compared%20to%20shifted%20window%0Aapproaches%2C%20while%20maintaining%20competitive%20accuracy.%20This%20work%20offers%20a%20more%0Aefficient%20and%20effective%20alternative%20for%20leveraging%20attention%20mechanisms%20in%0Avisual%20processing%20tasks%2C%20alleviating%20the%20challenges%20associated%20with%20windowed%0Aattention%20models.%20Code%20is%20available%20at%20https%3A//github.com/qingxiaoli/FwNet-ECA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFwNet-ECA%253A%2520Facilitating%2520Window%2520Attention%2520with%2520Global%2520Receptive%2520Fields%250A%2520%2520through%2520Fourier%2520Filtering%2520Operations%26entry.906535625%3DShengtian%2520Mian%2520and%2520Ya%2520Wang%2520and%2520Nannan%2520Gu%2520and%2520Yuping%2520Wang%2520and%2520Xiaoqing%2520Li%26entry.1292438233%3D%2520%2520Windowed%2520attention%2520mechanisms%2520were%2520introduced%2520to%2520mitigate%2520the%2520issue%2520of%250Aexcessive%2520computation%2520inherent%2520in%2520global%2520attention%2520mechanisms.%2520However%252C%2520In%2520this%250Apaper%252C%2520we%2520present%2520FwNet-ECA%252C%2520a%2520novel%2520method%2520that%2520utilizes%2520Fourier%2520transforms%250Apaired%2520with%2520learnable%2520weight%2520matrices%2520to%2520enhance%2520the%2520spectral%2520features%2520of%250Aimages.%2520This%2520strategy%2520facilitates%2520inter-window%2520connectivity%252C%2520thereby%2520maximizing%250Athe%2520receptive%2520field.%2520Additionally%252C%2520we%2520incorporate%2520the%2520Efficient%2520Channel%250AAttention%2520%2528ECA%2529%2520module%2520to%2520improve%2520communication%2520between%2520different%2520channels.%250AInstead%2520of%2520relying%2520on%2520physically%2520shifted%2520windows%252C%2520our%2520approach%2520leverages%250Afrequency%2520domain%2520enhancement%2520to%2520implicitly%2520bridge%2520information%2520across%2520spatial%250Aregions.%2520We%2520validate%2520our%2520model%2520on%2520the%2520iCartoonFace%2520dataset%2520and%2520conduct%250Adownstream%2520tasks%2520on%2520ImageNet%252C%2520demonstrating%2520that%2520our%2520model%2520achieves%2520lower%250Aparameter%2520counts%2520and%2520computational%2520overheads%2520compared%2520to%2520shifted%2520window%250Aapproaches%252C%2520while%2520maintaining%2520competitive%2520accuracy.%2520This%2520work%2520offers%2520a%2520more%250Aefficient%2520and%2520effective%2520alternative%2520for%2520leveraging%2520attention%2520mechanisms%2520in%250Avisual%2520processing%2520tasks%252C%2520alleviating%2520the%2520challenges%2520associated%2520with%2520windowed%250Aattention%2520models.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/qingxiaoli/FwNet-ECA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FwNet-ECA%3A%20Facilitating%20Window%20Attention%20with%20Global%20Receptive%20Fields%0A%20%20through%20Fourier%20Filtering%20Operations&entry.906535625=Shengtian%20Mian%20and%20Ya%20Wang%20and%20Nannan%20Gu%20and%20Yuping%20Wang%20and%20Xiaoqing%20Li&entry.1292438233=%20%20Windowed%20attention%20mechanisms%20were%20introduced%20to%20mitigate%20the%20issue%20of%0Aexcessive%20computation%20inherent%20in%20global%20attention%20mechanisms.%20However%2C%20In%20this%0Apaper%2C%20we%20present%20FwNet-ECA%2C%20a%20novel%20method%20that%20utilizes%20Fourier%20transforms%0Apaired%20with%20learnable%20weight%20matrices%20to%20enhance%20the%20spectral%20features%20of%0Aimages.%20This%20strategy%20facilitates%20inter-window%20connectivity%2C%20thereby%20maximizing%0Athe%20receptive%20field.%20Additionally%2C%20we%20incorporate%20the%20Efficient%20Channel%0AAttention%20%28ECA%29%20module%20to%20improve%20communication%20between%20different%20channels.%0AInstead%20of%20relying%20on%20physically%20shifted%20windows%2C%20our%20approach%20leverages%0Afrequency%20domain%20enhancement%20to%20implicitly%20bridge%20information%20across%20spatial%0Aregions.%20We%20validate%20our%20model%20on%20the%20iCartoonFace%20dataset%20and%20conduct%0Adownstream%20tasks%20on%20ImageNet%2C%20demonstrating%20that%20our%20model%20achieves%20lower%0Aparameter%20counts%20and%20computational%20overheads%20compared%20to%20shifted%20window%0Aapproaches%2C%20while%20maintaining%20competitive%20accuracy.%20This%20work%20offers%20a%20more%0Aefficient%20and%20effective%20alternative%20for%20leveraging%20attention%20mechanisms%20in%0Avisual%20processing%20tasks%2C%20alleviating%20the%20challenges%20associated%20with%20windowed%0Aattention%20models.%20Code%20is%20available%20at%20https%3A//github.com/qingxiaoli/FwNet-ECA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18094v1&entry.124074799=Read"},
{"title": "Mind the Gap: Examining the Self-Improvement Capabilities of Large\n  Language Models", "author": "Yuda Song and Hanlin Zhang and Carson Eisenach and Sham Kakade and Dean Foster and Udaya Ghai", "abstract": "  Self-improvement is a mechanism in Large Language Model (LLM) pre-training,\npost-training and test-time inference. We explore a framework where the model\nverifies its own outputs, filters or reweights data based on this verification,\nand distills the filtered data. Despite several empirical successes, a\nfundamental understanding is still lacking. In this work, we initiate a\ncomprehensive, modular and controlled study on LLM self-improvement. We provide\na mathematical formulation for self-improvement, which is largely governed by a\nquantity which we formalize as the generation-verification gap. Through\nexperiments with various model families and tasks, we discover a scaling\nphenomenon of self-improvement -- a variant of the generation-verification gap\nscales monotonically with the model pre-training flops. We also examine when\nself-improvement is possible, an iterative self-improvement procedure, and ways\nto improve its performance. Our findings not only advance understanding of LLM\nself-improvement with practical implications, but also open numerous avenues\nfor future research into its capabilities and boundaries.\n", "link": "http://arxiv.org/abs/2412.02674v2", "date": "2025-02-25", "relevancy": 2.0684, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5197}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5197}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20the%20Gap%3A%20Examining%20the%20Self-Improvement%20Capabilities%20of%20Large%0A%20%20Language%20Models&body=Title%3A%20Mind%20the%20Gap%3A%20Examining%20the%20Self-Improvement%20Capabilities%20of%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Yuda%20Song%20and%20Hanlin%20Zhang%20and%20Carson%20Eisenach%20and%20Sham%20Kakade%20and%20Dean%20Foster%20and%20Udaya%20Ghai%0AAbstract%3A%20%20%20Self-improvement%20is%20a%20mechanism%20in%20Large%20Language%20Model%20%28LLM%29%20pre-training%2C%0Apost-training%20and%20test-time%20inference.%20We%20explore%20a%20framework%20where%20the%20model%0Averifies%20its%20own%20outputs%2C%20filters%20or%20reweights%20data%20based%20on%20this%20verification%2C%0Aand%20distills%20the%20filtered%20data.%20Despite%20several%20empirical%20successes%2C%20a%0Afundamental%20understanding%20is%20still%20lacking.%20In%20this%20work%2C%20we%20initiate%20a%0Acomprehensive%2C%20modular%20and%20controlled%20study%20on%20LLM%20self-improvement.%20We%20provide%0Aa%20mathematical%20formulation%20for%20self-improvement%2C%20which%20is%20largely%20governed%20by%20a%0Aquantity%20which%20we%20formalize%20as%20the%20generation-verification%20gap.%20Through%0Aexperiments%20with%20various%20model%20families%20and%20tasks%2C%20we%20discover%20a%20scaling%0Aphenomenon%20of%20self-improvement%20--%20a%20variant%20of%20the%20generation-verification%20gap%0Ascales%20monotonically%20with%20the%20model%20pre-training%20flops.%20We%20also%20examine%20when%0Aself-improvement%20is%20possible%2C%20an%20iterative%20self-improvement%20procedure%2C%20and%20ways%0Ato%20improve%20its%20performance.%20Our%20findings%20not%20only%20advance%20understanding%20of%20LLM%0Aself-improvement%20with%20practical%20implications%2C%20but%20also%20open%20numerous%20avenues%0Afor%20future%20research%20into%20its%20capabilities%20and%20boundaries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02674v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520the%2520Gap%253A%2520Examining%2520the%2520Self-Improvement%2520Capabilities%2520of%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DYuda%2520Song%2520and%2520Hanlin%2520Zhang%2520and%2520Carson%2520Eisenach%2520and%2520Sham%2520Kakade%2520and%2520Dean%2520Foster%2520and%2520Udaya%2520Ghai%26entry.1292438233%3D%2520%2520Self-improvement%2520is%2520a%2520mechanism%2520in%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520pre-training%252C%250Apost-training%2520and%2520test-time%2520inference.%2520We%2520explore%2520a%2520framework%2520where%2520the%2520model%250Averifies%2520its%2520own%2520outputs%252C%2520filters%2520or%2520reweights%2520data%2520based%2520on%2520this%2520verification%252C%250Aand%2520distills%2520the%2520filtered%2520data.%2520Despite%2520several%2520empirical%2520successes%252C%2520a%250Afundamental%2520understanding%2520is%2520still%2520lacking.%2520In%2520this%2520work%252C%2520we%2520initiate%2520a%250Acomprehensive%252C%2520modular%2520and%2520controlled%2520study%2520on%2520LLM%2520self-improvement.%2520We%2520provide%250Aa%2520mathematical%2520formulation%2520for%2520self-improvement%252C%2520which%2520is%2520largely%2520governed%2520by%2520a%250Aquantity%2520which%2520we%2520formalize%2520as%2520the%2520generation-verification%2520gap.%2520Through%250Aexperiments%2520with%2520various%2520model%2520families%2520and%2520tasks%252C%2520we%2520discover%2520a%2520scaling%250Aphenomenon%2520of%2520self-improvement%2520--%2520a%2520variant%2520of%2520the%2520generation-verification%2520gap%250Ascales%2520monotonically%2520with%2520the%2520model%2520pre-training%2520flops.%2520We%2520also%2520examine%2520when%250Aself-improvement%2520is%2520possible%252C%2520an%2520iterative%2520self-improvement%2520procedure%252C%2520and%2520ways%250Ato%2520improve%2520its%2520performance.%2520Our%2520findings%2520not%2520only%2520advance%2520understanding%2520of%2520LLM%250Aself-improvement%2520with%2520practical%2520implications%252C%2520but%2520also%2520open%2520numerous%2520avenues%250Afor%2520future%2520research%2520into%2520its%2520capabilities%2520and%2520boundaries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02674v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20the%20Gap%3A%20Examining%20the%20Self-Improvement%20Capabilities%20of%20Large%0A%20%20Language%20Models&entry.906535625=Yuda%20Song%20and%20Hanlin%20Zhang%20and%20Carson%20Eisenach%20and%20Sham%20Kakade%20and%20Dean%20Foster%20and%20Udaya%20Ghai&entry.1292438233=%20%20Self-improvement%20is%20a%20mechanism%20in%20Large%20Language%20Model%20%28LLM%29%20pre-training%2C%0Apost-training%20and%20test-time%20inference.%20We%20explore%20a%20framework%20where%20the%20model%0Averifies%20its%20own%20outputs%2C%20filters%20or%20reweights%20data%20based%20on%20this%20verification%2C%0Aand%20distills%20the%20filtered%20data.%20Despite%20several%20empirical%20successes%2C%20a%0Afundamental%20understanding%20is%20still%20lacking.%20In%20this%20work%2C%20we%20initiate%20a%0Acomprehensive%2C%20modular%20and%20controlled%20study%20on%20LLM%20self-improvement.%20We%20provide%0Aa%20mathematical%20formulation%20for%20self-improvement%2C%20which%20is%20largely%20governed%20by%20a%0Aquantity%20which%20we%20formalize%20as%20the%20generation-verification%20gap.%20Through%0Aexperiments%20with%20various%20model%20families%20and%20tasks%2C%20we%20discover%20a%20scaling%0Aphenomenon%20of%20self-improvement%20--%20a%20variant%20of%20the%20generation-verification%20gap%0Ascales%20monotonically%20with%20the%20model%20pre-training%20flops.%20We%20also%20examine%20when%0Aself-improvement%20is%20possible%2C%20an%20iterative%20self-improvement%20procedure%2C%20and%20ways%0Ato%20improve%20its%20performance.%20Our%20findings%20not%20only%20advance%20understanding%20of%20LLM%0Aself-improvement%20with%20practical%20implications%2C%20but%20also%20open%20numerous%20avenues%0Afor%20future%20research%20into%20its%20capabilities%20and%20boundaries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02674v2&entry.124074799=Read"},
{"title": "A Theoretical Framework for Data Efficient Multi-Source Transfer\n  Learning Based on Cram\u00e9r-Rao Bound", "author": "Qingyue Zhang and Haohao Fu and Guanbo Huang and Yaoyuan Liang and Chang Chu and Tianren Peng and Yanru Wu and Qi Li and Yang Li and Shao-Lun Huang", "abstract": "  Multi-source transfer learning provides an effective solution to data\nscarcity in real-world supervised learning scenarios by leveraging multiple\nsource tasks. In this field, existing works typically use all available samples\nfrom sources in training, which constrains their training efficiency and may\nlead to suboptimal results. To address this, we propose a theoretical framework\nthat answers the question: what is the optimal quantity of source samples\nneeded from each source task to jointly train the target model? Specifically,\nwe introduce a generalization error measure that aligns with cross-entropy\nloss, and minimize it based on the Cram\\'er-Rao Bound to determine the optimal\ntransfer quantity for each source task. Additionally, we develop an\narchitecture-agnostic and data-efficient algorithm OTQMS to implement our\ntheoretical results for training deep multi-source transfer learning models.\nExperimental studies on diverse architectures and two real-world benchmark\ndatasets show that our proposed algorithm significantly outperforms\nstate-of-the-art approaches in both accuracy and data efficiency. The code and\nsupplementary materials are available in\nhttps://anonymous.4open.science/r/Materials.\n", "link": "http://arxiv.org/abs/2502.04242v2", "date": "2025-02-25", "relevancy": 2.0669, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5352}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5052}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Theoretical%20Framework%20for%20Data%20Efficient%20Multi-Source%20Transfer%0A%20%20Learning%20Based%20on%20Cram%C3%A9r-Rao%20Bound&body=Title%3A%20A%20Theoretical%20Framework%20for%20Data%20Efficient%20Multi-Source%20Transfer%0A%20%20Learning%20Based%20on%20Cram%C3%A9r-Rao%20Bound%0AAuthor%3A%20Qingyue%20Zhang%20and%20Haohao%20Fu%20and%20Guanbo%20Huang%20and%20Yaoyuan%20Liang%20and%20Chang%20Chu%20and%20Tianren%20Peng%20and%20Yanru%20Wu%20and%20Qi%20Li%20and%20Yang%20Li%20and%20Shao-Lun%20Huang%0AAbstract%3A%20%20%20Multi-source%20transfer%20learning%20provides%20an%20effective%20solution%20to%20data%0Ascarcity%20in%20real-world%20supervised%20learning%20scenarios%20by%20leveraging%20multiple%0Asource%20tasks.%20In%20this%20field%2C%20existing%20works%20typically%20use%20all%20available%20samples%0Afrom%20sources%20in%20training%2C%20which%20constrains%20their%20training%20efficiency%20and%20may%0Alead%20to%20suboptimal%20results.%20To%20address%20this%2C%20we%20propose%20a%20theoretical%20framework%0Athat%20answers%20the%20question%3A%20what%20is%20the%20optimal%20quantity%20of%20source%20samples%0Aneeded%20from%20each%20source%20task%20to%20jointly%20train%20the%20target%20model%3F%20Specifically%2C%0Awe%20introduce%20a%20generalization%20error%20measure%20that%20aligns%20with%20cross-entropy%0Aloss%2C%20and%20minimize%20it%20based%20on%20the%20Cram%5C%27er-Rao%20Bound%20to%20determine%20the%20optimal%0Atransfer%20quantity%20for%20each%20source%20task.%20Additionally%2C%20we%20develop%20an%0Aarchitecture-agnostic%20and%20data-efficient%20algorithm%20OTQMS%20to%20implement%20our%0Atheoretical%20results%20for%20training%20deep%20multi-source%20transfer%20learning%20models.%0AExperimental%20studies%20on%20diverse%20architectures%20and%20two%20real-world%20benchmark%0Adatasets%20show%20that%20our%20proposed%20algorithm%20significantly%20outperforms%0Astate-of-the-art%20approaches%20in%20both%20accuracy%20and%20data%20efficiency.%20The%20code%20and%0Asupplementary%20materials%20are%20available%20in%0Ahttps%3A//anonymous.4open.science/r/Materials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Theoretical%2520Framework%2520for%2520Data%2520Efficient%2520Multi-Source%2520Transfer%250A%2520%2520Learning%2520Based%2520on%2520Cram%25C3%25A9r-Rao%2520Bound%26entry.906535625%3DQingyue%2520Zhang%2520and%2520Haohao%2520Fu%2520and%2520Guanbo%2520Huang%2520and%2520Yaoyuan%2520Liang%2520and%2520Chang%2520Chu%2520and%2520Tianren%2520Peng%2520and%2520Yanru%2520Wu%2520and%2520Qi%2520Li%2520and%2520Yang%2520Li%2520and%2520Shao-Lun%2520Huang%26entry.1292438233%3D%2520%2520Multi-source%2520transfer%2520learning%2520provides%2520an%2520effective%2520solution%2520to%2520data%250Ascarcity%2520in%2520real-world%2520supervised%2520learning%2520scenarios%2520by%2520leveraging%2520multiple%250Asource%2520tasks.%2520In%2520this%2520field%252C%2520existing%2520works%2520typically%2520use%2520all%2520available%2520samples%250Afrom%2520sources%2520in%2520training%252C%2520which%2520constrains%2520their%2520training%2520efficiency%2520and%2520may%250Alead%2520to%2520suboptimal%2520results.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520theoretical%2520framework%250Athat%2520answers%2520the%2520question%253A%2520what%2520is%2520the%2520optimal%2520quantity%2520of%2520source%2520samples%250Aneeded%2520from%2520each%2520source%2520task%2520to%2520jointly%2520train%2520the%2520target%2520model%253F%2520Specifically%252C%250Awe%2520introduce%2520a%2520generalization%2520error%2520measure%2520that%2520aligns%2520with%2520cross-entropy%250Aloss%252C%2520and%2520minimize%2520it%2520based%2520on%2520the%2520Cram%255C%2527er-Rao%2520Bound%2520to%2520determine%2520the%2520optimal%250Atransfer%2520quantity%2520for%2520each%2520source%2520task.%2520Additionally%252C%2520we%2520develop%2520an%250Aarchitecture-agnostic%2520and%2520data-efficient%2520algorithm%2520OTQMS%2520to%2520implement%2520our%250Atheoretical%2520results%2520for%2520training%2520deep%2520multi-source%2520transfer%2520learning%2520models.%250AExperimental%2520studies%2520on%2520diverse%2520architectures%2520and%2520two%2520real-world%2520benchmark%250Adatasets%2520show%2520that%2520our%2520proposed%2520algorithm%2520significantly%2520outperforms%250Astate-of-the-art%2520approaches%2520in%2520both%2520accuracy%2520and%2520data%2520efficiency.%2520The%2520code%2520and%250Asupplementary%2520materials%2520are%2520available%2520in%250Ahttps%253A//anonymous.4open.science/r/Materials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Theoretical%20Framework%20for%20Data%20Efficient%20Multi-Source%20Transfer%0A%20%20Learning%20Based%20on%20Cram%C3%A9r-Rao%20Bound&entry.906535625=Qingyue%20Zhang%20and%20Haohao%20Fu%20and%20Guanbo%20Huang%20and%20Yaoyuan%20Liang%20and%20Chang%20Chu%20and%20Tianren%20Peng%20and%20Yanru%20Wu%20and%20Qi%20Li%20and%20Yang%20Li%20and%20Shao-Lun%20Huang&entry.1292438233=%20%20Multi-source%20transfer%20learning%20provides%20an%20effective%20solution%20to%20data%0Ascarcity%20in%20real-world%20supervised%20learning%20scenarios%20by%20leveraging%20multiple%0Asource%20tasks.%20In%20this%20field%2C%20existing%20works%20typically%20use%20all%20available%20samples%0Afrom%20sources%20in%20training%2C%20which%20constrains%20their%20training%20efficiency%20and%20may%0Alead%20to%20suboptimal%20results.%20To%20address%20this%2C%20we%20propose%20a%20theoretical%20framework%0Athat%20answers%20the%20question%3A%20what%20is%20the%20optimal%20quantity%20of%20source%20samples%0Aneeded%20from%20each%20source%20task%20to%20jointly%20train%20the%20target%20model%3F%20Specifically%2C%0Awe%20introduce%20a%20generalization%20error%20measure%20that%20aligns%20with%20cross-entropy%0Aloss%2C%20and%20minimize%20it%20based%20on%20the%20Cram%5C%27er-Rao%20Bound%20to%20determine%20the%20optimal%0Atransfer%20quantity%20for%20each%20source%20task.%20Additionally%2C%20we%20develop%20an%0Aarchitecture-agnostic%20and%20data-efficient%20algorithm%20OTQMS%20to%20implement%20our%0Atheoretical%20results%20for%20training%20deep%20multi-source%20transfer%20learning%20models.%0AExperimental%20studies%20on%20diverse%20architectures%20and%20two%20real-world%20benchmark%0Adatasets%20show%20that%20our%20proposed%20algorithm%20significantly%20outperforms%0Astate-of-the-art%20approaches%20in%20both%20accuracy%20and%20data%20efficiency.%20The%20code%20and%0Asupplementary%20materials%20are%20available%20in%0Ahttps%3A//anonymous.4open.science/r/Materials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04242v2&entry.124074799=Read"},
{"title": "TextGames: Learning to Self-Play Text-Based Puzzle Games via Language\n  Model Reasoning", "author": "Frederikus Hudi and Genta Indra Winata and Ruochen Zhang and Alham Fikri Aji", "abstract": "  Reasoning is a fundamental capability of large language models (LLMs),\nenabling them to comprehend, analyze, and solve complex problems. In this\npaper, we introduce TextGames, an innovative benchmark specifically crafted to\nassess LLMs through demanding text-based games that require advanced skills in\npattern recognition, spatial awareness, arithmetic, and logical reasoning. Our\nanalysis probes LLMs' performance in both single-turn and multi-turn reasoning,\nand their abilities in leveraging feedback to correct subsequent answers\nthrough self-reflection. Our findings reveal that, although LLMs exhibit\nproficiency in addressing most easy and medium-level problems, they face\nsignificant challenges with more difficult tasks. In contrast, humans are\ncapable of solving all tasks when given sufficient time. Moreover, we observe\nthat LLMs show improved performance in multi-turn predictions through\nself-reflection, yet they still struggle with sequencing, counting, and\nfollowing complex rules consistently. Additionally, models optimized for\nreasoning outperform pre-trained LLMs that prioritize instruction following,\nhighlighting the crucial role of reasoning skills in addressing highly complex\nproblems.\n", "link": "http://arxiv.org/abs/2502.18431v1", "date": "2025-02-25", "relevancy": 2.0638, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextGames%3A%20Learning%20to%20Self-Play%20Text-Based%20Puzzle%20Games%20via%20Language%0A%20%20Model%20Reasoning&body=Title%3A%20TextGames%3A%20Learning%20to%20Self-Play%20Text-Based%20Puzzle%20Games%20via%20Language%0A%20%20Model%20Reasoning%0AAuthor%3A%20Frederikus%20Hudi%20and%20Genta%20Indra%20Winata%20and%20Ruochen%20Zhang%20and%20Alham%20Fikri%20Aji%0AAbstract%3A%20%20%20Reasoning%20is%20a%20fundamental%20capability%20of%20large%20language%20models%20%28LLMs%29%2C%0Aenabling%20them%20to%20comprehend%2C%20analyze%2C%20and%20solve%20complex%20problems.%20In%20this%0Apaper%2C%20we%20introduce%20TextGames%2C%20an%20innovative%20benchmark%20specifically%20crafted%20to%0Aassess%20LLMs%20through%20demanding%20text-based%20games%20that%20require%20advanced%20skills%20in%0Apattern%20recognition%2C%20spatial%20awareness%2C%20arithmetic%2C%20and%20logical%20reasoning.%20Our%0Aanalysis%20probes%20LLMs%27%20performance%20in%20both%20single-turn%20and%20multi-turn%20reasoning%2C%0Aand%20their%20abilities%20in%20leveraging%20feedback%20to%20correct%20subsequent%20answers%0Athrough%20self-reflection.%20Our%20findings%20reveal%20that%2C%20although%20LLMs%20exhibit%0Aproficiency%20in%20addressing%20most%20easy%20and%20medium-level%20problems%2C%20they%20face%0Asignificant%20challenges%20with%20more%20difficult%20tasks.%20In%20contrast%2C%20humans%20are%0Acapable%20of%20solving%20all%20tasks%20when%20given%20sufficient%20time.%20Moreover%2C%20we%20observe%0Athat%20LLMs%20show%20improved%20performance%20in%20multi-turn%20predictions%20through%0Aself-reflection%2C%20yet%20they%20still%20struggle%20with%20sequencing%2C%20counting%2C%20and%0Afollowing%20complex%20rules%20consistently.%20Additionally%2C%20models%20optimized%20for%0Areasoning%20outperform%20pre-trained%20LLMs%20that%20prioritize%20instruction%20following%2C%0Ahighlighting%20the%20crucial%20role%20of%20reasoning%20skills%20in%20addressing%20highly%20complex%0Aproblems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18431v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextGames%253A%2520Learning%2520to%2520Self-Play%2520Text-Based%2520Puzzle%2520Games%2520via%2520Language%250A%2520%2520Model%2520Reasoning%26entry.906535625%3DFrederikus%2520Hudi%2520and%2520Genta%2520Indra%2520Winata%2520and%2520Ruochen%2520Zhang%2520and%2520Alham%2520Fikri%2520Aji%26entry.1292438233%3D%2520%2520Reasoning%2520is%2520a%2520fundamental%2520capability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%250Aenabling%2520them%2520to%2520comprehend%252C%2520analyze%252C%2520and%2520solve%2520complex%2520problems.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520TextGames%252C%2520an%2520innovative%2520benchmark%2520specifically%2520crafted%2520to%250Aassess%2520LLMs%2520through%2520demanding%2520text-based%2520games%2520that%2520require%2520advanced%2520skills%2520in%250Apattern%2520recognition%252C%2520spatial%2520awareness%252C%2520arithmetic%252C%2520and%2520logical%2520reasoning.%2520Our%250Aanalysis%2520probes%2520LLMs%2527%2520performance%2520in%2520both%2520single-turn%2520and%2520multi-turn%2520reasoning%252C%250Aand%2520their%2520abilities%2520in%2520leveraging%2520feedback%2520to%2520correct%2520subsequent%2520answers%250Athrough%2520self-reflection.%2520Our%2520findings%2520reveal%2520that%252C%2520although%2520LLMs%2520exhibit%250Aproficiency%2520in%2520addressing%2520most%2520easy%2520and%2520medium-level%2520problems%252C%2520they%2520face%250Asignificant%2520challenges%2520with%2520more%2520difficult%2520tasks.%2520In%2520contrast%252C%2520humans%2520are%250Acapable%2520of%2520solving%2520all%2520tasks%2520when%2520given%2520sufficient%2520time.%2520Moreover%252C%2520we%2520observe%250Athat%2520LLMs%2520show%2520improved%2520performance%2520in%2520multi-turn%2520predictions%2520through%250Aself-reflection%252C%2520yet%2520they%2520still%2520struggle%2520with%2520sequencing%252C%2520counting%252C%2520and%250Afollowing%2520complex%2520rules%2520consistently.%2520Additionally%252C%2520models%2520optimized%2520for%250Areasoning%2520outperform%2520pre-trained%2520LLMs%2520that%2520prioritize%2520instruction%2520following%252C%250Ahighlighting%2520the%2520crucial%2520role%2520of%2520reasoning%2520skills%2520in%2520addressing%2520highly%2520complex%250Aproblems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18431v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextGames%3A%20Learning%20to%20Self-Play%20Text-Based%20Puzzle%20Games%20via%20Language%0A%20%20Model%20Reasoning&entry.906535625=Frederikus%20Hudi%20and%20Genta%20Indra%20Winata%20and%20Ruochen%20Zhang%20and%20Alham%20Fikri%20Aji&entry.1292438233=%20%20Reasoning%20is%20a%20fundamental%20capability%20of%20large%20language%20models%20%28LLMs%29%2C%0Aenabling%20them%20to%20comprehend%2C%20analyze%2C%20and%20solve%20complex%20problems.%20In%20this%0Apaper%2C%20we%20introduce%20TextGames%2C%20an%20innovative%20benchmark%20specifically%20crafted%20to%0Aassess%20LLMs%20through%20demanding%20text-based%20games%20that%20require%20advanced%20skills%20in%0Apattern%20recognition%2C%20spatial%20awareness%2C%20arithmetic%2C%20and%20logical%20reasoning.%20Our%0Aanalysis%20probes%20LLMs%27%20performance%20in%20both%20single-turn%20and%20multi-turn%20reasoning%2C%0Aand%20their%20abilities%20in%20leveraging%20feedback%20to%20correct%20subsequent%20answers%0Athrough%20self-reflection.%20Our%20findings%20reveal%20that%2C%20although%20LLMs%20exhibit%0Aproficiency%20in%20addressing%20most%20easy%20and%20medium-level%20problems%2C%20they%20face%0Asignificant%20challenges%20with%20more%20difficult%20tasks.%20In%20contrast%2C%20humans%20are%0Acapable%20of%20solving%20all%20tasks%20when%20given%20sufficient%20time.%20Moreover%2C%20we%20observe%0Athat%20LLMs%20show%20improved%20performance%20in%20multi-turn%20predictions%20through%0Aself-reflection%2C%20yet%20they%20still%20struggle%20with%20sequencing%2C%20counting%2C%20and%0Afollowing%20complex%20rules%20consistently.%20Additionally%2C%20models%20optimized%20for%0Areasoning%20outperform%20pre-trained%20LLMs%20that%20prioritize%20instruction%20following%2C%0Ahighlighting%20the%20crucial%20role%20of%20reasoning%20skills%20in%20addressing%20highly%20complex%0Aproblems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18431v1&entry.124074799=Read"},
{"title": "Accelerating Unbiased LLM Evaluation via Synthetic Feedback", "author": "Zhaoyi Zhou and Yuda Song and Andrea Zanette", "abstract": "  When developing new large language models (LLMs), a key step is evaluating\ntheir final performance, often by computing the win-rate against a reference\nmodel based on external feedback. Human feedback is the gold standard,\nparticularly for capturing nuanced qualities like coherence, readability, and\nalignment with human expectations. However, human evaluations are costly --\neven for large tech companies -- and when conducted with active users, they may\nnegatively impact user experience. A promising alternative is synthetic\nfeedback, where evaluations are conducted by other large language models,\nincluding reward models. While this eliminates the need for costly human\nannotations, it introduces biases that may distort the evaluation process. In\nthis work, we propose a statistically principled framework that integrates\nhuman and synthetic feedback to reduce reliance on human annotations while\nmaintaining unbiased win-rate calculations. Our experiments demonstrate a\nreduction in human annotations by up to 12.2% with an off-the-shelf synthetic\nevaluator and up to 24.8% with a finetuned variant. Apart from being\ngeneralizable, scalable, and free of hyper-parameter tuning, our method offers\npredictable annotation savings, which can be estimated based on data-dependent\ncharacteristics.\n", "link": "http://arxiv.org/abs/2502.10563v2", "date": "2025-02-25", "relevancy": 0.9921, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5289}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4811}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Unbiased%20LLM%20Evaluation%20via%20Synthetic%20Feedback&body=Title%3A%20Accelerating%20Unbiased%20LLM%20Evaluation%20via%20Synthetic%20Feedback%0AAuthor%3A%20Zhaoyi%20Zhou%20and%20Yuda%20Song%20and%20Andrea%20Zanette%0AAbstract%3A%20%20%20When%20developing%20new%20large%20language%20models%20%28LLMs%29%2C%20a%20key%20step%20is%20evaluating%0Atheir%20final%20performance%2C%20often%20by%20computing%20the%20win-rate%20against%20a%20reference%0Amodel%20based%20on%20external%20feedback.%20Human%20feedback%20is%20the%20gold%20standard%2C%0Aparticularly%20for%20capturing%20nuanced%20qualities%20like%20coherence%2C%20readability%2C%20and%0Aalignment%20with%20human%20expectations.%20However%2C%20human%20evaluations%20are%20costly%20--%0Aeven%20for%20large%20tech%20companies%20--%20and%20when%20conducted%20with%20active%20users%2C%20they%20may%0Anegatively%20impact%20user%20experience.%20A%20promising%20alternative%20is%20synthetic%0Afeedback%2C%20where%20evaluations%20are%20conducted%20by%20other%20large%20language%20models%2C%0Aincluding%20reward%20models.%20While%20this%20eliminates%20the%20need%20for%20costly%20human%0Aannotations%2C%20it%20introduces%20biases%20that%20may%20distort%20the%20evaluation%20process.%20In%0Athis%20work%2C%20we%20propose%20a%20statistically%20principled%20framework%20that%20integrates%0Ahuman%20and%20synthetic%20feedback%20to%20reduce%20reliance%20on%20human%20annotations%20while%0Amaintaining%20unbiased%20win-rate%20calculations.%20Our%20experiments%20demonstrate%20a%0Areduction%20in%20human%20annotations%20by%20up%20to%2012.2%25%20with%20an%20off-the-shelf%20synthetic%0Aevaluator%20and%20up%20to%2024.8%25%20with%20a%20finetuned%20variant.%20Apart%20from%20being%0Ageneralizable%2C%20scalable%2C%20and%20free%20of%20hyper-parameter%20tuning%2C%20our%20method%20offers%0Apredictable%20annotation%20savings%2C%20which%20can%20be%20estimated%20based%20on%20data-dependent%0Acharacteristics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10563v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Unbiased%2520LLM%2520Evaluation%2520via%2520Synthetic%2520Feedback%26entry.906535625%3DZhaoyi%2520Zhou%2520and%2520Yuda%2520Song%2520and%2520Andrea%2520Zanette%26entry.1292438233%3D%2520%2520When%2520developing%2520new%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520a%2520key%2520step%2520is%2520evaluating%250Atheir%2520final%2520performance%252C%2520often%2520by%2520computing%2520the%2520win-rate%2520against%2520a%2520reference%250Amodel%2520based%2520on%2520external%2520feedback.%2520Human%2520feedback%2520is%2520the%2520gold%2520standard%252C%250Aparticularly%2520for%2520capturing%2520nuanced%2520qualities%2520like%2520coherence%252C%2520readability%252C%2520and%250Aalignment%2520with%2520human%2520expectations.%2520However%252C%2520human%2520evaluations%2520are%2520costly%2520--%250Aeven%2520for%2520large%2520tech%2520companies%2520--%2520and%2520when%2520conducted%2520with%2520active%2520users%252C%2520they%2520may%250Anegatively%2520impact%2520user%2520experience.%2520A%2520promising%2520alternative%2520is%2520synthetic%250Afeedback%252C%2520where%2520evaluations%2520are%2520conducted%2520by%2520other%2520large%2520language%2520models%252C%250Aincluding%2520reward%2520models.%2520While%2520this%2520eliminates%2520the%2520need%2520for%2520costly%2520human%250Aannotations%252C%2520it%2520introduces%2520biases%2520that%2520may%2520distort%2520the%2520evaluation%2520process.%2520In%250Athis%2520work%252C%2520we%2520propose%2520a%2520statistically%2520principled%2520framework%2520that%2520integrates%250Ahuman%2520and%2520synthetic%2520feedback%2520to%2520reduce%2520reliance%2520on%2520human%2520annotations%2520while%250Amaintaining%2520unbiased%2520win-rate%2520calculations.%2520Our%2520experiments%2520demonstrate%2520a%250Areduction%2520in%2520human%2520annotations%2520by%2520up%2520to%252012.2%2525%2520with%2520an%2520off-the-shelf%2520synthetic%250Aevaluator%2520and%2520up%2520to%252024.8%2525%2520with%2520a%2520finetuned%2520variant.%2520Apart%2520from%2520being%250Ageneralizable%252C%2520scalable%252C%2520and%2520free%2520of%2520hyper-parameter%2520tuning%252C%2520our%2520method%2520offers%250Apredictable%2520annotation%2520savings%252C%2520which%2520can%2520be%2520estimated%2520based%2520on%2520data-dependent%250Acharacteristics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10563v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Unbiased%20LLM%20Evaluation%20via%20Synthetic%20Feedback&entry.906535625=Zhaoyi%20Zhou%20and%20Yuda%20Song%20and%20Andrea%20Zanette&entry.1292438233=%20%20When%20developing%20new%20large%20language%20models%20%28LLMs%29%2C%20a%20key%20step%20is%20evaluating%0Atheir%20final%20performance%2C%20often%20by%20computing%20the%20win-rate%20against%20a%20reference%0Amodel%20based%20on%20external%20feedback.%20Human%20feedback%20is%20the%20gold%20standard%2C%0Aparticularly%20for%20capturing%20nuanced%20qualities%20like%20coherence%2C%20readability%2C%20and%0Aalignment%20with%20human%20expectations.%20However%2C%20human%20evaluations%20are%20costly%20--%0Aeven%20for%20large%20tech%20companies%20--%20and%20when%20conducted%20with%20active%20users%2C%20they%20may%0Anegatively%20impact%20user%20experience.%20A%20promising%20alternative%20is%20synthetic%0Afeedback%2C%20where%20evaluations%20are%20conducted%20by%20other%20large%20language%20models%2C%0Aincluding%20reward%20models.%20While%20this%20eliminates%20the%20need%20for%20costly%20human%0Aannotations%2C%20it%20introduces%20biases%20that%20may%20distort%20the%20evaluation%20process.%20In%0Athis%20work%2C%20we%20propose%20a%20statistically%20principled%20framework%20that%20integrates%0Ahuman%20and%20synthetic%20feedback%20to%20reduce%20reliance%20on%20human%20annotations%20while%0Amaintaining%20unbiased%20win-rate%20calculations.%20Our%20experiments%20demonstrate%20a%0Areduction%20in%20human%20annotations%20by%20up%20to%2012.2%25%20with%20an%20off-the-shelf%20synthetic%0Aevaluator%20and%20up%20to%2024.8%25%20with%20a%20finetuned%20variant.%20Apart%20from%20being%0Ageneralizable%2C%20scalable%2C%20and%20free%20of%20hyper-parameter%20tuning%2C%20our%20method%20offers%0Apredictable%20annotation%20savings%2C%20which%20can%20be%20estimated%20based%20on%20data-dependent%0Acharacteristics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10563v2&entry.124074799=Read"},
{"title": "Persistent Homology for Structural Characterization in Disordered\n  Systems", "author": "An Wang and Li Zou", "abstract": "  We propose a unified framework based on persistent homology (PH) to\ncharacterize both local and global structures in disordered systems. It can\nsimultaneously generate local and global descriptors using the same algorithm\nand data structure, and has shown to be highly effective and interpretable in\npredicting particle rearrangements and classifying global phases. We also\ndemonstrated that using a single variable enables a linear SVM to achieve\nnearly perfect three-phase classification. Inspired by this discovery, we\ndefine a non-parametric metric, the Separation Index (SI), which not only\nachieves this classification without sacrificing significant performance but\nalso establishes a connection between particle environments and the global\nphase structure. Our methods provide an effective framework for understanding\nand analyzing the properties of disordered materials, with broad potential\napplications in materials science and even wider studies of complex systems.\n", "link": "http://arxiv.org/abs/2411.14390v5", "date": "2025-02-25", "relevancy": 1.7426, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4385}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4373}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Persistent%20Homology%20for%20Structural%20Characterization%20in%20Disordered%0A%20%20Systems&body=Title%3A%20Persistent%20Homology%20for%20Structural%20Characterization%20in%20Disordered%0A%20%20Systems%0AAuthor%3A%20An%20Wang%20and%20Li%20Zou%0AAbstract%3A%20%20%20We%20propose%20a%20unified%20framework%20based%20on%20persistent%20homology%20%28PH%29%20to%0Acharacterize%20both%20local%20and%20global%20structures%20in%20disordered%20systems.%20It%20can%0Asimultaneously%20generate%20local%20and%20global%20descriptors%20using%20the%20same%20algorithm%0Aand%20data%20structure%2C%20and%20has%20shown%20to%20be%20highly%20effective%20and%20interpretable%20in%0Apredicting%20particle%20rearrangements%20and%20classifying%20global%20phases.%20We%20also%0Ademonstrated%20that%20using%20a%20single%20variable%20enables%20a%20linear%20SVM%20to%20achieve%0Anearly%20perfect%20three-phase%20classification.%20Inspired%20by%20this%20discovery%2C%20we%0Adefine%20a%20non-parametric%20metric%2C%20the%20Separation%20Index%20%28SI%29%2C%20which%20not%20only%0Aachieves%20this%20classification%20without%20sacrificing%20significant%20performance%20but%0Aalso%20establishes%20a%20connection%20between%20particle%20environments%20and%20the%20global%0Aphase%20structure.%20Our%20methods%20provide%20an%20effective%20framework%20for%20understanding%0Aand%20analyzing%20the%20properties%20of%20disordered%20materials%2C%20with%20broad%20potential%0Aapplications%20in%20materials%20science%20and%20even%20wider%20studies%20of%20complex%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14390v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersistent%2520Homology%2520for%2520Structural%2520Characterization%2520in%2520Disordered%250A%2520%2520Systems%26entry.906535625%3DAn%2520Wang%2520and%2520Li%2520Zou%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520unified%2520framework%2520based%2520on%2520persistent%2520homology%2520%2528PH%2529%2520to%250Acharacterize%2520both%2520local%2520and%2520global%2520structures%2520in%2520disordered%2520systems.%2520It%2520can%250Asimultaneously%2520generate%2520local%2520and%2520global%2520descriptors%2520using%2520the%2520same%2520algorithm%250Aand%2520data%2520structure%252C%2520and%2520has%2520shown%2520to%2520be%2520highly%2520effective%2520and%2520interpretable%2520in%250Apredicting%2520particle%2520rearrangements%2520and%2520classifying%2520global%2520phases.%2520We%2520also%250Ademonstrated%2520that%2520using%2520a%2520single%2520variable%2520enables%2520a%2520linear%2520SVM%2520to%2520achieve%250Anearly%2520perfect%2520three-phase%2520classification.%2520Inspired%2520by%2520this%2520discovery%252C%2520we%250Adefine%2520a%2520non-parametric%2520metric%252C%2520the%2520Separation%2520Index%2520%2528SI%2529%252C%2520which%2520not%2520only%250Aachieves%2520this%2520classification%2520without%2520sacrificing%2520significant%2520performance%2520but%250Aalso%2520establishes%2520a%2520connection%2520between%2520particle%2520environments%2520and%2520the%2520global%250Aphase%2520structure.%2520Our%2520methods%2520provide%2520an%2520effective%2520framework%2520for%2520understanding%250Aand%2520analyzing%2520the%2520properties%2520of%2520disordered%2520materials%252C%2520with%2520broad%2520potential%250Aapplications%2520in%2520materials%2520science%2520and%2520even%2520wider%2520studies%2520of%2520complex%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14390v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Persistent%20Homology%20for%20Structural%20Characterization%20in%20Disordered%0A%20%20Systems&entry.906535625=An%20Wang%20and%20Li%20Zou&entry.1292438233=%20%20We%20propose%20a%20unified%20framework%20based%20on%20persistent%20homology%20%28PH%29%20to%0Acharacterize%20both%20local%20and%20global%20structures%20in%20disordered%20systems.%20It%20can%0Asimultaneously%20generate%20local%20and%20global%20descriptors%20using%20the%20same%20algorithm%0Aand%20data%20structure%2C%20and%20has%20shown%20to%20be%20highly%20effective%20and%20interpretable%20in%0Apredicting%20particle%20rearrangements%20and%20classifying%20global%20phases.%20We%20also%0Ademonstrated%20that%20using%20a%20single%20variable%20enables%20a%20linear%20SVM%20to%20achieve%0Anearly%20perfect%20three-phase%20classification.%20Inspired%20by%20this%20discovery%2C%20we%0Adefine%20a%20non-parametric%20metric%2C%20the%20Separation%20Index%20%28SI%29%2C%20which%20not%20only%0Aachieves%20this%20classification%20without%20sacrificing%20significant%20performance%20but%0Aalso%20establishes%20a%20connection%20between%20particle%20environments%20and%20the%20global%0Aphase%20structure.%20Our%20methods%20provide%20an%20effective%20framework%20for%20understanding%0Aand%20analyzing%20the%20properties%20of%20disordered%20materials%2C%20with%20broad%20potential%0Aapplications%20in%20materials%20science%20and%20even%20wider%20studies%20of%20complex%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14390v5&entry.124074799=Read"},
{"title": "Causal AI-based Root Cause Identification: Research to Practice at Scale", "author": "Saurabh Jha and Ameet Rahane and Laura Shwartz and Marc Palaci-Olgun and Frank Bagehorn and Jesus Rios and Dan Stingaciu and Ragu Kattinakere and Debasish Banerjee", "abstract": "  Modern applications are built as large, distributed systems spanning numerous\nmodules, teams, and data centers. Despite robust engineering and recovery\nstrategies, failures and performance issues remain inevitable, risking\nsignificant disruptions and affecting end users. Rapid and accurate root cause\nidentification is therefore vital to ensure system reliability and maintain key\nservice metrics.\n  We have developed a novel causality-based Root Cause Identification (RCI)\nalgorithm that emphasizes causation over correlation. This algorithm has been\nintegrated into IBM Instana-bridging research to practice at scale-and is now\nin production use by enterprise customers. By leveraging \"causal AI,\" Instana\nstands apart from typical Application Performance Management (APM) tools,\npinpointing issues in near real-time. This paper highlights Instana's advanced\nfailure diagnosis capabilities, discussing both the theoretical underpinnings\nand practical implementations of the RCI algorithm. Real-world examples\nillustrate how our causality-based approach enhances reliability and\nperformance in today's complex system landscapes.\n", "link": "http://arxiv.org/abs/2502.18240v1", "date": "2025-02-25", "relevancy": 1.1148, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3913}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3782}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20AI-based%20Root%20Cause%20Identification%3A%20Research%20to%20Practice%20at%20Scale&body=Title%3A%20Causal%20AI-based%20Root%20Cause%20Identification%3A%20Research%20to%20Practice%20at%20Scale%0AAuthor%3A%20Saurabh%20Jha%20and%20Ameet%20Rahane%20and%20Laura%20Shwartz%20and%20Marc%20Palaci-Olgun%20and%20Frank%20Bagehorn%20and%20Jesus%20Rios%20and%20Dan%20Stingaciu%20and%20Ragu%20Kattinakere%20and%20Debasish%20Banerjee%0AAbstract%3A%20%20%20Modern%20applications%20are%20built%20as%20large%2C%20distributed%20systems%20spanning%20numerous%0Amodules%2C%20teams%2C%20and%20data%20centers.%20Despite%20robust%20engineering%20and%20recovery%0Astrategies%2C%20failures%20and%20performance%20issues%20remain%20inevitable%2C%20risking%0Asignificant%20disruptions%20and%20affecting%20end%20users.%20Rapid%20and%20accurate%20root%20cause%0Aidentification%20is%20therefore%20vital%20to%20ensure%20system%20reliability%20and%20maintain%20key%0Aservice%20metrics.%0A%20%20We%20have%20developed%20a%20novel%20causality-based%20Root%20Cause%20Identification%20%28RCI%29%0Aalgorithm%20that%20emphasizes%20causation%20over%20correlation.%20This%20algorithm%20has%20been%0Aintegrated%20into%20IBM%20Instana-bridging%20research%20to%20practice%20at%20scale-and%20is%20now%0Ain%20production%20use%20by%20enterprise%20customers.%20By%20leveraging%20%22causal%20AI%2C%22%20Instana%0Astands%20apart%20from%20typical%20Application%20Performance%20Management%20%28APM%29%20tools%2C%0Apinpointing%20issues%20in%20near%20real-time.%20This%20paper%20highlights%20Instana%27s%20advanced%0Afailure%20diagnosis%20capabilities%2C%20discussing%20both%20the%20theoretical%20underpinnings%0Aand%20practical%20implementations%20of%20the%20RCI%20algorithm.%20Real-world%20examples%0Aillustrate%20how%20our%20causality-based%20approach%20enhances%20reliability%20and%0Aperformance%20in%20today%27s%20complex%20system%20landscapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520AI-based%2520Root%2520Cause%2520Identification%253A%2520Research%2520to%2520Practice%2520at%2520Scale%26entry.906535625%3DSaurabh%2520Jha%2520and%2520Ameet%2520Rahane%2520and%2520Laura%2520Shwartz%2520and%2520Marc%2520Palaci-Olgun%2520and%2520Frank%2520Bagehorn%2520and%2520Jesus%2520Rios%2520and%2520Dan%2520Stingaciu%2520and%2520Ragu%2520Kattinakere%2520and%2520Debasish%2520Banerjee%26entry.1292438233%3D%2520%2520Modern%2520applications%2520are%2520built%2520as%2520large%252C%2520distributed%2520systems%2520spanning%2520numerous%250Amodules%252C%2520teams%252C%2520and%2520data%2520centers.%2520Despite%2520robust%2520engineering%2520and%2520recovery%250Astrategies%252C%2520failures%2520and%2520performance%2520issues%2520remain%2520inevitable%252C%2520risking%250Asignificant%2520disruptions%2520and%2520affecting%2520end%2520users.%2520Rapid%2520and%2520accurate%2520root%2520cause%250Aidentification%2520is%2520therefore%2520vital%2520to%2520ensure%2520system%2520reliability%2520and%2520maintain%2520key%250Aservice%2520metrics.%250A%2520%2520We%2520have%2520developed%2520a%2520novel%2520causality-based%2520Root%2520Cause%2520Identification%2520%2528RCI%2529%250Aalgorithm%2520that%2520emphasizes%2520causation%2520over%2520correlation.%2520This%2520algorithm%2520has%2520been%250Aintegrated%2520into%2520IBM%2520Instana-bridging%2520research%2520to%2520practice%2520at%2520scale-and%2520is%2520now%250Ain%2520production%2520use%2520by%2520enterprise%2520customers.%2520By%2520leveraging%2520%2522causal%2520AI%252C%2522%2520Instana%250Astands%2520apart%2520from%2520typical%2520Application%2520Performance%2520Management%2520%2528APM%2529%2520tools%252C%250Apinpointing%2520issues%2520in%2520near%2520real-time.%2520This%2520paper%2520highlights%2520Instana%2527s%2520advanced%250Afailure%2520diagnosis%2520capabilities%252C%2520discussing%2520both%2520the%2520theoretical%2520underpinnings%250Aand%2520practical%2520implementations%2520of%2520the%2520RCI%2520algorithm.%2520Real-world%2520examples%250Aillustrate%2520how%2520our%2520causality-based%2520approach%2520enhances%2520reliability%2520and%250Aperformance%2520in%2520today%2527s%2520complex%2520system%2520landscapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20AI-based%20Root%20Cause%20Identification%3A%20Research%20to%20Practice%20at%20Scale&entry.906535625=Saurabh%20Jha%20and%20Ameet%20Rahane%20and%20Laura%20Shwartz%20and%20Marc%20Palaci-Olgun%20and%20Frank%20Bagehorn%20and%20Jesus%20Rios%20and%20Dan%20Stingaciu%20and%20Ragu%20Kattinakere%20and%20Debasish%20Banerjee&entry.1292438233=%20%20Modern%20applications%20are%20built%20as%20large%2C%20distributed%20systems%20spanning%20numerous%0Amodules%2C%20teams%2C%20and%20data%20centers.%20Despite%20robust%20engineering%20and%20recovery%0Astrategies%2C%20failures%20and%20performance%20issues%20remain%20inevitable%2C%20risking%0Asignificant%20disruptions%20and%20affecting%20end%20users.%20Rapid%20and%20accurate%20root%20cause%0Aidentification%20is%20therefore%20vital%20to%20ensure%20system%20reliability%20and%20maintain%20key%0Aservice%20metrics.%0A%20%20We%20have%20developed%20a%20novel%20causality-based%20Root%20Cause%20Identification%20%28RCI%29%0Aalgorithm%20that%20emphasizes%20causation%20over%20correlation.%20This%20algorithm%20has%20been%0Aintegrated%20into%20IBM%20Instana-bridging%20research%20to%20practice%20at%20scale-and%20is%20now%0Ain%20production%20use%20by%20enterprise%20customers.%20By%20leveraging%20%22causal%20AI%2C%22%20Instana%0Astands%20apart%20from%20typical%20Application%20Performance%20Management%20%28APM%29%20tools%2C%0Apinpointing%20issues%20in%20near%20real-time.%20This%20paper%20highlights%20Instana%27s%20advanced%0Afailure%20diagnosis%20capabilities%2C%20discussing%20both%20the%20theoretical%20underpinnings%0Aand%20practical%20implementations%20of%20the%20RCI%20algorithm.%20Real-world%20examples%0Aillustrate%20how%20our%20causality-based%20approach%20enhances%20reliability%20and%0Aperformance%20in%20today%27s%20complex%20system%20landscapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18240v1&entry.124074799=Read"},
{"title": "Efficient Safety Retrofitting Against Jailbreaking for LLMs", "author": "Dario Garcia-Gasulla and Adrian Tormos and Anna Arias-Duart and Daniel Hinjos and Oscar Molina-Sedano and Ashwin Kumar Gururajan and Maria Eugenia Cardello", "abstract": "  Direct Preference Optimization (DPO) is an efficient alignment technique that\nsteers LLMs towards preferable outputs by training on preference data,\nbypassing the need for explicit reward models. Its simplicity enables easy\nadaptation to various domains and safety requirements. This paper examines\nDPO's effectiveness in model safety against jailbreaking attacks while\nminimizing data requirements and training costs. We introduce Egida, a dataset\nexpanded from multiple sources, which includes 27 different safety topics and\n18 different attack styles, complemented with synthetic and human labels. This\ndata is used to boost the safety of state-of-the-art LLMs\n(Llama-3.1-8B/70B-Instruct, Qwen-2.5-7B/72B-Instruct) across topics and attack\nstyles. In addition to safety evaluations, we assess their post-alignment\nperformance degradation in general purpose tasks, and their tendency to over\nrefusal. Following the proposed methodology, trained models reduce their Attack\nSuccess Rate by 10%-30%, using small training efforts (2,000 samples) with low\ncomputational cost (3\\$ for 8B models, 20\\$ for 72B models). Safety aligned\nmodels generalize to unseen topics and attack styles, with the most successful\nattack style reaching a success rate around 5%. Size and family are found to\nstrongly influence model malleability towards safety, pointing at the\nimportance of pre-training choices. To validate our findings, a large\nindependent assessment of human preference agreement with Llama-Guard-3-8B is\nconducted by the authors and the associated dataset Egida-HSafe is released.\nOverall, this study illustrates how affordable and accessible it is to enhance\nLLM safety using DPO while outlining its current limitations. All datasets and\nmodels are released to enable reproducibility and further research.\n", "link": "http://arxiv.org/abs/2502.13603v2", "date": "2025-02-25", "relevancy": 1.9035, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5158}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4722}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Safety%20Retrofitting%20Against%20Jailbreaking%20for%20LLMs&body=Title%3A%20Efficient%20Safety%20Retrofitting%20Against%20Jailbreaking%20for%20LLMs%0AAuthor%3A%20Dario%20Garcia-Gasulla%20and%20Adrian%20Tormos%20and%20Anna%20Arias-Duart%20and%20Daniel%20Hinjos%20and%20Oscar%20Molina-Sedano%20and%20Ashwin%20Kumar%20Gururajan%20and%20Maria%20Eugenia%20Cardello%0AAbstract%3A%20%20%20Direct%20Preference%20Optimization%20%28DPO%29%20is%20an%20efficient%20alignment%20technique%20that%0Asteers%20LLMs%20towards%20preferable%20outputs%20by%20training%20on%20preference%20data%2C%0Abypassing%20the%20need%20for%20explicit%20reward%20models.%20Its%20simplicity%20enables%20easy%0Aadaptation%20to%20various%20domains%20and%20safety%20requirements.%20This%20paper%20examines%0ADPO%27s%20effectiveness%20in%20model%20safety%20against%20jailbreaking%20attacks%20while%0Aminimizing%20data%20requirements%20and%20training%20costs.%20We%20introduce%20Egida%2C%20a%20dataset%0Aexpanded%20from%20multiple%20sources%2C%20which%20includes%2027%20different%20safety%20topics%20and%0A18%20different%20attack%20styles%2C%20complemented%20with%20synthetic%20and%20human%20labels.%20This%0Adata%20is%20used%20to%20boost%20the%20safety%20of%20state-of-the-art%20LLMs%0A%28Llama-3.1-8B/70B-Instruct%2C%20Qwen-2.5-7B/72B-Instruct%29%20across%20topics%20and%20attack%0Astyles.%20In%20addition%20to%20safety%20evaluations%2C%20we%20assess%20their%20post-alignment%0Aperformance%20degradation%20in%20general%20purpose%20tasks%2C%20and%20their%20tendency%20to%20over%0Arefusal.%20Following%20the%20proposed%20methodology%2C%20trained%20models%20reduce%20their%20Attack%0ASuccess%20Rate%20by%2010%25-30%25%2C%20using%20small%20training%20efforts%20%282%2C000%20samples%29%20with%20low%0Acomputational%20cost%20%283%5C%24%20for%208B%20models%2C%2020%5C%24%20for%2072B%20models%29.%20Safety%20aligned%0Amodels%20generalize%20to%20unseen%20topics%20and%20attack%20styles%2C%20with%20the%20most%20successful%0Aattack%20style%20reaching%20a%20success%20rate%20around%205%25.%20Size%20and%20family%20are%20found%20to%0Astrongly%20influence%20model%20malleability%20towards%20safety%2C%20pointing%20at%20the%0Aimportance%20of%20pre-training%20choices.%20To%20validate%20our%20findings%2C%20a%20large%0Aindependent%20assessment%20of%20human%20preference%20agreement%20with%20Llama-Guard-3-8B%20is%0Aconducted%20by%20the%20authors%20and%20the%20associated%20dataset%20Egida-HSafe%20is%20released.%0AOverall%2C%20this%20study%20illustrates%20how%20affordable%20and%20accessible%20it%20is%20to%20enhance%0ALLM%20safety%20using%20DPO%20while%20outlining%20its%20current%20limitations.%20All%20datasets%20and%0Amodels%20are%20released%20to%20enable%20reproducibility%20and%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13603v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Safety%2520Retrofitting%2520Against%2520Jailbreaking%2520for%2520LLMs%26entry.906535625%3DDario%2520Garcia-Gasulla%2520and%2520Adrian%2520Tormos%2520and%2520Anna%2520Arias-Duart%2520and%2520Daniel%2520Hinjos%2520and%2520Oscar%2520Molina-Sedano%2520and%2520Ashwin%2520Kumar%2520Gururajan%2520and%2520Maria%2520Eugenia%2520Cardello%26entry.1292438233%3D%2520%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520is%2520an%2520efficient%2520alignment%2520technique%2520that%250Asteers%2520LLMs%2520towards%2520preferable%2520outputs%2520by%2520training%2520on%2520preference%2520data%252C%250Abypassing%2520the%2520need%2520for%2520explicit%2520reward%2520models.%2520Its%2520simplicity%2520enables%2520easy%250Aadaptation%2520to%2520various%2520domains%2520and%2520safety%2520requirements.%2520This%2520paper%2520examines%250ADPO%2527s%2520effectiveness%2520in%2520model%2520safety%2520against%2520jailbreaking%2520attacks%2520while%250Aminimizing%2520data%2520requirements%2520and%2520training%2520costs.%2520We%2520introduce%2520Egida%252C%2520a%2520dataset%250Aexpanded%2520from%2520multiple%2520sources%252C%2520which%2520includes%252027%2520different%2520safety%2520topics%2520and%250A18%2520different%2520attack%2520styles%252C%2520complemented%2520with%2520synthetic%2520and%2520human%2520labels.%2520This%250Adata%2520is%2520used%2520to%2520boost%2520the%2520safety%2520of%2520state-of-the-art%2520LLMs%250A%2528Llama-3.1-8B/70B-Instruct%252C%2520Qwen-2.5-7B/72B-Instruct%2529%2520across%2520topics%2520and%2520attack%250Astyles.%2520In%2520addition%2520to%2520safety%2520evaluations%252C%2520we%2520assess%2520their%2520post-alignment%250Aperformance%2520degradation%2520in%2520general%2520purpose%2520tasks%252C%2520and%2520their%2520tendency%2520to%2520over%250Arefusal.%2520Following%2520the%2520proposed%2520methodology%252C%2520trained%2520models%2520reduce%2520their%2520Attack%250ASuccess%2520Rate%2520by%252010%2525-30%2525%252C%2520using%2520small%2520training%2520efforts%2520%25282%252C000%2520samples%2529%2520with%2520low%250Acomputational%2520cost%2520%25283%255C%2524%2520for%25208B%2520models%252C%252020%255C%2524%2520for%252072B%2520models%2529.%2520Safety%2520aligned%250Amodels%2520generalize%2520to%2520unseen%2520topics%2520and%2520attack%2520styles%252C%2520with%2520the%2520most%2520successful%250Aattack%2520style%2520reaching%2520a%2520success%2520rate%2520around%25205%2525.%2520Size%2520and%2520family%2520are%2520found%2520to%250Astrongly%2520influence%2520model%2520malleability%2520towards%2520safety%252C%2520pointing%2520at%2520the%250Aimportance%2520of%2520pre-training%2520choices.%2520To%2520validate%2520our%2520findings%252C%2520a%2520large%250Aindependent%2520assessment%2520of%2520human%2520preference%2520agreement%2520with%2520Llama-Guard-3-8B%2520is%250Aconducted%2520by%2520the%2520authors%2520and%2520the%2520associated%2520dataset%2520Egida-HSafe%2520is%2520released.%250AOverall%252C%2520this%2520study%2520illustrates%2520how%2520affordable%2520and%2520accessible%2520it%2520is%2520to%2520enhance%250ALLM%2520safety%2520using%2520DPO%2520while%2520outlining%2520its%2520current%2520limitations.%2520All%2520datasets%2520and%250Amodels%2520are%2520released%2520to%2520enable%2520reproducibility%2520and%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13603v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Safety%20Retrofitting%20Against%20Jailbreaking%20for%20LLMs&entry.906535625=Dario%20Garcia-Gasulla%20and%20Adrian%20Tormos%20and%20Anna%20Arias-Duart%20and%20Daniel%20Hinjos%20and%20Oscar%20Molina-Sedano%20and%20Ashwin%20Kumar%20Gururajan%20and%20Maria%20Eugenia%20Cardello&entry.1292438233=%20%20Direct%20Preference%20Optimization%20%28DPO%29%20is%20an%20efficient%20alignment%20technique%20that%0Asteers%20LLMs%20towards%20preferable%20outputs%20by%20training%20on%20preference%20data%2C%0Abypassing%20the%20need%20for%20explicit%20reward%20models.%20Its%20simplicity%20enables%20easy%0Aadaptation%20to%20various%20domains%20and%20safety%20requirements.%20This%20paper%20examines%0ADPO%27s%20effectiveness%20in%20model%20safety%20against%20jailbreaking%20attacks%20while%0Aminimizing%20data%20requirements%20and%20training%20costs.%20We%20introduce%20Egida%2C%20a%20dataset%0Aexpanded%20from%20multiple%20sources%2C%20which%20includes%2027%20different%20safety%20topics%20and%0A18%20different%20attack%20styles%2C%20complemented%20with%20synthetic%20and%20human%20labels.%20This%0Adata%20is%20used%20to%20boost%20the%20safety%20of%20state-of-the-art%20LLMs%0A%28Llama-3.1-8B/70B-Instruct%2C%20Qwen-2.5-7B/72B-Instruct%29%20across%20topics%20and%20attack%0Astyles.%20In%20addition%20to%20safety%20evaluations%2C%20we%20assess%20their%20post-alignment%0Aperformance%20degradation%20in%20general%20purpose%20tasks%2C%20and%20their%20tendency%20to%20over%0Arefusal.%20Following%20the%20proposed%20methodology%2C%20trained%20models%20reduce%20their%20Attack%0ASuccess%20Rate%20by%2010%25-30%25%2C%20using%20small%20training%20efforts%20%282%2C000%20samples%29%20with%20low%0Acomputational%20cost%20%283%5C%24%20for%208B%20models%2C%2020%5C%24%20for%2072B%20models%29.%20Safety%20aligned%0Amodels%20generalize%20to%20unseen%20topics%20and%20attack%20styles%2C%20with%20the%20most%20successful%0Aattack%20style%20reaching%20a%20success%20rate%20around%205%25.%20Size%20and%20family%20are%20found%20to%0Astrongly%20influence%20model%20malleability%20towards%20safety%2C%20pointing%20at%20the%0Aimportance%20of%20pre-training%20choices.%20To%20validate%20our%20findings%2C%20a%20large%0Aindependent%20assessment%20of%20human%20preference%20agreement%20with%20Llama-Guard-3-8B%20is%0Aconducted%20by%20the%20authors%20and%20the%20associated%20dataset%20Egida-HSafe%20is%20released.%0AOverall%2C%20this%20study%20illustrates%20how%20affordable%20and%20accessible%20it%20is%20to%20enhance%0ALLM%20safety%20using%20DPO%20while%20outlining%20its%20current%20limitations.%20All%20datasets%20and%0Amodels%20are%20released%20to%20enable%20reproducibility%20and%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13603v2&entry.124074799=Read"},
{"title": "Unveiling and Causalizing CoT: A Causal Pespective", "author": "Jiarun Fu and Lizhong Ding and Hao Li and Pengqi Li and Qiuning Wei and Xu Chen", "abstract": "  Although Chain-of-Thought (CoT) has achieved remarkable success in enhancing\nthe reasoning ability of large language models (LLMs), the mechanism of CoT\nremains a ``black box''. Even if the correct answers can frequently be\nobtained, existing CoTs struggle to make the reasoning understandable to human.\nIn this paper, we unveil and causalize CoT from a causal perspective to ensure\nboth correctness and understandability of all reasoning steps (to the best of\nour knowledge, the first such). We model causality of CoT via structural causal\nmodels (SCM) to unveil the reasoning mechanism of CoT. To measure the causality\nof CoT, we define the CoT Average Causal Effect (CACE) to test the causal\nrelations between steps. For those steps without causality (wrong or\nunintelligible steps), we design a role-playing causal query algorithm to\ncausalize these steps, resulting a causalized CoT with all steps correct and\nunderstandable. Experimental results on both open-source and closed-source LLMs\ndemonstrate that the causal errors commonly in steps are effectively corrected\nand the reasoning ability of LLMs is significantly improved.\n", "link": "http://arxiv.org/abs/2502.18239v1", "date": "2025-02-25", "relevancy": 1.8173, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4565}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20and%20Causalizing%20CoT%3A%20A%20Causal%20Pespective&body=Title%3A%20Unveiling%20and%20Causalizing%20CoT%3A%20A%20Causal%20Pespective%0AAuthor%3A%20Jiarun%20Fu%20and%20Lizhong%20Ding%20and%20Hao%20Li%20and%20Pengqi%20Li%20and%20Qiuning%20Wei%20and%20Xu%20Chen%0AAbstract%3A%20%20%20Although%20Chain-of-Thought%20%28CoT%29%20has%20achieved%20remarkable%20success%20in%20enhancing%0Athe%20reasoning%20ability%20of%20large%20language%20models%20%28LLMs%29%2C%20the%20mechanism%20of%20CoT%0Aremains%20a%20%60%60black%20box%27%27.%20Even%20if%20the%20correct%20answers%20can%20frequently%20be%0Aobtained%2C%20existing%20CoTs%20struggle%20to%20make%20the%20reasoning%20understandable%20to%20human.%0AIn%20this%20paper%2C%20we%20unveil%20and%20causalize%20CoT%20from%20a%20causal%20perspective%20to%20ensure%0Aboth%20correctness%20and%20understandability%20of%20all%20reasoning%20steps%20%28to%20the%20best%20of%0Aour%20knowledge%2C%20the%20first%20such%29.%20We%20model%20causality%20of%20CoT%20via%20structural%20causal%0Amodels%20%28SCM%29%20to%20unveil%20the%20reasoning%20mechanism%20of%20CoT.%20To%20measure%20the%20causality%0Aof%20CoT%2C%20we%20define%20the%20CoT%20Average%20Causal%20Effect%20%28CACE%29%20to%20test%20the%20causal%0Arelations%20between%20steps.%20For%20those%20steps%20without%20causality%20%28wrong%20or%0Aunintelligible%20steps%29%2C%20we%20design%20a%20role-playing%20causal%20query%20algorithm%20to%0Acausalize%20these%20steps%2C%20resulting%20a%20causalized%20CoT%20with%20all%20steps%20correct%20and%0Aunderstandable.%20Experimental%20results%20on%20both%20open-source%20and%20closed-source%20LLMs%0Ademonstrate%20that%20the%20causal%20errors%20commonly%20in%20steps%20are%20effectively%20corrected%0Aand%20the%20reasoning%20ability%20of%20LLMs%20is%20significantly%20improved.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520and%2520Causalizing%2520CoT%253A%2520A%2520Causal%2520Pespective%26entry.906535625%3DJiarun%2520Fu%2520and%2520Lizhong%2520Ding%2520and%2520Hao%2520Li%2520and%2520Pengqi%2520Li%2520and%2520Qiuning%2520Wei%2520and%2520Xu%2520Chen%26entry.1292438233%3D%2520%2520Although%2520Chain-of-Thought%2520%2528CoT%2529%2520has%2520achieved%2520remarkable%2520success%2520in%2520enhancing%250Athe%2520reasoning%2520ability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520the%2520mechanism%2520of%2520CoT%250Aremains%2520a%2520%2560%2560black%2520box%2527%2527.%2520Even%2520if%2520the%2520correct%2520answers%2520can%2520frequently%2520be%250Aobtained%252C%2520existing%2520CoTs%2520struggle%2520to%2520make%2520the%2520reasoning%2520understandable%2520to%2520human.%250AIn%2520this%2520paper%252C%2520we%2520unveil%2520and%2520causalize%2520CoT%2520from%2520a%2520causal%2520perspective%2520to%2520ensure%250Aboth%2520correctness%2520and%2520understandability%2520of%2520all%2520reasoning%2520steps%2520%2528to%2520the%2520best%2520of%250Aour%2520knowledge%252C%2520the%2520first%2520such%2529.%2520We%2520model%2520causality%2520of%2520CoT%2520via%2520structural%2520causal%250Amodels%2520%2528SCM%2529%2520to%2520unveil%2520the%2520reasoning%2520mechanism%2520of%2520CoT.%2520To%2520measure%2520the%2520causality%250Aof%2520CoT%252C%2520we%2520define%2520the%2520CoT%2520Average%2520Causal%2520Effect%2520%2528CACE%2529%2520to%2520test%2520the%2520causal%250Arelations%2520between%2520steps.%2520For%2520those%2520steps%2520without%2520causality%2520%2528wrong%2520or%250Aunintelligible%2520steps%2529%252C%2520we%2520design%2520a%2520role-playing%2520causal%2520query%2520algorithm%2520to%250Acausalize%2520these%2520steps%252C%2520resulting%2520a%2520causalized%2520CoT%2520with%2520all%2520steps%2520correct%2520and%250Aunderstandable.%2520Experimental%2520results%2520on%2520both%2520open-source%2520and%2520closed-source%2520LLMs%250Ademonstrate%2520that%2520the%2520causal%2520errors%2520commonly%2520in%2520steps%2520are%2520effectively%2520corrected%250Aand%2520the%2520reasoning%2520ability%2520of%2520LLMs%2520is%2520significantly%2520improved.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20and%20Causalizing%20CoT%3A%20A%20Causal%20Pespective&entry.906535625=Jiarun%20Fu%20and%20Lizhong%20Ding%20and%20Hao%20Li%20and%20Pengqi%20Li%20and%20Qiuning%20Wei%20and%20Xu%20Chen&entry.1292438233=%20%20Although%20Chain-of-Thought%20%28CoT%29%20has%20achieved%20remarkable%20success%20in%20enhancing%0Athe%20reasoning%20ability%20of%20large%20language%20models%20%28LLMs%29%2C%20the%20mechanism%20of%20CoT%0Aremains%20a%20%60%60black%20box%27%27.%20Even%20if%20the%20correct%20answers%20can%20frequently%20be%0Aobtained%2C%20existing%20CoTs%20struggle%20to%20make%20the%20reasoning%20understandable%20to%20human.%0AIn%20this%20paper%2C%20we%20unveil%20and%20causalize%20CoT%20from%20a%20causal%20perspective%20to%20ensure%0Aboth%20correctness%20and%20understandability%20of%20all%20reasoning%20steps%20%28to%20the%20best%20of%0Aour%20knowledge%2C%20the%20first%20such%29.%20We%20model%20causality%20of%20CoT%20via%20structural%20causal%0Amodels%20%28SCM%29%20to%20unveil%20the%20reasoning%20mechanism%20of%20CoT.%20To%20measure%20the%20causality%0Aof%20CoT%2C%20we%20define%20the%20CoT%20Average%20Causal%20Effect%20%28CACE%29%20to%20test%20the%20causal%0Arelations%20between%20steps.%20For%20those%20steps%20without%20causality%20%28wrong%20or%0Aunintelligible%20steps%29%2C%20we%20design%20a%20role-playing%20causal%20query%20algorithm%20to%0Acausalize%20these%20steps%2C%20resulting%20a%20causalized%20CoT%20with%20all%20steps%20correct%20and%0Aunderstandable.%20Experimental%20results%20on%20both%20open-source%20and%20closed-source%20LLMs%0Ademonstrate%20that%20the%20causal%20errors%20commonly%20in%20steps%20are%20effectively%20corrected%0Aand%20the%20reasoning%20ability%20of%20LLMs%20is%20significantly%20improved.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18239v1&entry.124074799=Read"},
{"title": "Retrieval Dexterity: Efficient Object Retrieval in Clutters with\n  Dexterous Hand", "author": "Fengshuo Bai and Yu Li and Jie Chu and Tawei Chou and Runchuan Zhu and Ying Wen and Yaodong Yang and Yuanpei Chen", "abstract": "  Retrieving objects buried beneath multiple objects is not only challenging\nbut also time-consuming. Performing manipulation in such environments presents\nsignificant difficulty due to complex contact relationships. Existing methods\ntypically address this task by sequentially grasping and removing each\noccluding object, resulting in lengthy execution times and requiring\nimpractical grasping capabilities for every occluding object. In this paper, we\npresent a dexterous arm-hand system for efficient object retrieval in\nmulti-object stacked environments. Our approach leverages large-scale parallel\nreinforcement learning within diverse and carefully designed cluttered\nenvironments to train policies. These policies demonstrate emergent\nmanipulation skills (e.g., pushing, stirring, and poking) that efficiently\nclear occluding objects to expose sufficient surface area of the target object.\nWe conduct extensive evaluations across a set of over 10 household objects in\ndiverse clutter configurations, demonstrating superior retrieval performance\nand efficiency for both trained and unseen objects. Furthermore, we\nsuccessfully transfer the learned policies to a real-world dexterous\nmulti-fingered robot system, validating their practical applicability in\nreal-world scenarios. Videos can be found on our project website\nhttps://ChangWinde.github.io/RetrDex.\n", "link": "http://arxiv.org/abs/2502.18423v1", "date": "2025-02-25", "relevancy": 1.7742, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6154}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6114}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrieval%20Dexterity%3A%20Efficient%20Object%20Retrieval%20in%20Clutters%20with%0A%20%20Dexterous%20Hand&body=Title%3A%20Retrieval%20Dexterity%3A%20Efficient%20Object%20Retrieval%20in%20Clutters%20with%0A%20%20Dexterous%20Hand%0AAuthor%3A%20Fengshuo%20Bai%20and%20Yu%20Li%20and%20Jie%20Chu%20and%20Tawei%20Chou%20and%20Runchuan%20Zhu%20and%20Ying%20Wen%20and%20Yaodong%20Yang%20and%20Yuanpei%20Chen%0AAbstract%3A%20%20%20Retrieving%20objects%20buried%20beneath%20multiple%20objects%20is%20not%20only%20challenging%0Abut%20also%20time-consuming.%20Performing%20manipulation%20in%20such%20environments%20presents%0Asignificant%20difficulty%20due%20to%20complex%20contact%20relationships.%20Existing%20methods%0Atypically%20address%20this%20task%20by%20sequentially%20grasping%20and%20removing%20each%0Aoccluding%20object%2C%20resulting%20in%20lengthy%20execution%20times%20and%20requiring%0Aimpractical%20grasping%20capabilities%20for%20every%20occluding%20object.%20In%20this%20paper%2C%20we%0Apresent%20a%20dexterous%20arm-hand%20system%20for%20efficient%20object%20retrieval%20in%0Amulti-object%20stacked%20environments.%20Our%20approach%20leverages%20large-scale%20parallel%0Areinforcement%20learning%20within%20diverse%20and%20carefully%20designed%20cluttered%0Aenvironments%20to%20train%20policies.%20These%20policies%20demonstrate%20emergent%0Amanipulation%20skills%20%28e.g.%2C%20pushing%2C%20stirring%2C%20and%20poking%29%20that%20efficiently%0Aclear%20occluding%20objects%20to%20expose%20sufficient%20surface%20area%20of%20the%20target%20object.%0AWe%20conduct%20extensive%20evaluations%20across%20a%20set%20of%20over%2010%20household%20objects%20in%0Adiverse%20clutter%20configurations%2C%20demonstrating%20superior%20retrieval%20performance%0Aand%20efficiency%20for%20both%20trained%20and%20unseen%20objects.%20Furthermore%2C%20we%0Asuccessfully%20transfer%20the%20learned%20policies%20to%20a%20real-world%20dexterous%0Amulti-fingered%20robot%20system%2C%20validating%20their%20practical%20applicability%20in%0Areal-world%20scenarios.%20Videos%20can%20be%20found%20on%20our%20project%20website%0Ahttps%3A//ChangWinde.github.io/RetrDex.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrieval%2520Dexterity%253A%2520Efficient%2520Object%2520Retrieval%2520in%2520Clutters%2520with%250A%2520%2520Dexterous%2520Hand%26entry.906535625%3DFengshuo%2520Bai%2520and%2520Yu%2520Li%2520and%2520Jie%2520Chu%2520and%2520Tawei%2520Chou%2520and%2520Runchuan%2520Zhu%2520and%2520Ying%2520Wen%2520and%2520Yaodong%2520Yang%2520and%2520Yuanpei%2520Chen%26entry.1292438233%3D%2520%2520Retrieving%2520objects%2520buried%2520beneath%2520multiple%2520objects%2520is%2520not%2520only%2520challenging%250Abut%2520also%2520time-consuming.%2520Performing%2520manipulation%2520in%2520such%2520environments%2520presents%250Asignificant%2520difficulty%2520due%2520to%2520complex%2520contact%2520relationships.%2520Existing%2520methods%250Atypically%2520address%2520this%2520task%2520by%2520sequentially%2520grasping%2520and%2520removing%2520each%250Aoccluding%2520object%252C%2520resulting%2520in%2520lengthy%2520execution%2520times%2520and%2520requiring%250Aimpractical%2520grasping%2520capabilities%2520for%2520every%2520occluding%2520object.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520a%2520dexterous%2520arm-hand%2520system%2520for%2520efficient%2520object%2520retrieval%2520in%250Amulti-object%2520stacked%2520environments.%2520Our%2520approach%2520leverages%2520large-scale%2520parallel%250Areinforcement%2520learning%2520within%2520diverse%2520and%2520carefully%2520designed%2520cluttered%250Aenvironments%2520to%2520train%2520policies.%2520These%2520policies%2520demonstrate%2520emergent%250Amanipulation%2520skills%2520%2528e.g.%252C%2520pushing%252C%2520stirring%252C%2520and%2520poking%2529%2520that%2520efficiently%250Aclear%2520occluding%2520objects%2520to%2520expose%2520sufficient%2520surface%2520area%2520of%2520the%2520target%2520object.%250AWe%2520conduct%2520extensive%2520evaluations%2520across%2520a%2520set%2520of%2520over%252010%2520household%2520objects%2520in%250Adiverse%2520clutter%2520configurations%252C%2520demonstrating%2520superior%2520retrieval%2520performance%250Aand%2520efficiency%2520for%2520both%2520trained%2520and%2520unseen%2520objects.%2520Furthermore%252C%2520we%250Asuccessfully%2520transfer%2520the%2520learned%2520policies%2520to%2520a%2520real-world%2520dexterous%250Amulti-fingered%2520robot%2520system%252C%2520validating%2520their%2520practical%2520applicability%2520in%250Areal-world%2520scenarios.%2520Videos%2520can%2520be%2520found%2520on%2520our%2520project%2520website%250Ahttps%253A//ChangWinde.github.io/RetrDex.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval%20Dexterity%3A%20Efficient%20Object%20Retrieval%20in%20Clutters%20with%0A%20%20Dexterous%20Hand&entry.906535625=Fengshuo%20Bai%20and%20Yu%20Li%20and%20Jie%20Chu%20and%20Tawei%20Chou%20and%20Runchuan%20Zhu%20and%20Ying%20Wen%20and%20Yaodong%20Yang%20and%20Yuanpei%20Chen&entry.1292438233=%20%20Retrieving%20objects%20buried%20beneath%20multiple%20objects%20is%20not%20only%20challenging%0Abut%20also%20time-consuming.%20Performing%20manipulation%20in%20such%20environments%20presents%0Asignificant%20difficulty%20due%20to%20complex%20contact%20relationships.%20Existing%20methods%0Atypically%20address%20this%20task%20by%20sequentially%20grasping%20and%20removing%20each%0Aoccluding%20object%2C%20resulting%20in%20lengthy%20execution%20times%20and%20requiring%0Aimpractical%20grasping%20capabilities%20for%20every%20occluding%20object.%20In%20this%20paper%2C%20we%0Apresent%20a%20dexterous%20arm-hand%20system%20for%20efficient%20object%20retrieval%20in%0Amulti-object%20stacked%20environments.%20Our%20approach%20leverages%20large-scale%20parallel%0Areinforcement%20learning%20within%20diverse%20and%20carefully%20designed%20cluttered%0Aenvironments%20to%20train%20policies.%20These%20policies%20demonstrate%20emergent%0Amanipulation%20skills%20%28e.g.%2C%20pushing%2C%20stirring%2C%20and%20poking%29%20that%20efficiently%0Aclear%20occluding%20objects%20to%20expose%20sufficient%20surface%20area%20of%20the%20target%20object.%0AWe%20conduct%20extensive%20evaluations%20across%20a%20set%20of%20over%2010%20household%20objects%20in%0Adiverse%20clutter%20configurations%2C%20demonstrating%20superior%20retrieval%20performance%0Aand%20efficiency%20for%20both%20trained%20and%20unseen%20objects.%20Furthermore%2C%20we%0Asuccessfully%20transfer%20the%20learned%20policies%20to%20a%20real-world%20dexterous%0Amulti-fingered%20robot%20system%2C%20validating%20their%20practical%20applicability%20in%0Areal-world%20scenarios.%20Videos%20can%20be%20found%20on%20our%20project%20website%0Ahttps%3A//ChangWinde.github.io/RetrDex.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18423v1&entry.124074799=Read"},
{"title": "Predicting Bad Goods Risk Scores with ARIMA Time Series: A Novel Risk\n  Assessment Approach", "author": "Bishwajit Prasad Gond", "abstract": "  The increasing complexity of supply chains and the rising costs associated\nwith defective or substandard goods (bad goods) highlight the urgent need for\nadvanced predictive methodologies to mitigate risks and enhance operational\nefficiency. This research presents a novel framework that integrates Time\nSeries ARIMA (AutoRegressive Integrated Moving Average) models with a\nproprietary formula specifically designed to calculate bad goods after time\nseries forecasting. By leveraging historical data patterns, including sales,\nreturns, and capacity, the model forecasts potential quality failures, enabling\nproactive decision-making. ARIMA is employed to capture temporal trends in time\nseries data, while the newly developed formula quantifies the likelihood and\nimpact of defects with greater precision. Experimental results, validated on a\ndataset spanning 2022-2024 for Organic Beer-G 1 Liter, demonstrate that the\nproposed method outperforms traditional statistical models, such as Exponential\nSmoothing and Holt-Winters, in both prediction accuracy and risk evaluation.\nThis study advances the field of predictive analytics by bridging time series\nforecasting, ARIMA, and risk management in supply chain quality control,\noffering a scalable and practical solution for minimizing losses due to bad\ngoods.\n", "link": "http://arxiv.org/abs/2502.16520v2", "date": "2025-02-25", "relevancy": 0.8215, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4431}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4023}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Bad%20Goods%20Risk%20Scores%20with%20ARIMA%20Time%20Series%3A%20A%20Novel%20Risk%0A%20%20Assessment%20Approach&body=Title%3A%20Predicting%20Bad%20Goods%20Risk%20Scores%20with%20ARIMA%20Time%20Series%3A%20A%20Novel%20Risk%0A%20%20Assessment%20Approach%0AAuthor%3A%20Bishwajit%20Prasad%20Gond%0AAbstract%3A%20%20%20The%20increasing%20complexity%20of%20supply%20chains%20and%20the%20rising%20costs%20associated%0Awith%20defective%20or%20substandard%20goods%20%28bad%20goods%29%20highlight%20the%20urgent%20need%20for%0Aadvanced%20predictive%20methodologies%20to%20mitigate%20risks%20and%20enhance%20operational%0Aefficiency.%20This%20research%20presents%20a%20novel%20framework%20that%20integrates%20Time%0ASeries%20ARIMA%20%28AutoRegressive%20Integrated%20Moving%20Average%29%20models%20with%20a%0Aproprietary%20formula%20specifically%20designed%20to%20calculate%20bad%20goods%20after%20time%0Aseries%20forecasting.%20By%20leveraging%20historical%20data%20patterns%2C%20including%20sales%2C%0Areturns%2C%20and%20capacity%2C%20the%20model%20forecasts%20potential%20quality%20failures%2C%20enabling%0Aproactive%20decision-making.%20ARIMA%20is%20employed%20to%20capture%20temporal%20trends%20in%20time%0Aseries%20data%2C%20while%20the%20newly%20developed%20formula%20quantifies%20the%20likelihood%20and%0Aimpact%20of%20defects%20with%20greater%20precision.%20Experimental%20results%2C%20validated%20on%20a%0Adataset%20spanning%202022-2024%20for%20Organic%20Beer-G%201%20Liter%2C%20demonstrate%20that%20the%0Aproposed%20method%20outperforms%20traditional%20statistical%20models%2C%20such%20as%20Exponential%0ASmoothing%20and%20Holt-Winters%2C%20in%20both%20prediction%20accuracy%20and%20risk%20evaluation.%0AThis%20study%20advances%20the%20field%20of%20predictive%20analytics%20by%20bridging%20time%20series%0Aforecasting%2C%20ARIMA%2C%20and%20risk%20management%20in%20supply%20chain%20quality%20control%2C%0Aoffering%20a%20scalable%20and%20practical%20solution%20for%20minimizing%20losses%20due%20to%20bad%0Agoods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.16520v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Bad%2520Goods%2520Risk%2520Scores%2520with%2520ARIMA%2520Time%2520Series%253A%2520A%2520Novel%2520Risk%250A%2520%2520Assessment%2520Approach%26entry.906535625%3DBishwajit%2520Prasad%2520Gond%26entry.1292438233%3D%2520%2520The%2520increasing%2520complexity%2520of%2520supply%2520chains%2520and%2520the%2520rising%2520costs%2520associated%250Awith%2520defective%2520or%2520substandard%2520goods%2520%2528bad%2520goods%2529%2520highlight%2520the%2520urgent%2520need%2520for%250Aadvanced%2520predictive%2520methodologies%2520to%2520mitigate%2520risks%2520and%2520enhance%2520operational%250Aefficiency.%2520This%2520research%2520presents%2520a%2520novel%2520framework%2520that%2520integrates%2520Time%250ASeries%2520ARIMA%2520%2528AutoRegressive%2520Integrated%2520Moving%2520Average%2529%2520models%2520with%2520a%250Aproprietary%2520formula%2520specifically%2520designed%2520to%2520calculate%2520bad%2520goods%2520after%2520time%250Aseries%2520forecasting.%2520By%2520leveraging%2520historical%2520data%2520patterns%252C%2520including%2520sales%252C%250Areturns%252C%2520and%2520capacity%252C%2520the%2520model%2520forecasts%2520potential%2520quality%2520failures%252C%2520enabling%250Aproactive%2520decision-making.%2520ARIMA%2520is%2520employed%2520to%2520capture%2520temporal%2520trends%2520in%2520time%250Aseries%2520data%252C%2520while%2520the%2520newly%2520developed%2520formula%2520quantifies%2520the%2520likelihood%2520and%250Aimpact%2520of%2520defects%2520with%2520greater%2520precision.%2520Experimental%2520results%252C%2520validated%2520on%2520a%250Adataset%2520spanning%25202022-2024%2520for%2520Organic%2520Beer-G%25201%2520Liter%252C%2520demonstrate%2520that%2520the%250Aproposed%2520method%2520outperforms%2520traditional%2520statistical%2520models%252C%2520such%2520as%2520Exponential%250ASmoothing%2520and%2520Holt-Winters%252C%2520in%2520both%2520prediction%2520accuracy%2520and%2520risk%2520evaluation.%250AThis%2520study%2520advances%2520the%2520field%2520of%2520predictive%2520analytics%2520by%2520bridging%2520time%2520series%250Aforecasting%252C%2520ARIMA%252C%2520and%2520risk%2520management%2520in%2520supply%2520chain%2520quality%2520control%252C%250Aoffering%2520a%2520scalable%2520and%2520practical%2520solution%2520for%2520minimizing%2520losses%2520due%2520to%2520bad%250Agoods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.16520v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Bad%20Goods%20Risk%20Scores%20with%20ARIMA%20Time%20Series%3A%20A%20Novel%20Risk%0A%20%20Assessment%20Approach&entry.906535625=Bishwajit%20Prasad%20Gond&entry.1292438233=%20%20The%20increasing%20complexity%20of%20supply%20chains%20and%20the%20rising%20costs%20associated%0Awith%20defective%20or%20substandard%20goods%20%28bad%20goods%29%20highlight%20the%20urgent%20need%20for%0Aadvanced%20predictive%20methodologies%20to%20mitigate%20risks%20and%20enhance%20operational%0Aefficiency.%20This%20research%20presents%20a%20novel%20framework%20that%20integrates%20Time%0ASeries%20ARIMA%20%28AutoRegressive%20Integrated%20Moving%20Average%29%20models%20with%20a%0Aproprietary%20formula%20specifically%20designed%20to%20calculate%20bad%20goods%20after%20time%0Aseries%20forecasting.%20By%20leveraging%20historical%20data%20patterns%2C%20including%20sales%2C%0Areturns%2C%20and%20capacity%2C%20the%20model%20forecasts%20potential%20quality%20failures%2C%20enabling%0Aproactive%20decision-making.%20ARIMA%20is%20employed%20to%20capture%20temporal%20trends%20in%20time%0Aseries%20data%2C%20while%20the%20newly%20developed%20formula%20quantifies%20the%20likelihood%20and%0Aimpact%20of%20defects%20with%20greater%20precision.%20Experimental%20results%2C%20validated%20on%20a%0Adataset%20spanning%202022-2024%20for%20Organic%20Beer-G%201%20Liter%2C%20demonstrate%20that%20the%0Aproposed%20method%20outperforms%20traditional%20statistical%20models%2C%20such%20as%20Exponential%0ASmoothing%20and%20Holt-Winters%2C%20in%20both%20prediction%20accuracy%20and%20risk%20evaluation.%0AThis%20study%20advances%20the%20field%20of%20predictive%20analytics%20by%20bridging%20time%20series%0Aforecasting%2C%20ARIMA%2C%20and%20risk%20management%20in%20supply%20chain%20quality%20control%2C%0Aoffering%20a%20scalable%20and%20practical%20solution%20for%20minimizing%20losses%20due%20to%20bad%0Agoods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.16520v2&entry.124074799=Read"},
{"title": "AgentRM: Enhancing Agent Generalization with Reward Modeling", "author": "Yu Xia and Jingru Fan and Weize Chen and Siyu Yan and Xin Cong and Zhong Zhang and Yaxi Lu and Yankai Lin and Zhiyuan Liu and Maosong Sun", "abstract": "  Existing LLM-based agents have achieved strong performance on held-in tasks,\nbut their generalizability to unseen tasks remains poor. Hence, some recent\nwork focus on fine-tuning the policy model with more diverse tasks to improve\nthe generalizability. In this work, we find that finetuning a reward model to\nguide the policy model is more robust than directly finetuning the policy\nmodel. Based on this finding, we propose AgentRM, a generalizable reward model,\nto guide the policy model for effective test-time search. We comprehensively\ninvestigate three approaches to construct the reward model, including explicit\nreward modeling, implicit reward modeling and LLM-as-a-judge. We then use\nAgentRM to guide the answer generation with Best-of-N sampling and step-level\nbeam search. On four types of nine agent tasks, AgentRM enhances the base\npolicy model by $8.8$ points on average, surpassing the top general agent by\n$4.0$. Moreover, it demonstrates weak-to-strong generalization, yielding\ngreater improvement of $12.6$ on LLaMA-3-70B policy model. As for the\nspecializability, AgentRM can also boost a finetuned policy model and\noutperform the top specialized agent by $11.4$ on three held-in tasks. Further\nanalysis verifies its effectiveness in test-time scaling. Codes will be\nreleased to facilitate the research in this area.\n", "link": "http://arxiv.org/abs/2502.18407v1", "date": "2025-02-25", "relevancy": 1.9464, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4952}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.484}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentRM%3A%20Enhancing%20Agent%20Generalization%20with%20Reward%20Modeling&body=Title%3A%20AgentRM%3A%20Enhancing%20Agent%20Generalization%20with%20Reward%20Modeling%0AAuthor%3A%20Yu%20Xia%20and%20Jingru%20Fan%20and%20Weize%20Chen%20and%20Siyu%20Yan%20and%20Xin%20Cong%20and%20Zhong%20Zhang%20and%20Yaxi%20Lu%20and%20Yankai%20Lin%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20Existing%20LLM-based%20agents%20have%20achieved%20strong%20performance%20on%20held-in%20tasks%2C%0Abut%20their%20generalizability%20to%20unseen%20tasks%20remains%20poor.%20Hence%2C%20some%20recent%0Awork%20focus%20on%20fine-tuning%20the%20policy%20model%20with%20more%20diverse%20tasks%20to%20improve%0Athe%20generalizability.%20In%20this%20work%2C%20we%20find%20that%20finetuning%20a%20reward%20model%20to%0Aguide%20the%20policy%20model%20is%20more%20robust%20than%20directly%20finetuning%20the%20policy%0Amodel.%20Based%20on%20this%20finding%2C%20we%20propose%20AgentRM%2C%20a%20generalizable%20reward%20model%2C%0Ato%20guide%20the%20policy%20model%20for%20effective%20test-time%20search.%20We%20comprehensively%0Ainvestigate%20three%20approaches%20to%20construct%20the%20reward%20model%2C%20including%20explicit%0Areward%20modeling%2C%20implicit%20reward%20modeling%20and%20LLM-as-a-judge.%20We%20then%20use%0AAgentRM%20to%20guide%20the%20answer%20generation%20with%20Best-of-N%20sampling%20and%20step-level%0Abeam%20search.%20On%20four%20types%20of%20nine%20agent%20tasks%2C%20AgentRM%20enhances%20the%20base%0Apolicy%20model%20by%20%248.8%24%20points%20on%20average%2C%20surpassing%20the%20top%20general%20agent%20by%0A%244.0%24.%20Moreover%2C%20it%20demonstrates%20weak-to-strong%20generalization%2C%20yielding%0Agreater%20improvement%20of%20%2412.6%24%20on%20LLaMA-3-70B%20policy%20model.%20As%20for%20the%0Aspecializability%2C%20AgentRM%20can%20also%20boost%20a%20finetuned%20policy%20model%20and%0Aoutperform%20the%20top%20specialized%20agent%20by%20%2411.4%24%20on%20three%20held-in%20tasks.%20Further%0Aanalysis%20verifies%20its%20effectiveness%20in%20test-time%20scaling.%20Codes%20will%20be%0Areleased%20to%20facilitate%20the%20research%20in%20this%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentRM%253A%2520Enhancing%2520Agent%2520Generalization%2520with%2520Reward%2520Modeling%26entry.906535625%3DYu%2520Xia%2520and%2520Jingru%2520Fan%2520and%2520Weize%2520Chen%2520and%2520Siyu%2520Yan%2520and%2520Xin%2520Cong%2520and%2520Zhong%2520Zhang%2520and%2520Yaxi%2520Lu%2520and%2520Yankai%2520Lin%2520and%2520Zhiyuan%2520Liu%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520Existing%2520LLM-based%2520agents%2520have%2520achieved%2520strong%2520performance%2520on%2520held-in%2520tasks%252C%250Abut%2520their%2520generalizability%2520to%2520unseen%2520tasks%2520remains%2520poor.%2520Hence%252C%2520some%2520recent%250Awork%2520focus%2520on%2520fine-tuning%2520the%2520policy%2520model%2520with%2520more%2520diverse%2520tasks%2520to%2520improve%250Athe%2520generalizability.%2520In%2520this%2520work%252C%2520we%2520find%2520that%2520finetuning%2520a%2520reward%2520model%2520to%250Aguide%2520the%2520policy%2520model%2520is%2520more%2520robust%2520than%2520directly%2520finetuning%2520the%2520policy%250Amodel.%2520Based%2520on%2520this%2520finding%252C%2520we%2520propose%2520AgentRM%252C%2520a%2520generalizable%2520reward%2520model%252C%250Ato%2520guide%2520the%2520policy%2520model%2520for%2520effective%2520test-time%2520search.%2520We%2520comprehensively%250Ainvestigate%2520three%2520approaches%2520to%2520construct%2520the%2520reward%2520model%252C%2520including%2520explicit%250Areward%2520modeling%252C%2520implicit%2520reward%2520modeling%2520and%2520LLM-as-a-judge.%2520We%2520then%2520use%250AAgentRM%2520to%2520guide%2520the%2520answer%2520generation%2520with%2520Best-of-N%2520sampling%2520and%2520step-level%250Abeam%2520search.%2520On%2520four%2520types%2520of%2520nine%2520agent%2520tasks%252C%2520AgentRM%2520enhances%2520the%2520base%250Apolicy%2520model%2520by%2520%25248.8%2524%2520points%2520on%2520average%252C%2520surpassing%2520the%2520top%2520general%2520agent%2520by%250A%25244.0%2524.%2520Moreover%252C%2520it%2520demonstrates%2520weak-to-strong%2520generalization%252C%2520yielding%250Agreater%2520improvement%2520of%2520%252412.6%2524%2520on%2520LLaMA-3-70B%2520policy%2520model.%2520As%2520for%2520the%250Aspecializability%252C%2520AgentRM%2520can%2520also%2520boost%2520a%2520finetuned%2520policy%2520model%2520and%250Aoutperform%2520the%2520top%2520specialized%2520agent%2520by%2520%252411.4%2524%2520on%2520three%2520held-in%2520tasks.%2520Further%250Aanalysis%2520verifies%2520its%2520effectiveness%2520in%2520test-time%2520scaling.%2520Codes%2520will%2520be%250Areleased%2520to%2520facilitate%2520the%2520research%2520in%2520this%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentRM%3A%20Enhancing%20Agent%20Generalization%20with%20Reward%20Modeling&entry.906535625=Yu%20Xia%20and%20Jingru%20Fan%20and%20Weize%20Chen%20and%20Siyu%20Yan%20and%20Xin%20Cong%20and%20Zhong%20Zhang%20and%20Yaxi%20Lu%20and%20Yankai%20Lin%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun&entry.1292438233=%20%20Existing%20LLM-based%20agents%20have%20achieved%20strong%20performance%20on%20held-in%20tasks%2C%0Abut%20their%20generalizability%20to%20unseen%20tasks%20remains%20poor.%20Hence%2C%20some%20recent%0Awork%20focus%20on%20fine-tuning%20the%20policy%20model%20with%20more%20diverse%20tasks%20to%20improve%0Athe%20generalizability.%20In%20this%20work%2C%20we%20find%20that%20finetuning%20a%20reward%20model%20to%0Aguide%20the%20policy%20model%20is%20more%20robust%20than%20directly%20finetuning%20the%20policy%0Amodel.%20Based%20on%20this%20finding%2C%20we%20propose%20AgentRM%2C%20a%20generalizable%20reward%20model%2C%0Ato%20guide%20the%20policy%20model%20for%20effective%20test-time%20search.%20We%20comprehensively%0Ainvestigate%20three%20approaches%20to%20construct%20the%20reward%20model%2C%20including%20explicit%0Areward%20modeling%2C%20implicit%20reward%20modeling%20and%20LLM-as-a-judge.%20We%20then%20use%0AAgentRM%20to%20guide%20the%20answer%20generation%20with%20Best-of-N%20sampling%20and%20step-level%0Abeam%20search.%20On%20four%20types%20of%20nine%20agent%20tasks%2C%20AgentRM%20enhances%20the%20base%0Apolicy%20model%20by%20%248.8%24%20points%20on%20average%2C%20surpassing%20the%20top%20general%20agent%20by%0A%244.0%24.%20Moreover%2C%20it%20demonstrates%20weak-to-strong%20generalization%2C%20yielding%0Agreater%20improvement%20of%20%2412.6%24%20on%20LLaMA-3-70B%20policy%20model.%20As%20for%20the%0Aspecializability%2C%20AgentRM%20can%20also%20boost%20a%20finetuned%20policy%20model%20and%0Aoutperform%20the%20top%20specialized%20agent%20by%20%2411.4%24%20on%20three%20held-in%20tasks.%20Further%0Aanalysis%20verifies%20its%20effectiveness%20in%20test-time%20scaling.%20Codes%20will%20be%0Areleased%20to%20facilitate%20the%20research%20in%20this%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18407v1&entry.124074799=Read"},
{"title": "Can LLMs Explain Themselves Counterfactually?", "author": "Zahra Dehghanighobadi and Asja Fischer and Muhammad Bilal Zafar", "abstract": "  Explanations are an important tool for gaining insights into the behavior of\nML models, calibrating user trust and ensuring regulatory compliance. Past few\nyears have seen a flurry of post-hoc methods for generating model explanations,\nmany of which involve computing model gradients or solving specially designed\noptimization problems. However, owing to the remarkable reasoning abilities of\nLarge Language Model (LLMs), self-explanation, that is, prompting the model to\nexplain its outputs has recently emerged as a new paradigm. In this work, we\nstudy a specific type of self-explanations, self-generated counterfactual\nexplanations (SCEs). We design tests for measuring the efficacy of LLMs in\ngenerating SCEs. Analysis over various LLM families, model sizes, temperature\nsettings, and datasets reveals that LLMs sometimes struggle to generate SCEs.\nEven when they do, their prediction often does not agree with their own\ncounterfactual reasoning.\n", "link": "http://arxiv.org/abs/2502.18156v1", "date": "2025-02-25", "relevancy": 1.852, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.466}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.466}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLMs%20Explain%20Themselves%20Counterfactually%3F&body=Title%3A%20Can%20LLMs%20Explain%20Themselves%20Counterfactually%3F%0AAuthor%3A%20Zahra%20Dehghanighobadi%20and%20Asja%20Fischer%20and%20Muhammad%20Bilal%20Zafar%0AAbstract%3A%20%20%20Explanations%20are%20an%20important%20tool%20for%20gaining%20insights%20into%20the%20behavior%20of%0AML%20models%2C%20calibrating%20user%20trust%20and%20ensuring%20regulatory%20compliance.%20Past%20few%0Ayears%20have%20seen%20a%20flurry%20of%20post-hoc%20methods%20for%20generating%20model%20explanations%2C%0Amany%20of%20which%20involve%20computing%20model%20gradients%20or%20solving%20specially%20designed%0Aoptimization%20problems.%20However%2C%20owing%20to%20the%20remarkable%20reasoning%20abilities%20of%0ALarge%20Language%20Model%20%28LLMs%29%2C%20self-explanation%2C%20that%20is%2C%20prompting%20the%20model%20to%0Aexplain%20its%20outputs%20has%20recently%20emerged%20as%20a%20new%20paradigm.%20In%20this%20work%2C%20we%0Astudy%20a%20specific%20type%20of%20self-explanations%2C%20self-generated%20counterfactual%0Aexplanations%20%28SCEs%29.%20We%20design%20tests%20for%20measuring%20the%20efficacy%20of%20LLMs%20in%0Agenerating%20SCEs.%20Analysis%20over%20various%20LLM%20families%2C%20model%20sizes%2C%20temperature%0Asettings%2C%20and%20datasets%20reveals%20that%20LLMs%20sometimes%20struggle%20to%20generate%20SCEs.%0AEven%20when%20they%20do%2C%20their%20prediction%20often%20does%20not%20agree%20with%20their%20own%0Acounterfactual%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLMs%2520Explain%2520Themselves%2520Counterfactually%253F%26entry.906535625%3DZahra%2520Dehghanighobadi%2520and%2520Asja%2520Fischer%2520and%2520Muhammad%2520Bilal%2520Zafar%26entry.1292438233%3D%2520%2520Explanations%2520are%2520an%2520important%2520tool%2520for%2520gaining%2520insights%2520into%2520the%2520behavior%2520of%250AML%2520models%252C%2520calibrating%2520user%2520trust%2520and%2520ensuring%2520regulatory%2520compliance.%2520Past%2520few%250Ayears%2520have%2520seen%2520a%2520flurry%2520of%2520post-hoc%2520methods%2520for%2520generating%2520model%2520explanations%252C%250Amany%2520of%2520which%2520involve%2520computing%2520model%2520gradients%2520or%2520solving%2520specially%2520designed%250Aoptimization%2520problems.%2520However%252C%2520owing%2520to%2520the%2520remarkable%2520reasoning%2520abilities%2520of%250ALarge%2520Language%2520Model%2520%2528LLMs%2529%252C%2520self-explanation%252C%2520that%2520is%252C%2520prompting%2520the%2520model%2520to%250Aexplain%2520its%2520outputs%2520has%2520recently%2520emerged%2520as%2520a%2520new%2520paradigm.%2520In%2520this%2520work%252C%2520we%250Astudy%2520a%2520specific%2520type%2520of%2520self-explanations%252C%2520self-generated%2520counterfactual%250Aexplanations%2520%2528SCEs%2529.%2520We%2520design%2520tests%2520for%2520measuring%2520the%2520efficacy%2520of%2520LLMs%2520in%250Agenerating%2520SCEs.%2520Analysis%2520over%2520various%2520LLM%2520families%252C%2520model%2520sizes%252C%2520temperature%250Asettings%252C%2520and%2520datasets%2520reveals%2520that%2520LLMs%2520sometimes%2520struggle%2520to%2520generate%2520SCEs.%250AEven%2520when%2520they%2520do%252C%2520their%2520prediction%2520often%2520does%2520not%2520agree%2520with%2520their%2520own%250Acounterfactual%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLMs%20Explain%20Themselves%20Counterfactually%3F&entry.906535625=Zahra%20Dehghanighobadi%20and%20Asja%20Fischer%20and%20Muhammad%20Bilal%20Zafar&entry.1292438233=%20%20Explanations%20are%20an%20important%20tool%20for%20gaining%20insights%20into%20the%20behavior%20of%0AML%20models%2C%20calibrating%20user%20trust%20and%20ensuring%20regulatory%20compliance.%20Past%20few%0Ayears%20have%20seen%20a%20flurry%20of%20post-hoc%20methods%20for%20generating%20model%20explanations%2C%0Amany%20of%20which%20involve%20computing%20model%20gradients%20or%20solving%20specially%20designed%0Aoptimization%20problems.%20However%2C%20owing%20to%20the%20remarkable%20reasoning%20abilities%20of%0ALarge%20Language%20Model%20%28LLMs%29%2C%20self-explanation%2C%20that%20is%2C%20prompting%20the%20model%20to%0Aexplain%20its%20outputs%20has%20recently%20emerged%20as%20a%20new%20paradigm.%20In%20this%20work%2C%20we%0Astudy%20a%20specific%20type%20of%20self-explanations%2C%20self-generated%20counterfactual%0Aexplanations%20%28SCEs%29.%20We%20design%20tests%20for%20measuring%20the%20efficacy%20of%20LLMs%20in%0Agenerating%20SCEs.%20Analysis%20over%20various%20LLM%20families%2C%20model%20sizes%2C%20temperature%0Asettings%2C%20and%20datasets%20reveals%20that%20LLMs%20sometimes%20struggle%20to%20generate%20SCEs.%0AEven%20when%20they%20do%2C%20their%20prediction%20often%20does%20not%20agree%20with%20their%20own%0Acounterfactual%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18156v1&entry.124074799=Read"},
{"title": "Pre-Surgical Planner for Robot-Assisted Vitreoretinal Surgery:\n  Integrating Eye Posture, Robot Position and Insertion Point", "author": "Satoshi Inagaki and Alireza Alikhani and Nassir Navab and Peter C. Issa and M. Ali Nasseri", "abstract": "  Several robotic frameworks have been recently developed to assist ophthalmic\nsurgeons in performing complex vitreoretinal procedures such as subretinal\ninjection of advanced therapeutics. These surgical robots show promising\ncapabilities; however, most of them have to limit their working volume to\nachieve maximum accuracy. Moreover, the visible area seen through the surgical\nmicroscope is limited and solely depends on the eye posture. If the eye\nposture, trocar position, and robot configuration are not correctly arranged,\nthe instrument may not reach the target position, and the preparation will have\nto be redone. Therefore, this paper proposes the optimization framework of the\neye tilting and the robot positioning to reach various target areas for\ndifferent patients. Our method was validated with an adjustable phantom eye\nmodel, and the error of this workflow was 0.13 +/- 1.65 deg (rotational joint\naround Y axis), -1.40 +/- 1.13 deg (around X axis), and 1.80 +/- 1.51 mm\n(depth, Z). The potential error sources are also analyzed in the discussion\nsection.\n", "link": "http://arxiv.org/abs/2502.18230v1", "date": "2025-02-25", "relevancy": 1.5422, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5448}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4823}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pre-Surgical%20Planner%20for%20Robot-Assisted%20Vitreoretinal%20Surgery%3A%0A%20%20Integrating%20Eye%20Posture%2C%20Robot%20Position%20and%20Insertion%20Point&body=Title%3A%20Pre-Surgical%20Planner%20for%20Robot-Assisted%20Vitreoretinal%20Surgery%3A%0A%20%20Integrating%20Eye%20Posture%2C%20Robot%20Position%20and%20Insertion%20Point%0AAuthor%3A%20Satoshi%20Inagaki%20and%20Alireza%20Alikhani%20and%20Nassir%20Navab%20and%20Peter%20C.%20Issa%20and%20M.%20Ali%20Nasseri%0AAbstract%3A%20%20%20Several%20robotic%20frameworks%20have%20been%20recently%20developed%20to%20assist%20ophthalmic%0Asurgeons%20in%20performing%20complex%20vitreoretinal%20procedures%20such%20as%20subretinal%0Ainjection%20of%20advanced%20therapeutics.%20These%20surgical%20robots%20show%20promising%0Acapabilities%3B%20however%2C%20most%20of%20them%20have%20to%20limit%20their%20working%20volume%20to%0Aachieve%20maximum%20accuracy.%20Moreover%2C%20the%20visible%20area%20seen%20through%20the%20surgical%0Amicroscope%20is%20limited%20and%20solely%20depends%20on%20the%20eye%20posture.%20If%20the%20eye%0Aposture%2C%20trocar%20position%2C%20and%20robot%20configuration%20are%20not%20correctly%20arranged%2C%0Athe%20instrument%20may%20not%20reach%20the%20target%20position%2C%20and%20the%20preparation%20will%20have%0Ato%20be%20redone.%20Therefore%2C%20this%20paper%20proposes%20the%20optimization%20framework%20of%20the%0Aeye%20tilting%20and%20the%20robot%20positioning%20to%20reach%20various%20target%20areas%20for%0Adifferent%20patients.%20Our%20method%20was%20validated%20with%20an%20adjustable%20phantom%20eye%0Amodel%2C%20and%20the%20error%20of%20this%20workflow%20was%200.13%20%2B/-%201.65%20deg%20%28rotational%20joint%0Aaround%20Y%20axis%29%2C%20-1.40%20%2B/-%201.13%20deg%20%28around%20X%20axis%29%2C%20and%201.80%20%2B/-%201.51%20mm%0A%28depth%2C%20Z%29.%20The%20potential%20error%20sources%20are%20also%20analyzed%20in%20the%20discussion%0Asection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.18230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPre-Surgical%2520Planner%2520for%2520Robot-Assisted%2520Vitreoretinal%2520Surgery%253A%250A%2520%2520Integrating%2520Eye%2520Posture%252C%2520Robot%2520Position%2520and%2520Insertion%2520Point%26entry.906535625%3DSatoshi%2520Inagaki%2520and%2520Alireza%2520Alikhani%2520and%2520Nassir%2520Navab%2520and%2520Peter%2520C.%2520Issa%2520and%2520M.%2520Ali%2520Nasseri%26entry.1292438233%3D%2520%2520Several%2520robotic%2520frameworks%2520have%2520been%2520recently%2520developed%2520to%2520assist%2520ophthalmic%250Asurgeons%2520in%2520performing%2520complex%2520vitreoretinal%2520procedures%2520such%2520as%2520subretinal%250Ainjection%2520of%2520advanced%2520therapeutics.%2520These%2520surgical%2520robots%2520show%2520promising%250Acapabilities%253B%2520however%252C%2520most%2520of%2520them%2520have%2520to%2520limit%2520their%2520working%2520volume%2520to%250Aachieve%2520maximum%2520accuracy.%2520Moreover%252C%2520the%2520visible%2520area%2520seen%2520through%2520the%2520surgical%250Amicroscope%2520is%2520limited%2520and%2520solely%2520depends%2520on%2520the%2520eye%2520posture.%2520If%2520the%2520eye%250Aposture%252C%2520trocar%2520position%252C%2520and%2520robot%2520configuration%2520are%2520not%2520correctly%2520arranged%252C%250Athe%2520instrument%2520may%2520not%2520reach%2520the%2520target%2520position%252C%2520and%2520the%2520preparation%2520will%2520have%250Ato%2520be%2520redone.%2520Therefore%252C%2520this%2520paper%2520proposes%2520the%2520optimization%2520framework%2520of%2520the%250Aeye%2520tilting%2520and%2520the%2520robot%2520positioning%2520to%2520reach%2520various%2520target%2520areas%2520for%250Adifferent%2520patients.%2520Our%2520method%2520was%2520validated%2520with%2520an%2520adjustable%2520phantom%2520eye%250Amodel%252C%2520and%2520the%2520error%2520of%2520this%2520workflow%2520was%25200.13%2520%252B/-%25201.65%2520deg%2520%2528rotational%2520joint%250Aaround%2520Y%2520axis%2529%252C%2520-1.40%2520%252B/-%25201.13%2520deg%2520%2528around%2520X%2520axis%2529%252C%2520and%25201.80%2520%252B/-%25201.51%2520mm%250A%2528depth%252C%2520Z%2529.%2520The%2520potential%2520error%2520sources%2520are%2520also%2520analyzed%2520in%2520the%2520discussion%250Asection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.18230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-Surgical%20Planner%20for%20Robot-Assisted%20Vitreoretinal%20Surgery%3A%0A%20%20Integrating%20Eye%20Posture%2C%20Robot%20Position%20and%20Insertion%20Point&entry.906535625=Satoshi%20Inagaki%20and%20Alireza%20Alikhani%20and%20Nassir%20Navab%20and%20Peter%20C.%20Issa%20and%20M.%20Ali%20Nasseri&entry.1292438233=%20%20Several%20robotic%20frameworks%20have%20been%20recently%20developed%20to%20assist%20ophthalmic%0Asurgeons%20in%20performing%20complex%20vitreoretinal%20procedures%20such%20as%20subretinal%0Ainjection%20of%20advanced%20therapeutics.%20These%20surgical%20robots%20show%20promising%0Acapabilities%3B%20however%2C%20most%20of%20them%20have%20to%20limit%20their%20working%20volume%20to%0Aachieve%20maximum%20accuracy.%20Moreover%2C%20the%20visible%20area%20seen%20through%20the%20surgical%0Amicroscope%20is%20limited%20and%20solely%20depends%20on%20the%20eye%20posture.%20If%20the%20eye%0Aposture%2C%20trocar%20position%2C%20and%20robot%20configuration%20are%20not%20correctly%20arranged%2C%0Athe%20instrument%20may%20not%20reach%20the%20target%20position%2C%20and%20the%20preparation%20will%20have%0Ato%20be%20redone.%20Therefore%2C%20this%20paper%20proposes%20the%20optimization%20framework%20of%20the%0Aeye%20tilting%20and%20the%20robot%20positioning%20to%20reach%20various%20target%20areas%20for%0Adifferent%20patients.%20Our%20method%20was%20validated%20with%20an%20adjustable%20phantom%20eye%0Amodel%2C%20and%20the%20error%20of%20this%20workflow%20was%200.13%20%2B/-%201.65%20deg%20%28rotational%20joint%0Aaround%20Y%20axis%29%2C%20-1.40%20%2B/-%201.13%20deg%20%28around%20X%20axis%29%2C%20and%201.80%20%2B/-%201.51%20mm%0A%28depth%2C%20Z%29.%20The%20potential%20error%20sources%20are%20also%20analyzed%20in%20the%20discussion%0Asection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.18230v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


