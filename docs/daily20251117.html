<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251116.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Duplex-GS: Proxy-Guided Weighted Blending for Real-Time Order-Independent Gaussian Splatting", "author": "Weihang Liu and Yuke Li and Yuxuan Li and Jingyi Yu and Xin Lou", "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable rendering fidelity and efficiency. However, these methods still rely on computationally expensive sequential alpha-blending operations, resulting in significant overhead, particularly on resource-constrained platforms. In this paper, we propose Duplex-GS, a dual-hierarchy framework that integrates proxy Gaussian representations with order-independent rendering techniques to achieve photorealistic results while sustaining real-time performance. To mitigate the overhead caused by view-adaptive radix sort, we introduce cell proxies for local Gaussians management and propose cell search rasterization for further acceleration. By seamlessly combining our framework with Order-Independent Transparency (OIT), we develop a physically inspired weighted sum rendering technique that simultaneously eliminates \"popping\" and \"transparency\" artifacts, yielding substantial improvements in both accuracy and efficiency. Extensive experiments on a variety of real-world datasets demonstrate the robustness of our method across diverse scenarios, including multi-scale training views and large-scale environments. Our results validate the advantages of the OIT rendering paradigm in Gaussian Splatting, achieving high-quality rendering with an impressive 1.5 to 4 speedup over existing OIT based Gaussian Splatting approaches and 52.2% to 86.9% reduction of the radix sort overhead without quality degradation.", "link": "http://arxiv.org/abs/2508.03180v2", "date": "2025-11-14", "relevancy": 3.2947, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6957}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6449}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Duplex-GS%3A%20Proxy-Guided%20Weighted%20Blending%20for%20Real-Time%20Order-Independent%20Gaussian%20Splatting&body=Title%3A%20Duplex-GS%3A%20Proxy-Guided%20Weighted%20Blending%20for%20Real-Time%20Order-Independent%20Gaussian%20Splatting%0AAuthor%3A%20Weihang%20Liu%20and%20Yuke%20Li%20and%20Yuxuan%20Li%20and%20Jingyi%20Yu%20and%20Xin%20Lou%0AAbstract%3A%20Recent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20demonstrated%20remarkable%20rendering%20fidelity%20and%20efficiency.%20However%2C%20these%20methods%20still%20rely%20on%20computationally%20expensive%20sequential%20alpha-blending%20operations%2C%20resulting%20in%20significant%20overhead%2C%20particularly%20on%20resource-constrained%20platforms.%20In%20this%20paper%2C%20we%20propose%20Duplex-GS%2C%20a%20dual-hierarchy%20framework%20that%20integrates%20proxy%20Gaussian%20representations%20with%20order-independent%20rendering%20techniques%20to%20achieve%20photorealistic%20results%20while%20sustaining%20real-time%20performance.%20To%20mitigate%20the%20overhead%20caused%20by%20view-adaptive%20radix%20sort%2C%20we%20introduce%20cell%20proxies%20for%20local%20Gaussians%20management%20and%20propose%20cell%20search%20rasterization%20for%20further%20acceleration.%20By%20seamlessly%20combining%20our%20framework%20with%20Order-Independent%20Transparency%20%28OIT%29%2C%20we%20develop%20a%20physically%20inspired%20weighted%20sum%20rendering%20technique%20that%20simultaneously%20eliminates%20%22popping%22%20and%20%22transparency%22%20artifacts%2C%20yielding%20substantial%20improvements%20in%20both%20accuracy%20and%20efficiency.%20Extensive%20experiments%20on%20a%20variety%20of%20real-world%20datasets%20demonstrate%20the%20robustness%20of%20our%20method%20across%20diverse%20scenarios%2C%20including%20multi-scale%20training%20views%20and%20large-scale%20environments.%20Our%20results%20validate%20the%20advantages%20of%20the%20OIT%20rendering%20paradigm%20in%20Gaussian%20Splatting%2C%20achieving%20high-quality%20rendering%20with%20an%20impressive%201.5%20to%204%20speedup%20over%20existing%20OIT%20based%20Gaussian%20Splatting%20approaches%20and%2052.2%25%20to%2086.9%25%20reduction%20of%20the%20radix%20sort%20overhead%20without%20quality%20degradation.%0ALink%3A%20http%3A//arxiv.org/abs/2508.03180v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDuplex-GS%253A%2520Proxy-Guided%2520Weighted%2520Blending%2520for%2520Real-Time%2520Order-Independent%2520Gaussian%2520Splatting%26entry.906535625%3DWeihang%2520Liu%2520and%2520Yuke%2520Li%2520and%2520Yuxuan%2520Li%2520and%2520Jingyi%2520Yu%2520and%2520Xin%2520Lou%26entry.1292438233%3DRecent%2520advances%2520in%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520have%2520demonstrated%2520remarkable%2520rendering%2520fidelity%2520and%2520efficiency.%2520However%252C%2520these%2520methods%2520still%2520rely%2520on%2520computationally%2520expensive%2520sequential%2520alpha-blending%2520operations%252C%2520resulting%2520in%2520significant%2520overhead%252C%2520particularly%2520on%2520resource-constrained%2520platforms.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Duplex-GS%252C%2520a%2520dual-hierarchy%2520framework%2520that%2520integrates%2520proxy%2520Gaussian%2520representations%2520with%2520order-independent%2520rendering%2520techniques%2520to%2520achieve%2520photorealistic%2520results%2520while%2520sustaining%2520real-time%2520performance.%2520To%2520mitigate%2520the%2520overhead%2520caused%2520by%2520view-adaptive%2520radix%2520sort%252C%2520we%2520introduce%2520cell%2520proxies%2520for%2520local%2520Gaussians%2520management%2520and%2520propose%2520cell%2520search%2520rasterization%2520for%2520further%2520acceleration.%2520By%2520seamlessly%2520combining%2520our%2520framework%2520with%2520Order-Independent%2520Transparency%2520%2528OIT%2529%252C%2520we%2520develop%2520a%2520physically%2520inspired%2520weighted%2520sum%2520rendering%2520technique%2520that%2520simultaneously%2520eliminates%2520%2522popping%2522%2520and%2520%2522transparency%2522%2520artifacts%252C%2520yielding%2520substantial%2520improvements%2520in%2520both%2520accuracy%2520and%2520efficiency.%2520Extensive%2520experiments%2520on%2520a%2520variety%2520of%2520real-world%2520datasets%2520demonstrate%2520the%2520robustness%2520of%2520our%2520method%2520across%2520diverse%2520scenarios%252C%2520including%2520multi-scale%2520training%2520views%2520and%2520large-scale%2520environments.%2520Our%2520results%2520validate%2520the%2520advantages%2520of%2520the%2520OIT%2520rendering%2520paradigm%2520in%2520Gaussian%2520Splatting%252C%2520achieving%2520high-quality%2520rendering%2520with%2520an%2520impressive%25201.5%2520to%25204%2520speedup%2520over%2520existing%2520OIT%2520based%2520Gaussian%2520Splatting%2520approaches%2520and%252052.2%2525%2520to%252086.9%2525%2520reduction%2520of%2520the%2520radix%2520sort%2520overhead%2520without%2520quality%2520degradation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03180v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Duplex-GS%3A%20Proxy-Guided%20Weighted%20Blending%20for%20Real-Time%20Order-Independent%20Gaussian%20Splatting&entry.906535625=Weihang%20Liu%20and%20Yuke%20Li%20and%20Yuxuan%20Li%20and%20Jingyi%20Yu%20and%20Xin%20Lou&entry.1292438233=Recent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20demonstrated%20remarkable%20rendering%20fidelity%20and%20efficiency.%20However%2C%20these%20methods%20still%20rely%20on%20computationally%20expensive%20sequential%20alpha-blending%20operations%2C%20resulting%20in%20significant%20overhead%2C%20particularly%20on%20resource-constrained%20platforms.%20In%20this%20paper%2C%20we%20propose%20Duplex-GS%2C%20a%20dual-hierarchy%20framework%20that%20integrates%20proxy%20Gaussian%20representations%20with%20order-independent%20rendering%20techniques%20to%20achieve%20photorealistic%20results%20while%20sustaining%20real-time%20performance.%20To%20mitigate%20the%20overhead%20caused%20by%20view-adaptive%20radix%20sort%2C%20we%20introduce%20cell%20proxies%20for%20local%20Gaussians%20management%20and%20propose%20cell%20search%20rasterization%20for%20further%20acceleration.%20By%20seamlessly%20combining%20our%20framework%20with%20Order-Independent%20Transparency%20%28OIT%29%2C%20we%20develop%20a%20physically%20inspired%20weighted%20sum%20rendering%20technique%20that%20simultaneously%20eliminates%20%22popping%22%20and%20%22transparency%22%20artifacts%2C%20yielding%20substantial%20improvements%20in%20both%20accuracy%20and%20efficiency.%20Extensive%20experiments%20on%20a%20variety%20of%20real-world%20datasets%20demonstrate%20the%20robustness%20of%20our%20method%20across%20diverse%20scenarios%2C%20including%20multi-scale%20training%20views%20and%20large-scale%20environments.%20Our%20results%20validate%20the%20advantages%20of%20the%20OIT%20rendering%20paradigm%20in%20Gaussian%20Splatting%2C%20achieving%20high-quality%20rendering%20with%20an%20impressive%201.5%20to%204%20speedup%20over%20existing%20OIT%20based%20Gaussian%20Splatting%20approaches%20and%2052.2%25%20to%2086.9%25%20reduction%20of%20the%20radix%20sort%20overhead%20without%20quality%20degradation.&entry.1838667208=http%3A//arxiv.org/abs/2508.03180v2&entry.124074799=Read"},
{"title": "BecomingLit: Relightable Gaussian Avatars with Hybrid Neural Shading", "author": "Jonathan Schmidt and Simon Giebenhain and Matthias Niessner", "abstract": "We introduce BecomingLit, a novel method for reconstructing relightable, high-resolution head avatars that can be rendered from novel viewpoints at interactive rates. Therefore, we propose a new low-cost light stage capture setup, tailored specifically towards capturing faces. Using this setup, we collect a novel dataset consisting of diverse multi-view sequences of numerous subjects under varying illumination conditions and facial expressions. By leveraging our new dataset, we introduce a new relightable avatar representation based on 3D Gaussian primitives that we animate with a parametric head model and an expression-dependent dynamics module. We propose a new hybrid neural shading approach, combining a neural diffuse BRDF with an analytical specular term. Our method reconstructs disentangled materials from our dynamic light stage recordings and enables all-frequency relighting of our avatars with both point lights and environment maps. In addition, our avatars can easily be animated and controlled from monocular videos. We validate our approach in extensive experiments on our dataset, where we consistently outperform existing state-of-the-art methods in relighting and reenactment by a significant margin.", "link": "http://arxiv.org/abs/2506.06271v2", "date": "2025-11-14", "relevancy": 3.2789, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6814}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6814}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BecomingLit%3A%20Relightable%20Gaussian%20Avatars%20with%20Hybrid%20Neural%20Shading&body=Title%3A%20BecomingLit%3A%20Relightable%20Gaussian%20Avatars%20with%20Hybrid%20Neural%20Shading%0AAuthor%3A%20Jonathan%20Schmidt%20and%20Simon%20Giebenhain%20and%20Matthias%20Niessner%0AAbstract%3A%20We%20introduce%20BecomingLit%2C%20a%20novel%20method%20for%20reconstructing%20relightable%2C%20high-resolution%20head%20avatars%20that%20can%20be%20rendered%20from%20novel%20viewpoints%20at%20interactive%20rates.%20Therefore%2C%20we%20propose%20a%20new%20low-cost%20light%20stage%20capture%20setup%2C%20tailored%20specifically%20towards%20capturing%20faces.%20Using%20this%20setup%2C%20we%20collect%20a%20novel%20dataset%20consisting%20of%20diverse%20multi-view%20sequences%20of%20numerous%20subjects%20under%20varying%20illumination%20conditions%20and%20facial%20expressions.%20By%20leveraging%20our%20new%20dataset%2C%20we%20introduce%20a%20new%20relightable%20avatar%20representation%20based%20on%203D%20Gaussian%20primitives%20that%20we%20animate%20with%20a%20parametric%20head%20model%20and%20an%20expression-dependent%20dynamics%20module.%20We%20propose%20a%20new%20hybrid%20neural%20shading%20approach%2C%20combining%20a%20neural%20diffuse%20BRDF%20with%20an%20analytical%20specular%20term.%20Our%20method%20reconstructs%20disentangled%20materials%20from%20our%20dynamic%20light%20stage%20recordings%20and%20enables%20all-frequency%20relighting%20of%20our%20avatars%20with%20both%20point%20lights%20and%20environment%20maps.%20In%20addition%2C%20our%20avatars%20can%20easily%20be%20animated%20and%20controlled%20from%20monocular%20videos.%20We%20validate%20our%20approach%20in%20extensive%20experiments%20on%20our%20dataset%2C%20where%20we%20consistently%20outperform%20existing%20state-of-the-art%20methods%20in%20relighting%20and%20reenactment%20by%20a%20significant%20margin.%0ALink%3A%20http%3A//arxiv.org/abs/2506.06271v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBecomingLit%253A%2520Relightable%2520Gaussian%2520Avatars%2520with%2520Hybrid%2520Neural%2520Shading%26entry.906535625%3DJonathan%2520Schmidt%2520and%2520Simon%2520Giebenhain%2520and%2520Matthias%2520Niessner%26entry.1292438233%3DWe%2520introduce%2520BecomingLit%252C%2520a%2520novel%2520method%2520for%2520reconstructing%2520relightable%252C%2520high-resolution%2520head%2520avatars%2520that%2520can%2520be%2520rendered%2520from%2520novel%2520viewpoints%2520at%2520interactive%2520rates.%2520Therefore%252C%2520we%2520propose%2520a%2520new%2520low-cost%2520light%2520stage%2520capture%2520setup%252C%2520tailored%2520specifically%2520towards%2520capturing%2520faces.%2520Using%2520this%2520setup%252C%2520we%2520collect%2520a%2520novel%2520dataset%2520consisting%2520of%2520diverse%2520multi-view%2520sequences%2520of%2520numerous%2520subjects%2520under%2520varying%2520illumination%2520conditions%2520and%2520facial%2520expressions.%2520By%2520leveraging%2520our%2520new%2520dataset%252C%2520we%2520introduce%2520a%2520new%2520relightable%2520avatar%2520representation%2520based%2520on%25203D%2520Gaussian%2520primitives%2520that%2520we%2520animate%2520with%2520a%2520parametric%2520head%2520model%2520and%2520an%2520expression-dependent%2520dynamics%2520module.%2520We%2520propose%2520a%2520new%2520hybrid%2520neural%2520shading%2520approach%252C%2520combining%2520a%2520neural%2520diffuse%2520BRDF%2520with%2520an%2520analytical%2520specular%2520term.%2520Our%2520method%2520reconstructs%2520disentangled%2520materials%2520from%2520our%2520dynamic%2520light%2520stage%2520recordings%2520and%2520enables%2520all-frequency%2520relighting%2520of%2520our%2520avatars%2520with%2520both%2520point%2520lights%2520and%2520environment%2520maps.%2520In%2520addition%252C%2520our%2520avatars%2520can%2520easily%2520be%2520animated%2520and%2520controlled%2520from%2520monocular%2520videos.%2520We%2520validate%2520our%2520approach%2520in%2520extensive%2520experiments%2520on%2520our%2520dataset%252C%2520where%2520we%2520consistently%2520outperform%2520existing%2520state-of-the-art%2520methods%2520in%2520relighting%2520and%2520reenactment%2520by%2520a%2520significant%2520margin.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06271v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BecomingLit%3A%20Relightable%20Gaussian%20Avatars%20with%20Hybrid%20Neural%20Shading&entry.906535625=Jonathan%20Schmidt%20and%20Simon%20Giebenhain%20and%20Matthias%20Niessner&entry.1292438233=We%20introduce%20BecomingLit%2C%20a%20novel%20method%20for%20reconstructing%20relightable%2C%20high-resolution%20head%20avatars%20that%20can%20be%20rendered%20from%20novel%20viewpoints%20at%20interactive%20rates.%20Therefore%2C%20we%20propose%20a%20new%20low-cost%20light%20stage%20capture%20setup%2C%20tailored%20specifically%20towards%20capturing%20faces.%20Using%20this%20setup%2C%20we%20collect%20a%20novel%20dataset%20consisting%20of%20diverse%20multi-view%20sequences%20of%20numerous%20subjects%20under%20varying%20illumination%20conditions%20and%20facial%20expressions.%20By%20leveraging%20our%20new%20dataset%2C%20we%20introduce%20a%20new%20relightable%20avatar%20representation%20based%20on%203D%20Gaussian%20primitives%20that%20we%20animate%20with%20a%20parametric%20head%20model%20and%20an%20expression-dependent%20dynamics%20module.%20We%20propose%20a%20new%20hybrid%20neural%20shading%20approach%2C%20combining%20a%20neural%20diffuse%20BRDF%20with%20an%20analytical%20specular%20term.%20Our%20method%20reconstructs%20disentangled%20materials%20from%20our%20dynamic%20light%20stage%20recordings%20and%20enables%20all-frequency%20relighting%20of%20our%20avatars%20with%20both%20point%20lights%20and%20environment%20maps.%20In%20addition%2C%20our%20avatars%20can%20easily%20be%20animated%20and%20controlled%20from%20monocular%20videos.%20We%20validate%20our%20approach%20in%20extensive%20experiments%20on%20our%20dataset%2C%20where%20we%20consistently%20outperform%20existing%20state-of-the-art%20methods%20in%20relighting%20and%20reenactment%20by%20a%20significant%20margin.&entry.1838667208=http%3A//arxiv.org/abs/2506.06271v2&entry.124074799=Read"},
{"title": "RealisticDreamer: Guidance Score Distillation for Few-shot Gaussian Splatting", "author": "Ruocheng Wu and Haolan He and Yufei Wang and Zhihao Li and Bihan Wen", "abstract": "3D Gaussian Splatting (3DGS) has recently gained great attention in the 3D scene representation for its high-quality real-time rendering capabilities. However, when the input comprises sparse training views, 3DGS is prone to overfitting, primarily due to the lack of intermediate-view supervision. Inspired by the recent success of Video Diffusion Models (VDM), we propose a framework called Guidance Score Distillation (GSD) to extract the rich multi-view consistency priors from pretrained VDMs. Building on the insights from Score Distillation Sampling (SDS), GSD supervises rendered images from multiple neighboring views, guiding the Gaussian splatting representation towards the generative direction of VDM. However, the generative direction often involves object motion and random camera trajectories, making it challenging for direct supervision in the optimization process. To address this problem, we introduce an unified guidance form to correct the noise prediction result of VDM. Specifically, we incorporate both a depth warp guidance based on real depth maps and a guidance based on semantic image features, ensuring that the score update direction from VDM aligns with the correct camera pose and accurate geometry. Experimental results show that our method outperforms existing approaches across multiple datasets.", "link": "http://arxiv.org/abs/2511.11213v1", "date": "2025-11-14", "relevancy": 3.2707, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6717}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6632}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RealisticDreamer%3A%20Guidance%20Score%20Distillation%20for%20Few-shot%20Gaussian%20Splatting&body=Title%3A%20RealisticDreamer%3A%20Guidance%20Score%20Distillation%20for%20Few-shot%20Gaussian%20Splatting%0AAuthor%3A%20Ruocheng%20Wu%20and%20Haolan%20He%20and%20Yufei%20Wang%20and%20Zhihao%20Li%20and%20Bihan%20Wen%0AAbstract%3A%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20gained%20great%20attention%20in%20the%203D%20scene%20representation%20for%20its%20high-quality%20real-time%20rendering%20capabilities.%20However%2C%20when%20the%20input%20comprises%20sparse%20training%20views%2C%203DGS%20is%20prone%20to%20overfitting%2C%20primarily%20due%20to%20the%20lack%20of%20intermediate-view%20supervision.%20Inspired%20by%20the%20recent%20success%20of%20Video%20Diffusion%20Models%20%28VDM%29%2C%20we%20propose%20a%20framework%20called%20Guidance%20Score%20Distillation%20%28GSD%29%20to%20extract%20the%20rich%20multi-view%20consistency%20priors%20from%20pretrained%20VDMs.%20Building%20on%20the%20insights%20from%20Score%20Distillation%20Sampling%20%28SDS%29%2C%20GSD%20supervises%20rendered%20images%20from%20multiple%20neighboring%20views%2C%20guiding%20the%20Gaussian%20splatting%20representation%20towards%20the%20generative%20direction%20of%20VDM.%20However%2C%20the%20generative%20direction%20often%20involves%20object%20motion%20and%20random%20camera%20trajectories%2C%20making%20it%20challenging%20for%20direct%20supervision%20in%20the%20optimization%20process.%20To%20address%20this%20problem%2C%20we%20introduce%20an%20unified%20guidance%20form%20to%20correct%20the%20noise%20prediction%20result%20of%20VDM.%20Specifically%2C%20we%20incorporate%20both%20a%20depth%20warp%20guidance%20based%20on%20real%20depth%20maps%20and%20a%20guidance%20based%20on%20semantic%20image%20features%2C%20ensuring%20that%20the%20score%20update%20direction%20from%20VDM%20aligns%20with%20the%20correct%20camera%20pose%20and%20accurate%20geometry.%20Experimental%20results%20show%20that%20our%20method%20outperforms%20existing%20approaches%20across%20multiple%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealisticDreamer%253A%2520Guidance%2520Score%2520Distillation%2520for%2520Few-shot%2520Gaussian%2520Splatting%26entry.906535625%3DRuocheng%2520Wu%2520and%2520Haolan%2520He%2520and%2520Yufei%2520Wang%2520and%2520Zhihao%2520Li%2520and%2520Bihan%2520Wen%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520recently%2520gained%2520great%2520attention%2520in%2520the%25203D%2520scene%2520representation%2520for%2520its%2520high-quality%2520real-time%2520rendering%2520capabilities.%2520However%252C%2520when%2520the%2520input%2520comprises%2520sparse%2520training%2520views%252C%25203DGS%2520is%2520prone%2520to%2520overfitting%252C%2520primarily%2520due%2520to%2520the%2520lack%2520of%2520intermediate-view%2520supervision.%2520Inspired%2520by%2520the%2520recent%2520success%2520of%2520Video%2520Diffusion%2520Models%2520%2528VDM%2529%252C%2520we%2520propose%2520a%2520framework%2520called%2520Guidance%2520Score%2520Distillation%2520%2528GSD%2529%2520to%2520extract%2520the%2520rich%2520multi-view%2520consistency%2520priors%2520from%2520pretrained%2520VDMs.%2520Building%2520on%2520the%2520insights%2520from%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529%252C%2520GSD%2520supervises%2520rendered%2520images%2520from%2520multiple%2520neighboring%2520views%252C%2520guiding%2520the%2520Gaussian%2520splatting%2520representation%2520towards%2520the%2520generative%2520direction%2520of%2520VDM.%2520However%252C%2520the%2520generative%2520direction%2520often%2520involves%2520object%2520motion%2520and%2520random%2520camera%2520trajectories%252C%2520making%2520it%2520challenging%2520for%2520direct%2520supervision%2520in%2520the%2520optimization%2520process.%2520To%2520address%2520this%2520problem%252C%2520we%2520introduce%2520an%2520unified%2520guidance%2520form%2520to%2520correct%2520the%2520noise%2520prediction%2520result%2520of%2520VDM.%2520Specifically%252C%2520we%2520incorporate%2520both%2520a%2520depth%2520warp%2520guidance%2520based%2520on%2520real%2520depth%2520maps%2520and%2520a%2520guidance%2520based%2520on%2520semantic%2520image%2520features%252C%2520ensuring%2520that%2520the%2520score%2520update%2520direction%2520from%2520VDM%2520aligns%2520with%2520the%2520correct%2520camera%2520pose%2520and%2520accurate%2520geometry.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520outperforms%2520existing%2520approaches%2520across%2520multiple%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealisticDreamer%3A%20Guidance%20Score%20Distillation%20for%20Few-shot%20Gaussian%20Splatting&entry.906535625=Ruocheng%20Wu%20and%20Haolan%20He%20and%20Yufei%20Wang%20and%20Zhihao%20Li%20and%20Bihan%20Wen&entry.1292438233=3D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20gained%20great%20attention%20in%20the%203D%20scene%20representation%20for%20its%20high-quality%20real-time%20rendering%20capabilities.%20However%2C%20when%20the%20input%20comprises%20sparse%20training%20views%2C%203DGS%20is%20prone%20to%20overfitting%2C%20primarily%20due%20to%20the%20lack%20of%20intermediate-view%20supervision.%20Inspired%20by%20the%20recent%20success%20of%20Video%20Diffusion%20Models%20%28VDM%29%2C%20we%20propose%20a%20framework%20called%20Guidance%20Score%20Distillation%20%28GSD%29%20to%20extract%20the%20rich%20multi-view%20consistency%20priors%20from%20pretrained%20VDMs.%20Building%20on%20the%20insights%20from%20Score%20Distillation%20Sampling%20%28SDS%29%2C%20GSD%20supervises%20rendered%20images%20from%20multiple%20neighboring%20views%2C%20guiding%20the%20Gaussian%20splatting%20representation%20towards%20the%20generative%20direction%20of%20VDM.%20However%2C%20the%20generative%20direction%20often%20involves%20object%20motion%20and%20random%20camera%20trajectories%2C%20making%20it%20challenging%20for%20direct%20supervision%20in%20the%20optimization%20process.%20To%20address%20this%20problem%2C%20we%20introduce%20an%20unified%20guidance%20form%20to%20correct%20the%20noise%20prediction%20result%20of%20VDM.%20Specifically%2C%20we%20incorporate%20both%20a%20depth%20warp%20guidance%20based%20on%20real%20depth%20maps%20and%20a%20guidance%20based%20on%20semantic%20image%20features%2C%20ensuring%20that%20the%20score%20update%20direction%20from%20VDM%20aligns%20with%20the%20correct%20camera%20pose%20and%20accurate%20geometry.%20Experimental%20results%20show%20that%20our%20method%20outperforms%20existing%20approaches%20across%20multiple%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2511.11213v1&entry.124074799=Read"},
{"title": "3D Gaussian and Diffusion-Based Gaze Redirection", "author": "Abiram Panchalingam and Indu Bodala and Stuart Middleton", "abstract": "High-fidelity gaze redirection is critical for generating augmented data to improve the generalization of gaze estimators. 3D Gaussian Splatting (3DGS) models like GazeGaussian represent the state-of-the-art but can struggle with rendering subtle, continuous gaze shifts. In this paper, we propose DiT-Gaze, a framework that enhances 3D gaze redirection models using a novel combination of Diffusion Transformer (DiT), weak supervision across gaze angles, and an orthogonality constraint loss. DiT allows higher-fidelity image synthesis, while our weak supervision strategy using synthetically generated intermediate gaze angles provides a smooth manifold of gaze directions during training. The orthogonality constraint loss mathematically enforces the disentanglement of internal representations for gaze, head pose, and expression. Comprehensive experiments show that DiT-Gaze sets a new state-of-the-art in both perceptual quality and redirection accuracy, reducing the state-of-the-art gaze error by 4.1% to 6.353 degrees, providing a superior method for creating synthetic training data. Our code and models will be made available for the research community to benchmark against.", "link": "http://arxiv.org/abs/2511.11231v1", "date": "2025-11-14", "relevancy": 3.1961, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6543}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6346}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Gaussian%20and%20Diffusion-Based%20Gaze%20Redirection&body=Title%3A%203D%20Gaussian%20and%20Diffusion-Based%20Gaze%20Redirection%0AAuthor%3A%20Abiram%20Panchalingam%20and%20Indu%20Bodala%20and%20Stuart%20Middleton%0AAbstract%3A%20High-fidelity%20gaze%20redirection%20is%20critical%20for%20generating%20augmented%20data%20to%20improve%20the%20generalization%20of%20gaze%20estimators.%203D%20Gaussian%20Splatting%20%283DGS%29%20models%20like%20GazeGaussian%20represent%20the%20state-of-the-art%20but%20can%20struggle%20with%20rendering%20subtle%2C%20continuous%20gaze%20shifts.%20In%20this%20paper%2C%20we%20propose%20DiT-Gaze%2C%20a%20framework%20that%20enhances%203D%20gaze%20redirection%20models%20using%20a%20novel%20combination%20of%20Diffusion%20Transformer%20%28DiT%29%2C%20weak%20supervision%20across%20gaze%20angles%2C%20and%20an%20orthogonality%20constraint%20loss.%20DiT%20allows%20higher-fidelity%20image%20synthesis%2C%20while%20our%20weak%20supervision%20strategy%20using%20synthetically%20generated%20intermediate%20gaze%20angles%20provides%20a%20smooth%20manifold%20of%20gaze%20directions%20during%20training.%20The%20orthogonality%20constraint%20loss%20mathematically%20enforces%20the%20disentanglement%20of%20internal%20representations%20for%20gaze%2C%20head%20pose%2C%20and%20expression.%20Comprehensive%20experiments%20show%20that%20DiT-Gaze%20sets%20a%20new%20state-of-the-art%20in%20both%20perceptual%20quality%20and%20redirection%20accuracy%2C%20reducing%20the%20state-of-the-art%20gaze%20error%20by%204.1%25%20to%206.353%20degrees%2C%20providing%20a%20superior%20method%20for%20creating%20synthetic%20training%20data.%20Our%20code%20and%20models%20will%20be%20made%20available%20for%20the%20research%20community%20to%20benchmark%20against.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11231v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Gaussian%2520and%2520Diffusion-Based%2520Gaze%2520Redirection%26entry.906535625%3DAbiram%2520Panchalingam%2520and%2520Indu%2520Bodala%2520and%2520Stuart%2520Middleton%26entry.1292438233%3DHigh-fidelity%2520gaze%2520redirection%2520is%2520critical%2520for%2520generating%2520augmented%2520data%2520to%2520improve%2520the%2520generalization%2520of%2520gaze%2520estimators.%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520models%2520like%2520GazeGaussian%2520represent%2520the%2520state-of-the-art%2520but%2520can%2520struggle%2520with%2520rendering%2520subtle%252C%2520continuous%2520gaze%2520shifts.%2520In%2520this%2520paper%252C%2520we%2520propose%2520DiT-Gaze%252C%2520a%2520framework%2520that%2520enhances%25203D%2520gaze%2520redirection%2520models%2520using%2520a%2520novel%2520combination%2520of%2520Diffusion%2520Transformer%2520%2528DiT%2529%252C%2520weak%2520supervision%2520across%2520gaze%2520angles%252C%2520and%2520an%2520orthogonality%2520constraint%2520loss.%2520DiT%2520allows%2520higher-fidelity%2520image%2520synthesis%252C%2520while%2520our%2520weak%2520supervision%2520strategy%2520using%2520synthetically%2520generated%2520intermediate%2520gaze%2520angles%2520provides%2520a%2520smooth%2520manifold%2520of%2520gaze%2520directions%2520during%2520training.%2520The%2520orthogonality%2520constraint%2520loss%2520mathematically%2520enforces%2520the%2520disentanglement%2520of%2520internal%2520representations%2520for%2520gaze%252C%2520head%2520pose%252C%2520and%2520expression.%2520Comprehensive%2520experiments%2520show%2520that%2520DiT-Gaze%2520sets%2520a%2520new%2520state-of-the-art%2520in%2520both%2520perceptual%2520quality%2520and%2520redirection%2520accuracy%252C%2520reducing%2520the%2520state-of-the-art%2520gaze%2520error%2520by%25204.1%2525%2520to%25206.353%2520degrees%252C%2520providing%2520a%2520superior%2520method%2520for%2520creating%2520synthetic%2520training%2520data.%2520Our%2520code%2520and%2520models%2520will%2520be%2520made%2520available%2520for%2520the%2520research%2520community%2520to%2520benchmark%2520against.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11231v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Gaussian%20and%20Diffusion-Based%20Gaze%20Redirection&entry.906535625=Abiram%20Panchalingam%20and%20Indu%20Bodala%20and%20Stuart%20Middleton&entry.1292438233=High-fidelity%20gaze%20redirection%20is%20critical%20for%20generating%20augmented%20data%20to%20improve%20the%20generalization%20of%20gaze%20estimators.%203D%20Gaussian%20Splatting%20%283DGS%29%20models%20like%20GazeGaussian%20represent%20the%20state-of-the-art%20but%20can%20struggle%20with%20rendering%20subtle%2C%20continuous%20gaze%20shifts.%20In%20this%20paper%2C%20we%20propose%20DiT-Gaze%2C%20a%20framework%20that%20enhances%203D%20gaze%20redirection%20models%20using%20a%20novel%20combination%20of%20Diffusion%20Transformer%20%28DiT%29%2C%20weak%20supervision%20across%20gaze%20angles%2C%20and%20an%20orthogonality%20constraint%20loss.%20DiT%20allows%20higher-fidelity%20image%20synthesis%2C%20while%20our%20weak%20supervision%20strategy%20using%20synthetically%20generated%20intermediate%20gaze%20angles%20provides%20a%20smooth%20manifold%20of%20gaze%20directions%20during%20training.%20The%20orthogonality%20constraint%20loss%20mathematically%20enforces%20the%20disentanglement%20of%20internal%20representations%20for%20gaze%2C%20head%20pose%2C%20and%20expression.%20Comprehensive%20experiments%20show%20that%20DiT-Gaze%20sets%20a%20new%20state-of-the-art%20in%20both%20perceptual%20quality%20and%20redirection%20accuracy%2C%20reducing%20the%20state-of-the-art%20gaze%20error%20by%204.1%25%20to%206.353%20degrees%2C%20providing%20a%20superior%20method%20for%20creating%20synthetic%20training%20data.%20Our%20code%20and%20models%20will%20be%20made%20available%20for%20the%20research%20community%20to%20benchmark%20against.&entry.1838667208=http%3A//arxiv.org/abs/2511.11231v1&entry.124074799=Read"},
{"title": "ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models", "author": "Zhaoyang Li and Zhan Ling and Yuchen Zhou and Litian Gong and Erdem B\u0131y\u0131k and Hao Su", "abstract": "Large Vision-Language Models (LVLMs) excel at captioning, visual question answering, and robotics by combining vision and language, yet they often miss obvious objects or hallucinate nonexistent ones in atypical scenes. We examine these failures through the lens of uncertainty, focusing on contextual incongruity, where objects appear unexpectedly or fail to appear in expected contexts, and show that such cases increase recognition difficulty for state-of-the-art LVLMs. To study this regime, we introduce the Object Recognition in Incongruous Context (ORIC) framework, which constructs incongruous object-context pairs through two complementary strategies: (1) LLM-guided sampling to identify hard-to-recognize objects present in the image and (2) CLIP-guided sampling to mine plausible but absent ones. Applied to MSCOCO, ORIC produces ORIC-Bench and ORIC-style training data. Evaluating 18 LVLMs and 2 open-vocabulary detectors reveals substantial performance drops and bias patterns under incongruous contexts. Fine-tuning Qwen3-VL-8B-Instruct with Visual Reinforcement Fine-Tuning on 600 ORIC-style samples improves results on ORIC-Bench, AMBER, and HallusionBench. Overall, we show that contextual incongruity is a key source of uncertainty and provide tools for more reliable LVLMs. The code is available at https://github.com/ZhaoyangLi-1/ORIC.", "link": "http://arxiv.org/abs/2509.15695v2", "date": "2025-11-14", "relevancy": 3.036, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6136}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6136}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ORIC%3A%20Benchmarking%20Object%20Recognition%20under%20Contextual%20Incongruity%20in%20Large%20Vision-Language%20Models&body=Title%3A%20ORIC%3A%20Benchmarking%20Object%20Recognition%20under%20Contextual%20Incongruity%20in%20Large%20Vision-Language%20Models%0AAuthor%3A%20Zhaoyang%20Li%20and%20Zhan%20Ling%20and%20Yuchen%20Zhou%20and%20Litian%20Gong%20and%20Erdem%20B%C4%B1y%C4%B1k%20and%20Hao%20Su%0AAbstract%3A%20Large%20Vision-Language%20Models%20%28LVLMs%29%20excel%20at%20captioning%2C%20visual%20question%20answering%2C%20and%20robotics%20by%20combining%20vision%20and%20language%2C%20yet%20they%20often%20miss%20obvious%20objects%20or%20hallucinate%20nonexistent%20ones%20in%20atypical%20scenes.%20We%20examine%20these%20failures%20through%20the%20lens%20of%20uncertainty%2C%20focusing%20on%20contextual%20incongruity%2C%20where%20objects%20appear%20unexpectedly%20or%20fail%20to%20appear%20in%20expected%20contexts%2C%20and%20show%20that%20such%20cases%20increase%20recognition%20difficulty%20for%20state-of-the-art%20LVLMs.%20To%20study%20this%20regime%2C%20we%20introduce%20the%20Object%20Recognition%20in%20Incongruous%20Context%20%28ORIC%29%20framework%2C%20which%20constructs%20incongruous%20object-context%20pairs%20through%20two%20complementary%20strategies%3A%20%281%29%20LLM-guided%20sampling%20to%20identify%20hard-to-recognize%20objects%20present%20in%20the%20image%20and%20%282%29%20CLIP-guided%20sampling%20to%20mine%20plausible%20but%20absent%20ones.%20Applied%20to%20MSCOCO%2C%20ORIC%20produces%20ORIC-Bench%20and%20ORIC-style%20training%20data.%20Evaluating%2018%20LVLMs%20and%202%20open-vocabulary%20detectors%20reveals%20substantial%20performance%20drops%20and%20bias%20patterns%20under%20incongruous%20contexts.%20Fine-tuning%20Qwen3-VL-8B-Instruct%20with%20Visual%20Reinforcement%20Fine-Tuning%20on%20600%20ORIC-style%20samples%20improves%20results%20on%20ORIC-Bench%2C%20AMBER%2C%20and%20HallusionBench.%20Overall%2C%20we%20show%20that%20contextual%20incongruity%20is%20a%20key%20source%20of%20uncertainty%20and%20provide%20tools%20for%20more%20reliable%20LVLMs.%20The%20code%20is%20available%20at%20https%3A//github.com/ZhaoyangLi-1/ORIC.%0ALink%3A%20http%3A//arxiv.org/abs/2509.15695v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DORIC%253A%2520Benchmarking%2520Object%2520Recognition%2520under%2520Contextual%2520Incongruity%2520in%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DZhaoyang%2520Li%2520and%2520Zhan%2520Ling%2520and%2520Yuchen%2520Zhou%2520and%2520Litian%2520Gong%2520and%2520Erdem%2520B%25C4%25B1y%25C4%25B1k%2520and%2520Hao%2520Su%26entry.1292438233%3DLarge%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520excel%2520at%2520captioning%252C%2520visual%2520question%2520answering%252C%2520and%2520robotics%2520by%2520combining%2520vision%2520and%2520language%252C%2520yet%2520they%2520often%2520miss%2520obvious%2520objects%2520or%2520hallucinate%2520nonexistent%2520ones%2520in%2520atypical%2520scenes.%2520We%2520examine%2520these%2520failures%2520through%2520the%2520lens%2520of%2520uncertainty%252C%2520focusing%2520on%2520contextual%2520incongruity%252C%2520where%2520objects%2520appear%2520unexpectedly%2520or%2520fail%2520to%2520appear%2520in%2520expected%2520contexts%252C%2520and%2520show%2520that%2520such%2520cases%2520increase%2520recognition%2520difficulty%2520for%2520state-of-the-art%2520LVLMs.%2520To%2520study%2520this%2520regime%252C%2520we%2520introduce%2520the%2520Object%2520Recognition%2520in%2520Incongruous%2520Context%2520%2528ORIC%2529%2520framework%252C%2520which%2520constructs%2520incongruous%2520object-context%2520pairs%2520through%2520two%2520complementary%2520strategies%253A%2520%25281%2529%2520LLM-guided%2520sampling%2520to%2520identify%2520hard-to-recognize%2520objects%2520present%2520in%2520the%2520image%2520and%2520%25282%2529%2520CLIP-guided%2520sampling%2520to%2520mine%2520plausible%2520but%2520absent%2520ones.%2520Applied%2520to%2520MSCOCO%252C%2520ORIC%2520produces%2520ORIC-Bench%2520and%2520ORIC-style%2520training%2520data.%2520Evaluating%252018%2520LVLMs%2520and%25202%2520open-vocabulary%2520detectors%2520reveals%2520substantial%2520performance%2520drops%2520and%2520bias%2520patterns%2520under%2520incongruous%2520contexts.%2520Fine-tuning%2520Qwen3-VL-8B-Instruct%2520with%2520Visual%2520Reinforcement%2520Fine-Tuning%2520on%2520600%2520ORIC-style%2520samples%2520improves%2520results%2520on%2520ORIC-Bench%252C%2520AMBER%252C%2520and%2520HallusionBench.%2520Overall%252C%2520we%2520show%2520that%2520contextual%2520incongruity%2520is%2520a%2520key%2520source%2520of%2520uncertainty%2520and%2520provide%2520tools%2520for%2520more%2520reliable%2520LVLMs.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/ZhaoyangLi-1/ORIC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15695v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ORIC%3A%20Benchmarking%20Object%20Recognition%20under%20Contextual%20Incongruity%20in%20Large%20Vision-Language%20Models&entry.906535625=Zhaoyang%20Li%20and%20Zhan%20Ling%20and%20Yuchen%20Zhou%20and%20Litian%20Gong%20and%20Erdem%20B%C4%B1y%C4%B1k%20and%20Hao%20Su&entry.1292438233=Large%20Vision-Language%20Models%20%28LVLMs%29%20excel%20at%20captioning%2C%20visual%20question%20answering%2C%20and%20robotics%20by%20combining%20vision%20and%20language%2C%20yet%20they%20often%20miss%20obvious%20objects%20or%20hallucinate%20nonexistent%20ones%20in%20atypical%20scenes.%20We%20examine%20these%20failures%20through%20the%20lens%20of%20uncertainty%2C%20focusing%20on%20contextual%20incongruity%2C%20where%20objects%20appear%20unexpectedly%20or%20fail%20to%20appear%20in%20expected%20contexts%2C%20and%20show%20that%20such%20cases%20increase%20recognition%20difficulty%20for%20state-of-the-art%20LVLMs.%20To%20study%20this%20regime%2C%20we%20introduce%20the%20Object%20Recognition%20in%20Incongruous%20Context%20%28ORIC%29%20framework%2C%20which%20constructs%20incongruous%20object-context%20pairs%20through%20two%20complementary%20strategies%3A%20%281%29%20LLM-guided%20sampling%20to%20identify%20hard-to-recognize%20objects%20present%20in%20the%20image%20and%20%282%29%20CLIP-guided%20sampling%20to%20mine%20plausible%20but%20absent%20ones.%20Applied%20to%20MSCOCO%2C%20ORIC%20produces%20ORIC-Bench%20and%20ORIC-style%20training%20data.%20Evaluating%2018%20LVLMs%20and%202%20open-vocabulary%20detectors%20reveals%20substantial%20performance%20drops%20and%20bias%20patterns%20under%20incongruous%20contexts.%20Fine-tuning%20Qwen3-VL-8B-Instruct%20with%20Visual%20Reinforcement%20Fine-Tuning%20on%20600%20ORIC-style%20samples%20improves%20results%20on%20ORIC-Bench%2C%20AMBER%2C%20and%20HallusionBench.%20Overall%2C%20we%20show%20that%20contextual%20incongruity%20is%20a%20key%20source%20of%20uncertainty%20and%20provide%20tools%20for%20more%20reliable%20LVLMs.%20The%20code%20is%20available%20at%20https%3A//github.com/ZhaoyangLi-1/ORIC.&entry.1838667208=http%3A//arxiv.org/abs/2509.15695v2&entry.124074799=Read"},
{"title": "DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding", "author": "Dawei Zhu and Rui Meng and Jiefeng Chen and Sujian Li and Tomas Pfister and Jinsung Yoon", "abstract": "Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.", "link": "http://arxiv.org/abs/2511.11552v1", "date": "2025-11-14", "relevancy": 3.0336, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6194}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6194}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DocLens%20%3A%20A%20Tool-Augmented%20Multi-Agent%20Framework%20for%20Long%20Visual%20Document%20Understanding&body=Title%3A%20DocLens%20%3A%20A%20Tool-Augmented%20Multi-Agent%20Framework%20for%20Long%20Visual%20Document%20Understanding%0AAuthor%3A%20Dawei%20Zhu%20and%20Rui%20Meng%20and%20Jiefeng%20Chen%20and%20Sujian%20Li%20and%20Tomas%20Pfister%20and%20Jinsung%20Yoon%0AAbstract%3A%20Comprehending%20long%20visual%20documents%2C%20where%20information%20is%20distributed%20across%20extensive%20pages%20of%20text%20and%20visual%20elements%2C%20is%20a%20critical%20but%20challenging%20task%20for%20modern%20Vision-Language%20Models%20%28VLMs%29.%20Existing%20approaches%20falter%20on%20a%20fundamental%20challenge%3A%20evidence%20localization.%20They%20struggle%20to%20retrieve%20relevant%20pages%20and%20overlook%20fine-grained%20details%20within%20visual%20elements%2C%20leading%20to%20limited%20performance%20and%20model%20hallucination.%20To%20address%20this%2C%20we%20propose%20DocLens%2C%20a%20tool-augmented%20multi-agent%20framework%20that%20effectively%20%60%60zooms%20in%27%27%20on%20evidence%20like%20a%20lens.%20It%20first%20navigates%20from%20the%20full%20document%20to%20specific%20visual%20elements%20on%20relevant%20pages%2C%20then%20employs%20a%20sampling-adjudication%20mechanism%20to%20generate%20a%20single%2C%20reliable%20answer.%20Paired%20with%20Gemini-2.5-Pro%2C%20DocLens%20achieves%20state-of-the-art%20performance%20on%20MMLongBench-Doc%20and%20FinRAGBench-V%2C%20surpassing%20even%20human%20experts.%20The%20framework%27s%20superiority%20is%20particularly%20evident%20on%20vision-centric%20and%20unanswerable%20queries%2C%20demonstrating%20the%20power%20of%20its%20enhanced%20localization%20capabilities.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocLens%2520%253A%2520A%2520Tool-Augmented%2520Multi-Agent%2520Framework%2520for%2520Long%2520Visual%2520Document%2520Understanding%26entry.906535625%3DDawei%2520Zhu%2520and%2520Rui%2520Meng%2520and%2520Jiefeng%2520Chen%2520and%2520Sujian%2520Li%2520and%2520Tomas%2520Pfister%2520and%2520Jinsung%2520Yoon%26entry.1292438233%3DComprehending%2520long%2520visual%2520documents%252C%2520where%2520information%2520is%2520distributed%2520across%2520extensive%2520pages%2520of%2520text%2520and%2520visual%2520elements%252C%2520is%2520a%2520critical%2520but%2520challenging%2520task%2520for%2520modern%2520Vision-Language%2520Models%2520%2528VLMs%2529.%2520Existing%2520approaches%2520falter%2520on%2520a%2520fundamental%2520challenge%253A%2520evidence%2520localization.%2520They%2520struggle%2520to%2520retrieve%2520relevant%2520pages%2520and%2520overlook%2520fine-grained%2520details%2520within%2520visual%2520elements%252C%2520leading%2520to%2520limited%2520performance%2520and%2520model%2520hallucination.%2520To%2520address%2520this%252C%2520we%2520propose%2520DocLens%252C%2520a%2520tool-augmented%2520multi-agent%2520framework%2520that%2520effectively%2520%2560%2560zooms%2520in%2527%2527%2520on%2520evidence%2520like%2520a%2520lens.%2520It%2520first%2520navigates%2520from%2520the%2520full%2520document%2520to%2520specific%2520visual%2520elements%2520on%2520relevant%2520pages%252C%2520then%2520employs%2520a%2520sampling-adjudication%2520mechanism%2520to%2520generate%2520a%2520single%252C%2520reliable%2520answer.%2520Paired%2520with%2520Gemini-2.5-Pro%252C%2520DocLens%2520achieves%2520state-of-the-art%2520performance%2520on%2520MMLongBench-Doc%2520and%2520FinRAGBench-V%252C%2520surpassing%2520even%2520human%2520experts.%2520The%2520framework%2527s%2520superiority%2520is%2520particularly%2520evident%2520on%2520vision-centric%2520and%2520unanswerable%2520queries%252C%2520demonstrating%2520the%2520power%2520of%2520its%2520enhanced%2520localization%2520capabilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DocLens%20%3A%20A%20Tool-Augmented%20Multi-Agent%20Framework%20for%20Long%20Visual%20Document%20Understanding&entry.906535625=Dawei%20Zhu%20and%20Rui%20Meng%20and%20Jiefeng%20Chen%20and%20Sujian%20Li%20and%20Tomas%20Pfister%20and%20Jinsung%20Yoon&entry.1292438233=Comprehending%20long%20visual%20documents%2C%20where%20information%20is%20distributed%20across%20extensive%20pages%20of%20text%20and%20visual%20elements%2C%20is%20a%20critical%20but%20challenging%20task%20for%20modern%20Vision-Language%20Models%20%28VLMs%29.%20Existing%20approaches%20falter%20on%20a%20fundamental%20challenge%3A%20evidence%20localization.%20They%20struggle%20to%20retrieve%20relevant%20pages%20and%20overlook%20fine-grained%20details%20within%20visual%20elements%2C%20leading%20to%20limited%20performance%20and%20model%20hallucination.%20To%20address%20this%2C%20we%20propose%20DocLens%2C%20a%20tool-augmented%20multi-agent%20framework%20that%20effectively%20%60%60zooms%20in%27%27%20on%20evidence%20like%20a%20lens.%20It%20first%20navigates%20from%20the%20full%20document%20to%20specific%20visual%20elements%20on%20relevant%20pages%2C%20then%20employs%20a%20sampling-adjudication%20mechanism%20to%20generate%20a%20single%2C%20reliable%20answer.%20Paired%20with%20Gemini-2.5-Pro%2C%20DocLens%20achieves%20state-of-the-art%20performance%20on%20MMLongBench-Doc%20and%20FinRAGBench-V%2C%20surpassing%20even%20human%20experts.%20The%20framework%27s%20superiority%20is%20particularly%20evident%20on%20vision-centric%20and%20unanswerable%20queries%2C%20demonstrating%20the%20power%20of%20its%20enhanced%20localization%20capabilities.&entry.1838667208=http%3A//arxiv.org/abs/2511.11552v1&entry.124074799=Read"},
{"title": "From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in VLMs", "author": "Massimo Rizzoli and Simone Alghisi and Seyed Mahed Mousavi and Giuseppe Riccardi", "abstract": "Fine-tuning Vision-Language Models (VLMs) is a common strategy to improve performance following an ad-hoc data collection and annotation of real-world scenes. However, this process is often prone to biases, errors, and distribution imbalance, resulting in overfitting and imbalanced performance. Although a few studies have tried to address this problem by generating synthetic data, they lacked control over distribution bias and annotation quality. To address these challenges, we redesign the fine-tuning process in two ways. First, we control the generation of data and its annotations, ensuring it is free from bias, distribution imbalance, and annotation errors. We automatically construct the dataset by comprehensively sampling objects' attributes, including color, shape, size, and position within the scene. Secondly, using this annotated dataset, we fine-tune state-of-the-art VLMs and assess performance transferability to real-world data on the absolute position task. We conduct exhaustive evaluations on both synthetic and real-world benchmarks. Our experiments reveal two key findings: 1) fine-tuning on balanced synthetic data yields uniform performance across the visual scene and mitigates common biases; and 2) fine-tuning on synthetic stimuli significantly improves performance on real-world data (COCO), outperforming models fine-tuned in the matched setting.", "link": "http://arxiv.org/abs/2511.11440v1", "date": "2025-11-14", "relevancy": 3.0257, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6204}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Synthetic%20Scenes%20to%20Real%20Performance%3A%20Enhancing%20Spatial%20Reasoning%20in%20VLMs&body=Title%3A%20From%20Synthetic%20Scenes%20to%20Real%20Performance%3A%20Enhancing%20Spatial%20Reasoning%20in%20VLMs%0AAuthor%3A%20Massimo%20Rizzoli%20and%20Simone%20Alghisi%20and%20Seyed%20Mahed%20Mousavi%20and%20Giuseppe%20Riccardi%0AAbstract%3A%20Fine-tuning%20Vision-Language%20Models%20%28VLMs%29%20is%20a%20common%20strategy%20to%20improve%20performance%20following%20an%20ad-hoc%20data%20collection%20and%20annotation%20of%20real-world%20scenes.%20However%2C%20this%20process%20is%20often%20prone%20to%20biases%2C%20errors%2C%20and%20distribution%20imbalance%2C%20resulting%20in%20overfitting%20and%20imbalanced%20performance.%20Although%20a%20few%20studies%20have%20tried%20to%20address%20this%20problem%20by%20generating%20synthetic%20data%2C%20they%20lacked%20control%20over%20distribution%20bias%20and%20annotation%20quality.%20To%20address%20these%20challenges%2C%20we%20redesign%20the%20fine-tuning%20process%20in%20two%20ways.%20First%2C%20we%20control%20the%20generation%20of%20data%20and%20its%20annotations%2C%20ensuring%20it%20is%20free%20from%20bias%2C%20distribution%20imbalance%2C%20and%20annotation%20errors.%20We%20automatically%20construct%20the%20dataset%20by%20comprehensively%20sampling%20objects%27%20attributes%2C%20including%20color%2C%20shape%2C%20size%2C%20and%20position%20within%20the%20scene.%20Secondly%2C%20using%20this%20annotated%20dataset%2C%20we%20fine-tune%20state-of-the-art%20VLMs%20and%20assess%20performance%20transferability%20to%20real-world%20data%20on%20the%20absolute%20position%20task.%20We%20conduct%20exhaustive%20evaluations%20on%20both%20synthetic%20and%20real-world%20benchmarks.%20Our%20experiments%20reveal%20two%20key%20findings%3A%201%29%20fine-tuning%20on%20balanced%20synthetic%20data%20yields%20uniform%20performance%20across%20the%20visual%20scene%20and%20mitigates%20common%20biases%3B%20and%202%29%20fine-tuning%20on%20synthetic%20stimuli%20significantly%20improves%20performance%20on%20real-world%20data%20%28COCO%29%2C%20outperforming%20models%20fine-tuned%20in%20the%20matched%20setting.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Synthetic%2520Scenes%2520to%2520Real%2520Performance%253A%2520Enhancing%2520Spatial%2520Reasoning%2520in%2520VLMs%26entry.906535625%3DMassimo%2520Rizzoli%2520and%2520Simone%2520Alghisi%2520and%2520Seyed%2520Mahed%2520Mousavi%2520and%2520Giuseppe%2520Riccardi%26entry.1292438233%3DFine-tuning%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520is%2520a%2520common%2520strategy%2520to%2520improve%2520performance%2520following%2520an%2520ad-hoc%2520data%2520collection%2520and%2520annotation%2520of%2520real-world%2520scenes.%2520However%252C%2520this%2520process%2520is%2520often%2520prone%2520to%2520biases%252C%2520errors%252C%2520and%2520distribution%2520imbalance%252C%2520resulting%2520in%2520overfitting%2520and%2520imbalanced%2520performance.%2520Although%2520a%2520few%2520studies%2520have%2520tried%2520to%2520address%2520this%2520problem%2520by%2520generating%2520synthetic%2520data%252C%2520they%2520lacked%2520control%2520over%2520distribution%2520bias%2520and%2520annotation%2520quality.%2520To%2520address%2520these%2520challenges%252C%2520we%2520redesign%2520the%2520fine-tuning%2520process%2520in%2520two%2520ways.%2520First%252C%2520we%2520control%2520the%2520generation%2520of%2520data%2520and%2520its%2520annotations%252C%2520ensuring%2520it%2520is%2520free%2520from%2520bias%252C%2520distribution%2520imbalance%252C%2520and%2520annotation%2520errors.%2520We%2520automatically%2520construct%2520the%2520dataset%2520by%2520comprehensively%2520sampling%2520objects%2527%2520attributes%252C%2520including%2520color%252C%2520shape%252C%2520size%252C%2520and%2520position%2520within%2520the%2520scene.%2520Secondly%252C%2520using%2520this%2520annotated%2520dataset%252C%2520we%2520fine-tune%2520state-of-the-art%2520VLMs%2520and%2520assess%2520performance%2520transferability%2520to%2520real-world%2520data%2520on%2520the%2520absolute%2520position%2520task.%2520We%2520conduct%2520exhaustive%2520evaluations%2520on%2520both%2520synthetic%2520and%2520real-world%2520benchmarks.%2520Our%2520experiments%2520reveal%2520two%2520key%2520findings%253A%25201%2529%2520fine-tuning%2520on%2520balanced%2520synthetic%2520data%2520yields%2520uniform%2520performance%2520across%2520the%2520visual%2520scene%2520and%2520mitigates%2520common%2520biases%253B%2520and%25202%2529%2520fine-tuning%2520on%2520synthetic%2520stimuli%2520significantly%2520improves%2520performance%2520on%2520real-world%2520data%2520%2528COCO%2529%252C%2520outperforming%2520models%2520fine-tuned%2520in%2520the%2520matched%2520setting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Synthetic%20Scenes%20to%20Real%20Performance%3A%20Enhancing%20Spatial%20Reasoning%20in%20VLMs&entry.906535625=Massimo%20Rizzoli%20and%20Simone%20Alghisi%20and%20Seyed%20Mahed%20Mousavi%20and%20Giuseppe%20Riccardi&entry.1292438233=Fine-tuning%20Vision-Language%20Models%20%28VLMs%29%20is%20a%20common%20strategy%20to%20improve%20performance%20following%20an%20ad-hoc%20data%20collection%20and%20annotation%20of%20real-world%20scenes.%20However%2C%20this%20process%20is%20often%20prone%20to%20biases%2C%20errors%2C%20and%20distribution%20imbalance%2C%20resulting%20in%20overfitting%20and%20imbalanced%20performance.%20Although%20a%20few%20studies%20have%20tried%20to%20address%20this%20problem%20by%20generating%20synthetic%20data%2C%20they%20lacked%20control%20over%20distribution%20bias%20and%20annotation%20quality.%20To%20address%20these%20challenges%2C%20we%20redesign%20the%20fine-tuning%20process%20in%20two%20ways.%20First%2C%20we%20control%20the%20generation%20of%20data%20and%20its%20annotations%2C%20ensuring%20it%20is%20free%20from%20bias%2C%20distribution%20imbalance%2C%20and%20annotation%20errors.%20We%20automatically%20construct%20the%20dataset%20by%20comprehensively%20sampling%20objects%27%20attributes%2C%20including%20color%2C%20shape%2C%20size%2C%20and%20position%20within%20the%20scene.%20Secondly%2C%20using%20this%20annotated%20dataset%2C%20we%20fine-tune%20state-of-the-art%20VLMs%20and%20assess%20performance%20transferability%20to%20real-world%20data%20on%20the%20absolute%20position%20task.%20We%20conduct%20exhaustive%20evaluations%20on%20both%20synthetic%20and%20real-world%20benchmarks.%20Our%20experiments%20reveal%20two%20key%20findings%3A%201%29%20fine-tuning%20on%20balanced%20synthetic%20data%20yields%20uniform%20performance%20across%20the%20visual%20scene%20and%20mitigates%20common%20biases%3B%20and%202%29%20fine-tuning%20on%20synthetic%20stimuli%20significantly%20improves%20performance%20on%20real-world%20data%20%28COCO%29%2C%20outperforming%20models%20fine-tuned%20in%20the%20matched%20setting.&entry.1838667208=http%3A//arxiv.org/abs/2511.11440v1&entry.124074799=Read"},
{"title": "Unifying Segment Anything in Microscopy with Vision-Language Knowledge", "author": "Manyu Li and Ruian He and Zixian Zhang and Chenxi Ma and Weimin Tan and Bo Yan", "abstract": "Accurate segmentation of regions of interest in biomedical images holds substantial value in image analysis. Although several foundation models for biomedical segmentation have currently achieved excellent performance on certain datasets, they typically demonstrate sub-optimal performance on unseen domain data. We owe the deficiency to lack of vision-language knowledge before segmentation. Multimodal Large Language Models (MLLMs) bring outstanding understanding and reasoning capabilities to multimodal tasks, which inspires us to leverage MLLMs to inject Vision-Language Knowledge (VLK), thereby enabling vision models to demonstrate superior generalization capabilities on cross-domain datasets. In this paper, we propose a novel framework that seamlessly uses MLLMs to guide SAM in learning microscopy cross-domain data, unifying Segment Anything in Microscopy, named uLLSAM. Specifically, we propose the Vision-Language Semantic Alignment (VLSA) module, which injects VLK into Segment Anything Model (SAM). We find that after SAM receives global VLK prompts, its performance improves significantly, but there are deficiencies in boundary contour perception. Therefore, we further propose Semantic Boundary Regularization (SBR) to regularize SAM. Our method achieves performance improvements of 11.8% in SA across 9 in-domain microscopy datasets, achieving state-of-the-art performance. Our method also demonstrates improvements of 9.2% in SA across 10 out-of-domain datasets, exhibiting strong generalization capabilities. Code is available at https://github.com/ieellee/uLLSAM.", "link": "http://arxiv.org/abs/2505.10769v2", "date": "2025-11-14", "relevancy": 2.9869, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6047}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unifying%20Segment%20Anything%20in%20Microscopy%20with%20Vision-Language%20Knowledge&body=Title%3A%20Unifying%20Segment%20Anything%20in%20Microscopy%20with%20Vision-Language%20Knowledge%0AAuthor%3A%20Manyu%20Li%20and%20Ruian%20He%20and%20Zixian%20Zhang%20and%20Chenxi%20Ma%20and%20Weimin%20Tan%20and%20Bo%20Yan%0AAbstract%3A%20Accurate%20segmentation%20of%20regions%20of%20interest%20in%20biomedical%20images%20holds%20substantial%20value%20in%20image%20analysis.%20Although%20several%20foundation%20models%20for%20biomedical%20segmentation%20have%20currently%20achieved%20excellent%20performance%20on%20certain%20datasets%2C%20they%20typically%20demonstrate%20sub-optimal%20performance%20on%20unseen%20domain%20data.%20We%20owe%20the%20deficiency%20to%20lack%20of%20vision-language%20knowledge%20before%20segmentation.%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20bring%20outstanding%20understanding%20and%20reasoning%20capabilities%20to%20multimodal%20tasks%2C%20which%20inspires%20us%20to%20leverage%20MLLMs%20to%20inject%20Vision-Language%20Knowledge%20%28VLK%29%2C%20thereby%20enabling%20vision%20models%20to%20demonstrate%20superior%20generalization%20capabilities%20on%20cross-domain%20datasets.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20that%20seamlessly%20uses%20MLLMs%20to%20guide%20SAM%20in%20learning%20microscopy%20cross-domain%20data%2C%20unifying%20Segment%20Anything%20in%20Microscopy%2C%20named%20uLLSAM.%20Specifically%2C%20we%20propose%20the%20Vision-Language%20Semantic%20Alignment%20%28VLSA%29%20module%2C%20which%20injects%20VLK%20into%20Segment%20Anything%20Model%20%28SAM%29.%20We%20find%20that%20after%20SAM%20receives%20global%20VLK%20prompts%2C%20its%20performance%20improves%20significantly%2C%20but%20there%20are%20deficiencies%20in%20boundary%20contour%20perception.%20Therefore%2C%20we%20further%20propose%20Semantic%20Boundary%20Regularization%20%28SBR%29%20to%20regularize%20SAM.%20Our%20method%20achieves%20performance%20improvements%20of%2011.8%25%20in%20SA%20across%209%20in-domain%20microscopy%20datasets%2C%20achieving%20state-of-the-art%20performance.%20Our%20method%20also%20demonstrates%20improvements%20of%209.2%25%20in%20SA%20across%2010%20out-of-domain%20datasets%2C%20exhibiting%20strong%20generalization%20capabilities.%20Code%20is%20available%20at%20https%3A//github.com/ieellee/uLLSAM.%0ALink%3A%20http%3A//arxiv.org/abs/2505.10769v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnifying%2520Segment%2520Anything%2520in%2520Microscopy%2520with%2520Vision-Language%2520Knowledge%26entry.906535625%3DManyu%2520Li%2520and%2520Ruian%2520He%2520and%2520Zixian%2520Zhang%2520and%2520Chenxi%2520Ma%2520and%2520Weimin%2520Tan%2520and%2520Bo%2520Yan%26entry.1292438233%3DAccurate%2520segmentation%2520of%2520regions%2520of%2520interest%2520in%2520biomedical%2520images%2520holds%2520substantial%2520value%2520in%2520image%2520analysis.%2520Although%2520several%2520foundation%2520models%2520for%2520biomedical%2520segmentation%2520have%2520currently%2520achieved%2520excellent%2520performance%2520on%2520certain%2520datasets%252C%2520they%2520typically%2520demonstrate%2520sub-optimal%2520performance%2520on%2520unseen%2520domain%2520data.%2520We%2520owe%2520the%2520deficiency%2520to%2520lack%2520of%2520vision-language%2520knowledge%2520before%2520segmentation.%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520bring%2520outstanding%2520understanding%2520and%2520reasoning%2520capabilities%2520to%2520multimodal%2520tasks%252C%2520which%2520inspires%2520us%2520to%2520leverage%2520MLLMs%2520to%2520inject%2520Vision-Language%2520Knowledge%2520%2528VLK%2529%252C%2520thereby%2520enabling%2520vision%2520models%2520to%2520demonstrate%2520superior%2520generalization%2520capabilities%2520on%2520cross-domain%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%2520seamlessly%2520uses%2520MLLMs%2520to%2520guide%2520SAM%2520in%2520learning%2520microscopy%2520cross-domain%2520data%252C%2520unifying%2520Segment%2520Anything%2520in%2520Microscopy%252C%2520named%2520uLLSAM.%2520Specifically%252C%2520we%2520propose%2520the%2520Vision-Language%2520Semantic%2520Alignment%2520%2528VLSA%2529%2520module%252C%2520which%2520injects%2520VLK%2520into%2520Segment%2520Anything%2520Model%2520%2528SAM%2529.%2520We%2520find%2520that%2520after%2520SAM%2520receives%2520global%2520VLK%2520prompts%252C%2520its%2520performance%2520improves%2520significantly%252C%2520but%2520there%2520are%2520deficiencies%2520in%2520boundary%2520contour%2520perception.%2520Therefore%252C%2520we%2520further%2520propose%2520Semantic%2520Boundary%2520Regularization%2520%2528SBR%2529%2520to%2520regularize%2520SAM.%2520Our%2520method%2520achieves%2520performance%2520improvements%2520of%252011.8%2525%2520in%2520SA%2520across%25209%2520in-domain%2520microscopy%2520datasets%252C%2520achieving%2520state-of-the-art%2520performance.%2520Our%2520method%2520also%2520demonstrates%2520improvements%2520of%25209.2%2525%2520in%2520SA%2520across%252010%2520out-of-domain%2520datasets%252C%2520exhibiting%2520strong%2520generalization%2520capabilities.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/ieellee/uLLSAM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10769v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unifying%20Segment%20Anything%20in%20Microscopy%20with%20Vision-Language%20Knowledge&entry.906535625=Manyu%20Li%20and%20Ruian%20He%20and%20Zixian%20Zhang%20and%20Chenxi%20Ma%20and%20Weimin%20Tan%20and%20Bo%20Yan&entry.1292438233=Accurate%20segmentation%20of%20regions%20of%20interest%20in%20biomedical%20images%20holds%20substantial%20value%20in%20image%20analysis.%20Although%20several%20foundation%20models%20for%20biomedical%20segmentation%20have%20currently%20achieved%20excellent%20performance%20on%20certain%20datasets%2C%20they%20typically%20demonstrate%20sub-optimal%20performance%20on%20unseen%20domain%20data.%20We%20owe%20the%20deficiency%20to%20lack%20of%20vision-language%20knowledge%20before%20segmentation.%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20bring%20outstanding%20understanding%20and%20reasoning%20capabilities%20to%20multimodal%20tasks%2C%20which%20inspires%20us%20to%20leverage%20MLLMs%20to%20inject%20Vision-Language%20Knowledge%20%28VLK%29%2C%20thereby%20enabling%20vision%20models%20to%20demonstrate%20superior%20generalization%20capabilities%20on%20cross-domain%20datasets.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20that%20seamlessly%20uses%20MLLMs%20to%20guide%20SAM%20in%20learning%20microscopy%20cross-domain%20data%2C%20unifying%20Segment%20Anything%20in%20Microscopy%2C%20named%20uLLSAM.%20Specifically%2C%20we%20propose%20the%20Vision-Language%20Semantic%20Alignment%20%28VLSA%29%20module%2C%20which%20injects%20VLK%20into%20Segment%20Anything%20Model%20%28SAM%29.%20We%20find%20that%20after%20SAM%20receives%20global%20VLK%20prompts%2C%20its%20performance%20improves%20significantly%2C%20but%20there%20are%20deficiencies%20in%20boundary%20contour%20perception.%20Therefore%2C%20we%20further%20propose%20Semantic%20Boundary%20Regularization%20%28SBR%29%20to%20regularize%20SAM.%20Our%20method%20achieves%20performance%20improvements%20of%2011.8%25%20in%20SA%20across%209%20in-domain%20microscopy%20datasets%2C%20achieving%20state-of-the-art%20performance.%20Our%20method%20also%20demonstrates%20improvements%20of%209.2%25%20in%20SA%20across%2010%20out-of-domain%20datasets%2C%20exhibiting%20strong%20generalization%20capabilities.%20Code%20is%20available%20at%20https%3A//github.com/ieellee/uLLSAM.&entry.1838667208=http%3A//arxiv.org/abs/2505.10769v2&entry.124074799=Read"},
{"title": "Sat2RealCity: Geometry-Aware and Appearance-Controllable 3D Urban Generation from Satellite Imagery", "author": "Yijie Kang and Xinliang Wang and Zhenyu Wu and Yifeng Shi and Hailong Zhu", "abstract": "Recent advances in generative modeling have substantially enhanced 3D urban generation, enabling applications in digital twins, virtual cities, and large-scale simulations. However, existing methods face two key challenges: (1) the need for large-scale 3D city assets for supervised training, which are difficult and costly to obtain, and (2) reliance on semantic or height maps, which are used exclusively for generating buildings in virtual worlds and lack connection to real-world appearance, limiting the realism and generalizability of generated cities. To address these limitations, we propose Sat2RealCity, a geometry-aware and appearance-controllable framework for 3D urban generation from real-world satellite imagery. Unlike previous city-level generation methods, Sat2RealCity builds generation upon individual building entities, enabling the use of rich priors and pretrained knowledge from 3D object generation while substantially reducing dependence on large-scale 3D city assets. Specifically, (1) we introduce the OSM-based spatial priors strategy to achieve interpretable geometric generation from spatial topology to building instances; (2) we design an appearance-guided controllable modeling mechanism for fine-grained appearance realism and style control; and (3) we construct an MLLM-powered semantic-guided generation pipeline, bridging semantic interpretation and geometric reconstruction. Extensive quantitative and qualitative experiments demonstrate that Sat2RealCity significantly surpasses existing baselines in structural consistency and appearance realism, establishing a strong foundation for real-world aligned 3D urban content creation. The code will be released soon.", "link": "http://arxiv.org/abs/2511.11470v1", "date": "2025-11-14", "relevancy": 2.9638, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5947}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5947}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sat2RealCity%3A%20Geometry-Aware%20and%20Appearance-Controllable%203D%20Urban%20Generation%20from%20Satellite%20Imagery&body=Title%3A%20Sat2RealCity%3A%20Geometry-Aware%20and%20Appearance-Controllable%203D%20Urban%20Generation%20from%20Satellite%20Imagery%0AAuthor%3A%20Yijie%20Kang%20and%20Xinliang%20Wang%20and%20Zhenyu%20Wu%20and%20Yifeng%20Shi%20and%20Hailong%20Zhu%0AAbstract%3A%20Recent%20advances%20in%20generative%20modeling%20have%20substantially%20enhanced%203D%20urban%20generation%2C%20enabling%20applications%20in%20digital%20twins%2C%20virtual%20cities%2C%20and%20large-scale%20simulations.%20However%2C%20existing%20methods%20face%20two%20key%20challenges%3A%20%281%29%20the%20need%20for%20large-scale%203D%20city%20assets%20for%20supervised%20training%2C%20which%20are%20difficult%20and%20costly%20to%20obtain%2C%20and%20%282%29%20reliance%20on%20semantic%20or%20height%20maps%2C%20which%20are%20used%20exclusively%20for%20generating%20buildings%20in%20virtual%20worlds%20and%20lack%20connection%20to%20real-world%20appearance%2C%20limiting%20the%20realism%20and%20generalizability%20of%20generated%20cities.%20To%20address%20these%20limitations%2C%20we%20propose%20Sat2RealCity%2C%20a%20geometry-aware%20and%20appearance-controllable%20framework%20for%203D%20urban%20generation%20from%20real-world%20satellite%20imagery.%20Unlike%20previous%20city-level%20generation%20methods%2C%20Sat2RealCity%20builds%20generation%20upon%20individual%20building%20entities%2C%20enabling%20the%20use%20of%20rich%20priors%20and%20pretrained%20knowledge%20from%203D%20object%20generation%20while%20substantially%20reducing%20dependence%20on%20large-scale%203D%20city%20assets.%20Specifically%2C%20%281%29%20we%20introduce%20the%20OSM-based%20spatial%20priors%20strategy%20to%20achieve%20interpretable%20geometric%20generation%20from%20spatial%20topology%20to%20building%20instances%3B%20%282%29%20we%20design%20an%20appearance-guided%20controllable%20modeling%20mechanism%20for%20fine-grained%20appearance%20realism%20and%20style%20control%3B%20and%20%283%29%20we%20construct%20an%20MLLM-powered%20semantic-guided%20generation%20pipeline%2C%20bridging%20semantic%20interpretation%20and%20geometric%20reconstruction.%20Extensive%20quantitative%20and%20qualitative%20experiments%20demonstrate%20that%20Sat2RealCity%20significantly%20surpasses%20existing%20baselines%20in%20structural%20consistency%20and%20appearance%20realism%2C%20establishing%20a%20strong%20foundation%20for%20real-world%20aligned%203D%20urban%20content%20creation.%20The%20code%20will%20be%20released%20soon.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11470v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSat2RealCity%253A%2520Geometry-Aware%2520and%2520Appearance-Controllable%25203D%2520Urban%2520Generation%2520from%2520Satellite%2520Imagery%26entry.906535625%3DYijie%2520Kang%2520and%2520Xinliang%2520Wang%2520and%2520Zhenyu%2520Wu%2520and%2520Yifeng%2520Shi%2520and%2520Hailong%2520Zhu%26entry.1292438233%3DRecent%2520advances%2520in%2520generative%2520modeling%2520have%2520substantially%2520enhanced%25203D%2520urban%2520generation%252C%2520enabling%2520applications%2520in%2520digital%2520twins%252C%2520virtual%2520cities%252C%2520and%2520large-scale%2520simulations.%2520However%252C%2520existing%2520methods%2520face%2520two%2520key%2520challenges%253A%2520%25281%2529%2520the%2520need%2520for%2520large-scale%25203D%2520city%2520assets%2520for%2520supervised%2520training%252C%2520which%2520are%2520difficult%2520and%2520costly%2520to%2520obtain%252C%2520and%2520%25282%2529%2520reliance%2520on%2520semantic%2520or%2520height%2520maps%252C%2520which%2520are%2520used%2520exclusively%2520for%2520generating%2520buildings%2520in%2520virtual%2520worlds%2520and%2520lack%2520connection%2520to%2520real-world%2520appearance%252C%2520limiting%2520the%2520realism%2520and%2520generalizability%2520of%2520generated%2520cities.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Sat2RealCity%252C%2520a%2520geometry-aware%2520and%2520appearance-controllable%2520framework%2520for%25203D%2520urban%2520generation%2520from%2520real-world%2520satellite%2520imagery.%2520Unlike%2520previous%2520city-level%2520generation%2520methods%252C%2520Sat2RealCity%2520builds%2520generation%2520upon%2520individual%2520building%2520entities%252C%2520enabling%2520the%2520use%2520of%2520rich%2520priors%2520and%2520pretrained%2520knowledge%2520from%25203D%2520object%2520generation%2520while%2520substantially%2520reducing%2520dependence%2520on%2520large-scale%25203D%2520city%2520assets.%2520Specifically%252C%2520%25281%2529%2520we%2520introduce%2520the%2520OSM-based%2520spatial%2520priors%2520strategy%2520to%2520achieve%2520interpretable%2520geometric%2520generation%2520from%2520spatial%2520topology%2520to%2520building%2520instances%253B%2520%25282%2529%2520we%2520design%2520an%2520appearance-guided%2520controllable%2520modeling%2520mechanism%2520for%2520fine-grained%2520appearance%2520realism%2520and%2520style%2520control%253B%2520and%2520%25283%2529%2520we%2520construct%2520an%2520MLLM-powered%2520semantic-guided%2520generation%2520pipeline%252C%2520bridging%2520semantic%2520interpretation%2520and%2520geometric%2520reconstruction.%2520Extensive%2520quantitative%2520and%2520qualitative%2520experiments%2520demonstrate%2520that%2520Sat2RealCity%2520significantly%2520surpasses%2520existing%2520baselines%2520in%2520structural%2520consistency%2520and%2520appearance%2520realism%252C%2520establishing%2520a%2520strong%2520foundation%2520for%2520real-world%2520aligned%25203D%2520urban%2520content%2520creation.%2520The%2520code%2520will%2520be%2520released%2520soon.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11470v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sat2RealCity%3A%20Geometry-Aware%20and%20Appearance-Controllable%203D%20Urban%20Generation%20from%20Satellite%20Imagery&entry.906535625=Yijie%20Kang%20and%20Xinliang%20Wang%20and%20Zhenyu%20Wu%20and%20Yifeng%20Shi%20and%20Hailong%20Zhu&entry.1292438233=Recent%20advances%20in%20generative%20modeling%20have%20substantially%20enhanced%203D%20urban%20generation%2C%20enabling%20applications%20in%20digital%20twins%2C%20virtual%20cities%2C%20and%20large-scale%20simulations.%20However%2C%20existing%20methods%20face%20two%20key%20challenges%3A%20%281%29%20the%20need%20for%20large-scale%203D%20city%20assets%20for%20supervised%20training%2C%20which%20are%20difficult%20and%20costly%20to%20obtain%2C%20and%20%282%29%20reliance%20on%20semantic%20or%20height%20maps%2C%20which%20are%20used%20exclusively%20for%20generating%20buildings%20in%20virtual%20worlds%20and%20lack%20connection%20to%20real-world%20appearance%2C%20limiting%20the%20realism%20and%20generalizability%20of%20generated%20cities.%20To%20address%20these%20limitations%2C%20we%20propose%20Sat2RealCity%2C%20a%20geometry-aware%20and%20appearance-controllable%20framework%20for%203D%20urban%20generation%20from%20real-world%20satellite%20imagery.%20Unlike%20previous%20city-level%20generation%20methods%2C%20Sat2RealCity%20builds%20generation%20upon%20individual%20building%20entities%2C%20enabling%20the%20use%20of%20rich%20priors%20and%20pretrained%20knowledge%20from%203D%20object%20generation%20while%20substantially%20reducing%20dependence%20on%20large-scale%203D%20city%20assets.%20Specifically%2C%20%281%29%20we%20introduce%20the%20OSM-based%20spatial%20priors%20strategy%20to%20achieve%20interpretable%20geometric%20generation%20from%20spatial%20topology%20to%20building%20instances%3B%20%282%29%20we%20design%20an%20appearance-guided%20controllable%20modeling%20mechanism%20for%20fine-grained%20appearance%20realism%20and%20style%20control%3B%20and%20%283%29%20we%20construct%20an%20MLLM-powered%20semantic-guided%20generation%20pipeline%2C%20bridging%20semantic%20interpretation%20and%20geometric%20reconstruction.%20Extensive%20quantitative%20and%20qualitative%20experiments%20demonstrate%20that%20Sat2RealCity%20significantly%20surpasses%20existing%20baselines%20in%20structural%20consistency%20and%20appearance%20realism%2C%20establishing%20a%20strong%20foundation%20for%20real-world%20aligned%203D%20urban%20content%20creation.%20The%20code%20will%20be%20released%20soon.&entry.1838667208=http%3A//arxiv.org/abs/2511.11470v1&entry.124074799=Read"},
{"title": "DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding", "author": "Tanveer Hannan and Dimitrios Mallios and Parth Pathak and Faegheh Sardari and Thomas Seidl and Gedas Bertasius and Mohsen Fayyaz and Sunando Sengupta", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated strong multimodal reasoning capabilities on long and complex documents. However, their high memory footprint makes them impractical for deployment on resource-constrained edge devices. We present DocSLM, an efficient Small Vision-Language Model designed for long-document understanding under constrained memory resources. DocSLM incorporates a Hierarchical Multimodal Compressor that jointly encodes visual, textual, and layout information from each page into a fixed-length sequence, greatly reducing memory consumption while preserving both local and global semantics. To enable scalable processing over arbitrarily long inputs, we introduce a Streaming Abstention mechanism that operates on document segments sequentially and filters low-confidence responses using an entropy-based uncertainty calibrator. Across multiple long multimodal document benchmarks, DocSLM matches or surpasses state-of-the-art methods while using 82\\% fewer visual tokens, 75\\% fewer parameters, and 71\\% lower latency, delivering reliable multimodal document understanding on lightweight edge devices. Code is available in the supplementary material.", "link": "http://arxiv.org/abs/2511.11313v1", "date": "2025-11-14", "relevancy": 2.9223, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6001}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6001}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DocSLM%3A%20A%20Small%20Vision-Language%20Model%20for%20Long%20Multimodal%20Document%20Understanding&body=Title%3A%20DocSLM%3A%20A%20Small%20Vision-Language%20Model%20for%20Long%20Multimodal%20Document%20Understanding%0AAuthor%3A%20Tanveer%20Hannan%20and%20Dimitrios%20Mallios%20and%20Parth%20Pathak%20and%20Faegheh%20Sardari%20and%20Thomas%20Seidl%20and%20Gedas%20Bertasius%20and%20Mohsen%20Fayyaz%20and%20Sunando%20Sengupta%0AAbstract%3A%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20strong%20multimodal%20reasoning%20capabilities%20on%20long%20and%20complex%20documents.%20However%2C%20their%20high%20memory%20footprint%20makes%20them%20impractical%20for%20deployment%20on%20resource-constrained%20edge%20devices.%20We%20present%20DocSLM%2C%20an%20efficient%20Small%20Vision-Language%20Model%20designed%20for%20long-document%20understanding%20under%20constrained%20memory%20resources.%20DocSLM%20incorporates%20a%20Hierarchical%20Multimodal%20Compressor%20that%20jointly%20encodes%20visual%2C%20textual%2C%20and%20layout%20information%20from%20each%20page%20into%20a%20fixed-length%20sequence%2C%20greatly%20reducing%20memory%20consumption%20while%20preserving%20both%20local%20and%20global%20semantics.%20To%20enable%20scalable%20processing%20over%20arbitrarily%20long%20inputs%2C%20we%20introduce%20a%20Streaming%20Abstention%20mechanism%20that%20operates%20on%20document%20segments%20sequentially%20and%20filters%20low-confidence%20responses%20using%20an%20entropy-based%20uncertainty%20calibrator.%20Across%20multiple%20long%20multimodal%20document%20benchmarks%2C%20DocSLM%20matches%20or%20surpasses%20state-of-the-art%20methods%20while%20using%2082%5C%25%20fewer%20visual%20tokens%2C%2075%5C%25%20fewer%20parameters%2C%20and%2071%5C%25%20lower%20latency%2C%20delivering%20reliable%20multimodal%20document%20understanding%20on%20lightweight%20edge%20devices.%20Code%20is%20available%20in%20the%20supplementary%20material.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocSLM%253A%2520A%2520Small%2520Vision-Language%2520Model%2520for%2520Long%2520Multimodal%2520Document%2520Understanding%26entry.906535625%3DTanveer%2520Hannan%2520and%2520Dimitrios%2520Mallios%2520and%2520Parth%2520Pathak%2520and%2520Faegheh%2520Sardari%2520and%2520Thomas%2520Seidl%2520and%2520Gedas%2520Bertasius%2520and%2520Mohsen%2520Fayyaz%2520and%2520Sunando%2520Sengupta%26entry.1292438233%3DLarge%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520strong%2520multimodal%2520reasoning%2520capabilities%2520on%2520long%2520and%2520complex%2520documents.%2520However%252C%2520their%2520high%2520memory%2520footprint%2520makes%2520them%2520impractical%2520for%2520deployment%2520on%2520resource-constrained%2520edge%2520devices.%2520We%2520present%2520DocSLM%252C%2520an%2520efficient%2520Small%2520Vision-Language%2520Model%2520designed%2520for%2520long-document%2520understanding%2520under%2520constrained%2520memory%2520resources.%2520DocSLM%2520incorporates%2520a%2520Hierarchical%2520Multimodal%2520Compressor%2520that%2520jointly%2520encodes%2520visual%252C%2520textual%252C%2520and%2520layout%2520information%2520from%2520each%2520page%2520into%2520a%2520fixed-length%2520sequence%252C%2520greatly%2520reducing%2520memory%2520consumption%2520while%2520preserving%2520both%2520local%2520and%2520global%2520semantics.%2520To%2520enable%2520scalable%2520processing%2520over%2520arbitrarily%2520long%2520inputs%252C%2520we%2520introduce%2520a%2520Streaming%2520Abstention%2520mechanism%2520that%2520operates%2520on%2520document%2520segments%2520sequentially%2520and%2520filters%2520low-confidence%2520responses%2520using%2520an%2520entropy-based%2520uncertainty%2520calibrator.%2520Across%2520multiple%2520long%2520multimodal%2520document%2520benchmarks%252C%2520DocSLM%2520matches%2520or%2520surpasses%2520state-of-the-art%2520methods%2520while%2520using%252082%255C%2525%2520fewer%2520visual%2520tokens%252C%252075%255C%2525%2520fewer%2520parameters%252C%2520and%252071%255C%2525%2520lower%2520latency%252C%2520delivering%2520reliable%2520multimodal%2520document%2520understanding%2520on%2520lightweight%2520edge%2520devices.%2520Code%2520is%2520available%2520in%2520the%2520supplementary%2520material.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DocSLM%3A%20A%20Small%20Vision-Language%20Model%20for%20Long%20Multimodal%20Document%20Understanding&entry.906535625=Tanveer%20Hannan%20and%20Dimitrios%20Mallios%20and%20Parth%20Pathak%20and%20Faegheh%20Sardari%20and%20Thomas%20Seidl%20and%20Gedas%20Bertasius%20and%20Mohsen%20Fayyaz%20and%20Sunando%20Sengupta&entry.1292438233=Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20strong%20multimodal%20reasoning%20capabilities%20on%20long%20and%20complex%20documents.%20However%2C%20their%20high%20memory%20footprint%20makes%20them%20impractical%20for%20deployment%20on%20resource-constrained%20edge%20devices.%20We%20present%20DocSLM%2C%20an%20efficient%20Small%20Vision-Language%20Model%20designed%20for%20long-document%20understanding%20under%20constrained%20memory%20resources.%20DocSLM%20incorporates%20a%20Hierarchical%20Multimodal%20Compressor%20that%20jointly%20encodes%20visual%2C%20textual%2C%20and%20layout%20information%20from%20each%20page%20into%20a%20fixed-length%20sequence%2C%20greatly%20reducing%20memory%20consumption%20while%20preserving%20both%20local%20and%20global%20semantics.%20To%20enable%20scalable%20processing%20over%20arbitrarily%20long%20inputs%2C%20we%20introduce%20a%20Streaming%20Abstention%20mechanism%20that%20operates%20on%20document%20segments%20sequentially%20and%20filters%20low-confidence%20responses%20using%20an%20entropy-based%20uncertainty%20calibrator.%20Across%20multiple%20long%20multimodal%20document%20benchmarks%2C%20DocSLM%20matches%20or%20surpasses%20state-of-the-art%20methods%20while%20using%2082%5C%25%20fewer%20visual%20tokens%2C%2075%5C%25%20fewer%20parameters%2C%20and%2071%5C%25%20lower%20latency%2C%20delivering%20reliable%20multimodal%20document%20understanding%20on%20lightweight%20edge%20devices.%20Code%20is%20available%20in%20the%20supplementary%20material.&entry.1838667208=http%3A//arxiv.org/abs/2511.11313v1&entry.124074799=Read"},
{"title": "Discovering Meaningful Units with Visually Grounded Semantics from Image Captions", "author": "Melika Behjati and James Henderson", "abstract": "Fine-grained knowledge is crucial for vision-language models to obtain a better understanding of the real world. While there has been work trying to acquire this kind of knowledge in the space of vision and language, it has mostly focused on aligning the image patches with the tokens on the language side. However, image patches do not have any meaning to the human eye, and individual tokens do not necessarily carry groundable information in the image. It is groups of tokens which describe different aspects of the scene. In this work, we propose a model which groups the caption tokens as part of its architecture in order to capture a fine-grained representation of the language. We expect our representations to be at the level of objects present in the image, and therefore align our representations with the output of an image encoder trained to discover objects. We show that by learning to group the tokens, the vision-language model has a better fine-grained understanding of vision and language. In addition, the token groups that our model discovers are highly similar to groundable phrases in text, both qualitatively and quantitatively.", "link": "http://arxiv.org/abs/2511.11262v1", "date": "2025-11-14", "relevancy": 2.9033, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6073}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6073}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovering%20Meaningful%20Units%20with%20Visually%20Grounded%20Semantics%20from%20Image%20Captions&body=Title%3A%20Discovering%20Meaningful%20Units%20with%20Visually%20Grounded%20Semantics%20from%20Image%20Captions%0AAuthor%3A%20Melika%20Behjati%20and%20James%20Henderson%0AAbstract%3A%20Fine-grained%20knowledge%20is%20crucial%20for%20vision-language%20models%20to%20obtain%20a%20better%20understanding%20of%20the%20real%20world.%20While%20there%20has%20been%20work%20trying%20to%20acquire%20this%20kind%20of%20knowledge%20in%20the%20space%20of%20vision%20and%20language%2C%20it%20has%20mostly%20focused%20on%20aligning%20the%20image%20patches%20with%20the%20tokens%20on%20the%20language%20side.%20However%2C%20image%20patches%20do%20not%20have%20any%20meaning%20to%20the%20human%20eye%2C%20and%20individual%20tokens%20do%20not%20necessarily%20carry%20groundable%20information%20in%20the%20image.%20It%20is%20groups%20of%20tokens%20which%20describe%20different%20aspects%20of%20the%20scene.%20In%20this%20work%2C%20we%20propose%20a%20model%20which%20groups%20the%20caption%20tokens%20as%20part%20of%20its%20architecture%20in%20order%20to%20capture%20a%20fine-grained%20representation%20of%20the%20language.%20We%20expect%20our%20representations%20to%20be%20at%20the%20level%20of%20objects%20present%20in%20the%20image%2C%20and%20therefore%20align%20our%20representations%20with%20the%20output%20of%20an%20image%20encoder%20trained%20to%20discover%20objects.%20We%20show%20that%20by%20learning%20to%20group%20the%20tokens%2C%20the%20vision-language%20model%20has%20a%20better%20fine-grained%20understanding%20of%20vision%20and%20language.%20In%20addition%2C%20the%20token%20groups%20that%20our%20model%20discovers%20are%20highly%20similar%20to%20groundable%20phrases%20in%20text%2C%20both%20qualitatively%20and%20quantitatively.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovering%2520Meaningful%2520Units%2520with%2520Visually%2520Grounded%2520Semantics%2520from%2520Image%2520Captions%26entry.906535625%3DMelika%2520Behjati%2520and%2520James%2520Henderson%26entry.1292438233%3DFine-grained%2520knowledge%2520is%2520crucial%2520for%2520vision-language%2520models%2520to%2520obtain%2520a%2520better%2520understanding%2520of%2520the%2520real%2520world.%2520While%2520there%2520has%2520been%2520work%2520trying%2520to%2520acquire%2520this%2520kind%2520of%2520knowledge%2520in%2520the%2520space%2520of%2520vision%2520and%2520language%252C%2520it%2520has%2520mostly%2520focused%2520on%2520aligning%2520the%2520image%2520patches%2520with%2520the%2520tokens%2520on%2520the%2520language%2520side.%2520However%252C%2520image%2520patches%2520do%2520not%2520have%2520any%2520meaning%2520to%2520the%2520human%2520eye%252C%2520and%2520individual%2520tokens%2520do%2520not%2520necessarily%2520carry%2520groundable%2520information%2520in%2520the%2520image.%2520It%2520is%2520groups%2520of%2520tokens%2520which%2520describe%2520different%2520aspects%2520of%2520the%2520scene.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520model%2520which%2520groups%2520the%2520caption%2520tokens%2520as%2520part%2520of%2520its%2520architecture%2520in%2520order%2520to%2520capture%2520a%2520fine-grained%2520representation%2520of%2520the%2520language.%2520We%2520expect%2520our%2520representations%2520to%2520be%2520at%2520the%2520level%2520of%2520objects%2520present%2520in%2520the%2520image%252C%2520and%2520therefore%2520align%2520our%2520representations%2520with%2520the%2520output%2520of%2520an%2520image%2520encoder%2520trained%2520to%2520discover%2520objects.%2520We%2520show%2520that%2520by%2520learning%2520to%2520group%2520the%2520tokens%252C%2520the%2520vision-language%2520model%2520has%2520a%2520better%2520fine-grained%2520understanding%2520of%2520vision%2520and%2520language.%2520In%2520addition%252C%2520the%2520token%2520groups%2520that%2520our%2520model%2520discovers%2520are%2520highly%2520similar%2520to%2520groundable%2520phrases%2520in%2520text%252C%2520both%2520qualitatively%2520and%2520quantitatively.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20Meaningful%20Units%20with%20Visually%20Grounded%20Semantics%20from%20Image%20Captions&entry.906535625=Melika%20Behjati%20and%20James%20Henderson&entry.1292438233=Fine-grained%20knowledge%20is%20crucial%20for%20vision-language%20models%20to%20obtain%20a%20better%20understanding%20of%20the%20real%20world.%20While%20there%20has%20been%20work%20trying%20to%20acquire%20this%20kind%20of%20knowledge%20in%20the%20space%20of%20vision%20and%20language%2C%20it%20has%20mostly%20focused%20on%20aligning%20the%20image%20patches%20with%20the%20tokens%20on%20the%20language%20side.%20However%2C%20image%20patches%20do%20not%20have%20any%20meaning%20to%20the%20human%20eye%2C%20and%20individual%20tokens%20do%20not%20necessarily%20carry%20groundable%20information%20in%20the%20image.%20It%20is%20groups%20of%20tokens%20which%20describe%20different%20aspects%20of%20the%20scene.%20In%20this%20work%2C%20we%20propose%20a%20model%20which%20groups%20the%20caption%20tokens%20as%20part%20of%20its%20architecture%20in%20order%20to%20capture%20a%20fine-grained%20representation%20of%20the%20language.%20We%20expect%20our%20representations%20to%20be%20at%20the%20level%20of%20objects%20present%20in%20the%20image%2C%20and%20therefore%20align%20our%20representations%20with%20the%20output%20of%20an%20image%20encoder%20trained%20to%20discover%20objects.%20We%20show%20that%20by%20learning%20to%20group%20the%20tokens%2C%20the%20vision-language%20model%20has%20a%20better%20fine-grained%20understanding%20of%20vision%20and%20language.%20In%20addition%2C%20the%20token%20groups%20that%20our%20model%20discovers%20are%20highly%20similar%20to%20groundable%20phrases%20in%20text%2C%20both%20qualitatively%20and%20quantitatively.&entry.1838667208=http%3A//arxiv.org/abs/2511.11262v1&entry.124074799=Read"},
{"title": "LARM: A Large Articulated-Object Reconstruction Model", "author": "Sylvia Yuan and Ruoxi Shi and Xinyue Wei and Xiaoshuai Zhang and Hao Su and Minghua Liu", "abstract": "Modeling 3D articulated objects with realistic geometry, textures, and kinematics is essential for a wide range of applications. However, existing optimization-based reconstruction methods often require dense multi-view inputs and expensive per-instance optimization, limiting their scalability. Recent feedforward approaches offer faster alternatives but frequently produce coarse geometry, lack texture reconstruction, and rely on brittle, complex multi-stage pipelines. We introduce LARM, a unified feedforward framework that reconstructs 3D articulated objects from sparse-view images by jointly recovering detailed geometry, realistic textures, and accurate joint structures. LARM extends LVSM a recent novel view synthesis (NVS) approach for static 3D objects into the articulated setting by jointly reasoning over camera pose and articulation variation using a transformer-based architecture, enabling scalable and accurate novel view synthesis. In addition, LARM generates auxiliary outputs such as depth maps and part masks to facilitate explicit 3D mesh extraction and joint estimation. Our pipeline eliminates the need for dense supervision and supports high-fidelity reconstruction across diverse object categories. Extensive experiments demonstrate that LARM outperforms state-of-the-art methods in both novel view and state synthesis as well as 3D articulated object reconstruction, generating high-quality meshes that closely adhere to the input images. project page: https://sylviayuan-sy.github.io/larm-site/", "link": "http://arxiv.org/abs/2511.11563v1", "date": "2025-11-14", "relevancy": 2.8597, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5833}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5697}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LARM%3A%20A%20Large%20Articulated-Object%20Reconstruction%20Model&body=Title%3A%20LARM%3A%20A%20Large%20Articulated-Object%20Reconstruction%20Model%0AAuthor%3A%20Sylvia%20Yuan%20and%20Ruoxi%20Shi%20and%20Xinyue%20Wei%20and%20Xiaoshuai%20Zhang%20and%20Hao%20Su%20and%20Minghua%20Liu%0AAbstract%3A%20Modeling%203D%20articulated%20objects%20with%20realistic%20geometry%2C%20textures%2C%20and%20kinematics%20is%20essential%20for%20a%20wide%20range%20of%20applications.%20However%2C%20existing%20optimization-based%20reconstruction%20methods%20often%20require%20dense%20multi-view%20inputs%20and%20expensive%20per-instance%20optimization%2C%20limiting%20their%20scalability.%20Recent%20feedforward%20approaches%20offer%20faster%20alternatives%20but%20frequently%20produce%20coarse%20geometry%2C%20lack%20texture%20reconstruction%2C%20and%20rely%20on%20brittle%2C%20complex%20multi-stage%20pipelines.%20We%20introduce%20LARM%2C%20a%20unified%20feedforward%20framework%20that%20reconstructs%203D%20articulated%20objects%20from%20sparse-view%20images%20by%20jointly%20recovering%20detailed%20geometry%2C%20realistic%20textures%2C%20and%20accurate%20joint%20structures.%20LARM%20extends%20LVSM%20a%20recent%20novel%20view%20synthesis%20%28NVS%29%20approach%20for%20static%203D%20objects%20into%20the%20articulated%20setting%20by%20jointly%20reasoning%20over%20camera%20pose%20and%20articulation%20variation%20using%20a%20transformer-based%20architecture%2C%20enabling%20scalable%20and%20accurate%20novel%20view%20synthesis.%20In%20addition%2C%20LARM%20generates%20auxiliary%20outputs%20such%20as%20depth%20maps%20and%20part%20masks%20to%20facilitate%20explicit%203D%20mesh%20extraction%20and%20joint%20estimation.%20Our%20pipeline%20eliminates%20the%20need%20for%20dense%20supervision%20and%20supports%20high-fidelity%20reconstruction%20across%20diverse%20object%20categories.%20Extensive%20experiments%20demonstrate%20that%20LARM%20outperforms%20state-of-the-art%20methods%20in%20both%20novel%20view%20and%20state%20synthesis%20as%20well%20as%203D%20articulated%20object%20reconstruction%2C%20generating%20high-quality%20meshes%20that%20closely%20adhere%20to%20the%20input%20images.%20project%20page%3A%20https%3A//sylviayuan-sy.github.io/larm-site/%0ALink%3A%20http%3A//arxiv.org/abs/2511.11563v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLARM%253A%2520A%2520Large%2520Articulated-Object%2520Reconstruction%2520Model%26entry.906535625%3DSylvia%2520Yuan%2520and%2520Ruoxi%2520Shi%2520and%2520Xinyue%2520Wei%2520and%2520Xiaoshuai%2520Zhang%2520and%2520Hao%2520Su%2520and%2520Minghua%2520Liu%26entry.1292438233%3DModeling%25203D%2520articulated%2520objects%2520with%2520realistic%2520geometry%252C%2520textures%252C%2520and%2520kinematics%2520is%2520essential%2520for%2520a%2520wide%2520range%2520of%2520applications.%2520However%252C%2520existing%2520optimization-based%2520reconstruction%2520methods%2520often%2520require%2520dense%2520multi-view%2520inputs%2520and%2520expensive%2520per-instance%2520optimization%252C%2520limiting%2520their%2520scalability.%2520Recent%2520feedforward%2520approaches%2520offer%2520faster%2520alternatives%2520but%2520frequently%2520produce%2520coarse%2520geometry%252C%2520lack%2520texture%2520reconstruction%252C%2520and%2520rely%2520on%2520brittle%252C%2520complex%2520multi-stage%2520pipelines.%2520We%2520introduce%2520LARM%252C%2520a%2520unified%2520feedforward%2520framework%2520that%2520reconstructs%25203D%2520articulated%2520objects%2520from%2520sparse-view%2520images%2520by%2520jointly%2520recovering%2520detailed%2520geometry%252C%2520realistic%2520textures%252C%2520and%2520accurate%2520joint%2520structures.%2520LARM%2520extends%2520LVSM%2520a%2520recent%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520approach%2520for%2520static%25203D%2520objects%2520into%2520the%2520articulated%2520setting%2520by%2520jointly%2520reasoning%2520over%2520camera%2520pose%2520and%2520articulation%2520variation%2520using%2520a%2520transformer-based%2520architecture%252C%2520enabling%2520scalable%2520and%2520accurate%2520novel%2520view%2520synthesis.%2520In%2520addition%252C%2520LARM%2520generates%2520auxiliary%2520outputs%2520such%2520as%2520depth%2520maps%2520and%2520part%2520masks%2520to%2520facilitate%2520explicit%25203D%2520mesh%2520extraction%2520and%2520joint%2520estimation.%2520Our%2520pipeline%2520eliminates%2520the%2520need%2520for%2520dense%2520supervision%2520and%2520supports%2520high-fidelity%2520reconstruction%2520across%2520diverse%2520object%2520categories.%2520Extensive%2520experiments%2520demonstrate%2520that%2520LARM%2520outperforms%2520state-of-the-art%2520methods%2520in%2520both%2520novel%2520view%2520and%2520state%2520synthesis%2520as%2520well%2520as%25203D%2520articulated%2520object%2520reconstruction%252C%2520generating%2520high-quality%2520meshes%2520that%2520closely%2520adhere%2520to%2520the%2520input%2520images.%2520project%2520page%253A%2520https%253A//sylviayuan-sy.github.io/larm-site/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11563v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LARM%3A%20A%20Large%20Articulated-Object%20Reconstruction%20Model&entry.906535625=Sylvia%20Yuan%20and%20Ruoxi%20Shi%20and%20Xinyue%20Wei%20and%20Xiaoshuai%20Zhang%20and%20Hao%20Su%20and%20Minghua%20Liu&entry.1292438233=Modeling%203D%20articulated%20objects%20with%20realistic%20geometry%2C%20textures%2C%20and%20kinematics%20is%20essential%20for%20a%20wide%20range%20of%20applications.%20However%2C%20existing%20optimization-based%20reconstruction%20methods%20often%20require%20dense%20multi-view%20inputs%20and%20expensive%20per-instance%20optimization%2C%20limiting%20their%20scalability.%20Recent%20feedforward%20approaches%20offer%20faster%20alternatives%20but%20frequently%20produce%20coarse%20geometry%2C%20lack%20texture%20reconstruction%2C%20and%20rely%20on%20brittle%2C%20complex%20multi-stage%20pipelines.%20We%20introduce%20LARM%2C%20a%20unified%20feedforward%20framework%20that%20reconstructs%203D%20articulated%20objects%20from%20sparse-view%20images%20by%20jointly%20recovering%20detailed%20geometry%2C%20realistic%20textures%2C%20and%20accurate%20joint%20structures.%20LARM%20extends%20LVSM%20a%20recent%20novel%20view%20synthesis%20%28NVS%29%20approach%20for%20static%203D%20objects%20into%20the%20articulated%20setting%20by%20jointly%20reasoning%20over%20camera%20pose%20and%20articulation%20variation%20using%20a%20transformer-based%20architecture%2C%20enabling%20scalable%20and%20accurate%20novel%20view%20synthesis.%20In%20addition%2C%20LARM%20generates%20auxiliary%20outputs%20such%20as%20depth%20maps%20and%20part%20masks%20to%20facilitate%20explicit%203D%20mesh%20extraction%20and%20joint%20estimation.%20Our%20pipeline%20eliminates%20the%20need%20for%20dense%20supervision%20and%20supports%20high-fidelity%20reconstruction%20across%20diverse%20object%20categories.%20Extensive%20experiments%20demonstrate%20that%20LARM%20outperforms%20state-of-the-art%20methods%20in%20both%20novel%20view%20and%20state%20synthesis%20as%20well%20as%203D%20articulated%20object%20reconstruction%2C%20generating%20high-quality%20meshes%20that%20closely%20adhere%20to%20the%20input%20images.%20project%20page%3A%20https%3A//sylviayuan-sy.github.io/larm-site/&entry.1838667208=http%3A//arxiv.org/abs/2511.11563v1&entry.124074799=Read"},
{"title": "Comprehension of Multilingual Expressions Referring to Target Objects in Visual Inputs", "author": "Francisco Nogueira and Alexandre Bernardino and Bruno Martins", "abstract": "Referring Expression Comprehension (REC) requires models to localize objects in images based on natural language descriptions. Research on the area remains predominantly English-centric, despite increasing global deployment demands. This work addresses multilingual REC through two main contributions. First, we construct a unified multilingual dataset spanning 10 languages, by systematically expanding 12 existing English REC benchmarks through machine translation and context-based translation enhancement. The resulting dataset comprises approximately 8 million multilingual referring expressions across 177,620 images, with 336,882 annotated objects. Second, we introduce an attention-anchored neural architecture that uses multilingual SigLIP2 encoders. Our attention-based approach generates coarse spatial anchors from attention distributions, which are subsequently refined through learned residuals. Experimental evaluation demonstrates competitive performance on standard benchmarks, e.g. achieving 86.9% accuracy at IoU@50 on RefCOCO aggregate multilingual evaluation, compared to an English-only result of 91.3%. Multilingual evaluation shows consistent capabilities across languages, establishing the practical feasibility of multilingual visual grounding systems. The dataset and model are available at $\\href{https://multilingual.franreno.com}{multilingual.franreno.com}$.", "link": "http://arxiv.org/abs/2511.11427v1", "date": "2025-11-14", "relevancy": 2.8548, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.597}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.597}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comprehension%20of%20Multilingual%20Expressions%20Referring%20to%20Target%20Objects%20in%20Visual%20Inputs&body=Title%3A%20Comprehension%20of%20Multilingual%20Expressions%20Referring%20to%20Target%20Objects%20in%20Visual%20Inputs%0AAuthor%3A%20Francisco%20Nogueira%20and%20Alexandre%20Bernardino%20and%20Bruno%20Martins%0AAbstract%3A%20Referring%20Expression%20Comprehension%20%28REC%29%20requires%20models%20to%20localize%20objects%20in%20images%20based%20on%20natural%20language%20descriptions.%20Research%20on%20the%20area%20remains%20predominantly%20English-centric%2C%20despite%20increasing%20global%20deployment%20demands.%20This%20work%20addresses%20multilingual%20REC%20through%20two%20main%20contributions.%20First%2C%20we%20construct%20a%20unified%20multilingual%20dataset%20spanning%2010%20languages%2C%20by%20systematically%20expanding%2012%20existing%20English%20REC%20benchmarks%20through%20machine%20translation%20and%20context-based%20translation%20enhancement.%20The%20resulting%20dataset%20comprises%20approximately%208%20million%20multilingual%20referring%20expressions%20across%20177%2C620%20images%2C%20with%20336%2C882%20annotated%20objects.%20Second%2C%20we%20introduce%20an%20attention-anchored%20neural%20architecture%20that%20uses%20multilingual%20SigLIP2%20encoders.%20Our%20attention-based%20approach%20generates%20coarse%20spatial%20anchors%20from%20attention%20distributions%2C%20which%20are%20subsequently%20refined%20through%20learned%20residuals.%20Experimental%20evaluation%20demonstrates%20competitive%20performance%20on%20standard%20benchmarks%2C%20e.g.%20achieving%2086.9%25%20accuracy%20at%20IoU%4050%20on%20RefCOCO%20aggregate%20multilingual%20evaluation%2C%20compared%20to%20an%20English-only%20result%20of%2091.3%25.%20Multilingual%20evaluation%20shows%20consistent%20capabilities%20across%20languages%2C%20establishing%20the%20practical%20feasibility%20of%20multilingual%20visual%20grounding%20systems.%20The%20dataset%20and%20model%20are%20available%20at%20%24%5Chref%7Bhttps%3A//multilingual.franreno.com%7D%7Bmultilingual.franreno.com%7D%24.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComprehension%2520of%2520Multilingual%2520Expressions%2520Referring%2520to%2520Target%2520Objects%2520in%2520Visual%2520Inputs%26entry.906535625%3DFrancisco%2520Nogueira%2520and%2520Alexandre%2520Bernardino%2520and%2520Bruno%2520Martins%26entry.1292438233%3DReferring%2520Expression%2520Comprehension%2520%2528REC%2529%2520requires%2520models%2520to%2520localize%2520objects%2520in%2520images%2520based%2520on%2520natural%2520language%2520descriptions.%2520Research%2520on%2520the%2520area%2520remains%2520predominantly%2520English-centric%252C%2520despite%2520increasing%2520global%2520deployment%2520demands.%2520This%2520work%2520addresses%2520multilingual%2520REC%2520through%2520two%2520main%2520contributions.%2520First%252C%2520we%2520construct%2520a%2520unified%2520multilingual%2520dataset%2520spanning%252010%2520languages%252C%2520by%2520systematically%2520expanding%252012%2520existing%2520English%2520REC%2520benchmarks%2520through%2520machine%2520translation%2520and%2520context-based%2520translation%2520enhancement.%2520The%2520resulting%2520dataset%2520comprises%2520approximately%25208%2520million%2520multilingual%2520referring%2520expressions%2520across%2520177%252C620%2520images%252C%2520with%2520336%252C882%2520annotated%2520objects.%2520Second%252C%2520we%2520introduce%2520an%2520attention-anchored%2520neural%2520architecture%2520that%2520uses%2520multilingual%2520SigLIP2%2520encoders.%2520Our%2520attention-based%2520approach%2520generates%2520coarse%2520spatial%2520anchors%2520from%2520attention%2520distributions%252C%2520which%2520are%2520subsequently%2520refined%2520through%2520learned%2520residuals.%2520Experimental%2520evaluation%2520demonstrates%2520competitive%2520performance%2520on%2520standard%2520benchmarks%252C%2520e.g.%2520achieving%252086.9%2525%2520accuracy%2520at%2520IoU%254050%2520on%2520RefCOCO%2520aggregate%2520multilingual%2520evaluation%252C%2520compared%2520to%2520an%2520English-only%2520result%2520of%252091.3%2525.%2520Multilingual%2520evaluation%2520shows%2520consistent%2520capabilities%2520across%2520languages%252C%2520establishing%2520the%2520practical%2520feasibility%2520of%2520multilingual%2520visual%2520grounding%2520systems.%2520The%2520dataset%2520and%2520model%2520are%2520available%2520at%2520%2524%255Chref%257Bhttps%253A//multilingual.franreno.com%257D%257Bmultilingual.franreno.com%257D%2524.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comprehension%20of%20Multilingual%20Expressions%20Referring%20to%20Target%20Objects%20in%20Visual%20Inputs&entry.906535625=Francisco%20Nogueira%20and%20Alexandre%20Bernardino%20and%20Bruno%20Martins&entry.1292438233=Referring%20Expression%20Comprehension%20%28REC%29%20requires%20models%20to%20localize%20objects%20in%20images%20based%20on%20natural%20language%20descriptions.%20Research%20on%20the%20area%20remains%20predominantly%20English-centric%2C%20despite%20increasing%20global%20deployment%20demands.%20This%20work%20addresses%20multilingual%20REC%20through%20two%20main%20contributions.%20First%2C%20we%20construct%20a%20unified%20multilingual%20dataset%20spanning%2010%20languages%2C%20by%20systematically%20expanding%2012%20existing%20English%20REC%20benchmarks%20through%20machine%20translation%20and%20context-based%20translation%20enhancement.%20The%20resulting%20dataset%20comprises%20approximately%208%20million%20multilingual%20referring%20expressions%20across%20177%2C620%20images%2C%20with%20336%2C882%20annotated%20objects.%20Second%2C%20we%20introduce%20an%20attention-anchored%20neural%20architecture%20that%20uses%20multilingual%20SigLIP2%20encoders.%20Our%20attention-based%20approach%20generates%20coarse%20spatial%20anchors%20from%20attention%20distributions%2C%20which%20are%20subsequently%20refined%20through%20learned%20residuals.%20Experimental%20evaluation%20demonstrates%20competitive%20performance%20on%20standard%20benchmarks%2C%20e.g.%20achieving%2086.9%25%20accuracy%20at%20IoU%4050%20on%20RefCOCO%20aggregate%20multilingual%20evaluation%2C%20compared%20to%20an%20English-only%20result%20of%2091.3%25.%20Multilingual%20evaluation%20shows%20consistent%20capabilities%20across%20languages%2C%20establishing%20the%20practical%20feasibility%20of%20multilingual%20visual%20grounding%20systems.%20The%20dataset%20and%20model%20are%20available%20at%20%24%5Chref%7Bhttps%3A//multilingual.franreno.com%7D%7Bmultilingual.franreno.com%7D%24.&entry.1838667208=http%3A//arxiv.org/abs/2511.11427v1&entry.124074799=Read"},
{"title": "Enhancing Video Inpainting with Aligned Frame Interval Guidance", "author": "Ming Xie and Junqiu Yu and Qiaole Dong and Xiangyang Xue and Yanwei Fu", "abstract": "Recent image-to-video (I2V) based video inpainting methods have made significant strides by leveraging single-image priors and modeling temporal consistency across masked frames. Nevertheless, these methods suffer from severe content degradation within video chunks. Furthermore, the absence of a robust frame alignment scheme compromises intra-chunk and inter-chunk spatiotemporal stability, resulting in insufficient control over the entire video. To address these limitations, we propose VidPivot, a novel framework that decouples video inpainting into two sub-tasks: multi-frame consistent image inpainting and masked area motion propagation. Our approach introduces frame interval priors as spatiotemporal cues to guide the inpainting process. To enhance cross-frame coherence, we design a FrameProp Module that implements a frame content propagation strategy, diffusing reference frame content into subsequent frames via a splicing mechanism. Additionally, a dedicated context controller encodes these coherent frame priors into the I2V generative backbone, effectively serving as soft constrain to suppress content distortion during generation. Extensive evaluations demonstrate that VidPivot achieves competitive performance across diverse benchmarks and generalizes well to different video inpainting scenarios.", "link": "http://arxiv.org/abs/2510.21461v2", "date": "2025-11-14", "relevancy": 2.8487, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5779}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5692}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Video%20Inpainting%20with%20Aligned%20Frame%20Interval%20Guidance&body=Title%3A%20Enhancing%20Video%20Inpainting%20with%20Aligned%20Frame%20Interval%20Guidance%0AAuthor%3A%20Ming%20Xie%20and%20Junqiu%20Yu%20and%20Qiaole%20Dong%20and%20Xiangyang%20Xue%20and%20Yanwei%20Fu%0AAbstract%3A%20Recent%20image-to-video%20%28I2V%29%20based%20video%20inpainting%20methods%20have%20made%20significant%20strides%20by%20leveraging%20single-image%20priors%20and%20modeling%20temporal%20consistency%20across%20masked%20frames.%20Nevertheless%2C%20these%20methods%20suffer%20from%20severe%20content%20degradation%20within%20video%20chunks.%20Furthermore%2C%20the%20absence%20of%20a%20robust%20frame%20alignment%20scheme%20compromises%20intra-chunk%20and%20inter-chunk%20spatiotemporal%20stability%2C%20resulting%20in%20insufficient%20control%20over%20the%20entire%20video.%20To%20address%20these%20limitations%2C%20we%20propose%20VidPivot%2C%20a%20novel%20framework%20that%20decouples%20video%20inpainting%20into%20two%20sub-tasks%3A%20multi-frame%20consistent%20image%20inpainting%20and%20masked%20area%20motion%20propagation.%20Our%20approach%20introduces%20frame%20interval%20priors%20as%20spatiotemporal%20cues%20to%20guide%20the%20inpainting%20process.%20To%20enhance%20cross-frame%20coherence%2C%20we%20design%20a%20FrameProp%20Module%20that%20implements%20a%20frame%20content%20propagation%20strategy%2C%20diffusing%20reference%20frame%20content%20into%20subsequent%20frames%20via%20a%20splicing%20mechanism.%20Additionally%2C%20a%20dedicated%20context%20controller%20encodes%20these%20coherent%20frame%20priors%20into%20the%20I2V%20generative%20backbone%2C%20effectively%20serving%20as%20soft%20constrain%20to%20suppress%20content%20distortion%20during%20generation.%20Extensive%20evaluations%20demonstrate%20that%20VidPivot%20achieves%20competitive%20performance%20across%20diverse%20benchmarks%20and%20generalizes%20well%20to%20different%20video%20inpainting%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2510.21461v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Video%2520Inpainting%2520with%2520Aligned%2520Frame%2520Interval%2520Guidance%26entry.906535625%3DMing%2520Xie%2520and%2520Junqiu%2520Yu%2520and%2520Qiaole%2520Dong%2520and%2520Xiangyang%2520Xue%2520and%2520Yanwei%2520Fu%26entry.1292438233%3DRecent%2520image-to-video%2520%2528I2V%2529%2520based%2520video%2520inpainting%2520methods%2520have%2520made%2520significant%2520strides%2520by%2520leveraging%2520single-image%2520priors%2520and%2520modeling%2520temporal%2520consistency%2520across%2520masked%2520frames.%2520Nevertheless%252C%2520these%2520methods%2520suffer%2520from%2520severe%2520content%2520degradation%2520within%2520video%2520chunks.%2520Furthermore%252C%2520the%2520absence%2520of%2520a%2520robust%2520frame%2520alignment%2520scheme%2520compromises%2520intra-chunk%2520and%2520inter-chunk%2520spatiotemporal%2520stability%252C%2520resulting%2520in%2520insufficient%2520control%2520over%2520the%2520entire%2520video.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520VidPivot%252C%2520a%2520novel%2520framework%2520that%2520decouples%2520video%2520inpainting%2520into%2520two%2520sub-tasks%253A%2520multi-frame%2520consistent%2520image%2520inpainting%2520and%2520masked%2520area%2520motion%2520propagation.%2520Our%2520approach%2520introduces%2520frame%2520interval%2520priors%2520as%2520spatiotemporal%2520cues%2520to%2520guide%2520the%2520inpainting%2520process.%2520To%2520enhance%2520cross-frame%2520coherence%252C%2520we%2520design%2520a%2520FrameProp%2520Module%2520that%2520implements%2520a%2520frame%2520content%2520propagation%2520strategy%252C%2520diffusing%2520reference%2520frame%2520content%2520into%2520subsequent%2520frames%2520via%2520a%2520splicing%2520mechanism.%2520Additionally%252C%2520a%2520dedicated%2520context%2520controller%2520encodes%2520these%2520coherent%2520frame%2520priors%2520into%2520the%2520I2V%2520generative%2520backbone%252C%2520effectively%2520serving%2520as%2520soft%2520constrain%2520to%2520suppress%2520content%2520distortion%2520during%2520generation.%2520Extensive%2520evaluations%2520demonstrate%2520that%2520VidPivot%2520achieves%2520competitive%2520performance%2520across%2520diverse%2520benchmarks%2520and%2520generalizes%2520well%2520to%2520different%2520video%2520inpainting%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.21461v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Video%20Inpainting%20with%20Aligned%20Frame%20Interval%20Guidance&entry.906535625=Ming%20Xie%20and%20Junqiu%20Yu%20and%20Qiaole%20Dong%20and%20Xiangyang%20Xue%20and%20Yanwei%20Fu&entry.1292438233=Recent%20image-to-video%20%28I2V%29%20based%20video%20inpainting%20methods%20have%20made%20significant%20strides%20by%20leveraging%20single-image%20priors%20and%20modeling%20temporal%20consistency%20across%20masked%20frames.%20Nevertheless%2C%20these%20methods%20suffer%20from%20severe%20content%20degradation%20within%20video%20chunks.%20Furthermore%2C%20the%20absence%20of%20a%20robust%20frame%20alignment%20scheme%20compromises%20intra-chunk%20and%20inter-chunk%20spatiotemporal%20stability%2C%20resulting%20in%20insufficient%20control%20over%20the%20entire%20video.%20To%20address%20these%20limitations%2C%20we%20propose%20VidPivot%2C%20a%20novel%20framework%20that%20decouples%20video%20inpainting%20into%20two%20sub-tasks%3A%20multi-frame%20consistent%20image%20inpainting%20and%20masked%20area%20motion%20propagation.%20Our%20approach%20introduces%20frame%20interval%20priors%20as%20spatiotemporal%20cues%20to%20guide%20the%20inpainting%20process.%20To%20enhance%20cross-frame%20coherence%2C%20we%20design%20a%20FrameProp%20Module%20that%20implements%20a%20frame%20content%20propagation%20strategy%2C%20diffusing%20reference%20frame%20content%20into%20subsequent%20frames%20via%20a%20splicing%20mechanism.%20Additionally%2C%20a%20dedicated%20context%20controller%20encodes%20these%20coherent%20frame%20priors%20into%20the%20I2V%20generative%20backbone%2C%20effectively%20serving%20as%20soft%20constrain%20to%20suppress%20content%20distortion%20during%20generation.%20Extensive%20evaluations%20demonstrate%20that%20VidPivot%20achieves%20competitive%20performance%20across%20diverse%20benchmarks%20and%20generalizes%20well%20to%20different%20video%20inpainting%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2510.21461v2&entry.124074799=Read"},
{"title": "Bridging Hidden States in Vision-Language Models", "author": "Benjamin Fein-Ashley and Jacob Fein-Ashley", "abstract": "Vision-Language Models (VLMs) are a new family of models that align image content with natural language. Existing approaches typically fuse either (a) early: by mixing tokens/features inside the encoders, or (b) late: by comparing pooled embeddings. Many methods also tie fusion to an autoregressive decoder. However, the hidden states of both modalities already carry rich, modality-specific structure (spatial layout in vision; syntax and semantics in text), so directly aligning these states is a natural way to match what the two modalities \"think\". We propose a lightweight fusion module: a few cross-only, bidirectional attention layers placed near the top of both encoders. Each layer projects the vision and text encoder hidden-state sequences into a shared space, attends across modalities, and sends gated residual updates back, with simple stabilizers to improve alignment. The encoders remain non-causal and strong for understanding, while generation stays cleanly decoupled via an optional decoder. Across standard retrieval, VQA, and visual reasoning benchmarks, BRIDGE outperforms comparable VLMs while preserving the bi-encoder efficiency of contrastive models. We make our code publicly available at https://github.com/jfeinashley/BRIDGE.", "link": "http://arxiv.org/abs/2511.11526v1", "date": "2025-11-14", "relevancy": 2.8425, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.587}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.587}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Hidden%20States%20in%20Vision-Language%20Models&body=Title%3A%20Bridging%20Hidden%20States%20in%20Vision-Language%20Models%0AAuthor%3A%20Benjamin%20Fein-Ashley%20and%20Jacob%20Fein-Ashley%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20are%20a%20new%20family%20of%20models%20that%20align%20image%20content%20with%20natural%20language.%20Existing%20approaches%20typically%20fuse%20either%20%28a%29%20early%3A%20by%20mixing%20tokens/features%20inside%20the%20encoders%2C%20or%20%28b%29%20late%3A%20by%20comparing%20pooled%20embeddings.%20Many%20methods%20also%20tie%20fusion%20to%20an%20autoregressive%20decoder.%20However%2C%20the%20hidden%20states%20of%20both%20modalities%20already%20carry%20rich%2C%20modality-specific%20structure%20%28spatial%20layout%20in%20vision%3B%20syntax%20and%20semantics%20in%20text%29%2C%20so%20directly%20aligning%20these%20states%20is%20a%20natural%20way%20to%20match%20what%20the%20two%20modalities%20%22think%22.%20We%20propose%20a%20lightweight%20fusion%20module%3A%20a%20few%20cross-only%2C%20bidirectional%20attention%20layers%20placed%20near%20the%20top%20of%20both%20encoders.%20Each%20layer%20projects%20the%20vision%20and%20text%20encoder%20hidden-state%20sequences%20into%20a%20shared%20space%2C%20attends%20across%20modalities%2C%20and%20sends%20gated%20residual%20updates%20back%2C%20with%20simple%20stabilizers%20to%20improve%20alignment.%20The%20encoders%20remain%20non-causal%20and%20strong%20for%20understanding%2C%20while%20generation%20stays%20cleanly%20decoupled%20via%20an%20optional%20decoder.%20Across%20standard%20retrieval%2C%20VQA%2C%20and%20visual%20reasoning%20benchmarks%2C%20BRIDGE%20outperforms%20comparable%20VLMs%20while%20preserving%20the%20bi-encoder%20efficiency%20of%20contrastive%20models.%20We%20make%20our%20code%20publicly%20available%20at%20https%3A//github.com/jfeinashley/BRIDGE.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11526v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Hidden%2520States%2520in%2520Vision-Language%2520Models%26entry.906535625%3DBenjamin%2520Fein-Ashley%2520and%2520Jacob%2520Fein-Ashley%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520are%2520a%2520new%2520family%2520of%2520models%2520that%2520align%2520image%2520content%2520with%2520natural%2520language.%2520Existing%2520approaches%2520typically%2520fuse%2520either%2520%2528a%2529%2520early%253A%2520by%2520mixing%2520tokens/features%2520inside%2520the%2520encoders%252C%2520or%2520%2528b%2529%2520late%253A%2520by%2520comparing%2520pooled%2520embeddings.%2520Many%2520methods%2520also%2520tie%2520fusion%2520to%2520an%2520autoregressive%2520decoder.%2520However%252C%2520the%2520hidden%2520states%2520of%2520both%2520modalities%2520already%2520carry%2520rich%252C%2520modality-specific%2520structure%2520%2528spatial%2520layout%2520in%2520vision%253B%2520syntax%2520and%2520semantics%2520in%2520text%2529%252C%2520so%2520directly%2520aligning%2520these%2520states%2520is%2520a%2520natural%2520way%2520to%2520match%2520what%2520the%2520two%2520modalities%2520%2522think%2522.%2520We%2520propose%2520a%2520lightweight%2520fusion%2520module%253A%2520a%2520few%2520cross-only%252C%2520bidirectional%2520attention%2520layers%2520placed%2520near%2520the%2520top%2520of%2520both%2520encoders.%2520Each%2520layer%2520projects%2520the%2520vision%2520and%2520text%2520encoder%2520hidden-state%2520sequences%2520into%2520a%2520shared%2520space%252C%2520attends%2520across%2520modalities%252C%2520and%2520sends%2520gated%2520residual%2520updates%2520back%252C%2520with%2520simple%2520stabilizers%2520to%2520improve%2520alignment.%2520The%2520encoders%2520remain%2520non-causal%2520and%2520strong%2520for%2520understanding%252C%2520while%2520generation%2520stays%2520cleanly%2520decoupled%2520via%2520an%2520optional%2520decoder.%2520Across%2520standard%2520retrieval%252C%2520VQA%252C%2520and%2520visual%2520reasoning%2520benchmarks%252C%2520BRIDGE%2520outperforms%2520comparable%2520VLMs%2520while%2520preserving%2520the%2520bi-encoder%2520efficiency%2520of%2520contrastive%2520models.%2520We%2520make%2520our%2520code%2520publicly%2520available%2520at%2520https%253A//github.com/jfeinashley/BRIDGE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11526v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Hidden%20States%20in%20Vision-Language%20Models&entry.906535625=Benjamin%20Fein-Ashley%20and%20Jacob%20Fein-Ashley&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20are%20a%20new%20family%20of%20models%20that%20align%20image%20content%20with%20natural%20language.%20Existing%20approaches%20typically%20fuse%20either%20%28a%29%20early%3A%20by%20mixing%20tokens/features%20inside%20the%20encoders%2C%20or%20%28b%29%20late%3A%20by%20comparing%20pooled%20embeddings.%20Many%20methods%20also%20tie%20fusion%20to%20an%20autoregressive%20decoder.%20However%2C%20the%20hidden%20states%20of%20both%20modalities%20already%20carry%20rich%2C%20modality-specific%20structure%20%28spatial%20layout%20in%20vision%3B%20syntax%20and%20semantics%20in%20text%29%2C%20so%20directly%20aligning%20these%20states%20is%20a%20natural%20way%20to%20match%20what%20the%20two%20modalities%20%22think%22.%20We%20propose%20a%20lightweight%20fusion%20module%3A%20a%20few%20cross-only%2C%20bidirectional%20attention%20layers%20placed%20near%20the%20top%20of%20both%20encoders.%20Each%20layer%20projects%20the%20vision%20and%20text%20encoder%20hidden-state%20sequences%20into%20a%20shared%20space%2C%20attends%20across%20modalities%2C%20and%20sends%20gated%20residual%20updates%20back%2C%20with%20simple%20stabilizers%20to%20improve%20alignment.%20The%20encoders%20remain%20non-causal%20and%20strong%20for%20understanding%2C%20while%20generation%20stays%20cleanly%20decoupled%20via%20an%20optional%20decoder.%20Across%20standard%20retrieval%2C%20VQA%2C%20and%20visual%20reasoning%20benchmarks%2C%20BRIDGE%20outperforms%20comparable%20VLMs%20while%20preserving%20the%20bi-encoder%20efficiency%20of%20contrastive%20models.%20We%20make%20our%20code%20publicly%20available%20at%20https%3A//github.com/jfeinashley/BRIDGE.&entry.1838667208=http%3A//arxiv.org/abs/2511.11526v1&entry.124074799=Read"},
{"title": "Quantifying the Limits of Segmentation Foundation Models: Modeling Challenges in Segmenting Tree-Like and Low-Contrast Objects", "author": "Yixin Zhang and Nicholas Konz and Kevin Kramer and Maciej A. Mazurowski", "abstract": "Image segmentation foundation models (SFMs) like Segment Anything Model (SAM) have achieved impressive zero-shot and interactive segmentation across diverse domains. However, they struggle to segment objects with certain structures, particularly those with dense, tree-like morphology and low textural contrast from their surroundings. These failure modes are crucial for understanding the limitations of SFMs in real-world applications. To systematically study this issue, we introduce interpretable metrics quantifying object tree-likeness and textural separability. On carefully controlled synthetic experiments and real-world datasets, we show that SFM performance (\\eg, SAM, SAM 2, HQ-SAM) noticeably correlates with these factors. We attribute these failures to SFMs misinterpreting local structure as global texture, resulting in over-segmentation or difficulty distinguishing objects from similar backgrounds. Notably, targeted fine-tuning fails to resolve this issue, indicating a fundamental limitation. Our study provides the first quantitative framework for modeling the behavior of SFMs on challenging structures, offering interpretable insights into their segmentation capabilities.", "link": "http://arxiv.org/abs/2412.04243v3", "date": "2025-11-14", "relevancy": 2.8376, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5802}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5802}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20the%20Limits%20of%20Segmentation%20Foundation%20Models%3A%20Modeling%20Challenges%20in%20Segmenting%20Tree-Like%20and%20Low-Contrast%20Objects&body=Title%3A%20Quantifying%20the%20Limits%20of%20Segmentation%20Foundation%20Models%3A%20Modeling%20Challenges%20in%20Segmenting%20Tree-Like%20and%20Low-Contrast%20Objects%0AAuthor%3A%20Yixin%20Zhang%20and%20Nicholas%20Konz%20and%20Kevin%20Kramer%20and%20Maciej%20A.%20Mazurowski%0AAbstract%3A%20Image%20segmentation%20foundation%20models%20%28SFMs%29%20like%20Segment%20Anything%20Model%20%28SAM%29%20have%20achieved%20impressive%20zero-shot%20and%20interactive%20segmentation%20across%20diverse%20domains.%20However%2C%20they%20struggle%20to%20segment%20objects%20with%20certain%20structures%2C%20particularly%20those%20with%20dense%2C%20tree-like%20morphology%20and%20low%20textural%20contrast%20from%20their%20surroundings.%20These%20failure%20modes%20are%20crucial%20for%20understanding%20the%20limitations%20of%20SFMs%20in%20real-world%20applications.%20To%20systematically%20study%20this%20issue%2C%20we%20introduce%20interpretable%20metrics%20quantifying%20object%20tree-likeness%20and%20textural%20separability.%20On%20carefully%20controlled%20synthetic%20experiments%20and%20real-world%20datasets%2C%20we%20show%20that%20SFM%20performance%20%28%5Ceg%2C%20SAM%2C%20SAM%202%2C%20HQ-SAM%29%20noticeably%20correlates%20with%20these%20factors.%20We%20attribute%20these%20failures%20to%20SFMs%20misinterpreting%20local%20structure%20as%20global%20texture%2C%20resulting%20in%20over-segmentation%20or%20difficulty%20distinguishing%20objects%20from%20similar%20backgrounds.%20Notably%2C%20targeted%20fine-tuning%20fails%20to%20resolve%20this%20issue%2C%20indicating%20a%20fundamental%20limitation.%20Our%20study%20provides%20the%20first%20quantitative%20framework%20for%20modeling%20the%20behavior%20of%20SFMs%20on%20challenging%20structures%2C%20offering%20interpretable%20insights%20into%20their%20segmentation%20capabilities.%0ALink%3A%20http%3A//arxiv.org/abs/2412.04243v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520the%2520Limits%2520of%2520Segmentation%2520Foundation%2520Models%253A%2520Modeling%2520Challenges%2520in%2520Segmenting%2520Tree-Like%2520and%2520Low-Contrast%2520Objects%26entry.906535625%3DYixin%2520Zhang%2520and%2520Nicholas%2520Konz%2520and%2520Kevin%2520Kramer%2520and%2520Maciej%2520A.%2520Mazurowski%26entry.1292438233%3DImage%2520segmentation%2520foundation%2520models%2520%2528SFMs%2529%2520like%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520have%2520achieved%2520impressive%2520zero-shot%2520and%2520interactive%2520segmentation%2520across%2520diverse%2520domains.%2520However%252C%2520they%2520struggle%2520to%2520segment%2520objects%2520with%2520certain%2520structures%252C%2520particularly%2520those%2520with%2520dense%252C%2520tree-like%2520morphology%2520and%2520low%2520textural%2520contrast%2520from%2520their%2520surroundings.%2520These%2520failure%2520modes%2520are%2520crucial%2520for%2520understanding%2520the%2520limitations%2520of%2520SFMs%2520in%2520real-world%2520applications.%2520To%2520systematically%2520study%2520this%2520issue%252C%2520we%2520introduce%2520interpretable%2520metrics%2520quantifying%2520object%2520tree-likeness%2520and%2520textural%2520separability.%2520On%2520carefully%2520controlled%2520synthetic%2520experiments%2520and%2520real-world%2520datasets%252C%2520we%2520show%2520that%2520SFM%2520performance%2520%2528%255Ceg%252C%2520SAM%252C%2520SAM%25202%252C%2520HQ-SAM%2529%2520noticeably%2520correlates%2520with%2520these%2520factors.%2520We%2520attribute%2520these%2520failures%2520to%2520SFMs%2520misinterpreting%2520local%2520structure%2520as%2520global%2520texture%252C%2520resulting%2520in%2520over-segmentation%2520or%2520difficulty%2520distinguishing%2520objects%2520from%2520similar%2520backgrounds.%2520Notably%252C%2520targeted%2520fine-tuning%2520fails%2520to%2520resolve%2520this%2520issue%252C%2520indicating%2520a%2520fundamental%2520limitation.%2520Our%2520study%2520provides%2520the%2520first%2520quantitative%2520framework%2520for%2520modeling%2520the%2520behavior%2520of%2520SFMs%2520on%2520challenging%2520structures%252C%2520offering%2520interpretable%2520insights%2520into%2520their%2520segmentation%2520capabilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04243v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20the%20Limits%20of%20Segmentation%20Foundation%20Models%3A%20Modeling%20Challenges%20in%20Segmenting%20Tree-Like%20and%20Low-Contrast%20Objects&entry.906535625=Yixin%20Zhang%20and%20Nicholas%20Konz%20and%20Kevin%20Kramer%20and%20Maciej%20A.%20Mazurowski&entry.1292438233=Image%20segmentation%20foundation%20models%20%28SFMs%29%20like%20Segment%20Anything%20Model%20%28SAM%29%20have%20achieved%20impressive%20zero-shot%20and%20interactive%20segmentation%20across%20diverse%20domains.%20However%2C%20they%20struggle%20to%20segment%20objects%20with%20certain%20structures%2C%20particularly%20those%20with%20dense%2C%20tree-like%20morphology%20and%20low%20textural%20contrast%20from%20their%20surroundings.%20These%20failure%20modes%20are%20crucial%20for%20understanding%20the%20limitations%20of%20SFMs%20in%20real-world%20applications.%20To%20systematically%20study%20this%20issue%2C%20we%20introduce%20interpretable%20metrics%20quantifying%20object%20tree-likeness%20and%20textural%20separability.%20On%20carefully%20controlled%20synthetic%20experiments%20and%20real-world%20datasets%2C%20we%20show%20that%20SFM%20performance%20%28%5Ceg%2C%20SAM%2C%20SAM%202%2C%20HQ-SAM%29%20noticeably%20correlates%20with%20these%20factors.%20We%20attribute%20these%20failures%20to%20SFMs%20misinterpreting%20local%20structure%20as%20global%20texture%2C%20resulting%20in%20over-segmentation%20or%20difficulty%20distinguishing%20objects%20from%20similar%20backgrounds.%20Notably%2C%20targeted%20fine-tuning%20fails%20to%20resolve%20this%20issue%2C%20indicating%20a%20fundamental%20limitation.%20Our%20study%20provides%20the%20first%20quantitative%20framework%20for%20modeling%20the%20behavior%20of%20SFMs%20on%20challenging%20structures%2C%20offering%20interpretable%20insights%20into%20their%20segmentation%20capabilities.&entry.1838667208=http%3A//arxiv.org/abs/2412.04243v3&entry.124074799=Read"},
{"title": "MAFM^3: Modular Adaptation of Foundation Models for Multi-Modal Medical AI", "author": "Mohammad Areeb Qazi and Munachiso S Nwadike and Ibrahim Almakky and Mohammad Yaqub and Numan Saeed", "abstract": "Foundational models are trained on extensive datasets to capture the general trends of a domain. However, in medical imaging, the scarcity of data makes pre-training for every domain, modality, or task challenging. Instead of building separate models, we propose MAFM^3 (Modular Adaptation of Foundation Models for Multi-Modal Medical AI), a framework that enables a single foundation model to expand into diverse domains, tasks, and modalities through lightweight modular components. These components serve as specialized skill sets that allow the system to flexibly activate the appropriate capability at the inference time, depending on the input type or clinical objective. Unlike conventional adaptation methods that treat each new task or modality in isolation, MAFM^3 provides a unified and expandable framework for efficient multitask and multimodality adaptation. Empirically, we validate our approach by adapting a chest CT foundation model initially trained for classification into prognosis and segmentation modules. Our results show improved performance on both tasks. Furthermore, by incorporating PET scans, MAFM^3 achieved an improvement in the Dice score 5% compared to the respective baselines. These findings establish that foundation models, when equipped with modular components, are not inherently constrained to their initial training scope but can evolve into multitask, multimodality systems for medical imaging. The code implementation of this work can be found at https://github.com/Areeb2735/CTscan_prognosis_VLM", "link": "http://arxiv.org/abs/2511.11212v1", "date": "2025-11-14", "relevancy": 2.8187, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6046}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5433}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAFM%5E3%3A%20Modular%20Adaptation%20of%20Foundation%20Models%20for%20Multi-Modal%20Medical%20AI&body=Title%3A%20MAFM%5E3%3A%20Modular%20Adaptation%20of%20Foundation%20Models%20for%20Multi-Modal%20Medical%20AI%0AAuthor%3A%20Mohammad%20Areeb%20Qazi%20and%20Munachiso%20S%20Nwadike%20and%20Ibrahim%20Almakky%20and%20Mohammad%20Yaqub%20and%20Numan%20Saeed%0AAbstract%3A%20Foundational%20models%20are%20trained%20on%20extensive%20datasets%20to%20capture%20the%20general%20trends%20of%20a%20domain.%20However%2C%20in%20medical%20imaging%2C%20the%20scarcity%20of%20data%20makes%20pre-training%20for%20every%20domain%2C%20modality%2C%20or%20task%20challenging.%20Instead%20of%20building%20separate%20models%2C%20we%20propose%20MAFM%5E3%20%28Modular%20Adaptation%20of%20Foundation%20Models%20for%20Multi-Modal%20Medical%20AI%29%2C%20a%20framework%20that%20enables%20a%20single%20foundation%20model%20to%20expand%20into%20diverse%20domains%2C%20tasks%2C%20and%20modalities%20through%20lightweight%20modular%20components.%20These%20components%20serve%20as%20specialized%20skill%20sets%20that%20allow%20the%20system%20to%20flexibly%20activate%20the%20appropriate%20capability%20at%20the%20inference%20time%2C%20depending%20on%20the%20input%20type%20or%20clinical%20objective.%20Unlike%20conventional%20adaptation%20methods%20that%20treat%20each%20new%20task%20or%20modality%20in%20isolation%2C%20MAFM%5E3%20provides%20a%20unified%20and%20expandable%20framework%20for%20efficient%20multitask%20and%20multimodality%20adaptation.%20Empirically%2C%20we%20validate%20our%20approach%20by%20adapting%20a%20chest%20CT%20foundation%20model%20initially%20trained%20for%20classification%20into%20prognosis%20and%20segmentation%20modules.%20Our%20results%20show%20improved%20performance%20on%20both%20tasks.%20Furthermore%2C%20by%20incorporating%20PET%20scans%2C%20MAFM%5E3%20achieved%20an%20improvement%20in%20the%20Dice%20score%205%25%20compared%20to%20the%20respective%20baselines.%20These%20findings%20establish%20that%20foundation%20models%2C%20when%20equipped%20with%20modular%20components%2C%20are%20not%20inherently%20constrained%20to%20their%20initial%20training%20scope%20but%20can%20evolve%20into%20multitask%2C%20multimodality%20systems%20for%20medical%20imaging.%20The%20code%20implementation%20of%20this%20work%20can%20be%20found%20at%20https%3A//github.com/Areeb2735/CTscan_prognosis_VLM%0ALink%3A%20http%3A//arxiv.org/abs/2511.11212v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAFM%255E3%253A%2520Modular%2520Adaptation%2520of%2520Foundation%2520Models%2520for%2520Multi-Modal%2520Medical%2520AI%26entry.906535625%3DMohammad%2520Areeb%2520Qazi%2520and%2520Munachiso%2520S%2520Nwadike%2520and%2520Ibrahim%2520Almakky%2520and%2520Mohammad%2520Yaqub%2520and%2520Numan%2520Saeed%26entry.1292438233%3DFoundational%2520models%2520are%2520trained%2520on%2520extensive%2520datasets%2520to%2520capture%2520the%2520general%2520trends%2520of%2520a%2520domain.%2520However%252C%2520in%2520medical%2520imaging%252C%2520the%2520scarcity%2520of%2520data%2520makes%2520pre-training%2520for%2520every%2520domain%252C%2520modality%252C%2520or%2520task%2520challenging.%2520Instead%2520of%2520building%2520separate%2520models%252C%2520we%2520propose%2520MAFM%255E3%2520%2528Modular%2520Adaptation%2520of%2520Foundation%2520Models%2520for%2520Multi-Modal%2520Medical%2520AI%2529%252C%2520a%2520framework%2520that%2520enables%2520a%2520single%2520foundation%2520model%2520to%2520expand%2520into%2520diverse%2520domains%252C%2520tasks%252C%2520and%2520modalities%2520through%2520lightweight%2520modular%2520components.%2520These%2520components%2520serve%2520as%2520specialized%2520skill%2520sets%2520that%2520allow%2520the%2520system%2520to%2520flexibly%2520activate%2520the%2520appropriate%2520capability%2520at%2520the%2520inference%2520time%252C%2520depending%2520on%2520the%2520input%2520type%2520or%2520clinical%2520objective.%2520Unlike%2520conventional%2520adaptation%2520methods%2520that%2520treat%2520each%2520new%2520task%2520or%2520modality%2520in%2520isolation%252C%2520MAFM%255E3%2520provides%2520a%2520unified%2520and%2520expandable%2520framework%2520for%2520efficient%2520multitask%2520and%2520multimodality%2520adaptation.%2520Empirically%252C%2520we%2520validate%2520our%2520approach%2520by%2520adapting%2520a%2520chest%2520CT%2520foundation%2520model%2520initially%2520trained%2520for%2520classification%2520into%2520prognosis%2520and%2520segmentation%2520modules.%2520Our%2520results%2520show%2520improved%2520performance%2520on%2520both%2520tasks.%2520Furthermore%252C%2520by%2520incorporating%2520PET%2520scans%252C%2520MAFM%255E3%2520achieved%2520an%2520improvement%2520in%2520the%2520Dice%2520score%25205%2525%2520compared%2520to%2520the%2520respective%2520baselines.%2520These%2520findings%2520establish%2520that%2520foundation%2520models%252C%2520when%2520equipped%2520with%2520modular%2520components%252C%2520are%2520not%2520inherently%2520constrained%2520to%2520their%2520initial%2520training%2520scope%2520but%2520can%2520evolve%2520into%2520multitask%252C%2520multimodality%2520systems%2520for%2520medical%2520imaging.%2520The%2520code%2520implementation%2520of%2520this%2520work%2520can%2520be%2520found%2520at%2520https%253A//github.com/Areeb2735/CTscan_prognosis_VLM%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11212v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAFM%5E3%3A%20Modular%20Adaptation%20of%20Foundation%20Models%20for%20Multi-Modal%20Medical%20AI&entry.906535625=Mohammad%20Areeb%20Qazi%20and%20Munachiso%20S%20Nwadike%20and%20Ibrahim%20Almakky%20and%20Mohammad%20Yaqub%20and%20Numan%20Saeed&entry.1292438233=Foundational%20models%20are%20trained%20on%20extensive%20datasets%20to%20capture%20the%20general%20trends%20of%20a%20domain.%20However%2C%20in%20medical%20imaging%2C%20the%20scarcity%20of%20data%20makes%20pre-training%20for%20every%20domain%2C%20modality%2C%20or%20task%20challenging.%20Instead%20of%20building%20separate%20models%2C%20we%20propose%20MAFM%5E3%20%28Modular%20Adaptation%20of%20Foundation%20Models%20for%20Multi-Modal%20Medical%20AI%29%2C%20a%20framework%20that%20enables%20a%20single%20foundation%20model%20to%20expand%20into%20diverse%20domains%2C%20tasks%2C%20and%20modalities%20through%20lightweight%20modular%20components.%20These%20components%20serve%20as%20specialized%20skill%20sets%20that%20allow%20the%20system%20to%20flexibly%20activate%20the%20appropriate%20capability%20at%20the%20inference%20time%2C%20depending%20on%20the%20input%20type%20or%20clinical%20objective.%20Unlike%20conventional%20adaptation%20methods%20that%20treat%20each%20new%20task%20or%20modality%20in%20isolation%2C%20MAFM%5E3%20provides%20a%20unified%20and%20expandable%20framework%20for%20efficient%20multitask%20and%20multimodality%20adaptation.%20Empirically%2C%20we%20validate%20our%20approach%20by%20adapting%20a%20chest%20CT%20foundation%20model%20initially%20trained%20for%20classification%20into%20prognosis%20and%20segmentation%20modules.%20Our%20results%20show%20improved%20performance%20on%20both%20tasks.%20Furthermore%2C%20by%20incorporating%20PET%20scans%2C%20MAFM%5E3%20achieved%20an%20improvement%20in%20the%20Dice%20score%205%25%20compared%20to%20the%20respective%20baselines.%20These%20findings%20establish%20that%20foundation%20models%2C%20when%20equipped%20with%20modular%20components%2C%20are%20not%20inherently%20constrained%20to%20their%20initial%20training%20scope%20but%20can%20evolve%20into%20multitask%2C%20multimodality%20systems%20for%20medical%20imaging.%20The%20code%20implementation%20of%20this%20work%20can%20be%20found%20at%20https%3A//github.com/Areeb2735/CTscan_prognosis_VLM&entry.1838667208=http%3A//arxiv.org/abs/2511.11212v1&entry.124074799=Read"},
{"title": "Large-scale modality-invariant foundation models for brain MRI analysis: Application to lesion segmentation", "author": "Petros Koutsouvelis and Matej Gazda and Leroy Volmer and Sina Amirrajab and Kamil Barbierik and Branislav Setlak and Jakub Gazda and Peter Drotar", "abstract": "The field of computer vision is undergoing a paradigm shift toward large-scale foundation model pre-training via self-supervised learning (SSL). Leveraging large volumes of unlabeled brain MRI data, such models can learn anatomical priors that improve few-shot performance in diverse neuroimaging tasks. However, most SSL frameworks are tailored to natural images, and their adaptation to capture multi-modal MRI information remains underexplored. This work proposes a modality-invariant representation learning setup and evaluates its effectiveness in stroke and epilepsy lesion segmentation, following large-scale pre-training. Experimental results suggest that despite successful cross-modality alignment, lesion segmentation primarily benefits from preserving fine-grained modality-specific features. Model checkpoints and code are made publicly available.", "link": "http://arxiv.org/abs/2511.11311v1", "date": "2025-11-14", "relevancy": 2.8134, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large-scale%20modality-invariant%20foundation%20models%20for%20brain%20MRI%20analysis%3A%20Application%20to%20lesion%20segmentation&body=Title%3A%20Large-scale%20modality-invariant%20foundation%20models%20for%20brain%20MRI%20analysis%3A%20Application%20to%20lesion%20segmentation%0AAuthor%3A%20Petros%20Koutsouvelis%20and%20Matej%20Gazda%20and%20Leroy%20Volmer%20and%20Sina%20Amirrajab%20and%20Kamil%20Barbierik%20and%20Branislav%20Setlak%20and%20Jakub%20Gazda%20and%20Peter%20Drotar%0AAbstract%3A%20The%20field%20of%20computer%20vision%20is%20undergoing%20a%20paradigm%20shift%20toward%20large-scale%20foundation%20model%20pre-training%20via%20self-supervised%20learning%20%28SSL%29.%20Leveraging%20large%20volumes%20of%20unlabeled%20brain%20MRI%20data%2C%20such%20models%20can%20learn%20anatomical%20priors%20that%20improve%20few-shot%20performance%20in%20diverse%20neuroimaging%20tasks.%20However%2C%20most%20SSL%20frameworks%20are%20tailored%20to%20natural%20images%2C%20and%20their%20adaptation%20to%20capture%20multi-modal%20MRI%20information%20remains%20underexplored.%20This%20work%20proposes%20a%20modality-invariant%20representation%20learning%20setup%20and%20evaluates%20its%20effectiveness%20in%20stroke%20and%20epilepsy%20lesion%20segmentation%2C%20following%20large-scale%20pre-training.%20Experimental%20results%20suggest%20that%20despite%20successful%20cross-modality%20alignment%2C%20lesion%20segmentation%20primarily%20benefits%20from%20preserving%20fine-grained%20modality-specific%20features.%20Model%20checkpoints%20and%20code%20are%20made%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11311v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge-scale%2520modality-invariant%2520foundation%2520models%2520for%2520brain%2520MRI%2520analysis%253A%2520Application%2520to%2520lesion%2520segmentation%26entry.906535625%3DPetros%2520Koutsouvelis%2520and%2520Matej%2520Gazda%2520and%2520Leroy%2520Volmer%2520and%2520Sina%2520Amirrajab%2520and%2520Kamil%2520Barbierik%2520and%2520Branislav%2520Setlak%2520and%2520Jakub%2520Gazda%2520and%2520Peter%2520Drotar%26entry.1292438233%3DThe%2520field%2520of%2520computer%2520vision%2520is%2520undergoing%2520a%2520paradigm%2520shift%2520toward%2520large-scale%2520foundation%2520model%2520pre-training%2520via%2520self-supervised%2520learning%2520%2528SSL%2529.%2520Leveraging%2520large%2520volumes%2520of%2520unlabeled%2520brain%2520MRI%2520data%252C%2520such%2520models%2520can%2520learn%2520anatomical%2520priors%2520that%2520improve%2520few-shot%2520performance%2520in%2520diverse%2520neuroimaging%2520tasks.%2520However%252C%2520most%2520SSL%2520frameworks%2520are%2520tailored%2520to%2520natural%2520images%252C%2520and%2520their%2520adaptation%2520to%2520capture%2520multi-modal%2520MRI%2520information%2520remains%2520underexplored.%2520This%2520work%2520proposes%2520a%2520modality-invariant%2520representation%2520learning%2520setup%2520and%2520evaluates%2520its%2520effectiveness%2520in%2520stroke%2520and%2520epilepsy%2520lesion%2520segmentation%252C%2520following%2520large-scale%2520pre-training.%2520Experimental%2520results%2520suggest%2520that%2520despite%2520successful%2520cross-modality%2520alignment%252C%2520lesion%2520segmentation%2520primarily%2520benefits%2520from%2520preserving%2520fine-grained%2520modality-specific%2520features.%2520Model%2520checkpoints%2520and%2520code%2520are%2520made%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11311v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-scale%20modality-invariant%20foundation%20models%20for%20brain%20MRI%20analysis%3A%20Application%20to%20lesion%20segmentation&entry.906535625=Petros%20Koutsouvelis%20and%20Matej%20Gazda%20and%20Leroy%20Volmer%20and%20Sina%20Amirrajab%20and%20Kamil%20Barbierik%20and%20Branislav%20Setlak%20and%20Jakub%20Gazda%20and%20Peter%20Drotar&entry.1292438233=The%20field%20of%20computer%20vision%20is%20undergoing%20a%20paradigm%20shift%20toward%20large-scale%20foundation%20model%20pre-training%20via%20self-supervised%20learning%20%28SSL%29.%20Leveraging%20large%20volumes%20of%20unlabeled%20brain%20MRI%20data%2C%20such%20models%20can%20learn%20anatomical%20priors%20that%20improve%20few-shot%20performance%20in%20diverse%20neuroimaging%20tasks.%20However%2C%20most%20SSL%20frameworks%20are%20tailored%20to%20natural%20images%2C%20and%20their%20adaptation%20to%20capture%20multi-modal%20MRI%20information%20remains%20underexplored.%20This%20work%20proposes%20a%20modality-invariant%20representation%20learning%20setup%20and%20evaluates%20its%20effectiveness%20in%20stroke%20and%20epilepsy%20lesion%20segmentation%2C%20following%20large-scale%20pre-training.%20Experimental%20results%20suggest%20that%20despite%20successful%20cross-modality%20alignment%2C%20lesion%20segmentation%20primarily%20benefits%20from%20preserving%20fine-grained%20modality-specific%20features.%20Model%20checkpoints%20and%20code%20are%20made%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2511.11311v1&entry.124074799=Read"},
{"title": "WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation", "author": "Wei Chow and Jiachun Pan and Yongyuan Liang and Mingze Zhou and Xue Song and Liyu Jia and Saining Zhang and Siliang Tang and Juncheng Li and Fengda Zhang and Weijia Wu and Hanwang Zhang and Tat-Seng Chua", "abstract": "Recent advances in unified multimodal models (UMMs) have enabled impressive progress in visual comprehension and generation. However, existing datasets and benchmarks focus primarily on single-turn interactions, failing to capture the multi-turn, context-dependent nature of real-world image creation and editing. To address this gap, we present WEAVE, the first suite for in-context interleaved cross-modality comprehension and generation. Our suite consists of two complementary parts. WEAVE-100k is a large-scale dataset of 100K interleaved samples spanning over 370K dialogue turns and 500K images, covering comprehension, editing, and generation tasks that require reasoning over historical context. WEAVEBench is a human-annotated benchmark with 100 tasks based on 480 images, featuring a hybrid VLM judger evaluation framework based on both the reference image and the combination of the original image with editing instructions that assesses models' abilities in multi-turn generation, visual memory, and world-knowledge reasoning across diverse domains. Experiments demonstrate that training on WEAVE-100k enables vision comprehension, image editing, and comprehension-generation collaboration capabilities. Furthermore, it facilitates UMMs to develop emergent visual-memory capabilities, while extensive evaluations on WEAVEBench expose the persistent limitations and challenges of current approaches in multi-turn, context-aware image generation and editing. We believe WEAVE provides a view and foundation for studying in-context interleaved comprehension and generation for multi-modal community.", "link": "http://arxiv.org/abs/2511.11434v1", "date": "2025-11-14", "relevancy": 2.7925, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5614}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5614}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WEAVE%3A%20Unleashing%20and%20Benchmarking%20the%20In-context%20Interleaved%20Comprehension%20and%20Generation&body=Title%3A%20WEAVE%3A%20Unleashing%20and%20Benchmarking%20the%20In-context%20Interleaved%20Comprehension%20and%20Generation%0AAuthor%3A%20Wei%20Chow%20and%20Jiachun%20Pan%20and%20Yongyuan%20Liang%20and%20Mingze%20Zhou%20and%20Xue%20Song%20and%20Liyu%20Jia%20and%20Saining%20Zhang%20and%20Siliang%20Tang%20and%20Juncheng%20Li%20and%20Fengda%20Zhang%20and%20Weijia%20Wu%20and%20Hanwang%20Zhang%20and%20Tat-Seng%20Chua%0AAbstract%3A%20Recent%20advances%20in%20unified%20multimodal%20models%20%28UMMs%29%20have%20enabled%20impressive%20progress%20in%20visual%20comprehension%20and%20generation.%20However%2C%20existing%20datasets%20and%20benchmarks%20focus%20primarily%20on%20single-turn%20interactions%2C%20failing%20to%20capture%20the%20multi-turn%2C%20context-dependent%20nature%20of%20real-world%20image%20creation%20and%20editing.%20To%20address%20this%20gap%2C%20we%20present%20WEAVE%2C%20the%20first%20suite%20for%20in-context%20interleaved%20cross-modality%20comprehension%20and%20generation.%20Our%20suite%20consists%20of%20two%20complementary%20parts.%20WEAVE-100k%20is%20a%20large-scale%20dataset%20of%20100K%20interleaved%20samples%20spanning%20over%20370K%20dialogue%20turns%20and%20500K%20images%2C%20covering%20comprehension%2C%20editing%2C%20and%20generation%20tasks%20that%20require%20reasoning%20over%20historical%20context.%20WEAVEBench%20is%20a%20human-annotated%20benchmark%20with%20100%20tasks%20based%20on%20480%20images%2C%20featuring%20a%20hybrid%20VLM%20judger%20evaluation%20framework%20based%20on%20both%20the%20reference%20image%20and%20the%20combination%20of%20the%20original%20image%20with%20editing%20instructions%20that%20assesses%20models%27%20abilities%20in%20multi-turn%20generation%2C%20visual%20memory%2C%20and%20world-knowledge%20reasoning%20across%20diverse%20domains.%20Experiments%20demonstrate%20that%20training%20on%20WEAVE-100k%20enables%20vision%20comprehension%2C%20image%20editing%2C%20and%20comprehension-generation%20collaboration%20capabilities.%20Furthermore%2C%20it%20facilitates%20UMMs%20to%20develop%20emergent%20visual-memory%20capabilities%2C%20while%20extensive%20evaluations%20on%20WEAVEBench%20expose%20the%20persistent%20limitations%20and%20challenges%20of%20current%20approaches%20in%20multi-turn%2C%20context-aware%20image%20generation%20and%20editing.%20We%20believe%20WEAVE%20provides%20a%20view%20and%20foundation%20for%20studying%20in-context%20interleaved%20comprehension%20and%20generation%20for%20multi-modal%20community.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWEAVE%253A%2520Unleashing%2520and%2520Benchmarking%2520the%2520In-context%2520Interleaved%2520Comprehension%2520and%2520Generation%26entry.906535625%3DWei%2520Chow%2520and%2520Jiachun%2520Pan%2520and%2520Yongyuan%2520Liang%2520and%2520Mingze%2520Zhou%2520and%2520Xue%2520Song%2520and%2520Liyu%2520Jia%2520and%2520Saining%2520Zhang%2520and%2520Siliang%2520Tang%2520and%2520Juncheng%2520Li%2520and%2520Fengda%2520Zhang%2520and%2520Weijia%2520Wu%2520and%2520Hanwang%2520Zhang%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3DRecent%2520advances%2520in%2520unified%2520multimodal%2520models%2520%2528UMMs%2529%2520have%2520enabled%2520impressive%2520progress%2520in%2520visual%2520comprehension%2520and%2520generation.%2520However%252C%2520existing%2520datasets%2520and%2520benchmarks%2520focus%2520primarily%2520on%2520single-turn%2520interactions%252C%2520failing%2520to%2520capture%2520the%2520multi-turn%252C%2520context-dependent%2520nature%2520of%2520real-world%2520image%2520creation%2520and%2520editing.%2520To%2520address%2520this%2520gap%252C%2520we%2520present%2520WEAVE%252C%2520the%2520first%2520suite%2520for%2520in-context%2520interleaved%2520cross-modality%2520comprehension%2520and%2520generation.%2520Our%2520suite%2520consists%2520of%2520two%2520complementary%2520parts.%2520WEAVE-100k%2520is%2520a%2520large-scale%2520dataset%2520of%2520100K%2520interleaved%2520samples%2520spanning%2520over%2520370K%2520dialogue%2520turns%2520and%2520500K%2520images%252C%2520covering%2520comprehension%252C%2520editing%252C%2520and%2520generation%2520tasks%2520that%2520require%2520reasoning%2520over%2520historical%2520context.%2520WEAVEBench%2520is%2520a%2520human-annotated%2520benchmark%2520with%2520100%2520tasks%2520based%2520on%2520480%2520images%252C%2520featuring%2520a%2520hybrid%2520VLM%2520judger%2520evaluation%2520framework%2520based%2520on%2520both%2520the%2520reference%2520image%2520and%2520the%2520combination%2520of%2520the%2520original%2520image%2520with%2520editing%2520instructions%2520that%2520assesses%2520models%2527%2520abilities%2520in%2520multi-turn%2520generation%252C%2520visual%2520memory%252C%2520and%2520world-knowledge%2520reasoning%2520across%2520diverse%2520domains.%2520Experiments%2520demonstrate%2520that%2520training%2520on%2520WEAVE-100k%2520enables%2520vision%2520comprehension%252C%2520image%2520editing%252C%2520and%2520comprehension-generation%2520collaboration%2520capabilities.%2520Furthermore%252C%2520it%2520facilitates%2520UMMs%2520to%2520develop%2520emergent%2520visual-memory%2520capabilities%252C%2520while%2520extensive%2520evaluations%2520on%2520WEAVEBench%2520expose%2520the%2520persistent%2520limitations%2520and%2520challenges%2520of%2520current%2520approaches%2520in%2520multi-turn%252C%2520context-aware%2520image%2520generation%2520and%2520editing.%2520We%2520believe%2520WEAVE%2520provides%2520a%2520view%2520and%2520foundation%2520for%2520studying%2520in-context%2520interleaved%2520comprehension%2520and%2520generation%2520for%2520multi-modal%2520community.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WEAVE%3A%20Unleashing%20and%20Benchmarking%20the%20In-context%20Interleaved%20Comprehension%20and%20Generation&entry.906535625=Wei%20Chow%20and%20Jiachun%20Pan%20and%20Yongyuan%20Liang%20and%20Mingze%20Zhou%20and%20Xue%20Song%20and%20Liyu%20Jia%20and%20Saining%20Zhang%20and%20Siliang%20Tang%20and%20Juncheng%20Li%20and%20Fengda%20Zhang%20and%20Weijia%20Wu%20and%20Hanwang%20Zhang%20and%20Tat-Seng%20Chua&entry.1292438233=Recent%20advances%20in%20unified%20multimodal%20models%20%28UMMs%29%20have%20enabled%20impressive%20progress%20in%20visual%20comprehension%20and%20generation.%20However%2C%20existing%20datasets%20and%20benchmarks%20focus%20primarily%20on%20single-turn%20interactions%2C%20failing%20to%20capture%20the%20multi-turn%2C%20context-dependent%20nature%20of%20real-world%20image%20creation%20and%20editing.%20To%20address%20this%20gap%2C%20we%20present%20WEAVE%2C%20the%20first%20suite%20for%20in-context%20interleaved%20cross-modality%20comprehension%20and%20generation.%20Our%20suite%20consists%20of%20two%20complementary%20parts.%20WEAVE-100k%20is%20a%20large-scale%20dataset%20of%20100K%20interleaved%20samples%20spanning%20over%20370K%20dialogue%20turns%20and%20500K%20images%2C%20covering%20comprehension%2C%20editing%2C%20and%20generation%20tasks%20that%20require%20reasoning%20over%20historical%20context.%20WEAVEBench%20is%20a%20human-annotated%20benchmark%20with%20100%20tasks%20based%20on%20480%20images%2C%20featuring%20a%20hybrid%20VLM%20judger%20evaluation%20framework%20based%20on%20both%20the%20reference%20image%20and%20the%20combination%20of%20the%20original%20image%20with%20editing%20instructions%20that%20assesses%20models%27%20abilities%20in%20multi-turn%20generation%2C%20visual%20memory%2C%20and%20world-knowledge%20reasoning%20across%20diverse%20domains.%20Experiments%20demonstrate%20that%20training%20on%20WEAVE-100k%20enables%20vision%20comprehension%2C%20image%20editing%2C%20and%20comprehension-generation%20collaboration%20capabilities.%20Furthermore%2C%20it%20facilitates%20UMMs%20to%20develop%20emergent%20visual-memory%20capabilities%2C%20while%20extensive%20evaluations%20on%20WEAVEBench%20expose%20the%20persistent%20limitations%20and%20challenges%20of%20current%20approaches%20in%20multi-turn%2C%20context-aware%20image%20generation%20and%20editing.%20We%20believe%20WEAVE%20provides%20a%20view%20and%20foundation%20for%20studying%20in-context%20interleaved%20comprehension%20and%20generation%20for%20multi-modal%20community.&entry.1838667208=http%3A//arxiv.org/abs/2511.11434v1&entry.124074799=Read"},
{"title": "Dynamic Gaussian Scene Reconstruction from Unsynchronized Videos", "author": "Zhixin Xu and Hengyu Zhou and Yuan Liu and Wenhan Xue and Hao Pan and Wenping Wang and Bin Wang", "abstract": "Multi-view video reconstruction plays a vital role in computer vision, enabling applications in film production, virtual reality, and motion analysis. While recent advances such as 4D Gaussian Splatting (4DGS) have demonstrated impressive capabilities in dynamic scene reconstruction, they typically rely on the assumption that input video streams are temporally synchronized. However, in real-world scenarios, this assumption often fails due to factors like camera trigger delays or independent recording setups, leading to temporal misalignment across views and reduced reconstruction quality. To address this challenge, a novel temporal alignment strategy is proposed for high-quality 4DGS reconstruction from unsynchronized multi-view videos. Our method features a coarse-to-fine alignment module that estimates and compensates for each camera's time shift. The method first determines a coarse, frame-level offset and then refines it to achieve sub-frame accuracy. This strategy can be integrated as a readily integrable module into existing 4DGS frameworks, enhancing their robustness when handling asynchronous data. Experiments show that our approach effectively processes temporally misaligned videos and significantly enhances baseline methods.", "link": "http://arxiv.org/abs/2511.11175v1", "date": "2025-11-14", "relevancy": 2.7907, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7482}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6772}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Gaussian%20Scene%20Reconstruction%20from%20Unsynchronized%20Videos&body=Title%3A%20Dynamic%20Gaussian%20Scene%20Reconstruction%20from%20Unsynchronized%20Videos%0AAuthor%3A%20Zhixin%20Xu%20and%20Hengyu%20Zhou%20and%20Yuan%20Liu%20and%20Wenhan%20Xue%20and%20Hao%20Pan%20and%20Wenping%20Wang%20and%20Bin%20Wang%0AAbstract%3A%20Multi-view%20video%20reconstruction%20plays%20a%20vital%20role%20in%20computer%20vision%2C%20enabling%20applications%20in%20film%20production%2C%20virtual%20reality%2C%20and%20motion%20analysis.%20While%20recent%20advances%20such%20as%204D%20Gaussian%20Splatting%20%284DGS%29%20have%20demonstrated%20impressive%20capabilities%20in%20dynamic%20scene%20reconstruction%2C%20they%20typically%20rely%20on%20the%20assumption%20that%20input%20video%20streams%20are%20temporally%20synchronized.%20However%2C%20in%20real-world%20scenarios%2C%20this%20assumption%20often%20fails%20due%20to%20factors%20like%20camera%20trigger%20delays%20or%20independent%20recording%20setups%2C%20leading%20to%20temporal%20misalignment%20across%20views%20and%20reduced%20reconstruction%20quality.%20To%20address%20this%20challenge%2C%20a%20novel%20temporal%20alignment%20strategy%20is%20proposed%20for%20high-quality%204DGS%20reconstruction%20from%20unsynchronized%20multi-view%20videos.%20Our%20method%20features%20a%20coarse-to-fine%20alignment%20module%20that%20estimates%20and%20compensates%20for%20each%20camera%27s%20time%20shift.%20The%20method%20first%20determines%20a%20coarse%2C%20frame-level%20offset%20and%20then%20refines%20it%20to%20achieve%20sub-frame%20accuracy.%20This%20strategy%20can%20be%20integrated%20as%20a%20readily%20integrable%20module%20into%20existing%204DGS%20frameworks%2C%20enhancing%20their%20robustness%20when%20handling%20asynchronous%20data.%20Experiments%20show%20that%20our%20approach%20effectively%20processes%20temporally%20misaligned%20videos%20and%20significantly%20enhances%20baseline%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Gaussian%2520Scene%2520Reconstruction%2520from%2520Unsynchronized%2520Videos%26entry.906535625%3DZhixin%2520Xu%2520and%2520Hengyu%2520Zhou%2520and%2520Yuan%2520Liu%2520and%2520Wenhan%2520Xue%2520and%2520Hao%2520Pan%2520and%2520Wenping%2520Wang%2520and%2520Bin%2520Wang%26entry.1292438233%3DMulti-view%2520video%2520reconstruction%2520plays%2520a%2520vital%2520role%2520in%2520computer%2520vision%252C%2520enabling%2520applications%2520in%2520film%2520production%252C%2520virtual%2520reality%252C%2520and%2520motion%2520analysis.%2520While%2520recent%2520advances%2520such%2520as%25204D%2520Gaussian%2520Splatting%2520%25284DGS%2529%2520have%2520demonstrated%2520impressive%2520capabilities%2520in%2520dynamic%2520scene%2520reconstruction%252C%2520they%2520typically%2520rely%2520on%2520the%2520assumption%2520that%2520input%2520video%2520streams%2520are%2520temporally%2520synchronized.%2520However%252C%2520in%2520real-world%2520scenarios%252C%2520this%2520assumption%2520often%2520fails%2520due%2520to%2520factors%2520like%2520camera%2520trigger%2520delays%2520or%2520independent%2520recording%2520setups%252C%2520leading%2520to%2520temporal%2520misalignment%2520across%2520views%2520and%2520reduced%2520reconstruction%2520quality.%2520To%2520address%2520this%2520challenge%252C%2520a%2520novel%2520temporal%2520alignment%2520strategy%2520is%2520proposed%2520for%2520high-quality%25204DGS%2520reconstruction%2520from%2520unsynchronized%2520multi-view%2520videos.%2520Our%2520method%2520features%2520a%2520coarse-to-fine%2520alignment%2520module%2520that%2520estimates%2520and%2520compensates%2520for%2520each%2520camera%2527s%2520time%2520shift.%2520The%2520method%2520first%2520determines%2520a%2520coarse%252C%2520frame-level%2520offset%2520and%2520then%2520refines%2520it%2520to%2520achieve%2520sub-frame%2520accuracy.%2520This%2520strategy%2520can%2520be%2520integrated%2520as%2520a%2520readily%2520integrable%2520module%2520into%2520existing%25204DGS%2520frameworks%252C%2520enhancing%2520their%2520robustness%2520when%2520handling%2520asynchronous%2520data.%2520Experiments%2520show%2520that%2520our%2520approach%2520effectively%2520processes%2520temporally%2520misaligned%2520videos%2520and%2520significantly%2520enhances%2520baseline%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Gaussian%20Scene%20Reconstruction%20from%20Unsynchronized%20Videos&entry.906535625=Zhixin%20Xu%20and%20Hengyu%20Zhou%20and%20Yuan%20Liu%20and%20Wenhan%20Xue%20and%20Hao%20Pan%20and%20Wenping%20Wang%20and%20Bin%20Wang&entry.1292438233=Multi-view%20video%20reconstruction%20plays%20a%20vital%20role%20in%20computer%20vision%2C%20enabling%20applications%20in%20film%20production%2C%20virtual%20reality%2C%20and%20motion%20analysis.%20While%20recent%20advances%20such%20as%204D%20Gaussian%20Splatting%20%284DGS%29%20have%20demonstrated%20impressive%20capabilities%20in%20dynamic%20scene%20reconstruction%2C%20they%20typically%20rely%20on%20the%20assumption%20that%20input%20video%20streams%20are%20temporally%20synchronized.%20However%2C%20in%20real-world%20scenarios%2C%20this%20assumption%20often%20fails%20due%20to%20factors%20like%20camera%20trigger%20delays%20or%20independent%20recording%20setups%2C%20leading%20to%20temporal%20misalignment%20across%20views%20and%20reduced%20reconstruction%20quality.%20To%20address%20this%20challenge%2C%20a%20novel%20temporal%20alignment%20strategy%20is%20proposed%20for%20high-quality%204DGS%20reconstruction%20from%20unsynchronized%20multi-view%20videos.%20Our%20method%20features%20a%20coarse-to-fine%20alignment%20module%20that%20estimates%20and%20compensates%20for%20each%20camera%27s%20time%20shift.%20The%20method%20first%20determines%20a%20coarse%2C%20frame-level%20offset%20and%20then%20refines%20it%20to%20achieve%20sub-frame%20accuracy.%20This%20strategy%20can%20be%20integrated%20as%20a%20readily%20integrable%20module%20into%20existing%204DGS%20frameworks%2C%20enhancing%20their%20robustness%20when%20handling%20asynchronous%20data.%20Experiments%20show%20that%20our%20approach%20effectively%20processes%20temporally%20misaligned%20videos%20and%20significantly%20enhances%20baseline%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.11175v1&entry.124074799=Read"},
{"title": "Latent Motion Profiling for Annotation-free Cardiac Phase Detection in Adult and Fetal Echocardiography Videos", "author": "Yingyu Yang and Qianye Yang and Kangning Cui and Can Peng and Elena D'Alberti and Netzahualcoyotl Hernandez-Cruz and Olga Patey and Aris T. Papageorghiou and J. Alison Noble", "abstract": "The identification of cardiac phase is an essential step for analysis and diagnosis of cardiac function. Automatic methods, especially data-driven methods for cardiac phase detection, typically require extensive annotations, which is time-consuming and labor-intensive. In this paper, we present an unsupervised framework for end-diastole (ED) and end-systole (ES) detection through self-supervised learning of latent cardiac motion trajectories from 4-chamber-view echocardiography videos. Our method eliminates the need for manual annotations, including ED and ES indices, segmentation, or volumetric measurements, by training a reconstruction model to encode interpretable spatiotemporal motion patterns. Evaluated on the EchoNet-Dynamic benchmark, the approach achieves mean absolute error (MAE) of 3 frames (58.3 ms) for ED and 2 frames (38.8 ms) for ES detection, matching state-of-the-art supervised methods. Extended to fetal echocardiography, the model demonstrates robust performance with MAE 1.46 frames (20.7 ms) for ED and 1.74 frames (25.3 ms) for ES, despite the fact that the fetal heart model is built using non-standardized heart views due to fetal heart positioning variability. Our results demonstrate the potential of the proposed latent motion trajectory strategy for cardiac phase detection in adult and fetal echocardiography. This work advances unsupervised cardiac motion analysis, offering a scalable solution for clinical populations lacking annotated data. Code will be released at https://github.com/YingyuYyy/CardiacPhase.", "link": "http://arxiv.org/abs/2507.05154v2", "date": "2025-11-14", "relevancy": 2.7739, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5741}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.55}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Motion%20Profiling%20for%20Annotation-free%20Cardiac%20Phase%20Detection%20in%20Adult%20and%20Fetal%20Echocardiography%20Videos&body=Title%3A%20Latent%20Motion%20Profiling%20for%20Annotation-free%20Cardiac%20Phase%20Detection%20in%20Adult%20and%20Fetal%20Echocardiography%20Videos%0AAuthor%3A%20Yingyu%20Yang%20and%20Qianye%20Yang%20and%20Kangning%20Cui%20and%20Can%20Peng%20and%20Elena%20D%27Alberti%20and%20Netzahualcoyotl%20Hernandez-Cruz%20and%20Olga%20Patey%20and%20Aris%20T.%20Papageorghiou%20and%20J.%20Alison%20Noble%0AAbstract%3A%20The%20identification%20of%20cardiac%20phase%20is%20an%20essential%20step%20for%20analysis%20and%20diagnosis%20of%20cardiac%20function.%20Automatic%20methods%2C%20especially%20data-driven%20methods%20for%20cardiac%20phase%20detection%2C%20typically%20require%20extensive%20annotations%2C%20which%20is%20time-consuming%20and%20labor-intensive.%20In%20this%20paper%2C%20we%20present%20an%20unsupervised%20framework%20for%20end-diastole%20%28ED%29%20and%20end-systole%20%28ES%29%20detection%20through%20self-supervised%20learning%20of%20latent%20cardiac%20motion%20trajectories%20from%204-chamber-view%20echocardiography%20videos.%20Our%20method%20eliminates%20the%20need%20for%20manual%20annotations%2C%20including%20ED%20and%20ES%20indices%2C%20segmentation%2C%20or%20volumetric%20measurements%2C%20by%20training%20a%20reconstruction%20model%20to%20encode%20interpretable%20spatiotemporal%20motion%20patterns.%20Evaluated%20on%20the%20EchoNet-Dynamic%20benchmark%2C%20the%20approach%20achieves%20mean%20absolute%20error%20%28MAE%29%20of%203%20frames%20%2858.3%20ms%29%20for%20ED%20and%202%20frames%20%2838.8%20ms%29%20for%20ES%20detection%2C%20matching%20state-of-the-art%20supervised%20methods.%20Extended%20to%20fetal%20echocardiography%2C%20the%20model%20demonstrates%20robust%20performance%20with%20MAE%201.46%20frames%20%2820.7%20ms%29%20for%20ED%20and%201.74%20frames%20%2825.3%20ms%29%20for%20ES%2C%20despite%20the%20fact%20that%20the%20fetal%20heart%20model%20is%20built%20using%20non-standardized%20heart%20views%20due%20to%20fetal%20heart%20positioning%20variability.%20Our%20results%20demonstrate%20the%20potential%20of%20the%20proposed%20latent%20motion%20trajectory%20strategy%20for%20cardiac%20phase%20detection%20in%20adult%20and%20fetal%20echocardiography.%20This%20work%20advances%20unsupervised%20cardiac%20motion%20analysis%2C%20offering%20a%20scalable%20solution%20for%20clinical%20populations%20lacking%20annotated%20data.%20Code%20will%20be%20released%20at%20https%3A//github.com/YingyuYyy/CardiacPhase.%0ALink%3A%20http%3A//arxiv.org/abs/2507.05154v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Motion%2520Profiling%2520for%2520Annotation-free%2520Cardiac%2520Phase%2520Detection%2520in%2520Adult%2520and%2520Fetal%2520Echocardiography%2520Videos%26entry.906535625%3DYingyu%2520Yang%2520and%2520Qianye%2520Yang%2520and%2520Kangning%2520Cui%2520and%2520Can%2520Peng%2520and%2520Elena%2520D%2527Alberti%2520and%2520Netzahualcoyotl%2520Hernandez-Cruz%2520and%2520Olga%2520Patey%2520and%2520Aris%2520T.%2520Papageorghiou%2520and%2520J.%2520Alison%2520Noble%26entry.1292438233%3DThe%2520identification%2520of%2520cardiac%2520phase%2520is%2520an%2520essential%2520step%2520for%2520analysis%2520and%2520diagnosis%2520of%2520cardiac%2520function.%2520Automatic%2520methods%252C%2520especially%2520data-driven%2520methods%2520for%2520cardiac%2520phase%2520detection%252C%2520typically%2520require%2520extensive%2520annotations%252C%2520which%2520is%2520time-consuming%2520and%2520labor-intensive.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520unsupervised%2520framework%2520for%2520end-diastole%2520%2528ED%2529%2520and%2520end-systole%2520%2528ES%2529%2520detection%2520through%2520self-supervised%2520learning%2520of%2520latent%2520cardiac%2520motion%2520trajectories%2520from%25204-chamber-view%2520echocardiography%2520videos.%2520Our%2520method%2520eliminates%2520the%2520need%2520for%2520manual%2520annotations%252C%2520including%2520ED%2520and%2520ES%2520indices%252C%2520segmentation%252C%2520or%2520volumetric%2520measurements%252C%2520by%2520training%2520a%2520reconstruction%2520model%2520to%2520encode%2520interpretable%2520spatiotemporal%2520motion%2520patterns.%2520Evaluated%2520on%2520the%2520EchoNet-Dynamic%2520benchmark%252C%2520the%2520approach%2520achieves%2520mean%2520absolute%2520error%2520%2528MAE%2529%2520of%25203%2520frames%2520%252858.3%2520ms%2529%2520for%2520ED%2520and%25202%2520frames%2520%252838.8%2520ms%2529%2520for%2520ES%2520detection%252C%2520matching%2520state-of-the-art%2520supervised%2520methods.%2520Extended%2520to%2520fetal%2520echocardiography%252C%2520the%2520model%2520demonstrates%2520robust%2520performance%2520with%2520MAE%25201.46%2520frames%2520%252820.7%2520ms%2529%2520for%2520ED%2520and%25201.74%2520frames%2520%252825.3%2520ms%2529%2520for%2520ES%252C%2520despite%2520the%2520fact%2520that%2520the%2520fetal%2520heart%2520model%2520is%2520built%2520using%2520non-standardized%2520heart%2520views%2520due%2520to%2520fetal%2520heart%2520positioning%2520variability.%2520Our%2520results%2520demonstrate%2520the%2520potential%2520of%2520the%2520proposed%2520latent%2520motion%2520trajectory%2520strategy%2520for%2520cardiac%2520phase%2520detection%2520in%2520adult%2520and%2520fetal%2520echocardiography.%2520This%2520work%2520advances%2520unsupervised%2520cardiac%2520motion%2520analysis%252C%2520offering%2520a%2520scalable%2520solution%2520for%2520clinical%2520populations%2520lacking%2520annotated%2520data.%2520Code%2520will%2520be%2520released%2520at%2520https%253A//github.com/YingyuYyy/CardiacPhase.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05154v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Motion%20Profiling%20for%20Annotation-free%20Cardiac%20Phase%20Detection%20in%20Adult%20and%20Fetal%20Echocardiography%20Videos&entry.906535625=Yingyu%20Yang%20and%20Qianye%20Yang%20and%20Kangning%20Cui%20and%20Can%20Peng%20and%20Elena%20D%27Alberti%20and%20Netzahualcoyotl%20Hernandez-Cruz%20and%20Olga%20Patey%20and%20Aris%20T.%20Papageorghiou%20and%20J.%20Alison%20Noble&entry.1292438233=The%20identification%20of%20cardiac%20phase%20is%20an%20essential%20step%20for%20analysis%20and%20diagnosis%20of%20cardiac%20function.%20Automatic%20methods%2C%20especially%20data-driven%20methods%20for%20cardiac%20phase%20detection%2C%20typically%20require%20extensive%20annotations%2C%20which%20is%20time-consuming%20and%20labor-intensive.%20In%20this%20paper%2C%20we%20present%20an%20unsupervised%20framework%20for%20end-diastole%20%28ED%29%20and%20end-systole%20%28ES%29%20detection%20through%20self-supervised%20learning%20of%20latent%20cardiac%20motion%20trajectories%20from%204-chamber-view%20echocardiography%20videos.%20Our%20method%20eliminates%20the%20need%20for%20manual%20annotations%2C%20including%20ED%20and%20ES%20indices%2C%20segmentation%2C%20or%20volumetric%20measurements%2C%20by%20training%20a%20reconstruction%20model%20to%20encode%20interpretable%20spatiotemporal%20motion%20patterns.%20Evaluated%20on%20the%20EchoNet-Dynamic%20benchmark%2C%20the%20approach%20achieves%20mean%20absolute%20error%20%28MAE%29%20of%203%20frames%20%2858.3%20ms%29%20for%20ED%20and%202%20frames%20%2838.8%20ms%29%20for%20ES%20detection%2C%20matching%20state-of-the-art%20supervised%20methods.%20Extended%20to%20fetal%20echocardiography%2C%20the%20model%20demonstrates%20robust%20performance%20with%20MAE%201.46%20frames%20%2820.7%20ms%29%20for%20ED%20and%201.74%20frames%20%2825.3%20ms%29%20for%20ES%2C%20despite%20the%20fact%20that%20the%20fetal%20heart%20model%20is%20built%20using%20non-standardized%20heart%20views%20due%20to%20fetal%20heart%20positioning%20variability.%20Our%20results%20demonstrate%20the%20potential%20of%20the%20proposed%20latent%20motion%20trajectory%20strategy%20for%20cardiac%20phase%20detection%20in%20adult%20and%20fetal%20echocardiography.%20This%20work%20advances%20unsupervised%20cardiac%20motion%20analysis%2C%20offering%20a%20scalable%20solution%20for%20clinical%20populations%20lacking%20annotated%20data.%20Code%20will%20be%20released%20at%20https%3A//github.com/YingyuYyy/CardiacPhase.&entry.1838667208=http%3A//arxiv.org/abs/2507.05154v2&entry.124074799=Read"},
{"title": "MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising", "author": "Chenghan Fu and Daoze Zhang and Yukang Lin and Zhanheng Nie and Xiang Zhang and Jianyu Liu and Yueran Liu and Wanxian Guan and Pengjie Wang and Jian Xu and Bo Zheng", "abstract": "We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications. MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on. The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement. Over the past three years, this project has delivered the largest improvement on CTR prediction task and undergone five full-scale iterations. Throughout the exploration and iteration of our MOON, we have accumulated valuable insights and practical experience that we believe will benefit the research community. MOON contains a three-stage training paradigm of \"Pretraining, Post-training, and Application\", allowing effective integration of multimodal representations with downstream tasks. Notably, to bridge the misalignment between the objectives of multimodal representation learning and downstream training, we define the exchange rate to quantify how effectively improvements in an intermediate metric can translate into downstream gains. Through this analysis, we identify the image-based search recall as a critical intermediate metric guiding the optimization of multimodal models. Over three years and five iterations, MOON has evolved along four critical dimensions: data processing, training strategy, model architecture, and downstream application. The lessons and insights gained through the iterative improvements will also be shared. As part of our exploration into scaling effects in the e-commerce field, we further conduct a systematic study of the scaling laws governing multimodal representation learning, examining multiple factors such as the number of training tokens, negative samples, and the length of user behavior sequences.", "link": "http://arxiv.org/abs/2511.11305v1", "date": "2025-11-14", "relevancy": 2.7592, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6013}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5311}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOON%20Embedding%3A%20Multimodal%20Representation%20Learning%20for%20E-commerce%20Search%20Advertising&body=Title%3A%20MOON%20Embedding%3A%20Multimodal%20Representation%20Learning%20for%20E-commerce%20Search%20Advertising%0AAuthor%3A%20Chenghan%20Fu%20and%20Daoze%20Zhang%20and%20Yukang%20Lin%20and%20Zhanheng%20Nie%20and%20Xiang%20Zhang%20and%20Jianyu%20Liu%20and%20Yueran%20Liu%20and%20Wanxian%20Guan%20and%20Pengjie%20Wang%20and%20Jian%20Xu%20and%20Bo%20Zheng%0AAbstract%3A%20We%20introduce%20MOON%2C%20our%20comprehensive%20set%20of%20sustainable%20iterative%20practices%20for%20multimodal%20representation%20learning%20for%20e-commerce%20applications.%20MOON%20has%20already%20been%20fully%20deployed%20across%20all%20stages%20of%20Taobao%20search%20advertising%20system%2C%20including%20retrieval%2C%20relevance%2C%20ranking%2C%20and%20so%20on.%20The%20performance%20gains%20are%20particularly%20significant%20on%20click-through%20rate%20%28CTR%29%20prediction%20task%2C%20which%20achieves%20an%20overall%20%2B20.00%25%20online%20CTR%20improvement.%20Over%20the%20past%20three%20years%2C%20this%20project%20has%20delivered%20the%20largest%20improvement%20on%20CTR%20prediction%20task%20and%20undergone%20five%20full-scale%20iterations.%20Throughout%20the%20exploration%20and%20iteration%20of%20our%20MOON%2C%20we%20have%20accumulated%20valuable%20insights%20and%20practical%20experience%20that%20we%20believe%20will%20benefit%20the%20research%20community.%20MOON%20contains%20a%20three-stage%20training%20paradigm%20of%20%22Pretraining%2C%20Post-training%2C%20and%20Application%22%2C%20allowing%20effective%20integration%20of%20multimodal%20representations%20with%20downstream%20tasks.%20Notably%2C%20to%20bridge%20the%20misalignment%20between%20the%20objectives%20of%20multimodal%20representation%20learning%20and%20downstream%20training%2C%20we%20define%20the%20exchange%20rate%20to%20quantify%20how%20effectively%20improvements%20in%20an%20intermediate%20metric%20can%20translate%20into%20downstream%20gains.%20Through%20this%20analysis%2C%20we%20identify%20the%20image-based%20search%20recall%20as%20a%20critical%20intermediate%20metric%20guiding%20the%20optimization%20of%20multimodal%20models.%20Over%20three%20years%20and%20five%20iterations%2C%20MOON%20has%20evolved%20along%20four%20critical%20dimensions%3A%20data%20processing%2C%20training%20strategy%2C%20model%20architecture%2C%20and%20downstream%20application.%20The%20lessons%20and%20insights%20gained%20through%20the%20iterative%20improvements%20will%20also%20be%20shared.%20As%20part%20of%20our%20exploration%20into%20scaling%20effects%20in%20the%20e-commerce%20field%2C%20we%20further%20conduct%20a%20systematic%20study%20of%20the%20scaling%20laws%20governing%20multimodal%20representation%20learning%2C%20examining%20multiple%20factors%20such%20as%20the%20number%20of%20training%20tokens%2C%20negative%20samples%2C%20and%20the%20length%20of%20user%20behavior%20sequences.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOON%2520Embedding%253A%2520Multimodal%2520Representation%2520Learning%2520for%2520E-commerce%2520Search%2520Advertising%26entry.906535625%3DChenghan%2520Fu%2520and%2520Daoze%2520Zhang%2520and%2520Yukang%2520Lin%2520and%2520Zhanheng%2520Nie%2520and%2520Xiang%2520Zhang%2520and%2520Jianyu%2520Liu%2520and%2520Yueran%2520Liu%2520and%2520Wanxian%2520Guan%2520and%2520Pengjie%2520Wang%2520and%2520Jian%2520Xu%2520and%2520Bo%2520Zheng%26entry.1292438233%3DWe%2520introduce%2520MOON%252C%2520our%2520comprehensive%2520set%2520of%2520sustainable%2520iterative%2520practices%2520for%2520multimodal%2520representation%2520learning%2520for%2520e-commerce%2520applications.%2520MOON%2520has%2520already%2520been%2520fully%2520deployed%2520across%2520all%2520stages%2520of%2520Taobao%2520search%2520advertising%2520system%252C%2520including%2520retrieval%252C%2520relevance%252C%2520ranking%252C%2520and%2520so%2520on.%2520The%2520performance%2520gains%2520are%2520particularly%2520significant%2520on%2520click-through%2520rate%2520%2528CTR%2529%2520prediction%2520task%252C%2520which%2520achieves%2520an%2520overall%2520%252B20.00%2525%2520online%2520CTR%2520improvement.%2520Over%2520the%2520past%2520three%2520years%252C%2520this%2520project%2520has%2520delivered%2520the%2520largest%2520improvement%2520on%2520CTR%2520prediction%2520task%2520and%2520undergone%2520five%2520full-scale%2520iterations.%2520Throughout%2520the%2520exploration%2520and%2520iteration%2520of%2520our%2520MOON%252C%2520we%2520have%2520accumulated%2520valuable%2520insights%2520and%2520practical%2520experience%2520that%2520we%2520believe%2520will%2520benefit%2520the%2520research%2520community.%2520MOON%2520contains%2520a%2520three-stage%2520training%2520paradigm%2520of%2520%2522Pretraining%252C%2520Post-training%252C%2520and%2520Application%2522%252C%2520allowing%2520effective%2520integration%2520of%2520multimodal%2520representations%2520with%2520downstream%2520tasks.%2520Notably%252C%2520to%2520bridge%2520the%2520misalignment%2520between%2520the%2520objectives%2520of%2520multimodal%2520representation%2520learning%2520and%2520downstream%2520training%252C%2520we%2520define%2520the%2520exchange%2520rate%2520to%2520quantify%2520how%2520effectively%2520improvements%2520in%2520an%2520intermediate%2520metric%2520can%2520translate%2520into%2520downstream%2520gains.%2520Through%2520this%2520analysis%252C%2520we%2520identify%2520the%2520image-based%2520search%2520recall%2520as%2520a%2520critical%2520intermediate%2520metric%2520guiding%2520the%2520optimization%2520of%2520multimodal%2520models.%2520Over%2520three%2520years%2520and%2520five%2520iterations%252C%2520MOON%2520has%2520evolved%2520along%2520four%2520critical%2520dimensions%253A%2520data%2520processing%252C%2520training%2520strategy%252C%2520model%2520architecture%252C%2520and%2520downstream%2520application.%2520The%2520lessons%2520and%2520insights%2520gained%2520through%2520the%2520iterative%2520improvements%2520will%2520also%2520be%2520shared.%2520As%2520part%2520of%2520our%2520exploration%2520into%2520scaling%2520effects%2520in%2520the%2520e-commerce%2520field%252C%2520we%2520further%2520conduct%2520a%2520systematic%2520study%2520of%2520the%2520scaling%2520laws%2520governing%2520multimodal%2520representation%2520learning%252C%2520examining%2520multiple%2520factors%2520such%2520as%2520the%2520number%2520of%2520training%2520tokens%252C%2520negative%2520samples%252C%2520and%2520the%2520length%2520of%2520user%2520behavior%2520sequences.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOON%20Embedding%3A%20Multimodal%20Representation%20Learning%20for%20E-commerce%20Search%20Advertising&entry.906535625=Chenghan%20Fu%20and%20Daoze%20Zhang%20and%20Yukang%20Lin%20and%20Zhanheng%20Nie%20and%20Xiang%20Zhang%20and%20Jianyu%20Liu%20and%20Yueran%20Liu%20and%20Wanxian%20Guan%20and%20Pengjie%20Wang%20and%20Jian%20Xu%20and%20Bo%20Zheng&entry.1292438233=We%20introduce%20MOON%2C%20our%20comprehensive%20set%20of%20sustainable%20iterative%20practices%20for%20multimodal%20representation%20learning%20for%20e-commerce%20applications.%20MOON%20has%20already%20been%20fully%20deployed%20across%20all%20stages%20of%20Taobao%20search%20advertising%20system%2C%20including%20retrieval%2C%20relevance%2C%20ranking%2C%20and%20so%20on.%20The%20performance%20gains%20are%20particularly%20significant%20on%20click-through%20rate%20%28CTR%29%20prediction%20task%2C%20which%20achieves%20an%20overall%20%2B20.00%25%20online%20CTR%20improvement.%20Over%20the%20past%20three%20years%2C%20this%20project%20has%20delivered%20the%20largest%20improvement%20on%20CTR%20prediction%20task%20and%20undergone%20five%20full-scale%20iterations.%20Throughout%20the%20exploration%20and%20iteration%20of%20our%20MOON%2C%20we%20have%20accumulated%20valuable%20insights%20and%20practical%20experience%20that%20we%20believe%20will%20benefit%20the%20research%20community.%20MOON%20contains%20a%20three-stage%20training%20paradigm%20of%20%22Pretraining%2C%20Post-training%2C%20and%20Application%22%2C%20allowing%20effective%20integration%20of%20multimodal%20representations%20with%20downstream%20tasks.%20Notably%2C%20to%20bridge%20the%20misalignment%20between%20the%20objectives%20of%20multimodal%20representation%20learning%20and%20downstream%20training%2C%20we%20define%20the%20exchange%20rate%20to%20quantify%20how%20effectively%20improvements%20in%20an%20intermediate%20metric%20can%20translate%20into%20downstream%20gains.%20Through%20this%20analysis%2C%20we%20identify%20the%20image-based%20search%20recall%20as%20a%20critical%20intermediate%20metric%20guiding%20the%20optimization%20of%20multimodal%20models.%20Over%20three%20years%20and%20five%20iterations%2C%20MOON%20has%20evolved%20along%20four%20critical%20dimensions%3A%20data%20processing%2C%20training%20strategy%2C%20model%20architecture%2C%20and%20downstream%20application.%20The%20lessons%20and%20insights%20gained%20through%20the%20iterative%20improvements%20will%20also%20be%20shared.%20As%20part%20of%20our%20exploration%20into%20scaling%20effects%20in%20the%20e-commerce%20field%2C%20we%20further%20conduct%20a%20systematic%20study%20of%20the%20scaling%20laws%20governing%20multimodal%20representation%20learning%2C%20examining%20multiple%20factors%20such%20as%20the%20number%20of%20training%20tokens%2C%20negative%20samples%2C%20and%20the%20length%20of%20user%20behavior%20sequences.&entry.1838667208=http%3A//arxiv.org/abs/2511.11305v1&entry.124074799=Read"},
{"title": "Dynamic Deep Graph Learning for Incomplete Multi-View Clustering with Masked Graph Reconstruction Loss", "author": "Zhenghao Zhang and Jun Xie and Xingchen Chen and Tao Yu and Hongzhu Yi and Kaixin Xu and Yuanxiang Wang and Tianyu Zong and Xinming Wang and Jiahuan Chen and Guoqing Chao and Feng Chen and Zhepeng Wang and Jungang Xu", "abstract": "The prevalence of real-world multi-view data makes incomplete multi-view clustering (IMVC) a crucial research. The rapid development of Graph Neural Networks (GNNs) has established them as one of the mainstream approaches for multi-view clustering. Despite significant progress in GNNs-based IMVC, some challenges remain: (1) Most methods rely on the K-Nearest Neighbors (KNN) algorithm to construct static graphs from raw data, which introduces noise and diminishes the robustness of the graph topology. (2) Existing methods typically utilize the Mean Squared Error (MSE) loss between the reconstructed graph and the sparse adjacency graph directly as the graph reconstruction loss, leading to substantial gradient noise during optimization. To address these issues, we propose a novel \\textbf{D}ynamic Deep \\textbf{G}raph Learning for \\textbf{I}ncomplete \\textbf{M}ulti-\\textbf{V}iew \\textbf{C}lustering with \\textbf{M}asked Graph Reconstruction Loss (DGIMVCM). Firstly, we construct a missing-robust global graph from the raw data. A graph convolutional embedding layer is then designed to extract primary features and refined dynamic view-specific graph structures, leveraging the global graph for imputation of missing views. This process is complemented by graph structure contrastive learning, which identifies consistency among view-specific graph structures. Secondly, a graph self-attention encoder is introduced to extract high-level representations based on the imputed primary features and view-specific graphs, and is optimized with a masked graph reconstruction loss to mitigate gradient noise during optimization. Finally, a clustering module is constructed and optimized through a pseudo-label self-supervised training mechanism. Extensive experiments on multiple datasets validate the effectiveness and superiority of DGIMVCM.", "link": "http://arxiv.org/abs/2511.11181v1", "date": "2025-11-14", "relevancy": 2.7502, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.591}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5344}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Deep%20Graph%20Learning%20for%20Incomplete%20Multi-View%20Clustering%20with%20Masked%20Graph%20Reconstruction%20Loss&body=Title%3A%20Dynamic%20Deep%20Graph%20Learning%20for%20Incomplete%20Multi-View%20Clustering%20with%20Masked%20Graph%20Reconstruction%20Loss%0AAuthor%3A%20Zhenghao%20Zhang%20and%20Jun%20Xie%20and%20Xingchen%20Chen%20and%20Tao%20Yu%20and%20Hongzhu%20Yi%20and%20Kaixin%20Xu%20and%20Yuanxiang%20Wang%20and%20Tianyu%20Zong%20and%20Xinming%20Wang%20and%20Jiahuan%20Chen%20and%20Guoqing%20Chao%20and%20Feng%20Chen%20and%20Zhepeng%20Wang%20and%20Jungang%20Xu%0AAbstract%3A%20The%20prevalence%20of%20real-world%20multi-view%20data%20makes%20incomplete%20multi-view%20clustering%20%28IMVC%29%20a%20crucial%20research.%20The%20rapid%20development%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20has%20established%20them%20as%20one%20of%20the%20mainstream%20approaches%20for%20multi-view%20clustering.%20Despite%20significant%20progress%20in%20GNNs-based%20IMVC%2C%20some%20challenges%20remain%3A%20%281%29%20Most%20methods%20rely%20on%20the%20K-Nearest%20Neighbors%20%28KNN%29%20algorithm%20to%20construct%20static%20graphs%20from%20raw%20data%2C%20which%20introduces%20noise%20and%20diminishes%20the%20robustness%20of%20the%20graph%20topology.%20%282%29%20Existing%20methods%20typically%20utilize%20the%20Mean%20Squared%20Error%20%28MSE%29%20loss%20between%20the%20reconstructed%20graph%20and%20the%20sparse%20adjacency%20graph%20directly%20as%20the%20graph%20reconstruction%20loss%2C%20leading%20to%20substantial%20gradient%20noise%20during%20optimization.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20%5Ctextbf%7BD%7Dynamic%20Deep%20%5Ctextbf%7BG%7Draph%20Learning%20for%20%5Ctextbf%7BI%7Dncomplete%20%5Ctextbf%7BM%7Dulti-%5Ctextbf%7BV%7Diew%20%5Ctextbf%7BC%7Dlustering%20with%20%5Ctextbf%7BM%7Dasked%20Graph%20Reconstruction%20Loss%20%28DGIMVCM%29.%20Firstly%2C%20we%20construct%20a%20missing-robust%20global%20graph%20from%20the%20raw%20data.%20A%20graph%20convolutional%20embedding%20layer%20is%20then%20designed%20to%20extract%20primary%20features%20and%20refined%20dynamic%20view-specific%20graph%20structures%2C%20leveraging%20the%20global%20graph%20for%20imputation%20of%20missing%20views.%20This%20process%20is%20complemented%20by%20graph%20structure%20contrastive%20learning%2C%20which%20identifies%20consistency%20among%20view-specific%20graph%20structures.%20Secondly%2C%20a%20graph%20self-attention%20encoder%20is%20introduced%20to%20extract%20high-level%20representations%20based%20on%20the%20imputed%20primary%20features%20and%20view-specific%20graphs%2C%20and%20is%20optimized%20with%20a%20masked%20graph%20reconstruction%20loss%20to%20mitigate%20gradient%20noise%20during%20optimization.%20Finally%2C%20a%20clustering%20module%20is%20constructed%20and%20optimized%20through%20a%20pseudo-label%20self-supervised%20training%20mechanism.%20Extensive%20experiments%20on%20multiple%20datasets%20validate%20the%20effectiveness%20and%20superiority%20of%20DGIMVCM.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Deep%2520Graph%2520Learning%2520for%2520Incomplete%2520Multi-View%2520Clustering%2520with%2520Masked%2520Graph%2520Reconstruction%2520Loss%26entry.906535625%3DZhenghao%2520Zhang%2520and%2520Jun%2520Xie%2520and%2520Xingchen%2520Chen%2520and%2520Tao%2520Yu%2520and%2520Hongzhu%2520Yi%2520and%2520Kaixin%2520Xu%2520and%2520Yuanxiang%2520Wang%2520and%2520Tianyu%2520Zong%2520and%2520Xinming%2520Wang%2520and%2520Jiahuan%2520Chen%2520and%2520Guoqing%2520Chao%2520and%2520Feng%2520Chen%2520and%2520Zhepeng%2520Wang%2520and%2520Jungang%2520Xu%26entry.1292438233%3DThe%2520prevalence%2520of%2520real-world%2520multi-view%2520data%2520makes%2520incomplete%2520multi-view%2520clustering%2520%2528IMVC%2529%2520a%2520crucial%2520research.%2520The%2520rapid%2520development%2520of%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520has%2520established%2520them%2520as%2520one%2520of%2520the%2520mainstream%2520approaches%2520for%2520multi-view%2520clustering.%2520Despite%2520significant%2520progress%2520in%2520GNNs-based%2520IMVC%252C%2520some%2520challenges%2520remain%253A%2520%25281%2529%2520Most%2520methods%2520rely%2520on%2520the%2520K-Nearest%2520Neighbors%2520%2528KNN%2529%2520algorithm%2520to%2520construct%2520static%2520graphs%2520from%2520raw%2520data%252C%2520which%2520introduces%2520noise%2520and%2520diminishes%2520the%2520robustness%2520of%2520the%2520graph%2520topology.%2520%25282%2529%2520Existing%2520methods%2520typically%2520utilize%2520the%2520Mean%2520Squared%2520Error%2520%2528MSE%2529%2520loss%2520between%2520the%2520reconstructed%2520graph%2520and%2520the%2520sparse%2520adjacency%2520graph%2520directly%2520as%2520the%2520graph%2520reconstruction%2520loss%252C%2520leading%2520to%2520substantial%2520gradient%2520noise%2520during%2520optimization.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520%255Ctextbf%257BD%257Dynamic%2520Deep%2520%255Ctextbf%257BG%257Draph%2520Learning%2520for%2520%255Ctextbf%257BI%257Dncomplete%2520%255Ctextbf%257BM%257Dulti-%255Ctextbf%257BV%257Diew%2520%255Ctextbf%257BC%257Dlustering%2520with%2520%255Ctextbf%257BM%257Dasked%2520Graph%2520Reconstruction%2520Loss%2520%2528DGIMVCM%2529.%2520Firstly%252C%2520we%2520construct%2520a%2520missing-robust%2520global%2520graph%2520from%2520the%2520raw%2520data.%2520A%2520graph%2520convolutional%2520embedding%2520layer%2520is%2520then%2520designed%2520to%2520extract%2520primary%2520features%2520and%2520refined%2520dynamic%2520view-specific%2520graph%2520structures%252C%2520leveraging%2520the%2520global%2520graph%2520for%2520imputation%2520of%2520missing%2520views.%2520This%2520process%2520is%2520complemented%2520by%2520graph%2520structure%2520contrastive%2520learning%252C%2520which%2520identifies%2520consistency%2520among%2520view-specific%2520graph%2520structures.%2520Secondly%252C%2520a%2520graph%2520self-attention%2520encoder%2520is%2520introduced%2520to%2520extract%2520high-level%2520representations%2520based%2520on%2520the%2520imputed%2520primary%2520features%2520and%2520view-specific%2520graphs%252C%2520and%2520is%2520optimized%2520with%2520a%2520masked%2520graph%2520reconstruction%2520loss%2520to%2520mitigate%2520gradient%2520noise%2520during%2520optimization.%2520Finally%252C%2520a%2520clustering%2520module%2520is%2520constructed%2520and%2520optimized%2520through%2520a%2520pseudo-label%2520self-supervised%2520training%2520mechanism.%2520Extensive%2520experiments%2520on%2520multiple%2520datasets%2520validate%2520the%2520effectiveness%2520and%2520superiority%2520of%2520DGIMVCM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Deep%20Graph%20Learning%20for%20Incomplete%20Multi-View%20Clustering%20with%20Masked%20Graph%20Reconstruction%20Loss&entry.906535625=Zhenghao%20Zhang%20and%20Jun%20Xie%20and%20Xingchen%20Chen%20and%20Tao%20Yu%20and%20Hongzhu%20Yi%20and%20Kaixin%20Xu%20and%20Yuanxiang%20Wang%20and%20Tianyu%20Zong%20and%20Xinming%20Wang%20and%20Jiahuan%20Chen%20and%20Guoqing%20Chao%20and%20Feng%20Chen%20and%20Zhepeng%20Wang%20and%20Jungang%20Xu&entry.1292438233=The%20prevalence%20of%20real-world%20multi-view%20data%20makes%20incomplete%20multi-view%20clustering%20%28IMVC%29%20a%20crucial%20research.%20The%20rapid%20development%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20has%20established%20them%20as%20one%20of%20the%20mainstream%20approaches%20for%20multi-view%20clustering.%20Despite%20significant%20progress%20in%20GNNs-based%20IMVC%2C%20some%20challenges%20remain%3A%20%281%29%20Most%20methods%20rely%20on%20the%20K-Nearest%20Neighbors%20%28KNN%29%20algorithm%20to%20construct%20static%20graphs%20from%20raw%20data%2C%20which%20introduces%20noise%20and%20diminishes%20the%20robustness%20of%20the%20graph%20topology.%20%282%29%20Existing%20methods%20typically%20utilize%20the%20Mean%20Squared%20Error%20%28MSE%29%20loss%20between%20the%20reconstructed%20graph%20and%20the%20sparse%20adjacency%20graph%20directly%20as%20the%20graph%20reconstruction%20loss%2C%20leading%20to%20substantial%20gradient%20noise%20during%20optimization.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20%5Ctextbf%7BD%7Dynamic%20Deep%20%5Ctextbf%7BG%7Draph%20Learning%20for%20%5Ctextbf%7BI%7Dncomplete%20%5Ctextbf%7BM%7Dulti-%5Ctextbf%7BV%7Diew%20%5Ctextbf%7BC%7Dlustering%20with%20%5Ctextbf%7BM%7Dasked%20Graph%20Reconstruction%20Loss%20%28DGIMVCM%29.%20Firstly%2C%20we%20construct%20a%20missing-robust%20global%20graph%20from%20the%20raw%20data.%20A%20graph%20convolutional%20embedding%20layer%20is%20then%20designed%20to%20extract%20primary%20features%20and%20refined%20dynamic%20view-specific%20graph%20structures%2C%20leveraging%20the%20global%20graph%20for%20imputation%20of%20missing%20views.%20This%20process%20is%20complemented%20by%20graph%20structure%20contrastive%20learning%2C%20which%20identifies%20consistency%20among%20view-specific%20graph%20structures.%20Secondly%2C%20a%20graph%20self-attention%20encoder%20is%20introduced%20to%20extract%20high-level%20representations%20based%20on%20the%20imputed%20primary%20features%20and%20view-specific%20graphs%2C%20and%20is%20optimized%20with%20a%20masked%20graph%20reconstruction%20loss%20to%20mitigate%20gradient%20noise%20during%20optimization.%20Finally%2C%20a%20clustering%20module%20is%20constructed%20and%20optimized%20through%20a%20pseudo-label%20self-supervised%20training%20mechanism.%20Extensive%20experiments%20on%20multiple%20datasets%20validate%20the%20effectiveness%20and%20superiority%20of%20DGIMVCM.&entry.1838667208=http%3A//arxiv.org/abs/2511.11181v1&entry.124074799=Read"},
{"title": "VoxTell: Free-Text Promptable Universal 3D Medical Image Segmentation", "author": "Maximilian Rokuss and Moritz Langenberg and Yannick Kirchhoff and Fabian Isensee and Benjamin Hamm and Constantin Ulrich and Sebastian Regnery and Lukas Bauer and Efthimios Katsigiannopulos and Tobias Norajitra and Klaus Maier-Hein", "abstract": "We introduce VoxTell, a vision-language model for text-prompted volumetric medical image segmentation. It maps free-form descriptions, from single words to full clinical sentences, to 3D masks. Trained on 62K+ CT, MRI, and PET volumes spanning over 1K anatomical and pathological classes, VoxTell uses multi-stage vision-language fusion across decoder layers to align textual and visual features at multiple scales. It achieves state-of-the-art zero-shot performance across modalities on unseen datasets, excelling on familiar concepts while generalizing to related unseen classes. Extensive experiments further demonstrate strong cross-modality transfer, robustness to linguistic variations and clinical language, as well as accurate instance-specific segmentation from real-world text. Code is available at: https://www.github.com/MIC-DKFZ/VoxTell", "link": "http://arxiv.org/abs/2511.11450v1", "date": "2025-11-14", "relevancy": 2.7426, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5543}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5543}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VoxTell%3A%20Free-Text%20Promptable%20Universal%203D%20Medical%20Image%20Segmentation&body=Title%3A%20VoxTell%3A%20Free-Text%20Promptable%20Universal%203D%20Medical%20Image%20Segmentation%0AAuthor%3A%20Maximilian%20Rokuss%20and%20Moritz%20Langenberg%20and%20Yannick%20Kirchhoff%20and%20Fabian%20Isensee%20and%20Benjamin%20Hamm%20and%20Constantin%20Ulrich%20and%20Sebastian%20Regnery%20and%20Lukas%20Bauer%20and%20Efthimios%20Katsigiannopulos%20and%20Tobias%20Norajitra%20and%20Klaus%20Maier-Hein%0AAbstract%3A%20We%20introduce%20VoxTell%2C%20a%20vision-language%20model%20for%20text-prompted%20volumetric%20medical%20image%20segmentation.%20It%20maps%20free-form%20descriptions%2C%20from%20single%20words%20to%20full%20clinical%20sentences%2C%20to%203D%20masks.%20Trained%20on%2062K%2B%20CT%2C%20MRI%2C%20and%20PET%20volumes%20spanning%20over%201K%20anatomical%20and%20pathological%20classes%2C%20VoxTell%20uses%20multi-stage%20vision-language%20fusion%20across%20decoder%20layers%20to%20align%20textual%20and%20visual%20features%20at%20multiple%20scales.%20It%20achieves%20state-of-the-art%20zero-shot%20performance%20across%20modalities%20on%20unseen%20datasets%2C%20excelling%20on%20familiar%20concepts%20while%20generalizing%20to%20related%20unseen%20classes.%20Extensive%20experiments%20further%20demonstrate%20strong%20cross-modality%20transfer%2C%20robustness%20to%20linguistic%20variations%20and%20clinical%20language%2C%20as%20well%20as%20accurate%20instance-specific%20segmentation%20from%20real-world%20text.%20Code%20is%20available%20at%3A%20https%3A//www.github.com/MIC-DKFZ/VoxTell%0ALink%3A%20http%3A//arxiv.org/abs/2511.11450v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoxTell%253A%2520Free-Text%2520Promptable%2520Universal%25203D%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DMaximilian%2520Rokuss%2520and%2520Moritz%2520Langenberg%2520and%2520Yannick%2520Kirchhoff%2520and%2520Fabian%2520Isensee%2520and%2520Benjamin%2520Hamm%2520and%2520Constantin%2520Ulrich%2520and%2520Sebastian%2520Regnery%2520and%2520Lukas%2520Bauer%2520and%2520Efthimios%2520Katsigiannopulos%2520and%2520Tobias%2520Norajitra%2520and%2520Klaus%2520Maier-Hein%26entry.1292438233%3DWe%2520introduce%2520VoxTell%252C%2520a%2520vision-language%2520model%2520for%2520text-prompted%2520volumetric%2520medical%2520image%2520segmentation.%2520It%2520maps%2520free-form%2520descriptions%252C%2520from%2520single%2520words%2520to%2520full%2520clinical%2520sentences%252C%2520to%25203D%2520masks.%2520Trained%2520on%252062K%252B%2520CT%252C%2520MRI%252C%2520and%2520PET%2520volumes%2520spanning%2520over%25201K%2520anatomical%2520and%2520pathological%2520classes%252C%2520VoxTell%2520uses%2520multi-stage%2520vision-language%2520fusion%2520across%2520decoder%2520layers%2520to%2520align%2520textual%2520and%2520visual%2520features%2520at%2520multiple%2520scales.%2520It%2520achieves%2520state-of-the-art%2520zero-shot%2520performance%2520across%2520modalities%2520on%2520unseen%2520datasets%252C%2520excelling%2520on%2520familiar%2520concepts%2520while%2520generalizing%2520to%2520related%2520unseen%2520classes.%2520Extensive%2520experiments%2520further%2520demonstrate%2520strong%2520cross-modality%2520transfer%252C%2520robustness%2520to%2520linguistic%2520variations%2520and%2520clinical%2520language%252C%2520as%2520well%2520as%2520accurate%2520instance-specific%2520segmentation%2520from%2520real-world%2520text.%2520Code%2520is%2520available%2520at%253A%2520https%253A//www.github.com/MIC-DKFZ/VoxTell%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11450v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VoxTell%3A%20Free-Text%20Promptable%20Universal%203D%20Medical%20Image%20Segmentation&entry.906535625=Maximilian%20Rokuss%20and%20Moritz%20Langenberg%20and%20Yannick%20Kirchhoff%20and%20Fabian%20Isensee%20and%20Benjamin%20Hamm%20and%20Constantin%20Ulrich%20and%20Sebastian%20Regnery%20and%20Lukas%20Bauer%20and%20Efthimios%20Katsigiannopulos%20and%20Tobias%20Norajitra%20and%20Klaus%20Maier-Hein&entry.1292438233=We%20introduce%20VoxTell%2C%20a%20vision-language%20model%20for%20text-prompted%20volumetric%20medical%20image%20segmentation.%20It%20maps%20free-form%20descriptions%2C%20from%20single%20words%20to%20full%20clinical%20sentences%2C%20to%203D%20masks.%20Trained%20on%2062K%2B%20CT%2C%20MRI%2C%20and%20PET%20volumes%20spanning%20over%201K%20anatomical%20and%20pathological%20classes%2C%20VoxTell%20uses%20multi-stage%20vision-language%20fusion%20across%20decoder%20layers%20to%20align%20textual%20and%20visual%20features%20at%20multiple%20scales.%20It%20achieves%20state-of-the-art%20zero-shot%20performance%20across%20modalities%20on%20unseen%20datasets%2C%20excelling%20on%20familiar%20concepts%20while%20generalizing%20to%20related%20unseen%20classes.%20Extensive%20experiments%20further%20demonstrate%20strong%20cross-modality%20transfer%2C%20robustness%20to%20linguistic%20variations%20and%20clinical%20language%2C%20as%20well%20as%20accurate%20instance-specific%20segmentation%20from%20real-world%20text.%20Code%20is%20available%20at%3A%20https%3A//www.github.com/MIC-DKFZ/VoxTell&entry.1838667208=http%3A//arxiv.org/abs/2511.11450v1&entry.124074799=Read"},
{"title": "CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios", "author": "Hangyu Li and Bofeng Cao and Zhaohui Liang and Wuzhen Li and Juyoung Oh and Yuxuan Chen and Shixiao Liang and Hang Zhou and Chengyuan Ma and Jiaxi Liu and Zheng Li and Peng Zhang and KeKe Long and Maolin Liu and Jackson Jiang and Chunlei Yu and Shengxiang Liu and Hongkai Yu and Xiaopeng Li", "abstract": "Vehicle-to-Vehicle (V2V) cooperative perception has great potential to enhance autonomous driving performance by overcoming perception limitations in complex adverse traffic scenarios (CATS). Meanwhile, data serves as the fundamental infrastructure for modern autonomous driving AI. However, due to stringent data collection requirements, existing datasets focus primarily on ordinary traffic scenarios, constraining the benefits of cooperative perception. To address this challenge, we introduce CATS-V2V, the first-of-its-kind real-world dataset for V2V cooperative perception under complex adverse traffic scenarios. The dataset was collected by two hardware time-synchronized vehicles, covering 10 weather and lighting conditions across 10 diverse locations. The 100-clip dataset includes 60K frames of 10 Hz LiDAR point clouds and 1.26M multi-view 30 Hz camera images, along with 750K anonymized yet high-precision RTK-fixed GNSS and IMU records. Correspondingly, we provide time-consistent 3D bounding box annotations for objects, as well as static scenes to construct a 4D BEV representation. On this basis, we propose a target-based temporal alignment method, ensuring that all objects are precisely aligned across all sensor modalities. We hope that CATS-V2V, the largest-scale, most supportive, and highest-quality dataset of its kind to date, will benefit the autonomous driving community in related tasks.", "link": "http://arxiv.org/abs/2511.11168v1", "date": "2025-11-14", "relevancy": 2.6992, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5421}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5421}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CATS-V2V%3A%20A%20Real-World%20Vehicle-to-Vehicle%20Cooperative%20Perception%20Dataset%20with%20Complex%20Adverse%20Traffic%20Scenarios&body=Title%3A%20CATS-V2V%3A%20A%20Real-World%20Vehicle-to-Vehicle%20Cooperative%20Perception%20Dataset%20with%20Complex%20Adverse%20Traffic%20Scenarios%0AAuthor%3A%20Hangyu%20Li%20and%20Bofeng%20Cao%20and%20Zhaohui%20Liang%20and%20Wuzhen%20Li%20and%20Juyoung%20Oh%20and%20Yuxuan%20Chen%20and%20Shixiao%20Liang%20and%20Hang%20Zhou%20and%20Chengyuan%20Ma%20and%20Jiaxi%20Liu%20and%20Zheng%20Li%20and%20Peng%20Zhang%20and%20KeKe%20Long%20and%20Maolin%20Liu%20and%20Jackson%20Jiang%20and%20Chunlei%20Yu%20and%20Shengxiang%20Liu%20and%20Hongkai%20Yu%20and%20Xiaopeng%20Li%0AAbstract%3A%20Vehicle-to-Vehicle%20%28V2V%29%20cooperative%20perception%20has%20great%20potential%20to%20enhance%20autonomous%20driving%20performance%20by%20overcoming%20perception%20limitations%20in%20complex%20adverse%20traffic%20scenarios%20%28CATS%29.%20Meanwhile%2C%20data%20serves%20as%20the%20fundamental%20infrastructure%20for%20modern%20autonomous%20driving%20AI.%20However%2C%20due%20to%20stringent%20data%20collection%20requirements%2C%20existing%20datasets%20focus%20primarily%20on%20ordinary%20traffic%20scenarios%2C%20constraining%20the%20benefits%20of%20cooperative%20perception.%20To%20address%20this%20challenge%2C%20we%20introduce%20CATS-V2V%2C%20the%20first-of-its-kind%20real-world%20dataset%20for%20V2V%20cooperative%20perception%20under%20complex%20adverse%20traffic%20scenarios.%20The%20dataset%20was%20collected%20by%20two%20hardware%20time-synchronized%20vehicles%2C%20covering%2010%20weather%20and%20lighting%20conditions%20across%2010%20diverse%20locations.%20The%20100-clip%20dataset%20includes%2060K%20frames%20of%2010%20Hz%20LiDAR%20point%20clouds%20and%201.26M%20multi-view%2030%20Hz%20camera%20images%2C%20along%20with%20750K%20anonymized%20yet%20high-precision%20RTK-fixed%20GNSS%20and%20IMU%20records.%20Correspondingly%2C%20we%20provide%20time-consistent%203D%20bounding%20box%20annotations%20for%20objects%2C%20as%20well%20as%20static%20scenes%20to%20construct%20a%204D%20BEV%20representation.%20On%20this%20basis%2C%20we%20propose%20a%20target-based%20temporal%20alignment%20method%2C%20ensuring%20that%20all%20objects%20are%20precisely%20aligned%20across%20all%20sensor%20modalities.%20We%20hope%20that%20CATS-V2V%2C%20the%20largest-scale%2C%20most%20supportive%2C%20and%20highest-quality%20dataset%20of%20its%20kind%20to%20date%2C%20will%20benefit%20the%20autonomous%20driving%20community%20in%20related%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCATS-V2V%253A%2520A%2520Real-World%2520Vehicle-to-Vehicle%2520Cooperative%2520Perception%2520Dataset%2520with%2520Complex%2520Adverse%2520Traffic%2520Scenarios%26entry.906535625%3DHangyu%2520Li%2520and%2520Bofeng%2520Cao%2520and%2520Zhaohui%2520Liang%2520and%2520Wuzhen%2520Li%2520and%2520Juyoung%2520Oh%2520and%2520Yuxuan%2520Chen%2520and%2520Shixiao%2520Liang%2520and%2520Hang%2520Zhou%2520and%2520Chengyuan%2520Ma%2520and%2520Jiaxi%2520Liu%2520and%2520Zheng%2520Li%2520and%2520Peng%2520Zhang%2520and%2520KeKe%2520Long%2520and%2520Maolin%2520Liu%2520and%2520Jackson%2520Jiang%2520and%2520Chunlei%2520Yu%2520and%2520Shengxiang%2520Liu%2520and%2520Hongkai%2520Yu%2520and%2520Xiaopeng%2520Li%26entry.1292438233%3DVehicle-to-Vehicle%2520%2528V2V%2529%2520cooperative%2520perception%2520has%2520great%2520potential%2520to%2520enhance%2520autonomous%2520driving%2520performance%2520by%2520overcoming%2520perception%2520limitations%2520in%2520complex%2520adverse%2520traffic%2520scenarios%2520%2528CATS%2529.%2520Meanwhile%252C%2520data%2520serves%2520as%2520the%2520fundamental%2520infrastructure%2520for%2520modern%2520autonomous%2520driving%2520AI.%2520However%252C%2520due%2520to%2520stringent%2520data%2520collection%2520requirements%252C%2520existing%2520datasets%2520focus%2520primarily%2520on%2520ordinary%2520traffic%2520scenarios%252C%2520constraining%2520the%2520benefits%2520of%2520cooperative%2520perception.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520CATS-V2V%252C%2520the%2520first-of-its-kind%2520real-world%2520dataset%2520for%2520V2V%2520cooperative%2520perception%2520under%2520complex%2520adverse%2520traffic%2520scenarios.%2520The%2520dataset%2520was%2520collected%2520by%2520two%2520hardware%2520time-synchronized%2520vehicles%252C%2520covering%252010%2520weather%2520and%2520lighting%2520conditions%2520across%252010%2520diverse%2520locations.%2520The%2520100-clip%2520dataset%2520includes%252060K%2520frames%2520of%252010%2520Hz%2520LiDAR%2520point%2520clouds%2520and%25201.26M%2520multi-view%252030%2520Hz%2520camera%2520images%252C%2520along%2520with%2520750K%2520anonymized%2520yet%2520high-precision%2520RTK-fixed%2520GNSS%2520and%2520IMU%2520records.%2520Correspondingly%252C%2520we%2520provide%2520time-consistent%25203D%2520bounding%2520box%2520annotations%2520for%2520objects%252C%2520as%2520well%2520as%2520static%2520scenes%2520to%2520construct%2520a%25204D%2520BEV%2520representation.%2520On%2520this%2520basis%252C%2520we%2520propose%2520a%2520target-based%2520temporal%2520alignment%2520method%252C%2520ensuring%2520that%2520all%2520objects%2520are%2520precisely%2520aligned%2520across%2520all%2520sensor%2520modalities.%2520We%2520hope%2520that%2520CATS-V2V%252C%2520the%2520largest-scale%252C%2520most%2520supportive%252C%2520and%2520highest-quality%2520dataset%2520of%2520its%2520kind%2520to%2520date%252C%2520will%2520benefit%2520the%2520autonomous%2520driving%2520community%2520in%2520related%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CATS-V2V%3A%20A%20Real-World%20Vehicle-to-Vehicle%20Cooperative%20Perception%20Dataset%20with%20Complex%20Adverse%20Traffic%20Scenarios&entry.906535625=Hangyu%20Li%20and%20Bofeng%20Cao%20and%20Zhaohui%20Liang%20and%20Wuzhen%20Li%20and%20Juyoung%20Oh%20and%20Yuxuan%20Chen%20and%20Shixiao%20Liang%20and%20Hang%20Zhou%20and%20Chengyuan%20Ma%20and%20Jiaxi%20Liu%20and%20Zheng%20Li%20and%20Peng%20Zhang%20and%20KeKe%20Long%20and%20Maolin%20Liu%20and%20Jackson%20Jiang%20and%20Chunlei%20Yu%20and%20Shengxiang%20Liu%20and%20Hongkai%20Yu%20and%20Xiaopeng%20Li&entry.1292438233=Vehicle-to-Vehicle%20%28V2V%29%20cooperative%20perception%20has%20great%20potential%20to%20enhance%20autonomous%20driving%20performance%20by%20overcoming%20perception%20limitations%20in%20complex%20adverse%20traffic%20scenarios%20%28CATS%29.%20Meanwhile%2C%20data%20serves%20as%20the%20fundamental%20infrastructure%20for%20modern%20autonomous%20driving%20AI.%20However%2C%20due%20to%20stringent%20data%20collection%20requirements%2C%20existing%20datasets%20focus%20primarily%20on%20ordinary%20traffic%20scenarios%2C%20constraining%20the%20benefits%20of%20cooperative%20perception.%20To%20address%20this%20challenge%2C%20we%20introduce%20CATS-V2V%2C%20the%20first-of-its-kind%20real-world%20dataset%20for%20V2V%20cooperative%20perception%20under%20complex%20adverse%20traffic%20scenarios.%20The%20dataset%20was%20collected%20by%20two%20hardware%20time-synchronized%20vehicles%2C%20covering%2010%20weather%20and%20lighting%20conditions%20across%2010%20diverse%20locations.%20The%20100-clip%20dataset%20includes%2060K%20frames%20of%2010%20Hz%20LiDAR%20point%20clouds%20and%201.26M%20multi-view%2030%20Hz%20camera%20images%2C%20along%20with%20750K%20anonymized%20yet%20high-precision%20RTK-fixed%20GNSS%20and%20IMU%20records.%20Correspondingly%2C%20we%20provide%20time-consistent%203D%20bounding%20box%20annotations%20for%20objects%2C%20as%20well%20as%20static%20scenes%20to%20construct%20a%204D%20BEV%20representation.%20On%20this%20basis%2C%20we%20propose%20a%20target-based%20temporal%20alignment%20method%2C%20ensuring%20that%20all%20objects%20are%20precisely%20aligned%20across%20all%20sensor%20modalities.%20We%20hope%20that%20CATS-V2V%2C%20the%20largest-scale%2C%20most%20supportive%2C%20and%20highest-quality%20dataset%20of%20its%20kind%20to%20date%2C%20will%20benefit%20the%20autonomous%20driving%20community%20in%20related%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2511.11168v1&entry.124074799=Read"},
{"title": "MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model", "author": "Manyu Li and Ruian He and Chenxi Ma and Weimin Tan and Bo Yan", "abstract": "Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of large-scale, high-quality training data. We introduce MicroVQA++, a three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA archive. Stage one bootstraps supervision from expert-validated figure-caption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, a novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based vision-language alignment, and agent signals to identify and filter inconsistent samples. Stage three uses a MultiModal Large Language Model (MLLM) agent to generate multiple-choice questions (MCQ) followed by human screening. The resulting release comprises a large training split and a human-checked test split whose Bloom's level hard-sample distribution exceeds the MicroVQA benchmark. Our work delivers (i) a quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes.", "link": "http://arxiv.org/abs/2511.11407v1", "date": "2025-11-14", "relevancy": 2.697, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5401}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5401}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MicroVQA%2B%2B%3A%20High-Quality%20Microscopy%20Reasoning%20Dataset%20with%20Weakly%20Supervised%20Graphs%20for%20Multimodal%20Large%20Language%20Model&body=Title%3A%20MicroVQA%2B%2B%3A%20High-Quality%20Microscopy%20Reasoning%20Dataset%20with%20Weakly%20Supervised%20Graphs%20for%20Multimodal%20Large%20Language%20Model%0AAuthor%3A%20Manyu%20Li%20and%20Ruian%20He%20and%20Chenxi%20Ma%20and%20Weimin%20Tan%20and%20Bo%20Yan%0AAbstract%3A%20Multimodal%20Large%20Language%20Models%20are%20increasingly%20applied%20to%20biomedical%20imaging%2C%20yet%20scientific%20reasoning%20for%20microscopy%20remains%20limited%20by%20the%20scarcity%20of%20large-scale%2C%20high-quality%20training%20data.%20We%20introduce%20MicroVQA%2B%2B%2C%20a%20three-stage%2C%20large-scale%20and%20high-quality%20microscopy%20VQA%20corpus%20derived%20from%20the%20BIOMEDICA%20archive.%20Stage%20one%20bootstraps%20supervision%20from%20expert-validated%20figure-caption%20pairs%20sourced%20from%20peer-reviewed%20articles.%20Stage%20two%20applies%20HiCQA-Graph%2C%20a%20novel%20heterogeneous%20graph%20over%20images%2C%20captions%2C%20and%20QAs%20that%20fuses%20NLI-based%20textual%20entailment%2C%20CLIP-based%20vision-language%20alignment%2C%20and%20agent%20signals%20to%20identify%20and%20filter%20inconsistent%20samples.%20Stage%20three%20uses%20a%20MultiModal%20Large%20Language%20Model%20%28MLLM%29%20agent%20to%20generate%20multiple-choice%20questions%20%28MCQ%29%20followed%20by%20human%20screening.%20The%20resulting%20release%20comprises%20a%20large%20training%20split%20and%20a%20human-checked%20test%20split%20whose%20Bloom%27s%20level%20hard-sample%20distribution%20exceeds%20the%20MicroVQA%20benchmark.%20Our%20work%20delivers%20%28i%29%20a%20quality-controlled%20dataset%20that%20couples%20expert%20literature%20with%20graph-based%20filtering%20and%20human%20refinement%3B%20%28ii%29%20HiCQA-Graph%2C%20the%20first%20graph%20that%20jointly%20models%20%28image%2C%20caption%2C%20QA%29%20for%20cross-modal%20consistency%20filtering%3B%20%28iii%29%20evidence%20that%20careful%20data%20construction%20enables%204B-scale%20MLLMs%20to%20reach%20competitive%20microscopy%20reasoning%20performance%20%28e.g.%2C%20GPT-5%29%20and%20achieve%20state-of-the-art%20performance%20among%20open-source%20MLLMs.%20Code%20and%20dataset%20will%20be%20released%20after%20the%20review%20process%20concludes.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMicroVQA%252B%252B%253A%2520High-Quality%2520Microscopy%2520Reasoning%2520Dataset%2520with%2520Weakly%2520Supervised%2520Graphs%2520for%2520Multimodal%2520Large%2520Language%2520Model%26entry.906535625%3DManyu%2520Li%2520and%2520Ruian%2520He%2520and%2520Chenxi%2520Ma%2520and%2520Weimin%2520Tan%2520and%2520Bo%2520Yan%26entry.1292438233%3DMultimodal%2520Large%2520Language%2520Models%2520are%2520increasingly%2520applied%2520to%2520biomedical%2520imaging%252C%2520yet%2520scientific%2520reasoning%2520for%2520microscopy%2520remains%2520limited%2520by%2520the%2520scarcity%2520of%2520large-scale%252C%2520high-quality%2520training%2520data.%2520We%2520introduce%2520MicroVQA%252B%252B%252C%2520a%2520three-stage%252C%2520large-scale%2520and%2520high-quality%2520microscopy%2520VQA%2520corpus%2520derived%2520from%2520the%2520BIOMEDICA%2520archive.%2520Stage%2520one%2520bootstraps%2520supervision%2520from%2520expert-validated%2520figure-caption%2520pairs%2520sourced%2520from%2520peer-reviewed%2520articles.%2520Stage%2520two%2520applies%2520HiCQA-Graph%252C%2520a%2520novel%2520heterogeneous%2520graph%2520over%2520images%252C%2520captions%252C%2520and%2520QAs%2520that%2520fuses%2520NLI-based%2520textual%2520entailment%252C%2520CLIP-based%2520vision-language%2520alignment%252C%2520and%2520agent%2520signals%2520to%2520identify%2520and%2520filter%2520inconsistent%2520samples.%2520Stage%2520three%2520uses%2520a%2520MultiModal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520agent%2520to%2520generate%2520multiple-choice%2520questions%2520%2528MCQ%2529%2520followed%2520by%2520human%2520screening.%2520The%2520resulting%2520release%2520comprises%2520a%2520large%2520training%2520split%2520and%2520a%2520human-checked%2520test%2520split%2520whose%2520Bloom%2527s%2520level%2520hard-sample%2520distribution%2520exceeds%2520the%2520MicroVQA%2520benchmark.%2520Our%2520work%2520delivers%2520%2528i%2529%2520a%2520quality-controlled%2520dataset%2520that%2520couples%2520expert%2520literature%2520with%2520graph-based%2520filtering%2520and%2520human%2520refinement%253B%2520%2528ii%2529%2520HiCQA-Graph%252C%2520the%2520first%2520graph%2520that%2520jointly%2520models%2520%2528image%252C%2520caption%252C%2520QA%2529%2520for%2520cross-modal%2520consistency%2520filtering%253B%2520%2528iii%2529%2520evidence%2520that%2520careful%2520data%2520construction%2520enables%25204B-scale%2520MLLMs%2520to%2520reach%2520competitive%2520microscopy%2520reasoning%2520performance%2520%2528e.g.%252C%2520GPT-5%2529%2520and%2520achieve%2520state-of-the-art%2520performance%2520among%2520open-source%2520MLLMs.%2520Code%2520and%2520dataset%2520will%2520be%2520released%2520after%2520the%2520review%2520process%2520concludes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MicroVQA%2B%2B%3A%20High-Quality%20Microscopy%20Reasoning%20Dataset%20with%20Weakly%20Supervised%20Graphs%20for%20Multimodal%20Large%20Language%20Model&entry.906535625=Manyu%20Li%20and%20Ruian%20He%20and%20Chenxi%20Ma%20and%20Weimin%20Tan%20and%20Bo%20Yan&entry.1292438233=Multimodal%20Large%20Language%20Models%20are%20increasingly%20applied%20to%20biomedical%20imaging%2C%20yet%20scientific%20reasoning%20for%20microscopy%20remains%20limited%20by%20the%20scarcity%20of%20large-scale%2C%20high-quality%20training%20data.%20We%20introduce%20MicroVQA%2B%2B%2C%20a%20three-stage%2C%20large-scale%20and%20high-quality%20microscopy%20VQA%20corpus%20derived%20from%20the%20BIOMEDICA%20archive.%20Stage%20one%20bootstraps%20supervision%20from%20expert-validated%20figure-caption%20pairs%20sourced%20from%20peer-reviewed%20articles.%20Stage%20two%20applies%20HiCQA-Graph%2C%20a%20novel%20heterogeneous%20graph%20over%20images%2C%20captions%2C%20and%20QAs%20that%20fuses%20NLI-based%20textual%20entailment%2C%20CLIP-based%20vision-language%20alignment%2C%20and%20agent%20signals%20to%20identify%20and%20filter%20inconsistent%20samples.%20Stage%20three%20uses%20a%20MultiModal%20Large%20Language%20Model%20%28MLLM%29%20agent%20to%20generate%20multiple-choice%20questions%20%28MCQ%29%20followed%20by%20human%20screening.%20The%20resulting%20release%20comprises%20a%20large%20training%20split%20and%20a%20human-checked%20test%20split%20whose%20Bloom%27s%20level%20hard-sample%20distribution%20exceeds%20the%20MicroVQA%20benchmark.%20Our%20work%20delivers%20%28i%29%20a%20quality-controlled%20dataset%20that%20couples%20expert%20literature%20with%20graph-based%20filtering%20and%20human%20refinement%3B%20%28ii%29%20HiCQA-Graph%2C%20the%20first%20graph%20that%20jointly%20models%20%28image%2C%20caption%2C%20QA%29%20for%20cross-modal%20consistency%20filtering%3B%20%28iii%29%20evidence%20that%20careful%20data%20construction%20enables%204B-scale%20MLLMs%20to%20reach%20competitive%20microscopy%20reasoning%20performance%20%28e.g.%2C%20GPT-5%29%20and%20achieve%20state-of-the-art%20performance%20among%20open-source%20MLLMs.%20Code%20and%20dataset%20will%20be%20released%20after%20the%20review%20process%20concludes.&entry.1838667208=http%3A//arxiv.org/abs/2511.11407v1&entry.124074799=Read"},
{"title": "Shrinking the Teacher: An Adaptive Teaching Paradigm for Asymmetric EEG-Vision Alignment", "author": "Lukun Wu and Jie Li and Ziqi Ren and Kaifan Zhang and Xinbo Gao", "abstract": "Decoding visual features from EEG signals is a central challenge in neuroscience, with cross-modal alignment as the dominant approach. We argue that the relationship between visual and brain modalities is fundamentally asymmetric, characterized by two critical gaps: a Fidelity Gap (stemming from EEG's inherent noise and signal degradation, vs. vision's high-fidelity features) and a Semantic Gap (arising from EEG's shallow conceptual representation, vs. vision's rich semantic depth). Previous methods often overlook this asymmetry, forcing alignment between the two modalities as if they were equal partners and thereby leading to poor generalization. To address this, we propose the adaptive teaching paradigm. This paradigm empowers the ``teacher\" modality (vision) to dynamically shrink and adjust its knowledge structure under task guidance, tailoring its semantically dense features to match the ``student\" modality (EEG)'s capacity. We implement this paradigm with the ShrinkAdapter, a simple yet effective module featuring a residual-free design and a bottleneck structure. Through extensive experiments, we validate the underlying rationale and effectiveness of our paradigm. Our method achieves a top-1 accuracy of 60.2\\% on the zero-shot brain-to-image retrieval task, surpassing previous state-of-the-art methods by a margin of 9.8\\%. Our work introduces a new perspective for asymmetric alignment: the teacher must shrink and adapt to bridge the vision-brain gap.", "link": "http://arxiv.org/abs/2511.11422v1", "date": "2025-11-14", "relevancy": 2.6959, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5708}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5234}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shrinking%20the%20Teacher%3A%20An%20Adaptive%20Teaching%20Paradigm%20for%20Asymmetric%20EEG-Vision%20Alignment&body=Title%3A%20Shrinking%20the%20Teacher%3A%20An%20Adaptive%20Teaching%20Paradigm%20for%20Asymmetric%20EEG-Vision%20Alignment%0AAuthor%3A%20Lukun%20Wu%20and%20Jie%20Li%20and%20Ziqi%20Ren%20and%20Kaifan%20Zhang%20and%20Xinbo%20Gao%0AAbstract%3A%20Decoding%20visual%20features%20from%20EEG%20signals%20is%20a%20central%20challenge%20in%20neuroscience%2C%20with%20cross-modal%20alignment%20as%20the%20dominant%20approach.%20We%20argue%20that%20the%20relationship%20between%20visual%20and%20brain%20modalities%20is%20fundamentally%20asymmetric%2C%20characterized%20by%20two%20critical%20gaps%3A%20a%20Fidelity%20Gap%20%28stemming%20from%20EEG%27s%20inherent%20noise%20and%20signal%20degradation%2C%20vs.%20vision%27s%20high-fidelity%20features%29%20and%20a%20Semantic%20Gap%20%28arising%20from%20EEG%27s%20shallow%20conceptual%20representation%2C%20vs.%20vision%27s%20rich%20semantic%20depth%29.%20Previous%20methods%20often%20overlook%20this%20asymmetry%2C%20forcing%20alignment%20between%20the%20two%20modalities%20as%20if%20they%20were%20equal%20partners%20and%20thereby%20leading%20to%20poor%20generalization.%20To%20address%20this%2C%20we%20propose%20the%20adaptive%20teaching%20paradigm.%20This%20paradigm%20empowers%20the%20%60%60teacher%22%20modality%20%28vision%29%20to%20dynamically%20shrink%20and%20adjust%20its%20knowledge%20structure%20under%20task%20guidance%2C%20tailoring%20its%20semantically%20dense%20features%20to%20match%20the%20%60%60student%22%20modality%20%28EEG%29%27s%20capacity.%20We%20implement%20this%20paradigm%20with%20the%20ShrinkAdapter%2C%20a%20simple%20yet%20effective%20module%20featuring%20a%20residual-free%20design%20and%20a%20bottleneck%20structure.%20Through%20extensive%20experiments%2C%20we%20validate%20the%20underlying%20rationale%20and%20effectiveness%20of%20our%20paradigm.%20Our%20method%20achieves%20a%20top-1%20accuracy%20of%2060.2%5C%25%20on%20the%20zero-shot%20brain-to-image%20retrieval%20task%2C%20surpassing%20previous%20state-of-the-art%20methods%20by%20a%20margin%20of%209.8%5C%25.%20Our%20work%20introduces%20a%20new%20perspective%20for%20asymmetric%20alignment%3A%20the%20teacher%20must%20shrink%20and%20adapt%20to%20bridge%20the%20vision-brain%20gap.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShrinking%2520the%2520Teacher%253A%2520An%2520Adaptive%2520Teaching%2520Paradigm%2520for%2520Asymmetric%2520EEG-Vision%2520Alignment%26entry.906535625%3DLukun%2520Wu%2520and%2520Jie%2520Li%2520and%2520Ziqi%2520Ren%2520and%2520Kaifan%2520Zhang%2520and%2520Xinbo%2520Gao%26entry.1292438233%3DDecoding%2520visual%2520features%2520from%2520EEG%2520signals%2520is%2520a%2520central%2520challenge%2520in%2520neuroscience%252C%2520with%2520cross-modal%2520alignment%2520as%2520the%2520dominant%2520approach.%2520We%2520argue%2520that%2520the%2520relationship%2520between%2520visual%2520and%2520brain%2520modalities%2520is%2520fundamentally%2520asymmetric%252C%2520characterized%2520by%2520two%2520critical%2520gaps%253A%2520a%2520Fidelity%2520Gap%2520%2528stemming%2520from%2520EEG%2527s%2520inherent%2520noise%2520and%2520signal%2520degradation%252C%2520vs.%2520vision%2527s%2520high-fidelity%2520features%2529%2520and%2520a%2520Semantic%2520Gap%2520%2528arising%2520from%2520EEG%2527s%2520shallow%2520conceptual%2520representation%252C%2520vs.%2520vision%2527s%2520rich%2520semantic%2520depth%2529.%2520Previous%2520methods%2520often%2520overlook%2520this%2520asymmetry%252C%2520forcing%2520alignment%2520between%2520the%2520two%2520modalities%2520as%2520if%2520they%2520were%2520equal%2520partners%2520and%2520thereby%2520leading%2520to%2520poor%2520generalization.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520adaptive%2520teaching%2520paradigm.%2520This%2520paradigm%2520empowers%2520the%2520%2560%2560teacher%2522%2520modality%2520%2528vision%2529%2520to%2520dynamically%2520shrink%2520and%2520adjust%2520its%2520knowledge%2520structure%2520under%2520task%2520guidance%252C%2520tailoring%2520its%2520semantically%2520dense%2520features%2520to%2520match%2520the%2520%2560%2560student%2522%2520modality%2520%2528EEG%2529%2527s%2520capacity.%2520We%2520implement%2520this%2520paradigm%2520with%2520the%2520ShrinkAdapter%252C%2520a%2520simple%2520yet%2520effective%2520module%2520featuring%2520a%2520residual-free%2520design%2520and%2520a%2520bottleneck%2520structure.%2520Through%2520extensive%2520experiments%252C%2520we%2520validate%2520the%2520underlying%2520rationale%2520and%2520effectiveness%2520of%2520our%2520paradigm.%2520Our%2520method%2520achieves%2520a%2520top-1%2520accuracy%2520of%252060.2%255C%2525%2520on%2520the%2520zero-shot%2520brain-to-image%2520retrieval%2520task%252C%2520surpassing%2520previous%2520state-of-the-art%2520methods%2520by%2520a%2520margin%2520of%25209.8%255C%2525.%2520Our%2520work%2520introduces%2520a%2520new%2520perspective%2520for%2520asymmetric%2520alignment%253A%2520the%2520teacher%2520must%2520shrink%2520and%2520adapt%2520to%2520bridge%2520the%2520vision-brain%2520gap.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shrinking%20the%20Teacher%3A%20An%20Adaptive%20Teaching%20Paradigm%20for%20Asymmetric%20EEG-Vision%20Alignment&entry.906535625=Lukun%20Wu%20and%20Jie%20Li%20and%20Ziqi%20Ren%20and%20Kaifan%20Zhang%20and%20Xinbo%20Gao&entry.1292438233=Decoding%20visual%20features%20from%20EEG%20signals%20is%20a%20central%20challenge%20in%20neuroscience%2C%20with%20cross-modal%20alignment%20as%20the%20dominant%20approach.%20We%20argue%20that%20the%20relationship%20between%20visual%20and%20brain%20modalities%20is%20fundamentally%20asymmetric%2C%20characterized%20by%20two%20critical%20gaps%3A%20a%20Fidelity%20Gap%20%28stemming%20from%20EEG%27s%20inherent%20noise%20and%20signal%20degradation%2C%20vs.%20vision%27s%20high-fidelity%20features%29%20and%20a%20Semantic%20Gap%20%28arising%20from%20EEG%27s%20shallow%20conceptual%20representation%2C%20vs.%20vision%27s%20rich%20semantic%20depth%29.%20Previous%20methods%20often%20overlook%20this%20asymmetry%2C%20forcing%20alignment%20between%20the%20two%20modalities%20as%20if%20they%20were%20equal%20partners%20and%20thereby%20leading%20to%20poor%20generalization.%20To%20address%20this%2C%20we%20propose%20the%20adaptive%20teaching%20paradigm.%20This%20paradigm%20empowers%20the%20%60%60teacher%22%20modality%20%28vision%29%20to%20dynamically%20shrink%20and%20adjust%20its%20knowledge%20structure%20under%20task%20guidance%2C%20tailoring%20its%20semantically%20dense%20features%20to%20match%20the%20%60%60student%22%20modality%20%28EEG%29%27s%20capacity.%20We%20implement%20this%20paradigm%20with%20the%20ShrinkAdapter%2C%20a%20simple%20yet%20effective%20module%20featuring%20a%20residual-free%20design%20and%20a%20bottleneck%20structure.%20Through%20extensive%20experiments%2C%20we%20validate%20the%20underlying%20rationale%20and%20effectiveness%20of%20our%20paradigm.%20Our%20method%20achieves%20a%20top-1%20accuracy%20of%2060.2%5C%25%20on%20the%20zero-shot%20brain-to-image%20retrieval%20task%2C%20surpassing%20previous%20state-of-the-art%20methods%20by%20a%20margin%20of%209.8%5C%25.%20Our%20work%20introduces%20a%20new%20perspective%20for%20asymmetric%20alignment%3A%20the%20teacher%20must%20shrink%20and%20adapt%20to%20bridge%20the%20vision-brain%20gap.&entry.1838667208=http%3A//arxiv.org/abs/2511.11422v1&entry.124074799=Read"},
{"title": "BOFA: Bridge-Layer Orthogonal Low-Rank Fusion for CLIP-Based Class-Incremental Learning", "author": "Lan Li and Tao Hu and Da-Wei Zhou and Han-Jia Ye and De-Chuan Zhan", "abstract": "Class-Incremental Learning (CIL) aims to continually learn new categories without forgetting previously acquired knowledge. Vision-language models such as CLIP offer strong transferable representations via multi-modal supervision, making them promising for CIL. However, applying CLIP to CIL poses two major challenges: (1) adapting to downstream tasks often requires additional learnable modules, increasing model complexity and susceptibility to forgetting; and (2) while multi-modal representations offer complementary strengths, existing methods have yet to fully realize their potential in effectively integrating visual and textual modalities. To address these issues, we propose BOFA (Bridge-layer Orthogonal Fusion for Adaptation), a novel framework for CIL. BOFA confines all model adaptation exclusively to CLIP's existing cross-modal bridge-layer, thereby adding no extra parameters or inference cost. To prevent forgetting within this layer, it leverages Orthogonal Low-Rank Fusion, a mechanism that constrains parameter updates to a low-rank ``safe subspace\" mathematically constructed to be orthogonal to past task features. This ensures stable knowledge accumulation without data replay. Furthermore, BOFA employs a cross-modal hybrid prototype that synergizes stable textual prototypes with visual counterparts derived from our stably adapted bridge-layer, enhancing classification performance. Extensive experiments on standard benchmarks show that BOFA achieves superior accuracy and efficiency compared to existing methods.", "link": "http://arxiv.org/abs/2511.11421v1", "date": "2025-11-14", "relevancy": 2.6797, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5646}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5366}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BOFA%3A%20Bridge-Layer%20Orthogonal%20Low-Rank%20Fusion%20for%20CLIP-Based%20Class-Incremental%20Learning&body=Title%3A%20BOFA%3A%20Bridge-Layer%20Orthogonal%20Low-Rank%20Fusion%20for%20CLIP-Based%20Class-Incremental%20Learning%0AAuthor%3A%20Lan%20Li%20and%20Tao%20Hu%20and%20Da-Wei%20Zhou%20and%20Han-Jia%20Ye%20and%20De-Chuan%20Zhan%0AAbstract%3A%20Class-Incremental%20Learning%20%28CIL%29%20aims%20to%20continually%20learn%20new%20categories%20without%20forgetting%20previously%20acquired%20knowledge.%20Vision-language%20models%20such%20as%20CLIP%20offer%20strong%20transferable%20representations%20via%20multi-modal%20supervision%2C%20making%20them%20promising%20for%20CIL.%20However%2C%20applying%20CLIP%20to%20CIL%20poses%20two%20major%20challenges%3A%20%281%29%20adapting%20to%20downstream%20tasks%20often%20requires%20additional%20learnable%20modules%2C%20increasing%20model%20complexity%20and%20susceptibility%20to%20forgetting%3B%20and%20%282%29%20while%20multi-modal%20representations%20offer%20complementary%20strengths%2C%20existing%20methods%20have%20yet%20to%20fully%20realize%20their%20potential%20in%20effectively%20integrating%20visual%20and%20textual%20modalities.%20To%20address%20these%20issues%2C%20we%20propose%20BOFA%20%28Bridge-layer%20Orthogonal%20Fusion%20for%20Adaptation%29%2C%20a%20novel%20framework%20for%20CIL.%20BOFA%20confines%20all%20model%20adaptation%20exclusively%20to%20CLIP%27s%20existing%20cross-modal%20bridge-layer%2C%20thereby%20adding%20no%20extra%20parameters%20or%20inference%20cost.%20To%20prevent%20forgetting%20within%20this%20layer%2C%20it%20leverages%20Orthogonal%20Low-Rank%20Fusion%2C%20a%20mechanism%20that%20constrains%20parameter%20updates%20to%20a%20low-rank%20%60%60safe%20subspace%22%20mathematically%20constructed%20to%20be%20orthogonal%20to%20past%20task%20features.%20This%20ensures%20stable%20knowledge%20accumulation%20without%20data%20replay.%20Furthermore%2C%20BOFA%20employs%20a%20cross-modal%20hybrid%20prototype%20that%20synergizes%20stable%20textual%20prototypes%20with%20visual%20counterparts%20derived%20from%20our%20stably%20adapted%20bridge-layer%2C%20enhancing%20classification%20performance.%20Extensive%20experiments%20on%20standard%20benchmarks%20show%20that%20BOFA%20achieves%20superior%20accuracy%20and%20efficiency%20compared%20to%20existing%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBOFA%253A%2520Bridge-Layer%2520Orthogonal%2520Low-Rank%2520Fusion%2520for%2520CLIP-Based%2520Class-Incremental%2520Learning%26entry.906535625%3DLan%2520Li%2520and%2520Tao%2520Hu%2520and%2520Da-Wei%2520Zhou%2520and%2520Han-Jia%2520Ye%2520and%2520De-Chuan%2520Zhan%26entry.1292438233%3DClass-Incremental%2520Learning%2520%2528CIL%2529%2520aims%2520to%2520continually%2520learn%2520new%2520categories%2520without%2520forgetting%2520previously%2520acquired%2520knowledge.%2520Vision-language%2520models%2520such%2520as%2520CLIP%2520offer%2520strong%2520transferable%2520representations%2520via%2520multi-modal%2520supervision%252C%2520making%2520them%2520promising%2520for%2520CIL.%2520However%252C%2520applying%2520CLIP%2520to%2520CIL%2520poses%2520two%2520major%2520challenges%253A%2520%25281%2529%2520adapting%2520to%2520downstream%2520tasks%2520often%2520requires%2520additional%2520learnable%2520modules%252C%2520increasing%2520model%2520complexity%2520and%2520susceptibility%2520to%2520forgetting%253B%2520and%2520%25282%2529%2520while%2520multi-modal%2520representations%2520offer%2520complementary%2520strengths%252C%2520existing%2520methods%2520have%2520yet%2520to%2520fully%2520realize%2520their%2520potential%2520in%2520effectively%2520integrating%2520visual%2520and%2520textual%2520modalities.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520BOFA%2520%2528Bridge-layer%2520Orthogonal%2520Fusion%2520for%2520Adaptation%2529%252C%2520a%2520novel%2520framework%2520for%2520CIL.%2520BOFA%2520confines%2520all%2520model%2520adaptation%2520exclusively%2520to%2520CLIP%2527s%2520existing%2520cross-modal%2520bridge-layer%252C%2520thereby%2520adding%2520no%2520extra%2520parameters%2520or%2520inference%2520cost.%2520To%2520prevent%2520forgetting%2520within%2520this%2520layer%252C%2520it%2520leverages%2520Orthogonal%2520Low-Rank%2520Fusion%252C%2520a%2520mechanism%2520that%2520constrains%2520parameter%2520updates%2520to%2520a%2520low-rank%2520%2560%2560safe%2520subspace%2522%2520mathematically%2520constructed%2520to%2520be%2520orthogonal%2520to%2520past%2520task%2520features.%2520This%2520ensures%2520stable%2520knowledge%2520accumulation%2520without%2520data%2520replay.%2520Furthermore%252C%2520BOFA%2520employs%2520a%2520cross-modal%2520hybrid%2520prototype%2520that%2520synergizes%2520stable%2520textual%2520prototypes%2520with%2520visual%2520counterparts%2520derived%2520from%2520our%2520stably%2520adapted%2520bridge-layer%252C%2520enhancing%2520classification%2520performance.%2520Extensive%2520experiments%2520on%2520standard%2520benchmarks%2520show%2520that%2520BOFA%2520achieves%2520superior%2520accuracy%2520and%2520efficiency%2520compared%2520to%2520existing%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BOFA%3A%20Bridge-Layer%20Orthogonal%20Low-Rank%20Fusion%20for%20CLIP-Based%20Class-Incremental%20Learning&entry.906535625=Lan%20Li%20and%20Tao%20Hu%20and%20Da-Wei%20Zhou%20and%20Han-Jia%20Ye%20and%20De-Chuan%20Zhan&entry.1292438233=Class-Incremental%20Learning%20%28CIL%29%20aims%20to%20continually%20learn%20new%20categories%20without%20forgetting%20previously%20acquired%20knowledge.%20Vision-language%20models%20such%20as%20CLIP%20offer%20strong%20transferable%20representations%20via%20multi-modal%20supervision%2C%20making%20them%20promising%20for%20CIL.%20However%2C%20applying%20CLIP%20to%20CIL%20poses%20two%20major%20challenges%3A%20%281%29%20adapting%20to%20downstream%20tasks%20often%20requires%20additional%20learnable%20modules%2C%20increasing%20model%20complexity%20and%20susceptibility%20to%20forgetting%3B%20and%20%282%29%20while%20multi-modal%20representations%20offer%20complementary%20strengths%2C%20existing%20methods%20have%20yet%20to%20fully%20realize%20their%20potential%20in%20effectively%20integrating%20visual%20and%20textual%20modalities.%20To%20address%20these%20issues%2C%20we%20propose%20BOFA%20%28Bridge-layer%20Orthogonal%20Fusion%20for%20Adaptation%29%2C%20a%20novel%20framework%20for%20CIL.%20BOFA%20confines%20all%20model%20adaptation%20exclusively%20to%20CLIP%27s%20existing%20cross-modal%20bridge-layer%2C%20thereby%20adding%20no%20extra%20parameters%20or%20inference%20cost.%20To%20prevent%20forgetting%20within%20this%20layer%2C%20it%20leverages%20Orthogonal%20Low-Rank%20Fusion%2C%20a%20mechanism%20that%20constrains%20parameter%20updates%20to%20a%20low-rank%20%60%60safe%20subspace%22%20mathematically%20constructed%20to%20be%20orthogonal%20to%20past%20task%20features.%20This%20ensures%20stable%20knowledge%20accumulation%20without%20data%20replay.%20Furthermore%2C%20BOFA%20employs%20a%20cross-modal%20hybrid%20prototype%20that%20synergizes%20stable%20textual%20prototypes%20with%20visual%20counterparts%20derived%20from%20our%20stably%20adapted%20bridge-layer%2C%20enhancing%20classification%20performance.%20Extensive%20experiments%20on%20standard%20benchmarks%20show%20that%20BOFA%20achieves%20superior%20accuracy%20and%20efficiency%20compared%20to%20existing%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.11421v1&entry.124074799=Read"},
{"title": "Questioning the Stability of Visual Question Answering", "author": "Amir Rosenfeld and Neta Glazer and Ethan Fetaya", "abstract": "Visual Language Models (VLMs) have achieved remarkable progress, yet their reliability under small, meaning-preserving input changes remains poorly understood. We present the first large-scale, systematic study of VLM robustness to benign visual and textual perturbations: pixel-level shifts, light geometric transformations, padded rescaling, paraphrasing, and multilingual rewrites that do not alter the underlying semantics of an image-question pair. Across a broad set of models and datasets, we find that modern VLMs are highly sensitive to such minor perturbations: a substantial fraction of samples change their predicted answer under at least one visual or textual modification. We characterize how this instability varies across perturbation types, question categories, and models, revealing that even state-of-the-art systems (e.g., GPT-4o, Gemini 2.0 Flash) frequently fail under shifts as small as a few pixels or harmless rephrasings. We further show that sample-level stability serves as a strong indicator of correctness: stable samples are consistently far more likely to be answered correctly. Leveraging this, we demonstrate that the stability patterns of small, accessible open-source models can be used to predict the correctness of much larger closed-source models with high precision. Our findings expose a fundamental fragility in current VLMs and highlight the need for robustness evaluations that go beyond adversarial perturbations, focusing instead on invariances that models should reliably uphold.", "link": "http://arxiv.org/abs/2511.11206v1", "date": "2025-11-14", "relevancy": 2.6548, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5325}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5325}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Questioning%20the%20Stability%20of%20Visual%20Question%20Answering&body=Title%3A%20Questioning%20the%20Stability%20of%20Visual%20Question%20Answering%0AAuthor%3A%20Amir%20Rosenfeld%20and%20Neta%20Glazer%20and%20Ethan%20Fetaya%0AAbstract%3A%20Visual%20Language%20Models%20%28VLMs%29%20have%20achieved%20remarkable%20progress%2C%20yet%20their%20reliability%20under%20small%2C%20meaning-preserving%20input%20changes%20remains%20poorly%20understood.%20We%20present%20the%20first%20large-scale%2C%20systematic%20study%20of%20VLM%20robustness%20to%20benign%20visual%20and%20textual%20perturbations%3A%20pixel-level%20shifts%2C%20light%20geometric%20transformations%2C%20padded%20rescaling%2C%20paraphrasing%2C%20and%20multilingual%20rewrites%20that%20do%20not%20alter%20the%20underlying%20semantics%20of%20an%20image-question%20pair.%20Across%20a%20broad%20set%20of%20models%20and%20datasets%2C%20we%20find%20that%20modern%20VLMs%20are%20highly%20sensitive%20to%20such%20minor%20perturbations%3A%20a%20substantial%20fraction%20of%20samples%20change%20their%20predicted%20answer%20under%20at%20least%20one%20visual%20or%20textual%20modification.%20We%20characterize%20how%20this%20instability%20varies%20across%20perturbation%20types%2C%20question%20categories%2C%20and%20models%2C%20revealing%20that%20even%20state-of-the-art%20systems%20%28e.g.%2C%20GPT-4o%2C%20Gemini%202.0%20Flash%29%20frequently%20fail%20under%20shifts%20as%20small%20as%20a%20few%20pixels%20or%20harmless%20rephrasings.%20We%20further%20show%20that%20sample-level%20stability%20serves%20as%20a%20strong%20indicator%20of%20correctness%3A%20stable%20samples%20are%20consistently%20far%20more%20likely%20to%20be%20answered%20correctly.%20Leveraging%20this%2C%20we%20demonstrate%20that%20the%20stability%20patterns%20of%20small%2C%20accessible%20open-source%20models%20can%20be%20used%20to%20predict%20the%20correctness%20of%20much%20larger%20closed-source%20models%20with%20high%20precision.%20Our%20findings%20expose%20a%20fundamental%20fragility%20in%20current%20VLMs%20and%20highlight%20the%20need%20for%20robustness%20evaluations%20that%20go%20beyond%20adversarial%20perturbations%2C%20focusing%20instead%20on%20invariances%20that%20models%20should%20reliably%20uphold.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuestioning%2520the%2520Stability%2520of%2520Visual%2520Question%2520Answering%26entry.906535625%3DAmir%2520Rosenfeld%2520and%2520Neta%2520Glazer%2520and%2520Ethan%2520Fetaya%26entry.1292438233%3DVisual%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520achieved%2520remarkable%2520progress%252C%2520yet%2520their%2520reliability%2520under%2520small%252C%2520meaning-preserving%2520input%2520changes%2520remains%2520poorly%2520understood.%2520We%2520present%2520the%2520first%2520large-scale%252C%2520systematic%2520study%2520of%2520VLM%2520robustness%2520to%2520benign%2520visual%2520and%2520textual%2520perturbations%253A%2520pixel-level%2520shifts%252C%2520light%2520geometric%2520transformations%252C%2520padded%2520rescaling%252C%2520paraphrasing%252C%2520and%2520multilingual%2520rewrites%2520that%2520do%2520not%2520alter%2520the%2520underlying%2520semantics%2520of%2520an%2520image-question%2520pair.%2520Across%2520a%2520broad%2520set%2520of%2520models%2520and%2520datasets%252C%2520we%2520find%2520that%2520modern%2520VLMs%2520are%2520highly%2520sensitive%2520to%2520such%2520minor%2520perturbations%253A%2520a%2520substantial%2520fraction%2520of%2520samples%2520change%2520their%2520predicted%2520answer%2520under%2520at%2520least%2520one%2520visual%2520or%2520textual%2520modification.%2520We%2520characterize%2520how%2520this%2520instability%2520varies%2520across%2520perturbation%2520types%252C%2520question%2520categories%252C%2520and%2520models%252C%2520revealing%2520that%2520even%2520state-of-the-art%2520systems%2520%2528e.g.%252C%2520GPT-4o%252C%2520Gemini%25202.0%2520Flash%2529%2520frequently%2520fail%2520under%2520shifts%2520as%2520small%2520as%2520a%2520few%2520pixels%2520or%2520harmless%2520rephrasings.%2520We%2520further%2520show%2520that%2520sample-level%2520stability%2520serves%2520as%2520a%2520strong%2520indicator%2520of%2520correctness%253A%2520stable%2520samples%2520are%2520consistently%2520far%2520more%2520likely%2520to%2520be%2520answered%2520correctly.%2520Leveraging%2520this%252C%2520we%2520demonstrate%2520that%2520the%2520stability%2520patterns%2520of%2520small%252C%2520accessible%2520open-source%2520models%2520can%2520be%2520used%2520to%2520predict%2520the%2520correctness%2520of%2520much%2520larger%2520closed-source%2520models%2520with%2520high%2520precision.%2520Our%2520findings%2520expose%2520a%2520fundamental%2520fragility%2520in%2520current%2520VLMs%2520and%2520highlight%2520the%2520need%2520for%2520robustness%2520evaluations%2520that%2520go%2520beyond%2520adversarial%2520perturbations%252C%2520focusing%2520instead%2520on%2520invariances%2520that%2520models%2520should%2520reliably%2520uphold.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Questioning%20the%20Stability%20of%20Visual%20Question%20Answering&entry.906535625=Amir%20Rosenfeld%20and%20Neta%20Glazer%20and%20Ethan%20Fetaya&entry.1292438233=Visual%20Language%20Models%20%28VLMs%29%20have%20achieved%20remarkable%20progress%2C%20yet%20their%20reliability%20under%20small%2C%20meaning-preserving%20input%20changes%20remains%20poorly%20understood.%20We%20present%20the%20first%20large-scale%2C%20systematic%20study%20of%20VLM%20robustness%20to%20benign%20visual%20and%20textual%20perturbations%3A%20pixel-level%20shifts%2C%20light%20geometric%20transformations%2C%20padded%20rescaling%2C%20paraphrasing%2C%20and%20multilingual%20rewrites%20that%20do%20not%20alter%20the%20underlying%20semantics%20of%20an%20image-question%20pair.%20Across%20a%20broad%20set%20of%20models%20and%20datasets%2C%20we%20find%20that%20modern%20VLMs%20are%20highly%20sensitive%20to%20such%20minor%20perturbations%3A%20a%20substantial%20fraction%20of%20samples%20change%20their%20predicted%20answer%20under%20at%20least%20one%20visual%20or%20textual%20modification.%20We%20characterize%20how%20this%20instability%20varies%20across%20perturbation%20types%2C%20question%20categories%2C%20and%20models%2C%20revealing%20that%20even%20state-of-the-art%20systems%20%28e.g.%2C%20GPT-4o%2C%20Gemini%202.0%20Flash%29%20frequently%20fail%20under%20shifts%20as%20small%20as%20a%20few%20pixels%20or%20harmless%20rephrasings.%20We%20further%20show%20that%20sample-level%20stability%20serves%20as%20a%20strong%20indicator%20of%20correctness%3A%20stable%20samples%20are%20consistently%20far%20more%20likely%20to%20be%20answered%20correctly.%20Leveraging%20this%2C%20we%20demonstrate%20that%20the%20stability%20patterns%20of%20small%2C%20accessible%20open-source%20models%20can%20be%20used%20to%20predict%20the%20correctness%20of%20much%20larger%20closed-source%20models%20with%20high%20precision.%20Our%20findings%20expose%20a%20fundamental%20fragility%20in%20current%20VLMs%20and%20highlight%20the%20need%20for%20robustness%20evaluations%20that%20go%20beyond%20adversarial%20perturbations%2C%20focusing%20instead%20on%20invariances%20that%20models%20should%20reliably%20uphold.&entry.1838667208=http%3A//arxiv.org/abs/2511.11206v1&entry.124074799=Read"},
{"title": "AV-Dialog: Spoken Dialogue Models with Audio-Visual Input", "author": "Tuochao Chen and Bandhav Veluri and Hongyu Gong and Shyamnath Gollakota", "abstract": "Dialogue models falter in noisy, multi-speaker environments, often producing irrelevant responses and awkward turn-taking. We present AV-Dialog, the first multimodal dialog framework that uses both audio and visual cues to track the target speaker, predict turn-taking, and generate coherent responses. By combining acoustic tokenization with multi-task, multi-stage training on monadic, synthetic, and real audio-visual dialogue datasets, AV-Dialog achieves robust streaming transcription, semantically grounded turn-boundary detection and accurate responses, resulting in a natural conversational flow. Experiments show that AV-Dialog outperforms audio-only models under interference, reducing transcription errors, improving turn-taking prediction, and enhancing human-rated dialogue quality. These results highlight the power of seeing as well as hearing for speaker-aware interaction, paving the way for {spoken} dialogue agents that perform {robustly} in real-world, noisy environments.", "link": "http://arxiv.org/abs/2511.11124v1", "date": "2025-11-14", "relevancy": 2.6511, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5442}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5442}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AV-Dialog%3A%20Spoken%20Dialogue%20Models%20with%20Audio-Visual%20Input&body=Title%3A%20AV-Dialog%3A%20Spoken%20Dialogue%20Models%20with%20Audio-Visual%20Input%0AAuthor%3A%20Tuochao%20Chen%20and%20Bandhav%20Veluri%20and%20Hongyu%20Gong%20and%20Shyamnath%20Gollakota%0AAbstract%3A%20Dialogue%20models%20falter%20in%20noisy%2C%20multi-speaker%20environments%2C%20often%20producing%20irrelevant%20responses%20and%20awkward%20turn-taking.%20We%20present%20AV-Dialog%2C%20the%20first%20multimodal%20dialog%20framework%20that%20uses%20both%20audio%20and%20visual%20cues%20to%20track%20the%20target%20speaker%2C%20predict%20turn-taking%2C%20and%20generate%20coherent%20responses.%20By%20combining%20acoustic%20tokenization%20with%20multi-task%2C%20multi-stage%20training%20on%20monadic%2C%20synthetic%2C%20and%20real%20audio-visual%20dialogue%20datasets%2C%20AV-Dialog%20achieves%20robust%20streaming%20transcription%2C%20semantically%20grounded%20turn-boundary%20detection%20and%20accurate%20responses%2C%20resulting%20in%20a%20natural%20conversational%20flow.%20Experiments%20show%20that%20AV-Dialog%20outperforms%20audio-only%20models%20under%20interference%2C%20reducing%20transcription%20errors%2C%20improving%20turn-taking%20prediction%2C%20and%20enhancing%20human-rated%20dialogue%20quality.%20These%20results%20highlight%20the%20power%20of%20seeing%20as%20well%20as%20hearing%20for%20speaker-aware%20interaction%2C%20paving%20the%20way%20for%20%7Bspoken%7D%20dialogue%20agents%20that%20perform%20%7Brobustly%7D%20in%20real-world%2C%20noisy%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11124v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAV-Dialog%253A%2520Spoken%2520Dialogue%2520Models%2520with%2520Audio-Visual%2520Input%26entry.906535625%3DTuochao%2520Chen%2520and%2520Bandhav%2520Veluri%2520and%2520Hongyu%2520Gong%2520and%2520Shyamnath%2520Gollakota%26entry.1292438233%3DDialogue%2520models%2520falter%2520in%2520noisy%252C%2520multi-speaker%2520environments%252C%2520often%2520producing%2520irrelevant%2520responses%2520and%2520awkward%2520turn-taking.%2520We%2520present%2520AV-Dialog%252C%2520the%2520first%2520multimodal%2520dialog%2520framework%2520that%2520uses%2520both%2520audio%2520and%2520visual%2520cues%2520to%2520track%2520the%2520target%2520speaker%252C%2520predict%2520turn-taking%252C%2520and%2520generate%2520coherent%2520responses.%2520By%2520combining%2520acoustic%2520tokenization%2520with%2520multi-task%252C%2520multi-stage%2520training%2520on%2520monadic%252C%2520synthetic%252C%2520and%2520real%2520audio-visual%2520dialogue%2520datasets%252C%2520AV-Dialog%2520achieves%2520robust%2520streaming%2520transcription%252C%2520semantically%2520grounded%2520turn-boundary%2520detection%2520and%2520accurate%2520responses%252C%2520resulting%2520in%2520a%2520natural%2520conversational%2520flow.%2520Experiments%2520show%2520that%2520AV-Dialog%2520outperforms%2520audio-only%2520models%2520under%2520interference%252C%2520reducing%2520transcription%2520errors%252C%2520improving%2520turn-taking%2520prediction%252C%2520and%2520enhancing%2520human-rated%2520dialogue%2520quality.%2520These%2520results%2520highlight%2520the%2520power%2520of%2520seeing%2520as%2520well%2520as%2520hearing%2520for%2520speaker-aware%2520interaction%252C%2520paving%2520the%2520way%2520for%2520%257Bspoken%257D%2520dialogue%2520agents%2520that%2520perform%2520%257Brobustly%257D%2520in%2520real-world%252C%2520noisy%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11124v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AV-Dialog%3A%20Spoken%20Dialogue%20Models%20with%20Audio-Visual%20Input&entry.906535625=Tuochao%20Chen%20and%20Bandhav%20Veluri%20and%20Hongyu%20Gong%20and%20Shyamnath%20Gollakota&entry.1292438233=Dialogue%20models%20falter%20in%20noisy%2C%20multi-speaker%20environments%2C%20often%20producing%20irrelevant%20responses%20and%20awkward%20turn-taking.%20We%20present%20AV-Dialog%2C%20the%20first%20multimodal%20dialog%20framework%20that%20uses%20both%20audio%20and%20visual%20cues%20to%20track%20the%20target%20speaker%2C%20predict%20turn-taking%2C%20and%20generate%20coherent%20responses.%20By%20combining%20acoustic%20tokenization%20with%20multi-task%2C%20multi-stage%20training%20on%20monadic%2C%20synthetic%2C%20and%20real%20audio-visual%20dialogue%20datasets%2C%20AV-Dialog%20achieves%20robust%20streaming%20transcription%2C%20semantically%20grounded%20turn-boundary%20detection%20and%20accurate%20responses%2C%20resulting%20in%20a%20natural%20conversational%20flow.%20Experiments%20show%20that%20AV-Dialog%20outperforms%20audio-only%20models%20under%20interference%2C%20reducing%20transcription%20errors%2C%20improving%20turn-taking%20prediction%2C%20and%20enhancing%20human-rated%20dialogue%20quality.%20These%20results%20highlight%20the%20power%20of%20seeing%20as%20well%20as%20hearing%20for%20speaker-aware%20interaction%2C%20paving%20the%20way%20for%20%7Bspoken%7D%20dialogue%20agents%20that%20perform%20%7Brobustly%7D%20in%20real-world%2C%20noisy%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2511.11124v1&entry.124074799=Read"},
{"title": "Towards Generalizable AI-Generated Image Detection via Image-Adaptive Prompt Learning", "author": "Yiheng Li and Zichang Tan and Zhen Lei and Xu Zhou and Yang Yang", "abstract": "In AI-generated image detection, current cutting-edge methods typically adapt pre-trained foundation models through partial-parameter fine-tuning. However, these approaches often struggle to generalize to forgeries from unseen generators, as the fine-tuned models capture only limited patterns from training data and fail to reflect the evolving traits of new ones. To overcome this limitation, we propose Image-Adaptive Prompt Learning (IAPL), a novel paradigm that dynamically adjusts the prompts fed into the encoder according to each testing image, rather than fixing them after training. This design significantly enhances robustness and adaptability to diverse forged images. The dynamic prompts integrate conditional information with test-time adaptive tokens through a lightweight learnable scaling factor. The conditional information is produced by a Conditional Information Learner, which leverages CNN-based feature extractors to model both forgery-specific and general conditions. The test-time adaptive tokens are optimized during inference on a single sample by enforcing prediction consistency across multiple views, ensuring that the parameters align with the current image. For the final decision, the optimal input with the highest prediction confidence is selected. Extensive experiments show that IAPL achieves state-of-the-art performance, with mean accuracies of 95.61% and 96.7% on the widely used UniversalFakeDetect and GenImage datasets, respectively. Codes and weights will be released on https://github.com/liyih/IAPL.", "link": "http://arxiv.org/abs/2508.01603v3", "date": "2025-11-14", "relevancy": 2.621, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5441}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5154}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Generalizable%20AI-Generated%20Image%20Detection%20via%20Image-Adaptive%20Prompt%20Learning&body=Title%3A%20Towards%20Generalizable%20AI-Generated%20Image%20Detection%20via%20Image-Adaptive%20Prompt%20Learning%0AAuthor%3A%20Yiheng%20Li%20and%20Zichang%20Tan%20and%20Zhen%20Lei%20and%20Xu%20Zhou%20and%20Yang%20Yang%0AAbstract%3A%20In%20AI-generated%20image%20detection%2C%20current%20cutting-edge%20methods%20typically%20adapt%20pre-trained%20foundation%20models%20through%20partial-parameter%20fine-tuning.%20However%2C%20these%20approaches%20often%20struggle%20to%20generalize%20to%20forgeries%20from%20unseen%20generators%2C%20as%20the%20fine-tuned%20models%20capture%20only%20limited%20patterns%20from%20training%20data%20and%20fail%20to%20reflect%20the%20evolving%20traits%20of%20new%20ones.%20To%20overcome%20this%20limitation%2C%20we%20propose%20Image-Adaptive%20Prompt%20Learning%20%28IAPL%29%2C%20a%20novel%20paradigm%20that%20dynamically%20adjusts%20the%20prompts%20fed%20into%20the%20encoder%20according%20to%20each%20testing%20image%2C%20rather%20than%20fixing%20them%20after%20training.%20This%20design%20significantly%20enhances%20robustness%20and%20adaptability%20to%20diverse%20forged%20images.%20The%20dynamic%20prompts%20integrate%20conditional%20information%20with%20test-time%20adaptive%20tokens%20through%20a%20lightweight%20learnable%20scaling%20factor.%20The%20conditional%20information%20is%20produced%20by%20a%20Conditional%20Information%20Learner%2C%20which%20leverages%20CNN-based%20feature%20extractors%20to%20model%20both%20forgery-specific%20and%20general%20conditions.%20The%20test-time%20adaptive%20tokens%20are%20optimized%20during%20inference%20on%20a%20single%20sample%20by%20enforcing%20prediction%20consistency%20across%20multiple%20views%2C%20ensuring%20that%20the%20parameters%20align%20with%20the%20current%20image.%20For%20the%20final%20decision%2C%20the%20optimal%20input%20with%20the%20highest%20prediction%20confidence%20is%20selected.%20Extensive%20experiments%20show%20that%20IAPL%20achieves%20state-of-the-art%20performance%2C%20with%20mean%20accuracies%20of%2095.61%25%20and%2096.7%25%20on%20the%20widely%20used%20UniversalFakeDetect%20and%20GenImage%20datasets%2C%20respectively.%20Codes%20and%20weights%20will%20be%20released%20on%20https%3A//github.com/liyih/IAPL.%0ALink%3A%20http%3A//arxiv.org/abs/2508.01603v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Generalizable%2520AI-Generated%2520Image%2520Detection%2520via%2520Image-Adaptive%2520Prompt%2520Learning%26entry.906535625%3DYiheng%2520Li%2520and%2520Zichang%2520Tan%2520and%2520Zhen%2520Lei%2520and%2520Xu%2520Zhou%2520and%2520Yang%2520Yang%26entry.1292438233%3DIn%2520AI-generated%2520image%2520detection%252C%2520current%2520cutting-edge%2520methods%2520typically%2520adapt%2520pre-trained%2520foundation%2520models%2520through%2520partial-parameter%2520fine-tuning.%2520However%252C%2520these%2520approaches%2520often%2520struggle%2520to%2520generalize%2520to%2520forgeries%2520from%2520unseen%2520generators%252C%2520as%2520the%2520fine-tuned%2520models%2520capture%2520only%2520limited%2520patterns%2520from%2520training%2520data%2520and%2520fail%2520to%2520reflect%2520the%2520evolving%2520traits%2520of%2520new%2520ones.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520Image-Adaptive%2520Prompt%2520Learning%2520%2528IAPL%2529%252C%2520a%2520novel%2520paradigm%2520that%2520dynamically%2520adjusts%2520the%2520prompts%2520fed%2520into%2520the%2520encoder%2520according%2520to%2520each%2520testing%2520image%252C%2520rather%2520than%2520fixing%2520them%2520after%2520training.%2520This%2520design%2520significantly%2520enhances%2520robustness%2520and%2520adaptability%2520to%2520diverse%2520forged%2520images.%2520The%2520dynamic%2520prompts%2520integrate%2520conditional%2520information%2520with%2520test-time%2520adaptive%2520tokens%2520through%2520a%2520lightweight%2520learnable%2520scaling%2520factor.%2520The%2520conditional%2520information%2520is%2520produced%2520by%2520a%2520Conditional%2520Information%2520Learner%252C%2520which%2520leverages%2520CNN-based%2520feature%2520extractors%2520to%2520model%2520both%2520forgery-specific%2520and%2520general%2520conditions.%2520The%2520test-time%2520adaptive%2520tokens%2520are%2520optimized%2520during%2520inference%2520on%2520a%2520single%2520sample%2520by%2520enforcing%2520prediction%2520consistency%2520across%2520multiple%2520views%252C%2520ensuring%2520that%2520the%2520parameters%2520align%2520with%2520the%2520current%2520image.%2520For%2520the%2520final%2520decision%252C%2520the%2520optimal%2520input%2520with%2520the%2520highest%2520prediction%2520confidence%2520is%2520selected.%2520Extensive%2520experiments%2520show%2520that%2520IAPL%2520achieves%2520state-of-the-art%2520performance%252C%2520with%2520mean%2520accuracies%2520of%252095.61%2525%2520and%252096.7%2525%2520on%2520the%2520widely%2520used%2520UniversalFakeDetect%2520and%2520GenImage%2520datasets%252C%2520respectively.%2520Codes%2520and%2520weights%2520will%2520be%2520released%2520on%2520https%253A//github.com/liyih/IAPL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01603v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Generalizable%20AI-Generated%20Image%20Detection%20via%20Image-Adaptive%20Prompt%20Learning&entry.906535625=Yiheng%20Li%20and%20Zichang%20Tan%20and%20Zhen%20Lei%20and%20Xu%20Zhou%20and%20Yang%20Yang&entry.1292438233=In%20AI-generated%20image%20detection%2C%20current%20cutting-edge%20methods%20typically%20adapt%20pre-trained%20foundation%20models%20through%20partial-parameter%20fine-tuning.%20However%2C%20these%20approaches%20often%20struggle%20to%20generalize%20to%20forgeries%20from%20unseen%20generators%2C%20as%20the%20fine-tuned%20models%20capture%20only%20limited%20patterns%20from%20training%20data%20and%20fail%20to%20reflect%20the%20evolving%20traits%20of%20new%20ones.%20To%20overcome%20this%20limitation%2C%20we%20propose%20Image-Adaptive%20Prompt%20Learning%20%28IAPL%29%2C%20a%20novel%20paradigm%20that%20dynamically%20adjusts%20the%20prompts%20fed%20into%20the%20encoder%20according%20to%20each%20testing%20image%2C%20rather%20than%20fixing%20them%20after%20training.%20This%20design%20significantly%20enhances%20robustness%20and%20adaptability%20to%20diverse%20forged%20images.%20The%20dynamic%20prompts%20integrate%20conditional%20information%20with%20test-time%20adaptive%20tokens%20through%20a%20lightweight%20learnable%20scaling%20factor.%20The%20conditional%20information%20is%20produced%20by%20a%20Conditional%20Information%20Learner%2C%20which%20leverages%20CNN-based%20feature%20extractors%20to%20model%20both%20forgery-specific%20and%20general%20conditions.%20The%20test-time%20adaptive%20tokens%20are%20optimized%20during%20inference%20on%20a%20single%20sample%20by%20enforcing%20prediction%20consistency%20across%20multiple%20views%2C%20ensuring%20that%20the%20parameters%20align%20with%20the%20current%20image.%20For%20the%20final%20decision%2C%20the%20optimal%20input%20with%20the%20highest%20prediction%20confidence%20is%20selected.%20Extensive%20experiments%20show%20that%20IAPL%20achieves%20state-of-the-art%20performance%2C%20with%20mean%20accuracies%20of%2095.61%25%20and%2096.7%25%20on%20the%20widely%20used%20UniversalFakeDetect%20and%20GenImage%20datasets%2C%20respectively.%20Codes%20and%20weights%20will%20be%20released%20on%20https%3A//github.com/liyih/IAPL.&entry.1838667208=http%3A//arxiv.org/abs/2508.01603v3&entry.124074799=Read"},
{"title": "Coordinative Learning with Ordinal and Relational Priors for Volumetric Medical Image Segmentation", "author": "Haoyi Wang", "abstract": "Volumetric medical image segmentation presents unique challenges due to the inherent anatomical structure and limited availability of annotations. While recent methods have shown promise by contrasting spatial relationships between slices, they rely on hard binary thresholds to define positive and negative samples, thereby discarding valuable continuous information about anatomical similarity. Moreover, these methods overlook the global directional consistency of anatomical progression, resulting in distorted feature spaces that fail to capture the canonical anatomical manifold shared across patients. To address these limitations, we propose Coordinative Ordinal-Relational Anatomical Learning (CORAL) to capture both local and global structure in volumetric images. First, CORAL employs a contrastive ranking objective to leverage continuous anatomical similarity, ensuring relational feature distances between slices are proportional to their anatomical position differences. In addition, CORAL incorporates an ordinal objective to enforce global directional consistency, aligning the learned feature distribution with the canonical anatomical progression across patients. Learning these inter-slice relationships produces anatomically informed representations that benefit the downstream segmentation task. Through this coordinative learning framework, CORAL achieves state-of-the-art performance on benchmark datasets under limited-annotation settings while learning representations with meaningful anatomical structure. Code is available at https://github.com/haoyiwang25/CORAL.", "link": "http://arxiv.org/abs/2511.11276v1", "date": "2025-11-14", "relevancy": 2.6165, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5329}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5269}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coordinative%20Learning%20with%20Ordinal%20and%20Relational%20Priors%20for%20Volumetric%20Medical%20Image%20Segmentation&body=Title%3A%20Coordinative%20Learning%20with%20Ordinal%20and%20Relational%20Priors%20for%20Volumetric%20Medical%20Image%20Segmentation%0AAuthor%3A%20Haoyi%20Wang%0AAbstract%3A%20Volumetric%20medical%20image%20segmentation%20presents%20unique%20challenges%20due%20to%20the%20inherent%20anatomical%20structure%20and%20limited%20availability%20of%20annotations.%20While%20recent%20methods%20have%20shown%20promise%20by%20contrasting%20spatial%20relationships%20between%20slices%2C%20they%20rely%20on%20hard%20binary%20thresholds%20to%20define%20positive%20and%20negative%20samples%2C%20thereby%20discarding%20valuable%20continuous%20information%20about%20anatomical%20similarity.%20Moreover%2C%20these%20methods%20overlook%20the%20global%20directional%20consistency%20of%20anatomical%20progression%2C%20resulting%20in%20distorted%20feature%20spaces%20that%20fail%20to%20capture%20the%20canonical%20anatomical%20manifold%20shared%20across%20patients.%20To%20address%20these%20limitations%2C%20we%20propose%20Coordinative%20Ordinal-Relational%20Anatomical%20Learning%20%28CORAL%29%20to%20capture%20both%20local%20and%20global%20structure%20in%20volumetric%20images.%20First%2C%20CORAL%20employs%20a%20contrastive%20ranking%20objective%20to%20leverage%20continuous%20anatomical%20similarity%2C%20ensuring%20relational%20feature%20distances%20between%20slices%20are%20proportional%20to%20their%20anatomical%20position%20differences.%20In%20addition%2C%20CORAL%20incorporates%20an%20ordinal%20objective%20to%20enforce%20global%20directional%20consistency%2C%20aligning%20the%20learned%20feature%20distribution%20with%20the%20canonical%20anatomical%20progression%20across%20patients.%20Learning%20these%20inter-slice%20relationships%20produces%20anatomically%20informed%20representations%20that%20benefit%20the%20downstream%20segmentation%20task.%20Through%20this%20coordinative%20learning%20framework%2C%20CORAL%20achieves%20state-of-the-art%20performance%20on%20benchmark%20datasets%20under%20limited-annotation%20settings%20while%20learning%20representations%20with%20meaningful%20anatomical%20structure.%20Code%20is%20available%20at%20https%3A//github.com/haoyiwang25/CORAL.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11276v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoordinative%2520Learning%2520with%2520Ordinal%2520and%2520Relational%2520Priors%2520for%2520Volumetric%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DHaoyi%2520Wang%26entry.1292438233%3DVolumetric%2520medical%2520image%2520segmentation%2520presents%2520unique%2520challenges%2520due%2520to%2520the%2520inherent%2520anatomical%2520structure%2520and%2520limited%2520availability%2520of%2520annotations.%2520While%2520recent%2520methods%2520have%2520shown%2520promise%2520by%2520contrasting%2520spatial%2520relationships%2520between%2520slices%252C%2520they%2520rely%2520on%2520hard%2520binary%2520thresholds%2520to%2520define%2520positive%2520and%2520negative%2520samples%252C%2520thereby%2520discarding%2520valuable%2520continuous%2520information%2520about%2520anatomical%2520similarity.%2520Moreover%252C%2520these%2520methods%2520overlook%2520the%2520global%2520directional%2520consistency%2520of%2520anatomical%2520progression%252C%2520resulting%2520in%2520distorted%2520feature%2520spaces%2520that%2520fail%2520to%2520capture%2520the%2520canonical%2520anatomical%2520manifold%2520shared%2520across%2520patients.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Coordinative%2520Ordinal-Relational%2520Anatomical%2520Learning%2520%2528CORAL%2529%2520to%2520capture%2520both%2520local%2520and%2520global%2520structure%2520in%2520volumetric%2520images.%2520First%252C%2520CORAL%2520employs%2520a%2520contrastive%2520ranking%2520objective%2520to%2520leverage%2520continuous%2520anatomical%2520similarity%252C%2520ensuring%2520relational%2520feature%2520distances%2520between%2520slices%2520are%2520proportional%2520to%2520their%2520anatomical%2520position%2520differences.%2520In%2520addition%252C%2520CORAL%2520incorporates%2520an%2520ordinal%2520objective%2520to%2520enforce%2520global%2520directional%2520consistency%252C%2520aligning%2520the%2520learned%2520feature%2520distribution%2520with%2520the%2520canonical%2520anatomical%2520progression%2520across%2520patients.%2520Learning%2520these%2520inter-slice%2520relationships%2520produces%2520anatomically%2520informed%2520representations%2520that%2520benefit%2520the%2520downstream%2520segmentation%2520task.%2520Through%2520this%2520coordinative%2520learning%2520framework%252C%2520CORAL%2520achieves%2520state-of-the-art%2520performance%2520on%2520benchmark%2520datasets%2520under%2520limited-annotation%2520settings%2520while%2520learning%2520representations%2520with%2520meaningful%2520anatomical%2520structure.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/haoyiwang25/CORAL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11276v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coordinative%20Learning%20with%20Ordinal%20and%20Relational%20Priors%20for%20Volumetric%20Medical%20Image%20Segmentation&entry.906535625=Haoyi%20Wang&entry.1292438233=Volumetric%20medical%20image%20segmentation%20presents%20unique%20challenges%20due%20to%20the%20inherent%20anatomical%20structure%20and%20limited%20availability%20of%20annotations.%20While%20recent%20methods%20have%20shown%20promise%20by%20contrasting%20spatial%20relationships%20between%20slices%2C%20they%20rely%20on%20hard%20binary%20thresholds%20to%20define%20positive%20and%20negative%20samples%2C%20thereby%20discarding%20valuable%20continuous%20information%20about%20anatomical%20similarity.%20Moreover%2C%20these%20methods%20overlook%20the%20global%20directional%20consistency%20of%20anatomical%20progression%2C%20resulting%20in%20distorted%20feature%20spaces%20that%20fail%20to%20capture%20the%20canonical%20anatomical%20manifold%20shared%20across%20patients.%20To%20address%20these%20limitations%2C%20we%20propose%20Coordinative%20Ordinal-Relational%20Anatomical%20Learning%20%28CORAL%29%20to%20capture%20both%20local%20and%20global%20structure%20in%20volumetric%20images.%20First%2C%20CORAL%20employs%20a%20contrastive%20ranking%20objective%20to%20leverage%20continuous%20anatomical%20similarity%2C%20ensuring%20relational%20feature%20distances%20between%20slices%20are%20proportional%20to%20their%20anatomical%20position%20differences.%20In%20addition%2C%20CORAL%20incorporates%20an%20ordinal%20objective%20to%20enforce%20global%20directional%20consistency%2C%20aligning%20the%20learned%20feature%20distribution%20with%20the%20canonical%20anatomical%20progression%20across%20patients.%20Learning%20these%20inter-slice%20relationships%20produces%20anatomically%20informed%20representations%20that%20benefit%20the%20downstream%20segmentation%20task.%20Through%20this%20coordinative%20learning%20framework%2C%20CORAL%20achieves%20state-of-the-art%20performance%20on%20benchmark%20datasets%20under%20limited-annotation%20settings%20while%20learning%20representations%20with%20meaningful%20anatomical%20structure.%20Code%20is%20available%20at%20https%3A//github.com/haoyiwang25/CORAL.&entry.1838667208=http%3A//arxiv.org/abs/2511.11276v1&entry.124074799=Read"},
{"title": "Explicit Multimodal Graph Modeling for Human-Object Interaction Detection", "author": "Wenxuan Ji and Haichao Shi and Xiao-Yu Zhang", "abstract": "Transformer-based methods have recently become the prevailing approach for Human-Object Interaction (HOI) detection. However, the Transformer architecture does not explicitly model the relational structures inherent in HOI detection, which impedes the recognition of interactions. In contrast, Graph Neural Networks (GNNs) are inherently better suited for this task, as they explicitly model the relationships between human-object pairs. Therefore, in this paper, we propose \\textbf{M}ultimodal \\textbf{G}raph \\textbf{N}etwork \\textbf{M}odeling (MGNM) that leverages GNN-based relational structures to enhance HOI detection. Specifically, we design a multimodal graph network framework that explicitly models the HOI task in a four-stage graph structure. Furthermore, we introduce a multi-level feature interaction mechanism within our graph network. This mechanism leverages multi-level visual and language features to enhance information propagation across human-object pairs. Consequently, our proposed MGNM achieves state-of-the-art (SOTA) performance on two widely used benchmarks: HICO-DET and V-COCO. Moreover, when integrated with a more advanced object detector, our method demonstrates a significant performance gain and maintains an effective balance between rare and non-rare classes.", "link": "http://arxiv.org/abs/2509.12554v2", "date": "2025-11-14", "relevancy": 2.6138, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5443}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.52}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explicit%20Multimodal%20Graph%20Modeling%20for%20Human-Object%20Interaction%20Detection&body=Title%3A%20Explicit%20Multimodal%20Graph%20Modeling%20for%20Human-Object%20Interaction%20Detection%0AAuthor%3A%20Wenxuan%20Ji%20and%20Haichao%20Shi%20and%20Xiao-Yu%20Zhang%0AAbstract%3A%20Transformer-based%20methods%20have%20recently%20become%20the%20prevailing%20approach%20for%20Human-Object%20Interaction%20%28HOI%29%20detection.%20However%2C%20the%20Transformer%20architecture%20does%20not%20explicitly%20model%20the%20relational%20structures%20inherent%20in%20HOI%20detection%2C%20which%20impedes%20the%20recognition%20of%20interactions.%20In%20contrast%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20inherently%20better%20suited%20for%20this%20task%2C%20as%20they%20explicitly%20model%20the%20relationships%20between%20human-object%20pairs.%20Therefore%2C%20in%20this%20paper%2C%20we%20propose%20%5Ctextbf%7BM%7Dultimodal%20%5Ctextbf%7BG%7Draph%20%5Ctextbf%7BN%7Detwork%20%5Ctextbf%7BM%7Dodeling%20%28MGNM%29%20that%20leverages%20GNN-based%20relational%20structures%20to%20enhance%20HOI%20detection.%20Specifically%2C%20we%20design%20a%20multimodal%20graph%20network%20framework%20that%20explicitly%20models%20the%20HOI%20task%20in%20a%20four-stage%20graph%20structure.%20Furthermore%2C%20we%20introduce%20a%20multi-level%20feature%20interaction%20mechanism%20within%20our%20graph%20network.%20This%20mechanism%20leverages%20multi-level%20visual%20and%20language%20features%20to%20enhance%20information%20propagation%20across%20human-object%20pairs.%20Consequently%2C%20our%20proposed%20MGNM%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20on%20two%20widely%20used%20benchmarks%3A%20HICO-DET%20and%20V-COCO.%20Moreover%2C%20when%20integrated%20with%20a%20more%20advanced%20object%20detector%2C%20our%20method%20demonstrates%20a%20significant%20performance%20gain%20and%20maintains%20an%20effective%20balance%20between%20rare%20and%20non-rare%20classes.%0ALink%3A%20http%3A//arxiv.org/abs/2509.12554v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplicit%2520Multimodal%2520Graph%2520Modeling%2520for%2520Human-Object%2520Interaction%2520Detection%26entry.906535625%3DWenxuan%2520Ji%2520and%2520Haichao%2520Shi%2520and%2520Xiao-Yu%2520Zhang%26entry.1292438233%3DTransformer-based%2520methods%2520have%2520recently%2520become%2520the%2520prevailing%2520approach%2520for%2520Human-Object%2520Interaction%2520%2528HOI%2529%2520detection.%2520However%252C%2520the%2520Transformer%2520architecture%2520does%2520not%2520explicitly%2520model%2520the%2520relational%2520structures%2520inherent%2520in%2520HOI%2520detection%252C%2520which%2520impedes%2520the%2520recognition%2520of%2520interactions.%2520In%2520contrast%252C%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520inherently%2520better%2520suited%2520for%2520this%2520task%252C%2520as%2520they%2520explicitly%2520model%2520the%2520relationships%2520between%2520human-object%2520pairs.%2520Therefore%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520%255Ctextbf%257BM%257Dultimodal%2520%255Ctextbf%257BG%257Draph%2520%255Ctextbf%257BN%257Detwork%2520%255Ctextbf%257BM%257Dodeling%2520%2528MGNM%2529%2520that%2520leverages%2520GNN-based%2520relational%2520structures%2520to%2520enhance%2520HOI%2520detection.%2520Specifically%252C%2520we%2520design%2520a%2520multimodal%2520graph%2520network%2520framework%2520that%2520explicitly%2520models%2520the%2520HOI%2520task%2520in%2520a%2520four-stage%2520graph%2520structure.%2520Furthermore%252C%2520we%2520introduce%2520a%2520multi-level%2520feature%2520interaction%2520mechanism%2520within%2520our%2520graph%2520network.%2520This%2520mechanism%2520leverages%2520multi-level%2520visual%2520and%2520language%2520features%2520to%2520enhance%2520information%2520propagation%2520across%2520human-object%2520pairs.%2520Consequently%252C%2520our%2520proposed%2520MGNM%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520on%2520two%2520widely%2520used%2520benchmarks%253A%2520HICO-DET%2520and%2520V-COCO.%2520Moreover%252C%2520when%2520integrated%2520with%2520a%2520more%2520advanced%2520object%2520detector%252C%2520our%2520method%2520demonstrates%2520a%2520significant%2520performance%2520gain%2520and%2520maintains%2520an%2520effective%2520balance%2520between%2520rare%2520and%2520non-rare%2520classes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12554v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explicit%20Multimodal%20Graph%20Modeling%20for%20Human-Object%20Interaction%20Detection&entry.906535625=Wenxuan%20Ji%20and%20Haichao%20Shi%20and%20Xiao-Yu%20Zhang&entry.1292438233=Transformer-based%20methods%20have%20recently%20become%20the%20prevailing%20approach%20for%20Human-Object%20Interaction%20%28HOI%29%20detection.%20However%2C%20the%20Transformer%20architecture%20does%20not%20explicitly%20model%20the%20relational%20structures%20inherent%20in%20HOI%20detection%2C%20which%20impedes%20the%20recognition%20of%20interactions.%20In%20contrast%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20inherently%20better%20suited%20for%20this%20task%2C%20as%20they%20explicitly%20model%20the%20relationships%20between%20human-object%20pairs.%20Therefore%2C%20in%20this%20paper%2C%20we%20propose%20%5Ctextbf%7BM%7Dultimodal%20%5Ctextbf%7BG%7Draph%20%5Ctextbf%7BN%7Detwork%20%5Ctextbf%7BM%7Dodeling%20%28MGNM%29%20that%20leverages%20GNN-based%20relational%20structures%20to%20enhance%20HOI%20detection.%20Specifically%2C%20we%20design%20a%20multimodal%20graph%20network%20framework%20that%20explicitly%20models%20the%20HOI%20task%20in%20a%20four-stage%20graph%20structure.%20Furthermore%2C%20we%20introduce%20a%20multi-level%20feature%20interaction%20mechanism%20within%20our%20graph%20network.%20This%20mechanism%20leverages%20multi-level%20visual%20and%20language%20features%20to%20enhance%20information%20propagation%20across%20human-object%20pairs.%20Consequently%2C%20our%20proposed%20MGNM%20achieves%20state-of-the-art%20%28SOTA%29%20performance%20on%20two%20widely%20used%20benchmarks%3A%20HICO-DET%20and%20V-COCO.%20Moreover%2C%20when%20integrated%20with%20a%20more%20advanced%20object%20detector%2C%20our%20method%20demonstrates%20a%20significant%20performance%20gain%20and%20maintains%20an%20effective%20balance%20between%20rare%20and%20non-rare%20classes.&entry.1838667208=http%3A//arxiv.org/abs/2509.12554v2&entry.124074799=Read"},
{"title": "UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation", "author": "Zhen Yang and Wenyi Hong and Mingde Xu and Xinyue Fan and Weihan Wang and Jiele Cheng and Xiaotao Gu and Jie Tang", "abstract": "User interface (UI) programming is a core yet highly complex part of modern software development. Recent advances in visual language models (VLMs) highlight the potential of automatic UI coding, but current approaches face two key limitations: multimodal coding capabilities remain underdeveloped, and single-turn paradigms make little use of iterative visual feedback. We address these challenges with an interactive UI-to-code paradigm that better reflects real-world workflows and raises the upper bound of achievable performance. Under this paradigm, we present UI2Code$^\\text{N}$, a visual language model trained through staged pretraining, fine-tuning, and reinforcement learning to achieve foundational improvements in multimodal coding. The model unifies three key capabilities: UI-to-code generation, UI editing, and UI polishing. We further explore test-time scaling for interactive generation, enabling systematic use of multi-turn feedback. Experiments on UI-to-code and UI polishing benchmarks show that UI2Code$^\\text{N}$ establishes a new state of the art among open-source models and achieves performance comparable to leading closed-source models such as Claude-4-Sonnet and GPT-5. Our code and models are available at https://github.com/zai-org/UI2Code_N.", "link": "http://arxiv.org/abs/2511.08195v2", "date": "2025-11-14", "relevancy": 2.5788, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5324}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5101}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UI2Code%5EN%3A%20A%20Visual%20Language%20Model%20for%20Test-Time%20Scalable%20Interactive%20UI-to-Code%20Generation&body=Title%3A%20UI2Code%5EN%3A%20A%20Visual%20Language%20Model%20for%20Test-Time%20Scalable%20Interactive%20UI-to-Code%20Generation%0AAuthor%3A%20Zhen%20Yang%20and%20Wenyi%20Hong%20and%20Mingde%20Xu%20and%20Xinyue%20Fan%20and%20Weihan%20Wang%20and%20Jiele%20Cheng%20and%20Xiaotao%20Gu%20and%20Jie%20Tang%0AAbstract%3A%20User%20interface%20%28UI%29%20programming%20is%20a%20core%20yet%20highly%20complex%20part%20of%20modern%20software%20development.%20Recent%20advances%20in%20visual%20language%20models%20%28VLMs%29%20highlight%20the%20potential%20of%20automatic%20UI%20coding%2C%20but%20current%20approaches%20face%20two%20key%20limitations%3A%20multimodal%20coding%20capabilities%20remain%20underdeveloped%2C%20and%20single-turn%20paradigms%20make%20little%20use%20of%20iterative%20visual%20feedback.%20We%20address%20these%20challenges%20with%20an%20interactive%20UI-to-code%20paradigm%20that%20better%20reflects%20real-world%20workflows%20and%20raises%20the%20upper%20bound%20of%20achievable%20performance.%20Under%20this%20paradigm%2C%20we%20present%20UI2Code%24%5E%5Ctext%7BN%7D%24%2C%20a%20visual%20language%20model%20trained%20through%20staged%20pretraining%2C%20fine-tuning%2C%20and%20reinforcement%20learning%20to%20achieve%20foundational%20improvements%20in%20multimodal%20coding.%20The%20model%20unifies%20three%20key%20capabilities%3A%20UI-to-code%20generation%2C%20UI%20editing%2C%20and%20UI%20polishing.%20We%20further%20explore%20test-time%20scaling%20for%20interactive%20generation%2C%20enabling%20systematic%20use%20of%20multi-turn%20feedback.%20Experiments%20on%20UI-to-code%20and%20UI%20polishing%20benchmarks%20show%20that%20UI2Code%24%5E%5Ctext%7BN%7D%24%20establishes%20a%20new%20state%20of%20the%20art%20among%20open-source%20models%20and%20achieves%20performance%20comparable%20to%20leading%20closed-source%20models%20such%20as%20Claude-4-Sonnet%20and%20GPT-5.%20Our%20code%20and%20models%20are%20available%20at%20https%3A//github.com/zai-org/UI2Code_N.%0ALink%3A%20http%3A//arxiv.org/abs/2511.08195v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUI2Code%255EN%253A%2520A%2520Visual%2520Language%2520Model%2520for%2520Test-Time%2520Scalable%2520Interactive%2520UI-to-Code%2520Generation%26entry.906535625%3DZhen%2520Yang%2520and%2520Wenyi%2520Hong%2520and%2520Mingde%2520Xu%2520and%2520Xinyue%2520Fan%2520and%2520Weihan%2520Wang%2520and%2520Jiele%2520Cheng%2520and%2520Xiaotao%2520Gu%2520and%2520Jie%2520Tang%26entry.1292438233%3DUser%2520interface%2520%2528UI%2529%2520programming%2520is%2520a%2520core%2520yet%2520highly%2520complex%2520part%2520of%2520modern%2520software%2520development.%2520Recent%2520advances%2520in%2520visual%2520language%2520models%2520%2528VLMs%2529%2520highlight%2520the%2520potential%2520of%2520automatic%2520UI%2520coding%252C%2520but%2520current%2520approaches%2520face%2520two%2520key%2520limitations%253A%2520multimodal%2520coding%2520capabilities%2520remain%2520underdeveloped%252C%2520and%2520single-turn%2520paradigms%2520make%2520little%2520use%2520of%2520iterative%2520visual%2520feedback.%2520We%2520address%2520these%2520challenges%2520with%2520an%2520interactive%2520UI-to-code%2520paradigm%2520that%2520better%2520reflects%2520real-world%2520workflows%2520and%2520raises%2520the%2520upper%2520bound%2520of%2520achievable%2520performance.%2520Under%2520this%2520paradigm%252C%2520we%2520present%2520UI2Code%2524%255E%255Ctext%257BN%257D%2524%252C%2520a%2520visual%2520language%2520model%2520trained%2520through%2520staged%2520pretraining%252C%2520fine-tuning%252C%2520and%2520reinforcement%2520learning%2520to%2520achieve%2520foundational%2520improvements%2520in%2520multimodal%2520coding.%2520The%2520model%2520unifies%2520three%2520key%2520capabilities%253A%2520UI-to-code%2520generation%252C%2520UI%2520editing%252C%2520and%2520UI%2520polishing.%2520We%2520further%2520explore%2520test-time%2520scaling%2520for%2520interactive%2520generation%252C%2520enabling%2520systematic%2520use%2520of%2520multi-turn%2520feedback.%2520Experiments%2520on%2520UI-to-code%2520and%2520UI%2520polishing%2520benchmarks%2520show%2520that%2520UI2Code%2524%255E%255Ctext%257BN%257D%2524%2520establishes%2520a%2520new%2520state%2520of%2520the%2520art%2520among%2520open-source%2520models%2520and%2520achieves%2520performance%2520comparable%2520to%2520leading%2520closed-source%2520models%2520such%2520as%2520Claude-4-Sonnet%2520and%2520GPT-5.%2520Our%2520code%2520and%2520models%2520are%2520available%2520at%2520https%253A//github.com/zai-org/UI2Code_N.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.08195v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UI2Code%5EN%3A%20A%20Visual%20Language%20Model%20for%20Test-Time%20Scalable%20Interactive%20UI-to-Code%20Generation&entry.906535625=Zhen%20Yang%20and%20Wenyi%20Hong%20and%20Mingde%20Xu%20and%20Xinyue%20Fan%20and%20Weihan%20Wang%20and%20Jiele%20Cheng%20and%20Xiaotao%20Gu%20and%20Jie%20Tang&entry.1292438233=User%20interface%20%28UI%29%20programming%20is%20a%20core%20yet%20highly%20complex%20part%20of%20modern%20software%20development.%20Recent%20advances%20in%20visual%20language%20models%20%28VLMs%29%20highlight%20the%20potential%20of%20automatic%20UI%20coding%2C%20but%20current%20approaches%20face%20two%20key%20limitations%3A%20multimodal%20coding%20capabilities%20remain%20underdeveloped%2C%20and%20single-turn%20paradigms%20make%20little%20use%20of%20iterative%20visual%20feedback.%20We%20address%20these%20challenges%20with%20an%20interactive%20UI-to-code%20paradigm%20that%20better%20reflects%20real-world%20workflows%20and%20raises%20the%20upper%20bound%20of%20achievable%20performance.%20Under%20this%20paradigm%2C%20we%20present%20UI2Code%24%5E%5Ctext%7BN%7D%24%2C%20a%20visual%20language%20model%20trained%20through%20staged%20pretraining%2C%20fine-tuning%2C%20and%20reinforcement%20learning%20to%20achieve%20foundational%20improvements%20in%20multimodal%20coding.%20The%20model%20unifies%20three%20key%20capabilities%3A%20UI-to-code%20generation%2C%20UI%20editing%2C%20and%20UI%20polishing.%20We%20further%20explore%20test-time%20scaling%20for%20interactive%20generation%2C%20enabling%20systematic%20use%20of%20multi-turn%20feedback.%20Experiments%20on%20UI-to-code%20and%20UI%20polishing%20benchmarks%20show%20that%20UI2Code%24%5E%5Ctext%7BN%7D%24%20establishes%20a%20new%20state%20of%20the%20art%20among%20open-source%20models%20and%20achieves%20performance%20comparable%20to%20leading%20closed-source%20models%20such%20as%20Claude-4-Sonnet%20and%20GPT-5.%20Our%20code%20and%20models%20are%20available%20at%20https%3A//github.com/zai-org/UI2Code_N.&entry.1838667208=http%3A//arxiv.org/abs/2511.08195v2&entry.124074799=Read"},
{"title": "Free3D: 3D Human Motion Emerges from Single-View 2D Supervision", "author": "Sheng Liu and Yuanzhi Liang and Sidan Du", "abstract": "Recent 3D human motion generation models demonstrate remarkable reconstruction accuracy yet struggle to generalize beyond training distributions. This limitation arises partly from the use of precise 3D supervision, which encourages models to fit fixed coordinate patterns instead of learning the essential 3D structure and motion semantic cues required for robust generalization.To overcome this limitation, we propose Free3D, a framework that synthesizes realistic 3D motions without any 3D motion annotations. Free3D introduces a Motion-Lifting Residual Quantized VAE (ML-RQ) that maps 2D motion sequences into 3D-consistent latent spaces, and a suite of 3D-free regularization objectives enforcing view consistency, orientation coherence, and physical plausibility. Trained entirely on 2D motion data, Free3D generates diverse, temporally coherent, and semantically aligned 3D motions, achieving performance comparable to or even surpassing fully 3D-supervised counterparts. These results suggest that relaxing explicit 3D supervision encourages stronger structural reasoning and generalization, offering a scalable and data-efficient paradigm for 3D motion generation.", "link": "http://arxiv.org/abs/2511.11368v1", "date": "2025-11-14", "relevancy": 2.5677, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7048}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6039}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Free3D%3A%203D%20Human%20Motion%20Emerges%20from%20Single-View%202D%20Supervision&body=Title%3A%20Free3D%3A%203D%20Human%20Motion%20Emerges%20from%20Single-View%202D%20Supervision%0AAuthor%3A%20Sheng%20Liu%20and%20Yuanzhi%20Liang%20and%20Sidan%20Du%0AAbstract%3A%20Recent%203D%20human%20motion%20generation%20models%20demonstrate%20remarkable%20reconstruction%20accuracy%20yet%20struggle%20to%20generalize%20beyond%20training%20distributions.%20This%20limitation%20arises%20partly%20from%20the%20use%20of%20precise%203D%20supervision%2C%20which%20encourages%20models%20to%20fit%20fixed%20coordinate%20patterns%20instead%20of%20learning%20the%20essential%203D%20structure%20and%20motion%20semantic%20cues%20required%20for%20robust%20generalization.To%20overcome%20this%20limitation%2C%20we%20propose%20Free3D%2C%20a%20framework%20that%20synthesizes%20realistic%203D%20motions%20without%20any%203D%20motion%20annotations.%20Free3D%20introduces%20a%20Motion-Lifting%20Residual%20Quantized%20VAE%20%28ML-RQ%29%20that%20maps%202D%20motion%20sequences%20into%203D-consistent%20latent%20spaces%2C%20and%20a%20suite%20of%203D-free%20regularization%20objectives%20enforcing%20view%20consistency%2C%20orientation%20coherence%2C%20and%20physical%20plausibility.%20Trained%20entirely%20on%202D%20motion%20data%2C%20Free3D%20generates%20diverse%2C%20temporally%20coherent%2C%20and%20semantically%20aligned%203D%20motions%2C%20achieving%20performance%20comparable%20to%20or%20even%20surpassing%20fully%203D-supervised%20counterparts.%20These%20results%20suggest%20that%20relaxing%20explicit%203D%20supervision%20encourages%20stronger%20structural%20reasoning%20and%20generalization%2C%20offering%20a%20scalable%20and%20data-efficient%20paradigm%20for%203D%20motion%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11368v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFree3D%253A%25203D%2520Human%2520Motion%2520Emerges%2520from%2520Single-View%25202D%2520Supervision%26entry.906535625%3DSheng%2520Liu%2520and%2520Yuanzhi%2520Liang%2520and%2520Sidan%2520Du%26entry.1292438233%3DRecent%25203D%2520human%2520motion%2520generation%2520models%2520demonstrate%2520remarkable%2520reconstruction%2520accuracy%2520yet%2520struggle%2520to%2520generalize%2520beyond%2520training%2520distributions.%2520This%2520limitation%2520arises%2520partly%2520from%2520the%2520use%2520of%2520precise%25203D%2520supervision%252C%2520which%2520encourages%2520models%2520to%2520fit%2520fixed%2520coordinate%2520patterns%2520instead%2520of%2520learning%2520the%2520essential%25203D%2520structure%2520and%2520motion%2520semantic%2520cues%2520required%2520for%2520robust%2520generalization.To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520Free3D%252C%2520a%2520framework%2520that%2520synthesizes%2520realistic%25203D%2520motions%2520without%2520any%25203D%2520motion%2520annotations.%2520Free3D%2520introduces%2520a%2520Motion-Lifting%2520Residual%2520Quantized%2520VAE%2520%2528ML-RQ%2529%2520that%2520maps%25202D%2520motion%2520sequences%2520into%25203D-consistent%2520latent%2520spaces%252C%2520and%2520a%2520suite%2520of%25203D-free%2520regularization%2520objectives%2520enforcing%2520view%2520consistency%252C%2520orientation%2520coherence%252C%2520and%2520physical%2520plausibility.%2520Trained%2520entirely%2520on%25202D%2520motion%2520data%252C%2520Free3D%2520generates%2520diverse%252C%2520temporally%2520coherent%252C%2520and%2520semantically%2520aligned%25203D%2520motions%252C%2520achieving%2520performance%2520comparable%2520to%2520or%2520even%2520surpassing%2520fully%25203D-supervised%2520counterparts.%2520These%2520results%2520suggest%2520that%2520relaxing%2520explicit%25203D%2520supervision%2520encourages%2520stronger%2520structural%2520reasoning%2520and%2520generalization%252C%2520offering%2520a%2520scalable%2520and%2520data-efficient%2520paradigm%2520for%25203D%2520motion%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11368v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Free3D%3A%203D%20Human%20Motion%20Emerges%20from%20Single-View%202D%20Supervision&entry.906535625=Sheng%20Liu%20and%20Yuanzhi%20Liang%20and%20Sidan%20Du&entry.1292438233=Recent%203D%20human%20motion%20generation%20models%20demonstrate%20remarkable%20reconstruction%20accuracy%20yet%20struggle%20to%20generalize%20beyond%20training%20distributions.%20This%20limitation%20arises%20partly%20from%20the%20use%20of%20precise%203D%20supervision%2C%20which%20encourages%20models%20to%20fit%20fixed%20coordinate%20patterns%20instead%20of%20learning%20the%20essential%203D%20structure%20and%20motion%20semantic%20cues%20required%20for%20robust%20generalization.To%20overcome%20this%20limitation%2C%20we%20propose%20Free3D%2C%20a%20framework%20that%20synthesizes%20realistic%203D%20motions%20without%20any%203D%20motion%20annotations.%20Free3D%20introduces%20a%20Motion-Lifting%20Residual%20Quantized%20VAE%20%28ML-RQ%29%20that%20maps%202D%20motion%20sequences%20into%203D-consistent%20latent%20spaces%2C%20and%20a%20suite%20of%203D-free%20regularization%20objectives%20enforcing%20view%20consistency%2C%20orientation%20coherence%2C%20and%20physical%20plausibility.%20Trained%20entirely%20on%202D%20motion%20data%2C%20Free3D%20generates%20diverse%2C%20temporally%20coherent%2C%20and%20semantically%20aligned%203D%20motions%2C%20achieving%20performance%20comparable%20to%20or%20even%20surpassing%20fully%203D-supervised%20counterparts.%20These%20results%20suggest%20that%20relaxing%20explicit%203D%20supervision%20encourages%20stronger%20structural%20reasoning%20and%20generalization%2C%20offering%20a%20scalable%20and%20data-efficient%20paradigm%20for%203D%20motion%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2511.11368v1&entry.124074799=Read"},
{"title": "Beyond Flatlands: Unlocking Spatial Intelligence by Decoupling 3D Reasoning from Numerical Regression", "author": "Zhongbin Guo and Jiahe Liu and Yushan Li and Wenyu Gao and Zhen Yang and Chenzhi Li and Xinyue Zhang and Ping Jian", "abstract": "Existing Vision Language Models (VLMs) architecturally rooted in \"flatland\" perception, fundamentally struggle to comprehend real-world 3D spatial intelligence. This failure stems from a dual-bottleneck: input-stage conflict between computationally exorbitant geometric-aware encoders and superficial 2D-only features, and output-stage misalignment where discrete tokenizers are structurally incapable of producing precise, continuous numerical values. To break this impasse, we introduce GEODE (Geometric-Output and Decoupled-Input Engine), a novel architecture that resolves this dual-bottleneck by decoupling 3D reasoning from numerical generation. GEODE augments main VLM with two specialized, plug-and-play modules: Decoupled Rationale Module (DRM) that acts as spatial co-processor, aligning explicit 3D data with 2D visual features via cross-attention and distilling spatial Chain-of-Thought (CoT) logic into injectable Rationale Tokens; and Direct Regression Head (DRH), an \"Embedding-as-Value\" paradigm which routes specialized control tokens to a lightweight MLP for precise, continuous regression of scalars and 3D bounding boxes. The synergy of these modules allows our 1.5B parameter model to function as a high-level semantic dispatcher, achieving state-of-the-art spatial reasoning performance that rivals 7B+ models.", "link": "http://arxiv.org/abs/2511.11239v1", "date": "2025-11-14", "relevancy": 2.5604, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6556}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Flatlands%3A%20Unlocking%20Spatial%20Intelligence%20by%20Decoupling%203D%20Reasoning%20from%20Numerical%20Regression&body=Title%3A%20Beyond%20Flatlands%3A%20Unlocking%20Spatial%20Intelligence%20by%20Decoupling%203D%20Reasoning%20from%20Numerical%20Regression%0AAuthor%3A%20Zhongbin%20Guo%20and%20Jiahe%20Liu%20and%20Yushan%20Li%20and%20Wenyu%20Gao%20and%20Zhen%20Yang%20and%20Chenzhi%20Li%20and%20Xinyue%20Zhang%20and%20Ping%20Jian%0AAbstract%3A%20Existing%20Vision%20Language%20Models%20%28VLMs%29%20architecturally%20rooted%20in%20%22flatland%22%20perception%2C%20fundamentally%20struggle%20to%20comprehend%20real-world%203D%20spatial%20intelligence.%20This%20failure%20stems%20from%20a%20dual-bottleneck%3A%20input-stage%20conflict%20between%20computationally%20exorbitant%20geometric-aware%20encoders%20and%20superficial%202D-only%20features%2C%20and%20output-stage%20misalignment%20where%20discrete%20tokenizers%20are%20structurally%20incapable%20of%20producing%20precise%2C%20continuous%20numerical%20values.%20To%20break%20this%20impasse%2C%20we%20introduce%20GEODE%20%28Geometric-Output%20and%20Decoupled-Input%20Engine%29%2C%20a%20novel%20architecture%20that%20resolves%20this%20dual-bottleneck%20by%20decoupling%203D%20reasoning%20from%20numerical%20generation.%20GEODE%20augments%20main%20VLM%20with%20two%20specialized%2C%20plug-and-play%20modules%3A%20Decoupled%20Rationale%20Module%20%28DRM%29%20that%20acts%20as%20spatial%20co-processor%2C%20aligning%20explicit%203D%20data%20with%202D%20visual%20features%20via%20cross-attention%20and%20distilling%20spatial%20Chain-of-Thought%20%28CoT%29%20logic%20into%20injectable%20Rationale%20Tokens%3B%20and%20Direct%20Regression%20Head%20%28DRH%29%2C%20an%20%22Embedding-as-Value%22%20paradigm%20which%20routes%20specialized%20control%20tokens%20to%20a%20lightweight%20MLP%20for%20precise%2C%20continuous%20regression%20of%20scalars%20and%203D%20bounding%20boxes.%20The%20synergy%20of%20these%20modules%20allows%20our%201.5B%20parameter%20model%20to%20function%20as%20a%20high-level%20semantic%20dispatcher%2C%20achieving%20state-of-the-art%20spatial%20reasoning%20performance%20that%20rivals%207B%2B%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Flatlands%253A%2520Unlocking%2520Spatial%2520Intelligence%2520by%2520Decoupling%25203D%2520Reasoning%2520from%2520Numerical%2520Regression%26entry.906535625%3DZhongbin%2520Guo%2520and%2520Jiahe%2520Liu%2520and%2520Yushan%2520Li%2520and%2520Wenyu%2520Gao%2520and%2520Zhen%2520Yang%2520and%2520Chenzhi%2520Li%2520and%2520Xinyue%2520Zhang%2520and%2520Ping%2520Jian%26entry.1292438233%3DExisting%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520architecturally%2520rooted%2520in%2520%2522flatland%2522%2520perception%252C%2520fundamentally%2520struggle%2520to%2520comprehend%2520real-world%25203D%2520spatial%2520intelligence.%2520This%2520failure%2520stems%2520from%2520a%2520dual-bottleneck%253A%2520input-stage%2520conflict%2520between%2520computationally%2520exorbitant%2520geometric-aware%2520encoders%2520and%2520superficial%25202D-only%2520features%252C%2520and%2520output-stage%2520misalignment%2520where%2520discrete%2520tokenizers%2520are%2520structurally%2520incapable%2520of%2520producing%2520precise%252C%2520continuous%2520numerical%2520values.%2520To%2520break%2520this%2520impasse%252C%2520we%2520introduce%2520GEODE%2520%2528Geometric-Output%2520and%2520Decoupled-Input%2520Engine%2529%252C%2520a%2520novel%2520architecture%2520that%2520resolves%2520this%2520dual-bottleneck%2520by%2520decoupling%25203D%2520reasoning%2520from%2520numerical%2520generation.%2520GEODE%2520augments%2520main%2520VLM%2520with%2520two%2520specialized%252C%2520plug-and-play%2520modules%253A%2520Decoupled%2520Rationale%2520Module%2520%2528DRM%2529%2520that%2520acts%2520as%2520spatial%2520co-processor%252C%2520aligning%2520explicit%25203D%2520data%2520with%25202D%2520visual%2520features%2520via%2520cross-attention%2520and%2520distilling%2520spatial%2520Chain-of-Thought%2520%2528CoT%2529%2520logic%2520into%2520injectable%2520Rationale%2520Tokens%253B%2520and%2520Direct%2520Regression%2520Head%2520%2528DRH%2529%252C%2520an%2520%2522Embedding-as-Value%2522%2520paradigm%2520which%2520routes%2520specialized%2520control%2520tokens%2520to%2520a%2520lightweight%2520MLP%2520for%2520precise%252C%2520continuous%2520regression%2520of%2520scalars%2520and%25203D%2520bounding%2520boxes.%2520The%2520synergy%2520of%2520these%2520modules%2520allows%2520our%25201.5B%2520parameter%2520model%2520to%2520function%2520as%2520a%2520high-level%2520semantic%2520dispatcher%252C%2520achieving%2520state-of-the-art%2520spatial%2520reasoning%2520performance%2520that%2520rivals%25207B%252B%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Flatlands%3A%20Unlocking%20Spatial%20Intelligence%20by%20Decoupling%203D%20Reasoning%20from%20Numerical%20Regression&entry.906535625=Zhongbin%20Guo%20and%20Jiahe%20Liu%20and%20Yushan%20Li%20and%20Wenyu%20Gao%20and%20Zhen%20Yang%20and%20Chenzhi%20Li%20and%20Xinyue%20Zhang%20and%20Ping%20Jian&entry.1292438233=Existing%20Vision%20Language%20Models%20%28VLMs%29%20architecturally%20rooted%20in%20%22flatland%22%20perception%2C%20fundamentally%20struggle%20to%20comprehend%20real-world%203D%20spatial%20intelligence.%20This%20failure%20stems%20from%20a%20dual-bottleneck%3A%20input-stage%20conflict%20between%20computationally%20exorbitant%20geometric-aware%20encoders%20and%20superficial%202D-only%20features%2C%20and%20output-stage%20misalignment%20where%20discrete%20tokenizers%20are%20structurally%20incapable%20of%20producing%20precise%2C%20continuous%20numerical%20values.%20To%20break%20this%20impasse%2C%20we%20introduce%20GEODE%20%28Geometric-Output%20and%20Decoupled-Input%20Engine%29%2C%20a%20novel%20architecture%20that%20resolves%20this%20dual-bottleneck%20by%20decoupling%203D%20reasoning%20from%20numerical%20generation.%20GEODE%20augments%20main%20VLM%20with%20two%20specialized%2C%20plug-and-play%20modules%3A%20Decoupled%20Rationale%20Module%20%28DRM%29%20that%20acts%20as%20spatial%20co-processor%2C%20aligning%20explicit%203D%20data%20with%202D%20visual%20features%20via%20cross-attention%20and%20distilling%20spatial%20Chain-of-Thought%20%28CoT%29%20logic%20into%20injectable%20Rationale%20Tokens%3B%20and%20Direct%20Regression%20Head%20%28DRH%29%2C%20an%20%22Embedding-as-Value%22%20paradigm%20which%20routes%20specialized%20control%20tokens%20to%20a%20lightweight%20MLP%20for%20precise%2C%20continuous%20regression%20of%20scalars%20and%203D%20bounding%20boxes.%20The%20synergy%20of%20these%20modules%20allows%20our%201.5B%20parameter%20model%20to%20function%20as%20a%20high-level%20semantic%20dispatcher%2C%20achieving%20state-of-the-art%20spatial%20reasoning%20performance%20that%20rivals%207B%2B%20models.&entry.1838667208=http%3A//arxiv.org/abs/2511.11239v1&entry.124074799=Read"},
{"title": "When Genes Speak: A Semantic-Guided Framework for Spatially Resolved Transcriptomics Data Clustering", "author": "Jiangkai Long and Yanran Zhu and Chang Tang and Kun Sun and Yuanyuan Liu and Xuesong Yan", "abstract": "Spatial transcriptomics enables gene expression profiling with spatial context, offering unprecedented insights into the tissue microenvironment. However, most computational models treat genes as isolated numerical features, ignoring the rich biological semantics encoded in their symbols. This prevents a truly deep understanding of critical biological characteristics. To overcome this limitation, we present SemST, a semantic-guided deep learning framework for spatial transcriptomics data clustering. SemST leverages Large Language Models (LLMs) to enable genes to \"speak\" through their symbolic meanings, transforming gene sets within each tissue spot into biologically informed embeddings. These embeddings are then fused with the spatial neighborhood relationships captured by Graph Neural Networks (GNNs), achieving a coherent integration of biological function and spatial structure. We further introduce the Fine-grained Semantic Modulation (FSM) module to optimally exploit these biological priors. The FSM module learns spot-specific affine transformations that empower the semantic embeddings to perform an element-wise calibration of the spatial features, thus dynamically injecting high-order biological knowledge into the spatial context. Extensive experiments on public spatial transcriptomics datasets show that SemST achieves state-of-the-art clustering performance. Crucially, the FSM module exhibits plug-and-play versatility, consistently improving the performance when integrated into other baseline methods.", "link": "http://arxiv.org/abs/2511.11380v1", "date": "2025-11-14", "relevancy": 2.5598, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5201}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5201}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Genes%20Speak%3A%20A%20Semantic-Guided%20Framework%20for%20Spatially%20Resolved%20Transcriptomics%20Data%20Clustering&body=Title%3A%20When%20Genes%20Speak%3A%20A%20Semantic-Guided%20Framework%20for%20Spatially%20Resolved%20Transcriptomics%20Data%20Clustering%0AAuthor%3A%20Jiangkai%20Long%20and%20Yanran%20Zhu%20and%20Chang%20Tang%20and%20Kun%20Sun%20and%20Yuanyuan%20Liu%20and%20Xuesong%20Yan%0AAbstract%3A%20Spatial%20transcriptomics%20enables%20gene%20expression%20profiling%20with%20spatial%20context%2C%20offering%20unprecedented%20insights%20into%20the%20tissue%20microenvironment.%20However%2C%20most%20computational%20models%20treat%20genes%20as%20isolated%20numerical%20features%2C%20ignoring%20the%20rich%20biological%20semantics%20encoded%20in%20their%20symbols.%20This%20prevents%20a%20truly%20deep%20understanding%20of%20critical%20biological%20characteristics.%20To%20overcome%20this%20limitation%2C%20we%20present%20SemST%2C%20a%20semantic-guided%20deep%20learning%20framework%20for%20spatial%20transcriptomics%20data%20clustering.%20SemST%20leverages%20Large%20Language%20Models%20%28LLMs%29%20to%20enable%20genes%20to%20%22speak%22%20through%20their%20symbolic%20meanings%2C%20transforming%20gene%20sets%20within%20each%20tissue%20spot%20into%20biologically%20informed%20embeddings.%20These%20embeddings%20are%20then%20fused%20with%20the%20spatial%20neighborhood%20relationships%20captured%20by%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20achieving%20a%20coherent%20integration%20of%20biological%20function%20and%20spatial%20structure.%20We%20further%20introduce%20the%20Fine-grained%20Semantic%20Modulation%20%28FSM%29%20module%20to%20optimally%20exploit%20these%20biological%20priors.%20The%20FSM%20module%20learns%20spot-specific%20affine%20transformations%20that%20empower%20the%20semantic%20embeddings%20to%20perform%20an%20element-wise%20calibration%20of%20the%20spatial%20features%2C%20thus%20dynamically%20injecting%20high-order%20biological%20knowledge%20into%20the%20spatial%20context.%20Extensive%20experiments%20on%20public%20spatial%20transcriptomics%20datasets%20show%20that%20SemST%20achieves%20state-of-the-art%20clustering%20performance.%20Crucially%2C%20the%20FSM%20module%20exhibits%20plug-and-play%20versatility%2C%20consistently%20improving%20the%20performance%20when%20integrated%20into%20other%20baseline%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Genes%2520Speak%253A%2520A%2520Semantic-Guided%2520Framework%2520for%2520Spatially%2520Resolved%2520Transcriptomics%2520Data%2520Clustering%26entry.906535625%3DJiangkai%2520Long%2520and%2520Yanran%2520Zhu%2520and%2520Chang%2520Tang%2520and%2520Kun%2520Sun%2520and%2520Yuanyuan%2520Liu%2520and%2520Xuesong%2520Yan%26entry.1292438233%3DSpatial%2520transcriptomics%2520enables%2520gene%2520expression%2520profiling%2520with%2520spatial%2520context%252C%2520offering%2520unprecedented%2520insights%2520into%2520the%2520tissue%2520microenvironment.%2520However%252C%2520most%2520computational%2520models%2520treat%2520genes%2520as%2520isolated%2520numerical%2520features%252C%2520ignoring%2520the%2520rich%2520biological%2520semantics%2520encoded%2520in%2520their%2520symbols.%2520This%2520prevents%2520a%2520truly%2520deep%2520understanding%2520of%2520critical%2520biological%2520characteristics.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520present%2520SemST%252C%2520a%2520semantic-guided%2520deep%2520learning%2520framework%2520for%2520spatial%2520transcriptomics%2520data%2520clustering.%2520SemST%2520leverages%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520enable%2520genes%2520to%2520%2522speak%2522%2520through%2520their%2520symbolic%2520meanings%252C%2520transforming%2520gene%2520sets%2520within%2520each%2520tissue%2520spot%2520into%2520biologically%2520informed%2520embeddings.%2520These%2520embeddings%2520are%2520then%2520fused%2520with%2520the%2520spatial%2520neighborhood%2520relationships%2520captured%2520by%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%252C%2520achieving%2520a%2520coherent%2520integration%2520of%2520biological%2520function%2520and%2520spatial%2520structure.%2520We%2520further%2520introduce%2520the%2520Fine-grained%2520Semantic%2520Modulation%2520%2528FSM%2529%2520module%2520to%2520optimally%2520exploit%2520these%2520biological%2520priors.%2520The%2520FSM%2520module%2520learns%2520spot-specific%2520affine%2520transformations%2520that%2520empower%2520the%2520semantic%2520embeddings%2520to%2520perform%2520an%2520element-wise%2520calibration%2520of%2520the%2520spatial%2520features%252C%2520thus%2520dynamically%2520injecting%2520high-order%2520biological%2520knowledge%2520into%2520the%2520spatial%2520context.%2520Extensive%2520experiments%2520on%2520public%2520spatial%2520transcriptomics%2520datasets%2520show%2520that%2520SemST%2520achieves%2520state-of-the-art%2520clustering%2520performance.%2520Crucially%252C%2520the%2520FSM%2520module%2520exhibits%2520plug-and-play%2520versatility%252C%2520consistently%2520improving%2520the%2520performance%2520when%2520integrated%2520into%2520other%2520baseline%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Genes%20Speak%3A%20A%20Semantic-Guided%20Framework%20for%20Spatially%20Resolved%20Transcriptomics%20Data%20Clustering&entry.906535625=Jiangkai%20Long%20and%20Yanran%20Zhu%20and%20Chang%20Tang%20and%20Kun%20Sun%20and%20Yuanyuan%20Liu%20and%20Xuesong%20Yan&entry.1292438233=Spatial%20transcriptomics%20enables%20gene%20expression%20profiling%20with%20spatial%20context%2C%20offering%20unprecedented%20insights%20into%20the%20tissue%20microenvironment.%20However%2C%20most%20computational%20models%20treat%20genes%20as%20isolated%20numerical%20features%2C%20ignoring%20the%20rich%20biological%20semantics%20encoded%20in%20their%20symbols.%20This%20prevents%20a%20truly%20deep%20understanding%20of%20critical%20biological%20characteristics.%20To%20overcome%20this%20limitation%2C%20we%20present%20SemST%2C%20a%20semantic-guided%20deep%20learning%20framework%20for%20spatial%20transcriptomics%20data%20clustering.%20SemST%20leverages%20Large%20Language%20Models%20%28LLMs%29%20to%20enable%20genes%20to%20%22speak%22%20through%20their%20symbolic%20meanings%2C%20transforming%20gene%20sets%20within%20each%20tissue%20spot%20into%20biologically%20informed%20embeddings.%20These%20embeddings%20are%20then%20fused%20with%20the%20spatial%20neighborhood%20relationships%20captured%20by%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20achieving%20a%20coherent%20integration%20of%20biological%20function%20and%20spatial%20structure.%20We%20further%20introduce%20the%20Fine-grained%20Semantic%20Modulation%20%28FSM%29%20module%20to%20optimally%20exploit%20these%20biological%20priors.%20The%20FSM%20module%20learns%20spot-specific%20affine%20transformations%20that%20empower%20the%20semantic%20embeddings%20to%20perform%20an%20element-wise%20calibration%20of%20the%20spatial%20features%2C%20thus%20dynamically%20injecting%20high-order%20biological%20knowledge%20into%20the%20spatial%20context.%20Extensive%20experiments%20on%20public%20spatial%20transcriptomics%20datasets%20show%20that%20SemST%20achieves%20state-of-the-art%20clustering%20performance.%20Crucially%2C%20the%20FSM%20module%20exhibits%20plug-and-play%20versatility%2C%20consistently%20improving%20the%20performance%20when%20integrated%20into%20other%20baseline%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.11380v1&entry.124074799=Read"},
{"title": "Self-Supervised Learning of Iterative Solvers for Constrained Optimization", "author": "Lukas L\u00fcken and Sergio Lucia", "abstract": "The real-time solution of parametric optimization problems is critical for applications that demand high accuracy under tight real-time constraints, such as model predictive control. To this end, this work presents a learning-based iterative solver for constrained optimization, comprising a neural network predictor that generates initial primal-dual solution estimates, followed by a learned iterative solver that refines these estimates to reach high accuracy. We introduce a novel loss function based on Karush-Kuhn-Tucker (KKT) optimality conditions, enabling fully self-supervised training without pre-sampled optimizer solutions. Theoretical guarantees ensure that the training loss function attains minima exclusively at KKT points. A convexification procedure enables application to nonconvex problems while preserving these guarantees. Experiments on two nonconvex case studies demonstrate speedups of up to one order of magnitude compared to state-of-the-art solvers such as IPOPT, while achieving orders of magnitude higher accuracy than competing learning-based approaches.", "link": "http://arxiv.org/abs/2409.08066v2", "date": "2025-11-14", "relevancy": 2.5571, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5409}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5075}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20of%20Iterative%20Solvers%20for%20Constrained%20Optimization&body=Title%3A%20Self-Supervised%20Learning%20of%20Iterative%20Solvers%20for%20Constrained%20Optimization%0AAuthor%3A%20Lukas%20L%C3%BCken%20and%20Sergio%20Lucia%0AAbstract%3A%20The%20real-time%20solution%20of%20parametric%20optimization%20problems%20is%20critical%20for%20applications%20that%20demand%20high%20accuracy%20under%20tight%20real-time%20constraints%2C%20such%20as%20model%20predictive%20control.%20To%20this%20end%2C%20this%20work%20presents%20a%20learning-based%20iterative%20solver%20for%20constrained%20optimization%2C%20comprising%20a%20neural%20network%20predictor%20that%20generates%20initial%20primal-dual%20solution%20estimates%2C%20followed%20by%20a%20learned%20iterative%20solver%20that%20refines%20these%20estimates%20to%20reach%20high%20accuracy.%20We%20introduce%20a%20novel%20loss%20function%20based%20on%20Karush-Kuhn-Tucker%20%28KKT%29%20optimality%20conditions%2C%20enabling%20fully%20self-supervised%20training%20without%20pre-sampled%20optimizer%20solutions.%20Theoretical%20guarantees%20ensure%20that%20the%20training%20loss%20function%20attains%20minima%20exclusively%20at%20KKT%20points.%20A%20convexification%20procedure%20enables%20application%20to%20nonconvex%20problems%20while%20preserving%20these%20guarantees.%20Experiments%20on%20two%20nonconvex%20case%20studies%20demonstrate%20speedups%20of%20up%20to%20one%20order%20of%20magnitude%20compared%20to%20state-of-the-art%20solvers%20such%20as%20IPOPT%2C%20while%20achieving%20orders%20of%20magnitude%20higher%20accuracy%20than%20competing%20learning-based%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2409.08066v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Learning%2520of%2520Iterative%2520Solvers%2520for%2520Constrained%2520Optimization%26entry.906535625%3DLukas%2520L%25C3%25BCken%2520and%2520Sergio%2520Lucia%26entry.1292438233%3DThe%2520real-time%2520solution%2520of%2520parametric%2520optimization%2520problems%2520is%2520critical%2520for%2520applications%2520that%2520demand%2520high%2520accuracy%2520under%2520tight%2520real-time%2520constraints%252C%2520such%2520as%2520model%2520predictive%2520control.%2520To%2520this%2520end%252C%2520this%2520work%2520presents%2520a%2520learning-based%2520iterative%2520solver%2520for%2520constrained%2520optimization%252C%2520comprising%2520a%2520neural%2520network%2520predictor%2520that%2520generates%2520initial%2520primal-dual%2520solution%2520estimates%252C%2520followed%2520by%2520a%2520learned%2520iterative%2520solver%2520that%2520refines%2520these%2520estimates%2520to%2520reach%2520high%2520accuracy.%2520We%2520introduce%2520a%2520novel%2520loss%2520function%2520based%2520on%2520Karush-Kuhn-Tucker%2520%2528KKT%2529%2520optimality%2520conditions%252C%2520enabling%2520fully%2520self-supervised%2520training%2520without%2520pre-sampled%2520optimizer%2520solutions.%2520Theoretical%2520guarantees%2520ensure%2520that%2520the%2520training%2520loss%2520function%2520attains%2520minima%2520exclusively%2520at%2520KKT%2520points.%2520A%2520convexification%2520procedure%2520enables%2520application%2520to%2520nonconvex%2520problems%2520while%2520preserving%2520these%2520guarantees.%2520Experiments%2520on%2520two%2520nonconvex%2520case%2520studies%2520demonstrate%2520speedups%2520of%2520up%2520to%2520one%2520order%2520of%2520magnitude%2520compared%2520to%2520state-of-the-art%2520solvers%2520such%2520as%2520IPOPT%252C%2520while%2520achieving%2520orders%2520of%2520magnitude%2520higher%2520accuracy%2520than%2520competing%2520learning-based%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08066v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20of%20Iterative%20Solvers%20for%20Constrained%20Optimization&entry.906535625=Lukas%20L%C3%BCken%20and%20Sergio%20Lucia&entry.1292438233=The%20real-time%20solution%20of%20parametric%20optimization%20problems%20is%20critical%20for%20applications%20that%20demand%20high%20accuracy%20under%20tight%20real-time%20constraints%2C%20such%20as%20model%20predictive%20control.%20To%20this%20end%2C%20this%20work%20presents%20a%20learning-based%20iterative%20solver%20for%20constrained%20optimization%2C%20comprising%20a%20neural%20network%20predictor%20that%20generates%20initial%20primal-dual%20solution%20estimates%2C%20followed%20by%20a%20learned%20iterative%20solver%20that%20refines%20these%20estimates%20to%20reach%20high%20accuracy.%20We%20introduce%20a%20novel%20loss%20function%20based%20on%20Karush-Kuhn-Tucker%20%28KKT%29%20optimality%20conditions%2C%20enabling%20fully%20self-supervised%20training%20without%20pre-sampled%20optimizer%20solutions.%20Theoretical%20guarantees%20ensure%20that%20the%20training%20loss%20function%20attains%20minima%20exclusively%20at%20KKT%20points.%20A%20convexification%20procedure%20enables%20application%20to%20nonconvex%20problems%20while%20preserving%20these%20guarantees.%20Experiments%20on%20two%20nonconvex%20case%20studies%20demonstrate%20speedups%20of%20up%20to%20one%20order%20of%20magnitude%20compared%20to%20state-of-the-art%20solvers%20such%20as%20IPOPT%2C%20while%20achieving%20orders%20of%20magnitude%20higher%20accuracy%20than%20competing%20learning-based%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2409.08066v2&entry.124074799=Read"},
{"title": "Non-Euclidean SGD for Structured Optimization: Unified Analysis and Improved Rates", "author": "Dmitry Kovalev and Ekaterina Borodich", "abstract": "Recently, several instances of non-Euclidean SGD, including SignSGD, Lion, and Muon, have attracted significant interest from the optimization community due to their practical success in training deep neural networks. Consequently, a number of works have attempted to explain this success by developing theoretical convergence analyses. Unfortunately, these results cannot properly justify the superior performance of these methods, as they could not beat the convergence rate of vanilla Euclidean SGD. We resolve this important open problem by developing a new unified convergence analysis under the structured smoothness and gradient noise assumption. In particular, our results indicate that non-Euclidean SGD (i) can exploit the sparsity or low-rank structure of the upper bounds on the Hessian and gradient noise, (ii) can provably benefit from popular algorithmic tools such as extrapolation or momentum variance reduction, and (iii) can match the state-of-the-art convergence rates of adaptive and more complex optimization algorithms such as AdaGrad and Shampoo.", "link": "http://arxiv.org/abs/2511.11466v1", "date": "2025-11-14", "relevancy": 2.5361, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5264}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5028}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Euclidean%20SGD%20for%20Structured%20Optimization%3A%20Unified%20Analysis%20and%20Improved%20Rates&body=Title%3A%20Non-Euclidean%20SGD%20for%20Structured%20Optimization%3A%20Unified%20Analysis%20and%20Improved%20Rates%0AAuthor%3A%20Dmitry%20Kovalev%20and%20Ekaterina%20Borodich%0AAbstract%3A%20Recently%2C%20several%20instances%20of%20non-Euclidean%20SGD%2C%20including%20SignSGD%2C%20Lion%2C%20and%20Muon%2C%20have%20attracted%20significant%20interest%20from%20the%20optimization%20community%20due%20to%20their%20practical%20success%20in%20training%20deep%20neural%20networks.%20Consequently%2C%20a%20number%20of%20works%20have%20attempted%20to%20explain%20this%20success%20by%20developing%20theoretical%20convergence%20analyses.%20Unfortunately%2C%20these%20results%20cannot%20properly%20justify%20the%20superior%20performance%20of%20these%20methods%2C%20as%20they%20could%20not%20beat%20the%20convergence%20rate%20of%20vanilla%20Euclidean%20SGD.%20We%20resolve%20this%20important%20open%20problem%20by%20developing%20a%20new%20unified%20convergence%20analysis%20under%20the%20structured%20smoothness%20and%20gradient%20noise%20assumption.%20In%20particular%2C%20our%20results%20indicate%20that%20non-Euclidean%20SGD%20%28i%29%20can%20exploit%20the%20sparsity%20or%20low-rank%20structure%20of%20the%20upper%20bounds%20on%20the%20Hessian%20and%20gradient%20noise%2C%20%28ii%29%20can%20provably%20benefit%20from%20popular%20algorithmic%20tools%20such%20as%20extrapolation%20or%20momentum%20variance%20reduction%2C%20and%20%28iii%29%20can%20match%20the%20state-of-the-art%20convergence%20rates%20of%20adaptive%20and%20more%20complex%20optimization%20algorithms%20such%20as%20AdaGrad%20and%20Shampoo.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11466v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Euclidean%2520SGD%2520for%2520Structured%2520Optimization%253A%2520Unified%2520Analysis%2520and%2520Improved%2520Rates%26entry.906535625%3DDmitry%2520Kovalev%2520and%2520Ekaterina%2520Borodich%26entry.1292438233%3DRecently%252C%2520several%2520instances%2520of%2520non-Euclidean%2520SGD%252C%2520including%2520SignSGD%252C%2520Lion%252C%2520and%2520Muon%252C%2520have%2520attracted%2520significant%2520interest%2520from%2520the%2520optimization%2520community%2520due%2520to%2520their%2520practical%2520success%2520in%2520training%2520deep%2520neural%2520networks.%2520Consequently%252C%2520a%2520number%2520of%2520works%2520have%2520attempted%2520to%2520explain%2520this%2520success%2520by%2520developing%2520theoretical%2520convergence%2520analyses.%2520Unfortunately%252C%2520these%2520results%2520cannot%2520properly%2520justify%2520the%2520superior%2520performance%2520of%2520these%2520methods%252C%2520as%2520they%2520could%2520not%2520beat%2520the%2520convergence%2520rate%2520of%2520vanilla%2520Euclidean%2520SGD.%2520We%2520resolve%2520this%2520important%2520open%2520problem%2520by%2520developing%2520a%2520new%2520unified%2520convergence%2520analysis%2520under%2520the%2520structured%2520smoothness%2520and%2520gradient%2520noise%2520assumption.%2520In%2520particular%252C%2520our%2520results%2520indicate%2520that%2520non-Euclidean%2520SGD%2520%2528i%2529%2520can%2520exploit%2520the%2520sparsity%2520or%2520low-rank%2520structure%2520of%2520the%2520upper%2520bounds%2520on%2520the%2520Hessian%2520and%2520gradient%2520noise%252C%2520%2528ii%2529%2520can%2520provably%2520benefit%2520from%2520popular%2520algorithmic%2520tools%2520such%2520as%2520extrapolation%2520or%2520momentum%2520variance%2520reduction%252C%2520and%2520%2528iii%2529%2520can%2520match%2520the%2520state-of-the-art%2520convergence%2520rates%2520of%2520adaptive%2520and%2520more%2520complex%2520optimization%2520algorithms%2520such%2520as%2520AdaGrad%2520and%2520Shampoo.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11466v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Euclidean%20SGD%20for%20Structured%20Optimization%3A%20Unified%20Analysis%20and%20Improved%20Rates&entry.906535625=Dmitry%20Kovalev%20and%20Ekaterina%20Borodich&entry.1292438233=Recently%2C%20several%20instances%20of%20non-Euclidean%20SGD%2C%20including%20SignSGD%2C%20Lion%2C%20and%20Muon%2C%20have%20attracted%20significant%20interest%20from%20the%20optimization%20community%20due%20to%20their%20practical%20success%20in%20training%20deep%20neural%20networks.%20Consequently%2C%20a%20number%20of%20works%20have%20attempted%20to%20explain%20this%20success%20by%20developing%20theoretical%20convergence%20analyses.%20Unfortunately%2C%20these%20results%20cannot%20properly%20justify%20the%20superior%20performance%20of%20these%20methods%2C%20as%20they%20could%20not%20beat%20the%20convergence%20rate%20of%20vanilla%20Euclidean%20SGD.%20We%20resolve%20this%20important%20open%20problem%20by%20developing%20a%20new%20unified%20convergence%20analysis%20under%20the%20structured%20smoothness%20and%20gradient%20noise%20assumption.%20In%20particular%2C%20our%20results%20indicate%20that%20non-Euclidean%20SGD%20%28i%29%20can%20exploit%20the%20sparsity%20or%20low-rank%20structure%20of%20the%20upper%20bounds%20on%20the%20Hessian%20and%20gradient%20noise%2C%20%28ii%29%20can%20provably%20benefit%20from%20popular%20algorithmic%20tools%20such%20as%20extrapolation%20or%20momentum%20variance%20reduction%2C%20and%20%28iii%29%20can%20match%20the%20state-of-the-art%20convergence%20rates%20of%20adaptive%20and%20more%20complex%20optimization%20algorithms%20such%20as%20AdaGrad%20and%20Shampoo.&entry.1838667208=http%3A//arxiv.org/abs/2511.11466v1&entry.124074799=Read"},
{"title": "Retrofit: Continual Learning with Bounded Forgetting for Security Applications", "author": "Yiling He and Junchi Lei and Hongyu She and Shuo Shao and Xinran Zheng and Yiping Liu and Zhan Qin and Lorenzo Cavallaro", "abstract": "Modern security analytics are increasingly powered by deep learning models, but their performance often degrades as threat landscapes evolve and data representations shift. While continual learning (CL) offers a promising paradigm to maintain model effectiveness, many approaches rely on full retraining or data replay, which are infeasible in data-sensitive environments. Moreover, existing methods remain inadequate for security-critical scenarios, facing two coupled challenges in knowledge transfer: preserving prior knowledge without old data and integrating new knowledge with minimal interference.\n  We propose RETROFIT, a data retrospective-free continual learning method that achieves bounded forgetting for effective knowledge transfer. Our key idea is to consolidate previously trained and newly fine-tuned models, serving as teachers of old and new knowledge, through parameter-level merging that eliminates the need for historical data. To mitigate interference, we apply low-rank and sparse updates that confine parameter changes to independent subspaces, while a knowledge arbitration dynamically balances the teacher contributions guided by model confidence. Our evaluation on two representative applications demonstrates that RETROFIT consistently mitigates forgetting while maintaining adaptability. In malware detection under temporal drift, it substantially improves the retention score, from 20.2% to 38.6% over CL baselines, and exceeds the oracle upper bound on new data. In binary summarization across decompilation levels, where analyzing stripped binaries is especially challenging, RETROFIT achieves around twice the BLEU score of transfer learning used in prior work and surpasses all baselines in cross-representation generalization.", "link": "http://arxiv.org/abs/2511.11439v1", "date": "2025-11-14", "relevancy": 2.5315, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5179}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5061}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrofit%3A%20Continual%20Learning%20with%20Bounded%20Forgetting%20for%20Security%20Applications&body=Title%3A%20Retrofit%3A%20Continual%20Learning%20with%20Bounded%20Forgetting%20for%20Security%20Applications%0AAuthor%3A%20Yiling%20He%20and%20Junchi%20Lei%20and%20Hongyu%20She%20and%20Shuo%20Shao%20and%20Xinran%20Zheng%20and%20Yiping%20Liu%20and%20Zhan%20Qin%20and%20Lorenzo%20Cavallaro%0AAbstract%3A%20Modern%20security%20analytics%20are%20increasingly%20powered%20by%20deep%20learning%20models%2C%20but%20their%20performance%20often%20degrades%20as%20threat%20landscapes%20evolve%20and%20data%20representations%20shift.%20While%20continual%20learning%20%28CL%29%20offers%20a%20promising%20paradigm%20to%20maintain%20model%20effectiveness%2C%20many%20approaches%20rely%20on%20full%20retraining%20or%20data%20replay%2C%20which%20are%20infeasible%20in%20data-sensitive%20environments.%20Moreover%2C%20existing%20methods%20remain%20inadequate%20for%20security-critical%20scenarios%2C%20facing%20two%20coupled%20challenges%20in%20knowledge%20transfer%3A%20preserving%20prior%20knowledge%20without%20old%20data%20and%20integrating%20new%20knowledge%20with%20minimal%20interference.%0A%20%20We%20propose%20RETROFIT%2C%20a%20data%20retrospective-free%20continual%20learning%20method%20that%20achieves%20bounded%20forgetting%20for%20effective%20knowledge%20transfer.%20Our%20key%20idea%20is%20to%20consolidate%20previously%20trained%20and%20newly%20fine-tuned%20models%2C%20serving%20as%20teachers%20of%20old%20and%20new%20knowledge%2C%20through%20parameter-level%20merging%20that%20eliminates%20the%20need%20for%20historical%20data.%20To%20mitigate%20interference%2C%20we%20apply%20low-rank%20and%20sparse%20updates%20that%20confine%20parameter%20changes%20to%20independent%20subspaces%2C%20while%20a%20knowledge%20arbitration%20dynamically%20balances%20the%20teacher%20contributions%20guided%20by%20model%20confidence.%20Our%20evaluation%20on%20two%20representative%20applications%20demonstrates%20that%20RETROFIT%20consistently%20mitigates%20forgetting%20while%20maintaining%20adaptability.%20In%20malware%20detection%20under%20temporal%20drift%2C%20it%20substantially%20improves%20the%20retention%20score%2C%20from%2020.2%25%20to%2038.6%25%20over%20CL%20baselines%2C%20and%20exceeds%20the%20oracle%20upper%20bound%20on%20new%20data.%20In%20binary%20summarization%20across%20decompilation%20levels%2C%20where%20analyzing%20stripped%20binaries%20is%20especially%20challenging%2C%20RETROFIT%20achieves%20around%20twice%20the%20BLEU%20score%20of%20transfer%20learning%20used%20in%20prior%20work%20and%20surpasses%20all%20baselines%20in%20cross-representation%20generalization.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrofit%253A%2520Continual%2520Learning%2520with%2520Bounded%2520Forgetting%2520for%2520Security%2520Applications%26entry.906535625%3DYiling%2520He%2520and%2520Junchi%2520Lei%2520and%2520Hongyu%2520She%2520and%2520Shuo%2520Shao%2520and%2520Xinran%2520Zheng%2520and%2520Yiping%2520Liu%2520and%2520Zhan%2520Qin%2520and%2520Lorenzo%2520Cavallaro%26entry.1292438233%3DModern%2520security%2520analytics%2520are%2520increasingly%2520powered%2520by%2520deep%2520learning%2520models%252C%2520but%2520their%2520performance%2520often%2520degrades%2520as%2520threat%2520landscapes%2520evolve%2520and%2520data%2520representations%2520shift.%2520While%2520continual%2520learning%2520%2528CL%2529%2520offers%2520a%2520promising%2520paradigm%2520to%2520maintain%2520model%2520effectiveness%252C%2520many%2520approaches%2520rely%2520on%2520full%2520retraining%2520or%2520data%2520replay%252C%2520which%2520are%2520infeasible%2520in%2520data-sensitive%2520environments.%2520Moreover%252C%2520existing%2520methods%2520remain%2520inadequate%2520for%2520security-critical%2520scenarios%252C%2520facing%2520two%2520coupled%2520challenges%2520in%2520knowledge%2520transfer%253A%2520preserving%2520prior%2520knowledge%2520without%2520old%2520data%2520and%2520integrating%2520new%2520knowledge%2520with%2520minimal%2520interference.%250A%2520%2520We%2520propose%2520RETROFIT%252C%2520a%2520data%2520retrospective-free%2520continual%2520learning%2520method%2520that%2520achieves%2520bounded%2520forgetting%2520for%2520effective%2520knowledge%2520transfer.%2520Our%2520key%2520idea%2520is%2520to%2520consolidate%2520previously%2520trained%2520and%2520newly%2520fine-tuned%2520models%252C%2520serving%2520as%2520teachers%2520of%2520old%2520and%2520new%2520knowledge%252C%2520through%2520parameter-level%2520merging%2520that%2520eliminates%2520the%2520need%2520for%2520historical%2520data.%2520To%2520mitigate%2520interference%252C%2520we%2520apply%2520low-rank%2520and%2520sparse%2520updates%2520that%2520confine%2520parameter%2520changes%2520to%2520independent%2520subspaces%252C%2520while%2520a%2520knowledge%2520arbitration%2520dynamically%2520balances%2520the%2520teacher%2520contributions%2520guided%2520by%2520model%2520confidence.%2520Our%2520evaluation%2520on%2520two%2520representative%2520applications%2520demonstrates%2520that%2520RETROFIT%2520consistently%2520mitigates%2520forgetting%2520while%2520maintaining%2520adaptability.%2520In%2520malware%2520detection%2520under%2520temporal%2520drift%252C%2520it%2520substantially%2520improves%2520the%2520retention%2520score%252C%2520from%252020.2%2525%2520to%252038.6%2525%2520over%2520CL%2520baselines%252C%2520and%2520exceeds%2520the%2520oracle%2520upper%2520bound%2520on%2520new%2520data.%2520In%2520binary%2520summarization%2520across%2520decompilation%2520levels%252C%2520where%2520analyzing%2520stripped%2520binaries%2520is%2520especially%2520challenging%252C%2520RETROFIT%2520achieves%2520around%2520twice%2520the%2520BLEU%2520score%2520of%2520transfer%2520learning%2520used%2520in%2520prior%2520work%2520and%2520surpasses%2520all%2520baselines%2520in%2520cross-representation%2520generalization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrofit%3A%20Continual%20Learning%20with%20Bounded%20Forgetting%20for%20Security%20Applications&entry.906535625=Yiling%20He%20and%20Junchi%20Lei%20and%20Hongyu%20She%20and%20Shuo%20Shao%20and%20Xinran%20Zheng%20and%20Yiping%20Liu%20and%20Zhan%20Qin%20and%20Lorenzo%20Cavallaro&entry.1292438233=Modern%20security%20analytics%20are%20increasingly%20powered%20by%20deep%20learning%20models%2C%20but%20their%20performance%20often%20degrades%20as%20threat%20landscapes%20evolve%20and%20data%20representations%20shift.%20While%20continual%20learning%20%28CL%29%20offers%20a%20promising%20paradigm%20to%20maintain%20model%20effectiveness%2C%20many%20approaches%20rely%20on%20full%20retraining%20or%20data%20replay%2C%20which%20are%20infeasible%20in%20data-sensitive%20environments.%20Moreover%2C%20existing%20methods%20remain%20inadequate%20for%20security-critical%20scenarios%2C%20facing%20two%20coupled%20challenges%20in%20knowledge%20transfer%3A%20preserving%20prior%20knowledge%20without%20old%20data%20and%20integrating%20new%20knowledge%20with%20minimal%20interference.%0A%20%20We%20propose%20RETROFIT%2C%20a%20data%20retrospective-free%20continual%20learning%20method%20that%20achieves%20bounded%20forgetting%20for%20effective%20knowledge%20transfer.%20Our%20key%20idea%20is%20to%20consolidate%20previously%20trained%20and%20newly%20fine-tuned%20models%2C%20serving%20as%20teachers%20of%20old%20and%20new%20knowledge%2C%20through%20parameter-level%20merging%20that%20eliminates%20the%20need%20for%20historical%20data.%20To%20mitigate%20interference%2C%20we%20apply%20low-rank%20and%20sparse%20updates%20that%20confine%20parameter%20changes%20to%20independent%20subspaces%2C%20while%20a%20knowledge%20arbitration%20dynamically%20balances%20the%20teacher%20contributions%20guided%20by%20model%20confidence.%20Our%20evaluation%20on%20two%20representative%20applications%20demonstrates%20that%20RETROFIT%20consistently%20mitigates%20forgetting%20while%20maintaining%20adaptability.%20In%20malware%20detection%20under%20temporal%20drift%2C%20it%20substantially%20improves%20the%20retention%20score%2C%20from%2020.2%25%20to%2038.6%25%20over%20CL%20baselines%2C%20and%20exceeds%20the%20oracle%20upper%20bound%20on%20new%20data.%20In%20binary%20summarization%20across%20decompilation%20levels%2C%20where%20analyzing%20stripped%20binaries%20is%20especially%20challenging%2C%20RETROFIT%20achieves%20around%20twice%20the%20BLEU%20score%20of%20transfer%20learning%20used%20in%20prior%20work%20and%20surpasses%20all%20baselines%20in%20cross-representation%20generalization.&entry.1838667208=http%3A//arxiv.org/abs/2511.11439v1&entry.124074799=Read"},
{"title": "$\\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge", "author": "Core Francisco Park and Zechen Zhang and Hidenori Tanaka", "abstract": "Humans and intelligent animals can internalize new information and accurately internalize their implications to perform downstream tasks. While large language models (LLMs) can achieve this through in-context learning (ICL) when the information (news) is explicitly given as context, adequately integrating the information into model weights via fine-tuning remains challenging. In this paper, we introduce New News, a dataset composed of hypothetical yet plausible news spanning multiple domains (mathematics, coding, discoveries, leaderboards, events), accompanied by downstream evaluation questions whose correct answers critically depend on understanding and internalizing the news. First, we demonstrate a substantial gap between naive fine-tuning and in-context learning (FT-ICL gap) on our dataset. To address this gap, we explore a suite of self-play data generation protocols -- paraphrases, implications, and Self-QA -- designed to distill the knowledge processed by the model with context into the weights of the model, which we term System-2 Fine-tuning (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance across data domains and model scales with the Qwen 2.5 family of models. Our results demonstrate that the Self-QA protocol of Sys2-FT significantly improves models' in-weight learning of the news while preserving general capabilities. Furthermore, we discover the contextual shadowing effect, where training with the news in context followed by its rephrases or QAs catastrophically degrades learning of the news. Finally, we show preliminary evidence of an emerging scaling law of Sys2-FT.", "link": "http://arxiv.org/abs/2505.01812v3", "date": "2025-11-14", "relevancy": 2.5296, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5124}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5124}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%5Ctextit%7BNew%20News%7D%24%3A%20System-2%20Fine-tuning%20for%20Robust%20Integration%20of%20New%20Knowledge&body=Title%3A%20%24%5Ctextit%7BNew%20News%7D%24%3A%20System-2%20Fine-tuning%20for%20Robust%20Integration%20of%20New%20Knowledge%0AAuthor%3A%20Core%20Francisco%20Park%20and%20Zechen%20Zhang%20and%20Hidenori%20Tanaka%0AAbstract%3A%20Humans%20and%20intelligent%20animals%20can%20internalize%20new%20information%20and%20accurately%20internalize%20their%20implications%20to%20perform%20downstream%20tasks.%20While%20large%20language%20models%20%28LLMs%29%20can%20achieve%20this%20through%20in-context%20learning%20%28ICL%29%20when%20the%20information%20%28news%29%20is%20explicitly%20given%20as%20context%2C%20adequately%20integrating%20the%20information%20into%20model%20weights%20via%20fine-tuning%20remains%20challenging.%20In%20this%20paper%2C%20we%20introduce%20New%20News%2C%20a%20dataset%20composed%20of%20hypothetical%20yet%20plausible%20news%20spanning%20multiple%20domains%20%28mathematics%2C%20coding%2C%20discoveries%2C%20leaderboards%2C%20events%29%2C%20accompanied%20by%20downstream%20evaluation%20questions%20whose%20correct%20answers%20critically%20depend%20on%20understanding%20and%20internalizing%20the%20news.%20First%2C%20we%20demonstrate%20a%20substantial%20gap%20between%20naive%20fine-tuning%20and%20in-context%20learning%20%28FT-ICL%20gap%29%20on%20our%20dataset.%20To%20address%20this%20gap%2C%20we%20explore%20a%20suite%20of%20self-play%20data%20generation%20protocols%20--%20paraphrases%2C%20implications%2C%20and%20Self-QA%20--%20designed%20to%20distill%20the%20knowledge%20processed%20by%20the%20model%20with%20context%20into%20the%20weights%20of%20the%20model%2C%20which%20we%20term%20System-2%20Fine-tuning%20%28Sys2-FT%29.%20We%20systematically%20evaluate%20ICL%20and%20Sys2-FT%20performance%20across%20data%20domains%20and%20model%20scales%20with%20the%20Qwen%202.5%20family%20of%20models.%20Our%20results%20demonstrate%20that%20the%20Self-QA%20protocol%20of%20Sys2-FT%20significantly%20improves%20models%27%20in-weight%20learning%20of%20the%20news%20while%20preserving%20general%20capabilities.%20Furthermore%2C%20we%20discover%20the%20contextual%20shadowing%20effect%2C%20where%20training%20with%20the%20news%20in%20context%20followed%20by%20its%20rephrases%20or%20QAs%20catastrophically%20degrades%20learning%20of%20the%20news.%20Finally%2C%20we%20show%20preliminary%20evidence%20of%20an%20emerging%20scaling%20law%20of%20Sys2-FT.%0ALink%3A%20http%3A//arxiv.org/abs/2505.01812v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%255Ctextit%257BNew%2520News%257D%2524%253A%2520System-2%2520Fine-tuning%2520for%2520Robust%2520Integration%2520of%2520New%2520Knowledge%26entry.906535625%3DCore%2520Francisco%2520Park%2520and%2520Zechen%2520Zhang%2520and%2520Hidenori%2520Tanaka%26entry.1292438233%3DHumans%2520and%2520intelligent%2520animals%2520can%2520internalize%2520new%2520information%2520and%2520accurately%2520internalize%2520their%2520implications%2520to%2520perform%2520downstream%2520tasks.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520can%2520achieve%2520this%2520through%2520in-context%2520learning%2520%2528ICL%2529%2520when%2520the%2520information%2520%2528news%2529%2520is%2520explicitly%2520given%2520as%2520context%252C%2520adequately%2520integrating%2520the%2520information%2520into%2520model%2520weights%2520via%2520fine-tuning%2520remains%2520challenging.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520New%2520News%252C%2520a%2520dataset%2520composed%2520of%2520hypothetical%2520yet%2520plausible%2520news%2520spanning%2520multiple%2520domains%2520%2528mathematics%252C%2520coding%252C%2520discoveries%252C%2520leaderboards%252C%2520events%2529%252C%2520accompanied%2520by%2520downstream%2520evaluation%2520questions%2520whose%2520correct%2520answers%2520critically%2520depend%2520on%2520understanding%2520and%2520internalizing%2520the%2520news.%2520First%252C%2520we%2520demonstrate%2520a%2520substantial%2520gap%2520between%2520naive%2520fine-tuning%2520and%2520in-context%2520learning%2520%2528FT-ICL%2520gap%2529%2520on%2520our%2520dataset.%2520To%2520address%2520this%2520gap%252C%2520we%2520explore%2520a%2520suite%2520of%2520self-play%2520data%2520generation%2520protocols%2520--%2520paraphrases%252C%2520implications%252C%2520and%2520Self-QA%2520--%2520designed%2520to%2520distill%2520the%2520knowledge%2520processed%2520by%2520the%2520model%2520with%2520context%2520into%2520the%2520weights%2520of%2520the%2520model%252C%2520which%2520we%2520term%2520System-2%2520Fine-tuning%2520%2528Sys2-FT%2529.%2520We%2520systematically%2520evaluate%2520ICL%2520and%2520Sys2-FT%2520performance%2520across%2520data%2520domains%2520and%2520model%2520scales%2520with%2520the%2520Qwen%25202.5%2520family%2520of%2520models.%2520Our%2520results%2520demonstrate%2520that%2520the%2520Self-QA%2520protocol%2520of%2520Sys2-FT%2520significantly%2520improves%2520models%2527%2520in-weight%2520learning%2520of%2520the%2520news%2520while%2520preserving%2520general%2520capabilities.%2520Furthermore%252C%2520we%2520discover%2520the%2520contextual%2520shadowing%2520effect%252C%2520where%2520training%2520with%2520the%2520news%2520in%2520context%2520followed%2520by%2520its%2520rephrases%2520or%2520QAs%2520catastrophically%2520degrades%2520learning%2520of%2520the%2520news.%2520Finally%252C%2520we%2520show%2520preliminary%2520evidence%2520of%2520an%2520emerging%2520scaling%2520law%2520of%2520Sys2-FT.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01812v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%5Ctextit%7BNew%20News%7D%24%3A%20System-2%20Fine-tuning%20for%20Robust%20Integration%20of%20New%20Knowledge&entry.906535625=Core%20Francisco%20Park%20and%20Zechen%20Zhang%20and%20Hidenori%20Tanaka&entry.1292438233=Humans%20and%20intelligent%20animals%20can%20internalize%20new%20information%20and%20accurately%20internalize%20their%20implications%20to%20perform%20downstream%20tasks.%20While%20large%20language%20models%20%28LLMs%29%20can%20achieve%20this%20through%20in-context%20learning%20%28ICL%29%20when%20the%20information%20%28news%29%20is%20explicitly%20given%20as%20context%2C%20adequately%20integrating%20the%20information%20into%20model%20weights%20via%20fine-tuning%20remains%20challenging.%20In%20this%20paper%2C%20we%20introduce%20New%20News%2C%20a%20dataset%20composed%20of%20hypothetical%20yet%20plausible%20news%20spanning%20multiple%20domains%20%28mathematics%2C%20coding%2C%20discoveries%2C%20leaderboards%2C%20events%29%2C%20accompanied%20by%20downstream%20evaluation%20questions%20whose%20correct%20answers%20critically%20depend%20on%20understanding%20and%20internalizing%20the%20news.%20First%2C%20we%20demonstrate%20a%20substantial%20gap%20between%20naive%20fine-tuning%20and%20in-context%20learning%20%28FT-ICL%20gap%29%20on%20our%20dataset.%20To%20address%20this%20gap%2C%20we%20explore%20a%20suite%20of%20self-play%20data%20generation%20protocols%20--%20paraphrases%2C%20implications%2C%20and%20Self-QA%20--%20designed%20to%20distill%20the%20knowledge%20processed%20by%20the%20model%20with%20context%20into%20the%20weights%20of%20the%20model%2C%20which%20we%20term%20System-2%20Fine-tuning%20%28Sys2-FT%29.%20We%20systematically%20evaluate%20ICL%20and%20Sys2-FT%20performance%20across%20data%20domains%20and%20model%20scales%20with%20the%20Qwen%202.5%20family%20of%20models.%20Our%20results%20demonstrate%20that%20the%20Self-QA%20protocol%20of%20Sys2-FT%20significantly%20improves%20models%27%20in-weight%20learning%20of%20the%20news%20while%20preserving%20general%20capabilities.%20Furthermore%2C%20we%20discover%20the%20contextual%20shadowing%20effect%2C%20where%20training%20with%20the%20news%20in%20context%20followed%20by%20its%20rephrases%20or%20QAs%20catastrophically%20degrades%20learning%20of%20the%20news.%20Finally%2C%20we%20show%20preliminary%20evidence%20of%20an%20emerging%20scaling%20law%20of%20Sys2-FT.&entry.1838667208=http%3A//arxiv.org/abs/2505.01812v3&entry.124074799=Read"},
{"title": "PAS : Prelim Attention Score for Detecting Object Hallucinations in Large Vision--Language Models", "author": "Nhat Hoang-Xuan and Minh Vu and My T. Thai and Manish Bhattarai", "abstract": "Large vision-language models (LVLMs) are powerful, yet they remain unreliable due to object hallucinations. In this work, we show that in many hallucinatory predictions the LVLM effectively ignores the image and instead relies on previously generated output (prelim) tokens to infer new objects. We quantify this behavior via the mutual information between the image and the predicted object conditioned on the prelim, demonstrating that weak image dependence strongly correlates with hallucination. Building on this finding, we introduce the Prelim Attention Score (PAS), a lightweight, training-free signal computed from attention weights over prelim tokens. PAS requires no additional forward passes and can be computed on the fly during inference. Exploiting this previously overlooked signal, PAS achieves state-of-the-art object-hallucination detection across multiple models and datasets, enabling real-time filtering and intervention.", "link": "http://arxiv.org/abs/2511.11502v1", "date": "2025-11-14", "relevancy": 2.522, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5021}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAS%20%3A%20Prelim%20Attention%20Score%20for%20Detecting%20Object%20Hallucinations%20in%20Large%20Vision--Language%20Models&body=Title%3A%20PAS%20%3A%20Prelim%20Attention%20Score%20for%20Detecting%20Object%20Hallucinations%20in%20Large%20Vision--Language%20Models%0AAuthor%3A%20Nhat%20Hoang-Xuan%20and%20Minh%20Vu%20and%20My%20T.%20Thai%20and%20Manish%20Bhattarai%0AAbstract%3A%20Large%20vision-language%20models%20%28LVLMs%29%20are%20powerful%2C%20yet%20they%20remain%20unreliable%20due%20to%20object%20hallucinations.%20In%20this%20work%2C%20we%20show%20that%20in%20many%20hallucinatory%20predictions%20the%20LVLM%20effectively%20ignores%20the%20image%20and%20instead%20relies%20on%20previously%20generated%20output%20%28prelim%29%20tokens%20to%20infer%20new%20objects.%20We%20quantify%20this%20behavior%20via%20the%20mutual%20information%20between%20the%20image%20and%20the%20predicted%20object%20conditioned%20on%20the%20prelim%2C%20demonstrating%20that%20weak%20image%20dependence%20strongly%20correlates%20with%20hallucination.%20Building%20on%20this%20finding%2C%20we%20introduce%20the%20Prelim%20Attention%20Score%20%28PAS%29%2C%20a%20lightweight%2C%20training-free%20signal%20computed%20from%20attention%20weights%20over%20prelim%20tokens.%20PAS%20requires%20no%20additional%20forward%20passes%20and%20can%20be%20computed%20on%20the%20fly%20during%20inference.%20Exploiting%20this%20previously%20overlooked%20signal%2C%20PAS%20achieves%20state-of-the-art%20object-hallucination%20detection%20across%20multiple%20models%20and%20datasets%2C%20enabling%20real-time%20filtering%20and%20intervention.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAS%2520%253A%2520Prelim%2520Attention%2520Score%2520for%2520Detecting%2520Object%2520Hallucinations%2520in%2520Large%2520Vision--Language%2520Models%26entry.906535625%3DNhat%2520Hoang-Xuan%2520and%2520Minh%2520Vu%2520and%2520My%2520T.%2520Thai%2520and%2520Manish%2520Bhattarai%26entry.1292438233%3DLarge%2520vision-language%2520models%2520%2528LVLMs%2529%2520are%2520powerful%252C%2520yet%2520they%2520remain%2520unreliable%2520due%2520to%2520object%2520hallucinations.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520in%2520many%2520hallucinatory%2520predictions%2520the%2520LVLM%2520effectively%2520ignores%2520the%2520image%2520and%2520instead%2520relies%2520on%2520previously%2520generated%2520output%2520%2528prelim%2529%2520tokens%2520to%2520infer%2520new%2520objects.%2520We%2520quantify%2520this%2520behavior%2520via%2520the%2520mutual%2520information%2520between%2520the%2520image%2520and%2520the%2520predicted%2520object%2520conditioned%2520on%2520the%2520prelim%252C%2520demonstrating%2520that%2520weak%2520image%2520dependence%2520strongly%2520correlates%2520with%2520hallucination.%2520Building%2520on%2520this%2520finding%252C%2520we%2520introduce%2520the%2520Prelim%2520Attention%2520Score%2520%2528PAS%2529%252C%2520a%2520lightweight%252C%2520training-free%2520signal%2520computed%2520from%2520attention%2520weights%2520over%2520prelim%2520tokens.%2520PAS%2520requires%2520no%2520additional%2520forward%2520passes%2520and%2520can%2520be%2520computed%2520on%2520the%2520fly%2520during%2520inference.%2520Exploiting%2520this%2520previously%2520overlooked%2520signal%252C%2520PAS%2520achieves%2520state-of-the-art%2520object-hallucination%2520detection%2520across%2520multiple%2520models%2520and%2520datasets%252C%2520enabling%2520real-time%2520filtering%2520and%2520intervention.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAS%20%3A%20Prelim%20Attention%20Score%20for%20Detecting%20Object%20Hallucinations%20in%20Large%20Vision--Language%20Models&entry.906535625=Nhat%20Hoang-Xuan%20and%20Minh%20Vu%20and%20My%20T.%20Thai%20and%20Manish%20Bhattarai&entry.1292438233=Large%20vision-language%20models%20%28LVLMs%29%20are%20powerful%2C%20yet%20they%20remain%20unreliable%20due%20to%20object%20hallucinations.%20In%20this%20work%2C%20we%20show%20that%20in%20many%20hallucinatory%20predictions%20the%20LVLM%20effectively%20ignores%20the%20image%20and%20instead%20relies%20on%20previously%20generated%20output%20%28prelim%29%20tokens%20to%20infer%20new%20objects.%20We%20quantify%20this%20behavior%20via%20the%20mutual%20information%20between%20the%20image%20and%20the%20predicted%20object%20conditioned%20on%20the%20prelim%2C%20demonstrating%20that%20weak%20image%20dependence%20strongly%20correlates%20with%20hallucination.%20Building%20on%20this%20finding%2C%20we%20introduce%20the%20Prelim%20Attention%20Score%20%28PAS%29%2C%20a%20lightweight%2C%20training-free%20signal%20computed%20from%20attention%20weights%20over%20prelim%20tokens.%20PAS%20requires%20no%20additional%20forward%20passes%20and%20can%20be%20computed%20on%20the%20fly%20during%20inference.%20Exploiting%20this%20previously%20overlooked%20signal%2C%20PAS%20achieves%20state-of-the-art%20object-hallucination%20detection%20across%20multiple%20models%20and%20datasets%2C%20enabling%20real-time%20filtering%20and%20intervention.&entry.1838667208=http%3A//arxiv.org/abs/2511.11502v1&entry.124074799=Read"},
{"title": "CAMA: Enhancing Mathematical Reasoning in Large Language Models with Causal Knowledge", "author": "Lei Zan and Keli Zhang and Ruichu Cai and Lujia Pan", "abstract": "Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks, yet they still struggle with complex mathematical reasoning, a challenge fundamentally rooted in deep structural dependencies. To address this challenge, we propose \\textbf{CA}usal \\textbf{MA}thematician (\\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit, reusable mathematical structure. In the learning stage, CAMA first constructs the \\textbf{M}athematical \\textbf{C}ausal \\textbf{G}raph (\\textbf{MCG}), a high-level representation of solution strategies, by combining LLM priors with causal discovery algorithms applied to a corpus of question-solution pairs. The resulting MCG encodes essential knowledge points and their causal dependencies. To better align the graph with downstream reasoning tasks, CAMA further refines the MCG through iterative feedback derived from a selected subset of the question-solution pairs. In the reasoning stage, given a new question, CAMA dynamically extracts a task-relevant subgraph from the MCG, conditioned on both the question content and the LLM's intermediate reasoning trace. This subgraph, which encodes the most pertinent knowledge points and their causal dependencies, is then injected back into the LLM to guide its reasoning process. Empirical results on real-world datasets show that CAMA significantly improves LLM performance on challenging mathematical problems. Furthermore, our experiments demonstrate that structured guidance consistently outperforms unstructured alternatives, and that incorporating asymmetric causal relationships yields greater improvements than using symmetric associations alone.", "link": "http://arxiv.org/abs/2508.02583v3", "date": "2025-11-14", "relevancy": 2.4482, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5179}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4755}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAMA%3A%20Enhancing%20Mathematical%20Reasoning%20in%20Large%20Language%20Models%20with%20Causal%20Knowledge&body=Title%3A%20CAMA%3A%20Enhancing%20Mathematical%20Reasoning%20in%20Large%20Language%20Models%20with%20Causal%20Knowledge%0AAuthor%3A%20Lei%20Zan%20and%20Keli%20Zhang%20and%20Ruichu%20Cai%20and%20Lujia%20Pan%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20strong%20performance%20across%20a%20wide%20range%20of%20tasks%2C%20yet%20they%20still%20struggle%20with%20complex%20mathematical%20reasoning%2C%20a%20challenge%20fundamentally%20rooted%20in%20deep%20structural%20dependencies.%20To%20address%20this%20challenge%2C%20we%20propose%20%5Ctextbf%7BCA%7Dusal%20%5Ctextbf%7BMA%7Dthematician%20%28%5Ctextbf%7BCAMA%7D%29%2C%20a%20two-stage%20causal%20framework%20that%20equips%20LLMs%20with%20explicit%2C%20reusable%20mathematical%20structure.%20In%20the%20learning%20stage%2C%20CAMA%20first%20constructs%20the%20%5Ctextbf%7BM%7Dathematical%20%5Ctextbf%7BC%7Dausal%20%5Ctextbf%7BG%7Draph%20%28%5Ctextbf%7BMCG%7D%29%2C%20a%20high-level%20representation%20of%20solution%20strategies%2C%20by%20combining%20LLM%20priors%20with%20causal%20discovery%20algorithms%20applied%20to%20a%20corpus%20of%20question-solution%20pairs.%20The%20resulting%20MCG%20encodes%20essential%20knowledge%20points%20and%20their%20causal%20dependencies.%20To%20better%20align%20the%20graph%20with%20downstream%20reasoning%20tasks%2C%20CAMA%20further%20refines%20the%20MCG%20through%20iterative%20feedback%20derived%20from%20a%20selected%20subset%20of%20the%20question-solution%20pairs.%20In%20the%20reasoning%20stage%2C%20given%20a%20new%20question%2C%20CAMA%20dynamically%20extracts%20a%20task-relevant%20subgraph%20from%20the%20MCG%2C%20conditioned%20on%20both%20the%20question%20content%20and%20the%20LLM%27s%20intermediate%20reasoning%20trace.%20This%20subgraph%2C%20which%20encodes%20the%20most%20pertinent%20knowledge%20points%20and%20their%20causal%20dependencies%2C%20is%20then%20injected%20back%20into%20the%20LLM%20to%20guide%20its%20reasoning%20process.%20Empirical%20results%20on%20real-world%20datasets%20show%20that%20CAMA%20significantly%20improves%20LLM%20performance%20on%20challenging%20mathematical%20problems.%20Furthermore%2C%20our%20experiments%20demonstrate%20that%20structured%20guidance%20consistently%20outperforms%20unstructured%20alternatives%2C%20and%20that%20incorporating%20asymmetric%20causal%20relationships%20yields%20greater%20improvements%20than%20using%20symmetric%20associations%20alone.%0ALink%3A%20http%3A//arxiv.org/abs/2508.02583v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAMA%253A%2520Enhancing%2520Mathematical%2520Reasoning%2520in%2520Large%2520Language%2520Models%2520with%2520Causal%2520Knowledge%26entry.906535625%3DLei%2520Zan%2520and%2520Keli%2520Zhang%2520and%2520Ruichu%2520Cai%2520and%2520Lujia%2520Pan%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520strong%2520performance%2520across%2520a%2520wide%2520range%2520of%2520tasks%252C%2520yet%2520they%2520still%2520struggle%2520with%2520complex%2520mathematical%2520reasoning%252C%2520a%2520challenge%2520fundamentally%2520rooted%2520in%2520deep%2520structural%2520dependencies.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520%255Ctextbf%257BCA%257Dusal%2520%255Ctextbf%257BMA%257Dthematician%2520%2528%255Ctextbf%257BCAMA%257D%2529%252C%2520a%2520two-stage%2520causal%2520framework%2520that%2520equips%2520LLMs%2520with%2520explicit%252C%2520reusable%2520mathematical%2520structure.%2520In%2520the%2520learning%2520stage%252C%2520CAMA%2520first%2520constructs%2520the%2520%255Ctextbf%257BM%257Dathematical%2520%255Ctextbf%257BC%257Dausal%2520%255Ctextbf%257BG%257Draph%2520%2528%255Ctextbf%257BMCG%257D%2529%252C%2520a%2520high-level%2520representation%2520of%2520solution%2520strategies%252C%2520by%2520combining%2520LLM%2520priors%2520with%2520causal%2520discovery%2520algorithms%2520applied%2520to%2520a%2520corpus%2520of%2520question-solution%2520pairs.%2520The%2520resulting%2520MCG%2520encodes%2520essential%2520knowledge%2520points%2520and%2520their%2520causal%2520dependencies.%2520To%2520better%2520align%2520the%2520graph%2520with%2520downstream%2520reasoning%2520tasks%252C%2520CAMA%2520further%2520refines%2520the%2520MCG%2520through%2520iterative%2520feedback%2520derived%2520from%2520a%2520selected%2520subset%2520of%2520the%2520question-solution%2520pairs.%2520In%2520the%2520reasoning%2520stage%252C%2520given%2520a%2520new%2520question%252C%2520CAMA%2520dynamically%2520extracts%2520a%2520task-relevant%2520subgraph%2520from%2520the%2520MCG%252C%2520conditioned%2520on%2520both%2520the%2520question%2520content%2520and%2520the%2520LLM%2527s%2520intermediate%2520reasoning%2520trace.%2520This%2520subgraph%252C%2520which%2520encodes%2520the%2520most%2520pertinent%2520knowledge%2520points%2520and%2520their%2520causal%2520dependencies%252C%2520is%2520then%2520injected%2520back%2520into%2520the%2520LLM%2520to%2520guide%2520its%2520reasoning%2520process.%2520Empirical%2520results%2520on%2520real-world%2520datasets%2520show%2520that%2520CAMA%2520significantly%2520improves%2520LLM%2520performance%2520on%2520challenging%2520mathematical%2520problems.%2520Furthermore%252C%2520our%2520experiments%2520demonstrate%2520that%2520structured%2520guidance%2520consistently%2520outperforms%2520unstructured%2520alternatives%252C%2520and%2520that%2520incorporating%2520asymmetric%2520causal%2520relationships%2520yields%2520greater%2520improvements%2520than%2520using%2520symmetric%2520associations%2520alone.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02583v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAMA%3A%20Enhancing%20Mathematical%20Reasoning%20in%20Large%20Language%20Models%20with%20Causal%20Knowledge&entry.906535625=Lei%20Zan%20and%20Keli%20Zhang%20and%20Ruichu%20Cai%20and%20Lujia%20Pan&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20strong%20performance%20across%20a%20wide%20range%20of%20tasks%2C%20yet%20they%20still%20struggle%20with%20complex%20mathematical%20reasoning%2C%20a%20challenge%20fundamentally%20rooted%20in%20deep%20structural%20dependencies.%20To%20address%20this%20challenge%2C%20we%20propose%20%5Ctextbf%7BCA%7Dusal%20%5Ctextbf%7BMA%7Dthematician%20%28%5Ctextbf%7BCAMA%7D%29%2C%20a%20two-stage%20causal%20framework%20that%20equips%20LLMs%20with%20explicit%2C%20reusable%20mathematical%20structure.%20In%20the%20learning%20stage%2C%20CAMA%20first%20constructs%20the%20%5Ctextbf%7BM%7Dathematical%20%5Ctextbf%7BC%7Dausal%20%5Ctextbf%7BG%7Draph%20%28%5Ctextbf%7BMCG%7D%29%2C%20a%20high-level%20representation%20of%20solution%20strategies%2C%20by%20combining%20LLM%20priors%20with%20causal%20discovery%20algorithms%20applied%20to%20a%20corpus%20of%20question-solution%20pairs.%20The%20resulting%20MCG%20encodes%20essential%20knowledge%20points%20and%20their%20causal%20dependencies.%20To%20better%20align%20the%20graph%20with%20downstream%20reasoning%20tasks%2C%20CAMA%20further%20refines%20the%20MCG%20through%20iterative%20feedback%20derived%20from%20a%20selected%20subset%20of%20the%20question-solution%20pairs.%20In%20the%20reasoning%20stage%2C%20given%20a%20new%20question%2C%20CAMA%20dynamically%20extracts%20a%20task-relevant%20subgraph%20from%20the%20MCG%2C%20conditioned%20on%20both%20the%20question%20content%20and%20the%20LLM%27s%20intermediate%20reasoning%20trace.%20This%20subgraph%2C%20which%20encodes%20the%20most%20pertinent%20knowledge%20points%20and%20their%20causal%20dependencies%2C%20is%20then%20injected%20back%20into%20the%20LLM%20to%20guide%20its%20reasoning%20process.%20Empirical%20results%20on%20real-world%20datasets%20show%20that%20CAMA%20significantly%20improves%20LLM%20performance%20on%20challenging%20mathematical%20problems.%20Furthermore%2C%20our%20experiments%20demonstrate%20that%20structured%20guidance%20consistently%20outperforms%20unstructured%20alternatives%2C%20and%20that%20incorporating%20asymmetric%20causal%20relationships%20yields%20greater%20improvements%20than%20using%20symmetric%20associations%20alone.&entry.1838667208=http%3A//arxiv.org/abs/2508.02583v3&entry.124074799=Read"},
{"title": "RTGaze: Real-Time 3D-Aware Gaze Redirection from a Single Image", "author": "Hengfei Wang and Zhongqun Zhang and Yihua Cheng and Hyung Jin Chang", "abstract": "Gaze redirection methods aim to generate realistic human face images with controllable eye movement. However, recent methods often struggle with 3D consistency, efficiency, or quality, limiting their practical applications. In this work, we propose RTGaze, a real-time and high-quality gaze redirection method. Our approach learns a gaze-controllable facial representation from face images and gaze prompts, then decodes this representation via neural rendering for gaze redirection. Additionally, we distill face geometric priors from a pretrained 3D portrait generator to enhance generation quality. We evaluate RTGaze both qualitatively and quantitatively, demonstrating state-of-the-art performance in efficiency, redirection accuracy, and image quality across multiple datasets. Our system achieves real-time, 3D-aware gaze redirection with a feedforward network (~0.06 sec/image), making it 800x faster than the previous state-of-the-art 3D-aware methods.", "link": "http://arxiv.org/abs/2511.11289v1", "date": "2025-11-14", "relevancy": 2.4447, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6459}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5933}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RTGaze%3A%20Real-Time%203D-Aware%20Gaze%20Redirection%20from%20a%20Single%20Image&body=Title%3A%20RTGaze%3A%20Real-Time%203D-Aware%20Gaze%20Redirection%20from%20a%20Single%20Image%0AAuthor%3A%20Hengfei%20Wang%20and%20Zhongqun%20Zhang%20and%20Yihua%20Cheng%20and%20Hyung%20Jin%20Chang%0AAbstract%3A%20Gaze%20redirection%20methods%20aim%20to%20generate%20realistic%20human%20face%20images%20with%20controllable%20eye%20movement.%20However%2C%20recent%20methods%20often%20struggle%20with%203D%20consistency%2C%20efficiency%2C%20or%20quality%2C%20limiting%20their%20practical%20applications.%20In%20this%20work%2C%20we%20propose%20RTGaze%2C%20a%20real-time%20and%20high-quality%20gaze%20redirection%20method.%20Our%20approach%20learns%20a%20gaze-controllable%20facial%20representation%20from%20face%20images%20and%20gaze%20prompts%2C%20then%20decodes%20this%20representation%20via%20neural%20rendering%20for%20gaze%20redirection.%20Additionally%2C%20we%20distill%20face%20geometric%20priors%20from%20a%20pretrained%203D%20portrait%20generator%20to%20enhance%20generation%20quality.%20We%20evaluate%20RTGaze%20both%20qualitatively%20and%20quantitatively%2C%20demonstrating%20state-of-the-art%20performance%20in%20efficiency%2C%20redirection%20accuracy%2C%20and%20image%20quality%20across%20multiple%20datasets.%20Our%20system%20achieves%20real-time%2C%203D-aware%20gaze%20redirection%20with%20a%20feedforward%20network%20%28~0.06%20sec/image%29%2C%20making%20it%20800x%20faster%20than%20the%20previous%20state-of-the-art%203D-aware%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11289v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRTGaze%253A%2520Real-Time%25203D-Aware%2520Gaze%2520Redirection%2520from%2520a%2520Single%2520Image%26entry.906535625%3DHengfei%2520Wang%2520and%2520Zhongqun%2520Zhang%2520and%2520Yihua%2520Cheng%2520and%2520Hyung%2520Jin%2520Chang%26entry.1292438233%3DGaze%2520redirection%2520methods%2520aim%2520to%2520generate%2520realistic%2520human%2520face%2520images%2520with%2520controllable%2520eye%2520movement.%2520However%252C%2520recent%2520methods%2520often%2520struggle%2520with%25203D%2520consistency%252C%2520efficiency%252C%2520or%2520quality%252C%2520limiting%2520their%2520practical%2520applications.%2520In%2520this%2520work%252C%2520we%2520propose%2520RTGaze%252C%2520a%2520real-time%2520and%2520high-quality%2520gaze%2520redirection%2520method.%2520Our%2520approach%2520learns%2520a%2520gaze-controllable%2520facial%2520representation%2520from%2520face%2520images%2520and%2520gaze%2520prompts%252C%2520then%2520decodes%2520this%2520representation%2520via%2520neural%2520rendering%2520for%2520gaze%2520redirection.%2520Additionally%252C%2520we%2520distill%2520face%2520geometric%2520priors%2520from%2520a%2520pretrained%25203D%2520portrait%2520generator%2520to%2520enhance%2520generation%2520quality.%2520We%2520evaluate%2520RTGaze%2520both%2520qualitatively%2520and%2520quantitatively%252C%2520demonstrating%2520state-of-the-art%2520performance%2520in%2520efficiency%252C%2520redirection%2520accuracy%252C%2520and%2520image%2520quality%2520across%2520multiple%2520datasets.%2520Our%2520system%2520achieves%2520real-time%252C%25203D-aware%2520gaze%2520redirection%2520with%2520a%2520feedforward%2520network%2520%2528~0.06%2520sec/image%2529%252C%2520making%2520it%2520800x%2520faster%2520than%2520the%2520previous%2520state-of-the-art%25203D-aware%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11289v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RTGaze%3A%20Real-Time%203D-Aware%20Gaze%20Redirection%20from%20a%20Single%20Image&entry.906535625=Hengfei%20Wang%20and%20Zhongqun%20Zhang%20and%20Yihua%20Cheng%20and%20Hyung%20Jin%20Chang&entry.1292438233=Gaze%20redirection%20methods%20aim%20to%20generate%20realistic%20human%20face%20images%20with%20controllable%20eye%20movement.%20However%2C%20recent%20methods%20often%20struggle%20with%203D%20consistency%2C%20efficiency%2C%20or%20quality%2C%20limiting%20their%20practical%20applications.%20In%20this%20work%2C%20we%20propose%20RTGaze%2C%20a%20real-time%20and%20high-quality%20gaze%20redirection%20method.%20Our%20approach%20learns%20a%20gaze-controllable%20facial%20representation%20from%20face%20images%20and%20gaze%20prompts%2C%20then%20decodes%20this%20representation%20via%20neural%20rendering%20for%20gaze%20redirection.%20Additionally%2C%20we%20distill%20face%20geometric%20priors%20from%20a%20pretrained%203D%20portrait%20generator%20to%20enhance%20generation%20quality.%20We%20evaluate%20RTGaze%20both%20qualitatively%20and%20quantitatively%2C%20demonstrating%20state-of-the-art%20performance%20in%20efficiency%2C%20redirection%20accuracy%2C%20and%20image%20quality%20across%20multiple%20datasets.%20Our%20system%20achieves%20real-time%2C%203D-aware%20gaze%20redirection%20with%20a%20feedforward%20network%20%28~0.06%20sec/image%29%2C%20making%20it%20800x%20faster%20than%20the%20previous%20state-of-the-art%203D-aware%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2511.11289v1&entry.124074799=Read"},
{"title": "Sparse Methods for Vector Embeddings of TPC Data", "author": "Tyler Wheeler and Michelle P. Kuchera and Raghuram Ramanujan and Ryan Krupp and Chris Wrede and Saiprasad Ravishankar and Connor L. Cross and Hoi Yan Ian Heung and Andrew J. Jones and Benjamin Votaw", "abstract": "Time Projection Chambers (TPCs) are versatile detectors that reconstruct charged-particle tracks in an ionizing medium, enabling sensitive measurements across a wide range of nuclear physics experiments. We explore sparse convolutional networks for representation learning on TPC data, finding that a sparse ResNet architecture, even with randomly set weights, provides useful structured vector embeddings of events. Pre-training this architecture on a simple physics-motivated binary classification task further improves the embedding quality. Using data from the GAseous Detector with GErmanium Tagging (GADGET) II TPC, a detector optimized for measuring low-energy $\u03b2$-delayed particle decays, we represent raw pad-level signals as sparse tensors, train Minkowski Engine ResNet models, and probe the resulting event-level embeddings which reveal rich event structure. As a cross-detector test, we embed data from the Active-Target TPC (AT-TPC) -- a detector designed for nuclear reaction studies in inverse kinematics -- using the same encoder. We find that even an untrained sparse ResNet model provides useful embeddings of AT-TPC data, and we observe improvements when the model is trained on GADGET data. Together, these results highlight the potential of sparse convolutional techniques as a general tool for representation learning in diverse TPC experiments.", "link": "http://arxiv.org/abs/2511.11221v1", "date": "2025-11-14", "relevancy": 2.4439, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4906}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4896}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Methods%20for%20Vector%20Embeddings%20of%20TPC%20Data&body=Title%3A%20Sparse%20Methods%20for%20Vector%20Embeddings%20of%20TPC%20Data%0AAuthor%3A%20Tyler%20Wheeler%20and%20Michelle%20P.%20Kuchera%20and%20Raghuram%20Ramanujan%20and%20Ryan%20Krupp%20and%20Chris%20Wrede%20and%20Saiprasad%20Ravishankar%20and%20Connor%20L.%20Cross%20and%20Hoi%20Yan%20Ian%20Heung%20and%20Andrew%20J.%20Jones%20and%20Benjamin%20Votaw%0AAbstract%3A%20Time%20Projection%20Chambers%20%28TPCs%29%20are%20versatile%20detectors%20that%20reconstruct%20charged-particle%20tracks%20in%20an%20ionizing%20medium%2C%20enabling%20sensitive%20measurements%20across%20a%20wide%20range%20of%20nuclear%20physics%20experiments.%20We%20explore%20sparse%20convolutional%20networks%20for%20representation%20learning%20on%20TPC%20data%2C%20finding%20that%20a%20sparse%20ResNet%20architecture%2C%20even%20with%20randomly%20set%20weights%2C%20provides%20useful%20structured%20vector%20embeddings%20of%20events.%20Pre-training%20this%20architecture%20on%20a%20simple%20physics-motivated%20binary%20classification%20task%20further%20improves%20the%20embedding%20quality.%20Using%20data%20from%20the%20GAseous%20Detector%20with%20GErmanium%20Tagging%20%28GADGET%29%20II%20TPC%2C%20a%20detector%20optimized%20for%20measuring%20low-energy%20%24%CE%B2%24-delayed%20particle%20decays%2C%20we%20represent%20raw%20pad-level%20signals%20as%20sparse%20tensors%2C%20train%20Minkowski%20Engine%20ResNet%20models%2C%20and%20probe%20the%20resulting%20event-level%20embeddings%20which%20reveal%20rich%20event%20structure.%20As%20a%20cross-detector%20test%2C%20we%20embed%20data%20from%20the%20Active-Target%20TPC%20%28AT-TPC%29%20--%20a%20detector%20designed%20for%20nuclear%20reaction%20studies%20in%20inverse%20kinematics%20--%20using%20the%20same%20encoder.%20We%20find%20that%20even%20an%20untrained%20sparse%20ResNet%20model%20provides%20useful%20embeddings%20of%20AT-TPC%20data%2C%20and%20we%20observe%20improvements%20when%20the%20model%20is%20trained%20on%20GADGET%20data.%20Together%2C%20these%20results%20highlight%20the%20potential%20of%20sparse%20convolutional%20techniques%20as%20a%20general%20tool%20for%20representation%20learning%20in%20diverse%20TPC%20experiments.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Methods%2520for%2520Vector%2520Embeddings%2520of%2520TPC%2520Data%26entry.906535625%3DTyler%2520Wheeler%2520and%2520Michelle%2520P.%2520Kuchera%2520and%2520Raghuram%2520Ramanujan%2520and%2520Ryan%2520Krupp%2520and%2520Chris%2520Wrede%2520and%2520Saiprasad%2520Ravishankar%2520and%2520Connor%2520L.%2520Cross%2520and%2520Hoi%2520Yan%2520Ian%2520Heung%2520and%2520Andrew%2520J.%2520Jones%2520and%2520Benjamin%2520Votaw%26entry.1292438233%3DTime%2520Projection%2520Chambers%2520%2528TPCs%2529%2520are%2520versatile%2520detectors%2520that%2520reconstruct%2520charged-particle%2520tracks%2520in%2520an%2520ionizing%2520medium%252C%2520enabling%2520sensitive%2520measurements%2520across%2520a%2520wide%2520range%2520of%2520nuclear%2520physics%2520experiments.%2520We%2520explore%2520sparse%2520convolutional%2520networks%2520for%2520representation%2520learning%2520on%2520TPC%2520data%252C%2520finding%2520that%2520a%2520sparse%2520ResNet%2520architecture%252C%2520even%2520with%2520randomly%2520set%2520weights%252C%2520provides%2520useful%2520structured%2520vector%2520embeddings%2520of%2520events.%2520Pre-training%2520this%2520architecture%2520on%2520a%2520simple%2520physics-motivated%2520binary%2520classification%2520task%2520further%2520improves%2520the%2520embedding%2520quality.%2520Using%2520data%2520from%2520the%2520GAseous%2520Detector%2520with%2520GErmanium%2520Tagging%2520%2528GADGET%2529%2520II%2520TPC%252C%2520a%2520detector%2520optimized%2520for%2520measuring%2520low-energy%2520%2524%25CE%25B2%2524-delayed%2520particle%2520decays%252C%2520we%2520represent%2520raw%2520pad-level%2520signals%2520as%2520sparse%2520tensors%252C%2520train%2520Minkowski%2520Engine%2520ResNet%2520models%252C%2520and%2520probe%2520the%2520resulting%2520event-level%2520embeddings%2520which%2520reveal%2520rich%2520event%2520structure.%2520As%2520a%2520cross-detector%2520test%252C%2520we%2520embed%2520data%2520from%2520the%2520Active-Target%2520TPC%2520%2528AT-TPC%2529%2520--%2520a%2520detector%2520designed%2520for%2520nuclear%2520reaction%2520studies%2520in%2520inverse%2520kinematics%2520--%2520using%2520the%2520same%2520encoder.%2520We%2520find%2520that%2520even%2520an%2520untrained%2520sparse%2520ResNet%2520model%2520provides%2520useful%2520embeddings%2520of%2520AT-TPC%2520data%252C%2520and%2520we%2520observe%2520improvements%2520when%2520the%2520model%2520is%2520trained%2520on%2520GADGET%2520data.%2520Together%252C%2520these%2520results%2520highlight%2520the%2520potential%2520of%2520sparse%2520convolutional%2520techniques%2520as%2520a%2520general%2520tool%2520for%2520representation%2520learning%2520in%2520diverse%2520TPC%2520experiments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Methods%20for%20Vector%20Embeddings%20of%20TPC%20Data&entry.906535625=Tyler%20Wheeler%20and%20Michelle%20P.%20Kuchera%20and%20Raghuram%20Ramanujan%20and%20Ryan%20Krupp%20and%20Chris%20Wrede%20and%20Saiprasad%20Ravishankar%20and%20Connor%20L.%20Cross%20and%20Hoi%20Yan%20Ian%20Heung%20and%20Andrew%20J.%20Jones%20and%20Benjamin%20Votaw&entry.1292438233=Time%20Projection%20Chambers%20%28TPCs%29%20are%20versatile%20detectors%20that%20reconstruct%20charged-particle%20tracks%20in%20an%20ionizing%20medium%2C%20enabling%20sensitive%20measurements%20across%20a%20wide%20range%20of%20nuclear%20physics%20experiments.%20We%20explore%20sparse%20convolutional%20networks%20for%20representation%20learning%20on%20TPC%20data%2C%20finding%20that%20a%20sparse%20ResNet%20architecture%2C%20even%20with%20randomly%20set%20weights%2C%20provides%20useful%20structured%20vector%20embeddings%20of%20events.%20Pre-training%20this%20architecture%20on%20a%20simple%20physics-motivated%20binary%20classification%20task%20further%20improves%20the%20embedding%20quality.%20Using%20data%20from%20the%20GAseous%20Detector%20with%20GErmanium%20Tagging%20%28GADGET%29%20II%20TPC%2C%20a%20detector%20optimized%20for%20measuring%20low-energy%20%24%CE%B2%24-delayed%20particle%20decays%2C%20we%20represent%20raw%20pad-level%20signals%20as%20sparse%20tensors%2C%20train%20Minkowski%20Engine%20ResNet%20models%2C%20and%20probe%20the%20resulting%20event-level%20embeddings%20which%20reveal%20rich%20event%20structure.%20As%20a%20cross-detector%20test%2C%20we%20embed%20data%20from%20the%20Active-Target%20TPC%20%28AT-TPC%29%20--%20a%20detector%20designed%20for%20nuclear%20reaction%20studies%20in%20inverse%20kinematics%20--%20using%20the%20same%20encoder.%20We%20find%20that%20even%20an%20untrained%20sparse%20ResNet%20model%20provides%20useful%20embeddings%20of%20AT-TPC%20data%2C%20and%20we%20observe%20improvements%20when%20the%20model%20is%20trained%20on%20GADGET%20data.%20Together%2C%20these%20results%20highlight%20the%20potential%20of%20sparse%20convolutional%20techniques%20as%20a%20general%20tool%20for%20representation%20learning%20in%20diverse%20TPC%20experiments.&entry.1838667208=http%3A//arxiv.org/abs/2511.11221v1&entry.124074799=Read"},
{"title": "A Global Geometric Analysis of Maximal Coding Rate Reduction", "author": "Peng Wang and Huikang Liu and Druv Pai and Yaodong Yu and Zhihui Zhu and Qing Qu and Yi Ma", "abstract": "The maximal coding rate reduction (MCR$^2$) objective for learning structured and compact deep representations is drawing increasing attention, especially after its recent usage in the derivation of fully explainable and highly effective deep network architectures. However, it lacks a complete theoretical justification: only the properties of its global optima are known, and its global landscape has not been studied. In this work, we give a complete characterization of the properties of all its local and global optima, as well as other types of critical points. Specifically, we show that each (local or global) maximizer of the MCR$^2$ problem corresponds to a low-dimensional, discriminative, and diverse representation, and furthermore, each critical point of the objective is either a local maximizer or a strict saddle point. Such a favorable landscape makes MCR$^2$ a natural choice of objective for learning diverse and discriminative representations via first-order optimization methods. To validate our theoretical findings, we conduct extensive experiments on both synthetic and real data sets.", "link": "http://arxiv.org/abs/2406.01909v2", "date": "2025-11-14", "relevancy": 2.4437, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4898}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4887}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Global%20Geometric%20Analysis%20of%20Maximal%20Coding%20Rate%20Reduction&body=Title%3A%20A%20Global%20Geometric%20Analysis%20of%20Maximal%20Coding%20Rate%20Reduction%0AAuthor%3A%20Peng%20Wang%20and%20Huikang%20Liu%20and%20Druv%20Pai%20and%20Yaodong%20Yu%20and%20Zhihui%20Zhu%20and%20Qing%20Qu%20and%20Yi%20Ma%0AAbstract%3A%20The%20maximal%20coding%20rate%20reduction%20%28MCR%24%5E2%24%29%20objective%20for%20learning%20structured%20and%20compact%20deep%20representations%20is%20drawing%20increasing%20attention%2C%20especially%20after%20its%20recent%20usage%20in%20the%20derivation%20of%20fully%20explainable%20and%20highly%20effective%20deep%20network%20architectures.%20However%2C%20it%20lacks%20a%20complete%20theoretical%20justification%3A%20only%20the%20properties%20of%20its%20global%20optima%20are%20known%2C%20and%20its%20global%20landscape%20has%20not%20been%20studied.%20In%20this%20work%2C%20we%20give%20a%20complete%20characterization%20of%20the%20properties%20of%20all%20its%20local%20and%20global%20optima%2C%20as%20well%20as%20other%20types%20of%20critical%20points.%20Specifically%2C%20we%20show%20that%20each%20%28local%20or%20global%29%20maximizer%20of%20the%20MCR%24%5E2%24%20problem%20corresponds%20to%20a%20low-dimensional%2C%20discriminative%2C%20and%20diverse%20representation%2C%20and%20furthermore%2C%20each%20critical%20point%20of%20the%20objective%20is%20either%20a%20local%20maximizer%20or%20a%20strict%20saddle%20point.%20Such%20a%20favorable%20landscape%20makes%20MCR%24%5E2%24%20a%20natural%20choice%20of%20objective%20for%20learning%20diverse%20and%20discriminative%20representations%20via%20first-order%20optimization%20methods.%20To%20validate%20our%20theoretical%20findings%2C%20we%20conduct%20extensive%20experiments%20on%20both%20synthetic%20and%20real%20data%20sets.%0ALink%3A%20http%3A//arxiv.org/abs/2406.01909v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Global%2520Geometric%2520Analysis%2520of%2520Maximal%2520Coding%2520Rate%2520Reduction%26entry.906535625%3DPeng%2520Wang%2520and%2520Huikang%2520Liu%2520and%2520Druv%2520Pai%2520and%2520Yaodong%2520Yu%2520and%2520Zhihui%2520Zhu%2520and%2520Qing%2520Qu%2520and%2520Yi%2520Ma%26entry.1292438233%3DThe%2520maximal%2520coding%2520rate%2520reduction%2520%2528MCR%2524%255E2%2524%2529%2520objective%2520for%2520learning%2520structured%2520and%2520compact%2520deep%2520representations%2520is%2520drawing%2520increasing%2520attention%252C%2520especially%2520after%2520its%2520recent%2520usage%2520in%2520the%2520derivation%2520of%2520fully%2520explainable%2520and%2520highly%2520effective%2520deep%2520network%2520architectures.%2520However%252C%2520it%2520lacks%2520a%2520complete%2520theoretical%2520justification%253A%2520only%2520the%2520properties%2520of%2520its%2520global%2520optima%2520are%2520known%252C%2520and%2520its%2520global%2520landscape%2520has%2520not%2520been%2520studied.%2520In%2520this%2520work%252C%2520we%2520give%2520a%2520complete%2520characterization%2520of%2520the%2520properties%2520of%2520all%2520its%2520local%2520and%2520global%2520optima%252C%2520as%2520well%2520as%2520other%2520types%2520of%2520critical%2520points.%2520Specifically%252C%2520we%2520show%2520that%2520each%2520%2528local%2520or%2520global%2529%2520maximizer%2520of%2520the%2520MCR%2524%255E2%2524%2520problem%2520corresponds%2520to%2520a%2520low-dimensional%252C%2520discriminative%252C%2520and%2520diverse%2520representation%252C%2520and%2520furthermore%252C%2520each%2520critical%2520point%2520of%2520the%2520objective%2520is%2520either%2520a%2520local%2520maximizer%2520or%2520a%2520strict%2520saddle%2520point.%2520Such%2520a%2520favorable%2520landscape%2520makes%2520MCR%2524%255E2%2524%2520a%2520natural%2520choice%2520of%2520objective%2520for%2520learning%2520diverse%2520and%2520discriminative%2520representations%2520via%2520first-order%2520optimization%2520methods.%2520To%2520validate%2520our%2520theoretical%2520findings%252C%2520we%2520conduct%2520extensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520real%2520data%2520sets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01909v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Global%20Geometric%20Analysis%20of%20Maximal%20Coding%20Rate%20Reduction&entry.906535625=Peng%20Wang%20and%20Huikang%20Liu%20and%20Druv%20Pai%20and%20Yaodong%20Yu%20and%20Zhihui%20Zhu%20and%20Qing%20Qu%20and%20Yi%20Ma&entry.1292438233=The%20maximal%20coding%20rate%20reduction%20%28MCR%24%5E2%24%29%20objective%20for%20learning%20structured%20and%20compact%20deep%20representations%20is%20drawing%20increasing%20attention%2C%20especially%20after%20its%20recent%20usage%20in%20the%20derivation%20of%20fully%20explainable%20and%20highly%20effective%20deep%20network%20architectures.%20However%2C%20it%20lacks%20a%20complete%20theoretical%20justification%3A%20only%20the%20properties%20of%20its%20global%20optima%20are%20known%2C%20and%20its%20global%20landscape%20has%20not%20been%20studied.%20In%20this%20work%2C%20we%20give%20a%20complete%20characterization%20of%20the%20properties%20of%20all%20its%20local%20and%20global%20optima%2C%20as%20well%20as%20other%20types%20of%20critical%20points.%20Specifically%2C%20we%20show%20that%20each%20%28local%20or%20global%29%20maximizer%20of%20the%20MCR%24%5E2%24%20problem%20corresponds%20to%20a%20low-dimensional%2C%20discriminative%2C%20and%20diverse%20representation%2C%20and%20furthermore%2C%20each%20critical%20point%20of%20the%20objective%20is%20either%20a%20local%20maximizer%20or%20a%20strict%20saddle%20point.%20Such%20a%20favorable%20landscape%20makes%20MCR%24%5E2%24%20a%20natural%20choice%20of%20objective%20for%20learning%20diverse%20and%20discriminative%20representations%20via%20first-order%20optimization%20methods.%20To%20validate%20our%20theoretical%20findings%2C%20we%20conduct%20extensive%20experiments%20on%20both%20synthetic%20and%20real%20data%20sets.&entry.1838667208=http%3A//arxiv.org/abs/2406.01909v2&entry.124074799=Read"},
{"title": "FlashI2V: Fourier-Guided Latent Shifting Prevents Conditional Image Leakage in Image-to-Video Generation", "author": "Yunyang Ge and Xinhua Cheng and Chengshu Zhao and Xianyi He and Shenghai Yuan and Bin Lin and Bin Zhu and Li Yuan", "abstract": "In Image-to-Video (I2V) generation, a video is created using an input image as the first-frame condition. Existing I2V methods concatenate the full information of the conditional image with noisy latents to achieve high fidelity. However, the denoisers in these methods tend to shortcut the conditional image, which is known as conditional image leakage, leading to performance degradation issues such as slow motion and color inconsistency. In this work, we further clarify that conditional image leakage leads to overfitting to in-domain data and decreases the performance in out-of-domain scenarios. Moreover, we introduce Fourier-Guided Latent Shifting I2V, named FlashI2V, to prevent conditional image leakage. Concretely, FlashI2V consists of: (1) Latent Shifting. We modify the source and target distributions of flow matching by subtracting the conditional image information from the noisy latents, thereby incorporating the condition implicitly. (2) Fourier Guidance. We use high-frequency magnitude features obtained by the Fourier Transform to accelerate convergence and enable the adjustment of detail levels in the generated video. Experimental results show that our method effectively overcomes conditional image leakage and achieves the best generalization and performance on out-of-domain data among various I2V paradigms. With only 1.3B parameters, FlashI2V achieves a dynamic degree score of 53.01 on Vbench-I2V, surpassing CogVideoX1.5-5B-I2V and Wan2.1-I2V-14B-480P. Project page: https://pku-yuangroup.github.io/FlashI2V/", "link": "http://arxiv.org/abs/2509.25187v2", "date": "2025-11-14", "relevancy": 2.4359, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6211}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6035}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlashI2V%3A%20Fourier-Guided%20Latent%20Shifting%20Prevents%20Conditional%20Image%20Leakage%20in%20Image-to-Video%20Generation&body=Title%3A%20FlashI2V%3A%20Fourier-Guided%20Latent%20Shifting%20Prevents%20Conditional%20Image%20Leakage%20in%20Image-to-Video%20Generation%0AAuthor%3A%20Yunyang%20Ge%20and%20Xinhua%20Cheng%20and%20Chengshu%20Zhao%20and%20Xianyi%20He%20and%20Shenghai%20Yuan%20and%20Bin%20Lin%20and%20Bin%20Zhu%20and%20Li%20Yuan%0AAbstract%3A%20In%20Image-to-Video%20%28I2V%29%20generation%2C%20a%20video%20is%20created%20using%20an%20input%20image%20as%20the%20first-frame%20condition.%20Existing%20I2V%20methods%20concatenate%20the%20full%20information%20of%20the%20conditional%20image%20with%20noisy%20latents%20to%20achieve%20high%20fidelity.%20However%2C%20the%20denoisers%20in%20these%20methods%20tend%20to%20shortcut%20the%20conditional%20image%2C%20which%20is%20known%20as%20conditional%20image%20leakage%2C%20leading%20to%20performance%20degradation%20issues%20such%20as%20slow%20motion%20and%20color%20inconsistency.%20In%20this%20work%2C%20we%20further%20clarify%20that%20conditional%20image%20leakage%20leads%20to%20overfitting%20to%20in-domain%20data%20and%20decreases%20the%20performance%20in%20out-of-domain%20scenarios.%20Moreover%2C%20we%20introduce%20Fourier-Guided%20Latent%20Shifting%20I2V%2C%20named%20FlashI2V%2C%20to%20prevent%20conditional%20image%20leakage.%20Concretely%2C%20FlashI2V%20consists%20of%3A%20%281%29%20Latent%20Shifting.%20We%20modify%20the%20source%20and%20target%20distributions%20of%20flow%20matching%20by%20subtracting%20the%20conditional%20image%20information%20from%20the%20noisy%20latents%2C%20thereby%20incorporating%20the%20condition%20implicitly.%20%282%29%20Fourier%20Guidance.%20We%20use%20high-frequency%20magnitude%20features%20obtained%20by%20the%20Fourier%20Transform%20to%20accelerate%20convergence%20and%20enable%20the%20adjustment%20of%20detail%20levels%20in%20the%20generated%20video.%20Experimental%20results%20show%20that%20our%20method%20effectively%20overcomes%20conditional%20image%20leakage%20and%20achieves%20the%20best%20generalization%20and%20performance%20on%20out-of-domain%20data%20among%20various%20I2V%20paradigms.%20With%20only%201.3B%20parameters%2C%20FlashI2V%20achieves%20a%20dynamic%20degree%20score%20of%2053.01%20on%20Vbench-I2V%2C%20surpassing%20CogVideoX1.5-5B-I2V%20and%20Wan2.1-I2V-14B-480P.%20Project%20page%3A%20https%3A//pku-yuangroup.github.io/FlashI2V/%0ALink%3A%20http%3A//arxiv.org/abs/2509.25187v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlashI2V%253A%2520Fourier-Guided%2520Latent%2520Shifting%2520Prevents%2520Conditional%2520Image%2520Leakage%2520in%2520Image-to-Video%2520Generation%26entry.906535625%3DYunyang%2520Ge%2520and%2520Xinhua%2520Cheng%2520and%2520Chengshu%2520Zhao%2520and%2520Xianyi%2520He%2520and%2520Shenghai%2520Yuan%2520and%2520Bin%2520Lin%2520and%2520Bin%2520Zhu%2520and%2520Li%2520Yuan%26entry.1292438233%3DIn%2520Image-to-Video%2520%2528I2V%2529%2520generation%252C%2520a%2520video%2520is%2520created%2520using%2520an%2520input%2520image%2520as%2520the%2520first-frame%2520condition.%2520Existing%2520I2V%2520methods%2520concatenate%2520the%2520full%2520information%2520of%2520the%2520conditional%2520image%2520with%2520noisy%2520latents%2520to%2520achieve%2520high%2520fidelity.%2520However%252C%2520the%2520denoisers%2520in%2520these%2520methods%2520tend%2520to%2520shortcut%2520the%2520conditional%2520image%252C%2520which%2520is%2520known%2520as%2520conditional%2520image%2520leakage%252C%2520leading%2520to%2520performance%2520degradation%2520issues%2520such%2520as%2520slow%2520motion%2520and%2520color%2520inconsistency.%2520In%2520this%2520work%252C%2520we%2520further%2520clarify%2520that%2520conditional%2520image%2520leakage%2520leads%2520to%2520overfitting%2520to%2520in-domain%2520data%2520and%2520decreases%2520the%2520performance%2520in%2520out-of-domain%2520scenarios.%2520Moreover%252C%2520we%2520introduce%2520Fourier-Guided%2520Latent%2520Shifting%2520I2V%252C%2520named%2520FlashI2V%252C%2520to%2520prevent%2520conditional%2520image%2520leakage.%2520Concretely%252C%2520FlashI2V%2520consists%2520of%253A%2520%25281%2529%2520Latent%2520Shifting.%2520We%2520modify%2520the%2520source%2520and%2520target%2520distributions%2520of%2520flow%2520matching%2520by%2520subtracting%2520the%2520conditional%2520image%2520information%2520from%2520the%2520noisy%2520latents%252C%2520thereby%2520incorporating%2520the%2520condition%2520implicitly.%2520%25282%2529%2520Fourier%2520Guidance.%2520We%2520use%2520high-frequency%2520magnitude%2520features%2520obtained%2520by%2520the%2520Fourier%2520Transform%2520to%2520accelerate%2520convergence%2520and%2520enable%2520the%2520adjustment%2520of%2520detail%2520levels%2520in%2520the%2520generated%2520video.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520effectively%2520overcomes%2520conditional%2520image%2520leakage%2520and%2520achieves%2520the%2520best%2520generalization%2520and%2520performance%2520on%2520out-of-domain%2520data%2520among%2520various%2520I2V%2520paradigms.%2520With%2520only%25201.3B%2520parameters%252C%2520FlashI2V%2520achieves%2520a%2520dynamic%2520degree%2520score%2520of%252053.01%2520on%2520Vbench-I2V%252C%2520surpassing%2520CogVideoX1.5-5B-I2V%2520and%2520Wan2.1-I2V-14B-480P.%2520Project%2520page%253A%2520https%253A//pku-yuangroup.github.io/FlashI2V/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25187v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlashI2V%3A%20Fourier-Guided%20Latent%20Shifting%20Prevents%20Conditional%20Image%20Leakage%20in%20Image-to-Video%20Generation&entry.906535625=Yunyang%20Ge%20and%20Xinhua%20Cheng%20and%20Chengshu%20Zhao%20and%20Xianyi%20He%20and%20Shenghai%20Yuan%20and%20Bin%20Lin%20and%20Bin%20Zhu%20and%20Li%20Yuan&entry.1292438233=In%20Image-to-Video%20%28I2V%29%20generation%2C%20a%20video%20is%20created%20using%20an%20input%20image%20as%20the%20first-frame%20condition.%20Existing%20I2V%20methods%20concatenate%20the%20full%20information%20of%20the%20conditional%20image%20with%20noisy%20latents%20to%20achieve%20high%20fidelity.%20However%2C%20the%20denoisers%20in%20these%20methods%20tend%20to%20shortcut%20the%20conditional%20image%2C%20which%20is%20known%20as%20conditional%20image%20leakage%2C%20leading%20to%20performance%20degradation%20issues%20such%20as%20slow%20motion%20and%20color%20inconsistency.%20In%20this%20work%2C%20we%20further%20clarify%20that%20conditional%20image%20leakage%20leads%20to%20overfitting%20to%20in-domain%20data%20and%20decreases%20the%20performance%20in%20out-of-domain%20scenarios.%20Moreover%2C%20we%20introduce%20Fourier-Guided%20Latent%20Shifting%20I2V%2C%20named%20FlashI2V%2C%20to%20prevent%20conditional%20image%20leakage.%20Concretely%2C%20FlashI2V%20consists%20of%3A%20%281%29%20Latent%20Shifting.%20We%20modify%20the%20source%20and%20target%20distributions%20of%20flow%20matching%20by%20subtracting%20the%20conditional%20image%20information%20from%20the%20noisy%20latents%2C%20thereby%20incorporating%20the%20condition%20implicitly.%20%282%29%20Fourier%20Guidance.%20We%20use%20high-frequency%20magnitude%20features%20obtained%20by%20the%20Fourier%20Transform%20to%20accelerate%20convergence%20and%20enable%20the%20adjustment%20of%20detail%20levels%20in%20the%20generated%20video.%20Experimental%20results%20show%20that%20our%20method%20effectively%20overcomes%20conditional%20image%20leakage%20and%20achieves%20the%20best%20generalization%20and%20performance%20on%20out-of-domain%20data%20among%20various%20I2V%20paradigms.%20With%20only%201.3B%20parameters%2C%20FlashI2V%20achieves%20a%20dynamic%20degree%20score%20of%2053.01%20on%20Vbench-I2V%2C%20surpassing%20CogVideoX1.5-5B-I2V%20and%20Wan2.1-I2V-14B-480P.%20Project%20page%3A%20https%3A//pku-yuangroup.github.io/FlashI2V/&entry.1838667208=http%3A//arxiv.org/abs/2509.25187v2&entry.124074799=Read"},
{"title": "Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions", "author": "Frederic Kirstein and Sonu Kumar and Terry Ruas and Bela Gipp", "abstract": "Meeting summarization with large language models (LLMs) remains error-prone, often producing outputs with hallucinations, omissions, and irrelevancies. We present FRAME, a modular pipeline that reframes summarization as a semantic enrichment task. FRAME extracts and scores salient facts, organizes them thematically, and uses these to enrich an outline into an abstractive summary. To personalize summaries, we introduce SCOPE, a reason-out-loud protocol that has the model build a reasoning trace by answering nine questions before content selection. For evaluation, we propose P-MESA, a multi-dimensional, reference-free evaluation framework to assess if a summary fits a target reader. P-MESA reliably identifies error instances, achieving >= 89% balanced accuracy against human annotations and strongly aligns with human severity ratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and omission by 2 out of 5 points (measured with MESA), while SCOPE improves knowledge fit and goal alignment over prompt-only baselines. Our findings advocate for rethinking summarization to improve control, faithfulness, and personalization.", "link": "http://arxiv.org/abs/2509.15901v2", "date": "2025-11-14", "relevancy": 2.4343, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5045}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5045}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re-FRAME%20the%20Meeting%20Summarization%20SCOPE%3A%20Fact-Based%20Summarization%20and%20Personalization%20via%20Questions&body=Title%3A%20Re-FRAME%20the%20Meeting%20Summarization%20SCOPE%3A%20Fact-Based%20Summarization%20and%20Personalization%20via%20Questions%0AAuthor%3A%20Frederic%20Kirstein%20and%20Sonu%20Kumar%20and%20Terry%20Ruas%20and%20Bela%20Gipp%0AAbstract%3A%20Meeting%20summarization%20with%20large%20language%20models%20%28LLMs%29%20remains%20error-prone%2C%20often%20producing%20outputs%20with%20hallucinations%2C%20omissions%2C%20and%20irrelevancies.%20We%20present%20FRAME%2C%20a%20modular%20pipeline%20that%20reframes%20summarization%20as%20a%20semantic%20enrichment%20task.%20FRAME%20extracts%20and%20scores%20salient%20facts%2C%20organizes%20them%20thematically%2C%20and%20uses%20these%20to%20enrich%20an%20outline%20into%20an%20abstractive%20summary.%20To%20personalize%20summaries%2C%20we%20introduce%20SCOPE%2C%20a%20reason-out-loud%20protocol%20that%20has%20the%20model%20build%20a%20reasoning%20trace%20by%20answering%20nine%20questions%20before%20content%20selection.%20For%20evaluation%2C%20we%20propose%20P-MESA%2C%20a%20multi-dimensional%2C%20reference-free%20evaluation%20framework%20to%20assess%20if%20a%20summary%20fits%20a%20target%20reader.%20P-MESA%20reliably%20identifies%20error%20instances%2C%20achieving%20%3E%3D%2089%25%20balanced%20accuracy%20against%20human%20annotations%20and%20strongly%20aligns%20with%20human%20severity%20ratings%20%28r%20%3E%3D%200.70%29.%20On%20QMSum%20and%20FAME%2C%20FRAME%20reduces%20hallucination%20and%20omission%20by%202%20out%20of%205%20points%20%28measured%20with%20MESA%29%2C%20while%20SCOPE%20improves%20knowledge%20fit%20and%20goal%20alignment%20over%20prompt-only%20baselines.%20Our%20findings%20advocate%20for%20rethinking%20summarization%20to%20improve%20control%2C%20faithfulness%2C%20and%20personalization.%0ALink%3A%20http%3A//arxiv.org/abs/2509.15901v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe-FRAME%2520the%2520Meeting%2520Summarization%2520SCOPE%253A%2520Fact-Based%2520Summarization%2520and%2520Personalization%2520via%2520Questions%26entry.906535625%3DFrederic%2520Kirstein%2520and%2520Sonu%2520Kumar%2520and%2520Terry%2520Ruas%2520and%2520Bela%2520Gipp%26entry.1292438233%3DMeeting%2520summarization%2520with%2520large%2520language%2520models%2520%2528LLMs%2529%2520remains%2520error-prone%252C%2520often%2520producing%2520outputs%2520with%2520hallucinations%252C%2520omissions%252C%2520and%2520irrelevancies.%2520We%2520present%2520FRAME%252C%2520a%2520modular%2520pipeline%2520that%2520reframes%2520summarization%2520as%2520a%2520semantic%2520enrichment%2520task.%2520FRAME%2520extracts%2520and%2520scores%2520salient%2520facts%252C%2520organizes%2520them%2520thematically%252C%2520and%2520uses%2520these%2520to%2520enrich%2520an%2520outline%2520into%2520an%2520abstractive%2520summary.%2520To%2520personalize%2520summaries%252C%2520we%2520introduce%2520SCOPE%252C%2520a%2520reason-out-loud%2520protocol%2520that%2520has%2520the%2520model%2520build%2520a%2520reasoning%2520trace%2520by%2520answering%2520nine%2520questions%2520before%2520content%2520selection.%2520For%2520evaluation%252C%2520we%2520propose%2520P-MESA%252C%2520a%2520multi-dimensional%252C%2520reference-free%2520evaluation%2520framework%2520to%2520assess%2520if%2520a%2520summary%2520fits%2520a%2520target%2520reader.%2520P-MESA%2520reliably%2520identifies%2520error%2520instances%252C%2520achieving%2520%253E%253D%252089%2525%2520balanced%2520accuracy%2520against%2520human%2520annotations%2520and%2520strongly%2520aligns%2520with%2520human%2520severity%2520ratings%2520%2528r%2520%253E%253D%25200.70%2529.%2520On%2520QMSum%2520and%2520FAME%252C%2520FRAME%2520reduces%2520hallucination%2520and%2520omission%2520by%25202%2520out%2520of%25205%2520points%2520%2528measured%2520with%2520MESA%2529%252C%2520while%2520SCOPE%2520improves%2520knowledge%2520fit%2520and%2520goal%2520alignment%2520over%2520prompt-only%2520baselines.%2520Our%2520findings%2520advocate%2520for%2520rethinking%2520summarization%2520to%2520improve%2520control%252C%2520faithfulness%252C%2520and%2520personalization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15901v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-FRAME%20the%20Meeting%20Summarization%20SCOPE%3A%20Fact-Based%20Summarization%20and%20Personalization%20via%20Questions&entry.906535625=Frederic%20Kirstein%20and%20Sonu%20Kumar%20and%20Terry%20Ruas%20and%20Bela%20Gipp&entry.1292438233=Meeting%20summarization%20with%20large%20language%20models%20%28LLMs%29%20remains%20error-prone%2C%20often%20producing%20outputs%20with%20hallucinations%2C%20omissions%2C%20and%20irrelevancies.%20We%20present%20FRAME%2C%20a%20modular%20pipeline%20that%20reframes%20summarization%20as%20a%20semantic%20enrichment%20task.%20FRAME%20extracts%20and%20scores%20salient%20facts%2C%20organizes%20them%20thematically%2C%20and%20uses%20these%20to%20enrich%20an%20outline%20into%20an%20abstractive%20summary.%20To%20personalize%20summaries%2C%20we%20introduce%20SCOPE%2C%20a%20reason-out-loud%20protocol%20that%20has%20the%20model%20build%20a%20reasoning%20trace%20by%20answering%20nine%20questions%20before%20content%20selection.%20For%20evaluation%2C%20we%20propose%20P-MESA%2C%20a%20multi-dimensional%2C%20reference-free%20evaluation%20framework%20to%20assess%20if%20a%20summary%20fits%20a%20target%20reader.%20P-MESA%20reliably%20identifies%20error%20instances%2C%20achieving%20%3E%3D%2089%25%20balanced%20accuracy%20against%20human%20annotations%20and%20strongly%20aligns%20with%20human%20severity%20ratings%20%28r%20%3E%3D%200.70%29.%20On%20QMSum%20and%20FAME%2C%20FRAME%20reduces%20hallucination%20and%20omission%20by%202%20out%20of%205%20points%20%28measured%20with%20MESA%29%2C%20while%20SCOPE%20improves%20knowledge%20fit%20and%20goal%20alignment%20over%20prompt-only%20baselines.%20Our%20findings%20advocate%20for%20rethinking%20summarization%20to%20improve%20control%2C%20faithfulness%2C%20and%20personalization.&entry.1838667208=http%3A//arxiv.org/abs/2509.15901v2&entry.124074799=Read"},
{"title": "Gaussian Process Tilted Nonparametric Density Estimation using Fisher Divergence Score Matching", "author": "John Paisley and Wei Zhang and Brian Barr", "abstract": "We propose a nonparametric density estimator based on the Gaussian process (GP) and derive three novel closed form learning algorithms based on Fisher divergence (FD) score matching. The density estimator is formed by multiplying a base multivariate normal distribution with an exponentiated GP refinement, and so we refer to it as a GP-tilted nonparametric density. By representing the GP part of the score as a linear function using the random Fourier feature (RFF) approximation, we show that optimization can be solved in closed form for the three FD-based objectives considered. This includes the basic and noise conditional versions of the Fisher divergence, as well as an alternative to noise conditional FD models based on variational inference (VI) that we propose in this paper. For this novel learning approach, we propose an ELBO-like optimization to approximate the posterior distribution, with which we then derive a Fisher variational predictive distribution. The RFF representation of the GP, which is functionally equivalent to a single layer neural network score model with cosine activation, provides a useful linear representation of the GP for which all expectations can be solved. The Gaussian base distribution also helps with tractability of the VI approximation and ensures that our proposed density is well-defined. We demonstrate our three learning algorithms, as well as a MAP baseline algorithm, on several low dimensional density estimation problems. The closed form nature of the learning problem removes the reliance on iterative learning algorithms, making this technique particularly well-suited to big data sets, since only sufficient statistics collected from a single pass through the data is needed.", "link": "http://arxiv.org/abs/2504.03485v2", "date": "2025-11-14", "relevancy": 2.4242, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4978}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4823}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Process%20Tilted%20Nonparametric%20Density%20Estimation%20using%20Fisher%20Divergence%20Score%20Matching&body=Title%3A%20Gaussian%20Process%20Tilted%20Nonparametric%20Density%20Estimation%20using%20Fisher%20Divergence%20Score%20Matching%0AAuthor%3A%20John%20Paisley%20and%20Wei%20Zhang%20and%20Brian%20Barr%0AAbstract%3A%20We%20propose%20a%20nonparametric%20density%20estimator%20based%20on%20the%20Gaussian%20process%20%28GP%29%20and%20derive%20three%20novel%20closed%20form%20learning%20algorithms%20based%20on%20Fisher%20divergence%20%28FD%29%20score%20matching.%20The%20density%20estimator%20is%20formed%20by%20multiplying%20a%20base%20multivariate%20normal%20distribution%20with%20an%20exponentiated%20GP%20refinement%2C%20and%20so%20we%20refer%20to%20it%20as%20a%20GP-tilted%20nonparametric%20density.%20By%20representing%20the%20GP%20part%20of%20the%20score%20as%20a%20linear%20function%20using%20the%20random%20Fourier%20feature%20%28RFF%29%20approximation%2C%20we%20show%20that%20optimization%20can%20be%20solved%20in%20closed%20form%20for%20the%20three%20FD-based%20objectives%20considered.%20This%20includes%20the%20basic%20and%20noise%20conditional%20versions%20of%20the%20Fisher%20divergence%2C%20as%20well%20as%20an%20alternative%20to%20noise%20conditional%20FD%20models%20based%20on%20variational%20inference%20%28VI%29%20that%20we%20propose%20in%20this%20paper.%20For%20this%20novel%20learning%20approach%2C%20we%20propose%20an%20ELBO-like%20optimization%20to%20approximate%20the%20posterior%20distribution%2C%20with%20which%20we%20then%20derive%20a%20Fisher%20variational%20predictive%20distribution.%20The%20RFF%20representation%20of%20the%20GP%2C%20which%20is%20functionally%20equivalent%20to%20a%20single%20layer%20neural%20network%20score%20model%20with%20cosine%20activation%2C%20provides%20a%20useful%20linear%20representation%20of%20the%20GP%20for%20which%20all%20expectations%20can%20be%20solved.%20The%20Gaussian%20base%20distribution%20also%20helps%20with%20tractability%20of%20the%20VI%20approximation%20and%20ensures%20that%20our%20proposed%20density%20is%20well-defined.%20We%20demonstrate%20our%20three%20learning%20algorithms%2C%20as%20well%20as%20a%20MAP%20baseline%20algorithm%2C%20on%20several%20low%20dimensional%20density%20estimation%20problems.%20The%20closed%20form%20nature%20of%20the%20learning%20problem%20removes%20the%20reliance%20on%20iterative%20learning%20algorithms%2C%20making%20this%20technique%20particularly%20well-suited%20to%20big%20data%20sets%2C%20since%20only%20sufficient%20statistics%20collected%20from%20a%20single%20pass%20through%20the%20data%20is%20needed.%0ALink%3A%20http%3A//arxiv.org/abs/2504.03485v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Process%2520Tilted%2520Nonparametric%2520Density%2520Estimation%2520using%2520Fisher%2520Divergence%2520Score%2520Matching%26entry.906535625%3DJohn%2520Paisley%2520and%2520Wei%2520Zhang%2520and%2520Brian%2520Barr%26entry.1292438233%3DWe%2520propose%2520a%2520nonparametric%2520density%2520estimator%2520based%2520on%2520the%2520Gaussian%2520process%2520%2528GP%2529%2520and%2520derive%2520three%2520novel%2520closed%2520form%2520learning%2520algorithms%2520based%2520on%2520Fisher%2520divergence%2520%2528FD%2529%2520score%2520matching.%2520The%2520density%2520estimator%2520is%2520formed%2520by%2520multiplying%2520a%2520base%2520multivariate%2520normal%2520distribution%2520with%2520an%2520exponentiated%2520GP%2520refinement%252C%2520and%2520so%2520we%2520refer%2520to%2520it%2520as%2520a%2520GP-tilted%2520nonparametric%2520density.%2520By%2520representing%2520the%2520GP%2520part%2520of%2520the%2520score%2520as%2520a%2520linear%2520function%2520using%2520the%2520random%2520Fourier%2520feature%2520%2528RFF%2529%2520approximation%252C%2520we%2520show%2520that%2520optimization%2520can%2520be%2520solved%2520in%2520closed%2520form%2520for%2520the%2520three%2520FD-based%2520objectives%2520considered.%2520This%2520includes%2520the%2520basic%2520and%2520noise%2520conditional%2520versions%2520of%2520the%2520Fisher%2520divergence%252C%2520as%2520well%2520as%2520an%2520alternative%2520to%2520noise%2520conditional%2520FD%2520models%2520based%2520on%2520variational%2520inference%2520%2528VI%2529%2520that%2520we%2520propose%2520in%2520this%2520paper.%2520For%2520this%2520novel%2520learning%2520approach%252C%2520we%2520propose%2520an%2520ELBO-like%2520optimization%2520to%2520approximate%2520the%2520posterior%2520distribution%252C%2520with%2520which%2520we%2520then%2520derive%2520a%2520Fisher%2520variational%2520predictive%2520distribution.%2520The%2520RFF%2520representation%2520of%2520the%2520GP%252C%2520which%2520is%2520functionally%2520equivalent%2520to%2520a%2520single%2520layer%2520neural%2520network%2520score%2520model%2520with%2520cosine%2520activation%252C%2520provides%2520a%2520useful%2520linear%2520representation%2520of%2520the%2520GP%2520for%2520which%2520all%2520expectations%2520can%2520be%2520solved.%2520The%2520Gaussian%2520base%2520distribution%2520also%2520helps%2520with%2520tractability%2520of%2520the%2520VI%2520approximation%2520and%2520ensures%2520that%2520our%2520proposed%2520density%2520is%2520well-defined.%2520We%2520demonstrate%2520our%2520three%2520learning%2520algorithms%252C%2520as%2520well%2520as%2520a%2520MAP%2520baseline%2520algorithm%252C%2520on%2520several%2520low%2520dimensional%2520density%2520estimation%2520problems.%2520The%2520closed%2520form%2520nature%2520of%2520the%2520learning%2520problem%2520removes%2520the%2520reliance%2520on%2520iterative%2520learning%2520algorithms%252C%2520making%2520this%2520technique%2520particularly%2520well-suited%2520to%2520big%2520data%2520sets%252C%2520since%2520only%2520sufficient%2520statistics%2520collected%2520from%2520a%2520single%2520pass%2520through%2520the%2520data%2520is%2520needed.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.03485v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Process%20Tilted%20Nonparametric%20Density%20Estimation%20using%20Fisher%20Divergence%20Score%20Matching&entry.906535625=John%20Paisley%20and%20Wei%20Zhang%20and%20Brian%20Barr&entry.1292438233=We%20propose%20a%20nonparametric%20density%20estimator%20based%20on%20the%20Gaussian%20process%20%28GP%29%20and%20derive%20three%20novel%20closed%20form%20learning%20algorithms%20based%20on%20Fisher%20divergence%20%28FD%29%20score%20matching.%20The%20density%20estimator%20is%20formed%20by%20multiplying%20a%20base%20multivariate%20normal%20distribution%20with%20an%20exponentiated%20GP%20refinement%2C%20and%20so%20we%20refer%20to%20it%20as%20a%20GP-tilted%20nonparametric%20density.%20By%20representing%20the%20GP%20part%20of%20the%20score%20as%20a%20linear%20function%20using%20the%20random%20Fourier%20feature%20%28RFF%29%20approximation%2C%20we%20show%20that%20optimization%20can%20be%20solved%20in%20closed%20form%20for%20the%20three%20FD-based%20objectives%20considered.%20This%20includes%20the%20basic%20and%20noise%20conditional%20versions%20of%20the%20Fisher%20divergence%2C%20as%20well%20as%20an%20alternative%20to%20noise%20conditional%20FD%20models%20based%20on%20variational%20inference%20%28VI%29%20that%20we%20propose%20in%20this%20paper.%20For%20this%20novel%20learning%20approach%2C%20we%20propose%20an%20ELBO-like%20optimization%20to%20approximate%20the%20posterior%20distribution%2C%20with%20which%20we%20then%20derive%20a%20Fisher%20variational%20predictive%20distribution.%20The%20RFF%20representation%20of%20the%20GP%2C%20which%20is%20functionally%20equivalent%20to%20a%20single%20layer%20neural%20network%20score%20model%20with%20cosine%20activation%2C%20provides%20a%20useful%20linear%20representation%20of%20the%20GP%20for%20which%20all%20expectations%20can%20be%20solved.%20The%20Gaussian%20base%20distribution%20also%20helps%20with%20tractability%20of%20the%20VI%20approximation%20and%20ensures%20that%20our%20proposed%20density%20is%20well-defined.%20We%20demonstrate%20our%20three%20learning%20algorithms%2C%20as%20well%20as%20a%20MAP%20baseline%20algorithm%2C%20on%20several%20low%20dimensional%20density%20estimation%20problems.%20The%20closed%20form%20nature%20of%20the%20learning%20problem%20removes%20the%20reliance%20on%20iterative%20learning%20algorithms%2C%20making%20this%20technique%20particularly%20well-suited%20to%20big%20data%20sets%2C%20since%20only%20sufficient%20statistics%20collected%20from%20a%20single%20pass%20through%20the%20data%20is%20needed.&entry.1838667208=http%3A//arxiv.org/abs/2504.03485v2&entry.124074799=Read"},
{"title": "ImAgent: A Unified Multimodal Agent Framework for Test-Time Scalable Image Generation", "author": "Kaishen Wang and Ruibo Chen and Tong Zheng and Heng Huang", "abstract": "Recent text-to-image (T2I) models have made remarkable progress in generating visually realistic and semantically coherent images. However, they still suffer from randomness and inconsistency with the given prompts, particularly when textual descriptions are vague or underspecified. Existing approaches, such as prompt rewriting, best-of-N sampling, and self-refinement, can mitigate these issues but usually require additional modules and operate independently, hindering test-time scaling efficiency and increasing computational overhead. In this paper, we introduce ImAgent, a training-free unified multimodal agent that integrates reasoning, generation, and self-evaluation within a single framework for efficient test-time scaling. Guided by a policy controller, multiple generation actions dynamically interact and self-organize to enhance image fidelity and semantic alignment without relying on external models. Extensive experiments on image generation and editing tasks demonstrate that ImAgent consistently improves over the backbone and even surpasses other strong baselines where the backbone model fails, highlighting the potential of unified multimodal agents for adaptive and efficient image generation under test-time scaling.", "link": "http://arxiv.org/abs/2511.11483v1", "date": "2025-11-14", "relevancy": 2.4193, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6111}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6019}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImAgent%3A%20A%20Unified%20Multimodal%20Agent%20Framework%20for%20Test-Time%20Scalable%20Image%20Generation&body=Title%3A%20ImAgent%3A%20A%20Unified%20Multimodal%20Agent%20Framework%20for%20Test-Time%20Scalable%20Image%20Generation%0AAuthor%3A%20Kaishen%20Wang%20and%20Ruibo%20Chen%20and%20Tong%20Zheng%20and%20Heng%20Huang%0AAbstract%3A%20Recent%20text-to-image%20%28T2I%29%20models%20have%20made%20remarkable%20progress%20in%20generating%20visually%20realistic%20and%20semantically%20coherent%20images.%20However%2C%20they%20still%20suffer%20from%20randomness%20and%20inconsistency%20with%20the%20given%20prompts%2C%20particularly%20when%20textual%20descriptions%20are%20vague%20or%20underspecified.%20Existing%20approaches%2C%20such%20as%20prompt%20rewriting%2C%20best-of-N%20sampling%2C%20and%20self-refinement%2C%20can%20mitigate%20these%20issues%20but%20usually%20require%20additional%20modules%20and%20operate%20independently%2C%20hindering%20test-time%20scaling%20efficiency%20and%20increasing%20computational%20overhead.%20In%20this%20paper%2C%20we%20introduce%20ImAgent%2C%20a%20training-free%20unified%20multimodal%20agent%20that%20integrates%20reasoning%2C%20generation%2C%20and%20self-evaluation%20within%20a%20single%20framework%20for%20efficient%20test-time%20scaling.%20Guided%20by%20a%20policy%20controller%2C%20multiple%20generation%20actions%20dynamically%20interact%20and%20self-organize%20to%20enhance%20image%20fidelity%20and%20semantic%20alignment%20without%20relying%20on%20external%20models.%20Extensive%20experiments%20on%20image%20generation%20and%20editing%20tasks%20demonstrate%20that%20ImAgent%20consistently%20improves%20over%20the%20backbone%20and%20even%20surpasses%20other%20strong%20baselines%20where%20the%20backbone%20model%20fails%2C%20highlighting%20the%20potential%20of%20unified%20multimodal%20agents%20for%20adaptive%20and%20efficient%20image%20generation%20under%20test-time%20scaling.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11483v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImAgent%253A%2520A%2520Unified%2520Multimodal%2520Agent%2520Framework%2520for%2520Test-Time%2520Scalable%2520Image%2520Generation%26entry.906535625%3DKaishen%2520Wang%2520and%2520Ruibo%2520Chen%2520and%2520Tong%2520Zheng%2520and%2520Heng%2520Huang%26entry.1292438233%3DRecent%2520text-to-image%2520%2528T2I%2529%2520models%2520have%2520made%2520remarkable%2520progress%2520in%2520generating%2520visually%2520realistic%2520and%2520semantically%2520coherent%2520images.%2520However%252C%2520they%2520still%2520suffer%2520from%2520randomness%2520and%2520inconsistency%2520with%2520the%2520given%2520prompts%252C%2520particularly%2520when%2520textual%2520descriptions%2520are%2520vague%2520or%2520underspecified.%2520Existing%2520approaches%252C%2520such%2520as%2520prompt%2520rewriting%252C%2520best-of-N%2520sampling%252C%2520and%2520self-refinement%252C%2520can%2520mitigate%2520these%2520issues%2520but%2520usually%2520require%2520additional%2520modules%2520and%2520operate%2520independently%252C%2520hindering%2520test-time%2520scaling%2520efficiency%2520and%2520increasing%2520computational%2520overhead.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520ImAgent%252C%2520a%2520training-free%2520unified%2520multimodal%2520agent%2520that%2520integrates%2520reasoning%252C%2520generation%252C%2520and%2520self-evaluation%2520within%2520a%2520single%2520framework%2520for%2520efficient%2520test-time%2520scaling.%2520Guided%2520by%2520a%2520policy%2520controller%252C%2520multiple%2520generation%2520actions%2520dynamically%2520interact%2520and%2520self-organize%2520to%2520enhance%2520image%2520fidelity%2520and%2520semantic%2520alignment%2520without%2520relying%2520on%2520external%2520models.%2520Extensive%2520experiments%2520on%2520image%2520generation%2520and%2520editing%2520tasks%2520demonstrate%2520that%2520ImAgent%2520consistently%2520improves%2520over%2520the%2520backbone%2520and%2520even%2520surpasses%2520other%2520strong%2520baselines%2520where%2520the%2520backbone%2520model%2520fails%252C%2520highlighting%2520the%2520potential%2520of%2520unified%2520multimodal%2520agents%2520for%2520adaptive%2520and%2520efficient%2520image%2520generation%2520under%2520test-time%2520scaling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11483v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImAgent%3A%20A%20Unified%20Multimodal%20Agent%20Framework%20for%20Test-Time%20Scalable%20Image%20Generation&entry.906535625=Kaishen%20Wang%20and%20Ruibo%20Chen%20and%20Tong%20Zheng%20and%20Heng%20Huang&entry.1292438233=Recent%20text-to-image%20%28T2I%29%20models%20have%20made%20remarkable%20progress%20in%20generating%20visually%20realistic%20and%20semantically%20coherent%20images.%20However%2C%20they%20still%20suffer%20from%20randomness%20and%20inconsistency%20with%20the%20given%20prompts%2C%20particularly%20when%20textual%20descriptions%20are%20vague%20or%20underspecified.%20Existing%20approaches%2C%20such%20as%20prompt%20rewriting%2C%20best-of-N%20sampling%2C%20and%20self-refinement%2C%20can%20mitigate%20these%20issues%20but%20usually%20require%20additional%20modules%20and%20operate%20independently%2C%20hindering%20test-time%20scaling%20efficiency%20and%20increasing%20computational%20overhead.%20In%20this%20paper%2C%20we%20introduce%20ImAgent%2C%20a%20training-free%20unified%20multimodal%20agent%20that%20integrates%20reasoning%2C%20generation%2C%20and%20self-evaluation%20within%20a%20single%20framework%20for%20efficient%20test-time%20scaling.%20Guided%20by%20a%20policy%20controller%2C%20multiple%20generation%20actions%20dynamically%20interact%20and%20self-organize%20to%20enhance%20image%20fidelity%20and%20semantic%20alignment%20without%20relying%20on%20external%20models.%20Extensive%20experiments%20on%20image%20generation%20and%20editing%20tasks%20demonstrate%20that%20ImAgent%20consistently%20improves%20over%20the%20backbone%20and%20even%20surpasses%20other%20strong%20baselines%20where%20the%20backbone%20model%20fails%2C%20highlighting%20the%20potential%20of%20unified%20multimodal%20agents%20for%20adaptive%20and%20efficient%20image%20generation%20under%20test-time%20scaling.&entry.1838667208=http%3A//arxiv.org/abs/2511.11483v1&entry.124074799=Read"},
{"title": "ICL-Router: In-Context Learned Model Representations for LLM Routing", "author": "Chenxu Wang and Hao Li and Yiqun Zhang and Linyao Chen and Jianhao Chen and Ping Jian and Peng Ye and Qiaosheng Zhang and Shuyue Hu", "abstract": "Large language models (LLMs) often exhibit complementary strengths. Model routing harnesses these strengths by dynamically directing each query to the most suitable model, given a candidate model pool. However, routing performance relies on accurate model representations, and adding new models typically requires retraining, limiting scalability. To address these challenges, we propose a novel routing method using in-context vectors to represent model capabilities. The method proceeds in two stages. First, queries are embedded and projected into vectors, with a projector and LLM-based router trained to reconstruct the original queries, aligning vector representations with the router's semantic space. Second, each candidate model is profiled on a query set, and the router learns -- based on in-context vectors of query and model performance -- to predict whether each model can correctly answer new queries. Extensive experiments demonstrate that our method achieves state-of-the-art routing performance in both in-distribution and out-of-distribution tasks. Moreover, our method allows for seamless integration of new models without retraining the router. The code is available at https://github.com/lalalamdbf/ICL-Router.", "link": "http://arxiv.org/abs/2510.09719v3", "date": "2025-11-14", "relevancy": 2.4118, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4843}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4814}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ICL-Router%3A%20In-Context%20Learned%20Model%20Representations%20for%20LLM%20Routing&body=Title%3A%20ICL-Router%3A%20In-Context%20Learned%20Model%20Representations%20for%20LLM%20Routing%0AAuthor%3A%20Chenxu%20Wang%20and%20Hao%20Li%20and%20Yiqun%20Zhang%20and%20Linyao%20Chen%20and%20Jianhao%20Chen%20and%20Ping%20Jian%20and%20Peng%20Ye%20and%20Qiaosheng%20Zhang%20and%20Shuyue%20Hu%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20often%20exhibit%20complementary%20strengths.%20Model%20routing%20harnesses%20these%20strengths%20by%20dynamically%20directing%20each%20query%20to%20the%20most%20suitable%20model%2C%20given%20a%20candidate%20model%20pool.%20However%2C%20routing%20performance%20relies%20on%20accurate%20model%20representations%2C%20and%20adding%20new%20models%20typically%20requires%20retraining%2C%20limiting%20scalability.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20routing%20method%20using%20in-context%20vectors%20to%20represent%20model%20capabilities.%20The%20method%20proceeds%20in%20two%20stages.%20First%2C%20queries%20are%20embedded%20and%20projected%20into%20vectors%2C%20with%20a%20projector%20and%20LLM-based%20router%20trained%20to%20reconstruct%20the%20original%20queries%2C%20aligning%20vector%20representations%20with%20the%20router%27s%20semantic%20space.%20Second%2C%20each%20candidate%20model%20is%20profiled%20on%20a%20query%20set%2C%20and%20the%20router%20learns%20--%20based%20on%20in-context%20vectors%20of%20query%20and%20model%20performance%20--%20to%20predict%20whether%20each%20model%20can%20correctly%20answer%20new%20queries.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20routing%20performance%20in%20both%20in-distribution%20and%20out-of-distribution%20tasks.%20Moreover%2C%20our%20method%20allows%20for%20seamless%20integration%20of%20new%20models%20without%20retraining%20the%20router.%20The%20code%20is%20available%20at%20https%3A//github.com/lalalamdbf/ICL-Router.%0ALink%3A%20http%3A//arxiv.org/abs/2510.09719v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DICL-Router%253A%2520In-Context%2520Learned%2520Model%2520Representations%2520for%2520LLM%2520Routing%26entry.906535625%3DChenxu%2520Wang%2520and%2520Hao%2520Li%2520and%2520Yiqun%2520Zhang%2520and%2520Linyao%2520Chen%2520and%2520Jianhao%2520Chen%2520and%2520Ping%2520Jian%2520and%2520Peng%2520Ye%2520and%2520Qiaosheng%2520Zhang%2520and%2520Shuyue%2520Hu%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520often%2520exhibit%2520complementary%2520strengths.%2520Model%2520routing%2520harnesses%2520these%2520strengths%2520by%2520dynamically%2520directing%2520each%2520query%2520to%2520the%2520most%2520suitable%2520model%252C%2520given%2520a%2520candidate%2520model%2520pool.%2520However%252C%2520routing%2520performance%2520relies%2520on%2520accurate%2520model%2520representations%252C%2520and%2520adding%2520new%2520models%2520typically%2520requires%2520retraining%252C%2520limiting%2520scalability.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520routing%2520method%2520using%2520in-context%2520vectors%2520to%2520represent%2520model%2520capabilities.%2520The%2520method%2520proceeds%2520in%2520two%2520stages.%2520First%252C%2520queries%2520are%2520embedded%2520and%2520projected%2520into%2520vectors%252C%2520with%2520a%2520projector%2520and%2520LLM-based%2520router%2520trained%2520to%2520reconstruct%2520the%2520original%2520queries%252C%2520aligning%2520vector%2520representations%2520with%2520the%2520router%2527s%2520semantic%2520space.%2520Second%252C%2520each%2520candidate%2520model%2520is%2520profiled%2520on%2520a%2520query%2520set%252C%2520and%2520the%2520router%2520learns%2520--%2520based%2520on%2520in-context%2520vectors%2520of%2520query%2520and%2520model%2520performance%2520--%2520to%2520predict%2520whether%2520each%2520model%2520can%2520correctly%2520answer%2520new%2520queries.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520routing%2520performance%2520in%2520both%2520in-distribution%2520and%2520out-of-distribution%2520tasks.%2520Moreover%252C%2520our%2520method%2520allows%2520for%2520seamless%2520integration%2520of%2520new%2520models%2520without%2520retraining%2520the%2520router.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/lalalamdbf/ICL-Router.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09719v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ICL-Router%3A%20In-Context%20Learned%20Model%20Representations%20for%20LLM%20Routing&entry.906535625=Chenxu%20Wang%20and%20Hao%20Li%20and%20Yiqun%20Zhang%20and%20Linyao%20Chen%20and%20Jianhao%20Chen%20and%20Ping%20Jian%20and%20Peng%20Ye%20and%20Qiaosheng%20Zhang%20and%20Shuyue%20Hu&entry.1292438233=Large%20language%20models%20%28LLMs%29%20often%20exhibit%20complementary%20strengths.%20Model%20routing%20harnesses%20these%20strengths%20by%20dynamically%20directing%20each%20query%20to%20the%20most%20suitable%20model%2C%20given%20a%20candidate%20model%20pool.%20However%2C%20routing%20performance%20relies%20on%20accurate%20model%20representations%2C%20and%20adding%20new%20models%20typically%20requires%20retraining%2C%20limiting%20scalability.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20routing%20method%20using%20in-context%20vectors%20to%20represent%20model%20capabilities.%20The%20method%20proceeds%20in%20two%20stages.%20First%2C%20queries%20are%20embedded%20and%20projected%20into%20vectors%2C%20with%20a%20projector%20and%20LLM-based%20router%20trained%20to%20reconstruct%20the%20original%20queries%2C%20aligning%20vector%20representations%20with%20the%20router%27s%20semantic%20space.%20Second%2C%20each%20candidate%20model%20is%20profiled%20on%20a%20query%20set%2C%20and%20the%20router%20learns%20--%20based%20on%20in-context%20vectors%20of%20query%20and%20model%20performance%20--%20to%20predict%20whether%20each%20model%20can%20correctly%20answer%20new%20queries.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20routing%20performance%20in%20both%20in-distribution%20and%20out-of-distribution%20tasks.%20Moreover%2C%20our%20method%20allows%20for%20seamless%20integration%20of%20new%20models%20without%20retraining%20the%20router.%20The%20code%20is%20available%20at%20https%3A//github.com/lalalamdbf/ICL-Router.&entry.1838667208=http%3A//arxiv.org/abs/2510.09719v3&entry.124074799=Read"},
{"title": "VP-Bench: A Comprehensive Benchmark for Visual Prompting in Multimodal Large Language Models", "author": "Mingjie Xu and Jinpeng Chen and Yuzhi Zhao and Jason Chun Lok Li and Yue Qiu and Zekang Du and Mengyang Wu and Pingping Zhang and Kun Li and Hongzheng Yang and Wenao Ma and Jiaheng Wei and Qinbin Li and Kangcheng Liu and Wenqiang Lei", "abstract": "Multimodal large language models (MLLMs) have enabled a wide range of advanced vision-language applications, including fine-grained object recognition and contextual understanding. When querying specific regions or objects in an image, human users naturally use \"visual prompts\" (VPs), such as bounding boxes, to provide reference. However, no existing benchmark systematically evaluates the ability of MLLMs to interpret such VPs. This gap leaves it unclear whether current MLLMs can effectively recognize VPs, an intuitive prompting method for humans, and use them to solve problems. To address this limitation, we introduce VP-Bench, a benchmark for assessing MLLMs' capability in VP perception and utilization. VP-Bench employs a two-stage evaluation framework: Stage 1 examines models' ability to perceive VPs in natural scenes, using 30k visualized prompts spanning eight shapes and 355 attribute combinations. Stage 2 investigates the impact of VPs on downstream tasks, measuring their effectiveness in real-world problem-solving scenarios. Using VP-Bench, we evaluate 28 MLLMs, including proprietary systems (e.g., GPT-4o) and open-source models (e.g., InternVL3 and Qwen2.5-VL), and provide a comprehensive analysis of factors that affect VP understanding, such as variations in VP attributes, question arrangement, and model scale. VP-Bench establishes a new reference framework for studying how MLLMs comprehend and resolve grounded referring questions.", "link": "http://arxiv.org/abs/2511.11438v1", "date": "2025-11-14", "relevancy": 2.3975, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6152}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6152}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VP-Bench%3A%20A%20Comprehensive%20Benchmark%20for%20Visual%20Prompting%20in%20Multimodal%20Large%20Language%20Models&body=Title%3A%20VP-Bench%3A%20A%20Comprehensive%20Benchmark%20for%20Visual%20Prompting%20in%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Mingjie%20Xu%20and%20Jinpeng%20Chen%20and%20Yuzhi%20Zhao%20and%20Jason%20Chun%20Lok%20Li%20and%20Yue%20Qiu%20and%20Zekang%20Du%20and%20Mengyang%20Wu%20and%20Pingping%20Zhang%20and%20Kun%20Li%20and%20Hongzheng%20Yang%20and%20Wenao%20Ma%20and%20Jiaheng%20Wei%20and%20Qinbin%20Li%20and%20Kangcheng%20Liu%20and%20Wenqiang%20Lei%0AAbstract%3A%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20enabled%20a%20wide%20range%20of%20advanced%20vision-language%20applications%2C%20including%20fine-grained%20object%20recognition%20and%20contextual%20understanding.%20When%20querying%20specific%20regions%20or%20objects%20in%20an%20image%2C%20human%20users%20naturally%20use%20%22visual%20prompts%22%20%28VPs%29%2C%20such%20as%20bounding%20boxes%2C%20to%20provide%20reference.%20However%2C%20no%20existing%20benchmark%20systematically%20evaluates%20the%20ability%20of%20MLLMs%20to%20interpret%20such%20VPs.%20This%20gap%20leaves%20it%20unclear%20whether%20current%20MLLMs%20can%20effectively%20recognize%20VPs%2C%20an%20intuitive%20prompting%20method%20for%20humans%2C%20and%20use%20them%20to%20solve%20problems.%20To%20address%20this%20limitation%2C%20we%20introduce%20VP-Bench%2C%20a%20benchmark%20for%20assessing%20MLLMs%27%20capability%20in%20VP%20perception%20and%20utilization.%20VP-Bench%20employs%20a%20two-stage%20evaluation%20framework%3A%20Stage%201%20examines%20models%27%20ability%20to%20perceive%20VPs%20in%20natural%20scenes%2C%20using%2030k%20visualized%20prompts%20spanning%20eight%20shapes%20and%20355%20attribute%20combinations.%20Stage%202%20investigates%20the%20impact%20of%20VPs%20on%20downstream%20tasks%2C%20measuring%20their%20effectiveness%20in%20real-world%20problem-solving%20scenarios.%20Using%20VP-Bench%2C%20we%20evaluate%2028%20MLLMs%2C%20including%20proprietary%20systems%20%28e.g.%2C%20GPT-4o%29%20and%20open-source%20models%20%28e.g.%2C%20InternVL3%20and%20Qwen2.5-VL%29%2C%20and%20provide%20a%20comprehensive%20analysis%20of%20factors%20that%20affect%20VP%20understanding%2C%20such%20as%20variations%20in%20VP%20attributes%2C%20question%20arrangement%2C%20and%20model%20scale.%20VP-Bench%20establishes%20a%20new%20reference%20framework%20for%20studying%20how%20MLLMs%20comprehend%20and%20resolve%20grounded%20referring%20questions.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVP-Bench%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Visual%2520Prompting%2520in%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DMingjie%2520Xu%2520and%2520Jinpeng%2520Chen%2520and%2520Yuzhi%2520Zhao%2520and%2520Jason%2520Chun%2520Lok%2520Li%2520and%2520Yue%2520Qiu%2520and%2520Zekang%2520Du%2520and%2520Mengyang%2520Wu%2520and%2520Pingping%2520Zhang%2520and%2520Kun%2520Li%2520and%2520Hongzheng%2520Yang%2520and%2520Wenao%2520Ma%2520and%2520Jiaheng%2520Wei%2520and%2520Qinbin%2520Li%2520and%2520Kangcheng%2520Liu%2520and%2520Wenqiang%2520Lei%26entry.1292438233%3DMultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520enabled%2520a%2520wide%2520range%2520of%2520advanced%2520vision-language%2520applications%252C%2520including%2520fine-grained%2520object%2520recognition%2520and%2520contextual%2520understanding.%2520When%2520querying%2520specific%2520regions%2520or%2520objects%2520in%2520an%2520image%252C%2520human%2520users%2520naturally%2520use%2520%2522visual%2520prompts%2522%2520%2528VPs%2529%252C%2520such%2520as%2520bounding%2520boxes%252C%2520to%2520provide%2520reference.%2520However%252C%2520no%2520existing%2520benchmark%2520systematically%2520evaluates%2520the%2520ability%2520of%2520MLLMs%2520to%2520interpret%2520such%2520VPs.%2520This%2520gap%2520leaves%2520it%2520unclear%2520whether%2520current%2520MLLMs%2520can%2520effectively%2520recognize%2520VPs%252C%2520an%2520intuitive%2520prompting%2520method%2520for%2520humans%252C%2520and%2520use%2520them%2520to%2520solve%2520problems.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520VP-Bench%252C%2520a%2520benchmark%2520for%2520assessing%2520MLLMs%2527%2520capability%2520in%2520VP%2520perception%2520and%2520utilization.%2520VP-Bench%2520employs%2520a%2520two-stage%2520evaluation%2520framework%253A%2520Stage%25201%2520examines%2520models%2527%2520ability%2520to%2520perceive%2520VPs%2520in%2520natural%2520scenes%252C%2520using%252030k%2520visualized%2520prompts%2520spanning%2520eight%2520shapes%2520and%2520355%2520attribute%2520combinations.%2520Stage%25202%2520investigates%2520the%2520impact%2520of%2520VPs%2520on%2520downstream%2520tasks%252C%2520measuring%2520their%2520effectiveness%2520in%2520real-world%2520problem-solving%2520scenarios.%2520Using%2520VP-Bench%252C%2520we%2520evaluate%252028%2520MLLMs%252C%2520including%2520proprietary%2520systems%2520%2528e.g.%252C%2520GPT-4o%2529%2520and%2520open-source%2520models%2520%2528e.g.%252C%2520InternVL3%2520and%2520Qwen2.5-VL%2529%252C%2520and%2520provide%2520a%2520comprehensive%2520analysis%2520of%2520factors%2520that%2520affect%2520VP%2520understanding%252C%2520such%2520as%2520variations%2520in%2520VP%2520attributes%252C%2520question%2520arrangement%252C%2520and%2520model%2520scale.%2520VP-Bench%2520establishes%2520a%2520new%2520reference%2520framework%2520for%2520studying%2520how%2520MLLMs%2520comprehend%2520and%2520resolve%2520grounded%2520referring%2520questions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VP-Bench%3A%20A%20Comprehensive%20Benchmark%20for%20Visual%20Prompting%20in%20Multimodal%20Large%20Language%20Models&entry.906535625=Mingjie%20Xu%20and%20Jinpeng%20Chen%20and%20Yuzhi%20Zhao%20and%20Jason%20Chun%20Lok%20Li%20and%20Yue%20Qiu%20and%20Zekang%20Du%20and%20Mengyang%20Wu%20and%20Pingping%20Zhang%20and%20Kun%20Li%20and%20Hongzheng%20Yang%20and%20Wenao%20Ma%20and%20Jiaheng%20Wei%20and%20Qinbin%20Li%20and%20Kangcheng%20Liu%20and%20Wenqiang%20Lei&entry.1292438233=Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20enabled%20a%20wide%20range%20of%20advanced%20vision-language%20applications%2C%20including%20fine-grained%20object%20recognition%20and%20contextual%20understanding.%20When%20querying%20specific%20regions%20or%20objects%20in%20an%20image%2C%20human%20users%20naturally%20use%20%22visual%20prompts%22%20%28VPs%29%2C%20such%20as%20bounding%20boxes%2C%20to%20provide%20reference.%20However%2C%20no%20existing%20benchmark%20systematically%20evaluates%20the%20ability%20of%20MLLMs%20to%20interpret%20such%20VPs.%20This%20gap%20leaves%20it%20unclear%20whether%20current%20MLLMs%20can%20effectively%20recognize%20VPs%2C%20an%20intuitive%20prompting%20method%20for%20humans%2C%20and%20use%20them%20to%20solve%20problems.%20To%20address%20this%20limitation%2C%20we%20introduce%20VP-Bench%2C%20a%20benchmark%20for%20assessing%20MLLMs%27%20capability%20in%20VP%20perception%20and%20utilization.%20VP-Bench%20employs%20a%20two-stage%20evaluation%20framework%3A%20Stage%201%20examines%20models%27%20ability%20to%20perceive%20VPs%20in%20natural%20scenes%2C%20using%2030k%20visualized%20prompts%20spanning%20eight%20shapes%20and%20355%20attribute%20combinations.%20Stage%202%20investigates%20the%20impact%20of%20VPs%20on%20downstream%20tasks%2C%20measuring%20their%20effectiveness%20in%20real-world%20problem-solving%20scenarios.%20Using%20VP-Bench%2C%20we%20evaluate%2028%20MLLMs%2C%20including%20proprietary%20systems%20%28e.g.%2C%20GPT-4o%29%20and%20open-source%20models%20%28e.g.%2C%20InternVL3%20and%20Qwen2.5-VL%29%2C%20and%20provide%20a%20comprehensive%20analysis%20of%20factors%20that%20affect%20VP%20understanding%2C%20such%20as%20variations%20in%20VP%20attributes%2C%20question%20arrangement%2C%20and%20model%20scale.%20VP-Bench%20establishes%20a%20new%20reference%20framework%20for%20studying%20how%20MLLMs%20comprehend%20and%20resolve%20grounded%20referring%20questions.&entry.1838667208=http%3A//arxiv.org/abs/2511.11438v1&entry.124074799=Read"},
{"title": "DoReMi: A Domain-Representation Mixture Framework for Generalizable 3D Understanding", "author": "Mingwei Xing and Xinliang Wang and Yifeng Shi", "abstract": "The generalization of 3D deep learning across multiple domains remains limited by the limited scale of existing datasets and the high heterogeneity of multi-source point clouds. Point clouds collected from different sensors (e.g., LiDAR scans and mesh-derived point clouds) exhibit substantial discrepancies in density and noise distribution, resulting in negative transfer during multi-domain fusion. Most existing approaches focus exclusively on either domain-aware or domain-general features, overlooking the potential synergy between them. To address this, we propose DoReMi (Domain-Representation Mixture), a Mixture-of-Experts (MoE) framework that jointly models Domain-aware Experts branch and a unified Representation branch to enable cooperative learning between specialized and generalizable knowledge. DoReMi dynamically activates domain-aware expert branch via Domain-Guided Spatial Routing (DSR) for context-aware expert selection and employs Entropy-Controlled Dynamic Allocation (EDA) for stable and efficient expert utilization, thereby adaptively modeling diverse domain distributions. Complemented by a frozen unified representation branch pretrained through robust multi-attribute self-supervised learning, DoReMi preserves cross-domain geometric and structural priors while maintaining global consistency. We evaluate DoReMi across multiple 3D understanding benchmarks. Notably, DoReMi achieves 80.1% mIoU on ScanNet Val and 77.2% mIoU on S3DIS, demonstrating competitive or superior performance compared to existing approaches, and showing strong potential as a foundation framework for future 3D understanding research. The code will be released soon.", "link": "http://arxiv.org/abs/2511.11232v1", "date": "2025-11-14", "relevancy": 2.3975, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6052}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6022}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DoReMi%3A%20A%20Domain-Representation%20Mixture%20Framework%20for%20Generalizable%203D%20Understanding&body=Title%3A%20DoReMi%3A%20A%20Domain-Representation%20Mixture%20Framework%20for%20Generalizable%203D%20Understanding%0AAuthor%3A%20Mingwei%20Xing%20and%20Xinliang%20Wang%20and%20Yifeng%20Shi%0AAbstract%3A%20The%20generalization%20of%203D%20deep%20learning%20across%20multiple%20domains%20remains%20limited%20by%20the%20limited%20scale%20of%20existing%20datasets%20and%20the%20high%20heterogeneity%20of%20multi-source%20point%20clouds.%20Point%20clouds%20collected%20from%20different%20sensors%20%28e.g.%2C%20LiDAR%20scans%20and%20mesh-derived%20point%20clouds%29%20exhibit%20substantial%20discrepancies%20in%20density%20and%20noise%20distribution%2C%20resulting%20in%20negative%20transfer%20during%20multi-domain%20fusion.%20Most%20existing%20approaches%20focus%20exclusively%20on%20either%20domain-aware%20or%20domain-general%20features%2C%20overlooking%20the%20potential%20synergy%20between%20them.%20To%20address%20this%2C%20we%20propose%20DoReMi%20%28Domain-Representation%20Mixture%29%2C%20a%20Mixture-of-Experts%20%28MoE%29%20framework%20that%20jointly%20models%20Domain-aware%20Experts%20branch%20and%20a%20unified%20Representation%20branch%20to%20enable%20cooperative%20learning%20between%20specialized%20and%20generalizable%20knowledge.%20DoReMi%20dynamically%20activates%20domain-aware%20expert%20branch%20via%20Domain-Guided%20Spatial%20Routing%20%28DSR%29%20for%20context-aware%20expert%20selection%20and%20employs%20Entropy-Controlled%20Dynamic%20Allocation%20%28EDA%29%20for%20stable%20and%20efficient%20expert%20utilization%2C%20thereby%20adaptively%20modeling%20diverse%20domain%20distributions.%20Complemented%20by%20a%20frozen%20unified%20representation%20branch%20pretrained%20through%20robust%20multi-attribute%20self-supervised%20learning%2C%20DoReMi%20preserves%20cross-domain%20geometric%20and%20structural%20priors%20while%20maintaining%20global%20consistency.%20We%20evaluate%20DoReMi%20across%20multiple%203D%20understanding%20benchmarks.%20Notably%2C%20DoReMi%20achieves%2080.1%25%20mIoU%20on%20ScanNet%20Val%20and%2077.2%25%20mIoU%20on%20S3DIS%2C%20demonstrating%20competitive%20or%20superior%20performance%20compared%20to%20existing%20approaches%2C%20and%20showing%20strong%20potential%20as%20a%20foundation%20framework%20for%20future%203D%20understanding%20research.%20The%20code%20will%20be%20released%20soon.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoReMi%253A%2520A%2520Domain-Representation%2520Mixture%2520Framework%2520for%2520Generalizable%25203D%2520Understanding%26entry.906535625%3DMingwei%2520Xing%2520and%2520Xinliang%2520Wang%2520and%2520Yifeng%2520Shi%26entry.1292438233%3DThe%2520generalization%2520of%25203D%2520deep%2520learning%2520across%2520multiple%2520domains%2520remains%2520limited%2520by%2520the%2520limited%2520scale%2520of%2520existing%2520datasets%2520and%2520the%2520high%2520heterogeneity%2520of%2520multi-source%2520point%2520clouds.%2520Point%2520clouds%2520collected%2520from%2520different%2520sensors%2520%2528e.g.%252C%2520LiDAR%2520scans%2520and%2520mesh-derived%2520point%2520clouds%2529%2520exhibit%2520substantial%2520discrepancies%2520in%2520density%2520and%2520noise%2520distribution%252C%2520resulting%2520in%2520negative%2520transfer%2520during%2520multi-domain%2520fusion.%2520Most%2520existing%2520approaches%2520focus%2520exclusively%2520on%2520either%2520domain-aware%2520or%2520domain-general%2520features%252C%2520overlooking%2520the%2520potential%2520synergy%2520between%2520them.%2520To%2520address%2520this%252C%2520we%2520propose%2520DoReMi%2520%2528Domain-Representation%2520Mixture%2529%252C%2520a%2520Mixture-of-Experts%2520%2528MoE%2529%2520framework%2520that%2520jointly%2520models%2520Domain-aware%2520Experts%2520branch%2520and%2520a%2520unified%2520Representation%2520branch%2520to%2520enable%2520cooperative%2520learning%2520between%2520specialized%2520and%2520generalizable%2520knowledge.%2520DoReMi%2520dynamically%2520activates%2520domain-aware%2520expert%2520branch%2520via%2520Domain-Guided%2520Spatial%2520Routing%2520%2528DSR%2529%2520for%2520context-aware%2520expert%2520selection%2520and%2520employs%2520Entropy-Controlled%2520Dynamic%2520Allocation%2520%2528EDA%2529%2520for%2520stable%2520and%2520efficient%2520expert%2520utilization%252C%2520thereby%2520adaptively%2520modeling%2520diverse%2520domain%2520distributions.%2520Complemented%2520by%2520a%2520frozen%2520unified%2520representation%2520branch%2520pretrained%2520through%2520robust%2520multi-attribute%2520self-supervised%2520learning%252C%2520DoReMi%2520preserves%2520cross-domain%2520geometric%2520and%2520structural%2520priors%2520while%2520maintaining%2520global%2520consistency.%2520We%2520evaluate%2520DoReMi%2520across%2520multiple%25203D%2520understanding%2520benchmarks.%2520Notably%252C%2520DoReMi%2520achieves%252080.1%2525%2520mIoU%2520on%2520ScanNet%2520Val%2520and%252077.2%2525%2520mIoU%2520on%2520S3DIS%252C%2520demonstrating%2520competitive%2520or%2520superior%2520performance%2520compared%2520to%2520existing%2520approaches%252C%2520and%2520showing%2520strong%2520potential%2520as%2520a%2520foundation%2520framework%2520for%2520future%25203D%2520understanding%2520research.%2520The%2520code%2520will%2520be%2520released%2520soon.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DoReMi%3A%20A%20Domain-Representation%20Mixture%20Framework%20for%20Generalizable%203D%20Understanding&entry.906535625=Mingwei%20Xing%20and%20Xinliang%20Wang%20and%20Yifeng%20Shi&entry.1292438233=The%20generalization%20of%203D%20deep%20learning%20across%20multiple%20domains%20remains%20limited%20by%20the%20limited%20scale%20of%20existing%20datasets%20and%20the%20high%20heterogeneity%20of%20multi-source%20point%20clouds.%20Point%20clouds%20collected%20from%20different%20sensors%20%28e.g.%2C%20LiDAR%20scans%20and%20mesh-derived%20point%20clouds%29%20exhibit%20substantial%20discrepancies%20in%20density%20and%20noise%20distribution%2C%20resulting%20in%20negative%20transfer%20during%20multi-domain%20fusion.%20Most%20existing%20approaches%20focus%20exclusively%20on%20either%20domain-aware%20or%20domain-general%20features%2C%20overlooking%20the%20potential%20synergy%20between%20them.%20To%20address%20this%2C%20we%20propose%20DoReMi%20%28Domain-Representation%20Mixture%29%2C%20a%20Mixture-of-Experts%20%28MoE%29%20framework%20that%20jointly%20models%20Domain-aware%20Experts%20branch%20and%20a%20unified%20Representation%20branch%20to%20enable%20cooperative%20learning%20between%20specialized%20and%20generalizable%20knowledge.%20DoReMi%20dynamically%20activates%20domain-aware%20expert%20branch%20via%20Domain-Guided%20Spatial%20Routing%20%28DSR%29%20for%20context-aware%20expert%20selection%20and%20employs%20Entropy-Controlled%20Dynamic%20Allocation%20%28EDA%29%20for%20stable%20and%20efficient%20expert%20utilization%2C%20thereby%20adaptively%20modeling%20diverse%20domain%20distributions.%20Complemented%20by%20a%20frozen%20unified%20representation%20branch%20pretrained%20through%20robust%20multi-attribute%20self-supervised%20learning%2C%20DoReMi%20preserves%20cross-domain%20geometric%20and%20structural%20priors%20while%20maintaining%20global%20consistency.%20We%20evaluate%20DoReMi%20across%20multiple%203D%20understanding%20benchmarks.%20Notably%2C%20DoReMi%20achieves%2080.1%25%20mIoU%20on%20ScanNet%20Val%20and%2077.2%25%20mIoU%20on%20S3DIS%2C%20demonstrating%20competitive%20or%20superior%20performance%20compared%20to%20existing%20approaches%2C%20and%20showing%20strong%20potential%20as%20a%20foundation%20framework%20for%20future%203D%20understanding%20research.%20The%20code%20will%20be%20released%20soon.&entry.1838667208=http%3A//arxiv.org/abs/2511.11232v1&entry.124074799=Read"},
{"title": "RodEpil: A Video Dataset of Laboratory Rodents for Seizure Detection and Benchmark Evaluation", "author": "Daniele Perlo and Vladimir Despotovic and Selma Boudissa and Sang-Yoon Kim and Petr V. Nazarov and Yanrong Zhang and Max Wintermark and Olivier Keunen", "abstract": "We introduce a curated video dataset of laboratory rodents for automatic detection of convulsive events. The dataset contains short (10~s) top-down and side-view video clips of individual rodents, labeled at clip level as normal activity or seizure. It includes 10,101 negative samples and 2,952 positive samples collected from 19 subjects. We describe the data curation, annotation protocol and preprocessing pipeline, and report baseline experiments using a transformer-based video classifier (TimeSformer). Experiments employ five-fold cross-validation with strict subject-wise partitioning to prevent data leakage (no subject appears in more than one fold). Results show that the TimeSformer architecture enables discrimination between seizure and normal activity with an average F1-score of 97%. The dataset and baseline code are publicly released to support reproducible research on non-invasive, video-based monitoring in preclinical epilepsy research. RodEpil Dataset access - DOI: 10.5281/zenodo.17601357", "link": "http://arxiv.org/abs/2511.10431v2", "date": "2025-11-14", "relevancy": 2.3896, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5011}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.482}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RodEpil%3A%20A%20Video%20Dataset%20of%20Laboratory%20Rodents%20for%20Seizure%20Detection%20and%20Benchmark%20Evaluation&body=Title%3A%20RodEpil%3A%20A%20Video%20Dataset%20of%20Laboratory%20Rodents%20for%20Seizure%20Detection%20and%20Benchmark%20Evaluation%0AAuthor%3A%20Daniele%20Perlo%20and%20Vladimir%20Despotovic%20and%20Selma%20Boudissa%20and%20Sang-Yoon%20Kim%20and%20Petr%20V.%20Nazarov%20and%20Yanrong%20Zhang%20and%20Max%20Wintermark%20and%20Olivier%20Keunen%0AAbstract%3A%20We%20introduce%20a%20curated%20video%20dataset%20of%20laboratory%20rodents%20for%20automatic%20detection%20of%20convulsive%20events.%20The%20dataset%20contains%20short%20%2810~s%29%20top-down%20and%20side-view%20video%20clips%20of%20individual%20rodents%2C%20labeled%20at%20clip%20level%20as%20normal%20activity%20or%20seizure.%20It%20includes%2010%2C101%20negative%20samples%20and%202%2C952%20positive%20samples%20collected%20from%2019%20subjects.%20We%20describe%20the%20data%20curation%2C%20annotation%20protocol%20and%20preprocessing%20pipeline%2C%20and%20report%20baseline%20experiments%20using%20a%20transformer-based%20video%20classifier%20%28TimeSformer%29.%20Experiments%20employ%20five-fold%20cross-validation%20with%20strict%20subject-wise%20partitioning%20to%20prevent%20data%20leakage%20%28no%20subject%20appears%20in%20more%20than%20one%20fold%29.%20Results%20show%20that%20the%20TimeSformer%20architecture%20enables%20discrimination%20between%20seizure%20and%20normal%20activity%20with%20an%20average%20F1-score%20of%2097%25.%20The%20dataset%20and%20baseline%20code%20are%20publicly%20released%20to%20support%20reproducible%20research%20on%20non-invasive%2C%20video-based%20monitoring%20in%20preclinical%20epilepsy%20research.%20RodEpil%20Dataset%20access%20-%20DOI%3A%2010.5281/zenodo.17601357%0ALink%3A%20http%3A//arxiv.org/abs/2511.10431v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRodEpil%253A%2520A%2520Video%2520Dataset%2520of%2520Laboratory%2520Rodents%2520for%2520Seizure%2520Detection%2520and%2520Benchmark%2520Evaluation%26entry.906535625%3DDaniele%2520Perlo%2520and%2520Vladimir%2520Despotovic%2520and%2520Selma%2520Boudissa%2520and%2520Sang-Yoon%2520Kim%2520and%2520Petr%2520V.%2520Nazarov%2520and%2520Yanrong%2520Zhang%2520and%2520Max%2520Wintermark%2520and%2520Olivier%2520Keunen%26entry.1292438233%3DWe%2520introduce%2520a%2520curated%2520video%2520dataset%2520of%2520laboratory%2520rodents%2520for%2520automatic%2520detection%2520of%2520convulsive%2520events.%2520The%2520dataset%2520contains%2520short%2520%252810~s%2529%2520top-down%2520and%2520side-view%2520video%2520clips%2520of%2520individual%2520rodents%252C%2520labeled%2520at%2520clip%2520level%2520as%2520normal%2520activity%2520or%2520seizure.%2520It%2520includes%252010%252C101%2520negative%2520samples%2520and%25202%252C952%2520positive%2520samples%2520collected%2520from%252019%2520subjects.%2520We%2520describe%2520the%2520data%2520curation%252C%2520annotation%2520protocol%2520and%2520preprocessing%2520pipeline%252C%2520and%2520report%2520baseline%2520experiments%2520using%2520a%2520transformer-based%2520video%2520classifier%2520%2528TimeSformer%2529.%2520Experiments%2520employ%2520five-fold%2520cross-validation%2520with%2520strict%2520subject-wise%2520partitioning%2520to%2520prevent%2520data%2520leakage%2520%2528no%2520subject%2520appears%2520in%2520more%2520than%2520one%2520fold%2529.%2520Results%2520show%2520that%2520the%2520TimeSformer%2520architecture%2520enables%2520discrimination%2520between%2520seizure%2520and%2520normal%2520activity%2520with%2520an%2520average%2520F1-score%2520of%252097%2525.%2520The%2520dataset%2520and%2520baseline%2520code%2520are%2520publicly%2520released%2520to%2520support%2520reproducible%2520research%2520on%2520non-invasive%252C%2520video-based%2520monitoring%2520in%2520preclinical%2520epilepsy%2520research.%2520RodEpil%2520Dataset%2520access%2520-%2520DOI%253A%252010.5281/zenodo.17601357%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10431v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RodEpil%3A%20A%20Video%20Dataset%20of%20Laboratory%20Rodents%20for%20Seizure%20Detection%20and%20Benchmark%20Evaluation&entry.906535625=Daniele%20Perlo%20and%20Vladimir%20Despotovic%20and%20Selma%20Boudissa%20and%20Sang-Yoon%20Kim%20and%20Petr%20V.%20Nazarov%20and%20Yanrong%20Zhang%20and%20Max%20Wintermark%20and%20Olivier%20Keunen&entry.1292438233=We%20introduce%20a%20curated%20video%20dataset%20of%20laboratory%20rodents%20for%20automatic%20detection%20of%20convulsive%20events.%20The%20dataset%20contains%20short%20%2810~s%29%20top-down%20and%20side-view%20video%20clips%20of%20individual%20rodents%2C%20labeled%20at%20clip%20level%20as%20normal%20activity%20or%20seizure.%20It%20includes%2010%2C101%20negative%20samples%20and%202%2C952%20positive%20samples%20collected%20from%2019%20subjects.%20We%20describe%20the%20data%20curation%2C%20annotation%20protocol%20and%20preprocessing%20pipeline%2C%20and%20report%20baseline%20experiments%20using%20a%20transformer-based%20video%20classifier%20%28TimeSformer%29.%20Experiments%20employ%20five-fold%20cross-validation%20with%20strict%20subject-wise%20partitioning%20to%20prevent%20data%20leakage%20%28no%20subject%20appears%20in%20more%20than%20one%20fold%29.%20Results%20show%20that%20the%20TimeSformer%20architecture%20enables%20discrimination%20between%20seizure%20and%20normal%20activity%20with%20an%20average%20F1-score%20of%2097%25.%20The%20dataset%20and%20baseline%20code%20are%20publicly%20released%20to%20support%20reproducible%20research%20on%20non-invasive%2C%20video-based%20monitoring%20in%20preclinical%20epilepsy%20research.%20RodEpil%20Dataset%20access%20-%20DOI%3A%2010.5281/zenodo.17601357&entry.1838667208=http%3A//arxiv.org/abs/2511.10431v2&entry.124074799=Read"},
{"title": "Towards Non-Stationary Time Series Forecasting with Temporal Stabilization and Frequency Differencing", "author": "Junkai Lu and Peng Chen and Chenjuan Guo and Yang Shu and Meng Wang and Bin Yang", "abstract": "Time series forecasting is critical for decision-making across dynamic domains such as energy, finance, transportation, and cloud computing. However, real-world time series often exhibit non-stationarity, including temporal distribution shifts and spectral variability, which pose significant challenges for long-term time series forecasting. In this paper, we propose DTAF, a dual-branch framework that addresses non-stationarity in both the temporal and frequency domains. For the temporal domain, the Temporal Stabilizing Fusion (TFS) module employs a non-stationary mix of experts (MOE) filter to disentangle and suppress temporal non-stationary patterns while preserving long-term dependencies. For the frequency domain, the Frequency Wave Modeling (FWM) module applies frequency differencing to dynamically highlight components with significant spectral shifts. By fusing the complementary outputs of TFS and FWM, DTAF generates robust forecasts that adapt to both temporal and frequency domain non-stationarity. Extensive experiments on real-world benchmarks demonstrate that DTAF outperforms state-of-the-art baselines, yielding significant improvements in forecasting accuracy under non-stationary conditions. All codes are available at https://github.com/PandaJunk/DTAF.", "link": "http://arxiv.org/abs/2511.08229v4", "date": "2025-11-14", "relevancy": 2.3888, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.505}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4693}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Non-Stationary%20Time%20Series%20Forecasting%20with%20Temporal%20Stabilization%20and%20Frequency%20Differencing&body=Title%3A%20Towards%20Non-Stationary%20Time%20Series%20Forecasting%20with%20Temporal%20Stabilization%20and%20Frequency%20Differencing%0AAuthor%3A%20Junkai%20Lu%20and%20Peng%20Chen%20and%20Chenjuan%20Guo%20and%20Yang%20Shu%20and%20Meng%20Wang%20and%20Bin%20Yang%0AAbstract%3A%20Time%20series%20forecasting%20is%20critical%20for%20decision-making%20across%20dynamic%20domains%20such%20as%20energy%2C%20finance%2C%20transportation%2C%20and%20cloud%20computing.%20However%2C%20real-world%20time%20series%20often%20exhibit%20non-stationarity%2C%20including%20temporal%20distribution%20shifts%20and%20spectral%20variability%2C%20which%20pose%20significant%20challenges%20for%20long-term%20time%20series%20forecasting.%20In%20this%20paper%2C%20we%20propose%20DTAF%2C%20a%20dual-branch%20framework%20that%20addresses%20non-stationarity%20in%20both%20the%20temporal%20and%20frequency%20domains.%20For%20the%20temporal%20domain%2C%20the%20Temporal%20Stabilizing%20Fusion%20%28TFS%29%20module%20employs%20a%20non-stationary%20mix%20of%20experts%20%28MOE%29%20filter%20to%20disentangle%20and%20suppress%20temporal%20non-stationary%20patterns%20while%20preserving%20long-term%20dependencies.%20For%20the%20frequency%20domain%2C%20the%20Frequency%20Wave%20Modeling%20%28FWM%29%20module%20applies%20frequency%20differencing%20to%20dynamically%20highlight%20components%20with%20significant%20spectral%20shifts.%20By%20fusing%20the%20complementary%20outputs%20of%20TFS%20and%20FWM%2C%20DTAF%20generates%20robust%20forecasts%20that%20adapt%20to%20both%20temporal%20and%20frequency%20domain%20non-stationarity.%20Extensive%20experiments%20on%20real-world%20benchmarks%20demonstrate%20that%20DTAF%20outperforms%20state-of-the-art%20baselines%2C%20yielding%20significant%20improvements%20in%20forecasting%20accuracy%20under%20non-stationary%20conditions.%20All%20codes%20are%20available%20at%20https%3A//github.com/PandaJunk/DTAF.%0ALink%3A%20http%3A//arxiv.org/abs/2511.08229v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Non-Stationary%2520Time%2520Series%2520Forecasting%2520with%2520Temporal%2520Stabilization%2520and%2520Frequency%2520Differencing%26entry.906535625%3DJunkai%2520Lu%2520and%2520Peng%2520Chen%2520and%2520Chenjuan%2520Guo%2520and%2520Yang%2520Shu%2520and%2520Meng%2520Wang%2520and%2520Bin%2520Yang%26entry.1292438233%3DTime%2520series%2520forecasting%2520is%2520critical%2520for%2520decision-making%2520across%2520dynamic%2520domains%2520such%2520as%2520energy%252C%2520finance%252C%2520transportation%252C%2520and%2520cloud%2520computing.%2520However%252C%2520real-world%2520time%2520series%2520often%2520exhibit%2520non-stationarity%252C%2520including%2520temporal%2520distribution%2520shifts%2520and%2520spectral%2520variability%252C%2520which%2520pose%2520significant%2520challenges%2520for%2520long-term%2520time%2520series%2520forecasting.%2520In%2520this%2520paper%252C%2520we%2520propose%2520DTAF%252C%2520a%2520dual-branch%2520framework%2520that%2520addresses%2520non-stationarity%2520in%2520both%2520the%2520temporal%2520and%2520frequency%2520domains.%2520For%2520the%2520temporal%2520domain%252C%2520the%2520Temporal%2520Stabilizing%2520Fusion%2520%2528TFS%2529%2520module%2520employs%2520a%2520non-stationary%2520mix%2520of%2520experts%2520%2528MOE%2529%2520filter%2520to%2520disentangle%2520and%2520suppress%2520temporal%2520non-stationary%2520patterns%2520while%2520preserving%2520long-term%2520dependencies.%2520For%2520the%2520frequency%2520domain%252C%2520the%2520Frequency%2520Wave%2520Modeling%2520%2528FWM%2529%2520module%2520applies%2520frequency%2520differencing%2520to%2520dynamically%2520highlight%2520components%2520with%2520significant%2520spectral%2520shifts.%2520By%2520fusing%2520the%2520complementary%2520outputs%2520of%2520TFS%2520and%2520FWM%252C%2520DTAF%2520generates%2520robust%2520forecasts%2520that%2520adapt%2520to%2520both%2520temporal%2520and%2520frequency%2520domain%2520non-stationarity.%2520Extensive%2520experiments%2520on%2520real-world%2520benchmarks%2520demonstrate%2520that%2520DTAF%2520outperforms%2520state-of-the-art%2520baselines%252C%2520yielding%2520significant%2520improvements%2520in%2520forecasting%2520accuracy%2520under%2520non-stationary%2520conditions.%2520All%2520codes%2520are%2520available%2520at%2520https%253A//github.com/PandaJunk/DTAF.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.08229v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Non-Stationary%20Time%20Series%20Forecasting%20with%20Temporal%20Stabilization%20and%20Frequency%20Differencing&entry.906535625=Junkai%20Lu%20and%20Peng%20Chen%20and%20Chenjuan%20Guo%20and%20Yang%20Shu%20and%20Meng%20Wang%20and%20Bin%20Yang&entry.1292438233=Time%20series%20forecasting%20is%20critical%20for%20decision-making%20across%20dynamic%20domains%20such%20as%20energy%2C%20finance%2C%20transportation%2C%20and%20cloud%20computing.%20However%2C%20real-world%20time%20series%20often%20exhibit%20non-stationarity%2C%20including%20temporal%20distribution%20shifts%20and%20spectral%20variability%2C%20which%20pose%20significant%20challenges%20for%20long-term%20time%20series%20forecasting.%20In%20this%20paper%2C%20we%20propose%20DTAF%2C%20a%20dual-branch%20framework%20that%20addresses%20non-stationarity%20in%20both%20the%20temporal%20and%20frequency%20domains.%20For%20the%20temporal%20domain%2C%20the%20Temporal%20Stabilizing%20Fusion%20%28TFS%29%20module%20employs%20a%20non-stationary%20mix%20of%20experts%20%28MOE%29%20filter%20to%20disentangle%20and%20suppress%20temporal%20non-stationary%20patterns%20while%20preserving%20long-term%20dependencies.%20For%20the%20frequency%20domain%2C%20the%20Frequency%20Wave%20Modeling%20%28FWM%29%20module%20applies%20frequency%20differencing%20to%20dynamically%20highlight%20components%20with%20significant%20spectral%20shifts.%20By%20fusing%20the%20complementary%20outputs%20of%20TFS%20and%20FWM%2C%20DTAF%20generates%20robust%20forecasts%20that%20adapt%20to%20both%20temporal%20and%20frequency%20domain%20non-stationarity.%20Extensive%20experiments%20on%20real-world%20benchmarks%20demonstrate%20that%20DTAF%20outperforms%20state-of-the-art%20baselines%2C%20yielding%20significant%20improvements%20in%20forecasting%20accuracy%20under%20non-stationary%20conditions.%20All%20codes%20are%20available%20at%20https%3A//github.com/PandaJunk/DTAF.&entry.1838667208=http%3A//arxiv.org/abs/2511.08229v4&entry.124074799=Read"},
{"title": "Medverse: A Universal Model for Full-Resolution 3D Medical Image Segmentation, Transformation and Enhancement", "author": "Jiesi Hu and Jianfeng Cao and Yanwu Yang and Chenfei Ye and Yixuan Zhang and Hanyang Peng and Ting Ma", "abstract": "In-context learning (ICL) offers a promising paradigm for universal medical image analysis, enabling models to perform diverse image processing tasks without retraining. However, current ICL models for medical imaging remain limited in two critical aspects: they cannot simultaneously achieve high-fidelity predictions and global anatomical understanding, and there is no unified model trained across diverse medical imaging tasks (e.g., segmentation and enhancement) and anatomical regions. As a result, the full potential of ICL in medical imaging remains underexplored. Thus, we present \\textbf{Medverse}, a universal ICL model for 3D medical imaging, trained on 22 datasets covering diverse tasks in universal image segmentation, transformation, and enhancement across multiple organs, imaging modalities, and clinical centers. Medverse employs a next-scale autoregressive in-context learning framework that progressively refines predictions from coarse to fine, generating consistent, full-resolution volumetric outputs and enabling multi-scale anatomical awareness. We further propose a blockwise cross-attention module that facilitates long-range interactions between context and target inputs while preserving computational efficiency through spatial sparsity. Medverse is extensively evaluated on a broad collection of held-out datasets covering previously unseen clinical centers, organs, species, and imaging modalities. Results demonstrate that Medverse substantially outperforms existing ICL baselines and establishes a novel paradigm for in-context learning. Code and model weights will be made publicly available. Our model are publicly available at https://github.com/jiesihu/Medverse.", "link": "http://arxiv.org/abs/2509.09232v2", "date": "2025-11-14", "relevancy": 2.3825, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6091}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5929}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Medverse%3A%20A%20Universal%20Model%20for%20Full-Resolution%203D%20Medical%20Image%20Segmentation%2C%20Transformation%20and%20Enhancement&body=Title%3A%20Medverse%3A%20A%20Universal%20Model%20for%20Full-Resolution%203D%20Medical%20Image%20Segmentation%2C%20Transformation%20and%20Enhancement%0AAuthor%3A%20Jiesi%20Hu%20and%20Jianfeng%20Cao%20and%20Yanwu%20Yang%20and%20Chenfei%20Ye%20and%20Yixuan%20Zhang%20and%20Hanyang%20Peng%20and%20Ting%20Ma%0AAbstract%3A%20In-context%20learning%20%28ICL%29%20offers%20a%20promising%20paradigm%20for%20universal%20medical%20image%20analysis%2C%20enabling%20models%20to%20perform%20diverse%20image%20processing%20tasks%20without%20retraining.%20However%2C%20current%20ICL%20models%20for%20medical%20imaging%20remain%20limited%20in%20two%20critical%20aspects%3A%20they%20cannot%20simultaneously%20achieve%20high-fidelity%20predictions%20and%20global%20anatomical%20understanding%2C%20and%20there%20is%20no%20unified%20model%20trained%20across%20diverse%20medical%20imaging%20tasks%20%28e.g.%2C%20segmentation%20and%20enhancement%29%20and%20anatomical%20regions.%20As%20a%20result%2C%20the%20full%20potential%20of%20ICL%20in%20medical%20imaging%20remains%20underexplored.%20Thus%2C%20we%20present%20%5Ctextbf%7BMedverse%7D%2C%20a%20universal%20ICL%20model%20for%203D%20medical%20imaging%2C%20trained%20on%2022%20datasets%20covering%20diverse%20tasks%20in%20universal%20image%20segmentation%2C%20transformation%2C%20and%20enhancement%20across%20multiple%20organs%2C%20imaging%20modalities%2C%20and%20clinical%20centers.%20Medverse%20employs%20a%20next-scale%20autoregressive%20in-context%20learning%20framework%20that%20progressively%20refines%20predictions%20from%20coarse%20to%20fine%2C%20generating%20consistent%2C%20full-resolution%20volumetric%20outputs%20and%20enabling%20multi-scale%20anatomical%20awareness.%20We%20further%20propose%20a%20blockwise%20cross-attention%20module%20that%20facilitates%20long-range%20interactions%20between%20context%20and%20target%20inputs%20while%20preserving%20computational%20efficiency%20through%20spatial%20sparsity.%20Medverse%20is%20extensively%20evaluated%20on%20a%20broad%20collection%20of%20held-out%20datasets%20covering%20previously%20unseen%20clinical%20centers%2C%20organs%2C%20species%2C%20and%20imaging%20modalities.%20Results%20demonstrate%20that%20Medverse%20substantially%20outperforms%20existing%20ICL%20baselines%20and%20establishes%20a%20novel%20paradigm%20for%20in-context%20learning.%20Code%20and%20model%20weights%20will%20be%20made%20publicly%20available.%20Our%20model%20are%20publicly%20available%20at%20https%3A//github.com/jiesihu/Medverse.%0ALink%3A%20http%3A//arxiv.org/abs/2509.09232v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedverse%253A%2520A%2520Universal%2520Model%2520for%2520Full-Resolution%25203D%2520Medical%2520Image%2520Segmentation%252C%2520Transformation%2520and%2520Enhancement%26entry.906535625%3DJiesi%2520Hu%2520and%2520Jianfeng%2520Cao%2520and%2520Yanwu%2520Yang%2520and%2520Chenfei%2520Ye%2520and%2520Yixuan%2520Zhang%2520and%2520Hanyang%2520Peng%2520and%2520Ting%2520Ma%26entry.1292438233%3DIn-context%2520learning%2520%2528ICL%2529%2520offers%2520a%2520promising%2520paradigm%2520for%2520universal%2520medical%2520image%2520analysis%252C%2520enabling%2520models%2520to%2520perform%2520diverse%2520image%2520processing%2520tasks%2520without%2520retraining.%2520However%252C%2520current%2520ICL%2520models%2520for%2520medical%2520imaging%2520remain%2520limited%2520in%2520two%2520critical%2520aspects%253A%2520they%2520cannot%2520simultaneously%2520achieve%2520high-fidelity%2520predictions%2520and%2520global%2520anatomical%2520understanding%252C%2520and%2520there%2520is%2520no%2520unified%2520model%2520trained%2520across%2520diverse%2520medical%2520imaging%2520tasks%2520%2528e.g.%252C%2520segmentation%2520and%2520enhancement%2529%2520and%2520anatomical%2520regions.%2520As%2520a%2520result%252C%2520the%2520full%2520potential%2520of%2520ICL%2520in%2520medical%2520imaging%2520remains%2520underexplored.%2520Thus%252C%2520we%2520present%2520%255Ctextbf%257BMedverse%257D%252C%2520a%2520universal%2520ICL%2520model%2520for%25203D%2520medical%2520imaging%252C%2520trained%2520on%252022%2520datasets%2520covering%2520diverse%2520tasks%2520in%2520universal%2520image%2520segmentation%252C%2520transformation%252C%2520and%2520enhancement%2520across%2520multiple%2520organs%252C%2520imaging%2520modalities%252C%2520and%2520clinical%2520centers.%2520Medverse%2520employs%2520a%2520next-scale%2520autoregressive%2520in-context%2520learning%2520framework%2520that%2520progressively%2520refines%2520predictions%2520from%2520coarse%2520to%2520fine%252C%2520generating%2520consistent%252C%2520full-resolution%2520volumetric%2520outputs%2520and%2520enabling%2520multi-scale%2520anatomical%2520awareness.%2520We%2520further%2520propose%2520a%2520blockwise%2520cross-attention%2520module%2520that%2520facilitates%2520long-range%2520interactions%2520between%2520context%2520and%2520target%2520inputs%2520while%2520preserving%2520computational%2520efficiency%2520through%2520spatial%2520sparsity.%2520Medverse%2520is%2520extensively%2520evaluated%2520on%2520a%2520broad%2520collection%2520of%2520held-out%2520datasets%2520covering%2520previously%2520unseen%2520clinical%2520centers%252C%2520organs%252C%2520species%252C%2520and%2520imaging%2520modalities.%2520Results%2520demonstrate%2520that%2520Medverse%2520substantially%2520outperforms%2520existing%2520ICL%2520baselines%2520and%2520establishes%2520a%2520novel%2520paradigm%2520for%2520in-context%2520learning.%2520Code%2520and%2520model%2520weights%2520will%2520be%2520made%2520publicly%2520available.%2520Our%2520model%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/jiesihu/Medverse.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.09232v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Medverse%3A%20A%20Universal%20Model%20for%20Full-Resolution%203D%20Medical%20Image%20Segmentation%2C%20Transformation%20and%20Enhancement&entry.906535625=Jiesi%20Hu%20and%20Jianfeng%20Cao%20and%20Yanwu%20Yang%20and%20Chenfei%20Ye%20and%20Yixuan%20Zhang%20and%20Hanyang%20Peng%20and%20Ting%20Ma&entry.1292438233=In-context%20learning%20%28ICL%29%20offers%20a%20promising%20paradigm%20for%20universal%20medical%20image%20analysis%2C%20enabling%20models%20to%20perform%20diverse%20image%20processing%20tasks%20without%20retraining.%20However%2C%20current%20ICL%20models%20for%20medical%20imaging%20remain%20limited%20in%20two%20critical%20aspects%3A%20they%20cannot%20simultaneously%20achieve%20high-fidelity%20predictions%20and%20global%20anatomical%20understanding%2C%20and%20there%20is%20no%20unified%20model%20trained%20across%20diverse%20medical%20imaging%20tasks%20%28e.g.%2C%20segmentation%20and%20enhancement%29%20and%20anatomical%20regions.%20As%20a%20result%2C%20the%20full%20potential%20of%20ICL%20in%20medical%20imaging%20remains%20underexplored.%20Thus%2C%20we%20present%20%5Ctextbf%7BMedverse%7D%2C%20a%20universal%20ICL%20model%20for%203D%20medical%20imaging%2C%20trained%20on%2022%20datasets%20covering%20diverse%20tasks%20in%20universal%20image%20segmentation%2C%20transformation%2C%20and%20enhancement%20across%20multiple%20organs%2C%20imaging%20modalities%2C%20and%20clinical%20centers.%20Medverse%20employs%20a%20next-scale%20autoregressive%20in-context%20learning%20framework%20that%20progressively%20refines%20predictions%20from%20coarse%20to%20fine%2C%20generating%20consistent%2C%20full-resolution%20volumetric%20outputs%20and%20enabling%20multi-scale%20anatomical%20awareness.%20We%20further%20propose%20a%20blockwise%20cross-attention%20module%20that%20facilitates%20long-range%20interactions%20between%20context%20and%20target%20inputs%20while%20preserving%20computational%20efficiency%20through%20spatial%20sparsity.%20Medverse%20is%20extensively%20evaluated%20on%20a%20broad%20collection%20of%20held-out%20datasets%20covering%20previously%20unseen%20clinical%20centers%2C%20organs%2C%20species%2C%20and%20imaging%20modalities.%20Results%20demonstrate%20that%20Medverse%20substantially%20outperforms%20existing%20ICL%20baselines%20and%20establishes%20a%20novel%20paradigm%20for%20in-context%20learning.%20Code%20and%20model%20weights%20will%20be%20made%20publicly%20available.%20Our%20model%20are%20publicly%20available%20at%20https%3A//github.com/jiesihu/Medverse.&entry.1838667208=http%3A//arxiv.org/abs/2509.09232v2&entry.124074799=Read"},
{"title": "GreatSplicing: A Semantically Rich Splicing Dataset", "author": "Jiaming Liang and Yuwan Xue and Haowei Liu and Zhenqi Dai and Yu Liao and Rui Wang and Weihao Jiang and Yaping Liu and Zhikun Chen and Guoxiao Liu and Bo Liu and Xiuli Bi", "abstract": "In existing splicing forgery datasets, the insufficient semantic variety of spliced regions causes trained detection models to overfit semantic features rather than learn genuine splicing traces. Meanwhile, the lack of a reasonable benchmark dataset has led to inconsistent experimental settings across existing detection methods. To address these issues, we propose GreatSplicing, a manually created, large-scale, high-quality splicing dataset. GreatSplicing comprises 5,000 spliced images and covers spliced regions across 335 distinct semantic categories, enabling detection models to learn splicing traces more effectively. Empirical results show that detection models trained on GreatSplicing achieve low misidentification rates and stronger cross-dataset generalization compared to existing datasets. GreatSplicing is now publicly available for research purposes at the following link.", "link": "http://arxiv.org/abs/2310.10070v3", "date": "2025-11-14", "relevancy": 2.3803, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5086}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4724}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GreatSplicing%3A%20A%20Semantically%20Rich%20Splicing%20Dataset&body=Title%3A%20GreatSplicing%3A%20A%20Semantically%20Rich%20Splicing%20Dataset%0AAuthor%3A%20Jiaming%20Liang%20and%20Yuwan%20Xue%20and%20Haowei%20Liu%20and%20Zhenqi%20Dai%20and%20Yu%20Liao%20and%20Rui%20Wang%20and%20Weihao%20Jiang%20and%20Yaping%20Liu%20and%20Zhikun%20Chen%20and%20Guoxiao%20Liu%20and%20Bo%20Liu%20and%20Xiuli%20Bi%0AAbstract%3A%20In%20existing%20splicing%20forgery%20datasets%2C%20the%20insufficient%20semantic%20variety%20of%20spliced%20regions%20causes%20trained%20detection%20models%20to%20overfit%20semantic%20features%20rather%20than%20learn%20genuine%20splicing%20traces.%20Meanwhile%2C%20the%20lack%20of%20a%20reasonable%20benchmark%20dataset%20has%20led%20to%20inconsistent%20experimental%20settings%20across%20existing%20detection%20methods.%20To%20address%20these%20issues%2C%20we%20propose%20GreatSplicing%2C%20a%20manually%20created%2C%20large-scale%2C%20high-quality%20splicing%20dataset.%20GreatSplicing%20comprises%205%2C000%20spliced%20images%20and%20covers%20spliced%20regions%20across%20335%20distinct%20semantic%20categories%2C%20enabling%20detection%20models%20to%20learn%20splicing%20traces%20more%20effectively.%20Empirical%20results%20show%20that%20detection%20models%20trained%20on%20GreatSplicing%20achieve%20low%20misidentification%20rates%20and%20stronger%20cross-dataset%20generalization%20compared%20to%20existing%20datasets.%20GreatSplicing%20is%20now%20publicly%20available%20for%20research%20purposes%20at%20the%20following%20link.%0ALink%3A%20http%3A//arxiv.org/abs/2310.10070v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGreatSplicing%253A%2520A%2520Semantically%2520Rich%2520Splicing%2520Dataset%26entry.906535625%3DJiaming%2520Liang%2520and%2520Yuwan%2520Xue%2520and%2520Haowei%2520Liu%2520and%2520Zhenqi%2520Dai%2520and%2520Yu%2520Liao%2520and%2520Rui%2520Wang%2520and%2520Weihao%2520Jiang%2520and%2520Yaping%2520Liu%2520and%2520Zhikun%2520Chen%2520and%2520Guoxiao%2520Liu%2520and%2520Bo%2520Liu%2520and%2520Xiuli%2520Bi%26entry.1292438233%3DIn%2520existing%2520splicing%2520forgery%2520datasets%252C%2520the%2520insufficient%2520semantic%2520variety%2520of%2520spliced%2520regions%2520causes%2520trained%2520detection%2520models%2520to%2520overfit%2520semantic%2520features%2520rather%2520than%2520learn%2520genuine%2520splicing%2520traces.%2520Meanwhile%252C%2520the%2520lack%2520of%2520a%2520reasonable%2520benchmark%2520dataset%2520has%2520led%2520to%2520inconsistent%2520experimental%2520settings%2520across%2520existing%2520detection%2520methods.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520GreatSplicing%252C%2520a%2520manually%2520created%252C%2520large-scale%252C%2520high-quality%2520splicing%2520dataset.%2520GreatSplicing%2520comprises%25205%252C000%2520spliced%2520images%2520and%2520covers%2520spliced%2520regions%2520across%2520335%2520distinct%2520semantic%2520categories%252C%2520enabling%2520detection%2520models%2520to%2520learn%2520splicing%2520traces%2520more%2520effectively.%2520Empirical%2520results%2520show%2520that%2520detection%2520models%2520trained%2520on%2520GreatSplicing%2520achieve%2520low%2520misidentification%2520rates%2520and%2520stronger%2520cross-dataset%2520generalization%2520compared%2520to%2520existing%2520datasets.%2520GreatSplicing%2520is%2520now%2520publicly%2520available%2520for%2520research%2520purposes%2520at%2520the%2520following%2520link.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.10070v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GreatSplicing%3A%20A%20Semantically%20Rich%20Splicing%20Dataset&entry.906535625=Jiaming%20Liang%20and%20Yuwan%20Xue%20and%20Haowei%20Liu%20and%20Zhenqi%20Dai%20and%20Yu%20Liao%20and%20Rui%20Wang%20and%20Weihao%20Jiang%20and%20Yaping%20Liu%20and%20Zhikun%20Chen%20and%20Guoxiao%20Liu%20and%20Bo%20Liu%20and%20Xiuli%20Bi&entry.1292438233=In%20existing%20splicing%20forgery%20datasets%2C%20the%20insufficient%20semantic%20variety%20of%20spliced%20regions%20causes%20trained%20detection%20models%20to%20overfit%20semantic%20features%20rather%20than%20learn%20genuine%20splicing%20traces.%20Meanwhile%2C%20the%20lack%20of%20a%20reasonable%20benchmark%20dataset%20has%20led%20to%20inconsistent%20experimental%20settings%20across%20existing%20detection%20methods.%20To%20address%20these%20issues%2C%20we%20propose%20GreatSplicing%2C%20a%20manually%20created%2C%20large-scale%2C%20high-quality%20splicing%20dataset.%20GreatSplicing%20comprises%205%2C000%20spliced%20images%20and%20covers%20spliced%20regions%20across%20335%20distinct%20semantic%20categories%2C%20enabling%20detection%20models%20to%20learn%20splicing%20traces%20more%20effectively.%20Empirical%20results%20show%20that%20detection%20models%20trained%20on%20GreatSplicing%20achieve%20low%20misidentification%20rates%20and%20stronger%20cross-dataset%20generalization%20compared%20to%20existing%20datasets.%20GreatSplicing%20is%20now%20publicly%20available%20for%20research%20purposes%20at%20the%20following%20link.&entry.1838667208=http%3A//arxiv.org/abs/2310.10070v3&entry.124074799=Read"},
{"title": "Hi-DREAM: Brain Inspired Hierarchical Diffusion for fMRI Reconstruction via ROI Encoder and visuAl Mapping", "author": "Guowei Zhang and Yun Zhao and Moein Khajehnejad and Adeel Razi and Levin Kuhlmann", "abstract": "Mapping human brain activity to natural images offers a new window into vision and cognition, yet current diffusion-based decoders face a core difficulty: most condition directly on fMRI features without analyzing how visual information is organized across the cortex. This overlooks the brain's hierarchical processing and blurs the roles of early, middle, and late visual areas. We propose Hi-DREAM, a brain-inspired conditional diffusion framework that makes the cortical organization explicit. A region-of-interest (ROI) adapter groups fMRI into early/mid/late streams and converts them into a multi-scale cortical pyramid aligned with the U-Net depth (shallow scales preserve layout and edges; deeper scales emphasize objects and semantics). A lightweight, depth-matched ControlNet injects these scale-specific hints during denoising. The result is an efficient and interpretable decoder in which each signal plays a brain-like role, allowing the model not only to reconstruct images but also to illuminate functional contributions of different visual areas. Experiments on the Natural Scenes Dataset (NSD) show that Hi-DREAM attains state-of-the-art performance on high-level semantic metrics while maintaining competitive low-level fidelity. These findings suggest that structuring conditioning by cortical hierarchy is a powerful alternative to purely data-driven embeddings and provides a useful lens for studying the visual cortex.", "link": "http://arxiv.org/abs/2511.11437v1", "date": "2025-11-14", "relevancy": 2.3771, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6059}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5919}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hi-DREAM%3A%20Brain%20Inspired%20Hierarchical%20Diffusion%20for%20fMRI%20Reconstruction%20via%20ROI%20Encoder%20and%20visuAl%20Mapping&body=Title%3A%20Hi-DREAM%3A%20Brain%20Inspired%20Hierarchical%20Diffusion%20for%20fMRI%20Reconstruction%20via%20ROI%20Encoder%20and%20visuAl%20Mapping%0AAuthor%3A%20Guowei%20Zhang%20and%20Yun%20Zhao%20and%20Moein%20Khajehnejad%20and%20Adeel%20Razi%20and%20Levin%20Kuhlmann%0AAbstract%3A%20Mapping%20human%20brain%20activity%20to%20natural%20images%20offers%20a%20new%20window%20into%20vision%20and%20cognition%2C%20yet%20current%20diffusion-based%20decoders%20face%20a%20core%20difficulty%3A%20most%20condition%20directly%20on%20fMRI%20features%20without%20analyzing%20how%20visual%20information%20is%20organized%20across%20the%20cortex.%20This%20overlooks%20the%20brain%27s%20hierarchical%20processing%20and%20blurs%20the%20roles%20of%20early%2C%20middle%2C%20and%20late%20visual%20areas.%20We%20propose%20Hi-DREAM%2C%20a%20brain-inspired%20conditional%20diffusion%20framework%20that%20makes%20the%20cortical%20organization%20explicit.%20A%20region-of-interest%20%28ROI%29%20adapter%20groups%20fMRI%20into%20early/mid/late%20streams%20and%20converts%20them%20into%20a%20multi-scale%20cortical%20pyramid%20aligned%20with%20the%20U-Net%20depth%20%28shallow%20scales%20preserve%20layout%20and%20edges%3B%20deeper%20scales%20emphasize%20objects%20and%20semantics%29.%20A%20lightweight%2C%20depth-matched%20ControlNet%20injects%20these%20scale-specific%20hints%20during%20denoising.%20The%20result%20is%20an%20efficient%20and%20interpretable%20decoder%20in%20which%20each%20signal%20plays%20a%20brain-like%20role%2C%20allowing%20the%20model%20not%20only%20to%20reconstruct%20images%20but%20also%20to%20illuminate%20functional%20contributions%20of%20different%20visual%20areas.%20Experiments%20on%20the%20Natural%20Scenes%20Dataset%20%28NSD%29%20show%20that%20Hi-DREAM%20attains%20state-of-the-art%20performance%20on%20high-level%20semantic%20metrics%20while%20maintaining%20competitive%20low-level%20fidelity.%20These%20findings%20suggest%20that%20structuring%20conditioning%20by%20cortical%20hierarchy%20is%20a%20powerful%20alternative%20to%20purely%20data-driven%20embeddings%20and%20provides%20a%20useful%20lens%20for%20studying%20the%20visual%20cortex.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHi-DREAM%253A%2520Brain%2520Inspired%2520Hierarchical%2520Diffusion%2520for%2520fMRI%2520Reconstruction%2520via%2520ROI%2520Encoder%2520and%2520visuAl%2520Mapping%26entry.906535625%3DGuowei%2520Zhang%2520and%2520Yun%2520Zhao%2520and%2520Moein%2520Khajehnejad%2520and%2520Adeel%2520Razi%2520and%2520Levin%2520Kuhlmann%26entry.1292438233%3DMapping%2520human%2520brain%2520activity%2520to%2520natural%2520images%2520offers%2520a%2520new%2520window%2520into%2520vision%2520and%2520cognition%252C%2520yet%2520current%2520diffusion-based%2520decoders%2520face%2520a%2520core%2520difficulty%253A%2520most%2520condition%2520directly%2520on%2520fMRI%2520features%2520without%2520analyzing%2520how%2520visual%2520information%2520is%2520organized%2520across%2520the%2520cortex.%2520This%2520overlooks%2520the%2520brain%2527s%2520hierarchical%2520processing%2520and%2520blurs%2520the%2520roles%2520of%2520early%252C%2520middle%252C%2520and%2520late%2520visual%2520areas.%2520We%2520propose%2520Hi-DREAM%252C%2520a%2520brain-inspired%2520conditional%2520diffusion%2520framework%2520that%2520makes%2520the%2520cortical%2520organization%2520explicit.%2520A%2520region-of-interest%2520%2528ROI%2529%2520adapter%2520groups%2520fMRI%2520into%2520early/mid/late%2520streams%2520and%2520converts%2520them%2520into%2520a%2520multi-scale%2520cortical%2520pyramid%2520aligned%2520with%2520the%2520U-Net%2520depth%2520%2528shallow%2520scales%2520preserve%2520layout%2520and%2520edges%253B%2520deeper%2520scales%2520emphasize%2520objects%2520and%2520semantics%2529.%2520A%2520lightweight%252C%2520depth-matched%2520ControlNet%2520injects%2520these%2520scale-specific%2520hints%2520during%2520denoising.%2520The%2520result%2520is%2520an%2520efficient%2520and%2520interpretable%2520decoder%2520in%2520which%2520each%2520signal%2520plays%2520a%2520brain-like%2520role%252C%2520allowing%2520the%2520model%2520not%2520only%2520to%2520reconstruct%2520images%2520but%2520also%2520to%2520illuminate%2520functional%2520contributions%2520of%2520different%2520visual%2520areas.%2520Experiments%2520on%2520the%2520Natural%2520Scenes%2520Dataset%2520%2528NSD%2529%2520show%2520that%2520Hi-DREAM%2520attains%2520state-of-the-art%2520performance%2520on%2520high-level%2520semantic%2520metrics%2520while%2520maintaining%2520competitive%2520low-level%2520fidelity.%2520These%2520findings%2520suggest%2520that%2520structuring%2520conditioning%2520by%2520cortical%2520hierarchy%2520is%2520a%2520powerful%2520alternative%2520to%2520purely%2520data-driven%2520embeddings%2520and%2520provides%2520a%2520useful%2520lens%2520for%2520studying%2520the%2520visual%2520cortex.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hi-DREAM%3A%20Brain%20Inspired%20Hierarchical%20Diffusion%20for%20fMRI%20Reconstruction%20via%20ROI%20Encoder%20and%20visuAl%20Mapping&entry.906535625=Guowei%20Zhang%20and%20Yun%20Zhao%20and%20Moein%20Khajehnejad%20and%20Adeel%20Razi%20and%20Levin%20Kuhlmann&entry.1292438233=Mapping%20human%20brain%20activity%20to%20natural%20images%20offers%20a%20new%20window%20into%20vision%20and%20cognition%2C%20yet%20current%20diffusion-based%20decoders%20face%20a%20core%20difficulty%3A%20most%20condition%20directly%20on%20fMRI%20features%20without%20analyzing%20how%20visual%20information%20is%20organized%20across%20the%20cortex.%20This%20overlooks%20the%20brain%27s%20hierarchical%20processing%20and%20blurs%20the%20roles%20of%20early%2C%20middle%2C%20and%20late%20visual%20areas.%20We%20propose%20Hi-DREAM%2C%20a%20brain-inspired%20conditional%20diffusion%20framework%20that%20makes%20the%20cortical%20organization%20explicit.%20A%20region-of-interest%20%28ROI%29%20adapter%20groups%20fMRI%20into%20early/mid/late%20streams%20and%20converts%20them%20into%20a%20multi-scale%20cortical%20pyramid%20aligned%20with%20the%20U-Net%20depth%20%28shallow%20scales%20preserve%20layout%20and%20edges%3B%20deeper%20scales%20emphasize%20objects%20and%20semantics%29.%20A%20lightweight%2C%20depth-matched%20ControlNet%20injects%20these%20scale-specific%20hints%20during%20denoising.%20The%20result%20is%20an%20efficient%20and%20interpretable%20decoder%20in%20which%20each%20signal%20plays%20a%20brain-like%20role%2C%20allowing%20the%20model%20not%20only%20to%20reconstruct%20images%20but%20also%20to%20illuminate%20functional%20contributions%20of%20different%20visual%20areas.%20Experiments%20on%20the%20Natural%20Scenes%20Dataset%20%28NSD%29%20show%20that%20Hi-DREAM%20attains%20state-of-the-art%20performance%20on%20high-level%20semantic%20metrics%20while%20maintaining%20competitive%20low-level%20fidelity.%20These%20findings%20suggest%20that%20structuring%20conditioning%20by%20cortical%20hierarchy%20is%20a%20powerful%20alternative%20to%20purely%20data-driven%20embeddings%20and%20provides%20a%20useful%20lens%20for%20studying%20the%20visual%20cortex.&entry.1838667208=http%3A//arxiv.org/abs/2511.11437v1&entry.124074799=Read"},
{"title": "PRSM: A Measure to Evaluate CLIP's Robustness Against Paraphrases", "author": "Udo Schlegel and Franziska Weeber and Jian Lan and Thomas Seidl", "abstract": "Contrastive Language-Image Pre-training (CLIP) is a widely used multimodal model that aligns text and image representations through large-scale training. While it performs strongly on zero-shot and few-shot tasks, its robustness to linguistic variation, particularly paraphrasing, remains underexplored. Paraphrase robustness is essential for reliable deployment, especially in socially sensitive contexts where inconsistent representations can amplify demographic biases. In this paper, we introduce the Paraphrase Ranking Stability Metric (PRSM), a novel measure for quantifying CLIP's sensitivity to paraphrased queries. Using the Social Counterfactuals dataset, a benchmark designed to reveal social and demographic biases, we empirically assess CLIP's stability under paraphrastic variation, examine the interaction between paraphrase robustness and gender, and discuss implications for fairness and equitable deployment of multimodal systems. Our analysis reveals that robustness varies across paraphrasing strategies, with subtle yet consistent differences observed between male- and female-associated queries.", "link": "http://arxiv.org/abs/2511.11141v1", "date": "2025-11-14", "relevancy": 2.365, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRSM%3A%20A%20Measure%20to%20Evaluate%20CLIP%27s%20Robustness%20Against%20Paraphrases&body=Title%3A%20PRSM%3A%20A%20Measure%20to%20Evaluate%20CLIP%27s%20Robustness%20Against%20Paraphrases%0AAuthor%3A%20Udo%20Schlegel%20and%20Franziska%20Weeber%20and%20Jian%20Lan%20and%20Thomas%20Seidl%0AAbstract%3A%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20is%20a%20widely%20used%20multimodal%20model%20that%20aligns%20text%20and%20image%20representations%20through%20large-scale%20training.%20While%20it%20performs%20strongly%20on%20zero-shot%20and%20few-shot%20tasks%2C%20its%20robustness%20to%20linguistic%20variation%2C%20particularly%20paraphrasing%2C%20remains%20underexplored.%20Paraphrase%20robustness%20is%20essential%20for%20reliable%20deployment%2C%20especially%20in%20socially%20sensitive%20contexts%20where%20inconsistent%20representations%20can%20amplify%20demographic%20biases.%20In%20this%20paper%2C%20we%20introduce%20the%20Paraphrase%20Ranking%20Stability%20Metric%20%28PRSM%29%2C%20a%20novel%20measure%20for%20quantifying%20CLIP%27s%20sensitivity%20to%20paraphrased%20queries.%20Using%20the%20Social%20Counterfactuals%20dataset%2C%20a%20benchmark%20designed%20to%20reveal%20social%20and%20demographic%20biases%2C%20we%20empirically%20assess%20CLIP%27s%20stability%20under%20paraphrastic%20variation%2C%20examine%20the%20interaction%20between%20paraphrase%20robustness%20and%20gender%2C%20and%20discuss%20implications%20for%20fairness%20and%20equitable%20deployment%20of%20multimodal%20systems.%20Our%20analysis%20reveals%20that%20robustness%20varies%20across%20paraphrasing%20strategies%2C%20with%20subtle%20yet%20consistent%20differences%20observed%20between%20male-%20and%20female-associated%20queries.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11141v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRSM%253A%2520A%2520Measure%2520to%2520Evaluate%2520CLIP%2527s%2520Robustness%2520Against%2520Paraphrases%26entry.906535625%3DUdo%2520Schlegel%2520and%2520Franziska%2520Weeber%2520and%2520Jian%2520Lan%2520and%2520Thomas%2520Seidl%26entry.1292438233%3DContrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520is%2520a%2520widely%2520used%2520multimodal%2520model%2520that%2520aligns%2520text%2520and%2520image%2520representations%2520through%2520large-scale%2520training.%2520While%2520it%2520performs%2520strongly%2520on%2520zero-shot%2520and%2520few-shot%2520tasks%252C%2520its%2520robustness%2520to%2520linguistic%2520variation%252C%2520particularly%2520paraphrasing%252C%2520remains%2520underexplored.%2520Paraphrase%2520robustness%2520is%2520essential%2520for%2520reliable%2520deployment%252C%2520especially%2520in%2520socially%2520sensitive%2520contexts%2520where%2520inconsistent%2520representations%2520can%2520amplify%2520demographic%2520biases.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Paraphrase%2520Ranking%2520Stability%2520Metric%2520%2528PRSM%2529%252C%2520a%2520novel%2520measure%2520for%2520quantifying%2520CLIP%2527s%2520sensitivity%2520to%2520paraphrased%2520queries.%2520Using%2520the%2520Social%2520Counterfactuals%2520dataset%252C%2520a%2520benchmark%2520designed%2520to%2520reveal%2520social%2520and%2520demographic%2520biases%252C%2520we%2520empirically%2520assess%2520CLIP%2527s%2520stability%2520under%2520paraphrastic%2520variation%252C%2520examine%2520the%2520interaction%2520between%2520paraphrase%2520robustness%2520and%2520gender%252C%2520and%2520discuss%2520implications%2520for%2520fairness%2520and%2520equitable%2520deployment%2520of%2520multimodal%2520systems.%2520Our%2520analysis%2520reveals%2520that%2520robustness%2520varies%2520across%2520paraphrasing%2520strategies%252C%2520with%2520subtle%2520yet%2520consistent%2520differences%2520observed%2520between%2520male-%2520and%2520female-associated%2520queries.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11141v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRSM%3A%20A%20Measure%20to%20Evaluate%20CLIP%27s%20Robustness%20Against%20Paraphrases&entry.906535625=Udo%20Schlegel%20and%20Franziska%20Weeber%20and%20Jian%20Lan%20and%20Thomas%20Seidl&entry.1292438233=Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20is%20a%20widely%20used%20multimodal%20model%20that%20aligns%20text%20and%20image%20representations%20through%20large-scale%20training.%20While%20it%20performs%20strongly%20on%20zero-shot%20and%20few-shot%20tasks%2C%20its%20robustness%20to%20linguistic%20variation%2C%20particularly%20paraphrasing%2C%20remains%20underexplored.%20Paraphrase%20robustness%20is%20essential%20for%20reliable%20deployment%2C%20especially%20in%20socially%20sensitive%20contexts%20where%20inconsistent%20representations%20can%20amplify%20demographic%20biases.%20In%20this%20paper%2C%20we%20introduce%20the%20Paraphrase%20Ranking%20Stability%20Metric%20%28PRSM%29%2C%20a%20novel%20measure%20for%20quantifying%20CLIP%27s%20sensitivity%20to%20paraphrased%20queries.%20Using%20the%20Social%20Counterfactuals%20dataset%2C%20a%20benchmark%20designed%20to%20reveal%20social%20and%20demographic%20biases%2C%20we%20empirically%20assess%20CLIP%27s%20stability%20under%20paraphrastic%20variation%2C%20examine%20the%20interaction%20between%20paraphrase%20robustness%20and%20gender%2C%20and%20discuss%20implications%20for%20fairness%20and%20equitable%20deployment%20of%20multimodal%20systems.%20Our%20analysis%20reveals%20that%20robustness%20varies%20across%20paraphrasing%20strategies%2C%20with%20subtle%20yet%20consistent%20differences%20observed%20between%20male-%20and%20female-associated%20queries.&entry.1838667208=http%3A//arxiv.org/abs/2511.11141v1&entry.124074799=Read"},
{"title": "Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities", "author": "Yiyun Zhou and Mingjing Xu and Jingwei Shi and Quanjiang Li and Jingyuan Chen", "abstract": "Tactile sensing offers rich and complementary information to vision and language, enabling robots to perceive fine-grained object properties. However, existing tactile sensors lack standardization, leading to redundant features that hinder cross-sensor generalization. Moreover, existing methods fail to fully integrate the intermediate communication among tactile, language, and vision modalities. To address this, we propose TLV-CoRe, a CLIP-based Tactile-Language-Vision Collaborative Representation learning method. TLV-CoRe introduces a Sensor-Aware Modulator to unify tactile features across different sensors and employs tactile-irrelevant decoupled learning to disentangle irrelevant tactile features. Additionally, a Unified Bridging Adapter is introduced to enhance tri-modal interaction within the shared representation space. To fairly evaluate the effectiveness of tactile models, we further propose the RSS evaluation framework, focusing on Robustness, Synergy, and Stability across different methods. Experimental results demonstrate that TLV-CoRe significantly improves sensor-agnostic representation learning and cross-modal alignment, offering a new direction for multimodal tactile representation.", "link": "http://arxiv.org/abs/2511.11512v1", "date": "2025-11-14", "relevancy": 2.3412, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6352}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Representation%20Learning%20for%20Alignment%20of%20Tactile%2C%20Language%2C%20and%20Vision%20Modalities&body=Title%3A%20Collaborative%20Representation%20Learning%20for%20Alignment%20of%20Tactile%2C%20Language%2C%20and%20Vision%20Modalities%0AAuthor%3A%20Yiyun%20Zhou%20and%20Mingjing%20Xu%20and%20Jingwei%20Shi%20and%20Quanjiang%20Li%20and%20Jingyuan%20Chen%0AAbstract%3A%20Tactile%20sensing%20offers%20rich%20and%20complementary%20information%20to%20vision%20and%20language%2C%20enabling%20robots%20to%20perceive%20fine-grained%20object%20properties.%20However%2C%20existing%20tactile%20sensors%20lack%20standardization%2C%20leading%20to%20redundant%20features%20that%20hinder%20cross-sensor%20generalization.%20Moreover%2C%20existing%20methods%20fail%20to%20fully%20integrate%20the%20intermediate%20communication%20among%20tactile%2C%20language%2C%20and%20vision%20modalities.%20To%20address%20this%2C%20we%20propose%20TLV-CoRe%2C%20a%20CLIP-based%20Tactile-Language-Vision%20Collaborative%20Representation%20learning%20method.%20TLV-CoRe%20introduces%20a%20Sensor-Aware%20Modulator%20to%20unify%20tactile%20features%20across%20different%20sensors%20and%20employs%20tactile-irrelevant%20decoupled%20learning%20to%20disentangle%20irrelevant%20tactile%20features.%20Additionally%2C%20a%20Unified%20Bridging%20Adapter%20is%20introduced%20to%20enhance%20tri-modal%20interaction%20within%20the%20shared%20representation%20space.%20To%20fairly%20evaluate%20the%20effectiveness%20of%20tactile%20models%2C%20we%20further%20propose%20the%20RSS%20evaluation%20framework%2C%20focusing%20on%20Robustness%2C%20Synergy%2C%20and%20Stability%20across%20different%20methods.%20Experimental%20results%20demonstrate%20that%20TLV-CoRe%20significantly%20improves%20sensor-agnostic%20representation%20learning%20and%20cross-modal%20alignment%2C%20offering%20a%20new%20direction%20for%20multimodal%20tactile%20representation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Representation%2520Learning%2520for%2520Alignment%2520of%2520Tactile%252C%2520Language%252C%2520and%2520Vision%2520Modalities%26entry.906535625%3DYiyun%2520Zhou%2520and%2520Mingjing%2520Xu%2520and%2520Jingwei%2520Shi%2520and%2520Quanjiang%2520Li%2520and%2520Jingyuan%2520Chen%26entry.1292438233%3DTactile%2520sensing%2520offers%2520rich%2520and%2520complementary%2520information%2520to%2520vision%2520and%2520language%252C%2520enabling%2520robots%2520to%2520perceive%2520fine-grained%2520object%2520properties.%2520However%252C%2520existing%2520tactile%2520sensors%2520lack%2520standardization%252C%2520leading%2520to%2520redundant%2520features%2520that%2520hinder%2520cross-sensor%2520generalization.%2520Moreover%252C%2520existing%2520methods%2520fail%2520to%2520fully%2520integrate%2520the%2520intermediate%2520communication%2520among%2520tactile%252C%2520language%252C%2520and%2520vision%2520modalities.%2520To%2520address%2520this%252C%2520we%2520propose%2520TLV-CoRe%252C%2520a%2520CLIP-based%2520Tactile-Language-Vision%2520Collaborative%2520Representation%2520learning%2520method.%2520TLV-CoRe%2520introduces%2520a%2520Sensor-Aware%2520Modulator%2520to%2520unify%2520tactile%2520features%2520across%2520different%2520sensors%2520and%2520employs%2520tactile-irrelevant%2520decoupled%2520learning%2520to%2520disentangle%2520irrelevant%2520tactile%2520features.%2520Additionally%252C%2520a%2520Unified%2520Bridging%2520Adapter%2520is%2520introduced%2520to%2520enhance%2520tri-modal%2520interaction%2520within%2520the%2520shared%2520representation%2520space.%2520To%2520fairly%2520evaluate%2520the%2520effectiveness%2520of%2520tactile%2520models%252C%2520we%2520further%2520propose%2520the%2520RSS%2520evaluation%2520framework%252C%2520focusing%2520on%2520Robustness%252C%2520Synergy%252C%2520and%2520Stability%2520across%2520different%2520methods.%2520Experimental%2520results%2520demonstrate%2520that%2520TLV-CoRe%2520significantly%2520improves%2520sensor-agnostic%2520representation%2520learning%2520and%2520cross-modal%2520alignment%252C%2520offering%2520a%2520new%2520direction%2520for%2520multimodal%2520tactile%2520representation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Representation%20Learning%20for%20Alignment%20of%20Tactile%2C%20Language%2C%20and%20Vision%20Modalities&entry.906535625=Yiyun%20Zhou%20and%20Mingjing%20Xu%20and%20Jingwei%20Shi%20and%20Quanjiang%20Li%20and%20Jingyuan%20Chen&entry.1292438233=Tactile%20sensing%20offers%20rich%20and%20complementary%20information%20to%20vision%20and%20language%2C%20enabling%20robots%20to%20perceive%20fine-grained%20object%20properties.%20However%2C%20existing%20tactile%20sensors%20lack%20standardization%2C%20leading%20to%20redundant%20features%20that%20hinder%20cross-sensor%20generalization.%20Moreover%2C%20existing%20methods%20fail%20to%20fully%20integrate%20the%20intermediate%20communication%20among%20tactile%2C%20language%2C%20and%20vision%20modalities.%20To%20address%20this%2C%20we%20propose%20TLV-CoRe%2C%20a%20CLIP-based%20Tactile-Language-Vision%20Collaborative%20Representation%20learning%20method.%20TLV-CoRe%20introduces%20a%20Sensor-Aware%20Modulator%20to%20unify%20tactile%20features%20across%20different%20sensors%20and%20employs%20tactile-irrelevant%20decoupled%20learning%20to%20disentangle%20irrelevant%20tactile%20features.%20Additionally%2C%20a%20Unified%20Bridging%20Adapter%20is%20introduced%20to%20enhance%20tri-modal%20interaction%20within%20the%20shared%20representation%20space.%20To%20fairly%20evaluate%20the%20effectiveness%20of%20tactile%20models%2C%20we%20further%20propose%20the%20RSS%20evaluation%20framework%2C%20focusing%20on%20Robustness%2C%20Synergy%2C%20and%20Stability%20across%20different%20methods.%20Experimental%20results%20demonstrate%20that%20TLV-CoRe%20significantly%20improves%20sensor-agnostic%20representation%20learning%20and%20cross-modal%20alignment%2C%20offering%20a%20new%20direction%20for%20multimodal%20tactile%20representation.&entry.1838667208=http%3A//arxiv.org/abs/2511.11512v1&entry.124074799=Read"},
{"title": "Adaptive LiDAR Scanning: Harnessing Temporal Cues for Efficient 3D Object Detection via Multi-Modal Fusion", "author": "Sara Shoouri and Morteza Tavakoli Taba and Hun-Seok Kim", "abstract": "Multi-sensor fusion using LiDAR and RGB cameras significantly enhances 3D object detection task. However, conventional LiDAR sensors perform dense, stateless scans, ignoring the strong temporal continuity in real-world scenes. This leads to substantial sensing redundancy and excessive power consumption, limiting their practicality on resource-constrained platforms. To address this inefficiency, we propose a predictive, history-aware adaptive scanning framework that anticipates informative regions of interest (ROI) based on past observations. Our approach introduces a lightweight predictor network that distills historical spatial and temporal contexts into refined query embeddings. These embeddings guide a differentiable Mask Generator network, which leverages Gumbel-Softmax sampling to produce binary masks identifying critical ROIs for the upcoming frame. Our method significantly reduces unnecessary data acquisition by concentrating dense LiDAR scanning only within these ROIs and sparsely sampling elsewhere. Experiments on nuScenes and Lyft benchmarks demonstrate that our adaptive scanning strategy reduces LiDAR energy consumption by over 65% while maintaining competitive or even superior 3D object detection performance compared to traditional LiDAR-camera fusion methods with dense LiDAR scanning.", "link": "http://arxiv.org/abs/2508.01562v2", "date": "2025-11-14", "relevancy": 2.3376, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6007}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5835}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20LiDAR%20Scanning%3A%20Harnessing%20Temporal%20Cues%20for%20Efficient%203D%20Object%20Detection%20via%20Multi-Modal%20Fusion&body=Title%3A%20Adaptive%20LiDAR%20Scanning%3A%20Harnessing%20Temporal%20Cues%20for%20Efficient%203D%20Object%20Detection%20via%20Multi-Modal%20Fusion%0AAuthor%3A%20Sara%20Shoouri%20and%20Morteza%20Tavakoli%20Taba%20and%20Hun-Seok%20Kim%0AAbstract%3A%20Multi-sensor%20fusion%20using%20LiDAR%20and%20RGB%20cameras%20significantly%20enhances%203D%20object%20detection%20task.%20However%2C%20conventional%20LiDAR%20sensors%20perform%20dense%2C%20stateless%20scans%2C%20ignoring%20the%20strong%20temporal%20continuity%20in%20real-world%20scenes.%20This%20leads%20to%20substantial%20sensing%20redundancy%20and%20excessive%20power%20consumption%2C%20limiting%20their%20practicality%20on%20resource-constrained%20platforms.%20To%20address%20this%20inefficiency%2C%20we%20propose%20a%20predictive%2C%20history-aware%20adaptive%20scanning%20framework%20that%20anticipates%20informative%20regions%20of%20interest%20%28ROI%29%20based%20on%20past%20observations.%20Our%20approach%20introduces%20a%20lightweight%20predictor%20network%20that%20distills%20historical%20spatial%20and%20temporal%20contexts%20into%20refined%20query%20embeddings.%20These%20embeddings%20guide%20a%20differentiable%20Mask%20Generator%20network%2C%20which%20leverages%20Gumbel-Softmax%20sampling%20to%20produce%20binary%20masks%20identifying%20critical%20ROIs%20for%20the%20upcoming%20frame.%20Our%20method%20significantly%20reduces%20unnecessary%20data%20acquisition%20by%20concentrating%20dense%20LiDAR%20scanning%20only%20within%20these%20ROIs%20and%20sparsely%20sampling%20elsewhere.%20Experiments%20on%20nuScenes%20and%20Lyft%20benchmarks%20demonstrate%20that%20our%20adaptive%20scanning%20strategy%20reduces%20LiDAR%20energy%20consumption%20by%20over%2065%25%20while%20maintaining%20competitive%20or%20even%20superior%203D%20object%20detection%20performance%20compared%20to%20traditional%20LiDAR-camera%20fusion%20methods%20with%20dense%20LiDAR%20scanning.%0ALink%3A%20http%3A//arxiv.org/abs/2508.01562v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520LiDAR%2520Scanning%253A%2520Harnessing%2520Temporal%2520Cues%2520for%2520Efficient%25203D%2520Object%2520Detection%2520via%2520Multi-Modal%2520Fusion%26entry.906535625%3DSara%2520Shoouri%2520and%2520Morteza%2520Tavakoli%2520Taba%2520and%2520Hun-Seok%2520Kim%26entry.1292438233%3DMulti-sensor%2520fusion%2520using%2520LiDAR%2520and%2520RGB%2520cameras%2520significantly%2520enhances%25203D%2520object%2520detection%2520task.%2520However%252C%2520conventional%2520LiDAR%2520sensors%2520perform%2520dense%252C%2520stateless%2520scans%252C%2520ignoring%2520the%2520strong%2520temporal%2520continuity%2520in%2520real-world%2520scenes.%2520This%2520leads%2520to%2520substantial%2520sensing%2520redundancy%2520and%2520excessive%2520power%2520consumption%252C%2520limiting%2520their%2520practicality%2520on%2520resource-constrained%2520platforms.%2520To%2520address%2520this%2520inefficiency%252C%2520we%2520propose%2520a%2520predictive%252C%2520history-aware%2520adaptive%2520scanning%2520framework%2520that%2520anticipates%2520informative%2520regions%2520of%2520interest%2520%2528ROI%2529%2520based%2520on%2520past%2520observations.%2520Our%2520approach%2520introduces%2520a%2520lightweight%2520predictor%2520network%2520that%2520distills%2520historical%2520spatial%2520and%2520temporal%2520contexts%2520into%2520refined%2520query%2520embeddings.%2520These%2520embeddings%2520guide%2520a%2520differentiable%2520Mask%2520Generator%2520network%252C%2520which%2520leverages%2520Gumbel-Softmax%2520sampling%2520to%2520produce%2520binary%2520masks%2520identifying%2520critical%2520ROIs%2520for%2520the%2520upcoming%2520frame.%2520Our%2520method%2520significantly%2520reduces%2520unnecessary%2520data%2520acquisition%2520by%2520concentrating%2520dense%2520LiDAR%2520scanning%2520only%2520within%2520these%2520ROIs%2520and%2520sparsely%2520sampling%2520elsewhere.%2520Experiments%2520on%2520nuScenes%2520and%2520Lyft%2520benchmarks%2520demonstrate%2520that%2520our%2520adaptive%2520scanning%2520strategy%2520reduces%2520LiDAR%2520energy%2520consumption%2520by%2520over%252065%2525%2520while%2520maintaining%2520competitive%2520or%2520even%2520superior%25203D%2520object%2520detection%2520performance%2520compared%2520to%2520traditional%2520LiDAR-camera%2520fusion%2520methods%2520with%2520dense%2520LiDAR%2520scanning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01562v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20LiDAR%20Scanning%3A%20Harnessing%20Temporal%20Cues%20for%20Efficient%203D%20Object%20Detection%20via%20Multi-Modal%20Fusion&entry.906535625=Sara%20Shoouri%20and%20Morteza%20Tavakoli%20Taba%20and%20Hun-Seok%20Kim&entry.1292438233=Multi-sensor%20fusion%20using%20LiDAR%20and%20RGB%20cameras%20significantly%20enhances%203D%20object%20detection%20task.%20However%2C%20conventional%20LiDAR%20sensors%20perform%20dense%2C%20stateless%20scans%2C%20ignoring%20the%20strong%20temporal%20continuity%20in%20real-world%20scenes.%20This%20leads%20to%20substantial%20sensing%20redundancy%20and%20excessive%20power%20consumption%2C%20limiting%20their%20practicality%20on%20resource-constrained%20platforms.%20To%20address%20this%20inefficiency%2C%20we%20propose%20a%20predictive%2C%20history-aware%20adaptive%20scanning%20framework%20that%20anticipates%20informative%20regions%20of%20interest%20%28ROI%29%20based%20on%20past%20observations.%20Our%20approach%20introduces%20a%20lightweight%20predictor%20network%20that%20distills%20historical%20spatial%20and%20temporal%20contexts%20into%20refined%20query%20embeddings.%20These%20embeddings%20guide%20a%20differentiable%20Mask%20Generator%20network%2C%20which%20leverages%20Gumbel-Softmax%20sampling%20to%20produce%20binary%20masks%20identifying%20critical%20ROIs%20for%20the%20upcoming%20frame.%20Our%20method%20significantly%20reduces%20unnecessary%20data%20acquisition%20by%20concentrating%20dense%20LiDAR%20scanning%20only%20within%20these%20ROIs%20and%20sparsely%20sampling%20elsewhere.%20Experiments%20on%20nuScenes%20and%20Lyft%20benchmarks%20demonstrate%20that%20our%20adaptive%20scanning%20strategy%20reduces%20LiDAR%20energy%20consumption%20by%20over%2065%25%20while%20maintaining%20competitive%20or%20even%20superior%203D%20object%20detection%20performance%20compared%20to%20traditional%20LiDAR-camera%20fusion%20methods%20with%20dense%20LiDAR%20scanning.&entry.1838667208=http%3A//arxiv.org/abs/2508.01562v2&entry.124074799=Read"},
{"title": "Building the Web for Agents: A Declarative Framework for Agent-Web Interaction", "author": "Sven Schultze and Meike Verena Kietzmann and Nils-Lucas Sch\u00f6nfeld and Ruth Stock-Homburg", "abstract": "The increasing deployment of autonomous AI agents on the web is hampered by a fundamental misalignment: agents must infer affordances from human-oriented user interfaces, leading to brittle, inefficient, and insecure interactions. To address this, we introduce VOIX, a web-native framework that enables websites to expose reliable, auditable, and privacy-preserving capabilities for AI agents through simple, declarative HTML elements. VOIX introduces <tool> and <context> tags, allowing developers to explicitly define available actions and relevant state, thereby creating a clear, machine-readable contract for agent behavior. This approach shifts control to the website developer while preserving user privacy by disconnecting the conversational interactions from the website. We evaluated the framework's practicality, learnability, and expressiveness in a three-day hackathon study with 16 developers. The results demonstrate that participants, regardless of prior experience, were able to rapidly build diverse and functional agent-enabled web applications. Ultimately, this work provides a foundational mechanism for realizing the Agentic Web, enabling a future of seamless and secure human-AI collaboration on the web.", "link": "http://arxiv.org/abs/2511.11287v1", "date": "2025-11-14", "relevancy": 2.3277, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4745}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4614}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20the%20Web%20for%20Agents%3A%20A%20Declarative%20Framework%20for%20Agent-Web%20Interaction&body=Title%3A%20Building%20the%20Web%20for%20Agents%3A%20A%20Declarative%20Framework%20for%20Agent-Web%20Interaction%0AAuthor%3A%20Sven%20Schultze%20and%20Meike%20Verena%20Kietzmann%20and%20Nils-Lucas%20Sch%C3%B6nfeld%20and%20Ruth%20Stock-Homburg%0AAbstract%3A%20The%20increasing%20deployment%20of%20autonomous%20AI%20agents%20on%20the%20web%20is%20hampered%20by%20a%20fundamental%20misalignment%3A%20agents%20must%20infer%20affordances%20from%20human-oriented%20user%20interfaces%2C%20leading%20to%20brittle%2C%20inefficient%2C%20and%20insecure%20interactions.%20To%20address%20this%2C%20we%20introduce%20VOIX%2C%20a%20web-native%20framework%20that%20enables%20websites%20to%20expose%20reliable%2C%20auditable%2C%20and%20privacy-preserving%20capabilities%20for%20AI%20agents%20through%20simple%2C%20declarative%20HTML%20elements.%20VOIX%20introduces%20%3Ctool%3E%20and%20%3Ccontext%3E%20tags%2C%20allowing%20developers%20to%20explicitly%20define%20available%20actions%20and%20relevant%20state%2C%20thereby%20creating%20a%20clear%2C%20machine-readable%20contract%20for%20agent%20behavior.%20This%20approach%20shifts%20control%20to%20the%20website%20developer%20while%20preserving%20user%20privacy%20by%20disconnecting%20the%20conversational%20interactions%20from%20the%20website.%20We%20evaluated%20the%20framework%27s%20practicality%2C%20learnability%2C%20and%20expressiveness%20in%20a%20three-day%20hackathon%20study%20with%2016%20developers.%20The%20results%20demonstrate%20that%20participants%2C%20regardless%20of%20prior%20experience%2C%20were%20able%20to%20rapidly%20build%20diverse%20and%20functional%20agent-enabled%20web%20applications.%20Ultimately%2C%20this%20work%20provides%20a%20foundational%20mechanism%20for%20realizing%20the%20Agentic%20Web%2C%20enabling%20a%20future%20of%20seamless%20and%20secure%20human-AI%20collaboration%20on%20the%20web.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11287v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520the%2520Web%2520for%2520Agents%253A%2520A%2520Declarative%2520Framework%2520for%2520Agent-Web%2520Interaction%26entry.906535625%3DSven%2520Schultze%2520and%2520Meike%2520Verena%2520Kietzmann%2520and%2520Nils-Lucas%2520Sch%25C3%25B6nfeld%2520and%2520Ruth%2520Stock-Homburg%26entry.1292438233%3DThe%2520increasing%2520deployment%2520of%2520autonomous%2520AI%2520agents%2520on%2520the%2520web%2520is%2520hampered%2520by%2520a%2520fundamental%2520misalignment%253A%2520agents%2520must%2520infer%2520affordances%2520from%2520human-oriented%2520user%2520interfaces%252C%2520leading%2520to%2520brittle%252C%2520inefficient%252C%2520and%2520insecure%2520interactions.%2520To%2520address%2520this%252C%2520we%2520introduce%2520VOIX%252C%2520a%2520web-native%2520framework%2520that%2520enables%2520websites%2520to%2520expose%2520reliable%252C%2520auditable%252C%2520and%2520privacy-preserving%2520capabilities%2520for%2520AI%2520agents%2520through%2520simple%252C%2520declarative%2520HTML%2520elements.%2520VOIX%2520introduces%2520%253Ctool%253E%2520and%2520%253Ccontext%253E%2520tags%252C%2520allowing%2520developers%2520to%2520explicitly%2520define%2520available%2520actions%2520and%2520relevant%2520state%252C%2520thereby%2520creating%2520a%2520clear%252C%2520machine-readable%2520contract%2520for%2520agent%2520behavior.%2520This%2520approach%2520shifts%2520control%2520to%2520the%2520website%2520developer%2520while%2520preserving%2520user%2520privacy%2520by%2520disconnecting%2520the%2520conversational%2520interactions%2520from%2520the%2520website.%2520We%2520evaluated%2520the%2520framework%2527s%2520practicality%252C%2520learnability%252C%2520and%2520expressiveness%2520in%2520a%2520three-day%2520hackathon%2520study%2520with%252016%2520developers.%2520The%2520results%2520demonstrate%2520that%2520participants%252C%2520regardless%2520of%2520prior%2520experience%252C%2520were%2520able%2520to%2520rapidly%2520build%2520diverse%2520and%2520functional%2520agent-enabled%2520web%2520applications.%2520Ultimately%252C%2520this%2520work%2520provides%2520a%2520foundational%2520mechanism%2520for%2520realizing%2520the%2520Agentic%2520Web%252C%2520enabling%2520a%2520future%2520of%2520seamless%2520and%2520secure%2520human-AI%2520collaboration%2520on%2520the%2520web.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11287v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20the%20Web%20for%20Agents%3A%20A%20Declarative%20Framework%20for%20Agent-Web%20Interaction&entry.906535625=Sven%20Schultze%20and%20Meike%20Verena%20Kietzmann%20and%20Nils-Lucas%20Sch%C3%B6nfeld%20and%20Ruth%20Stock-Homburg&entry.1292438233=The%20increasing%20deployment%20of%20autonomous%20AI%20agents%20on%20the%20web%20is%20hampered%20by%20a%20fundamental%20misalignment%3A%20agents%20must%20infer%20affordances%20from%20human-oriented%20user%20interfaces%2C%20leading%20to%20brittle%2C%20inefficient%2C%20and%20insecure%20interactions.%20To%20address%20this%2C%20we%20introduce%20VOIX%2C%20a%20web-native%20framework%20that%20enables%20websites%20to%20expose%20reliable%2C%20auditable%2C%20and%20privacy-preserving%20capabilities%20for%20AI%20agents%20through%20simple%2C%20declarative%20HTML%20elements.%20VOIX%20introduces%20%3Ctool%3E%20and%20%3Ccontext%3E%20tags%2C%20allowing%20developers%20to%20explicitly%20define%20available%20actions%20and%20relevant%20state%2C%20thereby%20creating%20a%20clear%2C%20machine-readable%20contract%20for%20agent%20behavior.%20This%20approach%20shifts%20control%20to%20the%20website%20developer%20while%20preserving%20user%20privacy%20by%20disconnecting%20the%20conversational%20interactions%20from%20the%20website.%20We%20evaluated%20the%20framework%27s%20practicality%2C%20learnability%2C%20and%20expressiveness%20in%20a%20three-day%20hackathon%20study%20with%2016%20developers.%20The%20results%20demonstrate%20that%20participants%2C%20regardless%20of%20prior%20experience%2C%20were%20able%20to%20rapidly%20build%20diverse%20and%20functional%20agent-enabled%20web%20applications.%20Ultimately%2C%20this%20work%20provides%20a%20foundational%20mechanism%20for%20realizing%20the%20Agentic%20Web%2C%20enabling%20a%20future%20of%20seamless%20and%20secure%20human-AI%20collaboration%20on%20the%20web.&entry.1838667208=http%3A//arxiv.org/abs/2511.11287v1&entry.124074799=Read"},
{"title": "GraphPilot: Grounded Scene Graph Conditioning for Language-Based Autonomous Driving", "author": "Fabian Schmidt and Markus Enzweiler and Abhinav Valada", "abstract": "Vision-language models have recently emerged as promising planners for autonomous driving, where success hinges on topology-aware reasoning over spatial structure and dynamic interactions from multimodal input. However, existing models are typically trained without supervision that explicitly encodes these relational dependencies, limiting their ability to infer how agents and other traffic entities influence one another from raw sensor data. In this work, we bridge this gap with a novel model-agnostic method that conditions language-based driving models on structured relational context in the form of traffic scene graphs. We serialize scene graphs at various abstraction levels and formats, and incorporate them into the models via structured prompt templates, enabling a systematic analysis of when and how relational supervision is most beneficial. Extensive evaluations on the public LangAuto benchmark show that scene graph conditioning of state-of-the-art approaches yields large and persistent improvement in driving performance. Notably, we observe up to a 15.6\\% increase in driving score for LMDrive and 17.5\\% for BEVDriver, indicating that models can better internalize and ground relational priors through scene graph-conditioned training, even without requiring scene graph input at test-time. Code, fine-tuned models, and our scene graph dataset are publicly available at https://github.com/iis-esslingen/GraphPilot.", "link": "http://arxiv.org/abs/2511.11266v1", "date": "2025-11-14", "relevancy": 2.3215, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5985}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5767}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphPilot%3A%20Grounded%20Scene%20Graph%20Conditioning%20for%20Language-Based%20Autonomous%20Driving&body=Title%3A%20GraphPilot%3A%20Grounded%20Scene%20Graph%20Conditioning%20for%20Language-Based%20Autonomous%20Driving%0AAuthor%3A%20Fabian%20Schmidt%20and%20Markus%20Enzweiler%20and%20Abhinav%20Valada%0AAbstract%3A%20Vision-language%20models%20have%20recently%20emerged%20as%20promising%20planners%20for%20autonomous%20driving%2C%20where%20success%20hinges%20on%20topology-aware%20reasoning%20over%20spatial%20structure%20and%20dynamic%20interactions%20from%20multimodal%20input.%20However%2C%20existing%20models%20are%20typically%20trained%20without%20supervision%20that%20explicitly%20encodes%20these%20relational%20dependencies%2C%20limiting%20their%20ability%20to%20infer%20how%20agents%20and%20other%20traffic%20entities%20influence%20one%20another%20from%20raw%20sensor%20data.%20In%20this%20work%2C%20we%20bridge%20this%20gap%20with%20a%20novel%20model-agnostic%20method%20that%20conditions%20language-based%20driving%20models%20on%20structured%20relational%20context%20in%20the%20form%20of%20traffic%20scene%20graphs.%20We%20serialize%20scene%20graphs%20at%20various%20abstraction%20levels%20and%20formats%2C%20and%20incorporate%20them%20into%20the%20models%20via%20structured%20prompt%20templates%2C%20enabling%20a%20systematic%20analysis%20of%20when%20and%20how%20relational%20supervision%20is%20most%20beneficial.%20Extensive%20evaluations%20on%20the%20public%20LangAuto%20benchmark%20show%20that%20scene%20graph%20conditioning%20of%20state-of-the-art%20approaches%20yields%20large%20and%20persistent%20improvement%20in%20driving%20performance.%20Notably%2C%20we%20observe%20up%20to%20a%2015.6%5C%25%20increase%20in%20driving%20score%20for%20LMDrive%20and%2017.5%5C%25%20for%20BEVDriver%2C%20indicating%20that%20models%20can%20better%20internalize%20and%20ground%20relational%20priors%20through%20scene%20graph-conditioned%20training%2C%20even%20without%20requiring%20scene%20graph%20input%20at%20test-time.%20Code%2C%20fine-tuned%20models%2C%20and%20our%20scene%20graph%20dataset%20are%20publicly%20available%20at%20https%3A//github.com/iis-esslingen/GraphPilot.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphPilot%253A%2520Grounded%2520Scene%2520Graph%2520Conditioning%2520for%2520Language-Based%2520Autonomous%2520Driving%26entry.906535625%3DFabian%2520Schmidt%2520and%2520Markus%2520Enzweiler%2520and%2520Abhinav%2520Valada%26entry.1292438233%3DVision-language%2520models%2520have%2520recently%2520emerged%2520as%2520promising%2520planners%2520for%2520autonomous%2520driving%252C%2520where%2520success%2520hinges%2520on%2520topology-aware%2520reasoning%2520over%2520spatial%2520structure%2520and%2520dynamic%2520interactions%2520from%2520multimodal%2520input.%2520However%252C%2520existing%2520models%2520are%2520typically%2520trained%2520without%2520supervision%2520that%2520explicitly%2520encodes%2520these%2520relational%2520dependencies%252C%2520limiting%2520their%2520ability%2520to%2520infer%2520how%2520agents%2520and%2520other%2520traffic%2520entities%2520influence%2520one%2520another%2520from%2520raw%2520sensor%2520data.%2520In%2520this%2520work%252C%2520we%2520bridge%2520this%2520gap%2520with%2520a%2520novel%2520model-agnostic%2520method%2520that%2520conditions%2520language-based%2520driving%2520models%2520on%2520structured%2520relational%2520context%2520in%2520the%2520form%2520of%2520traffic%2520scene%2520graphs.%2520We%2520serialize%2520scene%2520graphs%2520at%2520various%2520abstraction%2520levels%2520and%2520formats%252C%2520and%2520incorporate%2520them%2520into%2520the%2520models%2520via%2520structured%2520prompt%2520templates%252C%2520enabling%2520a%2520systematic%2520analysis%2520of%2520when%2520and%2520how%2520relational%2520supervision%2520is%2520most%2520beneficial.%2520Extensive%2520evaluations%2520on%2520the%2520public%2520LangAuto%2520benchmark%2520show%2520that%2520scene%2520graph%2520conditioning%2520of%2520state-of-the-art%2520approaches%2520yields%2520large%2520and%2520persistent%2520improvement%2520in%2520driving%2520performance.%2520Notably%252C%2520we%2520observe%2520up%2520to%2520a%252015.6%255C%2525%2520increase%2520in%2520driving%2520score%2520for%2520LMDrive%2520and%252017.5%255C%2525%2520for%2520BEVDriver%252C%2520indicating%2520that%2520models%2520can%2520better%2520internalize%2520and%2520ground%2520relational%2520priors%2520through%2520scene%2520graph-conditioned%2520training%252C%2520even%2520without%2520requiring%2520scene%2520graph%2520input%2520at%2520test-time.%2520Code%252C%2520fine-tuned%2520models%252C%2520and%2520our%2520scene%2520graph%2520dataset%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/iis-esslingen/GraphPilot.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphPilot%3A%20Grounded%20Scene%20Graph%20Conditioning%20for%20Language-Based%20Autonomous%20Driving&entry.906535625=Fabian%20Schmidt%20and%20Markus%20Enzweiler%20and%20Abhinav%20Valada&entry.1292438233=Vision-language%20models%20have%20recently%20emerged%20as%20promising%20planners%20for%20autonomous%20driving%2C%20where%20success%20hinges%20on%20topology-aware%20reasoning%20over%20spatial%20structure%20and%20dynamic%20interactions%20from%20multimodal%20input.%20However%2C%20existing%20models%20are%20typically%20trained%20without%20supervision%20that%20explicitly%20encodes%20these%20relational%20dependencies%2C%20limiting%20their%20ability%20to%20infer%20how%20agents%20and%20other%20traffic%20entities%20influence%20one%20another%20from%20raw%20sensor%20data.%20In%20this%20work%2C%20we%20bridge%20this%20gap%20with%20a%20novel%20model-agnostic%20method%20that%20conditions%20language-based%20driving%20models%20on%20structured%20relational%20context%20in%20the%20form%20of%20traffic%20scene%20graphs.%20We%20serialize%20scene%20graphs%20at%20various%20abstraction%20levels%20and%20formats%2C%20and%20incorporate%20them%20into%20the%20models%20via%20structured%20prompt%20templates%2C%20enabling%20a%20systematic%20analysis%20of%20when%20and%20how%20relational%20supervision%20is%20most%20beneficial.%20Extensive%20evaluations%20on%20the%20public%20LangAuto%20benchmark%20show%20that%20scene%20graph%20conditioning%20of%20state-of-the-art%20approaches%20yields%20large%20and%20persistent%20improvement%20in%20driving%20performance.%20Notably%2C%20we%20observe%20up%20to%20a%2015.6%5C%25%20increase%20in%20driving%20score%20for%20LMDrive%20and%2017.5%5C%25%20for%20BEVDriver%2C%20indicating%20that%20models%20can%20better%20internalize%20and%20ground%20relational%20priors%20through%20scene%20graph-conditioned%20training%2C%20even%20without%20requiring%20scene%20graph%20input%20at%20test-time.%20Code%2C%20fine-tuned%20models%2C%20and%20our%20scene%20graph%20dataset%20are%20publicly%20available%20at%20https%3A//github.com/iis-esslingen/GraphPilot.&entry.1838667208=http%3A//arxiv.org/abs/2511.11266v1&entry.124074799=Read"},
{"title": "Humanoid Whole-Body Badminton via Multi-Stage Reinforcement Learning", "author": "Chenhao Liu and Leyun Jiang and Yibo Wang and Kairan Yao and Jinchen Fu and Xiaoyu Ren", "abstract": "Humanoid robots have demonstrated strong capability for interacting with deterministic scenes across locomotion, manipulation, and more challenging loco-manipulation tasks. Yet the real world is dynamic, quasi-static interactions are insufficient to cope with the various environmental conditions. As a step toward more dynamic interaction scenario, we present a reinforcement-learning-based training pipeline that produces a unified whole-body controller for humanoid badminton, enabling coordinated lower-body footwork and upper-body striking without any motion priors or expert demonstrations. Training follows a three-stage curriculum: first footwork acquisition, then precision-guided racket swing generation, and finally task-focused refinement, yielding motions in which both legs and arms serve the hitting objective. For deployment, we incorporate an Extended Kalman Filter (EKF) to estimate and predict shuttlecock trajectories for target striking. We also introduce a prediction-free variant that dispenses with EKF and explicit trajectory prediction. To validate the framework, we conduct five sets of experiment in both simulation and the real world. In simulation, two robots sustain a rally of 21 consecutive hits. Moreover, the prediction-free variant achieves successful hits with comparable performance relative to the target-known policy. In real-world tests, both the prediction and controller module exhibit high accuracy, and on-court hitting achieves an outgoing shuttle speed up to 10 m/s with a mean return landing distance of 3.5 m. These experiment results show that our humanoid robot can deliver highly dynamic while precise goal striking in badminton, and can be adapted to more dynamism critical domains.", "link": "http://arxiv.org/abs/2511.11218v1", "date": "2025-11-14", "relevancy": 2.3204, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6051}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5753}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Humanoid%20Whole-Body%20Badminton%20via%20Multi-Stage%20Reinforcement%20Learning&body=Title%3A%20Humanoid%20Whole-Body%20Badminton%20via%20Multi-Stage%20Reinforcement%20Learning%0AAuthor%3A%20Chenhao%20Liu%20and%20Leyun%20Jiang%20and%20Yibo%20Wang%20and%20Kairan%20Yao%20and%20Jinchen%20Fu%20and%20Xiaoyu%20Ren%0AAbstract%3A%20Humanoid%20robots%20have%20demonstrated%20strong%20capability%20for%20interacting%20with%20deterministic%20scenes%20across%20locomotion%2C%20manipulation%2C%20and%20more%20challenging%20loco-manipulation%20tasks.%20Yet%20the%20real%20world%20is%20dynamic%2C%20quasi-static%20interactions%20are%20insufficient%20to%20cope%20with%20the%20various%20environmental%20conditions.%20As%20a%20step%20toward%20more%20dynamic%20interaction%20scenario%2C%20we%20present%20a%20reinforcement-learning-based%20training%20pipeline%20that%20produces%20a%20unified%20whole-body%20controller%20for%20humanoid%20badminton%2C%20enabling%20coordinated%20lower-body%20footwork%20and%20upper-body%20striking%20without%20any%20motion%20priors%20or%20expert%20demonstrations.%20Training%20follows%20a%20three-stage%20curriculum%3A%20first%20footwork%20acquisition%2C%20then%20precision-guided%20racket%20swing%20generation%2C%20and%20finally%20task-focused%20refinement%2C%20yielding%20motions%20in%20which%20both%20legs%20and%20arms%20serve%20the%20hitting%20objective.%20For%20deployment%2C%20we%20incorporate%20an%20Extended%20Kalman%20Filter%20%28EKF%29%20to%20estimate%20and%20predict%20shuttlecock%20trajectories%20for%20target%20striking.%20We%20also%20introduce%20a%20prediction-free%20variant%20that%20dispenses%20with%20EKF%20and%20explicit%20trajectory%20prediction.%20To%20validate%20the%20framework%2C%20we%20conduct%20five%20sets%20of%20experiment%20in%20both%20simulation%20and%20the%20real%20world.%20In%20simulation%2C%20two%20robots%20sustain%20a%20rally%20of%2021%20consecutive%20hits.%20Moreover%2C%20the%20prediction-free%20variant%20achieves%20successful%20hits%20with%20comparable%20performance%20relative%20to%20the%20target-known%20policy.%20In%20real-world%20tests%2C%20both%20the%20prediction%20and%20controller%20module%20exhibit%20high%20accuracy%2C%20and%20on-court%20hitting%20achieves%20an%20outgoing%20shuttle%20speed%20up%20to%2010%20m/s%20with%20a%20mean%20return%20landing%20distance%20of%203.5%20m.%20These%20experiment%20results%20show%20that%20our%20humanoid%20robot%20can%20deliver%20highly%20dynamic%20while%20precise%20goal%20striking%20in%20badminton%2C%20and%20can%20be%20adapted%20to%20more%20dynamism%20critical%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanoid%2520Whole-Body%2520Badminton%2520via%2520Multi-Stage%2520Reinforcement%2520Learning%26entry.906535625%3DChenhao%2520Liu%2520and%2520Leyun%2520Jiang%2520and%2520Yibo%2520Wang%2520and%2520Kairan%2520Yao%2520and%2520Jinchen%2520Fu%2520and%2520Xiaoyu%2520Ren%26entry.1292438233%3DHumanoid%2520robots%2520have%2520demonstrated%2520strong%2520capability%2520for%2520interacting%2520with%2520deterministic%2520scenes%2520across%2520locomotion%252C%2520manipulation%252C%2520and%2520more%2520challenging%2520loco-manipulation%2520tasks.%2520Yet%2520the%2520real%2520world%2520is%2520dynamic%252C%2520quasi-static%2520interactions%2520are%2520insufficient%2520to%2520cope%2520with%2520the%2520various%2520environmental%2520conditions.%2520As%2520a%2520step%2520toward%2520more%2520dynamic%2520interaction%2520scenario%252C%2520we%2520present%2520a%2520reinforcement-learning-based%2520training%2520pipeline%2520that%2520produces%2520a%2520unified%2520whole-body%2520controller%2520for%2520humanoid%2520badminton%252C%2520enabling%2520coordinated%2520lower-body%2520footwork%2520and%2520upper-body%2520striking%2520without%2520any%2520motion%2520priors%2520or%2520expert%2520demonstrations.%2520Training%2520follows%2520a%2520three-stage%2520curriculum%253A%2520first%2520footwork%2520acquisition%252C%2520then%2520precision-guided%2520racket%2520swing%2520generation%252C%2520and%2520finally%2520task-focused%2520refinement%252C%2520yielding%2520motions%2520in%2520which%2520both%2520legs%2520and%2520arms%2520serve%2520the%2520hitting%2520objective.%2520For%2520deployment%252C%2520we%2520incorporate%2520an%2520Extended%2520Kalman%2520Filter%2520%2528EKF%2529%2520to%2520estimate%2520and%2520predict%2520shuttlecock%2520trajectories%2520for%2520target%2520striking.%2520We%2520also%2520introduce%2520a%2520prediction-free%2520variant%2520that%2520dispenses%2520with%2520EKF%2520and%2520explicit%2520trajectory%2520prediction.%2520To%2520validate%2520the%2520framework%252C%2520we%2520conduct%2520five%2520sets%2520of%2520experiment%2520in%2520both%2520simulation%2520and%2520the%2520real%2520world.%2520In%2520simulation%252C%2520two%2520robots%2520sustain%2520a%2520rally%2520of%252021%2520consecutive%2520hits.%2520Moreover%252C%2520the%2520prediction-free%2520variant%2520achieves%2520successful%2520hits%2520with%2520comparable%2520performance%2520relative%2520to%2520the%2520target-known%2520policy.%2520In%2520real-world%2520tests%252C%2520both%2520the%2520prediction%2520and%2520controller%2520module%2520exhibit%2520high%2520accuracy%252C%2520and%2520on-court%2520hitting%2520achieves%2520an%2520outgoing%2520shuttle%2520speed%2520up%2520to%252010%2520m/s%2520with%2520a%2520mean%2520return%2520landing%2520distance%2520of%25203.5%2520m.%2520These%2520experiment%2520results%2520show%2520that%2520our%2520humanoid%2520robot%2520can%2520deliver%2520highly%2520dynamic%2520while%2520precise%2520goal%2520striking%2520in%2520badminton%252C%2520and%2520can%2520be%2520adapted%2520to%2520more%2520dynamism%2520critical%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Humanoid%20Whole-Body%20Badminton%20via%20Multi-Stage%20Reinforcement%20Learning&entry.906535625=Chenhao%20Liu%20and%20Leyun%20Jiang%20and%20Yibo%20Wang%20and%20Kairan%20Yao%20and%20Jinchen%20Fu%20and%20Xiaoyu%20Ren&entry.1292438233=Humanoid%20robots%20have%20demonstrated%20strong%20capability%20for%20interacting%20with%20deterministic%20scenes%20across%20locomotion%2C%20manipulation%2C%20and%20more%20challenging%20loco-manipulation%20tasks.%20Yet%20the%20real%20world%20is%20dynamic%2C%20quasi-static%20interactions%20are%20insufficient%20to%20cope%20with%20the%20various%20environmental%20conditions.%20As%20a%20step%20toward%20more%20dynamic%20interaction%20scenario%2C%20we%20present%20a%20reinforcement-learning-based%20training%20pipeline%20that%20produces%20a%20unified%20whole-body%20controller%20for%20humanoid%20badminton%2C%20enabling%20coordinated%20lower-body%20footwork%20and%20upper-body%20striking%20without%20any%20motion%20priors%20or%20expert%20demonstrations.%20Training%20follows%20a%20three-stage%20curriculum%3A%20first%20footwork%20acquisition%2C%20then%20precision-guided%20racket%20swing%20generation%2C%20and%20finally%20task-focused%20refinement%2C%20yielding%20motions%20in%20which%20both%20legs%20and%20arms%20serve%20the%20hitting%20objective.%20For%20deployment%2C%20we%20incorporate%20an%20Extended%20Kalman%20Filter%20%28EKF%29%20to%20estimate%20and%20predict%20shuttlecock%20trajectories%20for%20target%20striking.%20We%20also%20introduce%20a%20prediction-free%20variant%20that%20dispenses%20with%20EKF%20and%20explicit%20trajectory%20prediction.%20To%20validate%20the%20framework%2C%20we%20conduct%20five%20sets%20of%20experiment%20in%20both%20simulation%20and%20the%20real%20world.%20In%20simulation%2C%20two%20robots%20sustain%20a%20rally%20of%2021%20consecutive%20hits.%20Moreover%2C%20the%20prediction-free%20variant%20achieves%20successful%20hits%20with%20comparable%20performance%20relative%20to%20the%20target-known%20policy.%20In%20real-world%20tests%2C%20both%20the%20prediction%20and%20controller%20module%20exhibit%20high%20accuracy%2C%20and%20on-court%20hitting%20achieves%20an%20outgoing%20shuttle%20speed%20up%20to%2010%20m/s%20with%20a%20mean%20return%20landing%20distance%20of%203.5%20m.%20These%20experiment%20results%20show%20that%20our%20humanoid%20robot%20can%20deliver%20highly%20dynamic%20while%20precise%20goal%20striking%20in%20badminton%2C%20and%20can%20be%20adapted%20to%20more%20dynamism%20critical%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2511.11218v1&entry.124074799=Read"},
{"title": "Heterogeneous Attributed Graph Learning via Neighborhood-Aware Star Kernels", "author": "Hong Huang and Chengyu Yao and Haiming Chen and Hang Gao", "abstract": "Attributed graphs, typically characterized by irregular topologies and a mix of numerical and categorical attributes, are ubiquitous in diverse domains such as social networks, bioinformatics, and cheminformatics. While graph kernels provide a principled framework for measuring graph similarity, existing kernel methods often struggle to simultaneously capture heterogeneous attribute semantics and neighborhood information in attributed graphs. In this work, we propose the Neighborhood-Aware Star Kernel (NASK), a novel graph kernel designed for attributed graph learning. NASK leverages an exponential transformation of the Gower similarity coefficient to jointly model numerical and categorical features efficiently, and employs star substructures enhanced by Weisfeiler-Lehman iterations to integrate multi-scale neighborhood structural information. We theoretically prove that NASK is positive definite, ensuring compatibility with kernel-based learning frameworks such as SVMs. Extensive experiments are conducted on eleven attributed and four large-scale real-world graph benchmarks. The results demonstrate that NASK consistently achieves superior performance over sixteen state-of-the-art baselines, including nine graph kernels and seven Graph Neural Networks.", "link": "http://arxiv.org/abs/2511.11245v1", "date": "2025-11-14", "relevancy": 2.3195, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4696}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.467}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterogeneous%20Attributed%20Graph%20Learning%20via%20Neighborhood-Aware%20Star%20Kernels&body=Title%3A%20Heterogeneous%20Attributed%20Graph%20Learning%20via%20Neighborhood-Aware%20Star%20Kernels%0AAuthor%3A%20Hong%20Huang%20and%20Chengyu%20Yao%20and%20Haiming%20Chen%20and%20Hang%20Gao%0AAbstract%3A%20Attributed%20graphs%2C%20typically%20characterized%20by%20irregular%20topologies%20and%20a%20mix%20of%20numerical%20and%20categorical%20attributes%2C%20are%20ubiquitous%20in%20diverse%20domains%20such%20as%20social%20networks%2C%20bioinformatics%2C%20and%20cheminformatics.%20While%20graph%20kernels%20provide%20a%20principled%20framework%20for%20measuring%20graph%20similarity%2C%20existing%20kernel%20methods%20often%20struggle%20to%20simultaneously%20capture%20heterogeneous%20attribute%20semantics%20and%20neighborhood%20information%20in%20attributed%20graphs.%20In%20this%20work%2C%20we%20propose%20the%20Neighborhood-Aware%20Star%20Kernel%20%28NASK%29%2C%20a%20novel%20graph%20kernel%20designed%20for%20attributed%20graph%20learning.%20NASK%20leverages%20an%20exponential%20transformation%20of%20the%20Gower%20similarity%20coefficient%20to%20jointly%20model%20numerical%20and%20categorical%20features%20efficiently%2C%20and%20employs%20star%20substructures%20enhanced%20by%20Weisfeiler-Lehman%20iterations%20to%20integrate%20multi-scale%20neighborhood%20structural%20information.%20We%20theoretically%20prove%20that%20NASK%20is%20positive%20definite%2C%20ensuring%20compatibility%20with%20kernel-based%20learning%20frameworks%20such%20as%20SVMs.%20Extensive%20experiments%20are%20conducted%20on%20eleven%20attributed%20and%20four%20large-scale%20real-world%20graph%20benchmarks.%20The%20results%20demonstrate%20that%20NASK%20consistently%20achieves%20superior%20performance%20over%20sixteen%20state-of-the-art%20baselines%2C%20including%20nine%20graph%20kernels%20and%20seven%20Graph%20Neural%20Networks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11245v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterogeneous%2520Attributed%2520Graph%2520Learning%2520via%2520Neighborhood-Aware%2520Star%2520Kernels%26entry.906535625%3DHong%2520Huang%2520and%2520Chengyu%2520Yao%2520and%2520Haiming%2520Chen%2520and%2520Hang%2520Gao%26entry.1292438233%3DAttributed%2520graphs%252C%2520typically%2520characterized%2520by%2520irregular%2520topologies%2520and%2520a%2520mix%2520of%2520numerical%2520and%2520categorical%2520attributes%252C%2520are%2520ubiquitous%2520in%2520diverse%2520domains%2520such%2520as%2520social%2520networks%252C%2520bioinformatics%252C%2520and%2520cheminformatics.%2520While%2520graph%2520kernels%2520provide%2520a%2520principled%2520framework%2520for%2520measuring%2520graph%2520similarity%252C%2520existing%2520kernel%2520methods%2520often%2520struggle%2520to%2520simultaneously%2520capture%2520heterogeneous%2520attribute%2520semantics%2520and%2520neighborhood%2520information%2520in%2520attributed%2520graphs.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520Neighborhood-Aware%2520Star%2520Kernel%2520%2528NASK%2529%252C%2520a%2520novel%2520graph%2520kernel%2520designed%2520for%2520attributed%2520graph%2520learning.%2520NASK%2520leverages%2520an%2520exponential%2520transformation%2520of%2520the%2520Gower%2520similarity%2520coefficient%2520to%2520jointly%2520model%2520numerical%2520and%2520categorical%2520features%2520efficiently%252C%2520and%2520employs%2520star%2520substructures%2520enhanced%2520by%2520Weisfeiler-Lehman%2520iterations%2520to%2520integrate%2520multi-scale%2520neighborhood%2520structural%2520information.%2520We%2520theoretically%2520prove%2520that%2520NASK%2520is%2520positive%2520definite%252C%2520ensuring%2520compatibility%2520with%2520kernel-based%2520learning%2520frameworks%2520such%2520as%2520SVMs.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520eleven%2520attributed%2520and%2520four%2520large-scale%2520real-world%2520graph%2520benchmarks.%2520The%2520results%2520demonstrate%2520that%2520NASK%2520consistently%2520achieves%2520superior%2520performance%2520over%2520sixteen%2520state-of-the-art%2520baselines%252C%2520including%2520nine%2520graph%2520kernels%2520and%2520seven%2520Graph%2520Neural%2520Networks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11245v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneous%20Attributed%20Graph%20Learning%20via%20Neighborhood-Aware%20Star%20Kernels&entry.906535625=Hong%20Huang%20and%20Chengyu%20Yao%20and%20Haiming%20Chen%20and%20Hang%20Gao&entry.1292438233=Attributed%20graphs%2C%20typically%20characterized%20by%20irregular%20topologies%20and%20a%20mix%20of%20numerical%20and%20categorical%20attributes%2C%20are%20ubiquitous%20in%20diverse%20domains%20such%20as%20social%20networks%2C%20bioinformatics%2C%20and%20cheminformatics.%20While%20graph%20kernels%20provide%20a%20principled%20framework%20for%20measuring%20graph%20similarity%2C%20existing%20kernel%20methods%20often%20struggle%20to%20simultaneously%20capture%20heterogeneous%20attribute%20semantics%20and%20neighborhood%20information%20in%20attributed%20graphs.%20In%20this%20work%2C%20we%20propose%20the%20Neighborhood-Aware%20Star%20Kernel%20%28NASK%29%2C%20a%20novel%20graph%20kernel%20designed%20for%20attributed%20graph%20learning.%20NASK%20leverages%20an%20exponential%20transformation%20of%20the%20Gower%20similarity%20coefficient%20to%20jointly%20model%20numerical%20and%20categorical%20features%20efficiently%2C%20and%20employs%20star%20substructures%20enhanced%20by%20Weisfeiler-Lehman%20iterations%20to%20integrate%20multi-scale%20neighborhood%20structural%20information.%20We%20theoretically%20prove%20that%20NASK%20is%20positive%20definite%2C%20ensuring%20compatibility%20with%20kernel-based%20learning%20frameworks%20such%20as%20SVMs.%20Extensive%20experiments%20are%20conducted%20on%20eleven%20attributed%20and%20four%20large-scale%20real-world%20graph%20benchmarks.%20The%20results%20demonstrate%20that%20NASK%20consistently%20achieves%20superior%20performance%20over%20sixteen%20state-of-the-art%20baselines%2C%20including%20nine%20graph%20kernels%20and%20seven%20Graph%20Neural%20Networks.&entry.1838667208=http%3A//arxiv.org/abs/2511.11245v1&entry.124074799=Read"},
{"title": "GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models", "author": "Jingxuan Wei and Caijun Jia and Xi Bai and Xinglong Xu and Siyuan Li and Linzhuang Sun and Bihui Yu and Conghui He and Lijun Wu and Cheng Tan", "abstract": "The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.", "link": "http://arxiv.org/abs/2511.11134v1", "date": "2025-11-14", "relevancy": 2.3185, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6069}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5769}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GGBench%3A%20A%20Geometric%20Generative%20Reasoning%20Benchmark%20for%20Unified%20Multimodal%20Models&body=Title%3A%20GGBench%3A%20A%20Geometric%20Generative%20Reasoning%20Benchmark%20for%20Unified%20Multimodal%20Models%0AAuthor%3A%20Jingxuan%20Wei%20and%20Caijun%20Jia%20and%20Xi%20Bai%20and%20Xinglong%20Xu%20and%20Siyuan%20Li%20and%20Linzhuang%20Sun%20and%20Bihui%20Yu%20and%20Conghui%20He%20and%20Lijun%20Wu%20and%20Cheng%20Tan%0AAbstract%3A%20The%20advent%20of%20Unified%20Multimodal%20Models%20%28UMMs%29%20signals%20a%20paradigm%20shift%20in%20artificial%20intelligence%2C%20moving%20from%20passive%20perception%20to%20active%2C%20cross-modal%20generation.%20Despite%20their%20unprecedented%20ability%20to%20synthesize%20information%2C%20a%20critical%20gap%20persists%20in%20evaluation%3A%20existing%20benchmarks%20primarily%20assess%20discriminative%20understanding%20or%20unconstrained%20image%20generation%20separately%2C%20failing%20to%20measure%20the%20integrated%20cognitive%20process%20of%20generative%20reasoning.%20To%20bridge%20this%20gap%2C%20we%20propose%20that%20geometric%20construction%20provides%20an%20ideal%20testbed%20as%20it%20inherently%20demands%20a%20fusion%20of%20language%20comprehension%20and%20precise%20visual%20generation.%20We%20introduce%20GGBench%2C%20a%20benchmark%20designed%20specifically%20to%20evaluate%20geometric%20generative%20reasoning.%20It%20provides%20a%20comprehensive%20framework%20for%20systematically%20diagnosing%20a%20model%27s%20ability%20to%20not%20only%20understand%20and%20reason%20but%20to%20actively%20construct%20a%20solution%2C%20thereby%20setting%20a%20more%20rigorous%20standard%20for%20the%20next%20generation%20of%20intelligent%20systems.%20Project%20website%3A%20https%3A//opendatalab-raiser.github.io/GGBench/.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11134v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGGBench%253A%2520A%2520Geometric%2520Generative%2520Reasoning%2520Benchmark%2520for%2520Unified%2520Multimodal%2520Models%26entry.906535625%3DJingxuan%2520Wei%2520and%2520Caijun%2520Jia%2520and%2520Xi%2520Bai%2520and%2520Xinglong%2520Xu%2520and%2520Siyuan%2520Li%2520and%2520Linzhuang%2520Sun%2520and%2520Bihui%2520Yu%2520and%2520Conghui%2520He%2520and%2520Lijun%2520Wu%2520and%2520Cheng%2520Tan%26entry.1292438233%3DThe%2520advent%2520of%2520Unified%2520Multimodal%2520Models%2520%2528UMMs%2529%2520signals%2520a%2520paradigm%2520shift%2520in%2520artificial%2520intelligence%252C%2520moving%2520from%2520passive%2520perception%2520to%2520active%252C%2520cross-modal%2520generation.%2520Despite%2520their%2520unprecedented%2520ability%2520to%2520synthesize%2520information%252C%2520a%2520critical%2520gap%2520persists%2520in%2520evaluation%253A%2520existing%2520benchmarks%2520primarily%2520assess%2520discriminative%2520understanding%2520or%2520unconstrained%2520image%2520generation%2520separately%252C%2520failing%2520to%2520measure%2520the%2520integrated%2520cognitive%2520process%2520of%2520generative%2520reasoning.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520that%2520geometric%2520construction%2520provides%2520an%2520ideal%2520testbed%2520as%2520it%2520inherently%2520demands%2520a%2520fusion%2520of%2520language%2520comprehension%2520and%2520precise%2520visual%2520generation.%2520We%2520introduce%2520GGBench%252C%2520a%2520benchmark%2520designed%2520specifically%2520to%2520evaluate%2520geometric%2520generative%2520reasoning.%2520It%2520provides%2520a%2520comprehensive%2520framework%2520for%2520systematically%2520diagnosing%2520a%2520model%2527s%2520ability%2520to%2520not%2520only%2520understand%2520and%2520reason%2520but%2520to%2520actively%2520construct%2520a%2520solution%252C%2520thereby%2520setting%2520a%2520more%2520rigorous%2520standard%2520for%2520the%2520next%2520generation%2520of%2520intelligent%2520systems.%2520Project%2520website%253A%2520https%253A//opendatalab-raiser.github.io/GGBench/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11134v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GGBench%3A%20A%20Geometric%20Generative%20Reasoning%20Benchmark%20for%20Unified%20Multimodal%20Models&entry.906535625=Jingxuan%20Wei%20and%20Caijun%20Jia%20and%20Xi%20Bai%20and%20Xinglong%20Xu%20and%20Siyuan%20Li%20and%20Linzhuang%20Sun%20and%20Bihui%20Yu%20and%20Conghui%20He%20and%20Lijun%20Wu%20and%20Cheng%20Tan&entry.1292438233=The%20advent%20of%20Unified%20Multimodal%20Models%20%28UMMs%29%20signals%20a%20paradigm%20shift%20in%20artificial%20intelligence%2C%20moving%20from%20passive%20perception%20to%20active%2C%20cross-modal%20generation.%20Despite%20their%20unprecedented%20ability%20to%20synthesize%20information%2C%20a%20critical%20gap%20persists%20in%20evaluation%3A%20existing%20benchmarks%20primarily%20assess%20discriminative%20understanding%20or%20unconstrained%20image%20generation%20separately%2C%20failing%20to%20measure%20the%20integrated%20cognitive%20process%20of%20generative%20reasoning.%20To%20bridge%20this%20gap%2C%20we%20propose%20that%20geometric%20construction%20provides%20an%20ideal%20testbed%20as%20it%20inherently%20demands%20a%20fusion%20of%20language%20comprehension%20and%20precise%20visual%20generation.%20We%20introduce%20GGBench%2C%20a%20benchmark%20designed%20specifically%20to%20evaluate%20geometric%20generative%20reasoning.%20It%20provides%20a%20comprehensive%20framework%20for%20systematically%20diagnosing%20a%20model%27s%20ability%20to%20not%20only%20understand%20and%20reason%20but%20to%20actively%20construct%20a%20solution%2C%20thereby%20setting%20a%20more%20rigorous%20standard%20for%20the%20next%20generation%20of%20intelligent%20systems.%20Project%20website%3A%20https%3A//opendatalab-raiser.github.io/GGBench/.&entry.1838667208=http%3A//arxiv.org/abs/2511.11134v1&entry.124074799=Read"},
{"title": "Preserving Task-Relevant Information Under Linear Concept Removal", "author": "Floris Holstege and Shauli Ravfogel and Bram Wouters", "abstract": "Modern neural networks often encode unwanted concepts alongside task-relevant information, leading to fairness and interpretability concerns. Existing post-hoc approaches can remove undesired concepts but often degrade useful signals. We introduce SPLINCE-Simultaneous Projection for LINear concept removal and Covariance prEservation - which eliminates sensitive concepts from representations while exactly preserving their covariance with a target label. SPLINCE achieves this via an oblique projection that 'splices out' the unwanted direction yet protects important label correlations. Theoretically, it is the unique solution that removes linear concept predictability and maintains target covariance with minimal embedding distortion. Empirically, SPLINCE outperforms baselines on benchmarks such as Bias in Bios and Winobias, removing protected attributes while minimally damaging main-task information.", "link": "http://arxiv.org/abs/2506.10703v2", "date": "2025-11-14", "relevancy": 2.3115, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4675}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4675}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preserving%20Task-Relevant%20Information%20Under%20Linear%20Concept%20Removal&body=Title%3A%20Preserving%20Task-Relevant%20Information%20Under%20Linear%20Concept%20Removal%0AAuthor%3A%20Floris%20Holstege%20and%20Shauli%20Ravfogel%20and%20Bram%20Wouters%0AAbstract%3A%20Modern%20neural%20networks%20often%20encode%20unwanted%20concepts%20alongside%20task-relevant%20information%2C%20leading%20to%20fairness%20and%20interpretability%20concerns.%20Existing%20post-hoc%20approaches%20can%20remove%20undesired%20concepts%20but%20often%20degrade%20useful%20signals.%20We%20introduce%20SPLINCE-Simultaneous%20Projection%20for%20LINear%20concept%20removal%20and%20Covariance%20prEservation%20-%20which%20eliminates%20sensitive%20concepts%20from%20representations%20while%20exactly%20preserving%20their%20covariance%20with%20a%20target%20label.%20SPLINCE%20achieves%20this%20via%20an%20oblique%20projection%20that%20%27splices%20out%27%20the%20unwanted%20direction%20yet%20protects%20important%20label%20correlations.%20Theoretically%2C%20it%20is%20the%20unique%20solution%20that%20removes%20linear%20concept%20predictability%20and%20maintains%20target%20covariance%20with%20minimal%20embedding%20distortion.%20Empirically%2C%20SPLINCE%20outperforms%20baselines%20on%20benchmarks%20such%20as%20Bias%20in%20Bios%20and%20Winobias%2C%20removing%20protected%20attributes%20while%20minimally%20damaging%20main-task%20information.%0ALink%3A%20http%3A//arxiv.org/abs/2506.10703v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreserving%2520Task-Relevant%2520Information%2520Under%2520Linear%2520Concept%2520Removal%26entry.906535625%3DFloris%2520Holstege%2520and%2520Shauli%2520Ravfogel%2520and%2520Bram%2520Wouters%26entry.1292438233%3DModern%2520neural%2520networks%2520often%2520encode%2520unwanted%2520concepts%2520alongside%2520task-relevant%2520information%252C%2520leading%2520to%2520fairness%2520and%2520interpretability%2520concerns.%2520Existing%2520post-hoc%2520approaches%2520can%2520remove%2520undesired%2520concepts%2520but%2520often%2520degrade%2520useful%2520signals.%2520We%2520introduce%2520SPLINCE-Simultaneous%2520Projection%2520for%2520LINear%2520concept%2520removal%2520and%2520Covariance%2520prEservation%2520-%2520which%2520eliminates%2520sensitive%2520concepts%2520from%2520representations%2520while%2520exactly%2520preserving%2520their%2520covariance%2520with%2520a%2520target%2520label.%2520SPLINCE%2520achieves%2520this%2520via%2520an%2520oblique%2520projection%2520that%2520%2527splices%2520out%2527%2520the%2520unwanted%2520direction%2520yet%2520protects%2520important%2520label%2520correlations.%2520Theoretically%252C%2520it%2520is%2520the%2520unique%2520solution%2520that%2520removes%2520linear%2520concept%2520predictability%2520and%2520maintains%2520target%2520covariance%2520with%2520minimal%2520embedding%2520distortion.%2520Empirically%252C%2520SPLINCE%2520outperforms%2520baselines%2520on%2520benchmarks%2520such%2520as%2520Bias%2520in%2520Bios%2520and%2520Winobias%252C%2520removing%2520protected%2520attributes%2520while%2520minimally%2520damaging%2520main-task%2520information.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10703v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preserving%20Task-Relevant%20Information%20Under%20Linear%20Concept%20Removal&entry.906535625=Floris%20Holstege%20and%20Shauli%20Ravfogel%20and%20Bram%20Wouters&entry.1292438233=Modern%20neural%20networks%20often%20encode%20unwanted%20concepts%20alongside%20task-relevant%20information%2C%20leading%20to%20fairness%20and%20interpretability%20concerns.%20Existing%20post-hoc%20approaches%20can%20remove%20undesired%20concepts%20but%20often%20degrade%20useful%20signals.%20We%20introduce%20SPLINCE-Simultaneous%20Projection%20for%20LINear%20concept%20removal%20and%20Covariance%20prEservation%20-%20which%20eliminates%20sensitive%20concepts%20from%20representations%20while%20exactly%20preserving%20their%20covariance%20with%20a%20target%20label.%20SPLINCE%20achieves%20this%20via%20an%20oblique%20projection%20that%20%27splices%20out%27%20the%20unwanted%20direction%20yet%20protects%20important%20label%20correlations.%20Theoretically%2C%20it%20is%20the%20unique%20solution%20that%20removes%20linear%20concept%20predictability%20and%20maintains%20target%20covariance%20with%20minimal%20embedding%20distortion.%20Empirically%2C%20SPLINCE%20outperforms%20baselines%20on%20benchmarks%20such%20as%20Bias%20in%20Bios%20and%20Winobias%2C%20removing%20protected%20attributes%20while%20minimally%20damaging%20main-task%20information.&entry.1838667208=http%3A//arxiv.org/abs/2506.10703v2&entry.124074799=Read"},
{"title": "One-Shot Transfer Learning for Nonlinear PDEs with Perturbative PINNs", "author": "Samuel Auroy and Pavlos Protopapas", "abstract": "We propose a framework for solving nonlinear partial differential equations (PDEs) by combining perturbation theory with one-shot transfer learning in Physics-Informed Neural Networks (PINNs). Nonlinear PDEs with polynomial terms are decomposed into a sequence of linear subproblems, which are efficiently solved using a Multi-Head PINN. Once the latent representation of the linear operator is learned, solutions to new PDE instances with varying perturbations, forcing terms, or boundary/initial conditions can be obtained in closed form without retraining.\n  We validate the method on KPP-Fisher and wave equations, achieving errors on the order of 1e-3 while adapting to new problem instances in under 0.2 seconds; comparable accuracy to classical solvers but with faster transfer. Sensitivity analyses show predictable error growth with epsilon and polynomial degree, clarifying the method's effective regime.\n  Our contributions are: (i) extending one-shot transfer learning from nonlinear ODEs to PDEs, (ii) deriving a closed-form solution for adapting to new PDE instances, and (iii) demonstrating accuracy and efficiency on canonical nonlinear PDEs. We conclude by outlining extensions to derivative-dependent nonlinearities and higher-dimensional PDEs.", "link": "http://arxiv.org/abs/2511.11137v1", "date": "2025-11-14", "relevancy": 2.3104, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4653}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4625}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One-Shot%20Transfer%20Learning%20for%20Nonlinear%20PDEs%20with%20Perturbative%20PINNs&body=Title%3A%20One-Shot%20Transfer%20Learning%20for%20Nonlinear%20PDEs%20with%20Perturbative%20PINNs%0AAuthor%3A%20Samuel%20Auroy%20and%20Pavlos%20Protopapas%0AAbstract%3A%20We%20propose%20a%20framework%20for%20solving%20nonlinear%20partial%20differential%20equations%20%28PDEs%29%20by%20combining%20perturbation%20theory%20with%20one-shot%20transfer%20learning%20in%20Physics-Informed%20Neural%20Networks%20%28PINNs%29.%20Nonlinear%20PDEs%20with%20polynomial%20terms%20are%20decomposed%20into%20a%20sequence%20of%20linear%20subproblems%2C%20which%20are%20efficiently%20solved%20using%20a%20Multi-Head%20PINN.%20Once%20the%20latent%20representation%20of%20the%20linear%20operator%20is%20learned%2C%20solutions%20to%20new%20PDE%20instances%20with%20varying%20perturbations%2C%20forcing%20terms%2C%20or%20boundary/initial%20conditions%20can%20be%20obtained%20in%20closed%20form%20without%20retraining.%0A%20%20We%20validate%20the%20method%20on%20KPP-Fisher%20and%20wave%20equations%2C%20achieving%20errors%20on%20the%20order%20of%201e-3%20while%20adapting%20to%20new%20problem%20instances%20in%20under%200.2%20seconds%3B%20comparable%20accuracy%20to%20classical%20solvers%20but%20with%20faster%20transfer.%20Sensitivity%20analyses%20show%20predictable%20error%20growth%20with%20epsilon%20and%20polynomial%20degree%2C%20clarifying%20the%20method%27s%20effective%20regime.%0A%20%20Our%20contributions%20are%3A%20%28i%29%20extending%20one-shot%20transfer%20learning%20from%20nonlinear%20ODEs%20to%20PDEs%2C%20%28ii%29%20deriving%20a%20closed-form%20solution%20for%20adapting%20to%20new%20PDE%20instances%2C%20and%20%28iii%29%20demonstrating%20accuracy%20and%20efficiency%20on%20canonical%20nonlinear%20PDEs.%20We%20conclude%20by%20outlining%20extensions%20to%20derivative-dependent%20nonlinearities%20and%20higher-dimensional%20PDEs.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne-Shot%2520Transfer%2520Learning%2520for%2520Nonlinear%2520PDEs%2520with%2520Perturbative%2520PINNs%26entry.906535625%3DSamuel%2520Auroy%2520and%2520Pavlos%2520Protopapas%26entry.1292438233%3DWe%2520propose%2520a%2520framework%2520for%2520solving%2520nonlinear%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520by%2520combining%2520perturbation%2520theory%2520with%2520one-shot%2520transfer%2520learning%2520in%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529.%2520Nonlinear%2520PDEs%2520with%2520polynomial%2520terms%2520are%2520decomposed%2520into%2520a%2520sequence%2520of%2520linear%2520subproblems%252C%2520which%2520are%2520efficiently%2520solved%2520using%2520a%2520Multi-Head%2520PINN.%2520Once%2520the%2520latent%2520representation%2520of%2520the%2520linear%2520operator%2520is%2520learned%252C%2520solutions%2520to%2520new%2520PDE%2520instances%2520with%2520varying%2520perturbations%252C%2520forcing%2520terms%252C%2520or%2520boundary/initial%2520conditions%2520can%2520be%2520obtained%2520in%2520closed%2520form%2520without%2520retraining.%250A%2520%2520We%2520validate%2520the%2520method%2520on%2520KPP-Fisher%2520and%2520wave%2520equations%252C%2520achieving%2520errors%2520on%2520the%2520order%2520of%25201e-3%2520while%2520adapting%2520to%2520new%2520problem%2520instances%2520in%2520under%25200.2%2520seconds%253B%2520comparable%2520accuracy%2520to%2520classical%2520solvers%2520but%2520with%2520faster%2520transfer.%2520Sensitivity%2520analyses%2520show%2520predictable%2520error%2520growth%2520with%2520epsilon%2520and%2520polynomial%2520degree%252C%2520clarifying%2520the%2520method%2527s%2520effective%2520regime.%250A%2520%2520Our%2520contributions%2520are%253A%2520%2528i%2529%2520extending%2520one-shot%2520transfer%2520learning%2520from%2520nonlinear%2520ODEs%2520to%2520PDEs%252C%2520%2528ii%2529%2520deriving%2520a%2520closed-form%2520solution%2520for%2520adapting%2520to%2520new%2520PDE%2520instances%252C%2520and%2520%2528iii%2529%2520demonstrating%2520accuracy%2520and%2520efficiency%2520on%2520canonical%2520nonlinear%2520PDEs.%2520We%2520conclude%2520by%2520outlining%2520extensions%2520to%2520derivative-dependent%2520nonlinearities%2520and%2520higher-dimensional%2520PDEs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Shot%20Transfer%20Learning%20for%20Nonlinear%20PDEs%20with%20Perturbative%20PINNs&entry.906535625=Samuel%20Auroy%20and%20Pavlos%20Protopapas&entry.1292438233=We%20propose%20a%20framework%20for%20solving%20nonlinear%20partial%20differential%20equations%20%28PDEs%29%20by%20combining%20perturbation%20theory%20with%20one-shot%20transfer%20learning%20in%20Physics-Informed%20Neural%20Networks%20%28PINNs%29.%20Nonlinear%20PDEs%20with%20polynomial%20terms%20are%20decomposed%20into%20a%20sequence%20of%20linear%20subproblems%2C%20which%20are%20efficiently%20solved%20using%20a%20Multi-Head%20PINN.%20Once%20the%20latent%20representation%20of%20the%20linear%20operator%20is%20learned%2C%20solutions%20to%20new%20PDE%20instances%20with%20varying%20perturbations%2C%20forcing%20terms%2C%20or%20boundary/initial%20conditions%20can%20be%20obtained%20in%20closed%20form%20without%20retraining.%0A%20%20We%20validate%20the%20method%20on%20KPP-Fisher%20and%20wave%20equations%2C%20achieving%20errors%20on%20the%20order%20of%201e-3%20while%20adapting%20to%20new%20problem%20instances%20in%20under%200.2%20seconds%3B%20comparable%20accuracy%20to%20classical%20solvers%20but%20with%20faster%20transfer.%20Sensitivity%20analyses%20show%20predictable%20error%20growth%20with%20epsilon%20and%20polynomial%20degree%2C%20clarifying%20the%20method%27s%20effective%20regime.%0A%20%20Our%20contributions%20are%3A%20%28i%29%20extending%20one-shot%20transfer%20learning%20from%20nonlinear%20ODEs%20to%20PDEs%2C%20%28ii%29%20deriving%20a%20closed-form%20solution%20for%20adapting%20to%20new%20PDE%20instances%2C%20and%20%28iii%29%20demonstrating%20accuracy%20and%20efficiency%20on%20canonical%20nonlinear%20PDEs.%20We%20conclude%20by%20outlining%20extensions%20to%20derivative-dependent%20nonlinearities%20and%20higher-dimensional%20PDEs.&entry.1838667208=http%3A//arxiv.org/abs/2511.11137v1&entry.124074799=Read"},
{"title": "OT-ALD: Aligning Latent Distributions with Optimal Transport for Accelerated Image-to-Image Translation", "author": "Zhanpeng Wang and Shuting Cao and Yuhang Lu and Yuhan Li and Na Lei and Zhongxuan Luo", "abstract": "The Dual Diffusion Implicit Bridge (DDIB) is an emerging image-to-image (I2I) translation method that preserves cycle consistency while achieving strong flexibility. It links two independently trained diffusion models (DMs) in the source and target domains by first adding noise to a source image to obtain a latent code, then denoising it in the target domain to generate the translated image. However, this method faces two key challenges: (1) low translation efficiency, and (2) translation trajectory deviations caused by mismatched latent distributions. To address these issues, we propose a novel I2I translation framework, OT-ALD, grounded in optimal transport (OT) theory, which retains the strengths of DDIB-based approach. Specifically, we compute an OT map from the latent distribution of the source domain to that of the target domain, and use the mapped distribution as the starting point for the reverse diffusion process in the target domain. Our error analysis confirms that OT-ALD eliminates latent distribution mismatches. Moreover, OT-ALD effectively balances faster image translation with improved image quality. Experiments on four translation tasks across three high-resolution datasets show that OT-ALD improves sampling efficiency by 20.29% and reduces the FID score by 2.6 on average compared to the top-performing baseline models.", "link": "http://arxiv.org/abs/2511.11162v1", "date": "2025-11-14", "relevancy": 2.3026, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6337}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5649}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OT-ALD%3A%20Aligning%20Latent%20Distributions%20with%20Optimal%20Transport%20for%20Accelerated%20Image-to-Image%20Translation&body=Title%3A%20OT-ALD%3A%20Aligning%20Latent%20Distributions%20with%20Optimal%20Transport%20for%20Accelerated%20Image-to-Image%20Translation%0AAuthor%3A%20Zhanpeng%20Wang%20and%20Shuting%20Cao%20and%20Yuhang%20Lu%20and%20Yuhan%20Li%20and%20Na%20Lei%20and%20Zhongxuan%20Luo%0AAbstract%3A%20The%20Dual%20Diffusion%20Implicit%20Bridge%20%28DDIB%29%20is%20an%20emerging%20image-to-image%20%28I2I%29%20translation%20method%20that%20preserves%20cycle%20consistency%20while%20achieving%20strong%20flexibility.%20It%20links%20two%20independently%20trained%20diffusion%20models%20%28DMs%29%20in%20the%20source%20and%20target%20domains%20by%20first%20adding%20noise%20to%20a%20source%20image%20to%20obtain%20a%20latent%20code%2C%20then%20denoising%20it%20in%20the%20target%20domain%20to%20generate%20the%20translated%20image.%20However%2C%20this%20method%20faces%20two%20key%20challenges%3A%20%281%29%20low%20translation%20efficiency%2C%20and%20%282%29%20translation%20trajectory%20deviations%20caused%20by%20mismatched%20latent%20distributions.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20I2I%20translation%20framework%2C%20OT-ALD%2C%20grounded%20in%20optimal%20transport%20%28OT%29%20theory%2C%20which%20retains%20the%20strengths%20of%20DDIB-based%20approach.%20Specifically%2C%20we%20compute%20an%20OT%20map%20from%20the%20latent%20distribution%20of%20the%20source%20domain%20to%20that%20of%20the%20target%20domain%2C%20and%20use%20the%20mapped%20distribution%20as%20the%20starting%20point%20for%20the%20reverse%20diffusion%20process%20in%20the%20target%20domain.%20Our%20error%20analysis%20confirms%20that%20OT-ALD%20eliminates%20latent%20distribution%20mismatches.%20Moreover%2C%20OT-ALD%20effectively%20balances%20faster%20image%20translation%20with%20improved%20image%20quality.%20Experiments%20on%20four%20translation%20tasks%20across%20three%20high-resolution%20datasets%20show%20that%20OT-ALD%20improves%20sampling%20efficiency%20by%2020.29%25%20and%20reduces%20the%20FID%20score%20by%202.6%20on%20average%20compared%20to%20the%20top-performing%20baseline%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11162v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOT-ALD%253A%2520Aligning%2520Latent%2520Distributions%2520with%2520Optimal%2520Transport%2520for%2520Accelerated%2520Image-to-Image%2520Translation%26entry.906535625%3DZhanpeng%2520Wang%2520and%2520Shuting%2520Cao%2520and%2520Yuhang%2520Lu%2520and%2520Yuhan%2520Li%2520and%2520Na%2520Lei%2520and%2520Zhongxuan%2520Luo%26entry.1292438233%3DThe%2520Dual%2520Diffusion%2520Implicit%2520Bridge%2520%2528DDIB%2529%2520is%2520an%2520emerging%2520image-to-image%2520%2528I2I%2529%2520translation%2520method%2520that%2520preserves%2520cycle%2520consistency%2520while%2520achieving%2520strong%2520flexibility.%2520It%2520links%2520two%2520independently%2520trained%2520diffusion%2520models%2520%2528DMs%2529%2520in%2520the%2520source%2520and%2520target%2520domains%2520by%2520first%2520adding%2520noise%2520to%2520a%2520source%2520image%2520to%2520obtain%2520a%2520latent%2520code%252C%2520then%2520denoising%2520it%2520in%2520the%2520target%2520domain%2520to%2520generate%2520the%2520translated%2520image.%2520However%252C%2520this%2520method%2520faces%2520two%2520key%2520challenges%253A%2520%25281%2529%2520low%2520translation%2520efficiency%252C%2520and%2520%25282%2529%2520translation%2520trajectory%2520deviations%2520caused%2520by%2520mismatched%2520latent%2520distributions.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520I2I%2520translation%2520framework%252C%2520OT-ALD%252C%2520grounded%2520in%2520optimal%2520transport%2520%2528OT%2529%2520theory%252C%2520which%2520retains%2520the%2520strengths%2520of%2520DDIB-based%2520approach.%2520Specifically%252C%2520we%2520compute%2520an%2520OT%2520map%2520from%2520the%2520latent%2520distribution%2520of%2520the%2520source%2520domain%2520to%2520that%2520of%2520the%2520target%2520domain%252C%2520and%2520use%2520the%2520mapped%2520distribution%2520as%2520the%2520starting%2520point%2520for%2520the%2520reverse%2520diffusion%2520process%2520in%2520the%2520target%2520domain.%2520Our%2520error%2520analysis%2520confirms%2520that%2520OT-ALD%2520eliminates%2520latent%2520distribution%2520mismatches.%2520Moreover%252C%2520OT-ALD%2520effectively%2520balances%2520faster%2520image%2520translation%2520with%2520improved%2520image%2520quality.%2520Experiments%2520on%2520four%2520translation%2520tasks%2520across%2520three%2520high-resolution%2520datasets%2520show%2520that%2520OT-ALD%2520improves%2520sampling%2520efficiency%2520by%252020.29%2525%2520and%2520reduces%2520the%2520FID%2520score%2520by%25202.6%2520on%2520average%2520compared%2520to%2520the%2520top-performing%2520baseline%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11162v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OT-ALD%3A%20Aligning%20Latent%20Distributions%20with%20Optimal%20Transport%20for%20Accelerated%20Image-to-Image%20Translation&entry.906535625=Zhanpeng%20Wang%20and%20Shuting%20Cao%20and%20Yuhang%20Lu%20and%20Yuhan%20Li%20and%20Na%20Lei%20and%20Zhongxuan%20Luo&entry.1292438233=The%20Dual%20Diffusion%20Implicit%20Bridge%20%28DDIB%29%20is%20an%20emerging%20image-to-image%20%28I2I%29%20translation%20method%20that%20preserves%20cycle%20consistency%20while%20achieving%20strong%20flexibility.%20It%20links%20two%20independently%20trained%20diffusion%20models%20%28DMs%29%20in%20the%20source%20and%20target%20domains%20by%20first%20adding%20noise%20to%20a%20source%20image%20to%20obtain%20a%20latent%20code%2C%20then%20denoising%20it%20in%20the%20target%20domain%20to%20generate%20the%20translated%20image.%20However%2C%20this%20method%20faces%20two%20key%20challenges%3A%20%281%29%20low%20translation%20efficiency%2C%20and%20%282%29%20translation%20trajectory%20deviations%20caused%20by%20mismatched%20latent%20distributions.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20I2I%20translation%20framework%2C%20OT-ALD%2C%20grounded%20in%20optimal%20transport%20%28OT%29%20theory%2C%20which%20retains%20the%20strengths%20of%20DDIB-based%20approach.%20Specifically%2C%20we%20compute%20an%20OT%20map%20from%20the%20latent%20distribution%20of%20the%20source%20domain%20to%20that%20of%20the%20target%20domain%2C%20and%20use%20the%20mapped%20distribution%20as%20the%20starting%20point%20for%20the%20reverse%20diffusion%20process%20in%20the%20target%20domain.%20Our%20error%20analysis%20confirms%20that%20OT-ALD%20eliminates%20latent%20distribution%20mismatches.%20Moreover%2C%20OT-ALD%20effectively%20balances%20faster%20image%20translation%20with%20improved%20image%20quality.%20Experiments%20on%20four%20translation%20tasks%20across%20three%20high-resolution%20datasets%20show%20that%20OT-ALD%20improves%20sampling%20efficiency%20by%2020.29%25%20and%20reduces%20the%20FID%20score%20by%202.6%20on%20average%20compared%20to%20the%20top-performing%20baseline%20models.&entry.1838667208=http%3A//arxiv.org/abs/2511.11162v1&entry.124074799=Read"},
{"title": "Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models", "author": "Jiaxi Huang and Dongxu Wu and Hanwei Zhu and Lingyu Zhu and Jun Xing and Xu Wang and Baoliang Chen", "abstract": "The rapid advancement of Multi-modal Large Language Models (MLLMs) has expanded their capabilities beyond high-level vision tasks. Nevertheless, their potential for Document Image Quality Assessment (DIQA) remains underexplored. To bridge this gap, we propose Q-Doc, a three-tiered evaluation framework for systematically probing DIQA capabilities of MLLMs at coarse, middle, and fine granularity levels. a) At the coarse level, we instruct MLLMs to assign quality scores to document images and analyze their correlation with Quality Annotations. b) At the middle level, we design distortion-type identification tasks, including single-choice and multi-choice tests for multi-distortion scenarios. c) At the fine level, we introduce distortion-severity assessment where MLLMs classify distortion intensity against human-annotated references. Our evaluation demonstrates that while MLLMs possess nascent DIQA abilities, they exhibit critical limitations: inconsistent scoring, distortion misidentification, and severity misjudgment. Significantly, we show that Chain-of-Thought (CoT) prompting substantially enhances performance across all levels. Our work provides a benchmark for DIQA capabilities in MLLMs, revealing pronounced deficiencies in their quality perception and promising pathways for enhancement. The benchmark and code are publicly available at:\n  https://github.com/cydxf/Q-Doc.", "link": "http://arxiv.org/abs/2511.11410v1", "date": "2025-11-14", "relevancy": 2.2816, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5907}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5663}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Q-Doc%3A%20Benchmarking%20Document%20Image%20Quality%20Assessment%20Capabilities%20in%20Multi-modal%20Large%20Language%20Models&body=Title%3A%20Q-Doc%3A%20Benchmarking%20Document%20Image%20Quality%20Assessment%20Capabilities%20in%20Multi-modal%20Large%20Language%20Models%0AAuthor%3A%20Jiaxi%20Huang%20and%20Dongxu%20Wu%20and%20Hanwei%20Zhu%20and%20Lingyu%20Zhu%20and%20Jun%20Xing%20and%20Xu%20Wang%20and%20Baoliang%20Chen%0AAbstract%3A%20The%20rapid%20advancement%20of%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20has%20expanded%20their%20capabilities%20beyond%20high-level%20vision%20tasks.%20Nevertheless%2C%20their%20potential%20for%20Document%20Image%20Quality%20Assessment%20%28DIQA%29%20remains%20underexplored.%20To%20bridge%20this%20gap%2C%20we%20propose%20Q-Doc%2C%20a%20three-tiered%20evaluation%20framework%20for%20systematically%20probing%20DIQA%20capabilities%20of%20MLLMs%20at%20coarse%2C%20middle%2C%20and%20fine%20granularity%20levels.%20a%29%20At%20the%20coarse%20level%2C%20we%20instruct%20MLLMs%20to%20assign%20quality%20scores%20to%20document%20images%20and%20analyze%20their%20correlation%20with%20Quality%20Annotations.%20b%29%20At%20the%20middle%20level%2C%20we%20design%20distortion-type%20identification%20tasks%2C%20including%20single-choice%20and%20multi-choice%20tests%20for%20multi-distortion%20scenarios.%20c%29%20At%20the%20fine%20level%2C%20we%20introduce%20distortion-severity%20assessment%20where%20MLLMs%20classify%20distortion%20intensity%20against%20human-annotated%20references.%20Our%20evaluation%20demonstrates%20that%20while%20MLLMs%20possess%20nascent%20DIQA%20abilities%2C%20they%20exhibit%20critical%20limitations%3A%20inconsistent%20scoring%2C%20distortion%20misidentification%2C%20and%20severity%20misjudgment.%20Significantly%2C%20we%20show%20that%20Chain-of-Thought%20%28CoT%29%20prompting%20substantially%20enhances%20performance%20across%20all%20levels.%20Our%20work%20provides%20a%20benchmark%20for%20DIQA%20capabilities%20in%20MLLMs%2C%20revealing%20pronounced%20deficiencies%20in%20their%20quality%20perception%20and%20promising%20pathways%20for%20enhancement.%20The%20benchmark%20and%20code%20are%20publicly%20available%20at%3A%0A%20%20https%3A//github.com/cydxf/Q-Doc.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11410v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQ-Doc%253A%2520Benchmarking%2520Document%2520Image%2520Quality%2520Assessment%2520Capabilities%2520in%2520Multi-modal%2520Large%2520Language%2520Models%26entry.906535625%3DJiaxi%2520Huang%2520and%2520Dongxu%2520Wu%2520and%2520Hanwei%2520Zhu%2520and%2520Lingyu%2520Zhu%2520and%2520Jun%2520Xing%2520and%2520Xu%2520Wang%2520and%2520Baoliang%2520Chen%26entry.1292438233%3DThe%2520rapid%2520advancement%2520of%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520has%2520expanded%2520their%2520capabilities%2520beyond%2520high-level%2520vision%2520tasks.%2520Nevertheless%252C%2520their%2520potential%2520for%2520Document%2520Image%2520Quality%2520Assessment%2520%2528DIQA%2529%2520remains%2520underexplored.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520Q-Doc%252C%2520a%2520three-tiered%2520evaluation%2520framework%2520for%2520systematically%2520probing%2520DIQA%2520capabilities%2520of%2520MLLMs%2520at%2520coarse%252C%2520middle%252C%2520and%2520fine%2520granularity%2520levels.%2520a%2529%2520At%2520the%2520coarse%2520level%252C%2520we%2520instruct%2520MLLMs%2520to%2520assign%2520quality%2520scores%2520to%2520document%2520images%2520and%2520analyze%2520their%2520correlation%2520with%2520Quality%2520Annotations.%2520b%2529%2520At%2520the%2520middle%2520level%252C%2520we%2520design%2520distortion-type%2520identification%2520tasks%252C%2520including%2520single-choice%2520and%2520multi-choice%2520tests%2520for%2520multi-distortion%2520scenarios.%2520c%2529%2520At%2520the%2520fine%2520level%252C%2520we%2520introduce%2520distortion-severity%2520assessment%2520where%2520MLLMs%2520classify%2520distortion%2520intensity%2520against%2520human-annotated%2520references.%2520Our%2520evaluation%2520demonstrates%2520that%2520while%2520MLLMs%2520possess%2520nascent%2520DIQA%2520abilities%252C%2520they%2520exhibit%2520critical%2520limitations%253A%2520inconsistent%2520scoring%252C%2520distortion%2520misidentification%252C%2520and%2520severity%2520misjudgment.%2520Significantly%252C%2520we%2520show%2520that%2520Chain-of-Thought%2520%2528CoT%2529%2520prompting%2520substantially%2520enhances%2520performance%2520across%2520all%2520levels.%2520Our%2520work%2520provides%2520a%2520benchmark%2520for%2520DIQA%2520capabilities%2520in%2520MLLMs%252C%2520revealing%2520pronounced%2520deficiencies%2520in%2520their%2520quality%2520perception%2520and%2520promising%2520pathways%2520for%2520enhancement.%2520The%2520benchmark%2520and%2520code%2520are%2520publicly%2520available%2520at%253A%250A%2520%2520https%253A//github.com/cydxf/Q-Doc.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11410v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q-Doc%3A%20Benchmarking%20Document%20Image%20Quality%20Assessment%20Capabilities%20in%20Multi-modal%20Large%20Language%20Models&entry.906535625=Jiaxi%20Huang%20and%20Dongxu%20Wu%20and%20Hanwei%20Zhu%20and%20Lingyu%20Zhu%20and%20Jun%20Xing%20and%20Xu%20Wang%20and%20Baoliang%20Chen&entry.1292438233=The%20rapid%20advancement%20of%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20has%20expanded%20their%20capabilities%20beyond%20high-level%20vision%20tasks.%20Nevertheless%2C%20their%20potential%20for%20Document%20Image%20Quality%20Assessment%20%28DIQA%29%20remains%20underexplored.%20To%20bridge%20this%20gap%2C%20we%20propose%20Q-Doc%2C%20a%20three-tiered%20evaluation%20framework%20for%20systematically%20probing%20DIQA%20capabilities%20of%20MLLMs%20at%20coarse%2C%20middle%2C%20and%20fine%20granularity%20levels.%20a%29%20At%20the%20coarse%20level%2C%20we%20instruct%20MLLMs%20to%20assign%20quality%20scores%20to%20document%20images%20and%20analyze%20their%20correlation%20with%20Quality%20Annotations.%20b%29%20At%20the%20middle%20level%2C%20we%20design%20distortion-type%20identification%20tasks%2C%20including%20single-choice%20and%20multi-choice%20tests%20for%20multi-distortion%20scenarios.%20c%29%20At%20the%20fine%20level%2C%20we%20introduce%20distortion-severity%20assessment%20where%20MLLMs%20classify%20distortion%20intensity%20against%20human-annotated%20references.%20Our%20evaluation%20demonstrates%20that%20while%20MLLMs%20possess%20nascent%20DIQA%20abilities%2C%20they%20exhibit%20critical%20limitations%3A%20inconsistent%20scoring%2C%20distortion%20misidentification%2C%20and%20severity%20misjudgment.%20Significantly%2C%20we%20show%20that%20Chain-of-Thought%20%28CoT%29%20prompting%20substantially%20enhances%20performance%20across%20all%20levels.%20Our%20work%20provides%20a%20benchmark%20for%20DIQA%20capabilities%20in%20MLLMs%2C%20revealing%20pronounced%20deficiencies%20in%20their%20quality%20perception%20and%20promising%20pathways%20for%20enhancement.%20The%20benchmark%20and%20code%20are%20publicly%20available%20at%3A%0A%20%20https%3A//github.com/cydxf/Q-Doc.&entry.1838667208=http%3A//arxiv.org/abs/2511.11410v1&entry.124074799=Read"},
{"title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models", "author": "Chenghao Liu and Jiachen Zhang and Chengxuan Li and Zhimu Zhou and Shixin Wu and Songfang Huang and Huiling Duan", "abstract": "Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4\\% vs 68.4\\% baseline), cross-environment validation on SimplerEnv (4.8\\% relative improvement), and 8.7\\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates.", "link": "http://arxiv.org/abs/2508.19257v3", "date": "2025-11-14", "relevancy": 2.2716, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5762}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5736}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TTF-VLA%3A%20Temporal%20Token%20Fusion%20via%20Pixel-Attention%20Integration%20for%20Vision-Language-Action%20Models&body=Title%3A%20TTF-VLA%3A%20Temporal%20Token%20Fusion%20via%20Pixel-Attention%20Integration%20for%20Vision-Language-Action%20Models%0AAuthor%3A%20Chenghao%20Liu%20and%20Jiachen%20Zhang%20and%20Chengxuan%20Li%20and%20Zhimu%20Zhou%20and%20Shixin%20Wu%20and%20Songfang%20Huang%20and%20Huiling%20Duan%0AAbstract%3A%20Vision-Language-Action%20%28VLA%29%20models%20process%20visual%20inputs%20independently%20at%20each%20timestep%2C%20discarding%20valuable%20temporal%20information%20inherent%20in%20robotic%20manipulation%20tasks.%20This%20frame-by-frame%20processing%20makes%20models%20vulnerable%20to%20visual%20noise%20while%20ignoring%20the%20substantial%20coherence%20between%20consecutive%20frames%20in%20manipulation%20sequences.%20We%20propose%20Temporal%20Token%20Fusion%20%28TTF%29%2C%20a%20training-free%20approach%20that%20intelligently%20integrates%20historical%20and%20current%20visual%20representations%20to%20enhance%20VLA%20inference%20quality.%20Our%20method%20employs%20dual-dimension%20detection%20combining%20efficient%20grayscale%20pixel%20difference%20analysis%20with%20attention-based%20semantic%20relevance%20assessment%2C%20enabling%20selective%20temporal%20token%20fusion%20through%20hard%20fusion%20strategies%20and%20keyframe%20anchoring%20to%20prevent%20error%20accumulation.%20Comprehensive%20experiments%20across%20LIBERO%2C%20SimplerEnv%2C%20and%20real%20robot%20tasks%20demonstrate%20consistent%20improvements%3A%204.0%20percentage%20points%20average%20on%20LIBERO%20%2872.4%5C%25%20vs%2068.4%5C%25%20baseline%29%2C%20cross-environment%20validation%20on%20SimplerEnv%20%284.8%5C%25%20relative%20improvement%29%2C%20and%208.7%5C%25%20relative%20improvement%20on%20real%20robot%20tasks.%20Our%20approach%20proves%20model-agnostic%2C%20working%20across%20OpenVLA%20and%20VLA-Cache%20architectures.%20Notably%2C%20TTF%20reveals%20that%20selective%20Query%20matrix%20reuse%20in%20attention%20mechanisms%20enhances%20rather%20than%20compromises%20performance%2C%20suggesting%20promising%20directions%20for%20direct%20KQV%20matrix%20reuse%20strategies%20that%20achieve%20computational%20acceleration%20while%20improving%20task%20success%20rates.%0ALink%3A%20http%3A//arxiv.org/abs/2508.19257v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTTF-VLA%253A%2520Temporal%2520Token%2520Fusion%2520via%2520Pixel-Attention%2520Integration%2520for%2520Vision-Language-Action%2520Models%26entry.906535625%3DChenghao%2520Liu%2520and%2520Jiachen%2520Zhang%2520and%2520Chengxuan%2520Li%2520and%2520Zhimu%2520Zhou%2520and%2520Shixin%2520Wu%2520and%2520Songfang%2520Huang%2520and%2520Huiling%2520Duan%26entry.1292438233%3DVision-Language-Action%2520%2528VLA%2529%2520models%2520process%2520visual%2520inputs%2520independently%2520at%2520each%2520timestep%252C%2520discarding%2520valuable%2520temporal%2520information%2520inherent%2520in%2520robotic%2520manipulation%2520tasks.%2520This%2520frame-by-frame%2520processing%2520makes%2520models%2520vulnerable%2520to%2520visual%2520noise%2520while%2520ignoring%2520the%2520substantial%2520coherence%2520between%2520consecutive%2520frames%2520in%2520manipulation%2520sequences.%2520We%2520propose%2520Temporal%2520Token%2520Fusion%2520%2528TTF%2529%252C%2520a%2520training-free%2520approach%2520that%2520intelligently%2520integrates%2520historical%2520and%2520current%2520visual%2520representations%2520to%2520enhance%2520VLA%2520inference%2520quality.%2520Our%2520method%2520employs%2520dual-dimension%2520detection%2520combining%2520efficient%2520grayscale%2520pixel%2520difference%2520analysis%2520with%2520attention-based%2520semantic%2520relevance%2520assessment%252C%2520enabling%2520selective%2520temporal%2520token%2520fusion%2520through%2520hard%2520fusion%2520strategies%2520and%2520keyframe%2520anchoring%2520to%2520prevent%2520error%2520accumulation.%2520Comprehensive%2520experiments%2520across%2520LIBERO%252C%2520SimplerEnv%252C%2520and%2520real%2520robot%2520tasks%2520demonstrate%2520consistent%2520improvements%253A%25204.0%2520percentage%2520points%2520average%2520on%2520LIBERO%2520%252872.4%255C%2525%2520vs%252068.4%255C%2525%2520baseline%2529%252C%2520cross-environment%2520validation%2520on%2520SimplerEnv%2520%25284.8%255C%2525%2520relative%2520improvement%2529%252C%2520and%25208.7%255C%2525%2520relative%2520improvement%2520on%2520real%2520robot%2520tasks.%2520Our%2520approach%2520proves%2520model-agnostic%252C%2520working%2520across%2520OpenVLA%2520and%2520VLA-Cache%2520architectures.%2520Notably%252C%2520TTF%2520reveals%2520that%2520selective%2520Query%2520matrix%2520reuse%2520in%2520attention%2520mechanisms%2520enhances%2520rather%2520than%2520compromises%2520performance%252C%2520suggesting%2520promising%2520directions%2520for%2520direct%2520KQV%2520matrix%2520reuse%2520strategies%2520that%2520achieve%2520computational%2520acceleration%2520while%2520improving%2520task%2520success%2520rates.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19257v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TTF-VLA%3A%20Temporal%20Token%20Fusion%20via%20Pixel-Attention%20Integration%20for%20Vision-Language-Action%20Models&entry.906535625=Chenghao%20Liu%20and%20Jiachen%20Zhang%20and%20Chengxuan%20Li%20and%20Zhimu%20Zhou%20and%20Shixin%20Wu%20and%20Songfang%20Huang%20and%20Huiling%20Duan&entry.1292438233=Vision-Language-Action%20%28VLA%29%20models%20process%20visual%20inputs%20independently%20at%20each%20timestep%2C%20discarding%20valuable%20temporal%20information%20inherent%20in%20robotic%20manipulation%20tasks.%20This%20frame-by-frame%20processing%20makes%20models%20vulnerable%20to%20visual%20noise%20while%20ignoring%20the%20substantial%20coherence%20between%20consecutive%20frames%20in%20manipulation%20sequences.%20We%20propose%20Temporal%20Token%20Fusion%20%28TTF%29%2C%20a%20training-free%20approach%20that%20intelligently%20integrates%20historical%20and%20current%20visual%20representations%20to%20enhance%20VLA%20inference%20quality.%20Our%20method%20employs%20dual-dimension%20detection%20combining%20efficient%20grayscale%20pixel%20difference%20analysis%20with%20attention-based%20semantic%20relevance%20assessment%2C%20enabling%20selective%20temporal%20token%20fusion%20through%20hard%20fusion%20strategies%20and%20keyframe%20anchoring%20to%20prevent%20error%20accumulation.%20Comprehensive%20experiments%20across%20LIBERO%2C%20SimplerEnv%2C%20and%20real%20robot%20tasks%20demonstrate%20consistent%20improvements%3A%204.0%20percentage%20points%20average%20on%20LIBERO%20%2872.4%5C%25%20vs%2068.4%5C%25%20baseline%29%2C%20cross-environment%20validation%20on%20SimplerEnv%20%284.8%5C%25%20relative%20improvement%29%2C%20and%208.7%5C%25%20relative%20improvement%20on%20real%20robot%20tasks.%20Our%20approach%20proves%20model-agnostic%2C%20working%20across%20OpenVLA%20and%20VLA-Cache%20architectures.%20Notably%2C%20TTF%20reveals%20that%20selective%20Query%20matrix%20reuse%20in%20attention%20mechanisms%20enhances%20rather%20than%20compromises%20performance%2C%20suggesting%20promising%20directions%20for%20direct%20KQV%20matrix%20reuse%20strategies%20that%20achieve%20computational%20acceleration%20while%20improving%20task%20success%20rates.&entry.1838667208=http%3A//arxiv.org/abs/2508.19257v3&entry.124074799=Read"},
{"title": "CHNNet: An Artificial Neural Network With Connected Hidden Neurons", "author": "Rafiad Sadat Shahir and Zayed Humayun and Mashrufa Akter Tamim and Shouri Saha and Md. Golam Rabiul Alam and Abu Mohammad Khan", "abstract": "In contrast to biological neural circuits, conventional artificial neural networks are commonly organized as strictly hierarchical architectures that exclude direct connections among neurons within the same layer. Consequently, information flow is primarily confined to feedforward and feedback pathways across layers, which limits lateral interactions and constrains the potential for intra-layer information integration. We introduce an artificial neural network featuring intra-layer connections among hidden neurons to overcome this limitation. Owing to the proposed method for facilitating intra-layer connections, the model is theoretically anticipated to achieve faster convergence compared to conventional feedforward neural networks. The experimental findings provide further validation of the theoretical analysis.", "link": "http://arxiv.org/abs/2305.10468v3", "date": "2025-11-14", "relevancy": 2.2714, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5482}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4117}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CHNNet%3A%20An%20Artificial%20Neural%20Network%20With%20Connected%20Hidden%20Neurons&body=Title%3A%20CHNNet%3A%20An%20Artificial%20Neural%20Network%20With%20Connected%20Hidden%20Neurons%0AAuthor%3A%20Rafiad%20Sadat%20Shahir%20and%20Zayed%20Humayun%20and%20Mashrufa%20Akter%20Tamim%20and%20Shouri%20Saha%20and%20Md.%20Golam%20Rabiul%20Alam%20and%20Abu%20Mohammad%20Khan%0AAbstract%3A%20In%20contrast%20to%20biological%20neural%20circuits%2C%20conventional%20artificial%20neural%20networks%20are%20commonly%20organized%20as%20strictly%20hierarchical%20architectures%20that%20exclude%20direct%20connections%20among%20neurons%20within%20the%20same%20layer.%20Consequently%2C%20information%20flow%20is%20primarily%20confined%20to%20feedforward%20and%20feedback%20pathways%20across%20layers%2C%20which%20limits%20lateral%20interactions%20and%20constrains%20the%20potential%20for%20intra-layer%20information%20integration.%20We%20introduce%20an%20artificial%20neural%20network%20featuring%20intra-layer%20connections%20among%20hidden%20neurons%20to%20overcome%20this%20limitation.%20Owing%20to%20the%20proposed%20method%20for%20facilitating%20intra-layer%20connections%2C%20the%20model%20is%20theoretically%20anticipated%20to%20achieve%20faster%20convergence%20compared%20to%20conventional%20feedforward%20neural%20networks.%20The%20experimental%20findings%20provide%20further%20validation%20of%20the%20theoretical%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2305.10468v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCHNNet%253A%2520An%2520Artificial%2520Neural%2520Network%2520With%2520Connected%2520Hidden%2520Neurons%26entry.906535625%3DRafiad%2520Sadat%2520Shahir%2520and%2520Zayed%2520Humayun%2520and%2520Mashrufa%2520Akter%2520Tamim%2520and%2520Shouri%2520Saha%2520and%2520Md.%2520Golam%2520Rabiul%2520Alam%2520and%2520Abu%2520Mohammad%2520Khan%26entry.1292438233%3DIn%2520contrast%2520to%2520biological%2520neural%2520circuits%252C%2520conventional%2520artificial%2520neural%2520networks%2520are%2520commonly%2520organized%2520as%2520strictly%2520hierarchical%2520architectures%2520that%2520exclude%2520direct%2520connections%2520among%2520neurons%2520within%2520the%2520same%2520layer.%2520Consequently%252C%2520information%2520flow%2520is%2520primarily%2520confined%2520to%2520feedforward%2520and%2520feedback%2520pathways%2520across%2520layers%252C%2520which%2520limits%2520lateral%2520interactions%2520and%2520constrains%2520the%2520potential%2520for%2520intra-layer%2520information%2520integration.%2520We%2520introduce%2520an%2520artificial%2520neural%2520network%2520featuring%2520intra-layer%2520connections%2520among%2520hidden%2520neurons%2520to%2520overcome%2520this%2520limitation.%2520Owing%2520to%2520the%2520proposed%2520method%2520for%2520facilitating%2520intra-layer%2520connections%252C%2520the%2520model%2520is%2520theoretically%2520anticipated%2520to%2520achieve%2520faster%2520convergence%2520compared%2520to%2520conventional%2520feedforward%2520neural%2520networks.%2520The%2520experimental%2520findings%2520provide%2520further%2520validation%2520of%2520the%2520theoretical%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.10468v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CHNNet%3A%20An%20Artificial%20Neural%20Network%20With%20Connected%20Hidden%20Neurons&entry.906535625=Rafiad%20Sadat%20Shahir%20and%20Zayed%20Humayun%20and%20Mashrufa%20Akter%20Tamim%20and%20Shouri%20Saha%20and%20Md.%20Golam%20Rabiul%20Alam%20and%20Abu%20Mohammad%20Khan&entry.1292438233=In%20contrast%20to%20biological%20neural%20circuits%2C%20conventional%20artificial%20neural%20networks%20are%20commonly%20organized%20as%20strictly%20hierarchical%20architectures%20that%20exclude%20direct%20connections%20among%20neurons%20within%20the%20same%20layer.%20Consequently%2C%20information%20flow%20is%20primarily%20confined%20to%20feedforward%20and%20feedback%20pathways%20across%20layers%2C%20which%20limits%20lateral%20interactions%20and%20constrains%20the%20potential%20for%20intra-layer%20information%20integration.%20We%20introduce%20an%20artificial%20neural%20network%20featuring%20intra-layer%20connections%20among%20hidden%20neurons%20to%20overcome%20this%20limitation.%20Owing%20to%20the%20proposed%20method%20for%20facilitating%20intra-layer%20connections%2C%20the%20model%20is%20theoretically%20anticipated%20to%20achieve%20faster%20convergence%20compared%20to%20conventional%20feedforward%20neural%20networks.%20The%20experimental%20findings%20provide%20further%20validation%20of%20the%20theoretical%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2305.10468v3&entry.124074799=Read"},
{"title": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language Models", "author": "Khanh-Binh Nguyen and Phuoc-Nguyen Bui and Hyunseung Choo and Duc Thanh Nguyen", "abstract": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but suffer performance degradation under distribution shifts in downstream tasks, particularly in the absence of labeled data. Test-Time Adaptation (TTA) addresses this challenge by enabling online optimization of VLMs during inference, eliminating the need for annotated data. Cache-based TTA methods exploit historical knowledge by maintaining a dynamic memory cache of low-entropy or high-confidence samples, promoting efficient adaptation to out-of-distribution data. Nevertheless, these methods face two critical challenges: (1) unreliable confidence metrics under significant distribution shifts, resulting in error accumulation within the cache and degraded adaptation performance; and (2) rigid decision boundaries that fail to accommodate substantial distributional variations, leading to suboptimal predictions. To overcome these limitations, we introduce the Adaptive Cache Enhancement (ACE) framework, which constructs a robust cache by selectively storing high-confidence or low-entropy image embeddings per class, guided by dynamic, class-specific thresholds initialized from zero-shot statistics and iteratively refined using an exponential moving average and exploration-augmented updates. This approach enables adaptive, class-wise decision boundaries, ensuring robust and accurate predictions across diverse visual distributions. Extensive experiments on 15 diverse benchmark datasets demonstrate that ACE achieves state-of-the-art performance, delivering superior robustness and generalization compared to existing TTA methods in challenging out-of-distribution scenarios.", "link": "http://arxiv.org/abs/2508.07570v2", "date": "2025-11-14", "relevancy": 2.2608, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6004}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5608}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Cache%20Enhancement%20for%20Test-Time%20Adaptation%20of%20Vision-Language%20Models&body=Title%3A%20Adaptive%20Cache%20Enhancement%20for%20Test-Time%20Adaptation%20of%20Vision-Language%20Models%0AAuthor%3A%20Khanh-Binh%20Nguyen%20and%20Phuoc-Nguyen%20Bui%20and%20Hyunseung%20Choo%20and%20Duc%20Thanh%20Nguyen%0AAbstract%3A%20Vision-language%20models%20%28VLMs%29%20exhibit%20remarkable%20zero-shot%20generalization%20but%20suffer%20performance%20degradation%20under%20distribution%20shifts%20in%20downstream%20tasks%2C%20particularly%20in%20the%20absence%20of%20labeled%20data.%20Test-Time%20Adaptation%20%28TTA%29%20addresses%20this%20challenge%20by%20enabling%20online%20optimization%20of%20VLMs%20during%20inference%2C%20eliminating%20the%20need%20for%20annotated%20data.%20Cache-based%20TTA%20methods%20exploit%20historical%20knowledge%20by%20maintaining%20a%20dynamic%20memory%20cache%20of%20low-entropy%20or%20high-confidence%20samples%2C%20promoting%20efficient%20adaptation%20to%20out-of-distribution%20data.%20Nevertheless%2C%20these%20methods%20face%20two%20critical%20challenges%3A%20%281%29%20unreliable%20confidence%20metrics%20under%20significant%20distribution%20shifts%2C%20resulting%20in%20error%20accumulation%20within%20the%20cache%20and%20degraded%20adaptation%20performance%3B%20and%20%282%29%20rigid%20decision%20boundaries%20that%20fail%20to%20accommodate%20substantial%20distributional%20variations%2C%20leading%20to%20suboptimal%20predictions.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20the%20Adaptive%20Cache%20Enhancement%20%28ACE%29%20framework%2C%20which%20constructs%20a%20robust%20cache%20by%20selectively%20storing%20high-confidence%20or%20low-entropy%20image%20embeddings%20per%20class%2C%20guided%20by%20dynamic%2C%20class-specific%20thresholds%20initialized%20from%20zero-shot%20statistics%20and%20iteratively%20refined%20using%20an%20exponential%20moving%20average%20and%20exploration-augmented%20updates.%20This%20approach%20enables%20adaptive%2C%20class-wise%20decision%20boundaries%2C%20ensuring%20robust%20and%20accurate%20predictions%20across%20diverse%20visual%20distributions.%20Extensive%20experiments%20on%2015%20diverse%20benchmark%20datasets%20demonstrate%20that%20ACE%20achieves%20state-of-the-art%20performance%2C%20delivering%20superior%20robustness%20and%20generalization%20compared%20to%20existing%20TTA%20methods%20in%20challenging%20out-of-distribution%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2508.07570v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Cache%2520Enhancement%2520for%2520Test-Time%2520Adaptation%2520of%2520Vision-Language%2520Models%26entry.906535625%3DKhanh-Binh%2520Nguyen%2520and%2520Phuoc-Nguyen%2520Bui%2520and%2520Hyunseung%2520Choo%2520and%2520Duc%2520Thanh%2520Nguyen%26entry.1292438233%3DVision-language%2520models%2520%2528VLMs%2529%2520exhibit%2520remarkable%2520zero-shot%2520generalization%2520but%2520suffer%2520performance%2520degradation%2520under%2520distribution%2520shifts%2520in%2520downstream%2520tasks%252C%2520particularly%2520in%2520the%2520absence%2520of%2520labeled%2520data.%2520Test-Time%2520Adaptation%2520%2528TTA%2529%2520addresses%2520this%2520challenge%2520by%2520enabling%2520online%2520optimization%2520of%2520VLMs%2520during%2520inference%252C%2520eliminating%2520the%2520need%2520for%2520annotated%2520data.%2520Cache-based%2520TTA%2520methods%2520exploit%2520historical%2520knowledge%2520by%2520maintaining%2520a%2520dynamic%2520memory%2520cache%2520of%2520low-entropy%2520or%2520high-confidence%2520samples%252C%2520promoting%2520efficient%2520adaptation%2520to%2520out-of-distribution%2520data.%2520Nevertheless%252C%2520these%2520methods%2520face%2520two%2520critical%2520challenges%253A%2520%25281%2529%2520unreliable%2520confidence%2520metrics%2520under%2520significant%2520distribution%2520shifts%252C%2520resulting%2520in%2520error%2520accumulation%2520within%2520the%2520cache%2520and%2520degraded%2520adaptation%2520performance%253B%2520and%2520%25282%2529%2520rigid%2520decision%2520boundaries%2520that%2520fail%2520to%2520accommodate%2520substantial%2520distributional%2520variations%252C%2520leading%2520to%2520suboptimal%2520predictions.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520the%2520Adaptive%2520Cache%2520Enhancement%2520%2528ACE%2529%2520framework%252C%2520which%2520constructs%2520a%2520robust%2520cache%2520by%2520selectively%2520storing%2520high-confidence%2520or%2520low-entropy%2520image%2520embeddings%2520per%2520class%252C%2520guided%2520by%2520dynamic%252C%2520class-specific%2520thresholds%2520initialized%2520from%2520zero-shot%2520statistics%2520and%2520iteratively%2520refined%2520using%2520an%2520exponential%2520moving%2520average%2520and%2520exploration-augmented%2520updates.%2520This%2520approach%2520enables%2520adaptive%252C%2520class-wise%2520decision%2520boundaries%252C%2520ensuring%2520robust%2520and%2520accurate%2520predictions%2520across%2520diverse%2520visual%2520distributions.%2520Extensive%2520experiments%2520on%252015%2520diverse%2520benchmark%2520datasets%2520demonstrate%2520that%2520ACE%2520achieves%2520state-of-the-art%2520performance%252C%2520delivering%2520superior%2520robustness%2520and%2520generalization%2520compared%2520to%2520existing%2520TTA%2520methods%2520in%2520challenging%2520out-of-distribution%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07570v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Cache%20Enhancement%20for%20Test-Time%20Adaptation%20of%20Vision-Language%20Models&entry.906535625=Khanh-Binh%20Nguyen%20and%20Phuoc-Nguyen%20Bui%20and%20Hyunseung%20Choo%20and%20Duc%20Thanh%20Nguyen&entry.1292438233=Vision-language%20models%20%28VLMs%29%20exhibit%20remarkable%20zero-shot%20generalization%20but%20suffer%20performance%20degradation%20under%20distribution%20shifts%20in%20downstream%20tasks%2C%20particularly%20in%20the%20absence%20of%20labeled%20data.%20Test-Time%20Adaptation%20%28TTA%29%20addresses%20this%20challenge%20by%20enabling%20online%20optimization%20of%20VLMs%20during%20inference%2C%20eliminating%20the%20need%20for%20annotated%20data.%20Cache-based%20TTA%20methods%20exploit%20historical%20knowledge%20by%20maintaining%20a%20dynamic%20memory%20cache%20of%20low-entropy%20or%20high-confidence%20samples%2C%20promoting%20efficient%20adaptation%20to%20out-of-distribution%20data.%20Nevertheless%2C%20these%20methods%20face%20two%20critical%20challenges%3A%20%281%29%20unreliable%20confidence%20metrics%20under%20significant%20distribution%20shifts%2C%20resulting%20in%20error%20accumulation%20within%20the%20cache%20and%20degraded%20adaptation%20performance%3B%20and%20%282%29%20rigid%20decision%20boundaries%20that%20fail%20to%20accommodate%20substantial%20distributional%20variations%2C%20leading%20to%20suboptimal%20predictions.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20the%20Adaptive%20Cache%20Enhancement%20%28ACE%29%20framework%2C%20which%20constructs%20a%20robust%20cache%20by%20selectively%20storing%20high-confidence%20or%20low-entropy%20image%20embeddings%20per%20class%2C%20guided%20by%20dynamic%2C%20class-specific%20thresholds%20initialized%20from%20zero-shot%20statistics%20and%20iteratively%20refined%20using%20an%20exponential%20moving%20average%20and%20exploration-augmented%20updates.%20This%20approach%20enables%20adaptive%2C%20class-wise%20decision%20boundaries%2C%20ensuring%20robust%20and%20accurate%20predictions%20across%20diverse%20visual%20distributions.%20Extensive%20experiments%20on%2015%20diverse%20benchmark%20datasets%20demonstrate%20that%20ACE%20achieves%20state-of-the-art%20performance%2C%20delivering%20superior%20robustness%20and%20generalization%20compared%20to%20existing%20TTA%20methods%20in%20challenging%20out-of-distribution%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2508.07570v2&entry.124074799=Read"},
{"title": "Viper-F1: Fast and Fine-Grained Multimodal Understanding with Cross-Modal State-Space Modulation", "author": "Quoc-Huy Trinh and Mustapha Abdullahi and Do Duy Hung Trinh and Bo Zhao and Debesh Jha", "abstract": "Recent advances in multimodal large language models (MLLMs) have enabled impressive progress in vision-language understanding, yet their high computational cost limits deployment in resource-constrained scenarios such as robotic manipulation, personal assistants, and smart cameras. Most existing methods rely on Transformer-based cross-attention, whose quadratic complexity hinders efficiency. Moreover, small vision-language models often struggle to precisely capture fine-grained, task-relevant visual regions, leading to degraded performance on fine-grained reasoning tasks that limit their effectiveness in the real world. To address these issues, we introduce Viper-F1, a Hybrid State-Space Vision-Language Model that replaces attention with efficient Liquid State-Space Dynamics. To further enhance visual grounding, we propose a Token-Grid Correlation Module, which computes lightweight correlations between text tokens and image patches and modulates the state-space dynamics via FiLM conditioning. This enables the model to selectively emphasize visual regions relevant to the textual prompt while maintaining linear-time inference. Experimental results across multiple benchmarks demonstrate that Viper-F1 achieves accurate, fine-grained understanding with significantly improved efficiency.", "link": "http://arxiv.org/abs/2511.11177v1", "date": "2025-11-14", "relevancy": 2.2474, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5629}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5629}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Viper-F1%3A%20Fast%20and%20Fine-Grained%20Multimodal%20Understanding%20with%20Cross-Modal%20State-Space%20Modulation&body=Title%3A%20Viper-F1%3A%20Fast%20and%20Fine-Grained%20Multimodal%20Understanding%20with%20Cross-Modal%20State-Space%20Modulation%0AAuthor%3A%20Quoc-Huy%20Trinh%20and%20Mustapha%20Abdullahi%20and%20Do%20Duy%20Hung%20Trinh%20and%20Bo%20Zhao%20and%20Debesh%20Jha%0AAbstract%3A%20Recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20enabled%20impressive%20progress%20in%20vision-language%20understanding%2C%20yet%20their%20high%20computational%20cost%20limits%20deployment%20in%20resource-constrained%20scenarios%20such%20as%20robotic%20manipulation%2C%20personal%20assistants%2C%20and%20smart%20cameras.%20Most%20existing%20methods%20rely%20on%20Transformer-based%20cross-attention%2C%20whose%20quadratic%20complexity%20hinders%20efficiency.%20Moreover%2C%20small%20vision-language%20models%20often%20struggle%20to%20precisely%20capture%20fine-grained%2C%20task-relevant%20visual%20regions%2C%20leading%20to%20degraded%20performance%20on%20fine-grained%20reasoning%20tasks%20that%20limit%20their%20effectiveness%20in%20the%20real%20world.%20To%20address%20these%20issues%2C%20we%20introduce%20Viper-F1%2C%20a%20Hybrid%20State-Space%20Vision-Language%20Model%20that%20replaces%20attention%20with%20efficient%20Liquid%20State-Space%20Dynamics.%20To%20further%20enhance%20visual%20grounding%2C%20we%20propose%20a%20Token-Grid%20Correlation%20Module%2C%20which%20computes%20lightweight%20correlations%20between%20text%20tokens%20and%20image%20patches%20and%20modulates%20the%20state-space%20dynamics%20via%20FiLM%20conditioning.%20This%20enables%20the%20model%20to%20selectively%20emphasize%20visual%20regions%20relevant%20to%20the%20textual%20prompt%20while%20maintaining%20linear-time%20inference.%20Experimental%20results%20across%20multiple%20benchmarks%20demonstrate%20that%20Viper-F1%20achieves%20accurate%2C%20fine-grained%20understanding%20with%20significantly%20improved%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViper-F1%253A%2520Fast%2520and%2520Fine-Grained%2520Multimodal%2520Understanding%2520with%2520Cross-Modal%2520State-Space%2520Modulation%26entry.906535625%3DQuoc-Huy%2520Trinh%2520and%2520Mustapha%2520Abdullahi%2520and%2520Do%2520Duy%2520Hung%2520Trinh%2520and%2520Bo%2520Zhao%2520and%2520Debesh%2520Jha%26entry.1292438233%3DRecent%2520advances%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520enabled%2520impressive%2520progress%2520in%2520vision-language%2520understanding%252C%2520yet%2520their%2520high%2520computational%2520cost%2520limits%2520deployment%2520in%2520resource-constrained%2520scenarios%2520such%2520as%2520robotic%2520manipulation%252C%2520personal%2520assistants%252C%2520and%2520smart%2520cameras.%2520Most%2520existing%2520methods%2520rely%2520on%2520Transformer-based%2520cross-attention%252C%2520whose%2520quadratic%2520complexity%2520hinders%2520efficiency.%2520Moreover%252C%2520small%2520vision-language%2520models%2520often%2520struggle%2520to%2520precisely%2520capture%2520fine-grained%252C%2520task-relevant%2520visual%2520regions%252C%2520leading%2520to%2520degraded%2520performance%2520on%2520fine-grained%2520reasoning%2520tasks%2520that%2520limit%2520their%2520effectiveness%2520in%2520the%2520real%2520world.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520Viper-F1%252C%2520a%2520Hybrid%2520State-Space%2520Vision-Language%2520Model%2520that%2520replaces%2520attention%2520with%2520efficient%2520Liquid%2520State-Space%2520Dynamics.%2520To%2520further%2520enhance%2520visual%2520grounding%252C%2520we%2520propose%2520a%2520Token-Grid%2520Correlation%2520Module%252C%2520which%2520computes%2520lightweight%2520correlations%2520between%2520text%2520tokens%2520and%2520image%2520patches%2520and%2520modulates%2520the%2520state-space%2520dynamics%2520via%2520FiLM%2520conditioning.%2520This%2520enables%2520the%2520model%2520to%2520selectively%2520emphasize%2520visual%2520regions%2520relevant%2520to%2520the%2520textual%2520prompt%2520while%2520maintaining%2520linear-time%2520inference.%2520Experimental%2520results%2520across%2520multiple%2520benchmarks%2520demonstrate%2520that%2520Viper-F1%2520achieves%2520accurate%252C%2520fine-grained%2520understanding%2520with%2520significantly%2520improved%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Viper-F1%3A%20Fast%20and%20Fine-Grained%20Multimodal%20Understanding%20with%20Cross-Modal%20State-Space%20Modulation&entry.906535625=Quoc-Huy%20Trinh%20and%20Mustapha%20Abdullahi%20and%20Do%20Duy%20Hung%20Trinh%20and%20Bo%20Zhao%20and%20Debesh%20Jha&entry.1292438233=Recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20enabled%20impressive%20progress%20in%20vision-language%20understanding%2C%20yet%20their%20high%20computational%20cost%20limits%20deployment%20in%20resource-constrained%20scenarios%20such%20as%20robotic%20manipulation%2C%20personal%20assistants%2C%20and%20smart%20cameras.%20Most%20existing%20methods%20rely%20on%20Transformer-based%20cross-attention%2C%20whose%20quadratic%20complexity%20hinders%20efficiency.%20Moreover%2C%20small%20vision-language%20models%20often%20struggle%20to%20precisely%20capture%20fine-grained%2C%20task-relevant%20visual%20regions%2C%20leading%20to%20degraded%20performance%20on%20fine-grained%20reasoning%20tasks%20that%20limit%20their%20effectiveness%20in%20the%20real%20world.%20To%20address%20these%20issues%2C%20we%20introduce%20Viper-F1%2C%20a%20Hybrid%20State-Space%20Vision-Language%20Model%20that%20replaces%20attention%20with%20efficient%20Liquid%20State-Space%20Dynamics.%20To%20further%20enhance%20visual%20grounding%2C%20we%20propose%20a%20Token-Grid%20Correlation%20Module%2C%20which%20computes%20lightweight%20correlations%20between%20text%20tokens%20and%20image%20patches%20and%20modulates%20the%20state-space%20dynamics%20via%20FiLM%20conditioning.%20This%20enables%20the%20model%20to%20selectively%20emphasize%20visual%20regions%20relevant%20to%20the%20textual%20prompt%20while%20maintaining%20linear-time%20inference.%20Experimental%20results%20across%20multiple%20benchmarks%20demonstrate%20that%20Viper-F1%20achieves%20accurate%2C%20fine-grained%20understanding%20with%20significantly%20improved%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2511.11177v1&entry.124074799=Read"},
{"title": "FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning", "author": "Jiajun Cao and Qizhe Zhang and Peidong Jia and Xuhui Zhao and Bo Lan and Xiaoan Zhang and Zhuo Li and Xiaobao Wei and Sixiang Chen and Liyun Li and Xianming Liu and Ming Lu and Yang Wang and Shanghang Zhang", "abstract": "Vision-Language-Action (VLA) models have demonstrated significant potential in complex scene understanding and action reasoning, leading to their increasing adoption in end-to-end autonomous driving systems. However, the long visual tokens of VLA models greatly increase computational costs. Current visual token pruning methods in Vision-Language Models (VLM) rely on either visual token similarity or visual-text attention, but both have shown poor performance in autonomous driving scenarios. Given that human drivers concentrate on relevant foreground areas while driving, we assert that retaining visual tokens containing this foreground information is essential for effective decision-making. Inspired by this, we propose FastDriveVLA, a novel reconstruction-based vision token pruning framework designed specifically for autonomous driving. FastDriveVLA includes a plug-and-play visual token pruner called ReconPruner, which prioritizes foreground information through MAE-style pixel reconstruction. A novel adversarial foreground-background reconstruction strategy is designed to train ReconPruner for the visual encoder of VLA models. Once trained, ReconPruner can be seamlessly applied to different VLA models with the same visual encoder without retraining. To train ReconPruner, we also introduce a large-scale dataset called nuScenes-FG, consisting of 241K image-mask pairs with annotated foreground regions. Our approach achieves state-of-the-art results on the nuScenes open-loop planning benchmark across different pruning ratios.", "link": "http://arxiv.org/abs/2507.23318v4", "date": "2025-11-14", "relevancy": 2.2269, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5647}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5647}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastDriveVLA%3A%20Efficient%20End-to-End%20Driving%20via%20Plug-and-Play%20Reconstruction-based%20Token%20Pruning&body=Title%3A%20FastDriveVLA%3A%20Efficient%20End-to-End%20Driving%20via%20Plug-and-Play%20Reconstruction-based%20Token%20Pruning%0AAuthor%3A%20Jiajun%20Cao%20and%20Qizhe%20Zhang%20and%20Peidong%20Jia%20and%20Xuhui%20Zhao%20and%20Bo%20Lan%20and%20Xiaoan%20Zhang%20and%20Zhuo%20Li%20and%20Xiaobao%20Wei%20and%20Sixiang%20Chen%20and%20Liyun%20Li%20and%20Xianming%20Liu%20and%20Ming%20Lu%20and%20Yang%20Wang%20and%20Shanghang%20Zhang%0AAbstract%3A%20Vision-Language-Action%20%28VLA%29%20models%20have%20demonstrated%20significant%20potential%20in%20complex%20scene%20understanding%20and%20action%20reasoning%2C%20leading%20to%20their%20increasing%20adoption%20in%20end-to-end%20autonomous%20driving%20systems.%20However%2C%20the%20long%20visual%20tokens%20of%20VLA%20models%20greatly%20increase%20computational%20costs.%20Current%20visual%20token%20pruning%20methods%20in%20Vision-Language%20Models%20%28VLM%29%20rely%20on%20either%20visual%20token%20similarity%20or%20visual-text%20attention%2C%20but%20both%20have%20shown%20poor%20performance%20in%20autonomous%20driving%20scenarios.%20Given%20that%20human%20drivers%20concentrate%20on%20relevant%20foreground%20areas%20while%20driving%2C%20we%20assert%20that%20retaining%20visual%20tokens%20containing%20this%20foreground%20information%20is%20essential%20for%20effective%20decision-making.%20Inspired%20by%20this%2C%20we%20propose%20FastDriveVLA%2C%20a%20novel%20reconstruction-based%20vision%20token%20pruning%20framework%20designed%20specifically%20for%20autonomous%20driving.%20FastDriveVLA%20includes%20a%20plug-and-play%20visual%20token%20pruner%20called%20ReconPruner%2C%20which%20prioritizes%20foreground%20information%20through%20MAE-style%20pixel%20reconstruction.%20A%20novel%20adversarial%20foreground-background%20reconstruction%20strategy%20is%20designed%20to%20train%20ReconPruner%20for%20the%20visual%20encoder%20of%20VLA%20models.%20Once%20trained%2C%20ReconPruner%20can%20be%20seamlessly%20applied%20to%20different%20VLA%20models%20with%20the%20same%20visual%20encoder%20without%20retraining.%20To%20train%20ReconPruner%2C%20we%20also%20introduce%20a%20large-scale%20dataset%20called%20nuScenes-FG%2C%20consisting%20of%20241K%20image-mask%20pairs%20with%20annotated%20foreground%20regions.%20Our%20approach%20achieves%20state-of-the-art%20results%20on%20the%20nuScenes%20open-loop%20planning%20benchmark%20across%20different%20pruning%20ratios.%0ALink%3A%20http%3A//arxiv.org/abs/2507.23318v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastDriveVLA%253A%2520Efficient%2520End-to-End%2520Driving%2520via%2520Plug-and-Play%2520Reconstruction-based%2520Token%2520Pruning%26entry.906535625%3DJiajun%2520Cao%2520and%2520Qizhe%2520Zhang%2520and%2520Peidong%2520Jia%2520and%2520Xuhui%2520Zhao%2520and%2520Bo%2520Lan%2520and%2520Xiaoan%2520Zhang%2520and%2520Zhuo%2520Li%2520and%2520Xiaobao%2520Wei%2520and%2520Sixiang%2520Chen%2520and%2520Liyun%2520Li%2520and%2520Xianming%2520Liu%2520and%2520Ming%2520Lu%2520and%2520Yang%2520Wang%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3DVision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520demonstrated%2520significant%2520potential%2520in%2520complex%2520scene%2520understanding%2520and%2520action%2520reasoning%252C%2520leading%2520to%2520their%2520increasing%2520adoption%2520in%2520end-to-end%2520autonomous%2520driving%2520systems.%2520However%252C%2520the%2520long%2520visual%2520tokens%2520of%2520VLA%2520models%2520greatly%2520increase%2520computational%2520costs.%2520Current%2520visual%2520token%2520pruning%2520methods%2520in%2520Vision-Language%2520Models%2520%2528VLM%2529%2520rely%2520on%2520either%2520visual%2520token%2520similarity%2520or%2520visual-text%2520attention%252C%2520but%2520both%2520have%2520shown%2520poor%2520performance%2520in%2520autonomous%2520driving%2520scenarios.%2520Given%2520that%2520human%2520drivers%2520concentrate%2520on%2520relevant%2520foreground%2520areas%2520while%2520driving%252C%2520we%2520assert%2520that%2520retaining%2520visual%2520tokens%2520containing%2520this%2520foreground%2520information%2520is%2520essential%2520for%2520effective%2520decision-making.%2520Inspired%2520by%2520this%252C%2520we%2520propose%2520FastDriveVLA%252C%2520a%2520novel%2520reconstruction-based%2520vision%2520token%2520pruning%2520framework%2520designed%2520specifically%2520for%2520autonomous%2520driving.%2520FastDriveVLA%2520includes%2520a%2520plug-and-play%2520visual%2520token%2520pruner%2520called%2520ReconPruner%252C%2520which%2520prioritizes%2520foreground%2520information%2520through%2520MAE-style%2520pixel%2520reconstruction.%2520A%2520novel%2520adversarial%2520foreground-background%2520reconstruction%2520strategy%2520is%2520designed%2520to%2520train%2520ReconPruner%2520for%2520the%2520visual%2520encoder%2520of%2520VLA%2520models.%2520Once%2520trained%252C%2520ReconPruner%2520can%2520be%2520seamlessly%2520applied%2520to%2520different%2520VLA%2520models%2520with%2520the%2520same%2520visual%2520encoder%2520without%2520retraining.%2520To%2520train%2520ReconPruner%252C%2520we%2520also%2520introduce%2520a%2520large-scale%2520dataset%2520called%2520nuScenes-FG%252C%2520consisting%2520of%2520241K%2520image-mask%2520pairs%2520with%2520annotated%2520foreground%2520regions.%2520Our%2520approach%2520achieves%2520state-of-the-art%2520results%2520on%2520the%2520nuScenes%2520open-loop%2520planning%2520benchmark%2520across%2520different%2520pruning%2520ratios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23318v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastDriveVLA%3A%20Efficient%20End-to-End%20Driving%20via%20Plug-and-Play%20Reconstruction-based%20Token%20Pruning&entry.906535625=Jiajun%20Cao%20and%20Qizhe%20Zhang%20and%20Peidong%20Jia%20and%20Xuhui%20Zhao%20and%20Bo%20Lan%20and%20Xiaoan%20Zhang%20and%20Zhuo%20Li%20and%20Xiaobao%20Wei%20and%20Sixiang%20Chen%20and%20Liyun%20Li%20and%20Xianming%20Liu%20and%20Ming%20Lu%20and%20Yang%20Wang%20and%20Shanghang%20Zhang&entry.1292438233=Vision-Language-Action%20%28VLA%29%20models%20have%20demonstrated%20significant%20potential%20in%20complex%20scene%20understanding%20and%20action%20reasoning%2C%20leading%20to%20their%20increasing%20adoption%20in%20end-to-end%20autonomous%20driving%20systems.%20However%2C%20the%20long%20visual%20tokens%20of%20VLA%20models%20greatly%20increase%20computational%20costs.%20Current%20visual%20token%20pruning%20methods%20in%20Vision-Language%20Models%20%28VLM%29%20rely%20on%20either%20visual%20token%20similarity%20or%20visual-text%20attention%2C%20but%20both%20have%20shown%20poor%20performance%20in%20autonomous%20driving%20scenarios.%20Given%20that%20human%20drivers%20concentrate%20on%20relevant%20foreground%20areas%20while%20driving%2C%20we%20assert%20that%20retaining%20visual%20tokens%20containing%20this%20foreground%20information%20is%20essential%20for%20effective%20decision-making.%20Inspired%20by%20this%2C%20we%20propose%20FastDriveVLA%2C%20a%20novel%20reconstruction-based%20vision%20token%20pruning%20framework%20designed%20specifically%20for%20autonomous%20driving.%20FastDriveVLA%20includes%20a%20plug-and-play%20visual%20token%20pruner%20called%20ReconPruner%2C%20which%20prioritizes%20foreground%20information%20through%20MAE-style%20pixel%20reconstruction.%20A%20novel%20adversarial%20foreground-background%20reconstruction%20strategy%20is%20designed%20to%20train%20ReconPruner%20for%20the%20visual%20encoder%20of%20VLA%20models.%20Once%20trained%2C%20ReconPruner%20can%20be%20seamlessly%20applied%20to%20different%20VLA%20models%20with%20the%20same%20visual%20encoder%20without%20retraining.%20To%20train%20ReconPruner%2C%20we%20also%20introduce%20a%20large-scale%20dataset%20called%20nuScenes-FG%2C%20consisting%20of%20241K%20image-mask%20pairs%20with%20annotated%20foreground%20regions.%20Our%20approach%20achieves%20state-of-the-art%20results%20on%20the%20nuScenes%20open-loop%20planning%20benchmark%20across%20different%20pruning%20ratios.&entry.1838667208=http%3A//arxiv.org/abs/2507.23318v4&entry.124074799=Read"},
{"title": "Generative AI in Map-Making: A Technical Exploration and Its Implications for Cartographers", "author": "Claudio Affolter and Sidi Wu and Yizi Chen and Lorenz Hurni", "abstract": "Traditional map-making relies heavily on Geographic Information Systems (GIS), requiring domain expertise and being time-consuming, especially for repetitive tasks. Recent advances in generative AI (GenAI), particularly image diffusion models, offer new opportunities for automating and democratizing the map-making process. However, these models struggle with accurate map creation due to limited control over spatial composition and semantic layout. To address this, we integrate vector data to guide map generation in different styles, specified by the textual prompts. Our model is the first to generate accurate maps in controlled styles, and we have integrated it into a web application to improve its usability and accessibility. We conducted a user study with professional cartographers to assess the fidelity of generated maps, the usability of the web application, and the implications of ever-emerging GenAI in map-making. The findings have suggested the potential of our developed application and, more generally, the GenAI models in helping both non-expert users and professionals in creating maps more efficiently. We have also outlined further technical improvements and emphasized the new role of cartographers to advance the paradigm of AI-assisted map-making. The code and pre-trained models are available at https://github.com/claudaff/generative-ai-mapmaking/.", "link": "http://arxiv.org/abs/2508.18959v2", "date": "2025-11-14", "relevancy": 2.2269, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5698}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5481}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20in%20Map-Making%3A%20A%20Technical%20Exploration%20and%20Its%20Implications%20for%20Cartographers&body=Title%3A%20Generative%20AI%20in%20Map-Making%3A%20A%20Technical%20Exploration%20and%20Its%20Implications%20for%20Cartographers%0AAuthor%3A%20Claudio%20Affolter%20and%20Sidi%20Wu%20and%20Yizi%20Chen%20and%20Lorenz%20Hurni%0AAbstract%3A%20Traditional%20map-making%20relies%20heavily%20on%20Geographic%20Information%20Systems%20%28GIS%29%2C%20requiring%20domain%20expertise%20and%20being%20time-consuming%2C%20especially%20for%20repetitive%20tasks.%20Recent%20advances%20in%20generative%20AI%20%28GenAI%29%2C%20particularly%20image%20diffusion%20models%2C%20offer%20new%20opportunities%20for%20automating%20and%20democratizing%20the%20map-making%20process.%20However%2C%20these%20models%20struggle%20with%20accurate%20map%20creation%20due%20to%20limited%20control%20over%20spatial%20composition%20and%20semantic%20layout.%20To%20address%20this%2C%20we%20integrate%20vector%20data%20to%20guide%20map%20generation%20in%20different%20styles%2C%20specified%20by%20the%20textual%20prompts.%20Our%20model%20is%20the%20first%20to%20generate%20accurate%20maps%20in%20controlled%20styles%2C%20and%20we%20have%20integrated%20it%20into%20a%20web%20application%20to%20improve%20its%20usability%20and%20accessibility.%20We%20conducted%20a%20user%20study%20with%20professional%20cartographers%20to%20assess%20the%20fidelity%20of%20generated%20maps%2C%20the%20usability%20of%20the%20web%20application%2C%20and%20the%20implications%20of%20ever-emerging%20GenAI%20in%20map-making.%20The%20findings%20have%20suggested%20the%20potential%20of%20our%20developed%20application%20and%2C%20more%20generally%2C%20the%20GenAI%20models%20in%20helping%20both%20non-expert%20users%20and%20professionals%20in%20creating%20maps%20more%20efficiently.%20We%20have%20also%20outlined%20further%20technical%20improvements%20and%20emphasized%20the%20new%20role%20of%20cartographers%20to%20advance%20the%20paradigm%20of%20AI-assisted%20map-making.%20The%20code%20and%20pre-trained%20models%20are%20available%20at%20https%3A//github.com/claudaff/generative-ai-mapmaking/.%0ALink%3A%20http%3A//arxiv.org/abs/2508.18959v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520in%2520Map-Making%253A%2520A%2520Technical%2520Exploration%2520and%2520Its%2520Implications%2520for%2520Cartographers%26entry.906535625%3DClaudio%2520Affolter%2520and%2520Sidi%2520Wu%2520and%2520Yizi%2520Chen%2520and%2520Lorenz%2520Hurni%26entry.1292438233%3DTraditional%2520map-making%2520relies%2520heavily%2520on%2520Geographic%2520Information%2520Systems%2520%2528GIS%2529%252C%2520requiring%2520domain%2520expertise%2520and%2520being%2520time-consuming%252C%2520especially%2520for%2520repetitive%2520tasks.%2520Recent%2520advances%2520in%2520generative%2520AI%2520%2528GenAI%2529%252C%2520particularly%2520image%2520diffusion%2520models%252C%2520offer%2520new%2520opportunities%2520for%2520automating%2520and%2520democratizing%2520the%2520map-making%2520process.%2520However%252C%2520these%2520models%2520struggle%2520with%2520accurate%2520map%2520creation%2520due%2520to%2520limited%2520control%2520over%2520spatial%2520composition%2520and%2520semantic%2520layout.%2520To%2520address%2520this%252C%2520we%2520integrate%2520vector%2520data%2520to%2520guide%2520map%2520generation%2520in%2520different%2520styles%252C%2520specified%2520by%2520the%2520textual%2520prompts.%2520Our%2520model%2520is%2520the%2520first%2520to%2520generate%2520accurate%2520maps%2520in%2520controlled%2520styles%252C%2520and%2520we%2520have%2520integrated%2520it%2520into%2520a%2520web%2520application%2520to%2520improve%2520its%2520usability%2520and%2520accessibility.%2520We%2520conducted%2520a%2520user%2520study%2520with%2520professional%2520cartographers%2520to%2520assess%2520the%2520fidelity%2520of%2520generated%2520maps%252C%2520the%2520usability%2520of%2520the%2520web%2520application%252C%2520and%2520the%2520implications%2520of%2520ever-emerging%2520GenAI%2520in%2520map-making.%2520The%2520findings%2520have%2520suggested%2520the%2520potential%2520of%2520our%2520developed%2520application%2520and%252C%2520more%2520generally%252C%2520the%2520GenAI%2520models%2520in%2520helping%2520both%2520non-expert%2520users%2520and%2520professionals%2520in%2520creating%2520maps%2520more%2520efficiently.%2520We%2520have%2520also%2520outlined%2520further%2520technical%2520improvements%2520and%2520emphasized%2520the%2520new%2520role%2520of%2520cartographers%2520to%2520advance%2520the%2520paradigm%2520of%2520AI-assisted%2520map-making.%2520The%2520code%2520and%2520pre-trained%2520models%2520are%2520available%2520at%2520https%253A//github.com/claudaff/generative-ai-mapmaking/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18959v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20in%20Map-Making%3A%20A%20Technical%20Exploration%20and%20Its%20Implications%20for%20Cartographers&entry.906535625=Claudio%20Affolter%20and%20Sidi%20Wu%20and%20Yizi%20Chen%20and%20Lorenz%20Hurni&entry.1292438233=Traditional%20map-making%20relies%20heavily%20on%20Geographic%20Information%20Systems%20%28GIS%29%2C%20requiring%20domain%20expertise%20and%20being%20time-consuming%2C%20especially%20for%20repetitive%20tasks.%20Recent%20advances%20in%20generative%20AI%20%28GenAI%29%2C%20particularly%20image%20diffusion%20models%2C%20offer%20new%20opportunities%20for%20automating%20and%20democratizing%20the%20map-making%20process.%20However%2C%20these%20models%20struggle%20with%20accurate%20map%20creation%20due%20to%20limited%20control%20over%20spatial%20composition%20and%20semantic%20layout.%20To%20address%20this%2C%20we%20integrate%20vector%20data%20to%20guide%20map%20generation%20in%20different%20styles%2C%20specified%20by%20the%20textual%20prompts.%20Our%20model%20is%20the%20first%20to%20generate%20accurate%20maps%20in%20controlled%20styles%2C%20and%20we%20have%20integrated%20it%20into%20a%20web%20application%20to%20improve%20its%20usability%20and%20accessibility.%20We%20conducted%20a%20user%20study%20with%20professional%20cartographers%20to%20assess%20the%20fidelity%20of%20generated%20maps%2C%20the%20usability%20of%20the%20web%20application%2C%20and%20the%20implications%20of%20ever-emerging%20GenAI%20in%20map-making.%20The%20findings%20have%20suggested%20the%20potential%20of%20our%20developed%20application%20and%2C%20more%20generally%2C%20the%20GenAI%20models%20in%20helping%20both%20non-expert%20users%20and%20professionals%20in%20creating%20maps%20more%20efficiently.%20We%20have%20also%20outlined%20further%20technical%20improvements%20and%20emphasized%20the%20new%20role%20of%20cartographers%20to%20advance%20the%20paradigm%20of%20AI-assisted%20map-making.%20The%20code%20and%20pre-trained%20models%20are%20available%20at%20https%3A//github.com/claudaff/generative-ai-mapmaking/.&entry.1838667208=http%3A//arxiv.org/abs/2508.18959v2&entry.124074799=Read"},
{"title": "6D Strawberry Pose Estimation: Real-time and Edge AI Solutions Using Purely Synthetic Training Data", "author": "Saptarshi Neil Sinha and Julius K\u00fchn and Mika Silvan Goschke and Michael Weinmann", "abstract": "Automated and selective harvesting of fruits has become an important area of research, particularly due to challenges such as high costs and a shortage of seasonal labor in advanced economies. This paper focuses on 6D pose estimation of strawberries using purely synthetic data generated through a procedural pipeline for photorealistic rendering. We employ the YOLOX-6D-Pose algorithm, a single-shot approach that leverages the YOLOX backbone, known for its balance between speed and accuracy, and its support for edge inference. To address the lacking availability of training data, we introduce a robust and flexible pipeline for generating synthetic strawberry data from various 3D models via a procedural Blender pipeline, where we focus on enhancing the realism of the synthesized data in comparison to previous work to make it a valuable resource for training pose estimation algorithms. Quantitative evaluations indicate that our models achieve comparable accuracy on both the NVIDIA RTX 3090 and Jetson Orin Nano across several ADD-S metrics, with the RTX 3090 demonstrating superior processing speed. However, the Jetson Orin Nano is particularly suited for resource-constrained environments, making it an excellent choice for deployment in agricultural robotics. Qualitative assessments further confirm the model's performance, demonstrating its capability to accurately infer the poses of ripe and partially ripe strawberries, while facing challenges in detecting unripe specimens. This suggests opportunities for future improvements, especially in enhancing detection capabilities for unripe strawberries (if desired) by exploring variations in color. Furthermore, the methodology presented could be adapted easily for other fruits such as apples, peaches, and plums, thereby expanding its applicability and impact in the field of agricultural automation.", "link": "http://arxiv.org/abs/2511.11307v1", "date": "2025-11-14", "relevancy": 2.222, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5911}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5305}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%206D%20Strawberry%20Pose%20Estimation%3A%20Real-time%20and%20Edge%20AI%20Solutions%20Using%20Purely%20Synthetic%20Training%20Data&body=Title%3A%206D%20Strawberry%20Pose%20Estimation%3A%20Real-time%20and%20Edge%20AI%20Solutions%20Using%20Purely%20Synthetic%20Training%20Data%0AAuthor%3A%20Saptarshi%20Neil%20Sinha%20and%20Julius%20K%C3%BChn%20and%20Mika%20Silvan%20Goschke%20and%20Michael%20Weinmann%0AAbstract%3A%20Automated%20and%20selective%20harvesting%20of%20fruits%20has%20become%20an%20important%20area%20of%20research%2C%20particularly%20due%20to%20challenges%20such%20as%20high%20costs%20and%20a%20shortage%20of%20seasonal%20labor%20in%20advanced%20economies.%20This%20paper%20focuses%20on%206D%20pose%20estimation%20of%20strawberries%20using%20purely%20synthetic%20data%20generated%20through%20a%20procedural%20pipeline%20for%20photorealistic%20rendering.%20We%20employ%20the%20YOLOX-6D-Pose%20algorithm%2C%20a%20single-shot%20approach%20that%20leverages%20the%20YOLOX%20backbone%2C%20known%20for%20its%20balance%20between%20speed%20and%20accuracy%2C%20and%20its%20support%20for%20edge%20inference.%20To%20address%20the%20lacking%20availability%20of%20training%20data%2C%20we%20introduce%20a%20robust%20and%20flexible%20pipeline%20for%20generating%20synthetic%20strawberry%20data%20from%20various%203D%20models%20via%20a%20procedural%20Blender%20pipeline%2C%20where%20we%20focus%20on%20enhancing%20the%20realism%20of%20the%20synthesized%20data%20in%20comparison%20to%20previous%20work%20to%20make%20it%20a%20valuable%20resource%20for%20training%20pose%20estimation%20algorithms.%20Quantitative%20evaluations%20indicate%20that%20our%20models%20achieve%20comparable%20accuracy%20on%20both%20the%20NVIDIA%20RTX%203090%20and%20Jetson%20Orin%20Nano%20across%20several%20ADD-S%20metrics%2C%20with%20the%20RTX%203090%20demonstrating%20superior%20processing%20speed.%20However%2C%20the%20Jetson%20Orin%20Nano%20is%20particularly%20suited%20for%20resource-constrained%20environments%2C%20making%20it%20an%20excellent%20choice%20for%20deployment%20in%20agricultural%20robotics.%20Qualitative%20assessments%20further%20confirm%20the%20model%27s%20performance%2C%20demonstrating%20its%20capability%20to%20accurately%20infer%20the%20poses%20of%20ripe%20and%20partially%20ripe%20strawberries%2C%20while%20facing%20challenges%20in%20detecting%20unripe%20specimens.%20This%20suggests%20opportunities%20for%20future%20improvements%2C%20especially%20in%20enhancing%20detection%20capabilities%20for%20unripe%20strawberries%20%28if%20desired%29%20by%20exploring%20variations%20in%20color.%20Furthermore%2C%20the%20methodology%20presented%20could%20be%20adapted%20easily%20for%20other%20fruits%20such%20as%20apples%2C%20peaches%2C%20and%20plums%2C%20thereby%20expanding%20its%20applicability%20and%20impact%20in%20the%20field%20of%20agricultural%20automation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11307v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D6D%2520Strawberry%2520Pose%2520Estimation%253A%2520Real-time%2520and%2520Edge%2520AI%2520Solutions%2520Using%2520Purely%2520Synthetic%2520Training%2520Data%26entry.906535625%3DSaptarshi%2520Neil%2520Sinha%2520and%2520Julius%2520K%25C3%25BChn%2520and%2520Mika%2520Silvan%2520Goschke%2520and%2520Michael%2520Weinmann%26entry.1292438233%3DAutomated%2520and%2520selective%2520harvesting%2520of%2520fruits%2520has%2520become%2520an%2520important%2520area%2520of%2520research%252C%2520particularly%2520due%2520to%2520challenges%2520such%2520as%2520high%2520costs%2520and%2520a%2520shortage%2520of%2520seasonal%2520labor%2520in%2520advanced%2520economies.%2520This%2520paper%2520focuses%2520on%25206D%2520pose%2520estimation%2520of%2520strawberries%2520using%2520purely%2520synthetic%2520data%2520generated%2520through%2520a%2520procedural%2520pipeline%2520for%2520photorealistic%2520rendering.%2520We%2520employ%2520the%2520YOLOX-6D-Pose%2520algorithm%252C%2520a%2520single-shot%2520approach%2520that%2520leverages%2520the%2520YOLOX%2520backbone%252C%2520known%2520for%2520its%2520balance%2520between%2520speed%2520and%2520accuracy%252C%2520and%2520its%2520support%2520for%2520edge%2520inference.%2520To%2520address%2520the%2520lacking%2520availability%2520of%2520training%2520data%252C%2520we%2520introduce%2520a%2520robust%2520and%2520flexible%2520pipeline%2520for%2520generating%2520synthetic%2520strawberry%2520data%2520from%2520various%25203D%2520models%2520via%2520a%2520procedural%2520Blender%2520pipeline%252C%2520where%2520we%2520focus%2520on%2520enhancing%2520the%2520realism%2520of%2520the%2520synthesized%2520data%2520in%2520comparison%2520to%2520previous%2520work%2520to%2520make%2520it%2520a%2520valuable%2520resource%2520for%2520training%2520pose%2520estimation%2520algorithms.%2520Quantitative%2520evaluations%2520indicate%2520that%2520our%2520models%2520achieve%2520comparable%2520accuracy%2520on%2520both%2520the%2520NVIDIA%2520RTX%25203090%2520and%2520Jetson%2520Orin%2520Nano%2520across%2520several%2520ADD-S%2520metrics%252C%2520with%2520the%2520RTX%25203090%2520demonstrating%2520superior%2520processing%2520speed.%2520However%252C%2520the%2520Jetson%2520Orin%2520Nano%2520is%2520particularly%2520suited%2520for%2520resource-constrained%2520environments%252C%2520making%2520it%2520an%2520excellent%2520choice%2520for%2520deployment%2520in%2520agricultural%2520robotics.%2520Qualitative%2520assessments%2520further%2520confirm%2520the%2520model%2527s%2520performance%252C%2520demonstrating%2520its%2520capability%2520to%2520accurately%2520infer%2520the%2520poses%2520of%2520ripe%2520and%2520partially%2520ripe%2520strawberries%252C%2520while%2520facing%2520challenges%2520in%2520detecting%2520unripe%2520specimens.%2520This%2520suggests%2520opportunities%2520for%2520future%2520improvements%252C%2520especially%2520in%2520enhancing%2520detection%2520capabilities%2520for%2520unripe%2520strawberries%2520%2528if%2520desired%2529%2520by%2520exploring%2520variations%2520in%2520color.%2520Furthermore%252C%2520the%2520methodology%2520presented%2520could%2520be%2520adapted%2520easily%2520for%2520other%2520fruits%2520such%2520as%2520apples%252C%2520peaches%252C%2520and%2520plums%252C%2520thereby%2520expanding%2520its%2520applicability%2520and%2520impact%2520in%2520the%2520field%2520of%2520agricultural%2520automation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11307v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=6D%20Strawberry%20Pose%20Estimation%3A%20Real-time%20and%20Edge%20AI%20Solutions%20Using%20Purely%20Synthetic%20Training%20Data&entry.906535625=Saptarshi%20Neil%20Sinha%20and%20Julius%20K%C3%BChn%20and%20Mika%20Silvan%20Goschke%20and%20Michael%20Weinmann&entry.1292438233=Automated%20and%20selective%20harvesting%20of%20fruits%20has%20become%20an%20important%20area%20of%20research%2C%20particularly%20due%20to%20challenges%20such%20as%20high%20costs%20and%20a%20shortage%20of%20seasonal%20labor%20in%20advanced%20economies.%20This%20paper%20focuses%20on%206D%20pose%20estimation%20of%20strawberries%20using%20purely%20synthetic%20data%20generated%20through%20a%20procedural%20pipeline%20for%20photorealistic%20rendering.%20We%20employ%20the%20YOLOX-6D-Pose%20algorithm%2C%20a%20single-shot%20approach%20that%20leverages%20the%20YOLOX%20backbone%2C%20known%20for%20its%20balance%20between%20speed%20and%20accuracy%2C%20and%20its%20support%20for%20edge%20inference.%20To%20address%20the%20lacking%20availability%20of%20training%20data%2C%20we%20introduce%20a%20robust%20and%20flexible%20pipeline%20for%20generating%20synthetic%20strawberry%20data%20from%20various%203D%20models%20via%20a%20procedural%20Blender%20pipeline%2C%20where%20we%20focus%20on%20enhancing%20the%20realism%20of%20the%20synthesized%20data%20in%20comparison%20to%20previous%20work%20to%20make%20it%20a%20valuable%20resource%20for%20training%20pose%20estimation%20algorithms.%20Quantitative%20evaluations%20indicate%20that%20our%20models%20achieve%20comparable%20accuracy%20on%20both%20the%20NVIDIA%20RTX%203090%20and%20Jetson%20Orin%20Nano%20across%20several%20ADD-S%20metrics%2C%20with%20the%20RTX%203090%20demonstrating%20superior%20processing%20speed.%20However%2C%20the%20Jetson%20Orin%20Nano%20is%20particularly%20suited%20for%20resource-constrained%20environments%2C%20making%20it%20an%20excellent%20choice%20for%20deployment%20in%20agricultural%20robotics.%20Qualitative%20assessments%20further%20confirm%20the%20model%27s%20performance%2C%20demonstrating%20its%20capability%20to%20accurately%20infer%20the%20poses%20of%20ripe%20and%20partially%20ripe%20strawberries%2C%20while%20facing%20challenges%20in%20detecting%20unripe%20specimens.%20This%20suggests%20opportunities%20for%20future%20improvements%2C%20especially%20in%20enhancing%20detection%20capabilities%20for%20unripe%20strawberries%20%28if%20desired%29%20by%20exploring%20variations%20in%20color.%20Furthermore%2C%20the%20methodology%20presented%20could%20be%20adapted%20easily%20for%20other%20fruits%20such%20as%20apples%2C%20peaches%2C%20and%20plums%2C%20thereby%20expanding%20its%20applicability%20and%20impact%20in%20the%20field%20of%20agricultural%20automation.&entry.1838667208=http%3A//arxiv.org/abs/2511.11307v1&entry.124074799=Read"},
{"title": "Benchmarking Visual LLMs Resilience to Unanswerable Questions on Visually Rich Documents", "author": "Davide Napolitano and Luca Cagliero and Fabrizio Battiloro", "abstract": "The evolution of Visual Large Language Models (VLLMs) has revolutionized the automatic understanding of Visually Rich Documents (VRDs), which contain both textual and visual elements. Although VLLMs excel in Visual Question Answering (VQA) on multi-page VRDs, their ability to detect unanswerable questions is still an open research question. Our research delves into the robustness of the VLLMs to plausible yet unanswerable questions, i.e., questions that appear valid but cannot be answered due to subtle corruptions caused by swaps between related concepts or plausible question formulations. Corruptions are generated by replacing the original natural language entities with other ones of the same type, belonging to different document elements, and in different layout positions or pages of the related document. To this end, we present VRD-UQA (VISUALLY RICH DOCUMENT UNANSWERABLE QUESTION ANSWERING), a benchmark for evaluating VLLMs' resilience to plausible yet unanswerable questions across multiple dimensions. It automatically alters the questions of existing VQA datasets consisting of multi-page VRDs, verifies their unanswerability using a VLLM-as-a-judge approach, and then thoroughly evaluates VLLMs' performance. Experiments, run on 12 models, analyze: (1) The VLLMs' accuracy in detecting unanswerable questions at both page and document levels; (2) The effect of different types of corruption (NLP entity, document element, layout); (3) The effectiveness of different knowledge injection strategies based on in-context learning (OCR, multi-page selection, or the possibility of unanswerability). Our findings reveal VLLMs' limitations and demonstrate that VRD-UQA can serve as an evaluation framework for developing resilient document VQA systems.", "link": "http://arxiv.org/abs/2511.11468v1", "date": "2025-11-14", "relevancy": 2.214, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5536}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Visual%20LLMs%20Resilience%20to%20Unanswerable%20Questions%20on%20Visually%20Rich%20Documents&body=Title%3A%20Benchmarking%20Visual%20LLMs%20Resilience%20to%20Unanswerable%20Questions%20on%20Visually%20Rich%20Documents%0AAuthor%3A%20Davide%20Napolitano%20and%20Luca%20Cagliero%20and%20Fabrizio%20Battiloro%0AAbstract%3A%20The%20evolution%20of%20Visual%20Large%20Language%20Models%20%28VLLMs%29%20has%20revolutionized%20the%20automatic%20understanding%20of%20Visually%20Rich%20Documents%20%28VRDs%29%2C%20which%20contain%20both%20textual%20and%20visual%20elements.%20Although%20VLLMs%20excel%20in%20Visual%20Question%20Answering%20%28VQA%29%20on%20multi-page%20VRDs%2C%20their%20ability%20to%20detect%20unanswerable%20questions%20is%20still%20an%20open%20research%20question.%20Our%20research%20delves%20into%20the%20robustness%20of%20the%20VLLMs%20to%20plausible%20yet%20unanswerable%20questions%2C%20i.e.%2C%20questions%20that%20appear%20valid%20but%20cannot%20be%20answered%20due%20to%20subtle%20corruptions%20caused%20by%20swaps%20between%20related%20concepts%20or%20plausible%20question%20formulations.%20Corruptions%20are%20generated%20by%20replacing%20the%20original%20natural%20language%20entities%20with%20other%20ones%20of%20the%20same%20type%2C%20belonging%20to%20different%20document%20elements%2C%20and%20in%20different%20layout%20positions%20or%20pages%20of%20the%20related%20document.%20To%20this%20end%2C%20we%20present%20VRD-UQA%20%28VISUALLY%20RICH%20DOCUMENT%20UNANSWERABLE%20QUESTION%20ANSWERING%29%2C%20a%20benchmark%20for%20evaluating%20VLLMs%27%20resilience%20to%20plausible%20yet%20unanswerable%20questions%20across%20multiple%20dimensions.%20It%20automatically%20alters%20the%20questions%20of%20existing%20VQA%20datasets%20consisting%20of%20multi-page%20VRDs%2C%20verifies%20their%20unanswerability%20using%20a%20VLLM-as-a-judge%20approach%2C%20and%20then%20thoroughly%20evaluates%20VLLMs%27%20performance.%20Experiments%2C%20run%20on%2012%20models%2C%20analyze%3A%20%281%29%20The%20VLLMs%27%20accuracy%20in%20detecting%20unanswerable%20questions%20at%20both%20page%20and%20document%20levels%3B%20%282%29%20The%20effect%20of%20different%20types%20of%20corruption%20%28NLP%20entity%2C%20document%20element%2C%20layout%29%3B%20%283%29%20The%20effectiveness%20of%20different%20knowledge%20injection%20strategies%20based%20on%20in-context%20learning%20%28OCR%2C%20multi-page%20selection%2C%20or%20the%20possibility%20of%20unanswerability%29.%20Our%20findings%20reveal%20VLLMs%27%20limitations%20and%20demonstrate%20that%20VRD-UQA%20can%20serve%20as%20an%20evaluation%20framework%20for%20developing%20resilient%20document%20VQA%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Visual%2520LLMs%2520Resilience%2520to%2520Unanswerable%2520Questions%2520on%2520Visually%2520Rich%2520Documents%26entry.906535625%3DDavide%2520Napolitano%2520and%2520Luca%2520Cagliero%2520and%2520Fabrizio%2520Battiloro%26entry.1292438233%3DThe%2520evolution%2520of%2520Visual%2520Large%2520Language%2520Models%2520%2528VLLMs%2529%2520has%2520revolutionized%2520the%2520automatic%2520understanding%2520of%2520Visually%2520Rich%2520Documents%2520%2528VRDs%2529%252C%2520which%2520contain%2520both%2520textual%2520and%2520visual%2520elements.%2520Although%2520VLLMs%2520excel%2520in%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520on%2520multi-page%2520VRDs%252C%2520their%2520ability%2520to%2520detect%2520unanswerable%2520questions%2520is%2520still%2520an%2520open%2520research%2520question.%2520Our%2520research%2520delves%2520into%2520the%2520robustness%2520of%2520the%2520VLLMs%2520to%2520plausible%2520yet%2520unanswerable%2520questions%252C%2520i.e.%252C%2520questions%2520that%2520appear%2520valid%2520but%2520cannot%2520be%2520answered%2520due%2520to%2520subtle%2520corruptions%2520caused%2520by%2520swaps%2520between%2520related%2520concepts%2520or%2520plausible%2520question%2520formulations.%2520Corruptions%2520are%2520generated%2520by%2520replacing%2520the%2520original%2520natural%2520language%2520entities%2520with%2520other%2520ones%2520of%2520the%2520same%2520type%252C%2520belonging%2520to%2520different%2520document%2520elements%252C%2520and%2520in%2520different%2520layout%2520positions%2520or%2520pages%2520of%2520the%2520related%2520document.%2520To%2520this%2520end%252C%2520we%2520present%2520VRD-UQA%2520%2528VISUALLY%2520RICH%2520DOCUMENT%2520UNANSWERABLE%2520QUESTION%2520ANSWERING%2529%252C%2520a%2520benchmark%2520for%2520evaluating%2520VLLMs%2527%2520resilience%2520to%2520plausible%2520yet%2520unanswerable%2520questions%2520across%2520multiple%2520dimensions.%2520It%2520automatically%2520alters%2520the%2520questions%2520of%2520existing%2520VQA%2520datasets%2520consisting%2520of%2520multi-page%2520VRDs%252C%2520verifies%2520their%2520unanswerability%2520using%2520a%2520VLLM-as-a-judge%2520approach%252C%2520and%2520then%2520thoroughly%2520evaluates%2520VLLMs%2527%2520performance.%2520Experiments%252C%2520run%2520on%252012%2520models%252C%2520analyze%253A%2520%25281%2529%2520The%2520VLLMs%2527%2520accuracy%2520in%2520detecting%2520unanswerable%2520questions%2520at%2520both%2520page%2520and%2520document%2520levels%253B%2520%25282%2529%2520The%2520effect%2520of%2520different%2520types%2520of%2520corruption%2520%2528NLP%2520entity%252C%2520document%2520element%252C%2520layout%2529%253B%2520%25283%2529%2520The%2520effectiveness%2520of%2520different%2520knowledge%2520injection%2520strategies%2520based%2520on%2520in-context%2520learning%2520%2528OCR%252C%2520multi-page%2520selection%252C%2520or%2520the%2520possibility%2520of%2520unanswerability%2529.%2520Our%2520findings%2520reveal%2520VLLMs%2527%2520limitations%2520and%2520demonstrate%2520that%2520VRD-UQA%2520can%2520serve%2520as%2520an%2520evaluation%2520framework%2520for%2520developing%2520resilient%2520document%2520VQA%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Visual%20LLMs%20Resilience%20to%20Unanswerable%20Questions%20on%20Visually%20Rich%20Documents&entry.906535625=Davide%20Napolitano%20and%20Luca%20Cagliero%20and%20Fabrizio%20Battiloro&entry.1292438233=The%20evolution%20of%20Visual%20Large%20Language%20Models%20%28VLLMs%29%20has%20revolutionized%20the%20automatic%20understanding%20of%20Visually%20Rich%20Documents%20%28VRDs%29%2C%20which%20contain%20both%20textual%20and%20visual%20elements.%20Although%20VLLMs%20excel%20in%20Visual%20Question%20Answering%20%28VQA%29%20on%20multi-page%20VRDs%2C%20their%20ability%20to%20detect%20unanswerable%20questions%20is%20still%20an%20open%20research%20question.%20Our%20research%20delves%20into%20the%20robustness%20of%20the%20VLLMs%20to%20plausible%20yet%20unanswerable%20questions%2C%20i.e.%2C%20questions%20that%20appear%20valid%20but%20cannot%20be%20answered%20due%20to%20subtle%20corruptions%20caused%20by%20swaps%20between%20related%20concepts%20or%20plausible%20question%20formulations.%20Corruptions%20are%20generated%20by%20replacing%20the%20original%20natural%20language%20entities%20with%20other%20ones%20of%20the%20same%20type%2C%20belonging%20to%20different%20document%20elements%2C%20and%20in%20different%20layout%20positions%20or%20pages%20of%20the%20related%20document.%20To%20this%20end%2C%20we%20present%20VRD-UQA%20%28VISUALLY%20RICH%20DOCUMENT%20UNANSWERABLE%20QUESTION%20ANSWERING%29%2C%20a%20benchmark%20for%20evaluating%20VLLMs%27%20resilience%20to%20plausible%20yet%20unanswerable%20questions%20across%20multiple%20dimensions.%20It%20automatically%20alters%20the%20questions%20of%20existing%20VQA%20datasets%20consisting%20of%20multi-page%20VRDs%2C%20verifies%20their%20unanswerability%20using%20a%20VLLM-as-a-judge%20approach%2C%20and%20then%20thoroughly%20evaluates%20VLLMs%27%20performance.%20Experiments%2C%20run%20on%2012%20models%2C%20analyze%3A%20%281%29%20The%20VLLMs%27%20accuracy%20in%20detecting%20unanswerable%20questions%20at%20both%20page%20and%20document%20levels%3B%20%282%29%20The%20effect%20of%20different%20types%20of%20corruption%20%28NLP%20entity%2C%20document%20element%2C%20layout%29%3B%20%283%29%20The%20effectiveness%20of%20different%20knowledge%20injection%20strategies%20based%20on%20in-context%20learning%20%28OCR%2C%20multi-page%20selection%2C%20or%20the%20possibility%20of%20unanswerability%29.%20Our%20findings%20reveal%20VLLMs%27%20limitations%20and%20demonstrate%20that%20VRD-UQA%20can%20serve%20as%20an%20evaluation%20framework%20for%20developing%20resilient%20document%20VQA%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2511.11468v1&entry.124074799=Read"},
{"title": "Rethinking Efficient Mixture-of-Experts for Remote Sensing Modality-Missing Classification", "author": "Qinghao Gao and Jianhai Qu and Yunsong Li and Weiqiang Dong", "abstract": "Multimodal classification in remote sensing often suffers from missing modalities caused by environmental interference, sensor failures, or atmospheric effects, which severely degrade classification performance. Existing two-stage adaptation methods are computationally expensive and assume complete multimodal data during training, limiting their generalization to real-world incompleteness. To overcome these issues, we propose a Missing-aware Mixture-of-Loras (MaMOL) framework that reformulates modality missing as a multi-task learning problem. MaMOL introduces a dual-routing mechanism: a task-oriented dynamic router that adaptively activates experts for different missing patterns, and a modality-specific-shared static router that maintains stable cross-modal knowledge sharing. Unlike prior methods that train separate networks for each missing configuration, MaMOL achieves parameter-efficient adaptation via lightweight expert updates and shared expert reuse. Experiments on multiple remote sensing benchmarks demonstrate superior robustness and generalization under varying missing rates, with minimal computational overhead. Moreover, transfer experiments on natural image datasets validate its scalability and cross-domain applicability, highlighting MaMOL as a general and efficient solution for incomplete multimodal learning.", "link": "http://arxiv.org/abs/2511.11460v1", "date": "2025-11-14", "relevancy": 2.214, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5599}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5489}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Efficient%20Mixture-of-Experts%20for%20Remote%20Sensing%20Modality-Missing%20Classification&body=Title%3A%20Rethinking%20Efficient%20Mixture-of-Experts%20for%20Remote%20Sensing%20Modality-Missing%20Classification%0AAuthor%3A%20Qinghao%20Gao%20and%20Jianhai%20Qu%20and%20Yunsong%20Li%20and%20Weiqiang%20Dong%0AAbstract%3A%20Multimodal%20classification%20in%20remote%20sensing%20often%20suffers%20from%20missing%20modalities%20caused%20by%20environmental%20interference%2C%20sensor%20failures%2C%20or%20atmospheric%20effects%2C%20which%20severely%20degrade%20classification%20performance.%20Existing%20two-stage%20adaptation%20methods%20are%20computationally%20expensive%20and%20assume%20complete%20multimodal%20data%20during%20training%2C%20limiting%20their%20generalization%20to%20real-world%20incompleteness.%20To%20overcome%20these%20issues%2C%20we%20propose%20a%20Missing-aware%20Mixture-of-Loras%20%28MaMOL%29%20framework%20that%20reformulates%20modality%20missing%20as%20a%20multi-task%20learning%20problem.%20MaMOL%20introduces%20a%20dual-routing%20mechanism%3A%20a%20task-oriented%20dynamic%20router%20that%20adaptively%20activates%20experts%20for%20different%20missing%20patterns%2C%20and%20a%20modality-specific-shared%20static%20router%20that%20maintains%20stable%20cross-modal%20knowledge%20sharing.%20Unlike%20prior%20methods%20that%20train%20separate%20networks%20for%20each%20missing%20configuration%2C%20MaMOL%20achieves%20parameter-efficient%20adaptation%20via%20lightweight%20expert%20updates%20and%20shared%20expert%20reuse.%20Experiments%20on%20multiple%20remote%20sensing%20benchmarks%20demonstrate%20superior%20robustness%20and%20generalization%20under%20varying%20missing%20rates%2C%20with%20minimal%20computational%20overhead.%20Moreover%2C%20transfer%20experiments%20on%20natural%20image%20datasets%20validate%20its%20scalability%20and%20cross-domain%20applicability%2C%20highlighting%20MaMOL%20as%20a%20general%20and%20efficient%20solution%20for%20incomplete%20multimodal%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Efficient%2520Mixture-of-Experts%2520for%2520Remote%2520Sensing%2520Modality-Missing%2520Classification%26entry.906535625%3DQinghao%2520Gao%2520and%2520Jianhai%2520Qu%2520and%2520Yunsong%2520Li%2520and%2520Weiqiang%2520Dong%26entry.1292438233%3DMultimodal%2520classification%2520in%2520remote%2520sensing%2520often%2520suffers%2520from%2520missing%2520modalities%2520caused%2520by%2520environmental%2520interference%252C%2520sensor%2520failures%252C%2520or%2520atmospheric%2520effects%252C%2520which%2520severely%2520degrade%2520classification%2520performance.%2520Existing%2520two-stage%2520adaptation%2520methods%2520are%2520computationally%2520expensive%2520and%2520assume%2520complete%2520multimodal%2520data%2520during%2520training%252C%2520limiting%2520their%2520generalization%2520to%2520real-world%2520incompleteness.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520propose%2520a%2520Missing-aware%2520Mixture-of-Loras%2520%2528MaMOL%2529%2520framework%2520that%2520reformulates%2520modality%2520missing%2520as%2520a%2520multi-task%2520learning%2520problem.%2520MaMOL%2520introduces%2520a%2520dual-routing%2520mechanism%253A%2520a%2520task-oriented%2520dynamic%2520router%2520that%2520adaptively%2520activates%2520experts%2520for%2520different%2520missing%2520patterns%252C%2520and%2520a%2520modality-specific-shared%2520static%2520router%2520that%2520maintains%2520stable%2520cross-modal%2520knowledge%2520sharing.%2520Unlike%2520prior%2520methods%2520that%2520train%2520separate%2520networks%2520for%2520each%2520missing%2520configuration%252C%2520MaMOL%2520achieves%2520parameter-efficient%2520adaptation%2520via%2520lightweight%2520expert%2520updates%2520and%2520shared%2520expert%2520reuse.%2520Experiments%2520on%2520multiple%2520remote%2520sensing%2520benchmarks%2520demonstrate%2520superior%2520robustness%2520and%2520generalization%2520under%2520varying%2520missing%2520rates%252C%2520with%2520minimal%2520computational%2520overhead.%2520Moreover%252C%2520transfer%2520experiments%2520on%2520natural%2520image%2520datasets%2520validate%2520its%2520scalability%2520and%2520cross-domain%2520applicability%252C%2520highlighting%2520MaMOL%2520as%2520a%2520general%2520and%2520efficient%2520solution%2520for%2520incomplete%2520multimodal%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Efficient%20Mixture-of-Experts%20for%20Remote%20Sensing%20Modality-Missing%20Classification&entry.906535625=Qinghao%20Gao%20and%20Jianhai%20Qu%20and%20Yunsong%20Li%20and%20Weiqiang%20Dong&entry.1292438233=Multimodal%20classification%20in%20remote%20sensing%20often%20suffers%20from%20missing%20modalities%20caused%20by%20environmental%20interference%2C%20sensor%20failures%2C%20or%20atmospheric%20effects%2C%20which%20severely%20degrade%20classification%20performance.%20Existing%20two-stage%20adaptation%20methods%20are%20computationally%20expensive%20and%20assume%20complete%20multimodal%20data%20during%20training%2C%20limiting%20their%20generalization%20to%20real-world%20incompleteness.%20To%20overcome%20these%20issues%2C%20we%20propose%20a%20Missing-aware%20Mixture-of-Loras%20%28MaMOL%29%20framework%20that%20reformulates%20modality%20missing%20as%20a%20multi-task%20learning%20problem.%20MaMOL%20introduces%20a%20dual-routing%20mechanism%3A%20a%20task-oriented%20dynamic%20router%20that%20adaptively%20activates%20experts%20for%20different%20missing%20patterns%2C%20and%20a%20modality-specific-shared%20static%20router%20that%20maintains%20stable%20cross-modal%20knowledge%20sharing.%20Unlike%20prior%20methods%20that%20train%20separate%20networks%20for%20each%20missing%20configuration%2C%20MaMOL%20achieves%20parameter-efficient%20adaptation%20via%20lightweight%20expert%20updates%20and%20shared%20expert%20reuse.%20Experiments%20on%20multiple%20remote%20sensing%20benchmarks%20demonstrate%20superior%20robustness%20and%20generalization%20under%20varying%20missing%20rates%2C%20with%20minimal%20computational%20overhead.%20Moreover%2C%20transfer%20experiments%20on%20natural%20image%20datasets%20validate%20its%20scalability%20and%20cross-domain%20applicability%2C%20highlighting%20MaMOL%20as%20a%20general%20and%20efficient%20solution%20for%20incomplete%20multimodal%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2511.11460v1&entry.124074799=Read"},
{"title": "Enhancing Group Recommendation using Soft Impute Singular Value Decomposition", "author": "Mubaraka Sani Ibrahim and Isah Charles Saidu and Lehel Csato", "abstract": "The growing popularity of group activities increased the need to develop methods for providing recommendations to a group of users based on the collective preferences of the group members. Several group recommender systems have been proposed, but these methods often struggle due to sparsity and high-dimensionality of the available data, common in many real-world applications. In this paper, we propose a group recommender system called Group Soft-Impute SVD, which leverages soft-impute singular value decomposition to enhance group recommendations. This approach addresses the challenge of sparse high-dimensional data using low-rank matrix completion. We compared the performance of Group Soft-Impute SVD with Group MF based approaches and found that our method outperforms the baselines in recall for small user groups while achieving comparable results across all group sizes when tasked on Goodbooks, Movielens, and Synthetic datasets. Furthermore, our method recovers lower matrix ranks than the baselines, demonstrating its effectiveness in handling high-dimensional data.", "link": "http://arxiv.org/abs/2511.11172v1", "date": "2025-11-14", "relevancy": 2.2077, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4448}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4415}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Group%20Recommendation%20using%20Soft%20Impute%20Singular%20Value%20Decomposition&body=Title%3A%20Enhancing%20Group%20Recommendation%20using%20Soft%20Impute%20Singular%20Value%20Decomposition%0AAuthor%3A%20Mubaraka%20Sani%20Ibrahim%20and%20Isah%20Charles%20Saidu%20and%20Lehel%20Csato%0AAbstract%3A%20The%20growing%20popularity%20of%20group%20activities%20increased%20the%20need%20to%20develop%20methods%20for%20providing%20recommendations%20to%20a%20group%20of%20users%20based%20on%20the%20collective%20preferences%20of%20the%20group%20members.%20Several%20group%20recommender%20systems%20have%20been%20proposed%2C%20but%20these%20methods%20often%20struggle%20due%20to%20sparsity%20and%20high-dimensionality%20of%20the%20available%20data%2C%20common%20in%20many%20real-world%20applications.%20In%20this%20paper%2C%20we%20propose%20a%20group%20recommender%20system%20called%20Group%20Soft-Impute%20SVD%2C%20which%20leverages%20soft-impute%20singular%20value%20decomposition%20to%20enhance%20group%20recommendations.%20This%20approach%20addresses%20the%20challenge%20of%20sparse%20high-dimensional%20data%20using%20low-rank%20matrix%20completion.%20We%20compared%20the%20performance%20of%20Group%20Soft-Impute%20SVD%20with%20Group%20MF%20based%20approaches%20and%20found%20that%20our%20method%20outperforms%20the%20baselines%20in%20recall%20for%20small%20user%20groups%20while%20achieving%20comparable%20results%20across%20all%20group%20sizes%20when%20tasked%20on%20Goodbooks%2C%20Movielens%2C%20and%20Synthetic%20datasets.%20Furthermore%2C%20our%20method%20recovers%20lower%20matrix%20ranks%20than%20the%20baselines%2C%20demonstrating%20its%20effectiveness%20in%20handling%20high-dimensional%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Group%2520Recommendation%2520using%2520Soft%2520Impute%2520Singular%2520Value%2520Decomposition%26entry.906535625%3DMubaraka%2520Sani%2520Ibrahim%2520and%2520Isah%2520Charles%2520Saidu%2520and%2520Lehel%2520Csato%26entry.1292438233%3DThe%2520growing%2520popularity%2520of%2520group%2520activities%2520increased%2520the%2520need%2520to%2520develop%2520methods%2520for%2520providing%2520recommendations%2520to%2520a%2520group%2520of%2520users%2520based%2520on%2520the%2520collective%2520preferences%2520of%2520the%2520group%2520members.%2520Several%2520group%2520recommender%2520systems%2520have%2520been%2520proposed%252C%2520but%2520these%2520methods%2520often%2520struggle%2520due%2520to%2520sparsity%2520and%2520high-dimensionality%2520of%2520the%2520available%2520data%252C%2520common%2520in%2520many%2520real-world%2520applications.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520group%2520recommender%2520system%2520called%2520Group%2520Soft-Impute%2520SVD%252C%2520which%2520leverages%2520soft-impute%2520singular%2520value%2520decomposition%2520to%2520enhance%2520group%2520recommendations.%2520This%2520approach%2520addresses%2520the%2520challenge%2520of%2520sparse%2520high-dimensional%2520data%2520using%2520low-rank%2520matrix%2520completion.%2520We%2520compared%2520the%2520performance%2520of%2520Group%2520Soft-Impute%2520SVD%2520with%2520Group%2520MF%2520based%2520approaches%2520and%2520found%2520that%2520our%2520method%2520outperforms%2520the%2520baselines%2520in%2520recall%2520for%2520small%2520user%2520groups%2520while%2520achieving%2520comparable%2520results%2520across%2520all%2520group%2520sizes%2520when%2520tasked%2520on%2520Goodbooks%252C%2520Movielens%252C%2520and%2520Synthetic%2520datasets.%2520Furthermore%252C%2520our%2520method%2520recovers%2520lower%2520matrix%2520ranks%2520than%2520the%2520baselines%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520handling%2520high-dimensional%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Group%20Recommendation%20using%20Soft%20Impute%20Singular%20Value%20Decomposition&entry.906535625=Mubaraka%20Sani%20Ibrahim%20and%20Isah%20Charles%20Saidu%20and%20Lehel%20Csato&entry.1292438233=The%20growing%20popularity%20of%20group%20activities%20increased%20the%20need%20to%20develop%20methods%20for%20providing%20recommendations%20to%20a%20group%20of%20users%20based%20on%20the%20collective%20preferences%20of%20the%20group%20members.%20Several%20group%20recommender%20systems%20have%20been%20proposed%2C%20but%20these%20methods%20often%20struggle%20due%20to%20sparsity%20and%20high-dimensionality%20of%20the%20available%20data%2C%20common%20in%20many%20real-world%20applications.%20In%20this%20paper%2C%20we%20propose%20a%20group%20recommender%20system%20called%20Group%20Soft-Impute%20SVD%2C%20which%20leverages%20soft-impute%20singular%20value%20decomposition%20to%20enhance%20group%20recommendations.%20This%20approach%20addresses%20the%20challenge%20of%20sparse%20high-dimensional%20data%20using%20low-rank%20matrix%20completion.%20We%20compared%20the%20performance%20of%20Group%20Soft-Impute%20SVD%20with%20Group%20MF%20based%20approaches%20and%20found%20that%20our%20method%20outperforms%20the%20baselines%20in%20recall%20for%20small%20user%20groups%20while%20achieving%20comparable%20results%20across%20all%20group%20sizes%20when%20tasked%20on%20Goodbooks%2C%20Movielens%2C%20and%20Synthetic%20datasets.%20Furthermore%2C%20our%20method%20recovers%20lower%20matrix%20ranks%20than%20the%20baselines%2C%20demonstrating%20its%20effectiveness%20in%20handling%20high-dimensional%20data.&entry.1838667208=http%3A//arxiv.org/abs/2511.11172v1&entry.124074799=Read"},
{"title": "Unsupervised Segmentation of Micro-CT Scans of Polyurethane Structures By Combining Hidden-Markov-Random Fields and a U-Net", "author": "Julian Grolig and Lars Griem and Michael Selzer and Hans-Ulrich Kauczor and Simon M. F. Triphan and Britta Nestler and Arnd Koeppe", "abstract": "Extracting digital material representations from images is a necessary prerequisite for a quantitative analysis of material properties. Different segmentation approaches have been extensively studied in the past to achieve this task, but were often lacking accuracy or speed. With the advent of machine learning, supervised convolutional neural networks (CNNs) have achieved state-of-the-art performance for different segmentation tasks. However, these models are often trained in a supervised manner, which requires large labeled datasets. Unsupervised approaches do not require ground-truth data for learning, but suffer from long segmentation times and often worse segmentation accuracy. Hidden Markov Random Fields (HMRF) are an unsupervised segmentation approach that incorporates concepts of neighborhood and class distributions. We present a method that integrates HMRF theory and CNN segmentation, leveraging the advantages of both areas: unsupervised learning and fast segmentation times. We investigate the contribution of different neighborhood terms and components for the unsupervised HMRF loss. We demonstrate that the HMRF-UNet enables high segmentation accuracy without ground truth on a Micro-Computed Tomography ($\u03bc$CT) image dataset of Polyurethane (PU) foam structures. Finally, we propose and demonstrate a pre-training strategy that considerably reduces the required amount of ground-truth data when training a segmentation model.", "link": "http://arxiv.org/abs/2511.11378v1", "date": "2025-11-14", "relevancy": 2.1938, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5525}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5462}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Segmentation%20of%20Micro-CT%20Scans%20of%20Polyurethane%20Structures%20By%20Combining%20Hidden-Markov-Random%20Fields%20and%20a%20U-Net&body=Title%3A%20Unsupervised%20Segmentation%20of%20Micro-CT%20Scans%20of%20Polyurethane%20Structures%20By%20Combining%20Hidden-Markov-Random%20Fields%20and%20a%20U-Net%0AAuthor%3A%20Julian%20Grolig%20and%20Lars%20Griem%20and%20Michael%20Selzer%20and%20Hans-Ulrich%20Kauczor%20and%20Simon%20M.%20F.%20Triphan%20and%20Britta%20Nestler%20and%20Arnd%20Koeppe%0AAbstract%3A%20Extracting%20digital%20material%20representations%20from%20images%20is%20a%20necessary%20prerequisite%20for%20a%20quantitative%20analysis%20of%20material%20properties.%20Different%20segmentation%20approaches%20have%20been%20extensively%20studied%20in%20the%20past%20to%20achieve%20this%20task%2C%20but%20were%20often%20lacking%20accuracy%20or%20speed.%20With%20the%20advent%20of%20machine%20learning%2C%20supervised%20convolutional%20neural%20networks%20%28CNNs%29%20have%20achieved%20state-of-the-art%20performance%20for%20different%20segmentation%20tasks.%20However%2C%20these%20models%20are%20often%20trained%20in%20a%20supervised%20manner%2C%20which%20requires%20large%20labeled%20datasets.%20Unsupervised%20approaches%20do%20not%20require%20ground-truth%20data%20for%20learning%2C%20but%20suffer%20from%20long%20segmentation%20times%20and%20often%20worse%20segmentation%20accuracy.%20Hidden%20Markov%20Random%20Fields%20%28HMRF%29%20are%20an%20unsupervised%20segmentation%20approach%20that%20incorporates%20concepts%20of%20neighborhood%20and%20class%20distributions.%20We%20present%20a%20method%20that%20integrates%20HMRF%20theory%20and%20CNN%20segmentation%2C%20leveraging%20the%20advantages%20of%20both%20areas%3A%20unsupervised%20learning%20and%20fast%20segmentation%20times.%20We%20investigate%20the%20contribution%20of%20different%20neighborhood%20terms%20and%20components%20for%20the%20unsupervised%20HMRF%20loss.%20We%20demonstrate%20that%20the%20HMRF-UNet%20enables%20high%20segmentation%20accuracy%20without%20ground%20truth%20on%20a%20Micro-Computed%20Tomography%20%28%24%CE%BC%24CT%29%20image%20dataset%20of%20Polyurethane%20%28PU%29%20foam%20structures.%20Finally%2C%20we%20propose%20and%20demonstrate%20a%20pre-training%20strategy%20that%20considerably%20reduces%20the%20required%20amount%20of%20ground-truth%20data%20when%20training%20a%20segmentation%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11378v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Segmentation%2520of%2520Micro-CT%2520Scans%2520of%2520Polyurethane%2520Structures%2520By%2520Combining%2520Hidden-Markov-Random%2520Fields%2520and%2520a%2520U-Net%26entry.906535625%3DJulian%2520Grolig%2520and%2520Lars%2520Griem%2520and%2520Michael%2520Selzer%2520and%2520Hans-Ulrich%2520Kauczor%2520and%2520Simon%2520M.%2520F.%2520Triphan%2520and%2520Britta%2520Nestler%2520and%2520Arnd%2520Koeppe%26entry.1292438233%3DExtracting%2520digital%2520material%2520representations%2520from%2520images%2520is%2520a%2520necessary%2520prerequisite%2520for%2520a%2520quantitative%2520analysis%2520of%2520material%2520properties.%2520Different%2520segmentation%2520approaches%2520have%2520been%2520extensively%2520studied%2520in%2520the%2520past%2520to%2520achieve%2520this%2520task%252C%2520but%2520were%2520often%2520lacking%2520accuracy%2520or%2520speed.%2520With%2520the%2520advent%2520of%2520machine%2520learning%252C%2520supervised%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520have%2520achieved%2520state-of-the-art%2520performance%2520for%2520different%2520segmentation%2520tasks.%2520However%252C%2520these%2520models%2520are%2520often%2520trained%2520in%2520a%2520supervised%2520manner%252C%2520which%2520requires%2520large%2520labeled%2520datasets.%2520Unsupervised%2520approaches%2520do%2520not%2520require%2520ground-truth%2520data%2520for%2520learning%252C%2520but%2520suffer%2520from%2520long%2520segmentation%2520times%2520and%2520often%2520worse%2520segmentation%2520accuracy.%2520Hidden%2520Markov%2520Random%2520Fields%2520%2528HMRF%2529%2520are%2520an%2520unsupervised%2520segmentation%2520approach%2520that%2520incorporates%2520concepts%2520of%2520neighborhood%2520and%2520class%2520distributions.%2520We%2520present%2520a%2520method%2520that%2520integrates%2520HMRF%2520theory%2520and%2520CNN%2520segmentation%252C%2520leveraging%2520the%2520advantages%2520of%2520both%2520areas%253A%2520unsupervised%2520learning%2520and%2520fast%2520segmentation%2520times.%2520We%2520investigate%2520the%2520contribution%2520of%2520different%2520neighborhood%2520terms%2520and%2520components%2520for%2520the%2520unsupervised%2520HMRF%2520loss.%2520We%2520demonstrate%2520that%2520the%2520HMRF-UNet%2520enables%2520high%2520segmentation%2520accuracy%2520without%2520ground%2520truth%2520on%2520a%2520Micro-Computed%2520Tomography%2520%2528%2524%25CE%25BC%2524CT%2529%2520image%2520dataset%2520of%2520Polyurethane%2520%2528PU%2529%2520foam%2520structures.%2520Finally%252C%2520we%2520propose%2520and%2520demonstrate%2520a%2520pre-training%2520strategy%2520that%2520considerably%2520reduces%2520the%2520required%2520amount%2520of%2520ground-truth%2520data%2520when%2520training%2520a%2520segmentation%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11378v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Segmentation%20of%20Micro-CT%20Scans%20of%20Polyurethane%20Structures%20By%20Combining%20Hidden-Markov-Random%20Fields%20and%20a%20U-Net&entry.906535625=Julian%20Grolig%20and%20Lars%20Griem%20and%20Michael%20Selzer%20and%20Hans-Ulrich%20Kauczor%20and%20Simon%20M.%20F.%20Triphan%20and%20Britta%20Nestler%20and%20Arnd%20Koeppe&entry.1292438233=Extracting%20digital%20material%20representations%20from%20images%20is%20a%20necessary%20prerequisite%20for%20a%20quantitative%20analysis%20of%20material%20properties.%20Different%20segmentation%20approaches%20have%20been%20extensively%20studied%20in%20the%20past%20to%20achieve%20this%20task%2C%20but%20were%20often%20lacking%20accuracy%20or%20speed.%20With%20the%20advent%20of%20machine%20learning%2C%20supervised%20convolutional%20neural%20networks%20%28CNNs%29%20have%20achieved%20state-of-the-art%20performance%20for%20different%20segmentation%20tasks.%20However%2C%20these%20models%20are%20often%20trained%20in%20a%20supervised%20manner%2C%20which%20requires%20large%20labeled%20datasets.%20Unsupervised%20approaches%20do%20not%20require%20ground-truth%20data%20for%20learning%2C%20but%20suffer%20from%20long%20segmentation%20times%20and%20often%20worse%20segmentation%20accuracy.%20Hidden%20Markov%20Random%20Fields%20%28HMRF%29%20are%20an%20unsupervised%20segmentation%20approach%20that%20incorporates%20concepts%20of%20neighborhood%20and%20class%20distributions.%20We%20present%20a%20method%20that%20integrates%20HMRF%20theory%20and%20CNN%20segmentation%2C%20leveraging%20the%20advantages%20of%20both%20areas%3A%20unsupervised%20learning%20and%20fast%20segmentation%20times.%20We%20investigate%20the%20contribution%20of%20different%20neighborhood%20terms%20and%20components%20for%20the%20unsupervised%20HMRF%20loss.%20We%20demonstrate%20that%20the%20HMRF-UNet%20enables%20high%20segmentation%20accuracy%20without%20ground%20truth%20on%20a%20Micro-Computed%20Tomography%20%28%24%CE%BC%24CT%29%20image%20dataset%20of%20Polyurethane%20%28PU%29%20foam%20structures.%20Finally%2C%20we%20propose%20and%20demonstrate%20a%20pre-training%20strategy%20that%20considerably%20reduces%20the%20required%20amount%20of%20ground-truth%20data%20when%20training%20a%20segmentation%20model.&entry.1838667208=http%3A//arxiv.org/abs/2511.11378v1&entry.124074799=Read"},
{"title": "NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation", "author": "Max Gandyra and Alessandro Santonicola and Michael Beetz", "abstract": "Instance segmentation of novel objects instances in RGB images, given some example images for each object, is a well known problem in computer vision. Designing a model general enough to be employed for all kinds of novel objects without (re-) training has proven to be a difficult task. To handle this, we present a new training-free framework, called: Novel Object Cyclic Threshold based Instance Segmentation (NOCTIS). NOCTIS integrates two pre-trained models: Grounded-SAM 2 for object proposals with precise bounding boxes and corresponding segmentation masks; and DINOv2 for robust class and patch embeddings, due to its zero-shot capabilities. Internally, the proposal-object matching is realized by determining an object matching score based on the similarity of the class embeddings and the average maximum similarity of the patch embeddings with a new cyclic thresholding (CT) mechanism that mitigates unstable matches caused by repetitive textures or visually similar patterns. Beyond CT, NOCTIS introduces: (i) an appearance score that is unaffected by object selection bias; (ii) the usage of the average confidence of the proposals' bounding box and mask as a scoring component; and (iii) an RGB-only pipeline that performs even better than RGB-D ones. We empirically show that NOCTIS, without further training/fine tuning, outperforms the best RGB and RGB-D methods regarding the mean AP score on the seven core datasets of the BOP 2023 challenge for the \"Model-based 2D segmentation of unseen objects\" task.", "link": "http://arxiv.org/abs/2507.01463v3", "date": "2025-11-14", "relevancy": 2.1905, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5604}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5473}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NOCTIS%3A%20Novel%20Object%20Cyclic%20Threshold%20based%20Instance%20Segmentation&body=Title%3A%20NOCTIS%3A%20Novel%20Object%20Cyclic%20Threshold%20based%20Instance%20Segmentation%0AAuthor%3A%20Max%20Gandyra%20and%20Alessandro%20Santonicola%20and%20Michael%20Beetz%0AAbstract%3A%20Instance%20segmentation%20of%20novel%20objects%20instances%20in%20RGB%20images%2C%20given%20some%20example%20images%20for%20each%20object%2C%20is%20a%20well%20known%20problem%20in%20computer%20vision.%20Designing%20a%20model%20general%20enough%20to%20be%20employed%20for%20all%20kinds%20of%20novel%20objects%20without%20%28re-%29%20training%20has%20proven%20to%20be%20a%20difficult%20task.%20To%20handle%20this%2C%20we%20present%20a%20new%20training-free%20framework%2C%20called%3A%20Novel%20Object%20Cyclic%20Threshold%20based%20Instance%20Segmentation%20%28NOCTIS%29.%20NOCTIS%20integrates%20two%20pre-trained%20models%3A%20Grounded-SAM%202%20for%20object%20proposals%20with%20precise%20bounding%20boxes%20and%20corresponding%20segmentation%20masks%3B%20and%20DINOv2%20for%20robust%20class%20and%20patch%20embeddings%2C%20due%20to%20its%20zero-shot%20capabilities.%20Internally%2C%20the%20proposal-object%20matching%20is%20realized%20by%20determining%20an%20object%20matching%20score%20based%20on%20the%20similarity%20of%20the%20class%20embeddings%20and%20the%20average%20maximum%20similarity%20of%20the%20patch%20embeddings%20with%20a%20new%20cyclic%20thresholding%20%28CT%29%20mechanism%20that%20mitigates%20unstable%20matches%20caused%20by%20repetitive%20textures%20or%20visually%20similar%20patterns.%20Beyond%20CT%2C%20NOCTIS%20introduces%3A%20%28i%29%20an%20appearance%20score%20that%20is%20unaffected%20by%20object%20selection%20bias%3B%20%28ii%29%20the%20usage%20of%20the%20average%20confidence%20of%20the%20proposals%27%20bounding%20box%20and%20mask%20as%20a%20scoring%20component%3B%20and%20%28iii%29%20an%20RGB-only%20pipeline%20that%20performs%20even%20better%20than%20RGB-D%20ones.%20We%20empirically%20show%20that%20NOCTIS%2C%20without%20further%20training/fine%20tuning%2C%20outperforms%20the%20best%20RGB%20and%20RGB-D%20methods%20regarding%20the%20mean%20AP%20score%20on%20the%20seven%20core%20datasets%20of%20the%20BOP%202023%20challenge%20for%20the%20%22Model-based%202D%20segmentation%20of%20unseen%20objects%22%20task.%0ALink%3A%20http%3A//arxiv.org/abs/2507.01463v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNOCTIS%253A%2520Novel%2520Object%2520Cyclic%2520Threshold%2520based%2520Instance%2520Segmentation%26entry.906535625%3DMax%2520Gandyra%2520and%2520Alessandro%2520Santonicola%2520and%2520Michael%2520Beetz%26entry.1292438233%3DInstance%2520segmentation%2520of%2520novel%2520objects%2520instances%2520in%2520RGB%2520images%252C%2520given%2520some%2520example%2520images%2520for%2520each%2520object%252C%2520is%2520a%2520well%2520known%2520problem%2520in%2520computer%2520vision.%2520Designing%2520a%2520model%2520general%2520enough%2520to%2520be%2520employed%2520for%2520all%2520kinds%2520of%2520novel%2520objects%2520without%2520%2528re-%2529%2520training%2520has%2520proven%2520to%2520be%2520a%2520difficult%2520task.%2520To%2520handle%2520this%252C%2520we%2520present%2520a%2520new%2520training-free%2520framework%252C%2520called%253A%2520Novel%2520Object%2520Cyclic%2520Threshold%2520based%2520Instance%2520Segmentation%2520%2528NOCTIS%2529.%2520NOCTIS%2520integrates%2520two%2520pre-trained%2520models%253A%2520Grounded-SAM%25202%2520for%2520object%2520proposals%2520with%2520precise%2520bounding%2520boxes%2520and%2520corresponding%2520segmentation%2520masks%253B%2520and%2520DINOv2%2520for%2520robust%2520class%2520and%2520patch%2520embeddings%252C%2520due%2520to%2520its%2520zero-shot%2520capabilities.%2520Internally%252C%2520the%2520proposal-object%2520matching%2520is%2520realized%2520by%2520determining%2520an%2520object%2520matching%2520score%2520based%2520on%2520the%2520similarity%2520of%2520the%2520class%2520embeddings%2520and%2520the%2520average%2520maximum%2520similarity%2520of%2520the%2520patch%2520embeddings%2520with%2520a%2520new%2520cyclic%2520thresholding%2520%2528CT%2529%2520mechanism%2520that%2520mitigates%2520unstable%2520matches%2520caused%2520by%2520repetitive%2520textures%2520or%2520visually%2520similar%2520patterns.%2520Beyond%2520CT%252C%2520NOCTIS%2520introduces%253A%2520%2528i%2529%2520an%2520appearance%2520score%2520that%2520is%2520unaffected%2520by%2520object%2520selection%2520bias%253B%2520%2528ii%2529%2520the%2520usage%2520of%2520the%2520average%2520confidence%2520of%2520the%2520proposals%2527%2520bounding%2520box%2520and%2520mask%2520as%2520a%2520scoring%2520component%253B%2520and%2520%2528iii%2529%2520an%2520RGB-only%2520pipeline%2520that%2520performs%2520even%2520better%2520than%2520RGB-D%2520ones.%2520We%2520empirically%2520show%2520that%2520NOCTIS%252C%2520without%2520further%2520training/fine%2520tuning%252C%2520outperforms%2520the%2520best%2520RGB%2520and%2520RGB-D%2520methods%2520regarding%2520the%2520mean%2520AP%2520score%2520on%2520the%2520seven%2520core%2520datasets%2520of%2520the%2520BOP%25202023%2520challenge%2520for%2520the%2520%2522Model-based%25202D%2520segmentation%2520of%2520unseen%2520objects%2522%2520task.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01463v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NOCTIS%3A%20Novel%20Object%20Cyclic%20Threshold%20based%20Instance%20Segmentation&entry.906535625=Max%20Gandyra%20and%20Alessandro%20Santonicola%20and%20Michael%20Beetz&entry.1292438233=Instance%20segmentation%20of%20novel%20objects%20instances%20in%20RGB%20images%2C%20given%20some%20example%20images%20for%20each%20object%2C%20is%20a%20well%20known%20problem%20in%20computer%20vision.%20Designing%20a%20model%20general%20enough%20to%20be%20employed%20for%20all%20kinds%20of%20novel%20objects%20without%20%28re-%29%20training%20has%20proven%20to%20be%20a%20difficult%20task.%20To%20handle%20this%2C%20we%20present%20a%20new%20training-free%20framework%2C%20called%3A%20Novel%20Object%20Cyclic%20Threshold%20based%20Instance%20Segmentation%20%28NOCTIS%29.%20NOCTIS%20integrates%20two%20pre-trained%20models%3A%20Grounded-SAM%202%20for%20object%20proposals%20with%20precise%20bounding%20boxes%20and%20corresponding%20segmentation%20masks%3B%20and%20DINOv2%20for%20robust%20class%20and%20patch%20embeddings%2C%20due%20to%20its%20zero-shot%20capabilities.%20Internally%2C%20the%20proposal-object%20matching%20is%20realized%20by%20determining%20an%20object%20matching%20score%20based%20on%20the%20similarity%20of%20the%20class%20embeddings%20and%20the%20average%20maximum%20similarity%20of%20the%20patch%20embeddings%20with%20a%20new%20cyclic%20thresholding%20%28CT%29%20mechanism%20that%20mitigates%20unstable%20matches%20caused%20by%20repetitive%20textures%20or%20visually%20similar%20patterns.%20Beyond%20CT%2C%20NOCTIS%20introduces%3A%20%28i%29%20an%20appearance%20score%20that%20is%20unaffected%20by%20object%20selection%20bias%3B%20%28ii%29%20the%20usage%20of%20the%20average%20confidence%20of%20the%20proposals%27%20bounding%20box%20and%20mask%20as%20a%20scoring%20component%3B%20and%20%28iii%29%20an%20RGB-only%20pipeline%20that%20performs%20even%20better%20than%20RGB-D%20ones.%20We%20empirically%20show%20that%20NOCTIS%2C%20without%20further%20training/fine%20tuning%2C%20outperforms%20the%20best%20RGB%20and%20RGB-D%20methods%20regarding%20the%20mean%20AP%20score%20on%20the%20seven%20core%20datasets%20of%20the%20BOP%202023%20challenge%20for%20the%20%22Model-based%202D%20segmentation%20of%20unseen%20objects%22%20task.&entry.1838667208=http%3A//arxiv.org/abs/2507.01463v3&entry.124074799=Read"},
{"title": "Low-Bit, High-Fidelity: Optimal Transport Quantization for Flow Matching", "author": "Dara Varam and Diaa A. Abuhani and Imran Zualkernan and Raghad AlDamani and Lujain Khalil", "abstract": "Flow Matching (FM) generative models offer efficient simulation-free training and deterministic sampling, but their practical deployment is challenged by high-precision parameter requirements. We adapt optimal transport (OT)-based post-training quantization to FM models, minimizing the 2-Wasserstein distance between quantized and original weights, and systematically compare its effectiveness against uniform, piecewise, and logarithmic quantization schemes. Our theoretical analysis provides upper bounds on generative degradation under quantization, and empirical results across five benchmark datasets of varying complexity show that OT-based quantization preserves both visual generation quality and latent space stability down to 2-3 bits per parameter, where alternative methods fail. This establishes OT-based quantization as a principled, effective approach to compress FM generative models for edge and embedded AI applications.", "link": "http://arxiv.org/abs/2511.11418v1", "date": "2025-11-14", "relevancy": 2.1837, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5868}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5385}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Bit%2C%20High-Fidelity%3A%20Optimal%20Transport%20Quantization%20for%20Flow%20Matching&body=Title%3A%20Low-Bit%2C%20High-Fidelity%3A%20Optimal%20Transport%20Quantization%20for%20Flow%20Matching%0AAuthor%3A%20Dara%20Varam%20and%20Diaa%20A.%20Abuhani%20and%20Imran%20Zualkernan%20and%20Raghad%20AlDamani%20and%20Lujain%20Khalil%0AAbstract%3A%20Flow%20Matching%20%28FM%29%20generative%20models%20offer%20efficient%20simulation-free%20training%20and%20deterministic%20sampling%2C%20but%20their%20practical%20deployment%20is%20challenged%20by%20high-precision%20parameter%20requirements.%20We%20adapt%20optimal%20transport%20%28OT%29-based%20post-training%20quantization%20to%20FM%20models%2C%20minimizing%20the%202-Wasserstein%20distance%20between%20quantized%20and%20original%20weights%2C%20and%20systematically%20compare%20its%20effectiveness%20against%20uniform%2C%20piecewise%2C%20and%20logarithmic%20quantization%20schemes.%20Our%20theoretical%20analysis%20provides%20upper%20bounds%20on%20generative%20degradation%20under%20quantization%2C%20and%20empirical%20results%20across%20five%20benchmark%20datasets%20of%20varying%20complexity%20show%20that%20OT-based%20quantization%20preserves%20both%20visual%20generation%20quality%20and%20latent%20space%20stability%20down%20to%202-3%20bits%20per%20parameter%2C%20where%20alternative%20methods%20fail.%20This%20establishes%20OT-based%20quantization%20as%20a%20principled%2C%20effective%20approach%20to%20compress%20FM%20generative%20models%20for%20edge%20and%20embedded%20AI%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Bit%252C%2520High-Fidelity%253A%2520Optimal%2520Transport%2520Quantization%2520for%2520Flow%2520Matching%26entry.906535625%3DDara%2520Varam%2520and%2520Diaa%2520A.%2520Abuhani%2520and%2520Imran%2520Zualkernan%2520and%2520Raghad%2520AlDamani%2520and%2520Lujain%2520Khalil%26entry.1292438233%3DFlow%2520Matching%2520%2528FM%2529%2520generative%2520models%2520offer%2520efficient%2520simulation-free%2520training%2520and%2520deterministic%2520sampling%252C%2520but%2520their%2520practical%2520deployment%2520is%2520challenged%2520by%2520high-precision%2520parameter%2520requirements.%2520We%2520adapt%2520optimal%2520transport%2520%2528OT%2529-based%2520post-training%2520quantization%2520to%2520FM%2520models%252C%2520minimizing%2520the%25202-Wasserstein%2520distance%2520between%2520quantized%2520and%2520original%2520weights%252C%2520and%2520systematically%2520compare%2520its%2520effectiveness%2520against%2520uniform%252C%2520piecewise%252C%2520and%2520logarithmic%2520quantization%2520schemes.%2520Our%2520theoretical%2520analysis%2520provides%2520upper%2520bounds%2520on%2520generative%2520degradation%2520under%2520quantization%252C%2520and%2520empirical%2520results%2520across%2520five%2520benchmark%2520datasets%2520of%2520varying%2520complexity%2520show%2520that%2520OT-based%2520quantization%2520preserves%2520both%2520visual%2520generation%2520quality%2520and%2520latent%2520space%2520stability%2520down%2520to%25202-3%2520bits%2520per%2520parameter%252C%2520where%2520alternative%2520methods%2520fail.%2520This%2520establishes%2520OT-based%2520quantization%2520as%2520a%2520principled%252C%2520effective%2520approach%2520to%2520compress%2520FM%2520generative%2520models%2520for%2520edge%2520and%2520embedded%2520AI%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Bit%2C%20High-Fidelity%3A%20Optimal%20Transport%20Quantization%20for%20Flow%20Matching&entry.906535625=Dara%20Varam%20and%20Diaa%20A.%20Abuhani%20and%20Imran%20Zualkernan%20and%20Raghad%20AlDamani%20and%20Lujain%20Khalil&entry.1292438233=Flow%20Matching%20%28FM%29%20generative%20models%20offer%20efficient%20simulation-free%20training%20and%20deterministic%20sampling%2C%20but%20their%20practical%20deployment%20is%20challenged%20by%20high-precision%20parameter%20requirements.%20We%20adapt%20optimal%20transport%20%28OT%29-based%20post-training%20quantization%20to%20FM%20models%2C%20minimizing%20the%202-Wasserstein%20distance%20between%20quantized%20and%20original%20weights%2C%20and%20systematically%20compare%20its%20effectiveness%20against%20uniform%2C%20piecewise%2C%20and%20logarithmic%20quantization%20schemes.%20Our%20theoretical%20analysis%20provides%20upper%20bounds%20on%20generative%20degradation%20under%20quantization%2C%20and%20empirical%20results%20across%20five%20benchmark%20datasets%20of%20varying%20complexity%20show%20that%20OT-based%20quantization%20preserves%20both%20visual%20generation%20quality%20and%20latent%20space%20stability%20down%20to%202-3%20bits%20per%20parameter%2C%20where%20alternative%20methods%20fail.%20This%20establishes%20OT-based%20quantization%20as%20a%20principled%2C%20effective%20approach%20to%20compress%20FM%20generative%20models%20for%20edge%20and%20embedded%20AI%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2511.11418v1&entry.124074799=Read"},
{"title": "A Learning-Based Framework for Collision-Free Motion Planning", "author": "Mateus Salom\u00e3o and Tiany\u00fc Ren and Alexander K\u00f6nig", "abstract": "This paper presents a learning-based extension to a Circular Field (CF)-based motion planner for efficient, collision-free trajectory generation in cluttered environments. The proposed approach overcomes the limitations of hand-tuned force field parameters by employing a deep neural network trained to infer optimal planner gains from a single depth image of the scene. The pipeline incorporates a CUDA-accelerated perception module, a predictive agent-based planning strategy, and a dataset generated through Bayesian optimization in simulation. The resulting framework enables real-time planning without manual parameter tuning and is validated both in simulation and on a Franka Emika Panda robot. Experimental results demonstrate successful task completion and improved generalization compared to classical planners.", "link": "http://arxiv.org/abs/2508.07502v2", "date": "2025-11-14", "relevancy": 2.1828, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5631}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5435}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Learning-Based%20Framework%20for%20Collision-Free%20Motion%20Planning&body=Title%3A%20A%20Learning-Based%20Framework%20for%20Collision-Free%20Motion%20Planning%0AAuthor%3A%20Mateus%20Salom%C3%A3o%20and%20Tiany%C3%BC%20Ren%20and%20Alexander%20K%C3%B6nig%0AAbstract%3A%20This%20paper%20presents%20a%20learning-based%20extension%20to%20a%20Circular%20Field%20%28CF%29-based%20motion%20planner%20for%20efficient%2C%20collision-free%20trajectory%20generation%20in%20cluttered%20environments.%20The%20proposed%20approach%20overcomes%20the%20limitations%20of%20hand-tuned%20force%20field%20parameters%20by%20employing%20a%20deep%20neural%20network%20trained%20to%20infer%20optimal%20planner%20gains%20from%20a%20single%20depth%20image%20of%20the%20scene.%20The%20pipeline%20incorporates%20a%20CUDA-accelerated%20perception%20module%2C%20a%20predictive%20agent-based%20planning%20strategy%2C%20and%20a%20dataset%20generated%20through%20Bayesian%20optimization%20in%20simulation.%20The%20resulting%20framework%20enables%20real-time%20planning%20without%20manual%20parameter%20tuning%20and%20is%20validated%20both%20in%20simulation%20and%20on%20a%20Franka%20Emika%20Panda%20robot.%20Experimental%20results%20demonstrate%20successful%20task%20completion%20and%20improved%20generalization%20compared%20to%20classical%20planners.%0ALink%3A%20http%3A//arxiv.org/abs/2508.07502v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Learning-Based%2520Framework%2520for%2520Collision-Free%2520Motion%2520Planning%26entry.906535625%3DMateus%2520Salom%25C3%25A3o%2520and%2520Tiany%25C3%25BC%2520Ren%2520and%2520Alexander%2520K%25C3%25B6nig%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520learning-based%2520extension%2520to%2520a%2520Circular%2520Field%2520%2528CF%2529-based%2520motion%2520planner%2520for%2520efficient%252C%2520collision-free%2520trajectory%2520generation%2520in%2520cluttered%2520environments.%2520The%2520proposed%2520approach%2520overcomes%2520the%2520limitations%2520of%2520hand-tuned%2520force%2520field%2520parameters%2520by%2520employing%2520a%2520deep%2520neural%2520network%2520trained%2520to%2520infer%2520optimal%2520planner%2520gains%2520from%2520a%2520single%2520depth%2520image%2520of%2520the%2520scene.%2520The%2520pipeline%2520incorporates%2520a%2520CUDA-accelerated%2520perception%2520module%252C%2520a%2520predictive%2520agent-based%2520planning%2520strategy%252C%2520and%2520a%2520dataset%2520generated%2520through%2520Bayesian%2520optimization%2520in%2520simulation.%2520The%2520resulting%2520framework%2520enables%2520real-time%2520planning%2520without%2520manual%2520parameter%2520tuning%2520and%2520is%2520validated%2520both%2520in%2520simulation%2520and%2520on%2520a%2520Franka%2520Emika%2520Panda%2520robot.%2520Experimental%2520results%2520demonstrate%2520successful%2520task%2520completion%2520and%2520improved%2520generalization%2520compared%2520to%2520classical%2520planners.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07502v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Learning-Based%20Framework%20for%20Collision-Free%20Motion%20Planning&entry.906535625=Mateus%20Salom%C3%A3o%20and%20Tiany%C3%BC%20Ren%20and%20Alexander%20K%C3%B6nig&entry.1292438233=This%20paper%20presents%20a%20learning-based%20extension%20to%20a%20Circular%20Field%20%28CF%29-based%20motion%20planner%20for%20efficient%2C%20collision-free%20trajectory%20generation%20in%20cluttered%20environments.%20The%20proposed%20approach%20overcomes%20the%20limitations%20of%20hand-tuned%20force%20field%20parameters%20by%20employing%20a%20deep%20neural%20network%20trained%20to%20infer%20optimal%20planner%20gains%20from%20a%20single%20depth%20image%20of%20the%20scene.%20The%20pipeline%20incorporates%20a%20CUDA-accelerated%20perception%20module%2C%20a%20predictive%20agent-based%20planning%20strategy%2C%20and%20a%20dataset%20generated%20through%20Bayesian%20optimization%20in%20simulation.%20The%20resulting%20framework%20enables%20real-time%20planning%20without%20manual%20parameter%20tuning%20and%20is%20validated%20both%20in%20simulation%20and%20on%20a%20Franka%20Emika%20Panda%20robot.%20Experimental%20results%20demonstrate%20successful%20task%20completion%20and%20improved%20generalization%20compared%20to%20classical%20planners.&entry.1838667208=http%3A//arxiv.org/abs/2508.07502v2&entry.124074799=Read"},
{"title": "Hierarchical Mixing Architecture for Low-light RAW Image Enhancement", "author": "Xianmin Chen and Peiliang Huang and Longfei Han and Dingwen Zhang and Junwei Han", "abstract": "With the rapid development of deep learning, low-light RAW image enhancement (LLRIE) has achieved remarkable progress. However, the challenge that how to simultaneously achieve strong enhancement quality and high efficiency still remains. Leveraging the inherent efficiency of Channel Attention and Mamba, we introduce a Hierarchical Mixing Architecture (HiMA), a hybrid LLRIE framework built upon two core modules. Specifically, we introduce Large Scale Block (LSB) for upper layers and Small Scale Block (SSB) for lower layers that reduce the parameters while improve the performance. Based on this framework, we also introduce a novel Local Distribution Adjustment (LoDA) module that adaptively aligns local feature statistics in a content-aware manner by learning to adjust regional luminance and contrast distributions. Moreover, to alleviate the domain ambiguity commonly observed in existing LLRIE pipelines, we design a Multi-Prior Fusion (MPF) module that leverages three complementary priors extracted from the first stage of the hybrid architecture to maintain domain consistency. Extensive experiments on multiple public benchmarks demonstrate that our approach outperforms state-of-the-art methods, delivering superior performance with fewer parameters. Code is available at https://github.com/Cynicarlos/HiMA.", "link": "http://arxiv.org/abs/2510.15497v2", "date": "2025-11-14", "relevancy": 2.1826, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.549}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5448}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Mixing%20Architecture%20for%20Low-light%20RAW%20Image%20Enhancement&body=Title%3A%20Hierarchical%20Mixing%20Architecture%20for%20Low-light%20RAW%20Image%20Enhancement%0AAuthor%3A%20Xianmin%20Chen%20and%20Peiliang%20Huang%20and%20Longfei%20Han%20and%20Dingwen%20Zhang%20and%20Junwei%20Han%0AAbstract%3A%20With%20the%20rapid%20development%20of%20deep%20learning%2C%20low-light%20RAW%20image%20enhancement%20%28LLRIE%29%20has%20achieved%20remarkable%20progress.%20However%2C%20the%20challenge%20that%20how%20to%20simultaneously%20achieve%20strong%20enhancement%20quality%20and%20high%20efficiency%20still%20remains.%20Leveraging%20the%20inherent%20efficiency%20of%20Channel%20Attention%20and%20Mamba%2C%20we%20introduce%20a%20Hierarchical%20Mixing%20Architecture%20%28HiMA%29%2C%20a%20hybrid%20LLRIE%20framework%20built%20upon%20two%20core%20modules.%20Specifically%2C%20we%20introduce%20Large%20Scale%20Block%20%28LSB%29%20for%20upper%20layers%20and%20Small%20Scale%20Block%20%28SSB%29%20for%20lower%20layers%20that%20reduce%20the%20parameters%20while%20improve%20the%20performance.%20Based%20on%20this%20framework%2C%20we%20also%20introduce%20a%20novel%20Local%20Distribution%20Adjustment%20%28LoDA%29%20module%20that%20adaptively%20aligns%20local%20feature%20statistics%20in%20a%20content-aware%20manner%20by%20learning%20to%20adjust%20regional%20luminance%20and%20contrast%20distributions.%20Moreover%2C%20to%20alleviate%20the%20domain%20ambiguity%20commonly%20observed%20in%20existing%20LLRIE%20pipelines%2C%20we%20design%20a%20Multi-Prior%20Fusion%20%28MPF%29%20module%20that%20leverages%20three%20complementary%20priors%20extracted%20from%20the%20first%20stage%20of%20the%20hybrid%20architecture%20to%20maintain%20domain%20consistency.%20Extensive%20experiments%20on%20multiple%20public%20benchmarks%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%20methods%2C%20delivering%20superior%20performance%20with%20fewer%20parameters.%20Code%20is%20available%20at%20https%3A//github.com/Cynicarlos/HiMA.%0ALink%3A%20http%3A//arxiv.org/abs/2510.15497v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Mixing%2520Architecture%2520for%2520Low-light%2520RAW%2520Image%2520Enhancement%26entry.906535625%3DXianmin%2520Chen%2520and%2520Peiliang%2520Huang%2520and%2520Longfei%2520Han%2520and%2520Dingwen%2520Zhang%2520and%2520Junwei%2520Han%26entry.1292438233%3DWith%2520the%2520rapid%2520development%2520of%2520deep%2520learning%252C%2520low-light%2520RAW%2520image%2520enhancement%2520%2528LLRIE%2529%2520has%2520achieved%2520remarkable%2520progress.%2520However%252C%2520the%2520challenge%2520that%2520how%2520to%2520simultaneously%2520achieve%2520strong%2520enhancement%2520quality%2520and%2520high%2520efficiency%2520still%2520remains.%2520Leveraging%2520the%2520inherent%2520efficiency%2520of%2520Channel%2520Attention%2520and%2520Mamba%252C%2520we%2520introduce%2520a%2520Hierarchical%2520Mixing%2520Architecture%2520%2528HiMA%2529%252C%2520a%2520hybrid%2520LLRIE%2520framework%2520built%2520upon%2520two%2520core%2520modules.%2520Specifically%252C%2520we%2520introduce%2520Large%2520Scale%2520Block%2520%2528LSB%2529%2520for%2520upper%2520layers%2520and%2520Small%2520Scale%2520Block%2520%2528SSB%2529%2520for%2520lower%2520layers%2520that%2520reduce%2520the%2520parameters%2520while%2520improve%2520the%2520performance.%2520Based%2520on%2520this%2520framework%252C%2520we%2520also%2520introduce%2520a%2520novel%2520Local%2520Distribution%2520Adjustment%2520%2528LoDA%2529%2520module%2520that%2520adaptively%2520aligns%2520local%2520feature%2520statistics%2520in%2520a%2520content-aware%2520manner%2520by%2520learning%2520to%2520adjust%2520regional%2520luminance%2520and%2520contrast%2520distributions.%2520Moreover%252C%2520to%2520alleviate%2520the%2520domain%2520ambiguity%2520commonly%2520observed%2520in%2520existing%2520LLRIE%2520pipelines%252C%2520we%2520design%2520a%2520Multi-Prior%2520Fusion%2520%2528MPF%2529%2520module%2520that%2520leverages%2520three%2520complementary%2520priors%2520extracted%2520from%2520the%2520first%2520stage%2520of%2520the%2520hybrid%2520architecture%2520to%2520maintain%2520domain%2520consistency.%2520Extensive%2520experiments%2520on%2520multiple%2520public%2520benchmarks%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520state-of-the-art%2520methods%252C%2520delivering%2520superior%2520performance%2520with%2520fewer%2520parameters.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Cynicarlos/HiMA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15497v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Mixing%20Architecture%20for%20Low-light%20RAW%20Image%20Enhancement&entry.906535625=Xianmin%20Chen%20and%20Peiliang%20Huang%20and%20Longfei%20Han%20and%20Dingwen%20Zhang%20and%20Junwei%20Han&entry.1292438233=With%20the%20rapid%20development%20of%20deep%20learning%2C%20low-light%20RAW%20image%20enhancement%20%28LLRIE%29%20has%20achieved%20remarkable%20progress.%20However%2C%20the%20challenge%20that%20how%20to%20simultaneously%20achieve%20strong%20enhancement%20quality%20and%20high%20efficiency%20still%20remains.%20Leveraging%20the%20inherent%20efficiency%20of%20Channel%20Attention%20and%20Mamba%2C%20we%20introduce%20a%20Hierarchical%20Mixing%20Architecture%20%28HiMA%29%2C%20a%20hybrid%20LLRIE%20framework%20built%20upon%20two%20core%20modules.%20Specifically%2C%20we%20introduce%20Large%20Scale%20Block%20%28LSB%29%20for%20upper%20layers%20and%20Small%20Scale%20Block%20%28SSB%29%20for%20lower%20layers%20that%20reduce%20the%20parameters%20while%20improve%20the%20performance.%20Based%20on%20this%20framework%2C%20we%20also%20introduce%20a%20novel%20Local%20Distribution%20Adjustment%20%28LoDA%29%20module%20that%20adaptively%20aligns%20local%20feature%20statistics%20in%20a%20content-aware%20manner%20by%20learning%20to%20adjust%20regional%20luminance%20and%20contrast%20distributions.%20Moreover%2C%20to%20alleviate%20the%20domain%20ambiguity%20commonly%20observed%20in%20existing%20LLRIE%20pipelines%2C%20we%20design%20a%20Multi-Prior%20Fusion%20%28MPF%29%20module%20that%20leverages%20three%20complementary%20priors%20extracted%20from%20the%20first%20stage%20of%20the%20hybrid%20architecture%20to%20maintain%20domain%20consistency.%20Extensive%20experiments%20on%20multiple%20public%20benchmarks%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%20methods%2C%20delivering%20superior%20performance%20with%20fewer%20parameters.%20Code%20is%20available%20at%20https%3A//github.com/Cynicarlos/HiMA.&entry.1838667208=http%3A//arxiv.org/abs/2510.15497v2&entry.124074799=Read"},
{"title": "Unleashing the Potential of Large Language Models for Text-to-Image Generation through Autoregressive Representation Alignment", "author": "Xing Xie and Jiawei Liu and Ziyue Lin and Huijie Fan and Zhi Han and Yandong Tang and Liangqiong Qu", "abstract": "We present Autoregressive Representation Alignment (ARRA), a new training framework that unlocks global-coherent text-to-image generation in autoregressive LLMs without architectural modifications. Different from prior works that require complex architectural redesigns, ARRA aligns LLM's hidden states with visual representations from external visual foundational models via a global visual alignment loss and a hybrid token, <HYBNEXT>. This token enforces dual constraints: local next-token prediction and global semantic distillation, enabling LLMs to implicitly learn spatial and contextual coherence while retaining their original autoregressive paradigm. Extensive experiments validate ARRA's plug-and-play versatility. When training T2I LLMs from scratch, ARRA reduces FID by 16.6% (ImageNet), 12.0% (LAION-COCO) for autoregressive LLMs like LlamaGen, without modifying original architecture and inference mechanism. For training from text-generation-only LLMs, ARRA reduces FID by 25.5% (MIMIC-CXR), 8.8% (DeepEyeNet) for advanced LLMs like Chameleon. For domain adaptation, ARRA aligns general-purpose LLMs with specialized models (e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on medical imaging (MIMIC-CXR). These results demonstrate that training objective redesign, rather than architectural modifications, can resolve cross-modal global coherence challenges. ARRA offers a complementary paradigm for advancing autoregressive models. The code is available at https://github.com/HKU-HealthAI/ARRA.", "link": "http://arxiv.org/abs/2503.07334v4", "date": "2025-11-14", "relevancy": 2.1812, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.568}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5592}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20Potential%20of%20Large%20Language%20Models%20for%20Text-to-Image%20Generation%20through%20Autoregressive%20Representation%20Alignment&body=Title%3A%20Unleashing%20the%20Potential%20of%20Large%20Language%20Models%20for%20Text-to-Image%20Generation%20through%20Autoregressive%20Representation%20Alignment%0AAuthor%3A%20Xing%20Xie%20and%20Jiawei%20Liu%20and%20Ziyue%20Lin%20and%20Huijie%20Fan%20and%20Zhi%20Han%20and%20Yandong%20Tang%20and%20Liangqiong%20Qu%0AAbstract%3A%20We%20present%20Autoregressive%20Representation%20Alignment%20%28ARRA%29%2C%20a%20new%20training%20framework%20that%20unlocks%20global-coherent%20text-to-image%20generation%20in%20autoregressive%20LLMs%20without%20architectural%20modifications.%20Different%20from%20prior%20works%20that%20require%20complex%20architectural%20redesigns%2C%20ARRA%20aligns%20LLM%27s%20hidden%20states%20with%20visual%20representations%20from%20external%20visual%20foundational%20models%20via%20a%20global%20visual%20alignment%20loss%20and%20a%20hybrid%20token%2C%20%3CHYBNEXT%3E.%20This%20token%20enforces%20dual%20constraints%3A%20local%20next-token%20prediction%20and%20global%20semantic%20distillation%2C%20enabling%20LLMs%20to%20implicitly%20learn%20spatial%20and%20contextual%20coherence%20while%20retaining%20their%20original%20autoregressive%20paradigm.%20Extensive%20experiments%20validate%20ARRA%27s%20plug-and-play%20versatility.%20When%20training%20T2I%20LLMs%20from%20scratch%2C%20ARRA%20reduces%20FID%20by%2016.6%25%20%28ImageNet%29%2C%2012.0%25%20%28LAION-COCO%29%20for%20autoregressive%20LLMs%20like%20LlamaGen%2C%20without%20modifying%20original%20architecture%20and%20inference%20mechanism.%20For%20training%20from%20text-generation-only%20LLMs%2C%20ARRA%20reduces%20FID%20by%2025.5%25%20%28MIMIC-CXR%29%2C%208.8%25%20%28DeepEyeNet%29%20for%20advanced%20LLMs%20like%20Chameleon.%20For%20domain%20adaptation%2C%20ARRA%20aligns%20general-purpose%20LLMs%20with%20specialized%20models%20%28e.g.%2C%20BioMedCLIP%29%2C%20achieving%20an%2018.6%25%20FID%20reduction%20over%20direct%20fine-tuning%20on%20medical%20imaging%20%28MIMIC-CXR%29.%20These%20results%20demonstrate%20that%20training%20objective%20redesign%2C%20rather%20than%20architectural%20modifications%2C%20can%20resolve%20cross-modal%20global%20coherence%20challenges.%20ARRA%20offers%20a%20complementary%20paradigm%20for%20advancing%20autoregressive%20models.%20The%20code%20is%20available%20at%20https%3A//github.com/HKU-HealthAI/ARRA.%0ALink%3A%20http%3A//arxiv.org/abs/2503.07334v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520the%2520Potential%2520of%2520Large%2520Language%2520Models%2520for%2520Text-to-Image%2520Generation%2520through%2520Autoregressive%2520Representation%2520Alignment%26entry.906535625%3DXing%2520Xie%2520and%2520Jiawei%2520Liu%2520and%2520Ziyue%2520Lin%2520and%2520Huijie%2520Fan%2520and%2520Zhi%2520Han%2520and%2520Yandong%2520Tang%2520and%2520Liangqiong%2520Qu%26entry.1292438233%3DWe%2520present%2520Autoregressive%2520Representation%2520Alignment%2520%2528ARRA%2529%252C%2520a%2520new%2520training%2520framework%2520that%2520unlocks%2520global-coherent%2520text-to-image%2520generation%2520in%2520autoregressive%2520LLMs%2520without%2520architectural%2520modifications.%2520Different%2520from%2520prior%2520works%2520that%2520require%2520complex%2520architectural%2520redesigns%252C%2520ARRA%2520aligns%2520LLM%2527s%2520hidden%2520states%2520with%2520visual%2520representations%2520from%2520external%2520visual%2520foundational%2520models%2520via%2520a%2520global%2520visual%2520alignment%2520loss%2520and%2520a%2520hybrid%2520token%252C%2520%253CHYBNEXT%253E.%2520This%2520token%2520enforces%2520dual%2520constraints%253A%2520local%2520next-token%2520prediction%2520and%2520global%2520semantic%2520distillation%252C%2520enabling%2520LLMs%2520to%2520implicitly%2520learn%2520spatial%2520and%2520contextual%2520coherence%2520while%2520retaining%2520their%2520original%2520autoregressive%2520paradigm.%2520Extensive%2520experiments%2520validate%2520ARRA%2527s%2520plug-and-play%2520versatility.%2520When%2520training%2520T2I%2520LLMs%2520from%2520scratch%252C%2520ARRA%2520reduces%2520FID%2520by%252016.6%2525%2520%2528ImageNet%2529%252C%252012.0%2525%2520%2528LAION-COCO%2529%2520for%2520autoregressive%2520LLMs%2520like%2520LlamaGen%252C%2520without%2520modifying%2520original%2520architecture%2520and%2520inference%2520mechanism.%2520For%2520training%2520from%2520text-generation-only%2520LLMs%252C%2520ARRA%2520reduces%2520FID%2520by%252025.5%2525%2520%2528MIMIC-CXR%2529%252C%25208.8%2525%2520%2528DeepEyeNet%2529%2520for%2520advanced%2520LLMs%2520like%2520Chameleon.%2520For%2520domain%2520adaptation%252C%2520ARRA%2520aligns%2520general-purpose%2520LLMs%2520with%2520specialized%2520models%2520%2528e.g.%252C%2520BioMedCLIP%2529%252C%2520achieving%2520an%252018.6%2525%2520FID%2520reduction%2520over%2520direct%2520fine-tuning%2520on%2520medical%2520imaging%2520%2528MIMIC-CXR%2529.%2520These%2520results%2520demonstrate%2520that%2520training%2520objective%2520redesign%252C%2520rather%2520than%2520architectural%2520modifications%252C%2520can%2520resolve%2520cross-modal%2520global%2520coherence%2520challenges.%2520ARRA%2520offers%2520a%2520complementary%2520paradigm%2520for%2520advancing%2520autoregressive%2520models.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/HKU-HealthAI/ARRA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07334v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20Potential%20of%20Large%20Language%20Models%20for%20Text-to-Image%20Generation%20through%20Autoregressive%20Representation%20Alignment&entry.906535625=Xing%20Xie%20and%20Jiawei%20Liu%20and%20Ziyue%20Lin%20and%20Huijie%20Fan%20and%20Zhi%20Han%20and%20Yandong%20Tang%20and%20Liangqiong%20Qu&entry.1292438233=We%20present%20Autoregressive%20Representation%20Alignment%20%28ARRA%29%2C%20a%20new%20training%20framework%20that%20unlocks%20global-coherent%20text-to-image%20generation%20in%20autoregressive%20LLMs%20without%20architectural%20modifications.%20Different%20from%20prior%20works%20that%20require%20complex%20architectural%20redesigns%2C%20ARRA%20aligns%20LLM%27s%20hidden%20states%20with%20visual%20representations%20from%20external%20visual%20foundational%20models%20via%20a%20global%20visual%20alignment%20loss%20and%20a%20hybrid%20token%2C%20%3CHYBNEXT%3E.%20This%20token%20enforces%20dual%20constraints%3A%20local%20next-token%20prediction%20and%20global%20semantic%20distillation%2C%20enabling%20LLMs%20to%20implicitly%20learn%20spatial%20and%20contextual%20coherence%20while%20retaining%20their%20original%20autoregressive%20paradigm.%20Extensive%20experiments%20validate%20ARRA%27s%20plug-and-play%20versatility.%20When%20training%20T2I%20LLMs%20from%20scratch%2C%20ARRA%20reduces%20FID%20by%2016.6%25%20%28ImageNet%29%2C%2012.0%25%20%28LAION-COCO%29%20for%20autoregressive%20LLMs%20like%20LlamaGen%2C%20without%20modifying%20original%20architecture%20and%20inference%20mechanism.%20For%20training%20from%20text-generation-only%20LLMs%2C%20ARRA%20reduces%20FID%20by%2025.5%25%20%28MIMIC-CXR%29%2C%208.8%25%20%28DeepEyeNet%29%20for%20advanced%20LLMs%20like%20Chameleon.%20For%20domain%20adaptation%2C%20ARRA%20aligns%20general-purpose%20LLMs%20with%20specialized%20models%20%28e.g.%2C%20BioMedCLIP%29%2C%20achieving%20an%2018.6%25%20FID%20reduction%20over%20direct%20fine-tuning%20on%20medical%20imaging%20%28MIMIC-CXR%29.%20These%20results%20demonstrate%20that%20training%20objective%20redesign%2C%20rather%20than%20architectural%20modifications%2C%20can%20resolve%20cross-modal%20global%20coherence%20challenges.%20ARRA%20offers%20a%20complementary%20paradigm%20for%20advancing%20autoregressive%20models.%20The%20code%20is%20available%20at%20https%3A//github.com/HKU-HealthAI/ARRA.&entry.1838667208=http%3A//arxiv.org/abs/2503.07334v4&entry.124074799=Read"},
{"title": "Scalable Coverage Trajectory Synthesis on GPUs as Statistical Inference", "author": "Max M. Sun and Jueun Kwon and Todd Murphey", "abstract": "Coverage motion planning is essential to a wide range of robotic tasks. Unlike conventional motion planning problems, which reason over temporal sequences of states, coverage motion planning requires reasoning over the spatial distribution of entire trajectories, making standard motion planning methods limited in computational efficiency and less amenable to modern parallelization frameworks. In this work, we formulate the coverage motion planning problem as a statistical inference problem from the perspective of flow matching, a generative modeling technique that has gained significant attention in recent years. The proposed formulation unifies commonly used statistical discrepancy measures, such as Kullback-Leibler divergence and Sinkhorn divergence, with a standard linear quadratic regulator problem. More importantly, it decouples the generation of trajectory gradients for coverage from the synthesis of control under nonlinear system dynamics, enabling significant acceleration through parallelization on modern computational architectures, particularly Graphics Processing Units (GPUs). This paper focuses on the advantages of this formulation in terms of scalability through parallelization, highlighting its computational benefits compared to conventional methods based on waypoint tracking.", "link": "http://arxiv.org/abs/2511.11514v1", "date": "2025-11-14", "relevancy": 2.1788, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5492}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5489}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Coverage%20Trajectory%20Synthesis%20on%20GPUs%20as%20Statistical%20Inference&body=Title%3A%20Scalable%20Coverage%20Trajectory%20Synthesis%20on%20GPUs%20as%20Statistical%20Inference%0AAuthor%3A%20Max%20M.%20Sun%20and%20Jueun%20Kwon%20and%20Todd%20Murphey%0AAbstract%3A%20Coverage%20motion%20planning%20is%20essential%20to%20a%20wide%20range%20of%20robotic%20tasks.%20Unlike%20conventional%20motion%20planning%20problems%2C%20which%20reason%20over%20temporal%20sequences%20of%20states%2C%20coverage%20motion%20planning%20requires%20reasoning%20over%20the%20spatial%20distribution%20of%20entire%20trajectories%2C%20making%20standard%20motion%20planning%20methods%20limited%20in%20computational%20efficiency%20and%20less%20amenable%20to%20modern%20parallelization%20frameworks.%20In%20this%20work%2C%20we%20formulate%20the%20coverage%20motion%20planning%20problem%20as%20a%20statistical%20inference%20problem%20from%20the%20perspective%20of%20flow%20matching%2C%20a%20generative%20modeling%20technique%20that%20has%20gained%20significant%20attention%20in%20recent%20years.%20The%20proposed%20formulation%20unifies%20commonly%20used%20statistical%20discrepancy%20measures%2C%20such%20as%20Kullback-Leibler%20divergence%20and%20Sinkhorn%20divergence%2C%20with%20a%20standard%20linear%20quadratic%20regulator%20problem.%20More%20importantly%2C%20it%20decouples%20the%20generation%20of%20trajectory%20gradients%20for%20coverage%20from%20the%20synthesis%20of%20control%20under%20nonlinear%20system%20dynamics%2C%20enabling%20significant%20acceleration%20through%20parallelization%20on%20modern%20computational%20architectures%2C%20particularly%20Graphics%20Processing%20Units%20%28GPUs%29.%20This%20paper%20focuses%20on%20the%20advantages%20of%20this%20formulation%20in%20terms%20of%20scalability%20through%20parallelization%2C%20highlighting%20its%20computational%20benefits%20compared%20to%20conventional%20methods%20based%20on%20waypoint%20tracking.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11514v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Coverage%2520Trajectory%2520Synthesis%2520on%2520GPUs%2520as%2520Statistical%2520Inference%26entry.906535625%3DMax%2520M.%2520Sun%2520and%2520Jueun%2520Kwon%2520and%2520Todd%2520Murphey%26entry.1292438233%3DCoverage%2520motion%2520planning%2520is%2520essential%2520to%2520a%2520wide%2520range%2520of%2520robotic%2520tasks.%2520Unlike%2520conventional%2520motion%2520planning%2520problems%252C%2520which%2520reason%2520over%2520temporal%2520sequences%2520of%2520states%252C%2520coverage%2520motion%2520planning%2520requires%2520reasoning%2520over%2520the%2520spatial%2520distribution%2520of%2520entire%2520trajectories%252C%2520making%2520standard%2520motion%2520planning%2520methods%2520limited%2520in%2520computational%2520efficiency%2520and%2520less%2520amenable%2520to%2520modern%2520parallelization%2520frameworks.%2520In%2520this%2520work%252C%2520we%2520formulate%2520the%2520coverage%2520motion%2520planning%2520problem%2520as%2520a%2520statistical%2520inference%2520problem%2520from%2520the%2520perspective%2520of%2520flow%2520matching%252C%2520a%2520generative%2520modeling%2520technique%2520that%2520has%2520gained%2520significant%2520attention%2520in%2520recent%2520years.%2520The%2520proposed%2520formulation%2520unifies%2520commonly%2520used%2520statistical%2520discrepancy%2520measures%252C%2520such%2520as%2520Kullback-Leibler%2520divergence%2520and%2520Sinkhorn%2520divergence%252C%2520with%2520a%2520standard%2520linear%2520quadratic%2520regulator%2520problem.%2520More%2520importantly%252C%2520it%2520decouples%2520the%2520generation%2520of%2520trajectory%2520gradients%2520for%2520coverage%2520from%2520the%2520synthesis%2520of%2520control%2520under%2520nonlinear%2520system%2520dynamics%252C%2520enabling%2520significant%2520acceleration%2520through%2520parallelization%2520on%2520modern%2520computational%2520architectures%252C%2520particularly%2520Graphics%2520Processing%2520Units%2520%2528GPUs%2529.%2520This%2520paper%2520focuses%2520on%2520the%2520advantages%2520of%2520this%2520formulation%2520in%2520terms%2520of%2520scalability%2520through%2520parallelization%252C%2520highlighting%2520its%2520computational%2520benefits%2520compared%2520to%2520conventional%2520methods%2520based%2520on%2520waypoint%2520tracking.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11514v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Coverage%20Trajectory%20Synthesis%20on%20GPUs%20as%20Statistical%20Inference&entry.906535625=Max%20M.%20Sun%20and%20Jueun%20Kwon%20and%20Todd%20Murphey&entry.1292438233=Coverage%20motion%20planning%20is%20essential%20to%20a%20wide%20range%20of%20robotic%20tasks.%20Unlike%20conventional%20motion%20planning%20problems%2C%20which%20reason%20over%20temporal%20sequences%20of%20states%2C%20coverage%20motion%20planning%20requires%20reasoning%20over%20the%20spatial%20distribution%20of%20entire%20trajectories%2C%20making%20standard%20motion%20planning%20methods%20limited%20in%20computational%20efficiency%20and%20less%20amenable%20to%20modern%20parallelization%20frameworks.%20In%20this%20work%2C%20we%20formulate%20the%20coverage%20motion%20planning%20problem%20as%20a%20statistical%20inference%20problem%20from%20the%20perspective%20of%20flow%20matching%2C%20a%20generative%20modeling%20technique%20that%20has%20gained%20significant%20attention%20in%20recent%20years.%20The%20proposed%20formulation%20unifies%20commonly%20used%20statistical%20discrepancy%20measures%2C%20such%20as%20Kullback-Leibler%20divergence%20and%20Sinkhorn%20divergence%2C%20with%20a%20standard%20linear%20quadratic%20regulator%20problem.%20More%20importantly%2C%20it%20decouples%20the%20generation%20of%20trajectory%20gradients%20for%20coverage%20from%20the%20synthesis%20of%20control%20under%20nonlinear%20system%20dynamics%2C%20enabling%20significant%20acceleration%20through%20parallelization%20on%20modern%20computational%20architectures%2C%20particularly%20Graphics%20Processing%20Units%20%28GPUs%29.%20This%20paper%20focuses%20on%20the%20advantages%20of%20this%20formulation%20in%20terms%20of%20scalability%20through%20parallelization%2C%20highlighting%20its%20computational%20benefits%20compared%20to%20conventional%20methods%20based%20on%20waypoint%20tracking.&entry.1838667208=http%3A//arxiv.org/abs/2511.11514v1&entry.124074799=Read"},
{"title": "NervePool: A Simplicial Pooling Layer", "author": "Sarah McGuire Scullen and Ernst R\u00f6ell and Elizabeth Munch and Bastian Rieck and Matthew Hirn", "abstract": "For deep learning problems on graph-structured data, pooling layers are important for down sampling, reducing computational cost, and to minimize overfitting. We define a pooling layer, nervePool, for data structured as simplicial complexes, which are generalizations of graphs that include higher-dimensional simplices beyond vertices and edges; this structure allows for greater flexibility in modeling higher-order relationships. The proposed simplicial coarsening scheme is built upon partitions of vertices, which allow us to generate hierarchical representations of simplicial complexes, collapsing information in a learned fashion. NervePool builds on the learned vertex cluster assignments and extends to coarsening of higher dimensional simplices in a deterministic fashion. While in practice the pooling operations are computed via a series of matrix operations, the topological motivation is a set-theoretic construction based on unions of stars of simplices and the nerve complex.", "link": "http://arxiv.org/abs/2305.06315v2", "date": "2025-11-14", "relevancy": 2.1743, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4393}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4347}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NervePool%3A%20A%20Simplicial%20Pooling%20Layer&body=Title%3A%20NervePool%3A%20A%20Simplicial%20Pooling%20Layer%0AAuthor%3A%20Sarah%20McGuire%20Scullen%20and%20Ernst%20R%C3%B6ell%20and%20Elizabeth%20Munch%20and%20Bastian%20Rieck%20and%20Matthew%20Hirn%0AAbstract%3A%20For%20deep%20learning%20problems%20on%20graph-structured%20data%2C%20pooling%20layers%20are%20important%20for%20down%20sampling%2C%20reducing%20computational%20cost%2C%20and%20to%20minimize%20overfitting.%20We%20define%20a%20pooling%20layer%2C%20nervePool%2C%20for%20data%20structured%20as%20simplicial%20complexes%2C%20which%20are%20generalizations%20of%20graphs%20that%20include%20higher-dimensional%20simplices%20beyond%20vertices%20and%20edges%3B%20this%20structure%20allows%20for%20greater%20flexibility%20in%20modeling%20higher-order%20relationships.%20The%20proposed%20simplicial%20coarsening%20scheme%20is%20built%20upon%20partitions%20of%20vertices%2C%20which%20allow%20us%20to%20generate%20hierarchical%20representations%20of%20simplicial%20complexes%2C%20collapsing%20information%20in%20a%20learned%20fashion.%20NervePool%20builds%20on%20the%20learned%20vertex%20cluster%20assignments%20and%20extends%20to%20coarsening%20of%20higher%20dimensional%20simplices%20in%20a%20deterministic%20fashion.%20While%20in%20practice%20the%20pooling%20operations%20are%20computed%20via%20a%20series%20of%20matrix%20operations%2C%20the%20topological%20motivation%20is%20a%20set-theoretic%20construction%20based%20on%20unions%20of%20stars%20of%20simplices%20and%20the%20nerve%20complex.%0ALink%3A%20http%3A//arxiv.org/abs/2305.06315v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNervePool%253A%2520A%2520Simplicial%2520Pooling%2520Layer%26entry.906535625%3DSarah%2520McGuire%2520Scullen%2520and%2520Ernst%2520R%25C3%25B6ell%2520and%2520Elizabeth%2520Munch%2520and%2520Bastian%2520Rieck%2520and%2520Matthew%2520Hirn%26entry.1292438233%3DFor%2520deep%2520learning%2520problems%2520on%2520graph-structured%2520data%252C%2520pooling%2520layers%2520are%2520important%2520for%2520down%2520sampling%252C%2520reducing%2520computational%2520cost%252C%2520and%2520to%2520minimize%2520overfitting.%2520We%2520define%2520a%2520pooling%2520layer%252C%2520nervePool%252C%2520for%2520data%2520structured%2520as%2520simplicial%2520complexes%252C%2520which%2520are%2520generalizations%2520of%2520graphs%2520that%2520include%2520higher-dimensional%2520simplices%2520beyond%2520vertices%2520and%2520edges%253B%2520this%2520structure%2520allows%2520for%2520greater%2520flexibility%2520in%2520modeling%2520higher-order%2520relationships.%2520The%2520proposed%2520simplicial%2520coarsening%2520scheme%2520is%2520built%2520upon%2520partitions%2520of%2520vertices%252C%2520which%2520allow%2520us%2520to%2520generate%2520hierarchical%2520representations%2520of%2520simplicial%2520complexes%252C%2520collapsing%2520information%2520in%2520a%2520learned%2520fashion.%2520NervePool%2520builds%2520on%2520the%2520learned%2520vertex%2520cluster%2520assignments%2520and%2520extends%2520to%2520coarsening%2520of%2520higher%2520dimensional%2520simplices%2520in%2520a%2520deterministic%2520fashion.%2520While%2520in%2520practice%2520the%2520pooling%2520operations%2520are%2520computed%2520via%2520a%2520series%2520of%2520matrix%2520operations%252C%2520the%2520topological%2520motivation%2520is%2520a%2520set-theoretic%2520construction%2520based%2520on%2520unions%2520of%2520stars%2520of%2520simplices%2520and%2520the%2520nerve%2520complex.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.06315v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NervePool%3A%20A%20Simplicial%20Pooling%20Layer&entry.906535625=Sarah%20McGuire%20Scullen%20and%20Ernst%20R%C3%B6ell%20and%20Elizabeth%20Munch%20and%20Bastian%20Rieck%20and%20Matthew%20Hirn&entry.1292438233=For%20deep%20learning%20problems%20on%20graph-structured%20data%2C%20pooling%20layers%20are%20important%20for%20down%20sampling%2C%20reducing%20computational%20cost%2C%20and%20to%20minimize%20overfitting.%20We%20define%20a%20pooling%20layer%2C%20nervePool%2C%20for%20data%20structured%20as%20simplicial%20complexes%2C%20which%20are%20generalizations%20of%20graphs%20that%20include%20higher-dimensional%20simplices%20beyond%20vertices%20and%20edges%3B%20this%20structure%20allows%20for%20greater%20flexibility%20in%20modeling%20higher-order%20relationships.%20The%20proposed%20simplicial%20coarsening%20scheme%20is%20built%20upon%20partitions%20of%20vertices%2C%20which%20allow%20us%20to%20generate%20hierarchical%20representations%20of%20simplicial%20complexes%2C%20collapsing%20information%20in%20a%20learned%20fashion.%20NervePool%20builds%20on%20the%20learned%20vertex%20cluster%20assignments%20and%20extends%20to%20coarsening%20of%20higher%20dimensional%20simplices%20in%20a%20deterministic%20fashion.%20While%20in%20practice%20the%20pooling%20operations%20are%20computed%20via%20a%20series%20of%20matrix%20operations%2C%20the%20topological%20motivation%20is%20a%20set-theoretic%20construction%20based%20on%20unions%20of%20stars%20of%20simplices%20and%20the%20nerve%20complex.&entry.1838667208=http%3A//arxiv.org/abs/2305.06315v2&entry.124074799=Read"},
{"title": "One-to-N Backdoor Attack in 3D Point Cloud via Spherical Trigger", "author": "Dongmei Shan and Wei Lian and Chongxia Wang", "abstract": "Backdoor attacks represent a critical threat to deep learning systems, particularly in safety-sensitive 3D domains such as autonomous driving and robotics. However, existing backdoor attacks for 3D point clouds have been limited to a rigid one-to-one paradigm. To address this, we present the first one-to-N backdoor framework for 3D vision, based on a novel, configurable spherical trigger. Our key insight is to leverage the spatial properties of spheres as a parameter space, allowing a single trigger design to encode multiple target classes. We establish a theoretical foundation for one-to-N backdoor attacks in 3D, demonstrating that poisoned models can map distinct trigger configurations to different target labels. Experimental results systematically validate this conclusion across multiple datasets and model architectures, achieving high attack success rates (up to 100\\%) while maintaining accuracy on clean data. This work establishes a crucial benchmark for multi-target threats in 3D vision and provides the foundational understanding needed to secure future 3D-driven intelligent systems.", "link": "http://arxiv.org/abs/2511.11210v1", "date": "2025-11-14", "relevancy": 2.1683, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5607}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5541}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One-to-N%20Backdoor%20Attack%20in%203D%20Point%20Cloud%20via%20Spherical%20Trigger&body=Title%3A%20One-to-N%20Backdoor%20Attack%20in%203D%20Point%20Cloud%20via%20Spherical%20Trigger%0AAuthor%3A%20Dongmei%20Shan%20and%20Wei%20Lian%20and%20Chongxia%20Wang%0AAbstract%3A%20Backdoor%20attacks%20represent%20a%20critical%20threat%20to%20deep%20learning%20systems%2C%20particularly%20in%20safety-sensitive%203D%20domains%20such%20as%20autonomous%20driving%20and%20robotics.%20However%2C%20existing%20backdoor%20attacks%20for%203D%20point%20clouds%20have%20been%20limited%20to%20a%20rigid%20one-to-one%20paradigm.%20To%20address%20this%2C%20we%20present%20the%20first%20one-to-N%20backdoor%20framework%20for%203D%20vision%2C%20based%20on%20a%20novel%2C%20configurable%20spherical%20trigger.%20Our%20key%20insight%20is%20to%20leverage%20the%20spatial%20properties%20of%20spheres%20as%20a%20parameter%20space%2C%20allowing%20a%20single%20trigger%20design%20to%20encode%20multiple%20target%20classes.%20We%20establish%20a%20theoretical%20foundation%20for%20one-to-N%20backdoor%20attacks%20in%203D%2C%20demonstrating%20that%20poisoned%20models%20can%20map%20distinct%20trigger%20configurations%20to%20different%20target%20labels.%20Experimental%20results%20systematically%20validate%20this%20conclusion%20across%20multiple%20datasets%20and%20model%20architectures%2C%20achieving%20high%20attack%20success%20rates%20%28up%20to%20100%5C%25%29%20while%20maintaining%20accuracy%20on%20clean%20data.%20This%20work%20establishes%20a%20crucial%20benchmark%20for%20multi-target%20threats%20in%203D%20vision%20and%20provides%20the%20foundational%20understanding%20needed%20to%20secure%20future%203D-driven%20intelligent%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne-to-N%2520Backdoor%2520Attack%2520in%25203D%2520Point%2520Cloud%2520via%2520Spherical%2520Trigger%26entry.906535625%3DDongmei%2520Shan%2520and%2520Wei%2520Lian%2520and%2520Chongxia%2520Wang%26entry.1292438233%3DBackdoor%2520attacks%2520represent%2520a%2520critical%2520threat%2520to%2520deep%2520learning%2520systems%252C%2520particularly%2520in%2520safety-sensitive%25203D%2520domains%2520such%2520as%2520autonomous%2520driving%2520and%2520robotics.%2520However%252C%2520existing%2520backdoor%2520attacks%2520for%25203D%2520point%2520clouds%2520have%2520been%2520limited%2520to%2520a%2520rigid%2520one-to-one%2520paradigm.%2520To%2520address%2520this%252C%2520we%2520present%2520the%2520first%2520one-to-N%2520backdoor%2520framework%2520for%25203D%2520vision%252C%2520based%2520on%2520a%2520novel%252C%2520configurable%2520spherical%2520trigger.%2520Our%2520key%2520insight%2520is%2520to%2520leverage%2520the%2520spatial%2520properties%2520of%2520spheres%2520as%2520a%2520parameter%2520space%252C%2520allowing%2520a%2520single%2520trigger%2520design%2520to%2520encode%2520multiple%2520target%2520classes.%2520We%2520establish%2520a%2520theoretical%2520foundation%2520for%2520one-to-N%2520backdoor%2520attacks%2520in%25203D%252C%2520demonstrating%2520that%2520poisoned%2520models%2520can%2520map%2520distinct%2520trigger%2520configurations%2520to%2520different%2520target%2520labels.%2520Experimental%2520results%2520systematically%2520validate%2520this%2520conclusion%2520across%2520multiple%2520datasets%2520and%2520model%2520architectures%252C%2520achieving%2520high%2520attack%2520success%2520rates%2520%2528up%2520to%2520100%255C%2525%2529%2520while%2520maintaining%2520accuracy%2520on%2520clean%2520data.%2520This%2520work%2520establishes%2520a%2520crucial%2520benchmark%2520for%2520multi-target%2520threats%2520in%25203D%2520vision%2520and%2520provides%2520the%2520foundational%2520understanding%2520needed%2520to%2520secure%2520future%25203D-driven%2520intelligent%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-to-N%20Backdoor%20Attack%20in%203D%20Point%20Cloud%20via%20Spherical%20Trigger&entry.906535625=Dongmei%20Shan%20and%20Wei%20Lian%20and%20Chongxia%20Wang&entry.1292438233=Backdoor%20attacks%20represent%20a%20critical%20threat%20to%20deep%20learning%20systems%2C%20particularly%20in%20safety-sensitive%203D%20domains%20such%20as%20autonomous%20driving%20and%20robotics.%20However%2C%20existing%20backdoor%20attacks%20for%203D%20point%20clouds%20have%20been%20limited%20to%20a%20rigid%20one-to-one%20paradigm.%20To%20address%20this%2C%20we%20present%20the%20first%20one-to-N%20backdoor%20framework%20for%203D%20vision%2C%20based%20on%20a%20novel%2C%20configurable%20spherical%20trigger.%20Our%20key%20insight%20is%20to%20leverage%20the%20spatial%20properties%20of%20spheres%20as%20a%20parameter%20space%2C%20allowing%20a%20single%20trigger%20design%20to%20encode%20multiple%20target%20classes.%20We%20establish%20a%20theoretical%20foundation%20for%20one-to-N%20backdoor%20attacks%20in%203D%2C%20demonstrating%20that%20poisoned%20models%20can%20map%20distinct%20trigger%20configurations%20to%20different%20target%20labels.%20Experimental%20results%20systematically%20validate%20this%20conclusion%20across%20multiple%20datasets%20and%20model%20architectures%2C%20achieving%20high%20attack%20success%20rates%20%28up%20to%20100%5C%25%29%20while%20maintaining%20accuracy%20on%20clean%20data.%20This%20work%20establishes%20a%20crucial%20benchmark%20for%20multi-target%20threats%20in%203D%20vision%20and%20provides%20the%20foundational%20understanding%20needed%20to%20secure%20future%203D-driven%20intelligent%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2511.11210v1&entry.124074799=Read"},
{"title": "Nonlinear Laplacians: Tunable principal component analysis under directional prior information", "author": "Yuxin Ma and Dmitriy Kunisky", "abstract": "We introduce a new family of algorithms for detecting and estimating a rank-one signal from a noisy observation under prior information about that signal's direction, focusing on examples where the signal is known to have entries biased to be positive. Given a matrix observation $\\mathbf{Y}$, our algorithms construct a nonlinear Laplacian, another matrix of the form $\\mathbf{Y}+\\mathrm{diag}(\u03c3(\\mathbf{Y1}))$ for a nonlinear $\u03c3:\\mathbb{R}\\to\\mathbb{R}$, and examine the top eigenvalue and eigenvector of this matrix. When $\\mathbf{Y}$ is the (suitably normalized) adjacency matrix of a graph, our approach gives a class of algorithms that search for unusually dense subgraphs by computing a spectrum of the graph \"deformed\" by the degree profile $\\mathbf{Y1}$. We study the performance of such algorithms compared to direct spectral algorithms (the case $\u03c3=0$) on models of sparse principal component analysis with biased signals, including the Gaussian planted submatrix problem. For such models, we rigorously characterize the strength of rank-one signal, as a function of $\u03c3$, required for an outlier eigenvalue to appear in the spectrum of a nonlinear Laplacian matrix. While identifying the $\u03c3$ that minimizes the required signal strength in closed form seems intractable, we explore three approaches to design $\u03c3$ numerically: exhaustively searching over simple classes of $\u03c3$, learning $\u03c3$ from datasets of problem instances, and tuning $\u03c3$ using black-box optimization of the critical signal strength. We find both theoretically and empirically that, if $\u03c3$ is chosen appropriately, then nonlinear Laplacian spectral algorithms substantially outperform direct spectral algorithms, while retaining the conceptual simplicity of spectral methods compared to broader classes of computations like approximate message passing or general first order methods.", "link": "http://arxiv.org/abs/2505.12528v2", "date": "2025-11-14", "relevancy": 2.1543, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4336}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4335}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonlinear%20Laplacians%3A%20Tunable%20principal%20component%20analysis%20under%20directional%20prior%20information&body=Title%3A%20Nonlinear%20Laplacians%3A%20Tunable%20principal%20component%20analysis%20under%20directional%20prior%20information%0AAuthor%3A%20Yuxin%20Ma%20and%20Dmitriy%20Kunisky%0AAbstract%3A%20We%20introduce%20a%20new%20family%20of%20algorithms%20for%20detecting%20and%20estimating%20a%20rank-one%20signal%20from%20a%20noisy%20observation%20under%20prior%20information%20about%20that%20signal%27s%20direction%2C%20focusing%20on%20examples%20where%20the%20signal%20is%20known%20to%20have%20entries%20biased%20to%20be%20positive.%20Given%20a%20matrix%20observation%20%24%5Cmathbf%7BY%7D%24%2C%20our%20algorithms%20construct%20a%20nonlinear%20Laplacian%2C%20another%20matrix%20of%20the%20form%20%24%5Cmathbf%7BY%7D%2B%5Cmathrm%7Bdiag%7D%28%CF%83%28%5Cmathbf%7BY1%7D%29%29%24%20for%20a%20nonlinear%20%24%CF%83%3A%5Cmathbb%7BR%7D%5Cto%5Cmathbb%7BR%7D%24%2C%20and%20examine%20the%20top%20eigenvalue%20and%20eigenvector%20of%20this%20matrix.%20When%20%24%5Cmathbf%7BY%7D%24%20is%20the%20%28suitably%20normalized%29%20adjacency%20matrix%20of%20a%20graph%2C%20our%20approach%20gives%20a%20class%20of%20algorithms%20that%20search%20for%20unusually%20dense%20subgraphs%20by%20computing%20a%20spectrum%20of%20the%20graph%20%22deformed%22%20by%20the%20degree%20profile%20%24%5Cmathbf%7BY1%7D%24.%20We%20study%20the%20performance%20of%20such%20algorithms%20compared%20to%20direct%20spectral%20algorithms%20%28the%20case%20%24%CF%83%3D0%24%29%20on%20models%20of%20sparse%20principal%20component%20analysis%20with%20biased%20signals%2C%20including%20the%20Gaussian%20planted%20submatrix%20problem.%20For%20such%20models%2C%20we%20rigorously%20characterize%20the%20strength%20of%20rank-one%20signal%2C%20as%20a%20function%20of%20%24%CF%83%24%2C%20required%20for%20an%20outlier%20eigenvalue%20to%20appear%20in%20the%20spectrum%20of%20a%20nonlinear%20Laplacian%20matrix.%20While%20identifying%20the%20%24%CF%83%24%20that%20minimizes%20the%20required%20signal%20strength%20in%20closed%20form%20seems%20intractable%2C%20we%20explore%20three%20approaches%20to%20design%20%24%CF%83%24%20numerically%3A%20exhaustively%20searching%20over%20simple%20classes%20of%20%24%CF%83%24%2C%20learning%20%24%CF%83%24%20from%20datasets%20of%20problem%20instances%2C%20and%20tuning%20%24%CF%83%24%20using%20black-box%20optimization%20of%20the%20critical%20signal%20strength.%20We%20find%20both%20theoretically%20and%20empirically%20that%2C%20if%20%24%CF%83%24%20is%20chosen%20appropriately%2C%20then%20nonlinear%20Laplacian%20spectral%20algorithms%20substantially%20outperform%20direct%20spectral%20algorithms%2C%20while%20retaining%20the%20conceptual%20simplicity%20of%20spectral%20methods%20compared%20to%20broader%20classes%20of%20computations%20like%20approximate%20message%20passing%20or%20general%20first%20order%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2505.12528v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonlinear%2520Laplacians%253A%2520Tunable%2520principal%2520component%2520analysis%2520under%2520directional%2520prior%2520information%26entry.906535625%3DYuxin%2520Ma%2520and%2520Dmitriy%2520Kunisky%26entry.1292438233%3DWe%2520introduce%2520a%2520new%2520family%2520of%2520algorithms%2520for%2520detecting%2520and%2520estimating%2520a%2520rank-one%2520signal%2520from%2520a%2520noisy%2520observation%2520under%2520prior%2520information%2520about%2520that%2520signal%2527s%2520direction%252C%2520focusing%2520on%2520examples%2520where%2520the%2520signal%2520is%2520known%2520to%2520have%2520entries%2520biased%2520to%2520be%2520positive.%2520Given%2520a%2520matrix%2520observation%2520%2524%255Cmathbf%257BY%257D%2524%252C%2520our%2520algorithms%2520construct%2520a%2520nonlinear%2520Laplacian%252C%2520another%2520matrix%2520of%2520the%2520form%2520%2524%255Cmathbf%257BY%257D%252B%255Cmathrm%257Bdiag%257D%2528%25CF%2583%2528%255Cmathbf%257BY1%257D%2529%2529%2524%2520for%2520a%2520nonlinear%2520%2524%25CF%2583%253A%255Cmathbb%257BR%257D%255Cto%255Cmathbb%257BR%257D%2524%252C%2520and%2520examine%2520the%2520top%2520eigenvalue%2520and%2520eigenvector%2520of%2520this%2520matrix.%2520When%2520%2524%255Cmathbf%257BY%257D%2524%2520is%2520the%2520%2528suitably%2520normalized%2529%2520adjacency%2520matrix%2520of%2520a%2520graph%252C%2520our%2520approach%2520gives%2520a%2520class%2520of%2520algorithms%2520that%2520search%2520for%2520unusually%2520dense%2520subgraphs%2520by%2520computing%2520a%2520spectrum%2520of%2520the%2520graph%2520%2522deformed%2522%2520by%2520the%2520degree%2520profile%2520%2524%255Cmathbf%257BY1%257D%2524.%2520We%2520study%2520the%2520performance%2520of%2520such%2520algorithms%2520compared%2520to%2520direct%2520spectral%2520algorithms%2520%2528the%2520case%2520%2524%25CF%2583%253D0%2524%2529%2520on%2520models%2520of%2520sparse%2520principal%2520component%2520analysis%2520with%2520biased%2520signals%252C%2520including%2520the%2520Gaussian%2520planted%2520submatrix%2520problem.%2520For%2520such%2520models%252C%2520we%2520rigorously%2520characterize%2520the%2520strength%2520of%2520rank-one%2520signal%252C%2520as%2520a%2520function%2520of%2520%2524%25CF%2583%2524%252C%2520required%2520for%2520an%2520outlier%2520eigenvalue%2520to%2520appear%2520in%2520the%2520spectrum%2520of%2520a%2520nonlinear%2520Laplacian%2520matrix.%2520While%2520identifying%2520the%2520%2524%25CF%2583%2524%2520that%2520minimizes%2520the%2520required%2520signal%2520strength%2520in%2520closed%2520form%2520seems%2520intractable%252C%2520we%2520explore%2520three%2520approaches%2520to%2520design%2520%2524%25CF%2583%2524%2520numerically%253A%2520exhaustively%2520searching%2520over%2520simple%2520classes%2520of%2520%2524%25CF%2583%2524%252C%2520learning%2520%2524%25CF%2583%2524%2520from%2520datasets%2520of%2520problem%2520instances%252C%2520and%2520tuning%2520%2524%25CF%2583%2524%2520using%2520black-box%2520optimization%2520of%2520the%2520critical%2520signal%2520strength.%2520We%2520find%2520both%2520theoretically%2520and%2520empirically%2520that%252C%2520if%2520%2524%25CF%2583%2524%2520is%2520chosen%2520appropriately%252C%2520then%2520nonlinear%2520Laplacian%2520spectral%2520algorithms%2520substantially%2520outperform%2520direct%2520spectral%2520algorithms%252C%2520while%2520retaining%2520the%2520conceptual%2520simplicity%2520of%2520spectral%2520methods%2520compared%2520to%2520broader%2520classes%2520of%2520computations%2520like%2520approximate%2520message%2520passing%2520or%2520general%2520first%2520order%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12528v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonlinear%20Laplacians%3A%20Tunable%20principal%20component%20analysis%20under%20directional%20prior%20information&entry.906535625=Yuxin%20Ma%20and%20Dmitriy%20Kunisky&entry.1292438233=We%20introduce%20a%20new%20family%20of%20algorithms%20for%20detecting%20and%20estimating%20a%20rank-one%20signal%20from%20a%20noisy%20observation%20under%20prior%20information%20about%20that%20signal%27s%20direction%2C%20focusing%20on%20examples%20where%20the%20signal%20is%20known%20to%20have%20entries%20biased%20to%20be%20positive.%20Given%20a%20matrix%20observation%20%24%5Cmathbf%7BY%7D%24%2C%20our%20algorithms%20construct%20a%20nonlinear%20Laplacian%2C%20another%20matrix%20of%20the%20form%20%24%5Cmathbf%7BY%7D%2B%5Cmathrm%7Bdiag%7D%28%CF%83%28%5Cmathbf%7BY1%7D%29%29%24%20for%20a%20nonlinear%20%24%CF%83%3A%5Cmathbb%7BR%7D%5Cto%5Cmathbb%7BR%7D%24%2C%20and%20examine%20the%20top%20eigenvalue%20and%20eigenvector%20of%20this%20matrix.%20When%20%24%5Cmathbf%7BY%7D%24%20is%20the%20%28suitably%20normalized%29%20adjacency%20matrix%20of%20a%20graph%2C%20our%20approach%20gives%20a%20class%20of%20algorithms%20that%20search%20for%20unusually%20dense%20subgraphs%20by%20computing%20a%20spectrum%20of%20the%20graph%20%22deformed%22%20by%20the%20degree%20profile%20%24%5Cmathbf%7BY1%7D%24.%20We%20study%20the%20performance%20of%20such%20algorithms%20compared%20to%20direct%20spectral%20algorithms%20%28the%20case%20%24%CF%83%3D0%24%29%20on%20models%20of%20sparse%20principal%20component%20analysis%20with%20biased%20signals%2C%20including%20the%20Gaussian%20planted%20submatrix%20problem.%20For%20such%20models%2C%20we%20rigorously%20characterize%20the%20strength%20of%20rank-one%20signal%2C%20as%20a%20function%20of%20%24%CF%83%24%2C%20required%20for%20an%20outlier%20eigenvalue%20to%20appear%20in%20the%20spectrum%20of%20a%20nonlinear%20Laplacian%20matrix.%20While%20identifying%20the%20%24%CF%83%24%20that%20minimizes%20the%20required%20signal%20strength%20in%20closed%20form%20seems%20intractable%2C%20we%20explore%20three%20approaches%20to%20design%20%24%CF%83%24%20numerically%3A%20exhaustively%20searching%20over%20simple%20classes%20of%20%24%CF%83%24%2C%20learning%20%24%CF%83%24%20from%20datasets%20of%20problem%20instances%2C%20and%20tuning%20%24%CF%83%24%20using%20black-box%20optimization%20of%20the%20critical%20signal%20strength.%20We%20find%20both%20theoretically%20and%20empirically%20that%2C%20if%20%24%CF%83%24%20is%20chosen%20appropriately%2C%20then%20nonlinear%20Laplacian%20spectral%20algorithms%20substantially%20outperform%20direct%20spectral%20algorithms%2C%20while%20retaining%20the%20conceptual%20simplicity%20of%20spectral%20methods%20compared%20to%20broader%20classes%20of%20computations%20like%20approximate%20message%20passing%20or%20general%20first%20order%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2505.12528v2&entry.124074799=Read"},
{"title": "Training Neural Networks at Any Scale", "author": "Thomas Pethick and Kimon Antonakopoulos and Antonio Silveti-Falls and Leena Chennuru Vankadara and Volkan Cevher", "abstract": "This article reviews modern optimization methods for training neural networks with an emphasis on efficiency and scale. We present state-of-the-art optimization algorithms under a unified algorithmic template that highlights the importance of adapting to the structures in the problem. We then cover how to make these algorithms agnostic to the scale of the problem. Our exposition is intended as an introduction for both practitioners and researchers who wish to be involved in these exciting new developments.", "link": "http://arxiv.org/abs/2511.11163v1", "date": "2025-11-14", "relevancy": 2.0333, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5509}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4793}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Neural%20Networks%20at%20Any%20Scale&body=Title%3A%20Training%20Neural%20Networks%20at%20Any%20Scale%0AAuthor%3A%20Thomas%20Pethick%20and%20Kimon%20Antonakopoulos%20and%20Antonio%20Silveti-Falls%20and%20Leena%20Chennuru%20Vankadara%20and%20Volkan%20Cevher%0AAbstract%3A%20This%20article%20reviews%20modern%20optimization%20methods%20for%20training%20neural%20networks%20with%20an%20emphasis%20on%20efficiency%20and%20scale.%20We%20present%20state-of-the-art%20optimization%20algorithms%20under%20a%20unified%20algorithmic%20template%20that%20highlights%20the%20importance%20of%20adapting%20to%20the%20structures%20in%20the%20problem.%20We%20then%20cover%20how%20to%20make%20these%20algorithms%20agnostic%20to%20the%20scale%20of%20the%20problem.%20Our%20exposition%20is%20intended%20as%20an%20introduction%20for%20both%20practitioners%20and%20researchers%20who%20wish%20to%20be%20involved%20in%20these%20exciting%20new%20developments.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Neural%2520Networks%2520at%2520Any%2520Scale%26entry.906535625%3DThomas%2520Pethick%2520and%2520Kimon%2520Antonakopoulos%2520and%2520Antonio%2520Silveti-Falls%2520and%2520Leena%2520Chennuru%2520Vankadara%2520and%2520Volkan%2520Cevher%26entry.1292438233%3DThis%2520article%2520reviews%2520modern%2520optimization%2520methods%2520for%2520training%2520neural%2520networks%2520with%2520an%2520emphasis%2520on%2520efficiency%2520and%2520scale.%2520We%2520present%2520state-of-the-art%2520optimization%2520algorithms%2520under%2520a%2520unified%2520algorithmic%2520template%2520that%2520highlights%2520the%2520importance%2520of%2520adapting%2520to%2520the%2520structures%2520in%2520the%2520problem.%2520We%2520then%2520cover%2520how%2520to%2520make%2520these%2520algorithms%2520agnostic%2520to%2520the%2520scale%2520of%2520the%2520problem.%2520Our%2520exposition%2520is%2520intended%2520as%2520an%2520introduction%2520for%2520both%2520practitioners%2520and%2520researchers%2520who%2520wish%2520to%2520be%2520involved%2520in%2520these%2520exciting%2520new%2520developments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Neural%20Networks%20at%20Any%20Scale&entry.906535625=Thomas%20Pethick%20and%20Kimon%20Antonakopoulos%20and%20Antonio%20Silveti-Falls%20and%20Leena%20Chennuru%20Vankadara%20and%20Volkan%20Cevher&entry.1292438233=This%20article%20reviews%20modern%20optimization%20methods%20for%20training%20neural%20networks%20with%20an%20emphasis%20on%20efficiency%20and%20scale.%20We%20present%20state-of-the-art%20optimization%20algorithms%20under%20a%20unified%20algorithmic%20template%20that%20highlights%20the%20importance%20of%20adapting%20to%20the%20structures%20in%20the%20problem.%20We%20then%20cover%20how%20to%20make%20these%20algorithms%20agnostic%20to%20the%20scale%20of%20the%20problem.%20Our%20exposition%20is%20intended%20as%20an%20introduction%20for%20both%20practitioners%20and%20researchers%20who%20wish%20to%20be%20involved%20in%20these%20exciting%20new%20developments.&entry.1838667208=http%3A//arxiv.org/abs/2511.11163v1&entry.124074799=Read"},
{"title": "Parameter-Efficient MoE LoRA for Few-Shot Multi-Style Editing", "author": "Cong Cao and Yujie Xu and Xiaodong Xu", "abstract": "In recent years, image editing has garnered growing attention. However, general image editing models often fail to produce satisfactory results when confronted with new styles. The challenge lies in how to effectively fine-tune general image editing models to new styles using only a limited amount of paired data. To address this issue, this paper proposes a novel few-shot style editing framework. For this task, we construct a benchmark dataset that encompasses five distinct styles. Correspondingly, we propose a parameter-efficient multi-style Mixture-of-Experts Low-Rank Adaptation (MoE LoRA) with style-specific and style-shared routing mechanisms for jointly fine-tuning multiple styles. The style-specific routing ensures that different styles do not interfere with one another, while the style-shared routing adaptively allocates shared MoE LoRAs to learn common patterns. Our MoE LoRA can automatically determine the optimal ranks for each layer through a novel metric-guided approach that estimates the importance score of each single-rank component. Additionally, we explore the optimal location to insert LoRA within the Diffusion in Transformer (DiT) model and integrate adversarial learning and flow matching to guide the diffusion training process. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art approaches with significantly fewer LoRA parameters.", "link": "http://arxiv.org/abs/2511.11236v1", "date": "2025-11-14", "relevancy": 1.6549, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5846}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5438}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter-Efficient%20MoE%20LoRA%20for%20Few-Shot%20Multi-Style%20Editing&body=Title%3A%20Parameter-Efficient%20MoE%20LoRA%20for%20Few-Shot%20Multi-Style%20Editing%0AAuthor%3A%20Cong%20Cao%20and%20Yujie%20Xu%20and%20Xiaodong%20Xu%0AAbstract%3A%20In%20recent%20years%2C%20image%20editing%20has%20garnered%20growing%20attention.%20However%2C%20general%20image%20editing%20models%20often%20fail%20to%20produce%20satisfactory%20results%20when%20confronted%20with%20new%20styles.%20The%20challenge%20lies%20in%20how%20to%20effectively%20fine-tune%20general%20image%20editing%20models%20to%20new%20styles%20using%20only%20a%20limited%20amount%20of%20paired%20data.%20To%20address%20this%20issue%2C%20this%20paper%20proposes%20a%20novel%20few-shot%20style%20editing%20framework.%20For%20this%20task%2C%20we%20construct%20a%20benchmark%20dataset%20that%20encompasses%20five%20distinct%20styles.%20Correspondingly%2C%20we%20propose%20a%20parameter-efficient%20multi-style%20Mixture-of-Experts%20Low-Rank%20Adaptation%20%28MoE%20LoRA%29%20with%20style-specific%20and%20style-shared%20routing%20mechanisms%20for%20jointly%20fine-tuning%20multiple%20styles.%20The%20style-specific%20routing%20ensures%20that%20different%20styles%20do%20not%20interfere%20with%20one%20another%2C%20while%20the%20style-shared%20routing%20adaptively%20allocates%20shared%20MoE%20LoRAs%20to%20learn%20common%20patterns.%20Our%20MoE%20LoRA%20can%20automatically%20determine%20the%20optimal%20ranks%20for%20each%20layer%20through%20a%20novel%20metric-guided%20approach%20that%20estimates%20the%20importance%20score%20of%20each%20single-rank%20component.%20Additionally%2C%20we%20explore%20the%20optimal%20location%20to%20insert%20LoRA%20within%20the%20Diffusion%20in%20Transformer%20%28DiT%29%20model%20and%20integrate%20adversarial%20learning%20and%20flow%20matching%20to%20guide%20the%20diffusion%20training%20process.%20Experimental%20results%20demonstrate%20that%20our%20proposed%20method%20outperforms%20existing%20state-of-the-art%20approaches%20with%20significantly%20fewer%20LoRA%20parameters.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter-Efficient%2520MoE%2520LoRA%2520for%2520Few-Shot%2520Multi-Style%2520Editing%26entry.906535625%3DCong%2520Cao%2520and%2520Yujie%2520Xu%2520and%2520Xiaodong%2520Xu%26entry.1292438233%3DIn%2520recent%2520years%252C%2520image%2520editing%2520has%2520garnered%2520growing%2520attention.%2520However%252C%2520general%2520image%2520editing%2520models%2520often%2520fail%2520to%2520produce%2520satisfactory%2520results%2520when%2520confronted%2520with%2520new%2520styles.%2520The%2520challenge%2520lies%2520in%2520how%2520to%2520effectively%2520fine-tune%2520general%2520image%2520editing%2520models%2520to%2520new%2520styles%2520using%2520only%2520a%2520limited%2520amount%2520of%2520paired%2520data.%2520To%2520address%2520this%2520issue%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520few-shot%2520style%2520editing%2520framework.%2520For%2520this%2520task%252C%2520we%2520construct%2520a%2520benchmark%2520dataset%2520that%2520encompasses%2520five%2520distinct%2520styles.%2520Correspondingly%252C%2520we%2520propose%2520a%2520parameter-efficient%2520multi-style%2520Mixture-of-Experts%2520Low-Rank%2520Adaptation%2520%2528MoE%2520LoRA%2529%2520with%2520style-specific%2520and%2520style-shared%2520routing%2520mechanisms%2520for%2520jointly%2520fine-tuning%2520multiple%2520styles.%2520The%2520style-specific%2520routing%2520ensures%2520that%2520different%2520styles%2520do%2520not%2520interfere%2520with%2520one%2520another%252C%2520while%2520the%2520style-shared%2520routing%2520adaptively%2520allocates%2520shared%2520MoE%2520LoRAs%2520to%2520learn%2520common%2520patterns.%2520Our%2520MoE%2520LoRA%2520can%2520automatically%2520determine%2520the%2520optimal%2520ranks%2520for%2520each%2520layer%2520through%2520a%2520novel%2520metric-guided%2520approach%2520that%2520estimates%2520the%2520importance%2520score%2520of%2520each%2520single-rank%2520component.%2520Additionally%252C%2520we%2520explore%2520the%2520optimal%2520location%2520to%2520insert%2520LoRA%2520within%2520the%2520Diffusion%2520in%2520Transformer%2520%2528DiT%2529%2520model%2520and%2520integrate%2520adversarial%2520learning%2520and%2520flow%2520matching%2520to%2520guide%2520the%2520diffusion%2520training%2520process.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520proposed%2520method%2520outperforms%2520existing%2520state-of-the-art%2520approaches%2520with%2520significantly%2520fewer%2520LoRA%2520parameters.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter-Efficient%20MoE%20LoRA%20for%20Few-Shot%20Multi-Style%20Editing&entry.906535625=Cong%20Cao%20and%20Yujie%20Xu%20and%20Xiaodong%20Xu&entry.1292438233=In%20recent%20years%2C%20image%20editing%20has%20garnered%20growing%20attention.%20However%2C%20general%20image%20editing%20models%20often%20fail%20to%20produce%20satisfactory%20results%20when%20confronted%20with%20new%20styles.%20The%20challenge%20lies%20in%20how%20to%20effectively%20fine-tune%20general%20image%20editing%20models%20to%20new%20styles%20using%20only%20a%20limited%20amount%20of%20paired%20data.%20To%20address%20this%20issue%2C%20this%20paper%20proposes%20a%20novel%20few-shot%20style%20editing%20framework.%20For%20this%20task%2C%20we%20construct%20a%20benchmark%20dataset%20that%20encompasses%20five%20distinct%20styles.%20Correspondingly%2C%20we%20propose%20a%20parameter-efficient%20multi-style%20Mixture-of-Experts%20Low-Rank%20Adaptation%20%28MoE%20LoRA%29%20with%20style-specific%20and%20style-shared%20routing%20mechanisms%20for%20jointly%20fine-tuning%20multiple%20styles.%20The%20style-specific%20routing%20ensures%20that%20different%20styles%20do%20not%20interfere%20with%20one%20another%2C%20while%20the%20style-shared%20routing%20adaptively%20allocates%20shared%20MoE%20LoRAs%20to%20learn%20common%20patterns.%20Our%20MoE%20LoRA%20can%20automatically%20determine%20the%20optimal%20ranks%20for%20each%20layer%20through%20a%20novel%20metric-guided%20approach%20that%20estimates%20the%20importance%20score%20of%20each%20single-rank%20component.%20Additionally%2C%20we%20explore%20the%20optimal%20location%20to%20insert%20LoRA%20within%20the%20Diffusion%20in%20Transformer%20%28DiT%29%20model%20and%20integrate%20adversarial%20learning%20and%20flow%20matching%20to%20guide%20the%20diffusion%20training%20process.%20Experimental%20results%20demonstrate%20that%20our%20proposed%20method%20outperforms%20existing%20state-of-the-art%20approaches%20with%20significantly%20fewer%20LoRA%20parameters.&entry.1838667208=http%3A//arxiv.org/abs/2511.11236v1&entry.124074799=Read"},
{"title": "DiAReL: Reinforcement Learning with Disturbance Awareness for Robust Sim2Real Policy Transfer in Robot Control", "author": "Mohammadhossein Malmir and Josip Josifovski and Noah Klarmann and Alois Knoll", "abstract": "Delayed Markov decision processes (DMDPs) fulfill the Markov property by augmenting the state space of agents with a finite time window of recently committed actions. In reliance on these state augmentations, delay-resolved reinforcement learning algorithms train policies to learn optimal interactions with environments featuring observation or action delays. Although such methods can be directly trained on the real robots, due to sample inefficiency, limited resources, or safety constraints, a common approach is to transfer models trained in simulation to the physical robot. However, robotic simulations rely on approximated models of the physical systems, which hinders the sim2real transfer. In this work, we consider various uncertainties in modeling the robot or environment dynamics as unknown intrinsic disturbances applied to the system input. We introduce the disturbance-augmented Markov decision process (DAMDP) in delayed settings as a novel representation to incorporate disturbance estimation in training on-policy reinforcement learning algorithms. The proposed method is validated across several metrics on learning robotic reaching and pushing tasks and compared with disturbance-unaware baselines. The results show that the disturbance-augmented models can achieve higher stabilization and robustness in the control response, which in turn improves the prospects of successful sim2real transfer.", "link": "http://arxiv.org/abs/2306.09010v2", "date": "2025-11-14", "relevancy": 1.5899, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5586}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5293}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiAReL%3A%20Reinforcement%20Learning%20with%20Disturbance%20Awareness%20for%20Robust%20Sim2Real%20Policy%20Transfer%20in%20Robot%20Control&body=Title%3A%20DiAReL%3A%20Reinforcement%20Learning%20with%20Disturbance%20Awareness%20for%20Robust%20Sim2Real%20Policy%20Transfer%20in%20Robot%20Control%0AAuthor%3A%20Mohammadhossein%20Malmir%20and%20Josip%20Josifovski%20and%20Noah%20Klarmann%20and%20Alois%20Knoll%0AAbstract%3A%20Delayed%20Markov%20decision%20processes%20%28DMDPs%29%20fulfill%20the%20Markov%20property%20by%20augmenting%20the%20state%20space%20of%20agents%20with%20a%20finite%20time%20window%20of%20recently%20committed%20actions.%20In%20reliance%20on%20these%20state%20augmentations%2C%20delay-resolved%20reinforcement%20learning%20algorithms%20train%20policies%20to%20learn%20optimal%20interactions%20with%20environments%20featuring%20observation%20or%20action%20delays.%20Although%20such%20methods%20can%20be%20directly%20trained%20on%20the%20real%20robots%2C%20due%20to%20sample%20inefficiency%2C%20limited%20resources%2C%20or%20safety%20constraints%2C%20a%20common%20approach%20is%20to%20transfer%20models%20trained%20in%20simulation%20to%20the%20physical%20robot.%20However%2C%20robotic%20simulations%20rely%20on%20approximated%20models%20of%20the%20physical%20systems%2C%20which%20hinders%20the%20sim2real%20transfer.%20In%20this%20work%2C%20we%20consider%20various%20uncertainties%20in%20modeling%20the%20robot%20or%20environment%20dynamics%20as%20unknown%20intrinsic%20disturbances%20applied%20to%20the%20system%20input.%20We%20introduce%20the%20disturbance-augmented%20Markov%20decision%20process%20%28DAMDP%29%20in%20delayed%20settings%20as%20a%20novel%20representation%20to%20incorporate%20disturbance%20estimation%20in%20training%20on-policy%20reinforcement%20learning%20algorithms.%20The%20proposed%20method%20is%20validated%20across%20several%20metrics%20on%20learning%20robotic%20reaching%20and%20pushing%20tasks%20and%20compared%20with%20disturbance-unaware%20baselines.%20The%20results%20show%20that%20the%20disturbance-augmented%20models%20can%20achieve%20higher%20stabilization%20and%20robustness%20in%20the%20control%20response%2C%20which%20in%20turn%20improves%20the%20prospects%20of%20successful%20sim2real%20transfer.%0ALink%3A%20http%3A//arxiv.org/abs/2306.09010v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiAReL%253A%2520Reinforcement%2520Learning%2520with%2520Disturbance%2520Awareness%2520for%2520Robust%2520Sim2Real%2520Policy%2520Transfer%2520in%2520Robot%2520Control%26entry.906535625%3DMohammadhossein%2520Malmir%2520and%2520Josip%2520Josifovski%2520and%2520Noah%2520Klarmann%2520and%2520Alois%2520Knoll%26entry.1292438233%3DDelayed%2520Markov%2520decision%2520processes%2520%2528DMDPs%2529%2520fulfill%2520the%2520Markov%2520property%2520by%2520augmenting%2520the%2520state%2520space%2520of%2520agents%2520with%2520a%2520finite%2520time%2520window%2520of%2520recently%2520committed%2520actions.%2520In%2520reliance%2520on%2520these%2520state%2520augmentations%252C%2520delay-resolved%2520reinforcement%2520learning%2520algorithms%2520train%2520policies%2520to%2520learn%2520optimal%2520interactions%2520with%2520environments%2520featuring%2520observation%2520or%2520action%2520delays.%2520Although%2520such%2520methods%2520can%2520be%2520directly%2520trained%2520on%2520the%2520real%2520robots%252C%2520due%2520to%2520sample%2520inefficiency%252C%2520limited%2520resources%252C%2520or%2520safety%2520constraints%252C%2520a%2520common%2520approach%2520is%2520to%2520transfer%2520models%2520trained%2520in%2520simulation%2520to%2520the%2520physical%2520robot.%2520However%252C%2520robotic%2520simulations%2520rely%2520on%2520approximated%2520models%2520of%2520the%2520physical%2520systems%252C%2520which%2520hinders%2520the%2520sim2real%2520transfer.%2520In%2520this%2520work%252C%2520we%2520consider%2520various%2520uncertainties%2520in%2520modeling%2520the%2520robot%2520or%2520environment%2520dynamics%2520as%2520unknown%2520intrinsic%2520disturbances%2520applied%2520to%2520the%2520system%2520input.%2520We%2520introduce%2520the%2520disturbance-augmented%2520Markov%2520decision%2520process%2520%2528DAMDP%2529%2520in%2520delayed%2520settings%2520as%2520a%2520novel%2520representation%2520to%2520incorporate%2520disturbance%2520estimation%2520in%2520training%2520on-policy%2520reinforcement%2520learning%2520algorithms.%2520The%2520proposed%2520method%2520is%2520validated%2520across%2520several%2520metrics%2520on%2520learning%2520robotic%2520reaching%2520and%2520pushing%2520tasks%2520and%2520compared%2520with%2520disturbance-unaware%2520baselines.%2520The%2520results%2520show%2520that%2520the%2520disturbance-augmented%2520models%2520can%2520achieve%2520higher%2520stabilization%2520and%2520robustness%2520in%2520the%2520control%2520response%252C%2520which%2520in%2520turn%2520improves%2520the%2520prospects%2520of%2520successful%2520sim2real%2520transfer.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.09010v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiAReL%3A%20Reinforcement%20Learning%20with%20Disturbance%20Awareness%20for%20Robust%20Sim2Real%20Policy%20Transfer%20in%20Robot%20Control&entry.906535625=Mohammadhossein%20Malmir%20and%20Josip%20Josifovski%20and%20Noah%20Klarmann%20and%20Alois%20Knoll&entry.1292438233=Delayed%20Markov%20decision%20processes%20%28DMDPs%29%20fulfill%20the%20Markov%20property%20by%20augmenting%20the%20state%20space%20of%20agents%20with%20a%20finite%20time%20window%20of%20recently%20committed%20actions.%20In%20reliance%20on%20these%20state%20augmentations%2C%20delay-resolved%20reinforcement%20learning%20algorithms%20train%20policies%20to%20learn%20optimal%20interactions%20with%20environments%20featuring%20observation%20or%20action%20delays.%20Although%20such%20methods%20can%20be%20directly%20trained%20on%20the%20real%20robots%2C%20due%20to%20sample%20inefficiency%2C%20limited%20resources%2C%20or%20safety%20constraints%2C%20a%20common%20approach%20is%20to%20transfer%20models%20trained%20in%20simulation%20to%20the%20physical%20robot.%20However%2C%20robotic%20simulations%20rely%20on%20approximated%20models%20of%20the%20physical%20systems%2C%20which%20hinders%20the%20sim2real%20transfer.%20In%20this%20work%2C%20we%20consider%20various%20uncertainties%20in%20modeling%20the%20robot%20or%20environment%20dynamics%20as%20unknown%20intrinsic%20disturbances%20applied%20to%20the%20system%20input.%20We%20introduce%20the%20disturbance-augmented%20Markov%20decision%20process%20%28DAMDP%29%20in%20delayed%20settings%20as%20a%20novel%20representation%20to%20incorporate%20disturbance%20estimation%20in%20training%20on-policy%20reinforcement%20learning%20algorithms.%20The%20proposed%20method%20is%20validated%20across%20several%20metrics%20on%20learning%20robotic%20reaching%20and%20pushing%20tasks%20and%20compared%20with%20disturbance-unaware%20baselines.%20The%20results%20show%20that%20the%20disturbance-augmented%20models%20can%20achieve%20higher%20stabilization%20and%20robustness%20in%20the%20control%20response%2C%20which%20in%20turn%20improves%20the%20prospects%20of%20successful%20sim2real%20transfer.&entry.1838667208=http%3A//arxiv.org/abs/2306.09010v2&entry.124074799=Read"},
{"title": "M-DAIGT: A Shared Task on Multi-Domain Detection of AI-Generated Text", "author": "Salima Lamsiyah and Saad Ezzini and Abdelkader El Mahdaouy and Hamza Alami and Abdessamad Benlahbib and Samir El Amrany and Salmane Chafik and Hicham Hammouchi", "abstract": "The generation of highly fluent text by Large Language Models (LLMs) poses a significant challenge to information integrity and academic research. In this paper, we introduce the Multi-Domain Detection of AI-Generated Text (M-DAIGT) shared task, which focuses on detecting AI-generated text across multiple domains, particularly in news articles and academic writing. M-DAIGT comprises two binary classification subtasks: News Article Detection (NAD) (Subtask 1) and Academic Writing Detection (AWD) (Subtask 2). To support this task, we developed and released a new large-scale benchmark dataset of 30,000 samples, balanced between human-written and AI-generated texts. The AI-generated content was produced using a variety of modern LLMs (e.g., GPT-4, Claude) and diverse prompting strategies. A total of 46 unique teams registered for the shared task, of which four teams submitted final results. All four teams participated in both Subtask 1 and Subtask 2. We describe the methods employed by these participating teams and briefly discuss future directions for M-DAIGT.", "link": "http://arxiv.org/abs/2511.11340v1", "date": "2025-11-14", "relevancy": 2.0794, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5454}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5218}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M-DAIGT%3A%20A%20Shared%20Task%20on%20Multi-Domain%20Detection%20of%20AI-Generated%20Text&body=Title%3A%20M-DAIGT%3A%20A%20Shared%20Task%20on%20Multi-Domain%20Detection%20of%20AI-Generated%20Text%0AAuthor%3A%20Salima%20Lamsiyah%20and%20Saad%20Ezzini%20and%20Abdelkader%20El%20Mahdaouy%20and%20Hamza%20Alami%20and%20Abdessamad%20Benlahbib%20and%20Samir%20El%20Amrany%20and%20Salmane%20Chafik%20and%20Hicham%20Hammouchi%0AAbstract%3A%20The%20generation%20of%20highly%20fluent%20text%20by%20Large%20Language%20Models%20%28LLMs%29%20poses%20a%20significant%20challenge%20to%20information%20integrity%20and%20academic%20research.%20In%20this%20paper%2C%20we%20introduce%20the%20Multi-Domain%20Detection%20of%20AI-Generated%20Text%20%28M-DAIGT%29%20shared%20task%2C%20which%20focuses%20on%20detecting%20AI-generated%20text%20across%20multiple%20domains%2C%20particularly%20in%20news%20articles%20and%20academic%20writing.%20M-DAIGT%20comprises%20two%20binary%20classification%20subtasks%3A%20News%20Article%20Detection%20%28NAD%29%20%28Subtask%201%29%20and%20Academic%20Writing%20Detection%20%28AWD%29%20%28Subtask%202%29.%20To%20support%20this%20task%2C%20we%20developed%20and%20released%20a%20new%20large-scale%20benchmark%20dataset%20of%2030%2C000%20samples%2C%20balanced%20between%20human-written%20and%20AI-generated%20texts.%20The%20AI-generated%20content%20was%20produced%20using%20a%20variety%20of%20modern%20LLMs%20%28e.g.%2C%20GPT-4%2C%20Claude%29%20and%20diverse%20prompting%20strategies.%20A%20total%20of%2046%20unique%20teams%20registered%20for%20the%20shared%20task%2C%20of%20which%20four%20teams%20submitted%20final%20results.%20All%20four%20teams%20participated%20in%20both%20Subtask%201%20and%20Subtask%202.%20We%20describe%20the%20methods%20employed%20by%20these%20participating%20teams%20and%20briefly%20discuss%20future%20directions%20for%20M-DAIGT.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11340v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM-DAIGT%253A%2520A%2520Shared%2520Task%2520on%2520Multi-Domain%2520Detection%2520of%2520AI-Generated%2520Text%26entry.906535625%3DSalima%2520Lamsiyah%2520and%2520Saad%2520Ezzini%2520and%2520Abdelkader%2520El%2520Mahdaouy%2520and%2520Hamza%2520Alami%2520and%2520Abdessamad%2520Benlahbib%2520and%2520Samir%2520El%2520Amrany%2520and%2520Salmane%2520Chafik%2520and%2520Hicham%2520Hammouchi%26entry.1292438233%3DThe%2520generation%2520of%2520highly%2520fluent%2520text%2520by%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520poses%2520a%2520significant%2520challenge%2520to%2520information%2520integrity%2520and%2520academic%2520research.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Multi-Domain%2520Detection%2520of%2520AI-Generated%2520Text%2520%2528M-DAIGT%2529%2520shared%2520task%252C%2520which%2520focuses%2520on%2520detecting%2520AI-generated%2520text%2520across%2520multiple%2520domains%252C%2520particularly%2520in%2520news%2520articles%2520and%2520academic%2520writing.%2520M-DAIGT%2520comprises%2520two%2520binary%2520classification%2520subtasks%253A%2520News%2520Article%2520Detection%2520%2528NAD%2529%2520%2528Subtask%25201%2529%2520and%2520Academic%2520Writing%2520Detection%2520%2528AWD%2529%2520%2528Subtask%25202%2529.%2520To%2520support%2520this%2520task%252C%2520we%2520developed%2520and%2520released%2520a%2520new%2520large-scale%2520benchmark%2520dataset%2520of%252030%252C000%2520samples%252C%2520balanced%2520between%2520human-written%2520and%2520AI-generated%2520texts.%2520The%2520AI-generated%2520content%2520was%2520produced%2520using%2520a%2520variety%2520of%2520modern%2520LLMs%2520%2528e.g.%252C%2520GPT-4%252C%2520Claude%2529%2520and%2520diverse%2520prompting%2520strategies.%2520A%2520total%2520of%252046%2520unique%2520teams%2520registered%2520for%2520the%2520shared%2520task%252C%2520of%2520which%2520four%2520teams%2520submitted%2520final%2520results.%2520All%2520four%2520teams%2520participated%2520in%2520both%2520Subtask%25201%2520and%2520Subtask%25202.%2520We%2520describe%2520the%2520methods%2520employed%2520by%2520these%2520participating%2520teams%2520and%2520briefly%2520discuss%2520future%2520directions%2520for%2520M-DAIGT.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11340v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M-DAIGT%3A%20A%20Shared%20Task%20on%20Multi-Domain%20Detection%20of%20AI-Generated%20Text&entry.906535625=Salima%20Lamsiyah%20and%20Saad%20Ezzini%20and%20Abdelkader%20El%20Mahdaouy%20and%20Hamza%20Alami%20and%20Abdessamad%20Benlahbib%20and%20Samir%20El%20Amrany%20and%20Salmane%20Chafik%20and%20Hicham%20Hammouchi&entry.1292438233=The%20generation%20of%20highly%20fluent%20text%20by%20Large%20Language%20Models%20%28LLMs%29%20poses%20a%20significant%20challenge%20to%20information%20integrity%20and%20academic%20research.%20In%20this%20paper%2C%20we%20introduce%20the%20Multi-Domain%20Detection%20of%20AI-Generated%20Text%20%28M-DAIGT%29%20shared%20task%2C%20which%20focuses%20on%20detecting%20AI-generated%20text%20across%20multiple%20domains%2C%20particularly%20in%20news%20articles%20and%20academic%20writing.%20M-DAIGT%20comprises%20two%20binary%20classification%20subtasks%3A%20News%20Article%20Detection%20%28NAD%29%20%28Subtask%201%29%20and%20Academic%20Writing%20Detection%20%28AWD%29%20%28Subtask%202%29.%20To%20support%20this%20task%2C%20we%20developed%20and%20released%20a%20new%20large-scale%20benchmark%20dataset%20of%2030%2C000%20samples%2C%20balanced%20between%20human-written%20and%20AI-generated%20texts.%20The%20AI-generated%20content%20was%20produced%20using%20a%20variety%20of%20modern%20LLMs%20%28e.g.%2C%20GPT-4%2C%20Claude%29%20and%20diverse%20prompting%20strategies.%20A%20total%20of%2046%20unique%20teams%20registered%20for%20the%20shared%20task%2C%20of%20which%20four%20teams%20submitted%20final%20results.%20All%20four%20teams%20participated%20in%20both%20Subtask%201%20and%20Subtask%202.%20We%20describe%20the%20methods%20employed%20by%20these%20participating%20teams%20and%20briefly%20discuss%20future%20directions%20for%20M-DAIGT.&entry.1838667208=http%3A//arxiv.org/abs/2511.11340v1&entry.124074799=Read"},
{"title": "Active Learning and Explainable AI for Multi-Objective Optimization of Spin Coated Polymers", "author": "Brendan Young and Brendan Alvey and Andreas Werbrouck and Will Murphy and James Keller and Matthias J. Young and Matthew Maschmann", "abstract": "Spin coating polymer thin films to achieve specific mechanical properties is inherently a multi-objective optimization problem. We present a framework that integrates an active Pareto front learning algorithm (PyePAL) with visualization and explainable AI techniques to optimize processing parameters. PyePAL uses Gaussian process models to predict objective values (hardness and elasticity) from the design variables (spin speed, dilution, and polymer mixture), guiding the adaptive selection of samples toward promising regions of the design space. To enable interpretable insights into the high-dimensional design space, we utilize UMAP (Uniform Manifold Approximation and Projection) for two-dimensional visualization of the Pareto front exploration. Additionally, we incorporate fuzzy linguistic summaries, which translate the learned relationships between process parameters and performance objectives into linguistic statements, thus enhancing the explainability and understanding of the optimization results. Experimental results demonstrate that our method efficiently identifies promising polymer designs, while the visual and linguistic explanations facilitate expert-driven analysis and knowledge discovery.", "link": "http://arxiv.org/abs/2509.08988v2", "date": "2025-11-14", "relevancy": 1.4064, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5163}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.467}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Learning%20and%20Explainable%20AI%20for%20Multi-Objective%20Optimization%20of%20Spin%20Coated%20Polymers&body=Title%3A%20Active%20Learning%20and%20Explainable%20AI%20for%20Multi-Objective%20Optimization%20of%20Spin%20Coated%20Polymers%0AAuthor%3A%20Brendan%20Young%20and%20Brendan%20Alvey%20and%20Andreas%20Werbrouck%20and%20Will%20Murphy%20and%20James%20Keller%20and%20Matthias%20J.%20Young%20and%20Matthew%20Maschmann%0AAbstract%3A%20Spin%20coating%20polymer%20thin%20films%20to%20achieve%20specific%20mechanical%20properties%20is%20inherently%20a%20multi-objective%20optimization%20problem.%20We%20present%20a%20framework%20that%20integrates%20an%20active%20Pareto%20front%20learning%20algorithm%20%28PyePAL%29%20with%20visualization%20and%20explainable%20AI%20techniques%20to%20optimize%20processing%20parameters.%20PyePAL%20uses%20Gaussian%20process%20models%20to%20predict%20objective%20values%20%28hardness%20and%20elasticity%29%20from%20the%20design%20variables%20%28spin%20speed%2C%20dilution%2C%20and%20polymer%20mixture%29%2C%20guiding%20the%20adaptive%20selection%20of%20samples%20toward%20promising%20regions%20of%20the%20design%20space.%20To%20enable%20interpretable%20insights%20into%20the%20high-dimensional%20design%20space%2C%20we%20utilize%20UMAP%20%28Uniform%20Manifold%20Approximation%20and%20Projection%29%20for%20two-dimensional%20visualization%20of%20the%20Pareto%20front%20exploration.%20Additionally%2C%20we%20incorporate%20fuzzy%20linguistic%20summaries%2C%20which%20translate%20the%20learned%20relationships%20between%20process%20parameters%20and%20performance%20objectives%20into%20linguistic%20statements%2C%20thus%20enhancing%20the%20explainability%20and%20understanding%20of%20the%20optimization%20results.%20Experimental%20results%20demonstrate%20that%20our%20method%20efficiently%20identifies%20promising%20polymer%20designs%2C%20while%20the%20visual%20and%20linguistic%20explanations%20facilitate%20expert-driven%20analysis%20and%20knowledge%20discovery.%0ALink%3A%20http%3A//arxiv.org/abs/2509.08988v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Learning%2520and%2520Explainable%2520AI%2520for%2520Multi-Objective%2520Optimization%2520of%2520Spin%2520Coated%2520Polymers%26entry.906535625%3DBrendan%2520Young%2520and%2520Brendan%2520Alvey%2520and%2520Andreas%2520Werbrouck%2520and%2520Will%2520Murphy%2520and%2520James%2520Keller%2520and%2520Matthias%2520J.%2520Young%2520and%2520Matthew%2520Maschmann%26entry.1292438233%3DSpin%2520coating%2520polymer%2520thin%2520films%2520to%2520achieve%2520specific%2520mechanical%2520properties%2520is%2520inherently%2520a%2520multi-objective%2520optimization%2520problem.%2520We%2520present%2520a%2520framework%2520that%2520integrates%2520an%2520active%2520Pareto%2520front%2520learning%2520algorithm%2520%2528PyePAL%2529%2520with%2520visualization%2520and%2520explainable%2520AI%2520techniques%2520to%2520optimize%2520processing%2520parameters.%2520PyePAL%2520uses%2520Gaussian%2520process%2520models%2520to%2520predict%2520objective%2520values%2520%2528hardness%2520and%2520elasticity%2529%2520from%2520the%2520design%2520variables%2520%2528spin%2520speed%252C%2520dilution%252C%2520and%2520polymer%2520mixture%2529%252C%2520guiding%2520the%2520adaptive%2520selection%2520of%2520samples%2520toward%2520promising%2520regions%2520of%2520the%2520design%2520space.%2520To%2520enable%2520interpretable%2520insights%2520into%2520the%2520high-dimensional%2520design%2520space%252C%2520we%2520utilize%2520UMAP%2520%2528Uniform%2520Manifold%2520Approximation%2520and%2520Projection%2529%2520for%2520two-dimensional%2520visualization%2520of%2520the%2520Pareto%2520front%2520exploration.%2520Additionally%252C%2520we%2520incorporate%2520fuzzy%2520linguistic%2520summaries%252C%2520which%2520translate%2520the%2520learned%2520relationships%2520between%2520process%2520parameters%2520and%2520performance%2520objectives%2520into%2520linguistic%2520statements%252C%2520thus%2520enhancing%2520the%2520explainability%2520and%2520understanding%2520of%2520the%2520optimization%2520results.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520efficiently%2520identifies%2520promising%2520polymer%2520designs%252C%2520while%2520the%2520visual%2520and%2520linguistic%2520explanations%2520facilitate%2520expert-driven%2520analysis%2520and%2520knowledge%2520discovery.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.08988v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Learning%20and%20Explainable%20AI%20for%20Multi-Objective%20Optimization%20of%20Spin%20Coated%20Polymers&entry.906535625=Brendan%20Young%20and%20Brendan%20Alvey%20and%20Andreas%20Werbrouck%20and%20Will%20Murphy%20and%20James%20Keller%20and%20Matthias%20J.%20Young%20and%20Matthew%20Maschmann&entry.1292438233=Spin%20coating%20polymer%20thin%20films%20to%20achieve%20specific%20mechanical%20properties%20is%20inherently%20a%20multi-objective%20optimization%20problem.%20We%20present%20a%20framework%20that%20integrates%20an%20active%20Pareto%20front%20learning%20algorithm%20%28PyePAL%29%20with%20visualization%20and%20explainable%20AI%20techniques%20to%20optimize%20processing%20parameters.%20PyePAL%20uses%20Gaussian%20process%20models%20to%20predict%20objective%20values%20%28hardness%20and%20elasticity%29%20from%20the%20design%20variables%20%28spin%20speed%2C%20dilution%2C%20and%20polymer%20mixture%29%2C%20guiding%20the%20adaptive%20selection%20of%20samples%20toward%20promising%20regions%20of%20the%20design%20space.%20To%20enable%20interpretable%20insights%20into%20the%20high-dimensional%20design%20space%2C%20we%20utilize%20UMAP%20%28Uniform%20Manifold%20Approximation%20and%20Projection%29%20for%20two-dimensional%20visualization%20of%20the%20Pareto%20front%20exploration.%20Additionally%2C%20we%20incorporate%20fuzzy%20linguistic%20summaries%2C%20which%20translate%20the%20learned%20relationships%20between%20process%20parameters%20and%20performance%20objectives%20into%20linguistic%20statements%2C%20thus%20enhancing%20the%20explainability%20and%20understanding%20of%20the%20optimization%20results.%20Experimental%20results%20demonstrate%20that%20our%20method%20efficiently%20identifies%20promising%20polymer%20designs%2C%20while%20the%20visual%20and%20linguistic%20explanations%20facilitate%20expert-driven%20analysis%20and%20knowledge%20discovery.&entry.1838667208=http%3A//arxiv.org/abs/2509.08988v2&entry.124074799=Read"},
{"title": "Debiasing Machine Learning Predictions for Causal Inference Without Additional Ground Truth Data: \"One Map, Many Trials\" in Satellite-Driven Poverty Analysis", "author": "Markus B. Pettersson and Connor T. Jerzak and Adel Daoud", "abstract": "Machine learning models trained on Earth observation data, such as satellite imagery, have demonstrated significant promise in predicting household-level wealth indices, enabling the creation of high-resolution wealth maps that can be leveraged across multiple causal trials while addressing chronic data scarcity in global development research. However, because standard training objectives prioritize overall predictive accuracy, these predictions often suffer from shrinkage toward the mean, leading to attenuated estimates of causal treatment effects and limiting their utility in policy evaluations. Existing debiasing methods, such as Prediction-Powered Inference (PPI), can handle this attenuation bias but require additional fresh ground-truth data at the downstream stage of causal inference, which restricts their applicability in data-scarce environments. We introduce and evaluate two post-hoc correction methods -- Linear Calibration Correction (LCC) and a Tweedie's correction approach -- that substantially reduce shrinkage-induced prediction bias without relying on newly collected labeled data. LCC applies a simple linear transformation estimated on a held-out calibration split; Tweedie's method locally de-shrink predictions using density score estimates and a noise scale learned upstream. We provide practical diagnostics for when a correction is warranted and discuss practical limitations. Across analytical results, simulations, and experiments with Demographic and Health Surveys (DHS) data, both approaches reduce attenuation; Tweedie's correction yields nearly unbiased treatment-effect estimates, enabling a \"one map, many trials\" paradigm. Although we demonstrate on EO-ML wealth mapping, the methods are not geospatial-specific: they apply to any setting where imputed outcomes are reused downstream (e.g., pollution indices, population density, or LLM-derived indicators).", "link": "http://arxiv.org/abs/2508.01341v3", "date": "2025-11-14", "relevancy": 1.4307, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4957}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4751}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Debiasing%20Machine%20Learning%20Predictions%20for%20Causal%20Inference%20Without%20Additional%20Ground%20Truth%20Data%3A%20%22One%20Map%2C%20Many%20Trials%22%20in%20Satellite-Driven%20Poverty%20Analysis&body=Title%3A%20Debiasing%20Machine%20Learning%20Predictions%20for%20Causal%20Inference%20Without%20Additional%20Ground%20Truth%20Data%3A%20%22One%20Map%2C%20Many%20Trials%22%20in%20Satellite-Driven%20Poverty%20Analysis%0AAuthor%3A%20Markus%20B.%20Pettersson%20and%20Connor%20T.%20Jerzak%20and%20Adel%20Daoud%0AAbstract%3A%20Machine%20learning%20models%20trained%20on%20Earth%20observation%20data%2C%20such%20as%20satellite%20imagery%2C%20have%20demonstrated%20significant%20promise%20in%20predicting%20household-level%20wealth%20indices%2C%20enabling%20the%20creation%20of%20high-resolution%20wealth%20maps%20that%20can%20be%20leveraged%20across%20multiple%20causal%20trials%20while%20addressing%20chronic%20data%20scarcity%20in%20global%20development%20research.%20However%2C%20because%20standard%20training%20objectives%20prioritize%20overall%20predictive%20accuracy%2C%20these%20predictions%20often%20suffer%20from%20shrinkage%20toward%20the%20mean%2C%20leading%20to%20attenuated%20estimates%20of%20causal%20treatment%20effects%20and%20limiting%20their%20utility%20in%20policy%20evaluations.%20Existing%20debiasing%20methods%2C%20such%20as%20Prediction-Powered%20Inference%20%28PPI%29%2C%20can%20handle%20this%20attenuation%20bias%20but%20require%20additional%20fresh%20ground-truth%20data%20at%20the%20downstream%20stage%20of%20causal%20inference%2C%20which%20restricts%20their%20applicability%20in%20data-scarce%20environments.%20We%20introduce%20and%20evaluate%20two%20post-hoc%20correction%20methods%20--%20Linear%20Calibration%20Correction%20%28LCC%29%20and%20a%20Tweedie%27s%20correction%20approach%20--%20that%20substantially%20reduce%20shrinkage-induced%20prediction%20bias%20without%20relying%20on%20newly%20collected%20labeled%20data.%20LCC%20applies%20a%20simple%20linear%20transformation%20estimated%20on%20a%20held-out%20calibration%20split%3B%20Tweedie%27s%20method%20locally%20de-shrink%20predictions%20using%20density%20score%20estimates%20and%20a%20noise%20scale%20learned%20upstream.%20We%20provide%20practical%20diagnostics%20for%20when%20a%20correction%20is%20warranted%20and%20discuss%20practical%20limitations.%20Across%20analytical%20results%2C%20simulations%2C%20and%20experiments%20with%20Demographic%20and%20Health%20Surveys%20%28DHS%29%20data%2C%20both%20approaches%20reduce%20attenuation%3B%20Tweedie%27s%20correction%20yields%20nearly%20unbiased%20treatment-effect%20estimates%2C%20enabling%20a%20%22one%20map%2C%20many%20trials%22%20paradigm.%20Although%20we%20demonstrate%20on%20EO-ML%20wealth%20mapping%2C%20the%20methods%20are%20not%20geospatial-specific%3A%20they%20apply%20to%20any%20setting%20where%20imputed%20outcomes%20are%20reused%20downstream%20%28e.g.%2C%20pollution%20indices%2C%20population%20density%2C%20or%20LLM-derived%20indicators%29.%0ALink%3A%20http%3A//arxiv.org/abs/2508.01341v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDebiasing%2520Machine%2520Learning%2520Predictions%2520for%2520Causal%2520Inference%2520Without%2520Additional%2520Ground%2520Truth%2520Data%253A%2520%2522One%2520Map%252C%2520Many%2520Trials%2522%2520in%2520Satellite-Driven%2520Poverty%2520Analysis%26entry.906535625%3DMarkus%2520B.%2520Pettersson%2520and%2520Connor%2520T.%2520Jerzak%2520and%2520Adel%2520Daoud%26entry.1292438233%3DMachine%2520learning%2520models%2520trained%2520on%2520Earth%2520observation%2520data%252C%2520such%2520as%2520satellite%2520imagery%252C%2520have%2520demonstrated%2520significant%2520promise%2520in%2520predicting%2520household-level%2520wealth%2520indices%252C%2520enabling%2520the%2520creation%2520of%2520high-resolution%2520wealth%2520maps%2520that%2520can%2520be%2520leveraged%2520across%2520multiple%2520causal%2520trials%2520while%2520addressing%2520chronic%2520data%2520scarcity%2520in%2520global%2520development%2520research.%2520However%252C%2520because%2520standard%2520training%2520objectives%2520prioritize%2520overall%2520predictive%2520accuracy%252C%2520these%2520predictions%2520often%2520suffer%2520from%2520shrinkage%2520toward%2520the%2520mean%252C%2520leading%2520to%2520attenuated%2520estimates%2520of%2520causal%2520treatment%2520effects%2520and%2520limiting%2520their%2520utility%2520in%2520policy%2520evaluations.%2520Existing%2520debiasing%2520methods%252C%2520such%2520as%2520Prediction-Powered%2520Inference%2520%2528PPI%2529%252C%2520can%2520handle%2520this%2520attenuation%2520bias%2520but%2520require%2520additional%2520fresh%2520ground-truth%2520data%2520at%2520the%2520downstream%2520stage%2520of%2520causal%2520inference%252C%2520which%2520restricts%2520their%2520applicability%2520in%2520data-scarce%2520environments.%2520We%2520introduce%2520and%2520evaluate%2520two%2520post-hoc%2520correction%2520methods%2520--%2520Linear%2520Calibration%2520Correction%2520%2528LCC%2529%2520and%2520a%2520Tweedie%2527s%2520correction%2520approach%2520--%2520that%2520substantially%2520reduce%2520shrinkage-induced%2520prediction%2520bias%2520without%2520relying%2520on%2520newly%2520collected%2520labeled%2520data.%2520LCC%2520applies%2520a%2520simple%2520linear%2520transformation%2520estimated%2520on%2520a%2520held-out%2520calibration%2520split%253B%2520Tweedie%2527s%2520method%2520locally%2520de-shrink%2520predictions%2520using%2520density%2520score%2520estimates%2520and%2520a%2520noise%2520scale%2520learned%2520upstream.%2520We%2520provide%2520practical%2520diagnostics%2520for%2520when%2520a%2520correction%2520is%2520warranted%2520and%2520discuss%2520practical%2520limitations.%2520Across%2520analytical%2520results%252C%2520simulations%252C%2520and%2520experiments%2520with%2520Demographic%2520and%2520Health%2520Surveys%2520%2528DHS%2529%2520data%252C%2520both%2520approaches%2520reduce%2520attenuation%253B%2520Tweedie%2527s%2520correction%2520yields%2520nearly%2520unbiased%2520treatment-effect%2520estimates%252C%2520enabling%2520a%2520%2522one%2520map%252C%2520many%2520trials%2522%2520paradigm.%2520Although%2520we%2520demonstrate%2520on%2520EO-ML%2520wealth%2520mapping%252C%2520the%2520methods%2520are%2520not%2520geospatial-specific%253A%2520they%2520apply%2520to%2520any%2520setting%2520where%2520imputed%2520outcomes%2520are%2520reused%2520downstream%2520%2528e.g.%252C%2520pollution%2520indices%252C%2520population%2520density%252C%2520or%2520LLM-derived%2520indicators%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01341v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Debiasing%20Machine%20Learning%20Predictions%20for%20Causal%20Inference%20Without%20Additional%20Ground%20Truth%20Data%3A%20%22One%20Map%2C%20Many%20Trials%22%20in%20Satellite-Driven%20Poverty%20Analysis&entry.906535625=Markus%20B.%20Pettersson%20and%20Connor%20T.%20Jerzak%20and%20Adel%20Daoud&entry.1292438233=Machine%20learning%20models%20trained%20on%20Earth%20observation%20data%2C%20such%20as%20satellite%20imagery%2C%20have%20demonstrated%20significant%20promise%20in%20predicting%20household-level%20wealth%20indices%2C%20enabling%20the%20creation%20of%20high-resolution%20wealth%20maps%20that%20can%20be%20leveraged%20across%20multiple%20causal%20trials%20while%20addressing%20chronic%20data%20scarcity%20in%20global%20development%20research.%20However%2C%20because%20standard%20training%20objectives%20prioritize%20overall%20predictive%20accuracy%2C%20these%20predictions%20often%20suffer%20from%20shrinkage%20toward%20the%20mean%2C%20leading%20to%20attenuated%20estimates%20of%20causal%20treatment%20effects%20and%20limiting%20their%20utility%20in%20policy%20evaluations.%20Existing%20debiasing%20methods%2C%20such%20as%20Prediction-Powered%20Inference%20%28PPI%29%2C%20can%20handle%20this%20attenuation%20bias%20but%20require%20additional%20fresh%20ground-truth%20data%20at%20the%20downstream%20stage%20of%20causal%20inference%2C%20which%20restricts%20their%20applicability%20in%20data-scarce%20environments.%20We%20introduce%20and%20evaluate%20two%20post-hoc%20correction%20methods%20--%20Linear%20Calibration%20Correction%20%28LCC%29%20and%20a%20Tweedie%27s%20correction%20approach%20--%20that%20substantially%20reduce%20shrinkage-induced%20prediction%20bias%20without%20relying%20on%20newly%20collected%20labeled%20data.%20LCC%20applies%20a%20simple%20linear%20transformation%20estimated%20on%20a%20held-out%20calibration%20split%3B%20Tweedie%27s%20method%20locally%20de-shrink%20predictions%20using%20density%20score%20estimates%20and%20a%20noise%20scale%20learned%20upstream.%20We%20provide%20practical%20diagnostics%20for%20when%20a%20correction%20is%20warranted%20and%20discuss%20practical%20limitations.%20Across%20analytical%20results%2C%20simulations%2C%20and%20experiments%20with%20Demographic%20and%20Health%20Surveys%20%28DHS%29%20data%2C%20both%20approaches%20reduce%20attenuation%3B%20Tweedie%27s%20correction%20yields%20nearly%20unbiased%20treatment-effect%20estimates%2C%20enabling%20a%20%22one%20map%2C%20many%20trials%22%20paradigm.%20Although%20we%20demonstrate%20on%20EO-ML%20wealth%20mapping%2C%20the%20methods%20are%20not%20geospatial-specific%3A%20they%20apply%20to%20any%20setting%20where%20imputed%20outcomes%20are%20reused%20downstream%20%28e.g.%2C%20pollution%20indices%2C%20population%20density%2C%20or%20LLM-derived%20indicators%29.&entry.1838667208=http%3A//arxiv.org/abs/2508.01341v3&entry.124074799=Read"},
{"title": "Scalable Policy Evaluation with Video World Models", "author": "Wei-Cheng Tseng and Jinwei Gu and Qinsheng Zhang and Hanzi Mao and Ming-Yu Liu and Florian Shkurti and Lin Yen-Chen", "abstract": "Training generalist policies for robotic manipulation has shown great promise, as they enable language-conditioned, multi-task behaviors across diverse scenarios. However, evaluating these policies remains difficult because real-world testing is expensive, time-consuming, and labor-intensive. It also requires frequent environment resets and carries safety risks when deploying unproven policies on physical robots. Manually creating and populating simulation environments with assets for robotic manipulation has not addressed these issues, primarily due to the significant engineering effort required and the often substantial sim-to-real gap, both in terms of physics and rendering. In this paper, we explore the use of action-conditional video generation models as a scalable way to learn world models for policy evaluation. We demonstrate how to incorporate action conditioning into existing pre-trained video generation models. This allows leveraging internet-scale in-the-wild online videos during the pre-training stage, and alleviates the need for a large dataset of paired video-action data, which is expensive to collect for robotic manipulation. Our paper examines the effect of dataset diversity, pre-trained weight and common failure cases for the proposed evaluation pipeline.Our experiments demonstrate that, across various metrics, including policy ranking and the correlation between actual policy values and predicted policy values, these models offer a promising approach for evaluating policies without requiring real-world interactions.", "link": "http://arxiv.org/abs/2511.11520v1", "date": "2025-11-14", "relevancy": 1.8513, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6379}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6124}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Policy%20Evaluation%20with%20Video%20World%20Models&body=Title%3A%20Scalable%20Policy%20Evaluation%20with%20Video%20World%20Models%0AAuthor%3A%20Wei-Cheng%20Tseng%20and%20Jinwei%20Gu%20and%20Qinsheng%20Zhang%20and%20Hanzi%20Mao%20and%20Ming-Yu%20Liu%20and%20Florian%20Shkurti%20and%20Lin%20Yen-Chen%0AAbstract%3A%20Training%20generalist%20policies%20for%20robotic%20manipulation%20has%20shown%20great%20promise%2C%20as%20they%20enable%20language-conditioned%2C%20multi-task%20behaviors%20across%20diverse%20scenarios.%20However%2C%20evaluating%20these%20policies%20remains%20difficult%20because%20real-world%20testing%20is%20expensive%2C%20time-consuming%2C%20and%20labor-intensive.%20It%20also%20requires%20frequent%20environment%20resets%20and%20carries%20safety%20risks%20when%20deploying%20unproven%20policies%20on%20physical%20robots.%20Manually%20creating%20and%20populating%20simulation%20environments%20with%20assets%20for%20robotic%20manipulation%20has%20not%20addressed%20these%20issues%2C%20primarily%20due%20to%20the%20significant%20engineering%20effort%20required%20and%20the%20often%20substantial%20sim-to-real%20gap%2C%20both%20in%20terms%20of%20physics%20and%20rendering.%20In%20this%20paper%2C%20we%20explore%20the%20use%20of%20action-conditional%20video%20generation%20models%20as%20a%20scalable%20way%20to%20learn%20world%20models%20for%20policy%20evaluation.%20We%20demonstrate%20how%20to%20incorporate%20action%20conditioning%20into%20existing%20pre-trained%20video%20generation%20models.%20This%20allows%20leveraging%20internet-scale%20in-the-wild%20online%20videos%20during%20the%20pre-training%20stage%2C%20and%20alleviates%20the%20need%20for%20a%20large%20dataset%20of%20paired%20video-action%20data%2C%20which%20is%20expensive%20to%20collect%20for%20robotic%20manipulation.%20Our%20paper%20examines%20the%20effect%20of%20dataset%20diversity%2C%20pre-trained%20weight%20and%20common%20failure%20cases%20for%20the%20proposed%20evaluation%20pipeline.Our%20experiments%20demonstrate%20that%2C%20across%20various%20metrics%2C%20including%20policy%20ranking%20and%20the%20correlation%20between%20actual%20policy%20values%20and%20predicted%20policy%20values%2C%20these%20models%20offer%20a%20promising%20approach%20for%20evaluating%20policies%20without%20requiring%20real-world%20interactions.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Policy%2520Evaluation%2520with%2520Video%2520World%2520Models%26entry.906535625%3DWei-Cheng%2520Tseng%2520and%2520Jinwei%2520Gu%2520and%2520Qinsheng%2520Zhang%2520and%2520Hanzi%2520Mao%2520and%2520Ming-Yu%2520Liu%2520and%2520Florian%2520Shkurti%2520and%2520Lin%2520Yen-Chen%26entry.1292438233%3DTraining%2520generalist%2520policies%2520for%2520robotic%2520manipulation%2520has%2520shown%2520great%2520promise%252C%2520as%2520they%2520enable%2520language-conditioned%252C%2520multi-task%2520behaviors%2520across%2520diverse%2520scenarios.%2520However%252C%2520evaluating%2520these%2520policies%2520remains%2520difficult%2520because%2520real-world%2520testing%2520is%2520expensive%252C%2520time-consuming%252C%2520and%2520labor-intensive.%2520It%2520also%2520requires%2520frequent%2520environment%2520resets%2520and%2520carries%2520safety%2520risks%2520when%2520deploying%2520unproven%2520policies%2520on%2520physical%2520robots.%2520Manually%2520creating%2520and%2520populating%2520simulation%2520environments%2520with%2520assets%2520for%2520robotic%2520manipulation%2520has%2520not%2520addressed%2520these%2520issues%252C%2520primarily%2520due%2520to%2520the%2520significant%2520engineering%2520effort%2520required%2520and%2520the%2520often%2520substantial%2520sim-to-real%2520gap%252C%2520both%2520in%2520terms%2520of%2520physics%2520and%2520rendering.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520use%2520of%2520action-conditional%2520video%2520generation%2520models%2520as%2520a%2520scalable%2520way%2520to%2520learn%2520world%2520models%2520for%2520policy%2520evaluation.%2520We%2520demonstrate%2520how%2520to%2520incorporate%2520action%2520conditioning%2520into%2520existing%2520pre-trained%2520video%2520generation%2520models.%2520This%2520allows%2520leveraging%2520internet-scale%2520in-the-wild%2520online%2520videos%2520during%2520the%2520pre-training%2520stage%252C%2520and%2520alleviates%2520the%2520need%2520for%2520a%2520large%2520dataset%2520of%2520paired%2520video-action%2520data%252C%2520which%2520is%2520expensive%2520to%2520collect%2520for%2520robotic%2520manipulation.%2520Our%2520paper%2520examines%2520the%2520effect%2520of%2520dataset%2520diversity%252C%2520pre-trained%2520weight%2520and%2520common%2520failure%2520cases%2520for%2520the%2520proposed%2520evaluation%2520pipeline.Our%2520experiments%2520demonstrate%2520that%252C%2520across%2520various%2520metrics%252C%2520including%2520policy%2520ranking%2520and%2520the%2520correlation%2520between%2520actual%2520policy%2520values%2520and%2520predicted%2520policy%2520values%252C%2520these%2520models%2520offer%2520a%2520promising%2520approach%2520for%2520evaluating%2520policies%2520without%2520requiring%2520real-world%2520interactions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Policy%20Evaluation%20with%20Video%20World%20Models&entry.906535625=Wei-Cheng%20Tseng%20and%20Jinwei%20Gu%20and%20Qinsheng%20Zhang%20and%20Hanzi%20Mao%20and%20Ming-Yu%20Liu%20and%20Florian%20Shkurti%20and%20Lin%20Yen-Chen&entry.1292438233=Training%20generalist%20policies%20for%20robotic%20manipulation%20has%20shown%20great%20promise%2C%20as%20they%20enable%20language-conditioned%2C%20multi-task%20behaviors%20across%20diverse%20scenarios.%20However%2C%20evaluating%20these%20policies%20remains%20difficult%20because%20real-world%20testing%20is%20expensive%2C%20time-consuming%2C%20and%20labor-intensive.%20It%20also%20requires%20frequent%20environment%20resets%20and%20carries%20safety%20risks%20when%20deploying%20unproven%20policies%20on%20physical%20robots.%20Manually%20creating%20and%20populating%20simulation%20environments%20with%20assets%20for%20robotic%20manipulation%20has%20not%20addressed%20these%20issues%2C%20primarily%20due%20to%20the%20significant%20engineering%20effort%20required%20and%20the%20often%20substantial%20sim-to-real%20gap%2C%20both%20in%20terms%20of%20physics%20and%20rendering.%20In%20this%20paper%2C%20we%20explore%20the%20use%20of%20action-conditional%20video%20generation%20models%20as%20a%20scalable%20way%20to%20learn%20world%20models%20for%20policy%20evaluation.%20We%20demonstrate%20how%20to%20incorporate%20action%20conditioning%20into%20existing%20pre-trained%20video%20generation%20models.%20This%20allows%20leveraging%20internet-scale%20in-the-wild%20online%20videos%20during%20the%20pre-training%20stage%2C%20and%20alleviates%20the%20need%20for%20a%20large%20dataset%20of%20paired%20video-action%20data%2C%20which%20is%20expensive%20to%20collect%20for%20robotic%20manipulation.%20Our%20paper%20examines%20the%20effect%20of%20dataset%20diversity%2C%20pre-trained%20weight%20and%20common%20failure%20cases%20for%20the%20proposed%20evaluation%20pipeline.Our%20experiments%20demonstrate%20that%2C%20across%20various%20metrics%2C%20including%20policy%20ranking%20and%20the%20correlation%20between%20actual%20policy%20values%20and%20predicted%20policy%20values%2C%20these%20models%20offer%20a%20promising%20approach%20for%20evaluating%20policies%20without%20requiring%20real-world%20interactions.&entry.1838667208=http%3A//arxiv.org/abs/2511.11520v1&entry.124074799=Read"},
{"title": "RetrySQL: text-to-SQL training with retry data for self-correcting query generation", "author": "Alicja R\u0105czkowska and Riccardo Belluzzo and Piotr Zieli\u0144ski and Joanna Baran and Pawe\u0142 Olszewski", "abstract": "The text-to-SQL task is an active challenge in Natural Language Processing. Many existing solutions focus on using black-box language models extended with specialized components within customized end-to-end text-to-SQL pipelines. While these solutions use both closed-source proprietary language models and coding-oriented open-source models, there is a lack of research regarding SQL-specific generative models. At the same time, recent advancements in self-correcting generation strategies show promise for improving the capabilities of existing architectures. The application of these concepts to the text-to-SQL task remains unexplored. In this paper, we introduce RetrySQL, a new approach to training text-to-SQL generation models. We prepare reasoning steps for reference SQL queries and then corrupt them to create retry data that contains both incorrect and corrected steps, divided with a special token. We continuously pre-train an open-source coding model with this data and demonstrate that retry steps yield an improvement of up to 4 percentage points in both overall and challenging execution accuracy metrics, compared to pre-training without retry data. Additionally, we confirm that supervised fine-tuning with LoRA is ineffective for learning from retry data and that full-parameter pre-training is a necessary requirement for that task. We showcase that the self-correcting behavior is learned by the model and the increase in downstream accuracy metrics is a result of this additional skill. Finally, we incorporate RetrySQL-trained models into the full text-to-SQL pipeline and showcase that they are competitive in terms of execution accuracy with proprietary models that contain orders of magnitude more parameters. RetrySQL demonstrates that self-correction can be learned in the text-to-SQL task and provides a novel way of improving generation accuracy for SQL-oriented language models.", "link": "http://arxiv.org/abs/2507.02529v2", "date": "2025-11-14", "relevancy": 1.7685, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4868}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4339}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RetrySQL%3A%20text-to-SQL%20training%20with%20retry%20data%20for%20self-correcting%20query%20generation&body=Title%3A%20RetrySQL%3A%20text-to-SQL%20training%20with%20retry%20data%20for%20self-correcting%20query%20generation%0AAuthor%3A%20Alicja%20R%C4%85czkowska%20and%20Riccardo%20Belluzzo%20and%20Piotr%20Zieli%C5%84ski%20and%20Joanna%20Baran%20and%20Pawe%C5%82%20Olszewski%0AAbstract%3A%20The%20text-to-SQL%20task%20is%20an%20active%20challenge%20in%20Natural%20Language%20Processing.%20Many%20existing%20solutions%20focus%20on%20using%20black-box%20language%20models%20extended%20with%20specialized%20components%20within%20customized%20end-to-end%20text-to-SQL%20pipelines.%20While%20these%20solutions%20use%20both%20closed-source%20proprietary%20language%20models%20and%20coding-oriented%20open-source%20models%2C%20there%20is%20a%20lack%20of%20research%20regarding%20SQL-specific%20generative%20models.%20At%20the%20same%20time%2C%20recent%20advancements%20in%20self-correcting%20generation%20strategies%20show%20promise%20for%20improving%20the%20capabilities%20of%20existing%20architectures.%20The%20application%20of%20these%20concepts%20to%20the%20text-to-SQL%20task%20remains%20unexplored.%20In%20this%20paper%2C%20we%20introduce%20RetrySQL%2C%20a%20new%20approach%20to%20training%20text-to-SQL%20generation%20models.%20We%20prepare%20reasoning%20steps%20for%20reference%20SQL%20queries%20and%20then%20corrupt%20them%20to%20create%20retry%20data%20that%20contains%20both%20incorrect%20and%20corrected%20steps%2C%20divided%20with%20a%20special%20token.%20We%20continuously%20pre-train%20an%20open-source%20coding%20model%20with%20this%20data%20and%20demonstrate%20that%20retry%20steps%20yield%20an%20improvement%20of%20up%20to%204%20percentage%20points%20in%20both%20overall%20and%20challenging%20execution%20accuracy%20metrics%2C%20compared%20to%20pre-training%20without%20retry%20data.%20Additionally%2C%20we%20confirm%20that%20supervised%20fine-tuning%20with%20LoRA%20is%20ineffective%20for%20learning%20from%20retry%20data%20and%20that%20full-parameter%20pre-training%20is%20a%20necessary%20requirement%20for%20that%20task.%20We%20showcase%20that%20the%20self-correcting%20behavior%20is%20learned%20by%20the%20model%20and%20the%20increase%20in%20downstream%20accuracy%20metrics%20is%20a%20result%20of%20this%20additional%20skill.%20Finally%2C%20we%20incorporate%20RetrySQL-trained%20models%20into%20the%20full%20text-to-SQL%20pipeline%20and%20showcase%20that%20they%20are%20competitive%20in%20terms%20of%20execution%20accuracy%20with%20proprietary%20models%20that%20contain%20orders%20of%20magnitude%20more%20parameters.%20RetrySQL%20demonstrates%20that%20self-correction%20can%20be%20learned%20in%20the%20text-to-SQL%20task%20and%20provides%20a%20novel%20way%20of%20improving%20generation%20accuracy%20for%20SQL-oriented%20language%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2507.02529v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrySQL%253A%2520text-to-SQL%2520training%2520with%2520retry%2520data%2520for%2520self-correcting%2520query%2520generation%26entry.906535625%3DAlicja%2520R%25C4%2585czkowska%2520and%2520Riccardo%2520Belluzzo%2520and%2520Piotr%2520Zieli%25C5%2584ski%2520and%2520Joanna%2520Baran%2520and%2520Pawe%25C5%2582%2520Olszewski%26entry.1292438233%3DThe%2520text-to-SQL%2520task%2520is%2520an%2520active%2520challenge%2520in%2520Natural%2520Language%2520Processing.%2520Many%2520existing%2520solutions%2520focus%2520on%2520using%2520black-box%2520language%2520models%2520extended%2520with%2520specialized%2520components%2520within%2520customized%2520end-to-end%2520text-to-SQL%2520pipelines.%2520While%2520these%2520solutions%2520use%2520both%2520closed-source%2520proprietary%2520language%2520models%2520and%2520coding-oriented%2520open-source%2520models%252C%2520there%2520is%2520a%2520lack%2520of%2520research%2520regarding%2520SQL-specific%2520generative%2520models.%2520At%2520the%2520same%2520time%252C%2520recent%2520advancements%2520in%2520self-correcting%2520generation%2520strategies%2520show%2520promise%2520for%2520improving%2520the%2520capabilities%2520of%2520existing%2520architectures.%2520The%2520application%2520of%2520these%2520concepts%2520to%2520the%2520text-to-SQL%2520task%2520remains%2520unexplored.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520RetrySQL%252C%2520a%2520new%2520approach%2520to%2520training%2520text-to-SQL%2520generation%2520models.%2520We%2520prepare%2520reasoning%2520steps%2520for%2520reference%2520SQL%2520queries%2520and%2520then%2520corrupt%2520them%2520to%2520create%2520retry%2520data%2520that%2520contains%2520both%2520incorrect%2520and%2520corrected%2520steps%252C%2520divided%2520with%2520a%2520special%2520token.%2520We%2520continuously%2520pre-train%2520an%2520open-source%2520coding%2520model%2520with%2520this%2520data%2520and%2520demonstrate%2520that%2520retry%2520steps%2520yield%2520an%2520improvement%2520of%2520up%2520to%25204%2520percentage%2520points%2520in%2520both%2520overall%2520and%2520challenging%2520execution%2520accuracy%2520metrics%252C%2520compared%2520to%2520pre-training%2520without%2520retry%2520data.%2520Additionally%252C%2520we%2520confirm%2520that%2520supervised%2520fine-tuning%2520with%2520LoRA%2520is%2520ineffective%2520for%2520learning%2520from%2520retry%2520data%2520and%2520that%2520full-parameter%2520pre-training%2520is%2520a%2520necessary%2520requirement%2520for%2520that%2520task.%2520We%2520showcase%2520that%2520the%2520self-correcting%2520behavior%2520is%2520learned%2520by%2520the%2520model%2520and%2520the%2520increase%2520in%2520downstream%2520accuracy%2520metrics%2520is%2520a%2520result%2520of%2520this%2520additional%2520skill.%2520Finally%252C%2520we%2520incorporate%2520RetrySQL-trained%2520models%2520into%2520the%2520full%2520text-to-SQL%2520pipeline%2520and%2520showcase%2520that%2520they%2520are%2520competitive%2520in%2520terms%2520of%2520execution%2520accuracy%2520with%2520proprietary%2520models%2520that%2520contain%2520orders%2520of%2520magnitude%2520more%2520parameters.%2520RetrySQL%2520demonstrates%2520that%2520self-correction%2520can%2520be%2520learned%2520in%2520the%2520text-to-SQL%2520task%2520and%2520provides%2520a%2520novel%2520way%2520of%2520improving%2520generation%2520accuracy%2520for%2520SQL-oriented%2520language%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.02529v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RetrySQL%3A%20text-to-SQL%20training%20with%20retry%20data%20for%20self-correcting%20query%20generation&entry.906535625=Alicja%20R%C4%85czkowska%20and%20Riccardo%20Belluzzo%20and%20Piotr%20Zieli%C5%84ski%20and%20Joanna%20Baran%20and%20Pawe%C5%82%20Olszewski&entry.1292438233=The%20text-to-SQL%20task%20is%20an%20active%20challenge%20in%20Natural%20Language%20Processing.%20Many%20existing%20solutions%20focus%20on%20using%20black-box%20language%20models%20extended%20with%20specialized%20components%20within%20customized%20end-to-end%20text-to-SQL%20pipelines.%20While%20these%20solutions%20use%20both%20closed-source%20proprietary%20language%20models%20and%20coding-oriented%20open-source%20models%2C%20there%20is%20a%20lack%20of%20research%20regarding%20SQL-specific%20generative%20models.%20At%20the%20same%20time%2C%20recent%20advancements%20in%20self-correcting%20generation%20strategies%20show%20promise%20for%20improving%20the%20capabilities%20of%20existing%20architectures.%20The%20application%20of%20these%20concepts%20to%20the%20text-to-SQL%20task%20remains%20unexplored.%20In%20this%20paper%2C%20we%20introduce%20RetrySQL%2C%20a%20new%20approach%20to%20training%20text-to-SQL%20generation%20models.%20We%20prepare%20reasoning%20steps%20for%20reference%20SQL%20queries%20and%20then%20corrupt%20them%20to%20create%20retry%20data%20that%20contains%20both%20incorrect%20and%20corrected%20steps%2C%20divided%20with%20a%20special%20token.%20We%20continuously%20pre-train%20an%20open-source%20coding%20model%20with%20this%20data%20and%20demonstrate%20that%20retry%20steps%20yield%20an%20improvement%20of%20up%20to%204%20percentage%20points%20in%20both%20overall%20and%20challenging%20execution%20accuracy%20metrics%2C%20compared%20to%20pre-training%20without%20retry%20data.%20Additionally%2C%20we%20confirm%20that%20supervised%20fine-tuning%20with%20LoRA%20is%20ineffective%20for%20learning%20from%20retry%20data%20and%20that%20full-parameter%20pre-training%20is%20a%20necessary%20requirement%20for%20that%20task.%20We%20showcase%20that%20the%20self-correcting%20behavior%20is%20learned%20by%20the%20model%20and%20the%20increase%20in%20downstream%20accuracy%20metrics%20is%20a%20result%20of%20this%20additional%20skill.%20Finally%2C%20we%20incorporate%20RetrySQL-trained%20models%20into%20the%20full%20text-to-SQL%20pipeline%20and%20showcase%20that%20they%20are%20competitive%20in%20terms%20of%20execution%20accuracy%20with%20proprietary%20models%20that%20contain%20orders%20of%20magnitude%20more%20parameters.%20RetrySQL%20demonstrates%20that%20self-correction%20can%20be%20learned%20in%20the%20text-to-SQL%20task%20and%20provides%20a%20novel%20way%20of%20improving%20generation%20accuracy%20for%20SQL-oriented%20language%20models.&entry.1838667208=http%3A//arxiv.org/abs/2507.02529v2&entry.124074799=Read"},
{"title": "Training speedups via batching for geometric learning: an analysis of static and dynamic algorithms", "author": "Daniel T. Speckhard and Tim Bechtel and Sebastian Kehl and Jonathan Godwin and Claudia Draxl", "abstract": "Graph neural networks (GNN) have shown promising results for several domains such as materials science, chemistry, and the social sciences. GNN models often contain millions of parameters, and like other neural network (NN) models, are often fed only a fraction of the graphs that make up the training dataset in batches to update model parameters. The effect of batching algorithms on training time and model performance has been thoroughly explored for NNs but not yet for GNNs. We analyze two different batching algorithms for graph based models, namely static and dynamic batching for two datasets, the QM9 dataset of small molecules and the AFLOW materials database. Our experiments show that changing the batching algorithm can provide up to a 2.7x speedup, but the fastest algorithm depends on the data, model, batch size, hardware, and number of training steps run. Experiments show that for a select number of combinations of batch size, dataset, and model, significant differences in model learning metrics are observed between static and dynamic batching algorithms.", "link": "http://arxiv.org/abs/2502.00944v3", "date": "2025-11-14", "relevancy": 2.1035, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5541}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5093}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20speedups%20via%20batching%20for%20geometric%20learning%3A%20an%20analysis%20of%20static%20and%20dynamic%20algorithms&body=Title%3A%20Training%20speedups%20via%20batching%20for%20geometric%20learning%3A%20an%20analysis%20of%20static%20and%20dynamic%20algorithms%0AAuthor%3A%20Daniel%20T.%20Speckhard%20and%20Tim%20Bechtel%20and%20Sebastian%20Kehl%20and%20Jonathan%20Godwin%20and%20Claudia%20Draxl%0AAbstract%3A%20Graph%20neural%20networks%20%28GNN%29%20have%20shown%20promising%20results%20for%20several%20domains%20such%20as%20materials%20science%2C%20chemistry%2C%20and%20the%20social%20sciences.%20GNN%20models%20often%20contain%20millions%20of%20parameters%2C%20and%20like%20other%20neural%20network%20%28NN%29%20models%2C%20are%20often%20fed%20only%20a%20fraction%20of%20the%20graphs%20that%20make%20up%20the%20training%20dataset%20in%20batches%20to%20update%20model%20parameters.%20The%20effect%20of%20batching%20algorithms%20on%20training%20time%20and%20model%20performance%20has%20been%20thoroughly%20explored%20for%20NNs%20but%20not%20yet%20for%20GNNs.%20We%20analyze%20two%20different%20batching%20algorithms%20for%20graph%20based%20models%2C%20namely%20static%20and%20dynamic%20batching%20for%20two%20datasets%2C%20the%20QM9%20dataset%20of%20small%20molecules%20and%20the%20AFLOW%20materials%20database.%20Our%20experiments%20show%20that%20changing%20the%20batching%20algorithm%20can%20provide%20up%20to%20a%202.7x%20speedup%2C%20but%20the%20fastest%20algorithm%20depends%20on%20the%20data%2C%20model%2C%20batch%20size%2C%20hardware%2C%20and%20number%20of%20training%20steps%20run.%20Experiments%20show%20that%20for%20a%20select%20number%20of%20combinations%20of%20batch%20size%2C%20dataset%2C%20and%20model%2C%20significant%20differences%20in%20model%20learning%20metrics%20are%20observed%20between%20static%20and%20dynamic%20batching%20algorithms.%0ALink%3A%20http%3A//arxiv.org/abs/2502.00944v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520speedups%2520via%2520batching%2520for%2520geometric%2520learning%253A%2520an%2520analysis%2520of%2520static%2520and%2520dynamic%2520algorithms%26entry.906535625%3DDaniel%2520T.%2520Speckhard%2520and%2520Tim%2520Bechtel%2520and%2520Sebastian%2520Kehl%2520and%2520Jonathan%2520Godwin%2520and%2520Claudia%2520Draxl%26entry.1292438233%3DGraph%2520neural%2520networks%2520%2528GNN%2529%2520have%2520shown%2520promising%2520results%2520for%2520several%2520domains%2520such%2520as%2520materials%2520science%252C%2520chemistry%252C%2520and%2520the%2520social%2520sciences.%2520GNN%2520models%2520often%2520contain%2520millions%2520of%2520parameters%252C%2520and%2520like%2520other%2520neural%2520network%2520%2528NN%2529%2520models%252C%2520are%2520often%2520fed%2520only%2520a%2520fraction%2520of%2520the%2520graphs%2520that%2520make%2520up%2520the%2520training%2520dataset%2520in%2520batches%2520to%2520update%2520model%2520parameters.%2520The%2520effect%2520of%2520batching%2520algorithms%2520on%2520training%2520time%2520and%2520model%2520performance%2520has%2520been%2520thoroughly%2520explored%2520for%2520NNs%2520but%2520not%2520yet%2520for%2520GNNs.%2520We%2520analyze%2520two%2520different%2520batching%2520algorithms%2520for%2520graph%2520based%2520models%252C%2520namely%2520static%2520and%2520dynamic%2520batching%2520for%2520two%2520datasets%252C%2520the%2520QM9%2520dataset%2520of%2520small%2520molecules%2520and%2520the%2520AFLOW%2520materials%2520database.%2520Our%2520experiments%2520show%2520that%2520changing%2520the%2520batching%2520algorithm%2520can%2520provide%2520up%2520to%2520a%25202.7x%2520speedup%252C%2520but%2520the%2520fastest%2520algorithm%2520depends%2520on%2520the%2520data%252C%2520model%252C%2520batch%2520size%252C%2520hardware%252C%2520and%2520number%2520of%2520training%2520steps%2520run.%2520Experiments%2520show%2520that%2520for%2520a%2520select%2520number%2520of%2520combinations%2520of%2520batch%2520size%252C%2520dataset%252C%2520and%2520model%252C%2520significant%2520differences%2520in%2520model%2520learning%2520metrics%2520are%2520observed%2520between%2520static%2520and%2520dynamic%2520batching%2520algorithms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00944v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20speedups%20via%20batching%20for%20geometric%20learning%3A%20an%20analysis%20of%20static%20and%20dynamic%20algorithms&entry.906535625=Daniel%20T.%20Speckhard%20and%20Tim%20Bechtel%20and%20Sebastian%20Kehl%20and%20Jonathan%20Godwin%20and%20Claudia%20Draxl&entry.1292438233=Graph%20neural%20networks%20%28GNN%29%20have%20shown%20promising%20results%20for%20several%20domains%20such%20as%20materials%20science%2C%20chemistry%2C%20and%20the%20social%20sciences.%20GNN%20models%20often%20contain%20millions%20of%20parameters%2C%20and%20like%20other%20neural%20network%20%28NN%29%20models%2C%20are%20often%20fed%20only%20a%20fraction%20of%20the%20graphs%20that%20make%20up%20the%20training%20dataset%20in%20batches%20to%20update%20model%20parameters.%20The%20effect%20of%20batching%20algorithms%20on%20training%20time%20and%20model%20performance%20has%20been%20thoroughly%20explored%20for%20NNs%20but%20not%20yet%20for%20GNNs.%20We%20analyze%20two%20different%20batching%20algorithms%20for%20graph%20based%20models%2C%20namely%20static%20and%20dynamic%20batching%20for%20two%20datasets%2C%20the%20QM9%20dataset%20of%20small%20molecules%20and%20the%20AFLOW%20materials%20database.%20Our%20experiments%20show%20that%20changing%20the%20batching%20algorithm%20can%20provide%20up%20to%20a%202.7x%20speedup%2C%20but%20the%20fastest%20algorithm%20depends%20on%20the%20data%2C%20model%2C%20batch%20size%2C%20hardware%2C%20and%20number%20of%20training%20steps%20run.%20Experiments%20show%20that%20for%20a%20select%20number%20of%20combinations%20of%20batch%20size%2C%20dataset%2C%20and%20model%2C%20significant%20differences%20in%20model%20learning%20metrics%20are%20observed%20between%20static%20and%20dynamic%20batching%20algorithms.&entry.1838667208=http%3A//arxiv.org/abs/2502.00944v3&entry.124074799=Read"},
{"title": "Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning", "author": "Zheng Zhang", "abstract": "Large Language Models (LLMs) display striking surface fluency yet systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy, and logical consistency. This paper offers a structural diagnosis of such failures, revealing a persistent gap between \\textit{comprehension} and \\textit{competence}. Through controlled experiments and architectural analysis, we demonstrate that LLMs often articulate correct principles without reliably applying them--a failure rooted not in knowledge access, but in computational execution. We term this phenomenon the computational \\textit{split-brain syndrome}, where instruction and action pathways are geometrically and functionally dissociated. This core limitation recurs across domains, from mathematical operations to relational inferences, and explains why model behavior remains brittle even under idealized prompting. We argue that LLMs function as powerful pattern completion engines, but lack the architectural scaffolding for principled, compositional reasoning. Our findings delineate the boundary of current LLM capabilities and motivate future models with metacognitive control, principle lifting, and structurally grounded execution. This diagnosis also clarifies why mechanistic interpretability findings may reflect training-specific pattern coordination rather than universal computational principles, and why the geometric separation between instruction and execution pathways suggests limitations in neural introspection and mechanistic analysis.", "link": "http://arxiv.org/abs/2507.10624v3", "date": "2025-11-14", "relevancy": 2.0872, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5332}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5332}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comprehension%20Without%20Competence%3A%20Architectural%20Limits%20of%20LLMs%20in%20Symbolic%20Computation%20and%20Reasoning&body=Title%3A%20Comprehension%20Without%20Competence%3A%20Architectural%20Limits%20of%20LLMs%20in%20Symbolic%20Computation%20and%20Reasoning%0AAuthor%3A%20Zheng%20Zhang%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20display%20striking%20surface%20fluency%20yet%20systematically%20fail%20at%20tasks%20requiring%20symbolic%20reasoning%2C%20arithmetic%20accuracy%2C%20and%20logical%20consistency.%20This%20paper%20offers%20a%20structural%20diagnosis%20of%20such%20failures%2C%20revealing%20a%20persistent%20gap%20between%20%5Ctextit%7Bcomprehension%7D%20and%20%5Ctextit%7Bcompetence%7D.%20Through%20controlled%20experiments%20and%20architectural%20analysis%2C%20we%20demonstrate%20that%20LLMs%20often%20articulate%20correct%20principles%20without%20reliably%20applying%20them--a%20failure%20rooted%20not%20in%20knowledge%20access%2C%20but%20in%20computational%20execution.%20We%20term%20this%20phenomenon%20the%20computational%20%5Ctextit%7Bsplit-brain%20syndrome%7D%2C%20where%20instruction%20and%20action%20pathways%20are%20geometrically%20and%20functionally%20dissociated.%20This%20core%20limitation%20recurs%20across%20domains%2C%20from%20mathematical%20operations%20to%20relational%20inferences%2C%20and%20explains%20why%20model%20behavior%20remains%20brittle%20even%20under%20idealized%20prompting.%20We%20argue%20that%20LLMs%20function%20as%20powerful%20pattern%20completion%20engines%2C%20but%20lack%20the%20architectural%20scaffolding%20for%20principled%2C%20compositional%20reasoning.%20Our%20findings%20delineate%20the%20boundary%20of%20current%20LLM%20capabilities%20and%20motivate%20future%20models%20with%20metacognitive%20control%2C%20principle%20lifting%2C%20and%20structurally%20grounded%20execution.%20This%20diagnosis%20also%20clarifies%20why%20mechanistic%20interpretability%20findings%20may%20reflect%20training-specific%20pattern%20coordination%20rather%20than%20universal%20computational%20principles%2C%20and%20why%20the%20geometric%20separation%20between%20instruction%20and%20execution%20pathways%20suggests%20limitations%20in%20neural%20introspection%20and%20mechanistic%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2507.10624v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComprehension%2520Without%2520Competence%253A%2520Architectural%2520Limits%2520of%2520LLMs%2520in%2520Symbolic%2520Computation%2520and%2520Reasoning%26entry.906535625%3DZheng%2520Zhang%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520display%2520striking%2520surface%2520fluency%2520yet%2520systematically%2520fail%2520at%2520tasks%2520requiring%2520symbolic%2520reasoning%252C%2520arithmetic%2520accuracy%252C%2520and%2520logical%2520consistency.%2520This%2520paper%2520offers%2520a%2520structural%2520diagnosis%2520of%2520such%2520failures%252C%2520revealing%2520a%2520persistent%2520gap%2520between%2520%255Ctextit%257Bcomprehension%257D%2520and%2520%255Ctextit%257Bcompetence%257D.%2520Through%2520controlled%2520experiments%2520and%2520architectural%2520analysis%252C%2520we%2520demonstrate%2520that%2520LLMs%2520often%2520articulate%2520correct%2520principles%2520without%2520reliably%2520applying%2520them--a%2520failure%2520rooted%2520not%2520in%2520knowledge%2520access%252C%2520but%2520in%2520computational%2520execution.%2520We%2520term%2520this%2520phenomenon%2520the%2520computational%2520%255Ctextit%257Bsplit-brain%2520syndrome%257D%252C%2520where%2520instruction%2520and%2520action%2520pathways%2520are%2520geometrically%2520and%2520functionally%2520dissociated.%2520This%2520core%2520limitation%2520recurs%2520across%2520domains%252C%2520from%2520mathematical%2520operations%2520to%2520relational%2520inferences%252C%2520and%2520explains%2520why%2520model%2520behavior%2520remains%2520brittle%2520even%2520under%2520idealized%2520prompting.%2520We%2520argue%2520that%2520LLMs%2520function%2520as%2520powerful%2520pattern%2520completion%2520engines%252C%2520but%2520lack%2520the%2520architectural%2520scaffolding%2520for%2520principled%252C%2520compositional%2520reasoning.%2520Our%2520findings%2520delineate%2520the%2520boundary%2520of%2520current%2520LLM%2520capabilities%2520and%2520motivate%2520future%2520models%2520with%2520metacognitive%2520control%252C%2520principle%2520lifting%252C%2520and%2520structurally%2520grounded%2520execution.%2520This%2520diagnosis%2520also%2520clarifies%2520why%2520mechanistic%2520interpretability%2520findings%2520may%2520reflect%2520training-specific%2520pattern%2520coordination%2520rather%2520than%2520universal%2520computational%2520principles%252C%2520and%2520why%2520the%2520geometric%2520separation%2520between%2520instruction%2520and%2520execution%2520pathways%2520suggests%2520limitations%2520in%2520neural%2520introspection%2520and%2520mechanistic%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10624v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comprehension%20Without%20Competence%3A%20Architectural%20Limits%20of%20LLMs%20in%20Symbolic%20Computation%20and%20Reasoning&entry.906535625=Zheng%20Zhang&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20display%20striking%20surface%20fluency%20yet%20systematically%20fail%20at%20tasks%20requiring%20symbolic%20reasoning%2C%20arithmetic%20accuracy%2C%20and%20logical%20consistency.%20This%20paper%20offers%20a%20structural%20diagnosis%20of%20such%20failures%2C%20revealing%20a%20persistent%20gap%20between%20%5Ctextit%7Bcomprehension%7D%20and%20%5Ctextit%7Bcompetence%7D.%20Through%20controlled%20experiments%20and%20architectural%20analysis%2C%20we%20demonstrate%20that%20LLMs%20often%20articulate%20correct%20principles%20without%20reliably%20applying%20them--a%20failure%20rooted%20not%20in%20knowledge%20access%2C%20but%20in%20computational%20execution.%20We%20term%20this%20phenomenon%20the%20computational%20%5Ctextit%7Bsplit-brain%20syndrome%7D%2C%20where%20instruction%20and%20action%20pathways%20are%20geometrically%20and%20functionally%20dissociated.%20This%20core%20limitation%20recurs%20across%20domains%2C%20from%20mathematical%20operations%20to%20relational%20inferences%2C%20and%20explains%20why%20model%20behavior%20remains%20brittle%20even%20under%20idealized%20prompting.%20We%20argue%20that%20LLMs%20function%20as%20powerful%20pattern%20completion%20engines%2C%20but%20lack%20the%20architectural%20scaffolding%20for%20principled%2C%20compositional%20reasoning.%20Our%20findings%20delineate%20the%20boundary%20of%20current%20LLM%20capabilities%20and%20motivate%20future%20models%20with%20metacognitive%20control%2C%20principle%20lifting%2C%20and%20structurally%20grounded%20execution.%20This%20diagnosis%20also%20clarifies%20why%20mechanistic%20interpretability%20findings%20may%20reflect%20training-specific%20pattern%20coordination%20rather%20than%20universal%20computational%20principles%2C%20and%20why%20the%20geometric%20separation%20between%20instruction%20and%20execution%20pathways%20suggests%20limitations%20in%20neural%20introspection%20and%20mechanistic%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2507.10624v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


