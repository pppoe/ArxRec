<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251208.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects", "author": "Shuohan Tao and Boyao Zhou and Hanzhang Tu and Yuwang Wang and Yebin Liu", "abstract": "3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.", "link": "http://arxiv.org/abs/2512.07381v1", "date": "2025-12-08", "relevancy": 3.7237, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7918}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7212}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tessellation%20GS%3A%20Neural%20Mesh%20Gaussians%20for%20Robust%20Monocular%20Reconstruction%20of%20Dynamic%20Objects&body=Title%3A%20Tessellation%20GS%3A%20Neural%20Mesh%20Gaussians%20for%20Robust%20Monocular%20Reconstruction%20of%20Dynamic%20Objects%0AAuthor%3A%20Shuohan%20Tao%20and%20Boyao%20Zhou%20and%20Hanzhang%20Tu%20and%20Yuwang%20Wang%20and%20Yebin%20Liu%0AAbstract%3A%203D%20Gaussian%20Splatting%20%28GS%29%20enables%20highly%20photorealistic%20scene%20reconstruction%20from%20posed%20image%20sequences%20but%20struggles%20with%20viewpoint%20extrapolation%20due%20to%20its%20anisotropic%20nature%2C%20leading%20to%20overfitting%20and%20poor%20generalization%2C%20particularly%20in%20sparse-view%20and%20dynamic%20scene%20reconstruction.%20We%20propose%20Tessellation%20GS%2C%20a%20structured%202D%20GS%20approach%20anchored%20on%20mesh%20faces%2C%20to%20reconstruct%20dynamic%20scenes%20from%20a%20single%20continuously%20moving%20or%20static%20camera.%20Our%20method%20constrains%202D%20Gaussians%20to%20localized%20regions%20and%20infers%20their%20attributes%20via%20hierarchical%20neural%20features%20on%20mesh%20faces.%20Gaussian%20subdivision%20is%20guided%20by%20an%20adaptive%20face%20subdivision%20strategy%20driven%20by%20a%20detail-aware%20loss%20function.%20Additionally%2C%20we%20leverage%20priors%20from%20a%20reconstruction%20foundation%20model%20to%20initialize%20Gaussian%20deformations%2C%20enabling%20robust%20reconstruction%20of%20general%20dynamic%20objects%20from%20a%20single%20static%20camera%2C%20previously%20extremely%20challenging%20for%20optimization-based%20methods.%20Our%20method%20outperforms%20previous%20SOTA%20method%2C%20reducing%20LPIPS%20by%2029.1%25%20and%20Chamfer%20distance%20by%2049.2%25%20on%20appearance%20and%20mesh%20reconstruction%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTessellation%2520GS%253A%2520Neural%2520Mesh%2520Gaussians%2520for%2520Robust%2520Monocular%2520Reconstruction%2520of%2520Dynamic%2520Objects%26entry.906535625%3DShuohan%2520Tao%2520and%2520Boyao%2520Zhou%2520and%2520Hanzhang%2520Tu%2520and%2520Yuwang%2520Wang%2520and%2520Yebin%2520Liu%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520%2528GS%2529%2520enables%2520highly%2520photorealistic%2520scene%2520reconstruction%2520from%2520posed%2520image%2520sequences%2520but%2520struggles%2520with%2520viewpoint%2520extrapolation%2520due%2520to%2520its%2520anisotropic%2520nature%252C%2520leading%2520to%2520overfitting%2520and%2520poor%2520generalization%252C%2520particularly%2520in%2520sparse-view%2520and%2520dynamic%2520scene%2520reconstruction.%2520We%2520propose%2520Tessellation%2520GS%252C%2520a%2520structured%25202D%2520GS%2520approach%2520anchored%2520on%2520mesh%2520faces%252C%2520to%2520reconstruct%2520dynamic%2520scenes%2520from%2520a%2520single%2520continuously%2520moving%2520or%2520static%2520camera.%2520Our%2520method%2520constrains%25202D%2520Gaussians%2520to%2520localized%2520regions%2520and%2520infers%2520their%2520attributes%2520via%2520hierarchical%2520neural%2520features%2520on%2520mesh%2520faces.%2520Gaussian%2520subdivision%2520is%2520guided%2520by%2520an%2520adaptive%2520face%2520subdivision%2520strategy%2520driven%2520by%2520a%2520detail-aware%2520loss%2520function.%2520Additionally%252C%2520we%2520leverage%2520priors%2520from%2520a%2520reconstruction%2520foundation%2520model%2520to%2520initialize%2520Gaussian%2520deformations%252C%2520enabling%2520robust%2520reconstruction%2520of%2520general%2520dynamic%2520objects%2520from%2520a%2520single%2520static%2520camera%252C%2520previously%2520extremely%2520challenging%2520for%2520optimization-based%2520methods.%2520Our%2520method%2520outperforms%2520previous%2520SOTA%2520method%252C%2520reducing%2520LPIPS%2520by%252029.1%2525%2520and%2520Chamfer%2520distance%2520by%252049.2%2525%2520on%2520appearance%2520and%2520mesh%2520reconstruction%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tessellation%20GS%3A%20Neural%20Mesh%20Gaussians%20for%20Robust%20Monocular%20Reconstruction%20of%20Dynamic%20Objects&entry.906535625=Shuohan%20Tao%20and%20Boyao%20Zhou%20and%20Hanzhang%20Tu%20and%20Yuwang%20Wang%20and%20Yebin%20Liu&entry.1292438233=3D%20Gaussian%20Splatting%20%28GS%29%20enables%20highly%20photorealistic%20scene%20reconstruction%20from%20posed%20image%20sequences%20but%20struggles%20with%20viewpoint%20extrapolation%20due%20to%20its%20anisotropic%20nature%2C%20leading%20to%20overfitting%20and%20poor%20generalization%2C%20particularly%20in%20sparse-view%20and%20dynamic%20scene%20reconstruction.%20We%20propose%20Tessellation%20GS%2C%20a%20structured%202D%20GS%20approach%20anchored%20on%20mesh%20faces%2C%20to%20reconstruct%20dynamic%20scenes%20from%20a%20single%20continuously%20moving%20or%20static%20camera.%20Our%20method%20constrains%202D%20Gaussians%20to%20localized%20regions%20and%20infers%20their%20attributes%20via%20hierarchical%20neural%20features%20on%20mesh%20faces.%20Gaussian%20subdivision%20is%20guided%20by%20an%20adaptive%20face%20subdivision%20strategy%20driven%20by%20a%20detail-aware%20loss%20function.%20Additionally%2C%20we%20leverage%20priors%20from%20a%20reconstruction%20foundation%20model%20to%20initialize%20Gaussian%20deformations%2C%20enabling%20robust%20reconstruction%20of%20general%20dynamic%20objects%20from%20a%20single%20static%20camera%2C%20previously%20extremely%20challenging%20for%20optimization-based%20methods.%20Our%20method%20outperforms%20previous%20SOTA%20method%2C%20reducing%20LPIPS%20by%2029.1%25%20and%20Chamfer%20distance%20by%2049.2%25%20on%20appearance%20and%20mesh%20reconstruction%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.07381v1&entry.124074799=Read"},
{"title": "ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation", "author": "Fan Yang and Heyuan Li and Peihao Li and Weihao Yuan and Lingteng Qiu and Chaoyue Song and Cheng Chen and Yisheng He and Shifeng Zhang and Xiaoguang Han and Steven Hoi and Guosheng Lin", "abstract": "Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa", "link": "http://arxiv.org/abs/2512.07720v1", "date": "2025-12-08", "relevancy": 3.353, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6903}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6608}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViSA%3A%203D-Aware%20Video%20Shading%20for%20Real-Time%20Upper-Body%20Avatar%20Creation&body=Title%3A%20ViSA%3A%203D-Aware%20Video%20Shading%20for%20Real-Time%20Upper-Body%20Avatar%20Creation%0AAuthor%3A%20Fan%20Yang%20and%20Heyuan%20Li%20and%20Peihao%20Li%20and%20Weihao%20Yuan%20and%20Lingteng%20Qiu%20and%20Chaoyue%20Song%20and%20Cheng%20Chen%20and%20Yisheng%20He%20and%20Shifeng%20Zhang%20and%20Xiaoguang%20Han%20and%20Steven%20Hoi%20and%20Guosheng%20Lin%0AAbstract%3A%20Generating%20high-fidelity%20upper-body%203D%20avatars%20from%20one-shot%20input%20image%20remains%20a%20significant%20challenge.%20Current%203D%20avatar%20generation%20methods%2C%20which%20rely%20on%20large%20reconstruction%20models%2C%20are%20fast%20and%20capable%20of%20producing%20stable%20body%20structures%2C%20but%20they%20often%20suffer%20from%20artifacts%20such%20as%20blurry%20textures%20and%20stiff%2C%20unnatural%20motion.%20In%20contrast%2C%20generative%20video%20models%20show%20promising%20performance%20by%20synthesizing%20photorealistic%20and%20dynamic%20results%2C%20but%20they%20frequently%20struggle%20with%20unstable%20behavior%2C%20including%20body%20structural%20errors%20and%20identity%20drift.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20approach%20that%20combines%20the%20strengths%20of%20both%20paradigms.%20Our%20framework%20employs%20a%203D%20reconstruction%20model%20to%20provide%20robust%20structural%20and%20appearance%20priors%2C%20which%20in%20turn%20guides%20a%20real-time%20autoregressive%20video%20diffusion%20model%20for%20rendering.%20This%20process%20enables%20the%20model%20to%20synthesize%20high-frequency%2C%20photorealistic%20details%20and%20fluid%20dynamics%20in%20real%20time%2C%20effectively%20reducing%20texture%20blur%20and%20motion%20stiffness%20while%20preventing%20the%20structural%20inconsistencies%20common%20in%20video%20generation%20methods.%20By%20uniting%20the%20geometric%20stability%20of%203D%20reconstruction%20with%20the%20generative%20capabilities%20of%20video%20models%2C%20our%20method%20produces%20high-fidelity%20digital%20avatars%20with%20realistic%20appearance%20and%20dynamic%2C%20temporally%20coherent%20motion.%20Experiments%20demonstrate%20that%20our%20approach%20significantly%20reduces%20artifacts%20and%20achieves%20substantial%20improvements%20in%20visual%20quality%20over%20leading%20methods%2C%20providing%20a%20robust%20and%20efficient%20solution%20for%20real-time%20applications%20such%20as%20gaming%20and%20virtual%20reality.%20Project%20page%3A%20https%3A//lhyfst.github.io/visa%0ALink%3A%20http%3A//arxiv.org/abs/2512.07720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViSA%253A%25203D-Aware%2520Video%2520Shading%2520for%2520Real-Time%2520Upper-Body%2520Avatar%2520Creation%26entry.906535625%3DFan%2520Yang%2520and%2520Heyuan%2520Li%2520and%2520Peihao%2520Li%2520and%2520Weihao%2520Yuan%2520and%2520Lingteng%2520Qiu%2520and%2520Chaoyue%2520Song%2520and%2520Cheng%2520Chen%2520and%2520Yisheng%2520He%2520and%2520Shifeng%2520Zhang%2520and%2520Xiaoguang%2520Han%2520and%2520Steven%2520Hoi%2520and%2520Guosheng%2520Lin%26entry.1292438233%3DGenerating%2520high-fidelity%2520upper-body%25203D%2520avatars%2520from%2520one-shot%2520input%2520image%2520remains%2520a%2520significant%2520challenge.%2520Current%25203D%2520avatar%2520generation%2520methods%252C%2520which%2520rely%2520on%2520large%2520reconstruction%2520models%252C%2520are%2520fast%2520and%2520capable%2520of%2520producing%2520stable%2520body%2520structures%252C%2520but%2520they%2520often%2520suffer%2520from%2520artifacts%2520such%2520as%2520blurry%2520textures%2520and%2520stiff%252C%2520unnatural%2520motion.%2520In%2520contrast%252C%2520generative%2520video%2520models%2520show%2520promising%2520performance%2520by%2520synthesizing%2520photorealistic%2520and%2520dynamic%2520results%252C%2520but%2520they%2520frequently%2520struggle%2520with%2520unstable%2520behavior%252C%2520including%2520body%2520structural%2520errors%2520and%2520identity%2520drift.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520combines%2520the%2520strengths%2520of%2520both%2520paradigms.%2520Our%2520framework%2520employs%2520a%25203D%2520reconstruction%2520model%2520to%2520provide%2520robust%2520structural%2520and%2520appearance%2520priors%252C%2520which%2520in%2520turn%2520guides%2520a%2520real-time%2520autoregressive%2520video%2520diffusion%2520model%2520for%2520rendering.%2520This%2520process%2520enables%2520the%2520model%2520to%2520synthesize%2520high-frequency%252C%2520photorealistic%2520details%2520and%2520fluid%2520dynamics%2520in%2520real%2520time%252C%2520effectively%2520reducing%2520texture%2520blur%2520and%2520motion%2520stiffness%2520while%2520preventing%2520the%2520structural%2520inconsistencies%2520common%2520in%2520video%2520generation%2520methods.%2520By%2520uniting%2520the%2520geometric%2520stability%2520of%25203D%2520reconstruction%2520with%2520the%2520generative%2520capabilities%2520of%2520video%2520models%252C%2520our%2520method%2520produces%2520high-fidelity%2520digital%2520avatars%2520with%2520realistic%2520appearance%2520and%2520dynamic%252C%2520temporally%2520coherent%2520motion.%2520Experiments%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520reduces%2520artifacts%2520and%2520achieves%2520substantial%2520improvements%2520in%2520visual%2520quality%2520over%2520leading%2520methods%252C%2520providing%2520a%2520robust%2520and%2520efficient%2520solution%2520for%2520real-time%2520applications%2520such%2520as%2520gaming%2520and%2520virtual%2520reality.%2520Project%2520page%253A%2520https%253A//lhyfst.github.io/visa%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViSA%3A%203D-Aware%20Video%20Shading%20for%20Real-Time%20Upper-Body%20Avatar%20Creation&entry.906535625=Fan%20Yang%20and%20Heyuan%20Li%20and%20Peihao%20Li%20and%20Weihao%20Yuan%20and%20Lingteng%20Qiu%20and%20Chaoyue%20Song%20and%20Cheng%20Chen%20and%20Yisheng%20He%20and%20Shifeng%20Zhang%20and%20Xiaoguang%20Han%20and%20Steven%20Hoi%20and%20Guosheng%20Lin&entry.1292438233=Generating%20high-fidelity%20upper-body%203D%20avatars%20from%20one-shot%20input%20image%20remains%20a%20significant%20challenge.%20Current%203D%20avatar%20generation%20methods%2C%20which%20rely%20on%20large%20reconstruction%20models%2C%20are%20fast%20and%20capable%20of%20producing%20stable%20body%20structures%2C%20but%20they%20often%20suffer%20from%20artifacts%20such%20as%20blurry%20textures%20and%20stiff%2C%20unnatural%20motion.%20In%20contrast%2C%20generative%20video%20models%20show%20promising%20performance%20by%20synthesizing%20photorealistic%20and%20dynamic%20results%2C%20but%20they%20frequently%20struggle%20with%20unstable%20behavior%2C%20including%20body%20structural%20errors%20and%20identity%20drift.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20approach%20that%20combines%20the%20strengths%20of%20both%20paradigms.%20Our%20framework%20employs%20a%203D%20reconstruction%20model%20to%20provide%20robust%20structural%20and%20appearance%20priors%2C%20which%20in%20turn%20guides%20a%20real-time%20autoregressive%20video%20diffusion%20model%20for%20rendering.%20This%20process%20enables%20the%20model%20to%20synthesize%20high-frequency%2C%20photorealistic%20details%20and%20fluid%20dynamics%20in%20real%20time%2C%20effectively%20reducing%20texture%20blur%20and%20motion%20stiffness%20while%20preventing%20the%20structural%20inconsistencies%20common%20in%20video%20generation%20methods.%20By%20uniting%20the%20geometric%20stability%20of%203D%20reconstruction%20with%20the%20generative%20capabilities%20of%20video%20models%2C%20our%20method%20produces%20high-fidelity%20digital%20avatars%20with%20realistic%20appearance%20and%20dynamic%2C%20temporally%20coherent%20motion.%20Experiments%20demonstrate%20that%20our%20approach%20significantly%20reduces%20artifacts%20and%20achieves%20substantial%20improvements%20in%20visual%20quality%20over%20leading%20methods%2C%20providing%20a%20robust%20and%20efficient%20solution%20for%20real-time%20applications%20such%20as%20gaming%20and%20virtual%20reality.%20Project%20page%3A%20https%3A//lhyfst.github.io/visa&entry.1838667208=http%3A//arxiv.org/abs/2512.07720v1&entry.124074799=Read"},
{"title": "Human Geometry Distribution for 3D Animation Generation", "author": "Xiangjun Tang and Biao Zhang and Peter Wonka", "abstract": "Generating realistic human geometry animations remains a challenging task, as it requires modeling natural clothing dynamics with fine-grained geometric details under limited data. To address these challenges, we propose two novel designs. First, we propose a compact distribution-based latent representation that enables efficient and high-quality geometry generation. We improve upon previous work by establishing a more uniform mapping between SMPL and avatar geometries. Second, we introduce a generative animation model that fully exploits the diversity of limited motion data. We focus on short-term transitions while maintaining long-term consistency through an identity-conditioned design. These two designs formulate our method as a two-stage framework: the first stage learns a latent space, while the second learns to generate animations within this latent space. We conducted experiments on both our latent space and animation model. We demonstrate that our latent space produces high-fidelity human geometry surpassing previous methods ($90\\%$ lower Chamfer Dist.). The animation model synthesizes diverse animations with detailed and natural dynamics ($2.2 \\times$ higher user study score), achieving the best results across all evaluation metrics.", "link": "http://arxiv.org/abs/2512.07459v1", "date": "2025-12-08", "relevancy": 3.2311, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.708}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6264}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Geometry%20Distribution%20for%203D%20Animation%20Generation&body=Title%3A%20Human%20Geometry%20Distribution%20for%203D%20Animation%20Generation%0AAuthor%3A%20Xiangjun%20Tang%20and%20Biao%20Zhang%20and%20Peter%20Wonka%0AAbstract%3A%20Generating%20realistic%20human%20geometry%20animations%20remains%20a%20challenging%20task%2C%20as%20it%20requires%20modeling%20natural%20clothing%20dynamics%20with%20fine-grained%20geometric%20details%20under%20limited%20data.%20To%20address%20these%20challenges%2C%20we%20propose%20two%20novel%20designs.%20First%2C%20we%20propose%20a%20compact%20distribution-based%20latent%20representation%20that%20enables%20efficient%20and%20high-quality%20geometry%20generation.%20We%20improve%20upon%20previous%20work%20by%20establishing%20a%20more%20uniform%20mapping%20between%20SMPL%20and%20avatar%20geometries.%20Second%2C%20we%20introduce%20a%20generative%20animation%20model%20that%20fully%20exploits%20the%20diversity%20of%20limited%20motion%20data.%20We%20focus%20on%20short-term%20transitions%20while%20maintaining%20long-term%20consistency%20through%20an%20identity-conditioned%20design.%20These%20two%20designs%20formulate%20our%20method%20as%20a%20two-stage%20framework%3A%20the%20first%20stage%20learns%20a%20latent%20space%2C%20while%20the%20second%20learns%20to%20generate%20animations%20within%20this%20latent%20space.%20We%20conducted%20experiments%20on%20both%20our%20latent%20space%20and%20animation%20model.%20We%20demonstrate%20that%20our%20latent%20space%20produces%20high-fidelity%20human%20geometry%20surpassing%20previous%20methods%20%28%2490%5C%25%24%20lower%20Chamfer%20Dist.%29.%20The%20animation%20model%20synthesizes%20diverse%20animations%20with%20detailed%20and%20natural%20dynamics%20%28%242.2%20%5Ctimes%24%20higher%20user%20study%20score%29%2C%20achieving%20the%20best%20results%20across%20all%20evaluation%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Geometry%2520Distribution%2520for%25203D%2520Animation%2520Generation%26entry.906535625%3DXiangjun%2520Tang%2520and%2520Biao%2520Zhang%2520and%2520Peter%2520Wonka%26entry.1292438233%3DGenerating%2520realistic%2520human%2520geometry%2520animations%2520remains%2520a%2520challenging%2520task%252C%2520as%2520it%2520requires%2520modeling%2520natural%2520clothing%2520dynamics%2520with%2520fine-grained%2520geometric%2520details%2520under%2520limited%2520data.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520two%2520novel%2520designs.%2520First%252C%2520we%2520propose%2520a%2520compact%2520distribution-based%2520latent%2520representation%2520that%2520enables%2520efficient%2520and%2520high-quality%2520geometry%2520generation.%2520We%2520improve%2520upon%2520previous%2520work%2520by%2520establishing%2520a%2520more%2520uniform%2520mapping%2520between%2520SMPL%2520and%2520avatar%2520geometries.%2520Second%252C%2520we%2520introduce%2520a%2520generative%2520animation%2520model%2520that%2520fully%2520exploits%2520the%2520diversity%2520of%2520limited%2520motion%2520data.%2520We%2520focus%2520on%2520short-term%2520transitions%2520while%2520maintaining%2520long-term%2520consistency%2520through%2520an%2520identity-conditioned%2520design.%2520These%2520two%2520designs%2520formulate%2520our%2520method%2520as%2520a%2520two-stage%2520framework%253A%2520the%2520first%2520stage%2520learns%2520a%2520latent%2520space%252C%2520while%2520the%2520second%2520learns%2520to%2520generate%2520animations%2520within%2520this%2520latent%2520space.%2520We%2520conducted%2520experiments%2520on%2520both%2520our%2520latent%2520space%2520and%2520animation%2520model.%2520We%2520demonstrate%2520that%2520our%2520latent%2520space%2520produces%2520high-fidelity%2520human%2520geometry%2520surpassing%2520previous%2520methods%2520%2528%252490%255C%2525%2524%2520lower%2520Chamfer%2520Dist.%2529.%2520The%2520animation%2520model%2520synthesizes%2520diverse%2520animations%2520with%2520detailed%2520and%2520natural%2520dynamics%2520%2528%25242.2%2520%255Ctimes%2524%2520higher%2520user%2520study%2520score%2529%252C%2520achieving%2520the%2520best%2520results%2520across%2520all%2520evaluation%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Geometry%20Distribution%20for%203D%20Animation%20Generation&entry.906535625=Xiangjun%20Tang%20and%20Biao%20Zhang%20and%20Peter%20Wonka&entry.1292438233=Generating%20realistic%20human%20geometry%20animations%20remains%20a%20challenging%20task%2C%20as%20it%20requires%20modeling%20natural%20clothing%20dynamics%20with%20fine-grained%20geometric%20details%20under%20limited%20data.%20To%20address%20these%20challenges%2C%20we%20propose%20two%20novel%20designs.%20First%2C%20we%20propose%20a%20compact%20distribution-based%20latent%20representation%20that%20enables%20efficient%20and%20high-quality%20geometry%20generation.%20We%20improve%20upon%20previous%20work%20by%20establishing%20a%20more%20uniform%20mapping%20between%20SMPL%20and%20avatar%20geometries.%20Second%2C%20we%20introduce%20a%20generative%20animation%20model%20that%20fully%20exploits%20the%20diversity%20of%20limited%20motion%20data.%20We%20focus%20on%20short-term%20transitions%20while%20maintaining%20long-term%20consistency%20through%20an%20identity-conditioned%20design.%20These%20two%20designs%20formulate%20our%20method%20as%20a%20two-stage%20framework%3A%20the%20first%20stage%20learns%20a%20latent%20space%2C%20while%20the%20second%20learns%20to%20generate%20animations%20within%20this%20latent%20space.%20We%20conducted%20experiments%20on%20both%20our%20latent%20space%20and%20animation%20model.%20We%20demonstrate%20that%20our%20latent%20space%20produces%20high-fidelity%20human%20geometry%20surpassing%20previous%20methods%20%28%2490%5C%25%24%20lower%20Chamfer%20Dist.%29.%20The%20animation%20model%20synthesizes%20diverse%20animations%20with%20detailed%20and%20natural%20dynamics%20%28%242.2%20%5Ctimes%24%20higher%20user%20study%20score%29%2C%20achieving%20the%20best%20results%20across%20all%20evaluation%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2512.07459v1&entry.124074799=Read"},
{"title": "Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes", "author": "Shai Krakovsky and Gal Fiebelman and Sagie Benaim and Hadar Averbuch-Elor", "abstract": "Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.", "link": "http://arxiv.org/abs/2512.07807v1", "date": "2025-12-08", "relevancy": 3.1204, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6259}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6232}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lang3D-XL%3A%20Language%20Embedded%203D%20Gaussians%20for%20Large-scale%20Scenes&body=Title%3A%20Lang3D-XL%3A%20Language%20Embedded%203D%20Gaussians%20for%20Large-scale%20Scenes%0AAuthor%3A%20Shai%20Krakovsky%20and%20Gal%20Fiebelman%20and%20Sagie%20Benaim%20and%20Hadar%20Averbuch-Elor%0AAbstract%3A%20Embedding%20a%20language%20field%20in%20a%203D%20representation%20enables%20richer%20semantic%20understanding%20of%20spatial%20environments%20by%20linking%20geometry%20with%20descriptive%20meaning.%20This%20allows%20for%20a%20more%20intuitive%20human-computer%20interaction%2C%20enabling%20querying%20or%20editing%20scenes%20using%20natural%20language%2C%20and%20could%20potentially%20improve%20tasks%20like%20scene%20retrieval%2C%20navigation%2C%20and%20multimodal%20reasoning.%20While%20such%20capabilities%20could%20be%20transformative%2C%20in%20particular%20for%20large-scale%20scenes%2C%20we%20find%20that%20recent%20feature%20distillation%20approaches%20cannot%20effectively%20learn%20over%20massive%20Internet%20data%20due%20to%20challenges%20in%20semantic%20feature%20misalignment%20and%20inefficiency%20in%20memory%20and%20runtime.%20To%20this%20end%2C%20we%20propose%20a%20novel%20approach%20to%20address%20these%20challenges.%20First%2C%20we%20introduce%20extremely%20low-dimensional%20semantic%20bottleneck%20features%20as%20part%20of%20the%20underlying%203D%20Gaussian%20representation.%20These%20are%20processed%20by%20rendering%20and%20passing%20them%20through%20a%20multi-resolution%2C%20feature-based%2C%20hash%20encoder.%20This%20significantly%20improves%20efficiency%20both%20in%20runtime%20and%20GPU%20memory.%20Second%2C%20we%20introduce%20an%20Attenuated%20Downsampler%20module%20and%20propose%20several%20regularizations%20addressing%20the%20semantic%20misalignment%20of%20ground%20truth%202D%20features.%20We%20evaluate%20our%20method%20on%20the%20in-the-wild%20HolyScenes%20dataset%20and%20demonstrate%20that%20it%20surpasses%20existing%20approaches%20in%20both%20performance%20and%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLang3D-XL%253A%2520Language%2520Embedded%25203D%2520Gaussians%2520for%2520Large-scale%2520Scenes%26entry.906535625%3DShai%2520Krakovsky%2520and%2520Gal%2520Fiebelman%2520and%2520Sagie%2520Benaim%2520and%2520Hadar%2520Averbuch-Elor%26entry.1292438233%3DEmbedding%2520a%2520language%2520field%2520in%2520a%25203D%2520representation%2520enables%2520richer%2520semantic%2520understanding%2520of%2520spatial%2520environments%2520by%2520linking%2520geometry%2520with%2520descriptive%2520meaning.%2520This%2520allows%2520for%2520a%2520more%2520intuitive%2520human-computer%2520interaction%252C%2520enabling%2520querying%2520or%2520editing%2520scenes%2520using%2520natural%2520language%252C%2520and%2520could%2520potentially%2520improve%2520tasks%2520like%2520scene%2520retrieval%252C%2520navigation%252C%2520and%2520multimodal%2520reasoning.%2520While%2520such%2520capabilities%2520could%2520be%2520transformative%252C%2520in%2520particular%2520for%2520large-scale%2520scenes%252C%2520we%2520find%2520that%2520recent%2520feature%2520distillation%2520approaches%2520cannot%2520effectively%2520learn%2520over%2520massive%2520Internet%2520data%2520due%2520to%2520challenges%2520in%2520semantic%2520feature%2520misalignment%2520and%2520inefficiency%2520in%2520memory%2520and%2520runtime.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520approach%2520to%2520address%2520these%2520challenges.%2520First%252C%2520we%2520introduce%2520extremely%2520low-dimensional%2520semantic%2520bottleneck%2520features%2520as%2520part%2520of%2520the%2520underlying%25203D%2520Gaussian%2520representation.%2520These%2520are%2520processed%2520by%2520rendering%2520and%2520passing%2520them%2520through%2520a%2520multi-resolution%252C%2520feature-based%252C%2520hash%2520encoder.%2520This%2520significantly%2520improves%2520efficiency%2520both%2520in%2520runtime%2520and%2520GPU%2520memory.%2520Second%252C%2520we%2520introduce%2520an%2520Attenuated%2520Downsampler%2520module%2520and%2520propose%2520several%2520regularizations%2520addressing%2520the%2520semantic%2520misalignment%2520of%2520ground%2520truth%25202D%2520features.%2520We%2520evaluate%2520our%2520method%2520on%2520the%2520in-the-wild%2520HolyScenes%2520dataset%2520and%2520demonstrate%2520that%2520it%2520surpasses%2520existing%2520approaches%2520in%2520both%2520performance%2520and%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lang3D-XL%3A%20Language%20Embedded%203D%20Gaussians%20for%20Large-scale%20Scenes&entry.906535625=Shai%20Krakovsky%20and%20Gal%20Fiebelman%20and%20Sagie%20Benaim%20and%20Hadar%20Averbuch-Elor&entry.1292438233=Embedding%20a%20language%20field%20in%20a%203D%20representation%20enables%20richer%20semantic%20understanding%20of%20spatial%20environments%20by%20linking%20geometry%20with%20descriptive%20meaning.%20This%20allows%20for%20a%20more%20intuitive%20human-computer%20interaction%2C%20enabling%20querying%20or%20editing%20scenes%20using%20natural%20language%2C%20and%20could%20potentially%20improve%20tasks%20like%20scene%20retrieval%2C%20navigation%2C%20and%20multimodal%20reasoning.%20While%20such%20capabilities%20could%20be%20transformative%2C%20in%20particular%20for%20large-scale%20scenes%2C%20we%20find%20that%20recent%20feature%20distillation%20approaches%20cannot%20effectively%20learn%20over%20massive%20Internet%20data%20due%20to%20challenges%20in%20semantic%20feature%20misalignment%20and%20inefficiency%20in%20memory%20and%20runtime.%20To%20this%20end%2C%20we%20propose%20a%20novel%20approach%20to%20address%20these%20challenges.%20First%2C%20we%20introduce%20extremely%20low-dimensional%20semantic%20bottleneck%20features%20as%20part%20of%20the%20underlying%203D%20Gaussian%20representation.%20These%20are%20processed%20by%20rendering%20and%20passing%20them%20through%20a%20multi-resolution%2C%20feature-based%2C%20hash%20encoder.%20This%20significantly%20improves%20efficiency%20both%20in%20runtime%20and%20GPU%20memory.%20Second%2C%20we%20introduce%20an%20Attenuated%20Downsampler%20module%20and%20propose%20several%20regularizations%20addressing%20the%20semantic%20misalignment%20of%20ground%20truth%202D%20features.%20We%20evaluate%20our%20method%20on%20the%20in-the-wild%20HolyScenes%20dataset%20and%20demonstrate%20that%20it%20surpasses%20existing%20approaches%20in%20both%20performance%20and%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2512.07807v1&entry.124074799=Read"},
{"title": "TranSplat: Instant Cross-Scene Object Relighting in Gaussian Splatting via Spherical Harmonic Transfer", "author": " Boyang and  Yu and Yanlin Jin and Yun He and Akshat Dave and Guha Balakrishnan", "abstract": "We present TranSplat, a method for fast and accurate object relighting for the 3D Gaussian Splatting (GS) framework when transferring a 3D object from a source GS scene to a target GS scene. TranSplat is based on a theoretical radiance transfer identity for cross-scene relighting of objects with radially symmetric BRDFs that involves only taking simple products of spherical harmonic appearance coefficients of the object, source, and target environment maps without any explicit computation of scene quantities (e.g., the BRDFs themselves). TranSplat is the first method to demonstrate how this theoretical identity may be used to perform relighting within the GS framework, and furthermore, by automatically inferring unknown source and target environment maps directly from the source and target scene GS representations. We evaluated TranSplat on several synthetic and real-world scenes and objects, demonstrating comparable 3D object relighting performance to recent conventional inverse rendering-based GS methods with a fraction of their runtime. While TranSplat is theoretically best-suited for radially symmetric BRDFs, results demonstrate that TranSplat still offers perceptually realistic renderings on real scenes and opens a valuable, lightweight path forward to relighting with the GS framework.", "link": "http://arxiv.org/abs/2503.22676v3", "date": "2025-12-08", "relevancy": 3.1112, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6803}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6398}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TranSplat%3A%20Instant%20Cross-Scene%20Object%20Relighting%20in%20Gaussian%20Splatting%20via%20Spherical%20Harmonic%20Transfer&body=Title%3A%20TranSplat%3A%20Instant%20Cross-Scene%20Object%20Relighting%20in%20Gaussian%20Splatting%20via%20Spherical%20Harmonic%20Transfer%0AAuthor%3A%20%20Boyang%20and%20%20Yu%20and%20Yanlin%20Jin%20and%20Yun%20He%20and%20Akshat%20Dave%20and%20Guha%20Balakrishnan%0AAbstract%3A%20We%20present%20TranSplat%2C%20a%20method%20for%20fast%20and%20accurate%20object%20relighting%20for%20the%203D%20Gaussian%20Splatting%20%28GS%29%20framework%20when%20transferring%20a%203D%20object%20from%20a%20source%20GS%20scene%20to%20a%20target%20GS%20scene.%20TranSplat%20is%20based%20on%20a%20theoretical%20radiance%20transfer%20identity%20for%20cross-scene%20relighting%20of%20objects%20with%20radially%20symmetric%20BRDFs%20that%20involves%20only%20taking%20simple%20products%20of%20spherical%20harmonic%20appearance%20coefficients%20of%20the%20object%2C%20source%2C%20and%20target%20environment%20maps%20without%20any%20explicit%20computation%20of%20scene%20quantities%20%28e.g.%2C%20the%20BRDFs%20themselves%29.%20TranSplat%20is%20the%20first%20method%20to%20demonstrate%20how%20this%20theoretical%20identity%20may%20be%20used%20to%20perform%20relighting%20within%20the%20GS%20framework%2C%20and%20furthermore%2C%20by%20automatically%20inferring%20unknown%20source%20and%20target%20environment%20maps%20directly%20from%20the%20source%20and%20target%20scene%20GS%20representations.%20We%20evaluated%20TranSplat%20on%20several%20synthetic%20and%20real-world%20scenes%20and%20objects%2C%20demonstrating%20comparable%203D%20object%20relighting%20performance%20to%20recent%20conventional%20inverse%20rendering-based%20GS%20methods%20with%20a%20fraction%20of%20their%20runtime.%20While%20TranSplat%20is%20theoretically%20best-suited%20for%20radially%20symmetric%20BRDFs%2C%20results%20demonstrate%20that%20TranSplat%20still%20offers%20perceptually%20realistic%20renderings%20on%20real%20scenes%20and%20opens%20a%20valuable%2C%20lightweight%20path%20forward%20to%20relighting%20with%20the%20GS%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2503.22676v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTranSplat%253A%2520Instant%2520Cross-Scene%2520Object%2520Relighting%2520in%2520Gaussian%2520Splatting%2520via%2520Spherical%2520Harmonic%2520Transfer%26entry.906535625%3D%2520Boyang%2520and%2520%2520Yu%2520and%2520Yanlin%2520Jin%2520and%2520Yun%2520He%2520and%2520Akshat%2520Dave%2520and%2520Guha%2520Balakrishnan%26entry.1292438233%3DWe%2520present%2520TranSplat%252C%2520a%2520method%2520for%2520fast%2520and%2520accurate%2520object%2520relighting%2520for%2520the%25203D%2520Gaussian%2520Splatting%2520%2528GS%2529%2520framework%2520when%2520transferring%2520a%25203D%2520object%2520from%2520a%2520source%2520GS%2520scene%2520to%2520a%2520target%2520GS%2520scene.%2520TranSplat%2520is%2520based%2520on%2520a%2520theoretical%2520radiance%2520transfer%2520identity%2520for%2520cross-scene%2520relighting%2520of%2520objects%2520with%2520radially%2520symmetric%2520BRDFs%2520that%2520involves%2520only%2520taking%2520simple%2520products%2520of%2520spherical%2520harmonic%2520appearance%2520coefficients%2520of%2520the%2520object%252C%2520source%252C%2520and%2520target%2520environment%2520maps%2520without%2520any%2520explicit%2520computation%2520of%2520scene%2520quantities%2520%2528e.g.%252C%2520the%2520BRDFs%2520themselves%2529.%2520TranSplat%2520is%2520the%2520first%2520method%2520to%2520demonstrate%2520how%2520this%2520theoretical%2520identity%2520may%2520be%2520used%2520to%2520perform%2520relighting%2520within%2520the%2520GS%2520framework%252C%2520and%2520furthermore%252C%2520by%2520automatically%2520inferring%2520unknown%2520source%2520and%2520target%2520environment%2520maps%2520directly%2520from%2520the%2520source%2520and%2520target%2520scene%2520GS%2520representations.%2520We%2520evaluated%2520TranSplat%2520on%2520several%2520synthetic%2520and%2520real-world%2520scenes%2520and%2520objects%252C%2520demonstrating%2520comparable%25203D%2520object%2520relighting%2520performance%2520to%2520recent%2520conventional%2520inverse%2520rendering-based%2520GS%2520methods%2520with%2520a%2520fraction%2520of%2520their%2520runtime.%2520While%2520TranSplat%2520is%2520theoretically%2520best-suited%2520for%2520radially%2520symmetric%2520BRDFs%252C%2520results%2520demonstrate%2520that%2520TranSplat%2520still%2520offers%2520perceptually%2520realistic%2520renderings%2520on%2520real%2520scenes%2520and%2520opens%2520a%2520valuable%252C%2520lightweight%2520path%2520forward%2520to%2520relighting%2520with%2520the%2520GS%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.22676v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TranSplat%3A%20Instant%20Cross-Scene%20Object%20Relighting%20in%20Gaussian%20Splatting%20via%20Spherical%20Harmonic%20Transfer&entry.906535625=%20Boyang%20and%20%20Yu%20and%20Yanlin%20Jin%20and%20Yun%20He%20and%20Akshat%20Dave%20and%20Guha%20Balakrishnan&entry.1292438233=We%20present%20TranSplat%2C%20a%20method%20for%20fast%20and%20accurate%20object%20relighting%20for%20the%203D%20Gaussian%20Splatting%20%28GS%29%20framework%20when%20transferring%20a%203D%20object%20from%20a%20source%20GS%20scene%20to%20a%20target%20GS%20scene.%20TranSplat%20is%20based%20on%20a%20theoretical%20radiance%20transfer%20identity%20for%20cross-scene%20relighting%20of%20objects%20with%20radially%20symmetric%20BRDFs%20that%20involves%20only%20taking%20simple%20products%20of%20spherical%20harmonic%20appearance%20coefficients%20of%20the%20object%2C%20source%2C%20and%20target%20environment%20maps%20without%20any%20explicit%20computation%20of%20scene%20quantities%20%28e.g.%2C%20the%20BRDFs%20themselves%29.%20TranSplat%20is%20the%20first%20method%20to%20demonstrate%20how%20this%20theoretical%20identity%20may%20be%20used%20to%20perform%20relighting%20within%20the%20GS%20framework%2C%20and%20furthermore%2C%20by%20automatically%20inferring%20unknown%20source%20and%20target%20environment%20maps%20directly%20from%20the%20source%20and%20target%20scene%20GS%20representations.%20We%20evaluated%20TranSplat%20on%20several%20synthetic%20and%20real-world%20scenes%20and%20objects%2C%20demonstrating%20comparable%203D%20object%20relighting%20performance%20to%20recent%20conventional%20inverse%20rendering-based%20GS%20methods%20with%20a%20fraction%20of%20their%20runtime.%20While%20TranSplat%20is%20theoretically%20best-suited%20for%20radially%20symmetric%20BRDFs%2C%20results%20demonstrate%20that%20TranSplat%20still%20offers%20perceptually%20realistic%20renderings%20on%20real%20scenes%20and%20opens%20a%20valuable%2C%20lightweight%20path%20forward%20to%20relighting%20with%20the%20GS%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2503.22676v3&entry.124074799=Read"},
{"title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation", "author": "Jiehui Huang and Yuechen Zhang and Xu He and Yuan Gao and Zhi Cen and Bin Xia and Yan Zhou and Xin Tao and Pengfei Wan and Jiaya Jia", "abstract": "Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo", "link": "http://arxiv.org/abs/2512.07831v1", "date": "2025-12-08", "relevancy": 3.0635, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6321}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.603}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UnityVideo%3A%20Unified%20Multi-Modal%20Multi-Task%20Learning%20for%20Enhancing%20World-Aware%20Video%20Generation&body=Title%3A%20UnityVideo%3A%20Unified%20Multi-Modal%20Multi-Task%20Learning%20for%20Enhancing%20World-Aware%20Video%20Generation%0AAuthor%3A%20Jiehui%20Huang%20and%20Yuechen%20Zhang%20and%20Xu%20He%20and%20Yuan%20Gao%20and%20Zhi%20Cen%20and%20Bin%20Xia%20and%20Yan%20Zhou%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Jiaya%20Jia%0AAbstract%3A%20Recent%20video%20generation%20models%20demonstrate%20impressive%20synthesis%20capabilities%20but%20remain%20limited%20by%20single-modality%20conditioning%2C%20constraining%20their%20holistic%20world%20understanding.%20This%20stems%20from%20insufficient%20cross-modal%20interaction%20and%20limited%20modal%20diversity%20for%20comprehensive%20world%20knowledge%20representation.%20To%20address%20these%20limitations%2C%20we%20introduce%20UnityVideo%2C%20a%20unified%20framework%20for%20world-aware%20video%20generation%20that%20jointly%20learns%20across%20multiple%20modalities%20%28segmentation%20masks%2C%20human%20skeletons%2C%20DensePose%2C%20optical%20flow%2C%20and%20depth%20maps%29%20and%20training%20paradigms.%20Our%20approach%20features%20two%20core%20components%3A%20%281%29%20dynamic%20noising%20to%20unify%20heterogeneous%20training%20paradigms%2C%20and%20%282%29%20a%20modality%20switcher%20with%20an%20in-context%20learner%20that%20enables%20unified%20processing%20via%20modular%20parameters%20and%20contextual%20learning.%20We%20contribute%20a%20large-scale%20unified%20dataset%20with%201.3M%20samples.%20Through%20joint%20optimization%2C%20UnityVideo%20accelerates%20convergence%20and%20significantly%20enhances%20zero-shot%20generalization%20to%20unseen%20data.%20We%20demonstrate%20that%20UnityVideo%20achieves%20superior%20video%20quality%2C%20consistency%2C%20and%20improved%20alignment%20with%20physical%20world%20constraints.%20Code%20and%20data%20can%20be%20found%20at%3A%20https%3A//github.com/dvlab-research/UnityVideo%0ALink%3A%20http%3A//arxiv.org/abs/2512.07831v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnityVideo%253A%2520Unified%2520Multi-Modal%2520Multi-Task%2520Learning%2520for%2520Enhancing%2520World-Aware%2520Video%2520Generation%26entry.906535625%3DJiehui%2520Huang%2520and%2520Yuechen%2520Zhang%2520and%2520Xu%2520He%2520and%2520Yuan%2520Gao%2520and%2520Zhi%2520Cen%2520and%2520Bin%2520Xia%2520and%2520Yan%2520Zhou%2520and%2520Xin%2520Tao%2520and%2520Pengfei%2520Wan%2520and%2520Jiaya%2520Jia%26entry.1292438233%3DRecent%2520video%2520generation%2520models%2520demonstrate%2520impressive%2520synthesis%2520capabilities%2520but%2520remain%2520limited%2520by%2520single-modality%2520conditioning%252C%2520constraining%2520their%2520holistic%2520world%2520understanding.%2520This%2520stems%2520from%2520insufficient%2520cross-modal%2520interaction%2520and%2520limited%2520modal%2520diversity%2520for%2520comprehensive%2520world%2520knowledge%2520representation.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520UnityVideo%252C%2520a%2520unified%2520framework%2520for%2520world-aware%2520video%2520generation%2520that%2520jointly%2520learns%2520across%2520multiple%2520modalities%2520%2528segmentation%2520masks%252C%2520human%2520skeletons%252C%2520DensePose%252C%2520optical%2520flow%252C%2520and%2520depth%2520maps%2529%2520and%2520training%2520paradigms.%2520Our%2520approach%2520features%2520two%2520core%2520components%253A%2520%25281%2529%2520dynamic%2520noising%2520to%2520unify%2520heterogeneous%2520training%2520paradigms%252C%2520and%2520%25282%2529%2520a%2520modality%2520switcher%2520with%2520an%2520in-context%2520learner%2520that%2520enables%2520unified%2520processing%2520via%2520modular%2520parameters%2520and%2520contextual%2520learning.%2520We%2520contribute%2520a%2520large-scale%2520unified%2520dataset%2520with%25201.3M%2520samples.%2520Through%2520joint%2520optimization%252C%2520UnityVideo%2520accelerates%2520convergence%2520and%2520significantly%2520enhances%2520zero-shot%2520generalization%2520to%2520unseen%2520data.%2520We%2520demonstrate%2520that%2520UnityVideo%2520achieves%2520superior%2520video%2520quality%252C%2520consistency%252C%2520and%2520improved%2520alignment%2520with%2520physical%2520world%2520constraints.%2520Code%2520and%2520data%2520can%2520be%2520found%2520at%253A%2520https%253A//github.com/dvlab-research/UnityVideo%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07831v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UnityVideo%3A%20Unified%20Multi-Modal%20Multi-Task%20Learning%20for%20Enhancing%20World-Aware%20Video%20Generation&entry.906535625=Jiehui%20Huang%20and%20Yuechen%20Zhang%20and%20Xu%20He%20and%20Yuan%20Gao%20and%20Zhi%20Cen%20and%20Bin%20Xia%20and%20Yan%20Zhou%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Jiaya%20Jia&entry.1292438233=Recent%20video%20generation%20models%20demonstrate%20impressive%20synthesis%20capabilities%20but%20remain%20limited%20by%20single-modality%20conditioning%2C%20constraining%20their%20holistic%20world%20understanding.%20This%20stems%20from%20insufficient%20cross-modal%20interaction%20and%20limited%20modal%20diversity%20for%20comprehensive%20world%20knowledge%20representation.%20To%20address%20these%20limitations%2C%20we%20introduce%20UnityVideo%2C%20a%20unified%20framework%20for%20world-aware%20video%20generation%20that%20jointly%20learns%20across%20multiple%20modalities%20%28segmentation%20masks%2C%20human%20skeletons%2C%20DensePose%2C%20optical%20flow%2C%20and%20depth%20maps%29%20and%20training%20paradigms.%20Our%20approach%20features%20two%20core%20components%3A%20%281%29%20dynamic%20noising%20to%20unify%20heterogeneous%20training%20paradigms%2C%20and%20%282%29%20a%20modality%20switcher%20with%20an%20in-context%20learner%20that%20enables%20unified%20processing%20via%20modular%20parameters%20and%20contextual%20learning.%20We%20contribute%20a%20large-scale%20unified%20dataset%20with%201.3M%20samples.%20Through%20joint%20optimization%2C%20UnityVideo%20accelerates%20convergence%20and%20significantly%20enhances%20zero-shot%20generalization%20to%20unseen%20data.%20We%20demonstrate%20that%20UnityVideo%20achieves%20superior%20video%20quality%2C%20consistency%2C%20and%20improved%20alignment%20with%20physical%20world%20constraints.%20Code%20and%20data%20can%20be%20found%20at%3A%20https%3A//github.com/dvlab-research/UnityVideo&entry.1838667208=http%3A//arxiv.org/abs/2512.07831v1&entry.124074799=Read"},
{"title": "From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images", "author": "Fei Yu and Yu Liu and Luyang Tang and Mingchao Sun and Zengye Ge and Rui Bu and Yuchao Jin and Haisen Zhao and He Sun and Yangyan Li and Mu Xu and Wenzheng Chen and Baoquan Chen", "abstract": "City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail.\n  To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs.\n  Our method's scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4\\,\\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation.", "link": "http://arxiv.org/abs/2512.07527v1", "date": "2025-12-08", "relevancy": 3.0372, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6181}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6021}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Orbit%20to%20Ground%3A%20Generative%20City%20Photogrammetry%20from%20Extreme%20Off-Nadir%20Satellite%20Images&body=Title%3A%20From%20Orbit%20to%20Ground%3A%20Generative%20City%20Photogrammetry%20from%20Extreme%20Off-Nadir%20Satellite%20Images%0AAuthor%3A%20Fei%20Yu%20and%20Yu%20Liu%20and%20Luyang%20Tang%20and%20Mingchao%20Sun%20and%20Zengye%20Ge%20and%20Rui%20Bu%20and%20Yuchao%20Jin%20and%20Haisen%20Zhao%20and%20He%20Sun%20and%20Yangyan%20Li%20and%20Mu%20Xu%20and%20Wenzheng%20Chen%20and%20Baoquan%20Chen%0AAbstract%3A%20City-scale%203D%20reconstruction%20from%20satellite%20imagery%20presents%20the%20challenge%20of%20extreme%20viewpoint%20extrapolation%2C%20where%20our%20goal%20is%20to%20synthesize%20ground-level%20novel%20views%20from%20sparse%20orbital%20images%20with%20minimal%20parallax.%20This%20requires%20inferring%20nearly%20%2490%5E%5Ccirc%24%20viewpoint%20gaps%20from%20image%20sources%20with%20severely%20foreshortened%20facades%20and%20flawed%20textures%2C%20causing%20state-of-the-art%20reconstruction%20engines%20such%20as%20NeRF%20and%203DGS%20to%20fail.%0A%20%20To%20address%20this%20problem%2C%20we%20propose%20two%20design%20choices%20tailored%20for%20city%20structures%20and%20satellite%20inputs.%20First%2C%20we%20model%20city%20geometry%20as%20a%202.5D%20height%20map%2C%20implemented%20as%20a%20Z-monotonic%20signed%20distance%20field%20%28SDF%29%20that%20matches%20urban%20building%20layouts%20from%20top-down%20viewpoints.%20This%20stabilizes%20geometry%20optimization%20under%20sparse%2C%20off-nadir%20satellite%20views%20and%20yields%20a%20watertight%20mesh%20with%20crisp%20roofs%20and%20clean%2C%20vertically%20extruded%20facades.%20Second%2C%20we%20paint%20the%20mesh%20appearance%20from%20satellite%20images%20via%20differentiable%20rendering%20techniques.%20While%20the%20satellite%20inputs%20may%20contain%20long-range%2C%20blurry%20captures%2C%20we%20further%20train%20a%20generative%20texture%20restoration%20network%20to%20enhance%20the%20appearance%2C%20recovering%20high-frequency%2C%20plausible%20texture%20details%20from%20degraded%20inputs.%0A%20%20Our%20method%27s%20scalability%20and%20robustness%20are%20demonstrated%20through%20extensive%20experiments%20on%20large-scale%20urban%20reconstruction.%20For%20example%2C%20in%20our%20teaser%20figure%2C%20we%20reconstruct%20a%20%244%5C%2C%5Cmathrm%7Bkm%7D%5E2%24%20real-world%20region%20from%20only%20a%20few%20satellite%20images%2C%20achieving%20state-of-the-art%20performance%20in%20synthesizing%20photorealistic%20ground%20views.%20The%20resulting%20models%20are%20not%20only%20visually%20compelling%20but%20also%20serve%20as%20high-fidelity%2C%20application-ready%20assets%20for%20downstream%20tasks%20like%20urban%20planning%20and%20simulation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Orbit%2520to%2520Ground%253A%2520Generative%2520City%2520Photogrammetry%2520from%2520Extreme%2520Off-Nadir%2520Satellite%2520Images%26entry.906535625%3DFei%2520Yu%2520and%2520Yu%2520Liu%2520and%2520Luyang%2520Tang%2520and%2520Mingchao%2520Sun%2520and%2520Zengye%2520Ge%2520and%2520Rui%2520Bu%2520and%2520Yuchao%2520Jin%2520and%2520Haisen%2520Zhao%2520and%2520He%2520Sun%2520and%2520Yangyan%2520Li%2520and%2520Mu%2520Xu%2520and%2520Wenzheng%2520Chen%2520and%2520Baoquan%2520Chen%26entry.1292438233%3DCity-scale%25203D%2520reconstruction%2520from%2520satellite%2520imagery%2520presents%2520the%2520challenge%2520of%2520extreme%2520viewpoint%2520extrapolation%252C%2520where%2520our%2520goal%2520is%2520to%2520synthesize%2520ground-level%2520novel%2520views%2520from%2520sparse%2520orbital%2520images%2520with%2520minimal%2520parallax.%2520This%2520requires%2520inferring%2520nearly%2520%252490%255E%255Ccirc%2524%2520viewpoint%2520gaps%2520from%2520image%2520sources%2520with%2520severely%2520foreshortened%2520facades%2520and%2520flawed%2520textures%252C%2520causing%2520state-of-the-art%2520reconstruction%2520engines%2520such%2520as%2520NeRF%2520and%25203DGS%2520to%2520fail.%250A%2520%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520two%2520design%2520choices%2520tailored%2520for%2520city%2520structures%2520and%2520satellite%2520inputs.%2520First%252C%2520we%2520model%2520city%2520geometry%2520as%2520a%25202.5D%2520height%2520map%252C%2520implemented%2520as%2520a%2520Z-monotonic%2520signed%2520distance%2520field%2520%2528SDF%2529%2520that%2520matches%2520urban%2520building%2520layouts%2520from%2520top-down%2520viewpoints.%2520This%2520stabilizes%2520geometry%2520optimization%2520under%2520sparse%252C%2520off-nadir%2520satellite%2520views%2520and%2520yields%2520a%2520watertight%2520mesh%2520with%2520crisp%2520roofs%2520and%2520clean%252C%2520vertically%2520extruded%2520facades.%2520Second%252C%2520we%2520paint%2520the%2520mesh%2520appearance%2520from%2520satellite%2520images%2520via%2520differentiable%2520rendering%2520techniques.%2520While%2520the%2520satellite%2520inputs%2520may%2520contain%2520long-range%252C%2520blurry%2520captures%252C%2520we%2520further%2520train%2520a%2520generative%2520texture%2520restoration%2520network%2520to%2520enhance%2520the%2520appearance%252C%2520recovering%2520high-frequency%252C%2520plausible%2520texture%2520details%2520from%2520degraded%2520inputs.%250A%2520%2520Our%2520method%2527s%2520scalability%2520and%2520robustness%2520are%2520demonstrated%2520through%2520extensive%2520experiments%2520on%2520large-scale%2520urban%2520reconstruction.%2520For%2520example%252C%2520in%2520our%2520teaser%2520figure%252C%2520we%2520reconstruct%2520a%2520%25244%255C%252C%255Cmathrm%257Bkm%257D%255E2%2524%2520real-world%2520region%2520from%2520only%2520a%2520few%2520satellite%2520images%252C%2520achieving%2520state-of-the-art%2520performance%2520in%2520synthesizing%2520photorealistic%2520ground%2520views.%2520The%2520resulting%2520models%2520are%2520not%2520only%2520visually%2520compelling%2520but%2520also%2520serve%2520as%2520high-fidelity%252C%2520application-ready%2520assets%2520for%2520downstream%2520tasks%2520like%2520urban%2520planning%2520and%2520simulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Orbit%20to%20Ground%3A%20Generative%20City%20Photogrammetry%20from%20Extreme%20Off-Nadir%20Satellite%20Images&entry.906535625=Fei%20Yu%20and%20Yu%20Liu%20and%20Luyang%20Tang%20and%20Mingchao%20Sun%20and%20Zengye%20Ge%20and%20Rui%20Bu%20and%20Yuchao%20Jin%20and%20Haisen%20Zhao%20and%20He%20Sun%20and%20Yangyan%20Li%20and%20Mu%20Xu%20and%20Wenzheng%20Chen%20and%20Baoquan%20Chen&entry.1292438233=City-scale%203D%20reconstruction%20from%20satellite%20imagery%20presents%20the%20challenge%20of%20extreme%20viewpoint%20extrapolation%2C%20where%20our%20goal%20is%20to%20synthesize%20ground-level%20novel%20views%20from%20sparse%20orbital%20images%20with%20minimal%20parallax.%20This%20requires%20inferring%20nearly%20%2490%5E%5Ccirc%24%20viewpoint%20gaps%20from%20image%20sources%20with%20severely%20foreshortened%20facades%20and%20flawed%20textures%2C%20causing%20state-of-the-art%20reconstruction%20engines%20such%20as%20NeRF%20and%203DGS%20to%20fail.%0A%20%20To%20address%20this%20problem%2C%20we%20propose%20two%20design%20choices%20tailored%20for%20city%20structures%20and%20satellite%20inputs.%20First%2C%20we%20model%20city%20geometry%20as%20a%202.5D%20height%20map%2C%20implemented%20as%20a%20Z-monotonic%20signed%20distance%20field%20%28SDF%29%20that%20matches%20urban%20building%20layouts%20from%20top-down%20viewpoints.%20This%20stabilizes%20geometry%20optimization%20under%20sparse%2C%20off-nadir%20satellite%20views%20and%20yields%20a%20watertight%20mesh%20with%20crisp%20roofs%20and%20clean%2C%20vertically%20extruded%20facades.%20Second%2C%20we%20paint%20the%20mesh%20appearance%20from%20satellite%20images%20via%20differentiable%20rendering%20techniques.%20While%20the%20satellite%20inputs%20may%20contain%20long-range%2C%20blurry%20captures%2C%20we%20further%20train%20a%20generative%20texture%20restoration%20network%20to%20enhance%20the%20appearance%2C%20recovering%20high-frequency%2C%20plausible%20texture%20details%20from%20degraded%20inputs.%0A%20%20Our%20method%27s%20scalability%20and%20robustness%20are%20demonstrated%20through%20extensive%20experiments%20on%20large-scale%20urban%20reconstruction.%20For%20example%2C%20in%20our%20teaser%20figure%2C%20we%20reconstruct%20a%20%244%5C%2C%5Cmathrm%7Bkm%7D%5E2%24%20real-world%20region%20from%20only%20a%20few%20satellite%20images%2C%20achieving%20state-of-the-art%20performance%20in%20synthesizing%20photorealistic%20ground%20views.%20The%20resulting%20models%20are%20not%20only%20visually%20compelling%20but%20also%20serve%20as%20high-fidelity%2C%20application-ready%20assets%20for%20downstream%20tasks%20like%20urban%20planning%20and%20simulation.&entry.1838667208=http%3A//arxiv.org/abs/2512.07527v1&entry.124074799=Read"},
{"title": "Tokenizing Motion: A Generative Approach for Scene Dynamics Compression", "author": "Shanzhi Yin and Zihan Zhang and Bolin Chen and Shiqi Wang and Yan Ye", "abstract": "This paper proposes a novel generative video compression framework that leverages motion pattern priors, derived from subtle dynamics in common scenes (e.g., swaying flowers or a boat drifting on water), rather than relying on video content priors (e.g., talking faces or human bodies). These compact motion priors enable a new approach to ultra-low bitrate communication while achieving high-quality reconstruction across diverse scene contents. At the encoder side, motion priors can be streamlined into compact representations via a dense-to-sparse transformation. At the decoder side, these priors facilitate the reconstruction of scene dynamics using an advanced flow-driven diffusion model. Experimental results illustrate that the proposed method can achieve superior rate-distortion-performance and outperform the state-of-the-art conventional-video codec Enhanced Compression Model (ECM) on-scene dynamics sequences. The project page can be found at-https://github.com/xyzysz/GNVDC.", "link": "http://arxiv.org/abs/2410.09768v3", "date": "2025-12-08", "relevancy": 2.9702, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6005}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5933}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tokenizing%20Motion%3A%20A%20Generative%20Approach%20for%20Scene%20Dynamics%20Compression&body=Title%3A%20Tokenizing%20Motion%3A%20A%20Generative%20Approach%20for%20Scene%20Dynamics%20Compression%0AAuthor%3A%20Shanzhi%20Yin%20and%20Zihan%20Zhang%20and%20Bolin%20Chen%20and%20Shiqi%20Wang%20and%20Yan%20Ye%0AAbstract%3A%20This%20paper%20proposes%20a%20novel%20generative%20video%20compression%20framework%20that%20leverages%20motion%20pattern%20priors%2C%20derived%20from%20subtle%20dynamics%20in%20common%20scenes%20%28e.g.%2C%20swaying%20flowers%20or%20a%20boat%20drifting%20on%20water%29%2C%20rather%20than%20relying%20on%20video%20content%20priors%20%28e.g.%2C%20talking%20faces%20or%20human%20bodies%29.%20These%20compact%20motion%20priors%20enable%20a%20new%20approach%20to%20ultra-low%20bitrate%20communication%20while%20achieving%20high-quality%20reconstruction%20across%20diverse%20scene%20contents.%20At%20the%20encoder%20side%2C%20motion%20priors%20can%20be%20streamlined%20into%20compact%20representations%20via%20a%20dense-to-sparse%20transformation.%20At%20the%20decoder%20side%2C%20these%20priors%20facilitate%20the%20reconstruction%20of%20scene%20dynamics%20using%20an%20advanced%20flow-driven%20diffusion%20model.%20Experimental%20results%20illustrate%20that%20the%20proposed%20method%20can%20achieve%20superior%20rate-distortion-performance%20and%20outperform%20the%20state-of-the-art%20conventional-video%20codec%20Enhanced%20Compression%20Model%20%28ECM%29%20on-scene%20dynamics%20sequences.%20The%20project%20page%20can%20be%20found%20at-https%3A//github.com/xyzysz/GNVDC.%0ALink%3A%20http%3A//arxiv.org/abs/2410.09768v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenizing%2520Motion%253A%2520A%2520Generative%2520Approach%2520for%2520Scene%2520Dynamics%2520Compression%26entry.906535625%3DShanzhi%2520Yin%2520and%2520Zihan%2520Zhang%2520and%2520Bolin%2520Chen%2520and%2520Shiqi%2520Wang%2520and%2520Yan%2520Ye%26entry.1292438233%3DThis%2520paper%2520proposes%2520a%2520novel%2520generative%2520video%2520compression%2520framework%2520that%2520leverages%2520motion%2520pattern%2520priors%252C%2520derived%2520from%2520subtle%2520dynamics%2520in%2520common%2520scenes%2520%2528e.g.%252C%2520swaying%2520flowers%2520or%2520a%2520boat%2520drifting%2520on%2520water%2529%252C%2520rather%2520than%2520relying%2520on%2520video%2520content%2520priors%2520%2528e.g.%252C%2520talking%2520faces%2520or%2520human%2520bodies%2529.%2520These%2520compact%2520motion%2520priors%2520enable%2520a%2520new%2520approach%2520to%2520ultra-low%2520bitrate%2520communication%2520while%2520achieving%2520high-quality%2520reconstruction%2520across%2520diverse%2520scene%2520contents.%2520At%2520the%2520encoder%2520side%252C%2520motion%2520priors%2520can%2520be%2520streamlined%2520into%2520compact%2520representations%2520via%2520a%2520dense-to-sparse%2520transformation.%2520At%2520the%2520decoder%2520side%252C%2520these%2520priors%2520facilitate%2520the%2520reconstruction%2520of%2520scene%2520dynamics%2520using%2520an%2520advanced%2520flow-driven%2520diffusion%2520model.%2520Experimental%2520results%2520illustrate%2520that%2520the%2520proposed%2520method%2520can%2520achieve%2520superior%2520rate-distortion-performance%2520and%2520outperform%2520the%2520state-of-the-art%2520conventional-video%2520codec%2520Enhanced%2520Compression%2520Model%2520%2528ECM%2529%2520on-scene%2520dynamics%2520sequences.%2520The%2520project%2520page%2520can%2520be%2520found%2520at-https%253A//github.com/xyzysz/GNVDC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09768v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tokenizing%20Motion%3A%20A%20Generative%20Approach%20for%20Scene%20Dynamics%20Compression&entry.906535625=Shanzhi%20Yin%20and%20Zihan%20Zhang%20and%20Bolin%20Chen%20and%20Shiqi%20Wang%20and%20Yan%20Ye&entry.1292438233=This%20paper%20proposes%20a%20novel%20generative%20video%20compression%20framework%20that%20leverages%20motion%20pattern%20priors%2C%20derived%20from%20subtle%20dynamics%20in%20common%20scenes%20%28e.g.%2C%20swaying%20flowers%20or%20a%20boat%20drifting%20on%20water%29%2C%20rather%20than%20relying%20on%20video%20content%20priors%20%28e.g.%2C%20talking%20faces%20or%20human%20bodies%29.%20These%20compact%20motion%20priors%20enable%20a%20new%20approach%20to%20ultra-low%20bitrate%20communication%20while%20achieving%20high-quality%20reconstruction%20across%20diverse%20scene%20contents.%20At%20the%20encoder%20side%2C%20motion%20priors%20can%20be%20streamlined%20into%20compact%20representations%20via%20a%20dense-to-sparse%20transformation.%20At%20the%20decoder%20side%2C%20these%20priors%20facilitate%20the%20reconstruction%20of%20scene%20dynamics%20using%20an%20advanced%20flow-driven%20diffusion%20model.%20Experimental%20results%20illustrate%20that%20the%20proposed%20method%20can%20achieve%20superior%20rate-distortion-performance%20and%20outperform%20the%20state-of-the-art%20conventional-video%20codec%20Enhanced%20Compression%20Model%20%28ECM%29%20on-scene%20dynamics%20sequences.%20The%20project%20page%20can%20be%20found%20at-https%3A//github.com/xyzysz/GNVDC.&entry.1838667208=http%3A//arxiv.org/abs/2410.09768v3&entry.124074799=Read"},
{"title": "More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery", "author": "Wenzhen Dong and Jieming Yu and Yiming Huang and Hongqiu Wang and Lei Zhu and Albert C. S. Chung and Hongliang Ren and Long Bai", "abstract": "The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3's 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.", "link": "http://arxiv.org/abs/2512.07596v1", "date": "2025-12-08", "relevancy": 2.9288, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5951}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5951}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20More%20than%20Segmentation%3A%20Benchmarking%20SAM%203%20for%20Segmentation%2C%203D%20Perception%2C%20and%20Reconstruction%20in%20Robotic%20Surgery&body=Title%3A%20More%20than%20Segmentation%3A%20Benchmarking%20SAM%203%20for%20Segmentation%2C%203D%20Perception%2C%20and%20Reconstruction%20in%20Robotic%20Surgery%0AAuthor%3A%20Wenzhen%20Dong%20and%20Jieming%20Yu%20and%20Yiming%20Huang%20and%20Hongqiu%20Wang%20and%20Lei%20Zhu%20and%20Albert%20C.%20S.%20Chung%20and%20Hongliang%20Ren%20and%20Long%20Bai%0AAbstract%3A%20The%20recent%20Segment%20Anything%20Model%20%28SAM%29%203%20has%20introduced%20significant%20advancements%20over%20its%20predecessor%2C%20SAM%202%2C%20particularly%20with%20the%20integration%20of%20language-based%20segmentation%20and%20enhanced%203D%20perception%20capabilities.%20SAM%203%20supports%20zero-shot%20segmentation%20across%20a%20wide%20range%20of%20prompts%2C%20including%20point%2C%20bounding%20box%2C%20and%20language-based%20prompts%2C%20allowing%20for%20more%20flexible%20and%20intuitive%20interactions%20with%20the%20model.%20In%20this%20empirical%20evaluation%2C%20we%20assess%20the%20performance%20of%20SAM%203%20in%20robot-assisted%20surgery%2C%20benchmarking%20its%20zero-shot%20segmentation%20with%20point%20and%20bounding%20box%20prompts%20and%20exploring%20its%20effectiveness%20in%20dynamic%20video%20tracking%2C%20alongside%20its%20newly%20introduced%20language%20prompt%20segmentation.%20While%20language%20prompts%20show%20potential%2C%20their%20performance%20in%20the%20surgical%20domain%20is%20currently%20suboptimal%2C%20highlighting%20the%20need%20for%20further%20domain-specific%20training.%20Additionally%2C%20we%20investigate%20SAM%203%27s%203D%20reconstruction%20abilities%2C%20demonstrating%20its%20capacity%20to%20process%20surgical%20scene%20data%20and%20reconstruct%203D%20anatomical%20structures%20from%202D%20images.%20Through%20comprehensive%20testing%20on%20the%20MICCAI%20EndoVis%202017%20and%20EndoVis%202018%20benchmarks%2C%20SAM%203%20shows%20clear%20improvements%20over%20SAM%20and%20SAM%202%20in%20both%20image%20and%20video%20segmentation%20under%20spatial%20prompts%2C%20while%20zero-shot%20evaluations%20on%20SCARED%2C%20StereoMIS%2C%20and%20EndoNeRF%20indicate%20strong%20monocular%20depth%20estimation%20and%20realistic%203D%20instrument%20reconstruction%2C%20yet%20also%20reveal%20remaining%20limitations%20in%20complex%2C%20highly%20dynamic%20surgical%20scenes.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMore%2520than%2520Segmentation%253A%2520Benchmarking%2520SAM%25203%2520for%2520Segmentation%252C%25203D%2520Perception%252C%2520and%2520Reconstruction%2520in%2520Robotic%2520Surgery%26entry.906535625%3DWenzhen%2520Dong%2520and%2520Jieming%2520Yu%2520and%2520Yiming%2520Huang%2520and%2520Hongqiu%2520Wang%2520and%2520Lei%2520Zhu%2520and%2520Albert%2520C.%2520S.%2520Chung%2520and%2520Hongliang%2520Ren%2520and%2520Long%2520Bai%26entry.1292438233%3DThe%2520recent%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%25203%2520has%2520introduced%2520significant%2520advancements%2520over%2520its%2520predecessor%252C%2520SAM%25202%252C%2520particularly%2520with%2520the%2520integration%2520of%2520language-based%2520segmentation%2520and%2520enhanced%25203D%2520perception%2520capabilities.%2520SAM%25203%2520supports%2520zero-shot%2520segmentation%2520across%2520a%2520wide%2520range%2520of%2520prompts%252C%2520including%2520point%252C%2520bounding%2520box%252C%2520and%2520language-based%2520prompts%252C%2520allowing%2520for%2520more%2520flexible%2520and%2520intuitive%2520interactions%2520with%2520the%2520model.%2520In%2520this%2520empirical%2520evaluation%252C%2520we%2520assess%2520the%2520performance%2520of%2520SAM%25203%2520in%2520robot-assisted%2520surgery%252C%2520benchmarking%2520its%2520zero-shot%2520segmentation%2520with%2520point%2520and%2520bounding%2520box%2520prompts%2520and%2520exploring%2520its%2520effectiveness%2520in%2520dynamic%2520video%2520tracking%252C%2520alongside%2520its%2520newly%2520introduced%2520language%2520prompt%2520segmentation.%2520While%2520language%2520prompts%2520show%2520potential%252C%2520their%2520performance%2520in%2520the%2520surgical%2520domain%2520is%2520currently%2520suboptimal%252C%2520highlighting%2520the%2520need%2520for%2520further%2520domain-specific%2520training.%2520Additionally%252C%2520we%2520investigate%2520SAM%25203%2527s%25203D%2520reconstruction%2520abilities%252C%2520demonstrating%2520its%2520capacity%2520to%2520process%2520surgical%2520scene%2520data%2520and%2520reconstruct%25203D%2520anatomical%2520structures%2520from%25202D%2520images.%2520Through%2520comprehensive%2520testing%2520on%2520the%2520MICCAI%2520EndoVis%25202017%2520and%2520EndoVis%25202018%2520benchmarks%252C%2520SAM%25203%2520shows%2520clear%2520improvements%2520over%2520SAM%2520and%2520SAM%25202%2520in%2520both%2520image%2520and%2520video%2520segmentation%2520under%2520spatial%2520prompts%252C%2520while%2520zero-shot%2520evaluations%2520on%2520SCARED%252C%2520StereoMIS%252C%2520and%2520EndoNeRF%2520indicate%2520strong%2520monocular%2520depth%2520estimation%2520and%2520realistic%25203D%2520instrument%2520reconstruction%252C%2520yet%2520also%2520reveal%2520remaining%2520limitations%2520in%2520complex%252C%2520highly%2520dynamic%2520surgical%2520scenes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=More%20than%20Segmentation%3A%20Benchmarking%20SAM%203%20for%20Segmentation%2C%203D%20Perception%2C%20and%20Reconstruction%20in%20Robotic%20Surgery&entry.906535625=Wenzhen%20Dong%20and%20Jieming%20Yu%20and%20Yiming%20Huang%20and%20Hongqiu%20Wang%20and%20Lei%20Zhu%20and%20Albert%20C.%20S.%20Chung%20and%20Hongliang%20Ren%20and%20Long%20Bai&entry.1292438233=The%20recent%20Segment%20Anything%20Model%20%28SAM%29%203%20has%20introduced%20significant%20advancements%20over%20its%20predecessor%2C%20SAM%202%2C%20particularly%20with%20the%20integration%20of%20language-based%20segmentation%20and%20enhanced%203D%20perception%20capabilities.%20SAM%203%20supports%20zero-shot%20segmentation%20across%20a%20wide%20range%20of%20prompts%2C%20including%20point%2C%20bounding%20box%2C%20and%20language-based%20prompts%2C%20allowing%20for%20more%20flexible%20and%20intuitive%20interactions%20with%20the%20model.%20In%20this%20empirical%20evaluation%2C%20we%20assess%20the%20performance%20of%20SAM%203%20in%20robot-assisted%20surgery%2C%20benchmarking%20its%20zero-shot%20segmentation%20with%20point%20and%20bounding%20box%20prompts%20and%20exploring%20its%20effectiveness%20in%20dynamic%20video%20tracking%2C%20alongside%20its%20newly%20introduced%20language%20prompt%20segmentation.%20While%20language%20prompts%20show%20potential%2C%20their%20performance%20in%20the%20surgical%20domain%20is%20currently%20suboptimal%2C%20highlighting%20the%20need%20for%20further%20domain-specific%20training.%20Additionally%2C%20we%20investigate%20SAM%203%27s%203D%20reconstruction%20abilities%2C%20demonstrating%20its%20capacity%20to%20process%20surgical%20scene%20data%20and%20reconstruct%203D%20anatomical%20structures%20from%202D%20images.%20Through%20comprehensive%20testing%20on%20the%20MICCAI%20EndoVis%202017%20and%20EndoVis%202018%20benchmarks%2C%20SAM%203%20shows%20clear%20improvements%20over%20SAM%20and%20SAM%202%20in%20both%20image%20and%20video%20segmentation%20under%20spatial%20prompts%2C%20while%20zero-shot%20evaluations%20on%20SCARED%2C%20StereoMIS%2C%20and%20EndoNeRF%20indicate%20strong%20monocular%20depth%20estimation%20and%20realistic%203D%20instrument%20reconstruction%2C%20yet%20also%20reveal%20remaining%20limitations%20in%20complex%2C%20highly%20dynamic%20surgical%20scenes.&entry.1838667208=http%3A//arxiv.org/abs/2512.07596v1&entry.124074799=Read"},
{"title": "Enhanced Spatiotemporal Consistency for Image-to-LiDAR Data Pretraining", "author": "Xiang Xu and Lingdong Kong and Hui Shuai and Wenwei Zhang and Liang Pan and Kai Chen and Ziwei Liu and Qingshan Liu", "abstract": "LiDAR representation learning has emerged as a promising approach to reducing reliance on costly and labor-intensive human annotations. While existing methods primarily focus on spatial alignment between LiDAR and camera sensors, they often overlook the temporal dynamics critical for capturing motion and scene continuity in driving scenarios. To address this limitation, we propose SuperFlow++, a novel framework that integrates spatiotemporal cues in both pretraining and downstream tasks using consecutive LiDAR-camera pairs. SuperFlow++ introduces four key components: (1) a view consistency alignment module to unify semantic information across camera views, (2) a dense-to-sparse consistency regularization mechanism to enhance feature robustness across varying point cloud densities, (3) a flow-based contrastive learning approach that models temporal relationships for improved scene understanding, and (4) a temporal voting strategy that propagates semantic information across LiDAR scans to improve prediction consistency. Extensive evaluations on 11 heterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms state-of-the-art methods across diverse tasks and driving conditions. Furthermore, by scaling both 2D and 3D backbones during pretraining, we uncover emergent properties that provide deeper insights into developing scalable 3D foundation models. With strong generalizability and computational efficiency, SuperFlow++ establishes a new benchmark for data-efficient LiDAR-based perception in autonomous driving. The code is publicly available at https://github.com/Xiangxu-0103/SuperFlow", "link": "http://arxiv.org/abs/2503.19912v2", "date": "2025-12-08", "relevancy": 2.8515, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5725}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5719}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Spatiotemporal%20Consistency%20for%20Image-to-LiDAR%20Data%20Pretraining&body=Title%3A%20Enhanced%20Spatiotemporal%20Consistency%20for%20Image-to-LiDAR%20Data%20Pretraining%0AAuthor%3A%20Xiang%20Xu%20and%20Lingdong%20Kong%20and%20Hui%20Shuai%20and%20Wenwei%20Zhang%20and%20Liang%20Pan%20and%20Kai%20Chen%20and%20Ziwei%20Liu%20and%20Qingshan%20Liu%0AAbstract%3A%20LiDAR%20representation%20learning%20has%20emerged%20as%20a%20promising%20approach%20to%20reducing%20reliance%20on%20costly%20and%20labor-intensive%20human%20annotations.%20While%20existing%20methods%20primarily%20focus%20on%20spatial%20alignment%20between%20LiDAR%20and%20camera%20sensors%2C%20they%20often%20overlook%20the%20temporal%20dynamics%20critical%20for%20capturing%20motion%20and%20scene%20continuity%20in%20driving%20scenarios.%20To%20address%20this%20limitation%2C%20we%20propose%20SuperFlow%2B%2B%2C%20a%20novel%20framework%20that%20integrates%20spatiotemporal%20cues%20in%20both%20pretraining%20and%20downstream%20tasks%20using%20consecutive%20LiDAR-camera%20pairs.%20SuperFlow%2B%2B%20introduces%20four%20key%20components%3A%20%281%29%20a%20view%20consistency%20alignment%20module%20to%20unify%20semantic%20information%20across%20camera%20views%2C%20%282%29%20a%20dense-to-sparse%20consistency%20regularization%20mechanism%20to%20enhance%20feature%20robustness%20across%20varying%20point%20cloud%20densities%2C%20%283%29%20a%20flow-based%20contrastive%20learning%20approach%20that%20models%20temporal%20relationships%20for%20improved%20scene%20understanding%2C%20and%20%284%29%20a%20temporal%20voting%20strategy%20that%20propagates%20semantic%20information%20across%20LiDAR%20scans%20to%20improve%20prediction%20consistency.%20Extensive%20evaluations%20on%2011%20heterogeneous%20LiDAR%20datasets%20demonstrate%20that%20SuperFlow%2B%2B%20outperforms%20state-of-the-art%20methods%20across%20diverse%20tasks%20and%20driving%20conditions.%20Furthermore%2C%20by%20scaling%20both%202D%20and%203D%20backbones%20during%20pretraining%2C%20we%20uncover%20emergent%20properties%20that%20provide%20deeper%20insights%20into%20developing%20scalable%203D%20foundation%20models.%20With%20strong%20generalizability%20and%20computational%20efficiency%2C%20SuperFlow%2B%2B%20establishes%20a%20new%20benchmark%20for%20data-efficient%20LiDAR-based%20perception%20in%20autonomous%20driving.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/Xiangxu-0103/SuperFlow%0ALink%3A%20http%3A//arxiv.org/abs/2503.19912v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Spatiotemporal%2520Consistency%2520for%2520Image-to-LiDAR%2520Data%2520Pretraining%26entry.906535625%3DXiang%2520Xu%2520and%2520Lingdong%2520Kong%2520and%2520Hui%2520Shuai%2520and%2520Wenwei%2520Zhang%2520and%2520Liang%2520Pan%2520and%2520Kai%2520Chen%2520and%2520Ziwei%2520Liu%2520and%2520Qingshan%2520Liu%26entry.1292438233%3DLiDAR%2520representation%2520learning%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520to%2520reducing%2520reliance%2520on%2520costly%2520and%2520labor-intensive%2520human%2520annotations.%2520While%2520existing%2520methods%2520primarily%2520focus%2520on%2520spatial%2520alignment%2520between%2520LiDAR%2520and%2520camera%2520sensors%252C%2520they%2520often%2520overlook%2520the%2520temporal%2520dynamics%2520critical%2520for%2520capturing%2520motion%2520and%2520scene%2520continuity%2520in%2520driving%2520scenarios.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520SuperFlow%252B%252B%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520spatiotemporal%2520cues%2520in%2520both%2520pretraining%2520and%2520downstream%2520tasks%2520using%2520consecutive%2520LiDAR-camera%2520pairs.%2520SuperFlow%252B%252B%2520introduces%2520four%2520key%2520components%253A%2520%25281%2529%2520a%2520view%2520consistency%2520alignment%2520module%2520to%2520unify%2520semantic%2520information%2520across%2520camera%2520views%252C%2520%25282%2529%2520a%2520dense-to-sparse%2520consistency%2520regularization%2520mechanism%2520to%2520enhance%2520feature%2520robustness%2520across%2520varying%2520point%2520cloud%2520densities%252C%2520%25283%2529%2520a%2520flow-based%2520contrastive%2520learning%2520approach%2520that%2520models%2520temporal%2520relationships%2520for%2520improved%2520scene%2520understanding%252C%2520and%2520%25284%2529%2520a%2520temporal%2520voting%2520strategy%2520that%2520propagates%2520semantic%2520information%2520across%2520LiDAR%2520scans%2520to%2520improve%2520prediction%2520consistency.%2520Extensive%2520evaluations%2520on%252011%2520heterogeneous%2520LiDAR%2520datasets%2520demonstrate%2520that%2520SuperFlow%252B%252B%2520outperforms%2520state-of-the-art%2520methods%2520across%2520diverse%2520tasks%2520and%2520driving%2520conditions.%2520Furthermore%252C%2520by%2520scaling%2520both%25202D%2520and%25203D%2520backbones%2520during%2520pretraining%252C%2520we%2520uncover%2520emergent%2520properties%2520that%2520provide%2520deeper%2520insights%2520into%2520developing%2520scalable%25203D%2520foundation%2520models.%2520With%2520strong%2520generalizability%2520and%2520computational%2520efficiency%252C%2520SuperFlow%252B%252B%2520establishes%2520a%2520new%2520benchmark%2520for%2520data-efficient%2520LiDAR-based%2520perception%2520in%2520autonomous%2520driving.%2520The%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/Xiangxu-0103/SuperFlow%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19912v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Spatiotemporal%20Consistency%20for%20Image-to-LiDAR%20Data%20Pretraining&entry.906535625=Xiang%20Xu%20and%20Lingdong%20Kong%20and%20Hui%20Shuai%20and%20Wenwei%20Zhang%20and%20Liang%20Pan%20and%20Kai%20Chen%20and%20Ziwei%20Liu%20and%20Qingshan%20Liu&entry.1292438233=LiDAR%20representation%20learning%20has%20emerged%20as%20a%20promising%20approach%20to%20reducing%20reliance%20on%20costly%20and%20labor-intensive%20human%20annotations.%20While%20existing%20methods%20primarily%20focus%20on%20spatial%20alignment%20between%20LiDAR%20and%20camera%20sensors%2C%20they%20often%20overlook%20the%20temporal%20dynamics%20critical%20for%20capturing%20motion%20and%20scene%20continuity%20in%20driving%20scenarios.%20To%20address%20this%20limitation%2C%20we%20propose%20SuperFlow%2B%2B%2C%20a%20novel%20framework%20that%20integrates%20spatiotemporal%20cues%20in%20both%20pretraining%20and%20downstream%20tasks%20using%20consecutive%20LiDAR-camera%20pairs.%20SuperFlow%2B%2B%20introduces%20four%20key%20components%3A%20%281%29%20a%20view%20consistency%20alignment%20module%20to%20unify%20semantic%20information%20across%20camera%20views%2C%20%282%29%20a%20dense-to-sparse%20consistency%20regularization%20mechanism%20to%20enhance%20feature%20robustness%20across%20varying%20point%20cloud%20densities%2C%20%283%29%20a%20flow-based%20contrastive%20learning%20approach%20that%20models%20temporal%20relationships%20for%20improved%20scene%20understanding%2C%20and%20%284%29%20a%20temporal%20voting%20strategy%20that%20propagates%20semantic%20information%20across%20LiDAR%20scans%20to%20improve%20prediction%20consistency.%20Extensive%20evaluations%20on%2011%20heterogeneous%20LiDAR%20datasets%20demonstrate%20that%20SuperFlow%2B%2B%20outperforms%20state-of-the-art%20methods%20across%20diverse%20tasks%20and%20driving%20conditions.%20Furthermore%2C%20by%20scaling%20both%202D%20and%203D%20backbones%20during%20pretraining%2C%20we%20uncover%20emergent%20properties%20that%20provide%20deeper%20insights%20into%20developing%20scalable%203D%20foundation%20models.%20With%20strong%20generalizability%20and%20computational%20efficiency%2C%20SuperFlow%2B%2B%20establishes%20a%20new%20benchmark%20for%20data-efficient%20LiDAR-based%20perception%20in%20autonomous%20driving.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/Xiangxu-0103/SuperFlow&entry.1838667208=http%3A//arxiv.org/abs/2503.19912v2&entry.124074799=Read"},
{"title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture", "author": "Xin He and Longhui Wei and Jianbo Ouyang and Lingxi Xie and Qi Tian", "abstract": "We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.", "link": "http://arxiv.org/abs/2512.04810v3", "date": "2025-12-08", "relevancy": 2.8288, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5774}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5599}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMMA%3A%20Efficient%20Multimodal%20Understanding%2C%20Generation%2C%20and%20Editing%20with%20a%20Unified%20Architecture&body=Title%3A%20EMMA%3A%20Efficient%20Multimodal%20Understanding%2C%20Generation%2C%20and%20Editing%20with%20a%20Unified%20Architecture%0AAuthor%3A%20Xin%20He%20and%20Longhui%20Wei%20and%20Jianbo%20Ouyang%20and%20Lingxi%20Xie%20and%20Qi%20Tian%0AAbstract%3A%20We%20propose%20EMMA%2C%20an%20efficient%20and%20unified%20architecture%20for%20multimodal%20understanding%2C%20generation%20and%20editing.%20Specifically%2C%20EMMA%20primarily%20consists%20of%201%29%20An%20efficient%20autoencoder%20with%20a%2032x%20compression%20ratio%2C%20which%20significantly%20reduces%20the%20number%20of%20tokens%20required%20for%20generation.%20This%20also%20ensures%20the%20training%20balance%20between%20understanding%20and%20generation%20tasks%20by%20applying%20the%20same%20compression%20ratio%20to%20images.%202%29%20Channel-wise%20concatenation%20instead%20of%20token-wise%20concatenation%20among%20visual%20understanding%20and%20generation%20tokens%2C%20which%20further%20reduces%20the%20visual%20tokens%20in%20unified%20architectures.%203%29%20A%20shared-and-decoupled%20network%20that%20enables%20mutual%20improvements%20across%20tasks%20while%20meeting%20the%20task-specific%20modeling%20requirements.%204%29%20A%20mixture-of-experts%20mechanism%20adopted%20for%20visual%20understanding%20encoder%2C%20which%20substantially%20improves%20perceptual%20capabilities%20with%20a%20few%20parameters%20increase.%20Extensive%20experiments%20have%20shown%20that%20EMMA-4B%20can%20significantly%20outperform%20state-of-the-art%20unified%20multimodal%20approaches%20%28e.g.%2C%20BAGEL-7B%29%20in%20both%20efficiency%20and%20performance%2C%20while%20also%20achieving%20competitive%20results%20compared%20to%20recent%20multimodal%20understanding%20and%20generation%20experts%20%28e.g.%2C%20Qwen3-VL%20and%20Qwen-Image%29.%20We%20believe%20that%20EMMA%20lays%20a%20solid%20foundation%20for%20the%20future%20development%20of%20unified%20multimodal%20architectures.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04810v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMMA%253A%2520Efficient%2520Multimodal%2520Understanding%252C%2520Generation%252C%2520and%2520Editing%2520with%2520a%2520Unified%2520Architecture%26entry.906535625%3DXin%2520He%2520and%2520Longhui%2520Wei%2520and%2520Jianbo%2520Ouyang%2520and%2520Lingxi%2520Xie%2520and%2520Qi%2520Tian%26entry.1292438233%3DWe%2520propose%2520EMMA%252C%2520an%2520efficient%2520and%2520unified%2520architecture%2520for%2520multimodal%2520understanding%252C%2520generation%2520and%2520editing.%2520Specifically%252C%2520EMMA%2520primarily%2520consists%2520of%25201%2529%2520An%2520efficient%2520autoencoder%2520with%2520a%252032x%2520compression%2520ratio%252C%2520which%2520significantly%2520reduces%2520the%2520number%2520of%2520tokens%2520required%2520for%2520generation.%2520This%2520also%2520ensures%2520the%2520training%2520balance%2520between%2520understanding%2520and%2520generation%2520tasks%2520by%2520applying%2520the%2520same%2520compression%2520ratio%2520to%2520images.%25202%2529%2520Channel-wise%2520concatenation%2520instead%2520of%2520token-wise%2520concatenation%2520among%2520visual%2520understanding%2520and%2520generation%2520tokens%252C%2520which%2520further%2520reduces%2520the%2520visual%2520tokens%2520in%2520unified%2520architectures.%25203%2529%2520A%2520shared-and-decoupled%2520network%2520that%2520enables%2520mutual%2520improvements%2520across%2520tasks%2520while%2520meeting%2520the%2520task-specific%2520modeling%2520requirements.%25204%2529%2520A%2520mixture-of-experts%2520mechanism%2520adopted%2520for%2520visual%2520understanding%2520encoder%252C%2520which%2520substantially%2520improves%2520perceptual%2520capabilities%2520with%2520a%2520few%2520parameters%2520increase.%2520Extensive%2520experiments%2520have%2520shown%2520that%2520EMMA-4B%2520can%2520significantly%2520outperform%2520state-of-the-art%2520unified%2520multimodal%2520approaches%2520%2528e.g.%252C%2520BAGEL-7B%2529%2520in%2520both%2520efficiency%2520and%2520performance%252C%2520while%2520also%2520achieving%2520competitive%2520results%2520compared%2520to%2520recent%2520multimodal%2520understanding%2520and%2520generation%2520experts%2520%2528e.g.%252C%2520Qwen3-VL%2520and%2520Qwen-Image%2529.%2520We%2520believe%2520that%2520EMMA%2520lays%2520a%2520solid%2520foundation%2520for%2520the%2520future%2520development%2520of%2520unified%2520multimodal%2520architectures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04810v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMMA%3A%20Efficient%20Multimodal%20Understanding%2C%20Generation%2C%20and%20Editing%20with%20a%20Unified%20Architecture&entry.906535625=Xin%20He%20and%20Longhui%20Wei%20and%20Jianbo%20Ouyang%20and%20Lingxi%20Xie%20and%20Qi%20Tian&entry.1292438233=We%20propose%20EMMA%2C%20an%20efficient%20and%20unified%20architecture%20for%20multimodal%20understanding%2C%20generation%20and%20editing.%20Specifically%2C%20EMMA%20primarily%20consists%20of%201%29%20An%20efficient%20autoencoder%20with%20a%2032x%20compression%20ratio%2C%20which%20significantly%20reduces%20the%20number%20of%20tokens%20required%20for%20generation.%20This%20also%20ensures%20the%20training%20balance%20between%20understanding%20and%20generation%20tasks%20by%20applying%20the%20same%20compression%20ratio%20to%20images.%202%29%20Channel-wise%20concatenation%20instead%20of%20token-wise%20concatenation%20among%20visual%20understanding%20and%20generation%20tokens%2C%20which%20further%20reduces%20the%20visual%20tokens%20in%20unified%20architectures.%203%29%20A%20shared-and-decoupled%20network%20that%20enables%20mutual%20improvements%20across%20tasks%20while%20meeting%20the%20task-specific%20modeling%20requirements.%204%29%20A%20mixture-of-experts%20mechanism%20adopted%20for%20visual%20understanding%20encoder%2C%20which%20substantially%20improves%20perceptual%20capabilities%20with%20a%20few%20parameters%20increase.%20Extensive%20experiments%20have%20shown%20that%20EMMA-4B%20can%20significantly%20outperform%20state-of-the-art%20unified%20multimodal%20approaches%20%28e.g.%2C%20BAGEL-7B%29%20in%20both%20efficiency%20and%20performance%2C%20while%20also%20achieving%20competitive%20results%20compared%20to%20recent%20multimodal%20understanding%20and%20generation%20experts%20%28e.g.%2C%20Qwen3-VL%20and%20Qwen-Image%29.%20We%20believe%20that%20EMMA%20lays%20a%20solid%20foundation%20for%20the%20future%20development%20of%20unified%20multimodal%20architectures.&entry.1838667208=http%3A//arxiv.org/abs/2512.04810v3&entry.124074799=Read"},
{"title": "FLAIR: Frequency- and Locality-Aware Implicit Neural Representations", "author": "Sukhun Ko and Seokhyun Yoon and Dahyeon Kye and Kyle Min and Chanho Eom and Jihyong Oh", "abstract": "Implicit Neural Representations (INRs) leverage neural networks to map coordinates to corresponding signals, enabling continuous and compact representations. This paradigm has driven significant advances in various vision tasks. However, existing INRs lack frequency selectivity and spatial localization, leading to an over-reliance on redundant signal components. Consequently, they exhibit spectral bias, tending to learn low-frequency components early while struggling to capture fine high-frequency details. To address these issues, we propose FLAIR (Frequency- and Locality-Aware Implicit Neural Representations), which incorporates two key innovations. The first is Band-Localized Activation (BLA), a novel activation designed for joint frequency selection and spatial localization under the constraints of the time-frequency uncertainty principle (TFUP). Through structured frequency control and spatially localized responses, BLA effectively mitigates spectral bias and enhances training stability. The second is Wavelet-Energy-Guided Encoding (WEGE), which leverages the discrete wavelet transform to compute energy scores and explicitly guide frequency information to the network, enabling precise frequency selection and adaptive band control. Our method consistently outperforms existing INRs in 2D image representation, as well as 3D shape reconstruction and novel view synthesis.", "link": "http://arxiv.org/abs/2508.13544v4", "date": "2025-12-08", "relevancy": 2.786, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5897}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5538}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLAIR%3A%20Frequency-%20and%20Locality-Aware%20Implicit%20Neural%20Representations&body=Title%3A%20FLAIR%3A%20Frequency-%20and%20Locality-Aware%20Implicit%20Neural%20Representations%0AAuthor%3A%20Sukhun%20Ko%20and%20Seokhyun%20Yoon%20and%20Dahyeon%20Kye%20and%20Kyle%20Min%20and%20Chanho%20Eom%20and%20Jihyong%20Oh%0AAbstract%3A%20Implicit%20Neural%20Representations%20%28INRs%29%20leverage%20neural%20networks%20to%20map%20coordinates%20to%20corresponding%20signals%2C%20enabling%20continuous%20and%20compact%20representations.%20This%20paradigm%20has%20driven%20significant%20advances%20in%20various%20vision%20tasks.%20However%2C%20existing%20INRs%20lack%20frequency%20selectivity%20and%20spatial%20localization%2C%20leading%20to%20an%20over-reliance%20on%20redundant%20signal%20components.%20Consequently%2C%20they%20exhibit%20spectral%20bias%2C%20tending%20to%20learn%20low-frequency%20components%20early%20while%20struggling%20to%20capture%20fine%20high-frequency%20details.%20To%20address%20these%20issues%2C%20we%20propose%20FLAIR%20%28Frequency-%20and%20Locality-Aware%20Implicit%20Neural%20Representations%29%2C%20which%20incorporates%20two%20key%20innovations.%20The%20first%20is%20Band-Localized%20Activation%20%28BLA%29%2C%20a%20novel%20activation%20designed%20for%20joint%20frequency%20selection%20and%20spatial%20localization%20under%20the%20constraints%20of%20the%20time-frequency%20uncertainty%20principle%20%28TFUP%29.%20Through%20structured%20frequency%20control%20and%20spatially%20localized%20responses%2C%20BLA%20effectively%20mitigates%20spectral%20bias%20and%20enhances%20training%20stability.%20The%20second%20is%20Wavelet-Energy-Guided%20Encoding%20%28WEGE%29%2C%20which%20leverages%20the%20discrete%20wavelet%20transform%20to%20compute%20energy%20scores%20and%20explicitly%20guide%20frequency%20information%20to%20the%20network%2C%20enabling%20precise%20frequency%20selection%20and%20adaptive%20band%20control.%20Our%20method%20consistently%20outperforms%20existing%20INRs%20in%202D%20image%20representation%2C%20as%20well%20as%203D%20shape%20reconstruction%20and%20novel%20view%20synthesis.%0ALink%3A%20http%3A//arxiv.org/abs/2508.13544v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLAIR%253A%2520Frequency-%2520and%2520Locality-Aware%2520Implicit%2520Neural%2520Representations%26entry.906535625%3DSukhun%2520Ko%2520and%2520Seokhyun%2520Yoon%2520and%2520Dahyeon%2520Kye%2520and%2520Kyle%2520Min%2520and%2520Chanho%2520Eom%2520and%2520Jihyong%2520Oh%26entry.1292438233%3DImplicit%2520Neural%2520Representations%2520%2528INRs%2529%2520leverage%2520neural%2520networks%2520to%2520map%2520coordinates%2520to%2520corresponding%2520signals%252C%2520enabling%2520continuous%2520and%2520compact%2520representations.%2520This%2520paradigm%2520has%2520driven%2520significant%2520advances%2520in%2520various%2520vision%2520tasks.%2520However%252C%2520existing%2520INRs%2520lack%2520frequency%2520selectivity%2520and%2520spatial%2520localization%252C%2520leading%2520to%2520an%2520over-reliance%2520on%2520redundant%2520signal%2520components.%2520Consequently%252C%2520they%2520exhibit%2520spectral%2520bias%252C%2520tending%2520to%2520learn%2520low-frequency%2520components%2520early%2520while%2520struggling%2520to%2520capture%2520fine%2520high-frequency%2520details.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520FLAIR%2520%2528Frequency-%2520and%2520Locality-Aware%2520Implicit%2520Neural%2520Representations%2529%252C%2520which%2520incorporates%2520two%2520key%2520innovations.%2520The%2520first%2520is%2520Band-Localized%2520Activation%2520%2528BLA%2529%252C%2520a%2520novel%2520activation%2520designed%2520for%2520joint%2520frequency%2520selection%2520and%2520spatial%2520localization%2520under%2520the%2520constraints%2520of%2520the%2520time-frequency%2520uncertainty%2520principle%2520%2528TFUP%2529.%2520Through%2520structured%2520frequency%2520control%2520and%2520spatially%2520localized%2520responses%252C%2520BLA%2520effectively%2520mitigates%2520spectral%2520bias%2520and%2520enhances%2520training%2520stability.%2520The%2520second%2520is%2520Wavelet-Energy-Guided%2520Encoding%2520%2528WEGE%2529%252C%2520which%2520leverages%2520the%2520discrete%2520wavelet%2520transform%2520to%2520compute%2520energy%2520scores%2520and%2520explicitly%2520guide%2520frequency%2520information%2520to%2520the%2520network%252C%2520enabling%2520precise%2520frequency%2520selection%2520and%2520adaptive%2520band%2520control.%2520Our%2520method%2520consistently%2520outperforms%2520existing%2520INRs%2520in%25202D%2520image%2520representation%252C%2520as%2520well%2520as%25203D%2520shape%2520reconstruction%2520and%2520novel%2520view%2520synthesis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13544v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLAIR%3A%20Frequency-%20and%20Locality-Aware%20Implicit%20Neural%20Representations&entry.906535625=Sukhun%20Ko%20and%20Seokhyun%20Yoon%20and%20Dahyeon%20Kye%20and%20Kyle%20Min%20and%20Chanho%20Eom%20and%20Jihyong%20Oh&entry.1292438233=Implicit%20Neural%20Representations%20%28INRs%29%20leverage%20neural%20networks%20to%20map%20coordinates%20to%20corresponding%20signals%2C%20enabling%20continuous%20and%20compact%20representations.%20This%20paradigm%20has%20driven%20significant%20advances%20in%20various%20vision%20tasks.%20However%2C%20existing%20INRs%20lack%20frequency%20selectivity%20and%20spatial%20localization%2C%20leading%20to%20an%20over-reliance%20on%20redundant%20signal%20components.%20Consequently%2C%20they%20exhibit%20spectral%20bias%2C%20tending%20to%20learn%20low-frequency%20components%20early%20while%20struggling%20to%20capture%20fine%20high-frequency%20details.%20To%20address%20these%20issues%2C%20we%20propose%20FLAIR%20%28Frequency-%20and%20Locality-Aware%20Implicit%20Neural%20Representations%29%2C%20which%20incorporates%20two%20key%20innovations.%20The%20first%20is%20Band-Localized%20Activation%20%28BLA%29%2C%20a%20novel%20activation%20designed%20for%20joint%20frequency%20selection%20and%20spatial%20localization%20under%20the%20constraints%20of%20the%20time-frequency%20uncertainty%20principle%20%28TFUP%29.%20Through%20structured%20frequency%20control%20and%20spatially%20localized%20responses%2C%20BLA%20effectively%20mitigates%20spectral%20bias%20and%20enhances%20training%20stability.%20The%20second%20is%20Wavelet-Energy-Guided%20Encoding%20%28WEGE%29%2C%20which%20leverages%20the%20discrete%20wavelet%20transform%20to%20compute%20energy%20scores%20and%20explicitly%20guide%20frequency%20information%20to%20the%20network%2C%20enabling%20precise%20frequency%20selection%20and%20adaptive%20band%20control.%20Our%20method%20consistently%20outperforms%20existing%20INRs%20in%202D%20image%20representation%2C%20as%20well%20as%203D%20shape%20reconstruction%20and%20novel%20view%20synthesis.&entry.1838667208=http%3A//arxiv.org/abs/2508.13544v4&entry.124074799=Read"},
{"title": "A Geometric Unification of Concept Learning with Concept Cones", "author": "Alexandre Rocchi--Henry and Thomas Fel and Gianni Franchi", "abstract": "Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\\footnote{We adopt the terminology of \\citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.", "link": "http://arxiv.org/abs/2512.07355v1", "date": "2025-12-08", "relevancy": 2.7599, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5594}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5482}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Geometric%20Unification%20of%20Concept%20Learning%20with%20Concept%20Cones&body=Title%3A%20A%20Geometric%20Unification%20of%20Concept%20Learning%20with%20Concept%20Cones%0AAuthor%3A%20Alexandre%20Rocchi--Henry%20and%20Thomas%20Fel%20and%20Gianni%20Franchi%0AAbstract%3A%20Two%20traditions%20of%20interpretability%20have%20evolved%20side%20by%20side%20but%20seldom%20spoken%20to%20each%20other%3A%20Concept%20Bottleneck%20Models%20%28CBMs%29%2C%20which%20prescribe%20what%20a%20concept%20should%20be%2C%20and%20Sparse%20Autoencoders%20%28SAEs%29%2C%20which%20discover%20what%20concepts%20emerge.%20While%20CBMs%20use%20supervision%20to%20align%20activations%20with%20human-labeled%20concepts%2C%20SAEs%20rely%20on%20sparse%20coding%20to%20uncover%20emergent%20ones.%20We%20show%20that%20both%20paradigms%20instantiate%20the%20same%20geometric%20structure%3A%20each%20learns%20a%20set%20of%20linear%20directions%20in%20activation%20space%20whose%20nonnegative%20combinations%20form%20a%20concept%20cone.%20Supervised%20and%20unsupervised%20methods%20thus%20differ%20not%20in%20kind%20but%20in%20how%20they%20select%20this%20cone.%20Building%20on%20this%20view%2C%20we%20propose%20an%20operational%20bridge%20between%20the%20two%20paradigms.%20CBMs%20provide%20human-defined%20reference%20geometries%2C%20while%20SAEs%20can%20be%20evaluated%20by%20how%20well%20their%20learned%20cones%20approximate%20or%20contain%20those%20of%20CBMs.%20This%20containment%20framework%20yields%20quantitative%20metrics%20linking%20inductive%20biases%20--%20such%20as%20SAE%20type%2C%20sparsity%2C%20or%20expansion%20ratio%20--%20to%20emergence%20of%20plausible%5Cfootnote%7BWe%20adopt%20the%20terminology%20of%20%5Ccitet%7Bjacovi2020towards%7D%2C%20who%20distinguish%20between%20faithful%20explanations%20%28accurately%20reflecting%20model%20computations%29%20and%20plausible%20explanations%20%28aligning%20with%20human%20intuition%20and%20domain%20knowledge%29.%20CBM%20concepts%20are%20plausible%20by%20construction%20--%20selected%20or%20annotated%20by%20humans%20--%20though%20not%20necessarily%20faithful%20to%20the%20true%20latent%20factors%20that%20organise%20the%20data%20manifold.%7D%20concepts.%20Using%20these%20metrics%2C%20we%20uncover%20a%20%60%60sweet%20spot%27%27%20in%20both%20sparsity%20and%20expansion%20factor%20that%20maximizes%20both%20geometric%20and%20semantic%20alignment%20with%20CBM%20concepts.%20Overall%2C%20our%20work%20unifies%20supervised%20and%20unsupervised%20concept%20discovery%20through%20a%20shared%20geometric%20framework%2C%20providing%20principled%20metrics%20to%20measure%20SAE%20progress%20and%20assess%20how%20well%20discovered%20concept%20align%20with%20plausible%20human%20concepts.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07355v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Geometric%2520Unification%2520of%2520Concept%2520Learning%2520with%2520Concept%2520Cones%26entry.906535625%3DAlexandre%2520Rocchi--Henry%2520and%2520Thomas%2520Fel%2520and%2520Gianni%2520Franchi%26entry.1292438233%3DTwo%2520traditions%2520of%2520interpretability%2520have%2520evolved%2520side%2520by%2520side%2520but%2520seldom%2520spoken%2520to%2520each%2520other%253A%2520Concept%2520Bottleneck%2520Models%2520%2528CBMs%2529%252C%2520which%2520prescribe%2520what%2520a%2520concept%2520should%2520be%252C%2520and%2520Sparse%2520Autoencoders%2520%2528SAEs%2529%252C%2520which%2520discover%2520what%2520concepts%2520emerge.%2520While%2520CBMs%2520use%2520supervision%2520to%2520align%2520activations%2520with%2520human-labeled%2520concepts%252C%2520SAEs%2520rely%2520on%2520sparse%2520coding%2520to%2520uncover%2520emergent%2520ones.%2520We%2520show%2520that%2520both%2520paradigms%2520instantiate%2520the%2520same%2520geometric%2520structure%253A%2520each%2520learns%2520a%2520set%2520of%2520linear%2520directions%2520in%2520activation%2520space%2520whose%2520nonnegative%2520combinations%2520form%2520a%2520concept%2520cone.%2520Supervised%2520and%2520unsupervised%2520methods%2520thus%2520differ%2520not%2520in%2520kind%2520but%2520in%2520how%2520they%2520select%2520this%2520cone.%2520Building%2520on%2520this%2520view%252C%2520we%2520propose%2520an%2520operational%2520bridge%2520between%2520the%2520two%2520paradigms.%2520CBMs%2520provide%2520human-defined%2520reference%2520geometries%252C%2520while%2520SAEs%2520can%2520be%2520evaluated%2520by%2520how%2520well%2520their%2520learned%2520cones%2520approximate%2520or%2520contain%2520those%2520of%2520CBMs.%2520This%2520containment%2520framework%2520yields%2520quantitative%2520metrics%2520linking%2520inductive%2520biases%2520--%2520such%2520as%2520SAE%2520type%252C%2520sparsity%252C%2520or%2520expansion%2520ratio%2520--%2520to%2520emergence%2520of%2520plausible%255Cfootnote%257BWe%2520adopt%2520the%2520terminology%2520of%2520%255Ccitet%257Bjacovi2020towards%257D%252C%2520who%2520distinguish%2520between%2520faithful%2520explanations%2520%2528accurately%2520reflecting%2520model%2520computations%2529%2520and%2520plausible%2520explanations%2520%2528aligning%2520with%2520human%2520intuition%2520and%2520domain%2520knowledge%2529.%2520CBM%2520concepts%2520are%2520plausible%2520by%2520construction%2520--%2520selected%2520or%2520annotated%2520by%2520humans%2520--%2520though%2520not%2520necessarily%2520faithful%2520to%2520the%2520true%2520latent%2520factors%2520that%2520organise%2520the%2520data%2520manifold.%257D%2520concepts.%2520Using%2520these%2520metrics%252C%2520we%2520uncover%2520a%2520%2560%2560sweet%2520spot%2527%2527%2520in%2520both%2520sparsity%2520and%2520expansion%2520factor%2520that%2520maximizes%2520both%2520geometric%2520and%2520semantic%2520alignment%2520with%2520CBM%2520concepts.%2520Overall%252C%2520our%2520work%2520unifies%2520supervised%2520and%2520unsupervised%2520concept%2520discovery%2520through%2520a%2520shared%2520geometric%2520framework%252C%2520providing%2520principled%2520metrics%2520to%2520measure%2520SAE%2520progress%2520and%2520assess%2520how%2520well%2520discovered%2520concept%2520align%2520with%2520plausible%2520human%2520concepts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07355v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Geometric%20Unification%20of%20Concept%20Learning%20with%20Concept%20Cones&entry.906535625=Alexandre%20Rocchi--Henry%20and%20Thomas%20Fel%20and%20Gianni%20Franchi&entry.1292438233=Two%20traditions%20of%20interpretability%20have%20evolved%20side%20by%20side%20but%20seldom%20spoken%20to%20each%20other%3A%20Concept%20Bottleneck%20Models%20%28CBMs%29%2C%20which%20prescribe%20what%20a%20concept%20should%20be%2C%20and%20Sparse%20Autoencoders%20%28SAEs%29%2C%20which%20discover%20what%20concepts%20emerge.%20While%20CBMs%20use%20supervision%20to%20align%20activations%20with%20human-labeled%20concepts%2C%20SAEs%20rely%20on%20sparse%20coding%20to%20uncover%20emergent%20ones.%20We%20show%20that%20both%20paradigms%20instantiate%20the%20same%20geometric%20structure%3A%20each%20learns%20a%20set%20of%20linear%20directions%20in%20activation%20space%20whose%20nonnegative%20combinations%20form%20a%20concept%20cone.%20Supervised%20and%20unsupervised%20methods%20thus%20differ%20not%20in%20kind%20but%20in%20how%20they%20select%20this%20cone.%20Building%20on%20this%20view%2C%20we%20propose%20an%20operational%20bridge%20between%20the%20two%20paradigms.%20CBMs%20provide%20human-defined%20reference%20geometries%2C%20while%20SAEs%20can%20be%20evaluated%20by%20how%20well%20their%20learned%20cones%20approximate%20or%20contain%20those%20of%20CBMs.%20This%20containment%20framework%20yields%20quantitative%20metrics%20linking%20inductive%20biases%20--%20such%20as%20SAE%20type%2C%20sparsity%2C%20or%20expansion%20ratio%20--%20to%20emergence%20of%20plausible%5Cfootnote%7BWe%20adopt%20the%20terminology%20of%20%5Ccitet%7Bjacovi2020towards%7D%2C%20who%20distinguish%20between%20faithful%20explanations%20%28accurately%20reflecting%20model%20computations%29%20and%20plausible%20explanations%20%28aligning%20with%20human%20intuition%20and%20domain%20knowledge%29.%20CBM%20concepts%20are%20plausible%20by%20construction%20--%20selected%20or%20annotated%20by%20humans%20--%20though%20not%20necessarily%20faithful%20to%20the%20true%20latent%20factors%20that%20organise%20the%20data%20manifold.%7D%20concepts.%20Using%20these%20metrics%2C%20we%20uncover%20a%20%60%60sweet%20spot%27%27%20in%20both%20sparsity%20and%20expansion%20factor%20that%20maximizes%20both%20geometric%20and%20semantic%20alignment%20with%20CBM%20concepts.%20Overall%2C%20our%20work%20unifies%20supervised%20and%20unsupervised%20concept%20discovery%20through%20a%20shared%20geometric%20framework%2C%20providing%20principled%20metrics%20to%20measure%20SAE%20progress%20and%20assess%20how%20well%20discovered%20concept%20align%20with%20plausible%20human%20concepts.&entry.1838667208=http%3A//arxiv.org/abs/2512.07355v1&entry.124074799=Read"},
{"title": "Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation", "author": "Qiming Huang and Hao Ai and Jianbo Jiao", "abstract": "Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.", "link": "http://arxiv.org/abs/2512.07360v1", "date": "2025-12-08", "relevancy": 2.7523, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5592}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structure-Aware%20Feature%20Rectification%20with%20Region%20Adjacency%20Graphs%20for%20Training-Free%20Open-Vocabulary%20Semantic%20Segmentation&body=Title%3A%20Structure-Aware%20Feature%20Rectification%20with%20Region%20Adjacency%20Graphs%20for%20Training-Free%20Open-Vocabulary%20Semantic%20Segmentation%0AAuthor%3A%20Qiming%20Huang%20and%20Hao%20Ai%20and%20Jianbo%20Jiao%0AAbstract%3A%20Benefiting%20from%20the%20inductive%20biases%20learned%20from%20large-scale%20datasets%2C%20open-vocabulary%20semantic%20segmentation%20%28OVSS%29%20leverages%20the%20power%20of%20vision-language%20models%2C%20such%20as%20CLIP%2C%20to%20achieve%20remarkable%20progress%20without%20requiring%20task-specific%20training.%20However%2C%20due%20to%20CLIP%27s%20pre-training%20nature%20on%20image-text%20pairs%2C%20it%20tends%20to%20focus%20on%20global%20semantic%20alignment%2C%20resulting%20in%20suboptimal%20performance%20when%20associating%20fine-grained%20visual%20regions%20with%20text.%20This%20leads%20to%20noisy%20and%20inconsistent%20predictions%2C%20particularly%20in%20local%20areas.%20We%20attribute%20this%20to%20a%20dispersed%20bias%20stemming%20from%20its%20contrastive%20training%20paradigm%2C%20which%20is%20difficult%20to%20alleviate%20using%20CLIP%20features%20alone.%20To%20address%20this%2C%20we%20propose%20a%20structure-aware%20feature%20rectification%20approach%20that%20incorporates%20instance-specific%20priors%20derived%20directly%20from%20the%20image.%20Specifically%2C%20we%20construct%20a%20region%20adjacency%20graph%20%28RAG%29%20based%20on%20low-level%20features%20%28e.g.%2C%20colour%20and%20texture%29%20to%20capture%20local%20structural%20relationships%20and%20use%20it%20to%20refine%20CLIP%20features%20by%20enhancing%20local%20discrimination.%20Extensive%20experiments%20show%20that%20our%20method%20effectively%20suppresses%20segmentation%20noise%2C%20improves%20region-level%20consistency%2C%20and%20achieves%20strong%20performance%20on%20multiple%20open-vocabulary%20segmentation%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructure-Aware%2520Feature%2520Rectification%2520with%2520Region%2520Adjacency%2520Graphs%2520for%2520Training-Free%2520Open-Vocabulary%2520Semantic%2520Segmentation%26entry.906535625%3DQiming%2520Huang%2520and%2520Hao%2520Ai%2520and%2520Jianbo%2520Jiao%26entry.1292438233%3DBenefiting%2520from%2520the%2520inductive%2520biases%2520learned%2520from%2520large-scale%2520datasets%252C%2520open-vocabulary%2520semantic%2520segmentation%2520%2528OVSS%2529%2520leverages%2520the%2520power%2520of%2520vision-language%2520models%252C%2520such%2520as%2520CLIP%252C%2520to%2520achieve%2520remarkable%2520progress%2520without%2520requiring%2520task-specific%2520training.%2520However%252C%2520due%2520to%2520CLIP%2527s%2520pre-training%2520nature%2520on%2520image-text%2520pairs%252C%2520it%2520tends%2520to%2520focus%2520on%2520global%2520semantic%2520alignment%252C%2520resulting%2520in%2520suboptimal%2520performance%2520when%2520associating%2520fine-grained%2520visual%2520regions%2520with%2520text.%2520This%2520leads%2520to%2520noisy%2520and%2520inconsistent%2520predictions%252C%2520particularly%2520in%2520local%2520areas.%2520We%2520attribute%2520this%2520to%2520a%2520dispersed%2520bias%2520stemming%2520from%2520its%2520contrastive%2520training%2520paradigm%252C%2520which%2520is%2520difficult%2520to%2520alleviate%2520using%2520CLIP%2520features%2520alone.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520structure-aware%2520feature%2520rectification%2520approach%2520that%2520incorporates%2520instance-specific%2520priors%2520derived%2520directly%2520from%2520the%2520image.%2520Specifically%252C%2520we%2520construct%2520a%2520region%2520adjacency%2520graph%2520%2528RAG%2529%2520based%2520on%2520low-level%2520features%2520%2528e.g.%252C%2520colour%2520and%2520texture%2529%2520to%2520capture%2520local%2520structural%2520relationships%2520and%2520use%2520it%2520to%2520refine%2520CLIP%2520features%2520by%2520enhancing%2520local%2520discrimination.%2520Extensive%2520experiments%2520show%2520that%2520our%2520method%2520effectively%2520suppresses%2520segmentation%2520noise%252C%2520improves%2520region-level%2520consistency%252C%2520and%2520achieves%2520strong%2520performance%2520on%2520multiple%2520open-vocabulary%2520segmentation%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structure-Aware%20Feature%20Rectification%20with%20Region%20Adjacency%20Graphs%20for%20Training-Free%20Open-Vocabulary%20Semantic%20Segmentation&entry.906535625=Qiming%20Huang%20and%20Hao%20Ai%20and%20Jianbo%20Jiao&entry.1292438233=Benefiting%20from%20the%20inductive%20biases%20learned%20from%20large-scale%20datasets%2C%20open-vocabulary%20semantic%20segmentation%20%28OVSS%29%20leverages%20the%20power%20of%20vision-language%20models%2C%20such%20as%20CLIP%2C%20to%20achieve%20remarkable%20progress%20without%20requiring%20task-specific%20training.%20However%2C%20due%20to%20CLIP%27s%20pre-training%20nature%20on%20image-text%20pairs%2C%20it%20tends%20to%20focus%20on%20global%20semantic%20alignment%2C%20resulting%20in%20suboptimal%20performance%20when%20associating%20fine-grained%20visual%20regions%20with%20text.%20This%20leads%20to%20noisy%20and%20inconsistent%20predictions%2C%20particularly%20in%20local%20areas.%20We%20attribute%20this%20to%20a%20dispersed%20bias%20stemming%20from%20its%20contrastive%20training%20paradigm%2C%20which%20is%20difficult%20to%20alleviate%20using%20CLIP%20features%20alone.%20To%20address%20this%2C%20we%20propose%20a%20structure-aware%20feature%20rectification%20approach%20that%20incorporates%20instance-specific%20priors%20derived%20directly%20from%20the%20image.%20Specifically%2C%20we%20construct%20a%20region%20adjacency%20graph%20%28RAG%29%20based%20on%20low-level%20features%20%28e.g.%2C%20colour%20and%20texture%29%20to%20capture%20local%20structural%20relationships%20and%20use%20it%20to%20refine%20CLIP%20features%20by%20enhancing%20local%20discrimination.%20Extensive%20experiments%20show%20that%20our%20method%20effectively%20suppresses%20segmentation%20noise%2C%20improves%20region-level%20consistency%2C%20and%20achieves%20strong%20performance%20on%20multiple%20open-vocabulary%20segmentation%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2512.07360v1&entry.124074799=Read"},
{"title": "All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs", "author": "Yahong Wang and Juncheng Wu and Zhangkai Ni and Longzhen Yang and Yihang Liu and Chengmei Yang and Ying Wen and Xianfeng Tang and Hui Liu and Yuyin Zhou and Lianghua He", "abstract": "Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by \"vanishing token information\", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as \"information horizon\", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.", "link": "http://arxiv.org/abs/2512.07580v1", "date": "2025-12-08", "relevancy": 2.7246, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5549}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.54}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.54}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20All%20You%20Need%20Are%20Random%20Visual%20Tokens%3F%20Demystifying%20Token%20Pruning%20in%20VLLMs&body=Title%3A%20All%20You%20Need%20Are%20Random%20Visual%20Tokens%3F%20Demystifying%20Token%20Pruning%20in%20VLLMs%0AAuthor%3A%20Yahong%20Wang%20and%20Juncheng%20Wu%20and%20Zhangkai%20Ni%20and%20Longzhen%20Yang%20and%20Yihang%20Liu%20and%20Chengmei%20Yang%20and%20Ying%20Wen%20and%20Xianfeng%20Tang%20and%20Hui%20Liu%20and%20Yuyin%20Zhou%20and%20Lianghua%20He%0AAbstract%3A%20Vision%20Large%20Language%20Models%20%28VLLMs%29%20incur%20high%20computational%20costs%20due%20to%20their%20reliance%20on%20hundreds%20of%20visual%20tokens%20to%20represent%20images.%20While%20token%20pruning%20offers%20a%20promising%20solution%20for%20accelerating%20inference%2C%20this%20paper%2C%20however%2C%20identifies%20a%20key%20observation%3A%20in%20deeper%20layers%20%28e.g.%2C%20beyond%20the%2020th%29%2C%20existing%20training-free%20pruning%20methods%20perform%20no%20better%20than%20random%20pruning.%20We%20hypothesize%20that%20this%20degradation%20is%20caused%20by%20%22vanishing%20token%20information%22%2C%20where%20visual%20tokens%20progressively%20lose%20their%20salience%20with%20increasing%20network%20depth.%20To%20validate%20this%20hypothesis%2C%20we%20quantify%20a%20token%27s%20information%20content%20by%20measuring%20the%20change%20in%20the%20model%20output%20probabilities%20upon%20its%20removal.%20Using%20this%20proposed%20metric%2C%20our%20analysis%20of%20the%20information%20of%20visual%20tokens%20across%20layers%20reveals%20three%20key%20findings%3A%20%281%29%20As%20layers%20deepen%2C%20the%20information%20of%20visual%20tokens%20gradually%20becomes%20uniform%20and%20eventually%20vanishes%20at%20an%20intermediate%20layer%2C%20which%20we%20term%20as%20%22information%20horizon%22%2C%20beyond%20which%20the%20visual%20tokens%20become%20redundant%3B%20%282%29%20The%20position%20of%20this%20horizon%20is%20not%20static%3B%20it%20extends%20deeper%20for%20visually%20intensive%20tasks%2C%20such%20as%20Optical%20Character%20Recognition%20%28OCR%29%2C%20compared%20to%20more%20general%20tasks%20like%20Visual%20Question%20Answering%20%28VQA%29%3B%20%283%29%20This%20horizon%20is%20also%20strongly%20correlated%20with%20model%20capacity%2C%20as%20stronger%20VLLMs%20%28e.g.%2C%20Qwen2.5-VL%29%20employ%20deeper%20visual%20tokens%20than%20weaker%20models%20%28e.g.%2C%20LLaVA-1.5%29.%20Based%20on%20our%20findings%2C%20we%20show%20that%20simple%20random%20pruning%20in%20deep%20layers%20efficiently%20balances%20performance%20and%20efficiency.%20Moreover%2C%20integrating%20random%20pruning%20consistently%20enhances%20existing%20methods.%20Using%20DivPrune%20with%20random%20pruning%20achieves%20state-of-the-art%20results%2C%20maintaining%2096.9%25%20of%20Qwen-2.5-VL-7B%20performance%20while%20pruning%2050%25%20of%20visual%20tokens.%20The%20code%20will%20be%20publicly%20available%20at%20https%3A//github.com/YahongWang1/Information-Horizon.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAll%2520You%2520Need%2520Are%2520Random%2520Visual%2520Tokens%253F%2520Demystifying%2520Token%2520Pruning%2520in%2520VLLMs%26entry.906535625%3DYahong%2520Wang%2520and%2520Juncheng%2520Wu%2520and%2520Zhangkai%2520Ni%2520and%2520Longzhen%2520Yang%2520and%2520Yihang%2520Liu%2520and%2520Chengmei%2520Yang%2520and%2520Ying%2520Wen%2520and%2520Xianfeng%2520Tang%2520and%2520Hui%2520Liu%2520and%2520Yuyin%2520Zhou%2520and%2520Lianghua%2520He%26entry.1292438233%3DVision%2520Large%2520Language%2520Models%2520%2528VLLMs%2529%2520incur%2520high%2520computational%2520costs%2520due%2520to%2520their%2520reliance%2520on%2520hundreds%2520of%2520visual%2520tokens%2520to%2520represent%2520images.%2520While%2520token%2520pruning%2520offers%2520a%2520promising%2520solution%2520for%2520accelerating%2520inference%252C%2520this%2520paper%252C%2520however%252C%2520identifies%2520a%2520key%2520observation%253A%2520in%2520deeper%2520layers%2520%2528e.g.%252C%2520beyond%2520the%252020th%2529%252C%2520existing%2520training-free%2520pruning%2520methods%2520perform%2520no%2520better%2520than%2520random%2520pruning.%2520We%2520hypothesize%2520that%2520this%2520degradation%2520is%2520caused%2520by%2520%2522vanishing%2520token%2520information%2522%252C%2520where%2520visual%2520tokens%2520progressively%2520lose%2520their%2520salience%2520with%2520increasing%2520network%2520depth.%2520To%2520validate%2520this%2520hypothesis%252C%2520we%2520quantify%2520a%2520token%2527s%2520information%2520content%2520by%2520measuring%2520the%2520change%2520in%2520the%2520model%2520output%2520probabilities%2520upon%2520its%2520removal.%2520Using%2520this%2520proposed%2520metric%252C%2520our%2520analysis%2520of%2520the%2520information%2520of%2520visual%2520tokens%2520across%2520layers%2520reveals%2520three%2520key%2520findings%253A%2520%25281%2529%2520As%2520layers%2520deepen%252C%2520the%2520information%2520of%2520visual%2520tokens%2520gradually%2520becomes%2520uniform%2520and%2520eventually%2520vanishes%2520at%2520an%2520intermediate%2520layer%252C%2520which%2520we%2520term%2520as%2520%2522information%2520horizon%2522%252C%2520beyond%2520which%2520the%2520visual%2520tokens%2520become%2520redundant%253B%2520%25282%2529%2520The%2520position%2520of%2520this%2520horizon%2520is%2520not%2520static%253B%2520it%2520extends%2520deeper%2520for%2520visually%2520intensive%2520tasks%252C%2520such%2520as%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%252C%2520compared%2520to%2520more%2520general%2520tasks%2520like%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%253B%2520%25283%2529%2520This%2520horizon%2520is%2520also%2520strongly%2520correlated%2520with%2520model%2520capacity%252C%2520as%2520stronger%2520VLLMs%2520%2528e.g.%252C%2520Qwen2.5-VL%2529%2520employ%2520deeper%2520visual%2520tokens%2520than%2520weaker%2520models%2520%2528e.g.%252C%2520LLaVA-1.5%2529.%2520Based%2520on%2520our%2520findings%252C%2520we%2520show%2520that%2520simple%2520random%2520pruning%2520in%2520deep%2520layers%2520efficiently%2520balances%2520performance%2520and%2520efficiency.%2520Moreover%252C%2520integrating%2520random%2520pruning%2520consistently%2520enhances%2520existing%2520methods.%2520Using%2520DivPrune%2520with%2520random%2520pruning%2520achieves%2520state-of-the-art%2520results%252C%2520maintaining%252096.9%2525%2520of%2520Qwen-2.5-VL-7B%2520performance%2520while%2520pruning%252050%2525%2520of%2520visual%2520tokens.%2520The%2520code%2520will%2520be%2520publicly%2520available%2520at%2520https%253A//github.com/YahongWang1/Information-Horizon.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=All%20You%20Need%20Are%20Random%20Visual%20Tokens%3F%20Demystifying%20Token%20Pruning%20in%20VLLMs&entry.906535625=Yahong%20Wang%20and%20Juncheng%20Wu%20and%20Zhangkai%20Ni%20and%20Longzhen%20Yang%20and%20Yihang%20Liu%20and%20Chengmei%20Yang%20and%20Ying%20Wen%20and%20Xianfeng%20Tang%20and%20Hui%20Liu%20and%20Yuyin%20Zhou%20and%20Lianghua%20He&entry.1292438233=Vision%20Large%20Language%20Models%20%28VLLMs%29%20incur%20high%20computational%20costs%20due%20to%20their%20reliance%20on%20hundreds%20of%20visual%20tokens%20to%20represent%20images.%20While%20token%20pruning%20offers%20a%20promising%20solution%20for%20accelerating%20inference%2C%20this%20paper%2C%20however%2C%20identifies%20a%20key%20observation%3A%20in%20deeper%20layers%20%28e.g.%2C%20beyond%20the%2020th%29%2C%20existing%20training-free%20pruning%20methods%20perform%20no%20better%20than%20random%20pruning.%20We%20hypothesize%20that%20this%20degradation%20is%20caused%20by%20%22vanishing%20token%20information%22%2C%20where%20visual%20tokens%20progressively%20lose%20their%20salience%20with%20increasing%20network%20depth.%20To%20validate%20this%20hypothesis%2C%20we%20quantify%20a%20token%27s%20information%20content%20by%20measuring%20the%20change%20in%20the%20model%20output%20probabilities%20upon%20its%20removal.%20Using%20this%20proposed%20metric%2C%20our%20analysis%20of%20the%20information%20of%20visual%20tokens%20across%20layers%20reveals%20three%20key%20findings%3A%20%281%29%20As%20layers%20deepen%2C%20the%20information%20of%20visual%20tokens%20gradually%20becomes%20uniform%20and%20eventually%20vanishes%20at%20an%20intermediate%20layer%2C%20which%20we%20term%20as%20%22information%20horizon%22%2C%20beyond%20which%20the%20visual%20tokens%20become%20redundant%3B%20%282%29%20The%20position%20of%20this%20horizon%20is%20not%20static%3B%20it%20extends%20deeper%20for%20visually%20intensive%20tasks%2C%20such%20as%20Optical%20Character%20Recognition%20%28OCR%29%2C%20compared%20to%20more%20general%20tasks%20like%20Visual%20Question%20Answering%20%28VQA%29%3B%20%283%29%20This%20horizon%20is%20also%20strongly%20correlated%20with%20model%20capacity%2C%20as%20stronger%20VLLMs%20%28e.g.%2C%20Qwen2.5-VL%29%20employ%20deeper%20visual%20tokens%20than%20weaker%20models%20%28e.g.%2C%20LLaVA-1.5%29.%20Based%20on%20our%20findings%2C%20we%20show%20that%20simple%20random%20pruning%20in%20deep%20layers%20efficiently%20balances%20performance%20and%20efficiency.%20Moreover%2C%20integrating%20random%20pruning%20consistently%20enhances%20existing%20methods.%20Using%20DivPrune%20with%20random%20pruning%20achieves%20state-of-the-art%20results%2C%20maintaining%2096.9%25%20of%20Qwen-2.5-VL-7B%20performance%20while%20pruning%2050%25%20of%20visual%20tokens.%20The%20code%20will%20be%20publicly%20available%20at%20https%3A//github.com/YahongWang1/Information-Horizon.&entry.1838667208=http%3A//arxiv.org/abs/2512.07580v1&entry.124074799=Read"},
{"title": "Voxify3D: Pixel Art Meets Volumetric Rendering", "author": "Yi-Chuan Huang and Jiewen Chan and Hao-Jen Chien and Yu-Lun Liu", "abstract": "Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/", "link": "http://arxiv.org/abs/2512.07834v1", "date": "2025-12-08", "relevancy": 2.7243, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5533}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5533}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Voxify3D%3A%20Pixel%20Art%20Meets%20Volumetric%20Rendering&body=Title%3A%20Voxify3D%3A%20Pixel%20Art%20Meets%20Volumetric%20Rendering%0AAuthor%3A%20Yi-Chuan%20Huang%20and%20Jiewen%20Chan%20and%20Hao-Jen%20Chien%20and%20Yu-Lun%20Liu%0AAbstract%3A%20Voxel%20art%20is%20a%20distinctive%20stylization%20widely%20used%20in%20games%20and%20digital%20media%2C%20yet%20automated%20generation%20from%203D%20meshes%20remains%20challenging%20due%20to%20conflicting%20requirements%20of%20geometric%20abstraction%2C%20semantic%20preservation%2C%20and%20discrete%20color%20coherence.%20Existing%20methods%20either%20over-simplify%20geometry%20or%20fail%20to%20achieve%20the%20pixel-precise%2C%20palette-constrained%20aesthetics%20of%20voxel%20art.%20We%20introduce%20Voxify3D%2C%20a%20differentiable%20two-stage%20framework%20bridging%203D%20mesh%20optimization%20with%202D%20pixel%20art%20supervision.%20Our%20core%20innovation%20lies%20in%20the%20synergistic%20integration%20of%20three%20components%3A%20%281%29%20orthographic%20pixel%20art%20supervision%20that%20eliminates%20perspective%20distortion%20for%20precise%20voxel-pixel%20alignment%3B%20%282%29%20patch-based%20CLIP%20alignment%20that%20preserves%20semantics%20across%20discretization%20levels%3B%20%283%29%20palette-constrained%20Gumbel-Softmax%20quantization%20enabling%20differentiable%20optimization%20over%20discrete%20color%20spaces%20with%20controllable%20palette%20strategies.%20This%20integration%20addresses%20fundamental%20challenges%3A%20semantic%20preservation%20under%20extreme%20discretization%2C%20pixel-art%20aesthetics%20through%20volumetric%20rendering%2C%20and%20end-to-end%20discrete%20optimization.%20Experiments%20show%20superior%20performance%20%2837.12%20CLIP-IQA%2C%2077.90%5C%25%20user%20preference%29%20across%20diverse%20characters%20and%20controllable%20abstraction%20%282-8%20colors%2C%2020x-50x%20resolutions%29.%20Project%20page%3A%20https%3A//yichuanh.github.io/Voxify-3D/%0ALink%3A%20http%3A//arxiv.org/abs/2512.07834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoxify3D%253A%2520Pixel%2520Art%2520Meets%2520Volumetric%2520Rendering%26entry.906535625%3DYi-Chuan%2520Huang%2520and%2520Jiewen%2520Chan%2520and%2520Hao-Jen%2520Chien%2520and%2520Yu-Lun%2520Liu%26entry.1292438233%3DVoxel%2520art%2520is%2520a%2520distinctive%2520stylization%2520widely%2520used%2520in%2520games%2520and%2520digital%2520media%252C%2520yet%2520automated%2520generation%2520from%25203D%2520meshes%2520remains%2520challenging%2520due%2520to%2520conflicting%2520requirements%2520of%2520geometric%2520abstraction%252C%2520semantic%2520preservation%252C%2520and%2520discrete%2520color%2520coherence.%2520Existing%2520methods%2520either%2520over-simplify%2520geometry%2520or%2520fail%2520to%2520achieve%2520the%2520pixel-precise%252C%2520palette-constrained%2520aesthetics%2520of%2520voxel%2520art.%2520We%2520introduce%2520Voxify3D%252C%2520a%2520differentiable%2520two-stage%2520framework%2520bridging%25203D%2520mesh%2520optimization%2520with%25202D%2520pixel%2520art%2520supervision.%2520Our%2520core%2520innovation%2520lies%2520in%2520the%2520synergistic%2520integration%2520of%2520three%2520components%253A%2520%25281%2529%2520orthographic%2520pixel%2520art%2520supervision%2520that%2520eliminates%2520perspective%2520distortion%2520for%2520precise%2520voxel-pixel%2520alignment%253B%2520%25282%2529%2520patch-based%2520CLIP%2520alignment%2520that%2520preserves%2520semantics%2520across%2520discretization%2520levels%253B%2520%25283%2529%2520palette-constrained%2520Gumbel-Softmax%2520quantization%2520enabling%2520differentiable%2520optimization%2520over%2520discrete%2520color%2520spaces%2520with%2520controllable%2520palette%2520strategies.%2520This%2520integration%2520addresses%2520fundamental%2520challenges%253A%2520semantic%2520preservation%2520under%2520extreme%2520discretization%252C%2520pixel-art%2520aesthetics%2520through%2520volumetric%2520rendering%252C%2520and%2520end-to-end%2520discrete%2520optimization.%2520Experiments%2520show%2520superior%2520performance%2520%252837.12%2520CLIP-IQA%252C%252077.90%255C%2525%2520user%2520preference%2529%2520across%2520diverse%2520characters%2520and%2520controllable%2520abstraction%2520%25282-8%2520colors%252C%252020x-50x%2520resolutions%2529.%2520Project%2520page%253A%2520https%253A//yichuanh.github.io/Voxify-3D/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Voxify3D%3A%20Pixel%20Art%20Meets%20Volumetric%20Rendering&entry.906535625=Yi-Chuan%20Huang%20and%20Jiewen%20Chan%20and%20Hao-Jen%20Chien%20and%20Yu-Lun%20Liu&entry.1292438233=Voxel%20art%20is%20a%20distinctive%20stylization%20widely%20used%20in%20games%20and%20digital%20media%2C%20yet%20automated%20generation%20from%203D%20meshes%20remains%20challenging%20due%20to%20conflicting%20requirements%20of%20geometric%20abstraction%2C%20semantic%20preservation%2C%20and%20discrete%20color%20coherence.%20Existing%20methods%20either%20over-simplify%20geometry%20or%20fail%20to%20achieve%20the%20pixel-precise%2C%20palette-constrained%20aesthetics%20of%20voxel%20art.%20We%20introduce%20Voxify3D%2C%20a%20differentiable%20two-stage%20framework%20bridging%203D%20mesh%20optimization%20with%202D%20pixel%20art%20supervision.%20Our%20core%20innovation%20lies%20in%20the%20synergistic%20integration%20of%20three%20components%3A%20%281%29%20orthographic%20pixel%20art%20supervision%20that%20eliminates%20perspective%20distortion%20for%20precise%20voxel-pixel%20alignment%3B%20%282%29%20patch-based%20CLIP%20alignment%20that%20preserves%20semantics%20across%20discretization%20levels%3B%20%283%29%20palette-constrained%20Gumbel-Softmax%20quantization%20enabling%20differentiable%20optimization%20over%20discrete%20color%20spaces%20with%20controllable%20palette%20strategies.%20This%20integration%20addresses%20fundamental%20challenges%3A%20semantic%20preservation%20under%20extreme%20discretization%2C%20pixel-art%20aesthetics%20through%20volumetric%20rendering%2C%20and%20end-to-end%20discrete%20optimization.%20Experiments%20show%20superior%20performance%20%2837.12%20CLIP-IQA%2C%2077.90%5C%25%20user%20preference%29%20across%20diverse%20characters%20and%20controllable%20abstraction%20%282-8%20colors%2C%2020x-50x%20resolutions%29.%20Project%20page%3A%20https%3A//yichuanh.github.io/Voxify-3D/&entry.1838667208=http%3A//arxiv.org/abs/2512.07834v1&entry.124074799=Read"},
{"title": "Relational Visual Similarity", "author": "Thao Nguyen and Sicheng Mo and Krishna Kumar Singh and Yilin Wang and Jing Shi and Nicholas Kolkin and Eli Shechtman and Yong Jae Lee and Yuheng Li", "abstract": "Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.", "link": "http://arxiv.org/abs/2512.07833v1", "date": "2025-12-08", "relevancy": 2.6987, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5511}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relational%20Visual%20Similarity&body=Title%3A%20Relational%20Visual%20Similarity%0AAuthor%3A%20Thao%20Nguyen%20and%20Sicheng%20Mo%20and%20Krishna%20Kumar%20Singh%20and%20Yilin%20Wang%20and%20Jing%20Shi%20and%20Nicholas%20Kolkin%20and%20Eli%20Shechtman%20and%20Yong%20Jae%20Lee%20and%20Yuheng%20Li%0AAbstract%3A%20Humans%20do%20not%20just%20see%20attribute%20similarity%20--%20we%20also%20see%20relational%20similarity.%20An%20apple%20is%20like%20a%20peach%20because%20both%20are%20reddish%20fruit%2C%20but%20the%20Earth%20is%20also%20like%20a%20peach%3A%20its%20crust%2C%20mantle%2C%20and%20core%20correspond%20to%20the%20peach%27s%20skin%2C%20flesh%2C%20and%20pit.%20This%20ability%20to%20perceive%20and%20recognize%20relational%20similarity%2C%20is%20arguable%20by%20cognitive%20scientist%20to%20be%20what%20distinguishes%20humans%20from%20other%20species.%20Yet%2C%20all%20widely%20used%20visual%20similarity%20metrics%20today%20%28e.g.%2C%20LPIPS%2C%20CLIP%2C%20DINO%29%20focus%20solely%20on%20perceptual%20attribute%20similarity%20and%20fail%20to%20capture%20the%20rich%2C%20often%20surprising%20relational%20similarities%20that%20humans%20perceive.%20How%20can%20we%20go%20beyond%20the%20visible%20content%20of%20an%20image%20to%20capture%20its%20relational%20properties%3F%20How%20can%20we%20bring%20images%20with%20the%20same%20relational%20logic%20closer%20together%20in%20representation%20space%3F%20To%20answer%20these%20questions%2C%20we%20first%20formulate%20relational%20image%20similarity%20as%20a%20measurable%20problem%3A%20two%20images%20are%20relationally%20similar%20when%20their%20internal%20relations%20or%20functions%20among%20visual%20elements%20correspond%2C%20even%20if%20their%20visual%20attributes%20differ.%20We%20then%20curate%20114k%20image-caption%20dataset%20in%20which%20the%20captions%20are%20anonymized%20--%20describing%20the%20underlying%20relational%20logic%20of%20the%20scene%20rather%20than%20its%20surface%20content.%20Using%20this%20dataset%2C%20we%20finetune%20a%20Vision-Language%20model%20to%20measure%20the%20relational%20similarity%20between%20images.%20This%20model%20serves%20as%20the%20first%20step%20toward%20connecting%20images%20by%20their%20underlying%20relational%20structure%20rather%20than%20their%20visible%20appearance.%20Our%20study%20shows%20that%20while%20relational%20similarity%20has%20a%20lot%20of%20real-world%20applications%2C%20existing%20image%20similarity%20models%20fail%20to%20capture%20it%20--%20revealing%20a%20critical%20gap%20in%20visual%20computing.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07833v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelational%2520Visual%2520Similarity%26entry.906535625%3DThao%2520Nguyen%2520and%2520Sicheng%2520Mo%2520and%2520Krishna%2520Kumar%2520Singh%2520and%2520Yilin%2520Wang%2520and%2520Jing%2520Shi%2520and%2520Nicholas%2520Kolkin%2520and%2520Eli%2520Shechtman%2520and%2520Yong%2520Jae%2520Lee%2520and%2520Yuheng%2520Li%26entry.1292438233%3DHumans%2520do%2520not%2520just%2520see%2520attribute%2520similarity%2520--%2520we%2520also%2520see%2520relational%2520similarity.%2520An%2520apple%2520is%2520like%2520a%2520peach%2520because%2520both%2520are%2520reddish%2520fruit%252C%2520but%2520the%2520Earth%2520is%2520also%2520like%2520a%2520peach%253A%2520its%2520crust%252C%2520mantle%252C%2520and%2520core%2520correspond%2520to%2520the%2520peach%2527s%2520skin%252C%2520flesh%252C%2520and%2520pit.%2520This%2520ability%2520to%2520perceive%2520and%2520recognize%2520relational%2520similarity%252C%2520is%2520arguable%2520by%2520cognitive%2520scientist%2520to%2520be%2520what%2520distinguishes%2520humans%2520from%2520other%2520species.%2520Yet%252C%2520all%2520widely%2520used%2520visual%2520similarity%2520metrics%2520today%2520%2528e.g.%252C%2520LPIPS%252C%2520CLIP%252C%2520DINO%2529%2520focus%2520solely%2520on%2520perceptual%2520attribute%2520similarity%2520and%2520fail%2520to%2520capture%2520the%2520rich%252C%2520often%2520surprising%2520relational%2520similarities%2520that%2520humans%2520perceive.%2520How%2520can%2520we%2520go%2520beyond%2520the%2520visible%2520content%2520of%2520an%2520image%2520to%2520capture%2520its%2520relational%2520properties%253F%2520How%2520can%2520we%2520bring%2520images%2520with%2520the%2520same%2520relational%2520logic%2520closer%2520together%2520in%2520representation%2520space%253F%2520To%2520answer%2520these%2520questions%252C%2520we%2520first%2520formulate%2520relational%2520image%2520similarity%2520as%2520a%2520measurable%2520problem%253A%2520two%2520images%2520are%2520relationally%2520similar%2520when%2520their%2520internal%2520relations%2520or%2520functions%2520among%2520visual%2520elements%2520correspond%252C%2520even%2520if%2520their%2520visual%2520attributes%2520differ.%2520We%2520then%2520curate%2520114k%2520image-caption%2520dataset%2520in%2520which%2520the%2520captions%2520are%2520anonymized%2520--%2520describing%2520the%2520underlying%2520relational%2520logic%2520of%2520the%2520scene%2520rather%2520than%2520its%2520surface%2520content.%2520Using%2520this%2520dataset%252C%2520we%2520finetune%2520a%2520Vision-Language%2520model%2520to%2520measure%2520the%2520relational%2520similarity%2520between%2520images.%2520This%2520model%2520serves%2520as%2520the%2520first%2520step%2520toward%2520connecting%2520images%2520by%2520their%2520underlying%2520relational%2520structure%2520rather%2520than%2520their%2520visible%2520appearance.%2520Our%2520study%2520shows%2520that%2520while%2520relational%2520similarity%2520has%2520a%2520lot%2520of%2520real-world%2520applications%252C%2520existing%2520image%2520similarity%2520models%2520fail%2520to%2520capture%2520it%2520--%2520revealing%2520a%2520critical%2520gap%2520in%2520visual%2520computing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07833v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relational%20Visual%20Similarity&entry.906535625=Thao%20Nguyen%20and%20Sicheng%20Mo%20and%20Krishna%20Kumar%20Singh%20and%20Yilin%20Wang%20and%20Jing%20Shi%20and%20Nicholas%20Kolkin%20and%20Eli%20Shechtman%20and%20Yong%20Jae%20Lee%20and%20Yuheng%20Li&entry.1292438233=Humans%20do%20not%20just%20see%20attribute%20similarity%20--%20we%20also%20see%20relational%20similarity.%20An%20apple%20is%20like%20a%20peach%20because%20both%20are%20reddish%20fruit%2C%20but%20the%20Earth%20is%20also%20like%20a%20peach%3A%20its%20crust%2C%20mantle%2C%20and%20core%20correspond%20to%20the%20peach%27s%20skin%2C%20flesh%2C%20and%20pit.%20This%20ability%20to%20perceive%20and%20recognize%20relational%20similarity%2C%20is%20arguable%20by%20cognitive%20scientist%20to%20be%20what%20distinguishes%20humans%20from%20other%20species.%20Yet%2C%20all%20widely%20used%20visual%20similarity%20metrics%20today%20%28e.g.%2C%20LPIPS%2C%20CLIP%2C%20DINO%29%20focus%20solely%20on%20perceptual%20attribute%20similarity%20and%20fail%20to%20capture%20the%20rich%2C%20often%20surprising%20relational%20similarities%20that%20humans%20perceive.%20How%20can%20we%20go%20beyond%20the%20visible%20content%20of%20an%20image%20to%20capture%20its%20relational%20properties%3F%20How%20can%20we%20bring%20images%20with%20the%20same%20relational%20logic%20closer%20together%20in%20representation%20space%3F%20To%20answer%20these%20questions%2C%20we%20first%20formulate%20relational%20image%20similarity%20as%20a%20measurable%20problem%3A%20two%20images%20are%20relationally%20similar%20when%20their%20internal%20relations%20or%20functions%20among%20visual%20elements%20correspond%2C%20even%20if%20their%20visual%20attributes%20differ.%20We%20then%20curate%20114k%20image-caption%20dataset%20in%20which%20the%20captions%20are%20anonymized%20--%20describing%20the%20underlying%20relational%20logic%20of%20the%20scene%20rather%20than%20its%20surface%20content.%20Using%20this%20dataset%2C%20we%20finetune%20a%20Vision-Language%20model%20to%20measure%20the%20relational%20similarity%20between%20images.%20This%20model%20serves%20as%20the%20first%20step%20toward%20connecting%20images%20by%20their%20underlying%20relational%20structure%20rather%20than%20their%20visible%20appearance.%20Our%20study%20shows%20that%20while%20relational%20similarity%20has%20a%20lot%20of%20real-world%20applications%2C%20existing%20image%20similarity%20models%20fail%20to%20capture%20it%20--%20revealing%20a%20critical%20gap%20in%20visual%20computing.&entry.1838667208=http%3A//arxiv.org/abs/2512.07833v1&entry.124074799=Read"},
{"title": "Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification", "author": "Menglin Wang and Xiaojin Gong and Jiachen Li and Genlin Ji", "abstract": "Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast' strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.", "link": "http://arxiv.org/abs/2512.07760v1", "date": "2025-12-08", "relevancy": 2.6849, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5572}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.529}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modality-Aware%20Bias%20Mitigation%20and%20Invariance%20Learning%20for%20Unsupervised%20Visible-Infrared%20Person%20Re-Identification&body=Title%3A%20Modality-Aware%20Bias%20Mitigation%20and%20Invariance%20Learning%20for%20Unsupervised%20Visible-Infrared%20Person%20Re-Identification%0AAuthor%3A%20Menglin%20Wang%20and%20Xiaojin%20Gong%20and%20Jiachen%20Li%20and%20Genlin%20Ji%0AAbstract%3A%20Unsupervised%20visible-infrared%20person%20re-identification%20%28USVI-ReID%29%20aims%20to%20match%20individuals%20across%20visible%20and%20infrared%20cameras%20without%20relying%20on%20any%20annotation.%20Given%20the%20significant%20gap%20across%20visible%20and%20infrared%20modality%2C%20estimating%20reliable%20cross-modality%20association%20becomes%20a%20major%20challenge%20in%20USVI-ReID.%20Existing%20methods%20usually%20adopt%20optimal%20transport%20to%20associate%20the%20intra-modality%20clusters%2C%20which%20is%20prone%20to%20propagating%20the%20local%20cluster%20errors%2C%20and%20also%20overlooks%20global%20instance-level%20relations.%20By%20mining%20and%20attending%20to%20the%20visible-infrared%20modality%20bias%2C%20this%20paper%20focuses%20on%20addressing%20cross-modality%20learning%20from%20two%20aspects%3A%20bias-mitigated%20global%20association%20and%20modality-invariant%20representation%20learning.%20Motivated%20by%20the%20camera-aware%20distance%20rectification%20in%20single-modality%20re-ID%2C%20we%20propose%20modality-aware%20Jaccard%20distance%20to%20mitigate%20the%20distance%20bias%20caused%20by%20modality%20discrepancy%2C%20so%20that%20more%20reliable%20cross-modality%20associations%20can%20be%20estimated%20through%20global%20clustering.%20To%20further%20improve%20cross-modality%20representation%20learning%2C%20a%20%60split-and-contrast%27%20strategy%20is%20designed%20to%20obtain%20modality-specific%20global%20prototypes.%20By%20explicitly%20aligning%20these%20prototypes%20under%20global%20association%20guidance%2C%20modality-invariant%20yet%20ID-discriminative%20representation%20learning%20can%20be%20achieved.%20While%20conceptually%20simple%2C%20our%20method%20obtains%20state-of-the-art%20performance%20on%20benchmark%20VI-ReID%20datasets%20and%20outperforms%20existing%20methods%20by%20a%20significant%20margin%2C%20validating%20its%20effectiveness.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModality-Aware%2520Bias%2520Mitigation%2520and%2520Invariance%2520Learning%2520for%2520Unsupervised%2520Visible-Infrared%2520Person%2520Re-Identification%26entry.906535625%3DMenglin%2520Wang%2520and%2520Xiaojin%2520Gong%2520and%2520Jiachen%2520Li%2520and%2520Genlin%2520Ji%26entry.1292438233%3DUnsupervised%2520visible-infrared%2520person%2520re-identification%2520%2528USVI-ReID%2529%2520aims%2520to%2520match%2520individuals%2520across%2520visible%2520and%2520infrared%2520cameras%2520without%2520relying%2520on%2520any%2520annotation.%2520Given%2520the%2520significant%2520gap%2520across%2520visible%2520and%2520infrared%2520modality%252C%2520estimating%2520reliable%2520cross-modality%2520association%2520becomes%2520a%2520major%2520challenge%2520in%2520USVI-ReID.%2520Existing%2520methods%2520usually%2520adopt%2520optimal%2520transport%2520to%2520associate%2520the%2520intra-modality%2520clusters%252C%2520which%2520is%2520prone%2520to%2520propagating%2520the%2520local%2520cluster%2520errors%252C%2520and%2520also%2520overlooks%2520global%2520instance-level%2520relations.%2520By%2520mining%2520and%2520attending%2520to%2520the%2520visible-infrared%2520modality%2520bias%252C%2520this%2520paper%2520focuses%2520on%2520addressing%2520cross-modality%2520learning%2520from%2520two%2520aspects%253A%2520bias-mitigated%2520global%2520association%2520and%2520modality-invariant%2520representation%2520learning.%2520Motivated%2520by%2520the%2520camera-aware%2520distance%2520rectification%2520in%2520single-modality%2520re-ID%252C%2520we%2520propose%2520modality-aware%2520Jaccard%2520distance%2520to%2520mitigate%2520the%2520distance%2520bias%2520caused%2520by%2520modality%2520discrepancy%252C%2520so%2520that%2520more%2520reliable%2520cross-modality%2520associations%2520can%2520be%2520estimated%2520through%2520global%2520clustering.%2520To%2520further%2520improve%2520cross-modality%2520representation%2520learning%252C%2520a%2520%2560split-and-contrast%2527%2520strategy%2520is%2520designed%2520to%2520obtain%2520modality-specific%2520global%2520prototypes.%2520By%2520explicitly%2520aligning%2520these%2520prototypes%2520under%2520global%2520association%2520guidance%252C%2520modality-invariant%2520yet%2520ID-discriminative%2520representation%2520learning%2520can%2520be%2520achieved.%2520While%2520conceptually%2520simple%252C%2520our%2520method%2520obtains%2520state-of-the-art%2520performance%2520on%2520benchmark%2520VI-ReID%2520datasets%2520and%2520outperforms%2520existing%2520methods%2520by%2520a%2520significant%2520margin%252C%2520validating%2520its%2520effectiveness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modality-Aware%20Bias%20Mitigation%20and%20Invariance%20Learning%20for%20Unsupervised%20Visible-Infrared%20Person%20Re-Identification&entry.906535625=Menglin%20Wang%20and%20Xiaojin%20Gong%20and%20Jiachen%20Li%20and%20Genlin%20Ji&entry.1292438233=Unsupervised%20visible-infrared%20person%20re-identification%20%28USVI-ReID%29%20aims%20to%20match%20individuals%20across%20visible%20and%20infrared%20cameras%20without%20relying%20on%20any%20annotation.%20Given%20the%20significant%20gap%20across%20visible%20and%20infrared%20modality%2C%20estimating%20reliable%20cross-modality%20association%20becomes%20a%20major%20challenge%20in%20USVI-ReID.%20Existing%20methods%20usually%20adopt%20optimal%20transport%20to%20associate%20the%20intra-modality%20clusters%2C%20which%20is%20prone%20to%20propagating%20the%20local%20cluster%20errors%2C%20and%20also%20overlooks%20global%20instance-level%20relations.%20By%20mining%20and%20attending%20to%20the%20visible-infrared%20modality%20bias%2C%20this%20paper%20focuses%20on%20addressing%20cross-modality%20learning%20from%20two%20aspects%3A%20bias-mitigated%20global%20association%20and%20modality-invariant%20representation%20learning.%20Motivated%20by%20the%20camera-aware%20distance%20rectification%20in%20single-modality%20re-ID%2C%20we%20propose%20modality-aware%20Jaccard%20distance%20to%20mitigate%20the%20distance%20bias%20caused%20by%20modality%20discrepancy%2C%20so%20that%20more%20reliable%20cross-modality%20associations%20can%20be%20estimated%20through%20global%20clustering.%20To%20further%20improve%20cross-modality%20representation%20learning%2C%20a%20%60split-and-contrast%27%20strategy%20is%20designed%20to%20obtain%20modality-specific%20global%20prototypes.%20By%20explicitly%20aligning%20these%20prototypes%20under%20global%20association%20guidance%2C%20modality-invariant%20yet%20ID-discriminative%20representation%20learning%20can%20be%20achieved.%20While%20conceptually%20simple%2C%20our%20method%20obtains%20state-of-the-art%20performance%20on%20benchmark%20VI-ReID%20datasets%20and%20outperforms%20existing%20methods%20by%20a%20significant%20margin%2C%20validating%20its%20effectiveness.&entry.1838667208=http%3A//arxiv.org/abs/2512.07760v1&entry.124074799=Read"},
{"title": "MeshRipple: Structured Autoregressive Generation of Artist-Meshes", "author": "Junkai Lin and Hang Long and Huipeng Guo and Jielei Zhang and JiaYi Yang and Tianle Guo and Yang Yang and Jianwen Li and Wenxiao Zhang and Matthias Nie\u00dfner and Wei Yang", "abstract": "Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies.This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.", "link": "http://arxiv.org/abs/2512.07514v1", "date": "2025-12-08", "relevancy": 2.6698, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5964}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5033}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshRipple%3A%20Structured%20Autoregressive%20Generation%20of%20Artist-Meshes&body=Title%3A%20MeshRipple%3A%20Structured%20Autoregressive%20Generation%20of%20Artist-Meshes%0AAuthor%3A%20Junkai%20Lin%20and%20Hang%20Long%20and%20Huipeng%20Guo%20and%20Jielei%20Zhang%20and%20JiaYi%20Yang%20and%20Tianle%20Guo%20and%20Yang%20Yang%20and%20Jianwen%20Li%20and%20Wenxiao%20Zhang%20and%20Matthias%20Nie%C3%9Fner%20and%20Wei%20Yang%0AAbstract%3A%20Meshes%20serve%20as%20a%20primary%20representation%20for%203D%20assets.%20Autoregressive%20mesh%20generators%20serialize%20faces%20into%20sequences%20and%20train%20on%20truncated%20segments%20with%20sliding-window%20inference%20to%20cope%20with%20memory%20limits.%20However%2C%20this%20mismatch%20breaks%20long-range%20geometric%20dependencies%2C%20producing%20holes%20and%20fragmented%20components.%20To%20address%20this%20critical%20limitation%2C%20we%20introduce%20MeshRipple%2C%20which%20expands%20a%20mesh%20outward%20from%20an%20active%20generation%20frontier%2C%20akin%20to%20a%20ripple%20on%20a%20surface.MeshRipple%20rests%20on%20three%20key%20innovations%3A%20a%20frontier-aware%20BFS%20tokenization%20that%20aligns%20the%20generation%20order%20with%20surface%20topology%3B%20an%20expansive%20prediction%20strategy%20that%20maintains%20coherent%2C%20connected%20surface%20growth%3B%20and%20a%20sparse-attention%20global%20memory%20that%20provides%20an%20effectively%20unbounded%20receptive%20field%20to%20resolve%20long-range%20topological%20dependencies.This%20integrated%20design%20enables%20MeshRipple%20to%20generate%20meshes%20with%20high%20surface%20fidelity%20and%20topological%20completeness%2C%20outperforming%20strong%20recent%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07514v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshRipple%253A%2520Structured%2520Autoregressive%2520Generation%2520of%2520Artist-Meshes%26entry.906535625%3DJunkai%2520Lin%2520and%2520Hang%2520Long%2520and%2520Huipeng%2520Guo%2520and%2520Jielei%2520Zhang%2520and%2520JiaYi%2520Yang%2520and%2520Tianle%2520Guo%2520and%2520Yang%2520Yang%2520and%2520Jianwen%2520Li%2520and%2520Wenxiao%2520Zhang%2520and%2520Matthias%2520Nie%25C3%259Fner%2520and%2520Wei%2520Yang%26entry.1292438233%3DMeshes%2520serve%2520as%2520a%2520primary%2520representation%2520for%25203D%2520assets.%2520Autoregressive%2520mesh%2520generators%2520serialize%2520faces%2520into%2520sequences%2520and%2520train%2520on%2520truncated%2520segments%2520with%2520sliding-window%2520inference%2520to%2520cope%2520with%2520memory%2520limits.%2520However%252C%2520this%2520mismatch%2520breaks%2520long-range%2520geometric%2520dependencies%252C%2520producing%2520holes%2520and%2520fragmented%2520components.%2520To%2520address%2520this%2520critical%2520limitation%252C%2520we%2520introduce%2520MeshRipple%252C%2520which%2520expands%2520a%2520mesh%2520outward%2520from%2520an%2520active%2520generation%2520frontier%252C%2520akin%2520to%2520a%2520ripple%2520on%2520a%2520surface.MeshRipple%2520rests%2520on%2520three%2520key%2520innovations%253A%2520a%2520frontier-aware%2520BFS%2520tokenization%2520that%2520aligns%2520the%2520generation%2520order%2520with%2520surface%2520topology%253B%2520an%2520expansive%2520prediction%2520strategy%2520that%2520maintains%2520coherent%252C%2520connected%2520surface%2520growth%253B%2520and%2520a%2520sparse-attention%2520global%2520memory%2520that%2520provides%2520an%2520effectively%2520unbounded%2520receptive%2520field%2520to%2520resolve%2520long-range%2520topological%2520dependencies.This%2520integrated%2520design%2520enables%2520MeshRipple%2520to%2520generate%2520meshes%2520with%2520high%2520surface%2520fidelity%2520and%2520topological%2520completeness%252C%2520outperforming%2520strong%2520recent%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07514v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshRipple%3A%20Structured%20Autoregressive%20Generation%20of%20Artist-Meshes&entry.906535625=Junkai%20Lin%20and%20Hang%20Long%20and%20Huipeng%20Guo%20and%20Jielei%20Zhang%20and%20JiaYi%20Yang%20and%20Tianle%20Guo%20and%20Yang%20Yang%20and%20Jianwen%20Li%20and%20Wenxiao%20Zhang%20and%20Matthias%20Nie%C3%9Fner%20and%20Wei%20Yang&entry.1292438233=Meshes%20serve%20as%20a%20primary%20representation%20for%203D%20assets.%20Autoregressive%20mesh%20generators%20serialize%20faces%20into%20sequences%20and%20train%20on%20truncated%20segments%20with%20sliding-window%20inference%20to%20cope%20with%20memory%20limits.%20However%2C%20this%20mismatch%20breaks%20long-range%20geometric%20dependencies%2C%20producing%20holes%20and%20fragmented%20components.%20To%20address%20this%20critical%20limitation%2C%20we%20introduce%20MeshRipple%2C%20which%20expands%20a%20mesh%20outward%20from%20an%20active%20generation%20frontier%2C%20akin%20to%20a%20ripple%20on%20a%20surface.MeshRipple%20rests%20on%20three%20key%20innovations%3A%20a%20frontier-aware%20BFS%20tokenization%20that%20aligns%20the%20generation%20order%20with%20surface%20topology%3B%20an%20expansive%20prediction%20strategy%20that%20maintains%20coherent%2C%20connected%20surface%20growth%3B%20and%20a%20sparse-attention%20global%20memory%20that%20provides%20an%20effectively%20unbounded%20receptive%20field%20to%20resolve%20long-range%20topological%20dependencies.This%20integrated%20design%20enables%20MeshRipple%20to%20generate%20meshes%20with%20high%20surface%20fidelity%20and%20topological%20completeness%2C%20outperforming%20strong%20recent%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2512.07514v1&entry.124074799=Read"},
{"title": "Towards Robust DeepFake Detection under Unstable Face Sequences: Adaptive Sparse Graph Embedding with Order-Free Representation and Explicit Laplacian Spectral Prior", "author": "Chih-Chung Hsu and Shao-Ning Chen and Chia-Ming Lee and Yi-Fang Wang and Yi-Shiuan Chou", "abstract": "Ensuring the authenticity of video content remains challenging as DeepFake generation becomes increasingly realistic and robust against detection. Most existing detectors implicitly assume temporally consistent and clean facial sequences, an assumption that rarely holds in real-world scenarios where compression artifacts, occlusions, and adversarial attacks destabilize face detection and often lead to invalid or misdetected faces. To address these challenges, we propose a Laplacian-Regularized Graph Convolutional Network (LR-GCN) that robustly detects DeepFakes from noisy or unordered face sequences, while being trained only on clean facial data. Our method constructs an Order-Free Temporal Graph Embedding (OF-TGE) that organizes frame-wise CNN features into an adaptive sparse graph based on semantic affinities. Unlike traditional methods constrained by strict temporal continuity, OF-TGE captures intrinsic feature consistency across frames, making it resilient to shuffled, missing, or heavily corrupted inputs. We further impose a dual-level sparsity mechanism on both graph structure and node features to suppress the influence of invalid faces. Crucially, we introduce an explicit Graph Laplacian Spectral Prior that acts as a high-pass operator in the graph spectral domain, highlighting structural anomalies and forgery artifacts, which are then consolidated by a low-pass GCN aggregation. This sequential design effectively realizes a task-driven spectral band-pass mechanism that suppresses background information and random noise while preserving manipulation cues. Extensive experiments on FF++, Celeb-DFv2, and DFDC demonstrate that LR-GCN achieves state-of-the-art performance and significantly improved robustness under severe global and local disruptions, including missing faces, occlusions, and adversarially perturbed face detections.", "link": "http://arxiv.org/abs/2512.07498v1", "date": "2025-12-08", "relevancy": 2.6459, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5485}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5253}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Robust%20DeepFake%20Detection%20under%20Unstable%20Face%20Sequences%3A%20Adaptive%20Sparse%20Graph%20Embedding%20with%20Order-Free%20Representation%20and%20Explicit%20Laplacian%20Spectral%20Prior&body=Title%3A%20Towards%20Robust%20DeepFake%20Detection%20under%20Unstable%20Face%20Sequences%3A%20Adaptive%20Sparse%20Graph%20Embedding%20with%20Order-Free%20Representation%20and%20Explicit%20Laplacian%20Spectral%20Prior%0AAuthor%3A%20Chih-Chung%20Hsu%20and%20Shao-Ning%20Chen%20and%20Chia-Ming%20Lee%20and%20Yi-Fang%20Wang%20and%20Yi-Shiuan%20Chou%0AAbstract%3A%20Ensuring%20the%20authenticity%20of%20video%20content%20remains%20challenging%20as%20DeepFake%20generation%20becomes%20increasingly%20realistic%20and%20robust%20against%20detection.%20Most%20existing%20detectors%20implicitly%20assume%20temporally%20consistent%20and%20clean%20facial%20sequences%2C%20an%20assumption%20that%20rarely%20holds%20in%20real-world%20scenarios%20where%20compression%20artifacts%2C%20occlusions%2C%20and%20adversarial%20attacks%20destabilize%20face%20detection%20and%20often%20lead%20to%20invalid%20or%20misdetected%20faces.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20Laplacian-Regularized%20Graph%20Convolutional%20Network%20%28LR-GCN%29%20that%20robustly%20detects%20DeepFakes%20from%20noisy%20or%20unordered%20face%20sequences%2C%20while%20being%20trained%20only%20on%20clean%20facial%20data.%20Our%20method%20constructs%20an%20Order-Free%20Temporal%20Graph%20Embedding%20%28OF-TGE%29%20that%20organizes%20frame-wise%20CNN%20features%20into%20an%20adaptive%20sparse%20graph%20based%20on%20semantic%20affinities.%20Unlike%20traditional%20methods%20constrained%20by%20strict%20temporal%20continuity%2C%20OF-TGE%20captures%20intrinsic%20feature%20consistency%20across%20frames%2C%20making%20it%20resilient%20to%20shuffled%2C%20missing%2C%20or%20heavily%20corrupted%20inputs.%20We%20further%20impose%20a%20dual-level%20sparsity%20mechanism%20on%20both%20graph%20structure%20and%20node%20features%20to%20suppress%20the%20influence%20of%20invalid%20faces.%20Crucially%2C%20we%20introduce%20an%20explicit%20Graph%20Laplacian%20Spectral%20Prior%20that%20acts%20as%20a%20high-pass%20operator%20in%20the%20graph%20spectral%20domain%2C%20highlighting%20structural%20anomalies%20and%20forgery%20artifacts%2C%20which%20are%20then%20consolidated%20by%20a%20low-pass%20GCN%20aggregation.%20This%20sequential%20design%20effectively%20realizes%20a%20task-driven%20spectral%20band-pass%20mechanism%20that%20suppresses%20background%20information%20and%20random%20noise%20while%20preserving%20manipulation%20cues.%20Extensive%20experiments%20on%20FF%2B%2B%2C%20Celeb-DFv2%2C%20and%20DFDC%20demonstrate%20that%20LR-GCN%20achieves%20state-of-the-art%20performance%20and%20significantly%20improved%20robustness%20under%20severe%20global%20and%20local%20disruptions%2C%20including%20missing%20faces%2C%20occlusions%2C%20and%20adversarially%20perturbed%20face%20detections.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Robust%2520DeepFake%2520Detection%2520under%2520Unstable%2520Face%2520Sequences%253A%2520Adaptive%2520Sparse%2520Graph%2520Embedding%2520with%2520Order-Free%2520Representation%2520and%2520Explicit%2520Laplacian%2520Spectral%2520Prior%26entry.906535625%3DChih-Chung%2520Hsu%2520and%2520Shao-Ning%2520Chen%2520and%2520Chia-Ming%2520Lee%2520and%2520Yi-Fang%2520Wang%2520and%2520Yi-Shiuan%2520Chou%26entry.1292438233%3DEnsuring%2520the%2520authenticity%2520of%2520video%2520content%2520remains%2520challenging%2520as%2520DeepFake%2520generation%2520becomes%2520increasingly%2520realistic%2520and%2520robust%2520against%2520detection.%2520Most%2520existing%2520detectors%2520implicitly%2520assume%2520temporally%2520consistent%2520and%2520clean%2520facial%2520sequences%252C%2520an%2520assumption%2520that%2520rarely%2520holds%2520in%2520real-world%2520scenarios%2520where%2520compression%2520artifacts%252C%2520occlusions%252C%2520and%2520adversarial%2520attacks%2520destabilize%2520face%2520detection%2520and%2520often%2520lead%2520to%2520invalid%2520or%2520misdetected%2520faces.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520Laplacian-Regularized%2520Graph%2520Convolutional%2520Network%2520%2528LR-GCN%2529%2520that%2520robustly%2520detects%2520DeepFakes%2520from%2520noisy%2520or%2520unordered%2520face%2520sequences%252C%2520while%2520being%2520trained%2520only%2520on%2520clean%2520facial%2520data.%2520Our%2520method%2520constructs%2520an%2520Order-Free%2520Temporal%2520Graph%2520Embedding%2520%2528OF-TGE%2529%2520that%2520organizes%2520frame-wise%2520CNN%2520features%2520into%2520an%2520adaptive%2520sparse%2520graph%2520based%2520on%2520semantic%2520affinities.%2520Unlike%2520traditional%2520methods%2520constrained%2520by%2520strict%2520temporal%2520continuity%252C%2520OF-TGE%2520captures%2520intrinsic%2520feature%2520consistency%2520across%2520frames%252C%2520making%2520it%2520resilient%2520to%2520shuffled%252C%2520missing%252C%2520or%2520heavily%2520corrupted%2520inputs.%2520We%2520further%2520impose%2520a%2520dual-level%2520sparsity%2520mechanism%2520on%2520both%2520graph%2520structure%2520and%2520node%2520features%2520to%2520suppress%2520the%2520influence%2520of%2520invalid%2520faces.%2520Crucially%252C%2520we%2520introduce%2520an%2520explicit%2520Graph%2520Laplacian%2520Spectral%2520Prior%2520that%2520acts%2520as%2520a%2520high-pass%2520operator%2520in%2520the%2520graph%2520spectral%2520domain%252C%2520highlighting%2520structural%2520anomalies%2520and%2520forgery%2520artifacts%252C%2520which%2520are%2520then%2520consolidated%2520by%2520a%2520low-pass%2520GCN%2520aggregation.%2520This%2520sequential%2520design%2520effectively%2520realizes%2520a%2520task-driven%2520spectral%2520band-pass%2520mechanism%2520that%2520suppresses%2520background%2520information%2520and%2520random%2520noise%2520while%2520preserving%2520manipulation%2520cues.%2520Extensive%2520experiments%2520on%2520FF%252B%252B%252C%2520Celeb-DFv2%252C%2520and%2520DFDC%2520demonstrate%2520that%2520LR-GCN%2520achieves%2520state-of-the-art%2520performance%2520and%2520significantly%2520improved%2520robustness%2520under%2520severe%2520global%2520and%2520local%2520disruptions%252C%2520including%2520missing%2520faces%252C%2520occlusions%252C%2520and%2520adversarially%2520perturbed%2520face%2520detections.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Robust%20DeepFake%20Detection%20under%20Unstable%20Face%20Sequences%3A%20Adaptive%20Sparse%20Graph%20Embedding%20with%20Order-Free%20Representation%20and%20Explicit%20Laplacian%20Spectral%20Prior&entry.906535625=Chih-Chung%20Hsu%20and%20Shao-Ning%20Chen%20and%20Chia-Ming%20Lee%20and%20Yi-Fang%20Wang%20and%20Yi-Shiuan%20Chou&entry.1292438233=Ensuring%20the%20authenticity%20of%20video%20content%20remains%20challenging%20as%20DeepFake%20generation%20becomes%20increasingly%20realistic%20and%20robust%20against%20detection.%20Most%20existing%20detectors%20implicitly%20assume%20temporally%20consistent%20and%20clean%20facial%20sequences%2C%20an%20assumption%20that%20rarely%20holds%20in%20real-world%20scenarios%20where%20compression%20artifacts%2C%20occlusions%2C%20and%20adversarial%20attacks%20destabilize%20face%20detection%20and%20often%20lead%20to%20invalid%20or%20misdetected%20faces.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20Laplacian-Regularized%20Graph%20Convolutional%20Network%20%28LR-GCN%29%20that%20robustly%20detects%20DeepFakes%20from%20noisy%20or%20unordered%20face%20sequences%2C%20while%20being%20trained%20only%20on%20clean%20facial%20data.%20Our%20method%20constructs%20an%20Order-Free%20Temporal%20Graph%20Embedding%20%28OF-TGE%29%20that%20organizes%20frame-wise%20CNN%20features%20into%20an%20adaptive%20sparse%20graph%20based%20on%20semantic%20affinities.%20Unlike%20traditional%20methods%20constrained%20by%20strict%20temporal%20continuity%2C%20OF-TGE%20captures%20intrinsic%20feature%20consistency%20across%20frames%2C%20making%20it%20resilient%20to%20shuffled%2C%20missing%2C%20or%20heavily%20corrupted%20inputs.%20We%20further%20impose%20a%20dual-level%20sparsity%20mechanism%20on%20both%20graph%20structure%20and%20node%20features%20to%20suppress%20the%20influence%20of%20invalid%20faces.%20Crucially%2C%20we%20introduce%20an%20explicit%20Graph%20Laplacian%20Spectral%20Prior%20that%20acts%20as%20a%20high-pass%20operator%20in%20the%20graph%20spectral%20domain%2C%20highlighting%20structural%20anomalies%20and%20forgery%20artifacts%2C%20which%20are%20then%20consolidated%20by%20a%20low-pass%20GCN%20aggregation.%20This%20sequential%20design%20effectively%20realizes%20a%20task-driven%20spectral%20band-pass%20mechanism%20that%20suppresses%20background%20information%20and%20random%20noise%20while%20preserving%20manipulation%20cues.%20Extensive%20experiments%20on%20FF%2B%2B%2C%20Celeb-DFv2%2C%20and%20DFDC%20demonstrate%20that%20LR-GCN%20achieves%20state-of-the-art%20performance%20and%20significantly%20improved%20robustness%20under%20severe%20global%20and%20local%20disruptions%2C%20including%20missing%20faces%2C%20occlusions%2C%20and%20adversarially%20perturbed%20face%20detections.&entry.1838667208=http%3A//arxiv.org/abs/2512.07498v1&entry.124074799=Read"},
{"title": "EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset", "author": "Ronan John and Aditya Kesari and Vincenzo DiMatteo and Kristin Dana", "abstract": "We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .", "link": "http://arxiv.org/abs/2512.07668v1", "date": "2025-12-08", "relevancy": 2.6412, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5745}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5079}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoCampus%3A%20Egocentric%20Pedestrian%20Eye%20Gaze%20Model%20and%20Dataset&body=Title%3A%20EgoCampus%3A%20Egocentric%20Pedestrian%20Eye%20Gaze%20Model%20and%20Dataset%0AAuthor%3A%20Ronan%20John%20and%20Aditya%20Kesari%20and%20Vincenzo%20DiMatteo%20and%20Kristin%20Dana%0AAbstract%3A%20We%20address%20the%20challenge%20of%20predicting%20human%20visual%20attention%20during%20real-world%20navigation%20by%20measuring%20and%20modeling%20egocentric%20pedestrian%20eye%20gaze%20in%20an%20outdoor%20campus%20setting.%20We%20introduce%20the%20EgoCampus%20dataset%2C%20which%20spans%2025%20unique%20outdoor%20paths%20over%206%20km%20across%20a%20university%20campus%20with%20recordings%20from%20more%20than%2080%20distinct%20human%20pedestrians%2C%20resulting%20in%20a%20diverse%20set%20of%20gaze-annotated%20videos.%20The%20system%20used%20for%20collection%2C%20Meta%27s%20Project%20Aria%20glasses%2C%20integrates%20eye%20tracking%2C%20front-facing%20RGB%20cameras%2C%20inertial%20sensors%2C%20and%20GPS%20to%20provide%20rich%20data%20from%20the%20human%20perspective.%20Unlike%20many%20prior%20egocentric%20datasets%20that%20focus%20on%20indoor%20tasks%20or%20exclude%20eye%20gaze%20information%2C%20our%20work%20emphasizes%20visual%20attention%20while%20subjects%20walk%20in%20outdoor%20campus%20paths.%20Using%20this%20data%2C%20we%20develop%20EgoCampusNet%2C%20a%20novel%20method%20to%20predict%20eye%20gaze%20of%20navigating%20pedestrians%20as%20they%20move%20through%20outdoor%20environments.%20Our%20contributions%20provide%20both%20a%20new%20resource%20for%20studying%20real-world%20attention%20and%20a%20resource%20for%20future%20work%20in%20gaze%20prediction%20models%20for%20navigation.%20Dataset%20and%20code%20are%20available%20upon%20request%2C%20and%20will%20be%20made%20publicly%20available%20at%20a%20later%20date%20at%20https%3A//github.com/ComputerVisionRutgers/EgoCampus%20.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoCampus%253A%2520Egocentric%2520Pedestrian%2520Eye%2520Gaze%2520Model%2520and%2520Dataset%26entry.906535625%3DRonan%2520John%2520and%2520Aditya%2520Kesari%2520and%2520Vincenzo%2520DiMatteo%2520and%2520Kristin%2520Dana%26entry.1292438233%3DWe%2520address%2520the%2520challenge%2520of%2520predicting%2520human%2520visual%2520attention%2520during%2520real-world%2520navigation%2520by%2520measuring%2520and%2520modeling%2520egocentric%2520pedestrian%2520eye%2520gaze%2520in%2520an%2520outdoor%2520campus%2520setting.%2520We%2520introduce%2520the%2520EgoCampus%2520dataset%252C%2520which%2520spans%252025%2520unique%2520outdoor%2520paths%2520over%25206%2520km%2520across%2520a%2520university%2520campus%2520with%2520recordings%2520from%2520more%2520than%252080%2520distinct%2520human%2520pedestrians%252C%2520resulting%2520in%2520a%2520diverse%2520set%2520of%2520gaze-annotated%2520videos.%2520The%2520system%2520used%2520for%2520collection%252C%2520Meta%2527s%2520Project%2520Aria%2520glasses%252C%2520integrates%2520eye%2520tracking%252C%2520front-facing%2520RGB%2520cameras%252C%2520inertial%2520sensors%252C%2520and%2520GPS%2520to%2520provide%2520rich%2520data%2520from%2520the%2520human%2520perspective.%2520Unlike%2520many%2520prior%2520egocentric%2520datasets%2520that%2520focus%2520on%2520indoor%2520tasks%2520or%2520exclude%2520eye%2520gaze%2520information%252C%2520our%2520work%2520emphasizes%2520visual%2520attention%2520while%2520subjects%2520walk%2520in%2520outdoor%2520campus%2520paths.%2520Using%2520this%2520data%252C%2520we%2520develop%2520EgoCampusNet%252C%2520a%2520novel%2520method%2520to%2520predict%2520eye%2520gaze%2520of%2520navigating%2520pedestrians%2520as%2520they%2520move%2520through%2520outdoor%2520environments.%2520Our%2520contributions%2520provide%2520both%2520a%2520new%2520resource%2520for%2520studying%2520real-world%2520attention%2520and%2520a%2520resource%2520for%2520future%2520work%2520in%2520gaze%2520prediction%2520models%2520for%2520navigation.%2520Dataset%2520and%2520code%2520are%2520available%2520upon%2520request%252C%2520and%2520will%2520be%2520made%2520publicly%2520available%2520at%2520a%2520later%2520date%2520at%2520https%253A//github.com/ComputerVisionRutgers/EgoCampus%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoCampus%3A%20Egocentric%20Pedestrian%20Eye%20Gaze%20Model%20and%20Dataset&entry.906535625=Ronan%20John%20and%20Aditya%20Kesari%20and%20Vincenzo%20DiMatteo%20and%20Kristin%20Dana&entry.1292438233=We%20address%20the%20challenge%20of%20predicting%20human%20visual%20attention%20during%20real-world%20navigation%20by%20measuring%20and%20modeling%20egocentric%20pedestrian%20eye%20gaze%20in%20an%20outdoor%20campus%20setting.%20We%20introduce%20the%20EgoCampus%20dataset%2C%20which%20spans%2025%20unique%20outdoor%20paths%20over%206%20km%20across%20a%20university%20campus%20with%20recordings%20from%20more%20than%2080%20distinct%20human%20pedestrians%2C%20resulting%20in%20a%20diverse%20set%20of%20gaze-annotated%20videos.%20The%20system%20used%20for%20collection%2C%20Meta%27s%20Project%20Aria%20glasses%2C%20integrates%20eye%20tracking%2C%20front-facing%20RGB%20cameras%2C%20inertial%20sensors%2C%20and%20GPS%20to%20provide%20rich%20data%20from%20the%20human%20perspective.%20Unlike%20many%20prior%20egocentric%20datasets%20that%20focus%20on%20indoor%20tasks%20or%20exclude%20eye%20gaze%20information%2C%20our%20work%20emphasizes%20visual%20attention%20while%20subjects%20walk%20in%20outdoor%20campus%20paths.%20Using%20this%20data%2C%20we%20develop%20EgoCampusNet%2C%20a%20novel%20method%20to%20predict%20eye%20gaze%20of%20navigating%20pedestrians%20as%20they%20move%20through%20outdoor%20environments.%20Our%20contributions%20provide%20both%20a%20new%20resource%20for%20studying%20real-world%20attention%20and%20a%20resource%20for%20future%20work%20in%20gaze%20prediction%20models%20for%20navigation.%20Dataset%20and%20code%20are%20available%20upon%20request%2C%20and%20will%20be%20made%20publicly%20available%20at%20a%20later%20date%20at%20https%3A//github.com/ComputerVisionRutgers/EgoCampus%20.&entry.1838667208=http%3A//arxiv.org/abs/2512.07668v1&entry.124074799=Read"},
{"title": "Exploring the Potential of Encoder-free Architectures in 3D LMMs", "author": "Yiwen Tang and Zoey Guo and Zhuhao Wang and Ray Zhang and Qizhi Chen and Junli Liu and Delin Qu and Zhigang Wang and Dong Wang and Bin Zhao and Xuelong Li", "abstract": "Encoder-free architectures have been preliminarily explored in the 2D Large Multimodal Models (LMMs), yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to alleviate the challenges of encoder-based 3D LMMs. These long-standing challenges include the failure to adapt to varying point cloud resolutions during inference and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the pre-trained encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL. Our 7B model rivals the state-of-the-art model, PointLLM-PiSA-13B, achieving 57.91%, 61.0%, and 55.20% on the classification, captioning, and VQA tasks, respectively. Our results show that the encoder-free architecture is highly promising for replacing encoder-based architectures in the field of 3D understanding. The code is released at https://github.com/Ivan-Tang-3D/ENEL", "link": "http://arxiv.org/abs/2502.09620v4", "date": "2025-12-08", "relevancy": 2.6091, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.66}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.66}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Potential%20of%20Encoder-free%20Architectures%20in%203D%20LMMs&body=Title%3A%20Exploring%20the%20Potential%20of%20Encoder-free%20Architectures%20in%203D%20LMMs%0AAuthor%3A%20Yiwen%20Tang%20and%20Zoey%20Guo%20and%20Zhuhao%20Wang%20and%20Ray%20Zhang%20and%20Qizhi%20Chen%20and%20Junli%20Liu%20and%20Delin%20Qu%20and%20Zhigang%20Wang%20and%20Dong%20Wang%20and%20Bin%20Zhao%20and%20Xuelong%20Li%0AAbstract%3A%20Encoder-free%20architectures%20have%20been%20preliminarily%20explored%20in%20the%202D%20Large%20Multimodal%20Models%20%28LMMs%29%2C%20yet%20it%20remains%20an%20open%20question%20whether%20they%20can%20be%20effectively%20applied%20to%203D%20understanding%20scenarios.%20In%20this%20paper%2C%20we%20present%20the%20first%20comprehensive%20investigation%20into%20the%20potential%20of%20encoder-free%20architectures%20to%20alleviate%20the%20challenges%20of%20encoder-based%203D%20LMMs.%20These%20long-standing%20challenges%20include%20the%20failure%20to%20adapt%20to%20varying%20point%20cloud%20resolutions%20during%20inference%20and%20the%20point%20features%20from%20the%20encoder%20not%20meeting%20the%20semantic%20needs%20of%20Large%20Language%20Models%20%28LLMs%29.%20We%20identify%20key%20aspects%20for%203D%20LMMs%20to%20remove%20the%20pre-trained%20encoder%20and%20enable%20the%20LLM%20to%20assume%20the%20role%20of%20the%203D%20encoder%3A%201%29%20We%20propose%20the%20LLM-embedded%20Semantic%20Encoding%20strategy%20in%20the%20pre-training%20stage%2C%20exploring%20the%20effects%20of%20various%20point%20cloud%20self-supervised%20losses.%20And%20we%20present%20the%20Hybrid%20Semantic%20Loss%20to%20extract%20high-level%20semantics.%202%29%20We%20introduce%20the%20Hierarchical%20Geometry%20Aggregation%20strategy%20in%20the%20instruction%20tuning%20stage.%20This%20incorporates%20inductive%20bias%20into%20the%20LLM%20layers%20to%20focus%20on%20the%20local%20details%20of%20the%20point%20clouds.%20To%20the%20end%2C%20we%20present%20the%20first%20Encoder-free%203D%20LMM%2C%20ENEL.%20Our%207B%20model%20rivals%20the%20state-of-the-art%20model%2C%20PointLLM-PiSA-13B%2C%20achieving%2057.91%25%2C%2061.0%25%2C%20and%2055.20%25%20on%20the%20classification%2C%20captioning%2C%20and%20VQA%20tasks%2C%20respectively.%20Our%20results%20show%20that%20the%20encoder-free%20architecture%20is%20highly%20promising%20for%20replacing%20encoder-based%20architectures%20in%20the%20field%20of%203D%20understanding.%20The%20code%20is%20released%20at%20https%3A//github.com/Ivan-Tang-3D/ENEL%0ALink%3A%20http%3A//arxiv.org/abs/2502.09620v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Potential%2520of%2520Encoder-free%2520Architectures%2520in%25203D%2520LMMs%26entry.906535625%3DYiwen%2520Tang%2520and%2520Zoey%2520Guo%2520and%2520Zhuhao%2520Wang%2520and%2520Ray%2520Zhang%2520and%2520Qizhi%2520Chen%2520and%2520Junli%2520Liu%2520and%2520Delin%2520Qu%2520and%2520Zhigang%2520Wang%2520and%2520Dong%2520Wang%2520and%2520Bin%2520Zhao%2520and%2520Xuelong%2520Li%26entry.1292438233%3DEncoder-free%2520architectures%2520have%2520been%2520preliminarily%2520explored%2520in%2520the%25202D%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%252C%2520yet%2520it%2520remains%2520an%2520open%2520question%2520whether%2520they%2520can%2520be%2520effectively%2520applied%2520to%25203D%2520understanding%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520first%2520comprehensive%2520investigation%2520into%2520the%2520potential%2520of%2520encoder-free%2520architectures%2520to%2520alleviate%2520the%2520challenges%2520of%2520encoder-based%25203D%2520LMMs.%2520These%2520long-standing%2520challenges%2520include%2520the%2520failure%2520to%2520adapt%2520to%2520varying%2520point%2520cloud%2520resolutions%2520during%2520inference%2520and%2520the%2520point%2520features%2520from%2520the%2520encoder%2520not%2520meeting%2520the%2520semantic%2520needs%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520We%2520identify%2520key%2520aspects%2520for%25203D%2520LMMs%2520to%2520remove%2520the%2520pre-trained%2520encoder%2520and%2520enable%2520the%2520LLM%2520to%2520assume%2520the%2520role%2520of%2520the%25203D%2520encoder%253A%25201%2529%2520We%2520propose%2520the%2520LLM-embedded%2520Semantic%2520Encoding%2520strategy%2520in%2520the%2520pre-training%2520stage%252C%2520exploring%2520the%2520effects%2520of%2520various%2520point%2520cloud%2520self-supervised%2520losses.%2520And%2520we%2520present%2520the%2520Hybrid%2520Semantic%2520Loss%2520to%2520extract%2520high-level%2520semantics.%25202%2529%2520We%2520introduce%2520the%2520Hierarchical%2520Geometry%2520Aggregation%2520strategy%2520in%2520the%2520instruction%2520tuning%2520stage.%2520This%2520incorporates%2520inductive%2520bias%2520into%2520the%2520LLM%2520layers%2520to%2520focus%2520on%2520the%2520local%2520details%2520of%2520the%2520point%2520clouds.%2520To%2520the%2520end%252C%2520we%2520present%2520the%2520first%2520Encoder-free%25203D%2520LMM%252C%2520ENEL.%2520Our%25207B%2520model%2520rivals%2520the%2520state-of-the-art%2520model%252C%2520PointLLM-PiSA-13B%252C%2520achieving%252057.91%2525%252C%252061.0%2525%252C%2520and%252055.20%2525%2520on%2520the%2520classification%252C%2520captioning%252C%2520and%2520VQA%2520tasks%252C%2520respectively.%2520Our%2520results%2520show%2520that%2520the%2520encoder-free%2520architecture%2520is%2520highly%2520promising%2520for%2520replacing%2520encoder-based%2520architectures%2520in%2520the%2520field%2520of%25203D%2520understanding.%2520The%2520code%2520is%2520released%2520at%2520https%253A//github.com/Ivan-Tang-3D/ENEL%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09620v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Potential%20of%20Encoder-free%20Architectures%20in%203D%20LMMs&entry.906535625=Yiwen%20Tang%20and%20Zoey%20Guo%20and%20Zhuhao%20Wang%20and%20Ray%20Zhang%20and%20Qizhi%20Chen%20and%20Junli%20Liu%20and%20Delin%20Qu%20and%20Zhigang%20Wang%20and%20Dong%20Wang%20and%20Bin%20Zhao%20and%20Xuelong%20Li&entry.1292438233=Encoder-free%20architectures%20have%20been%20preliminarily%20explored%20in%20the%202D%20Large%20Multimodal%20Models%20%28LMMs%29%2C%20yet%20it%20remains%20an%20open%20question%20whether%20they%20can%20be%20effectively%20applied%20to%203D%20understanding%20scenarios.%20In%20this%20paper%2C%20we%20present%20the%20first%20comprehensive%20investigation%20into%20the%20potential%20of%20encoder-free%20architectures%20to%20alleviate%20the%20challenges%20of%20encoder-based%203D%20LMMs.%20These%20long-standing%20challenges%20include%20the%20failure%20to%20adapt%20to%20varying%20point%20cloud%20resolutions%20during%20inference%20and%20the%20point%20features%20from%20the%20encoder%20not%20meeting%20the%20semantic%20needs%20of%20Large%20Language%20Models%20%28LLMs%29.%20We%20identify%20key%20aspects%20for%203D%20LMMs%20to%20remove%20the%20pre-trained%20encoder%20and%20enable%20the%20LLM%20to%20assume%20the%20role%20of%20the%203D%20encoder%3A%201%29%20We%20propose%20the%20LLM-embedded%20Semantic%20Encoding%20strategy%20in%20the%20pre-training%20stage%2C%20exploring%20the%20effects%20of%20various%20point%20cloud%20self-supervised%20losses.%20And%20we%20present%20the%20Hybrid%20Semantic%20Loss%20to%20extract%20high-level%20semantics.%202%29%20We%20introduce%20the%20Hierarchical%20Geometry%20Aggregation%20strategy%20in%20the%20instruction%20tuning%20stage.%20This%20incorporates%20inductive%20bias%20into%20the%20LLM%20layers%20to%20focus%20on%20the%20local%20details%20of%20the%20point%20clouds.%20To%20the%20end%2C%20we%20present%20the%20first%20Encoder-free%203D%20LMM%2C%20ENEL.%20Our%207B%20model%20rivals%20the%20state-of-the-art%20model%2C%20PointLLM-PiSA-13B%2C%20achieving%2057.91%25%2C%2061.0%25%2C%20and%2055.20%25%20on%20the%20classification%2C%20captioning%2C%20and%20VQA%20tasks%2C%20respectively.%20Our%20results%20show%20that%20the%20encoder-free%20architecture%20is%20highly%20promising%20for%20replacing%20encoder-based%20architectures%20in%20the%20field%20of%203D%20understanding.%20The%20code%20is%20released%20at%20https%3A//github.com/Ivan-Tang-3D/ENEL&entry.1838667208=http%3A//arxiv.org/abs/2502.09620v4&entry.124074799=Read"},
{"title": "LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings", "author": "Sebastian Sztwiertnia and Felix Friedrich and Kristian Kersting and Patrick Schramowski and Bj\u00f6rn Deiseroth", "abstract": "Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.", "link": "http://arxiv.org/abs/2512.07522v1", "date": "2025-12-08", "relevancy": 2.5996, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5169}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LIME%3A%20Making%20LLM%20Data%20More%20Efficient%20with%20Linguistic%20Metadata%20Embeddings&body=Title%3A%20LIME%3A%20Making%20LLM%20Data%20More%20Efficient%20with%20Linguistic%20Metadata%20Embeddings%0AAuthor%3A%20Sebastian%20Sztwiertnia%20and%20Felix%20Friedrich%20and%20Kristian%20Kersting%20and%20Patrick%20Schramowski%20and%20Bj%C3%B6rn%20Deiseroth%0AAbstract%3A%20Pre-training%20decoder-only%20language%20models%20relies%20on%20vast%20amounts%20of%20high-quality%20data%2C%20yet%20the%20availability%20of%20such%20data%20is%20increasingly%20reaching%20its%20limits.%20While%20metadata%20is%20commonly%20used%20to%20create%20and%20curate%20these%20datasets%2C%20its%20potential%20as%20a%20direct%20training%20signal%20remains%20under-explored.%20We%20challenge%20this%20status%20quo%20and%20propose%20LIME%20%28Linguistic%20Metadata%20Embeddings%29%2C%20a%20method%20that%20enriches%20token%20embeddings%20with%20metadata%20capturing%20syntax%2C%20semantics%2C%20and%20contextual%20properties.%20LIME%20substantially%20improves%20pre-training%20efficiency.%20Specifically%2C%20it%20adapts%20up%20to%2056%25%20faster%20to%20the%20training%20data%20distribution%2C%20while%20introducing%20only%200.01%25%20additional%20parameters%20at%20negligible%20compute%20overhead.%20Beyond%20efficiency%2C%20LIME%20improves%20tokenization%2C%20leading%20to%20remarkably%20stronger%20language%20modeling%20capabilities%20and%20generative%20task%20performance.%20These%20benefits%20persist%20across%20model%20scales%20%28500M%20to%202B%29.%20In%20addition%2C%20we%20develop%20a%20variant%20with%20shifted%20metadata%2C%20LIME%2B1%2C%20that%20can%20guide%20token%20generation.%20Given%20prior%20metadata%20for%20the%20next%20token%2C%20LIME%2B1%20improves%20reasoning%20performance%20by%20up%20to%2038%25%20and%20arithmetic%20accuracy%20by%20up%20to%2035%25.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLIME%253A%2520Making%2520LLM%2520Data%2520More%2520Efficient%2520with%2520Linguistic%2520Metadata%2520Embeddings%26entry.906535625%3DSebastian%2520Sztwiertnia%2520and%2520Felix%2520Friedrich%2520and%2520Kristian%2520Kersting%2520and%2520Patrick%2520Schramowski%2520and%2520Bj%25C3%25B6rn%2520Deiseroth%26entry.1292438233%3DPre-training%2520decoder-only%2520language%2520models%2520relies%2520on%2520vast%2520amounts%2520of%2520high-quality%2520data%252C%2520yet%2520the%2520availability%2520of%2520such%2520data%2520is%2520increasingly%2520reaching%2520its%2520limits.%2520While%2520metadata%2520is%2520commonly%2520used%2520to%2520create%2520and%2520curate%2520these%2520datasets%252C%2520its%2520potential%2520as%2520a%2520direct%2520training%2520signal%2520remains%2520under-explored.%2520We%2520challenge%2520this%2520status%2520quo%2520and%2520propose%2520LIME%2520%2528Linguistic%2520Metadata%2520Embeddings%2529%252C%2520a%2520method%2520that%2520enriches%2520token%2520embeddings%2520with%2520metadata%2520capturing%2520syntax%252C%2520semantics%252C%2520and%2520contextual%2520properties.%2520LIME%2520substantially%2520improves%2520pre-training%2520efficiency.%2520Specifically%252C%2520it%2520adapts%2520up%2520to%252056%2525%2520faster%2520to%2520the%2520training%2520data%2520distribution%252C%2520while%2520introducing%2520only%25200.01%2525%2520additional%2520parameters%2520at%2520negligible%2520compute%2520overhead.%2520Beyond%2520efficiency%252C%2520LIME%2520improves%2520tokenization%252C%2520leading%2520to%2520remarkably%2520stronger%2520language%2520modeling%2520capabilities%2520and%2520generative%2520task%2520performance.%2520These%2520benefits%2520persist%2520across%2520model%2520scales%2520%2528500M%2520to%25202B%2529.%2520In%2520addition%252C%2520we%2520develop%2520a%2520variant%2520with%2520shifted%2520metadata%252C%2520LIME%252B1%252C%2520that%2520can%2520guide%2520token%2520generation.%2520Given%2520prior%2520metadata%2520for%2520the%2520next%2520token%252C%2520LIME%252B1%2520improves%2520reasoning%2520performance%2520by%2520up%2520to%252038%2525%2520and%2520arithmetic%2520accuracy%2520by%2520up%2520to%252035%2525.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LIME%3A%20Making%20LLM%20Data%20More%20Efficient%20with%20Linguistic%20Metadata%20Embeddings&entry.906535625=Sebastian%20Sztwiertnia%20and%20Felix%20Friedrich%20and%20Kristian%20Kersting%20and%20Patrick%20Schramowski%20and%20Bj%C3%B6rn%20Deiseroth&entry.1292438233=Pre-training%20decoder-only%20language%20models%20relies%20on%20vast%20amounts%20of%20high-quality%20data%2C%20yet%20the%20availability%20of%20such%20data%20is%20increasingly%20reaching%20its%20limits.%20While%20metadata%20is%20commonly%20used%20to%20create%20and%20curate%20these%20datasets%2C%20its%20potential%20as%20a%20direct%20training%20signal%20remains%20under-explored.%20We%20challenge%20this%20status%20quo%20and%20propose%20LIME%20%28Linguistic%20Metadata%20Embeddings%29%2C%20a%20method%20that%20enriches%20token%20embeddings%20with%20metadata%20capturing%20syntax%2C%20semantics%2C%20and%20contextual%20properties.%20LIME%20substantially%20improves%20pre-training%20efficiency.%20Specifically%2C%20it%20adapts%20up%20to%2056%25%20faster%20to%20the%20training%20data%20distribution%2C%20while%20introducing%20only%200.01%25%20additional%20parameters%20at%20negligible%20compute%20overhead.%20Beyond%20efficiency%2C%20LIME%20improves%20tokenization%2C%20leading%20to%20remarkably%20stronger%20language%20modeling%20capabilities%20and%20generative%20task%20performance.%20These%20benefits%20persist%20across%20model%20scales%20%28500M%20to%202B%29.%20In%20addition%2C%20we%20develop%20a%20variant%20with%20shifted%20metadata%2C%20LIME%2B1%2C%20that%20can%20guide%20token%20generation.%20Given%20prior%20metadata%20for%20the%20next%20token%2C%20LIME%2B1%20improves%20reasoning%20performance%20by%20up%20to%2038%25%20and%20arithmetic%20accuracy%20by%20up%20to%2035%25.&entry.1838667208=http%3A//arxiv.org/abs/2512.07522v1&entry.124074799=Read"},
{"title": "Materium: An Autoregressive Approach for Material Generation", "author": "Niklas Dobberstein and Jan Hamaekers", "abstract": "We present Materium: an autoregressive transformer for generating crystal structures that converts 3D material representations into token sequences. These sequences include elements with oxidation states, fractional coordinates and lattice parameters. Unlike diffusion approaches, which refine atomic positions iteratively through many denoising steps, Materium places atoms at precise fractional coordinates, enabling fast, scalable generation. With this design, the model can be trained in a few hours on a single GPU and generate samples much faster on GPUs and CPUs than diffusion-based approaches. The model was trained and evaluated using multiple properties as conditions, including fundamental properties, such as density and space group, as well as more practical targets, such as band gap and magnetic density. In both single and combined conditions, the model performs consistently well, producing candidates that align with the requested inputs.", "link": "http://arxiv.org/abs/2512.07486v1", "date": "2025-12-08", "relevancy": 2.5826, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5193}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5193}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Materium%3A%20An%20Autoregressive%20Approach%20for%20Material%20Generation&body=Title%3A%20Materium%3A%20An%20Autoregressive%20Approach%20for%20Material%20Generation%0AAuthor%3A%20Niklas%20Dobberstein%20and%20Jan%20Hamaekers%0AAbstract%3A%20We%20present%20Materium%3A%20an%20autoregressive%20transformer%20for%20generating%20crystal%20structures%20that%20converts%203D%20material%20representations%20into%20token%20sequences.%20These%20sequences%20include%20elements%20with%20oxidation%20states%2C%20fractional%20coordinates%20and%20lattice%20parameters.%20Unlike%20diffusion%20approaches%2C%20which%20refine%20atomic%20positions%20iteratively%20through%20many%20denoising%20steps%2C%20Materium%20places%20atoms%20at%20precise%20fractional%20coordinates%2C%20enabling%20fast%2C%20scalable%20generation.%20With%20this%20design%2C%20the%20model%20can%20be%20trained%20in%20a%20few%20hours%20on%20a%20single%20GPU%20and%20generate%20samples%20much%20faster%20on%20GPUs%20and%20CPUs%20than%20diffusion-based%20approaches.%20The%20model%20was%20trained%20and%20evaluated%20using%20multiple%20properties%20as%20conditions%2C%20including%20fundamental%20properties%2C%20such%20as%20density%20and%20space%20group%2C%20as%20well%20as%20more%20practical%20targets%2C%20such%20as%20band%20gap%20and%20magnetic%20density.%20In%20both%20single%20and%20combined%20conditions%2C%20the%20model%20performs%20consistently%20well%2C%20producing%20candidates%20that%20align%20with%20the%20requested%20inputs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07486v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaterium%253A%2520An%2520Autoregressive%2520Approach%2520for%2520Material%2520Generation%26entry.906535625%3DNiklas%2520Dobberstein%2520and%2520Jan%2520Hamaekers%26entry.1292438233%3DWe%2520present%2520Materium%253A%2520an%2520autoregressive%2520transformer%2520for%2520generating%2520crystal%2520structures%2520that%2520converts%25203D%2520material%2520representations%2520into%2520token%2520sequences.%2520These%2520sequences%2520include%2520elements%2520with%2520oxidation%2520states%252C%2520fractional%2520coordinates%2520and%2520lattice%2520parameters.%2520Unlike%2520diffusion%2520approaches%252C%2520which%2520refine%2520atomic%2520positions%2520iteratively%2520through%2520many%2520denoising%2520steps%252C%2520Materium%2520places%2520atoms%2520at%2520precise%2520fractional%2520coordinates%252C%2520enabling%2520fast%252C%2520scalable%2520generation.%2520With%2520this%2520design%252C%2520the%2520model%2520can%2520be%2520trained%2520in%2520a%2520few%2520hours%2520on%2520a%2520single%2520GPU%2520and%2520generate%2520samples%2520much%2520faster%2520on%2520GPUs%2520and%2520CPUs%2520than%2520diffusion-based%2520approaches.%2520The%2520model%2520was%2520trained%2520and%2520evaluated%2520using%2520multiple%2520properties%2520as%2520conditions%252C%2520including%2520fundamental%2520properties%252C%2520such%2520as%2520density%2520and%2520space%2520group%252C%2520as%2520well%2520as%2520more%2520practical%2520targets%252C%2520such%2520as%2520band%2520gap%2520and%2520magnetic%2520density.%2520In%2520both%2520single%2520and%2520combined%2520conditions%252C%2520the%2520model%2520performs%2520consistently%2520well%252C%2520producing%2520candidates%2520that%2520align%2520with%2520the%2520requested%2520inputs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07486v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Materium%3A%20An%20Autoregressive%20Approach%20for%20Material%20Generation&entry.906535625=Niklas%20Dobberstein%20and%20Jan%20Hamaekers&entry.1292438233=We%20present%20Materium%3A%20an%20autoregressive%20transformer%20for%20generating%20crystal%20structures%20that%20converts%203D%20material%20representations%20into%20token%20sequences.%20These%20sequences%20include%20elements%20with%20oxidation%20states%2C%20fractional%20coordinates%20and%20lattice%20parameters.%20Unlike%20diffusion%20approaches%2C%20which%20refine%20atomic%20positions%20iteratively%20through%20many%20denoising%20steps%2C%20Materium%20places%20atoms%20at%20precise%20fractional%20coordinates%2C%20enabling%20fast%2C%20scalable%20generation.%20With%20this%20design%2C%20the%20model%20can%20be%20trained%20in%20a%20few%20hours%20on%20a%20single%20GPU%20and%20generate%20samples%20much%20faster%20on%20GPUs%20and%20CPUs%20than%20diffusion-based%20approaches.%20The%20model%20was%20trained%20and%20evaluated%20using%20multiple%20properties%20as%20conditions%2C%20including%20fundamental%20properties%2C%20such%20as%20density%20and%20space%20group%2C%20as%20well%20as%20more%20practical%20targets%2C%20such%20as%20band%20gap%20and%20magnetic%20density.%20In%20both%20single%20and%20combined%20conditions%2C%20the%20model%20performs%20consistently%20well%2C%20producing%20candidates%20that%20align%20with%20the%20requested%20inputs.&entry.1838667208=http%3A//arxiv.org/abs/2512.07486v1&entry.124074799=Read"},
{"title": "Complementary Learning Approach for Text Classification using Large Language Models", "author": "Navid Asgari and Benjamin M. Cole", "abstract": "In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).", "link": "http://arxiv.org/abs/2512.07583v1", "date": "2025-12-08", "relevancy": 2.5647, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5214}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5087}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Complementary%20Learning%20Approach%20for%20Text%20Classification%20using%20Large%20Language%20Models&body=Title%3A%20Complementary%20Learning%20Approach%20for%20Text%20Classification%20using%20Large%20Language%20Models%0AAuthor%3A%20Navid%20Asgari%20and%20Benjamin%20M.%20Cole%0AAbstract%3A%20In%20this%20study%2C%20we%20propose%20a%20structured%20methodology%20that%20utilizes%20large%20language%20models%20%28LLMs%29%20in%20a%20cost-efficient%20and%20parsimonious%20manner%2C%20integrating%20the%20strengths%20of%20scholars%20and%20machines%20while%20offsetting%20their%20respective%20weaknesses.%20Our%20methodology%2C%20facilitated%20through%20a%20chain%20of%20thought%20and%20few-shot%20learning%20prompting%20from%20computer%20science%2C%20extends%20best%20practices%20for%20co-author%20teams%20in%20qualitative%20research%20to%20human-machine%20teams%20in%20quantitative%20research.%20This%20allows%20humans%20to%20utilize%20abductive%20reasoning%20and%20natural%20language%20to%20interrogate%20not%20just%20what%20the%20machine%20has%20done%20but%20also%20what%20the%20human%20has%20done.%20Our%20method%20highlights%20how%20scholars%20can%20manage%20inherent%20weaknesses%20OF%20LLMs%20using%20careful%2C%20low-cost%20techniques.%20We%20demonstrate%20how%20to%20use%20the%20methodology%20to%20interrogate%20human-machine%20rating%20discrepancies%20for%20a%20sample%20of%201%2C934%20press%20releases%20announcing%20pharmaceutical%20alliances%20%281990-2017%29.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComplementary%2520Learning%2520Approach%2520for%2520Text%2520Classification%2520using%2520Large%2520Language%2520Models%26entry.906535625%3DNavid%2520Asgari%2520and%2520Benjamin%2520M.%2520Cole%26entry.1292438233%3DIn%2520this%2520study%252C%2520we%2520propose%2520a%2520structured%2520methodology%2520that%2520utilizes%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520a%2520cost-efficient%2520and%2520parsimonious%2520manner%252C%2520integrating%2520the%2520strengths%2520of%2520scholars%2520and%2520machines%2520while%2520offsetting%2520their%2520respective%2520weaknesses.%2520Our%2520methodology%252C%2520facilitated%2520through%2520a%2520chain%2520of%2520thought%2520and%2520few-shot%2520learning%2520prompting%2520from%2520computer%2520science%252C%2520extends%2520best%2520practices%2520for%2520co-author%2520teams%2520in%2520qualitative%2520research%2520to%2520human-machine%2520teams%2520in%2520quantitative%2520research.%2520This%2520allows%2520humans%2520to%2520utilize%2520abductive%2520reasoning%2520and%2520natural%2520language%2520to%2520interrogate%2520not%2520just%2520what%2520the%2520machine%2520has%2520done%2520but%2520also%2520what%2520the%2520human%2520has%2520done.%2520Our%2520method%2520highlights%2520how%2520scholars%2520can%2520manage%2520inherent%2520weaknesses%2520OF%2520LLMs%2520using%2520careful%252C%2520low-cost%2520techniques.%2520We%2520demonstrate%2520how%2520to%2520use%2520the%2520methodology%2520to%2520interrogate%2520human-machine%2520rating%2520discrepancies%2520for%2520a%2520sample%2520of%25201%252C934%2520press%2520releases%2520announcing%2520pharmaceutical%2520alliances%2520%25281990-2017%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Complementary%20Learning%20Approach%20for%20Text%20Classification%20using%20Large%20Language%20Models&entry.906535625=Navid%20Asgari%20and%20Benjamin%20M.%20Cole&entry.1292438233=In%20this%20study%2C%20we%20propose%20a%20structured%20methodology%20that%20utilizes%20large%20language%20models%20%28LLMs%29%20in%20a%20cost-efficient%20and%20parsimonious%20manner%2C%20integrating%20the%20strengths%20of%20scholars%20and%20machines%20while%20offsetting%20their%20respective%20weaknesses.%20Our%20methodology%2C%20facilitated%20through%20a%20chain%20of%20thought%20and%20few-shot%20learning%20prompting%20from%20computer%20science%2C%20extends%20best%20practices%20for%20co-author%20teams%20in%20qualitative%20research%20to%20human-machine%20teams%20in%20quantitative%20research.%20This%20allows%20humans%20to%20utilize%20abductive%20reasoning%20and%20natural%20language%20to%20interrogate%20not%20just%20what%20the%20machine%20has%20done%20but%20also%20what%20the%20human%20has%20done.%20Our%20method%20highlights%20how%20scholars%20can%20manage%20inherent%20weaknesses%20OF%20LLMs%20using%20careful%2C%20low-cost%20techniques.%20We%20demonstrate%20how%20to%20use%20the%20methodology%20to%20interrogate%20human-machine%20rating%20discrepancies%20for%20a%20sample%20of%201%2C934%20press%20releases%20announcing%20pharmaceutical%20alliances%20%281990-2017%29.&entry.1838667208=http%3A//arxiv.org/abs/2512.07583v1&entry.124074799=Read"},
{"title": "Group Representational Position Encoding", "author": "Yifan Zhang and Zixiang Chen and Yifeng Liu and Zhen Qin and Huizhuo Yuan and Kangping Xu and Yang Yuan and Quanquan Gu and Andrew Chi-Chih Yao", "abstract": "We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\\mathrm{GL}$. In Multiplicative GRAPE, a position $n \\in \\mathbb{Z}$ (or $t \\in \\mathbb{R}$) acts as $\\mathbf{G}(n)=\\exp(n\\,\u03c9\\,\\mathbf{L})$ with a rank-2 skew generator $\\mathbf{L} \\in \\mathbb{R}^{d \\times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.", "link": "http://arxiv.org/abs/2512.07805v1", "date": "2025-12-08", "relevancy": 2.5502, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5363}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5138}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Group%20Representational%20Position%20Encoding&body=Title%3A%20Group%20Representational%20Position%20Encoding%0AAuthor%3A%20Yifan%20Zhang%20and%20Zixiang%20Chen%20and%20Yifeng%20Liu%20and%20Zhen%20Qin%20and%20Huizhuo%20Yuan%20and%20Kangping%20Xu%20and%20Yang%20Yuan%20and%20Quanquan%20Gu%20and%20Andrew%20Chi-Chih%20Yao%0AAbstract%3A%20We%20present%20GRAPE%20%28Group%20RepresentAtional%20Position%20Encoding%29%2C%20a%20unified%20framework%20for%20positional%20encoding%20based%20on%20group%20actions.%20GRAPE%20brings%20together%20two%20families%20of%20mechanisms%3A%20%28i%29%20multiplicative%20rotations%20%28Multiplicative%20GRAPE%29%20in%20%24%5Cmathrm%7BSO%7D%28d%29%24%20and%20%28ii%29%20additive%20logit%20biases%20%28Additive%20GRAPE%29%20arising%20from%20unipotent%20actions%20in%20the%20general%20linear%20group%20%24%5Cmathrm%7BGL%7D%24.%20In%20Multiplicative%20GRAPE%2C%20a%20position%20%24n%20%5Cin%20%5Cmathbb%7BZ%7D%24%20%28or%20%24t%20%5Cin%20%5Cmathbb%7BR%7D%24%29%20acts%20as%20%24%5Cmathbf%7BG%7D%28n%29%3D%5Cexp%28n%5C%2C%CF%89%5C%2C%5Cmathbf%7BL%7D%29%24%20with%20a%20rank-2%20skew%20generator%20%24%5Cmathbf%7BL%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd%20%5Ctimes%20d%7D%24%2C%20yielding%20a%20relative%2C%20compositional%2C%20norm-preserving%20map%20with%20a%20closed-form%20matrix%20exponential.%20RoPE%20is%20recovered%20exactly%20when%20the%20%24d/2%24%20planes%20are%20the%20canonical%20coordinate%20pairs%20with%20log-uniform%20spectrum.%20Learned%20commuting%20subspaces%20and%20compact%20non-commuting%20mixtures%20strictly%20extend%20this%20geometry%20to%20capture%20cross-subspace%20feature%20coupling%20at%20%24O%28d%29%24%20and%20%24O%28r%20d%29%24%20cost%20per%20head%2C%20respectively.%20In%20Additive%20GRAPE%2C%20additive%20logits%20arise%20as%20rank-1%20%28or%20low-rank%29%20unipotent%20actions%2C%20recovering%20ALiBi%20and%20the%20Forgetting%20Transformer%20%28FoX%29%20as%20exact%20special%20cases%20while%20preserving%20an%20exact%20relative%20law%20and%20streaming%20cacheability.%20Altogether%2C%20GRAPE%20supplies%20a%20principled%20design%20space%20for%20positional%20geometry%20in%20long-context%20models%2C%20subsuming%20RoPE%20and%20ALiBi%20as%20special%20cases.%20Project%20Page%3A%20https%3A//github.com/model-architectures/GRAPE.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGroup%2520Representational%2520Position%2520Encoding%26entry.906535625%3DYifan%2520Zhang%2520and%2520Zixiang%2520Chen%2520and%2520Yifeng%2520Liu%2520and%2520Zhen%2520Qin%2520and%2520Huizhuo%2520Yuan%2520and%2520Kangping%2520Xu%2520and%2520Yang%2520Yuan%2520and%2520Quanquan%2520Gu%2520and%2520Andrew%2520Chi-Chih%2520Yao%26entry.1292438233%3DWe%2520present%2520GRAPE%2520%2528Group%2520RepresentAtional%2520Position%2520Encoding%2529%252C%2520a%2520unified%2520framework%2520for%2520positional%2520encoding%2520based%2520on%2520group%2520actions.%2520GRAPE%2520brings%2520together%2520two%2520families%2520of%2520mechanisms%253A%2520%2528i%2529%2520multiplicative%2520rotations%2520%2528Multiplicative%2520GRAPE%2529%2520in%2520%2524%255Cmathrm%257BSO%257D%2528d%2529%2524%2520and%2520%2528ii%2529%2520additive%2520logit%2520biases%2520%2528Additive%2520GRAPE%2529%2520arising%2520from%2520unipotent%2520actions%2520in%2520the%2520general%2520linear%2520group%2520%2524%255Cmathrm%257BGL%257D%2524.%2520In%2520Multiplicative%2520GRAPE%252C%2520a%2520position%2520%2524n%2520%255Cin%2520%255Cmathbb%257BZ%257D%2524%2520%2528or%2520%2524t%2520%255Cin%2520%255Cmathbb%257BR%257D%2524%2529%2520acts%2520as%2520%2524%255Cmathbf%257BG%257D%2528n%2529%253D%255Cexp%2528n%255C%252C%25CF%2589%255C%252C%255Cmathbf%257BL%257D%2529%2524%2520with%2520a%2520rank-2%2520skew%2520generator%2520%2524%255Cmathbf%257BL%257D%2520%255Cin%2520%255Cmathbb%257BR%257D%255E%257Bd%2520%255Ctimes%2520d%257D%2524%252C%2520yielding%2520a%2520relative%252C%2520compositional%252C%2520norm-preserving%2520map%2520with%2520a%2520closed-form%2520matrix%2520exponential.%2520RoPE%2520is%2520recovered%2520exactly%2520when%2520the%2520%2524d/2%2524%2520planes%2520are%2520the%2520canonical%2520coordinate%2520pairs%2520with%2520log-uniform%2520spectrum.%2520Learned%2520commuting%2520subspaces%2520and%2520compact%2520non-commuting%2520mixtures%2520strictly%2520extend%2520this%2520geometry%2520to%2520capture%2520cross-subspace%2520feature%2520coupling%2520at%2520%2524O%2528d%2529%2524%2520and%2520%2524O%2528r%2520d%2529%2524%2520cost%2520per%2520head%252C%2520respectively.%2520In%2520Additive%2520GRAPE%252C%2520additive%2520logits%2520arise%2520as%2520rank-1%2520%2528or%2520low-rank%2529%2520unipotent%2520actions%252C%2520recovering%2520ALiBi%2520and%2520the%2520Forgetting%2520Transformer%2520%2528FoX%2529%2520as%2520exact%2520special%2520cases%2520while%2520preserving%2520an%2520exact%2520relative%2520law%2520and%2520streaming%2520cacheability.%2520Altogether%252C%2520GRAPE%2520supplies%2520a%2520principled%2520design%2520space%2520for%2520positional%2520geometry%2520in%2520long-context%2520models%252C%2520subsuming%2520RoPE%2520and%2520ALiBi%2520as%2520special%2520cases.%2520Project%2520Page%253A%2520https%253A//github.com/model-architectures/GRAPE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Group%20Representational%20Position%20Encoding&entry.906535625=Yifan%20Zhang%20and%20Zixiang%20Chen%20and%20Yifeng%20Liu%20and%20Zhen%20Qin%20and%20Huizhuo%20Yuan%20and%20Kangping%20Xu%20and%20Yang%20Yuan%20and%20Quanquan%20Gu%20and%20Andrew%20Chi-Chih%20Yao&entry.1292438233=We%20present%20GRAPE%20%28Group%20RepresentAtional%20Position%20Encoding%29%2C%20a%20unified%20framework%20for%20positional%20encoding%20based%20on%20group%20actions.%20GRAPE%20brings%20together%20two%20families%20of%20mechanisms%3A%20%28i%29%20multiplicative%20rotations%20%28Multiplicative%20GRAPE%29%20in%20%24%5Cmathrm%7BSO%7D%28d%29%24%20and%20%28ii%29%20additive%20logit%20biases%20%28Additive%20GRAPE%29%20arising%20from%20unipotent%20actions%20in%20the%20general%20linear%20group%20%24%5Cmathrm%7BGL%7D%24.%20In%20Multiplicative%20GRAPE%2C%20a%20position%20%24n%20%5Cin%20%5Cmathbb%7BZ%7D%24%20%28or%20%24t%20%5Cin%20%5Cmathbb%7BR%7D%24%29%20acts%20as%20%24%5Cmathbf%7BG%7D%28n%29%3D%5Cexp%28n%5C%2C%CF%89%5C%2C%5Cmathbf%7BL%7D%29%24%20with%20a%20rank-2%20skew%20generator%20%24%5Cmathbf%7BL%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd%20%5Ctimes%20d%7D%24%2C%20yielding%20a%20relative%2C%20compositional%2C%20norm-preserving%20map%20with%20a%20closed-form%20matrix%20exponential.%20RoPE%20is%20recovered%20exactly%20when%20the%20%24d/2%24%20planes%20are%20the%20canonical%20coordinate%20pairs%20with%20log-uniform%20spectrum.%20Learned%20commuting%20subspaces%20and%20compact%20non-commuting%20mixtures%20strictly%20extend%20this%20geometry%20to%20capture%20cross-subspace%20feature%20coupling%20at%20%24O%28d%29%24%20and%20%24O%28r%20d%29%24%20cost%20per%20head%2C%20respectively.%20In%20Additive%20GRAPE%2C%20additive%20logits%20arise%20as%20rank-1%20%28or%20low-rank%29%20unipotent%20actions%2C%20recovering%20ALiBi%20and%20the%20Forgetting%20Transformer%20%28FoX%29%20as%20exact%20special%20cases%20while%20preserving%20an%20exact%20relative%20law%20and%20streaming%20cacheability.%20Altogether%2C%20GRAPE%20supplies%20a%20principled%20design%20space%20for%20positional%20geometry%20in%20long-context%20models%2C%20subsuming%20RoPE%20and%20ALiBi%20as%20special%20cases.%20Project%20Page%3A%20https%3A//github.com/model-architectures/GRAPE.&entry.1838667208=http%3A//arxiv.org/abs/2512.07805v1&entry.124074799=Read"},
{"title": "WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling", "author": "Shaoheng Fang and Hanwen Jiang and Yunpeng Bai and Niloy J. Mitra and Qixing Huang", "abstract": "Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.", "link": "http://arxiv.org/abs/2512.07821v1", "date": "2025-12-08", "relevancy": 2.5388, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.7252}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6204}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WorldReel%3A%204D%20Video%20Generation%20with%20Consistent%20Geometry%20and%20Motion%20Modeling&body=Title%3A%20WorldReel%3A%204D%20Video%20Generation%20with%20Consistent%20Geometry%20and%20Motion%20Modeling%0AAuthor%3A%20Shaoheng%20Fang%20and%20Hanwen%20Jiang%20and%20Yunpeng%20Bai%20and%20Niloy%20J.%20Mitra%20and%20Qixing%20Huang%0AAbstract%3A%20Recent%20video%20generators%20achieve%20striking%20photorealism%2C%20yet%20remain%20fundamentally%20inconsistent%20in%203D.%20We%20present%20WorldReel%2C%20a%204D%20video%20generator%20that%20is%20natively%20spatio-temporally%20consistent.%20WorldReel%20jointly%20produces%20RGB%20frames%20together%20with%204D%20scene%20representations%2C%20including%20pointmaps%2C%20camera%20trajectory%2C%20and%20dense%20flow%20mapping%2C%20enabling%20coherent%20geometry%20and%20appearance%20modeling%20over%20time.%20Our%20explicit%204D%20representation%20enforces%20a%20single%20underlying%20scene%20that%20persists%20across%20viewpoints%20and%20dynamic%20content%2C%20yielding%20videos%20that%20remain%20consistent%20even%20under%20large%20non-rigid%20motion%20and%20significant%20camera%20movement.%20We%20train%20WorldReel%20by%20carefully%20combining%20synthetic%20and%20real%20data%3A%20synthetic%20data%20providing%20precise%204D%20supervision%20%28geometry%2C%20motion%2C%20and%20camera%29%2C%20while%20real%20videos%20contribute%20visual%20diversity%20and%20realism.%20This%20blend%20allows%20WorldReel%20to%20generalize%20to%20in-the-wild%20footage%20while%20preserving%20strong%20geometric%20fidelity.%20Extensive%20experiments%20demonstrate%20that%20WorldReel%20sets%20a%20new%20state-of-the-art%20for%20consistent%20video%20generation%20with%20dynamic%20scenes%20and%20moving%20cameras%2C%20improving%20metrics%20of%20geometric%20consistency%2C%20motion%20coherence%2C%20and%20reducing%20view-time%20artifacts%20over%20competing%20methods.%20We%20believe%20that%20WorldReel%20brings%20video%20generation%20closer%20to%204D-consistent%20world%20modeling%2C%20where%20agents%20can%20render%2C%20interact%2C%20and%20reason%20about%20scenes%20through%20a%20single%20and%20stable%20spatiotemporal%20representation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWorldReel%253A%25204D%2520Video%2520Generation%2520with%2520Consistent%2520Geometry%2520and%2520Motion%2520Modeling%26entry.906535625%3DShaoheng%2520Fang%2520and%2520Hanwen%2520Jiang%2520and%2520Yunpeng%2520Bai%2520and%2520Niloy%2520J.%2520Mitra%2520and%2520Qixing%2520Huang%26entry.1292438233%3DRecent%2520video%2520generators%2520achieve%2520striking%2520photorealism%252C%2520yet%2520remain%2520fundamentally%2520inconsistent%2520in%25203D.%2520We%2520present%2520WorldReel%252C%2520a%25204D%2520video%2520generator%2520that%2520is%2520natively%2520spatio-temporally%2520consistent.%2520WorldReel%2520jointly%2520produces%2520RGB%2520frames%2520together%2520with%25204D%2520scene%2520representations%252C%2520including%2520pointmaps%252C%2520camera%2520trajectory%252C%2520and%2520dense%2520flow%2520mapping%252C%2520enabling%2520coherent%2520geometry%2520and%2520appearance%2520modeling%2520over%2520time.%2520Our%2520explicit%25204D%2520representation%2520enforces%2520a%2520single%2520underlying%2520scene%2520that%2520persists%2520across%2520viewpoints%2520and%2520dynamic%2520content%252C%2520yielding%2520videos%2520that%2520remain%2520consistent%2520even%2520under%2520large%2520non-rigid%2520motion%2520and%2520significant%2520camera%2520movement.%2520We%2520train%2520WorldReel%2520by%2520carefully%2520combining%2520synthetic%2520and%2520real%2520data%253A%2520synthetic%2520data%2520providing%2520precise%25204D%2520supervision%2520%2528geometry%252C%2520motion%252C%2520and%2520camera%2529%252C%2520while%2520real%2520videos%2520contribute%2520visual%2520diversity%2520and%2520realism.%2520This%2520blend%2520allows%2520WorldReel%2520to%2520generalize%2520to%2520in-the-wild%2520footage%2520while%2520preserving%2520strong%2520geometric%2520fidelity.%2520Extensive%2520experiments%2520demonstrate%2520that%2520WorldReel%2520sets%2520a%2520new%2520state-of-the-art%2520for%2520consistent%2520video%2520generation%2520with%2520dynamic%2520scenes%2520and%2520moving%2520cameras%252C%2520improving%2520metrics%2520of%2520geometric%2520consistency%252C%2520motion%2520coherence%252C%2520and%2520reducing%2520view-time%2520artifacts%2520over%2520competing%2520methods.%2520We%2520believe%2520that%2520WorldReel%2520brings%2520video%2520generation%2520closer%2520to%25204D-consistent%2520world%2520modeling%252C%2520where%2520agents%2520can%2520render%252C%2520interact%252C%2520and%2520reason%2520about%2520scenes%2520through%2520a%2520single%2520and%2520stable%2520spatiotemporal%2520representation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WorldReel%3A%204D%20Video%20Generation%20with%20Consistent%20Geometry%20and%20Motion%20Modeling&entry.906535625=Shaoheng%20Fang%20and%20Hanwen%20Jiang%20and%20Yunpeng%20Bai%20and%20Niloy%20J.%20Mitra%20and%20Qixing%20Huang&entry.1292438233=Recent%20video%20generators%20achieve%20striking%20photorealism%2C%20yet%20remain%20fundamentally%20inconsistent%20in%203D.%20We%20present%20WorldReel%2C%20a%204D%20video%20generator%20that%20is%20natively%20spatio-temporally%20consistent.%20WorldReel%20jointly%20produces%20RGB%20frames%20together%20with%204D%20scene%20representations%2C%20including%20pointmaps%2C%20camera%20trajectory%2C%20and%20dense%20flow%20mapping%2C%20enabling%20coherent%20geometry%20and%20appearance%20modeling%20over%20time.%20Our%20explicit%204D%20representation%20enforces%20a%20single%20underlying%20scene%20that%20persists%20across%20viewpoints%20and%20dynamic%20content%2C%20yielding%20videos%20that%20remain%20consistent%20even%20under%20large%20non-rigid%20motion%20and%20significant%20camera%20movement.%20We%20train%20WorldReel%20by%20carefully%20combining%20synthetic%20and%20real%20data%3A%20synthetic%20data%20providing%20precise%204D%20supervision%20%28geometry%2C%20motion%2C%20and%20camera%29%2C%20while%20real%20videos%20contribute%20visual%20diversity%20and%20realism.%20This%20blend%20allows%20WorldReel%20to%20generalize%20to%20in-the-wild%20footage%20while%20preserving%20strong%20geometric%20fidelity.%20Extensive%20experiments%20demonstrate%20that%20WorldReel%20sets%20a%20new%20state-of-the-art%20for%20consistent%20video%20generation%20with%20dynamic%20scenes%20and%20moving%20cameras%2C%20improving%20metrics%20of%20geometric%20consistency%2C%20motion%20coherence%2C%20and%20reducing%20view-time%20artifacts%20over%20competing%20methods.%20We%20believe%20that%20WorldReel%20brings%20video%20generation%20closer%20to%204D-consistent%20world%20modeling%2C%20where%20agents%20can%20render%2C%20interact%2C%20and%20reason%20about%20scenes%20through%20a%20single%20and%20stable%20spatiotemporal%20representation.&entry.1838667208=http%3A//arxiv.org/abs/2512.07821v1&entry.124074799=Read"},
{"title": "Enhancing Small Object Detection with YOLO: A Novel Framework for Improved Accuracy and Efficiency", "author": "Mahila Moghadami and Mohammad Ali Keyvanrad and Melika Sabaghian", "abstract": "This paper investigates and develops methods for detecting small objects in large-scale aerial images. Current approaches for detecting small objects in aerial images often involve image cropping and modifications to detector network architectures. Techniques such as sliding window cropping and architectural enhancements, including higher-resolution feature maps and attention mechanisms, are commonly employed. Given the growing importance of aerial imagery in various critical and industrial applications, the need for robust frameworks for small object detection becomes imperative. To address this need, we adopted the base SW-YOLO approach to enhance speed and accuracy in small object detection by refining cropping dimensions and overlap in sliding window usage and subsequently enhanced it through architectural modifications. we propose a novel model by modifying the base model architecture, including advanced feature extraction modules in the neck for feature map enhancement, integrating CBAM in the backbone to preserve spatial and channel information, and introducing a new head to boost small object detection accuracy. Finally, we compared our method with SAHI, one of the most powerful frameworks for processing large-scale images, and CZDet, which is also based on image cropping, achieving significant improvements in accuracy. The proposed model achieves significant accuracy gains on the VisDrone2019 dataset, outperforming baseline YOLOv5L detection by a substantial margin. Specifically, the final proposed model elevates the mAP .5.5 accuracy on the VisDrone2019 dataset from the base accuracy of 35.5 achieved by the YOLOv5L detector to 61.2. Notably, the accuracy of CZDet, which is another classic method applied to this dataset, is 58.36. This research demonstrates a significant improvement, achieving an increase in accuracy from 35.5 to 61.2.", "link": "http://arxiv.org/abs/2512.07379v1", "date": "2025-12-08", "relevancy": 2.5321, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5138}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.504}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Small%20Object%20Detection%20with%20YOLO%3A%20A%20Novel%20Framework%20for%20Improved%20Accuracy%20and%20Efficiency&body=Title%3A%20Enhancing%20Small%20Object%20Detection%20with%20YOLO%3A%20A%20Novel%20Framework%20for%20Improved%20Accuracy%20and%20Efficiency%0AAuthor%3A%20Mahila%20Moghadami%20and%20Mohammad%20Ali%20Keyvanrad%20and%20Melika%20Sabaghian%0AAbstract%3A%20This%20paper%20investigates%20and%20develops%20methods%20for%20detecting%20small%20objects%20in%20large-scale%20aerial%20images.%20Current%20approaches%20for%20detecting%20small%20objects%20in%20aerial%20images%20often%20involve%20image%20cropping%20and%20modifications%20to%20detector%20network%20architectures.%20Techniques%20such%20as%20sliding%20window%20cropping%20and%20architectural%20enhancements%2C%20including%20higher-resolution%20feature%20maps%20and%20attention%20mechanisms%2C%20are%20commonly%20employed.%20Given%20the%20growing%20importance%20of%20aerial%20imagery%20in%20various%20critical%20and%20industrial%20applications%2C%20the%20need%20for%20robust%20frameworks%20for%20small%20object%20detection%20becomes%20imperative.%20To%20address%20this%20need%2C%20we%20adopted%20the%20base%20SW-YOLO%20approach%20to%20enhance%20speed%20and%20accuracy%20in%20small%20object%20detection%20by%20refining%20cropping%20dimensions%20and%20overlap%20in%20sliding%20window%20usage%20and%20subsequently%20enhanced%20it%20through%20architectural%20modifications.%20we%20propose%20a%20novel%20model%20by%20modifying%20the%20base%20model%20architecture%2C%20including%20advanced%20feature%20extraction%20modules%20in%20the%20neck%20for%20feature%20map%20enhancement%2C%20integrating%20CBAM%20in%20the%20backbone%20to%20preserve%20spatial%20and%20channel%20information%2C%20and%20introducing%20a%20new%20head%20to%20boost%20small%20object%20detection%20accuracy.%20Finally%2C%20we%20compared%20our%20method%20with%20SAHI%2C%20one%20of%20the%20most%20powerful%20frameworks%20for%20processing%20large-scale%20images%2C%20and%20CZDet%2C%20which%20is%20also%20based%20on%20image%20cropping%2C%20achieving%20significant%20improvements%20in%20accuracy.%20The%20proposed%20model%20achieves%20significant%20accuracy%20gains%20on%20the%20VisDrone2019%20dataset%2C%20outperforming%20baseline%20YOLOv5L%20detection%20by%20a%20substantial%20margin.%20Specifically%2C%20the%20final%20proposed%20model%20elevates%20the%20mAP%20.5.5%20accuracy%20on%20the%20VisDrone2019%20dataset%20from%20the%20base%20accuracy%20of%2035.5%20achieved%20by%20the%20YOLOv5L%20detector%20to%2061.2.%20Notably%2C%20the%20accuracy%20of%20CZDet%2C%20which%20is%20another%20classic%20method%20applied%20to%20this%20dataset%2C%20is%2058.36.%20This%20research%20demonstrates%20a%20significant%20improvement%2C%20achieving%20an%20increase%20in%20accuracy%20from%2035.5%20to%2061.2.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Small%2520Object%2520Detection%2520with%2520YOLO%253A%2520A%2520Novel%2520Framework%2520for%2520Improved%2520Accuracy%2520and%2520Efficiency%26entry.906535625%3DMahila%2520Moghadami%2520and%2520Mohammad%2520Ali%2520Keyvanrad%2520and%2520Melika%2520Sabaghian%26entry.1292438233%3DThis%2520paper%2520investigates%2520and%2520develops%2520methods%2520for%2520detecting%2520small%2520objects%2520in%2520large-scale%2520aerial%2520images.%2520Current%2520approaches%2520for%2520detecting%2520small%2520objects%2520in%2520aerial%2520images%2520often%2520involve%2520image%2520cropping%2520and%2520modifications%2520to%2520detector%2520network%2520architectures.%2520Techniques%2520such%2520as%2520sliding%2520window%2520cropping%2520and%2520architectural%2520enhancements%252C%2520including%2520higher-resolution%2520feature%2520maps%2520and%2520attention%2520mechanisms%252C%2520are%2520commonly%2520employed.%2520Given%2520the%2520growing%2520importance%2520of%2520aerial%2520imagery%2520in%2520various%2520critical%2520and%2520industrial%2520applications%252C%2520the%2520need%2520for%2520robust%2520frameworks%2520for%2520small%2520object%2520detection%2520becomes%2520imperative.%2520To%2520address%2520this%2520need%252C%2520we%2520adopted%2520the%2520base%2520SW-YOLO%2520approach%2520to%2520enhance%2520speed%2520and%2520accuracy%2520in%2520small%2520object%2520detection%2520by%2520refining%2520cropping%2520dimensions%2520and%2520overlap%2520in%2520sliding%2520window%2520usage%2520and%2520subsequently%2520enhanced%2520it%2520through%2520architectural%2520modifications.%2520we%2520propose%2520a%2520novel%2520model%2520by%2520modifying%2520the%2520base%2520model%2520architecture%252C%2520including%2520advanced%2520feature%2520extraction%2520modules%2520in%2520the%2520neck%2520for%2520feature%2520map%2520enhancement%252C%2520integrating%2520CBAM%2520in%2520the%2520backbone%2520to%2520preserve%2520spatial%2520and%2520channel%2520information%252C%2520and%2520introducing%2520a%2520new%2520head%2520to%2520boost%2520small%2520object%2520detection%2520accuracy.%2520Finally%252C%2520we%2520compared%2520our%2520method%2520with%2520SAHI%252C%2520one%2520of%2520the%2520most%2520powerful%2520frameworks%2520for%2520processing%2520large-scale%2520images%252C%2520and%2520CZDet%252C%2520which%2520is%2520also%2520based%2520on%2520image%2520cropping%252C%2520achieving%2520significant%2520improvements%2520in%2520accuracy.%2520The%2520proposed%2520model%2520achieves%2520significant%2520accuracy%2520gains%2520on%2520the%2520VisDrone2019%2520dataset%252C%2520outperforming%2520baseline%2520YOLOv5L%2520detection%2520by%2520a%2520substantial%2520margin.%2520Specifically%252C%2520the%2520final%2520proposed%2520model%2520elevates%2520the%2520mAP%2520.5.5%2520accuracy%2520on%2520the%2520VisDrone2019%2520dataset%2520from%2520the%2520base%2520accuracy%2520of%252035.5%2520achieved%2520by%2520the%2520YOLOv5L%2520detector%2520to%252061.2.%2520Notably%252C%2520the%2520accuracy%2520of%2520CZDet%252C%2520which%2520is%2520another%2520classic%2520method%2520applied%2520to%2520this%2520dataset%252C%2520is%252058.36.%2520This%2520research%2520demonstrates%2520a%2520significant%2520improvement%252C%2520achieving%2520an%2520increase%2520in%2520accuracy%2520from%252035.5%2520to%252061.2.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Small%20Object%20Detection%20with%20YOLO%3A%20A%20Novel%20Framework%20for%20Improved%20Accuracy%20and%20Efficiency&entry.906535625=Mahila%20Moghadami%20and%20Mohammad%20Ali%20Keyvanrad%20and%20Melika%20Sabaghian&entry.1292438233=This%20paper%20investigates%20and%20develops%20methods%20for%20detecting%20small%20objects%20in%20large-scale%20aerial%20images.%20Current%20approaches%20for%20detecting%20small%20objects%20in%20aerial%20images%20often%20involve%20image%20cropping%20and%20modifications%20to%20detector%20network%20architectures.%20Techniques%20such%20as%20sliding%20window%20cropping%20and%20architectural%20enhancements%2C%20including%20higher-resolution%20feature%20maps%20and%20attention%20mechanisms%2C%20are%20commonly%20employed.%20Given%20the%20growing%20importance%20of%20aerial%20imagery%20in%20various%20critical%20and%20industrial%20applications%2C%20the%20need%20for%20robust%20frameworks%20for%20small%20object%20detection%20becomes%20imperative.%20To%20address%20this%20need%2C%20we%20adopted%20the%20base%20SW-YOLO%20approach%20to%20enhance%20speed%20and%20accuracy%20in%20small%20object%20detection%20by%20refining%20cropping%20dimensions%20and%20overlap%20in%20sliding%20window%20usage%20and%20subsequently%20enhanced%20it%20through%20architectural%20modifications.%20we%20propose%20a%20novel%20model%20by%20modifying%20the%20base%20model%20architecture%2C%20including%20advanced%20feature%20extraction%20modules%20in%20the%20neck%20for%20feature%20map%20enhancement%2C%20integrating%20CBAM%20in%20the%20backbone%20to%20preserve%20spatial%20and%20channel%20information%2C%20and%20introducing%20a%20new%20head%20to%20boost%20small%20object%20detection%20accuracy.%20Finally%2C%20we%20compared%20our%20method%20with%20SAHI%2C%20one%20of%20the%20most%20powerful%20frameworks%20for%20processing%20large-scale%20images%2C%20and%20CZDet%2C%20which%20is%20also%20based%20on%20image%20cropping%2C%20achieving%20significant%20improvements%20in%20accuracy.%20The%20proposed%20model%20achieves%20significant%20accuracy%20gains%20on%20the%20VisDrone2019%20dataset%2C%20outperforming%20baseline%20YOLOv5L%20detection%20by%20a%20substantial%20margin.%20Specifically%2C%20the%20final%20proposed%20model%20elevates%20the%20mAP%20.5.5%20accuracy%20on%20the%20VisDrone2019%20dataset%20from%20the%20base%20accuracy%20of%2035.5%20achieved%20by%20the%20YOLOv5L%20detector%20to%2061.2.%20Notably%2C%20the%20accuracy%20of%20CZDet%2C%20which%20is%20another%20classic%20method%20applied%20to%20this%20dataset%2C%20is%2058.36.%20This%20research%20demonstrates%20a%20significant%20improvement%2C%20achieving%20an%20increase%20in%20accuracy%20from%2035.5%20to%2061.2.&entry.1838667208=http%3A//arxiv.org/abs/2512.07379v1&entry.124074799=Read"},
{"title": "Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling", "author": "Meng Cao and Haokun Lin and Haoyuan Li and Haoran Tang and Rongtao Xu and Dong An and Xue Liu and Ian Reid and Xiaodan Liang", "abstract": "Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM's symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.", "link": "http://arxiv.org/abs/2512.01821v2", "date": "2025-12-08", "relevancy": 2.5114, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6318}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6318}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20through%20Imagination%3A%20Learning%20Scene%20Geometry%20via%20Implicit%20Spatial%20World%20Modeling&body=Title%3A%20Seeing%20through%20Imagination%3A%20Learning%20Scene%20Geometry%20via%20Implicit%20Spatial%20World%20Modeling%0AAuthor%3A%20Meng%20Cao%20and%20Haokun%20Lin%20and%20Haoyuan%20Li%20and%20Haoran%20Tang%20and%20Rongtao%20Xu%20and%20Dong%20An%20and%20Xue%20Liu%20and%20Ian%20Reid%20and%20Xiaodan%20Liang%0AAbstract%3A%20Spatial%20reasoning%2C%20the%20ability%20to%20understand%20and%20interpret%20the%203D%20structure%20of%20the%20world%2C%20is%20a%20critical%20yet%20underdeveloped%20capability%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20Current%20methods%20predominantly%20rely%20on%20verbal%20descriptive%20tuning%2C%20which%20suffers%20from%20visual%20illiteracy%2C%20i.e.%2C%20they%20learn%20spatial%20concepts%20through%20textual%20symbols%20alone%2C%20devoid%20of%20connection%20to%20their%20visual%20manifestations.%20To%20bridge%20this%20gap%2C%20this%20paper%20introduces%20MILO%2C%20an%20Implicit%20spatIaL%20wOrld%20modeling%20paradigm%20that%20simulates%20human-like%20spatial%20imagination.%20MILO%20integrates%20a%20visual%20generator%20to%20provide%20geometry-aware%20feedback%2C%20thereby%20implicitly%20grounding%20the%20MLLM%27s%20symbolic%20reasoning%20in%20perceptual%20experience.%20Complementing%20this%20paradigm%2C%20we%20propose%20RePE%20%28Relative%20Positional%20Encoding%29%2C%20a%20novel%20encoding%20scheme%20that%20captures%20relative%20camera-pose%20transformations%2C%20offering%20superior%20performance%20over%20absolute%20coordinate%20systems.%20To%20support%20the%20training%2C%20we%20construct%20GeoGen%2C%20a%20large-scale%20Geometry-aware%20Generative%20dataset%20with%20approximately%202%2C241%20videos%20and%2067%2C827%20observation-action-outcome%20triplets.%20Experiments%20demonstrate%20that%20our%20approach%20significantly%20enhances%20spatial%20reasoning%20capabilities%20across%20multiple%20baselines%20and%20benchmarks%2C%20offering%20a%20more%20holistic%20understanding%20of%203D%20space.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01821v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520through%2520Imagination%253A%2520Learning%2520Scene%2520Geometry%2520via%2520Implicit%2520Spatial%2520World%2520Modeling%26entry.906535625%3DMeng%2520Cao%2520and%2520Haokun%2520Lin%2520and%2520Haoyuan%2520Li%2520and%2520Haoran%2520Tang%2520and%2520Rongtao%2520Xu%2520and%2520Dong%2520An%2520and%2520Xue%2520Liu%2520and%2520Ian%2520Reid%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3DSpatial%2520reasoning%252C%2520the%2520ability%2520to%2520understand%2520and%2520interpret%2520the%25203D%2520structure%2520of%2520the%2520world%252C%2520is%2520a%2520critical%2520yet%2520underdeveloped%2520capability%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%2520Current%2520methods%2520predominantly%2520rely%2520on%2520verbal%2520descriptive%2520tuning%252C%2520which%2520suffers%2520from%2520visual%2520illiteracy%252C%2520i.e.%252C%2520they%2520learn%2520spatial%2520concepts%2520through%2520textual%2520symbols%2520alone%252C%2520devoid%2520of%2520connection%2520to%2520their%2520visual%2520manifestations.%2520To%2520bridge%2520this%2520gap%252C%2520this%2520paper%2520introduces%2520MILO%252C%2520an%2520Implicit%2520spatIaL%2520wOrld%2520modeling%2520paradigm%2520that%2520simulates%2520human-like%2520spatial%2520imagination.%2520MILO%2520integrates%2520a%2520visual%2520generator%2520to%2520provide%2520geometry-aware%2520feedback%252C%2520thereby%2520implicitly%2520grounding%2520the%2520MLLM%2527s%2520symbolic%2520reasoning%2520in%2520perceptual%2520experience.%2520Complementing%2520this%2520paradigm%252C%2520we%2520propose%2520RePE%2520%2528Relative%2520Positional%2520Encoding%2529%252C%2520a%2520novel%2520encoding%2520scheme%2520that%2520captures%2520relative%2520camera-pose%2520transformations%252C%2520offering%2520superior%2520performance%2520over%2520absolute%2520coordinate%2520systems.%2520To%2520support%2520the%2520training%252C%2520we%2520construct%2520GeoGen%252C%2520a%2520large-scale%2520Geometry-aware%2520Generative%2520dataset%2520with%2520approximately%25202%252C241%2520videos%2520and%252067%252C827%2520observation-action-outcome%2520triplets.%2520Experiments%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520enhances%2520spatial%2520reasoning%2520capabilities%2520across%2520multiple%2520baselines%2520and%2520benchmarks%252C%2520offering%2520a%2520more%2520holistic%2520understanding%2520of%25203D%2520space.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01821v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20through%20Imagination%3A%20Learning%20Scene%20Geometry%20via%20Implicit%20Spatial%20World%20Modeling&entry.906535625=Meng%20Cao%20and%20Haokun%20Lin%20and%20Haoyuan%20Li%20and%20Haoran%20Tang%20and%20Rongtao%20Xu%20and%20Dong%20An%20and%20Xue%20Liu%20and%20Ian%20Reid%20and%20Xiaodan%20Liang&entry.1292438233=Spatial%20reasoning%2C%20the%20ability%20to%20understand%20and%20interpret%20the%203D%20structure%20of%20the%20world%2C%20is%20a%20critical%20yet%20underdeveloped%20capability%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20Current%20methods%20predominantly%20rely%20on%20verbal%20descriptive%20tuning%2C%20which%20suffers%20from%20visual%20illiteracy%2C%20i.e.%2C%20they%20learn%20spatial%20concepts%20through%20textual%20symbols%20alone%2C%20devoid%20of%20connection%20to%20their%20visual%20manifestations.%20To%20bridge%20this%20gap%2C%20this%20paper%20introduces%20MILO%2C%20an%20Implicit%20spatIaL%20wOrld%20modeling%20paradigm%20that%20simulates%20human-like%20spatial%20imagination.%20MILO%20integrates%20a%20visual%20generator%20to%20provide%20geometry-aware%20feedback%2C%20thereby%20implicitly%20grounding%20the%20MLLM%27s%20symbolic%20reasoning%20in%20perceptual%20experience.%20Complementing%20this%20paradigm%2C%20we%20propose%20RePE%20%28Relative%20Positional%20Encoding%29%2C%20a%20novel%20encoding%20scheme%20that%20captures%20relative%20camera-pose%20transformations%2C%20offering%20superior%20performance%20over%20absolute%20coordinate%20systems.%20To%20support%20the%20training%2C%20we%20construct%20GeoGen%2C%20a%20large-scale%20Geometry-aware%20Generative%20dataset%20with%20approximately%202%2C241%20videos%20and%2067%2C827%20observation-action-outcome%20triplets.%20Experiments%20demonstrate%20that%20our%20approach%20significantly%20enhances%20spatial%20reasoning%20capabilities%20across%20multiple%20baselines%20and%20benchmarks%2C%20offering%20a%20more%20holistic%20understanding%20of%203D%20space.&entry.1838667208=http%3A//arxiv.org/abs/2512.01821v2&entry.124074799=Read"},
{"title": "GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring", "author": "Maximilian Schall and Felix Leonard Kn\u00f6fel and Noah Elias K\u00f6nig and Jan Jonas Kubeler and Maximilian von Klinski and Joan Wilhelm Linnemann and Xiaoshi Liu and Iven Jelle Schlegelmilch and Ole Woyciniuk and Alexandra Schild and Dante Wasmuht and Magdalena Bermejo Espinet and German Illera Basas and Gerard de Melo", "abstract": "Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, \"in-the-wild\" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species", "link": "http://arxiv.org/abs/2512.07776v1", "date": "2025-12-08", "relevancy": 2.5068, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5241}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4922}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GorillaWatch%3A%20An%20Automated%20System%20for%20In-the-Wild%20Gorilla%20Re-Identification%20and%20Population%20Monitoring&body=Title%3A%20GorillaWatch%3A%20An%20Automated%20System%20for%20In-the-Wild%20Gorilla%20Re-Identification%20and%20Population%20Monitoring%0AAuthor%3A%20Maximilian%20Schall%20and%20Felix%20Leonard%20Kn%C3%B6fel%20and%20Noah%20Elias%20K%C3%B6nig%20and%20Jan%20Jonas%20Kubeler%20and%20Maximilian%20von%20Klinski%20and%20Joan%20Wilhelm%20Linnemann%20and%20Xiaoshi%20Liu%20and%20Iven%20Jelle%20Schlegelmilch%20and%20Ole%20Woyciniuk%20and%20Alexandra%20Schild%20and%20Dante%20Wasmuht%20and%20Magdalena%20Bermejo%20Espinet%20and%20German%20Illera%20Basas%20and%20Gerard%20de%20Melo%0AAbstract%3A%20Monitoring%20critically%20endangered%20western%20lowland%20gorillas%20is%20currently%20hampered%20by%20the%20immense%20manual%20effort%20required%20to%20re-identify%20individuals%20from%20vast%20archives%20of%20camera%20trap%20footage.%20The%20primary%20obstacle%20to%20automating%20this%20process%20has%20been%20the%20lack%20of%20large-scale%2C%20%22in-the-wild%22%20video%20datasets%20suitable%20for%20training%20robust%20deep%20learning%20models.%20To%20address%20this%20gap%2C%20we%20introduce%20a%20comprehensive%20benchmark%20with%20three%20novel%20datasets%3A%20Gorilla-SPAC-Wild%2C%20the%20largest%20video%20dataset%20for%20wild%20primate%20re-identification%20to%20date%3B%20Gorilla-Berlin-Zoo%2C%20for%20assessing%20cross-domain%20re-identification%20generalization%3B%20and%20Gorilla-SPAC-MoT%2C%20for%20evaluating%20multi-object%20tracking%20in%20camera%20trap%20footage.%20Building%20on%20these%20datasets%2C%20we%20present%20GorillaWatch%2C%20an%20end-to-end%20pipeline%20integrating%20detection%2C%20tracking%2C%20and%20re-identification.%20To%20exploit%20temporal%20information%2C%20we%20introduce%20a%20multi-frame%20self-supervised%20pretraining%20strategy%20that%20leverages%20consistency%20in%20tracklets%20to%20learn%20domain-specific%20features%20without%20manual%20labels.%20To%20ensure%20scientific%20validity%2C%20a%20differentiable%20adaptation%20of%20AttnLRP%20verifies%20that%20our%20model%20relies%20on%20discriminative%20biometric%20traits%20rather%20than%20background%20correlations.%20Extensive%20benchmarking%20subsequently%20demonstrates%20that%20aggregating%20features%20from%20large-scale%20image%20backbones%20outperforms%20specialized%20video%20architectures.%20Finally%2C%20we%20address%20unsupervised%20population%20counting%20by%20integrating%20spatiotemporal%20constraints%20into%20standard%20clustering%20to%20mitigate%20over-segmentation.%20We%20publicly%20release%20all%20code%20and%20datasets%20to%20facilitate%20scalable%2C%20non-invasive%20monitoring%20of%20endangered%20species%0ALink%3A%20http%3A//arxiv.org/abs/2512.07776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGorillaWatch%253A%2520An%2520Automated%2520System%2520for%2520In-the-Wild%2520Gorilla%2520Re-Identification%2520and%2520Population%2520Monitoring%26entry.906535625%3DMaximilian%2520Schall%2520and%2520Felix%2520Leonard%2520Kn%25C3%25B6fel%2520and%2520Noah%2520Elias%2520K%25C3%25B6nig%2520and%2520Jan%2520Jonas%2520Kubeler%2520and%2520Maximilian%2520von%2520Klinski%2520and%2520Joan%2520Wilhelm%2520Linnemann%2520and%2520Xiaoshi%2520Liu%2520and%2520Iven%2520Jelle%2520Schlegelmilch%2520and%2520Ole%2520Woyciniuk%2520and%2520Alexandra%2520Schild%2520and%2520Dante%2520Wasmuht%2520and%2520Magdalena%2520Bermejo%2520Espinet%2520and%2520German%2520Illera%2520Basas%2520and%2520Gerard%2520de%2520Melo%26entry.1292438233%3DMonitoring%2520critically%2520endangered%2520western%2520lowland%2520gorillas%2520is%2520currently%2520hampered%2520by%2520the%2520immense%2520manual%2520effort%2520required%2520to%2520re-identify%2520individuals%2520from%2520vast%2520archives%2520of%2520camera%2520trap%2520footage.%2520The%2520primary%2520obstacle%2520to%2520automating%2520this%2520process%2520has%2520been%2520the%2520lack%2520of%2520large-scale%252C%2520%2522in-the-wild%2522%2520video%2520datasets%2520suitable%2520for%2520training%2520robust%2520deep%2520learning%2520models.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520a%2520comprehensive%2520benchmark%2520with%2520three%2520novel%2520datasets%253A%2520Gorilla-SPAC-Wild%252C%2520the%2520largest%2520video%2520dataset%2520for%2520wild%2520primate%2520re-identification%2520to%2520date%253B%2520Gorilla-Berlin-Zoo%252C%2520for%2520assessing%2520cross-domain%2520re-identification%2520generalization%253B%2520and%2520Gorilla-SPAC-MoT%252C%2520for%2520evaluating%2520multi-object%2520tracking%2520in%2520camera%2520trap%2520footage.%2520Building%2520on%2520these%2520datasets%252C%2520we%2520present%2520GorillaWatch%252C%2520an%2520end-to-end%2520pipeline%2520integrating%2520detection%252C%2520tracking%252C%2520and%2520re-identification.%2520To%2520exploit%2520temporal%2520information%252C%2520we%2520introduce%2520a%2520multi-frame%2520self-supervised%2520pretraining%2520strategy%2520that%2520leverages%2520consistency%2520in%2520tracklets%2520to%2520learn%2520domain-specific%2520features%2520without%2520manual%2520labels.%2520To%2520ensure%2520scientific%2520validity%252C%2520a%2520differentiable%2520adaptation%2520of%2520AttnLRP%2520verifies%2520that%2520our%2520model%2520relies%2520on%2520discriminative%2520biometric%2520traits%2520rather%2520than%2520background%2520correlations.%2520Extensive%2520benchmarking%2520subsequently%2520demonstrates%2520that%2520aggregating%2520features%2520from%2520large-scale%2520image%2520backbones%2520outperforms%2520specialized%2520video%2520architectures.%2520Finally%252C%2520we%2520address%2520unsupervised%2520population%2520counting%2520by%2520integrating%2520spatiotemporal%2520constraints%2520into%2520standard%2520clustering%2520to%2520mitigate%2520over-segmentation.%2520We%2520publicly%2520release%2520all%2520code%2520and%2520datasets%2520to%2520facilitate%2520scalable%252C%2520non-invasive%2520monitoring%2520of%2520endangered%2520species%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GorillaWatch%3A%20An%20Automated%20System%20for%20In-the-Wild%20Gorilla%20Re-Identification%20and%20Population%20Monitoring&entry.906535625=Maximilian%20Schall%20and%20Felix%20Leonard%20Kn%C3%B6fel%20and%20Noah%20Elias%20K%C3%B6nig%20and%20Jan%20Jonas%20Kubeler%20and%20Maximilian%20von%20Klinski%20and%20Joan%20Wilhelm%20Linnemann%20and%20Xiaoshi%20Liu%20and%20Iven%20Jelle%20Schlegelmilch%20and%20Ole%20Woyciniuk%20and%20Alexandra%20Schild%20and%20Dante%20Wasmuht%20and%20Magdalena%20Bermejo%20Espinet%20and%20German%20Illera%20Basas%20and%20Gerard%20de%20Melo&entry.1292438233=Monitoring%20critically%20endangered%20western%20lowland%20gorillas%20is%20currently%20hampered%20by%20the%20immense%20manual%20effort%20required%20to%20re-identify%20individuals%20from%20vast%20archives%20of%20camera%20trap%20footage.%20The%20primary%20obstacle%20to%20automating%20this%20process%20has%20been%20the%20lack%20of%20large-scale%2C%20%22in-the-wild%22%20video%20datasets%20suitable%20for%20training%20robust%20deep%20learning%20models.%20To%20address%20this%20gap%2C%20we%20introduce%20a%20comprehensive%20benchmark%20with%20three%20novel%20datasets%3A%20Gorilla-SPAC-Wild%2C%20the%20largest%20video%20dataset%20for%20wild%20primate%20re-identification%20to%20date%3B%20Gorilla-Berlin-Zoo%2C%20for%20assessing%20cross-domain%20re-identification%20generalization%3B%20and%20Gorilla-SPAC-MoT%2C%20for%20evaluating%20multi-object%20tracking%20in%20camera%20trap%20footage.%20Building%20on%20these%20datasets%2C%20we%20present%20GorillaWatch%2C%20an%20end-to-end%20pipeline%20integrating%20detection%2C%20tracking%2C%20and%20re-identification.%20To%20exploit%20temporal%20information%2C%20we%20introduce%20a%20multi-frame%20self-supervised%20pretraining%20strategy%20that%20leverages%20consistency%20in%20tracklets%20to%20learn%20domain-specific%20features%20without%20manual%20labels.%20To%20ensure%20scientific%20validity%2C%20a%20differentiable%20adaptation%20of%20AttnLRP%20verifies%20that%20our%20model%20relies%20on%20discriminative%20biometric%20traits%20rather%20than%20background%20correlations.%20Extensive%20benchmarking%20subsequently%20demonstrates%20that%20aggregating%20features%20from%20large-scale%20image%20backbones%20outperforms%20specialized%20video%20architectures.%20Finally%2C%20we%20address%20unsupervised%20population%20counting%20by%20integrating%20spatiotemporal%20constraints%20into%20standard%20clustering%20to%20mitigate%20over-segmentation.%20We%20publicly%20release%20all%20code%20and%20datasets%20to%20facilitate%20scalable%2C%20non-invasive%20monitoring%20of%20endangered%20species&entry.1838667208=http%3A//arxiv.org/abs/2512.07776v1&entry.124074799=Read"},
{"title": "HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs", "author": "Sujoy Nath and Arkaprabha Basu and Sharanya Dasgupta and Swagatam Das", "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \\textsc{\\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.", "link": "http://arxiv.org/abs/2512.07687v1", "date": "2025-12-08", "relevancy": 2.504, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4983}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HalluShift%2B%2B%3A%20Bridging%20Language%20and%20Vision%20through%20Internal%20Representation%20Shifts%20for%20Hierarchical%20Hallucinations%20in%20MLLMs&body=Title%3A%20HalluShift%2B%2B%3A%20Bridging%20Language%20and%20Vision%20through%20Internal%20Representation%20Shifts%20for%20Hierarchical%20Hallucinations%20in%20MLLMs%0AAuthor%3A%20Sujoy%20Nath%20and%20Arkaprabha%20Basu%20and%20Sharanya%20Dasgupta%20and%20Swagatam%20Das%0AAbstract%3A%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%20vision-language%20understanding%20tasks.%20While%20these%20models%20often%20produce%20linguistically%20coherent%20output%2C%20they%20often%20suffer%20from%20hallucinations%2C%20generating%20descriptions%20that%20are%20factually%20inconsistent%20with%20the%20visual%20content%2C%20potentially%20leading%20to%20adverse%20consequences.%20Therefore%2C%20the%20assessment%20of%20hallucinations%20in%20MLLM%20has%20become%20increasingly%20crucial%20in%20the%20model%20development%20process.%20Contemporary%20methodologies%20predominantly%20depend%20on%20external%20LLM%20evaluators%2C%20which%20are%20themselves%20susceptible%20to%20hallucinations%20and%20may%20present%20challenges%20in%20terms%20of%20domain%20adaptation.%20In%20this%20study%2C%20we%20propose%20the%20hypothesis%20that%20hallucination%20manifests%20as%20measurable%20irregularities%20within%20the%20internal%20layer%20dynamics%20of%20MLLMs%2C%20not%20merely%20due%20to%20distributional%20shifts%20but%20also%20in%20the%20context%20of%20layer-wise%20analysis%20of%20specific%20assumptions.%20By%20incorporating%20such%20modifications%2C%20%5Ctextsc%7B%5Ctextsc%7BHalluShift%2B%2B%7D%7D%20broadens%20the%20efficacy%20of%20hallucination%20detection%20from%20text-based%20large%20language%20models%20%28LLMs%29%20to%20encompass%20multimodal%20scenarios.%20Our%20codebase%20is%20available%20at%20https%3A//github.com/C0mRD/HalluShift_Plus.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHalluShift%252B%252B%253A%2520Bridging%2520Language%2520and%2520Vision%2520through%2520Internal%2520Representation%2520Shifts%2520for%2520Hierarchical%2520Hallucinations%2520in%2520MLLMs%26entry.906535625%3DSujoy%2520Nath%2520and%2520Arkaprabha%2520Basu%2520and%2520Sharanya%2520Dasgupta%2520and%2520Swagatam%2520Das%26entry.1292438233%3DMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%2520vision-language%2520understanding%2520tasks.%2520While%2520these%2520models%2520often%2520produce%2520linguistically%2520coherent%2520output%252C%2520they%2520often%2520suffer%2520from%2520hallucinations%252C%2520generating%2520descriptions%2520that%2520are%2520factually%2520inconsistent%2520with%2520the%2520visual%2520content%252C%2520potentially%2520leading%2520to%2520adverse%2520consequences.%2520Therefore%252C%2520the%2520assessment%2520of%2520hallucinations%2520in%2520MLLM%2520has%2520become%2520increasingly%2520crucial%2520in%2520the%2520model%2520development%2520process.%2520Contemporary%2520methodologies%2520predominantly%2520depend%2520on%2520external%2520LLM%2520evaluators%252C%2520which%2520are%2520themselves%2520susceptible%2520to%2520hallucinations%2520and%2520may%2520present%2520challenges%2520in%2520terms%2520of%2520domain%2520adaptation.%2520In%2520this%2520study%252C%2520we%2520propose%2520the%2520hypothesis%2520that%2520hallucination%2520manifests%2520as%2520measurable%2520irregularities%2520within%2520the%2520internal%2520layer%2520dynamics%2520of%2520MLLMs%252C%2520not%2520merely%2520due%2520to%2520distributional%2520shifts%2520but%2520also%2520in%2520the%2520context%2520of%2520layer-wise%2520analysis%2520of%2520specific%2520assumptions.%2520By%2520incorporating%2520such%2520modifications%252C%2520%255Ctextsc%257B%255Ctextsc%257BHalluShift%252B%252B%257D%257D%2520broadens%2520the%2520efficacy%2520of%2520hallucination%2520detection%2520from%2520text-based%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520encompass%2520multimodal%2520scenarios.%2520Our%2520codebase%2520is%2520available%2520at%2520https%253A//github.com/C0mRD/HalluShift_Plus.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HalluShift%2B%2B%3A%20Bridging%20Language%20and%20Vision%20through%20Internal%20Representation%20Shifts%20for%20Hierarchical%20Hallucinations%20in%20MLLMs&entry.906535625=Sujoy%20Nath%20and%20Arkaprabha%20Basu%20and%20Sharanya%20Dasgupta%20and%20Swagatam%20Das&entry.1292438233=Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%20vision-language%20understanding%20tasks.%20While%20these%20models%20often%20produce%20linguistically%20coherent%20output%2C%20they%20often%20suffer%20from%20hallucinations%2C%20generating%20descriptions%20that%20are%20factually%20inconsistent%20with%20the%20visual%20content%2C%20potentially%20leading%20to%20adverse%20consequences.%20Therefore%2C%20the%20assessment%20of%20hallucinations%20in%20MLLM%20has%20become%20increasingly%20crucial%20in%20the%20model%20development%20process.%20Contemporary%20methodologies%20predominantly%20depend%20on%20external%20LLM%20evaluators%2C%20which%20are%20themselves%20susceptible%20to%20hallucinations%20and%20may%20present%20challenges%20in%20terms%20of%20domain%20adaptation.%20In%20this%20study%2C%20we%20propose%20the%20hypothesis%20that%20hallucination%20manifests%20as%20measurable%20irregularities%20within%20the%20internal%20layer%20dynamics%20of%20MLLMs%2C%20not%20merely%20due%20to%20distributional%20shifts%20but%20also%20in%20the%20context%20of%20layer-wise%20analysis%20of%20specific%20assumptions.%20By%20incorporating%20such%20modifications%2C%20%5Ctextsc%7B%5Ctextsc%7BHalluShift%2B%2B%7D%7D%20broadens%20the%20efficacy%20of%20hallucination%20detection%20from%20text-based%20large%20language%20models%20%28LLMs%29%20to%20encompass%20multimodal%20scenarios.%20Our%20codebase%20is%20available%20at%20https%3A//github.com/C0mRD/HalluShift_Plus.&entry.1838667208=http%3A//arxiv.org/abs/2512.07687v1&entry.124074799=Read"},
{"title": "Learning to Align Human Code Preferences", "author": "Xin Yin and Chao Ni and Xiaohu Yang", "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in automating software development tasks. While recent advances leverage Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to align models with human preferences, the optimal training strategy remains unclear across diverse code preference scenarios. This paper systematically investigates the roles of SFT and DPO in aligning LLMs with different code preferences. Through both theoretical analysis and empirical observation, we hypothesize that SFT excels in scenarios with objectively verifiable optimal solutions, while applying SFT followed by DPO (S&D) enables models to explore superior solutions in scenarios without objectively verifiable optimal solutions. Based on the analysis and experimental evidence, we propose Adaptive Preference Optimization (APO), a dynamic integration approach that adaptively amplifies preferred responses, suppresses dispreferred ones, and encourages exploration of potentially superior solutions during training. Extensive experiments across six representative code preference tasks validate our theoretical hypotheses and demonstrate that APO consistently matches or surpasses the performance of existing SFT and S&D strategies. Our work provides both theoretical foundations and practical guidance for selecting appropriate training strategies in different code preference alignment scenarios.", "link": "http://arxiv.org/abs/2507.20109v2", "date": "2025-12-08", "relevancy": 2.4994, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4998}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Align%20Human%20Code%20Preferences&body=Title%3A%20Learning%20to%20Align%20Human%20Code%20Preferences%0AAuthor%3A%20Xin%20Yin%20and%20Chao%20Ni%20and%20Xiaohu%20Yang%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20potential%20in%20automating%20software%20development%20tasks.%20While%20recent%20advances%20leverage%20Supervised%20Fine-Tuning%20%28SFT%29%20and%20Direct%20Preference%20Optimization%20%28DPO%29%20to%20align%20models%20with%20human%20preferences%2C%20the%20optimal%20training%20strategy%20remains%20unclear%20across%20diverse%20code%20preference%20scenarios.%20This%20paper%20systematically%20investigates%20the%20roles%20of%20SFT%20and%20DPO%20in%20aligning%20LLMs%20with%20different%20code%20preferences.%20Through%20both%20theoretical%20analysis%20and%20empirical%20observation%2C%20we%20hypothesize%20that%20SFT%20excels%20in%20scenarios%20with%20objectively%20verifiable%20optimal%20solutions%2C%20while%20applying%20SFT%20followed%20by%20DPO%20%28S%26D%29%20enables%20models%20to%20explore%20superior%20solutions%20in%20scenarios%20without%20objectively%20verifiable%20optimal%20solutions.%20Based%20on%20the%20analysis%20and%20experimental%20evidence%2C%20we%20propose%20Adaptive%20Preference%20Optimization%20%28APO%29%2C%20a%20dynamic%20integration%20approach%20that%20adaptively%20amplifies%20preferred%20responses%2C%20suppresses%20dispreferred%20ones%2C%20and%20encourages%20exploration%20of%20potentially%20superior%20solutions%20during%20training.%20Extensive%20experiments%20across%20six%20representative%20code%20preference%20tasks%20validate%20our%20theoretical%20hypotheses%20and%20demonstrate%20that%20APO%20consistently%20matches%20or%20surpasses%20the%20performance%20of%20existing%20SFT%20and%20S%26D%20strategies.%20Our%20work%20provides%20both%20theoretical%20foundations%20and%20practical%20guidance%20for%20selecting%20appropriate%20training%20strategies%20in%20different%20code%20preference%20alignment%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2507.20109v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Align%2520Human%2520Code%2520Preferences%26entry.906535625%3DXin%2520Yin%2520and%2520Chao%2520Ni%2520and%2520Xiaohu%2520Yang%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520potential%2520in%2520automating%2520software%2520development%2520tasks.%2520While%2520recent%2520advances%2520leverage%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520and%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520to%2520align%2520models%2520with%2520human%2520preferences%252C%2520the%2520optimal%2520training%2520strategy%2520remains%2520unclear%2520across%2520diverse%2520code%2520preference%2520scenarios.%2520This%2520paper%2520systematically%2520investigates%2520the%2520roles%2520of%2520SFT%2520and%2520DPO%2520in%2520aligning%2520LLMs%2520with%2520different%2520code%2520preferences.%2520Through%2520both%2520theoretical%2520analysis%2520and%2520empirical%2520observation%252C%2520we%2520hypothesize%2520that%2520SFT%2520excels%2520in%2520scenarios%2520with%2520objectively%2520verifiable%2520optimal%2520solutions%252C%2520while%2520applying%2520SFT%2520followed%2520by%2520DPO%2520%2528S%2526D%2529%2520enables%2520models%2520to%2520explore%2520superior%2520solutions%2520in%2520scenarios%2520without%2520objectively%2520verifiable%2520optimal%2520solutions.%2520Based%2520on%2520the%2520analysis%2520and%2520experimental%2520evidence%252C%2520we%2520propose%2520Adaptive%2520Preference%2520Optimization%2520%2528APO%2529%252C%2520a%2520dynamic%2520integration%2520approach%2520that%2520adaptively%2520amplifies%2520preferred%2520responses%252C%2520suppresses%2520dispreferred%2520ones%252C%2520and%2520encourages%2520exploration%2520of%2520potentially%2520superior%2520solutions%2520during%2520training.%2520Extensive%2520experiments%2520across%2520six%2520representative%2520code%2520preference%2520tasks%2520validate%2520our%2520theoretical%2520hypotheses%2520and%2520demonstrate%2520that%2520APO%2520consistently%2520matches%2520or%2520surpasses%2520the%2520performance%2520of%2520existing%2520SFT%2520and%2520S%2526D%2520strategies.%2520Our%2520work%2520provides%2520both%2520theoretical%2520foundations%2520and%2520practical%2520guidance%2520for%2520selecting%2520appropriate%2520training%2520strategies%2520in%2520different%2520code%2520preference%2520alignment%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20109v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Align%20Human%20Code%20Preferences&entry.906535625=Xin%20Yin%20and%20Chao%20Ni%20and%20Xiaohu%20Yang&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20potential%20in%20automating%20software%20development%20tasks.%20While%20recent%20advances%20leverage%20Supervised%20Fine-Tuning%20%28SFT%29%20and%20Direct%20Preference%20Optimization%20%28DPO%29%20to%20align%20models%20with%20human%20preferences%2C%20the%20optimal%20training%20strategy%20remains%20unclear%20across%20diverse%20code%20preference%20scenarios.%20This%20paper%20systematically%20investigates%20the%20roles%20of%20SFT%20and%20DPO%20in%20aligning%20LLMs%20with%20different%20code%20preferences.%20Through%20both%20theoretical%20analysis%20and%20empirical%20observation%2C%20we%20hypothesize%20that%20SFT%20excels%20in%20scenarios%20with%20objectively%20verifiable%20optimal%20solutions%2C%20while%20applying%20SFT%20followed%20by%20DPO%20%28S%26D%29%20enables%20models%20to%20explore%20superior%20solutions%20in%20scenarios%20without%20objectively%20verifiable%20optimal%20solutions.%20Based%20on%20the%20analysis%20and%20experimental%20evidence%2C%20we%20propose%20Adaptive%20Preference%20Optimization%20%28APO%29%2C%20a%20dynamic%20integration%20approach%20that%20adaptively%20amplifies%20preferred%20responses%2C%20suppresses%20dispreferred%20ones%2C%20and%20encourages%20exploration%20of%20potentially%20superior%20solutions%20during%20training.%20Extensive%20experiments%20across%20six%20representative%20code%20preference%20tasks%20validate%20our%20theoretical%20hypotheses%20and%20demonstrate%20that%20APO%20consistently%20matches%20or%20surpasses%20the%20performance%20of%20existing%20SFT%20and%20S%26D%20strategies.%20Our%20work%20provides%20both%20theoretical%20foundations%20and%20practical%20guidance%20for%20selecting%20appropriate%20training%20strategies%20in%20different%20code%20preference%20alignment%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2507.20109v2&entry.124074799=Read"},
{"title": "Online Segment Any 3D Thing as Instance Tracking", "author": "Hanshi Wang and Zijian Cai and Jin Gao and Yiwei Zhang and Weiming Hu and Ke Wang and Zhipeng Zhang", "abstract": "Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.", "link": "http://arxiv.org/abs/2512.07599v1", "date": "2025-12-08", "relevancy": 2.4851, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6251}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6251}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Segment%20Any%203D%20Thing%20as%20Instance%20Tracking&body=Title%3A%20Online%20Segment%20Any%203D%20Thing%20as%20Instance%20Tracking%0AAuthor%3A%20Hanshi%20Wang%20and%20Zijian%20Cai%20and%20Jin%20Gao%20and%20Yiwei%20Zhang%20and%20Weiming%20Hu%20and%20Ke%20Wang%20and%20Zhipeng%20Zhang%0AAbstract%3A%20Online%2C%20real-time%2C%20and%20fine-grained%203D%20segmentation%20constitutes%20a%20fundamental%20capability%20for%20embodied%20intelligent%20agents%20to%20perceive%20and%20comprehend%20their%20operational%20environments.%20Recent%20advancements%20employ%20predefined%20object%20queries%20to%20aggregate%20semantic%20information%20from%20Vision%20Foundation%20Models%20%28VFMs%29%20outputs%20that%20are%20lifted%20into%203D%20point%20clouds%2C%20facilitating%20spatial%20information%20propagation%20through%20inter-query%20interactions.%20Nevertheless%2C%20perception%20is%20an%20inherently%20dynamic%20process%2C%20rendering%20temporal%20understanding%20a%20critical%20yet%20overlooked%20dimension%20within%20these%20prevailing%20query-based%20pipelines.%20Therefore%2C%20to%20further%20unlock%20the%20temporal%20environmental%20perception%20capabilities%20of%20embodied%20agents%2C%20our%20work%20reconceptualizes%20online%203D%20segmentation%20as%20an%20instance%20tracking%20problem%20%28AutoSeg3D%29.%20Our%20core%20strategy%20involves%20utilizing%20object%20queries%20for%20temporal%20information%20propagation%2C%20where%20long-term%20instance%20association%20promotes%20the%20coherence%20of%20features%20and%20object%20identities%2C%20while%20short-term%20instance%20update%20enriches%20instant%20observations.%20Given%20that%20viewpoint%20variations%20in%20embodied%20robotics%20often%20lead%20to%20partial%20object%20visibility%20across%20frames%2C%20this%20mechanism%20aids%20the%20model%20in%20developing%20a%20holistic%20object%20understanding%20beyond%20incomplete%20instantaneous%20views.%20Furthermore%2C%20we%20introduce%20spatial%20consistency%20learning%20to%20mitigate%20the%20fragmentation%20problem%20inherent%20in%20VFMs%2C%20yielding%20more%20comprehensive%20instance%20information%20for%20enhancing%20the%20efficacy%20of%20both%20long-term%20and%20short-term%20temporal%20learning.%20The%20temporal%20information%20exchange%20and%20consistency%20learning%20facilitated%20by%20these%20sparse%20object%20queries%20not%20only%20enhance%20spatial%20comprehension%20but%20also%20circumvent%20the%20computational%20burden%20associated%20with%20dense%20temporal%20point%20cloud%20interactions.%20Our%20method%20establishes%20a%20new%20state-of-the-art%2C%20surpassing%20ESAM%20by%202.8%20AP%20on%20ScanNet200%20and%20delivering%20consistent%20gains%20on%20ScanNet%2C%20SceneNN%2C%20and%203RScan%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Segment%2520Any%25203D%2520Thing%2520as%2520Instance%2520Tracking%26entry.906535625%3DHanshi%2520Wang%2520and%2520Zijian%2520Cai%2520and%2520Jin%2520Gao%2520and%2520Yiwei%2520Zhang%2520and%2520Weiming%2520Hu%2520and%2520Ke%2520Wang%2520and%2520Zhipeng%2520Zhang%26entry.1292438233%3DOnline%252C%2520real-time%252C%2520and%2520fine-grained%25203D%2520segmentation%2520constitutes%2520a%2520fundamental%2520capability%2520for%2520embodied%2520intelligent%2520agents%2520to%2520perceive%2520and%2520comprehend%2520their%2520operational%2520environments.%2520Recent%2520advancements%2520employ%2520predefined%2520object%2520queries%2520to%2520aggregate%2520semantic%2520information%2520from%2520Vision%2520Foundation%2520Models%2520%2528VFMs%2529%2520outputs%2520that%2520are%2520lifted%2520into%25203D%2520point%2520clouds%252C%2520facilitating%2520spatial%2520information%2520propagation%2520through%2520inter-query%2520interactions.%2520Nevertheless%252C%2520perception%2520is%2520an%2520inherently%2520dynamic%2520process%252C%2520rendering%2520temporal%2520understanding%2520a%2520critical%2520yet%2520overlooked%2520dimension%2520within%2520these%2520prevailing%2520query-based%2520pipelines.%2520Therefore%252C%2520to%2520further%2520unlock%2520the%2520temporal%2520environmental%2520perception%2520capabilities%2520of%2520embodied%2520agents%252C%2520our%2520work%2520reconceptualizes%2520online%25203D%2520segmentation%2520as%2520an%2520instance%2520tracking%2520problem%2520%2528AutoSeg3D%2529.%2520Our%2520core%2520strategy%2520involves%2520utilizing%2520object%2520queries%2520for%2520temporal%2520information%2520propagation%252C%2520where%2520long-term%2520instance%2520association%2520promotes%2520the%2520coherence%2520of%2520features%2520and%2520object%2520identities%252C%2520while%2520short-term%2520instance%2520update%2520enriches%2520instant%2520observations.%2520Given%2520that%2520viewpoint%2520variations%2520in%2520embodied%2520robotics%2520often%2520lead%2520to%2520partial%2520object%2520visibility%2520across%2520frames%252C%2520this%2520mechanism%2520aids%2520the%2520model%2520in%2520developing%2520a%2520holistic%2520object%2520understanding%2520beyond%2520incomplete%2520instantaneous%2520views.%2520Furthermore%252C%2520we%2520introduce%2520spatial%2520consistency%2520learning%2520to%2520mitigate%2520the%2520fragmentation%2520problem%2520inherent%2520in%2520VFMs%252C%2520yielding%2520more%2520comprehensive%2520instance%2520information%2520for%2520enhancing%2520the%2520efficacy%2520of%2520both%2520long-term%2520and%2520short-term%2520temporal%2520learning.%2520The%2520temporal%2520information%2520exchange%2520and%2520consistency%2520learning%2520facilitated%2520by%2520these%2520sparse%2520object%2520queries%2520not%2520only%2520enhance%2520spatial%2520comprehension%2520but%2520also%2520circumvent%2520the%2520computational%2520burden%2520associated%2520with%2520dense%2520temporal%2520point%2520cloud%2520interactions.%2520Our%2520method%2520establishes%2520a%2520new%2520state-of-the-art%252C%2520surpassing%2520ESAM%2520by%25202.8%2520AP%2520on%2520ScanNet200%2520and%2520delivering%2520consistent%2520gains%2520on%2520ScanNet%252C%2520SceneNN%252C%2520and%25203RScan%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Segment%20Any%203D%20Thing%20as%20Instance%20Tracking&entry.906535625=Hanshi%20Wang%20and%20Zijian%20Cai%20and%20Jin%20Gao%20and%20Yiwei%20Zhang%20and%20Weiming%20Hu%20and%20Ke%20Wang%20and%20Zhipeng%20Zhang&entry.1292438233=Online%2C%20real-time%2C%20and%20fine-grained%203D%20segmentation%20constitutes%20a%20fundamental%20capability%20for%20embodied%20intelligent%20agents%20to%20perceive%20and%20comprehend%20their%20operational%20environments.%20Recent%20advancements%20employ%20predefined%20object%20queries%20to%20aggregate%20semantic%20information%20from%20Vision%20Foundation%20Models%20%28VFMs%29%20outputs%20that%20are%20lifted%20into%203D%20point%20clouds%2C%20facilitating%20spatial%20information%20propagation%20through%20inter-query%20interactions.%20Nevertheless%2C%20perception%20is%20an%20inherently%20dynamic%20process%2C%20rendering%20temporal%20understanding%20a%20critical%20yet%20overlooked%20dimension%20within%20these%20prevailing%20query-based%20pipelines.%20Therefore%2C%20to%20further%20unlock%20the%20temporal%20environmental%20perception%20capabilities%20of%20embodied%20agents%2C%20our%20work%20reconceptualizes%20online%203D%20segmentation%20as%20an%20instance%20tracking%20problem%20%28AutoSeg3D%29.%20Our%20core%20strategy%20involves%20utilizing%20object%20queries%20for%20temporal%20information%20propagation%2C%20where%20long-term%20instance%20association%20promotes%20the%20coherence%20of%20features%20and%20object%20identities%2C%20while%20short-term%20instance%20update%20enriches%20instant%20observations.%20Given%20that%20viewpoint%20variations%20in%20embodied%20robotics%20often%20lead%20to%20partial%20object%20visibility%20across%20frames%2C%20this%20mechanism%20aids%20the%20model%20in%20developing%20a%20holistic%20object%20understanding%20beyond%20incomplete%20instantaneous%20views.%20Furthermore%2C%20we%20introduce%20spatial%20consistency%20learning%20to%20mitigate%20the%20fragmentation%20problem%20inherent%20in%20VFMs%2C%20yielding%20more%20comprehensive%20instance%20information%20for%20enhancing%20the%20efficacy%20of%20both%20long-term%20and%20short-term%20temporal%20learning.%20The%20temporal%20information%20exchange%20and%20consistency%20learning%20facilitated%20by%20these%20sparse%20object%20queries%20not%20only%20enhance%20spatial%20comprehension%20but%20also%20circumvent%20the%20computational%20burden%20associated%20with%20dense%20temporal%20point%20cloud%20interactions.%20Our%20method%20establishes%20a%20new%20state-of-the-art%2C%20surpassing%20ESAM%20by%202.8%20AP%20on%20ScanNet200%20and%20delivering%20consistent%20gains%20on%20ScanNet%2C%20SceneNN%2C%20and%203RScan%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2512.07599v1&entry.124074799=Read"},
{"title": "HLTCOE Evaluation Team at TREC 2025: VQA Track", "author": "Dengjia Zhang and Charles Weng and Katherine Guerrerio and Yi Lu and Kenton Murray and Alexander Martin and Reno Kriz and Benjamin Van Durme", "abstract": "The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.", "link": "http://arxiv.org/abs/2512.07738v1", "date": "2025-12-08", "relevancy": 2.4645, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4932}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4932}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HLTCOE%20Evaluation%20Team%20at%20TREC%202025%3A%20VQA%20Track&body=Title%3A%20HLTCOE%20Evaluation%20Team%20at%20TREC%202025%3A%20VQA%20Track%0AAuthor%3A%20Dengjia%20Zhang%20and%20Charles%20Weng%20and%20Katherine%20Guerrerio%20and%20Yi%20Lu%20and%20Kenton%20Murray%20and%20Alexander%20Martin%20and%20Reno%20Kriz%20and%20Benjamin%20Van%20Durme%0AAbstract%3A%20The%20HLTCOE%20Evaluation%20team%20participated%20in%20TREC%20VQA%27s%20Answer%20Generation%20%28AG%29%20task%2C%20for%20which%20we%20developed%20a%20listwise%20learning%20framework%20that%20aims%20to%20improve%20semantic%20precision%20and%20ranking%20consistency%20in%20answer%20generation.%20Given%20a%20video-question%20pair%2C%20a%20base%20multimodal%20model%20first%20generates%20multiple%20candidate%20answers%2C%20which%20are%20then%20reranked%20using%20a%20model%20trained%20with%20a%20novel%20Masked%20Pointer%20Cross-Entropy%20Loss%20with%20Rank%20Weights.%20This%20objective%20integrates%20pointer-based%20candidate%20selection%2C%20rank-dependent%20weighting%2C%20and%20masked%20cross-entropy%20under%20vocabulary%20restriction%2C%20enabling%20stable%20and%20interpretable%20listwise%20optimization.%20By%20bridging%20generative%20modeling%20with%20discriminative%20ranking%2C%20our%20method%20produces%20coherent%2C%20fine-grained%20answer%20lists.%20Experiments%20reveal%20consistent%20gains%20in%20accuracy%20and%20ranking%20stability%2C%20especially%20for%20questions%20requiring%20temporal%20reasoning%20and%20semantic%20disambiguation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHLTCOE%2520Evaluation%2520Team%2520at%2520TREC%25202025%253A%2520VQA%2520Track%26entry.906535625%3DDengjia%2520Zhang%2520and%2520Charles%2520Weng%2520and%2520Katherine%2520Guerrerio%2520and%2520Yi%2520Lu%2520and%2520Kenton%2520Murray%2520and%2520Alexander%2520Martin%2520and%2520Reno%2520Kriz%2520and%2520Benjamin%2520Van%2520Durme%26entry.1292438233%3DThe%2520HLTCOE%2520Evaluation%2520team%2520participated%2520in%2520TREC%2520VQA%2527s%2520Answer%2520Generation%2520%2528AG%2529%2520task%252C%2520for%2520which%2520we%2520developed%2520a%2520listwise%2520learning%2520framework%2520that%2520aims%2520to%2520improve%2520semantic%2520precision%2520and%2520ranking%2520consistency%2520in%2520answer%2520generation.%2520Given%2520a%2520video-question%2520pair%252C%2520a%2520base%2520multimodal%2520model%2520first%2520generates%2520multiple%2520candidate%2520answers%252C%2520which%2520are%2520then%2520reranked%2520using%2520a%2520model%2520trained%2520with%2520a%2520novel%2520Masked%2520Pointer%2520Cross-Entropy%2520Loss%2520with%2520Rank%2520Weights.%2520This%2520objective%2520integrates%2520pointer-based%2520candidate%2520selection%252C%2520rank-dependent%2520weighting%252C%2520and%2520masked%2520cross-entropy%2520under%2520vocabulary%2520restriction%252C%2520enabling%2520stable%2520and%2520interpretable%2520listwise%2520optimization.%2520By%2520bridging%2520generative%2520modeling%2520with%2520discriminative%2520ranking%252C%2520our%2520method%2520produces%2520coherent%252C%2520fine-grained%2520answer%2520lists.%2520Experiments%2520reveal%2520consistent%2520gains%2520in%2520accuracy%2520and%2520ranking%2520stability%252C%2520especially%2520for%2520questions%2520requiring%2520temporal%2520reasoning%2520and%2520semantic%2520disambiguation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HLTCOE%20Evaluation%20Team%20at%20TREC%202025%3A%20VQA%20Track&entry.906535625=Dengjia%20Zhang%20and%20Charles%20Weng%20and%20Katherine%20Guerrerio%20and%20Yi%20Lu%20and%20Kenton%20Murray%20and%20Alexander%20Martin%20and%20Reno%20Kriz%20and%20Benjamin%20Van%20Durme&entry.1292438233=The%20HLTCOE%20Evaluation%20team%20participated%20in%20TREC%20VQA%27s%20Answer%20Generation%20%28AG%29%20task%2C%20for%20which%20we%20developed%20a%20listwise%20learning%20framework%20that%20aims%20to%20improve%20semantic%20precision%20and%20ranking%20consistency%20in%20answer%20generation.%20Given%20a%20video-question%20pair%2C%20a%20base%20multimodal%20model%20first%20generates%20multiple%20candidate%20answers%2C%20which%20are%20then%20reranked%20using%20a%20model%20trained%20with%20a%20novel%20Masked%20Pointer%20Cross-Entropy%20Loss%20with%20Rank%20Weights.%20This%20objective%20integrates%20pointer-based%20candidate%20selection%2C%20rank-dependent%20weighting%2C%20and%20masked%20cross-entropy%20under%20vocabulary%20restriction%2C%20enabling%20stable%20and%20interpretable%20listwise%20optimization.%20By%20bridging%20generative%20modeling%20with%20discriminative%20ranking%2C%20our%20method%20produces%20coherent%2C%20fine-grained%20answer%20lists.%20Experiments%20reveal%20consistent%20gains%20in%20accuracy%20and%20ranking%20stability%2C%20especially%20for%20questions%20requiring%20temporal%20reasoning%20and%20semantic%20disambiguation.&entry.1838667208=http%3A//arxiv.org/abs/2512.07738v1&entry.124074799=Read"},
{"title": "PCMind-2.1-Kaiyuan-2B Technical Report", "author": "Kairong Luo and Zhenbo Sun and Xinyu Shi and Shengqi Chen and Bowen Yu and Yunyi Chen and Chenyi Dang and Hengtao Tao and Hui Wang and Fangming Liu and Kaifeng Lyu and Wenguang Chen", "abstract": "The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.", "link": "http://arxiv.org/abs/2512.07612v1", "date": "2025-12-08", "relevancy": 2.4624, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4967}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4904}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PCMind-2.1-Kaiyuan-2B%20Technical%20Report&body=Title%3A%20PCMind-2.1-Kaiyuan-2B%20Technical%20Report%0AAuthor%3A%20Kairong%20Luo%20and%20Zhenbo%20Sun%20and%20Xinyu%20Shi%20and%20Shengqi%20Chen%20and%20Bowen%20Yu%20and%20Yunyi%20Chen%20and%20Chenyi%20Dang%20and%20Hengtao%20Tao%20and%20Hui%20Wang%20and%20Fangming%20Liu%20and%20Kaifeng%20Lyu%20and%20Wenguang%20Chen%0AAbstract%3A%20The%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20resulted%20in%20a%20significant%20knowledge%20gap%20between%20the%20open-source%20community%20and%20industry%2C%20primarily%20because%20the%20latter%20relies%20on%20closed-source%2C%20high-quality%20data%20and%20training%20recipes.%20To%20address%20this%2C%20we%20introduce%20PCMind-2.1-Kaiyuan-2B%2C%20a%20fully%20open-source%202-billion-parameter%20model%20focused%20on%20improving%20training%20efficiency%20and%20effectiveness%20under%20resource%20constraints.%20Our%20methodology%20includes%20three%20key%20innovations%3A%20a%20Quantile%20Data%20Benchmarking%20method%20for%20systematically%20comparing%20heterogeneous%20open-source%20datasets%20and%20providing%20insights%20on%20data%20mixing%20strategies%3B%20a%20Strategic%20Selective%20Repetition%20scheme%20within%20a%20multi-phase%20paradigm%20to%20effectively%20leverage%20sparse%2C%20high-quality%20data%3B%20and%20a%20Multi-Domain%20Curriculum%20Training%20policy%20that%20orders%20samples%20by%20quality.%20Supported%20by%20a%20highly%20optimized%20data%20preprocessing%20pipeline%20and%20architectural%20modifications%20for%20FP16%20stability%2C%20Kaiyuan-2B%20achieves%20performance%20competitive%20with%20state-of-the-art%20fully%20open-source%20models%2C%20demonstrating%20practical%20and%20scalable%20solutions%20for%20resource-limited%20pretraining.%20We%20release%20all%20assets%20%28including%20model%20weights%2C%20data%2C%20and%20code%29%20under%20Apache%202.0%20license%20at%20https%3A//huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPCMind-2.1-Kaiyuan-2B%2520Technical%2520Report%26entry.906535625%3DKairong%2520Luo%2520and%2520Zhenbo%2520Sun%2520and%2520Xinyu%2520Shi%2520and%2520Shengqi%2520Chen%2520and%2520Bowen%2520Yu%2520and%2520Yunyi%2520Chen%2520and%2520Chenyi%2520Dang%2520and%2520Hengtao%2520Tao%2520and%2520Hui%2520Wang%2520and%2520Fangming%2520Liu%2520and%2520Kaifeng%2520Lyu%2520and%2520Wenguang%2520Chen%26entry.1292438233%3DThe%2520rapid%2520advancement%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520resulted%2520in%2520a%2520significant%2520knowledge%2520gap%2520between%2520the%2520open-source%2520community%2520and%2520industry%252C%2520primarily%2520because%2520the%2520latter%2520relies%2520on%2520closed-source%252C%2520high-quality%2520data%2520and%2520training%2520recipes.%2520To%2520address%2520this%252C%2520we%2520introduce%2520PCMind-2.1-Kaiyuan-2B%252C%2520a%2520fully%2520open-source%25202-billion-parameter%2520model%2520focused%2520on%2520improving%2520training%2520efficiency%2520and%2520effectiveness%2520under%2520resource%2520constraints.%2520Our%2520methodology%2520includes%2520three%2520key%2520innovations%253A%2520a%2520Quantile%2520Data%2520Benchmarking%2520method%2520for%2520systematically%2520comparing%2520heterogeneous%2520open-source%2520datasets%2520and%2520providing%2520insights%2520on%2520data%2520mixing%2520strategies%253B%2520a%2520Strategic%2520Selective%2520Repetition%2520scheme%2520within%2520a%2520multi-phase%2520paradigm%2520to%2520effectively%2520leverage%2520sparse%252C%2520high-quality%2520data%253B%2520and%2520a%2520Multi-Domain%2520Curriculum%2520Training%2520policy%2520that%2520orders%2520samples%2520by%2520quality.%2520Supported%2520by%2520a%2520highly%2520optimized%2520data%2520preprocessing%2520pipeline%2520and%2520architectural%2520modifications%2520for%2520FP16%2520stability%252C%2520Kaiyuan-2B%2520achieves%2520performance%2520competitive%2520with%2520state-of-the-art%2520fully%2520open-source%2520models%252C%2520demonstrating%2520practical%2520and%2520scalable%2520solutions%2520for%2520resource-limited%2520pretraining.%2520We%2520release%2520all%2520assets%2520%2528including%2520model%2520weights%252C%2520data%252C%2520and%2520code%2529%2520under%2520Apache%25202.0%2520license%2520at%2520https%253A//huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PCMind-2.1-Kaiyuan-2B%20Technical%20Report&entry.906535625=Kairong%20Luo%20and%20Zhenbo%20Sun%20and%20Xinyu%20Shi%20and%20Shengqi%20Chen%20and%20Bowen%20Yu%20and%20Yunyi%20Chen%20and%20Chenyi%20Dang%20and%20Hengtao%20Tao%20and%20Hui%20Wang%20and%20Fangming%20Liu%20and%20Kaifeng%20Lyu%20and%20Wenguang%20Chen&entry.1292438233=The%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20resulted%20in%20a%20significant%20knowledge%20gap%20between%20the%20open-source%20community%20and%20industry%2C%20primarily%20because%20the%20latter%20relies%20on%20closed-source%2C%20high-quality%20data%20and%20training%20recipes.%20To%20address%20this%2C%20we%20introduce%20PCMind-2.1-Kaiyuan-2B%2C%20a%20fully%20open-source%202-billion-parameter%20model%20focused%20on%20improving%20training%20efficiency%20and%20effectiveness%20under%20resource%20constraints.%20Our%20methodology%20includes%20three%20key%20innovations%3A%20a%20Quantile%20Data%20Benchmarking%20method%20for%20systematically%20comparing%20heterogeneous%20open-source%20datasets%20and%20providing%20insights%20on%20data%20mixing%20strategies%3B%20a%20Strategic%20Selective%20Repetition%20scheme%20within%20a%20multi-phase%20paradigm%20to%20effectively%20leverage%20sparse%2C%20high-quality%20data%3B%20and%20a%20Multi-Domain%20Curriculum%20Training%20policy%20that%20orders%20samples%20by%20quality.%20Supported%20by%20a%20highly%20optimized%20data%20preprocessing%20pipeline%20and%20architectural%20modifications%20for%20FP16%20stability%2C%20Kaiyuan-2B%20achieves%20performance%20competitive%20with%20state-of-the-art%20fully%20open-source%20models%2C%20demonstrating%20practical%20and%20scalable%20solutions%20for%20resource-limited%20pretraining.%20We%20release%20all%20assets%20%28including%20model%20weights%2C%20data%2C%20and%20code%29%20under%20Apache%202.0%20license%20at%20https%3A//huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.&entry.1838667208=http%3A//arxiv.org/abs/2512.07612v1&entry.124074799=Read"},
{"title": "Graph-Based Learning of Spectro-Topographical EEG Representations with Gradient Alignment for Brain-Computer Interfaces", "author": "Prithila Angkan and Amin Jalali and Paul Hungler and Ali Etemad", "abstract": "We present a novel graph-based learning of EEG representations with gradient alignment (GEEGA) that leverages multi-domain information to learn EEG representations for brain-computer interfaces. Our model leverages graph convolutional networks to fuse embeddings from frequency-based topographical maps and time-frequency spectrograms, capturing inter-domain relationships. GEEGA addresses the challenge of achieving high inter-class separability, which arises from the temporally dynamic and subject-sensitive nature of EEG signals by incorporating the center loss and pairwise difference loss. Additionally, GEEGA incorporates a gradient alignment strategy to resolve conflicts between gradients from different domains and the fused embeddings, ensuring that discrepancies, where gradients point in conflicting directions, are aligned toward a unified optimization direction. We validate the efficacy of our method through extensive experiments on three publicly available EEG datasets: BCI-2a, CL-Drive and CLARE. Comprehensive ablation studies further highlight the impact of various components of our model.", "link": "http://arxiv.org/abs/2512.07820v1", "date": "2025-12-08", "relevancy": 2.438, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4908}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4876}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-Based%20Learning%20of%20Spectro-Topographical%20EEG%20Representations%20with%20Gradient%20Alignment%20for%20Brain-Computer%20Interfaces&body=Title%3A%20Graph-Based%20Learning%20of%20Spectro-Topographical%20EEG%20Representations%20with%20Gradient%20Alignment%20for%20Brain-Computer%20Interfaces%0AAuthor%3A%20Prithila%20Angkan%20and%20Amin%20Jalali%20and%20Paul%20Hungler%20and%20Ali%20Etemad%0AAbstract%3A%20We%20present%20a%20novel%20graph-based%20learning%20of%20EEG%20representations%20with%20gradient%20alignment%20%28GEEGA%29%20that%20leverages%20multi-domain%20information%20to%20learn%20EEG%20representations%20for%20brain-computer%20interfaces.%20Our%20model%20leverages%20graph%20convolutional%20networks%20to%20fuse%20embeddings%20from%20frequency-based%20topographical%20maps%20and%20time-frequency%20spectrograms%2C%20capturing%20inter-domain%20relationships.%20GEEGA%20addresses%20the%20challenge%20of%20achieving%20high%20inter-class%20separability%2C%20which%20arises%20from%20the%20temporally%20dynamic%20and%20subject-sensitive%20nature%20of%20EEG%20signals%20by%20incorporating%20the%20center%20loss%20and%20pairwise%20difference%20loss.%20Additionally%2C%20GEEGA%20incorporates%20a%20gradient%20alignment%20strategy%20to%20resolve%20conflicts%20between%20gradients%20from%20different%20domains%20and%20the%20fused%20embeddings%2C%20ensuring%20that%20discrepancies%2C%20where%20gradients%20point%20in%20conflicting%20directions%2C%20are%20aligned%20toward%20a%20unified%20optimization%20direction.%20We%20validate%20the%20efficacy%20of%20our%20method%20through%20extensive%20experiments%20on%20three%20publicly%20available%20EEG%20datasets%3A%20BCI-2a%2C%20CL-Drive%20and%20CLARE.%20Comprehensive%20ablation%20studies%20further%20highlight%20the%20impact%20of%20various%20components%20of%20our%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-Based%2520Learning%2520of%2520Spectro-Topographical%2520EEG%2520Representations%2520with%2520Gradient%2520Alignment%2520for%2520Brain-Computer%2520Interfaces%26entry.906535625%3DPrithila%2520Angkan%2520and%2520Amin%2520Jalali%2520and%2520Paul%2520Hungler%2520and%2520Ali%2520Etemad%26entry.1292438233%3DWe%2520present%2520a%2520novel%2520graph-based%2520learning%2520of%2520EEG%2520representations%2520with%2520gradient%2520alignment%2520%2528GEEGA%2529%2520that%2520leverages%2520multi-domain%2520information%2520to%2520learn%2520EEG%2520representations%2520for%2520brain-computer%2520interfaces.%2520Our%2520model%2520leverages%2520graph%2520convolutional%2520networks%2520to%2520fuse%2520embeddings%2520from%2520frequency-based%2520topographical%2520maps%2520and%2520time-frequency%2520spectrograms%252C%2520capturing%2520inter-domain%2520relationships.%2520GEEGA%2520addresses%2520the%2520challenge%2520of%2520achieving%2520high%2520inter-class%2520separability%252C%2520which%2520arises%2520from%2520the%2520temporally%2520dynamic%2520and%2520subject-sensitive%2520nature%2520of%2520EEG%2520signals%2520by%2520incorporating%2520the%2520center%2520loss%2520and%2520pairwise%2520difference%2520loss.%2520Additionally%252C%2520GEEGA%2520incorporates%2520a%2520gradient%2520alignment%2520strategy%2520to%2520resolve%2520conflicts%2520between%2520gradients%2520from%2520different%2520domains%2520and%2520the%2520fused%2520embeddings%252C%2520ensuring%2520that%2520discrepancies%252C%2520where%2520gradients%2520point%2520in%2520conflicting%2520directions%252C%2520are%2520aligned%2520toward%2520a%2520unified%2520optimization%2520direction.%2520We%2520validate%2520the%2520efficacy%2520of%2520our%2520method%2520through%2520extensive%2520experiments%2520on%2520three%2520publicly%2520available%2520EEG%2520datasets%253A%2520BCI-2a%252C%2520CL-Drive%2520and%2520CLARE.%2520Comprehensive%2520ablation%2520studies%2520further%2520highlight%2520the%2520impact%2520of%2520various%2520components%2520of%2520our%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-Based%20Learning%20of%20Spectro-Topographical%20EEG%20Representations%20with%20Gradient%20Alignment%20for%20Brain-Computer%20Interfaces&entry.906535625=Prithila%20Angkan%20and%20Amin%20Jalali%20and%20Paul%20Hungler%20and%20Ali%20Etemad&entry.1292438233=We%20present%20a%20novel%20graph-based%20learning%20of%20EEG%20representations%20with%20gradient%20alignment%20%28GEEGA%29%20that%20leverages%20multi-domain%20information%20to%20learn%20EEG%20representations%20for%20brain-computer%20interfaces.%20Our%20model%20leverages%20graph%20convolutional%20networks%20to%20fuse%20embeddings%20from%20frequency-based%20topographical%20maps%20and%20time-frequency%20spectrograms%2C%20capturing%20inter-domain%20relationships.%20GEEGA%20addresses%20the%20challenge%20of%20achieving%20high%20inter-class%20separability%2C%20which%20arises%20from%20the%20temporally%20dynamic%20and%20subject-sensitive%20nature%20of%20EEG%20signals%20by%20incorporating%20the%20center%20loss%20and%20pairwise%20difference%20loss.%20Additionally%2C%20GEEGA%20incorporates%20a%20gradient%20alignment%20strategy%20to%20resolve%20conflicts%20between%20gradients%20from%20different%20domains%20and%20the%20fused%20embeddings%2C%20ensuring%20that%20discrepancies%2C%20where%20gradients%20point%20in%20conflicting%20directions%2C%20are%20aligned%20toward%20a%20unified%20optimization%20direction.%20We%20validate%20the%20efficacy%20of%20our%20method%20through%20extensive%20experiments%20on%20three%20publicly%20available%20EEG%20datasets%3A%20BCI-2a%2C%20CL-Drive%20and%20CLARE.%20Comprehensive%20ablation%20studies%20further%20highlight%20the%20impact%20of%20various%20components%20of%20our%20model.&entry.1838667208=http%3A//arxiv.org/abs/2512.07820v1&entry.124074799=Read"},
{"title": "TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B", "author": "Toshiki Nakai and Ravi Kiran Chikkala and Lena Sophie Oberkircher and Nicholas Jennings and Natalia Skachkova and Tatiana Anikina and Jesujoba Oluwadara Alabi", "abstract": "The 2025 Multimodal Models for Low-Resource Contexts and Social Impact (MMLoSo) Language Challenge addresses one of India's most pressing linguistic gaps: the lack of resources for its diverse low-resource languages (LRLs). In this study, we investigate whether enforcing cross-lingual similarity in specific internal layers of a decoder-only multilingual large language model (LLM) can improve translation quality from LRL to high-resource language (HRL). Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric that encourages representations of different languages to align, with REPINA, a regularization method that constrains parameter updates to remain close to the pretrained model, into a joint method we call TRepLiNa. In this research project, we experiment with zero-shot, few-shot, and fine-tuning settings using Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari, Santali, Bhili) with Hindi/English pivots. Our results show that aligning mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach to improving LRL translation, especially in data-scarce settings.", "link": "http://arxiv.org/abs/2510.06249v4", "date": "2025-12-08", "relevancy": 2.4287, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5168}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4754}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRepLiNa%3A%20Layer-wise%20CKA%2BREPINA%20Alignment%20Improves%20Low-Resource%20Machine%20Translation%20in%20Aya-23%208B&body=Title%3A%20TRepLiNa%3A%20Layer-wise%20CKA%2BREPINA%20Alignment%20Improves%20Low-Resource%20Machine%20Translation%20in%20Aya-23%208B%0AAuthor%3A%20Toshiki%20Nakai%20and%20Ravi%20Kiran%20Chikkala%20and%20Lena%20Sophie%20Oberkircher%20and%20Nicholas%20Jennings%20and%20Natalia%20Skachkova%20and%20Tatiana%20Anikina%20and%20Jesujoba%20Oluwadara%20Alabi%0AAbstract%3A%20The%202025%20Multimodal%20Models%20for%20Low-Resource%20Contexts%20and%20Social%20Impact%20%28MMLoSo%29%20Language%20Challenge%20addresses%20one%20of%20India%27s%20most%20pressing%20linguistic%20gaps%3A%20the%20lack%20of%20resources%20for%20its%20diverse%20low-resource%20languages%20%28LRLs%29.%20In%20this%20study%2C%20we%20investigate%20whether%20enforcing%20cross-lingual%20similarity%20in%20specific%20internal%20layers%20of%20a%20decoder-only%20multilingual%20large%20language%20model%20%28LLM%29%20can%20improve%20translation%20quality%20from%20LRL%20to%20high-resource%20language%20%28HRL%29.%20Specifically%2C%20we%20combine%20Centered%20Kernel%20Alignment%20%28CKA%29%2C%20a%20similarity%20metric%20that%20encourages%20representations%20of%20different%20languages%20to%20align%2C%20with%20REPINA%2C%20a%20regularization%20method%20that%20constrains%20parameter%20updates%20to%20remain%20close%20to%20the%20pretrained%20model%2C%20into%20a%20joint%20method%20we%20call%20TRepLiNa.%20In%20this%20research%20project%2C%20we%20experiment%20with%20zero-shot%2C%20few-shot%2C%20and%20fine-tuning%20settings%20using%20Aya-23%208B%20with%20QLoRA%20across%20MMLoSo%20shared%20task%20language%20pairs%20%28Mundari%2C%20Santali%2C%20Bhili%29%20with%20Hindi/English%20pivots.%20Our%20results%20show%20that%20aligning%20mid-level%20layers%20using%20TRepLiNa%20%28CKA%2BREPINA%29%20is%20a%20low-cost%2C%20practical%20approach%20to%20improving%20LRL%20translation%2C%20especially%20in%20data-scarce%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2510.06249v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRepLiNa%253A%2520Layer-wise%2520CKA%252BREPINA%2520Alignment%2520Improves%2520Low-Resource%2520Machine%2520Translation%2520in%2520Aya-23%25208B%26entry.906535625%3DToshiki%2520Nakai%2520and%2520Ravi%2520Kiran%2520Chikkala%2520and%2520Lena%2520Sophie%2520Oberkircher%2520and%2520Nicholas%2520Jennings%2520and%2520Natalia%2520Skachkova%2520and%2520Tatiana%2520Anikina%2520and%2520Jesujoba%2520Oluwadara%2520Alabi%26entry.1292438233%3DThe%25202025%2520Multimodal%2520Models%2520for%2520Low-Resource%2520Contexts%2520and%2520Social%2520Impact%2520%2528MMLoSo%2529%2520Language%2520Challenge%2520addresses%2520one%2520of%2520India%2527s%2520most%2520pressing%2520linguistic%2520gaps%253A%2520the%2520lack%2520of%2520resources%2520for%2520its%2520diverse%2520low-resource%2520languages%2520%2528LRLs%2529.%2520In%2520this%2520study%252C%2520we%2520investigate%2520whether%2520enforcing%2520cross-lingual%2520similarity%2520in%2520specific%2520internal%2520layers%2520of%2520a%2520decoder-only%2520multilingual%2520large%2520language%2520model%2520%2528LLM%2529%2520can%2520improve%2520translation%2520quality%2520from%2520LRL%2520to%2520high-resource%2520language%2520%2528HRL%2529.%2520Specifically%252C%2520we%2520combine%2520Centered%2520Kernel%2520Alignment%2520%2528CKA%2529%252C%2520a%2520similarity%2520metric%2520that%2520encourages%2520representations%2520of%2520different%2520languages%2520to%2520align%252C%2520with%2520REPINA%252C%2520a%2520regularization%2520method%2520that%2520constrains%2520parameter%2520updates%2520to%2520remain%2520close%2520to%2520the%2520pretrained%2520model%252C%2520into%2520a%2520joint%2520method%2520we%2520call%2520TRepLiNa.%2520In%2520this%2520research%2520project%252C%2520we%2520experiment%2520with%2520zero-shot%252C%2520few-shot%252C%2520and%2520fine-tuning%2520settings%2520using%2520Aya-23%25208B%2520with%2520QLoRA%2520across%2520MMLoSo%2520shared%2520task%2520language%2520pairs%2520%2528Mundari%252C%2520Santali%252C%2520Bhili%2529%2520with%2520Hindi/English%2520pivots.%2520Our%2520results%2520show%2520that%2520aligning%2520mid-level%2520layers%2520using%2520TRepLiNa%2520%2528CKA%252BREPINA%2529%2520is%2520a%2520low-cost%252C%2520practical%2520approach%2520to%2520improving%2520LRL%2520translation%252C%2520especially%2520in%2520data-scarce%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06249v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRepLiNa%3A%20Layer-wise%20CKA%2BREPINA%20Alignment%20Improves%20Low-Resource%20Machine%20Translation%20in%20Aya-23%208B&entry.906535625=Toshiki%20Nakai%20and%20Ravi%20Kiran%20Chikkala%20and%20Lena%20Sophie%20Oberkircher%20and%20Nicholas%20Jennings%20and%20Natalia%20Skachkova%20and%20Tatiana%20Anikina%20and%20Jesujoba%20Oluwadara%20Alabi&entry.1292438233=The%202025%20Multimodal%20Models%20for%20Low-Resource%20Contexts%20and%20Social%20Impact%20%28MMLoSo%29%20Language%20Challenge%20addresses%20one%20of%20India%27s%20most%20pressing%20linguistic%20gaps%3A%20the%20lack%20of%20resources%20for%20its%20diverse%20low-resource%20languages%20%28LRLs%29.%20In%20this%20study%2C%20we%20investigate%20whether%20enforcing%20cross-lingual%20similarity%20in%20specific%20internal%20layers%20of%20a%20decoder-only%20multilingual%20large%20language%20model%20%28LLM%29%20can%20improve%20translation%20quality%20from%20LRL%20to%20high-resource%20language%20%28HRL%29.%20Specifically%2C%20we%20combine%20Centered%20Kernel%20Alignment%20%28CKA%29%2C%20a%20similarity%20metric%20that%20encourages%20representations%20of%20different%20languages%20to%20align%2C%20with%20REPINA%2C%20a%20regularization%20method%20that%20constrains%20parameter%20updates%20to%20remain%20close%20to%20the%20pretrained%20model%2C%20into%20a%20joint%20method%20we%20call%20TRepLiNa.%20In%20this%20research%20project%2C%20we%20experiment%20with%20zero-shot%2C%20few-shot%2C%20and%20fine-tuning%20settings%20using%20Aya-23%208B%20with%20QLoRA%20across%20MMLoSo%20shared%20task%20language%20pairs%20%28Mundari%2C%20Santali%2C%20Bhili%29%20with%20Hindi/English%20pivots.%20Our%20results%20show%20that%20aligning%20mid-level%20layers%20using%20TRepLiNa%20%28CKA%2BREPINA%29%20is%20a%20low-cost%2C%20practical%20approach%20to%20improving%20LRL%20translation%2C%20especially%20in%20data-scarce%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2510.06249v4&entry.124074799=Read"},
{"title": "Multi-Rigid-Body Approximation of Human Hands with Application to Digital Twin", "author": "Bin Zhao and Yiwen Lu and Haohua Zhu and Xiao Li and Sheng Yi", "abstract": "Human hand simulation plays a critical role in digital twin applications, requiring models that balance anatomical fidelity with computational efficiency. We present a complete pipeline for constructing multi-rigid-body approximations of human hands that preserve realistic appearance while enabling real-time physics simulation. Starting from optical motion capture of a specific human hand, we construct a personalized MANO (Multi-Abstracted hand model with Neural Operations) model and convert it to a URDF (Unified Robot Description Format) representation with anatomically consistent joint axes. The key technical challenge is projecting MANO's unconstrained SO(3) joint rotations onto the kinematically constrained joints of the rigid-body model. We derive closed-form solutions for single degree-of-freedom joints and introduce a Baker-Campbell-Hausdorff (BCH)-corrected iterative method for two degree-of-freedom joints that properly handles the non-commutativity of rotations. We validate our approach through digital twin experiments where reinforcement learning policies control the multi-rigid-body hand to replay captured human demonstrations. Quantitative evaluation shows sub-centimeter reconstruction error and successful grasp execution across diverse manipulation tasks.", "link": "http://arxiv.org/abs/2512.07359v1", "date": "2025-12-08", "relevancy": 2.4238, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6291}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.591}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Rigid-Body%20Approximation%20of%20Human%20Hands%20with%20Application%20to%20Digital%20Twin&body=Title%3A%20Multi-Rigid-Body%20Approximation%20of%20Human%20Hands%20with%20Application%20to%20Digital%20Twin%0AAuthor%3A%20Bin%20Zhao%20and%20Yiwen%20Lu%20and%20Haohua%20Zhu%20and%20Xiao%20Li%20and%20Sheng%20Yi%0AAbstract%3A%20Human%20hand%20simulation%20plays%20a%20critical%20role%20in%20digital%20twin%20applications%2C%20requiring%20models%20that%20balance%20anatomical%20fidelity%20with%20computational%20efficiency.%20We%20present%20a%20complete%20pipeline%20for%20constructing%20multi-rigid-body%20approximations%20of%20human%20hands%20that%20preserve%20realistic%20appearance%20while%20enabling%20real-time%20physics%20simulation.%20Starting%20from%20optical%20motion%20capture%20of%20a%20specific%20human%20hand%2C%20we%20construct%20a%20personalized%20MANO%20%28Multi-Abstracted%20hand%20model%20with%20Neural%20Operations%29%20model%20and%20convert%20it%20to%20a%20URDF%20%28Unified%20Robot%20Description%20Format%29%20representation%20with%20anatomically%20consistent%20joint%20axes.%20The%20key%20technical%20challenge%20is%20projecting%20MANO%27s%20unconstrained%20SO%283%29%20joint%20rotations%20onto%20the%20kinematically%20constrained%20joints%20of%20the%20rigid-body%20model.%20We%20derive%20closed-form%20solutions%20for%20single%20degree-of-freedom%20joints%20and%20introduce%20a%20Baker-Campbell-Hausdorff%20%28BCH%29-corrected%20iterative%20method%20for%20two%20degree-of-freedom%20joints%20that%20properly%20handles%20the%20non-commutativity%20of%20rotations.%20We%20validate%20our%20approach%20through%20digital%20twin%20experiments%20where%20reinforcement%20learning%20policies%20control%20the%20multi-rigid-body%20hand%20to%20replay%20captured%20human%20demonstrations.%20Quantitative%20evaluation%20shows%20sub-centimeter%20reconstruction%20error%20and%20successful%20grasp%20execution%20across%20diverse%20manipulation%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07359v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Rigid-Body%2520Approximation%2520of%2520Human%2520Hands%2520with%2520Application%2520to%2520Digital%2520Twin%26entry.906535625%3DBin%2520Zhao%2520and%2520Yiwen%2520Lu%2520and%2520Haohua%2520Zhu%2520and%2520Xiao%2520Li%2520and%2520Sheng%2520Yi%26entry.1292438233%3DHuman%2520hand%2520simulation%2520plays%2520a%2520critical%2520role%2520in%2520digital%2520twin%2520applications%252C%2520requiring%2520models%2520that%2520balance%2520anatomical%2520fidelity%2520with%2520computational%2520efficiency.%2520We%2520present%2520a%2520complete%2520pipeline%2520for%2520constructing%2520multi-rigid-body%2520approximations%2520of%2520human%2520hands%2520that%2520preserve%2520realistic%2520appearance%2520while%2520enabling%2520real-time%2520physics%2520simulation.%2520Starting%2520from%2520optical%2520motion%2520capture%2520of%2520a%2520specific%2520human%2520hand%252C%2520we%2520construct%2520a%2520personalized%2520MANO%2520%2528Multi-Abstracted%2520hand%2520model%2520with%2520Neural%2520Operations%2529%2520model%2520and%2520convert%2520it%2520to%2520a%2520URDF%2520%2528Unified%2520Robot%2520Description%2520Format%2529%2520representation%2520with%2520anatomically%2520consistent%2520joint%2520axes.%2520The%2520key%2520technical%2520challenge%2520is%2520projecting%2520MANO%2527s%2520unconstrained%2520SO%25283%2529%2520joint%2520rotations%2520onto%2520the%2520kinematically%2520constrained%2520joints%2520of%2520the%2520rigid-body%2520model.%2520We%2520derive%2520closed-form%2520solutions%2520for%2520single%2520degree-of-freedom%2520joints%2520and%2520introduce%2520a%2520Baker-Campbell-Hausdorff%2520%2528BCH%2529-corrected%2520iterative%2520method%2520for%2520two%2520degree-of-freedom%2520joints%2520that%2520properly%2520handles%2520the%2520non-commutativity%2520of%2520rotations.%2520We%2520validate%2520our%2520approach%2520through%2520digital%2520twin%2520experiments%2520where%2520reinforcement%2520learning%2520policies%2520control%2520the%2520multi-rigid-body%2520hand%2520to%2520replay%2520captured%2520human%2520demonstrations.%2520Quantitative%2520evaluation%2520shows%2520sub-centimeter%2520reconstruction%2520error%2520and%2520successful%2520grasp%2520execution%2520across%2520diverse%2520manipulation%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07359v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Rigid-Body%20Approximation%20of%20Human%20Hands%20with%20Application%20to%20Digital%20Twin&entry.906535625=Bin%20Zhao%20and%20Yiwen%20Lu%20and%20Haohua%20Zhu%20and%20Xiao%20Li%20and%20Sheng%20Yi&entry.1292438233=Human%20hand%20simulation%20plays%20a%20critical%20role%20in%20digital%20twin%20applications%2C%20requiring%20models%20that%20balance%20anatomical%20fidelity%20with%20computational%20efficiency.%20We%20present%20a%20complete%20pipeline%20for%20constructing%20multi-rigid-body%20approximations%20of%20human%20hands%20that%20preserve%20realistic%20appearance%20while%20enabling%20real-time%20physics%20simulation.%20Starting%20from%20optical%20motion%20capture%20of%20a%20specific%20human%20hand%2C%20we%20construct%20a%20personalized%20MANO%20%28Multi-Abstracted%20hand%20model%20with%20Neural%20Operations%29%20model%20and%20convert%20it%20to%20a%20URDF%20%28Unified%20Robot%20Description%20Format%29%20representation%20with%20anatomically%20consistent%20joint%20axes.%20The%20key%20technical%20challenge%20is%20projecting%20MANO%27s%20unconstrained%20SO%283%29%20joint%20rotations%20onto%20the%20kinematically%20constrained%20joints%20of%20the%20rigid-body%20model.%20We%20derive%20closed-form%20solutions%20for%20single%20degree-of-freedom%20joints%20and%20introduce%20a%20Baker-Campbell-Hausdorff%20%28BCH%29-corrected%20iterative%20method%20for%20two%20degree-of-freedom%20joints%20that%20properly%20handles%20the%20non-commutativity%20of%20rotations.%20We%20validate%20our%20approach%20through%20digital%20twin%20experiments%20where%20reinforcement%20learning%20policies%20control%20the%20multi-rigid-body%20hand%20to%20replay%20captured%20human%20demonstrations.%20Quantitative%20evaluation%20shows%20sub-centimeter%20reconstruction%20error%20and%20successful%20grasp%20execution%20across%20diverse%20manipulation%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.07359v1&entry.124074799=Read"},
{"title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation", "author": "Xiaochuang Han and Youssef Emad and Melissa Hall and John Nguyen and Karthik Padthe and Liam Robbins and Amir Bar and Delong Chen and Michal Drozdzal and Maha Elbayad and Yushi Hu and Shang-Wen Li and Sreya Dutta Roy and Jakob Verbeek and XuDong Wang and Marjan Ghazvininejad and Luke Zettlemoyer and Emily Dinan", "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.", "link": "http://arxiv.org/abs/2512.05103v2", "date": "2025-12-08", "relevancy": 2.4189, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6147}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6012}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TV2TV%3A%20A%20Unified%20Framework%20for%20Interleaved%20Language%20and%20Video%20Generation&body=Title%3A%20TV2TV%3A%20A%20Unified%20Framework%20for%20Interleaved%20Language%20and%20Video%20Generation%0AAuthor%3A%20Xiaochuang%20Han%20and%20Youssef%20Emad%20and%20Melissa%20Hall%20and%20John%20Nguyen%20and%20Karthik%20Padthe%20and%20Liam%20Robbins%20and%20Amir%20Bar%20and%20Delong%20Chen%20and%20Michal%20Drozdzal%20and%20Maha%20Elbayad%20and%20Yushi%20Hu%20and%20Shang-Wen%20Li%20and%20Sreya%20Dutta%20Roy%20and%20Jakob%20Verbeek%20and%20XuDong%20Wang%20and%20Marjan%20Ghazvininejad%20and%20Luke%20Zettlemoyer%20and%20Emily%20Dinan%0AAbstract%3A%20Video%20generation%20models%20are%20rapidly%20advancing%2C%20but%20can%20still%20struggle%20with%20complex%20video%20outputs%20that%20require%20significant%20semantic%20branching%20or%20repeated%20high-level%20reasoning%20about%20what%20should%20happen%20next.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20class%20of%20omni%20video-text%20models%20that%20integrate%20ideas%20from%20recent%20LM%20reasoning%20advances%20to%20address%20this%20challenge.%20More%20specifically%2C%20we%20present%20TV2TV%2C%20a%20unified%20generative%20modeling%20framework%20which%20decomposes%20video%20generation%20into%20an%20interleaved%20text%20and%20video%20generation%20process.%20TV2TV%20jointly%20learns%20language%20modeling%20%28next-token%20prediction%29%20and%20video%20flow%20matching%20%28next-frame%20prediction%29%20using%20a%20Mixture-of-Transformers%20%28MoT%29%20architecture.%20At%20inference%20time%2C%20TV2TV%20decides%20when%20to%20alternate%20between%20generating%20text%20and%20video%20frames%2C%20allowing%20the%20model%20to%20%22think%20in%20words%22%20about%20subsequent%20content%20before%20%60%60acting%20in%20pixels%27%27%20to%20produce%20frames.%20This%20design%20offloads%20much%20of%20the%20responsibility%20for%20deciding%20what%20should%20happen%20next%20to%20the%20language%20modeling%20tower%2C%20enabling%20improved%20visual%20quality%20and%20prompt%20alignment%20of%20generated%20videos.%20It%20also%20enables%20fine-grained%20controllability%2C%20allowing%20users%20to%20modify%20the%20video%20generation%20trajectory%20through%20text%20interventions%20at%20any%20point%20in%20the%20process.%20In%20controlled%20experiments%20on%20video%20game%20data%2C%20TV2TV%20demonstrates%20substantial%20improvements%20in%20both%20visual%20quality%20and%20controllability.%20TV2TV%20also%20scales%20to%20natural%20videos%2C%20as%20we%20show%20by%20augmenting%20sports%20videos%20with%20interleaved%20natural%20language%20action%20descriptions%20using%20vision-language%20models%20%28VLMs%29.%20Training%20TV2TV%20on%20this%20corpus%20yields%20strong%20visual%20quality%20and%20prompt%20alignment%2C%20showcasing%20the%20model%27s%20ability%20to%20reason%20about%20and%20generate%20complex%20real-world%20action%20sequences.%20Together%2C%20these%20results%20highlight%20TV2TV%20as%20a%20promising%20step%20toward%20video%20generation%20with%20open-ended%20textual%20reasoning%20and%20control.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05103v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTV2TV%253A%2520A%2520Unified%2520Framework%2520for%2520Interleaved%2520Language%2520and%2520Video%2520Generation%26entry.906535625%3DXiaochuang%2520Han%2520and%2520Youssef%2520Emad%2520and%2520Melissa%2520Hall%2520and%2520John%2520Nguyen%2520and%2520Karthik%2520Padthe%2520and%2520Liam%2520Robbins%2520and%2520Amir%2520Bar%2520and%2520Delong%2520Chen%2520and%2520Michal%2520Drozdzal%2520and%2520Maha%2520Elbayad%2520and%2520Yushi%2520Hu%2520and%2520Shang-Wen%2520Li%2520and%2520Sreya%2520Dutta%2520Roy%2520and%2520Jakob%2520Verbeek%2520and%2520XuDong%2520Wang%2520and%2520Marjan%2520Ghazvininejad%2520and%2520Luke%2520Zettlemoyer%2520and%2520Emily%2520Dinan%26entry.1292438233%3DVideo%2520generation%2520models%2520are%2520rapidly%2520advancing%252C%2520but%2520can%2520still%2520struggle%2520with%2520complex%2520video%2520outputs%2520that%2520require%2520significant%2520semantic%2520branching%2520or%2520repeated%2520high-level%2520reasoning%2520about%2520what%2520should%2520happen%2520next.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520class%2520of%2520omni%2520video-text%2520models%2520that%2520integrate%2520ideas%2520from%2520recent%2520LM%2520reasoning%2520advances%2520to%2520address%2520this%2520challenge.%2520More%2520specifically%252C%2520we%2520present%2520TV2TV%252C%2520a%2520unified%2520generative%2520modeling%2520framework%2520which%2520decomposes%2520video%2520generation%2520into%2520an%2520interleaved%2520text%2520and%2520video%2520generation%2520process.%2520TV2TV%2520jointly%2520learns%2520language%2520modeling%2520%2528next-token%2520prediction%2529%2520and%2520video%2520flow%2520matching%2520%2528next-frame%2520prediction%2529%2520using%2520a%2520Mixture-of-Transformers%2520%2528MoT%2529%2520architecture.%2520At%2520inference%2520time%252C%2520TV2TV%2520decides%2520when%2520to%2520alternate%2520between%2520generating%2520text%2520and%2520video%2520frames%252C%2520allowing%2520the%2520model%2520to%2520%2522think%2520in%2520words%2522%2520about%2520subsequent%2520content%2520before%2520%2560%2560acting%2520in%2520pixels%2527%2527%2520to%2520produce%2520frames.%2520This%2520design%2520offloads%2520much%2520of%2520the%2520responsibility%2520for%2520deciding%2520what%2520should%2520happen%2520next%2520to%2520the%2520language%2520modeling%2520tower%252C%2520enabling%2520improved%2520visual%2520quality%2520and%2520prompt%2520alignment%2520of%2520generated%2520videos.%2520It%2520also%2520enables%2520fine-grained%2520controllability%252C%2520allowing%2520users%2520to%2520modify%2520the%2520video%2520generation%2520trajectory%2520through%2520text%2520interventions%2520at%2520any%2520point%2520in%2520the%2520process.%2520In%2520controlled%2520experiments%2520on%2520video%2520game%2520data%252C%2520TV2TV%2520demonstrates%2520substantial%2520improvements%2520in%2520both%2520visual%2520quality%2520and%2520controllability.%2520TV2TV%2520also%2520scales%2520to%2520natural%2520videos%252C%2520as%2520we%2520show%2520by%2520augmenting%2520sports%2520videos%2520with%2520interleaved%2520natural%2520language%2520action%2520descriptions%2520using%2520vision-language%2520models%2520%2528VLMs%2529.%2520Training%2520TV2TV%2520on%2520this%2520corpus%2520yields%2520strong%2520visual%2520quality%2520and%2520prompt%2520alignment%252C%2520showcasing%2520the%2520model%2527s%2520ability%2520to%2520reason%2520about%2520and%2520generate%2520complex%2520real-world%2520action%2520sequences.%2520Together%252C%2520these%2520results%2520highlight%2520TV2TV%2520as%2520a%2520promising%2520step%2520toward%2520video%2520generation%2520with%2520open-ended%2520textual%2520reasoning%2520and%2520control.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05103v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TV2TV%3A%20A%20Unified%20Framework%20for%20Interleaved%20Language%20and%20Video%20Generation&entry.906535625=Xiaochuang%20Han%20and%20Youssef%20Emad%20and%20Melissa%20Hall%20and%20John%20Nguyen%20and%20Karthik%20Padthe%20and%20Liam%20Robbins%20and%20Amir%20Bar%20and%20Delong%20Chen%20and%20Michal%20Drozdzal%20and%20Maha%20Elbayad%20and%20Yushi%20Hu%20and%20Shang-Wen%20Li%20and%20Sreya%20Dutta%20Roy%20and%20Jakob%20Verbeek%20and%20XuDong%20Wang%20and%20Marjan%20Ghazvininejad%20and%20Luke%20Zettlemoyer%20and%20Emily%20Dinan&entry.1292438233=Video%20generation%20models%20are%20rapidly%20advancing%2C%20but%20can%20still%20struggle%20with%20complex%20video%20outputs%20that%20require%20significant%20semantic%20branching%20or%20repeated%20high-level%20reasoning%20about%20what%20should%20happen%20next.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20class%20of%20omni%20video-text%20models%20that%20integrate%20ideas%20from%20recent%20LM%20reasoning%20advances%20to%20address%20this%20challenge.%20More%20specifically%2C%20we%20present%20TV2TV%2C%20a%20unified%20generative%20modeling%20framework%20which%20decomposes%20video%20generation%20into%20an%20interleaved%20text%20and%20video%20generation%20process.%20TV2TV%20jointly%20learns%20language%20modeling%20%28next-token%20prediction%29%20and%20video%20flow%20matching%20%28next-frame%20prediction%29%20using%20a%20Mixture-of-Transformers%20%28MoT%29%20architecture.%20At%20inference%20time%2C%20TV2TV%20decides%20when%20to%20alternate%20between%20generating%20text%20and%20video%20frames%2C%20allowing%20the%20model%20to%20%22think%20in%20words%22%20about%20subsequent%20content%20before%20%60%60acting%20in%20pixels%27%27%20to%20produce%20frames.%20This%20design%20offloads%20much%20of%20the%20responsibility%20for%20deciding%20what%20should%20happen%20next%20to%20the%20language%20modeling%20tower%2C%20enabling%20improved%20visual%20quality%20and%20prompt%20alignment%20of%20generated%20videos.%20It%20also%20enables%20fine-grained%20controllability%2C%20allowing%20users%20to%20modify%20the%20video%20generation%20trajectory%20through%20text%20interventions%20at%20any%20point%20in%20the%20process.%20In%20controlled%20experiments%20on%20video%20game%20data%2C%20TV2TV%20demonstrates%20substantial%20improvements%20in%20both%20visual%20quality%20and%20controllability.%20TV2TV%20also%20scales%20to%20natural%20videos%2C%20as%20we%20show%20by%20augmenting%20sports%20videos%20with%20interleaved%20natural%20language%20action%20descriptions%20using%20vision-language%20models%20%28VLMs%29.%20Training%20TV2TV%20on%20this%20corpus%20yields%20strong%20visual%20quality%20and%20prompt%20alignment%2C%20showcasing%20the%20model%27s%20ability%20to%20reason%20about%20and%20generate%20complex%20real-world%20action%20sequences.%20Together%2C%20these%20results%20highlight%20TV2TV%20as%20a%20promising%20step%20toward%20video%20generation%20with%20open-ended%20textual%20reasoning%20and%20control.&entry.1838667208=http%3A//arxiv.org/abs/2512.05103v2&entry.124074799=Read"},
{"title": "Multi-Domain Motion Embedding: Expressive Real-Time Mimicry for Legged Robots", "author": "Matthias Heyrman and Chenhao Li and Victor Klemm and Dongho Kang and Stelian Coros and Marco Hutter", "abstract": "Effective motion representation is crucial for enabling robots to imitate expressive behaviors in real time, yet existing motion controllers often ignore inherent patterns in motion. Previous efforts in representation learning do not attempt to jointly capture structured periodic patterns and irregular variations in human and animal movement. To address this, we present Multi-Domain Motion Embedding (MDME), a motion representation that unifies the embedding of structured and unstructured features using a wavelet-based encoder and a probabilistic embedding in parallel. This produces a rich representation of reference motions from a minimal input set, enabling improved generalization across diverse motion styles and morphologies. We evaluate MDME on retargeting-free real-time motion imitation by conditioning robot control policies on the learned embeddings, demonstrating accurate reproduction of complex trajectories on both humanoid and quadruped platforms. Our comparative studies confirm that MDME outperforms prior approaches in reconstruction fidelity and generalizability to unseen motions. Furthermore, we demonstrate that MDME can reproduce novel motion styles in real-time through zero-shot deployment, eliminating the need for task-specific tuning or online retargeting. These results position MDME as a generalizable and structure-aware foundation for scalable real-time robot imitation.", "link": "http://arxiv.org/abs/2512.07673v1", "date": "2025-12-08", "relevancy": 2.4157, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6437}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6408}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Domain%20Motion%20Embedding%3A%20Expressive%20Real-Time%20Mimicry%20for%20Legged%20Robots&body=Title%3A%20Multi-Domain%20Motion%20Embedding%3A%20Expressive%20Real-Time%20Mimicry%20for%20Legged%20Robots%0AAuthor%3A%20Matthias%20Heyrman%20and%20Chenhao%20Li%20and%20Victor%20Klemm%20and%20Dongho%20Kang%20and%20Stelian%20Coros%20and%20Marco%20Hutter%0AAbstract%3A%20Effective%20motion%20representation%20is%20crucial%20for%20enabling%20robots%20to%20imitate%20expressive%20behaviors%20in%20real%20time%2C%20yet%20existing%20motion%20controllers%20often%20ignore%20inherent%20patterns%20in%20motion.%20Previous%20efforts%20in%20representation%20learning%20do%20not%20attempt%20to%20jointly%20capture%20structured%20periodic%20patterns%20and%20irregular%20variations%20in%20human%20and%20animal%20movement.%20To%20address%20this%2C%20we%20present%20Multi-Domain%20Motion%20Embedding%20%28MDME%29%2C%20a%20motion%20representation%20that%20unifies%20the%20embedding%20of%20structured%20and%20unstructured%20features%20using%20a%20wavelet-based%20encoder%20and%20a%20probabilistic%20embedding%20in%20parallel.%20This%20produces%20a%20rich%20representation%20of%20reference%20motions%20from%20a%20minimal%20input%20set%2C%20enabling%20improved%20generalization%20across%20diverse%20motion%20styles%20and%20morphologies.%20We%20evaluate%20MDME%20on%20retargeting-free%20real-time%20motion%20imitation%20by%20conditioning%20robot%20control%20policies%20on%20the%20learned%20embeddings%2C%20demonstrating%20accurate%20reproduction%20of%20complex%20trajectories%20on%20both%20humanoid%20and%20quadruped%20platforms.%20Our%20comparative%20studies%20confirm%20that%20MDME%20outperforms%20prior%20approaches%20in%20reconstruction%20fidelity%20and%20generalizability%20to%20unseen%20motions.%20Furthermore%2C%20we%20demonstrate%20that%20MDME%20can%20reproduce%20novel%20motion%20styles%20in%20real-time%20through%20zero-shot%20deployment%2C%20eliminating%20the%20need%20for%20task-specific%20tuning%20or%20online%20retargeting.%20These%20results%20position%20MDME%20as%20a%20generalizable%20and%20structure-aware%20foundation%20for%20scalable%20real-time%20robot%20imitation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07673v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Domain%2520Motion%2520Embedding%253A%2520Expressive%2520Real-Time%2520Mimicry%2520for%2520Legged%2520Robots%26entry.906535625%3DMatthias%2520Heyrman%2520and%2520Chenhao%2520Li%2520and%2520Victor%2520Klemm%2520and%2520Dongho%2520Kang%2520and%2520Stelian%2520Coros%2520and%2520Marco%2520Hutter%26entry.1292438233%3DEffective%2520motion%2520representation%2520is%2520crucial%2520for%2520enabling%2520robots%2520to%2520imitate%2520expressive%2520behaviors%2520in%2520real%2520time%252C%2520yet%2520existing%2520motion%2520controllers%2520often%2520ignore%2520inherent%2520patterns%2520in%2520motion.%2520Previous%2520efforts%2520in%2520representation%2520learning%2520do%2520not%2520attempt%2520to%2520jointly%2520capture%2520structured%2520periodic%2520patterns%2520and%2520irregular%2520variations%2520in%2520human%2520and%2520animal%2520movement.%2520To%2520address%2520this%252C%2520we%2520present%2520Multi-Domain%2520Motion%2520Embedding%2520%2528MDME%2529%252C%2520a%2520motion%2520representation%2520that%2520unifies%2520the%2520embedding%2520of%2520structured%2520and%2520unstructured%2520features%2520using%2520a%2520wavelet-based%2520encoder%2520and%2520a%2520probabilistic%2520embedding%2520in%2520parallel.%2520This%2520produces%2520a%2520rich%2520representation%2520of%2520reference%2520motions%2520from%2520a%2520minimal%2520input%2520set%252C%2520enabling%2520improved%2520generalization%2520across%2520diverse%2520motion%2520styles%2520and%2520morphologies.%2520We%2520evaluate%2520MDME%2520on%2520retargeting-free%2520real-time%2520motion%2520imitation%2520by%2520conditioning%2520robot%2520control%2520policies%2520on%2520the%2520learned%2520embeddings%252C%2520demonstrating%2520accurate%2520reproduction%2520of%2520complex%2520trajectories%2520on%2520both%2520humanoid%2520and%2520quadruped%2520platforms.%2520Our%2520comparative%2520studies%2520confirm%2520that%2520MDME%2520outperforms%2520prior%2520approaches%2520in%2520reconstruction%2520fidelity%2520and%2520generalizability%2520to%2520unseen%2520motions.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520MDME%2520can%2520reproduce%2520novel%2520motion%2520styles%2520in%2520real-time%2520through%2520zero-shot%2520deployment%252C%2520eliminating%2520the%2520need%2520for%2520task-specific%2520tuning%2520or%2520online%2520retargeting.%2520These%2520results%2520position%2520MDME%2520as%2520a%2520generalizable%2520and%2520structure-aware%2520foundation%2520for%2520scalable%2520real-time%2520robot%2520imitation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07673v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Domain%20Motion%20Embedding%3A%20Expressive%20Real-Time%20Mimicry%20for%20Legged%20Robots&entry.906535625=Matthias%20Heyrman%20and%20Chenhao%20Li%20and%20Victor%20Klemm%20and%20Dongho%20Kang%20and%20Stelian%20Coros%20and%20Marco%20Hutter&entry.1292438233=Effective%20motion%20representation%20is%20crucial%20for%20enabling%20robots%20to%20imitate%20expressive%20behaviors%20in%20real%20time%2C%20yet%20existing%20motion%20controllers%20often%20ignore%20inherent%20patterns%20in%20motion.%20Previous%20efforts%20in%20representation%20learning%20do%20not%20attempt%20to%20jointly%20capture%20structured%20periodic%20patterns%20and%20irregular%20variations%20in%20human%20and%20animal%20movement.%20To%20address%20this%2C%20we%20present%20Multi-Domain%20Motion%20Embedding%20%28MDME%29%2C%20a%20motion%20representation%20that%20unifies%20the%20embedding%20of%20structured%20and%20unstructured%20features%20using%20a%20wavelet-based%20encoder%20and%20a%20probabilistic%20embedding%20in%20parallel.%20This%20produces%20a%20rich%20representation%20of%20reference%20motions%20from%20a%20minimal%20input%20set%2C%20enabling%20improved%20generalization%20across%20diverse%20motion%20styles%20and%20morphologies.%20We%20evaluate%20MDME%20on%20retargeting-free%20real-time%20motion%20imitation%20by%20conditioning%20robot%20control%20policies%20on%20the%20learned%20embeddings%2C%20demonstrating%20accurate%20reproduction%20of%20complex%20trajectories%20on%20both%20humanoid%20and%20quadruped%20platforms.%20Our%20comparative%20studies%20confirm%20that%20MDME%20outperforms%20prior%20approaches%20in%20reconstruction%20fidelity%20and%20generalizability%20to%20unseen%20motions.%20Furthermore%2C%20we%20demonstrate%20that%20MDME%20can%20reproduce%20novel%20motion%20styles%20in%20real-time%20through%20zero-shot%20deployment%2C%20eliminating%20the%20need%20for%20task-specific%20tuning%20or%20online%20retargeting.%20These%20results%20position%20MDME%20as%20a%20generalizable%20and%20structure-aware%20foundation%20for%20scalable%20real-time%20robot%20imitation.&entry.1838667208=http%3A//arxiv.org/abs/2512.07673v1&entry.124074799=Read"},
{"title": "ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points", "author": "Ryota Okumura and Kaede Shiohara and Toshihiko Yamasaki", "abstract": "Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at https://github.com/RyotaOkumura/ControlVP .", "link": "http://arxiv.org/abs/2512.07504v1", "date": "2025-12-08", "relevancy": 2.4119, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6308}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5974}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ControlVP%3A%20Interactive%20Geometric%20Refinement%20of%20AI-Generated%20Images%20with%20Consistent%20Vanishing%20Points&body=Title%3A%20ControlVP%3A%20Interactive%20Geometric%20Refinement%20of%20AI-Generated%20Images%20with%20Consistent%20Vanishing%20Points%0AAuthor%3A%20Ryota%20Okumura%20and%20Kaede%20Shiohara%20and%20Toshihiko%20Yamasaki%0AAbstract%3A%20Recent%20text-to-image%20models%2C%20such%20as%20Stable%20Diffusion%2C%20have%20achieved%20impressive%20visual%20quality%2C%20yet%20they%20often%20suffer%20from%20geometric%20inconsistencies%20that%20undermine%20the%20structural%20realism%20of%20generated%20scenes.%20One%20prominent%20issue%20is%20vanishing%20point%20inconsistency%2C%20where%20projections%20of%20parallel%20lines%20fail%20to%20converge%20correctly%20in%202D%20space.%20This%20leads%20to%20structurally%20implausible%20geometry%20that%20degrades%20spatial%20realism%2C%20especially%20in%20architectural%20scenes.%20We%20propose%20ControlVP%2C%20a%20user-guided%20framework%20for%20correcting%20vanishing%20point%20inconsistencies%20in%20generated%20images.%20Our%20approach%20extends%20a%20pre-trained%20diffusion%20model%20by%20incorporating%20structural%20guidance%20derived%20from%20building%20contours.%20We%20also%20introduce%20geometric%20constraints%20that%20explicitly%20encourage%20alignment%20between%20image%20edges%20and%20perspective%20cues.%20Our%20method%20enhances%20global%20geometric%20consistency%20while%20maintaining%20visual%20fidelity%20comparable%20to%20the%20baselines.%20This%20capability%20is%20particularly%20valuable%20for%20applications%20that%20require%20accurate%20spatial%20structure%2C%20such%20as%20image-to-3D%20reconstruction.%20The%20dataset%20and%20source%20code%20are%20available%20at%20https%3A//github.com/RyotaOkumura/ControlVP%20.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07504v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControlVP%253A%2520Interactive%2520Geometric%2520Refinement%2520of%2520AI-Generated%2520Images%2520with%2520Consistent%2520Vanishing%2520Points%26entry.906535625%3DRyota%2520Okumura%2520and%2520Kaede%2520Shiohara%2520and%2520Toshihiko%2520Yamasaki%26entry.1292438233%3DRecent%2520text-to-image%2520models%252C%2520such%2520as%2520Stable%2520Diffusion%252C%2520have%2520achieved%2520impressive%2520visual%2520quality%252C%2520yet%2520they%2520often%2520suffer%2520from%2520geometric%2520inconsistencies%2520that%2520undermine%2520the%2520structural%2520realism%2520of%2520generated%2520scenes.%2520One%2520prominent%2520issue%2520is%2520vanishing%2520point%2520inconsistency%252C%2520where%2520projections%2520of%2520parallel%2520lines%2520fail%2520to%2520converge%2520correctly%2520in%25202D%2520space.%2520This%2520leads%2520to%2520structurally%2520implausible%2520geometry%2520that%2520degrades%2520spatial%2520realism%252C%2520especially%2520in%2520architectural%2520scenes.%2520We%2520propose%2520ControlVP%252C%2520a%2520user-guided%2520framework%2520for%2520correcting%2520vanishing%2520point%2520inconsistencies%2520in%2520generated%2520images.%2520Our%2520approach%2520extends%2520a%2520pre-trained%2520diffusion%2520model%2520by%2520incorporating%2520structural%2520guidance%2520derived%2520from%2520building%2520contours.%2520We%2520also%2520introduce%2520geometric%2520constraints%2520that%2520explicitly%2520encourage%2520alignment%2520between%2520image%2520edges%2520and%2520perspective%2520cues.%2520Our%2520method%2520enhances%2520global%2520geometric%2520consistency%2520while%2520maintaining%2520visual%2520fidelity%2520comparable%2520to%2520the%2520baselines.%2520This%2520capability%2520is%2520particularly%2520valuable%2520for%2520applications%2520that%2520require%2520accurate%2520spatial%2520structure%252C%2520such%2520as%2520image-to-3D%2520reconstruction.%2520The%2520dataset%2520and%2520source%2520code%2520are%2520available%2520at%2520https%253A//github.com/RyotaOkumura/ControlVP%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07504v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ControlVP%3A%20Interactive%20Geometric%20Refinement%20of%20AI-Generated%20Images%20with%20Consistent%20Vanishing%20Points&entry.906535625=Ryota%20Okumura%20and%20Kaede%20Shiohara%20and%20Toshihiko%20Yamasaki&entry.1292438233=Recent%20text-to-image%20models%2C%20such%20as%20Stable%20Diffusion%2C%20have%20achieved%20impressive%20visual%20quality%2C%20yet%20they%20often%20suffer%20from%20geometric%20inconsistencies%20that%20undermine%20the%20structural%20realism%20of%20generated%20scenes.%20One%20prominent%20issue%20is%20vanishing%20point%20inconsistency%2C%20where%20projections%20of%20parallel%20lines%20fail%20to%20converge%20correctly%20in%202D%20space.%20This%20leads%20to%20structurally%20implausible%20geometry%20that%20degrades%20spatial%20realism%2C%20especially%20in%20architectural%20scenes.%20We%20propose%20ControlVP%2C%20a%20user-guided%20framework%20for%20correcting%20vanishing%20point%20inconsistencies%20in%20generated%20images.%20Our%20approach%20extends%20a%20pre-trained%20diffusion%20model%20by%20incorporating%20structural%20guidance%20derived%20from%20building%20contours.%20We%20also%20introduce%20geometric%20constraints%20that%20explicitly%20encourage%20alignment%20between%20image%20edges%20and%20perspective%20cues.%20Our%20method%20enhances%20global%20geometric%20consistency%20while%20maintaining%20visual%20fidelity%20comparable%20to%20the%20baselines.%20This%20capability%20is%20particularly%20valuable%20for%20applications%20that%20require%20accurate%20spatial%20structure%2C%20such%20as%20image-to-3D%20reconstruction.%20The%20dataset%20and%20source%20code%20are%20available%20at%20https%3A//github.com/RyotaOkumura/ControlVP%20.&entry.1838667208=http%3A//arxiv.org/abs/2512.07504v1&entry.124074799=Read"},
{"title": "RealD$^2$iff: Bridging Real-World Gap in Robot Manipulation via Depth Diffusion", "author": "Xiujian Liang and Jiacheng Liu and Mingyang Sun and Qichen He and Cewu Lu and Jianhua Sun", "abstract": "Robot manipulation in the real world is fundamentally constrained by the visual sim2real gap, where depth observations collected in simulation fail to reflect the complex noise patterns inherent to real sensors. In this work, inspired by the denoising capability of diffusion models, we invert the conventional perspective and propose a clean-to-noisy paradigm that learns to synthesize noisy depth, thereby bridging the visual sim2real gap through purely simulation-driven robotic learning. Building on this idea, we introduce RealD$^2$iff, a hierarchical coarse-to-fine diffusion framework that decomposes depth noise into global structural distortions and fine-grained local perturbations. To enable progressive learning of these components, we further develop two complementary strategies: Frequency-Guided Supervision (FGS) for global structure modeling and Discrepancy-Guided Optimization (DGO) for localized refinement. To integrate RealD$^2$iff seamlessly into imitation learning, we construct a pipeline that spans six stages. We provide comprehensive empirical and experimental validation demonstrating the effectiveness of this paradigm. RealD$^2$iff enables two key applications: (1) generating real-world-like depth to construct clean-noisy paired datasets without manual sensor data collection. (2) Achieving zero-shot sim2real robot manipulation, substantially improving real-world performance without additional fine-tuning.", "link": "http://arxiv.org/abs/2511.22505v2", "date": "2025-12-08", "relevancy": 2.4068, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.604}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6005}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RealD%24%5E2%24iff%3A%20Bridging%20Real-World%20Gap%20in%20Robot%20Manipulation%20via%20Depth%20Diffusion&body=Title%3A%20RealD%24%5E2%24iff%3A%20Bridging%20Real-World%20Gap%20in%20Robot%20Manipulation%20via%20Depth%20Diffusion%0AAuthor%3A%20Xiujian%20Liang%20and%20Jiacheng%20Liu%20and%20Mingyang%20Sun%20and%20Qichen%20He%20and%20Cewu%20Lu%20and%20Jianhua%20Sun%0AAbstract%3A%20Robot%20manipulation%20in%20the%20real%20world%20is%20fundamentally%20constrained%20by%20the%20visual%20sim2real%20gap%2C%20where%20depth%20observations%20collected%20in%20simulation%20fail%20to%20reflect%20the%20complex%20noise%20patterns%20inherent%20to%20real%20sensors.%20In%20this%20work%2C%20inspired%20by%20the%20denoising%20capability%20of%20diffusion%20models%2C%20we%20invert%20the%20conventional%20perspective%20and%20propose%20a%20clean-to-noisy%20paradigm%20that%20learns%20to%20synthesize%20noisy%20depth%2C%20thereby%20bridging%20the%20visual%20sim2real%20gap%20through%20purely%20simulation-driven%20robotic%20learning.%20Building%20on%20this%20idea%2C%20we%20introduce%20RealD%24%5E2%24iff%2C%20a%20hierarchical%20coarse-to-fine%20diffusion%20framework%20that%20decomposes%20depth%20noise%20into%20global%20structural%20distortions%20and%20fine-grained%20local%20perturbations.%20To%20enable%20progressive%20learning%20of%20these%20components%2C%20we%20further%20develop%20two%20complementary%20strategies%3A%20Frequency-Guided%20Supervision%20%28FGS%29%20for%20global%20structure%20modeling%20and%20Discrepancy-Guided%20Optimization%20%28DGO%29%20for%20localized%20refinement.%20To%20integrate%20RealD%24%5E2%24iff%20seamlessly%20into%20imitation%20learning%2C%20we%20construct%20a%20pipeline%20that%20spans%20six%20stages.%20We%20provide%20comprehensive%20empirical%20and%20experimental%20validation%20demonstrating%20the%20effectiveness%20of%20this%20paradigm.%20RealD%24%5E2%24iff%20enables%20two%20key%20applications%3A%20%281%29%20generating%20real-world-like%20depth%20to%20construct%20clean-noisy%20paired%20datasets%20without%20manual%20sensor%20data%20collection.%20%282%29%20Achieving%20zero-shot%20sim2real%20robot%20manipulation%2C%20substantially%20improving%20real-world%20performance%20without%20additional%20fine-tuning.%0ALink%3A%20http%3A//arxiv.org/abs/2511.22505v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealD%2524%255E2%2524iff%253A%2520Bridging%2520Real-World%2520Gap%2520in%2520Robot%2520Manipulation%2520via%2520Depth%2520Diffusion%26entry.906535625%3DXiujian%2520Liang%2520and%2520Jiacheng%2520Liu%2520and%2520Mingyang%2520Sun%2520and%2520Qichen%2520He%2520and%2520Cewu%2520Lu%2520and%2520Jianhua%2520Sun%26entry.1292438233%3DRobot%2520manipulation%2520in%2520the%2520real%2520world%2520is%2520fundamentally%2520constrained%2520by%2520the%2520visual%2520sim2real%2520gap%252C%2520where%2520depth%2520observations%2520collected%2520in%2520simulation%2520fail%2520to%2520reflect%2520the%2520complex%2520noise%2520patterns%2520inherent%2520to%2520real%2520sensors.%2520In%2520this%2520work%252C%2520inspired%2520by%2520the%2520denoising%2520capability%2520of%2520diffusion%2520models%252C%2520we%2520invert%2520the%2520conventional%2520perspective%2520and%2520propose%2520a%2520clean-to-noisy%2520paradigm%2520that%2520learns%2520to%2520synthesize%2520noisy%2520depth%252C%2520thereby%2520bridging%2520the%2520visual%2520sim2real%2520gap%2520through%2520purely%2520simulation-driven%2520robotic%2520learning.%2520Building%2520on%2520this%2520idea%252C%2520we%2520introduce%2520RealD%2524%255E2%2524iff%252C%2520a%2520hierarchical%2520coarse-to-fine%2520diffusion%2520framework%2520that%2520decomposes%2520depth%2520noise%2520into%2520global%2520structural%2520distortions%2520and%2520fine-grained%2520local%2520perturbations.%2520To%2520enable%2520progressive%2520learning%2520of%2520these%2520components%252C%2520we%2520further%2520develop%2520two%2520complementary%2520strategies%253A%2520Frequency-Guided%2520Supervision%2520%2528FGS%2529%2520for%2520global%2520structure%2520modeling%2520and%2520Discrepancy-Guided%2520Optimization%2520%2528DGO%2529%2520for%2520localized%2520refinement.%2520To%2520integrate%2520RealD%2524%255E2%2524iff%2520seamlessly%2520into%2520imitation%2520learning%252C%2520we%2520construct%2520a%2520pipeline%2520that%2520spans%2520six%2520stages.%2520We%2520provide%2520comprehensive%2520empirical%2520and%2520experimental%2520validation%2520demonstrating%2520the%2520effectiveness%2520of%2520this%2520paradigm.%2520RealD%2524%255E2%2524iff%2520enables%2520two%2520key%2520applications%253A%2520%25281%2529%2520generating%2520real-world-like%2520depth%2520to%2520construct%2520clean-noisy%2520paired%2520datasets%2520without%2520manual%2520sensor%2520data%2520collection.%2520%25282%2529%2520Achieving%2520zero-shot%2520sim2real%2520robot%2520manipulation%252C%2520substantially%2520improving%2520real-world%2520performance%2520without%2520additional%2520fine-tuning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.22505v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealD%24%5E2%24iff%3A%20Bridging%20Real-World%20Gap%20in%20Robot%20Manipulation%20via%20Depth%20Diffusion&entry.906535625=Xiujian%20Liang%20and%20Jiacheng%20Liu%20and%20Mingyang%20Sun%20and%20Qichen%20He%20and%20Cewu%20Lu%20and%20Jianhua%20Sun&entry.1292438233=Robot%20manipulation%20in%20the%20real%20world%20is%20fundamentally%20constrained%20by%20the%20visual%20sim2real%20gap%2C%20where%20depth%20observations%20collected%20in%20simulation%20fail%20to%20reflect%20the%20complex%20noise%20patterns%20inherent%20to%20real%20sensors.%20In%20this%20work%2C%20inspired%20by%20the%20denoising%20capability%20of%20diffusion%20models%2C%20we%20invert%20the%20conventional%20perspective%20and%20propose%20a%20clean-to-noisy%20paradigm%20that%20learns%20to%20synthesize%20noisy%20depth%2C%20thereby%20bridging%20the%20visual%20sim2real%20gap%20through%20purely%20simulation-driven%20robotic%20learning.%20Building%20on%20this%20idea%2C%20we%20introduce%20RealD%24%5E2%24iff%2C%20a%20hierarchical%20coarse-to-fine%20diffusion%20framework%20that%20decomposes%20depth%20noise%20into%20global%20structural%20distortions%20and%20fine-grained%20local%20perturbations.%20To%20enable%20progressive%20learning%20of%20these%20components%2C%20we%20further%20develop%20two%20complementary%20strategies%3A%20Frequency-Guided%20Supervision%20%28FGS%29%20for%20global%20structure%20modeling%20and%20Discrepancy-Guided%20Optimization%20%28DGO%29%20for%20localized%20refinement.%20To%20integrate%20RealD%24%5E2%24iff%20seamlessly%20into%20imitation%20learning%2C%20we%20construct%20a%20pipeline%20that%20spans%20six%20stages.%20We%20provide%20comprehensive%20empirical%20and%20experimental%20validation%20demonstrating%20the%20effectiveness%20of%20this%20paradigm.%20RealD%24%5E2%24iff%20enables%20two%20key%20applications%3A%20%281%29%20generating%20real-world-like%20depth%20to%20construct%20clean-noisy%20paired%20datasets%20without%20manual%20sensor%20data%20collection.%20%282%29%20Achieving%20zero-shot%20sim2real%20robot%20manipulation%2C%20substantially%20improving%20real-world%20performance%20without%20additional%20fine-tuning.&entry.1838667208=http%3A//arxiv.org/abs/2511.22505v2&entry.124074799=Read"},
{"title": "A Bootstrap Perspective on Stochastic Gradient Descent", "author": "Hongjian Lan and Yucong Liu and Florian Sch\u00e4fer", "abstract": "Machine learning models trained with \\emph{stochastic} gradient descent (SGD) can generalize better than those trained with deterministic gradient descent (GD). In this work, we study SGD's impact on generalization through the lens of the statistical bootstrap: SGD uses gradient variability under batch sampling as a proxy for solution variability under the randomness of the data collection process. We use empirical results and theoretical analysis to substantiate this claim. In idealized experiments on empirical risk minimization, we show that SGD is drawn to parameter choices that are robust under resampling and thus avoids spurious solutions even if they lie in wider and deeper minima of the training loss. We prove rigorously that by implicitly regularizing the trace of the gradient covariance matrix, SGD controls the algorithmic variability. This regularization leads to solutions that are less sensitive to sampling noise, thereby improving generalization. Numerical experiments on neural network training show that explicitly incorporating the estimate of the algorithmic variability as a regularizer improves test performance. This fact supports our claim that bootstrap estimation underpins SGD's generalization advantages.", "link": "http://arxiv.org/abs/2512.07676v1", "date": "2025-12-08", "relevancy": 2.3923, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4894}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4865}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Bootstrap%20Perspective%20on%20Stochastic%20Gradient%20Descent&body=Title%3A%20A%20Bootstrap%20Perspective%20on%20Stochastic%20Gradient%20Descent%0AAuthor%3A%20Hongjian%20Lan%20and%20Yucong%20Liu%20and%20Florian%20Sch%C3%A4fer%0AAbstract%3A%20Machine%20learning%20models%20trained%20with%20%5Cemph%7Bstochastic%7D%20gradient%20descent%20%28SGD%29%20can%20generalize%20better%20than%20those%20trained%20with%20deterministic%20gradient%20descent%20%28GD%29.%20In%20this%20work%2C%20we%20study%20SGD%27s%20impact%20on%20generalization%20through%20the%20lens%20of%20the%20statistical%20bootstrap%3A%20SGD%20uses%20gradient%20variability%20under%20batch%20sampling%20as%20a%20proxy%20for%20solution%20variability%20under%20the%20randomness%20of%20the%20data%20collection%20process.%20We%20use%20empirical%20results%20and%20theoretical%20analysis%20to%20substantiate%20this%20claim.%20In%20idealized%20experiments%20on%20empirical%20risk%20minimization%2C%20we%20show%20that%20SGD%20is%20drawn%20to%20parameter%20choices%20that%20are%20robust%20under%20resampling%20and%20thus%20avoids%20spurious%20solutions%20even%20if%20they%20lie%20in%20wider%20and%20deeper%20minima%20of%20the%20training%20loss.%20We%20prove%20rigorously%20that%20by%20implicitly%20regularizing%20the%20trace%20of%20the%20gradient%20covariance%20matrix%2C%20SGD%20controls%20the%20algorithmic%20variability.%20This%20regularization%20leads%20to%20solutions%20that%20are%20less%20sensitive%20to%20sampling%20noise%2C%20thereby%20improving%20generalization.%20Numerical%20experiments%20on%20neural%20network%20training%20show%20that%20explicitly%20incorporating%20the%20estimate%20of%20the%20algorithmic%20variability%20as%20a%20regularizer%20improves%20test%20performance.%20This%20fact%20supports%20our%20claim%20that%20bootstrap%20estimation%20underpins%20SGD%27s%20generalization%20advantages.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Bootstrap%2520Perspective%2520on%2520Stochastic%2520Gradient%2520Descent%26entry.906535625%3DHongjian%2520Lan%2520and%2520Yucong%2520Liu%2520and%2520Florian%2520Sch%25C3%25A4fer%26entry.1292438233%3DMachine%2520learning%2520models%2520trained%2520with%2520%255Cemph%257Bstochastic%257D%2520gradient%2520descent%2520%2528SGD%2529%2520can%2520generalize%2520better%2520than%2520those%2520trained%2520with%2520deterministic%2520gradient%2520descent%2520%2528GD%2529.%2520In%2520this%2520work%252C%2520we%2520study%2520SGD%2527s%2520impact%2520on%2520generalization%2520through%2520the%2520lens%2520of%2520the%2520statistical%2520bootstrap%253A%2520SGD%2520uses%2520gradient%2520variability%2520under%2520batch%2520sampling%2520as%2520a%2520proxy%2520for%2520solution%2520variability%2520under%2520the%2520randomness%2520of%2520the%2520data%2520collection%2520process.%2520We%2520use%2520empirical%2520results%2520and%2520theoretical%2520analysis%2520to%2520substantiate%2520this%2520claim.%2520In%2520idealized%2520experiments%2520on%2520empirical%2520risk%2520minimization%252C%2520we%2520show%2520that%2520SGD%2520is%2520drawn%2520to%2520parameter%2520choices%2520that%2520are%2520robust%2520under%2520resampling%2520and%2520thus%2520avoids%2520spurious%2520solutions%2520even%2520if%2520they%2520lie%2520in%2520wider%2520and%2520deeper%2520minima%2520of%2520the%2520training%2520loss.%2520We%2520prove%2520rigorously%2520that%2520by%2520implicitly%2520regularizing%2520the%2520trace%2520of%2520the%2520gradient%2520covariance%2520matrix%252C%2520SGD%2520controls%2520the%2520algorithmic%2520variability.%2520This%2520regularization%2520leads%2520to%2520solutions%2520that%2520are%2520less%2520sensitive%2520to%2520sampling%2520noise%252C%2520thereby%2520improving%2520generalization.%2520Numerical%2520experiments%2520on%2520neural%2520network%2520training%2520show%2520that%2520explicitly%2520incorporating%2520the%2520estimate%2520of%2520the%2520algorithmic%2520variability%2520as%2520a%2520regularizer%2520improves%2520test%2520performance.%2520This%2520fact%2520supports%2520our%2520claim%2520that%2520bootstrap%2520estimation%2520underpins%2520SGD%2527s%2520generalization%2520advantages.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Bootstrap%20Perspective%20on%20Stochastic%20Gradient%20Descent&entry.906535625=Hongjian%20Lan%20and%20Yucong%20Liu%20and%20Florian%20Sch%C3%A4fer&entry.1292438233=Machine%20learning%20models%20trained%20with%20%5Cemph%7Bstochastic%7D%20gradient%20descent%20%28SGD%29%20can%20generalize%20better%20than%20those%20trained%20with%20deterministic%20gradient%20descent%20%28GD%29.%20In%20this%20work%2C%20we%20study%20SGD%27s%20impact%20on%20generalization%20through%20the%20lens%20of%20the%20statistical%20bootstrap%3A%20SGD%20uses%20gradient%20variability%20under%20batch%20sampling%20as%20a%20proxy%20for%20solution%20variability%20under%20the%20randomness%20of%20the%20data%20collection%20process.%20We%20use%20empirical%20results%20and%20theoretical%20analysis%20to%20substantiate%20this%20claim.%20In%20idealized%20experiments%20on%20empirical%20risk%20minimization%2C%20we%20show%20that%20SGD%20is%20drawn%20to%20parameter%20choices%20that%20are%20robust%20under%20resampling%20and%20thus%20avoids%20spurious%20solutions%20even%20if%20they%20lie%20in%20wider%20and%20deeper%20minima%20of%20the%20training%20loss.%20We%20prove%20rigorously%20that%20by%20implicitly%20regularizing%20the%20trace%20of%20the%20gradient%20covariance%20matrix%2C%20SGD%20controls%20the%20algorithmic%20variability.%20This%20regularization%20leads%20to%20solutions%20that%20are%20less%20sensitive%20to%20sampling%20noise%2C%20thereby%20improving%20generalization.%20Numerical%20experiments%20on%20neural%20network%20training%20show%20that%20explicitly%20incorporating%20the%20estimate%20of%20the%20algorithmic%20variability%20as%20a%20regularizer%20improves%20test%20performance.%20This%20fact%20supports%20our%20claim%20that%20bootstrap%20estimation%20underpins%20SGD%27s%20generalization%20advantages.&entry.1838667208=http%3A//arxiv.org/abs/2512.07676v1&entry.124074799=Read"},
{"title": "Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning", "author": "Amir Mohammad Akhlaghi and Amirhossein Shabani and Mostafa Abdolmaleki and Saeed Reza Kheradpisheh", "abstract": "The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique \"warm-up\" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.", "link": "http://arxiv.org/abs/2512.07454v1", "date": "2025-12-08", "relevancy": 2.381, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5144}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4576}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Persian-Phi%3A%20Efficient%20Cross-Lingual%20Adaptation%20of%20Compact%20LLMs%20via%20Curriculum%20Learning&body=Title%3A%20Persian-Phi%3A%20Efficient%20Cross-Lingual%20Adaptation%20of%20Compact%20LLMs%20via%20Curriculum%20Learning%0AAuthor%3A%20Amir%20Mohammad%20Akhlaghi%20and%20Amirhossein%20Shabani%20and%20Mostafa%20Abdolmaleki%20and%20Saeed%20Reza%20Kheradpisheh%0AAbstract%3A%20The%20democratization%20of%20AI%20is%20currently%20hindered%20by%20the%20immense%20computational%20costs%20required%20to%20train%20Large%20Language%20Models%20%28LLMs%29%20for%20low-resource%20languages.%20This%20paper%20presents%20Persian-Phi%2C%20a%203.8B%20parameter%20model%20that%20challenges%20the%20assumption%20that%20robust%20multilingual%20capabilities%20require%20massive%20model%20sizes%20or%20multilingual%20baselines.%20We%20demonstrate%20how%20Microsoft%20Phi-3%20Mini%20--%20originally%20a%20monolingual%20English%20model%20--%20can%20be%20effectively%20adapted%20to%20Persian%20through%20a%20novel%2C%20resource-efficient%20curriculum%20learning%20pipeline.%20Our%20approach%20employs%20a%20unique%20%22warm-up%22%20stage%20using%20bilingual%20narratives%20%28Tiny%20Stories%29%20to%20align%20embeddings%20prior%20to%20heavy%20training%2C%20followed%20by%20continual%20pretraining%20and%20instruction%20tuning%20via%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29.%20Despite%20its%20compact%20size%2C%20Persian-Phi%20achieves%20competitive%20results%20on%20Open%20Persian%20LLM%20Leaderboard%20in%20HuggingFace.%20Our%20findings%20provide%20a%20validated%2C%20scalable%20framework%20for%20extending%20the%20reach%20of%20state-of-the-art%20LLMs%20to%20underrepresented%20languages%20with%20minimal%20hardware%20resources.%20The%20Persian-Phi%20model%20is%20publicly%20available%20at%20https%3A//huggingface.co/amirakhlaghiqqq/PersianPhi.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersian-Phi%253A%2520Efficient%2520Cross-Lingual%2520Adaptation%2520of%2520Compact%2520LLMs%2520via%2520Curriculum%2520Learning%26entry.906535625%3DAmir%2520Mohammad%2520Akhlaghi%2520and%2520Amirhossein%2520Shabani%2520and%2520Mostafa%2520Abdolmaleki%2520and%2520Saeed%2520Reza%2520Kheradpisheh%26entry.1292438233%3DThe%2520democratization%2520of%2520AI%2520is%2520currently%2520hindered%2520by%2520the%2520immense%2520computational%2520costs%2520required%2520to%2520train%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520low-resource%2520languages.%2520This%2520paper%2520presents%2520Persian-Phi%252C%2520a%25203.8B%2520parameter%2520model%2520that%2520challenges%2520the%2520assumption%2520that%2520robust%2520multilingual%2520capabilities%2520require%2520massive%2520model%2520sizes%2520or%2520multilingual%2520baselines.%2520We%2520demonstrate%2520how%2520Microsoft%2520Phi-3%2520Mini%2520--%2520originally%2520a%2520monolingual%2520English%2520model%2520--%2520can%2520be%2520effectively%2520adapted%2520to%2520Persian%2520through%2520a%2520novel%252C%2520resource-efficient%2520curriculum%2520learning%2520pipeline.%2520Our%2520approach%2520employs%2520a%2520unique%2520%2522warm-up%2522%2520stage%2520using%2520bilingual%2520narratives%2520%2528Tiny%2520Stories%2529%2520to%2520align%2520embeddings%2520prior%2520to%2520heavy%2520training%252C%2520followed%2520by%2520continual%2520pretraining%2520and%2520instruction%2520tuning%2520via%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529.%2520Despite%2520its%2520compact%2520size%252C%2520Persian-Phi%2520achieves%2520competitive%2520results%2520on%2520Open%2520Persian%2520LLM%2520Leaderboard%2520in%2520HuggingFace.%2520Our%2520findings%2520provide%2520a%2520validated%252C%2520scalable%2520framework%2520for%2520extending%2520the%2520reach%2520of%2520state-of-the-art%2520LLMs%2520to%2520underrepresented%2520languages%2520with%2520minimal%2520hardware%2520resources.%2520The%2520Persian-Phi%2520model%2520is%2520publicly%2520available%2520at%2520https%253A//huggingface.co/amirakhlaghiqqq/PersianPhi.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Persian-Phi%3A%20Efficient%20Cross-Lingual%20Adaptation%20of%20Compact%20LLMs%20via%20Curriculum%20Learning&entry.906535625=Amir%20Mohammad%20Akhlaghi%20and%20Amirhossein%20Shabani%20and%20Mostafa%20Abdolmaleki%20and%20Saeed%20Reza%20Kheradpisheh&entry.1292438233=The%20democratization%20of%20AI%20is%20currently%20hindered%20by%20the%20immense%20computational%20costs%20required%20to%20train%20Large%20Language%20Models%20%28LLMs%29%20for%20low-resource%20languages.%20This%20paper%20presents%20Persian-Phi%2C%20a%203.8B%20parameter%20model%20that%20challenges%20the%20assumption%20that%20robust%20multilingual%20capabilities%20require%20massive%20model%20sizes%20or%20multilingual%20baselines.%20We%20demonstrate%20how%20Microsoft%20Phi-3%20Mini%20--%20originally%20a%20monolingual%20English%20model%20--%20can%20be%20effectively%20adapted%20to%20Persian%20through%20a%20novel%2C%20resource-efficient%20curriculum%20learning%20pipeline.%20Our%20approach%20employs%20a%20unique%20%22warm-up%22%20stage%20using%20bilingual%20narratives%20%28Tiny%20Stories%29%20to%20align%20embeddings%20prior%20to%20heavy%20training%2C%20followed%20by%20continual%20pretraining%20and%20instruction%20tuning%20via%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29.%20Despite%20its%20compact%20size%2C%20Persian-Phi%20achieves%20competitive%20results%20on%20Open%20Persian%20LLM%20Leaderboard%20in%20HuggingFace.%20Our%20findings%20provide%20a%20validated%2C%20scalable%20framework%20for%20extending%20the%20reach%20of%20state-of-the-art%20LLMs%20to%20underrepresented%20languages%20with%20minimal%20hardware%20resources.%20The%20Persian-Phi%20model%20is%20publicly%20available%20at%20https%3A//huggingface.co/amirakhlaghiqqq/PersianPhi.&entry.1838667208=http%3A//arxiv.org/abs/2512.07454v1&entry.124074799=Read"},
{"title": "Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning", "author": "Amir Mohammad Akhlaghi and Amirhossein Shabani and Mostafa Abdolmaleki and Saeed Reza Kheradpisheh", "abstract": "The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique \"warm-up\" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.", "link": "http://arxiv.org/abs/2512.07454v1", "date": "2025-12-08", "relevancy": 2.381, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5144}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4576}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Persian-Phi%3A%20Efficient%20Cross-Lingual%20Adaptation%20of%20Compact%20LLMs%20via%20Curriculum%20Learning&body=Title%3A%20Persian-Phi%3A%20Efficient%20Cross-Lingual%20Adaptation%20of%20Compact%20LLMs%20via%20Curriculum%20Learning%0AAuthor%3A%20Amir%20Mohammad%20Akhlaghi%20and%20Amirhossein%20Shabani%20and%20Mostafa%20Abdolmaleki%20and%20Saeed%20Reza%20Kheradpisheh%0AAbstract%3A%20The%20democratization%20of%20AI%20is%20currently%20hindered%20by%20the%20immense%20computational%20costs%20required%20to%20train%20Large%20Language%20Models%20%28LLMs%29%20for%20low-resource%20languages.%20This%20paper%20presents%20Persian-Phi%2C%20a%203.8B%20parameter%20model%20that%20challenges%20the%20assumption%20that%20robust%20multilingual%20capabilities%20require%20massive%20model%20sizes%20or%20multilingual%20baselines.%20We%20demonstrate%20how%20Microsoft%20Phi-3%20Mini%20--%20originally%20a%20monolingual%20English%20model%20--%20can%20be%20effectively%20adapted%20to%20Persian%20through%20a%20novel%2C%20resource-efficient%20curriculum%20learning%20pipeline.%20Our%20approach%20employs%20a%20unique%20%22warm-up%22%20stage%20using%20bilingual%20narratives%20%28Tiny%20Stories%29%20to%20align%20embeddings%20prior%20to%20heavy%20training%2C%20followed%20by%20continual%20pretraining%20and%20instruction%20tuning%20via%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29.%20Despite%20its%20compact%20size%2C%20Persian-Phi%20achieves%20competitive%20results%20on%20Open%20Persian%20LLM%20Leaderboard%20in%20HuggingFace.%20Our%20findings%20provide%20a%20validated%2C%20scalable%20framework%20for%20extending%20the%20reach%20of%20state-of-the-art%20LLMs%20to%20underrepresented%20languages%20with%20minimal%20hardware%20resources.%20The%20Persian-Phi%20model%20is%20publicly%20available%20at%20https%3A//huggingface.co/amirakhlaghiqqq/PersianPhi.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersian-Phi%253A%2520Efficient%2520Cross-Lingual%2520Adaptation%2520of%2520Compact%2520LLMs%2520via%2520Curriculum%2520Learning%26entry.906535625%3DAmir%2520Mohammad%2520Akhlaghi%2520and%2520Amirhossein%2520Shabani%2520and%2520Mostafa%2520Abdolmaleki%2520and%2520Saeed%2520Reza%2520Kheradpisheh%26entry.1292438233%3DThe%2520democratization%2520of%2520AI%2520is%2520currently%2520hindered%2520by%2520the%2520immense%2520computational%2520costs%2520required%2520to%2520train%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520low-resource%2520languages.%2520This%2520paper%2520presents%2520Persian-Phi%252C%2520a%25203.8B%2520parameter%2520model%2520that%2520challenges%2520the%2520assumption%2520that%2520robust%2520multilingual%2520capabilities%2520require%2520massive%2520model%2520sizes%2520or%2520multilingual%2520baselines.%2520We%2520demonstrate%2520how%2520Microsoft%2520Phi-3%2520Mini%2520--%2520originally%2520a%2520monolingual%2520English%2520model%2520--%2520can%2520be%2520effectively%2520adapted%2520to%2520Persian%2520through%2520a%2520novel%252C%2520resource-efficient%2520curriculum%2520learning%2520pipeline.%2520Our%2520approach%2520employs%2520a%2520unique%2520%2522warm-up%2522%2520stage%2520using%2520bilingual%2520narratives%2520%2528Tiny%2520Stories%2529%2520to%2520align%2520embeddings%2520prior%2520to%2520heavy%2520training%252C%2520followed%2520by%2520continual%2520pretraining%2520and%2520instruction%2520tuning%2520via%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529.%2520Despite%2520its%2520compact%2520size%252C%2520Persian-Phi%2520achieves%2520competitive%2520results%2520on%2520Open%2520Persian%2520LLM%2520Leaderboard%2520in%2520HuggingFace.%2520Our%2520findings%2520provide%2520a%2520validated%252C%2520scalable%2520framework%2520for%2520extending%2520the%2520reach%2520of%2520state-of-the-art%2520LLMs%2520to%2520underrepresented%2520languages%2520with%2520minimal%2520hardware%2520resources.%2520The%2520Persian-Phi%2520model%2520is%2520publicly%2520available%2520at%2520https%253A//huggingface.co/amirakhlaghiqqq/PersianPhi.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Persian-Phi%3A%20Efficient%20Cross-Lingual%20Adaptation%20of%20Compact%20LLMs%20via%20Curriculum%20Learning&entry.906535625=Amir%20Mohammad%20Akhlaghi%20and%20Amirhossein%20Shabani%20and%20Mostafa%20Abdolmaleki%20and%20Saeed%20Reza%20Kheradpisheh&entry.1292438233=The%20democratization%20of%20AI%20is%20currently%20hindered%20by%20the%20immense%20computational%20costs%20required%20to%20train%20Large%20Language%20Models%20%28LLMs%29%20for%20low-resource%20languages.%20This%20paper%20presents%20Persian-Phi%2C%20a%203.8B%20parameter%20model%20that%20challenges%20the%20assumption%20that%20robust%20multilingual%20capabilities%20require%20massive%20model%20sizes%20or%20multilingual%20baselines.%20We%20demonstrate%20how%20Microsoft%20Phi-3%20Mini%20--%20originally%20a%20monolingual%20English%20model%20--%20can%20be%20effectively%20adapted%20to%20Persian%20through%20a%20novel%2C%20resource-efficient%20curriculum%20learning%20pipeline.%20Our%20approach%20employs%20a%20unique%20%22warm-up%22%20stage%20using%20bilingual%20narratives%20%28Tiny%20Stories%29%20to%20align%20embeddings%20prior%20to%20heavy%20training%2C%20followed%20by%20continual%20pretraining%20and%20instruction%20tuning%20via%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29.%20Despite%20its%20compact%20size%2C%20Persian-Phi%20achieves%20competitive%20results%20on%20Open%20Persian%20LLM%20Leaderboard%20in%20HuggingFace.%20Our%20findings%20provide%20a%20validated%2C%20scalable%20framework%20for%20extending%20the%20reach%20of%20state-of-the-art%20LLMs%20to%20underrepresented%20languages%20with%20minimal%20hardware%20resources.%20The%20Persian-Phi%20model%20is%20publicly%20available%20at%20https%3A//huggingface.co/amirakhlaghiqqq/PersianPhi.&entry.1838667208=http%3A//arxiv.org/abs/2512.07454v1&entry.124074799=Read"},
{"title": "Gait-Adaptive Perceptive Humanoid Locomotion with Real-Time Under-Base Terrain Reconstruction", "author": "Haolin Song and Hongbo Zhu and Tao Yu and Yan Liu and Mingqi Yuan and Wengang Zhou and Hua Chen and Houqiang Li", "abstract": "For full-size humanoid robots, even with recent advances in reinforcement learning-based control, achieving reliable locomotion on complex terrains, such as long staircases, remains challenging. In such settings, limited perception, ambiguous terrain cues, and insufficient adaptation of gait timing can cause even a single misplaced or mistimed step to result in rapid loss of balance. We introduce a perceptive locomotion framework that merges terrain sensing, gait regulation, and whole-body control into a single reinforcement learning policy. A downward-facing depth camera mounted under the base observes the support region around the feet, and a compact U-Net reconstructs a dense egocentric height map from each frame in real time, operating at the same frequency as the control loop. The perceptual height map, together with proprioceptive observations, is processed by a unified policy that produces joint commands and a global stepping-phase signal, allowing gait timing and whole-body posture to be adapted jointly to the commanded motion and local terrain geometry. We further adopt a single-stage successive teacher-student training scheme for efficient policy learning and knowledge transfer. Experiments conducted on a 31-DoF, 1.65 m humanoid robot demonstrate robust locomotion in both simulation and real-world settings, including forward and backward stair ascent and descent, as well as crossing a 46 cm gap. Project Page:https://ga-phl.github.io/", "link": "http://arxiv.org/abs/2512.07464v1", "date": "2025-12-08", "relevancy": 2.3724, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6353}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6042}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gait-Adaptive%20Perceptive%20Humanoid%20Locomotion%20with%20Real-Time%20Under-Base%20Terrain%20Reconstruction&body=Title%3A%20Gait-Adaptive%20Perceptive%20Humanoid%20Locomotion%20with%20Real-Time%20Under-Base%20Terrain%20Reconstruction%0AAuthor%3A%20Haolin%20Song%20and%20Hongbo%20Zhu%20and%20Tao%20Yu%20and%20Yan%20Liu%20and%20Mingqi%20Yuan%20and%20Wengang%20Zhou%20and%20Hua%20Chen%20and%20Houqiang%20Li%0AAbstract%3A%20For%20full-size%20humanoid%20robots%2C%20even%20with%20recent%20advances%20in%20reinforcement%20learning-based%20control%2C%20achieving%20reliable%20locomotion%20on%20complex%20terrains%2C%20such%20as%20long%20staircases%2C%20remains%20challenging.%20In%20such%20settings%2C%20limited%20perception%2C%20ambiguous%20terrain%20cues%2C%20and%20insufficient%20adaptation%20of%20gait%20timing%20can%20cause%20even%20a%20single%20misplaced%20or%20mistimed%20step%20to%20result%20in%20rapid%20loss%20of%20balance.%20We%20introduce%20a%20perceptive%20locomotion%20framework%20that%20merges%20terrain%20sensing%2C%20gait%20regulation%2C%20and%20whole-body%20control%20into%20a%20single%20reinforcement%20learning%20policy.%20A%20downward-facing%20depth%20camera%20mounted%20under%20the%20base%20observes%20the%20support%20region%20around%20the%20feet%2C%20and%20a%20compact%20U-Net%20reconstructs%20a%20dense%20egocentric%20height%20map%20from%20each%20frame%20in%20real%20time%2C%20operating%20at%20the%20same%20frequency%20as%20the%20control%20loop.%20The%20perceptual%20height%20map%2C%20together%20with%20proprioceptive%20observations%2C%20is%20processed%20by%20a%20unified%20policy%20that%20produces%20joint%20commands%20and%20a%20global%20stepping-phase%20signal%2C%20allowing%20gait%20timing%20and%20whole-body%20posture%20to%20be%20adapted%20jointly%20to%20the%20commanded%20motion%20and%20local%20terrain%20geometry.%20We%20further%20adopt%20a%20single-stage%20successive%20teacher-student%20training%20scheme%20for%20efficient%20policy%20learning%20and%20knowledge%20transfer.%20Experiments%20conducted%20on%20a%2031-DoF%2C%201.65%20m%20humanoid%20robot%20demonstrate%20robust%20locomotion%20in%20both%20simulation%20and%20real-world%20settings%2C%20including%20forward%20and%20backward%20stair%20ascent%20and%20descent%2C%20as%20well%20as%20crossing%20a%2046%20cm%20gap.%20Project%20Page%3Ahttps%3A//ga-phl.github.io/%0ALink%3A%20http%3A//arxiv.org/abs/2512.07464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGait-Adaptive%2520Perceptive%2520Humanoid%2520Locomotion%2520with%2520Real-Time%2520Under-Base%2520Terrain%2520Reconstruction%26entry.906535625%3DHaolin%2520Song%2520and%2520Hongbo%2520Zhu%2520and%2520Tao%2520Yu%2520and%2520Yan%2520Liu%2520and%2520Mingqi%2520Yuan%2520and%2520Wengang%2520Zhou%2520and%2520Hua%2520Chen%2520and%2520Houqiang%2520Li%26entry.1292438233%3DFor%2520full-size%2520humanoid%2520robots%252C%2520even%2520with%2520recent%2520advances%2520in%2520reinforcement%2520learning-based%2520control%252C%2520achieving%2520reliable%2520locomotion%2520on%2520complex%2520terrains%252C%2520such%2520as%2520long%2520staircases%252C%2520remains%2520challenging.%2520In%2520such%2520settings%252C%2520limited%2520perception%252C%2520ambiguous%2520terrain%2520cues%252C%2520and%2520insufficient%2520adaptation%2520of%2520gait%2520timing%2520can%2520cause%2520even%2520a%2520single%2520misplaced%2520or%2520mistimed%2520step%2520to%2520result%2520in%2520rapid%2520loss%2520of%2520balance.%2520We%2520introduce%2520a%2520perceptive%2520locomotion%2520framework%2520that%2520merges%2520terrain%2520sensing%252C%2520gait%2520regulation%252C%2520and%2520whole-body%2520control%2520into%2520a%2520single%2520reinforcement%2520learning%2520policy.%2520A%2520downward-facing%2520depth%2520camera%2520mounted%2520under%2520the%2520base%2520observes%2520the%2520support%2520region%2520around%2520the%2520feet%252C%2520and%2520a%2520compact%2520U-Net%2520reconstructs%2520a%2520dense%2520egocentric%2520height%2520map%2520from%2520each%2520frame%2520in%2520real%2520time%252C%2520operating%2520at%2520the%2520same%2520frequency%2520as%2520the%2520control%2520loop.%2520The%2520perceptual%2520height%2520map%252C%2520together%2520with%2520proprioceptive%2520observations%252C%2520is%2520processed%2520by%2520a%2520unified%2520policy%2520that%2520produces%2520joint%2520commands%2520and%2520a%2520global%2520stepping-phase%2520signal%252C%2520allowing%2520gait%2520timing%2520and%2520whole-body%2520posture%2520to%2520be%2520adapted%2520jointly%2520to%2520the%2520commanded%2520motion%2520and%2520local%2520terrain%2520geometry.%2520We%2520further%2520adopt%2520a%2520single-stage%2520successive%2520teacher-student%2520training%2520scheme%2520for%2520efficient%2520policy%2520learning%2520and%2520knowledge%2520transfer.%2520Experiments%2520conducted%2520on%2520a%252031-DoF%252C%25201.65%2520m%2520humanoid%2520robot%2520demonstrate%2520robust%2520locomotion%2520in%2520both%2520simulation%2520and%2520real-world%2520settings%252C%2520including%2520forward%2520and%2520backward%2520stair%2520ascent%2520and%2520descent%252C%2520as%2520well%2520as%2520crossing%2520a%252046%2520cm%2520gap.%2520Project%2520Page%253Ahttps%253A//ga-phl.github.io/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gait-Adaptive%20Perceptive%20Humanoid%20Locomotion%20with%20Real-Time%20Under-Base%20Terrain%20Reconstruction&entry.906535625=Haolin%20Song%20and%20Hongbo%20Zhu%20and%20Tao%20Yu%20and%20Yan%20Liu%20and%20Mingqi%20Yuan%20and%20Wengang%20Zhou%20and%20Hua%20Chen%20and%20Houqiang%20Li&entry.1292438233=For%20full-size%20humanoid%20robots%2C%20even%20with%20recent%20advances%20in%20reinforcement%20learning-based%20control%2C%20achieving%20reliable%20locomotion%20on%20complex%20terrains%2C%20such%20as%20long%20staircases%2C%20remains%20challenging.%20In%20such%20settings%2C%20limited%20perception%2C%20ambiguous%20terrain%20cues%2C%20and%20insufficient%20adaptation%20of%20gait%20timing%20can%20cause%20even%20a%20single%20misplaced%20or%20mistimed%20step%20to%20result%20in%20rapid%20loss%20of%20balance.%20We%20introduce%20a%20perceptive%20locomotion%20framework%20that%20merges%20terrain%20sensing%2C%20gait%20regulation%2C%20and%20whole-body%20control%20into%20a%20single%20reinforcement%20learning%20policy.%20A%20downward-facing%20depth%20camera%20mounted%20under%20the%20base%20observes%20the%20support%20region%20around%20the%20feet%2C%20and%20a%20compact%20U-Net%20reconstructs%20a%20dense%20egocentric%20height%20map%20from%20each%20frame%20in%20real%20time%2C%20operating%20at%20the%20same%20frequency%20as%20the%20control%20loop.%20The%20perceptual%20height%20map%2C%20together%20with%20proprioceptive%20observations%2C%20is%20processed%20by%20a%20unified%20policy%20that%20produces%20joint%20commands%20and%20a%20global%20stepping-phase%20signal%2C%20allowing%20gait%20timing%20and%20whole-body%20posture%20to%20be%20adapted%20jointly%20to%20the%20commanded%20motion%20and%20local%20terrain%20geometry.%20We%20further%20adopt%20a%20single-stage%20successive%20teacher-student%20training%20scheme%20for%20efficient%20policy%20learning%20and%20knowledge%20transfer.%20Experiments%20conducted%20on%20a%2031-DoF%2C%201.65%20m%20humanoid%20robot%20demonstrate%20robust%20locomotion%20in%20both%20simulation%20and%20real-world%20settings%2C%20including%20forward%20and%20backward%20stair%20ascent%20and%20descent%2C%20as%20well%20as%20crossing%20a%2046%20cm%20gap.%20Project%20Page%3Ahttps%3A//ga-phl.github.io/&entry.1838667208=http%3A//arxiv.org/abs/2512.07464v1&entry.124074799=Read"},
{"title": "Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent", "author": "Zhiyu Liu and Zhi Han and Yandong Tang and Jun Fan and Yao Wang", "abstract": "The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient descent. However, this kind of approach requires an accurate estimate of the tensor rank, and when the rank is overestimated, the convergence of gradient descent and its variants slows down significantly or even diverges. To address this problem, we propose an Alternating Preconditioned Gradient Descent (APGD) algorithm, which accelerates convergence in the over-parameterized setting by adding a preconditioning term to the original gradient and updating these two factors alternately. Based on certain geometric assumptions on the objective function, we establish linear convergence guarantees for more general low-tubal-rank tensor estimation problems. Then we further analyze the specific cases of low-tubal-rank tensor factorization and low-tubal-rank tensor recovery. Our theoretical results show that APGD achieves linear convergence even under over-parameterization, and the convergence rate is independent of the tensor condition number. Extensive simulations on synthetic data are carried out to validate our theoretical assertions.", "link": "http://arxiv.org/abs/2512.07490v1", "date": "2025-12-08", "relevancy": 2.3702, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4749}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4739}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Low-Tubal-Rank%20Tensor%20Estimation%20via%20Alternating%20Preconditioned%20Gradient%20Descent&body=Title%3A%20Efficient%20Low-Tubal-Rank%20Tensor%20Estimation%20via%20Alternating%20Preconditioned%20Gradient%20Descent%0AAuthor%3A%20Zhiyu%20Liu%20and%20Zhi%20Han%20and%20Yandong%20Tang%20and%20Jun%20Fan%20and%20Yao%20Wang%0AAbstract%3A%20The%20problem%20of%20low-tubal-rank%20tensor%20estimation%20is%20a%20fundamental%20task%20with%20wide%20applications%20across%20high-dimensional%20signal%20processing%2C%20machine%20learning%2C%20and%20image%20science.%20Traditional%20approaches%20tackle%20such%20a%20problem%20by%20performing%20tensor%20singular%20value%20decomposition%2C%20which%20is%20computationally%20expensive%20and%20becomes%20infeasible%20for%20large-scale%20tensors.%20Recent%20approaches%20address%20this%20issue%20by%20factorizing%20the%20tensor%20into%20two%20smaller%20factor%20tensors%20and%20solving%20the%20resulting%20problem%20using%20gradient%20descent.%20However%2C%20this%20kind%20of%20approach%20requires%20an%20accurate%20estimate%20of%20the%20tensor%20rank%2C%20and%20when%20the%20rank%20is%20overestimated%2C%20the%20convergence%20of%20gradient%20descent%20and%20its%20variants%20slows%20down%20significantly%20or%20even%20diverges.%20To%20address%20this%20problem%2C%20we%20propose%20an%20Alternating%20Preconditioned%20Gradient%20Descent%20%28APGD%29%20algorithm%2C%20which%20accelerates%20convergence%20in%20the%20over-parameterized%20setting%20by%20adding%20a%20preconditioning%20term%20to%20the%20original%20gradient%20and%20updating%20these%20two%20factors%20alternately.%20Based%20on%20certain%20geometric%20assumptions%20on%20the%20objective%20function%2C%20we%20establish%20linear%20convergence%20guarantees%20for%20more%20general%20low-tubal-rank%20tensor%20estimation%20problems.%20Then%20we%20further%20analyze%20the%20specific%20cases%20of%20low-tubal-rank%20tensor%20factorization%20and%20low-tubal-rank%20tensor%20recovery.%20Our%20theoretical%20results%20show%20that%20APGD%20achieves%20linear%20convergence%20even%20under%20over-parameterization%2C%20and%20the%20convergence%20rate%20is%20independent%20of%20the%20tensor%20condition%20number.%20Extensive%20simulations%20on%20synthetic%20data%20are%20carried%20out%20to%20validate%20our%20theoretical%20assertions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Low-Tubal-Rank%2520Tensor%2520Estimation%2520via%2520Alternating%2520Preconditioned%2520Gradient%2520Descent%26entry.906535625%3DZhiyu%2520Liu%2520and%2520Zhi%2520Han%2520and%2520Yandong%2520Tang%2520and%2520Jun%2520Fan%2520and%2520Yao%2520Wang%26entry.1292438233%3DThe%2520problem%2520of%2520low-tubal-rank%2520tensor%2520estimation%2520is%2520a%2520fundamental%2520task%2520with%2520wide%2520applications%2520across%2520high-dimensional%2520signal%2520processing%252C%2520machine%2520learning%252C%2520and%2520image%2520science.%2520Traditional%2520approaches%2520tackle%2520such%2520a%2520problem%2520by%2520performing%2520tensor%2520singular%2520value%2520decomposition%252C%2520which%2520is%2520computationally%2520expensive%2520and%2520becomes%2520infeasible%2520for%2520large-scale%2520tensors.%2520Recent%2520approaches%2520address%2520this%2520issue%2520by%2520factorizing%2520the%2520tensor%2520into%2520two%2520smaller%2520factor%2520tensors%2520and%2520solving%2520the%2520resulting%2520problem%2520using%2520gradient%2520descent.%2520However%252C%2520this%2520kind%2520of%2520approach%2520requires%2520an%2520accurate%2520estimate%2520of%2520the%2520tensor%2520rank%252C%2520and%2520when%2520the%2520rank%2520is%2520overestimated%252C%2520the%2520convergence%2520of%2520gradient%2520descent%2520and%2520its%2520variants%2520slows%2520down%2520significantly%2520or%2520even%2520diverges.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520an%2520Alternating%2520Preconditioned%2520Gradient%2520Descent%2520%2528APGD%2529%2520algorithm%252C%2520which%2520accelerates%2520convergence%2520in%2520the%2520over-parameterized%2520setting%2520by%2520adding%2520a%2520preconditioning%2520term%2520to%2520the%2520original%2520gradient%2520and%2520updating%2520these%2520two%2520factors%2520alternately.%2520Based%2520on%2520certain%2520geometric%2520assumptions%2520on%2520the%2520objective%2520function%252C%2520we%2520establish%2520linear%2520convergence%2520guarantees%2520for%2520more%2520general%2520low-tubal-rank%2520tensor%2520estimation%2520problems.%2520Then%2520we%2520further%2520analyze%2520the%2520specific%2520cases%2520of%2520low-tubal-rank%2520tensor%2520factorization%2520and%2520low-tubal-rank%2520tensor%2520recovery.%2520Our%2520theoretical%2520results%2520show%2520that%2520APGD%2520achieves%2520linear%2520convergence%2520even%2520under%2520over-parameterization%252C%2520and%2520the%2520convergence%2520rate%2520is%2520independent%2520of%2520the%2520tensor%2520condition%2520number.%2520Extensive%2520simulations%2520on%2520synthetic%2520data%2520are%2520carried%2520out%2520to%2520validate%2520our%2520theoretical%2520assertions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Low-Tubal-Rank%20Tensor%20Estimation%20via%20Alternating%20Preconditioned%20Gradient%20Descent&entry.906535625=Zhiyu%20Liu%20and%20Zhi%20Han%20and%20Yandong%20Tang%20and%20Jun%20Fan%20and%20Yao%20Wang&entry.1292438233=The%20problem%20of%20low-tubal-rank%20tensor%20estimation%20is%20a%20fundamental%20task%20with%20wide%20applications%20across%20high-dimensional%20signal%20processing%2C%20machine%20learning%2C%20and%20image%20science.%20Traditional%20approaches%20tackle%20such%20a%20problem%20by%20performing%20tensor%20singular%20value%20decomposition%2C%20which%20is%20computationally%20expensive%20and%20becomes%20infeasible%20for%20large-scale%20tensors.%20Recent%20approaches%20address%20this%20issue%20by%20factorizing%20the%20tensor%20into%20two%20smaller%20factor%20tensors%20and%20solving%20the%20resulting%20problem%20using%20gradient%20descent.%20However%2C%20this%20kind%20of%20approach%20requires%20an%20accurate%20estimate%20of%20the%20tensor%20rank%2C%20and%20when%20the%20rank%20is%20overestimated%2C%20the%20convergence%20of%20gradient%20descent%20and%20its%20variants%20slows%20down%20significantly%20or%20even%20diverges.%20To%20address%20this%20problem%2C%20we%20propose%20an%20Alternating%20Preconditioned%20Gradient%20Descent%20%28APGD%29%20algorithm%2C%20which%20accelerates%20convergence%20in%20the%20over-parameterized%20setting%20by%20adding%20a%20preconditioning%20term%20to%20the%20original%20gradient%20and%20updating%20these%20two%20factors%20alternately.%20Based%20on%20certain%20geometric%20assumptions%20on%20the%20objective%20function%2C%20we%20establish%20linear%20convergence%20guarantees%20for%20more%20general%20low-tubal-rank%20tensor%20estimation%20problems.%20Then%20we%20further%20analyze%20the%20specific%20cases%20of%20low-tubal-rank%20tensor%20factorization%20and%20low-tubal-rank%20tensor%20recovery.%20Our%20theoretical%20results%20show%20that%20APGD%20achieves%20linear%20convergence%20even%20under%20over-parameterization%2C%20and%20the%20convergence%20rate%20is%20independent%20of%20the%20tensor%20condition%20number.%20Extensive%20simulations%20on%20synthetic%20data%20are%20carried%20out%20to%20validate%20our%20theoretical%20assertions.&entry.1838667208=http%3A//arxiv.org/abs/2512.07490v1&entry.124074799=Read"},
{"title": "Provable Long-Range Benefits of Next-Token Prediction", "author": "Xinyuan Cao and Santosh S. Vempala", "abstract": "Why do modern language models, trained to do well on next-word prediction, appear to generate coherent documents and capture long-range structure? Here we show that next-token prediction is provably powerful for learning longer-range structure, even with common neural network architectures. Specifically, we prove that optimizing next-token prediction over a Recurrent Neural Network (RNN) yields a model that closely approximates the training distribution: for held-out documents sampled from the training distribution, no algorithm of bounded description length limited to examining the next $k$ tokens, for any $k$, can distinguish between $k$ consecutive tokens of such documents and $k$ tokens generated by the learned language model following the same prefix. We provide polynomial bounds (in $k$, independent of the document length) on the model size needed to achieve such $k$-token indistinguishability, offering a complexity-theoretic explanation for the long-range coherence observed in practice.", "link": "http://arxiv.org/abs/2512.07818v1", "date": "2025-12-08", "relevancy": 2.3665, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4919}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4699}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provable%20Long-Range%20Benefits%20of%20Next-Token%20Prediction&body=Title%3A%20Provable%20Long-Range%20Benefits%20of%20Next-Token%20Prediction%0AAuthor%3A%20Xinyuan%20Cao%20and%20Santosh%20S.%20Vempala%0AAbstract%3A%20Why%20do%20modern%20language%20models%2C%20trained%20to%20do%20well%20on%20next-word%20prediction%2C%20appear%20to%20generate%20coherent%20documents%20and%20capture%20long-range%20structure%3F%20Here%20we%20show%20that%20next-token%20prediction%20is%20provably%20powerful%20for%20learning%20longer-range%20structure%2C%20even%20with%20common%20neural%20network%20architectures.%20Specifically%2C%20we%20prove%20that%20optimizing%20next-token%20prediction%20over%20a%20Recurrent%20Neural%20Network%20%28RNN%29%20yields%20a%20model%20that%20closely%20approximates%20the%20training%20distribution%3A%20for%20held-out%20documents%20sampled%20from%20the%20training%20distribution%2C%20no%20algorithm%20of%20bounded%20description%20length%20limited%20to%20examining%20the%20next%20%24k%24%20tokens%2C%20for%20any%20%24k%24%2C%20can%20distinguish%20between%20%24k%24%20consecutive%20tokens%20of%20such%20documents%20and%20%24k%24%20tokens%20generated%20by%20the%20learned%20language%20model%20following%20the%20same%20prefix.%20We%20provide%20polynomial%20bounds%20%28in%20%24k%24%2C%20independent%20of%20the%20document%20length%29%20on%20the%20model%20size%20needed%20to%20achieve%20such%20%24k%24-token%20indistinguishability%2C%20offering%20a%20complexity-theoretic%20explanation%20for%20the%20long-range%20coherence%20observed%20in%20practice.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvable%2520Long-Range%2520Benefits%2520of%2520Next-Token%2520Prediction%26entry.906535625%3DXinyuan%2520Cao%2520and%2520Santosh%2520S.%2520Vempala%26entry.1292438233%3DWhy%2520do%2520modern%2520language%2520models%252C%2520trained%2520to%2520do%2520well%2520on%2520next-word%2520prediction%252C%2520appear%2520to%2520generate%2520coherent%2520documents%2520and%2520capture%2520long-range%2520structure%253F%2520Here%2520we%2520show%2520that%2520next-token%2520prediction%2520is%2520provably%2520powerful%2520for%2520learning%2520longer-range%2520structure%252C%2520even%2520with%2520common%2520neural%2520network%2520architectures.%2520Specifically%252C%2520we%2520prove%2520that%2520optimizing%2520next-token%2520prediction%2520over%2520a%2520Recurrent%2520Neural%2520Network%2520%2528RNN%2529%2520yields%2520a%2520model%2520that%2520closely%2520approximates%2520the%2520training%2520distribution%253A%2520for%2520held-out%2520documents%2520sampled%2520from%2520the%2520training%2520distribution%252C%2520no%2520algorithm%2520of%2520bounded%2520description%2520length%2520limited%2520to%2520examining%2520the%2520next%2520%2524k%2524%2520tokens%252C%2520for%2520any%2520%2524k%2524%252C%2520can%2520distinguish%2520between%2520%2524k%2524%2520consecutive%2520tokens%2520of%2520such%2520documents%2520and%2520%2524k%2524%2520tokens%2520generated%2520by%2520the%2520learned%2520language%2520model%2520following%2520the%2520same%2520prefix.%2520We%2520provide%2520polynomial%2520bounds%2520%2528in%2520%2524k%2524%252C%2520independent%2520of%2520the%2520document%2520length%2529%2520on%2520the%2520model%2520size%2520needed%2520to%2520achieve%2520such%2520%2524k%2524-token%2520indistinguishability%252C%2520offering%2520a%2520complexity-theoretic%2520explanation%2520for%2520the%2520long-range%2520coherence%2520observed%2520in%2520practice.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provable%20Long-Range%20Benefits%20of%20Next-Token%20Prediction&entry.906535625=Xinyuan%20Cao%20and%20Santosh%20S.%20Vempala&entry.1292438233=Why%20do%20modern%20language%20models%2C%20trained%20to%20do%20well%20on%20next-word%20prediction%2C%20appear%20to%20generate%20coherent%20documents%20and%20capture%20long-range%20structure%3F%20Here%20we%20show%20that%20next-token%20prediction%20is%20provably%20powerful%20for%20learning%20longer-range%20structure%2C%20even%20with%20common%20neural%20network%20architectures.%20Specifically%2C%20we%20prove%20that%20optimizing%20next-token%20prediction%20over%20a%20Recurrent%20Neural%20Network%20%28RNN%29%20yields%20a%20model%20that%20closely%20approximates%20the%20training%20distribution%3A%20for%20held-out%20documents%20sampled%20from%20the%20training%20distribution%2C%20no%20algorithm%20of%20bounded%20description%20length%20limited%20to%20examining%20the%20next%20%24k%24%20tokens%2C%20for%20any%20%24k%24%2C%20can%20distinguish%20between%20%24k%24%20consecutive%20tokens%20of%20such%20documents%20and%20%24k%24%20tokens%20generated%20by%20the%20learned%20language%20model%20following%20the%20same%20prefix.%20We%20provide%20polynomial%20bounds%20%28in%20%24k%24%2C%20independent%20of%20the%20document%20length%29%20on%20the%20model%20size%20needed%20to%20achieve%20such%20%24k%24-token%20indistinguishability%2C%20offering%20a%20complexity-theoretic%20explanation%20for%20the%20long-range%20coherence%20observed%20in%20practice.&entry.1838667208=http%3A//arxiv.org/abs/2512.07818v1&entry.124074799=Read"},
{"title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory", "author": "Zhaochong An and Menglin Jia and Haonan Qiu and Zijian Zhou and Xiaoke Huang and Zhiheng Liu and Weiming Ren and Kumara Kahatapitiya and Ding Liu and Sen He and Chenyang Zhang and Tao Xiang and Fanny Yang and Serge Belongie and Tian Xie", "abstract": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.", "link": "http://arxiv.org/abs/2512.07802v1", "date": "2025-12-08", "relevancy": 2.3641, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.608}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5885}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OneStory%3A%20Coherent%20Multi-Shot%20Video%20Generation%20with%20Adaptive%20Memory&body=Title%3A%20OneStory%3A%20Coherent%20Multi-Shot%20Video%20Generation%20with%20Adaptive%20Memory%0AAuthor%3A%20Zhaochong%20An%20and%20Menglin%20Jia%20and%20Haonan%20Qiu%20and%20Zijian%20Zhou%20and%20Xiaoke%20Huang%20and%20Zhiheng%20Liu%20and%20Weiming%20Ren%20and%20Kumara%20Kahatapitiya%20and%20Ding%20Liu%20and%20Sen%20He%20and%20Chenyang%20Zhang%20and%20Tao%20Xiang%20and%20Fanny%20Yang%20and%20Serge%20Belongie%20and%20Tian%20Xie%0AAbstract%3A%20Storytelling%20in%20real-world%20videos%20often%20unfolds%20through%20multiple%20shots%20--%20discontinuous%20yet%20semantically%20connected%20clips%20that%20together%20convey%20a%20coherent%20narrative.%20However%2C%20existing%20multi-shot%20video%20generation%20%28MSV%29%20methods%20struggle%20to%20effectively%20model%20long-range%20cross-shot%20context%2C%20as%20they%20rely%20on%20limited%20temporal%20windows%20or%20single%20keyframe%20conditioning%2C%20leading%20to%20degraded%20performance%20under%20complex%20narratives.%20In%20this%20work%2C%20we%20propose%20OneStory%2C%20enabling%20global%20yet%20compact%20cross-shot%20context%20modeling%20for%20consistent%20and%20scalable%20narrative%20generation.%20OneStory%20reformulates%20MSV%20as%20a%20next-shot%20generation%20task%2C%20enabling%20autoregressive%20shot%20synthesis%20while%20leveraging%20pretrained%20image-to-video%20%28I2V%29%20models%20for%20strong%20visual%20conditioning.%20We%20introduce%20two%20key%20modules%3A%20a%20Frame%20Selection%20module%20that%20constructs%20a%20semantically-relevant%20global%20memory%20based%20on%20informative%20frames%20from%20prior%20shots%2C%20and%20an%20Adaptive%20Conditioner%20that%20performs%20importance-guided%20patchification%20to%20generate%20compact%20context%20for%20direct%20conditioning.%20We%20further%20curate%20a%20high-quality%20multi-shot%20dataset%20with%20referential%20captions%20to%20mirror%20real-world%20storytelling%20patterns%2C%20and%20design%20effective%20training%20strategies%20under%20the%20next-shot%20paradigm.%20Finetuned%20from%20a%20pretrained%20I2V%20model%20on%20our%20curated%2060K%20dataset%2C%20OneStory%20achieves%20state-of-the-art%20narrative%20coherence%20across%20diverse%20and%20complex%20scenes%20in%20both%20text-%20and%20image-conditioned%20settings%2C%20enabling%20controllable%20and%20immersive%20long-form%20video%20storytelling.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOneStory%253A%2520Coherent%2520Multi-Shot%2520Video%2520Generation%2520with%2520Adaptive%2520Memory%26entry.906535625%3DZhaochong%2520An%2520and%2520Menglin%2520Jia%2520and%2520Haonan%2520Qiu%2520and%2520Zijian%2520Zhou%2520and%2520Xiaoke%2520Huang%2520and%2520Zhiheng%2520Liu%2520and%2520Weiming%2520Ren%2520and%2520Kumara%2520Kahatapitiya%2520and%2520Ding%2520Liu%2520and%2520Sen%2520He%2520and%2520Chenyang%2520Zhang%2520and%2520Tao%2520Xiang%2520and%2520Fanny%2520Yang%2520and%2520Serge%2520Belongie%2520and%2520Tian%2520Xie%26entry.1292438233%3DStorytelling%2520in%2520real-world%2520videos%2520often%2520unfolds%2520through%2520multiple%2520shots%2520--%2520discontinuous%2520yet%2520semantically%2520connected%2520clips%2520that%2520together%2520convey%2520a%2520coherent%2520narrative.%2520However%252C%2520existing%2520multi-shot%2520video%2520generation%2520%2528MSV%2529%2520methods%2520struggle%2520to%2520effectively%2520model%2520long-range%2520cross-shot%2520context%252C%2520as%2520they%2520rely%2520on%2520limited%2520temporal%2520windows%2520or%2520single%2520keyframe%2520conditioning%252C%2520leading%2520to%2520degraded%2520performance%2520under%2520complex%2520narratives.%2520In%2520this%2520work%252C%2520we%2520propose%2520OneStory%252C%2520enabling%2520global%2520yet%2520compact%2520cross-shot%2520context%2520modeling%2520for%2520consistent%2520and%2520scalable%2520narrative%2520generation.%2520OneStory%2520reformulates%2520MSV%2520as%2520a%2520next-shot%2520generation%2520task%252C%2520enabling%2520autoregressive%2520shot%2520synthesis%2520while%2520leveraging%2520pretrained%2520image-to-video%2520%2528I2V%2529%2520models%2520for%2520strong%2520visual%2520conditioning.%2520We%2520introduce%2520two%2520key%2520modules%253A%2520a%2520Frame%2520Selection%2520module%2520that%2520constructs%2520a%2520semantically-relevant%2520global%2520memory%2520based%2520on%2520informative%2520frames%2520from%2520prior%2520shots%252C%2520and%2520an%2520Adaptive%2520Conditioner%2520that%2520performs%2520importance-guided%2520patchification%2520to%2520generate%2520compact%2520context%2520for%2520direct%2520conditioning.%2520We%2520further%2520curate%2520a%2520high-quality%2520multi-shot%2520dataset%2520with%2520referential%2520captions%2520to%2520mirror%2520real-world%2520storytelling%2520patterns%252C%2520and%2520design%2520effective%2520training%2520strategies%2520under%2520the%2520next-shot%2520paradigm.%2520Finetuned%2520from%2520a%2520pretrained%2520I2V%2520model%2520on%2520our%2520curated%252060K%2520dataset%252C%2520OneStory%2520achieves%2520state-of-the-art%2520narrative%2520coherence%2520across%2520diverse%2520and%2520complex%2520scenes%2520in%2520both%2520text-%2520and%2520image-conditioned%2520settings%252C%2520enabling%2520controllable%2520and%2520immersive%2520long-form%2520video%2520storytelling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneStory%3A%20Coherent%20Multi-Shot%20Video%20Generation%20with%20Adaptive%20Memory&entry.906535625=Zhaochong%20An%20and%20Menglin%20Jia%20and%20Haonan%20Qiu%20and%20Zijian%20Zhou%20and%20Xiaoke%20Huang%20and%20Zhiheng%20Liu%20and%20Weiming%20Ren%20and%20Kumara%20Kahatapitiya%20and%20Ding%20Liu%20and%20Sen%20He%20and%20Chenyang%20Zhang%20and%20Tao%20Xiang%20and%20Fanny%20Yang%20and%20Serge%20Belongie%20and%20Tian%20Xie&entry.1292438233=Storytelling%20in%20real-world%20videos%20often%20unfolds%20through%20multiple%20shots%20--%20discontinuous%20yet%20semantically%20connected%20clips%20that%20together%20convey%20a%20coherent%20narrative.%20However%2C%20existing%20multi-shot%20video%20generation%20%28MSV%29%20methods%20struggle%20to%20effectively%20model%20long-range%20cross-shot%20context%2C%20as%20they%20rely%20on%20limited%20temporal%20windows%20or%20single%20keyframe%20conditioning%2C%20leading%20to%20degraded%20performance%20under%20complex%20narratives.%20In%20this%20work%2C%20we%20propose%20OneStory%2C%20enabling%20global%20yet%20compact%20cross-shot%20context%20modeling%20for%20consistent%20and%20scalable%20narrative%20generation.%20OneStory%20reformulates%20MSV%20as%20a%20next-shot%20generation%20task%2C%20enabling%20autoregressive%20shot%20synthesis%20while%20leveraging%20pretrained%20image-to-video%20%28I2V%29%20models%20for%20strong%20visual%20conditioning.%20We%20introduce%20two%20key%20modules%3A%20a%20Frame%20Selection%20module%20that%20constructs%20a%20semantically-relevant%20global%20memory%20based%20on%20informative%20frames%20from%20prior%20shots%2C%20and%20an%20Adaptive%20Conditioner%20that%20performs%20importance-guided%20patchification%20to%20generate%20compact%20context%20for%20direct%20conditioning.%20We%20further%20curate%20a%20high-quality%20multi-shot%20dataset%20with%20referential%20captions%20to%20mirror%20real-world%20storytelling%20patterns%2C%20and%20design%20effective%20training%20strategies%20under%20the%20next-shot%20paradigm.%20Finetuned%20from%20a%20pretrained%20I2V%20model%20on%20our%20curated%2060K%20dataset%2C%20OneStory%20achieves%20state-of-the-art%20narrative%20coherence%20across%20diverse%20and%20complex%20scenes%20in%20both%20text-%20and%20image-conditioned%20settings%2C%20enabling%20controllable%20and%20immersive%20long-form%20video%20storytelling.&entry.1838667208=http%3A//arxiv.org/abs/2512.07802v1&entry.124074799=Read"},
{"title": "Efficient and Compliant Control Framework for Versatile Human-Humanoid Collaborative Transportation", "author": "Shubham S. Kumbhar and Abhijeet M. Kulkarni and Panagiotis Artemiadis", "abstract": "We present a control framework that enables humanoid robots to perform collaborative transportation tasks with a human partner. The framework supports both translational and rotational motions, which are fundamental to co-transport scenarios. It comprises three components: a high-level planner, a low-level controller, and a stiffness modulation mechanism. At the planning level, we introduce the Interaction Linear Inverted Pendulum (I-LIP), which, combined with an admittance model and an MPC formulation, generates dynamically feasible footstep plans. These are executed by a QP-based whole-body controller that accounts for the coupled humanoid-object dynamics. Stiffness modulation regulates robot-object interaction, ensuring convergence to the desired relative configuration defined by the distance between the object and the robot's center of mass. We validate the effectiveness of the framework through real-world experiments conducted on the Digit humanoid platform. To quantify collaboration quality, we propose an efficiency metric that captures both task performance and inter-agent coordination. We show that this metric highlights the role of compliance in collaborative tasks and offers insights into desirable trajectory characteristics across both high- and low-level control layers. Finally, we showcase experimental results on collaborative behaviors, including translation, turning, and combined motions such as semi circular trajectories, representative of naturally occurring co-transportation tasks.", "link": "http://arxiv.org/abs/2512.07819v1", "date": "2025-12-08", "relevancy": 2.3598, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6051}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5891}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Compliant%20Control%20Framework%20for%20Versatile%20Human-Humanoid%20Collaborative%20Transportation&body=Title%3A%20Efficient%20and%20Compliant%20Control%20Framework%20for%20Versatile%20Human-Humanoid%20Collaborative%20Transportation%0AAuthor%3A%20Shubham%20S.%20Kumbhar%20and%20Abhijeet%20M.%20Kulkarni%20and%20Panagiotis%20Artemiadis%0AAbstract%3A%20We%20present%20a%20control%20framework%20that%20enables%20humanoid%20robots%20to%20perform%20collaborative%20transportation%20tasks%20with%20a%20human%20partner.%20The%20framework%20supports%20both%20translational%20and%20rotational%20motions%2C%20which%20are%20fundamental%20to%20co-transport%20scenarios.%20It%20comprises%20three%20components%3A%20a%20high-level%20planner%2C%20a%20low-level%20controller%2C%20and%20a%20stiffness%20modulation%20mechanism.%20At%20the%20planning%20level%2C%20we%20introduce%20the%20Interaction%20Linear%20Inverted%20Pendulum%20%28I-LIP%29%2C%20which%2C%20combined%20with%20an%20admittance%20model%20and%20an%20MPC%20formulation%2C%20generates%20dynamically%20feasible%20footstep%20plans.%20These%20are%20executed%20by%20a%20QP-based%20whole-body%20controller%20that%20accounts%20for%20the%20coupled%20humanoid-object%20dynamics.%20Stiffness%20modulation%20regulates%20robot-object%20interaction%2C%20ensuring%20convergence%20to%20the%20desired%20relative%20configuration%20defined%20by%20the%20distance%20between%20the%20object%20and%20the%20robot%27s%20center%20of%20mass.%20We%20validate%20the%20effectiveness%20of%20the%20framework%20through%20real-world%20experiments%20conducted%20on%20the%20Digit%20humanoid%20platform.%20To%20quantify%20collaboration%20quality%2C%20we%20propose%20an%20efficiency%20metric%20that%20captures%20both%20task%20performance%20and%20inter-agent%20coordination.%20We%20show%20that%20this%20metric%20highlights%20the%20role%20of%20compliance%20in%20collaborative%20tasks%20and%20offers%20insights%20into%20desirable%20trajectory%20characteristics%20across%20both%20high-%20and%20low-level%20control%20layers.%20Finally%2C%20we%20showcase%20experimental%20results%20on%20collaborative%20behaviors%2C%20including%20translation%2C%20turning%2C%20and%20combined%20motions%20such%20as%20semi%20circular%20trajectories%2C%20representative%20of%20naturally%20occurring%20co-transportation%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520and%2520Compliant%2520Control%2520Framework%2520for%2520Versatile%2520Human-Humanoid%2520Collaborative%2520Transportation%26entry.906535625%3DShubham%2520S.%2520Kumbhar%2520and%2520Abhijeet%2520M.%2520Kulkarni%2520and%2520Panagiotis%2520Artemiadis%26entry.1292438233%3DWe%2520present%2520a%2520control%2520framework%2520that%2520enables%2520humanoid%2520robots%2520to%2520perform%2520collaborative%2520transportation%2520tasks%2520with%2520a%2520human%2520partner.%2520The%2520framework%2520supports%2520both%2520translational%2520and%2520rotational%2520motions%252C%2520which%2520are%2520fundamental%2520to%2520co-transport%2520scenarios.%2520It%2520comprises%2520three%2520components%253A%2520a%2520high-level%2520planner%252C%2520a%2520low-level%2520controller%252C%2520and%2520a%2520stiffness%2520modulation%2520mechanism.%2520At%2520the%2520planning%2520level%252C%2520we%2520introduce%2520the%2520Interaction%2520Linear%2520Inverted%2520Pendulum%2520%2528I-LIP%2529%252C%2520which%252C%2520combined%2520with%2520an%2520admittance%2520model%2520and%2520an%2520MPC%2520formulation%252C%2520generates%2520dynamically%2520feasible%2520footstep%2520plans.%2520These%2520are%2520executed%2520by%2520a%2520QP-based%2520whole-body%2520controller%2520that%2520accounts%2520for%2520the%2520coupled%2520humanoid-object%2520dynamics.%2520Stiffness%2520modulation%2520regulates%2520robot-object%2520interaction%252C%2520ensuring%2520convergence%2520to%2520the%2520desired%2520relative%2520configuration%2520defined%2520by%2520the%2520distance%2520between%2520the%2520object%2520and%2520the%2520robot%2527s%2520center%2520of%2520mass.%2520We%2520validate%2520the%2520effectiveness%2520of%2520the%2520framework%2520through%2520real-world%2520experiments%2520conducted%2520on%2520the%2520Digit%2520humanoid%2520platform.%2520To%2520quantify%2520collaboration%2520quality%252C%2520we%2520propose%2520an%2520efficiency%2520metric%2520that%2520captures%2520both%2520task%2520performance%2520and%2520inter-agent%2520coordination.%2520We%2520show%2520that%2520this%2520metric%2520highlights%2520the%2520role%2520of%2520compliance%2520in%2520collaborative%2520tasks%2520and%2520offers%2520insights%2520into%2520desirable%2520trajectory%2520characteristics%2520across%2520both%2520high-%2520and%2520low-level%2520control%2520layers.%2520Finally%252C%2520we%2520showcase%2520experimental%2520results%2520on%2520collaborative%2520behaviors%252C%2520including%2520translation%252C%2520turning%252C%2520and%2520combined%2520motions%2520such%2520as%2520semi%2520circular%2520trajectories%252C%2520representative%2520of%2520naturally%2520occurring%2520co-transportation%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Compliant%20Control%20Framework%20for%20Versatile%20Human-Humanoid%20Collaborative%20Transportation&entry.906535625=Shubham%20S.%20Kumbhar%20and%20Abhijeet%20M.%20Kulkarni%20and%20Panagiotis%20Artemiadis&entry.1292438233=We%20present%20a%20control%20framework%20that%20enables%20humanoid%20robots%20to%20perform%20collaborative%20transportation%20tasks%20with%20a%20human%20partner.%20The%20framework%20supports%20both%20translational%20and%20rotational%20motions%2C%20which%20are%20fundamental%20to%20co-transport%20scenarios.%20It%20comprises%20three%20components%3A%20a%20high-level%20planner%2C%20a%20low-level%20controller%2C%20and%20a%20stiffness%20modulation%20mechanism.%20At%20the%20planning%20level%2C%20we%20introduce%20the%20Interaction%20Linear%20Inverted%20Pendulum%20%28I-LIP%29%2C%20which%2C%20combined%20with%20an%20admittance%20model%20and%20an%20MPC%20formulation%2C%20generates%20dynamically%20feasible%20footstep%20plans.%20These%20are%20executed%20by%20a%20QP-based%20whole-body%20controller%20that%20accounts%20for%20the%20coupled%20humanoid-object%20dynamics.%20Stiffness%20modulation%20regulates%20robot-object%20interaction%2C%20ensuring%20convergence%20to%20the%20desired%20relative%20configuration%20defined%20by%20the%20distance%20between%20the%20object%20and%20the%20robot%27s%20center%20of%20mass.%20We%20validate%20the%20effectiveness%20of%20the%20framework%20through%20real-world%20experiments%20conducted%20on%20the%20Digit%20humanoid%20platform.%20To%20quantify%20collaboration%20quality%2C%20we%20propose%20an%20efficiency%20metric%20that%20captures%20both%20task%20performance%20and%20inter-agent%20coordination.%20We%20show%20that%20this%20metric%20highlights%20the%20role%20of%20compliance%20in%20collaborative%20tasks%20and%20offers%20insights%20into%20desirable%20trajectory%20characteristics%20across%20both%20high-%20and%20low-level%20control%20layers.%20Finally%2C%20we%20showcase%20experimental%20results%20on%20collaborative%20behaviors%2C%20including%20translation%2C%20turning%2C%20and%20combined%20motions%20such%20as%20semi%20circular%20trajectories%2C%20representative%20of%20naturally%20occurring%20co-transportation%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.07819v1&entry.124074799=Read"},
{"title": "LongCat-Image Technical Report", "author": " Meituan LongCat Team and Hanghang Ma and Haoxian Tan and Jiale Huang and Junqiang Wu and Jun-Yan He and Lishuai Gao and Songlin Xiao and Xiaoming Wei and Xiaoqi Ma and Xunliang Cai and Yayong Guan and Jie Hu", "abstract": "We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.", "link": "http://arxiv.org/abs/2512.07584v1", "date": "2025-12-08", "relevancy": 2.3575, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5936}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5936}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5685}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongCat-Image%20Technical%20Report&body=Title%3A%20LongCat-Image%20Technical%20Report%0AAuthor%3A%20%20Meituan%20LongCat%20Team%20and%20Hanghang%20Ma%20and%20Haoxian%20Tan%20and%20Jiale%20Huang%20and%20Junqiang%20Wu%20and%20Jun-Yan%20He%20and%20Lishuai%20Gao%20and%20Songlin%20Xiao%20and%20Xiaoming%20Wei%20and%20Xiaoqi%20Ma%20and%20Xunliang%20Cai%20and%20Yayong%20Guan%20and%20Jie%20Hu%0AAbstract%3A%20We%20introduce%20LongCat-Image%2C%20a%20pioneering%20open-source%20and%20bilingual%20%28Chinese-English%29%20foundation%20model%20for%20image%20generation%2C%20designed%20to%20address%20core%20challenges%20in%20multilingual%20text%20rendering%2C%20photorealism%2C%20deployment%20efficiency%2C%20and%20developer%20accessibility%20prevalent%20in%20current%20leading%20models.%201%29%20We%20achieve%20this%20through%20rigorous%20data%20curation%20strategies%20across%20the%20pre-training%2C%20mid-training%2C%20and%20SFT%20stages%2C%20complemented%20by%20the%20coordinated%20use%20of%20curated%20reward%20models%20during%20the%20RL%20phase.%20This%20strategy%20establishes%20the%20model%20as%20a%20new%20state-of-the-art%20%28SOTA%29%2C%20delivering%20superior%20text-rendering%20capabilities%20and%20remarkable%20photorealism%2C%20and%20significantly%20enhancing%20aesthetic%20quality.%202%29%20Notably%2C%20it%20sets%20a%20new%20industry%20standard%20for%20Chinese%20character%20rendering.%20By%20supporting%20even%20complex%20and%20rare%20characters%2C%20it%20outperforms%20both%20major%20open-source%20and%20commercial%20solutions%20in%20coverage%2C%20while%20also%20achieving%20superior%20accuracy.%203%29%20The%20model%20achieves%20remarkable%20efficiency%20through%20its%20compact%20design.%20With%20a%20core%20diffusion%20model%20of%20only%206B%20parameters%2C%20it%20is%20significantly%20smaller%20than%20the%20nearly%2020B%20or%20larger%20Mixture-of-Experts%20%28MoE%29%20architectures%20common%20in%20the%20field.%20This%20ensures%20minimal%20VRAM%20usage%20and%20rapid%20inference%2C%20significantly%20reducing%20deployment%20costs.%20Beyond%20generation%2C%20LongCat-Image%20also%20excels%20in%20image%20editing%2C%20achieving%20SOTA%20results%20on%20standard%20benchmarks%20with%20superior%20editing%20consistency%20compared%20to%20other%20open-source%20works.%204%29%20To%20fully%20empower%20the%20community%2C%20we%20have%20established%20the%20most%20comprehensive%20open-source%20ecosystem%20to%20date.%20We%20are%20releasing%20not%20only%20multiple%20model%20versions%20for%20text-to-image%20and%20image%20editing%2C%20including%20checkpoints%20after%20mid-training%20and%20post-training%20stages%2C%20but%20also%20the%20entire%20toolchain%20of%20training%20procedure.%20We%20believe%20that%20the%20openness%20of%20LongCat-Image%20will%20provide%20robust%20support%20for%20developers%20and%20researchers%2C%20pushing%20the%20frontiers%20of%20visual%20content%20creation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongCat-Image%2520Technical%2520Report%26entry.906535625%3D%2520Meituan%2520LongCat%2520Team%2520and%2520Hanghang%2520Ma%2520and%2520Haoxian%2520Tan%2520and%2520Jiale%2520Huang%2520and%2520Junqiang%2520Wu%2520and%2520Jun-Yan%2520He%2520and%2520Lishuai%2520Gao%2520and%2520Songlin%2520Xiao%2520and%2520Xiaoming%2520Wei%2520and%2520Xiaoqi%2520Ma%2520and%2520Xunliang%2520Cai%2520and%2520Yayong%2520Guan%2520and%2520Jie%2520Hu%26entry.1292438233%3DWe%2520introduce%2520LongCat-Image%252C%2520a%2520pioneering%2520open-source%2520and%2520bilingual%2520%2528Chinese-English%2529%2520foundation%2520model%2520for%2520image%2520generation%252C%2520designed%2520to%2520address%2520core%2520challenges%2520in%2520multilingual%2520text%2520rendering%252C%2520photorealism%252C%2520deployment%2520efficiency%252C%2520and%2520developer%2520accessibility%2520prevalent%2520in%2520current%2520leading%2520models.%25201%2529%2520We%2520achieve%2520this%2520through%2520rigorous%2520data%2520curation%2520strategies%2520across%2520the%2520pre-training%252C%2520mid-training%252C%2520and%2520SFT%2520stages%252C%2520complemented%2520by%2520the%2520coordinated%2520use%2520of%2520curated%2520reward%2520models%2520during%2520the%2520RL%2520phase.%2520This%2520strategy%2520establishes%2520the%2520model%2520as%2520a%2520new%2520state-of-the-art%2520%2528SOTA%2529%252C%2520delivering%2520superior%2520text-rendering%2520capabilities%2520and%2520remarkable%2520photorealism%252C%2520and%2520significantly%2520enhancing%2520aesthetic%2520quality.%25202%2529%2520Notably%252C%2520it%2520sets%2520a%2520new%2520industry%2520standard%2520for%2520Chinese%2520character%2520rendering.%2520By%2520supporting%2520even%2520complex%2520and%2520rare%2520characters%252C%2520it%2520outperforms%2520both%2520major%2520open-source%2520and%2520commercial%2520solutions%2520in%2520coverage%252C%2520while%2520also%2520achieving%2520superior%2520accuracy.%25203%2529%2520The%2520model%2520achieves%2520remarkable%2520efficiency%2520through%2520its%2520compact%2520design.%2520With%2520a%2520core%2520diffusion%2520model%2520of%2520only%25206B%2520parameters%252C%2520it%2520is%2520significantly%2520smaller%2520than%2520the%2520nearly%252020B%2520or%2520larger%2520Mixture-of-Experts%2520%2528MoE%2529%2520architectures%2520common%2520in%2520the%2520field.%2520This%2520ensures%2520minimal%2520VRAM%2520usage%2520and%2520rapid%2520inference%252C%2520significantly%2520reducing%2520deployment%2520costs.%2520Beyond%2520generation%252C%2520LongCat-Image%2520also%2520excels%2520in%2520image%2520editing%252C%2520achieving%2520SOTA%2520results%2520on%2520standard%2520benchmarks%2520with%2520superior%2520editing%2520consistency%2520compared%2520to%2520other%2520open-source%2520works.%25204%2529%2520To%2520fully%2520empower%2520the%2520community%252C%2520we%2520have%2520established%2520the%2520most%2520comprehensive%2520open-source%2520ecosystem%2520to%2520date.%2520We%2520are%2520releasing%2520not%2520only%2520multiple%2520model%2520versions%2520for%2520text-to-image%2520and%2520image%2520editing%252C%2520including%2520checkpoints%2520after%2520mid-training%2520and%2520post-training%2520stages%252C%2520but%2520also%2520the%2520entire%2520toolchain%2520of%2520training%2520procedure.%2520We%2520believe%2520that%2520the%2520openness%2520of%2520LongCat-Image%2520will%2520provide%2520robust%2520support%2520for%2520developers%2520and%2520researchers%252C%2520pushing%2520the%2520frontiers%2520of%2520visual%2520content%2520creation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongCat-Image%20Technical%20Report&entry.906535625=%20Meituan%20LongCat%20Team%20and%20Hanghang%20Ma%20and%20Haoxian%20Tan%20and%20Jiale%20Huang%20and%20Junqiang%20Wu%20and%20Jun-Yan%20He%20and%20Lishuai%20Gao%20and%20Songlin%20Xiao%20and%20Xiaoming%20Wei%20and%20Xiaoqi%20Ma%20and%20Xunliang%20Cai%20and%20Yayong%20Guan%20and%20Jie%20Hu&entry.1292438233=We%20introduce%20LongCat-Image%2C%20a%20pioneering%20open-source%20and%20bilingual%20%28Chinese-English%29%20foundation%20model%20for%20image%20generation%2C%20designed%20to%20address%20core%20challenges%20in%20multilingual%20text%20rendering%2C%20photorealism%2C%20deployment%20efficiency%2C%20and%20developer%20accessibility%20prevalent%20in%20current%20leading%20models.%201%29%20We%20achieve%20this%20through%20rigorous%20data%20curation%20strategies%20across%20the%20pre-training%2C%20mid-training%2C%20and%20SFT%20stages%2C%20complemented%20by%20the%20coordinated%20use%20of%20curated%20reward%20models%20during%20the%20RL%20phase.%20This%20strategy%20establishes%20the%20model%20as%20a%20new%20state-of-the-art%20%28SOTA%29%2C%20delivering%20superior%20text-rendering%20capabilities%20and%20remarkable%20photorealism%2C%20and%20significantly%20enhancing%20aesthetic%20quality.%202%29%20Notably%2C%20it%20sets%20a%20new%20industry%20standard%20for%20Chinese%20character%20rendering.%20By%20supporting%20even%20complex%20and%20rare%20characters%2C%20it%20outperforms%20both%20major%20open-source%20and%20commercial%20solutions%20in%20coverage%2C%20while%20also%20achieving%20superior%20accuracy.%203%29%20The%20model%20achieves%20remarkable%20efficiency%20through%20its%20compact%20design.%20With%20a%20core%20diffusion%20model%20of%20only%206B%20parameters%2C%20it%20is%20significantly%20smaller%20than%20the%20nearly%2020B%20or%20larger%20Mixture-of-Experts%20%28MoE%29%20architectures%20common%20in%20the%20field.%20This%20ensures%20minimal%20VRAM%20usage%20and%20rapid%20inference%2C%20significantly%20reducing%20deployment%20costs.%20Beyond%20generation%2C%20LongCat-Image%20also%20excels%20in%20image%20editing%2C%20achieving%20SOTA%20results%20on%20standard%20benchmarks%20with%20superior%20editing%20consistency%20compared%20to%20other%20open-source%20works.%204%29%20To%20fully%20empower%20the%20community%2C%20we%20have%20established%20the%20most%20comprehensive%20open-source%20ecosystem%20to%20date.%20We%20are%20releasing%20not%20only%20multiple%20model%20versions%20for%20text-to-image%20and%20image%20editing%2C%20including%20checkpoints%20after%20mid-training%20and%20post-training%20stages%2C%20but%20also%20the%20entire%20toolchain%20of%20training%20procedure.%20We%20believe%20that%20the%20openness%20of%20LongCat-Image%20will%20provide%20robust%20support%20for%20developers%20and%20researchers%2C%20pushing%20the%20frontiers%20of%20visual%20content%20creation.&entry.1838667208=http%3A//arxiv.org/abs/2512.07584v1&entry.124074799=Read"},
{"title": "Weighted Contrastive Learning for Anomaly-Aware Time-Series Forecasting", "author": "Joel Ekstrand and Tor Mattsson and Zahra Taghiyarrenani and Slawomir Nowaczyk and Jens Lundstr\u00f6m and Mikael Lind\u00e9n", "abstract": "Reliable forecasting of multivariate time series under anomalous conditions is crucial in applications such as ATM cash logistics, where sudden demand shifts can disrupt operations. Modern deep forecasters achieve high accuracy on normal data but often fail when distribution shifts occur. We propose Weighted Contrastive Adaptation (WECA), a Weighted contrastive objective that aligns normal and anomaly-augmented representations, preserving anomaly-relevant information while maintaining consistency under benign variations. Evaluations on a nationwide ATM transaction dataset with domain-informed anomaly injection show that WECA improves SMAPE on anomaly-affected data by 6.1 percentage points compared to a normally trained baseline, with negligible degradation on normal data. These results demonstrate that WECA enhances forecasting reliability under anomalies without sacrificing performance during regular operations.", "link": "http://arxiv.org/abs/2512.07569v1", "date": "2025-12-08", "relevancy": 2.3511, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4823}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.466}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weighted%20Contrastive%20Learning%20for%20Anomaly-Aware%20Time-Series%20Forecasting&body=Title%3A%20Weighted%20Contrastive%20Learning%20for%20Anomaly-Aware%20Time-Series%20Forecasting%0AAuthor%3A%20Joel%20Ekstrand%20and%20Tor%20Mattsson%20and%20Zahra%20Taghiyarrenani%20and%20Slawomir%20Nowaczyk%20and%20Jens%20Lundstr%C3%B6m%20and%20Mikael%20Lind%C3%A9n%0AAbstract%3A%20Reliable%20forecasting%20of%20multivariate%20time%20series%20under%20anomalous%20conditions%20is%20crucial%20in%20applications%20such%20as%20ATM%20cash%20logistics%2C%20where%20sudden%20demand%20shifts%20can%20disrupt%20operations.%20Modern%20deep%20forecasters%20achieve%20high%20accuracy%20on%20normal%20data%20but%20often%20fail%20when%20distribution%20shifts%20occur.%20We%20propose%20Weighted%20Contrastive%20Adaptation%20%28WECA%29%2C%20a%20Weighted%20contrastive%20objective%20that%20aligns%20normal%20and%20anomaly-augmented%20representations%2C%20preserving%20anomaly-relevant%20information%20while%20maintaining%20consistency%20under%20benign%20variations.%20Evaluations%20on%20a%20nationwide%20ATM%20transaction%20dataset%20with%20domain-informed%20anomaly%20injection%20show%20that%20WECA%20improves%20SMAPE%20on%20anomaly-affected%20data%20by%206.1%20percentage%20points%20compared%20to%20a%20normally%20trained%20baseline%2C%20with%20negligible%20degradation%20on%20normal%20data.%20These%20results%20demonstrate%20that%20WECA%20enhances%20forecasting%20reliability%20under%20anomalies%20without%20sacrificing%20performance%20during%20regular%20operations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeighted%2520Contrastive%2520Learning%2520for%2520Anomaly-Aware%2520Time-Series%2520Forecasting%26entry.906535625%3DJoel%2520Ekstrand%2520and%2520Tor%2520Mattsson%2520and%2520Zahra%2520Taghiyarrenani%2520and%2520Slawomir%2520Nowaczyk%2520and%2520Jens%2520Lundstr%25C3%25B6m%2520and%2520Mikael%2520Lind%25C3%25A9n%26entry.1292438233%3DReliable%2520forecasting%2520of%2520multivariate%2520time%2520series%2520under%2520anomalous%2520conditions%2520is%2520crucial%2520in%2520applications%2520such%2520as%2520ATM%2520cash%2520logistics%252C%2520where%2520sudden%2520demand%2520shifts%2520can%2520disrupt%2520operations.%2520Modern%2520deep%2520forecasters%2520achieve%2520high%2520accuracy%2520on%2520normal%2520data%2520but%2520often%2520fail%2520when%2520distribution%2520shifts%2520occur.%2520We%2520propose%2520Weighted%2520Contrastive%2520Adaptation%2520%2528WECA%2529%252C%2520a%2520Weighted%2520contrastive%2520objective%2520that%2520aligns%2520normal%2520and%2520anomaly-augmented%2520representations%252C%2520preserving%2520anomaly-relevant%2520information%2520while%2520maintaining%2520consistency%2520under%2520benign%2520variations.%2520Evaluations%2520on%2520a%2520nationwide%2520ATM%2520transaction%2520dataset%2520with%2520domain-informed%2520anomaly%2520injection%2520show%2520that%2520WECA%2520improves%2520SMAPE%2520on%2520anomaly-affected%2520data%2520by%25206.1%2520percentage%2520points%2520compared%2520to%2520a%2520normally%2520trained%2520baseline%252C%2520with%2520negligible%2520degradation%2520on%2520normal%2520data.%2520These%2520results%2520demonstrate%2520that%2520WECA%2520enhances%2520forecasting%2520reliability%2520under%2520anomalies%2520without%2520sacrificing%2520performance%2520during%2520regular%2520operations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weighted%20Contrastive%20Learning%20for%20Anomaly-Aware%20Time-Series%20Forecasting&entry.906535625=Joel%20Ekstrand%20and%20Tor%20Mattsson%20and%20Zahra%20Taghiyarrenani%20and%20Slawomir%20Nowaczyk%20and%20Jens%20Lundstr%C3%B6m%20and%20Mikael%20Lind%C3%A9n&entry.1292438233=Reliable%20forecasting%20of%20multivariate%20time%20series%20under%20anomalous%20conditions%20is%20crucial%20in%20applications%20such%20as%20ATM%20cash%20logistics%2C%20where%20sudden%20demand%20shifts%20can%20disrupt%20operations.%20Modern%20deep%20forecasters%20achieve%20high%20accuracy%20on%20normal%20data%20but%20often%20fail%20when%20distribution%20shifts%20occur.%20We%20propose%20Weighted%20Contrastive%20Adaptation%20%28WECA%29%2C%20a%20Weighted%20contrastive%20objective%20that%20aligns%20normal%20and%20anomaly-augmented%20representations%2C%20preserving%20anomaly-relevant%20information%20while%20maintaining%20consistency%20under%20benign%20variations.%20Evaluations%20on%20a%20nationwide%20ATM%20transaction%20dataset%20with%20domain-informed%20anomaly%20injection%20show%20that%20WECA%20improves%20SMAPE%20on%20anomaly-affected%20data%20by%206.1%20percentage%20points%20compared%20to%20a%20normally%20trained%20baseline%2C%20with%20negligible%20degradation%20on%20normal%20data.%20These%20results%20demonstrate%20that%20WECA%20enhances%20forecasting%20reliability%20under%20anomalies%20without%20sacrificing%20performance%20during%20regular%20operations.&entry.1838667208=http%3A//arxiv.org/abs/2512.07569v1&entry.124074799=Read"},
{"title": "sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only", "author": "Arslan Artykov and Corentin Sautier and Vincent Lepetit", "abstract": "Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: https://aartykov.github.io/sim2art/", "link": "http://arxiv.org/abs/2512.07698v1", "date": "2025-12-08", "relevancy": 2.35, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6136}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5909}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20sim2art%3A%20Accurate%20Articulated%20Object%20Modeling%20from%20a%20Single%20Video%20using%20Synthetic%20Training%20Data%20Only&body=Title%3A%20sim2art%3A%20Accurate%20Articulated%20Object%20Modeling%20from%20a%20Single%20Video%20using%20Synthetic%20Training%20Data%20Only%0AAuthor%3A%20Arslan%20Artykov%20and%20Corentin%20Sautier%20and%20Vincent%20Lepetit%0AAbstract%3A%20Understanding%20articulated%20objects%20is%20a%20fundamental%20challenge%20in%20robotics%20and%20digital%20twin%20creation.%20To%20effectively%20model%20such%20objects%2C%20it%20is%20essential%20to%20recover%20both%20part%20segmentation%20and%20the%20underlying%20joint%20parameters.%20Despite%20the%20importance%20of%20this%20task%2C%20previous%20work%20has%20largely%20focused%20on%20setups%20like%20multi-view%20systems%2C%20object%20scanning%2C%20or%20static%20cameras.%20In%20this%20paper%2C%20we%20present%20the%20first%20data-driven%20approach%20that%20jointly%20predicts%20part%20segmentation%20and%20joint%20parameters%20from%20monocular%20video%20captured%20with%20a%20freely%20moving%20camera.%20Trained%20solely%20on%20synthetic%20data%2C%20our%20method%20demonstrates%20strong%20generalization%20to%20real-world%20objects%2C%20offering%20a%20scalable%20and%20practical%20solution%20for%20articulated%20object%20understanding.%20Our%20approach%20operates%20directly%20on%20casually%20recorded%20video%2C%20making%20it%20suitable%20for%20real-time%20applications%20in%20dynamic%20environments.%20Project%20webpage%3A%20https%3A//aartykov.github.io/sim2art/%0ALink%3A%20http%3A//arxiv.org/abs/2512.07698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dsim2art%253A%2520Accurate%2520Articulated%2520Object%2520Modeling%2520from%2520a%2520Single%2520Video%2520using%2520Synthetic%2520Training%2520Data%2520Only%26entry.906535625%3DArslan%2520Artykov%2520and%2520Corentin%2520Sautier%2520and%2520Vincent%2520Lepetit%26entry.1292438233%3DUnderstanding%2520articulated%2520objects%2520is%2520a%2520fundamental%2520challenge%2520in%2520robotics%2520and%2520digital%2520twin%2520creation.%2520To%2520effectively%2520model%2520such%2520objects%252C%2520it%2520is%2520essential%2520to%2520recover%2520both%2520part%2520segmentation%2520and%2520the%2520underlying%2520joint%2520parameters.%2520Despite%2520the%2520importance%2520of%2520this%2520task%252C%2520previous%2520work%2520has%2520largely%2520focused%2520on%2520setups%2520like%2520multi-view%2520systems%252C%2520object%2520scanning%252C%2520or%2520static%2520cameras.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520first%2520data-driven%2520approach%2520that%2520jointly%2520predicts%2520part%2520segmentation%2520and%2520joint%2520parameters%2520from%2520monocular%2520video%2520captured%2520with%2520a%2520freely%2520moving%2520camera.%2520Trained%2520solely%2520on%2520synthetic%2520data%252C%2520our%2520method%2520demonstrates%2520strong%2520generalization%2520to%2520real-world%2520objects%252C%2520offering%2520a%2520scalable%2520and%2520practical%2520solution%2520for%2520articulated%2520object%2520understanding.%2520Our%2520approach%2520operates%2520directly%2520on%2520casually%2520recorded%2520video%252C%2520making%2520it%2520suitable%2520for%2520real-time%2520applications%2520in%2520dynamic%2520environments.%2520Project%2520webpage%253A%2520https%253A//aartykov.github.io/sim2art/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=sim2art%3A%20Accurate%20Articulated%20Object%20Modeling%20from%20a%20Single%20Video%20using%20Synthetic%20Training%20Data%20Only&entry.906535625=Arslan%20Artykov%20and%20Corentin%20Sautier%20and%20Vincent%20Lepetit&entry.1292438233=Understanding%20articulated%20objects%20is%20a%20fundamental%20challenge%20in%20robotics%20and%20digital%20twin%20creation.%20To%20effectively%20model%20such%20objects%2C%20it%20is%20essential%20to%20recover%20both%20part%20segmentation%20and%20the%20underlying%20joint%20parameters.%20Despite%20the%20importance%20of%20this%20task%2C%20previous%20work%20has%20largely%20focused%20on%20setups%20like%20multi-view%20systems%2C%20object%20scanning%2C%20or%20static%20cameras.%20In%20this%20paper%2C%20we%20present%20the%20first%20data-driven%20approach%20that%20jointly%20predicts%20part%20segmentation%20and%20joint%20parameters%20from%20monocular%20video%20captured%20with%20a%20freely%20moving%20camera.%20Trained%20solely%20on%20synthetic%20data%2C%20our%20method%20demonstrates%20strong%20generalization%20to%20real-world%20objects%2C%20offering%20a%20scalable%20and%20practical%20solution%20for%20articulated%20object%20understanding.%20Our%20approach%20operates%20directly%20on%20casually%20recorded%20video%2C%20making%20it%20suitable%20for%20real-time%20applications%20in%20dynamic%20environments.%20Project%20webpage%3A%20https%3A//aartykov.github.io/sim2art/&entry.1838667208=http%3A//arxiv.org/abs/2512.07698v1&entry.124074799=Read"},
{"title": "Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation", "author": "Shihao Zhao and Yitong Chen and Zeyinzi Jiang and Bojia Zi and Shaozhe Hao and Yu Liu and Chaojie Mao and Kwan-Yee K. Wong", "abstract": "Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.", "link": "http://arxiv.org/abs/2512.07747v1", "date": "2025-12-08", "relevancy": 2.339, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5903}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5855}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unison%3A%20A%20Fully%20Automatic%2C%20Task-Universal%2C%20and%20Low-Cost%20Framework%20for%20Unified%20Understanding%20and%20Generation&body=Title%3A%20Unison%3A%20A%20Fully%20Automatic%2C%20Task-Universal%2C%20and%20Low-Cost%20Framework%20for%20Unified%20Understanding%20and%20Generation%0AAuthor%3A%20Shihao%20Zhao%20and%20Yitong%20Chen%20and%20Zeyinzi%20Jiang%20and%20Bojia%20Zi%20and%20Shaozhe%20Hao%20and%20Yu%20Liu%20and%20Chaojie%20Mao%20and%20Kwan-Yee%20K.%20Wong%0AAbstract%3A%20Unified%20understanding%20and%20generation%20is%20a%20highly%20appealing%20research%20direction%20in%20multimodal%20learning.%20There%20exist%20two%20approaches%3A%20one%20trains%20a%20transformer%20via%20an%20auto-regressive%20paradigm%2C%20and%20the%20other%20adopts%20a%20two-stage%20scheme%20connecting%20pre-trained%20understanding%20and%20generative%20models%20for%20alignment%20fine-tuning.%20The%20former%20demands%20massive%20data%20and%20computing%20resources%20unaffordable%20for%20ordinary%20researchers.%20Though%20the%20latter%20requires%20a%20lower%20training%20cost%2C%20existing%20works%20often%20suffer%20from%20limited%20task%20coverage%20or%20poor%20generation%20quality.%20Both%20approaches%20lack%20the%20ability%20to%20parse%20input%20meta-information%20%28such%20as%20task%20type%2C%20image%20resolution%2C%20video%20duration%2C%20etc.%29%20and%20require%20manual%20parameter%20configuration%20that%20is%20tedious%20and%20non-intelligent.%20In%20this%20paper%2C%20we%20propose%20Unison%20which%20adopts%20the%20two-stage%20scheme%20while%20preserving%20the%20capabilities%20of%20the%20pre-trained%20models%20well.%20With%20an%20extremely%20low%20training%20cost%2C%20we%20cover%20a%20variety%20of%20multimodal%20understanding%20tasks%2C%20including%20text%2C%20image%2C%20and%20video%20understanding%2C%20as%20well%20as%20diverse%20generation%20tasks%2C%20such%20as%20text-to-visual%20content%20generation%2C%20editing%2C%20controllable%20generation%2C%20and%20IP-based%20reference%20generation.%20We%20also%20equip%20our%20model%20with%20the%20ability%20to%20automatically%20parse%20user%20intentions%2C%20determine%20the%20target%20task%20type%2C%20and%20accurately%20extract%20the%20meta-information%20required%20for%20the%20corresponding%20task.%20This%20enables%20full%20automation%20of%20various%20multimodal%20tasks%20without%20human%20intervention.%20Experiments%20demonstrate%20that%2C%20under%20a%20low-cost%20setting%20of%20only%20500k%20training%20samples%20and%2050%20GPU%20hours%2C%20our%20model%20can%20accurately%20and%20automatically%20identify%20tasks%20and%20extract%20relevant%20parameters%2C%20and%20achieve%20superior%20performance%20across%20a%20variety%20of%20understanding%20and%20generation%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnison%253A%2520A%2520Fully%2520Automatic%252C%2520Task-Universal%252C%2520and%2520Low-Cost%2520Framework%2520for%2520Unified%2520Understanding%2520and%2520Generation%26entry.906535625%3DShihao%2520Zhao%2520and%2520Yitong%2520Chen%2520and%2520Zeyinzi%2520Jiang%2520and%2520Bojia%2520Zi%2520and%2520Shaozhe%2520Hao%2520and%2520Yu%2520Liu%2520and%2520Chaojie%2520Mao%2520and%2520Kwan-Yee%2520K.%2520Wong%26entry.1292438233%3DUnified%2520understanding%2520and%2520generation%2520is%2520a%2520highly%2520appealing%2520research%2520direction%2520in%2520multimodal%2520learning.%2520There%2520exist%2520two%2520approaches%253A%2520one%2520trains%2520a%2520transformer%2520via%2520an%2520auto-regressive%2520paradigm%252C%2520and%2520the%2520other%2520adopts%2520a%2520two-stage%2520scheme%2520connecting%2520pre-trained%2520understanding%2520and%2520generative%2520models%2520for%2520alignment%2520fine-tuning.%2520The%2520former%2520demands%2520massive%2520data%2520and%2520computing%2520resources%2520unaffordable%2520for%2520ordinary%2520researchers.%2520Though%2520the%2520latter%2520requires%2520a%2520lower%2520training%2520cost%252C%2520existing%2520works%2520often%2520suffer%2520from%2520limited%2520task%2520coverage%2520or%2520poor%2520generation%2520quality.%2520Both%2520approaches%2520lack%2520the%2520ability%2520to%2520parse%2520input%2520meta-information%2520%2528such%2520as%2520task%2520type%252C%2520image%2520resolution%252C%2520video%2520duration%252C%2520etc.%2529%2520and%2520require%2520manual%2520parameter%2520configuration%2520that%2520is%2520tedious%2520and%2520non-intelligent.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Unison%2520which%2520adopts%2520the%2520two-stage%2520scheme%2520while%2520preserving%2520the%2520capabilities%2520of%2520the%2520pre-trained%2520models%2520well.%2520With%2520an%2520extremely%2520low%2520training%2520cost%252C%2520we%2520cover%2520a%2520variety%2520of%2520multimodal%2520understanding%2520tasks%252C%2520including%2520text%252C%2520image%252C%2520and%2520video%2520understanding%252C%2520as%2520well%2520as%2520diverse%2520generation%2520tasks%252C%2520such%2520as%2520text-to-visual%2520content%2520generation%252C%2520editing%252C%2520controllable%2520generation%252C%2520and%2520IP-based%2520reference%2520generation.%2520We%2520also%2520equip%2520our%2520model%2520with%2520the%2520ability%2520to%2520automatically%2520parse%2520user%2520intentions%252C%2520determine%2520the%2520target%2520task%2520type%252C%2520and%2520accurately%2520extract%2520the%2520meta-information%2520required%2520for%2520the%2520corresponding%2520task.%2520This%2520enables%2520full%2520automation%2520of%2520various%2520multimodal%2520tasks%2520without%2520human%2520intervention.%2520Experiments%2520demonstrate%2520that%252C%2520under%2520a%2520low-cost%2520setting%2520of%2520only%2520500k%2520training%2520samples%2520and%252050%2520GPU%2520hours%252C%2520our%2520model%2520can%2520accurately%2520and%2520automatically%2520identify%2520tasks%2520and%2520extract%2520relevant%2520parameters%252C%2520and%2520achieve%2520superior%2520performance%2520across%2520a%2520variety%2520of%2520understanding%2520and%2520generation%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unison%3A%20A%20Fully%20Automatic%2C%20Task-Universal%2C%20and%20Low-Cost%20Framework%20for%20Unified%20Understanding%20and%20Generation&entry.906535625=Shihao%20Zhao%20and%20Yitong%20Chen%20and%20Zeyinzi%20Jiang%20and%20Bojia%20Zi%20and%20Shaozhe%20Hao%20and%20Yu%20Liu%20and%20Chaojie%20Mao%20and%20Kwan-Yee%20K.%20Wong&entry.1292438233=Unified%20understanding%20and%20generation%20is%20a%20highly%20appealing%20research%20direction%20in%20multimodal%20learning.%20There%20exist%20two%20approaches%3A%20one%20trains%20a%20transformer%20via%20an%20auto-regressive%20paradigm%2C%20and%20the%20other%20adopts%20a%20two-stage%20scheme%20connecting%20pre-trained%20understanding%20and%20generative%20models%20for%20alignment%20fine-tuning.%20The%20former%20demands%20massive%20data%20and%20computing%20resources%20unaffordable%20for%20ordinary%20researchers.%20Though%20the%20latter%20requires%20a%20lower%20training%20cost%2C%20existing%20works%20often%20suffer%20from%20limited%20task%20coverage%20or%20poor%20generation%20quality.%20Both%20approaches%20lack%20the%20ability%20to%20parse%20input%20meta-information%20%28such%20as%20task%20type%2C%20image%20resolution%2C%20video%20duration%2C%20etc.%29%20and%20require%20manual%20parameter%20configuration%20that%20is%20tedious%20and%20non-intelligent.%20In%20this%20paper%2C%20we%20propose%20Unison%20which%20adopts%20the%20two-stage%20scheme%20while%20preserving%20the%20capabilities%20of%20the%20pre-trained%20models%20well.%20With%20an%20extremely%20low%20training%20cost%2C%20we%20cover%20a%20variety%20of%20multimodal%20understanding%20tasks%2C%20including%20text%2C%20image%2C%20and%20video%20understanding%2C%20as%20well%20as%20diverse%20generation%20tasks%2C%20such%20as%20text-to-visual%20content%20generation%2C%20editing%2C%20controllable%20generation%2C%20and%20IP-based%20reference%20generation.%20We%20also%20equip%20our%20model%20with%20the%20ability%20to%20automatically%20parse%20user%20intentions%2C%20determine%20the%20target%20task%20type%2C%20and%20accurately%20extract%20the%20meta-information%20required%20for%20the%20corresponding%20task.%20This%20enables%20full%20automation%20of%20various%20multimodal%20tasks%20without%20human%20intervention.%20Experiments%20demonstrate%20that%2C%20under%20a%20low-cost%20setting%20of%20only%20500k%20training%20samples%20and%2050%20GPU%20hours%2C%20our%20model%20can%20accurately%20and%20automatically%20identify%20tasks%20and%20extract%20relevant%20parameters%2C%20and%20achieve%20superior%20performance%20across%20a%20variety%20of%20understanding%20and%20generation%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.07747v1&entry.124074799=Read"},
{"title": "Forget and Explain: Transparent Verification of GNN Unlearning", "author": "Imran Ahsan and Hyunwook Yu and Jinsung Kim and Mucheol Kim", "abstract": "Graph neural networks (GNNs) are increasingly used to model complex patterns in graph-structured data. However, enabling them to \"forget\" designated information remains challenging, especially under privacy regulations such as the GDPR. Existing unlearning methods largely optimize for efficiency and scalability, yet they offer little transparency, and the black-box nature of GNNs makes it difficult to verify whether forgetting has truly occurred. We propose an explainability-driven verifier for GNN unlearning that snapshots the model before and after deletion, using attribution shifts and localized structural changes (for example, graph edit distance) as transparent evidence. The verifier uses five explainability metrics: residual attribution, heatmap shift, explainability score deviation, graph edit distance, and a diagnostic graph rule shift. We evaluate two backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics). Results show that Retrain and GNNDelete achieve near-complete forgetting, GraphEditor provides partial erasure, and IDEA leaves residual signals. These explanation deltas provide the primary, human-readable evidence of forgetting; we also report membership-inference ROC-AUC as a complementary, graph-wide privacy signal.", "link": "http://arxiv.org/abs/2512.07450v1", "date": "2025-12-08", "relevancy": 2.3171, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4761}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4574}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forget%20and%20Explain%3A%20Transparent%20Verification%20of%20GNN%20Unlearning&body=Title%3A%20Forget%20and%20Explain%3A%20Transparent%20Verification%20of%20GNN%20Unlearning%0AAuthor%3A%20Imran%20Ahsan%20and%20Hyunwook%20Yu%20and%20Jinsung%20Kim%20and%20Mucheol%20Kim%0AAbstract%3A%20Graph%20neural%20networks%20%28GNNs%29%20are%20increasingly%20used%20to%20model%20complex%20patterns%20in%20graph-structured%20data.%20However%2C%20enabling%20them%20to%20%22forget%22%20designated%20information%20remains%20challenging%2C%20especially%20under%20privacy%20regulations%20such%20as%20the%20GDPR.%20Existing%20unlearning%20methods%20largely%20optimize%20for%20efficiency%20and%20scalability%2C%20yet%20they%20offer%20little%20transparency%2C%20and%20the%20black-box%20nature%20of%20GNNs%20makes%20it%20difficult%20to%20verify%20whether%20forgetting%20has%20truly%20occurred.%20We%20propose%20an%20explainability-driven%20verifier%20for%20GNN%20unlearning%20that%20snapshots%20the%20model%20before%20and%20after%20deletion%2C%20using%20attribution%20shifts%20and%20localized%20structural%20changes%20%28for%20example%2C%20graph%20edit%20distance%29%20as%20transparent%20evidence.%20The%20verifier%20uses%20five%20explainability%20metrics%3A%20residual%20attribution%2C%20heatmap%20shift%2C%20explainability%20score%20deviation%2C%20graph%20edit%20distance%2C%20and%20a%20diagnostic%20graph%20rule%20shift.%20We%20evaluate%20two%20backbones%20%28GCN%2C%20GAT%29%20and%20four%20unlearning%20strategies%20%28Retrain%2C%20GraphEditor%2C%20GNNDelete%2C%20IDEA%29%20across%20five%20benchmarks%20%28Cora%2C%20Citeseer%2C%20Pubmed%2C%20Coauthor-CS%2C%20Coauthor-Physics%29.%20Results%20show%20that%20Retrain%20and%20GNNDelete%20achieve%20near-complete%20forgetting%2C%20GraphEditor%20provides%20partial%20erasure%2C%20and%20IDEA%20leaves%20residual%20signals.%20These%20explanation%20deltas%20provide%20the%20primary%2C%20human-readable%20evidence%20of%20forgetting%3B%20we%20also%20report%20membership-inference%20ROC-AUC%20as%20a%20complementary%2C%20graph-wide%20privacy%20signal.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07450v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForget%2520and%2520Explain%253A%2520Transparent%2520Verification%2520of%2520GNN%2520Unlearning%26entry.906535625%3DImran%2520Ahsan%2520and%2520Hyunwook%2520Yu%2520and%2520Jinsung%2520Kim%2520and%2520Mucheol%2520Kim%26entry.1292438233%3DGraph%2520neural%2520networks%2520%2528GNNs%2529%2520are%2520increasingly%2520used%2520to%2520model%2520complex%2520patterns%2520in%2520graph-structured%2520data.%2520However%252C%2520enabling%2520them%2520to%2520%2522forget%2522%2520designated%2520information%2520remains%2520challenging%252C%2520especially%2520under%2520privacy%2520regulations%2520such%2520as%2520the%2520GDPR.%2520Existing%2520unlearning%2520methods%2520largely%2520optimize%2520for%2520efficiency%2520and%2520scalability%252C%2520yet%2520they%2520offer%2520little%2520transparency%252C%2520and%2520the%2520black-box%2520nature%2520of%2520GNNs%2520makes%2520it%2520difficult%2520to%2520verify%2520whether%2520forgetting%2520has%2520truly%2520occurred.%2520We%2520propose%2520an%2520explainability-driven%2520verifier%2520for%2520GNN%2520unlearning%2520that%2520snapshots%2520the%2520model%2520before%2520and%2520after%2520deletion%252C%2520using%2520attribution%2520shifts%2520and%2520localized%2520structural%2520changes%2520%2528for%2520example%252C%2520graph%2520edit%2520distance%2529%2520as%2520transparent%2520evidence.%2520The%2520verifier%2520uses%2520five%2520explainability%2520metrics%253A%2520residual%2520attribution%252C%2520heatmap%2520shift%252C%2520explainability%2520score%2520deviation%252C%2520graph%2520edit%2520distance%252C%2520and%2520a%2520diagnostic%2520graph%2520rule%2520shift.%2520We%2520evaluate%2520two%2520backbones%2520%2528GCN%252C%2520GAT%2529%2520and%2520four%2520unlearning%2520strategies%2520%2528Retrain%252C%2520GraphEditor%252C%2520GNNDelete%252C%2520IDEA%2529%2520across%2520five%2520benchmarks%2520%2528Cora%252C%2520Citeseer%252C%2520Pubmed%252C%2520Coauthor-CS%252C%2520Coauthor-Physics%2529.%2520Results%2520show%2520that%2520Retrain%2520and%2520GNNDelete%2520achieve%2520near-complete%2520forgetting%252C%2520GraphEditor%2520provides%2520partial%2520erasure%252C%2520and%2520IDEA%2520leaves%2520residual%2520signals.%2520These%2520explanation%2520deltas%2520provide%2520the%2520primary%252C%2520human-readable%2520evidence%2520of%2520forgetting%253B%2520we%2520also%2520report%2520membership-inference%2520ROC-AUC%2520as%2520a%2520complementary%252C%2520graph-wide%2520privacy%2520signal.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07450v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forget%20and%20Explain%3A%20Transparent%20Verification%20of%20GNN%20Unlearning&entry.906535625=Imran%20Ahsan%20and%20Hyunwook%20Yu%20and%20Jinsung%20Kim%20and%20Mucheol%20Kim&entry.1292438233=Graph%20neural%20networks%20%28GNNs%29%20are%20increasingly%20used%20to%20model%20complex%20patterns%20in%20graph-structured%20data.%20However%2C%20enabling%20them%20to%20%22forget%22%20designated%20information%20remains%20challenging%2C%20especially%20under%20privacy%20regulations%20such%20as%20the%20GDPR.%20Existing%20unlearning%20methods%20largely%20optimize%20for%20efficiency%20and%20scalability%2C%20yet%20they%20offer%20little%20transparency%2C%20and%20the%20black-box%20nature%20of%20GNNs%20makes%20it%20difficult%20to%20verify%20whether%20forgetting%20has%20truly%20occurred.%20We%20propose%20an%20explainability-driven%20verifier%20for%20GNN%20unlearning%20that%20snapshots%20the%20model%20before%20and%20after%20deletion%2C%20using%20attribution%20shifts%20and%20localized%20structural%20changes%20%28for%20example%2C%20graph%20edit%20distance%29%20as%20transparent%20evidence.%20The%20verifier%20uses%20five%20explainability%20metrics%3A%20residual%20attribution%2C%20heatmap%20shift%2C%20explainability%20score%20deviation%2C%20graph%20edit%20distance%2C%20and%20a%20diagnostic%20graph%20rule%20shift.%20We%20evaluate%20two%20backbones%20%28GCN%2C%20GAT%29%20and%20four%20unlearning%20strategies%20%28Retrain%2C%20GraphEditor%2C%20GNNDelete%2C%20IDEA%29%20across%20five%20benchmarks%20%28Cora%2C%20Citeseer%2C%20Pubmed%2C%20Coauthor-CS%2C%20Coauthor-Physics%29.%20Results%20show%20that%20Retrain%20and%20GNNDelete%20achieve%20near-complete%20forgetting%2C%20GraphEditor%20provides%20partial%20erasure%2C%20and%20IDEA%20leaves%20residual%20signals.%20These%20explanation%20deltas%20provide%20the%20primary%2C%20human-readable%20evidence%20of%20forgetting%3B%20we%20also%20report%20membership-inference%20ROC-AUC%20as%20a%20complementary%2C%20graph-wide%20privacy%20signal.&entry.1838667208=http%3A//arxiv.org/abs/2512.07450v1&entry.124074799=Read"},
{"title": "OptMap: Geometric Map Distillation via Submodular Maximization", "author": "David Thorne and Nathan Chan and Christa S. Robison and Philip R. Osteen and Brett T. Lopez", "abstract": "Autonomous robots rely on geometric maps to inform a diverse set of perception and decision-making algorithms. As autonomy requires reasoning and planning on multiple scales of the environment, each algorithm may require a different map for optimal performance. Light Detection And Ranging (LiDAR) sensors generate an abundance of geometric data to satisfy these diverse requirements, but selecting informative, size-constrained maps is computationally challenging as it requires solving an NP-hard combinatorial optimization. In this work we present OptMap: a geometric map distillation algorithm which achieves real-time, application-specific map generation via multiple theoretical and algorithmic innovations. A central feature is the maximization of set functions that exhibit diminishing returns, i.e., submodularity, using polynomial-time algorithms with provably near-optimal solutions. We formulate a novel submodular reward function which quantifies informativeness, reduces input set sizes, and minimizes bias in sequentially collected datasets. Further, we propose a dynamically reordered streaming submodular algorithm which improves empirical solution quality and addresses input order bias via an online approximation of the value of all scans. Testing was conducted on open-source and custom datasets with an emphasis on long-duration mapping sessions, highlighting OptMap's minimal computation requirements. Open-source ROS1 and ROS2 packages are available and can be used alongside any LiDAR SLAM algorithm.", "link": "http://arxiv.org/abs/2512.07775v1", "date": "2025-12-08", "relevancy": 2.3155, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6116}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5749}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OptMap%3A%20Geometric%20Map%20Distillation%20via%20Submodular%20Maximization&body=Title%3A%20OptMap%3A%20Geometric%20Map%20Distillation%20via%20Submodular%20Maximization%0AAuthor%3A%20David%20Thorne%20and%20Nathan%20Chan%20and%20Christa%20S.%20Robison%20and%20Philip%20R.%20Osteen%20and%20Brett%20T.%20Lopez%0AAbstract%3A%20Autonomous%20robots%20rely%20on%20geometric%20maps%20to%20inform%20a%20diverse%20set%20of%20perception%20and%20decision-making%20algorithms.%20As%20autonomy%20requires%20reasoning%20and%20planning%20on%20multiple%20scales%20of%20the%20environment%2C%20each%20algorithm%20may%20require%20a%20different%20map%20for%20optimal%20performance.%20Light%20Detection%20And%20Ranging%20%28LiDAR%29%20sensors%20generate%20an%20abundance%20of%20geometric%20data%20to%20satisfy%20these%20diverse%20requirements%2C%20but%20selecting%20informative%2C%20size-constrained%20maps%20is%20computationally%20challenging%20as%20it%20requires%20solving%20an%20NP-hard%20combinatorial%20optimization.%20In%20this%20work%20we%20present%20OptMap%3A%20a%20geometric%20map%20distillation%20algorithm%20which%20achieves%20real-time%2C%20application-specific%20map%20generation%20via%20multiple%20theoretical%20and%20algorithmic%20innovations.%20A%20central%20feature%20is%20the%20maximization%20of%20set%20functions%20that%20exhibit%20diminishing%20returns%2C%20i.e.%2C%20submodularity%2C%20using%20polynomial-time%20algorithms%20with%20provably%20near-optimal%20solutions.%20We%20formulate%20a%20novel%20submodular%20reward%20function%20which%20quantifies%20informativeness%2C%20reduces%20input%20set%20sizes%2C%20and%20minimizes%20bias%20in%20sequentially%20collected%20datasets.%20Further%2C%20we%20propose%20a%20dynamically%20reordered%20streaming%20submodular%20algorithm%20which%20improves%20empirical%20solution%20quality%20and%20addresses%20input%20order%20bias%20via%20an%20online%20approximation%20of%20the%20value%20of%20all%20scans.%20Testing%20was%20conducted%20on%20open-source%20and%20custom%20datasets%20with%20an%20emphasis%20on%20long-duration%20mapping%20sessions%2C%20highlighting%20OptMap%27s%20minimal%20computation%20requirements.%20Open-source%20ROS1%20and%20ROS2%20packages%20are%20available%20and%20can%20be%20used%20alongside%20any%20LiDAR%20SLAM%20algorithm.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptMap%253A%2520Geometric%2520Map%2520Distillation%2520via%2520Submodular%2520Maximization%26entry.906535625%3DDavid%2520Thorne%2520and%2520Nathan%2520Chan%2520and%2520Christa%2520S.%2520Robison%2520and%2520Philip%2520R.%2520Osteen%2520and%2520Brett%2520T.%2520Lopez%26entry.1292438233%3DAutonomous%2520robots%2520rely%2520on%2520geometric%2520maps%2520to%2520inform%2520a%2520diverse%2520set%2520of%2520perception%2520and%2520decision-making%2520algorithms.%2520As%2520autonomy%2520requires%2520reasoning%2520and%2520planning%2520on%2520multiple%2520scales%2520of%2520the%2520environment%252C%2520each%2520algorithm%2520may%2520require%2520a%2520different%2520map%2520for%2520optimal%2520performance.%2520Light%2520Detection%2520And%2520Ranging%2520%2528LiDAR%2529%2520sensors%2520generate%2520an%2520abundance%2520of%2520geometric%2520data%2520to%2520satisfy%2520these%2520diverse%2520requirements%252C%2520but%2520selecting%2520informative%252C%2520size-constrained%2520maps%2520is%2520computationally%2520challenging%2520as%2520it%2520requires%2520solving%2520an%2520NP-hard%2520combinatorial%2520optimization.%2520In%2520this%2520work%2520we%2520present%2520OptMap%253A%2520a%2520geometric%2520map%2520distillation%2520algorithm%2520which%2520achieves%2520real-time%252C%2520application-specific%2520map%2520generation%2520via%2520multiple%2520theoretical%2520and%2520algorithmic%2520innovations.%2520A%2520central%2520feature%2520is%2520the%2520maximization%2520of%2520set%2520functions%2520that%2520exhibit%2520diminishing%2520returns%252C%2520i.e.%252C%2520submodularity%252C%2520using%2520polynomial-time%2520algorithms%2520with%2520provably%2520near-optimal%2520solutions.%2520We%2520formulate%2520a%2520novel%2520submodular%2520reward%2520function%2520which%2520quantifies%2520informativeness%252C%2520reduces%2520input%2520set%2520sizes%252C%2520and%2520minimizes%2520bias%2520in%2520sequentially%2520collected%2520datasets.%2520Further%252C%2520we%2520propose%2520a%2520dynamically%2520reordered%2520streaming%2520submodular%2520algorithm%2520which%2520improves%2520empirical%2520solution%2520quality%2520and%2520addresses%2520input%2520order%2520bias%2520via%2520an%2520online%2520approximation%2520of%2520the%2520value%2520of%2520all%2520scans.%2520Testing%2520was%2520conducted%2520on%2520open-source%2520and%2520custom%2520datasets%2520with%2520an%2520emphasis%2520on%2520long-duration%2520mapping%2520sessions%252C%2520highlighting%2520OptMap%2527s%2520minimal%2520computation%2520requirements.%2520Open-source%2520ROS1%2520and%2520ROS2%2520packages%2520are%2520available%2520and%2520can%2520be%2520used%2520alongside%2520any%2520LiDAR%2520SLAM%2520algorithm.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OptMap%3A%20Geometric%20Map%20Distillation%20via%20Submodular%20Maximization&entry.906535625=David%20Thorne%20and%20Nathan%20Chan%20and%20Christa%20S.%20Robison%20and%20Philip%20R.%20Osteen%20and%20Brett%20T.%20Lopez&entry.1292438233=Autonomous%20robots%20rely%20on%20geometric%20maps%20to%20inform%20a%20diverse%20set%20of%20perception%20and%20decision-making%20algorithms.%20As%20autonomy%20requires%20reasoning%20and%20planning%20on%20multiple%20scales%20of%20the%20environment%2C%20each%20algorithm%20may%20require%20a%20different%20map%20for%20optimal%20performance.%20Light%20Detection%20And%20Ranging%20%28LiDAR%29%20sensors%20generate%20an%20abundance%20of%20geometric%20data%20to%20satisfy%20these%20diverse%20requirements%2C%20but%20selecting%20informative%2C%20size-constrained%20maps%20is%20computationally%20challenging%20as%20it%20requires%20solving%20an%20NP-hard%20combinatorial%20optimization.%20In%20this%20work%20we%20present%20OptMap%3A%20a%20geometric%20map%20distillation%20algorithm%20which%20achieves%20real-time%2C%20application-specific%20map%20generation%20via%20multiple%20theoretical%20and%20algorithmic%20innovations.%20A%20central%20feature%20is%20the%20maximization%20of%20set%20functions%20that%20exhibit%20diminishing%20returns%2C%20i.e.%2C%20submodularity%2C%20using%20polynomial-time%20algorithms%20with%20provably%20near-optimal%20solutions.%20We%20formulate%20a%20novel%20submodular%20reward%20function%20which%20quantifies%20informativeness%2C%20reduces%20input%20set%20sizes%2C%20and%20minimizes%20bias%20in%20sequentially%20collected%20datasets.%20Further%2C%20we%20propose%20a%20dynamically%20reordered%20streaming%20submodular%20algorithm%20which%20improves%20empirical%20solution%20quality%20and%20addresses%20input%20order%20bias%20via%20an%20online%20approximation%20of%20the%20value%20of%20all%20scans.%20Testing%20was%20conducted%20on%20open-source%20and%20custom%20datasets%20with%20an%20emphasis%20on%20long-duration%20mapping%20sessions%2C%20highlighting%20OptMap%27s%20minimal%20computation%20requirements.%20Open-source%20ROS1%20and%20ROS2%20packages%20are%20available%20and%20can%20be%20used%20alongside%20any%20LiDAR%20SLAM%20algorithm.&entry.1838667208=http%3A//arxiv.org/abs/2512.07775v1&entry.124074799=Read"},
{"title": "MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation", "author": "Zhiqi Li and Wenhuan Li and Tengfei Wang and Zhenwei Wang and Junta Wu and Haoyuan Wang and Yunhan Yang and Zehuan Huang and Yang Li and Peidong Liu and Chunchao Guo", "abstract": "Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA", "link": "http://arxiv.org/abs/2512.07628v1", "date": "2025-12-08", "relevancy": 2.3053, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6164}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5683}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoCA%3A%20Mixture-of-Components%20Attention%20for%20Scalable%20Compositional%203D%20Generation&body=Title%3A%20MoCA%3A%20Mixture-of-Components%20Attention%20for%20Scalable%20Compositional%203D%20Generation%0AAuthor%3A%20Zhiqi%20Li%20and%20Wenhuan%20Li%20and%20Tengfei%20Wang%20and%20Zhenwei%20Wang%20and%20Junta%20Wu%20and%20Haoyuan%20Wang%20and%20Yunhan%20Yang%20and%20Zehuan%20Huang%20and%20Yang%20Li%20and%20Peidong%20Liu%20and%20Chunchao%20Guo%0AAbstract%3A%20Compositionality%20is%20critical%20for%203D%20object%20and%20scene%20generation%2C%20but%20existing%20part-aware%203D%20generation%20methods%20suffer%20from%20poor%20scalability%20due%20to%20quadratic%20global%20attention%20costs%20when%20increasing%20the%20number%20of%20components.%20In%20this%20work%2C%20we%20present%20MoCA%2C%20a%20compositional%203D%20generative%20model%20with%20two%20key%20designs%3A%20%281%29%20importance-based%20component%20routing%20that%20selects%20top-k%20relevant%20components%20for%20sparse%20global%20attention%2C%20and%20%282%29%20unimportant%20components%20compression%20that%20preserve%20contextual%20priors%20of%20unselected%20components%20while%20reducing%20computational%20complexity%20of%20global%20attention.%20With%20these%20designs%2C%20MoCA%20enables%20efficient%2C%20fine-grained%20compositional%203D%20asset%20creation%20with%20scalable%20number%20of%20components.%20Extensive%20experiments%20show%20MoCA%20outperforms%20baselines%20on%20both%20compositional%20object%20and%20scene%20generation%20tasks.%20Project%20page%3A%20https%3A//lizhiqi49.github.io/MoCA%0ALink%3A%20http%3A//arxiv.org/abs/2512.07628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoCA%253A%2520Mixture-of-Components%2520Attention%2520for%2520Scalable%2520Compositional%25203D%2520Generation%26entry.906535625%3DZhiqi%2520Li%2520and%2520Wenhuan%2520Li%2520and%2520Tengfei%2520Wang%2520and%2520Zhenwei%2520Wang%2520and%2520Junta%2520Wu%2520and%2520Haoyuan%2520Wang%2520and%2520Yunhan%2520Yang%2520and%2520Zehuan%2520Huang%2520and%2520Yang%2520Li%2520and%2520Peidong%2520Liu%2520and%2520Chunchao%2520Guo%26entry.1292438233%3DCompositionality%2520is%2520critical%2520for%25203D%2520object%2520and%2520scene%2520generation%252C%2520but%2520existing%2520part-aware%25203D%2520generation%2520methods%2520suffer%2520from%2520poor%2520scalability%2520due%2520to%2520quadratic%2520global%2520attention%2520costs%2520when%2520increasing%2520the%2520number%2520of%2520components.%2520In%2520this%2520work%252C%2520we%2520present%2520MoCA%252C%2520a%2520compositional%25203D%2520generative%2520model%2520with%2520two%2520key%2520designs%253A%2520%25281%2529%2520importance-based%2520component%2520routing%2520that%2520selects%2520top-k%2520relevant%2520components%2520for%2520sparse%2520global%2520attention%252C%2520and%2520%25282%2529%2520unimportant%2520components%2520compression%2520that%2520preserve%2520contextual%2520priors%2520of%2520unselected%2520components%2520while%2520reducing%2520computational%2520complexity%2520of%2520global%2520attention.%2520With%2520these%2520designs%252C%2520MoCA%2520enables%2520efficient%252C%2520fine-grained%2520compositional%25203D%2520asset%2520creation%2520with%2520scalable%2520number%2520of%2520components.%2520Extensive%2520experiments%2520show%2520MoCA%2520outperforms%2520baselines%2520on%2520both%2520compositional%2520object%2520and%2520scene%2520generation%2520tasks.%2520Project%2520page%253A%2520https%253A//lizhiqi49.github.io/MoCA%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoCA%3A%20Mixture-of-Components%20Attention%20for%20Scalable%20Compositional%203D%20Generation&entry.906535625=Zhiqi%20Li%20and%20Wenhuan%20Li%20and%20Tengfei%20Wang%20and%20Zhenwei%20Wang%20and%20Junta%20Wu%20and%20Haoyuan%20Wang%20and%20Yunhan%20Yang%20and%20Zehuan%20Huang%20and%20Yang%20Li%20and%20Peidong%20Liu%20and%20Chunchao%20Guo&entry.1292438233=Compositionality%20is%20critical%20for%203D%20object%20and%20scene%20generation%2C%20but%20existing%20part-aware%203D%20generation%20methods%20suffer%20from%20poor%20scalability%20due%20to%20quadratic%20global%20attention%20costs%20when%20increasing%20the%20number%20of%20components.%20In%20this%20work%2C%20we%20present%20MoCA%2C%20a%20compositional%203D%20generative%20model%20with%20two%20key%20designs%3A%20%281%29%20importance-based%20component%20routing%20that%20selects%20top-k%20relevant%20components%20for%20sparse%20global%20attention%2C%20and%20%282%29%20unimportant%20components%20compression%20that%20preserve%20contextual%20priors%20of%20unselected%20components%20while%20reducing%20computational%20complexity%20of%20global%20attention.%20With%20these%20designs%2C%20MoCA%20enables%20efficient%2C%20fine-grained%20compositional%203D%20asset%20creation%20with%20scalable%20number%20of%20components.%20Extensive%20experiments%20show%20MoCA%20outperforms%20baselines%20on%20both%20compositional%20object%20and%20scene%20generation%20tasks.%20Project%20page%3A%20https%3A//lizhiqi49.github.io/MoCA&entry.1838667208=http%3A//arxiv.org/abs/2512.07628v1&entry.124074799=Read"},
{"title": "Multi-view Pyramid Transformer: Look Coarser to See Broader", "author": "Gyeongjin Kang and Seungkwon Yang and Seungtae Nam and Younggeun Lee and Jungwoo Kim and Eunbyung Park", "abstract": "We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details,\" MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.", "link": "http://arxiv.org/abs/2512.07806v1", "date": "2025-12-08", "relevancy": 2.298, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5791}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5791}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-view%20Pyramid%20Transformer%3A%20Look%20Coarser%20to%20See%20Broader&body=Title%3A%20Multi-view%20Pyramid%20Transformer%3A%20Look%20Coarser%20to%20See%20Broader%0AAuthor%3A%20Gyeongjin%20Kang%20and%20Seungkwon%20Yang%20and%20Seungtae%20Nam%20and%20Younggeun%20Lee%20and%20Jungwoo%20Kim%20and%20Eunbyung%20Park%0AAbstract%3A%20We%20propose%20Multi-view%20Pyramid%20Transformer%20%28MVP%29%2C%20a%20scalable%20multi-view%20transformer%20architecture%20that%20directly%20reconstructs%20large%203D%20scenes%20from%20tens%20to%20hundreds%20of%20images%20in%20a%20single%20forward%20pass.%20Drawing%20on%20the%20idea%20of%20%60%60looking%20broader%20to%20see%20the%20whole%2C%20looking%20finer%20to%20see%20the%20details%2C%22%20MVP%20is%20built%20on%20two%20core%20design%20principles%3A%201%29%20a%20local-to-global%20inter-view%20hierarchy%20that%20gradually%20broadens%20the%20model%27s%20perspective%20from%20local%20views%20to%20groups%20and%20ultimately%20the%20full%20scene%2C%20and%202%29%20a%20fine-to-coarse%20intra-view%20hierarchy%20that%20starts%20from%20detailed%20spatial%20representations%20and%20progressively%20aggregates%20them%20into%20compact%2C%20information-dense%20tokens.%20This%20dual%20hierarchy%20achieves%20both%20computational%20efficiency%20and%20representational%20richness%2C%20enabling%20fast%20reconstruction%20of%20large%20and%20complex%20scenes.%20We%20validate%20MVP%20on%20diverse%20datasets%20and%20show%20that%2C%20when%20coupled%20with%203D%20Gaussian%20Splatting%20as%20the%20underlying%203D%20representation%2C%20it%20achieves%20state-of-the-art%20generalizable%20reconstruction%20quality%20while%20maintaining%20high%20efficiency%20and%20scalability%20across%20a%20wide%20range%20of%20view%20configurations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-view%2520Pyramid%2520Transformer%253A%2520Look%2520Coarser%2520to%2520See%2520Broader%26entry.906535625%3DGyeongjin%2520Kang%2520and%2520Seungkwon%2520Yang%2520and%2520Seungtae%2520Nam%2520and%2520Younggeun%2520Lee%2520and%2520Jungwoo%2520Kim%2520and%2520Eunbyung%2520Park%26entry.1292438233%3DWe%2520propose%2520Multi-view%2520Pyramid%2520Transformer%2520%2528MVP%2529%252C%2520a%2520scalable%2520multi-view%2520transformer%2520architecture%2520that%2520directly%2520reconstructs%2520large%25203D%2520scenes%2520from%2520tens%2520to%2520hundreds%2520of%2520images%2520in%2520a%2520single%2520forward%2520pass.%2520Drawing%2520on%2520the%2520idea%2520of%2520%2560%2560looking%2520broader%2520to%2520see%2520the%2520whole%252C%2520looking%2520finer%2520to%2520see%2520the%2520details%252C%2522%2520MVP%2520is%2520built%2520on%2520two%2520core%2520design%2520principles%253A%25201%2529%2520a%2520local-to-global%2520inter-view%2520hierarchy%2520that%2520gradually%2520broadens%2520the%2520model%2527s%2520perspective%2520from%2520local%2520views%2520to%2520groups%2520and%2520ultimately%2520the%2520full%2520scene%252C%2520and%25202%2529%2520a%2520fine-to-coarse%2520intra-view%2520hierarchy%2520that%2520starts%2520from%2520detailed%2520spatial%2520representations%2520and%2520progressively%2520aggregates%2520them%2520into%2520compact%252C%2520information-dense%2520tokens.%2520This%2520dual%2520hierarchy%2520achieves%2520both%2520computational%2520efficiency%2520and%2520representational%2520richness%252C%2520enabling%2520fast%2520reconstruction%2520of%2520large%2520and%2520complex%2520scenes.%2520We%2520validate%2520MVP%2520on%2520diverse%2520datasets%2520and%2520show%2520that%252C%2520when%2520coupled%2520with%25203D%2520Gaussian%2520Splatting%2520as%2520the%2520underlying%25203D%2520representation%252C%2520it%2520achieves%2520state-of-the-art%2520generalizable%2520reconstruction%2520quality%2520while%2520maintaining%2520high%2520efficiency%2520and%2520scalability%2520across%2520a%2520wide%2520range%2520of%2520view%2520configurations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-view%20Pyramid%20Transformer%3A%20Look%20Coarser%20to%20See%20Broader&entry.906535625=Gyeongjin%20Kang%20and%20Seungkwon%20Yang%20and%20Seungtae%20Nam%20and%20Younggeun%20Lee%20and%20Jungwoo%20Kim%20and%20Eunbyung%20Park&entry.1292438233=We%20propose%20Multi-view%20Pyramid%20Transformer%20%28MVP%29%2C%20a%20scalable%20multi-view%20transformer%20architecture%20that%20directly%20reconstructs%20large%203D%20scenes%20from%20tens%20to%20hundreds%20of%20images%20in%20a%20single%20forward%20pass.%20Drawing%20on%20the%20idea%20of%20%60%60looking%20broader%20to%20see%20the%20whole%2C%20looking%20finer%20to%20see%20the%20details%2C%22%20MVP%20is%20built%20on%20two%20core%20design%20principles%3A%201%29%20a%20local-to-global%20inter-view%20hierarchy%20that%20gradually%20broadens%20the%20model%27s%20perspective%20from%20local%20views%20to%20groups%20and%20ultimately%20the%20full%20scene%2C%20and%202%29%20a%20fine-to-coarse%20intra-view%20hierarchy%20that%20starts%20from%20detailed%20spatial%20representations%20and%20progressively%20aggregates%20them%20into%20compact%2C%20information-dense%20tokens.%20This%20dual%20hierarchy%20achieves%20both%20computational%20efficiency%20and%20representational%20richness%2C%20enabling%20fast%20reconstruction%20of%20large%20and%20complex%20scenes.%20We%20validate%20MVP%20on%20diverse%20datasets%20and%20show%20that%2C%20when%20coupled%20with%203D%20Gaussian%20Splatting%20as%20the%20underlying%203D%20representation%2C%20it%20achieves%20state-of-the-art%20generalizable%20reconstruction%20quality%20while%20maintaining%20high%20efficiency%20and%20scalability%20across%20a%20wide%20range%20of%20view%20configurations.&entry.1838667208=http%3A//arxiv.org/abs/2512.07806v1&entry.124074799=Read"},
{"title": "Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models", "author": "Kassoum Sanogo and Renzo Ardiccioni", "abstract": "Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.", "link": "http://arxiv.org/abs/2512.07564v1", "date": "2025-12-08", "relevancy": 2.2831, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5863}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5779}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20More%20Reliable%20Artificial%20Intelligence%3A%20Reducing%20Hallucinations%20in%20Vision-Language%20Models&body=Title%3A%20Toward%20More%20Reliable%20Artificial%20Intelligence%3A%20Reducing%20Hallucinations%20in%20Vision-Language%20Models%0AAuthor%3A%20Kassoum%20Sanogo%20and%20Renzo%20Ardiccioni%0AAbstract%3A%20Vision-language%20models%20%28VLMs%29%20frequently%20generate%20hallucinated%20content%20plausible%20but%20incorrect%20claims%20about%20image%20content.%20We%20propose%20a%20training-free%20self-correction%20framework%20enabling%20VLMs%20to%20iteratively%20refine%20responses%20through%20uncertainty-guided%20visual%20re-attention.%20Our%20method%20combines%20multidimensional%20uncertainty%20quantification%20%28token%20entropy%2C%20attention%20dispersion%2C%20semantic%20consistency%2C%20claim%20confidence%29%20with%20attention-guided%20cropping%20of%20under-explored%20regions.%20Operating%20entirely%20with%20frozen%2C%20pretrained%20VLMs%2C%20our%20framework%20requires%20no%20gradient%20updates.%20We%20validate%20our%20approach%20on%20the%20POPE%20and%20MMHAL%20BENCH%20benchmarks%20using%20the%20Qwen2.5-VL-7B%20%5B23%5D%20architecture.%20Experimental%20results%20demonstrate%20that%20our%20method%20reduces%20hallucination%20rates%20by%209.8%20percentage%20points%20compared%20to%20the%20baseline%2C%20while%20improving%20object%20existence%20accuracy%20by%204.7%20points%20on%20adversarial%20splits.%20Furthermore%2C%20qualitative%20analysis%20confirms%20that%20uncertainty-guided%20re-attention%20successfully%20grounds%20corrections%20in%20visual%20evidence%20where%20standard%20decoding%20fails.%20We%20validate%20our%20approach%20on%20Qwen2.5-VL-7B%20%5B23%5D%2C%20with%20plans%20to%20extend%20validation%20across%20diverse%20architectures%20in%20future%20versions.%20We%20release%20our%20code%20and%20methodology%20to%20facilitate%20future%20research%20in%20trustworthy%20multimodal%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520More%2520Reliable%2520Artificial%2520Intelligence%253A%2520Reducing%2520Hallucinations%2520in%2520Vision-Language%2520Models%26entry.906535625%3DKassoum%2520Sanogo%2520and%2520Renzo%2520Ardiccioni%26entry.1292438233%3DVision-language%2520models%2520%2528VLMs%2529%2520frequently%2520generate%2520hallucinated%2520content%2520plausible%2520but%2520incorrect%2520claims%2520about%2520image%2520content.%2520We%2520propose%2520a%2520training-free%2520self-correction%2520framework%2520enabling%2520VLMs%2520to%2520iteratively%2520refine%2520responses%2520through%2520uncertainty-guided%2520visual%2520re-attention.%2520Our%2520method%2520combines%2520multidimensional%2520uncertainty%2520quantification%2520%2528token%2520entropy%252C%2520attention%2520dispersion%252C%2520semantic%2520consistency%252C%2520claim%2520confidence%2529%2520with%2520attention-guided%2520cropping%2520of%2520under-explored%2520regions.%2520Operating%2520entirely%2520with%2520frozen%252C%2520pretrained%2520VLMs%252C%2520our%2520framework%2520requires%2520no%2520gradient%2520updates.%2520We%2520validate%2520our%2520approach%2520on%2520the%2520POPE%2520and%2520MMHAL%2520BENCH%2520benchmarks%2520using%2520the%2520Qwen2.5-VL-7B%2520%255B23%255D%2520architecture.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520reduces%2520hallucination%2520rates%2520by%25209.8%2520percentage%2520points%2520compared%2520to%2520the%2520baseline%252C%2520while%2520improving%2520object%2520existence%2520accuracy%2520by%25204.7%2520points%2520on%2520adversarial%2520splits.%2520Furthermore%252C%2520qualitative%2520analysis%2520confirms%2520that%2520uncertainty-guided%2520re-attention%2520successfully%2520grounds%2520corrections%2520in%2520visual%2520evidence%2520where%2520standard%2520decoding%2520fails.%2520We%2520validate%2520our%2520approach%2520on%2520Qwen2.5-VL-7B%2520%255B23%255D%252C%2520with%2520plans%2520to%2520extend%2520validation%2520across%2520diverse%2520architectures%2520in%2520future%2520versions.%2520We%2520release%2520our%2520code%2520and%2520methodology%2520to%2520facilitate%2520future%2520research%2520in%2520trustworthy%2520multimodal%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20More%20Reliable%20Artificial%20Intelligence%3A%20Reducing%20Hallucinations%20in%20Vision-Language%20Models&entry.906535625=Kassoum%20Sanogo%20and%20Renzo%20Ardiccioni&entry.1292438233=Vision-language%20models%20%28VLMs%29%20frequently%20generate%20hallucinated%20content%20plausible%20but%20incorrect%20claims%20about%20image%20content.%20We%20propose%20a%20training-free%20self-correction%20framework%20enabling%20VLMs%20to%20iteratively%20refine%20responses%20through%20uncertainty-guided%20visual%20re-attention.%20Our%20method%20combines%20multidimensional%20uncertainty%20quantification%20%28token%20entropy%2C%20attention%20dispersion%2C%20semantic%20consistency%2C%20claim%20confidence%29%20with%20attention-guided%20cropping%20of%20under-explored%20regions.%20Operating%20entirely%20with%20frozen%2C%20pretrained%20VLMs%2C%20our%20framework%20requires%20no%20gradient%20updates.%20We%20validate%20our%20approach%20on%20the%20POPE%20and%20MMHAL%20BENCH%20benchmarks%20using%20the%20Qwen2.5-VL-7B%20%5B23%5D%20architecture.%20Experimental%20results%20demonstrate%20that%20our%20method%20reduces%20hallucination%20rates%20by%209.8%20percentage%20points%20compared%20to%20the%20baseline%2C%20while%20improving%20object%20existence%20accuracy%20by%204.7%20points%20on%20adversarial%20splits.%20Furthermore%2C%20qualitative%20analysis%20confirms%20that%20uncertainty-guided%20re-attention%20successfully%20grounds%20corrections%20in%20visual%20evidence%20where%20standard%20decoding%20fails.%20We%20validate%20our%20approach%20on%20Qwen2.5-VL-7B%20%5B23%5D%2C%20with%20plans%20to%20extend%20validation%20across%20diverse%20architectures%20in%20future%20versions.%20We%20release%20our%20code%20and%20methodology%20to%20facilitate%20future%20research%20in%20trustworthy%20multimodal%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.07564v1&entry.124074799=Read"},
{"title": "DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations", "author": "Mehmet Yigit Avci and Pedro Borges and Virginia Fernandez and Paul Wright and Mehmet Yigitsoy and Sebastien Ourselin and Jorge Cardoso", "abstract": "Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.", "link": "http://arxiv.org/abs/2512.07674v1", "date": "2025-12-08", "relevancy": 2.2786, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6033}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5582}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIST-CLIP%3A%20Arbitrary%20Metadata%20and%20Image%20Guided%20MRI%20Harmonization%20via%20Disentangled%20Anatomy-Contrast%20Representations&body=Title%3A%20DIST-CLIP%3A%20Arbitrary%20Metadata%20and%20Image%20Guided%20MRI%20Harmonization%20via%20Disentangled%20Anatomy-Contrast%20Representations%0AAuthor%3A%20Mehmet%20Yigit%20Avci%20and%20Pedro%20Borges%20and%20Virginia%20Fernandez%20and%20Paul%20Wright%20and%20Mehmet%20Yigitsoy%20and%20Sebastien%20Ourselin%20and%20Jorge%20Cardoso%0AAbstract%3A%20Deep%20learning%20holds%20immense%20promise%20for%20transforming%20medical%20image%20analysis%2C%20yet%20its%20clinical%20generalization%20remains%20profoundly%20limited.%20A%20major%20barrier%20is%20data%20heterogeneity.%20This%20is%20particularly%20true%20in%20Magnetic%20Resonance%20Imaging%2C%20where%20scanner%20hardware%20differences%2C%20diverse%20acquisition%20protocols%2C%20and%20varying%20sequence%20parameters%20introduce%20substantial%20domain%20shifts%20that%20obscure%20underlying%20biological%20signals.%20Data%20harmonization%20methods%20aim%20to%20reduce%20these%20instrumental%20and%20acquisition%20variability%2C%20but%20existing%20approaches%20remain%20insufficient.%20When%20applied%20to%20imaging%20data%2C%20image-based%20harmonization%20approaches%20are%20often%20restricted%20by%20the%20need%20for%20target%20images%2C%20while%20existing%20text-guided%20methods%20rely%20on%20simplistic%20labels%20that%20fail%20to%20capture%20complex%20acquisition%20details%20or%20are%20typically%20restricted%20to%20datasets%20with%20limited%20variability%2C%20failing%20to%20capture%20the%20heterogeneity%20of%20real-world%20clinical%20environments.%20To%20address%20these%20limitations%2C%20we%20propose%20DIST-CLIP%20%28Disentangled%20Style%20Transfer%20with%20CLIP%20Guidance%29%2C%20a%20unified%20framework%20for%20MRI%20harmonization%20that%20flexibly%20uses%20either%20target%20images%20or%20DICOM%20metadata%20for%20guidance.%20Our%20framework%20explicitly%20disentangles%20anatomical%20content%20from%20image%20contrast%2C%20with%20the%20contrast%20representations%20being%20extracted%20using%20pre-trained%20CLIP%20encoders.%20These%20contrast%20embeddings%20are%20then%20integrated%20into%20the%20anatomical%20content%20via%20a%20novel%20Adaptive%20Style%20Transfer%20module.%20We%20trained%20and%20evaluated%20DIST-CLIP%20on%20diverse%20real-world%20clinical%20datasets%2C%20and%20showed%20significant%20improvements%20in%20performance%20when%20compared%20against%20state-of-the-art%20methods%20in%20both%20style%20translation%20fidelity%20and%20anatomical%20preservation%2C%20offering%20a%20flexible%20solution%20for%20style%20transfer%20and%20standardizing%20MRI%20data.%20Our%20code%20and%20weights%20will%20be%20made%20publicly%20available%20upon%20publication.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIST-CLIP%253A%2520Arbitrary%2520Metadata%2520and%2520Image%2520Guided%2520MRI%2520Harmonization%2520via%2520Disentangled%2520Anatomy-Contrast%2520Representations%26entry.906535625%3DMehmet%2520Yigit%2520Avci%2520and%2520Pedro%2520Borges%2520and%2520Virginia%2520Fernandez%2520and%2520Paul%2520Wright%2520and%2520Mehmet%2520Yigitsoy%2520and%2520Sebastien%2520Ourselin%2520and%2520Jorge%2520Cardoso%26entry.1292438233%3DDeep%2520learning%2520holds%2520immense%2520promise%2520for%2520transforming%2520medical%2520image%2520analysis%252C%2520yet%2520its%2520clinical%2520generalization%2520remains%2520profoundly%2520limited.%2520A%2520major%2520barrier%2520is%2520data%2520heterogeneity.%2520This%2520is%2520particularly%2520true%2520in%2520Magnetic%2520Resonance%2520Imaging%252C%2520where%2520scanner%2520hardware%2520differences%252C%2520diverse%2520acquisition%2520protocols%252C%2520and%2520varying%2520sequence%2520parameters%2520introduce%2520substantial%2520domain%2520shifts%2520that%2520obscure%2520underlying%2520biological%2520signals.%2520Data%2520harmonization%2520methods%2520aim%2520to%2520reduce%2520these%2520instrumental%2520and%2520acquisition%2520variability%252C%2520but%2520existing%2520approaches%2520remain%2520insufficient.%2520When%2520applied%2520to%2520imaging%2520data%252C%2520image-based%2520harmonization%2520approaches%2520are%2520often%2520restricted%2520by%2520the%2520need%2520for%2520target%2520images%252C%2520while%2520existing%2520text-guided%2520methods%2520rely%2520on%2520simplistic%2520labels%2520that%2520fail%2520to%2520capture%2520complex%2520acquisition%2520details%2520or%2520are%2520typically%2520restricted%2520to%2520datasets%2520with%2520limited%2520variability%252C%2520failing%2520to%2520capture%2520the%2520heterogeneity%2520of%2520real-world%2520clinical%2520environments.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520DIST-CLIP%2520%2528Disentangled%2520Style%2520Transfer%2520with%2520CLIP%2520Guidance%2529%252C%2520a%2520unified%2520framework%2520for%2520MRI%2520harmonization%2520that%2520flexibly%2520uses%2520either%2520target%2520images%2520or%2520DICOM%2520metadata%2520for%2520guidance.%2520Our%2520framework%2520explicitly%2520disentangles%2520anatomical%2520content%2520from%2520image%2520contrast%252C%2520with%2520the%2520contrast%2520representations%2520being%2520extracted%2520using%2520pre-trained%2520CLIP%2520encoders.%2520These%2520contrast%2520embeddings%2520are%2520then%2520integrated%2520into%2520the%2520anatomical%2520content%2520via%2520a%2520novel%2520Adaptive%2520Style%2520Transfer%2520module.%2520We%2520trained%2520and%2520evaluated%2520DIST-CLIP%2520on%2520diverse%2520real-world%2520clinical%2520datasets%252C%2520and%2520showed%2520significant%2520improvements%2520in%2520performance%2520when%2520compared%2520against%2520state-of-the-art%2520methods%2520in%2520both%2520style%2520translation%2520fidelity%2520and%2520anatomical%2520preservation%252C%2520offering%2520a%2520flexible%2520solution%2520for%2520style%2520transfer%2520and%2520standardizing%2520MRI%2520data.%2520Our%2520code%2520and%2520weights%2520will%2520be%2520made%2520publicly%2520available%2520upon%2520publication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIST-CLIP%3A%20Arbitrary%20Metadata%20and%20Image%20Guided%20MRI%20Harmonization%20via%20Disentangled%20Anatomy-Contrast%20Representations&entry.906535625=Mehmet%20Yigit%20Avci%20and%20Pedro%20Borges%20and%20Virginia%20Fernandez%20and%20Paul%20Wright%20and%20Mehmet%20Yigitsoy%20and%20Sebastien%20Ourselin%20and%20Jorge%20Cardoso&entry.1292438233=Deep%20learning%20holds%20immense%20promise%20for%20transforming%20medical%20image%20analysis%2C%20yet%20its%20clinical%20generalization%20remains%20profoundly%20limited.%20A%20major%20barrier%20is%20data%20heterogeneity.%20This%20is%20particularly%20true%20in%20Magnetic%20Resonance%20Imaging%2C%20where%20scanner%20hardware%20differences%2C%20diverse%20acquisition%20protocols%2C%20and%20varying%20sequence%20parameters%20introduce%20substantial%20domain%20shifts%20that%20obscure%20underlying%20biological%20signals.%20Data%20harmonization%20methods%20aim%20to%20reduce%20these%20instrumental%20and%20acquisition%20variability%2C%20but%20existing%20approaches%20remain%20insufficient.%20When%20applied%20to%20imaging%20data%2C%20image-based%20harmonization%20approaches%20are%20often%20restricted%20by%20the%20need%20for%20target%20images%2C%20while%20existing%20text-guided%20methods%20rely%20on%20simplistic%20labels%20that%20fail%20to%20capture%20complex%20acquisition%20details%20or%20are%20typically%20restricted%20to%20datasets%20with%20limited%20variability%2C%20failing%20to%20capture%20the%20heterogeneity%20of%20real-world%20clinical%20environments.%20To%20address%20these%20limitations%2C%20we%20propose%20DIST-CLIP%20%28Disentangled%20Style%20Transfer%20with%20CLIP%20Guidance%29%2C%20a%20unified%20framework%20for%20MRI%20harmonization%20that%20flexibly%20uses%20either%20target%20images%20or%20DICOM%20metadata%20for%20guidance.%20Our%20framework%20explicitly%20disentangles%20anatomical%20content%20from%20image%20contrast%2C%20with%20the%20contrast%20representations%20being%20extracted%20using%20pre-trained%20CLIP%20encoders.%20These%20contrast%20embeddings%20are%20then%20integrated%20into%20the%20anatomical%20content%20via%20a%20novel%20Adaptive%20Style%20Transfer%20module.%20We%20trained%20and%20evaluated%20DIST-CLIP%20on%20diverse%20real-world%20clinical%20datasets%2C%20and%20showed%20significant%20improvements%20in%20performance%20when%20compared%20against%20state-of-the-art%20methods%20in%20both%20style%20translation%20fidelity%20and%20anatomical%20preservation%2C%20offering%20a%20flexible%20solution%20for%20style%20transfer%20and%20standardizing%20MRI%20data.%20Our%20code%20and%20weights%20will%20be%20made%20publicly%20available%20upon%20publication.&entry.1838667208=http%3A//arxiv.org/abs/2512.07674v1&entry.124074799=Read"},
{"title": "RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models", "author": "Xiqiao Xiong and Ouxiang Li and Zhuo Liu and Moxin Li and Wentao Shi and Fuli Feng and Xiangnan He", "abstract": "Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.", "link": "http://arxiv.org/abs/2512.07761v1", "date": "2025-12-08", "relevancy": 2.2783, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4559}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4558}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RL-MTJail%3A%20Reinforcement%20Learning%20for%20Automated%20Black-Box%20Multi-Turn%20Jailbreaking%20of%20Large%20Language%20Models&body=Title%3A%20RL-MTJail%3A%20Reinforcement%20Learning%20for%20Automated%20Black-Box%20Multi-Turn%20Jailbreaking%20of%20Large%20Language%20Models%0AAuthor%3A%20Xiqiao%20Xiong%20and%20Ouxiang%20Li%20and%20Zhuo%20Liu%20and%20Moxin%20Li%20and%20Wentao%20Shi%20and%20Fuli%20Feng%20and%20Xiangnan%20He%0AAbstract%3A%20Large%20language%20models%20are%20vulnerable%20to%20jailbreak%20attacks%2C%20threatening%20their%20safe%20deployment%20in%20real-world%20applications.%20This%20paper%20studies%20black-box%20multi-turn%20jailbreaks%2C%20aiming%20to%20train%20attacker%20LLMs%20to%20elicit%20harmful%20content%20from%20black-box%20models%20through%20a%20sequence%20of%20prompt-output%20interactions.%20Existing%20approaches%20typically%20rely%20on%20single%20turn%20optimization%2C%20which%20is%20insufficient%20for%20learning%20long-term%20attack%20strategies.%20To%20bridge%20this%20gap%2C%20we%20formulate%20the%20problem%20as%20a%20multi-turn%20reinforcement%20learning%20task%2C%20directly%20optimizing%20the%20harmfulness%20of%20the%20final-turn%20output%20as%20the%20outcome%20reward.%20To%20mitigate%20sparse%20supervision%20and%20promote%20long-term%20attack%20strategies%2C%20we%20propose%20two%20heuristic%20process%20rewards%3A%20%281%29%20controlling%20the%20harmfulness%20of%20intermediate%20outputs%20to%20prevent%20triggering%20the%20black-box%20model%27s%20rejection%20mechanisms%2C%20and%20%282%29%20maintaining%20the%20semantic%20relevance%20of%20intermediate%20outputs%20to%20avoid%20drifting%20into%20irrelevant%20content.%20Experimental%20results%20on%20multiple%20benchmarks%20show%20consistently%20improved%20attack%20success%20rates%20across%20multiple%20models%2C%20highlighting%20the%20effectiveness%20of%20our%20approach.%20The%20code%20is%20available%20at%20https%3A//github.com/xxiqiao/RL-MTJail.%20Warning%3A%20This%20paper%20contains%20examples%20of%20harmful%20content.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRL-MTJail%253A%2520Reinforcement%2520Learning%2520for%2520Automated%2520Black-Box%2520Multi-Turn%2520Jailbreaking%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DXiqiao%2520Xiong%2520and%2520Ouxiang%2520Li%2520and%2520Zhuo%2520Liu%2520and%2520Moxin%2520Li%2520and%2520Wentao%2520Shi%2520and%2520Fuli%2520Feng%2520and%2520Xiangnan%2520He%26entry.1292438233%3DLarge%2520language%2520models%2520are%2520vulnerable%2520to%2520jailbreak%2520attacks%252C%2520threatening%2520their%2520safe%2520deployment%2520in%2520real-world%2520applications.%2520This%2520paper%2520studies%2520black-box%2520multi-turn%2520jailbreaks%252C%2520aiming%2520to%2520train%2520attacker%2520LLMs%2520to%2520elicit%2520harmful%2520content%2520from%2520black-box%2520models%2520through%2520a%2520sequence%2520of%2520prompt-output%2520interactions.%2520Existing%2520approaches%2520typically%2520rely%2520on%2520single%2520turn%2520optimization%252C%2520which%2520is%2520insufficient%2520for%2520learning%2520long-term%2520attack%2520strategies.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520formulate%2520the%2520problem%2520as%2520a%2520multi-turn%2520reinforcement%2520learning%2520task%252C%2520directly%2520optimizing%2520the%2520harmfulness%2520of%2520the%2520final-turn%2520output%2520as%2520the%2520outcome%2520reward.%2520To%2520mitigate%2520sparse%2520supervision%2520and%2520promote%2520long-term%2520attack%2520strategies%252C%2520we%2520propose%2520two%2520heuristic%2520process%2520rewards%253A%2520%25281%2529%2520controlling%2520the%2520harmfulness%2520of%2520intermediate%2520outputs%2520to%2520prevent%2520triggering%2520the%2520black-box%2520model%2527s%2520rejection%2520mechanisms%252C%2520and%2520%25282%2529%2520maintaining%2520the%2520semantic%2520relevance%2520of%2520intermediate%2520outputs%2520to%2520avoid%2520drifting%2520into%2520irrelevant%2520content.%2520Experimental%2520results%2520on%2520multiple%2520benchmarks%2520show%2520consistently%2520improved%2520attack%2520success%2520rates%2520across%2520multiple%2520models%252C%2520highlighting%2520the%2520effectiveness%2520of%2520our%2520approach.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/xxiqiao/RL-MTJail.%2520Warning%253A%2520This%2520paper%2520contains%2520examples%2520of%2520harmful%2520content.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RL-MTJail%3A%20Reinforcement%20Learning%20for%20Automated%20Black-Box%20Multi-Turn%20Jailbreaking%20of%20Large%20Language%20Models&entry.906535625=Xiqiao%20Xiong%20and%20Ouxiang%20Li%20and%20Zhuo%20Liu%20and%20Moxin%20Li%20and%20Wentao%20Shi%20and%20Fuli%20Feng%20and%20Xiangnan%20He&entry.1292438233=Large%20language%20models%20are%20vulnerable%20to%20jailbreak%20attacks%2C%20threatening%20their%20safe%20deployment%20in%20real-world%20applications.%20This%20paper%20studies%20black-box%20multi-turn%20jailbreaks%2C%20aiming%20to%20train%20attacker%20LLMs%20to%20elicit%20harmful%20content%20from%20black-box%20models%20through%20a%20sequence%20of%20prompt-output%20interactions.%20Existing%20approaches%20typically%20rely%20on%20single%20turn%20optimization%2C%20which%20is%20insufficient%20for%20learning%20long-term%20attack%20strategies.%20To%20bridge%20this%20gap%2C%20we%20formulate%20the%20problem%20as%20a%20multi-turn%20reinforcement%20learning%20task%2C%20directly%20optimizing%20the%20harmfulness%20of%20the%20final-turn%20output%20as%20the%20outcome%20reward.%20To%20mitigate%20sparse%20supervision%20and%20promote%20long-term%20attack%20strategies%2C%20we%20propose%20two%20heuristic%20process%20rewards%3A%20%281%29%20controlling%20the%20harmfulness%20of%20intermediate%20outputs%20to%20prevent%20triggering%20the%20black-box%20model%27s%20rejection%20mechanisms%2C%20and%20%282%29%20maintaining%20the%20semantic%20relevance%20of%20intermediate%20outputs%20to%20avoid%20drifting%20into%20irrelevant%20content.%20Experimental%20results%20on%20multiple%20benchmarks%20show%20consistently%20improved%20attack%20success%20rates%20across%20multiple%20models%2C%20highlighting%20the%20effectiveness%20of%20our%20approach.%20The%20code%20is%20available%20at%20https%3A//github.com/xxiqiao/RL-MTJail.%20Warning%3A%20This%20paper%20contains%20examples%20of%20harmful%20content.&entry.1838667208=http%3A//arxiv.org/abs/2512.07761v1&entry.124074799=Read"},
{"title": "Bimodal SegNet: Instance Segmentation Fusing Events and RGB Frames for Robotic Grasping", "author": "Sanket Kachole and Xiaoqian Huang and Fariborz Baghaei Naeini and Rajkumar Muthusamy and Dimitrios Makris and Yahya Zweiri", "abstract": "Object segmentation for robotic grasping under dynamic conditions often faces challenges such as occlusion, low light conditions, motion blur and object size variance. To address these challenges, we propose a Deep Learning network that fuses two types of visual signals, event-based data and RGB frame data. The proposed Bimodal SegNet network has two distinct encoders, one for each signal input and a spatial pyramidal pooling with atrous convolutions. Encoders capture rich contextual information by pooling the concatenated features at different resolutions while the decoder obtains sharp object boundaries. The evaluation of the proposed method undertakes five unique image degradation challenges including occlusion, blur, brightness, trajectory and scale variance on the Event-based Segmentation (ESD) Dataset. The evaluation results show a 6-10\\% segmentation accuracy improvement over state-of-the-art methods in terms of mean intersection over the union and pixel accuracy. The model code is available at https://github.com/sanket0707/Bimodal-SegNet.git", "link": "http://arxiv.org/abs/2303.11228v3", "date": "2025-12-08", "relevancy": 2.2665, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5817}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5777}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bimodal%20SegNet%3A%20Instance%20Segmentation%20Fusing%20Events%20and%20RGB%20Frames%20for%20Robotic%20Grasping&body=Title%3A%20Bimodal%20SegNet%3A%20Instance%20Segmentation%20Fusing%20Events%20and%20RGB%20Frames%20for%20Robotic%20Grasping%0AAuthor%3A%20Sanket%20Kachole%20and%20Xiaoqian%20Huang%20and%20Fariborz%20Baghaei%20Naeini%20and%20Rajkumar%20Muthusamy%20and%20Dimitrios%20Makris%20and%20Yahya%20Zweiri%0AAbstract%3A%20Object%20segmentation%20for%20robotic%20grasping%20under%20dynamic%20conditions%20often%20faces%20challenges%20such%20as%20occlusion%2C%20low%20light%20conditions%2C%20motion%20blur%20and%20object%20size%20variance.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20Deep%20Learning%20network%20that%20fuses%20two%20types%20of%20visual%20signals%2C%20event-based%20data%20and%20RGB%20frame%20data.%20The%20proposed%20Bimodal%20SegNet%20network%20has%20two%20distinct%20encoders%2C%20one%20for%20each%20signal%20input%20and%20a%20spatial%20pyramidal%20pooling%20with%20atrous%20convolutions.%20Encoders%20capture%20rich%20contextual%20information%20by%20pooling%20the%20concatenated%20features%20at%20different%20resolutions%20while%20the%20decoder%20obtains%20sharp%20object%20boundaries.%20The%20evaluation%20of%20the%20proposed%20method%20undertakes%20five%20unique%20image%20degradation%20challenges%20including%20occlusion%2C%20blur%2C%20brightness%2C%20trajectory%20and%20scale%20variance%20on%20the%20Event-based%20Segmentation%20%28ESD%29%20Dataset.%20The%20evaluation%20results%20show%20a%206-10%5C%25%20segmentation%20accuracy%20improvement%20over%20state-of-the-art%20methods%20in%20terms%20of%20mean%20intersection%20over%20the%20union%20and%20pixel%20accuracy.%20The%20model%20code%20is%20available%20at%20https%3A//github.com/sanket0707/Bimodal-SegNet.git%0ALink%3A%20http%3A//arxiv.org/abs/2303.11228v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBimodal%2520SegNet%253A%2520Instance%2520Segmentation%2520Fusing%2520Events%2520and%2520RGB%2520Frames%2520for%2520Robotic%2520Grasping%26entry.906535625%3DSanket%2520Kachole%2520and%2520Xiaoqian%2520Huang%2520and%2520Fariborz%2520Baghaei%2520Naeini%2520and%2520Rajkumar%2520Muthusamy%2520and%2520Dimitrios%2520Makris%2520and%2520Yahya%2520Zweiri%26entry.1292438233%3DObject%2520segmentation%2520for%2520robotic%2520grasping%2520under%2520dynamic%2520conditions%2520often%2520faces%2520challenges%2520such%2520as%2520occlusion%252C%2520low%2520light%2520conditions%252C%2520motion%2520blur%2520and%2520object%2520size%2520variance.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520Deep%2520Learning%2520network%2520that%2520fuses%2520two%2520types%2520of%2520visual%2520signals%252C%2520event-based%2520data%2520and%2520RGB%2520frame%2520data.%2520The%2520proposed%2520Bimodal%2520SegNet%2520network%2520has%2520two%2520distinct%2520encoders%252C%2520one%2520for%2520each%2520signal%2520input%2520and%2520a%2520spatial%2520pyramidal%2520pooling%2520with%2520atrous%2520convolutions.%2520Encoders%2520capture%2520rich%2520contextual%2520information%2520by%2520pooling%2520the%2520concatenated%2520features%2520at%2520different%2520resolutions%2520while%2520the%2520decoder%2520obtains%2520sharp%2520object%2520boundaries.%2520The%2520evaluation%2520of%2520the%2520proposed%2520method%2520undertakes%2520five%2520unique%2520image%2520degradation%2520challenges%2520including%2520occlusion%252C%2520blur%252C%2520brightness%252C%2520trajectory%2520and%2520scale%2520variance%2520on%2520the%2520Event-based%2520Segmentation%2520%2528ESD%2529%2520Dataset.%2520The%2520evaluation%2520results%2520show%2520a%25206-10%255C%2525%2520segmentation%2520accuracy%2520improvement%2520over%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520mean%2520intersection%2520over%2520the%2520union%2520and%2520pixel%2520accuracy.%2520The%2520model%2520code%2520is%2520available%2520at%2520https%253A//github.com/sanket0707/Bimodal-SegNet.git%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.11228v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bimodal%20SegNet%3A%20Instance%20Segmentation%20Fusing%20Events%20and%20RGB%20Frames%20for%20Robotic%20Grasping&entry.906535625=Sanket%20Kachole%20and%20Xiaoqian%20Huang%20and%20Fariborz%20Baghaei%20Naeini%20and%20Rajkumar%20Muthusamy%20and%20Dimitrios%20Makris%20and%20Yahya%20Zweiri&entry.1292438233=Object%20segmentation%20for%20robotic%20grasping%20under%20dynamic%20conditions%20often%20faces%20challenges%20such%20as%20occlusion%2C%20low%20light%20conditions%2C%20motion%20blur%20and%20object%20size%20variance.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20Deep%20Learning%20network%20that%20fuses%20two%20types%20of%20visual%20signals%2C%20event-based%20data%20and%20RGB%20frame%20data.%20The%20proposed%20Bimodal%20SegNet%20network%20has%20two%20distinct%20encoders%2C%20one%20for%20each%20signal%20input%20and%20a%20spatial%20pyramidal%20pooling%20with%20atrous%20convolutions.%20Encoders%20capture%20rich%20contextual%20information%20by%20pooling%20the%20concatenated%20features%20at%20different%20resolutions%20while%20the%20decoder%20obtains%20sharp%20object%20boundaries.%20The%20evaluation%20of%20the%20proposed%20method%20undertakes%20five%20unique%20image%20degradation%20challenges%20including%20occlusion%2C%20blur%2C%20brightness%2C%20trajectory%20and%20scale%20variance%20on%20the%20Event-based%20Segmentation%20%28ESD%29%20Dataset.%20The%20evaluation%20results%20show%20a%206-10%5C%25%20segmentation%20accuracy%20improvement%20over%20state-of-the-art%20methods%20in%20terms%20of%20mean%20intersection%20over%20the%20union%20and%20pixel%20accuracy.%20The%20model%20code%20is%20available%20at%20https%3A//github.com/sanket0707/Bimodal-SegNet.git&entry.1838667208=http%3A//arxiv.org/abs/2303.11228v3&entry.124074799=Read"},
{"title": "Building Gradient by Gradient: Decentralised Energy Functions for Bimanual Robot Assembly", "author": "Alexander L. Mitchell and Joe Watson and Ingmar Posner", "abstract": "There are many challenges in bimanual assembly, including high-level sequencing, multi-robot coordination, and low-level, contact-rich operations such as component mating. Task and motion planning (TAMP) methods, while effective in this domain, may be prohibitively slow to converge when adapting to disturbances that require new task sequencing and optimisation. These events are common during tight-tolerance assembly, where difficult-to-model dynamics such as friction or deformation require rapid replanning and reattempts. Moreover, defining explicit task sequences for assembly can be cumbersome, limiting flexibility when task replanning is required. To simplify this planning, we introduce a decentralised gradient-based framework that uses a piecewise continuous energy function through the automatic composition of adaptive potential functions. This approach generates sub-goals using only myopic optimisation, rather than long-horizon planning. It demonstrates effectiveness at solving long-horizon tasks due to the structure and adaptivity of the energy function. We show that our approach scales to physical bimanual assembly tasks for constructing tight-tolerance assemblies. In these experiments, we discover that our gradient-based rapid replanning framework generates automatic retries, coordinated motions and autonomous handovers in an emergent fashion.", "link": "http://arxiv.org/abs/2510.04696v2", "date": "2025-12-08", "relevancy": 2.2622, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5848}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5566}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Building%20Gradient%20by%20Gradient%3A%20Decentralised%20Energy%20Functions%20for%20Bimanual%20Robot%20Assembly&body=Title%3A%20Building%20Gradient%20by%20Gradient%3A%20Decentralised%20Energy%20Functions%20for%20Bimanual%20Robot%20Assembly%0AAuthor%3A%20Alexander%20L.%20Mitchell%20and%20Joe%20Watson%20and%20Ingmar%20Posner%0AAbstract%3A%20There%20are%20many%20challenges%20in%20bimanual%20assembly%2C%20including%20high-level%20sequencing%2C%20multi-robot%20coordination%2C%20and%20low-level%2C%20contact-rich%20operations%20such%20as%20component%20mating.%20Task%20and%20motion%20planning%20%28TAMP%29%20methods%2C%20while%20effective%20in%20this%20domain%2C%20may%20be%20prohibitively%20slow%20to%20converge%20when%20adapting%20to%20disturbances%20that%20require%20new%20task%20sequencing%20and%20optimisation.%20These%20events%20are%20common%20during%20tight-tolerance%20assembly%2C%20where%20difficult-to-model%20dynamics%20such%20as%20friction%20or%20deformation%20require%20rapid%20replanning%20and%20reattempts.%20Moreover%2C%20defining%20explicit%20task%20sequences%20for%20assembly%20can%20be%20cumbersome%2C%20limiting%20flexibility%20when%20task%20replanning%20is%20required.%20To%20simplify%20this%20planning%2C%20we%20introduce%20a%20decentralised%20gradient-based%20framework%20that%20uses%20a%20piecewise%20continuous%20energy%20function%20through%20the%20automatic%20composition%20of%20adaptive%20potential%20functions.%20This%20approach%20generates%20sub-goals%20using%20only%20myopic%20optimisation%2C%20rather%20than%20long-horizon%20planning.%20It%20demonstrates%20effectiveness%20at%20solving%20long-horizon%20tasks%20due%20to%20the%20structure%20and%20adaptivity%20of%20the%20energy%20function.%20We%20show%20that%20our%20approach%20scales%20to%20physical%20bimanual%20assembly%20tasks%20for%20constructing%20tight-tolerance%20assemblies.%20In%20these%20experiments%2C%20we%20discover%20that%20our%20gradient-based%20rapid%20replanning%20framework%20generates%20automatic%20retries%2C%20coordinated%20motions%20and%20autonomous%20handovers%20in%20an%20emergent%20fashion.%0ALink%3A%20http%3A//arxiv.org/abs/2510.04696v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBuilding%2520Gradient%2520by%2520Gradient%253A%2520Decentralised%2520Energy%2520Functions%2520for%2520Bimanual%2520Robot%2520Assembly%26entry.906535625%3DAlexander%2520L.%2520Mitchell%2520and%2520Joe%2520Watson%2520and%2520Ingmar%2520Posner%26entry.1292438233%3DThere%2520are%2520many%2520challenges%2520in%2520bimanual%2520assembly%252C%2520including%2520high-level%2520sequencing%252C%2520multi-robot%2520coordination%252C%2520and%2520low-level%252C%2520contact-rich%2520operations%2520such%2520as%2520component%2520mating.%2520Task%2520and%2520motion%2520planning%2520%2528TAMP%2529%2520methods%252C%2520while%2520effective%2520in%2520this%2520domain%252C%2520may%2520be%2520prohibitively%2520slow%2520to%2520converge%2520when%2520adapting%2520to%2520disturbances%2520that%2520require%2520new%2520task%2520sequencing%2520and%2520optimisation.%2520These%2520events%2520are%2520common%2520during%2520tight-tolerance%2520assembly%252C%2520where%2520difficult-to-model%2520dynamics%2520such%2520as%2520friction%2520or%2520deformation%2520require%2520rapid%2520replanning%2520and%2520reattempts.%2520Moreover%252C%2520defining%2520explicit%2520task%2520sequences%2520for%2520assembly%2520can%2520be%2520cumbersome%252C%2520limiting%2520flexibility%2520when%2520task%2520replanning%2520is%2520required.%2520To%2520simplify%2520this%2520planning%252C%2520we%2520introduce%2520a%2520decentralised%2520gradient-based%2520framework%2520that%2520uses%2520a%2520piecewise%2520continuous%2520energy%2520function%2520through%2520the%2520automatic%2520composition%2520of%2520adaptive%2520potential%2520functions.%2520This%2520approach%2520generates%2520sub-goals%2520using%2520only%2520myopic%2520optimisation%252C%2520rather%2520than%2520long-horizon%2520planning.%2520It%2520demonstrates%2520effectiveness%2520at%2520solving%2520long-horizon%2520tasks%2520due%2520to%2520the%2520structure%2520and%2520adaptivity%2520of%2520the%2520energy%2520function.%2520We%2520show%2520that%2520our%2520approach%2520scales%2520to%2520physical%2520bimanual%2520assembly%2520tasks%2520for%2520constructing%2520tight-tolerance%2520assemblies.%2520In%2520these%2520experiments%252C%2520we%2520discover%2520that%2520our%2520gradient-based%2520rapid%2520replanning%2520framework%2520generates%2520automatic%2520retries%252C%2520coordinated%2520motions%2520and%2520autonomous%2520handovers%2520in%2520an%2520emergent%2520fashion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04696v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Building%20Gradient%20by%20Gradient%3A%20Decentralised%20Energy%20Functions%20for%20Bimanual%20Robot%20Assembly&entry.906535625=Alexander%20L.%20Mitchell%20and%20Joe%20Watson%20and%20Ingmar%20Posner&entry.1292438233=There%20are%20many%20challenges%20in%20bimanual%20assembly%2C%20including%20high-level%20sequencing%2C%20multi-robot%20coordination%2C%20and%20low-level%2C%20contact-rich%20operations%20such%20as%20component%20mating.%20Task%20and%20motion%20planning%20%28TAMP%29%20methods%2C%20while%20effective%20in%20this%20domain%2C%20may%20be%20prohibitively%20slow%20to%20converge%20when%20adapting%20to%20disturbances%20that%20require%20new%20task%20sequencing%20and%20optimisation.%20These%20events%20are%20common%20during%20tight-tolerance%20assembly%2C%20where%20difficult-to-model%20dynamics%20such%20as%20friction%20or%20deformation%20require%20rapid%20replanning%20and%20reattempts.%20Moreover%2C%20defining%20explicit%20task%20sequences%20for%20assembly%20can%20be%20cumbersome%2C%20limiting%20flexibility%20when%20task%20replanning%20is%20required.%20To%20simplify%20this%20planning%2C%20we%20introduce%20a%20decentralised%20gradient-based%20framework%20that%20uses%20a%20piecewise%20continuous%20energy%20function%20through%20the%20automatic%20composition%20of%20adaptive%20potential%20functions.%20This%20approach%20generates%20sub-goals%20using%20only%20myopic%20optimisation%2C%20rather%20than%20long-horizon%20planning.%20It%20demonstrates%20effectiveness%20at%20solving%20long-horizon%20tasks%20due%20to%20the%20structure%20and%20adaptivity%20of%20the%20energy%20function.%20We%20show%20that%20our%20approach%20scales%20to%20physical%20bimanual%20assembly%20tasks%20for%20constructing%20tight-tolerance%20assemblies.%20In%20these%20experiments%2C%20we%20discover%20that%20our%20gradient-based%20rapid%20replanning%20framework%20generates%20automatic%20retries%2C%20coordinated%20motions%20and%20autonomous%20handovers%20in%20an%20emergent%20fashion.&entry.1838667208=http%3A//arxiv.org/abs/2510.04696v2&entry.124074799=Read"},
{"title": "Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse", "author": "Giulia Lanzillotta and Damiano Meier and Thomas Hofmann", "abstract": "A persistent paradox in continual learning (CL) is that neural networks often retain linearly separable representations of past tasks even when their output predictions fail. We formalize this distinction as the gap between deep feature-space and shallow classifier-level forgetting. We reveal a critical asymmetry in Experience Replay: while minimal buffers successfully anchor feature geometry and prevent deep forgetting, mitigating shallow forgetting typically requires substantially larger buffer capacities. To explain this, we extend the Neural Collapse framework to the sequential setting. We characterize deep forgetting as a geometric drift toward out-of-distribution subspaces and prove that any non-zero replay fraction asymptotically guarantees the retention of linear separability. Conversely, we identify that the \"strong collapse\" induced by small buffers leads to rank-deficient covariances and inflated class means, effectively blinding the classifier to true population boundaries. By unifying CL with out-of-distribution detection, our work challenges the prevailing reliance on large buffers, suggesting that explicitly correcting these statistical artifacts could unlock robust performance with minimal replay.", "link": "http://arxiv.org/abs/2512.07400v1", "date": "2025-12-08", "relevancy": 2.2611, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4595}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4563}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asymptotic%20analysis%20of%20shallow%20and%20deep%20forgetting%20in%20replay%20with%20Neural%20Collapse&body=Title%3A%20Asymptotic%20analysis%20of%20shallow%20and%20deep%20forgetting%20in%20replay%20with%20Neural%20Collapse%0AAuthor%3A%20Giulia%20Lanzillotta%20and%20Damiano%20Meier%20and%20Thomas%20Hofmann%0AAbstract%3A%20A%20persistent%20paradox%20in%20continual%20learning%20%28CL%29%20is%20that%20neural%20networks%20often%20retain%20linearly%20separable%20representations%20of%20past%20tasks%20even%20when%20their%20output%20predictions%20fail.%20We%20formalize%20this%20distinction%20as%20the%20gap%20between%20deep%20feature-space%20and%20shallow%20classifier-level%20forgetting.%20We%20reveal%20a%20critical%20asymmetry%20in%20Experience%20Replay%3A%20while%20minimal%20buffers%20successfully%20anchor%20feature%20geometry%20and%20prevent%20deep%20forgetting%2C%20mitigating%20shallow%20forgetting%20typically%20requires%20substantially%20larger%20buffer%20capacities.%20To%20explain%20this%2C%20we%20extend%20the%20Neural%20Collapse%20framework%20to%20the%20sequential%20setting.%20We%20characterize%20deep%20forgetting%20as%20a%20geometric%20drift%20toward%20out-of-distribution%20subspaces%20and%20prove%20that%20any%20non-zero%20replay%20fraction%20asymptotically%20guarantees%20the%20retention%20of%20linear%20separability.%20Conversely%2C%20we%20identify%20that%20the%20%22strong%20collapse%22%20induced%20by%20small%20buffers%20leads%20to%20rank-deficient%20covariances%20and%20inflated%20class%20means%2C%20effectively%20blinding%20the%20classifier%20to%20true%20population%20boundaries.%20By%20unifying%20CL%20with%20out-of-distribution%20detection%2C%20our%20work%20challenges%20the%20prevailing%20reliance%20on%20large%20buffers%2C%20suggesting%20that%20explicitly%20correcting%20these%20statistical%20artifacts%20could%20unlock%20robust%20performance%20with%20minimal%20replay.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07400v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsymptotic%2520analysis%2520of%2520shallow%2520and%2520deep%2520forgetting%2520in%2520replay%2520with%2520Neural%2520Collapse%26entry.906535625%3DGiulia%2520Lanzillotta%2520and%2520Damiano%2520Meier%2520and%2520Thomas%2520Hofmann%26entry.1292438233%3DA%2520persistent%2520paradox%2520in%2520continual%2520learning%2520%2528CL%2529%2520is%2520that%2520neural%2520networks%2520often%2520retain%2520linearly%2520separable%2520representations%2520of%2520past%2520tasks%2520even%2520when%2520their%2520output%2520predictions%2520fail.%2520We%2520formalize%2520this%2520distinction%2520as%2520the%2520gap%2520between%2520deep%2520feature-space%2520and%2520shallow%2520classifier-level%2520forgetting.%2520We%2520reveal%2520a%2520critical%2520asymmetry%2520in%2520Experience%2520Replay%253A%2520while%2520minimal%2520buffers%2520successfully%2520anchor%2520feature%2520geometry%2520and%2520prevent%2520deep%2520forgetting%252C%2520mitigating%2520shallow%2520forgetting%2520typically%2520requires%2520substantially%2520larger%2520buffer%2520capacities.%2520To%2520explain%2520this%252C%2520we%2520extend%2520the%2520Neural%2520Collapse%2520framework%2520to%2520the%2520sequential%2520setting.%2520We%2520characterize%2520deep%2520forgetting%2520as%2520a%2520geometric%2520drift%2520toward%2520out-of-distribution%2520subspaces%2520and%2520prove%2520that%2520any%2520non-zero%2520replay%2520fraction%2520asymptotically%2520guarantees%2520the%2520retention%2520of%2520linear%2520separability.%2520Conversely%252C%2520we%2520identify%2520that%2520the%2520%2522strong%2520collapse%2522%2520induced%2520by%2520small%2520buffers%2520leads%2520to%2520rank-deficient%2520covariances%2520and%2520inflated%2520class%2520means%252C%2520effectively%2520blinding%2520the%2520classifier%2520to%2520true%2520population%2520boundaries.%2520By%2520unifying%2520CL%2520with%2520out-of-distribution%2520detection%252C%2520our%2520work%2520challenges%2520the%2520prevailing%2520reliance%2520on%2520large%2520buffers%252C%2520suggesting%2520that%2520explicitly%2520correcting%2520these%2520statistical%2520artifacts%2520could%2520unlock%2520robust%2520performance%2520with%2520minimal%2520replay.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07400v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asymptotic%20analysis%20of%20shallow%20and%20deep%20forgetting%20in%20replay%20with%20Neural%20Collapse&entry.906535625=Giulia%20Lanzillotta%20and%20Damiano%20Meier%20and%20Thomas%20Hofmann&entry.1292438233=A%20persistent%20paradox%20in%20continual%20learning%20%28CL%29%20is%20that%20neural%20networks%20often%20retain%20linearly%20separable%20representations%20of%20past%20tasks%20even%20when%20their%20output%20predictions%20fail.%20We%20formalize%20this%20distinction%20as%20the%20gap%20between%20deep%20feature-space%20and%20shallow%20classifier-level%20forgetting.%20We%20reveal%20a%20critical%20asymmetry%20in%20Experience%20Replay%3A%20while%20minimal%20buffers%20successfully%20anchor%20feature%20geometry%20and%20prevent%20deep%20forgetting%2C%20mitigating%20shallow%20forgetting%20typically%20requires%20substantially%20larger%20buffer%20capacities.%20To%20explain%20this%2C%20we%20extend%20the%20Neural%20Collapse%20framework%20to%20the%20sequential%20setting.%20We%20characterize%20deep%20forgetting%20as%20a%20geometric%20drift%20toward%20out-of-distribution%20subspaces%20and%20prove%20that%20any%20non-zero%20replay%20fraction%20asymptotically%20guarantees%20the%20retention%20of%20linear%20separability.%20Conversely%2C%20we%20identify%20that%20the%20%22strong%20collapse%22%20induced%20by%20small%20buffers%20leads%20to%20rank-deficient%20covariances%20and%20inflated%20class%20means%2C%20effectively%20blinding%20the%20classifier%20to%20true%20population%20boundaries.%20By%20unifying%20CL%20with%20out-of-distribution%20detection%2C%20our%20work%20challenges%20the%20prevailing%20reliance%20on%20large%20buffers%2C%20suggesting%20that%20explicitly%20correcting%20these%20statistical%20artifacts%20could%20unlock%20robust%20performance%20with%20minimal%20replay.&entry.1838667208=http%3A//arxiv.org/abs/2512.07400v1&entry.124074799=Read"},
{"title": "DCoAR: Deep Concept Injection into Unified Autoregressive Models for Personalized Text-to-Image Generation", "author": "Fangtai Wu and Mushui Liu and Weijie He and Zhao Wang and Yunlong Yu", "abstract": "The unified autoregressive (AR) model excels at multimodal understanding and generation. However, its full potential in the domain of customized image generation has yet to be fully realized. Existing customization approaches for unified AR models face a fundamental dilemma: adaptation-based methods suffer from overfitting and scalability bottlenecks, while concept-injection paradigms are constrained by a shallow injection strategy that leads to poor visual fidelity and impaired re-contextualization. To address this, we propose DCoAR, a novel deep concept injection framework that maintains a completely frozen pre-trained model. DCoAR deeply integrates new concepts through a Layer-wise Multimodal Context Learning (LMCL) strategy, which is stabilized by a multi-faceted regularization scheme: a Dual Prior Preservation (DPP) loss to mitigate semantic drift and a Context-Aware Self-Regularization (CASR) loss to enhance re-contextualization. The framework also enables training-free subject customization in user-provided styles. Experiments demonstrate that DCoAR significantly outperforms previous injection-based methods and achieves performance competitive with adaptation-based approaches while requiring substantially fewer trainable parameters. Code: https://github.com/KZF-kzf/CoAR", "link": "http://arxiv.org/abs/2508.07341v2", "date": "2025-12-08", "relevancy": 2.2517, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5874}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5696}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DCoAR%3A%20Deep%20Concept%20Injection%20into%20Unified%20Autoregressive%20Models%20for%20Personalized%20Text-to-Image%20Generation&body=Title%3A%20DCoAR%3A%20Deep%20Concept%20Injection%20into%20Unified%20Autoregressive%20Models%20for%20Personalized%20Text-to-Image%20Generation%0AAuthor%3A%20Fangtai%20Wu%20and%20Mushui%20Liu%20and%20Weijie%20He%20and%20Zhao%20Wang%20and%20Yunlong%20Yu%0AAbstract%3A%20The%20unified%20autoregressive%20%28AR%29%20model%20excels%20at%20multimodal%20understanding%20and%20generation.%20However%2C%20its%20full%20potential%20in%20the%20domain%20of%20customized%20image%20generation%20has%20yet%20to%20be%20fully%20realized.%20Existing%20customization%20approaches%20for%20unified%20AR%20models%20face%20a%20fundamental%20dilemma%3A%20adaptation-based%20methods%20suffer%20from%20overfitting%20and%20scalability%20bottlenecks%2C%20while%20concept-injection%20paradigms%20are%20constrained%20by%20a%20shallow%20injection%20strategy%20that%20leads%20to%20poor%20visual%20fidelity%20and%20impaired%20re-contextualization.%20To%20address%20this%2C%20we%20propose%20DCoAR%2C%20a%20novel%20deep%20concept%20injection%20framework%20that%20maintains%20a%20completely%20frozen%20pre-trained%20model.%20DCoAR%20deeply%20integrates%20new%20concepts%20through%20a%20Layer-wise%20Multimodal%20Context%20Learning%20%28LMCL%29%20strategy%2C%20which%20is%20stabilized%20by%20a%20multi-faceted%20regularization%20scheme%3A%20a%20Dual%20Prior%20Preservation%20%28DPP%29%20loss%20to%20mitigate%20semantic%20drift%20and%20a%20Context-Aware%20Self-Regularization%20%28CASR%29%20loss%20to%20enhance%20re-contextualization.%20The%20framework%20also%20enables%20training-free%20subject%20customization%20in%20user-provided%20styles.%20Experiments%20demonstrate%20that%20DCoAR%20significantly%20outperforms%20previous%20injection-based%20methods%20and%20achieves%20performance%20competitive%20with%20adaptation-based%20approaches%20while%20requiring%20substantially%20fewer%20trainable%20parameters.%20Code%3A%20https%3A//github.com/KZF-kzf/CoAR%0ALink%3A%20http%3A//arxiv.org/abs/2508.07341v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDCoAR%253A%2520Deep%2520Concept%2520Injection%2520into%2520Unified%2520Autoregressive%2520Models%2520for%2520Personalized%2520Text-to-Image%2520Generation%26entry.906535625%3DFangtai%2520Wu%2520and%2520Mushui%2520Liu%2520and%2520Weijie%2520He%2520and%2520Zhao%2520Wang%2520and%2520Yunlong%2520Yu%26entry.1292438233%3DThe%2520unified%2520autoregressive%2520%2528AR%2529%2520model%2520excels%2520at%2520multimodal%2520understanding%2520and%2520generation.%2520However%252C%2520its%2520full%2520potential%2520in%2520the%2520domain%2520of%2520customized%2520image%2520generation%2520has%2520yet%2520to%2520be%2520fully%2520realized.%2520Existing%2520customization%2520approaches%2520for%2520unified%2520AR%2520models%2520face%2520a%2520fundamental%2520dilemma%253A%2520adaptation-based%2520methods%2520suffer%2520from%2520overfitting%2520and%2520scalability%2520bottlenecks%252C%2520while%2520concept-injection%2520paradigms%2520are%2520constrained%2520by%2520a%2520shallow%2520injection%2520strategy%2520that%2520leads%2520to%2520poor%2520visual%2520fidelity%2520and%2520impaired%2520re-contextualization.%2520To%2520address%2520this%252C%2520we%2520propose%2520DCoAR%252C%2520a%2520novel%2520deep%2520concept%2520injection%2520framework%2520that%2520maintains%2520a%2520completely%2520frozen%2520pre-trained%2520model.%2520DCoAR%2520deeply%2520integrates%2520new%2520concepts%2520through%2520a%2520Layer-wise%2520Multimodal%2520Context%2520Learning%2520%2528LMCL%2529%2520strategy%252C%2520which%2520is%2520stabilized%2520by%2520a%2520multi-faceted%2520regularization%2520scheme%253A%2520a%2520Dual%2520Prior%2520Preservation%2520%2528DPP%2529%2520loss%2520to%2520mitigate%2520semantic%2520drift%2520and%2520a%2520Context-Aware%2520Self-Regularization%2520%2528CASR%2529%2520loss%2520to%2520enhance%2520re-contextualization.%2520The%2520framework%2520also%2520enables%2520training-free%2520subject%2520customization%2520in%2520user-provided%2520styles.%2520Experiments%2520demonstrate%2520that%2520DCoAR%2520significantly%2520outperforms%2520previous%2520injection-based%2520methods%2520and%2520achieves%2520performance%2520competitive%2520with%2520adaptation-based%2520approaches%2520while%2520requiring%2520substantially%2520fewer%2520trainable%2520parameters.%2520Code%253A%2520https%253A//github.com/KZF-kzf/CoAR%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07341v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DCoAR%3A%20Deep%20Concept%20Injection%20into%20Unified%20Autoregressive%20Models%20for%20Personalized%20Text-to-Image%20Generation&entry.906535625=Fangtai%20Wu%20and%20Mushui%20Liu%20and%20Weijie%20He%20and%20Zhao%20Wang%20and%20Yunlong%20Yu&entry.1292438233=The%20unified%20autoregressive%20%28AR%29%20model%20excels%20at%20multimodal%20understanding%20and%20generation.%20However%2C%20its%20full%20potential%20in%20the%20domain%20of%20customized%20image%20generation%20has%20yet%20to%20be%20fully%20realized.%20Existing%20customization%20approaches%20for%20unified%20AR%20models%20face%20a%20fundamental%20dilemma%3A%20adaptation-based%20methods%20suffer%20from%20overfitting%20and%20scalability%20bottlenecks%2C%20while%20concept-injection%20paradigms%20are%20constrained%20by%20a%20shallow%20injection%20strategy%20that%20leads%20to%20poor%20visual%20fidelity%20and%20impaired%20re-contextualization.%20To%20address%20this%2C%20we%20propose%20DCoAR%2C%20a%20novel%20deep%20concept%20injection%20framework%20that%20maintains%20a%20completely%20frozen%20pre-trained%20model.%20DCoAR%20deeply%20integrates%20new%20concepts%20through%20a%20Layer-wise%20Multimodal%20Context%20Learning%20%28LMCL%29%20strategy%2C%20which%20is%20stabilized%20by%20a%20multi-faceted%20regularization%20scheme%3A%20a%20Dual%20Prior%20Preservation%20%28DPP%29%20loss%20to%20mitigate%20semantic%20drift%20and%20a%20Context-Aware%20Self-Regularization%20%28CASR%29%20loss%20to%20enhance%20re-contextualization.%20The%20framework%20also%20enables%20training-free%20subject%20customization%20in%20user-provided%20styles.%20Experiments%20demonstrate%20that%20DCoAR%20significantly%20outperforms%20previous%20injection-based%20methods%20and%20achieves%20performance%20competitive%20with%20adaptation-based%20approaches%20while%20requiring%20substantially%20fewer%20trainable%20parameters.%20Code%3A%20https%3A//github.com/KZF-kzf/CoAR&entry.1838667208=http%3A//arxiv.org/abs/2508.07341v2&entry.124074799=Read"},
{"title": "Toward Seamless Physical Human-Humanoid Interaction: Insights from Control, Intent, and Modeling with a Vision for What Comes Next", "author": "Gustavo A. Cardona and Shubham S. Kumbhar and Panagiotis Artemiadis", "abstract": "Physical Human-Humanoid Interaction (pHHI) is a rapidly advancing field with significant implications for deploying robots in unstructured, human-centric environments. In this review, we examine the current state of the art in pHHI through three core pillars: (i) humanoid modeling and control, (ii) human intent estimation, and (iii) computational human models. For each pillar, we survey representative approaches, identify open challenges, and analyze current limitations that hinder robust, scalable, and adaptive interaction. These include the need for whole-body control strategies capable of handling uncertain human dynamics, real-time intent inference under limited sensing, and modeling techniques that account for variability in human physical states. Although significant progress has been made within each domain, integration across pillars remains limited. We propose pathways for unifying methods across these areas to enable cohesive interaction frameworks. This structure enables us not only to map the current landscape but also to propose concrete directions for future research that aim to bridge these domains. Additionally, we introduce a unified taxonomy of interaction types based on modality, distinguishing between direct interactions (e.g., physical contact) and indirect interactions (e.g., object-mediated), and on the level of robot engagement, ranging from assistance to cooperation and collaboration. For each category in this taxonomy, we provide the three core pillars that highlight opportunities for cross-pillar unification. Our goal is to suggest avenues to advance robust, safe, and intuitive physical interaction, providing a roadmap for future research that will allow humanoid systems to effectively understand, anticipate, and collaborate with human partners in diverse real-world settings.", "link": "http://arxiv.org/abs/2512.07765v1", "date": "2025-12-08", "relevancy": 2.2418, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5778}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5537}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Seamless%20Physical%20Human-Humanoid%20Interaction%3A%20Insights%20from%20Control%2C%20Intent%2C%20and%20Modeling%20with%20a%20Vision%20for%20What%20Comes%20Next&body=Title%3A%20Toward%20Seamless%20Physical%20Human-Humanoid%20Interaction%3A%20Insights%20from%20Control%2C%20Intent%2C%20and%20Modeling%20with%20a%20Vision%20for%20What%20Comes%20Next%0AAuthor%3A%20Gustavo%20A.%20Cardona%20and%20Shubham%20S.%20Kumbhar%20and%20Panagiotis%20Artemiadis%0AAbstract%3A%20Physical%20Human-Humanoid%20Interaction%20%28pHHI%29%20is%20a%20rapidly%20advancing%20field%20with%20significant%20implications%20for%20deploying%20robots%20in%20unstructured%2C%20human-centric%20environments.%20In%20this%20review%2C%20we%20examine%20the%20current%20state%20of%20the%20art%20in%20pHHI%20through%20three%20core%20pillars%3A%20%28i%29%20humanoid%20modeling%20and%20control%2C%20%28ii%29%20human%20intent%20estimation%2C%20and%20%28iii%29%20computational%20human%20models.%20For%20each%20pillar%2C%20we%20survey%20representative%20approaches%2C%20identify%20open%20challenges%2C%20and%20analyze%20current%20limitations%20that%20hinder%20robust%2C%20scalable%2C%20and%20adaptive%20interaction.%20These%20include%20the%20need%20for%20whole-body%20control%20strategies%20capable%20of%20handling%20uncertain%20human%20dynamics%2C%20real-time%20intent%20inference%20under%20limited%20sensing%2C%20and%20modeling%20techniques%20that%20account%20for%20variability%20in%20human%20physical%20states.%20Although%20significant%20progress%20has%20been%20made%20within%20each%20domain%2C%20integration%20across%20pillars%20remains%20limited.%20We%20propose%20pathways%20for%20unifying%20methods%20across%20these%20areas%20to%20enable%20cohesive%20interaction%20frameworks.%20This%20structure%20enables%20us%20not%20only%20to%20map%20the%20current%20landscape%20but%20also%20to%20propose%20concrete%20directions%20for%20future%20research%20that%20aim%20to%20bridge%20these%20domains.%20Additionally%2C%20we%20introduce%20a%20unified%20taxonomy%20of%20interaction%20types%20based%20on%20modality%2C%20distinguishing%20between%20direct%20interactions%20%28e.g.%2C%20physical%20contact%29%20and%20indirect%20interactions%20%28e.g.%2C%20object-mediated%29%2C%20and%20on%20the%20level%20of%20robot%20engagement%2C%20ranging%20from%20assistance%20to%20cooperation%20and%20collaboration.%20For%20each%20category%20in%20this%20taxonomy%2C%20we%20provide%20the%20three%20core%20pillars%20that%20highlight%20opportunities%20for%20cross-pillar%20unification.%20Our%20goal%20is%20to%20suggest%20avenues%20to%20advance%20robust%2C%20safe%2C%20and%20intuitive%20physical%20interaction%2C%20providing%20a%20roadmap%20for%20future%20research%20that%20will%20allow%20humanoid%20systems%20to%20effectively%20understand%2C%20anticipate%2C%20and%20collaborate%20with%20human%20partners%20in%20diverse%20real-world%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Seamless%2520Physical%2520Human-Humanoid%2520Interaction%253A%2520Insights%2520from%2520Control%252C%2520Intent%252C%2520and%2520Modeling%2520with%2520a%2520Vision%2520for%2520What%2520Comes%2520Next%26entry.906535625%3DGustavo%2520A.%2520Cardona%2520and%2520Shubham%2520S.%2520Kumbhar%2520and%2520Panagiotis%2520Artemiadis%26entry.1292438233%3DPhysical%2520Human-Humanoid%2520Interaction%2520%2528pHHI%2529%2520is%2520a%2520rapidly%2520advancing%2520field%2520with%2520significant%2520implications%2520for%2520deploying%2520robots%2520in%2520unstructured%252C%2520human-centric%2520environments.%2520In%2520this%2520review%252C%2520we%2520examine%2520the%2520current%2520state%2520of%2520the%2520art%2520in%2520pHHI%2520through%2520three%2520core%2520pillars%253A%2520%2528i%2529%2520humanoid%2520modeling%2520and%2520control%252C%2520%2528ii%2529%2520human%2520intent%2520estimation%252C%2520and%2520%2528iii%2529%2520computational%2520human%2520models.%2520For%2520each%2520pillar%252C%2520we%2520survey%2520representative%2520approaches%252C%2520identify%2520open%2520challenges%252C%2520and%2520analyze%2520current%2520limitations%2520that%2520hinder%2520robust%252C%2520scalable%252C%2520and%2520adaptive%2520interaction.%2520These%2520include%2520the%2520need%2520for%2520whole-body%2520control%2520strategies%2520capable%2520of%2520handling%2520uncertain%2520human%2520dynamics%252C%2520real-time%2520intent%2520inference%2520under%2520limited%2520sensing%252C%2520and%2520modeling%2520techniques%2520that%2520account%2520for%2520variability%2520in%2520human%2520physical%2520states.%2520Although%2520significant%2520progress%2520has%2520been%2520made%2520within%2520each%2520domain%252C%2520integration%2520across%2520pillars%2520remains%2520limited.%2520We%2520propose%2520pathways%2520for%2520unifying%2520methods%2520across%2520these%2520areas%2520to%2520enable%2520cohesive%2520interaction%2520frameworks.%2520This%2520structure%2520enables%2520us%2520not%2520only%2520to%2520map%2520the%2520current%2520landscape%2520but%2520also%2520to%2520propose%2520concrete%2520directions%2520for%2520future%2520research%2520that%2520aim%2520to%2520bridge%2520these%2520domains.%2520Additionally%252C%2520we%2520introduce%2520a%2520unified%2520taxonomy%2520of%2520interaction%2520types%2520based%2520on%2520modality%252C%2520distinguishing%2520between%2520direct%2520interactions%2520%2528e.g.%252C%2520physical%2520contact%2529%2520and%2520indirect%2520interactions%2520%2528e.g.%252C%2520object-mediated%2529%252C%2520and%2520on%2520the%2520level%2520of%2520robot%2520engagement%252C%2520ranging%2520from%2520assistance%2520to%2520cooperation%2520and%2520collaboration.%2520For%2520each%2520category%2520in%2520this%2520taxonomy%252C%2520we%2520provide%2520the%2520three%2520core%2520pillars%2520that%2520highlight%2520opportunities%2520for%2520cross-pillar%2520unification.%2520Our%2520goal%2520is%2520to%2520suggest%2520avenues%2520to%2520advance%2520robust%252C%2520safe%252C%2520and%2520intuitive%2520physical%2520interaction%252C%2520providing%2520a%2520roadmap%2520for%2520future%2520research%2520that%2520will%2520allow%2520humanoid%2520systems%2520to%2520effectively%2520understand%252C%2520anticipate%252C%2520and%2520collaborate%2520with%2520human%2520partners%2520in%2520diverse%2520real-world%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Seamless%20Physical%20Human-Humanoid%20Interaction%3A%20Insights%20from%20Control%2C%20Intent%2C%20and%20Modeling%20with%20a%20Vision%20for%20What%20Comes%20Next&entry.906535625=Gustavo%20A.%20Cardona%20and%20Shubham%20S.%20Kumbhar%20and%20Panagiotis%20Artemiadis&entry.1292438233=Physical%20Human-Humanoid%20Interaction%20%28pHHI%29%20is%20a%20rapidly%20advancing%20field%20with%20significant%20implications%20for%20deploying%20robots%20in%20unstructured%2C%20human-centric%20environments.%20In%20this%20review%2C%20we%20examine%20the%20current%20state%20of%20the%20art%20in%20pHHI%20through%20three%20core%20pillars%3A%20%28i%29%20humanoid%20modeling%20and%20control%2C%20%28ii%29%20human%20intent%20estimation%2C%20and%20%28iii%29%20computational%20human%20models.%20For%20each%20pillar%2C%20we%20survey%20representative%20approaches%2C%20identify%20open%20challenges%2C%20and%20analyze%20current%20limitations%20that%20hinder%20robust%2C%20scalable%2C%20and%20adaptive%20interaction.%20These%20include%20the%20need%20for%20whole-body%20control%20strategies%20capable%20of%20handling%20uncertain%20human%20dynamics%2C%20real-time%20intent%20inference%20under%20limited%20sensing%2C%20and%20modeling%20techniques%20that%20account%20for%20variability%20in%20human%20physical%20states.%20Although%20significant%20progress%20has%20been%20made%20within%20each%20domain%2C%20integration%20across%20pillars%20remains%20limited.%20We%20propose%20pathways%20for%20unifying%20methods%20across%20these%20areas%20to%20enable%20cohesive%20interaction%20frameworks.%20This%20structure%20enables%20us%20not%20only%20to%20map%20the%20current%20landscape%20but%20also%20to%20propose%20concrete%20directions%20for%20future%20research%20that%20aim%20to%20bridge%20these%20domains.%20Additionally%2C%20we%20introduce%20a%20unified%20taxonomy%20of%20interaction%20types%20based%20on%20modality%2C%20distinguishing%20between%20direct%20interactions%20%28e.g.%2C%20physical%20contact%29%20and%20indirect%20interactions%20%28e.g.%2C%20object-mediated%29%2C%20and%20on%20the%20level%20of%20robot%20engagement%2C%20ranging%20from%20assistance%20to%20cooperation%20and%20collaboration.%20For%20each%20category%20in%20this%20taxonomy%2C%20we%20provide%20the%20three%20core%20pillars%20that%20highlight%20opportunities%20for%20cross-pillar%20unification.%20Our%20goal%20is%20to%20suggest%20avenues%20to%20advance%20robust%2C%20safe%2C%20and%20intuitive%20physical%20interaction%2C%20providing%20a%20roadmap%20for%20future%20research%20that%20will%20allow%20humanoid%20systems%20to%20effectively%20understand%2C%20anticipate%2C%20and%20collaborate%20with%20human%20partners%20in%20diverse%20real-world%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2512.07765v1&entry.124074799=Read"},
{"title": "When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks", "author": "Zihan Chen and Lanyu Yu", "abstract": "Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.", "link": "http://arxiv.org/abs/2512.07684v1", "date": "2025-12-08", "relevancy": 2.2316, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4619}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4406}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Large%20Language%20Models%20Do%20Not%20Work%3A%20Online%20Incivility%20Prediction%20through%20Graph%20Neural%20Networks&body=Title%3A%20When%20Large%20Language%20Models%20Do%20Not%20Work%3A%20Online%20Incivility%20Prediction%20through%20Graph%20Neural%20Networks%0AAuthor%3A%20Zihan%20Chen%20and%20Lanyu%20Yu%0AAbstract%3A%20Online%20incivility%20has%20emerged%20as%20a%20widespread%20and%20persistent%20problem%20in%20digital%20communities%2C%20imposing%20substantial%20social%20and%20psychological%20burdens%20on%20users.%20Although%20many%20platforms%20attempt%20to%20curb%20incivility%20through%20moderation%20and%20automated%20detection%2C%20the%20performance%20of%20existing%20approaches%20often%20remains%20limited%20in%20both%20accuracy%20and%20efficiency.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20Graph%20Neural%20Network%20%28GNN%29%20framework%20for%20detecting%20three%20types%20of%20uncivil%20behavior%20%28i.e.%2C%20toxicity%2C%20aggression%2C%20and%20personal%20attacks%29%20within%20the%20English%20Wikipedia%20community.%20Our%20model%20represents%20each%20user%20comment%20as%20a%20node%2C%20with%20textual%20similarity%20between%20comments%20defining%20the%20edges%2C%20allowing%20the%20network%20to%20jointly%20learn%20from%20both%20linguistic%20content%20and%20relational%20structures%20among%20comments.%20We%20also%20introduce%20a%20dynamically%20adjusted%20attention%20mechanism%20that%20adaptively%20balances%20nodal%20and%20topological%20features%20during%20information%20aggregation.%20Empirical%20evaluations%20demonstrate%20that%20our%20proposed%20architecture%20outperforms%2012%20state-of-the-art%20Large%20Language%20Models%20%28LLMs%29%20across%20multiple%20metrics%20while%20requiring%20significantly%20lower%20inference%20cost.%20These%20findings%20highlight%20the%20crucial%20role%20of%20structural%20context%20in%20detecting%20online%20incivility%20and%20address%20the%20limitations%20of%20text-only%20LLM%20paradigms%20in%20behavioral%20prediction.%20All%20datasets%20and%20comparative%20outputs%20will%20be%20publicly%20available%20in%20our%20repository%20to%20support%20further%20research%20and%20reproducibility.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Large%2520Language%2520Models%2520Do%2520Not%2520Work%253A%2520Online%2520Incivility%2520Prediction%2520through%2520Graph%2520Neural%2520Networks%26entry.906535625%3DZihan%2520Chen%2520and%2520Lanyu%2520Yu%26entry.1292438233%3DOnline%2520incivility%2520has%2520emerged%2520as%2520a%2520widespread%2520and%2520persistent%2520problem%2520in%2520digital%2520communities%252C%2520imposing%2520substantial%2520social%2520and%2520psychological%2520burdens%2520on%2520users.%2520Although%2520many%2520platforms%2520attempt%2520to%2520curb%2520incivility%2520through%2520moderation%2520and%2520automated%2520detection%252C%2520the%2520performance%2520of%2520existing%2520approaches%2520often%2520remains%2520limited%2520in%2520both%2520accuracy%2520and%2520efficiency.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%2520framework%2520for%2520detecting%2520three%2520types%2520of%2520uncivil%2520behavior%2520%2528i.e.%252C%2520toxicity%252C%2520aggression%252C%2520and%2520personal%2520attacks%2529%2520within%2520the%2520English%2520Wikipedia%2520community.%2520Our%2520model%2520represents%2520each%2520user%2520comment%2520as%2520a%2520node%252C%2520with%2520textual%2520similarity%2520between%2520comments%2520defining%2520the%2520edges%252C%2520allowing%2520the%2520network%2520to%2520jointly%2520learn%2520from%2520both%2520linguistic%2520content%2520and%2520relational%2520structures%2520among%2520comments.%2520We%2520also%2520introduce%2520a%2520dynamically%2520adjusted%2520attention%2520mechanism%2520that%2520adaptively%2520balances%2520nodal%2520and%2520topological%2520features%2520during%2520information%2520aggregation.%2520Empirical%2520evaluations%2520demonstrate%2520that%2520our%2520proposed%2520architecture%2520outperforms%252012%2520state-of-the-art%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520across%2520multiple%2520metrics%2520while%2520requiring%2520significantly%2520lower%2520inference%2520cost.%2520These%2520findings%2520highlight%2520the%2520crucial%2520role%2520of%2520structural%2520context%2520in%2520detecting%2520online%2520incivility%2520and%2520address%2520the%2520limitations%2520of%2520text-only%2520LLM%2520paradigms%2520in%2520behavioral%2520prediction.%2520All%2520datasets%2520and%2520comparative%2520outputs%2520will%2520be%2520publicly%2520available%2520in%2520our%2520repository%2520to%2520support%2520further%2520research%2520and%2520reproducibility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Large%20Language%20Models%20Do%20Not%20Work%3A%20Online%20Incivility%20Prediction%20through%20Graph%20Neural%20Networks&entry.906535625=Zihan%20Chen%20and%20Lanyu%20Yu&entry.1292438233=Online%20incivility%20has%20emerged%20as%20a%20widespread%20and%20persistent%20problem%20in%20digital%20communities%2C%20imposing%20substantial%20social%20and%20psychological%20burdens%20on%20users.%20Although%20many%20platforms%20attempt%20to%20curb%20incivility%20through%20moderation%20and%20automated%20detection%2C%20the%20performance%20of%20existing%20approaches%20often%20remains%20limited%20in%20both%20accuracy%20and%20efficiency.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20Graph%20Neural%20Network%20%28GNN%29%20framework%20for%20detecting%20three%20types%20of%20uncivil%20behavior%20%28i.e.%2C%20toxicity%2C%20aggression%2C%20and%20personal%20attacks%29%20within%20the%20English%20Wikipedia%20community.%20Our%20model%20represents%20each%20user%20comment%20as%20a%20node%2C%20with%20textual%20similarity%20between%20comments%20defining%20the%20edges%2C%20allowing%20the%20network%20to%20jointly%20learn%20from%20both%20linguistic%20content%20and%20relational%20structures%20among%20comments.%20We%20also%20introduce%20a%20dynamically%20adjusted%20attention%20mechanism%20that%20adaptively%20balances%20nodal%20and%20topological%20features%20during%20information%20aggregation.%20Empirical%20evaluations%20demonstrate%20that%20our%20proposed%20architecture%20outperforms%2012%20state-of-the-art%20Large%20Language%20Models%20%28LLMs%29%20across%20multiple%20metrics%20while%20requiring%20significantly%20lower%20inference%20cost.%20These%20findings%20highlight%20the%20crucial%20role%20of%20structural%20context%20in%20detecting%20online%20incivility%20and%20address%20the%20limitations%20of%20text-only%20LLM%20paradigms%20in%20behavioral%20prediction.%20All%20datasets%20and%20comparative%20outputs%20will%20be%20publicly%20available%20in%20our%20repository%20to%20support%20further%20research%20and%20reproducibility.&entry.1838667208=http%3A//arxiv.org/abs/2512.07684v1&entry.124074799=Read"},
{"title": "Event-Customized Image Generation", "author": "Zhen Wang and Yilei Jiang and Dong Zheng and Jun Xiao and Long Chen", "abstract": "Customized Image Generation, generating customized images with user-specified concepts, has raised significant attention due to its creativity and novelty. With impressive progress achieved in subject customization, some pioneer works further explored the customization of action and interaction beyond entity (i.e., human, animal, and object) appearance. However, these approaches only focus on basic actions and interactions between two entities, and their effects are limited by insufficient ''exactly same'' reference images. To extend customized image generation to more complex scenes for general real-world applications, we propose a new task: event-customized image generation. Given a single reference image, we define the ''event'' as all specific actions, poses, relations, or interactions between different entities in the scene. This task aims at accurately capturing the complex event and generating customized images with various target entities. To solve this task, we proposed a novel training-free event customization method: FreeEvent. Specifically, FreeEvent introduces two extra paths alongside the general diffusion denoising process: 1) Entity switching path: it applies cross-attention guidance and regulation for target entity generation. 2) Event transferring path: it injects the spatial feature and self-attention maps from the reference image to the target image for event generation. To further facilitate this new task, we collected two evaluation benchmarks: SWiG-Event and Real-Event. Extensive experiments and ablations have demonstrated the effectiveness of FreeEvent.", "link": "http://arxiv.org/abs/2410.02483v2", "date": "2025-12-08", "relevancy": 2.2297, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5717}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5565}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event-Customized%20Image%20Generation&body=Title%3A%20Event-Customized%20Image%20Generation%0AAuthor%3A%20Zhen%20Wang%20and%20Yilei%20Jiang%20and%20Dong%20Zheng%20and%20Jun%20Xiao%20and%20Long%20Chen%0AAbstract%3A%20Customized%20Image%20Generation%2C%20generating%20customized%20images%20with%20user-specified%20concepts%2C%20has%20raised%20significant%20attention%20due%20to%20its%20creativity%20and%20novelty.%20With%20impressive%20progress%20achieved%20in%20subject%20customization%2C%20some%20pioneer%20works%20further%20explored%20the%20customization%20of%20action%20and%20interaction%20beyond%20entity%20%28i.e.%2C%20human%2C%20animal%2C%20and%20object%29%20appearance.%20However%2C%20these%20approaches%20only%20focus%20on%20basic%20actions%20and%20interactions%20between%20two%20entities%2C%20and%20their%20effects%20are%20limited%20by%20insufficient%20%27%27exactly%20same%27%27%20reference%20images.%20To%20extend%20customized%20image%20generation%20to%20more%20complex%20scenes%20for%20general%20real-world%20applications%2C%20we%20propose%20a%20new%20task%3A%20event-customized%20image%20generation.%20Given%20a%20single%20reference%20image%2C%20we%20define%20the%20%27%27event%27%27%20as%20all%20specific%20actions%2C%20poses%2C%20relations%2C%20or%20interactions%20between%20different%20entities%20in%20the%20scene.%20This%20task%20aims%20at%20accurately%20capturing%20the%20complex%20event%20and%20generating%20customized%20images%20with%20various%20target%20entities.%20To%20solve%20this%20task%2C%20we%20proposed%20a%20novel%20training-free%20event%20customization%20method%3A%20FreeEvent.%20Specifically%2C%20FreeEvent%20introduces%20two%20extra%20paths%20alongside%20the%20general%20diffusion%20denoising%20process%3A%201%29%20Entity%20switching%20path%3A%20it%20applies%20cross-attention%20guidance%20and%20regulation%20for%20target%20entity%20generation.%202%29%20Event%20transferring%20path%3A%20it%20injects%20the%20spatial%20feature%20and%20self-attention%20maps%20from%20the%20reference%20image%20to%20the%20target%20image%20for%20event%20generation.%20To%20further%20facilitate%20this%20new%20task%2C%20we%20collected%20two%20evaluation%20benchmarks%3A%20SWiG-Event%20and%20Real-Event.%20Extensive%20experiments%20and%20ablations%20have%20demonstrated%20the%20effectiveness%20of%20FreeEvent.%0ALink%3A%20http%3A//arxiv.org/abs/2410.02483v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent-Customized%2520Image%2520Generation%26entry.906535625%3DZhen%2520Wang%2520and%2520Yilei%2520Jiang%2520and%2520Dong%2520Zheng%2520and%2520Jun%2520Xiao%2520and%2520Long%2520Chen%26entry.1292438233%3DCustomized%2520Image%2520Generation%252C%2520generating%2520customized%2520images%2520with%2520user-specified%2520concepts%252C%2520has%2520raised%2520significant%2520attention%2520due%2520to%2520its%2520creativity%2520and%2520novelty.%2520With%2520impressive%2520progress%2520achieved%2520in%2520subject%2520customization%252C%2520some%2520pioneer%2520works%2520further%2520explored%2520the%2520customization%2520of%2520action%2520and%2520interaction%2520beyond%2520entity%2520%2528i.e.%252C%2520human%252C%2520animal%252C%2520and%2520object%2529%2520appearance.%2520However%252C%2520these%2520approaches%2520only%2520focus%2520on%2520basic%2520actions%2520and%2520interactions%2520between%2520two%2520entities%252C%2520and%2520their%2520effects%2520are%2520limited%2520by%2520insufficient%2520%2527%2527exactly%2520same%2527%2527%2520reference%2520images.%2520To%2520extend%2520customized%2520image%2520generation%2520to%2520more%2520complex%2520scenes%2520for%2520general%2520real-world%2520applications%252C%2520we%2520propose%2520a%2520new%2520task%253A%2520event-customized%2520image%2520generation.%2520Given%2520a%2520single%2520reference%2520image%252C%2520we%2520define%2520the%2520%2527%2527event%2527%2527%2520as%2520all%2520specific%2520actions%252C%2520poses%252C%2520relations%252C%2520or%2520interactions%2520between%2520different%2520entities%2520in%2520the%2520scene.%2520This%2520task%2520aims%2520at%2520accurately%2520capturing%2520the%2520complex%2520event%2520and%2520generating%2520customized%2520images%2520with%2520various%2520target%2520entities.%2520To%2520solve%2520this%2520task%252C%2520we%2520proposed%2520a%2520novel%2520training-free%2520event%2520customization%2520method%253A%2520FreeEvent.%2520Specifically%252C%2520FreeEvent%2520introduces%2520two%2520extra%2520paths%2520alongside%2520the%2520general%2520diffusion%2520denoising%2520process%253A%25201%2529%2520Entity%2520switching%2520path%253A%2520it%2520applies%2520cross-attention%2520guidance%2520and%2520regulation%2520for%2520target%2520entity%2520generation.%25202%2529%2520Event%2520transferring%2520path%253A%2520it%2520injects%2520the%2520spatial%2520feature%2520and%2520self-attention%2520maps%2520from%2520the%2520reference%2520image%2520to%2520the%2520target%2520image%2520for%2520event%2520generation.%2520To%2520further%2520facilitate%2520this%2520new%2520task%252C%2520we%2520collected%2520two%2520evaluation%2520benchmarks%253A%2520SWiG-Event%2520and%2520Real-Event.%2520Extensive%2520experiments%2520and%2520ablations%2520have%2520demonstrated%2520the%2520effectiveness%2520of%2520FreeEvent.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02483v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event-Customized%20Image%20Generation&entry.906535625=Zhen%20Wang%20and%20Yilei%20Jiang%20and%20Dong%20Zheng%20and%20Jun%20Xiao%20and%20Long%20Chen&entry.1292438233=Customized%20Image%20Generation%2C%20generating%20customized%20images%20with%20user-specified%20concepts%2C%20has%20raised%20significant%20attention%20due%20to%20its%20creativity%20and%20novelty.%20With%20impressive%20progress%20achieved%20in%20subject%20customization%2C%20some%20pioneer%20works%20further%20explored%20the%20customization%20of%20action%20and%20interaction%20beyond%20entity%20%28i.e.%2C%20human%2C%20animal%2C%20and%20object%29%20appearance.%20However%2C%20these%20approaches%20only%20focus%20on%20basic%20actions%20and%20interactions%20between%20two%20entities%2C%20and%20their%20effects%20are%20limited%20by%20insufficient%20%27%27exactly%20same%27%27%20reference%20images.%20To%20extend%20customized%20image%20generation%20to%20more%20complex%20scenes%20for%20general%20real-world%20applications%2C%20we%20propose%20a%20new%20task%3A%20event-customized%20image%20generation.%20Given%20a%20single%20reference%20image%2C%20we%20define%20the%20%27%27event%27%27%20as%20all%20specific%20actions%2C%20poses%2C%20relations%2C%20or%20interactions%20between%20different%20entities%20in%20the%20scene.%20This%20task%20aims%20at%20accurately%20capturing%20the%20complex%20event%20and%20generating%20customized%20images%20with%20various%20target%20entities.%20To%20solve%20this%20task%2C%20we%20proposed%20a%20novel%20training-free%20event%20customization%20method%3A%20FreeEvent.%20Specifically%2C%20FreeEvent%20introduces%20two%20extra%20paths%20alongside%20the%20general%20diffusion%20denoising%20process%3A%201%29%20Entity%20switching%20path%3A%20it%20applies%20cross-attention%20guidance%20and%20regulation%20for%20target%20entity%20generation.%202%29%20Event%20transferring%20path%3A%20it%20injects%20the%20spatial%20feature%20and%20self-attention%20maps%20from%20the%20reference%20image%20to%20the%20target%20image%20for%20event%20generation.%20To%20further%20facilitate%20this%20new%20task%2C%20we%20collected%20two%20evaluation%20benchmarks%3A%20SWiG-Event%20and%20Real-Event.%20Extensive%20experiments%20and%20ablations%20have%20demonstrated%20the%20effectiveness%20of%20FreeEvent.&entry.1838667208=http%3A//arxiv.org/abs/2410.02483v2&entry.124074799=Read"},
{"title": "Do Generalisation Results Generalise?", "author": "Matteo Boglioni and Andrea Sgobbi and Gabriel Tavernini and Francesco Rita and Marius Mosbach and Tiago Pimentel", "abstract": "A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.", "link": "http://arxiv.org/abs/2512.07832v1", "date": "2025-12-08", "relevancy": 2.2295, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4588}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.42}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Generalisation%20Results%20Generalise%3F&body=Title%3A%20Do%20Generalisation%20Results%20Generalise%3F%0AAuthor%3A%20Matteo%20Boglioni%20and%20Andrea%20Sgobbi%20and%20Gabriel%20Tavernini%20and%20Francesco%20Rita%20and%20Marius%20Mosbach%20and%20Tiago%20Pimentel%0AAbstract%3A%20A%20large%20language%20model%27s%20%28LLM%27s%29%20out-of-distribution%20%28OOD%29%20generalisation%20ability%20is%20crucial%20to%20its%20deployment.%20Previous%20work%20assessing%20LLMs%27%20generalisation%20performance%2C%20however%2C%20typically%20focuses%20on%20a%20single%20out-of-distribution%20dataset.%20This%20approach%20may%20fail%20to%20precisely%20evaluate%20the%20capabilities%20of%20the%20model%2C%20as%20the%20data%20shifts%20encountered%20once%20a%20model%20is%20deployed%20are%20much%20more%20diverse.%20In%20this%20work%2C%20we%20investigate%20whether%20OOD%20generalisation%20results%20generalise.%20More%20specifically%2C%20we%20evaluate%20a%20model%27s%20performance%20across%20multiple%20OOD%20testsets%20throughout%20a%20finetuning%20run%3B%20we%20then%20evaluate%20the%20partial%20correlation%20of%20performances%20across%20these%20testsets%2C%20regressing%20out%20in-domain%20performance.%20This%20allows%20us%20to%20assess%20how%20correlated%20are%20generalisation%20performances%20once%20in-domain%20performance%20is%20controlled%20for.%20Analysing%20OLMo2%20and%20OPT%2C%20we%20observe%20no%20overarching%20trend%20in%20generalisation%20results%3A%20the%20existence%20of%20a%20positive%20or%20negative%20correlation%20between%20any%20two%20OOD%20testsets%20depends%20strongly%20on%20the%20specific%20choice%20of%20model%20analysed.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Generalisation%2520Results%2520Generalise%253F%26entry.906535625%3DMatteo%2520Boglioni%2520and%2520Andrea%2520Sgobbi%2520and%2520Gabriel%2520Tavernini%2520and%2520Francesco%2520Rita%2520and%2520Marius%2520Mosbach%2520and%2520Tiago%2520Pimentel%26entry.1292438233%3DA%2520large%2520language%2520model%2527s%2520%2528LLM%2527s%2529%2520out-of-distribution%2520%2528OOD%2529%2520generalisation%2520ability%2520is%2520crucial%2520to%2520its%2520deployment.%2520Previous%2520work%2520assessing%2520LLMs%2527%2520generalisation%2520performance%252C%2520however%252C%2520typically%2520focuses%2520on%2520a%2520single%2520out-of-distribution%2520dataset.%2520This%2520approach%2520may%2520fail%2520to%2520precisely%2520evaluate%2520the%2520capabilities%2520of%2520the%2520model%252C%2520as%2520the%2520data%2520shifts%2520encountered%2520once%2520a%2520model%2520is%2520deployed%2520are%2520much%2520more%2520diverse.%2520In%2520this%2520work%252C%2520we%2520investigate%2520whether%2520OOD%2520generalisation%2520results%2520generalise.%2520More%2520specifically%252C%2520we%2520evaluate%2520a%2520model%2527s%2520performance%2520across%2520multiple%2520OOD%2520testsets%2520throughout%2520a%2520finetuning%2520run%253B%2520we%2520then%2520evaluate%2520the%2520partial%2520correlation%2520of%2520performances%2520across%2520these%2520testsets%252C%2520regressing%2520out%2520in-domain%2520performance.%2520This%2520allows%2520us%2520to%2520assess%2520how%2520correlated%2520are%2520generalisation%2520performances%2520once%2520in-domain%2520performance%2520is%2520controlled%2520for.%2520Analysing%2520OLMo2%2520and%2520OPT%252C%2520we%2520observe%2520no%2520overarching%2520trend%2520in%2520generalisation%2520results%253A%2520the%2520existence%2520of%2520a%2520positive%2520or%2520negative%2520correlation%2520between%2520any%2520two%2520OOD%2520testsets%2520depends%2520strongly%2520on%2520the%2520specific%2520choice%2520of%2520model%2520analysed.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Generalisation%20Results%20Generalise%3F&entry.906535625=Matteo%20Boglioni%20and%20Andrea%20Sgobbi%20and%20Gabriel%20Tavernini%20and%20Francesco%20Rita%20and%20Marius%20Mosbach%20and%20Tiago%20Pimentel&entry.1292438233=A%20large%20language%20model%27s%20%28LLM%27s%29%20out-of-distribution%20%28OOD%29%20generalisation%20ability%20is%20crucial%20to%20its%20deployment.%20Previous%20work%20assessing%20LLMs%27%20generalisation%20performance%2C%20however%2C%20typically%20focuses%20on%20a%20single%20out-of-distribution%20dataset.%20This%20approach%20may%20fail%20to%20precisely%20evaluate%20the%20capabilities%20of%20the%20model%2C%20as%20the%20data%20shifts%20encountered%20once%20a%20model%20is%20deployed%20are%20much%20more%20diverse.%20In%20this%20work%2C%20we%20investigate%20whether%20OOD%20generalisation%20results%20generalise.%20More%20specifically%2C%20we%20evaluate%20a%20model%27s%20performance%20across%20multiple%20OOD%20testsets%20throughout%20a%20finetuning%20run%3B%20we%20then%20evaluate%20the%20partial%20correlation%20of%20performances%20across%20these%20testsets%2C%20regressing%20out%20in-domain%20performance.%20This%20allows%20us%20to%20assess%20how%20correlated%20are%20generalisation%20performances%20once%20in-domain%20performance%20is%20controlled%20for.%20Analysing%20OLMo2%20and%20OPT%2C%20we%20observe%20no%20overarching%20trend%20in%20generalisation%20results%3A%20the%20existence%20of%20a%20positive%20or%20negative%20correlation%20between%20any%20two%20OOD%20testsets%20depends%20strongly%20on%20the%20specific%20choice%20of%20model%20analysed.&entry.1838667208=http%3A//arxiv.org/abs/2512.07832v1&entry.124074799=Read"},
{"title": "PhyloLM : Inferring the Phylogeny of Large Language Models and Predicting their Performances in Benchmarks", "author": "Nicolas Yax and Pierre-Yves Oudeyer and Stefano Palminteri", "abstract": "This paper introduces PhyloLM, a method adapting phylogenetic algorithms to Large Language Models (LLMs) to explore whether and how they relate to each other and to predict their performance characteristics. Our method calculates a phylogenetic distance metric based on the similarity of LLMs' output. The resulting metric is then used to construct dendrograms, which satisfactorily capture known relationships across a set of 111 open-source and 45 closed models. Furthermore, our phylogenetic distance predicts performance in standard benchmarks, thus demonstrating its functional validity and paving the way for a time and cost-effective estimation of LLM capabilities. To sum up, by translating population genetic concepts to machine learning, we propose and validate a tool to evaluate LLM development, relationships and capabilities, even in the absence of transparent training information.", "link": "http://arxiv.org/abs/2404.04671v5", "date": "2025-12-08", "relevancy": 2.2149, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4486}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhyloLM%20%3A%20Inferring%20the%20Phylogeny%20of%20Large%20Language%20Models%20and%20Predicting%20their%20Performances%20in%20Benchmarks&body=Title%3A%20PhyloLM%20%3A%20Inferring%20the%20Phylogeny%20of%20Large%20Language%20Models%20and%20Predicting%20their%20Performances%20in%20Benchmarks%0AAuthor%3A%20Nicolas%20Yax%20and%20Pierre-Yves%20Oudeyer%20and%20Stefano%20Palminteri%0AAbstract%3A%20This%20paper%20introduces%20PhyloLM%2C%20a%20method%20adapting%20phylogenetic%20algorithms%20to%20Large%20Language%20Models%20%28LLMs%29%20to%20explore%20whether%20and%20how%20they%20relate%20to%20each%20other%20and%20to%20predict%20their%20performance%20characteristics.%20Our%20method%20calculates%20a%20phylogenetic%20distance%20metric%20based%20on%20the%20similarity%20of%20LLMs%27%20output.%20The%20resulting%20metric%20is%20then%20used%20to%20construct%20dendrograms%2C%20which%20satisfactorily%20capture%20known%20relationships%20across%20a%20set%20of%20111%20open-source%20and%2045%20closed%20models.%20Furthermore%2C%20our%20phylogenetic%20distance%20predicts%20performance%20in%20standard%20benchmarks%2C%20thus%20demonstrating%20its%20functional%20validity%20and%20paving%20the%20way%20for%20a%20time%20and%20cost-effective%20estimation%20of%20LLM%20capabilities.%20To%20sum%20up%2C%20by%20translating%20population%20genetic%20concepts%20to%20machine%20learning%2C%20we%20propose%20and%20validate%20a%20tool%20to%20evaluate%20LLM%20development%2C%20relationships%20and%20capabilities%2C%20even%20in%20the%20absence%20of%20transparent%20training%20information.%0ALink%3A%20http%3A//arxiv.org/abs/2404.04671v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhyloLM%2520%253A%2520Inferring%2520the%2520Phylogeny%2520of%2520Large%2520Language%2520Models%2520and%2520Predicting%2520their%2520Performances%2520in%2520Benchmarks%26entry.906535625%3DNicolas%2520Yax%2520and%2520Pierre-Yves%2520Oudeyer%2520and%2520Stefano%2520Palminteri%26entry.1292438233%3DThis%2520paper%2520introduces%2520PhyloLM%252C%2520a%2520method%2520adapting%2520phylogenetic%2520algorithms%2520to%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520explore%2520whether%2520and%2520how%2520they%2520relate%2520to%2520each%2520other%2520and%2520to%2520predict%2520their%2520performance%2520characteristics.%2520Our%2520method%2520calculates%2520a%2520phylogenetic%2520distance%2520metric%2520based%2520on%2520the%2520similarity%2520of%2520LLMs%2527%2520output.%2520The%2520resulting%2520metric%2520is%2520then%2520used%2520to%2520construct%2520dendrograms%252C%2520which%2520satisfactorily%2520capture%2520known%2520relationships%2520across%2520a%2520set%2520of%2520111%2520open-source%2520and%252045%2520closed%2520models.%2520Furthermore%252C%2520our%2520phylogenetic%2520distance%2520predicts%2520performance%2520in%2520standard%2520benchmarks%252C%2520thus%2520demonstrating%2520its%2520functional%2520validity%2520and%2520paving%2520the%2520way%2520for%2520a%2520time%2520and%2520cost-effective%2520estimation%2520of%2520LLM%2520capabilities.%2520To%2520sum%2520up%252C%2520by%2520translating%2520population%2520genetic%2520concepts%2520to%2520machine%2520learning%252C%2520we%2520propose%2520and%2520validate%2520a%2520tool%2520to%2520evaluate%2520LLM%2520development%252C%2520relationships%2520and%2520capabilities%252C%2520even%2520in%2520the%2520absence%2520of%2520transparent%2520training%2520information.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04671v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhyloLM%20%3A%20Inferring%20the%20Phylogeny%20of%20Large%20Language%20Models%20and%20Predicting%20their%20Performances%20in%20Benchmarks&entry.906535625=Nicolas%20Yax%20and%20Pierre-Yves%20Oudeyer%20and%20Stefano%20Palminteri&entry.1292438233=This%20paper%20introduces%20PhyloLM%2C%20a%20method%20adapting%20phylogenetic%20algorithms%20to%20Large%20Language%20Models%20%28LLMs%29%20to%20explore%20whether%20and%20how%20they%20relate%20to%20each%20other%20and%20to%20predict%20their%20performance%20characteristics.%20Our%20method%20calculates%20a%20phylogenetic%20distance%20metric%20based%20on%20the%20similarity%20of%20LLMs%27%20output.%20The%20resulting%20metric%20is%20then%20used%20to%20construct%20dendrograms%2C%20which%20satisfactorily%20capture%20known%20relationships%20across%20a%20set%20of%20111%20open-source%20and%2045%20closed%20models.%20Furthermore%2C%20our%20phylogenetic%20distance%20predicts%20performance%20in%20standard%20benchmarks%2C%20thus%20demonstrating%20its%20functional%20validity%20and%20paving%20the%20way%20for%20a%20time%20and%20cost-effective%20estimation%20of%20LLM%20capabilities.%20To%20sum%20up%2C%20by%20translating%20population%20genetic%20concepts%20to%20machine%20learning%2C%20we%20propose%20and%20validate%20a%20tool%20to%20evaluate%20LLM%20development%2C%20relationships%20and%20capabilities%2C%20even%20in%20the%20absence%20of%20transparent%20training%20information.&entry.1838667208=http%3A//arxiv.org/abs/2404.04671v5&entry.124074799=Read"},
{"title": "Do LLMs Trust the Code They Write?", "author": "Francisco Ribeiro and Claudio Spiess and Prem Devanbu and Sarah Nadi", "abstract": "Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.", "link": "http://arxiv.org/abs/2512.07404v1", "date": "2025-12-08", "relevancy": 2.2138, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4547}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4368}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20LLMs%20Trust%20the%20Code%20They%20Write%3F&body=Title%3A%20Do%20LLMs%20Trust%20the%20Code%20They%20Write%3F%0AAuthor%3A%20Francisco%20Ribeiro%20and%20Claudio%20Spiess%20and%20Prem%20Devanbu%20and%20Sarah%20Nadi%0AAbstract%3A%20Despite%20the%20effectiveness%20of%20large%20language%20models%20%28LLMs%29%20for%20code%20generation%2C%20they%20often%20output%20incorrect%20code.%20One%20reason%20is%20that%20model%20output%20probabilities%20are%20often%20not%20well-correlated%20with%20correctness%2C%20and%20reflect%20only%20the%20final%20output%20of%20the%20generation%20process.%20Inspired%20by%20findings%20that%20LLMs%20internally%20encode%20concepts%20like%20truthfulness%2C%20this%20paper%20explores%20if%20LLMs%20similarly%20represent%20code%20correctness.%20Specifically%2C%20we%20identify%20a%20correctness%20representation%20inside%20LLMs%20by%20contrasting%20the%20hidden%20states%20between%20pairs%20of%20correct%20and%20incorrect%20code%20for%20the%20same%20programming%20tasks.%20By%20experimenting%20on%20four%20LLMs%2C%20we%20show%20that%20exploiting%20this%20extracted%20correctness%20representation%20outperforms%20standard%20log-likelihood%20ranking%2C%20as%20well%20as%20verbalized%20model%20confidence.%20Furthermore%2C%20we%20explore%20how%20this%20internal%20correctness%20signal%20can%20be%20used%20to%20select%20higher-quality%20code%20samples%2C%20without%20requiring%20test%20execution.%20Ultimately%2C%20this%20work%20demonstrates%20how%20leveraging%20internal%20representations%20can%20enhance%20code%20generation%20systems%20and%20make%20LLMs%20more%20reliable%2C%20thus%20improving%20confidence%20in%20automatically%20generated%20code.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520LLMs%2520Trust%2520the%2520Code%2520They%2520Write%253F%26entry.906535625%3DFrancisco%2520Ribeiro%2520and%2520Claudio%2520Spiess%2520and%2520Prem%2520Devanbu%2520and%2520Sarah%2520Nadi%26entry.1292438233%3DDespite%2520the%2520effectiveness%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520code%2520generation%252C%2520they%2520often%2520output%2520incorrect%2520code.%2520One%2520reason%2520is%2520that%2520model%2520output%2520probabilities%2520are%2520often%2520not%2520well-correlated%2520with%2520correctness%252C%2520and%2520reflect%2520only%2520the%2520final%2520output%2520of%2520the%2520generation%2520process.%2520Inspired%2520by%2520findings%2520that%2520LLMs%2520internally%2520encode%2520concepts%2520like%2520truthfulness%252C%2520this%2520paper%2520explores%2520if%2520LLMs%2520similarly%2520represent%2520code%2520correctness.%2520Specifically%252C%2520we%2520identify%2520a%2520correctness%2520representation%2520inside%2520LLMs%2520by%2520contrasting%2520the%2520hidden%2520states%2520between%2520pairs%2520of%2520correct%2520and%2520incorrect%2520code%2520for%2520the%2520same%2520programming%2520tasks.%2520By%2520experimenting%2520on%2520four%2520LLMs%252C%2520we%2520show%2520that%2520exploiting%2520this%2520extracted%2520correctness%2520representation%2520outperforms%2520standard%2520log-likelihood%2520ranking%252C%2520as%2520well%2520as%2520verbalized%2520model%2520confidence.%2520Furthermore%252C%2520we%2520explore%2520how%2520this%2520internal%2520correctness%2520signal%2520can%2520be%2520used%2520to%2520select%2520higher-quality%2520code%2520samples%252C%2520without%2520requiring%2520test%2520execution.%2520Ultimately%252C%2520this%2520work%2520demonstrates%2520how%2520leveraging%2520internal%2520representations%2520can%2520enhance%2520code%2520generation%2520systems%2520and%2520make%2520LLMs%2520more%2520reliable%252C%2520thus%2520improving%2520confidence%2520in%2520automatically%2520generated%2520code.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20LLMs%20Trust%20the%20Code%20They%20Write%3F&entry.906535625=Francisco%20Ribeiro%20and%20Claudio%20Spiess%20and%20Prem%20Devanbu%20and%20Sarah%20Nadi&entry.1292438233=Despite%20the%20effectiveness%20of%20large%20language%20models%20%28LLMs%29%20for%20code%20generation%2C%20they%20often%20output%20incorrect%20code.%20One%20reason%20is%20that%20model%20output%20probabilities%20are%20often%20not%20well-correlated%20with%20correctness%2C%20and%20reflect%20only%20the%20final%20output%20of%20the%20generation%20process.%20Inspired%20by%20findings%20that%20LLMs%20internally%20encode%20concepts%20like%20truthfulness%2C%20this%20paper%20explores%20if%20LLMs%20similarly%20represent%20code%20correctness.%20Specifically%2C%20we%20identify%20a%20correctness%20representation%20inside%20LLMs%20by%20contrasting%20the%20hidden%20states%20between%20pairs%20of%20correct%20and%20incorrect%20code%20for%20the%20same%20programming%20tasks.%20By%20experimenting%20on%20four%20LLMs%2C%20we%20show%20that%20exploiting%20this%20extracted%20correctness%20representation%20outperforms%20standard%20log-likelihood%20ranking%2C%20as%20well%20as%20verbalized%20model%20confidence.%20Furthermore%2C%20we%20explore%20how%20this%20internal%20correctness%20signal%20can%20be%20used%20to%20select%20higher-quality%20code%20samples%2C%20without%20requiring%20test%20execution.%20Ultimately%2C%20this%20work%20demonstrates%20how%20leveraging%20internal%20representations%20can%20enhance%20code%20generation%20systems%20and%20make%20LLMs%20more%20reliable%2C%20thus%20improving%20confidence%20in%20automatically%20generated%20code.&entry.1838667208=http%3A//arxiv.org/abs/2512.07404v1&entry.124074799=Read"},
{"title": "SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination", "author": "Sangha Park and Seungryong Yoo and Jisoo Mok and Sungroh Yoon", "abstract": "Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\\%p improvement in CHAIR\\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.", "link": "http://arxiv.org/abs/2512.07730v1", "date": "2025-12-08", "relevancy": 2.203, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5514}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAVE%3A%20Sparse%20Autoencoder-Driven%20Visual%20Information%20Enhancement%20for%20Mitigating%20Object%20Hallucination&body=Title%3A%20SAVE%3A%20Sparse%20Autoencoder-Driven%20Visual%20Information%20Enhancement%20for%20Mitigating%20Object%20Hallucination%0AAuthor%3A%20Sangha%20Park%20and%20Seungryong%20Yoo%20and%20Jisoo%20Mok%20and%20Sungroh%20Yoon%0AAbstract%3A%20Although%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20advanced%20substantially%2C%20they%20remain%20vulnerable%20to%20object%20hallucination%20caused%20by%20language%20priors%20and%20visual%20information%20loss.%20To%20address%20this%2C%20we%20propose%20SAVE%20%28Sparse%20Autoencoder-Driven%20Visual%20Information%20Enhancement%29%2C%20a%20framework%20that%20mitigates%20hallucination%20by%20steering%20the%20model%20along%20Sparse%20Autoencoder%20%28SAE%29%20latent%20features.%20A%20binary%20object-presence%20question-answering%20probe%20identifies%20the%20SAE%20features%20most%20indicative%20of%20the%20model%27s%20visual%20information%20processing%2C%20referred%20to%20as%20visual%20understanding%20features.%20Steering%20the%20model%20along%20these%20identified%20features%20reinforces%20grounded%20visual%20understanding%20and%20effectively%20reduces%20hallucination.%20With%20its%20simple%20design%2C%20SAVE%20outperforms%20state-of-the-art%20training-free%20methods%20on%20standard%20benchmarks%2C%20achieving%20a%2010%5C%25p%20improvement%20in%20CHAIR%5C_S%20and%20consistent%20gains%20on%20POPE%20and%20MMHal-Bench.%20Extensive%20evaluations%20across%20multiple%20models%20and%20layers%20confirm%20the%20robustness%20and%20generalizability%20of%20our%20approach.%20Further%20analysis%20reveals%20that%20steering%20along%20visual%20understanding%20features%20suppresses%20the%20generation%20of%20uncertain%20object%20tokens%20and%20increases%20attention%20to%20image%20tokens%2C%20mitigating%20hallucination.%20Code%20is%20released%20at%20https%3A//github.com/wiarae/SAVE.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAVE%253A%2520Sparse%2520Autoencoder-Driven%2520Visual%2520Information%2520Enhancement%2520for%2520Mitigating%2520Object%2520Hallucination%26entry.906535625%3DSangha%2520Park%2520and%2520Seungryong%2520Yoo%2520and%2520Jisoo%2520Mok%2520and%2520Sungroh%2520Yoon%26entry.1292438233%3DAlthough%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520advanced%2520substantially%252C%2520they%2520remain%2520vulnerable%2520to%2520object%2520hallucination%2520caused%2520by%2520language%2520priors%2520and%2520visual%2520information%2520loss.%2520To%2520address%2520this%252C%2520we%2520propose%2520SAVE%2520%2528Sparse%2520Autoencoder-Driven%2520Visual%2520Information%2520Enhancement%2529%252C%2520a%2520framework%2520that%2520mitigates%2520hallucination%2520by%2520steering%2520the%2520model%2520along%2520Sparse%2520Autoencoder%2520%2528SAE%2529%2520latent%2520features.%2520A%2520binary%2520object-presence%2520question-answering%2520probe%2520identifies%2520the%2520SAE%2520features%2520most%2520indicative%2520of%2520the%2520model%2527s%2520visual%2520information%2520processing%252C%2520referred%2520to%2520as%2520visual%2520understanding%2520features.%2520Steering%2520the%2520model%2520along%2520these%2520identified%2520features%2520reinforces%2520grounded%2520visual%2520understanding%2520and%2520effectively%2520reduces%2520hallucination.%2520With%2520its%2520simple%2520design%252C%2520SAVE%2520outperforms%2520state-of-the-art%2520training-free%2520methods%2520on%2520standard%2520benchmarks%252C%2520achieving%2520a%252010%255C%2525p%2520improvement%2520in%2520CHAIR%255C_S%2520and%2520consistent%2520gains%2520on%2520POPE%2520and%2520MMHal-Bench.%2520Extensive%2520evaluations%2520across%2520multiple%2520models%2520and%2520layers%2520confirm%2520the%2520robustness%2520and%2520generalizability%2520of%2520our%2520approach.%2520Further%2520analysis%2520reveals%2520that%2520steering%2520along%2520visual%2520understanding%2520features%2520suppresses%2520the%2520generation%2520of%2520uncertain%2520object%2520tokens%2520and%2520increases%2520attention%2520to%2520image%2520tokens%252C%2520mitigating%2520hallucination.%2520Code%2520is%2520released%2520at%2520https%253A//github.com/wiarae/SAVE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAVE%3A%20Sparse%20Autoencoder-Driven%20Visual%20Information%20Enhancement%20for%20Mitigating%20Object%20Hallucination&entry.906535625=Sangha%20Park%20and%20Seungryong%20Yoo%20and%20Jisoo%20Mok%20and%20Sungroh%20Yoon&entry.1292438233=Although%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20advanced%20substantially%2C%20they%20remain%20vulnerable%20to%20object%20hallucination%20caused%20by%20language%20priors%20and%20visual%20information%20loss.%20To%20address%20this%2C%20we%20propose%20SAVE%20%28Sparse%20Autoencoder-Driven%20Visual%20Information%20Enhancement%29%2C%20a%20framework%20that%20mitigates%20hallucination%20by%20steering%20the%20model%20along%20Sparse%20Autoencoder%20%28SAE%29%20latent%20features.%20A%20binary%20object-presence%20question-answering%20probe%20identifies%20the%20SAE%20features%20most%20indicative%20of%20the%20model%27s%20visual%20information%20processing%2C%20referred%20to%20as%20visual%20understanding%20features.%20Steering%20the%20model%20along%20these%20identified%20features%20reinforces%20grounded%20visual%20understanding%20and%20effectively%20reduces%20hallucination.%20With%20its%20simple%20design%2C%20SAVE%20outperforms%20state-of-the-art%20training-free%20methods%20on%20standard%20benchmarks%2C%20achieving%20a%2010%5C%25p%20improvement%20in%20CHAIR%5C_S%20and%20consistent%20gains%20on%20POPE%20and%20MMHal-Bench.%20Extensive%20evaluations%20across%20multiple%20models%20and%20layers%20confirm%20the%20robustness%20and%20generalizability%20of%20our%20approach.%20Further%20analysis%20reveals%20that%20steering%20along%20visual%20understanding%20features%20suppresses%20the%20generation%20of%20uncertain%20object%20tokens%20and%20increases%20attention%20to%20image%20tokens%2C%20mitigating%20hallucination.%20Code%20is%20released%20at%20https%3A//github.com/wiarae/SAVE.&entry.1838667208=http%3A//arxiv.org/abs/2512.07730v1&entry.124074799=Read"},
{"title": "Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation", "author": "Kaili Qi and Zhongyi Huang and Wenli Yang", "abstract": "To address the challenge of segmenting noisy images with blurred or fragmented boundaries, this paper presents a robust version of Variational Model Based Tailored UNet (VM_TUNet), a hybrid framework that integrates variational methods with deep learning. The proposed approach incorporates physical priors, an edge detector and a mean curvature term, into a modified Cahn-Hilliard equation, aiming to combine the interpretability and boundary-smoothing advantages of variational partial differential equations (PDEs) with the strong representational ability of deep neural networks. The architecture consists of two collaborative modules: an F module, which conducts efficient frequency domain preprocessing to alleviate poor local minima, and a T module, which ensures accurate and stable local computations, backed by a stability estimate. Extensive experiments on three benchmark datasets indicate that the proposed method achieves a balanced trade-off between performance and computational efficiency, which yields competitive quantitative results and improved visual quality compared to pure convolutional neural network (CNN) based models, while achieving performance close to that of transformer-based method with reasonable computational expense.", "link": "http://arxiv.org/abs/2512.07590v1", "date": "2025-12-08", "relevancy": 2.2007, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5525}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5499}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Variational%20Model%20Based%20Tailored%20UNet%3A%20Leveraging%20Edge%20Detector%20and%20Mean%20Curvature%20for%20Improved%20Image%20Segmentation&body=Title%3A%20Robust%20Variational%20Model%20Based%20Tailored%20UNet%3A%20Leveraging%20Edge%20Detector%20and%20Mean%20Curvature%20for%20Improved%20Image%20Segmentation%0AAuthor%3A%20Kaili%20Qi%20and%20Zhongyi%20Huang%20and%20Wenli%20Yang%0AAbstract%3A%20To%20address%20the%20challenge%20of%20segmenting%20noisy%20images%20with%20blurred%20or%20fragmented%20boundaries%2C%20this%20paper%20presents%20a%20robust%20version%20of%20Variational%20Model%20Based%20Tailored%20UNet%20%28VM_TUNet%29%2C%20a%20hybrid%20framework%20that%20integrates%20variational%20methods%20with%20deep%20learning.%20The%20proposed%20approach%20incorporates%20physical%20priors%2C%20an%20edge%20detector%20and%20a%20mean%20curvature%20term%2C%20into%20a%20modified%20Cahn-Hilliard%20equation%2C%20aiming%20to%20combine%20the%20interpretability%20and%20boundary-smoothing%20advantages%20of%20variational%20partial%20differential%20equations%20%28PDEs%29%20with%20the%20strong%20representational%20ability%20of%20deep%20neural%20networks.%20The%20architecture%20consists%20of%20two%20collaborative%20modules%3A%20an%20F%20module%2C%20which%20conducts%20efficient%20frequency%20domain%20preprocessing%20to%20alleviate%20poor%20local%20minima%2C%20and%20a%20T%20module%2C%20which%20ensures%20accurate%20and%20stable%20local%20computations%2C%20backed%20by%20a%20stability%20estimate.%20Extensive%20experiments%20on%20three%20benchmark%20datasets%20indicate%20that%20the%20proposed%20method%20achieves%20a%20balanced%20trade-off%20between%20performance%20and%20computational%20efficiency%2C%20which%20yields%20competitive%20quantitative%20results%20and%20improved%20visual%20quality%20compared%20to%20pure%20convolutional%20neural%20network%20%28CNN%29%20based%20models%2C%20while%20achieving%20performance%20close%20to%20that%20of%20transformer-based%20method%20with%20reasonable%20computational%20expense.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Variational%2520Model%2520Based%2520Tailored%2520UNet%253A%2520Leveraging%2520Edge%2520Detector%2520and%2520Mean%2520Curvature%2520for%2520Improved%2520Image%2520Segmentation%26entry.906535625%3DKaili%2520Qi%2520and%2520Zhongyi%2520Huang%2520and%2520Wenli%2520Yang%26entry.1292438233%3DTo%2520address%2520the%2520challenge%2520of%2520segmenting%2520noisy%2520images%2520with%2520blurred%2520or%2520fragmented%2520boundaries%252C%2520this%2520paper%2520presents%2520a%2520robust%2520version%2520of%2520Variational%2520Model%2520Based%2520Tailored%2520UNet%2520%2528VM_TUNet%2529%252C%2520a%2520hybrid%2520framework%2520that%2520integrates%2520variational%2520methods%2520with%2520deep%2520learning.%2520The%2520proposed%2520approach%2520incorporates%2520physical%2520priors%252C%2520an%2520edge%2520detector%2520and%2520a%2520mean%2520curvature%2520term%252C%2520into%2520a%2520modified%2520Cahn-Hilliard%2520equation%252C%2520aiming%2520to%2520combine%2520the%2520interpretability%2520and%2520boundary-smoothing%2520advantages%2520of%2520variational%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520with%2520the%2520strong%2520representational%2520ability%2520of%2520deep%2520neural%2520networks.%2520The%2520architecture%2520consists%2520of%2520two%2520collaborative%2520modules%253A%2520an%2520F%2520module%252C%2520which%2520conducts%2520efficient%2520frequency%2520domain%2520preprocessing%2520to%2520alleviate%2520poor%2520local%2520minima%252C%2520and%2520a%2520T%2520module%252C%2520which%2520ensures%2520accurate%2520and%2520stable%2520local%2520computations%252C%2520backed%2520by%2520a%2520stability%2520estimate.%2520Extensive%2520experiments%2520on%2520three%2520benchmark%2520datasets%2520indicate%2520that%2520the%2520proposed%2520method%2520achieves%2520a%2520balanced%2520trade-off%2520between%2520performance%2520and%2520computational%2520efficiency%252C%2520which%2520yields%2520competitive%2520quantitative%2520results%2520and%2520improved%2520visual%2520quality%2520compared%2520to%2520pure%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520based%2520models%252C%2520while%2520achieving%2520performance%2520close%2520to%2520that%2520of%2520transformer-based%2520method%2520with%2520reasonable%2520computational%2520expense.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Variational%20Model%20Based%20Tailored%20UNet%3A%20Leveraging%20Edge%20Detector%20and%20Mean%20Curvature%20for%20Improved%20Image%20Segmentation&entry.906535625=Kaili%20Qi%20and%20Zhongyi%20Huang%20and%20Wenli%20Yang&entry.1292438233=To%20address%20the%20challenge%20of%20segmenting%20noisy%20images%20with%20blurred%20or%20fragmented%20boundaries%2C%20this%20paper%20presents%20a%20robust%20version%20of%20Variational%20Model%20Based%20Tailored%20UNet%20%28VM_TUNet%29%2C%20a%20hybrid%20framework%20that%20integrates%20variational%20methods%20with%20deep%20learning.%20The%20proposed%20approach%20incorporates%20physical%20priors%2C%20an%20edge%20detector%20and%20a%20mean%20curvature%20term%2C%20into%20a%20modified%20Cahn-Hilliard%20equation%2C%20aiming%20to%20combine%20the%20interpretability%20and%20boundary-smoothing%20advantages%20of%20variational%20partial%20differential%20equations%20%28PDEs%29%20with%20the%20strong%20representational%20ability%20of%20deep%20neural%20networks.%20The%20architecture%20consists%20of%20two%20collaborative%20modules%3A%20an%20F%20module%2C%20which%20conducts%20efficient%20frequency%20domain%20preprocessing%20to%20alleviate%20poor%20local%20minima%2C%20and%20a%20T%20module%2C%20which%20ensures%20accurate%20and%20stable%20local%20computations%2C%20backed%20by%20a%20stability%20estimate.%20Extensive%20experiments%20on%20three%20benchmark%20datasets%20indicate%20that%20the%20proposed%20method%20achieves%20a%20balanced%20trade-off%20between%20performance%20and%20computational%20efficiency%2C%20which%20yields%20competitive%20quantitative%20results%20and%20improved%20visual%20quality%20compared%20to%20pure%20convolutional%20neural%20network%20%28CNN%29%20based%20models%2C%20while%20achieving%20performance%20close%20to%20that%20of%20transformer-based%20method%20with%20reasonable%20computational%20expense.&entry.1838667208=http%3A//arxiv.org/abs/2512.07590v1&entry.124074799=Read"},
{"title": "PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention", "author": "Ziwen Li and Xin Wang and Hanlue Zhang and Runnan Chen and Runqi Lin and Xiao He and Han Huang and Yandong Guo and Fakhri Karray and Tongliang Liu and Mingming Gong", "abstract": "The Vision-Language-Action (VLA) models have demonstrated remarkable performance on embodied tasks and shown promising potential for real-world applications. However, current VLAs still struggle to produce consistent and precise target-oriented actions, as they often generate redundant or unstable motions along trajectories, limiting their applicability in time-sensitive scenarios.In this work, we attribute these redundant actions to the spatially uniform perception field of existing VLAs, which causes them to be distracted by target-irrelevant objects, especially in complex environments.To address this issue, we propose an efficient PosA-VLA framework that anchors visual attention via pose-conditioned supervision, consistently guiding the model's perception toward task-relevant regions. The pose-conditioned anchor attention mechanism enables the model to better align instruction semantics with actionable visual cues, thereby improving action generation precision and efficiency. Moreover, our framework adopts a lightweight architecture and requires no auxiliary perception modules (e.g., segmentation or grounding networks), ensuring efficient inference. Extensive experiments verify that our method executes embodied tasks with precise and time-efficient behavior across diverse robotic manipulation benchmarks and shows robust generalization in a variety of challenging environments.", "link": "http://arxiv.org/abs/2512.03724v2", "date": "2025-12-08", "relevancy": 2.1907, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5763}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PosA-VLA%3A%20Enhancing%20Action%20Generation%20via%20Pose-Conditioned%20Anchor%20Attention&body=Title%3A%20PosA-VLA%3A%20Enhancing%20Action%20Generation%20via%20Pose-Conditioned%20Anchor%20Attention%0AAuthor%3A%20Ziwen%20Li%20and%20Xin%20Wang%20and%20Hanlue%20Zhang%20and%20Runnan%20Chen%20and%20Runqi%20Lin%20and%20Xiao%20He%20and%20Han%20Huang%20and%20Yandong%20Guo%20and%20Fakhri%20Karray%20and%20Tongliang%20Liu%20and%20Mingming%20Gong%0AAbstract%3A%20The%20Vision-Language-Action%20%28VLA%29%20models%20have%20demonstrated%20remarkable%20performance%20on%20embodied%20tasks%20and%20shown%20promising%20potential%20for%20real-world%20applications.%20However%2C%20current%20VLAs%20still%20struggle%20to%20produce%20consistent%20and%20precise%20target-oriented%20actions%2C%20as%20they%20often%20generate%20redundant%20or%20unstable%20motions%20along%20trajectories%2C%20limiting%20their%20applicability%20in%20time-sensitive%20scenarios.In%20this%20work%2C%20we%20attribute%20these%20redundant%20actions%20to%20the%20spatially%20uniform%20perception%20field%20of%20existing%20VLAs%2C%20which%20causes%20them%20to%20be%20distracted%20by%20target-irrelevant%20objects%2C%20especially%20in%20complex%20environments.To%20address%20this%20issue%2C%20we%20propose%20an%20efficient%20PosA-VLA%20framework%20that%20anchors%20visual%20attention%20via%20pose-conditioned%20supervision%2C%20consistently%20guiding%20the%20model%27s%20perception%20toward%20task-relevant%20regions.%20The%20pose-conditioned%20anchor%20attention%20mechanism%20enables%20the%20model%20to%20better%20align%20instruction%20semantics%20with%20actionable%20visual%20cues%2C%20thereby%20improving%20action%20generation%20precision%20and%20efficiency.%20Moreover%2C%20our%20framework%20adopts%20a%20lightweight%20architecture%20and%20requires%20no%20auxiliary%20perception%20modules%20%28e.g.%2C%20segmentation%20or%20grounding%20networks%29%2C%20ensuring%20efficient%20inference.%20Extensive%20experiments%20verify%20that%20our%20method%20executes%20embodied%20tasks%20with%20precise%20and%20time-efficient%20behavior%20across%20diverse%20robotic%20manipulation%20benchmarks%20and%20shows%20robust%20generalization%20in%20a%20variety%20of%20challenging%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03724v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosA-VLA%253A%2520Enhancing%2520Action%2520Generation%2520via%2520Pose-Conditioned%2520Anchor%2520Attention%26entry.906535625%3DZiwen%2520Li%2520and%2520Xin%2520Wang%2520and%2520Hanlue%2520Zhang%2520and%2520Runnan%2520Chen%2520and%2520Runqi%2520Lin%2520and%2520Xiao%2520He%2520and%2520Han%2520Huang%2520and%2520Yandong%2520Guo%2520and%2520Fakhri%2520Karray%2520and%2520Tongliang%2520Liu%2520and%2520Mingming%2520Gong%26entry.1292438233%3DThe%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520demonstrated%2520remarkable%2520performance%2520on%2520embodied%2520tasks%2520and%2520shown%2520promising%2520potential%2520for%2520real-world%2520applications.%2520However%252C%2520current%2520VLAs%2520still%2520struggle%2520to%2520produce%2520consistent%2520and%2520precise%2520target-oriented%2520actions%252C%2520as%2520they%2520often%2520generate%2520redundant%2520or%2520unstable%2520motions%2520along%2520trajectories%252C%2520limiting%2520their%2520applicability%2520in%2520time-sensitive%2520scenarios.In%2520this%2520work%252C%2520we%2520attribute%2520these%2520redundant%2520actions%2520to%2520the%2520spatially%2520uniform%2520perception%2520field%2520of%2520existing%2520VLAs%252C%2520which%2520causes%2520them%2520to%2520be%2520distracted%2520by%2520target-irrelevant%2520objects%252C%2520especially%2520in%2520complex%2520environments.To%2520address%2520this%2520issue%252C%2520we%2520propose%2520an%2520efficient%2520PosA-VLA%2520framework%2520that%2520anchors%2520visual%2520attention%2520via%2520pose-conditioned%2520supervision%252C%2520consistently%2520guiding%2520the%2520model%2527s%2520perception%2520toward%2520task-relevant%2520regions.%2520The%2520pose-conditioned%2520anchor%2520attention%2520mechanism%2520enables%2520the%2520model%2520to%2520better%2520align%2520instruction%2520semantics%2520with%2520actionable%2520visual%2520cues%252C%2520thereby%2520improving%2520action%2520generation%2520precision%2520and%2520efficiency.%2520Moreover%252C%2520our%2520framework%2520adopts%2520a%2520lightweight%2520architecture%2520and%2520requires%2520no%2520auxiliary%2520perception%2520modules%2520%2528e.g.%252C%2520segmentation%2520or%2520grounding%2520networks%2529%252C%2520ensuring%2520efficient%2520inference.%2520Extensive%2520experiments%2520verify%2520that%2520our%2520method%2520executes%2520embodied%2520tasks%2520with%2520precise%2520and%2520time-efficient%2520behavior%2520across%2520diverse%2520robotic%2520manipulation%2520benchmarks%2520and%2520shows%2520robust%2520generalization%2520in%2520a%2520variety%2520of%2520challenging%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03724v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PosA-VLA%3A%20Enhancing%20Action%20Generation%20via%20Pose-Conditioned%20Anchor%20Attention&entry.906535625=Ziwen%20Li%20and%20Xin%20Wang%20and%20Hanlue%20Zhang%20and%20Runnan%20Chen%20and%20Runqi%20Lin%20and%20Xiao%20He%20and%20Han%20Huang%20and%20Yandong%20Guo%20and%20Fakhri%20Karray%20and%20Tongliang%20Liu%20and%20Mingming%20Gong&entry.1292438233=The%20Vision-Language-Action%20%28VLA%29%20models%20have%20demonstrated%20remarkable%20performance%20on%20embodied%20tasks%20and%20shown%20promising%20potential%20for%20real-world%20applications.%20However%2C%20current%20VLAs%20still%20struggle%20to%20produce%20consistent%20and%20precise%20target-oriented%20actions%2C%20as%20they%20often%20generate%20redundant%20or%20unstable%20motions%20along%20trajectories%2C%20limiting%20their%20applicability%20in%20time-sensitive%20scenarios.In%20this%20work%2C%20we%20attribute%20these%20redundant%20actions%20to%20the%20spatially%20uniform%20perception%20field%20of%20existing%20VLAs%2C%20which%20causes%20them%20to%20be%20distracted%20by%20target-irrelevant%20objects%2C%20especially%20in%20complex%20environments.To%20address%20this%20issue%2C%20we%20propose%20an%20efficient%20PosA-VLA%20framework%20that%20anchors%20visual%20attention%20via%20pose-conditioned%20supervision%2C%20consistently%20guiding%20the%20model%27s%20perception%20toward%20task-relevant%20regions.%20The%20pose-conditioned%20anchor%20attention%20mechanism%20enables%20the%20model%20to%20better%20align%20instruction%20semantics%20with%20actionable%20visual%20cues%2C%20thereby%20improving%20action%20generation%20precision%20and%20efficiency.%20Moreover%2C%20our%20framework%20adopts%20a%20lightweight%20architecture%20and%20requires%20no%20auxiliary%20perception%20modules%20%28e.g.%2C%20segmentation%20or%20grounding%20networks%29%2C%20ensuring%20efficient%20inference.%20Extensive%20experiments%20verify%20that%20our%20method%20executes%20embodied%20tasks%20with%20precise%20and%20time-efficient%20behavior%20across%20diverse%20robotic%20manipulation%20benchmarks%20and%20shows%20robust%20generalization%20in%20a%20variety%20of%20challenging%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2512.03724v2&entry.124074799=Read"},
{"title": "GlimmerNet: A Lightweight Grouped Dilated Depthwise Convolutions for UAV-Based Emergency Monitoring", "author": "\u0110or\u0111e Nedeljkovi\u0107", "abstract": "Convolutional Neural Networks (CNNs) have proven highly effective for edge and mobile vision tasks due to their computational efficiency. While many recent works seek to enhance CNNs with global contextual understanding via self-attention-based Vision Transformers, these approaches often introduce significant computational overhead. In this work, we demonstrate that it is possible to retain strong global perception without relying on computationally expensive components. We present GlimmerNet, an ultra-lightweight convolutional network built on the principle of separating receptive field diversity from feature recombination. GlimmerNet introduces Grouped Dilated Depthwise Convolutions(GDBlocks), which partition channels into groups with distinct dilation rates, enabling multi-scale feature extraction at no additional parameter cost. To fuse these features efficiently, we design a novel Aggregator module that recombines cross-group representations using grouped pointwise convolution, significantly lowering parameter overhead. With just 31K parameters and 29% fewer FLOPs than the most recent baseline, GlimmerNet achieves a new state-of-the-art weighted F1-score of 0.966 on the UAV-focused AIDERv2 dataset. These results establish a new accuracy-efficiency trade-off frontier for real-time emergency monitoring on resource-constrained UAV platforms. Our implementation is publicly available at https://github.com/djordjened92/gdd-cnn.", "link": "http://arxiv.org/abs/2512.07391v1", "date": "2025-12-08", "relevancy": 2.1856, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.567}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5512}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GlimmerNet%3A%20A%20Lightweight%20Grouped%20Dilated%20Depthwise%20Convolutions%20for%20UAV-Based%20Emergency%20Monitoring&body=Title%3A%20GlimmerNet%3A%20A%20Lightweight%20Grouped%20Dilated%20Depthwise%20Convolutions%20for%20UAV-Based%20Emergency%20Monitoring%0AAuthor%3A%20%C4%90or%C4%91e%20Nedeljkovi%C4%87%0AAbstract%3A%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20proven%20highly%20effective%20for%20edge%20and%20mobile%20vision%20tasks%20due%20to%20their%20computational%20efficiency.%20While%20many%20recent%20works%20seek%20to%20enhance%20CNNs%20with%20global%20contextual%20understanding%20via%20self-attention-based%20Vision%20Transformers%2C%20these%20approaches%20often%20introduce%20significant%20computational%20overhead.%20In%20this%20work%2C%20we%20demonstrate%20that%20it%20is%20possible%20to%20retain%20strong%20global%20perception%20without%20relying%20on%20computationally%20expensive%20components.%20We%20present%20GlimmerNet%2C%20an%20ultra-lightweight%20convolutional%20network%20built%20on%20the%20principle%20of%20separating%20receptive%20field%20diversity%20from%20feature%20recombination.%20GlimmerNet%20introduces%20Grouped%20Dilated%20Depthwise%20Convolutions%28GDBlocks%29%2C%20which%20partition%20channels%20into%20groups%20with%20distinct%20dilation%20rates%2C%20enabling%20multi-scale%20feature%20extraction%20at%20no%20additional%20parameter%20cost.%20To%20fuse%20these%20features%20efficiently%2C%20we%20design%20a%20novel%20Aggregator%20module%20that%20recombines%20cross-group%20representations%20using%20grouped%20pointwise%20convolution%2C%20significantly%20lowering%20parameter%20overhead.%20With%20just%2031K%20parameters%20and%2029%25%20fewer%20FLOPs%20than%20the%20most%20recent%20baseline%2C%20GlimmerNet%20achieves%20a%20new%20state-of-the-art%20weighted%20F1-score%20of%200.966%20on%20the%20UAV-focused%20AIDERv2%20dataset.%20These%20results%20establish%20a%20new%20accuracy-efficiency%20trade-off%20frontier%20for%20real-time%20emergency%20monitoring%20on%20resource-constrained%20UAV%20platforms.%20Our%20implementation%20is%20publicly%20available%20at%20https%3A//github.com/djordjened92/gdd-cnn.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlimmerNet%253A%2520A%2520Lightweight%2520Grouped%2520Dilated%2520Depthwise%2520Convolutions%2520for%2520UAV-Based%2520Emergency%2520Monitoring%26entry.906535625%3D%25C4%2590or%25C4%2591e%2520Nedeljkovi%25C4%2587%26entry.1292438233%3DConvolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520have%2520proven%2520highly%2520effective%2520for%2520edge%2520and%2520mobile%2520vision%2520tasks%2520due%2520to%2520their%2520computational%2520efficiency.%2520While%2520many%2520recent%2520works%2520seek%2520to%2520enhance%2520CNNs%2520with%2520global%2520contextual%2520understanding%2520via%2520self-attention-based%2520Vision%2520Transformers%252C%2520these%2520approaches%2520often%2520introduce%2520significant%2520computational%2520overhead.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%2520it%2520is%2520possible%2520to%2520retain%2520strong%2520global%2520perception%2520without%2520relying%2520on%2520computationally%2520expensive%2520components.%2520We%2520present%2520GlimmerNet%252C%2520an%2520ultra-lightweight%2520convolutional%2520network%2520built%2520on%2520the%2520principle%2520of%2520separating%2520receptive%2520field%2520diversity%2520from%2520feature%2520recombination.%2520GlimmerNet%2520introduces%2520Grouped%2520Dilated%2520Depthwise%2520Convolutions%2528GDBlocks%2529%252C%2520which%2520partition%2520channels%2520into%2520groups%2520with%2520distinct%2520dilation%2520rates%252C%2520enabling%2520multi-scale%2520feature%2520extraction%2520at%2520no%2520additional%2520parameter%2520cost.%2520To%2520fuse%2520these%2520features%2520efficiently%252C%2520we%2520design%2520a%2520novel%2520Aggregator%2520module%2520that%2520recombines%2520cross-group%2520representations%2520using%2520grouped%2520pointwise%2520convolution%252C%2520significantly%2520lowering%2520parameter%2520overhead.%2520With%2520just%252031K%2520parameters%2520and%252029%2525%2520fewer%2520FLOPs%2520than%2520the%2520most%2520recent%2520baseline%252C%2520GlimmerNet%2520achieves%2520a%2520new%2520state-of-the-art%2520weighted%2520F1-score%2520of%25200.966%2520on%2520the%2520UAV-focused%2520AIDERv2%2520dataset.%2520These%2520results%2520establish%2520a%2520new%2520accuracy-efficiency%2520trade-off%2520frontier%2520for%2520real-time%2520emergency%2520monitoring%2520on%2520resource-constrained%2520UAV%2520platforms.%2520Our%2520implementation%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/djordjened92/gdd-cnn.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GlimmerNet%3A%20A%20Lightweight%20Grouped%20Dilated%20Depthwise%20Convolutions%20for%20UAV-Based%20Emergency%20Monitoring&entry.906535625=%C4%90or%C4%91e%20Nedeljkovi%C4%87&entry.1292438233=Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20proven%20highly%20effective%20for%20edge%20and%20mobile%20vision%20tasks%20due%20to%20their%20computational%20efficiency.%20While%20many%20recent%20works%20seek%20to%20enhance%20CNNs%20with%20global%20contextual%20understanding%20via%20self-attention-based%20Vision%20Transformers%2C%20these%20approaches%20often%20introduce%20significant%20computational%20overhead.%20In%20this%20work%2C%20we%20demonstrate%20that%20it%20is%20possible%20to%20retain%20strong%20global%20perception%20without%20relying%20on%20computationally%20expensive%20components.%20We%20present%20GlimmerNet%2C%20an%20ultra-lightweight%20convolutional%20network%20built%20on%20the%20principle%20of%20separating%20receptive%20field%20diversity%20from%20feature%20recombination.%20GlimmerNet%20introduces%20Grouped%20Dilated%20Depthwise%20Convolutions%28GDBlocks%29%2C%20which%20partition%20channels%20into%20groups%20with%20distinct%20dilation%20rates%2C%20enabling%20multi-scale%20feature%20extraction%20at%20no%20additional%20parameter%20cost.%20To%20fuse%20these%20features%20efficiently%2C%20we%20design%20a%20novel%20Aggregator%20module%20that%20recombines%20cross-group%20representations%20using%20grouped%20pointwise%20convolution%2C%20significantly%20lowering%20parameter%20overhead.%20With%20just%2031K%20parameters%20and%2029%25%20fewer%20FLOPs%20than%20the%20most%20recent%20baseline%2C%20GlimmerNet%20achieves%20a%20new%20state-of-the-art%20weighted%20F1-score%20of%200.966%20on%20the%20UAV-focused%20AIDERv2%20dataset.%20These%20results%20establish%20a%20new%20accuracy-efficiency%20trade-off%20frontier%20for%20real-time%20emergency%20monitoring%20on%20resource-constrained%20UAV%20platforms.%20Our%20implementation%20is%20publicly%20available%20at%20https%3A//github.com/djordjened92/gdd-cnn.&entry.1838667208=http%3A//arxiv.org/abs/2512.07391v1&entry.124074799=Read"},
{"title": "Quantization-Free Autoregressive Action Transformer", "author": "Ziyad Sheebaelhamd and Michael Tschannen and Michael Muehlebach and Claire Vernade", "abstract": "Current transformer-based imitation learning approaches introduce discrete action representations and train an autoregressive transformer decoder on the resulting latent code. However, the initial quantization breaks the continuous structure of the action space thereby limiting the capabilities of the generative model. We propose a quantization-free method instead that leverages Generative Infinite-Vocabulary Transformers (GIVT) as a direct, continuous policy parametrization for autoregressive transformers. This simplifies the imitation learning pipeline while achieving state-of-the-art performance on a variety of popular simulated robotics tasks. We enhance our policy roll-outs by carefully studying sampling algorithms, further improving the results.", "link": "http://arxiv.org/abs/2503.14259v3", "date": "2025-12-08", "relevancy": 2.1829, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5779}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.556}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantization-Free%20Autoregressive%20Action%20Transformer&body=Title%3A%20Quantization-Free%20Autoregressive%20Action%20Transformer%0AAuthor%3A%20Ziyad%20Sheebaelhamd%20and%20Michael%20Tschannen%20and%20Michael%20Muehlebach%20and%20Claire%20Vernade%0AAbstract%3A%20Current%20transformer-based%20imitation%20learning%20approaches%20introduce%20discrete%20action%20representations%20and%20train%20an%20autoregressive%20transformer%20decoder%20on%20the%20resulting%20latent%20code.%20However%2C%20the%20initial%20quantization%20breaks%20the%20continuous%20structure%20of%20the%20action%20space%20thereby%20limiting%20the%20capabilities%20of%20the%20generative%20model.%20We%20propose%20a%20quantization-free%20method%20instead%20that%20leverages%20Generative%20Infinite-Vocabulary%20Transformers%20%28GIVT%29%20as%20a%20direct%2C%20continuous%20policy%20parametrization%20for%20autoregressive%20transformers.%20This%20simplifies%20the%20imitation%20learning%20pipeline%20while%20achieving%20state-of-the-art%20performance%20on%20a%20variety%20of%20popular%20simulated%20robotics%20tasks.%20We%20enhance%20our%20policy%20roll-outs%20by%20carefully%20studying%20sampling%20algorithms%2C%20further%20improving%20the%20results.%0ALink%3A%20http%3A//arxiv.org/abs/2503.14259v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantization-Free%2520Autoregressive%2520Action%2520Transformer%26entry.906535625%3DZiyad%2520Sheebaelhamd%2520and%2520Michael%2520Tschannen%2520and%2520Michael%2520Muehlebach%2520and%2520Claire%2520Vernade%26entry.1292438233%3DCurrent%2520transformer-based%2520imitation%2520learning%2520approaches%2520introduce%2520discrete%2520action%2520representations%2520and%2520train%2520an%2520autoregressive%2520transformer%2520decoder%2520on%2520the%2520resulting%2520latent%2520code.%2520However%252C%2520the%2520initial%2520quantization%2520breaks%2520the%2520continuous%2520structure%2520of%2520the%2520action%2520space%2520thereby%2520limiting%2520the%2520capabilities%2520of%2520the%2520generative%2520model.%2520We%2520propose%2520a%2520quantization-free%2520method%2520instead%2520that%2520leverages%2520Generative%2520Infinite-Vocabulary%2520Transformers%2520%2528GIVT%2529%2520as%2520a%2520direct%252C%2520continuous%2520policy%2520parametrization%2520for%2520autoregressive%2520transformers.%2520This%2520simplifies%2520the%2520imitation%2520learning%2520pipeline%2520while%2520achieving%2520state-of-the-art%2520performance%2520on%2520a%2520variety%2520of%2520popular%2520simulated%2520robotics%2520tasks.%2520We%2520enhance%2520our%2520policy%2520roll-outs%2520by%2520carefully%2520studying%2520sampling%2520algorithms%252C%2520further%2520improving%2520the%2520results.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.14259v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantization-Free%20Autoregressive%20Action%20Transformer&entry.906535625=Ziyad%20Sheebaelhamd%20and%20Michael%20Tschannen%20and%20Michael%20Muehlebach%20and%20Claire%20Vernade&entry.1292438233=Current%20transformer-based%20imitation%20learning%20approaches%20introduce%20discrete%20action%20representations%20and%20train%20an%20autoregressive%20transformer%20decoder%20on%20the%20resulting%20latent%20code.%20However%2C%20the%20initial%20quantization%20breaks%20the%20continuous%20structure%20of%20the%20action%20space%20thereby%20limiting%20the%20capabilities%20of%20the%20generative%20model.%20We%20propose%20a%20quantization-free%20method%20instead%20that%20leverages%20Generative%20Infinite-Vocabulary%20Transformers%20%28GIVT%29%20as%20a%20direct%2C%20continuous%20policy%20parametrization%20for%20autoregressive%20transformers.%20This%20simplifies%20the%20imitation%20learning%20pipeline%20while%20achieving%20state-of-the-art%20performance%20on%20a%20variety%20of%20popular%20simulated%20robotics%20tasks.%20We%20enhance%20our%20policy%20roll-outs%20by%20carefully%20studying%20sampling%20algorithms%2C%20further%20improving%20the%20results.&entry.1838667208=http%3A//arxiv.org/abs/2503.14259v3&entry.124074799=Read"},
{"title": "UnCageNet: Tracking and Pose Estimation of Caged Animal", "author": "Sayak Dutta and Harish Katti and Shashikant Verma and Shanmuganathan Raman", "abstract": "Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.", "link": "http://arxiv.org/abs/2512.07712v1", "date": "2025-12-08", "relevancy": 2.1785, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.597}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5397}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UnCageNet%3A%20Tracking%20and%20Pose%20Estimation%20of%20Caged%20Animal&body=Title%3A%20UnCageNet%3A%20Tracking%20and%20Pose%20Estimation%20of%20Caged%20Animal%0AAuthor%3A%20Sayak%20Dutta%20and%20Harish%20Katti%20and%20Shashikant%20Verma%20and%20Shanmuganathan%20Raman%0AAbstract%3A%20Animal%20tracking%20and%20pose%20estimation%20systems%2C%20such%20as%20STEP%20%28Simultaneous%20Tracking%20and%20Pose%20Estimation%29%20and%20ViTPose%2C%20experience%20substantial%20performance%20drops%20when%20processing%20images%20and%20videos%20with%20cage%20structures%20and%20systematic%20occlusions.%20We%20present%20a%20three-stage%20preprocessing%20pipeline%20that%20addresses%20this%20limitation%20through%3A%20%281%29%20cage%20segmentation%20using%20a%20Gabor-enhanced%20ResNet-UNet%20architecture%20with%20tunable%20orientation%20filters%2C%20%282%29%20cage%20inpainting%20using%20CRFill%20for%20content-aware%20reconstruction%20of%20occluded%20regions%2C%20and%20%283%29%20evaluation%20of%20pose%20estimation%20and%20tracking%20on%20the%20uncaged%20frames.%20Our%20Gabor-enhanced%20segmentation%20model%20leverages%20orientation-aware%20features%20with%2072%20directional%20kernels%20to%20accurately%20identify%20and%20segment%20cage%20structures%20that%20severely%20impair%20the%20performance%20of%20existing%20methods.%20Experimental%20validation%20demonstrates%20that%20removing%20cage%20occlusions%20through%20our%20pipeline%20enables%20pose%20estimation%20and%20tracking%20performance%20comparable%20to%20that%20in%20environments%20without%20occlusions.%20We%20also%20observe%20significant%20improvements%20in%20keypoint%20detection%20accuracy%20and%20trajectory%20consistency.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnCageNet%253A%2520Tracking%2520and%2520Pose%2520Estimation%2520of%2520Caged%2520Animal%26entry.906535625%3DSayak%2520Dutta%2520and%2520Harish%2520Katti%2520and%2520Shashikant%2520Verma%2520and%2520Shanmuganathan%2520Raman%26entry.1292438233%3DAnimal%2520tracking%2520and%2520pose%2520estimation%2520systems%252C%2520such%2520as%2520STEP%2520%2528Simultaneous%2520Tracking%2520and%2520Pose%2520Estimation%2529%2520and%2520ViTPose%252C%2520experience%2520substantial%2520performance%2520drops%2520when%2520processing%2520images%2520and%2520videos%2520with%2520cage%2520structures%2520and%2520systematic%2520occlusions.%2520We%2520present%2520a%2520three-stage%2520preprocessing%2520pipeline%2520that%2520addresses%2520this%2520limitation%2520through%253A%2520%25281%2529%2520cage%2520segmentation%2520using%2520a%2520Gabor-enhanced%2520ResNet-UNet%2520architecture%2520with%2520tunable%2520orientation%2520filters%252C%2520%25282%2529%2520cage%2520inpainting%2520using%2520CRFill%2520for%2520content-aware%2520reconstruction%2520of%2520occluded%2520regions%252C%2520and%2520%25283%2529%2520evaluation%2520of%2520pose%2520estimation%2520and%2520tracking%2520on%2520the%2520uncaged%2520frames.%2520Our%2520Gabor-enhanced%2520segmentation%2520model%2520leverages%2520orientation-aware%2520features%2520with%252072%2520directional%2520kernels%2520to%2520accurately%2520identify%2520and%2520segment%2520cage%2520structures%2520that%2520severely%2520impair%2520the%2520performance%2520of%2520existing%2520methods.%2520Experimental%2520validation%2520demonstrates%2520that%2520removing%2520cage%2520occlusions%2520through%2520our%2520pipeline%2520enables%2520pose%2520estimation%2520and%2520tracking%2520performance%2520comparable%2520to%2520that%2520in%2520environments%2520without%2520occlusions.%2520We%2520also%2520observe%2520significant%2520improvements%2520in%2520keypoint%2520detection%2520accuracy%2520and%2520trajectory%2520consistency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UnCageNet%3A%20Tracking%20and%20Pose%20Estimation%20of%20Caged%20Animal&entry.906535625=Sayak%20Dutta%20and%20Harish%20Katti%20and%20Shashikant%20Verma%20and%20Shanmuganathan%20Raman&entry.1292438233=Animal%20tracking%20and%20pose%20estimation%20systems%2C%20such%20as%20STEP%20%28Simultaneous%20Tracking%20and%20Pose%20Estimation%29%20and%20ViTPose%2C%20experience%20substantial%20performance%20drops%20when%20processing%20images%20and%20videos%20with%20cage%20structures%20and%20systematic%20occlusions.%20We%20present%20a%20three-stage%20preprocessing%20pipeline%20that%20addresses%20this%20limitation%20through%3A%20%281%29%20cage%20segmentation%20using%20a%20Gabor-enhanced%20ResNet-UNet%20architecture%20with%20tunable%20orientation%20filters%2C%20%282%29%20cage%20inpainting%20using%20CRFill%20for%20content-aware%20reconstruction%20of%20occluded%20regions%2C%20and%20%283%29%20evaluation%20of%20pose%20estimation%20and%20tracking%20on%20the%20uncaged%20frames.%20Our%20Gabor-enhanced%20segmentation%20model%20leverages%20orientation-aware%20features%20with%2072%20directional%20kernels%20to%20accurately%20identify%20and%20segment%20cage%20structures%20that%20severely%20impair%20the%20performance%20of%20existing%20methods.%20Experimental%20validation%20demonstrates%20that%20removing%20cage%20occlusions%20through%20our%20pipeline%20enables%20pose%20estimation%20and%20tracking%20performance%20comparable%20to%20that%20in%20environments%20without%20occlusions.%20We%20also%20observe%20significant%20improvements%20in%20keypoint%20detection%20accuracy%20and%20trajectory%20consistency.&entry.1838667208=http%3A//arxiv.org/abs/2512.07712v1&entry.124074799=Read"},
{"title": "A Biophysically-Conditioned Generative Framework for 3D Brain Tumor MRI Synthesis", "author": "Valentin Biller and Lucas Zimmer and Ayhan Can Erdur and Sandeep Nagar and Daniel R\u00fcckert and Niklas Bubeck and Jonas Weidner", "abstract": "Magnetic resonance imaging (MRI) inpainting supports numerous clinical and research applications. We introduce the first generative model that conditions on voxel-level, continuous tumor concentrations to synthesize high-fidelity brain tumor MRIs. For the BraTS 2025 Inpainting Challenge, we adapt this architecture to the complementary task of healthy tissue restoration by setting the tumor concentrations to zero. Our latent diffusion model conditioned on both tissue segmentations and the tumor concentrations generates 3D spatially coherent and anatomically consistent images for both tumor synthesis and healthy tissue inpainting. For healthy inpainting, we achieve a PSNR of 18.5, and for tumor inpainting, we achieve 17.4. Our code is available at: https://github.com/valentin-biller/ldm.git", "link": "http://arxiv.org/abs/2510.09365v2", "date": "2025-12-08", "relevancy": 2.1693, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5473}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5448}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Biophysically-Conditioned%20Generative%20Framework%20for%203D%20Brain%20Tumor%20MRI%20Synthesis&body=Title%3A%20A%20Biophysically-Conditioned%20Generative%20Framework%20for%203D%20Brain%20Tumor%20MRI%20Synthesis%0AAuthor%3A%20Valentin%20Biller%20and%20Lucas%20Zimmer%20and%20Ayhan%20Can%20Erdur%20and%20Sandeep%20Nagar%20and%20Daniel%20R%C3%BCckert%20and%20Niklas%20Bubeck%20and%20Jonas%20Weidner%0AAbstract%3A%20Magnetic%20resonance%20imaging%20%28MRI%29%20inpainting%20supports%20numerous%20clinical%20and%20research%20applications.%20We%20introduce%20the%20first%20generative%20model%20that%20conditions%20on%20voxel-level%2C%20continuous%20tumor%20concentrations%20to%20synthesize%20high-fidelity%20brain%20tumor%20MRIs.%20For%20the%20BraTS%202025%20Inpainting%20Challenge%2C%20we%20adapt%20this%20architecture%20to%20the%20complementary%20task%20of%20healthy%20tissue%20restoration%20by%20setting%20the%20tumor%20concentrations%20to%20zero.%20Our%20latent%20diffusion%20model%20conditioned%20on%20both%20tissue%20segmentations%20and%20the%20tumor%20concentrations%20generates%203D%20spatially%20coherent%20and%20anatomically%20consistent%20images%20for%20both%20tumor%20synthesis%20and%20healthy%20tissue%20inpainting.%20For%20healthy%20inpainting%2C%20we%20achieve%20a%20PSNR%20of%2018.5%2C%20and%20for%20tumor%20inpainting%2C%20we%20achieve%2017.4.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/valentin-biller/ldm.git%0ALink%3A%20http%3A//arxiv.org/abs/2510.09365v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Biophysically-Conditioned%2520Generative%2520Framework%2520for%25203D%2520Brain%2520Tumor%2520MRI%2520Synthesis%26entry.906535625%3DValentin%2520Biller%2520and%2520Lucas%2520Zimmer%2520and%2520Ayhan%2520Can%2520Erdur%2520and%2520Sandeep%2520Nagar%2520and%2520Daniel%2520R%25C3%25BCckert%2520and%2520Niklas%2520Bubeck%2520and%2520Jonas%2520Weidner%26entry.1292438233%3DMagnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520inpainting%2520supports%2520numerous%2520clinical%2520and%2520research%2520applications.%2520We%2520introduce%2520the%2520first%2520generative%2520model%2520that%2520conditions%2520on%2520voxel-level%252C%2520continuous%2520tumor%2520concentrations%2520to%2520synthesize%2520high-fidelity%2520brain%2520tumor%2520MRIs.%2520For%2520the%2520BraTS%25202025%2520Inpainting%2520Challenge%252C%2520we%2520adapt%2520this%2520architecture%2520to%2520the%2520complementary%2520task%2520of%2520healthy%2520tissue%2520restoration%2520by%2520setting%2520the%2520tumor%2520concentrations%2520to%2520zero.%2520Our%2520latent%2520diffusion%2520model%2520conditioned%2520on%2520both%2520tissue%2520segmentations%2520and%2520the%2520tumor%2520concentrations%2520generates%25203D%2520spatially%2520coherent%2520and%2520anatomically%2520consistent%2520images%2520for%2520both%2520tumor%2520synthesis%2520and%2520healthy%2520tissue%2520inpainting.%2520For%2520healthy%2520inpainting%252C%2520we%2520achieve%2520a%2520PSNR%2520of%252018.5%252C%2520and%2520for%2520tumor%2520inpainting%252C%2520we%2520achieve%252017.4.%2520Our%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/valentin-biller/ldm.git%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09365v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Biophysically-Conditioned%20Generative%20Framework%20for%203D%20Brain%20Tumor%20MRI%20Synthesis&entry.906535625=Valentin%20Biller%20and%20Lucas%20Zimmer%20and%20Ayhan%20Can%20Erdur%20and%20Sandeep%20Nagar%20and%20Daniel%20R%C3%BCckert%20and%20Niklas%20Bubeck%20and%20Jonas%20Weidner&entry.1292438233=Magnetic%20resonance%20imaging%20%28MRI%29%20inpainting%20supports%20numerous%20clinical%20and%20research%20applications.%20We%20introduce%20the%20first%20generative%20model%20that%20conditions%20on%20voxel-level%2C%20continuous%20tumor%20concentrations%20to%20synthesize%20high-fidelity%20brain%20tumor%20MRIs.%20For%20the%20BraTS%202025%20Inpainting%20Challenge%2C%20we%20adapt%20this%20architecture%20to%20the%20complementary%20task%20of%20healthy%20tissue%20restoration%20by%20setting%20the%20tumor%20concentrations%20to%20zero.%20Our%20latent%20diffusion%20model%20conditioned%20on%20both%20tissue%20segmentations%20and%20the%20tumor%20concentrations%20generates%203D%20spatially%20coherent%20and%20anatomically%20consistent%20images%20for%20both%20tumor%20synthesis%20and%20healthy%20tissue%20inpainting.%20For%20healthy%20inpainting%2C%20we%20achieve%20a%20PSNR%20of%2018.5%2C%20and%20for%20tumor%20inpainting%2C%20we%20achieve%2017.4.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/valentin-biller/ldm.git&entry.1838667208=http%3A//arxiv.org/abs/2510.09365v2&entry.124074799=Read"},
{"title": "Decomposition Sampling for Efficient Region Annotations in Active Learning", "author": "Jingna Qiu and Frauke Wilm and Mathias \u00d6ttl and Jonas Utz and Maja Schlereth and Moritz Schillinger and Marc Aubreville and Katharina Breininger", "abstract": "Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git.", "link": "http://arxiv.org/abs/2512.07606v1", "date": "2025-12-08", "relevancy": 2.1686, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5677}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5542}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decomposition%20Sampling%20for%20Efficient%20Region%20Annotations%20in%20Active%20Learning&body=Title%3A%20Decomposition%20Sampling%20for%20Efficient%20Region%20Annotations%20in%20Active%20Learning%0AAuthor%3A%20Jingna%20Qiu%20and%20Frauke%20Wilm%20and%20Mathias%20%C3%96ttl%20and%20Jonas%20Utz%20and%20Maja%20Schlereth%20and%20Moritz%20Schillinger%20and%20Marc%20Aubreville%20and%20Katharina%20Breininger%0AAbstract%3A%20Active%20learning%20improves%20annotation%20efficiency%20by%20selecting%20the%20most%20informative%20samples%20for%20annotation%20and%20model%20training.%20While%20most%20prior%20work%20has%20focused%20on%20selecting%20informative%20images%20for%20classification%20tasks%2C%20we%20investigate%20the%20more%20challenging%20setting%20of%20dense%20prediction%2C%20where%20annotations%20are%20more%20costly%20and%20time-intensive%2C%20especially%20in%20medical%20imaging.%20Region-level%20annotation%20has%20been%20shown%20to%20be%20more%20efficient%20than%20image-level%20annotation%20for%20these%20tasks.%20However%2C%20existing%20methods%20for%20representative%20annotation%20region%20selection%20suffer%20from%20high%20computational%20and%20memory%20costs%2C%20irrelevant%20region%20choices%2C%20and%20heavy%20reliance%20on%20uncertainty%20sampling.%20We%20propose%20decomposition%20sampling%20%28DECOMP%29%2C%20a%20new%20active%20learning%20sampling%20strategy%20that%20addresses%20these%20limitations.%20It%20enhances%20annotation%20diversity%20by%20decomposing%20images%20into%20class-specific%20components%20using%20pseudo-labels%20and%20sampling%20regions%20from%20each%20class.%20Class-wise%20predictive%20confidence%20further%20guides%20the%20sampling%20process%2C%20ensuring%20that%20difficult%20classes%20receive%20additional%20annotations.%20Across%20ROI%20classification%2C%202-D%20segmentation%2C%20and%203-D%20segmentation%2C%20DECOMP%20consistently%20surpasses%20baseline%20methods%20by%20better%20sampling%20minority-class%20regions%20and%20boosting%20performance%20on%20these%20challenging%20classes.%20Code%20is%20in%20https%3A//github.com/JingnaQiu/DECOMP.git.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecomposition%2520Sampling%2520for%2520Efficient%2520Region%2520Annotations%2520in%2520Active%2520Learning%26entry.906535625%3DJingna%2520Qiu%2520and%2520Frauke%2520Wilm%2520and%2520Mathias%2520%25C3%2596ttl%2520and%2520Jonas%2520Utz%2520and%2520Maja%2520Schlereth%2520and%2520Moritz%2520Schillinger%2520and%2520Marc%2520Aubreville%2520and%2520Katharina%2520Breininger%26entry.1292438233%3DActive%2520learning%2520improves%2520annotation%2520efficiency%2520by%2520selecting%2520the%2520most%2520informative%2520samples%2520for%2520annotation%2520and%2520model%2520training.%2520While%2520most%2520prior%2520work%2520has%2520focused%2520on%2520selecting%2520informative%2520images%2520for%2520classification%2520tasks%252C%2520we%2520investigate%2520the%2520more%2520challenging%2520setting%2520of%2520dense%2520prediction%252C%2520where%2520annotations%2520are%2520more%2520costly%2520and%2520time-intensive%252C%2520especially%2520in%2520medical%2520imaging.%2520Region-level%2520annotation%2520has%2520been%2520shown%2520to%2520be%2520more%2520efficient%2520than%2520image-level%2520annotation%2520for%2520these%2520tasks.%2520However%252C%2520existing%2520methods%2520for%2520representative%2520annotation%2520region%2520selection%2520suffer%2520from%2520high%2520computational%2520and%2520memory%2520costs%252C%2520irrelevant%2520region%2520choices%252C%2520and%2520heavy%2520reliance%2520on%2520uncertainty%2520sampling.%2520We%2520propose%2520decomposition%2520sampling%2520%2528DECOMP%2529%252C%2520a%2520new%2520active%2520learning%2520sampling%2520strategy%2520that%2520addresses%2520these%2520limitations.%2520It%2520enhances%2520annotation%2520diversity%2520by%2520decomposing%2520images%2520into%2520class-specific%2520components%2520using%2520pseudo-labels%2520and%2520sampling%2520regions%2520from%2520each%2520class.%2520Class-wise%2520predictive%2520confidence%2520further%2520guides%2520the%2520sampling%2520process%252C%2520ensuring%2520that%2520difficult%2520classes%2520receive%2520additional%2520annotations.%2520Across%2520ROI%2520classification%252C%25202-D%2520segmentation%252C%2520and%25203-D%2520segmentation%252C%2520DECOMP%2520consistently%2520surpasses%2520baseline%2520methods%2520by%2520better%2520sampling%2520minority-class%2520regions%2520and%2520boosting%2520performance%2520on%2520these%2520challenging%2520classes.%2520Code%2520is%2520in%2520https%253A//github.com/JingnaQiu/DECOMP.git.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decomposition%20Sampling%20for%20Efficient%20Region%20Annotations%20in%20Active%20Learning&entry.906535625=Jingna%20Qiu%20and%20Frauke%20Wilm%20and%20Mathias%20%C3%96ttl%20and%20Jonas%20Utz%20and%20Maja%20Schlereth%20and%20Moritz%20Schillinger%20and%20Marc%20Aubreville%20and%20Katharina%20Breininger&entry.1292438233=Active%20learning%20improves%20annotation%20efficiency%20by%20selecting%20the%20most%20informative%20samples%20for%20annotation%20and%20model%20training.%20While%20most%20prior%20work%20has%20focused%20on%20selecting%20informative%20images%20for%20classification%20tasks%2C%20we%20investigate%20the%20more%20challenging%20setting%20of%20dense%20prediction%2C%20where%20annotations%20are%20more%20costly%20and%20time-intensive%2C%20especially%20in%20medical%20imaging.%20Region-level%20annotation%20has%20been%20shown%20to%20be%20more%20efficient%20than%20image-level%20annotation%20for%20these%20tasks.%20However%2C%20existing%20methods%20for%20representative%20annotation%20region%20selection%20suffer%20from%20high%20computational%20and%20memory%20costs%2C%20irrelevant%20region%20choices%2C%20and%20heavy%20reliance%20on%20uncertainty%20sampling.%20We%20propose%20decomposition%20sampling%20%28DECOMP%29%2C%20a%20new%20active%20learning%20sampling%20strategy%20that%20addresses%20these%20limitations.%20It%20enhances%20annotation%20diversity%20by%20decomposing%20images%20into%20class-specific%20components%20using%20pseudo-labels%20and%20sampling%20regions%20from%20each%20class.%20Class-wise%20predictive%20confidence%20further%20guides%20the%20sampling%20process%2C%20ensuring%20that%20difficult%20classes%20receive%20additional%20annotations.%20Across%20ROI%20classification%2C%202-D%20segmentation%2C%20and%203-D%20segmentation%2C%20DECOMP%20consistently%20surpasses%20baseline%20methods%20by%20better%20sampling%20minority-class%20regions%20and%20boosting%20performance%20on%20these%20challenging%20classes.%20Code%20is%20in%20https%3A//github.com/JingnaQiu/DECOMP.git.&entry.1838667208=http%3A//arxiv.org/abs/2512.07606v1&entry.124074799=Read"},
{"title": "Suite-IN++: A FlexiWear BodyNet Integrating Global and Local Motion Features from Apple Suite for Robust Inertial Navigation", "author": "Lan Sun and Songpengcheng Xia and Jiarui Yang and Ling Pei", "abstract": "The proliferation of wearable technology has established multi-device ecosystems comprising smartphones, smartwatches, and headphones as critical enablers for ubiquitous pedestrian localization. However, traditional pedestrian dead reckoning (PDR) struggles with diverse motion modes, while data-driven methods, despite improving accuracy, often lack robustness due to their reliance on a single-device setup. Therefore, a promising solution is to fully leverage existing wearable devices to form a flexiwear bodynet for robust and accurate pedestrian localization. This paper presents Suite-IN++, a deep learning framework for flexiwear bodynet-based pedestrian localization. Suite-IN++ integrates motion data from wearable devices on different body parts, using contrastive learning to separate global and local motion features. It fuses global features based on the data reliability of each device to capture overall motion trends and employs an attention mechanism to uncover cross-device correlations in local features, extracting motion details helpful for accurate localization. To evaluate our method, we construct a real-life flexiwear bodynet dataset, incorporating Apple Suite (iPhone, Apple Watch, and AirPods) across diverse walking modes and device configurations. Experimental results demonstrate that Suite-IN++ achieves superior localization accuracy and robustness, significantly outperforming state-of-the-art models in real-life pedestrian tracking scenarios.", "link": "http://arxiv.org/abs/2504.00438v2", "date": "2025-12-08", "relevancy": 2.1646, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5656}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5414}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Suite-IN%2B%2B%3A%20A%20FlexiWear%20BodyNet%20Integrating%20Global%20and%20Local%20Motion%20Features%20from%20Apple%20Suite%20for%20Robust%20Inertial%20Navigation&body=Title%3A%20Suite-IN%2B%2B%3A%20A%20FlexiWear%20BodyNet%20Integrating%20Global%20and%20Local%20Motion%20Features%20from%20Apple%20Suite%20for%20Robust%20Inertial%20Navigation%0AAuthor%3A%20Lan%20Sun%20and%20Songpengcheng%20Xia%20and%20Jiarui%20Yang%20and%20Ling%20Pei%0AAbstract%3A%20The%20proliferation%20of%20wearable%20technology%20has%20established%20multi-device%20ecosystems%20comprising%20smartphones%2C%20smartwatches%2C%20and%20headphones%20as%20critical%20enablers%20for%20ubiquitous%20pedestrian%20localization.%20However%2C%20traditional%20pedestrian%20dead%20reckoning%20%28PDR%29%20struggles%20with%20diverse%20motion%20modes%2C%20while%20data-driven%20methods%2C%20despite%20improving%20accuracy%2C%20often%20lack%20robustness%20due%20to%20their%20reliance%20on%20a%20single-device%20setup.%20Therefore%2C%20a%20promising%20solution%20is%20to%20fully%20leverage%20existing%20wearable%20devices%20to%20form%20a%20flexiwear%20bodynet%20for%20robust%20and%20accurate%20pedestrian%20localization.%20This%20paper%20presents%20Suite-IN%2B%2B%2C%20a%20deep%20learning%20framework%20for%20flexiwear%20bodynet-based%20pedestrian%20localization.%20Suite-IN%2B%2B%20integrates%20motion%20data%20from%20wearable%20devices%20on%20different%20body%20parts%2C%20using%20contrastive%20learning%20to%20separate%20global%20and%20local%20motion%20features.%20It%20fuses%20global%20features%20based%20on%20the%20data%20reliability%20of%20each%20device%20to%20capture%20overall%20motion%20trends%20and%20employs%20an%20attention%20mechanism%20to%20uncover%20cross-device%20correlations%20in%20local%20features%2C%20extracting%20motion%20details%20helpful%20for%20accurate%20localization.%20To%20evaluate%20our%20method%2C%20we%20construct%20a%20real-life%20flexiwear%20bodynet%20dataset%2C%20incorporating%20Apple%20Suite%20%28iPhone%2C%20Apple%20Watch%2C%20and%20AirPods%29%20across%20diverse%20walking%20modes%20and%20device%20configurations.%20Experimental%20results%20demonstrate%20that%20Suite-IN%2B%2B%20achieves%20superior%20localization%20accuracy%20and%20robustness%2C%20significantly%20outperforming%20state-of-the-art%20models%20in%20real-life%20pedestrian%20tracking%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2504.00438v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuite-IN%252B%252B%253A%2520A%2520FlexiWear%2520BodyNet%2520Integrating%2520Global%2520and%2520Local%2520Motion%2520Features%2520from%2520Apple%2520Suite%2520for%2520Robust%2520Inertial%2520Navigation%26entry.906535625%3DLan%2520Sun%2520and%2520Songpengcheng%2520Xia%2520and%2520Jiarui%2520Yang%2520and%2520Ling%2520Pei%26entry.1292438233%3DThe%2520proliferation%2520of%2520wearable%2520technology%2520has%2520established%2520multi-device%2520ecosystems%2520comprising%2520smartphones%252C%2520smartwatches%252C%2520and%2520headphones%2520as%2520critical%2520enablers%2520for%2520ubiquitous%2520pedestrian%2520localization.%2520However%252C%2520traditional%2520pedestrian%2520dead%2520reckoning%2520%2528PDR%2529%2520struggles%2520with%2520diverse%2520motion%2520modes%252C%2520while%2520data-driven%2520methods%252C%2520despite%2520improving%2520accuracy%252C%2520often%2520lack%2520robustness%2520due%2520to%2520their%2520reliance%2520on%2520a%2520single-device%2520setup.%2520Therefore%252C%2520a%2520promising%2520solution%2520is%2520to%2520fully%2520leverage%2520existing%2520wearable%2520devices%2520to%2520form%2520a%2520flexiwear%2520bodynet%2520for%2520robust%2520and%2520accurate%2520pedestrian%2520localization.%2520This%2520paper%2520presents%2520Suite-IN%252B%252B%252C%2520a%2520deep%2520learning%2520framework%2520for%2520flexiwear%2520bodynet-based%2520pedestrian%2520localization.%2520Suite-IN%252B%252B%2520integrates%2520motion%2520data%2520from%2520wearable%2520devices%2520on%2520different%2520body%2520parts%252C%2520using%2520contrastive%2520learning%2520to%2520separate%2520global%2520and%2520local%2520motion%2520features.%2520It%2520fuses%2520global%2520features%2520based%2520on%2520the%2520data%2520reliability%2520of%2520each%2520device%2520to%2520capture%2520overall%2520motion%2520trends%2520and%2520employs%2520an%2520attention%2520mechanism%2520to%2520uncover%2520cross-device%2520correlations%2520in%2520local%2520features%252C%2520extracting%2520motion%2520details%2520helpful%2520for%2520accurate%2520localization.%2520To%2520evaluate%2520our%2520method%252C%2520we%2520construct%2520a%2520real-life%2520flexiwear%2520bodynet%2520dataset%252C%2520incorporating%2520Apple%2520Suite%2520%2528iPhone%252C%2520Apple%2520Watch%252C%2520and%2520AirPods%2529%2520across%2520diverse%2520walking%2520modes%2520and%2520device%2520configurations.%2520Experimental%2520results%2520demonstrate%2520that%2520Suite-IN%252B%252B%2520achieves%2520superior%2520localization%2520accuracy%2520and%2520robustness%252C%2520significantly%2520outperforming%2520state-of-the-art%2520models%2520in%2520real-life%2520pedestrian%2520tracking%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.00438v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Suite-IN%2B%2B%3A%20A%20FlexiWear%20BodyNet%20Integrating%20Global%20and%20Local%20Motion%20Features%20from%20Apple%20Suite%20for%20Robust%20Inertial%20Navigation&entry.906535625=Lan%20Sun%20and%20Songpengcheng%20Xia%20and%20Jiarui%20Yang%20and%20Ling%20Pei&entry.1292438233=The%20proliferation%20of%20wearable%20technology%20has%20established%20multi-device%20ecosystems%20comprising%20smartphones%2C%20smartwatches%2C%20and%20headphones%20as%20critical%20enablers%20for%20ubiquitous%20pedestrian%20localization.%20However%2C%20traditional%20pedestrian%20dead%20reckoning%20%28PDR%29%20struggles%20with%20diverse%20motion%20modes%2C%20while%20data-driven%20methods%2C%20despite%20improving%20accuracy%2C%20often%20lack%20robustness%20due%20to%20their%20reliance%20on%20a%20single-device%20setup.%20Therefore%2C%20a%20promising%20solution%20is%20to%20fully%20leverage%20existing%20wearable%20devices%20to%20form%20a%20flexiwear%20bodynet%20for%20robust%20and%20accurate%20pedestrian%20localization.%20This%20paper%20presents%20Suite-IN%2B%2B%2C%20a%20deep%20learning%20framework%20for%20flexiwear%20bodynet-based%20pedestrian%20localization.%20Suite-IN%2B%2B%20integrates%20motion%20data%20from%20wearable%20devices%20on%20different%20body%20parts%2C%20using%20contrastive%20learning%20to%20separate%20global%20and%20local%20motion%20features.%20It%20fuses%20global%20features%20based%20on%20the%20data%20reliability%20of%20each%20device%20to%20capture%20overall%20motion%20trends%20and%20employs%20an%20attention%20mechanism%20to%20uncover%20cross-device%20correlations%20in%20local%20features%2C%20extracting%20motion%20details%20helpful%20for%20accurate%20localization.%20To%20evaluate%20our%20method%2C%20we%20construct%20a%20real-life%20flexiwear%20bodynet%20dataset%2C%20incorporating%20Apple%20Suite%20%28iPhone%2C%20Apple%20Watch%2C%20and%20AirPods%29%20across%20diverse%20walking%20modes%20and%20device%20configurations.%20Experimental%20results%20demonstrate%20that%20Suite-IN%2B%2B%20achieves%20superior%20localization%20accuracy%20and%20robustness%2C%20significantly%20outperforming%20state-of-the-art%20models%20in%20real-life%20pedestrian%20tracking%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2504.00438v2&entry.124074799=Read"},
{"title": "Reconstructing Objects along Hand Interaction Timelines in Egocentric Video", "author": "Zhifan Zhu and Siddhant Bansal and Shashank Tripathi and Dima Damen", "abstract": "We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.", "link": "http://arxiv.org/abs/2512.07394v1", "date": "2025-12-08", "relevancy": 2.1625, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5512}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5416}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconstructing%20Objects%20along%20Hand%20Interaction%20Timelines%20in%20Egocentric%20Video&body=Title%3A%20Reconstructing%20Objects%20along%20Hand%20Interaction%20Timelines%20in%20Egocentric%20Video%0AAuthor%3A%20Zhifan%20Zhu%20and%20Siddhant%20Bansal%20and%20Shashank%20Tripathi%20and%20Dima%20Damen%0AAbstract%3A%20We%20introduce%20the%20task%20of%20Reconstructing%20Objects%20along%20Hand%20Interaction%20Timelines%20%28ROHIT%29.%20We%20first%20define%20the%20Hand%20Interaction%20Timeline%20%28HIT%29%20from%20a%20rigid%20object%27s%20perspective.%20In%20a%20HIT%2C%20an%20object%20is%20first%20static%20relative%20to%20the%20scene%2C%20then%20is%20held%20in%20hand%20following%20contact%2C%20where%20its%20pose%20changes.%20This%20is%20usually%20followed%20by%20a%20firm%20grip%20during%20use%2C%20before%20it%20is%20released%20to%20be%20static%20again%20w.r.t.%20to%20the%20scene.%20We%20model%20these%20pose%20constraints%20over%20the%20HIT%2C%20and%20propose%20to%20propagate%20the%20object%27s%20pose%20along%20the%20HIT%20enabling%20superior%20reconstruction%20using%20our%20proposed%20Constrained%20Optimisation%20and%20Propagation%20%28COP%29%20framework.%20Importantly%2C%20we%20focus%20on%20timelines%20with%20stable%20grasps%20-%20i.e.%20where%20the%20hand%20is%20stably%20holding%20an%20object%2C%20effectively%20maintaining%20constant%20contact%20during%20use.%20This%20allows%20us%20to%20efficiently%20annotate%2C%20study%2C%20and%20evaluate%20object%20reconstruction%20in%20videos%20without%203D%20ground%20truth.%20We%20evaluate%20our%20proposed%20task%2C%20ROHIT%2C%20over%20two%20egocentric%20datasets%2C%20HOT3D%20and%20in-the-wild%20EPIC-Kitchens.%20In%20HOT3D%2C%20we%20curate%201.2K%20clips%20of%20stable%20grasps.%20In%20EPIC-Kitchens%2C%20we%20annotate%202.4K%20clips%20of%20stable%20grasps%20including%20390%20object%20instances%20across%209%20categories%20from%20videos%20of%20daily%20interactions%20in%20141%20environments.%20Without%203D%20ground%20truth%2C%20we%20utilise%202D%20projection%20error%20to%20assess%20the%20reconstruction.%20Quantitatively%2C%20COP%20improves%20stable%20grasp%20reconstruction%20by%206.2-11.3%25%20and%20HIT%20reconstruction%20by%20up%20to%2024.5%25%20with%20constrained%20pose%20propagation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconstructing%2520Objects%2520along%2520Hand%2520Interaction%2520Timelines%2520in%2520Egocentric%2520Video%26entry.906535625%3DZhifan%2520Zhu%2520and%2520Siddhant%2520Bansal%2520and%2520Shashank%2520Tripathi%2520and%2520Dima%2520Damen%26entry.1292438233%3DWe%2520introduce%2520the%2520task%2520of%2520Reconstructing%2520Objects%2520along%2520Hand%2520Interaction%2520Timelines%2520%2528ROHIT%2529.%2520We%2520first%2520define%2520the%2520Hand%2520Interaction%2520Timeline%2520%2528HIT%2529%2520from%2520a%2520rigid%2520object%2527s%2520perspective.%2520In%2520a%2520HIT%252C%2520an%2520object%2520is%2520first%2520static%2520relative%2520to%2520the%2520scene%252C%2520then%2520is%2520held%2520in%2520hand%2520following%2520contact%252C%2520where%2520its%2520pose%2520changes.%2520This%2520is%2520usually%2520followed%2520by%2520a%2520firm%2520grip%2520during%2520use%252C%2520before%2520it%2520is%2520released%2520to%2520be%2520static%2520again%2520w.r.t.%2520to%2520the%2520scene.%2520We%2520model%2520these%2520pose%2520constraints%2520over%2520the%2520HIT%252C%2520and%2520propose%2520to%2520propagate%2520the%2520object%2527s%2520pose%2520along%2520the%2520HIT%2520enabling%2520superior%2520reconstruction%2520using%2520our%2520proposed%2520Constrained%2520Optimisation%2520and%2520Propagation%2520%2528COP%2529%2520framework.%2520Importantly%252C%2520we%2520focus%2520on%2520timelines%2520with%2520stable%2520grasps%2520-%2520i.e.%2520where%2520the%2520hand%2520is%2520stably%2520holding%2520an%2520object%252C%2520effectively%2520maintaining%2520constant%2520contact%2520during%2520use.%2520This%2520allows%2520us%2520to%2520efficiently%2520annotate%252C%2520study%252C%2520and%2520evaluate%2520object%2520reconstruction%2520in%2520videos%2520without%25203D%2520ground%2520truth.%2520We%2520evaluate%2520our%2520proposed%2520task%252C%2520ROHIT%252C%2520over%2520two%2520egocentric%2520datasets%252C%2520HOT3D%2520and%2520in-the-wild%2520EPIC-Kitchens.%2520In%2520HOT3D%252C%2520we%2520curate%25201.2K%2520clips%2520of%2520stable%2520grasps.%2520In%2520EPIC-Kitchens%252C%2520we%2520annotate%25202.4K%2520clips%2520of%2520stable%2520grasps%2520including%2520390%2520object%2520instances%2520across%25209%2520categories%2520from%2520videos%2520of%2520daily%2520interactions%2520in%2520141%2520environments.%2520Without%25203D%2520ground%2520truth%252C%2520we%2520utilise%25202D%2520projection%2520error%2520to%2520assess%2520the%2520reconstruction.%2520Quantitatively%252C%2520COP%2520improves%2520stable%2520grasp%2520reconstruction%2520by%25206.2-11.3%2525%2520and%2520HIT%2520reconstruction%2520by%2520up%2520to%252024.5%2525%2520with%2520constrained%2520pose%2520propagation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstructing%20Objects%20along%20Hand%20Interaction%20Timelines%20in%20Egocentric%20Video&entry.906535625=Zhifan%20Zhu%20and%20Siddhant%20Bansal%20and%20Shashank%20Tripathi%20and%20Dima%20Damen&entry.1292438233=We%20introduce%20the%20task%20of%20Reconstructing%20Objects%20along%20Hand%20Interaction%20Timelines%20%28ROHIT%29.%20We%20first%20define%20the%20Hand%20Interaction%20Timeline%20%28HIT%29%20from%20a%20rigid%20object%27s%20perspective.%20In%20a%20HIT%2C%20an%20object%20is%20first%20static%20relative%20to%20the%20scene%2C%20then%20is%20held%20in%20hand%20following%20contact%2C%20where%20its%20pose%20changes.%20This%20is%20usually%20followed%20by%20a%20firm%20grip%20during%20use%2C%20before%20it%20is%20released%20to%20be%20static%20again%20w.r.t.%20to%20the%20scene.%20We%20model%20these%20pose%20constraints%20over%20the%20HIT%2C%20and%20propose%20to%20propagate%20the%20object%27s%20pose%20along%20the%20HIT%20enabling%20superior%20reconstruction%20using%20our%20proposed%20Constrained%20Optimisation%20and%20Propagation%20%28COP%29%20framework.%20Importantly%2C%20we%20focus%20on%20timelines%20with%20stable%20grasps%20-%20i.e.%20where%20the%20hand%20is%20stably%20holding%20an%20object%2C%20effectively%20maintaining%20constant%20contact%20during%20use.%20This%20allows%20us%20to%20efficiently%20annotate%2C%20study%2C%20and%20evaluate%20object%20reconstruction%20in%20videos%20without%203D%20ground%20truth.%20We%20evaluate%20our%20proposed%20task%2C%20ROHIT%2C%20over%20two%20egocentric%20datasets%2C%20HOT3D%20and%20in-the-wild%20EPIC-Kitchens.%20In%20HOT3D%2C%20we%20curate%201.2K%20clips%20of%20stable%20grasps.%20In%20EPIC-Kitchens%2C%20we%20annotate%202.4K%20clips%20of%20stable%20grasps%20including%20390%20object%20instances%20across%209%20categories%20from%20videos%20of%20daily%20interactions%20in%20141%20environments.%20Without%203D%20ground%20truth%2C%20we%20utilise%202D%20projection%20error%20to%20assess%20the%20reconstruction.%20Quantitatively%2C%20COP%20improves%20stable%20grasp%20reconstruction%20by%206.2-11.3%25%20and%20HIT%20reconstruction%20by%20up%20to%2024.5%25%20with%20constrained%20pose%20propagation.&entry.1838667208=http%3A//arxiv.org/abs/2512.07394v1&entry.124074799=Read"},
{"title": "OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing", "author": "Haoyang He and Jie Wang and Jiangning Zhang and Zhucun Xue and Xingyuan Bu and Qiangpeng Yang and Shilei Wen and Lei Xie", "abstract": "The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.", "link": "http://arxiv.org/abs/2512.07826v1", "date": "2025-12-08", "relevancy": 2.161, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5702}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5338}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenVE-3M%3A%20A%20Large-Scale%20High-Quality%20Dataset%20for%20Instruction-Guided%20Video%20Editing&body=Title%3A%20OpenVE-3M%3A%20A%20Large-Scale%20High-Quality%20Dataset%20for%20Instruction-Guided%20Video%20Editing%0AAuthor%3A%20Haoyang%20He%20and%20Jie%20Wang%20and%20Jiangning%20Zhang%20and%20Zhucun%20Xue%20and%20Xingyuan%20Bu%20and%20Qiangpeng%20Yang%20and%20Shilei%20Wen%20and%20Lei%20Xie%0AAbstract%3A%20The%20quality%20and%20diversity%20of%20instruction-based%20image%20editing%20datasets%20are%20continuously%20increasing%2C%20yet%20large-scale%2C%20high-quality%20datasets%20for%20instruction-based%20video%20editing%20remain%20scarce.%20To%20address%20this%20gap%2C%20we%20introduce%20OpenVE-3M%2C%20an%20open-source%2C%20large-scale%2C%20and%20high-quality%20dataset%20for%20instruction-based%20video%20editing.%20It%20comprises%20two%20primary%20categories%3A%20spatially-aligned%20edits%20%28Global%20Style%2C%20Background%20Change%2C%20Local%20Change%2C%20Local%20Remove%2C%20Local%20Add%2C%20and%20Subtitles%20Edit%29%20and%20non-spatially-aligned%20edits%20%28Camera%20Multi-Shot%20Edit%20and%20Creative%20Edit%29.%20All%20edit%20types%20are%20generated%20via%20a%20meticulously%20designed%20data%20pipeline%20with%20rigorous%20quality%20filtering.%20OpenVE-3M%20surpasses%20existing%20open-source%20datasets%20in%20terms%20of%20scale%2C%20diversity%20of%20edit%20types%2C%20instruction%20length%2C%20and%20overall%20quality.%20Furthermore%2C%20to%20address%20the%20lack%20of%20a%20unified%20benchmark%20in%20the%20field%2C%20we%20construct%20OpenVE-Bench%2C%20containing%20431%20video-edit%20pairs%20that%20cover%20a%20diverse%20range%20of%20editing%20tasks%20with%20three%20key%20metrics%20highly%20aligned%20with%20human%20judgment.%20We%20present%20OpenVE-Edit%2C%20a%205B%20model%20trained%20on%20our%20dataset%20that%20demonstrates%20remarkable%20efficiency%20and%20effectiveness%20by%20setting%20a%20new%20state-of-the-art%20on%20OpenVE-Bench%2C%20outperforming%20all%20prior%20open-source%20models%20including%20a%2014B%20baseline.%20Project%20page%20is%20at%20https%3A//github.com/lewandofskee/OpenVE.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07826v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenVE-3M%253A%2520A%2520Large-Scale%2520High-Quality%2520Dataset%2520for%2520Instruction-Guided%2520Video%2520Editing%26entry.906535625%3DHaoyang%2520He%2520and%2520Jie%2520Wang%2520and%2520Jiangning%2520Zhang%2520and%2520Zhucun%2520Xue%2520and%2520Xingyuan%2520Bu%2520and%2520Qiangpeng%2520Yang%2520and%2520Shilei%2520Wen%2520and%2520Lei%2520Xie%26entry.1292438233%3DThe%2520quality%2520and%2520diversity%2520of%2520instruction-based%2520image%2520editing%2520datasets%2520are%2520continuously%2520increasing%252C%2520yet%2520large-scale%252C%2520high-quality%2520datasets%2520for%2520instruction-based%2520video%2520editing%2520remain%2520scarce.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520OpenVE-3M%252C%2520an%2520open-source%252C%2520large-scale%252C%2520and%2520high-quality%2520dataset%2520for%2520instruction-based%2520video%2520editing.%2520It%2520comprises%2520two%2520primary%2520categories%253A%2520spatially-aligned%2520edits%2520%2528Global%2520Style%252C%2520Background%2520Change%252C%2520Local%2520Change%252C%2520Local%2520Remove%252C%2520Local%2520Add%252C%2520and%2520Subtitles%2520Edit%2529%2520and%2520non-spatially-aligned%2520edits%2520%2528Camera%2520Multi-Shot%2520Edit%2520and%2520Creative%2520Edit%2529.%2520All%2520edit%2520types%2520are%2520generated%2520via%2520a%2520meticulously%2520designed%2520data%2520pipeline%2520with%2520rigorous%2520quality%2520filtering.%2520OpenVE-3M%2520surpasses%2520existing%2520open-source%2520datasets%2520in%2520terms%2520of%2520scale%252C%2520diversity%2520of%2520edit%2520types%252C%2520instruction%2520length%252C%2520and%2520overall%2520quality.%2520Furthermore%252C%2520to%2520address%2520the%2520lack%2520of%2520a%2520unified%2520benchmark%2520in%2520the%2520field%252C%2520we%2520construct%2520OpenVE-Bench%252C%2520containing%2520431%2520video-edit%2520pairs%2520that%2520cover%2520a%2520diverse%2520range%2520of%2520editing%2520tasks%2520with%2520three%2520key%2520metrics%2520highly%2520aligned%2520with%2520human%2520judgment.%2520We%2520present%2520OpenVE-Edit%252C%2520a%25205B%2520model%2520trained%2520on%2520our%2520dataset%2520that%2520demonstrates%2520remarkable%2520efficiency%2520and%2520effectiveness%2520by%2520setting%2520a%2520new%2520state-of-the-art%2520on%2520OpenVE-Bench%252C%2520outperforming%2520all%2520prior%2520open-source%2520models%2520including%2520a%252014B%2520baseline.%2520Project%2520page%2520is%2520at%2520https%253A//github.com/lewandofskee/OpenVE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07826v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenVE-3M%3A%20A%20Large-Scale%20High-Quality%20Dataset%20for%20Instruction-Guided%20Video%20Editing&entry.906535625=Haoyang%20He%20and%20Jie%20Wang%20and%20Jiangning%20Zhang%20and%20Zhucun%20Xue%20and%20Xingyuan%20Bu%20and%20Qiangpeng%20Yang%20and%20Shilei%20Wen%20and%20Lei%20Xie&entry.1292438233=The%20quality%20and%20diversity%20of%20instruction-based%20image%20editing%20datasets%20are%20continuously%20increasing%2C%20yet%20large-scale%2C%20high-quality%20datasets%20for%20instruction-based%20video%20editing%20remain%20scarce.%20To%20address%20this%20gap%2C%20we%20introduce%20OpenVE-3M%2C%20an%20open-source%2C%20large-scale%2C%20and%20high-quality%20dataset%20for%20instruction-based%20video%20editing.%20It%20comprises%20two%20primary%20categories%3A%20spatially-aligned%20edits%20%28Global%20Style%2C%20Background%20Change%2C%20Local%20Change%2C%20Local%20Remove%2C%20Local%20Add%2C%20and%20Subtitles%20Edit%29%20and%20non-spatially-aligned%20edits%20%28Camera%20Multi-Shot%20Edit%20and%20Creative%20Edit%29.%20All%20edit%20types%20are%20generated%20via%20a%20meticulously%20designed%20data%20pipeline%20with%20rigorous%20quality%20filtering.%20OpenVE-3M%20surpasses%20existing%20open-source%20datasets%20in%20terms%20of%20scale%2C%20diversity%20of%20edit%20types%2C%20instruction%20length%2C%20and%20overall%20quality.%20Furthermore%2C%20to%20address%20the%20lack%20of%20a%20unified%20benchmark%20in%20the%20field%2C%20we%20construct%20OpenVE-Bench%2C%20containing%20431%20video-edit%20pairs%20that%20cover%20a%20diverse%20range%20of%20editing%20tasks%20with%20three%20key%20metrics%20highly%20aligned%20with%20human%20judgment.%20We%20present%20OpenVE-Edit%2C%20a%205B%20model%20trained%20on%20our%20dataset%20that%20demonstrates%20remarkable%20efficiency%20and%20effectiveness%20by%20setting%20a%20new%20state-of-the-art%20on%20OpenVE-Bench%2C%20outperforming%20all%20prior%20open-source%20models%20including%20a%2014B%20baseline.%20Project%20page%20is%20at%20https%3A//github.com/lewandofskee/OpenVE.&entry.1838667208=http%3A//arxiv.org/abs/2512.07826v1&entry.124074799=Read"},
{"title": "Flatten Graphs as Sequences: Transformers are Scalable Graph Generators", "author": "Dexiong Chen and Markus Krimmel and Karsten Borgwardt", "abstract": "We introduce AutoGraph, a scalable autoregressive model for attributed graph generation using decoder-only transformers. By flattening graphs into random sequences of tokens through a reversible process, AutoGraph enables modeling graphs as sequences without relying on additional node features that are expensive to compute, in contrast to diffusion-based approaches. This results in sampling complexity and sequence lengths that scale optimally linearly with the number of edges, making it scalable and efficient for large, sparse graphs. A key success factor of AutoGraph is that its sequence prefixes represent induced subgraphs, creating a direct link to sub-sentences in language modeling. Empirically, AutoGraph achieves state-of-the-art performance on synthetic and molecular benchmarks, with up to 100x faster generation and 3x faster training than leading diffusion models. It also supports substructure-conditioned generation without fine-tuning and shows promising transferability, bridging language modeling and graph generation to lay the groundwork for graph foundation models. Our code is available at https://github.com/BorgwardtLab/AutoGraph.", "link": "http://arxiv.org/abs/2502.02216v3", "date": "2025-12-08", "relevancy": 2.159, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6146}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5253}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flatten%20Graphs%20as%20Sequences%3A%20Transformers%20are%20Scalable%20Graph%20Generators&body=Title%3A%20Flatten%20Graphs%20as%20Sequences%3A%20Transformers%20are%20Scalable%20Graph%20Generators%0AAuthor%3A%20Dexiong%20Chen%20and%20Markus%20Krimmel%20and%20Karsten%20Borgwardt%0AAbstract%3A%20We%20introduce%20AutoGraph%2C%20a%20scalable%20autoregressive%20model%20for%20attributed%20graph%20generation%20using%20decoder-only%20transformers.%20By%20flattening%20graphs%20into%20random%20sequences%20of%20tokens%20through%20a%20reversible%20process%2C%20AutoGraph%20enables%20modeling%20graphs%20as%20sequences%20without%20relying%20on%20additional%20node%20features%20that%20are%20expensive%20to%20compute%2C%20in%20contrast%20to%20diffusion-based%20approaches.%20This%20results%20in%20sampling%20complexity%20and%20sequence%20lengths%20that%20scale%20optimally%20linearly%20with%20the%20number%20of%20edges%2C%20making%20it%20scalable%20and%20efficient%20for%20large%2C%20sparse%20graphs.%20A%20key%20success%20factor%20of%20AutoGraph%20is%20that%20its%20sequence%20prefixes%20represent%20induced%20subgraphs%2C%20creating%20a%20direct%20link%20to%20sub-sentences%20in%20language%20modeling.%20Empirically%2C%20AutoGraph%20achieves%20state-of-the-art%20performance%20on%20synthetic%20and%20molecular%20benchmarks%2C%20with%20up%20to%20100x%20faster%20generation%20and%203x%20faster%20training%20than%20leading%20diffusion%20models.%20It%20also%20supports%20substructure-conditioned%20generation%20without%20fine-tuning%20and%20shows%20promising%20transferability%2C%20bridging%20language%20modeling%20and%20graph%20generation%20to%20lay%20the%20groundwork%20for%20graph%20foundation%20models.%20Our%20code%20is%20available%20at%20https%3A//github.com/BorgwardtLab/AutoGraph.%0ALink%3A%20http%3A//arxiv.org/abs/2502.02216v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlatten%2520Graphs%2520as%2520Sequences%253A%2520Transformers%2520are%2520Scalable%2520Graph%2520Generators%26entry.906535625%3DDexiong%2520Chen%2520and%2520Markus%2520Krimmel%2520and%2520Karsten%2520Borgwardt%26entry.1292438233%3DWe%2520introduce%2520AutoGraph%252C%2520a%2520scalable%2520autoregressive%2520model%2520for%2520attributed%2520graph%2520generation%2520using%2520decoder-only%2520transformers.%2520By%2520flattening%2520graphs%2520into%2520random%2520sequences%2520of%2520tokens%2520through%2520a%2520reversible%2520process%252C%2520AutoGraph%2520enables%2520modeling%2520graphs%2520as%2520sequences%2520without%2520relying%2520on%2520additional%2520node%2520features%2520that%2520are%2520expensive%2520to%2520compute%252C%2520in%2520contrast%2520to%2520diffusion-based%2520approaches.%2520This%2520results%2520in%2520sampling%2520complexity%2520and%2520sequence%2520lengths%2520that%2520scale%2520optimally%2520linearly%2520with%2520the%2520number%2520of%2520edges%252C%2520making%2520it%2520scalable%2520and%2520efficient%2520for%2520large%252C%2520sparse%2520graphs.%2520A%2520key%2520success%2520factor%2520of%2520AutoGraph%2520is%2520that%2520its%2520sequence%2520prefixes%2520represent%2520induced%2520subgraphs%252C%2520creating%2520a%2520direct%2520link%2520to%2520sub-sentences%2520in%2520language%2520modeling.%2520Empirically%252C%2520AutoGraph%2520achieves%2520state-of-the-art%2520performance%2520on%2520synthetic%2520and%2520molecular%2520benchmarks%252C%2520with%2520up%2520to%2520100x%2520faster%2520generation%2520and%25203x%2520faster%2520training%2520than%2520leading%2520diffusion%2520models.%2520It%2520also%2520supports%2520substructure-conditioned%2520generation%2520without%2520fine-tuning%2520and%2520shows%2520promising%2520transferability%252C%2520bridging%2520language%2520modeling%2520and%2520graph%2520generation%2520to%2520lay%2520the%2520groundwork%2520for%2520graph%2520foundation%2520models.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/BorgwardtLab/AutoGraph.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02216v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flatten%20Graphs%20as%20Sequences%3A%20Transformers%20are%20Scalable%20Graph%20Generators&entry.906535625=Dexiong%20Chen%20and%20Markus%20Krimmel%20and%20Karsten%20Borgwardt&entry.1292438233=We%20introduce%20AutoGraph%2C%20a%20scalable%20autoregressive%20model%20for%20attributed%20graph%20generation%20using%20decoder-only%20transformers.%20By%20flattening%20graphs%20into%20random%20sequences%20of%20tokens%20through%20a%20reversible%20process%2C%20AutoGraph%20enables%20modeling%20graphs%20as%20sequences%20without%20relying%20on%20additional%20node%20features%20that%20are%20expensive%20to%20compute%2C%20in%20contrast%20to%20diffusion-based%20approaches.%20This%20results%20in%20sampling%20complexity%20and%20sequence%20lengths%20that%20scale%20optimally%20linearly%20with%20the%20number%20of%20edges%2C%20making%20it%20scalable%20and%20efficient%20for%20large%2C%20sparse%20graphs.%20A%20key%20success%20factor%20of%20AutoGraph%20is%20that%20its%20sequence%20prefixes%20represent%20induced%20subgraphs%2C%20creating%20a%20direct%20link%20to%20sub-sentences%20in%20language%20modeling.%20Empirically%2C%20AutoGraph%20achieves%20state-of-the-art%20performance%20on%20synthetic%20and%20molecular%20benchmarks%2C%20with%20up%20to%20100x%20faster%20generation%20and%203x%20faster%20training%20than%20leading%20diffusion%20models.%20It%20also%20supports%20substructure-conditioned%20generation%20without%20fine-tuning%20and%20shows%20promising%20transferability%2C%20bridging%20language%20modeling%20and%20graph%20generation%20to%20lay%20the%20groundwork%20for%20graph%20foundation%20models.%20Our%20code%20is%20available%20at%20https%3A//github.com/BorgwardtLab/AutoGraph.&entry.1838667208=http%3A//arxiv.org/abs/2502.02216v3&entry.124074799=Read"},
{"title": "Transparent and Coherent Procedural Mistake Detection", "author": "Shane Storks and Itamar Bar-Yossef and Yayuan Li and Zheyuan Zhang and Jason J. Corso and Joyce Chai", "abstract": "Procedural mistake detection (PMD) is a challenging problem of classifying whether a human user (observed through egocentric video) has successfully executed a task (specified by a procedural text). Despite significant recent efforts, machine performance in the wild remains nonviable, and the reasoning processes underlying this performance are opaque. As such, we extend PMD to require generating visual self-dialog rationales to inform decisions. Given the impressive, mature image understanding capabilities observed in recent vision-and-language models (VLMs), we curate a suitable benchmark dataset for PMD based on individual frames. As our reformulation enables unprecedented transparency, we leverage a natural language inference (NLI) model to formulate two automated metrics for the coherence of generated rationales. We establish baselines for this reframed task, showing that VLMs struggle off-the-shelf, but with some trade-offs, their accuracy, coherence, and efficiency can be improved by incorporating these metrics into common inference and fine-tuning methods. Lastly, our multi-faceted metrics visualize common outcomes, highlighting areas for further improvement.", "link": "http://arxiv.org/abs/2412.11927v4", "date": "2025-12-08", "relevancy": 2.1458, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5501}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5371}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transparent%20and%20Coherent%20Procedural%20Mistake%20Detection&body=Title%3A%20Transparent%20and%20Coherent%20Procedural%20Mistake%20Detection%0AAuthor%3A%20Shane%20Storks%20and%20Itamar%20Bar-Yossef%20and%20Yayuan%20Li%20and%20Zheyuan%20Zhang%20and%20Jason%20J.%20Corso%20and%20Joyce%20Chai%0AAbstract%3A%20Procedural%20mistake%20detection%20%28PMD%29%20is%20a%20challenging%20problem%20of%20classifying%20whether%20a%20human%20user%20%28observed%20through%20egocentric%20video%29%20has%20successfully%20executed%20a%20task%20%28specified%20by%20a%20procedural%20text%29.%20Despite%20significant%20recent%20efforts%2C%20machine%20performance%20in%20the%20wild%20remains%20nonviable%2C%20and%20the%20reasoning%20processes%20underlying%20this%20performance%20are%20opaque.%20As%20such%2C%20we%20extend%20PMD%20to%20require%20generating%20visual%20self-dialog%20rationales%20to%20inform%20decisions.%20Given%20the%20impressive%2C%20mature%20image%20understanding%20capabilities%20observed%20in%20recent%20vision-and-language%20models%20%28VLMs%29%2C%20we%20curate%20a%20suitable%20benchmark%20dataset%20for%20PMD%20based%20on%20individual%20frames.%20As%20our%20reformulation%20enables%20unprecedented%20transparency%2C%20we%20leverage%20a%20natural%20language%20inference%20%28NLI%29%20model%20to%20formulate%20two%20automated%20metrics%20for%20the%20coherence%20of%20generated%20rationales.%20We%20establish%20baselines%20for%20this%20reframed%20task%2C%20showing%20that%20VLMs%20struggle%20off-the-shelf%2C%20but%20with%20some%20trade-offs%2C%20their%20accuracy%2C%20coherence%2C%20and%20efficiency%20can%20be%20improved%20by%20incorporating%20these%20metrics%20into%20common%20inference%20and%20fine-tuning%20methods.%20Lastly%2C%20our%20multi-faceted%20metrics%20visualize%20common%20outcomes%2C%20highlighting%20areas%20for%20further%20improvement.%0ALink%3A%20http%3A//arxiv.org/abs/2412.11927v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransparent%2520and%2520Coherent%2520Procedural%2520Mistake%2520Detection%26entry.906535625%3DShane%2520Storks%2520and%2520Itamar%2520Bar-Yossef%2520and%2520Yayuan%2520Li%2520and%2520Zheyuan%2520Zhang%2520and%2520Jason%2520J.%2520Corso%2520and%2520Joyce%2520Chai%26entry.1292438233%3DProcedural%2520mistake%2520detection%2520%2528PMD%2529%2520is%2520a%2520challenging%2520problem%2520of%2520classifying%2520whether%2520a%2520human%2520user%2520%2528observed%2520through%2520egocentric%2520video%2529%2520has%2520successfully%2520executed%2520a%2520task%2520%2528specified%2520by%2520a%2520procedural%2520text%2529.%2520Despite%2520significant%2520recent%2520efforts%252C%2520machine%2520performance%2520in%2520the%2520wild%2520remains%2520nonviable%252C%2520and%2520the%2520reasoning%2520processes%2520underlying%2520this%2520performance%2520are%2520opaque.%2520As%2520such%252C%2520we%2520extend%2520PMD%2520to%2520require%2520generating%2520visual%2520self-dialog%2520rationales%2520to%2520inform%2520decisions.%2520Given%2520the%2520impressive%252C%2520mature%2520image%2520understanding%2520capabilities%2520observed%2520in%2520recent%2520vision-and-language%2520models%2520%2528VLMs%2529%252C%2520we%2520curate%2520a%2520suitable%2520benchmark%2520dataset%2520for%2520PMD%2520based%2520on%2520individual%2520frames.%2520As%2520our%2520reformulation%2520enables%2520unprecedented%2520transparency%252C%2520we%2520leverage%2520a%2520natural%2520language%2520inference%2520%2528NLI%2529%2520model%2520to%2520formulate%2520two%2520automated%2520metrics%2520for%2520the%2520coherence%2520of%2520generated%2520rationales.%2520We%2520establish%2520baselines%2520for%2520this%2520reframed%2520task%252C%2520showing%2520that%2520VLMs%2520struggle%2520off-the-shelf%252C%2520but%2520with%2520some%2520trade-offs%252C%2520their%2520accuracy%252C%2520coherence%252C%2520and%2520efficiency%2520can%2520be%2520improved%2520by%2520incorporating%2520these%2520metrics%2520into%2520common%2520inference%2520and%2520fine-tuning%2520methods.%2520Lastly%252C%2520our%2520multi-faceted%2520metrics%2520visualize%2520common%2520outcomes%252C%2520highlighting%2520areas%2520for%2520further%2520improvement.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11927v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transparent%20and%20Coherent%20Procedural%20Mistake%20Detection&entry.906535625=Shane%20Storks%20and%20Itamar%20Bar-Yossef%20and%20Yayuan%20Li%20and%20Zheyuan%20Zhang%20and%20Jason%20J.%20Corso%20and%20Joyce%20Chai&entry.1292438233=Procedural%20mistake%20detection%20%28PMD%29%20is%20a%20challenging%20problem%20of%20classifying%20whether%20a%20human%20user%20%28observed%20through%20egocentric%20video%29%20has%20successfully%20executed%20a%20task%20%28specified%20by%20a%20procedural%20text%29.%20Despite%20significant%20recent%20efforts%2C%20machine%20performance%20in%20the%20wild%20remains%20nonviable%2C%20and%20the%20reasoning%20processes%20underlying%20this%20performance%20are%20opaque.%20As%20such%2C%20we%20extend%20PMD%20to%20require%20generating%20visual%20self-dialog%20rationales%20to%20inform%20decisions.%20Given%20the%20impressive%2C%20mature%20image%20understanding%20capabilities%20observed%20in%20recent%20vision-and-language%20models%20%28VLMs%29%2C%20we%20curate%20a%20suitable%20benchmark%20dataset%20for%20PMD%20based%20on%20individual%20frames.%20As%20our%20reformulation%20enables%20unprecedented%20transparency%2C%20we%20leverage%20a%20natural%20language%20inference%20%28NLI%29%20model%20to%20formulate%20two%20automated%20metrics%20for%20the%20coherence%20of%20generated%20rationales.%20We%20establish%20baselines%20for%20this%20reframed%20task%2C%20showing%20that%20VLMs%20struggle%20off-the-shelf%2C%20but%20with%20some%20trade-offs%2C%20their%20accuracy%2C%20coherence%2C%20and%20efficiency%20can%20be%20improved%20by%20incorporating%20these%20metrics%20into%20common%20inference%20and%20fine-tuning%20methods.%20Lastly%2C%20our%20multi-faceted%20metrics%20visualize%20common%20outcomes%2C%20highlighting%20areas%20for%20further%20improvement.&entry.1838667208=http%3A//arxiv.org/abs/2412.11927v4&entry.124074799=Read"},
{"title": "Switch-JustDance: Benchmarking Whole Body Motion Tracking Policies Using a Commercial Console Game", "author": "Jeonghwan Kim and Wontaek Kim and Yidan Lu and Jin Cheng and Fatemeh Zargarbashi and Zicheng Zeng and Zekun Qi and Zhiyang Dou and Nitish Sontakke and Donghoon Baek and Sehoon Ha and Tianyu Li", "abstract": "Recent advances in whole-body robot control have enabled humanoid and legged robots to perform increasingly agile and coordinated motions. However, standardized benchmarks for evaluating these capabilities in real-world settings, and in direct comparison to humans, remain scarce. Existing evaluations often rely on pre-collected human motion datasets or simulation-based experiments, which limit reproducibility, overlook hardware factors, and hinder fair human-robot comparisons. We present Switch-JustDance, a low-cost and reproducible benchmarking pipeline that leverages motion-sensing console games, Just Dance on the Nintendo Switch, to evaluate robot whole-body control. Using Just Dance on the Nintendo Switch as a representative platform, Switch-JustDance converts in-game choreography into robot-executable motions through streaming, motion reconstruction, and motion retargeting modules and enables users to evaluate controller performance through the game's built-in scoring system. We first validate the evaluation properties of Just Dance, analyzing its reliability, validity, sensitivity, and potential sources of bias. Our results show that the platform provides consistent and interpretable performance measures, making it a suitable tool for benchmarking embodied AI. Building on this foundation, we benchmark three state-of-the-art humanoid whole-body controllers on hardware and provide insights into their relative strengths and limitations.", "link": "http://arxiv.org/abs/2511.17925v2", "date": "2025-12-08", "relevancy": 2.1236, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5557}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5352}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Switch-JustDance%3A%20Benchmarking%20Whole%20Body%20Motion%20Tracking%20Policies%20Using%20a%20Commercial%20Console%20Game&body=Title%3A%20Switch-JustDance%3A%20Benchmarking%20Whole%20Body%20Motion%20Tracking%20Policies%20Using%20a%20Commercial%20Console%20Game%0AAuthor%3A%20Jeonghwan%20Kim%20and%20Wontaek%20Kim%20and%20Yidan%20Lu%20and%20Jin%20Cheng%20and%20Fatemeh%20Zargarbashi%20and%20Zicheng%20Zeng%20and%20Zekun%20Qi%20and%20Zhiyang%20Dou%20and%20Nitish%20Sontakke%20and%20Donghoon%20Baek%20and%20Sehoon%20Ha%20and%20Tianyu%20Li%0AAbstract%3A%20Recent%20advances%20in%20whole-body%20robot%20control%20have%20enabled%20humanoid%20and%20legged%20robots%20to%20perform%20increasingly%20agile%20and%20coordinated%20motions.%20However%2C%20standardized%20benchmarks%20for%20evaluating%20these%20capabilities%20in%20real-world%20settings%2C%20and%20in%20direct%20comparison%20to%20humans%2C%20remain%20scarce.%20Existing%20evaluations%20often%20rely%20on%20pre-collected%20human%20motion%20datasets%20or%20simulation-based%20experiments%2C%20which%20limit%20reproducibility%2C%20overlook%20hardware%20factors%2C%20and%20hinder%20fair%20human-robot%20comparisons.%20We%20present%20Switch-JustDance%2C%20a%20low-cost%20and%20reproducible%20benchmarking%20pipeline%20that%20leverages%20motion-sensing%20console%20games%2C%20Just%20Dance%20on%20the%20Nintendo%20Switch%2C%20to%20evaluate%20robot%20whole-body%20control.%20Using%20Just%20Dance%20on%20the%20Nintendo%20Switch%20as%20a%20representative%20platform%2C%20Switch-JustDance%20converts%20in-game%20choreography%20into%20robot-executable%20motions%20through%20streaming%2C%20motion%20reconstruction%2C%20and%20motion%20retargeting%20modules%20and%20enables%20users%20to%20evaluate%20controller%20performance%20through%20the%20game%27s%20built-in%20scoring%20system.%20We%20first%20validate%20the%20evaluation%20properties%20of%20Just%20Dance%2C%20analyzing%20its%20reliability%2C%20validity%2C%20sensitivity%2C%20and%20potential%20sources%20of%20bias.%20Our%20results%20show%20that%20the%20platform%20provides%20consistent%20and%20interpretable%20performance%20measures%2C%20making%20it%20a%20suitable%20tool%20for%20benchmarking%20embodied%20AI.%20Building%20on%20this%20foundation%2C%20we%20benchmark%20three%20state-of-the-art%20humanoid%20whole-body%20controllers%20on%20hardware%20and%20provide%20insights%20into%20their%20relative%20strengths%20and%20limitations.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17925v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSwitch-JustDance%253A%2520Benchmarking%2520Whole%2520Body%2520Motion%2520Tracking%2520Policies%2520Using%2520a%2520Commercial%2520Console%2520Game%26entry.906535625%3DJeonghwan%2520Kim%2520and%2520Wontaek%2520Kim%2520and%2520Yidan%2520Lu%2520and%2520Jin%2520Cheng%2520and%2520Fatemeh%2520Zargarbashi%2520and%2520Zicheng%2520Zeng%2520and%2520Zekun%2520Qi%2520and%2520Zhiyang%2520Dou%2520and%2520Nitish%2520Sontakke%2520and%2520Donghoon%2520Baek%2520and%2520Sehoon%2520Ha%2520and%2520Tianyu%2520Li%26entry.1292438233%3DRecent%2520advances%2520in%2520whole-body%2520robot%2520control%2520have%2520enabled%2520humanoid%2520and%2520legged%2520robots%2520to%2520perform%2520increasingly%2520agile%2520and%2520coordinated%2520motions.%2520However%252C%2520standardized%2520benchmarks%2520for%2520evaluating%2520these%2520capabilities%2520in%2520real-world%2520settings%252C%2520and%2520in%2520direct%2520comparison%2520to%2520humans%252C%2520remain%2520scarce.%2520Existing%2520evaluations%2520often%2520rely%2520on%2520pre-collected%2520human%2520motion%2520datasets%2520or%2520simulation-based%2520experiments%252C%2520which%2520limit%2520reproducibility%252C%2520overlook%2520hardware%2520factors%252C%2520and%2520hinder%2520fair%2520human-robot%2520comparisons.%2520We%2520present%2520Switch-JustDance%252C%2520a%2520low-cost%2520and%2520reproducible%2520benchmarking%2520pipeline%2520that%2520leverages%2520motion-sensing%2520console%2520games%252C%2520Just%2520Dance%2520on%2520the%2520Nintendo%2520Switch%252C%2520to%2520evaluate%2520robot%2520whole-body%2520control.%2520Using%2520Just%2520Dance%2520on%2520the%2520Nintendo%2520Switch%2520as%2520a%2520representative%2520platform%252C%2520Switch-JustDance%2520converts%2520in-game%2520choreography%2520into%2520robot-executable%2520motions%2520through%2520streaming%252C%2520motion%2520reconstruction%252C%2520and%2520motion%2520retargeting%2520modules%2520and%2520enables%2520users%2520to%2520evaluate%2520controller%2520performance%2520through%2520the%2520game%2527s%2520built-in%2520scoring%2520system.%2520We%2520first%2520validate%2520the%2520evaluation%2520properties%2520of%2520Just%2520Dance%252C%2520analyzing%2520its%2520reliability%252C%2520validity%252C%2520sensitivity%252C%2520and%2520potential%2520sources%2520of%2520bias.%2520Our%2520results%2520show%2520that%2520the%2520platform%2520provides%2520consistent%2520and%2520interpretable%2520performance%2520measures%252C%2520making%2520it%2520a%2520suitable%2520tool%2520for%2520benchmarking%2520embodied%2520AI.%2520Building%2520on%2520this%2520foundation%252C%2520we%2520benchmark%2520three%2520state-of-the-art%2520humanoid%2520whole-body%2520controllers%2520on%2520hardware%2520and%2520provide%2520insights%2520into%2520their%2520relative%2520strengths%2520and%2520limitations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17925v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Switch-JustDance%3A%20Benchmarking%20Whole%20Body%20Motion%20Tracking%20Policies%20Using%20a%20Commercial%20Console%20Game&entry.906535625=Jeonghwan%20Kim%20and%20Wontaek%20Kim%20and%20Yidan%20Lu%20and%20Jin%20Cheng%20and%20Fatemeh%20Zargarbashi%20and%20Zicheng%20Zeng%20and%20Zekun%20Qi%20and%20Zhiyang%20Dou%20and%20Nitish%20Sontakke%20and%20Donghoon%20Baek%20and%20Sehoon%20Ha%20and%20Tianyu%20Li&entry.1292438233=Recent%20advances%20in%20whole-body%20robot%20control%20have%20enabled%20humanoid%20and%20legged%20robots%20to%20perform%20increasingly%20agile%20and%20coordinated%20motions.%20However%2C%20standardized%20benchmarks%20for%20evaluating%20these%20capabilities%20in%20real-world%20settings%2C%20and%20in%20direct%20comparison%20to%20humans%2C%20remain%20scarce.%20Existing%20evaluations%20often%20rely%20on%20pre-collected%20human%20motion%20datasets%20or%20simulation-based%20experiments%2C%20which%20limit%20reproducibility%2C%20overlook%20hardware%20factors%2C%20and%20hinder%20fair%20human-robot%20comparisons.%20We%20present%20Switch-JustDance%2C%20a%20low-cost%20and%20reproducible%20benchmarking%20pipeline%20that%20leverages%20motion-sensing%20console%20games%2C%20Just%20Dance%20on%20the%20Nintendo%20Switch%2C%20to%20evaluate%20robot%20whole-body%20control.%20Using%20Just%20Dance%20on%20the%20Nintendo%20Switch%20as%20a%20representative%20platform%2C%20Switch-JustDance%20converts%20in-game%20choreography%20into%20robot-executable%20motions%20through%20streaming%2C%20motion%20reconstruction%2C%20and%20motion%20retargeting%20modules%20and%20enables%20users%20to%20evaluate%20controller%20performance%20through%20the%20game%27s%20built-in%20scoring%20system.%20We%20first%20validate%20the%20evaluation%20properties%20of%20Just%20Dance%2C%20analyzing%20its%20reliability%2C%20validity%2C%20sensitivity%2C%20and%20potential%20sources%20of%20bias.%20Our%20results%20show%20that%20the%20platform%20provides%20consistent%20and%20interpretable%20performance%20measures%2C%20making%20it%20a%20suitable%20tool%20for%20benchmarking%20embodied%20AI.%20Building%20on%20this%20foundation%2C%20we%20benchmark%20three%20state-of-the-art%20humanoid%20whole-body%20controllers%20on%20hardware%20and%20provide%20insights%20into%20their%20relative%20strengths%20and%20limitations.&entry.1838667208=http%3A//arxiv.org/abs/2511.17925v2&entry.124074799=Read"},
{"title": "Blurry-Edges: Photon-Limited Depth Estimation from Defocused Boundaries", "author": "Wei Xu and Charles James Wagner and Junjie Luo and Qi Guo", "abstract": "Extracting depth information from photon-limited, defocused images is challenging because depth from defocus (DfD) relies on accurate estimation of defocus blur, which is fundamentally sensitive to image noise. We present a novel approach to robustly measure object depths from photon-limited images along the defocused boundaries. It is based on a new image patch representation, Blurry-Edges, that explicitly stores and visualizes a rich set of low-level patch information, including boundaries, color, and smoothness. We develop a deep neural network architecture that predicts the Blurry-Edges representation from a pair of differently defocused images, from which depth can be calculated using a closed-form DfD relation we derive. The experimental results on synthetic and real data show that our method achieves the highest depth estimation accuracy on photon-limited images compared to a broad range of state-of-the-art DfD methods.", "link": "http://arxiv.org/abs/2503.23606v2", "date": "2025-12-08", "relevancy": 2.1185, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5352}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5295}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Blurry-Edges%3A%20Photon-Limited%20Depth%20Estimation%20from%20Defocused%20Boundaries&body=Title%3A%20Blurry-Edges%3A%20Photon-Limited%20Depth%20Estimation%20from%20Defocused%20Boundaries%0AAuthor%3A%20Wei%20Xu%20and%20Charles%20James%20Wagner%20and%20Junjie%20Luo%20and%20Qi%20Guo%0AAbstract%3A%20Extracting%20depth%20information%20from%20photon-limited%2C%20defocused%20images%20is%20challenging%20because%20depth%20from%20defocus%20%28DfD%29%20relies%20on%20accurate%20estimation%20of%20defocus%20blur%2C%20which%20is%20fundamentally%20sensitive%20to%20image%20noise.%20We%20present%20a%20novel%20approach%20to%20robustly%20measure%20object%20depths%20from%20photon-limited%20images%20along%20the%20defocused%20boundaries.%20It%20is%20based%20on%20a%20new%20image%20patch%20representation%2C%20Blurry-Edges%2C%20that%20explicitly%20stores%20and%20visualizes%20a%20rich%20set%20of%20low-level%20patch%20information%2C%20including%20boundaries%2C%20color%2C%20and%20smoothness.%20We%20develop%20a%20deep%20neural%20network%20architecture%20that%20predicts%20the%20Blurry-Edges%20representation%20from%20a%20pair%20of%20differently%20defocused%20images%2C%20from%20which%20depth%20can%20be%20calculated%20using%20a%20closed-form%20DfD%20relation%20we%20derive.%20The%20experimental%20results%20on%20synthetic%20and%20real%20data%20show%20that%20our%20method%20achieves%20the%20highest%20depth%20estimation%20accuracy%20on%20photon-limited%20images%20compared%20to%20a%20broad%20range%20of%20state-of-the-art%20DfD%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2503.23606v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlurry-Edges%253A%2520Photon-Limited%2520Depth%2520Estimation%2520from%2520Defocused%2520Boundaries%26entry.906535625%3DWei%2520Xu%2520and%2520Charles%2520James%2520Wagner%2520and%2520Junjie%2520Luo%2520and%2520Qi%2520Guo%26entry.1292438233%3DExtracting%2520depth%2520information%2520from%2520photon-limited%252C%2520defocused%2520images%2520is%2520challenging%2520because%2520depth%2520from%2520defocus%2520%2528DfD%2529%2520relies%2520on%2520accurate%2520estimation%2520of%2520defocus%2520blur%252C%2520which%2520is%2520fundamentally%2520sensitive%2520to%2520image%2520noise.%2520We%2520present%2520a%2520novel%2520approach%2520to%2520robustly%2520measure%2520object%2520depths%2520from%2520photon-limited%2520images%2520along%2520the%2520defocused%2520boundaries.%2520It%2520is%2520based%2520on%2520a%2520new%2520image%2520patch%2520representation%252C%2520Blurry-Edges%252C%2520that%2520explicitly%2520stores%2520and%2520visualizes%2520a%2520rich%2520set%2520of%2520low-level%2520patch%2520information%252C%2520including%2520boundaries%252C%2520color%252C%2520and%2520smoothness.%2520We%2520develop%2520a%2520deep%2520neural%2520network%2520architecture%2520that%2520predicts%2520the%2520Blurry-Edges%2520representation%2520from%2520a%2520pair%2520of%2520differently%2520defocused%2520images%252C%2520from%2520which%2520depth%2520can%2520be%2520calculated%2520using%2520a%2520closed-form%2520DfD%2520relation%2520we%2520derive.%2520The%2520experimental%2520results%2520on%2520synthetic%2520and%2520real%2520data%2520show%2520that%2520our%2520method%2520achieves%2520the%2520highest%2520depth%2520estimation%2520accuracy%2520on%2520photon-limited%2520images%2520compared%2520to%2520a%2520broad%2520range%2520of%2520state-of-the-art%2520DfD%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.23606v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blurry-Edges%3A%20Photon-Limited%20Depth%20Estimation%20from%20Defocused%20Boundaries&entry.906535625=Wei%20Xu%20and%20Charles%20James%20Wagner%20and%20Junjie%20Luo%20and%20Qi%20Guo&entry.1292438233=Extracting%20depth%20information%20from%20photon-limited%2C%20defocused%20images%20is%20challenging%20because%20depth%20from%20defocus%20%28DfD%29%20relies%20on%20accurate%20estimation%20of%20defocus%20blur%2C%20which%20is%20fundamentally%20sensitive%20to%20image%20noise.%20We%20present%20a%20novel%20approach%20to%20robustly%20measure%20object%20depths%20from%20photon-limited%20images%20along%20the%20defocused%20boundaries.%20It%20is%20based%20on%20a%20new%20image%20patch%20representation%2C%20Blurry-Edges%2C%20that%20explicitly%20stores%20and%20visualizes%20a%20rich%20set%20of%20low-level%20patch%20information%2C%20including%20boundaries%2C%20color%2C%20and%20smoothness.%20We%20develop%20a%20deep%20neural%20network%20architecture%20that%20predicts%20the%20Blurry-Edges%20representation%20from%20a%20pair%20of%20differently%20defocused%20images%2C%20from%20which%20depth%20can%20be%20calculated%20using%20a%20closed-form%20DfD%20relation%20we%20derive.%20The%20experimental%20results%20on%20synthetic%20and%20real%20data%20show%20that%20our%20method%20achieves%20the%20highest%20depth%20estimation%20accuracy%20on%20photon-limited%20images%20compared%20to%20a%20broad%20range%20of%20state-of-the-art%20DfD%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2503.23606v2&entry.124074799=Read"},
{"title": "ReLaX: Reasoning with Latent Exploration for Large Reasoning Models", "author": "Shimin Zhang and Xianwei Chen and Yufan Shen and Ziyuan Ye and Jibin Wu", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated remarkable potential in enhancing the reasoning capability of Large Reasoning Models (LRMs). However, RLVR often leads to entropy collapse, resulting in premature policy convergence and performance saturation. While manipulating token-level entropy has proven effective for promoting policy exploration, we argue that the latent dynamics underlying token generation encode a far richer computational structure for steering policy optimization toward a more effective exploration-exploitation tradeoff. To enable tractable analysis and intervention of the latent dynamics of LRMs, we leverage Koopman operator theory to obtain a linearized representation of their hidden-state dynamics. This enables us to introduce Dynamic Spectral Dispersion (DSD), a new metric to quantify the heterogeneity of the model's latent dynamics, serving as a direct indicator of policy exploration. Building upon these foundations, we propose Reasoning with Latent eXploration (ReLaX), a paradigm that explicitly incorporates latent dynamics to regulate exploration and exploitation during policy optimization. Comprehensive experiments across a wide range of multimodal and text-only reasoning benchmarks show that ReLaX significantly mitigates premature convergence and consistently achieves state-of-the-art performance.", "link": "http://arxiv.org/abs/2512.07558v1", "date": "2025-12-08", "relevancy": 2.1036, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5296}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5252}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReLaX%3A%20Reasoning%20with%20Latent%20Exploration%20for%20Large%20Reasoning%20Models&body=Title%3A%20ReLaX%3A%20Reasoning%20with%20Latent%20Exploration%20for%20Large%20Reasoning%20Models%0AAuthor%3A%20Shimin%20Zhang%20and%20Xianwei%20Chen%20and%20Yufan%20Shen%20and%20Ziyuan%20Ye%20and%20Jibin%20Wu%0AAbstract%3A%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20recently%20demonstrated%20remarkable%20potential%20in%20enhancing%20the%20reasoning%20capability%20of%20Large%20Reasoning%20Models%20%28LRMs%29.%20However%2C%20RLVR%20often%20leads%20to%20entropy%20collapse%2C%20resulting%20in%20premature%20policy%20convergence%20and%20performance%20saturation.%20While%20manipulating%20token-level%20entropy%20has%20proven%20effective%20for%20promoting%20policy%20exploration%2C%20we%20argue%20that%20the%20latent%20dynamics%20underlying%20token%20generation%20encode%20a%20far%20richer%20computational%20structure%20for%20steering%20policy%20optimization%20toward%20a%20more%20effective%20exploration-exploitation%20tradeoff.%20To%20enable%20tractable%20analysis%20and%20intervention%20of%20the%20latent%20dynamics%20of%20LRMs%2C%20we%20leverage%20Koopman%20operator%20theory%20to%20obtain%20a%20linearized%20representation%20of%20their%20hidden-state%20dynamics.%20This%20enables%20us%20to%20introduce%20Dynamic%20Spectral%20Dispersion%20%28DSD%29%2C%20a%20new%20metric%20to%20quantify%20the%20heterogeneity%20of%20the%20model%27s%20latent%20dynamics%2C%20serving%20as%20a%20direct%20indicator%20of%20policy%20exploration.%20Building%20upon%20these%20foundations%2C%20we%20propose%20Reasoning%20with%20Latent%20eXploration%20%28ReLaX%29%2C%20a%20paradigm%20that%20explicitly%20incorporates%20latent%20dynamics%20to%20regulate%20exploration%20and%20exploitation%20during%20policy%20optimization.%20Comprehensive%20experiments%20across%20a%20wide%20range%20of%20multimodal%20and%20text-only%20reasoning%20benchmarks%20show%20that%20ReLaX%20significantly%20mitigates%20premature%20convergence%20and%20consistently%20achieves%20state-of-the-art%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReLaX%253A%2520Reasoning%2520with%2520Latent%2520Exploration%2520for%2520Large%2520Reasoning%2520Models%26entry.906535625%3DShimin%2520Zhang%2520and%2520Xianwei%2520Chen%2520and%2520Yufan%2520Shen%2520and%2520Ziyuan%2520Ye%2520and%2520Jibin%2520Wu%26entry.1292438233%3DReinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520has%2520recently%2520demonstrated%2520remarkable%2520potential%2520in%2520enhancing%2520the%2520reasoning%2520capability%2520of%2520Large%2520Reasoning%2520Models%2520%2528LRMs%2529.%2520However%252C%2520RLVR%2520often%2520leads%2520to%2520entropy%2520collapse%252C%2520resulting%2520in%2520premature%2520policy%2520convergence%2520and%2520performance%2520saturation.%2520While%2520manipulating%2520token-level%2520entropy%2520has%2520proven%2520effective%2520for%2520promoting%2520policy%2520exploration%252C%2520we%2520argue%2520that%2520the%2520latent%2520dynamics%2520underlying%2520token%2520generation%2520encode%2520a%2520far%2520richer%2520computational%2520structure%2520for%2520steering%2520policy%2520optimization%2520toward%2520a%2520more%2520effective%2520exploration-exploitation%2520tradeoff.%2520To%2520enable%2520tractable%2520analysis%2520and%2520intervention%2520of%2520the%2520latent%2520dynamics%2520of%2520LRMs%252C%2520we%2520leverage%2520Koopman%2520operator%2520theory%2520to%2520obtain%2520a%2520linearized%2520representation%2520of%2520their%2520hidden-state%2520dynamics.%2520This%2520enables%2520us%2520to%2520introduce%2520Dynamic%2520Spectral%2520Dispersion%2520%2528DSD%2529%252C%2520a%2520new%2520metric%2520to%2520quantify%2520the%2520heterogeneity%2520of%2520the%2520model%2527s%2520latent%2520dynamics%252C%2520serving%2520as%2520a%2520direct%2520indicator%2520of%2520policy%2520exploration.%2520Building%2520upon%2520these%2520foundations%252C%2520we%2520propose%2520Reasoning%2520with%2520Latent%2520eXploration%2520%2528ReLaX%2529%252C%2520a%2520paradigm%2520that%2520explicitly%2520incorporates%2520latent%2520dynamics%2520to%2520regulate%2520exploration%2520and%2520exploitation%2520during%2520policy%2520optimization.%2520Comprehensive%2520experiments%2520across%2520a%2520wide%2520range%2520of%2520multimodal%2520and%2520text-only%2520reasoning%2520benchmarks%2520show%2520that%2520ReLaX%2520significantly%2520mitigates%2520premature%2520convergence%2520and%2520consistently%2520achieves%2520state-of-the-art%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReLaX%3A%20Reasoning%20with%20Latent%20Exploration%20for%20Large%20Reasoning%20Models&entry.906535625=Shimin%20Zhang%20and%20Xianwei%20Chen%20and%20Yufan%20Shen%20and%20Ziyuan%20Ye%20and%20Jibin%20Wu&entry.1292438233=Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20recently%20demonstrated%20remarkable%20potential%20in%20enhancing%20the%20reasoning%20capability%20of%20Large%20Reasoning%20Models%20%28LRMs%29.%20However%2C%20RLVR%20often%20leads%20to%20entropy%20collapse%2C%20resulting%20in%20premature%20policy%20convergence%20and%20performance%20saturation.%20While%20manipulating%20token-level%20entropy%20has%20proven%20effective%20for%20promoting%20policy%20exploration%2C%20we%20argue%20that%20the%20latent%20dynamics%20underlying%20token%20generation%20encode%20a%20far%20richer%20computational%20structure%20for%20steering%20policy%20optimization%20toward%20a%20more%20effective%20exploration-exploitation%20tradeoff.%20To%20enable%20tractable%20analysis%20and%20intervention%20of%20the%20latent%20dynamics%20of%20LRMs%2C%20we%20leverage%20Koopman%20operator%20theory%20to%20obtain%20a%20linearized%20representation%20of%20their%20hidden-state%20dynamics.%20This%20enables%20us%20to%20introduce%20Dynamic%20Spectral%20Dispersion%20%28DSD%29%2C%20a%20new%20metric%20to%20quantify%20the%20heterogeneity%20of%20the%20model%27s%20latent%20dynamics%2C%20serving%20as%20a%20direct%20indicator%20of%20policy%20exploration.%20Building%20upon%20these%20foundations%2C%20we%20propose%20Reasoning%20with%20Latent%20eXploration%20%28ReLaX%29%2C%20a%20paradigm%20that%20explicitly%20incorporates%20latent%20dynamics%20to%20regulate%20exploration%20and%20exploitation%20during%20policy%20optimization.%20Comprehensive%20experiments%20across%20a%20wide%20range%20of%20multimodal%20and%20text-only%20reasoning%20benchmarks%20show%20that%20ReLaX%20significantly%20mitigates%20premature%20convergence%20and%20consistently%20achieves%20state-of-the-art%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.07558v1&entry.124074799=Read"},
{"title": "Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment", "author": "Sangha Park and Eunji Kim and Yeongtak Oh and Jooyoung Choi and Sungroh Yoon", "abstract": "Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.", "link": "http://arxiv.org/abs/2512.07702v1", "date": "2025-12-08", "relevancy": 2.1035, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5419}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5271}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guiding%20What%20Not%20to%20Generate%3A%20Automated%20Negative%20Prompting%20for%20Text-Image%20Alignment&body=Title%3A%20Guiding%20What%20Not%20to%20Generate%3A%20Automated%20Negative%20Prompting%20for%20Text-Image%20Alignment%0AAuthor%3A%20Sangha%20Park%20and%20Eunji%20Kim%20and%20Yeongtak%20Oh%20and%20Jooyoung%20Choi%20and%20Sungroh%20Yoon%0AAbstract%3A%20Despite%20substantial%20progress%20in%20text-to-image%20generation%2C%20achieving%20precise%20text-image%20alignment%20remains%20challenging%2C%20particularly%20for%20prompts%20with%20rich%20compositional%20structure%20or%20imaginative%20elements.%20To%20address%20this%2C%20we%20introduce%20Negative%20Prompting%20for%20Image%20Correction%20%28NPC%29%2C%20an%20automated%20pipeline%20that%20improves%20alignment%20by%20identifying%20and%20applying%20negative%20prompts%20that%20suppress%20unintended%20content.%20We%20begin%20by%20analyzing%20cross-attention%20patterns%20to%20explain%20why%20both%20targeted%20negatives-those%20directly%20tied%20to%20the%20prompt%27s%20alignment%20error-and%20untargeted%20negatives-tokens%20unrelated%20to%20the%20prompt%20but%20present%20in%20the%20generated%20image-can%20enhance%20alignment.%20To%20discover%20useful%20negatives%2C%20NPC%20generates%20candidate%20prompts%20using%20a%20verifier-captioner-proposer%20framework%20and%20ranks%20them%20with%20a%20salient%20text-space%20score%2C%20enabling%20effective%20selection%20without%20requiring%20additional%20image%20synthesis.%20On%20GenEval%2B%2B%20and%20Imagine-Bench%2C%20NPC%20outperforms%20strong%20baselines%2C%20achieving%200.571%20vs.%200.371%20on%20GenEval%2B%2B%20and%20the%20best%20overall%20performance%20on%20Imagine-Bench.%20By%20guiding%20what%20not%20to%20generate%2C%20NPC%20provides%20a%20principled%2C%20fully%20automated%20route%20to%20stronger%20text-image%20alignment%20in%20diffusion%20models.%20Code%20is%20released%20at%20https%3A//github.com/wiarae/NPC.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuiding%2520What%2520Not%2520to%2520Generate%253A%2520Automated%2520Negative%2520Prompting%2520for%2520Text-Image%2520Alignment%26entry.906535625%3DSangha%2520Park%2520and%2520Eunji%2520Kim%2520and%2520Yeongtak%2520Oh%2520and%2520Jooyoung%2520Choi%2520and%2520Sungroh%2520Yoon%26entry.1292438233%3DDespite%2520substantial%2520progress%2520in%2520text-to-image%2520generation%252C%2520achieving%2520precise%2520text-image%2520alignment%2520remains%2520challenging%252C%2520particularly%2520for%2520prompts%2520with%2520rich%2520compositional%2520structure%2520or%2520imaginative%2520elements.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Negative%2520Prompting%2520for%2520Image%2520Correction%2520%2528NPC%2529%252C%2520an%2520automated%2520pipeline%2520that%2520improves%2520alignment%2520by%2520identifying%2520and%2520applying%2520negative%2520prompts%2520that%2520suppress%2520unintended%2520content.%2520We%2520begin%2520by%2520analyzing%2520cross-attention%2520patterns%2520to%2520explain%2520why%2520both%2520targeted%2520negatives-those%2520directly%2520tied%2520to%2520the%2520prompt%2527s%2520alignment%2520error-and%2520untargeted%2520negatives-tokens%2520unrelated%2520to%2520the%2520prompt%2520but%2520present%2520in%2520the%2520generated%2520image-can%2520enhance%2520alignment.%2520To%2520discover%2520useful%2520negatives%252C%2520NPC%2520generates%2520candidate%2520prompts%2520using%2520a%2520verifier-captioner-proposer%2520framework%2520and%2520ranks%2520them%2520with%2520a%2520salient%2520text-space%2520score%252C%2520enabling%2520effective%2520selection%2520without%2520requiring%2520additional%2520image%2520synthesis.%2520On%2520GenEval%252B%252B%2520and%2520Imagine-Bench%252C%2520NPC%2520outperforms%2520strong%2520baselines%252C%2520achieving%25200.571%2520vs.%25200.371%2520on%2520GenEval%252B%252B%2520and%2520the%2520best%2520overall%2520performance%2520on%2520Imagine-Bench.%2520By%2520guiding%2520what%2520not%2520to%2520generate%252C%2520NPC%2520provides%2520a%2520principled%252C%2520fully%2520automated%2520route%2520to%2520stronger%2520text-image%2520alignment%2520in%2520diffusion%2520models.%2520Code%2520is%2520released%2520at%2520https%253A//github.com/wiarae/NPC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guiding%20What%20Not%20to%20Generate%3A%20Automated%20Negative%20Prompting%20for%20Text-Image%20Alignment&entry.906535625=Sangha%20Park%20and%20Eunji%20Kim%20and%20Yeongtak%20Oh%20and%20Jooyoung%20Choi%20and%20Sungroh%20Yoon&entry.1292438233=Despite%20substantial%20progress%20in%20text-to-image%20generation%2C%20achieving%20precise%20text-image%20alignment%20remains%20challenging%2C%20particularly%20for%20prompts%20with%20rich%20compositional%20structure%20or%20imaginative%20elements.%20To%20address%20this%2C%20we%20introduce%20Negative%20Prompting%20for%20Image%20Correction%20%28NPC%29%2C%20an%20automated%20pipeline%20that%20improves%20alignment%20by%20identifying%20and%20applying%20negative%20prompts%20that%20suppress%20unintended%20content.%20We%20begin%20by%20analyzing%20cross-attention%20patterns%20to%20explain%20why%20both%20targeted%20negatives-those%20directly%20tied%20to%20the%20prompt%27s%20alignment%20error-and%20untargeted%20negatives-tokens%20unrelated%20to%20the%20prompt%20but%20present%20in%20the%20generated%20image-can%20enhance%20alignment.%20To%20discover%20useful%20negatives%2C%20NPC%20generates%20candidate%20prompts%20using%20a%20verifier-captioner-proposer%20framework%20and%20ranks%20them%20with%20a%20salient%20text-space%20score%2C%20enabling%20effective%20selection%20without%20requiring%20additional%20image%20synthesis.%20On%20GenEval%2B%2B%20and%20Imagine-Bench%2C%20NPC%20outperforms%20strong%20baselines%2C%20achieving%200.571%20vs.%200.371%20on%20GenEval%2B%2B%20and%20the%20best%20overall%20performance%20on%20Imagine-Bench.%20By%20guiding%20what%20not%20to%20generate%2C%20NPC%20provides%20a%20principled%2C%20fully%20automated%20route%20to%20stronger%20text-image%20alignment%20in%20diffusion%20models.%20Code%20is%20released%20at%20https%3A//github.com/wiarae/NPC.&entry.1838667208=http%3A//arxiv.org/abs/2512.07702v1&entry.124074799=Read"},
{"title": "Improving action classification with brain-inspired deep networks", "author": "Aidas Aglinskas and Stefano Anzellotti", "abstract": "Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.", "link": "http://arxiv.org/abs/2512.07729v1", "date": "2025-12-08", "relevancy": 2.1033, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5273}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.525}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20action%20classification%20with%20brain-inspired%20deep%20networks&body=Title%3A%20Improving%20action%20classification%20with%20brain-inspired%20deep%20networks%0AAuthor%3A%20Aidas%20Aglinskas%20and%20Stefano%20Anzellotti%0AAbstract%3A%20Action%20recognition%20is%20also%20key%20for%20applications%20ranging%20from%20robotics%20to%20healthcare%20monitoring.%20Action%20information%20can%20be%20extracted%20from%20the%20body%20pose%20and%20movements%2C%20as%20well%20as%20from%20the%20background%20scene.%20However%2C%20the%20extent%20to%20which%20deep%20neural%20networks%20%28DNNs%29%20make%20use%20of%20information%20about%20the%20body%20and%20information%20about%20the%20background%20remains%20unclear.%20Since%20these%20two%20sources%20of%20information%20may%20be%20correlated%20within%20a%20training%20dataset%2C%20DNNs%20might%20learn%20to%20rely%20predominantly%20on%20one%20of%20them%2C%20without%20taking%20full%20advantage%20of%20the%20other.%20Unlike%20DNNs%2C%20humans%20have%20domain-specific%20brain%20regions%20selective%20for%20perceiving%20bodies%2C%20and%20regions%20selective%20for%20perceiving%20scenes.%20The%20present%20work%20tests%20whether%20humans%20are%20thus%20more%20effective%20at%20extracting%20information%20from%20both%20body%20and%20background%2C%20and%20whether%20building%20brain-inspired%20deep%20network%20architectures%20with%20separate%20domain-specific%20streams%20for%20body%20and%20scene%20perception%20endows%20them%20with%20more%20human-like%20performance.%20We%20first%20demonstrate%20that%20DNNs%20trained%20using%20the%20HAA500%20dataset%20perform%20almost%20as%20accurately%20on%20versions%20of%20the%20stimuli%20that%20show%20both%20body%20and%20background%20and%20on%20versions%20of%20the%20stimuli%20from%20which%20the%20body%20was%20removed%2C%20but%20are%20at%20chance-level%20for%20versions%20of%20the%20stimuli%20from%20which%20the%20background%20was%20removed.%20Conversely%2C%20human%20participants%20%28N%3D28%29%20can%20recognize%20the%20same%20set%20of%20actions%20accurately%20with%20all%20three%20versions%20of%20the%20stimuli%2C%20and%20perform%20significantly%20better%20on%20stimuli%20that%20show%20only%20the%20body%20than%20on%20stimuli%20that%20show%20only%20the%20background.%20Finally%2C%20we%20implement%20and%20test%20a%20novel%20architecture%20patterned%20after%20domain%20specificity%20in%20the%20brain%20with%20separate%20streams%20to%20process%20body%20and%20background%20information.%20We%20show%20that%201%29%20this%20architecture%20improves%20action%20recognition%20performance%2C%20and%202%29%20its%20accuracy%20across%20different%20versions%20of%20the%20stimuli%20follows%20a%20pattern%20that%20matches%20more%20closely%20the%20pattern%20of%20accuracy%20observed%20in%20human%20participants.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07729v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520action%2520classification%2520with%2520brain-inspired%2520deep%2520networks%26entry.906535625%3DAidas%2520Aglinskas%2520and%2520Stefano%2520Anzellotti%26entry.1292438233%3DAction%2520recognition%2520is%2520also%2520key%2520for%2520applications%2520ranging%2520from%2520robotics%2520to%2520healthcare%2520monitoring.%2520Action%2520information%2520can%2520be%2520extracted%2520from%2520the%2520body%2520pose%2520and%2520movements%252C%2520as%2520well%2520as%2520from%2520the%2520background%2520scene.%2520However%252C%2520the%2520extent%2520to%2520which%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520make%2520use%2520of%2520information%2520about%2520the%2520body%2520and%2520information%2520about%2520the%2520background%2520remains%2520unclear.%2520Since%2520these%2520two%2520sources%2520of%2520information%2520may%2520be%2520correlated%2520within%2520a%2520training%2520dataset%252C%2520DNNs%2520might%2520learn%2520to%2520rely%2520predominantly%2520on%2520one%2520of%2520them%252C%2520without%2520taking%2520full%2520advantage%2520of%2520the%2520other.%2520Unlike%2520DNNs%252C%2520humans%2520have%2520domain-specific%2520brain%2520regions%2520selective%2520for%2520perceiving%2520bodies%252C%2520and%2520regions%2520selective%2520for%2520perceiving%2520scenes.%2520The%2520present%2520work%2520tests%2520whether%2520humans%2520are%2520thus%2520more%2520effective%2520at%2520extracting%2520information%2520from%2520both%2520body%2520and%2520background%252C%2520and%2520whether%2520building%2520brain-inspired%2520deep%2520network%2520architectures%2520with%2520separate%2520domain-specific%2520streams%2520for%2520body%2520and%2520scene%2520perception%2520endows%2520them%2520with%2520more%2520human-like%2520performance.%2520We%2520first%2520demonstrate%2520that%2520DNNs%2520trained%2520using%2520the%2520HAA500%2520dataset%2520perform%2520almost%2520as%2520accurately%2520on%2520versions%2520of%2520the%2520stimuli%2520that%2520show%2520both%2520body%2520and%2520background%2520and%2520on%2520versions%2520of%2520the%2520stimuli%2520from%2520which%2520the%2520body%2520was%2520removed%252C%2520but%2520are%2520at%2520chance-level%2520for%2520versions%2520of%2520the%2520stimuli%2520from%2520which%2520the%2520background%2520was%2520removed.%2520Conversely%252C%2520human%2520participants%2520%2528N%253D28%2529%2520can%2520recognize%2520the%2520same%2520set%2520of%2520actions%2520accurately%2520with%2520all%2520three%2520versions%2520of%2520the%2520stimuli%252C%2520and%2520perform%2520significantly%2520better%2520on%2520stimuli%2520that%2520show%2520only%2520the%2520body%2520than%2520on%2520stimuli%2520that%2520show%2520only%2520the%2520background.%2520Finally%252C%2520we%2520implement%2520and%2520test%2520a%2520novel%2520architecture%2520patterned%2520after%2520domain%2520specificity%2520in%2520the%2520brain%2520with%2520separate%2520streams%2520to%2520process%2520body%2520and%2520background%2520information.%2520We%2520show%2520that%25201%2529%2520this%2520architecture%2520improves%2520action%2520recognition%2520performance%252C%2520and%25202%2529%2520its%2520accuracy%2520across%2520different%2520versions%2520of%2520the%2520stimuli%2520follows%2520a%2520pattern%2520that%2520matches%2520more%2520closely%2520the%2520pattern%2520of%2520accuracy%2520observed%2520in%2520human%2520participants.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07729v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20action%20classification%20with%20brain-inspired%20deep%20networks&entry.906535625=Aidas%20Aglinskas%20and%20Stefano%20Anzellotti&entry.1292438233=Action%20recognition%20is%20also%20key%20for%20applications%20ranging%20from%20robotics%20to%20healthcare%20monitoring.%20Action%20information%20can%20be%20extracted%20from%20the%20body%20pose%20and%20movements%2C%20as%20well%20as%20from%20the%20background%20scene.%20However%2C%20the%20extent%20to%20which%20deep%20neural%20networks%20%28DNNs%29%20make%20use%20of%20information%20about%20the%20body%20and%20information%20about%20the%20background%20remains%20unclear.%20Since%20these%20two%20sources%20of%20information%20may%20be%20correlated%20within%20a%20training%20dataset%2C%20DNNs%20might%20learn%20to%20rely%20predominantly%20on%20one%20of%20them%2C%20without%20taking%20full%20advantage%20of%20the%20other.%20Unlike%20DNNs%2C%20humans%20have%20domain-specific%20brain%20regions%20selective%20for%20perceiving%20bodies%2C%20and%20regions%20selective%20for%20perceiving%20scenes.%20The%20present%20work%20tests%20whether%20humans%20are%20thus%20more%20effective%20at%20extracting%20information%20from%20both%20body%20and%20background%2C%20and%20whether%20building%20brain-inspired%20deep%20network%20architectures%20with%20separate%20domain-specific%20streams%20for%20body%20and%20scene%20perception%20endows%20them%20with%20more%20human-like%20performance.%20We%20first%20demonstrate%20that%20DNNs%20trained%20using%20the%20HAA500%20dataset%20perform%20almost%20as%20accurately%20on%20versions%20of%20the%20stimuli%20that%20show%20both%20body%20and%20background%20and%20on%20versions%20of%20the%20stimuli%20from%20which%20the%20body%20was%20removed%2C%20but%20are%20at%20chance-level%20for%20versions%20of%20the%20stimuli%20from%20which%20the%20background%20was%20removed.%20Conversely%2C%20human%20participants%20%28N%3D28%29%20can%20recognize%20the%20same%20set%20of%20actions%20accurately%20with%20all%20three%20versions%20of%20the%20stimuli%2C%20and%20perform%20significantly%20better%20on%20stimuli%20that%20show%20only%20the%20body%20than%20on%20stimuli%20that%20show%20only%20the%20background.%20Finally%2C%20we%20implement%20and%20test%20a%20novel%20architecture%20patterned%20after%20domain%20specificity%20in%20the%20brain%20with%20separate%20streams%20to%20process%20body%20and%20background%20information.%20We%20show%20that%201%29%20this%20architecture%20improves%20action%20recognition%20performance%2C%20and%202%29%20its%20accuracy%20across%20different%20versions%20of%20the%20stimuli%20follows%20a%20pattern%20that%20matches%20more%20closely%20the%20pattern%20of%20accuracy%20observed%20in%20human%20participants.&entry.1838667208=http%3A//arxiv.org/abs/2512.07729v1&entry.124074799=Read"},
{"title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization", "author": "Zhijian Zhuo and Yutao Zeng and Ya Wang and Sijun Zhang and Jian Yang and Xiaoqing Li and Xun Zhou and Jinwen Ma", "abstract": "Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, many challenges remain in training deep transformer networks, especially regarding the position of the layer normalization. While Pre-Norm structures facilitate more stable training owing to their stronger identity path, they often lead to suboptimal performance compared to Post-Norm. In this paper, we propose $\\textbf{HybridNorm}$, a simple yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. We provide both theoretical insights and empirical evidence to demonstrate that HybridNorm improves the gradient flow and the model robustness. Extensive experiments on large-scale transformer models, including both dense and sparse variants, show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches across multiple benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. Code is available at https://github.com/BryceZhuo/HybridNorm.", "link": "http://arxiv.org/abs/2503.04598v4", "date": "2025-12-08", "relevancy": 2.0997, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5631}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5484}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HybridNorm%3A%20Towards%20Stable%20and%20Efficient%20Transformer%20Training%20via%20Hybrid%20Normalization&body=Title%3A%20HybridNorm%3A%20Towards%20Stable%20and%20Efficient%20Transformer%20Training%20via%20Hybrid%20Normalization%0AAuthor%3A%20Zhijian%20Zhuo%20and%20Yutao%20Zeng%20and%20Ya%20Wang%20and%20Sijun%20Zhang%20and%20Jian%20Yang%20and%20Xiaoqing%20Li%20and%20Xun%20Zhou%20and%20Jinwen%20Ma%0AAbstract%3A%20Transformers%20have%20become%20the%20de%20facto%20architecture%20for%20a%20wide%20range%20of%20machine%20learning%20tasks%2C%20particularly%20in%20large%20language%20models%20%28LLMs%29.%20Despite%20their%20remarkable%20performance%2C%20many%20challenges%20remain%20in%20training%20deep%20transformer%20networks%2C%20especially%20regarding%20the%20position%20of%20the%20layer%20normalization.%20While%20Pre-Norm%20structures%20facilitate%20more%20stable%20training%20owing%20to%20their%20stronger%20identity%20path%2C%20they%20often%20lead%20to%20suboptimal%20performance%20compared%20to%20Post-Norm.%20In%20this%20paper%2C%20we%20propose%20%24%5Ctextbf%7BHybridNorm%7D%24%2C%20a%20simple%20yet%20effective%20hybrid%20normalization%20strategy%20that%20integrates%20the%20advantages%20of%20both%20Pre-Norm%20and%20Post-Norm.%20Specifically%2C%20HybridNorm%20employs%20QKV%20normalization%20within%20the%20attention%20mechanism%20and%20Post-Norm%20in%20the%20feed-forward%20network%20%28FFN%29%20of%20each%20transformer%20block.%20We%20provide%20both%20theoretical%20insights%20and%20empirical%20evidence%20to%20demonstrate%20that%20HybridNorm%20improves%20the%20gradient%20flow%20and%20the%20model%20robustness.%20Extensive%20experiments%20on%20large-scale%20transformer%20models%2C%20including%20both%20dense%20and%20sparse%20variants%2C%20show%20that%20HybridNorm%20consistently%20outperforms%20both%20Pre-Norm%20and%20Post-Norm%20approaches%20across%20multiple%20benchmarks.%20These%20findings%20highlight%20the%20potential%20of%20HybridNorm%20as%20a%20more%20stable%20and%20effective%20technique%20for%20improving%20the%20training%20and%20performance%20of%20deep%20transformer%20models.%20Code%20is%20available%20at%20https%3A//github.com/BryceZhuo/HybridNorm.%0ALink%3A%20http%3A//arxiv.org/abs/2503.04598v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybridNorm%253A%2520Towards%2520Stable%2520and%2520Efficient%2520Transformer%2520Training%2520via%2520Hybrid%2520Normalization%26entry.906535625%3DZhijian%2520Zhuo%2520and%2520Yutao%2520Zeng%2520and%2520Ya%2520Wang%2520and%2520Sijun%2520Zhang%2520and%2520Jian%2520Yang%2520and%2520Xiaoqing%2520Li%2520and%2520Xun%2520Zhou%2520and%2520Jinwen%2520Ma%26entry.1292438233%3DTransformers%2520have%2520become%2520the%2520de%2520facto%2520architecture%2520for%2520a%2520wide%2520range%2520of%2520machine%2520learning%2520tasks%252C%2520particularly%2520in%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Despite%2520their%2520remarkable%2520performance%252C%2520many%2520challenges%2520remain%2520in%2520training%2520deep%2520transformer%2520networks%252C%2520especially%2520regarding%2520the%2520position%2520of%2520the%2520layer%2520normalization.%2520While%2520Pre-Norm%2520structures%2520facilitate%2520more%2520stable%2520training%2520owing%2520to%2520their%2520stronger%2520identity%2520path%252C%2520they%2520often%2520lead%2520to%2520suboptimal%2520performance%2520compared%2520to%2520Post-Norm.%2520In%2520this%2520paper%252C%2520we%2520propose%2520%2524%255Ctextbf%257BHybridNorm%257D%2524%252C%2520a%2520simple%2520yet%2520effective%2520hybrid%2520normalization%2520strategy%2520that%2520integrates%2520the%2520advantages%2520of%2520both%2520Pre-Norm%2520and%2520Post-Norm.%2520Specifically%252C%2520HybridNorm%2520employs%2520QKV%2520normalization%2520within%2520the%2520attention%2520mechanism%2520and%2520Post-Norm%2520in%2520the%2520feed-forward%2520network%2520%2528FFN%2529%2520of%2520each%2520transformer%2520block.%2520We%2520provide%2520both%2520theoretical%2520insights%2520and%2520empirical%2520evidence%2520to%2520demonstrate%2520that%2520HybridNorm%2520improves%2520the%2520gradient%2520flow%2520and%2520the%2520model%2520robustness.%2520Extensive%2520experiments%2520on%2520large-scale%2520transformer%2520models%252C%2520including%2520both%2520dense%2520and%2520sparse%2520variants%252C%2520show%2520that%2520HybridNorm%2520consistently%2520outperforms%2520both%2520Pre-Norm%2520and%2520Post-Norm%2520approaches%2520across%2520multiple%2520benchmarks.%2520These%2520findings%2520highlight%2520the%2520potential%2520of%2520HybridNorm%2520as%2520a%2520more%2520stable%2520and%2520effective%2520technique%2520for%2520improving%2520the%2520training%2520and%2520performance%2520of%2520deep%2520transformer%2520models.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/BryceZhuo/HybridNorm.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04598v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HybridNorm%3A%20Towards%20Stable%20and%20Efficient%20Transformer%20Training%20via%20Hybrid%20Normalization&entry.906535625=Zhijian%20Zhuo%20and%20Yutao%20Zeng%20and%20Ya%20Wang%20and%20Sijun%20Zhang%20and%20Jian%20Yang%20and%20Xiaoqing%20Li%20and%20Xun%20Zhou%20and%20Jinwen%20Ma&entry.1292438233=Transformers%20have%20become%20the%20de%20facto%20architecture%20for%20a%20wide%20range%20of%20machine%20learning%20tasks%2C%20particularly%20in%20large%20language%20models%20%28LLMs%29.%20Despite%20their%20remarkable%20performance%2C%20many%20challenges%20remain%20in%20training%20deep%20transformer%20networks%2C%20especially%20regarding%20the%20position%20of%20the%20layer%20normalization.%20While%20Pre-Norm%20structures%20facilitate%20more%20stable%20training%20owing%20to%20their%20stronger%20identity%20path%2C%20they%20often%20lead%20to%20suboptimal%20performance%20compared%20to%20Post-Norm.%20In%20this%20paper%2C%20we%20propose%20%24%5Ctextbf%7BHybridNorm%7D%24%2C%20a%20simple%20yet%20effective%20hybrid%20normalization%20strategy%20that%20integrates%20the%20advantages%20of%20both%20Pre-Norm%20and%20Post-Norm.%20Specifically%2C%20HybridNorm%20employs%20QKV%20normalization%20within%20the%20attention%20mechanism%20and%20Post-Norm%20in%20the%20feed-forward%20network%20%28FFN%29%20of%20each%20transformer%20block.%20We%20provide%20both%20theoretical%20insights%20and%20empirical%20evidence%20to%20demonstrate%20that%20HybridNorm%20improves%20the%20gradient%20flow%20and%20the%20model%20robustness.%20Extensive%20experiments%20on%20large-scale%20transformer%20models%2C%20including%20both%20dense%20and%20sparse%20variants%2C%20show%20that%20HybridNorm%20consistently%20outperforms%20both%20Pre-Norm%20and%20Post-Norm%20approaches%20across%20multiple%20benchmarks.%20These%20findings%20highlight%20the%20potential%20of%20HybridNorm%20as%20a%20more%20stable%20and%20effective%20technique%20for%20improving%20the%20training%20and%20performance%20of%20deep%20transformer%20models.%20Code%20is%20available%20at%20https%3A//github.com/BryceZhuo/HybridNorm.&entry.1838667208=http%3A//arxiv.org/abs/2503.04598v4&entry.124074799=Read"},
{"title": "LAMP: Language-Assisted Motion Planning for Controllable Video Generation", "author": "Muhammed Burak Kizil and Enes Sanli and Niloy J. Mitra and Erkut Erdem and Aykut Erdem and Duygu Ceylan", "abstract": "Video generation has achieved remarkable progress in visual fidelity and controllability, enabling conditioning on text, layout, or motion. Among these, motion control - specifying object dynamics and camera trajectories - is essential for composing complex, cinematic scenes, yet existing interfaces remain limited. We introduce LAMP that leverages large language models (LLMs) as motion planners to translate natural language descriptions into explicit 3D trajectories for dynamic objects and (relatively defined) cameras. LAMP defines a motion domain-specific language (DSL), inspired by cinematography conventions. By harnessing program synthesis capabilities of LLMs, LAMP generates structured motion programs from natural language, which are deterministically mapped to 3D trajectories. We construct a large-scale procedural dataset pairing natural text descriptions with corresponding motion programs and 3D trajectories. Experiments demonstrate LAMP's improved performance in motion controllability and alignment with user intent compared to state-of-the-art alternatives establishing the first framework for generating both object and camera motions directly from natural language specifications. Code, models and data are available on our project page.", "link": "http://arxiv.org/abs/2512.03619v2", "date": "2025-12-08", "relevancy": 1.7966, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6059}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5913}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAMP%3A%20Language-Assisted%20Motion%20Planning%20for%20Controllable%20Video%20Generation&body=Title%3A%20LAMP%3A%20Language-Assisted%20Motion%20Planning%20for%20Controllable%20Video%20Generation%0AAuthor%3A%20Muhammed%20Burak%20Kizil%20and%20Enes%20Sanli%20and%20Niloy%20J.%20Mitra%20and%20Erkut%20Erdem%20and%20Aykut%20Erdem%20and%20Duygu%20Ceylan%0AAbstract%3A%20Video%20generation%20has%20achieved%20remarkable%20progress%20in%20visual%20fidelity%20and%20controllability%2C%20enabling%20conditioning%20on%20text%2C%20layout%2C%20or%20motion.%20Among%20these%2C%20motion%20control%20-%20specifying%20object%20dynamics%20and%20camera%20trajectories%20-%20is%20essential%20for%20composing%20complex%2C%20cinematic%20scenes%2C%20yet%20existing%20interfaces%20remain%20limited.%20We%20introduce%20LAMP%20that%20leverages%20large%20language%20models%20%28LLMs%29%20as%20motion%20planners%20to%20translate%20natural%20language%20descriptions%20into%20explicit%203D%20trajectories%20for%20dynamic%20objects%20and%20%28relatively%20defined%29%20cameras.%20LAMP%20defines%20a%20motion%20domain-specific%20language%20%28DSL%29%2C%20inspired%20by%20cinematography%20conventions.%20By%20harnessing%20program%20synthesis%20capabilities%20of%20LLMs%2C%20LAMP%20generates%20structured%20motion%20programs%20from%20natural%20language%2C%20which%20are%20deterministically%20mapped%20to%203D%20trajectories.%20We%20construct%20a%20large-scale%20procedural%20dataset%20pairing%20natural%20text%20descriptions%20with%20corresponding%20motion%20programs%20and%203D%20trajectories.%20Experiments%20demonstrate%20LAMP%27s%20improved%20performance%20in%20motion%20controllability%20and%20alignment%20with%20user%20intent%20compared%20to%20state-of-the-art%20alternatives%20establishing%20the%20first%20framework%20for%20generating%20both%20object%20and%20camera%20motions%20directly%20from%20natural%20language%20specifications.%20Code%2C%20models%20and%20data%20are%20available%20on%20our%20project%20page.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03619v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAMP%253A%2520Language-Assisted%2520Motion%2520Planning%2520for%2520Controllable%2520Video%2520Generation%26entry.906535625%3DMuhammed%2520Burak%2520Kizil%2520and%2520Enes%2520Sanli%2520and%2520Niloy%2520J.%2520Mitra%2520and%2520Erkut%2520Erdem%2520and%2520Aykut%2520Erdem%2520and%2520Duygu%2520Ceylan%26entry.1292438233%3DVideo%2520generation%2520has%2520achieved%2520remarkable%2520progress%2520in%2520visual%2520fidelity%2520and%2520controllability%252C%2520enabling%2520conditioning%2520on%2520text%252C%2520layout%252C%2520or%2520motion.%2520Among%2520these%252C%2520motion%2520control%2520-%2520specifying%2520object%2520dynamics%2520and%2520camera%2520trajectories%2520-%2520is%2520essential%2520for%2520composing%2520complex%252C%2520cinematic%2520scenes%252C%2520yet%2520existing%2520interfaces%2520remain%2520limited.%2520We%2520introduce%2520LAMP%2520that%2520leverages%2520large%2520language%2520models%2520%2528LLMs%2529%2520as%2520motion%2520planners%2520to%2520translate%2520natural%2520language%2520descriptions%2520into%2520explicit%25203D%2520trajectories%2520for%2520dynamic%2520objects%2520and%2520%2528relatively%2520defined%2529%2520cameras.%2520LAMP%2520defines%2520a%2520motion%2520domain-specific%2520language%2520%2528DSL%2529%252C%2520inspired%2520by%2520cinematography%2520conventions.%2520By%2520harnessing%2520program%2520synthesis%2520capabilities%2520of%2520LLMs%252C%2520LAMP%2520generates%2520structured%2520motion%2520programs%2520from%2520natural%2520language%252C%2520which%2520are%2520deterministically%2520mapped%2520to%25203D%2520trajectories.%2520We%2520construct%2520a%2520large-scale%2520procedural%2520dataset%2520pairing%2520natural%2520text%2520descriptions%2520with%2520corresponding%2520motion%2520programs%2520and%25203D%2520trajectories.%2520Experiments%2520demonstrate%2520LAMP%2527s%2520improved%2520performance%2520in%2520motion%2520controllability%2520and%2520alignment%2520with%2520user%2520intent%2520compared%2520to%2520state-of-the-art%2520alternatives%2520establishing%2520the%2520first%2520framework%2520for%2520generating%2520both%2520object%2520and%2520camera%2520motions%2520directly%2520from%2520natural%2520language%2520specifications.%2520Code%252C%2520models%2520and%2520data%2520are%2520available%2520on%2520our%2520project%2520page.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03619v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAMP%3A%20Language-Assisted%20Motion%20Planning%20for%20Controllable%20Video%20Generation&entry.906535625=Muhammed%20Burak%20Kizil%20and%20Enes%20Sanli%20and%20Niloy%20J.%20Mitra%20and%20Erkut%20Erdem%20and%20Aykut%20Erdem%20and%20Duygu%20Ceylan&entry.1292438233=Video%20generation%20has%20achieved%20remarkable%20progress%20in%20visual%20fidelity%20and%20controllability%2C%20enabling%20conditioning%20on%20text%2C%20layout%2C%20or%20motion.%20Among%20these%2C%20motion%20control%20-%20specifying%20object%20dynamics%20and%20camera%20trajectories%20-%20is%20essential%20for%20composing%20complex%2C%20cinematic%20scenes%2C%20yet%20existing%20interfaces%20remain%20limited.%20We%20introduce%20LAMP%20that%20leverages%20large%20language%20models%20%28LLMs%29%20as%20motion%20planners%20to%20translate%20natural%20language%20descriptions%20into%20explicit%203D%20trajectories%20for%20dynamic%20objects%20and%20%28relatively%20defined%29%20cameras.%20LAMP%20defines%20a%20motion%20domain-specific%20language%20%28DSL%29%2C%20inspired%20by%20cinematography%20conventions.%20By%20harnessing%20program%20synthesis%20capabilities%20of%20LLMs%2C%20LAMP%20generates%20structured%20motion%20programs%20from%20natural%20language%2C%20which%20are%20deterministically%20mapped%20to%203D%20trajectories.%20We%20construct%20a%20large-scale%20procedural%20dataset%20pairing%20natural%20text%20descriptions%20with%20corresponding%20motion%20programs%20and%203D%20trajectories.%20Experiments%20demonstrate%20LAMP%27s%20improved%20performance%20in%20motion%20controllability%20and%20alignment%20with%20user%20intent%20compared%20to%20state-of-the-art%20alternatives%20establishing%20the%20first%20framework%20for%20generating%20both%20object%20and%20camera%20motions%20directly%20from%20natural%20language%20specifications.%20Code%2C%20models%20and%20data%20are%20available%20on%20our%20project%20page.&entry.1838667208=http%3A//arxiv.org/abs/2512.03619v2&entry.124074799=Read"},
{"title": "Microseismic event classification with a lightweight Fourier Neural Operator model", "author": "Ayrat Abdullin and Umair bin Waheed and Leo Eisner and Abdullatif Al-Shuhail", "abstract": "Real-time monitoring of induced seismicity is crucial for mitigating operational hazards, relying on the rapid and accurate classification of microseismic events from continuous data streams. However, while many deep learning models excel at this task, their high computational requirements often limit their practical application in real-time monitoring systems. To address this limitation, a lightweight model based on the Fourier Neural Operator (FNO) is proposed for microseismic event classification, leveraging its inherent resolution-invariance and computational efficiency for waveform processing. In the STanford EArthquake Dataset (STEAD), a global and large-scale database of seismic waveforms, the FNO-based model demonstrates high effectiveness for trigger classification, with an F1 score of 95% even in the scenario of data sparsity in training. The new FNO model greatly decreases the computer power needed relative to current deep learning models without sacrificing the classification success rate measured by the F1 score. A test on a real microseismic dataset shows a classification success rate with an F1 score of 98%, outperforming many traditional deep-learning techniques. A combination of high success rate and low computational power indicates that the FNO model can serve as a methodology of choice for real-time monitoring of microseismicity for induced seismicity. The method saves computational resources and facilitates both post-processing and real-time seismic processing suitable for the implementation of traffic light systems to prevent undesired induced seismicity.", "link": "http://arxiv.org/abs/2512.07425v1", "date": "2025-12-08", "relevancy": 1.8047, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4707}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4542}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Microseismic%20event%20classification%20with%20a%20lightweight%20Fourier%20Neural%20Operator%20model&body=Title%3A%20Microseismic%20event%20classification%20with%20a%20lightweight%20Fourier%20Neural%20Operator%20model%0AAuthor%3A%20Ayrat%20Abdullin%20and%20Umair%20bin%20Waheed%20and%20Leo%20Eisner%20and%20Abdullatif%20Al-Shuhail%0AAbstract%3A%20Real-time%20monitoring%20of%20induced%20seismicity%20is%20crucial%20for%20mitigating%20operational%20hazards%2C%20relying%20on%20the%20rapid%20and%20accurate%20classification%20of%20microseismic%20events%20from%20continuous%20data%20streams.%20However%2C%20while%20many%20deep%20learning%20models%20excel%20at%20this%20task%2C%20their%20high%20computational%20requirements%20often%20limit%20their%20practical%20application%20in%20real-time%20monitoring%20systems.%20To%20address%20this%20limitation%2C%20a%20lightweight%20model%20based%20on%20the%20Fourier%20Neural%20Operator%20%28FNO%29%20is%20proposed%20for%20microseismic%20event%20classification%2C%20leveraging%20its%20inherent%20resolution-invariance%20and%20computational%20efficiency%20for%20waveform%20processing.%20In%20the%20STanford%20EArthquake%20Dataset%20%28STEAD%29%2C%20a%20global%20and%20large-scale%20database%20of%20seismic%20waveforms%2C%20the%20FNO-based%20model%20demonstrates%20high%20effectiveness%20for%20trigger%20classification%2C%20with%20an%20F1%20score%20of%2095%25%20even%20in%20the%20scenario%20of%20data%20sparsity%20in%20training.%20The%20new%20FNO%20model%20greatly%20decreases%20the%20computer%20power%20needed%20relative%20to%20current%20deep%20learning%20models%20without%20sacrificing%20the%20classification%20success%20rate%20measured%20by%20the%20F1%20score.%20A%20test%20on%20a%20real%20microseismic%20dataset%20shows%20a%20classification%20success%20rate%20with%20an%20F1%20score%20of%2098%25%2C%20outperforming%20many%20traditional%20deep-learning%20techniques.%20A%20combination%20of%20high%20success%20rate%20and%20low%20computational%20power%20indicates%20that%20the%20FNO%20model%20can%20serve%20as%20a%20methodology%20of%20choice%20for%20real-time%20monitoring%20of%20microseismicity%20for%20induced%20seismicity.%20The%20method%20saves%20computational%20resources%20and%20facilitates%20both%20post-processing%20and%20real-time%20seismic%20processing%20suitable%20for%20the%20implementation%20of%20traffic%20light%20systems%20to%20prevent%20undesired%20induced%20seismicity.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMicroseismic%2520event%2520classification%2520with%2520a%2520lightweight%2520Fourier%2520Neural%2520Operator%2520model%26entry.906535625%3DAyrat%2520Abdullin%2520and%2520Umair%2520bin%2520Waheed%2520and%2520Leo%2520Eisner%2520and%2520Abdullatif%2520Al-Shuhail%26entry.1292438233%3DReal-time%2520monitoring%2520of%2520induced%2520seismicity%2520is%2520crucial%2520for%2520mitigating%2520operational%2520hazards%252C%2520relying%2520on%2520the%2520rapid%2520and%2520accurate%2520classification%2520of%2520microseismic%2520events%2520from%2520continuous%2520data%2520streams.%2520However%252C%2520while%2520many%2520deep%2520learning%2520models%2520excel%2520at%2520this%2520task%252C%2520their%2520high%2520computational%2520requirements%2520often%2520limit%2520their%2520practical%2520application%2520in%2520real-time%2520monitoring%2520systems.%2520To%2520address%2520this%2520limitation%252C%2520a%2520lightweight%2520model%2520based%2520on%2520the%2520Fourier%2520Neural%2520Operator%2520%2528FNO%2529%2520is%2520proposed%2520for%2520microseismic%2520event%2520classification%252C%2520leveraging%2520its%2520inherent%2520resolution-invariance%2520and%2520computational%2520efficiency%2520for%2520waveform%2520processing.%2520In%2520the%2520STanford%2520EArthquake%2520Dataset%2520%2528STEAD%2529%252C%2520a%2520global%2520and%2520large-scale%2520database%2520of%2520seismic%2520waveforms%252C%2520the%2520FNO-based%2520model%2520demonstrates%2520high%2520effectiveness%2520for%2520trigger%2520classification%252C%2520with%2520an%2520F1%2520score%2520of%252095%2525%2520even%2520in%2520the%2520scenario%2520of%2520data%2520sparsity%2520in%2520training.%2520The%2520new%2520FNO%2520model%2520greatly%2520decreases%2520the%2520computer%2520power%2520needed%2520relative%2520to%2520current%2520deep%2520learning%2520models%2520without%2520sacrificing%2520the%2520classification%2520success%2520rate%2520measured%2520by%2520the%2520F1%2520score.%2520A%2520test%2520on%2520a%2520real%2520microseismic%2520dataset%2520shows%2520a%2520classification%2520success%2520rate%2520with%2520an%2520F1%2520score%2520of%252098%2525%252C%2520outperforming%2520many%2520traditional%2520deep-learning%2520techniques.%2520A%2520combination%2520of%2520high%2520success%2520rate%2520and%2520low%2520computational%2520power%2520indicates%2520that%2520the%2520FNO%2520model%2520can%2520serve%2520as%2520a%2520methodology%2520of%2520choice%2520for%2520real-time%2520monitoring%2520of%2520microseismicity%2520for%2520induced%2520seismicity.%2520The%2520method%2520saves%2520computational%2520resources%2520and%2520facilitates%2520both%2520post-processing%2520and%2520real-time%2520seismic%2520processing%2520suitable%2520for%2520the%2520implementation%2520of%2520traffic%2520light%2520systems%2520to%2520prevent%2520undesired%2520induced%2520seismicity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Microseismic%20event%20classification%20with%20a%20lightweight%20Fourier%20Neural%20Operator%20model&entry.906535625=Ayrat%20Abdullin%20and%20Umair%20bin%20Waheed%20and%20Leo%20Eisner%20and%20Abdullatif%20Al-Shuhail&entry.1292438233=Real-time%20monitoring%20of%20induced%20seismicity%20is%20crucial%20for%20mitigating%20operational%20hazards%2C%20relying%20on%20the%20rapid%20and%20accurate%20classification%20of%20microseismic%20events%20from%20continuous%20data%20streams.%20However%2C%20while%20many%20deep%20learning%20models%20excel%20at%20this%20task%2C%20their%20high%20computational%20requirements%20often%20limit%20their%20practical%20application%20in%20real-time%20monitoring%20systems.%20To%20address%20this%20limitation%2C%20a%20lightweight%20model%20based%20on%20the%20Fourier%20Neural%20Operator%20%28FNO%29%20is%20proposed%20for%20microseismic%20event%20classification%2C%20leveraging%20its%20inherent%20resolution-invariance%20and%20computational%20efficiency%20for%20waveform%20processing.%20In%20the%20STanford%20EArthquake%20Dataset%20%28STEAD%29%2C%20a%20global%20and%20large-scale%20database%20of%20seismic%20waveforms%2C%20the%20FNO-based%20model%20demonstrates%20high%20effectiveness%20for%20trigger%20classification%2C%20with%20an%20F1%20score%20of%2095%25%20even%20in%20the%20scenario%20of%20data%20sparsity%20in%20training.%20The%20new%20FNO%20model%20greatly%20decreases%20the%20computer%20power%20needed%20relative%20to%20current%20deep%20learning%20models%20without%20sacrificing%20the%20classification%20success%20rate%20measured%20by%20the%20F1%20score.%20A%20test%20on%20a%20real%20microseismic%20dataset%20shows%20a%20classification%20success%20rate%20with%20an%20F1%20score%20of%2098%25%2C%20outperforming%20many%20traditional%20deep-learning%20techniques.%20A%20combination%20of%20high%20success%20rate%20and%20low%20computational%20power%20indicates%20that%20the%20FNO%20model%20can%20serve%20as%20a%20methodology%20of%20choice%20for%20real-time%20monitoring%20of%20microseismicity%20for%20induced%20seismicity.%20The%20method%20saves%20computational%20resources%20and%20facilitates%20both%20post-processing%20and%20real-time%20seismic%20processing%20suitable%20for%20the%20implementation%20of%20traffic%20light%20systems%20to%20prevent%20undesired%20induced%20seismicity.&entry.1838667208=http%3A//arxiv.org/abs/2512.07425v1&entry.124074799=Read"},
{"title": "Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement", "author": "Yongsheng Lian", "abstract": "This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.\n  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.", "link": "http://arxiv.org/abs/2512.07611v1", "date": "2025-12-08", "relevancy": 1.9788, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Analysis%20and%20Parametric%20Tuning%20of%20PPO%2C%20GRPO%2C%20and%20DAPO%20for%20LLM%20Reasoning%20Enhancement&body=Title%3A%20Comparative%20Analysis%20and%20Parametric%20Tuning%20of%20PPO%2C%20GRPO%2C%20and%20DAPO%20for%20LLM%20Reasoning%20Enhancement%0AAuthor%3A%20Yongsheng%20Lian%0AAbstract%3A%20This%20study%20presents%20a%20systematic%20comparison%20of%20three%20Reinforcement%20Learning%20%28RL%29%20algorithms%20%28PPO%2C%20GRPO%2C%20and%20DAPO%29%20for%20improving%20complex%20reasoning%20in%20large%20language%20models%20%28LLMs%29.%20Our%20main%20contribution%20is%20a%20controlled%20transfer-learning%20evaluation%3A%20models%20are%20first%20fine-tuned%20on%20the%20specialized%20Countdown%20Game%20and%20then%20assessed%20on%20a%20suite%20of%20general-purpose%20reasoning%20benchmarks.%20Across%20all%20tasks%2C%20RL-trained%20models%20outperform%20their%20corresponding%20base%20models%2C%20although%20the%20degree%20of%20improvement%20differs%20by%20benchmark.%0A%20%20Our%20parametric%20analysis%20offers%20practical%20guidance%20for%20RL-based%20LLM%20training.%20Increasing%20the%20group%20size%20in%20GRPO%20and%20DAPO%20leads%20to%20more%20stable%20training%20dynamics%20and%20higher%20accuracy%2C%20while%20the%20impact%20of%20the%20KL-penalty%20coefficient%20is%20non-monotonic.%20Additionally%2C%20we%20find%20that%20the%20Dynamic%20Sampling%20%28DS%29%20component%20in%20DAPO%20does%20not%20improve%20performance%3B%20in%20fact%2C%20the%20best%20overall%20results%20are%20achieved%20with%20DAPO%20when%20DS%20is%20disabled.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Analysis%2520and%2520Parametric%2520Tuning%2520of%2520PPO%252C%2520GRPO%252C%2520and%2520DAPO%2520for%2520LLM%2520Reasoning%2520Enhancement%26entry.906535625%3DYongsheng%2520Lian%26entry.1292438233%3DThis%2520study%2520presents%2520a%2520systematic%2520comparison%2520of%2520three%2520Reinforcement%2520Learning%2520%2528RL%2529%2520algorithms%2520%2528PPO%252C%2520GRPO%252C%2520and%2520DAPO%2529%2520for%2520improving%2520complex%2520reasoning%2520in%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Our%2520main%2520contribution%2520is%2520a%2520controlled%2520transfer-learning%2520evaluation%253A%2520models%2520are%2520first%2520fine-tuned%2520on%2520the%2520specialized%2520Countdown%2520Game%2520and%2520then%2520assessed%2520on%2520a%2520suite%2520of%2520general-purpose%2520reasoning%2520benchmarks.%2520Across%2520all%2520tasks%252C%2520RL-trained%2520models%2520outperform%2520their%2520corresponding%2520base%2520models%252C%2520although%2520the%2520degree%2520of%2520improvement%2520differs%2520by%2520benchmark.%250A%2520%2520Our%2520parametric%2520analysis%2520offers%2520practical%2520guidance%2520for%2520RL-based%2520LLM%2520training.%2520Increasing%2520the%2520group%2520size%2520in%2520GRPO%2520and%2520DAPO%2520leads%2520to%2520more%2520stable%2520training%2520dynamics%2520and%2520higher%2520accuracy%252C%2520while%2520the%2520impact%2520of%2520the%2520KL-penalty%2520coefficient%2520is%2520non-monotonic.%2520Additionally%252C%2520we%2520find%2520that%2520the%2520Dynamic%2520Sampling%2520%2528DS%2529%2520component%2520in%2520DAPO%2520does%2520not%2520improve%2520performance%253B%2520in%2520fact%252C%2520the%2520best%2520overall%2520results%2520are%2520achieved%2520with%2520DAPO%2520when%2520DS%2520is%2520disabled.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Analysis%20and%20Parametric%20Tuning%20of%20PPO%2C%20GRPO%2C%20and%20DAPO%20for%20LLM%20Reasoning%20Enhancement&entry.906535625=Yongsheng%20Lian&entry.1292438233=This%20study%20presents%20a%20systematic%20comparison%20of%20three%20Reinforcement%20Learning%20%28RL%29%20algorithms%20%28PPO%2C%20GRPO%2C%20and%20DAPO%29%20for%20improving%20complex%20reasoning%20in%20large%20language%20models%20%28LLMs%29.%20Our%20main%20contribution%20is%20a%20controlled%20transfer-learning%20evaluation%3A%20models%20are%20first%20fine-tuned%20on%20the%20specialized%20Countdown%20Game%20and%20then%20assessed%20on%20a%20suite%20of%20general-purpose%20reasoning%20benchmarks.%20Across%20all%20tasks%2C%20RL-trained%20models%20outperform%20their%20corresponding%20base%20models%2C%20although%20the%20degree%20of%20improvement%20differs%20by%20benchmark.%0A%20%20Our%20parametric%20analysis%20offers%20practical%20guidance%20for%20RL-based%20LLM%20training.%20Increasing%20the%20group%20size%20in%20GRPO%20and%20DAPO%20leads%20to%20more%20stable%20training%20dynamics%20and%20higher%20accuracy%2C%20while%20the%20impact%20of%20the%20KL-penalty%20coefficient%20is%20non-monotonic.%20Additionally%2C%20we%20find%20that%20the%20Dynamic%20Sampling%20%28DS%29%20component%20in%20DAPO%20does%20not%20improve%20performance%3B%20in%20fact%2C%20the%20best%20overall%20results%20are%20achieved%20with%20DAPO%20when%20DS%20is%20disabled.&entry.1838667208=http%3A//arxiv.org/abs/2512.07611v1&entry.124074799=Read"},
{"title": "MAPLE: Encoding Dexterous Robotic Manipulation Priors Learned From Egocentric Videos", "author": "Alexey Gavryushin and Xi Wang and Robert J. S. Malate and Chenyu Yang and Davide Liconti and Ren\u00e9 Zurbr\u00fcgg and Robert K. Katzschmann and Marc Pollefeys", "abstract": "Large-scale egocentric video datasets capture diverse human activities across a wide range of scenarios, offering rich and detailed insights into how humans interact with objects, especially those that require fine-grained dexterous control. Such complex, dexterous skills with precise controls are crucial for many robotic manipulation tasks, yet are often insufficiently addressed by traditional data-driven approaches to robotic manipulation. To address this gap, we leverage manipulation priors learned from large-scale egocentric video datasets to improve policy learning for dexterous robotic manipulation tasks. We present MAPLE, a novel method for dexterous robotic manipulation that learns features to predict object contact points and detailed hand poses at the moment of contact from egocentric images. We then use the learned features to train policies for downstream manipulation tasks. Experimental results demonstrate the effectiveness of MAPLE across 4 existing simulation benchmarks, as well as a newly designed set of 4 challenging simulation tasks requiring fine-grained object control and complex dexterous skills. The benefits of MAPLE are further highlighted in real-world experiments using a 17 DoF dexterous robotic hand, whereas the simultaneous evaluation across both simulation and real-world experiments has remained underexplored in prior work. We additionally showcase the efficacy of our model on an egocentric contact point prediction task, validating its usefulness beyond dexterous manipulation policy learning.", "link": "http://arxiv.org/abs/2504.06084v2", "date": "2025-12-08", "relevancy": 1.8187, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6344}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6253}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAPLE%3A%20Encoding%20Dexterous%20Robotic%20Manipulation%20Priors%20Learned%20From%20Egocentric%20Videos&body=Title%3A%20MAPLE%3A%20Encoding%20Dexterous%20Robotic%20Manipulation%20Priors%20Learned%20From%20Egocentric%20Videos%0AAuthor%3A%20Alexey%20Gavryushin%20and%20Xi%20Wang%20and%20Robert%20J.%20S.%20Malate%20and%20Chenyu%20Yang%20and%20Davide%20Liconti%20and%20Ren%C3%A9%20Zurbr%C3%BCgg%20and%20Robert%20K.%20Katzschmann%20and%20Marc%20Pollefeys%0AAbstract%3A%20Large-scale%20egocentric%20video%20datasets%20capture%20diverse%20human%20activities%20across%20a%20wide%20range%20of%20scenarios%2C%20offering%20rich%20and%20detailed%20insights%20into%20how%20humans%20interact%20with%20objects%2C%20especially%20those%20that%20require%20fine-grained%20dexterous%20control.%20Such%20complex%2C%20dexterous%20skills%20with%20precise%20controls%20are%20crucial%20for%20many%20robotic%20manipulation%20tasks%2C%20yet%20are%20often%20insufficiently%20addressed%20by%20traditional%20data-driven%20approaches%20to%20robotic%20manipulation.%20To%20address%20this%20gap%2C%20we%20leverage%20manipulation%20priors%20learned%20from%20large-scale%20egocentric%20video%20datasets%20to%20improve%20policy%20learning%20for%20dexterous%20robotic%20manipulation%20tasks.%20We%20present%20MAPLE%2C%20a%20novel%20method%20for%20dexterous%20robotic%20manipulation%20that%20learns%20features%20to%20predict%20object%20contact%20points%20and%20detailed%20hand%20poses%20at%20the%20moment%20of%20contact%20from%20egocentric%20images.%20We%20then%20use%20the%20learned%20features%20to%20train%20policies%20for%20downstream%20manipulation%20tasks.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20MAPLE%20across%204%20existing%20simulation%20benchmarks%2C%20as%20well%20as%20a%20newly%20designed%20set%20of%204%20challenging%20simulation%20tasks%20requiring%20fine-grained%20object%20control%20and%20complex%20dexterous%20skills.%20The%20benefits%20of%20MAPLE%20are%20further%20highlighted%20in%20real-world%20experiments%20using%20a%2017%20DoF%20dexterous%20robotic%20hand%2C%20whereas%20the%20simultaneous%20evaluation%20across%20both%20simulation%20and%20real-world%20experiments%20has%20remained%20underexplored%20in%20prior%20work.%20We%20additionally%20showcase%20the%20efficacy%20of%20our%20model%20on%20an%20egocentric%20contact%20point%20prediction%20task%2C%20validating%20its%20usefulness%20beyond%20dexterous%20manipulation%20policy%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2504.06084v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAPLE%253A%2520Encoding%2520Dexterous%2520Robotic%2520Manipulation%2520Priors%2520Learned%2520From%2520Egocentric%2520Videos%26entry.906535625%3DAlexey%2520Gavryushin%2520and%2520Xi%2520Wang%2520and%2520Robert%2520J.%2520S.%2520Malate%2520and%2520Chenyu%2520Yang%2520and%2520Davide%2520Liconti%2520and%2520Ren%25C3%25A9%2520Zurbr%25C3%25BCgg%2520and%2520Robert%2520K.%2520Katzschmann%2520and%2520Marc%2520Pollefeys%26entry.1292438233%3DLarge-scale%2520egocentric%2520video%2520datasets%2520capture%2520diverse%2520human%2520activities%2520across%2520a%2520wide%2520range%2520of%2520scenarios%252C%2520offering%2520rich%2520and%2520detailed%2520insights%2520into%2520how%2520humans%2520interact%2520with%2520objects%252C%2520especially%2520those%2520that%2520require%2520fine-grained%2520dexterous%2520control.%2520Such%2520complex%252C%2520dexterous%2520skills%2520with%2520precise%2520controls%2520are%2520crucial%2520for%2520many%2520robotic%2520manipulation%2520tasks%252C%2520yet%2520are%2520often%2520insufficiently%2520addressed%2520by%2520traditional%2520data-driven%2520approaches%2520to%2520robotic%2520manipulation.%2520To%2520address%2520this%2520gap%252C%2520we%2520leverage%2520manipulation%2520priors%2520learned%2520from%2520large-scale%2520egocentric%2520video%2520datasets%2520to%2520improve%2520policy%2520learning%2520for%2520dexterous%2520robotic%2520manipulation%2520tasks.%2520We%2520present%2520MAPLE%252C%2520a%2520novel%2520method%2520for%2520dexterous%2520robotic%2520manipulation%2520that%2520learns%2520features%2520to%2520predict%2520object%2520contact%2520points%2520and%2520detailed%2520hand%2520poses%2520at%2520the%2520moment%2520of%2520contact%2520from%2520egocentric%2520images.%2520We%2520then%2520use%2520the%2520learned%2520features%2520to%2520train%2520policies%2520for%2520downstream%2520manipulation%2520tasks.%2520Experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520MAPLE%2520across%25204%2520existing%2520simulation%2520benchmarks%252C%2520as%2520well%2520as%2520a%2520newly%2520designed%2520set%2520of%25204%2520challenging%2520simulation%2520tasks%2520requiring%2520fine-grained%2520object%2520control%2520and%2520complex%2520dexterous%2520skills.%2520The%2520benefits%2520of%2520MAPLE%2520are%2520further%2520highlighted%2520in%2520real-world%2520experiments%2520using%2520a%252017%2520DoF%2520dexterous%2520robotic%2520hand%252C%2520whereas%2520the%2520simultaneous%2520evaluation%2520across%2520both%2520simulation%2520and%2520real-world%2520experiments%2520has%2520remained%2520underexplored%2520in%2520prior%2520work.%2520We%2520additionally%2520showcase%2520the%2520efficacy%2520of%2520our%2520model%2520on%2520an%2520egocentric%2520contact%2520point%2520prediction%2520task%252C%2520validating%2520its%2520usefulness%2520beyond%2520dexterous%2520manipulation%2520policy%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06084v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAPLE%3A%20Encoding%20Dexterous%20Robotic%20Manipulation%20Priors%20Learned%20From%20Egocentric%20Videos&entry.906535625=Alexey%20Gavryushin%20and%20Xi%20Wang%20and%20Robert%20J.%20S.%20Malate%20and%20Chenyu%20Yang%20and%20Davide%20Liconti%20and%20Ren%C3%A9%20Zurbr%C3%BCgg%20and%20Robert%20K.%20Katzschmann%20and%20Marc%20Pollefeys&entry.1292438233=Large-scale%20egocentric%20video%20datasets%20capture%20diverse%20human%20activities%20across%20a%20wide%20range%20of%20scenarios%2C%20offering%20rich%20and%20detailed%20insights%20into%20how%20humans%20interact%20with%20objects%2C%20especially%20those%20that%20require%20fine-grained%20dexterous%20control.%20Such%20complex%2C%20dexterous%20skills%20with%20precise%20controls%20are%20crucial%20for%20many%20robotic%20manipulation%20tasks%2C%20yet%20are%20often%20insufficiently%20addressed%20by%20traditional%20data-driven%20approaches%20to%20robotic%20manipulation.%20To%20address%20this%20gap%2C%20we%20leverage%20manipulation%20priors%20learned%20from%20large-scale%20egocentric%20video%20datasets%20to%20improve%20policy%20learning%20for%20dexterous%20robotic%20manipulation%20tasks.%20We%20present%20MAPLE%2C%20a%20novel%20method%20for%20dexterous%20robotic%20manipulation%20that%20learns%20features%20to%20predict%20object%20contact%20points%20and%20detailed%20hand%20poses%20at%20the%20moment%20of%20contact%20from%20egocentric%20images.%20We%20then%20use%20the%20learned%20features%20to%20train%20policies%20for%20downstream%20manipulation%20tasks.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20MAPLE%20across%204%20existing%20simulation%20benchmarks%2C%20as%20well%20as%20a%20newly%20designed%20set%20of%204%20challenging%20simulation%20tasks%20requiring%20fine-grained%20object%20control%20and%20complex%20dexterous%20skills.%20The%20benefits%20of%20MAPLE%20are%20further%20highlighted%20in%20real-world%20experiments%20using%20a%2017%20DoF%20dexterous%20robotic%20hand%2C%20whereas%20the%20simultaneous%20evaluation%20across%20both%20simulation%20and%20real-world%20experiments%20has%20remained%20underexplored%20in%20prior%20work.%20We%20additionally%20showcase%20the%20efficacy%20of%20our%20model%20on%20an%20egocentric%20contact%20point%20prediction%20task%2C%20validating%20its%20usefulness%20beyond%20dexterous%20manipulation%20policy%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2504.06084v2&entry.124074799=Read"},
{"title": "Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support", "author": "Raunak Jain and Mudita Khurana", "abstract": "LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.", "link": "http://arxiv.org/abs/2512.07801v1", "date": "2025-12-08", "relevancy": 1.4279, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4996}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4751}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Causal%20Sensemaking%3A%20Closing%20the%20Complementarity%20Gap%20in%20Human-AI%20Decision%20Support&body=Title%3A%20Collaborative%20Causal%20Sensemaking%3A%20Closing%20the%20Complementarity%20Gap%20in%20Human-AI%20Decision%20Support%0AAuthor%3A%20Raunak%20Jain%20and%20Mudita%20Khurana%0AAbstract%3A%20LLM-based%20agents%20are%20rapidly%20being%20plugged%20into%20expert%20decision-support%2C%20yet%20in%20messy%2C%20high-stakes%20settings%20they%20rarely%20make%20the%20team%20smarter%3A%20human-AI%20teams%20often%20underperform%20the%20best%20individual%2C%20experts%20oscillate%20between%20verification%20loops%20and%20over-reliance%2C%20and%20the%20promised%20complementarity%20does%20not%20materialise.%20We%20argue%20this%20is%20not%20just%20a%20matter%20of%20accuracy%2C%20but%20a%20fundamental%20gap%20in%20how%20we%20conceive%20AI%20assistance%3A%20expert%20decisions%20are%20made%20through%20collaborative%20cognitive%20processes%20where%20mental%20models%2C%20goals%2C%20and%20constraints%20are%20continually%20co-constructed%2C%20tested%2C%20and%20revised%20between%20human%20and%20AI.%20We%20propose%20Collaborative%20Causal%20Sensemaking%20%28CCS%29%20as%20a%20research%20agenda%20and%20organizing%20framework%20for%20decision-support%20agents%3A%20systems%20designed%20as%20partners%20in%20cognitive%20work%2C%20maintaining%20evolving%20models%20of%20how%20particular%20experts%20reason%2C%20helping%20articulate%20and%20revise%20goals%2C%20co-constructing%20and%20stress-testing%20causal%20hypotheses%2C%20and%20learning%20from%20the%20outcomes%20of%20joint%20decisions%20so%20that%20both%20human%20and%20agent%20improve%20over%20time.%20We%20sketch%20challenges%20around%20training%20ecologies%20that%20make%20collaborative%20thinking%20instrumentally%20valuable%2C%20representations%20and%20interaction%20protocols%20for%20co-authored%20models%2C%20and%20evaluation%20centred%20on%20trust%20and%20complementarity.%20These%20directions%20can%20reframe%20MAS%20research%20around%20agents%20that%20participate%20in%20collaborative%20sensemaking%20and%20act%20as%20AI%20teammates%20that%20think%20with%20their%20human%20partners.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Causal%2520Sensemaking%253A%2520Closing%2520the%2520Complementarity%2520Gap%2520in%2520Human-AI%2520Decision%2520Support%26entry.906535625%3DRaunak%2520Jain%2520and%2520Mudita%2520Khurana%26entry.1292438233%3DLLM-based%2520agents%2520are%2520rapidly%2520being%2520plugged%2520into%2520expert%2520decision-support%252C%2520yet%2520in%2520messy%252C%2520high-stakes%2520settings%2520they%2520rarely%2520make%2520the%2520team%2520smarter%253A%2520human-AI%2520teams%2520often%2520underperform%2520the%2520best%2520individual%252C%2520experts%2520oscillate%2520between%2520verification%2520loops%2520and%2520over-reliance%252C%2520and%2520the%2520promised%2520complementarity%2520does%2520not%2520materialise.%2520We%2520argue%2520this%2520is%2520not%2520just%2520a%2520matter%2520of%2520accuracy%252C%2520but%2520a%2520fundamental%2520gap%2520in%2520how%2520we%2520conceive%2520AI%2520assistance%253A%2520expert%2520decisions%2520are%2520made%2520through%2520collaborative%2520cognitive%2520processes%2520where%2520mental%2520models%252C%2520goals%252C%2520and%2520constraints%2520are%2520continually%2520co-constructed%252C%2520tested%252C%2520and%2520revised%2520between%2520human%2520and%2520AI.%2520We%2520propose%2520Collaborative%2520Causal%2520Sensemaking%2520%2528CCS%2529%2520as%2520a%2520research%2520agenda%2520and%2520organizing%2520framework%2520for%2520decision-support%2520agents%253A%2520systems%2520designed%2520as%2520partners%2520in%2520cognitive%2520work%252C%2520maintaining%2520evolving%2520models%2520of%2520how%2520particular%2520experts%2520reason%252C%2520helping%2520articulate%2520and%2520revise%2520goals%252C%2520co-constructing%2520and%2520stress-testing%2520causal%2520hypotheses%252C%2520and%2520learning%2520from%2520the%2520outcomes%2520of%2520joint%2520decisions%2520so%2520that%2520both%2520human%2520and%2520agent%2520improve%2520over%2520time.%2520We%2520sketch%2520challenges%2520around%2520training%2520ecologies%2520that%2520make%2520collaborative%2520thinking%2520instrumentally%2520valuable%252C%2520representations%2520and%2520interaction%2520protocols%2520for%2520co-authored%2520models%252C%2520and%2520evaluation%2520centred%2520on%2520trust%2520and%2520complementarity.%2520These%2520directions%2520can%2520reframe%2520MAS%2520research%2520around%2520agents%2520that%2520participate%2520in%2520collaborative%2520sensemaking%2520and%2520act%2520as%2520AI%2520teammates%2520that%2520think%2520with%2520their%2520human%2520partners.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Causal%20Sensemaking%3A%20Closing%20the%20Complementarity%20Gap%20in%20Human-AI%20Decision%20Support&entry.906535625=Raunak%20Jain%20and%20Mudita%20Khurana&entry.1292438233=LLM-based%20agents%20are%20rapidly%20being%20plugged%20into%20expert%20decision-support%2C%20yet%20in%20messy%2C%20high-stakes%20settings%20they%20rarely%20make%20the%20team%20smarter%3A%20human-AI%20teams%20often%20underperform%20the%20best%20individual%2C%20experts%20oscillate%20between%20verification%20loops%20and%20over-reliance%2C%20and%20the%20promised%20complementarity%20does%20not%20materialise.%20We%20argue%20this%20is%20not%20just%20a%20matter%20of%20accuracy%2C%20but%20a%20fundamental%20gap%20in%20how%20we%20conceive%20AI%20assistance%3A%20expert%20decisions%20are%20made%20through%20collaborative%20cognitive%20processes%20where%20mental%20models%2C%20goals%2C%20and%20constraints%20are%20continually%20co-constructed%2C%20tested%2C%20and%20revised%20between%20human%20and%20AI.%20We%20propose%20Collaborative%20Causal%20Sensemaking%20%28CCS%29%20as%20a%20research%20agenda%20and%20organizing%20framework%20for%20decision-support%20agents%3A%20systems%20designed%20as%20partners%20in%20cognitive%20work%2C%20maintaining%20evolving%20models%20of%20how%20particular%20experts%20reason%2C%20helping%20articulate%20and%20revise%20goals%2C%20co-constructing%20and%20stress-testing%20causal%20hypotheses%2C%20and%20learning%20from%20the%20outcomes%20of%20joint%20decisions%20so%20that%20both%20human%20and%20agent%20improve%20over%20time.%20We%20sketch%20challenges%20around%20training%20ecologies%20that%20make%20collaborative%20thinking%20instrumentally%20valuable%2C%20representations%20and%20interaction%20protocols%20for%20co-authored%20models%2C%20and%20evaluation%20centred%20on%20trust%20and%20complementarity.%20These%20directions%20can%20reframe%20MAS%20research%20around%20agents%20that%20participate%20in%20collaborative%20sensemaking%20and%20act%20as%20AI%20teammates%20that%20think%20with%20their%20human%20partners.&entry.1838667208=http%3A//arxiv.org/abs/2512.07801v1&entry.124074799=Read"},
{"title": "AI/ML in 3GPP 5G Advanced -- Services and Architecture", "author": "Pradnya Taksande and Shwetha Kiran and Pranav Jha and Prasanna Chaporkar", "abstract": "The 3rd Generation Partnership Project (3GPP), the standards body for mobile networks, is in the final phase of Release 19 standardization and is beginning Release 20. Artificial Intelligence/ Machine Learning (AI/ML) has brought about a paradigm shift in technology and it is being adopted across industries and verticals. 3GPP has been integrating AI/ML into the 5G advanced system since Release 18. This paper focuses on the AI/ML related technological advancements and features introduced in Release 19 within the Service and System Aspects (SA) Technical specifications group of 3GPP. The advancements relate to two paradigms: (i) enhancements that AI/ML brought to the 5G advanced system (AI for network), e.g. resource optimization, and (ii) enhancements that were made to the 5G system to support AI/ML applications (Network for AI), e.g. image recognition.", "link": "http://arxiv.org/abs/2512.03728v3", "date": "2025-12-08", "relevancy": 1.1383, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3838}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3809}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI/ML%20in%203GPP%205G%20Advanced%20--%20Services%20and%20Architecture&body=Title%3A%20AI/ML%20in%203GPP%205G%20Advanced%20--%20Services%20and%20Architecture%0AAuthor%3A%20Pradnya%20Taksande%20and%20Shwetha%20Kiran%20and%20Pranav%20Jha%20and%20Prasanna%20Chaporkar%0AAbstract%3A%20The%203rd%20Generation%20Partnership%20Project%20%283GPP%29%2C%20the%20standards%20body%20for%20mobile%20networks%2C%20is%20in%20the%20final%20phase%20of%20Release%2019%20standardization%20and%20is%20beginning%20Release%2020.%20Artificial%20Intelligence/%20Machine%20Learning%20%28AI/ML%29%20has%20brought%20about%20a%20paradigm%20shift%20in%20technology%20and%20it%20is%20being%20adopted%20across%20industries%20and%20verticals.%203GPP%20has%20been%20integrating%20AI/ML%20into%20the%205G%20advanced%20system%20since%20Release%2018.%20This%20paper%20focuses%20on%20the%20AI/ML%20related%20technological%20advancements%20and%20features%20introduced%20in%20Release%2019%20within%20the%20Service%20and%20System%20Aspects%20%28SA%29%20Technical%20specifications%20group%20of%203GPP.%20The%20advancements%20relate%20to%20two%20paradigms%3A%20%28i%29%20enhancements%20that%20AI/ML%20brought%20to%20the%205G%20advanced%20system%20%28AI%20for%20network%29%2C%20e.g.%20resource%20optimization%2C%20and%20%28ii%29%20enhancements%20that%20were%20made%20to%20the%205G%20system%20to%20support%20AI/ML%20applications%20%28Network%20for%20AI%29%2C%20e.g.%20image%20recognition.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03728v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI/ML%2520in%25203GPP%25205G%2520Advanced%2520--%2520Services%2520and%2520Architecture%26entry.906535625%3DPradnya%2520Taksande%2520and%2520Shwetha%2520Kiran%2520and%2520Pranav%2520Jha%2520and%2520Prasanna%2520Chaporkar%26entry.1292438233%3DThe%25203rd%2520Generation%2520Partnership%2520Project%2520%25283GPP%2529%252C%2520the%2520standards%2520body%2520for%2520mobile%2520networks%252C%2520is%2520in%2520the%2520final%2520phase%2520of%2520Release%252019%2520standardization%2520and%2520is%2520beginning%2520Release%252020.%2520Artificial%2520Intelligence/%2520Machine%2520Learning%2520%2528AI/ML%2529%2520has%2520brought%2520about%2520a%2520paradigm%2520shift%2520in%2520technology%2520and%2520it%2520is%2520being%2520adopted%2520across%2520industries%2520and%2520verticals.%25203GPP%2520has%2520been%2520integrating%2520AI/ML%2520into%2520the%25205G%2520advanced%2520system%2520since%2520Release%252018.%2520This%2520paper%2520focuses%2520on%2520the%2520AI/ML%2520related%2520technological%2520advancements%2520and%2520features%2520introduced%2520in%2520Release%252019%2520within%2520the%2520Service%2520and%2520System%2520Aspects%2520%2528SA%2529%2520Technical%2520specifications%2520group%2520of%25203GPP.%2520The%2520advancements%2520relate%2520to%2520two%2520paradigms%253A%2520%2528i%2529%2520enhancements%2520that%2520AI/ML%2520brought%2520to%2520the%25205G%2520advanced%2520system%2520%2528AI%2520for%2520network%2529%252C%2520e.g.%2520resource%2520optimization%252C%2520and%2520%2528ii%2529%2520enhancements%2520that%2520were%2520made%2520to%2520the%25205G%2520system%2520to%2520support%2520AI/ML%2520applications%2520%2528Network%2520for%2520AI%2529%252C%2520e.g.%2520image%2520recognition.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03728v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI/ML%20in%203GPP%205G%20Advanced%20--%20Services%20and%20Architecture&entry.906535625=Pradnya%20Taksande%20and%20Shwetha%20Kiran%20and%20Pranav%20Jha%20and%20Prasanna%20Chaporkar&entry.1292438233=The%203rd%20Generation%20Partnership%20Project%20%283GPP%29%2C%20the%20standards%20body%20for%20mobile%20networks%2C%20is%20in%20the%20final%20phase%20of%20Release%2019%20standardization%20and%20is%20beginning%20Release%2020.%20Artificial%20Intelligence/%20Machine%20Learning%20%28AI/ML%29%20has%20brought%20about%20a%20paradigm%20shift%20in%20technology%20and%20it%20is%20being%20adopted%20across%20industries%20and%20verticals.%203GPP%20has%20been%20integrating%20AI/ML%20into%20the%205G%20advanced%20system%20since%20Release%2018.%20This%20paper%20focuses%20on%20the%20AI/ML%20related%20technological%20advancements%20and%20features%20introduced%20in%20Release%2019%20within%20the%20Service%20and%20System%20Aspects%20%28SA%29%20Technical%20specifications%20group%20of%203GPP.%20The%20advancements%20relate%20to%20two%20paradigms%3A%20%28i%29%20enhancements%20that%20AI/ML%20brought%20to%20the%205G%20advanced%20system%20%28AI%20for%20network%29%2C%20e.g.%20resource%20optimization%2C%20and%20%28ii%29%20enhancements%20that%20were%20made%20to%20the%205G%20system%20to%20support%20AI/ML%20applications%20%28Network%20for%20AI%29%2C%20e.g.%20image%20recognition.&entry.1838667208=http%3A//arxiv.org/abs/2512.03728v3&entry.124074799=Read"},
{"title": "AMBER: Aerial deployable gripping crawler with compliant microspine for canopy manipulation", "author": "P. A. Wigner and L. Romanello and A. Hammad and P. H. Nguyen and T. Lan and S. F. Armanini and B. B. Kocer and M. Kovac", "abstract": "This paper presents an aerially deployable crawler designed for adaptive locomotion and manipulation within tree canopies. The system combines compliant microspine-based tracks, a dual-track rotary gripper, and an elastic tail, enabling secure attachment and stable traversal across branches of varying curvature and inclination.\n  Experiments demonstrate reliable gripping up to 90 degrees of body roll and inclination, while effective climbing on branches inclined up to 67.5 degrees, achieving a maximum speed of 0.55 body lengths per second on horizontal branches. The compliant tracks allow yaw steering of up to 10 degrees, enhancing maneuverability on irregular surfaces.\n  Power measurements show efficient operation with a dimensionless cost of transport over an order of magnitude lower than typical hovering power consumption in aerial robots. Integrated within a drone-tether deployment system, the crawler provides a robust, low-power platform for environmental sampling and in-canopy sensing, bridging the gap between aerial and surface-based ecological robotics.", "link": "http://arxiv.org/abs/2512.07680v1", "date": "2025-12-08", "relevancy": 1.3935, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4723}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4646}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AMBER%3A%20Aerial%20deployable%20gripping%20crawler%20with%20compliant%20microspine%20for%20canopy%20manipulation&body=Title%3A%20AMBER%3A%20Aerial%20deployable%20gripping%20crawler%20with%20compliant%20microspine%20for%20canopy%20manipulation%0AAuthor%3A%20P.%20A.%20Wigner%20and%20L.%20Romanello%20and%20A.%20Hammad%20and%20P.%20H.%20Nguyen%20and%20T.%20Lan%20and%20S.%20F.%20Armanini%20and%20B.%20B.%20Kocer%20and%20M.%20Kovac%0AAbstract%3A%20This%20paper%20presents%20an%20aerially%20deployable%20crawler%20designed%20for%20adaptive%20locomotion%20and%20manipulation%20within%20tree%20canopies.%20The%20system%20combines%20compliant%20microspine-based%20tracks%2C%20a%20dual-track%20rotary%20gripper%2C%20and%20an%20elastic%20tail%2C%20enabling%20secure%20attachment%20and%20stable%20traversal%20across%20branches%20of%20varying%20curvature%20and%20inclination.%0A%20%20Experiments%20demonstrate%20reliable%20gripping%20up%20to%2090%20degrees%20of%20body%20roll%20and%20inclination%2C%20while%20effective%20climbing%20on%20branches%20inclined%20up%20to%2067.5%20degrees%2C%20achieving%20a%20maximum%20speed%20of%200.55%20body%20lengths%20per%20second%20on%20horizontal%20branches.%20The%20compliant%20tracks%20allow%20yaw%20steering%20of%20up%20to%2010%20degrees%2C%20enhancing%20maneuverability%20on%20irregular%20surfaces.%0A%20%20Power%20measurements%20show%20efficient%20operation%20with%20a%20dimensionless%20cost%20of%20transport%20over%20an%20order%20of%20magnitude%20lower%20than%20typical%20hovering%20power%20consumption%20in%20aerial%20robots.%20Integrated%20within%20a%20drone-tether%20deployment%20system%2C%20the%20crawler%20provides%20a%20robust%2C%20low-power%20platform%20for%20environmental%20sampling%20and%20in-canopy%20sensing%2C%20bridging%20the%20gap%20between%20aerial%20and%20surface-based%20ecological%20robotics.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAMBER%253A%2520Aerial%2520deployable%2520gripping%2520crawler%2520with%2520compliant%2520microspine%2520for%2520canopy%2520manipulation%26entry.906535625%3DP.%2520A.%2520Wigner%2520and%2520L.%2520Romanello%2520and%2520A.%2520Hammad%2520and%2520P.%2520H.%2520Nguyen%2520and%2520T.%2520Lan%2520and%2520S.%2520F.%2520Armanini%2520and%2520B.%2520B.%2520Kocer%2520and%2520M.%2520Kovac%26entry.1292438233%3DThis%2520paper%2520presents%2520an%2520aerially%2520deployable%2520crawler%2520designed%2520for%2520adaptive%2520locomotion%2520and%2520manipulation%2520within%2520tree%2520canopies.%2520The%2520system%2520combines%2520compliant%2520microspine-based%2520tracks%252C%2520a%2520dual-track%2520rotary%2520gripper%252C%2520and%2520an%2520elastic%2520tail%252C%2520enabling%2520secure%2520attachment%2520and%2520stable%2520traversal%2520across%2520branches%2520of%2520varying%2520curvature%2520and%2520inclination.%250A%2520%2520Experiments%2520demonstrate%2520reliable%2520gripping%2520up%2520to%252090%2520degrees%2520of%2520body%2520roll%2520and%2520inclination%252C%2520while%2520effective%2520climbing%2520on%2520branches%2520inclined%2520up%2520to%252067.5%2520degrees%252C%2520achieving%2520a%2520maximum%2520speed%2520of%25200.55%2520body%2520lengths%2520per%2520second%2520on%2520horizontal%2520branches.%2520The%2520compliant%2520tracks%2520allow%2520yaw%2520steering%2520of%2520up%2520to%252010%2520degrees%252C%2520enhancing%2520maneuverability%2520on%2520irregular%2520surfaces.%250A%2520%2520Power%2520measurements%2520show%2520efficient%2520operation%2520with%2520a%2520dimensionless%2520cost%2520of%2520transport%2520over%2520an%2520order%2520of%2520magnitude%2520lower%2520than%2520typical%2520hovering%2520power%2520consumption%2520in%2520aerial%2520robots.%2520Integrated%2520within%2520a%2520drone-tether%2520deployment%2520system%252C%2520the%2520crawler%2520provides%2520a%2520robust%252C%2520low-power%2520platform%2520for%2520environmental%2520sampling%2520and%2520in-canopy%2520sensing%252C%2520bridging%2520the%2520gap%2520between%2520aerial%2520and%2520surface-based%2520ecological%2520robotics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AMBER%3A%20Aerial%20deployable%20gripping%20crawler%20with%20compliant%20microspine%20for%20canopy%20manipulation&entry.906535625=P.%20A.%20Wigner%20and%20L.%20Romanello%20and%20A.%20Hammad%20and%20P.%20H.%20Nguyen%20and%20T.%20Lan%20and%20S.%20F.%20Armanini%20and%20B.%20B.%20Kocer%20and%20M.%20Kovac&entry.1292438233=This%20paper%20presents%20an%20aerially%20deployable%20crawler%20designed%20for%20adaptive%20locomotion%20and%20manipulation%20within%20tree%20canopies.%20The%20system%20combines%20compliant%20microspine-based%20tracks%2C%20a%20dual-track%20rotary%20gripper%2C%20and%20an%20elastic%20tail%2C%20enabling%20secure%20attachment%20and%20stable%20traversal%20across%20branches%20of%20varying%20curvature%20and%20inclination.%0A%20%20Experiments%20demonstrate%20reliable%20gripping%20up%20to%2090%20degrees%20of%20body%20roll%20and%20inclination%2C%20while%20effective%20climbing%20on%20branches%20inclined%20up%20to%2067.5%20degrees%2C%20achieving%20a%20maximum%20speed%20of%200.55%20body%20lengths%20per%20second%20on%20horizontal%20branches.%20The%20compliant%20tracks%20allow%20yaw%20steering%20of%20up%20to%2010%20degrees%2C%20enhancing%20maneuverability%20on%20irregular%20surfaces.%0A%20%20Power%20measurements%20show%20efficient%20operation%20with%20a%20dimensionless%20cost%20of%20transport%20over%20an%20order%20of%20magnitude%20lower%20than%20typical%20hovering%20power%20consumption%20in%20aerial%20robots.%20Integrated%20within%20a%20drone-tether%20deployment%20system%2C%20the%20crawler%20provides%20a%20robust%2C%20low-power%20platform%20for%20environmental%20sampling%20and%20in-canopy%20sensing%2C%20bridging%20the%20gap%20between%20aerial%20and%20surface-based%20ecological%20robotics.&entry.1838667208=http%3A//arxiv.org/abs/2512.07680v1&entry.124074799=Read"},
{"title": "The Loss of Control Playbook: Degrees, Dynamics, and Preparedness", "author": "Charlotte Stix and Annika Hallensleben and Alejandro Ortega and Matteo Pistillo", "abstract": "This research report addresses the absence of an actionable definition for Loss of Control (LoC) in AI systems by developing a novel taxonomy and preparedness framework. Despite increasing policy and research attention, existing LoC definitions vary significantly in scope and timeline, hindering effective LoC assessment and mitigation. To address this issue, we draw from an extensive literature review and propose a graded LoC taxonomy, based on the metrics of severity and persistence, that distinguishes between Deviation, Bounded LoC, and Strict LoC. We model pathways toward a societal state of vulnerability in which sufficiently advanced AI systems have acquired or could acquire the means to cause Bounded or Strict LoC once a catalyst, either misalignment or pure malfunction, materializes. We argue that this state becomes increasingly likely over time, absent strategic intervention, and propose a strategy to avoid reaching a state of vulnerability. Rather than focusing solely on intervening on AI capabilities and propensities potentially relevant for LoC or on preventing potential catalysts, we introduce a complementary framework that emphasizes three extrinsic factors: Deployment context, Affordances, and Permissions (the DAP framework). Compared to work on intrinsic factors and catalysts, this framework has the unfair advantage of being actionable today. Finally, we put forward a plan to maintain preparedness and prevent the occurrence of LoC outcomes should a state of societal vulnerability be reached, focusing on governance measures (threat modeling, deployment policies, emergency response) and technical controls (pre-deployment testing, control measures, monitoring) that could maintain a condition of perennial suspension.", "link": "http://arxiv.org/abs/2511.15846v5", "date": "2025-12-08", "relevancy": 1.7402, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4581}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4191}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Loss%20of%20Control%20Playbook%3A%20Degrees%2C%20Dynamics%2C%20and%20Preparedness&body=Title%3A%20The%20Loss%20of%20Control%20Playbook%3A%20Degrees%2C%20Dynamics%2C%20and%20Preparedness%0AAuthor%3A%20Charlotte%20Stix%20and%20Annika%20Hallensleben%20and%20Alejandro%20Ortega%20and%20Matteo%20Pistillo%0AAbstract%3A%20This%20research%20report%20addresses%20the%20absence%20of%20an%20actionable%20definition%20for%20Loss%20of%20Control%20%28LoC%29%20in%20AI%20systems%20by%20developing%20a%20novel%20taxonomy%20and%20preparedness%20framework.%20Despite%20increasing%20policy%20and%20research%20attention%2C%20existing%20LoC%20definitions%20vary%20significantly%20in%20scope%20and%20timeline%2C%20hindering%20effective%20LoC%20assessment%20and%20mitigation.%20To%20address%20this%20issue%2C%20we%20draw%20from%20an%20extensive%20literature%20review%20and%20propose%20a%20graded%20LoC%20taxonomy%2C%20based%20on%20the%20metrics%20of%20severity%20and%20persistence%2C%20that%20distinguishes%20between%20Deviation%2C%20Bounded%20LoC%2C%20and%20Strict%20LoC.%20We%20model%20pathways%20toward%20a%20societal%20state%20of%20vulnerability%20in%20which%20sufficiently%20advanced%20AI%20systems%20have%20acquired%20or%20could%20acquire%20the%20means%20to%20cause%20Bounded%20or%20Strict%20LoC%20once%20a%20catalyst%2C%20either%20misalignment%20or%20pure%20malfunction%2C%20materializes.%20We%20argue%20that%20this%20state%20becomes%20increasingly%20likely%20over%20time%2C%20absent%20strategic%20intervention%2C%20and%20propose%20a%20strategy%20to%20avoid%20reaching%20a%20state%20of%20vulnerability.%20Rather%20than%20focusing%20solely%20on%20intervening%20on%20AI%20capabilities%20and%20propensities%20potentially%20relevant%20for%20LoC%20or%20on%20preventing%20potential%20catalysts%2C%20we%20introduce%20a%20complementary%20framework%20that%20emphasizes%20three%20extrinsic%20factors%3A%20Deployment%20context%2C%20Affordances%2C%20and%20Permissions%20%28the%20DAP%20framework%29.%20Compared%20to%20work%20on%20intrinsic%20factors%20and%20catalysts%2C%20this%20framework%20has%20the%20unfair%20advantage%20of%20being%20actionable%20today.%20Finally%2C%20we%20put%20forward%20a%20plan%20to%20maintain%20preparedness%20and%20prevent%20the%20occurrence%20of%20LoC%20outcomes%20should%20a%20state%20of%20societal%20vulnerability%20be%20reached%2C%20focusing%20on%20governance%20measures%20%28threat%20modeling%2C%20deployment%20policies%2C%20emergency%20response%29%20and%20technical%20controls%20%28pre-deployment%20testing%2C%20control%20measures%2C%20monitoring%29%20that%20could%20maintain%20a%20condition%20of%20perennial%20suspension.%0ALink%3A%20http%3A//arxiv.org/abs/2511.15846v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Loss%2520of%2520Control%2520Playbook%253A%2520Degrees%252C%2520Dynamics%252C%2520and%2520Preparedness%26entry.906535625%3DCharlotte%2520Stix%2520and%2520Annika%2520Hallensleben%2520and%2520Alejandro%2520Ortega%2520and%2520Matteo%2520Pistillo%26entry.1292438233%3DThis%2520research%2520report%2520addresses%2520the%2520absence%2520of%2520an%2520actionable%2520definition%2520for%2520Loss%2520of%2520Control%2520%2528LoC%2529%2520in%2520AI%2520systems%2520by%2520developing%2520a%2520novel%2520taxonomy%2520and%2520preparedness%2520framework.%2520Despite%2520increasing%2520policy%2520and%2520research%2520attention%252C%2520existing%2520LoC%2520definitions%2520vary%2520significantly%2520in%2520scope%2520and%2520timeline%252C%2520hindering%2520effective%2520LoC%2520assessment%2520and%2520mitigation.%2520To%2520address%2520this%2520issue%252C%2520we%2520draw%2520from%2520an%2520extensive%2520literature%2520review%2520and%2520propose%2520a%2520graded%2520LoC%2520taxonomy%252C%2520based%2520on%2520the%2520metrics%2520of%2520severity%2520and%2520persistence%252C%2520that%2520distinguishes%2520between%2520Deviation%252C%2520Bounded%2520LoC%252C%2520and%2520Strict%2520LoC.%2520We%2520model%2520pathways%2520toward%2520a%2520societal%2520state%2520of%2520vulnerability%2520in%2520which%2520sufficiently%2520advanced%2520AI%2520systems%2520have%2520acquired%2520or%2520could%2520acquire%2520the%2520means%2520to%2520cause%2520Bounded%2520or%2520Strict%2520LoC%2520once%2520a%2520catalyst%252C%2520either%2520misalignment%2520or%2520pure%2520malfunction%252C%2520materializes.%2520We%2520argue%2520that%2520this%2520state%2520becomes%2520increasingly%2520likely%2520over%2520time%252C%2520absent%2520strategic%2520intervention%252C%2520and%2520propose%2520a%2520strategy%2520to%2520avoid%2520reaching%2520a%2520state%2520of%2520vulnerability.%2520Rather%2520than%2520focusing%2520solely%2520on%2520intervening%2520on%2520AI%2520capabilities%2520and%2520propensities%2520potentially%2520relevant%2520for%2520LoC%2520or%2520on%2520preventing%2520potential%2520catalysts%252C%2520we%2520introduce%2520a%2520complementary%2520framework%2520that%2520emphasizes%2520three%2520extrinsic%2520factors%253A%2520Deployment%2520context%252C%2520Affordances%252C%2520and%2520Permissions%2520%2528the%2520DAP%2520framework%2529.%2520Compared%2520to%2520work%2520on%2520intrinsic%2520factors%2520and%2520catalysts%252C%2520this%2520framework%2520has%2520the%2520unfair%2520advantage%2520of%2520being%2520actionable%2520today.%2520Finally%252C%2520we%2520put%2520forward%2520a%2520plan%2520to%2520maintain%2520preparedness%2520and%2520prevent%2520the%2520occurrence%2520of%2520LoC%2520outcomes%2520should%2520a%2520state%2520of%2520societal%2520vulnerability%2520be%2520reached%252C%2520focusing%2520on%2520governance%2520measures%2520%2528threat%2520modeling%252C%2520deployment%2520policies%252C%2520emergency%2520response%2529%2520and%2520technical%2520controls%2520%2528pre-deployment%2520testing%252C%2520control%2520measures%252C%2520monitoring%2529%2520that%2520could%2520maintain%2520a%2520condition%2520of%2520perennial%2520suspension.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.15846v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Loss%20of%20Control%20Playbook%3A%20Degrees%2C%20Dynamics%2C%20and%20Preparedness&entry.906535625=Charlotte%20Stix%20and%20Annika%20Hallensleben%20and%20Alejandro%20Ortega%20and%20Matteo%20Pistillo&entry.1292438233=This%20research%20report%20addresses%20the%20absence%20of%20an%20actionable%20definition%20for%20Loss%20of%20Control%20%28LoC%29%20in%20AI%20systems%20by%20developing%20a%20novel%20taxonomy%20and%20preparedness%20framework.%20Despite%20increasing%20policy%20and%20research%20attention%2C%20existing%20LoC%20definitions%20vary%20significantly%20in%20scope%20and%20timeline%2C%20hindering%20effective%20LoC%20assessment%20and%20mitigation.%20To%20address%20this%20issue%2C%20we%20draw%20from%20an%20extensive%20literature%20review%20and%20propose%20a%20graded%20LoC%20taxonomy%2C%20based%20on%20the%20metrics%20of%20severity%20and%20persistence%2C%20that%20distinguishes%20between%20Deviation%2C%20Bounded%20LoC%2C%20and%20Strict%20LoC.%20We%20model%20pathways%20toward%20a%20societal%20state%20of%20vulnerability%20in%20which%20sufficiently%20advanced%20AI%20systems%20have%20acquired%20or%20could%20acquire%20the%20means%20to%20cause%20Bounded%20or%20Strict%20LoC%20once%20a%20catalyst%2C%20either%20misalignment%20or%20pure%20malfunction%2C%20materializes.%20We%20argue%20that%20this%20state%20becomes%20increasingly%20likely%20over%20time%2C%20absent%20strategic%20intervention%2C%20and%20propose%20a%20strategy%20to%20avoid%20reaching%20a%20state%20of%20vulnerability.%20Rather%20than%20focusing%20solely%20on%20intervening%20on%20AI%20capabilities%20and%20propensities%20potentially%20relevant%20for%20LoC%20or%20on%20preventing%20potential%20catalysts%2C%20we%20introduce%20a%20complementary%20framework%20that%20emphasizes%20three%20extrinsic%20factors%3A%20Deployment%20context%2C%20Affordances%2C%20and%20Permissions%20%28the%20DAP%20framework%29.%20Compared%20to%20work%20on%20intrinsic%20factors%20and%20catalysts%2C%20this%20framework%20has%20the%20unfair%20advantage%20of%20being%20actionable%20today.%20Finally%2C%20we%20put%20forward%20a%20plan%20to%20maintain%20preparedness%20and%20prevent%20the%20occurrence%20of%20LoC%20outcomes%20should%20a%20state%20of%20societal%20vulnerability%20be%20reached%2C%20focusing%20on%20governance%20measures%20%28threat%20modeling%2C%20deployment%20policies%2C%20emergency%20response%29%20and%20technical%20controls%20%28pre-deployment%20testing%2C%20control%20measures%2C%20monitoring%29%20that%20could%20maintain%20a%20condition%20of%20perennial%20suspension.&entry.1838667208=http%3A//arxiv.org/abs/2511.15846v5&entry.124074799=Read"},
{"title": "$\u03c6$-test: Global Feature Selection and Inference for Shapley Additive Explanations", "author": "Dongseok Kim and Hyoungsun Choi and Mohamed Jismy Aashik Rasool and Gisung Oh", "abstract": "We propose $\u03c6$-test, a global feature-selection and significance procedure for black-box predictors that combines Shapley attributions with selective inference. Given a trained model and an evaluation dataset, $\u03c6$-test performs SHAP-guided screening and fits a linear surrogate on the screened features via a selection rule with a tractable selective-inference form. For each retained feature, it outputs a Shapley-based global score, a surrogate coefficient, and post-selection $p$-values and confidence intervals in a global feature-importance table. Experiments on real tabular regression tasks with tree-based and neural backbones suggest that $\u03c6$-test can retain much of the predictive ability of the original model while using only a few features and producing feature sets that remain fairly stable across resamples and backbone classes. In these settings, $\u03c6$-test acts as a practical global explanation layer linking Shapley-based importance summaries with classical statistical inference.", "link": "http://arxiv.org/abs/2512.07578v1", "date": "2025-12-08", "relevancy": 1.488, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3879}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3746}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%CF%86%24-test%3A%20Global%20Feature%20Selection%20and%20Inference%20for%20Shapley%20Additive%20Explanations&body=Title%3A%20%24%CF%86%24-test%3A%20Global%20Feature%20Selection%20and%20Inference%20for%20Shapley%20Additive%20Explanations%0AAuthor%3A%20Dongseok%20Kim%20and%20Hyoungsun%20Choi%20and%20Mohamed%20Jismy%20Aashik%20Rasool%20and%20Gisung%20Oh%0AAbstract%3A%20We%20propose%20%24%CF%86%24-test%2C%20a%20global%20feature-selection%20and%20significance%20procedure%20for%20black-box%20predictors%20that%20combines%20Shapley%20attributions%20with%20selective%20inference.%20Given%20a%20trained%20model%20and%20an%20evaluation%20dataset%2C%20%24%CF%86%24-test%20performs%20SHAP-guided%20screening%20and%20fits%20a%20linear%20surrogate%20on%20the%20screened%20features%20via%20a%20selection%20rule%20with%20a%20tractable%20selective-inference%20form.%20For%20each%20retained%20feature%2C%20it%20outputs%20a%20Shapley-based%20global%20score%2C%20a%20surrogate%20coefficient%2C%20and%20post-selection%20%24p%24-values%20and%20confidence%20intervals%20in%20a%20global%20feature-importance%20table.%20Experiments%20on%20real%20tabular%20regression%20tasks%20with%20tree-based%20and%20neural%20backbones%20suggest%20that%20%24%CF%86%24-test%20can%20retain%20much%20of%20the%20predictive%20ability%20of%20the%20original%20model%20while%20using%20only%20a%20few%20features%20and%20producing%20feature%20sets%20that%20remain%20fairly%20stable%20across%20resamples%20and%20backbone%20classes.%20In%20these%20settings%2C%20%24%CF%86%24-test%20acts%20as%20a%20practical%20global%20explanation%20layer%20linking%20Shapley-based%20importance%20summaries%20with%20classical%20statistical%20inference.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07578v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%25CF%2586%2524-test%253A%2520Global%2520Feature%2520Selection%2520and%2520Inference%2520for%2520Shapley%2520Additive%2520Explanations%26entry.906535625%3DDongseok%2520Kim%2520and%2520Hyoungsun%2520Choi%2520and%2520Mohamed%2520Jismy%2520Aashik%2520Rasool%2520and%2520Gisung%2520Oh%26entry.1292438233%3DWe%2520propose%2520%2524%25CF%2586%2524-test%252C%2520a%2520global%2520feature-selection%2520and%2520significance%2520procedure%2520for%2520black-box%2520predictors%2520that%2520combines%2520Shapley%2520attributions%2520with%2520selective%2520inference.%2520Given%2520a%2520trained%2520model%2520and%2520an%2520evaluation%2520dataset%252C%2520%2524%25CF%2586%2524-test%2520performs%2520SHAP-guided%2520screening%2520and%2520fits%2520a%2520linear%2520surrogate%2520on%2520the%2520screened%2520features%2520via%2520a%2520selection%2520rule%2520with%2520a%2520tractable%2520selective-inference%2520form.%2520For%2520each%2520retained%2520feature%252C%2520it%2520outputs%2520a%2520Shapley-based%2520global%2520score%252C%2520a%2520surrogate%2520coefficient%252C%2520and%2520post-selection%2520%2524p%2524-values%2520and%2520confidence%2520intervals%2520in%2520a%2520global%2520feature-importance%2520table.%2520Experiments%2520on%2520real%2520tabular%2520regression%2520tasks%2520with%2520tree-based%2520and%2520neural%2520backbones%2520suggest%2520that%2520%2524%25CF%2586%2524-test%2520can%2520retain%2520much%2520of%2520the%2520predictive%2520ability%2520of%2520the%2520original%2520model%2520while%2520using%2520only%2520a%2520few%2520features%2520and%2520producing%2520feature%2520sets%2520that%2520remain%2520fairly%2520stable%2520across%2520resamples%2520and%2520backbone%2520classes.%2520In%2520these%2520settings%252C%2520%2524%25CF%2586%2524-test%2520acts%2520as%2520a%2520practical%2520global%2520explanation%2520layer%2520linking%2520Shapley-based%2520importance%2520summaries%2520with%2520classical%2520statistical%2520inference.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07578v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%CF%86%24-test%3A%20Global%20Feature%20Selection%20and%20Inference%20for%20Shapley%20Additive%20Explanations&entry.906535625=Dongseok%20Kim%20and%20Hyoungsun%20Choi%20and%20Mohamed%20Jismy%20Aashik%20Rasool%20and%20Gisung%20Oh&entry.1292438233=We%20propose%20%24%CF%86%24-test%2C%20a%20global%20feature-selection%20and%20significance%20procedure%20for%20black-box%20predictors%20that%20combines%20Shapley%20attributions%20with%20selective%20inference.%20Given%20a%20trained%20model%20and%20an%20evaluation%20dataset%2C%20%24%CF%86%24-test%20performs%20SHAP-guided%20screening%20and%20fits%20a%20linear%20surrogate%20on%20the%20screened%20features%20via%20a%20selection%20rule%20with%20a%20tractable%20selective-inference%20form.%20For%20each%20retained%20feature%2C%20it%20outputs%20a%20Shapley-based%20global%20score%2C%20a%20surrogate%20coefficient%2C%20and%20post-selection%20%24p%24-values%20and%20confidence%20intervals%20in%20a%20global%20feature-importance%20table.%20Experiments%20on%20real%20tabular%20regression%20tasks%20with%20tree-based%20and%20neural%20backbones%20suggest%20that%20%24%CF%86%24-test%20can%20retain%20much%20of%20the%20predictive%20ability%20of%20the%20original%20model%20while%20using%20only%20a%20few%20features%20and%20producing%20feature%20sets%20that%20remain%20fairly%20stable%20across%20resamples%20and%20backbone%20classes.%20In%20these%20settings%2C%20%24%CF%86%24-test%20acts%20as%20a%20practical%20global%20explanation%20layer%20linking%20Shapley-based%20importance%20summaries%20with%20classical%20statistical%20inference.&entry.1838667208=http%3A//arxiv.org/abs/2512.07578v1&entry.124074799=Read"},
{"title": "LUNA: LUT-Based Neural Architecture for Fast and Low-Cost Qubit Readout", "author": "M. A. Farooq and G. Di Guglielmo and A. Rajagopala and N. Tran and V. A. Chhabria and A. Arora", "abstract": "Qubit readout is a critical operation in quantum computing systems, which maps the analog response of qubits into discrete classical states. Deep neural networks (DNNs) have recently emerged as a promising solution to improve readout accuracy . Prior hardware implementations of DNN-based readout are resource-intensive and suffer from high inference latency, limiting their practical use in low-latency decoding and quantum error correction (QEC) loops. This paper proposes LUNA, a fast and efficient superconducting qubit readout accelerator that combines low-cost integrator-based preprocessing with Look-Up Table (LUT) based neural networks for classification. The architecture uses simple integrators for dimensionality reduction with minimal hardware overhead, and employs LogicNets (DNNs synthesized into LUT logic) to drastically reduce resource usage while enabling ultra-low-latency inference. We integrate this with a differential evolution based exploration and optimization framework to identify high-quality design points. Our results show up to a 10.95x reduction in area and 30% lower latency with little to no loss in fidelity compared to the state-of-the-art. LUNA enables scalable, low-footprint, and high-speed qubit readout, supporting the development of larger and more reliable quantum computing systems.", "link": "http://arxiv.org/abs/2512.07808v1", "date": "2025-12-08", "relevancy": 1.7337, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4537}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4399}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LUNA%3A%20LUT-Based%20Neural%20Architecture%20for%20Fast%20and%20Low-Cost%20Qubit%20Readout&body=Title%3A%20LUNA%3A%20LUT-Based%20Neural%20Architecture%20for%20Fast%20and%20Low-Cost%20Qubit%20Readout%0AAuthor%3A%20M.%20A.%20Farooq%20and%20G.%20Di%20Guglielmo%20and%20A.%20Rajagopala%20and%20N.%20Tran%20and%20V.%20A.%20Chhabria%20and%20A.%20Arora%0AAbstract%3A%20Qubit%20readout%20is%20a%20critical%20operation%20in%20quantum%20computing%20systems%2C%20which%20maps%20the%20analog%20response%20of%20qubits%20into%20discrete%20classical%20states.%20Deep%20neural%20networks%20%28DNNs%29%20have%20recently%20emerged%20as%20a%20promising%20solution%20to%20improve%20readout%20accuracy%20.%20Prior%20hardware%20implementations%20of%20DNN-based%20readout%20are%20resource-intensive%20and%20suffer%20from%20high%20inference%20latency%2C%20limiting%20their%20practical%20use%20in%20low-latency%20decoding%20and%20quantum%20error%20correction%20%28QEC%29%20loops.%20This%20paper%20proposes%20LUNA%2C%20a%20fast%20and%20efficient%20superconducting%20qubit%20readout%20accelerator%20that%20combines%20low-cost%20integrator-based%20preprocessing%20with%20Look-Up%20Table%20%28LUT%29%20based%20neural%20networks%20for%20classification.%20The%20architecture%20uses%20simple%20integrators%20for%20dimensionality%20reduction%20with%20minimal%20hardware%20overhead%2C%20and%20employs%20LogicNets%20%28DNNs%20synthesized%20into%20LUT%20logic%29%20to%20drastically%20reduce%20resource%20usage%20while%20enabling%20ultra-low-latency%20inference.%20We%20integrate%20this%20with%20a%20differential%20evolution%20based%20exploration%20and%20optimization%20framework%20to%20identify%20high-quality%20design%20points.%20Our%20results%20show%20up%20to%20a%2010.95x%20reduction%20in%20area%20and%2030%25%20lower%20latency%20with%20little%20to%20no%20loss%20in%20fidelity%20compared%20to%20the%20state-of-the-art.%20LUNA%20enables%20scalable%2C%20low-footprint%2C%20and%20high-speed%20qubit%20readout%2C%20supporting%20the%20development%20of%20larger%20and%20more%20reliable%20quantum%20computing%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLUNA%253A%2520LUT-Based%2520Neural%2520Architecture%2520for%2520Fast%2520and%2520Low-Cost%2520Qubit%2520Readout%26entry.906535625%3DM.%2520A.%2520Farooq%2520and%2520G.%2520Di%2520Guglielmo%2520and%2520A.%2520Rajagopala%2520and%2520N.%2520Tran%2520and%2520V.%2520A.%2520Chhabria%2520and%2520A.%2520Arora%26entry.1292438233%3DQubit%2520readout%2520is%2520a%2520critical%2520operation%2520in%2520quantum%2520computing%2520systems%252C%2520which%2520maps%2520the%2520analog%2520response%2520of%2520qubits%2520into%2520discrete%2520classical%2520states.%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520have%2520recently%2520emerged%2520as%2520a%2520promising%2520solution%2520to%2520improve%2520readout%2520accuracy%2520.%2520Prior%2520hardware%2520implementations%2520of%2520DNN-based%2520readout%2520are%2520resource-intensive%2520and%2520suffer%2520from%2520high%2520inference%2520latency%252C%2520limiting%2520their%2520practical%2520use%2520in%2520low-latency%2520decoding%2520and%2520quantum%2520error%2520correction%2520%2528QEC%2529%2520loops.%2520This%2520paper%2520proposes%2520LUNA%252C%2520a%2520fast%2520and%2520efficient%2520superconducting%2520qubit%2520readout%2520accelerator%2520that%2520combines%2520low-cost%2520integrator-based%2520preprocessing%2520with%2520Look-Up%2520Table%2520%2528LUT%2529%2520based%2520neural%2520networks%2520for%2520classification.%2520The%2520architecture%2520uses%2520simple%2520integrators%2520for%2520dimensionality%2520reduction%2520with%2520minimal%2520hardware%2520overhead%252C%2520and%2520employs%2520LogicNets%2520%2528DNNs%2520synthesized%2520into%2520LUT%2520logic%2529%2520to%2520drastically%2520reduce%2520resource%2520usage%2520while%2520enabling%2520ultra-low-latency%2520inference.%2520We%2520integrate%2520this%2520with%2520a%2520differential%2520evolution%2520based%2520exploration%2520and%2520optimization%2520framework%2520to%2520identify%2520high-quality%2520design%2520points.%2520Our%2520results%2520show%2520up%2520to%2520a%252010.95x%2520reduction%2520in%2520area%2520and%252030%2525%2520lower%2520latency%2520with%2520little%2520to%2520no%2520loss%2520in%2520fidelity%2520compared%2520to%2520the%2520state-of-the-art.%2520LUNA%2520enables%2520scalable%252C%2520low-footprint%252C%2520and%2520high-speed%2520qubit%2520readout%252C%2520supporting%2520the%2520development%2520of%2520larger%2520and%2520more%2520reliable%2520quantum%2520computing%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LUNA%3A%20LUT-Based%20Neural%20Architecture%20for%20Fast%20and%20Low-Cost%20Qubit%20Readout&entry.906535625=M.%20A.%20Farooq%20and%20G.%20Di%20Guglielmo%20and%20A.%20Rajagopala%20and%20N.%20Tran%20and%20V.%20A.%20Chhabria%20and%20A.%20Arora&entry.1292438233=Qubit%20readout%20is%20a%20critical%20operation%20in%20quantum%20computing%20systems%2C%20which%20maps%20the%20analog%20response%20of%20qubits%20into%20discrete%20classical%20states.%20Deep%20neural%20networks%20%28DNNs%29%20have%20recently%20emerged%20as%20a%20promising%20solution%20to%20improve%20readout%20accuracy%20.%20Prior%20hardware%20implementations%20of%20DNN-based%20readout%20are%20resource-intensive%20and%20suffer%20from%20high%20inference%20latency%2C%20limiting%20their%20practical%20use%20in%20low-latency%20decoding%20and%20quantum%20error%20correction%20%28QEC%29%20loops.%20This%20paper%20proposes%20LUNA%2C%20a%20fast%20and%20efficient%20superconducting%20qubit%20readout%20accelerator%20that%20combines%20low-cost%20integrator-based%20preprocessing%20with%20Look-Up%20Table%20%28LUT%29%20based%20neural%20networks%20for%20classification.%20The%20architecture%20uses%20simple%20integrators%20for%20dimensionality%20reduction%20with%20minimal%20hardware%20overhead%2C%20and%20employs%20LogicNets%20%28DNNs%20synthesized%20into%20LUT%20logic%29%20to%20drastically%20reduce%20resource%20usage%20while%20enabling%20ultra-low-latency%20inference.%20We%20integrate%20this%20with%20a%20differential%20evolution%20based%20exploration%20and%20optimization%20framework%20to%20identify%20high-quality%20design%20points.%20Our%20results%20show%20up%20to%20a%2010.95x%20reduction%20in%20area%20and%2030%25%20lower%20latency%20with%20little%20to%20no%20loss%20in%20fidelity%20compared%20to%20the%20state-of-the-art.%20LUNA%20enables%20scalable%2C%20low-footprint%2C%20and%20high-speed%20qubit%20readout%2C%20supporting%20the%20development%20of%20larger%20and%20more%20reliable%20quantum%20computing%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.07808v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


