<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content", "author": "Qihao Zhao and Yalun Dai and Hao Li and Wei Hu and Fan Zhang and Jun Liu", "abstract": "  Long-tail recognition is challenging because it requires the model to learn\ngood representations from tail categories and address imbalances across all\ncategories. In this paper, we propose a novel generative and fine-tuning\nframework, LTGC, to handle long-tail recognition via leveraging generated\ncontent. Firstly, inspired by the rich implicit knowledge in large-scale models\n(e.g., large language models, LLMs), LTGC leverages the power of these models\nto parse and reason over the original tail data to produce diverse tail-class\ncontent. We then propose several novel designs for LTGC to ensure the quality\nof the generated data and to efficiently fine-tune the model using both the\ngenerated and original data. The visualization demonstrates the effectiveness\nof the generation module in LTGC, which produces accurate and diverse tail\ndata. Additionally, the experimental results demonstrate that our LTGC\noutperforms existing state-of-the-art methods on popular long-tailed\nbenchmarks.\n", "link": "http://arxiv.org/abs/2403.05854v2", "date": "2024-03-12", "relevancy": 2.9382, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6325}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5662}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5642}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20LTGC%3A%20Long-tail%20Recognition%20via%20Leveraging%20LLMs-driven%20Generated%20Content&body=Title%3A%20LTGC%3A%20Long-tail%20Recognition%20via%20Leveraging%20LLMs-driven%20Generated%20Content%0AAuthor%3A%20Qihao%20Zhao%20and%20Yalun%20Dai%20and%20Hao%20Li%20and%20Wei%20Hu%20and%20Fan%20Zhang%20and%20Jun%20Liu%0AAbstract%3A%20%20%20Long-tail%20recognition%20is%20challenging%20because%20it%20requires%20the%20model%20to%20learn%0Agood%20representations%20from%20tail%20categories%20and%20address%20imbalances%20across%20all%0Acategories.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20generative%20and%20fine-tuning%0Aframework%2C%20LTGC%2C%20to%20handle%20long-tail%20recognition%20via%20leveraging%20generated%0Acontent.%20Firstly%2C%20inspired%20by%20the%20rich%20implicit%20knowledge%20in%20large-scale%20models%0A%28e.g.%2C%20large%20language%20models%2C%20LLMs%29%2C%20LTGC%20leverages%20the%20power%20of%20these%20models%0Ato%20parse%20and%20reason%20over%20the%20original%20tail%20data%20to%20produce%20diverse%20tail-class%0Acontent.%20We%20then%20propose%20several%20novel%20designs%20for%20LTGC%20to%20ensure%20the%20quality%0Aof%20the%20generated%20data%20and%20to%20efficiently%20fine-tune%20the%20model%20using%20both%20the%0Agenerated%20and%20original%20data.%20The%20visualization%20demonstrates%20the%20effectiveness%0Aof%20the%20generation%20module%20in%20LTGC%2C%20which%20produces%20accurate%20and%20diverse%20tail%0Adata.%20Additionally%2C%20the%20experimental%20results%20demonstrate%20that%20our%20LTGC%0Aoutperforms%20existing%20state-of-the-art%20methods%20on%20popular%20long-tailed%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05854v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LTGC%3A%20Long-tail%20Recognition%20via%20Leveraging%20LLMs-driven%20Generated%20Content&entry.906535625=Qihao%20Zhao%20and%20Yalun%20Dai%20and%20Hao%20Li%20and%20Wei%20Hu%20and%20Fan%20Zhang%20and%20Jun%20Liu&entry.1292438233=%20%20Long-tail%20recognition%20is%20challenging%20because%20it%20requires%20the%20model%20to%20learn%0Agood%20representations%20from%20tail%20categories%20and%20address%20imbalances%20across%20all%0Acategories.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20generative%20and%20fine-tuning%0Aframework%2C%20LTGC%2C%20to%20handle%20long-tail%20recognition%20via%20leveraging%20generated%0Acontent.%20Firstly%2C%20inspired%20by%20the%20rich%20implicit%20knowledge%20in%20large-scale%20models%0A%28e.g.%2C%20large%20language%20models%2C%20LLMs%29%2C%20LTGC%20leverages%20the%20power%20of%20these%20models%0Ato%20parse%20and%20reason%20over%20the%20original%20tail%20data%20to%20produce%20diverse%20tail-class%0Acontent.%20We%20then%20propose%20several%20novel%20designs%20for%20LTGC%20to%20ensure%20the%20quality%0Aof%20the%20generated%20data%20and%20to%20efficiently%20fine-tune%20the%20model%20using%20both%20the%0Agenerated%20and%20original%20data.%20The%20visualization%20demonstrates%20the%20effectiveness%0Aof%20the%20generation%20module%20in%20LTGC%2C%20which%20produces%20accurate%20and%20diverse%20tail%0Adata.%20Additionally%2C%20the%20experimental%20results%20demonstrate%20that%20our%20LTGC%0Aoutperforms%20existing%20state-of-the-art%20methods%20on%20popular%20long-tailed%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05854v2&entry.124074799=Read"},
{"title": "Fusing Climate Data Products using a Spatially Varying Autoencoder", "author": "Jacob A. Johnson and Matthew J. Heaton and William F. Christensen and Lynsie R. Warr and Summer B. Rupper", "abstract": "  Autoencoders are powerful machine learning models used to compress\ninformation from multiple data sources. However, autoencoders, like all\nartificial neural networks, are often unidentifiable and uninterpretable. This\nresearch focuses on creating an identifiable and interpretable autoencoder that\ncan be used to meld and combine climate data products. The proposed autoencoder\nutilizes a Bayesian statistical framework, allowing for probabilistic\ninterpretations while also varying spatially to capture useful spatial patterns\nacross the various data products. Constraints are placed on the autoencoder as\nit learns patterns in the data, creating an interpretable consensus that\nincludes the important features from each input. We demonstrate the utility of\nthe autoencoder by combining information from multiple precipitation products\nin High Mountain Asia.\n", "link": "http://arxiv.org/abs/2403.07822v1", "date": "2024-03-12", "relevancy": 2.5193, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.566}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4847}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4609}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Fusing%20Climate%20Data%20Products%20using%20a%20Spatially%20Varying%20Autoencoder&body=Title%3A%20Fusing%20Climate%20Data%20Products%20using%20a%20Spatially%20Varying%20Autoencoder%0AAuthor%3A%20Jacob%20A.%20Johnson%20and%20Matthew%20J.%20Heaton%20and%20William%20F.%20Christensen%20and%20Lynsie%20R.%20Warr%20and%20Summer%20B.%20Rupper%0AAbstract%3A%20%20%20Autoencoders%20are%20powerful%20machine%20learning%20models%20used%20to%20compress%0Ainformation%20from%20multiple%20data%20sources.%20However%2C%20autoencoders%2C%20like%20all%0Aartificial%20neural%20networks%2C%20are%20often%20unidentifiable%20and%20uninterpretable.%20This%0Aresearch%20focuses%20on%20creating%20an%20identifiable%20and%20interpretable%20autoencoder%20that%0Acan%20be%20used%20to%20meld%20and%20combine%20climate%20data%20products.%20The%20proposed%20autoencoder%0Autilizes%20a%20Bayesian%20statistical%20framework%2C%20allowing%20for%20probabilistic%0Ainterpretations%20while%20also%20varying%20spatially%20to%20capture%20useful%20spatial%20patterns%0Aacross%20the%20various%20data%20products.%20Constraints%20are%20placed%20on%20the%20autoencoder%20as%0Ait%20learns%20patterns%20in%20the%20data%2C%20creating%20an%20interpretable%20consensus%20that%0Aincludes%20the%20important%20features%20from%20each%20input.%20We%20demonstrate%20the%20utility%20of%0Athe%20autoencoder%20by%20combining%20information%20from%20multiple%20precipitation%20products%0Ain%20High%20Mountain%20Asia.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07822v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusing%20Climate%20Data%20Products%20using%20a%20Spatially%20Varying%20Autoencoder&entry.906535625=Jacob%20A.%20Johnson%20and%20Matthew%20J.%20Heaton%20and%20William%20F.%20Christensen%20and%20Lynsie%20R.%20Warr%20and%20Summer%20B.%20Rupper&entry.1292438233=%20%20Autoencoders%20are%20powerful%20machine%20learning%20models%20used%20to%20compress%0Ainformation%20from%20multiple%20data%20sources.%20However%2C%20autoencoders%2C%20like%20all%0Aartificial%20neural%20networks%2C%20are%20often%20unidentifiable%20and%20uninterpretable.%20This%0Aresearch%20focuses%20on%20creating%20an%20identifiable%20and%20interpretable%20autoencoder%20that%0Acan%20be%20used%20to%20meld%20and%20combine%20climate%20data%20products.%20The%20proposed%20autoencoder%0Autilizes%20a%20Bayesian%20statistical%20framework%2C%20allowing%20for%20probabilistic%0Ainterpretations%20while%20also%20varying%20spatially%20to%20capture%20useful%20spatial%20patterns%0Aacross%20the%20various%20data%20products.%20Constraints%20are%20placed%20on%20the%20autoencoder%20as%0Ait%20learns%20patterns%20in%20the%20data%2C%20creating%20an%20interpretable%20consensus%20that%0Aincludes%20the%20important%20features%20from%20each%20input.%20We%20demonstrate%20the%20utility%20of%0Athe%20autoencoder%20by%20combining%20information%20from%20multiple%20precipitation%20products%0Ain%20High%20Mountain%20Asia.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07822v1&entry.124074799=Read"},
{"title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM", "author": "Sainbayar Sukhbaatar and Olga Golovneva and Vasu Sharma and Hu Xu and Xi Victoria Lin and Baptiste Rozi\u00e8re and Jacob Kahn and Daniel Li and Wen-tau Yih and Jason Weston and Xian Li", "abstract": "  We investigate efficient methods for training Large Language Models (LLMs) to\npossess capabilities in multiple specialized domains, such as coding, math\nreasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts\nfrom a seed model, which is branched to train experts in embarrassingly\nparallel fashion with high throughput and reduced communication cost. After\nindividual experts are asynchronously trained, BTX brings together their\nfeedforward parameters as experts in Mixture-of-Expert (MoE) layers and\naverages the remaining parameters, followed by an MoE-finetuning stage to learn\ntoken-level routing. BTX generalizes two special cases, the Branch-Train-Merge\nmethod, which does not have the MoE finetuning stage to learn routing, and\nsparse upcycling, which omits the stage of training experts asynchronously.\nCompared to alternative approaches, BTX achieves the best accuracy-efficiency\ntradeoff.\n", "link": "http://arxiv.org/abs/2403.07816v1", "date": "2024-03-12", "relevancy": 2.4555, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5053}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.488}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.48}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Branch-Train-MiX%3A%20Mixing%20Expert%20LLMs%20into%20a%20Mixture-of-Experts%20LLM&body=Title%3A%20Branch-Train-MiX%3A%20Mixing%20Expert%20LLMs%20into%20a%20Mixture-of-Experts%20LLM%0AAuthor%3A%20Sainbayar%20Sukhbaatar%20and%20Olga%20Golovneva%20and%20Vasu%20Sharma%20and%20Hu%20Xu%20and%20Xi%20Victoria%20Lin%20and%20Baptiste%20Rozi%C3%A8re%20and%20Jacob%20Kahn%20and%20Daniel%20Li%20and%20Wen-tau%20Yih%20and%20Jason%20Weston%20and%20Xian%20Li%0AAbstract%3A%20%20%20We%20investigate%20efficient%20methods%20for%20training%20Large%20Language%20Models%20%28LLMs%29%20to%0Apossess%20capabilities%20in%20multiple%20specialized%20domains%2C%20such%20as%20coding%2C%20math%0Areasoning%20and%20world%20knowledge.%20Our%20method%2C%20named%20Branch-Train-MiX%20%28BTX%29%2C%20starts%0Afrom%20a%20seed%20model%2C%20which%20is%20branched%20to%20train%20experts%20in%20embarrassingly%0Aparallel%20fashion%20with%20high%20throughput%20and%20reduced%20communication%20cost.%20After%0Aindividual%20experts%20are%20asynchronously%20trained%2C%20BTX%20brings%20together%20their%0Afeedforward%20parameters%20as%20experts%20in%20Mixture-of-Expert%20%28MoE%29%20layers%20and%0Aaverages%20the%20remaining%20parameters%2C%20followed%20by%20an%20MoE-finetuning%20stage%20to%20learn%0Atoken-level%20routing.%20BTX%20generalizes%20two%20special%20cases%2C%20the%20Branch-Train-Merge%0Amethod%2C%20which%20does%20not%20have%20the%20MoE%20finetuning%20stage%20to%20learn%20routing%2C%20and%0Asparse%20upcycling%2C%20which%20omits%20the%20stage%20of%20training%20experts%20asynchronously.%0ACompared%20to%20alternative%20approaches%2C%20BTX%20achieves%20the%20best%20accuracy-efficiency%0Atradeoff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07816v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Branch-Train-MiX%3A%20Mixing%20Expert%20LLMs%20into%20a%20Mixture-of-Experts%20LLM&entry.906535625=Sainbayar%20Sukhbaatar%20and%20Olga%20Golovneva%20and%20Vasu%20Sharma%20and%20Hu%20Xu%20and%20Xi%20Victoria%20Lin%20and%20Baptiste%20Rozi%C3%A8re%20and%20Jacob%20Kahn%20and%20Daniel%20Li%20and%20Wen-tau%20Yih%20and%20Jason%20Weston%20and%20Xian%20Li&entry.1292438233=%20%20We%20investigate%20efficient%20methods%20for%20training%20Large%20Language%20Models%20%28LLMs%29%20to%0Apossess%20capabilities%20in%20multiple%20specialized%20domains%2C%20such%20as%20coding%2C%20math%0Areasoning%20and%20world%20knowledge.%20Our%20method%2C%20named%20Branch-Train-MiX%20%28BTX%29%2C%20starts%0Afrom%20a%20seed%20model%2C%20which%20is%20branched%20to%20train%20experts%20in%20embarrassingly%0Aparallel%20fashion%20with%20high%20throughput%20and%20reduced%20communication%20cost.%20After%0Aindividual%20experts%20are%20asynchronously%20trained%2C%20BTX%20brings%20together%20their%0Afeedforward%20parameters%20as%20experts%20in%20Mixture-of-Expert%20%28MoE%29%20layers%20and%0Aaverages%20the%20remaining%20parameters%2C%20followed%20by%20an%20MoE-finetuning%20stage%20to%20learn%0Atoken-level%20routing.%20BTX%20generalizes%20two%20special%20cases%2C%20the%20Branch-Train-Merge%0Amethod%2C%20which%20does%20not%20have%20the%20MoE%20finetuning%20stage%20to%20learn%20routing%2C%20and%0Asparse%20upcycling%2C%20which%20omits%20the%20stage%20of%20training%20experts%20asynchronously.%0ACompared%20to%20alternative%20approaches%2C%20BTX%20achieves%20the%20best%20accuracy-efficiency%0Atradeoff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07816v1&entry.124074799=Read"},
{"title": "Boosting keyword spotting through on-device learnable user speech\n  characteristics", "author": "Cristian Cioflan and Lukas Cavigelli and Luca Benini", "abstract": "  Keyword spotting systems for always-on TinyML-constrained applications\nrequire on-site tuning to boost the accuracy of offline trained classifiers\nwhen deployed in unseen inference conditions. Adapting to the speech\npeculiarities of target users requires many in-domain samples, often\nunavailable in real-world scenarios. Furthermore, current on-device learning\ntechniques rely on computationally intensive and memory-hungry backbone update\nschemes, unfit for always-on, battery-powered devices. In this work, we propose\na novel on-device learning architecture, composed of a pretrained backbone and\na user-aware embedding learning the user's speech characteristics. The\nso-generated features are fused and used to classify the input utterance. For\ndomain shifts generated by unseen speakers, we measure error rate reductions of\nup to 19% from 30.1% to 24.3% based on the 35-class problem of the Google\nSpeech Commands dataset, through the inexpensive update of the user\nprojections. We moreover demonstrate the few-shot learning capabilities of our\nproposed architecture in sample- and class-scarce learning conditions. With\n23.7 kparameters and 1 MFLOP per epoch required for on-device training, our\nsystem is feasible for TinyML applications aimed at battery-powered\nmicrocontrollers.\n", "link": "http://arxiv.org/abs/2403.07802v1", "date": "2024-03-12", "relevancy": 2.3789, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4831}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4814}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4628}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Boosting%20keyword%20spotting%20through%20on-device%20learnable%20user%20speech%0A%20%20characteristics&body=Title%3A%20Boosting%20keyword%20spotting%20through%20on-device%20learnable%20user%20speech%0A%20%20characteristics%0AAuthor%3A%20Cristian%20Cioflan%20and%20Lukas%20Cavigelli%20and%20Luca%20Benini%0AAbstract%3A%20%20%20Keyword%20spotting%20systems%20for%20always-on%20TinyML-constrained%20applications%0Arequire%20on-site%20tuning%20to%20boost%20the%20accuracy%20of%20offline%20trained%20classifiers%0Awhen%20deployed%20in%20unseen%20inference%20conditions.%20Adapting%20to%20the%20speech%0Apeculiarities%20of%20target%20users%20requires%20many%20in-domain%20samples%2C%20often%0Aunavailable%20in%20real-world%20scenarios.%20Furthermore%2C%20current%20on-device%20learning%0Atechniques%20rely%20on%20computationally%20intensive%20and%20memory-hungry%20backbone%20update%0Aschemes%2C%20unfit%20for%20always-on%2C%20battery-powered%20devices.%20In%20this%20work%2C%20we%20propose%0Aa%20novel%20on-device%20learning%20architecture%2C%20composed%20of%20a%20pretrained%20backbone%20and%0Aa%20user-aware%20embedding%20learning%20the%20user%27s%20speech%20characteristics.%20The%0Aso-generated%20features%20are%20fused%20and%20used%20to%20classify%20the%20input%20utterance.%20For%0Adomain%20shifts%20generated%20by%20unseen%20speakers%2C%20we%20measure%20error%20rate%20reductions%20of%0Aup%20to%2019%25%20from%2030.1%25%20to%2024.3%25%20based%20on%20the%2035-class%20problem%20of%20the%20Google%0ASpeech%20Commands%20dataset%2C%20through%20the%20inexpensive%20update%20of%20the%20user%0Aprojections.%20We%20moreover%20demonstrate%20the%20few-shot%20learning%20capabilities%20of%20our%0Aproposed%20architecture%20in%20sample-%20and%20class-scarce%20learning%20conditions.%20With%0A23.7%20kparameters%20and%201%20MFLOP%20per%20epoch%20required%20for%20on-device%20training%2C%20our%0Asystem%20is%20feasible%20for%20TinyML%20applications%20aimed%20at%20battery-powered%0Amicrocontrollers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07802v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20keyword%20spotting%20through%20on-device%20learnable%20user%20speech%0A%20%20characteristics&entry.906535625=Cristian%20Cioflan%20and%20Lukas%20Cavigelli%20and%20Luca%20Benini&entry.1292438233=%20%20Keyword%20spotting%20systems%20for%20always-on%20TinyML-constrained%20applications%0Arequire%20on-site%20tuning%20to%20boost%20the%20accuracy%20of%20offline%20trained%20classifiers%0Awhen%20deployed%20in%20unseen%20inference%20conditions.%20Adapting%20to%20the%20speech%0Apeculiarities%20of%20target%20users%20requires%20many%20in-domain%20samples%2C%20often%0Aunavailable%20in%20real-world%20scenarios.%20Furthermore%2C%20current%20on-device%20learning%0Atechniques%20rely%20on%20computationally%20intensive%20and%20memory-hungry%20backbone%20update%0Aschemes%2C%20unfit%20for%20always-on%2C%20battery-powered%20devices.%20In%20this%20work%2C%20we%20propose%0Aa%20novel%20on-device%20learning%20architecture%2C%20composed%20of%20a%20pretrained%20backbone%20and%0Aa%20user-aware%20embedding%20learning%20the%20user%27s%20speech%20characteristics.%20The%0Aso-generated%20features%20are%20fused%20and%20used%20to%20classify%20the%20input%20utterance.%20For%0Adomain%20shifts%20generated%20by%20unseen%20speakers%2C%20we%20measure%20error%20rate%20reductions%20of%0Aup%20to%2019%25%20from%2030.1%25%20to%2024.3%25%20based%20on%20the%2035-class%20problem%20of%20the%20Google%0ASpeech%20Commands%20dataset%2C%20through%20the%20inexpensive%20update%20of%20the%20user%0Aprojections.%20We%20moreover%20demonstrate%20the%20few-shot%20learning%20capabilities%20of%20our%0Aproposed%20architecture%20in%20sample-%20and%20class-scarce%20learning%20conditions.%20With%0A23.7%20kparameters%20and%201%20MFLOP%20per%20epoch%20required%20for%20on-device%20training%2C%20our%0Asystem%20is%20feasible%20for%20TinyML%20applications%20aimed%20at%20battery-powered%0Amicrocontrollers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07802v1&entry.124074799=Read"},
{"title": "12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning", "author": "Yoga Esa Wibowo and Cristian Cioflan and Thorir Mar Ingolfsson and Michael Hersche and Leo Zhao and Abbas Rahimi and Luca Benini", "abstract": "  Few-Shot Class-Incremental Learning (FSCIL) enables machine learning systems\nto expand their inference capabilities to new classes using only a few labeled\nexamples, without forgetting the previously learned classes. Classical\nbackpropagation-based learning and its variants are often unsuitable for\nbattery-powered, memory-constrained systems at the extreme edge. In this work,\nwe introduce Online Few-Shot Class-Incremental Learning (O-FSCIL), based on a\nlightweight model consisting of a pretrained and metalearned feature extractor\nand an expandable explicit memory storing the class prototypes. The\narchitecture is pretrained with a novel feature orthogonality regularization\nand metalearned with a multi-margin loss. For learning a new class, our\napproach extends the explicit memory with novel class prototypes, while the\nremaining architecture is kept frozen. This allows learning previously unseen\nclasses based on only a few examples with one single pass (hence online).\nO-FSCIL obtains an average accuracy of 68.62% on the FSCIL CIFAR100 benchmark,\nachieving state-of-the-art results. Tailored for ultra-low-power platforms, we\nimplement O-FSCIL on the 60 mW GAP9 microcontroller, demonstrating online\nlearning capabilities within just 12 mJ per new class.\n", "link": "http://arxiv.org/abs/2403.07851v1", "date": "2024-03-12", "relevancy": 2.3194, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4689}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4642}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4585}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%2012%20mJ%20per%20Class%20On-Device%20Online%20Few-Shot%20Class-Incremental%20Learning&body=Title%3A%2012%20mJ%20per%20Class%20On-Device%20Online%20Few-Shot%20Class-Incremental%20Learning%0AAuthor%3A%20Yoga%20Esa%20Wibowo%20and%20Cristian%20Cioflan%20and%20Thorir%20Mar%20Ingolfsson%20and%20Michael%20Hersche%20and%20Leo%20Zhao%20and%20Abbas%20Rahimi%20and%20Luca%20Benini%0AAbstract%3A%20%20%20Few-Shot%20Class-Incremental%20Learning%20%28FSCIL%29%20enables%20machine%20learning%20systems%0Ato%20expand%20their%20inference%20capabilities%20to%20new%20classes%20using%20only%20a%20few%20labeled%0Aexamples%2C%20without%20forgetting%20the%20previously%20learned%20classes.%20Classical%0Abackpropagation-based%20learning%20and%20its%20variants%20are%20often%20unsuitable%20for%0Abattery-powered%2C%20memory-constrained%20systems%20at%20the%20extreme%20edge.%20In%20this%20work%2C%0Awe%20introduce%20Online%20Few-Shot%20Class-Incremental%20Learning%20%28O-FSCIL%29%2C%20based%20on%20a%0Alightweight%20model%20consisting%20of%20a%20pretrained%20and%20metalearned%20feature%20extractor%0Aand%20an%20expandable%20explicit%20memory%20storing%20the%20class%20prototypes.%20The%0Aarchitecture%20is%20pretrained%20with%20a%20novel%20feature%20orthogonality%20regularization%0Aand%20metalearned%20with%20a%20multi-margin%20loss.%20For%20learning%20a%20new%20class%2C%20our%0Aapproach%20extends%20the%20explicit%20memory%20with%20novel%20class%20prototypes%2C%20while%20the%0Aremaining%20architecture%20is%20kept%20frozen.%20This%20allows%20learning%20previously%20unseen%0Aclasses%20based%20on%20only%20a%20few%20examples%20with%20one%20single%20pass%20%28hence%20online%29.%0AO-FSCIL%20obtains%20an%20average%20accuracy%20of%2068.62%25%20on%20the%20FSCIL%20CIFAR100%20benchmark%2C%0Aachieving%20state-of-the-art%20results.%20Tailored%20for%20ultra-low-power%20platforms%2C%20we%0Aimplement%20O-FSCIL%20on%20the%2060%20mW%20GAP9%20microcontroller%2C%20demonstrating%20online%0Alearning%20capabilities%20within%20just%2012%20mJ%20per%20new%20class.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07851v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=12%20mJ%20per%20Class%20On-Device%20Online%20Few-Shot%20Class-Incremental%20Learning&entry.906535625=Yoga%20Esa%20Wibowo%20and%20Cristian%20Cioflan%20and%20Thorir%20Mar%20Ingolfsson%20and%20Michael%20Hersche%20and%20Leo%20Zhao%20and%20Abbas%20Rahimi%20and%20Luca%20Benini&entry.1292438233=%20%20Few-Shot%20Class-Incremental%20Learning%20%28FSCIL%29%20enables%20machine%20learning%20systems%0Ato%20expand%20their%20inference%20capabilities%20to%20new%20classes%20using%20only%20a%20few%20labeled%0Aexamples%2C%20without%20forgetting%20the%20previously%20learned%20classes.%20Classical%0Abackpropagation-based%20learning%20and%20its%20variants%20are%20often%20unsuitable%20for%0Abattery-powered%2C%20memory-constrained%20systems%20at%20the%20extreme%20edge.%20In%20this%20work%2C%0Awe%20introduce%20Online%20Few-Shot%20Class-Incremental%20Learning%20%28O-FSCIL%29%2C%20based%20on%20a%0Alightweight%20model%20consisting%20of%20a%20pretrained%20and%20metalearned%20feature%20extractor%0Aand%20an%20expandable%20explicit%20memory%20storing%20the%20class%20prototypes.%20The%0Aarchitecture%20is%20pretrained%20with%20a%20novel%20feature%20orthogonality%20regularization%0Aand%20metalearned%20with%20a%20multi-margin%20loss.%20For%20learning%20a%20new%20class%2C%20our%0Aapproach%20extends%20the%20explicit%20memory%20with%20novel%20class%20prototypes%2C%20while%20the%0Aremaining%20architecture%20is%20kept%20frozen.%20This%20allows%20learning%20previously%20unseen%0Aclasses%20based%20on%20only%20a%20few%20examples%20with%20one%20single%20pass%20%28hence%20online%29.%0AO-FSCIL%20obtains%20an%20average%20accuracy%20of%2068.62%25%20on%20the%20FSCIL%20CIFAR100%20benchmark%2C%0Aachieving%20state-of-the-art%20results.%20Tailored%20for%20ultra-low-power%20platforms%2C%20we%0Aimplement%20O-FSCIL%20on%20the%2060%20mW%20GAP9%20microcontroller%2C%20demonstrating%20online%0Alearning%20capabilities%20within%20just%2012%20mJ%20per%20new%20class.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07851v1&entry.124074799=Read"},
{"title": "OPEN TEACH: A Versatile Teleoperation System for Robotic Manipulation", "author": "Aadhithya Iyer and Zhuoran Peng and Yinlong Dai and Irmak Guzey and Siddhant Haldar and Soumith Chintala and Lerrel Pinto", "abstract": "  Open-sourced, user-friendly tools form the bedrock of scientific advancement\nacross disciplines. The widespread adoption of data-driven learning has led to\nremarkable progress in multi-fingered dexterity, bimanual manipulation, and\napplications ranging from logistics to home robotics. However, existing data\ncollection platforms are often proprietary, costly, or tailored to specific\nrobotic morphologies. We present OPEN TEACH, a new teleoperation system\nleveraging VR headsets to immerse users in mixed reality for intuitive robot\ncontrol. Built on the affordable Meta Quest 3, which costs $500, OPEN TEACH\nenables real-time control of various robots, including multi-fingered hands and\nbimanual arms, through an easy-to-use app. Using natural hand gestures and\nmovements, users can manipulate robots at up to 90Hz with smooth visual\nfeedback and interface widgets offering closeup environment views. We\ndemonstrate the versatility of OPEN TEACH across 38 tasks on different robots.\nA comprehensive user study indicates significant improvement in teleoperation\ncapability over the AnyTeleop framework. Further experiments exhibit that the\ncollected data is compatible with policy learning on 10 dexterous and\ncontact-rich manipulation tasks. Currently supporting Franka, xArm, Jaco, and\nAllegro platforms, OPEN TEACH is fully open-sourced to promote broader\nadoption. Videos are available at https://open-teach.github.io/.\n", "link": "http://arxiv.org/abs/2403.07870v1", "date": "2024-03-12", "relevancy": 2.2544, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5908}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5879}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5284}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20OPEN%20TEACH%3A%20A%20Versatile%20Teleoperation%20System%20for%20Robotic%20Manipulation&body=Title%3A%20OPEN%20TEACH%3A%20A%20Versatile%20Teleoperation%20System%20for%20Robotic%20Manipulation%0AAuthor%3A%20Aadhithya%20Iyer%20and%20Zhuoran%20Peng%20and%20Yinlong%20Dai%20and%20Irmak%20Guzey%20and%20Siddhant%20Haldar%20and%20Soumith%20Chintala%20and%20Lerrel%20Pinto%0AAbstract%3A%20%20%20Open-sourced%2C%20user-friendly%20tools%20form%20the%20bedrock%20of%20scientific%20advancement%0Aacross%20disciplines.%20The%20widespread%20adoption%20of%20data-driven%20learning%20has%20led%20to%0Aremarkable%20progress%20in%20multi-fingered%20dexterity%2C%20bimanual%20manipulation%2C%20and%0Aapplications%20ranging%20from%20logistics%20to%20home%20robotics.%20However%2C%20existing%20data%0Acollection%20platforms%20are%20often%20proprietary%2C%20costly%2C%20or%20tailored%20to%20specific%0Arobotic%20morphologies.%20We%20present%20OPEN%20TEACH%2C%20a%20new%20teleoperation%20system%0Aleveraging%20VR%20headsets%20to%20immerse%20users%20in%20mixed%20reality%20for%20intuitive%20robot%0Acontrol.%20Built%20on%20the%20affordable%20Meta%20Quest%203%2C%20which%20costs%20%24500%2C%20OPEN%20TEACH%0Aenables%20real-time%20control%20of%20various%20robots%2C%20including%20multi-fingered%20hands%20and%0Abimanual%20arms%2C%20through%20an%20easy-to-use%20app.%20Using%20natural%20hand%20gestures%20and%0Amovements%2C%20users%20can%20manipulate%20robots%20at%20up%20to%2090Hz%20with%20smooth%20visual%0Afeedback%20and%20interface%20widgets%20offering%20closeup%20environment%20views.%20We%0Ademonstrate%20the%20versatility%20of%20OPEN%20TEACH%20across%2038%20tasks%20on%20different%20robots.%0AA%20comprehensive%20user%20study%20indicates%20significant%20improvement%20in%20teleoperation%0Acapability%20over%20the%20AnyTeleop%20framework.%20Further%20experiments%20exhibit%20that%20the%0Acollected%20data%20is%20compatible%20with%20policy%20learning%20on%2010%20dexterous%20and%0Acontact-rich%20manipulation%20tasks.%20Currently%20supporting%20Franka%2C%20xArm%2C%20Jaco%2C%20and%0AAllegro%20platforms%2C%20OPEN%20TEACH%20is%20fully%20open-sourced%20to%20promote%20broader%0Aadoption.%20Videos%20are%20available%20at%20https%3A//open-teach.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07870v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OPEN%20TEACH%3A%20A%20Versatile%20Teleoperation%20System%20for%20Robotic%20Manipulation&entry.906535625=Aadhithya%20Iyer%20and%20Zhuoran%20Peng%20and%20Yinlong%20Dai%20and%20Irmak%20Guzey%20and%20Siddhant%20Haldar%20and%20Soumith%20Chintala%20and%20Lerrel%20Pinto&entry.1292438233=%20%20Open-sourced%2C%20user-friendly%20tools%20form%20the%20bedrock%20of%20scientific%20advancement%0Aacross%20disciplines.%20The%20widespread%20adoption%20of%20data-driven%20learning%20has%20led%20to%0Aremarkable%20progress%20in%20multi-fingered%20dexterity%2C%20bimanual%20manipulation%2C%20and%0Aapplications%20ranging%20from%20logistics%20to%20home%20robotics.%20However%2C%20existing%20data%0Acollection%20platforms%20are%20often%20proprietary%2C%20costly%2C%20or%20tailored%20to%20specific%0Arobotic%20morphologies.%20We%20present%20OPEN%20TEACH%2C%20a%20new%20teleoperation%20system%0Aleveraging%20VR%20headsets%20to%20immerse%20users%20in%20mixed%20reality%20for%20intuitive%20robot%0Acontrol.%20Built%20on%20the%20affordable%20Meta%20Quest%203%2C%20which%20costs%20%24500%2C%20OPEN%20TEACH%0Aenables%20real-time%20control%20of%20various%20robots%2C%20including%20multi-fingered%20hands%20and%0Abimanual%20arms%2C%20through%20an%20easy-to-use%20app.%20Using%20natural%20hand%20gestures%20and%0Amovements%2C%20users%20can%20manipulate%20robots%20at%20up%20to%2090Hz%20with%20smooth%20visual%0Afeedback%20and%20interface%20widgets%20offering%20closeup%20environment%20views.%20We%0Ademonstrate%20the%20versatility%20of%20OPEN%20TEACH%20across%2038%20tasks%20on%20different%20robots.%0AA%20comprehensive%20user%20study%20indicates%20significant%20improvement%20in%20teleoperation%0Acapability%20over%20the%20AnyTeleop%20framework.%20Further%20experiments%20exhibit%20that%20the%0Acollected%20data%20is%20compatible%20with%20policy%20learning%20on%2010%20dexterous%20and%0Acontact-rich%20manipulation%20tasks.%20Currently%20supporting%20Franka%2C%20xArm%2C%20Jaco%2C%20and%0AAllegro%20platforms%2C%20OPEN%20TEACH%20is%20fully%20open-sourced%20to%20promote%20broader%0Aadoption.%20Videos%20are%20available%20at%20https%3A//open-teach.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07870v1&entry.124074799=Read"},
{"title": "Adversarial Distortion Learning for Medical Image Denoising", "author": "Morteza Ghahremani and Mohammad Khateri and Alejandra Sierra and Jussi Tohka", "abstract": "  We present a novel adversarial distortion learning (ADL) for denoising two-\nand three-dimensional (2D/3D) biomedical image data. The proposed ADL consists\nof two auto-encoders: a denoiser and a discriminator. The denoiser removes\nnoise from input data and the discriminator compares the denoised result to its\nnoise-free counterpart. This process is repeated until the discriminator cannot\ndifferentiate the denoised data from the reference. Both the denoiser and the\ndiscriminator are built upon a proposed auto-encoder called Efficient-Unet.\nEfficient-Unet has a light architecture that uses the residual blocks and a\nnovel pyramidal approach in the backbone to efficiently extract and re-use\nfeature maps. During training, the textural information and contrast are\ncontrolled by two novel loss functions. The architecture of Efficient-Unet\nallows generalizing the proposed method to any sort of biomedical data. The 2D\nversion of our network was trained on ImageNet and tested on biomedical\ndatasets whose distribution is completely different from ImageNet; so, there is\nno need for re-training. Experimental results carried out on magnetic resonance\nimaging (MRI), dermatoscopy, electron microscopy and X-ray datasets show that\nthe proposed method achieved the best on each benchmark. Our implementation and\npre-trained models are available at https://github.com/mogvision/ADL.\n", "link": "http://arxiv.org/abs/2204.14100v2", "date": "2024-03-12", "relevancy": 2.1965, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5779}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5505}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5362}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Distortion%20Learning%20for%20Medical%20Image%20Denoising&body=Title%3A%20Adversarial%20Distortion%20Learning%20for%20Medical%20Image%20Denoising%0AAuthor%3A%20Morteza%20Ghahremani%20and%20Mohammad%20Khateri%20and%20Alejandra%20Sierra%20and%20Jussi%20Tohka%0AAbstract%3A%20%20%20We%20present%20a%20novel%20adversarial%20distortion%20learning%20%28ADL%29%20for%20denoising%20two-%0Aand%20three-dimensional%20%282D/3D%29%20biomedical%20image%20data.%20The%20proposed%20ADL%20consists%0Aof%20two%20auto-encoders%3A%20a%20denoiser%20and%20a%20discriminator.%20The%20denoiser%20removes%0Anoise%20from%20input%20data%20and%20the%20discriminator%20compares%20the%20denoised%20result%20to%20its%0Anoise-free%20counterpart.%20This%20process%20is%20repeated%20until%20the%20discriminator%20cannot%0Adifferentiate%20the%20denoised%20data%20from%20the%20reference.%20Both%20the%20denoiser%20and%20the%0Adiscriminator%20are%20built%20upon%20a%20proposed%20auto-encoder%20called%20Efficient-Unet.%0AEfficient-Unet%20has%20a%20light%20architecture%20that%20uses%20the%20residual%20blocks%20and%20a%0Anovel%20pyramidal%20approach%20in%20the%20backbone%20to%20efficiently%20extract%20and%20re-use%0Afeature%20maps.%20During%20training%2C%20the%20textural%20information%20and%20contrast%20are%0Acontrolled%20by%20two%20novel%20loss%20functions.%20The%20architecture%20of%20Efficient-Unet%0Aallows%20generalizing%20the%20proposed%20method%20to%20any%20sort%20of%20biomedical%20data.%20The%202D%0Aversion%20of%20our%20network%20was%20trained%20on%20ImageNet%20and%20tested%20on%20biomedical%0Adatasets%20whose%20distribution%20is%20completely%20different%20from%20ImageNet%3B%20so%2C%20there%20is%0Ano%20need%20for%20re-training.%20Experimental%20results%20carried%20out%20on%20magnetic%20resonance%0Aimaging%20%28MRI%29%2C%20dermatoscopy%2C%20electron%20microscopy%20and%20X-ray%20datasets%20show%20that%0Athe%20proposed%20method%20achieved%20the%20best%20on%20each%20benchmark.%20Our%20implementation%20and%0Apre-trained%20models%20are%20available%20at%20https%3A//github.com/mogvision/ADL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2204.14100v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Distortion%20Learning%20for%20Medical%20Image%20Denoising&entry.906535625=Morteza%20Ghahremani%20and%20Mohammad%20Khateri%20and%20Alejandra%20Sierra%20and%20Jussi%20Tohka&entry.1292438233=%20%20We%20present%20a%20novel%20adversarial%20distortion%20learning%20%28ADL%29%20for%20denoising%20two-%0Aand%20three-dimensional%20%282D/3D%29%20biomedical%20image%20data.%20The%20proposed%20ADL%20consists%0Aof%20two%20auto-encoders%3A%20a%20denoiser%20and%20a%20discriminator.%20The%20denoiser%20removes%0Anoise%20from%20input%20data%20and%20the%20discriminator%20compares%20the%20denoised%20result%20to%20its%0Anoise-free%20counterpart.%20This%20process%20is%20repeated%20until%20the%20discriminator%20cannot%0Adifferentiate%20the%20denoised%20data%20from%20the%20reference.%20Both%20the%20denoiser%20and%20the%0Adiscriminator%20are%20built%20upon%20a%20proposed%20auto-encoder%20called%20Efficient-Unet.%0AEfficient-Unet%20has%20a%20light%20architecture%20that%20uses%20the%20residual%20blocks%20and%20a%0Anovel%20pyramidal%20approach%20in%20the%20backbone%20to%20efficiently%20extract%20and%20re-use%0Afeature%20maps.%20During%20training%2C%20the%20textural%20information%20and%20contrast%20are%0Acontrolled%20by%20two%20novel%20loss%20functions.%20The%20architecture%20of%20Efficient-Unet%0Aallows%20generalizing%20the%20proposed%20method%20to%20any%20sort%20of%20biomedical%20data.%20The%202D%0Aversion%20of%20our%20network%20was%20trained%20on%20ImageNet%20and%20tested%20on%20biomedical%0Adatasets%20whose%20distribution%20is%20completely%20different%20from%20ImageNet%3B%20so%2C%20there%20is%0Ano%20need%20for%20re-training.%20Experimental%20results%20carried%20out%20on%20magnetic%20resonance%0Aimaging%20%28MRI%29%2C%20dermatoscopy%2C%20electron%20microscopy%20and%20X-ray%20datasets%20show%20that%0Athe%20proposed%20method%20achieved%20the%20best%20on%20each%20benchmark.%20Our%20implementation%20and%0Apre-trained%20models%20are%20available%20at%20https%3A//github.com/mogvision/ADL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2204.14100v2&entry.124074799=Read"},
{"title": "Beyond Text: Frozen Large Language Models in Visual Signal Comprehension", "author": "Lei Zhu and Fangyun Wei and Yanye Lu", "abstract": "  In this work, we investigate the potential of a large language model (LLM) to\ndirectly comprehend visual signals without the necessity of fine-tuning on\nmulti-modal datasets. The foundational concept of our method views an image as\na linguistic entity, and translates it to a set of discrete words derived from\nthe LLM's vocabulary. To achieve this, we present the Vision-to-Language\nTokenizer, abbreviated as V2T Tokenizer, which transforms an image into a\n``foreign language'' with the combined aid of an encoder-decoder, the LLM\nvocabulary, and a CLIP model. With this innovative image encoding, the LLM\ngains the ability not only for visual comprehension but also for image\ndenoising and restoration in an auto-regressive fashion-crucially, without any\nfine-tuning. We undertake rigorous experiments to validate our method,\nencompassing understanding tasks like image recognition, image captioning, and\nvisual question answering, as well as image denoising tasks like inpainting,\noutpainting, deblurring, and shift restoration. Code and models are available\nat https://github.com/zh460045050/V2L-Tokenizer.\n", "link": "http://arxiv.org/abs/2403.07874v1", "date": "2024-03-12", "relevancy": 2.1796, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5609}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5512}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5264}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Beyond%20Text%3A%20Frozen%20Large%20Language%20Models%20in%20Visual%20Signal%20Comprehension&body=Title%3A%20Beyond%20Text%3A%20Frozen%20Large%20Language%20Models%20in%20Visual%20Signal%20Comprehension%0AAuthor%3A%20Lei%20Zhu%20and%20Fangyun%20Wei%20and%20Yanye%20Lu%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20investigate%20the%20potential%20of%20a%20large%20language%20model%20%28LLM%29%20to%0Adirectly%20comprehend%20visual%20signals%20without%20the%20necessity%20of%20fine-tuning%20on%0Amulti-modal%20datasets.%20The%20foundational%20concept%20of%20our%20method%20views%20an%20image%20as%0Aa%20linguistic%20entity%2C%20and%20translates%20it%20to%20a%20set%20of%20discrete%20words%20derived%20from%0Athe%20LLM%27s%20vocabulary.%20To%20achieve%20this%2C%20we%20present%20the%20Vision-to-Language%0ATokenizer%2C%20abbreviated%20as%20V2T%20Tokenizer%2C%20which%20transforms%20an%20image%20into%20a%0A%60%60foreign%20language%27%27%20with%20the%20combined%20aid%20of%20an%20encoder-decoder%2C%20the%20LLM%0Avocabulary%2C%20and%20a%20CLIP%20model.%20With%20this%20innovative%20image%20encoding%2C%20the%20LLM%0Agains%20the%20ability%20not%20only%20for%20visual%20comprehension%20but%20also%20for%20image%0Adenoising%20and%20restoration%20in%20an%20auto-regressive%20fashion-crucially%2C%20without%20any%0Afine-tuning.%20We%20undertake%20rigorous%20experiments%20to%20validate%20our%20method%2C%0Aencompassing%20understanding%20tasks%20like%20image%20recognition%2C%20image%20captioning%2C%20and%0Avisual%20question%20answering%2C%20as%20well%20as%20image%20denoising%20tasks%20like%20inpainting%2C%0Aoutpainting%2C%20deblurring%2C%20and%20shift%20restoration.%20Code%20and%20models%20are%20available%0Aat%20https%3A//github.com/zh460045050/V2L-Tokenizer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07874v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Text%3A%20Frozen%20Large%20Language%20Models%20in%20Visual%20Signal%20Comprehension&entry.906535625=Lei%20Zhu%20and%20Fangyun%20Wei%20and%20Yanye%20Lu&entry.1292438233=%20%20In%20this%20work%2C%20we%20investigate%20the%20potential%20of%20a%20large%20language%20model%20%28LLM%29%20to%0Adirectly%20comprehend%20visual%20signals%20without%20the%20necessity%20of%20fine-tuning%20on%0Amulti-modal%20datasets.%20The%20foundational%20concept%20of%20our%20method%20views%20an%20image%20as%0Aa%20linguistic%20entity%2C%20and%20translates%20it%20to%20a%20set%20of%20discrete%20words%20derived%20from%0Athe%20LLM%27s%20vocabulary.%20To%20achieve%20this%2C%20we%20present%20the%20Vision-to-Language%0ATokenizer%2C%20abbreviated%20as%20V2T%20Tokenizer%2C%20which%20transforms%20an%20image%20into%20a%0A%60%60foreign%20language%27%27%20with%20the%20combined%20aid%20of%20an%20encoder-decoder%2C%20the%20LLM%0Avocabulary%2C%20and%20a%20CLIP%20model.%20With%20this%20innovative%20image%20encoding%2C%20the%20LLM%0Agains%20the%20ability%20not%20only%20for%20visual%20comprehension%20but%20also%20for%20image%0Adenoising%20and%20restoration%20in%20an%20auto-regressive%20fashion-crucially%2C%20without%20any%0Afine-tuning.%20We%20undertake%20rigorous%20experiments%20to%20validate%20our%20method%2C%0Aencompassing%20understanding%20tasks%20like%20image%20recognition%2C%20image%20captioning%2C%20and%0Avisual%20question%20answering%2C%20as%20well%20as%20image%20denoising%20tasks%20like%20inpainting%2C%0Aoutpainting%2C%20deblurring%2C%20and%20shift%20restoration.%20Code%20and%20models%20are%20available%0Aat%20https%3A//github.com/zh460045050/V2L-Tokenizer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07874v1&entry.124074799=Read"},
{"title": "Rotation-Agnostic Image Representation Learning for Digital Pathology", "author": "Saghir Alfasly and Abubakr Shafique and Peyman Nejat and Jibran Khan and Areej Alsaafin and Ghazal Alabtah and H. R. Tizhoosh", "abstract": "  This paper addresses complex challenges in histopathological image analysis\nthrough three key contributions. Firstly, it introduces a fast patch selection\nmethod, FPS, for whole-slide image (WSI) analysis, significantly reducing\ncomputational cost while maintaining accuracy. Secondly, it presents PathDino,\na lightweight histopathology feature extractor with a minimal configuration of\nfive Transformer blocks and only 9 million parameters, markedly fewer than\nalternatives. Thirdly, it introduces a rotation-agnostic representation\nlearning paradigm using self-supervised learning, effectively mitigating\noverfitting. We also show that our compact model outperforms existing\nstate-of-the-art histopathology-specific vision transformers on 12 diverse\ndatasets, including both internal datasets spanning four sites (breast, liver,\nskin, and colorectal) and seven public datasets (PANDA, CAMELYON16, BRACS,\nDigestPath, Kather, PanNuke, and WSSS4LUAD). Notably, even with a training\ndataset of 6 million histopathology patches from The Cancer Genome Atlas\n(TCGA), our approach demonstrates an average 8.5% improvement in patch-level\nmajority vote performance. These contributions provide a robust framework for\nenhancing image analysis in digital pathology, rigorously validated through\nextensive evaluation. Project Page:\nhttps://kimialabmayo.github.io/PathDino-Page/\n", "link": "http://arxiv.org/abs/2311.08359v2", "date": "2024-03-12", "relevancy": 2.1703, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5494}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5409}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5298}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Rotation-Agnostic%20Image%20Representation%20Learning%20for%20Digital%20Pathology&body=Title%3A%20Rotation-Agnostic%20Image%20Representation%20Learning%20for%20Digital%20Pathology%0AAuthor%3A%20Saghir%20Alfasly%20and%20Abubakr%20Shafique%20and%20Peyman%20Nejat%20and%20Jibran%20Khan%20and%20Areej%20Alsaafin%20and%20Ghazal%20Alabtah%20and%20H.%20R.%20Tizhoosh%0AAbstract%3A%20%20%20This%20paper%20addresses%20complex%20challenges%20in%20histopathological%20image%20analysis%0Athrough%20three%20key%20contributions.%20Firstly%2C%20it%20introduces%20a%20fast%20patch%20selection%0Amethod%2C%20FPS%2C%20for%20whole-slide%20image%20%28WSI%29%20analysis%2C%20significantly%20reducing%0Acomputational%20cost%20while%20maintaining%20accuracy.%20Secondly%2C%20it%20presents%20PathDino%2C%0Aa%20lightweight%20histopathology%20feature%20extractor%20with%20a%20minimal%20configuration%20of%0Afive%20Transformer%20blocks%20and%20only%209%20million%20parameters%2C%20markedly%20fewer%20than%0Aalternatives.%20Thirdly%2C%20it%20introduces%20a%20rotation-agnostic%20representation%0Alearning%20paradigm%20using%20self-supervised%20learning%2C%20effectively%20mitigating%0Aoverfitting.%20We%20also%20show%20that%20our%20compact%20model%20outperforms%20existing%0Astate-of-the-art%20histopathology-specific%20vision%20transformers%20on%2012%20diverse%0Adatasets%2C%20including%20both%20internal%20datasets%20spanning%20four%20sites%20%28breast%2C%20liver%2C%0Askin%2C%20and%20colorectal%29%20and%20seven%20public%20datasets%20%28PANDA%2C%20CAMELYON16%2C%20BRACS%2C%0ADigestPath%2C%20Kather%2C%20PanNuke%2C%20and%20WSSS4LUAD%29.%20Notably%2C%20even%20with%20a%20training%0Adataset%20of%206%20million%20histopathology%20patches%20from%20The%20Cancer%20Genome%20Atlas%0A%28TCGA%29%2C%20our%20approach%20demonstrates%20an%20average%208.5%25%20improvement%20in%20patch-level%0Amajority%20vote%20performance.%20These%20contributions%20provide%20a%20robust%20framework%20for%0Aenhancing%20image%20analysis%20in%20digital%20pathology%2C%20rigorously%20validated%20through%0Aextensive%20evaluation.%20Project%20Page%3A%0Ahttps%3A//kimialabmayo.github.io/PathDino-Page/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.08359v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rotation-Agnostic%20Image%20Representation%20Learning%20for%20Digital%20Pathology&entry.906535625=Saghir%20Alfasly%20and%20Abubakr%20Shafique%20and%20Peyman%20Nejat%20and%20Jibran%20Khan%20and%20Areej%20Alsaafin%20and%20Ghazal%20Alabtah%20and%20H.%20R.%20Tizhoosh&entry.1292438233=%20%20This%20paper%20addresses%20complex%20challenges%20in%20histopathological%20image%20analysis%0Athrough%20three%20key%20contributions.%20Firstly%2C%20it%20introduces%20a%20fast%20patch%20selection%0Amethod%2C%20FPS%2C%20for%20whole-slide%20image%20%28WSI%29%20analysis%2C%20significantly%20reducing%0Acomputational%20cost%20while%20maintaining%20accuracy.%20Secondly%2C%20it%20presents%20PathDino%2C%0Aa%20lightweight%20histopathology%20feature%20extractor%20with%20a%20minimal%20configuration%20of%0Afive%20Transformer%20blocks%20and%20only%209%20million%20parameters%2C%20markedly%20fewer%20than%0Aalternatives.%20Thirdly%2C%20it%20introduces%20a%20rotation-agnostic%20representation%0Alearning%20paradigm%20using%20self-supervised%20learning%2C%20effectively%20mitigating%0Aoverfitting.%20We%20also%20show%20that%20our%20compact%20model%20outperforms%20existing%0Astate-of-the-art%20histopathology-specific%20vision%20transformers%20on%2012%20diverse%0Adatasets%2C%20including%20both%20internal%20datasets%20spanning%20four%20sites%20%28breast%2C%20liver%2C%0Askin%2C%20and%20colorectal%29%20and%20seven%20public%20datasets%20%28PANDA%2C%20CAMELYON16%2C%20BRACS%2C%0ADigestPath%2C%20Kather%2C%20PanNuke%2C%20and%20WSSS4LUAD%29.%20Notably%2C%20even%20with%20a%20training%0Adataset%20of%206%20million%20histopathology%20patches%20from%20The%20Cancer%20Genome%20Atlas%0A%28TCGA%29%2C%20our%20approach%20demonstrates%20an%20average%208.5%25%20improvement%20in%20patch-level%0Amajority%20vote%20performance.%20These%20contributions%20provide%20a%20robust%20framework%20for%0Aenhancing%20image%20analysis%20in%20digital%20pathology%2C%20rigorously%20validated%20through%0Aextensive%20evaluation.%20Project%20Page%3A%0Ahttps%3A//kimialabmayo.github.io/PathDino-Page/%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.08359v2&entry.124074799=Read"},
{"title": "CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\n  Compositional Reasoning via Counterfactual Examples", "author": "Jianrui Zhang and Mu Cai and Tengyang Xie and Yong Jae Lee", "abstract": "  We propose CounterCurate, a framework to comprehensively improve the\nvisio-linguistic compositional reasoning capability for both contrastive and\ngenerative multimodal models. In particular, we identify two critical\nunder-explored problems: the neglect of the physically grounded reasoning\n(counting and position understanding) and the potential of using highly capable\ntext and image generation models for semantic counterfactual fine-tuning. Our\nwork pioneers an approach that addresses these gaps. We first spotlight the\nnear-chance performance of multimodal models like CLIP and LLaVA in physically\ngrounded compositional reasoning. We then apply simple data augmentation using\ngrounded image generation model GLIGEN to generate fine-tuning data, resulting\nin significant performance improvements: +33% and +37% for CLIP and LLaVA,\nrespectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we\nexploit the capabilities of high-performing text generation and image\ngeneration models, specifically GPT-4V and DALLE-3, to curate challenging\nsemantic counterfactuals, thereby further enhancing compositional reasoning\ncapabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms\nGPT-4V.\n", "link": "http://arxiv.org/abs/2402.13254v2", "date": "2024-03-12", "relevancy": 2.1458, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5483}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5298}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5235}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20CounterCurate%3A%20Enhancing%20Physical%20and%20Semantic%20Visio-Linguistic%0A%20%20Compositional%20Reasoning%20via%20Counterfactual%20Examples&body=Title%3A%20CounterCurate%3A%20Enhancing%20Physical%20and%20Semantic%20Visio-Linguistic%0A%20%20Compositional%20Reasoning%20via%20Counterfactual%20Examples%0AAuthor%3A%20Jianrui%20Zhang%20and%20Mu%20Cai%20and%20Tengyang%20Xie%20and%20Yong%20Jae%20Lee%0AAbstract%3A%20%20%20We%20propose%20CounterCurate%2C%20a%20framework%20to%20comprehensively%20improve%20the%0Avisio-linguistic%20compositional%20reasoning%20capability%20for%20both%20contrastive%20and%0Agenerative%20multimodal%20models.%20In%20particular%2C%20we%20identify%20two%20critical%0Aunder-explored%20problems%3A%20the%20neglect%20of%20the%20physically%20grounded%20reasoning%0A%28counting%20and%20position%20understanding%29%20and%20the%20potential%20of%20using%20highly%20capable%0Atext%20and%20image%20generation%20models%20for%20semantic%20counterfactual%20fine-tuning.%20Our%0Awork%20pioneers%20an%20approach%20that%20addresses%20these%20gaps.%20We%20first%20spotlight%20the%0Anear-chance%20performance%20of%20multimodal%20models%20like%20CLIP%20and%20LLaVA%20in%20physically%0Agrounded%20compositional%20reasoning.%20We%20then%20apply%20simple%20data%20augmentation%20using%0Agrounded%20image%20generation%20model%20GLIGEN%20to%20generate%20fine-tuning%20data%2C%20resulting%0Ain%20significant%20performance%20improvements%3A%20%2B33%25%20and%20%2B37%25%20for%20CLIP%20and%20LLaVA%2C%0Arespectively%2C%20on%20our%20newly%20curated%20Flickr30k-Positions%20benchmark.%20Moreover%2C%20we%0Aexploit%20the%20capabilities%20of%20high-performing%20text%20generation%20and%20image%0Ageneration%20models%2C%20specifically%20GPT-4V%20and%20DALLE-3%2C%20to%20curate%20challenging%0Asemantic%20counterfactuals%2C%20thereby%20further%20enhancing%20compositional%20reasoning%0Acapabilities%20on%20benchmarks%20such%20as%20SugarCrepe%2C%20where%20CounterCurate%20outperforms%0AGPT-4V.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13254v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CounterCurate%3A%20Enhancing%20Physical%20and%20Semantic%20Visio-Linguistic%0A%20%20Compositional%20Reasoning%20via%20Counterfactual%20Examples&entry.906535625=Jianrui%20Zhang%20and%20Mu%20Cai%20and%20Tengyang%20Xie%20and%20Yong%20Jae%20Lee&entry.1292438233=%20%20We%20propose%20CounterCurate%2C%20a%20framework%20to%20comprehensively%20improve%20the%0Avisio-linguistic%20compositional%20reasoning%20capability%20for%20both%20contrastive%20and%0Agenerative%20multimodal%20models.%20In%20particular%2C%20we%20identify%20two%20critical%0Aunder-explored%20problems%3A%20the%20neglect%20of%20the%20physically%20grounded%20reasoning%0A%28counting%20and%20position%20understanding%29%20and%20the%20potential%20of%20using%20highly%20capable%0Atext%20and%20image%20generation%20models%20for%20semantic%20counterfactual%20fine-tuning.%20Our%0Awork%20pioneers%20an%20approach%20that%20addresses%20these%20gaps.%20We%20first%20spotlight%20the%0Anear-chance%20performance%20of%20multimodal%20models%20like%20CLIP%20and%20LLaVA%20in%20physically%0Agrounded%20compositional%20reasoning.%20We%20then%20apply%20simple%20data%20augmentation%20using%0Agrounded%20image%20generation%20model%20GLIGEN%20to%20generate%20fine-tuning%20data%2C%20resulting%0Ain%20significant%20performance%20improvements%3A%20%2B33%25%20and%20%2B37%25%20for%20CLIP%20and%20LLaVA%2C%0Arespectively%2C%20on%20our%20newly%20curated%20Flickr30k-Positions%20benchmark.%20Moreover%2C%20we%0Aexploit%20the%20capabilities%20of%20high-performing%20text%20generation%20and%20image%0Ageneration%20models%2C%20specifically%20GPT-4V%20and%20DALLE-3%2C%20to%20curate%20challenging%0Asemantic%20counterfactuals%2C%20thereby%20further%20enhancing%20compositional%20reasoning%0Acapabilities%20on%20benchmarks%20such%20as%20SugarCrepe%2C%20where%20CounterCurate%20outperforms%0AGPT-4V.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13254v2&entry.124074799=Read"},
{"title": "MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with\n  Module-wise Pruning Error Metric", "author": "Haokun Lin and Haoli Bai and Zhili Liu and Lu Hou and Muyi Sun and Linqi Song and Ying Wei and Zhenan Sun", "abstract": "  Vision-language pre-trained models have achieved impressive performance on\nvarious downstream tasks. However, their large model sizes hinder their\nutilization on platforms with limited computational resources. We find that\ndirectly using smaller pre-trained models and applying magnitude-based pruning\non CLIP models leads to inflexibility and inferior performance. Recent efforts\nfor VLP compression either adopt uni-modal compression metrics resulting in\nlimited performance or involve costly mask-search processes with learnable\nmasks. In this paper, we first propose the Module-wise Pruning Error (MoPE)\nmetric, accurately assessing CLIP module importance by performance decline on\ncross-modal tasks. Using the MoPE metric, we introduce a unified pruning\nframework applicable to both pre-training and task-specific fine-tuning\ncompression stages. For pre-training, MoPE-CLIP effectively leverages knowledge\nfrom the teacher model, significantly reducing pre-training costs while\nmaintaining strong zero-shot capabilities. For fine-tuning, consecutive pruning\nfrom width to depth yields highly competitive task-specific models. Extensive\nexperiments in two stages demonstrate the effectiveness of the MoPE metric, and\nMoPE-CLIP outperforms previous state-of-the-art VLP compression methods.\n", "link": "http://arxiv.org/abs/2403.07839v1", "date": "2024-03-12", "relevancy": 2.1357, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5682}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5219}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4784}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20MoPE-CLIP%3A%20Structured%20Pruning%20for%20Efficient%20Vision-Language%20Models%20with%0A%20%20Module-wise%20Pruning%20Error%20Metric&body=Title%3A%20MoPE-CLIP%3A%20Structured%20Pruning%20for%20Efficient%20Vision-Language%20Models%20with%0A%20%20Module-wise%20Pruning%20Error%20Metric%0AAuthor%3A%20Haokun%20Lin%20and%20Haoli%20Bai%20and%20Zhili%20Liu%20and%20Lu%20Hou%20and%20Muyi%20Sun%20and%20Linqi%20Song%20and%20Ying%20Wei%20and%20Zhenan%20Sun%0AAbstract%3A%20%20%20Vision-language%20pre-trained%20models%20have%20achieved%20impressive%20performance%20on%0Avarious%20downstream%20tasks.%20However%2C%20their%20large%20model%20sizes%20hinder%20their%0Autilization%20on%20platforms%20with%20limited%20computational%20resources.%20We%20find%20that%0Adirectly%20using%20smaller%20pre-trained%20models%20and%20applying%20magnitude-based%20pruning%0Aon%20CLIP%20models%20leads%20to%20inflexibility%20and%20inferior%20performance.%20Recent%20efforts%0Afor%20VLP%20compression%20either%20adopt%20uni-modal%20compression%20metrics%20resulting%20in%0Alimited%20performance%20or%20involve%20costly%20mask-search%20processes%20with%20learnable%0Amasks.%20In%20this%20paper%2C%20we%20first%20propose%20the%20Module-wise%20Pruning%20Error%20%28MoPE%29%0Ametric%2C%20accurately%20assessing%20CLIP%20module%20importance%20by%20performance%20decline%20on%0Across-modal%20tasks.%20Using%20the%20MoPE%20metric%2C%20we%20introduce%20a%20unified%20pruning%0Aframework%20applicable%20to%20both%20pre-training%20and%20task-specific%20fine-tuning%0Acompression%20stages.%20For%20pre-training%2C%20MoPE-CLIP%20effectively%20leverages%20knowledge%0Afrom%20the%20teacher%20model%2C%20significantly%20reducing%20pre-training%20costs%20while%0Amaintaining%20strong%20zero-shot%20capabilities.%20For%20fine-tuning%2C%20consecutive%20pruning%0Afrom%20width%20to%20depth%20yields%20highly%20competitive%20task-specific%20models.%20Extensive%0Aexperiments%20in%20two%20stages%20demonstrate%20the%20effectiveness%20of%20the%20MoPE%20metric%2C%20and%0AMoPE-CLIP%20outperforms%20previous%20state-of-the-art%20VLP%20compression%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07839v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoPE-CLIP%3A%20Structured%20Pruning%20for%20Efficient%20Vision-Language%20Models%20with%0A%20%20Module-wise%20Pruning%20Error%20Metric&entry.906535625=Haokun%20Lin%20and%20Haoli%20Bai%20and%20Zhili%20Liu%20and%20Lu%20Hou%20and%20Muyi%20Sun%20and%20Linqi%20Song%20and%20Ying%20Wei%20and%20Zhenan%20Sun&entry.1292438233=%20%20Vision-language%20pre-trained%20models%20have%20achieved%20impressive%20performance%20on%0Avarious%20downstream%20tasks.%20However%2C%20their%20large%20model%20sizes%20hinder%20their%0Autilization%20on%20platforms%20with%20limited%20computational%20resources.%20We%20find%20that%0Adirectly%20using%20smaller%20pre-trained%20models%20and%20applying%20magnitude-based%20pruning%0Aon%20CLIP%20models%20leads%20to%20inflexibility%20and%20inferior%20performance.%20Recent%20efforts%0Afor%20VLP%20compression%20either%20adopt%20uni-modal%20compression%20metrics%20resulting%20in%0Alimited%20performance%20or%20involve%20costly%20mask-search%20processes%20with%20learnable%0Amasks.%20In%20this%20paper%2C%20we%20first%20propose%20the%20Module-wise%20Pruning%20Error%20%28MoPE%29%0Ametric%2C%20accurately%20assessing%20CLIP%20module%20importance%20by%20performance%20decline%20on%0Across-modal%20tasks.%20Using%20the%20MoPE%20metric%2C%20we%20introduce%20a%20unified%20pruning%0Aframework%20applicable%20to%20both%20pre-training%20and%20task-specific%20fine-tuning%0Acompression%20stages.%20For%20pre-training%2C%20MoPE-CLIP%20effectively%20leverages%20knowledge%0Afrom%20the%20teacher%20model%2C%20significantly%20reducing%20pre-training%20costs%20while%0Amaintaining%20strong%20zero-shot%20capabilities.%20For%20fine-tuning%2C%20consecutive%20pruning%0Afrom%20width%20to%20depth%20yields%20highly%20competitive%20task-specific%20models.%20Extensive%0Aexperiments%20in%20two%20stages%20demonstrate%20the%20effectiveness%20of%20the%20MoPE%20metric%2C%20and%0AMoPE-CLIP%20outperforms%20previous%20state-of-the-art%20VLP%20compression%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07839v1&entry.124074799=Read"},
{"title": "CarbonNet: How Computer Vision Plays a Role in Climate Change?\n  Application: Learning Geomechanics from Subsurface Geometry of CCS to\n  Mitigate Global Warming", "author": "Wei Chen and Yunan Li and Yuan Tian", "abstract": "  We introduce a new approach using computer vision to predict the land surface\ndisplacement from subsurface geometry images for Carbon Capture and\nSequestration (CCS). CCS has been proved to be a key component for a carbon\nneutral society. However, scientists see there are challenges along the way\nincluding the high computational cost due to the large model scale and\nlimitations to generalize a pre-trained model with complex physics. We tackle\nthose challenges by training models directly from the subsurface geometry\nimages. The goal is to understand the respons of land surface displacement due\nto carbon injection and utilize our trained models to inform decision making in\nCCS projects.\n  We implement multiple models (CNN, ResNet, and ResNetUNet) for static\nmechanics problem, which is a image prediction problem. Next, we use the LSTM\nand transformer for transient mechanics scenario, which is a video prediction\nproblem. It shows ResNetUNet outperforms the others thanks to its architecture\nin static mechanics problem, and LSTM shows comparable performance to\ntransformer in transient problem. This report proceeds by outlining our dataset\nin detail followed by model descriptions in method section. Result and\ndiscussion state the key learning, observations, and conclusion with future\nwork rounds out the paper.\n", "link": "http://arxiv.org/abs/2403.06025v2", "date": "2024-03-12", "relevancy": 2.1299, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.579}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5327}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5137}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20CarbonNet%3A%20How%20Computer%20Vision%20Plays%20a%20Role%20in%20Climate%20Change%3F%0A%20%20Application%3A%20Learning%20Geomechanics%20from%20Subsurface%20Geometry%20of%20CCS%20to%0A%20%20Mitigate%20Global%20Warming&body=Title%3A%20CarbonNet%3A%20How%20Computer%20Vision%20Plays%20a%20Role%20in%20Climate%20Change%3F%0A%20%20Application%3A%20Learning%20Geomechanics%20from%20Subsurface%20Geometry%20of%20CCS%20to%0A%20%20Mitigate%20Global%20Warming%0AAuthor%3A%20Wei%20Chen%20and%20Yunan%20Li%20and%20Yuan%20Tian%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20approach%20using%20computer%20vision%20to%20predict%20the%20land%20surface%0Adisplacement%20from%20subsurface%20geometry%20images%20for%20Carbon%20Capture%20and%0ASequestration%20%28CCS%29.%20CCS%20has%20been%20proved%20to%20be%20a%20key%20component%20for%20a%20carbon%0Aneutral%20society.%20However%2C%20scientists%20see%20there%20are%20challenges%20along%20the%20way%0Aincluding%20the%20high%20computational%20cost%20due%20to%20the%20large%20model%20scale%20and%0Alimitations%20to%20generalize%20a%20pre-trained%20model%20with%20complex%20physics.%20We%20tackle%0Athose%20challenges%20by%20training%20models%20directly%20from%20the%20subsurface%20geometry%0Aimages.%20The%20goal%20is%20to%20understand%20the%20respons%20of%20land%20surface%20displacement%20due%0Ato%20carbon%20injection%20and%20utilize%20our%20trained%20models%20to%20inform%20decision%20making%20in%0ACCS%20projects.%0A%20%20We%20implement%20multiple%20models%20%28CNN%2C%20ResNet%2C%20and%20ResNetUNet%29%20for%20static%0Amechanics%20problem%2C%20which%20is%20a%20image%20prediction%20problem.%20Next%2C%20we%20use%20the%20LSTM%0Aand%20transformer%20for%20transient%20mechanics%20scenario%2C%20which%20is%20a%20video%20prediction%0Aproblem.%20It%20shows%20ResNetUNet%20outperforms%20the%20others%20thanks%20to%20its%20architecture%0Ain%20static%20mechanics%20problem%2C%20and%20LSTM%20shows%20comparable%20performance%20to%0Atransformer%20in%20transient%20problem.%20This%20report%20proceeds%20by%20outlining%20our%20dataset%0Ain%20detail%20followed%20by%20model%20descriptions%20in%20method%20section.%20Result%20and%0Adiscussion%20state%20the%20key%20learning%2C%20observations%2C%20and%20conclusion%20with%20future%0Awork%20rounds%20out%20the%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06025v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CarbonNet%3A%20How%20Computer%20Vision%20Plays%20a%20Role%20in%20Climate%20Change%3F%0A%20%20Application%3A%20Learning%20Geomechanics%20from%20Subsurface%20Geometry%20of%20CCS%20to%0A%20%20Mitigate%20Global%20Warming&entry.906535625=Wei%20Chen%20and%20Yunan%20Li%20and%20Yuan%20Tian&entry.1292438233=%20%20We%20introduce%20a%20new%20approach%20using%20computer%20vision%20to%20predict%20the%20land%20surface%0Adisplacement%20from%20subsurface%20geometry%20images%20for%20Carbon%20Capture%20and%0ASequestration%20%28CCS%29.%20CCS%20has%20been%20proved%20to%20be%20a%20key%20component%20for%20a%20carbon%0Aneutral%20society.%20However%2C%20scientists%20see%20there%20are%20challenges%20along%20the%20way%0Aincluding%20the%20high%20computational%20cost%20due%20to%20the%20large%20model%20scale%20and%0Alimitations%20to%20generalize%20a%20pre-trained%20model%20with%20complex%20physics.%20We%20tackle%0Athose%20challenges%20by%20training%20models%20directly%20from%20the%20subsurface%20geometry%0Aimages.%20The%20goal%20is%20to%20understand%20the%20respons%20of%20land%20surface%20displacement%20due%0Ato%20carbon%20injection%20and%20utilize%20our%20trained%20models%20to%20inform%20decision%20making%20in%0ACCS%20projects.%0A%20%20We%20implement%20multiple%20models%20%28CNN%2C%20ResNet%2C%20and%20ResNetUNet%29%20for%20static%0Amechanics%20problem%2C%20which%20is%20a%20image%20prediction%20problem.%20Next%2C%20we%20use%20the%20LSTM%0Aand%20transformer%20for%20transient%20mechanics%20scenario%2C%20which%20is%20a%20video%20prediction%0Aproblem.%20It%20shows%20ResNetUNet%20outperforms%20the%20others%20thanks%20to%20its%20architecture%0Ain%20static%20mechanics%20problem%2C%20and%20LSTM%20shows%20comparable%20performance%20to%0Atransformer%20in%20transient%20problem.%20This%20report%20proceeds%20by%20outlining%20our%20dataset%0Ain%20detail%20followed%20by%20model%20descriptions%20in%20method%20section.%20Result%20and%0Adiscussion%20state%20the%20key%20learning%2C%20observations%2C%20and%20conclusion%20with%20future%0Awork%20rounds%20out%20the%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06025v2&entry.124074799=Read"},
{"title": "Recent Advances in Embedding Methods for Multi-Object Tracking: A Survey", "author": "Gaoang Wang and Mingli Song and Jenq-Neng Hwang", "abstract": "  Multi-object tracking (MOT) aims to associate target objects across video\nframes in order to obtain entire moving trajectories. With the advancement of\ndeep neural networks and the increasing demand for intelligent video analysis,\nMOT has gained significantly increased interest in the computer vision\ncommunity. Embedding methods play an essential role in object location\nestimation and temporal identity association in MOT. Unlike other computer\nvision tasks, such as image classification, object detection,\nre-identification, and segmentation, embedding methods in MOT have large\nvariations, and they have never been systematically analyzed and summarized. In\nthis survey, we first conduct a comprehensive overview with in-depth analysis\nfor embedding methods in MOT from seven different perspectives, including\npatch-level embedding, single-frame embedding, cross-frame joint embedding,\ncorrelation embedding, sequential embedding, tracklet embedding, and\ncross-track relational embedding. We further summarize the existing widely used\nMOT datasets and analyze the advantages of existing state-of-the-art methods\naccording to their embedding strategies. Finally, some critical yet\nunder-investigated areas and future research directions are discussed.\n", "link": "http://arxiv.org/abs/2205.10766v2", "date": "2024-03-12", "relevancy": 2.1287, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5551}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5288}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5264}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Recent%20Advances%20in%20Embedding%20Methods%20for%20Multi-Object%20Tracking%3A%20A%20Survey&body=Title%3A%20Recent%20Advances%20in%20Embedding%20Methods%20for%20Multi-Object%20Tracking%3A%20A%20Survey%0AAuthor%3A%20Gaoang%20Wang%20and%20Mingli%20Song%20and%20Jenq-Neng%20Hwang%0AAbstract%3A%20%20%20Multi-object%20tracking%20%28MOT%29%20aims%20to%20associate%20target%20objects%20across%20video%0Aframes%20in%20order%20to%20obtain%20entire%20moving%20trajectories.%20With%20the%20advancement%20of%0Adeep%20neural%20networks%20and%20the%20increasing%20demand%20for%20intelligent%20video%20analysis%2C%0AMOT%20has%20gained%20significantly%20increased%20interest%20in%20the%20computer%20vision%0Acommunity.%20Embedding%20methods%20play%20an%20essential%20role%20in%20object%20location%0Aestimation%20and%20temporal%20identity%20association%20in%20MOT.%20Unlike%20other%20computer%0Avision%20tasks%2C%20such%20as%20image%20classification%2C%20object%20detection%2C%0Are-identification%2C%20and%20segmentation%2C%20embedding%20methods%20in%20MOT%20have%20large%0Avariations%2C%20and%20they%20have%20never%20been%20systematically%20analyzed%20and%20summarized.%20In%0Athis%20survey%2C%20we%20first%20conduct%20a%20comprehensive%20overview%20with%20in-depth%20analysis%0Afor%20embedding%20methods%20in%20MOT%20from%20seven%20different%20perspectives%2C%20including%0Apatch-level%20embedding%2C%20single-frame%20embedding%2C%20cross-frame%20joint%20embedding%2C%0Acorrelation%20embedding%2C%20sequential%20embedding%2C%20tracklet%20embedding%2C%20and%0Across-track%20relational%20embedding.%20We%20further%20summarize%20the%20existing%20widely%20used%0AMOT%20datasets%20and%20analyze%20the%20advantages%20of%20existing%20state-of-the-art%20methods%0Aaccording%20to%20their%20embedding%20strategies.%20Finally%2C%20some%20critical%20yet%0Aunder-investigated%20areas%20and%20future%20research%20directions%20are%20discussed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.10766v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recent%20Advances%20in%20Embedding%20Methods%20for%20Multi-Object%20Tracking%3A%20A%20Survey&entry.906535625=Gaoang%20Wang%20and%20Mingli%20Song%20and%20Jenq-Neng%20Hwang&entry.1292438233=%20%20Multi-object%20tracking%20%28MOT%29%20aims%20to%20associate%20target%20objects%20across%20video%0Aframes%20in%20order%20to%20obtain%20entire%20moving%20trajectories.%20With%20the%20advancement%20of%0Adeep%20neural%20networks%20and%20the%20increasing%20demand%20for%20intelligent%20video%20analysis%2C%0AMOT%20has%20gained%20significantly%20increased%20interest%20in%20the%20computer%20vision%0Acommunity.%20Embedding%20methods%20play%20an%20essential%20role%20in%20object%20location%0Aestimation%20and%20temporal%20identity%20association%20in%20MOT.%20Unlike%20other%20computer%0Avision%20tasks%2C%20such%20as%20image%20classification%2C%20object%20detection%2C%0Are-identification%2C%20and%20segmentation%2C%20embedding%20methods%20in%20MOT%20have%20large%0Avariations%2C%20and%20they%20have%20never%20been%20systematically%20analyzed%20and%20summarized.%20In%0Athis%20survey%2C%20we%20first%20conduct%20a%20comprehensive%20overview%20with%20in-depth%20analysis%0Afor%20embedding%20methods%20in%20MOT%20from%20seven%20different%20perspectives%2C%20including%0Apatch-level%20embedding%2C%20single-frame%20embedding%2C%20cross-frame%20joint%20embedding%2C%0Acorrelation%20embedding%2C%20sequential%20embedding%2C%20tracklet%20embedding%2C%20and%0Across-track%20relational%20embedding.%20We%20further%20summarize%20the%20existing%20widely%20used%0AMOT%20datasets%20and%20analyze%20the%20advantages%20of%20existing%20state-of-the-art%20methods%0Aaccording%20to%20their%20embedding%20strategies.%20Finally%2C%20some%20critical%20yet%0Aunder-investigated%20areas%20and%20future%20research%20directions%20are%20discussed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.10766v2&entry.124074799=Read"},
{"title": "Goal-Reaching Trajectory Design Near Danger with Piecewise Affine\n  Reach-avoid Computation", "author": "Long Kiu Chung and Wonsuhk Jung and Chuizheng Kong and Shreyas Kousik", "abstract": "  Autonomous mobile robots must maintain safety, but should not sacrifice\nperformance, leading to the classical reach-avoid problem. This paper seeks to\ncompute trajectory plans for which a robot is guaranteed to reach a goal and\navoid obstacles in the specific near-danger case that the obstacles and goal\nare near each other. The proposed method builds off of a common approach of\nusing a simplified planning model to generate plans, which are then tracked\nusing a high-fidelity tracking model and controller. Existing safe planning\napproaches use reachability analysis to overapproximate the error between these\nmodels, but this introduces additional numerical approximation error and\nthereby conservativeness that prevents goal-reaching. The present work instead\nproposes a Piecewise Affine Reach-avoid Computation (PARC) method to tightly\napproximate the reachable set of the planning model. With PARC, the main source\nof conservativeness is the model mismatch, which can be mitigated by careful\ncontroller and planning model design. The utility of this method is\ndemonstrated through extensive numerical experiments in which PARC outperforms\nstate-of-the-art reach-avoid methods in near-danger goal-reaching. Furthermore,\nin a simulated demonstration, PARC enables the generation of provably-safe\nextreme vehicle dynamics drift parking maneuvers.\n", "link": "http://arxiv.org/abs/2402.15604v2", "date": "2024-03-12", "relevancy": 2.1263, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5445}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5293}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5287}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Goal-Reaching%20Trajectory%20Design%20Near%20Danger%20with%20Piecewise%20Affine%0A%20%20Reach-avoid%20Computation&body=Title%3A%20Goal-Reaching%20Trajectory%20Design%20Near%20Danger%20with%20Piecewise%20Affine%0A%20%20Reach-avoid%20Computation%0AAuthor%3A%20Long%20Kiu%20Chung%20and%20Wonsuhk%20Jung%20and%20Chuizheng%20Kong%20and%20Shreyas%20Kousik%0AAbstract%3A%20%20%20Autonomous%20mobile%20robots%20must%20maintain%20safety%2C%20but%20should%20not%20sacrifice%0Aperformance%2C%20leading%20to%20the%20classical%20reach-avoid%20problem.%20This%20paper%20seeks%20to%0Acompute%20trajectory%20plans%20for%20which%20a%20robot%20is%20guaranteed%20to%20reach%20a%20goal%20and%0Aavoid%20obstacles%20in%20the%20specific%20near-danger%20case%20that%20the%20obstacles%20and%20goal%0Aare%20near%20each%20other.%20The%20proposed%20method%20builds%20off%20of%20a%20common%20approach%20of%0Ausing%20a%20simplified%20planning%20model%20to%20generate%20plans%2C%20which%20are%20then%20tracked%0Ausing%20a%20high-fidelity%20tracking%20model%20and%20controller.%20Existing%20safe%20planning%0Aapproaches%20use%20reachability%20analysis%20to%20overapproximate%20the%20error%20between%20these%0Amodels%2C%20but%20this%20introduces%20additional%20numerical%20approximation%20error%20and%0Athereby%20conservativeness%20that%20prevents%20goal-reaching.%20The%20present%20work%20instead%0Aproposes%20a%20Piecewise%20Affine%20Reach-avoid%20Computation%20%28PARC%29%20method%20to%20tightly%0Aapproximate%20the%20reachable%20set%20of%20the%20planning%20model.%20With%20PARC%2C%20the%20main%20source%0Aof%20conservativeness%20is%20the%20model%20mismatch%2C%20which%20can%20be%20mitigated%20by%20careful%0Acontroller%20and%20planning%20model%20design.%20The%20utility%20of%20this%20method%20is%0Ademonstrated%20through%20extensive%20numerical%20experiments%20in%20which%20PARC%20outperforms%0Astate-of-the-art%20reach-avoid%20methods%20in%20near-danger%20goal-reaching.%20Furthermore%2C%0Ain%20a%20simulated%20demonstration%2C%20PARC%20enables%20the%20generation%20of%20provably-safe%0Aextreme%20vehicle%20dynamics%20drift%20parking%20maneuvers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15604v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Goal-Reaching%20Trajectory%20Design%20Near%20Danger%20with%20Piecewise%20Affine%0A%20%20Reach-avoid%20Computation&entry.906535625=Long%20Kiu%20Chung%20and%20Wonsuhk%20Jung%20and%20Chuizheng%20Kong%20and%20Shreyas%20Kousik&entry.1292438233=%20%20Autonomous%20mobile%20robots%20must%20maintain%20safety%2C%20but%20should%20not%20sacrifice%0Aperformance%2C%20leading%20to%20the%20classical%20reach-avoid%20problem.%20This%20paper%20seeks%20to%0Acompute%20trajectory%20plans%20for%20which%20a%20robot%20is%20guaranteed%20to%20reach%20a%20goal%20and%0Aavoid%20obstacles%20in%20the%20specific%20near-danger%20case%20that%20the%20obstacles%20and%20goal%0Aare%20near%20each%20other.%20The%20proposed%20method%20builds%20off%20of%20a%20common%20approach%20of%0Ausing%20a%20simplified%20planning%20model%20to%20generate%20plans%2C%20which%20are%20then%20tracked%0Ausing%20a%20high-fidelity%20tracking%20model%20and%20controller.%20Existing%20safe%20planning%0Aapproaches%20use%20reachability%20analysis%20to%20overapproximate%20the%20error%20between%20these%0Amodels%2C%20but%20this%20introduces%20additional%20numerical%20approximation%20error%20and%0Athereby%20conservativeness%20that%20prevents%20goal-reaching.%20The%20present%20work%20instead%0Aproposes%20a%20Piecewise%20Affine%20Reach-avoid%20Computation%20%28PARC%29%20method%20to%20tightly%0Aapproximate%20the%20reachable%20set%20of%20the%20planning%20model.%20With%20PARC%2C%20the%20main%20source%0Aof%20conservativeness%20is%20the%20model%20mismatch%2C%20which%20can%20be%20mitigated%20by%20careful%0Acontroller%20and%20planning%20model%20design.%20The%20utility%20of%20this%20method%20is%0Ademonstrated%20through%20extensive%20numerical%20experiments%20in%20which%20PARC%20outperforms%0Astate-of-the-art%20reach-avoid%20methods%20in%20near-danger%20goal-reaching.%20Furthermore%2C%0Ain%20a%20simulated%20demonstration%2C%20PARC%20enables%20the%20generation%20of%20provably-safe%0Aextreme%20vehicle%20dynamics%20drift%20parking%20maneuvers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15604v2&entry.124074799=Read"},
{"title": "Label Dropout: Improved Deep Learning Echocardiography Segmentation\n  Using Multiple Datasets With Domain Shift and Partial Labelling", "author": "Iman Islam and Esther Puyol-Ant\u00f3n and Bram Ruijsink and Andrew J. Reader and Andrew P. King", "abstract": "  Echocardiography (echo) is the first imaging modality used when assessing\ncardiac function. The measurement of functional biomarkers from echo relies\nupon the segmentation of cardiac structures and deep learning models have been\nproposed to automate the segmentation process. However, in order to translate\nthese tools to widespread clinical use it is important that the segmentation\nmodels are robust to a wide variety of images (e.g. acquired from different\nscanners, by operators with different levels of expertise etc.). To achieve\nthis level of robustness it is necessary that the models are trained with\nmultiple diverse datasets. A significant challenge faced when training with\nmultiple diverse datasets is the variation in label presence, i.e. the combined\ndata are often partially-labelled. Adaptations of the cross entropy loss\nfunction have been proposed to deal with partially labelled data. In this paper\nwe show that training naively with such a loss function and multiple diverse\ndatasets can lead to a form of shortcut learning, where the model associates\nlabel presence with domain characteristics, leading to a drop in performance.\nTo address this problem, we propose a novel label dropout scheme to break the\nlink between domain characteristics and the presence or absence of labels. We\ndemonstrate that label dropout improves echo segmentation Dice score by 62% and\n25% on two cardiac structures when training using multiple diverse partially\nlabelled datasets.\n", "link": "http://arxiv.org/abs/2403.07818v1", "date": "2024-03-12", "relevancy": 2.0604, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5654}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5087}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5014}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Label%20Dropout%3A%20Improved%20Deep%20Learning%20Echocardiography%20Segmentation%0A%20%20Using%20Multiple%20Datasets%20With%20Domain%20Shift%20and%20Partial%20Labelling&body=Title%3A%20Label%20Dropout%3A%20Improved%20Deep%20Learning%20Echocardiography%20Segmentation%0A%20%20Using%20Multiple%20Datasets%20With%20Domain%20Shift%20and%20Partial%20Labelling%0AAuthor%3A%20Iman%20Islam%20and%20Esther%20Puyol-Ant%C3%B3n%20and%20Bram%20Ruijsink%20and%20Andrew%20J.%20Reader%20and%20Andrew%20P.%20King%0AAbstract%3A%20%20%20Echocardiography%20%28echo%29%20is%20the%20first%20imaging%20modality%20used%20when%20assessing%0Acardiac%20function.%20The%20measurement%20of%20functional%20biomarkers%20from%20echo%20relies%0Aupon%20the%20segmentation%20of%20cardiac%20structures%20and%20deep%20learning%20models%20have%20been%0Aproposed%20to%20automate%20the%20segmentation%20process.%20However%2C%20in%20order%20to%20translate%0Athese%20tools%20to%20widespread%20clinical%20use%20it%20is%20important%20that%20the%20segmentation%0Amodels%20are%20robust%20to%20a%20wide%20variety%20of%20images%20%28e.g.%20acquired%20from%20different%0Ascanners%2C%20by%20operators%20with%20different%20levels%20of%20expertise%20etc.%29.%20To%20achieve%0Athis%20level%20of%20robustness%20it%20is%20necessary%20that%20the%20models%20are%20trained%20with%0Amultiple%20diverse%20datasets.%20A%20significant%20challenge%20faced%20when%20training%20with%0Amultiple%20diverse%20datasets%20is%20the%20variation%20in%20label%20presence%2C%20i.e.%20the%20combined%0Adata%20are%20often%20partially-labelled.%20Adaptations%20of%20the%20cross%20entropy%20loss%0Afunction%20have%20been%20proposed%20to%20deal%20with%20partially%20labelled%20data.%20In%20this%20paper%0Awe%20show%20that%20training%20naively%20with%20such%20a%20loss%20function%20and%20multiple%20diverse%0Adatasets%20can%20lead%20to%20a%20form%20of%20shortcut%20learning%2C%20where%20the%20model%20associates%0Alabel%20presence%20with%20domain%20characteristics%2C%20leading%20to%20a%20drop%20in%20performance.%0ATo%20address%20this%20problem%2C%20we%20propose%20a%20novel%20label%20dropout%20scheme%20to%20break%20the%0Alink%20between%20domain%20characteristics%20and%20the%20presence%20or%20absence%20of%20labels.%20We%0Ademonstrate%20that%20label%20dropout%20improves%20echo%20segmentation%20Dice%20score%20by%2062%25%20and%0A25%25%20on%20two%20cardiac%20structures%20when%20training%20using%20multiple%20diverse%20partially%0Alabelled%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07818v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label%20Dropout%3A%20Improved%20Deep%20Learning%20Echocardiography%20Segmentation%0A%20%20Using%20Multiple%20Datasets%20With%20Domain%20Shift%20and%20Partial%20Labelling&entry.906535625=Iman%20Islam%20and%20Esther%20Puyol-Ant%C3%B3n%20and%20Bram%20Ruijsink%20and%20Andrew%20J.%20Reader%20and%20Andrew%20P.%20King&entry.1292438233=%20%20Echocardiography%20%28echo%29%20is%20the%20first%20imaging%20modality%20used%20when%20assessing%0Acardiac%20function.%20The%20measurement%20of%20functional%20biomarkers%20from%20echo%20relies%0Aupon%20the%20segmentation%20of%20cardiac%20structures%20and%20deep%20learning%20models%20have%20been%0Aproposed%20to%20automate%20the%20segmentation%20process.%20However%2C%20in%20order%20to%20translate%0Athese%20tools%20to%20widespread%20clinical%20use%20it%20is%20important%20that%20the%20segmentation%0Amodels%20are%20robust%20to%20a%20wide%20variety%20of%20images%20%28e.g.%20acquired%20from%20different%0Ascanners%2C%20by%20operators%20with%20different%20levels%20of%20expertise%20etc.%29.%20To%20achieve%0Athis%20level%20of%20robustness%20it%20is%20necessary%20that%20the%20models%20are%20trained%20with%0Amultiple%20diverse%20datasets.%20A%20significant%20challenge%20faced%20when%20training%20with%0Amultiple%20diverse%20datasets%20is%20the%20variation%20in%20label%20presence%2C%20i.e.%20the%20combined%0Adata%20are%20often%20partially-labelled.%20Adaptations%20of%20the%20cross%20entropy%20loss%0Afunction%20have%20been%20proposed%20to%20deal%20with%20partially%20labelled%20data.%20In%20this%20paper%0Awe%20show%20that%20training%20naively%20with%20such%20a%20loss%20function%20and%20multiple%20diverse%0Adatasets%20can%20lead%20to%20a%20form%20of%20shortcut%20learning%2C%20where%20the%20model%20associates%0Alabel%20presence%20with%20domain%20characteristics%2C%20leading%20to%20a%20drop%20in%20performance.%0ATo%20address%20this%20problem%2C%20we%20propose%20a%20novel%20label%20dropout%20scheme%20to%20break%20the%0Alink%20between%20domain%20characteristics%20and%20the%20presence%20or%20absence%20of%20labels.%20We%0Ademonstrate%20that%20label%20dropout%20improves%20echo%20segmentation%20Dice%20score%20by%2062%25%20and%0A25%25%20on%20two%20cardiac%20structures%20when%20training%20using%20multiple%20diverse%20partially%0Alabelled%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07818v1&entry.124074799=Read"},
{"title": "A Fourier Transform Framework for Domain Adaptation", "author": "Le Luo and Bingrong Xu and Qingyong Zhang and Cheng Lian and Jie Luo", "abstract": "  By using unsupervised domain adaptation (UDA), knowledge can be transferred\nfrom a label-rich source domain to a target domain that contains relevant\ninformation but lacks labels. Many existing UDA algorithms suffer from directly\nusing raw images as input, resulting in models that overly focus on redundant\ninformation and exhibit poor generalization capability. To address this issue,\nwe attempt to improve the performance of unsupervised domain adaptation by\nemploying the Fourier method (FTF).Specifically, FTF is inspired by the\namplitude of Fourier spectra, which primarily preserves low-level statistical\ninformation. In FTF, we effectively incorporate low-level information from the\ntarget domain into the source domain by fusing the amplitudes of both domains\nin the Fourier domain. Additionally, we observe that extracting features from\nbatches of images can eliminate redundant information while retaining\nclass-specific features relevant to the task. Building upon this observation,\nwe apply the Fourier Transform at the data stream level for the first time. To\nfurther align multiple sources of data, we introduce the concept of correlation\nalignment. To evaluate the effectiveness of our FTF method, we conducted\nevaluations on four benchmark datasets for domain adaptation, including\nOffice-31, Office-Home, ImageCLEF-DA, and Office-Caltech. Our results\ndemonstrate superior performance.\n", "link": "http://arxiv.org/abs/2403.07798v1", "date": "2024-03-12", "relevancy": 2.0229, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5181}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5078}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4987}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20A%20Fourier%20Transform%20Framework%20for%20Domain%20Adaptation&body=Title%3A%20A%20Fourier%20Transform%20Framework%20for%20Domain%20Adaptation%0AAuthor%3A%20Le%20Luo%20and%20Bingrong%20Xu%20and%20Qingyong%20Zhang%20and%20Cheng%20Lian%20and%20Jie%20Luo%0AAbstract%3A%20%20%20By%20using%20unsupervised%20domain%20adaptation%20%28UDA%29%2C%20knowledge%20can%20be%20transferred%0Afrom%20a%20label-rich%20source%20domain%20to%20a%20target%20domain%20that%20contains%20relevant%0Ainformation%20but%20lacks%20labels.%20Many%20existing%20UDA%20algorithms%20suffer%20from%20directly%0Ausing%20raw%20images%20as%20input%2C%20resulting%20in%20models%20that%20overly%20focus%20on%20redundant%0Ainformation%20and%20exhibit%20poor%20generalization%20capability.%20To%20address%20this%20issue%2C%0Awe%20attempt%20to%20improve%20the%20performance%20of%20unsupervised%20domain%20adaptation%20by%0Aemploying%20the%20Fourier%20method%20%28FTF%29.Specifically%2C%20FTF%20is%20inspired%20by%20the%0Aamplitude%20of%20Fourier%20spectra%2C%20which%20primarily%20preserves%20low-level%20statistical%0Ainformation.%20In%20FTF%2C%20we%20effectively%20incorporate%20low-level%20information%20from%20the%0Atarget%20domain%20into%20the%20source%20domain%20by%20fusing%20the%20amplitudes%20of%20both%20domains%0Ain%20the%20Fourier%20domain.%20Additionally%2C%20we%20observe%20that%20extracting%20features%20from%0Abatches%20of%20images%20can%20eliminate%20redundant%20information%20while%20retaining%0Aclass-specific%20features%20relevant%20to%20the%20task.%20Building%20upon%20this%20observation%2C%0Awe%20apply%20the%20Fourier%20Transform%20at%20the%20data%20stream%20level%20for%20the%20first%20time.%20To%0Afurther%20align%20multiple%20sources%20of%20data%2C%20we%20introduce%20the%20concept%20of%20correlation%0Aalignment.%20To%20evaluate%20the%20effectiveness%20of%20our%20FTF%20method%2C%20we%20conducted%0Aevaluations%20on%20four%20benchmark%20datasets%20for%20domain%20adaptation%2C%20including%0AOffice-31%2C%20Office-Home%2C%20ImageCLEF-DA%2C%20and%20Office-Caltech.%20Our%20results%0Ademonstrate%20superior%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07798v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Fourier%20Transform%20Framework%20for%20Domain%20Adaptation&entry.906535625=Le%20Luo%20and%20Bingrong%20Xu%20and%20Qingyong%20Zhang%20and%20Cheng%20Lian%20and%20Jie%20Luo&entry.1292438233=%20%20By%20using%20unsupervised%20domain%20adaptation%20%28UDA%29%2C%20knowledge%20can%20be%20transferred%0Afrom%20a%20label-rich%20source%20domain%20to%20a%20target%20domain%20that%20contains%20relevant%0Ainformation%20but%20lacks%20labels.%20Many%20existing%20UDA%20algorithms%20suffer%20from%20directly%0Ausing%20raw%20images%20as%20input%2C%20resulting%20in%20models%20that%20overly%20focus%20on%20redundant%0Ainformation%20and%20exhibit%20poor%20generalization%20capability.%20To%20address%20this%20issue%2C%0Awe%20attempt%20to%20improve%20the%20performance%20of%20unsupervised%20domain%20adaptation%20by%0Aemploying%20the%20Fourier%20method%20%28FTF%29.Specifically%2C%20FTF%20is%20inspired%20by%20the%0Aamplitude%20of%20Fourier%20spectra%2C%20which%20primarily%20preserves%20low-level%20statistical%0Ainformation.%20In%20FTF%2C%20we%20effectively%20incorporate%20low-level%20information%20from%20the%0Atarget%20domain%20into%20the%20source%20domain%20by%20fusing%20the%20amplitudes%20of%20both%20domains%0Ain%20the%20Fourier%20domain.%20Additionally%2C%20we%20observe%20that%20extracting%20features%20from%0Abatches%20of%20images%20can%20eliminate%20redundant%20information%20while%20retaining%0Aclass-specific%20features%20relevant%20to%20the%20task.%20Building%20upon%20this%20observation%2C%0Awe%20apply%20the%20Fourier%20Transform%20at%20the%20data%20stream%20level%20for%20the%20first%20time.%20To%0Afurther%20align%20multiple%20sources%20of%20data%2C%20we%20introduce%20the%20concept%20of%20correlation%0Aalignment.%20To%20evaluate%20the%20effectiveness%20of%20our%20FTF%20method%2C%20we%20conducted%0Aevaluations%20on%20four%20benchmark%20datasets%20for%20domain%20adaptation%2C%20including%0AOffice-31%2C%20Office-Home%2C%20ImageCLEF-DA%2C%20and%20Office-Caltech.%20Our%20results%0Ademonstrate%20superior%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07798v1&entry.124074799=Read"},
{"title": "Data Interpreter: An LLM Agent For Data Science", "author": "Sirui Hong and Yizhang Lin and Bang Liu and Bangbang Liu and Binhao Wu and Danyang Li and Jiaqi Chen and Jiayi Zhang and Jinlin Wang and Li Zhang and Lingyao Zhang and Min Yang and Mingchen Zhuge and Taicheng Guo and Tuo Zhou and Wei Tao and Wenyi Wang and Xiangru Tang and Xiangtao Lu and Xiawu Zheng and Xinbing Liang and Yaying Fei and Yuheng Cheng and Zongze Xu and Chenglin Wu", "abstract": "  Large Language Model (LLM)-based agents have demonstrated remarkable\neffectiveness. However, their performance can be compromised in data science\nscenarios that require real-time data adjustment, expertise in optimization due\nto complex dependencies among various tasks, and the ability to identify\nlogical errors for precise reasoning. In this study, we introduce the Data\nInterpreter, a solution designed to solve with code that emphasizes three\npivotal techniques to augment problem-solving in data science: 1) dynamic\nplanning with hierarchical graph structures for real-time data adaptability;2)\ntool integration dynamically to enhance code proficiency during execution,\nenriching the requisite expertise;3) logical inconsistency identification in\nfeedback, and efficiency enhancement through experience recording. We evaluate\nthe Data Interpreter on various data science and real-world tasks. Compared to\nopen-source baselines, it demonstrated superior performance, exhibiting\nsignificant improvements in machine learning tasks, increasing from 0.86 to\n0.95. Additionally, it showed a 26% increase in the MATH dataset and a\nremarkable 112% improvement in open-ended tasks. The solution will be released\nat https://github.com/geekan/MetaGPT.\n", "link": "http://arxiv.org/abs/2402.18679v3", "date": "2024-03-12", "relevancy": 2.006, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5078}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5044}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.494}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Data%20Interpreter%3A%20An%20LLM%20Agent%20For%20Data%20Science&body=Title%3A%20Data%20Interpreter%3A%20An%20LLM%20Agent%20For%20Data%20Science%0AAuthor%3A%20Sirui%20Hong%20and%20Yizhang%20Lin%20and%20Bang%20Liu%20and%20Bangbang%20Liu%20and%20Binhao%20Wu%20and%20Danyang%20Li%20and%20Jiaqi%20Chen%20and%20Jiayi%20Zhang%20and%20Jinlin%20Wang%20and%20Li%20Zhang%20and%20Lingyao%20Zhang%20and%20Min%20Yang%20and%20Mingchen%20Zhuge%20and%20Taicheng%20Guo%20and%20Tuo%20Zhou%20and%20Wei%20Tao%20and%20Wenyi%20Wang%20and%20Xiangru%20Tang%20and%20Xiangtao%20Lu%20and%20Xiawu%20Zheng%20and%20Xinbing%20Liang%20and%20Yaying%20Fei%20and%20Yuheng%20Cheng%20and%20Zongze%20Xu%20and%20Chenglin%20Wu%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29-based%20agents%20have%20demonstrated%20remarkable%0Aeffectiveness.%20However%2C%20their%20performance%20can%20be%20compromised%20in%20data%20science%0Ascenarios%20that%20require%20real-time%20data%20adjustment%2C%20expertise%20in%20optimization%20due%0Ato%20complex%20dependencies%20among%20various%20tasks%2C%20and%20the%20ability%20to%20identify%0Alogical%20errors%20for%20precise%20reasoning.%20In%20this%20study%2C%20we%20introduce%20the%20Data%0AInterpreter%2C%20a%20solution%20designed%20to%20solve%20with%20code%20that%20emphasizes%20three%0Apivotal%20techniques%20to%20augment%20problem-solving%20in%20data%20science%3A%201%29%20dynamic%0Aplanning%20with%20hierarchical%20graph%20structures%20for%20real-time%20data%20adaptability%3B2%29%0Atool%20integration%20dynamically%20to%20enhance%20code%20proficiency%20during%20execution%2C%0Aenriching%20the%20requisite%20expertise%3B3%29%20logical%20inconsistency%20identification%20in%0Afeedback%2C%20and%20efficiency%20enhancement%20through%20experience%20recording.%20We%20evaluate%0Athe%20Data%20Interpreter%20on%20various%20data%20science%20and%20real-world%20tasks.%20Compared%20to%0Aopen-source%20baselines%2C%20it%20demonstrated%20superior%20performance%2C%20exhibiting%0Asignificant%20improvements%20in%20machine%20learning%20tasks%2C%20increasing%20from%200.86%20to%0A0.95.%20Additionally%2C%20it%20showed%20a%2026%25%20increase%20in%20the%20MATH%20dataset%20and%20a%0Aremarkable%20112%25%20improvement%20in%20open-ended%20tasks.%20The%20solution%20will%20be%20released%0Aat%20https%3A//github.com/geekan/MetaGPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18679v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Interpreter%3A%20An%20LLM%20Agent%20For%20Data%20Science&entry.906535625=Sirui%20Hong%20and%20Yizhang%20Lin%20and%20Bang%20Liu%20and%20Bangbang%20Liu%20and%20Binhao%20Wu%20and%20Danyang%20Li%20and%20Jiaqi%20Chen%20and%20Jiayi%20Zhang%20and%20Jinlin%20Wang%20and%20Li%20Zhang%20and%20Lingyao%20Zhang%20and%20Min%20Yang%20and%20Mingchen%20Zhuge%20and%20Taicheng%20Guo%20and%20Tuo%20Zhou%20and%20Wei%20Tao%20and%20Wenyi%20Wang%20and%20Xiangru%20Tang%20and%20Xiangtao%20Lu%20and%20Xiawu%20Zheng%20and%20Xinbing%20Liang%20and%20Yaying%20Fei%20and%20Yuheng%20Cheng%20and%20Zongze%20Xu%20and%20Chenglin%20Wu&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29-based%20agents%20have%20demonstrated%20remarkable%0Aeffectiveness.%20However%2C%20their%20performance%20can%20be%20compromised%20in%20data%20science%0Ascenarios%20that%20require%20real-time%20data%20adjustment%2C%20expertise%20in%20optimization%20due%0Ato%20complex%20dependencies%20among%20various%20tasks%2C%20and%20the%20ability%20to%20identify%0Alogical%20errors%20for%20precise%20reasoning.%20In%20this%20study%2C%20we%20introduce%20the%20Data%0AInterpreter%2C%20a%20solution%20designed%20to%20solve%20with%20code%20that%20emphasizes%20three%0Apivotal%20techniques%20to%20augment%20problem-solving%20in%20data%20science%3A%201%29%20dynamic%0Aplanning%20with%20hierarchical%20graph%20structures%20for%20real-time%20data%20adaptability%3B2%29%0Atool%20integration%20dynamically%20to%20enhance%20code%20proficiency%20during%20execution%2C%0Aenriching%20the%20requisite%20expertise%3B3%29%20logical%20inconsistency%20identification%20in%0Afeedback%2C%20and%20efficiency%20enhancement%20through%20experience%20recording.%20We%20evaluate%0Athe%20Data%20Interpreter%20on%20various%20data%20science%20and%20real-world%20tasks.%20Compared%20to%0Aopen-source%20baselines%2C%20it%20demonstrated%20superior%20performance%2C%20exhibiting%0Asignificant%20improvements%20in%20machine%20learning%20tasks%2C%20increasing%20from%200.86%20to%0A0.95.%20Additionally%2C%20it%20showed%20a%2026%25%20increase%20in%20the%20MATH%20dataset%20and%20a%0Aremarkable%20112%25%20improvement%20in%20open-ended%20tasks.%20The%20solution%20will%20be%20released%0Aat%20https%3A//github.com/geekan/MetaGPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18679v3&entry.124074799=Read"},
{"title": "MMSR: Symbolic Regression is a Multimodal Task", "author": "Yanjie Li and Jingyi Liu and Weijun Li and Lina Yu and Min Wu and Wenqiang Li and Meilan Hao and Su Wei and Yusong Deng", "abstract": "  Mathematical formulas are the crystallization of human wisdom in exploring\nthe laws of nature for thousands of years. Describing the complex laws of\nnature with a concise mathematical formula is a constant pursuit of scientists\nand a great challenge for artificial intelligence. This field is called\nsymbolic regression. Symbolic regression was originally formulated as a\ncombinatorial optimization problem, and GP and reinforcement learning\nalgorithms were used to solve it. However, GP is sensitive to hyperparameters,\nand these two types of algorithms are inefficient. To solve this problem,\nresearchers treat the mapping from data to expressions as a translation\nproblem. And the corresponding large-scale pre-trained model is introduced.\nHowever, the data and expression skeletons do not have very clear word\ncorrespondences as the two languages do. Instead, they are more like two\nmodalities (e.g., image and text). Therefore, in this paper, we proposed MMSR.\nThe SR problem is solved as a pure multimodal problem, and contrastive learning\nis also introduced in the training process for modal alignment to facilitate\nlater modal feature fusion. It is worth noting that in order to better promote\nthe modal feature fusion, we adopt the strategy of training contrastive\nlearning loss and other losses at the same time, which only needs one-step\ntraining, instead of training contrastive learning loss first and then training\nother losses. Because our experiments prove training together can make the\nfeature extraction module and feature fusion module running-in better.\nExperimental results show that compared with multiple large-scale pre-training\nbaselines, MMSR achieves the most advanced results on multiple mainstream\ndatasets including SRBench.\n", "link": "http://arxiv.org/abs/2402.18603v3", "date": "2024-03-12", "relevancy": 1.9851, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5258}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5151}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4657}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20MMSR%3A%20Symbolic%20Regression%20is%20a%20Multimodal%20Task&body=Title%3A%20MMSR%3A%20Symbolic%20Regression%20is%20a%20Multimodal%20Task%0AAuthor%3A%20Yanjie%20Li%20and%20Jingyi%20Liu%20and%20Weijun%20Li%20and%20Lina%20Yu%20and%20Min%20Wu%20and%20Wenqiang%20Li%20and%20Meilan%20Hao%20and%20Su%20Wei%20and%20Yusong%20Deng%0AAbstract%3A%20%20%20Mathematical%20formulas%20are%20the%20crystallization%20of%20human%20wisdom%20in%20exploring%0Athe%20laws%20of%20nature%20for%20thousands%20of%20years.%20Describing%20the%20complex%20laws%20of%0Anature%20with%20a%20concise%20mathematical%20formula%20is%20a%20constant%20pursuit%20of%20scientists%0Aand%20a%20great%20challenge%20for%20artificial%20intelligence.%20This%20field%20is%20called%0Asymbolic%20regression.%20Symbolic%20regression%20was%20originally%20formulated%20as%20a%0Acombinatorial%20optimization%20problem%2C%20and%20GP%20and%20reinforcement%20learning%0Aalgorithms%20were%20used%20to%20solve%20it.%20However%2C%20GP%20is%20sensitive%20to%20hyperparameters%2C%0Aand%20these%20two%20types%20of%20algorithms%20are%20inefficient.%20To%20solve%20this%20problem%2C%0Aresearchers%20treat%20the%20mapping%20from%20data%20to%20expressions%20as%20a%20translation%0Aproblem.%20And%20the%20corresponding%20large-scale%20pre-trained%20model%20is%20introduced.%0AHowever%2C%20the%20data%20and%20expression%20skeletons%20do%20not%20have%20very%20clear%20word%0Acorrespondences%20as%20the%20two%20languages%20do.%20Instead%2C%20they%20are%20more%20like%20two%0Amodalities%20%28e.g.%2C%20image%20and%20text%29.%20Therefore%2C%20in%20this%20paper%2C%20we%20proposed%20MMSR.%0AThe%20SR%20problem%20is%20solved%20as%20a%20pure%20multimodal%20problem%2C%20and%20contrastive%20learning%0Ais%20also%20introduced%20in%20the%20training%20process%20for%20modal%20alignment%20to%20facilitate%0Alater%20modal%20feature%20fusion.%20It%20is%20worth%20noting%20that%20in%20order%20to%20better%20promote%0Athe%20modal%20feature%20fusion%2C%20we%20adopt%20the%20strategy%20of%20training%20contrastive%0Alearning%20loss%20and%20other%20losses%20at%20the%20same%20time%2C%20which%20only%20needs%20one-step%0Atraining%2C%20instead%20of%20training%20contrastive%20learning%20loss%20first%20and%20then%20training%0Aother%20losses.%20Because%20our%20experiments%20prove%20training%20together%20can%20make%20the%0Afeature%20extraction%20module%20and%20feature%20fusion%20module%20running-in%20better.%0AExperimental%20results%20show%20that%20compared%20with%20multiple%20large-scale%20pre-training%0Abaselines%2C%20MMSR%20achieves%20the%20most%20advanced%20results%20on%20multiple%20mainstream%0Adatasets%20including%20SRBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18603v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMSR%3A%20Symbolic%20Regression%20is%20a%20Multimodal%20Task&entry.906535625=Yanjie%20Li%20and%20Jingyi%20Liu%20and%20Weijun%20Li%20and%20Lina%20Yu%20and%20Min%20Wu%20and%20Wenqiang%20Li%20and%20Meilan%20Hao%20and%20Su%20Wei%20and%20Yusong%20Deng&entry.1292438233=%20%20Mathematical%20formulas%20are%20the%20crystallization%20of%20human%20wisdom%20in%20exploring%0Athe%20laws%20of%20nature%20for%20thousands%20of%20years.%20Describing%20the%20complex%20laws%20of%0Anature%20with%20a%20concise%20mathematical%20formula%20is%20a%20constant%20pursuit%20of%20scientists%0Aand%20a%20great%20challenge%20for%20artificial%20intelligence.%20This%20field%20is%20called%0Asymbolic%20regression.%20Symbolic%20regression%20was%20originally%20formulated%20as%20a%0Acombinatorial%20optimization%20problem%2C%20and%20GP%20and%20reinforcement%20learning%0Aalgorithms%20were%20used%20to%20solve%20it.%20However%2C%20GP%20is%20sensitive%20to%20hyperparameters%2C%0Aand%20these%20two%20types%20of%20algorithms%20are%20inefficient.%20To%20solve%20this%20problem%2C%0Aresearchers%20treat%20the%20mapping%20from%20data%20to%20expressions%20as%20a%20translation%0Aproblem.%20And%20the%20corresponding%20large-scale%20pre-trained%20model%20is%20introduced.%0AHowever%2C%20the%20data%20and%20expression%20skeletons%20do%20not%20have%20very%20clear%20word%0Acorrespondences%20as%20the%20two%20languages%20do.%20Instead%2C%20they%20are%20more%20like%20two%0Amodalities%20%28e.g.%2C%20image%20and%20text%29.%20Therefore%2C%20in%20this%20paper%2C%20we%20proposed%20MMSR.%0AThe%20SR%20problem%20is%20solved%20as%20a%20pure%20multimodal%20problem%2C%20and%20contrastive%20learning%0Ais%20also%20introduced%20in%20the%20training%20process%20for%20modal%20alignment%20to%20facilitate%0Alater%20modal%20feature%20fusion.%20It%20is%20worth%20noting%20that%20in%20order%20to%20better%20promote%0Athe%20modal%20feature%20fusion%2C%20we%20adopt%20the%20strategy%20of%20training%20contrastive%0Alearning%20loss%20and%20other%20losses%20at%20the%20same%20time%2C%20which%20only%20needs%20one-step%0Atraining%2C%20instead%20of%20training%20contrastive%20learning%20loss%20first%20and%20then%20training%0Aother%20losses.%20Because%20our%20experiments%20prove%20training%20together%20can%20make%20the%0Afeature%20extraction%20module%20and%20feature%20fusion%20module%20running-in%20better.%0AExperimental%20results%20show%20that%20compared%20with%20multiple%20large-scale%20pre-training%0Abaselines%2C%20MMSR%20achieves%20the%20most%20advanced%20results%20on%20multiple%20mainstream%0Adatasets%20including%20SRBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18603v3&entry.124074799=Read"},
{"title": "Joint Selection: Adaptively Incorporating Public Information for Private\n  Synthetic Data", "author": "Miguel Fuentes and Brett Mullins and Ryan McKenna and Gerome Miklau and Daniel Sheldon", "abstract": "  Mechanisms for generating differentially private synthetic data based on\nmarginals and graphical models have been successful in a wide range of\nsettings. However, one limitation of these methods is their inability to\nincorporate public data. Initializing a data generating model by pre-training\non public data has shown to improve the quality of synthetic data, but this\ntechnique is not applicable when model structure is not determined a priori. We\ndevelop the mechanism jam-pgm, which expands the adaptive measurements\nframework to jointly select between measuring public data and private data.\nThis technique allows for public data to be included in a graphical-model-based\nmechanism. We show that jam-pgm is able to outperform both publicly assisted\nand non publicly assisted synthetic data generation mechanisms even when the\npublic data distribution is biased.\n", "link": "http://arxiv.org/abs/2403.07797v1", "date": "2024-03-12", "relevancy": 1.9773, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5085}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5051}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4759}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Joint%20Selection%3A%20Adaptively%20Incorporating%20Public%20Information%20for%20Private%0A%20%20Synthetic%20Data&body=Title%3A%20Joint%20Selection%3A%20Adaptively%20Incorporating%20Public%20Information%20for%20Private%0A%20%20Synthetic%20Data%0AAuthor%3A%20Miguel%20Fuentes%20and%20Brett%20Mullins%20and%20Ryan%20McKenna%20and%20Gerome%20Miklau%20and%20Daniel%20Sheldon%0AAbstract%3A%20%20%20Mechanisms%20for%20generating%20differentially%20private%20synthetic%20data%20based%20on%0Amarginals%20and%20graphical%20models%20have%20been%20successful%20in%20a%20wide%20range%20of%0Asettings.%20However%2C%20one%20limitation%20of%20these%20methods%20is%20their%20inability%20to%0Aincorporate%20public%20data.%20Initializing%20a%20data%20generating%20model%20by%20pre-training%0Aon%20public%20data%20has%20shown%20to%20improve%20the%20quality%20of%20synthetic%20data%2C%20but%20this%0Atechnique%20is%20not%20applicable%20when%20model%20structure%20is%20not%20determined%20a%20priori.%20We%0Adevelop%20the%20mechanism%20jam-pgm%2C%20which%20expands%20the%20adaptive%20measurements%0Aframework%20to%20jointly%20select%20between%20measuring%20public%20data%20and%20private%20data.%0AThis%20technique%20allows%20for%20public%20data%20to%20be%20included%20in%20a%20graphical-model-based%0Amechanism.%20We%20show%20that%20jam-pgm%20is%20able%20to%20outperform%20both%20publicly%20assisted%0Aand%20non%20publicly%20assisted%20synthetic%20data%20generation%20mechanisms%20even%20when%20the%0Apublic%20data%20distribution%20is%20biased.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07797v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Selection%3A%20Adaptively%20Incorporating%20Public%20Information%20for%20Private%0A%20%20Synthetic%20Data&entry.906535625=Miguel%20Fuentes%20and%20Brett%20Mullins%20and%20Ryan%20McKenna%20and%20Gerome%20Miklau%20and%20Daniel%20Sheldon&entry.1292438233=%20%20Mechanisms%20for%20generating%20differentially%20private%20synthetic%20data%20based%20on%0Amarginals%20and%20graphical%20models%20have%20been%20successful%20in%20a%20wide%20range%20of%0Asettings.%20However%2C%20one%20limitation%20of%20these%20methods%20is%20their%20inability%20to%0Aincorporate%20public%20data.%20Initializing%20a%20data%20generating%20model%20by%20pre-training%0Aon%20public%20data%20has%20shown%20to%20improve%20the%20quality%20of%20synthetic%20data%2C%20but%20this%0Atechnique%20is%20not%20applicable%20when%20model%20structure%20is%20not%20determined%20a%20priori.%20We%0Adevelop%20the%20mechanism%20jam-pgm%2C%20which%20expands%20the%20adaptive%20measurements%0Aframework%20to%20jointly%20select%20between%20measuring%20public%20data%20and%20private%20data.%0AThis%20technique%20allows%20for%20public%20data%20to%20be%20included%20in%20a%20graphical-model-based%0Amechanism.%20We%20show%20that%20jam-pgm%20is%20able%20to%20outperform%20both%20publicly%20assisted%0Aand%20non%20publicly%20assisted%20synthetic%20data%20generation%20mechanisms%20even%20when%20the%0Apublic%20data%20distribution%20is%20biased.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07797v1&entry.124074799=Read"},
{"title": "Geometry of the Visual Cortex with Applications to Image Inpainting and\n  Enhancement", "author": "Francesco Ballerin and Erlend Grong", "abstract": "  Equipping the rototranslation group $SE(2)$ with a sub-Riemannian structure\ninspired by the visual cortex V1, we propose algorithms for image inpainting\nand enhancement based on hypoelliptic diffusion. We innovate on previous\nimplementations of the methods by Citti, Sarti, and Boscain et al., by\nproposing an alternative that prevents fading and is capable of producing\nsharper results in a procedure that we call WaxOn-WaxOff. We also exploit the\nsub-Riemannian structure to define a completely new unsharp filter using\n$SE(2)$, analogous to the classical unsharp filter for 2D image processing. We\ndemonstrate our method on blood vessels enhancement in retinal scans.\n", "link": "http://arxiv.org/abs/2308.07652v2", "date": "2024-03-12", "relevancy": 1.9695, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5104}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4932}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4843}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Geometry%20of%20the%20Visual%20Cortex%20with%20Applications%20to%20Image%20Inpainting%20and%0A%20%20Enhancement&body=Title%3A%20Geometry%20of%20the%20Visual%20Cortex%20with%20Applications%20to%20Image%20Inpainting%20and%0A%20%20Enhancement%0AAuthor%3A%20Francesco%20Ballerin%20and%20Erlend%20Grong%0AAbstract%3A%20%20%20Equipping%20the%20rototranslation%20group%20%24SE%282%29%24%20with%20a%20sub-Riemannian%20structure%0Ainspired%20by%20the%20visual%20cortex%20V1%2C%20we%20propose%20algorithms%20for%20image%20inpainting%0Aand%20enhancement%20based%20on%20hypoelliptic%20diffusion.%20We%20innovate%20on%20previous%0Aimplementations%20of%20the%20methods%20by%20Citti%2C%20Sarti%2C%20and%20Boscain%20et%20al.%2C%20by%0Aproposing%20an%20alternative%20that%20prevents%20fading%20and%20is%20capable%20of%20producing%0Asharper%20results%20in%20a%20procedure%20that%20we%20call%20WaxOn-WaxOff.%20We%20also%20exploit%20the%0Asub-Riemannian%20structure%20to%20define%20a%20completely%20new%20unsharp%20filter%20using%0A%24SE%282%29%24%2C%20analogous%20to%20the%20classical%20unsharp%20filter%20for%202D%20image%20processing.%20We%0Ademonstrate%20our%20method%20on%20blood%20vessels%20enhancement%20in%20retinal%20scans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.07652v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry%20of%20the%20Visual%20Cortex%20with%20Applications%20to%20Image%20Inpainting%20and%0A%20%20Enhancement&entry.906535625=Francesco%20Ballerin%20and%20Erlend%20Grong&entry.1292438233=%20%20Equipping%20the%20rototranslation%20group%20%24SE%282%29%24%20with%20a%20sub-Riemannian%20structure%0Ainspired%20by%20the%20visual%20cortex%20V1%2C%20we%20propose%20algorithms%20for%20image%20inpainting%0Aand%20enhancement%20based%20on%20hypoelliptic%20diffusion.%20We%20innovate%20on%20previous%0Aimplementations%20of%20the%20methods%20by%20Citti%2C%20Sarti%2C%20and%20Boscain%20et%20al.%2C%20by%0Aproposing%20an%20alternative%20that%20prevents%20fading%20and%20is%20capable%20of%20producing%0Asharper%20results%20in%20a%20procedure%20that%20we%20call%20WaxOn-WaxOff.%20We%20also%20exploit%20the%0Asub-Riemannian%20structure%20to%20define%20a%20completely%20new%20unsharp%20filter%20using%0A%24SE%282%29%24%2C%20analogous%20to%20the%20classical%20unsharp%20filter%20for%202D%20image%20processing.%20We%0Ademonstrate%20our%20method%20on%20blood%20vessels%20enhancement%20in%20retinal%20scans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.07652v2&entry.124074799=Read"},
{"title": "Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining\n  of Explanations", "author": "Harish G. Naik and Jan Polster and Raj Shekhar and Tam\u00e1s Horv\u00e1th and Gy\u00f6rgy Tur\u00e1n", "abstract": "  We formulate an XAI-based model improvement approach for Graph Neural\nNetworks (GNNs) for node classification, called Explanation Enhanced Graph\nLearning (EEGL). The goal is to improve predictive performance of GNN using\nexplanations. EEGL is an iterative self-improving algorithm, which starts with\na learned \"vanilla\" GNN, and repeatedly uses frequent subgraph mining to find\nrelevant patterns in explanation subgraphs. These patterns are then filtered\nfurther to obtain application-dependent features corresponding to the presence\nof certain subgraphs in the node neighborhoods. Giving an application-dependent\nalgorithm for such a subgraph-based extension of the Weisfeiler-Leman (1-WL)\nalgorithm has previously been posed as an open problem. We present experimental\nevidence, with synthetic and real-world data, which show that EEGL outperforms\nrelated approaches in predictive performance and that it has a\nnode-distinguishing power beyond that of vanilla GNNs. We also analyze EEGL's\ntraining dynamics.\n", "link": "http://arxiv.org/abs/2403.07849v1", "date": "2024-03-12", "relevancy": 1.9481, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5175}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4833}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4581}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Iterative%20Graph%20Neural%20Network%20Enhancement%20via%20Frequent%20Subgraph%20Mining%0A%20%20of%20Explanations&body=Title%3A%20Iterative%20Graph%20Neural%20Network%20Enhancement%20via%20Frequent%20Subgraph%20Mining%0A%20%20of%20Explanations%0AAuthor%3A%20Harish%20G.%20Naik%20and%20Jan%20Polster%20and%20Raj%20Shekhar%20and%20Tam%C3%A1s%20Horv%C3%A1th%20and%20Gy%C3%B6rgy%20Tur%C3%A1n%0AAbstract%3A%20%20%20We%20formulate%20an%20XAI-based%20model%20improvement%20approach%20for%20Graph%20Neural%0ANetworks%20%28GNNs%29%20for%20node%20classification%2C%20called%20Explanation%20Enhanced%20Graph%0ALearning%20%28EEGL%29.%20The%20goal%20is%20to%20improve%20predictive%20performance%20of%20GNN%20using%0Aexplanations.%20EEGL%20is%20an%20iterative%20self-improving%20algorithm%2C%20which%20starts%20with%0Aa%20learned%20%22vanilla%22%20GNN%2C%20and%20repeatedly%20uses%20frequent%20subgraph%20mining%20to%20find%0Arelevant%20patterns%20in%20explanation%20subgraphs.%20These%20patterns%20are%20then%20filtered%0Afurther%20to%20obtain%20application-dependent%20features%20corresponding%20to%20the%20presence%0Aof%20certain%20subgraphs%20in%20the%20node%20neighborhoods.%20Giving%20an%20application-dependent%0Aalgorithm%20for%20such%20a%20subgraph-based%20extension%20of%20the%20Weisfeiler-Leman%20%281-WL%29%0Aalgorithm%20has%20previously%20been%20posed%20as%20an%20open%20problem.%20We%20present%20experimental%0Aevidence%2C%20with%20synthetic%20and%20real-world%20data%2C%20which%20show%20that%20EEGL%20outperforms%0Arelated%20approaches%20in%20predictive%20performance%20and%20that%20it%20has%20a%0Anode-distinguishing%20power%20beyond%20that%20of%20vanilla%20GNNs.%20We%20also%20analyze%20EEGL%27s%0Atraining%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07849v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Graph%20Neural%20Network%20Enhancement%20via%20Frequent%20Subgraph%20Mining%0A%20%20of%20Explanations&entry.906535625=Harish%20G.%20Naik%20and%20Jan%20Polster%20and%20Raj%20Shekhar%20and%20Tam%C3%A1s%20Horv%C3%A1th%20and%20Gy%C3%B6rgy%20Tur%C3%A1n&entry.1292438233=%20%20We%20formulate%20an%20XAI-based%20model%20improvement%20approach%20for%20Graph%20Neural%0ANetworks%20%28GNNs%29%20for%20node%20classification%2C%20called%20Explanation%20Enhanced%20Graph%0ALearning%20%28EEGL%29.%20The%20goal%20is%20to%20improve%20predictive%20performance%20of%20GNN%20using%0Aexplanations.%20EEGL%20is%20an%20iterative%20self-improving%20algorithm%2C%20which%20starts%20with%0Aa%20learned%20%22vanilla%22%20GNN%2C%20and%20repeatedly%20uses%20frequent%20subgraph%20mining%20to%20find%0Arelevant%20patterns%20in%20explanation%20subgraphs.%20These%20patterns%20are%20then%20filtered%0Afurther%20to%20obtain%20application-dependent%20features%20corresponding%20to%20the%20presence%0Aof%20certain%20subgraphs%20in%20the%20node%20neighborhoods.%20Giving%20an%20application-dependent%0Aalgorithm%20for%20such%20a%20subgraph-based%20extension%20of%20the%20Weisfeiler-Leman%20%281-WL%29%0Aalgorithm%20has%20previously%20been%20posed%20as%20an%20open%20problem.%20We%20present%20experimental%0Aevidence%2C%20with%20synthetic%20and%20real-world%20data%2C%20which%20show%20that%20EEGL%20outperforms%0Arelated%20approaches%20in%20predictive%20performance%20and%20that%20it%20has%20a%0Anode-distinguishing%20power%20beyond%20that%20of%20vanilla%20GNNs.%20We%20also%20analyze%20EEGL%27s%0Atraining%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07849v1&entry.124074799=Read"},
{"title": "BraSyn 2023 challenge: Missing MRI synthesis and the effect of different\n  learning objectives", "author": "Ivo M. Baltruschat and Parvaneh Janbakhshi and Matthias Lenga", "abstract": "  This work is addressing the Brain Magnetic Resonance Image Synthesis for\nTumor Segmentation (BraSyn) challenge which was hosted as part of the Brain\nTumor Segmentation challenge (BraTS) 2023. In this challenge researchers are\ninvited to work on synthesizing a missing magnetic resonance image sequence\ngiven other available sequences to facilitate tumor segmentation pipelines\ntrained on complete sets of image sequences. This problem can be addressed\nusing deep learning in the framework of paired images-to-image translation. In\nthis work, we proposed to investigate the effectiveness of a commonly-used deep\nlearning framework such as Pix2Pix trained under supervision of different\nimage-quality loss functions. Our results indicate that using different loss\nfunctions significantly affects the synthesis quality. We systematically study\nthe impact of different loss functions in the multi-sequence MR image synthesis\nsetting of the BraSyn challenge. Furthermore, we show how image synthesis\nperformance can be optimized by beneficially combining different learning\nobjectives.\n", "link": "http://arxiv.org/abs/2403.07800v1", "date": "2024-03-12", "relevancy": 1.9232, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4896}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4797}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4784}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20BraSyn%202023%20challenge%3A%20Missing%20MRI%20synthesis%20and%20the%20effect%20of%20different%0A%20%20learning%20objectives&body=Title%3A%20BraSyn%202023%20challenge%3A%20Missing%20MRI%20synthesis%20and%20the%20effect%20of%20different%0A%20%20learning%20objectives%0AAuthor%3A%20Ivo%20M.%20Baltruschat%20and%20Parvaneh%20Janbakhshi%20and%20Matthias%20Lenga%0AAbstract%3A%20%20%20This%20work%20is%20addressing%20the%20Brain%20Magnetic%20Resonance%20Image%20Synthesis%20for%0ATumor%20Segmentation%20%28BraSyn%29%20challenge%20which%20was%20hosted%20as%20part%20of%20the%20Brain%0ATumor%20Segmentation%20challenge%20%28BraTS%29%202023.%20In%20this%20challenge%20researchers%20are%0Ainvited%20to%20work%20on%20synthesizing%20a%20missing%20magnetic%20resonance%20image%20sequence%0Agiven%20other%20available%20sequences%20to%20facilitate%20tumor%20segmentation%20pipelines%0Atrained%20on%20complete%20sets%20of%20image%20sequences.%20This%20problem%20can%20be%20addressed%0Ausing%20deep%20learning%20in%20the%20framework%20of%20paired%20images-to-image%20translation.%20In%0Athis%20work%2C%20we%20proposed%20to%20investigate%20the%20effectiveness%20of%20a%20commonly-used%20deep%0Alearning%20framework%20such%20as%20Pix2Pix%20trained%20under%20supervision%20of%20different%0Aimage-quality%20loss%20functions.%20Our%20results%20indicate%20that%20using%20different%20loss%0Afunctions%20significantly%20affects%20the%20synthesis%20quality.%20We%20systematically%20study%0Athe%20impact%20of%20different%20loss%20functions%20in%20the%20multi-sequence%20MR%20image%20synthesis%0Asetting%20of%20the%20BraSyn%20challenge.%20Furthermore%2C%20we%20show%20how%20image%20synthesis%0Aperformance%20can%20be%20optimized%20by%20beneficially%20combining%20different%20learning%0Aobjectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07800v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BraSyn%202023%20challenge%3A%20Missing%20MRI%20synthesis%20and%20the%20effect%20of%20different%0A%20%20learning%20objectives&entry.906535625=Ivo%20M.%20Baltruschat%20and%20Parvaneh%20Janbakhshi%20and%20Matthias%20Lenga&entry.1292438233=%20%20This%20work%20is%20addressing%20the%20Brain%20Magnetic%20Resonance%20Image%20Synthesis%20for%0ATumor%20Segmentation%20%28BraSyn%29%20challenge%20which%20was%20hosted%20as%20part%20of%20the%20Brain%0ATumor%20Segmentation%20challenge%20%28BraTS%29%202023.%20In%20this%20challenge%20researchers%20are%0Ainvited%20to%20work%20on%20synthesizing%20a%20missing%20magnetic%20resonance%20image%20sequence%0Agiven%20other%20available%20sequences%20to%20facilitate%20tumor%20segmentation%20pipelines%0Atrained%20on%20complete%20sets%20of%20image%20sequences.%20This%20problem%20can%20be%20addressed%0Ausing%20deep%20learning%20in%20the%20framework%20of%20paired%20images-to-image%20translation.%20In%0Athis%20work%2C%20we%20proposed%20to%20investigate%20the%20effectiveness%20of%20a%20commonly-used%20deep%0Alearning%20framework%20such%20as%20Pix2Pix%20trained%20under%20supervision%20of%20different%0Aimage-quality%20loss%20functions.%20Our%20results%20indicate%20that%20using%20different%20loss%0Afunctions%20significantly%20affects%20the%20synthesis%20quality.%20We%20systematically%20study%0Athe%20impact%20of%20different%20loss%20functions%20in%20the%20multi-sequence%20MR%20image%20synthesis%0Asetting%20of%20the%20BraSyn%20challenge.%20Furthermore%2C%20we%20show%20how%20image%20synthesis%0Aperformance%20can%20be%20optimized%20by%20beneficially%20combining%20different%20learning%0Aobjectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07800v1&entry.124074799=Read"},
{"title": "Fine-tuning of diffusion models via stochastic control: entropy\n  regularization and beyond", "author": "Wenpin Tang", "abstract": "  This paper aims to develop and provide a rigorous treatment to the problem of\nentropy regularized fine-tuning in the context of continuous-time diffusion\nmodels, which was recently proposed by Uehara et al. (arXiv:2402.15194, 2024).\nThe idea is to use stochastic control for sample generation, where the entropy\nregularizer is introduced to mitigate reward collapse. We also show how the\nanalysis can be extended to fine-tuning involving a general $f$-divergence\nregularizer.\n", "link": "http://arxiv.org/abs/2403.06279v2", "date": "2024-03-12", "relevancy": 1.9137, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4974}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4751}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4608}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Fine-tuning%20of%20diffusion%20models%20via%20stochastic%20control%3A%20entropy%0A%20%20regularization%20and%20beyond&body=Title%3A%20Fine-tuning%20of%20diffusion%20models%20via%20stochastic%20control%3A%20entropy%0A%20%20regularization%20and%20beyond%0AAuthor%3A%20Wenpin%20Tang%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20develop%20and%20provide%20a%20rigorous%20treatment%20to%20the%20problem%20of%0Aentropy%20regularized%20fine-tuning%20in%20the%20context%20of%20continuous-time%20diffusion%0Amodels%2C%20which%20was%20recently%20proposed%20by%20Uehara%20et%20al.%20%28arXiv%3A2402.15194%2C%202024%29.%0AThe%20idea%20is%20to%20use%20stochastic%20control%20for%20sample%20generation%2C%20where%20the%20entropy%0Aregularizer%20is%20introduced%20to%20mitigate%20reward%20collapse.%20We%20also%20show%20how%20the%0Aanalysis%20can%20be%20extended%20to%20fine-tuning%20involving%20a%20general%20%24f%24-divergence%0Aregularizer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06279v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-tuning%20of%20diffusion%20models%20via%20stochastic%20control%3A%20entropy%0A%20%20regularization%20and%20beyond&entry.906535625=Wenpin%20Tang&entry.1292438233=%20%20This%20paper%20aims%20to%20develop%20and%20provide%20a%20rigorous%20treatment%20to%20the%20problem%20of%0Aentropy%20regularized%20fine-tuning%20in%20the%20context%20of%20continuous-time%20diffusion%0Amodels%2C%20which%20was%20recently%20proposed%20by%20Uehara%20et%20al.%20%28arXiv%3A2402.15194%2C%202024%29.%0AThe%20idea%20is%20to%20use%20stochastic%20control%20for%20sample%20generation%2C%20where%20the%20entropy%0Aregularizer%20is%20introduced%20to%20mitigate%20reward%20collapse.%20We%20also%20show%20how%20the%0Aanalysis%20can%20be%20extended%20to%20fine-tuning%20involving%20a%20general%20%24f%24-divergence%0Aregularizer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06279v2&entry.124074799=Read"},
{"title": "When Eye-Tracking Meets Machine Learning: A Systematic Review on\n  Applications in Medical Image Analysis", "author": "Sahar Moradizeyveh and Mehnaz Tabassum and Sidong Liu and Robert Ahadizad Newport and Amin Beheshti and Antonio Di Ieva", "abstract": "  Eye-gaze tracking research offers significant promise in enhancing various\nhealthcare-related tasks, above all in medical image analysis and\ninterpretation. Eye tracking, a technology that monitors and records the\nmovement of the eyes, provides valuable insights into human visual attention\npatterns. This technology can transform how healthcare professionals and\nmedical specialists engage with and analyze diagnostic images, offering a more\ninsightful and efficient approach to medical diagnostics. Hence, extracting\nmeaningful features and insights from medical images by leveraging eye-gaze\ndata improves our understanding of how radiologists and other medical experts\nmonitor, interpret, and understand images for diagnostic purposes. Eye-tracking\ndata, with intricate human visual attention patterns embedded, provides a\nbridge to integrating artificial intelligence (AI) development and human\ncognition. This integration allows novel methods to incorporate domain\nknowledge into machine learning (ML) and deep learning (DL) approaches to\nenhance their alignment with human-like perception and decision-making.\nMoreover, extensive collections of eye-tracking data have also enabled novel\nML/DL methods to analyze human visual patterns, paving the way to a better\nunderstanding of human vision, attention, and cognition. This systematic review\ninvestigates eye-gaze tracking applications and methodologies for enhancing\nML/DL algorithms for medical image analysis in depth.\n", "link": "http://arxiv.org/abs/2403.07834v1", "date": "2024-03-12", "relevancy": 1.8846, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4907}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4672}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4532}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20When%20Eye-Tracking%20Meets%20Machine%20Learning%3A%20A%20Systematic%20Review%20on%0A%20%20Applications%20in%20Medical%20Image%20Analysis&body=Title%3A%20When%20Eye-Tracking%20Meets%20Machine%20Learning%3A%20A%20Systematic%20Review%20on%0A%20%20Applications%20in%20Medical%20Image%20Analysis%0AAuthor%3A%20Sahar%20Moradizeyveh%20and%20Mehnaz%20Tabassum%20and%20Sidong%20Liu%20and%20Robert%20Ahadizad%20Newport%20and%20Amin%20Beheshti%20and%20Antonio%20Di%20Ieva%0AAbstract%3A%20%20%20Eye-gaze%20tracking%20research%20offers%20significant%20promise%20in%20enhancing%20various%0Ahealthcare-related%20tasks%2C%20above%20all%20in%20medical%20image%20analysis%20and%0Ainterpretation.%20Eye%20tracking%2C%20a%20technology%20that%20monitors%20and%20records%20the%0Amovement%20of%20the%20eyes%2C%20provides%20valuable%20insights%20into%20human%20visual%20attention%0Apatterns.%20This%20technology%20can%20transform%20how%20healthcare%20professionals%20and%0Amedical%20specialists%20engage%20with%20and%20analyze%20diagnostic%20images%2C%20offering%20a%20more%0Ainsightful%20and%20efficient%20approach%20to%20medical%20diagnostics.%20Hence%2C%20extracting%0Ameaningful%20features%20and%20insights%20from%20medical%20images%20by%20leveraging%20eye-gaze%0Adata%20improves%20our%20understanding%20of%20how%20radiologists%20and%20other%20medical%20experts%0Amonitor%2C%20interpret%2C%20and%20understand%20images%20for%20diagnostic%20purposes.%20Eye-tracking%0Adata%2C%20with%20intricate%20human%20visual%20attention%20patterns%20embedded%2C%20provides%20a%0Abridge%20to%20integrating%20artificial%20intelligence%20%28AI%29%20development%20and%20human%0Acognition.%20This%20integration%20allows%20novel%20methods%20to%20incorporate%20domain%0Aknowledge%20into%20machine%20learning%20%28ML%29%20and%20deep%20learning%20%28DL%29%20approaches%20to%0Aenhance%20their%20alignment%20with%20human-like%20perception%20and%20decision-making.%0AMoreover%2C%20extensive%20collections%20of%20eye-tracking%20data%20have%20also%20enabled%20novel%0AML/DL%20methods%20to%20analyze%20human%20visual%20patterns%2C%20paving%20the%20way%20to%20a%20better%0Aunderstanding%20of%20human%20vision%2C%20attention%2C%20and%20cognition.%20This%20systematic%20review%0Ainvestigates%20eye-gaze%20tracking%20applications%20and%20methodologies%20for%20enhancing%0AML/DL%20algorithms%20for%20medical%20image%20analysis%20in%20depth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07834v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Eye-Tracking%20Meets%20Machine%20Learning%3A%20A%20Systematic%20Review%20on%0A%20%20Applications%20in%20Medical%20Image%20Analysis&entry.906535625=Sahar%20Moradizeyveh%20and%20Mehnaz%20Tabassum%20and%20Sidong%20Liu%20and%20Robert%20Ahadizad%20Newport%20and%20Amin%20Beheshti%20and%20Antonio%20Di%20Ieva&entry.1292438233=%20%20Eye-gaze%20tracking%20research%20offers%20significant%20promise%20in%20enhancing%20various%0Ahealthcare-related%20tasks%2C%20above%20all%20in%20medical%20image%20analysis%20and%0Ainterpretation.%20Eye%20tracking%2C%20a%20technology%20that%20monitors%20and%20records%20the%0Amovement%20of%20the%20eyes%2C%20provides%20valuable%20insights%20into%20human%20visual%20attention%0Apatterns.%20This%20technology%20can%20transform%20how%20healthcare%20professionals%20and%0Amedical%20specialists%20engage%20with%20and%20analyze%20diagnostic%20images%2C%20offering%20a%20more%0Ainsightful%20and%20efficient%20approach%20to%20medical%20diagnostics.%20Hence%2C%20extracting%0Ameaningful%20features%20and%20insights%20from%20medical%20images%20by%20leveraging%20eye-gaze%0Adata%20improves%20our%20understanding%20of%20how%20radiologists%20and%20other%20medical%20experts%0Amonitor%2C%20interpret%2C%20and%20understand%20images%20for%20diagnostic%20purposes.%20Eye-tracking%0Adata%2C%20with%20intricate%20human%20visual%20attention%20patterns%20embedded%2C%20provides%20a%0Abridge%20to%20integrating%20artificial%20intelligence%20%28AI%29%20development%20and%20human%0Acognition.%20This%20integration%20allows%20novel%20methods%20to%20incorporate%20domain%0Aknowledge%20into%20machine%20learning%20%28ML%29%20and%20deep%20learning%20%28DL%29%20approaches%20to%0Aenhance%20their%20alignment%20with%20human-like%20perception%20and%20decision-making.%0AMoreover%2C%20extensive%20collections%20of%20eye-tracking%20data%20have%20also%20enabled%20novel%0AML/DL%20methods%20to%20analyze%20human%20visual%20patterns%2C%20paving%20the%20way%20to%20a%20better%0Aunderstanding%20of%20human%20vision%2C%20attention%2C%20and%20cognition.%20This%20systematic%20review%0Ainvestigates%20eye-gaze%20tracking%20applications%20and%20methodologies%20for%20enhancing%0AML/DL%20algorithms%20for%20medical%20image%20analysis%20in%20depth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07834v1&entry.124074799=Read"},
{"title": "Unveiling the Pitfalls of Knowledge Editing for Large Language Models", "author": "Zhoubo Li and Ningyu Zhang and Yunzhi Yao and Mengru Wang and Xi Chen and Huajun Chen", "abstract": "  As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.\n", "link": "http://arxiv.org/abs/2310.02129v3", "date": "2024-03-12", "relevancy": 1.8699, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5256}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4636}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4481}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20Pitfalls%20of%20Knowledge%20Editing%20for%20Large%20Language%20Models&body=Title%3A%20Unveiling%20the%20Pitfalls%20of%20Knowledge%20Editing%20for%20Large%20Language%20Models%0AAuthor%3A%20Zhoubo%20Li%20and%20Ningyu%20Zhang%20and%20Yunzhi%20Yao%20and%20Mengru%20Wang%20and%20Xi%20Chen%20and%20Huajun%20Chen%0AAbstract%3A%20%20%20As%20the%20cost%20associated%20with%20fine-tuning%20Large%20Language%20Models%20%28LLMs%29%0Acontinues%20to%20rise%2C%20recent%20research%20efforts%20have%20pivoted%20towards%20developing%0Amethodologies%20to%20edit%20implicit%20knowledge%20embedded%20within%20LLMs.%20Yet%2C%20there%27s%0Astill%20a%20dark%20cloud%20lingering%20overhead%20--%20will%20knowledge%20editing%20trigger%0Abutterfly%20effect%3F%20since%20it%20is%20still%20unclear%20whether%20knowledge%20editing%20might%0Aintroduce%20side%20effects%20that%20pose%20potential%20risks%20or%20not.%20This%20paper%20pioneers%0Athe%20investigation%20into%20the%20potential%20pitfalls%20associated%20with%20knowledge%20editing%0Afor%20LLMs.%20To%20achieve%20this%2C%20we%20introduce%20new%20benchmark%20datasets%20and%20propose%0Ainnovative%20evaluation%20metrics.%20Our%20results%20underline%20two%20pivotal%20concerns%3A%20%281%29%0AKnowledge%20Conflict%3A%20Editing%20groups%20of%20facts%20that%20logically%20clash%20can%20magnify%0Athe%20inherent%20inconsistencies%20in%20LLMs-a%20facet%20neglected%20by%20previous%20methods.%20%282%29%0AKnowledge%20Distortion%3A%20Altering%20parameters%20with%20the%20aim%20of%20editing%20factual%0Aknowledge%20can%20irrevocably%20warp%20the%20innate%20knowledge%20structure%20of%20LLMs.%0AExperimental%20results%20vividly%20demonstrate%20that%20knowledge%20editing%20might%0Ainadvertently%20cast%20a%20shadow%20of%20unintended%20consequences%20on%20LLMs%2C%20which%20warrant%0Aattention%20and%20efforts%20for%20future%20works.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/zjunlp/PitfallsKnowledgeEditing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02129v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20Pitfalls%20of%20Knowledge%20Editing%20for%20Large%20Language%20Models&entry.906535625=Zhoubo%20Li%20and%20Ningyu%20Zhang%20and%20Yunzhi%20Yao%20and%20Mengru%20Wang%20and%20Xi%20Chen%20and%20Huajun%20Chen&entry.1292438233=%20%20As%20the%20cost%20associated%20with%20fine-tuning%20Large%20Language%20Models%20%28LLMs%29%0Acontinues%20to%20rise%2C%20recent%20research%20efforts%20have%20pivoted%20towards%20developing%0Amethodologies%20to%20edit%20implicit%20knowledge%20embedded%20within%20LLMs.%20Yet%2C%20there%27s%0Astill%20a%20dark%20cloud%20lingering%20overhead%20--%20will%20knowledge%20editing%20trigger%0Abutterfly%20effect%3F%20since%20it%20is%20still%20unclear%20whether%20knowledge%20editing%20might%0Aintroduce%20side%20effects%20that%20pose%20potential%20risks%20or%20not.%20This%20paper%20pioneers%0Athe%20investigation%20into%20the%20potential%20pitfalls%20associated%20with%20knowledge%20editing%0Afor%20LLMs.%20To%20achieve%20this%2C%20we%20introduce%20new%20benchmark%20datasets%20and%20propose%0Ainnovative%20evaluation%20metrics.%20Our%20results%20underline%20two%20pivotal%20concerns%3A%20%281%29%0AKnowledge%20Conflict%3A%20Editing%20groups%20of%20facts%20that%20logically%20clash%20can%20magnify%0Athe%20inherent%20inconsistencies%20in%20LLMs-a%20facet%20neglected%20by%20previous%20methods.%20%282%29%0AKnowledge%20Distortion%3A%20Altering%20parameters%20with%20the%20aim%20of%20editing%20factual%0Aknowledge%20can%20irrevocably%20warp%20the%20innate%20knowledge%20structure%20of%20LLMs.%0AExperimental%20results%20vividly%20demonstrate%20that%20knowledge%20editing%20might%0Ainadvertently%20cast%20a%20shadow%20of%20unintended%20consequences%20on%20LLMs%2C%20which%20warrant%0Aattention%20and%20efforts%20for%20future%20works.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/zjunlp/PitfallsKnowledgeEditing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02129v3&entry.124074799=Read"},
{"title": "Beyond Memorization: The Challenge of Random Memory Access in Language\n  Models", "author": "Tongyao Zhu and Qian Liu and Liang Pang and Zhengbao Jiang and Min-Yen Kan and Min Lin", "abstract": "  Recent developments in Language Models (LMs) have shown their effectiveness\nin NLP tasks, particularly in knowledge-intensive tasks. However, the\nmechanisms underlying knowledge storage and memory access within their\nparameters remain elusive. In this paper, we investigate whether a generative\nLM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through\ncarefully-designed synthetic tasks, covering the scenarios of full recitation,\nselective recitation and grounded question answering, we reveal that LMs manage\nto sequentially access their memory while encountering challenges in randomly\naccessing memorized content. We find that techniques including recitation and\npermutation improve the random memory access capability of LMs. Furthermore, by\napplying this intervention to realistic scenarios of open-domain question\nanswering, we validate that enhancing random access by recitation leads to\nnotable improvements in question answering. The code to reproduce our\nexperiments can be found at https://github.\ncom/sail-sg/lm-random-memory-access.\n", "link": "http://arxiv.org/abs/2403.07805v1", "date": "2024-03-12", "relevancy": 1.8578, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4677}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4655}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4621}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Beyond%20Memorization%3A%20The%20Challenge%20of%20Random%20Memory%20Access%20in%20Language%0A%20%20Models&body=Title%3A%20Beyond%20Memorization%3A%20The%20Challenge%20of%20Random%20Memory%20Access%20in%20Language%0A%20%20Models%0AAuthor%3A%20Tongyao%20Zhu%20and%20Qian%20Liu%20and%20Liang%20Pang%20and%20Zhengbao%20Jiang%20and%20Min-Yen%20Kan%20and%20Min%20Lin%0AAbstract%3A%20%20%20Recent%20developments%20in%20Language%20Models%20%28LMs%29%20have%20shown%20their%20effectiveness%0Ain%20NLP%20tasks%2C%20particularly%20in%20knowledge-intensive%20tasks.%20However%2C%20the%0Amechanisms%20underlying%20knowledge%20storage%20and%20memory%20access%20within%20their%0Aparameters%20remain%20elusive.%20In%20this%20paper%2C%20we%20investigate%20whether%20a%20generative%0ALM%20%28e.g.%2C%20GPT-2%29%20is%20able%20to%20access%20its%20memory%20sequentially%20or%20randomly.%20Through%0Acarefully-designed%20synthetic%20tasks%2C%20covering%20the%20scenarios%20of%20full%20recitation%2C%0Aselective%20recitation%20and%20grounded%20question%20answering%2C%20we%20reveal%20that%20LMs%20manage%0Ato%20sequentially%20access%20their%20memory%20while%20encountering%20challenges%20in%20randomly%0Aaccessing%20memorized%20content.%20We%20find%20that%20techniques%20including%20recitation%20and%0Apermutation%20improve%20the%20random%20memory%20access%20capability%20of%20LMs.%20Furthermore%2C%20by%0Aapplying%20this%20intervention%20to%20realistic%20scenarios%20of%20open-domain%20question%0Aanswering%2C%20we%20validate%20that%20enhancing%20random%20access%20by%20recitation%20leads%20to%0Anotable%20improvements%20in%20question%20answering.%20The%20code%20to%20reproduce%20our%0Aexperiments%20can%20be%20found%20at%20https%3A//github.%0Acom/sail-sg/lm-random-memory-access.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07805v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Memorization%3A%20The%20Challenge%20of%20Random%20Memory%20Access%20in%20Language%0A%20%20Models&entry.906535625=Tongyao%20Zhu%20and%20Qian%20Liu%20and%20Liang%20Pang%20and%20Zhengbao%20Jiang%20and%20Min-Yen%20Kan%20and%20Min%20Lin&entry.1292438233=%20%20Recent%20developments%20in%20Language%20Models%20%28LMs%29%20have%20shown%20their%20effectiveness%0Ain%20NLP%20tasks%2C%20particularly%20in%20knowledge-intensive%20tasks.%20However%2C%20the%0Amechanisms%20underlying%20knowledge%20storage%20and%20memory%20access%20within%20their%0Aparameters%20remain%20elusive.%20In%20this%20paper%2C%20we%20investigate%20whether%20a%20generative%0ALM%20%28e.g.%2C%20GPT-2%29%20is%20able%20to%20access%20its%20memory%20sequentially%20or%20randomly.%20Through%0Acarefully-designed%20synthetic%20tasks%2C%20covering%20the%20scenarios%20of%20full%20recitation%2C%0Aselective%20recitation%20and%20grounded%20question%20answering%2C%20we%20reveal%20that%20LMs%20manage%0Ato%20sequentially%20access%20their%20memory%20while%20encountering%20challenges%20in%20randomly%0Aaccessing%20memorized%20content.%20We%20find%20that%20techniques%20including%20recitation%20and%0Apermutation%20improve%20the%20random%20memory%20access%20capability%20of%20LMs.%20Furthermore%2C%20by%0Aapplying%20this%20intervention%20to%20realistic%20scenarios%20of%20open-domain%20question%0Aanswering%2C%20we%20validate%20that%20enhancing%20random%20access%20by%20recitation%20leads%20to%0Anotable%20improvements%20in%20question%20answering.%20The%20code%20to%20reproduce%20our%0Aexperiments%20can%20be%20found%20at%20https%3A//github.%0Acom/sail-sg/lm-random-memory-access.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07805v1&entry.124074799=Read"},
{"title": "Group Decision-Making among Privacy-Aware Agents", "author": "Marios Papachristou and M. Amin Rahimian", "abstract": "  How can individuals exchange information to learn from each other despite\ntheir privacy needs and security concerns? For example, consider individuals\ndeliberating a contentious topic and being concerned about divulging their\nprivate experiences. Preserving individual privacy and enabling efficient\nsocial learning are both important desiderata but seem fundamentally at odds\nwith each other and very hard to reconcile. We do so by controlling information\nleakage using rigorous statistical guarantees that are based on differential\nprivacy (DP). Our agents use log-linear rules to update their beliefs after\ncommunicating with their neighbors. Adding DP randomization noise to beliefs\nprovides communicating agents with plausible deniability with regard to their\nprivate information and their network neighborhoods. We consider two learning\nenvironments one for distributed maximum-likelihood estimation given a finite\nnumber of private signals and another for online learning from an infinite,\nintermittent signal stream. Noisy information aggregation in the finite case\nleads to interesting tradeoffs between rejecting low-quality states and making\nsure all high-quality states are accepted in the algorithm output. Our results\nflesh out the nature of the trade-offs in both cases between the quality of the\ngroup decision outcomes, learning accuracy, communication cost, and the level\nof privacy protections that the agents are afforded.\n", "link": "http://arxiv.org/abs/2402.08156v3", "date": "2024-03-12", "relevancy": 1.8302, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4654}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4565}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4407}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Group%20Decision-Making%20among%20Privacy-Aware%20Agents&body=Title%3A%20Group%20Decision-Making%20among%20Privacy-Aware%20Agents%0AAuthor%3A%20Marios%20Papachristou%20and%20M.%20Amin%20Rahimian%0AAbstract%3A%20%20%20How%20can%20individuals%20exchange%20information%20to%20learn%20from%20each%20other%20despite%0Atheir%20privacy%20needs%20and%20security%20concerns%3F%20For%20example%2C%20consider%20individuals%0Adeliberating%20a%20contentious%20topic%20and%20being%20concerned%20about%20divulging%20their%0Aprivate%20experiences.%20Preserving%20individual%20privacy%20and%20enabling%20efficient%0Asocial%20learning%20are%20both%20important%20desiderata%20but%20seem%20fundamentally%20at%20odds%0Awith%20each%20other%20and%20very%20hard%20to%20reconcile.%20We%20do%20so%20by%20controlling%20information%0Aleakage%20using%20rigorous%20statistical%20guarantees%20that%20are%20based%20on%20differential%0Aprivacy%20%28DP%29.%20Our%20agents%20use%20log-linear%20rules%20to%20update%20their%20beliefs%20after%0Acommunicating%20with%20their%20neighbors.%20Adding%20DP%20randomization%20noise%20to%20beliefs%0Aprovides%20communicating%20agents%20with%20plausible%20deniability%20with%20regard%20to%20their%0Aprivate%20information%20and%20their%20network%20neighborhoods.%20We%20consider%20two%20learning%0Aenvironments%20one%20for%20distributed%20maximum-likelihood%20estimation%20given%20a%20finite%0Anumber%20of%20private%20signals%20and%20another%20for%20online%20learning%20from%20an%20infinite%2C%0Aintermittent%20signal%20stream.%20Noisy%20information%20aggregation%20in%20the%20finite%20case%0Aleads%20to%20interesting%20tradeoffs%20between%20rejecting%20low-quality%20states%20and%20making%0Asure%20all%20high-quality%20states%20are%20accepted%20in%20the%20algorithm%20output.%20Our%20results%0Aflesh%20out%20the%20nature%20of%20the%20trade-offs%20in%20both%20cases%20between%20the%20quality%20of%20the%0Agroup%20decision%20outcomes%2C%20learning%20accuracy%2C%20communication%20cost%2C%20and%20the%20level%0Aof%20privacy%20protections%20that%20the%20agents%20are%20afforded.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08156v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Group%20Decision-Making%20among%20Privacy-Aware%20Agents&entry.906535625=Marios%20Papachristou%20and%20M.%20Amin%20Rahimian&entry.1292438233=%20%20How%20can%20individuals%20exchange%20information%20to%20learn%20from%20each%20other%20despite%0Atheir%20privacy%20needs%20and%20security%20concerns%3F%20For%20example%2C%20consider%20individuals%0Adeliberating%20a%20contentious%20topic%20and%20being%20concerned%20about%20divulging%20their%0Aprivate%20experiences.%20Preserving%20individual%20privacy%20and%20enabling%20efficient%0Asocial%20learning%20are%20both%20important%20desiderata%20but%20seem%20fundamentally%20at%20odds%0Awith%20each%20other%20and%20very%20hard%20to%20reconcile.%20We%20do%20so%20by%20controlling%20information%0Aleakage%20using%20rigorous%20statistical%20guarantees%20that%20are%20based%20on%20differential%0Aprivacy%20%28DP%29.%20Our%20agents%20use%20log-linear%20rules%20to%20update%20their%20beliefs%20after%0Acommunicating%20with%20their%20neighbors.%20Adding%20DP%20randomization%20noise%20to%20beliefs%0Aprovides%20communicating%20agents%20with%20plausible%20deniability%20with%20regard%20to%20their%0Aprivate%20information%20and%20their%20network%20neighborhoods.%20We%20consider%20two%20learning%0Aenvironments%20one%20for%20distributed%20maximum-likelihood%20estimation%20given%20a%20finite%0Anumber%20of%20private%20signals%20and%20another%20for%20online%20learning%20from%20an%20infinite%2C%0Aintermittent%20signal%20stream.%20Noisy%20information%20aggregation%20in%20the%20finite%20case%0Aleads%20to%20interesting%20tradeoffs%20between%20rejecting%20low-quality%20states%20and%20making%0Asure%20all%20high-quality%20states%20are%20accepted%20in%20the%20algorithm%20output.%20Our%20results%0Aflesh%20out%20the%20nature%20of%20the%20trade-offs%20in%20both%20cases%20between%20the%20quality%20of%20the%0Agroup%20decision%20outcomes%2C%20learning%20accuracy%2C%20communication%20cost%2C%20and%20the%20level%0Aof%20privacy%20protections%20that%20the%20agents%20are%20afforded.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08156v3&entry.124074799=Read"},
{"title": "Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias", "author": "Sierra Wyllie and Ilia Shumailov and Nicolas Papernot", "abstract": "  Model-induced distribution shifts (MIDS) occur as previous model outputs\npollute new model training sets over generations of models. This is known as\nmodel collapse in the case of generative models, and performative prediction or\nunfairness feedback loops for supervised models. When a model induces a\ndistribution shift, it also encodes its mistakes, biases, and unfairnesses into\nthe ground truth of its data ecosystem. We introduce a framework that allows us\nto track multiple MIDS over many generations, finding that they can lead to\nloss in performance, fairness, and minoritized group representation, even in\ninitially unbiased datasets. Despite these negative consequences, we identify\nhow models might be used for positive, intentional, interventions in their data\necosystems, providing redress for historical discrimination through a framework\ncalled algorithmic reparation (AR). We simulate AR interventions by curating\nrepresentative training batches for stochastic gradient descent to demonstrate\nhow AR can improve upon the unfairnesses of models and data ecosystems subject\nto other MIDS. Our work takes an important step towards identifying,\nmitigating, and taking accountability for the unfair feedback loops enabled by\nthe idea that ML systems are inherently neutral and objective.\n", "link": "http://arxiv.org/abs/2403.07857v1", "date": "2024-03-12", "relevancy": 1.8262, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4962}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.452}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4452}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Fairness%20Feedback%20Loops%3A%20Training%20on%20Synthetic%20Data%20Amplifies%20Bias&body=Title%3A%20Fairness%20Feedback%20Loops%3A%20Training%20on%20Synthetic%20Data%20Amplifies%20Bias%0AAuthor%3A%20Sierra%20Wyllie%20and%20Ilia%20Shumailov%20and%20Nicolas%20Papernot%0AAbstract%3A%20%20%20Model-induced%20distribution%20shifts%20%28MIDS%29%20occur%20as%20previous%20model%20outputs%0Apollute%20new%20model%20training%20sets%20over%20generations%20of%20models.%20This%20is%20known%20as%0Amodel%20collapse%20in%20the%20case%20of%20generative%20models%2C%20and%20performative%20prediction%20or%0Aunfairness%20feedback%20loops%20for%20supervised%20models.%20When%20a%20model%20induces%20a%0Adistribution%20shift%2C%20it%20also%20encodes%20its%20mistakes%2C%20biases%2C%20and%20unfairnesses%20into%0Athe%20ground%20truth%20of%20its%20data%20ecosystem.%20We%20introduce%20a%20framework%20that%20allows%20us%0Ato%20track%20multiple%20MIDS%20over%20many%20generations%2C%20finding%20that%20they%20can%20lead%20to%0Aloss%20in%20performance%2C%20fairness%2C%20and%20minoritized%20group%20representation%2C%20even%20in%0Ainitially%20unbiased%20datasets.%20Despite%20these%20negative%20consequences%2C%20we%20identify%0Ahow%20models%20might%20be%20used%20for%20positive%2C%20intentional%2C%20interventions%20in%20their%20data%0Aecosystems%2C%20providing%20redress%20for%20historical%20discrimination%20through%20a%20framework%0Acalled%20algorithmic%20reparation%20%28AR%29.%20We%20simulate%20AR%20interventions%20by%20curating%0Arepresentative%20training%20batches%20for%20stochastic%20gradient%20descent%20to%20demonstrate%0Ahow%20AR%20can%20improve%20upon%20the%20unfairnesses%20of%20models%20and%20data%20ecosystems%20subject%0Ato%20other%20MIDS.%20Our%20work%20takes%20an%20important%20step%20towards%20identifying%2C%0Amitigating%2C%20and%20taking%20accountability%20for%20the%20unfair%20feedback%20loops%20enabled%20by%0Athe%20idea%20that%20ML%20systems%20are%20inherently%20neutral%20and%20objective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07857v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fairness%20Feedback%20Loops%3A%20Training%20on%20Synthetic%20Data%20Amplifies%20Bias&entry.906535625=Sierra%20Wyllie%20and%20Ilia%20Shumailov%20and%20Nicolas%20Papernot&entry.1292438233=%20%20Model-induced%20distribution%20shifts%20%28MIDS%29%20occur%20as%20previous%20model%20outputs%0Apollute%20new%20model%20training%20sets%20over%20generations%20of%20models.%20This%20is%20known%20as%0Amodel%20collapse%20in%20the%20case%20of%20generative%20models%2C%20and%20performative%20prediction%20or%0Aunfairness%20feedback%20loops%20for%20supervised%20models.%20When%20a%20model%20induces%20a%0Adistribution%20shift%2C%20it%20also%20encodes%20its%20mistakes%2C%20biases%2C%20and%20unfairnesses%20into%0Athe%20ground%20truth%20of%20its%20data%20ecosystem.%20We%20introduce%20a%20framework%20that%20allows%20us%0Ato%20track%20multiple%20MIDS%20over%20many%20generations%2C%20finding%20that%20they%20can%20lead%20to%0Aloss%20in%20performance%2C%20fairness%2C%20and%20minoritized%20group%20representation%2C%20even%20in%0Ainitially%20unbiased%20datasets.%20Despite%20these%20negative%20consequences%2C%20we%20identify%0Ahow%20models%20might%20be%20used%20for%20positive%2C%20intentional%2C%20interventions%20in%20their%20data%0Aecosystems%2C%20providing%20redress%20for%20historical%20discrimination%20through%20a%20framework%0Acalled%20algorithmic%20reparation%20%28AR%29.%20We%20simulate%20AR%20interventions%20by%20curating%0Arepresentative%20training%20batches%20for%20stochastic%20gradient%20descent%20to%20demonstrate%0Ahow%20AR%20can%20improve%20upon%20the%20unfairnesses%20of%20models%20and%20data%20ecosystems%20subject%0Ato%20other%20MIDS.%20Our%20work%20takes%20an%20important%20step%20towards%20identifying%2C%0Amitigating%2C%20and%20taking%20accountability%20for%20the%20unfair%20feedback%20loops%20enabled%20by%0Athe%20idea%20that%20ML%20systems%20are%20inherently%20neutral%20and%20objective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07857v1&entry.124074799=Read"},
{"title": "Chronos: Learning the Language of Time Series", "author": "Abdul Fatir Ansari and Lorenzo Stella and Caner Turkmen and Xiyuan Zhang and Pedro Mercado and Huibin Shen and Oleksandr Shchur and Syama Sundar Rangapuram and Sebastian Pineda Arango and Shubham Kapoor and Jasper Zschiegner and Danielle C. Maddix and Michael W. Mahoney and Kari Torkkola and Andrew Gordon Wilson and Michael Bohlke-Schneider and Yuyang Wang", "abstract": "  We introduce Chronos, a simple yet effective framework for pretrained\nprobabilistic time series models. Chronos tokenizes time series values using\nscaling and quantization into a fixed vocabulary and trains existing\ntransformer-based language model architectures on these tokenized time series\nvia the cross-entropy loss. We pretrained Chronos models based on the T5 family\n(ranging from 20M to 710M parameters) on a large collection of publicly\navailable datasets, complemented by a synthetic dataset that we generated via\nGaussian processes to improve generalization. In a comprehensive benchmark\nconsisting of 42 datasets, and comprising both classical local models and deep\nlearning methods, we show that Chronos models: (a) significantly outperform\nother methods on datasets that were part of the training corpus; and (b) have\ncomparable and occasionally superior zero-shot performance on new datasets,\nrelative to methods that were trained specifically on them. Our results\ndemonstrate that Chronos models can leverage time series data from diverse\ndomains to improve zero-shot accuracy on unseen forecasting tasks, positioning\npretrained models as a viable tool to greatly simplify forecasting pipelines.\n", "link": "http://arxiv.org/abs/2403.07815v1", "date": "2024-03-12", "relevancy": 1.8146, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4715}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4573}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4429}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Chronos%3A%20Learning%20the%20Language%20of%20Time%20Series&body=Title%3A%20Chronos%3A%20Learning%20the%20Language%20of%20Time%20Series%0AAuthor%3A%20Abdul%20Fatir%20Ansari%20and%20Lorenzo%20Stella%20and%20Caner%20Turkmen%20and%20Xiyuan%20Zhang%20and%20Pedro%20Mercado%20and%20Huibin%20Shen%20and%20Oleksandr%20Shchur%20and%20Syama%20Sundar%20Rangapuram%20and%20Sebastian%20Pineda%20Arango%20and%20Shubham%20Kapoor%20and%20Jasper%20Zschiegner%20and%20Danielle%20C.%20Maddix%20and%20Michael%20W.%20Mahoney%20and%20Kari%20Torkkola%20and%20Andrew%20Gordon%20Wilson%20and%20Michael%20Bohlke-Schneider%20and%20Yuyang%20Wang%0AAbstract%3A%20%20%20We%20introduce%20Chronos%2C%20a%20simple%20yet%20effective%20framework%20for%20pretrained%0Aprobabilistic%20time%20series%20models.%20Chronos%20tokenizes%20time%20series%20values%20using%0Ascaling%20and%20quantization%20into%20a%20fixed%20vocabulary%20and%20trains%20existing%0Atransformer-based%20language%20model%20architectures%20on%20these%20tokenized%20time%20series%0Avia%20the%20cross-entropy%20loss.%20We%20pretrained%20Chronos%20models%20based%20on%20the%20T5%20family%0A%28ranging%20from%2020M%20to%20710M%20parameters%29%20on%20a%20large%20collection%20of%20publicly%0Aavailable%20datasets%2C%20complemented%20by%20a%20synthetic%20dataset%20that%20we%20generated%20via%0AGaussian%20processes%20to%20improve%20generalization.%20In%20a%20comprehensive%20benchmark%0Aconsisting%20of%2042%20datasets%2C%20and%20comprising%20both%20classical%20local%20models%20and%20deep%0Alearning%20methods%2C%20we%20show%20that%20Chronos%20models%3A%20%28a%29%20significantly%20outperform%0Aother%20methods%20on%20datasets%20that%20were%20part%20of%20the%20training%20corpus%3B%20and%20%28b%29%20have%0Acomparable%20and%20occasionally%20superior%20zero-shot%20performance%20on%20new%20datasets%2C%0Arelative%20to%20methods%20that%20were%20trained%20specifically%20on%20them.%20Our%20results%0Ademonstrate%20that%20Chronos%20models%20can%20leverage%20time%20series%20data%20from%20diverse%0Adomains%20to%20improve%20zero-shot%20accuracy%20on%20unseen%20forecasting%20tasks%2C%20positioning%0Apretrained%20models%20as%20a%20viable%20tool%20to%20greatly%20simplify%20forecasting%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07815v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chronos%3A%20Learning%20the%20Language%20of%20Time%20Series&entry.906535625=Abdul%20Fatir%20Ansari%20and%20Lorenzo%20Stella%20and%20Caner%20Turkmen%20and%20Xiyuan%20Zhang%20and%20Pedro%20Mercado%20and%20Huibin%20Shen%20and%20Oleksandr%20Shchur%20and%20Syama%20Sundar%20Rangapuram%20and%20Sebastian%20Pineda%20Arango%20and%20Shubham%20Kapoor%20and%20Jasper%20Zschiegner%20and%20Danielle%20C.%20Maddix%20and%20Michael%20W.%20Mahoney%20and%20Kari%20Torkkola%20and%20Andrew%20Gordon%20Wilson%20and%20Michael%20Bohlke-Schneider%20and%20Yuyang%20Wang&entry.1292438233=%20%20We%20introduce%20Chronos%2C%20a%20simple%20yet%20effective%20framework%20for%20pretrained%0Aprobabilistic%20time%20series%20models.%20Chronos%20tokenizes%20time%20series%20values%20using%0Ascaling%20and%20quantization%20into%20a%20fixed%20vocabulary%20and%20trains%20existing%0Atransformer-based%20language%20model%20architectures%20on%20these%20tokenized%20time%20series%0Avia%20the%20cross-entropy%20loss.%20We%20pretrained%20Chronos%20models%20based%20on%20the%20T5%20family%0A%28ranging%20from%2020M%20to%20710M%20parameters%29%20on%20a%20large%20collection%20of%20publicly%0Aavailable%20datasets%2C%20complemented%20by%20a%20synthetic%20dataset%20that%20we%20generated%20via%0AGaussian%20processes%20to%20improve%20generalization.%20In%20a%20comprehensive%20benchmark%0Aconsisting%20of%2042%20datasets%2C%20and%20comprising%20both%20classical%20local%20models%20and%20deep%0Alearning%20methods%2C%20we%20show%20that%20Chronos%20models%3A%20%28a%29%20significantly%20outperform%0Aother%20methods%20on%20datasets%20that%20were%20part%20of%20the%20training%20corpus%3B%20and%20%28b%29%20have%0Acomparable%20and%20occasionally%20superior%20zero-shot%20performance%20on%20new%20datasets%2C%0Arelative%20to%20methods%20that%20were%20trained%20specifically%20on%20them.%20Our%20results%0Ademonstrate%20that%20Chronos%20models%20can%20leverage%20time%20series%20data%20from%20diverse%0Adomains%20to%20improve%20zero-shot%20accuracy%20on%20unseen%20forecasting%20tasks%2C%20positioning%0Apretrained%20models%20as%20a%20viable%20tool%20to%20greatly%20simplify%20forecasting%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07815v1&entry.124074799=Read"},
{"title": "TeleMoMa: A Modular and Versatile Teleoperation System for Mobile\n  Manipulation", "author": "Shivin Dass and Wensi Ai and Yuqian Jiang and Samik Singh and Jiaheng Hu and Ruohan Zhang and Peter Stone and Ben Abbatematteo and Roberto Martin-Martin", "abstract": "  A critical bottleneck limiting imitation learning in robotics is the lack of\ndata. This problem is more severe in mobile manipulation, where collecting\ndemonstrations is harder than in stationary manipulation due to the lack of\navailable and easy-to-use teleoperation interfaces. In this work, we\ndemonstrate TeleMoMa, a general and modular interface for whole-body\nteleoperation of mobile manipulators. TeleMoMa unifies multiple human\ninterfaces including RGB and depth cameras, virtual reality controllers,\nkeyboard, joysticks, etc., and any combination thereof. In its more accessible\nversion, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering\nthe entry bar for humans to provide mobile manipulation demonstrations. We\ndemonstrate the versatility of TeleMoMa by teleoperating several existing\nmobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and\nthe real world. We demonstrate the quality of the demonstrations collected with\nTeleMoMa by training imitation learning policies for mobile manipulation tasks\ninvolving synchronized whole-body motion. Finally, we also show that TeleMoMa's\nteleoperation channel enables teleoperation on site, looking at the robot, or\nremote, sending commands and observations through a computer network, and\nperform user studies to evaluate how easy it is for novice users to learn to\ncollect demonstrations with different combinations of human interfaces enabled\nby our system. We hope TeleMoMa becomes a helpful tool for the community\nenabling researchers to collect whole-body mobile manipulation demonstrations.\nFor more information and video results,\nhttps://robin-lab.cs.utexas.edu/telemoma-web.\n", "link": "http://arxiv.org/abs/2403.07869v1", "date": "2024-03-12", "relevancy": 1.7434, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5925}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5855}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5588}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20TeleMoMa%3A%20A%20Modular%20and%20Versatile%20Teleoperation%20System%20for%20Mobile%0A%20%20Manipulation&body=Title%3A%20TeleMoMa%3A%20A%20Modular%20and%20Versatile%20Teleoperation%20System%20for%20Mobile%0A%20%20Manipulation%0AAuthor%3A%20Shivin%20Dass%20and%20Wensi%20Ai%20and%20Yuqian%20Jiang%20and%20Samik%20Singh%20and%20Jiaheng%20Hu%20and%20Ruohan%20Zhang%20and%20Peter%20Stone%20and%20Ben%20Abbatematteo%20and%20Roberto%20Martin-Martin%0AAbstract%3A%20%20%20A%20critical%20bottleneck%20limiting%20imitation%20learning%20in%20robotics%20is%20the%20lack%20of%0Adata.%20This%20problem%20is%20more%20severe%20in%20mobile%20manipulation%2C%20where%20collecting%0Ademonstrations%20is%20harder%20than%20in%20stationary%20manipulation%20due%20to%20the%20lack%20of%0Aavailable%20and%20easy-to-use%20teleoperation%20interfaces.%20In%20this%20work%2C%20we%0Ademonstrate%20TeleMoMa%2C%20a%20general%20and%20modular%20interface%20for%20whole-body%0Ateleoperation%20of%20mobile%20manipulators.%20TeleMoMa%20unifies%20multiple%20human%0Ainterfaces%20including%20RGB%20and%20depth%20cameras%2C%20virtual%20reality%20controllers%2C%0Akeyboard%2C%20joysticks%2C%20etc.%2C%20and%20any%20combination%20thereof.%20In%20its%20more%20accessible%0Aversion%2C%20TeleMoMa%20works%20using%20simply%20vision%20%28e.g.%2C%20an%20RGB-D%20camera%29%2C%20lowering%0Athe%20entry%20bar%20for%20humans%20to%20provide%20mobile%20manipulation%20demonstrations.%20We%0Ademonstrate%20the%20versatility%20of%20TeleMoMa%20by%20teleoperating%20several%20existing%0Amobile%20manipulators%20-%20PAL%20Tiago%2B%2B%2C%20Toyota%20HSR%2C%20and%20Fetch%20-%20in%20simulation%20and%0Athe%20real%20world.%20We%20demonstrate%20the%20quality%20of%20the%20demonstrations%20collected%20with%0ATeleMoMa%20by%20training%20imitation%20learning%20policies%20for%20mobile%20manipulation%20tasks%0Ainvolving%20synchronized%20whole-body%20motion.%20Finally%2C%20we%20also%20show%20that%20TeleMoMa%27s%0Ateleoperation%20channel%20enables%20teleoperation%20on%20site%2C%20looking%20at%20the%20robot%2C%20or%0Aremote%2C%20sending%20commands%20and%20observations%20through%20a%20computer%20network%2C%20and%0Aperform%20user%20studies%20to%20evaluate%20how%20easy%20it%20is%20for%20novice%20users%20to%20learn%20to%0Acollect%20demonstrations%20with%20different%20combinations%20of%20human%20interfaces%20enabled%0Aby%20our%20system.%20We%20hope%20TeleMoMa%20becomes%20a%20helpful%20tool%20for%20the%20community%0Aenabling%20researchers%20to%20collect%20whole-body%20mobile%20manipulation%20demonstrations.%0AFor%20more%20information%20and%20video%20results%2C%0Ahttps%3A//robin-lab.cs.utexas.edu/telemoma-web.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07869v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TeleMoMa%3A%20A%20Modular%20and%20Versatile%20Teleoperation%20System%20for%20Mobile%0A%20%20Manipulation&entry.906535625=Shivin%20Dass%20and%20Wensi%20Ai%20and%20Yuqian%20Jiang%20and%20Samik%20Singh%20and%20Jiaheng%20Hu%20and%20Ruohan%20Zhang%20and%20Peter%20Stone%20and%20Ben%20Abbatematteo%20and%20Roberto%20Martin-Martin&entry.1292438233=%20%20A%20critical%20bottleneck%20limiting%20imitation%20learning%20in%20robotics%20is%20the%20lack%20of%0Adata.%20This%20problem%20is%20more%20severe%20in%20mobile%20manipulation%2C%20where%20collecting%0Ademonstrations%20is%20harder%20than%20in%20stationary%20manipulation%20due%20to%20the%20lack%20of%0Aavailable%20and%20easy-to-use%20teleoperation%20interfaces.%20In%20this%20work%2C%20we%0Ademonstrate%20TeleMoMa%2C%20a%20general%20and%20modular%20interface%20for%20whole-body%0Ateleoperation%20of%20mobile%20manipulators.%20TeleMoMa%20unifies%20multiple%20human%0Ainterfaces%20including%20RGB%20and%20depth%20cameras%2C%20virtual%20reality%20controllers%2C%0Akeyboard%2C%20joysticks%2C%20etc.%2C%20and%20any%20combination%20thereof.%20In%20its%20more%20accessible%0Aversion%2C%20TeleMoMa%20works%20using%20simply%20vision%20%28e.g.%2C%20an%20RGB-D%20camera%29%2C%20lowering%0Athe%20entry%20bar%20for%20humans%20to%20provide%20mobile%20manipulation%20demonstrations.%20We%0Ademonstrate%20the%20versatility%20of%20TeleMoMa%20by%20teleoperating%20several%20existing%0Amobile%20manipulators%20-%20PAL%20Tiago%2B%2B%2C%20Toyota%20HSR%2C%20and%20Fetch%20-%20in%20simulation%20and%0Athe%20real%20world.%20We%20demonstrate%20the%20quality%20of%20the%20demonstrations%20collected%20with%0ATeleMoMa%20by%20training%20imitation%20learning%20policies%20for%20mobile%20manipulation%20tasks%0Ainvolving%20synchronized%20whole-body%20motion.%20Finally%2C%20we%20also%20show%20that%20TeleMoMa%27s%0Ateleoperation%20channel%20enables%20teleoperation%20on%20site%2C%20looking%20at%20the%20robot%2C%20or%0Aremote%2C%20sending%20commands%20and%20observations%20through%20a%20computer%20network%2C%20and%0Aperform%20user%20studies%20to%20evaluate%20how%20easy%20it%20is%20for%20novice%20users%20to%20learn%20to%0Acollect%20demonstrations%20with%20different%20combinations%20of%20human%20interfaces%20enabled%0Aby%20our%20system.%20We%20hope%20TeleMoMa%20becomes%20a%20helpful%20tool%20for%20the%20community%0Aenabling%20researchers%20to%20collect%20whole-body%20mobile%20manipulation%20demonstrations.%0AFor%20more%20information%20and%20video%20results%2C%0Ahttps%3A//robin-lab.cs.utexas.edu/telemoma-web.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07869v1&entry.124074799=Read"},
{"title": "A Machine learning and Empirical Bayesian Approach for Predictive Buying\n  in B2B E-commerce", "author": "Tuhin Subhra De and Pranjal Singh and Alok Patel", "abstract": "  In the context of developing nations like India, traditional business to\nbusiness (B2B) commerce heavily relies on the establishment of robust\nrelationships, trust, and credit arrangements between buyers and sellers.\nConsequently, ecommerce enterprises frequently. Established in 2016 with a\nvision to revolutionize trade in India through technology, Udaan is the\ncountrys largest business to business ecommerce platform. Udaan operates across\ndiverse product categories, including lifestyle, electronics, home and employ\ntelecallers to cultivate buyer relationships, streamline order placement\nprocedures, and promote special promotions. The accurate anticipation of buyer\norder placement behavior emerges as a pivotal factor for attaining sustainable\ngrowth, heightening competitiveness, and optimizing the efficiency of these\ntelecallers. To address this challenge, we have employed an ensemble approach\ncomprising XGBoost and a modified version of Poisson Gamma model to predict\ncustomer order patterns with precision. This paper provides an in-depth\nexploration of the strategic fusion of machine learning and an empirical\nBayesian approach, bolstered by the judicious selection of pertinent features.\nThis innovative approach has yielded a remarkable 3 times increase in customer\norder rates, show casing its potential for transformative impact in the\necommerce industry.\n", "link": "http://arxiv.org/abs/2403.07843v1", "date": "2024-03-12", "relevancy": 1.7078, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4716}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4208}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4152}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20A%20Machine%20learning%20and%20Empirical%20Bayesian%20Approach%20for%20Predictive%20Buying%0A%20%20in%20B2B%20E-commerce&body=Title%3A%20A%20Machine%20learning%20and%20Empirical%20Bayesian%20Approach%20for%20Predictive%20Buying%0A%20%20in%20B2B%20E-commerce%0AAuthor%3A%20Tuhin%20Subhra%20De%20and%20Pranjal%20Singh%20and%20Alok%20Patel%0AAbstract%3A%20%20%20In%20the%20context%20of%20developing%20nations%20like%20India%2C%20traditional%20business%20to%0Abusiness%20%28B2B%29%20commerce%20heavily%20relies%20on%20the%20establishment%20of%20robust%0Arelationships%2C%20trust%2C%20and%20credit%20arrangements%20between%20buyers%20and%20sellers.%0AConsequently%2C%20ecommerce%20enterprises%20frequently.%20Established%20in%202016%20with%20a%0Avision%20to%20revolutionize%20trade%20in%20India%20through%20technology%2C%20Udaan%20is%20the%0Acountrys%20largest%20business%20to%20business%20ecommerce%20platform.%20Udaan%20operates%20across%0Adiverse%20product%20categories%2C%20including%20lifestyle%2C%20electronics%2C%20home%20and%20employ%0Atelecallers%20to%20cultivate%20buyer%20relationships%2C%20streamline%20order%20placement%0Aprocedures%2C%20and%20promote%20special%20promotions.%20The%20accurate%20anticipation%20of%20buyer%0Aorder%20placement%20behavior%20emerges%20as%20a%20pivotal%20factor%20for%20attaining%20sustainable%0Agrowth%2C%20heightening%20competitiveness%2C%20and%20optimizing%20the%20efficiency%20of%20these%0Atelecallers.%20To%20address%20this%20challenge%2C%20we%20have%20employed%20an%20ensemble%20approach%0Acomprising%20XGBoost%20and%20a%20modified%20version%20of%20Poisson%20Gamma%20model%20to%20predict%0Acustomer%20order%20patterns%20with%20precision.%20This%20paper%20provides%20an%20in-depth%0Aexploration%20of%20the%20strategic%20fusion%20of%20machine%20learning%20and%20an%20empirical%0ABayesian%20approach%2C%20bolstered%20by%20the%20judicious%20selection%20of%20pertinent%20features.%0AThis%20innovative%20approach%20has%20yielded%20a%20remarkable%203%20times%20increase%20in%20customer%0Aorder%20rates%2C%20show%20casing%20its%20potential%20for%20transformative%20impact%20in%20the%0Aecommerce%20industry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07843v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Machine%20learning%20and%20Empirical%20Bayesian%20Approach%20for%20Predictive%20Buying%0A%20%20in%20B2B%20E-commerce&entry.906535625=Tuhin%20Subhra%20De%20and%20Pranjal%20Singh%20and%20Alok%20Patel&entry.1292438233=%20%20In%20the%20context%20of%20developing%20nations%20like%20India%2C%20traditional%20business%20to%0Abusiness%20%28B2B%29%20commerce%20heavily%20relies%20on%20the%20establishment%20of%20robust%0Arelationships%2C%20trust%2C%20and%20credit%20arrangements%20between%20buyers%20and%20sellers.%0AConsequently%2C%20ecommerce%20enterprises%20frequently.%20Established%20in%202016%20with%20a%0Avision%20to%20revolutionize%20trade%20in%20India%20through%20technology%2C%20Udaan%20is%20the%0Acountrys%20largest%20business%20to%20business%20ecommerce%20platform.%20Udaan%20operates%20across%0Adiverse%20product%20categories%2C%20including%20lifestyle%2C%20electronics%2C%20home%20and%20employ%0Atelecallers%20to%20cultivate%20buyer%20relationships%2C%20streamline%20order%20placement%0Aprocedures%2C%20and%20promote%20special%20promotions.%20The%20accurate%20anticipation%20of%20buyer%0Aorder%20placement%20behavior%20emerges%20as%20a%20pivotal%20factor%20for%20attaining%20sustainable%0Agrowth%2C%20heightening%20competitiveness%2C%20and%20optimizing%20the%20efficiency%20of%20these%0Atelecallers.%20To%20address%20this%20challenge%2C%20we%20have%20employed%20an%20ensemble%20approach%0Acomprising%20XGBoost%20and%20a%20modified%20version%20of%20Poisson%20Gamma%20model%20to%20predict%0Acustomer%20order%20patterns%20with%20precision.%20This%20paper%20provides%20an%20in-depth%0Aexploration%20of%20the%20strategic%20fusion%20of%20machine%20learning%20and%20an%20empirical%0ABayesian%20approach%2C%20bolstered%20by%20the%20judicious%20selection%20of%20pertinent%20features.%0AThis%20innovative%20approach%20has%20yielded%20a%20remarkable%203%20times%20increase%20in%20customer%0Aorder%20rates%2C%20show%20casing%20its%20potential%20for%20transformative%20impact%20in%20the%0Aecommerce%20industry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07843v1&entry.124074799=Read"},
{"title": "Bridging Different Language Models and Generative Vision Models for\n  Text-to-Image Generation", "author": "Shihao Zhao and Shaozhe Hao and Bojia Zi and Huaizhe Xu and Kwan-Yee K. Wong", "abstract": "  Text-to-image generation has made significant advancements with the\nintroduction of text-to-image diffusion models. These models typically consist\nof a language model that interprets user prompts and a vision model that\ngenerates corresponding images. As language and vision models continue to\nprogress in their respective domains, there is a great potential in exploring\nthe replacement of components in text-to-image diffusion models with more\nadvanced counterparts. A broader research objective would therefore be to\ninvestigate the integration of any two unrelated language and generative vision\nmodels for text-to-image generation. In this paper, we explore this objective\nand propose LaVi-Bridge, a pipeline that enables the integration of diverse\npre-trained language models and generative vision models for text-to-image\ngeneration. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and\nplug-and-play approach without requiring modifications to the original weights\nof the language and vision models. Our pipeline is compatible with various\nlanguage models and generative vision models, accommodating different\nstructures. Within this framework, we demonstrate that incorporating superior\nmodules, such as more advanced language models or generative vision models,\nresults in notable improvements in capabilities like text alignment or image\nquality. Extensive evaluations have been conducted to verify the effectiveness\nof LaVi-Bridge. Code is available at\nhttps://github.com/ShihaoZhaoZSH/LaVi-Bridge.\n", "link": "http://arxiv.org/abs/2403.07860v1", "date": "2024-03-12", "relevancy": 1.7038, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5851}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5647}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5589}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Bridging%20Different%20Language%20Models%20and%20Generative%20Vision%20Models%20for%0A%20%20Text-to-Image%20Generation&body=Title%3A%20Bridging%20Different%20Language%20Models%20and%20Generative%20Vision%20Models%20for%0A%20%20Text-to-Image%20Generation%0AAuthor%3A%20Shihao%20Zhao%20and%20Shaozhe%20Hao%20and%20Bojia%20Zi%20and%20Huaizhe%20Xu%20and%20Kwan-Yee%20K.%20Wong%0AAbstract%3A%20%20%20Text-to-image%20generation%20has%20made%20significant%20advancements%20with%20the%0Aintroduction%20of%20text-to-image%20diffusion%20models.%20These%20models%20typically%20consist%0Aof%20a%20language%20model%20that%20interprets%20user%20prompts%20and%20a%20vision%20model%20that%0Agenerates%20corresponding%20images.%20As%20language%20and%20vision%20models%20continue%20to%0Aprogress%20in%20their%20respective%20domains%2C%20there%20is%20a%20great%20potential%20in%20exploring%0Athe%20replacement%20of%20components%20in%20text-to-image%20diffusion%20models%20with%20more%0Aadvanced%20counterparts.%20A%20broader%20research%20objective%20would%20therefore%20be%20to%0Ainvestigate%20the%20integration%20of%20any%20two%20unrelated%20language%20and%20generative%20vision%0Amodels%20for%20text-to-image%20generation.%20In%20this%20paper%2C%20we%20explore%20this%20objective%0Aand%20propose%20LaVi-Bridge%2C%20a%20pipeline%20that%20enables%20the%20integration%20of%20diverse%0Apre-trained%20language%20models%20and%20generative%20vision%20models%20for%20text-to-image%0Ageneration.%20By%20leveraging%20LoRA%20and%20adapters%2C%20LaVi-Bridge%20offers%20a%20flexible%20and%0Aplug-and-play%20approach%20without%20requiring%20modifications%20to%20the%20original%20weights%0Aof%20the%20language%20and%20vision%20models.%20Our%20pipeline%20is%20compatible%20with%20various%0Alanguage%20models%20and%20generative%20vision%20models%2C%20accommodating%20different%0Astructures.%20Within%20this%20framework%2C%20we%20demonstrate%20that%20incorporating%20superior%0Amodules%2C%20such%20as%20more%20advanced%20language%20models%20or%20generative%20vision%20models%2C%0Aresults%20in%20notable%20improvements%20in%20capabilities%20like%20text%20alignment%20or%20image%0Aquality.%20Extensive%20evaluations%20have%20been%20conducted%20to%20verify%20the%20effectiveness%0Aof%20LaVi-Bridge.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ShihaoZhaoZSH/LaVi-Bridge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07860v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Different%20Language%20Models%20and%20Generative%20Vision%20Models%20for%0A%20%20Text-to-Image%20Generation&entry.906535625=Shihao%20Zhao%20and%20Shaozhe%20Hao%20and%20Bojia%20Zi%20and%20Huaizhe%20Xu%20and%20Kwan-Yee%20K.%20Wong&entry.1292438233=%20%20Text-to-image%20generation%20has%20made%20significant%20advancements%20with%20the%0Aintroduction%20of%20text-to-image%20diffusion%20models.%20These%20models%20typically%20consist%0Aof%20a%20language%20model%20that%20interprets%20user%20prompts%20and%20a%20vision%20model%20that%0Agenerates%20corresponding%20images.%20As%20language%20and%20vision%20models%20continue%20to%0Aprogress%20in%20their%20respective%20domains%2C%20there%20is%20a%20great%20potential%20in%20exploring%0Athe%20replacement%20of%20components%20in%20text-to-image%20diffusion%20models%20with%20more%0Aadvanced%20counterparts.%20A%20broader%20research%20objective%20would%20therefore%20be%20to%0Ainvestigate%20the%20integration%20of%20any%20two%20unrelated%20language%20and%20generative%20vision%0Amodels%20for%20text-to-image%20generation.%20In%20this%20paper%2C%20we%20explore%20this%20objective%0Aand%20propose%20LaVi-Bridge%2C%20a%20pipeline%20that%20enables%20the%20integration%20of%20diverse%0Apre-trained%20language%20models%20and%20generative%20vision%20models%20for%20text-to-image%0Ageneration.%20By%20leveraging%20LoRA%20and%20adapters%2C%20LaVi-Bridge%20offers%20a%20flexible%20and%0Aplug-and-play%20approach%20without%20requiring%20modifications%20to%20the%20original%20weights%0Aof%20the%20language%20and%20vision%20models.%20Our%20pipeline%20is%20compatible%20with%20various%0Alanguage%20models%20and%20generative%20vision%20models%2C%20accommodating%20different%0Astructures.%20Within%20this%20framework%2C%20we%20demonstrate%20that%20incorporating%20superior%0Amodules%2C%20such%20as%20more%20advanced%20language%20models%20or%20generative%20vision%20models%2C%0Aresults%20in%20notable%20improvements%20in%20capabilities%20like%20text%20alignment%20or%20image%0Aquality.%20Extensive%20evaluations%20have%20been%20conducted%20to%20verify%20the%20effectiveness%0Aof%20LaVi-Bridge.%20Code%20is%20available%20at%0Ahttps%3A//github.com/ShihaoZhaoZSH/LaVi-Bridge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07860v1&entry.124074799=Read"},
{"title": "pyvene: A Library for Understanding and Improving PyTorch Models via\n  Interventions", "author": "Zhengxuan Wu and Atticus Geiger and Aryaman Arora and Jing Huang and Zheng Wang and Noah D. Goodman and Christopher D. Manning and Christopher Potts", "abstract": "  Interventions on model-internal states are fundamental operations in many\nareas of AI, including model editing, steering, robustness, and\ninterpretability. To facilitate such research, we introduce $\\textbf{pyvene}$,\nan open-source Python library that supports customizable interventions on a\nrange of different PyTorch modules. $\\textbf{pyvene}$ supports complex\nintervention schemes with an intuitive configuration format, and its\ninterventions can be static or include trainable parameters. We show how\n$\\textbf{pyvene}$ provides a unified and extensible framework for performing\ninterventions on neural models and sharing the intervened upon models with\nothers. We illustrate the power of the library via interpretability analyses\nusing causal abstraction and knowledge localization. We publish our library\nthrough Python Package Index (PyPI) and provide code, documentation, and\ntutorials at https://github.com/stanfordnlp/pyvene.\n", "link": "http://arxiv.org/abs/2403.07809v1", "date": "2024-03-12", "relevancy": 1.7011, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.436}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4338}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4111}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20pyvene%3A%20A%20Library%20for%20Understanding%20and%20Improving%20PyTorch%20Models%20via%0A%20%20Interventions&body=Title%3A%20pyvene%3A%20A%20Library%20for%20Understanding%20and%20Improving%20PyTorch%20Models%20via%0A%20%20Interventions%0AAuthor%3A%20Zhengxuan%20Wu%20and%20Atticus%20Geiger%20and%20Aryaman%20Arora%20and%20Jing%20Huang%20and%20Zheng%20Wang%20and%20Noah%20D.%20Goodman%20and%20Christopher%20D.%20Manning%20and%20Christopher%20Potts%0AAbstract%3A%20%20%20Interventions%20on%20model-internal%20states%20are%20fundamental%20operations%20in%20many%0Aareas%20of%20AI%2C%20including%20model%20editing%2C%20steering%2C%20robustness%2C%20and%0Ainterpretability.%20To%20facilitate%20such%20research%2C%20we%20introduce%20%24%5Ctextbf%7Bpyvene%7D%24%2C%0Aan%20open-source%20Python%20library%20that%20supports%20customizable%20interventions%20on%20a%0Arange%20of%20different%20PyTorch%20modules.%20%24%5Ctextbf%7Bpyvene%7D%24%20supports%20complex%0Aintervention%20schemes%20with%20an%20intuitive%20configuration%20format%2C%20and%20its%0Ainterventions%20can%20be%20static%20or%20include%20trainable%20parameters.%20We%20show%20how%0A%24%5Ctextbf%7Bpyvene%7D%24%20provides%20a%20unified%20and%20extensible%20framework%20for%20performing%0Ainterventions%20on%20neural%20models%20and%20sharing%20the%20intervened%20upon%20models%20with%0Aothers.%20We%20illustrate%20the%20power%20of%20the%20library%20via%20interpretability%20analyses%0Ausing%20causal%20abstraction%20and%20knowledge%20localization.%20We%20publish%20our%20library%0Athrough%20Python%20Package%20Index%20%28PyPI%29%20and%20provide%20code%2C%20documentation%2C%20and%0Atutorials%20at%20https%3A//github.com/stanfordnlp/pyvene.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07809v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=pyvene%3A%20A%20Library%20for%20Understanding%20and%20Improving%20PyTorch%20Models%20via%0A%20%20Interventions&entry.906535625=Zhengxuan%20Wu%20and%20Atticus%20Geiger%20and%20Aryaman%20Arora%20and%20Jing%20Huang%20and%20Zheng%20Wang%20and%20Noah%20D.%20Goodman%20and%20Christopher%20D.%20Manning%20and%20Christopher%20Potts&entry.1292438233=%20%20Interventions%20on%20model-internal%20states%20are%20fundamental%20operations%20in%20many%0Aareas%20of%20AI%2C%20including%20model%20editing%2C%20steering%2C%20robustness%2C%20and%0Ainterpretability.%20To%20facilitate%20such%20research%2C%20we%20introduce%20%24%5Ctextbf%7Bpyvene%7D%24%2C%0Aan%20open-source%20Python%20library%20that%20supports%20customizable%20interventions%20on%20a%0Arange%20of%20different%20PyTorch%20modules.%20%24%5Ctextbf%7Bpyvene%7D%24%20supports%20complex%0Aintervention%20schemes%20with%20an%20intuitive%20configuration%20format%2C%20and%20its%0Ainterventions%20can%20be%20static%20or%20include%20trainable%20parameters.%20We%20show%20how%0A%24%5Ctextbf%7Bpyvene%7D%24%20provides%20a%20unified%20and%20extensible%20framework%20for%20performing%0Ainterventions%20on%20neural%20models%20and%20sharing%20the%20intervened%20upon%20models%20with%0Aothers.%20We%20illustrate%20the%20power%20of%20the%20library%20via%20interpretability%20analyses%0Ausing%20causal%20abstraction%20and%20knowledge%20localization.%20We%20publish%20our%20library%0Athrough%20Python%20Package%20Index%20%28PyPI%29%20and%20provide%20code%2C%20documentation%2C%20and%0Atutorials%20at%20https%3A//github.com/stanfordnlp/pyvene.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07809v1&entry.124074799=Read"},
{"title": "DeliGrasp: Inferring Object Mass, Friction, and Compliance with LLMs for\n  Adaptive and Minimally Deforming Grasp Policies", "author": "William Xie and Jensen Lavering and Nikolaus Correll", "abstract": "  Large language models (LLMs) can provide rich physical descriptions of most\nworldly objects, allowing robots to achieve more informed and capable grasping.\nWe leverage LLMs' common sense physical reasoning and code-writing abilities to\ninfer an object's physical characteristics--mass $m$, friction coefficient\n$\\mu$, and spring constant $k$--from a semantic description, and then translate\nthose characteristics into an executable adaptive grasp policy. Using a\ncurrent-controllable, two-finger gripper with a built-in depth camera, we\ndemonstrate that LLM-generated, physically-grounded grasp policies outperform\ntraditional grasp policies on a custom benchmark of 12 delicate and deformable\nitems including food, produce, toys, and other everyday items, spanning two\norders of magnitude in mass and required pick-up force. We also demonstrate how\ncompliance feedback from DeliGrasp policies can aid in downstream tasks such as\nmeasuring produce ripeness. Our code and videos are available at:\nhttps://deligrasp.github.io\n", "link": "http://arxiv.org/abs/2403.07832v1", "date": "2024-03-12", "relevancy": 1.6926, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5797}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5594}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5304}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20DeliGrasp%3A%20Inferring%20Object%20Mass%2C%20Friction%2C%20and%20Compliance%20with%20LLMs%20for%0A%20%20Adaptive%20and%20Minimally%20Deforming%20Grasp%20Policies&body=Title%3A%20DeliGrasp%3A%20Inferring%20Object%20Mass%2C%20Friction%2C%20and%20Compliance%20with%20LLMs%20for%0A%20%20Adaptive%20and%20Minimally%20Deforming%20Grasp%20Policies%0AAuthor%3A%20William%20Xie%20and%20Jensen%20Lavering%20and%20Nikolaus%20Correll%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20can%20provide%20rich%20physical%20descriptions%20of%20most%0Aworldly%20objects%2C%20allowing%20robots%20to%20achieve%20more%20informed%20and%20capable%20grasping.%0AWe%20leverage%20LLMs%27%20common%20sense%20physical%20reasoning%20and%20code-writing%20abilities%20to%0Ainfer%20an%20object%27s%20physical%20characteristics--mass%20%24m%24%2C%20friction%20coefficient%0A%24%5Cmu%24%2C%20and%20spring%20constant%20%24k%24--from%20a%20semantic%20description%2C%20and%20then%20translate%0Athose%20characteristics%20into%20an%20executable%20adaptive%20grasp%20policy.%20Using%20a%0Acurrent-controllable%2C%20two-finger%20gripper%20with%20a%20built-in%20depth%20camera%2C%20we%0Ademonstrate%20that%20LLM-generated%2C%20physically-grounded%20grasp%20policies%20outperform%0Atraditional%20grasp%20policies%20on%20a%20custom%20benchmark%20of%2012%20delicate%20and%20deformable%0Aitems%20including%20food%2C%20produce%2C%20toys%2C%20and%20other%20everyday%20items%2C%20spanning%20two%0Aorders%20of%20magnitude%20in%20mass%20and%20required%20pick-up%20force.%20We%20also%20demonstrate%20how%0Acompliance%20feedback%20from%20DeliGrasp%20policies%20can%20aid%20in%20downstream%20tasks%20such%20as%0Ameasuring%20produce%20ripeness.%20Our%20code%20and%20videos%20are%20available%20at%3A%0Ahttps%3A//deligrasp.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07832v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeliGrasp%3A%20Inferring%20Object%20Mass%2C%20Friction%2C%20and%20Compliance%20with%20LLMs%20for%0A%20%20Adaptive%20and%20Minimally%20Deforming%20Grasp%20Policies&entry.906535625=William%20Xie%20and%20Jensen%20Lavering%20and%20Nikolaus%20Correll&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20can%20provide%20rich%20physical%20descriptions%20of%20most%0Aworldly%20objects%2C%20allowing%20robots%20to%20achieve%20more%20informed%20and%20capable%20grasping.%0AWe%20leverage%20LLMs%27%20common%20sense%20physical%20reasoning%20and%20code-writing%20abilities%20to%0Ainfer%20an%20object%27s%20physical%20characteristics--mass%20%24m%24%2C%20friction%20coefficient%0A%24%5Cmu%24%2C%20and%20spring%20constant%20%24k%24--from%20a%20semantic%20description%2C%20and%20then%20translate%0Athose%20characteristics%20into%20an%20executable%20adaptive%20grasp%20policy.%20Using%20a%0Acurrent-controllable%2C%20two-finger%20gripper%20with%20a%20built-in%20depth%20camera%2C%20we%0Ademonstrate%20that%20LLM-generated%2C%20physically-grounded%20grasp%20policies%20outperform%0Atraditional%20grasp%20policies%20on%20a%20custom%20benchmark%20of%2012%20delicate%20and%20deformable%0Aitems%20including%20food%2C%20produce%2C%20toys%2C%20and%20other%20everyday%20items%2C%20spanning%20two%0Aorders%20of%20magnitude%20in%20mass%20and%20required%20pick-up%20force.%20We%20also%20demonstrate%20how%0Acompliance%20feedback%20from%20DeliGrasp%20policies%20can%20aid%20in%20downstream%20tasks%20such%20as%0Ameasuring%20produce%20ripeness.%20Our%20code%20and%20videos%20are%20available%20at%3A%0Ahttps%3A//deligrasp.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07832v1&entry.124074799=Read"},
{"title": "LOTUS: Continual Imitation Learning for Robot Manipulation Through\n  Unsupervised Skill Discovery", "author": "Weikang Wan and Yifeng Zhu and Rutav Shah and Yuke Zhu", "abstract": "  We introduce LOTUS, a continual imitation learning algorithm that empowers a\nphysical robot to continuously and efficiently learn to solve new manipulation\ntasks throughout its lifespan. The core idea behind LOTUS is constructing an\never-growing skill library from a sequence of new tasks with a small number of\nhuman demonstrations. LOTUS starts with a continual skill discovery process\nusing an open-vocabulary vision model, which extracts skills as recurring\npatterns presented in unsegmented demonstrations. Continual skill discovery\nupdates existing skills to avoid catastrophic forgetting of previous tasks and\nadds new skills to solve novel tasks. LOTUS trains a meta-controller that\nflexibly composes various skills to tackle vision-based manipulation tasks in\nthe lifelong learning process. Our comprehensive experiments show that LOTUS\noutperforms state-of-the-art baselines by over 11% in success rate, showing its\nsuperior knowledge transfer ability compared to prior methods. More results and\nvideos can be found on the project website:\nhttps://ut-austin-rpl.github.io/Lotus/.\n", "link": "http://arxiv.org/abs/2311.02058v3", "date": "2024-03-12", "relevancy": 1.6778, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5876}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5533}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5458}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20LOTUS%3A%20Continual%20Imitation%20Learning%20for%20Robot%20Manipulation%20Through%0A%20%20Unsupervised%20Skill%20Discovery&body=Title%3A%20LOTUS%3A%20Continual%20Imitation%20Learning%20for%20Robot%20Manipulation%20Through%0A%20%20Unsupervised%20Skill%20Discovery%0AAuthor%3A%20Weikang%20Wan%20and%20Yifeng%20Zhu%20and%20Rutav%20Shah%20and%20Yuke%20Zhu%0AAbstract%3A%20%20%20We%20introduce%20LOTUS%2C%20a%20continual%20imitation%20learning%20algorithm%20that%20empowers%20a%0Aphysical%20robot%20to%20continuously%20and%20efficiently%20learn%20to%20solve%20new%20manipulation%0Atasks%20throughout%20its%20lifespan.%20The%20core%20idea%20behind%20LOTUS%20is%20constructing%20an%0Aever-growing%20skill%20library%20from%20a%20sequence%20of%20new%20tasks%20with%20a%20small%20number%20of%0Ahuman%20demonstrations.%20LOTUS%20starts%20with%20a%20continual%20skill%20discovery%20process%0Ausing%20an%20open-vocabulary%20vision%20model%2C%20which%20extracts%20skills%20as%20recurring%0Apatterns%20presented%20in%20unsegmented%20demonstrations.%20Continual%20skill%20discovery%0Aupdates%20existing%20skills%20to%20avoid%20catastrophic%20forgetting%20of%20previous%20tasks%20and%0Aadds%20new%20skills%20to%20solve%20novel%20tasks.%20LOTUS%20trains%20a%20meta-controller%20that%0Aflexibly%20composes%20various%20skills%20to%20tackle%20vision-based%20manipulation%20tasks%20in%0Athe%20lifelong%20learning%20process.%20Our%20comprehensive%20experiments%20show%20that%20LOTUS%0Aoutperforms%20state-of-the-art%20baselines%20by%20over%2011%25%20in%20success%20rate%2C%20showing%20its%0Asuperior%20knowledge%20transfer%20ability%20compared%20to%20prior%20methods.%20More%20results%20and%0Avideos%20can%20be%20found%20on%20the%20project%20website%3A%0Ahttps%3A//ut-austin-rpl.github.io/Lotus/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.02058v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOTUS%3A%20Continual%20Imitation%20Learning%20for%20Robot%20Manipulation%20Through%0A%20%20Unsupervised%20Skill%20Discovery&entry.906535625=Weikang%20Wan%20and%20Yifeng%20Zhu%20and%20Rutav%20Shah%20and%20Yuke%20Zhu&entry.1292438233=%20%20We%20introduce%20LOTUS%2C%20a%20continual%20imitation%20learning%20algorithm%20that%20empowers%20a%0Aphysical%20robot%20to%20continuously%20and%20efficiently%20learn%20to%20solve%20new%20manipulation%0Atasks%20throughout%20its%20lifespan.%20The%20core%20idea%20behind%20LOTUS%20is%20constructing%20an%0Aever-growing%20skill%20library%20from%20a%20sequence%20of%20new%20tasks%20with%20a%20small%20number%20of%0Ahuman%20demonstrations.%20LOTUS%20starts%20with%20a%20continual%20skill%20discovery%20process%0Ausing%20an%20open-vocabulary%20vision%20model%2C%20which%20extracts%20skills%20as%20recurring%0Apatterns%20presented%20in%20unsegmented%20demonstrations.%20Continual%20skill%20discovery%0Aupdates%20existing%20skills%20to%20avoid%20catastrophic%20forgetting%20of%20previous%20tasks%20and%0Aadds%20new%20skills%20to%20solve%20novel%20tasks.%20LOTUS%20trains%20a%20meta-controller%20that%0Aflexibly%20composes%20various%20skills%20to%20tackle%20vision-based%20manipulation%20tasks%20in%0Athe%20lifelong%20learning%20process.%20Our%20comprehensive%20experiments%20show%20that%20LOTUS%0Aoutperforms%20state-of-the-art%20baselines%20by%20over%2011%25%20in%20success%20rate%2C%20showing%20its%0Asuperior%20knowledge%20transfer%20ability%20compared%20to%20prior%20methods.%20More%20results%20and%0Avideos%20can%20be%20found%20on%20the%20project%20website%3A%0Ahttps%3A//ut-austin-rpl.github.io/Lotus/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.02058v3&entry.124074799=Read"},
{"title": "Novelty Detection on Radio Astronomy Data using Signatures", "author": "Paola Arrubarrena and Maud Lemercier and Bojan Nikolic and Terry Lyons and Thomas Cass", "abstract": "  We introduce SigNova, a new semi-supervised framework for detecting anomalies\nin streamed data. While our initial examples focus on detecting radio-frequency\ninterference (RFI) in digitized signals within the field of radio astronomy, it\nis important to note that SigNova's applicability extends to any type of\nstreamed data. The framework comprises three primary components. Firstly, we\nuse the signature transform to extract a canonical collection of summary\nstatistics from observational sequences. This allows us to represent\nvariable-length visibility samples as finite-dimensional feature vectors.\nSecondly, each feature vector is assigned a novelty score, calculated as the\nMahalanobis distance to its nearest neighbor in an RFI-free training set. By\nthresholding these scores we identify observation ranges that deviate from the\nexpected behavior of RFI-free visibility samples without relying on stringent\ndistributional assumptions. Thirdly, we integrate this anomaly detector with\nPysegments, a segmentation algorithm, to localize consecutive observations\ncontaminated with RFI, if any. This approach provides a compelling alternative\nto classical windowing techniques commonly used for RFI detection. Importantly,\nthe complexity of our algorithm depends on the RFI pattern rather than on the\nsize of the observation window. We demonstrate how SigNova improves the\ndetection of various types of RFI (e.g., broadband and narrowband) in\ntime-frequency visibility data. We validate our framework on the Murchison\nWidefield Array (MWA) telescope and simulated data and the Hydrogen Epoch of\nReionization Array (HERA).\n", "link": "http://arxiv.org/abs/2402.14892v2", "date": "2024-03-12", "relevancy": 1.6729, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4295}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4155}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3968}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Novelty%20Detection%20on%20Radio%20Astronomy%20Data%20using%20Signatures&body=Title%3A%20Novelty%20Detection%20on%20Radio%20Astronomy%20Data%20using%20Signatures%0AAuthor%3A%20Paola%20Arrubarrena%20and%20Maud%20Lemercier%20and%20Bojan%20Nikolic%20and%20Terry%20Lyons%20and%20Thomas%20Cass%0AAbstract%3A%20%20%20We%20introduce%20SigNova%2C%20a%20new%20semi-supervised%20framework%20for%20detecting%20anomalies%0Ain%20streamed%20data.%20While%20our%20initial%20examples%20focus%20on%20detecting%20radio-frequency%0Ainterference%20%28RFI%29%20in%20digitized%20signals%20within%20the%20field%20of%20radio%20astronomy%2C%20it%0Ais%20important%20to%20note%20that%20SigNova%27s%20applicability%20extends%20to%20any%20type%20of%0Astreamed%20data.%20The%20framework%20comprises%20three%20primary%20components.%20Firstly%2C%20we%0Ause%20the%20signature%20transform%20to%20extract%20a%20canonical%20collection%20of%20summary%0Astatistics%20from%20observational%20sequences.%20This%20allows%20us%20to%20represent%0Avariable-length%20visibility%20samples%20as%20finite-dimensional%20feature%20vectors.%0ASecondly%2C%20each%20feature%20vector%20is%20assigned%20a%20novelty%20score%2C%20calculated%20as%20the%0AMahalanobis%20distance%20to%20its%20nearest%20neighbor%20in%20an%20RFI-free%20training%20set.%20By%0Athresholding%20these%20scores%20we%20identify%20observation%20ranges%20that%20deviate%20from%20the%0Aexpected%20behavior%20of%20RFI-free%20visibility%20samples%20without%20relying%20on%20stringent%0Adistributional%20assumptions.%20Thirdly%2C%20we%20integrate%20this%20anomaly%20detector%20with%0APysegments%2C%20a%20segmentation%20algorithm%2C%20to%20localize%20consecutive%20observations%0Acontaminated%20with%20RFI%2C%20if%20any.%20This%20approach%20provides%20a%20compelling%20alternative%0Ato%20classical%20windowing%20techniques%20commonly%20used%20for%20RFI%20detection.%20Importantly%2C%0Athe%20complexity%20of%20our%20algorithm%20depends%20on%20the%20RFI%20pattern%20rather%20than%20on%20the%0Asize%20of%20the%20observation%20window.%20We%20demonstrate%20how%20SigNova%20improves%20the%0Adetection%20of%20various%20types%20of%20RFI%20%28e.g.%2C%20broadband%20and%20narrowband%29%20in%0Atime-frequency%20visibility%20data.%20We%20validate%20our%20framework%20on%20the%20Murchison%0AWidefield%20Array%20%28MWA%29%20telescope%20and%20simulated%20data%20and%20the%20Hydrogen%20Epoch%20of%0AReionization%20Array%20%28HERA%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14892v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novelty%20Detection%20on%20Radio%20Astronomy%20Data%20using%20Signatures&entry.906535625=Paola%20Arrubarrena%20and%20Maud%20Lemercier%20and%20Bojan%20Nikolic%20and%20Terry%20Lyons%20and%20Thomas%20Cass&entry.1292438233=%20%20We%20introduce%20SigNova%2C%20a%20new%20semi-supervised%20framework%20for%20detecting%20anomalies%0Ain%20streamed%20data.%20While%20our%20initial%20examples%20focus%20on%20detecting%20radio-frequency%0Ainterference%20%28RFI%29%20in%20digitized%20signals%20within%20the%20field%20of%20radio%20astronomy%2C%20it%0Ais%20important%20to%20note%20that%20SigNova%27s%20applicability%20extends%20to%20any%20type%20of%0Astreamed%20data.%20The%20framework%20comprises%20three%20primary%20components.%20Firstly%2C%20we%0Ause%20the%20signature%20transform%20to%20extract%20a%20canonical%20collection%20of%20summary%0Astatistics%20from%20observational%20sequences.%20This%20allows%20us%20to%20represent%0Avariable-length%20visibility%20samples%20as%20finite-dimensional%20feature%20vectors.%0ASecondly%2C%20each%20feature%20vector%20is%20assigned%20a%20novelty%20score%2C%20calculated%20as%20the%0AMahalanobis%20distance%20to%20its%20nearest%20neighbor%20in%20an%20RFI-free%20training%20set.%20By%0Athresholding%20these%20scores%20we%20identify%20observation%20ranges%20that%20deviate%20from%20the%0Aexpected%20behavior%20of%20RFI-free%20visibility%20samples%20without%20relying%20on%20stringent%0Adistributional%20assumptions.%20Thirdly%2C%20we%20integrate%20this%20anomaly%20detector%20with%0APysegments%2C%20a%20segmentation%20algorithm%2C%20to%20localize%20consecutive%20observations%0Acontaminated%20with%20RFI%2C%20if%20any.%20This%20approach%20provides%20a%20compelling%20alternative%0Ato%20classical%20windowing%20techniques%20commonly%20used%20for%20RFI%20detection.%20Importantly%2C%0Athe%20complexity%20of%20our%20algorithm%20depends%20on%20the%20RFI%20pattern%20rather%20than%20on%20the%0Asize%20of%20the%20observation%20window.%20We%20demonstrate%20how%20SigNova%20improves%20the%0Adetection%20of%20various%20types%20of%20RFI%20%28e.g.%2C%20broadband%20and%20narrowband%29%20in%0Atime-frequency%20visibility%20data.%20We%20validate%20our%20framework%20on%20the%20Murchison%0AWidefield%20Array%20%28MWA%29%20telescope%20and%20simulated%20data%20and%20the%20Hydrogen%20Epoch%20of%0AReionization%20Array%20%28HERA%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14892v2&entry.124074799=Read"},
{"title": "Quantum Support Vector Machine for Prostate Cancer Detection: A\n  Performance Analysis", "author": "Walid El Maouaki and Taoufik Said and Mohamed Bennai", "abstract": "  This study addresses the urgent need for improved prostate cancer detection\nmethods by harnessing the power of advanced technological solutions. We\nintroduce the application of Quantum Support Vector Machine (QSVM) to this\ncritical healthcare challenge, showcasing an enhancement in diagnostic\nperformance over the classical Support Vector Machine (SVM) approach. Our study\nnot only outlines the remarkable improvements in diagnostic performance made by\nQSVM over the classic SVM technique, but it delves into the advancements\nbrought about by the quantum feature map architecture, which has been carefully\nidentified and evaluated, ensuring it aligns seamlessly with the unique\ncharacteristics of our prostate cancer dataset. This architecture succeded in\ncreating a distinct feature space, enabling the detection of complex,\nnon-linear patterns in the data. The findings reveal not only a comparable\naccuracy with classical SVM ($92\\%$) but also a $7.14\\%$ increase in\nsensitivity and a notably high F1-Score ($93.33\\%$). This study's important\ncombination of quantum computing in medical diagnostics marks a pivotal step\nforward in cancer detection, offering promising implications for the future of\nhealthcare technology.\n", "link": "http://arxiv.org/abs/2403.07856v1", "date": "2024-03-12", "relevancy": 1.6684, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4354}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4147}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4122}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Quantum%20Support%20Vector%20Machine%20for%20Prostate%20Cancer%20Detection%3A%20A%0A%20%20Performance%20Analysis&body=Title%3A%20Quantum%20Support%20Vector%20Machine%20for%20Prostate%20Cancer%20Detection%3A%20A%0A%20%20Performance%20Analysis%0AAuthor%3A%20Walid%20El%20Maouaki%20and%20Taoufik%20Said%20and%20Mohamed%20Bennai%0AAbstract%3A%20%20%20This%20study%20addresses%20the%20urgent%20need%20for%20improved%20prostate%20cancer%20detection%0Amethods%20by%20harnessing%20the%20power%20of%20advanced%20technological%20solutions.%20We%0Aintroduce%20the%20application%20of%20Quantum%20Support%20Vector%20Machine%20%28QSVM%29%20to%20this%0Acritical%20healthcare%20challenge%2C%20showcasing%20an%20enhancement%20in%20diagnostic%0Aperformance%20over%20the%20classical%20Support%20Vector%20Machine%20%28SVM%29%20approach.%20Our%20study%0Anot%20only%20outlines%20the%20remarkable%20improvements%20in%20diagnostic%20performance%20made%20by%0AQSVM%20over%20the%20classic%20SVM%20technique%2C%20but%20it%20delves%20into%20the%20advancements%0Abrought%20about%20by%20the%20quantum%20feature%20map%20architecture%2C%20which%20has%20been%20carefully%0Aidentified%20and%20evaluated%2C%20ensuring%20it%20aligns%20seamlessly%20with%20the%20unique%0Acharacteristics%20of%20our%20prostate%20cancer%20dataset.%20This%20architecture%20succeded%20in%0Acreating%20a%20distinct%20feature%20space%2C%20enabling%20the%20detection%20of%20complex%2C%0Anon-linear%20patterns%20in%20the%20data.%20The%20findings%20reveal%20not%20only%20a%20comparable%0Aaccuracy%20with%20classical%20SVM%20%28%2492%5C%25%24%29%20but%20also%20a%20%247.14%5C%25%24%20increase%20in%0Asensitivity%20and%20a%20notably%20high%20F1-Score%20%28%2493.33%5C%25%24%29.%20This%20study%27s%20important%0Acombination%20of%20quantum%20computing%20in%20medical%20diagnostics%20marks%20a%20pivotal%20step%0Aforward%20in%20cancer%20detection%2C%20offering%20promising%20implications%20for%20the%20future%20of%0Ahealthcare%20technology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07856v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Support%20Vector%20Machine%20for%20Prostate%20Cancer%20Detection%3A%20A%0A%20%20Performance%20Analysis&entry.906535625=Walid%20El%20Maouaki%20and%20Taoufik%20Said%20and%20Mohamed%20Bennai&entry.1292438233=%20%20This%20study%20addresses%20the%20urgent%20need%20for%20improved%20prostate%20cancer%20detection%0Amethods%20by%20harnessing%20the%20power%20of%20advanced%20technological%20solutions.%20We%0Aintroduce%20the%20application%20of%20Quantum%20Support%20Vector%20Machine%20%28QSVM%29%20to%20this%0Acritical%20healthcare%20challenge%2C%20showcasing%20an%20enhancement%20in%20diagnostic%0Aperformance%20over%20the%20classical%20Support%20Vector%20Machine%20%28SVM%29%20approach.%20Our%20study%0Anot%20only%20outlines%20the%20remarkable%20improvements%20in%20diagnostic%20performance%20made%20by%0AQSVM%20over%20the%20classic%20SVM%20technique%2C%20but%20it%20delves%20into%20the%20advancements%0Abrought%20about%20by%20the%20quantum%20feature%20map%20architecture%2C%20which%20has%20been%20carefully%0Aidentified%20and%20evaluated%2C%20ensuring%20it%20aligns%20seamlessly%20with%20the%20unique%0Acharacteristics%20of%20our%20prostate%20cancer%20dataset.%20This%20architecture%20succeded%20in%0Acreating%20a%20distinct%20feature%20space%2C%20enabling%20the%20detection%20of%20complex%2C%0Anon-linear%20patterns%20in%20the%20data.%20The%20findings%20reveal%20not%20only%20a%20comparable%0Aaccuracy%20with%20classical%20SVM%20%28%2492%5C%25%24%29%20but%20also%20a%20%247.14%5C%25%24%20increase%20in%0Asensitivity%20and%20a%20notably%20high%20F1-Score%20%28%2493.33%5C%25%24%29.%20This%20study%27s%20important%0Acombination%20of%20quantum%20computing%20in%20medical%20diagnostics%20marks%20a%20pivotal%20step%0Aforward%20in%20cancer%20detection%2C%20offering%20promising%20implications%20for%20the%20future%20of%0Ahealthcare%20technology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07856v1&entry.124074799=Read"},
{"title": "RobotCycle: Assessing Cycling Safety in Urban Environments", "author": "Efimia Panagiotaki and Tyler Reinmund and Brian Liu and Stephan Mouton and Luke Pitt and Arundathi Shaji Shanthini and Matthew Towlson and Wayne Tubby and Chris Prahacs and Daniele De Martini and Lars Kunze", "abstract": "  This paper introduces RobotCycle, a novel ongoing project that leverages\nAutonomous Vehicle (AV) research to investigate how cycling infrastructure\ninfluences cyclist behaviour and safety during real-world journeys. The\nproject's requirements were defined in collaboration with key stakeholders\n(i.e. city planners, cyclists, and policymakers), informing the design of risk\nand safety metrics and the data collection criteria. We propose a data-driven\napproach relying on a novel, rich dataset of diverse traffic scenes captured\nthrough a custom-designed wearable sensing unit. We extract road-user\ntrajectories and analyse deviations suggesting risk or potentially hazardous\ninteractions in correlation with infrastructural elements in the environment.\nDriving profiles and trajectory patterns are associated with local road\nsegments, driving conditions, and road-user interactions to predict traffic\nbehaviour and identify critical scenarios. Moreover, leveraging advancements in\nAV research, the project extracts detailed 3D maps, traffic flow patterns, and\ntrajectory models to provide an in-depth assessment and analysis of the\nbehaviour of all traffic agents. This data can then inform the design of\ncyclist-friendly road infrastructure, improving road safety and cyclability, as\nit provides valuable insights for enhancing cyclist protection and promoting\nsustainable urban mobility.\n", "link": "http://arxiv.org/abs/2403.07789v1", "date": "2024-03-12", "relevancy": 1.6554, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5809}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5182}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5127}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20RobotCycle%3A%20Assessing%20Cycling%20Safety%20in%20Urban%20Environments&body=Title%3A%20RobotCycle%3A%20Assessing%20Cycling%20Safety%20in%20Urban%20Environments%0AAuthor%3A%20Efimia%20Panagiotaki%20and%20Tyler%20Reinmund%20and%20Brian%20Liu%20and%20Stephan%20Mouton%20and%20Luke%20Pitt%20and%20Arundathi%20Shaji%20Shanthini%20and%20Matthew%20Towlson%20and%20Wayne%20Tubby%20and%20Chris%20Prahacs%20and%20Daniele%20De%20Martini%20and%20Lars%20Kunze%0AAbstract%3A%20%20%20This%20paper%20introduces%20RobotCycle%2C%20a%20novel%20ongoing%20project%20that%20leverages%0AAutonomous%20Vehicle%20%28AV%29%20research%20to%20investigate%20how%20cycling%20infrastructure%0Ainfluences%20cyclist%20behaviour%20and%20safety%20during%20real-world%20journeys.%20The%0Aproject%27s%20requirements%20were%20defined%20in%20collaboration%20with%20key%20stakeholders%0A%28i.e.%20city%20planners%2C%20cyclists%2C%20and%20policymakers%29%2C%20informing%20the%20design%20of%20risk%0Aand%20safety%20metrics%20and%20the%20data%20collection%20criteria.%20We%20propose%20a%20data-driven%0Aapproach%20relying%20on%20a%20novel%2C%20rich%20dataset%20of%20diverse%20traffic%20scenes%20captured%0Athrough%20a%20custom-designed%20wearable%20sensing%20unit.%20We%20extract%20road-user%0Atrajectories%20and%20analyse%20deviations%20suggesting%20risk%20or%20potentially%20hazardous%0Ainteractions%20in%20correlation%20with%20infrastructural%20elements%20in%20the%20environment.%0ADriving%20profiles%20and%20trajectory%20patterns%20are%20associated%20with%20local%20road%0Asegments%2C%20driving%20conditions%2C%20and%20road-user%20interactions%20to%20predict%20traffic%0Abehaviour%20and%20identify%20critical%20scenarios.%20Moreover%2C%20leveraging%20advancements%20in%0AAV%20research%2C%20the%20project%20extracts%20detailed%203D%20maps%2C%20traffic%20flow%20patterns%2C%20and%0Atrajectory%20models%20to%20provide%20an%20in-depth%20assessment%20and%20analysis%20of%20the%0Abehaviour%20of%20all%20traffic%20agents.%20This%20data%20can%20then%20inform%20the%20design%20of%0Acyclist-friendly%20road%20infrastructure%2C%20improving%20road%20safety%20and%20cyclability%2C%20as%0Ait%20provides%20valuable%20insights%20for%20enhancing%20cyclist%20protection%20and%20promoting%0Asustainable%20urban%20mobility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07789v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RobotCycle%3A%20Assessing%20Cycling%20Safety%20in%20Urban%20Environments&entry.906535625=Efimia%20Panagiotaki%20and%20Tyler%20Reinmund%20and%20Brian%20Liu%20and%20Stephan%20Mouton%20and%20Luke%20Pitt%20and%20Arundathi%20Shaji%20Shanthini%20and%20Matthew%20Towlson%20and%20Wayne%20Tubby%20and%20Chris%20Prahacs%20and%20Daniele%20De%20Martini%20and%20Lars%20Kunze&entry.1292438233=%20%20This%20paper%20introduces%20RobotCycle%2C%20a%20novel%20ongoing%20project%20that%20leverages%0AAutonomous%20Vehicle%20%28AV%29%20research%20to%20investigate%20how%20cycling%20infrastructure%0Ainfluences%20cyclist%20behaviour%20and%20safety%20during%20real-world%20journeys.%20The%0Aproject%27s%20requirements%20were%20defined%20in%20collaboration%20with%20key%20stakeholders%0A%28i.e.%20city%20planners%2C%20cyclists%2C%20and%20policymakers%29%2C%20informing%20the%20design%20of%20risk%0Aand%20safety%20metrics%20and%20the%20data%20collection%20criteria.%20We%20propose%20a%20data-driven%0Aapproach%20relying%20on%20a%20novel%2C%20rich%20dataset%20of%20diverse%20traffic%20scenes%20captured%0Athrough%20a%20custom-designed%20wearable%20sensing%20unit.%20We%20extract%20road-user%0Atrajectories%20and%20analyse%20deviations%20suggesting%20risk%20or%20potentially%20hazardous%0Ainteractions%20in%20correlation%20with%20infrastructural%20elements%20in%20the%20environment.%0ADriving%20profiles%20and%20trajectory%20patterns%20are%20associated%20with%20local%20road%0Asegments%2C%20driving%20conditions%2C%20and%20road-user%20interactions%20to%20predict%20traffic%0Abehaviour%20and%20identify%20critical%20scenarios.%20Moreover%2C%20leveraging%20advancements%20in%0AAV%20research%2C%20the%20project%20extracts%20detailed%203D%20maps%2C%20traffic%20flow%20patterns%2C%20and%0Atrajectory%20models%20to%20provide%20an%20in-depth%20assessment%20and%20analysis%20of%20the%0Abehaviour%20of%20all%20traffic%20agents.%20This%20data%20can%20then%20inform%20the%20design%20of%0Acyclist-friendly%20road%20infrastructure%2C%20improving%20road%20safety%20and%20cyclability%2C%20as%0Ait%20provides%20valuable%20insights%20for%20enhancing%20cyclist%20protection%20and%20promoting%0Asustainable%20urban%20mobility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07789v1&entry.124074799=Read"},
{"title": "The Virtues of Laziness: Multi-Query Kinodynamic Motion Planning with\n  Lazy Methods", "author": "Anuj Pasricha and Alessandro Roncone", "abstract": "  In this work, we introduce LazyBoE, a multi-query method for kinodynamic\nmotion planning with forward propagation. This algorithm allows for the\nsimultaneous exploration of a robot's state and control spaces, thereby\nenabling a wider suite of dynamic tasks in real-world applications. Our\ncontributions are three-fold: i) a method for discretizing the state and\ncontrol spaces to amortize planning times across multiple queries; ii) lazy\napproaches to collision checking and propagation of control sequences that\ndecrease the cost of physics-based simulation; and iii) LazyBoE, a robust\nkinodynamic planner that leverages these two contributions to produce\ndynamically-feasible trajectories. The proposed framework not only reduces\nplanning time but also increases success rate in comparison to previous\napproaches.\n", "link": "http://arxiv.org/abs/2403.07867v1", "date": "2024-03-12", "relevancy": 1.6523, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5669}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5541}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5262}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20The%20Virtues%20of%20Laziness%3A%20Multi-Query%20Kinodynamic%20Motion%20Planning%20with%0A%20%20Lazy%20Methods&body=Title%3A%20The%20Virtues%20of%20Laziness%3A%20Multi-Query%20Kinodynamic%20Motion%20Planning%20with%0A%20%20Lazy%20Methods%0AAuthor%3A%20Anuj%20Pasricha%20and%20Alessandro%20Roncone%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20LazyBoE%2C%20a%20multi-query%20method%20for%20kinodynamic%0Amotion%20planning%20with%20forward%20propagation.%20This%20algorithm%20allows%20for%20the%0Asimultaneous%20exploration%20of%20a%20robot%27s%20state%20and%20control%20spaces%2C%20thereby%0Aenabling%20a%20wider%20suite%20of%20dynamic%20tasks%20in%20real-world%20applications.%20Our%0Acontributions%20are%20three-fold%3A%20i%29%20a%20method%20for%20discretizing%20the%20state%20and%0Acontrol%20spaces%20to%20amortize%20planning%20times%20across%20multiple%20queries%3B%20ii%29%20lazy%0Aapproaches%20to%20collision%20checking%20and%20propagation%20of%20control%20sequences%20that%0Adecrease%20the%20cost%20of%20physics-based%20simulation%3B%20and%20iii%29%20LazyBoE%2C%20a%20robust%0Akinodynamic%20planner%20that%20leverages%20these%20two%20contributions%20to%20produce%0Adynamically-feasible%20trajectories.%20The%20proposed%20framework%20not%20only%20reduces%0Aplanning%20time%20but%20also%20increases%20success%20rate%20in%20comparison%20to%20previous%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07867v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Virtues%20of%20Laziness%3A%20Multi-Query%20Kinodynamic%20Motion%20Planning%20with%0A%20%20Lazy%20Methods&entry.906535625=Anuj%20Pasricha%20and%20Alessandro%20Roncone&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20LazyBoE%2C%20a%20multi-query%20method%20for%20kinodynamic%0Amotion%20planning%20with%20forward%20propagation.%20This%20algorithm%20allows%20for%20the%0Asimultaneous%20exploration%20of%20a%20robot%27s%20state%20and%20control%20spaces%2C%20thereby%0Aenabling%20a%20wider%20suite%20of%20dynamic%20tasks%20in%20real-world%20applications.%20Our%0Acontributions%20are%20three-fold%3A%20i%29%20a%20method%20for%20discretizing%20the%20state%20and%0Acontrol%20spaces%20to%20amortize%20planning%20times%20across%20multiple%20queries%3B%20ii%29%20lazy%0Aapproaches%20to%20collision%20checking%20and%20propagation%20of%20control%20sequences%20that%0Adecrease%20the%20cost%20of%20physics-based%20simulation%3B%20and%20iii%29%20LazyBoE%2C%20a%20robust%0Akinodynamic%20planner%20that%20leverages%20these%20two%20contributions%20to%20produce%0Adynamically-feasible%20trajectories.%20The%20proposed%20framework%20not%20only%20reduces%0Aplanning%20time%20but%20also%20increases%20success%20rate%20in%20comparison%20to%20previous%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07867v1&entry.124074799=Read"},
{"title": "Unsupervised Multi-Person 3D Human Pose Estimation From 2D Poses Alone", "author": "Peter Hardy and Hansung Kim", "abstract": "  Current unsupervised 2D-3D human pose estimation (HPE) methods do not work in\nmulti-person scenarios due to perspective ambiguity in monocular images.\nTherefore, we present one of the first studies investigating the feasibility of\nunsupervised multi-person 2D-3D HPE from just 2D poses alone, focusing on\nreconstructing human interactions. To address the issue of perspective\nambiguity, we expand upon prior work by predicting the cameras' elevation angle\nrelative to the subjects' pelvis. This allows us to rotate the predicted poses\nto be level with the ground plane, while obtaining an estimate for the vertical\noffset in 3D between individuals. Our method involves independently lifting\neach subject's 2D pose to 3D, before combining them in a shared 3D coordinate\nsystem. The poses are then rotated and offset by the predicted elevation angle\nbefore being scaled. This by itself enables us to retrieve an accurate 3D\nreconstruction of their poses. We present our results on the CHI3D dataset,\nintroducing its use for unsupervised 2D-3D pose estimation with three new\nquantitative metrics, and establishing a benchmark for future research.\n", "link": "http://arxiv.org/abs/2309.14865v3", "date": "2024-03-12", "relevancy": 1.5797, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5341}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5199}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5144}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Multi-Person%203D%20Human%20Pose%20Estimation%20From%202D%20Poses%20Alone&body=Title%3A%20Unsupervised%20Multi-Person%203D%20Human%20Pose%20Estimation%20From%202D%20Poses%20Alone%0AAuthor%3A%20Peter%20Hardy%20and%20Hansung%20Kim%0AAbstract%3A%20%20%20Current%20unsupervised%202D-3D%20human%20pose%20estimation%20%28HPE%29%20methods%20do%20not%20work%20in%0Amulti-person%20scenarios%20due%20to%20perspective%20ambiguity%20in%20monocular%20images.%0ATherefore%2C%20we%20present%20one%20of%20the%20first%20studies%20investigating%20the%20feasibility%20of%0Aunsupervised%20multi-person%202D-3D%20HPE%20from%20just%202D%20poses%20alone%2C%20focusing%20on%0Areconstructing%20human%20interactions.%20To%20address%20the%20issue%20of%20perspective%0Aambiguity%2C%20we%20expand%20upon%20prior%20work%20by%20predicting%20the%20cameras%27%20elevation%20angle%0Arelative%20to%20the%20subjects%27%20pelvis.%20This%20allows%20us%20to%20rotate%20the%20predicted%20poses%0Ato%20be%20level%20with%20the%20ground%20plane%2C%20while%20obtaining%20an%20estimate%20for%20the%20vertical%0Aoffset%20in%203D%20between%20individuals.%20Our%20method%20involves%20independently%20lifting%0Aeach%20subject%27s%202D%20pose%20to%203D%2C%20before%20combining%20them%20in%20a%20shared%203D%20coordinate%0Asystem.%20The%20poses%20are%20then%20rotated%20and%20offset%20by%20the%20predicted%20elevation%20angle%0Abefore%20being%20scaled.%20This%20by%20itself%20enables%20us%20to%20retrieve%20an%20accurate%203D%0Areconstruction%20of%20their%20poses.%20We%20present%20our%20results%20on%20the%20CHI3D%20dataset%2C%0Aintroducing%20its%20use%20for%20unsupervised%202D-3D%20pose%20estimation%20with%20three%20new%0Aquantitative%20metrics%2C%20and%20establishing%20a%20benchmark%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.14865v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Multi-Person%203D%20Human%20Pose%20Estimation%20From%202D%20Poses%20Alone&entry.906535625=Peter%20Hardy%20and%20Hansung%20Kim&entry.1292438233=%20%20Current%20unsupervised%202D-3D%20human%20pose%20estimation%20%28HPE%29%20methods%20do%20not%20work%20in%0Amulti-person%20scenarios%20due%20to%20perspective%20ambiguity%20in%20monocular%20images.%0ATherefore%2C%20we%20present%20one%20of%20the%20first%20studies%20investigating%20the%20feasibility%20of%0Aunsupervised%20multi-person%202D-3D%20HPE%20from%20just%202D%20poses%20alone%2C%20focusing%20on%0Areconstructing%20human%20interactions.%20To%20address%20the%20issue%20of%20perspective%0Aambiguity%2C%20we%20expand%20upon%20prior%20work%20by%20predicting%20the%20cameras%27%20elevation%20angle%0Arelative%20to%20the%20subjects%27%20pelvis.%20This%20allows%20us%20to%20rotate%20the%20predicted%20poses%0Ato%20be%20level%20with%20the%20ground%20plane%2C%20while%20obtaining%20an%20estimate%20for%20the%20vertical%0Aoffset%20in%203D%20between%20individuals.%20Our%20method%20involves%20independently%20lifting%0Aeach%20subject%27s%202D%20pose%20to%203D%2C%20before%20combining%20them%20in%20a%20shared%203D%20coordinate%0Asystem.%20The%20poses%20are%20then%20rotated%20and%20offset%20by%20the%20predicted%20elevation%20angle%0Abefore%20being%20scaled.%20This%20by%20itself%20enables%20us%20to%20retrieve%20an%20accurate%203D%0Areconstruction%20of%20their%20poses.%20We%20present%20our%20results%20on%20the%20CHI3D%20dataset%2C%0Aintroducing%20its%20use%20for%20unsupervised%202D-3D%20pose%20estimation%20with%20three%20new%0Aquantitative%20metrics%2C%20and%20establishing%20a%20benchmark%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.14865v3&entry.124074799=Read"},
{"title": "Rigid Transformations for Stabilized Lower Dimensional Space to Support\n  Subsurface Uncertainty Quantification and Interpretation", "author": "Ademide O. Mabadeje and Michael J. Pyrcz", "abstract": "  Subsurface datasets inherently possess big data characteristics such as vast\nvolume, diverse features, and high sampling speeds, further compounded by the\ncurse of dimensionality from various physical, engineering, and geological\ninputs. Among the existing dimensionality reduction (DR) methods, nonlinear\ndimensionality reduction (NDR) methods, especially Metric-multidimensional\nscaling (MDS), are preferred for subsurface datasets due to their inherent\ncomplexity. While MDS retains intrinsic data structure and quantifies\nuncertainty, its limitations include unstabilized unique solutions invariant to\nEuclidean transformations and an absence of out-of-sample points (OOSP)\nextension. To enhance subsurface inferential and machine learning workflows,\ndatasets must be transformed into stable, reduced-dimension representations\nthat accommodate OOSP.\n  Our solution employs rigid transformations for a stabilized Euclidean\ninvariant representation for LDS. By computing an MDS input dissimilarity\nmatrix, and applying rigid transformations on multiple realizations, we ensure\ntransformation invariance and integrate OOSP. This process leverages a convex\nhull algorithm and incorporates loss function and normalized stress for\ndistortion quantification. We validate our approach with synthetic data,\nvarying distance metrics, and real-world wells from the Duvernay Formation.\nResults confirm our method's efficacy in achieving consistent LDS\nrepresentations. Furthermore, our proposed \"stress ratio\" (SR) metric provides\ninsight into uncertainty, beneficial for model adjustments and inferential\nanalysis. Consequently, our workflow promises enhanced repeatability and\ncomparability in NDR for subsurface energy resource engineering and associated\nbig data workflows.\n", "link": "http://arxiv.org/abs/2308.08079v3", "date": "2024-03-12", "relevancy": 1.57, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.535}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5272}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.502}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Rigid%20Transformations%20for%20Stabilized%20Lower%20Dimensional%20Space%20to%20Support%0A%20%20Subsurface%20Uncertainty%20Quantification%20and%20Interpretation&body=Title%3A%20Rigid%20Transformations%20for%20Stabilized%20Lower%20Dimensional%20Space%20to%20Support%0A%20%20Subsurface%20Uncertainty%20Quantification%20and%20Interpretation%0AAuthor%3A%20Ademide%20O.%20Mabadeje%20and%20Michael%20J.%20Pyrcz%0AAbstract%3A%20%20%20Subsurface%20datasets%20inherently%20possess%20big%20data%20characteristics%20such%20as%20vast%0Avolume%2C%20diverse%20features%2C%20and%20high%20sampling%20speeds%2C%20further%20compounded%20by%20the%0Acurse%20of%20dimensionality%20from%20various%20physical%2C%20engineering%2C%20and%20geological%0Ainputs.%20Among%20the%20existing%20dimensionality%20reduction%20%28DR%29%20methods%2C%20nonlinear%0Adimensionality%20reduction%20%28NDR%29%20methods%2C%20especially%20Metric-multidimensional%0Ascaling%20%28MDS%29%2C%20are%20preferred%20for%20subsurface%20datasets%20due%20to%20their%20inherent%0Acomplexity.%20While%20MDS%20retains%20intrinsic%20data%20structure%20and%20quantifies%0Auncertainty%2C%20its%20limitations%20include%20unstabilized%20unique%20solutions%20invariant%20to%0AEuclidean%20transformations%20and%20an%20absence%20of%20out-of-sample%20points%20%28OOSP%29%0Aextension.%20To%20enhance%20subsurface%20inferential%20and%20machine%20learning%20workflows%2C%0Adatasets%20must%20be%20transformed%20into%20stable%2C%20reduced-dimension%20representations%0Athat%20accommodate%20OOSP.%0A%20%20Our%20solution%20employs%20rigid%20transformations%20for%20a%20stabilized%20Euclidean%0Ainvariant%20representation%20for%20LDS.%20By%20computing%20an%20MDS%20input%20dissimilarity%0Amatrix%2C%20and%20applying%20rigid%20transformations%20on%20multiple%20realizations%2C%20we%20ensure%0Atransformation%20invariance%20and%20integrate%20OOSP.%20This%20process%20leverages%20a%20convex%0Ahull%20algorithm%20and%20incorporates%20loss%20function%20and%20normalized%20stress%20for%0Adistortion%20quantification.%20We%20validate%20our%20approach%20with%20synthetic%20data%2C%0Avarying%20distance%20metrics%2C%20and%20real-world%20wells%20from%20the%20Duvernay%20Formation.%0AResults%20confirm%20our%20method%27s%20efficacy%20in%20achieving%20consistent%20LDS%0Arepresentations.%20Furthermore%2C%20our%20proposed%20%22stress%20ratio%22%20%28SR%29%20metric%20provides%0Ainsight%20into%20uncertainty%2C%20beneficial%20for%20model%20adjustments%20and%20inferential%0Aanalysis.%20Consequently%2C%20our%20workflow%20promises%20enhanced%20repeatability%20and%0Acomparability%20in%20NDR%20for%20subsurface%20energy%20resource%20engineering%20and%20associated%0Abig%20data%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.08079v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rigid%20Transformations%20for%20Stabilized%20Lower%20Dimensional%20Space%20to%20Support%0A%20%20Subsurface%20Uncertainty%20Quantification%20and%20Interpretation&entry.906535625=Ademide%20O.%20Mabadeje%20and%20Michael%20J.%20Pyrcz&entry.1292438233=%20%20Subsurface%20datasets%20inherently%20possess%20big%20data%20characteristics%20such%20as%20vast%0Avolume%2C%20diverse%20features%2C%20and%20high%20sampling%20speeds%2C%20further%20compounded%20by%20the%0Acurse%20of%20dimensionality%20from%20various%20physical%2C%20engineering%2C%20and%20geological%0Ainputs.%20Among%20the%20existing%20dimensionality%20reduction%20%28DR%29%20methods%2C%20nonlinear%0Adimensionality%20reduction%20%28NDR%29%20methods%2C%20especially%20Metric-multidimensional%0Ascaling%20%28MDS%29%2C%20are%20preferred%20for%20subsurface%20datasets%20due%20to%20their%20inherent%0Acomplexity.%20While%20MDS%20retains%20intrinsic%20data%20structure%20and%20quantifies%0Auncertainty%2C%20its%20limitations%20include%20unstabilized%20unique%20solutions%20invariant%20to%0AEuclidean%20transformations%20and%20an%20absence%20of%20out-of-sample%20points%20%28OOSP%29%0Aextension.%20To%20enhance%20subsurface%20inferential%20and%20machine%20learning%20workflows%2C%0Adatasets%20must%20be%20transformed%20into%20stable%2C%20reduced-dimension%20representations%0Athat%20accommodate%20OOSP.%0A%20%20Our%20solution%20employs%20rigid%20transformations%20for%20a%20stabilized%20Euclidean%0Ainvariant%20representation%20for%20LDS.%20By%20computing%20an%20MDS%20input%20dissimilarity%0Amatrix%2C%20and%20applying%20rigid%20transformations%20on%20multiple%20realizations%2C%20we%20ensure%0Atransformation%20invariance%20and%20integrate%20OOSP.%20This%20process%20leverages%20a%20convex%0Ahull%20algorithm%20and%20incorporates%20loss%20function%20and%20normalized%20stress%20for%0Adistortion%20quantification.%20We%20validate%20our%20approach%20with%20synthetic%20data%2C%0Avarying%20distance%20metrics%2C%20and%20real-world%20wells%20from%20the%20Duvernay%20Formation.%0AResults%20confirm%20our%20method%27s%20efficacy%20in%20achieving%20consistent%20LDS%0Arepresentations.%20Furthermore%2C%20our%20proposed%20%22stress%20ratio%22%20%28SR%29%20metric%20provides%0Ainsight%20into%20uncertainty%2C%20beneficial%20for%20model%20adjustments%20and%20inferential%0Aanalysis.%20Consequently%2C%20our%20workflow%20promises%20enhanced%20repeatability%20and%0Acomparability%20in%20NDR%20for%20subsurface%20energy%20resource%20engineering%20and%20associated%0Abig%20data%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.08079v3&entry.124074799=Read"},
{"title": "Quantifying and Mitigating Privacy Risks for Tabular Generative Models", "author": "Chaoyi Zhu and Jiayi Tang and Hans Brouwer and Juan F. P\u00e9rez and Marten van Dijk and Lydia Y. Chen", "abstract": "  Synthetic data from generative models emerges as the privacy-preserving\ndata-sharing solution. Such a synthetic data set shall resemble the original\ndata without revealing identifiable private information. The backbone\ntechnology of tabular synthesizers is rooted in image generative models,\nranging from Generative Adversarial Networks (GANs) to recent diffusion models.\nRecent prior work sheds light on the utility-privacy tradeoff on tabular data,\nrevealing and quantifying privacy risks on synthetic data. We first conduct an\nexhaustive empirical analysis, highlighting the utility-privacy tradeoff of\nfive state-of-the-art tabular synthesizers, against eight privacy attacks, with\na special focus on membership inference attacks. Motivated by the observation\nof high data quality but also high privacy risk in tabular diffusion, we\npropose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which\nis composed of an autoencoder network to encode the tabular data and a latent\ndiffusion model to synthesize the latent tables. Following the emerging f-DP\nframework, we apply DP-SGD to train the auto-encoder in combination with batch\nclipping and use the separation value as the privacy metric to better capture\nthe privacy gain from DP algorithms. Our empirical evaluation demonstrates that\nDP-TLDM is capable of achieving a meaningful theoretical privacy guarantee\nwhile also significantly enhancing the utility of synthetic data. Specifically,\ncompared to other DP-protected tabular generative models, DP-TLDM improves the\nsynthetic quality by an average of 35% in data resemblance, 15% in the utility\nfor downstream tasks, and 50% in data discriminability, all while preserving a\ncomparable level of privacy risk.\n", "link": "http://arxiv.org/abs/2403.07842v1", "date": "2024-03-12", "relevancy": 1.5229, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5254}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5095}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4853}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Quantifying%20and%20Mitigating%20Privacy%20Risks%20for%20Tabular%20Generative%20Models&body=Title%3A%20Quantifying%20and%20Mitigating%20Privacy%20Risks%20for%20Tabular%20Generative%20Models%0AAuthor%3A%20Chaoyi%20Zhu%20and%20Jiayi%20Tang%20and%20Hans%20Brouwer%20and%20Juan%20F.%20P%C3%A9rez%20and%20Marten%20van%20Dijk%20and%20Lydia%20Y.%20Chen%0AAbstract%3A%20%20%20Synthetic%20data%20from%20generative%20models%20emerges%20as%20the%20privacy-preserving%0Adata-sharing%20solution.%20Such%20a%20synthetic%20data%20set%20shall%20resemble%20the%20original%0Adata%20without%20revealing%20identifiable%20private%20information.%20The%20backbone%0Atechnology%20of%20tabular%20synthesizers%20is%20rooted%20in%20image%20generative%20models%2C%0Aranging%20from%20Generative%20Adversarial%20Networks%20%28GANs%29%20to%20recent%20diffusion%20models.%0ARecent%20prior%20work%20sheds%20light%20on%20the%20utility-privacy%20tradeoff%20on%20tabular%20data%2C%0Arevealing%20and%20quantifying%20privacy%20risks%20on%20synthetic%20data.%20We%20first%20conduct%20an%0Aexhaustive%20empirical%20analysis%2C%20highlighting%20the%20utility-privacy%20tradeoff%20of%0Afive%20state-of-the-art%20tabular%20synthesizers%2C%20against%20eight%20privacy%20attacks%2C%20with%0Aa%20special%20focus%20on%20membership%20inference%20attacks.%20Motivated%20by%20the%20observation%0Aof%20high%20data%20quality%20but%20also%20high%20privacy%20risk%20in%20tabular%20diffusion%2C%20we%0Apropose%20DP-TLDM%2C%20Differentially%20Private%20Tabular%20Latent%20Diffusion%20Model%2C%20which%0Ais%20composed%20of%20an%20autoencoder%20network%20to%20encode%20the%20tabular%20data%20and%20a%20latent%0Adiffusion%20model%20to%20synthesize%20the%20latent%20tables.%20Following%20the%20emerging%20f-DP%0Aframework%2C%20we%20apply%20DP-SGD%20to%20train%20the%20auto-encoder%20in%20combination%20with%20batch%0Aclipping%20and%20use%20the%20separation%20value%20as%20the%20privacy%20metric%20to%20better%20capture%0Athe%20privacy%20gain%20from%20DP%20algorithms.%20Our%20empirical%20evaluation%20demonstrates%20that%0ADP-TLDM%20is%20capable%20of%20achieving%20a%20meaningful%20theoretical%20privacy%20guarantee%0Awhile%20also%20significantly%20enhancing%20the%20utility%20of%20synthetic%20data.%20Specifically%2C%0Acompared%20to%20other%20DP-protected%20tabular%20generative%20models%2C%20DP-TLDM%20improves%20the%0Asynthetic%20quality%20by%20an%20average%20of%2035%25%20in%20data%20resemblance%2C%2015%25%20in%20the%20utility%0Afor%20downstream%20tasks%2C%20and%2050%25%20in%20data%20discriminability%2C%20all%20while%20preserving%20a%0Acomparable%20level%20of%20privacy%20risk.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07842v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20and%20Mitigating%20Privacy%20Risks%20for%20Tabular%20Generative%20Models&entry.906535625=Chaoyi%20Zhu%20and%20Jiayi%20Tang%20and%20Hans%20Brouwer%20and%20Juan%20F.%20P%C3%A9rez%20and%20Marten%20van%20Dijk%20and%20Lydia%20Y.%20Chen&entry.1292438233=%20%20Synthetic%20data%20from%20generative%20models%20emerges%20as%20the%20privacy-preserving%0Adata-sharing%20solution.%20Such%20a%20synthetic%20data%20set%20shall%20resemble%20the%20original%0Adata%20without%20revealing%20identifiable%20private%20information.%20The%20backbone%0Atechnology%20of%20tabular%20synthesizers%20is%20rooted%20in%20image%20generative%20models%2C%0Aranging%20from%20Generative%20Adversarial%20Networks%20%28GANs%29%20to%20recent%20diffusion%20models.%0ARecent%20prior%20work%20sheds%20light%20on%20the%20utility-privacy%20tradeoff%20on%20tabular%20data%2C%0Arevealing%20and%20quantifying%20privacy%20risks%20on%20synthetic%20data.%20We%20first%20conduct%20an%0Aexhaustive%20empirical%20analysis%2C%20highlighting%20the%20utility-privacy%20tradeoff%20of%0Afive%20state-of-the-art%20tabular%20synthesizers%2C%20against%20eight%20privacy%20attacks%2C%20with%0Aa%20special%20focus%20on%20membership%20inference%20attacks.%20Motivated%20by%20the%20observation%0Aof%20high%20data%20quality%20but%20also%20high%20privacy%20risk%20in%20tabular%20diffusion%2C%20we%0Apropose%20DP-TLDM%2C%20Differentially%20Private%20Tabular%20Latent%20Diffusion%20Model%2C%20which%0Ais%20composed%20of%20an%20autoencoder%20network%20to%20encode%20the%20tabular%20data%20and%20a%20latent%0Adiffusion%20model%20to%20synthesize%20the%20latent%20tables.%20Following%20the%20emerging%20f-DP%0Aframework%2C%20we%20apply%20DP-SGD%20to%20train%20the%20auto-encoder%20in%20combination%20with%20batch%0Aclipping%20and%20use%20the%20separation%20value%20as%20the%20privacy%20metric%20to%20better%20capture%0Athe%20privacy%20gain%20from%20DP%20algorithms.%20Our%20empirical%20evaluation%20demonstrates%20that%0ADP-TLDM%20is%20capable%20of%20achieving%20a%20meaningful%20theoretical%20privacy%20guarantee%0Awhile%20also%20significantly%20enhancing%20the%20utility%20of%20synthetic%20data.%20Specifically%2C%0Acompared%20to%20other%20DP-protected%20tabular%20generative%20models%2C%20DP-TLDM%20improves%20the%0Asynthetic%20quality%20by%20an%20average%20of%2035%25%20in%20data%20resemblance%2C%2015%25%20in%20the%20utility%0Afor%20downstream%20tasks%2C%20and%2050%25%20in%20data%20discriminability%2C%20all%20while%20preserving%20a%0Acomparable%20level%20of%20privacy%20risk.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07842v1&entry.124074799=Read"},
{"title": "StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting", "author": "Kunhao Liu and Fangneng Zhan and Muyu Xu and Christian Theobalt and Ling Shao and Shijian Lu", "abstract": "  We introduce StyleGaussian, a novel 3D style transfer technique that allows\ninstant transfer of any image's style to a 3D scene at 10 frames per second\n(fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style\ntransfer without compromising its real-time rendering ability and multi-view\nconsistency. It achieves instant style transfer with three steps: embedding,\ntransfer, and decoding. Initially, 2D VGG scene features are embedded into\nreconstructed 3D Gaussians. Next, the embedded features are transformed\naccording to a reference style image. Finally, the transformed features are\ndecoded into the stylized RGB. StyleGaussian has two novel designs. The first\nis an efficient feature rendering strategy that first renders low-dimensional\nfeatures and then maps them into high-dimensional features while embedding VGG\nfeatures. It cuts the memory consumption significantly and enables 3DGS to\nrender the high-dimensional memory-intensive features. The second is a\nK-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized\nfeatures, it eliminates the 2D CNN operations that compromise strict multi-view\nconsistency. Extensive experiments show that StyleGaussian achieves instant 3D\nstylization with superior stylization quality while preserving real-time\nrendering and strict multi-view consistency. Project page:\nhttps://kunhao-liu.github.io/StyleGaussian/\n", "link": "http://arxiv.org/abs/2403.07807v1", "date": "2024-03-12", "relevancy": 1.4939, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5064}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4977}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4947}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20StyleGaussian%3A%20Instant%203D%20Style%20Transfer%20with%20Gaussian%20Splatting&body=Title%3A%20StyleGaussian%3A%20Instant%203D%20Style%20Transfer%20with%20Gaussian%20Splatting%0AAuthor%3A%20Kunhao%20Liu%20and%20Fangneng%20Zhan%20and%20Muyu%20Xu%20and%20Christian%20Theobalt%20and%20Ling%20Shao%20and%20Shijian%20Lu%0AAbstract%3A%20%20%20We%20introduce%20StyleGaussian%2C%20a%20novel%203D%20style%20transfer%20technique%20that%20allows%0Ainstant%20transfer%20of%20any%20image%27s%20style%20to%20a%203D%20scene%20at%2010%20frames%20per%20second%0A%28fps%29.%20Leveraging%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20StyleGaussian%20achieves%20style%0Atransfer%20without%20compromising%20its%20real-time%20rendering%20ability%20and%20multi-view%0Aconsistency.%20It%20achieves%20instant%20style%20transfer%20with%20three%20steps%3A%20embedding%2C%0Atransfer%2C%20and%20decoding.%20Initially%2C%202D%20VGG%20scene%20features%20are%20embedded%20into%0Areconstructed%203D%20Gaussians.%20Next%2C%20the%20embedded%20features%20are%20transformed%0Aaccording%20to%20a%20reference%20style%20image.%20Finally%2C%20the%20transformed%20features%20are%0Adecoded%20into%20the%20stylized%20RGB.%20StyleGaussian%20has%20two%20novel%20designs.%20The%20first%0Ais%20an%20efficient%20feature%20rendering%20strategy%20that%20first%20renders%20low-dimensional%0Afeatures%20and%20then%20maps%20them%20into%20high-dimensional%20features%20while%20embedding%20VGG%0Afeatures.%20It%20cuts%20the%20memory%20consumption%20significantly%20and%20enables%203DGS%20to%0Arender%20the%20high-dimensional%20memory-intensive%20features.%20The%20second%20is%20a%0AK-nearest-neighbor-based%203D%20CNN.%20Working%20as%20the%20decoder%20for%20the%20stylized%0Afeatures%2C%20it%20eliminates%20the%202D%20CNN%20operations%20that%20compromise%20strict%20multi-view%0Aconsistency.%20Extensive%20experiments%20show%20that%20StyleGaussian%20achieves%20instant%203D%0Astylization%20with%20superior%20stylization%20quality%20while%20preserving%20real-time%0Arendering%20and%20strict%20multi-view%20consistency.%20Project%20page%3A%0Ahttps%3A//kunhao-liu.github.io/StyleGaussian/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07807v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StyleGaussian%3A%20Instant%203D%20Style%20Transfer%20with%20Gaussian%20Splatting&entry.906535625=Kunhao%20Liu%20and%20Fangneng%20Zhan%20and%20Muyu%20Xu%20and%20Christian%20Theobalt%20and%20Ling%20Shao%20and%20Shijian%20Lu&entry.1292438233=%20%20We%20introduce%20StyleGaussian%2C%20a%20novel%203D%20style%20transfer%20technique%20that%20allows%0Ainstant%20transfer%20of%20any%20image%27s%20style%20to%20a%203D%20scene%20at%2010%20frames%20per%20second%0A%28fps%29.%20Leveraging%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20StyleGaussian%20achieves%20style%0Atransfer%20without%20compromising%20its%20real-time%20rendering%20ability%20and%20multi-view%0Aconsistency.%20It%20achieves%20instant%20style%20transfer%20with%20three%20steps%3A%20embedding%2C%0Atransfer%2C%20and%20decoding.%20Initially%2C%202D%20VGG%20scene%20features%20are%20embedded%20into%0Areconstructed%203D%20Gaussians.%20Next%2C%20the%20embedded%20features%20are%20transformed%0Aaccording%20to%20a%20reference%20style%20image.%20Finally%2C%20the%20transformed%20features%20are%0Adecoded%20into%20the%20stylized%20RGB.%20StyleGaussian%20has%20two%20novel%20designs.%20The%20first%0Ais%20an%20efficient%20feature%20rendering%20strategy%20that%20first%20renders%20low-dimensional%0Afeatures%20and%20then%20maps%20them%20into%20high-dimensional%20features%20while%20embedding%20VGG%0Afeatures.%20It%20cuts%20the%20memory%20consumption%20significantly%20and%20enables%203DGS%20to%0Arender%20the%20high-dimensional%20memory-intensive%20features.%20The%20second%20is%20a%0AK-nearest-neighbor-based%203D%20CNN.%20Working%20as%20the%20decoder%20for%20the%20stylized%0Afeatures%2C%20it%20eliminates%20the%202D%20CNN%20operations%20that%20compromise%20strict%20multi-view%0Aconsistency.%20Extensive%20experiments%20show%20that%20StyleGaussian%20achieves%20instant%203D%0Astylization%20with%20superior%20stylization%20quality%20while%20preserving%20real-time%0Arendering%20and%20strict%20multi-view%20consistency.%20Project%20page%3A%0Ahttps%3A//kunhao-liu.github.io/StyleGaussian/%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07807v1&entry.124074799=Read"},
{"title": "On Solving Close Enough Orienteering Problem with Overlapped\n  Neighborhoods", "author": "Qiuchen Qian and Yanran Wang and David Boyle", "abstract": "  Close Enough Traveling Salesman Problem (CETSP) is a well-known variant of\nTSP whereby the agent may complete its mission at any point within a target\nneighborhood. Heuristics based on overlapped neighborhoods, known as Steiner\nZones (SZ), have gained attention in addressing CETSP. While SZs offer\neffective approximations to the original graph, their inherent overlap imposes\nconstraints on search space, potentially conflicting with global optimization\nobjectives. Here we show how such limitations can be converted into advantages\nin a Close Enough Orienteering Problem (CEOP) by aggregating prizes across\noverlapped neighborhoods. We further extend classic CEOP with Non-uniform\nNeighborhoods (CEOP-N) by introducing non-uniform costs for prize collection.\nTo tackle CEOP and CEOP-N, we develop a new approach featuring a Randomized\nSteiner Zone Discretization (RSZD) scheme coupled with a hybrid algorithm based\non Particle Swarm Optimization (PSO) and Ant Colony System (ACS), CRaSZe-AntS.\nThe RSZD scheme identifies sub-regions for PSO exploration, and ACS determines\nthe discrete visiting sequence. We evaluate the RSZD's discretization\nperformance on CEOP instances derived from established CETSP instances and\ncompare CRaSZe-AntS against the most relevant state-of-the-art heuristic\nfocused on single-neighborhood optimization for CEOP instances. We also compare\nthe performance of the interior search within SZs and the boundary search on\nindividual neighborhoods in the context of CEOP-N. Our experimental results\nshow that CRaSZe-AntS can yield comparable solution quality with significantly\nreduced computation time compared to the single neighborhood strategy, where we\nobserve an average 140.44% increase in prize collection and a 55.18% reduction\nin algorithm execution time. CRaSZe-AntS is thus highly effective in solving\nemerging CEOP-N, examples of which include truck-and-drone delivery scenarios.\n", "link": "http://arxiv.org/abs/2310.04257v2", "date": "2024-03-12", "relevancy": 1.4519, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5323}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4797}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4464}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20On%20Solving%20Close%20Enough%20Orienteering%20Problem%20with%20Overlapped%0A%20%20Neighborhoods&body=Title%3A%20On%20Solving%20Close%20Enough%20Orienteering%20Problem%20with%20Overlapped%0A%20%20Neighborhoods%0AAuthor%3A%20Qiuchen%20Qian%20and%20Yanran%20Wang%20and%20David%20Boyle%0AAbstract%3A%20%20%20Close%20Enough%20Traveling%20Salesman%20Problem%20%28CETSP%29%20is%20a%20well-known%20variant%20of%0ATSP%20whereby%20the%20agent%20may%20complete%20its%20mission%20at%20any%20point%20within%20a%20target%0Aneighborhood.%20Heuristics%20based%20on%20overlapped%20neighborhoods%2C%20known%20as%20Steiner%0AZones%20%28SZ%29%2C%20have%20gained%20attention%20in%20addressing%20CETSP.%20While%20SZs%20offer%0Aeffective%20approximations%20to%20the%20original%20graph%2C%20their%20inherent%20overlap%20imposes%0Aconstraints%20on%20search%20space%2C%20potentially%20conflicting%20with%20global%20optimization%0Aobjectives.%20Here%20we%20show%20how%20such%20limitations%20can%20be%20converted%20into%20advantages%0Ain%20a%20Close%20Enough%20Orienteering%20Problem%20%28CEOP%29%20by%20aggregating%20prizes%20across%0Aoverlapped%20neighborhoods.%20We%20further%20extend%20classic%20CEOP%20with%20Non-uniform%0ANeighborhoods%20%28CEOP-N%29%20by%20introducing%20non-uniform%20costs%20for%20prize%20collection.%0ATo%20tackle%20CEOP%20and%20CEOP-N%2C%20we%20develop%20a%20new%20approach%20featuring%20a%20Randomized%0ASteiner%20Zone%20Discretization%20%28RSZD%29%20scheme%20coupled%20with%20a%20hybrid%20algorithm%20based%0Aon%20Particle%20Swarm%20Optimization%20%28PSO%29%20and%20Ant%20Colony%20System%20%28ACS%29%2C%20CRaSZe-AntS.%0AThe%20RSZD%20scheme%20identifies%20sub-regions%20for%20PSO%20exploration%2C%20and%20ACS%20determines%0Athe%20discrete%20visiting%20sequence.%20We%20evaluate%20the%20RSZD%27s%20discretization%0Aperformance%20on%20CEOP%20instances%20derived%20from%20established%20CETSP%20instances%20and%0Acompare%20CRaSZe-AntS%20against%20the%20most%20relevant%20state-of-the-art%20heuristic%0Afocused%20on%20single-neighborhood%20optimization%20for%20CEOP%20instances.%20We%20also%20compare%0Athe%20performance%20of%20the%20interior%20search%20within%20SZs%20and%20the%20boundary%20search%20on%0Aindividual%20neighborhoods%20in%20the%20context%20of%20CEOP-N.%20Our%20experimental%20results%0Ashow%20that%20CRaSZe-AntS%20can%20yield%20comparable%20solution%20quality%20with%20significantly%0Areduced%20computation%20time%20compared%20to%20the%20single%20neighborhood%20strategy%2C%20where%20we%0Aobserve%20an%20average%20140.44%25%20increase%20in%20prize%20collection%20and%20a%2055.18%25%20reduction%0Ain%20algorithm%20execution%20time.%20CRaSZe-AntS%20is%20thus%20highly%20effective%20in%20solving%0Aemerging%20CEOP-N%2C%20examples%20of%20which%20include%20truck-and-drone%20delivery%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.04257v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Solving%20Close%20Enough%20Orienteering%20Problem%20with%20Overlapped%0A%20%20Neighborhoods&entry.906535625=Qiuchen%20Qian%20and%20Yanran%20Wang%20and%20David%20Boyle&entry.1292438233=%20%20Close%20Enough%20Traveling%20Salesman%20Problem%20%28CETSP%29%20is%20a%20well-known%20variant%20of%0ATSP%20whereby%20the%20agent%20may%20complete%20its%20mission%20at%20any%20point%20within%20a%20target%0Aneighborhood.%20Heuristics%20based%20on%20overlapped%20neighborhoods%2C%20known%20as%20Steiner%0AZones%20%28SZ%29%2C%20have%20gained%20attention%20in%20addressing%20CETSP.%20While%20SZs%20offer%0Aeffective%20approximations%20to%20the%20original%20graph%2C%20their%20inherent%20overlap%20imposes%0Aconstraints%20on%20search%20space%2C%20potentially%20conflicting%20with%20global%20optimization%0Aobjectives.%20Here%20we%20show%20how%20such%20limitations%20can%20be%20converted%20into%20advantages%0Ain%20a%20Close%20Enough%20Orienteering%20Problem%20%28CEOP%29%20by%20aggregating%20prizes%20across%0Aoverlapped%20neighborhoods.%20We%20further%20extend%20classic%20CEOP%20with%20Non-uniform%0ANeighborhoods%20%28CEOP-N%29%20by%20introducing%20non-uniform%20costs%20for%20prize%20collection.%0ATo%20tackle%20CEOP%20and%20CEOP-N%2C%20we%20develop%20a%20new%20approach%20featuring%20a%20Randomized%0ASteiner%20Zone%20Discretization%20%28RSZD%29%20scheme%20coupled%20with%20a%20hybrid%20algorithm%20based%0Aon%20Particle%20Swarm%20Optimization%20%28PSO%29%20and%20Ant%20Colony%20System%20%28ACS%29%2C%20CRaSZe-AntS.%0AThe%20RSZD%20scheme%20identifies%20sub-regions%20for%20PSO%20exploration%2C%20and%20ACS%20determines%0Athe%20discrete%20visiting%20sequence.%20We%20evaluate%20the%20RSZD%27s%20discretization%0Aperformance%20on%20CEOP%20instances%20derived%20from%20established%20CETSP%20instances%20and%0Acompare%20CRaSZe-AntS%20against%20the%20most%20relevant%20state-of-the-art%20heuristic%0Afocused%20on%20single-neighborhood%20optimization%20for%20CEOP%20instances.%20We%20also%20compare%0Athe%20performance%20of%20the%20interior%20search%20within%20SZs%20and%20the%20boundary%20search%20on%0Aindividual%20neighborhoods%20in%20the%20context%20of%20CEOP-N.%20Our%20experimental%20results%0Ashow%20that%20CRaSZe-AntS%20can%20yield%20comparable%20solution%20quality%20with%20significantly%0Areduced%20computation%20time%20compared%20to%20the%20single%20neighborhood%20strategy%2C%20where%20we%0Aobserve%20an%20average%20140.44%25%20increase%20in%20prize%20collection%20and%20a%2055.18%25%20reduction%0Ain%20algorithm%20execution%20time.%20CRaSZe-AntS%20is%20thus%20highly%20effective%20in%20solving%0Aemerging%20CEOP-N%2C%20examples%20of%20which%20include%20truck-and-drone%20delivery%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04257v2&entry.124074799=Read"},
{"title": "Exploring Safety Generalization Challenges of Large Language Models via\n  Code", "author": "Qibing Ren and Chang Gao and Jing Shao and Junchi Yan and Xin Tan and Wai Lam and Lizhuang Ma", "abstract": "  The rapid advancement of Large Language Models (LLMs) has brought about\nremarkable capabilities in natural language processing but also raised concerns\nabout their potential misuse. While strategies like supervised fine-tuning and\nreinforcement learning from human feedback have enhanced their safety, these\nmethods primarily focus on natural languages, which may not generalize to other\ndomains. This paper introduces CodeAttack, a framework that transforms natural\nlanguage inputs into code inputs, presenting a novel environment for testing\nthe safety generalization of LLMs. Our comprehensive studies on\nstate-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a\ncommon safety vulnerability of these models against code input: CodeAttack\nconsistently bypasses the safety guardrails of all models more than 80\\% of the\ntime. Furthermore, we find that a larger distribution gap between CodeAttack\nand natural language leads to weaker safety generalization, such as encoding\nnatural language input with data structures or using less popular programming\nlanguages. These findings highlight new safety risks in the code domain and the\nneed for more robust safety alignment algorithms to match the code capabilities\nof LLMs.\n", "link": "http://arxiv.org/abs/2403.07865v1", "date": "2024-03-12", "relevancy": 1.4458, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4858}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4814}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4806}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Exploring%20Safety%20Generalization%20Challenges%20of%20Large%20Language%20Models%20via%0A%20%20Code&body=Title%3A%20Exploring%20Safety%20Generalization%20Challenges%20of%20Large%20Language%20Models%20via%0A%20%20Code%0AAuthor%3A%20Qibing%20Ren%20and%20Chang%20Gao%20and%20Jing%20Shao%20and%20Junchi%20Yan%20and%20Xin%20Tan%20and%20Wai%20Lam%20and%20Lizhuang%20Ma%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20brought%20about%0Aremarkable%20capabilities%20in%20natural%20language%20processing%20but%20also%20raised%20concerns%0Aabout%20their%20potential%20misuse.%20While%20strategies%20like%20supervised%20fine-tuning%20and%0Areinforcement%20learning%20from%20human%20feedback%20have%20enhanced%20their%20safety%2C%20these%0Amethods%20primarily%20focus%20on%20natural%20languages%2C%20which%20may%20not%20generalize%20to%20other%0Adomains.%20This%20paper%20introduces%20CodeAttack%2C%20a%20framework%20that%20transforms%20natural%0Alanguage%20inputs%20into%20code%20inputs%2C%20presenting%20a%20novel%20environment%20for%20testing%0Athe%20safety%20generalization%20of%20LLMs.%20Our%20comprehensive%20studies%20on%0Astate-of-the-art%20LLMs%20including%20GPT-4%2C%20Claude-2%2C%20and%20Llama-2%20series%20reveal%20a%0Acommon%20safety%20vulnerability%20of%20these%20models%20against%20code%20input%3A%20CodeAttack%0Aconsistently%20bypasses%20the%20safety%20guardrails%20of%20all%20models%20more%20than%2080%5C%25%20of%20the%0Atime.%20Furthermore%2C%20we%20find%20that%20a%20larger%20distribution%20gap%20between%20CodeAttack%0Aand%20natural%20language%20leads%20to%20weaker%20safety%20generalization%2C%20such%20as%20encoding%0Anatural%20language%20input%20with%20data%20structures%20or%20using%20less%20popular%20programming%0Alanguages.%20These%20findings%20highlight%20new%20safety%20risks%20in%20the%20code%20domain%20and%20the%0Aneed%20for%20more%20robust%20safety%20alignment%20algorithms%20to%20match%20the%20code%20capabilities%0Aof%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07865v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Safety%20Generalization%20Challenges%20of%20Large%20Language%20Models%20via%0A%20%20Code&entry.906535625=Qibing%20Ren%20and%20Chang%20Gao%20and%20Jing%20Shao%20and%20Junchi%20Yan%20and%20Xin%20Tan%20and%20Wai%20Lam%20and%20Lizhuang%20Ma&entry.1292438233=%20%20The%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20brought%20about%0Aremarkable%20capabilities%20in%20natural%20language%20processing%20but%20also%20raised%20concerns%0Aabout%20their%20potential%20misuse.%20While%20strategies%20like%20supervised%20fine-tuning%20and%0Areinforcement%20learning%20from%20human%20feedback%20have%20enhanced%20their%20safety%2C%20these%0Amethods%20primarily%20focus%20on%20natural%20languages%2C%20which%20may%20not%20generalize%20to%20other%0Adomains.%20This%20paper%20introduces%20CodeAttack%2C%20a%20framework%20that%20transforms%20natural%0Alanguage%20inputs%20into%20code%20inputs%2C%20presenting%20a%20novel%20environment%20for%20testing%0Athe%20safety%20generalization%20of%20LLMs.%20Our%20comprehensive%20studies%20on%0Astate-of-the-art%20LLMs%20including%20GPT-4%2C%20Claude-2%2C%20and%20Llama-2%20series%20reveal%20a%0Acommon%20safety%20vulnerability%20of%20these%20models%20against%20code%20input%3A%20CodeAttack%0Aconsistently%20bypasses%20the%20safety%20guardrails%20of%20all%20models%20more%20than%2080%5C%25%20of%20the%0Atime.%20Furthermore%2C%20we%20find%20that%20a%20larger%20distribution%20gap%20between%20CodeAttack%0Aand%20natural%20language%20leads%20to%20weaker%20safety%20generalization%2C%20such%20as%20encoding%0Anatural%20language%20input%20with%20data%20structures%20or%20using%20less%20popular%20programming%0Alanguages.%20These%20findings%20highlight%20new%20safety%20risks%20in%20the%20code%20domain%20and%20the%0Aneed%20for%20more%20robust%20safety%20alignment%20algorithms%20to%20match%20the%20code%20capabilities%0Aof%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07865v1&entry.124074799=Read"},
{"title": "RudolfV: A Foundation Model by Pathologists for Pathologists", "author": "Jonas Dippel and Barbara Feulner and Tobias Winterhoff and Simon Schallenberg and Gabriel Dernbach and Andreas Kunft and Stephan Tietz and Timo Milbich and Simon Heinke and Marie-Lisa Eich and Julika Ribbat-Idel and Rosemarie Krupar and Philipp Jurmeister and David Horst and Lukas Ruff and Klaus-Robert M\u00fcller and Frederick Klauschen and Maximilian Alber", "abstract": "  Histopathology plays a central role in clinical medicine and biomedical\nresearch. While artificial intelligence shows promising results on many\npathological tasks, generalization and dealing with rare diseases, where\ntraining data is scarce, remains a challenge. Distilling knowledge from\nunlabelled data into a foundation model before learning from, potentially\nlimited, labelled data provides a viable path to address these challenges. In\nthis work, we extend the state of the art of foundation models for digital\npathology whole slide images by semi-automated data curation and incorporating\npathologist domain knowledge. Specifically, we combine computational and\npathologist domain knowledge (1) to curate a diverse dataset of 133k slides\ncorresponding to 1.2 billion image patches covering data from different\nfixation, staining, and scanning protocols as well as data from different\nindications and labs across the EU and US, (2) for grouping semantically\nsimilar slides and tissue patches, and (3) to augment the input images during\ntraining. We evaluate the resulting model on a set of public and internal\nbenchmarks and show that although our foundation model is trained with an order\nof magnitude less slides, it performs on par or better than competing models.\nWe expect that scaling our approach to more data and larger models will further\nincrease its performance and capacity to deal with increasingly complex real\nworld tasks in diagnostics and biomedical research.\n", "link": "http://arxiv.org/abs/2401.04079v3", "date": "2024-03-12", "relevancy": 1.4307, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5002}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4723}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4652}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20RudolfV%3A%20A%20Foundation%20Model%20by%20Pathologists%20for%20Pathologists&body=Title%3A%20RudolfV%3A%20A%20Foundation%20Model%20by%20Pathologists%20for%20Pathologists%0AAuthor%3A%20Jonas%20Dippel%20and%20Barbara%20Feulner%20and%20Tobias%20Winterhoff%20and%20Simon%20Schallenberg%20and%20Gabriel%20Dernbach%20and%20Andreas%20Kunft%20and%20Stephan%20Tietz%20and%20Timo%20Milbich%20and%20Simon%20Heinke%20and%20Marie-Lisa%20Eich%20and%20Julika%20Ribbat-Idel%20and%20Rosemarie%20Krupar%20and%20Philipp%20Jurmeister%20and%20David%20Horst%20and%20Lukas%20Ruff%20and%20Klaus-Robert%20M%C3%BCller%20and%20Frederick%20Klauschen%20and%20Maximilian%20Alber%0AAbstract%3A%20%20%20Histopathology%20plays%20a%20central%20role%20in%20clinical%20medicine%20and%20biomedical%0Aresearch.%20While%20artificial%20intelligence%20shows%20promising%20results%20on%20many%0Apathological%20tasks%2C%20generalization%20and%20dealing%20with%20rare%20diseases%2C%20where%0Atraining%20data%20is%20scarce%2C%20remains%20a%20challenge.%20Distilling%20knowledge%20from%0Aunlabelled%20data%20into%20a%20foundation%20model%20before%20learning%20from%2C%20potentially%0Alimited%2C%20labelled%20data%20provides%20a%20viable%20path%20to%20address%20these%20challenges.%20In%0Athis%20work%2C%20we%20extend%20the%20state%20of%20the%20art%20of%20foundation%20models%20for%20digital%0Apathology%20whole%20slide%20images%20by%20semi-automated%20data%20curation%20and%20incorporating%0Apathologist%20domain%20knowledge.%20Specifically%2C%20we%20combine%20computational%20and%0Apathologist%20domain%20knowledge%20%281%29%20to%20curate%20a%20diverse%20dataset%20of%20133k%20slides%0Acorresponding%20to%201.2%20billion%20image%20patches%20covering%20data%20from%20different%0Afixation%2C%20staining%2C%20and%20scanning%20protocols%20as%20well%20as%20data%20from%20different%0Aindications%20and%20labs%20across%20the%20EU%20and%20US%2C%20%282%29%20for%20grouping%20semantically%0Asimilar%20slides%20and%20tissue%20patches%2C%20and%20%283%29%20to%20augment%20the%20input%20images%20during%0Atraining.%20We%20evaluate%20the%20resulting%20model%20on%20a%20set%20of%20public%20and%20internal%0Abenchmarks%20and%20show%20that%20although%20our%20foundation%20model%20is%20trained%20with%20an%20order%0Aof%20magnitude%20less%20slides%2C%20it%20performs%20on%20par%20or%20better%20than%20competing%20models.%0AWe%20expect%20that%20scaling%20our%20approach%20to%20more%20data%20and%20larger%20models%20will%20further%0Aincrease%20its%20performance%20and%20capacity%20to%20deal%20with%20increasingly%20complex%20real%0Aworld%20tasks%20in%20diagnostics%20and%20biomedical%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.04079v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RudolfV%3A%20A%20Foundation%20Model%20by%20Pathologists%20for%20Pathologists&entry.906535625=Jonas%20Dippel%20and%20Barbara%20Feulner%20and%20Tobias%20Winterhoff%20and%20Simon%20Schallenberg%20and%20Gabriel%20Dernbach%20and%20Andreas%20Kunft%20and%20Stephan%20Tietz%20and%20Timo%20Milbich%20and%20Simon%20Heinke%20and%20Marie-Lisa%20Eich%20and%20Julika%20Ribbat-Idel%20and%20Rosemarie%20Krupar%20and%20Philipp%20Jurmeister%20and%20David%20Horst%20and%20Lukas%20Ruff%20and%20Klaus-Robert%20M%C3%BCller%20and%20Frederick%20Klauschen%20and%20Maximilian%20Alber&entry.1292438233=%20%20Histopathology%20plays%20a%20central%20role%20in%20clinical%20medicine%20and%20biomedical%0Aresearch.%20While%20artificial%20intelligence%20shows%20promising%20results%20on%20many%0Apathological%20tasks%2C%20generalization%20and%20dealing%20with%20rare%20diseases%2C%20where%0Atraining%20data%20is%20scarce%2C%20remains%20a%20challenge.%20Distilling%20knowledge%20from%0Aunlabelled%20data%20into%20a%20foundation%20model%20before%20learning%20from%2C%20potentially%0Alimited%2C%20labelled%20data%20provides%20a%20viable%20path%20to%20address%20these%20challenges.%20In%0Athis%20work%2C%20we%20extend%20the%20state%20of%20the%20art%20of%20foundation%20models%20for%20digital%0Apathology%20whole%20slide%20images%20by%20semi-automated%20data%20curation%20and%20incorporating%0Apathologist%20domain%20knowledge.%20Specifically%2C%20we%20combine%20computational%20and%0Apathologist%20domain%20knowledge%20%281%29%20to%20curate%20a%20diverse%20dataset%20of%20133k%20slides%0Acorresponding%20to%201.2%20billion%20image%20patches%20covering%20data%20from%20different%0Afixation%2C%20staining%2C%20and%20scanning%20protocols%20as%20well%20as%20data%20from%20different%0Aindications%20and%20labs%20across%20the%20EU%20and%20US%2C%20%282%29%20for%20grouping%20semantically%0Asimilar%20slides%20and%20tissue%20patches%2C%20and%20%283%29%20to%20augment%20the%20input%20images%20during%0Atraining.%20We%20evaluate%20the%20resulting%20model%20on%20a%20set%20of%20public%20and%20internal%0Abenchmarks%20and%20show%20that%20although%20our%20foundation%20model%20is%20trained%20with%20an%20order%0Aof%20magnitude%20less%20slides%2C%20it%20performs%20on%20par%20or%20better%20than%20competing%20models.%0AWe%20expect%20that%20scaling%20our%20approach%20to%20more%20data%20and%20larger%20models%20will%20further%0Aincrease%20its%20performance%20and%20capacity%20to%20deal%20with%20increasingly%20complex%20real%0Aworld%20tasks%20in%20diagnostics%20and%20biomedical%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.04079v3&entry.124074799=Read"},
{"title": "The 6th Affective Behavior Analysis in-the-wild (ABAW) Competition", "author": "Dimitrios Kollias and Panagiotis Tzirakis and Alan Cowen and Stefanos Zafeiriou and Irene Kotsia and Alice Baird and Chris Gagne and Chunchang Shao and Guanyu Hu", "abstract": "  This paper describes the 6th Affective Behavior Analysis in-the-wild (ABAW)\nCompetition, which is part of the respective Workshop held in conjunction with\nIEEE CVPR 2024. The 6th ABAW Competition addresses contemporary challenges in\nunderstanding human emotions and behaviors, crucial for the development of\nhuman-centered technologies. In more detail, the Competition focuses on affect\nrelated benchmarking tasks and comprises of five sub-challenges: i)\nValence-Arousal Estimation (the target is to estimate two continuous affect\ndimensions, valence and arousal), ii) Expression Recognition (the target is to\nrecognise between the mutually exclusive classes of the 7 basic expressions and\n'other'), iii) Action Unit Detection (the target is to detect 12 action units),\niv) Compound Expression Recognition (the target is to recognise between the 7\nmutually exclusive compound expression classes), and v) Emotional Mimicry\nIntensity Estimation (the target is to estimate six continuous emotion\ndimensions). In the paper, we present these Challenges, describe their\nrespective datasets and challenge protocols (we outline the evaluation metrics)\nand present the baseline systems as well as their obtained performance. More\ninformation for the Competition can be found in:\nhttps://affective-behavior-analysis-in-the-wild.github.io/6th.\n", "link": "http://arxiv.org/abs/2402.19344v3", "date": "2024-03-12", "relevancy": 1.3728, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4749}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4727}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4446}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20The%206th%20Affective%20Behavior%20Analysis%20in-the-wild%20%28ABAW%29%20Competition&body=Title%3A%20The%206th%20Affective%20Behavior%20Analysis%20in-the-wild%20%28ABAW%29%20Competition%0AAuthor%3A%20Dimitrios%20Kollias%20and%20Panagiotis%20Tzirakis%20and%20Alan%20Cowen%20and%20Stefanos%20Zafeiriou%20and%20Irene%20Kotsia%20and%20Alice%20Baird%20and%20Chris%20Gagne%20and%20Chunchang%20Shao%20and%20Guanyu%20Hu%0AAbstract%3A%20%20%20This%20paper%20describes%20the%206th%20Affective%20Behavior%20Analysis%20in-the-wild%20%28ABAW%29%0ACompetition%2C%20which%20is%20part%20of%20the%20respective%20Workshop%20held%20in%20conjunction%20with%0AIEEE%20CVPR%202024.%20The%206th%20ABAW%20Competition%20addresses%20contemporary%20challenges%20in%0Aunderstanding%20human%20emotions%20and%20behaviors%2C%20crucial%20for%20the%20development%20of%0Ahuman-centered%20technologies.%20In%20more%20detail%2C%20the%20Competition%20focuses%20on%20affect%0Arelated%20benchmarking%20tasks%20and%20comprises%20of%20five%20sub-challenges%3A%20i%29%0AValence-Arousal%20Estimation%20%28the%20target%20is%20to%20estimate%20two%20continuous%20affect%0Adimensions%2C%20valence%20and%20arousal%29%2C%20ii%29%20Expression%20Recognition%20%28the%20target%20is%20to%0Arecognise%20between%20the%20mutually%20exclusive%20classes%20of%20the%207%20basic%20expressions%20and%0A%27other%27%29%2C%20iii%29%20Action%20Unit%20Detection%20%28the%20target%20is%20to%20detect%2012%20action%20units%29%2C%0Aiv%29%20Compound%20Expression%20Recognition%20%28the%20target%20is%20to%20recognise%20between%20the%207%0Amutually%20exclusive%20compound%20expression%20classes%29%2C%20and%20v%29%20Emotional%20Mimicry%0AIntensity%20Estimation%20%28the%20target%20is%20to%20estimate%20six%20continuous%20emotion%0Adimensions%29.%20In%20the%20paper%2C%20we%20present%20these%20Challenges%2C%20describe%20their%0Arespective%20datasets%20and%20challenge%20protocols%20%28we%20outline%20the%20evaluation%20metrics%29%0Aand%20present%20the%20baseline%20systems%20as%20well%20as%20their%20obtained%20performance.%20More%0Ainformation%20for%20the%20Competition%20can%20be%20found%20in%3A%0Ahttps%3A//affective-behavior-analysis-in-the-wild.github.io/6th.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19344v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%206th%20Affective%20Behavior%20Analysis%20in-the-wild%20%28ABAW%29%20Competition&entry.906535625=Dimitrios%20Kollias%20and%20Panagiotis%20Tzirakis%20and%20Alan%20Cowen%20and%20Stefanos%20Zafeiriou%20and%20Irene%20Kotsia%20and%20Alice%20Baird%20and%20Chris%20Gagne%20and%20Chunchang%20Shao%20and%20Guanyu%20Hu&entry.1292438233=%20%20This%20paper%20describes%20the%206th%20Affective%20Behavior%20Analysis%20in-the-wild%20%28ABAW%29%0ACompetition%2C%20which%20is%20part%20of%20the%20respective%20Workshop%20held%20in%20conjunction%20with%0AIEEE%20CVPR%202024.%20The%206th%20ABAW%20Competition%20addresses%20contemporary%20challenges%20in%0Aunderstanding%20human%20emotions%20and%20behaviors%2C%20crucial%20for%20the%20development%20of%0Ahuman-centered%20technologies.%20In%20more%20detail%2C%20the%20Competition%20focuses%20on%20affect%0Arelated%20benchmarking%20tasks%20and%20comprises%20of%20five%20sub-challenges%3A%20i%29%0AValence-Arousal%20Estimation%20%28the%20target%20is%20to%20estimate%20two%20continuous%20affect%0Adimensions%2C%20valence%20and%20arousal%29%2C%20ii%29%20Expression%20Recognition%20%28the%20target%20is%20to%0Arecognise%20between%20the%20mutually%20exclusive%20classes%20of%20the%207%20basic%20expressions%20and%0A%27other%27%29%2C%20iii%29%20Action%20Unit%20Detection%20%28the%20target%20is%20to%20detect%2012%20action%20units%29%2C%0Aiv%29%20Compound%20Expression%20Recognition%20%28the%20target%20is%20to%20recognise%20between%20the%207%0Amutually%20exclusive%20compound%20expression%20classes%29%2C%20and%20v%29%20Emotional%20Mimicry%0AIntensity%20Estimation%20%28the%20target%20is%20to%20estimate%20six%20continuous%20emotion%0Adimensions%29.%20In%20the%20paper%2C%20we%20present%20these%20Challenges%2C%20describe%20their%0Arespective%20datasets%20and%20challenge%20protocols%20%28we%20outline%20the%20evaluation%20metrics%29%0Aand%20present%20the%20baseline%20systems%20as%20well%20as%20their%20obtained%20performance.%20More%0Ainformation%20for%20the%20Competition%20can%20be%20found%20in%3A%0Ahttps%3A//affective-behavior-analysis-in-the-wild.github.io/6th.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19344v3&entry.124074799=Read"},
{"title": "Distilling the Knowledge in Data Pruning", "author": "Emanuel Ben-Baruch and Adam Botach and Igor Kviatkovsky and Manoj Aggarwal and G\u00e9rard Medioni", "abstract": "  With the increasing size of datasets used for training neural networks, data\npruning becomes an attractive field of research. However, most current data\npruning algorithms are limited in their ability to preserve accuracy compared\nto models trained on the full data, especially in high pruning regimes. In this\npaper we explore the application of data pruning while incorporating knowledge\ndistillation (KD) when training on a pruned subset. That is, rather than\nrelying solely on ground-truth labels, we also use the soft predictions from a\nteacher network pre-trained on the complete data. By integrating KD into\ntraining, we demonstrate significant improvement across datasets, pruning\nmethods, and on all pruning fractions. We first establish a theoretical\nmotivation for employing self-distillation to improve training on pruned data.\nThen, we empirically make a compelling and highly practical observation: using\nKD, simple random pruning is comparable or superior to sophisticated pruning\nmethods across all pruning regimes. On ImageNet for example, we achieve\nsuperior accuracy despite training on a random subset of only 50% of the data.\nAdditionally, we demonstrate a crucial connection between the pruning factor\nand the optimal knowledge distillation weight. This helps mitigate the impact\nof samples with noisy labels and low-quality images retained by typical pruning\nalgorithms. Finally, we make an intriguing observation: when using lower\npruning fractions, larger teachers lead to accuracy degradation, while\nsurprisingly, employing teachers with a smaller capacity than the student's may\nimprove results. Our code will be made available.\n", "link": "http://arxiv.org/abs/2403.07854v1", "date": "2024-03-12", "relevancy": 1.3725, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4668}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4515}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4404}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Distilling%20the%20Knowledge%20in%20Data%20Pruning&body=Title%3A%20Distilling%20the%20Knowledge%20in%20Data%20Pruning%0AAuthor%3A%20Emanuel%20Ben-Baruch%20and%20Adam%20Botach%20and%20Igor%20Kviatkovsky%20and%20Manoj%20Aggarwal%20and%20G%C3%A9rard%20Medioni%0AAbstract%3A%20%20%20With%20the%20increasing%20size%20of%20datasets%20used%20for%20training%20neural%20networks%2C%20data%0Apruning%20becomes%20an%20attractive%20field%20of%20research.%20However%2C%20most%20current%20data%0Apruning%20algorithms%20are%20limited%20in%20their%20ability%20to%20preserve%20accuracy%20compared%0Ato%20models%20trained%20on%20the%20full%20data%2C%20especially%20in%20high%20pruning%20regimes.%20In%20this%0Apaper%20we%20explore%20the%20application%20of%20data%20pruning%20while%20incorporating%20knowledge%0Adistillation%20%28KD%29%20when%20training%20on%20a%20pruned%20subset.%20That%20is%2C%20rather%20than%0Arelying%20solely%20on%20ground-truth%20labels%2C%20we%20also%20use%20the%20soft%20predictions%20from%20a%0Ateacher%20network%20pre-trained%20on%20the%20complete%20data.%20By%20integrating%20KD%20into%0Atraining%2C%20we%20demonstrate%20significant%20improvement%20across%20datasets%2C%20pruning%0Amethods%2C%20and%20on%20all%20pruning%20fractions.%20We%20first%20establish%20a%20theoretical%0Amotivation%20for%20employing%20self-distillation%20to%20improve%20training%20on%20pruned%20data.%0AThen%2C%20we%20empirically%20make%20a%20compelling%20and%20highly%20practical%20observation%3A%20using%0AKD%2C%20simple%20random%20pruning%20is%20comparable%20or%20superior%20to%20sophisticated%20pruning%0Amethods%20across%20all%20pruning%20regimes.%20On%20ImageNet%20for%20example%2C%20we%20achieve%0Asuperior%20accuracy%20despite%20training%20on%20a%20random%20subset%20of%20only%2050%25%20of%20the%20data.%0AAdditionally%2C%20we%20demonstrate%20a%20crucial%20connection%20between%20the%20pruning%20factor%0Aand%20the%20optimal%20knowledge%20distillation%20weight.%20This%20helps%20mitigate%20the%20impact%0Aof%20samples%20with%20noisy%20labels%20and%20low-quality%20images%20retained%20by%20typical%20pruning%0Aalgorithms.%20Finally%2C%20we%20make%20an%20intriguing%20observation%3A%20when%20using%20lower%0Apruning%20fractions%2C%20larger%20teachers%20lead%20to%20accuracy%20degradation%2C%20while%0Asurprisingly%2C%20employing%20teachers%20with%20a%20smaller%20capacity%20than%20the%20student%27s%20may%0Aimprove%20results.%20Our%20code%20will%20be%20made%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07854v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distilling%20the%20Knowledge%20in%20Data%20Pruning&entry.906535625=Emanuel%20Ben-Baruch%20and%20Adam%20Botach%20and%20Igor%20Kviatkovsky%20and%20Manoj%20Aggarwal%20and%20G%C3%A9rard%20Medioni&entry.1292438233=%20%20With%20the%20increasing%20size%20of%20datasets%20used%20for%20training%20neural%20networks%2C%20data%0Apruning%20becomes%20an%20attractive%20field%20of%20research.%20However%2C%20most%20current%20data%0Apruning%20algorithms%20are%20limited%20in%20their%20ability%20to%20preserve%20accuracy%20compared%0Ato%20models%20trained%20on%20the%20full%20data%2C%20especially%20in%20high%20pruning%20regimes.%20In%20this%0Apaper%20we%20explore%20the%20application%20of%20data%20pruning%20while%20incorporating%20knowledge%0Adistillation%20%28KD%29%20when%20training%20on%20a%20pruned%20subset.%20That%20is%2C%20rather%20than%0Arelying%20solely%20on%20ground-truth%20labels%2C%20we%20also%20use%20the%20soft%20predictions%20from%20a%0Ateacher%20network%20pre-trained%20on%20the%20complete%20data.%20By%20integrating%20KD%20into%0Atraining%2C%20we%20demonstrate%20significant%20improvement%20across%20datasets%2C%20pruning%0Amethods%2C%20and%20on%20all%20pruning%20fractions.%20We%20first%20establish%20a%20theoretical%0Amotivation%20for%20employing%20self-distillation%20to%20improve%20training%20on%20pruned%20data.%0AThen%2C%20we%20empirically%20make%20a%20compelling%20and%20highly%20practical%20observation%3A%20using%0AKD%2C%20simple%20random%20pruning%20is%20comparable%20or%20superior%20to%20sophisticated%20pruning%0Amethods%20across%20all%20pruning%20regimes.%20On%20ImageNet%20for%20example%2C%20we%20achieve%0Asuperior%20accuracy%20despite%20training%20on%20a%20random%20subset%20of%20only%2050%25%20of%20the%20data.%0AAdditionally%2C%20we%20demonstrate%20a%20crucial%20connection%20between%20the%20pruning%20factor%0Aand%20the%20optimal%20knowledge%20distillation%20weight.%20This%20helps%20mitigate%20the%20impact%0Aof%20samples%20with%20noisy%20labels%20and%20low-quality%20images%20retained%20by%20typical%20pruning%0Aalgorithms.%20Finally%2C%20we%20make%20an%20intriguing%20observation%3A%20when%20using%20lower%0Apruning%20fractions%2C%20larger%20teachers%20lead%20to%20accuracy%20degradation%2C%20while%0Asurprisingly%2C%20employing%20teachers%20with%20a%20smaller%20capacity%20than%20the%20student%27s%20may%0Aimprove%20results.%20Our%20code%20will%20be%20made%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07854v1&entry.124074799=Read"},
{"title": "Computational limits to the legibility of the imaged human brain", "author": "James K Ruffle and Robert J Gray and Samia Mohinta and Guilherme Pombo and Chaitanya Kaul and Harpreet Hyare and Geraint Rees and Parashkev Nachev", "abstract": "  Our knowledge of the organisation of the human brain at the population-level\nis yet to translate into power to predict functional differences at the\nindividual-level, limiting clinical applications, and casting doubt on the\ngeneralisability of inferred mechanisms. It remains unknown whether the\ndifficulty arises from the absence of individuating biological patterns within\nthe brain, or from limited power to access them with the models and compute at\nour disposal. Here we comprehensively investigate the resolvability of such\npatterns with data and compute at unprecedented scale. Across 23 810 unique\nparticipants from UK Biobank, we systematically evaluate the predictability of\n25 individual biological characteristics, from all available combinations of\nstructural and functional neuroimaging data. Over 4526 GPU hours of\ncomputation, we train, optimize, and evaluate out-of-sample 700 individual\npredictive models, including fully-connected feed-forward neural networks of\ndemographic, psychological, serological, chronic disease, and functional\nconnectivity characteristics, and both uni- and multi-modal 3D convolutional\nneural network models of macro- and micro-structural brain imaging. We find a\nmarked discrepancy between the high predictability of sex (balanced accuracy\n99.7%), age (mean absolute error 2.048 years, R2 0.859), and weight (mean\nabsolute error 2.609Kg, R2 0.625), for which we set new state-of-the-art\nperformance, and the surprisingly low predictability of other characteristics.\nNeither structural nor functional imaging predicted psychology better than the\ncoincidence of chronic disease (p<0.05). Serology predicted chronic disease\n(p<0.05) and was best predicted by it (p<0.001), followed by structural\nneuroimaging (p<0.05). Our findings suggest either more informative imaging or\nmore powerful models are needed to decipher individual level characteristics\nfrom the human brain.\n", "link": "http://arxiv.org/abs/2309.07096v3", "date": "2024-03-12", "relevancy": 1.3467, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4593}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4567}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4416}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20Computational%20limits%20to%20the%20legibility%20of%20the%20imaged%20human%20brain&body=Title%3A%20Computational%20limits%20to%20the%20legibility%20of%20the%20imaged%20human%20brain%0AAuthor%3A%20James%20K%20Ruffle%20and%20Robert%20J%20Gray%20and%20Samia%20Mohinta%20and%20Guilherme%20Pombo%20and%20Chaitanya%20Kaul%20and%20Harpreet%20Hyare%20and%20Geraint%20Rees%20and%20Parashkev%20Nachev%0AAbstract%3A%20%20%20Our%20knowledge%20of%20the%20organisation%20of%20the%20human%20brain%20at%20the%20population-level%0Ais%20yet%20to%20translate%20into%20power%20to%20predict%20functional%20differences%20at%20the%0Aindividual-level%2C%20limiting%20clinical%20applications%2C%20and%20casting%20doubt%20on%20the%0Ageneralisability%20of%20inferred%20mechanisms.%20It%20remains%20unknown%20whether%20the%0Adifficulty%20arises%20from%20the%20absence%20of%20individuating%20biological%20patterns%20within%0Athe%20brain%2C%20or%20from%20limited%20power%20to%20access%20them%20with%20the%20models%20and%20compute%20at%0Aour%20disposal.%20Here%20we%20comprehensively%20investigate%20the%20resolvability%20of%20such%0Apatterns%20with%20data%20and%20compute%20at%20unprecedented%20scale.%20Across%2023%20810%20unique%0Aparticipants%20from%20UK%20Biobank%2C%20we%20systematically%20evaluate%20the%20predictability%20of%0A25%20individual%20biological%20characteristics%2C%20from%20all%20available%20combinations%20of%0Astructural%20and%20functional%20neuroimaging%20data.%20Over%204526%20GPU%20hours%20of%0Acomputation%2C%20we%20train%2C%20optimize%2C%20and%20evaluate%20out-of-sample%20700%20individual%0Apredictive%20models%2C%20including%20fully-connected%20feed-forward%20neural%20networks%20of%0Ademographic%2C%20psychological%2C%20serological%2C%20chronic%20disease%2C%20and%20functional%0Aconnectivity%20characteristics%2C%20and%20both%20uni-%20and%20multi-modal%203D%20convolutional%0Aneural%20network%20models%20of%20macro-%20and%20micro-structural%20brain%20imaging.%20We%20find%20a%0Amarked%20discrepancy%20between%20the%20high%20predictability%20of%20sex%20%28balanced%20accuracy%0A99.7%25%29%2C%20age%20%28mean%20absolute%20error%202.048%20years%2C%20R2%200.859%29%2C%20and%20weight%20%28mean%0Aabsolute%20error%202.609Kg%2C%20R2%200.625%29%2C%20for%20which%20we%20set%20new%20state-of-the-art%0Aperformance%2C%20and%20the%20surprisingly%20low%20predictability%20of%20other%20characteristics.%0ANeither%20structural%20nor%20functional%20imaging%20predicted%20psychology%20better%20than%20the%0Acoincidence%20of%20chronic%20disease%20%28p%3C0.05%29.%20Serology%20predicted%20chronic%20disease%0A%28p%3C0.05%29%20and%20was%20best%20predicted%20by%20it%20%28p%3C0.001%29%2C%20followed%20by%20structural%0Aneuroimaging%20%28p%3C0.05%29.%20Our%20findings%20suggest%20either%20more%20informative%20imaging%20or%0Amore%20powerful%20models%20are%20needed%20to%20decipher%20individual%20level%20characteristics%0Afrom%20the%20human%20brain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.07096v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computational%20limits%20to%20the%20legibility%20of%20the%20imaged%20human%20brain&entry.906535625=James%20K%20Ruffle%20and%20Robert%20J%20Gray%20and%20Samia%20Mohinta%20and%20Guilherme%20Pombo%20and%20Chaitanya%20Kaul%20and%20Harpreet%20Hyare%20and%20Geraint%20Rees%20and%20Parashkev%20Nachev&entry.1292438233=%20%20Our%20knowledge%20of%20the%20organisation%20of%20the%20human%20brain%20at%20the%20population-level%0Ais%20yet%20to%20translate%20into%20power%20to%20predict%20functional%20differences%20at%20the%0Aindividual-level%2C%20limiting%20clinical%20applications%2C%20and%20casting%20doubt%20on%20the%0Ageneralisability%20of%20inferred%20mechanisms.%20It%20remains%20unknown%20whether%20the%0Adifficulty%20arises%20from%20the%20absence%20of%20individuating%20biological%20patterns%20within%0Athe%20brain%2C%20or%20from%20limited%20power%20to%20access%20them%20with%20the%20models%20and%20compute%20at%0Aour%20disposal.%20Here%20we%20comprehensively%20investigate%20the%20resolvability%20of%20such%0Apatterns%20with%20data%20and%20compute%20at%20unprecedented%20scale.%20Across%2023%20810%20unique%0Aparticipants%20from%20UK%20Biobank%2C%20we%20systematically%20evaluate%20the%20predictability%20of%0A25%20individual%20biological%20characteristics%2C%20from%20all%20available%20combinations%20of%0Astructural%20and%20functional%20neuroimaging%20data.%20Over%204526%20GPU%20hours%20of%0Acomputation%2C%20we%20train%2C%20optimize%2C%20and%20evaluate%20out-of-sample%20700%20individual%0Apredictive%20models%2C%20including%20fully-connected%20feed-forward%20neural%20networks%20of%0Ademographic%2C%20psychological%2C%20serological%2C%20chronic%20disease%2C%20and%20functional%0Aconnectivity%20characteristics%2C%20and%20both%20uni-%20and%20multi-modal%203D%20convolutional%0Aneural%20network%20models%20of%20macro-%20and%20micro-structural%20brain%20imaging.%20We%20find%20a%0Amarked%20discrepancy%20between%20the%20high%20predictability%20of%20sex%20%28balanced%20accuracy%0A99.7%25%29%2C%20age%20%28mean%20absolute%20error%202.048%20years%2C%20R2%200.859%29%2C%20and%20weight%20%28mean%0Aabsolute%20error%202.609Kg%2C%20R2%200.625%29%2C%20for%20which%20we%20set%20new%20state-of-the-art%0Aperformance%2C%20and%20the%20surprisingly%20low%20predictability%20of%20other%20characteristics.%0ANeither%20structural%20nor%20functional%20imaging%20predicted%20psychology%20better%20than%20the%0Acoincidence%20of%20chronic%20disease%20%28p%3C0.05%29.%20Serology%20predicted%20chronic%20disease%0A%28p%3C0.05%29%20and%20was%20best%20predicted%20by%20it%20%28p%3C0.001%29%2C%20followed%20by%20structural%0Aneuroimaging%20%28p%3C0.05%29.%20Our%20findings%20suggest%20either%20more%20informative%20imaging%20or%0Amore%20powerful%20models%20are%20needed%20to%20decipher%20individual%20level%20characteristics%0Afrom%20the%20human%20brain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.07096v3&entry.124074799=Read"},
{"title": "A Two-Stage Feature Selection Approach for Robust Evaluation of\n  Treatment Effects in High-Dimensional Observational Data", "author": "Md Saiful Islam and Sahil Shikalgar and Md. Noor-E-Alam", "abstract": "  A Randomized Control Trial (RCT) is considered as the gold standard for\nevaluating the effect of any intervention or treatment. However, its\nfeasibility is often hindered by ethical, economical, and legal considerations,\nmaking observational data a valuable alternative for drawing causal\nconclusions. Nevertheless, healthcare observational data presents a difficult\nchallenge due to its high dimensionality, requiring careful consideration to\nensure unbiased, reliable, and robust causal inferences. To overcome this\nchallenge, in this study, we propose a novel two-stage feature selection\ntechnique called, Outcome Adaptive Elastic Net (OAENet), explicitly designed\nfor making robust causal inference decisions using matching techniques. OAENet\noffers several key advantages over existing methods: superior performance on\ncorrelated and high-dimensional data compared to the existing methods and the\nability to select specific sets of variables (including confounders and\nvariables associated only with the outcome). This ensures robustness and\nfacilitates an unbiased estimate of the causal effect. Numerical experiments on\nsimulated data demonstrate that OAENet significantly outperforms\nstate-of-the-art methods by either producing a higher-quality estimate or a\ncomparable estimate in significantly less time. To illustrate the applicability\nof OAENet, we employ large-scale US healthcare data to estimate the effect of\nOpioid Use Disorder (OUD) on suicidal behavior. When compared to competing\nmethods, OAENet closely aligns with existing literature on the relationship\nbetween OUD and suicidal behavior. Performance on both simulated and real-world\ndata highlights that OAENet notably enhances the accuracy of estimating\ntreatment effects or evaluating policy decision-making with causal inference.\n", "link": "http://arxiv.org/abs/2111.13800v2", "date": "2024-03-12", "relevancy": 1.2793, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.429}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4256}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.421}], "mailto": "mailto:mr.pppeo@gmail.com?subject=%5BarXrec%5D%20A%20Two-Stage%20Feature%20Selection%20Approach%20for%20Robust%20Evaluation%20of%0A%20%20Treatment%20Effects%20in%20High-Dimensional%20Observational%20Data&body=Title%3A%20A%20Two-Stage%20Feature%20Selection%20Approach%20for%20Robust%20Evaluation%20of%0A%20%20Treatment%20Effects%20in%20High-Dimensional%20Observational%20Data%0AAuthor%3A%20Md%20Saiful%20Islam%20and%20Sahil%20Shikalgar%20and%20Md.%20Noor-E-Alam%0AAbstract%3A%20%20%20A%20Randomized%20Control%20Trial%20%28RCT%29%20is%20considered%20as%20the%20gold%20standard%20for%0Aevaluating%20the%20effect%20of%20any%20intervention%20or%20treatment.%20However%2C%20its%0Afeasibility%20is%20often%20hindered%20by%20ethical%2C%20economical%2C%20and%20legal%20considerations%2C%0Amaking%20observational%20data%20a%20valuable%20alternative%20for%20drawing%20causal%0Aconclusions.%20Nevertheless%2C%20healthcare%20observational%20data%20presents%20a%20difficult%0Achallenge%20due%20to%20its%20high%20dimensionality%2C%20requiring%20careful%20consideration%20to%0Aensure%20unbiased%2C%20reliable%2C%20and%20robust%20causal%20inferences.%20To%20overcome%20this%0Achallenge%2C%20in%20this%20study%2C%20we%20propose%20a%20novel%20two-stage%20feature%20selection%0Atechnique%20called%2C%20Outcome%20Adaptive%20Elastic%20Net%20%28OAENet%29%2C%20explicitly%20designed%0Afor%20making%20robust%20causal%20inference%20decisions%20using%20matching%20techniques.%20OAENet%0Aoffers%20several%20key%20advantages%20over%20existing%20methods%3A%20superior%20performance%20on%0Acorrelated%20and%20high-dimensional%20data%20compared%20to%20the%20existing%20methods%20and%20the%0Aability%20to%20select%20specific%20sets%20of%20variables%20%28including%20confounders%20and%0Avariables%20associated%20only%20with%20the%20outcome%29.%20This%20ensures%20robustness%20and%0Afacilitates%20an%20unbiased%20estimate%20of%20the%20causal%20effect.%20Numerical%20experiments%20on%0Asimulated%20data%20demonstrate%20that%20OAENet%20significantly%20outperforms%0Astate-of-the-art%20methods%20by%20either%20producing%20a%20higher-quality%20estimate%20or%20a%0Acomparable%20estimate%20in%20significantly%20less%20time.%20To%20illustrate%20the%20applicability%0Aof%20OAENet%2C%20we%20employ%20large-scale%20US%20healthcare%20data%20to%20estimate%20the%20effect%20of%0AOpioid%20Use%20Disorder%20%28OUD%29%20on%20suicidal%20behavior.%20When%20compared%20to%20competing%0Amethods%2C%20OAENet%20closely%20aligns%20with%20existing%20literature%20on%20the%20relationship%0Abetween%20OUD%20and%20suicidal%20behavior.%20Performance%20on%20both%20simulated%20and%20real-world%0Adata%20highlights%20that%20OAENet%20notably%20enhances%20the%20accuracy%20of%20estimating%0Atreatment%20effects%20or%20evaluating%20policy%20decision-making%20with%20causal%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2111.13800v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Two-Stage%20Feature%20Selection%20Approach%20for%20Robust%20Evaluation%20of%0A%20%20Treatment%20Effects%20in%20High-Dimensional%20Observational%20Data&entry.906535625=Md%20Saiful%20Islam%20and%20Sahil%20Shikalgar%20and%20Md.%20Noor-E-Alam&entry.1292438233=%20%20A%20Randomized%20Control%20Trial%20%28RCT%29%20is%20considered%20as%20the%20gold%20standard%20for%0Aevaluating%20the%20effect%20of%20any%20intervention%20or%20treatment.%20However%2C%20its%0Afeasibility%20is%20often%20hindered%20by%20ethical%2C%20economical%2C%20and%20legal%20considerations%2C%0Amaking%20observational%20data%20a%20valuable%20alternative%20for%20drawing%20causal%0Aconclusions.%20Nevertheless%2C%20healthcare%20observational%20data%20presents%20a%20difficult%0Achallenge%20due%20to%20its%20high%20dimensionality%2C%20requiring%20careful%20consideration%20to%0Aensure%20unbiased%2C%20reliable%2C%20and%20robust%20causal%20inferences.%20To%20overcome%20this%0Achallenge%2C%20in%20this%20study%2C%20we%20propose%20a%20novel%20two-stage%20feature%20selection%0Atechnique%20called%2C%20Outcome%20Adaptive%20Elastic%20Net%20%28OAENet%29%2C%20explicitly%20designed%0Afor%20making%20robust%20causal%20inference%20decisions%20using%20matching%20techniques.%20OAENet%0Aoffers%20several%20key%20advantages%20over%20existing%20methods%3A%20superior%20performance%20on%0Acorrelated%20and%20high-dimensional%20data%20compared%20to%20the%20existing%20methods%20and%20the%0Aability%20to%20select%20specific%20sets%20of%20variables%20%28including%20confounders%20and%0Avariables%20associated%20only%20with%20the%20outcome%29.%20This%20ensures%20robustness%20and%0Afacilitates%20an%20unbiased%20estimate%20of%20the%20causal%20effect.%20Numerical%20experiments%20on%0Asimulated%20data%20demonstrate%20that%20OAENet%20significantly%20outperforms%0Astate-of-the-art%20methods%20by%20either%20producing%20a%20higher-quality%20estimate%20or%20a%0Acomparable%20estimate%20in%20significantly%20less%20time.%20To%20illustrate%20the%20applicability%0Aof%20OAENet%2C%20we%20employ%20large-scale%20US%20healthcare%20data%20to%20estimate%20the%20effect%20of%0AOpioid%20Use%20Disorder%20%28OUD%29%20on%20suicidal%20behavior.%20When%20compared%20to%20competing%0Amethods%2C%20OAENet%20closely%20aligns%20with%20existing%20literature%20on%20the%20relationship%0Abetween%20OUD%20and%20suicidal%20behavior.%20Performance%20on%20both%20simulated%20and%20real-world%0Adata%20highlights%20that%20OAENet%20notably%20enhances%20the%20accuracy%20of%20estimating%0Atreatment%20effects%20or%20evaluating%20policy%20decision-making%20with%20causal%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2111.13800v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


