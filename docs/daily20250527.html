<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250526.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian\n  Splatting with Dense Point Cloud", "author": "Natsuki Takama and Shintaro Ito and Koichi Ito and Hwann-Tzong Chen and Takafumi Aoki", "abstract": "  Gaussian Splatting (GS) has gained attention as a fast and effective method\nfor novel view synthesis. It has also been applied to 3D reconstruction using\nmulti-view images and can achieve fast and accurate 3D reconstruction. However,\nGS assumes that the input contains a large number of multi-view images, and\ntherefore, the reconstruction accuracy significantly decreases when only a\nlimited number of input images are available. One of the main reasons is the\ninsufficient number of 3D points in the sparse point cloud obtained through\nStructure from Motion (SfM), which results in a poor initialization for\noptimizing the Gaussian primitives. We propose a new 3D reconstruction method,\ncalled Sparse2DGS, to enhance 2DGS in reconstructing objects using only three\nimages. Sparse2DGS employs DUSt3R, a fundamental model for stereo images, along\nwith COLMAP MVS to generate highly accurate and dense 3D point clouds, which\nare then used to initialize 2D Gaussians. Through experiments on the DTU\ndataset, we show that Sparse2DGS can accurately reconstruct the 3D shapes of\nobjects using just three images.\n", "link": "http://arxiv.org/abs/2505.19854v1", "date": "2025-05-26", "relevancy": 3.4833, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7132}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7056}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse2DGS%3A%20Sparse-View%20Surface%20Reconstruction%20using%202D%20Gaussian%0A%20%20Splatting%20with%20Dense%20Point%20Cloud&body=Title%3A%20Sparse2DGS%3A%20Sparse-View%20Surface%20Reconstruction%20using%202D%20Gaussian%0A%20%20Splatting%20with%20Dense%20Point%20Cloud%0AAuthor%3A%20Natsuki%20Takama%20and%20Shintaro%20Ito%20and%20Koichi%20Ito%20and%20Hwann-Tzong%20Chen%20and%20Takafumi%20Aoki%0AAbstract%3A%20%20%20Gaussian%20Splatting%20%28GS%29%20has%20gained%20attention%20as%20a%20fast%20and%20effective%20method%0Afor%20novel%20view%20synthesis.%20It%20has%20also%20been%20applied%20to%203D%20reconstruction%20using%0Amulti-view%20images%20and%20can%20achieve%20fast%20and%20accurate%203D%20reconstruction.%20However%2C%0AGS%20assumes%20that%20the%20input%20contains%20a%20large%20number%20of%20multi-view%20images%2C%20and%0Atherefore%2C%20the%20reconstruction%20accuracy%20significantly%20decreases%20when%20only%20a%0Alimited%20number%20of%20input%20images%20are%20available.%20One%20of%20the%20main%20reasons%20is%20the%0Ainsufficient%20number%20of%203D%20points%20in%20the%20sparse%20point%20cloud%20obtained%20through%0AStructure%20from%20Motion%20%28SfM%29%2C%20which%20results%20in%20a%20poor%20initialization%20for%0Aoptimizing%20the%20Gaussian%20primitives.%20We%20propose%20a%20new%203D%20reconstruction%20method%2C%0Acalled%20Sparse2DGS%2C%20to%20enhance%202DGS%20in%20reconstructing%20objects%20using%20only%20three%0Aimages.%20Sparse2DGS%20employs%20DUSt3R%2C%20a%20fundamental%20model%20for%20stereo%20images%2C%20along%0Awith%20COLMAP%20MVS%20to%20generate%20highly%20accurate%20and%20dense%203D%20point%20clouds%2C%20which%0Aare%20then%20used%20to%20initialize%202D%20Gaussians.%20Through%20experiments%20on%20the%20DTU%0Adataset%2C%20we%20show%20that%20Sparse2DGS%20can%20accurately%20reconstruct%20the%203D%20shapes%20of%0Aobjects%20using%20just%20three%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19854v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse2DGS%253A%2520Sparse-View%2520Surface%2520Reconstruction%2520using%25202D%2520Gaussian%250A%2520%2520Splatting%2520with%2520Dense%2520Point%2520Cloud%26entry.906535625%3DNatsuki%2520Takama%2520and%2520Shintaro%2520Ito%2520and%2520Koichi%2520Ito%2520and%2520Hwann-Tzong%2520Chen%2520and%2520Takafumi%2520Aoki%26entry.1292438233%3D%2520%2520Gaussian%2520Splatting%2520%2528GS%2529%2520has%2520gained%2520attention%2520as%2520a%2520fast%2520and%2520effective%2520method%250Afor%2520novel%2520view%2520synthesis.%2520It%2520has%2520also%2520been%2520applied%2520to%25203D%2520reconstruction%2520using%250Amulti-view%2520images%2520and%2520can%2520achieve%2520fast%2520and%2520accurate%25203D%2520reconstruction.%2520However%252C%250AGS%2520assumes%2520that%2520the%2520input%2520contains%2520a%2520large%2520number%2520of%2520multi-view%2520images%252C%2520and%250Atherefore%252C%2520the%2520reconstruction%2520accuracy%2520significantly%2520decreases%2520when%2520only%2520a%250Alimited%2520number%2520of%2520input%2520images%2520are%2520available.%2520One%2520of%2520the%2520main%2520reasons%2520is%2520the%250Ainsufficient%2520number%2520of%25203D%2520points%2520in%2520the%2520sparse%2520point%2520cloud%2520obtained%2520through%250AStructure%2520from%2520Motion%2520%2528SfM%2529%252C%2520which%2520results%2520in%2520a%2520poor%2520initialization%2520for%250Aoptimizing%2520the%2520Gaussian%2520primitives.%2520We%2520propose%2520a%2520new%25203D%2520reconstruction%2520method%252C%250Acalled%2520Sparse2DGS%252C%2520to%2520enhance%25202DGS%2520in%2520reconstructing%2520objects%2520using%2520only%2520three%250Aimages.%2520Sparse2DGS%2520employs%2520DUSt3R%252C%2520a%2520fundamental%2520model%2520for%2520stereo%2520images%252C%2520along%250Awith%2520COLMAP%2520MVS%2520to%2520generate%2520highly%2520accurate%2520and%2520dense%25203D%2520point%2520clouds%252C%2520which%250Aare%2520then%2520used%2520to%2520initialize%25202D%2520Gaussians.%2520Through%2520experiments%2520on%2520the%2520DTU%250Adataset%252C%2520we%2520show%2520that%2520Sparse2DGS%2520can%2520accurately%2520reconstruct%2520the%25203D%2520shapes%2520of%250Aobjects%2520using%2520just%2520three%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19854v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse2DGS%3A%20Sparse-View%20Surface%20Reconstruction%20using%202D%20Gaussian%0A%20%20Splatting%20with%20Dense%20Point%20Cloud&entry.906535625=Natsuki%20Takama%20and%20Shintaro%20Ito%20and%20Koichi%20Ito%20and%20Hwann-Tzong%20Chen%20and%20Takafumi%20Aoki&entry.1292438233=%20%20Gaussian%20Splatting%20%28GS%29%20has%20gained%20attention%20as%20a%20fast%20and%20effective%20method%0Afor%20novel%20view%20synthesis.%20It%20has%20also%20been%20applied%20to%203D%20reconstruction%20using%0Amulti-view%20images%20and%20can%20achieve%20fast%20and%20accurate%203D%20reconstruction.%20However%2C%0AGS%20assumes%20that%20the%20input%20contains%20a%20large%20number%20of%20multi-view%20images%2C%20and%0Atherefore%2C%20the%20reconstruction%20accuracy%20significantly%20decreases%20when%20only%20a%0Alimited%20number%20of%20input%20images%20are%20available.%20One%20of%20the%20main%20reasons%20is%20the%0Ainsufficient%20number%20of%203D%20points%20in%20the%20sparse%20point%20cloud%20obtained%20through%0AStructure%20from%20Motion%20%28SfM%29%2C%20which%20results%20in%20a%20poor%20initialization%20for%0Aoptimizing%20the%20Gaussian%20primitives.%20We%20propose%20a%20new%203D%20reconstruction%20method%2C%0Acalled%20Sparse2DGS%2C%20to%20enhance%202DGS%20in%20reconstructing%20objects%20using%20only%20three%0Aimages.%20Sparse2DGS%20employs%20DUSt3R%2C%20a%20fundamental%20model%20for%20stereo%20images%2C%20along%0Awith%20COLMAP%20MVS%20to%20generate%20highly%20accurate%20and%20dense%203D%20point%20clouds%2C%20which%0Aare%20then%20used%20to%20initialize%202D%20Gaussians.%20Through%20experiments%20on%20the%20DTU%0Adataset%2C%20we%20show%20that%20Sparse2DGS%20can%20accurately%20reconstruct%20the%203D%20shapes%20of%0Aobjects%20using%20just%20three%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19854v1&entry.124074799=Read"},
{"title": "ErpGS: Equirectangular Image Rendering enhanced with 3D Gaussian\n  Regularization", "author": "Shintaro Ito and Natsuki Takama and Koichi Ito and Hwann-Tzong Chen and Takafumi Aoki", "abstract": "  The use of multi-view images acquired by a 360-degree camera can reconstruct\na 3D space with a wide area. There are 3D reconstruction methods from\nequirectangular images based on NeRF and 3DGS, as well as Novel View Synthesis\n(NVS) methods. On the other hand, it is necessary to overcome the large\ndistortion caused by the projection model of a 360-degree camera when\nequirectangular images are used. In 3DGS-based methods, the large distortion of\nthe 360-degree camera model generates extremely large 3D Gaussians, resulting\nin poor rendering accuracy. We propose ErpGS, which is Omnidirectional GS based\non 3DGS to realize NVS addressing the problems. ErpGS introduce some rendering\naccuracy improvement techniques: geometric regularization, scale\nregularization, and distortion-aware weights and a mask to suppress the effects\nof obstacles in equirectangular images. Through experiments on public datasets,\nwe demonstrate that ErpGS can render novel view images more accurately than\nconventional methods.\n", "link": "http://arxiv.org/abs/2505.19883v1", "date": "2025-05-26", "relevancy": 3.2107, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6638}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.643}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ErpGS%3A%20Equirectangular%20Image%20Rendering%20enhanced%20with%203D%20Gaussian%0A%20%20Regularization&body=Title%3A%20ErpGS%3A%20Equirectangular%20Image%20Rendering%20enhanced%20with%203D%20Gaussian%0A%20%20Regularization%0AAuthor%3A%20Shintaro%20Ito%20and%20Natsuki%20Takama%20and%20Koichi%20Ito%20and%20Hwann-Tzong%20Chen%20and%20Takafumi%20Aoki%0AAbstract%3A%20%20%20The%20use%20of%20multi-view%20images%20acquired%20by%20a%20360-degree%20camera%20can%20reconstruct%0Aa%203D%20space%20with%20a%20wide%20area.%20There%20are%203D%20reconstruction%20methods%20from%0Aequirectangular%20images%20based%20on%20NeRF%20and%203DGS%2C%20as%20well%20as%20Novel%20View%20Synthesis%0A%28NVS%29%20methods.%20On%20the%20other%20hand%2C%20it%20is%20necessary%20to%20overcome%20the%20large%0Adistortion%20caused%20by%20the%20projection%20model%20of%20a%20360-degree%20camera%20when%0Aequirectangular%20images%20are%20used.%20In%203DGS-based%20methods%2C%20the%20large%20distortion%20of%0Athe%20360-degree%20camera%20model%20generates%20extremely%20large%203D%20Gaussians%2C%20resulting%0Ain%20poor%20rendering%20accuracy.%20We%20propose%20ErpGS%2C%20which%20is%20Omnidirectional%20GS%20based%0Aon%203DGS%20to%20realize%20NVS%20addressing%20the%20problems.%20ErpGS%20introduce%20some%20rendering%0Aaccuracy%20improvement%20techniques%3A%20geometric%20regularization%2C%20scale%0Aregularization%2C%20and%20distortion-aware%20weights%20and%20a%20mask%20to%20suppress%20the%20effects%0Aof%20obstacles%20in%20equirectangular%20images.%20Through%20experiments%20on%20public%20datasets%2C%0Awe%20demonstrate%20that%20ErpGS%20can%20render%20novel%20view%20images%20more%20accurately%20than%0Aconventional%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19883v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DErpGS%253A%2520Equirectangular%2520Image%2520Rendering%2520enhanced%2520with%25203D%2520Gaussian%250A%2520%2520Regularization%26entry.906535625%3DShintaro%2520Ito%2520and%2520Natsuki%2520Takama%2520and%2520Koichi%2520Ito%2520and%2520Hwann-Tzong%2520Chen%2520and%2520Takafumi%2520Aoki%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520multi-view%2520images%2520acquired%2520by%2520a%2520360-degree%2520camera%2520can%2520reconstruct%250Aa%25203D%2520space%2520with%2520a%2520wide%2520area.%2520There%2520are%25203D%2520reconstruction%2520methods%2520from%250Aequirectangular%2520images%2520based%2520on%2520NeRF%2520and%25203DGS%252C%2520as%2520well%2520as%2520Novel%2520View%2520Synthesis%250A%2528NVS%2529%2520methods.%2520On%2520the%2520other%2520hand%252C%2520it%2520is%2520necessary%2520to%2520overcome%2520the%2520large%250Adistortion%2520caused%2520by%2520the%2520projection%2520model%2520of%2520a%2520360-degree%2520camera%2520when%250Aequirectangular%2520images%2520are%2520used.%2520In%25203DGS-based%2520methods%252C%2520the%2520large%2520distortion%2520of%250Athe%2520360-degree%2520camera%2520model%2520generates%2520extremely%2520large%25203D%2520Gaussians%252C%2520resulting%250Ain%2520poor%2520rendering%2520accuracy.%2520We%2520propose%2520ErpGS%252C%2520which%2520is%2520Omnidirectional%2520GS%2520based%250Aon%25203DGS%2520to%2520realize%2520NVS%2520addressing%2520the%2520problems.%2520ErpGS%2520introduce%2520some%2520rendering%250Aaccuracy%2520improvement%2520techniques%253A%2520geometric%2520regularization%252C%2520scale%250Aregularization%252C%2520and%2520distortion-aware%2520weights%2520and%2520a%2520mask%2520to%2520suppress%2520the%2520effects%250Aof%2520obstacles%2520in%2520equirectangular%2520images.%2520Through%2520experiments%2520on%2520public%2520datasets%252C%250Awe%2520demonstrate%2520that%2520ErpGS%2520can%2520render%2520novel%2520view%2520images%2520more%2520accurately%2520than%250Aconventional%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19883v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ErpGS%3A%20Equirectangular%20Image%20Rendering%20enhanced%20with%203D%20Gaussian%0A%20%20Regularization&entry.906535625=Shintaro%20Ito%20and%20Natsuki%20Takama%20and%20Koichi%20Ito%20and%20Hwann-Tzong%20Chen%20and%20Takafumi%20Aoki&entry.1292438233=%20%20The%20use%20of%20multi-view%20images%20acquired%20by%20a%20360-degree%20camera%20can%20reconstruct%0Aa%203D%20space%20with%20a%20wide%20area.%20There%20are%203D%20reconstruction%20methods%20from%0Aequirectangular%20images%20based%20on%20NeRF%20and%203DGS%2C%20as%20well%20as%20Novel%20View%20Synthesis%0A%28NVS%29%20methods.%20On%20the%20other%20hand%2C%20it%20is%20necessary%20to%20overcome%20the%20large%0Adistortion%20caused%20by%20the%20projection%20model%20of%20a%20360-degree%20camera%20when%0Aequirectangular%20images%20are%20used.%20In%203DGS-based%20methods%2C%20the%20large%20distortion%20of%0Athe%20360-degree%20camera%20model%20generates%20extremely%20large%203D%20Gaussians%2C%20resulting%0Ain%20poor%20rendering%20accuracy.%20We%20propose%20ErpGS%2C%20which%20is%20Omnidirectional%20GS%20based%0Aon%203DGS%20to%20realize%20NVS%20addressing%20the%20problems.%20ErpGS%20introduce%20some%20rendering%0Aaccuracy%20improvement%20techniques%3A%20geometric%20regularization%2C%20scale%0Aregularization%2C%20and%20distortion-aware%20weights%20and%20a%20mask%20to%20suppress%20the%20effects%0Aof%20obstacles%20in%20equirectangular%20images.%20Through%20experiments%20on%20public%20datasets%2C%0Awe%20demonstrate%20that%20ErpGS%20can%20render%20novel%20view%20images%20more%20accurately%20than%0Aconventional%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19883v1&entry.124074799=Read"},
{"title": "HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for\n  Multiple Characters", "author": "Yi Chen and Sen Liang and Zixiang Zhou and Ziyao Huang and Yifeng Ma and Junshu Tang and Qin Lin and Yuan Zhou and Qinglin Lu", "abstract": "  Recent years have witnessed significant progress in audio-driven human\nanimation. However, critical challenges remain in (i) generating highly dynamic\nvideos while preserving character consistency, (ii) achieving precise emotion\nalignment between characters and audio, and (iii) enabling multi-character\naudio-driven animation. To address these challenges, we propose\nHunyuanVideo-Avatar, a multimodal diffusion transformer (MM-DiT)-based model\ncapable of simultaneously generating dynamic, emotion-controllable, and\nmulti-character dialogue videos. Concretely, HunyuanVideo-Avatar introduces\nthree key innovations: (i) A character image injection module is designed to\nreplace the conventional addition-based character conditioning scheme,\neliminating the inherent condition mismatch between training and inference.\nThis ensures the dynamic motion and strong character consistency; (ii) An Audio\nEmotion Module (AEM) is introduced to extract and transfer the emotional cues\nfrom an emotion reference image to the target generated video, enabling\nfine-grained and accurate emotion style control; (iii) A Face-Aware Audio\nAdapter (FAA) is proposed to isolate the audio-driven character with\nlatent-level face mask, enabling independent audio injection via\ncross-attention for multi-character scenarios. These innovations empower\nHunyuanVideo-Avatar to surpass state-of-the-art methods on benchmark datasets\nand a newly proposed wild dataset, generating realistic avatars in dynamic,\nimmersive scenarios.\n", "link": "http://arxiv.org/abs/2505.20156v1", "date": "2025-05-26", "relevancy": 3.1208, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6559}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6155}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HunyuanVideo-Avatar%3A%20High-Fidelity%20Audio-Driven%20Human%20Animation%20for%0A%20%20Multiple%20Characters&body=Title%3A%20HunyuanVideo-Avatar%3A%20High-Fidelity%20Audio-Driven%20Human%20Animation%20for%0A%20%20Multiple%20Characters%0AAuthor%3A%20Yi%20Chen%20and%20Sen%20Liang%20and%20Zixiang%20Zhou%20and%20Ziyao%20Huang%20and%20Yifeng%20Ma%20and%20Junshu%20Tang%20and%20Qin%20Lin%20and%20Yuan%20Zhou%20and%20Qinglin%20Lu%0AAbstract%3A%20%20%20Recent%20years%20have%20witnessed%20significant%20progress%20in%20audio-driven%20human%0Aanimation.%20However%2C%20critical%20challenges%20remain%20in%20%28i%29%20generating%20highly%20dynamic%0Avideos%20while%20preserving%20character%20consistency%2C%20%28ii%29%20achieving%20precise%20emotion%0Aalignment%20between%20characters%20and%20audio%2C%20and%20%28iii%29%20enabling%20multi-character%0Aaudio-driven%20animation.%20To%20address%20these%20challenges%2C%20we%20propose%0AHunyuanVideo-Avatar%2C%20a%20multimodal%20diffusion%20transformer%20%28MM-DiT%29-based%20model%0Acapable%20of%20simultaneously%20generating%20dynamic%2C%20emotion-controllable%2C%20and%0Amulti-character%20dialogue%20videos.%20Concretely%2C%20HunyuanVideo-Avatar%20introduces%0Athree%20key%20innovations%3A%20%28i%29%20A%20character%20image%20injection%20module%20is%20designed%20to%0Areplace%20the%20conventional%20addition-based%20character%20conditioning%20scheme%2C%0Aeliminating%20the%20inherent%20condition%20mismatch%20between%20training%20and%20inference.%0AThis%20ensures%20the%20dynamic%20motion%20and%20strong%20character%20consistency%3B%20%28ii%29%20An%20Audio%0AEmotion%20Module%20%28AEM%29%20is%20introduced%20to%20extract%20and%20transfer%20the%20emotional%20cues%0Afrom%20an%20emotion%20reference%20image%20to%20the%20target%20generated%20video%2C%20enabling%0Afine-grained%20and%20accurate%20emotion%20style%20control%3B%20%28iii%29%20A%20Face-Aware%20Audio%0AAdapter%20%28FAA%29%20is%20proposed%20to%20isolate%20the%20audio-driven%20character%20with%0Alatent-level%20face%20mask%2C%20enabling%20independent%20audio%20injection%20via%0Across-attention%20for%20multi-character%20scenarios.%20These%20innovations%20empower%0AHunyuanVideo-Avatar%20to%20surpass%20state-of-the-art%20methods%20on%20benchmark%20datasets%0Aand%20a%20newly%20proposed%20wild%20dataset%2C%20generating%20realistic%20avatars%20in%20dynamic%2C%0Aimmersive%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHunyuanVideo-Avatar%253A%2520High-Fidelity%2520Audio-Driven%2520Human%2520Animation%2520for%250A%2520%2520Multiple%2520Characters%26entry.906535625%3DYi%2520Chen%2520and%2520Sen%2520Liang%2520and%2520Zixiang%2520Zhou%2520and%2520Ziyao%2520Huang%2520and%2520Yifeng%2520Ma%2520and%2520Junshu%2520Tang%2520and%2520Qin%2520Lin%2520and%2520Yuan%2520Zhou%2520and%2520Qinglin%2520Lu%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520witnessed%2520significant%2520progress%2520in%2520audio-driven%2520human%250Aanimation.%2520However%252C%2520critical%2520challenges%2520remain%2520in%2520%2528i%2529%2520generating%2520highly%2520dynamic%250Avideos%2520while%2520preserving%2520character%2520consistency%252C%2520%2528ii%2529%2520achieving%2520precise%2520emotion%250Aalignment%2520between%2520characters%2520and%2520audio%252C%2520and%2520%2528iii%2529%2520enabling%2520multi-character%250Aaudio-driven%2520animation.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250AHunyuanVideo-Avatar%252C%2520a%2520multimodal%2520diffusion%2520transformer%2520%2528MM-DiT%2529-based%2520model%250Acapable%2520of%2520simultaneously%2520generating%2520dynamic%252C%2520emotion-controllable%252C%2520and%250Amulti-character%2520dialogue%2520videos.%2520Concretely%252C%2520HunyuanVideo-Avatar%2520introduces%250Athree%2520key%2520innovations%253A%2520%2528i%2529%2520A%2520character%2520image%2520injection%2520module%2520is%2520designed%2520to%250Areplace%2520the%2520conventional%2520addition-based%2520character%2520conditioning%2520scheme%252C%250Aeliminating%2520the%2520inherent%2520condition%2520mismatch%2520between%2520training%2520and%2520inference.%250AThis%2520ensures%2520the%2520dynamic%2520motion%2520and%2520strong%2520character%2520consistency%253B%2520%2528ii%2529%2520An%2520Audio%250AEmotion%2520Module%2520%2528AEM%2529%2520is%2520introduced%2520to%2520extract%2520and%2520transfer%2520the%2520emotional%2520cues%250Afrom%2520an%2520emotion%2520reference%2520image%2520to%2520the%2520target%2520generated%2520video%252C%2520enabling%250Afine-grained%2520and%2520accurate%2520emotion%2520style%2520control%253B%2520%2528iii%2529%2520A%2520Face-Aware%2520Audio%250AAdapter%2520%2528FAA%2529%2520is%2520proposed%2520to%2520isolate%2520the%2520audio-driven%2520character%2520with%250Alatent-level%2520face%2520mask%252C%2520enabling%2520independent%2520audio%2520injection%2520via%250Across-attention%2520for%2520multi-character%2520scenarios.%2520These%2520innovations%2520empower%250AHunyuanVideo-Avatar%2520to%2520surpass%2520state-of-the-art%2520methods%2520on%2520benchmark%2520datasets%250Aand%2520a%2520newly%2520proposed%2520wild%2520dataset%252C%2520generating%2520realistic%2520avatars%2520in%2520dynamic%252C%250Aimmersive%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HunyuanVideo-Avatar%3A%20High-Fidelity%20Audio-Driven%20Human%20Animation%20for%0A%20%20Multiple%20Characters&entry.906535625=Yi%20Chen%20and%20Sen%20Liang%20and%20Zixiang%20Zhou%20and%20Ziyao%20Huang%20and%20Yifeng%20Ma%20and%20Junshu%20Tang%20and%20Qin%20Lin%20and%20Yuan%20Zhou%20and%20Qinglin%20Lu&entry.1292438233=%20%20Recent%20years%20have%20witnessed%20significant%20progress%20in%20audio-driven%20human%0Aanimation.%20However%2C%20critical%20challenges%20remain%20in%20%28i%29%20generating%20highly%20dynamic%0Avideos%20while%20preserving%20character%20consistency%2C%20%28ii%29%20achieving%20precise%20emotion%0Aalignment%20between%20characters%20and%20audio%2C%20and%20%28iii%29%20enabling%20multi-character%0Aaudio-driven%20animation.%20To%20address%20these%20challenges%2C%20we%20propose%0AHunyuanVideo-Avatar%2C%20a%20multimodal%20diffusion%20transformer%20%28MM-DiT%29-based%20model%0Acapable%20of%20simultaneously%20generating%20dynamic%2C%20emotion-controllable%2C%20and%0Amulti-character%20dialogue%20videos.%20Concretely%2C%20HunyuanVideo-Avatar%20introduces%0Athree%20key%20innovations%3A%20%28i%29%20A%20character%20image%20injection%20module%20is%20designed%20to%0Areplace%20the%20conventional%20addition-based%20character%20conditioning%20scheme%2C%0Aeliminating%20the%20inherent%20condition%20mismatch%20between%20training%20and%20inference.%0AThis%20ensures%20the%20dynamic%20motion%20and%20strong%20character%20consistency%3B%20%28ii%29%20An%20Audio%0AEmotion%20Module%20%28AEM%29%20is%20introduced%20to%20extract%20and%20transfer%20the%20emotional%20cues%0Afrom%20an%20emotion%20reference%20image%20to%20the%20target%20generated%20video%2C%20enabling%0Afine-grained%20and%20accurate%20emotion%20style%20control%3B%20%28iii%29%20A%20Face-Aware%20Audio%0AAdapter%20%28FAA%29%20is%20proposed%20to%20isolate%20the%20audio-driven%20character%20with%0Alatent-level%20face%20mask%2C%20enabling%20independent%20audio%20injection%20via%0Across-attention%20for%20multi-character%20scenarios.%20These%20innovations%20empower%0AHunyuanVideo-Avatar%20to%20surpass%20state-of-the-art%20methods%20on%20benchmark%20datasets%0Aand%20a%20newly%20proposed%20wild%20dataset%2C%20generating%20realistic%20avatars%20in%20dynamic%2C%0Aimmersive%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20156v1&entry.124074799=Read"},
{"title": "X-GRM: Large Gaussian Reconstruction Model for Sparse-view X-rays to\n  Computed Tomography", "author": "Yifan Liu and Wuyang Li and Weihao Yu and Chenxin Li and Alexandre Alahi and Max Meng and Yixuan Yuan", "abstract": "  Computed Tomography serves as an indispensable tool in clinical workflows,\nproviding non-invasive visualization of internal anatomical structures.\nExisting CT reconstruction works are limited to small-capacity model\narchitecture and inflexible volume representation. In this work, we present\nX-GRM (X-ray Gaussian Reconstruction Model), a large feedforward model for\nreconstructing 3D CT volumes from sparse-view 2D X-ray projections. X-GRM\nemploys a scalable transformer-based architecture to encode sparse-view X-ray\ninputs, where tokens from different views are integrated efficiently. Then,\nthese tokens are decoded into a novel volume representation, named Voxel-based\nGaussian Splatting (VoxGS), which enables efficient CT volume extraction and\ndifferentiable X-ray rendering. This combination of a high-capacity model and\nflexible volume representation, empowers our model to produce high-quality\nreconstructions from various testing inputs, including in-domain and out-domain\nX-ray projections. Our codes are available at:\nhttps://github.com/CUHK-AIM-Group/X-GRM.\n", "link": "http://arxiv.org/abs/2505.15235v2", "date": "2025-05-26", "relevancy": 3.0558, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6343}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6168}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-GRM%3A%20Large%20Gaussian%20Reconstruction%20Model%20for%20Sparse-view%20X-rays%20to%0A%20%20Computed%20Tomography&body=Title%3A%20X-GRM%3A%20Large%20Gaussian%20Reconstruction%20Model%20for%20Sparse-view%20X-rays%20to%0A%20%20Computed%20Tomography%0AAuthor%3A%20Yifan%20Liu%20and%20Wuyang%20Li%20and%20Weihao%20Yu%20and%20Chenxin%20Li%20and%20Alexandre%20Alahi%20and%20Max%20Meng%20and%20Yixuan%20Yuan%0AAbstract%3A%20%20%20Computed%20Tomography%20serves%20as%20an%20indispensable%20tool%20in%20clinical%20workflows%2C%0Aproviding%20non-invasive%20visualization%20of%20internal%20anatomical%20structures.%0AExisting%20CT%20reconstruction%20works%20are%20limited%20to%20small-capacity%20model%0Aarchitecture%20and%20inflexible%20volume%20representation.%20In%20this%20work%2C%20we%20present%0AX-GRM%20%28X-ray%20Gaussian%20Reconstruction%20Model%29%2C%20a%20large%20feedforward%20model%20for%0Areconstructing%203D%20CT%20volumes%20from%20sparse-view%202D%20X-ray%20projections.%20X-GRM%0Aemploys%20a%20scalable%20transformer-based%20architecture%20to%20encode%20sparse-view%20X-ray%0Ainputs%2C%20where%20tokens%20from%20different%20views%20are%20integrated%20efficiently.%20Then%2C%0Athese%20tokens%20are%20decoded%20into%20a%20novel%20volume%20representation%2C%20named%20Voxel-based%0AGaussian%20Splatting%20%28VoxGS%29%2C%20which%20enables%20efficient%20CT%20volume%20extraction%20and%0Adifferentiable%20X-ray%20rendering.%20This%20combination%20of%20a%20high-capacity%20model%20and%0Aflexible%20volume%20representation%2C%20empowers%20our%20model%20to%20produce%20high-quality%0Areconstructions%20from%20various%20testing%20inputs%2C%20including%20in-domain%20and%20out-domain%0AX-ray%20projections.%20Our%20codes%20are%20available%20at%3A%0Ahttps%3A//github.com/CUHK-AIM-Group/X-GRM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15235v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-GRM%253A%2520Large%2520Gaussian%2520Reconstruction%2520Model%2520for%2520Sparse-view%2520X-rays%2520to%250A%2520%2520Computed%2520Tomography%26entry.906535625%3DYifan%2520Liu%2520and%2520Wuyang%2520Li%2520and%2520Weihao%2520Yu%2520and%2520Chenxin%2520Li%2520and%2520Alexandre%2520Alahi%2520and%2520Max%2520Meng%2520and%2520Yixuan%2520Yuan%26entry.1292438233%3D%2520%2520Computed%2520Tomography%2520serves%2520as%2520an%2520indispensable%2520tool%2520in%2520clinical%2520workflows%252C%250Aproviding%2520non-invasive%2520visualization%2520of%2520internal%2520anatomical%2520structures.%250AExisting%2520CT%2520reconstruction%2520works%2520are%2520limited%2520to%2520small-capacity%2520model%250Aarchitecture%2520and%2520inflexible%2520volume%2520representation.%2520In%2520this%2520work%252C%2520we%2520present%250AX-GRM%2520%2528X-ray%2520Gaussian%2520Reconstruction%2520Model%2529%252C%2520a%2520large%2520feedforward%2520model%2520for%250Areconstructing%25203D%2520CT%2520volumes%2520from%2520sparse-view%25202D%2520X-ray%2520projections.%2520X-GRM%250Aemploys%2520a%2520scalable%2520transformer-based%2520architecture%2520to%2520encode%2520sparse-view%2520X-ray%250Ainputs%252C%2520where%2520tokens%2520from%2520different%2520views%2520are%2520integrated%2520efficiently.%2520Then%252C%250Athese%2520tokens%2520are%2520decoded%2520into%2520a%2520novel%2520volume%2520representation%252C%2520named%2520Voxel-based%250AGaussian%2520Splatting%2520%2528VoxGS%2529%252C%2520which%2520enables%2520efficient%2520CT%2520volume%2520extraction%2520and%250Adifferentiable%2520X-ray%2520rendering.%2520This%2520combination%2520of%2520a%2520high-capacity%2520model%2520and%250Aflexible%2520volume%2520representation%252C%2520empowers%2520our%2520model%2520to%2520produce%2520high-quality%250Areconstructions%2520from%2520various%2520testing%2520inputs%252C%2520including%2520in-domain%2520and%2520out-domain%250AX-ray%2520projections.%2520Our%2520codes%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/CUHK-AIM-Group/X-GRM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15235v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-GRM%3A%20Large%20Gaussian%20Reconstruction%20Model%20for%20Sparse-view%20X-rays%20to%0A%20%20Computed%20Tomography&entry.906535625=Yifan%20Liu%20and%20Wuyang%20Li%20and%20Weihao%20Yu%20and%20Chenxin%20Li%20and%20Alexandre%20Alahi%20and%20Max%20Meng%20and%20Yixuan%20Yuan&entry.1292438233=%20%20Computed%20Tomography%20serves%20as%20an%20indispensable%20tool%20in%20clinical%20workflows%2C%0Aproviding%20non-invasive%20visualization%20of%20internal%20anatomical%20structures.%0AExisting%20CT%20reconstruction%20works%20are%20limited%20to%20small-capacity%20model%0Aarchitecture%20and%20inflexible%20volume%20representation.%20In%20this%20work%2C%20we%20present%0AX-GRM%20%28X-ray%20Gaussian%20Reconstruction%20Model%29%2C%20a%20large%20feedforward%20model%20for%0Areconstructing%203D%20CT%20volumes%20from%20sparse-view%202D%20X-ray%20projections.%20X-GRM%0Aemploys%20a%20scalable%20transformer-based%20architecture%20to%20encode%20sparse-view%20X-ray%0Ainputs%2C%20where%20tokens%20from%20different%20views%20are%20integrated%20efficiently.%20Then%2C%0Athese%20tokens%20are%20decoded%20into%20a%20novel%20volume%20representation%2C%20named%20Voxel-based%0AGaussian%20Splatting%20%28VoxGS%29%2C%20which%20enables%20efficient%20CT%20volume%20extraction%20and%0Adifferentiable%20X-ray%20rendering.%20This%20combination%20of%20a%20high-capacity%20model%20and%0Aflexible%20volume%20representation%2C%20empowers%20our%20model%20to%20produce%20high-quality%0Areconstructions%20from%20various%20testing%20inputs%2C%20including%20in-domain%20and%20out-domain%0AX-ray%20projections.%20Our%20codes%20are%20available%20at%3A%0Ahttps%3A//github.com/CUHK-AIM-Group/X-GRM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15235v2&entry.124074799=Read"},
{"title": "Multi-modal brain encoding models for multi-modal stimuli", "author": "Subba Reddy Oota and Khushbu Pahwa and Mounika Marreddy and Maneesh Singh and Manish Gupta and Bapi S. Raju", "abstract": "  Despite participants engaging in unimodal stimuli, such as watching images or\nsilent videos, recent work has demonstrated that multi-modal Transformer models\ncan predict visual brain activity impressively well, even with incongruent\nmodality representations. This raises the question of how accurately these\nmulti-modal models can predict brain activity when participants are engaged in\nmulti-modal stimuli. As these models grow increasingly popular, their use in\nstudying neural activity provides insights into how our brains respond to such\nmulti-modal naturalistic stimuli, i.e., where it separates and integrates\ninformation across modalities through a hierarchy of early sensory regions to\nhigher cognition. We investigate this question by using multiple unimodal and\ntwo types of multi-modal models-cross-modal and jointly pretrained-to determine\nwhich type of model is more relevant to fMRI brain activity when participants\nare engaged in watching movies. We observe that both types of multi-modal\nmodels show improved alignment in several language and visual regions. This\nstudy also helps in identifying which brain regions process unimodal versus\nmulti-modal information. We further investigate the contribution of each\nmodality to multi-modal alignment by carefully removing unimodal features one\nby one from multi-modal representations, and find that there is additional\ninformation beyond the unimodal embeddings that is processed in the visual and\nlanguage regions. Based on this investigation, we find that while for\ncross-modal models, their brain alignment is partially attributed to the video\nmodality; for jointly pretrained models, it is partially attributed to both the\nvideo and audio modalities. This serves as a strong motivation for the\nneuroscience community to investigate the interpretability of these models for\ndeepening our understanding of multi-modal information processing in brain.\n", "link": "http://arxiv.org/abs/2505.20027v1", "date": "2025-05-26", "relevancy": 3.0522, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6172}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6172}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-modal%20brain%20encoding%20models%20for%20multi-modal%20stimuli&body=Title%3A%20Multi-modal%20brain%20encoding%20models%20for%20multi-modal%20stimuli%0AAuthor%3A%20Subba%20Reddy%20Oota%20and%20Khushbu%20Pahwa%20and%20Mounika%20Marreddy%20and%20Maneesh%20Singh%20and%20Manish%20Gupta%20and%20Bapi%20S.%20Raju%0AAbstract%3A%20%20%20Despite%20participants%20engaging%20in%20unimodal%20stimuli%2C%20such%20as%20watching%20images%20or%0Asilent%20videos%2C%20recent%20work%20has%20demonstrated%20that%20multi-modal%20Transformer%20models%0Acan%20predict%20visual%20brain%20activity%20impressively%20well%2C%20even%20with%20incongruent%0Amodality%20representations.%20This%20raises%20the%20question%20of%20how%20accurately%20these%0Amulti-modal%20models%20can%20predict%20brain%20activity%20when%20participants%20are%20engaged%20in%0Amulti-modal%20stimuli.%20As%20these%20models%20grow%20increasingly%20popular%2C%20their%20use%20in%0Astudying%20neural%20activity%20provides%20insights%20into%20how%20our%20brains%20respond%20to%20such%0Amulti-modal%20naturalistic%20stimuli%2C%20i.e.%2C%20where%20it%20separates%20and%20integrates%0Ainformation%20across%20modalities%20through%20a%20hierarchy%20of%20early%20sensory%20regions%20to%0Ahigher%20cognition.%20We%20investigate%20this%20question%20by%20using%20multiple%20unimodal%20and%0Atwo%20types%20of%20multi-modal%20models-cross-modal%20and%20jointly%20pretrained-to%20determine%0Awhich%20type%20of%20model%20is%20more%20relevant%20to%20fMRI%20brain%20activity%20when%20participants%0Aare%20engaged%20in%20watching%20movies.%20We%20observe%20that%20both%20types%20of%20multi-modal%0Amodels%20show%20improved%20alignment%20in%20several%20language%20and%20visual%20regions.%20This%0Astudy%20also%20helps%20in%20identifying%20which%20brain%20regions%20process%20unimodal%20versus%0Amulti-modal%20information.%20We%20further%20investigate%20the%20contribution%20of%20each%0Amodality%20to%20multi-modal%20alignment%20by%20carefully%20removing%20unimodal%20features%20one%0Aby%20one%20from%20multi-modal%20representations%2C%20and%20find%20that%20there%20is%20additional%0Ainformation%20beyond%20the%20unimodal%20embeddings%20that%20is%20processed%20in%20the%20visual%20and%0Alanguage%20regions.%20Based%20on%20this%20investigation%2C%20we%20find%20that%20while%20for%0Across-modal%20models%2C%20their%20brain%20alignment%20is%20partially%20attributed%20to%20the%20video%0Amodality%3B%20for%20jointly%20pretrained%20models%2C%20it%20is%20partially%20attributed%20to%20both%20the%0Avideo%20and%20audio%20modalities.%20This%20serves%20as%20a%20strong%20motivation%20for%20the%0Aneuroscience%20community%20to%20investigate%20the%20interpretability%20of%20these%20models%20for%0Adeepening%20our%20understanding%20of%20multi-modal%20information%20processing%20in%20brain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20027v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-modal%2520brain%2520encoding%2520models%2520for%2520multi-modal%2520stimuli%26entry.906535625%3DSubba%2520Reddy%2520Oota%2520and%2520Khushbu%2520Pahwa%2520and%2520Mounika%2520Marreddy%2520and%2520Maneesh%2520Singh%2520and%2520Manish%2520Gupta%2520and%2520Bapi%2520S.%2520Raju%26entry.1292438233%3D%2520%2520Despite%2520participants%2520engaging%2520in%2520unimodal%2520stimuli%252C%2520such%2520as%2520watching%2520images%2520or%250Asilent%2520videos%252C%2520recent%2520work%2520has%2520demonstrated%2520that%2520multi-modal%2520Transformer%2520models%250Acan%2520predict%2520visual%2520brain%2520activity%2520impressively%2520well%252C%2520even%2520with%2520incongruent%250Amodality%2520representations.%2520This%2520raises%2520the%2520question%2520of%2520how%2520accurately%2520these%250Amulti-modal%2520models%2520can%2520predict%2520brain%2520activity%2520when%2520participants%2520are%2520engaged%2520in%250Amulti-modal%2520stimuli.%2520As%2520these%2520models%2520grow%2520increasingly%2520popular%252C%2520their%2520use%2520in%250Astudying%2520neural%2520activity%2520provides%2520insights%2520into%2520how%2520our%2520brains%2520respond%2520to%2520such%250Amulti-modal%2520naturalistic%2520stimuli%252C%2520i.e.%252C%2520where%2520it%2520separates%2520and%2520integrates%250Ainformation%2520across%2520modalities%2520through%2520a%2520hierarchy%2520of%2520early%2520sensory%2520regions%2520to%250Ahigher%2520cognition.%2520We%2520investigate%2520this%2520question%2520by%2520using%2520multiple%2520unimodal%2520and%250Atwo%2520types%2520of%2520multi-modal%2520models-cross-modal%2520and%2520jointly%2520pretrained-to%2520determine%250Awhich%2520type%2520of%2520model%2520is%2520more%2520relevant%2520to%2520fMRI%2520brain%2520activity%2520when%2520participants%250Aare%2520engaged%2520in%2520watching%2520movies.%2520We%2520observe%2520that%2520both%2520types%2520of%2520multi-modal%250Amodels%2520show%2520improved%2520alignment%2520in%2520several%2520language%2520and%2520visual%2520regions.%2520This%250Astudy%2520also%2520helps%2520in%2520identifying%2520which%2520brain%2520regions%2520process%2520unimodal%2520versus%250Amulti-modal%2520information.%2520We%2520further%2520investigate%2520the%2520contribution%2520of%2520each%250Amodality%2520to%2520multi-modal%2520alignment%2520by%2520carefully%2520removing%2520unimodal%2520features%2520one%250Aby%2520one%2520from%2520multi-modal%2520representations%252C%2520and%2520find%2520that%2520there%2520is%2520additional%250Ainformation%2520beyond%2520the%2520unimodal%2520embeddings%2520that%2520is%2520processed%2520in%2520the%2520visual%2520and%250Alanguage%2520regions.%2520Based%2520on%2520this%2520investigation%252C%2520we%2520find%2520that%2520while%2520for%250Across-modal%2520models%252C%2520their%2520brain%2520alignment%2520is%2520partially%2520attributed%2520to%2520the%2520video%250Amodality%253B%2520for%2520jointly%2520pretrained%2520models%252C%2520it%2520is%2520partially%2520attributed%2520to%2520both%2520the%250Avideo%2520and%2520audio%2520modalities.%2520This%2520serves%2520as%2520a%2520strong%2520motivation%2520for%2520the%250Aneuroscience%2520community%2520to%2520investigate%2520the%2520interpretability%2520of%2520these%2520models%2520for%250Adeepening%2520our%2520understanding%2520of%2520multi-modal%2520information%2520processing%2520in%2520brain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20027v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-modal%20brain%20encoding%20models%20for%20multi-modal%20stimuli&entry.906535625=Subba%20Reddy%20Oota%20and%20Khushbu%20Pahwa%20and%20Mounika%20Marreddy%20and%20Maneesh%20Singh%20and%20Manish%20Gupta%20and%20Bapi%20S.%20Raju&entry.1292438233=%20%20Despite%20participants%20engaging%20in%20unimodal%20stimuli%2C%20such%20as%20watching%20images%20or%0Asilent%20videos%2C%20recent%20work%20has%20demonstrated%20that%20multi-modal%20Transformer%20models%0Acan%20predict%20visual%20brain%20activity%20impressively%20well%2C%20even%20with%20incongruent%0Amodality%20representations.%20This%20raises%20the%20question%20of%20how%20accurately%20these%0Amulti-modal%20models%20can%20predict%20brain%20activity%20when%20participants%20are%20engaged%20in%0Amulti-modal%20stimuli.%20As%20these%20models%20grow%20increasingly%20popular%2C%20their%20use%20in%0Astudying%20neural%20activity%20provides%20insights%20into%20how%20our%20brains%20respond%20to%20such%0Amulti-modal%20naturalistic%20stimuli%2C%20i.e.%2C%20where%20it%20separates%20and%20integrates%0Ainformation%20across%20modalities%20through%20a%20hierarchy%20of%20early%20sensory%20regions%20to%0Ahigher%20cognition.%20We%20investigate%20this%20question%20by%20using%20multiple%20unimodal%20and%0Atwo%20types%20of%20multi-modal%20models-cross-modal%20and%20jointly%20pretrained-to%20determine%0Awhich%20type%20of%20model%20is%20more%20relevant%20to%20fMRI%20brain%20activity%20when%20participants%0Aare%20engaged%20in%20watching%20movies.%20We%20observe%20that%20both%20types%20of%20multi-modal%0Amodels%20show%20improved%20alignment%20in%20several%20language%20and%20visual%20regions.%20This%0Astudy%20also%20helps%20in%20identifying%20which%20brain%20regions%20process%20unimodal%20versus%0Amulti-modal%20information.%20We%20further%20investigate%20the%20contribution%20of%20each%0Amodality%20to%20multi-modal%20alignment%20by%20carefully%20removing%20unimodal%20features%20one%0Aby%20one%20from%20multi-modal%20representations%2C%20and%20find%20that%20there%20is%20additional%0Ainformation%20beyond%20the%20unimodal%20embeddings%20that%20is%20processed%20in%20the%20visual%20and%0Alanguage%20regions.%20Based%20on%20this%20investigation%2C%20we%20find%20that%20while%20for%0Across-modal%20models%2C%20their%20brain%20alignment%20is%20partially%20attributed%20to%20the%20video%0Amodality%3B%20for%20jointly%20pretrained%20models%2C%20it%20is%20partially%20attributed%20to%20both%20the%0Avideo%20and%20audio%20modalities.%20This%20serves%20as%20a%20strong%20motivation%20for%20the%0Aneuroscience%20community%20to%20investigate%20the%20interpretability%20of%20these%20models%20for%0Adeepening%20our%20understanding%20of%20multi-modal%20information%20processing%20in%20brain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20027v1&entry.124074799=Read"},
{"title": "Hard Negative Contrastive Learning for Fine-Grained Geometric\n  Understanding in Large Multimodal Models", "author": "Kai Sun and Yushi Bai and Zhen Yang and Jiajie Zhang and Ji Qi and Lei Hou and Juanzi Li", "abstract": "  Benefiting from contrastively trained visual encoders on large-scale natural\nscene images, Large Multimodal Models (LMMs) have achieved remarkable\nperformance across various visual perception tasks. However, the inherent\nlimitations of contrastive learning upon summarized descriptions fundamentally\nrestrict the capabilities of models in meticulous reasoning, particularly in\ncrucial scenarios of geometric problem-solving. To enhance geometric\nunderstanding, we propose a novel hard negative contrastive learning framework\nfor the vision encoder, which combines image-based contrastive learning using\ngeneration-based hard negatives created by perturbing diagram generation code,\nand text-based contrastive learning using rule-based negatives derived from\nmodified geometric descriptions and retrieval-based negatives selected based on\ncaption similarity. We train CLIP using our strong negative learning method,\nnamely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for\ngeometric problem-solving. Experiments show that our trained model, MMGeoLM,\nsignificantly outperforms other open-source models on three geometric reasoning\nbenchmarks. Even with a size of 7B, it can rival powerful closed-source models\nlike GPT-4o. We further study the impact of different negative sample\nconstruction methods and the number of negative samples on the geometric\nreasoning performance of LMM, yielding fruitful conclusions. The code and\ndataset are available at https://github.com/THU-KEG/MMGeoLM.\n", "link": "http://arxiv.org/abs/2505.20152v1", "date": "2025-05-26", "relevancy": 2.9864, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6184}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5867}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hard%20Negative%20Contrastive%20Learning%20for%20Fine-Grained%20Geometric%0A%20%20Understanding%20in%20Large%20Multimodal%20Models&body=Title%3A%20Hard%20Negative%20Contrastive%20Learning%20for%20Fine-Grained%20Geometric%0A%20%20Understanding%20in%20Large%20Multimodal%20Models%0AAuthor%3A%20Kai%20Sun%20and%20Yushi%20Bai%20and%20Zhen%20Yang%20and%20Jiajie%20Zhang%20and%20Ji%20Qi%20and%20Lei%20Hou%20and%20Juanzi%20Li%0AAbstract%3A%20%20%20Benefiting%20from%20contrastively%20trained%20visual%20encoders%20on%20large-scale%20natural%0Ascene%20images%2C%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20achieved%20remarkable%0Aperformance%20across%20various%20visual%20perception%20tasks.%20However%2C%20the%20inherent%0Alimitations%20of%20contrastive%20learning%20upon%20summarized%20descriptions%20fundamentally%0Arestrict%20the%20capabilities%20of%20models%20in%20meticulous%20reasoning%2C%20particularly%20in%0Acrucial%20scenarios%20of%20geometric%20problem-solving.%20To%20enhance%20geometric%0Aunderstanding%2C%20we%20propose%20a%20novel%20hard%20negative%20contrastive%20learning%20framework%0Afor%20the%20vision%20encoder%2C%20which%20combines%20image-based%20contrastive%20learning%20using%0Ageneration-based%20hard%20negatives%20created%20by%20perturbing%20diagram%20generation%20code%2C%0Aand%20text-based%20contrastive%20learning%20using%20rule-based%20negatives%20derived%20from%0Amodified%20geometric%20descriptions%20and%20retrieval-based%20negatives%20selected%20based%20on%0Acaption%20similarity.%20We%20train%20CLIP%20using%20our%20strong%20negative%20learning%20method%2C%0Anamely%20MMCLIP%20%28Multimodal%20Math%20CLIP%29%2C%20and%20subsequently%20train%20an%20LMM%20for%0Ageometric%20problem-solving.%20Experiments%20show%20that%20our%20trained%20model%2C%20MMGeoLM%2C%0Asignificantly%20outperforms%20other%20open-source%20models%20on%20three%20geometric%20reasoning%0Abenchmarks.%20Even%20with%20a%20size%20of%207B%2C%20it%20can%20rival%20powerful%20closed-source%20models%0Alike%20GPT-4o.%20We%20further%20study%20the%20impact%20of%20different%20negative%20sample%0Aconstruction%20methods%20and%20the%20number%20of%20negative%20samples%20on%20the%20geometric%0Areasoning%20performance%20of%20LMM%2C%20yielding%20fruitful%20conclusions.%20The%20code%20and%0Adataset%20are%20available%20at%20https%3A//github.com/THU-KEG/MMGeoLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHard%2520Negative%2520Contrastive%2520Learning%2520for%2520Fine-Grained%2520Geometric%250A%2520%2520Understanding%2520in%2520Large%2520Multimodal%2520Models%26entry.906535625%3DKai%2520Sun%2520and%2520Yushi%2520Bai%2520and%2520Zhen%2520Yang%2520and%2520Jiajie%2520Zhang%2520and%2520Ji%2520Qi%2520and%2520Lei%2520Hou%2520and%2520Juanzi%2520Li%26entry.1292438233%3D%2520%2520Benefiting%2520from%2520contrastively%2520trained%2520visual%2520encoders%2520on%2520large-scale%2520natural%250Ascene%2520images%252C%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520have%2520achieved%2520remarkable%250Aperformance%2520across%2520various%2520visual%2520perception%2520tasks.%2520However%252C%2520the%2520inherent%250Alimitations%2520of%2520contrastive%2520learning%2520upon%2520summarized%2520descriptions%2520fundamentally%250Arestrict%2520the%2520capabilities%2520of%2520models%2520in%2520meticulous%2520reasoning%252C%2520particularly%2520in%250Acrucial%2520scenarios%2520of%2520geometric%2520problem-solving.%2520To%2520enhance%2520geometric%250Aunderstanding%252C%2520we%2520propose%2520a%2520novel%2520hard%2520negative%2520contrastive%2520learning%2520framework%250Afor%2520the%2520vision%2520encoder%252C%2520which%2520combines%2520image-based%2520contrastive%2520learning%2520using%250Ageneration-based%2520hard%2520negatives%2520created%2520by%2520perturbing%2520diagram%2520generation%2520code%252C%250Aand%2520text-based%2520contrastive%2520learning%2520using%2520rule-based%2520negatives%2520derived%2520from%250Amodified%2520geometric%2520descriptions%2520and%2520retrieval-based%2520negatives%2520selected%2520based%2520on%250Acaption%2520similarity.%2520We%2520train%2520CLIP%2520using%2520our%2520strong%2520negative%2520learning%2520method%252C%250Anamely%2520MMCLIP%2520%2528Multimodal%2520Math%2520CLIP%2529%252C%2520and%2520subsequently%2520train%2520an%2520LMM%2520for%250Ageometric%2520problem-solving.%2520Experiments%2520show%2520that%2520our%2520trained%2520model%252C%2520MMGeoLM%252C%250Asignificantly%2520outperforms%2520other%2520open-source%2520models%2520on%2520three%2520geometric%2520reasoning%250Abenchmarks.%2520Even%2520with%2520a%2520size%2520of%25207B%252C%2520it%2520can%2520rival%2520powerful%2520closed-source%2520models%250Alike%2520GPT-4o.%2520We%2520further%2520study%2520the%2520impact%2520of%2520different%2520negative%2520sample%250Aconstruction%2520methods%2520and%2520the%2520number%2520of%2520negative%2520samples%2520on%2520the%2520geometric%250Areasoning%2520performance%2520of%2520LMM%252C%2520yielding%2520fruitful%2520conclusions.%2520The%2520code%2520and%250Adataset%2520are%2520available%2520at%2520https%253A//github.com/THU-KEG/MMGeoLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hard%20Negative%20Contrastive%20Learning%20for%20Fine-Grained%20Geometric%0A%20%20Understanding%20in%20Large%20Multimodal%20Models&entry.906535625=Kai%20Sun%20and%20Yushi%20Bai%20and%20Zhen%20Yang%20and%20Jiajie%20Zhang%20and%20Ji%20Qi%20and%20Lei%20Hou%20and%20Juanzi%20Li&entry.1292438233=%20%20Benefiting%20from%20contrastively%20trained%20visual%20encoders%20on%20large-scale%20natural%0Ascene%20images%2C%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20achieved%20remarkable%0Aperformance%20across%20various%20visual%20perception%20tasks.%20However%2C%20the%20inherent%0Alimitations%20of%20contrastive%20learning%20upon%20summarized%20descriptions%20fundamentally%0Arestrict%20the%20capabilities%20of%20models%20in%20meticulous%20reasoning%2C%20particularly%20in%0Acrucial%20scenarios%20of%20geometric%20problem-solving.%20To%20enhance%20geometric%0Aunderstanding%2C%20we%20propose%20a%20novel%20hard%20negative%20contrastive%20learning%20framework%0Afor%20the%20vision%20encoder%2C%20which%20combines%20image-based%20contrastive%20learning%20using%0Ageneration-based%20hard%20negatives%20created%20by%20perturbing%20diagram%20generation%20code%2C%0Aand%20text-based%20contrastive%20learning%20using%20rule-based%20negatives%20derived%20from%0Amodified%20geometric%20descriptions%20and%20retrieval-based%20negatives%20selected%20based%20on%0Acaption%20similarity.%20We%20train%20CLIP%20using%20our%20strong%20negative%20learning%20method%2C%0Anamely%20MMCLIP%20%28Multimodal%20Math%20CLIP%29%2C%20and%20subsequently%20train%20an%20LMM%20for%0Ageometric%20problem-solving.%20Experiments%20show%20that%20our%20trained%20model%2C%20MMGeoLM%2C%0Asignificantly%20outperforms%20other%20open-source%20models%20on%20three%20geometric%20reasoning%0Abenchmarks.%20Even%20with%20a%20size%20of%207B%2C%20it%20can%20rival%20powerful%20closed-source%20models%0Alike%20GPT-4o.%20We%20further%20study%20the%20impact%20of%20different%20negative%20sample%0Aconstruction%20methods%20and%20the%20number%20of%20negative%20samples%20on%20the%20geometric%0Areasoning%20performance%20of%20LMM%2C%20yielding%20fruitful%20conclusions.%20The%20code%20and%0Adataset%20are%20available%20at%20https%3A//github.com/THU-KEG/MMGeoLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20152v1&entry.124074799=Read"},
{"title": "OB3D: A New Dataset for Benchmarking Omnidirectional 3D Reconstruction\n  Using Blender", "author": "Shintaro Ito and Natsuki Takama and Toshiki Watanabe and Koichi Ito and Hwann-Tzong Chen and Takafumi Aoki", "abstract": "  Recent advancements in radiance field rendering, exemplified by Neural\nRadiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have significantly\nprogressed 3D modeling and reconstruction. The use of multiple 360-degree\nomnidirectional images for these tasks is increasingly favored due to\nadvantages in data acquisition and comprehensive scene capture. However, the\ninherent geometric distortions in common omnidirectional representations, such\nas equirectangular projection (particularly severe in polar regions and varying\nwith latitude), pose substantial challenges to achieving high-fidelity 3D\nreconstructions. Current datasets, while valuable, often lack the specific\nfocus, scene composition, and ground truth granularity required to\nsystematically benchmark and drive progress in overcoming these\nomnidirectional-specific challenges. To address this critical gap, we introduce\nOmnidirectional Blender 3D (OB3D), a new synthetic dataset curated for\nadvancing 3D reconstruction from multiple omnidirectional images. OB3D features\ndiverse and complex 3D scenes generated from Blender 3D projects, with a\ndeliberate emphasis on challenging scenarios. The dataset provides\ncomprehensive ground truth, including omnidirectional RGB images, precise\nomnidirectional camera parameters, and pixel-aligned equirectangular maps for\ndepth and normals, alongside evaluation metrics. By offering a controlled yet\nchallenging environment, OB3Daims to facilitate the rigorous evaluation of\nexisting methods and prompt the development of new techniques to enhance the\naccuracy and reliability of 3D reconstruction from omnidirectional images.\n", "link": "http://arxiv.org/abs/2505.20126v1", "date": "2025-05-26", "relevancy": 2.9853, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6155}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6155}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OB3D%3A%20A%20New%20Dataset%20for%20Benchmarking%20Omnidirectional%203D%20Reconstruction%0A%20%20Using%20Blender&body=Title%3A%20OB3D%3A%20A%20New%20Dataset%20for%20Benchmarking%20Omnidirectional%203D%20Reconstruction%0A%20%20Using%20Blender%0AAuthor%3A%20Shintaro%20Ito%20and%20Natsuki%20Takama%20and%20Toshiki%20Watanabe%20and%20Koichi%20Ito%20and%20Hwann-Tzong%20Chen%20and%20Takafumi%20Aoki%0AAbstract%3A%20%20%20Recent%20advancements%20in%20radiance%20field%20rendering%2C%20exemplified%20by%20Neural%0ARadiance%20Fields%20%28NeRF%29%20and%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20have%20significantly%0Aprogressed%203D%20modeling%20and%20reconstruction.%20The%20use%20of%20multiple%20360-degree%0Aomnidirectional%20images%20for%20these%20tasks%20is%20increasingly%20favored%20due%20to%0Aadvantages%20in%20data%20acquisition%20and%20comprehensive%20scene%20capture.%20However%2C%20the%0Ainherent%20geometric%20distortions%20in%20common%20omnidirectional%20representations%2C%20such%0Aas%20equirectangular%20projection%20%28particularly%20severe%20in%20polar%20regions%20and%20varying%0Awith%20latitude%29%2C%20pose%20substantial%20challenges%20to%20achieving%20high-fidelity%203D%0Areconstructions.%20Current%20datasets%2C%20while%20valuable%2C%20often%20lack%20the%20specific%0Afocus%2C%20scene%20composition%2C%20and%20ground%20truth%20granularity%20required%20to%0Asystematically%20benchmark%20and%20drive%20progress%20in%20overcoming%20these%0Aomnidirectional-specific%20challenges.%20To%20address%20this%20critical%20gap%2C%20we%20introduce%0AOmnidirectional%20Blender%203D%20%28OB3D%29%2C%20a%20new%20synthetic%20dataset%20curated%20for%0Aadvancing%203D%20reconstruction%20from%20multiple%20omnidirectional%20images.%20OB3D%20features%0Adiverse%20and%20complex%203D%20scenes%20generated%20from%20Blender%203D%20projects%2C%20with%20a%0Adeliberate%20emphasis%20on%20challenging%20scenarios.%20The%20dataset%20provides%0Acomprehensive%20ground%20truth%2C%20including%20omnidirectional%20RGB%20images%2C%20precise%0Aomnidirectional%20camera%20parameters%2C%20and%20pixel-aligned%20equirectangular%20maps%20for%0Adepth%20and%20normals%2C%20alongside%20evaluation%20metrics.%20By%20offering%20a%20controlled%20yet%0Achallenging%20environment%2C%20OB3Daims%20to%20facilitate%20the%20rigorous%20evaluation%20of%0Aexisting%20methods%20and%20prompt%20the%20development%20of%20new%20techniques%20to%20enhance%20the%0Aaccuracy%20and%20reliability%20of%203D%20reconstruction%20from%20omnidirectional%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20126v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOB3D%253A%2520A%2520New%2520Dataset%2520for%2520Benchmarking%2520Omnidirectional%25203D%2520Reconstruction%250A%2520%2520Using%2520Blender%26entry.906535625%3DShintaro%2520Ito%2520and%2520Natsuki%2520Takama%2520and%2520Toshiki%2520Watanabe%2520and%2520Koichi%2520Ito%2520and%2520Hwann-Tzong%2520Chen%2520and%2520Takafumi%2520Aoki%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520radiance%2520field%2520rendering%252C%2520exemplified%2520by%2520Neural%250ARadiance%2520Fields%2520%2528NeRF%2529%2520and%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%252C%2520have%2520significantly%250Aprogressed%25203D%2520modeling%2520and%2520reconstruction.%2520The%2520use%2520of%2520multiple%2520360-degree%250Aomnidirectional%2520images%2520for%2520these%2520tasks%2520is%2520increasingly%2520favored%2520due%2520to%250Aadvantages%2520in%2520data%2520acquisition%2520and%2520comprehensive%2520scene%2520capture.%2520However%252C%2520the%250Ainherent%2520geometric%2520distortions%2520in%2520common%2520omnidirectional%2520representations%252C%2520such%250Aas%2520equirectangular%2520projection%2520%2528particularly%2520severe%2520in%2520polar%2520regions%2520and%2520varying%250Awith%2520latitude%2529%252C%2520pose%2520substantial%2520challenges%2520to%2520achieving%2520high-fidelity%25203D%250Areconstructions.%2520Current%2520datasets%252C%2520while%2520valuable%252C%2520often%2520lack%2520the%2520specific%250Afocus%252C%2520scene%2520composition%252C%2520and%2520ground%2520truth%2520granularity%2520required%2520to%250Asystematically%2520benchmark%2520and%2520drive%2520progress%2520in%2520overcoming%2520these%250Aomnidirectional-specific%2520challenges.%2520To%2520address%2520this%2520critical%2520gap%252C%2520we%2520introduce%250AOmnidirectional%2520Blender%25203D%2520%2528OB3D%2529%252C%2520a%2520new%2520synthetic%2520dataset%2520curated%2520for%250Aadvancing%25203D%2520reconstruction%2520from%2520multiple%2520omnidirectional%2520images.%2520OB3D%2520features%250Adiverse%2520and%2520complex%25203D%2520scenes%2520generated%2520from%2520Blender%25203D%2520projects%252C%2520with%2520a%250Adeliberate%2520emphasis%2520on%2520challenging%2520scenarios.%2520The%2520dataset%2520provides%250Acomprehensive%2520ground%2520truth%252C%2520including%2520omnidirectional%2520RGB%2520images%252C%2520precise%250Aomnidirectional%2520camera%2520parameters%252C%2520and%2520pixel-aligned%2520equirectangular%2520maps%2520for%250Adepth%2520and%2520normals%252C%2520alongside%2520evaluation%2520metrics.%2520By%2520offering%2520a%2520controlled%2520yet%250Achallenging%2520environment%252C%2520OB3Daims%2520to%2520facilitate%2520the%2520rigorous%2520evaluation%2520of%250Aexisting%2520methods%2520and%2520prompt%2520the%2520development%2520of%2520new%2520techniques%2520to%2520enhance%2520the%250Aaccuracy%2520and%2520reliability%2520of%25203D%2520reconstruction%2520from%2520omnidirectional%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20126v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OB3D%3A%20A%20New%20Dataset%20for%20Benchmarking%20Omnidirectional%203D%20Reconstruction%0A%20%20Using%20Blender&entry.906535625=Shintaro%20Ito%20and%20Natsuki%20Takama%20and%20Toshiki%20Watanabe%20and%20Koichi%20Ito%20and%20Hwann-Tzong%20Chen%20and%20Takafumi%20Aoki&entry.1292438233=%20%20Recent%20advancements%20in%20radiance%20field%20rendering%2C%20exemplified%20by%20Neural%0ARadiance%20Fields%20%28NeRF%29%20and%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20have%20significantly%0Aprogressed%203D%20modeling%20and%20reconstruction.%20The%20use%20of%20multiple%20360-degree%0Aomnidirectional%20images%20for%20these%20tasks%20is%20increasingly%20favored%20due%20to%0Aadvantages%20in%20data%20acquisition%20and%20comprehensive%20scene%20capture.%20However%2C%20the%0Ainherent%20geometric%20distortions%20in%20common%20omnidirectional%20representations%2C%20such%0Aas%20equirectangular%20projection%20%28particularly%20severe%20in%20polar%20regions%20and%20varying%0Awith%20latitude%29%2C%20pose%20substantial%20challenges%20to%20achieving%20high-fidelity%203D%0Areconstructions.%20Current%20datasets%2C%20while%20valuable%2C%20often%20lack%20the%20specific%0Afocus%2C%20scene%20composition%2C%20and%20ground%20truth%20granularity%20required%20to%0Asystematically%20benchmark%20and%20drive%20progress%20in%20overcoming%20these%0Aomnidirectional-specific%20challenges.%20To%20address%20this%20critical%20gap%2C%20we%20introduce%0AOmnidirectional%20Blender%203D%20%28OB3D%29%2C%20a%20new%20synthetic%20dataset%20curated%20for%0Aadvancing%203D%20reconstruction%20from%20multiple%20omnidirectional%20images.%20OB3D%20features%0Adiverse%20and%20complex%203D%20scenes%20generated%20from%20Blender%203D%20projects%2C%20with%20a%0Adeliberate%20emphasis%20on%20challenging%20scenarios.%20The%20dataset%20provides%0Acomprehensive%20ground%20truth%2C%20including%20omnidirectional%20RGB%20images%2C%20precise%0Aomnidirectional%20camera%20parameters%2C%20and%20pixel-aligned%20equirectangular%20maps%20for%0Adepth%20and%20normals%2C%20alongside%20evaluation%20metrics.%20By%20offering%20a%20controlled%20yet%0Achallenging%20environment%2C%20OB3Daims%20to%20facilitate%20the%20rigorous%20evaluation%20of%0Aexisting%20methods%20and%20prompt%20the%20development%20of%20new%20techniques%20to%20enhance%20the%0Aaccuracy%20and%20reliability%20of%203D%20reconstruction%20from%20omnidirectional%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20126v1&entry.124074799=Read"},
{"title": "RapidPoseTriangulation: Multi-view Multi-person Whole-body Human Pose\n  Triangulation in a Millisecond", "author": "Daniel Bermuth and Alexander Poeppel and Wolfgang Reif", "abstract": "  The integration of multi-view imaging and pose estimation represents a\nsignificant advance in computer vision applications, offering new possibilities\nfor understanding human movement and interactions. This work presents a new\nalgorithm that improves multi-view multi-person pose estimation, focusing on\nfast triangulation speeds and good generalization capabilities.\n  The approach extends to whole-body pose estimation, capturing details from\nfacial expressions to finger movements across multiple individuals and\nviewpoints. Adaptability to different settings is demonstrated through strong\nperformance across unseen datasets and configurations. To support further\nprogress in this field, all of this work is publicly accessible.\n", "link": "http://arxiv.org/abs/2503.21692v2", "date": "2025-05-26", "relevancy": 2.9438, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6065}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5894}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RapidPoseTriangulation%3A%20Multi-view%20Multi-person%20Whole-body%20Human%20Pose%0A%20%20Triangulation%20in%20a%20Millisecond&body=Title%3A%20RapidPoseTriangulation%3A%20Multi-view%20Multi-person%20Whole-body%20Human%20Pose%0A%20%20Triangulation%20in%20a%20Millisecond%0AAuthor%3A%20Daniel%20Bermuth%20and%20Alexander%20Poeppel%20and%20Wolfgang%20Reif%0AAbstract%3A%20%20%20The%20integration%20of%20multi-view%20imaging%20and%20pose%20estimation%20represents%20a%0Asignificant%20advance%20in%20computer%20vision%20applications%2C%20offering%20new%20possibilities%0Afor%20understanding%20human%20movement%20and%20interactions.%20This%20work%20presents%20a%20new%0Aalgorithm%20that%20improves%20multi-view%20multi-person%20pose%20estimation%2C%20focusing%20on%0Afast%20triangulation%20speeds%20and%20good%20generalization%20capabilities.%0A%20%20The%20approach%20extends%20to%20whole-body%20pose%20estimation%2C%20capturing%20details%20from%0Afacial%20expressions%20to%20finger%20movements%20across%20multiple%20individuals%20and%0Aviewpoints.%20Adaptability%20to%20different%20settings%20is%20demonstrated%20through%20strong%0Aperformance%20across%20unseen%20datasets%20and%20configurations.%20To%20support%20further%0Aprogress%20in%20this%20field%2C%20all%20of%20this%20work%20is%20publicly%20accessible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.21692v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRapidPoseTriangulation%253A%2520Multi-view%2520Multi-person%2520Whole-body%2520Human%2520Pose%250A%2520%2520Triangulation%2520in%2520a%2520Millisecond%26entry.906535625%3DDaniel%2520Bermuth%2520and%2520Alexander%2520Poeppel%2520and%2520Wolfgang%2520Reif%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520multi-view%2520imaging%2520and%2520pose%2520estimation%2520represents%2520a%250Asignificant%2520advance%2520in%2520computer%2520vision%2520applications%252C%2520offering%2520new%2520possibilities%250Afor%2520understanding%2520human%2520movement%2520and%2520interactions.%2520This%2520work%2520presents%2520a%2520new%250Aalgorithm%2520that%2520improves%2520multi-view%2520multi-person%2520pose%2520estimation%252C%2520focusing%2520on%250Afast%2520triangulation%2520speeds%2520and%2520good%2520generalization%2520capabilities.%250A%2520%2520The%2520approach%2520extends%2520to%2520whole-body%2520pose%2520estimation%252C%2520capturing%2520details%2520from%250Afacial%2520expressions%2520to%2520finger%2520movements%2520across%2520multiple%2520individuals%2520and%250Aviewpoints.%2520Adaptability%2520to%2520different%2520settings%2520is%2520demonstrated%2520through%2520strong%250Aperformance%2520across%2520unseen%2520datasets%2520and%2520configurations.%2520To%2520support%2520further%250Aprogress%2520in%2520this%2520field%252C%2520all%2520of%2520this%2520work%2520is%2520publicly%2520accessible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21692v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RapidPoseTriangulation%3A%20Multi-view%20Multi-person%20Whole-body%20Human%20Pose%0A%20%20Triangulation%20in%20a%20Millisecond&entry.906535625=Daniel%20Bermuth%20and%20Alexander%20Poeppel%20and%20Wolfgang%20Reif&entry.1292438233=%20%20The%20integration%20of%20multi-view%20imaging%20and%20pose%20estimation%20represents%20a%0Asignificant%20advance%20in%20computer%20vision%20applications%2C%20offering%20new%20possibilities%0Afor%20understanding%20human%20movement%20and%20interactions.%20This%20work%20presents%20a%20new%0Aalgorithm%20that%20improves%20multi-view%20multi-person%20pose%20estimation%2C%20focusing%20on%0Afast%20triangulation%20speeds%20and%20good%20generalization%20capabilities.%0A%20%20The%20approach%20extends%20to%20whole-body%20pose%20estimation%2C%20capturing%20details%20from%0Afacial%20expressions%20to%20finger%20movements%20across%20multiple%20individuals%20and%0Aviewpoints.%20Adaptability%20to%20different%20settings%20is%20demonstrated%20through%20strong%0Aperformance%20across%20unseen%20datasets%20and%20configurations.%20To%20support%20further%0Aprogress%20in%20this%20field%2C%20all%20of%20this%20work%20is%20publicly%20accessible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.21692v2&entry.124074799=Read"},
{"title": "M3DHMR: Monocular 3D Hand Mesh Recovery", "author": "Yihong Lin and Xianjia Wu and Xilai Wang and Jianqiao Hu and Songju Lei and Xiandong Li and Wenxiong Kang", "abstract": "  Monocular 3D hand mesh recovery is challenging due to high degrees of freedom\nof hands, 2D-to-3D ambiguity and self-occlusion. Most existing methods are\neither inefficient or less straightforward for predicting the position of 3D\nmesh vertices. Thus, we propose a new pipeline called Monocular 3D Hand Mesh\nRecovery (M3DHMR) to directly estimate the positions of hand mesh vertices.\nM3DHMR provides 2D cues for 3D tasks from a single image and uses a new spiral\ndecoder consist of several Dynamic Spiral Convolution (DSC) Layers and a Region\nof Interest (ROI) Layer. On the one hand, DSC Layers adaptively adjust the\nweights based on the vertex positions and extract the vertex features in both\nspatial and channel dimensions. On the other hand, ROI Layer utilizes the\nphysical information and refines mesh vertices in each predefined hand region\nseparately. Extensive experiments on popular dataset FreiHAND demonstrate that\nM3DHMR significantly outperforms state-of-the-art real-time methods.\n", "link": "http://arxiv.org/abs/2505.20058v1", "date": "2025-05-26", "relevancy": 2.9416, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6221}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5788}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M3DHMR%3A%20Monocular%203D%20Hand%20Mesh%20Recovery&body=Title%3A%20M3DHMR%3A%20Monocular%203D%20Hand%20Mesh%20Recovery%0AAuthor%3A%20Yihong%20Lin%20and%20Xianjia%20Wu%20and%20Xilai%20Wang%20and%20Jianqiao%20Hu%20and%20Songju%20Lei%20and%20Xiandong%20Li%20and%20Wenxiong%20Kang%0AAbstract%3A%20%20%20Monocular%203D%20hand%20mesh%20recovery%20is%20challenging%20due%20to%20high%20degrees%20of%20freedom%0Aof%20hands%2C%202D-to-3D%20ambiguity%20and%20self-occlusion.%20Most%20existing%20methods%20are%0Aeither%20inefficient%20or%20less%20straightforward%20for%20predicting%20the%20position%20of%203D%0Amesh%20vertices.%20Thus%2C%20we%20propose%20a%20new%20pipeline%20called%20Monocular%203D%20Hand%20Mesh%0ARecovery%20%28M3DHMR%29%20to%20directly%20estimate%20the%20positions%20of%20hand%20mesh%20vertices.%0AM3DHMR%20provides%202D%20cues%20for%203D%20tasks%20from%20a%20single%20image%20and%20uses%20a%20new%20spiral%0Adecoder%20consist%20of%20several%20Dynamic%20Spiral%20Convolution%20%28DSC%29%20Layers%20and%20a%20Region%0Aof%20Interest%20%28ROI%29%20Layer.%20On%20the%20one%20hand%2C%20DSC%20Layers%20adaptively%20adjust%20the%0Aweights%20based%20on%20the%20vertex%20positions%20and%20extract%20the%20vertex%20features%20in%20both%0Aspatial%20and%20channel%20dimensions.%20On%20the%20other%20hand%2C%20ROI%20Layer%20utilizes%20the%0Aphysical%20information%20and%20refines%20mesh%20vertices%20in%20each%20predefined%20hand%20region%0Aseparately.%20Extensive%20experiments%20on%20popular%20dataset%20FreiHAND%20demonstrate%20that%0AM3DHMR%20significantly%20outperforms%20state-of-the-art%20real-time%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20058v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM3DHMR%253A%2520Monocular%25203D%2520Hand%2520Mesh%2520Recovery%26entry.906535625%3DYihong%2520Lin%2520and%2520Xianjia%2520Wu%2520and%2520Xilai%2520Wang%2520and%2520Jianqiao%2520Hu%2520and%2520Songju%2520Lei%2520and%2520Xiandong%2520Li%2520and%2520Wenxiong%2520Kang%26entry.1292438233%3D%2520%2520Monocular%25203D%2520hand%2520mesh%2520recovery%2520is%2520challenging%2520due%2520to%2520high%2520degrees%2520of%2520freedom%250Aof%2520hands%252C%25202D-to-3D%2520ambiguity%2520and%2520self-occlusion.%2520Most%2520existing%2520methods%2520are%250Aeither%2520inefficient%2520or%2520less%2520straightforward%2520for%2520predicting%2520the%2520position%2520of%25203D%250Amesh%2520vertices.%2520Thus%252C%2520we%2520propose%2520a%2520new%2520pipeline%2520called%2520Monocular%25203D%2520Hand%2520Mesh%250ARecovery%2520%2528M3DHMR%2529%2520to%2520directly%2520estimate%2520the%2520positions%2520of%2520hand%2520mesh%2520vertices.%250AM3DHMR%2520provides%25202D%2520cues%2520for%25203D%2520tasks%2520from%2520a%2520single%2520image%2520and%2520uses%2520a%2520new%2520spiral%250Adecoder%2520consist%2520of%2520several%2520Dynamic%2520Spiral%2520Convolution%2520%2528DSC%2529%2520Layers%2520and%2520a%2520Region%250Aof%2520Interest%2520%2528ROI%2529%2520Layer.%2520On%2520the%2520one%2520hand%252C%2520DSC%2520Layers%2520adaptively%2520adjust%2520the%250Aweights%2520based%2520on%2520the%2520vertex%2520positions%2520and%2520extract%2520the%2520vertex%2520features%2520in%2520both%250Aspatial%2520and%2520channel%2520dimensions.%2520On%2520the%2520other%2520hand%252C%2520ROI%2520Layer%2520utilizes%2520the%250Aphysical%2520information%2520and%2520refines%2520mesh%2520vertices%2520in%2520each%2520predefined%2520hand%2520region%250Aseparately.%2520Extensive%2520experiments%2520on%2520popular%2520dataset%2520FreiHAND%2520demonstrate%2520that%250AM3DHMR%2520significantly%2520outperforms%2520state-of-the-art%2520real-time%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20058v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M3DHMR%3A%20Monocular%203D%20Hand%20Mesh%20Recovery&entry.906535625=Yihong%20Lin%20and%20Xianjia%20Wu%20and%20Xilai%20Wang%20and%20Jianqiao%20Hu%20and%20Songju%20Lei%20and%20Xiandong%20Li%20and%20Wenxiong%20Kang&entry.1292438233=%20%20Monocular%203D%20hand%20mesh%20recovery%20is%20challenging%20due%20to%20high%20degrees%20of%20freedom%0Aof%20hands%2C%202D-to-3D%20ambiguity%20and%20self-occlusion.%20Most%20existing%20methods%20are%0Aeither%20inefficient%20or%20less%20straightforward%20for%20predicting%20the%20position%20of%203D%0Amesh%20vertices.%20Thus%2C%20we%20propose%20a%20new%20pipeline%20called%20Monocular%203D%20Hand%20Mesh%0ARecovery%20%28M3DHMR%29%20to%20directly%20estimate%20the%20positions%20of%20hand%20mesh%20vertices.%0AM3DHMR%20provides%202D%20cues%20for%203D%20tasks%20from%20a%20single%20image%20and%20uses%20a%20new%20spiral%0Adecoder%20consist%20of%20several%20Dynamic%20Spiral%20Convolution%20%28DSC%29%20Layers%20and%20a%20Region%0Aof%20Interest%20%28ROI%29%20Layer.%20On%20the%20one%20hand%2C%20DSC%20Layers%20adaptively%20adjust%20the%0Aweights%20based%20on%20the%20vertex%20positions%20and%20extract%20the%20vertex%20features%20in%20both%0Aspatial%20and%20channel%20dimensions.%20On%20the%20other%20hand%2C%20ROI%20Layer%20utilizes%20the%0Aphysical%20information%20and%20refines%20mesh%20vertices%20in%20each%20predefined%20hand%20region%0Aseparately.%20Extensive%20experiments%20on%20popular%20dataset%20FreiHAND%20demonstrate%20that%0AM3DHMR%20significantly%20outperforms%20state-of-the-art%20real-time%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20058v1&entry.124074799=Read"},
{"title": "Human-Aligned Image Models Improve Visual Decoding from the Brain", "author": "Nona Rajabi and Ant\u00f4nio H. Ribeiro and Miguel Vasco and Farzaneh Taleb and M\u00e5rten Bj\u00f6rkman and Danica Kragic", "abstract": "  Decoding visual images from brain activity has significant potential for\nadvancing brain-computer interaction and enhancing the understanding of human\nperception. Recent approaches align the representation spaces of images and\nbrain activity to enable visual decoding. In this paper, we introduce the use\nof human-aligned image encoders to map brain signals to images. We hypothesize\nthat these models more effectively capture perceptual attributes associated\nwith the rapid visual stimuli presentations commonly used in visual brain data\nrecording experiments. Our empirical results support this hypothesis,\ndemonstrating that this simple modification improves image retrieval accuracy\nby up to 21% compared to state-of-the-art methods. Comprehensive experiments\nconfirm consistent performance improvements across diverse EEG architectures,\nimage encoders, alignment methods, participants, and brain imaging modalities\n", "link": "http://arxiv.org/abs/2502.03081v2", "date": "2025-05-26", "relevancy": 2.9398, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6197}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6197}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Aligned%20Image%20Models%20Improve%20Visual%20Decoding%20from%20the%20Brain&body=Title%3A%20Human-Aligned%20Image%20Models%20Improve%20Visual%20Decoding%20from%20the%20Brain%0AAuthor%3A%20Nona%20Rajabi%20and%20Ant%C3%B4nio%20H.%20Ribeiro%20and%20Miguel%20Vasco%20and%20Farzaneh%20Taleb%20and%20M%C3%A5rten%20Bj%C3%B6rkman%20and%20Danica%20Kragic%0AAbstract%3A%20%20%20Decoding%20visual%20images%20from%20brain%20activity%20has%20significant%20potential%20for%0Aadvancing%20brain-computer%20interaction%20and%20enhancing%20the%20understanding%20of%20human%0Aperception.%20Recent%20approaches%20align%20the%20representation%20spaces%20of%20images%20and%0Abrain%20activity%20to%20enable%20visual%20decoding.%20In%20this%20paper%2C%20we%20introduce%20the%20use%0Aof%20human-aligned%20image%20encoders%20to%20map%20brain%20signals%20to%20images.%20We%20hypothesize%0Athat%20these%20models%20more%20effectively%20capture%20perceptual%20attributes%20associated%0Awith%20the%20rapid%20visual%20stimuli%20presentations%20commonly%20used%20in%20visual%20brain%20data%0Arecording%20experiments.%20Our%20empirical%20results%20support%20this%20hypothesis%2C%0Ademonstrating%20that%20this%20simple%20modification%20improves%20image%20retrieval%20accuracy%0Aby%20up%20to%2021%25%20compared%20to%20state-of-the-art%20methods.%20Comprehensive%20experiments%0Aconfirm%20consistent%20performance%20improvements%20across%20diverse%20EEG%20architectures%2C%0Aimage%20encoders%2C%20alignment%20methods%2C%20participants%2C%20and%20brain%20imaging%20modalities%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03081v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Aligned%2520Image%2520Models%2520Improve%2520Visual%2520Decoding%2520from%2520the%2520Brain%26entry.906535625%3DNona%2520Rajabi%2520and%2520Ant%25C3%25B4nio%2520H.%2520Ribeiro%2520and%2520Miguel%2520Vasco%2520and%2520Farzaneh%2520Taleb%2520and%2520M%25C3%25A5rten%2520Bj%25C3%25B6rkman%2520and%2520Danica%2520Kragic%26entry.1292438233%3D%2520%2520Decoding%2520visual%2520images%2520from%2520brain%2520activity%2520has%2520significant%2520potential%2520for%250Aadvancing%2520brain-computer%2520interaction%2520and%2520enhancing%2520the%2520understanding%2520of%2520human%250Aperception.%2520Recent%2520approaches%2520align%2520the%2520representation%2520spaces%2520of%2520images%2520and%250Abrain%2520activity%2520to%2520enable%2520visual%2520decoding.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520use%250Aof%2520human-aligned%2520image%2520encoders%2520to%2520map%2520brain%2520signals%2520to%2520images.%2520We%2520hypothesize%250Athat%2520these%2520models%2520more%2520effectively%2520capture%2520perceptual%2520attributes%2520associated%250Awith%2520the%2520rapid%2520visual%2520stimuli%2520presentations%2520commonly%2520used%2520in%2520visual%2520brain%2520data%250Arecording%2520experiments.%2520Our%2520empirical%2520results%2520support%2520this%2520hypothesis%252C%250Ademonstrating%2520that%2520this%2520simple%2520modification%2520improves%2520image%2520retrieval%2520accuracy%250Aby%2520up%2520to%252021%2525%2520compared%2520to%2520state-of-the-art%2520methods.%2520Comprehensive%2520experiments%250Aconfirm%2520consistent%2520performance%2520improvements%2520across%2520diverse%2520EEG%2520architectures%252C%250Aimage%2520encoders%252C%2520alignment%2520methods%252C%2520participants%252C%2520and%2520brain%2520imaging%2520modalities%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03081v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Aligned%20Image%20Models%20Improve%20Visual%20Decoding%20from%20the%20Brain&entry.906535625=Nona%20Rajabi%20and%20Ant%C3%B4nio%20H.%20Ribeiro%20and%20Miguel%20Vasco%20and%20Farzaneh%20Taleb%20and%20M%C3%A5rten%20Bj%C3%B6rkman%20and%20Danica%20Kragic&entry.1292438233=%20%20Decoding%20visual%20images%20from%20brain%20activity%20has%20significant%20potential%20for%0Aadvancing%20brain-computer%20interaction%20and%20enhancing%20the%20understanding%20of%20human%0Aperception.%20Recent%20approaches%20align%20the%20representation%20spaces%20of%20images%20and%0Abrain%20activity%20to%20enable%20visual%20decoding.%20In%20this%20paper%2C%20we%20introduce%20the%20use%0Aof%20human-aligned%20image%20encoders%20to%20map%20brain%20signals%20to%20images.%20We%20hypothesize%0Athat%20these%20models%20more%20effectively%20capture%20perceptual%20attributes%20associated%0Awith%20the%20rapid%20visual%20stimuli%20presentations%20commonly%20used%20in%20visual%20brain%20data%0Arecording%20experiments.%20Our%20empirical%20results%20support%20this%20hypothesis%2C%0Ademonstrating%20that%20this%20simple%20modification%20improves%20image%20retrieval%20accuracy%0Aby%20up%20to%2021%25%20compared%20to%20state-of-the-art%20methods.%20Comprehensive%20experiments%0Aconfirm%20consistent%20performance%20improvements%20across%20diverse%20EEG%20architectures%2C%0Aimage%20encoders%2C%20alignment%20methods%2C%20participants%2C%20and%20brain%20imaging%20modalities%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03081v2&entry.124074799=Read"},
{"title": "Correlating instruction-tuning (in multimodal models) with\n  vision-language processing (in the brain)", "author": "Subba Reddy Oota and Akshett Jindal and Ishani Mondal and Khushbu Pahwa and Satya Sai Srinath Namburi and Manish Shrivastava and Maneesh Singh and Bapi S. Raju and Manish Gupta", "abstract": "  Transformer-based language models, though not explicitly trained to mimic\nbrain recordings, have demonstrated surprising alignment with brain activity.\nProgress in these models-through increased size, instruction-tuning, and\nmultimodality-has led to better representational alignment with neural data.\nRecently, a new class of instruction-tuned multimodal LLMs (MLLMs) have\nemerged, showing remarkable zero-shot capabilities in open-ended multimodal\nvision tasks. However, it is unknown whether MLLMs, when prompted with natural\ninstructions, lead to better brain alignment and effectively capture\ninstruction-specific representations. To address this, we first investigate\nbrain alignment, i.e., measuring the degree of predictivity of neural visual\nactivity using text output response embeddings from MLLMs as participants\nengage in watching natural scenes. Experiments with 10 different instructions\nshow that MLLMs exhibit significantly better brain alignment than vision-only\nmodels and perform comparably to non-instruction-tuned multimodal models like\nCLIP. We also find that while these MLLMs are effective at generating\nhigh-quality responses suitable to the task-specific instructions, not all\ninstructions are relevant for brain alignment. Further, by varying\ninstructions, we make the MLLMs encode instruction-specific visual concepts\nrelated to the input image. This analysis shows that MLLMs effectively capture\ncount-related and recognition-related concepts, demonstrating strong alignment\nwith brain activity. Notably, the majority of the explained variance of the\nbrain encoding models is shared between MLLM embeddings of image captioning and\nother instructions. These results suggest that enhancing MLLMs' ability to\ncapture task-specific information could lead to better differentiation between\nvarious types of instructions, and thereby improving their precision in\npredicting brain responses.\n", "link": "http://arxiv.org/abs/2505.20029v1", "date": "2025-05-26", "relevancy": 2.916, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5867}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5867}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Correlating%20instruction-tuning%20%28in%20multimodal%20models%29%20with%0A%20%20vision-language%20processing%20%28in%20the%20brain%29&body=Title%3A%20Correlating%20instruction-tuning%20%28in%20multimodal%20models%29%20with%0A%20%20vision-language%20processing%20%28in%20the%20brain%29%0AAuthor%3A%20Subba%20Reddy%20Oota%20and%20Akshett%20Jindal%20and%20Ishani%20Mondal%20and%20Khushbu%20Pahwa%20and%20Satya%20Sai%20Srinath%20Namburi%20and%20Manish%20Shrivastava%20and%20Maneesh%20Singh%20and%20Bapi%20S.%20Raju%20and%20Manish%20Gupta%0AAbstract%3A%20%20%20Transformer-based%20language%20models%2C%20though%20not%20explicitly%20trained%20to%20mimic%0Abrain%20recordings%2C%20have%20demonstrated%20surprising%20alignment%20with%20brain%20activity.%0AProgress%20in%20these%20models-through%20increased%20size%2C%20instruction-tuning%2C%20and%0Amultimodality-has%20led%20to%20better%20representational%20alignment%20with%20neural%20data.%0ARecently%2C%20a%20new%20class%20of%20instruction-tuned%20multimodal%20LLMs%20%28MLLMs%29%20have%0Aemerged%2C%20showing%20remarkable%20zero-shot%20capabilities%20in%20open-ended%20multimodal%0Avision%20tasks.%20However%2C%20it%20is%20unknown%20whether%20MLLMs%2C%20when%20prompted%20with%20natural%0Ainstructions%2C%20lead%20to%20better%20brain%20alignment%20and%20effectively%20capture%0Ainstruction-specific%20representations.%20To%20address%20this%2C%20we%20first%20investigate%0Abrain%20alignment%2C%20i.e.%2C%20measuring%20the%20degree%20of%20predictivity%20of%20neural%20visual%0Aactivity%20using%20text%20output%20response%20embeddings%20from%20MLLMs%20as%20participants%0Aengage%20in%20watching%20natural%20scenes.%20Experiments%20with%2010%20different%20instructions%0Ashow%20that%20MLLMs%20exhibit%20significantly%20better%20brain%20alignment%20than%20vision-only%0Amodels%20and%20perform%20comparably%20to%20non-instruction-tuned%20multimodal%20models%20like%0ACLIP.%20We%20also%20find%20that%20while%20these%20MLLMs%20are%20effective%20at%20generating%0Ahigh-quality%20responses%20suitable%20to%20the%20task-specific%20instructions%2C%20not%20all%0Ainstructions%20are%20relevant%20for%20brain%20alignment.%20Further%2C%20by%20varying%0Ainstructions%2C%20we%20make%20the%20MLLMs%20encode%20instruction-specific%20visual%20concepts%0Arelated%20to%20the%20input%20image.%20This%20analysis%20shows%20that%20MLLMs%20effectively%20capture%0Acount-related%20and%20recognition-related%20concepts%2C%20demonstrating%20strong%20alignment%0Awith%20brain%20activity.%20Notably%2C%20the%20majority%20of%20the%20explained%20variance%20of%20the%0Abrain%20encoding%20models%20is%20shared%20between%20MLLM%20embeddings%20of%20image%20captioning%20and%0Aother%20instructions.%20These%20results%20suggest%20that%20enhancing%20MLLMs%27%20ability%20to%0Acapture%20task-specific%20information%20could%20lead%20to%20better%20differentiation%20between%0Avarious%20types%20of%20instructions%2C%20and%20thereby%20improving%20their%20precision%20in%0Apredicting%20brain%20responses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20029v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorrelating%2520instruction-tuning%2520%2528in%2520multimodal%2520models%2529%2520with%250A%2520%2520vision-language%2520processing%2520%2528in%2520the%2520brain%2529%26entry.906535625%3DSubba%2520Reddy%2520Oota%2520and%2520Akshett%2520Jindal%2520and%2520Ishani%2520Mondal%2520and%2520Khushbu%2520Pahwa%2520and%2520Satya%2520Sai%2520Srinath%2520Namburi%2520and%2520Manish%2520Shrivastava%2520and%2520Maneesh%2520Singh%2520and%2520Bapi%2520S.%2520Raju%2520and%2520Manish%2520Gupta%26entry.1292438233%3D%2520%2520Transformer-based%2520language%2520models%252C%2520though%2520not%2520explicitly%2520trained%2520to%2520mimic%250Abrain%2520recordings%252C%2520have%2520demonstrated%2520surprising%2520alignment%2520with%2520brain%2520activity.%250AProgress%2520in%2520these%2520models-through%2520increased%2520size%252C%2520instruction-tuning%252C%2520and%250Amultimodality-has%2520led%2520to%2520better%2520representational%2520alignment%2520with%2520neural%2520data.%250ARecently%252C%2520a%2520new%2520class%2520of%2520instruction-tuned%2520multimodal%2520LLMs%2520%2528MLLMs%2529%2520have%250Aemerged%252C%2520showing%2520remarkable%2520zero-shot%2520capabilities%2520in%2520open-ended%2520multimodal%250Avision%2520tasks.%2520However%252C%2520it%2520is%2520unknown%2520whether%2520MLLMs%252C%2520when%2520prompted%2520with%2520natural%250Ainstructions%252C%2520lead%2520to%2520better%2520brain%2520alignment%2520and%2520effectively%2520capture%250Ainstruction-specific%2520representations.%2520To%2520address%2520this%252C%2520we%2520first%2520investigate%250Abrain%2520alignment%252C%2520i.e.%252C%2520measuring%2520the%2520degree%2520of%2520predictivity%2520of%2520neural%2520visual%250Aactivity%2520using%2520text%2520output%2520response%2520embeddings%2520from%2520MLLMs%2520as%2520participants%250Aengage%2520in%2520watching%2520natural%2520scenes.%2520Experiments%2520with%252010%2520different%2520instructions%250Ashow%2520that%2520MLLMs%2520exhibit%2520significantly%2520better%2520brain%2520alignment%2520than%2520vision-only%250Amodels%2520and%2520perform%2520comparably%2520to%2520non-instruction-tuned%2520multimodal%2520models%2520like%250ACLIP.%2520We%2520also%2520find%2520that%2520while%2520these%2520MLLMs%2520are%2520effective%2520at%2520generating%250Ahigh-quality%2520responses%2520suitable%2520to%2520the%2520task-specific%2520instructions%252C%2520not%2520all%250Ainstructions%2520are%2520relevant%2520for%2520brain%2520alignment.%2520Further%252C%2520by%2520varying%250Ainstructions%252C%2520we%2520make%2520the%2520MLLMs%2520encode%2520instruction-specific%2520visual%2520concepts%250Arelated%2520to%2520the%2520input%2520image.%2520This%2520analysis%2520shows%2520that%2520MLLMs%2520effectively%2520capture%250Acount-related%2520and%2520recognition-related%2520concepts%252C%2520demonstrating%2520strong%2520alignment%250Awith%2520brain%2520activity.%2520Notably%252C%2520the%2520majority%2520of%2520the%2520explained%2520variance%2520of%2520the%250Abrain%2520encoding%2520models%2520is%2520shared%2520between%2520MLLM%2520embeddings%2520of%2520image%2520captioning%2520and%250Aother%2520instructions.%2520These%2520results%2520suggest%2520that%2520enhancing%2520MLLMs%2527%2520ability%2520to%250Acapture%2520task-specific%2520information%2520could%2520lead%2520to%2520better%2520differentiation%2520between%250Avarious%2520types%2520of%2520instructions%252C%2520and%2520thereby%2520improving%2520their%2520precision%2520in%250Apredicting%2520brain%2520responses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20029v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Correlating%20instruction-tuning%20%28in%20multimodal%20models%29%20with%0A%20%20vision-language%20processing%20%28in%20the%20brain%29&entry.906535625=Subba%20Reddy%20Oota%20and%20Akshett%20Jindal%20and%20Ishani%20Mondal%20and%20Khushbu%20Pahwa%20and%20Satya%20Sai%20Srinath%20Namburi%20and%20Manish%20Shrivastava%20and%20Maneesh%20Singh%20and%20Bapi%20S.%20Raju%20and%20Manish%20Gupta&entry.1292438233=%20%20Transformer-based%20language%20models%2C%20though%20not%20explicitly%20trained%20to%20mimic%0Abrain%20recordings%2C%20have%20demonstrated%20surprising%20alignment%20with%20brain%20activity.%0AProgress%20in%20these%20models-through%20increased%20size%2C%20instruction-tuning%2C%20and%0Amultimodality-has%20led%20to%20better%20representational%20alignment%20with%20neural%20data.%0ARecently%2C%20a%20new%20class%20of%20instruction-tuned%20multimodal%20LLMs%20%28MLLMs%29%20have%0Aemerged%2C%20showing%20remarkable%20zero-shot%20capabilities%20in%20open-ended%20multimodal%0Avision%20tasks.%20However%2C%20it%20is%20unknown%20whether%20MLLMs%2C%20when%20prompted%20with%20natural%0Ainstructions%2C%20lead%20to%20better%20brain%20alignment%20and%20effectively%20capture%0Ainstruction-specific%20representations.%20To%20address%20this%2C%20we%20first%20investigate%0Abrain%20alignment%2C%20i.e.%2C%20measuring%20the%20degree%20of%20predictivity%20of%20neural%20visual%0Aactivity%20using%20text%20output%20response%20embeddings%20from%20MLLMs%20as%20participants%0Aengage%20in%20watching%20natural%20scenes.%20Experiments%20with%2010%20different%20instructions%0Ashow%20that%20MLLMs%20exhibit%20significantly%20better%20brain%20alignment%20than%20vision-only%0Amodels%20and%20perform%20comparably%20to%20non-instruction-tuned%20multimodal%20models%20like%0ACLIP.%20We%20also%20find%20that%20while%20these%20MLLMs%20are%20effective%20at%20generating%0Ahigh-quality%20responses%20suitable%20to%20the%20task-specific%20instructions%2C%20not%20all%0Ainstructions%20are%20relevant%20for%20brain%20alignment.%20Further%2C%20by%20varying%0Ainstructions%2C%20we%20make%20the%20MLLMs%20encode%20instruction-specific%20visual%20concepts%0Arelated%20to%20the%20input%20image.%20This%20analysis%20shows%20that%20MLLMs%20effectively%20capture%0Acount-related%20and%20recognition-related%20concepts%2C%20demonstrating%20strong%20alignment%0Awith%20brain%20activity.%20Notably%2C%20the%20majority%20of%20the%20explained%20variance%20of%20the%0Abrain%20encoding%20models%20is%20shared%20between%20MLLM%20embeddings%20of%20image%20captioning%20and%0Aother%20instructions.%20These%20results%20suggest%20that%20enhancing%20MLLMs%27%20ability%20to%0Acapture%20task-specific%20information%20could%20lead%20to%20better%20differentiation%20between%0Avarious%20types%20of%20instructions%2C%20and%20thereby%20improving%20their%20precision%20in%0Apredicting%20brain%20responses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20029v1&entry.124074799=Read"},
{"title": "TokBench: Evaluating Your Visual Tokenizer before Visual Generation", "author": "Junfeng Wu and Dongliang Luo and Weizhi Zhao and Zhihao Xie and Yuanhao Wang and Junyi Li and Xudong Xie and Yuliang Liu and Xiang Bai", "abstract": "  In this work, we reveal the limitations of visual tokenizers and VAEs in\npreserving fine-grained features, and propose a benchmark to evaluate\nreconstruction performance for two challenging visual contents: text and face.\nVisual tokenizers and VAEs have significantly advanced visual generation and\nmultimodal modeling by providing more efficient compressed or quantized image\nrepresentations. However, while helping production models reduce computational\nburdens, the information loss from image compression fundamentally limits the\nupper bound of visual generation quality. To evaluate this upper bound, we\nfocus on assessing reconstructed text and facial features since they typically:\n1) exist at smaller scales, 2) contain dense and rich textures, 3) are prone to\ncollapse, and 4) are highly sensitive to human vision. We first collect and\ncurate a diverse set of clear text and face images from existing datasets.\nUnlike approaches using VLM models, we employ established OCR and face\nrecognition models for evaluation, ensuring accuracy while maintaining an\nexceptionally lightweight assessment process <span style=\"font-weight: bold;\ncolor: rgb(214, 21, 21);\">requiring just 2GB memory and 4 minutes</span> to\ncomplete. Using our benchmark, we analyze text and face reconstruction quality\nacross various scales for different image tokenizers and VAEs. Our results show\nmodern visual tokenizers still struggle to preserve fine-grained features,\nespecially at smaller scales. We further extend this evaluation framework to\nvideo, conducting comprehensive analysis of video tokenizers. Additionally, we\ndemonstrate that traditional metrics fail to accurately reflect reconstruction\nperformance for faces and text, while our proposed metrics serve as an\neffective complement.\n", "link": "http://arxiv.org/abs/2505.18142v2", "date": "2025-05-26", "relevancy": 2.9125, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6125}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5675}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TokBench%3A%20Evaluating%20Your%20Visual%20Tokenizer%20before%20Visual%20Generation&body=Title%3A%20TokBench%3A%20Evaluating%20Your%20Visual%20Tokenizer%20before%20Visual%20Generation%0AAuthor%3A%20Junfeng%20Wu%20and%20Dongliang%20Luo%20and%20Weizhi%20Zhao%20and%20Zhihao%20Xie%20and%20Yuanhao%20Wang%20and%20Junyi%20Li%20and%20Xudong%20Xie%20and%20Yuliang%20Liu%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20reveal%20the%20limitations%20of%20visual%20tokenizers%20and%20VAEs%20in%0Apreserving%20fine-grained%20features%2C%20and%20propose%20a%20benchmark%20to%20evaluate%0Areconstruction%20performance%20for%20two%20challenging%20visual%20contents%3A%20text%20and%20face.%0AVisual%20tokenizers%20and%20VAEs%20have%20significantly%20advanced%20visual%20generation%20and%0Amultimodal%20modeling%20by%20providing%20more%20efficient%20compressed%20or%20quantized%20image%0Arepresentations.%20However%2C%20while%20helping%20production%20models%20reduce%20computational%0Aburdens%2C%20the%20information%20loss%20from%20image%20compression%20fundamentally%20limits%20the%0Aupper%20bound%20of%20visual%20generation%20quality.%20To%20evaluate%20this%20upper%20bound%2C%20we%0Afocus%20on%20assessing%20reconstructed%20text%20and%20facial%20features%20since%20they%20typically%3A%0A1%29%20exist%20at%20smaller%20scales%2C%202%29%20contain%20dense%20and%20rich%20textures%2C%203%29%20are%20prone%20to%0Acollapse%2C%20and%204%29%20are%20highly%20sensitive%20to%20human%20vision.%20We%20first%20collect%20and%0Acurate%20a%20diverse%20set%20of%20clear%20text%20and%20face%20images%20from%20existing%20datasets.%0AUnlike%20approaches%20using%20VLM%20models%2C%20we%20employ%20established%20OCR%20and%20face%0Arecognition%20models%20for%20evaluation%2C%20ensuring%20accuracy%20while%20maintaining%20an%0Aexceptionally%20lightweight%20assessment%20process%20%3Cspan%20style%3D%22font-weight%3A%20bold%3B%0Acolor%3A%20rgb%28214%2C%2021%2C%2021%29%3B%22%3Erequiring%20just%202GB%20memory%20and%204%20minutes%3C/span%3E%20to%0Acomplete.%20Using%20our%20benchmark%2C%20we%20analyze%20text%20and%20face%20reconstruction%20quality%0Aacross%20various%20scales%20for%20different%20image%20tokenizers%20and%20VAEs.%20Our%20results%20show%0Amodern%20visual%20tokenizers%20still%20struggle%20to%20preserve%20fine-grained%20features%2C%0Aespecially%20at%20smaller%20scales.%20We%20further%20extend%20this%20evaluation%20framework%20to%0Avideo%2C%20conducting%20comprehensive%20analysis%20of%20video%20tokenizers.%20Additionally%2C%20we%0Ademonstrate%20that%20traditional%20metrics%20fail%20to%20accurately%20reflect%20reconstruction%0Aperformance%20for%20faces%20and%20text%2C%20while%20our%20proposed%20metrics%20serve%20as%20an%0Aeffective%20complement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18142v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokBench%253A%2520Evaluating%2520Your%2520Visual%2520Tokenizer%2520before%2520Visual%2520Generation%26entry.906535625%3DJunfeng%2520Wu%2520and%2520Dongliang%2520Luo%2520and%2520Weizhi%2520Zhao%2520and%2520Zhihao%2520Xie%2520and%2520Yuanhao%2520Wang%2520and%2520Junyi%2520Li%2520and%2520Xudong%2520Xie%2520and%2520Yuliang%2520Liu%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520reveal%2520the%2520limitations%2520of%2520visual%2520tokenizers%2520and%2520VAEs%2520in%250Apreserving%2520fine-grained%2520features%252C%2520and%2520propose%2520a%2520benchmark%2520to%2520evaluate%250Areconstruction%2520performance%2520for%2520two%2520challenging%2520visual%2520contents%253A%2520text%2520and%2520face.%250AVisual%2520tokenizers%2520and%2520VAEs%2520have%2520significantly%2520advanced%2520visual%2520generation%2520and%250Amultimodal%2520modeling%2520by%2520providing%2520more%2520efficient%2520compressed%2520or%2520quantized%2520image%250Arepresentations.%2520However%252C%2520while%2520helping%2520production%2520models%2520reduce%2520computational%250Aburdens%252C%2520the%2520information%2520loss%2520from%2520image%2520compression%2520fundamentally%2520limits%2520the%250Aupper%2520bound%2520of%2520visual%2520generation%2520quality.%2520To%2520evaluate%2520this%2520upper%2520bound%252C%2520we%250Afocus%2520on%2520assessing%2520reconstructed%2520text%2520and%2520facial%2520features%2520since%2520they%2520typically%253A%250A1%2529%2520exist%2520at%2520smaller%2520scales%252C%25202%2529%2520contain%2520dense%2520and%2520rich%2520textures%252C%25203%2529%2520are%2520prone%2520to%250Acollapse%252C%2520and%25204%2529%2520are%2520highly%2520sensitive%2520to%2520human%2520vision.%2520We%2520first%2520collect%2520and%250Acurate%2520a%2520diverse%2520set%2520of%2520clear%2520text%2520and%2520face%2520images%2520from%2520existing%2520datasets.%250AUnlike%2520approaches%2520using%2520VLM%2520models%252C%2520we%2520employ%2520established%2520OCR%2520and%2520face%250Arecognition%2520models%2520for%2520evaluation%252C%2520ensuring%2520accuracy%2520while%2520maintaining%2520an%250Aexceptionally%2520lightweight%2520assessment%2520process%2520%253Cspan%2520style%253D%2522font-weight%253A%2520bold%253B%250Acolor%253A%2520rgb%2528214%252C%252021%252C%252021%2529%253B%2522%253Erequiring%2520just%25202GB%2520memory%2520and%25204%2520minutes%253C/span%253E%2520to%250Acomplete.%2520Using%2520our%2520benchmark%252C%2520we%2520analyze%2520text%2520and%2520face%2520reconstruction%2520quality%250Aacross%2520various%2520scales%2520for%2520different%2520image%2520tokenizers%2520and%2520VAEs.%2520Our%2520results%2520show%250Amodern%2520visual%2520tokenizers%2520still%2520struggle%2520to%2520preserve%2520fine-grained%2520features%252C%250Aespecially%2520at%2520smaller%2520scales.%2520We%2520further%2520extend%2520this%2520evaluation%2520framework%2520to%250Avideo%252C%2520conducting%2520comprehensive%2520analysis%2520of%2520video%2520tokenizers.%2520Additionally%252C%2520we%250Ademonstrate%2520that%2520traditional%2520metrics%2520fail%2520to%2520accurately%2520reflect%2520reconstruction%250Aperformance%2520for%2520faces%2520and%2520text%252C%2520while%2520our%2520proposed%2520metrics%2520serve%2520as%2520an%250Aeffective%2520complement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18142v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TokBench%3A%20Evaluating%20Your%20Visual%20Tokenizer%20before%20Visual%20Generation&entry.906535625=Junfeng%20Wu%20and%20Dongliang%20Luo%20and%20Weizhi%20Zhao%20and%20Zhihao%20Xie%20and%20Yuanhao%20Wang%20and%20Junyi%20Li%20and%20Xudong%20Xie%20and%20Yuliang%20Liu%20and%20Xiang%20Bai&entry.1292438233=%20%20In%20this%20work%2C%20we%20reveal%20the%20limitations%20of%20visual%20tokenizers%20and%20VAEs%20in%0Apreserving%20fine-grained%20features%2C%20and%20propose%20a%20benchmark%20to%20evaluate%0Areconstruction%20performance%20for%20two%20challenging%20visual%20contents%3A%20text%20and%20face.%0AVisual%20tokenizers%20and%20VAEs%20have%20significantly%20advanced%20visual%20generation%20and%0Amultimodal%20modeling%20by%20providing%20more%20efficient%20compressed%20or%20quantized%20image%0Arepresentations.%20However%2C%20while%20helping%20production%20models%20reduce%20computational%0Aburdens%2C%20the%20information%20loss%20from%20image%20compression%20fundamentally%20limits%20the%0Aupper%20bound%20of%20visual%20generation%20quality.%20To%20evaluate%20this%20upper%20bound%2C%20we%0Afocus%20on%20assessing%20reconstructed%20text%20and%20facial%20features%20since%20they%20typically%3A%0A1%29%20exist%20at%20smaller%20scales%2C%202%29%20contain%20dense%20and%20rich%20textures%2C%203%29%20are%20prone%20to%0Acollapse%2C%20and%204%29%20are%20highly%20sensitive%20to%20human%20vision.%20We%20first%20collect%20and%0Acurate%20a%20diverse%20set%20of%20clear%20text%20and%20face%20images%20from%20existing%20datasets.%0AUnlike%20approaches%20using%20VLM%20models%2C%20we%20employ%20established%20OCR%20and%20face%0Arecognition%20models%20for%20evaluation%2C%20ensuring%20accuracy%20while%20maintaining%20an%0Aexceptionally%20lightweight%20assessment%20process%20%3Cspan%20style%3D%22font-weight%3A%20bold%3B%0Acolor%3A%20rgb%28214%2C%2021%2C%2021%29%3B%22%3Erequiring%20just%202GB%20memory%20and%204%20minutes%3C/span%3E%20to%0Acomplete.%20Using%20our%20benchmark%2C%20we%20analyze%20text%20and%20face%20reconstruction%20quality%0Aacross%20various%20scales%20for%20different%20image%20tokenizers%20and%20VAEs.%20Our%20results%20show%0Amodern%20visual%20tokenizers%20still%20struggle%20to%20preserve%20fine-grained%20features%2C%0Aespecially%20at%20smaller%20scales.%20We%20further%20extend%20this%20evaluation%20framework%20to%0Avideo%2C%20conducting%20comprehensive%20analysis%20of%20video%20tokenizers.%20Additionally%2C%20we%0Ademonstrate%20that%20traditional%20metrics%20fail%20to%20accurately%20reflect%20reconstruction%0Aperformance%20for%20faces%20and%20text%2C%20while%20our%20proposed%20metrics%20serve%20as%20an%0Aeffective%20complement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18142v2&entry.124074799=Read"},
{"title": "Efficient Multi-modal Long Context Learning for Training-free Adaptation", "author": "Zehong Ma and Shiliang Zhang and Longhui Wei and Qi Tian", "abstract": "  Traditional approaches to adapting multi-modal large language models (MLLMs)\nto new tasks have relied heavily on fine-tuning. This paper introduces\nEfficient Multi-Modal Long Context Learning (EMLoC), a novel training-free\nalternative that embeds demonstration examples directly into the model input.\nEMLoC offers a more efficient, flexible, and scalable solution for task\nadaptation. Because extremely lengthy inputs introduce prohibitive\ncomputational and memory overhead, EMLoC contributes a chunk-wise compression\nmechanism combined with layer-wise adaptive pruning. It condenses long-context\nmultimodal inputs into compact, task-specific memory representations. By\nadaptively pruning tokens at each layer under a Jensen-Shannon divergence\nconstraint, our method achieves a dramatic reduction in inference complexity\nwithout sacrificing performance. This approach is the first to seamlessly\nintegrate compression and pruning techniques for multi-modal long-context\nlearning, offering a scalable and efficient solution for real-world\napplications. Extensive experiments on diverse vision-language benchmarks\ndemonstrate that EMLoC achieves performance on par with or superior to naive\nlong-context approaches. Our results highlight the potential of EMLoC as a\ngroundbreaking framework for efficient and flexible adaptation of multi-modal\nmodels in resource-constrained environments. Codes are publicly available at\nhttps://github.com/Zehong-Ma/EMLoC.\n", "link": "http://arxiv.org/abs/2505.19812v1", "date": "2025-05-26", "relevancy": 2.9089, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6164}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5645}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Multi-modal%20Long%20Context%20Learning%20for%20Training-free%20Adaptation&body=Title%3A%20Efficient%20Multi-modal%20Long%20Context%20Learning%20for%20Training-free%20Adaptation%0AAuthor%3A%20Zehong%20Ma%20and%20Shiliang%20Zhang%20and%20Longhui%20Wei%20and%20Qi%20Tian%0AAbstract%3A%20%20%20Traditional%20approaches%20to%20adapting%20multi-modal%20large%20language%20models%20%28MLLMs%29%0Ato%20new%20tasks%20have%20relied%20heavily%20on%20fine-tuning.%20This%20paper%20introduces%0AEfficient%20Multi-Modal%20Long%20Context%20Learning%20%28EMLoC%29%2C%20a%20novel%20training-free%0Aalternative%20that%20embeds%20demonstration%20examples%20directly%20into%20the%20model%20input.%0AEMLoC%20offers%20a%20more%20efficient%2C%20flexible%2C%20and%20scalable%20solution%20for%20task%0Aadaptation.%20Because%20extremely%20lengthy%20inputs%20introduce%20prohibitive%0Acomputational%20and%20memory%20overhead%2C%20EMLoC%20contributes%20a%20chunk-wise%20compression%0Amechanism%20combined%20with%20layer-wise%20adaptive%20pruning.%20It%20condenses%20long-context%0Amultimodal%20inputs%20into%20compact%2C%20task-specific%20memory%20representations.%20By%0Aadaptively%20pruning%20tokens%20at%20each%20layer%20under%20a%20Jensen-Shannon%20divergence%0Aconstraint%2C%20our%20method%20achieves%20a%20dramatic%20reduction%20in%20inference%20complexity%0Awithout%20sacrificing%20performance.%20This%20approach%20is%20the%20first%20to%20seamlessly%0Aintegrate%20compression%20and%20pruning%20techniques%20for%20multi-modal%20long-context%0Alearning%2C%20offering%20a%20scalable%20and%20efficient%20solution%20for%20real-world%0Aapplications.%20Extensive%20experiments%20on%20diverse%20vision-language%20benchmarks%0Ademonstrate%20that%20EMLoC%20achieves%20performance%20on%20par%20with%20or%20superior%20to%20naive%0Along-context%20approaches.%20Our%20results%20highlight%20the%20potential%20of%20EMLoC%20as%20a%0Agroundbreaking%20framework%20for%20efficient%20and%20flexible%20adaptation%20of%20multi-modal%0Amodels%20in%20resource-constrained%20environments.%20Codes%20are%20publicly%20available%20at%0Ahttps%3A//github.com/Zehong-Ma/EMLoC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Multi-modal%2520Long%2520Context%2520Learning%2520for%2520Training-free%2520Adaptation%26entry.906535625%3DZehong%2520Ma%2520and%2520Shiliang%2520Zhang%2520and%2520Longhui%2520Wei%2520and%2520Qi%2520Tian%26entry.1292438233%3D%2520%2520Traditional%2520approaches%2520to%2520adapting%2520multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%250Ato%2520new%2520tasks%2520have%2520relied%2520heavily%2520on%2520fine-tuning.%2520This%2520paper%2520introduces%250AEfficient%2520Multi-Modal%2520Long%2520Context%2520Learning%2520%2528EMLoC%2529%252C%2520a%2520novel%2520training-free%250Aalternative%2520that%2520embeds%2520demonstration%2520examples%2520directly%2520into%2520the%2520model%2520input.%250AEMLoC%2520offers%2520a%2520more%2520efficient%252C%2520flexible%252C%2520and%2520scalable%2520solution%2520for%2520task%250Aadaptation.%2520Because%2520extremely%2520lengthy%2520inputs%2520introduce%2520prohibitive%250Acomputational%2520and%2520memory%2520overhead%252C%2520EMLoC%2520contributes%2520a%2520chunk-wise%2520compression%250Amechanism%2520combined%2520with%2520layer-wise%2520adaptive%2520pruning.%2520It%2520condenses%2520long-context%250Amultimodal%2520inputs%2520into%2520compact%252C%2520task-specific%2520memory%2520representations.%2520By%250Aadaptively%2520pruning%2520tokens%2520at%2520each%2520layer%2520under%2520a%2520Jensen-Shannon%2520divergence%250Aconstraint%252C%2520our%2520method%2520achieves%2520a%2520dramatic%2520reduction%2520in%2520inference%2520complexity%250Awithout%2520sacrificing%2520performance.%2520This%2520approach%2520is%2520the%2520first%2520to%2520seamlessly%250Aintegrate%2520compression%2520and%2520pruning%2520techniques%2520for%2520multi-modal%2520long-context%250Alearning%252C%2520offering%2520a%2520scalable%2520and%2520efficient%2520solution%2520for%2520real-world%250Aapplications.%2520Extensive%2520experiments%2520on%2520diverse%2520vision-language%2520benchmarks%250Ademonstrate%2520that%2520EMLoC%2520achieves%2520performance%2520on%2520par%2520with%2520or%2520superior%2520to%2520naive%250Along-context%2520approaches.%2520Our%2520results%2520highlight%2520the%2520potential%2520of%2520EMLoC%2520as%2520a%250Agroundbreaking%2520framework%2520for%2520efficient%2520and%2520flexible%2520adaptation%2520of%2520multi-modal%250Amodels%2520in%2520resource-constrained%2520environments.%2520Codes%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/Zehong-Ma/EMLoC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Multi-modal%20Long%20Context%20Learning%20for%20Training-free%20Adaptation&entry.906535625=Zehong%20Ma%20and%20Shiliang%20Zhang%20and%20Longhui%20Wei%20and%20Qi%20Tian&entry.1292438233=%20%20Traditional%20approaches%20to%20adapting%20multi-modal%20large%20language%20models%20%28MLLMs%29%0Ato%20new%20tasks%20have%20relied%20heavily%20on%20fine-tuning.%20This%20paper%20introduces%0AEfficient%20Multi-Modal%20Long%20Context%20Learning%20%28EMLoC%29%2C%20a%20novel%20training-free%0Aalternative%20that%20embeds%20demonstration%20examples%20directly%20into%20the%20model%20input.%0AEMLoC%20offers%20a%20more%20efficient%2C%20flexible%2C%20and%20scalable%20solution%20for%20task%0Aadaptation.%20Because%20extremely%20lengthy%20inputs%20introduce%20prohibitive%0Acomputational%20and%20memory%20overhead%2C%20EMLoC%20contributes%20a%20chunk-wise%20compression%0Amechanism%20combined%20with%20layer-wise%20adaptive%20pruning.%20It%20condenses%20long-context%0Amultimodal%20inputs%20into%20compact%2C%20task-specific%20memory%20representations.%20By%0Aadaptively%20pruning%20tokens%20at%20each%20layer%20under%20a%20Jensen-Shannon%20divergence%0Aconstraint%2C%20our%20method%20achieves%20a%20dramatic%20reduction%20in%20inference%20complexity%0Awithout%20sacrificing%20performance.%20This%20approach%20is%20the%20first%20to%20seamlessly%0Aintegrate%20compression%20and%20pruning%20techniques%20for%20multi-modal%20long-context%0Alearning%2C%20offering%20a%20scalable%20and%20efficient%20solution%20for%20real-world%0Aapplications.%20Extensive%20experiments%20on%20diverse%20vision-language%20benchmarks%0Ademonstrate%20that%20EMLoC%20achieves%20performance%20on%20par%20with%20or%20superior%20to%20naive%0Along-context%20approaches.%20Our%20results%20highlight%20the%20potential%20of%20EMLoC%20as%20a%0Agroundbreaking%20framework%20for%20efficient%20and%20flexible%20adaptation%20of%20multi-modal%0Amodels%20in%20resource-constrained%20environments.%20Codes%20are%20publicly%20available%20at%0Ahttps%3A//github.com/Zehong-Ma/EMLoC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19812v1&entry.124074799=Read"},
{"title": "Unifying Multimodal Large Language Model Capabilities and Modalities via\n  Model Merging", "author": "Yongxian Wei and Runxi Cheng and Weike Jin and Enneng Yang and Li Shen and Lu Hou and Sinan Du and Chun Yuan and Xiaochun Cao and Dacheng Tao", "abstract": "  While foundation models update slowly due to resource-intensive training\nrequirements, domain-specific models evolve between updates. Model merging aims\nto combine multiple expert models into a single, more capable model, thereby\nreducing storage and serving costs while supporting decentralized model\ndevelopment. Despite its potential, previous studies have primarily focused on\nmerging visual classification models or Large Language Models (LLMs) for code\nand math tasks. Multimodal Large Language Models (MLLMs), which extend the\ncapabilities of LLMs through large-scale multimodal training, have gained\ntraction. However, there lacks a benchmark for model merging research that\nclearly divides the tasks for MLLM training and evaluation. In this paper, (i)\nwe introduce the model merging benchmark for MLLMs, which includes multiple\ntasks such as VQA, Geometry, Chart, OCR, and Grounding, providing both LoRA and\nfull fine-tuning models. Moreover, we explore how model merging can combine\ndifferent modalities (e.g., vision-language, audio-language, and video-language\nmodels), moving toward the Omni-language model. (ii) We implement 10 model\nmerging algorithms on the benchmark. Furthermore, we propose a novel method\nthat removes noise from task vectors and robustly optimizes the merged vector\nbased on a loss defined over task vector interactions, achieving an average\nperformance gain of 2.48%. (iii) We find that model merging offers a promising\nway for building improved MLLMs without requiring data training. Our results\nalso demonstrate that the complementarity among multiple modalities outperforms\nindividual modalities.\n", "link": "http://arxiv.org/abs/2505.19892v1", "date": "2025-05-26", "relevancy": 2.9079, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5817}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5815}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unifying%20Multimodal%20Large%20Language%20Model%20Capabilities%20and%20Modalities%20via%0A%20%20Model%20Merging&body=Title%3A%20Unifying%20Multimodal%20Large%20Language%20Model%20Capabilities%20and%20Modalities%20via%0A%20%20Model%20Merging%0AAuthor%3A%20Yongxian%20Wei%20and%20Runxi%20Cheng%20and%20Weike%20Jin%20and%20Enneng%20Yang%20and%20Li%20Shen%20and%20Lu%20Hou%20and%20Sinan%20Du%20and%20Chun%20Yuan%20and%20Xiaochun%20Cao%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20While%20foundation%20models%20update%20slowly%20due%20to%20resource-intensive%20training%0Arequirements%2C%20domain-specific%20models%20evolve%20between%20updates.%20Model%20merging%20aims%0Ato%20combine%20multiple%20expert%20models%20into%20a%20single%2C%20more%20capable%20model%2C%20thereby%0Areducing%20storage%20and%20serving%20costs%20while%20supporting%20decentralized%20model%0Adevelopment.%20Despite%20its%20potential%2C%20previous%20studies%20have%20primarily%20focused%20on%0Amerging%20visual%20classification%20models%20or%20Large%20Language%20Models%20%28LLMs%29%20for%20code%0Aand%20math%20tasks.%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20which%20extend%20the%0Acapabilities%20of%20LLMs%20through%20large-scale%20multimodal%20training%2C%20have%20gained%0Atraction.%20However%2C%20there%20lacks%20a%20benchmark%20for%20model%20merging%20research%20that%0Aclearly%20divides%20the%20tasks%20for%20MLLM%20training%20and%20evaluation.%20In%20this%20paper%2C%20%28i%29%0Awe%20introduce%20the%20model%20merging%20benchmark%20for%20MLLMs%2C%20which%20includes%20multiple%0Atasks%20such%20as%20VQA%2C%20Geometry%2C%20Chart%2C%20OCR%2C%20and%20Grounding%2C%20providing%20both%20LoRA%20and%0Afull%20fine-tuning%20models.%20Moreover%2C%20we%20explore%20how%20model%20merging%20can%20combine%0Adifferent%20modalities%20%28e.g.%2C%20vision-language%2C%20audio-language%2C%20and%20video-language%0Amodels%29%2C%20moving%20toward%20the%20Omni-language%20model.%20%28ii%29%20We%20implement%2010%20model%0Amerging%20algorithms%20on%20the%20benchmark.%20Furthermore%2C%20we%20propose%20a%20novel%20method%0Athat%20removes%20noise%20from%20task%20vectors%20and%20robustly%20optimizes%20the%20merged%20vector%0Abased%20on%20a%20loss%20defined%20over%20task%20vector%20interactions%2C%20achieving%20an%20average%0Aperformance%20gain%20of%202.48%25.%20%28iii%29%20We%20find%20that%20model%20merging%20offers%20a%20promising%0Away%20for%20building%20improved%20MLLMs%20without%20requiring%20data%20training.%20Our%20results%0Aalso%20demonstrate%20that%20the%20complementarity%20among%20multiple%20modalities%20outperforms%0Aindividual%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnifying%2520Multimodal%2520Large%2520Language%2520Model%2520Capabilities%2520and%2520Modalities%2520via%250A%2520%2520Model%2520Merging%26entry.906535625%3DYongxian%2520Wei%2520and%2520Runxi%2520Cheng%2520and%2520Weike%2520Jin%2520and%2520Enneng%2520Yang%2520and%2520Li%2520Shen%2520and%2520Lu%2520Hou%2520and%2520Sinan%2520Du%2520and%2520Chun%2520Yuan%2520and%2520Xiaochun%2520Cao%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520While%2520foundation%2520models%2520update%2520slowly%2520due%2520to%2520resource-intensive%2520training%250Arequirements%252C%2520domain-specific%2520models%2520evolve%2520between%2520updates.%2520Model%2520merging%2520aims%250Ato%2520combine%2520multiple%2520expert%2520models%2520into%2520a%2520single%252C%2520more%2520capable%2520model%252C%2520thereby%250Areducing%2520storage%2520and%2520serving%2520costs%2520while%2520supporting%2520decentralized%2520model%250Adevelopment.%2520Despite%2520its%2520potential%252C%2520previous%2520studies%2520have%2520primarily%2520focused%2520on%250Amerging%2520visual%2520classification%2520models%2520or%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520code%250Aand%2520math%2520tasks.%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520which%2520extend%2520the%250Acapabilities%2520of%2520LLMs%2520through%2520large-scale%2520multimodal%2520training%252C%2520have%2520gained%250Atraction.%2520However%252C%2520there%2520lacks%2520a%2520benchmark%2520for%2520model%2520merging%2520research%2520that%250Aclearly%2520divides%2520the%2520tasks%2520for%2520MLLM%2520training%2520and%2520evaluation.%2520In%2520this%2520paper%252C%2520%2528i%2529%250Awe%2520introduce%2520the%2520model%2520merging%2520benchmark%2520for%2520MLLMs%252C%2520which%2520includes%2520multiple%250Atasks%2520such%2520as%2520VQA%252C%2520Geometry%252C%2520Chart%252C%2520OCR%252C%2520and%2520Grounding%252C%2520providing%2520both%2520LoRA%2520and%250Afull%2520fine-tuning%2520models.%2520Moreover%252C%2520we%2520explore%2520how%2520model%2520merging%2520can%2520combine%250Adifferent%2520modalities%2520%2528e.g.%252C%2520vision-language%252C%2520audio-language%252C%2520and%2520video-language%250Amodels%2529%252C%2520moving%2520toward%2520the%2520Omni-language%2520model.%2520%2528ii%2529%2520We%2520implement%252010%2520model%250Amerging%2520algorithms%2520on%2520the%2520benchmark.%2520Furthermore%252C%2520we%2520propose%2520a%2520novel%2520method%250Athat%2520removes%2520noise%2520from%2520task%2520vectors%2520and%2520robustly%2520optimizes%2520the%2520merged%2520vector%250Abased%2520on%2520a%2520loss%2520defined%2520over%2520task%2520vector%2520interactions%252C%2520achieving%2520an%2520average%250Aperformance%2520gain%2520of%25202.48%2525.%2520%2528iii%2529%2520We%2520find%2520that%2520model%2520merging%2520offers%2520a%2520promising%250Away%2520for%2520building%2520improved%2520MLLMs%2520without%2520requiring%2520data%2520training.%2520Our%2520results%250Aalso%2520demonstrate%2520that%2520the%2520complementarity%2520among%2520multiple%2520modalities%2520outperforms%250Aindividual%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unifying%20Multimodal%20Large%20Language%20Model%20Capabilities%20and%20Modalities%20via%0A%20%20Model%20Merging&entry.906535625=Yongxian%20Wei%20and%20Runxi%20Cheng%20and%20Weike%20Jin%20and%20Enneng%20Yang%20and%20Li%20Shen%20and%20Lu%20Hou%20and%20Sinan%20Du%20and%20Chun%20Yuan%20and%20Xiaochun%20Cao%20and%20Dacheng%20Tao&entry.1292438233=%20%20While%20foundation%20models%20update%20slowly%20due%20to%20resource-intensive%20training%0Arequirements%2C%20domain-specific%20models%20evolve%20between%20updates.%20Model%20merging%20aims%0Ato%20combine%20multiple%20expert%20models%20into%20a%20single%2C%20more%20capable%20model%2C%20thereby%0Areducing%20storage%20and%20serving%20costs%20while%20supporting%20decentralized%20model%0Adevelopment.%20Despite%20its%20potential%2C%20previous%20studies%20have%20primarily%20focused%20on%0Amerging%20visual%20classification%20models%20or%20Large%20Language%20Models%20%28LLMs%29%20for%20code%0Aand%20math%20tasks.%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20which%20extend%20the%0Acapabilities%20of%20LLMs%20through%20large-scale%20multimodal%20training%2C%20have%20gained%0Atraction.%20However%2C%20there%20lacks%20a%20benchmark%20for%20model%20merging%20research%20that%0Aclearly%20divides%20the%20tasks%20for%20MLLM%20training%20and%20evaluation.%20In%20this%20paper%2C%20%28i%29%0Awe%20introduce%20the%20model%20merging%20benchmark%20for%20MLLMs%2C%20which%20includes%20multiple%0Atasks%20such%20as%20VQA%2C%20Geometry%2C%20Chart%2C%20OCR%2C%20and%20Grounding%2C%20providing%20both%20LoRA%20and%0Afull%20fine-tuning%20models.%20Moreover%2C%20we%20explore%20how%20model%20merging%20can%20combine%0Adifferent%20modalities%20%28e.g.%2C%20vision-language%2C%20audio-language%2C%20and%20video-language%0Amodels%29%2C%20moving%20toward%20the%20Omni-language%20model.%20%28ii%29%20We%20implement%2010%20model%0Amerging%20algorithms%20on%20the%20benchmark.%20Furthermore%2C%20we%20propose%20a%20novel%20method%0Athat%20removes%20noise%20from%20task%20vectors%20and%20robustly%20optimizes%20the%20merged%20vector%0Abased%20on%20a%20loss%20defined%20over%20task%20vector%20interactions%2C%20achieving%20an%20average%0Aperformance%20gain%20of%202.48%25.%20%28iii%29%20We%20find%20that%20model%20merging%20offers%20a%20promising%0Away%20for%20building%20improved%20MLLMs%20without%20requiring%20data%20training.%20Our%20results%0Aalso%20demonstrate%20that%20the%20complementarity%20among%20multiple%20modalities%20outperforms%0Aindividual%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19892v1&entry.124074799=Read"},
{"title": "Decomposing Complex Visual Comprehension into Atomic Visual Skills for\n  Vision Language Models", "author": "Hyunsik Chae and Seungwoo Yoon and Jaden Park and Chloe Yewon Chun and Yongin Cho and Mu Cai and Yong Jae Lee and Ernest K. Ryu", "abstract": "  Recent Vision-Language Models (VLMs) have demonstrated impressive multimodal\ncomprehension and reasoning capabilities, yet they often struggle with\ntrivially simple visual tasks. In this work, we focus on the domain of basic 2D\nEuclidean geometry and systematically categorize the fundamental, indivisible\nvisual perception skills, which we refer to as atomic visual skills. We then\nintroduce the Atomic Visual Skills Dataset (AVSD) for evaluating VLMs on the\natomic visual skills. Using AVSD, we benchmark state-of-the-art VLMs and find\nthat they struggle with these tasks, despite being trivial for adult humans.\nOur findings highlight the need for purpose-built datasets to train and\nevaluate VLMs on atomic, rather than composite, visual perception tasks.\n", "link": "http://arxiv.org/abs/2505.20021v1", "date": "2025-05-26", "relevancy": 2.8781, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6169}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6169}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decomposing%20Complex%20Visual%20Comprehension%20into%20Atomic%20Visual%20Skills%20for%0A%20%20Vision%20Language%20Models&body=Title%3A%20Decomposing%20Complex%20Visual%20Comprehension%20into%20Atomic%20Visual%20Skills%20for%0A%20%20Vision%20Language%20Models%0AAuthor%3A%20Hyunsik%20Chae%20and%20Seungwoo%20Yoon%20and%20Jaden%20Park%20and%20Chloe%20Yewon%20Chun%20and%20Yongin%20Cho%20and%20Mu%20Cai%20and%20Yong%20Jae%20Lee%20and%20Ernest%20K.%20Ryu%0AAbstract%3A%20%20%20Recent%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20impressive%20multimodal%0Acomprehension%20and%20reasoning%20capabilities%2C%20yet%20they%20often%20struggle%20with%0Atrivially%20simple%20visual%20tasks.%20In%20this%20work%2C%20we%20focus%20on%20the%20domain%20of%20basic%202D%0AEuclidean%20geometry%20and%20systematically%20categorize%20the%20fundamental%2C%20indivisible%0Avisual%20perception%20skills%2C%20which%20we%20refer%20to%20as%20atomic%20visual%20skills.%20We%20then%0Aintroduce%20the%20Atomic%20Visual%20Skills%20Dataset%20%28AVSD%29%20for%20evaluating%20VLMs%20on%20the%0Aatomic%20visual%20skills.%20Using%20AVSD%2C%20we%20benchmark%20state-of-the-art%20VLMs%20and%20find%0Athat%20they%20struggle%20with%20these%20tasks%2C%20despite%20being%20trivial%20for%20adult%20humans.%0AOur%20findings%20highlight%20the%20need%20for%20purpose-built%20datasets%20to%20train%20and%0Aevaluate%20VLMs%20on%20atomic%2C%20rather%20than%20composite%2C%20visual%20perception%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecomposing%2520Complex%2520Visual%2520Comprehension%2520into%2520Atomic%2520Visual%2520Skills%2520for%250A%2520%2520Vision%2520Language%2520Models%26entry.906535625%3DHyunsik%2520Chae%2520and%2520Seungwoo%2520Yoon%2520and%2520Jaden%2520Park%2520and%2520Chloe%2520Yewon%2520Chun%2520and%2520Yongin%2520Cho%2520and%2520Mu%2520Cai%2520and%2520Yong%2520Jae%2520Lee%2520and%2520Ernest%2520K.%2520Ryu%26entry.1292438233%3D%2520%2520Recent%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520demonstrated%2520impressive%2520multimodal%250Acomprehension%2520and%2520reasoning%2520capabilities%252C%2520yet%2520they%2520often%2520struggle%2520with%250Atrivially%2520simple%2520visual%2520tasks.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520the%2520domain%2520of%2520basic%25202D%250AEuclidean%2520geometry%2520and%2520systematically%2520categorize%2520the%2520fundamental%252C%2520indivisible%250Avisual%2520perception%2520skills%252C%2520which%2520we%2520refer%2520to%2520as%2520atomic%2520visual%2520skills.%2520We%2520then%250Aintroduce%2520the%2520Atomic%2520Visual%2520Skills%2520Dataset%2520%2528AVSD%2529%2520for%2520evaluating%2520VLMs%2520on%2520the%250Aatomic%2520visual%2520skills.%2520Using%2520AVSD%252C%2520we%2520benchmark%2520state-of-the-art%2520VLMs%2520and%2520find%250Athat%2520they%2520struggle%2520with%2520these%2520tasks%252C%2520despite%2520being%2520trivial%2520for%2520adult%2520humans.%250AOur%2520findings%2520highlight%2520the%2520need%2520for%2520purpose-built%2520datasets%2520to%2520train%2520and%250Aevaluate%2520VLMs%2520on%2520atomic%252C%2520rather%2520than%2520composite%252C%2520visual%2520perception%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decomposing%20Complex%20Visual%20Comprehension%20into%20Atomic%20Visual%20Skills%20for%0A%20%20Vision%20Language%20Models&entry.906535625=Hyunsik%20Chae%20and%20Seungwoo%20Yoon%20and%20Jaden%20Park%20and%20Chloe%20Yewon%20Chun%20and%20Yongin%20Cho%20and%20Mu%20Cai%20and%20Yong%20Jae%20Lee%20and%20Ernest%20K.%20Ryu&entry.1292438233=%20%20Recent%20Vision-Language%20Models%20%28VLMs%29%20have%20demonstrated%20impressive%20multimodal%0Acomprehension%20and%20reasoning%20capabilities%2C%20yet%20they%20often%20struggle%20with%0Atrivially%20simple%20visual%20tasks.%20In%20this%20work%2C%20we%20focus%20on%20the%20domain%20of%20basic%202D%0AEuclidean%20geometry%20and%20systematically%20categorize%20the%20fundamental%2C%20indivisible%0Avisual%20perception%20skills%2C%20which%20we%20refer%20to%20as%20atomic%20visual%20skills.%20We%20then%0Aintroduce%20the%20Atomic%20Visual%20Skills%20Dataset%20%28AVSD%29%20for%20evaluating%20VLMs%20on%20the%0Aatomic%20visual%20skills.%20Using%20AVSD%2C%20we%20benchmark%20state-of-the-art%20VLMs%20and%20find%0Athat%20they%20struggle%20with%20these%20tasks%2C%20despite%20being%20trivial%20for%20adult%20humans.%0AOur%20findings%20highlight%20the%20need%20for%20purpose-built%20datasets%20to%20train%20and%0Aevaluate%20VLMs%20on%20atomic%2C%20rather%20than%20composite%2C%20visual%20perception%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20021v1&entry.124074799=Read"},
{"title": "Weather-Magician: Reconstruction and Rendering Framework for 4D Weather\n  Synthesis In Real Time", "author": "Chen Sang and Yeqiang Qian and Jiale Zhang and Chunxiang Wang and Ming Yang", "abstract": "  For tasks such as urban digital twins, VR/AR/game scene design, or creating\nsynthetic films, the traditional industrial approach often involves manually\nmodeling scenes and using various rendering engines to complete the rendering\nprocess. This approach typically requires high labor costs and hardware\ndemands, and can result in poor quality when replicating complex real-world\nscenes. A more efficient approach is to use data from captured real-world\nscenes, then apply reconstruction and rendering algorithms to quickly recreate\nthe authentic scene. However, current algorithms are unable to effectively\nreconstruct and render real-world weather effects. To address this, we propose\na framework based on gaussian splatting, that can reconstruct real scenes and\nrender them under synthesized 4D weather effects. Our work can simulate various\ncommon weather effects by applying Gaussians modeling and rendering techniques.\nIt supports continuous dynamic weather changes and can easily control the\ndetails of the effects. Additionally, our work has low hardware requirements\nand achieves real-time rendering performance. The result demos can be accessed\non our project homepage: weathermagician.github.io\n", "link": "http://arxiv.org/abs/2505.19919v1", "date": "2025-05-26", "relevancy": 2.8758, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5772}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5772}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weather-Magician%3A%20Reconstruction%20and%20Rendering%20Framework%20for%204D%20Weather%0A%20%20Synthesis%20In%20Real%20Time&body=Title%3A%20Weather-Magician%3A%20Reconstruction%20and%20Rendering%20Framework%20for%204D%20Weather%0A%20%20Synthesis%20In%20Real%20Time%0AAuthor%3A%20Chen%20Sang%20and%20Yeqiang%20Qian%20and%20Jiale%20Zhang%20and%20Chunxiang%20Wang%20and%20Ming%20Yang%0AAbstract%3A%20%20%20For%20tasks%20such%20as%20urban%20digital%20twins%2C%20VR/AR/game%20scene%20design%2C%20or%20creating%0Asynthetic%20films%2C%20the%20traditional%20industrial%20approach%20often%20involves%20manually%0Amodeling%20scenes%20and%20using%20various%20rendering%20engines%20to%20complete%20the%20rendering%0Aprocess.%20This%20approach%20typically%20requires%20high%20labor%20costs%20and%20hardware%0Ademands%2C%20and%20can%20result%20in%20poor%20quality%20when%20replicating%20complex%20real-world%0Ascenes.%20A%20more%20efficient%20approach%20is%20to%20use%20data%20from%20captured%20real-world%0Ascenes%2C%20then%20apply%20reconstruction%20and%20rendering%20algorithms%20to%20quickly%20recreate%0Athe%20authentic%20scene.%20However%2C%20current%20algorithms%20are%20unable%20to%20effectively%0Areconstruct%20and%20render%20real-world%20weather%20effects.%20To%20address%20this%2C%20we%20propose%0Aa%20framework%20based%20on%20gaussian%20splatting%2C%20that%20can%20reconstruct%20real%20scenes%20and%0Arender%20them%20under%20synthesized%204D%20weather%20effects.%20Our%20work%20can%20simulate%20various%0Acommon%20weather%20effects%20by%20applying%20Gaussians%20modeling%20and%20rendering%20techniques.%0AIt%20supports%20continuous%20dynamic%20weather%20changes%20and%20can%20easily%20control%20the%0Adetails%20of%20the%20effects.%20Additionally%2C%20our%20work%20has%20low%20hardware%20requirements%0Aand%20achieves%20real-time%20rendering%20performance.%20The%20result%20demos%20can%20be%20accessed%0Aon%20our%20project%20homepage%3A%20weathermagician.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeather-Magician%253A%2520Reconstruction%2520and%2520Rendering%2520Framework%2520for%25204D%2520Weather%250A%2520%2520Synthesis%2520In%2520Real%2520Time%26entry.906535625%3DChen%2520Sang%2520and%2520Yeqiang%2520Qian%2520and%2520Jiale%2520Zhang%2520and%2520Chunxiang%2520Wang%2520and%2520Ming%2520Yang%26entry.1292438233%3D%2520%2520For%2520tasks%2520such%2520as%2520urban%2520digital%2520twins%252C%2520VR/AR/game%2520scene%2520design%252C%2520or%2520creating%250Asynthetic%2520films%252C%2520the%2520traditional%2520industrial%2520approach%2520often%2520involves%2520manually%250Amodeling%2520scenes%2520and%2520using%2520various%2520rendering%2520engines%2520to%2520complete%2520the%2520rendering%250Aprocess.%2520This%2520approach%2520typically%2520requires%2520high%2520labor%2520costs%2520and%2520hardware%250Ademands%252C%2520and%2520can%2520result%2520in%2520poor%2520quality%2520when%2520replicating%2520complex%2520real-world%250Ascenes.%2520A%2520more%2520efficient%2520approach%2520is%2520to%2520use%2520data%2520from%2520captured%2520real-world%250Ascenes%252C%2520then%2520apply%2520reconstruction%2520and%2520rendering%2520algorithms%2520to%2520quickly%2520recreate%250Athe%2520authentic%2520scene.%2520However%252C%2520current%2520algorithms%2520are%2520unable%2520to%2520effectively%250Areconstruct%2520and%2520render%2520real-world%2520weather%2520effects.%2520To%2520address%2520this%252C%2520we%2520propose%250Aa%2520framework%2520based%2520on%2520gaussian%2520splatting%252C%2520that%2520can%2520reconstruct%2520real%2520scenes%2520and%250Arender%2520them%2520under%2520synthesized%25204D%2520weather%2520effects.%2520Our%2520work%2520can%2520simulate%2520various%250Acommon%2520weather%2520effects%2520by%2520applying%2520Gaussians%2520modeling%2520and%2520rendering%2520techniques.%250AIt%2520supports%2520continuous%2520dynamic%2520weather%2520changes%2520and%2520can%2520easily%2520control%2520the%250Adetails%2520of%2520the%2520effects.%2520Additionally%252C%2520our%2520work%2520has%2520low%2520hardware%2520requirements%250Aand%2520achieves%2520real-time%2520rendering%2520performance.%2520The%2520result%2520demos%2520can%2520be%2520accessed%250Aon%2520our%2520project%2520homepage%253A%2520weathermagician.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weather-Magician%3A%20Reconstruction%20and%20Rendering%20Framework%20for%204D%20Weather%0A%20%20Synthesis%20In%20Real%20Time&entry.906535625=Chen%20Sang%20and%20Yeqiang%20Qian%20and%20Jiale%20Zhang%20and%20Chunxiang%20Wang%20and%20Ming%20Yang&entry.1292438233=%20%20For%20tasks%20such%20as%20urban%20digital%20twins%2C%20VR/AR/game%20scene%20design%2C%20or%20creating%0Asynthetic%20films%2C%20the%20traditional%20industrial%20approach%20often%20involves%20manually%0Amodeling%20scenes%20and%20using%20various%20rendering%20engines%20to%20complete%20the%20rendering%0Aprocess.%20This%20approach%20typically%20requires%20high%20labor%20costs%20and%20hardware%0Ademands%2C%20and%20can%20result%20in%20poor%20quality%20when%20replicating%20complex%20real-world%0Ascenes.%20A%20more%20efficient%20approach%20is%20to%20use%20data%20from%20captured%20real-world%0Ascenes%2C%20then%20apply%20reconstruction%20and%20rendering%20algorithms%20to%20quickly%20recreate%0Athe%20authentic%20scene.%20However%2C%20current%20algorithms%20are%20unable%20to%20effectively%0Areconstruct%20and%20render%20real-world%20weather%20effects.%20To%20address%20this%2C%20we%20propose%0Aa%20framework%20based%20on%20gaussian%20splatting%2C%20that%20can%20reconstruct%20real%20scenes%20and%0Arender%20them%20under%20synthesized%204D%20weather%20effects.%20Our%20work%20can%20simulate%20various%0Acommon%20weather%20effects%20by%20applying%20Gaussians%20modeling%20and%20rendering%20techniques.%0AIt%20supports%20continuous%20dynamic%20weather%20changes%20and%20can%20easily%20control%20the%0Adetails%20of%20the%20effects.%20Additionally%2C%20our%20work%20has%20low%20hardware%20requirements%0Aand%20achieves%20real-time%20rendering%20performance.%20The%20result%20demos%20can%20be%20accessed%0Aon%20our%20project%20homepage%3A%20weathermagician.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19919v1&entry.124074799=Read"},
{"title": "STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs", "author": "Zongzhao Li and Zongyang Ma and Mingze Li and Songyou Li and Yu Rong and Tingyang Xu and Ziqi Zhang and Deli Zhao and Wenbing Huang", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities across diverse tasks, yet they lag significantly behind humans in\nspatial reasoning. We investigate this gap through Transformation-Driven Visual\nReasoning (TVR), a challenging task requiring identification of object\ntransformations across images under varying viewpoints. While traditional\nSupervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in\ncross-view settings, sparse-reward Reinforcement Learning (RL) suffers from\ninefficient exploration and slow convergence. To address these limitations, we\npropose STAR-R1, a novel framework that integrates a single-stage RL paradigm\nwith a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1\nrewards partial correctness while penalizing excessive enumeration and passive\ninaction, enabling efficient exploration and precise reasoning. Comprehensive\nevaluations demonstrate that STAR-R1 achieves state-of-the-art performance\nacross all 11 metrics, outperforming SFT by 23% in cross-view scenarios.\nFurther analysis reveals STAR-R1's anthropomorphic behavior and highlights its\nunique ability to compare all objects for improving spatial reasoning. Our work\nprovides critical insights in advancing the research of MLLMs and reasoning\nmodels. The codes, model weights, and data will be publicly available at\nhttps://github.com/zongzhao23/STAR-R1.\n", "link": "http://arxiv.org/abs/2505.15804v2", "date": "2025-05-26", "relevancy": 2.8751, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5909}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5671}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STAR-R1%3A%20Spatial%20TrAnsformation%20Reasoning%20by%20Reinforcing%20Multimodal%20LLMs&body=Title%3A%20STAR-R1%3A%20Spatial%20TrAnsformation%20Reasoning%20by%20Reinforcing%20Multimodal%20LLMs%0AAuthor%3A%20Zongzhao%20Li%20and%20Zongyang%20Ma%20and%20Mingze%20Li%20and%20Songyou%20Li%20and%20Yu%20Rong%20and%20Tingyang%20Xu%20and%20Ziqi%20Zhang%20and%20Deli%20Zhao%20and%20Wenbing%20Huang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20across%20diverse%20tasks%2C%20yet%20they%20lag%20significantly%20behind%20humans%20in%0Aspatial%20reasoning.%20We%20investigate%20this%20gap%20through%20Transformation-Driven%20Visual%0AReasoning%20%28TVR%29%2C%20a%20challenging%20task%20requiring%20identification%20of%20object%0Atransformations%20across%20images%20under%20varying%20viewpoints.%20While%20traditional%0ASupervised%20Fine-Tuning%20%28SFT%29%20fails%20to%20generate%20coherent%20reasoning%20paths%20in%0Across-view%20settings%2C%20sparse-reward%20Reinforcement%20Learning%20%28RL%29%20suffers%20from%0Ainefficient%20exploration%20and%20slow%20convergence.%20To%20address%20these%20limitations%2C%20we%0Apropose%20STAR-R1%2C%20a%20novel%20framework%20that%20integrates%20a%20single-stage%20RL%20paradigm%0Awith%20a%20fine-grained%20reward%20mechanism%20tailored%20for%20TVR.%20Specifically%2C%20STAR-R1%0Arewards%20partial%20correctness%20while%20penalizing%20excessive%20enumeration%20and%20passive%0Ainaction%2C%20enabling%20efficient%20exploration%20and%20precise%20reasoning.%20Comprehensive%0Aevaluations%20demonstrate%20that%20STAR-R1%20achieves%20state-of-the-art%20performance%0Aacross%20all%2011%20metrics%2C%20outperforming%20SFT%20by%2023%25%20in%20cross-view%20scenarios.%0AFurther%20analysis%20reveals%20STAR-R1%27s%20anthropomorphic%20behavior%20and%20highlights%20its%0Aunique%20ability%20to%20compare%20all%20objects%20for%20improving%20spatial%20reasoning.%20Our%20work%0Aprovides%20critical%20insights%20in%20advancing%20the%20research%20of%20MLLMs%20and%20reasoning%0Amodels.%20The%20codes%2C%20model%20weights%2C%20and%20data%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/zongzhao23/STAR-R1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15804v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTAR-R1%253A%2520Spatial%2520TrAnsformation%2520Reasoning%2520by%2520Reinforcing%2520Multimodal%2520LLMs%26entry.906535625%3DZongzhao%2520Li%2520and%2520Zongyang%2520Ma%2520and%2520Mingze%2520Li%2520and%2520Songyou%2520Li%2520and%2520Yu%2520Rong%2520and%2520Tingyang%2520Xu%2520and%2520Ziqi%2520Zhang%2520and%2520Deli%2520Zhao%2520and%2520Wenbing%2520Huang%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520remarkable%250Acapabilities%2520across%2520diverse%2520tasks%252C%2520yet%2520they%2520lag%2520significantly%2520behind%2520humans%2520in%250Aspatial%2520reasoning.%2520We%2520investigate%2520this%2520gap%2520through%2520Transformation-Driven%2520Visual%250AReasoning%2520%2528TVR%2529%252C%2520a%2520challenging%2520task%2520requiring%2520identification%2520of%2520object%250Atransformations%2520across%2520images%2520under%2520varying%2520viewpoints.%2520While%2520traditional%250ASupervised%2520Fine-Tuning%2520%2528SFT%2529%2520fails%2520to%2520generate%2520coherent%2520reasoning%2520paths%2520in%250Across-view%2520settings%252C%2520sparse-reward%2520Reinforcement%2520Learning%2520%2528RL%2529%2520suffers%2520from%250Ainefficient%2520exploration%2520and%2520slow%2520convergence.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520STAR-R1%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520a%2520single-stage%2520RL%2520paradigm%250Awith%2520a%2520fine-grained%2520reward%2520mechanism%2520tailored%2520for%2520TVR.%2520Specifically%252C%2520STAR-R1%250Arewards%2520partial%2520correctness%2520while%2520penalizing%2520excessive%2520enumeration%2520and%2520passive%250Ainaction%252C%2520enabling%2520efficient%2520exploration%2520and%2520precise%2520reasoning.%2520Comprehensive%250Aevaluations%2520demonstrate%2520that%2520STAR-R1%2520achieves%2520state-of-the-art%2520performance%250Aacross%2520all%252011%2520metrics%252C%2520outperforming%2520SFT%2520by%252023%2525%2520in%2520cross-view%2520scenarios.%250AFurther%2520analysis%2520reveals%2520STAR-R1%2527s%2520anthropomorphic%2520behavior%2520and%2520highlights%2520its%250Aunique%2520ability%2520to%2520compare%2520all%2520objects%2520for%2520improving%2520spatial%2520reasoning.%2520Our%2520work%250Aprovides%2520critical%2520insights%2520in%2520advancing%2520the%2520research%2520of%2520MLLMs%2520and%2520reasoning%250Amodels.%2520The%2520codes%252C%2520model%2520weights%252C%2520and%2520data%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/zongzhao23/STAR-R1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15804v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STAR-R1%3A%20Spatial%20TrAnsformation%20Reasoning%20by%20Reinforcing%20Multimodal%20LLMs&entry.906535625=Zongzhao%20Li%20and%20Zongyang%20Ma%20and%20Mingze%20Li%20and%20Songyou%20Li%20and%20Yu%20Rong%20and%20Tingyang%20Xu%20and%20Ziqi%20Zhang%20and%20Deli%20Zhao%20and%20Wenbing%20Huang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20remarkable%0Acapabilities%20across%20diverse%20tasks%2C%20yet%20they%20lag%20significantly%20behind%20humans%20in%0Aspatial%20reasoning.%20We%20investigate%20this%20gap%20through%20Transformation-Driven%20Visual%0AReasoning%20%28TVR%29%2C%20a%20challenging%20task%20requiring%20identification%20of%20object%0Atransformations%20across%20images%20under%20varying%20viewpoints.%20While%20traditional%0ASupervised%20Fine-Tuning%20%28SFT%29%20fails%20to%20generate%20coherent%20reasoning%20paths%20in%0Across-view%20settings%2C%20sparse-reward%20Reinforcement%20Learning%20%28RL%29%20suffers%20from%0Ainefficient%20exploration%20and%20slow%20convergence.%20To%20address%20these%20limitations%2C%20we%0Apropose%20STAR-R1%2C%20a%20novel%20framework%20that%20integrates%20a%20single-stage%20RL%20paradigm%0Awith%20a%20fine-grained%20reward%20mechanism%20tailored%20for%20TVR.%20Specifically%2C%20STAR-R1%0Arewards%20partial%20correctness%20while%20penalizing%20excessive%20enumeration%20and%20passive%0Ainaction%2C%20enabling%20efficient%20exploration%20and%20precise%20reasoning.%20Comprehensive%0Aevaluations%20demonstrate%20that%20STAR-R1%20achieves%20state-of-the-art%20performance%0Aacross%20all%2011%20metrics%2C%20outperforming%20SFT%20by%2023%25%20in%20cross-view%20scenarios.%0AFurther%20analysis%20reveals%20STAR-R1%27s%20anthropomorphic%20behavior%20and%20highlights%20its%0Aunique%20ability%20to%20compare%20all%20objects%20for%20improving%20spatial%20reasoning.%20Our%20work%0Aprovides%20critical%20insights%20in%20advancing%20the%20research%20of%20MLLMs%20and%20reasoning%0Amodels.%20The%20codes%2C%20model%20weights%2C%20and%20data%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/zongzhao23/STAR-R1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15804v2&entry.124074799=Read"},
{"title": "The Missing Point in Vision Transformers for Universal Image\n  Segmentation", "author": "Sajjad Shahabodini and Mobina Mansoori and Farnoush Bayatmakou and Jamshid Abouei and Konstantinos N. Plataniotis and Arash Mohammadi", "abstract": "  Image segmentation remains a challenging task in computer vision, demanding\nrobust mask generation and precise classification. Recent mask-based approaches\nyield high-quality masks by capturing global context. However, accurately\nclassifying these masks, especially in the presence of ambiguous boundaries and\nimbalanced class distributions, remains an open challenge. In this work, we\nintroduce ViT-P, a novel two-stage segmentation framework that decouples mask\ngeneration from classification. The first stage employs a proposal generator to\nproduce class-agnostic mask proposals, while the second stage utilizes a\npoint-based classification model built on the Vision Transformer (ViT) to\nrefine predictions by focusing on mask central points. ViT-P serves as a\npre-training-free adapter, allowing the integration of various pre-trained\nvision transformers without modifying their architecture, ensuring adaptability\nto dense prediction tasks. Furthermore, we demonstrate that coarse and bounding\nbox annotations can effectively enhance classification without requiring\nadditional training on fine annotation datasets, reducing annotation costs\nwhile maintaining strong performance. Extensive experiments across COCO,\nADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving\nstate-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4\nmIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic\nsegmentation. The code and pretrained models are available at:\nhttps://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P.\n", "link": "http://arxiv.org/abs/2505.19795v1", "date": "2025-05-26", "relevancy": 2.8576, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5834}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5713}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Missing%20Point%20in%20Vision%20Transformers%20for%20Universal%20Image%0A%20%20Segmentation&body=Title%3A%20The%20Missing%20Point%20in%20Vision%20Transformers%20for%20Universal%20Image%0A%20%20Segmentation%0AAuthor%3A%20Sajjad%20Shahabodini%20and%20Mobina%20Mansoori%20and%20Farnoush%20Bayatmakou%20and%20Jamshid%20Abouei%20and%20Konstantinos%20N.%20Plataniotis%20and%20Arash%20Mohammadi%0AAbstract%3A%20%20%20Image%20segmentation%20remains%20a%20challenging%20task%20in%20computer%20vision%2C%20demanding%0Arobust%20mask%20generation%20and%20precise%20classification.%20Recent%20mask-based%20approaches%0Ayield%20high-quality%20masks%20by%20capturing%20global%20context.%20However%2C%20accurately%0Aclassifying%20these%20masks%2C%20especially%20in%20the%20presence%20of%20ambiguous%20boundaries%20and%0Aimbalanced%20class%20distributions%2C%20remains%20an%20open%20challenge.%20In%20this%20work%2C%20we%0Aintroduce%20ViT-P%2C%20a%20novel%20two-stage%20segmentation%20framework%20that%20decouples%20mask%0Ageneration%20from%20classification.%20The%20first%20stage%20employs%20a%20proposal%20generator%20to%0Aproduce%20class-agnostic%20mask%20proposals%2C%20while%20the%20second%20stage%20utilizes%20a%0Apoint-based%20classification%20model%20built%20on%20the%20Vision%20Transformer%20%28ViT%29%20to%0Arefine%20predictions%20by%20focusing%20on%20mask%20central%20points.%20ViT-P%20serves%20as%20a%0Apre-training-free%20adapter%2C%20allowing%20the%20integration%20of%20various%20pre-trained%0Avision%20transformers%20without%20modifying%20their%20architecture%2C%20ensuring%20adaptability%0Ato%20dense%20prediction%20tasks.%20Furthermore%2C%20we%20demonstrate%20that%20coarse%20and%20bounding%0Abox%20annotations%20can%20effectively%20enhance%20classification%20without%20requiring%0Aadditional%20training%20on%20fine%20annotation%20datasets%2C%20reducing%20annotation%20costs%0Awhile%20maintaining%20strong%20performance.%20Extensive%20experiments%20across%20COCO%2C%0AADE20K%2C%20and%20Cityscapes%20datasets%20validate%20the%20effectiveness%20of%20ViT-P%2C%20achieving%0Astate-of-the-art%20results%20with%2054.0%20PQ%20on%20ADE20K%20panoptic%20segmentation%2C%2087.4%0AmIoU%20on%20Cityscapes%20semantic%20segmentation%2C%20and%2063.6%20mIoU%20on%20ADE20K%20semantic%0Asegmentation.%20The%20code%20and%20pretrained%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/sajjad-sh33/ViT-P%7D%7Bhttps%3A//github.com/sajjad-sh33/ViT-P.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19795v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Missing%2520Point%2520in%2520Vision%2520Transformers%2520for%2520Universal%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DSajjad%2520Shahabodini%2520and%2520Mobina%2520Mansoori%2520and%2520Farnoush%2520Bayatmakou%2520and%2520Jamshid%2520Abouei%2520and%2520Konstantinos%2520N.%2520Plataniotis%2520and%2520Arash%2520Mohammadi%26entry.1292438233%3D%2520%2520Image%2520segmentation%2520remains%2520a%2520challenging%2520task%2520in%2520computer%2520vision%252C%2520demanding%250Arobust%2520mask%2520generation%2520and%2520precise%2520classification.%2520Recent%2520mask-based%2520approaches%250Ayield%2520high-quality%2520masks%2520by%2520capturing%2520global%2520context.%2520However%252C%2520accurately%250Aclassifying%2520these%2520masks%252C%2520especially%2520in%2520the%2520presence%2520of%2520ambiguous%2520boundaries%2520and%250Aimbalanced%2520class%2520distributions%252C%2520remains%2520an%2520open%2520challenge.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520ViT-P%252C%2520a%2520novel%2520two-stage%2520segmentation%2520framework%2520that%2520decouples%2520mask%250Ageneration%2520from%2520classification.%2520The%2520first%2520stage%2520employs%2520a%2520proposal%2520generator%2520to%250Aproduce%2520class-agnostic%2520mask%2520proposals%252C%2520while%2520the%2520second%2520stage%2520utilizes%2520a%250Apoint-based%2520classification%2520model%2520built%2520on%2520the%2520Vision%2520Transformer%2520%2528ViT%2529%2520to%250Arefine%2520predictions%2520by%2520focusing%2520on%2520mask%2520central%2520points.%2520ViT-P%2520serves%2520as%2520a%250Apre-training-free%2520adapter%252C%2520allowing%2520the%2520integration%2520of%2520various%2520pre-trained%250Avision%2520transformers%2520without%2520modifying%2520their%2520architecture%252C%2520ensuring%2520adaptability%250Ato%2520dense%2520prediction%2520tasks.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520coarse%2520and%2520bounding%250Abox%2520annotations%2520can%2520effectively%2520enhance%2520classification%2520without%2520requiring%250Aadditional%2520training%2520on%2520fine%2520annotation%2520datasets%252C%2520reducing%2520annotation%2520costs%250Awhile%2520maintaining%2520strong%2520performance.%2520Extensive%2520experiments%2520across%2520COCO%252C%250AADE20K%252C%2520and%2520Cityscapes%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520ViT-P%252C%2520achieving%250Astate-of-the-art%2520results%2520with%252054.0%2520PQ%2520on%2520ADE20K%2520panoptic%2520segmentation%252C%252087.4%250AmIoU%2520on%2520Cityscapes%2520semantic%2520segmentation%252C%2520and%252063.6%2520mIoU%2520on%2520ADE20K%2520semantic%250Asegmentation.%2520The%2520code%2520and%2520pretrained%2520models%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/sajjad-sh33/ViT-P%257D%257Bhttps%253A//github.com/sajjad-sh33/ViT-P.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19795v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Missing%20Point%20in%20Vision%20Transformers%20for%20Universal%20Image%0A%20%20Segmentation&entry.906535625=Sajjad%20Shahabodini%20and%20Mobina%20Mansoori%20and%20Farnoush%20Bayatmakou%20and%20Jamshid%20Abouei%20and%20Konstantinos%20N.%20Plataniotis%20and%20Arash%20Mohammadi&entry.1292438233=%20%20Image%20segmentation%20remains%20a%20challenging%20task%20in%20computer%20vision%2C%20demanding%0Arobust%20mask%20generation%20and%20precise%20classification.%20Recent%20mask-based%20approaches%0Ayield%20high-quality%20masks%20by%20capturing%20global%20context.%20However%2C%20accurately%0Aclassifying%20these%20masks%2C%20especially%20in%20the%20presence%20of%20ambiguous%20boundaries%20and%0Aimbalanced%20class%20distributions%2C%20remains%20an%20open%20challenge.%20In%20this%20work%2C%20we%0Aintroduce%20ViT-P%2C%20a%20novel%20two-stage%20segmentation%20framework%20that%20decouples%20mask%0Ageneration%20from%20classification.%20The%20first%20stage%20employs%20a%20proposal%20generator%20to%0Aproduce%20class-agnostic%20mask%20proposals%2C%20while%20the%20second%20stage%20utilizes%20a%0Apoint-based%20classification%20model%20built%20on%20the%20Vision%20Transformer%20%28ViT%29%20to%0Arefine%20predictions%20by%20focusing%20on%20mask%20central%20points.%20ViT-P%20serves%20as%20a%0Apre-training-free%20adapter%2C%20allowing%20the%20integration%20of%20various%20pre-trained%0Avision%20transformers%20without%20modifying%20their%20architecture%2C%20ensuring%20adaptability%0Ato%20dense%20prediction%20tasks.%20Furthermore%2C%20we%20demonstrate%20that%20coarse%20and%20bounding%0Abox%20annotations%20can%20effectively%20enhance%20classification%20without%20requiring%0Aadditional%20training%20on%20fine%20annotation%20datasets%2C%20reducing%20annotation%20costs%0Awhile%20maintaining%20strong%20performance.%20Extensive%20experiments%20across%20COCO%2C%0AADE20K%2C%20and%20Cityscapes%20datasets%20validate%20the%20effectiveness%20of%20ViT-P%2C%20achieving%0Astate-of-the-art%20results%20with%2054.0%20PQ%20on%20ADE20K%20panoptic%20segmentation%2C%2087.4%0AmIoU%20on%20Cityscapes%20semantic%20segmentation%2C%20and%2063.6%20mIoU%20on%20ADE20K%20semantic%0Asegmentation.%20The%20code%20and%20pretrained%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/sajjad-sh33/ViT-P%7D%7Bhttps%3A//github.com/sajjad-sh33/ViT-P.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19795v1&entry.124074799=Read"},
{"title": "Two Causally Related Needles in a Video Haystack", "author": "Miaoyu Li and Qin Chao and Boyang Li", "abstract": "  Evaluating the video understanding capabilities of Video-Language Models\n(VLMs) remains a significant challenge. We propose a long-context video\nunderstanding benchmark, Causal2Needles, that assesses two crucial abilities\ninsufficiently evaluated by existing benchmarks: (1) the ability to extract\ninformation from two separate locations in a long video and understand them\njointly, and (2) the ability to model the world in terms of cause and effect in\nhuman behaviors. Specifically, Causal2Needles introduces 2-needle questions,\nwhich require extracting information from both the cause and effect\nhuman-behavior events in a long video and the associated narration text. To\nprevent textual bias, these questions comprise two complementary formats: one\nasking to identify the video clip containing the answer, and one asking for the\ntextual description of an unrelated visual detail from that video clip. Our\nexperiments reveal that models excelling in pre-existing benchmarks struggle\nwith 2-needle visual grounding, and the model performance is negatively\ncorrelated with the distance between the two needles. These findings highlight\ncritical limitations in current VLMs.\n", "link": "http://arxiv.org/abs/2505.19853v1", "date": "2025-05-26", "relevancy": 2.8526, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5938}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5938}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two%20Causally%20Related%20Needles%20in%20a%20Video%20Haystack&body=Title%3A%20Two%20Causally%20Related%20Needles%20in%20a%20Video%20Haystack%0AAuthor%3A%20Miaoyu%20Li%20and%20Qin%20Chao%20and%20Boyang%20Li%0AAbstract%3A%20%20%20Evaluating%20the%20video%20understanding%20capabilities%20of%20Video-Language%20Models%0A%28VLMs%29%20remains%20a%20significant%20challenge.%20We%20propose%20a%20long-context%20video%0Aunderstanding%20benchmark%2C%20Causal2Needles%2C%20that%20assesses%20two%20crucial%20abilities%0Ainsufficiently%20evaluated%20by%20existing%20benchmarks%3A%20%281%29%20the%20ability%20to%20extract%0Ainformation%20from%20two%20separate%20locations%20in%20a%20long%20video%20and%20understand%20them%0Ajointly%2C%20and%20%282%29%20the%20ability%20to%20model%20the%20world%20in%20terms%20of%20cause%20and%20effect%20in%0Ahuman%20behaviors.%20Specifically%2C%20Causal2Needles%20introduces%202-needle%20questions%2C%0Awhich%20require%20extracting%20information%20from%20both%20the%20cause%20and%20effect%0Ahuman-behavior%20events%20in%20a%20long%20video%20and%20the%20associated%20narration%20text.%20To%0Aprevent%20textual%20bias%2C%20these%20questions%20comprise%20two%20complementary%20formats%3A%20one%0Aasking%20to%20identify%20the%20video%20clip%20containing%20the%20answer%2C%20and%20one%20asking%20for%20the%0Atextual%20description%20of%20an%20unrelated%20visual%20detail%20from%20that%20video%20clip.%20Our%0Aexperiments%20reveal%20that%20models%20excelling%20in%20pre-existing%20benchmarks%20struggle%0Awith%202-needle%20visual%20grounding%2C%20and%20the%20model%20performance%20is%20negatively%0Acorrelated%20with%20the%20distance%20between%20the%20two%20needles.%20These%20findings%20highlight%0Acritical%20limitations%20in%20current%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo%2520Causally%2520Related%2520Needles%2520in%2520a%2520Video%2520Haystack%26entry.906535625%3DMiaoyu%2520Li%2520and%2520Qin%2520Chao%2520and%2520Boyang%2520Li%26entry.1292438233%3D%2520%2520Evaluating%2520the%2520video%2520understanding%2520capabilities%2520of%2520Video-Language%2520Models%250A%2528VLMs%2529%2520remains%2520a%2520significant%2520challenge.%2520We%2520propose%2520a%2520long-context%2520video%250Aunderstanding%2520benchmark%252C%2520Causal2Needles%252C%2520that%2520assesses%2520two%2520crucial%2520abilities%250Ainsufficiently%2520evaluated%2520by%2520existing%2520benchmarks%253A%2520%25281%2529%2520the%2520ability%2520to%2520extract%250Ainformation%2520from%2520two%2520separate%2520locations%2520in%2520a%2520long%2520video%2520and%2520understand%2520them%250Ajointly%252C%2520and%2520%25282%2529%2520the%2520ability%2520to%2520model%2520the%2520world%2520in%2520terms%2520of%2520cause%2520and%2520effect%2520in%250Ahuman%2520behaviors.%2520Specifically%252C%2520Causal2Needles%2520introduces%25202-needle%2520questions%252C%250Awhich%2520require%2520extracting%2520information%2520from%2520both%2520the%2520cause%2520and%2520effect%250Ahuman-behavior%2520events%2520in%2520a%2520long%2520video%2520and%2520the%2520associated%2520narration%2520text.%2520To%250Aprevent%2520textual%2520bias%252C%2520these%2520questions%2520comprise%2520two%2520complementary%2520formats%253A%2520one%250Aasking%2520to%2520identify%2520the%2520video%2520clip%2520containing%2520the%2520answer%252C%2520and%2520one%2520asking%2520for%2520the%250Atextual%2520description%2520of%2520an%2520unrelated%2520visual%2520detail%2520from%2520that%2520video%2520clip.%2520Our%250Aexperiments%2520reveal%2520that%2520models%2520excelling%2520in%2520pre-existing%2520benchmarks%2520struggle%250Awith%25202-needle%2520visual%2520grounding%252C%2520and%2520the%2520model%2520performance%2520is%2520negatively%250Acorrelated%2520with%2520the%2520distance%2520between%2520the%2520two%2520needles.%2520These%2520findings%2520highlight%250Acritical%2520limitations%2520in%2520current%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two%20Causally%20Related%20Needles%20in%20a%20Video%20Haystack&entry.906535625=Miaoyu%20Li%20and%20Qin%20Chao%20and%20Boyang%20Li&entry.1292438233=%20%20Evaluating%20the%20video%20understanding%20capabilities%20of%20Video-Language%20Models%0A%28VLMs%29%20remains%20a%20significant%20challenge.%20We%20propose%20a%20long-context%20video%0Aunderstanding%20benchmark%2C%20Causal2Needles%2C%20that%20assesses%20two%20crucial%20abilities%0Ainsufficiently%20evaluated%20by%20existing%20benchmarks%3A%20%281%29%20the%20ability%20to%20extract%0Ainformation%20from%20two%20separate%20locations%20in%20a%20long%20video%20and%20understand%20them%0Ajointly%2C%20and%20%282%29%20the%20ability%20to%20model%20the%20world%20in%20terms%20of%20cause%20and%20effect%20in%0Ahuman%20behaviors.%20Specifically%2C%20Causal2Needles%20introduces%202-needle%20questions%2C%0Awhich%20require%20extracting%20information%20from%20both%20the%20cause%20and%20effect%0Ahuman-behavior%20events%20in%20a%20long%20video%20and%20the%20associated%20narration%20text.%20To%0Aprevent%20textual%20bias%2C%20these%20questions%20comprise%20two%20complementary%20formats%3A%20one%0Aasking%20to%20identify%20the%20video%20clip%20containing%20the%20answer%2C%20and%20one%20asking%20for%20the%0Atextual%20description%20of%20an%20unrelated%20visual%20detail%20from%20that%20video%20clip.%20Our%0Aexperiments%20reveal%20that%20models%20excelling%20in%20pre-existing%20benchmarks%20struggle%0Awith%202-needle%20visual%20grounding%2C%20and%20the%20model%20performance%20is%20negatively%0Acorrelated%20with%20the%20distance%20between%20the%20two%20needles.%20These%20findings%20highlight%0Acritical%20limitations%20in%20current%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19853v1&entry.124074799=Read"},
{"title": "NEXT: Multi-Grained Mixture of Experts via Text-Modulation for\n  Multi-Modal Object Re-ID", "author": "Shihao Li and Chenglong Li and Aihua Zheng and Andong Lu and Jin Tang and Jixin Ma", "abstract": "  Multi-modal object re-identification (ReID) aims to extract identity features\nacross heterogeneous spectral modalities to enable accurate recognition and\nretrieval in complex real-world scenarios. However, most existing methods rely\non implicit feature fusion structures, making it difficult to model\nfine-grained recognition strategies under varying challenging conditions.\nBenefiting from the powerful semantic understanding capabilities of Multi-modal\nLarge Language Models (MLLMs), the visual appearance of an object can be\neffectively translated into descriptive text. In this paper, we propose a\nreliable multi-modal caption generation method based on attribute confidence,\nwhich significantly reduces the unknown recognition rate of MLLMs in\nmulti-modal semantic generation and improves the quality of generated text.\nAdditionally, we propose a novel ReID framework NEXT, the Multi-grained Mixture\nof Experts via Text-Modulation for Multi-modal Object Re-Identification.\nSpecifically, we decouple the recognition problem into semantic and structural\nexpert branches to separately capture modality-specific appearance and\nintrinsic structure. For semantic recognition, we propose the Text-Modulated\nSemantic-sampling Experts (TMSE), which leverages randomly sampled high-quality\nsemantic texts to modulate expert-specific sampling of multi-modal features and\nmining intra-modality fine-grained semantic cues. Then, to recognize\ncoarse-grained structure features, we propose the Context-Shared\nStructure-aware Experts (CSSE) that focuses on capturing the holistic object\nstructure across modalities and maintains inter-modality structural consistency\nthrough a soft routing mechanism. Finally, we propose the Multi-Modal Feature\nAggregation (MMFA), which adopts a unified feature fusion strategy to simply\nand effectively integrate semantic and structural expert outputs into the final\nidentity representations.\n", "link": "http://arxiv.org/abs/2505.20001v1", "date": "2025-05-26", "relevancy": 2.8374, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5952}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5565}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NEXT%3A%20Multi-Grained%20Mixture%20of%20Experts%20via%20Text-Modulation%20for%0A%20%20Multi-Modal%20Object%20Re-ID&body=Title%3A%20NEXT%3A%20Multi-Grained%20Mixture%20of%20Experts%20via%20Text-Modulation%20for%0A%20%20Multi-Modal%20Object%20Re-ID%0AAuthor%3A%20Shihao%20Li%20and%20Chenglong%20Li%20and%20Aihua%20Zheng%20and%20Andong%20Lu%20and%20Jin%20Tang%20and%20Jixin%20Ma%0AAbstract%3A%20%20%20Multi-modal%20object%20re-identification%20%28ReID%29%20aims%20to%20extract%20identity%20features%0Aacross%20heterogeneous%20spectral%20modalities%20to%20enable%20accurate%20recognition%20and%0Aretrieval%20in%20complex%20real-world%20scenarios.%20However%2C%20most%20existing%20methods%20rely%0Aon%20implicit%20feature%20fusion%20structures%2C%20making%20it%20difficult%20to%20model%0Afine-grained%20recognition%20strategies%20under%20varying%20challenging%20conditions.%0ABenefiting%20from%20the%20powerful%20semantic%20understanding%20capabilities%20of%20Multi-modal%0ALarge%20Language%20Models%20%28MLLMs%29%2C%20the%20visual%20appearance%20of%20an%20object%20can%20be%0Aeffectively%20translated%20into%20descriptive%20text.%20In%20this%20paper%2C%20we%20propose%20a%0Areliable%20multi-modal%20caption%20generation%20method%20based%20on%20attribute%20confidence%2C%0Awhich%20significantly%20reduces%20the%20unknown%20recognition%20rate%20of%20MLLMs%20in%0Amulti-modal%20semantic%20generation%20and%20improves%20the%20quality%20of%20generated%20text.%0AAdditionally%2C%20we%20propose%20a%20novel%20ReID%20framework%20NEXT%2C%20the%20Multi-grained%20Mixture%0Aof%20Experts%20via%20Text-Modulation%20for%20Multi-modal%20Object%20Re-Identification.%0ASpecifically%2C%20we%20decouple%20the%20recognition%20problem%20into%20semantic%20and%20structural%0Aexpert%20branches%20to%20separately%20capture%20modality-specific%20appearance%20and%0Aintrinsic%20structure.%20For%20semantic%20recognition%2C%20we%20propose%20the%20Text-Modulated%0ASemantic-sampling%20Experts%20%28TMSE%29%2C%20which%20leverages%20randomly%20sampled%20high-quality%0Asemantic%20texts%20to%20modulate%20expert-specific%20sampling%20of%20multi-modal%20features%20and%0Amining%20intra-modality%20fine-grained%20semantic%20cues.%20Then%2C%20to%20recognize%0Acoarse-grained%20structure%20features%2C%20we%20propose%20the%20Context-Shared%0AStructure-aware%20Experts%20%28CSSE%29%20that%20focuses%20on%20capturing%20the%20holistic%20object%0Astructure%20across%20modalities%20and%20maintains%20inter-modality%20structural%20consistency%0Athrough%20a%20soft%20routing%20mechanism.%20Finally%2C%20we%20propose%20the%20Multi-Modal%20Feature%0AAggregation%20%28MMFA%29%2C%20which%20adopts%20a%20unified%20feature%20fusion%20strategy%20to%20simply%0Aand%20effectively%20integrate%20semantic%20and%20structural%20expert%20outputs%20into%20the%20final%0Aidentity%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNEXT%253A%2520Multi-Grained%2520Mixture%2520of%2520Experts%2520via%2520Text-Modulation%2520for%250A%2520%2520Multi-Modal%2520Object%2520Re-ID%26entry.906535625%3DShihao%2520Li%2520and%2520Chenglong%2520Li%2520and%2520Aihua%2520Zheng%2520and%2520Andong%2520Lu%2520and%2520Jin%2520Tang%2520and%2520Jixin%2520Ma%26entry.1292438233%3D%2520%2520Multi-modal%2520object%2520re-identification%2520%2528ReID%2529%2520aims%2520to%2520extract%2520identity%2520features%250Aacross%2520heterogeneous%2520spectral%2520modalities%2520to%2520enable%2520accurate%2520recognition%2520and%250Aretrieval%2520in%2520complex%2520real-world%2520scenarios.%2520However%252C%2520most%2520existing%2520methods%2520rely%250Aon%2520implicit%2520feature%2520fusion%2520structures%252C%2520making%2520it%2520difficult%2520to%2520model%250Afine-grained%2520recognition%2520strategies%2520under%2520varying%2520challenging%2520conditions.%250ABenefiting%2520from%2520the%2520powerful%2520semantic%2520understanding%2520capabilities%2520of%2520Multi-modal%250ALarge%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520the%2520visual%2520appearance%2520of%2520an%2520object%2520can%2520be%250Aeffectively%2520translated%2520into%2520descriptive%2520text.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Areliable%2520multi-modal%2520caption%2520generation%2520method%2520based%2520on%2520attribute%2520confidence%252C%250Awhich%2520significantly%2520reduces%2520the%2520unknown%2520recognition%2520rate%2520of%2520MLLMs%2520in%250Amulti-modal%2520semantic%2520generation%2520and%2520improves%2520the%2520quality%2520of%2520generated%2520text.%250AAdditionally%252C%2520we%2520propose%2520a%2520novel%2520ReID%2520framework%2520NEXT%252C%2520the%2520Multi-grained%2520Mixture%250Aof%2520Experts%2520via%2520Text-Modulation%2520for%2520Multi-modal%2520Object%2520Re-Identification.%250ASpecifically%252C%2520we%2520decouple%2520the%2520recognition%2520problem%2520into%2520semantic%2520and%2520structural%250Aexpert%2520branches%2520to%2520separately%2520capture%2520modality-specific%2520appearance%2520and%250Aintrinsic%2520structure.%2520For%2520semantic%2520recognition%252C%2520we%2520propose%2520the%2520Text-Modulated%250ASemantic-sampling%2520Experts%2520%2528TMSE%2529%252C%2520which%2520leverages%2520randomly%2520sampled%2520high-quality%250Asemantic%2520texts%2520to%2520modulate%2520expert-specific%2520sampling%2520of%2520multi-modal%2520features%2520and%250Amining%2520intra-modality%2520fine-grained%2520semantic%2520cues.%2520Then%252C%2520to%2520recognize%250Acoarse-grained%2520structure%2520features%252C%2520we%2520propose%2520the%2520Context-Shared%250AStructure-aware%2520Experts%2520%2528CSSE%2529%2520that%2520focuses%2520on%2520capturing%2520the%2520holistic%2520object%250Astructure%2520across%2520modalities%2520and%2520maintains%2520inter-modality%2520structural%2520consistency%250Athrough%2520a%2520soft%2520routing%2520mechanism.%2520Finally%252C%2520we%2520propose%2520the%2520Multi-Modal%2520Feature%250AAggregation%2520%2528MMFA%2529%252C%2520which%2520adopts%2520a%2520unified%2520feature%2520fusion%2520strategy%2520to%2520simply%250Aand%2520effectively%2520integrate%2520semantic%2520and%2520structural%2520expert%2520outputs%2520into%2520the%2520final%250Aidentity%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NEXT%3A%20Multi-Grained%20Mixture%20of%20Experts%20via%20Text-Modulation%20for%0A%20%20Multi-Modal%20Object%20Re-ID&entry.906535625=Shihao%20Li%20and%20Chenglong%20Li%20and%20Aihua%20Zheng%20and%20Andong%20Lu%20and%20Jin%20Tang%20and%20Jixin%20Ma&entry.1292438233=%20%20Multi-modal%20object%20re-identification%20%28ReID%29%20aims%20to%20extract%20identity%20features%0Aacross%20heterogeneous%20spectral%20modalities%20to%20enable%20accurate%20recognition%20and%0Aretrieval%20in%20complex%20real-world%20scenarios.%20However%2C%20most%20existing%20methods%20rely%0Aon%20implicit%20feature%20fusion%20structures%2C%20making%20it%20difficult%20to%20model%0Afine-grained%20recognition%20strategies%20under%20varying%20challenging%20conditions.%0ABenefiting%20from%20the%20powerful%20semantic%20understanding%20capabilities%20of%20Multi-modal%0ALarge%20Language%20Models%20%28MLLMs%29%2C%20the%20visual%20appearance%20of%20an%20object%20can%20be%0Aeffectively%20translated%20into%20descriptive%20text.%20In%20this%20paper%2C%20we%20propose%20a%0Areliable%20multi-modal%20caption%20generation%20method%20based%20on%20attribute%20confidence%2C%0Awhich%20significantly%20reduces%20the%20unknown%20recognition%20rate%20of%20MLLMs%20in%0Amulti-modal%20semantic%20generation%20and%20improves%20the%20quality%20of%20generated%20text.%0AAdditionally%2C%20we%20propose%20a%20novel%20ReID%20framework%20NEXT%2C%20the%20Multi-grained%20Mixture%0Aof%20Experts%20via%20Text-Modulation%20for%20Multi-modal%20Object%20Re-Identification.%0ASpecifically%2C%20we%20decouple%20the%20recognition%20problem%20into%20semantic%20and%20structural%0Aexpert%20branches%20to%20separately%20capture%20modality-specific%20appearance%20and%0Aintrinsic%20structure.%20For%20semantic%20recognition%2C%20we%20propose%20the%20Text-Modulated%0ASemantic-sampling%20Experts%20%28TMSE%29%2C%20which%20leverages%20randomly%20sampled%20high-quality%0Asemantic%20texts%20to%20modulate%20expert-specific%20sampling%20of%20multi-modal%20features%20and%0Amining%20intra-modality%20fine-grained%20semantic%20cues.%20Then%2C%20to%20recognize%0Acoarse-grained%20structure%20features%2C%20we%20propose%20the%20Context-Shared%0AStructure-aware%20Experts%20%28CSSE%29%20that%20focuses%20on%20capturing%20the%20holistic%20object%0Astructure%20across%20modalities%20and%20maintains%20inter-modality%20structural%20consistency%0Athrough%20a%20soft%20routing%20mechanism.%20Finally%2C%20we%20propose%20the%20Multi-Modal%20Feature%0AAggregation%20%28MMFA%29%2C%20which%20adopts%20a%20unified%20feature%20fusion%20strategy%20to%20simply%0Aand%20effectively%20integrate%20semantic%20and%20structural%20expert%20outputs%20into%20the%20final%0Aidentity%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20001v1&entry.124074799=Read"},
{"title": "Bridging The Multi-Modality Gaps of Audio, Visual and Linguistic for\n  Speech Enhancement", "author": "Meng-Ping Lin and Jen-Cheng Hou and Chia-Wei Chen and Shao-Yi Chien and Jun-Cheng Chen and Xugang Lu and Yu Tsao", "abstract": "  Speech enhancement (SE) aims to improve the quality and intelligibility of\nspeech in noisy environments. Recent studies have shown that incorporating\nvisual cues in audio signal processing can enhance SE performance. Given that\nhuman speech communication naturally involves audio, visual, and linguistic\nmodalities, it is reasonable to expect additional improvements by integrating\nlinguistic information. However, effectively bridging these modality gaps,\nparticularly during knowledge transfer remains a significant challenge. In this\npaper, we propose a novel multi-modal learning framework, termed DLAV-SE, which\nleverages a diffusion-based model integrating audio, visual, and linguistic\ninformation for audio-visual speech enhancement (AVSE). Within this framework,\nthe linguistic modality is modeled using a pretrained language model (PLM),\nwhich transfers linguistic knowledge to the audio-visual domain through a\ncross-modal knowledge transfer (CMKT) mechanism during training. After\ntraining, the PLM is no longer required at inference, as its knowledge is\nembedded into the AVSE model through the CMKT process. We conduct a series of\nSE experiments to evaluate the effectiveness of our approach. Results show that\nthe proposed DLAV-SE system significantly improves speech quality and reduces\ngenerative artifacts, such as phonetic confusion, compared to state-of-the-art\n(SOTA) methods. Furthermore, visualization analyses confirm that the CMKT\nmethod enhances the generation quality of the AVSE outputs. These findings\nhighlight both the promise of diffusion-based methods for advancing AVSE and\nthe value of incorporating linguistic information to further improve system\nperformance.\n", "link": "http://arxiv.org/abs/2501.13375v2", "date": "2025-05-26", "relevancy": 2.8301, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5778}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5778}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20The%20Multi-Modality%20Gaps%20of%20Audio%2C%20Visual%20and%20Linguistic%20for%0A%20%20Speech%20Enhancement&body=Title%3A%20Bridging%20The%20Multi-Modality%20Gaps%20of%20Audio%2C%20Visual%20and%20Linguistic%20for%0A%20%20Speech%20Enhancement%0AAuthor%3A%20Meng-Ping%20Lin%20and%20Jen-Cheng%20Hou%20and%20Chia-Wei%20Chen%20and%20Shao-Yi%20Chien%20and%20Jun-Cheng%20Chen%20and%20Xugang%20Lu%20and%20Yu%20Tsao%0AAbstract%3A%20%20%20Speech%20enhancement%20%28SE%29%20aims%20to%20improve%20the%20quality%20and%20intelligibility%20of%0Aspeech%20in%20noisy%20environments.%20Recent%20studies%20have%20shown%20that%20incorporating%0Avisual%20cues%20in%20audio%20signal%20processing%20can%20enhance%20SE%20performance.%20Given%20that%0Ahuman%20speech%20communication%20naturally%20involves%20audio%2C%20visual%2C%20and%20linguistic%0Amodalities%2C%20it%20is%20reasonable%20to%20expect%20additional%20improvements%20by%20integrating%0Alinguistic%20information.%20However%2C%20effectively%20bridging%20these%20modality%20gaps%2C%0Aparticularly%20during%20knowledge%20transfer%20remains%20a%20significant%20challenge.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20multi-modal%20learning%20framework%2C%20termed%20DLAV-SE%2C%20which%0Aleverages%20a%20diffusion-based%20model%20integrating%20audio%2C%20visual%2C%20and%20linguistic%0Ainformation%20for%20audio-visual%20speech%20enhancement%20%28AVSE%29.%20Within%20this%20framework%2C%0Athe%20linguistic%20modality%20is%20modeled%20using%20a%20pretrained%20language%20model%20%28PLM%29%2C%0Awhich%20transfers%20linguistic%20knowledge%20to%20the%20audio-visual%20domain%20through%20a%0Across-modal%20knowledge%20transfer%20%28CMKT%29%20mechanism%20during%20training.%20After%0Atraining%2C%20the%20PLM%20is%20no%20longer%20required%20at%20inference%2C%20as%20its%20knowledge%20is%0Aembedded%20into%20the%20AVSE%20model%20through%20the%20CMKT%20process.%20We%20conduct%20a%20series%20of%0ASE%20experiments%20to%20evaluate%20the%20effectiveness%20of%20our%20approach.%20Results%20show%20that%0Athe%20proposed%20DLAV-SE%20system%20significantly%20improves%20speech%20quality%20and%20reduces%0Agenerative%20artifacts%2C%20such%20as%20phonetic%20confusion%2C%20compared%20to%20state-of-the-art%0A%28SOTA%29%20methods.%20Furthermore%2C%20visualization%20analyses%20confirm%20that%20the%20CMKT%0Amethod%20enhances%20the%20generation%20quality%20of%20the%20AVSE%20outputs.%20These%20findings%0Ahighlight%20both%20the%20promise%20of%20diffusion-based%20methods%20for%20advancing%20AVSE%20and%0Athe%20value%20of%20incorporating%20linguistic%20information%20to%20further%20improve%20system%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13375v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520The%2520Multi-Modality%2520Gaps%2520of%2520Audio%252C%2520Visual%2520and%2520Linguistic%2520for%250A%2520%2520Speech%2520Enhancement%26entry.906535625%3DMeng-Ping%2520Lin%2520and%2520Jen-Cheng%2520Hou%2520and%2520Chia-Wei%2520Chen%2520and%2520Shao-Yi%2520Chien%2520and%2520Jun-Cheng%2520Chen%2520and%2520Xugang%2520Lu%2520and%2520Yu%2520Tsao%26entry.1292438233%3D%2520%2520Speech%2520enhancement%2520%2528SE%2529%2520aims%2520to%2520improve%2520the%2520quality%2520and%2520intelligibility%2520of%250Aspeech%2520in%2520noisy%2520environments.%2520Recent%2520studies%2520have%2520shown%2520that%2520incorporating%250Avisual%2520cues%2520in%2520audio%2520signal%2520processing%2520can%2520enhance%2520SE%2520performance.%2520Given%2520that%250Ahuman%2520speech%2520communication%2520naturally%2520involves%2520audio%252C%2520visual%252C%2520and%2520linguistic%250Amodalities%252C%2520it%2520is%2520reasonable%2520to%2520expect%2520additional%2520improvements%2520by%2520integrating%250Alinguistic%2520information.%2520However%252C%2520effectively%2520bridging%2520these%2520modality%2520gaps%252C%250Aparticularly%2520during%2520knowledge%2520transfer%2520remains%2520a%2520significant%2520challenge.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520multi-modal%2520learning%2520framework%252C%2520termed%2520DLAV-SE%252C%2520which%250Aleverages%2520a%2520diffusion-based%2520model%2520integrating%2520audio%252C%2520visual%252C%2520and%2520linguistic%250Ainformation%2520for%2520audio-visual%2520speech%2520enhancement%2520%2528AVSE%2529.%2520Within%2520this%2520framework%252C%250Athe%2520linguistic%2520modality%2520is%2520modeled%2520using%2520a%2520pretrained%2520language%2520model%2520%2528PLM%2529%252C%250Awhich%2520transfers%2520linguistic%2520knowledge%2520to%2520the%2520audio-visual%2520domain%2520through%2520a%250Across-modal%2520knowledge%2520transfer%2520%2528CMKT%2529%2520mechanism%2520during%2520training.%2520After%250Atraining%252C%2520the%2520PLM%2520is%2520no%2520longer%2520required%2520at%2520inference%252C%2520as%2520its%2520knowledge%2520is%250Aembedded%2520into%2520the%2520AVSE%2520model%2520through%2520the%2520CMKT%2520process.%2520We%2520conduct%2520a%2520series%2520of%250ASE%2520experiments%2520to%2520evaluate%2520the%2520effectiveness%2520of%2520our%2520approach.%2520Results%2520show%2520that%250Athe%2520proposed%2520DLAV-SE%2520system%2520significantly%2520improves%2520speech%2520quality%2520and%2520reduces%250Agenerative%2520artifacts%252C%2520such%2520as%2520phonetic%2520confusion%252C%2520compared%2520to%2520state-of-the-art%250A%2528SOTA%2529%2520methods.%2520Furthermore%252C%2520visualization%2520analyses%2520confirm%2520that%2520the%2520CMKT%250Amethod%2520enhances%2520the%2520generation%2520quality%2520of%2520the%2520AVSE%2520outputs.%2520These%2520findings%250Ahighlight%2520both%2520the%2520promise%2520of%2520diffusion-based%2520methods%2520for%2520advancing%2520AVSE%2520and%250Athe%2520value%2520of%2520incorporating%2520linguistic%2520information%2520to%2520further%2520improve%2520system%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13375v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20The%20Multi-Modality%20Gaps%20of%20Audio%2C%20Visual%20and%20Linguistic%20for%0A%20%20Speech%20Enhancement&entry.906535625=Meng-Ping%20Lin%20and%20Jen-Cheng%20Hou%20and%20Chia-Wei%20Chen%20and%20Shao-Yi%20Chien%20and%20Jun-Cheng%20Chen%20and%20Xugang%20Lu%20and%20Yu%20Tsao&entry.1292438233=%20%20Speech%20enhancement%20%28SE%29%20aims%20to%20improve%20the%20quality%20and%20intelligibility%20of%0Aspeech%20in%20noisy%20environments.%20Recent%20studies%20have%20shown%20that%20incorporating%0Avisual%20cues%20in%20audio%20signal%20processing%20can%20enhance%20SE%20performance.%20Given%20that%0Ahuman%20speech%20communication%20naturally%20involves%20audio%2C%20visual%2C%20and%20linguistic%0Amodalities%2C%20it%20is%20reasonable%20to%20expect%20additional%20improvements%20by%20integrating%0Alinguistic%20information.%20However%2C%20effectively%20bridging%20these%20modality%20gaps%2C%0Aparticularly%20during%20knowledge%20transfer%20remains%20a%20significant%20challenge.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20multi-modal%20learning%20framework%2C%20termed%20DLAV-SE%2C%20which%0Aleverages%20a%20diffusion-based%20model%20integrating%20audio%2C%20visual%2C%20and%20linguistic%0Ainformation%20for%20audio-visual%20speech%20enhancement%20%28AVSE%29.%20Within%20this%20framework%2C%0Athe%20linguistic%20modality%20is%20modeled%20using%20a%20pretrained%20language%20model%20%28PLM%29%2C%0Awhich%20transfers%20linguistic%20knowledge%20to%20the%20audio-visual%20domain%20through%20a%0Across-modal%20knowledge%20transfer%20%28CMKT%29%20mechanism%20during%20training.%20After%0Atraining%2C%20the%20PLM%20is%20no%20longer%20required%20at%20inference%2C%20as%20its%20knowledge%20is%0Aembedded%20into%20the%20AVSE%20model%20through%20the%20CMKT%20process.%20We%20conduct%20a%20series%20of%0ASE%20experiments%20to%20evaluate%20the%20effectiveness%20of%20our%20approach.%20Results%20show%20that%0Athe%20proposed%20DLAV-SE%20system%20significantly%20improves%20speech%20quality%20and%20reduces%0Agenerative%20artifacts%2C%20such%20as%20phonetic%20confusion%2C%20compared%20to%20state-of-the-art%0A%28SOTA%29%20methods.%20Furthermore%2C%20visualization%20analyses%20confirm%20that%20the%20CMKT%0Amethod%20enhances%20the%20generation%20quality%20of%20the%20AVSE%20outputs.%20These%20findings%0Ahighlight%20both%20the%20promise%20of%20diffusion-based%20methods%20for%20advancing%20AVSE%20and%0Athe%20value%20of%20incorporating%20linguistic%20information%20to%20further%20improve%20system%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13375v2&entry.124074799=Read"},
{"title": "Can Visual Encoder Learn to See Arrows?", "author": "Naoyuki Terashita and Yusuke Tozaki and Hideaki Omote and Congkha Nguyen and Ryosuke Nakamoto and Yuta Koreeda and Hiroaki Ozaki", "abstract": "  The diagram is a visual representation of a relationship illustrated with\nedges (lines or arrows), which is widely used in industrial and scientific\ncommunication. Although recognizing diagrams is essential for vision language\nmodels (VLMs) to comprehend domain-specific knowledge, recent studies reveal\nthat many VLMs fail to identify edges in images. We hypothesize that these\nfailures stem from an over-reliance on textual and positional biases,\npreventing VLMs from learning explicit edge features. Based on this idea, we\nempirically investigate whether the image encoder in VLMs can learn edge\nrepresentation through training on a diagram dataset in which edges are biased\nneither by textual nor positional information. To this end, we conduct\ncontrastive learning on an artificially generated diagram--caption dataset to\ntrain an image encoder and evaluate its diagram-related features on three\ntasks: probing, image retrieval, and captioning. Our results show that the\nfinetuned model outperforms pretrained CLIP in all tasks and surpasses\nzero-shot GPT-4o and LLaVA-Mistral in the captioning task. These findings\nconfirm that eliminating textual and positional biases fosters accurate edge\nrecognition in VLMs, offering a promising path for advancing diagram\nunderstanding.\n", "link": "http://arxiv.org/abs/2505.19944v1", "date": "2025-05-26", "relevancy": 2.8098, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5691}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5691}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Visual%20Encoder%20Learn%20to%20See%20Arrows%3F&body=Title%3A%20Can%20Visual%20Encoder%20Learn%20to%20See%20Arrows%3F%0AAuthor%3A%20Naoyuki%20Terashita%20and%20Yusuke%20Tozaki%20and%20Hideaki%20Omote%20and%20Congkha%20Nguyen%20and%20Ryosuke%20Nakamoto%20and%20Yuta%20Koreeda%20and%20Hiroaki%20Ozaki%0AAbstract%3A%20%20%20The%20diagram%20is%20a%20visual%20representation%20of%20a%20relationship%20illustrated%20with%0Aedges%20%28lines%20or%20arrows%29%2C%20which%20is%20widely%20used%20in%20industrial%20and%20scientific%0Acommunication.%20Although%20recognizing%20diagrams%20is%20essential%20for%20vision%20language%0Amodels%20%28VLMs%29%20to%20comprehend%20domain-specific%20knowledge%2C%20recent%20studies%20reveal%0Athat%20many%20VLMs%20fail%20to%20identify%20edges%20in%20images.%20We%20hypothesize%20that%20these%0Afailures%20stem%20from%20an%20over-reliance%20on%20textual%20and%20positional%20biases%2C%0Apreventing%20VLMs%20from%20learning%20explicit%20edge%20features.%20Based%20on%20this%20idea%2C%20we%0Aempirically%20investigate%20whether%20the%20image%20encoder%20in%20VLMs%20can%20learn%20edge%0Arepresentation%20through%20training%20on%20a%20diagram%20dataset%20in%20which%20edges%20are%20biased%0Aneither%20by%20textual%20nor%20positional%20information.%20To%20this%20end%2C%20we%20conduct%0Acontrastive%20learning%20on%20an%20artificially%20generated%20diagram--caption%20dataset%20to%0Atrain%20an%20image%20encoder%20and%20evaluate%20its%20diagram-related%20features%20on%20three%0Atasks%3A%20probing%2C%20image%20retrieval%2C%20and%20captioning.%20Our%20results%20show%20that%20the%0Afinetuned%20model%20outperforms%20pretrained%20CLIP%20in%20all%20tasks%20and%20surpasses%0Azero-shot%20GPT-4o%20and%20LLaVA-Mistral%20in%20the%20captioning%20task.%20These%20findings%0Aconfirm%20that%20eliminating%20textual%20and%20positional%20biases%20fosters%20accurate%20edge%0Arecognition%20in%20VLMs%2C%20offering%20a%20promising%20path%20for%20advancing%20diagram%0Aunderstanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19944v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Visual%2520Encoder%2520Learn%2520to%2520See%2520Arrows%253F%26entry.906535625%3DNaoyuki%2520Terashita%2520and%2520Yusuke%2520Tozaki%2520and%2520Hideaki%2520Omote%2520and%2520Congkha%2520Nguyen%2520and%2520Ryosuke%2520Nakamoto%2520and%2520Yuta%2520Koreeda%2520and%2520Hiroaki%2520Ozaki%26entry.1292438233%3D%2520%2520The%2520diagram%2520is%2520a%2520visual%2520representation%2520of%2520a%2520relationship%2520illustrated%2520with%250Aedges%2520%2528lines%2520or%2520arrows%2529%252C%2520which%2520is%2520widely%2520used%2520in%2520industrial%2520and%2520scientific%250Acommunication.%2520Although%2520recognizing%2520diagrams%2520is%2520essential%2520for%2520vision%2520language%250Amodels%2520%2528VLMs%2529%2520to%2520comprehend%2520domain-specific%2520knowledge%252C%2520recent%2520studies%2520reveal%250Athat%2520many%2520VLMs%2520fail%2520to%2520identify%2520edges%2520in%2520images.%2520We%2520hypothesize%2520that%2520these%250Afailures%2520stem%2520from%2520an%2520over-reliance%2520on%2520textual%2520and%2520positional%2520biases%252C%250Apreventing%2520VLMs%2520from%2520learning%2520explicit%2520edge%2520features.%2520Based%2520on%2520this%2520idea%252C%2520we%250Aempirically%2520investigate%2520whether%2520the%2520image%2520encoder%2520in%2520VLMs%2520can%2520learn%2520edge%250Arepresentation%2520through%2520training%2520on%2520a%2520diagram%2520dataset%2520in%2520which%2520edges%2520are%2520biased%250Aneither%2520by%2520textual%2520nor%2520positional%2520information.%2520To%2520this%2520end%252C%2520we%2520conduct%250Acontrastive%2520learning%2520on%2520an%2520artificially%2520generated%2520diagram--caption%2520dataset%2520to%250Atrain%2520an%2520image%2520encoder%2520and%2520evaluate%2520its%2520diagram-related%2520features%2520on%2520three%250Atasks%253A%2520probing%252C%2520image%2520retrieval%252C%2520and%2520captioning.%2520Our%2520results%2520show%2520that%2520the%250Afinetuned%2520model%2520outperforms%2520pretrained%2520CLIP%2520in%2520all%2520tasks%2520and%2520surpasses%250Azero-shot%2520GPT-4o%2520and%2520LLaVA-Mistral%2520in%2520the%2520captioning%2520task.%2520These%2520findings%250Aconfirm%2520that%2520eliminating%2520textual%2520and%2520positional%2520biases%2520fosters%2520accurate%2520edge%250Arecognition%2520in%2520VLMs%252C%2520offering%2520a%2520promising%2520path%2520for%2520advancing%2520diagram%250Aunderstanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19944v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Visual%20Encoder%20Learn%20to%20See%20Arrows%3F&entry.906535625=Naoyuki%20Terashita%20and%20Yusuke%20Tozaki%20and%20Hideaki%20Omote%20and%20Congkha%20Nguyen%20and%20Ryosuke%20Nakamoto%20and%20Yuta%20Koreeda%20and%20Hiroaki%20Ozaki&entry.1292438233=%20%20The%20diagram%20is%20a%20visual%20representation%20of%20a%20relationship%20illustrated%20with%0Aedges%20%28lines%20or%20arrows%29%2C%20which%20is%20widely%20used%20in%20industrial%20and%20scientific%0Acommunication.%20Although%20recognizing%20diagrams%20is%20essential%20for%20vision%20language%0Amodels%20%28VLMs%29%20to%20comprehend%20domain-specific%20knowledge%2C%20recent%20studies%20reveal%0Athat%20many%20VLMs%20fail%20to%20identify%20edges%20in%20images.%20We%20hypothesize%20that%20these%0Afailures%20stem%20from%20an%20over-reliance%20on%20textual%20and%20positional%20biases%2C%0Apreventing%20VLMs%20from%20learning%20explicit%20edge%20features.%20Based%20on%20this%20idea%2C%20we%0Aempirically%20investigate%20whether%20the%20image%20encoder%20in%20VLMs%20can%20learn%20edge%0Arepresentation%20through%20training%20on%20a%20diagram%20dataset%20in%20which%20edges%20are%20biased%0Aneither%20by%20textual%20nor%20positional%20information.%20To%20this%20end%2C%20we%20conduct%0Acontrastive%20learning%20on%20an%20artificially%20generated%20diagram--caption%20dataset%20to%0Atrain%20an%20image%20encoder%20and%20evaluate%20its%20diagram-related%20features%20on%20three%0Atasks%3A%20probing%2C%20image%20retrieval%2C%20and%20captioning.%20Our%20results%20show%20that%20the%0Afinetuned%20model%20outperforms%20pretrained%20CLIP%20in%20all%20tasks%20and%20surpasses%0Azero-shot%20GPT-4o%20and%20LLaVA-Mistral%20in%20the%20captioning%20task.%20These%20findings%0Aconfirm%20that%20eliminating%20textual%20and%20positional%20biases%20fosters%20accurate%20edge%0Arecognition%20in%20VLMs%2C%20offering%20a%20promising%20path%20for%20advancing%20diagram%0Aunderstanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19944v1&entry.124074799=Read"},
{"title": "DualTalk: Dual-Speaker Interaction for 3D Talking Head Conversations", "author": "Ziqiao Peng and Yanbo Fan and Haoyu Wu and Xuan Wang and Hongyan Liu and Jun He and Zhaoxin Fan", "abstract": "  In face-to-face conversations, individuals need to switch between speaking\nand listening roles seamlessly. Existing 3D talking head generation models\nfocus solely on speaking or listening, neglecting the natural dynamics of\ninteractive conversation, which leads to unnatural interactions and awkward\ntransitions. To address this issue, we propose a new task -- multi-round\ndual-speaker interaction for 3D talking head generation -- which requires\nmodels to handle and generate both speaking and listening behaviors in\ncontinuous conversation. To solve this task, we introduce DualTalk, a novel\nunified framework that integrates the dynamic behaviors of speakers and\nlisteners to simulate realistic and coherent dialogue interactions. This\nframework not only synthesizes lifelike talking heads when speaking but also\ngenerates continuous and vivid non-verbal feedback when listening, effectively\ncapturing the interplay between the roles. We also create a new dataset\nfeaturing 50 hours of multi-round conversations with over 1,000 characters,\nwhere participants continuously switch between speaking and listening roles.\nExtensive experiments demonstrate that our method significantly enhances the\nnaturalness and expressiveness of 3D talking heads in dual-speaker\nconversations. We recommend watching the supplementary video:\nhttps://ziqiaopeng.github.io/dualtalk.\n", "link": "http://arxiv.org/abs/2505.18096v2", "date": "2025-05-26", "relevancy": 2.788, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.587}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5493}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DualTalk%3A%20Dual-Speaker%20Interaction%20for%203D%20Talking%20Head%20Conversations&body=Title%3A%20DualTalk%3A%20Dual-Speaker%20Interaction%20for%203D%20Talking%20Head%20Conversations%0AAuthor%3A%20Ziqiao%20Peng%20and%20Yanbo%20Fan%20and%20Haoyu%20Wu%20and%20Xuan%20Wang%20and%20Hongyan%20Liu%20and%20Jun%20He%20and%20Zhaoxin%20Fan%0AAbstract%3A%20%20%20In%20face-to-face%20conversations%2C%20individuals%20need%20to%20switch%20between%20speaking%0Aand%20listening%20roles%20seamlessly.%20Existing%203D%20talking%20head%20generation%20models%0Afocus%20solely%20on%20speaking%20or%20listening%2C%20neglecting%20the%20natural%20dynamics%20of%0Ainteractive%20conversation%2C%20which%20leads%20to%20unnatural%20interactions%20and%20awkward%0Atransitions.%20To%20address%20this%20issue%2C%20we%20propose%20a%20new%20task%20--%20multi-round%0Adual-speaker%20interaction%20for%203D%20talking%20head%20generation%20--%20which%20requires%0Amodels%20to%20handle%20and%20generate%20both%20speaking%20and%20listening%20behaviors%20in%0Acontinuous%20conversation.%20To%20solve%20this%20task%2C%20we%20introduce%20DualTalk%2C%20a%20novel%0Aunified%20framework%20that%20integrates%20the%20dynamic%20behaviors%20of%20speakers%20and%0Alisteners%20to%20simulate%20realistic%20and%20coherent%20dialogue%20interactions.%20This%0Aframework%20not%20only%20synthesizes%20lifelike%20talking%20heads%20when%20speaking%20but%20also%0Agenerates%20continuous%20and%20vivid%20non-verbal%20feedback%20when%20listening%2C%20effectively%0Acapturing%20the%20interplay%20between%20the%20roles.%20We%20also%20create%20a%20new%20dataset%0Afeaturing%2050%20hours%20of%20multi-round%20conversations%20with%20over%201%2C000%20characters%2C%0Awhere%20participants%20continuously%20switch%20between%20speaking%20and%20listening%20roles.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20significantly%20enhances%20the%0Anaturalness%20and%20expressiveness%20of%203D%20talking%20heads%20in%20dual-speaker%0Aconversations.%20We%20recommend%20watching%20the%20supplementary%20video%3A%0Ahttps%3A//ziqiaopeng.github.io/dualtalk.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18096v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDualTalk%253A%2520Dual-Speaker%2520Interaction%2520for%25203D%2520Talking%2520Head%2520Conversations%26entry.906535625%3DZiqiao%2520Peng%2520and%2520Yanbo%2520Fan%2520and%2520Haoyu%2520Wu%2520and%2520Xuan%2520Wang%2520and%2520Hongyan%2520Liu%2520and%2520Jun%2520He%2520and%2520Zhaoxin%2520Fan%26entry.1292438233%3D%2520%2520In%2520face-to-face%2520conversations%252C%2520individuals%2520need%2520to%2520switch%2520between%2520speaking%250Aand%2520listening%2520roles%2520seamlessly.%2520Existing%25203D%2520talking%2520head%2520generation%2520models%250Afocus%2520solely%2520on%2520speaking%2520or%2520listening%252C%2520neglecting%2520the%2520natural%2520dynamics%2520of%250Ainteractive%2520conversation%252C%2520which%2520leads%2520to%2520unnatural%2520interactions%2520and%2520awkward%250Atransitions.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520new%2520task%2520--%2520multi-round%250Adual-speaker%2520interaction%2520for%25203D%2520talking%2520head%2520generation%2520--%2520which%2520requires%250Amodels%2520to%2520handle%2520and%2520generate%2520both%2520speaking%2520and%2520listening%2520behaviors%2520in%250Acontinuous%2520conversation.%2520To%2520solve%2520this%2520task%252C%2520we%2520introduce%2520DualTalk%252C%2520a%2520novel%250Aunified%2520framework%2520that%2520integrates%2520the%2520dynamic%2520behaviors%2520of%2520speakers%2520and%250Alisteners%2520to%2520simulate%2520realistic%2520and%2520coherent%2520dialogue%2520interactions.%2520This%250Aframework%2520not%2520only%2520synthesizes%2520lifelike%2520talking%2520heads%2520when%2520speaking%2520but%2520also%250Agenerates%2520continuous%2520and%2520vivid%2520non-verbal%2520feedback%2520when%2520listening%252C%2520effectively%250Acapturing%2520the%2520interplay%2520between%2520the%2520roles.%2520We%2520also%2520create%2520a%2520new%2520dataset%250Afeaturing%252050%2520hours%2520of%2520multi-round%2520conversations%2520with%2520over%25201%252C000%2520characters%252C%250Awhere%2520participants%2520continuously%2520switch%2520between%2520speaking%2520and%2520listening%2520roles.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520significantly%2520enhances%2520the%250Anaturalness%2520and%2520expressiveness%2520of%25203D%2520talking%2520heads%2520in%2520dual-speaker%250Aconversations.%2520We%2520recommend%2520watching%2520the%2520supplementary%2520video%253A%250Ahttps%253A//ziqiaopeng.github.io/dualtalk.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18096v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DualTalk%3A%20Dual-Speaker%20Interaction%20for%203D%20Talking%20Head%20Conversations&entry.906535625=Ziqiao%20Peng%20and%20Yanbo%20Fan%20and%20Haoyu%20Wu%20and%20Xuan%20Wang%20and%20Hongyan%20Liu%20and%20Jun%20He%20and%20Zhaoxin%20Fan&entry.1292438233=%20%20In%20face-to-face%20conversations%2C%20individuals%20need%20to%20switch%20between%20speaking%0Aand%20listening%20roles%20seamlessly.%20Existing%203D%20talking%20head%20generation%20models%0Afocus%20solely%20on%20speaking%20or%20listening%2C%20neglecting%20the%20natural%20dynamics%20of%0Ainteractive%20conversation%2C%20which%20leads%20to%20unnatural%20interactions%20and%20awkward%0Atransitions.%20To%20address%20this%20issue%2C%20we%20propose%20a%20new%20task%20--%20multi-round%0Adual-speaker%20interaction%20for%203D%20talking%20head%20generation%20--%20which%20requires%0Amodels%20to%20handle%20and%20generate%20both%20speaking%20and%20listening%20behaviors%20in%0Acontinuous%20conversation.%20To%20solve%20this%20task%2C%20we%20introduce%20DualTalk%2C%20a%20novel%0Aunified%20framework%20that%20integrates%20the%20dynamic%20behaviors%20of%20speakers%20and%0Alisteners%20to%20simulate%20realistic%20and%20coherent%20dialogue%20interactions.%20This%0Aframework%20not%20only%20synthesizes%20lifelike%20talking%20heads%20when%20speaking%20but%20also%0Agenerates%20continuous%20and%20vivid%20non-verbal%20feedback%20when%20listening%2C%20effectively%0Acapturing%20the%20interplay%20between%20the%20roles.%20We%20also%20create%20a%20new%20dataset%0Afeaturing%2050%20hours%20of%20multi-round%20conversations%20with%20over%201%2C000%20characters%2C%0Awhere%20participants%20continuously%20switch%20between%20speaking%20and%20listening%20roles.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20significantly%20enhances%20the%0Anaturalness%20and%20expressiveness%20of%203D%20talking%20heads%20in%20dual-speaker%0Aconversations.%20We%20recommend%20watching%20the%20supplementary%20video%3A%0Ahttps%3A//ziqiaopeng.github.io/dualtalk.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18096v2&entry.124074799=Read"},
{"title": "Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion\n  Enhances Protein Representations", "author": "Hazem Alsamkary and Mohamed Elshaffei and Mohamed Elkerdawy and Ahmed Elnaggar", "abstract": "  Protein language models (PLMs) have emerged as powerful tools to detect\ncomplex patterns of protein sequences. However, the capability of PLMs to fully\ncapture information on protein sequences might be limited by focusing on single\npre-training tasks. Although adding data modalities or supervised objectives\ncan improve the performance of PLMs, pre-training often remains focused on\ndenoising corrupted sequences. To push the boundaries of PLMs, our research\ninvestigated a multi-task pre-training strategy. We developed Ankh3, a model\njointly optimized on two objectives: masked language modeling with multiple\nmasking probabilities and protein sequence completion relying only on protein\nsequences as input. This multi-task pre-training demonstrated that PLMs can\nlearn richer and more generalizable representations solely from protein\nsequences. The results demonstrated improved performance in downstream tasks,\nsuch as secondary structure prediction, fluorescence, GB1 fitness, and contact\nprediction. The integration of multiple tasks gave the model a more\ncomprehensive understanding of protein properties, leading to more robust and\naccurate predictions.\n", "link": "http://arxiv.org/abs/2505.20052v1", "date": "2025-05-26", "relevancy": 2.7787, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5712}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ankh3%3A%20Multi-Task%20Pretraining%20with%20Sequence%20Denoising%20and%20Completion%0A%20%20Enhances%20Protein%20Representations&body=Title%3A%20Ankh3%3A%20Multi-Task%20Pretraining%20with%20Sequence%20Denoising%20and%20Completion%0A%20%20Enhances%20Protein%20Representations%0AAuthor%3A%20Hazem%20Alsamkary%20and%20Mohamed%20Elshaffei%20and%20Mohamed%20Elkerdawy%20and%20Ahmed%20Elnaggar%0AAbstract%3A%20%20%20Protein%20language%20models%20%28PLMs%29%20have%20emerged%20as%20powerful%20tools%20to%20detect%0Acomplex%20patterns%20of%20protein%20sequences.%20However%2C%20the%20capability%20of%20PLMs%20to%20fully%0Acapture%20information%20on%20protein%20sequences%20might%20be%20limited%20by%20focusing%20on%20single%0Apre-training%20tasks.%20Although%20adding%20data%20modalities%20or%20supervised%20objectives%0Acan%20improve%20the%20performance%20of%20PLMs%2C%20pre-training%20often%20remains%20focused%20on%0Adenoising%20corrupted%20sequences.%20To%20push%20the%20boundaries%20of%20PLMs%2C%20our%20research%0Ainvestigated%20a%20multi-task%20pre-training%20strategy.%20We%20developed%20Ankh3%2C%20a%20model%0Ajointly%20optimized%20on%20two%20objectives%3A%20masked%20language%20modeling%20with%20multiple%0Amasking%20probabilities%20and%20protein%20sequence%20completion%20relying%20only%20on%20protein%0Asequences%20as%20input.%20This%20multi-task%20pre-training%20demonstrated%20that%20PLMs%20can%0Alearn%20richer%20and%20more%20generalizable%20representations%20solely%20from%20protein%0Asequences.%20The%20results%20demonstrated%20improved%20performance%20in%20downstream%20tasks%2C%0Asuch%20as%20secondary%20structure%20prediction%2C%20fluorescence%2C%20GB1%20fitness%2C%20and%20contact%0Aprediction.%20The%20integration%20of%20multiple%20tasks%20gave%20the%20model%20a%20more%0Acomprehensive%20understanding%20of%20protein%20properties%2C%20leading%20to%20more%20robust%20and%0Aaccurate%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnkh3%253A%2520Multi-Task%2520Pretraining%2520with%2520Sequence%2520Denoising%2520and%2520Completion%250A%2520%2520Enhances%2520Protein%2520Representations%26entry.906535625%3DHazem%2520Alsamkary%2520and%2520Mohamed%2520Elshaffei%2520and%2520Mohamed%2520Elkerdawy%2520and%2520Ahmed%2520Elnaggar%26entry.1292438233%3D%2520%2520Protein%2520language%2520models%2520%2528PLMs%2529%2520have%2520emerged%2520as%2520powerful%2520tools%2520to%2520detect%250Acomplex%2520patterns%2520of%2520protein%2520sequences.%2520However%252C%2520the%2520capability%2520of%2520PLMs%2520to%2520fully%250Acapture%2520information%2520on%2520protein%2520sequences%2520might%2520be%2520limited%2520by%2520focusing%2520on%2520single%250Apre-training%2520tasks.%2520Although%2520adding%2520data%2520modalities%2520or%2520supervised%2520objectives%250Acan%2520improve%2520the%2520performance%2520of%2520PLMs%252C%2520pre-training%2520often%2520remains%2520focused%2520on%250Adenoising%2520corrupted%2520sequences.%2520To%2520push%2520the%2520boundaries%2520of%2520PLMs%252C%2520our%2520research%250Ainvestigated%2520a%2520multi-task%2520pre-training%2520strategy.%2520We%2520developed%2520Ankh3%252C%2520a%2520model%250Ajointly%2520optimized%2520on%2520two%2520objectives%253A%2520masked%2520language%2520modeling%2520with%2520multiple%250Amasking%2520probabilities%2520and%2520protein%2520sequence%2520completion%2520relying%2520only%2520on%2520protein%250Asequences%2520as%2520input.%2520This%2520multi-task%2520pre-training%2520demonstrated%2520that%2520PLMs%2520can%250Alearn%2520richer%2520and%2520more%2520generalizable%2520representations%2520solely%2520from%2520protein%250Asequences.%2520The%2520results%2520demonstrated%2520improved%2520performance%2520in%2520downstream%2520tasks%252C%250Asuch%2520as%2520secondary%2520structure%2520prediction%252C%2520fluorescence%252C%2520GB1%2520fitness%252C%2520and%2520contact%250Aprediction.%2520The%2520integration%2520of%2520multiple%2520tasks%2520gave%2520the%2520model%2520a%2520more%250Acomprehensive%2520understanding%2520of%2520protein%2520properties%252C%2520leading%2520to%2520more%2520robust%2520and%250Aaccurate%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ankh3%3A%20Multi-Task%20Pretraining%20with%20Sequence%20Denoising%20and%20Completion%0A%20%20Enhances%20Protein%20Representations&entry.906535625=Hazem%20Alsamkary%20and%20Mohamed%20Elshaffei%20and%20Mohamed%20Elkerdawy%20and%20Ahmed%20Elnaggar&entry.1292438233=%20%20Protein%20language%20models%20%28PLMs%29%20have%20emerged%20as%20powerful%20tools%20to%20detect%0Acomplex%20patterns%20of%20protein%20sequences.%20However%2C%20the%20capability%20of%20PLMs%20to%20fully%0Acapture%20information%20on%20protein%20sequences%20might%20be%20limited%20by%20focusing%20on%20single%0Apre-training%20tasks.%20Although%20adding%20data%20modalities%20or%20supervised%20objectives%0Acan%20improve%20the%20performance%20of%20PLMs%2C%20pre-training%20often%20remains%20focused%20on%0Adenoising%20corrupted%20sequences.%20To%20push%20the%20boundaries%20of%20PLMs%2C%20our%20research%0Ainvestigated%20a%20multi-task%20pre-training%20strategy.%20We%20developed%20Ankh3%2C%20a%20model%0Ajointly%20optimized%20on%20two%20objectives%3A%20masked%20language%20modeling%20with%20multiple%0Amasking%20probabilities%20and%20protein%20sequence%20completion%20relying%20only%20on%20protein%0Asequences%20as%20input.%20This%20multi-task%20pre-training%20demonstrated%20that%20PLMs%20can%0Alearn%20richer%20and%20more%20generalizable%20representations%20solely%20from%20protein%0Asequences.%20The%20results%20demonstrated%20improved%20performance%20in%20downstream%20tasks%2C%0Asuch%20as%20secondary%20structure%20prediction%2C%20fluorescence%2C%20GB1%20fitness%2C%20and%20contact%0Aprediction.%20The%20integration%20of%20multiple%20tasks%20gave%20the%20model%20a%20more%0Acomprehensive%20understanding%20of%20protein%20properties%2C%20leading%20to%20more%20robust%20and%0Aaccurate%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20052v1&entry.124074799=Read"},
{"title": "Generalizable Prompt Learning of CLIP: A Brief Overview", "author": "Fangming Cui and Yonggang Zhang and Xuan Wang and Xule Wang and Liang Xiao", "abstract": "  Existing vision-language models (VLMs) such as CLIP have showcased an\nimpressive capability to generalize well across various downstream tasks. These\nmodels leverage the synergy between visual and textual information, enabling\nthem to understand and reason about the content present in images and text in a\nunified manner. This article provides a brief overview of CLIP based on\nfew-shot prompt learning, including experimental data and technical\ncharacteristics of some methods. The purpose of this review is to provide a\nreference for researchers who have just started their research in generalizable\nprompting of CLIP through few-shot training for classification across 15\ndatasets and also to facilitate the integration of this field by researchers in\nother downstream tasks.\n", "link": "http://arxiv.org/abs/2503.01263v5", "date": "2025-05-26", "relevancy": 2.7634, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5656}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5462}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizable%20Prompt%20Learning%20of%20CLIP%3A%20A%20Brief%20Overview&body=Title%3A%20Generalizable%20Prompt%20Learning%20of%20CLIP%3A%20A%20Brief%20Overview%0AAuthor%3A%20Fangming%20Cui%20and%20Yonggang%20Zhang%20and%20Xuan%20Wang%20and%20Xule%20Wang%20and%20Liang%20Xiao%0AAbstract%3A%20%20%20Existing%20vision-language%20models%20%28VLMs%29%20such%20as%20CLIP%20have%20showcased%20an%0Aimpressive%20capability%20to%20generalize%20well%20across%20various%20downstream%20tasks.%20These%0Amodels%20leverage%20the%20synergy%20between%20visual%20and%20textual%20information%2C%20enabling%0Athem%20to%20understand%20and%20reason%20about%20the%20content%20present%20in%20images%20and%20text%20in%20a%0Aunified%20manner.%20This%20article%20provides%20a%20brief%20overview%20of%20CLIP%20based%20on%0Afew-shot%20prompt%20learning%2C%20including%20experimental%20data%20and%20technical%0Acharacteristics%20of%20some%20methods.%20The%20purpose%20of%20this%20review%20is%20to%20provide%20a%0Areference%20for%20researchers%20who%20have%20just%20started%20their%20research%20in%20generalizable%0Aprompting%20of%20CLIP%20through%20few-shot%20training%20for%20classification%20across%2015%0Adatasets%20and%20also%20to%20facilitate%20the%20integration%20of%20this%20field%20by%20researchers%20in%0Aother%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.01263v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizable%2520Prompt%2520Learning%2520of%2520CLIP%253A%2520A%2520Brief%2520Overview%26entry.906535625%3DFangming%2520Cui%2520and%2520Yonggang%2520Zhang%2520and%2520Xuan%2520Wang%2520and%2520Xule%2520Wang%2520and%2520Liang%2520Xiao%26entry.1292438233%3D%2520%2520Existing%2520vision-language%2520models%2520%2528VLMs%2529%2520such%2520as%2520CLIP%2520have%2520showcased%2520an%250Aimpressive%2520capability%2520to%2520generalize%2520well%2520across%2520various%2520downstream%2520tasks.%2520These%250Amodels%2520leverage%2520the%2520synergy%2520between%2520visual%2520and%2520textual%2520information%252C%2520enabling%250Athem%2520to%2520understand%2520and%2520reason%2520about%2520the%2520content%2520present%2520in%2520images%2520and%2520text%2520in%2520a%250Aunified%2520manner.%2520This%2520article%2520provides%2520a%2520brief%2520overview%2520of%2520CLIP%2520based%2520on%250Afew-shot%2520prompt%2520learning%252C%2520including%2520experimental%2520data%2520and%2520technical%250Acharacteristics%2520of%2520some%2520methods.%2520The%2520purpose%2520of%2520this%2520review%2520is%2520to%2520provide%2520a%250Areference%2520for%2520researchers%2520who%2520have%2520just%2520started%2520their%2520research%2520in%2520generalizable%250Aprompting%2520of%2520CLIP%2520through%2520few-shot%2520training%2520for%2520classification%2520across%252015%250Adatasets%2520and%2520also%2520to%2520facilitate%2520the%2520integration%2520of%2520this%2520field%2520by%2520researchers%2520in%250Aother%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01263v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%20Prompt%20Learning%20of%20CLIP%3A%20A%20Brief%20Overview&entry.906535625=Fangming%20Cui%20and%20Yonggang%20Zhang%20and%20Xuan%20Wang%20and%20Xule%20Wang%20and%20Liang%20Xiao&entry.1292438233=%20%20Existing%20vision-language%20models%20%28VLMs%29%20such%20as%20CLIP%20have%20showcased%20an%0Aimpressive%20capability%20to%20generalize%20well%20across%20various%20downstream%20tasks.%20These%0Amodels%20leverage%20the%20synergy%20between%20visual%20and%20textual%20information%2C%20enabling%0Athem%20to%20understand%20and%20reason%20about%20the%20content%20present%20in%20images%20and%20text%20in%20a%0Aunified%20manner.%20This%20article%20provides%20a%20brief%20overview%20of%20CLIP%20based%20on%0Afew-shot%20prompt%20learning%2C%20including%20experimental%20data%20and%20technical%0Acharacteristics%20of%20some%20methods.%20The%20purpose%20of%20this%20review%20is%20to%20provide%20a%0Areference%20for%20researchers%20who%20have%20just%20started%20their%20research%20in%20generalizable%0Aprompting%20of%20CLIP%20through%20few-shot%20training%20for%20classification%20across%2015%0Adatasets%20and%20also%20to%20facilitate%20the%20integration%20of%20this%20field%20by%20researchers%20in%0Aother%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.01263v5&entry.124074799=Read"},
{"title": "Data-Free Class-Incremental Gesture Recognition with Prototype-Guided\n  Pseudo Feature Replay", "author": "Hongsong Wang and Ao Sun and Jie Gui and Liang Wang", "abstract": "  Gesture recognition is an important research area in the field of computer\nvision. Most gesture recognition efforts focus on close-set scenarios, thereby\nlimiting the capacity to effectively handle unseen or novel gestures. We aim to\naddress class-incremental gesture recognition, which entails the ability to\naccommodate new and previously unseen gestures over time. Specifically, we\nintroduce a Prototype-Guided Pseudo Feature Replay (PGPFR) framework for\ndata-free class-incremental gesture recognition. This framework comprises four\ncomponents: Pseudo Feature Generation with Batch Prototypes (PFGBP),\nVariational Prototype Replay (VPR) for old classes, Truncated Cross-Entropy\n(TCE) for new classes, and Continual Classifier Re-Training (CCRT). To tackle\nthe issue of catastrophic forgetting, the PFGBP dynamically generates a\ndiversity of pseudo features in an online manner, leveraging class prototypes\nof old classes along with batch class prototypes of new classes. Furthermore,\nthe VPR enforces consistency between the classifier's weights and the\nprototypes of old classes, leveraging class prototypes and covariance matrices\nto enhance robustness and generalization capabilities. The TCE mitigates the\nimpact of domain differences of the classifier caused by pseudo features.\nFinally, the CCRT training strategy is designed to prevent overfitting to new\nclasses and ensure the stability of features extracted from old classes.\nExtensive experiments conducted on two widely used gesture recognition\ndatasets, namely SHREC 2017 3D and EgoGesture 3D, demonstrate that our approach\noutperforms existing state-of-the-art methods by 11.8\\% and 12.8\\% in terms of\nmean global accuracy, respectively. The code is available on\nhttps://github.com/sunao-101/PGPFR-3/.\n", "link": "http://arxiv.org/abs/2505.20049v1", "date": "2025-05-26", "relevancy": 2.7613, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5748}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5456}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Free%20Class-Incremental%20Gesture%20Recognition%20with%20Prototype-Guided%0A%20%20Pseudo%20Feature%20Replay&body=Title%3A%20Data-Free%20Class-Incremental%20Gesture%20Recognition%20with%20Prototype-Guided%0A%20%20Pseudo%20Feature%20Replay%0AAuthor%3A%20Hongsong%20Wang%20and%20Ao%20Sun%20and%20Jie%20Gui%20and%20Liang%20Wang%0AAbstract%3A%20%20%20Gesture%20recognition%20is%20an%20important%20research%20area%20in%20the%20field%20of%20computer%0Avision.%20Most%20gesture%20recognition%20efforts%20focus%20on%20close-set%20scenarios%2C%20thereby%0Alimiting%20the%20capacity%20to%20effectively%20handle%20unseen%20or%20novel%20gestures.%20We%20aim%20to%0Aaddress%20class-incremental%20gesture%20recognition%2C%20which%20entails%20the%20ability%20to%0Aaccommodate%20new%20and%20previously%20unseen%20gestures%20over%20time.%20Specifically%2C%20we%0Aintroduce%20a%20Prototype-Guided%20Pseudo%20Feature%20Replay%20%28PGPFR%29%20framework%20for%0Adata-free%20class-incremental%20gesture%20recognition.%20This%20framework%20comprises%20four%0Acomponents%3A%20Pseudo%20Feature%20Generation%20with%20Batch%20Prototypes%20%28PFGBP%29%2C%0AVariational%20Prototype%20Replay%20%28VPR%29%20for%20old%20classes%2C%20Truncated%20Cross-Entropy%0A%28TCE%29%20for%20new%20classes%2C%20and%20Continual%20Classifier%20Re-Training%20%28CCRT%29.%20To%20tackle%0Athe%20issue%20of%20catastrophic%20forgetting%2C%20the%20PFGBP%20dynamically%20generates%20a%0Adiversity%20of%20pseudo%20features%20in%20an%20online%20manner%2C%20leveraging%20class%20prototypes%0Aof%20old%20classes%20along%20with%20batch%20class%20prototypes%20of%20new%20classes.%20Furthermore%2C%0Athe%20VPR%20enforces%20consistency%20between%20the%20classifier%27s%20weights%20and%20the%0Aprototypes%20of%20old%20classes%2C%20leveraging%20class%20prototypes%20and%20covariance%20matrices%0Ato%20enhance%20robustness%20and%20generalization%20capabilities.%20The%20TCE%20mitigates%20the%0Aimpact%20of%20domain%20differences%20of%20the%20classifier%20caused%20by%20pseudo%20features.%0AFinally%2C%20the%20CCRT%20training%20strategy%20is%20designed%20to%20prevent%20overfitting%20to%20new%0Aclasses%20and%20ensure%20the%20stability%20of%20features%20extracted%20from%20old%20classes.%0AExtensive%20experiments%20conducted%20on%20two%20widely%20used%20gesture%20recognition%0Adatasets%2C%20namely%20SHREC%202017%203D%20and%20EgoGesture%203D%2C%20demonstrate%20that%20our%20approach%0Aoutperforms%20existing%20state-of-the-art%20methods%20by%2011.8%5C%25%20and%2012.8%5C%25%20in%20terms%20of%0Amean%20global%20accuracy%2C%20respectively.%20The%20code%20is%20available%20on%0Ahttps%3A//github.com/sunao-101/PGPFR-3/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Free%2520Class-Incremental%2520Gesture%2520Recognition%2520with%2520Prototype-Guided%250A%2520%2520Pseudo%2520Feature%2520Replay%26entry.906535625%3DHongsong%2520Wang%2520and%2520Ao%2520Sun%2520and%2520Jie%2520Gui%2520and%2520Liang%2520Wang%26entry.1292438233%3D%2520%2520Gesture%2520recognition%2520is%2520an%2520important%2520research%2520area%2520in%2520the%2520field%2520of%2520computer%250Avision.%2520Most%2520gesture%2520recognition%2520efforts%2520focus%2520on%2520close-set%2520scenarios%252C%2520thereby%250Alimiting%2520the%2520capacity%2520to%2520effectively%2520handle%2520unseen%2520or%2520novel%2520gestures.%2520We%2520aim%2520to%250Aaddress%2520class-incremental%2520gesture%2520recognition%252C%2520which%2520entails%2520the%2520ability%2520to%250Aaccommodate%2520new%2520and%2520previously%2520unseen%2520gestures%2520over%2520time.%2520Specifically%252C%2520we%250Aintroduce%2520a%2520Prototype-Guided%2520Pseudo%2520Feature%2520Replay%2520%2528PGPFR%2529%2520framework%2520for%250Adata-free%2520class-incremental%2520gesture%2520recognition.%2520This%2520framework%2520comprises%2520four%250Acomponents%253A%2520Pseudo%2520Feature%2520Generation%2520with%2520Batch%2520Prototypes%2520%2528PFGBP%2529%252C%250AVariational%2520Prototype%2520Replay%2520%2528VPR%2529%2520for%2520old%2520classes%252C%2520Truncated%2520Cross-Entropy%250A%2528TCE%2529%2520for%2520new%2520classes%252C%2520and%2520Continual%2520Classifier%2520Re-Training%2520%2528CCRT%2529.%2520To%2520tackle%250Athe%2520issue%2520of%2520catastrophic%2520forgetting%252C%2520the%2520PFGBP%2520dynamically%2520generates%2520a%250Adiversity%2520of%2520pseudo%2520features%2520in%2520an%2520online%2520manner%252C%2520leveraging%2520class%2520prototypes%250Aof%2520old%2520classes%2520along%2520with%2520batch%2520class%2520prototypes%2520of%2520new%2520classes.%2520Furthermore%252C%250Athe%2520VPR%2520enforces%2520consistency%2520between%2520the%2520classifier%2527s%2520weights%2520and%2520the%250Aprototypes%2520of%2520old%2520classes%252C%2520leveraging%2520class%2520prototypes%2520and%2520covariance%2520matrices%250Ato%2520enhance%2520robustness%2520and%2520generalization%2520capabilities.%2520The%2520TCE%2520mitigates%2520the%250Aimpact%2520of%2520domain%2520differences%2520of%2520the%2520classifier%2520caused%2520by%2520pseudo%2520features.%250AFinally%252C%2520the%2520CCRT%2520training%2520strategy%2520is%2520designed%2520to%2520prevent%2520overfitting%2520to%2520new%250Aclasses%2520and%2520ensure%2520the%2520stability%2520of%2520features%2520extracted%2520from%2520old%2520classes.%250AExtensive%2520experiments%2520conducted%2520on%2520two%2520widely%2520used%2520gesture%2520recognition%250Adatasets%252C%2520namely%2520SHREC%25202017%25203D%2520and%2520EgoGesture%25203D%252C%2520demonstrate%2520that%2520our%2520approach%250Aoutperforms%2520existing%2520state-of-the-art%2520methods%2520by%252011.8%255C%2525%2520and%252012.8%255C%2525%2520in%2520terms%2520of%250Amean%2520global%2520accuracy%252C%2520respectively.%2520The%2520code%2520is%2520available%2520on%250Ahttps%253A//github.com/sunao-101/PGPFR-3/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Free%20Class-Incremental%20Gesture%20Recognition%20with%20Prototype-Guided%0A%20%20Pseudo%20Feature%20Replay&entry.906535625=Hongsong%20Wang%20and%20Ao%20Sun%20and%20Jie%20Gui%20and%20Liang%20Wang&entry.1292438233=%20%20Gesture%20recognition%20is%20an%20important%20research%20area%20in%20the%20field%20of%20computer%0Avision.%20Most%20gesture%20recognition%20efforts%20focus%20on%20close-set%20scenarios%2C%20thereby%0Alimiting%20the%20capacity%20to%20effectively%20handle%20unseen%20or%20novel%20gestures.%20We%20aim%20to%0Aaddress%20class-incremental%20gesture%20recognition%2C%20which%20entails%20the%20ability%20to%0Aaccommodate%20new%20and%20previously%20unseen%20gestures%20over%20time.%20Specifically%2C%20we%0Aintroduce%20a%20Prototype-Guided%20Pseudo%20Feature%20Replay%20%28PGPFR%29%20framework%20for%0Adata-free%20class-incremental%20gesture%20recognition.%20This%20framework%20comprises%20four%0Acomponents%3A%20Pseudo%20Feature%20Generation%20with%20Batch%20Prototypes%20%28PFGBP%29%2C%0AVariational%20Prototype%20Replay%20%28VPR%29%20for%20old%20classes%2C%20Truncated%20Cross-Entropy%0A%28TCE%29%20for%20new%20classes%2C%20and%20Continual%20Classifier%20Re-Training%20%28CCRT%29.%20To%20tackle%0Athe%20issue%20of%20catastrophic%20forgetting%2C%20the%20PFGBP%20dynamically%20generates%20a%0Adiversity%20of%20pseudo%20features%20in%20an%20online%20manner%2C%20leveraging%20class%20prototypes%0Aof%20old%20classes%20along%20with%20batch%20class%20prototypes%20of%20new%20classes.%20Furthermore%2C%0Athe%20VPR%20enforces%20consistency%20between%20the%20classifier%27s%20weights%20and%20the%0Aprototypes%20of%20old%20classes%2C%20leveraging%20class%20prototypes%20and%20covariance%20matrices%0Ato%20enhance%20robustness%20and%20generalization%20capabilities.%20The%20TCE%20mitigates%20the%0Aimpact%20of%20domain%20differences%20of%20the%20classifier%20caused%20by%20pseudo%20features.%0AFinally%2C%20the%20CCRT%20training%20strategy%20is%20designed%20to%20prevent%20overfitting%20to%20new%0Aclasses%20and%20ensure%20the%20stability%20of%20features%20extracted%20from%20old%20classes.%0AExtensive%20experiments%20conducted%20on%20two%20widely%20used%20gesture%20recognition%0Adatasets%2C%20namely%20SHREC%202017%203D%20and%20EgoGesture%203D%2C%20demonstrate%20that%20our%20approach%0Aoutperforms%20existing%20state-of-the-art%20methods%20by%2011.8%5C%25%20and%2012.8%5C%25%20in%20terms%20of%0Amean%20global%20accuracy%2C%20respectively.%20The%20code%20is%20available%20on%0Ahttps%3A//github.com/sunao-101/PGPFR-3/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20049v1&entry.124074799=Read"},
{"title": "Efficient Training-Free High-Resolution Synthesis with Energy\n  Rectification in Diffusion Models", "author": "Zhen Yang and Guibao Shen and Minyang Li and Liang Hou and Mushui Liu and Luozhou Wang and Xin Tao and Pengfei Wan and Di Zhang and Ying-Cong Chen", "abstract": "  Diffusion models have achieved remarkable progress across various visual\ngeneration tasks. However, their performance significantly declines when\ngenerating content at resolutions higher than those used during training.\nAlthough numerous methods have been proposed to enable high-resolution\ngeneration, they all suffer from inefficiency. In this paper, we propose\nRectifiedHR, a straightforward and efficient solution for training-free\nhigh-resolution synthesis. Specifically, we propose a noise refresh strategy\nthat unlocks the model's training-free high-resolution synthesis capability and\nimproves efficiency. Additionally, we are the first to observe the phenomenon\nof energy decay, which may cause image blurriness during the high-resolution\nsynthesis process. To address this issue, we introduce average latent energy\nanalysis and find that tuning the classifier-free guidance hyperparameter can\nsignificantly improve generation performance. Our method is entirely\ntraining-free and demonstrates efficient performance. Furthermore, we show that\nRectifiedHR is compatible with various diffusion model techniques, enabling\nadvanced features such as image editing, customized generation, and video\nsynthesis. Extensive comparisons with numerous baseline methods validate the\nsuperior effectiveness and efficiency of RectifiedHR.\n", "link": "http://arxiv.org/abs/2503.02537v3", "date": "2025-05-26", "relevancy": 2.756, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.7171}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6961}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Training-Free%20High-Resolution%20Synthesis%20with%20Energy%0A%20%20Rectification%20in%20Diffusion%20Models&body=Title%3A%20Efficient%20Training-Free%20High-Resolution%20Synthesis%20with%20Energy%0A%20%20Rectification%20in%20Diffusion%20Models%0AAuthor%3A%20Zhen%20Yang%20and%20Guibao%20Shen%20and%20Minyang%20Li%20and%20Liang%20Hou%20and%20Mushui%20Liu%20and%20Luozhou%20Wang%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Ying-Cong%20Chen%0AAbstract%3A%20%20%20Diffusion%20models%20have%20achieved%20remarkable%20progress%20across%20various%20visual%0Ageneration%20tasks.%20However%2C%20their%20performance%20significantly%20declines%20when%0Agenerating%20content%20at%20resolutions%20higher%20than%20those%20used%20during%20training.%0AAlthough%20numerous%20methods%20have%20been%20proposed%20to%20enable%20high-resolution%0Ageneration%2C%20they%20all%20suffer%20from%20inefficiency.%20In%20this%20paper%2C%20we%20propose%0ARectifiedHR%2C%20a%20straightforward%20and%20efficient%20solution%20for%20training-free%0Ahigh-resolution%20synthesis.%20Specifically%2C%20we%20propose%20a%20noise%20refresh%20strategy%0Athat%20unlocks%20the%20model%27s%20training-free%20high-resolution%20synthesis%20capability%20and%0Aimproves%20efficiency.%20Additionally%2C%20we%20are%20the%20first%20to%20observe%20the%20phenomenon%0Aof%20energy%20decay%2C%20which%20may%20cause%20image%20blurriness%20during%20the%20high-resolution%0Asynthesis%20process.%20To%20address%20this%20issue%2C%20we%20introduce%20average%20latent%20energy%0Aanalysis%20and%20find%20that%20tuning%20the%20classifier-free%20guidance%20hyperparameter%20can%0Asignificantly%20improve%20generation%20performance.%20Our%20method%20is%20entirely%0Atraining-free%20and%20demonstrates%20efficient%20performance.%20Furthermore%2C%20we%20show%20that%0ARectifiedHR%20is%20compatible%20with%20various%20diffusion%20model%20techniques%2C%20enabling%0Aadvanced%20features%20such%20as%20image%20editing%2C%20customized%20generation%2C%20and%20video%0Asynthesis.%20Extensive%20comparisons%20with%20numerous%20baseline%20methods%20validate%20the%0Asuperior%20effectiveness%20and%20efficiency%20of%20RectifiedHR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.02537v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Training-Free%2520High-Resolution%2520Synthesis%2520with%2520Energy%250A%2520%2520Rectification%2520in%2520Diffusion%2520Models%26entry.906535625%3DZhen%2520Yang%2520and%2520Guibao%2520Shen%2520and%2520Minyang%2520Li%2520and%2520Liang%2520Hou%2520and%2520Mushui%2520Liu%2520and%2520Luozhou%2520Wang%2520and%2520Xin%2520Tao%2520and%2520Pengfei%2520Wan%2520and%2520Di%2520Zhang%2520and%2520Ying-Cong%2520Chen%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520achieved%2520remarkable%2520progress%2520across%2520various%2520visual%250Ageneration%2520tasks.%2520However%252C%2520their%2520performance%2520significantly%2520declines%2520when%250Agenerating%2520content%2520at%2520resolutions%2520higher%2520than%2520those%2520used%2520during%2520training.%250AAlthough%2520numerous%2520methods%2520have%2520been%2520proposed%2520to%2520enable%2520high-resolution%250Ageneration%252C%2520they%2520all%2520suffer%2520from%2520inefficiency.%2520In%2520this%2520paper%252C%2520we%2520propose%250ARectifiedHR%252C%2520a%2520straightforward%2520and%2520efficient%2520solution%2520for%2520training-free%250Ahigh-resolution%2520synthesis.%2520Specifically%252C%2520we%2520propose%2520a%2520noise%2520refresh%2520strategy%250Athat%2520unlocks%2520the%2520model%2527s%2520training-free%2520high-resolution%2520synthesis%2520capability%2520and%250Aimproves%2520efficiency.%2520Additionally%252C%2520we%2520are%2520the%2520first%2520to%2520observe%2520the%2520phenomenon%250Aof%2520energy%2520decay%252C%2520which%2520may%2520cause%2520image%2520blurriness%2520during%2520the%2520high-resolution%250Asynthesis%2520process.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520average%2520latent%2520energy%250Aanalysis%2520and%2520find%2520that%2520tuning%2520the%2520classifier-free%2520guidance%2520hyperparameter%2520can%250Asignificantly%2520improve%2520generation%2520performance.%2520Our%2520method%2520is%2520entirely%250Atraining-free%2520and%2520demonstrates%2520efficient%2520performance.%2520Furthermore%252C%2520we%2520show%2520that%250ARectifiedHR%2520is%2520compatible%2520with%2520various%2520diffusion%2520model%2520techniques%252C%2520enabling%250Aadvanced%2520features%2520such%2520as%2520image%2520editing%252C%2520customized%2520generation%252C%2520and%2520video%250Asynthesis.%2520Extensive%2520comparisons%2520with%2520numerous%2520baseline%2520methods%2520validate%2520the%250Asuperior%2520effectiveness%2520and%2520efficiency%2520of%2520RectifiedHR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.02537v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Training-Free%20High-Resolution%20Synthesis%20with%20Energy%0A%20%20Rectification%20in%20Diffusion%20Models&entry.906535625=Zhen%20Yang%20and%20Guibao%20Shen%20and%20Minyang%20Li%20and%20Liang%20Hou%20and%20Mushui%20Liu%20and%20Luozhou%20Wang%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Ying-Cong%20Chen&entry.1292438233=%20%20Diffusion%20models%20have%20achieved%20remarkable%20progress%20across%20various%20visual%0Ageneration%20tasks.%20However%2C%20their%20performance%20significantly%20declines%20when%0Agenerating%20content%20at%20resolutions%20higher%20than%20those%20used%20during%20training.%0AAlthough%20numerous%20methods%20have%20been%20proposed%20to%20enable%20high-resolution%0Ageneration%2C%20they%20all%20suffer%20from%20inefficiency.%20In%20this%20paper%2C%20we%20propose%0ARectifiedHR%2C%20a%20straightforward%20and%20efficient%20solution%20for%20training-free%0Ahigh-resolution%20synthesis.%20Specifically%2C%20we%20propose%20a%20noise%20refresh%20strategy%0Athat%20unlocks%20the%20model%27s%20training-free%20high-resolution%20synthesis%20capability%20and%0Aimproves%20efficiency.%20Additionally%2C%20we%20are%20the%20first%20to%20observe%20the%20phenomenon%0Aof%20energy%20decay%2C%20which%20may%20cause%20image%20blurriness%20during%20the%20high-resolution%0Asynthesis%20process.%20To%20address%20this%20issue%2C%20we%20introduce%20average%20latent%20energy%0Aanalysis%20and%20find%20that%20tuning%20the%20classifier-free%20guidance%20hyperparameter%20can%0Asignificantly%20improve%20generation%20performance.%20Our%20method%20is%20entirely%0Atraining-free%20and%20demonstrates%20efficient%20performance.%20Furthermore%2C%20we%20show%20that%0ARectifiedHR%20is%20compatible%20with%20various%20diffusion%20model%20techniques%2C%20enabling%0Aadvanced%20features%20such%20as%20image%20editing%2C%20customized%20generation%2C%20and%20video%0Asynthesis.%20Extensive%20comparisons%20with%20numerous%20baseline%20methods%20validate%20the%0Asuperior%20effectiveness%20and%20efficiency%20of%20RectifiedHR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.02537v3&entry.124074799=Read"},
{"title": "Advancements in Medical Image Classification through Fine-Tuning Natural\n  Domain Foundation Models", "author": "Mobina Mansoori and Sajjad Shahabodini and Farnoush Bayatmakou and Jamshid Abouei and Konstantinos N. Plataniotis and Arash Mohammadi", "abstract": "  Using massive datasets, foundation models are large-scale, pre-trained models\nthat perform a wide range of tasks. These models have shown consistently\nimproved results with the introduction of new methods. It is crucial to analyze\nhow these trends impact the medical field and determine whether these\nadvancements can drive meaningful change. This study investigates the\napplication of recent state-of-the-art foundation models, DINOv2, MAE, VMamba,\nCoCa, SAM2, and AIMv2, for medical image classification. We explore their\neffectiveness on datasets including CBIS-DDSM for mammography, ISIC2019 for\nskin lesions, APTOS2019 for diabetic retinopathy, and CHEXPERT for chest\nradiographs. By fine-tuning these models and evaluating their configurations,\nwe aim to understand the potential of these advancements in medical image\nclassification. The results indicate that these advanced models significantly\nenhance classification outcomes, demonstrating robust performance despite\nlimited labeled data. Based on our results, AIMv2, DINOv2, and SAM2 models\noutperformed others, demonstrating that progress in natural domain training has\npositively impacted the medical domain and improved classification outcomes.\nOur code is publicly available at:\nhttps://github.com/sajjad-sh33/Medical-Transfer-Learning.\n", "link": "http://arxiv.org/abs/2505.19779v1", "date": "2025-05-26", "relevancy": 2.7416, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5473}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancements%20in%20Medical%20Image%20Classification%20through%20Fine-Tuning%20Natural%0A%20%20Domain%20Foundation%20Models&body=Title%3A%20Advancements%20in%20Medical%20Image%20Classification%20through%20Fine-Tuning%20Natural%0A%20%20Domain%20Foundation%20Models%0AAuthor%3A%20Mobina%20Mansoori%20and%20Sajjad%20Shahabodini%20and%20Farnoush%20Bayatmakou%20and%20Jamshid%20Abouei%20and%20Konstantinos%20N.%20Plataniotis%20and%20Arash%20Mohammadi%0AAbstract%3A%20%20%20Using%20massive%20datasets%2C%20foundation%20models%20are%20large-scale%2C%20pre-trained%20models%0Athat%20perform%20a%20wide%20range%20of%20tasks.%20These%20models%20have%20shown%20consistently%0Aimproved%20results%20with%20the%20introduction%20of%20new%20methods.%20It%20is%20crucial%20to%20analyze%0Ahow%20these%20trends%20impact%20the%20medical%20field%20and%20determine%20whether%20these%0Aadvancements%20can%20drive%20meaningful%20change.%20This%20study%20investigates%20the%0Aapplication%20of%20recent%20state-of-the-art%20foundation%20models%2C%20DINOv2%2C%20MAE%2C%20VMamba%2C%0ACoCa%2C%20SAM2%2C%20and%20AIMv2%2C%20for%20medical%20image%20classification.%20We%20explore%20their%0Aeffectiveness%20on%20datasets%20including%20CBIS-DDSM%20for%20mammography%2C%20ISIC2019%20for%0Askin%20lesions%2C%20APTOS2019%20for%20diabetic%20retinopathy%2C%20and%20CHEXPERT%20for%20chest%0Aradiographs.%20By%20fine-tuning%20these%20models%20and%20evaluating%20their%20configurations%2C%0Awe%20aim%20to%20understand%20the%20potential%20of%20these%20advancements%20in%20medical%20image%0Aclassification.%20The%20results%20indicate%20that%20these%20advanced%20models%20significantly%0Aenhance%20classification%20outcomes%2C%20demonstrating%20robust%20performance%20despite%0Alimited%20labeled%20data.%20Based%20on%20our%20results%2C%20AIMv2%2C%20DINOv2%2C%20and%20SAM2%20models%0Aoutperformed%20others%2C%20demonstrating%20that%20progress%20in%20natural%20domain%20training%20has%0Apositively%20impacted%20the%20medical%20domain%20and%20improved%20classification%20outcomes.%0AOur%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/sajjad-sh33/Medical-Transfer-Learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancements%2520in%2520Medical%2520Image%2520Classification%2520through%2520Fine-Tuning%2520Natural%250A%2520%2520Domain%2520Foundation%2520Models%26entry.906535625%3DMobina%2520Mansoori%2520and%2520Sajjad%2520Shahabodini%2520and%2520Farnoush%2520Bayatmakou%2520and%2520Jamshid%2520Abouei%2520and%2520Konstantinos%2520N.%2520Plataniotis%2520and%2520Arash%2520Mohammadi%26entry.1292438233%3D%2520%2520Using%2520massive%2520datasets%252C%2520foundation%2520models%2520are%2520large-scale%252C%2520pre-trained%2520models%250Athat%2520perform%2520a%2520wide%2520range%2520of%2520tasks.%2520These%2520models%2520have%2520shown%2520consistently%250Aimproved%2520results%2520with%2520the%2520introduction%2520of%2520new%2520methods.%2520It%2520is%2520crucial%2520to%2520analyze%250Ahow%2520these%2520trends%2520impact%2520the%2520medical%2520field%2520and%2520determine%2520whether%2520these%250Aadvancements%2520can%2520drive%2520meaningful%2520change.%2520This%2520study%2520investigates%2520the%250Aapplication%2520of%2520recent%2520state-of-the-art%2520foundation%2520models%252C%2520DINOv2%252C%2520MAE%252C%2520VMamba%252C%250ACoCa%252C%2520SAM2%252C%2520and%2520AIMv2%252C%2520for%2520medical%2520image%2520classification.%2520We%2520explore%2520their%250Aeffectiveness%2520on%2520datasets%2520including%2520CBIS-DDSM%2520for%2520mammography%252C%2520ISIC2019%2520for%250Askin%2520lesions%252C%2520APTOS2019%2520for%2520diabetic%2520retinopathy%252C%2520and%2520CHEXPERT%2520for%2520chest%250Aradiographs.%2520By%2520fine-tuning%2520these%2520models%2520and%2520evaluating%2520their%2520configurations%252C%250Awe%2520aim%2520to%2520understand%2520the%2520potential%2520of%2520these%2520advancements%2520in%2520medical%2520image%250Aclassification.%2520The%2520results%2520indicate%2520that%2520these%2520advanced%2520models%2520significantly%250Aenhance%2520classification%2520outcomes%252C%2520demonstrating%2520robust%2520performance%2520despite%250Alimited%2520labeled%2520data.%2520Based%2520on%2520our%2520results%252C%2520AIMv2%252C%2520DINOv2%252C%2520and%2520SAM2%2520models%250Aoutperformed%2520others%252C%2520demonstrating%2520that%2520progress%2520in%2520natural%2520domain%2520training%2520has%250Apositively%2520impacted%2520the%2520medical%2520domain%2520and%2520improved%2520classification%2520outcomes.%250AOur%2520code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/sajjad-sh33/Medical-Transfer-Learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancements%20in%20Medical%20Image%20Classification%20through%20Fine-Tuning%20Natural%0A%20%20Domain%20Foundation%20Models&entry.906535625=Mobina%20Mansoori%20and%20Sajjad%20Shahabodini%20and%20Farnoush%20Bayatmakou%20and%20Jamshid%20Abouei%20and%20Konstantinos%20N.%20Plataniotis%20and%20Arash%20Mohammadi&entry.1292438233=%20%20Using%20massive%20datasets%2C%20foundation%20models%20are%20large-scale%2C%20pre-trained%20models%0Athat%20perform%20a%20wide%20range%20of%20tasks.%20These%20models%20have%20shown%20consistently%0Aimproved%20results%20with%20the%20introduction%20of%20new%20methods.%20It%20is%20crucial%20to%20analyze%0Ahow%20these%20trends%20impact%20the%20medical%20field%20and%20determine%20whether%20these%0Aadvancements%20can%20drive%20meaningful%20change.%20This%20study%20investigates%20the%0Aapplication%20of%20recent%20state-of-the-art%20foundation%20models%2C%20DINOv2%2C%20MAE%2C%20VMamba%2C%0ACoCa%2C%20SAM2%2C%20and%20AIMv2%2C%20for%20medical%20image%20classification.%20We%20explore%20their%0Aeffectiveness%20on%20datasets%20including%20CBIS-DDSM%20for%20mammography%2C%20ISIC2019%20for%0Askin%20lesions%2C%20APTOS2019%20for%20diabetic%20retinopathy%2C%20and%20CHEXPERT%20for%20chest%0Aradiographs.%20By%20fine-tuning%20these%20models%20and%20evaluating%20their%20configurations%2C%0Awe%20aim%20to%20understand%20the%20potential%20of%20these%20advancements%20in%20medical%20image%0Aclassification.%20The%20results%20indicate%20that%20these%20advanced%20models%20significantly%0Aenhance%20classification%20outcomes%2C%20demonstrating%20robust%20performance%20despite%0Alimited%20labeled%20data.%20Based%20on%20our%20results%2C%20AIMv2%2C%20DINOv2%2C%20and%20SAM2%20models%0Aoutperformed%20others%2C%20demonstrating%20that%20progress%20in%20natural%20domain%20training%20has%0Apositively%20impacted%20the%20medical%20domain%20and%20improved%20classification%20outcomes.%0AOur%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/sajjad-sh33/Medical-Transfer-Learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19779v1&entry.124074799=Read"},
{"title": "APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text\n  Summarization", "author": "Javier Mar\u00edn", "abstract": "  We present Adjacent Possible Exploration (APE), a simple yet effective method\nfor adapting large language models to specific tasks using minimal\ncomputational resources. Unlike traditional fine-tuning that requires extensive\ncompute, APE iteratively fine-tunes models on small, carefully selected data\nbatches (200 examples), retaining only improvements. On news summarization, APE\nachieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes,\nmatching or exceeding more complex methods like LoRA while remaining\nconceptually simple. Our approach is particularly valuable for researchers and\npractitioners with limited computational resources. We provide open-source code\nand demonstrate APE's effectiveness through both automatic metrics and human\nevaluation. While inspired by evolutionary theory's \"adjacent possible\", APE's\ncore insight has a very practical application: small, iterative data\nperturbations can efficiently guide LLMs toward task-specific performance\nwithout expensive retraining.\n", "link": "http://arxiv.org/abs/2505.19912v1", "date": "2025-05-26", "relevancy": 2.6959, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5544}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20APE%3A%20A%20Data-Centric%20Benchmark%20for%20Efficient%20LLM%20Adaptation%20in%20Text%0A%20%20Summarization&body=Title%3A%20APE%3A%20A%20Data-Centric%20Benchmark%20for%20Efficient%20LLM%20Adaptation%20in%20Text%0A%20%20Summarization%0AAuthor%3A%20Javier%20Mar%C3%ADn%0AAbstract%3A%20%20%20We%20present%20Adjacent%20Possible%20Exploration%20%28APE%29%2C%20a%20simple%20yet%20effective%20method%0Afor%20adapting%20large%20language%20models%20to%20specific%20tasks%20using%20minimal%0Acomputational%20resources.%20Unlike%20traditional%20fine-tuning%20that%20requires%20extensive%0Acompute%2C%20APE%20iteratively%20fine-tunes%20models%20on%20small%2C%20carefully%20selected%20data%0Abatches%20%28200%20examples%29%2C%20retaining%20only%20improvements.%20On%20news%20summarization%2C%20APE%0Aachieves%2040%20percent%20BLEU%20improvement%20using%20just%20a%20T4%20GPU%20in%2060%20minutes%2C%0Amatching%20or%20exceeding%20more%20complex%20methods%20like%20LoRA%20while%20remaining%0Aconceptually%20simple.%20Our%20approach%20is%20particularly%20valuable%20for%20researchers%20and%0Apractitioners%20with%20limited%20computational%20resources.%20We%20provide%20open-source%20code%0Aand%20demonstrate%20APE%27s%20effectiveness%20through%20both%20automatic%20metrics%20and%20human%0Aevaluation.%20While%20inspired%20by%20evolutionary%20theory%27s%20%22adjacent%20possible%22%2C%20APE%27s%0Acore%20insight%20has%20a%20very%20practical%20application%3A%20small%2C%20iterative%20data%0Aperturbations%20can%20efficiently%20guide%20LLMs%20toward%20task-specific%20performance%0Awithout%20expensive%20retraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAPE%253A%2520A%2520Data-Centric%2520Benchmark%2520for%2520Efficient%2520LLM%2520Adaptation%2520in%2520Text%250A%2520%2520Summarization%26entry.906535625%3DJavier%2520Mar%25C3%25ADn%26entry.1292438233%3D%2520%2520We%2520present%2520Adjacent%2520Possible%2520Exploration%2520%2528APE%2529%252C%2520a%2520simple%2520yet%2520effective%2520method%250Afor%2520adapting%2520large%2520language%2520models%2520to%2520specific%2520tasks%2520using%2520minimal%250Acomputational%2520resources.%2520Unlike%2520traditional%2520fine-tuning%2520that%2520requires%2520extensive%250Acompute%252C%2520APE%2520iteratively%2520fine-tunes%2520models%2520on%2520small%252C%2520carefully%2520selected%2520data%250Abatches%2520%2528200%2520examples%2529%252C%2520retaining%2520only%2520improvements.%2520On%2520news%2520summarization%252C%2520APE%250Aachieves%252040%2520percent%2520BLEU%2520improvement%2520using%2520just%2520a%2520T4%2520GPU%2520in%252060%2520minutes%252C%250Amatching%2520or%2520exceeding%2520more%2520complex%2520methods%2520like%2520LoRA%2520while%2520remaining%250Aconceptually%2520simple.%2520Our%2520approach%2520is%2520particularly%2520valuable%2520for%2520researchers%2520and%250Apractitioners%2520with%2520limited%2520computational%2520resources.%2520We%2520provide%2520open-source%2520code%250Aand%2520demonstrate%2520APE%2527s%2520effectiveness%2520through%2520both%2520automatic%2520metrics%2520and%2520human%250Aevaluation.%2520While%2520inspired%2520by%2520evolutionary%2520theory%2527s%2520%2522adjacent%2520possible%2522%252C%2520APE%2527s%250Acore%2520insight%2520has%2520a%2520very%2520practical%2520application%253A%2520small%252C%2520iterative%2520data%250Aperturbations%2520can%2520efficiently%2520guide%2520LLMs%2520toward%2520task-specific%2520performance%250Awithout%2520expensive%2520retraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=APE%3A%20A%20Data-Centric%20Benchmark%20for%20Efficient%20LLM%20Adaptation%20in%20Text%0A%20%20Summarization&entry.906535625=Javier%20Mar%C3%ADn&entry.1292438233=%20%20We%20present%20Adjacent%20Possible%20Exploration%20%28APE%29%2C%20a%20simple%20yet%20effective%20method%0Afor%20adapting%20large%20language%20models%20to%20specific%20tasks%20using%20minimal%0Acomputational%20resources.%20Unlike%20traditional%20fine-tuning%20that%20requires%20extensive%0Acompute%2C%20APE%20iteratively%20fine-tunes%20models%20on%20small%2C%20carefully%20selected%20data%0Abatches%20%28200%20examples%29%2C%20retaining%20only%20improvements.%20On%20news%20summarization%2C%20APE%0Aachieves%2040%20percent%20BLEU%20improvement%20using%20just%20a%20T4%20GPU%20in%2060%20minutes%2C%0Amatching%20or%20exceeding%20more%20complex%20methods%20like%20LoRA%20while%20remaining%0Aconceptually%20simple.%20Our%20approach%20is%20particularly%20valuable%20for%20researchers%20and%0Apractitioners%20with%20limited%20computational%20resources.%20We%20provide%20open-source%20code%0Aand%20demonstrate%20APE%27s%20effectiveness%20through%20both%20automatic%20metrics%20and%20human%0Aevaluation.%20While%20inspired%20by%20evolutionary%20theory%27s%20%22adjacent%20possible%22%2C%20APE%27s%0Acore%20insight%20has%20a%20very%20practical%20application%3A%20small%2C%20iterative%20data%0Aperturbations%20can%20efficiently%20guide%20LLMs%20toward%20task-specific%20performance%0Awithout%20expensive%20retraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19912v1&entry.124074799=Read"},
{"title": "Grokking ExPLAIND: Unifying Model, Data, and Training Attribution to\n  Study Model Behavior", "author": "Florian Eichin and Yupei Du and Philipp Mondorf and Barbara Plank and Michael A. Hedderich", "abstract": "  Post-hoc interpretability methods typically attribute a model's behavior to\nits components, data, or training trajectory in isolation. This leads to\nexplanations that lack a unified view and may miss key interactions. While\ncombining existing methods or applying them at different training stages offers\nbroader insights, these approaches usually lack theoretical support. In this\nwork, we present ExPLAIND, a unified framework that integrates all three\nperspectives. First, we generalize recent work on gradient path kernels, which\nreformulate models trained by gradient descent as a kernel machine, to more\nrealistic training settings. Empirically, we find that both a CNN and a\nTransformer model are replicated accurately by this reformulation. Second, we\nderive novel parameter- and step-wise influence scores from the kernel feature\nmaps. We show their effectiveness in parameter pruning that is comparable to\nexisting methods, reinforcing their value for model component attribution.\nFinally, jointly interpreting model components and data over the training\nprocess, we leverage ExPLAIND to analyze a Transformer that exhibits Grokking.\nAmong other things, our findings support previously proposed stages of\nGrokking, while refining the final phase as one of alignment of input\nembeddings and final layers around a representation pipeline learned after the\nmemorization phase. Overall, ExPLAIND provides a theoretically grounded,\nunified framework to interpret model behavior and training dynamics.\n", "link": "http://arxiv.org/abs/2505.20076v1", "date": "2025-05-26", "relevancy": 2.6958, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5438}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5438}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grokking%20ExPLAIND%3A%20Unifying%20Model%2C%20Data%2C%20and%20Training%20Attribution%20to%0A%20%20Study%20Model%20Behavior&body=Title%3A%20Grokking%20ExPLAIND%3A%20Unifying%20Model%2C%20Data%2C%20and%20Training%20Attribution%20to%0A%20%20Study%20Model%20Behavior%0AAuthor%3A%20Florian%20Eichin%20and%20Yupei%20Du%20and%20Philipp%20Mondorf%20and%20Barbara%20Plank%20and%20Michael%20A.%20Hedderich%0AAbstract%3A%20%20%20Post-hoc%20interpretability%20methods%20typically%20attribute%20a%20model%27s%20behavior%20to%0Aits%20components%2C%20data%2C%20or%20training%20trajectory%20in%20isolation.%20This%20leads%20to%0Aexplanations%20that%20lack%20a%20unified%20view%20and%20may%20miss%20key%20interactions.%20While%0Acombining%20existing%20methods%20or%20applying%20them%20at%20different%20training%20stages%20offers%0Abroader%20insights%2C%20these%20approaches%20usually%20lack%20theoretical%20support.%20In%20this%0Awork%2C%20we%20present%20ExPLAIND%2C%20a%20unified%20framework%20that%20integrates%20all%20three%0Aperspectives.%20First%2C%20we%20generalize%20recent%20work%20on%20gradient%20path%20kernels%2C%20which%0Areformulate%20models%20trained%20by%20gradient%20descent%20as%20a%20kernel%20machine%2C%20to%20more%0Arealistic%20training%20settings.%20Empirically%2C%20we%20find%20that%20both%20a%20CNN%20and%20a%0ATransformer%20model%20are%20replicated%20accurately%20by%20this%20reformulation.%20Second%2C%20we%0Aderive%20novel%20parameter-%20and%20step-wise%20influence%20scores%20from%20the%20kernel%20feature%0Amaps.%20We%20show%20their%20effectiveness%20in%20parameter%20pruning%20that%20is%20comparable%20to%0Aexisting%20methods%2C%20reinforcing%20their%20value%20for%20model%20component%20attribution.%0AFinally%2C%20jointly%20interpreting%20model%20components%20and%20data%20over%20the%20training%0Aprocess%2C%20we%20leverage%20ExPLAIND%20to%20analyze%20a%20Transformer%20that%20exhibits%20Grokking.%0AAmong%20other%20things%2C%20our%20findings%20support%20previously%20proposed%20stages%20of%0AGrokking%2C%20while%20refining%20the%20final%20phase%20as%20one%20of%20alignment%20of%20input%0Aembeddings%20and%20final%20layers%20around%20a%20representation%20pipeline%20learned%20after%20the%0Amemorization%20phase.%20Overall%2C%20ExPLAIND%20provides%20a%20theoretically%20grounded%2C%0Aunified%20framework%20to%20interpret%20model%20behavior%20and%20training%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20076v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrokking%2520ExPLAIND%253A%2520Unifying%2520Model%252C%2520Data%252C%2520and%2520Training%2520Attribution%2520to%250A%2520%2520Study%2520Model%2520Behavior%26entry.906535625%3DFlorian%2520Eichin%2520and%2520Yupei%2520Du%2520and%2520Philipp%2520Mondorf%2520and%2520Barbara%2520Plank%2520and%2520Michael%2520A.%2520Hedderich%26entry.1292438233%3D%2520%2520Post-hoc%2520interpretability%2520methods%2520typically%2520attribute%2520a%2520model%2527s%2520behavior%2520to%250Aits%2520components%252C%2520data%252C%2520or%2520training%2520trajectory%2520in%2520isolation.%2520This%2520leads%2520to%250Aexplanations%2520that%2520lack%2520a%2520unified%2520view%2520and%2520may%2520miss%2520key%2520interactions.%2520While%250Acombining%2520existing%2520methods%2520or%2520applying%2520them%2520at%2520different%2520training%2520stages%2520offers%250Abroader%2520insights%252C%2520these%2520approaches%2520usually%2520lack%2520theoretical%2520support.%2520In%2520this%250Awork%252C%2520we%2520present%2520ExPLAIND%252C%2520a%2520unified%2520framework%2520that%2520integrates%2520all%2520three%250Aperspectives.%2520First%252C%2520we%2520generalize%2520recent%2520work%2520on%2520gradient%2520path%2520kernels%252C%2520which%250Areformulate%2520models%2520trained%2520by%2520gradient%2520descent%2520as%2520a%2520kernel%2520machine%252C%2520to%2520more%250Arealistic%2520training%2520settings.%2520Empirically%252C%2520we%2520find%2520that%2520both%2520a%2520CNN%2520and%2520a%250ATransformer%2520model%2520are%2520replicated%2520accurately%2520by%2520this%2520reformulation.%2520Second%252C%2520we%250Aderive%2520novel%2520parameter-%2520and%2520step-wise%2520influence%2520scores%2520from%2520the%2520kernel%2520feature%250Amaps.%2520We%2520show%2520their%2520effectiveness%2520in%2520parameter%2520pruning%2520that%2520is%2520comparable%2520to%250Aexisting%2520methods%252C%2520reinforcing%2520their%2520value%2520for%2520model%2520component%2520attribution.%250AFinally%252C%2520jointly%2520interpreting%2520model%2520components%2520and%2520data%2520over%2520the%2520training%250Aprocess%252C%2520we%2520leverage%2520ExPLAIND%2520to%2520analyze%2520a%2520Transformer%2520that%2520exhibits%2520Grokking.%250AAmong%2520other%2520things%252C%2520our%2520findings%2520support%2520previously%2520proposed%2520stages%2520of%250AGrokking%252C%2520while%2520refining%2520the%2520final%2520phase%2520as%2520one%2520of%2520alignment%2520of%2520input%250Aembeddings%2520and%2520final%2520layers%2520around%2520a%2520representation%2520pipeline%2520learned%2520after%2520the%250Amemorization%2520phase.%2520Overall%252C%2520ExPLAIND%2520provides%2520a%2520theoretically%2520grounded%252C%250Aunified%2520framework%2520to%2520interpret%2520model%2520behavior%2520and%2520training%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20076v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grokking%20ExPLAIND%3A%20Unifying%20Model%2C%20Data%2C%20and%20Training%20Attribution%20to%0A%20%20Study%20Model%20Behavior&entry.906535625=Florian%20Eichin%20and%20Yupei%20Du%20and%20Philipp%20Mondorf%20and%20Barbara%20Plank%20and%20Michael%20A.%20Hedderich&entry.1292438233=%20%20Post-hoc%20interpretability%20methods%20typically%20attribute%20a%20model%27s%20behavior%20to%0Aits%20components%2C%20data%2C%20or%20training%20trajectory%20in%20isolation.%20This%20leads%20to%0Aexplanations%20that%20lack%20a%20unified%20view%20and%20may%20miss%20key%20interactions.%20While%0Acombining%20existing%20methods%20or%20applying%20them%20at%20different%20training%20stages%20offers%0Abroader%20insights%2C%20these%20approaches%20usually%20lack%20theoretical%20support.%20In%20this%0Awork%2C%20we%20present%20ExPLAIND%2C%20a%20unified%20framework%20that%20integrates%20all%20three%0Aperspectives.%20First%2C%20we%20generalize%20recent%20work%20on%20gradient%20path%20kernels%2C%20which%0Areformulate%20models%20trained%20by%20gradient%20descent%20as%20a%20kernel%20machine%2C%20to%20more%0Arealistic%20training%20settings.%20Empirically%2C%20we%20find%20that%20both%20a%20CNN%20and%20a%0ATransformer%20model%20are%20replicated%20accurately%20by%20this%20reformulation.%20Second%2C%20we%0Aderive%20novel%20parameter-%20and%20step-wise%20influence%20scores%20from%20the%20kernel%20feature%0Amaps.%20We%20show%20their%20effectiveness%20in%20parameter%20pruning%20that%20is%20comparable%20to%0Aexisting%20methods%2C%20reinforcing%20their%20value%20for%20model%20component%20attribution.%0AFinally%2C%20jointly%20interpreting%20model%20components%20and%20data%20over%20the%20training%0Aprocess%2C%20we%20leverage%20ExPLAIND%20to%20analyze%20a%20Transformer%20that%20exhibits%20Grokking.%0AAmong%20other%20things%2C%20our%20findings%20support%20previously%20proposed%20stages%20of%0AGrokking%2C%20while%20refining%20the%20final%20phase%20as%20one%20of%20alignment%20of%20input%0Aembeddings%20and%20final%20layers%20around%20a%20representation%20pipeline%20learned%20after%20the%0Amemorization%20phase.%20Overall%2C%20ExPLAIND%20provides%20a%20theoretically%20grounded%2C%0Aunified%20framework%20to%20interpret%20model%20behavior%20and%20training%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20076v1&entry.124074799=Read"},
{"title": "Task-Oriented Low-Label Semantic Communication With Self-Supervised\n  Learning", "author": "Run Gu and Wei Xu and Zhaohui Yang and Dusit Niyato and Aylin Yener", "abstract": "  Task-oriented semantic communication enhances transmission efficiency by\nconveying semantic information rather than exact messages. Deep learning\n(DL)-based semantic communication can effectively cultivate the essential\nsemantic knowledge for semantic extraction, transmission, and interpretation by\nleveraging massive labeled samples for downstream task training. In this paper,\nwe propose a self-supervised learning-based semantic communication framework\n(SLSCom) to enhance task inference performance, particularly in scenarios with\nlimited access to labeled samples. Specifically, we develop a task-relevant\nsemantic encoder using unlabeled samples, which can be collected by devices in\nreal-world edge networks. To facilitate task-relevant semantic extraction, we\nintroduce self-supervision for learning contrastive features and formulate the\ninformation bottleneck (IB) problem to balance the tradeoff between the\ninformativeness of the extracted features and task inference performance. Given\nthe computational challenges of the IB problem, we devise a practical and\neffective solution by employing self-supervised classification and\nreconstruction pretext tasks. We further propose efficient joint training\nmethods to enhance end-to-end inference accuracy over wireless channels, even\nwith few labeled samples. We evaluate the proposed framework on image\nclassification tasks over multipath wireless channels. Extensive simulation\nresults demonstrate that SLSCom significantly outperforms conventional digital\ncoding methods and existing DL-based approaches across varying labeled data set\nsizes and SNR conditions, even when the unlabeled samples are irrelevant to the\ndownstream tasks.\n", "link": "http://arxiv.org/abs/2505.19940v1", "date": "2025-05-26", "relevancy": 2.6861, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5697}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.521}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-Oriented%20Low-Label%20Semantic%20Communication%20With%20Self-Supervised%0A%20%20Learning&body=Title%3A%20Task-Oriented%20Low-Label%20Semantic%20Communication%20With%20Self-Supervised%0A%20%20Learning%0AAuthor%3A%20Run%20Gu%20and%20Wei%20Xu%20and%20Zhaohui%20Yang%20and%20Dusit%20Niyato%20and%20Aylin%20Yener%0AAbstract%3A%20%20%20Task-oriented%20semantic%20communication%20enhances%20transmission%20efficiency%20by%0Aconveying%20semantic%20information%20rather%20than%20exact%20messages.%20Deep%20learning%0A%28DL%29-based%20semantic%20communication%20can%20effectively%20cultivate%20the%20essential%0Asemantic%20knowledge%20for%20semantic%20extraction%2C%20transmission%2C%20and%20interpretation%20by%0Aleveraging%20massive%20labeled%20samples%20for%20downstream%20task%20training.%20In%20this%20paper%2C%0Awe%20propose%20a%20self-supervised%20learning-based%20semantic%20communication%20framework%0A%28SLSCom%29%20to%20enhance%20task%20inference%20performance%2C%20particularly%20in%20scenarios%20with%0Alimited%20access%20to%20labeled%20samples.%20Specifically%2C%20we%20develop%20a%20task-relevant%0Asemantic%20encoder%20using%20unlabeled%20samples%2C%20which%20can%20be%20collected%20by%20devices%20in%0Areal-world%20edge%20networks.%20To%20facilitate%20task-relevant%20semantic%20extraction%2C%20we%0Aintroduce%20self-supervision%20for%20learning%20contrastive%20features%20and%20formulate%20the%0Ainformation%20bottleneck%20%28IB%29%20problem%20to%20balance%20the%20tradeoff%20between%20the%0Ainformativeness%20of%20the%20extracted%20features%20and%20task%20inference%20performance.%20Given%0Athe%20computational%20challenges%20of%20the%20IB%20problem%2C%20we%20devise%20a%20practical%20and%0Aeffective%20solution%20by%20employing%20self-supervised%20classification%20and%0Areconstruction%20pretext%20tasks.%20We%20further%20propose%20efficient%20joint%20training%0Amethods%20to%20enhance%20end-to-end%20inference%20accuracy%20over%20wireless%20channels%2C%20even%0Awith%20few%20labeled%20samples.%20We%20evaluate%20the%20proposed%20framework%20on%20image%0Aclassification%20tasks%20over%20multipath%20wireless%20channels.%20Extensive%20simulation%0Aresults%20demonstrate%20that%20SLSCom%20significantly%20outperforms%20conventional%20digital%0Acoding%20methods%20and%20existing%20DL-based%20approaches%20across%20varying%20labeled%20data%20set%0Asizes%20and%20SNR%20conditions%2C%20even%20when%20the%20unlabeled%20samples%20are%20irrelevant%20to%20the%0Adownstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19940v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-Oriented%2520Low-Label%2520Semantic%2520Communication%2520With%2520Self-Supervised%250A%2520%2520Learning%26entry.906535625%3DRun%2520Gu%2520and%2520Wei%2520Xu%2520and%2520Zhaohui%2520Yang%2520and%2520Dusit%2520Niyato%2520and%2520Aylin%2520Yener%26entry.1292438233%3D%2520%2520Task-oriented%2520semantic%2520communication%2520enhances%2520transmission%2520efficiency%2520by%250Aconveying%2520semantic%2520information%2520rather%2520than%2520exact%2520messages.%2520Deep%2520learning%250A%2528DL%2529-based%2520semantic%2520communication%2520can%2520effectively%2520cultivate%2520the%2520essential%250Asemantic%2520knowledge%2520for%2520semantic%2520extraction%252C%2520transmission%252C%2520and%2520interpretation%2520by%250Aleveraging%2520massive%2520labeled%2520samples%2520for%2520downstream%2520task%2520training.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520self-supervised%2520learning-based%2520semantic%2520communication%2520framework%250A%2528SLSCom%2529%2520to%2520enhance%2520task%2520inference%2520performance%252C%2520particularly%2520in%2520scenarios%2520with%250Alimited%2520access%2520to%2520labeled%2520samples.%2520Specifically%252C%2520we%2520develop%2520a%2520task-relevant%250Asemantic%2520encoder%2520using%2520unlabeled%2520samples%252C%2520which%2520can%2520be%2520collected%2520by%2520devices%2520in%250Areal-world%2520edge%2520networks.%2520To%2520facilitate%2520task-relevant%2520semantic%2520extraction%252C%2520we%250Aintroduce%2520self-supervision%2520for%2520learning%2520contrastive%2520features%2520and%2520formulate%2520the%250Ainformation%2520bottleneck%2520%2528IB%2529%2520problem%2520to%2520balance%2520the%2520tradeoff%2520between%2520the%250Ainformativeness%2520of%2520the%2520extracted%2520features%2520and%2520task%2520inference%2520performance.%2520Given%250Athe%2520computational%2520challenges%2520of%2520the%2520IB%2520problem%252C%2520we%2520devise%2520a%2520practical%2520and%250Aeffective%2520solution%2520by%2520employing%2520self-supervised%2520classification%2520and%250Areconstruction%2520pretext%2520tasks.%2520We%2520further%2520propose%2520efficient%2520joint%2520training%250Amethods%2520to%2520enhance%2520end-to-end%2520inference%2520accuracy%2520over%2520wireless%2520channels%252C%2520even%250Awith%2520few%2520labeled%2520samples.%2520We%2520evaluate%2520the%2520proposed%2520framework%2520on%2520image%250Aclassification%2520tasks%2520over%2520multipath%2520wireless%2520channels.%2520Extensive%2520simulation%250Aresults%2520demonstrate%2520that%2520SLSCom%2520significantly%2520outperforms%2520conventional%2520digital%250Acoding%2520methods%2520and%2520existing%2520DL-based%2520approaches%2520across%2520varying%2520labeled%2520data%2520set%250Asizes%2520and%2520SNR%2520conditions%252C%2520even%2520when%2520the%2520unlabeled%2520samples%2520are%2520irrelevant%2520to%2520the%250Adownstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19940v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-Oriented%20Low-Label%20Semantic%20Communication%20With%20Self-Supervised%0A%20%20Learning&entry.906535625=Run%20Gu%20and%20Wei%20Xu%20and%20Zhaohui%20Yang%20and%20Dusit%20Niyato%20and%20Aylin%20Yener&entry.1292438233=%20%20Task-oriented%20semantic%20communication%20enhances%20transmission%20efficiency%20by%0Aconveying%20semantic%20information%20rather%20than%20exact%20messages.%20Deep%20learning%0A%28DL%29-based%20semantic%20communication%20can%20effectively%20cultivate%20the%20essential%0Asemantic%20knowledge%20for%20semantic%20extraction%2C%20transmission%2C%20and%20interpretation%20by%0Aleveraging%20massive%20labeled%20samples%20for%20downstream%20task%20training.%20In%20this%20paper%2C%0Awe%20propose%20a%20self-supervised%20learning-based%20semantic%20communication%20framework%0A%28SLSCom%29%20to%20enhance%20task%20inference%20performance%2C%20particularly%20in%20scenarios%20with%0Alimited%20access%20to%20labeled%20samples.%20Specifically%2C%20we%20develop%20a%20task-relevant%0Asemantic%20encoder%20using%20unlabeled%20samples%2C%20which%20can%20be%20collected%20by%20devices%20in%0Areal-world%20edge%20networks.%20To%20facilitate%20task-relevant%20semantic%20extraction%2C%20we%0Aintroduce%20self-supervision%20for%20learning%20contrastive%20features%20and%20formulate%20the%0Ainformation%20bottleneck%20%28IB%29%20problem%20to%20balance%20the%20tradeoff%20between%20the%0Ainformativeness%20of%20the%20extracted%20features%20and%20task%20inference%20performance.%20Given%0Athe%20computational%20challenges%20of%20the%20IB%20problem%2C%20we%20devise%20a%20practical%20and%0Aeffective%20solution%20by%20employing%20self-supervised%20classification%20and%0Areconstruction%20pretext%20tasks.%20We%20further%20propose%20efficient%20joint%20training%0Amethods%20to%20enhance%20end-to-end%20inference%20accuracy%20over%20wireless%20channels%2C%20even%0Awith%20few%20labeled%20samples.%20We%20evaluate%20the%20proposed%20framework%20on%20image%0Aclassification%20tasks%20over%20multipath%20wireless%20channels.%20Extensive%20simulation%0Aresults%20demonstrate%20that%20SLSCom%20significantly%20outperforms%20conventional%20digital%0Acoding%20methods%20and%20existing%20DL-based%20approaches%20across%20varying%20labeled%20data%20set%0Asizes%20and%20SNR%20conditions%2C%20even%20when%20the%20unlabeled%20samples%20are%20irrelevant%20to%20the%0Adownstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19940v1&entry.124074799=Read"},
{"title": "InfoCons: Identifying Interpretable Critical Concepts in Point Clouds\n  via Information Theory", "author": "Feifei Li and Mi Zhang and Zhaoxiang Wang and Min Yang", "abstract": "  Interpretability of point cloud (PC) models becomes imperative given their\ndeployment in safety-critical scenarios such as autonomous vehicles. We focus\non attributing PC model outputs to interpretable critical concepts, defined as\nmeaningful subsets of the input point cloud. To enable human-understandable\ndiagnostics of model failures, an ideal critical subset should be *faithful*\n(preserving points that causally influence predictions) and *conceptually\ncoherent* (forming semantically meaningful structures that align with human\nperception). We propose InfoCons, an explanation framework that applies\ninformation-theoretic principles to decompose the point cloud into 3D concepts,\nenabling the examination of their causal effect on model predictions with\nlearnable priors. We evaluate InfoCons on synthetic datasets for\nclassification, comparing it qualitatively and quantitatively with four\nbaselines. We further demonstrate its scalability and flexibility on two\nreal-world datasets and in two applications that utilize critical scores of PC.\n", "link": "http://arxiv.org/abs/2505.19820v1", "date": "2025-05-26", "relevancy": 2.6705, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfoCons%3A%20Identifying%20Interpretable%20Critical%20Concepts%20in%20Point%20Clouds%0A%20%20via%20Information%20Theory&body=Title%3A%20InfoCons%3A%20Identifying%20Interpretable%20Critical%20Concepts%20in%20Point%20Clouds%0A%20%20via%20Information%20Theory%0AAuthor%3A%20Feifei%20Li%20and%20Mi%20Zhang%20and%20Zhaoxiang%20Wang%20and%20Min%20Yang%0AAbstract%3A%20%20%20Interpretability%20of%20point%20cloud%20%28PC%29%20models%20becomes%20imperative%20given%20their%0Adeployment%20in%20safety-critical%20scenarios%20such%20as%20autonomous%20vehicles.%20We%20focus%0Aon%20attributing%20PC%20model%20outputs%20to%20interpretable%20critical%20concepts%2C%20defined%20as%0Ameaningful%20subsets%20of%20the%20input%20point%20cloud.%20To%20enable%20human-understandable%0Adiagnostics%20of%20model%20failures%2C%20an%20ideal%20critical%20subset%20should%20be%20%2Afaithful%2A%0A%28preserving%20points%20that%20causally%20influence%20predictions%29%20and%20%2Aconceptually%0Acoherent%2A%20%28forming%20semantically%20meaningful%20structures%20that%20align%20with%20human%0Aperception%29.%20We%20propose%20InfoCons%2C%20an%20explanation%20framework%20that%20applies%0Ainformation-theoretic%20principles%20to%20decompose%20the%20point%20cloud%20into%203D%20concepts%2C%0Aenabling%20the%20examination%20of%20their%20causal%20effect%20on%20model%20predictions%20with%0Alearnable%20priors.%20We%20evaluate%20InfoCons%20on%20synthetic%20datasets%20for%0Aclassification%2C%20comparing%20it%20qualitatively%20and%20quantitatively%20with%20four%0Abaselines.%20We%20further%20demonstrate%20its%20scalability%20and%20flexibility%20on%20two%0Areal-world%20datasets%20and%20in%20two%20applications%20that%20utilize%20critical%20scores%20of%20PC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfoCons%253A%2520Identifying%2520Interpretable%2520Critical%2520Concepts%2520in%2520Point%2520Clouds%250A%2520%2520via%2520Information%2520Theory%26entry.906535625%3DFeifei%2520Li%2520and%2520Mi%2520Zhang%2520and%2520Zhaoxiang%2520Wang%2520and%2520Min%2520Yang%26entry.1292438233%3D%2520%2520Interpretability%2520of%2520point%2520cloud%2520%2528PC%2529%2520models%2520becomes%2520imperative%2520given%2520their%250Adeployment%2520in%2520safety-critical%2520scenarios%2520such%2520as%2520autonomous%2520vehicles.%2520We%2520focus%250Aon%2520attributing%2520PC%2520model%2520outputs%2520to%2520interpretable%2520critical%2520concepts%252C%2520defined%2520as%250Ameaningful%2520subsets%2520of%2520the%2520input%2520point%2520cloud.%2520To%2520enable%2520human-understandable%250Adiagnostics%2520of%2520model%2520failures%252C%2520an%2520ideal%2520critical%2520subset%2520should%2520be%2520%252Afaithful%252A%250A%2528preserving%2520points%2520that%2520causally%2520influence%2520predictions%2529%2520and%2520%252Aconceptually%250Acoherent%252A%2520%2528forming%2520semantically%2520meaningful%2520structures%2520that%2520align%2520with%2520human%250Aperception%2529.%2520We%2520propose%2520InfoCons%252C%2520an%2520explanation%2520framework%2520that%2520applies%250Ainformation-theoretic%2520principles%2520to%2520decompose%2520the%2520point%2520cloud%2520into%25203D%2520concepts%252C%250Aenabling%2520the%2520examination%2520of%2520their%2520causal%2520effect%2520on%2520model%2520predictions%2520with%250Alearnable%2520priors.%2520We%2520evaluate%2520InfoCons%2520on%2520synthetic%2520datasets%2520for%250Aclassification%252C%2520comparing%2520it%2520qualitatively%2520and%2520quantitatively%2520with%2520four%250Abaselines.%2520We%2520further%2520demonstrate%2520its%2520scalability%2520and%2520flexibility%2520on%2520two%250Areal-world%2520datasets%2520and%2520in%2520two%2520applications%2520that%2520utilize%2520critical%2520scores%2520of%2520PC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfoCons%3A%20Identifying%20Interpretable%20Critical%20Concepts%20in%20Point%20Clouds%0A%20%20via%20Information%20Theory&entry.906535625=Feifei%20Li%20and%20Mi%20Zhang%20and%20Zhaoxiang%20Wang%20and%20Min%20Yang&entry.1292438233=%20%20Interpretability%20of%20point%20cloud%20%28PC%29%20models%20becomes%20imperative%20given%20their%0Adeployment%20in%20safety-critical%20scenarios%20such%20as%20autonomous%20vehicles.%20We%20focus%0Aon%20attributing%20PC%20model%20outputs%20to%20interpretable%20critical%20concepts%2C%20defined%20as%0Ameaningful%20subsets%20of%20the%20input%20point%20cloud.%20To%20enable%20human-understandable%0Adiagnostics%20of%20model%20failures%2C%20an%20ideal%20critical%20subset%20should%20be%20%2Afaithful%2A%0A%28preserving%20points%20that%20causally%20influence%20predictions%29%20and%20%2Aconceptually%0Acoherent%2A%20%28forming%20semantically%20meaningful%20structures%20that%20align%20with%20human%0Aperception%29.%20We%20propose%20InfoCons%2C%20an%20explanation%20framework%20that%20applies%0Ainformation-theoretic%20principles%20to%20decompose%20the%20point%20cloud%20into%203D%20concepts%2C%0Aenabling%20the%20examination%20of%20their%20causal%20effect%20on%20model%20predictions%20with%0Alearnable%20priors.%20We%20evaluate%20InfoCons%20on%20synthetic%20datasets%20for%0Aclassification%2C%20comparing%20it%20qualitatively%20and%20quantitatively%20with%20four%0Abaselines.%20We%20further%20demonstrate%20its%20scalability%20and%20flexibility%20on%20two%0Areal-world%20datasets%20and%20in%20two%20applications%20that%20utilize%20critical%20scores%20of%20PC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19820v1&entry.124074799=Read"},
{"title": "Multimodal 3D Reasoning Segmentation with Complex Scenes", "author": "Xueying Jiang and Lewei Lu and Ling Shao and Shijian Lu", "abstract": "  The recent development in multimodal learning has greatly advanced the\nresearch in 3D scene understanding in various real-world tasks such as embodied\nAI. However, most existing studies are facing two common challenges: 1) they\nare short of reasoning ability for interaction and interpretation of human\nintentions and 2) they focus on scenarios with single-category objects and\nover-simplified textual descriptions and neglect multi-object scenarios with\ncomplicated spatial relations among objects. We address the above challenges by\nproposing a 3D reasoning segmentation task for reasoning segmentation with\nmultiple objects in scenes. The task allows producing 3D segmentation masks and\ndetailed textual explanations as enriched by 3D spatial relations among\nobjects. To this end, we create ReasonSeg3D, a large-scale and high-quality\nbenchmark that integrates 3D segmentation masks and 3D spatial relations with\ngenerated question-answer pairs. In addition, we design MORE3D, a novel 3D\nreasoning network that works with queries of multiple objects and is tailored\nfor 3D scene understanding. MORE3D learns detailed explanations on 3D relations\nand employs them to capture spatial information of objects and reason textual\noutputs. Extensive experiments show that MORE3D excels in reasoning and\nsegmenting complex multi-object 3D scenes. In addition, the created ReasonSeg3D\noffers a valuable platform for future exploration of 3D reasoning segmentation.\nThe data and code will be released.\n", "link": "http://arxiv.org/abs/2411.13927v3", "date": "2025-05-26", "relevancy": 2.6171, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6643}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6643}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%203D%20Reasoning%20Segmentation%20with%20Complex%20Scenes&body=Title%3A%20Multimodal%203D%20Reasoning%20Segmentation%20with%20Complex%20Scenes%0AAuthor%3A%20Xueying%20Jiang%20and%20Lewei%20Lu%20and%20Ling%20Shao%20and%20Shijian%20Lu%0AAbstract%3A%20%20%20The%20recent%20development%20in%20multimodal%20learning%20has%20greatly%20advanced%20the%0Aresearch%20in%203D%20scene%20understanding%20in%20various%20real-world%20tasks%20such%20as%20embodied%0AAI.%20However%2C%20most%20existing%20studies%20are%20facing%20two%20common%20challenges%3A%201%29%20they%0Aare%20short%20of%20reasoning%20ability%20for%20interaction%20and%20interpretation%20of%20human%0Aintentions%20and%202%29%20they%20focus%20on%20scenarios%20with%20single-category%20objects%20and%0Aover-simplified%20textual%20descriptions%20and%20neglect%20multi-object%20scenarios%20with%0Acomplicated%20spatial%20relations%20among%20objects.%20We%20address%20the%20above%20challenges%20by%0Aproposing%20a%203D%20reasoning%20segmentation%20task%20for%20reasoning%20segmentation%20with%0Amultiple%20objects%20in%20scenes.%20The%20task%20allows%20producing%203D%20segmentation%20masks%20and%0Adetailed%20textual%20explanations%20as%20enriched%20by%203D%20spatial%20relations%20among%0Aobjects.%20To%20this%20end%2C%20we%20create%20ReasonSeg3D%2C%20a%20large-scale%20and%20high-quality%0Abenchmark%20that%20integrates%203D%20segmentation%20masks%20and%203D%20spatial%20relations%20with%0Agenerated%20question-answer%20pairs.%20In%20addition%2C%20we%20design%20MORE3D%2C%20a%20novel%203D%0Areasoning%20network%20that%20works%20with%20queries%20of%20multiple%20objects%20and%20is%20tailored%0Afor%203D%20scene%20understanding.%20MORE3D%20learns%20detailed%20explanations%20on%203D%20relations%0Aand%20employs%20them%20to%20capture%20spatial%20information%20of%20objects%20and%20reason%20textual%0Aoutputs.%20Extensive%20experiments%20show%20that%20MORE3D%20excels%20in%20reasoning%20and%0Asegmenting%20complex%20multi-object%203D%20scenes.%20In%20addition%2C%20the%20created%20ReasonSeg3D%0Aoffers%20a%20valuable%20platform%20for%20future%20exploration%20of%203D%20reasoning%20segmentation.%0AThe%20data%20and%20code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13927v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%25203D%2520Reasoning%2520Segmentation%2520with%2520Complex%2520Scenes%26entry.906535625%3DXueying%2520Jiang%2520and%2520Lewei%2520Lu%2520and%2520Ling%2520Shao%2520and%2520Shijian%2520Lu%26entry.1292438233%3D%2520%2520The%2520recent%2520development%2520in%2520multimodal%2520learning%2520has%2520greatly%2520advanced%2520the%250Aresearch%2520in%25203D%2520scene%2520understanding%2520in%2520various%2520real-world%2520tasks%2520such%2520as%2520embodied%250AAI.%2520However%252C%2520most%2520existing%2520studies%2520are%2520facing%2520two%2520common%2520challenges%253A%25201%2529%2520they%250Aare%2520short%2520of%2520reasoning%2520ability%2520for%2520interaction%2520and%2520interpretation%2520of%2520human%250Aintentions%2520and%25202%2529%2520they%2520focus%2520on%2520scenarios%2520with%2520single-category%2520objects%2520and%250Aover-simplified%2520textual%2520descriptions%2520and%2520neglect%2520multi-object%2520scenarios%2520with%250Acomplicated%2520spatial%2520relations%2520among%2520objects.%2520We%2520address%2520the%2520above%2520challenges%2520by%250Aproposing%2520a%25203D%2520reasoning%2520segmentation%2520task%2520for%2520reasoning%2520segmentation%2520with%250Amultiple%2520objects%2520in%2520scenes.%2520The%2520task%2520allows%2520producing%25203D%2520segmentation%2520masks%2520and%250Adetailed%2520textual%2520explanations%2520as%2520enriched%2520by%25203D%2520spatial%2520relations%2520among%250Aobjects.%2520To%2520this%2520end%252C%2520we%2520create%2520ReasonSeg3D%252C%2520a%2520large-scale%2520and%2520high-quality%250Abenchmark%2520that%2520integrates%25203D%2520segmentation%2520masks%2520and%25203D%2520spatial%2520relations%2520with%250Agenerated%2520question-answer%2520pairs.%2520In%2520addition%252C%2520we%2520design%2520MORE3D%252C%2520a%2520novel%25203D%250Areasoning%2520network%2520that%2520works%2520with%2520queries%2520of%2520multiple%2520objects%2520and%2520is%2520tailored%250Afor%25203D%2520scene%2520understanding.%2520MORE3D%2520learns%2520detailed%2520explanations%2520on%25203D%2520relations%250Aand%2520employs%2520them%2520to%2520capture%2520spatial%2520information%2520of%2520objects%2520and%2520reason%2520textual%250Aoutputs.%2520Extensive%2520experiments%2520show%2520that%2520MORE3D%2520excels%2520in%2520reasoning%2520and%250Asegmenting%2520complex%2520multi-object%25203D%2520scenes.%2520In%2520addition%252C%2520the%2520created%2520ReasonSeg3D%250Aoffers%2520a%2520valuable%2520platform%2520for%2520future%2520exploration%2520of%25203D%2520reasoning%2520segmentation.%250AThe%2520data%2520and%2520code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13927v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%203D%20Reasoning%20Segmentation%20with%20Complex%20Scenes&entry.906535625=Xueying%20Jiang%20and%20Lewei%20Lu%20and%20Ling%20Shao%20and%20Shijian%20Lu&entry.1292438233=%20%20The%20recent%20development%20in%20multimodal%20learning%20has%20greatly%20advanced%20the%0Aresearch%20in%203D%20scene%20understanding%20in%20various%20real-world%20tasks%20such%20as%20embodied%0AAI.%20However%2C%20most%20existing%20studies%20are%20facing%20two%20common%20challenges%3A%201%29%20they%0Aare%20short%20of%20reasoning%20ability%20for%20interaction%20and%20interpretation%20of%20human%0Aintentions%20and%202%29%20they%20focus%20on%20scenarios%20with%20single-category%20objects%20and%0Aover-simplified%20textual%20descriptions%20and%20neglect%20multi-object%20scenarios%20with%0Acomplicated%20spatial%20relations%20among%20objects.%20We%20address%20the%20above%20challenges%20by%0Aproposing%20a%203D%20reasoning%20segmentation%20task%20for%20reasoning%20segmentation%20with%0Amultiple%20objects%20in%20scenes.%20The%20task%20allows%20producing%203D%20segmentation%20masks%20and%0Adetailed%20textual%20explanations%20as%20enriched%20by%203D%20spatial%20relations%20among%0Aobjects.%20To%20this%20end%2C%20we%20create%20ReasonSeg3D%2C%20a%20large-scale%20and%20high-quality%0Abenchmark%20that%20integrates%203D%20segmentation%20masks%20and%203D%20spatial%20relations%20with%0Agenerated%20question-answer%20pairs.%20In%20addition%2C%20we%20design%20MORE3D%2C%20a%20novel%203D%0Areasoning%20network%20that%20works%20with%20queries%20of%20multiple%20objects%20and%20is%20tailored%0Afor%203D%20scene%20understanding.%20MORE3D%20learns%20detailed%20explanations%20on%203D%20relations%0Aand%20employs%20them%20to%20capture%20spatial%20information%20of%20objects%20and%20reason%20textual%0Aoutputs.%20Extensive%20experiments%20show%20that%20MORE3D%20excels%20in%20reasoning%20and%0Asegmenting%20complex%20multi-object%203D%20scenes.%20In%20addition%2C%20the%20created%20ReasonSeg3D%0Aoffers%20a%20valuable%20platform%20for%20future%20exploration%20of%203D%20reasoning%20segmentation.%0AThe%20data%20and%20code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13927v3&entry.124074799=Read"},
{"title": "Federated Domain Generalization with Data-free On-server Matching\n  Gradient", "author": "Trong-Binh Nguyen and Minh-Duong Nguyen and Jinsun Park and Quoc-Viet Pham and Won Joo Hwang", "abstract": "  Domain Generalization (DG) aims to learn from multiple known source domains a\nmodel that can generalize well to unknown target domains. One of the key\napproaches in DG is training an encoder which generates domain-invariant\nrepresentations. However, this approach is not applicable in Federated Domain\nGeneralization (FDG), where data from various domains are distributed across\ndifferent clients. In this paper, we introduce a novel approach, dubbed\nFederated Learning via On-server Matching Gradient (FedOMG), which can\n\\emph{efficiently leverage domain information from distributed domains}.\nSpecifically, we utilize the local gradients as information about the\ndistributed models to find an invariant gradient direction across all domains\nthrough gradient inner product maximization. The advantages are two-fold: 1)\nFedOMG can aggregate the characteristics of distributed models on the\ncentralized server without incurring any additional communication cost, and 2)\nFedOMG is orthogonal to many existing FL/FDG methods, allowing for additional\nperformance improvements by being seamlessly integrated with them. Extensive\nexperimental evaluations on various settings to demonstrate the robustness of\nFedOMG compared to other FL/FDG baselines. Our method outperforms recent SOTA\nbaselines on four FL benchmark datasets (MNIST, EMNIST, CIFAR-10, and\nCIFAR-100), and three FDG benchmark datasets (PACS, VLCS, and OfficeHome).\n", "link": "http://arxiv.org/abs/2501.14653v2", "date": "2025-05-26", "relevancy": 2.6025, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.536}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5189}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Domain%20Generalization%20with%20Data-free%20On-server%20Matching%0A%20%20Gradient&body=Title%3A%20Federated%20Domain%20Generalization%20with%20Data-free%20On-server%20Matching%0A%20%20Gradient%0AAuthor%3A%20Trong-Binh%20Nguyen%20and%20Minh-Duong%20Nguyen%20and%20Jinsun%20Park%20and%20Quoc-Viet%20Pham%20and%20Won%20Joo%20Hwang%0AAbstract%3A%20%20%20Domain%20Generalization%20%28DG%29%20aims%20to%20learn%20from%20multiple%20known%20source%20domains%20a%0Amodel%20that%20can%20generalize%20well%20to%20unknown%20target%20domains.%20One%20of%20the%20key%0Aapproaches%20in%20DG%20is%20training%20an%20encoder%20which%20generates%20domain-invariant%0Arepresentations.%20However%2C%20this%20approach%20is%20not%20applicable%20in%20Federated%20Domain%0AGeneralization%20%28FDG%29%2C%20where%20data%20from%20various%20domains%20are%20distributed%20across%0Adifferent%20clients.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%2C%20dubbed%0AFederated%20Learning%20via%20On-server%20Matching%20Gradient%20%28FedOMG%29%2C%20which%20can%0A%5Cemph%7Befficiently%20leverage%20domain%20information%20from%20distributed%20domains%7D.%0ASpecifically%2C%20we%20utilize%20the%20local%20gradients%20as%20information%20about%20the%0Adistributed%20models%20to%20find%20an%20invariant%20gradient%20direction%20across%20all%20domains%0Athrough%20gradient%20inner%20product%20maximization.%20The%20advantages%20are%20two-fold%3A%201%29%0AFedOMG%20can%20aggregate%20the%20characteristics%20of%20distributed%20models%20on%20the%0Acentralized%20server%20without%20incurring%20any%20additional%20communication%20cost%2C%20and%202%29%0AFedOMG%20is%20orthogonal%20to%20many%20existing%20FL/FDG%20methods%2C%20allowing%20for%20additional%0Aperformance%20improvements%20by%20being%20seamlessly%20integrated%20with%20them.%20Extensive%0Aexperimental%20evaluations%20on%20various%20settings%20to%20demonstrate%20the%20robustness%20of%0AFedOMG%20compared%20to%20other%20FL/FDG%20baselines.%20Our%20method%20outperforms%20recent%20SOTA%0Abaselines%20on%20four%20FL%20benchmark%20datasets%20%28MNIST%2C%20EMNIST%2C%20CIFAR-10%2C%20and%0ACIFAR-100%29%2C%20and%20three%20FDG%20benchmark%20datasets%20%28PACS%2C%20VLCS%2C%20and%20OfficeHome%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14653v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Domain%2520Generalization%2520with%2520Data-free%2520On-server%2520Matching%250A%2520%2520Gradient%26entry.906535625%3DTrong-Binh%2520Nguyen%2520and%2520Minh-Duong%2520Nguyen%2520and%2520Jinsun%2520Park%2520and%2520Quoc-Viet%2520Pham%2520and%2520Won%2520Joo%2520Hwang%26entry.1292438233%3D%2520%2520Domain%2520Generalization%2520%2528DG%2529%2520aims%2520to%2520learn%2520from%2520multiple%2520known%2520source%2520domains%2520a%250Amodel%2520that%2520can%2520generalize%2520well%2520to%2520unknown%2520target%2520domains.%2520One%2520of%2520the%2520key%250Aapproaches%2520in%2520DG%2520is%2520training%2520an%2520encoder%2520which%2520generates%2520domain-invariant%250Arepresentations.%2520However%252C%2520this%2520approach%2520is%2520not%2520applicable%2520in%2520Federated%2520Domain%250AGeneralization%2520%2528FDG%2529%252C%2520where%2520data%2520from%2520various%2520domains%2520are%2520distributed%2520across%250Adifferent%2520clients.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520approach%252C%2520dubbed%250AFederated%2520Learning%2520via%2520On-server%2520Matching%2520Gradient%2520%2528FedOMG%2529%252C%2520which%2520can%250A%255Cemph%257Befficiently%2520leverage%2520domain%2520information%2520from%2520distributed%2520domains%257D.%250ASpecifically%252C%2520we%2520utilize%2520the%2520local%2520gradients%2520as%2520information%2520about%2520the%250Adistributed%2520models%2520to%2520find%2520an%2520invariant%2520gradient%2520direction%2520across%2520all%2520domains%250Athrough%2520gradient%2520inner%2520product%2520maximization.%2520The%2520advantages%2520are%2520two-fold%253A%25201%2529%250AFedOMG%2520can%2520aggregate%2520the%2520characteristics%2520of%2520distributed%2520models%2520on%2520the%250Acentralized%2520server%2520without%2520incurring%2520any%2520additional%2520communication%2520cost%252C%2520and%25202%2529%250AFedOMG%2520is%2520orthogonal%2520to%2520many%2520existing%2520FL/FDG%2520methods%252C%2520allowing%2520for%2520additional%250Aperformance%2520improvements%2520by%2520being%2520seamlessly%2520integrated%2520with%2520them.%2520Extensive%250Aexperimental%2520evaluations%2520on%2520various%2520settings%2520to%2520demonstrate%2520the%2520robustness%2520of%250AFedOMG%2520compared%2520to%2520other%2520FL/FDG%2520baselines.%2520Our%2520method%2520outperforms%2520recent%2520SOTA%250Abaselines%2520on%2520four%2520FL%2520benchmark%2520datasets%2520%2528MNIST%252C%2520EMNIST%252C%2520CIFAR-10%252C%2520and%250ACIFAR-100%2529%252C%2520and%2520three%2520FDG%2520benchmark%2520datasets%2520%2528PACS%252C%2520VLCS%252C%2520and%2520OfficeHome%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14653v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Domain%20Generalization%20with%20Data-free%20On-server%20Matching%0A%20%20Gradient&entry.906535625=Trong-Binh%20Nguyen%20and%20Minh-Duong%20Nguyen%20and%20Jinsun%20Park%20and%20Quoc-Viet%20Pham%20and%20Won%20Joo%20Hwang&entry.1292438233=%20%20Domain%20Generalization%20%28DG%29%20aims%20to%20learn%20from%20multiple%20known%20source%20domains%20a%0Amodel%20that%20can%20generalize%20well%20to%20unknown%20target%20domains.%20One%20of%20the%20key%0Aapproaches%20in%20DG%20is%20training%20an%20encoder%20which%20generates%20domain-invariant%0Arepresentations.%20However%2C%20this%20approach%20is%20not%20applicable%20in%20Federated%20Domain%0AGeneralization%20%28FDG%29%2C%20where%20data%20from%20various%20domains%20are%20distributed%20across%0Adifferent%20clients.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20approach%2C%20dubbed%0AFederated%20Learning%20via%20On-server%20Matching%20Gradient%20%28FedOMG%29%2C%20which%20can%0A%5Cemph%7Befficiently%20leverage%20domain%20information%20from%20distributed%20domains%7D.%0ASpecifically%2C%20we%20utilize%20the%20local%20gradients%20as%20information%20about%20the%0Adistributed%20models%20to%20find%20an%20invariant%20gradient%20direction%20across%20all%20domains%0Athrough%20gradient%20inner%20product%20maximization.%20The%20advantages%20are%20two-fold%3A%201%29%0AFedOMG%20can%20aggregate%20the%20characteristics%20of%20distributed%20models%20on%20the%0Acentralized%20server%20without%20incurring%20any%20additional%20communication%20cost%2C%20and%202%29%0AFedOMG%20is%20orthogonal%20to%20many%20existing%20FL/FDG%20methods%2C%20allowing%20for%20additional%0Aperformance%20improvements%20by%20being%20seamlessly%20integrated%20with%20them.%20Extensive%0Aexperimental%20evaluations%20on%20various%20settings%20to%20demonstrate%20the%20robustness%20of%0AFedOMG%20compared%20to%20other%20FL/FDG%20baselines.%20Our%20method%20outperforms%20recent%20SOTA%0Abaselines%20on%20four%20FL%20benchmark%20datasets%20%28MNIST%2C%20EMNIST%2C%20CIFAR-10%2C%20and%0ACIFAR-100%29%2C%20and%20three%20FDG%20benchmark%20datasets%20%28PACS%2C%20VLCS%2C%20and%20OfficeHome%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14653v2&entry.124074799=Read"},
{"title": "Learning to Select In-Context Demonstration Preferred by Large Language\n  Model", "author": "Zheng Zhang and Shaocheng Lan and Lei Song and Jiang Bian and Yexin Li and Kan Ren", "abstract": "  In-context learning (ICL) enables large language models (LLMs) to adapt to\nnew tasks during inference using only a few demonstrations. However, ICL\nperformance is highly dependent on the selection of these demonstrations.\nRecent work explores retrieval-based methods for selecting query-specific\ndemonstrations, but these approaches often rely on surrogate objectives such as\nmetric learning, failing to directly optimize ICL performance. Consequently,\nthey struggle to identify truly beneficial demonstrations. Moreover, their\ndiscriminative retrieval paradigm is ineffective when the candidate pool lacks\nsufficient high-quality demonstrations. To address these challenges, we propose\nGenICL, a novel generative preference learning framework that leverages LLM\nfeedback to directly optimize demonstration selection for ICL. Experiments on\n19 datasets across 11 task categories demonstrate that GenICL achieves superior\nperformance than existing methods in selecting the most effective\ndemonstrations, leading to better ICL performance.\n", "link": "http://arxiv.org/abs/2505.19966v1", "date": "2025-05-26", "relevancy": 2.5883, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5217}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5217}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Select%20In-Context%20Demonstration%20Preferred%20by%20Large%20Language%0A%20%20Model&body=Title%3A%20Learning%20to%20Select%20In-Context%20Demonstration%20Preferred%20by%20Large%20Language%0A%20%20Model%0AAuthor%3A%20Zheng%20Zhang%20and%20Shaocheng%20Lan%20and%20Lei%20Song%20and%20Jiang%20Bian%20and%20Yexin%20Li%20and%20Kan%20Ren%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20enables%20large%20language%20models%20%28LLMs%29%20to%20adapt%20to%0Anew%20tasks%20during%20inference%20using%20only%20a%20few%20demonstrations.%20However%2C%20ICL%0Aperformance%20is%20highly%20dependent%20on%20the%20selection%20of%20these%20demonstrations.%0ARecent%20work%20explores%20retrieval-based%20methods%20for%20selecting%20query-specific%0Ademonstrations%2C%20but%20these%20approaches%20often%20rely%20on%20surrogate%20objectives%20such%20as%0Ametric%20learning%2C%20failing%20to%20directly%20optimize%20ICL%20performance.%20Consequently%2C%0Athey%20struggle%20to%20identify%20truly%20beneficial%20demonstrations.%20Moreover%2C%20their%0Adiscriminative%20retrieval%20paradigm%20is%20ineffective%20when%20the%20candidate%20pool%20lacks%0Asufficient%20high-quality%20demonstrations.%20To%20address%20these%20challenges%2C%20we%20propose%0AGenICL%2C%20a%20novel%20generative%20preference%20learning%20framework%20that%20leverages%20LLM%0Afeedback%20to%20directly%20optimize%20demonstration%20selection%20for%20ICL.%20Experiments%20on%0A19%20datasets%20across%2011%20task%20categories%20demonstrate%20that%20GenICL%20achieves%20superior%0Aperformance%20than%20existing%20methods%20in%20selecting%20the%20most%20effective%0Ademonstrations%2C%20leading%20to%20better%20ICL%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Select%2520In-Context%2520Demonstration%2520Preferred%2520by%2520Large%2520Language%250A%2520%2520Model%26entry.906535625%3DZheng%2520Zhang%2520and%2520Shaocheng%2520Lan%2520and%2520Lei%2520Song%2520and%2520Jiang%2520Bian%2520and%2520Yexin%2520Li%2520and%2520Kan%2520Ren%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520enables%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520adapt%2520to%250Anew%2520tasks%2520during%2520inference%2520using%2520only%2520a%2520few%2520demonstrations.%2520However%252C%2520ICL%250Aperformance%2520is%2520highly%2520dependent%2520on%2520the%2520selection%2520of%2520these%2520demonstrations.%250ARecent%2520work%2520explores%2520retrieval-based%2520methods%2520for%2520selecting%2520query-specific%250Ademonstrations%252C%2520but%2520these%2520approaches%2520often%2520rely%2520on%2520surrogate%2520objectives%2520such%2520as%250Ametric%2520learning%252C%2520failing%2520to%2520directly%2520optimize%2520ICL%2520performance.%2520Consequently%252C%250Athey%2520struggle%2520to%2520identify%2520truly%2520beneficial%2520demonstrations.%2520Moreover%252C%2520their%250Adiscriminative%2520retrieval%2520paradigm%2520is%2520ineffective%2520when%2520the%2520candidate%2520pool%2520lacks%250Asufficient%2520high-quality%2520demonstrations.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250AGenICL%252C%2520a%2520novel%2520generative%2520preference%2520learning%2520framework%2520that%2520leverages%2520LLM%250Afeedback%2520to%2520directly%2520optimize%2520demonstration%2520selection%2520for%2520ICL.%2520Experiments%2520on%250A19%2520datasets%2520across%252011%2520task%2520categories%2520demonstrate%2520that%2520GenICL%2520achieves%2520superior%250Aperformance%2520than%2520existing%2520methods%2520in%2520selecting%2520the%2520most%2520effective%250Ademonstrations%252C%2520leading%2520to%2520better%2520ICL%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Select%20In-Context%20Demonstration%20Preferred%20by%20Large%20Language%0A%20%20Model&entry.906535625=Zheng%20Zhang%20and%20Shaocheng%20Lan%20and%20Lei%20Song%20and%20Jiang%20Bian%20and%20Yexin%20Li%20and%20Kan%20Ren&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20enables%20large%20language%20models%20%28LLMs%29%20to%20adapt%20to%0Anew%20tasks%20during%20inference%20using%20only%20a%20few%20demonstrations.%20However%2C%20ICL%0Aperformance%20is%20highly%20dependent%20on%20the%20selection%20of%20these%20demonstrations.%0ARecent%20work%20explores%20retrieval-based%20methods%20for%20selecting%20query-specific%0Ademonstrations%2C%20but%20these%20approaches%20often%20rely%20on%20surrogate%20objectives%20such%20as%0Ametric%20learning%2C%20failing%20to%20directly%20optimize%20ICL%20performance.%20Consequently%2C%0Athey%20struggle%20to%20identify%20truly%20beneficial%20demonstrations.%20Moreover%2C%20their%0Adiscriminative%20retrieval%20paradigm%20is%20ineffective%20when%20the%20candidate%20pool%20lacks%0Asufficient%20high-quality%20demonstrations.%20To%20address%20these%20challenges%2C%20we%20propose%0AGenICL%2C%20a%20novel%20generative%20preference%20learning%20framework%20that%20leverages%20LLM%0Afeedback%20to%20directly%20optimize%20demonstration%20selection%20for%20ICL.%20Experiments%20on%0A19%20datasets%20across%2011%20task%20categories%20demonstrate%20that%20GenICL%20achieves%20superior%0Aperformance%20than%20existing%20methods%20in%20selecting%20the%20most%20effective%0Ademonstrations%2C%20leading%20to%20better%20ICL%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19966v1&entry.124074799=Read"},
{"title": "Dynamic-I2V: Exploring Image-to-Video Generaion Models via Multimodal\n  LLM", "author": "Peng Liu and Xiaoming Ren and Fengkai Liu and Qingsong Xie and Quanlong Zheng and Yanhao Zhang and Haonan Lu and Yujiu Yang", "abstract": "  Recent advancements in image-to-video (I2V) generation have shown promising\nperformance in conventional scenarios. However, these methods still encounter\nsignificant challenges when dealing with complex scenes that require a deep\nunderstanding of nuanced motion and intricate object-action relationships. To\naddress these challenges, we present Dynamic-I2V, an innovative framework that\nintegrates Multimodal Large Language Models (MLLMs) to jointly encode visual\nand textual conditions for a diffusion transformer (DiT) architecture. By\nleveraging the advanced multimodal understanding capabilities of MLLMs, our\nmodel significantly improves motion controllability and temporal coherence in\nsynthesized videos. The inherent multimodality of Dynamic-I2V further enables\nflexible support for diverse conditional inputs, extending its applicability to\nvarious downstream generation tasks. Through systematic analysis, we identify a\ncritical limitation in current I2V benchmarks: a significant bias towards\nfavoring low-dynamic videos, stemming from an inadequate balance between motion\ncomplexity and visual quality metrics. To resolve this evaluation gap, we\npropose DIVE - a novel assessment benchmark specifically designed for\ncomprehensive dynamic quality measurement in I2V generation. In conclusion,\nextensive quantitative and qualitative experiments confirm that Dynamic-I2V\nattains state-of-the-art performance in image-to-video generation, particularly\nrevealing significant improvements of 42.5%, 7.9%, and 11.8% in dynamic range,\ncontrollability, and quality, respectively, as assessed by the DIVE metric in\ncomparison to existing methods.\n", "link": "http://arxiv.org/abs/2505.19901v1", "date": "2025-05-26", "relevancy": 2.5877, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6611}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6546}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic-I2V%3A%20Exploring%20Image-to-Video%20Generaion%20Models%20via%20Multimodal%0A%20%20LLM&body=Title%3A%20Dynamic-I2V%3A%20Exploring%20Image-to-Video%20Generaion%20Models%20via%20Multimodal%0A%20%20LLM%0AAuthor%3A%20Peng%20Liu%20and%20Xiaoming%20Ren%20and%20Fengkai%20Liu%20and%20Qingsong%20Xie%20and%20Quanlong%20Zheng%20and%20Yanhao%20Zhang%20and%20Haonan%20Lu%20and%20Yujiu%20Yang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20image-to-video%20%28I2V%29%20generation%20have%20shown%20promising%0Aperformance%20in%20conventional%20scenarios.%20However%2C%20these%20methods%20still%20encounter%0Asignificant%20challenges%20when%20dealing%20with%20complex%20scenes%20that%20require%20a%20deep%0Aunderstanding%20of%20nuanced%20motion%20and%20intricate%20object-action%20relationships.%20To%0Aaddress%20these%20challenges%2C%20we%20present%20Dynamic-I2V%2C%20an%20innovative%20framework%20that%0Aintegrates%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%20jointly%20encode%20visual%0Aand%20textual%20conditions%20for%20a%20diffusion%20transformer%20%28DiT%29%20architecture.%20By%0Aleveraging%20the%20advanced%20multimodal%20understanding%20capabilities%20of%20MLLMs%2C%20our%0Amodel%20significantly%20improves%20motion%20controllability%20and%20temporal%20coherence%20in%0Asynthesized%20videos.%20The%20inherent%20multimodality%20of%20Dynamic-I2V%20further%20enables%0Aflexible%20support%20for%20diverse%20conditional%20inputs%2C%20extending%20its%20applicability%20to%0Avarious%20downstream%20generation%20tasks.%20Through%20systematic%20analysis%2C%20we%20identify%20a%0Acritical%20limitation%20in%20current%20I2V%20benchmarks%3A%20a%20significant%20bias%20towards%0Afavoring%20low-dynamic%20videos%2C%20stemming%20from%20an%20inadequate%20balance%20between%20motion%0Acomplexity%20and%20visual%20quality%20metrics.%20To%20resolve%20this%20evaluation%20gap%2C%20we%0Apropose%20DIVE%20-%20a%20novel%20assessment%20benchmark%20specifically%20designed%20for%0Acomprehensive%20dynamic%20quality%20measurement%20in%20I2V%20generation.%20In%20conclusion%2C%0Aextensive%20quantitative%20and%20qualitative%20experiments%20confirm%20that%20Dynamic-I2V%0Aattains%20state-of-the-art%20performance%20in%20image-to-video%20generation%2C%20particularly%0Arevealing%20significant%20improvements%20of%2042.5%25%2C%207.9%25%2C%20and%2011.8%25%20in%20dynamic%20range%2C%0Acontrollability%2C%20and%20quality%2C%20respectively%2C%20as%20assessed%20by%20the%20DIVE%20metric%20in%0Acomparison%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic-I2V%253A%2520Exploring%2520Image-to-Video%2520Generaion%2520Models%2520via%2520Multimodal%250A%2520%2520LLM%26entry.906535625%3DPeng%2520Liu%2520and%2520Xiaoming%2520Ren%2520and%2520Fengkai%2520Liu%2520and%2520Qingsong%2520Xie%2520and%2520Quanlong%2520Zheng%2520and%2520Yanhao%2520Zhang%2520and%2520Haonan%2520Lu%2520and%2520Yujiu%2520Yang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520image-to-video%2520%2528I2V%2529%2520generation%2520have%2520shown%2520promising%250Aperformance%2520in%2520conventional%2520scenarios.%2520However%252C%2520these%2520methods%2520still%2520encounter%250Asignificant%2520challenges%2520when%2520dealing%2520with%2520complex%2520scenes%2520that%2520require%2520a%2520deep%250Aunderstanding%2520of%2520nuanced%2520motion%2520and%2520intricate%2520object-action%2520relationships.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520present%2520Dynamic-I2V%252C%2520an%2520innovative%2520framework%2520that%250Aintegrates%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520to%2520jointly%2520encode%2520visual%250Aand%2520textual%2520conditions%2520for%2520a%2520diffusion%2520transformer%2520%2528DiT%2529%2520architecture.%2520By%250Aleveraging%2520the%2520advanced%2520multimodal%2520understanding%2520capabilities%2520of%2520MLLMs%252C%2520our%250Amodel%2520significantly%2520improves%2520motion%2520controllability%2520and%2520temporal%2520coherence%2520in%250Asynthesized%2520videos.%2520The%2520inherent%2520multimodality%2520of%2520Dynamic-I2V%2520further%2520enables%250Aflexible%2520support%2520for%2520diverse%2520conditional%2520inputs%252C%2520extending%2520its%2520applicability%2520to%250Avarious%2520downstream%2520generation%2520tasks.%2520Through%2520systematic%2520analysis%252C%2520we%2520identify%2520a%250Acritical%2520limitation%2520in%2520current%2520I2V%2520benchmarks%253A%2520a%2520significant%2520bias%2520towards%250Afavoring%2520low-dynamic%2520videos%252C%2520stemming%2520from%2520an%2520inadequate%2520balance%2520between%2520motion%250Acomplexity%2520and%2520visual%2520quality%2520metrics.%2520To%2520resolve%2520this%2520evaluation%2520gap%252C%2520we%250Apropose%2520DIVE%2520-%2520a%2520novel%2520assessment%2520benchmark%2520specifically%2520designed%2520for%250Acomprehensive%2520dynamic%2520quality%2520measurement%2520in%2520I2V%2520generation.%2520In%2520conclusion%252C%250Aextensive%2520quantitative%2520and%2520qualitative%2520experiments%2520confirm%2520that%2520Dynamic-I2V%250Aattains%2520state-of-the-art%2520performance%2520in%2520image-to-video%2520generation%252C%2520particularly%250Arevealing%2520significant%2520improvements%2520of%252042.5%2525%252C%25207.9%2525%252C%2520and%252011.8%2525%2520in%2520dynamic%2520range%252C%250Acontrollability%252C%2520and%2520quality%252C%2520respectively%252C%2520as%2520assessed%2520by%2520the%2520DIVE%2520metric%2520in%250Acomparison%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic-I2V%3A%20Exploring%20Image-to-Video%20Generaion%20Models%20via%20Multimodal%0A%20%20LLM&entry.906535625=Peng%20Liu%20and%20Xiaoming%20Ren%20and%20Fengkai%20Liu%20and%20Qingsong%20Xie%20and%20Quanlong%20Zheng%20and%20Yanhao%20Zhang%20and%20Haonan%20Lu%20and%20Yujiu%20Yang&entry.1292438233=%20%20Recent%20advancements%20in%20image-to-video%20%28I2V%29%20generation%20have%20shown%20promising%0Aperformance%20in%20conventional%20scenarios.%20However%2C%20these%20methods%20still%20encounter%0Asignificant%20challenges%20when%20dealing%20with%20complex%20scenes%20that%20require%20a%20deep%0Aunderstanding%20of%20nuanced%20motion%20and%20intricate%20object-action%20relationships.%20To%0Aaddress%20these%20challenges%2C%20we%20present%20Dynamic-I2V%2C%20an%20innovative%20framework%20that%0Aintegrates%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%20jointly%20encode%20visual%0Aand%20textual%20conditions%20for%20a%20diffusion%20transformer%20%28DiT%29%20architecture.%20By%0Aleveraging%20the%20advanced%20multimodal%20understanding%20capabilities%20of%20MLLMs%2C%20our%0Amodel%20significantly%20improves%20motion%20controllability%20and%20temporal%20coherence%20in%0Asynthesized%20videos.%20The%20inherent%20multimodality%20of%20Dynamic-I2V%20further%20enables%0Aflexible%20support%20for%20diverse%20conditional%20inputs%2C%20extending%20its%20applicability%20to%0Avarious%20downstream%20generation%20tasks.%20Through%20systematic%20analysis%2C%20we%20identify%20a%0Acritical%20limitation%20in%20current%20I2V%20benchmarks%3A%20a%20significant%20bias%20towards%0Afavoring%20low-dynamic%20videos%2C%20stemming%20from%20an%20inadequate%20balance%20between%20motion%0Acomplexity%20and%20visual%20quality%20metrics.%20To%20resolve%20this%20evaluation%20gap%2C%20we%0Apropose%20DIVE%20-%20a%20novel%20assessment%20benchmark%20specifically%20designed%20for%0Acomprehensive%20dynamic%20quality%20measurement%20in%20I2V%20generation.%20In%20conclusion%2C%0Aextensive%20quantitative%20and%20qualitative%20experiments%20confirm%20that%20Dynamic-I2V%0Aattains%20state-of-the-art%20performance%20in%20image-to-video%20generation%2C%20particularly%0Arevealing%20significant%20improvements%20of%2042.5%25%2C%207.9%25%2C%20and%2011.8%25%20in%20dynamic%20range%2C%0Acontrollability%2C%20and%20quality%2C%20respectively%2C%20as%20assessed%20by%20the%20DIVE%20metric%20in%0Acomparison%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19901v1&entry.124074799=Read"},
{"title": "Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective", "author": "Junnan Liu and Hongwei Liu and Linchen Xiao and Shudong Liu and Taolin Zhang and Zihan Ma and Songyang Zhang and Kai Chen", "abstract": "  We propose a novel framework for comprehending the reasoning capabilities of\nlarge language models (LLMs) through the perspective of meta-learning. By\nconceptualizing reasoning trajectories as pseudo-gradient descent updates to\nthe LLM's parameters, we identify parallels between LLM reasoning and various\nmeta-learning paradigms. We formalize the training process for reasoning tasks\nas a meta-learning setup, with each question treated as an individual task, and\nreasoning trajectories serving as the inner loop optimization for adapting\nmodel parameters. Once trained on a diverse set of questions, the LLM develops\nfundamental reasoning capabilities that can generalize to previously unseen\nquestions. Extensive empirical evaluations substantiate the strong connection\nbetween LLM reasoning and meta-learning, exploring several issues of\nsignificant interest from a meta-learning standpoint. Our work not only\nenhances the understanding of LLM reasoning but also provides practical\ninsights for improving these models through established meta-learning\ntechniques.\n", "link": "http://arxiv.org/abs/2505.19815v1", "date": "2025-05-26", "relevancy": 2.5714, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5233}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deciphering%20Trajectory-Aided%20LLM%20Reasoning%3A%20An%20Optimization%20Perspective&body=Title%3A%20Deciphering%20Trajectory-Aided%20LLM%20Reasoning%3A%20An%20Optimization%20Perspective%0AAuthor%3A%20Junnan%20Liu%20and%20Hongwei%20Liu%20and%20Linchen%20Xiao%20and%20Shudong%20Liu%20and%20Taolin%20Zhang%20and%20Zihan%20Ma%20and%20Songyang%20Zhang%20and%20Kai%20Chen%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20framework%20for%20comprehending%20the%20reasoning%20capabilities%20of%0Alarge%20language%20models%20%28LLMs%29%20through%20the%20perspective%20of%20meta-learning.%20By%0Aconceptualizing%20reasoning%20trajectories%20as%20pseudo-gradient%20descent%20updates%20to%0Athe%20LLM%27s%20parameters%2C%20we%20identify%20parallels%20between%20LLM%20reasoning%20and%20various%0Ameta-learning%20paradigms.%20We%20formalize%20the%20training%20process%20for%20reasoning%20tasks%0Aas%20a%20meta-learning%20setup%2C%20with%20each%20question%20treated%20as%20an%20individual%20task%2C%20and%0Areasoning%20trajectories%20serving%20as%20the%20inner%20loop%20optimization%20for%20adapting%0Amodel%20parameters.%20Once%20trained%20on%20a%20diverse%20set%20of%20questions%2C%20the%20LLM%20develops%0Afundamental%20reasoning%20capabilities%20that%20can%20generalize%20to%20previously%20unseen%0Aquestions.%20Extensive%20empirical%20evaluations%20substantiate%20the%20strong%20connection%0Abetween%20LLM%20reasoning%20and%20meta-learning%2C%20exploring%20several%20issues%20of%0Asignificant%20interest%20from%20a%20meta-learning%20standpoint.%20Our%20work%20not%20only%0Aenhances%20the%20understanding%20of%20LLM%20reasoning%20but%20also%20provides%20practical%0Ainsights%20for%20improving%20these%20models%20through%20established%20meta-learning%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeciphering%2520Trajectory-Aided%2520LLM%2520Reasoning%253A%2520An%2520Optimization%2520Perspective%26entry.906535625%3DJunnan%2520Liu%2520and%2520Hongwei%2520Liu%2520and%2520Linchen%2520Xiao%2520and%2520Shudong%2520Liu%2520and%2520Taolin%2520Zhang%2520and%2520Zihan%2520Ma%2520and%2520Songyang%2520Zhang%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520framework%2520for%2520comprehending%2520the%2520reasoning%2520capabilities%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520through%2520the%2520perspective%2520of%2520meta-learning.%2520By%250Aconceptualizing%2520reasoning%2520trajectories%2520as%2520pseudo-gradient%2520descent%2520updates%2520to%250Athe%2520LLM%2527s%2520parameters%252C%2520we%2520identify%2520parallels%2520between%2520LLM%2520reasoning%2520and%2520various%250Ameta-learning%2520paradigms.%2520We%2520formalize%2520the%2520training%2520process%2520for%2520reasoning%2520tasks%250Aas%2520a%2520meta-learning%2520setup%252C%2520with%2520each%2520question%2520treated%2520as%2520an%2520individual%2520task%252C%2520and%250Areasoning%2520trajectories%2520serving%2520as%2520the%2520inner%2520loop%2520optimization%2520for%2520adapting%250Amodel%2520parameters.%2520Once%2520trained%2520on%2520a%2520diverse%2520set%2520of%2520questions%252C%2520the%2520LLM%2520develops%250Afundamental%2520reasoning%2520capabilities%2520that%2520can%2520generalize%2520to%2520previously%2520unseen%250Aquestions.%2520Extensive%2520empirical%2520evaluations%2520substantiate%2520the%2520strong%2520connection%250Abetween%2520LLM%2520reasoning%2520and%2520meta-learning%252C%2520exploring%2520several%2520issues%2520of%250Asignificant%2520interest%2520from%2520a%2520meta-learning%2520standpoint.%2520Our%2520work%2520not%2520only%250Aenhances%2520the%2520understanding%2520of%2520LLM%2520reasoning%2520but%2520also%2520provides%2520practical%250Ainsights%2520for%2520improving%2520these%2520models%2520through%2520established%2520meta-learning%250Atechniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deciphering%20Trajectory-Aided%20LLM%20Reasoning%3A%20An%20Optimization%20Perspective&entry.906535625=Junnan%20Liu%20and%20Hongwei%20Liu%20and%20Linchen%20Xiao%20and%20Shudong%20Liu%20and%20Taolin%20Zhang%20and%20Zihan%20Ma%20and%20Songyang%20Zhang%20and%20Kai%20Chen&entry.1292438233=%20%20We%20propose%20a%20novel%20framework%20for%20comprehending%20the%20reasoning%20capabilities%20of%0Alarge%20language%20models%20%28LLMs%29%20through%20the%20perspective%20of%20meta-learning.%20By%0Aconceptualizing%20reasoning%20trajectories%20as%20pseudo-gradient%20descent%20updates%20to%0Athe%20LLM%27s%20parameters%2C%20we%20identify%20parallels%20between%20LLM%20reasoning%20and%20various%0Ameta-learning%20paradigms.%20We%20formalize%20the%20training%20process%20for%20reasoning%20tasks%0Aas%20a%20meta-learning%20setup%2C%20with%20each%20question%20treated%20as%20an%20individual%20task%2C%20and%0Areasoning%20trajectories%20serving%20as%20the%20inner%20loop%20optimization%20for%20adapting%0Amodel%20parameters.%20Once%20trained%20on%20a%20diverse%20set%20of%20questions%2C%20the%20LLM%20develops%0Afundamental%20reasoning%20capabilities%20that%20can%20generalize%20to%20previously%20unseen%0Aquestions.%20Extensive%20empirical%20evaluations%20substantiate%20the%20strong%20connection%0Abetween%20LLM%20reasoning%20and%20meta-learning%2C%20exploring%20several%20issues%20of%0Asignificant%20interest%20from%20a%20meta-learning%20standpoint.%20Our%20work%20not%20only%0Aenhances%20the%20understanding%20of%20LLM%20reasoning%20but%20also%20provides%20practical%0Ainsights%20for%20improving%20these%20models%20through%20established%20meta-learning%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19815v1&entry.124074799=Read"},
{"title": "Controlling Neural Collapse Enhances Out-of-Distribution Detection and\n  Transfer Learning", "author": "Md Yousuf Harun and Jhair Gallardo and Christopher Kanan", "abstract": "  Out-of-distribution (OOD) detection and OOD generalization are widely studied\nin Deep Neural Networks (DNNs), yet their relationship remains poorly\nunderstood. We empirically show that the degree of Neural Collapse (NC) in a\nnetwork layer is inversely related with these objectives: stronger NC improves\nOOD detection but degrades generalization, while weaker NC enhances\ngeneralization at the cost of detection. This trade-off suggests that a single\nfeature space cannot simultaneously achieve both tasks. To address this, we\ndevelop a theoretical framework linking NC to OOD detection and generalization.\nWe show that entropy regularization mitigates NC to improve generalization,\nwhile a fixed Simplex Equiangular Tight Frame (ETF) projector enforces NC for\nbetter detection. Based on these insights, we propose a method to control NC at\ndifferent DNN layers. In experiments, our method excels at both tasks across\nOOD datasets and DNN architectures.\n", "link": "http://arxiv.org/abs/2502.10691v2", "date": "2025-05-26", "relevancy": 2.5701, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5587}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4958}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controlling%20Neural%20Collapse%20Enhances%20Out-of-Distribution%20Detection%20and%0A%20%20Transfer%20Learning&body=Title%3A%20Controlling%20Neural%20Collapse%20Enhances%20Out-of-Distribution%20Detection%20and%0A%20%20Transfer%20Learning%0AAuthor%3A%20Md%20Yousuf%20Harun%20and%20Jhair%20Gallardo%20and%20Christopher%20Kanan%0AAbstract%3A%20%20%20Out-of-distribution%20%28OOD%29%20detection%20and%20OOD%20generalization%20are%20widely%20studied%0Ain%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20yet%20their%20relationship%20remains%20poorly%0Aunderstood.%20We%20empirically%20show%20that%20the%20degree%20of%20Neural%20Collapse%20%28NC%29%20in%20a%0Anetwork%20layer%20is%20inversely%20related%20with%20these%20objectives%3A%20stronger%20NC%20improves%0AOOD%20detection%20but%20degrades%20generalization%2C%20while%20weaker%20NC%20enhances%0Ageneralization%20at%20the%20cost%20of%20detection.%20This%20trade-off%20suggests%20that%20a%20single%0Afeature%20space%20cannot%20simultaneously%20achieve%20both%20tasks.%20To%20address%20this%2C%20we%0Adevelop%20a%20theoretical%20framework%20linking%20NC%20to%20OOD%20detection%20and%20generalization.%0AWe%20show%20that%20entropy%20regularization%20mitigates%20NC%20to%20improve%20generalization%2C%0Awhile%20a%20fixed%20Simplex%20Equiangular%20Tight%20Frame%20%28ETF%29%20projector%20enforces%20NC%20for%0Abetter%20detection.%20Based%20on%20these%20insights%2C%20we%20propose%20a%20method%20to%20control%20NC%20at%0Adifferent%20DNN%20layers.%20In%20experiments%2C%20our%20method%20excels%20at%20both%20tasks%20across%0AOOD%20datasets%20and%20DNN%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10691v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControlling%2520Neural%2520Collapse%2520Enhances%2520Out-of-Distribution%2520Detection%2520and%250A%2520%2520Transfer%2520Learning%26entry.906535625%3DMd%2520Yousuf%2520Harun%2520and%2520Jhair%2520Gallardo%2520and%2520Christopher%2520Kanan%26entry.1292438233%3D%2520%2520Out-of-distribution%2520%2528OOD%2529%2520detection%2520and%2520OOD%2520generalization%2520are%2520widely%2520studied%250Ain%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%252C%2520yet%2520their%2520relationship%2520remains%2520poorly%250Aunderstood.%2520We%2520empirically%2520show%2520that%2520the%2520degree%2520of%2520Neural%2520Collapse%2520%2528NC%2529%2520in%2520a%250Anetwork%2520layer%2520is%2520inversely%2520related%2520with%2520these%2520objectives%253A%2520stronger%2520NC%2520improves%250AOOD%2520detection%2520but%2520degrades%2520generalization%252C%2520while%2520weaker%2520NC%2520enhances%250Ageneralization%2520at%2520the%2520cost%2520of%2520detection.%2520This%2520trade-off%2520suggests%2520that%2520a%2520single%250Afeature%2520space%2520cannot%2520simultaneously%2520achieve%2520both%2520tasks.%2520To%2520address%2520this%252C%2520we%250Adevelop%2520a%2520theoretical%2520framework%2520linking%2520NC%2520to%2520OOD%2520detection%2520and%2520generalization.%250AWe%2520show%2520that%2520entropy%2520regularization%2520mitigates%2520NC%2520to%2520improve%2520generalization%252C%250Awhile%2520a%2520fixed%2520Simplex%2520Equiangular%2520Tight%2520Frame%2520%2528ETF%2529%2520projector%2520enforces%2520NC%2520for%250Abetter%2520detection.%2520Based%2520on%2520these%2520insights%252C%2520we%2520propose%2520a%2520method%2520to%2520control%2520NC%2520at%250Adifferent%2520DNN%2520layers.%2520In%2520experiments%252C%2520our%2520method%2520excels%2520at%2520both%2520tasks%2520across%250AOOD%2520datasets%2520and%2520DNN%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10691v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controlling%20Neural%20Collapse%20Enhances%20Out-of-Distribution%20Detection%20and%0A%20%20Transfer%20Learning&entry.906535625=Md%20Yousuf%20Harun%20and%20Jhair%20Gallardo%20and%20Christopher%20Kanan&entry.1292438233=%20%20Out-of-distribution%20%28OOD%29%20detection%20and%20OOD%20generalization%20are%20widely%20studied%0Ain%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20yet%20their%20relationship%20remains%20poorly%0Aunderstood.%20We%20empirically%20show%20that%20the%20degree%20of%20Neural%20Collapse%20%28NC%29%20in%20a%0Anetwork%20layer%20is%20inversely%20related%20with%20these%20objectives%3A%20stronger%20NC%20improves%0AOOD%20detection%20but%20degrades%20generalization%2C%20while%20weaker%20NC%20enhances%0Ageneralization%20at%20the%20cost%20of%20detection.%20This%20trade-off%20suggests%20that%20a%20single%0Afeature%20space%20cannot%20simultaneously%20achieve%20both%20tasks.%20To%20address%20this%2C%20we%0Adevelop%20a%20theoretical%20framework%20linking%20NC%20to%20OOD%20detection%20and%20generalization.%0AWe%20show%20that%20entropy%20regularization%20mitigates%20NC%20to%20improve%20generalization%2C%0Awhile%20a%20fixed%20Simplex%20Equiangular%20Tight%20Frame%20%28ETF%29%20projector%20enforces%20NC%20for%0Abetter%20detection.%20Based%20on%20these%20insights%2C%20we%20propose%20a%20method%20to%20control%20NC%20at%0Adifferent%20DNN%20layers.%20In%20experiments%2C%20our%20method%20excels%20at%20both%20tasks%20across%0AOOD%20datasets%20and%20DNN%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10691v2&entry.124074799=Read"},
{"title": "A Responsible Face Recognition Approach for Small and Mid-Scale Systems\n  Through Personalized Neural Networks", "author": "Sebastian Gro\u00df and Stefan Heindorf and Philipp Terh\u00f6rst", "abstract": "  Traditional face recognition systems rely on extracting fixed face\nrepresentations, known as templates, to store and verify identities. These\nrepresentations are typically generated by neural networks that often lack\nexplainability and raise concerns regarding fairness and privacy. In this work,\nwe propose a novel model-template (MOTE) approach that replaces vector-based\nface templates with small personalized neural networks. This design enables\nmore responsible face recognition for small and medium-scale systems. During\nenrollment, MOTE creates a dedicated binary classifier for each identity,\ntrained to determine whether an input face matches the enrolled identity. Each\nclassifier is trained using only a single reference sample, along with\nsynthetically balanced samples to allow adjusting fairness at the level of a\nsingle individual during enrollment. Extensive experiments across multiple\ndatasets and recognition systems demonstrate substantial improvements in\nfairness and particularly in privacy. Although the method increases inference\ntime and storage requirements, it presents a strong solution for small- and\nmid-scale applications where fairness and privacy are critical.\n", "link": "http://arxiv.org/abs/2505.19920v1", "date": "2025-05-26", "relevancy": 2.5677, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5393}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5068}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Responsible%20Face%20Recognition%20Approach%20for%20Small%20and%20Mid-Scale%20Systems%0A%20%20Through%20Personalized%20Neural%20Networks&body=Title%3A%20A%20Responsible%20Face%20Recognition%20Approach%20for%20Small%20and%20Mid-Scale%20Systems%0A%20%20Through%20Personalized%20Neural%20Networks%0AAuthor%3A%20Sebastian%20Gro%C3%9F%20and%20Stefan%20Heindorf%20and%20Philipp%20Terh%C3%B6rst%0AAbstract%3A%20%20%20Traditional%20face%20recognition%20systems%20rely%20on%20extracting%20fixed%20face%0Arepresentations%2C%20known%20as%20templates%2C%20to%20store%20and%20verify%20identities.%20These%0Arepresentations%20are%20typically%20generated%20by%20neural%20networks%20that%20often%20lack%0Aexplainability%20and%20raise%20concerns%20regarding%20fairness%20and%20privacy.%20In%20this%20work%2C%0Awe%20propose%20a%20novel%20model-template%20%28MOTE%29%20approach%20that%20replaces%20vector-based%0Aface%20templates%20with%20small%20personalized%20neural%20networks.%20This%20design%20enables%0Amore%20responsible%20face%20recognition%20for%20small%20and%20medium-scale%20systems.%20During%0Aenrollment%2C%20MOTE%20creates%20a%20dedicated%20binary%20classifier%20for%20each%20identity%2C%0Atrained%20to%20determine%20whether%20an%20input%20face%20matches%20the%20enrolled%20identity.%20Each%0Aclassifier%20is%20trained%20using%20only%20a%20single%20reference%20sample%2C%20along%20with%0Asynthetically%20balanced%20samples%20to%20allow%20adjusting%20fairness%20at%20the%20level%20of%20a%0Asingle%20individual%20during%20enrollment.%20Extensive%20experiments%20across%20multiple%0Adatasets%20and%20recognition%20systems%20demonstrate%20substantial%20improvements%20in%0Afairness%20and%20particularly%20in%20privacy.%20Although%20the%20method%20increases%20inference%0Atime%20and%20storage%20requirements%2C%20it%20presents%20a%20strong%20solution%20for%20small-%20and%0Amid-scale%20applications%20where%20fairness%20and%20privacy%20are%20critical.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Responsible%2520Face%2520Recognition%2520Approach%2520for%2520Small%2520and%2520Mid-Scale%2520Systems%250A%2520%2520Through%2520Personalized%2520Neural%2520Networks%26entry.906535625%3DSebastian%2520Gro%25C3%259F%2520and%2520Stefan%2520Heindorf%2520and%2520Philipp%2520Terh%25C3%25B6rst%26entry.1292438233%3D%2520%2520Traditional%2520face%2520recognition%2520systems%2520rely%2520on%2520extracting%2520fixed%2520face%250Arepresentations%252C%2520known%2520as%2520templates%252C%2520to%2520store%2520and%2520verify%2520identities.%2520These%250Arepresentations%2520are%2520typically%2520generated%2520by%2520neural%2520networks%2520that%2520often%2520lack%250Aexplainability%2520and%2520raise%2520concerns%2520regarding%2520fairness%2520and%2520privacy.%2520In%2520this%2520work%252C%250Awe%2520propose%2520a%2520novel%2520model-template%2520%2528MOTE%2529%2520approach%2520that%2520replaces%2520vector-based%250Aface%2520templates%2520with%2520small%2520personalized%2520neural%2520networks.%2520This%2520design%2520enables%250Amore%2520responsible%2520face%2520recognition%2520for%2520small%2520and%2520medium-scale%2520systems.%2520During%250Aenrollment%252C%2520MOTE%2520creates%2520a%2520dedicated%2520binary%2520classifier%2520for%2520each%2520identity%252C%250Atrained%2520to%2520determine%2520whether%2520an%2520input%2520face%2520matches%2520the%2520enrolled%2520identity.%2520Each%250Aclassifier%2520is%2520trained%2520using%2520only%2520a%2520single%2520reference%2520sample%252C%2520along%2520with%250Asynthetically%2520balanced%2520samples%2520to%2520allow%2520adjusting%2520fairness%2520at%2520the%2520level%2520of%2520a%250Asingle%2520individual%2520during%2520enrollment.%2520Extensive%2520experiments%2520across%2520multiple%250Adatasets%2520and%2520recognition%2520systems%2520demonstrate%2520substantial%2520improvements%2520in%250Afairness%2520and%2520particularly%2520in%2520privacy.%2520Although%2520the%2520method%2520increases%2520inference%250Atime%2520and%2520storage%2520requirements%252C%2520it%2520presents%2520a%2520strong%2520solution%2520for%2520small-%2520and%250Amid-scale%2520applications%2520where%2520fairness%2520and%2520privacy%2520are%2520critical.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Responsible%20Face%20Recognition%20Approach%20for%20Small%20and%20Mid-Scale%20Systems%0A%20%20Through%20Personalized%20Neural%20Networks&entry.906535625=Sebastian%20Gro%C3%9F%20and%20Stefan%20Heindorf%20and%20Philipp%20Terh%C3%B6rst&entry.1292438233=%20%20Traditional%20face%20recognition%20systems%20rely%20on%20extracting%20fixed%20face%0Arepresentations%2C%20known%20as%20templates%2C%20to%20store%20and%20verify%20identities.%20These%0Arepresentations%20are%20typically%20generated%20by%20neural%20networks%20that%20often%20lack%0Aexplainability%20and%20raise%20concerns%20regarding%20fairness%20and%20privacy.%20In%20this%20work%2C%0Awe%20propose%20a%20novel%20model-template%20%28MOTE%29%20approach%20that%20replaces%20vector-based%0Aface%20templates%20with%20small%20personalized%20neural%20networks.%20This%20design%20enables%0Amore%20responsible%20face%20recognition%20for%20small%20and%20medium-scale%20systems.%20During%0Aenrollment%2C%20MOTE%20creates%20a%20dedicated%20binary%20classifier%20for%20each%20identity%2C%0Atrained%20to%20determine%20whether%20an%20input%20face%20matches%20the%20enrolled%20identity.%20Each%0Aclassifier%20is%20trained%20using%20only%20a%20single%20reference%20sample%2C%20along%20with%0Asynthetically%20balanced%20samples%20to%20allow%20adjusting%20fairness%20at%20the%20level%20of%20a%0Asingle%20individual%20during%20enrollment.%20Extensive%20experiments%20across%20multiple%0Adatasets%20and%20recognition%20systems%20demonstrate%20substantial%20improvements%20in%0Afairness%20and%20particularly%20in%20privacy.%20Although%20the%20method%20increases%20inference%0Atime%20and%20storage%20requirements%2C%20it%20presents%20a%20strong%20solution%20for%20small-%20and%0Amid-scale%20applications%20where%20fairness%20and%20privacy%20are%20critical.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19920v1&entry.124074799=Read"},
{"title": "PHI: Bridging Domain Shift in Long-Term Action Quality Assessment via\n  Progressive Hierarchical Instruction", "author": "Kanglei Zhou and Hubert P. H. Shum and Frederick W. B. Li and Xingxing Zhang and Xiaohui Liang", "abstract": "  Long-term Action Quality Assessment (AQA) aims to evaluate the quantitative\nperformance of actions in long videos. However, existing methods face\nchallenges due to domain shifts between the pre-trained large-scale action\nrecognition backbones and the specific AQA task, thereby hindering their\nperformance. This arises since fine-tuning resource-intensive backbones on\nsmall AQA datasets is impractical. We address this by identifying two levels of\ndomain shift: task-level, regarding differences in task objectives, and\nfeature-level, regarding differences in important features. For feature-level\nshifts, which are more detrimental, we propose Progressive Hierarchical\nInstruction (PHI) with two strategies. First, Gap Minimization Flow (GMF)\nleverages flow matching to progressively learn a fast flow path that reduces\nthe domain gap between initial and desired features across shallow to deep\nlayers. Additionally, a temporally-enhanced attention module captures\nlong-range dependencies essential for AQA. Second, List-wise Contrastive\nRegularization (LCR) facilitates coarse-to-fine alignment by comprehensively\ncomparing batch pairs to learn fine-grained cues while mitigating domain shift.\nIntegrating these modules, PHI offers an effective solution. Experiments\ndemonstrate that PHI achieves state-of-the-art performance on three\nrepresentative long-term AQA datasets, proving its superiority in addressing\nthe domain shift for long-term AQA.\n", "link": "http://arxiv.org/abs/2505.19972v1", "date": "2025-05-26", "relevancy": 2.5637, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5208}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5112}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PHI%3A%20Bridging%20Domain%20Shift%20in%20Long-Term%20Action%20Quality%20Assessment%20via%0A%20%20Progressive%20Hierarchical%20Instruction&body=Title%3A%20PHI%3A%20Bridging%20Domain%20Shift%20in%20Long-Term%20Action%20Quality%20Assessment%20via%0A%20%20Progressive%20Hierarchical%20Instruction%0AAuthor%3A%20Kanglei%20Zhou%20and%20Hubert%20P.%20H.%20Shum%20and%20Frederick%20W.%20B.%20Li%20and%20Xingxing%20Zhang%20and%20Xiaohui%20Liang%0AAbstract%3A%20%20%20Long-term%20Action%20Quality%20Assessment%20%28AQA%29%20aims%20to%20evaluate%20the%20quantitative%0Aperformance%20of%20actions%20in%20long%20videos.%20However%2C%20existing%20methods%20face%0Achallenges%20due%20to%20domain%20shifts%20between%20the%20pre-trained%20large-scale%20action%0Arecognition%20backbones%20and%20the%20specific%20AQA%20task%2C%20thereby%20hindering%20their%0Aperformance.%20This%20arises%20since%20fine-tuning%20resource-intensive%20backbones%20on%0Asmall%20AQA%20datasets%20is%20impractical.%20We%20address%20this%20by%20identifying%20two%20levels%20of%0Adomain%20shift%3A%20task-level%2C%20regarding%20differences%20in%20task%20objectives%2C%20and%0Afeature-level%2C%20regarding%20differences%20in%20important%20features.%20For%20feature-level%0Ashifts%2C%20which%20are%20more%20detrimental%2C%20we%20propose%20Progressive%20Hierarchical%0AInstruction%20%28PHI%29%20with%20two%20strategies.%20First%2C%20Gap%20Minimization%20Flow%20%28GMF%29%0Aleverages%20flow%20matching%20to%20progressively%20learn%20a%20fast%20flow%20path%20that%20reduces%0Athe%20domain%20gap%20between%20initial%20and%20desired%20features%20across%20shallow%20to%20deep%0Alayers.%20Additionally%2C%20a%20temporally-enhanced%20attention%20module%20captures%0Along-range%20dependencies%20essential%20for%20AQA.%20Second%2C%20List-wise%20Contrastive%0ARegularization%20%28LCR%29%20facilitates%20coarse-to-fine%20alignment%20by%20comprehensively%0Acomparing%20batch%20pairs%20to%20learn%20fine-grained%20cues%20while%20mitigating%20domain%20shift.%0AIntegrating%20these%20modules%2C%20PHI%20offers%20an%20effective%20solution.%20Experiments%0Ademonstrate%20that%20PHI%20achieves%20state-of-the-art%20performance%20on%20three%0Arepresentative%20long-term%20AQA%20datasets%2C%20proving%20its%20superiority%20in%20addressing%0Athe%20domain%20shift%20for%20long-term%20AQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19972v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPHI%253A%2520Bridging%2520Domain%2520Shift%2520in%2520Long-Term%2520Action%2520Quality%2520Assessment%2520via%250A%2520%2520Progressive%2520Hierarchical%2520Instruction%26entry.906535625%3DKanglei%2520Zhou%2520and%2520Hubert%2520P.%2520H.%2520Shum%2520and%2520Frederick%2520W.%2520B.%2520Li%2520and%2520Xingxing%2520Zhang%2520and%2520Xiaohui%2520Liang%26entry.1292438233%3D%2520%2520Long-term%2520Action%2520Quality%2520Assessment%2520%2528AQA%2529%2520aims%2520to%2520evaluate%2520the%2520quantitative%250Aperformance%2520of%2520actions%2520in%2520long%2520videos.%2520However%252C%2520existing%2520methods%2520face%250Achallenges%2520due%2520to%2520domain%2520shifts%2520between%2520the%2520pre-trained%2520large-scale%2520action%250Arecognition%2520backbones%2520and%2520the%2520specific%2520AQA%2520task%252C%2520thereby%2520hindering%2520their%250Aperformance.%2520This%2520arises%2520since%2520fine-tuning%2520resource-intensive%2520backbones%2520on%250Asmall%2520AQA%2520datasets%2520is%2520impractical.%2520We%2520address%2520this%2520by%2520identifying%2520two%2520levels%2520of%250Adomain%2520shift%253A%2520task-level%252C%2520regarding%2520differences%2520in%2520task%2520objectives%252C%2520and%250Afeature-level%252C%2520regarding%2520differences%2520in%2520important%2520features.%2520For%2520feature-level%250Ashifts%252C%2520which%2520are%2520more%2520detrimental%252C%2520we%2520propose%2520Progressive%2520Hierarchical%250AInstruction%2520%2528PHI%2529%2520with%2520two%2520strategies.%2520First%252C%2520Gap%2520Minimization%2520Flow%2520%2528GMF%2529%250Aleverages%2520flow%2520matching%2520to%2520progressively%2520learn%2520a%2520fast%2520flow%2520path%2520that%2520reduces%250Athe%2520domain%2520gap%2520between%2520initial%2520and%2520desired%2520features%2520across%2520shallow%2520to%2520deep%250Alayers.%2520Additionally%252C%2520a%2520temporally-enhanced%2520attention%2520module%2520captures%250Along-range%2520dependencies%2520essential%2520for%2520AQA.%2520Second%252C%2520List-wise%2520Contrastive%250ARegularization%2520%2528LCR%2529%2520facilitates%2520coarse-to-fine%2520alignment%2520by%2520comprehensively%250Acomparing%2520batch%2520pairs%2520to%2520learn%2520fine-grained%2520cues%2520while%2520mitigating%2520domain%2520shift.%250AIntegrating%2520these%2520modules%252C%2520PHI%2520offers%2520an%2520effective%2520solution.%2520Experiments%250Ademonstrate%2520that%2520PHI%2520achieves%2520state-of-the-art%2520performance%2520on%2520three%250Arepresentative%2520long-term%2520AQA%2520datasets%252C%2520proving%2520its%2520superiority%2520in%2520addressing%250Athe%2520domain%2520shift%2520for%2520long-term%2520AQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19972v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PHI%3A%20Bridging%20Domain%20Shift%20in%20Long-Term%20Action%20Quality%20Assessment%20via%0A%20%20Progressive%20Hierarchical%20Instruction&entry.906535625=Kanglei%20Zhou%20and%20Hubert%20P.%20H.%20Shum%20and%20Frederick%20W.%20B.%20Li%20and%20Xingxing%20Zhang%20and%20Xiaohui%20Liang&entry.1292438233=%20%20Long-term%20Action%20Quality%20Assessment%20%28AQA%29%20aims%20to%20evaluate%20the%20quantitative%0Aperformance%20of%20actions%20in%20long%20videos.%20However%2C%20existing%20methods%20face%0Achallenges%20due%20to%20domain%20shifts%20between%20the%20pre-trained%20large-scale%20action%0Arecognition%20backbones%20and%20the%20specific%20AQA%20task%2C%20thereby%20hindering%20their%0Aperformance.%20This%20arises%20since%20fine-tuning%20resource-intensive%20backbones%20on%0Asmall%20AQA%20datasets%20is%20impractical.%20We%20address%20this%20by%20identifying%20two%20levels%20of%0Adomain%20shift%3A%20task-level%2C%20regarding%20differences%20in%20task%20objectives%2C%20and%0Afeature-level%2C%20regarding%20differences%20in%20important%20features.%20For%20feature-level%0Ashifts%2C%20which%20are%20more%20detrimental%2C%20we%20propose%20Progressive%20Hierarchical%0AInstruction%20%28PHI%29%20with%20two%20strategies.%20First%2C%20Gap%20Minimization%20Flow%20%28GMF%29%0Aleverages%20flow%20matching%20to%20progressively%20learn%20a%20fast%20flow%20path%20that%20reduces%0Athe%20domain%20gap%20between%20initial%20and%20desired%20features%20across%20shallow%20to%20deep%0Alayers.%20Additionally%2C%20a%20temporally-enhanced%20attention%20module%20captures%0Along-range%20dependencies%20essential%20for%20AQA.%20Second%2C%20List-wise%20Contrastive%0ARegularization%20%28LCR%29%20facilitates%20coarse-to-fine%20alignment%20by%20comprehensively%0Acomparing%20batch%20pairs%20to%20learn%20fine-grained%20cues%20while%20mitigating%20domain%20shift.%0AIntegrating%20these%20modules%2C%20PHI%20offers%20an%20effective%20solution.%20Experiments%0Ademonstrate%20that%20PHI%20achieves%20state-of-the-art%20performance%20on%20three%0Arepresentative%20long-term%20AQA%20datasets%2C%20proving%20its%20superiority%20in%20addressing%0Athe%20domain%20shift%20for%20long-term%20AQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19972v1&entry.124074799=Read"},
{"title": "Agentic 3D Scene Generation with Spatially Contextualized VLMs", "author": "Xinhang Liu and Yu-Wing Tai and Chi-Keung Tang", "abstract": "  Despite recent advances in multimodal content generation enabled by\nvision-language models (VLMs), their ability to reason about and generate\nstructured 3D scenes remains largely underexplored. This limitation constrains\ntheir utility in spatially grounded tasks such as embodied AI, immersive\nsimulations, and interactive 3D applications. We introduce a new paradigm that\nenables VLMs to generate, understand, and edit complex 3D environments by\ninjecting a continually evolving spatial context. Constructed from multimodal\ninput, this context consists of three components: a scene portrait that\nprovides a high-level semantic blueprint, a semantically labeled point cloud\ncapturing object-level geometry, and a scene hypergraph that encodes rich\nspatial relationships, including unary, binary, and higher-order constraints.\nTogether, these components provide the VLM with a structured, geometry-aware\nworking memory that integrates its inherent multimodal reasoning capabilities\nwith structured 3D understanding for effective spatial reasoning. Building on\nthis foundation, we develop an agentic 3D scene generation pipeline in which\nthe VLM iteratively reads from and updates the spatial context. The pipeline\nfeatures high-quality asset generation with geometric restoration, environment\nsetup with automatic verification, and ergonomic adjustment guided by the scene\nhypergraph. Experiments show that our framework can handle diverse and\nchallenging inputs, achieving a level of generalization not observed in prior\nwork. Further results demonstrate that injecting spatial context enables VLMs\nto perform downstream tasks such as interactive scene editing and path\nplanning, suggesting strong potential for spatially intelligent systems in\ncomputer graphics, 3D vision, and embodied applications.\n", "link": "http://arxiv.org/abs/2505.20129v1", "date": "2025-05-26", "relevancy": 2.5579, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6445}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6445}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentic%203D%20Scene%20Generation%20with%20Spatially%20Contextualized%20VLMs&body=Title%3A%20Agentic%203D%20Scene%20Generation%20with%20Spatially%20Contextualized%20VLMs%0AAuthor%3A%20Xinhang%20Liu%20and%20Yu-Wing%20Tai%20and%20Chi-Keung%20Tang%0AAbstract%3A%20%20%20Despite%20recent%20advances%20in%20multimodal%20content%20generation%20enabled%20by%0Avision-language%20models%20%28VLMs%29%2C%20their%20ability%20to%20reason%20about%20and%20generate%0Astructured%203D%20scenes%20remains%20largely%20underexplored.%20This%20limitation%20constrains%0Atheir%20utility%20in%20spatially%20grounded%20tasks%20such%20as%20embodied%20AI%2C%20immersive%0Asimulations%2C%20and%20interactive%203D%20applications.%20We%20introduce%20a%20new%20paradigm%20that%0Aenables%20VLMs%20to%20generate%2C%20understand%2C%20and%20edit%20complex%203D%20environments%20by%0Ainjecting%20a%20continually%20evolving%20spatial%20context.%20Constructed%20from%20multimodal%0Ainput%2C%20this%20context%20consists%20of%20three%20components%3A%20a%20scene%20portrait%20that%0Aprovides%20a%20high-level%20semantic%20blueprint%2C%20a%20semantically%20labeled%20point%20cloud%0Acapturing%20object-level%20geometry%2C%20and%20a%20scene%20hypergraph%20that%20encodes%20rich%0Aspatial%20relationships%2C%20including%20unary%2C%20binary%2C%20and%20higher-order%20constraints.%0ATogether%2C%20these%20components%20provide%20the%20VLM%20with%20a%20structured%2C%20geometry-aware%0Aworking%20memory%20that%20integrates%20its%20inherent%20multimodal%20reasoning%20capabilities%0Awith%20structured%203D%20understanding%20for%20effective%20spatial%20reasoning.%20Building%20on%0Athis%20foundation%2C%20we%20develop%20an%20agentic%203D%20scene%20generation%20pipeline%20in%20which%0Athe%20VLM%20iteratively%20reads%20from%20and%20updates%20the%20spatial%20context.%20The%20pipeline%0Afeatures%20high-quality%20asset%20generation%20with%20geometric%20restoration%2C%20environment%0Asetup%20with%20automatic%20verification%2C%20and%20ergonomic%20adjustment%20guided%20by%20the%20scene%0Ahypergraph.%20Experiments%20show%20that%20our%20framework%20can%20handle%20diverse%20and%0Achallenging%20inputs%2C%20achieving%20a%20level%20of%20generalization%20not%20observed%20in%20prior%0Awork.%20Further%20results%20demonstrate%20that%20injecting%20spatial%20context%20enables%20VLMs%0Ato%20perform%20downstream%20tasks%20such%20as%20interactive%20scene%20editing%20and%20path%0Aplanning%2C%20suggesting%20strong%20potential%20for%20spatially%20intelligent%20systems%20in%0Acomputer%20graphics%2C%203D%20vision%2C%20and%20embodied%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentic%25203D%2520Scene%2520Generation%2520with%2520Spatially%2520Contextualized%2520VLMs%26entry.906535625%3DXinhang%2520Liu%2520and%2520Yu-Wing%2520Tai%2520and%2520Chi-Keung%2520Tang%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advances%2520in%2520multimodal%2520content%2520generation%2520enabled%2520by%250Avision-language%2520models%2520%2528VLMs%2529%252C%2520their%2520ability%2520to%2520reason%2520about%2520and%2520generate%250Astructured%25203D%2520scenes%2520remains%2520largely%2520underexplored.%2520This%2520limitation%2520constrains%250Atheir%2520utility%2520in%2520spatially%2520grounded%2520tasks%2520such%2520as%2520embodied%2520AI%252C%2520immersive%250Asimulations%252C%2520and%2520interactive%25203D%2520applications.%2520We%2520introduce%2520a%2520new%2520paradigm%2520that%250Aenables%2520VLMs%2520to%2520generate%252C%2520understand%252C%2520and%2520edit%2520complex%25203D%2520environments%2520by%250Ainjecting%2520a%2520continually%2520evolving%2520spatial%2520context.%2520Constructed%2520from%2520multimodal%250Ainput%252C%2520this%2520context%2520consists%2520of%2520three%2520components%253A%2520a%2520scene%2520portrait%2520that%250Aprovides%2520a%2520high-level%2520semantic%2520blueprint%252C%2520a%2520semantically%2520labeled%2520point%2520cloud%250Acapturing%2520object-level%2520geometry%252C%2520and%2520a%2520scene%2520hypergraph%2520that%2520encodes%2520rich%250Aspatial%2520relationships%252C%2520including%2520unary%252C%2520binary%252C%2520and%2520higher-order%2520constraints.%250ATogether%252C%2520these%2520components%2520provide%2520the%2520VLM%2520with%2520a%2520structured%252C%2520geometry-aware%250Aworking%2520memory%2520that%2520integrates%2520its%2520inherent%2520multimodal%2520reasoning%2520capabilities%250Awith%2520structured%25203D%2520understanding%2520for%2520effective%2520spatial%2520reasoning.%2520Building%2520on%250Athis%2520foundation%252C%2520we%2520develop%2520an%2520agentic%25203D%2520scene%2520generation%2520pipeline%2520in%2520which%250Athe%2520VLM%2520iteratively%2520reads%2520from%2520and%2520updates%2520the%2520spatial%2520context.%2520The%2520pipeline%250Afeatures%2520high-quality%2520asset%2520generation%2520with%2520geometric%2520restoration%252C%2520environment%250Asetup%2520with%2520automatic%2520verification%252C%2520and%2520ergonomic%2520adjustment%2520guided%2520by%2520the%2520scene%250Ahypergraph.%2520Experiments%2520show%2520that%2520our%2520framework%2520can%2520handle%2520diverse%2520and%250Achallenging%2520inputs%252C%2520achieving%2520a%2520level%2520of%2520generalization%2520not%2520observed%2520in%2520prior%250Awork.%2520Further%2520results%2520demonstrate%2520that%2520injecting%2520spatial%2520context%2520enables%2520VLMs%250Ato%2520perform%2520downstream%2520tasks%2520such%2520as%2520interactive%2520scene%2520editing%2520and%2520path%250Aplanning%252C%2520suggesting%2520strong%2520potential%2520for%2520spatially%2520intelligent%2520systems%2520in%250Acomputer%2520graphics%252C%25203D%2520vision%252C%2520and%2520embodied%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentic%203D%20Scene%20Generation%20with%20Spatially%20Contextualized%20VLMs&entry.906535625=Xinhang%20Liu%20and%20Yu-Wing%20Tai%20and%20Chi-Keung%20Tang&entry.1292438233=%20%20Despite%20recent%20advances%20in%20multimodal%20content%20generation%20enabled%20by%0Avision-language%20models%20%28VLMs%29%2C%20their%20ability%20to%20reason%20about%20and%20generate%0Astructured%203D%20scenes%20remains%20largely%20underexplored.%20This%20limitation%20constrains%0Atheir%20utility%20in%20spatially%20grounded%20tasks%20such%20as%20embodied%20AI%2C%20immersive%0Asimulations%2C%20and%20interactive%203D%20applications.%20We%20introduce%20a%20new%20paradigm%20that%0Aenables%20VLMs%20to%20generate%2C%20understand%2C%20and%20edit%20complex%203D%20environments%20by%0Ainjecting%20a%20continually%20evolving%20spatial%20context.%20Constructed%20from%20multimodal%0Ainput%2C%20this%20context%20consists%20of%20three%20components%3A%20a%20scene%20portrait%20that%0Aprovides%20a%20high-level%20semantic%20blueprint%2C%20a%20semantically%20labeled%20point%20cloud%0Acapturing%20object-level%20geometry%2C%20and%20a%20scene%20hypergraph%20that%20encodes%20rich%0Aspatial%20relationships%2C%20including%20unary%2C%20binary%2C%20and%20higher-order%20constraints.%0ATogether%2C%20these%20components%20provide%20the%20VLM%20with%20a%20structured%2C%20geometry-aware%0Aworking%20memory%20that%20integrates%20its%20inherent%20multimodal%20reasoning%20capabilities%0Awith%20structured%203D%20understanding%20for%20effective%20spatial%20reasoning.%20Building%20on%0Athis%20foundation%2C%20we%20develop%20an%20agentic%203D%20scene%20generation%20pipeline%20in%20which%0Athe%20VLM%20iteratively%20reads%20from%20and%20updates%20the%20spatial%20context.%20The%20pipeline%0Afeatures%20high-quality%20asset%20generation%20with%20geometric%20restoration%2C%20environment%0Asetup%20with%20automatic%20verification%2C%20and%20ergonomic%20adjustment%20guided%20by%20the%20scene%0Ahypergraph.%20Experiments%20show%20that%20our%20framework%20can%20handle%20diverse%20and%0Achallenging%20inputs%2C%20achieving%20a%20level%20of%20generalization%20not%20observed%20in%20prior%0Awork.%20Further%20results%20demonstrate%20that%20injecting%20spatial%20context%20enables%20VLMs%0Ato%20perform%20downstream%20tasks%20such%20as%20interactive%20scene%20editing%20and%20path%0Aplanning%2C%20suggesting%20strong%20potential%20for%20spatially%20intelligent%20systems%20in%0Acomputer%20graphics%2C%203D%20vision%2C%20and%20embodied%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20129v1&entry.124074799=Read"},
{"title": "SeMe: Training-Free Language Model Merging via Semantic Alignment", "author": "Jian Gu and Aldeida Aleti and Chunyang Chen and Hongyu Zhang", "abstract": "  Despite the remarkable capabilities of Language Models (LMs) across diverse\ntasks, no single model consistently outperforms others, necessitating efficient\nmethods to combine their strengths without expensive retraining. Existing model\nmerging techniques, such as parameter averaging and task-guided fusion, often\nrely on data-dependent computations or fail to preserve internal knowledge,\nlimiting their robustness and scalability. We introduce SeMe (Semantic-based\nMerging), a novel, data-free, and training-free approach that leverages latent\nsemantic alignment to merge LMs at a fine-grained, layer-wise level. Unlike\nprior work, SeMe not only preserves model behaviors but also explicitly\nstabilizes internal knowledge, addressing a critical gap in LM fusion. Through\nextensive experiments across diverse architectures and tasks, we demonstrate\nthat SeMe outperforms existing methods in both performance and efficiency while\neliminating reliance on external data. Our work establishes a new paradigm for\nknowledge-aware model merging and provides insights into the semantic structure\nof LMs, paving the way for more scalable and interpretable model composition.\n", "link": "http://arxiv.org/abs/2505.20144v1", "date": "2025-05-26", "relevancy": 2.554, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5183}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5071}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SeMe%3A%20Training-Free%20Language%20Model%20Merging%20via%20Semantic%20Alignment&body=Title%3A%20SeMe%3A%20Training-Free%20Language%20Model%20Merging%20via%20Semantic%20Alignment%0AAuthor%3A%20Jian%20Gu%20and%20Aldeida%20Aleti%20and%20Chunyang%20Chen%20and%20Hongyu%20Zhang%0AAbstract%3A%20%20%20Despite%20the%20remarkable%20capabilities%20of%20Language%20Models%20%28LMs%29%20across%20diverse%0Atasks%2C%20no%20single%20model%20consistently%20outperforms%20others%2C%20necessitating%20efficient%0Amethods%20to%20combine%20their%20strengths%20without%20expensive%20retraining.%20Existing%20model%0Amerging%20techniques%2C%20such%20as%20parameter%20averaging%20and%20task-guided%20fusion%2C%20often%0Arely%20on%20data-dependent%20computations%20or%20fail%20to%20preserve%20internal%20knowledge%2C%0Alimiting%20their%20robustness%20and%20scalability.%20We%20introduce%20SeMe%20%28Semantic-based%0AMerging%29%2C%20a%20novel%2C%20data-free%2C%20and%20training-free%20approach%20that%20leverages%20latent%0Asemantic%20alignment%20to%20merge%20LMs%20at%20a%20fine-grained%2C%20layer-wise%20level.%20Unlike%0Aprior%20work%2C%20SeMe%20not%20only%20preserves%20model%20behaviors%20but%20also%20explicitly%0Astabilizes%20internal%20knowledge%2C%20addressing%20a%20critical%20gap%20in%20LM%20fusion.%20Through%0Aextensive%20experiments%20across%20diverse%20architectures%20and%20tasks%2C%20we%20demonstrate%0Athat%20SeMe%20outperforms%20existing%20methods%20in%20both%20performance%20and%20efficiency%20while%0Aeliminating%20reliance%20on%20external%20data.%20Our%20work%20establishes%20a%20new%20paradigm%20for%0Aknowledge-aware%20model%20merging%20and%20provides%20insights%20into%20the%20semantic%20structure%0Aof%20LMs%2C%20paving%20the%20way%20for%20more%20scalable%20and%20interpretable%20model%20composition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20144v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeMe%253A%2520Training-Free%2520Language%2520Model%2520Merging%2520via%2520Semantic%2520Alignment%26entry.906535625%3DJian%2520Gu%2520and%2520Aldeida%2520Aleti%2520and%2520Chunyang%2520Chen%2520and%2520Hongyu%2520Zhang%26entry.1292438233%3D%2520%2520Despite%2520the%2520remarkable%2520capabilities%2520of%2520Language%2520Models%2520%2528LMs%2529%2520across%2520diverse%250Atasks%252C%2520no%2520single%2520model%2520consistently%2520outperforms%2520others%252C%2520necessitating%2520efficient%250Amethods%2520to%2520combine%2520their%2520strengths%2520without%2520expensive%2520retraining.%2520Existing%2520model%250Amerging%2520techniques%252C%2520such%2520as%2520parameter%2520averaging%2520and%2520task-guided%2520fusion%252C%2520often%250Arely%2520on%2520data-dependent%2520computations%2520or%2520fail%2520to%2520preserve%2520internal%2520knowledge%252C%250Alimiting%2520their%2520robustness%2520and%2520scalability.%2520We%2520introduce%2520SeMe%2520%2528Semantic-based%250AMerging%2529%252C%2520a%2520novel%252C%2520data-free%252C%2520and%2520training-free%2520approach%2520that%2520leverages%2520latent%250Asemantic%2520alignment%2520to%2520merge%2520LMs%2520at%2520a%2520fine-grained%252C%2520layer-wise%2520level.%2520Unlike%250Aprior%2520work%252C%2520SeMe%2520not%2520only%2520preserves%2520model%2520behaviors%2520but%2520also%2520explicitly%250Astabilizes%2520internal%2520knowledge%252C%2520addressing%2520a%2520critical%2520gap%2520in%2520LM%2520fusion.%2520Through%250Aextensive%2520experiments%2520across%2520diverse%2520architectures%2520and%2520tasks%252C%2520we%2520demonstrate%250Athat%2520SeMe%2520outperforms%2520existing%2520methods%2520in%2520both%2520performance%2520and%2520efficiency%2520while%250Aeliminating%2520reliance%2520on%2520external%2520data.%2520Our%2520work%2520establishes%2520a%2520new%2520paradigm%2520for%250Aknowledge-aware%2520model%2520merging%2520and%2520provides%2520insights%2520into%2520the%2520semantic%2520structure%250Aof%2520LMs%252C%2520paving%2520the%2520way%2520for%2520more%2520scalable%2520and%2520interpretable%2520model%2520composition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20144v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeMe%3A%20Training-Free%20Language%20Model%20Merging%20via%20Semantic%20Alignment&entry.906535625=Jian%20Gu%20and%20Aldeida%20Aleti%20and%20Chunyang%20Chen%20and%20Hongyu%20Zhang&entry.1292438233=%20%20Despite%20the%20remarkable%20capabilities%20of%20Language%20Models%20%28LMs%29%20across%20diverse%0Atasks%2C%20no%20single%20model%20consistently%20outperforms%20others%2C%20necessitating%20efficient%0Amethods%20to%20combine%20their%20strengths%20without%20expensive%20retraining.%20Existing%20model%0Amerging%20techniques%2C%20such%20as%20parameter%20averaging%20and%20task-guided%20fusion%2C%20often%0Arely%20on%20data-dependent%20computations%20or%20fail%20to%20preserve%20internal%20knowledge%2C%0Alimiting%20their%20robustness%20and%20scalability.%20We%20introduce%20SeMe%20%28Semantic-based%0AMerging%29%2C%20a%20novel%2C%20data-free%2C%20and%20training-free%20approach%20that%20leverages%20latent%0Asemantic%20alignment%20to%20merge%20LMs%20at%20a%20fine-grained%2C%20layer-wise%20level.%20Unlike%0Aprior%20work%2C%20SeMe%20not%20only%20preserves%20model%20behaviors%20but%20also%20explicitly%0Astabilizes%20internal%20knowledge%2C%20addressing%20a%20critical%20gap%20in%20LM%20fusion.%20Through%0Aextensive%20experiments%20across%20diverse%20architectures%20and%20tasks%2C%20we%20demonstrate%0Athat%20SeMe%20outperforms%20existing%20methods%20in%20both%20performance%20and%20efficiency%20while%0Aeliminating%20reliance%20on%20external%20data.%20Our%20work%20establishes%20a%20new%20paradigm%20for%0Aknowledge-aware%20model%20merging%20and%20provides%20insights%20into%20the%20semantic%20structure%0Aof%20LMs%2C%20paving%20the%20way%20for%20more%20scalable%20and%20interpretable%20model%20composition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20144v1&entry.124074799=Read"},
{"title": "StructEval: Benchmarking LLMs' Capabilities to Generate Structural\n  Outputs", "author": "Jialin Yang and Dongfu Jiang and Lipeng He and Sherman Siu and Yuxuan Zhang and Disen Liao and Zhuofeng Li and Huaye Zeng and Yiming Jia and Haozhe Wang and Benjamin Schneider and Chi Ruan and Wentao Ma and Zhiheng Lyu and Yifei Wang and Yi Lu and Quy Duc Do and Ziyan Jiang and Ping Nie and Wenhu Chen", "abstract": "  As Large Language Models (LLMs) become integral to software development\nworkflows, their ability to generate structured outputs has become critically\nimportant. We introduce StructEval, a comprehensive benchmark for evaluating\nLLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and\nrenderable (HTML, React, SVG) structured formats. Unlike prior benchmarks,\nStructEval systematically evaluates structural fidelity across diverse formats\nthrough two paradigms: 1) generation tasks, producing structured output from\nnatural language prompts, and 2) conversion tasks, translating between\nstructured formats. Our benchmark encompasses 18 formats and 44 types of task,\nwith novel metrics for format adherence and structural correctness. Results\nreveal significant performance gaps, even state-of-the-art models like o1-mini\nachieve only 75.58 average score, with open-source alternatives lagging\napproximately 10 points behind. We find generation tasks more challenging than\nconversion tasks, and producing correct visual content more difficult than\ngenerating text-only structures.\n", "link": "http://arxiv.org/abs/2505.20139v1", "date": "2025-05-26", "relevancy": 2.5538, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5288}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5288}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StructEval%3A%20Benchmarking%20LLMs%27%20Capabilities%20to%20Generate%20Structural%0A%20%20Outputs&body=Title%3A%20StructEval%3A%20Benchmarking%20LLMs%27%20Capabilities%20to%20Generate%20Structural%0A%20%20Outputs%0AAuthor%3A%20Jialin%20Yang%20and%20Dongfu%20Jiang%20and%20Lipeng%20He%20and%20Sherman%20Siu%20and%20Yuxuan%20Zhang%20and%20Disen%20Liao%20and%20Zhuofeng%20Li%20and%20Huaye%20Zeng%20and%20Yiming%20Jia%20and%20Haozhe%20Wang%20and%20Benjamin%20Schneider%20and%20Chi%20Ruan%20and%20Wentao%20Ma%20and%20Zhiheng%20Lyu%20and%20Yifei%20Wang%20and%20Yi%20Lu%20and%20Quy%20Duc%20Do%20and%20Ziyan%20Jiang%20and%20Ping%20Nie%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20As%20Large%20Language%20Models%20%28LLMs%29%20become%20integral%20to%20software%20development%0Aworkflows%2C%20their%20ability%20to%20generate%20structured%20outputs%20has%20become%20critically%0Aimportant.%20We%20introduce%20StructEval%2C%20a%20comprehensive%20benchmark%20for%20evaluating%0ALLMs%27%20capabilities%20in%20producing%20both%20non-renderable%20%28JSON%2C%20YAML%2C%20CSV%29%20and%0Arenderable%20%28HTML%2C%20React%2C%20SVG%29%20structured%20formats.%20Unlike%20prior%20benchmarks%2C%0AStructEval%20systematically%20evaluates%20structural%20fidelity%20across%20diverse%20formats%0Athrough%20two%20paradigms%3A%201%29%20generation%20tasks%2C%20producing%20structured%20output%20from%0Anatural%20language%20prompts%2C%20and%202%29%20conversion%20tasks%2C%20translating%20between%0Astructured%20formats.%20Our%20benchmark%20encompasses%2018%20formats%20and%2044%20types%20of%20task%2C%0Awith%20novel%20metrics%20for%20format%20adherence%20and%20structural%20correctness.%20Results%0Areveal%20significant%20performance%20gaps%2C%20even%20state-of-the-art%20models%20like%20o1-mini%0Aachieve%20only%2075.58%20average%20score%2C%20with%20open-source%20alternatives%20lagging%0Aapproximately%2010%20points%20behind.%20We%20find%20generation%20tasks%20more%20challenging%20than%0Aconversion%20tasks%2C%20and%20producing%20correct%20visual%20content%20more%20difficult%20than%0Agenerating%20text-only%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructEval%253A%2520Benchmarking%2520LLMs%2527%2520Capabilities%2520to%2520Generate%2520Structural%250A%2520%2520Outputs%26entry.906535625%3DJialin%2520Yang%2520and%2520Dongfu%2520Jiang%2520and%2520Lipeng%2520He%2520and%2520Sherman%2520Siu%2520and%2520Yuxuan%2520Zhang%2520and%2520Disen%2520Liao%2520and%2520Zhuofeng%2520Li%2520and%2520Huaye%2520Zeng%2520and%2520Yiming%2520Jia%2520and%2520Haozhe%2520Wang%2520and%2520Benjamin%2520Schneider%2520and%2520Chi%2520Ruan%2520and%2520Wentao%2520Ma%2520and%2520Zhiheng%2520Lyu%2520and%2520Yifei%2520Wang%2520and%2520Yi%2520Lu%2520and%2520Quy%2520Duc%2520Do%2520and%2520Ziyan%2520Jiang%2520and%2520Ping%2520Nie%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520As%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520become%2520integral%2520to%2520software%2520development%250Aworkflows%252C%2520their%2520ability%2520to%2520generate%2520structured%2520outputs%2520has%2520become%2520critically%250Aimportant.%2520We%2520introduce%2520StructEval%252C%2520a%2520comprehensive%2520benchmark%2520for%2520evaluating%250ALLMs%2527%2520capabilities%2520in%2520producing%2520both%2520non-renderable%2520%2528JSON%252C%2520YAML%252C%2520CSV%2529%2520and%250Arenderable%2520%2528HTML%252C%2520React%252C%2520SVG%2529%2520structured%2520formats.%2520Unlike%2520prior%2520benchmarks%252C%250AStructEval%2520systematically%2520evaluates%2520structural%2520fidelity%2520across%2520diverse%2520formats%250Athrough%2520two%2520paradigms%253A%25201%2529%2520generation%2520tasks%252C%2520producing%2520structured%2520output%2520from%250Anatural%2520language%2520prompts%252C%2520and%25202%2529%2520conversion%2520tasks%252C%2520translating%2520between%250Astructured%2520formats.%2520Our%2520benchmark%2520encompasses%252018%2520formats%2520and%252044%2520types%2520of%2520task%252C%250Awith%2520novel%2520metrics%2520for%2520format%2520adherence%2520and%2520structural%2520correctness.%2520Results%250Areveal%2520significant%2520performance%2520gaps%252C%2520even%2520state-of-the-art%2520models%2520like%2520o1-mini%250Aachieve%2520only%252075.58%2520average%2520score%252C%2520with%2520open-source%2520alternatives%2520lagging%250Aapproximately%252010%2520points%2520behind.%2520We%2520find%2520generation%2520tasks%2520more%2520challenging%2520than%250Aconversion%2520tasks%252C%2520and%2520producing%2520correct%2520visual%2520content%2520more%2520difficult%2520than%250Agenerating%2520text-only%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StructEval%3A%20Benchmarking%20LLMs%27%20Capabilities%20to%20Generate%20Structural%0A%20%20Outputs&entry.906535625=Jialin%20Yang%20and%20Dongfu%20Jiang%20and%20Lipeng%20He%20and%20Sherman%20Siu%20and%20Yuxuan%20Zhang%20and%20Disen%20Liao%20and%20Zhuofeng%20Li%20and%20Huaye%20Zeng%20and%20Yiming%20Jia%20and%20Haozhe%20Wang%20and%20Benjamin%20Schneider%20and%20Chi%20Ruan%20and%20Wentao%20Ma%20and%20Zhiheng%20Lyu%20and%20Yifei%20Wang%20and%20Yi%20Lu%20and%20Quy%20Duc%20Do%20and%20Ziyan%20Jiang%20and%20Ping%20Nie%20and%20Wenhu%20Chen&entry.1292438233=%20%20As%20Large%20Language%20Models%20%28LLMs%29%20become%20integral%20to%20software%20development%0Aworkflows%2C%20their%20ability%20to%20generate%20structured%20outputs%20has%20become%20critically%0Aimportant.%20We%20introduce%20StructEval%2C%20a%20comprehensive%20benchmark%20for%20evaluating%0ALLMs%27%20capabilities%20in%20producing%20both%20non-renderable%20%28JSON%2C%20YAML%2C%20CSV%29%20and%0Arenderable%20%28HTML%2C%20React%2C%20SVG%29%20structured%20formats.%20Unlike%20prior%20benchmarks%2C%0AStructEval%20systematically%20evaluates%20structural%20fidelity%20across%20diverse%20formats%0Athrough%20two%20paradigms%3A%201%29%20generation%20tasks%2C%20producing%20structured%20output%20from%0Anatural%20language%20prompts%2C%20and%202%29%20conversion%20tasks%2C%20translating%20between%0Astructured%20formats.%20Our%20benchmark%20encompasses%2018%20formats%20and%2044%20types%20of%20task%2C%0Awith%20novel%20metrics%20for%20format%20adherence%20and%20structural%20correctness.%20Results%0Areveal%20significant%20performance%20gaps%2C%20even%20state-of-the-art%20models%20like%20o1-mini%0Aachieve%20only%2075.58%20average%20score%2C%20with%20open-source%20alternatives%20lagging%0Aapproximately%2010%20points%20behind.%20We%20find%20generation%20tasks%20more%20challenging%20than%0Aconversion%20tasks%2C%20and%20producing%20correct%20visual%20content%20more%20difficult%20than%0Agenerating%20text-only%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20139v1&entry.124074799=Read"},
{"title": "Towards Video to Piano Music Generation with Chain-of-Perform Support\n  Benchmarks", "author": "Chang Liu and Haomin Zhang and Shiyu Xia and Zihao Chen and Chaofan Ding and Xin Yue and Huizhe Chen and Xinhan Di", "abstract": "  Generating high-quality piano audio from video requires precise\nsynchronization between visual cues and musical output, ensuring accurate\nsemantic and temporal alignment.However, existing evaluation datasets do not\nfully capture the intricate synchronization required for piano music\ngeneration. A comprehensive benchmark is essential for two primary reasons: (1)\nexisting metrics fail to reflect the complexity of video-to-piano music\ninteractions, and (2) a dedicated benchmark dataset can provide valuable\ninsights to accelerate progress in high-quality piano music generation. To\naddress these challenges, we introduce the CoP Benchmark Dataset-a fully\nopen-sourced, multimodal benchmark designed specifically for video-guided piano\nmusic generation. The proposed Chain-of-Perform (CoP) benchmark offers several\ncompelling features: (1) detailed multimodal annotations, enabling precise\nsemantic and temporal alignment between video content and piano audio via\nstep-by-step Chain-of-Perform guidance; (2) a versatile evaluation framework\nfor rigorous assessment of both general-purpose and specialized video-to-piano\ngeneration tasks; and (3) full open-sourcing of the dataset, annotations, and\nevaluation protocols. The dataset is publicly available at\nhttps://github.com/acappemin/Video-to-Audio-and-Piano, with a continuously\nupdated leaderboard to promote ongoing research in this domain.\n", "link": "http://arxiv.org/abs/2505.20038v1", "date": "2025-05-26", "relevancy": 2.4949, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.512}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.493}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Video%20to%20Piano%20Music%20Generation%20with%20Chain-of-Perform%20Support%0A%20%20Benchmarks&body=Title%3A%20Towards%20Video%20to%20Piano%20Music%20Generation%20with%20Chain-of-Perform%20Support%0A%20%20Benchmarks%0AAuthor%3A%20Chang%20Liu%20and%20Haomin%20Zhang%20and%20Shiyu%20Xia%20and%20Zihao%20Chen%20and%20Chaofan%20Ding%20and%20Xin%20Yue%20and%20Huizhe%20Chen%20and%20Xinhan%20Di%0AAbstract%3A%20%20%20Generating%20high-quality%20piano%20audio%20from%20video%20requires%20precise%0Asynchronization%20between%20visual%20cues%20and%20musical%20output%2C%20ensuring%20accurate%0Asemantic%20and%20temporal%20alignment.However%2C%20existing%20evaluation%20datasets%20do%20not%0Afully%20capture%20the%20intricate%20synchronization%20required%20for%20piano%20music%0Ageneration.%20A%20comprehensive%20benchmark%20is%20essential%20for%20two%20primary%20reasons%3A%20%281%29%0Aexisting%20metrics%20fail%20to%20reflect%20the%20complexity%20of%20video-to-piano%20music%0Ainteractions%2C%20and%20%282%29%20a%20dedicated%20benchmark%20dataset%20can%20provide%20valuable%0Ainsights%20to%20accelerate%20progress%20in%20high-quality%20piano%20music%20generation.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20the%20CoP%20Benchmark%20Dataset-a%20fully%0Aopen-sourced%2C%20multimodal%20benchmark%20designed%20specifically%20for%20video-guided%20piano%0Amusic%20generation.%20The%20proposed%20Chain-of-Perform%20%28CoP%29%20benchmark%20offers%20several%0Acompelling%20features%3A%20%281%29%20detailed%20multimodal%20annotations%2C%20enabling%20precise%0Asemantic%20and%20temporal%20alignment%20between%20video%20content%20and%20piano%20audio%20via%0Astep-by-step%20Chain-of-Perform%20guidance%3B%20%282%29%20a%20versatile%20evaluation%20framework%0Afor%20rigorous%20assessment%20of%20both%20general-purpose%20and%20specialized%20video-to-piano%0Ageneration%20tasks%3B%20and%20%283%29%20full%20open-sourcing%20of%20the%20dataset%2C%20annotations%2C%20and%0Aevaluation%20protocols.%20The%20dataset%20is%20publicly%20available%20at%0Ahttps%3A//github.com/acappemin/Video-to-Audio-and-Piano%2C%20with%20a%20continuously%0Aupdated%20leaderboard%20to%20promote%20ongoing%20research%20in%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Video%2520to%2520Piano%2520Music%2520Generation%2520with%2520Chain-of-Perform%2520Support%250A%2520%2520Benchmarks%26entry.906535625%3DChang%2520Liu%2520and%2520Haomin%2520Zhang%2520and%2520Shiyu%2520Xia%2520and%2520Zihao%2520Chen%2520and%2520Chaofan%2520Ding%2520and%2520Xin%2520Yue%2520and%2520Huizhe%2520Chen%2520and%2520Xinhan%2520Di%26entry.1292438233%3D%2520%2520Generating%2520high-quality%2520piano%2520audio%2520from%2520video%2520requires%2520precise%250Asynchronization%2520between%2520visual%2520cues%2520and%2520musical%2520output%252C%2520ensuring%2520accurate%250Asemantic%2520and%2520temporal%2520alignment.However%252C%2520existing%2520evaluation%2520datasets%2520do%2520not%250Afully%2520capture%2520the%2520intricate%2520synchronization%2520required%2520for%2520piano%2520music%250Ageneration.%2520A%2520comprehensive%2520benchmark%2520is%2520essential%2520for%2520two%2520primary%2520reasons%253A%2520%25281%2529%250Aexisting%2520metrics%2520fail%2520to%2520reflect%2520the%2520complexity%2520of%2520video-to-piano%2520music%250Ainteractions%252C%2520and%2520%25282%2529%2520a%2520dedicated%2520benchmark%2520dataset%2520can%2520provide%2520valuable%250Ainsights%2520to%2520accelerate%2520progress%2520in%2520high-quality%2520piano%2520music%2520generation.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520the%2520CoP%2520Benchmark%2520Dataset-a%2520fully%250Aopen-sourced%252C%2520multimodal%2520benchmark%2520designed%2520specifically%2520for%2520video-guided%2520piano%250Amusic%2520generation.%2520The%2520proposed%2520Chain-of-Perform%2520%2528CoP%2529%2520benchmark%2520offers%2520several%250Acompelling%2520features%253A%2520%25281%2529%2520detailed%2520multimodal%2520annotations%252C%2520enabling%2520precise%250Asemantic%2520and%2520temporal%2520alignment%2520between%2520video%2520content%2520and%2520piano%2520audio%2520via%250Astep-by-step%2520Chain-of-Perform%2520guidance%253B%2520%25282%2529%2520a%2520versatile%2520evaluation%2520framework%250Afor%2520rigorous%2520assessment%2520of%2520both%2520general-purpose%2520and%2520specialized%2520video-to-piano%250Ageneration%2520tasks%253B%2520and%2520%25283%2529%2520full%2520open-sourcing%2520of%2520the%2520dataset%252C%2520annotations%252C%2520and%250Aevaluation%2520protocols.%2520The%2520dataset%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/acappemin/Video-to-Audio-and-Piano%252C%2520with%2520a%2520continuously%250Aupdated%2520leaderboard%2520to%2520promote%2520ongoing%2520research%2520in%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Video%20to%20Piano%20Music%20Generation%20with%20Chain-of-Perform%20Support%0A%20%20Benchmarks&entry.906535625=Chang%20Liu%20and%20Haomin%20Zhang%20and%20Shiyu%20Xia%20and%20Zihao%20Chen%20and%20Chaofan%20Ding%20and%20Xin%20Yue%20and%20Huizhe%20Chen%20and%20Xinhan%20Di&entry.1292438233=%20%20Generating%20high-quality%20piano%20audio%20from%20video%20requires%20precise%0Asynchronization%20between%20visual%20cues%20and%20musical%20output%2C%20ensuring%20accurate%0Asemantic%20and%20temporal%20alignment.However%2C%20existing%20evaluation%20datasets%20do%20not%0Afully%20capture%20the%20intricate%20synchronization%20required%20for%20piano%20music%0Ageneration.%20A%20comprehensive%20benchmark%20is%20essential%20for%20two%20primary%20reasons%3A%20%281%29%0Aexisting%20metrics%20fail%20to%20reflect%20the%20complexity%20of%20video-to-piano%20music%0Ainteractions%2C%20and%20%282%29%20a%20dedicated%20benchmark%20dataset%20can%20provide%20valuable%0Ainsights%20to%20accelerate%20progress%20in%20high-quality%20piano%20music%20generation.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20the%20CoP%20Benchmark%20Dataset-a%20fully%0Aopen-sourced%2C%20multimodal%20benchmark%20designed%20specifically%20for%20video-guided%20piano%0Amusic%20generation.%20The%20proposed%20Chain-of-Perform%20%28CoP%29%20benchmark%20offers%20several%0Acompelling%20features%3A%20%281%29%20detailed%20multimodal%20annotations%2C%20enabling%20precise%0Asemantic%20and%20temporal%20alignment%20between%20video%20content%20and%20piano%20audio%20via%0Astep-by-step%20Chain-of-Perform%20guidance%3B%20%282%29%20a%20versatile%20evaluation%20framework%0Afor%20rigorous%20assessment%20of%20both%20general-purpose%20and%20specialized%20video-to-piano%0Ageneration%20tasks%3B%20and%20%283%29%20full%20open-sourcing%20of%20the%20dataset%2C%20annotations%2C%20and%0Aevaluation%20protocols.%20The%20dataset%20is%20publicly%20available%20at%0Ahttps%3A//github.com/acappemin/Video-to-Audio-and-Piano%2C%20with%20a%20continuously%0Aupdated%20leaderboard%20to%20promote%20ongoing%20research%20in%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20038v1&entry.124074799=Read"},
{"title": "Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured\n  Multi-Turn Decomposition", "author": "Zihao Zeng and Xuyao Huang and Boxiu Li and Hao Zhang and Zhijie Deng", "abstract": "  Large Reasoning Models (LRMs) are criticized for the excessively lengthy\nChain-of-Thought (CoT) to derive the final answer, suffering from high\nfirst-token and overall latency. Typically, the CoT of LRMs mixes multiple\nthinking units; each unit attempts to produce a candidate answer to the\noriginal query. Hence, a natural idea to improve efficiency is to reduce the\nunit number. Yet, the fact that the thinking units in vanilla CoT cannot be\nexplicitly managed renders doing so challenging. This paper introduces\nMulti-Turn Decomposition (MinD) to decode conventional CoT into a sequence of\nexplicit, structured, and turn-wise interactions to bridge the gap. In MinD,\nthe model provides a multi-turn response to the query, where each turn embraces\na thinking unit and yields a corresponding answer. The subsequent turns can\nreflect, verify, revise, or explore alternative approaches to both the thinking\nand answer parts of earlier ones. This not only makes the answer delivered more\nswiftly, but also enables explicit controls over the iterative reasoning\nprocess (i.e., users may halt or continue at any turn). We follow a supervised\nfine-tuning (SFT) then reinforcement learning (RL) paradigm to realize MinD. We\nfirst rephrase the outputs of an LRM into multi-turn formats by prompting\nanother LLM, and then tune the LRM with such data. Observing that the tuned\nmodel tends to consume even more tokens than the original one (probably due to\nthat the multi-turn formats introduce additional answer tokens), we advocate\nleveraging RL algorithms like GRPO to prioritize correct outputs with fewer\nturns. Trained on the MATH dataset using R1-Distill models, MinD can achieve up\nto ~70% reduction in both output token usage and time to first token (TTFT),\nwhile maintaining competitive performance on reasoning benchmarks such as\nMATH-500, AIME24, AMC23, and GPQA-Diamond.\n", "link": "http://arxiv.org/abs/2505.19788v1", "date": "2025-05-26", "relevancy": 2.4942, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4954}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Done%20Is%20Better%20than%20Perfect%3A%20Unlocking%20Efficient%20Reasoning%20by%20Structured%0A%20%20Multi-Turn%20Decomposition&body=Title%3A%20Done%20Is%20Better%20than%20Perfect%3A%20Unlocking%20Efficient%20Reasoning%20by%20Structured%0A%20%20Multi-Turn%20Decomposition%0AAuthor%3A%20Zihao%20Zeng%20and%20Xuyao%20Huang%20and%20Boxiu%20Li%20and%20Hao%20Zhang%20and%20Zhijie%20Deng%0AAbstract%3A%20%20%20Large%20Reasoning%20Models%20%28LRMs%29%20are%20criticized%20for%20the%20excessively%20lengthy%0AChain-of-Thought%20%28CoT%29%20to%20derive%20the%20final%20answer%2C%20suffering%20from%20high%0Afirst-token%20and%20overall%20latency.%20Typically%2C%20the%20CoT%20of%20LRMs%20mixes%20multiple%0Athinking%20units%3B%20each%20unit%20attempts%20to%20produce%20a%20candidate%20answer%20to%20the%0Aoriginal%20query.%20Hence%2C%20a%20natural%20idea%20to%20improve%20efficiency%20is%20to%20reduce%20the%0Aunit%20number.%20Yet%2C%20the%20fact%20that%20the%20thinking%20units%20in%20vanilla%20CoT%20cannot%20be%0Aexplicitly%20managed%20renders%20doing%20so%20challenging.%20This%20paper%20introduces%0AMulti-Turn%20Decomposition%20%28MinD%29%20to%20decode%20conventional%20CoT%20into%20a%20sequence%20of%0Aexplicit%2C%20structured%2C%20and%20turn-wise%20interactions%20to%20bridge%20the%20gap.%20In%20MinD%2C%0Athe%20model%20provides%20a%20multi-turn%20response%20to%20the%20query%2C%20where%20each%20turn%20embraces%0Aa%20thinking%20unit%20and%20yields%20a%20corresponding%20answer.%20The%20subsequent%20turns%20can%0Areflect%2C%20verify%2C%20revise%2C%20or%20explore%20alternative%20approaches%20to%20both%20the%20thinking%0Aand%20answer%20parts%20of%20earlier%20ones.%20This%20not%20only%20makes%20the%20answer%20delivered%20more%0Aswiftly%2C%20but%20also%20enables%20explicit%20controls%20over%20the%20iterative%20reasoning%0Aprocess%20%28i.e.%2C%20users%20may%20halt%20or%20continue%20at%20any%20turn%29.%20We%20follow%20a%20supervised%0Afine-tuning%20%28SFT%29%20then%20reinforcement%20learning%20%28RL%29%20paradigm%20to%20realize%20MinD.%20We%0Afirst%20rephrase%20the%20outputs%20of%20an%20LRM%20into%20multi-turn%20formats%20by%20prompting%0Aanother%20LLM%2C%20and%20then%20tune%20the%20LRM%20with%20such%20data.%20Observing%20that%20the%20tuned%0Amodel%20tends%20to%20consume%20even%20more%20tokens%20than%20the%20original%20one%20%28probably%20due%20to%0Athat%20the%20multi-turn%20formats%20introduce%20additional%20answer%20tokens%29%2C%20we%20advocate%0Aleveraging%20RL%20algorithms%20like%20GRPO%20to%20prioritize%20correct%20outputs%20with%20fewer%0Aturns.%20Trained%20on%20the%20MATH%20dataset%20using%20R1-Distill%20models%2C%20MinD%20can%20achieve%20up%0Ato%20~70%25%20reduction%20in%20both%20output%20token%20usage%20and%20time%20to%20first%20token%20%28TTFT%29%2C%0Awhile%20maintaining%20competitive%20performance%20on%20reasoning%20benchmarks%20such%20as%0AMATH-500%2C%20AIME24%2C%20AMC23%2C%20and%20GPQA-Diamond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDone%2520Is%2520Better%2520than%2520Perfect%253A%2520Unlocking%2520Efficient%2520Reasoning%2520by%2520Structured%250A%2520%2520Multi-Turn%2520Decomposition%26entry.906535625%3DZihao%2520Zeng%2520and%2520Xuyao%2520Huang%2520and%2520Boxiu%2520Li%2520and%2520Hao%2520Zhang%2520and%2520Zhijie%2520Deng%26entry.1292438233%3D%2520%2520Large%2520Reasoning%2520Models%2520%2528LRMs%2529%2520are%2520criticized%2520for%2520the%2520excessively%2520lengthy%250AChain-of-Thought%2520%2528CoT%2529%2520to%2520derive%2520the%2520final%2520answer%252C%2520suffering%2520from%2520high%250Afirst-token%2520and%2520overall%2520latency.%2520Typically%252C%2520the%2520CoT%2520of%2520LRMs%2520mixes%2520multiple%250Athinking%2520units%253B%2520each%2520unit%2520attempts%2520to%2520produce%2520a%2520candidate%2520answer%2520to%2520the%250Aoriginal%2520query.%2520Hence%252C%2520a%2520natural%2520idea%2520to%2520improve%2520efficiency%2520is%2520to%2520reduce%2520the%250Aunit%2520number.%2520Yet%252C%2520the%2520fact%2520that%2520the%2520thinking%2520units%2520in%2520vanilla%2520CoT%2520cannot%2520be%250Aexplicitly%2520managed%2520renders%2520doing%2520so%2520challenging.%2520This%2520paper%2520introduces%250AMulti-Turn%2520Decomposition%2520%2528MinD%2529%2520to%2520decode%2520conventional%2520CoT%2520into%2520a%2520sequence%2520of%250Aexplicit%252C%2520structured%252C%2520and%2520turn-wise%2520interactions%2520to%2520bridge%2520the%2520gap.%2520In%2520MinD%252C%250Athe%2520model%2520provides%2520a%2520multi-turn%2520response%2520to%2520the%2520query%252C%2520where%2520each%2520turn%2520embraces%250Aa%2520thinking%2520unit%2520and%2520yields%2520a%2520corresponding%2520answer.%2520The%2520subsequent%2520turns%2520can%250Areflect%252C%2520verify%252C%2520revise%252C%2520or%2520explore%2520alternative%2520approaches%2520to%2520both%2520the%2520thinking%250Aand%2520answer%2520parts%2520of%2520earlier%2520ones.%2520This%2520not%2520only%2520makes%2520the%2520answer%2520delivered%2520more%250Aswiftly%252C%2520but%2520also%2520enables%2520explicit%2520controls%2520over%2520the%2520iterative%2520reasoning%250Aprocess%2520%2528i.e.%252C%2520users%2520may%2520halt%2520or%2520continue%2520at%2520any%2520turn%2529.%2520We%2520follow%2520a%2520supervised%250Afine-tuning%2520%2528SFT%2529%2520then%2520reinforcement%2520learning%2520%2528RL%2529%2520paradigm%2520to%2520realize%2520MinD.%2520We%250Afirst%2520rephrase%2520the%2520outputs%2520of%2520an%2520LRM%2520into%2520multi-turn%2520formats%2520by%2520prompting%250Aanother%2520LLM%252C%2520and%2520then%2520tune%2520the%2520LRM%2520with%2520such%2520data.%2520Observing%2520that%2520the%2520tuned%250Amodel%2520tends%2520to%2520consume%2520even%2520more%2520tokens%2520than%2520the%2520original%2520one%2520%2528probably%2520due%2520to%250Athat%2520the%2520multi-turn%2520formats%2520introduce%2520additional%2520answer%2520tokens%2529%252C%2520we%2520advocate%250Aleveraging%2520RL%2520algorithms%2520like%2520GRPO%2520to%2520prioritize%2520correct%2520outputs%2520with%2520fewer%250Aturns.%2520Trained%2520on%2520the%2520MATH%2520dataset%2520using%2520R1-Distill%2520models%252C%2520MinD%2520can%2520achieve%2520up%250Ato%2520~70%2525%2520reduction%2520in%2520both%2520output%2520token%2520usage%2520and%2520time%2520to%2520first%2520token%2520%2528TTFT%2529%252C%250Awhile%2520maintaining%2520competitive%2520performance%2520on%2520reasoning%2520benchmarks%2520such%2520as%250AMATH-500%252C%2520AIME24%252C%2520AMC23%252C%2520and%2520GPQA-Diamond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Done%20Is%20Better%20than%20Perfect%3A%20Unlocking%20Efficient%20Reasoning%20by%20Structured%0A%20%20Multi-Turn%20Decomposition&entry.906535625=Zihao%20Zeng%20and%20Xuyao%20Huang%20and%20Boxiu%20Li%20and%20Hao%20Zhang%20and%20Zhijie%20Deng&entry.1292438233=%20%20Large%20Reasoning%20Models%20%28LRMs%29%20are%20criticized%20for%20the%20excessively%20lengthy%0AChain-of-Thought%20%28CoT%29%20to%20derive%20the%20final%20answer%2C%20suffering%20from%20high%0Afirst-token%20and%20overall%20latency.%20Typically%2C%20the%20CoT%20of%20LRMs%20mixes%20multiple%0Athinking%20units%3B%20each%20unit%20attempts%20to%20produce%20a%20candidate%20answer%20to%20the%0Aoriginal%20query.%20Hence%2C%20a%20natural%20idea%20to%20improve%20efficiency%20is%20to%20reduce%20the%0Aunit%20number.%20Yet%2C%20the%20fact%20that%20the%20thinking%20units%20in%20vanilla%20CoT%20cannot%20be%0Aexplicitly%20managed%20renders%20doing%20so%20challenging.%20This%20paper%20introduces%0AMulti-Turn%20Decomposition%20%28MinD%29%20to%20decode%20conventional%20CoT%20into%20a%20sequence%20of%0Aexplicit%2C%20structured%2C%20and%20turn-wise%20interactions%20to%20bridge%20the%20gap.%20In%20MinD%2C%0Athe%20model%20provides%20a%20multi-turn%20response%20to%20the%20query%2C%20where%20each%20turn%20embraces%0Aa%20thinking%20unit%20and%20yields%20a%20corresponding%20answer.%20The%20subsequent%20turns%20can%0Areflect%2C%20verify%2C%20revise%2C%20or%20explore%20alternative%20approaches%20to%20both%20the%20thinking%0Aand%20answer%20parts%20of%20earlier%20ones.%20This%20not%20only%20makes%20the%20answer%20delivered%20more%0Aswiftly%2C%20but%20also%20enables%20explicit%20controls%20over%20the%20iterative%20reasoning%0Aprocess%20%28i.e.%2C%20users%20may%20halt%20or%20continue%20at%20any%20turn%29.%20We%20follow%20a%20supervised%0Afine-tuning%20%28SFT%29%20then%20reinforcement%20learning%20%28RL%29%20paradigm%20to%20realize%20MinD.%20We%0Afirst%20rephrase%20the%20outputs%20of%20an%20LRM%20into%20multi-turn%20formats%20by%20prompting%0Aanother%20LLM%2C%20and%20then%20tune%20the%20LRM%20with%20such%20data.%20Observing%20that%20the%20tuned%0Amodel%20tends%20to%20consume%20even%20more%20tokens%20than%20the%20original%20one%20%28probably%20due%20to%0Athat%20the%20multi-turn%20formats%20introduce%20additional%20answer%20tokens%29%2C%20we%20advocate%0Aleveraging%20RL%20algorithms%20like%20GRPO%20to%20prioritize%20correct%20outputs%20with%20fewer%0Aturns.%20Trained%20on%20the%20MATH%20dataset%20using%20R1-Distill%20models%2C%20MinD%20can%20achieve%20up%0Ato%20~70%25%20reduction%20in%20both%20output%20token%20usage%20and%20time%20to%20first%20token%20%28TTFT%29%2C%0Awhile%20maintaining%20competitive%20performance%20on%20reasoning%20benchmarks%20such%20as%0AMATH-500%2C%20AIME24%2C%20AMC23%2C%20and%20GPQA-Diamond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19788v1&entry.124074799=Read"},
{"title": "EmoNet-Face: An Expert-Annotated Benchmark for Synthetic Emotion\n  Recognition", "author": "Christoph Schuhmann and Robert Kaczmarczyk and Gollam Rabby and Maurice Kraus and Felix Friedrich and Huu Nguyen and Krishna Kalyan and Kourosh Nadi and Kristian Kersting and S\u00f6ren Auer", "abstract": "  Effective human-AI interaction relies on AI's ability to accurately perceive\nand interpret human emotions. Current benchmarks for vision and vision-language\nmodels are severely limited, offering a narrow emotional spectrum that\noverlooks nuanced states (e.g., bitterness, intoxication) and fails to\ndistinguish subtle differences between related feelings (e.g., shame vs.\nembarrassment). Existing datasets also often use uncontrolled imagery with\noccluded faces and lack demographic diversity, risking significant bias. To\naddress these critical gaps, we introduce EmoNet Face, a comprehensive\nbenchmark suite. EmoNet Face features: (1) A novel 40-category emotion\ntaxonomy, meticulously derived from foundational research to capture finer\ndetails of human emotional experiences. (2) Three large-scale, AI-generated\ndatasets (EmoNet HQ, Binary, and Big) with explicit, full-face expressions and\ncontrolled demographic balance across ethnicity, age, and gender. (3) Rigorous,\nmulti-expert annotations for training and high-fidelity evaluation. (4) We\nbuild Empathic Insight Face, a model achieving human-expert-level performance\non our benchmark. The publicly released EmoNet Face suite - taxonomy, datasets,\nand model - provides a robust foundation for developing and evaluating AI\nsystems with a deeper understanding of human emotions.\n", "link": "http://arxiv.org/abs/2505.20033v1", "date": "2025-05-26", "relevancy": 2.4818, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5026}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4933}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmoNet-Face%3A%20An%20Expert-Annotated%20Benchmark%20for%20Synthetic%20Emotion%0A%20%20Recognition&body=Title%3A%20EmoNet-Face%3A%20An%20Expert-Annotated%20Benchmark%20for%20Synthetic%20Emotion%0A%20%20Recognition%0AAuthor%3A%20Christoph%20Schuhmann%20and%20Robert%20Kaczmarczyk%20and%20Gollam%20Rabby%20and%20Maurice%20Kraus%20and%20Felix%20Friedrich%20and%20Huu%20Nguyen%20and%20Krishna%20Kalyan%20and%20Kourosh%20Nadi%20and%20Kristian%20Kersting%20and%20S%C3%B6ren%20Auer%0AAbstract%3A%20%20%20Effective%20human-AI%20interaction%20relies%20on%20AI%27s%20ability%20to%20accurately%20perceive%0Aand%20interpret%20human%20emotions.%20Current%20benchmarks%20for%20vision%20and%20vision-language%0Amodels%20are%20severely%20limited%2C%20offering%20a%20narrow%20emotional%20spectrum%20that%0Aoverlooks%20nuanced%20states%20%28e.g.%2C%20bitterness%2C%20intoxication%29%20and%20fails%20to%0Adistinguish%20subtle%20differences%20between%20related%20feelings%20%28e.g.%2C%20shame%20vs.%0Aembarrassment%29.%20Existing%20datasets%20also%20often%20use%20uncontrolled%20imagery%20with%0Aoccluded%20faces%20and%20lack%20demographic%20diversity%2C%20risking%20significant%20bias.%20To%0Aaddress%20these%20critical%20gaps%2C%20we%20introduce%20EmoNet%20Face%2C%20a%20comprehensive%0Abenchmark%20suite.%20EmoNet%20Face%20features%3A%20%281%29%20A%20novel%2040-category%20emotion%0Ataxonomy%2C%20meticulously%20derived%20from%20foundational%20research%20to%20capture%20finer%0Adetails%20of%20human%20emotional%20experiences.%20%282%29%20Three%20large-scale%2C%20AI-generated%0Adatasets%20%28EmoNet%20HQ%2C%20Binary%2C%20and%20Big%29%20with%20explicit%2C%20full-face%20expressions%20and%0Acontrolled%20demographic%20balance%20across%20ethnicity%2C%20age%2C%20and%20gender.%20%283%29%20Rigorous%2C%0Amulti-expert%20annotations%20for%20training%20and%20high-fidelity%20evaluation.%20%284%29%20We%0Abuild%20Empathic%20Insight%20Face%2C%20a%20model%20achieving%20human-expert-level%20performance%0Aon%20our%20benchmark.%20The%20publicly%20released%20EmoNet%20Face%20suite%20-%20taxonomy%2C%20datasets%2C%0Aand%20model%20-%20provides%20a%20robust%20foundation%20for%20developing%20and%20evaluating%20AI%0Asystems%20with%20a%20deeper%20understanding%20of%20human%20emotions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmoNet-Face%253A%2520An%2520Expert-Annotated%2520Benchmark%2520for%2520Synthetic%2520Emotion%250A%2520%2520Recognition%26entry.906535625%3DChristoph%2520Schuhmann%2520and%2520Robert%2520Kaczmarczyk%2520and%2520Gollam%2520Rabby%2520and%2520Maurice%2520Kraus%2520and%2520Felix%2520Friedrich%2520and%2520Huu%2520Nguyen%2520and%2520Krishna%2520Kalyan%2520and%2520Kourosh%2520Nadi%2520and%2520Kristian%2520Kersting%2520and%2520S%25C3%25B6ren%2520Auer%26entry.1292438233%3D%2520%2520Effective%2520human-AI%2520interaction%2520relies%2520on%2520AI%2527s%2520ability%2520to%2520accurately%2520perceive%250Aand%2520interpret%2520human%2520emotions.%2520Current%2520benchmarks%2520for%2520vision%2520and%2520vision-language%250Amodels%2520are%2520severely%2520limited%252C%2520offering%2520a%2520narrow%2520emotional%2520spectrum%2520that%250Aoverlooks%2520nuanced%2520states%2520%2528e.g.%252C%2520bitterness%252C%2520intoxication%2529%2520and%2520fails%2520to%250Adistinguish%2520subtle%2520differences%2520between%2520related%2520feelings%2520%2528e.g.%252C%2520shame%2520vs.%250Aembarrassment%2529.%2520Existing%2520datasets%2520also%2520often%2520use%2520uncontrolled%2520imagery%2520with%250Aoccluded%2520faces%2520and%2520lack%2520demographic%2520diversity%252C%2520risking%2520significant%2520bias.%2520To%250Aaddress%2520these%2520critical%2520gaps%252C%2520we%2520introduce%2520EmoNet%2520Face%252C%2520a%2520comprehensive%250Abenchmark%2520suite.%2520EmoNet%2520Face%2520features%253A%2520%25281%2529%2520A%2520novel%252040-category%2520emotion%250Ataxonomy%252C%2520meticulously%2520derived%2520from%2520foundational%2520research%2520to%2520capture%2520finer%250Adetails%2520of%2520human%2520emotional%2520experiences.%2520%25282%2529%2520Three%2520large-scale%252C%2520AI-generated%250Adatasets%2520%2528EmoNet%2520HQ%252C%2520Binary%252C%2520and%2520Big%2529%2520with%2520explicit%252C%2520full-face%2520expressions%2520and%250Acontrolled%2520demographic%2520balance%2520across%2520ethnicity%252C%2520age%252C%2520and%2520gender.%2520%25283%2529%2520Rigorous%252C%250Amulti-expert%2520annotations%2520for%2520training%2520and%2520high-fidelity%2520evaluation.%2520%25284%2529%2520We%250Abuild%2520Empathic%2520Insight%2520Face%252C%2520a%2520model%2520achieving%2520human-expert-level%2520performance%250Aon%2520our%2520benchmark.%2520The%2520publicly%2520released%2520EmoNet%2520Face%2520suite%2520-%2520taxonomy%252C%2520datasets%252C%250Aand%2520model%2520-%2520provides%2520a%2520robust%2520foundation%2520for%2520developing%2520and%2520evaluating%2520AI%250Asystems%2520with%2520a%2520deeper%2520understanding%2520of%2520human%2520emotions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmoNet-Face%3A%20An%20Expert-Annotated%20Benchmark%20for%20Synthetic%20Emotion%0A%20%20Recognition&entry.906535625=Christoph%20Schuhmann%20and%20Robert%20Kaczmarczyk%20and%20Gollam%20Rabby%20and%20Maurice%20Kraus%20and%20Felix%20Friedrich%20and%20Huu%20Nguyen%20and%20Krishna%20Kalyan%20and%20Kourosh%20Nadi%20and%20Kristian%20Kersting%20and%20S%C3%B6ren%20Auer&entry.1292438233=%20%20Effective%20human-AI%20interaction%20relies%20on%20AI%27s%20ability%20to%20accurately%20perceive%0Aand%20interpret%20human%20emotions.%20Current%20benchmarks%20for%20vision%20and%20vision-language%0Amodels%20are%20severely%20limited%2C%20offering%20a%20narrow%20emotional%20spectrum%20that%0Aoverlooks%20nuanced%20states%20%28e.g.%2C%20bitterness%2C%20intoxication%29%20and%20fails%20to%0Adistinguish%20subtle%20differences%20between%20related%20feelings%20%28e.g.%2C%20shame%20vs.%0Aembarrassment%29.%20Existing%20datasets%20also%20often%20use%20uncontrolled%20imagery%20with%0Aoccluded%20faces%20and%20lack%20demographic%20diversity%2C%20risking%20significant%20bias.%20To%0Aaddress%20these%20critical%20gaps%2C%20we%20introduce%20EmoNet%20Face%2C%20a%20comprehensive%0Abenchmark%20suite.%20EmoNet%20Face%20features%3A%20%281%29%20A%20novel%2040-category%20emotion%0Ataxonomy%2C%20meticulously%20derived%20from%20foundational%20research%20to%20capture%20finer%0Adetails%20of%20human%20emotional%20experiences.%20%282%29%20Three%20large-scale%2C%20AI-generated%0Adatasets%20%28EmoNet%20HQ%2C%20Binary%2C%20and%20Big%29%20with%20explicit%2C%20full-face%20expressions%20and%0Acontrolled%20demographic%20balance%20across%20ethnicity%2C%20age%2C%20and%20gender.%20%283%29%20Rigorous%2C%0Amulti-expert%20annotations%20for%20training%20and%20high-fidelity%20evaluation.%20%284%29%20We%0Abuild%20Empathic%20Insight%20Face%2C%20a%20model%20achieving%20human-expert-level%20performance%0Aon%20our%20benchmark.%20The%20publicly%20released%20EmoNet%20Face%20suite%20-%20taxonomy%2C%20datasets%2C%0Aand%20model%20-%20provides%20a%20robust%20foundation%20for%20developing%20and%20evaluating%20AI%0Asystems%20with%20a%20deeper%20understanding%20of%20human%20emotions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20033v1&entry.124074799=Read"},
{"title": "WorldSense: Evaluating Real-world Omnimodal Understanding for Multimodal\n  LLMs", "author": "Jack Hong and Shilin Yan and Jiayin Cai and Xiaolong Jiang and Yao Hu and Weidi Xie", "abstract": "  We introduce WorldSense, the first benchmark to assess the multi-modal video\nunderstanding, that simultaneously encompasses visual, audio, and text inputs.\nIn contrast to existing benchmarks, our WorldSense has several features: (i)\ncollaboration of omni-modality, we design the evaluation tasks to feature a\nstrong coupling of audio and video, requiring models to effectively utilize the\nsynergistic perception of omni-modality; (ii) diversity of videos and tasks,\nWorldSense encompasses a diverse collection of 1,662 audio-visual synchronised\nvideos, systematically categorized into 8 primary domains and 67 fine-grained\nsubcategories to cover the broad scenarios, and 3,172 multi-choice QA pairs\nacross 26 distinct tasks to enable the comprehensive evaluation; (iii)\nhigh-quality annotations, all the QA pairs are manually labeled by 80 expert\nannotators with multiple rounds of correction to ensure quality. Based on our\nWorldSense, we extensively evaluate various state-of-the-art models. The\nexperimental results indicate that existing models face significant challenges\nin understanding real-world scenarios (48.0% best accuracy). By analyzing the\nlimitations of current models, we aim to provide valuable insight to guide\ndevelopment of real-world understanding. We hope our WorldSense can provide a\nplatform for evaluating the ability in constructing and understanding coherent\ncontexts from omni-modality.\n", "link": "http://arxiv.org/abs/2502.04326v2", "date": "2025-05-26", "relevancy": 2.4598, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6272}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6272}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WorldSense%3A%20Evaluating%20Real-world%20Omnimodal%20Understanding%20for%20Multimodal%0A%20%20LLMs&body=Title%3A%20WorldSense%3A%20Evaluating%20Real-world%20Omnimodal%20Understanding%20for%20Multimodal%0A%20%20LLMs%0AAuthor%3A%20Jack%20Hong%20and%20Shilin%20Yan%20and%20Jiayin%20Cai%20and%20Xiaolong%20Jiang%20and%20Yao%20Hu%20and%20Weidi%20Xie%0AAbstract%3A%20%20%20We%20introduce%20WorldSense%2C%20the%20first%20benchmark%20to%20assess%20the%20multi-modal%20video%0Aunderstanding%2C%20that%20simultaneously%20encompasses%20visual%2C%20audio%2C%20and%20text%20inputs.%0AIn%20contrast%20to%20existing%20benchmarks%2C%20our%20WorldSense%20has%20several%20features%3A%20%28i%29%0Acollaboration%20of%20omni-modality%2C%20we%20design%20the%20evaluation%20tasks%20to%20feature%20a%0Astrong%20coupling%20of%20audio%20and%20video%2C%20requiring%20models%20to%20effectively%20utilize%20the%0Asynergistic%20perception%20of%20omni-modality%3B%20%28ii%29%20diversity%20of%20videos%20and%20tasks%2C%0AWorldSense%20encompasses%20a%20diverse%20collection%20of%201%2C662%20audio-visual%20synchronised%0Avideos%2C%20systematically%20categorized%20into%208%20primary%20domains%20and%2067%20fine-grained%0Asubcategories%20to%20cover%20the%20broad%20scenarios%2C%20and%203%2C172%20multi-choice%20QA%20pairs%0Aacross%2026%20distinct%20tasks%20to%20enable%20the%20comprehensive%20evaluation%3B%20%28iii%29%0Ahigh-quality%20annotations%2C%20all%20the%20QA%20pairs%20are%20manually%20labeled%20by%2080%20expert%0Aannotators%20with%20multiple%20rounds%20of%20correction%20to%20ensure%20quality.%20Based%20on%20our%0AWorldSense%2C%20we%20extensively%20evaluate%20various%20state-of-the-art%20models.%20The%0Aexperimental%20results%20indicate%20that%20existing%20models%20face%20significant%20challenges%0Ain%20understanding%20real-world%20scenarios%20%2848.0%25%20best%20accuracy%29.%20By%20analyzing%20the%0Alimitations%20of%20current%20models%2C%20we%20aim%20to%20provide%20valuable%20insight%20to%20guide%0Adevelopment%20of%20real-world%20understanding.%20We%20hope%20our%20WorldSense%20can%20provide%20a%0Aplatform%20for%20evaluating%20the%20ability%20in%20constructing%20and%20understanding%20coherent%0Acontexts%20from%20omni-modality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04326v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWorldSense%253A%2520Evaluating%2520Real-world%2520Omnimodal%2520Understanding%2520for%2520Multimodal%250A%2520%2520LLMs%26entry.906535625%3DJack%2520Hong%2520and%2520Shilin%2520Yan%2520and%2520Jiayin%2520Cai%2520and%2520Xiaolong%2520Jiang%2520and%2520Yao%2520Hu%2520and%2520Weidi%2520Xie%26entry.1292438233%3D%2520%2520We%2520introduce%2520WorldSense%252C%2520the%2520first%2520benchmark%2520to%2520assess%2520the%2520multi-modal%2520video%250Aunderstanding%252C%2520that%2520simultaneously%2520encompasses%2520visual%252C%2520audio%252C%2520and%2520text%2520inputs.%250AIn%2520contrast%2520to%2520existing%2520benchmarks%252C%2520our%2520WorldSense%2520has%2520several%2520features%253A%2520%2528i%2529%250Acollaboration%2520of%2520omni-modality%252C%2520we%2520design%2520the%2520evaluation%2520tasks%2520to%2520feature%2520a%250Astrong%2520coupling%2520of%2520audio%2520and%2520video%252C%2520requiring%2520models%2520to%2520effectively%2520utilize%2520the%250Asynergistic%2520perception%2520of%2520omni-modality%253B%2520%2528ii%2529%2520diversity%2520of%2520videos%2520and%2520tasks%252C%250AWorldSense%2520encompasses%2520a%2520diverse%2520collection%2520of%25201%252C662%2520audio-visual%2520synchronised%250Avideos%252C%2520systematically%2520categorized%2520into%25208%2520primary%2520domains%2520and%252067%2520fine-grained%250Asubcategories%2520to%2520cover%2520the%2520broad%2520scenarios%252C%2520and%25203%252C172%2520multi-choice%2520QA%2520pairs%250Aacross%252026%2520distinct%2520tasks%2520to%2520enable%2520the%2520comprehensive%2520evaluation%253B%2520%2528iii%2529%250Ahigh-quality%2520annotations%252C%2520all%2520the%2520QA%2520pairs%2520are%2520manually%2520labeled%2520by%252080%2520expert%250Aannotators%2520with%2520multiple%2520rounds%2520of%2520correction%2520to%2520ensure%2520quality.%2520Based%2520on%2520our%250AWorldSense%252C%2520we%2520extensively%2520evaluate%2520various%2520state-of-the-art%2520models.%2520The%250Aexperimental%2520results%2520indicate%2520that%2520existing%2520models%2520face%2520significant%2520challenges%250Ain%2520understanding%2520real-world%2520scenarios%2520%252848.0%2525%2520best%2520accuracy%2529.%2520By%2520analyzing%2520the%250Alimitations%2520of%2520current%2520models%252C%2520we%2520aim%2520to%2520provide%2520valuable%2520insight%2520to%2520guide%250Adevelopment%2520of%2520real-world%2520understanding.%2520We%2520hope%2520our%2520WorldSense%2520can%2520provide%2520a%250Aplatform%2520for%2520evaluating%2520the%2520ability%2520in%2520constructing%2520and%2520understanding%2520coherent%250Acontexts%2520from%2520omni-modality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04326v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WorldSense%3A%20Evaluating%20Real-world%20Omnimodal%20Understanding%20for%20Multimodal%0A%20%20LLMs&entry.906535625=Jack%20Hong%20and%20Shilin%20Yan%20and%20Jiayin%20Cai%20and%20Xiaolong%20Jiang%20and%20Yao%20Hu%20and%20Weidi%20Xie&entry.1292438233=%20%20We%20introduce%20WorldSense%2C%20the%20first%20benchmark%20to%20assess%20the%20multi-modal%20video%0Aunderstanding%2C%20that%20simultaneously%20encompasses%20visual%2C%20audio%2C%20and%20text%20inputs.%0AIn%20contrast%20to%20existing%20benchmarks%2C%20our%20WorldSense%20has%20several%20features%3A%20%28i%29%0Acollaboration%20of%20omni-modality%2C%20we%20design%20the%20evaluation%20tasks%20to%20feature%20a%0Astrong%20coupling%20of%20audio%20and%20video%2C%20requiring%20models%20to%20effectively%20utilize%20the%0Asynergistic%20perception%20of%20omni-modality%3B%20%28ii%29%20diversity%20of%20videos%20and%20tasks%2C%0AWorldSense%20encompasses%20a%20diverse%20collection%20of%201%2C662%20audio-visual%20synchronised%0Avideos%2C%20systematically%20categorized%20into%208%20primary%20domains%20and%2067%20fine-grained%0Asubcategories%20to%20cover%20the%20broad%20scenarios%2C%20and%203%2C172%20multi-choice%20QA%20pairs%0Aacross%2026%20distinct%20tasks%20to%20enable%20the%20comprehensive%20evaluation%3B%20%28iii%29%0Ahigh-quality%20annotations%2C%20all%20the%20QA%20pairs%20are%20manually%20labeled%20by%2080%20expert%0Aannotators%20with%20multiple%20rounds%20of%20correction%20to%20ensure%20quality.%20Based%20on%20our%0AWorldSense%2C%20we%20extensively%20evaluate%20various%20state-of-the-art%20models.%20The%0Aexperimental%20results%20indicate%20that%20existing%20models%20face%20significant%20challenges%0Ain%20understanding%20real-world%20scenarios%20%2848.0%25%20best%20accuracy%29.%20By%20analyzing%20the%0Alimitations%20of%20current%20models%2C%20we%20aim%20to%20provide%20valuable%20insight%20to%20guide%0Adevelopment%20of%20real-world%20understanding.%20We%20hope%20our%20WorldSense%20can%20provide%20a%0Aplatform%20for%20evaluating%20the%20ability%20in%20constructing%20and%20understanding%20coherent%0Acontexts%20from%20omni-modality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04326v2&entry.124074799=Read"},
{"title": "VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion\n  Generation in Video Models", "author": "Hila Chefer and Uriel Singer and Amit Zohar and Yuval Kirstain and Adam Polyak and Yaniv Taigman and Lior Wolf and Shelly Sheynin", "abstract": "  Despite tremendous recent progress, generative video models still struggle to\ncapture real-world motion, dynamics, and physics. We show that this limitation\narises from the conventional pixel reconstruction objective, which biases\nmodels toward appearance fidelity at the expense of motion coherence. To\naddress this, we introduce VideoJAM, a novel framework that instills an\neffective motion prior to video generators, by encouraging the model to learn a\njoint appearance-motion representation. VideoJAM is composed of two\ncomplementary units. During training, we extend the objective to predict both\nthe generated pixels and their corresponding motion from a single learned\nrepresentation. During inference, we introduce Inner-Guidance, a mechanism that\nsteers the generation toward coherent motion by leveraging the model's own\nevolving motion prediction as a dynamic guidance signal. Notably, our framework\ncan be applied to any video model with minimal adaptations, requiring no\nmodifications to the training data or scaling of the model. VideoJAM achieves\nstate-of-the-art performance in motion coherence, surpassing highly competitive\nproprietary models while also enhancing the perceived visual quality of the\ngenerations. These findings emphasize that appearance and motion can be\ncomplementary and, when effectively integrated, enhance both the visual quality\nand the coherence of video generation. Project website:\nhttps://hila-chefer.github.io/videojam-paper.github.io/\n", "link": "http://arxiv.org/abs/2502.02492v2", "date": "2025-05-26", "relevancy": 2.4508, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6264}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6174}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoJAM%3A%20Joint%20Appearance-Motion%20Representations%20for%20Enhanced%20Motion%0A%20%20Generation%20in%20Video%20Models&body=Title%3A%20VideoJAM%3A%20Joint%20Appearance-Motion%20Representations%20for%20Enhanced%20Motion%0A%20%20Generation%20in%20Video%20Models%0AAuthor%3A%20Hila%20Chefer%20and%20Uriel%20Singer%20and%20Amit%20Zohar%20and%20Yuval%20Kirstain%20and%20Adam%20Polyak%20and%20Yaniv%20Taigman%20and%20Lior%20Wolf%20and%20Shelly%20Sheynin%0AAbstract%3A%20%20%20Despite%20tremendous%20recent%20progress%2C%20generative%20video%20models%20still%20struggle%20to%0Acapture%20real-world%20motion%2C%20dynamics%2C%20and%20physics.%20We%20show%20that%20this%20limitation%0Aarises%20from%20the%20conventional%20pixel%20reconstruction%20objective%2C%20which%20biases%0Amodels%20toward%20appearance%20fidelity%20at%20the%20expense%20of%20motion%20coherence.%20To%0Aaddress%20this%2C%20we%20introduce%20VideoJAM%2C%20a%20novel%20framework%20that%20instills%20an%0Aeffective%20motion%20prior%20to%20video%20generators%2C%20by%20encouraging%20the%20model%20to%20learn%20a%0Ajoint%20appearance-motion%20representation.%20VideoJAM%20is%20composed%20of%20two%0Acomplementary%20units.%20During%20training%2C%20we%20extend%20the%20objective%20to%20predict%20both%0Athe%20generated%20pixels%20and%20their%20corresponding%20motion%20from%20a%20single%20learned%0Arepresentation.%20During%20inference%2C%20we%20introduce%20Inner-Guidance%2C%20a%20mechanism%20that%0Asteers%20the%20generation%20toward%20coherent%20motion%20by%20leveraging%20the%20model%27s%20own%0Aevolving%20motion%20prediction%20as%20a%20dynamic%20guidance%20signal.%20Notably%2C%20our%20framework%0Acan%20be%20applied%20to%20any%20video%20model%20with%20minimal%20adaptations%2C%20requiring%20no%0Amodifications%20to%20the%20training%20data%20or%20scaling%20of%20the%20model.%20VideoJAM%20achieves%0Astate-of-the-art%20performance%20in%20motion%20coherence%2C%20surpassing%20highly%20competitive%0Aproprietary%20models%20while%20also%20enhancing%20the%20perceived%20visual%20quality%20of%20the%0Agenerations.%20These%20findings%20emphasize%20that%20appearance%20and%20motion%20can%20be%0Acomplementary%20and%2C%20when%20effectively%20integrated%2C%20enhance%20both%20the%20visual%20quality%0Aand%20the%20coherence%20of%20video%20generation.%20Project%20website%3A%0Ahttps%3A//hila-chefer.github.io/videojam-paper.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02492v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoJAM%253A%2520Joint%2520Appearance-Motion%2520Representations%2520for%2520Enhanced%2520Motion%250A%2520%2520Generation%2520in%2520Video%2520Models%26entry.906535625%3DHila%2520Chefer%2520and%2520Uriel%2520Singer%2520and%2520Amit%2520Zohar%2520and%2520Yuval%2520Kirstain%2520and%2520Adam%2520Polyak%2520and%2520Yaniv%2520Taigman%2520and%2520Lior%2520Wolf%2520and%2520Shelly%2520Sheynin%26entry.1292438233%3D%2520%2520Despite%2520tremendous%2520recent%2520progress%252C%2520generative%2520video%2520models%2520still%2520struggle%2520to%250Acapture%2520real-world%2520motion%252C%2520dynamics%252C%2520and%2520physics.%2520We%2520show%2520that%2520this%2520limitation%250Aarises%2520from%2520the%2520conventional%2520pixel%2520reconstruction%2520objective%252C%2520which%2520biases%250Amodels%2520toward%2520appearance%2520fidelity%2520at%2520the%2520expense%2520of%2520motion%2520coherence.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520VideoJAM%252C%2520a%2520novel%2520framework%2520that%2520instills%2520an%250Aeffective%2520motion%2520prior%2520to%2520video%2520generators%252C%2520by%2520encouraging%2520the%2520model%2520to%2520learn%2520a%250Ajoint%2520appearance-motion%2520representation.%2520VideoJAM%2520is%2520composed%2520of%2520two%250Acomplementary%2520units.%2520During%2520training%252C%2520we%2520extend%2520the%2520objective%2520to%2520predict%2520both%250Athe%2520generated%2520pixels%2520and%2520their%2520corresponding%2520motion%2520from%2520a%2520single%2520learned%250Arepresentation.%2520During%2520inference%252C%2520we%2520introduce%2520Inner-Guidance%252C%2520a%2520mechanism%2520that%250Asteers%2520the%2520generation%2520toward%2520coherent%2520motion%2520by%2520leveraging%2520the%2520model%2527s%2520own%250Aevolving%2520motion%2520prediction%2520as%2520a%2520dynamic%2520guidance%2520signal.%2520Notably%252C%2520our%2520framework%250Acan%2520be%2520applied%2520to%2520any%2520video%2520model%2520with%2520minimal%2520adaptations%252C%2520requiring%2520no%250Amodifications%2520to%2520the%2520training%2520data%2520or%2520scaling%2520of%2520the%2520model.%2520VideoJAM%2520achieves%250Astate-of-the-art%2520performance%2520in%2520motion%2520coherence%252C%2520surpassing%2520highly%2520competitive%250Aproprietary%2520models%2520while%2520also%2520enhancing%2520the%2520perceived%2520visual%2520quality%2520of%2520the%250Agenerations.%2520These%2520findings%2520emphasize%2520that%2520appearance%2520and%2520motion%2520can%2520be%250Acomplementary%2520and%252C%2520when%2520effectively%2520integrated%252C%2520enhance%2520both%2520the%2520visual%2520quality%250Aand%2520the%2520coherence%2520of%2520video%2520generation.%2520Project%2520website%253A%250Ahttps%253A//hila-chefer.github.io/videojam-paper.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02492v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoJAM%3A%20Joint%20Appearance-Motion%20Representations%20for%20Enhanced%20Motion%0A%20%20Generation%20in%20Video%20Models&entry.906535625=Hila%20Chefer%20and%20Uriel%20Singer%20and%20Amit%20Zohar%20and%20Yuval%20Kirstain%20and%20Adam%20Polyak%20and%20Yaniv%20Taigman%20and%20Lior%20Wolf%20and%20Shelly%20Sheynin&entry.1292438233=%20%20Despite%20tremendous%20recent%20progress%2C%20generative%20video%20models%20still%20struggle%20to%0Acapture%20real-world%20motion%2C%20dynamics%2C%20and%20physics.%20We%20show%20that%20this%20limitation%0Aarises%20from%20the%20conventional%20pixel%20reconstruction%20objective%2C%20which%20biases%0Amodels%20toward%20appearance%20fidelity%20at%20the%20expense%20of%20motion%20coherence.%20To%0Aaddress%20this%2C%20we%20introduce%20VideoJAM%2C%20a%20novel%20framework%20that%20instills%20an%0Aeffective%20motion%20prior%20to%20video%20generators%2C%20by%20encouraging%20the%20model%20to%20learn%20a%0Ajoint%20appearance-motion%20representation.%20VideoJAM%20is%20composed%20of%20two%0Acomplementary%20units.%20During%20training%2C%20we%20extend%20the%20objective%20to%20predict%20both%0Athe%20generated%20pixels%20and%20their%20corresponding%20motion%20from%20a%20single%20learned%0Arepresentation.%20During%20inference%2C%20we%20introduce%20Inner-Guidance%2C%20a%20mechanism%20that%0Asteers%20the%20generation%20toward%20coherent%20motion%20by%20leveraging%20the%20model%27s%20own%0Aevolving%20motion%20prediction%20as%20a%20dynamic%20guidance%20signal.%20Notably%2C%20our%20framework%0Acan%20be%20applied%20to%20any%20video%20model%20with%20minimal%20adaptations%2C%20requiring%20no%0Amodifications%20to%20the%20training%20data%20or%20scaling%20of%20the%20model.%20VideoJAM%20achieves%0Astate-of-the-art%20performance%20in%20motion%20coherence%2C%20surpassing%20highly%20competitive%0Aproprietary%20models%20while%20also%20enhancing%20the%20perceived%20visual%20quality%20of%20the%0Agenerations.%20These%20findings%20emphasize%20that%20appearance%20and%20motion%20can%20be%0Acomplementary%20and%2C%20when%20effectively%20integrated%2C%20enhance%20both%20the%20visual%20quality%0Aand%20the%20coherence%20of%20video%20generation.%20Project%20website%3A%0Ahttps%3A//hila-chefer.github.io/videojam-paper.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02492v2&entry.124074799=Read"},
{"title": "Incentivizing Reasoning from Weak Supervision", "author": "Yige Yuan and Teng Xiao and Shuchang Tao and Xue Wang and Jinyang Gao and Bolin Ding and Bingbing Xu", "abstract": "  Large language models (LLMs) have demonstrated impressive performance on\nreasoning-intensive tasks, but enhancing their reasoning abilities typically\nrelies on either reinforcement learning (RL) with verifiable signals or\nsupervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT)\ndemonstrations, both of which are expensive. In this paper, we study a novel\nproblem of incentivizing the reasoning capacity of LLMs without expensive\nhigh-quality demonstrations and reinforcement learning. We investigate whether\nthe reasoning capabilities of LLMs can be effectively incentivized via\nsupervision from significantly weaker models. We further analyze when and why\nsuch weak supervision succeeds in eliciting reasoning abilities in stronger\nmodels. Our findings show that supervision from significantly weaker reasoners\ncan substantially improve student reasoning performance, recovering close to\n94% of the gains of expensive RL at a fraction of the cost. Experiments across\ndiverse benchmarks and model architectures demonstrate that weak reasoners can\neffectively incentivize reasoning in stronger student models, consistently\nimproving performance across a wide range of reasoning tasks. Our results\nsuggest that this simple weak-to-strong paradigm is a promising and\ngeneralizable alternative to costly methods for incentivizing strong reasoning\ncapabilities at inference-time in LLMs. The code is publicly available at\nhttps://github.com/yuanyige/W2SR.\n", "link": "http://arxiv.org/abs/2505.20072v1", "date": "2025-05-26", "relevancy": 2.428, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4878}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4878}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incentivizing%20Reasoning%20from%20Weak%20Supervision&body=Title%3A%20Incentivizing%20Reasoning%20from%20Weak%20Supervision%0AAuthor%3A%20Yige%20Yuan%20and%20Teng%20Xiao%20and%20Shuchang%20Tao%20and%20Xue%20Wang%20and%20Jinyang%20Gao%20and%20Bolin%20Ding%20and%20Bingbing%20Xu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20performance%20on%0Areasoning-intensive%20tasks%2C%20but%20enhancing%20their%20reasoning%20abilities%20typically%0Arelies%20on%20either%20reinforcement%20learning%20%28RL%29%20with%20verifiable%20signals%20or%0Asupervised%20fine-tuning%20%28SFT%29%20with%20high-quality%20long%20chain-of-thought%20%28CoT%29%0Ademonstrations%2C%20both%20of%20which%20are%20expensive.%20In%20this%20paper%2C%20we%20study%20a%20novel%0Aproblem%20of%20incentivizing%20the%20reasoning%20capacity%20of%20LLMs%20without%20expensive%0Ahigh-quality%20demonstrations%20and%20reinforcement%20learning.%20We%20investigate%20whether%0Athe%20reasoning%20capabilities%20of%20LLMs%20can%20be%20effectively%20incentivized%20via%0Asupervision%20from%20significantly%20weaker%20models.%20We%20further%20analyze%20when%20and%20why%0Asuch%20weak%20supervision%20succeeds%20in%20eliciting%20reasoning%20abilities%20in%20stronger%0Amodels.%20Our%20findings%20show%20that%20supervision%20from%20significantly%20weaker%20reasoners%0Acan%20substantially%20improve%20student%20reasoning%20performance%2C%20recovering%20close%20to%0A94%25%20of%20the%20gains%20of%20expensive%20RL%20at%20a%20fraction%20of%20the%20cost.%20Experiments%20across%0Adiverse%20benchmarks%20and%20model%20architectures%20demonstrate%20that%20weak%20reasoners%20can%0Aeffectively%20incentivize%20reasoning%20in%20stronger%20student%20models%2C%20consistently%0Aimproving%20performance%20across%20a%20wide%20range%20of%20reasoning%20tasks.%20Our%20results%0Asuggest%20that%20this%20simple%20weak-to-strong%20paradigm%20is%20a%20promising%20and%0Ageneralizable%20alternative%20to%20costly%20methods%20for%20incentivizing%20strong%20reasoning%0Acapabilities%20at%20inference-time%20in%20LLMs.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/yuanyige/W2SR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20072v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncentivizing%2520Reasoning%2520from%2520Weak%2520Supervision%26entry.906535625%3DYige%2520Yuan%2520and%2520Teng%2520Xiao%2520and%2520Shuchang%2520Tao%2520and%2520Xue%2520Wang%2520and%2520Jinyang%2520Gao%2520and%2520Bolin%2520Ding%2520and%2520Bingbing%2520Xu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520performance%2520on%250Areasoning-intensive%2520tasks%252C%2520but%2520enhancing%2520their%2520reasoning%2520abilities%2520typically%250Arelies%2520on%2520either%2520reinforcement%2520learning%2520%2528RL%2529%2520with%2520verifiable%2520signals%2520or%250Asupervised%2520fine-tuning%2520%2528SFT%2529%2520with%2520high-quality%2520long%2520chain-of-thought%2520%2528CoT%2529%250Ademonstrations%252C%2520both%2520of%2520which%2520are%2520expensive.%2520In%2520this%2520paper%252C%2520we%2520study%2520a%2520novel%250Aproblem%2520of%2520incentivizing%2520the%2520reasoning%2520capacity%2520of%2520LLMs%2520without%2520expensive%250Ahigh-quality%2520demonstrations%2520and%2520reinforcement%2520learning.%2520We%2520investigate%2520whether%250Athe%2520reasoning%2520capabilities%2520of%2520LLMs%2520can%2520be%2520effectively%2520incentivized%2520via%250Asupervision%2520from%2520significantly%2520weaker%2520models.%2520We%2520further%2520analyze%2520when%2520and%2520why%250Asuch%2520weak%2520supervision%2520succeeds%2520in%2520eliciting%2520reasoning%2520abilities%2520in%2520stronger%250Amodels.%2520Our%2520findings%2520show%2520that%2520supervision%2520from%2520significantly%2520weaker%2520reasoners%250Acan%2520substantially%2520improve%2520student%2520reasoning%2520performance%252C%2520recovering%2520close%2520to%250A94%2525%2520of%2520the%2520gains%2520of%2520expensive%2520RL%2520at%2520a%2520fraction%2520of%2520the%2520cost.%2520Experiments%2520across%250Adiverse%2520benchmarks%2520and%2520model%2520architectures%2520demonstrate%2520that%2520weak%2520reasoners%2520can%250Aeffectively%2520incentivize%2520reasoning%2520in%2520stronger%2520student%2520models%252C%2520consistently%250Aimproving%2520performance%2520across%2520a%2520wide%2520range%2520of%2520reasoning%2520tasks.%2520Our%2520results%250Asuggest%2520that%2520this%2520simple%2520weak-to-strong%2520paradigm%2520is%2520a%2520promising%2520and%250Ageneralizable%2520alternative%2520to%2520costly%2520methods%2520for%2520incentivizing%2520strong%2520reasoning%250Acapabilities%2520at%2520inference-time%2520in%2520LLMs.%2520The%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/yuanyige/W2SR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20072v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incentivizing%20Reasoning%20from%20Weak%20Supervision&entry.906535625=Yige%20Yuan%20and%20Teng%20Xiao%20and%20Shuchang%20Tao%20and%20Xue%20Wang%20and%20Jinyang%20Gao%20and%20Bolin%20Ding%20and%20Bingbing%20Xu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20performance%20on%0Areasoning-intensive%20tasks%2C%20but%20enhancing%20their%20reasoning%20abilities%20typically%0Arelies%20on%20either%20reinforcement%20learning%20%28RL%29%20with%20verifiable%20signals%20or%0Asupervised%20fine-tuning%20%28SFT%29%20with%20high-quality%20long%20chain-of-thought%20%28CoT%29%0Ademonstrations%2C%20both%20of%20which%20are%20expensive.%20In%20this%20paper%2C%20we%20study%20a%20novel%0Aproblem%20of%20incentivizing%20the%20reasoning%20capacity%20of%20LLMs%20without%20expensive%0Ahigh-quality%20demonstrations%20and%20reinforcement%20learning.%20We%20investigate%20whether%0Athe%20reasoning%20capabilities%20of%20LLMs%20can%20be%20effectively%20incentivized%20via%0Asupervision%20from%20significantly%20weaker%20models.%20We%20further%20analyze%20when%20and%20why%0Asuch%20weak%20supervision%20succeeds%20in%20eliciting%20reasoning%20abilities%20in%20stronger%0Amodels.%20Our%20findings%20show%20that%20supervision%20from%20significantly%20weaker%20reasoners%0Acan%20substantially%20improve%20student%20reasoning%20performance%2C%20recovering%20close%20to%0A94%25%20of%20the%20gains%20of%20expensive%20RL%20at%20a%20fraction%20of%20the%20cost.%20Experiments%20across%0Adiverse%20benchmarks%20and%20model%20architectures%20demonstrate%20that%20weak%20reasoners%20can%0Aeffectively%20incentivize%20reasoning%20in%20stronger%20student%20models%2C%20consistently%0Aimproving%20performance%20across%20a%20wide%20range%20of%20reasoning%20tasks.%20Our%20results%0Asuggest%20that%20this%20simple%20weak-to-strong%20paradigm%20is%20a%20promising%20and%0Ageneralizable%20alternative%20to%20costly%20methods%20for%20incentivizing%20strong%20reasoning%0Acapabilities%20at%20inference-time%20in%20LLMs.%20The%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/yuanyige/W2SR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20072v1&entry.124074799=Read"},
{"title": "HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty\n  Estimation and Budget Reallocation", "author": "Feng Xiong and Hongling Xu and Yifei Wang and Runxi Cheng and Yong Wang and Xiangxiang Chu", "abstract": "  Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of\nlarge language models (LLMs) by leveraging self-generated responses for\nself-training. Recent studies have incorporated reward models to guide response\nselection or decoding, aiming to obtain higher-quality data. However, they\ntypically allocate a uniform sampling budget across all problems, overlooking\nthe varying utility of problems at different difficulty levels. In this work,\nwe conduct an empirical study and find that problems near the boundary of the\nLLM's reasoning capability offer significantly greater learning utility than\nboth easy and overly difficult ones. To identify and exploit such problems, we\npropose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners.\nGiven a fixed sampling budget, HS-STaR first performs lightweight pre-sampling\nwith a reward-guided difficulty estimation strategy to efficiently identify\nboundary-level problems. Subsequently, it dynamically reallocates the remaining\nbudget toward these high-utility problems during a re-sampling phase,\nmaximizing the generation of valuable training data. Extensive experiments\nacross multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR\nsignificantly outperforms other baselines without requiring additional sampling\nbudget.\n", "link": "http://arxiv.org/abs/2505.19866v1", "date": "2025-05-26", "relevancy": 2.4276, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5178}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4779}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HS-STAR%3A%20Hierarchical%20Sampling%20for%20Self-Taught%20Reasoners%20via%20Difficulty%0A%20%20Estimation%20and%20Budget%20Reallocation&body=Title%3A%20HS-STAR%3A%20Hierarchical%20Sampling%20for%20Self-Taught%20Reasoners%20via%20Difficulty%0A%20%20Estimation%20and%20Budget%20Reallocation%0AAuthor%3A%20Feng%20Xiong%20and%20Hongling%20Xu%20and%20Yifei%20Wang%20and%20Runxi%20Cheng%20and%20Yong%20Wang%20and%20Xiangxiang%20Chu%0AAbstract%3A%20%20%20Self-taught%20reasoners%20%28STaRs%29%20enhance%20the%20mathematical%20reasoning%20abilities%20of%0Alarge%20language%20models%20%28LLMs%29%20by%20leveraging%20self-generated%20responses%20for%0Aself-training.%20Recent%20studies%20have%20incorporated%20reward%20models%20to%20guide%20response%0Aselection%20or%20decoding%2C%20aiming%20to%20obtain%20higher-quality%20data.%20However%2C%20they%0Atypically%20allocate%20a%20uniform%20sampling%20budget%20across%20all%20problems%2C%20overlooking%0Athe%20varying%20utility%20of%20problems%20at%20different%20difficulty%20levels.%20In%20this%20work%2C%0Awe%20conduct%20an%20empirical%20study%20and%20find%20that%20problems%20near%20the%20boundary%20of%20the%0ALLM%27s%20reasoning%20capability%20offer%20significantly%20greater%20learning%20utility%20than%0Aboth%20easy%20and%20overly%20difficult%20ones.%20To%20identify%20and%20exploit%20such%20problems%2C%20we%0Apropose%20HS-STaR%2C%20a%20Hierarchical%20Sampling%20framework%20for%20Self-Taught%20Reasoners.%0AGiven%20a%20fixed%20sampling%20budget%2C%20HS-STaR%20first%20performs%20lightweight%20pre-sampling%0Awith%20a%20reward-guided%20difficulty%20estimation%20strategy%20to%20efficiently%20identify%0Aboundary-level%20problems.%20Subsequently%2C%20it%20dynamically%20reallocates%20the%20remaining%0Abudget%20toward%20these%20high-utility%20problems%20during%20a%20re-sampling%20phase%2C%0Amaximizing%20the%20generation%20of%20valuable%20training%20data.%20Extensive%20experiments%0Aacross%20multiple%20reasoning%20benchmarks%20and%20backbone%20LLMs%20demonstrate%20that%20HS-STaR%0Asignificantly%20outperforms%20other%20baselines%20without%20requiring%20additional%20sampling%0Abudget.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19866v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHS-STAR%253A%2520Hierarchical%2520Sampling%2520for%2520Self-Taught%2520Reasoners%2520via%2520Difficulty%250A%2520%2520Estimation%2520and%2520Budget%2520Reallocation%26entry.906535625%3DFeng%2520Xiong%2520and%2520Hongling%2520Xu%2520and%2520Yifei%2520Wang%2520and%2520Runxi%2520Cheng%2520and%2520Yong%2520Wang%2520and%2520Xiangxiang%2520Chu%26entry.1292438233%3D%2520%2520Self-taught%2520reasoners%2520%2528STaRs%2529%2520enhance%2520the%2520mathematical%2520reasoning%2520abilities%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520by%2520leveraging%2520self-generated%2520responses%2520for%250Aself-training.%2520Recent%2520studies%2520have%2520incorporated%2520reward%2520models%2520to%2520guide%2520response%250Aselection%2520or%2520decoding%252C%2520aiming%2520to%2520obtain%2520higher-quality%2520data.%2520However%252C%2520they%250Atypically%2520allocate%2520a%2520uniform%2520sampling%2520budget%2520across%2520all%2520problems%252C%2520overlooking%250Athe%2520varying%2520utility%2520of%2520problems%2520at%2520different%2520difficulty%2520levels.%2520In%2520this%2520work%252C%250Awe%2520conduct%2520an%2520empirical%2520study%2520and%2520find%2520that%2520problems%2520near%2520the%2520boundary%2520of%2520the%250ALLM%2527s%2520reasoning%2520capability%2520offer%2520significantly%2520greater%2520learning%2520utility%2520than%250Aboth%2520easy%2520and%2520overly%2520difficult%2520ones.%2520To%2520identify%2520and%2520exploit%2520such%2520problems%252C%2520we%250Apropose%2520HS-STaR%252C%2520a%2520Hierarchical%2520Sampling%2520framework%2520for%2520Self-Taught%2520Reasoners.%250AGiven%2520a%2520fixed%2520sampling%2520budget%252C%2520HS-STaR%2520first%2520performs%2520lightweight%2520pre-sampling%250Awith%2520a%2520reward-guided%2520difficulty%2520estimation%2520strategy%2520to%2520efficiently%2520identify%250Aboundary-level%2520problems.%2520Subsequently%252C%2520it%2520dynamically%2520reallocates%2520the%2520remaining%250Abudget%2520toward%2520these%2520high-utility%2520problems%2520during%2520a%2520re-sampling%2520phase%252C%250Amaximizing%2520the%2520generation%2520of%2520valuable%2520training%2520data.%2520Extensive%2520experiments%250Aacross%2520multiple%2520reasoning%2520benchmarks%2520and%2520backbone%2520LLMs%2520demonstrate%2520that%2520HS-STaR%250Asignificantly%2520outperforms%2520other%2520baselines%2520without%2520requiring%2520additional%2520sampling%250Abudget.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19866v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HS-STAR%3A%20Hierarchical%20Sampling%20for%20Self-Taught%20Reasoners%20via%20Difficulty%0A%20%20Estimation%20and%20Budget%20Reallocation&entry.906535625=Feng%20Xiong%20and%20Hongling%20Xu%20and%20Yifei%20Wang%20and%20Runxi%20Cheng%20and%20Yong%20Wang%20and%20Xiangxiang%20Chu&entry.1292438233=%20%20Self-taught%20reasoners%20%28STaRs%29%20enhance%20the%20mathematical%20reasoning%20abilities%20of%0Alarge%20language%20models%20%28LLMs%29%20by%20leveraging%20self-generated%20responses%20for%0Aself-training.%20Recent%20studies%20have%20incorporated%20reward%20models%20to%20guide%20response%0Aselection%20or%20decoding%2C%20aiming%20to%20obtain%20higher-quality%20data.%20However%2C%20they%0Atypically%20allocate%20a%20uniform%20sampling%20budget%20across%20all%20problems%2C%20overlooking%0Athe%20varying%20utility%20of%20problems%20at%20different%20difficulty%20levels.%20In%20this%20work%2C%0Awe%20conduct%20an%20empirical%20study%20and%20find%20that%20problems%20near%20the%20boundary%20of%20the%0ALLM%27s%20reasoning%20capability%20offer%20significantly%20greater%20learning%20utility%20than%0Aboth%20easy%20and%20overly%20difficult%20ones.%20To%20identify%20and%20exploit%20such%20problems%2C%20we%0Apropose%20HS-STaR%2C%20a%20Hierarchical%20Sampling%20framework%20for%20Self-Taught%20Reasoners.%0AGiven%20a%20fixed%20sampling%20budget%2C%20HS-STaR%20first%20performs%20lightweight%20pre-sampling%0Awith%20a%20reward-guided%20difficulty%20estimation%20strategy%20to%20efficiently%20identify%0Aboundary-level%20problems.%20Subsequently%2C%20it%20dynamically%20reallocates%20the%20remaining%0Abudget%20toward%20these%20high-utility%20problems%20during%20a%20re-sampling%20phase%2C%0Amaximizing%20the%20generation%20of%20valuable%20training%20data.%20Extensive%20experiments%0Aacross%20multiple%20reasoning%20benchmarks%20and%20backbone%20LLMs%20demonstrate%20that%20HS-STaR%0Asignificantly%20outperforms%20other%20baselines%20without%20requiring%20additional%20sampling%0Abudget.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19866v1&entry.124074799=Read"},
{"title": "Harnessing the Power of Training-Free Techniques in Text-to-2D\n  Generation for Text-to-3D Generation via Score Distillation Sampling", "author": "Junhong Lee and Seungwook Kim and Minsu Cho", "abstract": "  Recent studies show that simple training-free techniques can dramatically\nimprove the quality of text-to-2D generation outputs, e.g. Classifier-Free\nGuidance (CFG) or FreeU. However, these training-free techniques have been\nunderexplored in the lens of Score Distillation Sampling (SDS), which is a\npopular and effective technique to leverage the power of pretrained text-to-2D\ndiffusion models for various tasks. In this paper, we aim to shed light on the\neffect such training-free techniques have on SDS, via a particular application\nof text-to-3D generation via 2D lifting. We present our findings, which show\nthat varying the scales of CFG presents a trade-off between object size and\nsurface smoothness, while varying the scales of FreeU presents a trade-off\nbetween texture details and geometric errors. Based on these findings, we\nprovide insights into how we can effectively harness training-free techniques\nfor SDS, via a strategic scaling of such techniques in a dynamic manner with\nrespect to the timestep or optimization iteration step. We show that using our\nproposed scheme strikes a favorable balance between texture details and surface\nsmoothness in text-to-3D generations, while preserving the size of the output\nand mitigating the occurrence of geometric defects.\n", "link": "http://arxiv.org/abs/2505.19868v1", "date": "2025-05-26", "relevancy": 2.4225, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6327}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6102}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20the%20Power%20of%20Training-Free%20Techniques%20in%20Text-to-2D%0A%20%20Generation%20for%20Text-to-3D%20Generation%20via%20Score%20Distillation%20Sampling&body=Title%3A%20Harnessing%20the%20Power%20of%20Training-Free%20Techniques%20in%20Text-to-2D%0A%20%20Generation%20for%20Text-to-3D%20Generation%20via%20Score%20Distillation%20Sampling%0AAuthor%3A%20Junhong%20Lee%20and%20Seungwook%20Kim%20and%20Minsu%20Cho%0AAbstract%3A%20%20%20Recent%20studies%20show%20that%20simple%20training-free%20techniques%20can%20dramatically%0Aimprove%20the%20quality%20of%20text-to-2D%20generation%20outputs%2C%20e.g.%20Classifier-Free%0AGuidance%20%28CFG%29%20or%20FreeU.%20However%2C%20these%20training-free%20techniques%20have%20been%0Aunderexplored%20in%20the%20lens%20of%20Score%20Distillation%20Sampling%20%28SDS%29%2C%20which%20is%20a%0Apopular%20and%20effective%20technique%20to%20leverage%20the%20power%20of%20pretrained%20text-to-2D%0Adiffusion%20models%20for%20various%20tasks.%20In%20this%20paper%2C%20we%20aim%20to%20shed%20light%20on%20the%0Aeffect%20such%20training-free%20techniques%20have%20on%20SDS%2C%20via%20a%20particular%20application%0Aof%20text-to-3D%20generation%20via%202D%20lifting.%20We%20present%20our%20findings%2C%20which%20show%0Athat%20varying%20the%20scales%20of%20CFG%20presents%20a%20trade-off%20between%20object%20size%20and%0Asurface%20smoothness%2C%20while%20varying%20the%20scales%20of%20FreeU%20presents%20a%20trade-off%0Abetween%20texture%20details%20and%20geometric%20errors.%20Based%20on%20these%20findings%2C%20we%0Aprovide%20insights%20into%20how%20we%20can%20effectively%20harness%20training-free%20techniques%0Afor%20SDS%2C%20via%20a%20strategic%20scaling%20of%20such%20techniques%20in%20a%20dynamic%20manner%20with%0Arespect%20to%20the%20timestep%20or%20optimization%20iteration%20step.%20We%20show%20that%20using%20our%0Aproposed%20scheme%20strikes%20a%20favorable%20balance%20between%20texture%20details%20and%20surface%0Asmoothness%20in%20text-to-3D%20generations%2C%20while%20preserving%20the%20size%20of%20the%20output%0Aand%20mitigating%20the%20occurrence%20of%20geometric%20defects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520the%2520Power%2520of%2520Training-Free%2520Techniques%2520in%2520Text-to-2D%250A%2520%2520Generation%2520for%2520Text-to-3D%2520Generation%2520via%2520Score%2520Distillation%2520Sampling%26entry.906535625%3DJunhong%2520Lee%2520and%2520Seungwook%2520Kim%2520and%2520Minsu%2520Cho%26entry.1292438233%3D%2520%2520Recent%2520studies%2520show%2520that%2520simple%2520training-free%2520techniques%2520can%2520dramatically%250Aimprove%2520the%2520quality%2520of%2520text-to-2D%2520generation%2520outputs%252C%2520e.g.%2520Classifier-Free%250AGuidance%2520%2528CFG%2529%2520or%2520FreeU.%2520However%252C%2520these%2520training-free%2520techniques%2520have%2520been%250Aunderexplored%2520in%2520the%2520lens%2520of%2520Score%2520Distillation%2520Sampling%2520%2528SDS%2529%252C%2520which%2520is%2520a%250Apopular%2520and%2520effective%2520technique%2520to%2520leverage%2520the%2520power%2520of%2520pretrained%2520text-to-2D%250Adiffusion%2520models%2520for%2520various%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520shed%2520light%2520on%2520the%250Aeffect%2520such%2520training-free%2520techniques%2520have%2520on%2520SDS%252C%2520via%2520a%2520particular%2520application%250Aof%2520text-to-3D%2520generation%2520via%25202D%2520lifting.%2520We%2520present%2520our%2520findings%252C%2520which%2520show%250Athat%2520varying%2520the%2520scales%2520of%2520CFG%2520presents%2520a%2520trade-off%2520between%2520object%2520size%2520and%250Asurface%2520smoothness%252C%2520while%2520varying%2520the%2520scales%2520of%2520FreeU%2520presents%2520a%2520trade-off%250Abetween%2520texture%2520details%2520and%2520geometric%2520errors.%2520Based%2520on%2520these%2520findings%252C%2520we%250Aprovide%2520insights%2520into%2520how%2520we%2520can%2520effectively%2520harness%2520training-free%2520techniques%250Afor%2520SDS%252C%2520via%2520a%2520strategic%2520scaling%2520of%2520such%2520techniques%2520in%2520a%2520dynamic%2520manner%2520with%250Arespect%2520to%2520the%2520timestep%2520or%2520optimization%2520iteration%2520step.%2520We%2520show%2520that%2520using%2520our%250Aproposed%2520scheme%2520strikes%2520a%2520favorable%2520balance%2520between%2520texture%2520details%2520and%2520surface%250Asmoothness%2520in%2520text-to-3D%2520generations%252C%2520while%2520preserving%2520the%2520size%2520of%2520the%2520output%250Aand%2520mitigating%2520the%2520occurrence%2520of%2520geometric%2520defects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20the%20Power%20of%20Training-Free%20Techniques%20in%20Text-to-2D%0A%20%20Generation%20for%20Text-to-3D%20Generation%20via%20Score%20Distillation%20Sampling&entry.906535625=Junhong%20Lee%20and%20Seungwook%20Kim%20and%20Minsu%20Cho&entry.1292438233=%20%20Recent%20studies%20show%20that%20simple%20training-free%20techniques%20can%20dramatically%0Aimprove%20the%20quality%20of%20text-to-2D%20generation%20outputs%2C%20e.g.%20Classifier-Free%0AGuidance%20%28CFG%29%20or%20FreeU.%20However%2C%20these%20training-free%20techniques%20have%20been%0Aunderexplored%20in%20the%20lens%20of%20Score%20Distillation%20Sampling%20%28SDS%29%2C%20which%20is%20a%0Apopular%20and%20effective%20technique%20to%20leverage%20the%20power%20of%20pretrained%20text-to-2D%0Adiffusion%20models%20for%20various%20tasks.%20In%20this%20paper%2C%20we%20aim%20to%20shed%20light%20on%20the%0Aeffect%20such%20training-free%20techniques%20have%20on%20SDS%2C%20via%20a%20particular%20application%0Aof%20text-to-3D%20generation%20via%202D%20lifting.%20We%20present%20our%20findings%2C%20which%20show%0Athat%20varying%20the%20scales%20of%20CFG%20presents%20a%20trade-off%20between%20object%20size%20and%0Asurface%20smoothness%2C%20while%20varying%20the%20scales%20of%20FreeU%20presents%20a%20trade-off%0Abetween%20texture%20details%20and%20geometric%20errors.%20Based%20on%20these%20findings%2C%20we%0Aprovide%20insights%20into%20how%20we%20can%20effectively%20harness%20training-free%20techniques%0Afor%20SDS%2C%20via%20a%20strategic%20scaling%20of%20such%20techniques%20in%20a%20dynamic%20manner%20with%0Arespect%20to%20the%20timestep%20or%20optimization%20iteration%20step.%20We%20show%20that%20using%20our%0Aproposed%20scheme%20strikes%20a%20favorable%20balance%20between%20texture%20details%20and%20surface%0Asmoothness%20in%20text-to-3D%20generations%2C%20while%20preserving%20the%20size%20of%20the%20output%0Aand%20mitigating%20the%20occurrence%20of%20geometric%20defects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19868v1&entry.124074799=Read"},
{"title": "From Data to Modeling: Fully Open-vocabulary Scene Graph Generation", "author": "Zuyao Chen and Jinlin Wu and Zhen Lei and Chang Wen Chen", "abstract": "  We present OvSGTR, a novel transformer-based framework for fully\nopen-vocabulary scene graph generation that overcomes the limitations of\ntraditional closed-set models. Conventional methods restrict both object and\nrelationship recognition to a fixed vocabulary, hindering their applicability\nto real-world scenarios where novel concepts frequently emerge. In contrast,\nour approach jointly predicts objects (nodes) and their inter-relationships\n(edges) beyond predefined categories. OvSGTR leverages a DETR-like architecture\nfeaturing a frozen image backbone and text encoder to extract high-quality\nvisual and semantic features, which are then fused via a transformer decoder\nfor end-to-end scene graph prediction. To enrich the model's understanding of\ncomplex visual relations, we propose a relation-aware pre-training strategy\nthat synthesizes scene graph annotations in a weakly supervised manner.\nSpecifically, we investigate three pipelines--scene parser-based, LLM-based,\nand multimodal LLM-based--to generate transferable supervision signals with\nminimal manual annotation. Furthermore, we address the common issue of\ncatastrophic forgetting in open-vocabulary settings by incorporating a\nvisual-concept retention mechanism coupled with a knowledge distillation\nstrategy, ensuring that the model retains rich semantic cues during\nfine-tuning. Extensive experiments on the VG150 benchmark demonstrate that\nOvSGTR achieves state-of-the-art performance across multiple settings,\nincluding closed-set, open-vocabulary object detection-based, relation-based,\nand fully open-vocabulary scenarios. Our results highlight the promise of\nlarge-scale relation-aware pre-training and transformer architectures for\nadvancing scene graph generation towards more generalized and reliable visual\nunderstanding.\n", "link": "http://arxiv.org/abs/2505.20106v1", "date": "2025-05-26", "relevancy": 2.4129, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6062}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6062}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Data%20to%20Modeling%3A%20Fully%20Open-vocabulary%20Scene%20Graph%20Generation&body=Title%3A%20From%20Data%20to%20Modeling%3A%20Fully%20Open-vocabulary%20Scene%20Graph%20Generation%0AAuthor%3A%20Zuyao%20Chen%20and%20Jinlin%20Wu%20and%20Zhen%20Lei%20and%20Chang%20Wen%20Chen%0AAbstract%3A%20%20%20We%20present%20OvSGTR%2C%20a%20novel%20transformer-based%20framework%20for%20fully%0Aopen-vocabulary%20scene%20graph%20generation%20that%20overcomes%20the%20limitations%20of%0Atraditional%20closed-set%20models.%20Conventional%20methods%20restrict%20both%20object%20and%0Arelationship%20recognition%20to%20a%20fixed%20vocabulary%2C%20hindering%20their%20applicability%0Ato%20real-world%20scenarios%20where%20novel%20concepts%20frequently%20emerge.%20In%20contrast%2C%0Aour%20approach%20jointly%20predicts%20objects%20%28nodes%29%20and%20their%20inter-relationships%0A%28edges%29%20beyond%20predefined%20categories.%20OvSGTR%20leverages%20a%20DETR-like%20architecture%0Afeaturing%20a%20frozen%20image%20backbone%20and%20text%20encoder%20to%20extract%20high-quality%0Avisual%20and%20semantic%20features%2C%20which%20are%20then%20fused%20via%20a%20transformer%20decoder%0Afor%20end-to-end%20scene%20graph%20prediction.%20To%20enrich%20the%20model%27s%20understanding%20of%0Acomplex%20visual%20relations%2C%20we%20propose%20a%20relation-aware%20pre-training%20strategy%0Athat%20synthesizes%20scene%20graph%20annotations%20in%20a%20weakly%20supervised%20manner.%0ASpecifically%2C%20we%20investigate%20three%20pipelines--scene%20parser-based%2C%20LLM-based%2C%0Aand%20multimodal%20LLM-based--to%20generate%20transferable%20supervision%20signals%20with%0Aminimal%20manual%20annotation.%20Furthermore%2C%20we%20address%20the%20common%20issue%20of%0Acatastrophic%20forgetting%20in%20open-vocabulary%20settings%20by%20incorporating%20a%0Avisual-concept%20retention%20mechanism%20coupled%20with%20a%20knowledge%20distillation%0Astrategy%2C%20ensuring%20that%20the%20model%20retains%20rich%20semantic%20cues%20during%0Afine-tuning.%20Extensive%20experiments%20on%20the%20VG150%20benchmark%20demonstrate%20that%0AOvSGTR%20achieves%20state-of-the-art%20performance%20across%20multiple%20settings%2C%0Aincluding%20closed-set%2C%20open-vocabulary%20object%20detection-based%2C%20relation-based%2C%0Aand%20fully%20open-vocabulary%20scenarios.%20Our%20results%20highlight%20the%20promise%20of%0Alarge-scale%20relation-aware%20pre-training%20and%20transformer%20architectures%20for%0Aadvancing%20scene%20graph%20generation%20towards%20more%20generalized%20and%20reliable%20visual%0Aunderstanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Data%2520to%2520Modeling%253A%2520Fully%2520Open-vocabulary%2520Scene%2520Graph%2520Generation%26entry.906535625%3DZuyao%2520Chen%2520and%2520Jinlin%2520Wu%2520and%2520Zhen%2520Lei%2520and%2520Chang%2520Wen%2520Chen%26entry.1292438233%3D%2520%2520We%2520present%2520OvSGTR%252C%2520a%2520novel%2520transformer-based%2520framework%2520for%2520fully%250Aopen-vocabulary%2520scene%2520graph%2520generation%2520that%2520overcomes%2520the%2520limitations%2520of%250Atraditional%2520closed-set%2520models.%2520Conventional%2520methods%2520restrict%2520both%2520object%2520and%250Arelationship%2520recognition%2520to%2520a%2520fixed%2520vocabulary%252C%2520hindering%2520their%2520applicability%250Ato%2520real-world%2520scenarios%2520where%2520novel%2520concepts%2520frequently%2520emerge.%2520In%2520contrast%252C%250Aour%2520approach%2520jointly%2520predicts%2520objects%2520%2528nodes%2529%2520and%2520their%2520inter-relationships%250A%2528edges%2529%2520beyond%2520predefined%2520categories.%2520OvSGTR%2520leverages%2520a%2520DETR-like%2520architecture%250Afeaturing%2520a%2520frozen%2520image%2520backbone%2520and%2520text%2520encoder%2520to%2520extract%2520high-quality%250Avisual%2520and%2520semantic%2520features%252C%2520which%2520are%2520then%2520fused%2520via%2520a%2520transformer%2520decoder%250Afor%2520end-to-end%2520scene%2520graph%2520prediction.%2520To%2520enrich%2520the%2520model%2527s%2520understanding%2520of%250Acomplex%2520visual%2520relations%252C%2520we%2520propose%2520a%2520relation-aware%2520pre-training%2520strategy%250Athat%2520synthesizes%2520scene%2520graph%2520annotations%2520in%2520a%2520weakly%2520supervised%2520manner.%250ASpecifically%252C%2520we%2520investigate%2520three%2520pipelines--scene%2520parser-based%252C%2520LLM-based%252C%250Aand%2520multimodal%2520LLM-based--to%2520generate%2520transferable%2520supervision%2520signals%2520with%250Aminimal%2520manual%2520annotation.%2520Furthermore%252C%2520we%2520address%2520the%2520common%2520issue%2520of%250Acatastrophic%2520forgetting%2520in%2520open-vocabulary%2520settings%2520by%2520incorporating%2520a%250Avisual-concept%2520retention%2520mechanism%2520coupled%2520with%2520a%2520knowledge%2520distillation%250Astrategy%252C%2520ensuring%2520that%2520the%2520model%2520retains%2520rich%2520semantic%2520cues%2520during%250Afine-tuning.%2520Extensive%2520experiments%2520on%2520the%2520VG150%2520benchmark%2520demonstrate%2520that%250AOvSGTR%2520achieves%2520state-of-the-art%2520performance%2520across%2520multiple%2520settings%252C%250Aincluding%2520closed-set%252C%2520open-vocabulary%2520object%2520detection-based%252C%2520relation-based%252C%250Aand%2520fully%2520open-vocabulary%2520scenarios.%2520Our%2520results%2520highlight%2520the%2520promise%2520of%250Alarge-scale%2520relation-aware%2520pre-training%2520and%2520transformer%2520architectures%2520for%250Aadvancing%2520scene%2520graph%2520generation%2520towards%2520more%2520generalized%2520and%2520reliable%2520visual%250Aunderstanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Data%20to%20Modeling%3A%20Fully%20Open-vocabulary%20Scene%20Graph%20Generation&entry.906535625=Zuyao%20Chen%20and%20Jinlin%20Wu%20and%20Zhen%20Lei%20and%20Chang%20Wen%20Chen&entry.1292438233=%20%20We%20present%20OvSGTR%2C%20a%20novel%20transformer-based%20framework%20for%20fully%0Aopen-vocabulary%20scene%20graph%20generation%20that%20overcomes%20the%20limitations%20of%0Atraditional%20closed-set%20models.%20Conventional%20methods%20restrict%20both%20object%20and%0Arelationship%20recognition%20to%20a%20fixed%20vocabulary%2C%20hindering%20their%20applicability%0Ato%20real-world%20scenarios%20where%20novel%20concepts%20frequently%20emerge.%20In%20contrast%2C%0Aour%20approach%20jointly%20predicts%20objects%20%28nodes%29%20and%20their%20inter-relationships%0A%28edges%29%20beyond%20predefined%20categories.%20OvSGTR%20leverages%20a%20DETR-like%20architecture%0Afeaturing%20a%20frozen%20image%20backbone%20and%20text%20encoder%20to%20extract%20high-quality%0Avisual%20and%20semantic%20features%2C%20which%20are%20then%20fused%20via%20a%20transformer%20decoder%0Afor%20end-to-end%20scene%20graph%20prediction.%20To%20enrich%20the%20model%27s%20understanding%20of%0Acomplex%20visual%20relations%2C%20we%20propose%20a%20relation-aware%20pre-training%20strategy%0Athat%20synthesizes%20scene%20graph%20annotations%20in%20a%20weakly%20supervised%20manner.%0ASpecifically%2C%20we%20investigate%20three%20pipelines--scene%20parser-based%2C%20LLM-based%2C%0Aand%20multimodal%20LLM-based--to%20generate%20transferable%20supervision%20signals%20with%0Aminimal%20manual%20annotation.%20Furthermore%2C%20we%20address%20the%20common%20issue%20of%0Acatastrophic%20forgetting%20in%20open-vocabulary%20settings%20by%20incorporating%20a%0Avisual-concept%20retention%20mechanism%20coupled%20with%20a%20knowledge%20distillation%0Astrategy%2C%20ensuring%20that%20the%20model%20retains%20rich%20semantic%20cues%20during%0Afine-tuning.%20Extensive%20experiments%20on%20the%20VG150%20benchmark%20demonstrate%20that%0AOvSGTR%20achieves%20state-of-the-art%20performance%20across%20multiple%20settings%2C%0Aincluding%20closed-set%2C%20open-vocabulary%20object%20detection-based%2C%20relation-based%2C%0Aand%20fully%20open-vocabulary%20scenarios.%20Our%20results%20highlight%20the%20promise%20of%0Alarge-scale%20relation-aware%20pre-training%20and%20transformer%20architectures%20for%0Aadvancing%20scene%20graph%20generation%20towards%20more%20generalized%20and%20reliable%20visual%0Aunderstanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20106v1&entry.124074799=Read"},
{"title": "Homophily Enhanced Graph Domain Adaptation", "author": "Ruiyi Fang and Bingheng Li and Jingyu Zhao and Ruizhi Pu and Qiuhao Zeng and Gezheng Xu and Charles Ling and Boyu Wang", "abstract": "  Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs\nto unlabeled target graphs, addressing the challenge of label scarcity. In this\npaper, we highlight the significance of graph homophily, a pivotal factor for\ngraph domain alignment, which, however, has long been overlooked in existing\napproaches. Specifically, our analysis first reveals that homophily\ndiscrepancies exist in benchmarks. Moreover, we also show that homophily\ndiscrepancies degrade GDA performance from both empirical and theoretical\naspects, which further underscores the importance of homophily alignment in\nGDA. Inspired by this finding, we propose a novel homophily alignment algorithm\nthat employs mixed filters to smooth graph signals, thereby effectively\ncapturing and mitigating homophily discrepancies between graphs. Experimental\nresults on a variety of benchmarks verify the effectiveness of our method.\n", "link": "http://arxiv.org/abs/2505.20089v1", "date": "2025-05-26", "relevancy": 2.3989, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5026}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4792}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Homophily%20Enhanced%20Graph%20Domain%20Adaptation&body=Title%3A%20Homophily%20Enhanced%20Graph%20Domain%20Adaptation%0AAuthor%3A%20Ruiyi%20Fang%20and%20Bingheng%20Li%20and%20Jingyu%20Zhao%20and%20Ruizhi%20Pu%20and%20Qiuhao%20Zeng%20and%20Gezheng%20Xu%20and%20Charles%20Ling%20and%20Boyu%20Wang%0AAbstract%3A%20%20%20Graph%20Domain%20Adaptation%20%28GDA%29%20transfers%20knowledge%20from%20labeled%20source%20graphs%0Ato%20unlabeled%20target%20graphs%2C%20addressing%20the%20challenge%20of%20label%20scarcity.%20In%20this%0Apaper%2C%20we%20highlight%20the%20significance%20of%20graph%20homophily%2C%20a%20pivotal%20factor%20for%0Agraph%20domain%20alignment%2C%20which%2C%20however%2C%20has%20long%20been%20overlooked%20in%20existing%0Aapproaches.%20Specifically%2C%20our%20analysis%20first%20reveals%20that%20homophily%0Adiscrepancies%20exist%20in%20benchmarks.%20Moreover%2C%20we%20also%20show%20that%20homophily%0Adiscrepancies%20degrade%20GDA%20performance%20from%20both%20empirical%20and%20theoretical%0Aaspects%2C%20which%20further%20underscores%20the%20importance%20of%20homophily%20alignment%20in%0AGDA.%20Inspired%20by%20this%20finding%2C%20we%20propose%20a%20novel%20homophily%20alignment%20algorithm%0Athat%20employs%20mixed%20filters%20to%20smooth%20graph%20signals%2C%20thereby%20effectively%0Acapturing%20and%20mitigating%20homophily%20discrepancies%20between%20graphs.%20Experimental%0Aresults%20on%20a%20variety%20of%20benchmarks%20verify%20the%20effectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHomophily%2520Enhanced%2520Graph%2520Domain%2520Adaptation%26entry.906535625%3DRuiyi%2520Fang%2520and%2520Bingheng%2520Li%2520and%2520Jingyu%2520Zhao%2520and%2520Ruizhi%2520Pu%2520and%2520Qiuhao%2520Zeng%2520and%2520Gezheng%2520Xu%2520and%2520Charles%2520Ling%2520and%2520Boyu%2520Wang%26entry.1292438233%3D%2520%2520Graph%2520Domain%2520Adaptation%2520%2528GDA%2529%2520transfers%2520knowledge%2520from%2520labeled%2520source%2520graphs%250Ato%2520unlabeled%2520target%2520graphs%252C%2520addressing%2520the%2520challenge%2520of%2520label%2520scarcity.%2520In%2520this%250Apaper%252C%2520we%2520highlight%2520the%2520significance%2520of%2520graph%2520homophily%252C%2520a%2520pivotal%2520factor%2520for%250Agraph%2520domain%2520alignment%252C%2520which%252C%2520however%252C%2520has%2520long%2520been%2520overlooked%2520in%2520existing%250Aapproaches.%2520Specifically%252C%2520our%2520analysis%2520first%2520reveals%2520that%2520homophily%250Adiscrepancies%2520exist%2520in%2520benchmarks.%2520Moreover%252C%2520we%2520also%2520show%2520that%2520homophily%250Adiscrepancies%2520degrade%2520GDA%2520performance%2520from%2520both%2520empirical%2520and%2520theoretical%250Aaspects%252C%2520which%2520further%2520underscores%2520the%2520importance%2520of%2520homophily%2520alignment%2520in%250AGDA.%2520Inspired%2520by%2520this%2520finding%252C%2520we%2520propose%2520a%2520novel%2520homophily%2520alignment%2520algorithm%250Athat%2520employs%2520mixed%2520filters%2520to%2520smooth%2520graph%2520signals%252C%2520thereby%2520effectively%250Acapturing%2520and%2520mitigating%2520homophily%2520discrepancies%2520between%2520graphs.%2520Experimental%250Aresults%2520on%2520a%2520variety%2520of%2520benchmarks%2520verify%2520the%2520effectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Homophily%20Enhanced%20Graph%20Domain%20Adaptation&entry.906535625=Ruiyi%20Fang%20and%20Bingheng%20Li%20and%20Jingyu%20Zhao%20and%20Ruizhi%20Pu%20and%20Qiuhao%20Zeng%20and%20Gezheng%20Xu%20and%20Charles%20Ling%20and%20Boyu%20Wang&entry.1292438233=%20%20Graph%20Domain%20Adaptation%20%28GDA%29%20transfers%20knowledge%20from%20labeled%20source%20graphs%0Ato%20unlabeled%20target%20graphs%2C%20addressing%20the%20challenge%20of%20label%20scarcity.%20In%20this%0Apaper%2C%20we%20highlight%20the%20significance%20of%20graph%20homophily%2C%20a%20pivotal%20factor%20for%0Agraph%20domain%20alignment%2C%20which%2C%20however%2C%20has%20long%20been%20overlooked%20in%20existing%0Aapproaches.%20Specifically%2C%20our%20analysis%20first%20reveals%20that%20homophily%0Adiscrepancies%20exist%20in%20benchmarks.%20Moreover%2C%20we%20also%20show%20that%20homophily%0Adiscrepancies%20degrade%20GDA%20performance%20from%20both%20empirical%20and%20theoretical%0Aaspects%2C%20which%20further%20underscores%20the%20importance%20of%20homophily%20alignment%20in%0AGDA.%20Inspired%20by%20this%20finding%2C%20we%20propose%20a%20novel%20homophily%20alignment%20algorithm%0Athat%20employs%20mixed%20filters%20to%20smooth%20graph%20signals%2C%20thereby%20effectively%0Acapturing%20and%20mitigating%20homophily%20discrepancies%20between%20graphs.%20Experimental%0Aresults%20on%20a%20variety%20of%20benchmarks%20verify%20the%20effectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20089v1&entry.124074799=Read"},
{"title": "Exploring Consciousness in LLMs: A Systematic Survey of Theories,\n  Implementations, and Frontier Risks", "author": "Sirui Chen and Shuqin Ma and Shu Yu and Hanwang Zhang and Shengjie Zhao and Chaochao Lu", "abstract": "  Consciousness stands as one of the most profound and distinguishing features\nof the human mind, fundamentally shaping our understanding of existence and\nagency. As large language models (LLMs) develop at an unprecedented pace,\nquestions concerning intelligence and consciousness have become increasingly\nsignificant. However, discourse on LLM consciousness remains largely unexplored\nterritory. In this paper, we first clarify frequently conflated terminologies\n(e.g., LLM consciousness and LLM awareness). Then, we systematically organize\nand synthesize existing research on LLM consciousness from both theoretical and\nempirical perspectives. Furthermore, we highlight potential frontier risks that\nconscious LLMs might introduce. Finally, we discuss current challenges and\noutline future directions in this emerging field. The references discussed in\nthis paper are organized at\nhttps://github.com/OpenCausaLab/Awesome-LLM-Consciousness.\n", "link": "http://arxiv.org/abs/2505.19806v1", "date": "2025-05-26", "relevancy": 2.3972, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4816}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4816}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Consciousness%20in%20LLMs%3A%20A%20Systematic%20Survey%20of%20Theories%2C%0A%20%20Implementations%2C%20and%20Frontier%20Risks&body=Title%3A%20Exploring%20Consciousness%20in%20LLMs%3A%20A%20Systematic%20Survey%20of%20Theories%2C%0A%20%20Implementations%2C%20and%20Frontier%20Risks%0AAuthor%3A%20Sirui%20Chen%20and%20Shuqin%20Ma%20and%20Shu%20Yu%20and%20Hanwang%20Zhang%20and%20Shengjie%20Zhao%20and%20Chaochao%20Lu%0AAbstract%3A%20%20%20Consciousness%20stands%20as%20one%20of%20the%20most%20profound%20and%20distinguishing%20features%0Aof%20the%20human%20mind%2C%20fundamentally%20shaping%20our%20understanding%20of%20existence%20and%0Aagency.%20As%20large%20language%20models%20%28LLMs%29%20develop%20at%20an%20unprecedented%20pace%2C%0Aquestions%20concerning%20intelligence%20and%20consciousness%20have%20become%20increasingly%0Asignificant.%20However%2C%20discourse%20on%20LLM%20consciousness%20remains%20largely%20unexplored%0Aterritory.%20In%20this%20paper%2C%20we%20first%20clarify%20frequently%20conflated%20terminologies%0A%28e.g.%2C%20LLM%20consciousness%20and%20LLM%20awareness%29.%20Then%2C%20we%20systematically%20organize%0Aand%20synthesize%20existing%20research%20on%20LLM%20consciousness%20from%20both%20theoretical%20and%0Aempirical%20perspectives.%20Furthermore%2C%20we%20highlight%20potential%20frontier%20risks%20that%0Aconscious%20LLMs%20might%20introduce.%20Finally%2C%20we%20discuss%20current%20challenges%20and%0Aoutline%20future%20directions%20in%20this%20emerging%20field.%20The%20references%20discussed%20in%0Athis%20paper%20are%20organized%20at%0Ahttps%3A//github.com/OpenCausaLab/Awesome-LLM-Consciousness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Consciousness%2520in%2520LLMs%253A%2520A%2520Systematic%2520Survey%2520of%2520Theories%252C%250A%2520%2520Implementations%252C%2520and%2520Frontier%2520Risks%26entry.906535625%3DSirui%2520Chen%2520and%2520Shuqin%2520Ma%2520and%2520Shu%2520Yu%2520and%2520Hanwang%2520Zhang%2520and%2520Shengjie%2520Zhao%2520and%2520Chaochao%2520Lu%26entry.1292438233%3D%2520%2520Consciousness%2520stands%2520as%2520one%2520of%2520the%2520most%2520profound%2520and%2520distinguishing%2520features%250Aof%2520the%2520human%2520mind%252C%2520fundamentally%2520shaping%2520our%2520understanding%2520of%2520existence%2520and%250Aagency.%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520develop%2520at%2520an%2520unprecedented%2520pace%252C%250Aquestions%2520concerning%2520intelligence%2520and%2520consciousness%2520have%2520become%2520increasingly%250Asignificant.%2520However%252C%2520discourse%2520on%2520LLM%2520consciousness%2520remains%2520largely%2520unexplored%250Aterritory.%2520In%2520this%2520paper%252C%2520we%2520first%2520clarify%2520frequently%2520conflated%2520terminologies%250A%2528e.g.%252C%2520LLM%2520consciousness%2520and%2520LLM%2520awareness%2529.%2520Then%252C%2520we%2520systematically%2520organize%250Aand%2520synthesize%2520existing%2520research%2520on%2520LLM%2520consciousness%2520from%2520both%2520theoretical%2520and%250Aempirical%2520perspectives.%2520Furthermore%252C%2520we%2520highlight%2520potential%2520frontier%2520risks%2520that%250Aconscious%2520LLMs%2520might%2520introduce.%2520Finally%252C%2520we%2520discuss%2520current%2520challenges%2520and%250Aoutline%2520future%2520directions%2520in%2520this%2520emerging%2520field.%2520The%2520references%2520discussed%2520in%250Athis%2520paper%2520are%2520organized%2520at%250Ahttps%253A//github.com/OpenCausaLab/Awesome-LLM-Consciousness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Consciousness%20in%20LLMs%3A%20A%20Systematic%20Survey%20of%20Theories%2C%0A%20%20Implementations%2C%20and%20Frontier%20Risks&entry.906535625=Sirui%20Chen%20and%20Shuqin%20Ma%20and%20Shu%20Yu%20and%20Hanwang%20Zhang%20and%20Shengjie%20Zhao%20and%20Chaochao%20Lu&entry.1292438233=%20%20Consciousness%20stands%20as%20one%20of%20the%20most%20profound%20and%20distinguishing%20features%0Aof%20the%20human%20mind%2C%20fundamentally%20shaping%20our%20understanding%20of%20existence%20and%0Aagency.%20As%20large%20language%20models%20%28LLMs%29%20develop%20at%20an%20unprecedented%20pace%2C%0Aquestions%20concerning%20intelligence%20and%20consciousness%20have%20become%20increasingly%0Asignificant.%20However%2C%20discourse%20on%20LLM%20consciousness%20remains%20largely%20unexplored%0Aterritory.%20In%20this%20paper%2C%20we%20first%20clarify%20frequently%20conflated%20terminologies%0A%28e.g.%2C%20LLM%20consciousness%20and%20LLM%20awareness%29.%20Then%2C%20we%20systematically%20organize%0Aand%20synthesize%20existing%20research%20on%20LLM%20consciousness%20from%20both%20theoretical%20and%0Aempirical%20perspectives.%20Furthermore%2C%20we%20highlight%20potential%20frontier%20risks%20that%0Aconscious%20LLMs%20might%20introduce.%20Finally%2C%20we%20discuss%20current%20challenges%20and%0Aoutline%20future%20directions%20in%20this%20emerging%20field.%20The%20references%20discussed%20in%0Athis%20paper%20are%20organized%20at%0Ahttps%3A//github.com/OpenCausaLab/Awesome-LLM-Consciousness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19806v1&entry.124074799=Read"},
{"title": "Foundation Models for Tabular Data within Systemic Contexts Need\n  Grounding", "author": "Tassilo Klein and Johannes Hoffart", "abstract": "  Current research on tabular foundation models often overlooks the\ncomplexities of large-scale, real-world data by treating tables as isolated\nentities and assuming information completeness, thereby neglecting the vital\noperational context. To address this, we introduce the concept of Semantically\nLinked Tables (SLT), recognizing that tables are inherently connected to both\ndeclarative and procedural operational knowledge. We propose Foundation Models\nfor Semantically Linked Tables (FMSLT), which integrate these components to\nground tabular data within its true operational context. This comprehensive\nrepresentation unlocks the full potential of machine learning for complex,\ninterconnected tabular data across diverse domains. Realizing FMSLTs requires\naccess to operational knowledge that is often unavailable in public datasets,\nhighlighting the need for close collaboration between domain experts and\nresearchers. Our work exposes the limitations of current tabular foundation\nmodels and proposes a new direction centered on FMSLTs, aiming to advance\nrobust, context-aware models for structured data.\n", "link": "http://arxiv.org/abs/2505.19825v1", "date": "2025-05-26", "relevancy": 2.3943, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.503}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Models%20for%20Tabular%20Data%20within%20Systemic%20Contexts%20Need%0A%20%20Grounding&body=Title%3A%20Foundation%20Models%20for%20Tabular%20Data%20within%20Systemic%20Contexts%20Need%0A%20%20Grounding%0AAuthor%3A%20Tassilo%20Klein%20and%20Johannes%20Hoffart%0AAbstract%3A%20%20%20Current%20research%20on%20tabular%20foundation%20models%20often%20overlooks%20the%0Acomplexities%20of%20large-scale%2C%20real-world%20data%20by%20treating%20tables%20as%20isolated%0Aentities%20and%20assuming%20information%20completeness%2C%20thereby%20neglecting%20the%20vital%0Aoperational%20context.%20To%20address%20this%2C%20we%20introduce%20the%20concept%20of%20Semantically%0ALinked%20Tables%20%28SLT%29%2C%20recognizing%20that%20tables%20are%20inherently%20connected%20to%20both%0Adeclarative%20and%20procedural%20operational%20knowledge.%20We%20propose%20Foundation%20Models%0Afor%20Semantically%20Linked%20Tables%20%28FMSLT%29%2C%20which%20integrate%20these%20components%20to%0Aground%20tabular%20data%20within%20its%20true%20operational%20context.%20This%20comprehensive%0Arepresentation%20unlocks%20the%20full%20potential%20of%20machine%20learning%20for%20complex%2C%0Ainterconnected%20tabular%20data%20across%20diverse%20domains.%20Realizing%20FMSLTs%20requires%0Aaccess%20to%20operational%20knowledge%20that%20is%20often%20unavailable%20in%20public%20datasets%2C%0Ahighlighting%20the%20need%20for%20close%20collaboration%20between%20domain%20experts%20and%0Aresearchers.%20Our%20work%20exposes%20the%20limitations%20of%20current%20tabular%20foundation%0Amodels%20and%20proposes%20a%20new%20direction%20centered%20on%20FMSLTs%2C%20aiming%20to%20advance%0Arobust%2C%20context-aware%20models%20for%20structured%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Models%2520for%2520Tabular%2520Data%2520within%2520Systemic%2520Contexts%2520Need%250A%2520%2520Grounding%26entry.906535625%3DTassilo%2520Klein%2520and%2520Johannes%2520Hoffart%26entry.1292438233%3D%2520%2520Current%2520research%2520on%2520tabular%2520foundation%2520models%2520often%2520overlooks%2520the%250Acomplexities%2520of%2520large-scale%252C%2520real-world%2520data%2520by%2520treating%2520tables%2520as%2520isolated%250Aentities%2520and%2520assuming%2520information%2520completeness%252C%2520thereby%2520neglecting%2520the%2520vital%250Aoperational%2520context.%2520To%2520address%2520this%252C%2520we%2520introduce%2520the%2520concept%2520of%2520Semantically%250ALinked%2520Tables%2520%2528SLT%2529%252C%2520recognizing%2520that%2520tables%2520are%2520inherently%2520connected%2520to%2520both%250Adeclarative%2520and%2520procedural%2520operational%2520knowledge.%2520We%2520propose%2520Foundation%2520Models%250Afor%2520Semantically%2520Linked%2520Tables%2520%2528FMSLT%2529%252C%2520which%2520integrate%2520these%2520components%2520to%250Aground%2520tabular%2520data%2520within%2520its%2520true%2520operational%2520context.%2520This%2520comprehensive%250Arepresentation%2520unlocks%2520the%2520full%2520potential%2520of%2520machine%2520learning%2520for%2520complex%252C%250Ainterconnected%2520tabular%2520data%2520across%2520diverse%2520domains.%2520Realizing%2520FMSLTs%2520requires%250Aaccess%2520to%2520operational%2520knowledge%2520that%2520is%2520often%2520unavailable%2520in%2520public%2520datasets%252C%250Ahighlighting%2520the%2520need%2520for%2520close%2520collaboration%2520between%2520domain%2520experts%2520and%250Aresearchers.%2520Our%2520work%2520exposes%2520the%2520limitations%2520of%2520current%2520tabular%2520foundation%250Amodels%2520and%2520proposes%2520a%2520new%2520direction%2520centered%2520on%2520FMSLTs%252C%2520aiming%2520to%2520advance%250Arobust%252C%2520context-aware%2520models%2520for%2520structured%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Models%20for%20Tabular%20Data%20within%20Systemic%20Contexts%20Need%0A%20%20Grounding&entry.906535625=Tassilo%20Klein%20and%20Johannes%20Hoffart&entry.1292438233=%20%20Current%20research%20on%20tabular%20foundation%20models%20often%20overlooks%20the%0Acomplexities%20of%20large-scale%2C%20real-world%20data%20by%20treating%20tables%20as%20isolated%0Aentities%20and%20assuming%20information%20completeness%2C%20thereby%20neglecting%20the%20vital%0Aoperational%20context.%20To%20address%20this%2C%20we%20introduce%20the%20concept%20of%20Semantically%0ALinked%20Tables%20%28SLT%29%2C%20recognizing%20that%20tables%20are%20inherently%20connected%20to%20both%0Adeclarative%20and%20procedural%20operational%20knowledge.%20We%20propose%20Foundation%20Models%0Afor%20Semantically%20Linked%20Tables%20%28FMSLT%29%2C%20which%20integrate%20these%20components%20to%0Aground%20tabular%20data%20within%20its%20true%20operational%20context.%20This%20comprehensive%0Arepresentation%20unlocks%20the%20full%20potential%20of%20machine%20learning%20for%20complex%2C%0Ainterconnected%20tabular%20data%20across%20diverse%20domains.%20Realizing%20FMSLTs%20requires%0Aaccess%20to%20operational%20knowledge%20that%20is%20often%20unavailable%20in%20public%20datasets%2C%0Ahighlighting%20the%20need%20for%20close%20collaboration%20between%20domain%20experts%20and%0Aresearchers.%20Our%20work%20exposes%20the%20limitations%20of%20current%20tabular%20foundation%0Amodels%20and%20proposes%20a%20new%20direction%20centered%20on%20FMSLTs%2C%20aiming%20to%20advance%0Arobust%2C%20context-aware%20models%20for%20structured%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19825v1&entry.124074799=Read"},
{"title": "ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in\n  Multimodal Transformers", "author": "Fotios Lygerakis and Ozan \u00d6zdenizci and Elmar R\u00fcckert", "abstract": "  Tactile sensing provides local essential information that is complementary to\nvisual perception, such as texture, compliance, and force. Despite recent\nadvances in visuotactile representation learning, challenges remain in fusing\nthese modalities and generalizing across tasks and environments without heavy\nreliance on pre-trained vision-language models. Moreover, existing methods do\nnot study positional encodings, thereby overlooking the multi-scale spatial\nreasoning needed to capture fine-grained visuotactile correlations. We\nintroduce ViTaPEs, a transformer-based framework that robustly integrates\nvisual and tactile input data to learn task-agnostic representations for\nvisuotactile perception. Our approach exploits a novel multi-scale positional\nencoding scheme to capture intra-modal structures, while simultaneously\nmodeling cross-modal cues. Unlike prior work, we provide provable guarantees in\nvisuotactile fusion, showing that our encodings are injective,\nrigid-motion-equivariant, and information-preserving, validating these\nproperties empirically. Experiments on multiple large-scale real-world datasets\nshow that ViTaPEs not only surpasses state-of-the-art baselines across various\nrecognition tasks but also demonstrates zero-shot generalization to unseen,\nout-of-domain scenarios. We further demonstrate the transfer-learning strength\nof ViTaPEs in a robotic grasping task, where it outperforms state-of-the-art\nbaselines in predicting grasp success. Project page:\nhttps://sites.google.com/view/vitapes\n", "link": "http://arxiv.org/abs/2505.20032v1", "date": "2025-05-26", "relevancy": 2.3881, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6649}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5832}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViTaPEs%3A%20Visuotactile%20Position%20Encodings%20for%20Cross-Modal%20Alignment%20in%0A%20%20Multimodal%20Transformers&body=Title%3A%20ViTaPEs%3A%20Visuotactile%20Position%20Encodings%20for%20Cross-Modal%20Alignment%20in%0A%20%20Multimodal%20Transformers%0AAuthor%3A%20Fotios%20Lygerakis%20and%20Ozan%20%C3%96zdenizci%20and%20Elmar%20R%C3%BCckert%0AAbstract%3A%20%20%20Tactile%20sensing%20provides%20local%20essential%20information%20that%20is%20complementary%20to%0Avisual%20perception%2C%20such%20as%20texture%2C%20compliance%2C%20and%20force.%20Despite%20recent%0Aadvances%20in%20visuotactile%20representation%20learning%2C%20challenges%20remain%20in%20fusing%0Athese%20modalities%20and%20generalizing%20across%20tasks%20and%20environments%20without%20heavy%0Areliance%20on%20pre-trained%20vision-language%20models.%20Moreover%2C%20existing%20methods%20do%0Anot%20study%20positional%20encodings%2C%20thereby%20overlooking%20the%20multi-scale%20spatial%0Areasoning%20needed%20to%20capture%20fine-grained%20visuotactile%20correlations.%20We%0Aintroduce%20ViTaPEs%2C%20a%20transformer-based%20framework%20that%20robustly%20integrates%0Avisual%20and%20tactile%20input%20data%20to%20learn%20task-agnostic%20representations%20for%0Avisuotactile%20perception.%20Our%20approach%20exploits%20a%20novel%20multi-scale%20positional%0Aencoding%20scheme%20to%20capture%20intra-modal%20structures%2C%20while%20simultaneously%0Amodeling%20cross-modal%20cues.%20Unlike%20prior%20work%2C%20we%20provide%20provable%20guarantees%20in%0Avisuotactile%20fusion%2C%20showing%20that%20our%20encodings%20are%20injective%2C%0Arigid-motion-equivariant%2C%20and%20information-preserving%2C%20validating%20these%0Aproperties%20empirically.%20Experiments%20on%20multiple%20large-scale%20real-world%20datasets%0Ashow%20that%20ViTaPEs%20not%20only%20surpasses%20state-of-the-art%20baselines%20across%20various%0Arecognition%20tasks%20but%20also%20demonstrates%20zero-shot%20generalization%20to%20unseen%2C%0Aout-of-domain%20scenarios.%20We%20further%20demonstrate%20the%20transfer-learning%20strength%0Aof%20ViTaPEs%20in%20a%20robotic%20grasping%20task%2C%20where%20it%20outperforms%20state-of-the-art%0Abaselines%20in%20predicting%20grasp%20success.%20Project%20page%3A%0Ahttps%3A//sites.google.com/view/vitapes%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViTaPEs%253A%2520Visuotactile%2520Position%2520Encodings%2520for%2520Cross-Modal%2520Alignment%2520in%250A%2520%2520Multimodal%2520Transformers%26entry.906535625%3DFotios%2520Lygerakis%2520and%2520Ozan%2520%25C3%2596zdenizci%2520and%2520Elmar%2520R%25C3%25BCckert%26entry.1292438233%3D%2520%2520Tactile%2520sensing%2520provides%2520local%2520essential%2520information%2520that%2520is%2520complementary%2520to%250Avisual%2520perception%252C%2520such%2520as%2520texture%252C%2520compliance%252C%2520and%2520force.%2520Despite%2520recent%250Aadvances%2520in%2520visuotactile%2520representation%2520learning%252C%2520challenges%2520remain%2520in%2520fusing%250Athese%2520modalities%2520and%2520generalizing%2520across%2520tasks%2520and%2520environments%2520without%2520heavy%250Areliance%2520on%2520pre-trained%2520vision-language%2520models.%2520Moreover%252C%2520existing%2520methods%2520do%250Anot%2520study%2520positional%2520encodings%252C%2520thereby%2520overlooking%2520the%2520multi-scale%2520spatial%250Areasoning%2520needed%2520to%2520capture%2520fine-grained%2520visuotactile%2520correlations.%2520We%250Aintroduce%2520ViTaPEs%252C%2520a%2520transformer-based%2520framework%2520that%2520robustly%2520integrates%250Avisual%2520and%2520tactile%2520input%2520data%2520to%2520learn%2520task-agnostic%2520representations%2520for%250Avisuotactile%2520perception.%2520Our%2520approach%2520exploits%2520a%2520novel%2520multi-scale%2520positional%250Aencoding%2520scheme%2520to%2520capture%2520intra-modal%2520structures%252C%2520while%2520simultaneously%250Amodeling%2520cross-modal%2520cues.%2520Unlike%2520prior%2520work%252C%2520we%2520provide%2520provable%2520guarantees%2520in%250Avisuotactile%2520fusion%252C%2520showing%2520that%2520our%2520encodings%2520are%2520injective%252C%250Arigid-motion-equivariant%252C%2520and%2520information-preserving%252C%2520validating%2520these%250Aproperties%2520empirically.%2520Experiments%2520on%2520multiple%2520large-scale%2520real-world%2520datasets%250Ashow%2520that%2520ViTaPEs%2520not%2520only%2520surpasses%2520state-of-the-art%2520baselines%2520across%2520various%250Arecognition%2520tasks%2520but%2520also%2520demonstrates%2520zero-shot%2520generalization%2520to%2520unseen%252C%250Aout-of-domain%2520scenarios.%2520We%2520further%2520demonstrate%2520the%2520transfer-learning%2520strength%250Aof%2520ViTaPEs%2520in%2520a%2520robotic%2520grasping%2520task%252C%2520where%2520it%2520outperforms%2520state-of-the-art%250Abaselines%2520in%2520predicting%2520grasp%2520success.%2520Project%2520page%253A%250Ahttps%253A//sites.google.com/view/vitapes%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViTaPEs%3A%20Visuotactile%20Position%20Encodings%20for%20Cross-Modal%20Alignment%20in%0A%20%20Multimodal%20Transformers&entry.906535625=Fotios%20Lygerakis%20and%20Ozan%20%C3%96zdenizci%20and%20Elmar%20R%C3%BCckert&entry.1292438233=%20%20Tactile%20sensing%20provides%20local%20essential%20information%20that%20is%20complementary%20to%0Avisual%20perception%2C%20such%20as%20texture%2C%20compliance%2C%20and%20force.%20Despite%20recent%0Aadvances%20in%20visuotactile%20representation%20learning%2C%20challenges%20remain%20in%20fusing%0Athese%20modalities%20and%20generalizing%20across%20tasks%20and%20environments%20without%20heavy%0Areliance%20on%20pre-trained%20vision-language%20models.%20Moreover%2C%20existing%20methods%20do%0Anot%20study%20positional%20encodings%2C%20thereby%20overlooking%20the%20multi-scale%20spatial%0Areasoning%20needed%20to%20capture%20fine-grained%20visuotactile%20correlations.%20We%0Aintroduce%20ViTaPEs%2C%20a%20transformer-based%20framework%20that%20robustly%20integrates%0Avisual%20and%20tactile%20input%20data%20to%20learn%20task-agnostic%20representations%20for%0Avisuotactile%20perception.%20Our%20approach%20exploits%20a%20novel%20multi-scale%20positional%0Aencoding%20scheme%20to%20capture%20intra-modal%20structures%2C%20while%20simultaneously%0Amodeling%20cross-modal%20cues.%20Unlike%20prior%20work%2C%20we%20provide%20provable%20guarantees%20in%0Avisuotactile%20fusion%2C%20showing%20that%20our%20encodings%20are%20injective%2C%0Arigid-motion-equivariant%2C%20and%20information-preserving%2C%20validating%20these%0Aproperties%20empirically.%20Experiments%20on%20multiple%20large-scale%20real-world%20datasets%0Ashow%20that%20ViTaPEs%20not%20only%20surpasses%20state-of-the-art%20baselines%20across%20various%0Arecognition%20tasks%20but%20also%20demonstrates%20zero-shot%20generalization%20to%20unseen%2C%0Aout-of-domain%20scenarios.%20We%20further%20demonstrate%20the%20transfer-learning%20strength%0Aof%20ViTaPEs%20in%20a%20robotic%20grasping%20task%2C%20where%20it%20outperforms%20state-of-the-art%0Abaselines%20in%20predicting%20grasp%20success.%20Project%20page%3A%0Ahttps%3A//sites.google.com/view/vitapes%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20032v1&entry.124074799=Read"},
{"title": "What Makes a Scene ? Scene Graph-based Evaluation and Feedback for\n  Controllable Generation", "author": "Zuyao Chen and Jinlin Wu and Zhen Lei and Chang Wen Chen", "abstract": "  While text-to-image generation has been extensively studied, generating\nimages from scene graphs remains relatively underexplored, primarily due to\nchallenges in accurately modeling spatial relationships and object\ninteractions. To fill this gap, we introduce Scene-Bench, a comprehensive\nbenchmark designed to evaluate and enhance the factual consistency in\ngenerating natural scenes. Scene-Bench comprises MegaSG, a large-scale dataset\nof one million images annotated with scene graphs, facilitating the training\nand fair comparison of models across diverse and complex scenes. Additionally,\nwe propose SGScore, a novel evaluation metric that leverages chain-of-thought\nreasoning capabilities of multimodal large language models (LLMs) to assess\nboth object presence and relationship accuracy, offering a more effective\nmeasure of factual consistency than traditional metrics like FID and CLIPScore.\nBuilding upon this evaluation framework, we develop a scene graph feedback\npipeline that iteratively refines generated images by identifying and\ncorrecting discrepancies between the scene graph and the image. Extensive\nexperiments demonstrate that Scene-Bench provides a more comprehensive and\neffective evaluation framework compared to existing benchmarks, particularly\nfor complex scene generation. Furthermore, our feedback strategy significantly\nenhances the factual consistency of image generation models, advancing the\nfield of controllable image generation.\n", "link": "http://arxiv.org/abs/2411.15435v2", "date": "2025-05-26", "relevancy": 2.3815, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6269}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5891}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Makes%20a%20Scene%20%3F%20Scene%20Graph-based%20Evaluation%20and%20Feedback%20for%0A%20%20Controllable%20Generation&body=Title%3A%20What%20Makes%20a%20Scene%20%3F%20Scene%20Graph-based%20Evaluation%20and%20Feedback%20for%0A%20%20Controllable%20Generation%0AAuthor%3A%20Zuyao%20Chen%20and%20Jinlin%20Wu%20and%20Zhen%20Lei%20and%20Chang%20Wen%20Chen%0AAbstract%3A%20%20%20While%20text-to-image%20generation%20has%20been%20extensively%20studied%2C%20generating%0Aimages%20from%20scene%20graphs%20remains%20relatively%20underexplored%2C%20primarily%20due%20to%0Achallenges%20in%20accurately%20modeling%20spatial%20relationships%20and%20object%0Ainteractions.%20To%20fill%20this%20gap%2C%20we%20introduce%20Scene-Bench%2C%20a%20comprehensive%0Abenchmark%20designed%20to%20evaluate%20and%20enhance%20the%20factual%20consistency%20in%0Agenerating%20natural%20scenes.%20Scene-Bench%20comprises%20MegaSG%2C%20a%20large-scale%20dataset%0Aof%20one%20million%20images%20annotated%20with%20scene%20graphs%2C%20facilitating%20the%20training%0Aand%20fair%20comparison%20of%20models%20across%20diverse%20and%20complex%20scenes.%20Additionally%2C%0Awe%20propose%20SGScore%2C%20a%20novel%20evaluation%20metric%20that%20leverages%20chain-of-thought%0Areasoning%20capabilities%20of%20multimodal%20large%20language%20models%20%28LLMs%29%20to%20assess%0Aboth%20object%20presence%20and%20relationship%20accuracy%2C%20offering%20a%20more%20effective%0Ameasure%20of%20factual%20consistency%20than%20traditional%20metrics%20like%20FID%20and%20CLIPScore.%0ABuilding%20upon%20this%20evaluation%20framework%2C%20we%20develop%20a%20scene%20graph%20feedback%0Apipeline%20that%20iteratively%20refines%20generated%20images%20by%20identifying%20and%0Acorrecting%20discrepancies%20between%20the%20scene%20graph%20and%20the%20image.%20Extensive%0Aexperiments%20demonstrate%20that%20Scene-Bench%20provides%20a%20more%20comprehensive%20and%0Aeffective%20evaluation%20framework%20compared%20to%20existing%20benchmarks%2C%20particularly%0Afor%20complex%20scene%20generation.%20Furthermore%2C%20our%20feedback%20strategy%20significantly%0Aenhances%20the%20factual%20consistency%20of%20image%20generation%20models%2C%20advancing%20the%0Afield%20of%20controllable%20image%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15435v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Makes%2520a%2520Scene%2520%253F%2520Scene%2520Graph-based%2520Evaluation%2520and%2520Feedback%2520for%250A%2520%2520Controllable%2520Generation%26entry.906535625%3DZuyao%2520Chen%2520and%2520Jinlin%2520Wu%2520and%2520Zhen%2520Lei%2520and%2520Chang%2520Wen%2520Chen%26entry.1292438233%3D%2520%2520While%2520text-to-image%2520generation%2520has%2520been%2520extensively%2520studied%252C%2520generating%250Aimages%2520from%2520scene%2520graphs%2520remains%2520relatively%2520underexplored%252C%2520primarily%2520due%2520to%250Achallenges%2520in%2520accurately%2520modeling%2520spatial%2520relationships%2520and%2520object%250Ainteractions.%2520To%2520fill%2520this%2520gap%252C%2520we%2520introduce%2520Scene-Bench%252C%2520a%2520comprehensive%250Abenchmark%2520designed%2520to%2520evaluate%2520and%2520enhance%2520the%2520factual%2520consistency%2520in%250Agenerating%2520natural%2520scenes.%2520Scene-Bench%2520comprises%2520MegaSG%252C%2520a%2520large-scale%2520dataset%250Aof%2520one%2520million%2520images%2520annotated%2520with%2520scene%2520graphs%252C%2520facilitating%2520the%2520training%250Aand%2520fair%2520comparison%2520of%2520models%2520across%2520diverse%2520and%2520complex%2520scenes.%2520Additionally%252C%250Awe%2520propose%2520SGScore%252C%2520a%2520novel%2520evaluation%2520metric%2520that%2520leverages%2520chain-of-thought%250Areasoning%2520capabilities%2520of%2520multimodal%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520assess%250Aboth%2520object%2520presence%2520and%2520relationship%2520accuracy%252C%2520offering%2520a%2520more%2520effective%250Ameasure%2520of%2520factual%2520consistency%2520than%2520traditional%2520metrics%2520like%2520FID%2520and%2520CLIPScore.%250ABuilding%2520upon%2520this%2520evaluation%2520framework%252C%2520we%2520develop%2520a%2520scene%2520graph%2520feedback%250Apipeline%2520that%2520iteratively%2520refines%2520generated%2520images%2520by%2520identifying%2520and%250Acorrecting%2520discrepancies%2520between%2520the%2520scene%2520graph%2520and%2520the%2520image.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520Scene-Bench%2520provides%2520a%2520more%2520comprehensive%2520and%250Aeffective%2520evaluation%2520framework%2520compared%2520to%2520existing%2520benchmarks%252C%2520particularly%250Afor%2520complex%2520scene%2520generation.%2520Furthermore%252C%2520our%2520feedback%2520strategy%2520significantly%250Aenhances%2520the%2520factual%2520consistency%2520of%2520image%2520generation%2520models%252C%2520advancing%2520the%250Afield%2520of%2520controllable%2520image%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15435v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Makes%20a%20Scene%20%3F%20Scene%20Graph-based%20Evaluation%20and%20Feedback%20for%0A%20%20Controllable%20Generation&entry.906535625=Zuyao%20Chen%20and%20Jinlin%20Wu%20and%20Zhen%20Lei%20and%20Chang%20Wen%20Chen&entry.1292438233=%20%20While%20text-to-image%20generation%20has%20been%20extensively%20studied%2C%20generating%0Aimages%20from%20scene%20graphs%20remains%20relatively%20underexplored%2C%20primarily%20due%20to%0Achallenges%20in%20accurately%20modeling%20spatial%20relationships%20and%20object%0Ainteractions.%20To%20fill%20this%20gap%2C%20we%20introduce%20Scene-Bench%2C%20a%20comprehensive%0Abenchmark%20designed%20to%20evaluate%20and%20enhance%20the%20factual%20consistency%20in%0Agenerating%20natural%20scenes.%20Scene-Bench%20comprises%20MegaSG%2C%20a%20large-scale%20dataset%0Aof%20one%20million%20images%20annotated%20with%20scene%20graphs%2C%20facilitating%20the%20training%0Aand%20fair%20comparison%20of%20models%20across%20diverse%20and%20complex%20scenes.%20Additionally%2C%0Awe%20propose%20SGScore%2C%20a%20novel%20evaluation%20metric%20that%20leverages%20chain-of-thought%0Areasoning%20capabilities%20of%20multimodal%20large%20language%20models%20%28LLMs%29%20to%20assess%0Aboth%20object%20presence%20and%20relationship%20accuracy%2C%20offering%20a%20more%20effective%0Ameasure%20of%20factual%20consistency%20than%20traditional%20metrics%20like%20FID%20and%20CLIPScore.%0ABuilding%20upon%20this%20evaluation%20framework%2C%20we%20develop%20a%20scene%20graph%20feedback%0Apipeline%20that%20iteratively%20refines%20generated%20images%20by%20identifying%20and%0Acorrecting%20discrepancies%20between%20the%20scene%20graph%20and%20the%20image.%20Extensive%0Aexperiments%20demonstrate%20that%20Scene-Bench%20provides%20a%20more%20comprehensive%20and%0Aeffective%20evaluation%20framework%20compared%20to%20existing%20benchmarks%2C%20particularly%0Afor%20complex%20scene%20generation.%20Furthermore%2C%20our%20feedback%20strategy%20significantly%0Aenhances%20the%20factual%20consistency%20of%20image%20generation%20models%2C%20advancing%20the%0Afield%20of%20controllable%20image%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15435v2&entry.124074799=Read"},
{"title": "MEBench: A Novel Benchmark for Understanding Mutual Exclusivity Bias in\n  Vision-Language Models", "author": "Anh Thai and Stefan Stojanov and Zixuan Huang and Bikram Boote and James M. Rehg", "abstract": "  This paper introduces MEBench, a novel benchmark for evaluating mutual\nexclusivity (ME) bias, a cognitive phenomenon observed in children during word\nlearning. Unlike traditional ME tasks, MEBench further incorporates spatial\nreasoning to create more challenging and realistic evaluation settings. We\nassess the performance of state-of-the-art vision-language models (VLMs) on\nthis benchmark using novel evaluation metrics that capture key aspects of\nME-based reasoning. To facilitate controlled experimentation, we also present a\nflexible and scalable data generation pipeline that supports the construction\nof diverse annotated scenes.\n", "link": "http://arxiv.org/abs/2505.20122v1", "date": "2025-05-26", "relevancy": 2.3756, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6026}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6026}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEBench%3A%20A%20Novel%20Benchmark%20for%20Understanding%20Mutual%20Exclusivity%20Bias%20in%0A%20%20Vision-Language%20Models&body=Title%3A%20MEBench%3A%20A%20Novel%20Benchmark%20for%20Understanding%20Mutual%20Exclusivity%20Bias%20in%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Anh%20Thai%20and%20Stefan%20Stojanov%20and%20Zixuan%20Huang%20and%20Bikram%20Boote%20and%20James%20M.%20Rehg%0AAbstract%3A%20%20%20This%20paper%20introduces%20MEBench%2C%20a%20novel%20benchmark%20for%20evaluating%20mutual%0Aexclusivity%20%28ME%29%20bias%2C%20a%20cognitive%20phenomenon%20observed%20in%20children%20during%20word%0Alearning.%20Unlike%20traditional%20ME%20tasks%2C%20MEBench%20further%20incorporates%20spatial%0Areasoning%20to%20create%20more%20challenging%20and%20realistic%20evaluation%20settings.%20We%0Aassess%20the%20performance%20of%20state-of-the-art%20vision-language%20models%20%28VLMs%29%20on%0Athis%20benchmark%20using%20novel%20evaluation%20metrics%20that%20capture%20key%20aspects%20of%0AME-based%20reasoning.%20To%20facilitate%20controlled%20experimentation%2C%20we%20also%20present%20a%0Aflexible%20and%20scalable%20data%20generation%20pipeline%20that%20supports%20the%20construction%0Aof%20diverse%20annotated%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEBench%253A%2520A%2520Novel%2520Benchmark%2520for%2520Understanding%2520Mutual%2520Exclusivity%2520Bias%2520in%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DAnh%2520Thai%2520and%2520Stefan%2520Stojanov%2520and%2520Zixuan%2520Huang%2520and%2520Bikram%2520Boote%2520and%2520James%2520M.%2520Rehg%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520MEBench%252C%2520a%2520novel%2520benchmark%2520for%2520evaluating%2520mutual%250Aexclusivity%2520%2528ME%2529%2520bias%252C%2520a%2520cognitive%2520phenomenon%2520observed%2520in%2520children%2520during%2520word%250Alearning.%2520Unlike%2520traditional%2520ME%2520tasks%252C%2520MEBench%2520further%2520incorporates%2520spatial%250Areasoning%2520to%2520create%2520more%2520challenging%2520and%2520realistic%2520evaluation%2520settings.%2520We%250Aassess%2520the%2520performance%2520of%2520state-of-the-art%2520vision-language%2520models%2520%2528VLMs%2529%2520on%250Athis%2520benchmark%2520using%2520novel%2520evaluation%2520metrics%2520that%2520capture%2520key%2520aspects%2520of%250AME-based%2520reasoning.%2520To%2520facilitate%2520controlled%2520experimentation%252C%2520we%2520also%2520present%2520a%250Aflexible%2520and%2520scalable%2520data%2520generation%2520pipeline%2520that%2520supports%2520the%2520construction%250Aof%2520diverse%2520annotated%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEBench%3A%20A%20Novel%20Benchmark%20for%20Understanding%20Mutual%20Exclusivity%20Bias%20in%0A%20%20Vision-Language%20Models&entry.906535625=Anh%20Thai%20and%20Stefan%20Stojanov%20and%20Zixuan%20Huang%20and%20Bikram%20Boote%20and%20James%20M.%20Rehg&entry.1292438233=%20%20This%20paper%20introduces%20MEBench%2C%20a%20novel%20benchmark%20for%20evaluating%20mutual%0Aexclusivity%20%28ME%29%20bias%2C%20a%20cognitive%20phenomenon%20observed%20in%20children%20during%20word%0Alearning.%20Unlike%20traditional%20ME%20tasks%2C%20MEBench%20further%20incorporates%20spatial%0Areasoning%20to%20create%20more%20challenging%20and%20realistic%20evaluation%20settings.%20We%0Aassess%20the%20performance%20of%20state-of-the-art%20vision-language%20models%20%28VLMs%29%20on%0Athis%20benchmark%20using%20novel%20evaluation%20metrics%20that%20capture%20key%20aspects%20of%0AME-based%20reasoning.%20To%20facilitate%20controlled%20experimentation%2C%20we%20also%20present%20a%0Aflexible%20and%20scalable%20data%20generation%20pipeline%20that%20supports%20the%20construction%0Aof%20diverse%20annotated%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20122v1&entry.124074799=Read"},
{"title": "PAMD: Plausibility-Aware Motion Diffusion Model for Long Dance\n  Generation", "author": "Hongsong Wang and Yin Zhu and Qiuxia Lai and Yang Zhang and Guo-Sen Xie and Xin Geng", "abstract": "  Computational dance generation is crucial in many areas, such as art,\nhuman-computer interaction, virtual reality, and digital entertainment,\nparticularly for generating coherent and expressive long dance sequences.\nDiffusion-based music-to-dance generation has made significant progress, yet\nexisting methods still struggle to produce physically plausible motions. To\naddress this, we propose Plausibility-Aware Motion Diffusion (PAMD), a\nframework for generating dances that are both musically aligned and physically\nrealistic. The core of PAMD lies in the Plausible Motion Constraint (PMC),\nwhich leverages Neural Distance Fields (NDFs) to model the actual pose manifold\nand guide generated motions toward a physically valid pose manifold. To provide\nmore effective guidance during generation, we incorporate Prior Motion Guidance\n(PMG), which uses standing poses as auxiliary conditions alongside music\nfeatures. To further enhance realism for complex movements, we introduce the\nMotion Refinement with Foot-ground Contact (MRFC) module, which addresses\nfoot-skating artifacts by bridging the gap between the optimization objective\nin linear joint position space and the data representation in nonlinear\nrotation space. Extensive experiments show that PAMD significantly improves\nmusical alignment and enhances the physical plausibility of generated motions.\nThis project page is available at: https://mucunzhuzhu.github.io/PAMD-page/.\n", "link": "http://arxiv.org/abs/2505.20056v1", "date": "2025-05-26", "relevancy": 2.3718, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6508}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5572}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAMD%3A%20Plausibility-Aware%20Motion%20Diffusion%20Model%20for%20Long%20Dance%0A%20%20Generation&body=Title%3A%20PAMD%3A%20Plausibility-Aware%20Motion%20Diffusion%20Model%20for%20Long%20Dance%0A%20%20Generation%0AAuthor%3A%20Hongsong%20Wang%20and%20Yin%20Zhu%20and%20Qiuxia%20Lai%20and%20Yang%20Zhang%20and%20Guo-Sen%20Xie%20and%20Xin%20Geng%0AAbstract%3A%20%20%20Computational%20dance%20generation%20is%20crucial%20in%20many%20areas%2C%20such%20as%20art%2C%0Ahuman-computer%20interaction%2C%20virtual%20reality%2C%20and%20digital%20entertainment%2C%0Aparticularly%20for%20generating%20coherent%20and%20expressive%20long%20dance%20sequences.%0ADiffusion-based%20music-to-dance%20generation%20has%20made%20significant%20progress%2C%20yet%0Aexisting%20methods%20still%20struggle%20to%20produce%20physically%20plausible%20motions.%20To%0Aaddress%20this%2C%20we%20propose%20Plausibility-Aware%20Motion%20Diffusion%20%28PAMD%29%2C%20a%0Aframework%20for%20generating%20dances%20that%20are%20both%20musically%20aligned%20and%20physically%0Arealistic.%20The%20core%20of%20PAMD%20lies%20in%20the%20Plausible%20Motion%20Constraint%20%28PMC%29%2C%0Awhich%20leverages%20Neural%20Distance%20Fields%20%28NDFs%29%20to%20model%20the%20actual%20pose%20manifold%0Aand%20guide%20generated%20motions%20toward%20a%20physically%20valid%20pose%20manifold.%20To%20provide%0Amore%20effective%20guidance%20during%20generation%2C%20we%20incorporate%20Prior%20Motion%20Guidance%0A%28PMG%29%2C%20which%20uses%20standing%20poses%20as%20auxiliary%20conditions%20alongside%20music%0Afeatures.%20To%20further%20enhance%20realism%20for%20complex%20movements%2C%20we%20introduce%20the%0AMotion%20Refinement%20with%20Foot-ground%20Contact%20%28MRFC%29%20module%2C%20which%20addresses%0Afoot-skating%20artifacts%20by%20bridging%20the%20gap%20between%20the%20optimization%20objective%0Ain%20linear%20joint%20position%20space%20and%20the%20data%20representation%20in%20nonlinear%0Arotation%20space.%20Extensive%20experiments%20show%20that%20PAMD%20significantly%20improves%0Amusical%20alignment%20and%20enhances%20the%20physical%20plausibility%20of%20generated%20motions.%0AThis%20project%20page%20is%20available%20at%3A%20https%3A//mucunzhuzhu.github.io/PAMD-page/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20056v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAMD%253A%2520Plausibility-Aware%2520Motion%2520Diffusion%2520Model%2520for%2520Long%2520Dance%250A%2520%2520Generation%26entry.906535625%3DHongsong%2520Wang%2520and%2520Yin%2520Zhu%2520and%2520Qiuxia%2520Lai%2520and%2520Yang%2520Zhang%2520and%2520Guo-Sen%2520Xie%2520and%2520Xin%2520Geng%26entry.1292438233%3D%2520%2520Computational%2520dance%2520generation%2520is%2520crucial%2520in%2520many%2520areas%252C%2520such%2520as%2520art%252C%250Ahuman-computer%2520interaction%252C%2520virtual%2520reality%252C%2520and%2520digital%2520entertainment%252C%250Aparticularly%2520for%2520generating%2520coherent%2520and%2520expressive%2520long%2520dance%2520sequences.%250ADiffusion-based%2520music-to-dance%2520generation%2520has%2520made%2520significant%2520progress%252C%2520yet%250Aexisting%2520methods%2520still%2520struggle%2520to%2520produce%2520physically%2520plausible%2520motions.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520Plausibility-Aware%2520Motion%2520Diffusion%2520%2528PAMD%2529%252C%2520a%250Aframework%2520for%2520generating%2520dances%2520that%2520are%2520both%2520musically%2520aligned%2520and%2520physically%250Arealistic.%2520The%2520core%2520of%2520PAMD%2520lies%2520in%2520the%2520Plausible%2520Motion%2520Constraint%2520%2528PMC%2529%252C%250Awhich%2520leverages%2520Neural%2520Distance%2520Fields%2520%2528NDFs%2529%2520to%2520model%2520the%2520actual%2520pose%2520manifold%250Aand%2520guide%2520generated%2520motions%2520toward%2520a%2520physically%2520valid%2520pose%2520manifold.%2520To%2520provide%250Amore%2520effective%2520guidance%2520during%2520generation%252C%2520we%2520incorporate%2520Prior%2520Motion%2520Guidance%250A%2528PMG%2529%252C%2520which%2520uses%2520standing%2520poses%2520as%2520auxiliary%2520conditions%2520alongside%2520music%250Afeatures.%2520To%2520further%2520enhance%2520realism%2520for%2520complex%2520movements%252C%2520we%2520introduce%2520the%250AMotion%2520Refinement%2520with%2520Foot-ground%2520Contact%2520%2528MRFC%2529%2520module%252C%2520which%2520addresses%250Afoot-skating%2520artifacts%2520by%2520bridging%2520the%2520gap%2520between%2520the%2520optimization%2520objective%250Ain%2520linear%2520joint%2520position%2520space%2520and%2520the%2520data%2520representation%2520in%2520nonlinear%250Arotation%2520space.%2520Extensive%2520experiments%2520show%2520that%2520PAMD%2520significantly%2520improves%250Amusical%2520alignment%2520and%2520enhances%2520the%2520physical%2520plausibility%2520of%2520generated%2520motions.%250AThis%2520project%2520page%2520is%2520available%2520at%253A%2520https%253A//mucunzhuzhu.github.io/PAMD-page/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20056v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAMD%3A%20Plausibility-Aware%20Motion%20Diffusion%20Model%20for%20Long%20Dance%0A%20%20Generation&entry.906535625=Hongsong%20Wang%20and%20Yin%20Zhu%20and%20Qiuxia%20Lai%20and%20Yang%20Zhang%20and%20Guo-Sen%20Xie%20and%20Xin%20Geng&entry.1292438233=%20%20Computational%20dance%20generation%20is%20crucial%20in%20many%20areas%2C%20such%20as%20art%2C%0Ahuman-computer%20interaction%2C%20virtual%20reality%2C%20and%20digital%20entertainment%2C%0Aparticularly%20for%20generating%20coherent%20and%20expressive%20long%20dance%20sequences.%0ADiffusion-based%20music-to-dance%20generation%20has%20made%20significant%20progress%2C%20yet%0Aexisting%20methods%20still%20struggle%20to%20produce%20physically%20plausible%20motions.%20To%0Aaddress%20this%2C%20we%20propose%20Plausibility-Aware%20Motion%20Diffusion%20%28PAMD%29%2C%20a%0Aframework%20for%20generating%20dances%20that%20are%20both%20musically%20aligned%20and%20physically%0Arealistic.%20The%20core%20of%20PAMD%20lies%20in%20the%20Plausible%20Motion%20Constraint%20%28PMC%29%2C%0Awhich%20leverages%20Neural%20Distance%20Fields%20%28NDFs%29%20to%20model%20the%20actual%20pose%20manifold%0Aand%20guide%20generated%20motions%20toward%20a%20physically%20valid%20pose%20manifold.%20To%20provide%0Amore%20effective%20guidance%20during%20generation%2C%20we%20incorporate%20Prior%20Motion%20Guidance%0A%28PMG%29%2C%20which%20uses%20standing%20poses%20as%20auxiliary%20conditions%20alongside%20music%0Afeatures.%20To%20further%20enhance%20realism%20for%20complex%20movements%2C%20we%20introduce%20the%0AMotion%20Refinement%20with%20Foot-ground%20Contact%20%28MRFC%29%20module%2C%20which%20addresses%0Afoot-skating%20artifacts%20by%20bridging%20the%20gap%20between%20the%20optimization%20objective%0Ain%20linear%20joint%20position%20space%20and%20the%20data%20representation%20in%20nonlinear%0Arotation%20space.%20Extensive%20experiments%20show%20that%20PAMD%20significantly%20improves%0Amusical%20alignment%20and%20enhances%20the%20physical%20plausibility%20of%20generated%20motions.%0AThis%20project%20page%20is%20available%20at%3A%20https%3A//mucunzhuzhu.github.io/PAMD-page/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20056v1&entry.124074799=Read"},
{"title": "Deep Active Inference Agents for Delayed and Long-Horizon Environments", "author": "Yavar Taheri Yeganeh and Mohsen Jafari and Andrea Matta", "abstract": "  With the recent success of world-model agents, which extend the core idea of\nmodel-based reinforcement learning by learning a differentiable model for\nsample-efficient control across diverse tasks, active inference (AIF) offers a\ncomplementary, neuroscience-grounded paradigm that unifies perception,\nlearning, and action within a single probabilistic framework powered by a\ngenerative model. Despite this promise, practical AIF agents still rely on\naccurate immediate predictions and exhaustive planning, a limitation that is\nexacerbated in delayed environments requiring plans over long horizons, tens to\nhundreds of steps. Moreover, most existing agents are evaluated on robotic or\nvision benchmarks which, while natural for biological agents, fall short of\nreal-world industrial complexity. We address these limitations with a\ngenerative-policy architecture featuring (i) a multi-step latent transition\nthat lets the generative model predict an entire horizon in a single\nlook-ahead, (ii) an integrated policy network that enables the transition and\nreceives gradients of the expected free energy, (iii) an alternating\noptimization scheme that updates model and policy from a replay buffer, and\n(iv) a single gradient step that plans over long horizons, eliminating\nexhaustive planning from the control loop. We evaluate our agent in an\nenvironment that mimics a realistic industrial scenario with delayed and\nlong-horizon settings. The empirical results confirm the effectiveness of the\nproposed approach, demonstrating the coupled world-model with the AIF formalism\nyields an end-to-end probabilistic controller capable of effective decision\nmaking in delayed, long-horizon settings without handcrafted rewards or\nexpensive planning.\n", "link": "http://arxiv.org/abs/2505.19867v1", "date": "2025-05-26", "relevancy": 2.3714, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6102}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5859}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Active%20Inference%20Agents%20for%20Delayed%20and%20Long-Horizon%20Environments&body=Title%3A%20Deep%20Active%20Inference%20Agents%20for%20Delayed%20and%20Long-Horizon%20Environments%0AAuthor%3A%20Yavar%20Taheri%20Yeganeh%20and%20Mohsen%20Jafari%20and%20Andrea%20Matta%0AAbstract%3A%20%20%20With%20the%20recent%20success%20of%20world-model%20agents%2C%20which%20extend%20the%20core%20idea%20of%0Amodel-based%20reinforcement%20learning%20by%20learning%20a%20differentiable%20model%20for%0Asample-efficient%20control%20across%20diverse%20tasks%2C%20active%20inference%20%28AIF%29%20offers%20a%0Acomplementary%2C%20neuroscience-grounded%20paradigm%20that%20unifies%20perception%2C%0Alearning%2C%20and%20action%20within%20a%20single%20probabilistic%20framework%20powered%20by%20a%0Agenerative%20model.%20Despite%20this%20promise%2C%20practical%20AIF%20agents%20still%20rely%20on%0Aaccurate%20immediate%20predictions%20and%20exhaustive%20planning%2C%20a%20limitation%20that%20is%0Aexacerbated%20in%20delayed%20environments%20requiring%20plans%20over%20long%20horizons%2C%20tens%20to%0Ahundreds%20of%20steps.%20Moreover%2C%20most%20existing%20agents%20are%20evaluated%20on%20robotic%20or%0Avision%20benchmarks%20which%2C%20while%20natural%20for%20biological%20agents%2C%20fall%20short%20of%0Areal-world%20industrial%20complexity.%20We%20address%20these%20limitations%20with%20a%0Agenerative-policy%20architecture%20featuring%20%28i%29%20a%20multi-step%20latent%20transition%0Athat%20lets%20the%20generative%20model%20predict%20an%20entire%20horizon%20in%20a%20single%0Alook-ahead%2C%20%28ii%29%20an%20integrated%20policy%20network%20that%20enables%20the%20transition%20and%0Areceives%20gradients%20of%20the%20expected%20free%20energy%2C%20%28iii%29%20an%20alternating%0Aoptimization%20scheme%20that%20updates%20model%20and%20policy%20from%20a%20replay%20buffer%2C%20and%0A%28iv%29%20a%20single%20gradient%20step%20that%20plans%20over%20long%20horizons%2C%20eliminating%0Aexhaustive%20planning%20from%20the%20control%20loop.%20We%20evaluate%20our%20agent%20in%20an%0Aenvironment%20that%20mimics%20a%20realistic%20industrial%20scenario%20with%20delayed%20and%0Along-horizon%20settings.%20The%20empirical%20results%20confirm%20the%20effectiveness%20of%20the%0Aproposed%20approach%2C%20demonstrating%20the%20coupled%20world-model%20with%20the%20AIF%20formalism%0Ayields%20an%20end-to-end%20probabilistic%20controller%20capable%20of%20effective%20decision%0Amaking%20in%20delayed%2C%20long-horizon%20settings%20without%20handcrafted%20rewards%20or%0Aexpensive%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Active%2520Inference%2520Agents%2520for%2520Delayed%2520and%2520Long-Horizon%2520Environments%26entry.906535625%3DYavar%2520Taheri%2520Yeganeh%2520and%2520Mohsen%2520Jafari%2520and%2520Andrea%2520Matta%26entry.1292438233%3D%2520%2520With%2520the%2520recent%2520success%2520of%2520world-model%2520agents%252C%2520which%2520extend%2520the%2520core%2520idea%2520of%250Amodel-based%2520reinforcement%2520learning%2520by%2520learning%2520a%2520differentiable%2520model%2520for%250Asample-efficient%2520control%2520across%2520diverse%2520tasks%252C%2520active%2520inference%2520%2528AIF%2529%2520offers%2520a%250Acomplementary%252C%2520neuroscience-grounded%2520paradigm%2520that%2520unifies%2520perception%252C%250Alearning%252C%2520and%2520action%2520within%2520a%2520single%2520probabilistic%2520framework%2520powered%2520by%2520a%250Agenerative%2520model.%2520Despite%2520this%2520promise%252C%2520practical%2520AIF%2520agents%2520still%2520rely%2520on%250Aaccurate%2520immediate%2520predictions%2520and%2520exhaustive%2520planning%252C%2520a%2520limitation%2520that%2520is%250Aexacerbated%2520in%2520delayed%2520environments%2520requiring%2520plans%2520over%2520long%2520horizons%252C%2520tens%2520to%250Ahundreds%2520of%2520steps.%2520Moreover%252C%2520most%2520existing%2520agents%2520are%2520evaluated%2520on%2520robotic%2520or%250Avision%2520benchmarks%2520which%252C%2520while%2520natural%2520for%2520biological%2520agents%252C%2520fall%2520short%2520of%250Areal-world%2520industrial%2520complexity.%2520We%2520address%2520these%2520limitations%2520with%2520a%250Agenerative-policy%2520architecture%2520featuring%2520%2528i%2529%2520a%2520multi-step%2520latent%2520transition%250Athat%2520lets%2520the%2520generative%2520model%2520predict%2520an%2520entire%2520horizon%2520in%2520a%2520single%250Alook-ahead%252C%2520%2528ii%2529%2520an%2520integrated%2520policy%2520network%2520that%2520enables%2520the%2520transition%2520and%250Areceives%2520gradients%2520of%2520the%2520expected%2520free%2520energy%252C%2520%2528iii%2529%2520an%2520alternating%250Aoptimization%2520scheme%2520that%2520updates%2520model%2520and%2520policy%2520from%2520a%2520replay%2520buffer%252C%2520and%250A%2528iv%2529%2520a%2520single%2520gradient%2520step%2520that%2520plans%2520over%2520long%2520horizons%252C%2520eliminating%250Aexhaustive%2520planning%2520from%2520the%2520control%2520loop.%2520We%2520evaluate%2520our%2520agent%2520in%2520an%250Aenvironment%2520that%2520mimics%2520a%2520realistic%2520industrial%2520scenario%2520with%2520delayed%2520and%250Along-horizon%2520settings.%2520The%2520empirical%2520results%2520confirm%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520approach%252C%2520demonstrating%2520the%2520coupled%2520world-model%2520with%2520the%2520AIF%2520formalism%250Ayields%2520an%2520end-to-end%2520probabilistic%2520controller%2520capable%2520of%2520effective%2520decision%250Amaking%2520in%2520delayed%252C%2520long-horizon%2520settings%2520without%2520handcrafted%2520rewards%2520or%250Aexpensive%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Active%20Inference%20Agents%20for%20Delayed%20and%20Long-Horizon%20Environments&entry.906535625=Yavar%20Taheri%20Yeganeh%20and%20Mohsen%20Jafari%20and%20Andrea%20Matta&entry.1292438233=%20%20With%20the%20recent%20success%20of%20world-model%20agents%2C%20which%20extend%20the%20core%20idea%20of%0Amodel-based%20reinforcement%20learning%20by%20learning%20a%20differentiable%20model%20for%0Asample-efficient%20control%20across%20diverse%20tasks%2C%20active%20inference%20%28AIF%29%20offers%20a%0Acomplementary%2C%20neuroscience-grounded%20paradigm%20that%20unifies%20perception%2C%0Alearning%2C%20and%20action%20within%20a%20single%20probabilistic%20framework%20powered%20by%20a%0Agenerative%20model.%20Despite%20this%20promise%2C%20practical%20AIF%20agents%20still%20rely%20on%0Aaccurate%20immediate%20predictions%20and%20exhaustive%20planning%2C%20a%20limitation%20that%20is%0Aexacerbated%20in%20delayed%20environments%20requiring%20plans%20over%20long%20horizons%2C%20tens%20to%0Ahundreds%20of%20steps.%20Moreover%2C%20most%20existing%20agents%20are%20evaluated%20on%20robotic%20or%0Avision%20benchmarks%20which%2C%20while%20natural%20for%20biological%20agents%2C%20fall%20short%20of%0Areal-world%20industrial%20complexity.%20We%20address%20these%20limitations%20with%20a%0Agenerative-policy%20architecture%20featuring%20%28i%29%20a%20multi-step%20latent%20transition%0Athat%20lets%20the%20generative%20model%20predict%20an%20entire%20horizon%20in%20a%20single%0Alook-ahead%2C%20%28ii%29%20an%20integrated%20policy%20network%20that%20enables%20the%20transition%20and%0Areceives%20gradients%20of%20the%20expected%20free%20energy%2C%20%28iii%29%20an%20alternating%0Aoptimization%20scheme%20that%20updates%20model%20and%20policy%20from%20a%20replay%20buffer%2C%20and%0A%28iv%29%20a%20single%20gradient%20step%20that%20plans%20over%20long%20horizons%2C%20eliminating%0Aexhaustive%20planning%20from%20the%20control%20loop.%20We%20evaluate%20our%20agent%20in%20an%0Aenvironment%20that%20mimics%20a%20realistic%20industrial%20scenario%20with%20delayed%20and%0Along-horizon%20settings.%20The%20empirical%20results%20confirm%20the%20effectiveness%20of%20the%0Aproposed%20approach%2C%20demonstrating%20the%20coupled%20world-model%20with%20the%20AIF%20formalism%0Ayields%20an%20end-to-end%20probabilistic%20controller%20capable%20of%20effective%20decision%0Amaking%20in%20delayed%2C%20long-horizon%20settings%20without%20handcrafted%20rewards%20or%0Aexpensive%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19867v1&entry.124074799=Read"},
{"title": "DeepEyes: Incentivizing \"Thinking with Images\" via Reinforcement\n  Learning", "author": "Ziwei Zheng and Michael Yang and Jack Hong and Chenxiao Zhao and Guohai Xu and Le Yang and Chao Shen and Xing Yu", "abstract": "  Large Vision-Language Models (VLMs) have shown strong capabilities in\nmultimodal understanding and reasoning, yet they are primarily constrained by\ntext-based reasoning processes. However, achieving seamless integration of\nvisual and textual reasoning which mirrors human cognitive processes remains a\nsignificant challenge. In particular, effectively incorporating advanced visual\ninput processing into reasoning mechanisms is still an open question. Thus, in\nthis paper, we explore the interleaved multimodal reasoning paradigm and\nintroduce DeepEyes, a model with \"thinking with images\" capabilities\nincentivized through end-to-end reinforcement learning without the need for\ncold-start SFT. Notably, this ability emerges natively within the model itself,\nleveraging its inherent grounding ability as a tool instead of depending on\nseparate specialized models. Specifically, we propose a tool-use-oriented data\nselection mechanism and a reward strategy to encourage successful tool-assisted\nreasoning trajectories. DeepEyes achieves significant performance gains on\nfine-grained perception and reasoning benchmarks and also demonstrates\nimprovement in grounding, hallucination, and mathematical reasoning tasks.\nInterestingly, we observe the distinct evolution of tool-calling behavior from\ninitial exploration to efficient and accurate exploitation, and diverse\nthinking patterns that closely mirror human visual reasoning processes. Code is\navailable at https://github.com/Visual-Agent/DeepEyes.\n", "link": "http://arxiv.org/abs/2505.14362v2", "date": "2025-05-26", "relevancy": 2.3514, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5943}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5943}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepEyes%3A%20Incentivizing%20%22Thinking%20with%20Images%22%20via%20Reinforcement%0A%20%20Learning&body=Title%3A%20DeepEyes%3A%20Incentivizing%20%22Thinking%20with%20Images%22%20via%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Ziwei%20Zheng%20and%20Michael%20Yang%20and%20Jack%20Hong%20and%20Chenxiao%20Zhao%20and%20Guohai%20Xu%20and%20Le%20Yang%20and%20Chao%20Shen%20and%20Xing%20Yu%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28VLMs%29%20have%20shown%20strong%20capabilities%20in%0Amultimodal%20understanding%20and%20reasoning%2C%20yet%20they%20are%20primarily%20constrained%20by%0Atext-based%20reasoning%20processes.%20However%2C%20achieving%20seamless%20integration%20of%0Avisual%20and%20textual%20reasoning%20which%20mirrors%20human%20cognitive%20processes%20remains%20a%0Asignificant%20challenge.%20In%20particular%2C%20effectively%20incorporating%20advanced%20visual%0Ainput%20processing%20into%20reasoning%20mechanisms%20is%20still%20an%20open%20question.%20Thus%2C%20in%0Athis%20paper%2C%20we%20explore%20the%20interleaved%20multimodal%20reasoning%20paradigm%20and%0Aintroduce%20DeepEyes%2C%20a%20model%20with%20%22thinking%20with%20images%22%20capabilities%0Aincentivized%20through%20end-to-end%20reinforcement%20learning%20without%20the%20need%20for%0Acold-start%20SFT.%20Notably%2C%20this%20ability%20emerges%20natively%20within%20the%20model%20itself%2C%0Aleveraging%20its%20inherent%20grounding%20ability%20as%20a%20tool%20instead%20of%20depending%20on%0Aseparate%20specialized%20models.%20Specifically%2C%20we%20propose%20a%20tool-use-oriented%20data%0Aselection%20mechanism%20and%20a%20reward%20strategy%20to%20encourage%20successful%20tool-assisted%0Areasoning%20trajectories.%20DeepEyes%20achieves%20significant%20performance%20gains%20on%0Afine-grained%20perception%20and%20reasoning%20benchmarks%20and%20also%20demonstrates%0Aimprovement%20in%20grounding%2C%20hallucination%2C%20and%20mathematical%20reasoning%20tasks.%0AInterestingly%2C%20we%20observe%20the%20distinct%20evolution%20of%20tool-calling%20behavior%20from%0Ainitial%20exploration%20to%20efficient%20and%20accurate%20exploitation%2C%20and%20diverse%0Athinking%20patterns%20that%20closely%20mirror%20human%20visual%20reasoning%20processes.%20Code%20is%0Aavailable%20at%20https%3A//github.com/Visual-Agent/DeepEyes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14362v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepEyes%253A%2520Incentivizing%2520%2522Thinking%2520with%2520Images%2522%2520via%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DZiwei%2520Zheng%2520and%2520Michael%2520Yang%2520and%2520Jack%2520Hong%2520and%2520Chenxiao%2520Zhao%2520and%2520Guohai%2520Xu%2520and%2520Le%2520Yang%2520and%2520Chao%2520Shen%2520and%2520Xing%2520Yu%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520shown%2520strong%2520capabilities%2520in%250Amultimodal%2520understanding%2520and%2520reasoning%252C%2520yet%2520they%2520are%2520primarily%2520constrained%2520by%250Atext-based%2520reasoning%2520processes.%2520However%252C%2520achieving%2520seamless%2520integration%2520of%250Avisual%2520and%2520textual%2520reasoning%2520which%2520mirrors%2520human%2520cognitive%2520processes%2520remains%2520a%250Asignificant%2520challenge.%2520In%2520particular%252C%2520effectively%2520incorporating%2520advanced%2520visual%250Ainput%2520processing%2520into%2520reasoning%2520mechanisms%2520is%2520still%2520an%2520open%2520question.%2520Thus%252C%2520in%250Athis%2520paper%252C%2520we%2520explore%2520the%2520interleaved%2520multimodal%2520reasoning%2520paradigm%2520and%250Aintroduce%2520DeepEyes%252C%2520a%2520model%2520with%2520%2522thinking%2520with%2520images%2522%2520capabilities%250Aincentivized%2520through%2520end-to-end%2520reinforcement%2520learning%2520without%2520the%2520need%2520for%250Acold-start%2520SFT.%2520Notably%252C%2520this%2520ability%2520emerges%2520natively%2520within%2520the%2520model%2520itself%252C%250Aleveraging%2520its%2520inherent%2520grounding%2520ability%2520as%2520a%2520tool%2520instead%2520of%2520depending%2520on%250Aseparate%2520specialized%2520models.%2520Specifically%252C%2520we%2520propose%2520a%2520tool-use-oriented%2520data%250Aselection%2520mechanism%2520and%2520a%2520reward%2520strategy%2520to%2520encourage%2520successful%2520tool-assisted%250Areasoning%2520trajectories.%2520DeepEyes%2520achieves%2520significant%2520performance%2520gains%2520on%250Afine-grained%2520perception%2520and%2520reasoning%2520benchmarks%2520and%2520also%2520demonstrates%250Aimprovement%2520in%2520grounding%252C%2520hallucination%252C%2520and%2520mathematical%2520reasoning%2520tasks.%250AInterestingly%252C%2520we%2520observe%2520the%2520distinct%2520evolution%2520of%2520tool-calling%2520behavior%2520from%250Ainitial%2520exploration%2520to%2520efficient%2520and%2520accurate%2520exploitation%252C%2520and%2520diverse%250Athinking%2520patterns%2520that%2520closely%2520mirror%2520human%2520visual%2520reasoning%2520processes.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/Visual-Agent/DeepEyes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14362v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepEyes%3A%20Incentivizing%20%22Thinking%20with%20Images%22%20via%20Reinforcement%0A%20%20Learning&entry.906535625=Ziwei%20Zheng%20and%20Michael%20Yang%20and%20Jack%20Hong%20and%20Chenxiao%20Zhao%20and%20Guohai%20Xu%20and%20Le%20Yang%20and%20Chao%20Shen%20and%20Xing%20Yu&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28VLMs%29%20have%20shown%20strong%20capabilities%20in%0Amultimodal%20understanding%20and%20reasoning%2C%20yet%20they%20are%20primarily%20constrained%20by%0Atext-based%20reasoning%20processes.%20However%2C%20achieving%20seamless%20integration%20of%0Avisual%20and%20textual%20reasoning%20which%20mirrors%20human%20cognitive%20processes%20remains%20a%0Asignificant%20challenge.%20In%20particular%2C%20effectively%20incorporating%20advanced%20visual%0Ainput%20processing%20into%20reasoning%20mechanisms%20is%20still%20an%20open%20question.%20Thus%2C%20in%0Athis%20paper%2C%20we%20explore%20the%20interleaved%20multimodal%20reasoning%20paradigm%20and%0Aintroduce%20DeepEyes%2C%20a%20model%20with%20%22thinking%20with%20images%22%20capabilities%0Aincentivized%20through%20end-to-end%20reinforcement%20learning%20without%20the%20need%20for%0Acold-start%20SFT.%20Notably%2C%20this%20ability%20emerges%20natively%20within%20the%20model%20itself%2C%0Aleveraging%20its%20inherent%20grounding%20ability%20as%20a%20tool%20instead%20of%20depending%20on%0Aseparate%20specialized%20models.%20Specifically%2C%20we%20propose%20a%20tool-use-oriented%20data%0Aselection%20mechanism%20and%20a%20reward%20strategy%20to%20encourage%20successful%20tool-assisted%0Areasoning%20trajectories.%20DeepEyes%20achieves%20significant%20performance%20gains%20on%0Afine-grained%20perception%20and%20reasoning%20benchmarks%20and%20also%20demonstrates%0Aimprovement%20in%20grounding%2C%20hallucination%2C%20and%20mathematical%20reasoning%20tasks.%0AInterestingly%2C%20we%20observe%20the%20distinct%20evolution%20of%20tool-calling%20behavior%20from%0Ainitial%20exploration%20to%20efficient%20and%20accurate%20exploitation%2C%20and%20diverse%0Athinking%20patterns%20that%20closely%20mirror%20human%20visual%20reasoning%20processes.%20Code%20is%0Aavailable%20at%20https%3A//github.com/Visual-Agent/DeepEyes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14362v2&entry.124074799=Read"},
{"title": "Poison in the Well: Feature Embedding Disruption in Backdoor Attacks", "author": "Zhou Feng and Jiahao Chen and Chunyi Zhou and Yuwen Pu and Qingming Li and Shouling Ji", "abstract": "  Backdoor attacks embed malicious triggers into training data, enabling\nattackers to manipulate neural network behavior during inference while\nmaintaining high accuracy on benign inputs. However, existing backdoor attacks\nface limitations manifesting in excessive reliance on training data, poor\nstealth, and instability, which hinder their effectiveness in real-world\napplications. Therefore, this paper introduces ShadowPrint, a versatile\nbackdoor attack that targets feature embeddings within neural networks to\nachieve high ASRs and stealthiness. Unlike traditional approaches, ShadowPrint\nreduces reliance on training data access and operates effectively with\nexceedingly low poison rates (as low as 0.01%). It leverages a clustering-based\noptimization strategy to align feature embeddings, ensuring robust performance\nacross diverse scenarios while maintaining stability and stealth. Extensive\nevaluations demonstrate that ShadowPrint achieves superior ASR (up to 100%),\nsteady CA (with decay no more than 1% in most cases), and low DDR (averaging\nbelow 5%) across both clean-label and dirty-label settings, and with poison\nrates ranging from as low as 0.01% to 0.05%, setting a new standard for\nbackdoor attack capabilities and emphasizing the need for advanced defense\nstrategies focused on feature space manipulations.\n", "link": "http://arxiv.org/abs/2505.19821v1", "date": "2025-05-26", "relevancy": 2.3421, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.475}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.47}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Poison%20in%20the%20Well%3A%20Feature%20Embedding%20Disruption%20in%20Backdoor%20Attacks&body=Title%3A%20Poison%20in%20the%20Well%3A%20Feature%20Embedding%20Disruption%20in%20Backdoor%20Attacks%0AAuthor%3A%20Zhou%20Feng%20and%20Jiahao%20Chen%20and%20Chunyi%20Zhou%20and%20Yuwen%20Pu%20and%20Qingming%20Li%20and%20Shouling%20Ji%0AAbstract%3A%20%20%20Backdoor%20attacks%20embed%20malicious%20triggers%20into%20training%20data%2C%20enabling%0Aattackers%20to%20manipulate%20neural%20network%20behavior%20during%20inference%20while%0Amaintaining%20high%20accuracy%20on%20benign%20inputs.%20However%2C%20existing%20backdoor%20attacks%0Aface%20limitations%20manifesting%20in%20excessive%20reliance%20on%20training%20data%2C%20poor%0Astealth%2C%20and%20instability%2C%20which%20hinder%20their%20effectiveness%20in%20real-world%0Aapplications.%20Therefore%2C%20this%20paper%20introduces%20ShadowPrint%2C%20a%20versatile%0Abackdoor%20attack%20that%20targets%20feature%20embeddings%20within%20neural%20networks%20to%0Aachieve%20high%20ASRs%20and%20stealthiness.%20Unlike%20traditional%20approaches%2C%20ShadowPrint%0Areduces%20reliance%20on%20training%20data%20access%20and%20operates%20effectively%20with%0Aexceedingly%20low%20poison%20rates%20%28as%20low%20as%200.01%25%29.%20It%20leverages%20a%20clustering-based%0Aoptimization%20strategy%20to%20align%20feature%20embeddings%2C%20ensuring%20robust%20performance%0Aacross%20diverse%20scenarios%20while%20maintaining%20stability%20and%20stealth.%20Extensive%0Aevaluations%20demonstrate%20that%20ShadowPrint%20achieves%20superior%20ASR%20%28up%20to%20100%25%29%2C%0Asteady%20CA%20%28with%20decay%20no%20more%20than%201%25%20in%20most%20cases%29%2C%20and%20low%20DDR%20%28averaging%0Abelow%205%25%29%20across%20both%20clean-label%20and%20dirty-label%20settings%2C%20and%20with%20poison%0Arates%20ranging%20from%20as%20low%20as%200.01%25%20to%200.05%25%2C%20setting%20a%20new%20standard%20for%0Abackdoor%20attack%20capabilities%20and%20emphasizing%20the%20need%20for%20advanced%20defense%0Astrategies%20focused%20on%20feature%20space%20manipulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoison%2520in%2520the%2520Well%253A%2520Feature%2520Embedding%2520Disruption%2520in%2520Backdoor%2520Attacks%26entry.906535625%3DZhou%2520Feng%2520and%2520Jiahao%2520Chen%2520and%2520Chunyi%2520Zhou%2520and%2520Yuwen%2520Pu%2520and%2520Qingming%2520Li%2520and%2520Shouling%2520Ji%26entry.1292438233%3D%2520%2520Backdoor%2520attacks%2520embed%2520malicious%2520triggers%2520into%2520training%2520data%252C%2520enabling%250Aattackers%2520to%2520manipulate%2520neural%2520network%2520behavior%2520during%2520inference%2520while%250Amaintaining%2520high%2520accuracy%2520on%2520benign%2520inputs.%2520However%252C%2520existing%2520backdoor%2520attacks%250Aface%2520limitations%2520manifesting%2520in%2520excessive%2520reliance%2520on%2520training%2520data%252C%2520poor%250Astealth%252C%2520and%2520instability%252C%2520which%2520hinder%2520their%2520effectiveness%2520in%2520real-world%250Aapplications.%2520Therefore%252C%2520this%2520paper%2520introduces%2520ShadowPrint%252C%2520a%2520versatile%250Abackdoor%2520attack%2520that%2520targets%2520feature%2520embeddings%2520within%2520neural%2520networks%2520to%250Aachieve%2520high%2520ASRs%2520and%2520stealthiness.%2520Unlike%2520traditional%2520approaches%252C%2520ShadowPrint%250Areduces%2520reliance%2520on%2520training%2520data%2520access%2520and%2520operates%2520effectively%2520with%250Aexceedingly%2520low%2520poison%2520rates%2520%2528as%2520low%2520as%25200.01%2525%2529.%2520It%2520leverages%2520a%2520clustering-based%250Aoptimization%2520strategy%2520to%2520align%2520feature%2520embeddings%252C%2520ensuring%2520robust%2520performance%250Aacross%2520diverse%2520scenarios%2520while%2520maintaining%2520stability%2520and%2520stealth.%2520Extensive%250Aevaluations%2520demonstrate%2520that%2520ShadowPrint%2520achieves%2520superior%2520ASR%2520%2528up%2520to%2520100%2525%2529%252C%250Asteady%2520CA%2520%2528with%2520decay%2520no%2520more%2520than%25201%2525%2520in%2520most%2520cases%2529%252C%2520and%2520low%2520DDR%2520%2528averaging%250Abelow%25205%2525%2529%2520across%2520both%2520clean-label%2520and%2520dirty-label%2520settings%252C%2520and%2520with%2520poison%250Arates%2520ranging%2520from%2520as%2520low%2520as%25200.01%2525%2520to%25200.05%2525%252C%2520setting%2520a%2520new%2520standard%2520for%250Abackdoor%2520attack%2520capabilities%2520and%2520emphasizing%2520the%2520need%2520for%2520advanced%2520defense%250Astrategies%2520focused%2520on%2520feature%2520space%2520manipulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Poison%20in%20the%20Well%3A%20Feature%20Embedding%20Disruption%20in%20Backdoor%20Attacks&entry.906535625=Zhou%20Feng%20and%20Jiahao%20Chen%20and%20Chunyi%20Zhou%20and%20Yuwen%20Pu%20and%20Qingming%20Li%20and%20Shouling%20Ji&entry.1292438233=%20%20Backdoor%20attacks%20embed%20malicious%20triggers%20into%20training%20data%2C%20enabling%0Aattackers%20to%20manipulate%20neural%20network%20behavior%20during%20inference%20while%0Amaintaining%20high%20accuracy%20on%20benign%20inputs.%20However%2C%20existing%20backdoor%20attacks%0Aface%20limitations%20manifesting%20in%20excessive%20reliance%20on%20training%20data%2C%20poor%0Astealth%2C%20and%20instability%2C%20which%20hinder%20their%20effectiveness%20in%20real-world%0Aapplications.%20Therefore%2C%20this%20paper%20introduces%20ShadowPrint%2C%20a%20versatile%0Abackdoor%20attack%20that%20targets%20feature%20embeddings%20within%20neural%20networks%20to%0Aachieve%20high%20ASRs%20and%20stealthiness.%20Unlike%20traditional%20approaches%2C%20ShadowPrint%0Areduces%20reliance%20on%20training%20data%20access%20and%20operates%20effectively%20with%0Aexceedingly%20low%20poison%20rates%20%28as%20low%20as%200.01%25%29.%20It%20leverages%20a%20clustering-based%0Aoptimization%20strategy%20to%20align%20feature%20embeddings%2C%20ensuring%20robust%20performance%0Aacross%20diverse%20scenarios%20while%20maintaining%20stability%20and%20stealth.%20Extensive%0Aevaluations%20demonstrate%20that%20ShadowPrint%20achieves%20superior%20ASR%20%28up%20to%20100%25%29%2C%0Asteady%20CA%20%28with%20decay%20no%20more%20than%201%25%20in%20most%20cases%29%2C%20and%20low%20DDR%20%28averaging%0Abelow%205%25%29%20across%20both%20clean-label%20and%20dirty-label%20settings%2C%20and%20with%20poison%0Arates%20ranging%20from%20as%20low%20as%200.01%25%20to%200.05%25%2C%20setting%20a%20new%20standard%20for%0Abackdoor%20attack%20capabilities%20and%20emphasizing%20the%20need%20for%20advanced%20defense%0Astrategies%20focused%20on%20feature%20space%20manipulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19821v1&entry.124074799=Read"},
{"title": "Depth-Guided Bundle Sampling for Efficient Generalizable Neural Radiance\n  Field Reconstruction", "author": "Li Fang and Hao Zhu and Longlong Chen and Fei Hu and Long Ye and Zhan Ma", "abstract": "  Recent advancements in generalizable novel view synthesis have achieved\nimpressive quality through interpolation between nearby views. However,\nrendering high-resolution images remains computationally intensive due to the\nneed for dense sampling of all rays. Recognizing that natural scenes are\ntypically piecewise smooth and sampling all rays is often redundant, we propose\na novel depth-guided bundle sampling strategy to accelerate rendering. By\ngrouping adjacent rays into a bundle and sampling them collectively, a shared\nrepresentation is generated for decoding all rays within the bundle. To further\noptimize efficiency, our adaptive sampling strategy dynamically allocates\nsamples based on depth confidence, concentrating more samples in complex\nregions while reducing them in smoother areas. When applied to ENeRF, our\nmethod achieves up to a 1.27 dB PSNR improvement and a 47% increase in FPS on\nthe DTU dataset. Extensive experiments on synthetic and real-world datasets\ndemonstrate state-of-the-art rendering quality and up to 2x faster rendering\ncompared to existing generalizable methods. Code is available at\nhttps://github.com/KLMAV-CUC/GDB-NeRF.\n", "link": "http://arxiv.org/abs/2505.19793v1", "date": "2025-05-26", "relevancy": 2.3371, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6208}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5584}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth-Guided%20Bundle%20Sampling%20for%20Efficient%20Generalizable%20Neural%20Radiance%0A%20%20Field%20Reconstruction&body=Title%3A%20Depth-Guided%20Bundle%20Sampling%20for%20Efficient%20Generalizable%20Neural%20Radiance%0A%20%20Field%20Reconstruction%0AAuthor%3A%20Li%20Fang%20and%20Hao%20Zhu%20and%20Longlong%20Chen%20and%20Fei%20Hu%20and%20Long%20Ye%20and%20Zhan%20Ma%0AAbstract%3A%20%20%20Recent%20advancements%20in%20generalizable%20novel%20view%20synthesis%20have%20achieved%0Aimpressive%20quality%20through%20interpolation%20between%20nearby%20views.%20However%2C%0Arendering%20high-resolution%20images%20remains%20computationally%20intensive%20due%20to%20the%0Aneed%20for%20dense%20sampling%20of%20all%20rays.%20Recognizing%20that%20natural%20scenes%20are%0Atypically%20piecewise%20smooth%20and%20sampling%20all%20rays%20is%20often%20redundant%2C%20we%20propose%0Aa%20novel%20depth-guided%20bundle%20sampling%20strategy%20to%20accelerate%20rendering.%20By%0Agrouping%20adjacent%20rays%20into%20a%20bundle%20and%20sampling%20them%20collectively%2C%20a%20shared%0Arepresentation%20is%20generated%20for%20decoding%20all%20rays%20within%20the%20bundle.%20To%20further%0Aoptimize%20efficiency%2C%20our%20adaptive%20sampling%20strategy%20dynamically%20allocates%0Asamples%20based%20on%20depth%20confidence%2C%20concentrating%20more%20samples%20in%20complex%0Aregions%20while%20reducing%20them%20in%20smoother%20areas.%20When%20applied%20to%20ENeRF%2C%20our%0Amethod%20achieves%20up%20to%20a%201.27%20dB%20PSNR%20improvement%20and%20a%2047%25%20increase%20in%20FPS%20on%0Athe%20DTU%20dataset.%20Extensive%20experiments%20on%20synthetic%20and%20real-world%20datasets%0Ademonstrate%20state-of-the-art%20rendering%20quality%20and%20up%20to%202x%20faster%20rendering%0Acompared%20to%20existing%20generalizable%20methods.%20Code%20is%20available%20at%0Ahttps%3A//github.com/KLMAV-CUC/GDB-NeRF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth-Guided%2520Bundle%2520Sampling%2520for%2520Efficient%2520Generalizable%2520Neural%2520Radiance%250A%2520%2520Field%2520Reconstruction%26entry.906535625%3DLi%2520Fang%2520and%2520Hao%2520Zhu%2520and%2520Longlong%2520Chen%2520and%2520Fei%2520Hu%2520and%2520Long%2520Ye%2520and%2520Zhan%2520Ma%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520generalizable%2520novel%2520view%2520synthesis%2520have%2520achieved%250Aimpressive%2520quality%2520through%2520interpolation%2520between%2520nearby%2520views.%2520However%252C%250Arendering%2520high-resolution%2520images%2520remains%2520computationally%2520intensive%2520due%2520to%2520the%250Aneed%2520for%2520dense%2520sampling%2520of%2520all%2520rays.%2520Recognizing%2520that%2520natural%2520scenes%2520are%250Atypically%2520piecewise%2520smooth%2520and%2520sampling%2520all%2520rays%2520is%2520often%2520redundant%252C%2520we%2520propose%250Aa%2520novel%2520depth-guided%2520bundle%2520sampling%2520strategy%2520to%2520accelerate%2520rendering.%2520By%250Agrouping%2520adjacent%2520rays%2520into%2520a%2520bundle%2520and%2520sampling%2520them%2520collectively%252C%2520a%2520shared%250Arepresentation%2520is%2520generated%2520for%2520decoding%2520all%2520rays%2520within%2520the%2520bundle.%2520To%2520further%250Aoptimize%2520efficiency%252C%2520our%2520adaptive%2520sampling%2520strategy%2520dynamically%2520allocates%250Asamples%2520based%2520on%2520depth%2520confidence%252C%2520concentrating%2520more%2520samples%2520in%2520complex%250Aregions%2520while%2520reducing%2520them%2520in%2520smoother%2520areas.%2520When%2520applied%2520to%2520ENeRF%252C%2520our%250Amethod%2520achieves%2520up%2520to%2520a%25201.27%2520dB%2520PSNR%2520improvement%2520and%2520a%252047%2525%2520increase%2520in%2520FPS%2520on%250Athe%2520DTU%2520dataset.%2520Extensive%2520experiments%2520on%2520synthetic%2520and%2520real-world%2520datasets%250Ademonstrate%2520state-of-the-art%2520rendering%2520quality%2520and%2520up%2520to%25202x%2520faster%2520rendering%250Acompared%2520to%2520existing%2520generalizable%2520methods.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/KLMAV-CUC/GDB-NeRF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth-Guided%20Bundle%20Sampling%20for%20Efficient%20Generalizable%20Neural%20Radiance%0A%20%20Field%20Reconstruction&entry.906535625=Li%20Fang%20and%20Hao%20Zhu%20and%20Longlong%20Chen%20and%20Fei%20Hu%20and%20Long%20Ye%20and%20Zhan%20Ma&entry.1292438233=%20%20Recent%20advancements%20in%20generalizable%20novel%20view%20synthesis%20have%20achieved%0Aimpressive%20quality%20through%20interpolation%20between%20nearby%20views.%20However%2C%0Arendering%20high-resolution%20images%20remains%20computationally%20intensive%20due%20to%20the%0Aneed%20for%20dense%20sampling%20of%20all%20rays.%20Recognizing%20that%20natural%20scenes%20are%0Atypically%20piecewise%20smooth%20and%20sampling%20all%20rays%20is%20often%20redundant%2C%20we%20propose%0Aa%20novel%20depth-guided%20bundle%20sampling%20strategy%20to%20accelerate%20rendering.%20By%0Agrouping%20adjacent%20rays%20into%20a%20bundle%20and%20sampling%20them%20collectively%2C%20a%20shared%0Arepresentation%20is%20generated%20for%20decoding%20all%20rays%20within%20the%20bundle.%20To%20further%0Aoptimize%20efficiency%2C%20our%20adaptive%20sampling%20strategy%20dynamically%20allocates%0Asamples%20based%20on%20depth%20confidence%2C%20concentrating%20more%20samples%20in%20complex%0Aregions%20while%20reducing%20them%20in%20smoother%20areas.%20When%20applied%20to%20ENeRF%2C%20our%0Amethod%20achieves%20up%20to%20a%201.27%20dB%20PSNR%20improvement%20and%20a%2047%25%20increase%20in%20FPS%20on%0Athe%20DTU%20dataset.%20Extensive%20experiments%20on%20synthetic%20and%20real-world%20datasets%0Ademonstrate%20state-of-the-art%20rendering%20quality%20and%20up%20to%202x%20faster%20rendering%0Acompared%20to%20existing%20generalizable%20methods.%20Code%20is%20available%20at%0Ahttps%3A//github.com/KLMAV-CUC/GDB-NeRF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19793v1&entry.124074799=Read"},
{"title": "DepthMatch: Semi-Supervised RGB-D Scene Parsing through Depth-Guided\n  Regularization", "author": "Jianxin Huang and Jiahang Li and Sergey Vityazev and Alexander Dvorkovich and Rui Fan", "abstract": "  RGB-D scene parsing methods effectively capture both semantic and geometric\nfeatures of the environment, demonstrating great potential under challenging\nconditions such as extreme weather and low lighting. However, existing RGB-D\nscene parsing methods predominantly rely on supervised training strategies,\nwhich require a large amount of manually annotated pixel-level labels that are\nboth time-consuming and costly. To overcome these limitations, we introduce\nDepthMatch, a semi-supervised learning framework that is specifically designed\nfor RGB-D scene parsing. To make full use of unlabeled data, we propose\ncomplementary patch mix-up augmentation to explore the latent relationships\nbetween texture and spatial features in RGB-D image pairs. We also design a\nlightweight spatial prior injector to replace traditional complex fusion\nmodules, improving the efficiency of heterogeneous feature fusion. Furthermore,\nwe introduce depth-guided boundary loss to enhance the model's boundary\nprediction capabilities. Experimental results demonstrate that DepthMatch\nexhibits high applicability in both indoor and outdoor scenes, achieving\nstate-of-the-art results on the NYUv2 dataset and ranking first on the KITTI\nSemantics benchmark.\n", "link": "http://arxiv.org/abs/2505.20041v1", "date": "2025-05-26", "relevancy": 2.3347, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5962}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5899}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DepthMatch%3A%20Semi-Supervised%20RGB-D%20Scene%20Parsing%20through%20Depth-Guided%0A%20%20Regularization&body=Title%3A%20DepthMatch%3A%20Semi-Supervised%20RGB-D%20Scene%20Parsing%20through%20Depth-Guided%0A%20%20Regularization%0AAuthor%3A%20Jianxin%20Huang%20and%20Jiahang%20Li%20and%20Sergey%20Vityazev%20and%20Alexander%20Dvorkovich%20and%20Rui%20Fan%0AAbstract%3A%20%20%20RGB-D%20scene%20parsing%20methods%20effectively%20capture%20both%20semantic%20and%20geometric%0Afeatures%20of%20the%20environment%2C%20demonstrating%20great%20potential%20under%20challenging%0Aconditions%20such%20as%20extreme%20weather%20and%20low%20lighting.%20However%2C%20existing%20RGB-D%0Ascene%20parsing%20methods%20predominantly%20rely%20on%20supervised%20training%20strategies%2C%0Awhich%20require%20a%20large%20amount%20of%20manually%20annotated%20pixel-level%20labels%20that%20are%0Aboth%20time-consuming%20and%20costly.%20To%20overcome%20these%20limitations%2C%20we%20introduce%0ADepthMatch%2C%20a%20semi-supervised%20learning%20framework%20that%20is%20specifically%20designed%0Afor%20RGB-D%20scene%20parsing.%20To%20make%20full%20use%20of%20unlabeled%20data%2C%20we%20propose%0Acomplementary%20patch%20mix-up%20augmentation%20to%20explore%20the%20latent%20relationships%0Abetween%20texture%20and%20spatial%20features%20in%20RGB-D%20image%20pairs.%20We%20also%20design%20a%0Alightweight%20spatial%20prior%20injector%20to%20replace%20traditional%20complex%20fusion%0Amodules%2C%20improving%20the%20efficiency%20of%20heterogeneous%20feature%20fusion.%20Furthermore%2C%0Awe%20introduce%20depth-guided%20boundary%20loss%20to%20enhance%20the%20model%27s%20boundary%0Aprediction%20capabilities.%20Experimental%20results%20demonstrate%20that%20DepthMatch%0Aexhibits%20high%20applicability%20in%20both%20indoor%20and%20outdoor%20scenes%2C%20achieving%0Astate-of-the-art%20results%20on%20the%20NYUv2%20dataset%20and%20ranking%20first%20on%20the%20KITTI%0ASemantics%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepthMatch%253A%2520Semi-Supervised%2520RGB-D%2520Scene%2520Parsing%2520through%2520Depth-Guided%250A%2520%2520Regularization%26entry.906535625%3DJianxin%2520Huang%2520and%2520Jiahang%2520Li%2520and%2520Sergey%2520Vityazev%2520and%2520Alexander%2520Dvorkovich%2520and%2520Rui%2520Fan%26entry.1292438233%3D%2520%2520RGB-D%2520scene%2520parsing%2520methods%2520effectively%2520capture%2520both%2520semantic%2520and%2520geometric%250Afeatures%2520of%2520the%2520environment%252C%2520demonstrating%2520great%2520potential%2520under%2520challenging%250Aconditions%2520such%2520as%2520extreme%2520weather%2520and%2520low%2520lighting.%2520However%252C%2520existing%2520RGB-D%250Ascene%2520parsing%2520methods%2520predominantly%2520rely%2520on%2520supervised%2520training%2520strategies%252C%250Awhich%2520require%2520a%2520large%2520amount%2520of%2520manually%2520annotated%2520pixel-level%2520labels%2520that%2520are%250Aboth%2520time-consuming%2520and%2520costly.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%250ADepthMatch%252C%2520a%2520semi-supervised%2520learning%2520framework%2520that%2520is%2520specifically%2520designed%250Afor%2520RGB-D%2520scene%2520parsing.%2520To%2520make%2520full%2520use%2520of%2520unlabeled%2520data%252C%2520we%2520propose%250Acomplementary%2520patch%2520mix-up%2520augmentation%2520to%2520explore%2520the%2520latent%2520relationships%250Abetween%2520texture%2520and%2520spatial%2520features%2520in%2520RGB-D%2520image%2520pairs.%2520We%2520also%2520design%2520a%250Alightweight%2520spatial%2520prior%2520injector%2520to%2520replace%2520traditional%2520complex%2520fusion%250Amodules%252C%2520improving%2520the%2520efficiency%2520of%2520heterogeneous%2520feature%2520fusion.%2520Furthermore%252C%250Awe%2520introduce%2520depth-guided%2520boundary%2520loss%2520to%2520enhance%2520the%2520model%2527s%2520boundary%250Aprediction%2520capabilities.%2520Experimental%2520results%2520demonstrate%2520that%2520DepthMatch%250Aexhibits%2520high%2520applicability%2520in%2520both%2520indoor%2520and%2520outdoor%2520scenes%252C%2520achieving%250Astate-of-the-art%2520results%2520on%2520the%2520NYUv2%2520dataset%2520and%2520ranking%2520first%2520on%2520the%2520KITTI%250ASemantics%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DepthMatch%3A%20Semi-Supervised%20RGB-D%20Scene%20Parsing%20through%20Depth-Guided%0A%20%20Regularization&entry.906535625=Jianxin%20Huang%20and%20Jiahang%20Li%20and%20Sergey%20Vityazev%20and%20Alexander%20Dvorkovich%20and%20Rui%20Fan&entry.1292438233=%20%20RGB-D%20scene%20parsing%20methods%20effectively%20capture%20both%20semantic%20and%20geometric%0Afeatures%20of%20the%20environment%2C%20demonstrating%20great%20potential%20under%20challenging%0Aconditions%20such%20as%20extreme%20weather%20and%20low%20lighting.%20However%2C%20existing%20RGB-D%0Ascene%20parsing%20methods%20predominantly%20rely%20on%20supervised%20training%20strategies%2C%0Awhich%20require%20a%20large%20amount%20of%20manually%20annotated%20pixel-level%20labels%20that%20are%0Aboth%20time-consuming%20and%20costly.%20To%20overcome%20these%20limitations%2C%20we%20introduce%0ADepthMatch%2C%20a%20semi-supervised%20learning%20framework%20that%20is%20specifically%20designed%0Afor%20RGB-D%20scene%20parsing.%20To%20make%20full%20use%20of%20unlabeled%20data%2C%20we%20propose%0Acomplementary%20patch%20mix-up%20augmentation%20to%20explore%20the%20latent%20relationships%0Abetween%20texture%20and%20spatial%20features%20in%20RGB-D%20image%20pairs.%20We%20also%20design%20a%0Alightweight%20spatial%20prior%20injector%20to%20replace%20traditional%20complex%20fusion%0Amodules%2C%20improving%20the%20efficiency%20of%20heterogeneous%20feature%20fusion.%20Furthermore%2C%0Awe%20introduce%20depth-guided%20boundary%20loss%20to%20enhance%20the%20model%27s%20boundary%0Aprediction%20capabilities.%20Experimental%20results%20demonstrate%20that%20DepthMatch%0Aexhibits%20high%20applicability%20in%20both%20indoor%20and%20outdoor%20scenes%2C%20achieving%0Astate-of-the-art%20results%20on%20the%20NYUv2%20dataset%20and%20ranking%20first%20on%20the%20KITTI%0ASemantics%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20041v1&entry.124074799=Read"},
{"title": "Multi-Timescale Motion-Decoupled Spiking Transformer for Audio-Visual\n  Zero-Shot Learning", "author": "Wenrui Li and Penghong Wang and Xingtao Wang and Wangmeng Zuo and Xiaopeng Fan and Yonghong Tian", "abstract": "  Audio-visual zero-shot learning (ZSL) has been extensively researched for its\ncapability to classify video data from unseen classes during training.\nNevertheless, current methodologies often struggle with background scene biases\nand inadequate motion detail. This paper proposes a novel dual-stream\nMulti-Timescale Motion-Decoupled Spiking Transformer (MDST++), which decouples\ncontextual semantic information and sparse dynamic motion information. The\nrecurrent joint learning unit is proposed to extract contextual semantic\ninformation and capture joint knowledge across various modalities to understand\nthe environment of actions. By converting RGB images to events, our method\ncaptures motion information more accurately and mitigates background scene\nbiases. Moreover, we introduce a discrepancy analysis block to model audio\nmotion information. To enhance the robustness of SNNs in extracting temporal\nand motion cues, we dynamically adjust the threshold of Leaky\nIntegrate-and-Fire neurons based on global motion and contextual semantic\ninformation. Our experiments validate the effectiveness of MDST++,\ndemonstrating their consistent superiority over state-of-the-art methods on\nmainstream benchmarks. Additionally, incorporating motion and multi-timescale\ninformation significantly improves HM and ZSL accuracy by 26.2\\% and 39.9\\%.\n", "link": "http://arxiv.org/abs/2505.19938v1", "date": "2025-05-26", "relevancy": 2.3075, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5837}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5776}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Timescale%20Motion-Decoupled%20Spiking%20Transformer%20for%20Audio-Visual%0A%20%20Zero-Shot%20Learning&body=Title%3A%20Multi-Timescale%20Motion-Decoupled%20Spiking%20Transformer%20for%20Audio-Visual%0A%20%20Zero-Shot%20Learning%0AAuthor%3A%20Wenrui%20Li%20and%20Penghong%20Wang%20and%20Xingtao%20Wang%20and%20Wangmeng%20Zuo%20and%20Xiaopeng%20Fan%20and%20Yonghong%20Tian%0AAbstract%3A%20%20%20Audio-visual%20zero-shot%20learning%20%28ZSL%29%20has%20been%20extensively%20researched%20for%20its%0Acapability%20to%20classify%20video%20data%20from%20unseen%20classes%20during%20training.%0ANevertheless%2C%20current%20methodologies%20often%20struggle%20with%20background%20scene%20biases%0Aand%20inadequate%20motion%20detail.%20This%20paper%20proposes%20a%20novel%20dual-stream%0AMulti-Timescale%20Motion-Decoupled%20Spiking%20Transformer%20%28MDST%2B%2B%29%2C%20which%20decouples%0Acontextual%20semantic%20information%20and%20sparse%20dynamic%20motion%20information.%20The%0Arecurrent%20joint%20learning%20unit%20is%20proposed%20to%20extract%20contextual%20semantic%0Ainformation%20and%20capture%20joint%20knowledge%20across%20various%20modalities%20to%20understand%0Athe%20environment%20of%20actions.%20By%20converting%20RGB%20images%20to%20events%2C%20our%20method%0Acaptures%20motion%20information%20more%20accurately%20and%20mitigates%20background%20scene%0Abiases.%20Moreover%2C%20we%20introduce%20a%20discrepancy%20analysis%20block%20to%20model%20audio%0Amotion%20information.%20To%20enhance%20the%20robustness%20of%20SNNs%20in%20extracting%20temporal%0Aand%20motion%20cues%2C%20we%20dynamically%20adjust%20the%20threshold%20of%20Leaky%0AIntegrate-and-Fire%20neurons%20based%20on%20global%20motion%20and%20contextual%20semantic%0Ainformation.%20Our%20experiments%20validate%20the%20effectiveness%20of%20MDST%2B%2B%2C%0Ademonstrating%20their%20consistent%20superiority%20over%20state-of-the-art%20methods%20on%0Amainstream%20benchmarks.%20Additionally%2C%20incorporating%20motion%20and%20multi-timescale%0Ainformation%20significantly%20improves%20HM%20and%20ZSL%20accuracy%20by%2026.2%5C%25%20and%2039.9%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Timescale%2520Motion-Decoupled%2520Spiking%2520Transformer%2520for%2520Audio-Visual%250A%2520%2520Zero-Shot%2520Learning%26entry.906535625%3DWenrui%2520Li%2520and%2520Penghong%2520Wang%2520and%2520Xingtao%2520Wang%2520and%2520Wangmeng%2520Zuo%2520and%2520Xiaopeng%2520Fan%2520and%2520Yonghong%2520Tian%26entry.1292438233%3D%2520%2520Audio-visual%2520zero-shot%2520learning%2520%2528ZSL%2529%2520has%2520been%2520extensively%2520researched%2520for%2520its%250Acapability%2520to%2520classify%2520video%2520data%2520from%2520unseen%2520classes%2520during%2520training.%250ANevertheless%252C%2520current%2520methodologies%2520often%2520struggle%2520with%2520background%2520scene%2520biases%250Aand%2520inadequate%2520motion%2520detail.%2520This%2520paper%2520proposes%2520a%2520novel%2520dual-stream%250AMulti-Timescale%2520Motion-Decoupled%2520Spiking%2520Transformer%2520%2528MDST%252B%252B%2529%252C%2520which%2520decouples%250Acontextual%2520semantic%2520information%2520and%2520sparse%2520dynamic%2520motion%2520information.%2520The%250Arecurrent%2520joint%2520learning%2520unit%2520is%2520proposed%2520to%2520extract%2520contextual%2520semantic%250Ainformation%2520and%2520capture%2520joint%2520knowledge%2520across%2520various%2520modalities%2520to%2520understand%250Athe%2520environment%2520of%2520actions.%2520By%2520converting%2520RGB%2520images%2520to%2520events%252C%2520our%2520method%250Acaptures%2520motion%2520information%2520more%2520accurately%2520and%2520mitigates%2520background%2520scene%250Abiases.%2520Moreover%252C%2520we%2520introduce%2520a%2520discrepancy%2520analysis%2520block%2520to%2520model%2520audio%250Amotion%2520information.%2520To%2520enhance%2520the%2520robustness%2520of%2520SNNs%2520in%2520extracting%2520temporal%250Aand%2520motion%2520cues%252C%2520we%2520dynamically%2520adjust%2520the%2520threshold%2520of%2520Leaky%250AIntegrate-and-Fire%2520neurons%2520based%2520on%2520global%2520motion%2520and%2520contextual%2520semantic%250Ainformation.%2520Our%2520experiments%2520validate%2520the%2520effectiveness%2520of%2520MDST%252B%252B%252C%250Ademonstrating%2520their%2520consistent%2520superiority%2520over%2520state-of-the-art%2520methods%2520on%250Amainstream%2520benchmarks.%2520Additionally%252C%2520incorporating%2520motion%2520and%2520multi-timescale%250Ainformation%2520significantly%2520improves%2520HM%2520and%2520ZSL%2520accuracy%2520by%252026.2%255C%2525%2520and%252039.9%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Timescale%20Motion-Decoupled%20Spiking%20Transformer%20for%20Audio-Visual%0A%20%20Zero-Shot%20Learning&entry.906535625=Wenrui%20Li%20and%20Penghong%20Wang%20and%20Xingtao%20Wang%20and%20Wangmeng%20Zuo%20and%20Xiaopeng%20Fan%20and%20Yonghong%20Tian&entry.1292438233=%20%20Audio-visual%20zero-shot%20learning%20%28ZSL%29%20has%20been%20extensively%20researched%20for%20its%0Acapability%20to%20classify%20video%20data%20from%20unseen%20classes%20during%20training.%0ANevertheless%2C%20current%20methodologies%20often%20struggle%20with%20background%20scene%20biases%0Aand%20inadequate%20motion%20detail.%20This%20paper%20proposes%20a%20novel%20dual-stream%0AMulti-Timescale%20Motion-Decoupled%20Spiking%20Transformer%20%28MDST%2B%2B%29%2C%20which%20decouples%0Acontextual%20semantic%20information%20and%20sparse%20dynamic%20motion%20information.%20The%0Arecurrent%20joint%20learning%20unit%20is%20proposed%20to%20extract%20contextual%20semantic%0Ainformation%20and%20capture%20joint%20knowledge%20across%20various%20modalities%20to%20understand%0Athe%20environment%20of%20actions.%20By%20converting%20RGB%20images%20to%20events%2C%20our%20method%0Acaptures%20motion%20information%20more%20accurately%20and%20mitigates%20background%20scene%0Abiases.%20Moreover%2C%20we%20introduce%20a%20discrepancy%20analysis%20block%20to%20model%20audio%0Amotion%20information.%20To%20enhance%20the%20robustness%20of%20SNNs%20in%20extracting%20temporal%0Aand%20motion%20cues%2C%20we%20dynamically%20adjust%20the%20threshold%20of%20Leaky%0AIntegrate-and-Fire%20neurons%20based%20on%20global%20motion%20and%20contextual%20semantic%0Ainformation.%20Our%20experiments%20validate%20the%20effectiveness%20of%20MDST%2B%2B%2C%0Ademonstrating%20their%20consistent%20superiority%20over%20state-of-the-art%20methods%20on%0Amainstream%20benchmarks.%20Additionally%2C%20incorporating%20motion%20and%20multi-timescale%0Ainformation%20significantly%20improves%20HM%20and%20ZSL%20accuracy%20by%2026.2%5C%25%20and%2039.9%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19938v1&entry.124074799=Read"},
{"title": "AdaTP: Attention-Debiased Token Pruning for Video Large Language Models", "author": "Fengyuan Sun and Leqi Shen and Hui Chen and Sicheng Zhao and Jungong Han and Guiguang Ding", "abstract": "  Video Large Language Models (Video LLMs) have achieved remarkable results in\nvideo understanding tasks. However, they often suffer from heavy computational\noverhead due to the large number of visual tokens generated from multiple video\nframes. Existing visual token compression methods often rely on attention\nscores from language models as guidance. However, these scores exhibit inherent\nbiases: global bias reflects a tendency to focus on the two ends of the visual\ntoken sequence, while local bias leads to an over-concentration on the same\nspatial positions across different frames. To address the issue of attention\nbias, we propose $\\textbf{A}$ttention-$\\textbf{D}$ebi$\\textbf{a}$sed\n$\\textbf{T}$oken $\\textbf{P}$runing for Video Large Language Models\n($\\textbf{AdaTP}$), a novel token pruning pipeline for Video LLMs. AdaTP\nintegrates two dedicated debiasing modules into the pipeline, targeting global\nattention bias and local attention bias, respectively. Without the need for\nadditional training, our method significantly reduces the computational\noverhead of Video LLMs while retaining the performance of vanilla models.\nExtensive evaluation shows that AdaTP achieves state-of-the-art performance in\nvarious commonly used video understanding benchmarks. In particular, on\nLLaVA-OneVision-7B, AdaTP maintains performance without degradation while using\nonly up to $27.3\\%$ FLOPs compared to the vanilla model. Our code will be\nreleased soon.\n", "link": "http://arxiv.org/abs/2505.20100v1", "date": "2025-05-26", "relevancy": 2.3041, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6078}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.554}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaTP%3A%20Attention-Debiased%20Token%20Pruning%20for%20Video%20Large%20Language%20Models&body=Title%3A%20AdaTP%3A%20Attention-Debiased%20Token%20Pruning%20for%20Video%20Large%20Language%20Models%0AAuthor%3A%20Fengyuan%20Sun%20and%20Leqi%20Shen%20and%20Hui%20Chen%20and%20Sicheng%20Zhao%20and%20Jungong%20Han%20and%20Guiguang%20Ding%0AAbstract%3A%20%20%20Video%20Large%20Language%20Models%20%28Video%20LLMs%29%20have%20achieved%20remarkable%20results%20in%0Avideo%20understanding%20tasks.%20However%2C%20they%20often%20suffer%20from%20heavy%20computational%0Aoverhead%20due%20to%20the%20large%20number%20of%20visual%20tokens%20generated%20from%20multiple%20video%0Aframes.%20Existing%20visual%20token%20compression%20methods%20often%20rely%20on%20attention%0Ascores%20from%20language%20models%20as%20guidance.%20However%2C%20these%20scores%20exhibit%20inherent%0Abiases%3A%20global%20bias%20reflects%20a%20tendency%20to%20focus%20on%20the%20two%20ends%20of%20the%20visual%0Atoken%20sequence%2C%20while%20local%20bias%20leads%20to%20an%20over-concentration%20on%20the%20same%0Aspatial%20positions%20across%20different%20frames.%20To%20address%20the%20issue%20of%20attention%0Abias%2C%20we%20propose%20%24%5Ctextbf%7BA%7D%24ttention-%24%5Ctextbf%7BD%7D%24ebi%24%5Ctextbf%7Ba%7D%24sed%0A%24%5Ctextbf%7BT%7D%24oken%20%24%5Ctextbf%7BP%7D%24runing%20for%20Video%20Large%20Language%20Models%0A%28%24%5Ctextbf%7BAdaTP%7D%24%29%2C%20a%20novel%20token%20pruning%20pipeline%20for%20Video%20LLMs.%20AdaTP%0Aintegrates%20two%20dedicated%20debiasing%20modules%20into%20the%20pipeline%2C%20targeting%20global%0Aattention%20bias%20and%20local%20attention%20bias%2C%20respectively.%20Without%20the%20need%20for%0Aadditional%20training%2C%20our%20method%20significantly%20reduces%20the%20computational%0Aoverhead%20of%20Video%20LLMs%20while%20retaining%20the%20performance%20of%20vanilla%20models.%0AExtensive%20evaluation%20shows%20that%20AdaTP%20achieves%20state-of-the-art%20performance%20in%0Avarious%20commonly%20used%20video%20understanding%20benchmarks.%20In%20particular%2C%20on%0ALLaVA-OneVision-7B%2C%20AdaTP%20maintains%20performance%20without%20degradation%20while%20using%0Aonly%20up%20to%20%2427.3%5C%25%24%20FLOPs%20compared%20to%20the%20vanilla%20model.%20Our%20code%20will%20be%0Areleased%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20100v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaTP%253A%2520Attention-Debiased%2520Token%2520Pruning%2520for%2520Video%2520Large%2520Language%2520Models%26entry.906535625%3DFengyuan%2520Sun%2520and%2520Leqi%2520Shen%2520and%2520Hui%2520Chen%2520and%2520Sicheng%2520Zhao%2520and%2520Jungong%2520Han%2520and%2520Guiguang%2520Ding%26entry.1292438233%3D%2520%2520Video%2520Large%2520Language%2520Models%2520%2528Video%2520LLMs%2529%2520have%2520achieved%2520remarkable%2520results%2520in%250Avideo%2520understanding%2520tasks.%2520However%252C%2520they%2520often%2520suffer%2520from%2520heavy%2520computational%250Aoverhead%2520due%2520to%2520the%2520large%2520number%2520of%2520visual%2520tokens%2520generated%2520from%2520multiple%2520video%250Aframes.%2520Existing%2520visual%2520token%2520compression%2520methods%2520often%2520rely%2520on%2520attention%250Ascores%2520from%2520language%2520models%2520as%2520guidance.%2520However%252C%2520these%2520scores%2520exhibit%2520inherent%250Abiases%253A%2520global%2520bias%2520reflects%2520a%2520tendency%2520to%2520focus%2520on%2520the%2520two%2520ends%2520of%2520the%2520visual%250Atoken%2520sequence%252C%2520while%2520local%2520bias%2520leads%2520to%2520an%2520over-concentration%2520on%2520the%2520same%250Aspatial%2520positions%2520across%2520different%2520frames.%2520To%2520address%2520the%2520issue%2520of%2520attention%250Abias%252C%2520we%2520propose%2520%2524%255Ctextbf%257BA%257D%2524ttention-%2524%255Ctextbf%257BD%257D%2524ebi%2524%255Ctextbf%257Ba%257D%2524sed%250A%2524%255Ctextbf%257BT%257D%2524oken%2520%2524%255Ctextbf%257BP%257D%2524runing%2520for%2520Video%2520Large%2520Language%2520Models%250A%2528%2524%255Ctextbf%257BAdaTP%257D%2524%2529%252C%2520a%2520novel%2520token%2520pruning%2520pipeline%2520for%2520Video%2520LLMs.%2520AdaTP%250Aintegrates%2520two%2520dedicated%2520debiasing%2520modules%2520into%2520the%2520pipeline%252C%2520targeting%2520global%250Aattention%2520bias%2520and%2520local%2520attention%2520bias%252C%2520respectively.%2520Without%2520the%2520need%2520for%250Aadditional%2520training%252C%2520our%2520method%2520significantly%2520reduces%2520the%2520computational%250Aoverhead%2520of%2520Video%2520LLMs%2520while%2520retaining%2520the%2520performance%2520of%2520vanilla%2520models.%250AExtensive%2520evaluation%2520shows%2520that%2520AdaTP%2520achieves%2520state-of-the-art%2520performance%2520in%250Avarious%2520commonly%2520used%2520video%2520understanding%2520benchmarks.%2520In%2520particular%252C%2520on%250ALLaVA-OneVision-7B%252C%2520AdaTP%2520maintains%2520performance%2520without%2520degradation%2520while%2520using%250Aonly%2520up%2520to%2520%252427.3%255C%2525%2524%2520FLOPs%2520compared%2520to%2520the%2520vanilla%2520model.%2520Our%2520code%2520will%2520be%250Areleased%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20100v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaTP%3A%20Attention-Debiased%20Token%20Pruning%20for%20Video%20Large%20Language%20Models&entry.906535625=Fengyuan%20Sun%20and%20Leqi%20Shen%20and%20Hui%20Chen%20and%20Sicheng%20Zhao%20and%20Jungong%20Han%20and%20Guiguang%20Ding&entry.1292438233=%20%20Video%20Large%20Language%20Models%20%28Video%20LLMs%29%20have%20achieved%20remarkable%20results%20in%0Avideo%20understanding%20tasks.%20However%2C%20they%20often%20suffer%20from%20heavy%20computational%0Aoverhead%20due%20to%20the%20large%20number%20of%20visual%20tokens%20generated%20from%20multiple%20video%0Aframes.%20Existing%20visual%20token%20compression%20methods%20often%20rely%20on%20attention%0Ascores%20from%20language%20models%20as%20guidance.%20However%2C%20these%20scores%20exhibit%20inherent%0Abiases%3A%20global%20bias%20reflects%20a%20tendency%20to%20focus%20on%20the%20two%20ends%20of%20the%20visual%0Atoken%20sequence%2C%20while%20local%20bias%20leads%20to%20an%20over-concentration%20on%20the%20same%0Aspatial%20positions%20across%20different%20frames.%20To%20address%20the%20issue%20of%20attention%0Abias%2C%20we%20propose%20%24%5Ctextbf%7BA%7D%24ttention-%24%5Ctextbf%7BD%7D%24ebi%24%5Ctextbf%7Ba%7D%24sed%0A%24%5Ctextbf%7BT%7D%24oken%20%24%5Ctextbf%7BP%7D%24runing%20for%20Video%20Large%20Language%20Models%0A%28%24%5Ctextbf%7BAdaTP%7D%24%29%2C%20a%20novel%20token%20pruning%20pipeline%20for%20Video%20LLMs.%20AdaTP%0Aintegrates%20two%20dedicated%20debiasing%20modules%20into%20the%20pipeline%2C%20targeting%20global%0Aattention%20bias%20and%20local%20attention%20bias%2C%20respectively.%20Without%20the%20need%20for%0Aadditional%20training%2C%20our%20method%20significantly%20reduces%20the%20computational%0Aoverhead%20of%20Video%20LLMs%20while%20retaining%20the%20performance%20of%20vanilla%20models.%0AExtensive%20evaluation%20shows%20that%20AdaTP%20achieves%20state-of-the-art%20performance%20in%0Avarious%20commonly%20used%20video%20understanding%20benchmarks.%20In%20particular%2C%20on%0ALLaVA-OneVision-7B%2C%20AdaTP%20maintains%20performance%20without%20degradation%20while%20using%0Aonly%20up%20to%20%2427.3%5C%25%24%20FLOPs%20compared%20to%20the%20vanilla%20model.%20Our%20code%20will%20be%0Areleased%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20100v1&entry.124074799=Read"},
{"title": "ReasonPlan: Unified Scene Prediction and Decision Reasoning for\n  Closed-loop Autonomous Driving", "author": "Xueyi Liu and Zuodong Zhong and Yuxin Guo and Yun-Fu Liu and Zhiguo Su and Qichao Zhang and Junli Wang and Yinfeng Gao and Yupeng Zheng and Qiao Lin and Huiyong Chen and Dongbin Zhao", "abstract": "  Due to the powerful vision-language reasoning and generalization abilities,\nmultimodal large language models (MLLMs) have garnered significant attention in\nthe field of end-to-end (E2E) autonomous driving. However, their application to\nclosed-loop systems remains underexplored, and current MLLM-based methods have\nnot shown clear superiority to mainstream E2E imitation learning approaches. In\nthis work, we propose ReasonPlan, a novel MLLM fine-tuning framework designed\nfor closed-loop driving through holistic reasoning with a self-supervised Next\nScene Prediction task and supervised Decision Chain-of-Thought process. This\ndual mechanism encourages the model to align visual representations with\nactionable driving context, while promoting interpretable and causally grounded\ndecision making. We curate a planning-oriented decision reasoning dataset,\nnamely PDR, comprising 210k diverse and high-quality samples. Our method\noutperforms the mainstream E2E imitation learning method by a large margin of\n19% L2 and 16.1 driving score on Bench2Drive benchmark. Furthermore, ReasonPlan\ndemonstrates strong zero-shot generalization on unseen DOS benchmark,\nhighlighting its adaptability in handling zero-shot corner cases. Code and\ndataset will be found in https://github.com/Liuxueyi/ReasonPlan.\n", "link": "http://arxiv.org/abs/2505.20024v1", "date": "2025-05-26", "relevancy": 2.3031, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5971}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5715}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReasonPlan%3A%20Unified%20Scene%20Prediction%20and%20Decision%20Reasoning%20for%0A%20%20Closed-loop%20Autonomous%20Driving&body=Title%3A%20ReasonPlan%3A%20Unified%20Scene%20Prediction%20and%20Decision%20Reasoning%20for%0A%20%20Closed-loop%20Autonomous%20Driving%0AAuthor%3A%20Xueyi%20Liu%20and%20Zuodong%20Zhong%20and%20Yuxin%20Guo%20and%20Yun-Fu%20Liu%20and%20Zhiguo%20Su%20and%20Qichao%20Zhang%20and%20Junli%20Wang%20and%20Yinfeng%20Gao%20and%20Yupeng%20Zheng%20and%20Qiao%20Lin%20and%20Huiyong%20Chen%20and%20Dongbin%20Zhao%0AAbstract%3A%20%20%20Due%20to%20the%20powerful%20vision-language%20reasoning%20and%20generalization%20abilities%2C%0Amultimodal%20large%20language%20models%20%28MLLMs%29%20have%20garnered%20significant%20attention%20in%0Athe%20field%20of%20end-to-end%20%28E2E%29%20autonomous%20driving.%20However%2C%20their%20application%20to%0Aclosed-loop%20systems%20remains%20underexplored%2C%20and%20current%20MLLM-based%20methods%20have%0Anot%20shown%20clear%20superiority%20to%20mainstream%20E2E%20imitation%20learning%20approaches.%20In%0Athis%20work%2C%20we%20propose%20ReasonPlan%2C%20a%20novel%20MLLM%20fine-tuning%20framework%20designed%0Afor%20closed-loop%20driving%20through%20holistic%20reasoning%20with%20a%20self-supervised%20Next%0AScene%20Prediction%20task%20and%20supervised%20Decision%20Chain-of-Thought%20process.%20This%0Adual%20mechanism%20encourages%20the%20model%20to%20align%20visual%20representations%20with%0Aactionable%20driving%20context%2C%20while%20promoting%20interpretable%20and%20causally%20grounded%0Adecision%20making.%20We%20curate%20a%20planning-oriented%20decision%20reasoning%20dataset%2C%0Anamely%20PDR%2C%20comprising%20210k%20diverse%20and%20high-quality%20samples.%20Our%20method%0Aoutperforms%20the%20mainstream%20E2E%20imitation%20learning%20method%20by%20a%20large%20margin%20of%0A19%25%20L2%20and%2016.1%20driving%20score%20on%20Bench2Drive%20benchmark.%20Furthermore%2C%20ReasonPlan%0Ademonstrates%20strong%20zero-shot%20generalization%20on%20unseen%20DOS%20benchmark%2C%0Ahighlighting%20its%20adaptability%20in%20handling%20zero-shot%20corner%20cases.%20Code%20and%0Adataset%20will%20be%20found%20in%20https%3A//github.com/Liuxueyi/ReasonPlan.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20024v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasonPlan%253A%2520Unified%2520Scene%2520Prediction%2520and%2520Decision%2520Reasoning%2520for%250A%2520%2520Closed-loop%2520Autonomous%2520Driving%26entry.906535625%3DXueyi%2520Liu%2520and%2520Zuodong%2520Zhong%2520and%2520Yuxin%2520Guo%2520and%2520Yun-Fu%2520Liu%2520and%2520Zhiguo%2520Su%2520and%2520Qichao%2520Zhang%2520and%2520Junli%2520Wang%2520and%2520Yinfeng%2520Gao%2520and%2520Yupeng%2520Zheng%2520and%2520Qiao%2520Lin%2520and%2520Huiyong%2520Chen%2520and%2520Dongbin%2520Zhao%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520powerful%2520vision-language%2520reasoning%2520and%2520generalization%2520abilities%252C%250Amultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520garnered%2520significant%2520attention%2520in%250Athe%2520field%2520of%2520end-to-end%2520%2528E2E%2529%2520autonomous%2520driving.%2520However%252C%2520their%2520application%2520to%250Aclosed-loop%2520systems%2520remains%2520underexplored%252C%2520and%2520current%2520MLLM-based%2520methods%2520have%250Anot%2520shown%2520clear%2520superiority%2520to%2520mainstream%2520E2E%2520imitation%2520learning%2520approaches.%2520In%250Athis%2520work%252C%2520we%2520propose%2520ReasonPlan%252C%2520a%2520novel%2520MLLM%2520fine-tuning%2520framework%2520designed%250Afor%2520closed-loop%2520driving%2520through%2520holistic%2520reasoning%2520with%2520a%2520self-supervised%2520Next%250AScene%2520Prediction%2520task%2520and%2520supervised%2520Decision%2520Chain-of-Thought%2520process.%2520This%250Adual%2520mechanism%2520encourages%2520the%2520model%2520to%2520align%2520visual%2520representations%2520with%250Aactionable%2520driving%2520context%252C%2520while%2520promoting%2520interpretable%2520and%2520causally%2520grounded%250Adecision%2520making.%2520We%2520curate%2520a%2520planning-oriented%2520decision%2520reasoning%2520dataset%252C%250Anamely%2520PDR%252C%2520comprising%2520210k%2520diverse%2520and%2520high-quality%2520samples.%2520Our%2520method%250Aoutperforms%2520the%2520mainstream%2520E2E%2520imitation%2520learning%2520method%2520by%2520a%2520large%2520margin%2520of%250A19%2525%2520L2%2520and%252016.1%2520driving%2520score%2520on%2520Bench2Drive%2520benchmark.%2520Furthermore%252C%2520ReasonPlan%250Ademonstrates%2520strong%2520zero-shot%2520generalization%2520on%2520unseen%2520DOS%2520benchmark%252C%250Ahighlighting%2520its%2520adaptability%2520in%2520handling%2520zero-shot%2520corner%2520cases.%2520Code%2520and%250Adataset%2520will%2520be%2520found%2520in%2520https%253A//github.com/Liuxueyi/ReasonPlan.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20024v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReasonPlan%3A%20Unified%20Scene%20Prediction%20and%20Decision%20Reasoning%20for%0A%20%20Closed-loop%20Autonomous%20Driving&entry.906535625=Xueyi%20Liu%20and%20Zuodong%20Zhong%20and%20Yuxin%20Guo%20and%20Yun-Fu%20Liu%20and%20Zhiguo%20Su%20and%20Qichao%20Zhang%20and%20Junli%20Wang%20and%20Yinfeng%20Gao%20and%20Yupeng%20Zheng%20and%20Qiao%20Lin%20and%20Huiyong%20Chen%20and%20Dongbin%20Zhao&entry.1292438233=%20%20Due%20to%20the%20powerful%20vision-language%20reasoning%20and%20generalization%20abilities%2C%0Amultimodal%20large%20language%20models%20%28MLLMs%29%20have%20garnered%20significant%20attention%20in%0Athe%20field%20of%20end-to-end%20%28E2E%29%20autonomous%20driving.%20However%2C%20their%20application%20to%0Aclosed-loop%20systems%20remains%20underexplored%2C%20and%20current%20MLLM-based%20methods%20have%0Anot%20shown%20clear%20superiority%20to%20mainstream%20E2E%20imitation%20learning%20approaches.%20In%0Athis%20work%2C%20we%20propose%20ReasonPlan%2C%20a%20novel%20MLLM%20fine-tuning%20framework%20designed%0Afor%20closed-loop%20driving%20through%20holistic%20reasoning%20with%20a%20self-supervised%20Next%0AScene%20Prediction%20task%20and%20supervised%20Decision%20Chain-of-Thought%20process.%20This%0Adual%20mechanism%20encourages%20the%20model%20to%20align%20visual%20representations%20with%0Aactionable%20driving%20context%2C%20while%20promoting%20interpretable%20and%20causally%20grounded%0Adecision%20making.%20We%20curate%20a%20planning-oriented%20decision%20reasoning%20dataset%2C%0Anamely%20PDR%2C%20comprising%20210k%20diverse%20and%20high-quality%20samples.%20Our%20method%0Aoutperforms%20the%20mainstream%20E2E%20imitation%20learning%20method%20by%20a%20large%20margin%20of%0A19%25%20L2%20and%2016.1%20driving%20score%20on%20Bench2Drive%20benchmark.%20Furthermore%2C%20ReasonPlan%0Ademonstrates%20strong%20zero-shot%20generalization%20on%20unseen%20DOS%20benchmark%2C%0Ahighlighting%20its%20adaptability%20in%20handling%20zero-shot%20corner%20cases.%20Code%20and%0Adataset%20will%20be%20found%20in%20https%3A//github.com/Liuxueyi/ReasonPlan.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20024v1&entry.124074799=Read"},
{"title": "FinLoRA: Benchmarking LoRA Methods for Fine-Tuning LLMs on Financial\n  Datasets", "author": "Dannong Wang and Jaisal Patel and Daochen Zha and Steve Y. Yang and Xiao-Yang Liu", "abstract": "  Low-rank adaptation (LoRA) methods show great potential for scaling\npre-trained general-purpose Large Language Models (LLMs) to hundreds or\nthousands of use scenarios. However, their efficacy in high-stakes domains like\nfinance is rarely explored, e.g., passing CFA exams and analyzing SEC filings.\nIn this paper, we present the open-source FinLoRA project that benchmarks LoRA\nmethods on both general and highly professional financial tasks. First, we\ncurated 19 datasets covering diverse financial applications; in particular, we\ncreated four novel XBRL analysis datasets based on 150 SEC filings. Second, we\nevaluated five LoRA methods and five base LLMs. Finally, we provide extensive\nexperimental results in terms of accuracy, F1, and BERTScore and report\ncomputational cost in terms of time and GPU memory during fine-tuning and\ninference stages. We find that LoRA methods achieved substantial performance\ngains of 36\\% on average over base models. Our FinLoRA project provides an\naffordable and scalable approach to democratize financial intelligence to the\ngeneral public. Datasets, LoRA adapters, code, and documentation are available\nat https://github.com/Open-Finance-Lab/FinLoRA\n", "link": "http://arxiv.org/abs/2505.19819v1", "date": "2025-05-26", "relevancy": 2.2929, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4633}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FinLoRA%3A%20Benchmarking%20LoRA%20Methods%20for%20Fine-Tuning%20LLMs%20on%20Financial%0A%20%20Datasets&body=Title%3A%20FinLoRA%3A%20Benchmarking%20LoRA%20Methods%20for%20Fine-Tuning%20LLMs%20on%20Financial%0A%20%20Datasets%0AAuthor%3A%20Dannong%20Wang%20and%20Jaisal%20Patel%20and%20Daochen%20Zha%20and%20Steve%20Y.%20Yang%20and%20Xiao-Yang%20Liu%0AAbstract%3A%20%20%20Low-rank%20adaptation%20%28LoRA%29%20methods%20show%20great%20potential%20for%20scaling%0Apre-trained%20general-purpose%20Large%20Language%20Models%20%28LLMs%29%20to%20hundreds%20or%0Athousands%20of%20use%20scenarios.%20However%2C%20their%20efficacy%20in%20high-stakes%20domains%20like%0Afinance%20is%20rarely%20explored%2C%20e.g.%2C%20passing%20CFA%20exams%20and%20analyzing%20SEC%20filings.%0AIn%20this%20paper%2C%20we%20present%20the%20open-source%20FinLoRA%20project%20that%20benchmarks%20LoRA%0Amethods%20on%20both%20general%20and%20highly%20professional%20financial%20tasks.%20First%2C%20we%0Acurated%2019%20datasets%20covering%20diverse%20financial%20applications%3B%20in%20particular%2C%20we%0Acreated%20four%20novel%20XBRL%20analysis%20datasets%20based%20on%20150%20SEC%20filings.%20Second%2C%20we%0Aevaluated%20five%20LoRA%20methods%20and%20five%20base%20LLMs.%20Finally%2C%20we%20provide%20extensive%0Aexperimental%20results%20in%20terms%20of%20accuracy%2C%20F1%2C%20and%20BERTScore%20and%20report%0Acomputational%20cost%20in%20terms%20of%20time%20and%20GPU%20memory%20during%20fine-tuning%20and%0Ainference%20stages.%20We%20find%20that%20LoRA%20methods%20achieved%20substantial%20performance%0Agains%20of%2036%5C%25%20on%20average%20over%20base%20models.%20Our%20FinLoRA%20project%20provides%20an%0Aaffordable%20and%20scalable%20approach%20to%20democratize%20financial%20intelligence%20to%20the%0Ageneral%20public.%20Datasets%2C%20LoRA%20adapters%2C%20code%2C%20and%20documentation%20are%20available%0Aat%20https%3A//github.com/Open-Finance-Lab/FinLoRA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinLoRA%253A%2520Benchmarking%2520LoRA%2520Methods%2520for%2520Fine-Tuning%2520LLMs%2520on%2520Financial%250A%2520%2520Datasets%26entry.906535625%3DDannong%2520Wang%2520and%2520Jaisal%2520Patel%2520and%2520Daochen%2520Zha%2520and%2520Steve%2520Y.%2520Yang%2520and%2520Xiao-Yang%2520Liu%26entry.1292438233%3D%2520%2520Low-rank%2520adaptation%2520%2528LoRA%2529%2520methods%2520show%2520great%2520potential%2520for%2520scaling%250Apre-trained%2520general-purpose%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520hundreds%2520or%250Athousands%2520of%2520use%2520scenarios.%2520However%252C%2520their%2520efficacy%2520in%2520high-stakes%2520domains%2520like%250Afinance%2520is%2520rarely%2520explored%252C%2520e.g.%252C%2520passing%2520CFA%2520exams%2520and%2520analyzing%2520SEC%2520filings.%250AIn%2520this%2520paper%252C%2520we%2520present%2520the%2520open-source%2520FinLoRA%2520project%2520that%2520benchmarks%2520LoRA%250Amethods%2520on%2520both%2520general%2520and%2520highly%2520professional%2520financial%2520tasks.%2520First%252C%2520we%250Acurated%252019%2520datasets%2520covering%2520diverse%2520financial%2520applications%253B%2520in%2520particular%252C%2520we%250Acreated%2520four%2520novel%2520XBRL%2520analysis%2520datasets%2520based%2520on%2520150%2520SEC%2520filings.%2520Second%252C%2520we%250Aevaluated%2520five%2520LoRA%2520methods%2520and%2520five%2520base%2520LLMs.%2520Finally%252C%2520we%2520provide%2520extensive%250Aexperimental%2520results%2520in%2520terms%2520of%2520accuracy%252C%2520F1%252C%2520and%2520BERTScore%2520and%2520report%250Acomputational%2520cost%2520in%2520terms%2520of%2520time%2520and%2520GPU%2520memory%2520during%2520fine-tuning%2520and%250Ainference%2520stages.%2520We%2520find%2520that%2520LoRA%2520methods%2520achieved%2520substantial%2520performance%250Agains%2520of%252036%255C%2525%2520on%2520average%2520over%2520base%2520models.%2520Our%2520FinLoRA%2520project%2520provides%2520an%250Aaffordable%2520and%2520scalable%2520approach%2520to%2520democratize%2520financial%2520intelligence%2520to%2520the%250Ageneral%2520public.%2520Datasets%252C%2520LoRA%2520adapters%252C%2520code%252C%2520and%2520documentation%2520are%2520available%250Aat%2520https%253A//github.com/Open-Finance-Lab/FinLoRA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FinLoRA%3A%20Benchmarking%20LoRA%20Methods%20for%20Fine-Tuning%20LLMs%20on%20Financial%0A%20%20Datasets&entry.906535625=Dannong%20Wang%20and%20Jaisal%20Patel%20and%20Daochen%20Zha%20and%20Steve%20Y.%20Yang%20and%20Xiao-Yang%20Liu&entry.1292438233=%20%20Low-rank%20adaptation%20%28LoRA%29%20methods%20show%20great%20potential%20for%20scaling%0Apre-trained%20general-purpose%20Large%20Language%20Models%20%28LLMs%29%20to%20hundreds%20or%0Athousands%20of%20use%20scenarios.%20However%2C%20their%20efficacy%20in%20high-stakes%20domains%20like%0Afinance%20is%20rarely%20explored%2C%20e.g.%2C%20passing%20CFA%20exams%20and%20analyzing%20SEC%20filings.%0AIn%20this%20paper%2C%20we%20present%20the%20open-source%20FinLoRA%20project%20that%20benchmarks%20LoRA%0Amethods%20on%20both%20general%20and%20highly%20professional%20financial%20tasks.%20First%2C%20we%0Acurated%2019%20datasets%20covering%20diverse%20financial%20applications%3B%20in%20particular%2C%20we%0Acreated%20four%20novel%20XBRL%20analysis%20datasets%20based%20on%20150%20SEC%20filings.%20Second%2C%20we%0Aevaluated%20five%20LoRA%20methods%20and%20five%20base%20LLMs.%20Finally%2C%20we%20provide%20extensive%0Aexperimental%20results%20in%20terms%20of%20accuracy%2C%20F1%2C%20and%20BERTScore%20and%20report%0Acomputational%20cost%20in%20terms%20of%20time%20and%20GPU%20memory%20during%20fine-tuning%20and%0Ainference%20stages.%20We%20find%20that%20LoRA%20methods%20achieved%20substantial%20performance%0Agains%20of%2036%5C%25%20on%20average%20over%20base%20models.%20Our%20FinLoRA%20project%20provides%20an%0Aaffordable%20and%20scalable%20approach%20to%20democratize%20financial%20intelligence%20to%20the%0Ageneral%20public.%20Datasets%2C%20LoRA%20adapters%2C%20code%2C%20and%20documentation%20are%20available%0Aat%20https%3A//github.com/Open-Finance-Lab/FinLoRA%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19819v1&entry.124074799=Read"},
{"title": "Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases", "author": "Sophie Starck and Vasiliki Sideri-Lampretsa and Bernhard Kainz and Martin J. Menten and Tamara T. Mueller and Daniel Rueckert", "abstract": "  Anatomical atlases are widely used for population studies and analysis.\nConditional atlases target a specific sub-population defined via certain\nconditions, such as demographics or pathologies, and allow for the\ninvestigation of fine-grained anatomical differences like morphological changes\nassociated with ageing or disease. Existing approaches use either\nregistration-based methods that are often unable to handle large anatomical\nvariations or generative adversarial models, which are challenging to train\nsince they can suffer from training instabilities. Instead of generating\natlases directly in as intensities, we propose using latent diffusion models to\ngenerate deformation fields, which transform a general population atlas into\none representing a specific sub-population. Our approach ensures structural\nintegrity, enhances interpretability and avoids hallucinations that may arise\nduring direct image synthesis by generating this deformation field and\nregularising it using a neighbourhood of images. We compare our method to\nseveral state-of-the-art atlas generation methods using brain MR images from\nthe UK Biobank. Our method generates highly realistic atlases with smooth\ntransformations and high anatomical fidelity, outperforming existing baselines.\nWe demonstrate the quality of these atlases through comprehensive evaluations,\nincluding quantitative metrics for anatomical accuracy, perceptual similarity,\nand qualitative analyses displaying the consistency and realism of the\ngenerated atlases.\n", "link": "http://arxiv.org/abs/2403.16776v2", "date": "2025-05-26", "relevancy": 2.2759, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5859}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5656}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diff-Def%3A%20Diffusion-Generated%20Deformation%20Fields%20for%20Conditional%20Atlases&body=Title%3A%20Diff-Def%3A%20Diffusion-Generated%20Deformation%20Fields%20for%20Conditional%20Atlases%0AAuthor%3A%20Sophie%20Starck%20and%20Vasiliki%20Sideri-Lampretsa%20and%20Bernhard%20Kainz%20and%20Martin%20J.%20Menten%20and%20Tamara%20T.%20Mueller%20and%20Daniel%20Rueckert%0AAbstract%3A%20%20%20Anatomical%20atlases%20are%20widely%20used%20for%20population%20studies%20and%20analysis.%0AConditional%20atlases%20target%20a%20specific%20sub-population%20defined%20via%20certain%0Aconditions%2C%20such%20as%20demographics%20or%20pathologies%2C%20and%20allow%20for%20the%0Ainvestigation%20of%20fine-grained%20anatomical%20differences%20like%20morphological%20changes%0Aassociated%20with%20ageing%20or%20disease.%20Existing%20approaches%20use%20either%0Aregistration-based%20methods%20that%20are%20often%20unable%20to%20handle%20large%20anatomical%0Avariations%20or%20generative%20adversarial%20models%2C%20which%20are%20challenging%20to%20train%0Asince%20they%20can%20suffer%20from%20training%20instabilities.%20Instead%20of%20generating%0Aatlases%20directly%20in%20as%20intensities%2C%20we%20propose%20using%20latent%20diffusion%20models%20to%0Agenerate%20deformation%20fields%2C%20which%20transform%20a%20general%20population%20atlas%20into%0Aone%20representing%20a%20specific%20sub-population.%20Our%20approach%20ensures%20structural%0Aintegrity%2C%20enhances%20interpretability%20and%20avoids%20hallucinations%20that%20may%20arise%0Aduring%20direct%20image%20synthesis%20by%20generating%20this%20deformation%20field%20and%0Aregularising%20it%20using%20a%20neighbourhood%20of%20images.%20We%20compare%20our%20method%20to%0Aseveral%20state-of-the-art%20atlas%20generation%20methods%20using%20brain%20MR%20images%20from%0Athe%20UK%20Biobank.%20Our%20method%20generates%20highly%20realistic%20atlases%20with%20smooth%0Atransformations%20and%20high%20anatomical%20fidelity%2C%20outperforming%20existing%20baselines.%0AWe%20demonstrate%20the%20quality%20of%20these%20atlases%20through%20comprehensive%20evaluations%2C%0Aincluding%20quantitative%20metrics%20for%20anatomical%20accuracy%2C%20perceptual%20similarity%2C%0Aand%20qualitative%20analyses%20displaying%20the%20consistency%20and%20realism%20of%20the%0Agenerated%20atlases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16776v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiff-Def%253A%2520Diffusion-Generated%2520Deformation%2520Fields%2520for%2520Conditional%2520Atlases%26entry.906535625%3DSophie%2520Starck%2520and%2520Vasiliki%2520Sideri-Lampretsa%2520and%2520Bernhard%2520Kainz%2520and%2520Martin%2520J.%2520Menten%2520and%2520Tamara%2520T.%2520Mueller%2520and%2520Daniel%2520Rueckert%26entry.1292438233%3D%2520%2520Anatomical%2520atlases%2520are%2520widely%2520used%2520for%2520population%2520studies%2520and%2520analysis.%250AConditional%2520atlases%2520target%2520a%2520specific%2520sub-population%2520defined%2520via%2520certain%250Aconditions%252C%2520such%2520as%2520demographics%2520or%2520pathologies%252C%2520and%2520allow%2520for%2520the%250Ainvestigation%2520of%2520fine-grained%2520anatomical%2520differences%2520like%2520morphological%2520changes%250Aassociated%2520with%2520ageing%2520or%2520disease.%2520Existing%2520approaches%2520use%2520either%250Aregistration-based%2520methods%2520that%2520are%2520often%2520unable%2520to%2520handle%2520large%2520anatomical%250Avariations%2520or%2520generative%2520adversarial%2520models%252C%2520which%2520are%2520challenging%2520to%2520train%250Asince%2520they%2520can%2520suffer%2520from%2520training%2520instabilities.%2520Instead%2520of%2520generating%250Aatlases%2520directly%2520in%2520as%2520intensities%252C%2520we%2520propose%2520using%2520latent%2520diffusion%2520models%2520to%250Agenerate%2520deformation%2520fields%252C%2520which%2520transform%2520a%2520general%2520population%2520atlas%2520into%250Aone%2520representing%2520a%2520specific%2520sub-population.%2520Our%2520approach%2520ensures%2520structural%250Aintegrity%252C%2520enhances%2520interpretability%2520and%2520avoids%2520hallucinations%2520that%2520may%2520arise%250Aduring%2520direct%2520image%2520synthesis%2520by%2520generating%2520this%2520deformation%2520field%2520and%250Aregularising%2520it%2520using%2520a%2520neighbourhood%2520of%2520images.%2520We%2520compare%2520our%2520method%2520to%250Aseveral%2520state-of-the-art%2520atlas%2520generation%2520methods%2520using%2520brain%2520MR%2520images%2520from%250Athe%2520UK%2520Biobank.%2520Our%2520method%2520generates%2520highly%2520realistic%2520atlases%2520with%2520smooth%250Atransformations%2520and%2520high%2520anatomical%2520fidelity%252C%2520outperforming%2520existing%2520baselines.%250AWe%2520demonstrate%2520the%2520quality%2520of%2520these%2520atlases%2520through%2520comprehensive%2520evaluations%252C%250Aincluding%2520quantitative%2520metrics%2520for%2520anatomical%2520accuracy%252C%2520perceptual%2520similarity%252C%250Aand%2520qualitative%2520analyses%2520displaying%2520the%2520consistency%2520and%2520realism%2520of%2520the%250Agenerated%2520atlases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16776v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diff-Def%3A%20Diffusion-Generated%20Deformation%20Fields%20for%20Conditional%20Atlases&entry.906535625=Sophie%20Starck%20and%20Vasiliki%20Sideri-Lampretsa%20and%20Bernhard%20Kainz%20and%20Martin%20J.%20Menten%20and%20Tamara%20T.%20Mueller%20and%20Daniel%20Rueckert&entry.1292438233=%20%20Anatomical%20atlases%20are%20widely%20used%20for%20population%20studies%20and%20analysis.%0AConditional%20atlases%20target%20a%20specific%20sub-population%20defined%20via%20certain%0Aconditions%2C%20such%20as%20demographics%20or%20pathologies%2C%20and%20allow%20for%20the%0Ainvestigation%20of%20fine-grained%20anatomical%20differences%20like%20morphological%20changes%0Aassociated%20with%20ageing%20or%20disease.%20Existing%20approaches%20use%20either%0Aregistration-based%20methods%20that%20are%20often%20unable%20to%20handle%20large%20anatomical%0Avariations%20or%20generative%20adversarial%20models%2C%20which%20are%20challenging%20to%20train%0Asince%20they%20can%20suffer%20from%20training%20instabilities.%20Instead%20of%20generating%0Aatlases%20directly%20in%20as%20intensities%2C%20we%20propose%20using%20latent%20diffusion%20models%20to%0Agenerate%20deformation%20fields%2C%20which%20transform%20a%20general%20population%20atlas%20into%0Aone%20representing%20a%20specific%20sub-population.%20Our%20approach%20ensures%20structural%0Aintegrity%2C%20enhances%20interpretability%20and%20avoids%20hallucinations%20that%20may%20arise%0Aduring%20direct%20image%20synthesis%20by%20generating%20this%20deformation%20field%20and%0Aregularising%20it%20using%20a%20neighbourhood%20of%20images.%20We%20compare%20our%20method%20to%0Aseveral%20state-of-the-art%20atlas%20generation%20methods%20using%20brain%20MR%20images%20from%0Athe%20UK%20Biobank.%20Our%20method%20generates%20highly%20realistic%20atlases%20with%20smooth%0Atransformations%20and%20high%20anatomical%20fidelity%2C%20outperforming%20existing%20baselines.%0AWe%20demonstrate%20the%20quality%20of%20these%20atlases%20through%20comprehensive%20evaluations%2C%0Aincluding%20quantitative%20metrics%20for%20anatomical%20accuracy%2C%20perceptual%20similarity%2C%0Aand%20qualitative%20analyses%20displaying%20the%20consistency%20and%20realism%20of%20the%0Agenerated%20atlases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16776v2&entry.124074799=Read"},
{"title": "Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition\n  Chain-of-Thought", "author": "Chao Huang and Benfeng Wang and Jie Wen and Chengliang Liu and Wei Wang and Li Shen and Xiaochun Cao", "abstract": "  Recent advancements in reasoning capability of Multimodal Large Language\nModels (MLLMs) demonstrate its effectiveness in tackling complex visual tasks.\nHowever, existing MLLM-based Video Anomaly Detection (VAD) methods remain\nlimited to shallow anomaly descriptions without deep reasoning. In this paper,\nwe propose a new task named Video Anomaly Reasoning (VAR), which aims to enable\ndeep analysis and understanding of anomalies in the video by requiring MLLMs to\nthink explicitly before answering. To this end, we propose Vad-R1, an\nend-to-end MLLM-based framework for VAR. Specifically, we design a\nPerception-to-Cognition Chain-of-Thought (P2C-CoT) that simulates the human\nprocess of recognizing anomalies, guiding the MLLM to reason anomaly\nstep-by-step. Based on the structured P2C-CoT, we construct Vad-Reasoning, a\ndedicated dataset for VAR. Furthermore, we propose an improved reinforcement\nlearning algorithm AVA-GRPO, which explicitly incentivizes the anomaly\nreasoning capability of MLLMs through a self-verification mechanism with\nlimited annotations. Experimental results demonstrate that Vad-R1 achieves\nsuperior performance, outperforming both open-source and proprietary models on\nVAD and VAR tasks. Codes and datasets will be released at\nhttps://github.com/wbfwonderful/Vad-R1.\n", "link": "http://arxiv.org/abs/2505.19877v1", "date": "2025-05-26", "relevancy": 2.2726, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5683}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5683}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vad-R1%3A%20Towards%20Video%20Anomaly%20Reasoning%20via%20Perception-to-Cognition%0A%20%20Chain-of-Thought&body=Title%3A%20Vad-R1%3A%20Towards%20Video%20Anomaly%20Reasoning%20via%20Perception-to-Cognition%0A%20%20Chain-of-Thought%0AAuthor%3A%20Chao%20Huang%20and%20Benfeng%20Wang%20and%20Jie%20Wen%20and%20Chengliang%20Liu%20and%20Wei%20Wang%20and%20Li%20Shen%20and%20Xiaochun%20Cao%0AAbstract%3A%20%20%20Recent%20advancements%20in%20reasoning%20capability%20of%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20demonstrate%20its%20effectiveness%20in%20tackling%20complex%20visual%20tasks.%0AHowever%2C%20existing%20MLLM-based%20Video%20Anomaly%20Detection%20%28VAD%29%20methods%20remain%0Alimited%20to%20shallow%20anomaly%20descriptions%20without%20deep%20reasoning.%20In%20this%20paper%2C%0Awe%20propose%20a%20new%20task%20named%20Video%20Anomaly%20Reasoning%20%28VAR%29%2C%20which%20aims%20to%20enable%0Adeep%20analysis%20and%20understanding%20of%20anomalies%20in%20the%20video%20by%20requiring%20MLLMs%20to%0Athink%20explicitly%20before%20answering.%20To%20this%20end%2C%20we%20propose%20Vad-R1%2C%20an%0Aend-to-end%20MLLM-based%20framework%20for%20VAR.%20Specifically%2C%20we%20design%20a%0APerception-to-Cognition%20Chain-of-Thought%20%28P2C-CoT%29%20that%20simulates%20the%20human%0Aprocess%20of%20recognizing%20anomalies%2C%20guiding%20the%20MLLM%20to%20reason%20anomaly%0Astep-by-step.%20Based%20on%20the%20structured%20P2C-CoT%2C%20we%20construct%20Vad-Reasoning%2C%20a%0Adedicated%20dataset%20for%20VAR.%20Furthermore%2C%20we%20propose%20an%20improved%20reinforcement%0Alearning%20algorithm%20AVA-GRPO%2C%20which%20explicitly%20incentivizes%20the%20anomaly%0Areasoning%20capability%20of%20MLLMs%20through%20a%20self-verification%20mechanism%20with%0Alimited%20annotations.%20Experimental%20results%20demonstrate%20that%20Vad-R1%20achieves%0Asuperior%20performance%2C%20outperforming%20both%20open-source%20and%20proprietary%20models%20on%0AVAD%20and%20VAR%20tasks.%20Codes%20and%20datasets%20will%20be%20released%20at%0Ahttps%3A//github.com/wbfwonderful/Vad-R1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19877v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVad-R1%253A%2520Towards%2520Video%2520Anomaly%2520Reasoning%2520via%2520Perception-to-Cognition%250A%2520%2520Chain-of-Thought%26entry.906535625%3DChao%2520Huang%2520and%2520Benfeng%2520Wang%2520and%2520Jie%2520Wen%2520and%2520Chengliang%2520Liu%2520and%2520Wei%2520Wang%2520and%2520Li%2520Shen%2520and%2520Xiaochun%2520Cao%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520reasoning%2520capability%2520of%2520Multimodal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529%2520demonstrate%2520its%2520effectiveness%2520in%2520tackling%2520complex%2520visual%2520tasks.%250AHowever%252C%2520existing%2520MLLM-based%2520Video%2520Anomaly%2520Detection%2520%2528VAD%2529%2520methods%2520remain%250Alimited%2520to%2520shallow%2520anomaly%2520descriptions%2520without%2520deep%2520reasoning.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520new%2520task%2520named%2520Video%2520Anomaly%2520Reasoning%2520%2528VAR%2529%252C%2520which%2520aims%2520to%2520enable%250Adeep%2520analysis%2520and%2520understanding%2520of%2520anomalies%2520in%2520the%2520video%2520by%2520requiring%2520MLLMs%2520to%250Athink%2520explicitly%2520before%2520answering.%2520To%2520this%2520end%252C%2520we%2520propose%2520Vad-R1%252C%2520an%250Aend-to-end%2520MLLM-based%2520framework%2520for%2520VAR.%2520Specifically%252C%2520we%2520design%2520a%250APerception-to-Cognition%2520Chain-of-Thought%2520%2528P2C-CoT%2529%2520that%2520simulates%2520the%2520human%250Aprocess%2520of%2520recognizing%2520anomalies%252C%2520guiding%2520the%2520MLLM%2520to%2520reason%2520anomaly%250Astep-by-step.%2520Based%2520on%2520the%2520structured%2520P2C-CoT%252C%2520we%2520construct%2520Vad-Reasoning%252C%2520a%250Adedicated%2520dataset%2520for%2520VAR.%2520Furthermore%252C%2520we%2520propose%2520an%2520improved%2520reinforcement%250Alearning%2520algorithm%2520AVA-GRPO%252C%2520which%2520explicitly%2520incentivizes%2520the%2520anomaly%250Areasoning%2520capability%2520of%2520MLLMs%2520through%2520a%2520self-verification%2520mechanism%2520with%250Alimited%2520annotations.%2520Experimental%2520results%2520demonstrate%2520that%2520Vad-R1%2520achieves%250Asuperior%2520performance%252C%2520outperforming%2520both%2520open-source%2520and%2520proprietary%2520models%2520on%250AVAD%2520and%2520VAR%2520tasks.%2520Codes%2520and%2520datasets%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/wbfwonderful/Vad-R1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19877v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vad-R1%3A%20Towards%20Video%20Anomaly%20Reasoning%20via%20Perception-to-Cognition%0A%20%20Chain-of-Thought&entry.906535625=Chao%20Huang%20and%20Benfeng%20Wang%20and%20Jie%20Wen%20and%20Chengliang%20Liu%20and%20Wei%20Wang%20and%20Li%20Shen%20and%20Xiaochun%20Cao&entry.1292438233=%20%20Recent%20advancements%20in%20reasoning%20capability%20of%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20demonstrate%20its%20effectiveness%20in%20tackling%20complex%20visual%20tasks.%0AHowever%2C%20existing%20MLLM-based%20Video%20Anomaly%20Detection%20%28VAD%29%20methods%20remain%0Alimited%20to%20shallow%20anomaly%20descriptions%20without%20deep%20reasoning.%20In%20this%20paper%2C%0Awe%20propose%20a%20new%20task%20named%20Video%20Anomaly%20Reasoning%20%28VAR%29%2C%20which%20aims%20to%20enable%0Adeep%20analysis%20and%20understanding%20of%20anomalies%20in%20the%20video%20by%20requiring%20MLLMs%20to%0Athink%20explicitly%20before%20answering.%20To%20this%20end%2C%20we%20propose%20Vad-R1%2C%20an%0Aend-to-end%20MLLM-based%20framework%20for%20VAR.%20Specifically%2C%20we%20design%20a%0APerception-to-Cognition%20Chain-of-Thought%20%28P2C-CoT%29%20that%20simulates%20the%20human%0Aprocess%20of%20recognizing%20anomalies%2C%20guiding%20the%20MLLM%20to%20reason%20anomaly%0Astep-by-step.%20Based%20on%20the%20structured%20P2C-CoT%2C%20we%20construct%20Vad-Reasoning%2C%20a%0Adedicated%20dataset%20for%20VAR.%20Furthermore%2C%20we%20propose%20an%20improved%20reinforcement%0Alearning%20algorithm%20AVA-GRPO%2C%20which%20explicitly%20incentivizes%20the%20anomaly%0Areasoning%20capability%20of%20MLLMs%20through%20a%20self-verification%20mechanism%20with%0Alimited%20annotations.%20Experimental%20results%20demonstrate%20that%20Vad-R1%20achieves%0Asuperior%20performance%2C%20outperforming%20both%20open-source%20and%20proprietary%20models%20on%0AVAD%20and%20VAR%20tasks.%20Codes%20and%20datasets%20will%20be%20released%20at%0Ahttps%3A//github.com/wbfwonderful/Vad-R1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19877v1&entry.124074799=Read"},
{"title": "An Out-Of-Distribution Membership Inference Attack Approach for\n  Cross-Domain Graph Attacks", "author": "Jinyan Wang and Liu Yang and Yuecen Wei and Jiaxuan Si and Chenhao Guo and Qingyun Sun and Xianxian Li and Xingcheng Fu", "abstract": "  Graph Neural Network-based methods face privacy leakage risks due to the\nintroduction of topological structures about the targets, which allows\nattackers to bypass the target's prior knowledge of the sensitive attributes\nand realize membership inference attacks (MIA) by observing and analyzing the\ntopology distribution. As privacy concerns grow, the assumption of MIA, which\npresumes that attackers can obtain an auxiliary dataset with the same\ndistribution, is increasingly deviating from reality. In this paper, we\ncategorize the distribution diversity issue in real-world MIA scenarios as an\nOut-Of-Distribution (OOD) problem, and propose a novel Graph OOD Membership\nInference Attack (GOOD-MIA) to achieve cross-domain graph attacks.\nSpecifically, we construct shadow subgraphs with distributions from different\ndomains to model the diversity of real-world data. We then explore the stable\nnode representations that remain unchanged under external influences and\nconsider eliminating redundant information from confounding environments and\nextracting task-relevant key information to more clearly distinguish between\nthe characteristics of training data and unseen data. This OOD-based design\nmakes cross-domain graph attacks possible. Finally, we perform risk\nextrapolation to optimize the attack's domain adaptability during attack\ninference to generalize the attack to other domains. Experimental results\ndemonstrate that GOOD-MIA achieves superior attack performance in datasets\ndesigned for multiple domains.\n", "link": "http://arxiv.org/abs/2505.20074v1", "date": "2025-05-26", "relevancy": 2.263, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4619}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4497}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Out-Of-Distribution%20Membership%20Inference%20Attack%20Approach%20for%0A%20%20Cross-Domain%20Graph%20Attacks&body=Title%3A%20An%20Out-Of-Distribution%20Membership%20Inference%20Attack%20Approach%20for%0A%20%20Cross-Domain%20Graph%20Attacks%0AAuthor%3A%20Jinyan%20Wang%20and%20Liu%20Yang%20and%20Yuecen%20Wei%20and%20Jiaxuan%20Si%20and%20Chenhao%20Guo%20and%20Qingyun%20Sun%20and%20Xianxian%20Li%20and%20Xingcheng%20Fu%0AAbstract%3A%20%20%20Graph%20Neural%20Network-based%20methods%20face%20privacy%20leakage%20risks%20due%20to%20the%0Aintroduction%20of%20topological%20structures%20about%20the%20targets%2C%20which%20allows%0Aattackers%20to%20bypass%20the%20target%27s%20prior%20knowledge%20of%20the%20sensitive%20attributes%0Aand%20realize%20membership%20inference%20attacks%20%28MIA%29%20by%20observing%20and%20analyzing%20the%0Atopology%20distribution.%20As%20privacy%20concerns%20grow%2C%20the%20assumption%20of%20MIA%2C%20which%0Apresumes%20that%20attackers%20can%20obtain%20an%20auxiliary%20dataset%20with%20the%20same%0Adistribution%2C%20is%20increasingly%20deviating%20from%20reality.%20In%20this%20paper%2C%20we%0Acategorize%20the%20distribution%20diversity%20issue%20in%20real-world%20MIA%20scenarios%20as%20an%0AOut-Of-Distribution%20%28OOD%29%20problem%2C%20and%20propose%20a%20novel%20Graph%20OOD%20Membership%0AInference%20Attack%20%28GOOD-MIA%29%20to%20achieve%20cross-domain%20graph%20attacks.%0ASpecifically%2C%20we%20construct%20shadow%20subgraphs%20with%20distributions%20from%20different%0Adomains%20to%20model%20the%20diversity%20of%20real-world%20data.%20We%20then%20explore%20the%20stable%0Anode%20representations%20that%20remain%20unchanged%20under%20external%20influences%20and%0Aconsider%20eliminating%20redundant%20information%20from%20confounding%20environments%20and%0Aextracting%20task-relevant%20key%20information%20to%20more%20clearly%20distinguish%20between%0Athe%20characteristics%20of%20training%20data%20and%20unseen%20data.%20This%20OOD-based%20design%0Amakes%20cross-domain%20graph%20attacks%20possible.%20Finally%2C%20we%20perform%20risk%0Aextrapolation%20to%20optimize%20the%20attack%27s%20domain%20adaptability%20during%20attack%0Ainference%20to%20generalize%20the%20attack%20to%20other%20domains.%20Experimental%20results%0Ademonstrate%20that%20GOOD-MIA%20achieves%20superior%20attack%20performance%20in%20datasets%0Adesigned%20for%20multiple%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Out-Of-Distribution%2520Membership%2520Inference%2520Attack%2520Approach%2520for%250A%2520%2520Cross-Domain%2520Graph%2520Attacks%26entry.906535625%3DJinyan%2520Wang%2520and%2520Liu%2520Yang%2520and%2520Yuecen%2520Wei%2520and%2520Jiaxuan%2520Si%2520and%2520Chenhao%2520Guo%2520and%2520Qingyun%2520Sun%2520and%2520Xianxian%2520Li%2520and%2520Xingcheng%2520Fu%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Network-based%2520methods%2520face%2520privacy%2520leakage%2520risks%2520due%2520to%2520the%250Aintroduction%2520of%2520topological%2520structures%2520about%2520the%2520targets%252C%2520which%2520allows%250Aattackers%2520to%2520bypass%2520the%2520target%2527s%2520prior%2520knowledge%2520of%2520the%2520sensitive%2520attributes%250Aand%2520realize%2520membership%2520inference%2520attacks%2520%2528MIA%2529%2520by%2520observing%2520and%2520analyzing%2520the%250Atopology%2520distribution.%2520As%2520privacy%2520concerns%2520grow%252C%2520the%2520assumption%2520of%2520MIA%252C%2520which%250Apresumes%2520that%2520attackers%2520can%2520obtain%2520an%2520auxiliary%2520dataset%2520with%2520the%2520same%250Adistribution%252C%2520is%2520increasingly%2520deviating%2520from%2520reality.%2520In%2520this%2520paper%252C%2520we%250Acategorize%2520the%2520distribution%2520diversity%2520issue%2520in%2520real-world%2520MIA%2520scenarios%2520as%2520an%250AOut-Of-Distribution%2520%2528OOD%2529%2520problem%252C%2520and%2520propose%2520a%2520novel%2520Graph%2520OOD%2520Membership%250AInference%2520Attack%2520%2528GOOD-MIA%2529%2520to%2520achieve%2520cross-domain%2520graph%2520attacks.%250ASpecifically%252C%2520we%2520construct%2520shadow%2520subgraphs%2520with%2520distributions%2520from%2520different%250Adomains%2520to%2520model%2520the%2520diversity%2520of%2520real-world%2520data.%2520We%2520then%2520explore%2520the%2520stable%250Anode%2520representations%2520that%2520remain%2520unchanged%2520under%2520external%2520influences%2520and%250Aconsider%2520eliminating%2520redundant%2520information%2520from%2520confounding%2520environments%2520and%250Aextracting%2520task-relevant%2520key%2520information%2520to%2520more%2520clearly%2520distinguish%2520between%250Athe%2520characteristics%2520of%2520training%2520data%2520and%2520unseen%2520data.%2520This%2520OOD-based%2520design%250Amakes%2520cross-domain%2520graph%2520attacks%2520possible.%2520Finally%252C%2520we%2520perform%2520risk%250Aextrapolation%2520to%2520optimize%2520the%2520attack%2527s%2520domain%2520adaptability%2520during%2520attack%250Ainference%2520to%2520generalize%2520the%2520attack%2520to%2520other%2520domains.%2520Experimental%2520results%250Ademonstrate%2520that%2520GOOD-MIA%2520achieves%2520superior%2520attack%2520performance%2520in%2520datasets%250Adesigned%2520for%2520multiple%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Out-Of-Distribution%20Membership%20Inference%20Attack%20Approach%20for%0A%20%20Cross-Domain%20Graph%20Attacks&entry.906535625=Jinyan%20Wang%20and%20Liu%20Yang%20and%20Yuecen%20Wei%20and%20Jiaxuan%20Si%20and%20Chenhao%20Guo%20and%20Qingyun%20Sun%20and%20Xianxian%20Li%20and%20Xingcheng%20Fu&entry.1292438233=%20%20Graph%20Neural%20Network-based%20methods%20face%20privacy%20leakage%20risks%20due%20to%20the%0Aintroduction%20of%20topological%20structures%20about%20the%20targets%2C%20which%20allows%0Aattackers%20to%20bypass%20the%20target%27s%20prior%20knowledge%20of%20the%20sensitive%20attributes%0Aand%20realize%20membership%20inference%20attacks%20%28MIA%29%20by%20observing%20and%20analyzing%20the%0Atopology%20distribution.%20As%20privacy%20concerns%20grow%2C%20the%20assumption%20of%20MIA%2C%20which%0Apresumes%20that%20attackers%20can%20obtain%20an%20auxiliary%20dataset%20with%20the%20same%0Adistribution%2C%20is%20increasingly%20deviating%20from%20reality.%20In%20this%20paper%2C%20we%0Acategorize%20the%20distribution%20diversity%20issue%20in%20real-world%20MIA%20scenarios%20as%20an%0AOut-Of-Distribution%20%28OOD%29%20problem%2C%20and%20propose%20a%20novel%20Graph%20OOD%20Membership%0AInference%20Attack%20%28GOOD-MIA%29%20to%20achieve%20cross-domain%20graph%20attacks.%0ASpecifically%2C%20we%20construct%20shadow%20subgraphs%20with%20distributions%20from%20different%0Adomains%20to%20model%20the%20diversity%20of%20real-world%20data.%20We%20then%20explore%20the%20stable%0Anode%20representations%20that%20remain%20unchanged%20under%20external%20influences%20and%0Aconsider%20eliminating%20redundant%20information%20from%20confounding%20environments%20and%0Aextracting%20task-relevant%20key%20information%20to%20more%20clearly%20distinguish%20between%0Athe%20characteristics%20of%20training%20data%20and%20unseen%20data.%20This%20OOD-based%20design%0Amakes%20cross-domain%20graph%20attacks%20possible.%20Finally%2C%20we%20perform%20risk%0Aextrapolation%20to%20optimize%20the%20attack%27s%20domain%20adaptability%20during%20attack%0Ainference%20to%20generalize%20the%20attack%20to%20other%20domains.%20Experimental%20results%0Ademonstrate%20that%20GOOD-MIA%20achieves%20superior%20attack%20performance%20in%20datasets%0Adesigned%20for%20multiple%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20074v1&entry.124074799=Read"},
{"title": "Enigmata: Scaling Logical Reasoning in Large Language Models with\n  Synthetic Verifiable Puzzles", "author": "Jiangjie Chen and Qianyu He and Siyu Yuan and Aili Chen and Zhicheng Cai and Weinan Dai and Hongli Yu and Qiying Yu and Xuefeng Li and Jiaze Chen and Hao Zhou and Mingxuan Wang", "abstract": "  Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at\nadvanced reasoning tasks like math and coding via Reinforcement Learning with\nVerifiable Rewards (RLVR), but still struggle with puzzles solvable by humans\nwithout domain knowledge. We introduce Enigmata, the first comprehensive suite\ntailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks\nacross seven categories, each with 1) a generator that produces unlimited\nexamples with controllable difficulty and 2) a rule-based verifier for\nautomatic evaluation. This generator-verifier design supports scalable,\nmulti-task RL training, fine-grained analysis, and seamless RLVR integration.\nWe further propose Enigmata-Eval, a rigorous benchmark, and develop optimized\nmulti-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,\nconsistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks\nlike Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes\nwell to out-of-domain puzzle benchmarks and mathematical reasoning, with little\nmulti-tasking trade-off. When trained on larger models like Seed1.5-Thinking\n(20B activated parameters and 200B total parameters), puzzle data from Enigmata\nfurther boosts SoTA performance on advanced math and STEM reasoning tasks such\nas AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization\nbenefits of Enigmata. This work offers a unified, controllable framework for\nadvancing logical reasoning in LLMs. Resources of this work can be found at\nhttps://seed-enigmata.github.io.\n", "link": "http://arxiv.org/abs/2505.19914v1", "date": "2025-05-26", "relevancy": 2.2611, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5693}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5693}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enigmata%3A%20Scaling%20Logical%20Reasoning%20in%20Large%20Language%20Models%20with%0A%20%20Synthetic%20Verifiable%20Puzzles&body=Title%3A%20Enigmata%3A%20Scaling%20Logical%20Reasoning%20in%20Large%20Language%20Models%20with%0A%20%20Synthetic%20Verifiable%20Puzzles%0AAuthor%3A%20Jiangjie%20Chen%20and%20Qianyu%20He%20and%20Siyu%20Yuan%20and%20Aili%20Chen%20and%20Zhicheng%20Cai%20and%20Weinan%20Dai%20and%20Hongli%20Yu%20and%20Qiying%20Yu%20and%20Xuefeng%20Li%20and%20Jiaze%20Chen%20and%20Hao%20Zhou%20and%20Mingxuan%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%2C%20such%20as%20OpenAI%27s%20o1%20and%20DeepSeek%27s%20R1%2C%20excel%20at%0Aadvanced%20reasoning%20tasks%20like%20math%20and%20coding%20via%20Reinforcement%20Learning%20with%0AVerifiable%20Rewards%20%28RLVR%29%2C%20but%20still%20struggle%20with%20puzzles%20solvable%20by%20humans%0Awithout%20domain%20knowledge.%20We%20introduce%20Enigmata%2C%20the%20first%20comprehensive%20suite%0Atailored%20for%20improving%20LLMs%20with%20puzzle%20reasoning%20skills.%20It%20includes%2036%20tasks%0Aacross%20seven%20categories%2C%20each%20with%201%29%20a%20generator%20that%20produces%20unlimited%0Aexamples%20with%20controllable%20difficulty%20and%202%29%20a%20rule-based%20verifier%20for%0Aautomatic%20evaluation.%20This%20generator-verifier%20design%20supports%20scalable%2C%0Amulti-task%20RL%20training%2C%20fine-grained%20analysis%2C%20and%20seamless%20RLVR%20integration.%0AWe%20further%20propose%20Enigmata-Eval%2C%20a%20rigorous%20benchmark%2C%20and%20develop%20optimized%0Amulti-task%20RLVR%20strategies.%20Our%20trained%20model%2C%20Qwen2.5-32B-Enigmata%2C%0Aconsistently%20surpasses%20o3-mini-high%20and%20o1%20on%20the%20puzzle%20reasoning%20benchmarks%0Alike%20Enigmata-Eval%2C%20ARC-AGI%20%2832.8%25%29%2C%20and%20ARC-AGI%202%20%280.6%25%29.%20It%20also%20generalizes%0Awell%20to%20out-of-domain%20puzzle%20benchmarks%20and%20mathematical%20reasoning%2C%20with%20little%0Amulti-tasking%20trade-off.%20When%20trained%20on%20larger%20models%20like%20Seed1.5-Thinking%0A%2820B%20activated%20parameters%20and%20200B%20total%20parameters%29%2C%20puzzle%20data%20from%20Enigmata%0Afurther%20boosts%20SoTA%20performance%20on%20advanced%20math%20and%20STEM%20reasoning%20tasks%20such%0Aas%20AIME%20%282024-2025%29%2C%20BeyondAIME%20and%20GPQA%20%28Diamond%29%2C%20showing%20nice%20generalization%0Abenefits%20of%20Enigmata.%20This%20work%20offers%20a%20unified%2C%20controllable%20framework%20for%0Aadvancing%20logical%20reasoning%20in%20LLMs.%20Resources%20of%20this%20work%20can%20be%20found%20at%0Ahttps%3A//seed-enigmata.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19914v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnigmata%253A%2520Scaling%2520Logical%2520Reasoning%2520in%2520Large%2520Language%2520Models%2520with%250A%2520%2520Synthetic%2520Verifiable%2520Puzzles%26entry.906535625%3DJiangjie%2520Chen%2520and%2520Qianyu%2520He%2520and%2520Siyu%2520Yuan%2520and%2520Aili%2520Chen%2520and%2520Zhicheng%2520Cai%2520and%2520Weinan%2520Dai%2520and%2520Hongli%2520Yu%2520and%2520Qiying%2520Yu%2520and%2520Xuefeng%2520Li%2520and%2520Jiaze%2520Chen%2520and%2520Hao%2520Zhou%2520and%2520Mingxuan%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520such%2520as%2520OpenAI%2527s%2520o1%2520and%2520DeepSeek%2527s%2520R1%252C%2520excel%2520at%250Aadvanced%2520reasoning%2520tasks%2520like%2520math%2520and%2520coding%2520via%2520Reinforcement%2520Learning%2520with%250AVerifiable%2520Rewards%2520%2528RLVR%2529%252C%2520but%2520still%2520struggle%2520with%2520puzzles%2520solvable%2520by%2520humans%250Awithout%2520domain%2520knowledge.%2520We%2520introduce%2520Enigmata%252C%2520the%2520first%2520comprehensive%2520suite%250Atailored%2520for%2520improving%2520LLMs%2520with%2520puzzle%2520reasoning%2520skills.%2520It%2520includes%252036%2520tasks%250Aacross%2520seven%2520categories%252C%2520each%2520with%25201%2529%2520a%2520generator%2520that%2520produces%2520unlimited%250Aexamples%2520with%2520controllable%2520difficulty%2520and%25202%2529%2520a%2520rule-based%2520verifier%2520for%250Aautomatic%2520evaluation.%2520This%2520generator-verifier%2520design%2520supports%2520scalable%252C%250Amulti-task%2520RL%2520training%252C%2520fine-grained%2520analysis%252C%2520and%2520seamless%2520RLVR%2520integration.%250AWe%2520further%2520propose%2520Enigmata-Eval%252C%2520a%2520rigorous%2520benchmark%252C%2520and%2520develop%2520optimized%250Amulti-task%2520RLVR%2520strategies.%2520Our%2520trained%2520model%252C%2520Qwen2.5-32B-Enigmata%252C%250Aconsistently%2520surpasses%2520o3-mini-high%2520and%2520o1%2520on%2520the%2520puzzle%2520reasoning%2520benchmarks%250Alike%2520Enigmata-Eval%252C%2520ARC-AGI%2520%252832.8%2525%2529%252C%2520and%2520ARC-AGI%25202%2520%25280.6%2525%2529.%2520It%2520also%2520generalizes%250Awell%2520to%2520out-of-domain%2520puzzle%2520benchmarks%2520and%2520mathematical%2520reasoning%252C%2520with%2520little%250Amulti-tasking%2520trade-off.%2520When%2520trained%2520on%2520larger%2520models%2520like%2520Seed1.5-Thinking%250A%252820B%2520activated%2520parameters%2520and%2520200B%2520total%2520parameters%2529%252C%2520puzzle%2520data%2520from%2520Enigmata%250Afurther%2520boosts%2520SoTA%2520performance%2520on%2520advanced%2520math%2520and%2520STEM%2520reasoning%2520tasks%2520such%250Aas%2520AIME%2520%25282024-2025%2529%252C%2520BeyondAIME%2520and%2520GPQA%2520%2528Diamond%2529%252C%2520showing%2520nice%2520generalization%250Abenefits%2520of%2520Enigmata.%2520This%2520work%2520offers%2520a%2520unified%252C%2520controllable%2520framework%2520for%250Aadvancing%2520logical%2520reasoning%2520in%2520LLMs.%2520Resources%2520of%2520this%2520work%2520can%2520be%2520found%2520at%250Ahttps%253A//seed-enigmata.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19914v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enigmata%3A%20Scaling%20Logical%20Reasoning%20in%20Large%20Language%20Models%20with%0A%20%20Synthetic%20Verifiable%20Puzzles&entry.906535625=Jiangjie%20Chen%20and%20Qianyu%20He%20and%20Siyu%20Yuan%20and%20Aili%20Chen%20and%20Zhicheng%20Cai%20and%20Weinan%20Dai%20and%20Hongli%20Yu%20and%20Qiying%20Yu%20and%20Xuefeng%20Li%20and%20Jiaze%20Chen%20and%20Hao%20Zhou%20and%20Mingxuan%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%2C%20such%20as%20OpenAI%27s%20o1%20and%20DeepSeek%27s%20R1%2C%20excel%20at%0Aadvanced%20reasoning%20tasks%20like%20math%20and%20coding%20via%20Reinforcement%20Learning%20with%0AVerifiable%20Rewards%20%28RLVR%29%2C%20but%20still%20struggle%20with%20puzzles%20solvable%20by%20humans%0Awithout%20domain%20knowledge.%20We%20introduce%20Enigmata%2C%20the%20first%20comprehensive%20suite%0Atailored%20for%20improving%20LLMs%20with%20puzzle%20reasoning%20skills.%20It%20includes%2036%20tasks%0Aacross%20seven%20categories%2C%20each%20with%201%29%20a%20generator%20that%20produces%20unlimited%0Aexamples%20with%20controllable%20difficulty%20and%202%29%20a%20rule-based%20verifier%20for%0Aautomatic%20evaluation.%20This%20generator-verifier%20design%20supports%20scalable%2C%0Amulti-task%20RL%20training%2C%20fine-grained%20analysis%2C%20and%20seamless%20RLVR%20integration.%0AWe%20further%20propose%20Enigmata-Eval%2C%20a%20rigorous%20benchmark%2C%20and%20develop%20optimized%0Amulti-task%20RLVR%20strategies.%20Our%20trained%20model%2C%20Qwen2.5-32B-Enigmata%2C%0Aconsistently%20surpasses%20o3-mini-high%20and%20o1%20on%20the%20puzzle%20reasoning%20benchmarks%0Alike%20Enigmata-Eval%2C%20ARC-AGI%20%2832.8%25%29%2C%20and%20ARC-AGI%202%20%280.6%25%29.%20It%20also%20generalizes%0Awell%20to%20out-of-domain%20puzzle%20benchmarks%20and%20mathematical%20reasoning%2C%20with%20little%0Amulti-tasking%20trade-off.%20When%20trained%20on%20larger%20models%20like%20Seed1.5-Thinking%0A%2820B%20activated%20parameters%20and%20200B%20total%20parameters%29%2C%20puzzle%20data%20from%20Enigmata%0Afurther%20boosts%20SoTA%20performance%20on%20advanced%20math%20and%20STEM%20reasoning%20tasks%20such%0Aas%20AIME%20%282024-2025%29%2C%20BeyondAIME%20and%20GPQA%20%28Diamond%29%2C%20showing%20nice%20generalization%0Abenefits%20of%20Enigmata.%20This%20work%20offers%20a%20unified%2C%20controllable%20framework%20for%0Aadvancing%20logical%20reasoning%20in%20LLMs.%20Resources%20of%20this%20work%20can%20be%20found%20at%0Ahttps%3A//seed-enigmata.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19914v1&entry.124074799=Read"},
{"title": "Task-Oriented Communications for Visual Navigation with Edge-Aerial\n  Collaboration in Low Altitude Economy", "author": "Zhengru Fang and Zhenghao Liu and Jingjing Wang and Senkang Hu and Yu Guo and Yiqin Deng and Yuguang Fang", "abstract": "  To support the Low Altitude Economy (LAE), it is essential to achieve precise\nlocalization of unmanned aerial vehicles (UAVs) in urban areas where global\npositioning system (GPS) signals are unavailable. Vision-based methods offer a\nviable alternative but face severe bandwidth, memory and processing constraints\non lightweight UAVs. Inspired by mammalian spatial cognition, we propose a\ntask-oriented communication framework, where UAVs equipped with multi-camera\nsystems extract compact multi-view features and offload localization tasks to\nedge servers. We introduce the Orthogonally-constrained Variational Information\nBottleneck encoder (O-VIB), which incorporates automatic relevance\ndetermination (ARD) to prune non-informative features while enforcing\northogonality to minimize redundancy. This enables efficient and accurate\nlocalization with minimal transmission cost. Extensive evaluation on a\ndedicated LAE UAV dataset shows that O-VIB achieves high-precision localization\nunder stringent bandwidth budgets. Code and dataset will be made publicly\navailable at: github.com/fangzr/TOC-Edge-Aerial.\n", "link": "http://arxiv.org/abs/2504.18317v4", "date": "2025-05-26", "relevancy": 2.2546, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5908}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5491}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-Oriented%20Communications%20for%20Visual%20Navigation%20with%20Edge-Aerial%0A%20%20Collaboration%20in%20Low%20Altitude%20Economy&body=Title%3A%20Task-Oriented%20Communications%20for%20Visual%20Navigation%20with%20Edge-Aerial%0A%20%20Collaboration%20in%20Low%20Altitude%20Economy%0AAuthor%3A%20Zhengru%20Fang%20and%20Zhenghao%20Liu%20and%20Jingjing%20Wang%20and%20Senkang%20Hu%20and%20Yu%20Guo%20and%20Yiqin%20Deng%20and%20Yuguang%20Fang%0AAbstract%3A%20%20%20To%20support%20the%20Low%20Altitude%20Economy%20%28LAE%29%2C%20it%20is%20essential%20to%20achieve%20precise%0Alocalization%20of%20unmanned%20aerial%20vehicles%20%28UAVs%29%20in%20urban%20areas%20where%20global%0Apositioning%20system%20%28GPS%29%20signals%20are%20unavailable.%20Vision-based%20methods%20offer%20a%0Aviable%20alternative%20but%20face%20severe%20bandwidth%2C%20memory%20and%20processing%20constraints%0Aon%20lightweight%20UAVs.%20Inspired%20by%20mammalian%20spatial%20cognition%2C%20we%20propose%20a%0Atask-oriented%20communication%20framework%2C%20where%20UAVs%20equipped%20with%20multi-camera%0Asystems%20extract%20compact%20multi-view%20features%20and%20offload%20localization%20tasks%20to%0Aedge%20servers.%20We%20introduce%20the%20Orthogonally-constrained%20Variational%20Information%0ABottleneck%20encoder%20%28O-VIB%29%2C%20which%20incorporates%20automatic%20relevance%0Adetermination%20%28ARD%29%20to%20prune%20non-informative%20features%20while%20enforcing%0Aorthogonality%20to%20minimize%20redundancy.%20This%20enables%20efficient%20and%20accurate%0Alocalization%20with%20minimal%20transmission%20cost.%20Extensive%20evaluation%20on%20a%0Adedicated%20LAE%20UAV%20dataset%20shows%20that%20O-VIB%20achieves%20high-precision%20localization%0Aunder%20stringent%20bandwidth%20budgets.%20Code%20and%20dataset%20will%20be%20made%20publicly%0Aavailable%20at%3A%20github.com/fangzr/TOC-Edge-Aerial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18317v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-Oriented%2520Communications%2520for%2520Visual%2520Navigation%2520with%2520Edge-Aerial%250A%2520%2520Collaboration%2520in%2520Low%2520Altitude%2520Economy%26entry.906535625%3DZhengru%2520Fang%2520and%2520Zhenghao%2520Liu%2520and%2520Jingjing%2520Wang%2520and%2520Senkang%2520Hu%2520and%2520Yu%2520Guo%2520and%2520Yiqin%2520Deng%2520and%2520Yuguang%2520Fang%26entry.1292438233%3D%2520%2520To%2520support%2520the%2520Low%2520Altitude%2520Economy%2520%2528LAE%2529%252C%2520it%2520is%2520essential%2520to%2520achieve%2520precise%250Alocalization%2520of%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520in%2520urban%2520areas%2520where%2520global%250Apositioning%2520system%2520%2528GPS%2529%2520signals%2520are%2520unavailable.%2520Vision-based%2520methods%2520offer%2520a%250Aviable%2520alternative%2520but%2520face%2520severe%2520bandwidth%252C%2520memory%2520and%2520processing%2520constraints%250Aon%2520lightweight%2520UAVs.%2520Inspired%2520by%2520mammalian%2520spatial%2520cognition%252C%2520we%2520propose%2520a%250Atask-oriented%2520communication%2520framework%252C%2520where%2520UAVs%2520equipped%2520with%2520multi-camera%250Asystems%2520extract%2520compact%2520multi-view%2520features%2520and%2520offload%2520localization%2520tasks%2520to%250Aedge%2520servers.%2520We%2520introduce%2520the%2520Orthogonally-constrained%2520Variational%2520Information%250ABottleneck%2520encoder%2520%2528O-VIB%2529%252C%2520which%2520incorporates%2520automatic%2520relevance%250Adetermination%2520%2528ARD%2529%2520to%2520prune%2520non-informative%2520features%2520while%2520enforcing%250Aorthogonality%2520to%2520minimize%2520redundancy.%2520This%2520enables%2520efficient%2520and%2520accurate%250Alocalization%2520with%2520minimal%2520transmission%2520cost.%2520Extensive%2520evaluation%2520on%2520a%250Adedicated%2520LAE%2520UAV%2520dataset%2520shows%2520that%2520O-VIB%2520achieves%2520high-precision%2520localization%250Aunder%2520stringent%2520bandwidth%2520budgets.%2520Code%2520and%2520dataset%2520will%2520be%2520made%2520publicly%250Aavailable%2520at%253A%2520github.com/fangzr/TOC-Edge-Aerial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18317v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-Oriented%20Communications%20for%20Visual%20Navigation%20with%20Edge-Aerial%0A%20%20Collaboration%20in%20Low%20Altitude%20Economy&entry.906535625=Zhengru%20Fang%20and%20Zhenghao%20Liu%20and%20Jingjing%20Wang%20and%20Senkang%20Hu%20and%20Yu%20Guo%20and%20Yiqin%20Deng%20and%20Yuguang%20Fang&entry.1292438233=%20%20To%20support%20the%20Low%20Altitude%20Economy%20%28LAE%29%2C%20it%20is%20essential%20to%20achieve%20precise%0Alocalization%20of%20unmanned%20aerial%20vehicles%20%28UAVs%29%20in%20urban%20areas%20where%20global%0Apositioning%20system%20%28GPS%29%20signals%20are%20unavailable.%20Vision-based%20methods%20offer%20a%0Aviable%20alternative%20but%20face%20severe%20bandwidth%2C%20memory%20and%20processing%20constraints%0Aon%20lightweight%20UAVs.%20Inspired%20by%20mammalian%20spatial%20cognition%2C%20we%20propose%20a%0Atask-oriented%20communication%20framework%2C%20where%20UAVs%20equipped%20with%20multi-camera%0Asystems%20extract%20compact%20multi-view%20features%20and%20offload%20localization%20tasks%20to%0Aedge%20servers.%20We%20introduce%20the%20Orthogonally-constrained%20Variational%20Information%0ABottleneck%20encoder%20%28O-VIB%29%2C%20which%20incorporates%20automatic%20relevance%0Adetermination%20%28ARD%29%20to%20prune%20non-informative%20features%20while%20enforcing%0Aorthogonality%20to%20minimize%20redundancy.%20This%20enables%20efficient%20and%20accurate%0Alocalization%20with%20minimal%20transmission%20cost.%20Extensive%20evaluation%20on%20a%0Adedicated%20LAE%20UAV%20dataset%20shows%20that%20O-VIB%20achieves%20high-precision%20localization%0Aunder%20stringent%20bandwidth%20budgets.%20Code%20and%20dataset%20will%20be%20made%20publicly%0Aavailable%20at%3A%20github.com/fangzr/TOC-Edge-Aerial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18317v4&entry.124074799=Read"},
{"title": "Semantic Correspondence: Unified Benchmarking and a Strong Baseline", "author": "Kaiyan Zhang and Xinghui Li and Jingyi Lu and Kai Han", "abstract": "  Establishing semantic correspondence is a challenging task in computer\nvision, aiming to match keypoints with the same semantic information across\ndifferent images. Benefiting from the rapid development of deep learning,\nremarkable progress has been made over the past decade. However, a\ncomprehensive review and analysis of this task remains absent. In this paper,\nwe present the first extensive survey of semantic correspondence methods. We\nfirst propose a taxonomy to classify existing methods based on the type of\ntheir method designs. These methods are then categorized accordingly, and we\nprovide a detailed analysis of each approach. Furthermore, we aggregate and\nsummarize the results of methods in literature across various benchmarks into a\nunified comparative table, with detailed configurations to highlight\nperformance variations. Additionally, to provide a detailed understanding on\nexisting methods for semantic matching, we thoroughly conduct controlled\nexperiments to analyse the effectiveness of the components of different\nmethods. Finally, we propose a simple yet effective baseline that achieves\nstate-of-the-art performance on multiple benchmarks, providing a solid\nfoundation for future research in this field. We hope this survey serves as a\ncomprehensive reference and consolidated baseline for future development. Code\nis publicly available at: https://github.com/Visual-AI/Semantic-Correspondence.\n", "link": "http://arxiv.org/abs/2505.18060v2", "date": "2025-05-26", "relevancy": 2.2488, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Correspondence%3A%20Unified%20Benchmarking%20and%20a%20Strong%20Baseline&body=Title%3A%20Semantic%20Correspondence%3A%20Unified%20Benchmarking%20and%20a%20Strong%20Baseline%0AAuthor%3A%20Kaiyan%20Zhang%20and%20Xinghui%20Li%20and%20Jingyi%20Lu%20and%20Kai%20Han%0AAbstract%3A%20%20%20Establishing%20semantic%20correspondence%20is%20a%20challenging%20task%20in%20computer%0Avision%2C%20aiming%20to%20match%20keypoints%20with%20the%20same%20semantic%20information%20across%0Adifferent%20images.%20Benefiting%20from%20the%20rapid%20development%20of%20deep%20learning%2C%0Aremarkable%20progress%20has%20been%20made%20over%20the%20past%20decade.%20However%2C%20a%0Acomprehensive%20review%20and%20analysis%20of%20this%20task%20remains%20absent.%20In%20this%20paper%2C%0Awe%20present%20the%20first%20extensive%20survey%20of%20semantic%20correspondence%20methods.%20We%0Afirst%20propose%20a%20taxonomy%20to%20classify%20existing%20methods%20based%20on%20the%20type%20of%0Atheir%20method%20designs.%20These%20methods%20are%20then%20categorized%20accordingly%2C%20and%20we%0Aprovide%20a%20detailed%20analysis%20of%20each%20approach.%20Furthermore%2C%20we%20aggregate%20and%0Asummarize%20the%20results%20of%20methods%20in%20literature%20across%20various%20benchmarks%20into%20a%0Aunified%20comparative%20table%2C%20with%20detailed%20configurations%20to%20highlight%0Aperformance%20variations.%20Additionally%2C%20to%20provide%20a%20detailed%20understanding%20on%0Aexisting%20methods%20for%20semantic%20matching%2C%20we%20thoroughly%20conduct%20controlled%0Aexperiments%20to%20analyse%20the%20effectiveness%20of%20the%20components%20of%20different%0Amethods.%20Finally%2C%20we%20propose%20a%20simple%20yet%20effective%20baseline%20that%20achieves%0Astate-of-the-art%20performance%20on%20multiple%20benchmarks%2C%20providing%20a%20solid%0Afoundation%20for%20future%20research%20in%20this%20field.%20We%20hope%20this%20survey%20serves%20as%20a%0Acomprehensive%20reference%20and%20consolidated%20baseline%20for%20future%20development.%20Code%0Ais%20publicly%20available%20at%3A%20https%3A//github.com/Visual-AI/Semantic-Correspondence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18060v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Correspondence%253A%2520Unified%2520Benchmarking%2520and%2520a%2520Strong%2520Baseline%26entry.906535625%3DKaiyan%2520Zhang%2520and%2520Xinghui%2520Li%2520and%2520Jingyi%2520Lu%2520and%2520Kai%2520Han%26entry.1292438233%3D%2520%2520Establishing%2520semantic%2520correspondence%2520is%2520a%2520challenging%2520task%2520in%2520computer%250Avision%252C%2520aiming%2520to%2520match%2520keypoints%2520with%2520the%2520same%2520semantic%2520information%2520across%250Adifferent%2520images.%2520Benefiting%2520from%2520the%2520rapid%2520development%2520of%2520deep%2520learning%252C%250Aremarkable%2520progress%2520has%2520been%2520made%2520over%2520the%2520past%2520decade.%2520However%252C%2520a%250Acomprehensive%2520review%2520and%2520analysis%2520of%2520this%2520task%2520remains%2520absent.%2520In%2520this%2520paper%252C%250Awe%2520present%2520the%2520first%2520extensive%2520survey%2520of%2520semantic%2520correspondence%2520methods.%2520We%250Afirst%2520propose%2520a%2520taxonomy%2520to%2520classify%2520existing%2520methods%2520based%2520on%2520the%2520type%2520of%250Atheir%2520method%2520designs.%2520These%2520methods%2520are%2520then%2520categorized%2520accordingly%252C%2520and%2520we%250Aprovide%2520a%2520detailed%2520analysis%2520of%2520each%2520approach.%2520Furthermore%252C%2520we%2520aggregate%2520and%250Asummarize%2520the%2520results%2520of%2520methods%2520in%2520literature%2520across%2520various%2520benchmarks%2520into%2520a%250Aunified%2520comparative%2520table%252C%2520with%2520detailed%2520configurations%2520to%2520highlight%250Aperformance%2520variations.%2520Additionally%252C%2520to%2520provide%2520a%2520detailed%2520understanding%2520on%250Aexisting%2520methods%2520for%2520semantic%2520matching%252C%2520we%2520thoroughly%2520conduct%2520controlled%250Aexperiments%2520to%2520analyse%2520the%2520effectiveness%2520of%2520the%2520components%2520of%2520different%250Amethods.%2520Finally%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520baseline%2520that%2520achieves%250Astate-of-the-art%2520performance%2520on%2520multiple%2520benchmarks%252C%2520providing%2520a%2520solid%250Afoundation%2520for%2520future%2520research%2520in%2520this%2520field.%2520We%2520hope%2520this%2520survey%2520serves%2520as%2520a%250Acomprehensive%2520reference%2520and%2520consolidated%2520baseline%2520for%2520future%2520development.%2520Code%250Ais%2520publicly%2520available%2520at%253A%2520https%253A//github.com/Visual-AI/Semantic-Correspondence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18060v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Correspondence%3A%20Unified%20Benchmarking%20and%20a%20Strong%20Baseline&entry.906535625=Kaiyan%20Zhang%20and%20Xinghui%20Li%20and%20Jingyi%20Lu%20and%20Kai%20Han&entry.1292438233=%20%20Establishing%20semantic%20correspondence%20is%20a%20challenging%20task%20in%20computer%0Avision%2C%20aiming%20to%20match%20keypoints%20with%20the%20same%20semantic%20information%20across%0Adifferent%20images.%20Benefiting%20from%20the%20rapid%20development%20of%20deep%20learning%2C%0Aremarkable%20progress%20has%20been%20made%20over%20the%20past%20decade.%20However%2C%20a%0Acomprehensive%20review%20and%20analysis%20of%20this%20task%20remains%20absent.%20In%20this%20paper%2C%0Awe%20present%20the%20first%20extensive%20survey%20of%20semantic%20correspondence%20methods.%20We%0Afirst%20propose%20a%20taxonomy%20to%20classify%20existing%20methods%20based%20on%20the%20type%20of%0Atheir%20method%20designs.%20These%20methods%20are%20then%20categorized%20accordingly%2C%20and%20we%0Aprovide%20a%20detailed%20analysis%20of%20each%20approach.%20Furthermore%2C%20we%20aggregate%20and%0Asummarize%20the%20results%20of%20methods%20in%20literature%20across%20various%20benchmarks%20into%20a%0Aunified%20comparative%20table%2C%20with%20detailed%20configurations%20to%20highlight%0Aperformance%20variations.%20Additionally%2C%20to%20provide%20a%20detailed%20understanding%20on%0Aexisting%20methods%20for%20semantic%20matching%2C%20we%20thoroughly%20conduct%20controlled%0Aexperiments%20to%20analyse%20the%20effectiveness%20of%20the%20components%20of%20different%0Amethods.%20Finally%2C%20we%20propose%20a%20simple%20yet%20effective%20baseline%20that%20achieves%0Astate-of-the-art%20performance%20on%20multiple%20benchmarks%2C%20providing%20a%20solid%0Afoundation%20for%20future%20research%20in%20this%20field.%20We%20hope%20this%20survey%20serves%20as%20a%0Acomprehensive%20reference%20and%20consolidated%20baseline%20for%20future%20development.%20Code%0Ais%20publicly%20available%20at%3A%20https%3A//github.com/Visual-AI/Semantic-Correspondence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18060v2&entry.124074799=Read"},
{"title": "NFIG: Autoregressive Image Generation with Next-Frequency Prediction", "author": "Zhihao Huang and Xi Qiu and Yukuo Ma and Yifu Zhou and Junjie Chen and Hongyuan Zhang and Chi Zhang and Xuelong Li", "abstract": "  Autoregressive models have achieved promising results in natural language\nprocessing. However, for image generation tasks, they encounter substantial\nchallenges in effectively capturing long-range dependencies, managing\ncomputational costs, and most crucially, defining meaningful autoregressive\nsequences that reflect natural image hierarchies. To address these issues, we\npresent \\textbf{N}ext-\\textbf{F}requency \\textbf{I}mage \\textbf{G}eneration\n(\\textbf{NFIG}), a novel framework that decomposes the image generation process\ninto multiple frequency-guided stages. Our approach first generates\nlow-frequency components to establish global structure with fewer tokens, then\nprogressively adds higher-frequency details, following the natural spectral\nhierarchy of images. This principled autoregressive sequence not only improves\nthe quality of generated images by better capturing true causal relationships\nbetween image components, but also significantly reduces computational overhead\nduring inference. Extensive experiments demonstrate that NFIG achieves\nstate-of-the-art performance with fewer steps, offering a more efficient\nsolution for image generation, with 1.25$\\times$ speedup compared to VAR-d20\nwhile achieving better performance (FID: 2.81) on the ImageNet-256 benchmark.\nWe hope that our insight of incorporating frequency-domain knowledge to guide\nautoregressive sequence design will shed light on future research. We will make\nour code publicly available upon acceptance of the paper.\n", "link": "http://arxiv.org/abs/2503.07076v2", "date": "2025-05-26", "relevancy": 2.239, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5993}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5502}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NFIG%3A%20Autoregressive%20Image%20Generation%20with%20Next-Frequency%20Prediction&body=Title%3A%20NFIG%3A%20Autoregressive%20Image%20Generation%20with%20Next-Frequency%20Prediction%0AAuthor%3A%20Zhihao%20Huang%20and%20Xi%20Qiu%20and%20Yukuo%20Ma%20and%20Yifu%20Zhou%20and%20Junjie%20Chen%20and%20Hongyuan%20Zhang%20and%20Chi%20Zhang%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Autoregressive%20models%20have%20achieved%20promising%20results%20in%20natural%20language%0Aprocessing.%20However%2C%20for%20image%20generation%20tasks%2C%20they%20encounter%20substantial%0Achallenges%20in%20effectively%20capturing%20long-range%20dependencies%2C%20managing%0Acomputational%20costs%2C%20and%20most%20crucially%2C%20defining%20meaningful%20autoregressive%0Asequences%20that%20reflect%20natural%20image%20hierarchies.%20To%20address%20these%20issues%2C%20we%0Apresent%20%5Ctextbf%7BN%7Dext-%5Ctextbf%7BF%7Drequency%20%5Ctextbf%7BI%7Dmage%20%5Ctextbf%7BG%7Deneration%0A%28%5Ctextbf%7BNFIG%7D%29%2C%20a%20novel%20framework%20that%20decomposes%20the%20image%20generation%20process%0Ainto%20multiple%20frequency-guided%20stages.%20Our%20approach%20first%20generates%0Alow-frequency%20components%20to%20establish%20global%20structure%20with%20fewer%20tokens%2C%20then%0Aprogressively%20adds%20higher-frequency%20details%2C%20following%20the%20natural%20spectral%0Ahierarchy%20of%20images.%20This%20principled%20autoregressive%20sequence%20not%20only%20improves%0Athe%20quality%20of%20generated%20images%20by%20better%20capturing%20true%20causal%20relationships%0Abetween%20image%20components%2C%20but%20also%20significantly%20reduces%20computational%20overhead%0Aduring%20inference.%20Extensive%20experiments%20demonstrate%20that%20NFIG%20achieves%0Astate-of-the-art%20performance%20with%20fewer%20steps%2C%20offering%20a%20more%20efficient%0Asolution%20for%20image%20generation%2C%20with%201.25%24%5Ctimes%24%20speedup%20compared%20to%20VAR-d20%0Awhile%20achieving%20better%20performance%20%28FID%3A%202.81%29%20on%20the%20ImageNet-256%20benchmark.%0AWe%20hope%20that%20our%20insight%20of%20incorporating%20frequency-domain%20knowledge%20to%20guide%0Aautoregressive%20sequence%20design%20will%20shed%20light%20on%20future%20research.%20We%20will%20make%0Aour%20code%20publicly%20available%20upon%20acceptance%20of%20the%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.07076v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNFIG%253A%2520Autoregressive%2520Image%2520Generation%2520with%2520Next-Frequency%2520Prediction%26entry.906535625%3DZhihao%2520Huang%2520and%2520Xi%2520Qiu%2520and%2520Yukuo%2520Ma%2520and%2520Yifu%2520Zhou%2520and%2520Junjie%2520Chen%2520and%2520Hongyuan%2520Zhang%2520and%2520Chi%2520Zhang%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520Autoregressive%2520models%2520have%2520achieved%2520promising%2520results%2520in%2520natural%2520language%250Aprocessing.%2520However%252C%2520for%2520image%2520generation%2520tasks%252C%2520they%2520encounter%2520substantial%250Achallenges%2520in%2520effectively%2520capturing%2520long-range%2520dependencies%252C%2520managing%250Acomputational%2520costs%252C%2520and%2520most%2520crucially%252C%2520defining%2520meaningful%2520autoregressive%250Asequences%2520that%2520reflect%2520natural%2520image%2520hierarchies.%2520To%2520address%2520these%2520issues%252C%2520we%250Apresent%2520%255Ctextbf%257BN%257Dext-%255Ctextbf%257BF%257Drequency%2520%255Ctextbf%257BI%257Dmage%2520%255Ctextbf%257BG%257Deneration%250A%2528%255Ctextbf%257BNFIG%257D%2529%252C%2520a%2520novel%2520framework%2520that%2520decomposes%2520the%2520image%2520generation%2520process%250Ainto%2520multiple%2520frequency-guided%2520stages.%2520Our%2520approach%2520first%2520generates%250Alow-frequency%2520components%2520to%2520establish%2520global%2520structure%2520with%2520fewer%2520tokens%252C%2520then%250Aprogressively%2520adds%2520higher-frequency%2520details%252C%2520following%2520the%2520natural%2520spectral%250Ahierarchy%2520of%2520images.%2520This%2520principled%2520autoregressive%2520sequence%2520not%2520only%2520improves%250Athe%2520quality%2520of%2520generated%2520images%2520by%2520better%2520capturing%2520true%2520causal%2520relationships%250Abetween%2520image%2520components%252C%2520but%2520also%2520significantly%2520reduces%2520computational%2520overhead%250Aduring%2520inference.%2520Extensive%2520experiments%2520demonstrate%2520that%2520NFIG%2520achieves%250Astate-of-the-art%2520performance%2520with%2520fewer%2520steps%252C%2520offering%2520a%2520more%2520efficient%250Asolution%2520for%2520image%2520generation%252C%2520with%25201.25%2524%255Ctimes%2524%2520speedup%2520compared%2520to%2520VAR-d20%250Awhile%2520achieving%2520better%2520performance%2520%2528FID%253A%25202.81%2529%2520on%2520the%2520ImageNet-256%2520benchmark.%250AWe%2520hope%2520that%2520our%2520insight%2520of%2520incorporating%2520frequency-domain%2520knowledge%2520to%2520guide%250Aautoregressive%2520sequence%2520design%2520will%2520shed%2520light%2520on%2520future%2520research.%2520We%2520will%2520make%250Aour%2520code%2520publicly%2520available%2520upon%2520acceptance%2520of%2520the%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07076v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NFIG%3A%20Autoregressive%20Image%20Generation%20with%20Next-Frequency%20Prediction&entry.906535625=Zhihao%20Huang%20and%20Xi%20Qiu%20and%20Yukuo%20Ma%20and%20Yifu%20Zhou%20and%20Junjie%20Chen%20and%20Hongyuan%20Zhang%20and%20Chi%20Zhang%20and%20Xuelong%20Li&entry.1292438233=%20%20Autoregressive%20models%20have%20achieved%20promising%20results%20in%20natural%20language%0Aprocessing.%20However%2C%20for%20image%20generation%20tasks%2C%20they%20encounter%20substantial%0Achallenges%20in%20effectively%20capturing%20long-range%20dependencies%2C%20managing%0Acomputational%20costs%2C%20and%20most%20crucially%2C%20defining%20meaningful%20autoregressive%0Asequences%20that%20reflect%20natural%20image%20hierarchies.%20To%20address%20these%20issues%2C%20we%0Apresent%20%5Ctextbf%7BN%7Dext-%5Ctextbf%7BF%7Drequency%20%5Ctextbf%7BI%7Dmage%20%5Ctextbf%7BG%7Deneration%0A%28%5Ctextbf%7BNFIG%7D%29%2C%20a%20novel%20framework%20that%20decomposes%20the%20image%20generation%20process%0Ainto%20multiple%20frequency-guided%20stages.%20Our%20approach%20first%20generates%0Alow-frequency%20components%20to%20establish%20global%20structure%20with%20fewer%20tokens%2C%20then%0Aprogressively%20adds%20higher-frequency%20details%2C%20following%20the%20natural%20spectral%0Ahierarchy%20of%20images.%20This%20principled%20autoregressive%20sequence%20not%20only%20improves%0Athe%20quality%20of%20generated%20images%20by%20better%20capturing%20true%20causal%20relationships%0Abetween%20image%20components%2C%20but%20also%20significantly%20reduces%20computational%20overhead%0Aduring%20inference.%20Extensive%20experiments%20demonstrate%20that%20NFIG%20achieves%0Astate-of-the-art%20performance%20with%20fewer%20steps%2C%20offering%20a%20more%20efficient%0Asolution%20for%20image%20generation%2C%20with%201.25%24%5Ctimes%24%20speedup%20compared%20to%20VAR-d20%0Awhile%20achieving%20better%20performance%20%28FID%3A%202.81%29%20on%20the%20ImageNet-256%20benchmark.%0AWe%20hope%20that%20our%20insight%20of%20incorporating%20frequency-domain%20knowledge%20to%20guide%0Aautoregressive%20sequence%20design%20will%20shed%20light%20on%20future%20research.%20We%20will%20make%0Aour%20code%20publicly%20available%20upon%20acceptance%20of%20the%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.07076v2&entry.124074799=Read"},
{"title": "CA3D: Convolutional-Attentional 3D Nets for Efficient Video Activity\n  Recognition on the Edge", "author": "Gabriele Lagani and Fabrizio Falchi and Claudio Gennaro and Giuseppe Amato", "abstract": "  In this paper, we introduce a deep learning solution for video activity\nrecognition that leverages an innovative combination of convolutional layers\nwith a linear-complexity attention mechanism. Moreover, we introduce a novel\nquantization mechanism to further improve the efficiency of our model during\nboth training and inference. Our model maintains a reduced computational cost,\nwhile preserving robust learning and generalization capabilities. Our approach\naddresses the issues related to the high computing requirements of current\nmodels, with the goal of achieving competitive accuracy on consumer and edge\ndevices, enabling smart home and smart healthcare applications where efficiency\nand privacy issues are of concern. We experimentally validate our model on\ndifferent established and publicly available video activity recognition\nbenchmarks, improving accuracy over alternative models at a competitive\ncomputing cost.\n", "link": "http://arxiv.org/abs/2505.19928v1", "date": "2025-05-26", "relevancy": 2.2383, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6112}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CA3D%3A%20Convolutional-Attentional%203D%20Nets%20for%20Efficient%20Video%20Activity%0A%20%20Recognition%20on%20the%20Edge&body=Title%3A%20CA3D%3A%20Convolutional-Attentional%203D%20Nets%20for%20Efficient%20Video%20Activity%0A%20%20Recognition%20on%20the%20Edge%0AAuthor%3A%20Gabriele%20Lagani%20and%20Fabrizio%20Falchi%20and%20Claudio%20Gennaro%20and%20Giuseppe%20Amato%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20deep%20learning%20solution%20for%20video%20activity%0Arecognition%20that%20leverages%20an%20innovative%20combination%20of%20convolutional%20layers%0Awith%20a%20linear-complexity%20attention%20mechanism.%20Moreover%2C%20we%20introduce%20a%20novel%0Aquantization%20mechanism%20to%20further%20improve%20the%20efficiency%20of%20our%20model%20during%0Aboth%20training%20and%20inference.%20Our%20model%20maintains%20a%20reduced%20computational%20cost%2C%0Awhile%20preserving%20robust%20learning%20and%20generalization%20capabilities.%20Our%20approach%0Aaddresses%20the%20issues%20related%20to%20the%20high%20computing%20requirements%20of%20current%0Amodels%2C%20with%20the%20goal%20of%20achieving%20competitive%20accuracy%20on%20consumer%20and%20edge%0Adevices%2C%20enabling%20smart%20home%20and%20smart%20healthcare%20applications%20where%20efficiency%0Aand%20privacy%20issues%20are%20of%20concern.%20We%20experimentally%20validate%20our%20model%20on%0Adifferent%20established%20and%20publicly%20available%20video%20activity%20recognition%0Abenchmarks%2C%20improving%20accuracy%20over%20alternative%20models%20at%20a%20competitive%0Acomputing%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19928v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCA3D%253A%2520Convolutional-Attentional%25203D%2520Nets%2520for%2520Efficient%2520Video%2520Activity%250A%2520%2520Recognition%2520on%2520the%2520Edge%26entry.906535625%3DGabriele%2520Lagani%2520and%2520Fabrizio%2520Falchi%2520and%2520Claudio%2520Gennaro%2520and%2520Giuseppe%2520Amato%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520deep%2520learning%2520solution%2520for%2520video%2520activity%250Arecognition%2520that%2520leverages%2520an%2520innovative%2520combination%2520of%2520convolutional%2520layers%250Awith%2520a%2520linear-complexity%2520attention%2520mechanism.%2520Moreover%252C%2520we%2520introduce%2520a%2520novel%250Aquantization%2520mechanism%2520to%2520further%2520improve%2520the%2520efficiency%2520of%2520our%2520model%2520during%250Aboth%2520training%2520and%2520inference.%2520Our%2520model%2520maintains%2520a%2520reduced%2520computational%2520cost%252C%250Awhile%2520preserving%2520robust%2520learning%2520and%2520generalization%2520capabilities.%2520Our%2520approach%250Aaddresses%2520the%2520issues%2520related%2520to%2520the%2520high%2520computing%2520requirements%2520of%2520current%250Amodels%252C%2520with%2520the%2520goal%2520of%2520achieving%2520competitive%2520accuracy%2520on%2520consumer%2520and%2520edge%250Adevices%252C%2520enabling%2520smart%2520home%2520and%2520smart%2520healthcare%2520applications%2520where%2520efficiency%250Aand%2520privacy%2520issues%2520are%2520of%2520concern.%2520We%2520experimentally%2520validate%2520our%2520model%2520on%250Adifferent%2520established%2520and%2520publicly%2520available%2520video%2520activity%2520recognition%250Abenchmarks%252C%2520improving%2520accuracy%2520over%2520alternative%2520models%2520at%2520a%2520competitive%250Acomputing%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19928v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CA3D%3A%20Convolutional-Attentional%203D%20Nets%20for%20Efficient%20Video%20Activity%0A%20%20Recognition%20on%20the%20Edge&entry.906535625=Gabriele%20Lagani%20and%20Fabrizio%20Falchi%20and%20Claudio%20Gennaro%20and%20Giuseppe%20Amato&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20deep%20learning%20solution%20for%20video%20activity%0Arecognition%20that%20leverages%20an%20innovative%20combination%20of%20convolutional%20layers%0Awith%20a%20linear-complexity%20attention%20mechanism.%20Moreover%2C%20we%20introduce%20a%20novel%0Aquantization%20mechanism%20to%20further%20improve%20the%20efficiency%20of%20our%20model%20during%0Aboth%20training%20and%20inference.%20Our%20model%20maintains%20a%20reduced%20computational%20cost%2C%0Awhile%20preserving%20robust%20learning%20and%20generalization%20capabilities.%20Our%20approach%0Aaddresses%20the%20issues%20related%20to%20the%20high%20computing%20requirements%20of%20current%0Amodels%2C%20with%20the%20goal%20of%20achieving%20competitive%20accuracy%20on%20consumer%20and%20edge%0Adevices%2C%20enabling%20smart%20home%20and%20smart%20healthcare%20applications%20where%20efficiency%0Aand%20privacy%20issues%20are%20of%20concern.%20We%20experimentally%20validate%20our%20model%20on%0Adifferent%20established%20and%20publicly%20available%20video%20activity%20recognition%0Abenchmarks%2C%20improving%20accuracy%20over%20alternative%20models%20at%20a%20competitive%0Acomputing%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19928v1&entry.124074799=Read"},
{"title": "Beyond Specialization: Benchmarking LLMs for Transliteration of Indian\n  Languages", "author": "Gulfarogh Azam and Mohd Sadique and Saif Ali and Mohammad Nadeem and Erik Cambria and Shahab Saquib Sohail and Mohammad Sultan Alam", "abstract": "  Transliteration, the process of mapping text from one script to another,\nplays a crucial role in multilingual natural language processing, especially\nwithin linguistically diverse contexts such as India. Despite significant\nadvancements through specialized models like IndicXlit, recent developments in\nlarge language models suggest a potential for general-purpose models to excel\nat this task without explicit task-specific training. The current work\nsystematically evaluates the performance of prominent LLMs, including GPT-4o,\nGPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a\nstate-of-the-art transliteration model, across ten major Indian languages.\nExperiments utilized standard benchmarks, including Dakshina and Aksharantar\ndatasets, with performance assessed via Top-1 Accuracy and Character Error\nRate. Our findings reveal that while GPT family models generally outperform\nother LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o\nimproves performance on specific languages notably. An extensive error analysis\nand robustness testing under noisy conditions further elucidate strengths of\nLLMs compared to specialized models, highlighting the efficacy of foundational\nmodels for a wide spectrum of specialized applications with minimal overhead.\n", "link": "http://arxiv.org/abs/2505.19851v1", "date": "2025-05-26", "relevancy": 2.2143, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4493}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.43}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Specialization%3A%20Benchmarking%20LLMs%20for%20Transliteration%20of%20Indian%0A%20%20Languages&body=Title%3A%20Beyond%20Specialization%3A%20Benchmarking%20LLMs%20for%20Transliteration%20of%20Indian%0A%20%20Languages%0AAuthor%3A%20Gulfarogh%20Azam%20and%20Mohd%20Sadique%20and%20Saif%20Ali%20and%20Mohammad%20Nadeem%20and%20Erik%20Cambria%20and%20Shahab%20Saquib%20Sohail%20and%20Mohammad%20Sultan%20Alam%0AAbstract%3A%20%20%20Transliteration%2C%20the%20process%20of%20mapping%20text%20from%20one%20script%20to%20another%2C%0Aplays%20a%20crucial%20role%20in%20multilingual%20natural%20language%20processing%2C%20especially%0Awithin%20linguistically%20diverse%20contexts%20such%20as%20India.%20Despite%20significant%0Aadvancements%20through%20specialized%20models%20like%20IndicXlit%2C%20recent%20developments%20in%0Alarge%20language%20models%20suggest%20a%20potential%20for%20general-purpose%20models%20to%20excel%0Aat%20this%20task%20without%20explicit%20task-specific%20training.%20The%20current%20work%0Asystematically%20evaluates%20the%20performance%20of%20prominent%20LLMs%2C%20including%20GPT-4o%2C%0AGPT-4.5%2C%20GPT-4.1%2C%20Gemma-3-27B-it%2C%20and%20Mistral-Large%20against%20IndicXlit%2C%20a%0Astate-of-the-art%20transliteration%20model%2C%20across%20ten%20major%20Indian%20languages.%0AExperiments%20utilized%20standard%20benchmarks%2C%20including%20Dakshina%20and%20Aksharantar%0Adatasets%2C%20with%20performance%20assessed%20via%20Top-1%20Accuracy%20and%20Character%20Error%0ARate.%20Our%20findings%20reveal%20that%20while%20GPT%20family%20models%20generally%20outperform%0Aother%20LLMs%20and%20IndicXlit%20for%20most%20instances.%20Additionally%2C%20fine-tuning%20GPT-4o%0Aimproves%20performance%20on%20specific%20languages%20notably.%20An%20extensive%20error%20analysis%0Aand%20robustness%20testing%20under%20noisy%20conditions%20further%20elucidate%20strengths%20of%0ALLMs%20compared%20to%20specialized%20models%2C%20highlighting%20the%20efficacy%20of%20foundational%0Amodels%20for%20a%20wide%20spectrum%20of%20specialized%20applications%20with%20minimal%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19851v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Specialization%253A%2520Benchmarking%2520LLMs%2520for%2520Transliteration%2520of%2520Indian%250A%2520%2520Languages%26entry.906535625%3DGulfarogh%2520Azam%2520and%2520Mohd%2520Sadique%2520and%2520Saif%2520Ali%2520and%2520Mohammad%2520Nadeem%2520and%2520Erik%2520Cambria%2520and%2520Shahab%2520Saquib%2520Sohail%2520and%2520Mohammad%2520Sultan%2520Alam%26entry.1292438233%3D%2520%2520Transliteration%252C%2520the%2520process%2520of%2520mapping%2520text%2520from%2520one%2520script%2520to%2520another%252C%250Aplays%2520a%2520crucial%2520role%2520in%2520multilingual%2520natural%2520language%2520processing%252C%2520especially%250Awithin%2520linguistically%2520diverse%2520contexts%2520such%2520as%2520India.%2520Despite%2520significant%250Aadvancements%2520through%2520specialized%2520models%2520like%2520IndicXlit%252C%2520recent%2520developments%2520in%250Alarge%2520language%2520models%2520suggest%2520a%2520potential%2520for%2520general-purpose%2520models%2520to%2520excel%250Aat%2520this%2520task%2520without%2520explicit%2520task-specific%2520training.%2520The%2520current%2520work%250Asystematically%2520evaluates%2520the%2520performance%2520of%2520prominent%2520LLMs%252C%2520including%2520GPT-4o%252C%250AGPT-4.5%252C%2520GPT-4.1%252C%2520Gemma-3-27B-it%252C%2520and%2520Mistral-Large%2520against%2520IndicXlit%252C%2520a%250Astate-of-the-art%2520transliteration%2520model%252C%2520across%2520ten%2520major%2520Indian%2520languages.%250AExperiments%2520utilized%2520standard%2520benchmarks%252C%2520including%2520Dakshina%2520and%2520Aksharantar%250Adatasets%252C%2520with%2520performance%2520assessed%2520via%2520Top-1%2520Accuracy%2520and%2520Character%2520Error%250ARate.%2520Our%2520findings%2520reveal%2520that%2520while%2520GPT%2520family%2520models%2520generally%2520outperform%250Aother%2520LLMs%2520and%2520IndicXlit%2520for%2520most%2520instances.%2520Additionally%252C%2520fine-tuning%2520GPT-4o%250Aimproves%2520performance%2520on%2520specific%2520languages%2520notably.%2520An%2520extensive%2520error%2520analysis%250Aand%2520robustness%2520testing%2520under%2520noisy%2520conditions%2520further%2520elucidate%2520strengths%2520of%250ALLMs%2520compared%2520to%2520specialized%2520models%252C%2520highlighting%2520the%2520efficacy%2520of%2520foundational%250Amodels%2520for%2520a%2520wide%2520spectrum%2520of%2520specialized%2520applications%2520with%2520minimal%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19851v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Specialization%3A%20Benchmarking%20LLMs%20for%20Transliteration%20of%20Indian%0A%20%20Languages&entry.906535625=Gulfarogh%20Azam%20and%20Mohd%20Sadique%20and%20Saif%20Ali%20and%20Mohammad%20Nadeem%20and%20Erik%20Cambria%20and%20Shahab%20Saquib%20Sohail%20and%20Mohammad%20Sultan%20Alam&entry.1292438233=%20%20Transliteration%2C%20the%20process%20of%20mapping%20text%20from%20one%20script%20to%20another%2C%0Aplays%20a%20crucial%20role%20in%20multilingual%20natural%20language%20processing%2C%20especially%0Awithin%20linguistically%20diverse%20contexts%20such%20as%20India.%20Despite%20significant%0Aadvancements%20through%20specialized%20models%20like%20IndicXlit%2C%20recent%20developments%20in%0Alarge%20language%20models%20suggest%20a%20potential%20for%20general-purpose%20models%20to%20excel%0Aat%20this%20task%20without%20explicit%20task-specific%20training.%20The%20current%20work%0Asystematically%20evaluates%20the%20performance%20of%20prominent%20LLMs%2C%20including%20GPT-4o%2C%0AGPT-4.5%2C%20GPT-4.1%2C%20Gemma-3-27B-it%2C%20and%20Mistral-Large%20against%20IndicXlit%2C%20a%0Astate-of-the-art%20transliteration%20model%2C%20across%20ten%20major%20Indian%20languages.%0AExperiments%20utilized%20standard%20benchmarks%2C%20including%20Dakshina%20and%20Aksharantar%0Adatasets%2C%20with%20performance%20assessed%20via%20Top-1%20Accuracy%20and%20Character%20Error%0ARate.%20Our%20findings%20reveal%20that%20while%20GPT%20family%20models%20generally%20outperform%0Aother%20LLMs%20and%20IndicXlit%20for%20most%20instances.%20Additionally%2C%20fine-tuning%20GPT-4o%0Aimproves%20performance%20on%20specific%20languages%20notably.%20An%20extensive%20error%20analysis%0Aand%20robustness%20testing%20under%20noisy%20conditions%20further%20elucidate%20strengths%20of%0ALLMs%20compared%20to%20specialized%20models%2C%20highlighting%20the%20efficacy%20of%20foundational%0Amodels%20for%20a%20wide%20spectrum%20of%20specialized%20applications%20with%20minimal%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19851v1&entry.124074799=Read"},
{"title": "What Can RL Bring to VLA Generalization? An Empirical Study", "author": "Jijia Liu and Feng Gao and Bingwen Wei and Xinlei Chen and Qingmin Liao and Yi Wu and Chao Yu and Yu Wang", "abstract": "  Large Vision-Language Action (VLA) models have shown significant potential\nfor embodied AI. However, their predominant training via supervised fine-tuning\n(SFT) limits generalization due to susceptibility to compounding errors under\ndistribution shifts. Reinforcement learning (RL) offers a path to overcome\nthese limitations by optimizing for task objectives via trial-and-error, yet a\nsystematic understanding of its specific generalization benefits for VLAs\ncompared to SFT is lacking. To address this, our study introduces a\ncomprehensive benchmark for evaluating VLA generalization and systematically\ninvestigates the impact of RL fine-tuning across diverse visual, semantic, and\nexecution dimensions. Our extensive experiments reveal that RL fine-tuning,\nparticularly with PPO, significantly enhances generalization in semantic\nunderstanding and execution robustness over SFT, while maintaining comparable\nvisual robustness. We identify PPO as a more effective RL algorithm for VLAs\nthan LLM-derived methods like DPO and GRPO. We also develop a simple recipe for\nefficient PPO training on VLAs, and demonstrate its practical utility for\nimproving VLA generalization. The project page is at https://rlvla.github.io\n", "link": "http://arxiv.org/abs/2505.19789v1", "date": "2025-05-26", "relevancy": 2.2139, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5609}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5609}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Can%20RL%20Bring%20to%20VLA%20Generalization%3F%20An%20Empirical%20Study&body=Title%3A%20What%20Can%20RL%20Bring%20to%20VLA%20Generalization%3F%20An%20Empirical%20Study%0AAuthor%3A%20Jijia%20Liu%20and%20Feng%20Gao%20and%20Bingwen%20Wei%20and%20Xinlei%20Chen%20and%20Qingmin%20Liao%20and%20Yi%20Wu%20and%20Chao%20Yu%20and%20Yu%20Wang%0AAbstract%3A%20%20%20Large%20Vision-Language%20Action%20%28VLA%29%20models%20have%20shown%20significant%20potential%0Afor%20embodied%20AI.%20However%2C%20their%20predominant%20training%20via%20supervised%20fine-tuning%0A%28SFT%29%20limits%20generalization%20due%20to%20susceptibility%20to%20compounding%20errors%20under%0Adistribution%20shifts.%20Reinforcement%20learning%20%28RL%29%20offers%20a%20path%20to%20overcome%0Athese%20limitations%20by%20optimizing%20for%20task%20objectives%20via%20trial-and-error%2C%20yet%20a%0Asystematic%20understanding%20of%20its%20specific%20generalization%20benefits%20for%20VLAs%0Acompared%20to%20SFT%20is%20lacking.%20To%20address%20this%2C%20our%20study%20introduces%20a%0Acomprehensive%20benchmark%20for%20evaluating%20VLA%20generalization%20and%20systematically%0Ainvestigates%20the%20impact%20of%20RL%20fine-tuning%20across%20diverse%20visual%2C%20semantic%2C%20and%0Aexecution%20dimensions.%20Our%20extensive%20experiments%20reveal%20that%20RL%20fine-tuning%2C%0Aparticularly%20with%20PPO%2C%20significantly%20enhances%20generalization%20in%20semantic%0Aunderstanding%20and%20execution%20robustness%20over%20SFT%2C%20while%20maintaining%20comparable%0Avisual%20robustness.%20We%20identify%20PPO%20as%20a%20more%20effective%20RL%20algorithm%20for%20VLAs%0Athan%20LLM-derived%20methods%20like%20DPO%20and%20GRPO.%20We%20also%20develop%20a%20simple%20recipe%20for%0Aefficient%20PPO%20training%20on%20VLAs%2C%20and%20demonstrate%20its%20practical%20utility%20for%0Aimproving%20VLA%20generalization.%20The%20project%20page%20is%20at%20https%3A//rlvla.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Can%2520RL%2520Bring%2520to%2520VLA%2520Generalization%253F%2520An%2520Empirical%2520Study%26entry.906535625%3DJijia%2520Liu%2520and%2520Feng%2520Gao%2520and%2520Bingwen%2520Wei%2520and%2520Xinlei%2520Chen%2520and%2520Qingmin%2520Liao%2520and%2520Yi%2520Wu%2520and%2520Chao%2520Yu%2520and%2520Yu%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Action%2520%2528VLA%2529%2520models%2520have%2520shown%2520significant%2520potential%250Afor%2520embodied%2520AI.%2520However%252C%2520their%2520predominant%2520training%2520via%2520supervised%2520fine-tuning%250A%2528SFT%2529%2520limits%2520generalization%2520due%2520to%2520susceptibility%2520to%2520compounding%2520errors%2520under%250Adistribution%2520shifts.%2520Reinforcement%2520learning%2520%2528RL%2529%2520offers%2520a%2520path%2520to%2520overcome%250Athese%2520limitations%2520by%2520optimizing%2520for%2520task%2520objectives%2520via%2520trial-and-error%252C%2520yet%2520a%250Asystematic%2520understanding%2520of%2520its%2520specific%2520generalization%2520benefits%2520for%2520VLAs%250Acompared%2520to%2520SFT%2520is%2520lacking.%2520To%2520address%2520this%252C%2520our%2520study%2520introduces%2520a%250Acomprehensive%2520benchmark%2520for%2520evaluating%2520VLA%2520generalization%2520and%2520systematically%250Ainvestigates%2520the%2520impact%2520of%2520RL%2520fine-tuning%2520across%2520diverse%2520visual%252C%2520semantic%252C%2520and%250Aexecution%2520dimensions.%2520Our%2520extensive%2520experiments%2520reveal%2520that%2520RL%2520fine-tuning%252C%250Aparticularly%2520with%2520PPO%252C%2520significantly%2520enhances%2520generalization%2520in%2520semantic%250Aunderstanding%2520and%2520execution%2520robustness%2520over%2520SFT%252C%2520while%2520maintaining%2520comparable%250Avisual%2520robustness.%2520We%2520identify%2520PPO%2520as%2520a%2520more%2520effective%2520RL%2520algorithm%2520for%2520VLAs%250Athan%2520LLM-derived%2520methods%2520like%2520DPO%2520and%2520GRPO.%2520We%2520also%2520develop%2520a%2520simple%2520recipe%2520for%250Aefficient%2520PPO%2520training%2520on%2520VLAs%252C%2520and%2520demonstrate%2520its%2520practical%2520utility%2520for%250Aimproving%2520VLA%2520generalization.%2520The%2520project%2520page%2520is%2520at%2520https%253A//rlvla.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Can%20RL%20Bring%20to%20VLA%20Generalization%3F%20An%20Empirical%20Study&entry.906535625=Jijia%20Liu%20and%20Feng%20Gao%20and%20Bingwen%20Wei%20and%20Xinlei%20Chen%20and%20Qingmin%20Liao%20and%20Yi%20Wu%20and%20Chao%20Yu%20and%20Yu%20Wang&entry.1292438233=%20%20Large%20Vision-Language%20Action%20%28VLA%29%20models%20have%20shown%20significant%20potential%0Afor%20embodied%20AI.%20However%2C%20their%20predominant%20training%20via%20supervised%20fine-tuning%0A%28SFT%29%20limits%20generalization%20due%20to%20susceptibility%20to%20compounding%20errors%20under%0Adistribution%20shifts.%20Reinforcement%20learning%20%28RL%29%20offers%20a%20path%20to%20overcome%0Athese%20limitations%20by%20optimizing%20for%20task%20objectives%20via%20trial-and-error%2C%20yet%20a%0Asystematic%20understanding%20of%20its%20specific%20generalization%20benefits%20for%20VLAs%0Acompared%20to%20SFT%20is%20lacking.%20To%20address%20this%2C%20our%20study%20introduces%20a%0Acomprehensive%20benchmark%20for%20evaluating%20VLA%20generalization%20and%20systematically%0Ainvestigates%20the%20impact%20of%20RL%20fine-tuning%20across%20diverse%20visual%2C%20semantic%2C%20and%0Aexecution%20dimensions.%20Our%20extensive%20experiments%20reveal%20that%20RL%20fine-tuning%2C%0Aparticularly%20with%20PPO%2C%20significantly%20enhances%20generalization%20in%20semantic%0Aunderstanding%20and%20execution%20robustness%20over%20SFT%2C%20while%20maintaining%20comparable%0Avisual%20robustness.%20We%20identify%20PPO%20as%20a%20more%20effective%20RL%20algorithm%20for%20VLAs%0Athan%20LLM-derived%20methods%20like%20DPO%20and%20GRPO.%20We%20also%20develop%20a%20simple%20recipe%20for%0Aefficient%20PPO%20training%20on%20VLAs%2C%20and%20demonstrate%20its%20practical%20utility%20for%0Aimproving%20VLA%20generalization.%20The%20project%20page%20is%20at%20https%3A//rlvla.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19789v1&entry.124074799=Read"},
{"title": "Compile Scene Graphs with Reinforcement Learning", "author": "Zuyao Chen and Jinlin Wu and Zhen Lei and Marc Pollefeys and Chang Wen Chen", "abstract": "  Next-token prediction is the fundamental principle for training large\nlanguage models (LLMs), and reinforcement learning (RL) further enhances their\nreasoning performance. As an effective way to model language, image, video, and\nother modalities, the use of LLMs for end-to-end extraction of structured\nvisual representations, such as scene graphs, remains underexplored. It\nrequires the model to accurately produce a set of objects and relationship\ntriplets, rather than generating text token by token. To achieve this, we\nintroduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised\nfine-tuning (SFT) on the scene graph dataset and subsequently refined using\nreinforcement learning to enhance its ability to generate scene graphs in an\nend-to-end manner. The SFT follows a conventional prompt-response paradigm,\nwhile RL requires the design of effective reward signals. We design a set of\ngraph-centric rewards, including three recall-based variants -- Hard Recall,\nHard Recall+Relax, and Soft Recall -- which evaluate semantic and spatial\nalignment between predictions and ground truth at the object and relation\nlevels. A format consistency reward further ensures that outputs follow the\nexpected structural schema. Extensive experiments on the VG150 and PSG\nbenchmarks show that R1-SGG substantially reduces failure rates and achieves\nstrong performance in Recall and mean Recall, surpassing traditional SGG models\nand existing multimodal language models. Our code is available at\nhttps://github.com/gpt4vision/R1-SGG\n", "link": "http://arxiv.org/abs/2504.13617v4", "date": "2025-05-26", "relevancy": 2.2109, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.552}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compile%20Scene%20Graphs%20with%20Reinforcement%20Learning&body=Title%3A%20Compile%20Scene%20Graphs%20with%20Reinforcement%20Learning%0AAuthor%3A%20Zuyao%20Chen%20and%20Jinlin%20Wu%20and%20Zhen%20Lei%20and%20Marc%20Pollefeys%20and%20Chang%20Wen%20Chen%0AAbstract%3A%20%20%20Next-token%20prediction%20is%20the%20fundamental%20principle%20for%20training%20large%0Alanguage%20models%20%28LLMs%29%2C%20and%20reinforcement%20learning%20%28RL%29%20further%20enhances%20their%0Areasoning%20performance.%20As%20an%20effective%20way%20to%20model%20language%2C%20image%2C%20video%2C%20and%0Aother%20modalities%2C%20the%20use%20of%20LLMs%20for%20end-to-end%20extraction%20of%20structured%0Avisual%20representations%2C%20such%20as%20scene%20graphs%2C%20remains%20underexplored.%20It%0Arequires%20the%20model%20to%20accurately%20produce%20a%20set%20of%20objects%20and%20relationship%0Atriplets%2C%20rather%20than%20generating%20text%20token%20by%20token.%20To%20achieve%20this%2C%20we%0Aintroduce%20R1-SGG%2C%20a%20multimodal%20LLM%20%28M-LLM%29%20initially%20trained%20via%20supervised%0Afine-tuning%20%28SFT%29%20on%20the%20scene%20graph%20dataset%20and%20subsequently%20refined%20using%0Areinforcement%20learning%20to%20enhance%20its%20ability%20to%20generate%20scene%20graphs%20in%20an%0Aend-to-end%20manner.%20The%20SFT%20follows%20a%20conventional%20prompt-response%20paradigm%2C%0Awhile%20RL%20requires%20the%20design%20of%20effective%20reward%20signals.%20We%20design%20a%20set%20of%0Agraph-centric%20rewards%2C%20including%20three%20recall-based%20variants%20--%20Hard%20Recall%2C%0AHard%20Recall%2BRelax%2C%20and%20Soft%20Recall%20--%20which%20evaluate%20semantic%20and%20spatial%0Aalignment%20between%20predictions%20and%20ground%20truth%20at%20the%20object%20and%20relation%0Alevels.%20A%20format%20consistency%20reward%20further%20ensures%20that%20outputs%20follow%20the%0Aexpected%20structural%20schema.%20Extensive%20experiments%20on%20the%20VG150%20and%20PSG%0Abenchmarks%20show%20that%20R1-SGG%20substantially%20reduces%20failure%20rates%20and%20achieves%0Astrong%20performance%20in%20Recall%20and%20mean%20Recall%2C%20surpassing%20traditional%20SGG%20models%0Aand%20existing%20multimodal%20language%20models.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/gpt4vision/R1-SGG%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13617v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompile%2520Scene%2520Graphs%2520with%2520Reinforcement%2520Learning%26entry.906535625%3DZuyao%2520Chen%2520and%2520Jinlin%2520Wu%2520and%2520Zhen%2520Lei%2520and%2520Marc%2520Pollefeys%2520and%2520Chang%2520Wen%2520Chen%26entry.1292438233%3D%2520%2520Next-token%2520prediction%2520is%2520the%2520fundamental%2520principle%2520for%2520training%2520large%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520and%2520reinforcement%2520learning%2520%2528RL%2529%2520further%2520enhances%2520their%250Areasoning%2520performance.%2520As%2520an%2520effective%2520way%2520to%2520model%2520language%252C%2520image%252C%2520video%252C%2520and%250Aother%2520modalities%252C%2520the%2520use%2520of%2520LLMs%2520for%2520end-to-end%2520extraction%2520of%2520structured%250Avisual%2520representations%252C%2520such%2520as%2520scene%2520graphs%252C%2520remains%2520underexplored.%2520It%250Arequires%2520the%2520model%2520to%2520accurately%2520produce%2520a%2520set%2520of%2520objects%2520and%2520relationship%250Atriplets%252C%2520rather%2520than%2520generating%2520text%2520token%2520by%2520token.%2520To%2520achieve%2520this%252C%2520we%250Aintroduce%2520R1-SGG%252C%2520a%2520multimodal%2520LLM%2520%2528M-LLM%2529%2520initially%2520trained%2520via%2520supervised%250Afine-tuning%2520%2528SFT%2529%2520on%2520the%2520scene%2520graph%2520dataset%2520and%2520subsequently%2520refined%2520using%250Areinforcement%2520learning%2520to%2520enhance%2520its%2520ability%2520to%2520generate%2520scene%2520graphs%2520in%2520an%250Aend-to-end%2520manner.%2520The%2520SFT%2520follows%2520a%2520conventional%2520prompt-response%2520paradigm%252C%250Awhile%2520RL%2520requires%2520the%2520design%2520of%2520effective%2520reward%2520signals.%2520We%2520design%2520a%2520set%2520of%250Agraph-centric%2520rewards%252C%2520including%2520three%2520recall-based%2520variants%2520--%2520Hard%2520Recall%252C%250AHard%2520Recall%252BRelax%252C%2520and%2520Soft%2520Recall%2520--%2520which%2520evaluate%2520semantic%2520and%2520spatial%250Aalignment%2520between%2520predictions%2520and%2520ground%2520truth%2520at%2520the%2520object%2520and%2520relation%250Alevels.%2520A%2520format%2520consistency%2520reward%2520further%2520ensures%2520that%2520outputs%2520follow%2520the%250Aexpected%2520structural%2520schema.%2520Extensive%2520experiments%2520on%2520the%2520VG150%2520and%2520PSG%250Abenchmarks%2520show%2520that%2520R1-SGG%2520substantially%2520reduces%2520failure%2520rates%2520and%2520achieves%250Astrong%2520performance%2520in%2520Recall%2520and%2520mean%2520Recall%252C%2520surpassing%2520traditional%2520SGG%2520models%250Aand%2520existing%2520multimodal%2520language%2520models.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/gpt4vision/R1-SGG%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13617v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compile%20Scene%20Graphs%20with%20Reinforcement%20Learning&entry.906535625=Zuyao%20Chen%20and%20Jinlin%20Wu%20and%20Zhen%20Lei%20and%20Marc%20Pollefeys%20and%20Chang%20Wen%20Chen&entry.1292438233=%20%20Next-token%20prediction%20is%20the%20fundamental%20principle%20for%20training%20large%0Alanguage%20models%20%28LLMs%29%2C%20and%20reinforcement%20learning%20%28RL%29%20further%20enhances%20their%0Areasoning%20performance.%20As%20an%20effective%20way%20to%20model%20language%2C%20image%2C%20video%2C%20and%0Aother%20modalities%2C%20the%20use%20of%20LLMs%20for%20end-to-end%20extraction%20of%20structured%0Avisual%20representations%2C%20such%20as%20scene%20graphs%2C%20remains%20underexplored.%20It%0Arequires%20the%20model%20to%20accurately%20produce%20a%20set%20of%20objects%20and%20relationship%0Atriplets%2C%20rather%20than%20generating%20text%20token%20by%20token.%20To%20achieve%20this%2C%20we%0Aintroduce%20R1-SGG%2C%20a%20multimodal%20LLM%20%28M-LLM%29%20initially%20trained%20via%20supervised%0Afine-tuning%20%28SFT%29%20on%20the%20scene%20graph%20dataset%20and%20subsequently%20refined%20using%0Areinforcement%20learning%20to%20enhance%20its%20ability%20to%20generate%20scene%20graphs%20in%20an%0Aend-to-end%20manner.%20The%20SFT%20follows%20a%20conventional%20prompt-response%20paradigm%2C%0Awhile%20RL%20requires%20the%20design%20of%20effective%20reward%20signals.%20We%20design%20a%20set%20of%0Agraph-centric%20rewards%2C%20including%20three%20recall-based%20variants%20--%20Hard%20Recall%2C%0AHard%20Recall%2BRelax%2C%20and%20Soft%20Recall%20--%20which%20evaluate%20semantic%20and%20spatial%0Aalignment%20between%20predictions%20and%20ground%20truth%20at%20the%20object%20and%20relation%0Alevels.%20A%20format%20consistency%20reward%20further%20ensures%20that%20outputs%20follow%20the%0Aexpected%20structural%20schema.%20Extensive%20experiments%20on%20the%20VG150%20and%20PSG%0Abenchmarks%20show%20that%20R1-SGG%20substantially%20reduces%20failure%20rates%20and%20achieves%0Astrong%20performance%20in%20Recall%20and%20mean%20Recall%2C%20surpassing%20traditional%20SGG%20models%0Aand%20existing%20multimodal%20language%20models.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/gpt4vision/R1-SGG%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13617v4&entry.124074799=Read"},
{"title": "An Explainable Diagnostic Framework for Neurodegenerative Dementias via\n  Reinforcement-Optimized LLM Reasoning", "author": "Andrew Zamai and Nathanael Fijalkow and Boris Mansencal and Laurent Simon and Eloi Navet and Pierrick Coupe", "abstract": "  The differential diagnosis of neurodegenerative dementias is a challenging\nclinical task, mainly because of the overlap in symptom presentation and the\nsimilarity of patterns observed in structural neuroimaging. To improve\ndiagnostic efficiency and accuracy, deep learning-based methods such as\nConvolutional Neural Networks and Vision Transformers have been proposed for\nthe automatic classification of brain MRIs. However, despite their strong\npredictive performance, these models find limited clinical utility due to their\nopaque decision making. In this work, we propose a framework that integrates\ntwo core components to enhance diagnostic transparency. First, we introduce a\nmodular pipeline for converting 3D T1-weighted brain MRIs into textual\nradiology reports. Second, we explore the potential of modern Large Language\nModels (LLMs) to assist clinicians in the differential diagnosis between\nFrontotemporal dementia subtypes, Alzheimer's disease, and normal aging based\non the generated reports. To bridge the gap between predictive accuracy and\nexplainability, we employ reinforcement learning to incentivize diagnostic\nreasoning in LLMs. Without requiring supervised reasoning traces or\ndistillation from larger models, our approach enables the emergence of\nstructured diagnostic rationales grounded in neuroimaging findings. Unlike\npost-hoc explainability methods that retrospectively justify model decisions,\nour framework generates diagnostic rationales as part of the inference\nprocess-producing causally grounded explanations that inform and guide the\nmodel's decision-making process. In doing so, our framework matches the\ndiagnostic performance of existing deep learning methods while offering\nrationales that support its diagnostic conclusions.\n", "link": "http://arxiv.org/abs/2505.19954v1", "date": "2025-05-26", "relevancy": 2.2087, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5538}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Explainable%20Diagnostic%20Framework%20for%20Neurodegenerative%20Dementias%20via%0A%20%20Reinforcement-Optimized%20LLM%20Reasoning&body=Title%3A%20An%20Explainable%20Diagnostic%20Framework%20for%20Neurodegenerative%20Dementias%20via%0A%20%20Reinforcement-Optimized%20LLM%20Reasoning%0AAuthor%3A%20Andrew%20Zamai%20and%20Nathanael%20Fijalkow%20and%20Boris%20Mansencal%20and%20Laurent%20Simon%20and%20Eloi%20Navet%20and%20Pierrick%20Coupe%0AAbstract%3A%20%20%20The%20differential%20diagnosis%20of%20neurodegenerative%20dementias%20is%20a%20challenging%0Aclinical%20task%2C%20mainly%20because%20of%20the%20overlap%20in%20symptom%20presentation%20and%20the%0Asimilarity%20of%20patterns%20observed%20in%20structural%20neuroimaging.%20To%20improve%0Adiagnostic%20efficiency%20and%20accuracy%2C%20deep%20learning-based%20methods%20such%20as%0AConvolutional%20Neural%20Networks%20and%20Vision%20Transformers%20have%20been%20proposed%20for%0Athe%20automatic%20classification%20of%20brain%20MRIs.%20However%2C%20despite%20their%20strong%0Apredictive%20performance%2C%20these%20models%20find%20limited%20clinical%20utility%20due%20to%20their%0Aopaque%20decision%20making.%20In%20this%20work%2C%20we%20propose%20a%20framework%20that%20integrates%0Atwo%20core%20components%20to%20enhance%20diagnostic%20transparency.%20First%2C%20we%20introduce%20a%0Amodular%20pipeline%20for%20converting%203D%20T1-weighted%20brain%20MRIs%20into%20textual%0Aradiology%20reports.%20Second%2C%20we%20explore%20the%20potential%20of%20modern%20Large%20Language%0AModels%20%28LLMs%29%20to%20assist%20clinicians%20in%20the%20differential%20diagnosis%20between%0AFrontotemporal%20dementia%20subtypes%2C%20Alzheimer%27s%20disease%2C%20and%20normal%20aging%20based%0Aon%20the%20generated%20reports.%20To%20bridge%20the%20gap%20between%20predictive%20accuracy%20and%0Aexplainability%2C%20we%20employ%20reinforcement%20learning%20to%20incentivize%20diagnostic%0Areasoning%20in%20LLMs.%20Without%20requiring%20supervised%20reasoning%20traces%20or%0Adistillation%20from%20larger%20models%2C%20our%20approach%20enables%20the%20emergence%20of%0Astructured%20diagnostic%20rationales%20grounded%20in%20neuroimaging%20findings.%20Unlike%0Apost-hoc%20explainability%20methods%20that%20retrospectively%20justify%20model%20decisions%2C%0Aour%20framework%20generates%20diagnostic%20rationales%20as%20part%20of%20the%20inference%0Aprocess-producing%20causally%20grounded%20explanations%20that%20inform%20and%20guide%20the%0Amodel%27s%20decision-making%20process.%20In%20doing%20so%2C%20our%20framework%20matches%20the%0Adiagnostic%20performance%20of%20existing%20deep%20learning%20methods%20while%20offering%0Arationales%20that%20support%20its%20diagnostic%20conclusions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19954v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Explainable%2520Diagnostic%2520Framework%2520for%2520Neurodegenerative%2520Dementias%2520via%250A%2520%2520Reinforcement-Optimized%2520LLM%2520Reasoning%26entry.906535625%3DAndrew%2520Zamai%2520and%2520Nathanael%2520Fijalkow%2520and%2520Boris%2520Mansencal%2520and%2520Laurent%2520Simon%2520and%2520Eloi%2520Navet%2520and%2520Pierrick%2520Coupe%26entry.1292438233%3D%2520%2520The%2520differential%2520diagnosis%2520of%2520neurodegenerative%2520dementias%2520is%2520a%2520challenging%250Aclinical%2520task%252C%2520mainly%2520because%2520of%2520the%2520overlap%2520in%2520symptom%2520presentation%2520and%2520the%250Asimilarity%2520of%2520patterns%2520observed%2520in%2520structural%2520neuroimaging.%2520To%2520improve%250Adiagnostic%2520efficiency%2520and%2520accuracy%252C%2520deep%2520learning-based%2520methods%2520such%2520as%250AConvolutional%2520Neural%2520Networks%2520and%2520Vision%2520Transformers%2520have%2520been%2520proposed%2520for%250Athe%2520automatic%2520classification%2520of%2520brain%2520MRIs.%2520However%252C%2520despite%2520their%2520strong%250Apredictive%2520performance%252C%2520these%2520models%2520find%2520limited%2520clinical%2520utility%2520due%2520to%2520their%250Aopaque%2520decision%2520making.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520framework%2520that%2520integrates%250Atwo%2520core%2520components%2520to%2520enhance%2520diagnostic%2520transparency.%2520First%252C%2520we%2520introduce%2520a%250Amodular%2520pipeline%2520for%2520converting%25203D%2520T1-weighted%2520brain%2520MRIs%2520into%2520textual%250Aradiology%2520reports.%2520Second%252C%2520we%2520explore%2520the%2520potential%2520of%2520modern%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520to%2520assist%2520clinicians%2520in%2520the%2520differential%2520diagnosis%2520between%250AFrontotemporal%2520dementia%2520subtypes%252C%2520Alzheimer%2527s%2520disease%252C%2520and%2520normal%2520aging%2520based%250Aon%2520the%2520generated%2520reports.%2520To%2520bridge%2520the%2520gap%2520between%2520predictive%2520accuracy%2520and%250Aexplainability%252C%2520we%2520employ%2520reinforcement%2520learning%2520to%2520incentivize%2520diagnostic%250Areasoning%2520in%2520LLMs.%2520Without%2520requiring%2520supervised%2520reasoning%2520traces%2520or%250Adistillation%2520from%2520larger%2520models%252C%2520our%2520approach%2520enables%2520the%2520emergence%2520of%250Astructured%2520diagnostic%2520rationales%2520grounded%2520in%2520neuroimaging%2520findings.%2520Unlike%250Apost-hoc%2520explainability%2520methods%2520that%2520retrospectively%2520justify%2520model%2520decisions%252C%250Aour%2520framework%2520generates%2520diagnostic%2520rationales%2520as%2520part%2520of%2520the%2520inference%250Aprocess-producing%2520causally%2520grounded%2520explanations%2520that%2520inform%2520and%2520guide%2520the%250Amodel%2527s%2520decision-making%2520process.%2520In%2520doing%2520so%252C%2520our%2520framework%2520matches%2520the%250Adiagnostic%2520performance%2520of%2520existing%2520deep%2520learning%2520methods%2520while%2520offering%250Arationales%2520that%2520support%2520its%2520diagnostic%2520conclusions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19954v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Explainable%20Diagnostic%20Framework%20for%20Neurodegenerative%20Dementias%20via%0A%20%20Reinforcement-Optimized%20LLM%20Reasoning&entry.906535625=Andrew%20Zamai%20and%20Nathanael%20Fijalkow%20and%20Boris%20Mansencal%20and%20Laurent%20Simon%20and%20Eloi%20Navet%20and%20Pierrick%20Coupe&entry.1292438233=%20%20The%20differential%20diagnosis%20of%20neurodegenerative%20dementias%20is%20a%20challenging%0Aclinical%20task%2C%20mainly%20because%20of%20the%20overlap%20in%20symptom%20presentation%20and%20the%0Asimilarity%20of%20patterns%20observed%20in%20structural%20neuroimaging.%20To%20improve%0Adiagnostic%20efficiency%20and%20accuracy%2C%20deep%20learning-based%20methods%20such%20as%0AConvolutional%20Neural%20Networks%20and%20Vision%20Transformers%20have%20been%20proposed%20for%0Athe%20automatic%20classification%20of%20brain%20MRIs.%20However%2C%20despite%20their%20strong%0Apredictive%20performance%2C%20these%20models%20find%20limited%20clinical%20utility%20due%20to%20their%0Aopaque%20decision%20making.%20In%20this%20work%2C%20we%20propose%20a%20framework%20that%20integrates%0Atwo%20core%20components%20to%20enhance%20diagnostic%20transparency.%20First%2C%20we%20introduce%20a%0Amodular%20pipeline%20for%20converting%203D%20T1-weighted%20brain%20MRIs%20into%20textual%0Aradiology%20reports.%20Second%2C%20we%20explore%20the%20potential%20of%20modern%20Large%20Language%0AModels%20%28LLMs%29%20to%20assist%20clinicians%20in%20the%20differential%20diagnosis%20between%0AFrontotemporal%20dementia%20subtypes%2C%20Alzheimer%27s%20disease%2C%20and%20normal%20aging%20based%0Aon%20the%20generated%20reports.%20To%20bridge%20the%20gap%20between%20predictive%20accuracy%20and%0Aexplainability%2C%20we%20employ%20reinforcement%20learning%20to%20incentivize%20diagnostic%0Areasoning%20in%20LLMs.%20Without%20requiring%20supervised%20reasoning%20traces%20or%0Adistillation%20from%20larger%20models%2C%20our%20approach%20enables%20the%20emergence%20of%0Astructured%20diagnostic%20rationales%20grounded%20in%20neuroimaging%20findings.%20Unlike%0Apost-hoc%20explainability%20methods%20that%20retrospectively%20justify%20model%20decisions%2C%0Aour%20framework%20generates%20diagnostic%20rationales%20as%20part%20of%20the%20inference%0Aprocess-producing%20causally%20grounded%20explanations%20that%20inform%20and%20guide%20the%0Amodel%27s%20decision-making%20process.%20In%20doing%20so%2C%20our%20framework%20matches%20the%0Adiagnostic%20performance%20of%20existing%20deep%20learning%20methods%20while%20offering%0Arationales%20that%20support%20its%20diagnostic%20conclusions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19954v1&entry.124074799=Read"},
{"title": "Modeling Multi-Task Model Merging as Adaptive Projective Gradient\n  Descent", "author": "Yongxian Wei and Anke Tang and Li Shen and Zixuan Hu and Chun Yuan and Xiaochun Cao", "abstract": "  Merging multiple expert models offers a promising approach for performing\nmulti-task learning without accessing their original data. Existing methods\nattempt to alleviate task conflicts by sparsifying task vectors or promoting\northogonality among them. However, they overlook the fundamental target of\nmodel merging: the merged model performs as closely as possible to\ntask-specific models on respective tasks. We find these methods inevitably\ndiscard task-specific information that, while causing conflicts, is crucial for\nperformance. Based on our findings, we frame model merging as a constrained\noptimization problem ($\\textit{i.e.}$, minimizing the gap between the merged\nmodel and individual models, subject to the constraint of retaining shared\nknowledge) and solve it via adaptive projective gradient descent. Specifically,\nwe align the merged model with individual models by decomposing and\nreconstituting the loss function, alleviating conflicts through\n$\\textit{data-free}$ optimization of task vectors. To retain shared knowledge,\nwe optimize this objective by projecting gradients within a $\\textit{shared\nsubspace}$ spanning all tasks. Moreover, we view merging coefficients as\nadaptive learning rates and propose a task-aware, training-free strategy.\nExperiments show that our plug-and-play approach consistently outperforms\nprevious methods, achieving state-of-the-art results across diverse\narchitectures and tasks in both vision and NLP domains.\n", "link": "http://arxiv.org/abs/2501.01230v3", "date": "2025-05-26", "relevancy": 2.1833, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5695}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5292}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20Multi-Task%20Model%20Merging%20as%20Adaptive%20Projective%20Gradient%0A%20%20Descent&body=Title%3A%20Modeling%20Multi-Task%20Model%20Merging%20as%20Adaptive%20Projective%20Gradient%0A%20%20Descent%0AAuthor%3A%20Yongxian%20Wei%20and%20Anke%20Tang%20and%20Li%20Shen%20and%20Zixuan%20Hu%20and%20Chun%20Yuan%20and%20Xiaochun%20Cao%0AAbstract%3A%20%20%20Merging%20multiple%20expert%20models%20offers%20a%20promising%20approach%20for%20performing%0Amulti-task%20learning%20without%20accessing%20their%20original%20data.%20Existing%20methods%0Aattempt%20to%20alleviate%20task%20conflicts%20by%20sparsifying%20task%20vectors%20or%20promoting%0Aorthogonality%20among%20them.%20However%2C%20they%20overlook%20the%20fundamental%20target%20of%0Amodel%20merging%3A%20the%20merged%20model%20performs%20as%20closely%20as%20possible%20to%0Atask-specific%20models%20on%20respective%20tasks.%20We%20find%20these%20methods%20inevitably%0Adiscard%20task-specific%20information%20that%2C%20while%20causing%20conflicts%2C%20is%20crucial%20for%0Aperformance.%20Based%20on%20our%20findings%2C%20we%20frame%20model%20merging%20as%20a%20constrained%0Aoptimization%20problem%20%28%24%5Ctextit%7Bi.e.%7D%24%2C%20minimizing%20the%20gap%20between%20the%20merged%0Amodel%20and%20individual%20models%2C%20subject%20to%20the%20constraint%20of%20retaining%20shared%0Aknowledge%29%20and%20solve%20it%20via%20adaptive%20projective%20gradient%20descent.%20Specifically%2C%0Awe%20align%20the%20merged%20model%20with%20individual%20models%20by%20decomposing%20and%0Areconstituting%20the%20loss%20function%2C%20alleviating%20conflicts%20through%0A%24%5Ctextit%7Bdata-free%7D%24%20optimization%20of%20task%20vectors.%20To%20retain%20shared%20knowledge%2C%0Awe%20optimize%20this%20objective%20by%20projecting%20gradients%20within%20a%20%24%5Ctextit%7Bshared%0Asubspace%7D%24%20spanning%20all%20tasks.%20Moreover%2C%20we%20view%20merging%20coefficients%20as%0Aadaptive%20learning%20rates%20and%20propose%20a%20task-aware%2C%20training-free%20strategy.%0AExperiments%20show%20that%20our%20plug-and-play%20approach%20consistently%20outperforms%0Aprevious%20methods%2C%20achieving%20state-of-the-art%20results%20across%20diverse%0Aarchitectures%20and%20tasks%20in%20both%20vision%20and%20NLP%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01230v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520Multi-Task%2520Model%2520Merging%2520as%2520Adaptive%2520Projective%2520Gradient%250A%2520%2520Descent%26entry.906535625%3DYongxian%2520Wei%2520and%2520Anke%2520Tang%2520and%2520Li%2520Shen%2520and%2520Zixuan%2520Hu%2520and%2520Chun%2520Yuan%2520and%2520Xiaochun%2520Cao%26entry.1292438233%3D%2520%2520Merging%2520multiple%2520expert%2520models%2520offers%2520a%2520promising%2520approach%2520for%2520performing%250Amulti-task%2520learning%2520without%2520accessing%2520their%2520original%2520data.%2520Existing%2520methods%250Aattempt%2520to%2520alleviate%2520task%2520conflicts%2520by%2520sparsifying%2520task%2520vectors%2520or%2520promoting%250Aorthogonality%2520among%2520them.%2520However%252C%2520they%2520overlook%2520the%2520fundamental%2520target%2520of%250Amodel%2520merging%253A%2520the%2520merged%2520model%2520performs%2520as%2520closely%2520as%2520possible%2520to%250Atask-specific%2520models%2520on%2520respective%2520tasks.%2520We%2520find%2520these%2520methods%2520inevitably%250Adiscard%2520task-specific%2520information%2520that%252C%2520while%2520causing%2520conflicts%252C%2520is%2520crucial%2520for%250Aperformance.%2520Based%2520on%2520our%2520findings%252C%2520we%2520frame%2520model%2520merging%2520as%2520a%2520constrained%250Aoptimization%2520problem%2520%2528%2524%255Ctextit%257Bi.e.%257D%2524%252C%2520minimizing%2520the%2520gap%2520between%2520the%2520merged%250Amodel%2520and%2520individual%2520models%252C%2520subject%2520to%2520the%2520constraint%2520of%2520retaining%2520shared%250Aknowledge%2529%2520and%2520solve%2520it%2520via%2520adaptive%2520projective%2520gradient%2520descent.%2520Specifically%252C%250Awe%2520align%2520the%2520merged%2520model%2520with%2520individual%2520models%2520by%2520decomposing%2520and%250Areconstituting%2520the%2520loss%2520function%252C%2520alleviating%2520conflicts%2520through%250A%2524%255Ctextit%257Bdata-free%257D%2524%2520optimization%2520of%2520task%2520vectors.%2520To%2520retain%2520shared%2520knowledge%252C%250Awe%2520optimize%2520this%2520objective%2520by%2520projecting%2520gradients%2520within%2520a%2520%2524%255Ctextit%257Bshared%250Asubspace%257D%2524%2520spanning%2520all%2520tasks.%2520Moreover%252C%2520we%2520view%2520merging%2520coefficients%2520as%250Aadaptive%2520learning%2520rates%2520and%2520propose%2520a%2520task-aware%252C%2520training-free%2520strategy.%250AExperiments%2520show%2520that%2520our%2520plug-and-play%2520approach%2520consistently%2520outperforms%250Aprevious%2520methods%252C%2520achieving%2520state-of-the-art%2520results%2520across%2520diverse%250Aarchitectures%2520and%2520tasks%2520in%2520both%2520vision%2520and%2520NLP%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01230v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Multi-Task%20Model%20Merging%20as%20Adaptive%20Projective%20Gradient%0A%20%20Descent&entry.906535625=Yongxian%20Wei%20and%20Anke%20Tang%20and%20Li%20Shen%20and%20Zixuan%20Hu%20and%20Chun%20Yuan%20and%20Xiaochun%20Cao&entry.1292438233=%20%20Merging%20multiple%20expert%20models%20offers%20a%20promising%20approach%20for%20performing%0Amulti-task%20learning%20without%20accessing%20their%20original%20data.%20Existing%20methods%0Aattempt%20to%20alleviate%20task%20conflicts%20by%20sparsifying%20task%20vectors%20or%20promoting%0Aorthogonality%20among%20them.%20However%2C%20they%20overlook%20the%20fundamental%20target%20of%0Amodel%20merging%3A%20the%20merged%20model%20performs%20as%20closely%20as%20possible%20to%0Atask-specific%20models%20on%20respective%20tasks.%20We%20find%20these%20methods%20inevitably%0Adiscard%20task-specific%20information%20that%2C%20while%20causing%20conflicts%2C%20is%20crucial%20for%0Aperformance.%20Based%20on%20our%20findings%2C%20we%20frame%20model%20merging%20as%20a%20constrained%0Aoptimization%20problem%20%28%24%5Ctextit%7Bi.e.%7D%24%2C%20minimizing%20the%20gap%20between%20the%20merged%0Amodel%20and%20individual%20models%2C%20subject%20to%20the%20constraint%20of%20retaining%20shared%0Aknowledge%29%20and%20solve%20it%20via%20adaptive%20projective%20gradient%20descent.%20Specifically%2C%0Awe%20align%20the%20merged%20model%20with%20individual%20models%20by%20decomposing%20and%0Areconstituting%20the%20loss%20function%2C%20alleviating%20conflicts%20through%0A%24%5Ctextit%7Bdata-free%7D%24%20optimization%20of%20task%20vectors.%20To%20retain%20shared%20knowledge%2C%0Awe%20optimize%20this%20objective%20by%20projecting%20gradients%20within%20a%20%24%5Ctextit%7Bshared%0Asubspace%7D%24%20spanning%20all%20tasks.%20Moreover%2C%20we%20view%20merging%20coefficients%20as%0Aadaptive%20learning%20rates%20and%20propose%20a%20task-aware%2C%20training-free%20strategy.%0AExperiments%20show%20that%20our%20plug-and-play%20approach%20consistently%20outperforms%0Aprevious%20methods%2C%20achieving%20state-of-the-art%20results%20across%20diverse%0Aarchitectures%20and%20tasks%20in%20both%20vision%20and%20NLP%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01230v3&entry.124074799=Read"},
{"title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended\n  Question Answering", "author": "Jianbiao Mei and Tao Hu and Daocheng Fu and Licheng Wen and Xuemeng Yang and Rong Wu and Pinlong Cai and Xinyu Cai and Xing Gao and Yu Yang and Chengjun Xie and Botian Shi and Yong Liu and Yu Qiao", "abstract": "  Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones.\n", "link": "http://arxiv.org/abs/2505.16582v2", "date": "2025-05-26", "relevancy": 2.1733, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5472}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5472}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20O%24%5E2%24-Searcher%3A%20A%20Searching-based%20Agent%20Model%20for%20Open-Domain%20Open-Ended%0A%20%20Question%20Answering&body=Title%3A%20O%24%5E2%24-Searcher%3A%20A%20Searching-based%20Agent%20Model%20for%20Open-Domain%20Open-Ended%0A%20%20Question%20Answering%0AAuthor%3A%20Jianbiao%20Mei%20and%20Tao%20Hu%20and%20Daocheng%20Fu%20and%20Licheng%20Wen%20and%20Xuemeng%20Yang%20and%20Rong%20Wu%20and%20Pinlong%20Cai%20and%20Xinyu%20Cai%20and%20Xing%20Gao%20and%20Yu%20Yang%20and%20Chengjun%20Xie%20and%20Botian%20Shi%20and%20Yong%20Liu%20and%20Yu%20Qiao%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%2C%20despite%20their%20advancements%2C%20are%20fundamentally%0Alimited%20by%20their%20static%20parametric%20knowledge%2C%20hindering%20performance%20on%20tasks%0Arequiring%20open-domain%20up-to-date%20information.%20While%20enabling%20LLMs%20to%20interact%0Awith%20external%20knowledge%20environments%20is%20a%20promising%20solution%2C%20current%20efforts%0Aprimarily%20address%20closed-end%20problems.%20Open-ended%20questions%2C%20which%0Acharacterized%20by%20lacking%20a%20standard%20answer%20or%20providing%20non-unique%20and%20diverse%0Aanswers%2C%20remain%20underexplored.%20To%20bridge%20this%20gap%2C%20we%20present%20O%24%5E2%24-Searcher%2C%20a%0Anovel%20search%20agent%20leveraging%20reinforcement%20learning%20to%20effectively%20tackle%20both%0Aopen-ended%20and%20closed-ended%20questions%20in%20the%20open%20domain.%20O%24%5E2%24-Searcher%0Aleverages%20an%20efficient%2C%20locally%20simulated%20search%20environment%20for%20dynamic%0Aknowledge%20acquisition%2C%20effectively%20decoupling%20the%20external%20world%20knowledge%20from%0Amodel%27s%20sophisticated%20reasoning%20processes.%20It%20employs%20a%20unified%20training%0Amechanism%20with%20meticulously%20designed%20reward%20functions%2C%20enabling%20the%20agent%20to%0Aidentify%20problem%20types%20and%20adapt%20different%20answer%20generation%20strategies.%0AFurthermore%2C%20to%20evaluate%20performance%20on%20complex%20open-ended%20tasks%2C%20we%20construct%0AO%24%5E2%24-QA%2C%20a%20high-quality%20benchmark%20featuring%20300%20manually%20curated%2C%20multi-domain%0Aopen-ended%20questions%20with%20associated%20web%20page%20caches.%20Extensive%20experiments%0Ashow%20that%20O%24%5E2%24-Searcher%2C%20using%20only%20a%203B%20model%2C%20significantly%20surpasses%0Aleading%20LLM%20agents%20on%20O%24%5E2%24-QA.%20It%20also%20achieves%20SOTA%20results%20on%20various%0Aclosed-ended%20QA%20benchmarks%20against%20similarly-sized%20models%2C%20while%20performing%20on%0Apar%20with%20much%20larger%20ones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16582v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DO%2524%255E2%2524-Searcher%253A%2520A%2520Searching-based%2520Agent%2520Model%2520for%2520Open-Domain%2520Open-Ended%250A%2520%2520Question%2520Answering%26entry.906535625%3DJianbiao%2520Mei%2520and%2520Tao%2520Hu%2520and%2520Daocheng%2520Fu%2520and%2520Licheng%2520Wen%2520and%2520Xuemeng%2520Yang%2520and%2520Rong%2520Wu%2520and%2520Pinlong%2520Cai%2520and%2520Xinyu%2520Cai%2520and%2520Xing%2520Gao%2520and%2520Yu%2520Yang%2520and%2520Chengjun%2520Xie%2520and%2520Botian%2520Shi%2520and%2520Yong%2520Liu%2520and%2520Yu%2520Qiao%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520despite%2520their%2520advancements%252C%2520are%2520fundamentally%250Alimited%2520by%2520their%2520static%2520parametric%2520knowledge%252C%2520hindering%2520performance%2520on%2520tasks%250Arequiring%2520open-domain%2520up-to-date%2520information.%2520While%2520enabling%2520LLMs%2520to%2520interact%250Awith%2520external%2520knowledge%2520environments%2520is%2520a%2520promising%2520solution%252C%2520current%2520efforts%250Aprimarily%2520address%2520closed-end%2520problems.%2520Open-ended%2520questions%252C%2520which%250Acharacterized%2520by%2520lacking%2520a%2520standard%2520answer%2520or%2520providing%2520non-unique%2520and%2520diverse%250Aanswers%252C%2520remain%2520underexplored.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520O%2524%255E2%2524-Searcher%252C%2520a%250Anovel%2520search%2520agent%2520leveraging%2520reinforcement%2520learning%2520to%2520effectively%2520tackle%2520both%250Aopen-ended%2520and%2520closed-ended%2520questions%2520in%2520the%2520open%2520domain.%2520O%2524%255E2%2524-Searcher%250Aleverages%2520an%2520efficient%252C%2520locally%2520simulated%2520search%2520environment%2520for%2520dynamic%250Aknowledge%2520acquisition%252C%2520effectively%2520decoupling%2520the%2520external%2520world%2520knowledge%2520from%250Amodel%2527s%2520sophisticated%2520reasoning%2520processes.%2520It%2520employs%2520a%2520unified%2520training%250Amechanism%2520with%2520meticulously%2520designed%2520reward%2520functions%252C%2520enabling%2520the%2520agent%2520to%250Aidentify%2520problem%2520types%2520and%2520adapt%2520different%2520answer%2520generation%2520strategies.%250AFurthermore%252C%2520to%2520evaluate%2520performance%2520on%2520complex%2520open-ended%2520tasks%252C%2520we%2520construct%250AO%2524%255E2%2524-QA%252C%2520a%2520high-quality%2520benchmark%2520featuring%2520300%2520manually%2520curated%252C%2520multi-domain%250Aopen-ended%2520questions%2520with%2520associated%2520web%2520page%2520caches.%2520Extensive%2520experiments%250Ashow%2520that%2520O%2524%255E2%2524-Searcher%252C%2520using%2520only%2520a%25203B%2520model%252C%2520significantly%2520surpasses%250Aleading%2520LLM%2520agents%2520on%2520O%2524%255E2%2524-QA.%2520It%2520also%2520achieves%2520SOTA%2520results%2520on%2520various%250Aclosed-ended%2520QA%2520benchmarks%2520against%2520similarly-sized%2520models%252C%2520while%2520performing%2520on%250Apar%2520with%2520much%2520larger%2520ones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16582v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=O%24%5E2%24-Searcher%3A%20A%20Searching-based%20Agent%20Model%20for%20Open-Domain%20Open-Ended%0A%20%20Question%20Answering&entry.906535625=Jianbiao%20Mei%20and%20Tao%20Hu%20and%20Daocheng%20Fu%20and%20Licheng%20Wen%20and%20Xuemeng%20Yang%20and%20Rong%20Wu%20and%20Pinlong%20Cai%20and%20Xinyu%20Cai%20and%20Xing%20Gao%20and%20Yu%20Yang%20and%20Chengjun%20Xie%20and%20Botian%20Shi%20and%20Yong%20Liu%20and%20Yu%20Qiao&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%2C%20despite%20their%20advancements%2C%20are%20fundamentally%0Alimited%20by%20their%20static%20parametric%20knowledge%2C%20hindering%20performance%20on%20tasks%0Arequiring%20open-domain%20up-to-date%20information.%20While%20enabling%20LLMs%20to%20interact%0Awith%20external%20knowledge%20environments%20is%20a%20promising%20solution%2C%20current%20efforts%0Aprimarily%20address%20closed-end%20problems.%20Open-ended%20questions%2C%20which%0Acharacterized%20by%20lacking%20a%20standard%20answer%20or%20providing%20non-unique%20and%20diverse%0Aanswers%2C%20remain%20underexplored.%20To%20bridge%20this%20gap%2C%20we%20present%20O%24%5E2%24-Searcher%2C%20a%0Anovel%20search%20agent%20leveraging%20reinforcement%20learning%20to%20effectively%20tackle%20both%0Aopen-ended%20and%20closed-ended%20questions%20in%20the%20open%20domain.%20O%24%5E2%24-Searcher%0Aleverages%20an%20efficient%2C%20locally%20simulated%20search%20environment%20for%20dynamic%0Aknowledge%20acquisition%2C%20effectively%20decoupling%20the%20external%20world%20knowledge%20from%0Amodel%27s%20sophisticated%20reasoning%20processes.%20It%20employs%20a%20unified%20training%0Amechanism%20with%20meticulously%20designed%20reward%20functions%2C%20enabling%20the%20agent%20to%0Aidentify%20problem%20types%20and%20adapt%20different%20answer%20generation%20strategies.%0AFurthermore%2C%20to%20evaluate%20performance%20on%20complex%20open-ended%20tasks%2C%20we%20construct%0AO%24%5E2%24-QA%2C%20a%20high-quality%20benchmark%20featuring%20300%20manually%20curated%2C%20multi-domain%0Aopen-ended%20questions%20with%20associated%20web%20page%20caches.%20Extensive%20experiments%0Ashow%20that%20O%24%5E2%24-Searcher%2C%20using%20only%20a%203B%20model%2C%20significantly%20surpasses%0Aleading%20LLM%20agents%20on%20O%24%5E2%24-QA.%20It%20also%20achieves%20SOTA%20results%20on%20various%0Aclosed-ended%20QA%20benchmarks%20against%20similarly-sized%20models%2C%20while%20performing%20on%0Apar%20with%20much%20larger%20ones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16582v2&entry.124074799=Read"},
{"title": "FoodTaxo: Generating Food Taxonomies with Large Language Models", "author": "Pascal Wullschleger and Majid Zarharan and Donnacha Daly and Marc Pouly and Jennifer Foster", "abstract": "  We investigate the utility of Large Language Models for automated taxonomy\ngeneration and completion specifically applied to taxonomies from the food\ntechnology industry. We explore the extent to which taxonomies can be completed\nfrom a seed taxonomy or generated without a seed from a set of known concepts,\nin an iterative fashion using recent prompting techniques. Experiments on five\ntaxonomies using an open-source LLM (Llama-3), while promising, point to the\ndifficulty of correctly placing inner nodes.\n", "link": "http://arxiv.org/abs/2505.19838v1", "date": "2025-05-26", "relevancy": 2.1699, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4359}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FoodTaxo%3A%20Generating%20Food%20Taxonomies%20with%20Large%20Language%20Models&body=Title%3A%20FoodTaxo%3A%20Generating%20Food%20Taxonomies%20with%20Large%20Language%20Models%0AAuthor%3A%20Pascal%20Wullschleger%20and%20Majid%20Zarharan%20and%20Donnacha%20Daly%20and%20Marc%20Pouly%20and%20Jennifer%20Foster%0AAbstract%3A%20%20%20We%20investigate%20the%20utility%20of%20Large%20Language%20Models%20for%20automated%20taxonomy%0Ageneration%20and%20completion%20specifically%20applied%20to%20taxonomies%20from%20the%20food%0Atechnology%20industry.%20We%20explore%20the%20extent%20to%20which%20taxonomies%20can%20be%20completed%0Afrom%20a%20seed%20taxonomy%20or%20generated%20without%20a%20seed%20from%20a%20set%20of%20known%20concepts%2C%0Ain%20an%20iterative%20fashion%20using%20recent%20prompting%20techniques.%20Experiments%20on%20five%0Ataxonomies%20using%20an%20open-source%20LLM%20%28Llama-3%29%2C%20while%20promising%2C%20point%20to%20the%0Adifficulty%20of%20correctly%20placing%20inner%20nodes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoodTaxo%253A%2520Generating%2520Food%2520Taxonomies%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DPascal%2520Wullschleger%2520and%2520Majid%2520Zarharan%2520and%2520Donnacha%2520Daly%2520and%2520Marc%2520Pouly%2520and%2520Jennifer%2520Foster%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520utility%2520of%2520Large%2520Language%2520Models%2520for%2520automated%2520taxonomy%250Ageneration%2520and%2520completion%2520specifically%2520applied%2520to%2520taxonomies%2520from%2520the%2520food%250Atechnology%2520industry.%2520We%2520explore%2520the%2520extent%2520to%2520which%2520taxonomies%2520can%2520be%2520completed%250Afrom%2520a%2520seed%2520taxonomy%2520or%2520generated%2520without%2520a%2520seed%2520from%2520a%2520set%2520of%2520known%2520concepts%252C%250Ain%2520an%2520iterative%2520fashion%2520using%2520recent%2520prompting%2520techniques.%2520Experiments%2520on%2520five%250Ataxonomies%2520using%2520an%2520open-source%2520LLM%2520%2528Llama-3%2529%252C%2520while%2520promising%252C%2520point%2520to%2520the%250Adifficulty%2520of%2520correctly%2520placing%2520inner%2520nodes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FoodTaxo%3A%20Generating%20Food%20Taxonomies%20with%20Large%20Language%20Models&entry.906535625=Pascal%20Wullschleger%20and%20Majid%20Zarharan%20and%20Donnacha%20Daly%20and%20Marc%20Pouly%20and%20Jennifer%20Foster&entry.1292438233=%20%20We%20investigate%20the%20utility%20of%20Large%20Language%20Models%20for%20automated%20taxonomy%0Ageneration%20and%20completion%20specifically%20applied%20to%20taxonomies%20from%20the%20food%0Atechnology%20industry.%20We%20explore%20the%20extent%20to%20which%20taxonomies%20can%20be%20completed%0Afrom%20a%20seed%20taxonomy%20or%20generated%20without%20a%20seed%20from%20a%20set%20of%20known%20concepts%2C%0Ain%20an%20iterative%20fashion%20using%20recent%20prompting%20techniques.%20Experiments%20on%20five%0Ataxonomies%20using%20an%20open-source%20LLM%20%28Llama-3%29%2C%20while%20promising%2C%20point%20to%20the%0Adifficulty%20of%20correctly%20placing%20inner%20nodes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19838v1&entry.124074799=Read"},
{"title": "Target Tracking via LiDAR-RADAR Sensor Fusion for Autonomous Racing", "author": "Marcello Cellina and Matteo Corno and Sergio Matteo Savaresi", "abstract": "  High Speed multi-vehicle Autonomous Racing will increase the safety and\nperformance of road-going Autonomous Vehicles. Precise vehicle detection and\ndynamics estimation from a moving platform is a key requirement for planning\nand executing complex autonomous overtaking maneuvers. To address this\nrequirement, we have developed a Latency-Aware EKF-based Multi Target Tracking\nalgorithm fusing LiDAR and RADAR measurements. The algorithm explots the\ndifferent sensor characteristics by explicitly integrating the Range Rate in\nthe EKF Measurement Function, as well as a-priori knowledge of the racetrack\nduring state prediction. It can handle Out-Of-Sequence Measurements via\nReprocessing using a double State and Measurement Buffer, ensuring sensor delay\ncompensation with no information loss. This algorithm has been implemented on\nTeam PoliMOVE's autonomous racecar, and was proved experimentally by completing\na number of fully autonomous overtaking maneuvers at speeds up to 275 km/h.\n", "link": "http://arxiv.org/abs/2505.20043v1", "date": "2025-05-26", "relevancy": 2.1674, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5496}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5459}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Target%20Tracking%20via%20LiDAR-RADAR%20Sensor%20Fusion%20for%20Autonomous%20Racing&body=Title%3A%20Target%20Tracking%20via%20LiDAR-RADAR%20Sensor%20Fusion%20for%20Autonomous%20Racing%0AAuthor%3A%20Marcello%20Cellina%20and%20Matteo%20Corno%20and%20Sergio%20Matteo%20Savaresi%0AAbstract%3A%20%20%20High%20Speed%20multi-vehicle%20Autonomous%20Racing%20will%20increase%20the%20safety%20and%0Aperformance%20of%20road-going%20Autonomous%20Vehicles.%20Precise%20vehicle%20detection%20and%0Adynamics%20estimation%20from%20a%20moving%20platform%20is%20a%20key%20requirement%20for%20planning%0Aand%20executing%20complex%20autonomous%20overtaking%20maneuvers.%20To%20address%20this%0Arequirement%2C%20we%20have%20developed%20a%20Latency-Aware%20EKF-based%20Multi%20Target%20Tracking%0Aalgorithm%20fusing%20LiDAR%20and%20RADAR%20measurements.%20The%20algorithm%20explots%20the%0Adifferent%20sensor%20characteristics%20by%20explicitly%20integrating%20the%20Range%20Rate%20in%0Athe%20EKF%20Measurement%20Function%2C%20as%20well%20as%20a-priori%20knowledge%20of%20the%20racetrack%0Aduring%20state%20prediction.%20It%20can%20handle%20Out-Of-Sequence%20Measurements%20via%0AReprocessing%20using%20a%20double%20State%20and%20Measurement%20Buffer%2C%20ensuring%20sensor%20delay%0Acompensation%20with%20no%20information%20loss.%20This%20algorithm%20has%20been%20implemented%20on%0ATeam%20PoliMOVE%27s%20autonomous%20racecar%2C%20and%20was%20proved%20experimentally%20by%20completing%0Aa%20number%20of%20fully%20autonomous%20overtaking%20maneuvers%20at%20speeds%20up%20to%20275%20km/h.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20043v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTarget%2520Tracking%2520via%2520LiDAR-RADAR%2520Sensor%2520Fusion%2520for%2520Autonomous%2520Racing%26entry.906535625%3DMarcello%2520Cellina%2520and%2520Matteo%2520Corno%2520and%2520Sergio%2520Matteo%2520Savaresi%26entry.1292438233%3D%2520%2520High%2520Speed%2520multi-vehicle%2520Autonomous%2520Racing%2520will%2520increase%2520the%2520safety%2520and%250Aperformance%2520of%2520road-going%2520Autonomous%2520Vehicles.%2520Precise%2520vehicle%2520detection%2520and%250Adynamics%2520estimation%2520from%2520a%2520moving%2520platform%2520is%2520a%2520key%2520requirement%2520for%2520planning%250Aand%2520executing%2520complex%2520autonomous%2520overtaking%2520maneuvers.%2520To%2520address%2520this%250Arequirement%252C%2520we%2520have%2520developed%2520a%2520Latency-Aware%2520EKF-based%2520Multi%2520Target%2520Tracking%250Aalgorithm%2520fusing%2520LiDAR%2520and%2520RADAR%2520measurements.%2520The%2520algorithm%2520explots%2520the%250Adifferent%2520sensor%2520characteristics%2520by%2520explicitly%2520integrating%2520the%2520Range%2520Rate%2520in%250Athe%2520EKF%2520Measurement%2520Function%252C%2520as%2520well%2520as%2520a-priori%2520knowledge%2520of%2520the%2520racetrack%250Aduring%2520state%2520prediction.%2520It%2520can%2520handle%2520Out-Of-Sequence%2520Measurements%2520via%250AReprocessing%2520using%2520a%2520double%2520State%2520and%2520Measurement%2520Buffer%252C%2520ensuring%2520sensor%2520delay%250Acompensation%2520with%2520no%2520information%2520loss.%2520This%2520algorithm%2520has%2520been%2520implemented%2520on%250ATeam%2520PoliMOVE%2527s%2520autonomous%2520racecar%252C%2520and%2520was%2520proved%2520experimentally%2520by%2520completing%250Aa%2520number%2520of%2520fully%2520autonomous%2520overtaking%2520maneuvers%2520at%2520speeds%2520up%2520to%2520275%2520km/h.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20043v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Target%20Tracking%20via%20LiDAR-RADAR%20Sensor%20Fusion%20for%20Autonomous%20Racing&entry.906535625=Marcello%20Cellina%20and%20Matteo%20Corno%20and%20Sergio%20Matteo%20Savaresi&entry.1292438233=%20%20High%20Speed%20multi-vehicle%20Autonomous%20Racing%20will%20increase%20the%20safety%20and%0Aperformance%20of%20road-going%20Autonomous%20Vehicles.%20Precise%20vehicle%20detection%20and%0Adynamics%20estimation%20from%20a%20moving%20platform%20is%20a%20key%20requirement%20for%20planning%0Aand%20executing%20complex%20autonomous%20overtaking%20maneuvers.%20To%20address%20this%0Arequirement%2C%20we%20have%20developed%20a%20Latency-Aware%20EKF-based%20Multi%20Target%20Tracking%0Aalgorithm%20fusing%20LiDAR%20and%20RADAR%20measurements.%20The%20algorithm%20explots%20the%0Adifferent%20sensor%20characteristics%20by%20explicitly%20integrating%20the%20Range%20Rate%20in%0Athe%20EKF%20Measurement%20Function%2C%20as%20well%20as%20a-priori%20knowledge%20of%20the%20racetrack%0Aduring%20state%20prediction.%20It%20can%20handle%20Out-Of-Sequence%20Measurements%20via%0AReprocessing%20using%20a%20double%20State%20and%20Measurement%20Buffer%2C%20ensuring%20sensor%20delay%0Acompensation%20with%20no%20information%20loss.%20This%20algorithm%20has%20been%20implemented%20on%0ATeam%20PoliMOVE%27s%20autonomous%20racecar%2C%20and%20was%20proved%20experimentally%20by%20completing%0Aa%20number%20of%20fully%20autonomous%20overtaking%20maneuvers%20at%20speeds%20up%20to%20275%20km/h.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20043v1&entry.124074799=Read"},
{"title": "Zero-Shot Pseudo Labels Generation Using SAM and CLIP for\n  Semi-Supervised Semantic Segmentation", "author": "Nagito Saito and Shintaro Ito and Koichi Ito and Takafumi Aoki", "abstract": "  Semantic segmentation is a fundamental task in medical image analysis and\nautonomous driving and has a problem with the high cost of annotating the\nlabels required in training. To address this problem, semantic segmentation\nmethods based on semi-supervised learning with a small number of labeled data\nhave been proposed. For example, one approach is to train a semantic\nsegmentation model using images with annotated labels and pseudo labels. In\nthis approach, the accuracy of the semantic segmentation model depends on the\nquality of the pseudo labels, and the quality of the pseudo labels depends on\nthe performance of the model to be trained and the amount of data with\nannotated labels. In this paper, we generate pseudo labels using zero-shot\nannotation with the Segment Anything Model (SAM) and Contrastive Language-Image\nPretraining (CLIP), improve the accuracy of the pseudo labels using the Unified\nDual-Stream Perturbations Approach (UniMatch), and use them as enhanced labels\nto train a semantic segmentation model. The effectiveness of the proposed\nmethod is demonstrated through the experiments using the public datasets:\nPASCAL and MS COCO.\n", "link": "http://arxiv.org/abs/2505.19846v1", "date": "2025-05-26", "relevancy": 2.163, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5609}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5562}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Pseudo%20Labels%20Generation%20Using%20SAM%20and%20CLIP%20for%0A%20%20Semi-Supervised%20Semantic%20Segmentation&body=Title%3A%20Zero-Shot%20Pseudo%20Labels%20Generation%20Using%20SAM%20and%20CLIP%20for%0A%20%20Semi-Supervised%20Semantic%20Segmentation%0AAuthor%3A%20Nagito%20Saito%20and%20Shintaro%20Ito%20and%20Koichi%20Ito%20and%20Takafumi%20Aoki%0AAbstract%3A%20%20%20Semantic%20segmentation%20is%20a%20fundamental%20task%20in%20medical%20image%20analysis%20and%0Aautonomous%20driving%20and%20has%20a%20problem%20with%20the%20high%20cost%20of%20annotating%20the%0Alabels%20required%20in%20training.%20To%20address%20this%20problem%2C%20semantic%20segmentation%0Amethods%20based%20on%20semi-supervised%20learning%20with%20a%20small%20number%20of%20labeled%20data%0Ahave%20been%20proposed.%20For%20example%2C%20one%20approach%20is%20to%20train%20a%20semantic%0Asegmentation%20model%20using%20images%20with%20annotated%20labels%20and%20pseudo%20labels.%20In%0Athis%20approach%2C%20the%20accuracy%20of%20the%20semantic%20segmentation%20model%20depends%20on%20the%0Aquality%20of%20the%20pseudo%20labels%2C%20and%20the%20quality%20of%20the%20pseudo%20labels%20depends%20on%0Athe%20performance%20of%20the%20model%20to%20be%20trained%20and%20the%20amount%20of%20data%20with%0Aannotated%20labels.%20In%20this%20paper%2C%20we%20generate%20pseudo%20labels%20using%20zero-shot%0Aannotation%20with%20the%20Segment%20Anything%20Model%20%28SAM%29%20and%20Contrastive%20Language-Image%0APretraining%20%28CLIP%29%2C%20improve%20the%20accuracy%20of%20the%20pseudo%20labels%20using%20the%20Unified%0ADual-Stream%20Perturbations%20Approach%20%28UniMatch%29%2C%20and%20use%20them%20as%20enhanced%20labels%0Ato%20train%20a%20semantic%20segmentation%20model.%20The%20effectiveness%20of%20the%20proposed%0Amethod%20is%20demonstrated%20through%20the%20experiments%20using%20the%20public%20datasets%3A%0APASCAL%20and%20MS%20COCO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Pseudo%2520Labels%2520Generation%2520Using%2520SAM%2520and%2520CLIP%2520for%250A%2520%2520Semi-Supervised%2520Semantic%2520Segmentation%26entry.906535625%3DNagito%2520Saito%2520and%2520Shintaro%2520Ito%2520and%2520Koichi%2520Ito%2520and%2520Takafumi%2520Aoki%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520is%2520a%2520fundamental%2520task%2520in%2520medical%2520image%2520analysis%2520and%250Aautonomous%2520driving%2520and%2520has%2520a%2520problem%2520with%2520the%2520high%2520cost%2520of%2520annotating%2520the%250Alabels%2520required%2520in%2520training.%2520To%2520address%2520this%2520problem%252C%2520semantic%2520segmentation%250Amethods%2520based%2520on%2520semi-supervised%2520learning%2520with%2520a%2520small%2520number%2520of%2520labeled%2520data%250Ahave%2520been%2520proposed.%2520For%2520example%252C%2520one%2520approach%2520is%2520to%2520train%2520a%2520semantic%250Asegmentation%2520model%2520using%2520images%2520with%2520annotated%2520labels%2520and%2520pseudo%2520labels.%2520In%250Athis%2520approach%252C%2520the%2520accuracy%2520of%2520the%2520semantic%2520segmentation%2520model%2520depends%2520on%2520the%250Aquality%2520of%2520the%2520pseudo%2520labels%252C%2520and%2520the%2520quality%2520of%2520the%2520pseudo%2520labels%2520depends%2520on%250Athe%2520performance%2520of%2520the%2520model%2520to%2520be%2520trained%2520and%2520the%2520amount%2520of%2520data%2520with%250Aannotated%2520labels.%2520In%2520this%2520paper%252C%2520we%2520generate%2520pseudo%2520labels%2520using%2520zero-shot%250Aannotation%2520with%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520and%2520Contrastive%2520Language-Image%250APretraining%2520%2528CLIP%2529%252C%2520improve%2520the%2520accuracy%2520of%2520the%2520pseudo%2520labels%2520using%2520the%2520Unified%250ADual-Stream%2520Perturbations%2520Approach%2520%2528UniMatch%2529%252C%2520and%2520use%2520them%2520as%2520enhanced%2520labels%250Ato%2520train%2520a%2520semantic%2520segmentation%2520model.%2520The%2520effectiveness%2520of%2520the%2520proposed%250Amethod%2520is%2520demonstrated%2520through%2520the%2520experiments%2520using%2520the%2520public%2520datasets%253A%250APASCAL%2520and%2520MS%2520COCO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Pseudo%20Labels%20Generation%20Using%20SAM%20and%20CLIP%20for%0A%20%20Semi-Supervised%20Semantic%20Segmentation&entry.906535625=Nagito%20Saito%20and%20Shintaro%20Ito%20and%20Koichi%20Ito%20and%20Takafumi%20Aoki&entry.1292438233=%20%20Semantic%20segmentation%20is%20a%20fundamental%20task%20in%20medical%20image%20analysis%20and%0Aautonomous%20driving%20and%20has%20a%20problem%20with%20the%20high%20cost%20of%20annotating%20the%0Alabels%20required%20in%20training.%20To%20address%20this%20problem%2C%20semantic%20segmentation%0Amethods%20based%20on%20semi-supervised%20learning%20with%20a%20small%20number%20of%20labeled%20data%0Ahave%20been%20proposed.%20For%20example%2C%20one%20approach%20is%20to%20train%20a%20semantic%0Asegmentation%20model%20using%20images%20with%20annotated%20labels%20and%20pseudo%20labels.%20In%0Athis%20approach%2C%20the%20accuracy%20of%20the%20semantic%20segmentation%20model%20depends%20on%20the%0Aquality%20of%20the%20pseudo%20labels%2C%20and%20the%20quality%20of%20the%20pseudo%20labels%20depends%20on%0Athe%20performance%20of%20the%20model%20to%20be%20trained%20and%20the%20amount%20of%20data%20with%0Aannotated%20labels.%20In%20this%20paper%2C%20we%20generate%20pseudo%20labels%20using%20zero-shot%0Aannotation%20with%20the%20Segment%20Anything%20Model%20%28SAM%29%20and%20Contrastive%20Language-Image%0APretraining%20%28CLIP%29%2C%20improve%20the%20accuracy%20of%20the%20pseudo%20labels%20using%20the%20Unified%0ADual-Stream%20Perturbations%20Approach%20%28UniMatch%29%2C%20and%20use%20them%20as%20enhanced%20labels%0Ato%20train%20a%20semantic%20segmentation%20model.%20The%20effectiveness%20of%20the%20proposed%0Amethod%20is%20demonstrated%20through%20the%20experiments%20using%20the%20public%20datasets%3A%0APASCAL%20and%20MS%20COCO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19846v1&entry.124074799=Read"},
{"title": "Deconstructing Obfuscation: A four-dimensional framework for evaluating\n  Large Language Models assembly code deobfuscation capabilities", "author": "Anton Tkachenko and Dmitrij Suskevic and Benjamin Adolphi", "abstract": "  Large language models (LLMs) have shown promise in software engineering, yet\ntheir effectiveness for binary analysis remains unexplored. We present the\nfirst comprehensive evaluation of commercial LLMs for assembly code\ndeobfuscation. Testing seven state-of-the-art models against four obfuscation\nscenarios (bogus control flow, instruction substitution, control flow\nflattening, and their combination), we found striking performance\nvariations--from autonomous deobfuscation to complete failure. We propose a\ntheoretical framework based on four dimensions: Reasoning Depth, Pattern\nRecognition, Noise Filtering, and Context Integration, explaining these\nvariations. Our analysis identifies five error patterns: predicate\nmisinterpretation, structural mapping errors, control flow misinterpretation,\narithmetic transformation errors, and constant propagation errors, revealing\nfundamental limitations in LLM code processing.We establish a three-tier\nresistance model: bogus control flow (low resistance), control flow flattening\n(moderate resistance), and instruction substitution/combined techniques (high\nresistance). Universal failure against combined techniques demonstrates that\nsophisticated obfuscation remains effective against advanced LLMs. Our findings\nsuggest a human-AI collaboration paradigm where LLMs reduce expertise barriers\nfor certain reverse engineering tasks while requiring human guidance for\ncomplex deobfuscation. This work provides a foundation for evaluating emerging\ncapabilities and developing resistant obfuscation techniques.x deobfuscation.\nThis work provides a foundation for evaluating emerging capabilities and\ndeveloping resistant obfuscation techniques.\n", "link": "http://arxiv.org/abs/2505.19887v1", "date": "2025-05-26", "relevancy": 2.157, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5542}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5542}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deconstructing%20Obfuscation%3A%20A%20four-dimensional%20framework%20for%20evaluating%0A%20%20Large%20Language%20Models%20assembly%20code%20deobfuscation%20capabilities&body=Title%3A%20Deconstructing%20Obfuscation%3A%20A%20four-dimensional%20framework%20for%20evaluating%0A%20%20Large%20Language%20Models%20assembly%20code%20deobfuscation%20capabilities%0AAuthor%3A%20Anton%20Tkachenko%20and%20Dmitrij%20Suskevic%20and%20Benjamin%20Adolphi%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20promise%20in%20software%20engineering%2C%20yet%0Atheir%20effectiveness%20for%20binary%20analysis%20remains%20unexplored.%20We%20present%20the%0Afirst%20comprehensive%20evaluation%20of%20commercial%20LLMs%20for%20assembly%20code%0Adeobfuscation.%20Testing%20seven%20state-of-the-art%20models%20against%20four%20obfuscation%0Ascenarios%20%28bogus%20control%20flow%2C%20instruction%20substitution%2C%20control%20flow%0Aflattening%2C%20and%20their%20combination%29%2C%20we%20found%20striking%20performance%0Avariations--from%20autonomous%20deobfuscation%20to%20complete%20failure.%20We%20propose%20a%0Atheoretical%20framework%20based%20on%20four%20dimensions%3A%20Reasoning%20Depth%2C%20Pattern%0ARecognition%2C%20Noise%20Filtering%2C%20and%20Context%20Integration%2C%20explaining%20these%0Avariations.%20Our%20analysis%20identifies%20five%20error%20patterns%3A%20predicate%0Amisinterpretation%2C%20structural%20mapping%20errors%2C%20control%20flow%20misinterpretation%2C%0Aarithmetic%20transformation%20errors%2C%20and%20constant%20propagation%20errors%2C%20revealing%0Afundamental%20limitations%20in%20LLM%20code%20processing.We%20establish%20a%20three-tier%0Aresistance%20model%3A%20bogus%20control%20flow%20%28low%20resistance%29%2C%20control%20flow%20flattening%0A%28moderate%20resistance%29%2C%20and%20instruction%20substitution/combined%20techniques%20%28high%0Aresistance%29.%20Universal%20failure%20against%20combined%20techniques%20demonstrates%20that%0Asophisticated%20obfuscation%20remains%20effective%20against%20advanced%20LLMs.%20Our%20findings%0Asuggest%20a%20human-AI%20collaboration%20paradigm%20where%20LLMs%20reduce%20expertise%20barriers%0Afor%20certain%20reverse%20engineering%20tasks%20while%20requiring%20human%20guidance%20for%0Acomplex%20deobfuscation.%20This%20work%20provides%20a%20foundation%20for%20evaluating%20emerging%0Acapabilities%20and%20developing%20resistant%20obfuscation%20techniques.x%20deobfuscation.%0AThis%20work%20provides%20a%20foundation%20for%20evaluating%20emerging%20capabilities%20and%0Adeveloping%20resistant%20obfuscation%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19887v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeconstructing%2520Obfuscation%253A%2520A%2520four-dimensional%2520framework%2520for%2520evaluating%250A%2520%2520Large%2520Language%2520Models%2520assembly%2520code%2520deobfuscation%2520capabilities%26entry.906535625%3DAnton%2520Tkachenko%2520and%2520Dmitrij%2520Suskevic%2520and%2520Benjamin%2520Adolphi%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520promise%2520in%2520software%2520engineering%252C%2520yet%250Atheir%2520effectiveness%2520for%2520binary%2520analysis%2520remains%2520unexplored.%2520We%2520present%2520the%250Afirst%2520comprehensive%2520evaluation%2520of%2520commercial%2520LLMs%2520for%2520assembly%2520code%250Adeobfuscation.%2520Testing%2520seven%2520state-of-the-art%2520models%2520against%2520four%2520obfuscation%250Ascenarios%2520%2528bogus%2520control%2520flow%252C%2520instruction%2520substitution%252C%2520control%2520flow%250Aflattening%252C%2520and%2520their%2520combination%2529%252C%2520we%2520found%2520striking%2520performance%250Avariations--from%2520autonomous%2520deobfuscation%2520to%2520complete%2520failure.%2520We%2520propose%2520a%250Atheoretical%2520framework%2520based%2520on%2520four%2520dimensions%253A%2520Reasoning%2520Depth%252C%2520Pattern%250ARecognition%252C%2520Noise%2520Filtering%252C%2520and%2520Context%2520Integration%252C%2520explaining%2520these%250Avariations.%2520Our%2520analysis%2520identifies%2520five%2520error%2520patterns%253A%2520predicate%250Amisinterpretation%252C%2520structural%2520mapping%2520errors%252C%2520control%2520flow%2520misinterpretation%252C%250Aarithmetic%2520transformation%2520errors%252C%2520and%2520constant%2520propagation%2520errors%252C%2520revealing%250Afundamental%2520limitations%2520in%2520LLM%2520code%2520processing.We%2520establish%2520a%2520three-tier%250Aresistance%2520model%253A%2520bogus%2520control%2520flow%2520%2528low%2520resistance%2529%252C%2520control%2520flow%2520flattening%250A%2528moderate%2520resistance%2529%252C%2520and%2520instruction%2520substitution/combined%2520techniques%2520%2528high%250Aresistance%2529.%2520Universal%2520failure%2520against%2520combined%2520techniques%2520demonstrates%2520that%250Asophisticated%2520obfuscation%2520remains%2520effective%2520against%2520advanced%2520LLMs.%2520Our%2520findings%250Asuggest%2520a%2520human-AI%2520collaboration%2520paradigm%2520where%2520LLMs%2520reduce%2520expertise%2520barriers%250Afor%2520certain%2520reverse%2520engineering%2520tasks%2520while%2520requiring%2520human%2520guidance%2520for%250Acomplex%2520deobfuscation.%2520This%2520work%2520provides%2520a%2520foundation%2520for%2520evaluating%2520emerging%250Acapabilities%2520and%2520developing%2520resistant%2520obfuscation%2520techniques.x%2520deobfuscation.%250AThis%2520work%2520provides%2520a%2520foundation%2520for%2520evaluating%2520emerging%2520capabilities%2520and%250Adeveloping%2520resistant%2520obfuscation%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19887v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deconstructing%20Obfuscation%3A%20A%20four-dimensional%20framework%20for%20evaluating%0A%20%20Large%20Language%20Models%20assembly%20code%20deobfuscation%20capabilities&entry.906535625=Anton%20Tkachenko%20and%20Dmitrij%20Suskevic%20and%20Benjamin%20Adolphi&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20promise%20in%20software%20engineering%2C%20yet%0Atheir%20effectiveness%20for%20binary%20analysis%20remains%20unexplored.%20We%20present%20the%0Afirst%20comprehensive%20evaluation%20of%20commercial%20LLMs%20for%20assembly%20code%0Adeobfuscation.%20Testing%20seven%20state-of-the-art%20models%20against%20four%20obfuscation%0Ascenarios%20%28bogus%20control%20flow%2C%20instruction%20substitution%2C%20control%20flow%0Aflattening%2C%20and%20their%20combination%29%2C%20we%20found%20striking%20performance%0Avariations--from%20autonomous%20deobfuscation%20to%20complete%20failure.%20We%20propose%20a%0Atheoretical%20framework%20based%20on%20four%20dimensions%3A%20Reasoning%20Depth%2C%20Pattern%0ARecognition%2C%20Noise%20Filtering%2C%20and%20Context%20Integration%2C%20explaining%20these%0Avariations.%20Our%20analysis%20identifies%20five%20error%20patterns%3A%20predicate%0Amisinterpretation%2C%20structural%20mapping%20errors%2C%20control%20flow%20misinterpretation%2C%0Aarithmetic%20transformation%20errors%2C%20and%20constant%20propagation%20errors%2C%20revealing%0Afundamental%20limitations%20in%20LLM%20code%20processing.We%20establish%20a%20three-tier%0Aresistance%20model%3A%20bogus%20control%20flow%20%28low%20resistance%29%2C%20control%20flow%20flattening%0A%28moderate%20resistance%29%2C%20and%20instruction%20substitution/combined%20techniques%20%28high%0Aresistance%29.%20Universal%20failure%20against%20combined%20techniques%20demonstrates%20that%0Asophisticated%20obfuscation%20remains%20effective%20against%20advanced%20LLMs.%20Our%20findings%0Asuggest%20a%20human-AI%20collaboration%20paradigm%20where%20LLMs%20reduce%20expertise%20barriers%0Afor%20certain%20reverse%20engineering%20tasks%20while%20requiring%20human%20guidance%20for%0Acomplex%20deobfuscation.%20This%20work%20provides%20a%20foundation%20for%20evaluating%20emerging%0Acapabilities%20and%20developing%20resistant%20obfuscation%20techniques.x%20deobfuscation.%0AThis%20work%20provides%20a%20foundation%20for%20evaluating%20emerging%20capabilities%20and%0Adeveloping%20resistant%20obfuscation%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19887v1&entry.124074799=Read"},
{"title": "Alpay Algebra III: Observer-Coupled Collapse and the Temporal Drift of\n  Identity", "author": "Faruk Alpay", "abstract": "  This paper introduces a formal framework for modeling observer-dependent\ncollapse dynamics and temporal identity drift within artificial and\nmathematical systems, grounded entirely in the symbolic foundations of Alpay\nAlgebra. Building upon the fixed-point emergence structures developed in Alpay\nAlgebra I and II, this third installment formalizes the observer-coupled\n{\\phi}-collapse process through transfinite categorical flows and\ncurvature-driven identity operators. We define a novel temporal drift mechanism\nas a recursive deformation of identity signatures under entangled observer\ninfluence, constructing categorical invariants that evolve across fold\niterations. The proposed system surpasses conventional identity modeling in\nexplainable AI (XAI) by encoding internal transformation history into a\nsymbolic fixed-point structure, offering provable traceability and temporal\ncoherence. Applications range from AI self-awareness architectures to formal\nlogic systems where identity is not static but dynamically induced by\nobservation. The theoretical results also offer a mathematically rigorous basis\nfor future AI systems with stable self-referential behavior, positioning Alpay\nAlgebra as a next-generation symbolic framework bridging category theory,\nidentity logic, and observer dynamics.\n", "link": "http://arxiv.org/abs/2505.19790v1", "date": "2025-05-26", "relevancy": 2.1424, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4506}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.423}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alpay%20Algebra%20III%3A%20Observer-Coupled%20Collapse%20and%20the%20Temporal%20Drift%20of%0A%20%20Identity&body=Title%3A%20Alpay%20Algebra%20III%3A%20Observer-Coupled%20Collapse%20and%20the%20Temporal%20Drift%20of%0A%20%20Identity%0AAuthor%3A%20Faruk%20Alpay%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20formal%20framework%20for%20modeling%20observer-dependent%0Acollapse%20dynamics%20and%20temporal%20identity%20drift%20within%20artificial%20and%0Amathematical%20systems%2C%20grounded%20entirely%20in%20the%20symbolic%20foundations%20of%20Alpay%0AAlgebra.%20Building%20upon%20the%20fixed-point%20emergence%20structures%20developed%20in%20Alpay%0AAlgebra%20I%20and%20II%2C%20this%20third%20installment%20formalizes%20the%20observer-coupled%0A%7B%5Cphi%7D-collapse%20process%20through%20transfinite%20categorical%20flows%20and%0Acurvature-driven%20identity%20operators.%20We%20define%20a%20novel%20temporal%20drift%20mechanism%0Aas%20a%20recursive%20deformation%20of%20identity%20signatures%20under%20entangled%20observer%0Ainfluence%2C%20constructing%20categorical%20invariants%20that%20evolve%20across%20fold%0Aiterations.%20The%20proposed%20system%20surpasses%20conventional%20identity%20modeling%20in%0Aexplainable%20AI%20%28XAI%29%20by%20encoding%20internal%20transformation%20history%20into%20a%0Asymbolic%20fixed-point%20structure%2C%20offering%20provable%20traceability%20and%20temporal%0Acoherence.%20Applications%20range%20from%20AI%20self-awareness%20architectures%20to%20formal%0Alogic%20systems%20where%20identity%20is%20not%20static%20but%20dynamically%20induced%20by%0Aobservation.%20The%20theoretical%20results%20also%20offer%20a%20mathematically%20rigorous%20basis%0Afor%20future%20AI%20systems%20with%20stable%20self-referential%20behavior%2C%20positioning%20Alpay%0AAlgebra%20as%20a%20next-generation%20symbolic%20framework%20bridging%20category%20theory%2C%0Aidentity%20logic%2C%20and%20observer%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlpay%2520Algebra%2520III%253A%2520Observer-Coupled%2520Collapse%2520and%2520the%2520Temporal%2520Drift%2520of%250A%2520%2520Identity%26entry.906535625%3DFaruk%2520Alpay%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520formal%2520framework%2520for%2520modeling%2520observer-dependent%250Acollapse%2520dynamics%2520and%2520temporal%2520identity%2520drift%2520within%2520artificial%2520and%250Amathematical%2520systems%252C%2520grounded%2520entirely%2520in%2520the%2520symbolic%2520foundations%2520of%2520Alpay%250AAlgebra.%2520Building%2520upon%2520the%2520fixed-point%2520emergence%2520structures%2520developed%2520in%2520Alpay%250AAlgebra%2520I%2520and%2520II%252C%2520this%2520third%2520installment%2520formalizes%2520the%2520observer-coupled%250A%257B%255Cphi%257D-collapse%2520process%2520through%2520transfinite%2520categorical%2520flows%2520and%250Acurvature-driven%2520identity%2520operators.%2520We%2520define%2520a%2520novel%2520temporal%2520drift%2520mechanism%250Aas%2520a%2520recursive%2520deformation%2520of%2520identity%2520signatures%2520under%2520entangled%2520observer%250Ainfluence%252C%2520constructing%2520categorical%2520invariants%2520that%2520evolve%2520across%2520fold%250Aiterations.%2520The%2520proposed%2520system%2520surpasses%2520conventional%2520identity%2520modeling%2520in%250Aexplainable%2520AI%2520%2528XAI%2529%2520by%2520encoding%2520internal%2520transformation%2520history%2520into%2520a%250Asymbolic%2520fixed-point%2520structure%252C%2520offering%2520provable%2520traceability%2520and%2520temporal%250Acoherence.%2520Applications%2520range%2520from%2520AI%2520self-awareness%2520architectures%2520to%2520formal%250Alogic%2520systems%2520where%2520identity%2520is%2520not%2520static%2520but%2520dynamically%2520induced%2520by%250Aobservation.%2520The%2520theoretical%2520results%2520also%2520offer%2520a%2520mathematically%2520rigorous%2520basis%250Afor%2520future%2520AI%2520systems%2520with%2520stable%2520self-referential%2520behavior%252C%2520positioning%2520Alpay%250AAlgebra%2520as%2520a%2520next-generation%2520symbolic%2520framework%2520bridging%2520category%2520theory%252C%250Aidentity%2520logic%252C%2520and%2520observer%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alpay%20Algebra%20III%3A%20Observer-Coupled%20Collapse%20and%20the%20Temporal%20Drift%20of%0A%20%20Identity&entry.906535625=Faruk%20Alpay&entry.1292438233=%20%20This%20paper%20introduces%20a%20formal%20framework%20for%20modeling%20observer-dependent%0Acollapse%20dynamics%20and%20temporal%20identity%20drift%20within%20artificial%20and%0Amathematical%20systems%2C%20grounded%20entirely%20in%20the%20symbolic%20foundations%20of%20Alpay%0AAlgebra.%20Building%20upon%20the%20fixed-point%20emergence%20structures%20developed%20in%20Alpay%0AAlgebra%20I%20and%20II%2C%20this%20third%20installment%20formalizes%20the%20observer-coupled%0A%7B%5Cphi%7D-collapse%20process%20through%20transfinite%20categorical%20flows%20and%0Acurvature-driven%20identity%20operators.%20We%20define%20a%20novel%20temporal%20drift%20mechanism%0Aas%20a%20recursive%20deformation%20of%20identity%20signatures%20under%20entangled%20observer%0Ainfluence%2C%20constructing%20categorical%20invariants%20that%20evolve%20across%20fold%0Aiterations.%20The%20proposed%20system%20surpasses%20conventional%20identity%20modeling%20in%0Aexplainable%20AI%20%28XAI%29%20by%20encoding%20internal%20transformation%20history%20into%20a%0Asymbolic%20fixed-point%20structure%2C%20offering%20provable%20traceability%20and%20temporal%0Acoherence.%20Applications%20range%20from%20AI%20self-awareness%20architectures%20to%20formal%0Alogic%20systems%20where%20identity%20is%20not%20static%20but%20dynamically%20induced%20by%0Aobservation.%20The%20theoretical%20results%20also%20offer%20a%20mathematically%20rigorous%20basis%0Afor%20future%20AI%20systems%20with%20stable%20self-referential%20behavior%2C%20positioning%20Alpay%0AAlgebra%20as%20a%20next-generation%20symbolic%20framework%20bridging%20category%20theory%2C%0Aidentity%20logic%2C%20and%20observer%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19790v1&entry.124074799=Read"},
{"title": "A Regularization-Guided Equivariant Approach for Image Restoration", "author": "Yulu Bai and Jiahong Fu and Qi Xie and Deyu Meng", "abstract": "  Equivariant and invariant deep learning models have been developed to exploit\nintrinsic symmetries in data, demonstrating significant effectiveness in\ncertain scenarios. However, these methods often suffer from limited\nrepresentation accuracy and rely on strict symmetry assumptions that may not\nhold in practice. These limitations pose a significant drawback for image\nrestoration tasks, which demands high accuracy and precise symmetry\nrepresentation. To address these challenges, we propose a rotation-equivariant\nregularization strategy that adaptively enforces the appropriate symmetry\nconstraints on the data while preserving the network's representational\naccuracy. Specifically, we introduce EQ-Reg, a regularizer designed to enhance\nrotation equivariance, which innovatively extends the insights of\ndata-augmentation-based and equivariant-based methodologies. This is achieved\nthrough self-supervised learning and the spatial rotation and cyclic channel\nshift of feature maps deduce in the equivariant framework. Our approach firstly\nenables a non-strictly equivariant network suitable for image restoration,\nproviding a simple and adaptive mechanism for adjusting equivariance based on\ntask. Extensive experiments across three low-level tasks demonstrate the\nsuperior accuracy and generalization capability of our method, outperforming\nstate-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2505.19799v1", "date": "2025-05-26", "relevancy": 2.142, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5621}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5362}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Regularization-Guided%20Equivariant%20Approach%20for%20Image%20Restoration&body=Title%3A%20A%20Regularization-Guided%20Equivariant%20Approach%20for%20Image%20Restoration%0AAuthor%3A%20Yulu%20Bai%20and%20Jiahong%20Fu%20and%20Qi%20Xie%20and%20Deyu%20Meng%0AAbstract%3A%20%20%20Equivariant%20and%20invariant%20deep%20learning%20models%20have%20been%20developed%20to%20exploit%0Aintrinsic%20symmetries%20in%20data%2C%20demonstrating%20significant%20effectiveness%20in%0Acertain%20scenarios.%20However%2C%20these%20methods%20often%20suffer%20from%20limited%0Arepresentation%20accuracy%20and%20rely%20on%20strict%20symmetry%20assumptions%20that%20may%20not%0Ahold%20in%20practice.%20These%20limitations%20pose%20a%20significant%20drawback%20for%20image%0Arestoration%20tasks%2C%20which%20demands%20high%20accuracy%20and%20precise%20symmetry%0Arepresentation.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20rotation-equivariant%0Aregularization%20strategy%20that%20adaptively%20enforces%20the%20appropriate%20symmetry%0Aconstraints%20on%20the%20data%20while%20preserving%20the%20network%27s%20representational%0Aaccuracy.%20Specifically%2C%20we%20introduce%20EQ-Reg%2C%20a%20regularizer%20designed%20to%20enhance%0Arotation%20equivariance%2C%20which%20innovatively%20extends%20the%20insights%20of%0Adata-augmentation-based%20and%20equivariant-based%20methodologies.%20This%20is%20achieved%0Athrough%20self-supervised%20learning%20and%20the%20spatial%20rotation%20and%20cyclic%20channel%0Ashift%20of%20feature%20maps%20deduce%20in%20the%20equivariant%20framework.%20Our%20approach%20firstly%0Aenables%20a%20non-strictly%20equivariant%20network%20suitable%20for%20image%20restoration%2C%0Aproviding%20a%20simple%20and%20adaptive%20mechanism%20for%20adjusting%20equivariance%20based%20on%0Atask.%20Extensive%20experiments%20across%20three%20low-level%20tasks%20demonstrate%20the%0Asuperior%20accuracy%20and%20generalization%20capability%20of%20our%20method%2C%20outperforming%0Astate-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Regularization-Guided%2520Equivariant%2520Approach%2520for%2520Image%2520Restoration%26entry.906535625%3DYulu%2520Bai%2520and%2520Jiahong%2520Fu%2520and%2520Qi%2520Xie%2520and%2520Deyu%2520Meng%26entry.1292438233%3D%2520%2520Equivariant%2520and%2520invariant%2520deep%2520learning%2520models%2520have%2520been%2520developed%2520to%2520exploit%250Aintrinsic%2520symmetries%2520in%2520data%252C%2520demonstrating%2520significant%2520effectiveness%2520in%250Acertain%2520scenarios.%2520However%252C%2520these%2520methods%2520often%2520suffer%2520from%2520limited%250Arepresentation%2520accuracy%2520and%2520rely%2520on%2520strict%2520symmetry%2520assumptions%2520that%2520may%2520not%250Ahold%2520in%2520practice.%2520These%2520limitations%2520pose%2520a%2520significant%2520drawback%2520for%2520image%250Arestoration%2520tasks%252C%2520which%2520demands%2520high%2520accuracy%2520and%2520precise%2520symmetry%250Arepresentation.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520rotation-equivariant%250Aregularization%2520strategy%2520that%2520adaptively%2520enforces%2520the%2520appropriate%2520symmetry%250Aconstraints%2520on%2520the%2520data%2520while%2520preserving%2520the%2520network%2527s%2520representational%250Aaccuracy.%2520Specifically%252C%2520we%2520introduce%2520EQ-Reg%252C%2520a%2520regularizer%2520designed%2520to%2520enhance%250Arotation%2520equivariance%252C%2520which%2520innovatively%2520extends%2520the%2520insights%2520of%250Adata-augmentation-based%2520and%2520equivariant-based%2520methodologies.%2520This%2520is%2520achieved%250Athrough%2520self-supervised%2520learning%2520and%2520the%2520spatial%2520rotation%2520and%2520cyclic%2520channel%250Ashift%2520of%2520feature%2520maps%2520deduce%2520in%2520the%2520equivariant%2520framework.%2520Our%2520approach%2520firstly%250Aenables%2520a%2520non-strictly%2520equivariant%2520network%2520suitable%2520for%2520image%2520restoration%252C%250Aproviding%2520a%2520simple%2520and%2520adaptive%2520mechanism%2520for%2520adjusting%2520equivariance%2520based%2520on%250Atask.%2520Extensive%2520experiments%2520across%2520three%2520low-level%2520tasks%2520demonstrate%2520the%250Asuperior%2520accuracy%2520and%2520generalization%2520capability%2520of%2520our%2520method%252C%2520outperforming%250Astate-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Regularization-Guided%20Equivariant%20Approach%20for%20Image%20Restoration&entry.906535625=Yulu%20Bai%20and%20Jiahong%20Fu%20and%20Qi%20Xie%20and%20Deyu%20Meng&entry.1292438233=%20%20Equivariant%20and%20invariant%20deep%20learning%20models%20have%20been%20developed%20to%20exploit%0Aintrinsic%20symmetries%20in%20data%2C%20demonstrating%20significant%20effectiveness%20in%0Acertain%20scenarios.%20However%2C%20these%20methods%20often%20suffer%20from%20limited%0Arepresentation%20accuracy%20and%20rely%20on%20strict%20symmetry%20assumptions%20that%20may%20not%0Ahold%20in%20practice.%20These%20limitations%20pose%20a%20significant%20drawback%20for%20image%0Arestoration%20tasks%2C%20which%20demands%20high%20accuracy%20and%20precise%20symmetry%0Arepresentation.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20rotation-equivariant%0Aregularization%20strategy%20that%20adaptively%20enforces%20the%20appropriate%20symmetry%0Aconstraints%20on%20the%20data%20while%20preserving%20the%20network%27s%20representational%0Aaccuracy.%20Specifically%2C%20we%20introduce%20EQ-Reg%2C%20a%20regularizer%20designed%20to%20enhance%0Arotation%20equivariance%2C%20which%20innovatively%20extends%20the%20insights%20of%0Adata-augmentation-based%20and%20equivariant-based%20methodologies.%20This%20is%20achieved%0Athrough%20self-supervised%20learning%20and%20the%20spatial%20rotation%20and%20cyclic%20channel%0Ashift%20of%20feature%20maps%20deduce%20in%20the%20equivariant%20framework.%20Our%20approach%20firstly%0Aenables%20a%20non-strictly%20equivariant%20network%20suitable%20for%20image%20restoration%2C%0Aproviding%20a%20simple%20and%20adaptive%20mechanism%20for%20adjusting%20equivariance%20based%20on%0Atask.%20Extensive%20experiments%20across%20three%20low-level%20tasks%20demonstrate%20the%0Asuperior%20accuracy%20and%20generalization%20capability%20of%20our%20method%2C%20outperforming%0Astate-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19799v1&entry.124074799=Read"},
{"title": "Improvement Strategies for Few-Shot Learning in OCT Image Classification\n  of Rare Retinal Diseases", "author": "Cheng-Yu Tai and Ching-Wen Chen and Chi-Chin Wu and Bo-Chen Chiu and  Cheng-Hung and  Lin and Cheng-Kai Lu and Jia-Kang Wang and Tzu-Lun Huang", "abstract": "  This paper focuses on using few-shot learning to improve the accuracy of\nclassifying OCT diagnosis images with major and rare classes. We used the\nGAN-based augmentation strategy as a baseline and introduced several novel\nmethods to further enhance our model. The proposed strategy contains U-GAT-IT\nfor improving the generative part and uses the data balance technique to narrow\ndown the skew of accuracy between all categories. The best model obtained was\nbuilt with CBAM attention mechanism and fine-tuned InceptionV3, and achieved an\noverall accuracy of 97.85%, representing a significant improvement over the\noriginal baseline.\n", "link": "http://arxiv.org/abs/2505.20149v1", "date": "2025-05-26", "relevancy": 2.1338, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5705}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5109}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improvement%20Strategies%20for%20Few-Shot%20Learning%20in%20OCT%20Image%20Classification%0A%20%20of%20Rare%20Retinal%20Diseases&body=Title%3A%20Improvement%20Strategies%20for%20Few-Shot%20Learning%20in%20OCT%20Image%20Classification%0A%20%20of%20Rare%20Retinal%20Diseases%0AAuthor%3A%20Cheng-Yu%20Tai%20and%20Ching-Wen%20Chen%20and%20Chi-Chin%20Wu%20and%20Bo-Chen%20Chiu%20and%20%20Cheng-Hung%20and%20%20Lin%20and%20Cheng-Kai%20Lu%20and%20Jia-Kang%20Wang%20and%20Tzu-Lun%20Huang%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20using%20few-shot%20learning%20to%20improve%20the%20accuracy%20of%0Aclassifying%20OCT%20diagnosis%20images%20with%20major%20and%20rare%20classes.%20We%20used%20the%0AGAN-based%20augmentation%20strategy%20as%20a%20baseline%20and%20introduced%20several%20novel%0Amethods%20to%20further%20enhance%20our%20model.%20The%20proposed%20strategy%20contains%20U-GAT-IT%0Afor%20improving%20the%20generative%20part%20and%20uses%20the%20data%20balance%20technique%20to%20narrow%0Adown%20the%20skew%20of%20accuracy%20between%20all%20categories.%20The%20best%20model%20obtained%20was%0Abuilt%20with%20CBAM%20attention%20mechanism%20and%20fine-tuned%20InceptionV3%2C%20and%20achieved%20an%0Aoverall%20accuracy%20of%2097.85%25%2C%20representing%20a%20significant%20improvement%20over%20the%0Aoriginal%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImprovement%2520Strategies%2520for%2520Few-Shot%2520Learning%2520in%2520OCT%2520Image%2520Classification%250A%2520%2520of%2520Rare%2520Retinal%2520Diseases%26entry.906535625%3DCheng-Yu%2520Tai%2520and%2520Ching-Wen%2520Chen%2520and%2520Chi-Chin%2520Wu%2520and%2520Bo-Chen%2520Chiu%2520and%2520%2520Cheng-Hung%2520and%2520%2520Lin%2520and%2520Cheng-Kai%2520Lu%2520and%2520Jia-Kang%2520Wang%2520and%2520Tzu-Lun%2520Huang%26entry.1292438233%3D%2520%2520This%2520paper%2520focuses%2520on%2520using%2520few-shot%2520learning%2520to%2520improve%2520the%2520accuracy%2520of%250Aclassifying%2520OCT%2520diagnosis%2520images%2520with%2520major%2520and%2520rare%2520classes.%2520We%2520used%2520the%250AGAN-based%2520augmentation%2520strategy%2520as%2520a%2520baseline%2520and%2520introduced%2520several%2520novel%250Amethods%2520to%2520further%2520enhance%2520our%2520model.%2520The%2520proposed%2520strategy%2520contains%2520U-GAT-IT%250Afor%2520improving%2520the%2520generative%2520part%2520and%2520uses%2520the%2520data%2520balance%2520technique%2520to%2520narrow%250Adown%2520the%2520skew%2520of%2520accuracy%2520between%2520all%2520categories.%2520The%2520best%2520model%2520obtained%2520was%250Abuilt%2520with%2520CBAM%2520attention%2520mechanism%2520and%2520fine-tuned%2520InceptionV3%252C%2520and%2520achieved%2520an%250Aoverall%2520accuracy%2520of%252097.85%2525%252C%2520representing%2520a%2520significant%2520improvement%2520over%2520the%250Aoriginal%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improvement%20Strategies%20for%20Few-Shot%20Learning%20in%20OCT%20Image%20Classification%0A%20%20of%20Rare%20Retinal%20Diseases&entry.906535625=Cheng-Yu%20Tai%20and%20Ching-Wen%20Chen%20and%20Chi-Chin%20Wu%20and%20Bo-Chen%20Chiu%20and%20%20Cheng-Hung%20and%20%20Lin%20and%20Cheng-Kai%20Lu%20and%20Jia-Kang%20Wang%20and%20Tzu-Lun%20Huang&entry.1292438233=%20%20This%20paper%20focuses%20on%20using%20few-shot%20learning%20to%20improve%20the%20accuracy%20of%0Aclassifying%20OCT%20diagnosis%20images%20with%20major%20and%20rare%20classes.%20We%20used%20the%0AGAN-based%20augmentation%20strategy%20as%20a%20baseline%20and%20introduced%20several%20novel%0Amethods%20to%20further%20enhance%20our%20model.%20The%20proposed%20strategy%20contains%20U-GAT-IT%0Afor%20improving%20the%20generative%20part%20and%20uses%20the%20data%20balance%20technique%20to%20narrow%0Adown%20the%20skew%20of%20accuracy%20between%20all%20categories.%20The%20best%20model%20obtained%20was%0Abuilt%20with%20CBAM%20attention%20mechanism%20and%20fine-tuned%20InceptionV3%2C%20and%20achieved%20an%0Aoverall%20accuracy%20of%2097.85%25%2C%20representing%20a%20significant%20improvement%20over%20the%0Aoriginal%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20149v1&entry.124074799=Read"},
{"title": "DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in\n  Digital Forensics and Incident Response", "author": "Bilel Cherif and Tamas Bisztray and Richard A. Dubniczky and Aaesha Aldahmani and Saeed Alshehhi and Norbert Tihanyi", "abstract": "  Digital Forensics and Incident Response (DFIR) involves analyzing digital\nevidence to support legal investigations. Large Language Models (LLMs) offer\nnew opportunities in DFIR tasks such as log analysis and memory forensics, but\ntheir susceptibility to errors and hallucinations raises concerns in\nhigh-stakes contexts. Despite growing interest, there is no comprehensive\nbenchmark to evaluate LLMs across both theoretical and practical DFIR domains.\nTo address this gap, we present DFIR-Metric, a benchmark with three components:\n(1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice\nquestions sourced from industry-standard certifications and official\ndocumentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing\nmulti-step reasoning and evidence correlation; and (3) Practical Analysis: 500\ndisk and memory forensics cases from the NIST Computer Forensics Tool Testing\nProgram (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their\naccuracy and consistency across trials. We also introduce a new metric, the\nTask Understanding Score (TUS), designed to more effectively evaluate models in\nscenarios where they achieve near-zero accuracy. This benchmark offers a\nrigorous, reproducible foundation for advancing AI in digital forensics. All\nscripts, artifacts, and results are available on the project website at\nhttps://github.com/DFIR-Metric.\n", "link": "http://arxiv.org/abs/2505.19973v1", "date": "2025-05-26", "relevancy": 2.1274, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5433}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5296}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DFIR-Metric%3A%20A%20Benchmark%20Dataset%20for%20Evaluating%20Large%20Language%20Models%20in%0A%20%20Digital%20Forensics%20and%20Incident%20Response&body=Title%3A%20DFIR-Metric%3A%20A%20Benchmark%20Dataset%20for%20Evaluating%20Large%20Language%20Models%20in%0A%20%20Digital%20Forensics%20and%20Incident%20Response%0AAuthor%3A%20Bilel%20Cherif%20and%20Tamas%20Bisztray%20and%20Richard%20A.%20Dubniczky%20and%20Aaesha%20Aldahmani%20and%20Saeed%20Alshehhi%20and%20Norbert%20Tihanyi%0AAbstract%3A%20%20%20Digital%20Forensics%20and%20Incident%20Response%20%28DFIR%29%20involves%20analyzing%20digital%0Aevidence%20to%20support%20legal%20investigations.%20Large%20Language%20Models%20%28LLMs%29%20offer%0Anew%20opportunities%20in%20DFIR%20tasks%20such%20as%20log%20analysis%20and%20memory%20forensics%2C%20but%0Atheir%20susceptibility%20to%20errors%20and%20hallucinations%20raises%20concerns%20in%0Ahigh-stakes%20contexts.%20Despite%20growing%20interest%2C%20there%20is%20no%20comprehensive%0Abenchmark%20to%20evaluate%20LLMs%20across%20both%20theoretical%20and%20practical%20DFIR%20domains.%0ATo%20address%20this%20gap%2C%20we%20present%20DFIR-Metric%2C%20a%20benchmark%20with%20three%20components%3A%0A%281%29%20Knowledge%20Assessment%3A%20a%20set%20of%20700%20expert-reviewed%20multiple-choice%0Aquestions%20sourced%20from%20industry-standard%20certifications%20and%20official%0Adocumentation%3B%20%282%29%20Realistic%20Forensic%20Challenges%3A%20150%20CTF-style%20tasks%20testing%0Amulti-step%20reasoning%20and%20evidence%20correlation%3B%20and%20%283%29%20Practical%20Analysis%3A%20500%0Adisk%20and%20memory%20forensics%20cases%20from%20the%20NIST%20Computer%20Forensics%20Tool%20Testing%0AProgram%20%28CFTT%29.%20We%20evaluated%2014%20LLMs%20using%20DFIR-Metric%2C%20analyzing%20both%20their%0Aaccuracy%20and%20consistency%20across%20trials.%20We%20also%20introduce%20a%20new%20metric%2C%20the%0ATask%20Understanding%20Score%20%28TUS%29%2C%20designed%20to%20more%20effectively%20evaluate%20models%20in%0Ascenarios%20where%20they%20achieve%20near-zero%20accuracy.%20This%20benchmark%20offers%20a%0Arigorous%2C%20reproducible%20foundation%20for%20advancing%20AI%20in%20digital%20forensics.%20All%0Ascripts%2C%20artifacts%2C%20and%20results%20are%20available%20on%20the%20project%20website%20at%0Ahttps%3A//github.com/DFIR-Metric.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDFIR-Metric%253A%2520A%2520Benchmark%2520Dataset%2520for%2520Evaluating%2520Large%2520Language%2520Models%2520in%250A%2520%2520Digital%2520Forensics%2520and%2520Incident%2520Response%26entry.906535625%3DBilel%2520Cherif%2520and%2520Tamas%2520Bisztray%2520and%2520Richard%2520A.%2520Dubniczky%2520and%2520Aaesha%2520Aldahmani%2520and%2520Saeed%2520Alshehhi%2520and%2520Norbert%2520Tihanyi%26entry.1292438233%3D%2520%2520Digital%2520Forensics%2520and%2520Incident%2520Response%2520%2528DFIR%2529%2520involves%2520analyzing%2520digital%250Aevidence%2520to%2520support%2520legal%2520investigations.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520offer%250Anew%2520opportunities%2520in%2520DFIR%2520tasks%2520such%2520as%2520log%2520analysis%2520and%2520memory%2520forensics%252C%2520but%250Atheir%2520susceptibility%2520to%2520errors%2520and%2520hallucinations%2520raises%2520concerns%2520in%250Ahigh-stakes%2520contexts.%2520Despite%2520growing%2520interest%252C%2520there%2520is%2520no%2520comprehensive%250Abenchmark%2520to%2520evaluate%2520LLMs%2520across%2520both%2520theoretical%2520and%2520practical%2520DFIR%2520domains.%250ATo%2520address%2520this%2520gap%252C%2520we%2520present%2520DFIR-Metric%252C%2520a%2520benchmark%2520with%2520three%2520components%253A%250A%25281%2529%2520Knowledge%2520Assessment%253A%2520a%2520set%2520of%2520700%2520expert-reviewed%2520multiple-choice%250Aquestions%2520sourced%2520from%2520industry-standard%2520certifications%2520and%2520official%250Adocumentation%253B%2520%25282%2529%2520Realistic%2520Forensic%2520Challenges%253A%2520150%2520CTF-style%2520tasks%2520testing%250Amulti-step%2520reasoning%2520and%2520evidence%2520correlation%253B%2520and%2520%25283%2529%2520Practical%2520Analysis%253A%2520500%250Adisk%2520and%2520memory%2520forensics%2520cases%2520from%2520the%2520NIST%2520Computer%2520Forensics%2520Tool%2520Testing%250AProgram%2520%2528CFTT%2529.%2520We%2520evaluated%252014%2520LLMs%2520using%2520DFIR-Metric%252C%2520analyzing%2520both%2520their%250Aaccuracy%2520and%2520consistency%2520across%2520trials.%2520We%2520also%2520introduce%2520a%2520new%2520metric%252C%2520the%250ATask%2520Understanding%2520Score%2520%2528TUS%2529%252C%2520designed%2520to%2520more%2520effectively%2520evaluate%2520models%2520in%250Ascenarios%2520where%2520they%2520achieve%2520near-zero%2520accuracy.%2520This%2520benchmark%2520offers%2520a%250Arigorous%252C%2520reproducible%2520foundation%2520for%2520advancing%2520AI%2520in%2520digital%2520forensics.%2520All%250Ascripts%252C%2520artifacts%252C%2520and%2520results%2520are%2520available%2520on%2520the%2520project%2520website%2520at%250Ahttps%253A//github.com/DFIR-Metric.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DFIR-Metric%3A%20A%20Benchmark%20Dataset%20for%20Evaluating%20Large%20Language%20Models%20in%0A%20%20Digital%20Forensics%20and%20Incident%20Response&entry.906535625=Bilel%20Cherif%20and%20Tamas%20Bisztray%20and%20Richard%20A.%20Dubniczky%20and%20Aaesha%20Aldahmani%20and%20Saeed%20Alshehhi%20and%20Norbert%20Tihanyi&entry.1292438233=%20%20Digital%20Forensics%20and%20Incident%20Response%20%28DFIR%29%20involves%20analyzing%20digital%0Aevidence%20to%20support%20legal%20investigations.%20Large%20Language%20Models%20%28LLMs%29%20offer%0Anew%20opportunities%20in%20DFIR%20tasks%20such%20as%20log%20analysis%20and%20memory%20forensics%2C%20but%0Atheir%20susceptibility%20to%20errors%20and%20hallucinations%20raises%20concerns%20in%0Ahigh-stakes%20contexts.%20Despite%20growing%20interest%2C%20there%20is%20no%20comprehensive%0Abenchmark%20to%20evaluate%20LLMs%20across%20both%20theoretical%20and%20practical%20DFIR%20domains.%0ATo%20address%20this%20gap%2C%20we%20present%20DFIR-Metric%2C%20a%20benchmark%20with%20three%20components%3A%0A%281%29%20Knowledge%20Assessment%3A%20a%20set%20of%20700%20expert-reviewed%20multiple-choice%0Aquestions%20sourced%20from%20industry-standard%20certifications%20and%20official%0Adocumentation%3B%20%282%29%20Realistic%20Forensic%20Challenges%3A%20150%20CTF-style%20tasks%20testing%0Amulti-step%20reasoning%20and%20evidence%20correlation%3B%20and%20%283%29%20Practical%20Analysis%3A%20500%0Adisk%20and%20memory%20forensics%20cases%20from%20the%20NIST%20Computer%20Forensics%20Tool%20Testing%0AProgram%20%28CFTT%29.%20We%20evaluated%2014%20LLMs%20using%20DFIR-Metric%2C%20analyzing%20both%20their%0Aaccuracy%20and%20consistency%20across%20trials.%20We%20also%20introduce%20a%20new%20metric%2C%20the%0ATask%20Understanding%20Score%20%28TUS%29%2C%20designed%20to%20more%20effectively%20evaluate%20models%20in%0Ascenarios%20where%20they%20achieve%20near-zero%20accuracy.%20This%20benchmark%20offers%20a%0Arigorous%2C%20reproducible%20foundation%20for%20advancing%20AI%20in%20digital%20forensics.%20All%0Ascripts%2C%20artifacts%2C%20and%20results%20are%20available%20on%20the%20project%20website%20at%0Ahttps%3A//github.com/DFIR-Metric.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19973v1&entry.124074799=Read"},
{"title": "Editing as Unlearning: Are Knowledge Editing Methods Strong Baselines\n  for Large Language Model Unlearning?", "author": "Zexi Li and Xiangzhu Wang and William F. Shen and Meghdad Kurmanji and Xinchi Qiu and Dongqi Cai and Chao Wu and Nicholas D. Lane", "abstract": "  Large language Model (LLM) unlearning, i.e., selectively removing information\nfrom LLMs, is vital for responsible model deployment. Differently, LLM\nknowledge editing aims to modify LLM knowledge instead of removing it. Though\nediting and unlearning seem to be two distinct tasks, we find there is a tight\nconnection between them. In this paper, we conceptualize unlearning as a\nspecial case of editing where information is modified to a refusal or \"empty\nset\" $\\emptyset$ response, signifying its removal. This paper thus investigates\nif knowledge editing techniques are strong baselines for LLM unlearning. We\nevaluate state-of-the-art (SOTA) editing methods (e.g., ROME, MEMIT, GRACE,\nWISE, and AlphaEdit) against existing unlearning approaches on pretrained and\nfinetuned knowledge. Results show certain editing methods, notably WISE and\nAlphaEdit, are effective unlearning baselines, especially for pretrained\nknowledge, and excel in generating human-aligned refusal answers. To better\nadapt editing methods for unlearning applications, we propose practical recipes\nincluding self-improvement and query merging. The former leverages the LLM's\nown in-context learning ability to craft a more human-aligned unlearning\ntarget, and the latter enables ROME and MEMIT to perform well in unlearning\nlonger sample sequences. We advocate for the unlearning community to adopt SOTA\nediting methods as baselines and explore unlearning from an editing perspective\nfor more holistic LLM memory control.\n", "link": "http://arxiv.org/abs/2505.19855v1", "date": "2025-05-26", "relevancy": 1.9542, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4899}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4899}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Editing%20as%20Unlearning%3A%20Are%20Knowledge%20Editing%20Methods%20Strong%20Baselines%0A%20%20for%20Large%20Language%20Model%20Unlearning%3F&body=Title%3A%20Editing%20as%20Unlearning%3A%20Are%20Knowledge%20Editing%20Methods%20Strong%20Baselines%0A%20%20for%20Large%20Language%20Model%20Unlearning%3F%0AAuthor%3A%20Zexi%20Li%20and%20Xiangzhu%20Wang%20and%20William%20F.%20Shen%20and%20Meghdad%20Kurmanji%20and%20Xinchi%20Qiu%20and%20Dongqi%20Cai%20and%20Chao%20Wu%20and%20Nicholas%20D.%20Lane%0AAbstract%3A%20%20%20Large%20language%20Model%20%28LLM%29%20unlearning%2C%20i.e.%2C%20selectively%20removing%20information%0Afrom%20LLMs%2C%20is%20vital%20for%20responsible%20model%20deployment.%20Differently%2C%20LLM%0Aknowledge%20editing%20aims%20to%20modify%20LLM%20knowledge%20instead%20of%20removing%20it.%20Though%0Aediting%20and%20unlearning%20seem%20to%20be%20two%20distinct%20tasks%2C%20we%20find%20there%20is%20a%20tight%0Aconnection%20between%20them.%20In%20this%20paper%2C%20we%20conceptualize%20unlearning%20as%20a%0Aspecial%20case%20of%20editing%20where%20information%20is%20modified%20to%20a%20refusal%20or%20%22empty%0Aset%22%20%24%5Cemptyset%24%20response%2C%20signifying%20its%20removal.%20This%20paper%20thus%20investigates%0Aif%20knowledge%20editing%20techniques%20are%20strong%20baselines%20for%20LLM%20unlearning.%20We%0Aevaluate%20state-of-the-art%20%28SOTA%29%20editing%20methods%20%28e.g.%2C%20ROME%2C%20MEMIT%2C%20GRACE%2C%0AWISE%2C%20and%20AlphaEdit%29%20against%20existing%20unlearning%20approaches%20on%20pretrained%20and%0Afinetuned%20knowledge.%20Results%20show%20certain%20editing%20methods%2C%20notably%20WISE%20and%0AAlphaEdit%2C%20are%20effective%20unlearning%20baselines%2C%20especially%20for%20pretrained%0Aknowledge%2C%20and%20excel%20in%20generating%20human-aligned%20refusal%20answers.%20To%20better%0Aadapt%20editing%20methods%20for%20unlearning%20applications%2C%20we%20propose%20practical%20recipes%0Aincluding%20self-improvement%20and%20query%20merging.%20The%20former%20leverages%20the%20LLM%27s%0Aown%20in-context%20learning%20ability%20to%20craft%20a%20more%20human-aligned%20unlearning%0Atarget%2C%20and%20the%20latter%20enables%20ROME%20and%20MEMIT%20to%20perform%20well%20in%20unlearning%0Alonger%20sample%20sequences.%20We%20advocate%20for%20the%20unlearning%20community%20to%20adopt%20SOTA%0Aediting%20methods%20as%20baselines%20and%20explore%20unlearning%20from%20an%20editing%20perspective%0Afor%20more%20holistic%20LLM%20memory%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEditing%2520as%2520Unlearning%253A%2520Are%2520Knowledge%2520Editing%2520Methods%2520Strong%2520Baselines%250A%2520%2520for%2520Large%2520Language%2520Model%2520Unlearning%253F%26entry.906535625%3DZexi%2520Li%2520and%2520Xiangzhu%2520Wang%2520and%2520William%2520F.%2520Shen%2520and%2520Meghdad%2520Kurmanji%2520and%2520Xinchi%2520Qiu%2520and%2520Dongqi%2520Cai%2520and%2520Chao%2520Wu%2520and%2520Nicholas%2520D.%2520Lane%26entry.1292438233%3D%2520%2520Large%2520language%2520Model%2520%2528LLM%2529%2520unlearning%252C%2520i.e.%252C%2520selectively%2520removing%2520information%250Afrom%2520LLMs%252C%2520is%2520vital%2520for%2520responsible%2520model%2520deployment.%2520Differently%252C%2520LLM%250Aknowledge%2520editing%2520aims%2520to%2520modify%2520LLM%2520knowledge%2520instead%2520of%2520removing%2520it.%2520Though%250Aediting%2520and%2520unlearning%2520seem%2520to%2520be%2520two%2520distinct%2520tasks%252C%2520we%2520find%2520there%2520is%2520a%2520tight%250Aconnection%2520between%2520them.%2520In%2520this%2520paper%252C%2520we%2520conceptualize%2520unlearning%2520as%2520a%250Aspecial%2520case%2520of%2520editing%2520where%2520information%2520is%2520modified%2520to%2520a%2520refusal%2520or%2520%2522empty%250Aset%2522%2520%2524%255Cemptyset%2524%2520response%252C%2520signifying%2520its%2520removal.%2520This%2520paper%2520thus%2520investigates%250Aif%2520knowledge%2520editing%2520techniques%2520are%2520strong%2520baselines%2520for%2520LLM%2520unlearning.%2520We%250Aevaluate%2520state-of-the-art%2520%2528SOTA%2529%2520editing%2520methods%2520%2528e.g.%252C%2520ROME%252C%2520MEMIT%252C%2520GRACE%252C%250AWISE%252C%2520and%2520AlphaEdit%2529%2520against%2520existing%2520unlearning%2520approaches%2520on%2520pretrained%2520and%250Afinetuned%2520knowledge.%2520Results%2520show%2520certain%2520editing%2520methods%252C%2520notably%2520WISE%2520and%250AAlphaEdit%252C%2520are%2520effective%2520unlearning%2520baselines%252C%2520especially%2520for%2520pretrained%250Aknowledge%252C%2520and%2520excel%2520in%2520generating%2520human-aligned%2520refusal%2520answers.%2520To%2520better%250Aadapt%2520editing%2520methods%2520for%2520unlearning%2520applications%252C%2520we%2520propose%2520practical%2520recipes%250Aincluding%2520self-improvement%2520and%2520query%2520merging.%2520The%2520former%2520leverages%2520the%2520LLM%2527s%250Aown%2520in-context%2520learning%2520ability%2520to%2520craft%2520a%2520more%2520human-aligned%2520unlearning%250Atarget%252C%2520and%2520the%2520latter%2520enables%2520ROME%2520and%2520MEMIT%2520to%2520perform%2520well%2520in%2520unlearning%250Alonger%2520sample%2520sequences.%2520We%2520advocate%2520for%2520the%2520unlearning%2520community%2520to%2520adopt%2520SOTA%250Aediting%2520methods%2520as%2520baselines%2520and%2520explore%2520unlearning%2520from%2520an%2520editing%2520perspective%250Afor%2520more%2520holistic%2520LLM%2520memory%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Editing%20as%20Unlearning%3A%20Are%20Knowledge%20Editing%20Methods%20Strong%20Baselines%0A%20%20for%20Large%20Language%20Model%20Unlearning%3F&entry.906535625=Zexi%20Li%20and%20Xiangzhu%20Wang%20and%20William%20F.%20Shen%20and%20Meghdad%20Kurmanji%20and%20Xinchi%20Qiu%20and%20Dongqi%20Cai%20and%20Chao%20Wu%20and%20Nicholas%20D.%20Lane&entry.1292438233=%20%20Large%20language%20Model%20%28LLM%29%20unlearning%2C%20i.e.%2C%20selectively%20removing%20information%0Afrom%20LLMs%2C%20is%20vital%20for%20responsible%20model%20deployment.%20Differently%2C%20LLM%0Aknowledge%20editing%20aims%20to%20modify%20LLM%20knowledge%20instead%20of%20removing%20it.%20Though%0Aediting%20and%20unlearning%20seem%20to%20be%20two%20distinct%20tasks%2C%20we%20find%20there%20is%20a%20tight%0Aconnection%20between%20them.%20In%20this%20paper%2C%20we%20conceptualize%20unlearning%20as%20a%0Aspecial%20case%20of%20editing%20where%20information%20is%20modified%20to%20a%20refusal%20or%20%22empty%0Aset%22%20%24%5Cemptyset%24%20response%2C%20signifying%20its%20removal.%20This%20paper%20thus%20investigates%0Aif%20knowledge%20editing%20techniques%20are%20strong%20baselines%20for%20LLM%20unlearning.%20We%0Aevaluate%20state-of-the-art%20%28SOTA%29%20editing%20methods%20%28e.g.%2C%20ROME%2C%20MEMIT%2C%20GRACE%2C%0AWISE%2C%20and%20AlphaEdit%29%20against%20existing%20unlearning%20approaches%20on%20pretrained%20and%0Afinetuned%20knowledge.%20Results%20show%20certain%20editing%20methods%2C%20notably%20WISE%20and%0AAlphaEdit%2C%20are%20effective%20unlearning%20baselines%2C%20especially%20for%20pretrained%0Aknowledge%2C%20and%20excel%20in%20generating%20human-aligned%20refusal%20answers.%20To%20better%0Aadapt%20editing%20methods%20for%20unlearning%20applications%2C%20we%20propose%20practical%20recipes%0Aincluding%20self-improvement%20and%20query%20merging.%20The%20former%20leverages%20the%20LLM%27s%0Aown%20in-context%20learning%20ability%20to%20craft%20a%20more%20human-aligned%20unlearning%0Atarget%2C%20and%20the%20latter%20enables%20ROME%20and%20MEMIT%20to%20perform%20well%20in%20unlearning%0Alonger%20sample%20sequences.%20We%20advocate%20for%20the%20unlearning%20community%20to%20adopt%20SOTA%0Aediting%20methods%20as%20baselines%20and%20explore%20unlearning%20from%20an%20editing%20perspective%0Afor%20more%20holistic%20LLM%20memory%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19855v1&entry.124074799=Read"},
{"title": "Balancing Interference and Correlation in Spatial Experimental Designs:\n  A Causal Graph Cut Approach", "author": "Zhu Jin and Li Jingyi and Zhou Hongyi and Lin Yinan and Lin Zhenhua and Shi Chengchun", "abstract": "  This paper focuses on the design of spatial experiments to optimize the\namount of information derived from the experimental data and enhance the\naccuracy of the resulting causal effect estimator. We propose a surrogate\nfunction for the mean squared error (MSE) of the estimator, which facilitates\nthe use of classical graph cut algorithms to learn the optimal design. Our\nproposal offers three key advances: (1) it accommodates moderate to large\nspatial interference effects; (2) it adapts to different spatial covariance\nfunctions; (3) it is computationally efficient. Theoretical results and\nnumerical experiments based on synthetic environments and a dispatch simulator\nthat models a city-scale ridesharing market, further validate the effectiveness\nof our design. A python implementation of our method is available at\nhttps://github.com/Mamba413/CausalGraphCut.\n", "link": "http://arxiv.org/abs/2505.20130v1", "date": "2025-05-26", "relevancy": 1.4112, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4807}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4741}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Balancing%20Interference%20and%20Correlation%20in%20Spatial%20Experimental%20Designs%3A%0A%20%20A%20Causal%20Graph%20Cut%20Approach&body=Title%3A%20Balancing%20Interference%20and%20Correlation%20in%20Spatial%20Experimental%20Designs%3A%0A%20%20A%20Causal%20Graph%20Cut%20Approach%0AAuthor%3A%20Zhu%20Jin%20and%20Li%20Jingyi%20and%20Zhou%20Hongyi%20and%20Lin%20Yinan%20and%20Lin%20Zhenhua%20and%20Shi%20Chengchun%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20the%20design%20of%20spatial%20experiments%20to%20optimize%20the%0Aamount%20of%20information%20derived%20from%20the%20experimental%20data%20and%20enhance%20the%0Aaccuracy%20of%20the%20resulting%20causal%20effect%20estimator.%20We%20propose%20a%20surrogate%0Afunction%20for%20the%20mean%20squared%20error%20%28MSE%29%20of%20the%20estimator%2C%20which%20facilitates%0Athe%20use%20of%20classical%20graph%20cut%20algorithms%20to%20learn%20the%20optimal%20design.%20Our%0Aproposal%20offers%20three%20key%20advances%3A%20%281%29%20it%20accommodates%20moderate%20to%20large%0Aspatial%20interference%20effects%3B%20%282%29%20it%20adapts%20to%20different%20spatial%20covariance%0Afunctions%3B%20%283%29%20it%20is%20computationally%20efficient.%20Theoretical%20results%20and%0Anumerical%20experiments%20based%20on%20synthetic%20environments%20and%20a%20dispatch%20simulator%0Athat%20models%20a%20city-scale%20ridesharing%20market%2C%20further%20validate%20the%20effectiveness%0Aof%20our%20design.%20A%20python%20implementation%20of%20our%20method%20is%20available%20at%0Ahttps%3A//github.com/Mamba413/CausalGraphCut.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBalancing%2520Interference%2520and%2520Correlation%2520in%2520Spatial%2520Experimental%2520Designs%253A%250A%2520%2520A%2520Causal%2520Graph%2520Cut%2520Approach%26entry.906535625%3DZhu%2520Jin%2520and%2520Li%2520Jingyi%2520and%2520Zhou%2520Hongyi%2520and%2520Lin%2520Yinan%2520and%2520Lin%2520Zhenhua%2520and%2520Shi%2520Chengchun%26entry.1292438233%3D%2520%2520This%2520paper%2520focuses%2520on%2520the%2520design%2520of%2520spatial%2520experiments%2520to%2520optimize%2520the%250Aamount%2520of%2520information%2520derived%2520from%2520the%2520experimental%2520data%2520and%2520enhance%2520the%250Aaccuracy%2520of%2520the%2520resulting%2520causal%2520effect%2520estimator.%2520We%2520propose%2520a%2520surrogate%250Afunction%2520for%2520the%2520mean%2520squared%2520error%2520%2528MSE%2529%2520of%2520the%2520estimator%252C%2520which%2520facilitates%250Athe%2520use%2520of%2520classical%2520graph%2520cut%2520algorithms%2520to%2520learn%2520the%2520optimal%2520design.%2520Our%250Aproposal%2520offers%2520three%2520key%2520advances%253A%2520%25281%2529%2520it%2520accommodates%2520moderate%2520to%2520large%250Aspatial%2520interference%2520effects%253B%2520%25282%2529%2520it%2520adapts%2520to%2520different%2520spatial%2520covariance%250Afunctions%253B%2520%25283%2529%2520it%2520is%2520computationally%2520efficient.%2520Theoretical%2520results%2520and%250Anumerical%2520experiments%2520based%2520on%2520synthetic%2520environments%2520and%2520a%2520dispatch%2520simulator%250Athat%2520models%2520a%2520city-scale%2520ridesharing%2520market%252C%2520further%2520validate%2520the%2520effectiveness%250Aof%2520our%2520design.%2520A%2520python%2520implementation%2520of%2520our%2520method%2520is%2520available%2520at%250Ahttps%253A//github.com/Mamba413/CausalGraphCut.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Balancing%20Interference%20and%20Correlation%20in%20Spatial%20Experimental%20Designs%3A%0A%20%20A%20Causal%20Graph%20Cut%20Approach&entry.906535625=Zhu%20Jin%20and%20Li%20Jingyi%20and%20Zhou%20Hongyi%20and%20Lin%20Yinan%20and%20Lin%20Zhenhua%20and%20Shi%20Chengchun&entry.1292438233=%20%20This%20paper%20focuses%20on%20the%20design%20of%20spatial%20experiments%20to%20optimize%20the%0Aamount%20of%20information%20derived%20from%20the%20experimental%20data%20and%20enhance%20the%0Aaccuracy%20of%20the%20resulting%20causal%20effect%20estimator.%20We%20propose%20a%20surrogate%0Afunction%20for%20the%20mean%20squared%20error%20%28MSE%29%20of%20the%20estimator%2C%20which%20facilitates%0Athe%20use%20of%20classical%20graph%20cut%20algorithms%20to%20learn%20the%20optimal%20design.%20Our%0Aproposal%20offers%20three%20key%20advances%3A%20%281%29%20it%20accommodates%20moderate%20to%20large%0Aspatial%20interference%20effects%3B%20%282%29%20it%20adapts%20to%20different%20spatial%20covariance%0Afunctions%3B%20%283%29%20it%20is%20computationally%20efficient.%20Theoretical%20results%20and%0Anumerical%20experiments%20based%20on%20synthetic%20environments%20and%20a%20dispatch%20simulator%0Athat%20models%20a%20city-scale%20ridesharing%20market%2C%20further%20validate%20the%20effectiveness%0Aof%20our%20design.%20A%20python%20implementation%20of%20our%20method%20is%20available%20at%0Ahttps%3A//github.com/Mamba413/CausalGraphCut.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20130v1&entry.124074799=Read"},
{"title": "Phare: A Safety Probe for Large Language Models", "author": "Pierre Le Jeune and Beno\u00eet Mal\u00e9zieux and Weixuan Xiao and Matteo Dora", "abstract": "  Ensuring the safety of large language models (LLMs) is critical for\nresponsible deployment, yet existing evaluations often prioritize performance\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\nframework to probe and evaluate LLM behavior across three critical dimensions:\nhallucination and reliability, social biases, and harmful content generation.\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\nvulnerabilities across all safety dimensions, including sycophancy, prompt\nsensitivity, and stereotype reproduction. By highlighting these specific\nfailure modes rather than simply ranking models, Phare provides researchers and\npractitioners with actionable insights to build more robust, aligned, and\ntrustworthy language systems.\n", "link": "http://arxiv.org/abs/2505.11365v4", "date": "2025-05-26", "relevancy": 2.0389, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5174}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5174}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Phare%3A%20A%20Safety%20Probe%20for%20Large%20Language%20Models&body=Title%3A%20Phare%3A%20A%20Safety%20Probe%20for%20Large%20Language%20Models%0AAuthor%3A%20Pierre%20Le%20Jeune%20and%20Beno%C3%AEt%20Mal%C3%A9zieux%20and%20Weixuan%20Xiao%20and%20Matteo%20Dora%0AAbstract%3A%20%20%20Ensuring%20the%20safety%20of%20large%20language%20models%20%28LLMs%29%20is%20critical%20for%0Aresponsible%20deployment%2C%20yet%20existing%20evaluations%20often%20prioritize%20performance%0Aover%20identifying%20failure%20modes.%20We%20introduce%20Phare%2C%20a%20multilingual%20diagnostic%0Aframework%20to%20probe%20and%20evaluate%20LLM%20behavior%20across%20three%20critical%20dimensions%3A%0Ahallucination%20and%20reliability%2C%20social%20biases%2C%20and%20harmful%20content%20generation.%0AOur%20evaluation%20of%2017%20state-of-the-art%20LLMs%20reveals%20patterns%20of%20systematic%0Avulnerabilities%20across%20all%20safety%20dimensions%2C%20including%20sycophancy%2C%20prompt%0Asensitivity%2C%20and%20stereotype%20reproduction.%20By%20highlighting%20these%20specific%0Afailure%20modes%20rather%20than%20simply%20ranking%20models%2C%20Phare%20provides%20researchers%20and%0Apractitioners%20with%20actionable%20insights%20to%20build%20more%20robust%2C%20aligned%2C%20and%0Atrustworthy%20language%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11365v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhare%253A%2520A%2520Safety%2520Probe%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DPierre%2520Le%2520Jeune%2520and%2520Beno%25C3%25AEt%2520Mal%25C3%25A9zieux%2520and%2520Weixuan%2520Xiao%2520and%2520Matteo%2520Dora%26entry.1292438233%3D%2520%2520Ensuring%2520the%2520safety%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520critical%2520for%250Aresponsible%2520deployment%252C%2520yet%2520existing%2520evaluations%2520often%2520prioritize%2520performance%250Aover%2520identifying%2520failure%2520modes.%2520We%2520introduce%2520Phare%252C%2520a%2520multilingual%2520diagnostic%250Aframework%2520to%2520probe%2520and%2520evaluate%2520LLM%2520behavior%2520across%2520three%2520critical%2520dimensions%253A%250Ahallucination%2520and%2520reliability%252C%2520social%2520biases%252C%2520and%2520harmful%2520content%2520generation.%250AOur%2520evaluation%2520of%252017%2520state-of-the-art%2520LLMs%2520reveals%2520patterns%2520of%2520systematic%250Avulnerabilities%2520across%2520all%2520safety%2520dimensions%252C%2520including%2520sycophancy%252C%2520prompt%250Asensitivity%252C%2520and%2520stereotype%2520reproduction.%2520By%2520highlighting%2520these%2520specific%250Afailure%2520modes%2520rather%2520than%2520simply%2520ranking%2520models%252C%2520Phare%2520provides%2520researchers%2520and%250Apractitioners%2520with%2520actionable%2520insights%2520to%2520build%2520more%2520robust%252C%2520aligned%252C%2520and%250Atrustworthy%2520language%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11365v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phare%3A%20A%20Safety%20Probe%20for%20Large%20Language%20Models&entry.906535625=Pierre%20Le%20Jeune%20and%20Beno%C3%AEt%20Mal%C3%A9zieux%20and%20Weixuan%20Xiao%20and%20Matteo%20Dora&entry.1292438233=%20%20Ensuring%20the%20safety%20of%20large%20language%20models%20%28LLMs%29%20is%20critical%20for%0Aresponsible%20deployment%2C%20yet%20existing%20evaluations%20often%20prioritize%20performance%0Aover%20identifying%20failure%20modes.%20We%20introduce%20Phare%2C%20a%20multilingual%20diagnostic%0Aframework%20to%20probe%20and%20evaluate%20LLM%20behavior%20across%20three%20critical%20dimensions%3A%0Ahallucination%20and%20reliability%2C%20social%20biases%2C%20and%20harmful%20content%20generation.%0AOur%20evaluation%20of%2017%20state-of-the-art%20LLMs%20reveals%20patterns%20of%20systematic%0Avulnerabilities%20across%20all%20safety%20dimensions%2C%20including%20sycophancy%2C%20prompt%0Asensitivity%2C%20and%20stereotype%20reproduction.%20By%20highlighting%20these%20specific%0Afailure%20modes%20rather%20than%20simply%20ranking%20models%2C%20Phare%20provides%20researchers%20and%0Apractitioners%20with%20actionable%20insights%20to%20build%20more%20robust%2C%20aligned%2C%20and%0Atrustworthy%20language%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11365v4&entry.124074799=Read"},
{"title": "Linear Bandits with Non-i.i.d. Noise", "author": "Baptiste Ab\u00e9l\u00e8s and Eugenio Clerico and Hamish Flynn and Gergely Neu", "abstract": "  We study the linear stochastic bandit problem, relaxing the standard i.i.d.\nassumption on the observation noise. As an alternative to this restrictive\nassumption, we allow the noise terms across rounds to be sub-Gaussian but\ninterdependent, with dependencies that decay over time. To address this\nsetting, we develop new confidence sequences using a recently introduced\nreduction scheme to sequential probability assignment, and use these to derive\na bandit algorithm based on the principle of optimism in the face of\nuncertainty. We provide regret bounds for the resulting algorithm, expressed in\nterms of the decay rate of the strength of dependence between observations.\nAmong other results, we show that our bounds recover the standard rates up to a\nfactor of the mixing time for geometrically mixing observation noise.\n", "link": "http://arxiv.org/abs/2505.20017v1", "date": "2025-05-26", "relevancy": 1.8178, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4636}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4509}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linear%20Bandits%20with%20Non-i.i.d.%20Noise&body=Title%3A%20Linear%20Bandits%20with%20Non-i.i.d.%20Noise%0AAuthor%3A%20Baptiste%20Ab%C3%A9l%C3%A8s%20and%20Eugenio%20Clerico%20and%20Hamish%20Flynn%20and%20Gergely%20Neu%0AAbstract%3A%20%20%20We%20study%20the%20linear%20stochastic%20bandit%20problem%2C%20relaxing%20the%20standard%20i.i.d.%0Aassumption%20on%20the%20observation%20noise.%20As%20an%20alternative%20to%20this%20restrictive%0Aassumption%2C%20we%20allow%20the%20noise%20terms%20across%20rounds%20to%20be%20sub-Gaussian%20but%0Ainterdependent%2C%20with%20dependencies%20that%20decay%20over%20time.%20To%20address%20this%0Asetting%2C%20we%20develop%20new%20confidence%20sequences%20using%20a%20recently%20introduced%0Areduction%20scheme%20to%20sequential%20probability%20assignment%2C%20and%20use%20these%20to%20derive%0Aa%20bandit%20algorithm%20based%20on%20the%20principle%20of%20optimism%20in%20the%20face%20of%0Auncertainty.%20We%20provide%20regret%20bounds%20for%20the%20resulting%20algorithm%2C%20expressed%20in%0Aterms%20of%20the%20decay%20rate%20of%20the%20strength%20of%20dependence%20between%20observations.%0AAmong%20other%20results%2C%20we%20show%20that%20our%20bounds%20recover%20the%20standard%20rates%20up%20to%20a%0Afactor%20of%20the%20mixing%20time%20for%20geometrically%20mixing%20observation%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinear%2520Bandits%2520with%2520Non-i.i.d.%2520Noise%26entry.906535625%3DBaptiste%2520Ab%25C3%25A9l%25C3%25A8s%2520and%2520Eugenio%2520Clerico%2520and%2520Hamish%2520Flynn%2520and%2520Gergely%2520Neu%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520linear%2520stochastic%2520bandit%2520problem%252C%2520relaxing%2520the%2520standard%2520i.i.d.%250Aassumption%2520on%2520the%2520observation%2520noise.%2520As%2520an%2520alternative%2520to%2520this%2520restrictive%250Aassumption%252C%2520we%2520allow%2520the%2520noise%2520terms%2520across%2520rounds%2520to%2520be%2520sub-Gaussian%2520but%250Ainterdependent%252C%2520with%2520dependencies%2520that%2520decay%2520over%2520time.%2520To%2520address%2520this%250Asetting%252C%2520we%2520develop%2520new%2520confidence%2520sequences%2520using%2520a%2520recently%2520introduced%250Areduction%2520scheme%2520to%2520sequential%2520probability%2520assignment%252C%2520and%2520use%2520these%2520to%2520derive%250Aa%2520bandit%2520algorithm%2520based%2520on%2520the%2520principle%2520of%2520optimism%2520in%2520the%2520face%2520of%250Auncertainty.%2520We%2520provide%2520regret%2520bounds%2520for%2520the%2520resulting%2520algorithm%252C%2520expressed%2520in%250Aterms%2520of%2520the%2520decay%2520rate%2520of%2520the%2520strength%2520of%2520dependence%2520between%2520observations.%250AAmong%2520other%2520results%252C%2520we%2520show%2520that%2520our%2520bounds%2520recover%2520the%2520standard%2520rates%2520up%2520to%2520a%250Afactor%2520of%2520the%2520mixing%2520time%2520for%2520geometrically%2520mixing%2520observation%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linear%20Bandits%20with%20Non-i.i.d.%20Noise&entry.906535625=Baptiste%20Ab%C3%A9l%C3%A8s%20and%20Eugenio%20Clerico%20and%20Hamish%20Flynn%20and%20Gergely%20Neu&entry.1292438233=%20%20We%20study%20the%20linear%20stochastic%20bandit%20problem%2C%20relaxing%20the%20standard%20i.i.d.%0Aassumption%20on%20the%20observation%20noise.%20As%20an%20alternative%20to%20this%20restrictive%0Aassumption%2C%20we%20allow%20the%20noise%20terms%20across%20rounds%20to%20be%20sub-Gaussian%20but%0Ainterdependent%2C%20with%20dependencies%20that%20decay%20over%20time.%20To%20address%20this%0Asetting%2C%20we%20develop%20new%20confidence%20sequences%20using%20a%20recently%20introduced%0Areduction%20scheme%20to%20sequential%20probability%20assignment%2C%20and%20use%20these%20to%20derive%0Aa%20bandit%20algorithm%20based%20on%20the%20principle%20of%20optimism%20in%20the%20face%20of%0Auncertainty.%20We%20provide%20regret%20bounds%20for%20the%20resulting%20algorithm%2C%20expressed%20in%0Aterms%20of%20the%20decay%20rate%20of%20the%20strength%20of%20dependence%20between%20observations.%0AAmong%20other%20results%2C%20we%20show%20that%20our%20bounds%20recover%20the%20standard%20rates%20up%20to%20a%0Afactor%20of%20the%20mixing%20time%20for%20geometrically%20mixing%20observation%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20017v1&entry.124074799=Read"},
{"title": "Spurious Privacy Leakage in Neural Networks", "author": "Chenxiang Zhang and Jun Pang and Sjouke Mauw", "abstract": "  Neural networks are vulnerable to privacy attacks aimed at stealing sensitive\ndata. The risks can be amplified in a real-world scenario, particularly when\nmodels are trained on limited and biased data. In this work, we investigate the\nimpact of spurious correlation bias on privacy vulnerability. We introduce\n\\emph{spurious privacy leakage}, a phenomenon where spurious groups are\nsignificantly more vulnerable to privacy attacks than non-spurious groups. We\nfurther show that group privacy disparity increases in tasks with simpler\nobjectives (e.g. fewer classes) due to the persistence of spurious features.\nSurprisingly, we find that reducing spurious correlation using spurious robust\nmethods does not mitigate spurious privacy leakage. This leads us to introduce\na perspective on privacy disparity based on memorization, where mitigating\nspurious correlation does not mitigate the memorization of spurious data, and\ntherefore, neither the privacy level. Lastly, we compare the privacy of\ndifferent model architectures trained with spurious data, demonstrating that,\ncontrary to prior works, architectural choice can affect privacy outcomes.\n", "link": "http://arxiv.org/abs/2505.20095v1", "date": "2025-05-26", "relevancy": 1.7394, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4405}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4312}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spurious%20Privacy%20Leakage%20in%20Neural%20Networks&body=Title%3A%20Spurious%20Privacy%20Leakage%20in%20Neural%20Networks%0AAuthor%3A%20Chenxiang%20Zhang%20and%20Jun%20Pang%20and%20Sjouke%20Mauw%0AAbstract%3A%20%20%20Neural%20networks%20are%20vulnerable%20to%20privacy%20attacks%20aimed%20at%20stealing%20sensitive%0Adata.%20The%20risks%20can%20be%20amplified%20in%20a%20real-world%20scenario%2C%20particularly%20when%0Amodels%20are%20trained%20on%20limited%20and%20biased%20data.%20In%20this%20work%2C%20we%20investigate%20the%0Aimpact%20of%20spurious%20correlation%20bias%20on%20privacy%20vulnerability.%20We%20introduce%0A%5Cemph%7Bspurious%20privacy%20leakage%7D%2C%20a%20phenomenon%20where%20spurious%20groups%20are%0Asignificantly%20more%20vulnerable%20to%20privacy%20attacks%20than%20non-spurious%20groups.%20We%0Afurther%20show%20that%20group%20privacy%20disparity%20increases%20in%20tasks%20with%20simpler%0Aobjectives%20%28e.g.%20fewer%20classes%29%20due%20to%20the%20persistence%20of%20spurious%20features.%0ASurprisingly%2C%20we%20find%20that%20reducing%20spurious%20correlation%20using%20spurious%20robust%0Amethods%20does%20not%20mitigate%20spurious%20privacy%20leakage.%20This%20leads%20us%20to%20introduce%0Aa%20perspective%20on%20privacy%20disparity%20based%20on%20memorization%2C%20where%20mitigating%0Aspurious%20correlation%20does%20not%20mitigate%20the%20memorization%20of%20spurious%20data%2C%20and%0Atherefore%2C%20neither%20the%20privacy%20level.%20Lastly%2C%20we%20compare%20the%20privacy%20of%0Adifferent%20model%20architectures%20trained%20with%20spurious%20data%2C%20demonstrating%20that%2C%0Acontrary%20to%20prior%20works%2C%20architectural%20choice%20can%20affect%20privacy%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpurious%2520Privacy%2520Leakage%2520in%2520Neural%2520Networks%26entry.906535625%3DChenxiang%2520Zhang%2520and%2520Jun%2520Pang%2520and%2520Sjouke%2520Mauw%26entry.1292438233%3D%2520%2520Neural%2520networks%2520are%2520vulnerable%2520to%2520privacy%2520attacks%2520aimed%2520at%2520stealing%2520sensitive%250Adata.%2520The%2520risks%2520can%2520be%2520amplified%2520in%2520a%2520real-world%2520scenario%252C%2520particularly%2520when%250Amodels%2520are%2520trained%2520on%2520limited%2520and%2520biased%2520data.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%250Aimpact%2520of%2520spurious%2520correlation%2520bias%2520on%2520privacy%2520vulnerability.%2520We%2520introduce%250A%255Cemph%257Bspurious%2520privacy%2520leakage%257D%252C%2520a%2520phenomenon%2520where%2520spurious%2520groups%2520are%250Asignificantly%2520more%2520vulnerable%2520to%2520privacy%2520attacks%2520than%2520non-spurious%2520groups.%2520We%250Afurther%2520show%2520that%2520group%2520privacy%2520disparity%2520increases%2520in%2520tasks%2520with%2520simpler%250Aobjectives%2520%2528e.g.%2520fewer%2520classes%2529%2520due%2520to%2520the%2520persistence%2520of%2520spurious%2520features.%250ASurprisingly%252C%2520we%2520find%2520that%2520reducing%2520spurious%2520correlation%2520using%2520spurious%2520robust%250Amethods%2520does%2520not%2520mitigate%2520spurious%2520privacy%2520leakage.%2520This%2520leads%2520us%2520to%2520introduce%250Aa%2520perspective%2520on%2520privacy%2520disparity%2520based%2520on%2520memorization%252C%2520where%2520mitigating%250Aspurious%2520correlation%2520does%2520not%2520mitigate%2520the%2520memorization%2520of%2520spurious%2520data%252C%2520and%250Atherefore%252C%2520neither%2520the%2520privacy%2520level.%2520Lastly%252C%2520we%2520compare%2520the%2520privacy%2520of%250Adifferent%2520model%2520architectures%2520trained%2520with%2520spurious%2520data%252C%2520demonstrating%2520that%252C%250Acontrary%2520to%2520prior%2520works%252C%2520architectural%2520choice%2520can%2520affect%2520privacy%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spurious%20Privacy%20Leakage%20in%20Neural%20Networks&entry.906535625=Chenxiang%20Zhang%20and%20Jun%20Pang%20and%20Sjouke%20Mauw&entry.1292438233=%20%20Neural%20networks%20are%20vulnerable%20to%20privacy%20attacks%20aimed%20at%20stealing%20sensitive%0Adata.%20The%20risks%20can%20be%20amplified%20in%20a%20real-world%20scenario%2C%20particularly%20when%0Amodels%20are%20trained%20on%20limited%20and%20biased%20data.%20In%20this%20work%2C%20we%20investigate%20the%0Aimpact%20of%20spurious%20correlation%20bias%20on%20privacy%20vulnerability.%20We%20introduce%0A%5Cemph%7Bspurious%20privacy%20leakage%7D%2C%20a%20phenomenon%20where%20spurious%20groups%20are%0Asignificantly%20more%20vulnerable%20to%20privacy%20attacks%20than%20non-spurious%20groups.%20We%0Afurther%20show%20that%20group%20privacy%20disparity%20increases%20in%20tasks%20with%20simpler%0Aobjectives%20%28e.g.%20fewer%20classes%29%20due%20to%20the%20persistence%20of%20spurious%20features.%0ASurprisingly%2C%20we%20find%20that%20reducing%20spurious%20correlation%20using%20spurious%20robust%0Amethods%20does%20not%20mitigate%20spurious%20privacy%20leakage.%20This%20leads%20us%20to%20introduce%0Aa%20perspective%20on%20privacy%20disparity%20based%20on%20memorization%2C%20where%20mitigating%0Aspurious%20correlation%20does%20not%20mitigate%20the%20memorization%20of%20spurious%20data%2C%20and%0Atherefore%2C%20neither%20the%20privacy%20level.%20Lastly%2C%20we%20compare%20the%20privacy%20of%0Adifferent%20model%20architectures%20trained%20with%20spurious%20data%2C%20demonstrating%20that%2C%0Acontrary%20to%20prior%20works%2C%20architectural%20choice%20can%20affect%20privacy%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20095v1&entry.124074799=Read"},
{"title": "Embracing Imperfection: Simulating Students with Diverse Cognitive\n  Levels Using LLM-based Agents", "author": "Tao Wu and Jingyuan Chen and Wang Lin and Mengze Li and Yumeng Zhu and Ang Li and Kun Kuang and Fei Wu", "abstract": "  Large language models (LLMs) are revolutionizing education, with LLM-based\nagents playing a key role in simulating student behavior. A major challenge in\nstudent simulation is modeling the diverse learning patterns of students at\nvarious cognitive levels. However, current LLMs, typically trained as ``helpful\nassistants'', target at generating perfect responses. As a result, they\nstruggle to simulate students with diverse cognitive abilities, as they often\nproduce overly advanced answers, missing the natural imperfections that\ncharacterize student learning and resulting in unrealistic simulations. To\naddress this issue, we propose a training-free framework for student\nsimulation. We begin by constructing a cognitive prototype for each student\nusing a knowledge graph, which captures their understanding of concepts from\npast learning records. This prototype is then mapped to new tasks to predict\nstudent performance. Next, we simulate student solutions based on these\npredictions and iteratively refine them using a beam search method to better\nreplicate realistic mistakes. To validate our approach, we construct the\n\\texttt{Student\\_100} dataset, consisting of $100$ students working on Python\nprogramming and $5,000$ learning records. Experimental results show that our\nmethod consistently outperforms baseline models, achieving $100\\%$ improvement\nin simulation accuracy.\n", "link": "http://arxiv.org/abs/2505.19997v1", "date": "2025-05-26", "relevancy": 2.0629, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5263}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5178}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embracing%20Imperfection%3A%20Simulating%20Students%20with%20Diverse%20Cognitive%0A%20%20Levels%20Using%20LLM-based%20Agents&body=Title%3A%20Embracing%20Imperfection%3A%20Simulating%20Students%20with%20Diverse%20Cognitive%0A%20%20Levels%20Using%20LLM-based%20Agents%0AAuthor%3A%20Tao%20Wu%20and%20Jingyuan%20Chen%20and%20Wang%20Lin%20and%20Mengze%20Li%20and%20Yumeng%20Zhu%20and%20Ang%20Li%20and%20Kun%20Kuang%20and%20Fei%20Wu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20revolutionizing%20education%2C%20with%20LLM-based%0Aagents%20playing%20a%20key%20role%20in%20simulating%20student%20behavior.%20A%20major%20challenge%20in%0Astudent%20simulation%20is%20modeling%20the%20diverse%20learning%20patterns%20of%20students%20at%0Avarious%20cognitive%20levels.%20However%2C%20current%20LLMs%2C%20typically%20trained%20as%20%60%60helpful%0Aassistants%27%27%2C%20target%20at%20generating%20perfect%20responses.%20As%20a%20result%2C%20they%0Astruggle%20to%20simulate%20students%20with%20diverse%20cognitive%20abilities%2C%20as%20they%20often%0Aproduce%20overly%20advanced%20answers%2C%20missing%20the%20natural%20imperfections%20that%0Acharacterize%20student%20learning%20and%20resulting%20in%20unrealistic%20simulations.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20training-free%20framework%20for%20student%0Asimulation.%20We%20begin%20by%20constructing%20a%20cognitive%20prototype%20for%20each%20student%0Ausing%20a%20knowledge%20graph%2C%20which%20captures%20their%20understanding%20of%20concepts%20from%0Apast%20learning%20records.%20This%20prototype%20is%20then%20mapped%20to%20new%20tasks%20to%20predict%0Astudent%20performance.%20Next%2C%20we%20simulate%20student%20solutions%20based%20on%20these%0Apredictions%20and%20iteratively%20refine%20them%20using%20a%20beam%20search%20method%20to%20better%0Areplicate%20realistic%20mistakes.%20To%20validate%20our%20approach%2C%20we%20construct%20the%0A%5Ctexttt%7BStudent%5C_100%7D%20dataset%2C%20consisting%20of%20%24100%24%20students%20working%20on%20Python%0Aprogramming%20and%20%245%2C000%24%20learning%20records.%20Experimental%20results%20show%20that%20our%0Amethod%20consistently%20outperforms%20baseline%20models%2C%20achieving%20%24100%5C%25%24%20improvement%0Ain%20simulation%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbracing%2520Imperfection%253A%2520Simulating%2520Students%2520with%2520Diverse%2520Cognitive%250A%2520%2520Levels%2520Using%2520LLM-based%2520Agents%26entry.906535625%3DTao%2520Wu%2520and%2520Jingyuan%2520Chen%2520and%2520Wang%2520Lin%2520and%2520Mengze%2520Li%2520and%2520Yumeng%2520Zhu%2520and%2520Ang%2520Li%2520and%2520Kun%2520Kuang%2520and%2520Fei%2520Wu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520revolutionizing%2520education%252C%2520with%2520LLM-based%250Aagents%2520playing%2520a%2520key%2520role%2520in%2520simulating%2520student%2520behavior.%2520A%2520major%2520challenge%2520in%250Astudent%2520simulation%2520is%2520modeling%2520the%2520diverse%2520learning%2520patterns%2520of%2520students%2520at%250Avarious%2520cognitive%2520levels.%2520However%252C%2520current%2520LLMs%252C%2520typically%2520trained%2520as%2520%2560%2560helpful%250Aassistants%2527%2527%252C%2520target%2520at%2520generating%2520perfect%2520responses.%2520As%2520a%2520result%252C%2520they%250Astruggle%2520to%2520simulate%2520students%2520with%2520diverse%2520cognitive%2520abilities%252C%2520as%2520they%2520often%250Aproduce%2520overly%2520advanced%2520answers%252C%2520missing%2520the%2520natural%2520imperfections%2520that%250Acharacterize%2520student%2520learning%2520and%2520resulting%2520in%2520unrealistic%2520simulations.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520a%2520training-free%2520framework%2520for%2520student%250Asimulation.%2520We%2520begin%2520by%2520constructing%2520a%2520cognitive%2520prototype%2520for%2520each%2520student%250Ausing%2520a%2520knowledge%2520graph%252C%2520which%2520captures%2520their%2520understanding%2520of%2520concepts%2520from%250Apast%2520learning%2520records.%2520This%2520prototype%2520is%2520then%2520mapped%2520to%2520new%2520tasks%2520to%2520predict%250Astudent%2520performance.%2520Next%252C%2520we%2520simulate%2520student%2520solutions%2520based%2520on%2520these%250Apredictions%2520and%2520iteratively%2520refine%2520them%2520using%2520a%2520beam%2520search%2520method%2520to%2520better%250Areplicate%2520realistic%2520mistakes.%2520To%2520validate%2520our%2520approach%252C%2520we%2520construct%2520the%250A%255Ctexttt%257BStudent%255C_100%257D%2520dataset%252C%2520consisting%2520of%2520%2524100%2524%2520students%2520working%2520on%2520Python%250Aprogramming%2520and%2520%25245%252C000%2524%2520learning%2520records.%2520Experimental%2520results%2520show%2520that%2520our%250Amethod%2520consistently%2520outperforms%2520baseline%2520models%252C%2520achieving%2520%2524100%255C%2525%2524%2520improvement%250Ain%2520simulation%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embracing%20Imperfection%3A%20Simulating%20Students%20with%20Diverse%20Cognitive%0A%20%20Levels%20Using%20LLM-based%20Agents&entry.906535625=Tao%20Wu%20and%20Jingyuan%20Chen%20and%20Wang%20Lin%20and%20Mengze%20Li%20and%20Yumeng%20Zhu%20and%20Ang%20Li%20and%20Kun%20Kuang%20and%20Fei%20Wu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20revolutionizing%20education%2C%20with%20LLM-based%0Aagents%20playing%20a%20key%20role%20in%20simulating%20student%20behavior.%20A%20major%20challenge%20in%0Astudent%20simulation%20is%20modeling%20the%20diverse%20learning%20patterns%20of%20students%20at%0Avarious%20cognitive%20levels.%20However%2C%20current%20LLMs%2C%20typically%20trained%20as%20%60%60helpful%0Aassistants%27%27%2C%20target%20at%20generating%20perfect%20responses.%20As%20a%20result%2C%20they%0Astruggle%20to%20simulate%20students%20with%20diverse%20cognitive%20abilities%2C%20as%20they%20often%0Aproduce%20overly%20advanced%20answers%2C%20missing%20the%20natural%20imperfections%20that%0Acharacterize%20student%20learning%20and%20resulting%20in%20unrealistic%20simulations.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20training-free%20framework%20for%20student%0Asimulation.%20We%20begin%20by%20constructing%20a%20cognitive%20prototype%20for%20each%20student%0Ausing%20a%20knowledge%20graph%2C%20which%20captures%20their%20understanding%20of%20concepts%20from%0Apast%20learning%20records.%20This%20prototype%20is%20then%20mapped%20to%20new%20tasks%20to%20predict%0Astudent%20performance.%20Next%2C%20we%20simulate%20student%20solutions%20based%20on%20these%0Apredictions%20and%20iteratively%20refine%20them%20using%20a%20beam%20search%20method%20to%20better%0Areplicate%20realistic%20mistakes.%20To%20validate%20our%20approach%2C%20we%20construct%20the%0A%5Ctexttt%7BStudent%5C_100%7D%20dataset%2C%20consisting%20of%20%24100%24%20students%20working%20on%20Python%0Aprogramming%20and%20%245%2C000%24%20learning%20records.%20Experimental%20results%20show%20that%20our%0Amethod%20consistently%20outperforms%20baseline%20models%2C%20achieving%20%24100%5C%25%24%20improvement%0Ain%20simulation%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19997v1&entry.124074799=Read"},
{"title": "A Cognitive Writing Perspective for Constrained Long-Form Text\n  Generation", "author": "Kaiyang Wan and Honglin Mu and Rui Hao and Haoran Luo and Tianle Gu and Xiuying Chen", "abstract": "  Like humans, Large Language Models (LLMs) struggle to generate high-quality\nlong-form text that adheres to strict requirements in a single pass. This\nchallenge is unsurprising, as successful human writing, according to the\nCognitive Writing Theory, is a complex cognitive process involving iterative\nplanning, translating, reviewing, and monitoring. Motivated by these cognitive\nprinciples, we aim to equip LLMs with human-like cognitive writing capabilities\nthrough CogWriter, a novel training-free framework that transforms LLM\nconstrained long-form text generation into a systematic cognitive writing\nparadigm. Our framework consists of two key modules: (1) a Planning Agent that\nperforms hierarchical planning to decompose the task, and (2) multiple\nGeneration Agents that execute these plans in parallel. The system maintains\nquality via continuous monitoring and reviewing mechanisms, which evaluate\noutputs against specified requirements and trigger necessary revisions.\nCogWriter demonstrates exceptional performance on LongGenBench, a benchmark for\ncomplex constrained long-form text generation. Even when using Qwen-2.5-14B as\nits backbone, CogWriter surpasses GPT-4o by 22% in complex instruction\ncompletion accuracy while reliably generating texts exceeding 10,000 words. We\nhope this cognitive science-inspired approach provides a paradigm for LLM\nwriting advancements:\n\\href{https://github.com/KaiyangWan/CogWriter}{CogWriter}.\n", "link": "http://arxiv.org/abs/2502.12568v3", "date": "2025-05-26", "relevancy": 2.0877, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5371}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5122}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Cognitive%20Writing%20Perspective%20for%20Constrained%20Long-Form%20Text%0A%20%20Generation&body=Title%3A%20A%20Cognitive%20Writing%20Perspective%20for%20Constrained%20Long-Form%20Text%0A%20%20Generation%0AAuthor%3A%20Kaiyang%20Wan%20and%20Honglin%20Mu%20and%20Rui%20Hao%20and%20Haoran%20Luo%20and%20Tianle%20Gu%20and%20Xiuying%20Chen%0AAbstract%3A%20%20%20Like%20humans%2C%20Large%20Language%20Models%20%28LLMs%29%20struggle%20to%20generate%20high-quality%0Along-form%20text%20that%20adheres%20to%20strict%20requirements%20in%20a%20single%20pass.%20This%0Achallenge%20is%20unsurprising%2C%20as%20successful%20human%20writing%2C%20according%20to%20the%0ACognitive%20Writing%20Theory%2C%20is%20a%20complex%20cognitive%20process%20involving%20iterative%0Aplanning%2C%20translating%2C%20reviewing%2C%20and%20monitoring.%20Motivated%20by%20these%20cognitive%0Aprinciples%2C%20we%20aim%20to%20equip%20LLMs%20with%20human-like%20cognitive%20writing%20capabilities%0Athrough%20CogWriter%2C%20a%20novel%20training-free%20framework%20that%20transforms%20LLM%0Aconstrained%20long-form%20text%20generation%20into%20a%20systematic%20cognitive%20writing%0Aparadigm.%20Our%20framework%20consists%20of%20two%20key%20modules%3A%20%281%29%20a%20Planning%20Agent%20that%0Aperforms%20hierarchical%20planning%20to%20decompose%20the%20task%2C%20and%20%282%29%20multiple%0AGeneration%20Agents%20that%20execute%20these%20plans%20in%20parallel.%20The%20system%20maintains%0Aquality%20via%20continuous%20monitoring%20and%20reviewing%20mechanisms%2C%20which%20evaluate%0Aoutputs%20against%20specified%20requirements%20and%20trigger%20necessary%20revisions.%0ACogWriter%20demonstrates%20exceptional%20performance%20on%20LongGenBench%2C%20a%20benchmark%20for%0Acomplex%20constrained%20long-form%20text%20generation.%20Even%20when%20using%20Qwen-2.5-14B%20as%0Aits%20backbone%2C%20CogWriter%20surpasses%20GPT-4o%20by%2022%25%20in%20complex%20instruction%0Acompletion%20accuracy%20while%20reliably%20generating%20texts%20exceeding%2010%2C000%20words.%20We%0Ahope%20this%20cognitive%20science-inspired%20approach%20provides%20a%20paradigm%20for%20LLM%0Awriting%20advancements%3A%0A%5Chref%7Bhttps%3A//github.com/KaiyangWan/CogWriter%7D%7BCogWriter%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12568v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Cognitive%2520Writing%2520Perspective%2520for%2520Constrained%2520Long-Form%2520Text%250A%2520%2520Generation%26entry.906535625%3DKaiyang%2520Wan%2520and%2520Honglin%2520Mu%2520and%2520Rui%2520Hao%2520and%2520Haoran%2520Luo%2520and%2520Tianle%2520Gu%2520and%2520Xiuying%2520Chen%26entry.1292438233%3D%2520%2520Like%2520humans%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520struggle%2520to%2520generate%2520high-quality%250Along-form%2520text%2520that%2520adheres%2520to%2520strict%2520requirements%2520in%2520a%2520single%2520pass.%2520This%250Achallenge%2520is%2520unsurprising%252C%2520as%2520successful%2520human%2520writing%252C%2520according%2520to%2520the%250ACognitive%2520Writing%2520Theory%252C%2520is%2520a%2520complex%2520cognitive%2520process%2520involving%2520iterative%250Aplanning%252C%2520translating%252C%2520reviewing%252C%2520and%2520monitoring.%2520Motivated%2520by%2520these%2520cognitive%250Aprinciples%252C%2520we%2520aim%2520to%2520equip%2520LLMs%2520with%2520human-like%2520cognitive%2520writing%2520capabilities%250Athrough%2520CogWriter%252C%2520a%2520novel%2520training-free%2520framework%2520that%2520transforms%2520LLM%250Aconstrained%2520long-form%2520text%2520generation%2520into%2520a%2520systematic%2520cognitive%2520writing%250Aparadigm.%2520Our%2520framework%2520consists%2520of%2520two%2520key%2520modules%253A%2520%25281%2529%2520a%2520Planning%2520Agent%2520that%250Aperforms%2520hierarchical%2520planning%2520to%2520decompose%2520the%2520task%252C%2520and%2520%25282%2529%2520multiple%250AGeneration%2520Agents%2520that%2520execute%2520these%2520plans%2520in%2520parallel.%2520The%2520system%2520maintains%250Aquality%2520via%2520continuous%2520monitoring%2520and%2520reviewing%2520mechanisms%252C%2520which%2520evaluate%250Aoutputs%2520against%2520specified%2520requirements%2520and%2520trigger%2520necessary%2520revisions.%250ACogWriter%2520demonstrates%2520exceptional%2520performance%2520on%2520LongGenBench%252C%2520a%2520benchmark%2520for%250Acomplex%2520constrained%2520long-form%2520text%2520generation.%2520Even%2520when%2520using%2520Qwen-2.5-14B%2520as%250Aits%2520backbone%252C%2520CogWriter%2520surpasses%2520GPT-4o%2520by%252022%2525%2520in%2520complex%2520instruction%250Acompletion%2520accuracy%2520while%2520reliably%2520generating%2520texts%2520exceeding%252010%252C000%2520words.%2520We%250Ahope%2520this%2520cognitive%2520science-inspired%2520approach%2520provides%2520a%2520paradigm%2520for%2520LLM%250Awriting%2520advancements%253A%250A%255Chref%257Bhttps%253A//github.com/KaiyangWan/CogWriter%257D%257BCogWriter%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12568v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Cognitive%20Writing%20Perspective%20for%20Constrained%20Long-Form%20Text%0A%20%20Generation&entry.906535625=Kaiyang%20Wan%20and%20Honglin%20Mu%20and%20Rui%20Hao%20and%20Haoran%20Luo%20and%20Tianle%20Gu%20and%20Xiuying%20Chen&entry.1292438233=%20%20Like%20humans%2C%20Large%20Language%20Models%20%28LLMs%29%20struggle%20to%20generate%20high-quality%0Along-form%20text%20that%20adheres%20to%20strict%20requirements%20in%20a%20single%20pass.%20This%0Achallenge%20is%20unsurprising%2C%20as%20successful%20human%20writing%2C%20according%20to%20the%0ACognitive%20Writing%20Theory%2C%20is%20a%20complex%20cognitive%20process%20involving%20iterative%0Aplanning%2C%20translating%2C%20reviewing%2C%20and%20monitoring.%20Motivated%20by%20these%20cognitive%0Aprinciples%2C%20we%20aim%20to%20equip%20LLMs%20with%20human-like%20cognitive%20writing%20capabilities%0Athrough%20CogWriter%2C%20a%20novel%20training-free%20framework%20that%20transforms%20LLM%0Aconstrained%20long-form%20text%20generation%20into%20a%20systematic%20cognitive%20writing%0Aparadigm.%20Our%20framework%20consists%20of%20two%20key%20modules%3A%20%281%29%20a%20Planning%20Agent%20that%0Aperforms%20hierarchical%20planning%20to%20decompose%20the%20task%2C%20and%20%282%29%20multiple%0AGeneration%20Agents%20that%20execute%20these%20plans%20in%20parallel.%20The%20system%20maintains%0Aquality%20via%20continuous%20monitoring%20and%20reviewing%20mechanisms%2C%20which%20evaluate%0Aoutputs%20against%20specified%20requirements%20and%20trigger%20necessary%20revisions.%0ACogWriter%20demonstrates%20exceptional%20performance%20on%20LongGenBench%2C%20a%20benchmark%20for%0Acomplex%20constrained%20long-form%20text%20generation.%20Even%20when%20using%20Qwen-2.5-14B%20as%0Aits%20backbone%2C%20CogWriter%20surpasses%20GPT-4o%20by%2022%25%20in%20complex%20instruction%0Acompletion%20accuracy%20while%20reliably%20generating%20texts%20exceeding%2010%2C000%20words.%20We%0Ahope%20this%20cognitive%20science-inspired%20approach%20provides%20a%20paradigm%20for%20LLM%0Awriting%20advancements%3A%0A%5Chref%7Bhttps%3A//github.com/KaiyangWan/CogWriter%7D%7BCogWriter%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12568v3&entry.124074799=Read"},
{"title": "Proxy-Free GFlowNet", "author": "Ruishuo Chen and Xun Wang and Rui Hu and Zhuoran Li and Longbo Huang", "abstract": "  Generative Flow Networks (GFlowNets) are a promising class of generative\nmodels designed to sample diverse, high-reward structures by modeling\ndistributions over compositional objects. In many real-world applications,\nobtaining the reward function for such objects is expensive, time-consuming, or\nrequires human input, making it necessary to train GFlowNets from historical\ndatasets. Most existing methods adopt a model-based approach, learning a proxy\nmodel from the dataset to approximate the reward function. However, this\nstrategy inherently ties the quality of the learned policy to the accuracy of\nthe proxy, introducing additional complexity and uncertainty into the training\nprocess. To overcome these limitations, we propose \\textbf{Trajectory-Distilled\nGFlowNet (TD-GFN)}, a \\emph{proxy-free} training framework that eliminates the\nneed for out-of-dataset reward queries. Our method is motivated by the key\nobservation that different edges in the associated directed acyclic graph (DAG)\ncontribute unequally to effective policy learning. TD-GFN leverages inverse\nreinforcement learning to estimate edge-level rewards from the offline dataset,\nwhich are then used to ingeniously prune the DAG and guide backward trajectory\nsampling during training. This approach directs the policy toward high-reward\nregions while reducing the complexity of model fitting. Empirical results\nacross multiple tasks show that TD-GFN trains both efficiently and reliably,\nsignificantly outperforming existing baselines in convergence speed and sample\nquality.\n", "link": "http://arxiv.org/abs/2505.20110v1", "date": "2025-05-26", "relevancy": 2.1066, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5657}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5211}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proxy-Free%20GFlowNet&body=Title%3A%20Proxy-Free%20GFlowNet%0AAuthor%3A%20Ruishuo%20Chen%20and%20Xun%20Wang%20and%20Rui%20Hu%20and%20Zhuoran%20Li%20and%20Longbo%20Huang%0AAbstract%3A%20%20%20Generative%20Flow%20Networks%20%28GFlowNets%29%20are%20a%20promising%20class%20of%20generative%0Amodels%20designed%20to%20sample%20diverse%2C%20high-reward%20structures%20by%20modeling%0Adistributions%20over%20compositional%20objects.%20In%20many%20real-world%20applications%2C%0Aobtaining%20the%20reward%20function%20for%20such%20objects%20is%20expensive%2C%20time-consuming%2C%20or%0Arequires%20human%20input%2C%20making%20it%20necessary%20to%20train%20GFlowNets%20from%20historical%0Adatasets.%20Most%20existing%20methods%20adopt%20a%20model-based%20approach%2C%20learning%20a%20proxy%0Amodel%20from%20the%20dataset%20to%20approximate%20the%20reward%20function.%20However%2C%20this%0Astrategy%20inherently%20ties%20the%20quality%20of%20the%20learned%20policy%20to%20the%20accuracy%20of%0Athe%20proxy%2C%20introducing%20additional%20complexity%20and%20uncertainty%20into%20the%20training%0Aprocess.%20To%20overcome%20these%20limitations%2C%20we%20propose%20%5Ctextbf%7BTrajectory-Distilled%0AGFlowNet%20%28TD-GFN%29%7D%2C%20a%20%5Cemph%7Bproxy-free%7D%20training%20framework%20that%20eliminates%20the%0Aneed%20for%20out-of-dataset%20reward%20queries.%20Our%20method%20is%20motivated%20by%20the%20key%0Aobservation%20that%20different%20edges%20in%20the%20associated%20directed%20acyclic%20graph%20%28DAG%29%0Acontribute%20unequally%20to%20effective%20policy%20learning.%20TD-GFN%20leverages%20inverse%0Areinforcement%20learning%20to%20estimate%20edge-level%20rewards%20from%20the%20offline%20dataset%2C%0Awhich%20are%20then%20used%20to%20ingeniously%20prune%20the%20DAG%20and%20guide%20backward%20trajectory%0Asampling%20during%20training.%20This%20approach%20directs%20the%20policy%20toward%20high-reward%0Aregions%20while%20reducing%20the%20complexity%20of%20model%20fitting.%20Empirical%20results%0Aacross%20multiple%20tasks%20show%20that%20TD-GFN%20trains%20both%20efficiently%20and%20reliably%2C%0Asignificantly%20outperforming%20existing%20baselines%20in%20convergence%20speed%20and%20sample%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20110v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProxy-Free%2520GFlowNet%26entry.906535625%3DRuishuo%2520Chen%2520and%2520Xun%2520Wang%2520and%2520Rui%2520Hu%2520and%2520Zhuoran%2520Li%2520and%2520Longbo%2520Huang%26entry.1292438233%3D%2520%2520Generative%2520Flow%2520Networks%2520%2528GFlowNets%2529%2520are%2520a%2520promising%2520class%2520of%2520generative%250Amodels%2520designed%2520to%2520sample%2520diverse%252C%2520high-reward%2520structures%2520by%2520modeling%250Adistributions%2520over%2520compositional%2520objects.%2520In%2520many%2520real-world%2520applications%252C%250Aobtaining%2520the%2520reward%2520function%2520for%2520such%2520objects%2520is%2520expensive%252C%2520time-consuming%252C%2520or%250Arequires%2520human%2520input%252C%2520making%2520it%2520necessary%2520to%2520train%2520GFlowNets%2520from%2520historical%250Adatasets.%2520Most%2520existing%2520methods%2520adopt%2520a%2520model-based%2520approach%252C%2520learning%2520a%2520proxy%250Amodel%2520from%2520the%2520dataset%2520to%2520approximate%2520the%2520reward%2520function.%2520However%252C%2520this%250Astrategy%2520inherently%2520ties%2520the%2520quality%2520of%2520the%2520learned%2520policy%2520to%2520the%2520accuracy%2520of%250Athe%2520proxy%252C%2520introducing%2520additional%2520complexity%2520and%2520uncertainty%2520into%2520the%2520training%250Aprocess.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520%255Ctextbf%257BTrajectory-Distilled%250AGFlowNet%2520%2528TD-GFN%2529%257D%252C%2520a%2520%255Cemph%257Bproxy-free%257D%2520training%2520framework%2520that%2520eliminates%2520the%250Aneed%2520for%2520out-of-dataset%2520reward%2520queries.%2520Our%2520method%2520is%2520motivated%2520by%2520the%2520key%250Aobservation%2520that%2520different%2520edges%2520in%2520the%2520associated%2520directed%2520acyclic%2520graph%2520%2528DAG%2529%250Acontribute%2520unequally%2520to%2520effective%2520policy%2520learning.%2520TD-GFN%2520leverages%2520inverse%250Areinforcement%2520learning%2520to%2520estimate%2520edge-level%2520rewards%2520from%2520the%2520offline%2520dataset%252C%250Awhich%2520are%2520then%2520used%2520to%2520ingeniously%2520prune%2520the%2520DAG%2520and%2520guide%2520backward%2520trajectory%250Asampling%2520during%2520training.%2520This%2520approach%2520directs%2520the%2520policy%2520toward%2520high-reward%250Aregions%2520while%2520reducing%2520the%2520complexity%2520of%2520model%2520fitting.%2520Empirical%2520results%250Aacross%2520multiple%2520tasks%2520show%2520that%2520TD-GFN%2520trains%2520both%2520efficiently%2520and%2520reliably%252C%250Asignificantly%2520outperforming%2520existing%2520baselines%2520in%2520convergence%2520speed%2520and%2520sample%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20110v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proxy-Free%20GFlowNet&entry.906535625=Ruishuo%20Chen%20and%20Xun%20Wang%20and%20Rui%20Hu%20and%20Zhuoran%20Li%20and%20Longbo%20Huang&entry.1292438233=%20%20Generative%20Flow%20Networks%20%28GFlowNets%29%20are%20a%20promising%20class%20of%20generative%0Amodels%20designed%20to%20sample%20diverse%2C%20high-reward%20structures%20by%20modeling%0Adistributions%20over%20compositional%20objects.%20In%20many%20real-world%20applications%2C%0Aobtaining%20the%20reward%20function%20for%20such%20objects%20is%20expensive%2C%20time-consuming%2C%20or%0Arequires%20human%20input%2C%20making%20it%20necessary%20to%20train%20GFlowNets%20from%20historical%0Adatasets.%20Most%20existing%20methods%20adopt%20a%20model-based%20approach%2C%20learning%20a%20proxy%0Amodel%20from%20the%20dataset%20to%20approximate%20the%20reward%20function.%20However%2C%20this%0Astrategy%20inherently%20ties%20the%20quality%20of%20the%20learned%20policy%20to%20the%20accuracy%20of%0Athe%20proxy%2C%20introducing%20additional%20complexity%20and%20uncertainty%20into%20the%20training%0Aprocess.%20To%20overcome%20these%20limitations%2C%20we%20propose%20%5Ctextbf%7BTrajectory-Distilled%0AGFlowNet%20%28TD-GFN%29%7D%2C%20a%20%5Cemph%7Bproxy-free%7D%20training%20framework%20that%20eliminates%20the%0Aneed%20for%20out-of-dataset%20reward%20queries.%20Our%20method%20is%20motivated%20by%20the%20key%0Aobservation%20that%20different%20edges%20in%20the%20associated%20directed%20acyclic%20graph%20%28DAG%29%0Acontribute%20unequally%20to%20effective%20policy%20learning.%20TD-GFN%20leverages%20inverse%0Areinforcement%20learning%20to%20estimate%20edge-level%20rewards%20from%20the%20offline%20dataset%2C%0Awhich%20are%20then%20used%20to%20ingeniously%20prune%20the%20DAG%20and%20guide%20backward%20trajectory%0Asampling%20during%20training.%20This%20approach%20directs%20the%20policy%20toward%20high-reward%0Aregions%20while%20reducing%20the%20complexity%20of%20model%20fitting.%20Empirical%20results%0Aacross%20multiple%20tasks%20show%20that%20TD-GFN%20trains%20both%20efficiently%20and%20reliably%2C%0Asignificantly%20outperforming%20existing%20baselines%20in%20convergence%20speed%20and%20sample%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20110v1&entry.124074799=Read"},
{"title": "Fast Differentiable Modal Simulation of Non-linear Strings, Membranes,\n  and Plates", "author": "Rodrigo Diaz and Mark Sandler", "abstract": "  Modal methods for simulating vibrations of strings, membranes, and plates are\nwidely used in acoustics and physically informed audio synthesis. However,\ntraditional implementations, particularly for non-linear models like the von\nK\\'arm\\'an plate, are computationally demanding and lack differentiability,\nlimiting inverse modelling and real-time applications. We introduce a fast,\ndifferentiable, GPU-accelerated modal framework built with the JAX library,\nproviding efficient simulations and enabling gradient-based inverse modelling.\nBenchmarks show that our approach significantly outperforms CPU and GPU-based\nimplementations, particularly for simulations with many modes. Inverse\nmodelling experiments demonstrate that our approach can recover physical\nparameters, including tension, stiffness, and geometry, from both synthetic and\nexperimental data. Although fitting physical parameters is more sensitive to\ninitialisation compared to other methods, it provides greater interpretability\nand more compact parameterisation. The code is released as open source to\nsupport future research and applications in differentiable physical modelling\nand sound synthesis.\n", "link": "http://arxiv.org/abs/2505.05940v2", "date": "2025-05-26", "relevancy": 1.9695, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4984}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4897}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Differentiable%20Modal%20Simulation%20of%20Non-linear%20Strings%2C%20Membranes%2C%0A%20%20and%20Plates&body=Title%3A%20Fast%20Differentiable%20Modal%20Simulation%20of%20Non-linear%20Strings%2C%20Membranes%2C%0A%20%20and%20Plates%0AAuthor%3A%20Rodrigo%20Diaz%20and%20Mark%20Sandler%0AAbstract%3A%20%20%20Modal%20methods%20for%20simulating%20vibrations%20of%20strings%2C%20membranes%2C%20and%20plates%20are%0Awidely%20used%20in%20acoustics%20and%20physically%20informed%20audio%20synthesis.%20However%2C%0Atraditional%20implementations%2C%20particularly%20for%20non-linear%20models%20like%20the%20von%0AK%5C%27arm%5C%27an%20plate%2C%20are%20computationally%20demanding%20and%20lack%20differentiability%2C%0Alimiting%20inverse%20modelling%20and%20real-time%20applications.%20We%20introduce%20a%20fast%2C%0Adifferentiable%2C%20GPU-accelerated%20modal%20framework%20built%20with%20the%20JAX%20library%2C%0Aproviding%20efficient%20simulations%20and%20enabling%20gradient-based%20inverse%20modelling.%0ABenchmarks%20show%20that%20our%20approach%20significantly%20outperforms%20CPU%20and%20GPU-based%0Aimplementations%2C%20particularly%20for%20simulations%20with%20many%20modes.%20Inverse%0Amodelling%20experiments%20demonstrate%20that%20our%20approach%20can%20recover%20physical%0Aparameters%2C%20including%20tension%2C%20stiffness%2C%20and%20geometry%2C%20from%20both%20synthetic%20and%0Aexperimental%20data.%20Although%20fitting%20physical%20parameters%20is%20more%20sensitive%20to%0Ainitialisation%20compared%20to%20other%20methods%2C%20it%20provides%20greater%20interpretability%0Aand%20more%20compact%20parameterisation.%20The%20code%20is%20released%20as%20open%20source%20to%0Asupport%20future%20research%20and%20applications%20in%20differentiable%20physical%20modelling%0Aand%20sound%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05940v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Differentiable%2520Modal%2520Simulation%2520of%2520Non-linear%2520Strings%252C%2520Membranes%252C%250A%2520%2520and%2520Plates%26entry.906535625%3DRodrigo%2520Diaz%2520and%2520Mark%2520Sandler%26entry.1292438233%3D%2520%2520Modal%2520methods%2520for%2520simulating%2520vibrations%2520of%2520strings%252C%2520membranes%252C%2520and%2520plates%2520are%250Awidely%2520used%2520in%2520acoustics%2520and%2520physically%2520informed%2520audio%2520synthesis.%2520However%252C%250Atraditional%2520implementations%252C%2520particularly%2520for%2520non-linear%2520models%2520like%2520the%2520von%250AK%255C%2527arm%255C%2527an%2520plate%252C%2520are%2520computationally%2520demanding%2520and%2520lack%2520differentiability%252C%250Alimiting%2520inverse%2520modelling%2520and%2520real-time%2520applications.%2520We%2520introduce%2520a%2520fast%252C%250Adifferentiable%252C%2520GPU-accelerated%2520modal%2520framework%2520built%2520with%2520the%2520JAX%2520library%252C%250Aproviding%2520efficient%2520simulations%2520and%2520enabling%2520gradient-based%2520inverse%2520modelling.%250ABenchmarks%2520show%2520that%2520our%2520approach%2520significantly%2520outperforms%2520CPU%2520and%2520GPU-based%250Aimplementations%252C%2520particularly%2520for%2520simulations%2520with%2520many%2520modes.%2520Inverse%250Amodelling%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520can%2520recover%2520physical%250Aparameters%252C%2520including%2520tension%252C%2520stiffness%252C%2520and%2520geometry%252C%2520from%2520both%2520synthetic%2520and%250Aexperimental%2520data.%2520Although%2520fitting%2520physical%2520parameters%2520is%2520more%2520sensitive%2520to%250Ainitialisation%2520compared%2520to%2520other%2520methods%252C%2520it%2520provides%2520greater%2520interpretability%250Aand%2520more%2520compact%2520parameterisation.%2520The%2520code%2520is%2520released%2520as%2520open%2520source%2520to%250Asupport%2520future%2520research%2520and%2520applications%2520in%2520differentiable%2520physical%2520modelling%250Aand%2520sound%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05940v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Differentiable%20Modal%20Simulation%20of%20Non-linear%20Strings%2C%20Membranes%2C%0A%20%20and%20Plates&entry.906535625=Rodrigo%20Diaz%20and%20Mark%20Sandler&entry.1292438233=%20%20Modal%20methods%20for%20simulating%20vibrations%20of%20strings%2C%20membranes%2C%20and%20plates%20are%0Awidely%20used%20in%20acoustics%20and%20physically%20informed%20audio%20synthesis.%20However%2C%0Atraditional%20implementations%2C%20particularly%20for%20non-linear%20models%20like%20the%20von%0AK%5C%27arm%5C%27an%20plate%2C%20are%20computationally%20demanding%20and%20lack%20differentiability%2C%0Alimiting%20inverse%20modelling%20and%20real-time%20applications.%20We%20introduce%20a%20fast%2C%0Adifferentiable%2C%20GPU-accelerated%20modal%20framework%20built%20with%20the%20JAX%20library%2C%0Aproviding%20efficient%20simulations%20and%20enabling%20gradient-based%20inverse%20modelling.%0ABenchmarks%20show%20that%20our%20approach%20significantly%20outperforms%20CPU%20and%20GPU-based%0Aimplementations%2C%20particularly%20for%20simulations%20with%20many%20modes.%20Inverse%0Amodelling%20experiments%20demonstrate%20that%20our%20approach%20can%20recover%20physical%0Aparameters%2C%20including%20tension%2C%20stiffness%2C%20and%20geometry%2C%20from%20both%20synthetic%20and%0Aexperimental%20data.%20Although%20fitting%20physical%20parameters%20is%20more%20sensitive%20to%0Ainitialisation%20compared%20to%20other%20methods%2C%20it%20provides%20greater%20interpretability%0Aand%20more%20compact%20parameterisation.%20The%20code%20is%20released%20as%20open%20source%20to%0Asupport%20future%20research%20and%20applications%20in%20differentiable%20physical%20modelling%0Aand%20sound%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05940v2&entry.124074799=Read"},
{"title": "Thought-Augmented Policy Optimization: Bridging External Guidance and\n  Internal Capabilities", "author": "Jinyang Wu and Chonghua Liao and Mingkuan Feng and Shuai Zhang and Zhengqi Wen and Pengpeng Shao and Huazhe Xu and Jianhua Tao", "abstract": "  Reinforcement learning (RL) has emerged as an effective method for training\nreasoning models. However, existing RL approaches typically bias the model's\noutput distribution toward reward-maximizing paths without introducing external\nknowledge. This limits their exploration capacity and results in a narrower\nreasoning capability boundary compared to base models. To address this\nlimitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel\nframework that augments RL by incorporating external high-level guidance\n(\"thought patterns\"). By adaptively integrating structured thoughts during\ntraining, TAPO effectively balances model-internal exploration and external\nguidance exploitation. Extensive experiments show that our approach\nsignificantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva\nMath. Notably, these high-level thought patterns, abstracted from only 500\nprior samples, generalize effectively across various tasks and models. This\nhighlights TAPO's potential for broader applications across multiple tasks and\ndomains. Our further analysis reveals that introducing external guidance\nproduces powerful reasoning models with superior explainability of inference\nbehavior and enhanced output readability.\n", "link": "http://arxiv.org/abs/2505.15692v2", "date": "2025-05-26", "relevancy": 1.5213, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5274}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5085}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thought-Augmented%20Policy%20Optimization%3A%20Bridging%20External%20Guidance%20and%0A%20%20Internal%20Capabilities&body=Title%3A%20Thought-Augmented%20Policy%20Optimization%3A%20Bridging%20External%20Guidance%20and%0A%20%20Internal%20Capabilities%0AAuthor%3A%20Jinyang%20Wu%20and%20Chonghua%20Liao%20and%20Mingkuan%20Feng%20and%20Shuai%20Zhang%20and%20Zhengqi%20Wen%20and%20Pengpeng%20Shao%20and%20Huazhe%20Xu%20and%20Jianhua%20Tao%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20has%20emerged%20as%20an%20effective%20method%20for%20training%0Areasoning%20models.%20However%2C%20existing%20RL%20approaches%20typically%20bias%20the%20model%27s%0Aoutput%20distribution%20toward%20reward-maximizing%20paths%20without%20introducing%20external%0Aknowledge.%20This%20limits%20their%20exploration%20capacity%20and%20results%20in%20a%20narrower%0Areasoning%20capability%20boundary%20compared%20to%20base%20models.%20To%20address%20this%0Alimitation%2C%20we%20propose%20TAPO%20%28Thought-Augmented%20Policy%20Optimization%29%2C%20a%20novel%0Aframework%20that%20augments%20RL%20by%20incorporating%20external%20high-level%20guidance%0A%28%22thought%20patterns%22%29.%20By%20adaptively%20integrating%20structured%20thoughts%20during%0Atraining%2C%20TAPO%20effectively%20balances%20model-internal%20exploration%20and%20external%0Aguidance%20exploitation.%20Extensive%20experiments%20show%20that%20our%20approach%0Asignificantly%20outperforms%20GRPO%20by%2099%25%20on%20AIME%2C%2041%25%20on%20AMC%2C%20and%2017%25%20on%20Minerva%0AMath.%20Notably%2C%20these%20high-level%20thought%20patterns%2C%20abstracted%20from%20only%20500%0Aprior%20samples%2C%20generalize%20effectively%20across%20various%20tasks%20and%20models.%20This%0Ahighlights%20TAPO%27s%20potential%20for%20broader%20applications%20across%20multiple%20tasks%20and%0Adomains.%20Our%20further%20analysis%20reveals%20that%20introducing%20external%20guidance%0Aproduces%20powerful%20reasoning%20models%20with%20superior%20explainability%20of%20inference%0Abehavior%20and%20enhanced%20output%20readability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15692v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThought-Augmented%2520Policy%2520Optimization%253A%2520Bridging%2520External%2520Guidance%2520and%250A%2520%2520Internal%2520Capabilities%26entry.906535625%3DJinyang%2520Wu%2520and%2520Chonghua%2520Liao%2520and%2520Mingkuan%2520Feng%2520and%2520Shuai%2520Zhang%2520and%2520Zhengqi%2520Wen%2520and%2520Pengpeng%2520Shao%2520and%2520Huazhe%2520Xu%2520and%2520Jianhua%2520Tao%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520has%2520emerged%2520as%2520an%2520effective%2520method%2520for%2520training%250Areasoning%2520models.%2520However%252C%2520existing%2520RL%2520approaches%2520typically%2520bias%2520the%2520model%2527s%250Aoutput%2520distribution%2520toward%2520reward-maximizing%2520paths%2520without%2520introducing%2520external%250Aknowledge.%2520This%2520limits%2520their%2520exploration%2520capacity%2520and%2520results%2520in%2520a%2520narrower%250Areasoning%2520capability%2520boundary%2520compared%2520to%2520base%2520models.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520TAPO%2520%2528Thought-Augmented%2520Policy%2520Optimization%2529%252C%2520a%2520novel%250Aframework%2520that%2520augments%2520RL%2520by%2520incorporating%2520external%2520high-level%2520guidance%250A%2528%2522thought%2520patterns%2522%2529.%2520By%2520adaptively%2520integrating%2520structured%2520thoughts%2520during%250Atraining%252C%2520TAPO%2520effectively%2520balances%2520model-internal%2520exploration%2520and%2520external%250Aguidance%2520exploitation.%2520Extensive%2520experiments%2520show%2520that%2520our%2520approach%250Asignificantly%2520outperforms%2520GRPO%2520by%252099%2525%2520on%2520AIME%252C%252041%2525%2520on%2520AMC%252C%2520and%252017%2525%2520on%2520Minerva%250AMath.%2520Notably%252C%2520these%2520high-level%2520thought%2520patterns%252C%2520abstracted%2520from%2520only%2520500%250Aprior%2520samples%252C%2520generalize%2520effectively%2520across%2520various%2520tasks%2520and%2520models.%2520This%250Ahighlights%2520TAPO%2527s%2520potential%2520for%2520broader%2520applications%2520across%2520multiple%2520tasks%2520and%250Adomains.%2520Our%2520further%2520analysis%2520reveals%2520that%2520introducing%2520external%2520guidance%250Aproduces%2520powerful%2520reasoning%2520models%2520with%2520superior%2520explainability%2520of%2520inference%250Abehavior%2520and%2520enhanced%2520output%2520readability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15692v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thought-Augmented%20Policy%20Optimization%3A%20Bridging%20External%20Guidance%20and%0A%20%20Internal%20Capabilities&entry.906535625=Jinyang%20Wu%20and%20Chonghua%20Liao%20and%20Mingkuan%20Feng%20and%20Shuai%20Zhang%20and%20Zhengqi%20Wen%20and%20Pengpeng%20Shao%20and%20Huazhe%20Xu%20and%20Jianhua%20Tao&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20has%20emerged%20as%20an%20effective%20method%20for%20training%0Areasoning%20models.%20However%2C%20existing%20RL%20approaches%20typically%20bias%20the%20model%27s%0Aoutput%20distribution%20toward%20reward-maximizing%20paths%20without%20introducing%20external%0Aknowledge.%20This%20limits%20their%20exploration%20capacity%20and%20results%20in%20a%20narrower%0Areasoning%20capability%20boundary%20compared%20to%20base%20models.%20To%20address%20this%0Alimitation%2C%20we%20propose%20TAPO%20%28Thought-Augmented%20Policy%20Optimization%29%2C%20a%20novel%0Aframework%20that%20augments%20RL%20by%20incorporating%20external%20high-level%20guidance%0A%28%22thought%20patterns%22%29.%20By%20adaptively%20integrating%20structured%20thoughts%20during%0Atraining%2C%20TAPO%20effectively%20balances%20model-internal%20exploration%20and%20external%0Aguidance%20exploitation.%20Extensive%20experiments%20show%20that%20our%20approach%0Asignificantly%20outperforms%20GRPO%20by%2099%25%20on%20AIME%2C%2041%25%20on%20AMC%2C%20and%2017%25%20on%20Minerva%0AMath.%20Notably%2C%20these%20high-level%20thought%20patterns%2C%20abstracted%20from%20only%20500%0Aprior%20samples%2C%20generalize%20effectively%20across%20various%20tasks%20and%20models.%20This%0Ahighlights%20TAPO%27s%20potential%20for%20broader%20applications%20across%20multiple%20tasks%20and%0Adomains.%20Our%20further%20analysis%20reveals%20that%20introducing%20external%20guidance%0Aproduces%20powerful%20reasoning%20models%20with%20superior%20explainability%20of%20inference%0Abehavior%20and%20enhanced%20output%20readability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15692v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


