<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240521.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with\n  Anchor Gaussian Guided Texture Warping", "author": "Tianhao Wu and Jing Yang and Zhilin Guo and Jingyi Wan and Fangcheng Zhong and Cengiz Oztireli", "abstract": "  By equipping the most recent 3D Gaussian Splatting representation with head\n3D morphable models (3DMM), existing methods manage to create head avatars with\nhigh fidelity. However, most existing methods only reconstruct a head without\nthe body, substantially limiting their application scenarios. We found that\nnaively applying Gaussians to model the clothed chest and shoulders tends to\nresult in blurry reconstruction and noisy floaters under novel poses. This is\nbecause of the fundamental limitation of Gaussians and point clouds -- each\nGaussian or point can only have a single directional radiance without spatial\nvariance, therefore an unnecessarily large number of them is required to\nrepresent complicated spatially varying texture, even for simple geometry. In\ncontrast, we propose to model the body part with a neural texture that consists\nof coarse and pose-dependent fine colors. To properly render the body texture\nfor each view and pose without accurate geometry nor UV mapping, we optimize\nanother sparse set of Gaussians as anchors that constrain the neural warping\nfield that maps image plane coordinates to the texture space. We demonstrate\nthat Gaussian Head & Shoulders can fit the high-frequency details on the\nclothed upper body with high fidelity and potentially improve the accuracy and\nfidelity of the head region. We evaluate our method with casual phone-captured\nand internet videos and show our method archives superior reconstruction\nquality and robustness in both self and cross reenactment tasks. To fully\nutilize the efficient rendering speed of Gaussian splatting, we additionally\npropose an accelerated inference method of our trained model without\nMulti-Layer Perceptron (MLP) queries and reach a stable rendering speed of\naround 130 FPS for any subjects.\n", "link": "http://arxiv.org/abs/2405.12069v2", "date": "2024-05-21", "relevancy": 3.6791, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7599}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7599}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Head%20%26%20Shoulders%3A%20High%20Fidelity%20Neural%20Upper%20Body%20Avatars%20with%0A%20%20Anchor%20Gaussian%20Guided%20Texture%20Warping&body=Title%3A%20Gaussian%20Head%20%26%20Shoulders%3A%20High%20Fidelity%20Neural%20Upper%20Body%20Avatars%20with%0A%20%20Anchor%20Gaussian%20Guided%20Texture%20Warping%0AAuthor%3A%20Tianhao%20Wu%20and%20Jing%20Yang%20and%20Zhilin%20Guo%20and%20Jingyi%20Wan%20and%20Fangcheng%20Zhong%20and%20Cengiz%20Oztireli%0AAbstract%3A%20%20%20By%20equipping%20the%20most%20recent%203D%20Gaussian%20Splatting%20representation%20with%20head%0A3D%20morphable%20models%20%283DMM%29%2C%20existing%20methods%20manage%20to%20create%20head%20avatars%20with%0Ahigh%20fidelity.%20However%2C%20most%20existing%20methods%20only%20reconstruct%20a%20head%20without%0Athe%20body%2C%20substantially%20limiting%20their%20application%20scenarios.%20We%20found%20that%0Anaively%20applying%20Gaussians%20to%20model%20the%20clothed%20chest%20and%20shoulders%20tends%20to%0Aresult%20in%20blurry%20reconstruction%20and%20noisy%20floaters%20under%20novel%20poses.%20This%20is%0Abecause%20of%20the%20fundamental%20limitation%20of%20Gaussians%20and%20point%20clouds%20--%20each%0AGaussian%20or%20point%20can%20only%20have%20a%20single%20directional%20radiance%20without%20spatial%0Avariance%2C%20therefore%20an%20unnecessarily%20large%20number%20of%20them%20is%20required%20to%0Arepresent%20complicated%20spatially%20varying%20texture%2C%20even%20for%20simple%20geometry.%20In%0Acontrast%2C%20we%20propose%20to%20model%20the%20body%20part%20with%20a%20neural%20texture%20that%20consists%0Aof%20coarse%20and%20pose-dependent%20fine%20colors.%20To%20properly%20render%20the%20body%20texture%0Afor%20each%20view%20and%20pose%20without%20accurate%20geometry%20nor%20UV%20mapping%2C%20we%20optimize%0Aanother%20sparse%20set%20of%20Gaussians%20as%20anchors%20that%20constrain%20the%20neural%20warping%0Afield%20that%20maps%20image%20plane%20coordinates%20to%20the%20texture%20space.%20We%20demonstrate%0Athat%20Gaussian%20Head%20%26%20Shoulders%20can%20fit%20the%20high-frequency%20details%20on%20the%0Aclothed%20upper%20body%20with%20high%20fidelity%20and%20potentially%20improve%20the%20accuracy%20and%0Afidelity%20of%20the%20head%20region.%20We%20evaluate%20our%20method%20with%20casual%20phone-captured%0Aand%20internet%20videos%20and%20show%20our%20method%20archives%20superior%20reconstruction%0Aquality%20and%20robustness%20in%20both%20self%20and%20cross%20reenactment%20tasks.%20To%20fully%0Autilize%20the%20efficient%20rendering%20speed%20of%20Gaussian%20splatting%2C%20we%20additionally%0Apropose%20an%20accelerated%20inference%20method%20of%20our%20trained%20model%20without%0AMulti-Layer%20Perceptron%20%28MLP%29%20queries%20and%20reach%20a%20stable%20rendering%20speed%20of%0Aaround%20130%20FPS%20for%20any%20subjects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12069v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Head%2520%2526%2520Shoulders%253A%2520High%2520Fidelity%2520Neural%2520Upper%2520Body%2520Avatars%2520with%250A%2520%2520Anchor%2520Gaussian%2520Guided%2520Texture%2520Warping%26entry.906535625%3DTianhao%2520Wu%2520and%2520Jing%2520Yang%2520and%2520Zhilin%2520Guo%2520and%2520Jingyi%2520Wan%2520and%2520Fangcheng%2520Zhong%2520and%2520Cengiz%2520Oztireli%26entry.1292438233%3D%2520%2520By%2520equipping%2520the%2520most%2520recent%25203D%2520Gaussian%2520Splatting%2520representation%2520with%2520head%250A3D%2520morphable%2520models%2520%25283DMM%2529%252C%2520existing%2520methods%2520manage%2520to%2520create%2520head%2520avatars%2520with%250Ahigh%2520fidelity.%2520However%252C%2520most%2520existing%2520methods%2520only%2520reconstruct%2520a%2520head%2520without%250Athe%2520body%252C%2520substantially%2520limiting%2520their%2520application%2520scenarios.%2520We%2520found%2520that%250Anaively%2520applying%2520Gaussians%2520to%2520model%2520the%2520clothed%2520chest%2520and%2520shoulders%2520tends%2520to%250Aresult%2520in%2520blurry%2520reconstruction%2520and%2520noisy%2520floaters%2520under%2520novel%2520poses.%2520This%2520is%250Abecause%2520of%2520the%2520fundamental%2520limitation%2520of%2520Gaussians%2520and%2520point%2520clouds%2520--%2520each%250AGaussian%2520or%2520point%2520can%2520only%2520have%2520a%2520single%2520directional%2520radiance%2520without%2520spatial%250Avariance%252C%2520therefore%2520an%2520unnecessarily%2520large%2520number%2520of%2520them%2520is%2520required%2520to%250Arepresent%2520complicated%2520spatially%2520varying%2520texture%252C%2520even%2520for%2520simple%2520geometry.%2520In%250Acontrast%252C%2520we%2520propose%2520to%2520model%2520the%2520body%2520part%2520with%2520a%2520neural%2520texture%2520that%2520consists%250Aof%2520coarse%2520and%2520pose-dependent%2520fine%2520colors.%2520To%2520properly%2520render%2520the%2520body%2520texture%250Afor%2520each%2520view%2520and%2520pose%2520without%2520accurate%2520geometry%2520nor%2520UV%2520mapping%252C%2520we%2520optimize%250Aanother%2520sparse%2520set%2520of%2520Gaussians%2520as%2520anchors%2520that%2520constrain%2520the%2520neural%2520warping%250Afield%2520that%2520maps%2520image%2520plane%2520coordinates%2520to%2520the%2520texture%2520space.%2520We%2520demonstrate%250Athat%2520Gaussian%2520Head%2520%2526%2520Shoulders%2520can%2520fit%2520the%2520high-frequency%2520details%2520on%2520the%250Aclothed%2520upper%2520body%2520with%2520high%2520fidelity%2520and%2520potentially%2520improve%2520the%2520accuracy%2520and%250Afidelity%2520of%2520the%2520head%2520region.%2520We%2520evaluate%2520our%2520method%2520with%2520casual%2520phone-captured%250Aand%2520internet%2520videos%2520and%2520show%2520our%2520method%2520archives%2520superior%2520reconstruction%250Aquality%2520and%2520robustness%2520in%2520both%2520self%2520and%2520cross%2520reenactment%2520tasks.%2520To%2520fully%250Autilize%2520the%2520efficient%2520rendering%2520speed%2520of%2520Gaussian%2520splatting%252C%2520we%2520additionally%250Apropose%2520an%2520accelerated%2520inference%2520method%2520of%2520our%2520trained%2520model%2520without%250AMulti-Layer%2520Perceptron%2520%2528MLP%2529%2520queries%2520and%2520reach%2520a%2520stable%2520rendering%2520speed%2520of%250Aaround%2520130%2520FPS%2520for%2520any%2520subjects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12069v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Head%20%26%20Shoulders%3A%20High%20Fidelity%20Neural%20Upper%20Body%20Avatars%20with%0A%20%20Anchor%20Gaussian%20Guided%20Texture%20Warping&entry.906535625=Tianhao%20Wu%20and%20Jing%20Yang%20and%20Zhilin%20Guo%20and%20Jingyi%20Wan%20and%20Fangcheng%20Zhong%20and%20Cengiz%20Oztireli&entry.1292438233=%20%20By%20equipping%20the%20most%20recent%203D%20Gaussian%20Splatting%20representation%20with%20head%0A3D%20morphable%20models%20%283DMM%29%2C%20existing%20methods%20manage%20to%20create%20head%20avatars%20with%0Ahigh%20fidelity.%20However%2C%20most%20existing%20methods%20only%20reconstruct%20a%20head%20without%0Athe%20body%2C%20substantially%20limiting%20their%20application%20scenarios.%20We%20found%20that%0Anaively%20applying%20Gaussians%20to%20model%20the%20clothed%20chest%20and%20shoulders%20tends%20to%0Aresult%20in%20blurry%20reconstruction%20and%20noisy%20floaters%20under%20novel%20poses.%20This%20is%0Abecause%20of%20the%20fundamental%20limitation%20of%20Gaussians%20and%20point%20clouds%20--%20each%0AGaussian%20or%20point%20can%20only%20have%20a%20single%20directional%20radiance%20without%20spatial%0Avariance%2C%20therefore%20an%20unnecessarily%20large%20number%20of%20them%20is%20required%20to%0Arepresent%20complicated%20spatially%20varying%20texture%2C%20even%20for%20simple%20geometry.%20In%0Acontrast%2C%20we%20propose%20to%20model%20the%20body%20part%20with%20a%20neural%20texture%20that%20consists%0Aof%20coarse%20and%20pose-dependent%20fine%20colors.%20To%20properly%20render%20the%20body%20texture%0Afor%20each%20view%20and%20pose%20without%20accurate%20geometry%20nor%20UV%20mapping%2C%20we%20optimize%0Aanother%20sparse%20set%20of%20Gaussians%20as%20anchors%20that%20constrain%20the%20neural%20warping%0Afield%20that%20maps%20image%20plane%20coordinates%20to%20the%20texture%20space.%20We%20demonstrate%0Athat%20Gaussian%20Head%20%26%20Shoulders%20can%20fit%20the%20high-frequency%20details%20on%20the%0Aclothed%20upper%20body%20with%20high%20fidelity%20and%20potentially%20improve%20the%20accuracy%20and%0Afidelity%20of%20the%20head%20region.%20We%20evaluate%20our%20method%20with%20casual%20phone-captured%0Aand%20internet%20videos%20and%20show%20our%20method%20archives%20superior%20reconstruction%0Aquality%20and%20robustness%20in%20both%20self%20and%20cross%20reenactment%20tasks.%20To%20fully%0Autilize%20the%20efficient%20rendering%20speed%20of%20Gaussian%20splatting%2C%20we%20additionally%0Apropose%20an%20accelerated%20inference%20method%20of%20our%20trained%20model%20without%0AMulti-Layer%20Perceptron%20%28MLP%29%20queries%20and%20reach%20a%20stable%20rendering%20speed%20of%0Aaround%20130%20FPS%20for%20any%20subjects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12069v2&entry.124074799=Read"},
{"title": "LAGA: Layered 3D Avatar Generation and Customization via Gaussian\n  Splatting", "author": "Jia Gong and Shenyu Ji and Lin Geng Foo and Kang Chen and Hossein Rahmani and Jun Liu", "abstract": "  Creating and customizing a 3D clothed avatar from textual descriptions is a\ncritical and challenging task. Traditional methods often treat the human body\nand clothing as inseparable, limiting users' ability to freely mix and match\ngarments. In response to this limitation, we present LAyered Gaussian Avatar\n(LAGA), a carefully designed framework enabling the creation of high-fidelity\ndecomposable avatars with diverse garments. By decoupling garments from avatar,\nour framework empowers users to conviniently edit avatars at the garment level.\nOur approach begins by modeling the avatar using a set of Gaussian points\norganized in a layered structure, where each layer corresponds to a specific\ngarment or the human body itself. To generate high-quality garments for each\nlayer, we introduce a coarse-to-fine strategy for diverse garment generation\nand a novel dual-SDS loss function to maintain coherence between the generated\ngarments and avatar components, including the human body and other garments.\nMoreover, we introduce three regularization losses to guide the movement of\nGaussians for garment transfer, allowing garments to be freely transferred to\nvarious avatars. Extensive experimentation demonstrates that our approach\nsurpasses existing methods in the generation of 3D clothed humans.\n", "link": "http://arxiv.org/abs/2405.12663v1", "date": "2024-05-21", "relevancy": 3.225, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6659}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6345}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAGA%3A%20Layered%203D%20Avatar%20Generation%20and%20Customization%20via%20Gaussian%0A%20%20Splatting&body=Title%3A%20LAGA%3A%20Layered%203D%20Avatar%20Generation%20and%20Customization%20via%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Jia%20Gong%20and%20Shenyu%20Ji%20and%20Lin%20Geng%20Foo%20and%20Kang%20Chen%20and%20Hossein%20Rahmani%20and%20Jun%20Liu%0AAbstract%3A%20%20%20Creating%20and%20customizing%20a%203D%20clothed%20avatar%20from%20textual%20descriptions%20is%20a%0Acritical%20and%20challenging%20task.%20Traditional%20methods%20often%20treat%20the%20human%20body%0Aand%20clothing%20as%20inseparable%2C%20limiting%20users%27%20ability%20to%20freely%20mix%20and%20match%0Agarments.%20In%20response%20to%20this%20limitation%2C%20we%20present%20LAyered%20Gaussian%20Avatar%0A%28LAGA%29%2C%20a%20carefully%20designed%20framework%20enabling%20the%20creation%20of%20high-fidelity%0Adecomposable%20avatars%20with%20diverse%20garments.%20By%20decoupling%20garments%20from%20avatar%2C%0Aour%20framework%20empowers%20users%20to%20conviniently%20edit%20avatars%20at%20the%20garment%20level.%0AOur%20approach%20begins%20by%20modeling%20the%20avatar%20using%20a%20set%20of%20Gaussian%20points%0Aorganized%20in%20a%20layered%20structure%2C%20where%20each%20layer%20corresponds%20to%20a%20specific%0Agarment%20or%20the%20human%20body%20itself.%20To%20generate%20high-quality%20garments%20for%20each%0Alayer%2C%20we%20introduce%20a%20coarse-to-fine%20strategy%20for%20diverse%20garment%20generation%0Aand%20a%20novel%20dual-SDS%20loss%20function%20to%20maintain%20coherence%20between%20the%20generated%0Agarments%20and%20avatar%20components%2C%20including%20the%20human%20body%20and%20other%20garments.%0AMoreover%2C%20we%20introduce%20three%20regularization%20losses%20to%20guide%20the%20movement%20of%0AGaussians%20for%20garment%20transfer%2C%20allowing%20garments%20to%20be%20freely%20transferred%20to%0Avarious%20avatars.%20Extensive%20experimentation%20demonstrates%20that%20our%20approach%0Asurpasses%20existing%20methods%20in%20the%20generation%20of%203D%20clothed%20humans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAGA%253A%2520Layered%25203D%2520Avatar%2520Generation%2520and%2520Customization%2520via%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DJia%2520Gong%2520and%2520Shenyu%2520Ji%2520and%2520Lin%2520Geng%2520Foo%2520and%2520Kang%2520Chen%2520and%2520Hossein%2520Rahmani%2520and%2520Jun%2520Liu%26entry.1292438233%3D%2520%2520Creating%2520and%2520customizing%2520a%25203D%2520clothed%2520avatar%2520from%2520textual%2520descriptions%2520is%2520a%250Acritical%2520and%2520challenging%2520task.%2520Traditional%2520methods%2520often%2520treat%2520the%2520human%2520body%250Aand%2520clothing%2520as%2520inseparable%252C%2520limiting%2520users%2527%2520ability%2520to%2520freely%2520mix%2520and%2520match%250Agarments.%2520In%2520response%2520to%2520this%2520limitation%252C%2520we%2520present%2520LAyered%2520Gaussian%2520Avatar%250A%2528LAGA%2529%252C%2520a%2520carefully%2520designed%2520framework%2520enabling%2520the%2520creation%2520of%2520high-fidelity%250Adecomposable%2520avatars%2520with%2520diverse%2520garments.%2520By%2520decoupling%2520garments%2520from%2520avatar%252C%250Aour%2520framework%2520empowers%2520users%2520to%2520conviniently%2520edit%2520avatars%2520at%2520the%2520garment%2520level.%250AOur%2520approach%2520begins%2520by%2520modeling%2520the%2520avatar%2520using%2520a%2520set%2520of%2520Gaussian%2520points%250Aorganized%2520in%2520a%2520layered%2520structure%252C%2520where%2520each%2520layer%2520corresponds%2520to%2520a%2520specific%250Agarment%2520or%2520the%2520human%2520body%2520itself.%2520To%2520generate%2520high-quality%2520garments%2520for%2520each%250Alayer%252C%2520we%2520introduce%2520a%2520coarse-to-fine%2520strategy%2520for%2520diverse%2520garment%2520generation%250Aand%2520a%2520novel%2520dual-SDS%2520loss%2520function%2520to%2520maintain%2520coherence%2520between%2520the%2520generated%250Agarments%2520and%2520avatar%2520components%252C%2520including%2520the%2520human%2520body%2520and%2520other%2520garments.%250AMoreover%252C%2520we%2520introduce%2520three%2520regularization%2520losses%2520to%2520guide%2520the%2520movement%2520of%250AGaussians%2520for%2520garment%2520transfer%252C%2520allowing%2520garments%2520to%2520be%2520freely%2520transferred%2520to%250Avarious%2520avatars.%2520Extensive%2520experimentation%2520demonstrates%2520that%2520our%2520approach%250Asurpasses%2520existing%2520methods%2520in%2520the%2520generation%2520of%25203D%2520clothed%2520humans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAGA%3A%20Layered%203D%20Avatar%20Generation%20and%20Customization%20via%20Gaussian%0A%20%20Splatting&entry.906535625=Jia%20Gong%20and%20Shenyu%20Ji%20and%20Lin%20Geng%20Foo%20and%20Kang%20Chen%20and%20Hossein%20Rahmani%20and%20Jun%20Liu&entry.1292438233=%20%20Creating%20and%20customizing%20a%203D%20clothed%20avatar%20from%20textual%20descriptions%20is%20a%0Acritical%20and%20challenging%20task.%20Traditional%20methods%20often%20treat%20the%20human%20body%0Aand%20clothing%20as%20inseparable%2C%20limiting%20users%27%20ability%20to%20freely%20mix%20and%20match%0Agarments.%20In%20response%20to%20this%20limitation%2C%20we%20present%20LAyered%20Gaussian%20Avatar%0A%28LAGA%29%2C%20a%20carefully%20designed%20framework%20enabling%20the%20creation%20of%20high-fidelity%0Adecomposable%20avatars%20with%20diverse%20garments.%20By%20decoupling%20garments%20from%20avatar%2C%0Aour%20framework%20empowers%20users%20to%20conviniently%20edit%20avatars%20at%20the%20garment%20level.%0AOur%20approach%20begins%20by%20modeling%20the%20avatar%20using%20a%20set%20of%20Gaussian%20points%0Aorganized%20in%20a%20layered%20structure%2C%20where%20each%20layer%20corresponds%20to%20a%20specific%0Agarment%20or%20the%20human%20body%20itself.%20To%20generate%20high-quality%20garments%20for%20each%0Alayer%2C%20we%20introduce%20a%20coarse-to-fine%20strategy%20for%20diverse%20garment%20generation%0Aand%20a%20novel%20dual-SDS%20loss%20function%20to%20maintain%20coherence%20between%20the%20generated%0Agarments%20and%20avatar%20components%2C%20including%20the%20human%20body%20and%20other%20garments.%0AMoreover%2C%20we%20introduce%20three%20regularization%20losses%20to%20guide%20the%20movement%20of%0AGaussians%20for%20garment%20transfer%2C%20allowing%20garments%20to%20be%20freely%20transferred%20to%0Avarious%20avatars.%20Extensive%20experimentation%20demonstrates%20that%20our%20approach%0Asurpasses%20existing%20methods%20in%20the%20generation%20of%203D%20clothed%20humans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12663v1&entry.124074799=Read"},
{"title": "MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video", "author": "Hongsheng Wang and Xiang Cai and Xi Sun and Jinhong Yue and Shengyu Zhang and Feng Lin and Fei Wu", "abstract": "  Single-view clothed human reconstruction holds a central position in virtual\nreality applications, especially in contexts involving intricate human motions.\nIt presents notable challenges in achieving realistic clothing deformation.\nCurrent methodologies often overlook the influence of motion on surface\ndeformation, resulting in surfaces lacking the constraints imposed by global\nmotion. To overcome these limitations, we introduce an innovative framework,\nMotion-Based 3D Clothed Humans Synthesis (MOSS), which employs kinematic\ninformation to achieve motion-aware Gaussian split on the human surface. Our\nframework consists of two modules: Kinematic Gaussian Locating Splatting (KGAS)\nand Surface Deformation Detector (UID). KGAS incorporates matrix-Fisher\ndistribution to propagate global motion across the body surface. The density\nand rotation factors of this distribution explicitly control the Gaussians,\nthereby enhancing the realism of the reconstructed surface. Additionally, to\naddress local occlusions in single-view, based on KGAS, UID identifies\nsignificant surfaces, and geometric reconstruction is performed to compensate\nfor these deformations. Experimental results demonstrate that MOSS achieves\nstate-of-the-art visual quality in 3D clothed human synthesis from monocular\nvideos. Notably, we improve the Human NeRF and the Gaussian Splatting by 33.94%\nand 16.75% in LPIPS* respectively. Codes are available at\nhttps://wanghongsheng01.github.io/MOSS/.\n", "link": "http://arxiv.org/abs/2405.12806v1", "date": "2024-05-21", "relevancy": 3.0971, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.671}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5984}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOSS%3A%20Motion-based%203D%20Clothed%20Human%20Synthesis%20from%20Monocular%20Video&body=Title%3A%20MOSS%3A%20Motion-based%203D%20Clothed%20Human%20Synthesis%20from%20Monocular%20Video%0AAuthor%3A%20Hongsheng%20Wang%20and%20Xiang%20Cai%20and%20Xi%20Sun%20and%20Jinhong%20Yue%20and%20Shengyu%20Zhang%20and%20Feng%20Lin%20and%20Fei%20Wu%0AAbstract%3A%20%20%20Single-view%20clothed%20human%20reconstruction%20holds%20a%20central%20position%20in%20virtual%0Areality%20applications%2C%20especially%20in%20contexts%20involving%20intricate%20human%20motions.%0AIt%20presents%20notable%20challenges%20in%20achieving%20realistic%20clothing%20deformation.%0ACurrent%20methodologies%20often%20overlook%20the%20influence%20of%20motion%20on%20surface%0Adeformation%2C%20resulting%20in%20surfaces%20lacking%20the%20constraints%20imposed%20by%20global%0Amotion.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20an%20innovative%20framework%2C%0AMotion-Based%203D%20Clothed%20Humans%20Synthesis%20%28MOSS%29%2C%20which%20employs%20kinematic%0Ainformation%20to%20achieve%20motion-aware%20Gaussian%20split%20on%20the%20human%20surface.%20Our%0Aframework%20consists%20of%20two%20modules%3A%20Kinematic%20Gaussian%20Locating%20Splatting%20%28KGAS%29%0Aand%20Surface%20Deformation%20Detector%20%28UID%29.%20KGAS%20incorporates%20matrix-Fisher%0Adistribution%20to%20propagate%20global%20motion%20across%20the%20body%20surface.%20The%20density%0Aand%20rotation%20factors%20of%20this%20distribution%20explicitly%20control%20the%20Gaussians%2C%0Athereby%20enhancing%20the%20realism%20of%20the%20reconstructed%20surface.%20Additionally%2C%20to%0Aaddress%20local%20occlusions%20in%20single-view%2C%20based%20on%20KGAS%2C%20UID%20identifies%0Asignificant%20surfaces%2C%20and%20geometric%20reconstruction%20is%20performed%20to%20compensate%0Afor%20these%20deformations.%20Experimental%20results%20demonstrate%20that%20MOSS%20achieves%0Astate-of-the-art%20visual%20quality%20in%203D%20clothed%20human%20synthesis%20from%20monocular%0Avideos.%20Notably%2C%20we%20improve%20the%20Human%20NeRF%20and%20the%20Gaussian%20Splatting%20by%2033.94%25%0Aand%2016.75%25%20in%20LPIPS%2A%20respectively.%20Codes%20are%20available%20at%0Ahttps%3A//wanghongsheng01.github.io/MOSS/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOSS%253A%2520Motion-based%25203D%2520Clothed%2520Human%2520Synthesis%2520from%2520Monocular%2520Video%26entry.906535625%3DHongsheng%2520Wang%2520and%2520Xiang%2520Cai%2520and%2520Xi%2520Sun%2520and%2520Jinhong%2520Yue%2520and%2520Shengyu%2520Zhang%2520and%2520Feng%2520Lin%2520and%2520Fei%2520Wu%26entry.1292438233%3D%2520%2520Single-view%2520clothed%2520human%2520reconstruction%2520holds%2520a%2520central%2520position%2520in%2520virtual%250Areality%2520applications%252C%2520especially%2520in%2520contexts%2520involving%2520intricate%2520human%2520motions.%250AIt%2520presents%2520notable%2520challenges%2520in%2520achieving%2520realistic%2520clothing%2520deformation.%250ACurrent%2520methodologies%2520often%2520overlook%2520the%2520influence%2520of%2520motion%2520on%2520surface%250Adeformation%252C%2520resulting%2520in%2520surfaces%2520lacking%2520the%2520constraints%2520imposed%2520by%2520global%250Amotion.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520an%2520innovative%2520framework%252C%250AMotion-Based%25203D%2520Clothed%2520Humans%2520Synthesis%2520%2528MOSS%2529%252C%2520which%2520employs%2520kinematic%250Ainformation%2520to%2520achieve%2520motion-aware%2520Gaussian%2520split%2520on%2520the%2520human%2520surface.%2520Our%250Aframework%2520consists%2520of%2520two%2520modules%253A%2520Kinematic%2520Gaussian%2520Locating%2520Splatting%2520%2528KGAS%2529%250Aand%2520Surface%2520Deformation%2520Detector%2520%2528UID%2529.%2520KGAS%2520incorporates%2520matrix-Fisher%250Adistribution%2520to%2520propagate%2520global%2520motion%2520across%2520the%2520body%2520surface.%2520The%2520density%250Aand%2520rotation%2520factors%2520of%2520this%2520distribution%2520explicitly%2520control%2520the%2520Gaussians%252C%250Athereby%2520enhancing%2520the%2520realism%2520of%2520the%2520reconstructed%2520surface.%2520Additionally%252C%2520to%250Aaddress%2520local%2520occlusions%2520in%2520single-view%252C%2520based%2520on%2520KGAS%252C%2520UID%2520identifies%250Asignificant%2520surfaces%252C%2520and%2520geometric%2520reconstruction%2520is%2520performed%2520to%2520compensate%250Afor%2520these%2520deformations.%2520Experimental%2520results%2520demonstrate%2520that%2520MOSS%2520achieves%250Astate-of-the-art%2520visual%2520quality%2520in%25203D%2520clothed%2520human%2520synthesis%2520from%2520monocular%250Avideos.%2520Notably%252C%2520we%2520improve%2520the%2520Human%2520NeRF%2520and%2520the%2520Gaussian%2520Splatting%2520by%252033.94%2525%250Aand%252016.75%2525%2520in%2520LPIPS%252A%2520respectively.%2520Codes%2520are%2520available%2520at%250Ahttps%253A//wanghongsheng01.github.io/MOSS/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOSS%3A%20Motion-based%203D%20Clothed%20Human%20Synthesis%20from%20Monocular%20Video&entry.906535625=Hongsheng%20Wang%20and%20Xiang%20Cai%20and%20Xi%20Sun%20and%20Jinhong%20Yue%20and%20Shengyu%20Zhang%20and%20Feng%20Lin%20and%20Fei%20Wu&entry.1292438233=%20%20Single-view%20clothed%20human%20reconstruction%20holds%20a%20central%20position%20in%20virtual%0Areality%20applications%2C%20especially%20in%20contexts%20involving%20intricate%20human%20motions.%0AIt%20presents%20notable%20challenges%20in%20achieving%20realistic%20clothing%20deformation.%0ACurrent%20methodologies%20often%20overlook%20the%20influence%20of%20motion%20on%20surface%0Adeformation%2C%20resulting%20in%20surfaces%20lacking%20the%20constraints%20imposed%20by%20global%0Amotion.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20an%20innovative%20framework%2C%0AMotion-Based%203D%20Clothed%20Humans%20Synthesis%20%28MOSS%29%2C%20which%20employs%20kinematic%0Ainformation%20to%20achieve%20motion-aware%20Gaussian%20split%20on%20the%20human%20surface.%20Our%0Aframework%20consists%20of%20two%20modules%3A%20Kinematic%20Gaussian%20Locating%20Splatting%20%28KGAS%29%0Aand%20Surface%20Deformation%20Detector%20%28UID%29.%20KGAS%20incorporates%20matrix-Fisher%0Adistribution%20to%20propagate%20global%20motion%20across%20the%20body%20surface.%20The%20density%0Aand%20rotation%20factors%20of%20this%20distribution%20explicitly%20control%20the%20Gaussians%2C%0Athereby%20enhancing%20the%20realism%20of%20the%20reconstructed%20surface.%20Additionally%2C%20to%0Aaddress%20local%20occlusions%20in%20single-view%2C%20based%20on%20KGAS%2C%20UID%20identifies%0Asignificant%20surfaces%2C%20and%20geometric%20reconstruction%20is%20performed%20to%20compensate%0Afor%20these%20deformations.%20Experimental%20results%20demonstrate%20that%20MOSS%20achieves%0Astate-of-the-art%20visual%20quality%20in%203D%20clothed%20human%20synthesis%20from%20monocular%0Avideos.%20Notably%2C%20we%20improve%20the%20Human%20NeRF%20and%20the%20Gaussian%20Splatting%20by%2033.94%25%0Aand%2016.75%25%20in%20LPIPS%2A%20respectively.%20Codes%20are%20available%20at%0Ahttps%3A//wanghongsheng01.github.io/MOSS/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12806v1&entry.124074799=Read"},
{"title": "Auto-Linear Phenomenon in Subsurface Imaging", "author": "Yinan Feng and Yinpeng Chen and Peng Jin and Shihang Feng and Zicheng Liu and Youzuo Lin", "abstract": "  Subsurface imaging involves solving full waveform inversion (FWI) to predict\ngeophysical properties from measurements. This problem can be reframed as an\nimage-to-image translation, with the usual approach being to train an\nencoder-decoder network using paired data from two domains: geophysical\nproperty and measurement. A recent seminal work (InvLINT) demonstrates there is\nonly a linear mapping between the latent spaces of the two domains, and the\ndecoder requires paired data for training.\n  This paper extends this direction by demonstrating that only linear mapping\nnecessitates paired data, while both the encoder and decoder can be learned\nfrom their respective domains through self-supervised learning. This unveils an\nintriguing phenomenon (named Auto-Linear) where the self-learned features of\ntwo separate domains are automatically linearly correlated. Compared with\nexisting methods, our Auto-Linear has four advantages: (a) solving both forward\nand inverse modeling simultaneously, (b) applicable to different subsurface\nimaging tasks and achieving markedly better results than previous methods,\n(c)enhanced performance, especially in scenarios with limited paired data and\nin the presence of noisy data, and (d) strong generalization ability of the\ntrained encoder and decoder.\n", "link": "http://arxiv.org/abs/2305.13314v3", "date": "2024-05-21", "relevancy": 2.8755, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5974}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.587}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auto-Linear%20Phenomenon%20in%20Subsurface%20Imaging&body=Title%3A%20Auto-Linear%20Phenomenon%20in%20Subsurface%20Imaging%0AAuthor%3A%20Yinan%20Feng%20and%20Yinpeng%20Chen%20and%20Peng%20Jin%20and%20Shihang%20Feng%20and%20Zicheng%20Liu%20and%20Youzuo%20Lin%0AAbstract%3A%20%20%20Subsurface%20imaging%20involves%20solving%20full%20waveform%20inversion%20%28FWI%29%20to%20predict%0Ageophysical%20properties%20from%20measurements.%20This%20problem%20can%20be%20reframed%20as%20an%0Aimage-to-image%20translation%2C%20with%20the%20usual%20approach%20being%20to%20train%20an%0Aencoder-decoder%20network%20using%20paired%20data%20from%20two%20domains%3A%20geophysical%0Aproperty%20and%20measurement.%20A%20recent%20seminal%20work%20%28InvLINT%29%20demonstrates%20there%20is%0Aonly%20a%20linear%20mapping%20between%20the%20latent%20spaces%20of%20the%20two%20domains%2C%20and%20the%0Adecoder%20requires%20paired%20data%20for%20training.%0A%20%20This%20paper%20extends%20this%20direction%20by%20demonstrating%20that%20only%20linear%20mapping%0Anecessitates%20paired%20data%2C%20while%20both%20the%20encoder%20and%20decoder%20can%20be%20learned%0Afrom%20their%20respective%20domains%20through%20self-supervised%20learning.%20This%20unveils%20an%0Aintriguing%20phenomenon%20%28named%20Auto-Linear%29%20where%20the%20self-learned%20features%20of%0Atwo%20separate%20domains%20are%20automatically%20linearly%20correlated.%20Compared%20with%0Aexisting%20methods%2C%20our%20Auto-Linear%20has%20four%20advantages%3A%20%28a%29%20solving%20both%20forward%0Aand%20inverse%20modeling%20simultaneously%2C%20%28b%29%20applicable%20to%20different%20subsurface%0Aimaging%20tasks%20and%20achieving%20markedly%20better%20results%20than%20previous%20methods%2C%0A%28c%29enhanced%20performance%2C%20especially%20in%20scenarios%20with%20limited%20paired%20data%20and%0Ain%20the%20presence%20of%20noisy%20data%2C%20and%20%28d%29%20strong%20generalization%20ability%20of%20the%0Atrained%20encoder%20and%20decoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.13314v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuto-Linear%2520Phenomenon%2520in%2520Subsurface%2520Imaging%26entry.906535625%3DYinan%2520Feng%2520and%2520Yinpeng%2520Chen%2520and%2520Peng%2520Jin%2520and%2520Shihang%2520Feng%2520and%2520Zicheng%2520Liu%2520and%2520Youzuo%2520Lin%26entry.1292438233%3D%2520%2520Subsurface%2520imaging%2520involves%2520solving%2520full%2520waveform%2520inversion%2520%2528FWI%2529%2520to%2520predict%250Ageophysical%2520properties%2520from%2520measurements.%2520This%2520problem%2520can%2520be%2520reframed%2520as%2520an%250Aimage-to-image%2520translation%252C%2520with%2520the%2520usual%2520approach%2520being%2520to%2520train%2520an%250Aencoder-decoder%2520network%2520using%2520paired%2520data%2520from%2520two%2520domains%253A%2520geophysical%250Aproperty%2520and%2520measurement.%2520A%2520recent%2520seminal%2520work%2520%2528InvLINT%2529%2520demonstrates%2520there%2520is%250Aonly%2520a%2520linear%2520mapping%2520between%2520the%2520latent%2520spaces%2520of%2520the%2520two%2520domains%252C%2520and%2520the%250Adecoder%2520requires%2520paired%2520data%2520for%2520training.%250A%2520%2520This%2520paper%2520extends%2520this%2520direction%2520by%2520demonstrating%2520that%2520only%2520linear%2520mapping%250Anecessitates%2520paired%2520data%252C%2520while%2520both%2520the%2520encoder%2520and%2520decoder%2520can%2520be%2520learned%250Afrom%2520their%2520respective%2520domains%2520through%2520self-supervised%2520learning.%2520This%2520unveils%2520an%250Aintriguing%2520phenomenon%2520%2528named%2520Auto-Linear%2529%2520where%2520the%2520self-learned%2520features%2520of%250Atwo%2520separate%2520domains%2520are%2520automatically%2520linearly%2520correlated.%2520Compared%2520with%250Aexisting%2520methods%252C%2520our%2520Auto-Linear%2520has%2520four%2520advantages%253A%2520%2528a%2529%2520solving%2520both%2520forward%250Aand%2520inverse%2520modeling%2520simultaneously%252C%2520%2528b%2529%2520applicable%2520to%2520different%2520subsurface%250Aimaging%2520tasks%2520and%2520achieving%2520markedly%2520better%2520results%2520than%2520previous%2520methods%252C%250A%2528c%2529enhanced%2520performance%252C%2520especially%2520in%2520scenarios%2520with%2520limited%2520paired%2520data%2520and%250Ain%2520the%2520presence%2520of%2520noisy%2520data%252C%2520and%2520%2528d%2529%2520strong%2520generalization%2520ability%2520of%2520the%250Atrained%2520encoder%2520and%2520decoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.13314v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auto-Linear%20Phenomenon%20in%20Subsurface%20Imaging&entry.906535625=Yinan%20Feng%20and%20Yinpeng%20Chen%20and%20Peng%20Jin%20and%20Shihang%20Feng%20and%20Zicheng%20Liu%20and%20Youzuo%20Lin&entry.1292438233=%20%20Subsurface%20imaging%20involves%20solving%20full%20waveform%20inversion%20%28FWI%29%20to%20predict%0Ageophysical%20properties%20from%20measurements.%20This%20problem%20can%20be%20reframed%20as%20an%0Aimage-to-image%20translation%2C%20with%20the%20usual%20approach%20being%20to%20train%20an%0Aencoder-decoder%20network%20using%20paired%20data%20from%20two%20domains%3A%20geophysical%0Aproperty%20and%20measurement.%20A%20recent%20seminal%20work%20%28InvLINT%29%20demonstrates%20there%20is%0Aonly%20a%20linear%20mapping%20between%20the%20latent%20spaces%20of%20the%20two%20domains%2C%20and%20the%0Adecoder%20requires%20paired%20data%20for%20training.%0A%20%20This%20paper%20extends%20this%20direction%20by%20demonstrating%20that%20only%20linear%20mapping%0Anecessitates%20paired%20data%2C%20while%20both%20the%20encoder%20and%20decoder%20can%20be%20learned%0Afrom%20their%20respective%20domains%20through%20self-supervised%20learning.%20This%20unveils%20an%0Aintriguing%20phenomenon%20%28named%20Auto-Linear%29%20where%20the%20self-learned%20features%20of%0Atwo%20separate%20domains%20are%20automatically%20linearly%20correlated.%20Compared%20with%0Aexisting%20methods%2C%20our%20Auto-Linear%20has%20four%20advantages%3A%20%28a%29%20solving%20both%20forward%0Aand%20inverse%20modeling%20simultaneously%2C%20%28b%29%20applicable%20to%20different%20subsurface%0Aimaging%20tasks%20and%20achieving%20markedly%20better%20results%20than%20previous%20methods%2C%0A%28c%29enhanced%20performance%2C%20especially%20in%20scenarios%20with%20limited%20paired%20data%20and%0Ain%20the%20presence%20of%20noisy%20data%2C%20and%20%28d%29%20strong%20generalization%20ability%20of%20the%0Atrained%20encoder%20and%20decoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.13314v3&entry.124074799=Read"},
{"title": "Implicit-ARAP: Efficient Handle-Guided Deformation of High-Resolution\n  Meshes and Neural Fields via Local Patch Meshing", "author": "Daniele Baieri and Filippo Maggioli and Zorah L\u00e4hner and Simone Melzi and Emanuele Rodol\u00e0", "abstract": "  In this work, we present the local patch mesh representation for neural\nsigned distance fields. This technique allows to discretize local regions of\nthe level sets of an input SDF by projecting and deforming flat patch meshes\nonto the level set surface, using exclusively the SDF information and its\ngradient. Our analysis reveals this method to be more accurate than the\nstandard marching cubes algorithm for approximating the implicit surface. Then,\nwe apply this representation in the setting of handle-guided deformation: we\nintroduce two distinct pipelines, which make use of 3D neural fields to compute\nAs-Rigid-As-Possible deformations of both high-resolution meshes and neural\nfields under a given set of constraints. We run a comprehensive evaluation of\nour method and various baselines for neural field and mesh deformation which\nshow both pipelines achieve impressive efficiency and notable improvements in\nterms of quality of results and robustness. With our novel pipeline, we\nintroduce a scalable approach to solve a well-established geometry processing\nproblem on high-resolution meshes, and pave the way for extending other\ngeometric tasks to the domain of implicit surfaces via local patch meshing.\n", "link": "http://arxiv.org/abs/2405.12895v1", "date": "2024-05-21", "relevancy": 2.8025, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6155}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5515}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit-ARAP%3A%20Efficient%20Handle-Guided%20Deformation%20of%20High-Resolution%0A%20%20Meshes%20and%20Neural%20Fields%20via%20Local%20Patch%20Meshing&body=Title%3A%20Implicit-ARAP%3A%20Efficient%20Handle-Guided%20Deformation%20of%20High-Resolution%0A%20%20Meshes%20and%20Neural%20Fields%20via%20Local%20Patch%20Meshing%0AAuthor%3A%20Daniele%20Baieri%20and%20Filippo%20Maggioli%20and%20Zorah%20L%C3%A4hner%20and%20Simone%20Melzi%20and%20Emanuele%20Rodol%C3%A0%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20the%20local%20patch%20mesh%20representation%20for%20neural%0Asigned%20distance%20fields.%20This%20technique%20allows%20to%20discretize%20local%20regions%20of%0Athe%20level%20sets%20of%20an%20input%20SDF%20by%20projecting%20and%20deforming%20flat%20patch%20meshes%0Aonto%20the%20level%20set%20surface%2C%20using%20exclusively%20the%20SDF%20information%20and%20its%0Agradient.%20Our%20analysis%20reveals%20this%20method%20to%20be%20more%20accurate%20than%20the%0Astandard%20marching%20cubes%20algorithm%20for%20approximating%20the%20implicit%20surface.%20Then%2C%0Awe%20apply%20this%20representation%20in%20the%20setting%20of%20handle-guided%20deformation%3A%20we%0Aintroduce%20two%20distinct%20pipelines%2C%20which%20make%20use%20of%203D%20neural%20fields%20to%20compute%0AAs-Rigid-As-Possible%20deformations%20of%20both%20high-resolution%20meshes%20and%20neural%0Afields%20under%20a%20given%20set%20of%20constraints.%20We%20run%20a%20comprehensive%20evaluation%20of%0Aour%20method%20and%20various%20baselines%20for%20neural%20field%20and%20mesh%20deformation%20which%0Ashow%20both%20pipelines%20achieve%20impressive%20efficiency%20and%20notable%20improvements%20in%0Aterms%20of%20quality%20of%20results%20and%20robustness.%20With%20our%20novel%20pipeline%2C%20we%0Aintroduce%20a%20scalable%20approach%20to%20solve%20a%20well-established%20geometry%20processing%0Aproblem%20on%20high-resolution%20meshes%2C%20and%20pave%20the%20way%20for%20extending%20other%0Ageometric%20tasks%20to%20the%20domain%20of%20implicit%20surfaces%20via%20local%20patch%20meshing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit-ARAP%253A%2520Efficient%2520Handle-Guided%2520Deformation%2520of%2520High-Resolution%250A%2520%2520Meshes%2520and%2520Neural%2520Fields%2520via%2520Local%2520Patch%2520Meshing%26entry.906535625%3DDaniele%2520Baieri%2520and%2520Filippo%2520Maggioli%2520and%2520Zorah%2520L%25C3%25A4hner%2520and%2520Simone%2520Melzi%2520and%2520Emanuele%2520Rodol%25C3%25A0%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520local%2520patch%2520mesh%2520representation%2520for%2520neural%250Asigned%2520distance%2520fields.%2520This%2520technique%2520allows%2520to%2520discretize%2520local%2520regions%2520of%250Athe%2520level%2520sets%2520of%2520an%2520input%2520SDF%2520by%2520projecting%2520and%2520deforming%2520flat%2520patch%2520meshes%250Aonto%2520the%2520level%2520set%2520surface%252C%2520using%2520exclusively%2520the%2520SDF%2520information%2520and%2520its%250Agradient.%2520Our%2520analysis%2520reveals%2520this%2520method%2520to%2520be%2520more%2520accurate%2520than%2520the%250Astandard%2520marching%2520cubes%2520algorithm%2520for%2520approximating%2520the%2520implicit%2520surface.%2520Then%252C%250Awe%2520apply%2520this%2520representation%2520in%2520the%2520setting%2520of%2520handle-guided%2520deformation%253A%2520we%250Aintroduce%2520two%2520distinct%2520pipelines%252C%2520which%2520make%2520use%2520of%25203D%2520neural%2520fields%2520to%2520compute%250AAs-Rigid-As-Possible%2520deformations%2520of%2520both%2520high-resolution%2520meshes%2520and%2520neural%250Afields%2520under%2520a%2520given%2520set%2520of%2520constraints.%2520We%2520run%2520a%2520comprehensive%2520evaluation%2520of%250Aour%2520method%2520and%2520various%2520baselines%2520for%2520neural%2520field%2520and%2520mesh%2520deformation%2520which%250Ashow%2520both%2520pipelines%2520achieve%2520impressive%2520efficiency%2520and%2520notable%2520improvements%2520in%250Aterms%2520of%2520quality%2520of%2520results%2520and%2520robustness.%2520With%2520our%2520novel%2520pipeline%252C%2520we%250Aintroduce%2520a%2520scalable%2520approach%2520to%2520solve%2520a%2520well-established%2520geometry%2520processing%250Aproblem%2520on%2520high-resolution%2520meshes%252C%2520and%2520pave%2520the%2520way%2520for%2520extending%2520other%250Ageometric%2520tasks%2520to%2520the%2520domain%2520of%2520implicit%2520surfaces%2520via%2520local%2520patch%2520meshing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit-ARAP%3A%20Efficient%20Handle-Guided%20Deformation%20of%20High-Resolution%0A%20%20Meshes%20and%20Neural%20Fields%20via%20Local%20Patch%20Meshing&entry.906535625=Daniele%20Baieri%20and%20Filippo%20Maggioli%20and%20Zorah%20L%C3%A4hner%20and%20Simone%20Melzi%20and%20Emanuele%20Rodol%C3%A0&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20the%20local%20patch%20mesh%20representation%20for%20neural%0Asigned%20distance%20fields.%20This%20technique%20allows%20to%20discretize%20local%20regions%20of%0Athe%20level%20sets%20of%20an%20input%20SDF%20by%20projecting%20and%20deforming%20flat%20patch%20meshes%0Aonto%20the%20level%20set%20surface%2C%20using%20exclusively%20the%20SDF%20information%20and%20its%0Agradient.%20Our%20analysis%20reveals%20this%20method%20to%20be%20more%20accurate%20than%20the%0Astandard%20marching%20cubes%20algorithm%20for%20approximating%20the%20implicit%20surface.%20Then%2C%0Awe%20apply%20this%20representation%20in%20the%20setting%20of%20handle-guided%20deformation%3A%20we%0Aintroduce%20two%20distinct%20pipelines%2C%20which%20make%20use%20of%203D%20neural%20fields%20to%20compute%0AAs-Rigid-As-Possible%20deformations%20of%20both%20high-resolution%20meshes%20and%20neural%0Afields%20under%20a%20given%20set%20of%20constraints.%20We%20run%20a%20comprehensive%20evaluation%20of%0Aour%20method%20and%20various%20baselines%20for%20neural%20field%20and%20mesh%20deformation%20which%0Ashow%20both%20pipelines%20achieve%20impressive%20efficiency%20and%20notable%20improvements%20in%0Aterms%20of%20quality%20of%20results%20and%20robustness.%20With%20our%20novel%20pipeline%2C%20we%0Aintroduce%20a%20scalable%20approach%20to%20solve%20a%20well-established%20geometry%20processing%0Aproblem%20on%20high-resolution%20meshes%2C%20and%20pave%20the%20way%20for%20extending%20other%0Ageometric%20tasks%20to%20the%20domain%20of%20implicit%20surfaces%20via%20local%20patch%20meshing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12895v1&entry.124074799=Read"},
{"title": "AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion", "author": "Mohamad Qadri and Kevin Zhang and Akshay Hinduja and Michael Kaess and Adithya Pediredla and Christopher A. Metzler", "abstract": "  Underwater perception and 3D surface reconstruction are challenging problems\nwith broad applications in construction, security, marine archaeology, and\nenvironmental monitoring. Treacherous operating conditions, fragile\nsurroundings, and limited navigation control often dictate that submersibles\nrestrict their range of motion and, thus, the baseline over which they can\ncapture measurements. In the context of 3D scene reconstruction, it is\nwell-known that smaller baselines make reconstruction more challenging. Our\nwork develops a physics-based multimodal acoustic-optical neural surface\nreconstruction framework (AONeuS) capable of effectively integrating\nhigh-resolution RGB measurements with low-resolution depth-resolved imaging\nsonar measurements. By fusing these complementary modalities, our framework can\nreconstruct accurate high-resolution 3D surfaces from measurements captured\nover heavily-restricted baselines. Through extensive simulations and in-lab\nexperiments, we demonstrate that AONeuS dramatically outperforms recent\nRGB-only and sonar-only inverse-differentiable-rendering--based surface\nreconstruction methods. A website visualizing the results of our paper is\nlocated at this address: https://aoneus.github.io/\n", "link": "http://arxiv.org/abs/2402.03309v2", "date": "2024-05-21", "relevancy": 2.7652, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5675}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5458}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AONeuS%3A%20A%20Neural%20Rendering%20Framework%20for%20Acoustic-Optical%20Sensor%20Fusion&body=Title%3A%20AONeuS%3A%20A%20Neural%20Rendering%20Framework%20for%20Acoustic-Optical%20Sensor%20Fusion%0AAuthor%3A%20Mohamad%20Qadri%20and%20Kevin%20Zhang%20and%20Akshay%20Hinduja%20and%20Michael%20Kaess%20and%20Adithya%20Pediredla%20and%20Christopher%20A.%20Metzler%0AAbstract%3A%20%20%20Underwater%20perception%20and%203D%20surface%20reconstruction%20are%20challenging%20problems%0Awith%20broad%20applications%20in%20construction%2C%20security%2C%20marine%20archaeology%2C%20and%0Aenvironmental%20monitoring.%20Treacherous%20operating%20conditions%2C%20fragile%0Asurroundings%2C%20and%20limited%20navigation%20control%20often%20dictate%20that%20submersibles%0Arestrict%20their%20range%20of%20motion%20and%2C%20thus%2C%20the%20baseline%20over%20which%20they%20can%0Acapture%20measurements.%20In%20the%20context%20of%203D%20scene%20reconstruction%2C%20it%20is%0Awell-known%20that%20smaller%20baselines%20make%20reconstruction%20more%20challenging.%20Our%0Awork%20develops%20a%20physics-based%20multimodal%20acoustic-optical%20neural%20surface%0Areconstruction%20framework%20%28AONeuS%29%20capable%20of%20effectively%20integrating%0Ahigh-resolution%20RGB%20measurements%20with%20low-resolution%20depth-resolved%20imaging%0Asonar%20measurements.%20By%20fusing%20these%20complementary%20modalities%2C%20our%20framework%20can%0Areconstruct%20accurate%20high-resolution%203D%20surfaces%20from%20measurements%20captured%0Aover%20heavily-restricted%20baselines.%20Through%20extensive%20simulations%20and%20in-lab%0Aexperiments%2C%20we%20demonstrate%20that%20AONeuS%20dramatically%20outperforms%20recent%0ARGB-only%20and%20sonar-only%20inverse-differentiable-rendering--based%20surface%0Areconstruction%20methods.%20A%20website%20visualizing%20the%20results%20of%20our%20paper%20is%0Alocated%20at%20this%20address%3A%20https%3A//aoneus.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03309v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAONeuS%253A%2520A%2520Neural%2520Rendering%2520Framework%2520for%2520Acoustic-Optical%2520Sensor%2520Fusion%26entry.906535625%3DMohamad%2520Qadri%2520and%2520Kevin%2520Zhang%2520and%2520Akshay%2520Hinduja%2520and%2520Michael%2520Kaess%2520and%2520Adithya%2520Pediredla%2520and%2520Christopher%2520A.%2520Metzler%26entry.1292438233%3D%2520%2520Underwater%2520perception%2520and%25203D%2520surface%2520reconstruction%2520are%2520challenging%2520problems%250Awith%2520broad%2520applications%2520in%2520construction%252C%2520security%252C%2520marine%2520archaeology%252C%2520and%250Aenvironmental%2520monitoring.%2520Treacherous%2520operating%2520conditions%252C%2520fragile%250Asurroundings%252C%2520and%2520limited%2520navigation%2520control%2520often%2520dictate%2520that%2520submersibles%250Arestrict%2520their%2520range%2520of%2520motion%2520and%252C%2520thus%252C%2520the%2520baseline%2520over%2520which%2520they%2520can%250Acapture%2520measurements.%2520In%2520the%2520context%2520of%25203D%2520scene%2520reconstruction%252C%2520it%2520is%250Awell-known%2520that%2520smaller%2520baselines%2520make%2520reconstruction%2520more%2520challenging.%2520Our%250Awork%2520develops%2520a%2520physics-based%2520multimodal%2520acoustic-optical%2520neural%2520surface%250Areconstruction%2520framework%2520%2528AONeuS%2529%2520capable%2520of%2520effectively%2520integrating%250Ahigh-resolution%2520RGB%2520measurements%2520with%2520low-resolution%2520depth-resolved%2520imaging%250Asonar%2520measurements.%2520By%2520fusing%2520these%2520complementary%2520modalities%252C%2520our%2520framework%2520can%250Areconstruct%2520accurate%2520high-resolution%25203D%2520surfaces%2520from%2520measurements%2520captured%250Aover%2520heavily-restricted%2520baselines.%2520Through%2520extensive%2520simulations%2520and%2520in-lab%250Aexperiments%252C%2520we%2520demonstrate%2520that%2520AONeuS%2520dramatically%2520outperforms%2520recent%250ARGB-only%2520and%2520sonar-only%2520inverse-differentiable-rendering--based%2520surface%250Areconstruction%2520methods.%2520A%2520website%2520visualizing%2520the%2520results%2520of%2520our%2520paper%2520is%250Alocated%2520at%2520this%2520address%253A%2520https%253A//aoneus.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03309v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AONeuS%3A%20A%20Neural%20Rendering%20Framework%20for%20Acoustic-Optical%20Sensor%20Fusion&entry.906535625=Mohamad%20Qadri%20and%20Kevin%20Zhang%20and%20Akshay%20Hinduja%20and%20Michael%20Kaess%20and%20Adithya%20Pediredla%20and%20Christopher%20A.%20Metzler&entry.1292438233=%20%20Underwater%20perception%20and%203D%20surface%20reconstruction%20are%20challenging%20problems%0Awith%20broad%20applications%20in%20construction%2C%20security%2C%20marine%20archaeology%2C%20and%0Aenvironmental%20monitoring.%20Treacherous%20operating%20conditions%2C%20fragile%0Asurroundings%2C%20and%20limited%20navigation%20control%20often%20dictate%20that%20submersibles%0Arestrict%20their%20range%20of%20motion%20and%2C%20thus%2C%20the%20baseline%20over%20which%20they%20can%0Acapture%20measurements.%20In%20the%20context%20of%203D%20scene%20reconstruction%2C%20it%20is%0Awell-known%20that%20smaller%20baselines%20make%20reconstruction%20more%20challenging.%20Our%0Awork%20develops%20a%20physics-based%20multimodal%20acoustic-optical%20neural%20surface%0Areconstruction%20framework%20%28AONeuS%29%20capable%20of%20effectively%20integrating%0Ahigh-resolution%20RGB%20measurements%20with%20low-resolution%20depth-resolved%20imaging%0Asonar%20measurements.%20By%20fusing%20these%20complementary%20modalities%2C%20our%20framework%20can%0Areconstruct%20accurate%20high-resolution%203D%20surfaces%20from%20measurements%20captured%0Aover%20heavily-restricted%20baselines.%20Through%20extensive%20simulations%20and%20in-lab%0Aexperiments%2C%20we%20demonstrate%20that%20AONeuS%20dramatically%20outperforms%20recent%0ARGB-only%20and%20sonar-only%20inverse-differentiable-rendering--based%20surface%0Areconstruction%20methods.%20A%20website%20visualizing%20the%20results%20of%20our%20paper%20is%0Alocated%20at%20this%20address%3A%20https%3A//aoneus.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03309v2&entry.124074799=Read"},
{"title": "OmniGlue: Generalizable Feature Matching with Foundation Model Guidance", "author": "Hanwen Jiang and Arjun Karpur and Bingyi Cao and Qixing Huang and Andre Araujo", "abstract": "  The image matching field has been witnessing a continuous emergence of novel\nlearnable feature matching techniques, with ever-improving performance on\nconventional benchmarks. However, our investigation shows that despite these\ngains, their potential for real-world applications is restricted by their\nlimited generalization capabilities to novel image domains. In this paper, we\nintroduce OmniGlue, the first learnable image matcher that is designed with\ngeneralization as a core principle. OmniGlue leverages broad knowledge from a\nvision foundation model to guide the feature matching process, boosting\ngeneralization to domains not seen at training time. Additionally, we propose a\nnovel keypoint position-guided attention mechanism which disentangles spatial\nand appearance information, leading to enhanced matching descriptors. We\nperform comprehensive experiments on a suite of $7$ datasets with varied image\ndomains, including scene-level, object-centric and aerial images. OmniGlue's\nnovel components lead to relative gains on unseen domains of $20.9\\%$ with\nrespect to a directly comparable reference model, while also outperforming the\nrecent LightGlue method by $9.5\\%$ relatively.Code and model can be found at\nhttps://hwjiang1510.github.io/OmniGlue\n", "link": "http://arxiv.org/abs/2405.12979v1", "date": "2024-05-21", "relevancy": 2.749, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5756}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5444}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniGlue%3A%20Generalizable%20Feature%20Matching%20with%20Foundation%20Model%20Guidance&body=Title%3A%20OmniGlue%3A%20Generalizable%20Feature%20Matching%20with%20Foundation%20Model%20Guidance%0AAuthor%3A%20Hanwen%20Jiang%20and%20Arjun%20Karpur%20and%20Bingyi%20Cao%20and%20Qixing%20Huang%20and%20Andre%20Araujo%0AAbstract%3A%20%20%20The%20image%20matching%20field%20has%20been%20witnessing%20a%20continuous%20emergence%20of%20novel%0Alearnable%20feature%20matching%20techniques%2C%20with%20ever-improving%20performance%20on%0Aconventional%20benchmarks.%20However%2C%20our%20investigation%20shows%20that%20despite%20these%0Agains%2C%20their%20potential%20for%20real-world%20applications%20is%20restricted%20by%20their%0Alimited%20generalization%20capabilities%20to%20novel%20image%20domains.%20In%20this%20paper%2C%20we%0Aintroduce%20OmniGlue%2C%20the%20first%20learnable%20image%20matcher%20that%20is%20designed%20with%0Ageneralization%20as%20a%20core%20principle.%20OmniGlue%20leverages%20broad%20knowledge%20from%20a%0Avision%20foundation%20model%20to%20guide%20the%20feature%20matching%20process%2C%20boosting%0Ageneralization%20to%20domains%20not%20seen%20at%20training%20time.%20Additionally%2C%20we%20propose%20a%0Anovel%20keypoint%20position-guided%20attention%20mechanism%20which%20disentangles%20spatial%0Aand%20appearance%20information%2C%20leading%20to%20enhanced%20matching%20descriptors.%20We%0Aperform%20comprehensive%20experiments%20on%20a%20suite%20of%20%247%24%20datasets%20with%20varied%20image%0Adomains%2C%20including%20scene-level%2C%20object-centric%20and%20aerial%20images.%20OmniGlue%27s%0Anovel%20components%20lead%20to%20relative%20gains%20on%20unseen%20domains%20of%20%2420.9%5C%25%24%20with%0Arespect%20to%20a%20directly%20comparable%20reference%20model%2C%20while%20also%20outperforming%20the%0Arecent%20LightGlue%20method%20by%20%249.5%5C%25%24%20relatively.Code%20and%20model%20can%20be%20found%20at%0Ahttps%3A//hwjiang1510.github.io/OmniGlue%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniGlue%253A%2520Generalizable%2520Feature%2520Matching%2520with%2520Foundation%2520Model%2520Guidance%26entry.906535625%3DHanwen%2520Jiang%2520and%2520Arjun%2520Karpur%2520and%2520Bingyi%2520Cao%2520and%2520Qixing%2520Huang%2520and%2520Andre%2520Araujo%26entry.1292438233%3D%2520%2520The%2520image%2520matching%2520field%2520has%2520been%2520witnessing%2520a%2520continuous%2520emergence%2520of%2520novel%250Alearnable%2520feature%2520matching%2520techniques%252C%2520with%2520ever-improving%2520performance%2520on%250Aconventional%2520benchmarks.%2520However%252C%2520our%2520investigation%2520shows%2520that%2520despite%2520these%250Agains%252C%2520their%2520potential%2520for%2520real-world%2520applications%2520is%2520restricted%2520by%2520their%250Alimited%2520generalization%2520capabilities%2520to%2520novel%2520image%2520domains.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520OmniGlue%252C%2520the%2520first%2520learnable%2520image%2520matcher%2520that%2520is%2520designed%2520with%250Ageneralization%2520as%2520a%2520core%2520principle.%2520OmniGlue%2520leverages%2520broad%2520knowledge%2520from%2520a%250Avision%2520foundation%2520model%2520to%2520guide%2520the%2520feature%2520matching%2520process%252C%2520boosting%250Ageneralization%2520to%2520domains%2520not%2520seen%2520at%2520training%2520time.%2520Additionally%252C%2520we%2520propose%2520a%250Anovel%2520keypoint%2520position-guided%2520attention%2520mechanism%2520which%2520disentangles%2520spatial%250Aand%2520appearance%2520information%252C%2520leading%2520to%2520enhanced%2520matching%2520descriptors.%2520We%250Aperform%2520comprehensive%2520experiments%2520on%2520a%2520suite%2520of%2520%25247%2524%2520datasets%2520with%2520varied%2520image%250Adomains%252C%2520including%2520scene-level%252C%2520object-centric%2520and%2520aerial%2520images.%2520OmniGlue%2527s%250Anovel%2520components%2520lead%2520to%2520relative%2520gains%2520on%2520unseen%2520domains%2520of%2520%252420.9%255C%2525%2524%2520with%250Arespect%2520to%2520a%2520directly%2520comparable%2520reference%2520model%252C%2520while%2520also%2520outperforming%2520the%250Arecent%2520LightGlue%2520method%2520by%2520%25249.5%255C%2525%2524%2520relatively.Code%2520and%2520model%2520can%2520be%2520found%2520at%250Ahttps%253A//hwjiang1510.github.io/OmniGlue%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniGlue%3A%20Generalizable%20Feature%20Matching%20with%20Foundation%20Model%20Guidance&entry.906535625=Hanwen%20Jiang%20and%20Arjun%20Karpur%20and%20Bingyi%20Cao%20and%20Qixing%20Huang%20and%20Andre%20Araujo&entry.1292438233=%20%20The%20image%20matching%20field%20has%20been%20witnessing%20a%20continuous%20emergence%20of%20novel%0Alearnable%20feature%20matching%20techniques%2C%20with%20ever-improving%20performance%20on%0Aconventional%20benchmarks.%20However%2C%20our%20investigation%20shows%20that%20despite%20these%0Agains%2C%20their%20potential%20for%20real-world%20applications%20is%20restricted%20by%20their%0Alimited%20generalization%20capabilities%20to%20novel%20image%20domains.%20In%20this%20paper%2C%20we%0Aintroduce%20OmniGlue%2C%20the%20first%20learnable%20image%20matcher%20that%20is%20designed%20with%0Ageneralization%20as%20a%20core%20principle.%20OmniGlue%20leverages%20broad%20knowledge%20from%20a%0Avision%20foundation%20model%20to%20guide%20the%20feature%20matching%20process%2C%20boosting%0Ageneralization%20to%20domains%20not%20seen%20at%20training%20time.%20Additionally%2C%20we%20propose%20a%0Anovel%20keypoint%20position-guided%20attention%20mechanism%20which%20disentangles%20spatial%0Aand%20appearance%20information%2C%20leading%20to%20enhanced%20matching%20descriptors.%20We%0Aperform%20comprehensive%20experiments%20on%20a%20suite%20of%20%247%24%20datasets%20with%20varied%20image%0Adomains%2C%20including%20scene-level%2C%20object-centric%20and%20aerial%20images.%20OmniGlue%27s%0Anovel%20components%20lead%20to%20relative%20gains%20on%20unseen%20domains%20of%20%2420.9%5C%25%24%20with%0Arespect%20to%20a%20directly%20comparable%20reference%20model%2C%20while%20also%20outperforming%20the%0Arecent%20LightGlue%20method%20by%20%249.5%5C%25%24%20relatively.Code%20and%20model%20can%20be%20found%20at%0Ahttps%3A//hwjiang1510.github.io/OmniGlue%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12979v1&entry.124074799=Read"},
{"title": "Dynamic Identity-Guided Attention Network for Visible-Infrared Person\n  Re-identification", "author": "Peng Gao and Yujian Lee and Hui Zhang and Xubo Liu and Yiyang Hu and Guquan Jing", "abstract": "  Visible-infrared person re-identification (VI-ReID) aims to match people with\nthe same identity between visible and infrared modalities. VI-ReID is a\nchallenging task due to the large differences in individual appearance under\ndifferent modalities. Existing methods generally try to bridge the cross-modal\ndifferences at image or feature level, which lacks exploring the discriminative\nembeddings. Effectively minimizing these cross-modal discrepancies relies on\nobtaining representations that are guided by identity and consistent across\nmodalities, while also filtering out representations that are irrelevant to\nidentity. To address these challenges, we introduce a dynamic identity-guided\nattention network (DIAN) to mine identity-guided and modality-consistent\nembeddings, facilitating effective bridging the gap between different\nmodalities. Specifically, in DIAN, to pursue a semantically richer\nrepresentation, we first use orthogonal projection to fuse the features from\ntwo connected coarse and fine layers. Furthermore, we first use dynamic\nconvolution kernels to mine identity-guided and modality-consistent\nrepresentations. More notably, a cross embedding balancing loss is introduced\nto effectively bridge cross-modal discrepancies by above embeddings.\nExperimental results on SYSU-MM01 and RegDB datasets show that DIAN achieves\nstate-of-the-art performance. Specifically, for indoor search on SYSU-MM01, our\nmethod achieves 86.28% rank-1 accuracy and 87.41% mAP, respectively. Our code\nwill be available soon.\n", "link": "http://arxiv.org/abs/2405.12713v1", "date": "2024-05-21", "relevancy": 2.7185, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5763}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5336}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Identity-Guided%20Attention%20Network%20for%20Visible-Infrared%20Person%0A%20%20Re-identification&body=Title%3A%20Dynamic%20Identity-Guided%20Attention%20Network%20for%20Visible-Infrared%20Person%0A%20%20Re-identification%0AAuthor%3A%20Peng%20Gao%20and%20Yujian%20Lee%20and%20Hui%20Zhang%20and%20Xubo%20Liu%20and%20Yiyang%20Hu%20and%20Guquan%20Jing%0AAbstract%3A%20%20%20Visible-infrared%20person%20re-identification%20%28VI-ReID%29%20aims%20to%20match%20people%20with%0Athe%20same%20identity%20between%20visible%20and%20infrared%20modalities.%20VI-ReID%20is%20a%0Achallenging%20task%20due%20to%20the%20large%20differences%20in%20individual%20appearance%20under%0Adifferent%20modalities.%20Existing%20methods%20generally%20try%20to%20bridge%20the%20cross-modal%0Adifferences%20at%20image%20or%20feature%20level%2C%20which%20lacks%20exploring%20the%20discriminative%0Aembeddings.%20Effectively%20minimizing%20these%20cross-modal%20discrepancies%20relies%20on%0Aobtaining%20representations%20that%20are%20guided%20by%20identity%20and%20consistent%20across%0Amodalities%2C%20while%20also%20filtering%20out%20representations%20that%20are%20irrelevant%20to%0Aidentity.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20dynamic%20identity-guided%0Aattention%20network%20%28DIAN%29%20to%20mine%20identity-guided%20and%20modality-consistent%0Aembeddings%2C%20facilitating%20effective%20bridging%20the%20gap%20between%20different%0Amodalities.%20Specifically%2C%20in%20DIAN%2C%20to%20pursue%20a%20semantically%20richer%0Arepresentation%2C%20we%20first%20use%20orthogonal%20projection%20to%20fuse%20the%20features%20from%0Atwo%20connected%20coarse%20and%20fine%20layers.%20Furthermore%2C%20we%20first%20use%20dynamic%0Aconvolution%20kernels%20to%20mine%20identity-guided%20and%20modality-consistent%0Arepresentations.%20More%20notably%2C%20a%20cross%20embedding%20balancing%20loss%20is%20introduced%0Ato%20effectively%20bridge%20cross-modal%20discrepancies%20by%20above%20embeddings.%0AExperimental%20results%20on%20SYSU-MM01%20and%20RegDB%20datasets%20show%20that%20DIAN%20achieves%0Astate-of-the-art%20performance.%20Specifically%2C%20for%20indoor%20search%20on%20SYSU-MM01%2C%20our%0Amethod%20achieves%2086.28%25%20rank-1%20accuracy%20and%2087.41%25%20mAP%2C%20respectively.%20Our%20code%0Awill%20be%20available%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Identity-Guided%2520Attention%2520Network%2520for%2520Visible-Infrared%2520Person%250A%2520%2520Re-identification%26entry.906535625%3DPeng%2520Gao%2520and%2520Yujian%2520Lee%2520and%2520Hui%2520Zhang%2520and%2520Xubo%2520Liu%2520and%2520Yiyang%2520Hu%2520and%2520Guquan%2520Jing%26entry.1292438233%3D%2520%2520Visible-infrared%2520person%2520re-identification%2520%2528VI-ReID%2529%2520aims%2520to%2520match%2520people%2520with%250Athe%2520same%2520identity%2520between%2520visible%2520and%2520infrared%2520modalities.%2520VI-ReID%2520is%2520a%250Achallenging%2520task%2520due%2520to%2520the%2520large%2520differences%2520in%2520individual%2520appearance%2520under%250Adifferent%2520modalities.%2520Existing%2520methods%2520generally%2520try%2520to%2520bridge%2520the%2520cross-modal%250Adifferences%2520at%2520image%2520or%2520feature%2520level%252C%2520which%2520lacks%2520exploring%2520the%2520discriminative%250Aembeddings.%2520Effectively%2520minimizing%2520these%2520cross-modal%2520discrepancies%2520relies%2520on%250Aobtaining%2520representations%2520that%2520are%2520guided%2520by%2520identity%2520and%2520consistent%2520across%250Amodalities%252C%2520while%2520also%2520filtering%2520out%2520representations%2520that%2520are%2520irrelevant%2520to%250Aidentity.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520dynamic%2520identity-guided%250Aattention%2520network%2520%2528DIAN%2529%2520to%2520mine%2520identity-guided%2520and%2520modality-consistent%250Aembeddings%252C%2520facilitating%2520effective%2520bridging%2520the%2520gap%2520between%2520different%250Amodalities.%2520Specifically%252C%2520in%2520DIAN%252C%2520to%2520pursue%2520a%2520semantically%2520richer%250Arepresentation%252C%2520we%2520first%2520use%2520orthogonal%2520projection%2520to%2520fuse%2520the%2520features%2520from%250Atwo%2520connected%2520coarse%2520and%2520fine%2520layers.%2520Furthermore%252C%2520we%2520first%2520use%2520dynamic%250Aconvolution%2520kernels%2520to%2520mine%2520identity-guided%2520and%2520modality-consistent%250Arepresentations.%2520More%2520notably%252C%2520a%2520cross%2520embedding%2520balancing%2520loss%2520is%2520introduced%250Ato%2520effectively%2520bridge%2520cross-modal%2520discrepancies%2520by%2520above%2520embeddings.%250AExperimental%2520results%2520on%2520SYSU-MM01%2520and%2520RegDB%2520datasets%2520show%2520that%2520DIAN%2520achieves%250Astate-of-the-art%2520performance.%2520Specifically%252C%2520for%2520indoor%2520search%2520on%2520SYSU-MM01%252C%2520our%250Amethod%2520achieves%252086.28%2525%2520rank-1%2520accuracy%2520and%252087.41%2525%2520mAP%252C%2520respectively.%2520Our%2520code%250Awill%2520be%2520available%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Identity-Guided%20Attention%20Network%20for%20Visible-Infrared%20Person%0A%20%20Re-identification&entry.906535625=Peng%20Gao%20and%20Yujian%20Lee%20and%20Hui%20Zhang%20and%20Xubo%20Liu%20and%20Yiyang%20Hu%20and%20Guquan%20Jing&entry.1292438233=%20%20Visible-infrared%20person%20re-identification%20%28VI-ReID%29%20aims%20to%20match%20people%20with%0Athe%20same%20identity%20between%20visible%20and%20infrared%20modalities.%20VI-ReID%20is%20a%0Achallenging%20task%20due%20to%20the%20large%20differences%20in%20individual%20appearance%20under%0Adifferent%20modalities.%20Existing%20methods%20generally%20try%20to%20bridge%20the%20cross-modal%0Adifferences%20at%20image%20or%20feature%20level%2C%20which%20lacks%20exploring%20the%20discriminative%0Aembeddings.%20Effectively%20minimizing%20these%20cross-modal%20discrepancies%20relies%20on%0Aobtaining%20representations%20that%20are%20guided%20by%20identity%20and%20consistent%20across%0Amodalities%2C%20while%20also%20filtering%20out%20representations%20that%20are%20irrelevant%20to%0Aidentity.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20dynamic%20identity-guided%0Aattention%20network%20%28DIAN%29%20to%20mine%20identity-guided%20and%20modality-consistent%0Aembeddings%2C%20facilitating%20effective%20bridging%20the%20gap%20between%20different%0Amodalities.%20Specifically%2C%20in%20DIAN%2C%20to%20pursue%20a%20semantically%20richer%0Arepresentation%2C%20we%20first%20use%20orthogonal%20projection%20to%20fuse%20the%20features%20from%0Atwo%20connected%20coarse%20and%20fine%20layers.%20Furthermore%2C%20we%20first%20use%20dynamic%0Aconvolution%20kernels%20to%20mine%20identity-guided%20and%20modality-consistent%0Arepresentations.%20More%20notably%2C%20a%20cross%20embedding%20balancing%20loss%20is%20introduced%0Ato%20effectively%20bridge%20cross-modal%20discrepancies%20by%20above%20embeddings.%0AExperimental%20results%20on%20SYSU-MM01%20and%20RegDB%20datasets%20show%20that%20DIAN%20achieves%0Astate-of-the-art%20performance.%20Specifically%2C%20for%20indoor%20search%20on%20SYSU-MM01%2C%20our%0Amethod%20achieves%2086.28%25%20rank-1%20accuracy%20and%2087.41%25%20mAP%2C%20respectively.%20Our%20code%0Awill%20be%20available%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12713v1&entry.124074799=Read"},
{"title": "RemoCap: Disentangled Representation Learning for Motion Capture", "author": "Hongsheng Wang and Lizao Zhang and Zhangnan Zhong and Shuolin Xu and Xinrui Zhou and Shengyu Zhang and Huahao Xu and Fei Wu and Feng Lin", "abstract": "  Reconstructing 3D human bodies from realistic motion sequences remains a\nchallenge due to pervasive and complex occlusions. Current methods struggle to\ncapture the dynamics of occluded body parts, leading to model penetration and\ndistorted motion. RemoCap leverages Spatial Disentanglement (SD) and Motion\nDisentanglement (MD) to overcome these limitations. SD addresses occlusion\ninterference between the target human body and surrounding objects. It achieves\nthis by disentangling target features along the dimension axis. By aligning\nfeatures based on their spatial positions in each dimension, SD isolates the\ntarget object's response within a global window, enabling accurate capture\ndespite occlusions. The MD module employs a channel-wise temporal shuffling\nstrategy to simulate diverse scene dynamics. This process effectively\ndisentangles motion features, allowing RemoCap to reconstruct occluded parts\nwith greater fidelity. Furthermore, this paper introduces a sequence velocity\nloss that promotes temporal coherence. This loss constrains inter-frame\nvelocity errors, ensuring the predicted motion exhibits realistic consistency.\nExtensive comparisons with state-of-the-art (SOTA) methods on benchmark\ndatasets demonstrate RemoCap's superior performance in 3D human body\nreconstruction. On the 3DPW dataset, RemoCap surpasses all competitors,\nachieving the best results in MPVPE (81.9), MPJPE (72.7), and PA-MPJPE (44.1)\nmetrics. Codes are available at https://wanghongsheng01.github.io/RemoCap/.\n", "link": "http://arxiv.org/abs/2405.12724v1", "date": "2024-05-21", "relevancy": 2.714, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5468}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5416}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.54}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RemoCap%3A%20Disentangled%20Representation%20Learning%20for%20Motion%20Capture&body=Title%3A%20RemoCap%3A%20Disentangled%20Representation%20Learning%20for%20Motion%20Capture%0AAuthor%3A%20Hongsheng%20Wang%20and%20Lizao%20Zhang%20and%20Zhangnan%20Zhong%20and%20Shuolin%20Xu%20and%20Xinrui%20Zhou%20and%20Shengyu%20Zhang%20and%20Huahao%20Xu%20and%20Fei%20Wu%20and%20Feng%20Lin%0AAbstract%3A%20%20%20Reconstructing%203D%20human%20bodies%20from%20realistic%20motion%20sequences%20remains%20a%0Achallenge%20due%20to%20pervasive%20and%20complex%20occlusions.%20Current%20methods%20struggle%20to%0Acapture%20the%20dynamics%20of%20occluded%20body%20parts%2C%20leading%20to%20model%20penetration%20and%0Adistorted%20motion.%20RemoCap%20leverages%20Spatial%20Disentanglement%20%28SD%29%20and%20Motion%0ADisentanglement%20%28MD%29%20to%20overcome%20these%20limitations.%20SD%20addresses%20occlusion%0Ainterference%20between%20the%20target%20human%20body%20and%20surrounding%20objects.%20It%20achieves%0Athis%20by%20disentangling%20target%20features%20along%20the%20dimension%20axis.%20By%20aligning%0Afeatures%20based%20on%20their%20spatial%20positions%20in%20each%20dimension%2C%20SD%20isolates%20the%0Atarget%20object%27s%20response%20within%20a%20global%20window%2C%20enabling%20accurate%20capture%0Adespite%20occlusions.%20The%20MD%20module%20employs%20a%20channel-wise%20temporal%20shuffling%0Astrategy%20to%20simulate%20diverse%20scene%20dynamics.%20This%20process%20effectively%0Adisentangles%20motion%20features%2C%20allowing%20RemoCap%20to%20reconstruct%20occluded%20parts%0Awith%20greater%20fidelity.%20Furthermore%2C%20this%20paper%20introduces%20a%20sequence%20velocity%0Aloss%20that%20promotes%20temporal%20coherence.%20This%20loss%20constrains%20inter-frame%0Avelocity%20errors%2C%20ensuring%20the%20predicted%20motion%20exhibits%20realistic%20consistency.%0AExtensive%20comparisons%20with%20state-of-the-art%20%28SOTA%29%20methods%20on%20benchmark%0Adatasets%20demonstrate%20RemoCap%27s%20superior%20performance%20in%203D%20human%20body%0Areconstruction.%20On%20the%203DPW%20dataset%2C%20RemoCap%20surpasses%20all%20competitors%2C%0Aachieving%20the%20best%20results%20in%20MPVPE%20%2881.9%29%2C%20MPJPE%20%2872.7%29%2C%20and%20PA-MPJPE%20%2844.1%29%0Ametrics.%20Codes%20are%20available%20at%20https%3A//wanghongsheng01.github.io/RemoCap/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRemoCap%253A%2520Disentangled%2520Representation%2520Learning%2520for%2520Motion%2520Capture%26entry.906535625%3DHongsheng%2520Wang%2520and%2520Lizao%2520Zhang%2520and%2520Zhangnan%2520Zhong%2520and%2520Shuolin%2520Xu%2520and%2520Xinrui%2520Zhou%2520and%2520Shengyu%2520Zhang%2520and%2520Huahao%2520Xu%2520and%2520Fei%2520Wu%2520and%2520Feng%2520Lin%26entry.1292438233%3D%2520%2520Reconstructing%25203D%2520human%2520bodies%2520from%2520realistic%2520motion%2520sequences%2520remains%2520a%250Achallenge%2520due%2520to%2520pervasive%2520and%2520complex%2520occlusions.%2520Current%2520methods%2520struggle%2520to%250Acapture%2520the%2520dynamics%2520of%2520occluded%2520body%2520parts%252C%2520leading%2520to%2520model%2520penetration%2520and%250Adistorted%2520motion.%2520RemoCap%2520leverages%2520Spatial%2520Disentanglement%2520%2528SD%2529%2520and%2520Motion%250ADisentanglement%2520%2528MD%2529%2520to%2520overcome%2520these%2520limitations.%2520SD%2520addresses%2520occlusion%250Ainterference%2520between%2520the%2520target%2520human%2520body%2520and%2520surrounding%2520objects.%2520It%2520achieves%250Athis%2520by%2520disentangling%2520target%2520features%2520along%2520the%2520dimension%2520axis.%2520By%2520aligning%250Afeatures%2520based%2520on%2520their%2520spatial%2520positions%2520in%2520each%2520dimension%252C%2520SD%2520isolates%2520the%250Atarget%2520object%2527s%2520response%2520within%2520a%2520global%2520window%252C%2520enabling%2520accurate%2520capture%250Adespite%2520occlusions.%2520The%2520MD%2520module%2520employs%2520a%2520channel-wise%2520temporal%2520shuffling%250Astrategy%2520to%2520simulate%2520diverse%2520scene%2520dynamics.%2520This%2520process%2520effectively%250Adisentangles%2520motion%2520features%252C%2520allowing%2520RemoCap%2520to%2520reconstruct%2520occluded%2520parts%250Awith%2520greater%2520fidelity.%2520Furthermore%252C%2520this%2520paper%2520introduces%2520a%2520sequence%2520velocity%250Aloss%2520that%2520promotes%2520temporal%2520coherence.%2520This%2520loss%2520constrains%2520inter-frame%250Avelocity%2520errors%252C%2520ensuring%2520the%2520predicted%2520motion%2520exhibits%2520realistic%2520consistency.%250AExtensive%2520comparisons%2520with%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520on%2520benchmark%250Adatasets%2520demonstrate%2520RemoCap%2527s%2520superior%2520performance%2520in%25203D%2520human%2520body%250Areconstruction.%2520On%2520the%25203DPW%2520dataset%252C%2520RemoCap%2520surpasses%2520all%2520competitors%252C%250Aachieving%2520the%2520best%2520results%2520in%2520MPVPE%2520%252881.9%2529%252C%2520MPJPE%2520%252872.7%2529%252C%2520and%2520PA-MPJPE%2520%252844.1%2529%250Ametrics.%2520Codes%2520are%2520available%2520at%2520https%253A//wanghongsheng01.github.io/RemoCap/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RemoCap%3A%20Disentangled%20Representation%20Learning%20for%20Motion%20Capture&entry.906535625=Hongsheng%20Wang%20and%20Lizao%20Zhang%20and%20Zhangnan%20Zhong%20and%20Shuolin%20Xu%20and%20Xinrui%20Zhou%20and%20Shengyu%20Zhang%20and%20Huahao%20Xu%20and%20Fei%20Wu%20and%20Feng%20Lin&entry.1292438233=%20%20Reconstructing%203D%20human%20bodies%20from%20realistic%20motion%20sequences%20remains%20a%0Achallenge%20due%20to%20pervasive%20and%20complex%20occlusions.%20Current%20methods%20struggle%20to%0Acapture%20the%20dynamics%20of%20occluded%20body%20parts%2C%20leading%20to%20model%20penetration%20and%0Adistorted%20motion.%20RemoCap%20leverages%20Spatial%20Disentanglement%20%28SD%29%20and%20Motion%0ADisentanglement%20%28MD%29%20to%20overcome%20these%20limitations.%20SD%20addresses%20occlusion%0Ainterference%20between%20the%20target%20human%20body%20and%20surrounding%20objects.%20It%20achieves%0Athis%20by%20disentangling%20target%20features%20along%20the%20dimension%20axis.%20By%20aligning%0Afeatures%20based%20on%20their%20spatial%20positions%20in%20each%20dimension%2C%20SD%20isolates%20the%0Atarget%20object%27s%20response%20within%20a%20global%20window%2C%20enabling%20accurate%20capture%0Adespite%20occlusions.%20The%20MD%20module%20employs%20a%20channel-wise%20temporal%20shuffling%0Astrategy%20to%20simulate%20diverse%20scene%20dynamics.%20This%20process%20effectively%0Adisentangles%20motion%20features%2C%20allowing%20RemoCap%20to%20reconstruct%20occluded%20parts%0Awith%20greater%20fidelity.%20Furthermore%2C%20this%20paper%20introduces%20a%20sequence%20velocity%0Aloss%20that%20promotes%20temporal%20coherence.%20This%20loss%20constrains%20inter-frame%0Avelocity%20errors%2C%20ensuring%20the%20predicted%20motion%20exhibits%20realistic%20consistency.%0AExtensive%20comparisons%20with%20state-of-the-art%20%28SOTA%29%20methods%20on%20benchmark%0Adatasets%20demonstrate%20RemoCap%27s%20superior%20performance%20in%203D%20human%20body%0Areconstruction.%20On%20the%203DPW%20dataset%2C%20RemoCap%20surpasses%20all%20competitors%2C%0Aachieving%20the%20best%20results%20in%20MPVPE%20%2881.9%29%2C%20MPJPE%20%2872.7%29%2C%20and%20PA-MPJPE%20%2844.1%29%0Ametrics.%20Codes%20are%20available%20at%20https%3A//wanghongsheng01.github.io/RemoCap/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12724v1&entry.124074799=Read"},
{"title": "C3L: Content Correlated Vision-Language Instruction Tuning Data\n  Generation via Contrastive Learning", "author": "Ji Ma and Wei Suo and Peng Wang and Yanning Zhang", "abstract": "  Vision-Language Instruction Tuning (VLIT) is a critical training phase for\nLarge Vision-Language Models (LVLMs). With the improving capabilities of\nopen-source LVLMs, researchers have increasingly turned to generate VLIT data\nby using open-source LVLMs and achieved significant progress. However, such\ndata generation approaches are bottlenecked by the following challenges: 1)\nSince multi-modal models tend to be influenced by prior language knowledge,\ndirectly using LVLMs to generate VLIT data would inevitably lead to low content\nrelevance between generated data and images. 2) To improve the ability of the\nmodels to generate VLIT data, previous methods have incorporated an additional\ntraining phase to boost the generative capacity. This process hurts the\ngeneralization of the models to unseen inputs (i.e., \"exposure bias\" problem).\nIn this paper, we propose a new Content Correlated VLIT data generation via\nContrastive Learning (C3L). Specifically, we design a new content relevance\nmodule which enhances the content relevance between VLIT data and images by\ncomputing Image Instruction Correspondence Scores S(I2C). Moreover, a\ncontrastive learning module is introduced to further boost the VLIT data\ngeneration capability of the LVLMs. A large number of automatic measures on\nfour benchmarks show the effectiveness of our method.\n", "link": "http://arxiv.org/abs/2405.12752v1", "date": "2024-05-21", "relevancy": 2.6989, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5514}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5352}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C3L%3A%20Content%20Correlated%20Vision-Language%20Instruction%20Tuning%20Data%0A%20%20Generation%20via%20Contrastive%20Learning&body=Title%3A%20C3L%3A%20Content%20Correlated%20Vision-Language%20Instruction%20Tuning%20Data%0A%20%20Generation%20via%20Contrastive%20Learning%0AAuthor%3A%20Ji%20Ma%20and%20Wei%20Suo%20and%20Peng%20Wang%20and%20Yanning%20Zhang%0AAbstract%3A%20%20%20Vision-Language%20Instruction%20Tuning%20%28VLIT%29%20is%20a%20critical%20training%20phase%20for%0ALarge%20Vision-Language%20Models%20%28LVLMs%29.%20With%20the%20improving%20capabilities%20of%0Aopen-source%20LVLMs%2C%20researchers%20have%20increasingly%20turned%20to%20generate%20VLIT%20data%0Aby%20using%20open-source%20LVLMs%20and%20achieved%20significant%20progress.%20However%2C%20such%0Adata%20generation%20approaches%20are%20bottlenecked%20by%20the%20following%20challenges%3A%201%29%0ASince%20multi-modal%20models%20tend%20to%20be%20influenced%20by%20prior%20language%20knowledge%2C%0Adirectly%20using%20LVLMs%20to%20generate%20VLIT%20data%20would%20inevitably%20lead%20to%20low%20content%0Arelevance%20between%20generated%20data%20and%20images.%202%29%20To%20improve%20the%20ability%20of%20the%0Amodels%20to%20generate%20VLIT%20data%2C%20previous%20methods%20have%20incorporated%20an%20additional%0Atraining%20phase%20to%20boost%20the%20generative%20capacity.%20This%20process%20hurts%20the%0Ageneralization%20of%20the%20models%20to%20unseen%20inputs%20%28i.e.%2C%20%22exposure%20bias%22%20problem%29.%0AIn%20this%20paper%2C%20we%20propose%20a%20new%20Content%20Correlated%20VLIT%20data%20generation%20via%0AContrastive%20Learning%20%28C3L%29.%20Specifically%2C%20we%20design%20a%20new%20content%20relevance%0Amodule%20which%20enhances%20the%20content%20relevance%20between%20VLIT%20data%20and%20images%20by%0Acomputing%20Image%20Instruction%20Correspondence%20Scores%20S%28I2C%29.%20Moreover%2C%20a%0Acontrastive%20learning%20module%20is%20introduced%20to%20further%20boost%20the%20VLIT%20data%0Ageneration%20capability%20of%20the%20LVLMs.%20A%20large%20number%20of%20automatic%20measures%20on%0Afour%20benchmarks%20show%20the%20effectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC3L%253A%2520Content%2520Correlated%2520Vision-Language%2520Instruction%2520Tuning%2520Data%250A%2520%2520Generation%2520via%2520Contrastive%2520Learning%26entry.906535625%3DJi%2520Ma%2520and%2520Wei%2520Suo%2520and%2520Peng%2520Wang%2520and%2520Yanning%2520Zhang%26entry.1292438233%3D%2520%2520Vision-Language%2520Instruction%2520Tuning%2520%2528VLIT%2529%2520is%2520a%2520critical%2520training%2520phase%2520for%250ALarge%2520Vision-Language%2520Models%2520%2528LVLMs%2529.%2520With%2520the%2520improving%2520capabilities%2520of%250Aopen-source%2520LVLMs%252C%2520researchers%2520have%2520increasingly%2520turned%2520to%2520generate%2520VLIT%2520data%250Aby%2520using%2520open-source%2520LVLMs%2520and%2520achieved%2520significant%2520progress.%2520However%252C%2520such%250Adata%2520generation%2520approaches%2520are%2520bottlenecked%2520by%2520the%2520following%2520challenges%253A%25201%2529%250ASince%2520multi-modal%2520models%2520tend%2520to%2520be%2520influenced%2520by%2520prior%2520language%2520knowledge%252C%250Adirectly%2520using%2520LVLMs%2520to%2520generate%2520VLIT%2520data%2520would%2520inevitably%2520lead%2520to%2520low%2520content%250Arelevance%2520between%2520generated%2520data%2520and%2520images.%25202%2529%2520To%2520improve%2520the%2520ability%2520of%2520the%250Amodels%2520to%2520generate%2520VLIT%2520data%252C%2520previous%2520methods%2520have%2520incorporated%2520an%2520additional%250Atraining%2520phase%2520to%2520boost%2520the%2520generative%2520capacity.%2520This%2520process%2520hurts%2520the%250Ageneralization%2520of%2520the%2520models%2520to%2520unseen%2520inputs%2520%2528i.e.%252C%2520%2522exposure%2520bias%2522%2520problem%2529.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520Content%2520Correlated%2520VLIT%2520data%2520generation%2520via%250AContrastive%2520Learning%2520%2528C3L%2529.%2520Specifically%252C%2520we%2520design%2520a%2520new%2520content%2520relevance%250Amodule%2520which%2520enhances%2520the%2520content%2520relevance%2520between%2520VLIT%2520data%2520and%2520images%2520by%250Acomputing%2520Image%2520Instruction%2520Correspondence%2520Scores%2520S%2528I2C%2529.%2520Moreover%252C%2520a%250Acontrastive%2520learning%2520module%2520is%2520introduced%2520to%2520further%2520boost%2520the%2520VLIT%2520data%250Ageneration%2520capability%2520of%2520the%2520LVLMs.%2520A%2520large%2520number%2520of%2520automatic%2520measures%2520on%250Afour%2520benchmarks%2520show%2520the%2520effectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C3L%3A%20Content%20Correlated%20Vision-Language%20Instruction%20Tuning%20Data%0A%20%20Generation%20via%20Contrastive%20Learning&entry.906535625=Ji%20Ma%20and%20Wei%20Suo%20and%20Peng%20Wang%20and%20Yanning%20Zhang&entry.1292438233=%20%20Vision-Language%20Instruction%20Tuning%20%28VLIT%29%20is%20a%20critical%20training%20phase%20for%0ALarge%20Vision-Language%20Models%20%28LVLMs%29.%20With%20the%20improving%20capabilities%20of%0Aopen-source%20LVLMs%2C%20researchers%20have%20increasingly%20turned%20to%20generate%20VLIT%20data%0Aby%20using%20open-source%20LVLMs%20and%20achieved%20significant%20progress.%20However%2C%20such%0Adata%20generation%20approaches%20are%20bottlenecked%20by%20the%20following%20challenges%3A%201%29%0ASince%20multi-modal%20models%20tend%20to%20be%20influenced%20by%20prior%20language%20knowledge%2C%0Adirectly%20using%20LVLMs%20to%20generate%20VLIT%20data%20would%20inevitably%20lead%20to%20low%20content%0Arelevance%20between%20generated%20data%20and%20images.%202%29%20To%20improve%20the%20ability%20of%20the%0Amodels%20to%20generate%20VLIT%20data%2C%20previous%20methods%20have%20incorporated%20an%20additional%0Atraining%20phase%20to%20boost%20the%20generative%20capacity.%20This%20process%20hurts%20the%0Ageneralization%20of%20the%20models%20to%20unseen%20inputs%20%28i.e.%2C%20%22exposure%20bias%22%20problem%29.%0AIn%20this%20paper%2C%20we%20propose%20a%20new%20Content%20Correlated%20VLIT%20data%20generation%20via%0AContrastive%20Learning%20%28C3L%29.%20Specifically%2C%20we%20design%20a%20new%20content%20relevance%0Amodule%20which%20enhances%20the%20content%20relevance%20between%20VLIT%20data%20and%20images%20by%0Acomputing%20Image%20Instruction%20Correspondence%20Scores%20S%28I2C%29.%20Moreover%2C%20a%0Acontrastive%20learning%20module%20is%20introduced%20to%20further%20boost%20the%20VLIT%20data%0Ageneration%20capability%20of%20the%20LVLMs.%20A%20large%20number%20of%20automatic%20measures%20on%0Afour%20benchmarks%20show%20the%20effectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12752v1&entry.124074799=Read"},
{"title": "Neural Bounding", "author": "Stephanie Wenxin Liu and Michael Fischer and Paul D. Yoo and Tobias Ritschel", "abstract": "  Bounding volumes are an established concept in computer graphics and vision\ntasks but have seen little change since their early inception. In this work, we\nstudy the use of neural networks as bounding volumes. Our key observation is\nthat bounding, which so far has primarily been considered a problem of\ncomputational geometry, can be redefined as a problem of learning to classify\nspace into free or occupied. This learning-based approach is particularly\nadvantageous in high-dimensional spaces, such as animated scenes with complex\nqueries, where neural networks are known to excel. However, unlocking neural\nbounding requires a twist: allowing -- but also limiting -- false positives,\nwhile ensuring that the number of false negatives is strictly zero. We enable\nsuch tight and conservative results using a dynamically-weighted asymmetric\nloss function. Our results show that our neural bounding produces up to an\norder of magnitude fewer false positives than traditional methods. In addition,\nwe propose an extension of our bounding method using early exits that\naccelerates query speeds by 25%. We also demonstrate that our approach is\napplicable to non-deep learning models that train within seconds. Our project\npage is at: https://wenxin-liu.github.io/neural_bounding/.\n", "link": "http://arxiv.org/abs/2310.06822v3", "date": "2024-05-21", "relevancy": 2.6957, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5447}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5442}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Bounding&body=Title%3A%20Neural%20Bounding%0AAuthor%3A%20Stephanie%20Wenxin%20Liu%20and%20Michael%20Fischer%20and%20Paul%20D.%20Yoo%20and%20Tobias%20Ritschel%0AAbstract%3A%20%20%20Bounding%20volumes%20are%20an%20established%20concept%20in%20computer%20graphics%20and%20vision%0Atasks%20but%20have%20seen%20little%20change%20since%20their%20early%20inception.%20In%20this%20work%2C%20we%0Astudy%20the%20use%20of%20neural%20networks%20as%20bounding%20volumes.%20Our%20key%20observation%20is%0Athat%20bounding%2C%20which%20so%20far%20has%20primarily%20been%20considered%20a%20problem%20of%0Acomputational%20geometry%2C%20can%20be%20redefined%20as%20a%20problem%20of%20learning%20to%20classify%0Aspace%20into%20free%20or%20occupied.%20This%20learning-based%20approach%20is%20particularly%0Aadvantageous%20in%20high-dimensional%20spaces%2C%20such%20as%20animated%20scenes%20with%20complex%0Aqueries%2C%20where%20neural%20networks%20are%20known%20to%20excel.%20However%2C%20unlocking%20neural%0Abounding%20requires%20a%20twist%3A%20allowing%20--%20but%20also%20limiting%20--%20false%20positives%2C%0Awhile%20ensuring%20that%20the%20number%20of%20false%20negatives%20is%20strictly%20zero.%20We%20enable%0Asuch%20tight%20and%20conservative%20results%20using%20a%20dynamically-weighted%20asymmetric%0Aloss%20function.%20Our%20results%20show%20that%20our%20neural%20bounding%20produces%20up%20to%20an%0Aorder%20of%20magnitude%20fewer%20false%20positives%20than%20traditional%20methods.%20In%20addition%2C%0Awe%20propose%20an%20extension%20of%20our%20bounding%20method%20using%20early%20exits%20that%0Aaccelerates%20query%20speeds%20by%2025%25.%20We%20also%20demonstrate%20that%20our%20approach%20is%0Aapplicable%20to%20non-deep%20learning%20models%20that%20train%20within%20seconds.%20Our%20project%0Apage%20is%20at%3A%20https%3A//wenxin-liu.github.io/neural_bounding/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.06822v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Bounding%26entry.906535625%3DStephanie%2520Wenxin%2520Liu%2520and%2520Michael%2520Fischer%2520and%2520Paul%2520D.%2520Yoo%2520and%2520Tobias%2520Ritschel%26entry.1292438233%3D%2520%2520Bounding%2520volumes%2520are%2520an%2520established%2520concept%2520in%2520computer%2520graphics%2520and%2520vision%250Atasks%2520but%2520have%2520seen%2520little%2520change%2520since%2520their%2520early%2520inception.%2520In%2520this%2520work%252C%2520we%250Astudy%2520the%2520use%2520of%2520neural%2520networks%2520as%2520bounding%2520volumes.%2520Our%2520key%2520observation%2520is%250Athat%2520bounding%252C%2520which%2520so%2520far%2520has%2520primarily%2520been%2520considered%2520a%2520problem%2520of%250Acomputational%2520geometry%252C%2520can%2520be%2520redefined%2520as%2520a%2520problem%2520of%2520learning%2520to%2520classify%250Aspace%2520into%2520free%2520or%2520occupied.%2520This%2520learning-based%2520approach%2520is%2520particularly%250Aadvantageous%2520in%2520high-dimensional%2520spaces%252C%2520such%2520as%2520animated%2520scenes%2520with%2520complex%250Aqueries%252C%2520where%2520neural%2520networks%2520are%2520known%2520to%2520excel.%2520However%252C%2520unlocking%2520neural%250Abounding%2520requires%2520a%2520twist%253A%2520allowing%2520--%2520but%2520also%2520limiting%2520--%2520false%2520positives%252C%250Awhile%2520ensuring%2520that%2520the%2520number%2520of%2520false%2520negatives%2520is%2520strictly%2520zero.%2520We%2520enable%250Asuch%2520tight%2520and%2520conservative%2520results%2520using%2520a%2520dynamically-weighted%2520asymmetric%250Aloss%2520function.%2520Our%2520results%2520show%2520that%2520our%2520neural%2520bounding%2520produces%2520up%2520to%2520an%250Aorder%2520of%2520magnitude%2520fewer%2520false%2520positives%2520than%2520traditional%2520methods.%2520In%2520addition%252C%250Awe%2520propose%2520an%2520extension%2520of%2520our%2520bounding%2520method%2520using%2520early%2520exits%2520that%250Aaccelerates%2520query%2520speeds%2520by%252025%2525.%2520We%2520also%2520demonstrate%2520that%2520our%2520approach%2520is%250Aapplicable%2520to%2520non-deep%2520learning%2520models%2520that%2520train%2520within%2520seconds.%2520Our%2520project%250Apage%2520is%2520at%253A%2520https%253A//wenxin-liu.github.io/neural_bounding/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.06822v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Bounding&entry.906535625=Stephanie%20Wenxin%20Liu%20and%20Michael%20Fischer%20and%20Paul%20D.%20Yoo%20and%20Tobias%20Ritschel&entry.1292438233=%20%20Bounding%20volumes%20are%20an%20established%20concept%20in%20computer%20graphics%20and%20vision%0Atasks%20but%20have%20seen%20little%20change%20since%20their%20early%20inception.%20In%20this%20work%2C%20we%0Astudy%20the%20use%20of%20neural%20networks%20as%20bounding%20volumes.%20Our%20key%20observation%20is%0Athat%20bounding%2C%20which%20so%20far%20has%20primarily%20been%20considered%20a%20problem%20of%0Acomputational%20geometry%2C%20can%20be%20redefined%20as%20a%20problem%20of%20learning%20to%20classify%0Aspace%20into%20free%20or%20occupied.%20This%20learning-based%20approach%20is%20particularly%0Aadvantageous%20in%20high-dimensional%20spaces%2C%20such%20as%20animated%20scenes%20with%20complex%0Aqueries%2C%20where%20neural%20networks%20are%20known%20to%20excel.%20However%2C%20unlocking%20neural%0Abounding%20requires%20a%20twist%3A%20allowing%20--%20but%20also%20limiting%20--%20false%20positives%2C%0Awhile%20ensuring%20that%20the%20number%20of%20false%20negatives%20is%20strictly%20zero.%20We%20enable%0Asuch%20tight%20and%20conservative%20results%20using%20a%20dynamically-weighted%20asymmetric%0Aloss%20function.%20Our%20results%20show%20that%20our%20neural%20bounding%20produces%20up%20to%20an%0Aorder%20of%20magnitude%20fewer%20false%20positives%20than%20traditional%20methods.%20In%20addition%2C%0Awe%20propose%20an%20extension%20of%20our%20bounding%20method%20using%20early%20exits%20that%0Aaccelerates%20query%20speeds%20by%2025%25.%20We%20also%20demonstrate%20that%20our%20approach%20is%0Aapplicable%20to%20non-deep%20learning%20models%20that%20train%20within%20seconds.%20Our%20project%0Apage%20is%20at%3A%20https%3A//wenxin-liu.github.io/neural_bounding/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.06822v3&entry.124074799=Read"},
{"title": "CLIP in Medical Imaging: A Comprehensive Survey", "author": "Zihao Zhao and Yuxiao Liu and Han Wu and Yonghao Li and Sheng Wang and Lin Teng and Disheng Liu and Zhiming Cui and Qian Wang and Dinggang Shen", "abstract": "  Contrastive Language-Image Pre-training (CLIP), a simple yet effective\npre-training paradigm, successfully introduces text supervision to vision\nmodels. It has shown promising results across various tasks, attributable to\nits generalizability and interpretability. The use of CLIP has recently gained\nincreasing interest in the medical imaging domain, serving both as a\npre-training paradigm for aligning medical vision and language, and as a\ncritical component in diverse clinical tasks. With the aim of facilitating a\ndeeper understanding of this promising direction, this survey offers an\nin-depth exploration of the CLIP paradigm within the domain of medical imaging,\nregarding both refined CLIP pre-training and CLIP-driven applications. In this\nstudy, We (1) start with a brief introduction to the fundamentals of CLIP\nmethodology. (2) Then, we investigate the adaptation of CLIP pre-training in\nthe medical domain, focusing on how to optimize CLIP given characteristics of\nmedical images and reports. (3) Furthermore, we explore the practical\nutilization of CLIP pre-trained models in various tasks, including\nclassification, dense prediction, and cross-modal tasks. (4) Finally, we\ndiscuss existing limitations of CLIP in the context of medical imaging and\npropose forward-looking directions to address the demands of medical imaging\ndomain. We expect that this comprehensive survey will provide researchers in\nthe field of medical image analysis with a holistic understanding of the CLIP\nparadigm and its potential implications. The project page can be found on\nhttps://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging.\n", "link": "http://arxiv.org/abs/2312.07353v4", "date": "2024-05-21", "relevancy": 2.6038, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5945}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4924}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP%20in%20Medical%20Imaging%3A%20A%20Comprehensive%20Survey&body=Title%3A%20CLIP%20in%20Medical%20Imaging%3A%20A%20Comprehensive%20Survey%0AAuthor%3A%20Zihao%20Zhao%20and%20Yuxiao%20Liu%20and%20Han%20Wu%20and%20Yonghao%20Li%20and%20Sheng%20Wang%20and%20Lin%20Teng%20and%20Disheng%20Liu%20and%20Zhiming%20Cui%20and%20Qian%20Wang%20and%20Dinggang%20Shen%0AAbstract%3A%20%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%2C%20a%20simple%20yet%20effective%0Apre-training%20paradigm%2C%20successfully%20introduces%20text%20supervision%20to%20vision%0Amodels.%20It%20has%20shown%20promising%20results%20across%20various%20tasks%2C%20attributable%20to%0Aits%20generalizability%20and%20interpretability.%20The%20use%20of%20CLIP%20has%20recently%20gained%0Aincreasing%20interest%20in%20the%20medical%20imaging%20domain%2C%20serving%20both%20as%20a%0Apre-training%20paradigm%20for%20aligning%20medical%20vision%20and%20language%2C%20and%20as%20a%0Acritical%20component%20in%20diverse%20clinical%20tasks.%20With%20the%20aim%20of%20facilitating%20a%0Adeeper%20understanding%20of%20this%20promising%20direction%2C%20this%20survey%20offers%20an%0Ain-depth%20exploration%20of%20the%20CLIP%20paradigm%20within%20the%20domain%20of%20medical%20imaging%2C%0Aregarding%20both%20refined%20CLIP%20pre-training%20and%20CLIP-driven%20applications.%20In%20this%0Astudy%2C%20We%20%281%29%20start%20with%20a%20brief%20introduction%20to%20the%20fundamentals%20of%20CLIP%0Amethodology.%20%282%29%20Then%2C%20we%20investigate%20the%20adaptation%20of%20CLIP%20pre-training%20in%0Athe%20medical%20domain%2C%20focusing%20on%20how%20to%20optimize%20CLIP%20given%20characteristics%20of%0Amedical%20images%20and%20reports.%20%283%29%20Furthermore%2C%20we%20explore%20the%20practical%0Autilization%20of%20CLIP%20pre-trained%20models%20in%20various%20tasks%2C%20including%0Aclassification%2C%20dense%20prediction%2C%20and%20cross-modal%20tasks.%20%284%29%20Finally%2C%20we%0Adiscuss%20existing%20limitations%20of%20CLIP%20in%20the%20context%20of%20medical%20imaging%20and%0Apropose%20forward-looking%20directions%20to%20address%20the%20demands%20of%20medical%20imaging%0Adomain.%20We%20expect%20that%20this%20comprehensive%20survey%20will%20provide%20researchers%20in%0Athe%20field%20of%20medical%20image%20analysis%20with%20a%20holistic%20understanding%20of%20the%20CLIP%0Aparadigm%20and%20its%20potential%20implications.%20The%20project%20page%20can%20be%20found%20on%0Ahttps%3A//github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07353v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP%2520in%2520Medical%2520Imaging%253A%2520A%2520Comprehensive%2520Survey%26entry.906535625%3DZihao%2520Zhao%2520and%2520Yuxiao%2520Liu%2520and%2520Han%2520Wu%2520and%2520Yonghao%2520Li%2520and%2520Sheng%2520Wang%2520and%2520Lin%2520Teng%2520and%2520Disheng%2520Liu%2520and%2520Zhiming%2520Cui%2520and%2520Qian%2520Wang%2520and%2520Dinggang%2520Shen%26entry.1292438233%3D%2520%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%252C%2520a%2520simple%2520yet%2520effective%250Apre-training%2520paradigm%252C%2520successfully%2520introduces%2520text%2520supervision%2520to%2520vision%250Amodels.%2520It%2520has%2520shown%2520promising%2520results%2520across%2520various%2520tasks%252C%2520attributable%2520to%250Aits%2520generalizability%2520and%2520interpretability.%2520The%2520use%2520of%2520CLIP%2520has%2520recently%2520gained%250Aincreasing%2520interest%2520in%2520the%2520medical%2520imaging%2520domain%252C%2520serving%2520both%2520as%2520a%250Apre-training%2520paradigm%2520for%2520aligning%2520medical%2520vision%2520and%2520language%252C%2520and%2520as%2520a%250Acritical%2520component%2520in%2520diverse%2520clinical%2520tasks.%2520With%2520the%2520aim%2520of%2520facilitating%2520a%250Adeeper%2520understanding%2520of%2520this%2520promising%2520direction%252C%2520this%2520survey%2520offers%2520an%250Ain-depth%2520exploration%2520of%2520the%2520CLIP%2520paradigm%2520within%2520the%2520domain%2520of%2520medical%2520imaging%252C%250Aregarding%2520both%2520refined%2520CLIP%2520pre-training%2520and%2520CLIP-driven%2520applications.%2520In%2520this%250Astudy%252C%2520We%2520%25281%2529%2520start%2520with%2520a%2520brief%2520introduction%2520to%2520the%2520fundamentals%2520of%2520CLIP%250Amethodology.%2520%25282%2529%2520Then%252C%2520we%2520investigate%2520the%2520adaptation%2520of%2520CLIP%2520pre-training%2520in%250Athe%2520medical%2520domain%252C%2520focusing%2520on%2520how%2520to%2520optimize%2520CLIP%2520given%2520characteristics%2520of%250Amedical%2520images%2520and%2520reports.%2520%25283%2529%2520Furthermore%252C%2520we%2520explore%2520the%2520practical%250Autilization%2520of%2520CLIP%2520pre-trained%2520models%2520in%2520various%2520tasks%252C%2520including%250Aclassification%252C%2520dense%2520prediction%252C%2520and%2520cross-modal%2520tasks.%2520%25284%2529%2520Finally%252C%2520we%250Adiscuss%2520existing%2520limitations%2520of%2520CLIP%2520in%2520the%2520context%2520of%2520medical%2520imaging%2520and%250Apropose%2520forward-looking%2520directions%2520to%2520address%2520the%2520demands%2520of%2520medical%2520imaging%250Adomain.%2520We%2520expect%2520that%2520this%2520comprehensive%2520survey%2520will%2520provide%2520researchers%2520in%250Athe%2520field%2520of%2520medical%2520image%2520analysis%2520with%2520a%2520holistic%2520understanding%2520of%2520the%2520CLIP%250Aparadigm%2520and%2520its%2520potential%2520implications.%2520The%2520project%2520page%2520can%2520be%2520found%2520on%250Ahttps%253A//github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07353v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP%20in%20Medical%20Imaging%3A%20A%20Comprehensive%20Survey&entry.906535625=Zihao%20Zhao%20and%20Yuxiao%20Liu%20and%20Han%20Wu%20and%20Yonghao%20Li%20and%20Sheng%20Wang%20and%20Lin%20Teng%20and%20Disheng%20Liu%20and%20Zhiming%20Cui%20and%20Qian%20Wang%20and%20Dinggang%20Shen&entry.1292438233=%20%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%2C%20a%20simple%20yet%20effective%0Apre-training%20paradigm%2C%20successfully%20introduces%20text%20supervision%20to%20vision%0Amodels.%20It%20has%20shown%20promising%20results%20across%20various%20tasks%2C%20attributable%20to%0Aits%20generalizability%20and%20interpretability.%20The%20use%20of%20CLIP%20has%20recently%20gained%0Aincreasing%20interest%20in%20the%20medical%20imaging%20domain%2C%20serving%20both%20as%20a%0Apre-training%20paradigm%20for%20aligning%20medical%20vision%20and%20language%2C%20and%20as%20a%0Acritical%20component%20in%20diverse%20clinical%20tasks.%20With%20the%20aim%20of%20facilitating%20a%0Adeeper%20understanding%20of%20this%20promising%20direction%2C%20this%20survey%20offers%20an%0Ain-depth%20exploration%20of%20the%20CLIP%20paradigm%20within%20the%20domain%20of%20medical%20imaging%2C%0Aregarding%20both%20refined%20CLIP%20pre-training%20and%20CLIP-driven%20applications.%20In%20this%0Astudy%2C%20We%20%281%29%20start%20with%20a%20brief%20introduction%20to%20the%20fundamentals%20of%20CLIP%0Amethodology.%20%282%29%20Then%2C%20we%20investigate%20the%20adaptation%20of%20CLIP%20pre-training%20in%0Athe%20medical%20domain%2C%20focusing%20on%20how%20to%20optimize%20CLIP%20given%20characteristics%20of%0Amedical%20images%20and%20reports.%20%283%29%20Furthermore%2C%20we%20explore%20the%20practical%0Autilization%20of%20CLIP%20pre-trained%20models%20in%20various%20tasks%2C%20including%0Aclassification%2C%20dense%20prediction%2C%20and%20cross-modal%20tasks.%20%284%29%20Finally%2C%20we%0Adiscuss%20existing%20limitations%20of%20CLIP%20in%20the%20context%20of%20medical%20imaging%20and%0Apropose%20forward-looking%20directions%20to%20address%20the%20demands%20of%20medical%20imaging%0Adomain.%20We%20expect%20that%20this%20comprehensive%20survey%20will%20provide%20researchers%20in%0Athe%20field%20of%20medical%20image%20analysis%20with%20a%20holistic%20understanding%20of%20the%20CLIP%0Aparadigm%20and%20its%20potential%20implications.%20The%20project%20page%20can%20be%20found%20on%0Ahttps%3A//github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07353v4&entry.124074799=Read"},
{"title": "SARA: Controllable Makeup Transfer with Spatial Alignment and\n  Region-Adaptive Normalization", "author": "Xiaojing Zhong and Xinyi Huang and Zhonghua Wu and Guosheng Lin and Qingyao Wu", "abstract": "  Makeup transfer is a process of transferring the makeup style from a\nreference image to the source images, while preserving the source images'\nidentities. This technique is highly desirable and finds many applications.\nHowever, existing methods lack fine-level control of the makeup style, making\nit challenging to achieve high-quality results when dealing with large spatial\nmisalignments. To address this problem, we propose a novel Spatial Alignment\nand Region-Adaptive normalization method (SARA) in this paper. Our method\ngenerates detailed makeup transfer results that can handle large spatial\nmisalignments and achieve part-specific and shade-controllable makeup transfer.\nSpecifically, SARA comprises three modules: Firstly, a spatial alignment module\nthat preserves the spatial context of makeup and provides a target semantic map\nfor guiding the shape-independent style codes. Secondly, a region-adaptive\nnormalization module that decouples shape and makeup style using per-region\nencoding and normalization, which facilitates the elimination of spatial\nmisalignments. Lastly, a makeup fusion module blends identity features and\nmakeup style by injecting learned scale and bias parameters. Experimental\nresults show that our SARA method outperforms existing methods and achieves\nstate-of-the-art performance on two public datasets.\n", "link": "http://arxiv.org/abs/2311.16828v2", "date": "2024-05-21", "relevancy": 2.5879, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5312}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5216}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SARA%3A%20Controllable%20Makeup%20Transfer%20with%20Spatial%20Alignment%20and%0A%20%20Region-Adaptive%20Normalization&body=Title%3A%20SARA%3A%20Controllable%20Makeup%20Transfer%20with%20Spatial%20Alignment%20and%0A%20%20Region-Adaptive%20Normalization%0AAuthor%3A%20Xiaojing%20Zhong%20and%20Xinyi%20Huang%20and%20Zhonghua%20Wu%20and%20Guosheng%20Lin%20and%20Qingyao%20Wu%0AAbstract%3A%20%20%20Makeup%20transfer%20is%20a%20process%20of%20transferring%20the%20makeup%20style%20from%20a%0Areference%20image%20to%20the%20source%20images%2C%20while%20preserving%20the%20source%20images%27%0Aidentities.%20This%20technique%20is%20highly%20desirable%20and%20finds%20many%20applications.%0AHowever%2C%20existing%20methods%20lack%20fine-level%20control%20of%20the%20makeup%20style%2C%20making%0Ait%20challenging%20to%20achieve%20high-quality%20results%20when%20dealing%20with%20large%20spatial%0Amisalignments.%20To%20address%20this%20problem%2C%20we%20propose%20a%20novel%20Spatial%20Alignment%0Aand%20Region-Adaptive%20normalization%20method%20%28SARA%29%20in%20this%20paper.%20Our%20method%0Agenerates%20detailed%20makeup%20transfer%20results%20that%20can%20handle%20large%20spatial%0Amisalignments%20and%20achieve%20part-specific%20and%20shade-controllable%20makeup%20transfer.%0ASpecifically%2C%20SARA%20comprises%20three%20modules%3A%20Firstly%2C%20a%20spatial%20alignment%20module%0Athat%20preserves%20the%20spatial%20context%20of%20makeup%20and%20provides%20a%20target%20semantic%20map%0Afor%20guiding%20the%20shape-independent%20style%20codes.%20Secondly%2C%20a%20region-adaptive%0Anormalization%20module%20that%20decouples%20shape%20and%20makeup%20style%20using%20per-region%0Aencoding%20and%20normalization%2C%20which%20facilitates%20the%20elimination%20of%20spatial%0Amisalignments.%20Lastly%2C%20a%20makeup%20fusion%20module%20blends%20identity%20features%20and%0Amakeup%20style%20by%20injecting%20learned%20scale%20and%20bias%20parameters.%20Experimental%0Aresults%20show%20that%20our%20SARA%20method%20outperforms%20existing%20methods%20and%20achieves%0Astate-of-the-art%20performance%20on%20two%20public%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16828v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSARA%253A%2520Controllable%2520Makeup%2520Transfer%2520with%2520Spatial%2520Alignment%2520and%250A%2520%2520Region-Adaptive%2520Normalization%26entry.906535625%3DXiaojing%2520Zhong%2520and%2520Xinyi%2520Huang%2520and%2520Zhonghua%2520Wu%2520and%2520Guosheng%2520Lin%2520and%2520Qingyao%2520Wu%26entry.1292438233%3D%2520%2520Makeup%2520transfer%2520is%2520a%2520process%2520of%2520transferring%2520the%2520makeup%2520style%2520from%2520a%250Areference%2520image%2520to%2520the%2520source%2520images%252C%2520while%2520preserving%2520the%2520source%2520images%2527%250Aidentities.%2520This%2520technique%2520is%2520highly%2520desirable%2520and%2520finds%2520many%2520applications.%250AHowever%252C%2520existing%2520methods%2520lack%2520fine-level%2520control%2520of%2520the%2520makeup%2520style%252C%2520making%250Ait%2520challenging%2520to%2520achieve%2520high-quality%2520results%2520when%2520dealing%2520with%2520large%2520spatial%250Amisalignments.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520a%2520novel%2520Spatial%2520Alignment%250Aand%2520Region-Adaptive%2520normalization%2520method%2520%2528SARA%2529%2520in%2520this%2520paper.%2520Our%2520method%250Agenerates%2520detailed%2520makeup%2520transfer%2520results%2520that%2520can%2520handle%2520large%2520spatial%250Amisalignments%2520and%2520achieve%2520part-specific%2520and%2520shade-controllable%2520makeup%2520transfer.%250ASpecifically%252C%2520SARA%2520comprises%2520three%2520modules%253A%2520Firstly%252C%2520a%2520spatial%2520alignment%2520module%250Athat%2520preserves%2520the%2520spatial%2520context%2520of%2520makeup%2520and%2520provides%2520a%2520target%2520semantic%2520map%250Afor%2520guiding%2520the%2520shape-independent%2520style%2520codes.%2520Secondly%252C%2520a%2520region-adaptive%250Anormalization%2520module%2520that%2520decouples%2520shape%2520and%2520makeup%2520style%2520using%2520per-region%250Aencoding%2520and%2520normalization%252C%2520which%2520facilitates%2520the%2520elimination%2520of%2520spatial%250Amisalignments.%2520Lastly%252C%2520a%2520makeup%2520fusion%2520module%2520blends%2520identity%2520features%2520and%250Amakeup%2520style%2520by%2520injecting%2520learned%2520scale%2520and%2520bias%2520parameters.%2520Experimental%250Aresults%2520show%2520that%2520our%2520SARA%2520method%2520outperforms%2520existing%2520methods%2520and%2520achieves%250Astate-of-the-art%2520performance%2520on%2520two%2520public%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.16828v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SARA%3A%20Controllable%20Makeup%20Transfer%20with%20Spatial%20Alignment%20and%0A%20%20Region-Adaptive%20Normalization&entry.906535625=Xiaojing%20Zhong%20and%20Xinyi%20Huang%20and%20Zhonghua%20Wu%20and%20Guosheng%20Lin%20and%20Qingyao%20Wu&entry.1292438233=%20%20Makeup%20transfer%20is%20a%20process%20of%20transferring%20the%20makeup%20style%20from%20a%0Areference%20image%20to%20the%20source%20images%2C%20while%20preserving%20the%20source%20images%27%0Aidentities.%20This%20technique%20is%20highly%20desirable%20and%20finds%20many%20applications.%0AHowever%2C%20existing%20methods%20lack%20fine-level%20control%20of%20the%20makeup%20style%2C%20making%0Ait%20challenging%20to%20achieve%20high-quality%20results%20when%20dealing%20with%20large%20spatial%0Amisalignments.%20To%20address%20this%20problem%2C%20we%20propose%20a%20novel%20Spatial%20Alignment%0Aand%20Region-Adaptive%20normalization%20method%20%28SARA%29%20in%20this%20paper.%20Our%20method%0Agenerates%20detailed%20makeup%20transfer%20results%20that%20can%20handle%20large%20spatial%0Amisalignments%20and%20achieve%20part-specific%20and%20shade-controllable%20makeup%20transfer.%0ASpecifically%2C%20SARA%20comprises%20three%20modules%3A%20Firstly%2C%20a%20spatial%20alignment%20module%0Athat%20preserves%20the%20spatial%20context%20of%20makeup%20and%20provides%20a%20target%20semantic%20map%0Afor%20guiding%20the%20shape-independent%20style%20codes.%20Secondly%2C%20a%20region-adaptive%0Anormalization%20module%20that%20decouples%20shape%20and%20makeup%20style%20using%20per-region%0Aencoding%20and%20normalization%2C%20which%20facilitates%20the%20elimination%20of%20spatial%0Amisalignments.%20Lastly%2C%20a%20makeup%20fusion%20module%20blends%20identity%20features%20and%0Amakeup%20style%20by%20injecting%20learned%20scale%20and%20bias%20parameters.%20Experimental%0Aresults%20show%20that%20our%20SARA%20method%20outperforms%20existing%20methods%20and%20achieves%0Astate-of-the-art%20performance%20on%20two%20public%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16828v2&entry.124074799=Read"},
{"title": "DisenStudio: Customized Multi-subject Text-to-Video Generation with\n  Disentangled Spatial Control", "author": "Hong Chen and Xin Wang and Yipeng Zhang and Yuwei Zhou and Zeyang Zhang and Siao Tang and Wenwu Zhu", "abstract": "  Generating customized content in videos has received increasing attention\nrecently. However, existing works primarily focus on customized text-to-video\ngeneration for single subject, suffering from subject-missing and\nattribute-binding problems when the video is expected to contain multiple\nsubjects. Furthermore, existing models struggle to assign the desired actions\nto the corresponding subjects (action-binding problem), failing to achieve\nsatisfactory multi-subject generation performance. To tackle the problems, in\nthis paper, we propose DisenStudio, a novel framework that can generate\ntext-guided videos for customized multiple subjects, given few images for each\nsubject. Specifically, DisenStudio enhances a pretrained diffusion-based\ntext-to-video model with our proposed spatial-disentangled cross-attention\nmechanism to associate each subject with the desired action. Then the model is\ncustomized for the multiple subjects with the proposed motion-preserved\ndisentangled finetuning, which involves three tuning strategies: multi-subject\nco-occurrence tuning, masked single-subject tuning, and multi-subject\nmotion-preserved tuning. The first two strategies guarantee the subject\noccurrence and preserve their visual attributes, and the third strategy helps\nthe model maintain the temporal motion-generation ability when finetuning on\nstatic images. We conduct extensive experiments to demonstrate our proposed\nDisenStudio significantly outperforms existing methods in various metrics.\nAdditionally, we show that DisenStudio can be used as a powerful tool for\nvarious controllable generation applications.\n", "link": "http://arxiv.org/abs/2405.12796v1", "date": "2024-05-21", "relevancy": 2.548, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6904}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6683}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DisenStudio%3A%20Customized%20Multi-subject%20Text-to-Video%20Generation%20with%0A%20%20Disentangled%20Spatial%20Control&body=Title%3A%20DisenStudio%3A%20Customized%20Multi-subject%20Text-to-Video%20Generation%20with%0A%20%20Disentangled%20Spatial%20Control%0AAuthor%3A%20Hong%20Chen%20and%20Xin%20Wang%20and%20Yipeng%20Zhang%20and%20Yuwei%20Zhou%20and%20Zeyang%20Zhang%20and%20Siao%20Tang%20and%20Wenwu%20Zhu%0AAbstract%3A%20%20%20Generating%20customized%20content%20in%20videos%20has%20received%20increasing%20attention%0Arecently.%20However%2C%20existing%20works%20primarily%20focus%20on%20customized%20text-to-video%0Ageneration%20for%20single%20subject%2C%20suffering%20from%20subject-missing%20and%0Aattribute-binding%20problems%20when%20the%20video%20is%20expected%20to%20contain%20multiple%0Asubjects.%20Furthermore%2C%20existing%20models%20struggle%20to%20assign%20the%20desired%20actions%0Ato%20the%20corresponding%20subjects%20%28action-binding%20problem%29%2C%20failing%20to%20achieve%0Asatisfactory%20multi-subject%20generation%20performance.%20To%20tackle%20the%20problems%2C%20in%0Athis%20paper%2C%20we%20propose%20DisenStudio%2C%20a%20novel%20framework%20that%20can%20generate%0Atext-guided%20videos%20for%20customized%20multiple%20subjects%2C%20given%20few%20images%20for%20each%0Asubject.%20Specifically%2C%20DisenStudio%20enhances%20a%20pretrained%20diffusion-based%0Atext-to-video%20model%20with%20our%20proposed%20spatial-disentangled%20cross-attention%0Amechanism%20to%20associate%20each%20subject%20with%20the%20desired%20action.%20Then%20the%20model%20is%0Acustomized%20for%20the%20multiple%20subjects%20with%20the%20proposed%20motion-preserved%0Adisentangled%20finetuning%2C%20which%20involves%20three%20tuning%20strategies%3A%20multi-subject%0Aco-occurrence%20tuning%2C%20masked%20single-subject%20tuning%2C%20and%20multi-subject%0Amotion-preserved%20tuning.%20The%20first%20two%20strategies%20guarantee%20the%20subject%0Aoccurrence%20and%20preserve%20their%20visual%20attributes%2C%20and%20the%20third%20strategy%20helps%0Athe%20model%20maintain%20the%20temporal%20motion-generation%20ability%20when%20finetuning%20on%0Astatic%20images.%20We%20conduct%20extensive%20experiments%20to%20demonstrate%20our%20proposed%0ADisenStudio%20significantly%20outperforms%20existing%20methods%20in%20various%20metrics.%0AAdditionally%2C%20we%20show%20that%20DisenStudio%20can%20be%20used%20as%20a%20powerful%20tool%20for%0Avarious%20controllable%20generation%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisenStudio%253A%2520Customized%2520Multi-subject%2520Text-to-Video%2520Generation%2520with%250A%2520%2520Disentangled%2520Spatial%2520Control%26entry.906535625%3DHong%2520Chen%2520and%2520Xin%2520Wang%2520and%2520Yipeng%2520Zhang%2520and%2520Yuwei%2520Zhou%2520and%2520Zeyang%2520Zhang%2520and%2520Siao%2520Tang%2520and%2520Wenwu%2520Zhu%26entry.1292438233%3D%2520%2520Generating%2520customized%2520content%2520in%2520videos%2520has%2520received%2520increasing%2520attention%250Arecently.%2520However%252C%2520existing%2520works%2520primarily%2520focus%2520on%2520customized%2520text-to-video%250Ageneration%2520for%2520single%2520subject%252C%2520suffering%2520from%2520subject-missing%2520and%250Aattribute-binding%2520problems%2520when%2520the%2520video%2520is%2520expected%2520to%2520contain%2520multiple%250Asubjects.%2520Furthermore%252C%2520existing%2520models%2520struggle%2520to%2520assign%2520the%2520desired%2520actions%250Ato%2520the%2520corresponding%2520subjects%2520%2528action-binding%2520problem%2529%252C%2520failing%2520to%2520achieve%250Asatisfactory%2520multi-subject%2520generation%2520performance.%2520To%2520tackle%2520the%2520problems%252C%2520in%250Athis%2520paper%252C%2520we%2520propose%2520DisenStudio%252C%2520a%2520novel%2520framework%2520that%2520can%2520generate%250Atext-guided%2520videos%2520for%2520customized%2520multiple%2520subjects%252C%2520given%2520few%2520images%2520for%2520each%250Asubject.%2520Specifically%252C%2520DisenStudio%2520enhances%2520a%2520pretrained%2520diffusion-based%250Atext-to-video%2520model%2520with%2520our%2520proposed%2520spatial-disentangled%2520cross-attention%250Amechanism%2520to%2520associate%2520each%2520subject%2520with%2520the%2520desired%2520action.%2520Then%2520the%2520model%2520is%250Acustomized%2520for%2520the%2520multiple%2520subjects%2520with%2520the%2520proposed%2520motion-preserved%250Adisentangled%2520finetuning%252C%2520which%2520involves%2520three%2520tuning%2520strategies%253A%2520multi-subject%250Aco-occurrence%2520tuning%252C%2520masked%2520single-subject%2520tuning%252C%2520and%2520multi-subject%250Amotion-preserved%2520tuning.%2520The%2520first%2520two%2520strategies%2520guarantee%2520the%2520subject%250Aoccurrence%2520and%2520preserve%2520their%2520visual%2520attributes%252C%2520and%2520the%2520third%2520strategy%2520helps%250Athe%2520model%2520maintain%2520the%2520temporal%2520motion-generation%2520ability%2520when%2520finetuning%2520on%250Astatic%2520images.%2520We%2520conduct%2520extensive%2520experiments%2520to%2520demonstrate%2520our%2520proposed%250ADisenStudio%2520significantly%2520outperforms%2520existing%2520methods%2520in%2520various%2520metrics.%250AAdditionally%252C%2520we%2520show%2520that%2520DisenStudio%2520can%2520be%2520used%2520as%2520a%2520powerful%2520tool%2520for%250Avarious%2520controllable%2520generation%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DisenStudio%3A%20Customized%20Multi-subject%20Text-to-Video%20Generation%20with%0A%20%20Disentangled%20Spatial%20Control&entry.906535625=Hong%20Chen%20and%20Xin%20Wang%20and%20Yipeng%20Zhang%20and%20Yuwei%20Zhou%20and%20Zeyang%20Zhang%20and%20Siao%20Tang%20and%20Wenwu%20Zhu&entry.1292438233=%20%20Generating%20customized%20content%20in%20videos%20has%20received%20increasing%20attention%0Arecently.%20However%2C%20existing%20works%20primarily%20focus%20on%20customized%20text-to-video%0Ageneration%20for%20single%20subject%2C%20suffering%20from%20subject-missing%20and%0Aattribute-binding%20problems%20when%20the%20video%20is%20expected%20to%20contain%20multiple%0Asubjects.%20Furthermore%2C%20existing%20models%20struggle%20to%20assign%20the%20desired%20actions%0Ato%20the%20corresponding%20subjects%20%28action-binding%20problem%29%2C%20failing%20to%20achieve%0Asatisfactory%20multi-subject%20generation%20performance.%20To%20tackle%20the%20problems%2C%20in%0Athis%20paper%2C%20we%20propose%20DisenStudio%2C%20a%20novel%20framework%20that%20can%20generate%0Atext-guided%20videos%20for%20customized%20multiple%20subjects%2C%20given%20few%20images%20for%20each%0Asubject.%20Specifically%2C%20DisenStudio%20enhances%20a%20pretrained%20diffusion-based%0Atext-to-video%20model%20with%20our%20proposed%20spatial-disentangled%20cross-attention%0Amechanism%20to%20associate%20each%20subject%20with%20the%20desired%20action.%20Then%20the%20model%20is%0Acustomized%20for%20the%20multiple%20subjects%20with%20the%20proposed%20motion-preserved%0Adisentangled%20finetuning%2C%20which%20involves%20three%20tuning%20strategies%3A%20multi-subject%0Aco-occurrence%20tuning%2C%20masked%20single-subject%20tuning%2C%20and%20multi-subject%0Amotion-preserved%20tuning.%20The%20first%20two%20strategies%20guarantee%20the%20subject%0Aoccurrence%20and%20preserve%20their%20visual%20attributes%2C%20and%20the%20third%20strategy%20helps%0Athe%20model%20maintain%20the%20temporal%20motion-generation%20ability%20when%20finetuning%20on%0Astatic%20images.%20We%20conduct%20extensive%20experiments%20to%20demonstrate%20our%20proposed%0ADisenStudio%20significantly%20outperforms%20existing%20methods%20in%20various%20metrics.%0AAdditionally%2C%20we%20show%20that%20DisenStudio%20can%20be%20used%20as%20a%20powerful%20tool%20for%0Avarious%20controllable%20generation%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12796v1&entry.124074799=Read"},
{"title": "IAIFNet: An Illumination-Aware Infrared and Visible Image Fusion Network", "author": "Qiao Yang and Yu Zhang and Jian Zhang and Zijing Zhao and Shunli Zhang and Jinqiao Wang and Junzhe Chen", "abstract": "  Infrared and visible image fusion (IVIF) is used to generate fusion images\nwith comprehensive features of both images, which is beneficial for downstream\nvision tasks. However, current methods rarely consider the illumination\ncondition in low-light environments, and the targets in the fused images are\noften not prominent. To address the above issues, we propose an\nIllumination-Aware Infrared and Visible Image Fusion Network, named as IAIFNet.\nIn our framework, an illumination enhancement network first estimates the\nincident illumination maps of input images. Afterwards, with the help of\nproposed adaptive differential fusion module (ADFM) and salient target aware\nmodule (STAM), an image fusion network effectively integrates the salient\nfeatures of the illumination-enhanced infrared and visible images into a fusion\nimage of high visual quality. Extensive experimental results verify that our\nmethod outperforms five state-of-the-art methods of fusing infrared and visible\nimages.\n", "link": "http://arxiv.org/abs/2309.14997v2", "date": "2024-05-21", "relevancy": 2.4038, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.494}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4807}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IAIFNet%3A%20An%20Illumination-Aware%20Infrared%20and%20Visible%20Image%20Fusion%20Network&body=Title%3A%20IAIFNet%3A%20An%20Illumination-Aware%20Infrared%20and%20Visible%20Image%20Fusion%20Network%0AAuthor%3A%20Qiao%20Yang%20and%20Yu%20Zhang%20and%20Jian%20Zhang%20and%20Zijing%20Zhao%20and%20Shunli%20Zhang%20and%20Jinqiao%20Wang%20and%20Junzhe%20Chen%0AAbstract%3A%20%20%20Infrared%20and%20visible%20image%20fusion%20%28IVIF%29%20is%20used%20to%20generate%20fusion%20images%0Awith%20comprehensive%20features%20of%20both%20images%2C%20which%20is%20beneficial%20for%20downstream%0Avision%20tasks.%20However%2C%20current%20methods%20rarely%20consider%20the%20illumination%0Acondition%20in%20low-light%20environments%2C%20and%20the%20targets%20in%20the%20fused%20images%20are%0Aoften%20not%20prominent.%20To%20address%20the%20above%20issues%2C%20we%20propose%20an%0AIllumination-Aware%20Infrared%20and%20Visible%20Image%20Fusion%20Network%2C%20named%20as%20IAIFNet.%0AIn%20our%20framework%2C%20an%20illumination%20enhancement%20network%20first%20estimates%20the%0Aincident%20illumination%20maps%20of%20input%20images.%20Afterwards%2C%20with%20the%20help%20of%0Aproposed%20adaptive%20differential%20fusion%20module%20%28ADFM%29%20and%20salient%20target%20aware%0Amodule%20%28STAM%29%2C%20an%20image%20fusion%20network%20effectively%20integrates%20the%20salient%0Afeatures%20of%20the%20illumination-enhanced%20infrared%20and%20visible%20images%20into%20a%20fusion%0Aimage%20of%20high%20visual%20quality.%20Extensive%20experimental%20results%20verify%20that%20our%0Amethod%20outperforms%20five%20state-of-the-art%20methods%20of%20fusing%20infrared%20and%20visible%0Aimages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.14997v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIAIFNet%253A%2520An%2520Illumination-Aware%2520Infrared%2520and%2520Visible%2520Image%2520Fusion%2520Network%26entry.906535625%3DQiao%2520Yang%2520and%2520Yu%2520Zhang%2520and%2520Jian%2520Zhang%2520and%2520Zijing%2520Zhao%2520and%2520Shunli%2520Zhang%2520and%2520Jinqiao%2520Wang%2520and%2520Junzhe%2520Chen%26entry.1292438233%3D%2520%2520Infrared%2520and%2520visible%2520image%2520fusion%2520%2528IVIF%2529%2520is%2520used%2520to%2520generate%2520fusion%2520images%250Awith%2520comprehensive%2520features%2520of%2520both%2520images%252C%2520which%2520is%2520beneficial%2520for%2520downstream%250Avision%2520tasks.%2520However%252C%2520current%2520methods%2520rarely%2520consider%2520the%2520illumination%250Acondition%2520in%2520low-light%2520environments%252C%2520and%2520the%2520targets%2520in%2520the%2520fused%2520images%2520are%250Aoften%2520not%2520prominent.%2520To%2520address%2520the%2520above%2520issues%252C%2520we%2520propose%2520an%250AIllumination-Aware%2520Infrared%2520and%2520Visible%2520Image%2520Fusion%2520Network%252C%2520named%2520as%2520IAIFNet.%250AIn%2520our%2520framework%252C%2520an%2520illumination%2520enhancement%2520network%2520first%2520estimates%2520the%250Aincident%2520illumination%2520maps%2520of%2520input%2520images.%2520Afterwards%252C%2520with%2520the%2520help%2520of%250Aproposed%2520adaptive%2520differential%2520fusion%2520module%2520%2528ADFM%2529%2520and%2520salient%2520target%2520aware%250Amodule%2520%2528STAM%2529%252C%2520an%2520image%2520fusion%2520network%2520effectively%2520integrates%2520the%2520salient%250Afeatures%2520of%2520the%2520illumination-enhanced%2520infrared%2520and%2520visible%2520images%2520into%2520a%2520fusion%250Aimage%2520of%2520high%2520visual%2520quality.%2520Extensive%2520experimental%2520results%2520verify%2520that%2520our%250Amethod%2520outperforms%2520five%2520state-of-the-art%2520methods%2520of%2520fusing%2520infrared%2520and%2520visible%250Aimages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.14997v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IAIFNet%3A%20An%20Illumination-Aware%20Infrared%20and%20Visible%20Image%20Fusion%20Network&entry.906535625=Qiao%20Yang%20and%20Yu%20Zhang%20and%20Jian%20Zhang%20and%20Zijing%20Zhao%20and%20Shunli%20Zhang%20and%20Jinqiao%20Wang%20and%20Junzhe%20Chen&entry.1292438233=%20%20Infrared%20and%20visible%20image%20fusion%20%28IVIF%29%20is%20used%20to%20generate%20fusion%20images%0Awith%20comprehensive%20features%20of%20both%20images%2C%20which%20is%20beneficial%20for%20downstream%0Avision%20tasks.%20However%2C%20current%20methods%20rarely%20consider%20the%20illumination%0Acondition%20in%20low-light%20environments%2C%20and%20the%20targets%20in%20the%20fused%20images%20are%0Aoften%20not%20prominent.%20To%20address%20the%20above%20issues%2C%20we%20propose%20an%0AIllumination-Aware%20Infrared%20and%20Visible%20Image%20Fusion%20Network%2C%20named%20as%20IAIFNet.%0AIn%20our%20framework%2C%20an%20illumination%20enhancement%20network%20first%20estimates%20the%0Aincident%20illumination%20maps%20of%20input%20images.%20Afterwards%2C%20with%20the%20help%20of%0Aproposed%20adaptive%20differential%20fusion%20module%20%28ADFM%29%20and%20salient%20target%20aware%0Amodule%20%28STAM%29%2C%20an%20image%20fusion%20network%20effectively%20integrates%20the%20salient%0Afeatures%20of%20the%20illumination-enhanced%20infrared%20and%20visible%20images%20into%20a%20fusion%0Aimage%20of%20high%20visual%20quality.%20Extensive%20experimental%20results%20verify%20that%20our%0Amethod%20outperforms%20five%20state-of-the-art%20methods%20of%20fusing%20infrared%20and%20visible%0Aimages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.14997v2&entry.124074799=Read"},
{"title": "Spotting AI's Touch: Identifying LLM-Paraphrased Spans in Text", "author": "Yafu Li and Zhilin Wang and Leyang Cui and Wei Bi and Shuming Shi and Yue Zhang", "abstract": "  AI-generated text detection has attracted increasing attention as powerful\nlanguage models approach human-level generation. Limited work is devoted to\ndetecting (partially) AI-paraphrased texts. However, AI paraphrasing is\ncommonly employed in various application scenarios for text refinement and\ndiversity. To this end, we propose a novel detection framework, paraphrased\ntext span detection (PTD), aiming to identify paraphrased text spans within a\ntext. Different from text-level detection, PTD takes in the full text and\nassigns each of the sentences with a score indicating the paraphrasing degree.\nWe construct a dedicated dataset, PASTED, for paraphrased text span detection.\nBoth in-distribution and out-of-distribution results demonstrate the\neffectiveness of PTD models in identifying AI-paraphrased text spans.\nStatistical and model analysis explains the crucial role of the surrounding\ncontext of the paraphrased text spans. Extensive experiments show that PTD\nmodels can generalize to versatile paraphrasing prompts and multiple\nparaphrased text spans. We release our resources at\nhttps://github.com/Linzwcs/PASTED.\n", "link": "http://arxiv.org/abs/2405.12689v1", "date": "2024-05-21", "relevancy": 2.4034, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4834}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4808}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spotting%20AI%27s%20Touch%3A%20Identifying%20LLM-Paraphrased%20Spans%20in%20Text&body=Title%3A%20Spotting%20AI%27s%20Touch%3A%20Identifying%20LLM-Paraphrased%20Spans%20in%20Text%0AAuthor%3A%20Yafu%20Li%20and%20Zhilin%20Wang%20and%20Leyang%20Cui%20and%20Wei%20Bi%20and%20Shuming%20Shi%20and%20Yue%20Zhang%0AAbstract%3A%20%20%20AI-generated%20text%20detection%20has%20attracted%20increasing%20attention%20as%20powerful%0Alanguage%20models%20approach%20human-level%20generation.%20Limited%20work%20is%20devoted%20to%0Adetecting%20%28partially%29%20AI-paraphrased%20texts.%20However%2C%20AI%20paraphrasing%20is%0Acommonly%20employed%20in%20various%20application%20scenarios%20for%20text%20refinement%20and%0Adiversity.%20To%20this%20end%2C%20we%20propose%20a%20novel%20detection%20framework%2C%20paraphrased%0Atext%20span%20detection%20%28PTD%29%2C%20aiming%20to%20identify%20paraphrased%20text%20spans%20within%20a%0Atext.%20Different%20from%20text-level%20detection%2C%20PTD%20takes%20in%20the%20full%20text%20and%0Aassigns%20each%20of%20the%20sentences%20with%20a%20score%20indicating%20the%20paraphrasing%20degree.%0AWe%20construct%20a%20dedicated%20dataset%2C%20PASTED%2C%20for%20paraphrased%20text%20span%20detection.%0ABoth%20in-distribution%20and%20out-of-distribution%20results%20demonstrate%20the%0Aeffectiveness%20of%20PTD%20models%20in%20identifying%20AI-paraphrased%20text%20spans.%0AStatistical%20and%20model%20analysis%20explains%20the%20crucial%20role%20of%20the%20surrounding%0Acontext%20of%20the%20paraphrased%20text%20spans.%20Extensive%20experiments%20show%20that%20PTD%0Amodels%20can%20generalize%20to%20versatile%20paraphrasing%20prompts%20and%20multiple%0Aparaphrased%20text%20spans.%20We%20release%20our%20resources%20at%0Ahttps%3A//github.com/Linzwcs/PASTED.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpotting%2520AI%2527s%2520Touch%253A%2520Identifying%2520LLM-Paraphrased%2520Spans%2520in%2520Text%26entry.906535625%3DYafu%2520Li%2520and%2520Zhilin%2520Wang%2520and%2520Leyang%2520Cui%2520and%2520Wei%2520Bi%2520and%2520Shuming%2520Shi%2520and%2520Yue%2520Zhang%26entry.1292438233%3D%2520%2520AI-generated%2520text%2520detection%2520has%2520attracted%2520increasing%2520attention%2520as%2520powerful%250Alanguage%2520models%2520approach%2520human-level%2520generation.%2520Limited%2520work%2520is%2520devoted%2520to%250Adetecting%2520%2528partially%2529%2520AI-paraphrased%2520texts.%2520However%252C%2520AI%2520paraphrasing%2520is%250Acommonly%2520employed%2520in%2520various%2520application%2520scenarios%2520for%2520text%2520refinement%2520and%250Adiversity.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520detection%2520framework%252C%2520paraphrased%250Atext%2520span%2520detection%2520%2528PTD%2529%252C%2520aiming%2520to%2520identify%2520paraphrased%2520text%2520spans%2520within%2520a%250Atext.%2520Different%2520from%2520text-level%2520detection%252C%2520PTD%2520takes%2520in%2520the%2520full%2520text%2520and%250Aassigns%2520each%2520of%2520the%2520sentences%2520with%2520a%2520score%2520indicating%2520the%2520paraphrasing%2520degree.%250AWe%2520construct%2520a%2520dedicated%2520dataset%252C%2520PASTED%252C%2520for%2520paraphrased%2520text%2520span%2520detection.%250ABoth%2520in-distribution%2520and%2520out-of-distribution%2520results%2520demonstrate%2520the%250Aeffectiveness%2520of%2520PTD%2520models%2520in%2520identifying%2520AI-paraphrased%2520text%2520spans.%250AStatistical%2520and%2520model%2520analysis%2520explains%2520the%2520crucial%2520role%2520of%2520the%2520surrounding%250Acontext%2520of%2520the%2520paraphrased%2520text%2520spans.%2520Extensive%2520experiments%2520show%2520that%2520PTD%250Amodels%2520can%2520generalize%2520to%2520versatile%2520paraphrasing%2520prompts%2520and%2520multiple%250Aparaphrased%2520text%2520spans.%2520We%2520release%2520our%2520resources%2520at%250Ahttps%253A//github.com/Linzwcs/PASTED.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spotting%20AI%27s%20Touch%3A%20Identifying%20LLM-Paraphrased%20Spans%20in%20Text&entry.906535625=Yafu%20Li%20and%20Zhilin%20Wang%20and%20Leyang%20Cui%20and%20Wei%20Bi%20and%20Shuming%20Shi%20and%20Yue%20Zhang&entry.1292438233=%20%20AI-generated%20text%20detection%20has%20attracted%20increasing%20attention%20as%20powerful%0Alanguage%20models%20approach%20human-level%20generation.%20Limited%20work%20is%20devoted%20to%0Adetecting%20%28partially%29%20AI-paraphrased%20texts.%20However%2C%20AI%20paraphrasing%20is%0Acommonly%20employed%20in%20various%20application%20scenarios%20for%20text%20refinement%20and%0Adiversity.%20To%20this%20end%2C%20we%20propose%20a%20novel%20detection%20framework%2C%20paraphrased%0Atext%20span%20detection%20%28PTD%29%2C%20aiming%20to%20identify%20paraphrased%20text%20spans%20within%20a%0Atext.%20Different%20from%20text-level%20detection%2C%20PTD%20takes%20in%20the%20full%20text%20and%0Aassigns%20each%20of%20the%20sentences%20with%20a%20score%20indicating%20the%20paraphrasing%20degree.%0AWe%20construct%20a%20dedicated%20dataset%2C%20PASTED%2C%20for%20paraphrased%20text%20span%20detection.%0ABoth%20in-distribution%20and%20out-of-distribution%20results%20demonstrate%20the%0Aeffectiveness%20of%20PTD%20models%20in%20identifying%20AI-paraphrased%20text%20spans.%0AStatistical%20and%20model%20analysis%20explains%20the%20crucial%20role%20of%20the%20surrounding%0Acontext%20of%20the%20paraphrased%20text%20spans.%20Extensive%20experiments%20show%20that%20PTD%0Amodels%20can%20generalize%20to%20versatile%20paraphrasing%20prompts%20and%20multiple%0Aparaphrased%20text%20spans.%20We%20release%20our%20resources%20at%0Ahttps%3A//github.com/Linzwcs/PASTED.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12689v1&entry.124074799=Read"},
{"title": "Unsupervised Episode Generation for Graph Meta-learning", "author": "Jihyeong Jung and Sangwoo Seo and Sungwon Kim and Chanyoung Park", "abstract": "  We propose Unsupervised Episode Generation method called Neighbors as Queries\n(NaQ) to solve the Few-Shot Node-Classification (FSNC) task by unsupervised\nGraph Meta-learning. Doing so enables full utilization of the information of\nall nodes in a graph, which is not possible in current supervised meta-learning\nmethods for FSNC due to the label-scarcity problem. In addition, unlike\nunsupervised Graph Contrastive Learning (GCL) methods that overlook the\ndownstream task to be solved at the training phase resulting in vulnerability\nto class imbalance of a graph, we adopt the episodic learning framework that\nallows the model to be aware of the downstream task format, i.e., FSNC. The\nproposed NaQ is a simple but effective unsupervised episode generation method\nthat randomly samples nodes from a graph to make a support set, followed by\nsimilarity-based sampling of nodes to make the corresponding query set. Since\nNaQ is model-agnostic, any existing supervised graph meta-learning methods can\nbe trained in an unsupervised manner, while not sacrificing much of their\nperformance or sometimes even improving them. Extensive experimental results\ndemonstrate the effectiveness of our proposed unsupervised episode generation\nmethod for graph meta-learning towards the FSNC task. Our code is available at:\nhttps://github.com/JhngJng/NaQ-PyTorch.\n", "link": "http://arxiv.org/abs/2306.15217v3", "date": "2024-05-21", "relevancy": 2.4027, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5067}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4771}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Episode%20Generation%20for%20Graph%20Meta-learning&body=Title%3A%20Unsupervised%20Episode%20Generation%20for%20Graph%20Meta-learning%0AAuthor%3A%20Jihyeong%20Jung%20and%20Sangwoo%20Seo%20and%20Sungwon%20Kim%20and%20Chanyoung%20Park%0AAbstract%3A%20%20%20We%20propose%20Unsupervised%20Episode%20Generation%20method%20called%20Neighbors%20as%20Queries%0A%28NaQ%29%20to%20solve%20the%20Few-Shot%20Node-Classification%20%28FSNC%29%20task%20by%20unsupervised%0AGraph%20Meta-learning.%20Doing%20so%20enables%20full%20utilization%20of%20the%20information%20of%0Aall%20nodes%20in%20a%20graph%2C%20which%20is%20not%20possible%20in%20current%20supervised%20meta-learning%0Amethods%20for%20FSNC%20due%20to%20the%20label-scarcity%20problem.%20In%20addition%2C%20unlike%0Aunsupervised%20Graph%20Contrastive%20Learning%20%28GCL%29%20methods%20that%20overlook%20the%0Adownstream%20task%20to%20be%20solved%20at%20the%20training%20phase%20resulting%20in%20vulnerability%0Ato%20class%20imbalance%20of%20a%20graph%2C%20we%20adopt%20the%20episodic%20learning%20framework%20that%0Aallows%20the%20model%20to%20be%20aware%20of%20the%20downstream%20task%20format%2C%20i.e.%2C%20FSNC.%20The%0Aproposed%20NaQ%20is%20a%20simple%20but%20effective%20unsupervised%20episode%20generation%20method%0Athat%20randomly%20samples%20nodes%20from%20a%20graph%20to%20make%20a%20support%20set%2C%20followed%20by%0Asimilarity-based%20sampling%20of%20nodes%20to%20make%20the%20corresponding%20query%20set.%20Since%0ANaQ%20is%20model-agnostic%2C%20any%20existing%20supervised%20graph%20meta-learning%20methods%20can%0Abe%20trained%20in%20an%20unsupervised%20manner%2C%20while%20not%20sacrificing%20much%20of%20their%0Aperformance%20or%20sometimes%20even%20improving%20them.%20Extensive%20experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20our%20proposed%20unsupervised%20episode%20generation%0Amethod%20for%20graph%20meta-learning%20towards%20the%20FSNC%20task.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/JhngJng/NaQ-PyTorch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.15217v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Episode%2520Generation%2520for%2520Graph%2520Meta-learning%26entry.906535625%3DJihyeong%2520Jung%2520and%2520Sangwoo%2520Seo%2520and%2520Sungwon%2520Kim%2520and%2520Chanyoung%2520Park%26entry.1292438233%3D%2520%2520We%2520propose%2520Unsupervised%2520Episode%2520Generation%2520method%2520called%2520Neighbors%2520as%2520Queries%250A%2528NaQ%2529%2520to%2520solve%2520the%2520Few-Shot%2520Node-Classification%2520%2528FSNC%2529%2520task%2520by%2520unsupervised%250AGraph%2520Meta-learning.%2520Doing%2520so%2520enables%2520full%2520utilization%2520of%2520the%2520information%2520of%250Aall%2520nodes%2520in%2520a%2520graph%252C%2520which%2520is%2520not%2520possible%2520in%2520current%2520supervised%2520meta-learning%250Amethods%2520for%2520FSNC%2520due%2520to%2520the%2520label-scarcity%2520problem.%2520In%2520addition%252C%2520unlike%250Aunsupervised%2520Graph%2520Contrastive%2520Learning%2520%2528GCL%2529%2520methods%2520that%2520overlook%2520the%250Adownstream%2520task%2520to%2520be%2520solved%2520at%2520the%2520training%2520phase%2520resulting%2520in%2520vulnerability%250Ato%2520class%2520imbalance%2520of%2520a%2520graph%252C%2520we%2520adopt%2520the%2520episodic%2520learning%2520framework%2520that%250Aallows%2520the%2520model%2520to%2520be%2520aware%2520of%2520the%2520downstream%2520task%2520format%252C%2520i.e.%252C%2520FSNC.%2520The%250Aproposed%2520NaQ%2520is%2520a%2520simple%2520but%2520effective%2520unsupervised%2520episode%2520generation%2520method%250Athat%2520randomly%2520samples%2520nodes%2520from%2520a%2520graph%2520to%2520make%2520a%2520support%2520set%252C%2520followed%2520by%250Asimilarity-based%2520sampling%2520of%2520nodes%2520to%2520make%2520the%2520corresponding%2520query%2520set.%2520Since%250ANaQ%2520is%2520model-agnostic%252C%2520any%2520existing%2520supervised%2520graph%2520meta-learning%2520methods%2520can%250Abe%2520trained%2520in%2520an%2520unsupervised%2520manner%252C%2520while%2520not%2520sacrificing%2520much%2520of%2520their%250Aperformance%2520or%2520sometimes%2520even%2520improving%2520them.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520unsupervised%2520episode%2520generation%250Amethod%2520for%2520graph%2520meta-learning%2520towards%2520the%2520FSNC%2520task.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/JhngJng/NaQ-PyTorch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.15217v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Episode%20Generation%20for%20Graph%20Meta-learning&entry.906535625=Jihyeong%20Jung%20and%20Sangwoo%20Seo%20and%20Sungwon%20Kim%20and%20Chanyoung%20Park&entry.1292438233=%20%20We%20propose%20Unsupervised%20Episode%20Generation%20method%20called%20Neighbors%20as%20Queries%0A%28NaQ%29%20to%20solve%20the%20Few-Shot%20Node-Classification%20%28FSNC%29%20task%20by%20unsupervised%0AGraph%20Meta-learning.%20Doing%20so%20enables%20full%20utilization%20of%20the%20information%20of%0Aall%20nodes%20in%20a%20graph%2C%20which%20is%20not%20possible%20in%20current%20supervised%20meta-learning%0Amethods%20for%20FSNC%20due%20to%20the%20label-scarcity%20problem.%20In%20addition%2C%20unlike%0Aunsupervised%20Graph%20Contrastive%20Learning%20%28GCL%29%20methods%20that%20overlook%20the%0Adownstream%20task%20to%20be%20solved%20at%20the%20training%20phase%20resulting%20in%20vulnerability%0Ato%20class%20imbalance%20of%20a%20graph%2C%20we%20adopt%20the%20episodic%20learning%20framework%20that%0Aallows%20the%20model%20to%20be%20aware%20of%20the%20downstream%20task%20format%2C%20i.e.%2C%20FSNC.%20The%0Aproposed%20NaQ%20is%20a%20simple%20but%20effective%20unsupervised%20episode%20generation%20method%0Athat%20randomly%20samples%20nodes%20from%20a%20graph%20to%20make%20a%20support%20set%2C%20followed%20by%0Asimilarity-based%20sampling%20of%20nodes%20to%20make%20the%20corresponding%20query%20set.%20Since%0ANaQ%20is%20model-agnostic%2C%20any%20existing%20supervised%20graph%20meta-learning%20methods%20can%0Abe%20trained%20in%20an%20unsupervised%20manner%2C%20while%20not%20sacrificing%20much%20of%20their%0Aperformance%20or%20sometimes%20even%20improving%20them.%20Extensive%20experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20our%20proposed%20unsupervised%20episode%20generation%0Amethod%20for%20graph%20meta-learning%20towards%20the%20FSNC%20task.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/JhngJng/NaQ-PyTorch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.15217v3&entry.124074799=Read"},
{"title": "PoseGravity: Pose Estimation from Points and Lines with Axis Prior", "author": "Akshay Chandrasekhar", "abstract": "  This paper presents a new algorithm to estimate absolute camera pose given an\naxis of the camera's rotation matrix. Current algorithms solve the problem via\nalgebraic solutions on limited input domains. This paper shows that the problem\ncan be solved efficiently by finding the intersection points of a hyperbola and\nthe unit circle. The solution can flexibly accommodate combinations of point\nand line features in minimal and overconstrained configurations. In addition,\nthe two special cases of planar and minimal configurations are identified to\nyield simpler closed-form solutions. Extensive experiments validate the\napproach.\n", "link": "http://arxiv.org/abs/2405.12646v1", "date": "2024-05-21", "relevancy": 2.3996, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4849}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4774}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoseGravity%3A%20Pose%20Estimation%20from%20Points%20and%20Lines%20with%20Axis%20Prior&body=Title%3A%20PoseGravity%3A%20Pose%20Estimation%20from%20Points%20and%20Lines%20with%20Axis%20Prior%0AAuthor%3A%20Akshay%20Chandrasekhar%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20new%20algorithm%20to%20estimate%20absolute%20camera%20pose%20given%20an%0Aaxis%20of%20the%20camera%27s%20rotation%20matrix.%20Current%20algorithms%20solve%20the%20problem%20via%0Aalgebraic%20solutions%20on%20limited%20input%20domains.%20This%20paper%20shows%20that%20the%20problem%0Acan%20be%20solved%20efficiently%20by%20finding%20the%20intersection%20points%20of%20a%20hyperbola%20and%0Athe%20unit%20circle.%20The%20solution%20can%20flexibly%20accommodate%20combinations%20of%20point%0Aand%20line%20features%20in%20minimal%20and%20overconstrained%20configurations.%20In%20addition%2C%0Athe%20two%20special%20cases%20of%20planar%20and%20minimal%20configurations%20are%20identified%20to%0Ayield%20simpler%20closed-form%20solutions.%20Extensive%20experiments%20validate%20the%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoseGravity%253A%2520Pose%2520Estimation%2520from%2520Points%2520and%2520Lines%2520with%2520Axis%2520Prior%26entry.906535625%3DAkshay%2520Chandrasekhar%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520new%2520algorithm%2520to%2520estimate%2520absolute%2520camera%2520pose%2520given%2520an%250Aaxis%2520of%2520the%2520camera%2527s%2520rotation%2520matrix.%2520Current%2520algorithms%2520solve%2520the%2520problem%2520via%250Aalgebraic%2520solutions%2520on%2520limited%2520input%2520domains.%2520This%2520paper%2520shows%2520that%2520the%2520problem%250Acan%2520be%2520solved%2520efficiently%2520by%2520finding%2520the%2520intersection%2520points%2520of%2520a%2520hyperbola%2520and%250Athe%2520unit%2520circle.%2520The%2520solution%2520can%2520flexibly%2520accommodate%2520combinations%2520of%2520point%250Aand%2520line%2520features%2520in%2520minimal%2520and%2520overconstrained%2520configurations.%2520In%2520addition%252C%250Athe%2520two%2520special%2520cases%2520of%2520planar%2520and%2520minimal%2520configurations%2520are%2520identified%2520to%250Ayield%2520simpler%2520closed-form%2520solutions.%2520Extensive%2520experiments%2520validate%2520the%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoseGravity%3A%20Pose%20Estimation%20from%20Points%20and%20Lines%20with%20Axis%20Prior&entry.906535625=Akshay%20Chandrasekhar&entry.1292438233=%20%20This%20paper%20presents%20a%20new%20algorithm%20to%20estimate%20absolute%20camera%20pose%20given%20an%0Aaxis%20of%20the%20camera%27s%20rotation%20matrix.%20Current%20algorithms%20solve%20the%20problem%20via%0Aalgebraic%20solutions%20on%20limited%20input%20domains.%20This%20paper%20shows%20that%20the%20problem%0Acan%20be%20solved%20efficiently%20by%20finding%20the%20intersection%20points%20of%20a%20hyperbola%20and%0Athe%20unit%20circle.%20The%20solution%20can%20flexibly%20accommodate%20combinations%20of%20point%0Aand%20line%20features%20in%20minimal%20and%20overconstrained%20configurations.%20In%20addition%2C%0Athe%20two%20special%20cases%20of%20planar%20and%20minimal%20configurations%20are%20identified%20to%0Ayield%20simpler%20closed-form%20solutions.%20Extensive%20experiments%20validate%20the%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12646v1&entry.124074799=Read"},
{"title": "BIMM: Brain Inspired Masked Modeling for Video Representation Learning", "author": "Zhifan Wan and Jie Zhang and Changzhen Li and Shiguang Shan", "abstract": "  The visual pathway of human brain includes two sub-pathways, ie, the ventral\npathway and the dorsal pathway, which focus on object identification and\ndynamic information modeling, respectively. Both pathways comprise multi-layer\nstructures, with each layer responsible for processing different aspects of\nvisual information. Inspired by visual information processing mechanism of the\nhuman brain, we propose the Brain Inspired Masked Modeling (BIMM) framework,\naiming to learn comprehensive representations from videos. Specifically, our\napproach consists of ventral and dorsal branches, which learn image and video\nrepresentations, respectively. Both branches employ the Vision Transformer\n(ViT) as their backbone and are trained using masked modeling method. To\nachieve the goals of different visual cortices in the brain, we segment the\nencoder of each branch into three intermediate blocks and reconstruct\nprogressive prediction targets with light weight decoders. Furthermore, drawing\ninspiration from the information-sharing mechanism in the visual pathways, we\npropose a partial parameter sharing strategy between the branches during\ntraining. Extensive experiments demonstrate that BIMM achieves superior\nperformance compared to the state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2405.12757v1", "date": "2024-05-21", "relevancy": 2.3853, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6103}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5945}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BIMM%3A%20Brain%20Inspired%20Masked%20Modeling%20for%20Video%20Representation%20Learning&body=Title%3A%20BIMM%3A%20Brain%20Inspired%20Masked%20Modeling%20for%20Video%20Representation%20Learning%0AAuthor%3A%20Zhifan%20Wan%20and%20Jie%20Zhang%20and%20Changzhen%20Li%20and%20Shiguang%20Shan%0AAbstract%3A%20%20%20The%20visual%20pathway%20of%20human%20brain%20includes%20two%20sub-pathways%2C%20ie%2C%20the%20ventral%0Apathway%20and%20the%20dorsal%20pathway%2C%20which%20focus%20on%20object%20identification%20and%0Adynamic%20information%20modeling%2C%20respectively.%20Both%20pathways%20comprise%20multi-layer%0Astructures%2C%20with%20each%20layer%20responsible%20for%20processing%20different%20aspects%20of%0Avisual%20information.%20Inspired%20by%20visual%20information%20processing%20mechanism%20of%20the%0Ahuman%20brain%2C%20we%20propose%20the%20Brain%20Inspired%20Masked%20Modeling%20%28BIMM%29%20framework%2C%0Aaiming%20to%20learn%20comprehensive%20representations%20from%20videos.%20Specifically%2C%20our%0Aapproach%20consists%20of%20ventral%20and%20dorsal%20branches%2C%20which%20learn%20image%20and%20video%0Arepresentations%2C%20respectively.%20Both%20branches%20employ%20the%20Vision%20Transformer%0A%28ViT%29%20as%20their%20backbone%20and%20are%20trained%20using%20masked%20modeling%20method.%20To%0Aachieve%20the%20goals%20of%20different%20visual%20cortices%20in%20the%20brain%2C%20we%20segment%20the%0Aencoder%20of%20each%20branch%20into%20three%20intermediate%20blocks%20and%20reconstruct%0Aprogressive%20prediction%20targets%20with%20light%20weight%20decoders.%20Furthermore%2C%20drawing%0Ainspiration%20from%20the%20information-sharing%20mechanism%20in%20the%20visual%20pathways%2C%20we%0Apropose%20a%20partial%20parameter%20sharing%20strategy%20between%20the%20branches%20during%0Atraining.%20Extensive%20experiments%20demonstrate%20that%20BIMM%20achieves%20superior%0Aperformance%20compared%20to%20the%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12757v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBIMM%253A%2520Brain%2520Inspired%2520Masked%2520Modeling%2520for%2520Video%2520Representation%2520Learning%26entry.906535625%3DZhifan%2520Wan%2520and%2520Jie%2520Zhang%2520and%2520Changzhen%2520Li%2520and%2520Shiguang%2520Shan%26entry.1292438233%3D%2520%2520The%2520visual%2520pathway%2520of%2520human%2520brain%2520includes%2520two%2520sub-pathways%252C%2520ie%252C%2520the%2520ventral%250Apathway%2520and%2520the%2520dorsal%2520pathway%252C%2520which%2520focus%2520on%2520object%2520identification%2520and%250Adynamic%2520information%2520modeling%252C%2520respectively.%2520Both%2520pathways%2520comprise%2520multi-layer%250Astructures%252C%2520with%2520each%2520layer%2520responsible%2520for%2520processing%2520different%2520aspects%2520of%250Avisual%2520information.%2520Inspired%2520by%2520visual%2520information%2520processing%2520mechanism%2520of%2520the%250Ahuman%2520brain%252C%2520we%2520propose%2520the%2520Brain%2520Inspired%2520Masked%2520Modeling%2520%2528BIMM%2529%2520framework%252C%250Aaiming%2520to%2520learn%2520comprehensive%2520representations%2520from%2520videos.%2520Specifically%252C%2520our%250Aapproach%2520consists%2520of%2520ventral%2520and%2520dorsal%2520branches%252C%2520which%2520learn%2520image%2520and%2520video%250Arepresentations%252C%2520respectively.%2520Both%2520branches%2520employ%2520the%2520Vision%2520Transformer%250A%2528ViT%2529%2520as%2520their%2520backbone%2520and%2520are%2520trained%2520using%2520masked%2520modeling%2520method.%2520To%250Aachieve%2520the%2520goals%2520of%2520different%2520visual%2520cortices%2520in%2520the%2520brain%252C%2520we%2520segment%2520the%250Aencoder%2520of%2520each%2520branch%2520into%2520three%2520intermediate%2520blocks%2520and%2520reconstruct%250Aprogressive%2520prediction%2520targets%2520with%2520light%2520weight%2520decoders.%2520Furthermore%252C%2520drawing%250Ainspiration%2520from%2520the%2520information-sharing%2520mechanism%2520in%2520the%2520visual%2520pathways%252C%2520we%250Apropose%2520a%2520partial%2520parameter%2520sharing%2520strategy%2520between%2520the%2520branches%2520during%250Atraining.%2520Extensive%2520experiments%2520demonstrate%2520that%2520BIMM%2520achieves%2520superior%250Aperformance%2520compared%2520to%2520the%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12757v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BIMM%3A%20Brain%20Inspired%20Masked%20Modeling%20for%20Video%20Representation%20Learning&entry.906535625=Zhifan%20Wan%20and%20Jie%20Zhang%20and%20Changzhen%20Li%20and%20Shiguang%20Shan&entry.1292438233=%20%20The%20visual%20pathway%20of%20human%20brain%20includes%20two%20sub-pathways%2C%20ie%2C%20the%20ventral%0Apathway%20and%20the%20dorsal%20pathway%2C%20which%20focus%20on%20object%20identification%20and%0Adynamic%20information%20modeling%2C%20respectively.%20Both%20pathways%20comprise%20multi-layer%0Astructures%2C%20with%20each%20layer%20responsible%20for%20processing%20different%20aspects%20of%0Avisual%20information.%20Inspired%20by%20visual%20information%20processing%20mechanism%20of%20the%0Ahuman%20brain%2C%20we%20propose%20the%20Brain%20Inspired%20Masked%20Modeling%20%28BIMM%29%20framework%2C%0Aaiming%20to%20learn%20comprehensive%20representations%20from%20videos.%20Specifically%2C%20our%0Aapproach%20consists%20of%20ventral%20and%20dorsal%20branches%2C%20which%20learn%20image%20and%20video%0Arepresentations%2C%20respectively.%20Both%20branches%20employ%20the%20Vision%20Transformer%0A%28ViT%29%20as%20their%20backbone%20and%20are%20trained%20using%20masked%20modeling%20method.%20To%0Aachieve%20the%20goals%20of%20different%20visual%20cortices%20in%20the%20brain%2C%20we%20segment%20the%0Aencoder%20of%20each%20branch%20into%20three%20intermediate%20blocks%20and%20reconstruct%0Aprogressive%20prediction%20targets%20with%20light%20weight%20decoders.%20Furthermore%2C%20drawing%0Ainspiration%20from%20the%20information-sharing%20mechanism%20in%20the%20visual%20pathways%2C%20we%0Apropose%20a%20partial%20parameter%20sharing%20strategy%20between%20the%20branches%20during%0Atraining.%20Extensive%20experiments%20demonstrate%20that%20BIMM%20achieves%20superior%0Aperformance%20compared%20to%20the%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12757v1&entry.124074799=Read"},
{"title": "Text-Video Retrieval with Global-Local Semantic Consistent Learning", "author": "Haonan Zhang and Pengpeng Zeng and Lianli Gao and Jingkuan Song and Yihang Duan and Xinyu Lyu and Hengtao Shen", "abstract": "  Adapting large-scale image-text pre-training models, e.g., CLIP, to the video\ndomain represents the current state-of-the-art for text-video retrieval. The\nprimary approaches involve transferring text-video pairs to a common embedding\nspace and leveraging cross-modal interactions on specific entities for semantic\nalignment. Though effective, these paradigms entail prohibitive computational\ncosts, leading to inefficient retrieval. To address this, we propose a simple\nyet effective method, Global-Local Semantic Consistent Learning (GLSCL), which\ncapitalizes on latent shared semantics across modalities for text-video\nretrieval. Specifically, we introduce a parameter-free global interaction\nmodule to explore coarse-grained alignment. Then, we devise a shared local\ninteraction module that employs several learnable queries to capture latent\nsemantic concepts for learning fine-grained alignment. Furthermore, an\nInter-Consistency Loss (ICL) is devised to accomplish the concept alignment\nbetween the visual query and corresponding textual query, and an\nIntra-Diversity Loss (IDL) is developed to repulse the distribution within\nvisual (textual) queries to generate more discriminative concepts. Extensive\nexperiments on five widely used benchmarks (i.e., MSR-VTT, MSVD, DiDeMo, LSMDC,\nand ActivityNet) substantiate the superior effectiveness and efficiency of the\nproposed method. Remarkably, our method achieves comparable performance with\nSOTA as well as being nearly 220 times faster in terms of computational cost.\nCode is available at: https://github.com/zchoi/GLSCL.\n", "link": "http://arxiv.org/abs/2405.12710v1", "date": "2024-05-21", "relevancy": 2.3851, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6165}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5923}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-Video%20Retrieval%20with%20Global-Local%20Semantic%20Consistent%20Learning&body=Title%3A%20Text-Video%20Retrieval%20with%20Global-Local%20Semantic%20Consistent%20Learning%0AAuthor%3A%20Haonan%20Zhang%20and%20Pengpeng%20Zeng%20and%20Lianli%20Gao%20and%20Jingkuan%20Song%20and%20Yihang%20Duan%20and%20Xinyu%20Lyu%20and%20Hengtao%20Shen%0AAbstract%3A%20%20%20Adapting%20large-scale%20image-text%20pre-training%20models%2C%20e.g.%2C%20CLIP%2C%20to%20the%20video%0Adomain%20represents%20the%20current%20state-of-the-art%20for%20text-video%20retrieval.%20The%0Aprimary%20approaches%20involve%20transferring%20text-video%20pairs%20to%20a%20common%20embedding%0Aspace%20and%20leveraging%20cross-modal%20interactions%20on%20specific%20entities%20for%20semantic%0Aalignment.%20Though%20effective%2C%20these%20paradigms%20entail%20prohibitive%20computational%0Acosts%2C%20leading%20to%20inefficient%20retrieval.%20To%20address%20this%2C%20we%20propose%20a%20simple%0Ayet%20effective%20method%2C%20Global-Local%20Semantic%20Consistent%20Learning%20%28GLSCL%29%2C%20which%0Acapitalizes%20on%20latent%20shared%20semantics%20across%20modalities%20for%20text-video%0Aretrieval.%20Specifically%2C%20we%20introduce%20a%20parameter-free%20global%20interaction%0Amodule%20to%20explore%20coarse-grained%20alignment.%20Then%2C%20we%20devise%20a%20shared%20local%0Ainteraction%20module%20that%20employs%20several%20learnable%20queries%20to%20capture%20latent%0Asemantic%20concepts%20for%20learning%20fine-grained%20alignment.%20Furthermore%2C%20an%0AInter-Consistency%20Loss%20%28ICL%29%20is%20devised%20to%20accomplish%20the%20concept%20alignment%0Abetween%20the%20visual%20query%20and%20corresponding%20textual%20query%2C%20and%20an%0AIntra-Diversity%20Loss%20%28IDL%29%20is%20developed%20to%20repulse%20the%20distribution%20within%0Avisual%20%28textual%29%20queries%20to%20generate%20more%20discriminative%20concepts.%20Extensive%0Aexperiments%20on%20five%20widely%20used%20benchmarks%20%28i.e.%2C%20MSR-VTT%2C%20MSVD%2C%20DiDeMo%2C%20LSMDC%2C%0Aand%20ActivityNet%29%20substantiate%20the%20superior%20effectiveness%20and%20efficiency%20of%20the%0Aproposed%20method.%20Remarkably%2C%20our%20method%20achieves%20comparable%20performance%20with%0ASOTA%20as%20well%20as%20being%20nearly%20220%20times%20faster%20in%20terms%20of%20computational%20cost.%0ACode%20is%20available%20at%3A%20https%3A//github.com/zchoi/GLSCL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-Video%2520Retrieval%2520with%2520Global-Local%2520Semantic%2520Consistent%2520Learning%26entry.906535625%3DHaonan%2520Zhang%2520and%2520Pengpeng%2520Zeng%2520and%2520Lianli%2520Gao%2520and%2520Jingkuan%2520Song%2520and%2520Yihang%2520Duan%2520and%2520Xinyu%2520Lyu%2520and%2520Hengtao%2520Shen%26entry.1292438233%3D%2520%2520Adapting%2520large-scale%2520image-text%2520pre-training%2520models%252C%2520e.g.%252C%2520CLIP%252C%2520to%2520the%2520video%250Adomain%2520represents%2520the%2520current%2520state-of-the-art%2520for%2520text-video%2520retrieval.%2520The%250Aprimary%2520approaches%2520involve%2520transferring%2520text-video%2520pairs%2520to%2520a%2520common%2520embedding%250Aspace%2520and%2520leveraging%2520cross-modal%2520interactions%2520on%2520specific%2520entities%2520for%2520semantic%250Aalignment.%2520Though%2520effective%252C%2520these%2520paradigms%2520entail%2520prohibitive%2520computational%250Acosts%252C%2520leading%2520to%2520inefficient%2520retrieval.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520simple%250Ayet%2520effective%2520method%252C%2520Global-Local%2520Semantic%2520Consistent%2520Learning%2520%2528GLSCL%2529%252C%2520which%250Acapitalizes%2520on%2520latent%2520shared%2520semantics%2520across%2520modalities%2520for%2520text-video%250Aretrieval.%2520Specifically%252C%2520we%2520introduce%2520a%2520parameter-free%2520global%2520interaction%250Amodule%2520to%2520explore%2520coarse-grained%2520alignment.%2520Then%252C%2520we%2520devise%2520a%2520shared%2520local%250Ainteraction%2520module%2520that%2520employs%2520several%2520learnable%2520queries%2520to%2520capture%2520latent%250Asemantic%2520concepts%2520for%2520learning%2520fine-grained%2520alignment.%2520Furthermore%252C%2520an%250AInter-Consistency%2520Loss%2520%2528ICL%2529%2520is%2520devised%2520to%2520accomplish%2520the%2520concept%2520alignment%250Abetween%2520the%2520visual%2520query%2520and%2520corresponding%2520textual%2520query%252C%2520and%2520an%250AIntra-Diversity%2520Loss%2520%2528IDL%2529%2520is%2520developed%2520to%2520repulse%2520the%2520distribution%2520within%250Avisual%2520%2528textual%2529%2520queries%2520to%2520generate%2520more%2520discriminative%2520concepts.%2520Extensive%250Aexperiments%2520on%2520five%2520widely%2520used%2520benchmarks%2520%2528i.e.%252C%2520MSR-VTT%252C%2520MSVD%252C%2520DiDeMo%252C%2520LSMDC%252C%250Aand%2520ActivityNet%2529%2520substantiate%2520the%2520superior%2520effectiveness%2520and%2520efficiency%2520of%2520the%250Aproposed%2520method.%2520Remarkably%252C%2520our%2520method%2520achieves%2520comparable%2520performance%2520with%250ASOTA%2520as%2520well%2520as%2520being%2520nearly%2520220%2520times%2520faster%2520in%2520terms%2520of%2520computational%2520cost.%250ACode%2520is%2520available%2520at%253A%2520https%253A//github.com/zchoi/GLSCL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-Video%20Retrieval%20with%20Global-Local%20Semantic%20Consistent%20Learning&entry.906535625=Haonan%20Zhang%20and%20Pengpeng%20Zeng%20and%20Lianli%20Gao%20and%20Jingkuan%20Song%20and%20Yihang%20Duan%20and%20Xinyu%20Lyu%20and%20Hengtao%20Shen&entry.1292438233=%20%20Adapting%20large-scale%20image-text%20pre-training%20models%2C%20e.g.%2C%20CLIP%2C%20to%20the%20video%0Adomain%20represents%20the%20current%20state-of-the-art%20for%20text-video%20retrieval.%20The%0Aprimary%20approaches%20involve%20transferring%20text-video%20pairs%20to%20a%20common%20embedding%0Aspace%20and%20leveraging%20cross-modal%20interactions%20on%20specific%20entities%20for%20semantic%0Aalignment.%20Though%20effective%2C%20these%20paradigms%20entail%20prohibitive%20computational%0Acosts%2C%20leading%20to%20inefficient%20retrieval.%20To%20address%20this%2C%20we%20propose%20a%20simple%0Ayet%20effective%20method%2C%20Global-Local%20Semantic%20Consistent%20Learning%20%28GLSCL%29%2C%20which%0Acapitalizes%20on%20latent%20shared%20semantics%20across%20modalities%20for%20text-video%0Aretrieval.%20Specifically%2C%20we%20introduce%20a%20parameter-free%20global%20interaction%0Amodule%20to%20explore%20coarse-grained%20alignment.%20Then%2C%20we%20devise%20a%20shared%20local%0Ainteraction%20module%20that%20employs%20several%20learnable%20queries%20to%20capture%20latent%0Asemantic%20concepts%20for%20learning%20fine-grained%20alignment.%20Furthermore%2C%20an%0AInter-Consistency%20Loss%20%28ICL%29%20is%20devised%20to%20accomplish%20the%20concept%20alignment%0Abetween%20the%20visual%20query%20and%20corresponding%20textual%20query%2C%20and%20an%0AIntra-Diversity%20Loss%20%28IDL%29%20is%20developed%20to%20repulse%20the%20distribution%20within%0Avisual%20%28textual%29%20queries%20to%20generate%20more%20discriminative%20concepts.%20Extensive%0Aexperiments%20on%20five%20widely%20used%20benchmarks%20%28i.e.%2C%20MSR-VTT%2C%20MSVD%2C%20DiDeMo%2C%20LSMDC%2C%0Aand%20ActivityNet%29%20substantiate%20the%20superior%20effectiveness%20and%20efficiency%20of%20the%0Aproposed%20method.%20Remarkably%2C%20our%20method%20achieves%20comparable%20performance%20with%0ASOTA%20as%20well%20as%20being%20nearly%20220%20times%20faster%20in%20terms%20of%20computational%20cost.%0ACode%20is%20available%20at%3A%20https%3A//github.com/zchoi/GLSCL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12710v1&entry.124074799=Read"},
{"title": "Weakly supervised alignment and registration of MR-CT for cervical\n  cancer radiotherapy", "author": "Jjahao Zhang and Yin Gu and Deyu Sun and Yuhua Gao and Ming Gao and Ming Cui and Teng Zhang and He Ma", "abstract": "  Cervical cancer is one of the leading causes of death in women, and\nbrachytherapy is currently the primary treatment method. However, it is\nimportant to precisely define the extent of paracervical tissue invasion to\nimprove cancer diagnosis and treatment options. The fusion of the information\ncharacteristics of both computed tomography (CT) and magnetic resonance\nimaging(MRI) modalities may be useful in achieving a precise outline of the\nextent of paracervical tissue invasion. Registration is the initial step in\ninformation fusion. However, when aligning multimodal images with varying\ndepths, manual alignment is prone to large errors and is time-consuming.\nFurthermore, the variations in the size of the Region of Interest (ROI) and the\nshape of multimodal images pose a significant challenge for achieving accurate\nregistration.In this paper, we propose a preliminary spatial alignment\nalgorithm and a weakly supervised multimodal registration network. The spatial\nposition alignment algorithm efficiently utilizes the limited annotation\ninformation in the two modal images provided by the doctor to automatically\nalign multimodal images with varying depths. By utilizing aligned multimodal\nimages for weakly supervised registration and incorporating pyramidal features\nand cost volume to estimate the optical flow, the results indicate that the\nproposed method outperforms traditional volume rendering alignment methods and\nregistration networks in various evaluation metrics. This demonstrates the\neffectiveness of our model in multimodal image registration.\n", "link": "http://arxiv.org/abs/2405.12850v1", "date": "2024-05-21", "relevancy": 2.3796, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4825}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4735}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly%20supervised%20alignment%20and%20registration%20of%20MR-CT%20for%20cervical%0A%20%20cancer%20radiotherapy&body=Title%3A%20Weakly%20supervised%20alignment%20and%20registration%20of%20MR-CT%20for%20cervical%0A%20%20cancer%20radiotherapy%0AAuthor%3A%20Jjahao%20Zhang%20and%20Yin%20Gu%20and%20Deyu%20Sun%20and%20Yuhua%20Gao%20and%20Ming%20Gao%20and%20Ming%20Cui%20and%20Teng%20Zhang%20and%20He%20Ma%0AAbstract%3A%20%20%20Cervical%20cancer%20is%20one%20of%20the%20leading%20causes%20of%20death%20in%20women%2C%20and%0Abrachytherapy%20is%20currently%20the%20primary%20treatment%20method.%20However%2C%20it%20is%0Aimportant%20to%20precisely%20define%20the%20extent%20of%20paracervical%20tissue%20invasion%20to%0Aimprove%20cancer%20diagnosis%20and%20treatment%20options.%20The%20fusion%20of%20the%20information%0Acharacteristics%20of%20both%20computed%20tomography%20%28CT%29%20and%20magnetic%20resonance%0Aimaging%28MRI%29%20modalities%20may%20be%20useful%20in%20achieving%20a%20precise%20outline%20of%20the%0Aextent%20of%20paracervical%20tissue%20invasion.%20Registration%20is%20the%20initial%20step%20in%0Ainformation%20fusion.%20However%2C%20when%20aligning%20multimodal%20images%20with%20varying%0Adepths%2C%20manual%20alignment%20is%20prone%20to%20large%20errors%20and%20is%20time-consuming.%0AFurthermore%2C%20the%20variations%20in%20the%20size%20of%20the%20Region%20of%20Interest%20%28ROI%29%20and%20the%0Ashape%20of%20multimodal%20images%20pose%20a%20significant%20challenge%20for%20achieving%20accurate%0Aregistration.In%20this%20paper%2C%20we%20propose%20a%20preliminary%20spatial%20alignment%0Aalgorithm%20and%20a%20weakly%20supervised%20multimodal%20registration%20network.%20The%20spatial%0Aposition%20alignment%20algorithm%20efficiently%20utilizes%20the%20limited%20annotation%0Ainformation%20in%20the%20two%20modal%20images%20provided%20by%20the%20doctor%20to%20automatically%0Aalign%20multimodal%20images%20with%20varying%20depths.%20By%20utilizing%20aligned%20multimodal%0Aimages%20for%20weakly%20supervised%20registration%20and%20incorporating%20pyramidal%20features%0Aand%20cost%20volume%20to%20estimate%20the%20optical%20flow%2C%20the%20results%20indicate%20that%20the%0Aproposed%20method%20outperforms%20traditional%20volume%20rendering%20alignment%20methods%20and%0Aregistration%20networks%20in%20various%20evaluation%20metrics.%20This%20demonstrates%20the%0Aeffectiveness%20of%20our%20model%20in%20multimodal%20image%20registration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly%2520supervised%2520alignment%2520and%2520registration%2520of%2520MR-CT%2520for%2520cervical%250A%2520%2520cancer%2520radiotherapy%26entry.906535625%3DJjahao%2520Zhang%2520and%2520Yin%2520Gu%2520and%2520Deyu%2520Sun%2520and%2520Yuhua%2520Gao%2520and%2520Ming%2520Gao%2520and%2520Ming%2520Cui%2520and%2520Teng%2520Zhang%2520and%2520He%2520Ma%26entry.1292438233%3D%2520%2520Cervical%2520cancer%2520is%2520one%2520of%2520the%2520leading%2520causes%2520of%2520death%2520in%2520women%252C%2520and%250Abrachytherapy%2520is%2520currently%2520the%2520primary%2520treatment%2520method.%2520However%252C%2520it%2520is%250Aimportant%2520to%2520precisely%2520define%2520the%2520extent%2520of%2520paracervical%2520tissue%2520invasion%2520to%250Aimprove%2520cancer%2520diagnosis%2520and%2520treatment%2520options.%2520The%2520fusion%2520of%2520the%2520information%250Acharacteristics%2520of%2520both%2520computed%2520tomography%2520%2528CT%2529%2520and%2520magnetic%2520resonance%250Aimaging%2528MRI%2529%2520modalities%2520may%2520be%2520useful%2520in%2520achieving%2520a%2520precise%2520outline%2520of%2520the%250Aextent%2520of%2520paracervical%2520tissue%2520invasion.%2520Registration%2520is%2520the%2520initial%2520step%2520in%250Ainformation%2520fusion.%2520However%252C%2520when%2520aligning%2520multimodal%2520images%2520with%2520varying%250Adepths%252C%2520manual%2520alignment%2520is%2520prone%2520to%2520large%2520errors%2520and%2520is%2520time-consuming.%250AFurthermore%252C%2520the%2520variations%2520in%2520the%2520size%2520of%2520the%2520Region%2520of%2520Interest%2520%2528ROI%2529%2520and%2520the%250Ashape%2520of%2520multimodal%2520images%2520pose%2520a%2520significant%2520challenge%2520for%2520achieving%2520accurate%250Aregistration.In%2520this%2520paper%252C%2520we%2520propose%2520a%2520preliminary%2520spatial%2520alignment%250Aalgorithm%2520and%2520a%2520weakly%2520supervised%2520multimodal%2520registration%2520network.%2520The%2520spatial%250Aposition%2520alignment%2520algorithm%2520efficiently%2520utilizes%2520the%2520limited%2520annotation%250Ainformation%2520in%2520the%2520two%2520modal%2520images%2520provided%2520by%2520the%2520doctor%2520to%2520automatically%250Aalign%2520multimodal%2520images%2520with%2520varying%2520depths.%2520By%2520utilizing%2520aligned%2520multimodal%250Aimages%2520for%2520weakly%2520supervised%2520registration%2520and%2520incorporating%2520pyramidal%2520features%250Aand%2520cost%2520volume%2520to%2520estimate%2520the%2520optical%2520flow%252C%2520the%2520results%2520indicate%2520that%2520the%250Aproposed%2520method%2520outperforms%2520traditional%2520volume%2520rendering%2520alignment%2520methods%2520and%250Aregistration%2520networks%2520in%2520various%2520evaluation%2520metrics.%2520This%2520demonstrates%2520the%250Aeffectiveness%2520of%2520our%2520model%2520in%2520multimodal%2520image%2520registration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly%20supervised%20alignment%20and%20registration%20of%20MR-CT%20for%20cervical%0A%20%20cancer%20radiotherapy&entry.906535625=Jjahao%20Zhang%20and%20Yin%20Gu%20and%20Deyu%20Sun%20and%20Yuhua%20Gao%20and%20Ming%20Gao%20and%20Ming%20Cui%20and%20Teng%20Zhang%20and%20He%20Ma&entry.1292438233=%20%20Cervical%20cancer%20is%20one%20of%20the%20leading%20causes%20of%20death%20in%20women%2C%20and%0Abrachytherapy%20is%20currently%20the%20primary%20treatment%20method.%20However%2C%20it%20is%0Aimportant%20to%20precisely%20define%20the%20extent%20of%20paracervical%20tissue%20invasion%20to%0Aimprove%20cancer%20diagnosis%20and%20treatment%20options.%20The%20fusion%20of%20the%20information%0Acharacteristics%20of%20both%20computed%20tomography%20%28CT%29%20and%20magnetic%20resonance%0Aimaging%28MRI%29%20modalities%20may%20be%20useful%20in%20achieving%20a%20precise%20outline%20of%20the%0Aextent%20of%20paracervical%20tissue%20invasion.%20Registration%20is%20the%20initial%20step%20in%0Ainformation%20fusion.%20However%2C%20when%20aligning%20multimodal%20images%20with%20varying%0Adepths%2C%20manual%20alignment%20is%20prone%20to%20large%20errors%20and%20is%20time-consuming.%0AFurthermore%2C%20the%20variations%20in%20the%20size%20of%20the%20Region%20of%20Interest%20%28ROI%29%20and%20the%0Ashape%20of%20multimodal%20images%20pose%20a%20significant%20challenge%20for%20achieving%20accurate%0Aregistration.In%20this%20paper%2C%20we%20propose%20a%20preliminary%20spatial%20alignment%0Aalgorithm%20and%20a%20weakly%20supervised%20multimodal%20registration%20network.%20The%20spatial%0Aposition%20alignment%20algorithm%20efficiently%20utilizes%20the%20limited%20annotation%0Ainformation%20in%20the%20two%20modal%20images%20provided%20by%20the%20doctor%20to%20automatically%0Aalign%20multimodal%20images%20with%20varying%20depths.%20By%20utilizing%20aligned%20multimodal%0Aimages%20for%20weakly%20supervised%20registration%20and%20incorporating%20pyramidal%20features%0Aand%20cost%20volume%20to%20estimate%20the%20optical%20flow%2C%20the%20results%20indicate%20that%20the%0Aproposed%20method%20outperforms%20traditional%20volume%20rendering%20alignment%20methods%20and%0Aregistration%20networks%20in%20various%20evaluation%20metrics.%20This%20demonstrates%20the%0Aeffectiveness%20of%20our%20model%20in%20multimodal%20image%20registration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12850v1&entry.124074799=Read"},
{"title": "Lift-Attend-Splat: Bird's-eye-view camera-lidar fusion using\n  transformers", "author": "James Gunn and Zygmunt Lenyk and Anuj Sharma and Andrea Donati and Alexandru Buburuzan and John Redford and Romain Mueller", "abstract": "  Combining complementary sensor modalities is crucial to providing robust\nperception for safety-critical robotics applications such as autonomous driving\n(AD). Recent state-of-the-art camera-lidar fusion methods for AD rely on\nmonocular depth estimation which is a notoriously difficult task compared to\nusing depth information from the lidar directly. Here, we find that this\napproach does not leverage depth as expected and show that naively improving\ndepth estimation does not lead to improvements in object detection performance.\nStrikingly, we also find that removing depth estimation altogether does not\ndegrade object detection performance substantially, suggesting that relying on\nmonocular depth could be an unnecessary architectural bottleneck during\ncamera-lidar fusion. In this work, we introduce a novel fusion method that\nbypasses monocular depth estimation altogether and instead selects and fuses\ncamera and lidar features in a bird's-eye-view grid using a simple attention\nmechanism. We show that our model can modulate its use of camera features based\non the availability of lidar features and that it yields better 3D object\ndetection on the nuScenes dataset than baselines relying on monocular depth\nestimation.\n", "link": "http://arxiv.org/abs/2312.14919v3", "date": "2024-05-21", "relevancy": 2.2907, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5854}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5672}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lift-Attend-Splat%3A%20Bird%27s-eye-view%20camera-lidar%20fusion%20using%0A%20%20transformers&body=Title%3A%20Lift-Attend-Splat%3A%20Bird%27s-eye-view%20camera-lidar%20fusion%20using%0A%20%20transformers%0AAuthor%3A%20James%20Gunn%20and%20Zygmunt%20Lenyk%20and%20Anuj%20Sharma%20and%20Andrea%20Donati%20and%20Alexandru%20Buburuzan%20and%20John%20Redford%20and%20Romain%20Mueller%0AAbstract%3A%20%20%20Combining%20complementary%20sensor%20modalities%20is%20crucial%20to%20providing%20robust%0Aperception%20for%20safety-critical%20robotics%20applications%20such%20as%20autonomous%20driving%0A%28AD%29.%20Recent%20state-of-the-art%20camera-lidar%20fusion%20methods%20for%20AD%20rely%20on%0Amonocular%20depth%20estimation%20which%20is%20a%20notoriously%20difficult%20task%20compared%20to%0Ausing%20depth%20information%20from%20the%20lidar%20directly.%20Here%2C%20we%20find%20that%20this%0Aapproach%20does%20not%20leverage%20depth%20as%20expected%20and%20show%20that%20naively%20improving%0Adepth%20estimation%20does%20not%20lead%20to%20improvements%20in%20object%20detection%20performance.%0AStrikingly%2C%20we%20also%20find%20that%20removing%20depth%20estimation%20altogether%20does%20not%0Adegrade%20object%20detection%20performance%20substantially%2C%20suggesting%20that%20relying%20on%0Amonocular%20depth%20could%20be%20an%20unnecessary%20architectural%20bottleneck%20during%0Acamera-lidar%20fusion.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20fusion%20method%20that%0Abypasses%20monocular%20depth%20estimation%20altogether%20and%20instead%20selects%20and%20fuses%0Acamera%20and%20lidar%20features%20in%20a%20bird%27s-eye-view%20grid%20using%20a%20simple%20attention%0Amechanism.%20We%20show%20that%20our%20model%20can%20modulate%20its%20use%20of%20camera%20features%20based%0Aon%20the%20availability%20of%20lidar%20features%20and%20that%20it%20yields%20better%203D%20object%0Adetection%20on%20the%20nuScenes%20dataset%20than%20baselines%20relying%20on%20monocular%20depth%0Aestimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14919v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLift-Attend-Splat%253A%2520Bird%2527s-eye-view%2520camera-lidar%2520fusion%2520using%250A%2520%2520transformers%26entry.906535625%3DJames%2520Gunn%2520and%2520Zygmunt%2520Lenyk%2520and%2520Anuj%2520Sharma%2520and%2520Andrea%2520Donati%2520and%2520Alexandru%2520Buburuzan%2520and%2520John%2520Redford%2520and%2520Romain%2520Mueller%26entry.1292438233%3D%2520%2520Combining%2520complementary%2520sensor%2520modalities%2520is%2520crucial%2520to%2520providing%2520robust%250Aperception%2520for%2520safety-critical%2520robotics%2520applications%2520such%2520as%2520autonomous%2520driving%250A%2528AD%2529.%2520Recent%2520state-of-the-art%2520camera-lidar%2520fusion%2520methods%2520for%2520AD%2520rely%2520on%250Amonocular%2520depth%2520estimation%2520which%2520is%2520a%2520notoriously%2520difficult%2520task%2520compared%2520to%250Ausing%2520depth%2520information%2520from%2520the%2520lidar%2520directly.%2520Here%252C%2520we%2520find%2520that%2520this%250Aapproach%2520does%2520not%2520leverage%2520depth%2520as%2520expected%2520and%2520show%2520that%2520naively%2520improving%250Adepth%2520estimation%2520does%2520not%2520lead%2520to%2520improvements%2520in%2520object%2520detection%2520performance.%250AStrikingly%252C%2520we%2520also%2520find%2520that%2520removing%2520depth%2520estimation%2520altogether%2520does%2520not%250Adegrade%2520object%2520detection%2520performance%2520substantially%252C%2520suggesting%2520that%2520relying%2520on%250Amonocular%2520depth%2520could%2520be%2520an%2520unnecessary%2520architectural%2520bottleneck%2520during%250Acamera-lidar%2520fusion.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520fusion%2520method%2520that%250Abypasses%2520monocular%2520depth%2520estimation%2520altogether%2520and%2520instead%2520selects%2520and%2520fuses%250Acamera%2520and%2520lidar%2520features%2520in%2520a%2520bird%2527s-eye-view%2520grid%2520using%2520a%2520simple%2520attention%250Amechanism.%2520We%2520show%2520that%2520our%2520model%2520can%2520modulate%2520its%2520use%2520of%2520camera%2520features%2520based%250Aon%2520the%2520availability%2520of%2520lidar%2520features%2520and%2520that%2520it%2520yields%2520better%25203D%2520object%250Adetection%2520on%2520the%2520nuScenes%2520dataset%2520than%2520baselines%2520relying%2520on%2520monocular%2520depth%250Aestimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14919v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lift-Attend-Splat%3A%20Bird%27s-eye-view%20camera-lidar%20fusion%20using%0A%20%20transformers&entry.906535625=James%20Gunn%20and%20Zygmunt%20Lenyk%20and%20Anuj%20Sharma%20and%20Andrea%20Donati%20and%20Alexandru%20Buburuzan%20and%20John%20Redford%20and%20Romain%20Mueller&entry.1292438233=%20%20Combining%20complementary%20sensor%20modalities%20is%20crucial%20to%20providing%20robust%0Aperception%20for%20safety-critical%20robotics%20applications%20such%20as%20autonomous%20driving%0A%28AD%29.%20Recent%20state-of-the-art%20camera-lidar%20fusion%20methods%20for%20AD%20rely%20on%0Amonocular%20depth%20estimation%20which%20is%20a%20notoriously%20difficult%20task%20compared%20to%0Ausing%20depth%20information%20from%20the%20lidar%20directly.%20Here%2C%20we%20find%20that%20this%0Aapproach%20does%20not%20leverage%20depth%20as%20expected%20and%20show%20that%20naively%20improving%0Adepth%20estimation%20does%20not%20lead%20to%20improvements%20in%20object%20detection%20performance.%0AStrikingly%2C%20we%20also%20find%20that%20removing%20depth%20estimation%20altogether%20does%20not%0Adegrade%20object%20detection%20performance%20substantially%2C%20suggesting%20that%20relying%20on%0Amonocular%20depth%20could%20be%20an%20unnecessary%20architectural%20bottleneck%20during%0Acamera-lidar%20fusion.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20fusion%20method%20that%0Abypasses%20monocular%20depth%20estimation%20altogether%20and%20instead%20selects%20and%20fuses%0Acamera%20and%20lidar%20features%20in%20a%20bird%27s-eye-view%20grid%20using%20a%20simple%20attention%0Amechanism.%20We%20show%20that%20our%20model%20can%20modulate%20its%20use%20of%20camera%20features%20based%0Aon%20the%20availability%20of%20lidar%20features%20and%20that%20it%20yields%20better%203D%20object%0Adetection%20on%20the%20nuScenes%20dataset%20than%20baselines%20relying%20on%20monocular%20depth%0Aestimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14919v3&entry.124074799=Read"},
{"title": "StarLKNet: Star Mixup with Large Kernel Networks for Palm Vein\n  Identification", "author": "Xin Jin and Hongyu Zhu and Moun\u00eem A. El Yacoubi and Hongchao Liao and Huafeng Qin and Yun Jiang", "abstract": "  As a representative of a new generation of biometrics, vein identification\ntechnology offers a high level of security and convenience. Convolutional\nneural networks (CNNs), a prominent class of deep learning architectures, have\nbeen extensively utilized for vein identification. Since their performance and\nrobustness are limited by small Effective Receptive Fields (e.g. 3$\\times$3\nkernels) and insufficient training samples, however, they are unable to extract\nglobal feature representations from vein images in an effective manner. To\naddress these issues, we propose StarLKNet, a large kernel convolution-based\npalm-vein identification network, with the Mixup approach. Our StarMix learns\neffectively the distribution of vein features to expand samples. To enable CNNs\nto capture comprehensive feature representations from palm-vein images, we\nexplored the effect of convolutional kernel size on the performance of\npalm-vein identification networks and designed LaKNet, a network leveraging\nlarge kernel convolution and gating mechanism. In light of the current state of\nknowledge, this represents an inaugural instance of the deployment of a CNN\nwith large kernels in the domain of vein identification. Extensive experiments\nwere conducted to validate the performance of StarLKNet on two public palm-vein\ndatasets. The results demonstrated that StarMix provided superior augmentation,\nand LakNet exhibited more stable performance gains compared to mainstream\napproaches, resulting in the highest recognition accuracy and lowest\nidentification error.\n", "link": "http://arxiv.org/abs/2405.12721v1", "date": "2024-05-21", "relevancy": 2.2899, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4686}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4684}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StarLKNet%3A%20Star%20Mixup%20with%20Large%20Kernel%20Networks%20for%20Palm%20Vein%0A%20%20Identification&body=Title%3A%20StarLKNet%3A%20Star%20Mixup%20with%20Large%20Kernel%20Networks%20for%20Palm%20Vein%0A%20%20Identification%0AAuthor%3A%20Xin%20Jin%20and%20Hongyu%20Zhu%20and%20Moun%C3%AEm%20A.%20El%20Yacoubi%20and%20Hongchao%20Liao%20and%20Huafeng%20Qin%20and%20Yun%20Jiang%0AAbstract%3A%20%20%20As%20a%20representative%20of%20a%20new%20generation%20of%20biometrics%2C%20vein%20identification%0Atechnology%20offers%20a%20high%20level%20of%20security%20and%20convenience.%20Convolutional%0Aneural%20networks%20%28CNNs%29%2C%20a%20prominent%20class%20of%20deep%20learning%20architectures%2C%20have%0Abeen%20extensively%20utilized%20for%20vein%20identification.%20Since%20their%20performance%20and%0Arobustness%20are%20limited%20by%20small%20Effective%20Receptive%20Fields%20%28e.g.%203%24%5Ctimes%243%0Akernels%29%20and%20insufficient%20training%20samples%2C%20however%2C%20they%20are%20unable%20to%20extract%0Aglobal%20feature%20representations%20from%20vein%20images%20in%20an%20effective%20manner.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20StarLKNet%2C%20a%20large%20kernel%20convolution-based%0Apalm-vein%20identification%20network%2C%20with%20the%20Mixup%20approach.%20Our%20StarMix%20learns%0Aeffectively%20the%20distribution%20of%20vein%20features%20to%20expand%20samples.%20To%20enable%20CNNs%0Ato%20capture%20comprehensive%20feature%20representations%20from%20palm-vein%20images%2C%20we%0Aexplored%20the%20effect%20of%20convolutional%20kernel%20size%20on%20the%20performance%20of%0Apalm-vein%20identification%20networks%20and%20designed%20LaKNet%2C%20a%20network%20leveraging%0Alarge%20kernel%20convolution%20and%20gating%20mechanism.%20In%20light%20of%20the%20current%20state%20of%0Aknowledge%2C%20this%20represents%20an%20inaugural%20instance%20of%20the%20deployment%20of%20a%20CNN%0Awith%20large%20kernels%20in%20the%20domain%20of%20vein%20identification.%20Extensive%20experiments%0Awere%20conducted%20to%20validate%20the%20performance%20of%20StarLKNet%20on%20two%20public%20palm-vein%0Adatasets.%20The%20results%20demonstrated%20that%20StarMix%20provided%20superior%20augmentation%2C%0Aand%20LakNet%20exhibited%20more%20stable%20performance%20gains%20compared%20to%20mainstream%0Aapproaches%2C%20resulting%20in%20the%20highest%20recognition%20accuracy%20and%20lowest%0Aidentification%20error.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12721v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStarLKNet%253A%2520Star%2520Mixup%2520with%2520Large%2520Kernel%2520Networks%2520for%2520Palm%2520Vein%250A%2520%2520Identification%26entry.906535625%3DXin%2520Jin%2520and%2520Hongyu%2520Zhu%2520and%2520Moun%25C3%25AEm%2520A.%2520El%2520Yacoubi%2520and%2520Hongchao%2520Liao%2520and%2520Huafeng%2520Qin%2520and%2520Yun%2520Jiang%26entry.1292438233%3D%2520%2520As%2520a%2520representative%2520of%2520a%2520new%2520generation%2520of%2520biometrics%252C%2520vein%2520identification%250Atechnology%2520offers%2520a%2520high%2520level%2520of%2520security%2520and%2520convenience.%2520Convolutional%250Aneural%2520networks%2520%2528CNNs%2529%252C%2520a%2520prominent%2520class%2520of%2520deep%2520learning%2520architectures%252C%2520have%250Abeen%2520extensively%2520utilized%2520for%2520vein%2520identification.%2520Since%2520their%2520performance%2520and%250Arobustness%2520are%2520limited%2520by%2520small%2520Effective%2520Receptive%2520Fields%2520%2528e.g.%25203%2524%255Ctimes%25243%250Akernels%2529%2520and%2520insufficient%2520training%2520samples%252C%2520however%252C%2520they%2520are%2520unable%2520to%2520extract%250Aglobal%2520feature%2520representations%2520from%2520vein%2520images%2520in%2520an%2520effective%2520manner.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520propose%2520StarLKNet%252C%2520a%2520large%2520kernel%2520convolution-based%250Apalm-vein%2520identification%2520network%252C%2520with%2520the%2520Mixup%2520approach.%2520Our%2520StarMix%2520learns%250Aeffectively%2520the%2520distribution%2520of%2520vein%2520features%2520to%2520expand%2520samples.%2520To%2520enable%2520CNNs%250Ato%2520capture%2520comprehensive%2520feature%2520representations%2520from%2520palm-vein%2520images%252C%2520we%250Aexplored%2520the%2520effect%2520of%2520convolutional%2520kernel%2520size%2520on%2520the%2520performance%2520of%250Apalm-vein%2520identification%2520networks%2520and%2520designed%2520LaKNet%252C%2520a%2520network%2520leveraging%250Alarge%2520kernel%2520convolution%2520and%2520gating%2520mechanism.%2520In%2520light%2520of%2520the%2520current%2520state%2520of%250Aknowledge%252C%2520this%2520represents%2520an%2520inaugural%2520instance%2520of%2520the%2520deployment%2520of%2520a%2520CNN%250Awith%2520large%2520kernels%2520in%2520the%2520domain%2520of%2520vein%2520identification.%2520Extensive%2520experiments%250Awere%2520conducted%2520to%2520validate%2520the%2520performance%2520of%2520StarLKNet%2520on%2520two%2520public%2520palm-vein%250Adatasets.%2520The%2520results%2520demonstrated%2520that%2520StarMix%2520provided%2520superior%2520augmentation%252C%250Aand%2520LakNet%2520exhibited%2520more%2520stable%2520performance%2520gains%2520compared%2520to%2520mainstream%250Aapproaches%252C%2520resulting%2520in%2520the%2520highest%2520recognition%2520accuracy%2520and%2520lowest%250Aidentification%2520error.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12721v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StarLKNet%3A%20Star%20Mixup%20with%20Large%20Kernel%20Networks%20for%20Palm%20Vein%0A%20%20Identification&entry.906535625=Xin%20Jin%20and%20Hongyu%20Zhu%20and%20Moun%C3%AEm%20A.%20El%20Yacoubi%20and%20Hongchao%20Liao%20and%20Huafeng%20Qin%20and%20Yun%20Jiang&entry.1292438233=%20%20As%20a%20representative%20of%20a%20new%20generation%20of%20biometrics%2C%20vein%20identification%0Atechnology%20offers%20a%20high%20level%20of%20security%20and%20convenience.%20Convolutional%0Aneural%20networks%20%28CNNs%29%2C%20a%20prominent%20class%20of%20deep%20learning%20architectures%2C%20have%0Abeen%20extensively%20utilized%20for%20vein%20identification.%20Since%20their%20performance%20and%0Arobustness%20are%20limited%20by%20small%20Effective%20Receptive%20Fields%20%28e.g.%203%24%5Ctimes%243%0Akernels%29%20and%20insufficient%20training%20samples%2C%20however%2C%20they%20are%20unable%20to%20extract%0Aglobal%20feature%20representations%20from%20vein%20images%20in%20an%20effective%20manner.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20StarLKNet%2C%20a%20large%20kernel%20convolution-based%0Apalm-vein%20identification%20network%2C%20with%20the%20Mixup%20approach.%20Our%20StarMix%20learns%0Aeffectively%20the%20distribution%20of%20vein%20features%20to%20expand%20samples.%20To%20enable%20CNNs%0Ato%20capture%20comprehensive%20feature%20representations%20from%20palm-vein%20images%2C%20we%0Aexplored%20the%20effect%20of%20convolutional%20kernel%20size%20on%20the%20performance%20of%0Apalm-vein%20identification%20networks%20and%20designed%20LaKNet%2C%20a%20network%20leveraging%0Alarge%20kernel%20convolution%20and%20gating%20mechanism.%20In%20light%20of%20the%20current%20state%20of%0Aknowledge%2C%20this%20represents%20an%20inaugural%20instance%20of%20the%20deployment%20of%20a%20CNN%0Awith%20large%20kernels%20in%20the%20domain%20of%20vein%20identification.%20Extensive%20experiments%0Awere%20conducted%20to%20validate%20the%20performance%20of%20StarLKNet%20on%20two%20public%20palm-vein%0Adatasets.%20The%20results%20demonstrated%20that%20StarMix%20provided%20superior%20augmentation%2C%0Aand%20LakNet%20exhibited%20more%20stable%20performance%20gains%20compared%20to%20mainstream%0Aapproaches%2C%20resulting%20in%20the%20highest%20recognition%20accuracy%20and%20lowest%0Aidentification%20error.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12721v1&entry.124074799=Read"},
{"title": "DARK: Denoising, Amplification, Restoration Kit", "author": "Zhuoheng Li and Yuheng Pan and Houcheng Yu and Zhiheng Zhang", "abstract": "  This paper introduces a novel lightweight computational framework for\nenhancing images under low-light conditions, utilizing advanced machine\nlearning and convolutional neural networks (CNNs). Traditional enhancement\ntechniques often fail to adequately address issues like noise, color\ndistortion, and detail loss in challenging lighting environments. Our approach\nleverages insights from the Retinex theory and recent advances in image\nrestoration networks to develop a streamlined model that efficiently processes\nillumination components and integrates context-sensitive enhancements through\noptimized convolutional blocks. This results in significantly improved image\nclarity and color fidelity, while avoiding over-enhancement and unnatural color\nshifts. Crucially, our model is designed to be lightweight, ensuring low\ncomputational demand and suitability for real-time applications on standard\nconsumer hardware. Performance evaluations confirm that our model not only\nsurpasses existing methods in enhancing low-light images but also maintains a\nminimal computational footprint.\n", "link": "http://arxiv.org/abs/2405.12891v1", "date": "2024-05-21", "relevancy": 2.2735, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5889}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5787}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DARK%3A%20Denoising%2C%20Amplification%2C%20Restoration%20Kit&body=Title%3A%20DARK%3A%20Denoising%2C%20Amplification%2C%20Restoration%20Kit%0AAuthor%3A%20Zhuoheng%20Li%20and%20Yuheng%20Pan%20and%20Houcheng%20Yu%20and%20Zhiheng%20Zhang%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20lightweight%20computational%20framework%20for%0Aenhancing%20images%20under%20low-light%20conditions%2C%20utilizing%20advanced%20machine%0Alearning%20and%20convolutional%20neural%20networks%20%28CNNs%29.%20Traditional%20enhancement%0Atechniques%20often%20fail%20to%20adequately%20address%20issues%20like%20noise%2C%20color%0Adistortion%2C%20and%20detail%20loss%20in%20challenging%20lighting%20environments.%20Our%20approach%0Aleverages%20insights%20from%20the%20Retinex%20theory%20and%20recent%20advances%20in%20image%0Arestoration%20networks%20to%20develop%20a%20streamlined%20model%20that%20efficiently%20processes%0Aillumination%20components%20and%20integrates%20context-sensitive%20enhancements%20through%0Aoptimized%20convolutional%20blocks.%20This%20results%20in%20significantly%20improved%20image%0Aclarity%20and%20color%20fidelity%2C%20while%20avoiding%20over-enhancement%20and%20unnatural%20color%0Ashifts.%20Crucially%2C%20our%20model%20is%20designed%20to%20be%20lightweight%2C%20ensuring%20low%0Acomputational%20demand%20and%20suitability%20for%20real-time%20applications%20on%20standard%0Aconsumer%20hardware.%20Performance%20evaluations%20confirm%20that%20our%20model%20not%20only%0Asurpasses%20existing%20methods%20in%20enhancing%20low-light%20images%20but%20also%20maintains%20a%0Aminimal%20computational%20footprint.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDARK%253A%2520Denoising%252C%2520Amplification%252C%2520Restoration%2520Kit%26entry.906535625%3DZhuoheng%2520Li%2520and%2520Yuheng%2520Pan%2520and%2520Houcheng%2520Yu%2520and%2520Zhiheng%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520lightweight%2520computational%2520framework%2520for%250Aenhancing%2520images%2520under%2520low-light%2520conditions%252C%2520utilizing%2520advanced%2520machine%250Alearning%2520and%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529.%2520Traditional%2520enhancement%250Atechniques%2520often%2520fail%2520to%2520adequately%2520address%2520issues%2520like%2520noise%252C%2520color%250Adistortion%252C%2520and%2520detail%2520loss%2520in%2520challenging%2520lighting%2520environments.%2520Our%2520approach%250Aleverages%2520insights%2520from%2520the%2520Retinex%2520theory%2520and%2520recent%2520advances%2520in%2520image%250Arestoration%2520networks%2520to%2520develop%2520a%2520streamlined%2520model%2520that%2520efficiently%2520processes%250Aillumination%2520components%2520and%2520integrates%2520context-sensitive%2520enhancements%2520through%250Aoptimized%2520convolutional%2520blocks.%2520This%2520results%2520in%2520significantly%2520improved%2520image%250Aclarity%2520and%2520color%2520fidelity%252C%2520while%2520avoiding%2520over-enhancement%2520and%2520unnatural%2520color%250Ashifts.%2520Crucially%252C%2520our%2520model%2520is%2520designed%2520to%2520be%2520lightweight%252C%2520ensuring%2520low%250Acomputational%2520demand%2520and%2520suitability%2520for%2520real-time%2520applications%2520on%2520standard%250Aconsumer%2520hardware.%2520Performance%2520evaluations%2520confirm%2520that%2520our%2520model%2520not%2520only%250Asurpasses%2520existing%2520methods%2520in%2520enhancing%2520low-light%2520images%2520but%2520also%2520maintains%2520a%250Aminimal%2520computational%2520footprint.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DARK%3A%20Denoising%2C%20Amplification%2C%20Restoration%20Kit&entry.906535625=Zhuoheng%20Li%20and%20Yuheng%20Pan%20and%20Houcheng%20Yu%20and%20Zhiheng%20Zhang&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20lightweight%20computational%20framework%20for%0Aenhancing%20images%20under%20low-light%20conditions%2C%20utilizing%20advanced%20machine%0Alearning%20and%20convolutional%20neural%20networks%20%28CNNs%29.%20Traditional%20enhancement%0Atechniques%20often%20fail%20to%20adequately%20address%20issues%20like%20noise%2C%20color%0Adistortion%2C%20and%20detail%20loss%20in%20challenging%20lighting%20environments.%20Our%20approach%0Aleverages%20insights%20from%20the%20Retinex%20theory%20and%20recent%20advances%20in%20image%0Arestoration%20networks%20to%20develop%20a%20streamlined%20model%20that%20efficiently%20processes%0Aillumination%20components%20and%20integrates%20context-sensitive%20enhancements%20through%0Aoptimized%20convolutional%20blocks.%20This%20results%20in%20significantly%20improved%20image%0Aclarity%20and%20color%20fidelity%2C%20while%20avoiding%20over-enhancement%20and%20unnatural%20color%0Ashifts.%20Crucially%2C%20our%20model%20is%20designed%20to%20be%20lightweight%2C%20ensuring%20low%0Acomputational%20demand%20and%20suitability%20for%20real-time%20applications%20on%20standard%0Aconsumer%20hardware.%20Performance%20evaluations%20confirm%20that%20our%20model%20not%20only%0Asurpasses%20existing%20methods%20in%20enhancing%20low-light%20images%20but%20also%20maintains%20a%0Aminimal%20computational%20footprint.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12891v1&entry.124074799=Read"},
{"title": "An Empirical Study and Analysis of Text-to-Image Generation Using Large\n  Language Model-Powered Textual Representation", "author": "Zhiyu Tan and Mengping Yang and Luozheng Qin and Hao Yang and Ye Qian and Qiang Zhou and Cheng Zhang and Hao Li", "abstract": "  One critical prerequisite for faithful text-to-image generation is the\naccurate understanding of text inputs. Existing methods leverage the text\nencoder of the CLIP model to represent input prompts. However, the pre-trained\nCLIP model can merely encode English with a maximum token length of 77.\nMoreover, the model capacity of the text encoder from CLIP is relatively\nlimited compared to Large Language Models (LLMs), which offer multilingual\ninput, accommodate longer context, and achieve superior text representation. In\nthis paper, we investigate LLMs as the text encoder to improve the language\nunderstanding in text-to-image generation. Unfortunately, training\ntext-to-image generative model with LLMs from scratch demands significant\ncomputational resources and data. To this end, we introduce a three-stage\ntraining pipeline that effectively and efficiently integrates the existing\ntext-to-image model with LLMs. Specifically, we propose a lightweight adapter\nthat enables fast training of the text-to-image model using the textual\nrepresentations from LLMs. Extensive experiments demonstrate that our model\nsupports not only multilingual but also longer input context with superior\nimage generation quality.\n", "link": "http://arxiv.org/abs/2405.12914v1", "date": "2024-05-21", "relevancy": 2.2648, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5918}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5774}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Empirical%20Study%20and%20Analysis%20of%20Text-to-Image%20Generation%20Using%20Large%0A%20%20Language%20Model-Powered%20Textual%20Representation&body=Title%3A%20An%20Empirical%20Study%20and%20Analysis%20of%20Text-to-Image%20Generation%20Using%20Large%0A%20%20Language%20Model-Powered%20Textual%20Representation%0AAuthor%3A%20Zhiyu%20Tan%20and%20Mengping%20Yang%20and%20Luozheng%20Qin%20and%20Hao%20Yang%20and%20Ye%20Qian%20and%20Qiang%20Zhou%20and%20Cheng%20Zhang%20and%20Hao%20Li%0AAbstract%3A%20%20%20One%20critical%20prerequisite%20for%20faithful%20text-to-image%20generation%20is%20the%0Aaccurate%20understanding%20of%20text%20inputs.%20Existing%20methods%20leverage%20the%20text%0Aencoder%20of%20the%20CLIP%20model%20to%20represent%20input%20prompts.%20However%2C%20the%20pre-trained%0ACLIP%20model%20can%20merely%20encode%20English%20with%20a%20maximum%20token%20length%20of%2077.%0AMoreover%2C%20the%20model%20capacity%20of%20the%20text%20encoder%20from%20CLIP%20is%20relatively%0Alimited%20compared%20to%20Large%20Language%20Models%20%28LLMs%29%2C%20which%20offer%20multilingual%0Ainput%2C%20accommodate%20longer%20context%2C%20and%20achieve%20superior%20text%20representation.%20In%0Athis%20paper%2C%20we%20investigate%20LLMs%20as%20the%20text%20encoder%20to%20improve%20the%20language%0Aunderstanding%20in%20text-to-image%20generation.%20Unfortunately%2C%20training%0Atext-to-image%20generative%20model%20with%20LLMs%20from%20scratch%20demands%20significant%0Acomputational%20resources%20and%20data.%20To%20this%20end%2C%20we%20introduce%20a%20three-stage%0Atraining%20pipeline%20that%20effectively%20and%20efficiently%20integrates%20the%20existing%0Atext-to-image%20model%20with%20LLMs.%20Specifically%2C%20we%20propose%20a%20lightweight%20adapter%0Athat%20enables%20fast%20training%20of%20the%20text-to-image%20model%20using%20the%20textual%0Arepresentations%20from%20LLMs.%20Extensive%20experiments%20demonstrate%20that%20our%20model%0Asupports%20not%20only%20multilingual%20but%20also%20longer%20input%20context%20with%20superior%0Aimage%20generation%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12914v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Empirical%2520Study%2520and%2520Analysis%2520of%2520Text-to-Image%2520Generation%2520Using%2520Large%250A%2520%2520Language%2520Model-Powered%2520Textual%2520Representation%26entry.906535625%3DZhiyu%2520Tan%2520and%2520Mengping%2520Yang%2520and%2520Luozheng%2520Qin%2520and%2520Hao%2520Yang%2520and%2520Ye%2520Qian%2520and%2520Qiang%2520Zhou%2520and%2520Cheng%2520Zhang%2520and%2520Hao%2520Li%26entry.1292438233%3D%2520%2520One%2520critical%2520prerequisite%2520for%2520faithful%2520text-to-image%2520generation%2520is%2520the%250Aaccurate%2520understanding%2520of%2520text%2520inputs.%2520Existing%2520methods%2520leverage%2520the%2520text%250Aencoder%2520of%2520the%2520CLIP%2520model%2520to%2520represent%2520input%2520prompts.%2520However%252C%2520the%2520pre-trained%250ACLIP%2520model%2520can%2520merely%2520encode%2520English%2520with%2520a%2520maximum%2520token%2520length%2520of%252077.%250AMoreover%252C%2520the%2520model%2520capacity%2520of%2520the%2520text%2520encoder%2520from%2520CLIP%2520is%2520relatively%250Alimited%2520compared%2520to%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520which%2520offer%2520multilingual%250Ainput%252C%2520accommodate%2520longer%2520context%252C%2520and%2520achieve%2520superior%2520text%2520representation.%2520In%250Athis%2520paper%252C%2520we%2520investigate%2520LLMs%2520as%2520the%2520text%2520encoder%2520to%2520improve%2520the%2520language%250Aunderstanding%2520in%2520text-to-image%2520generation.%2520Unfortunately%252C%2520training%250Atext-to-image%2520generative%2520model%2520with%2520LLMs%2520from%2520scratch%2520demands%2520significant%250Acomputational%2520resources%2520and%2520data.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520three-stage%250Atraining%2520pipeline%2520that%2520effectively%2520and%2520efficiently%2520integrates%2520the%2520existing%250Atext-to-image%2520model%2520with%2520LLMs.%2520Specifically%252C%2520we%2520propose%2520a%2520lightweight%2520adapter%250Athat%2520enables%2520fast%2520training%2520of%2520the%2520text-to-image%2520model%2520using%2520the%2520textual%250Arepresentations%2520from%2520LLMs.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520model%250Asupports%2520not%2520only%2520multilingual%2520but%2520also%2520longer%2520input%2520context%2520with%2520superior%250Aimage%2520generation%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12914v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Empirical%20Study%20and%20Analysis%20of%20Text-to-Image%20Generation%20Using%20Large%0A%20%20Language%20Model-Powered%20Textual%20Representation&entry.906535625=Zhiyu%20Tan%20and%20Mengping%20Yang%20and%20Luozheng%20Qin%20and%20Hao%20Yang%20and%20Ye%20Qian%20and%20Qiang%20Zhou%20and%20Cheng%20Zhang%20and%20Hao%20Li&entry.1292438233=%20%20One%20critical%20prerequisite%20for%20faithful%20text-to-image%20generation%20is%20the%0Aaccurate%20understanding%20of%20text%20inputs.%20Existing%20methods%20leverage%20the%20text%0Aencoder%20of%20the%20CLIP%20model%20to%20represent%20input%20prompts.%20However%2C%20the%20pre-trained%0ACLIP%20model%20can%20merely%20encode%20English%20with%20a%20maximum%20token%20length%20of%2077.%0AMoreover%2C%20the%20model%20capacity%20of%20the%20text%20encoder%20from%20CLIP%20is%20relatively%0Alimited%20compared%20to%20Large%20Language%20Models%20%28LLMs%29%2C%20which%20offer%20multilingual%0Ainput%2C%20accommodate%20longer%20context%2C%20and%20achieve%20superior%20text%20representation.%20In%0Athis%20paper%2C%20we%20investigate%20LLMs%20as%20the%20text%20encoder%20to%20improve%20the%20language%0Aunderstanding%20in%20text-to-image%20generation.%20Unfortunately%2C%20training%0Atext-to-image%20generative%20model%20with%20LLMs%20from%20scratch%20demands%20significant%0Acomputational%20resources%20and%20data.%20To%20this%20end%2C%20we%20introduce%20a%20three-stage%0Atraining%20pipeline%20that%20effectively%20and%20efficiently%20integrates%20the%20existing%0Atext-to-image%20model%20with%20LLMs.%20Specifically%2C%20we%20propose%20a%20lightweight%20adapter%0Athat%20enables%20fast%20training%20of%20the%20text-to-image%20model%20using%20the%20textual%0Arepresentations%20from%20LLMs.%20Extensive%20experiments%20demonstrate%20that%20our%20model%0Asupports%20not%20only%20multilingual%20but%20also%20longer%20input%20context%20with%20superior%0Aimage%20generation%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12914v1&entry.124074799=Read"},
{"title": "Cross-spectral Gated-RGB Stereo Depth Estimation", "author": "Samuel Brucker and Stefanie Walz and Mario Bijelic and Felix Heide", "abstract": "  Gated cameras flood-illuminate a scene and capture the time-gated impulse\nresponse of a scene. By employing nanosecond-scale gates, existing sensors are\ncapable of capturing mega-pixel gated images, delivering dense depth improving\non today's LiDAR sensors in spatial resolution and depth precision. Although\ngated depth estimation methods deliver a million of depth estimates per frame,\ntheir resolution is still an order below existing RGB imaging methods. In this\nwork, we combine high-resolution stereo HDR RCCB cameras with gated imaging,\nallowing us to exploit depth cues from active gating, multi-view RGB and\nmulti-view NIR sensing -- multi-view and gated cues across the entire spectrum.\nThe resulting capture system consists only of low-cost CMOS sensors and\nflood-illumination. We propose a novel stereo-depth estimation method that is\ncapable of exploiting these multi-modal multi-view depth cues, including the\nactive illumination that is measured by the RCCB camera when removing the\nIR-cut filter. The proposed method achieves accurate depth at long ranges,\noutperforming the next best existing method by 39% for ranges of 100 to 220m in\nMAE on accumulated LiDAR ground-truth. Our code, models and datasets are\navailable at https://light.princeton.edu/gatedrccbstereo/ .\n", "link": "http://arxiv.org/abs/2405.12759v1", "date": "2024-05-21", "relevancy": 2.2478, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6349}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.514}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-spectral%20Gated-RGB%20Stereo%20Depth%20Estimation&body=Title%3A%20Cross-spectral%20Gated-RGB%20Stereo%20Depth%20Estimation%0AAuthor%3A%20Samuel%20Brucker%20and%20Stefanie%20Walz%20and%20Mario%20Bijelic%20and%20Felix%20Heide%0AAbstract%3A%20%20%20Gated%20cameras%20flood-illuminate%20a%20scene%20and%20capture%20the%20time-gated%20impulse%0Aresponse%20of%20a%20scene.%20By%20employing%20nanosecond-scale%20gates%2C%20existing%20sensors%20are%0Acapable%20of%20capturing%20mega-pixel%20gated%20images%2C%20delivering%20dense%20depth%20improving%0Aon%20today%27s%20LiDAR%20sensors%20in%20spatial%20resolution%20and%20depth%20precision.%20Although%0Agated%20depth%20estimation%20methods%20deliver%20a%20million%20of%20depth%20estimates%20per%20frame%2C%0Atheir%20resolution%20is%20still%20an%20order%20below%20existing%20RGB%20imaging%20methods.%20In%20this%0Awork%2C%20we%20combine%20high-resolution%20stereo%20HDR%20RCCB%20cameras%20with%20gated%20imaging%2C%0Aallowing%20us%20to%20exploit%20depth%20cues%20from%20active%20gating%2C%20multi-view%20RGB%20and%0Amulti-view%20NIR%20sensing%20--%20multi-view%20and%20gated%20cues%20across%20the%20entire%20spectrum.%0AThe%20resulting%20capture%20system%20consists%20only%20of%20low-cost%20CMOS%20sensors%20and%0Aflood-illumination.%20We%20propose%20a%20novel%20stereo-depth%20estimation%20method%20that%20is%0Acapable%20of%20exploiting%20these%20multi-modal%20multi-view%20depth%20cues%2C%20including%20the%0Aactive%20illumination%20that%20is%20measured%20by%20the%20RCCB%20camera%20when%20removing%20the%0AIR-cut%20filter.%20The%20proposed%20method%20achieves%20accurate%20depth%20at%20long%20ranges%2C%0Aoutperforming%20the%20next%20best%20existing%20method%20by%2039%25%20for%20ranges%20of%20100%20to%20220m%20in%0AMAE%20on%20accumulated%20LiDAR%20ground-truth.%20Our%20code%2C%20models%20and%20datasets%20are%0Aavailable%20at%20https%3A//light.princeton.edu/gatedrccbstereo/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-spectral%2520Gated-RGB%2520Stereo%2520Depth%2520Estimation%26entry.906535625%3DSamuel%2520Brucker%2520and%2520Stefanie%2520Walz%2520and%2520Mario%2520Bijelic%2520and%2520Felix%2520Heide%26entry.1292438233%3D%2520%2520Gated%2520cameras%2520flood-illuminate%2520a%2520scene%2520and%2520capture%2520the%2520time-gated%2520impulse%250Aresponse%2520of%2520a%2520scene.%2520By%2520employing%2520nanosecond-scale%2520gates%252C%2520existing%2520sensors%2520are%250Acapable%2520of%2520capturing%2520mega-pixel%2520gated%2520images%252C%2520delivering%2520dense%2520depth%2520improving%250Aon%2520today%2527s%2520LiDAR%2520sensors%2520in%2520spatial%2520resolution%2520and%2520depth%2520precision.%2520Although%250Agated%2520depth%2520estimation%2520methods%2520deliver%2520a%2520million%2520of%2520depth%2520estimates%2520per%2520frame%252C%250Atheir%2520resolution%2520is%2520still%2520an%2520order%2520below%2520existing%2520RGB%2520imaging%2520methods.%2520In%2520this%250Awork%252C%2520we%2520combine%2520high-resolution%2520stereo%2520HDR%2520RCCB%2520cameras%2520with%2520gated%2520imaging%252C%250Aallowing%2520us%2520to%2520exploit%2520depth%2520cues%2520from%2520active%2520gating%252C%2520multi-view%2520RGB%2520and%250Amulti-view%2520NIR%2520sensing%2520--%2520multi-view%2520and%2520gated%2520cues%2520across%2520the%2520entire%2520spectrum.%250AThe%2520resulting%2520capture%2520system%2520consists%2520only%2520of%2520low-cost%2520CMOS%2520sensors%2520and%250Aflood-illumination.%2520We%2520propose%2520a%2520novel%2520stereo-depth%2520estimation%2520method%2520that%2520is%250Acapable%2520of%2520exploiting%2520these%2520multi-modal%2520multi-view%2520depth%2520cues%252C%2520including%2520the%250Aactive%2520illumination%2520that%2520is%2520measured%2520by%2520the%2520RCCB%2520camera%2520when%2520removing%2520the%250AIR-cut%2520filter.%2520The%2520proposed%2520method%2520achieves%2520accurate%2520depth%2520at%2520long%2520ranges%252C%250Aoutperforming%2520the%2520next%2520best%2520existing%2520method%2520by%252039%2525%2520for%2520ranges%2520of%2520100%2520to%2520220m%2520in%250AMAE%2520on%2520accumulated%2520LiDAR%2520ground-truth.%2520Our%2520code%252C%2520models%2520and%2520datasets%2520are%250Aavailable%2520at%2520https%253A//light.princeton.edu/gatedrccbstereo/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-spectral%20Gated-RGB%20Stereo%20Depth%20Estimation&entry.906535625=Samuel%20Brucker%20and%20Stefanie%20Walz%20and%20Mario%20Bijelic%20and%20Felix%20Heide&entry.1292438233=%20%20Gated%20cameras%20flood-illuminate%20a%20scene%20and%20capture%20the%20time-gated%20impulse%0Aresponse%20of%20a%20scene.%20By%20employing%20nanosecond-scale%20gates%2C%20existing%20sensors%20are%0Acapable%20of%20capturing%20mega-pixel%20gated%20images%2C%20delivering%20dense%20depth%20improving%0Aon%20today%27s%20LiDAR%20sensors%20in%20spatial%20resolution%20and%20depth%20precision.%20Although%0Agated%20depth%20estimation%20methods%20deliver%20a%20million%20of%20depth%20estimates%20per%20frame%2C%0Atheir%20resolution%20is%20still%20an%20order%20below%20existing%20RGB%20imaging%20methods.%20In%20this%0Awork%2C%20we%20combine%20high-resolution%20stereo%20HDR%20RCCB%20cameras%20with%20gated%20imaging%2C%0Aallowing%20us%20to%20exploit%20depth%20cues%20from%20active%20gating%2C%20multi-view%20RGB%20and%0Amulti-view%20NIR%20sensing%20--%20multi-view%20and%20gated%20cues%20across%20the%20entire%20spectrum.%0AThe%20resulting%20capture%20system%20consists%20only%20of%20low-cost%20CMOS%20sensors%20and%0Aflood-illumination.%20We%20propose%20a%20novel%20stereo-depth%20estimation%20method%20that%20is%0Acapable%20of%20exploiting%20these%20multi-modal%20multi-view%20depth%20cues%2C%20including%20the%0Aactive%20illumination%20that%20is%20measured%20by%20the%20RCCB%20camera%20when%20removing%20the%0AIR-cut%20filter.%20The%20proposed%20method%20achieves%20accurate%20depth%20at%20long%20ranges%2C%0Aoutperforming%20the%20next%20best%20existing%20method%20by%2039%25%20for%20ranges%20of%20100%20to%20220m%20in%0AMAE%20on%20accumulated%20LiDAR%20ground-truth.%20Our%20code%2C%20models%20and%20datasets%20are%0Aavailable%20at%20https%3A//light.princeton.edu/gatedrccbstereo/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12759v1&entry.124074799=Read"},
{"title": "High-Resolution Cranial Defect Reconstruction by Iterative,\n  Low-Resolution, Point Cloud Completion Transformers", "author": "Marek Wodzinski and Mateusz Daniol and Daria Hemmerling and Miroslaw Socha", "abstract": "  Each year thousands of people suffer from various types of cranial injuries\nand require personalized implants whose manual design is expensive and\ntime-consuming. Therefore, an automatic, dedicated system to increase the\navailability of personalized cranial reconstruction is highly desirable. The\nproblem of the automatic cranial defect reconstruction can be formulated as the\nshape completion task and solved using dedicated deep networks. Currently, the\nmost common approach is to use the volumetric representation and apply deep\nnetworks dedicated to image segmentation. However, this approach has several\nlimitations and does not scale well into high-resolution volumes, nor takes\ninto account the data sparsity. In our work, we reformulate the problem into a\npoint cloud completion task. We propose an iterative, transformer-based method\nto reconstruct the cranial defect at any resolution while also being fast and\nresource-efficient during training and inference. We compare the proposed\nmethods to the state-of-the-art volumetric approaches and show superior\nperformance in terms of GPU memory consumption while maintaining high-quality\nof the reconstructed defects.\n", "link": "http://arxiv.org/abs/2308.03813v2", "date": "2024-05-21", "relevancy": 2.2204, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5704}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5531}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Resolution%20Cranial%20Defect%20Reconstruction%20by%20Iterative%2C%0A%20%20Low-Resolution%2C%20Point%20Cloud%20Completion%20Transformers&body=Title%3A%20High-Resolution%20Cranial%20Defect%20Reconstruction%20by%20Iterative%2C%0A%20%20Low-Resolution%2C%20Point%20Cloud%20Completion%20Transformers%0AAuthor%3A%20Marek%20Wodzinski%20and%20Mateusz%20Daniol%20and%20Daria%20Hemmerling%20and%20Miroslaw%20Socha%0AAbstract%3A%20%20%20Each%20year%20thousands%20of%20people%20suffer%20from%20various%20types%20of%20cranial%20injuries%0Aand%20require%20personalized%20implants%20whose%20manual%20design%20is%20expensive%20and%0Atime-consuming.%20Therefore%2C%20an%20automatic%2C%20dedicated%20system%20to%20increase%20the%0Aavailability%20of%20personalized%20cranial%20reconstruction%20is%20highly%20desirable.%20The%0Aproblem%20of%20the%20automatic%20cranial%20defect%20reconstruction%20can%20be%20formulated%20as%20the%0Ashape%20completion%20task%20and%20solved%20using%20dedicated%20deep%20networks.%20Currently%2C%20the%0Amost%20common%20approach%20is%20to%20use%20the%20volumetric%20representation%20and%20apply%20deep%0Anetworks%20dedicated%20to%20image%20segmentation.%20However%2C%20this%20approach%20has%20several%0Alimitations%20and%20does%20not%20scale%20well%20into%20high-resolution%20volumes%2C%20nor%20takes%0Ainto%20account%20the%20data%20sparsity.%20In%20our%20work%2C%20we%20reformulate%20the%20problem%20into%20a%0Apoint%20cloud%20completion%20task.%20We%20propose%20an%20iterative%2C%20transformer-based%20method%0Ato%20reconstruct%20the%20cranial%20defect%20at%20any%20resolution%20while%20also%20being%20fast%20and%0Aresource-efficient%20during%20training%20and%20inference.%20We%20compare%20the%20proposed%0Amethods%20to%20the%20state-of-the-art%20volumetric%20approaches%20and%20show%20superior%0Aperformance%20in%20terms%20of%20GPU%20memory%20consumption%20while%20maintaining%20high-quality%0Aof%20the%20reconstructed%20defects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.03813v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Resolution%2520Cranial%2520Defect%2520Reconstruction%2520by%2520Iterative%252C%250A%2520%2520Low-Resolution%252C%2520Point%2520Cloud%2520Completion%2520Transformers%26entry.906535625%3DMarek%2520Wodzinski%2520and%2520Mateusz%2520Daniol%2520and%2520Daria%2520Hemmerling%2520and%2520Miroslaw%2520Socha%26entry.1292438233%3D%2520%2520Each%2520year%2520thousands%2520of%2520people%2520suffer%2520from%2520various%2520types%2520of%2520cranial%2520injuries%250Aand%2520require%2520personalized%2520implants%2520whose%2520manual%2520design%2520is%2520expensive%2520and%250Atime-consuming.%2520Therefore%252C%2520an%2520automatic%252C%2520dedicated%2520system%2520to%2520increase%2520the%250Aavailability%2520of%2520personalized%2520cranial%2520reconstruction%2520is%2520highly%2520desirable.%2520The%250Aproblem%2520of%2520the%2520automatic%2520cranial%2520defect%2520reconstruction%2520can%2520be%2520formulated%2520as%2520the%250Ashape%2520completion%2520task%2520and%2520solved%2520using%2520dedicated%2520deep%2520networks.%2520Currently%252C%2520the%250Amost%2520common%2520approach%2520is%2520to%2520use%2520the%2520volumetric%2520representation%2520and%2520apply%2520deep%250Anetworks%2520dedicated%2520to%2520image%2520segmentation.%2520However%252C%2520this%2520approach%2520has%2520several%250Alimitations%2520and%2520does%2520not%2520scale%2520well%2520into%2520high-resolution%2520volumes%252C%2520nor%2520takes%250Ainto%2520account%2520the%2520data%2520sparsity.%2520In%2520our%2520work%252C%2520we%2520reformulate%2520the%2520problem%2520into%2520a%250Apoint%2520cloud%2520completion%2520task.%2520We%2520propose%2520an%2520iterative%252C%2520transformer-based%2520method%250Ato%2520reconstruct%2520the%2520cranial%2520defect%2520at%2520any%2520resolution%2520while%2520also%2520being%2520fast%2520and%250Aresource-efficient%2520during%2520training%2520and%2520inference.%2520We%2520compare%2520the%2520proposed%250Amethods%2520to%2520the%2520state-of-the-art%2520volumetric%2520approaches%2520and%2520show%2520superior%250Aperformance%2520in%2520terms%2520of%2520GPU%2520memory%2520consumption%2520while%2520maintaining%2520high-quality%250Aof%2520the%2520reconstructed%2520defects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.03813v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Resolution%20Cranial%20Defect%20Reconstruction%20by%20Iterative%2C%0A%20%20Low-Resolution%2C%20Point%20Cloud%20Completion%20Transformers&entry.906535625=Marek%20Wodzinski%20and%20Mateusz%20Daniol%20and%20Daria%20Hemmerling%20and%20Miroslaw%20Socha&entry.1292438233=%20%20Each%20year%20thousands%20of%20people%20suffer%20from%20various%20types%20of%20cranial%20injuries%0Aand%20require%20personalized%20implants%20whose%20manual%20design%20is%20expensive%20and%0Atime-consuming.%20Therefore%2C%20an%20automatic%2C%20dedicated%20system%20to%20increase%20the%0Aavailability%20of%20personalized%20cranial%20reconstruction%20is%20highly%20desirable.%20The%0Aproblem%20of%20the%20automatic%20cranial%20defect%20reconstruction%20can%20be%20formulated%20as%20the%0Ashape%20completion%20task%20and%20solved%20using%20dedicated%20deep%20networks.%20Currently%2C%20the%0Amost%20common%20approach%20is%20to%20use%20the%20volumetric%20representation%20and%20apply%20deep%0Anetworks%20dedicated%20to%20image%20segmentation.%20However%2C%20this%20approach%20has%20several%0Alimitations%20and%20does%20not%20scale%20well%20into%20high-resolution%20volumes%2C%20nor%20takes%0Ainto%20account%20the%20data%20sparsity.%20In%20our%20work%2C%20we%20reformulate%20the%20problem%20into%20a%0Apoint%20cloud%20completion%20task.%20We%20propose%20an%20iterative%2C%20transformer-based%20method%0Ato%20reconstruct%20the%20cranial%20defect%20at%20any%20resolution%20while%20also%20being%20fast%20and%0Aresource-efficient%20during%20training%20and%20inference.%20We%20compare%20the%20proposed%0Amethods%20to%20the%20state-of-the-art%20volumetric%20approaches%20and%20show%20superior%0Aperformance%20in%20terms%20of%20GPU%20memory%20consumption%20while%20maintaining%20high-quality%0Aof%20the%20reconstructed%20defects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.03813v2&entry.124074799=Read"},
{"title": "A Masked Semi-Supervised Learning Approach for Otago Micro Labels\n  Recognition", "author": "Meng Shang and Lenore Dedeyne and Jolan Dupont and Laura Vercauteren and Nadjia Amini and Laurence Lapauw and Evelien Gielen and Sabine Verschueren and Carolina Varon and Walter De Raedt and Bart Vanrumste", "abstract": "  The Otago Exercise Program (OEP) serves as a vital rehabilitation initiative\nfor older adults, aiming to enhance their strength and balance, and\nconsequently prevent falls. While Human Activity Recognition (HAR) systems have\nbeen widely employed in recognizing the activities of individuals, existing\nsystems focus on the duration of macro activities (i.e. a sequence of\nrepetitions of the same exercise), neglecting the ability to discern micro\nactivities (i.e. the individual repetitions of the exercises), in the case of\nOEP. This study presents a novel semi-supervised machine learning approach\naimed at bridging this gap in recognizing the micro activities of OEP. To\nmanage the limited dataset size, our model utilizes a Transformer encoder for\nfeature extraction, subsequently classified by a Temporal Convolutional Network\n(TCN). Simultaneously, the Transformer encoder is employed for masked\nunsupervised learning to reconstruct input signals. Results indicate that the\nmasked unsupervised learning task enhances the performance of the supervised\nlearning (classification task), as evidenced by f1-scores surpassing the\nclinically applicable threshold of 0.8. From the micro activities, two\nclinically relevant outcomes emerge: counting the number of repetitions of each\nexercise and calculating the velocity during chair rising. These outcomes\nenable the automatic monitoring of exercise intensity and difficulty in the\ndaily lives of older adults.\n", "link": "http://arxiv.org/abs/2405.12711v1", "date": "2024-05-21", "relevancy": 2.2161, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5728}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5446}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Masked%20Semi-Supervised%20Learning%20Approach%20for%20Otago%20Micro%20Labels%0A%20%20Recognition&body=Title%3A%20A%20Masked%20Semi-Supervised%20Learning%20Approach%20for%20Otago%20Micro%20Labels%0A%20%20Recognition%0AAuthor%3A%20Meng%20Shang%20and%20Lenore%20Dedeyne%20and%20Jolan%20Dupont%20and%20Laura%20Vercauteren%20and%20Nadjia%20Amini%20and%20Laurence%20Lapauw%20and%20Evelien%20Gielen%20and%20Sabine%20Verschueren%20and%20Carolina%20Varon%20and%20Walter%20De%20Raedt%20and%20Bart%20Vanrumste%0AAbstract%3A%20%20%20The%20Otago%20Exercise%20Program%20%28OEP%29%20serves%20as%20a%20vital%20rehabilitation%20initiative%0Afor%20older%20adults%2C%20aiming%20to%20enhance%20their%20strength%20and%20balance%2C%20and%0Aconsequently%20prevent%20falls.%20While%20Human%20Activity%20Recognition%20%28HAR%29%20systems%20have%0Abeen%20widely%20employed%20in%20recognizing%20the%20activities%20of%20individuals%2C%20existing%0Asystems%20focus%20on%20the%20duration%20of%20macro%20activities%20%28i.e.%20a%20sequence%20of%0Arepetitions%20of%20the%20same%20exercise%29%2C%20neglecting%20the%20ability%20to%20discern%20micro%0Aactivities%20%28i.e.%20the%20individual%20repetitions%20of%20the%20exercises%29%2C%20in%20the%20case%20of%0AOEP.%20This%20study%20presents%20a%20novel%20semi-supervised%20machine%20learning%20approach%0Aaimed%20at%20bridging%20this%20gap%20in%20recognizing%20the%20micro%20activities%20of%20OEP.%20To%0Amanage%20the%20limited%20dataset%20size%2C%20our%20model%20utilizes%20a%20Transformer%20encoder%20for%0Afeature%20extraction%2C%20subsequently%20classified%20by%20a%20Temporal%20Convolutional%20Network%0A%28TCN%29.%20Simultaneously%2C%20the%20Transformer%20encoder%20is%20employed%20for%20masked%0Aunsupervised%20learning%20to%20reconstruct%20input%20signals.%20Results%20indicate%20that%20the%0Amasked%20unsupervised%20learning%20task%20enhances%20the%20performance%20of%20the%20supervised%0Alearning%20%28classification%20task%29%2C%20as%20evidenced%20by%20f1-scores%20surpassing%20the%0Aclinically%20applicable%20threshold%20of%200.8.%20From%20the%20micro%20activities%2C%20two%0Aclinically%20relevant%20outcomes%20emerge%3A%20counting%20the%20number%20of%20repetitions%20of%20each%0Aexercise%20and%20calculating%20the%20velocity%20during%20chair%20rising.%20These%20outcomes%0Aenable%20the%20automatic%20monitoring%20of%20exercise%20intensity%20and%20difficulty%20in%20the%0Adaily%20lives%20of%20older%20adults.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Masked%2520Semi-Supervised%2520Learning%2520Approach%2520for%2520Otago%2520Micro%2520Labels%250A%2520%2520Recognition%26entry.906535625%3DMeng%2520Shang%2520and%2520Lenore%2520Dedeyne%2520and%2520Jolan%2520Dupont%2520and%2520Laura%2520Vercauteren%2520and%2520Nadjia%2520Amini%2520and%2520Laurence%2520Lapauw%2520and%2520Evelien%2520Gielen%2520and%2520Sabine%2520Verschueren%2520and%2520Carolina%2520Varon%2520and%2520Walter%2520De%2520Raedt%2520and%2520Bart%2520Vanrumste%26entry.1292438233%3D%2520%2520The%2520Otago%2520Exercise%2520Program%2520%2528OEP%2529%2520serves%2520as%2520a%2520vital%2520rehabilitation%2520initiative%250Afor%2520older%2520adults%252C%2520aiming%2520to%2520enhance%2520their%2520strength%2520and%2520balance%252C%2520and%250Aconsequently%2520prevent%2520falls.%2520While%2520Human%2520Activity%2520Recognition%2520%2528HAR%2529%2520systems%2520have%250Abeen%2520widely%2520employed%2520in%2520recognizing%2520the%2520activities%2520of%2520individuals%252C%2520existing%250Asystems%2520focus%2520on%2520the%2520duration%2520of%2520macro%2520activities%2520%2528i.e.%2520a%2520sequence%2520of%250Arepetitions%2520of%2520the%2520same%2520exercise%2529%252C%2520neglecting%2520the%2520ability%2520to%2520discern%2520micro%250Aactivities%2520%2528i.e.%2520the%2520individual%2520repetitions%2520of%2520the%2520exercises%2529%252C%2520in%2520the%2520case%2520of%250AOEP.%2520This%2520study%2520presents%2520a%2520novel%2520semi-supervised%2520machine%2520learning%2520approach%250Aaimed%2520at%2520bridging%2520this%2520gap%2520in%2520recognizing%2520the%2520micro%2520activities%2520of%2520OEP.%2520To%250Amanage%2520the%2520limited%2520dataset%2520size%252C%2520our%2520model%2520utilizes%2520a%2520Transformer%2520encoder%2520for%250Afeature%2520extraction%252C%2520subsequently%2520classified%2520by%2520a%2520Temporal%2520Convolutional%2520Network%250A%2528TCN%2529.%2520Simultaneously%252C%2520the%2520Transformer%2520encoder%2520is%2520employed%2520for%2520masked%250Aunsupervised%2520learning%2520to%2520reconstruct%2520input%2520signals.%2520Results%2520indicate%2520that%2520the%250Amasked%2520unsupervised%2520learning%2520task%2520enhances%2520the%2520performance%2520of%2520the%2520supervised%250Alearning%2520%2528classification%2520task%2529%252C%2520as%2520evidenced%2520by%2520f1-scores%2520surpassing%2520the%250Aclinically%2520applicable%2520threshold%2520of%25200.8.%2520From%2520the%2520micro%2520activities%252C%2520two%250Aclinically%2520relevant%2520outcomes%2520emerge%253A%2520counting%2520the%2520number%2520of%2520repetitions%2520of%2520each%250Aexercise%2520and%2520calculating%2520the%2520velocity%2520during%2520chair%2520rising.%2520These%2520outcomes%250Aenable%2520the%2520automatic%2520monitoring%2520of%2520exercise%2520intensity%2520and%2520difficulty%2520in%2520the%250Adaily%2520lives%2520of%2520older%2520adults.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Masked%20Semi-Supervised%20Learning%20Approach%20for%20Otago%20Micro%20Labels%0A%20%20Recognition&entry.906535625=Meng%20Shang%20and%20Lenore%20Dedeyne%20and%20Jolan%20Dupont%20and%20Laura%20Vercauteren%20and%20Nadjia%20Amini%20and%20Laurence%20Lapauw%20and%20Evelien%20Gielen%20and%20Sabine%20Verschueren%20and%20Carolina%20Varon%20and%20Walter%20De%20Raedt%20and%20Bart%20Vanrumste&entry.1292438233=%20%20The%20Otago%20Exercise%20Program%20%28OEP%29%20serves%20as%20a%20vital%20rehabilitation%20initiative%0Afor%20older%20adults%2C%20aiming%20to%20enhance%20their%20strength%20and%20balance%2C%20and%0Aconsequently%20prevent%20falls.%20While%20Human%20Activity%20Recognition%20%28HAR%29%20systems%20have%0Abeen%20widely%20employed%20in%20recognizing%20the%20activities%20of%20individuals%2C%20existing%0Asystems%20focus%20on%20the%20duration%20of%20macro%20activities%20%28i.e.%20a%20sequence%20of%0Arepetitions%20of%20the%20same%20exercise%29%2C%20neglecting%20the%20ability%20to%20discern%20micro%0Aactivities%20%28i.e.%20the%20individual%20repetitions%20of%20the%20exercises%29%2C%20in%20the%20case%20of%0AOEP.%20This%20study%20presents%20a%20novel%20semi-supervised%20machine%20learning%20approach%0Aaimed%20at%20bridging%20this%20gap%20in%20recognizing%20the%20micro%20activities%20of%20OEP.%20To%0Amanage%20the%20limited%20dataset%20size%2C%20our%20model%20utilizes%20a%20Transformer%20encoder%20for%0Afeature%20extraction%2C%20subsequently%20classified%20by%20a%20Temporal%20Convolutional%20Network%0A%28TCN%29.%20Simultaneously%2C%20the%20Transformer%20encoder%20is%20employed%20for%20masked%0Aunsupervised%20learning%20to%20reconstruct%20input%20signals.%20Results%20indicate%20that%20the%0Amasked%20unsupervised%20learning%20task%20enhances%20the%20performance%20of%20the%20supervised%0Alearning%20%28classification%20task%29%2C%20as%20evidenced%20by%20f1-scores%20surpassing%20the%0Aclinically%20applicable%20threshold%20of%200.8.%20From%20the%20micro%20activities%2C%20two%0Aclinically%20relevant%20outcomes%20emerge%3A%20counting%20the%20number%20of%20repetitions%20of%20each%0Aexercise%20and%20calculating%20the%20velocity%20during%20chair%20rising.%20These%20outcomes%0Aenable%20the%20automatic%20monitoring%20of%20exercise%20intensity%20and%20difficulty%20in%20the%0Adaily%20lives%20of%20older%20adults.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12711v1&entry.124074799=Read"},
{"title": "WeightedPose: Generalizable Cross-Pose Estimation via Weighted SVD", "author": "Xuxin Cheng and Heng Yu and Harry Zhang and Wenxing Deng", "abstract": "  We introduce a new approach for robotic manipulation tasks in human settings\nthat necessitates understanding the 3D geometric connections between a pair of\nobjects. Conventional end-to-end training approaches, which convert pixel\nobservations directly into robot actions, often fail to effectively understand\ncomplex pose relationships and do not easily adapt to new object\nconfigurations. To overcome these issues, our method focuses on learning the 3D\ngeometric relationships, particularly how critical parts of one object relate\nto those of another. We employ Weighted SVD in our standalone model to analyze\npose relationships both in articulated parts and in free-floating objects. For\ninstance, our model can comprehend the spatial relationship between an oven\ndoor and the oven body, as well as between a lasagna plate and the oven. By\nconcentrating on the 3D geometric connections, our strategy empowers robots to\ncarry out intricate manipulation tasks based on object-centric perspectives\n", "link": "http://arxiv.org/abs/2405.02241v2", "date": "2024-05-21", "relevancy": 2.2112, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5758}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5501}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WeightedPose%3A%20Generalizable%20Cross-Pose%20Estimation%20via%20Weighted%20SVD&body=Title%3A%20WeightedPose%3A%20Generalizable%20Cross-Pose%20Estimation%20via%20Weighted%20SVD%0AAuthor%3A%20Xuxin%20Cheng%20and%20Heng%20Yu%20and%20Harry%20Zhang%20and%20Wenxing%20Deng%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20approach%20for%20robotic%20manipulation%20tasks%20in%20human%20settings%0Athat%20necessitates%20understanding%20the%203D%20geometric%20connections%20between%20a%20pair%20of%0Aobjects.%20Conventional%20end-to-end%20training%20approaches%2C%20which%20convert%20pixel%0Aobservations%20directly%20into%20robot%20actions%2C%20often%20fail%20to%20effectively%20understand%0Acomplex%20pose%20relationships%20and%20do%20not%20easily%20adapt%20to%20new%20object%0Aconfigurations.%20To%20overcome%20these%20issues%2C%20our%20method%20focuses%20on%20learning%20the%203D%0Ageometric%20relationships%2C%20particularly%20how%20critical%20parts%20of%20one%20object%20relate%0Ato%20those%20of%20another.%20We%20employ%20Weighted%20SVD%20in%20our%20standalone%20model%20to%20analyze%0Apose%20relationships%20both%20in%20articulated%20parts%20and%20in%20free-floating%20objects.%20For%0Ainstance%2C%20our%20model%20can%20comprehend%20the%20spatial%20relationship%20between%20an%20oven%0Adoor%20and%20the%20oven%20body%2C%20as%20well%20as%20between%20a%20lasagna%20plate%20and%20the%20oven.%20By%0Aconcentrating%20on%20the%203D%20geometric%20connections%2C%20our%20strategy%20empowers%20robots%20to%0Acarry%20out%20intricate%20manipulation%20tasks%20based%20on%20object-centric%20perspectives%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02241v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeightedPose%253A%2520Generalizable%2520Cross-Pose%2520Estimation%2520via%2520Weighted%2520SVD%26entry.906535625%3DXuxin%2520Cheng%2520and%2520Heng%2520Yu%2520and%2520Harry%2520Zhang%2520and%2520Wenxing%2520Deng%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520approach%2520for%2520robotic%2520manipulation%2520tasks%2520in%2520human%2520settings%250Athat%2520necessitates%2520understanding%2520the%25203D%2520geometric%2520connections%2520between%2520a%2520pair%2520of%250Aobjects.%2520Conventional%2520end-to-end%2520training%2520approaches%252C%2520which%2520convert%2520pixel%250Aobservations%2520directly%2520into%2520robot%2520actions%252C%2520often%2520fail%2520to%2520effectively%2520understand%250Acomplex%2520pose%2520relationships%2520and%2520do%2520not%2520easily%2520adapt%2520to%2520new%2520object%250Aconfigurations.%2520To%2520overcome%2520these%2520issues%252C%2520our%2520method%2520focuses%2520on%2520learning%2520the%25203D%250Ageometric%2520relationships%252C%2520particularly%2520how%2520critical%2520parts%2520of%2520one%2520object%2520relate%250Ato%2520those%2520of%2520another.%2520We%2520employ%2520Weighted%2520SVD%2520in%2520our%2520standalone%2520model%2520to%2520analyze%250Apose%2520relationships%2520both%2520in%2520articulated%2520parts%2520and%2520in%2520free-floating%2520objects.%2520For%250Ainstance%252C%2520our%2520model%2520can%2520comprehend%2520the%2520spatial%2520relationship%2520between%2520an%2520oven%250Adoor%2520and%2520the%2520oven%2520body%252C%2520as%2520well%2520as%2520between%2520a%2520lasagna%2520plate%2520and%2520the%2520oven.%2520By%250Aconcentrating%2520on%2520the%25203D%2520geometric%2520connections%252C%2520our%2520strategy%2520empowers%2520robots%2520to%250Acarry%2520out%2520intricate%2520manipulation%2520tasks%2520based%2520on%2520object-centric%2520perspectives%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02241v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WeightedPose%3A%20Generalizable%20Cross-Pose%20Estimation%20via%20Weighted%20SVD&entry.906535625=Xuxin%20Cheng%20and%20Heng%20Yu%20and%20Harry%20Zhang%20and%20Wenxing%20Deng&entry.1292438233=%20%20We%20introduce%20a%20new%20approach%20for%20robotic%20manipulation%20tasks%20in%20human%20settings%0Athat%20necessitates%20understanding%20the%203D%20geometric%20connections%20between%20a%20pair%20of%0Aobjects.%20Conventional%20end-to-end%20training%20approaches%2C%20which%20convert%20pixel%0Aobservations%20directly%20into%20robot%20actions%2C%20often%20fail%20to%20effectively%20understand%0Acomplex%20pose%20relationships%20and%20do%20not%20easily%20adapt%20to%20new%20object%0Aconfigurations.%20To%20overcome%20these%20issues%2C%20our%20method%20focuses%20on%20learning%20the%203D%0Ageometric%20relationships%2C%20particularly%20how%20critical%20parts%20of%20one%20object%20relate%0Ato%20those%20of%20another.%20We%20employ%20Weighted%20SVD%20in%20our%20standalone%20model%20to%20analyze%0Apose%20relationships%20both%20in%20articulated%20parts%20and%20in%20free-floating%20objects.%20For%0Ainstance%2C%20our%20model%20can%20comprehend%20the%20spatial%20relationship%20between%20an%20oven%0Adoor%20and%20the%20oven%20body%2C%20as%20well%20as%20between%20a%20lasagna%20plate%20and%20the%20oven.%20By%0Aconcentrating%20on%20the%203D%20geometric%20connections%2C%20our%20strategy%20empowers%20robots%20to%0Acarry%20out%20intricate%20manipulation%20tasks%20based%20on%20object-centric%20perspectives%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02241v2&entry.124074799=Read"},
{"title": "Self-Supervised Modality-Agnostic Pre-Training of Swin Transformers", "author": "Abhiroop Talasila and Maitreya Maity and U. Deva Priyakumar", "abstract": "  Unsupervised pre-training has emerged as a transformative paradigm,\ndisplaying remarkable advancements in various domains. However, the\nsusceptibility to domain shift, where pre-training data distribution differs\nfrom fine-tuning, poses a significant obstacle. To address this, we augment the\nSwin Transformer to learn from different medical imaging modalities, enhancing\ndownstream performance. Our model, dubbed SwinFUSE (Swin Multi-Modal Fusion for\nUnSupervised Enhancement), offers three key advantages: (i) it learns from both\nComputed Tomography (CT) and Magnetic Resonance Images (MRI) during\npre-training, resulting in complementary feature representations; (ii) a\ndomain-invariance module (DIM) that effectively highlights salient input\nregions, enhancing adaptability; (iii) exhibits remarkable generalizability,\nsurpassing the confines of tasks it was initially pre-trained on. Our\nexperiments on two publicly available 3D segmentation datasets show a modest\n1-2% performance trade-off compared to single-modality models, yet significant\nout-performance of up to 27% on out-of-distribution modality. This substantial\nimprovement underscores our proposed approach's practical relevance and\nreal-world applicability. Code is available at:\nhttps://github.com/devalab/SwinFUSE\n", "link": "http://arxiv.org/abs/2405.12781v1", "date": "2024-05-21", "relevancy": 2.2069, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6004}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5245}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Modality-Agnostic%20Pre-Training%20of%20Swin%20Transformers&body=Title%3A%20Self-Supervised%20Modality-Agnostic%20Pre-Training%20of%20Swin%20Transformers%0AAuthor%3A%20Abhiroop%20Talasila%20and%20Maitreya%20Maity%20and%20U.%20Deva%20Priyakumar%0AAbstract%3A%20%20%20Unsupervised%20pre-training%20has%20emerged%20as%20a%20transformative%20paradigm%2C%0Adisplaying%20remarkable%20advancements%20in%20various%20domains.%20However%2C%20the%0Asusceptibility%20to%20domain%20shift%2C%20where%20pre-training%20data%20distribution%20differs%0Afrom%20fine-tuning%2C%20poses%20a%20significant%20obstacle.%20To%20address%20this%2C%20we%20augment%20the%0ASwin%20Transformer%20to%20learn%20from%20different%20medical%20imaging%20modalities%2C%20enhancing%0Adownstream%20performance.%20Our%20model%2C%20dubbed%20SwinFUSE%20%28Swin%20Multi-Modal%20Fusion%20for%0AUnSupervised%20Enhancement%29%2C%20offers%20three%20key%20advantages%3A%20%28i%29%20it%20learns%20from%20both%0AComputed%20Tomography%20%28CT%29%20and%20Magnetic%20Resonance%20Images%20%28MRI%29%20during%0Apre-training%2C%20resulting%20in%20complementary%20feature%20representations%3B%20%28ii%29%20a%0Adomain-invariance%20module%20%28DIM%29%20that%20effectively%20highlights%20salient%20input%0Aregions%2C%20enhancing%20adaptability%3B%20%28iii%29%20exhibits%20remarkable%20generalizability%2C%0Asurpassing%20the%20confines%20of%20tasks%20it%20was%20initially%20pre-trained%20on.%20Our%0Aexperiments%20on%20two%20publicly%20available%203D%20segmentation%20datasets%20show%20a%20modest%0A1-2%25%20performance%20trade-off%20compared%20to%20single-modality%20models%2C%20yet%20significant%0Aout-performance%20of%20up%20to%2027%25%20on%20out-of-distribution%20modality.%20This%20substantial%0Aimprovement%20underscores%20our%20proposed%20approach%27s%20practical%20relevance%20and%0Areal-world%20applicability.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/devalab/SwinFUSE%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Modality-Agnostic%2520Pre-Training%2520of%2520Swin%2520Transformers%26entry.906535625%3DAbhiroop%2520Talasila%2520and%2520Maitreya%2520Maity%2520and%2520U.%2520Deva%2520Priyakumar%26entry.1292438233%3D%2520%2520Unsupervised%2520pre-training%2520has%2520emerged%2520as%2520a%2520transformative%2520paradigm%252C%250Adisplaying%2520remarkable%2520advancements%2520in%2520various%2520domains.%2520However%252C%2520the%250Asusceptibility%2520to%2520domain%2520shift%252C%2520where%2520pre-training%2520data%2520distribution%2520differs%250Afrom%2520fine-tuning%252C%2520poses%2520a%2520significant%2520obstacle.%2520To%2520address%2520this%252C%2520we%2520augment%2520the%250ASwin%2520Transformer%2520to%2520learn%2520from%2520different%2520medical%2520imaging%2520modalities%252C%2520enhancing%250Adownstream%2520performance.%2520Our%2520model%252C%2520dubbed%2520SwinFUSE%2520%2528Swin%2520Multi-Modal%2520Fusion%2520for%250AUnSupervised%2520Enhancement%2529%252C%2520offers%2520three%2520key%2520advantages%253A%2520%2528i%2529%2520it%2520learns%2520from%2520both%250AComputed%2520Tomography%2520%2528CT%2529%2520and%2520Magnetic%2520Resonance%2520Images%2520%2528MRI%2529%2520during%250Apre-training%252C%2520resulting%2520in%2520complementary%2520feature%2520representations%253B%2520%2528ii%2529%2520a%250Adomain-invariance%2520module%2520%2528DIM%2529%2520that%2520effectively%2520highlights%2520salient%2520input%250Aregions%252C%2520enhancing%2520adaptability%253B%2520%2528iii%2529%2520exhibits%2520remarkable%2520generalizability%252C%250Asurpassing%2520the%2520confines%2520of%2520tasks%2520it%2520was%2520initially%2520pre-trained%2520on.%2520Our%250Aexperiments%2520on%2520two%2520publicly%2520available%25203D%2520segmentation%2520datasets%2520show%2520a%2520modest%250A1-2%2525%2520performance%2520trade-off%2520compared%2520to%2520single-modality%2520models%252C%2520yet%2520significant%250Aout-performance%2520of%2520up%2520to%252027%2525%2520on%2520out-of-distribution%2520modality.%2520This%2520substantial%250Aimprovement%2520underscores%2520our%2520proposed%2520approach%2527s%2520practical%2520relevance%2520and%250Areal-world%2520applicability.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/devalab/SwinFUSE%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Modality-Agnostic%20Pre-Training%20of%20Swin%20Transformers&entry.906535625=Abhiroop%20Talasila%20and%20Maitreya%20Maity%20and%20U.%20Deva%20Priyakumar&entry.1292438233=%20%20Unsupervised%20pre-training%20has%20emerged%20as%20a%20transformative%20paradigm%2C%0Adisplaying%20remarkable%20advancements%20in%20various%20domains.%20However%2C%20the%0Asusceptibility%20to%20domain%20shift%2C%20where%20pre-training%20data%20distribution%20differs%0Afrom%20fine-tuning%2C%20poses%20a%20significant%20obstacle.%20To%20address%20this%2C%20we%20augment%20the%0ASwin%20Transformer%20to%20learn%20from%20different%20medical%20imaging%20modalities%2C%20enhancing%0Adownstream%20performance.%20Our%20model%2C%20dubbed%20SwinFUSE%20%28Swin%20Multi-Modal%20Fusion%20for%0AUnSupervised%20Enhancement%29%2C%20offers%20three%20key%20advantages%3A%20%28i%29%20it%20learns%20from%20both%0AComputed%20Tomography%20%28CT%29%20and%20Magnetic%20Resonance%20Images%20%28MRI%29%20during%0Apre-training%2C%20resulting%20in%20complementary%20feature%20representations%3B%20%28ii%29%20a%0Adomain-invariance%20module%20%28DIM%29%20that%20effectively%20highlights%20salient%20input%0Aregions%2C%20enhancing%20adaptability%3B%20%28iii%29%20exhibits%20remarkable%20generalizability%2C%0Asurpassing%20the%20confines%20of%20tasks%20it%20was%20initially%20pre-trained%20on.%20Our%0Aexperiments%20on%20two%20publicly%20available%203D%20segmentation%20datasets%20show%20a%20modest%0A1-2%25%20performance%20trade-off%20compared%20to%20single-modality%20models%2C%20yet%20significant%0Aout-performance%20of%20up%20to%2027%25%20on%20out-of-distribution%20modality.%20This%20substantial%0Aimprovement%20underscores%20our%20proposed%20approach%27s%20practical%20relevance%20and%0Areal-world%20applicability.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/devalab/SwinFUSE%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12781v1&entry.124074799=Read"},
{"title": "Talk2Radar: Bridging Natural Language with 4D mmWave Radar for 3D\n  Referring Expression Comprehension", "author": "Runwei Guan and Ruixiao Zhang and Ningwei Ouyang and Jianan Liu and Ka Lok Man and Xiaohao Cai and Ming Xu and Jeremy Smith and Eng Gee Lim and Yutao Yue and Hui Xiong", "abstract": "  Embodied perception is essential for intelligent vehicles and robots,\nenabling more natural interaction and task execution. However, these\nadvancements currently embrace vision level, rarely focusing on using 3D\nmodeling sensors, which limits the full understanding of surrounding objects\nwith multi-granular characteristics. Recently, as a promising automotive sensor\nwith affordable cost, 4D Millimeter-Wave radar provides denser point clouds\nthan conventional radar and perceives both semantic and physical\ncharacteristics of objects, thus enhancing the reliability of perception\nsystem. To foster the development of natural language-driven context\nunderstanding in radar scenes for 3D grounding, we construct the first dataset,\nTalk2Radar, which bridges these two modalities for 3D Referring Expression\nComprehension. Talk2Radar contains 8,682 referring prompt samples with 20,558\nreferred objects. Moreover, we propose a novel model, T-RadarNet for 3D REC\nupon point clouds, achieving state-of-the-art performances on Talk2Radar\ndataset compared with counterparts, where Deformable-FPN and Gated Graph Fusion\nare meticulously designed for efficient point cloud feature modeling and\ncross-modal fusion between radar and text features, respectively. Further,\ncomprehensive experiments are conducted to give a deep insight into radar-based\n3D REC. We release our project at https://github.com/GuanRunwei/Talk2Radar.\n", "link": "http://arxiv.org/abs/2405.12821v1", "date": "2024-05-21", "relevancy": 2.1941, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.553}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5477}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Talk2Radar%3A%20Bridging%20Natural%20Language%20with%204D%20mmWave%20Radar%20for%203D%0A%20%20Referring%20Expression%20Comprehension&body=Title%3A%20Talk2Radar%3A%20Bridging%20Natural%20Language%20with%204D%20mmWave%20Radar%20for%203D%0A%20%20Referring%20Expression%20Comprehension%0AAuthor%3A%20Runwei%20Guan%20and%20Ruixiao%20Zhang%20and%20Ningwei%20Ouyang%20and%20Jianan%20Liu%20and%20Ka%20Lok%20Man%20and%20Xiaohao%20Cai%20and%20Ming%20Xu%20and%20Jeremy%20Smith%20and%20Eng%20Gee%20Lim%20and%20Yutao%20Yue%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20Embodied%20perception%20is%20essential%20for%20intelligent%20vehicles%20and%20robots%2C%0Aenabling%20more%20natural%20interaction%20and%20task%20execution.%20However%2C%20these%0Aadvancements%20currently%20embrace%20vision%20level%2C%20rarely%20focusing%20on%20using%203D%0Amodeling%20sensors%2C%20which%20limits%20the%20full%20understanding%20of%20surrounding%20objects%0Awith%20multi-granular%20characteristics.%20Recently%2C%20as%20a%20promising%20automotive%20sensor%0Awith%20affordable%20cost%2C%204D%20Millimeter-Wave%20radar%20provides%20denser%20point%20clouds%0Athan%20conventional%20radar%20and%20perceives%20both%20semantic%20and%20physical%0Acharacteristics%20of%20objects%2C%20thus%20enhancing%20the%20reliability%20of%20perception%0Asystem.%20To%20foster%20the%20development%20of%20natural%20language-driven%20context%0Aunderstanding%20in%20radar%20scenes%20for%203D%20grounding%2C%20we%20construct%20the%20first%20dataset%2C%0ATalk2Radar%2C%20which%20bridges%20these%20two%20modalities%20for%203D%20Referring%20Expression%0AComprehension.%20Talk2Radar%20contains%208%2C682%20referring%20prompt%20samples%20with%2020%2C558%0Areferred%20objects.%20Moreover%2C%20we%20propose%20a%20novel%20model%2C%20T-RadarNet%20for%203D%20REC%0Aupon%20point%20clouds%2C%20achieving%20state-of-the-art%20performances%20on%20Talk2Radar%0Adataset%20compared%20with%20counterparts%2C%20where%20Deformable-FPN%20and%20Gated%20Graph%20Fusion%0Aare%20meticulously%20designed%20for%20efficient%20point%20cloud%20feature%20modeling%20and%0Across-modal%20fusion%20between%20radar%20and%20text%20features%2C%20respectively.%20Further%2C%0Acomprehensive%20experiments%20are%20conducted%20to%20give%20a%20deep%20insight%20into%20radar-based%0A3D%20REC.%20We%20release%20our%20project%20at%20https%3A//github.com/GuanRunwei/Talk2Radar.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTalk2Radar%253A%2520Bridging%2520Natural%2520Language%2520with%25204D%2520mmWave%2520Radar%2520for%25203D%250A%2520%2520Referring%2520Expression%2520Comprehension%26entry.906535625%3DRunwei%2520Guan%2520and%2520Ruixiao%2520Zhang%2520and%2520Ningwei%2520Ouyang%2520and%2520Jianan%2520Liu%2520and%2520Ka%2520Lok%2520Man%2520and%2520Xiaohao%2520Cai%2520and%2520Ming%2520Xu%2520and%2520Jeremy%2520Smith%2520and%2520Eng%2520Gee%2520Lim%2520and%2520Yutao%2520Yue%2520and%2520Hui%2520Xiong%26entry.1292438233%3D%2520%2520Embodied%2520perception%2520is%2520essential%2520for%2520intelligent%2520vehicles%2520and%2520robots%252C%250Aenabling%2520more%2520natural%2520interaction%2520and%2520task%2520execution.%2520However%252C%2520these%250Aadvancements%2520currently%2520embrace%2520vision%2520level%252C%2520rarely%2520focusing%2520on%2520using%25203D%250Amodeling%2520sensors%252C%2520which%2520limits%2520the%2520full%2520understanding%2520of%2520surrounding%2520objects%250Awith%2520multi-granular%2520characteristics.%2520Recently%252C%2520as%2520a%2520promising%2520automotive%2520sensor%250Awith%2520affordable%2520cost%252C%25204D%2520Millimeter-Wave%2520radar%2520provides%2520denser%2520point%2520clouds%250Athan%2520conventional%2520radar%2520and%2520perceives%2520both%2520semantic%2520and%2520physical%250Acharacteristics%2520of%2520objects%252C%2520thus%2520enhancing%2520the%2520reliability%2520of%2520perception%250Asystem.%2520To%2520foster%2520the%2520development%2520of%2520natural%2520language-driven%2520context%250Aunderstanding%2520in%2520radar%2520scenes%2520for%25203D%2520grounding%252C%2520we%2520construct%2520the%2520first%2520dataset%252C%250ATalk2Radar%252C%2520which%2520bridges%2520these%2520two%2520modalities%2520for%25203D%2520Referring%2520Expression%250AComprehension.%2520Talk2Radar%2520contains%25208%252C682%2520referring%2520prompt%2520samples%2520with%252020%252C558%250Areferred%2520objects.%2520Moreover%252C%2520we%2520propose%2520a%2520novel%2520model%252C%2520T-RadarNet%2520for%25203D%2520REC%250Aupon%2520point%2520clouds%252C%2520achieving%2520state-of-the-art%2520performances%2520on%2520Talk2Radar%250Adataset%2520compared%2520with%2520counterparts%252C%2520where%2520Deformable-FPN%2520and%2520Gated%2520Graph%2520Fusion%250Aare%2520meticulously%2520designed%2520for%2520efficient%2520point%2520cloud%2520feature%2520modeling%2520and%250Across-modal%2520fusion%2520between%2520radar%2520and%2520text%2520features%252C%2520respectively.%2520Further%252C%250Acomprehensive%2520experiments%2520are%2520conducted%2520to%2520give%2520a%2520deep%2520insight%2520into%2520radar-based%250A3D%2520REC.%2520We%2520release%2520our%2520project%2520at%2520https%253A//github.com/GuanRunwei/Talk2Radar.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Talk2Radar%3A%20Bridging%20Natural%20Language%20with%204D%20mmWave%20Radar%20for%203D%0A%20%20Referring%20Expression%20Comprehension&entry.906535625=Runwei%20Guan%20and%20Ruixiao%20Zhang%20and%20Ningwei%20Ouyang%20and%20Jianan%20Liu%20and%20Ka%20Lok%20Man%20and%20Xiaohao%20Cai%20and%20Ming%20Xu%20and%20Jeremy%20Smith%20and%20Eng%20Gee%20Lim%20and%20Yutao%20Yue%20and%20Hui%20Xiong&entry.1292438233=%20%20Embodied%20perception%20is%20essential%20for%20intelligent%20vehicles%20and%20robots%2C%0Aenabling%20more%20natural%20interaction%20and%20task%20execution.%20However%2C%20these%0Aadvancements%20currently%20embrace%20vision%20level%2C%20rarely%20focusing%20on%20using%203D%0Amodeling%20sensors%2C%20which%20limits%20the%20full%20understanding%20of%20surrounding%20objects%0Awith%20multi-granular%20characteristics.%20Recently%2C%20as%20a%20promising%20automotive%20sensor%0Awith%20affordable%20cost%2C%204D%20Millimeter-Wave%20radar%20provides%20denser%20point%20clouds%0Athan%20conventional%20radar%20and%20perceives%20both%20semantic%20and%20physical%0Acharacteristics%20of%20objects%2C%20thus%20enhancing%20the%20reliability%20of%20perception%0Asystem.%20To%20foster%20the%20development%20of%20natural%20language-driven%20context%0Aunderstanding%20in%20radar%20scenes%20for%203D%20grounding%2C%20we%20construct%20the%20first%20dataset%2C%0ATalk2Radar%2C%20which%20bridges%20these%20two%20modalities%20for%203D%20Referring%20Expression%0AComprehension.%20Talk2Radar%20contains%208%2C682%20referring%20prompt%20samples%20with%2020%2C558%0Areferred%20objects.%20Moreover%2C%20we%20propose%20a%20novel%20model%2C%20T-RadarNet%20for%203D%20REC%0Aupon%20point%20clouds%2C%20achieving%20state-of-the-art%20performances%20on%20Talk2Radar%0Adataset%20compared%20with%20counterparts%2C%20where%20Deformable-FPN%20and%20Gated%20Graph%20Fusion%0Aare%20meticulously%20designed%20for%20efficient%20point%20cloud%20feature%20modeling%20and%0Across-modal%20fusion%20between%20radar%20and%20text%20features%2C%20respectively.%20Further%2C%0Acomprehensive%20experiments%20are%20conducted%20to%20give%20a%20deep%20insight%20into%20radar-based%0A3D%20REC.%20We%20release%20our%20project%20at%20https%3A//github.com/GuanRunwei/Talk2Radar.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12821v1&entry.124074799=Read"},
{"title": "Mitigating Overconfidence in Out-of-Distribution Detection by Capturing\n  Extreme Activations", "author": "Mohammad Azizmalayeri and Ameen Abu-Hanna and Giovanni Cin\u00e0", "abstract": "  Detecting out-of-distribution (OOD) instances is crucial for the reliable\ndeployment of machine learning models in real-world scenarios. OOD inputs are\ncommonly expected to cause a more uncertain prediction in the primary task;\nhowever, there are OOD cases for which the model returns a highly confident\nprediction. This phenomenon, denoted as \"overconfidence\", presents a challenge\nto OOD detection. Specifically, theoretical evidence indicates that\noverconfidence is an intrinsic property of certain neural network\narchitectures, leading to poor OOD detection. In this work, we address this\nissue by measuring extreme activation values in the penultimate layer of neural\nnetworks and then leverage this proxy of overconfidence to improve on several\nOOD detection baselines. We test our method on a wide array of experiments\nspanning synthetic data and real-world data, tabular and image datasets,\nmultiple architectures such as ResNet and Transformer, different training loss\nfunctions, and include the scenarios examined in previous theoretical work.\nCompared to the baselines, our method often grants substantial improvements,\nwith double-digit increases in OOD detection AUC, and it does not damage\nperformance in any scenario.\n", "link": "http://arxiv.org/abs/2405.12658v1", "date": "2024-05-21", "relevancy": 2.1929, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5682}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5628}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Overconfidence%20in%20Out-of-Distribution%20Detection%20by%20Capturing%0A%20%20Extreme%20Activations&body=Title%3A%20Mitigating%20Overconfidence%20in%20Out-of-Distribution%20Detection%20by%20Capturing%0A%20%20Extreme%20Activations%0AAuthor%3A%20Mohammad%20Azizmalayeri%20and%20Ameen%20Abu-Hanna%20and%20Giovanni%20Cin%C3%A0%0AAbstract%3A%20%20%20Detecting%20out-of-distribution%20%28OOD%29%20instances%20is%20crucial%20for%20the%20reliable%0Adeployment%20of%20machine%20learning%20models%20in%20real-world%20scenarios.%20OOD%20inputs%20are%0Acommonly%20expected%20to%20cause%20a%20more%20uncertain%20prediction%20in%20the%20primary%20task%3B%0Ahowever%2C%20there%20are%20OOD%20cases%20for%20which%20the%20model%20returns%20a%20highly%20confident%0Aprediction.%20This%20phenomenon%2C%20denoted%20as%20%22overconfidence%22%2C%20presents%20a%20challenge%0Ato%20OOD%20detection.%20Specifically%2C%20theoretical%20evidence%20indicates%20that%0Aoverconfidence%20is%20an%20intrinsic%20property%20of%20certain%20neural%20network%0Aarchitectures%2C%20leading%20to%20poor%20OOD%20detection.%20In%20this%20work%2C%20we%20address%20this%0Aissue%20by%20measuring%20extreme%20activation%20values%20in%20the%20penultimate%20layer%20of%20neural%0Anetworks%20and%20then%20leverage%20this%20proxy%20of%20overconfidence%20to%20improve%20on%20several%0AOOD%20detection%20baselines.%20We%20test%20our%20method%20on%20a%20wide%20array%20of%20experiments%0Aspanning%20synthetic%20data%20and%20real-world%20data%2C%20tabular%20and%20image%20datasets%2C%0Amultiple%20architectures%20such%20as%20ResNet%20and%20Transformer%2C%20different%20training%20loss%0Afunctions%2C%20and%20include%20the%20scenarios%20examined%20in%20previous%20theoretical%20work.%0ACompared%20to%20the%20baselines%2C%20our%20method%20often%20grants%20substantial%20improvements%2C%0Awith%20double-digit%20increases%20in%20OOD%20detection%20AUC%2C%20and%20it%20does%20not%20damage%0Aperformance%20in%20any%20scenario.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12658v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Overconfidence%2520in%2520Out-of-Distribution%2520Detection%2520by%2520Capturing%250A%2520%2520Extreme%2520Activations%26entry.906535625%3DMohammad%2520Azizmalayeri%2520and%2520Ameen%2520Abu-Hanna%2520and%2520Giovanni%2520Cin%25C3%25A0%26entry.1292438233%3D%2520%2520Detecting%2520out-of-distribution%2520%2528OOD%2529%2520instances%2520is%2520crucial%2520for%2520the%2520reliable%250Adeployment%2520of%2520machine%2520learning%2520models%2520in%2520real-world%2520scenarios.%2520OOD%2520inputs%2520are%250Acommonly%2520expected%2520to%2520cause%2520a%2520more%2520uncertain%2520prediction%2520in%2520the%2520primary%2520task%253B%250Ahowever%252C%2520there%2520are%2520OOD%2520cases%2520for%2520which%2520the%2520model%2520returns%2520a%2520highly%2520confident%250Aprediction.%2520This%2520phenomenon%252C%2520denoted%2520as%2520%2522overconfidence%2522%252C%2520presents%2520a%2520challenge%250Ato%2520OOD%2520detection.%2520Specifically%252C%2520theoretical%2520evidence%2520indicates%2520that%250Aoverconfidence%2520is%2520an%2520intrinsic%2520property%2520of%2520certain%2520neural%2520network%250Aarchitectures%252C%2520leading%2520to%2520poor%2520OOD%2520detection.%2520In%2520this%2520work%252C%2520we%2520address%2520this%250Aissue%2520by%2520measuring%2520extreme%2520activation%2520values%2520in%2520the%2520penultimate%2520layer%2520of%2520neural%250Anetworks%2520and%2520then%2520leverage%2520this%2520proxy%2520of%2520overconfidence%2520to%2520improve%2520on%2520several%250AOOD%2520detection%2520baselines.%2520We%2520test%2520our%2520method%2520on%2520a%2520wide%2520array%2520of%2520experiments%250Aspanning%2520synthetic%2520data%2520and%2520real-world%2520data%252C%2520tabular%2520and%2520image%2520datasets%252C%250Amultiple%2520architectures%2520such%2520as%2520ResNet%2520and%2520Transformer%252C%2520different%2520training%2520loss%250Afunctions%252C%2520and%2520include%2520the%2520scenarios%2520examined%2520in%2520previous%2520theoretical%2520work.%250ACompared%2520to%2520the%2520baselines%252C%2520our%2520method%2520often%2520grants%2520substantial%2520improvements%252C%250Awith%2520double-digit%2520increases%2520in%2520OOD%2520detection%2520AUC%252C%2520and%2520it%2520does%2520not%2520damage%250Aperformance%2520in%2520any%2520scenario.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12658v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Overconfidence%20in%20Out-of-Distribution%20Detection%20by%20Capturing%0A%20%20Extreme%20Activations&entry.906535625=Mohammad%20Azizmalayeri%20and%20Ameen%20Abu-Hanna%20and%20Giovanni%20Cin%C3%A0&entry.1292438233=%20%20Detecting%20out-of-distribution%20%28OOD%29%20instances%20is%20crucial%20for%20the%20reliable%0Adeployment%20of%20machine%20learning%20models%20in%20real-world%20scenarios.%20OOD%20inputs%20are%0Acommonly%20expected%20to%20cause%20a%20more%20uncertain%20prediction%20in%20the%20primary%20task%3B%0Ahowever%2C%20there%20are%20OOD%20cases%20for%20which%20the%20model%20returns%20a%20highly%20confident%0Aprediction.%20This%20phenomenon%2C%20denoted%20as%20%22overconfidence%22%2C%20presents%20a%20challenge%0Ato%20OOD%20detection.%20Specifically%2C%20theoretical%20evidence%20indicates%20that%0Aoverconfidence%20is%20an%20intrinsic%20property%20of%20certain%20neural%20network%0Aarchitectures%2C%20leading%20to%20poor%20OOD%20detection.%20In%20this%20work%2C%20we%20address%20this%0Aissue%20by%20measuring%20extreme%20activation%20values%20in%20the%20penultimate%20layer%20of%20neural%0Anetworks%20and%20then%20leverage%20this%20proxy%20of%20overconfidence%20to%20improve%20on%20several%0AOOD%20detection%20baselines.%20We%20test%20our%20method%20on%20a%20wide%20array%20of%20experiments%0Aspanning%20synthetic%20data%20and%20real-world%20data%2C%20tabular%20and%20image%20datasets%2C%0Amultiple%20architectures%20such%20as%20ResNet%20and%20Transformer%2C%20different%20training%20loss%0Afunctions%2C%20and%20include%20the%20scenarios%20examined%20in%20previous%20theoretical%20work.%0ACompared%20to%20the%20baselines%2C%20our%20method%20often%20grants%20substantial%20improvements%2C%0Awith%20double-digit%20increases%20in%20OOD%20detection%20AUC%2C%20and%20it%20does%20not%20damage%0Aperformance%20in%20any%20scenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12658v1&entry.124074799=Read"},
{"title": "An Aligning and Training Framework for Multimodal Recommendations", "author": "Yifan Liu and Kangning Zhang and Xiangyuan Ren and Yanhua Huang and Jiarui Jin and Yingjie Qin and Ruilong Su and Ruiwen Xu and Weinan Zhang", "abstract": "  With the development of multimedia applications, multimodal recommendations\nplay an essential role, as they can leverage rich contexts beyond user and item\ninteractions. Existing methods mainly use them to help learn ID features;\nhowever, there exist semantic gaps among multimodal content features and ID\nfeatures. Directly using multimodal information as an auxiliary would lead to\nmisalignment in items' and users' representations. In this paper, we first\nsystematically investigate the misalignment issue in multimodal\nrecommendations, and propose a solution named AlignRec. In AlignRec, the\nrecommendation objective is decomposed into three alignments, namely alignment\nwithin contents, alignment between content and categorical ID, and alignment\nbetween users and items. Each alignment is characterized by a distinct\nobjective function. To effectively train AlignRec, we propose starting from\npre-training the first alignment to obtain unified multimodal features and\nsubsequently training the following two alignments together. As it is essential\nto analyze whether each multimodal feature helps in training, we design three\nnew classes of metrics to evaluate intermediate performance. Our extensive\nexperiments on three real-world datasets consistently verify the superiority of\nAlignRec compared to nine baselines. We also find that the multimodal features\ngenerated by our framework are better than currently used ones, which are to be\nopen-sourced.\n", "link": "http://arxiv.org/abs/2403.12384v3", "date": "2024-05-21", "relevancy": 2.1918, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5742}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.533}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Aligning%20and%20Training%20Framework%20for%20Multimodal%20Recommendations&body=Title%3A%20An%20Aligning%20and%20Training%20Framework%20for%20Multimodal%20Recommendations%0AAuthor%3A%20Yifan%20Liu%20and%20Kangning%20Zhang%20and%20Xiangyuan%20Ren%20and%20Yanhua%20Huang%20and%20Jiarui%20Jin%20and%20Yingjie%20Qin%20and%20Ruilong%20Su%20and%20Ruiwen%20Xu%20and%20Weinan%20Zhang%0AAbstract%3A%20%20%20With%20the%20development%20of%20multimedia%20applications%2C%20multimodal%20recommendations%0Aplay%20an%20essential%20role%2C%20as%20they%20can%20leverage%20rich%20contexts%20beyond%20user%20and%20item%0Ainteractions.%20Existing%20methods%20mainly%20use%20them%20to%20help%20learn%20ID%20features%3B%0Ahowever%2C%20there%20exist%20semantic%20gaps%20among%20multimodal%20content%20features%20and%20ID%0Afeatures.%20Directly%20using%20multimodal%20information%20as%20an%20auxiliary%20would%20lead%20to%0Amisalignment%20in%20items%27%20and%20users%27%20representations.%20In%20this%20paper%2C%20we%20first%0Asystematically%20investigate%20the%20misalignment%20issue%20in%20multimodal%0Arecommendations%2C%20and%20propose%20a%20solution%20named%20AlignRec.%20In%20AlignRec%2C%20the%0Arecommendation%20objective%20is%20decomposed%20into%20three%20alignments%2C%20namely%20alignment%0Awithin%20contents%2C%20alignment%20between%20content%20and%20categorical%20ID%2C%20and%20alignment%0Abetween%20users%20and%20items.%20Each%20alignment%20is%20characterized%20by%20a%20distinct%0Aobjective%20function.%20To%20effectively%20train%20AlignRec%2C%20we%20propose%20starting%20from%0Apre-training%20the%20first%20alignment%20to%20obtain%20unified%20multimodal%20features%20and%0Asubsequently%20training%20the%20following%20two%20alignments%20together.%20As%20it%20is%20essential%0Ato%20analyze%20whether%20each%20multimodal%20feature%20helps%20in%20training%2C%20we%20design%20three%0Anew%20classes%20of%20metrics%20to%20evaluate%20intermediate%20performance.%20Our%20extensive%0Aexperiments%20on%20three%20real-world%20datasets%20consistently%20verify%20the%20superiority%20of%0AAlignRec%20compared%20to%20nine%20baselines.%20We%20also%20find%20that%20the%20multimodal%20features%0Agenerated%20by%20our%20framework%20are%20better%20than%20currently%20used%20ones%2C%20which%20are%20to%20be%0Aopen-sourced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12384v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Aligning%2520and%2520Training%2520Framework%2520for%2520Multimodal%2520Recommendations%26entry.906535625%3DYifan%2520Liu%2520and%2520Kangning%2520Zhang%2520and%2520Xiangyuan%2520Ren%2520and%2520Yanhua%2520Huang%2520and%2520Jiarui%2520Jin%2520and%2520Yingjie%2520Qin%2520and%2520Ruilong%2520Su%2520and%2520Ruiwen%2520Xu%2520and%2520Weinan%2520Zhang%26entry.1292438233%3D%2520%2520With%2520the%2520development%2520of%2520multimedia%2520applications%252C%2520multimodal%2520recommendations%250Aplay%2520an%2520essential%2520role%252C%2520as%2520they%2520can%2520leverage%2520rich%2520contexts%2520beyond%2520user%2520and%2520item%250Ainteractions.%2520Existing%2520methods%2520mainly%2520use%2520them%2520to%2520help%2520learn%2520ID%2520features%253B%250Ahowever%252C%2520there%2520exist%2520semantic%2520gaps%2520among%2520multimodal%2520content%2520features%2520and%2520ID%250Afeatures.%2520Directly%2520using%2520multimodal%2520information%2520as%2520an%2520auxiliary%2520would%2520lead%2520to%250Amisalignment%2520in%2520items%2527%2520and%2520users%2527%2520representations.%2520In%2520this%2520paper%252C%2520we%2520first%250Asystematically%2520investigate%2520the%2520misalignment%2520issue%2520in%2520multimodal%250Arecommendations%252C%2520and%2520propose%2520a%2520solution%2520named%2520AlignRec.%2520In%2520AlignRec%252C%2520the%250Arecommendation%2520objective%2520is%2520decomposed%2520into%2520three%2520alignments%252C%2520namely%2520alignment%250Awithin%2520contents%252C%2520alignment%2520between%2520content%2520and%2520categorical%2520ID%252C%2520and%2520alignment%250Abetween%2520users%2520and%2520items.%2520Each%2520alignment%2520is%2520characterized%2520by%2520a%2520distinct%250Aobjective%2520function.%2520To%2520effectively%2520train%2520AlignRec%252C%2520we%2520propose%2520starting%2520from%250Apre-training%2520the%2520first%2520alignment%2520to%2520obtain%2520unified%2520multimodal%2520features%2520and%250Asubsequently%2520training%2520the%2520following%2520two%2520alignments%2520together.%2520As%2520it%2520is%2520essential%250Ato%2520analyze%2520whether%2520each%2520multimodal%2520feature%2520helps%2520in%2520training%252C%2520we%2520design%2520three%250Anew%2520classes%2520of%2520metrics%2520to%2520evaluate%2520intermediate%2520performance.%2520Our%2520extensive%250Aexperiments%2520on%2520three%2520real-world%2520datasets%2520consistently%2520verify%2520the%2520superiority%2520of%250AAlignRec%2520compared%2520to%2520nine%2520baselines.%2520We%2520also%2520find%2520that%2520the%2520multimodal%2520features%250Agenerated%2520by%2520our%2520framework%2520are%2520better%2520than%2520currently%2520used%2520ones%252C%2520which%2520are%2520to%2520be%250Aopen-sourced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12384v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Aligning%20and%20Training%20Framework%20for%20Multimodal%20Recommendations&entry.906535625=Yifan%20Liu%20and%20Kangning%20Zhang%20and%20Xiangyuan%20Ren%20and%20Yanhua%20Huang%20and%20Jiarui%20Jin%20and%20Yingjie%20Qin%20and%20Ruilong%20Su%20and%20Ruiwen%20Xu%20and%20Weinan%20Zhang&entry.1292438233=%20%20With%20the%20development%20of%20multimedia%20applications%2C%20multimodal%20recommendations%0Aplay%20an%20essential%20role%2C%20as%20they%20can%20leverage%20rich%20contexts%20beyond%20user%20and%20item%0Ainteractions.%20Existing%20methods%20mainly%20use%20them%20to%20help%20learn%20ID%20features%3B%0Ahowever%2C%20there%20exist%20semantic%20gaps%20among%20multimodal%20content%20features%20and%20ID%0Afeatures.%20Directly%20using%20multimodal%20information%20as%20an%20auxiliary%20would%20lead%20to%0Amisalignment%20in%20items%27%20and%20users%27%20representations.%20In%20this%20paper%2C%20we%20first%0Asystematically%20investigate%20the%20misalignment%20issue%20in%20multimodal%0Arecommendations%2C%20and%20propose%20a%20solution%20named%20AlignRec.%20In%20AlignRec%2C%20the%0Arecommendation%20objective%20is%20decomposed%20into%20three%20alignments%2C%20namely%20alignment%0Awithin%20contents%2C%20alignment%20between%20content%20and%20categorical%20ID%2C%20and%20alignment%0Abetween%20users%20and%20items.%20Each%20alignment%20is%20characterized%20by%20a%20distinct%0Aobjective%20function.%20To%20effectively%20train%20AlignRec%2C%20we%20propose%20starting%20from%0Apre-training%20the%20first%20alignment%20to%20obtain%20unified%20multimodal%20features%20and%0Asubsequently%20training%20the%20following%20two%20alignments%20together.%20As%20it%20is%20essential%0Ato%20analyze%20whether%20each%20multimodal%20feature%20helps%20in%20training%2C%20we%20design%20three%0Anew%20classes%20of%20metrics%20to%20evaluate%20intermediate%20performance.%20Our%20extensive%0Aexperiments%20on%20three%20real-world%20datasets%20consistently%20verify%20the%20superiority%20of%0AAlignRec%20compared%20to%20nine%20baselines.%20We%20also%20find%20that%20the%20multimodal%20features%0Agenerated%20by%20our%20framework%20are%20better%20than%20currently%20used%20ones%2C%20which%20are%20to%20be%0Aopen-sourced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12384v3&entry.124074799=Read"},
{"title": "Equivariant Spatio-Temporal Attentive Graph Networks to Simulate\n  Physical Dynamics", "author": "Liming Wu and Zhichao Hou and Jirui Yuan and Yu Rong and Wenbing Huang", "abstract": "  Learning to represent and simulate the dynamics of physical systems is a\ncrucial yet challenging task. Existing equivariant Graph Neural Network (GNN)\nbased methods have encapsulated the symmetry of physics, \\emph{e.g.},\ntranslations, rotations, etc, leading to better generalization ability.\nNevertheless, their frame-to-frame formulation of the task overlooks the\nnon-Markov property mainly incurred by unobserved dynamics in the environment.\nIn this paper, we reformulate dynamics simulation as a spatio-temporal\nprediction task, by employing the trajectory in the past period to recover the\nNon-Markovian interactions. We propose Equivariant Spatio-Temporal Attentive\nGraph Networks (ESTAG), an equivariant version of spatio-temporal GNNs, to\nfulfill our purpose. At its core, we design a novel Equivariant Discrete\nFourier Transform (EDFT) to extract periodic patterns from the history frames,\nand then construct an Equivariant Spatial Module (ESM) to accomplish spatial\nmessage passing, and an Equivariant Temporal Module (ETM) with the forward\nattention and equivariant pooling mechanisms to aggregate temporal message. We\nevaluate our model on three real datasets corresponding to the molecular-,\nprotein- and macro-level. Experimental results verify the effectiveness of\nESTAG compared to typical spatio-temporal GNNs and equivariant GNNs.\n", "link": "http://arxiv.org/abs/2405.12868v1", "date": "2024-05-21", "relevancy": 2.1885, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5544}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5507}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivariant%20Spatio-Temporal%20Attentive%20Graph%20Networks%20to%20Simulate%0A%20%20Physical%20Dynamics&body=Title%3A%20Equivariant%20Spatio-Temporal%20Attentive%20Graph%20Networks%20to%20Simulate%0A%20%20Physical%20Dynamics%0AAuthor%3A%20Liming%20Wu%20and%20Zhichao%20Hou%20and%20Jirui%20Yuan%20and%20Yu%20Rong%20and%20Wenbing%20Huang%0AAbstract%3A%20%20%20Learning%20to%20represent%20and%20simulate%20the%20dynamics%20of%20physical%20systems%20is%20a%0Acrucial%20yet%20challenging%20task.%20Existing%20equivariant%20Graph%20Neural%20Network%20%28GNN%29%0Abased%20methods%20have%20encapsulated%20the%20symmetry%20of%20physics%2C%20%5Cemph%7Be.g.%7D%2C%0Atranslations%2C%20rotations%2C%20etc%2C%20leading%20to%20better%20generalization%20ability.%0ANevertheless%2C%20their%20frame-to-frame%20formulation%20of%20the%20task%20overlooks%20the%0Anon-Markov%20property%20mainly%20incurred%20by%20unobserved%20dynamics%20in%20the%20environment.%0AIn%20this%20paper%2C%20we%20reformulate%20dynamics%20simulation%20as%20a%20spatio-temporal%0Aprediction%20task%2C%20by%20employing%20the%20trajectory%20in%20the%20past%20period%20to%20recover%20the%0ANon-Markovian%20interactions.%20We%20propose%20Equivariant%20Spatio-Temporal%20Attentive%0AGraph%20Networks%20%28ESTAG%29%2C%20an%20equivariant%20version%20of%20spatio-temporal%20GNNs%2C%20to%0Afulfill%20our%20purpose.%20At%20its%20core%2C%20we%20design%20a%20novel%20Equivariant%20Discrete%0AFourier%20Transform%20%28EDFT%29%20to%20extract%20periodic%20patterns%20from%20the%20history%20frames%2C%0Aand%20then%20construct%20an%20Equivariant%20Spatial%20Module%20%28ESM%29%20to%20accomplish%20spatial%0Amessage%20passing%2C%20and%20an%20Equivariant%20Temporal%20Module%20%28ETM%29%20with%20the%20forward%0Aattention%20and%20equivariant%20pooling%20mechanisms%20to%20aggregate%20temporal%20message.%20We%0Aevaluate%20our%20model%20on%20three%20real%20datasets%20corresponding%20to%20the%20molecular-%2C%0Aprotein-%20and%20macro-level.%20Experimental%20results%20verify%20the%20effectiveness%20of%0AESTAG%20compared%20to%20typical%20spatio-temporal%20GNNs%20and%20equivariant%20GNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivariant%2520Spatio-Temporal%2520Attentive%2520Graph%2520Networks%2520to%2520Simulate%250A%2520%2520Physical%2520Dynamics%26entry.906535625%3DLiming%2520Wu%2520and%2520Zhichao%2520Hou%2520and%2520Jirui%2520Yuan%2520and%2520Yu%2520Rong%2520and%2520Wenbing%2520Huang%26entry.1292438233%3D%2520%2520Learning%2520to%2520represent%2520and%2520simulate%2520the%2520dynamics%2520of%2520physical%2520systems%2520is%2520a%250Acrucial%2520yet%2520challenging%2520task.%2520Existing%2520equivariant%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%250Abased%2520methods%2520have%2520encapsulated%2520the%2520symmetry%2520of%2520physics%252C%2520%255Cemph%257Be.g.%257D%252C%250Atranslations%252C%2520rotations%252C%2520etc%252C%2520leading%2520to%2520better%2520generalization%2520ability.%250ANevertheless%252C%2520their%2520frame-to-frame%2520formulation%2520of%2520the%2520task%2520overlooks%2520the%250Anon-Markov%2520property%2520mainly%2520incurred%2520by%2520unobserved%2520dynamics%2520in%2520the%2520environment.%250AIn%2520this%2520paper%252C%2520we%2520reformulate%2520dynamics%2520simulation%2520as%2520a%2520spatio-temporal%250Aprediction%2520task%252C%2520by%2520employing%2520the%2520trajectory%2520in%2520the%2520past%2520period%2520to%2520recover%2520the%250ANon-Markovian%2520interactions.%2520We%2520propose%2520Equivariant%2520Spatio-Temporal%2520Attentive%250AGraph%2520Networks%2520%2528ESTAG%2529%252C%2520an%2520equivariant%2520version%2520of%2520spatio-temporal%2520GNNs%252C%2520to%250Afulfill%2520our%2520purpose.%2520At%2520its%2520core%252C%2520we%2520design%2520a%2520novel%2520Equivariant%2520Discrete%250AFourier%2520Transform%2520%2528EDFT%2529%2520to%2520extract%2520periodic%2520patterns%2520from%2520the%2520history%2520frames%252C%250Aand%2520then%2520construct%2520an%2520Equivariant%2520Spatial%2520Module%2520%2528ESM%2529%2520to%2520accomplish%2520spatial%250Amessage%2520passing%252C%2520and%2520an%2520Equivariant%2520Temporal%2520Module%2520%2528ETM%2529%2520with%2520the%2520forward%250Aattention%2520and%2520equivariant%2520pooling%2520mechanisms%2520to%2520aggregate%2520temporal%2520message.%2520We%250Aevaluate%2520our%2520model%2520on%2520three%2520real%2520datasets%2520corresponding%2520to%2520the%2520molecular-%252C%250Aprotein-%2520and%2520macro-level.%2520Experimental%2520results%2520verify%2520the%2520effectiveness%2520of%250AESTAG%2520compared%2520to%2520typical%2520spatio-temporal%2520GNNs%2520and%2520equivariant%2520GNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariant%20Spatio-Temporal%20Attentive%20Graph%20Networks%20to%20Simulate%0A%20%20Physical%20Dynamics&entry.906535625=Liming%20Wu%20and%20Zhichao%20Hou%20and%20Jirui%20Yuan%20and%20Yu%20Rong%20and%20Wenbing%20Huang&entry.1292438233=%20%20Learning%20to%20represent%20and%20simulate%20the%20dynamics%20of%20physical%20systems%20is%20a%0Acrucial%20yet%20challenging%20task.%20Existing%20equivariant%20Graph%20Neural%20Network%20%28GNN%29%0Abased%20methods%20have%20encapsulated%20the%20symmetry%20of%20physics%2C%20%5Cemph%7Be.g.%7D%2C%0Atranslations%2C%20rotations%2C%20etc%2C%20leading%20to%20better%20generalization%20ability.%0ANevertheless%2C%20their%20frame-to-frame%20formulation%20of%20the%20task%20overlooks%20the%0Anon-Markov%20property%20mainly%20incurred%20by%20unobserved%20dynamics%20in%20the%20environment.%0AIn%20this%20paper%2C%20we%20reformulate%20dynamics%20simulation%20as%20a%20spatio-temporal%0Aprediction%20task%2C%20by%20employing%20the%20trajectory%20in%20the%20past%20period%20to%20recover%20the%0ANon-Markovian%20interactions.%20We%20propose%20Equivariant%20Spatio-Temporal%20Attentive%0AGraph%20Networks%20%28ESTAG%29%2C%20an%20equivariant%20version%20of%20spatio-temporal%20GNNs%2C%20to%0Afulfill%20our%20purpose.%20At%20its%20core%2C%20we%20design%20a%20novel%20Equivariant%20Discrete%0AFourier%20Transform%20%28EDFT%29%20to%20extract%20periodic%20patterns%20from%20the%20history%20frames%2C%0Aand%20then%20construct%20an%20Equivariant%20Spatial%20Module%20%28ESM%29%20to%20accomplish%20spatial%0Amessage%20passing%2C%20and%20an%20Equivariant%20Temporal%20Module%20%28ETM%29%20with%20the%20forward%0Aattention%20and%20equivariant%20pooling%20mechanisms%20to%20aggregate%20temporal%20message.%20We%0Aevaluate%20our%20model%20on%20three%20real%20datasets%20corresponding%20to%20the%20molecular-%2C%0Aprotein-%20and%20macro-level.%20Experimental%20results%20verify%20the%20effectiveness%20of%0AESTAG%20compared%20to%20typical%20spatio-temporal%20GNNs%20and%20equivariant%20GNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12868v1&entry.124074799=Read"},
{"title": "Leveraging Neural Radiance Fields for Pose Estimation of an Unknown\n  Space Object during Proximity Operations", "author": "Antoine Legrand and Renaud Detry and Christophe De Vleeschouwer", "abstract": "  We address the estimation of the 6D pose of an unknown target spacecraft\nrelative to a monocular camera, a key step towards the autonomous rendezvous\nand proximity operations required by future Active Debris Removal missions. We\npresent a novel method that enables an \"off-the-shelf\" spacecraft pose\nestimator, which is supposed to known the target CAD model, to be applied on an\nunknown target. Our method relies on an in-the wild NeRF, i.e., a Neural\nRadiance Field that employs learnable appearance embeddings to represent\nvarying illumination conditions found in natural scenes. We train the NeRF\nmodel using a sparse collection of images that depict the target, and in turn\ngenerate a large dataset that is diverse both in terms of viewpoint and\nillumination. This dataset is then used to train the pose estimation network.\nWe validate our method on the Hardware-In-the-Loop images of SPEED+ that\nemulate lighting conditions close to those encountered on orbit. We demonstrate\nthat our method successfully enables the training of an off-the-shelf\nspacecraft pose estimation network from a sparse set of images. Furthermore, we\nshow that a network trained using our method performs similarly to a model\ntrained on synthetic images generated using the CAD model of the target.\n", "link": "http://arxiv.org/abs/2405.12728v1", "date": "2024-05-21", "relevancy": 2.1852, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5519}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5491}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Neural%20Radiance%20Fields%20for%20Pose%20Estimation%20of%20an%20Unknown%0A%20%20Space%20Object%20during%20Proximity%20Operations&body=Title%3A%20Leveraging%20Neural%20Radiance%20Fields%20for%20Pose%20Estimation%20of%20an%20Unknown%0A%20%20Space%20Object%20during%20Proximity%20Operations%0AAuthor%3A%20Antoine%20Legrand%20and%20Renaud%20Detry%20and%20Christophe%20De%20Vleeschouwer%0AAbstract%3A%20%20%20We%20address%20the%20estimation%20of%20the%206D%20pose%20of%20an%20unknown%20target%20spacecraft%0Arelative%20to%20a%20monocular%20camera%2C%20a%20key%20step%20towards%20the%20autonomous%20rendezvous%0Aand%20proximity%20operations%20required%20by%20future%20Active%20Debris%20Removal%20missions.%20We%0Apresent%20a%20novel%20method%20that%20enables%20an%20%22off-the-shelf%22%20spacecraft%20pose%0Aestimator%2C%20which%20is%20supposed%20to%20known%20the%20target%20CAD%20model%2C%20to%20be%20applied%20on%20an%0Aunknown%20target.%20Our%20method%20relies%20on%20an%20in-the%20wild%20NeRF%2C%20i.e.%2C%20a%20Neural%0ARadiance%20Field%20that%20employs%20learnable%20appearance%20embeddings%20to%20represent%0Avarying%20illumination%20conditions%20found%20in%20natural%20scenes.%20We%20train%20the%20NeRF%0Amodel%20using%20a%20sparse%20collection%20of%20images%20that%20depict%20the%20target%2C%20and%20in%20turn%0Agenerate%20a%20large%20dataset%20that%20is%20diverse%20both%20in%20terms%20of%20viewpoint%20and%0Aillumination.%20This%20dataset%20is%20then%20used%20to%20train%20the%20pose%20estimation%20network.%0AWe%20validate%20our%20method%20on%20the%20Hardware-In-the-Loop%20images%20of%20SPEED%2B%20that%0Aemulate%20lighting%20conditions%20close%20to%20those%20encountered%20on%20orbit.%20We%20demonstrate%0Athat%20our%20method%20successfully%20enables%20the%20training%20of%20an%20off-the-shelf%0Aspacecraft%20pose%20estimation%20network%20from%20a%20sparse%20set%20of%20images.%20Furthermore%2C%20we%0Ashow%20that%20a%20network%20trained%20using%20our%20method%20performs%20similarly%20to%20a%20model%0Atrained%20on%20synthetic%20images%20generated%20using%20the%20CAD%20model%20of%20the%20target.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12728v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Neural%2520Radiance%2520Fields%2520for%2520Pose%2520Estimation%2520of%2520an%2520Unknown%250A%2520%2520Space%2520Object%2520during%2520Proximity%2520Operations%26entry.906535625%3DAntoine%2520Legrand%2520and%2520Renaud%2520Detry%2520and%2520Christophe%2520De%2520Vleeschouwer%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520estimation%2520of%2520the%25206D%2520pose%2520of%2520an%2520unknown%2520target%2520spacecraft%250Arelative%2520to%2520a%2520monocular%2520camera%252C%2520a%2520key%2520step%2520towards%2520the%2520autonomous%2520rendezvous%250Aand%2520proximity%2520operations%2520required%2520by%2520future%2520Active%2520Debris%2520Removal%2520missions.%2520We%250Apresent%2520a%2520novel%2520method%2520that%2520enables%2520an%2520%2522off-the-shelf%2522%2520spacecraft%2520pose%250Aestimator%252C%2520which%2520is%2520supposed%2520to%2520known%2520the%2520target%2520CAD%2520model%252C%2520to%2520be%2520applied%2520on%2520an%250Aunknown%2520target.%2520Our%2520method%2520relies%2520on%2520an%2520in-the%2520wild%2520NeRF%252C%2520i.e.%252C%2520a%2520Neural%250ARadiance%2520Field%2520that%2520employs%2520learnable%2520appearance%2520embeddings%2520to%2520represent%250Avarying%2520illumination%2520conditions%2520found%2520in%2520natural%2520scenes.%2520We%2520train%2520the%2520NeRF%250Amodel%2520using%2520a%2520sparse%2520collection%2520of%2520images%2520that%2520depict%2520the%2520target%252C%2520and%2520in%2520turn%250Agenerate%2520a%2520large%2520dataset%2520that%2520is%2520diverse%2520both%2520in%2520terms%2520of%2520viewpoint%2520and%250Aillumination.%2520This%2520dataset%2520is%2520then%2520used%2520to%2520train%2520the%2520pose%2520estimation%2520network.%250AWe%2520validate%2520our%2520method%2520on%2520the%2520Hardware-In-the-Loop%2520images%2520of%2520SPEED%252B%2520that%250Aemulate%2520lighting%2520conditions%2520close%2520to%2520those%2520encountered%2520on%2520orbit.%2520We%2520demonstrate%250Athat%2520our%2520method%2520successfully%2520enables%2520the%2520training%2520of%2520an%2520off-the-shelf%250Aspacecraft%2520pose%2520estimation%2520network%2520from%2520a%2520sparse%2520set%2520of%2520images.%2520Furthermore%252C%2520we%250Ashow%2520that%2520a%2520network%2520trained%2520using%2520our%2520method%2520performs%2520similarly%2520to%2520a%2520model%250Atrained%2520on%2520synthetic%2520images%2520generated%2520using%2520the%2520CAD%2520model%2520of%2520the%2520target.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12728v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Neural%20Radiance%20Fields%20for%20Pose%20Estimation%20of%20an%20Unknown%0A%20%20Space%20Object%20during%20Proximity%20Operations&entry.906535625=Antoine%20Legrand%20and%20Renaud%20Detry%20and%20Christophe%20De%20Vleeschouwer&entry.1292438233=%20%20We%20address%20the%20estimation%20of%20the%206D%20pose%20of%20an%20unknown%20target%20spacecraft%0Arelative%20to%20a%20monocular%20camera%2C%20a%20key%20step%20towards%20the%20autonomous%20rendezvous%0Aand%20proximity%20operations%20required%20by%20future%20Active%20Debris%20Removal%20missions.%20We%0Apresent%20a%20novel%20method%20that%20enables%20an%20%22off-the-shelf%22%20spacecraft%20pose%0Aestimator%2C%20which%20is%20supposed%20to%20known%20the%20target%20CAD%20model%2C%20to%20be%20applied%20on%20an%0Aunknown%20target.%20Our%20method%20relies%20on%20an%20in-the%20wild%20NeRF%2C%20i.e.%2C%20a%20Neural%0ARadiance%20Field%20that%20employs%20learnable%20appearance%20embeddings%20to%20represent%0Avarying%20illumination%20conditions%20found%20in%20natural%20scenes.%20We%20train%20the%20NeRF%0Amodel%20using%20a%20sparse%20collection%20of%20images%20that%20depict%20the%20target%2C%20and%20in%20turn%0Agenerate%20a%20large%20dataset%20that%20is%20diverse%20both%20in%20terms%20of%20viewpoint%20and%0Aillumination.%20This%20dataset%20is%20then%20used%20to%20train%20the%20pose%20estimation%20network.%0AWe%20validate%20our%20method%20on%20the%20Hardware-In-the-Loop%20images%20of%20SPEED%2B%20that%0Aemulate%20lighting%20conditions%20close%20to%20those%20encountered%20on%20orbit.%20We%20demonstrate%0Athat%20our%20method%20successfully%20enables%20the%20training%20of%20an%20off-the-shelf%0Aspacecraft%20pose%20estimation%20network%20from%20a%20sparse%20set%20of%20images.%20Furthermore%2C%20we%0Ashow%20that%20a%20network%20trained%20using%20our%20method%20performs%20similarly%20to%20a%20model%0Atrained%20on%20synthetic%20images%20generated%20using%20the%20CAD%20model%20of%20the%20target.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12728v1&entry.124074799=Read"},
{"title": "Scene Graph Generation Strategy with Co-occurrence Knowledge and\n  Learnable Term Frequency", "author": "Hyeongjin Kim and Sangwon Kim and Dasom Ahn and Jong Taek Lee and Byoung Chul Ko", "abstract": "  Scene graph generation (SGG) is an important task in image understanding\nbecause it represents the relationships between objects in an image as a graph\nstructure, making it possible to understand the semantic relationships between\nobjects intuitively. Previous SGG studies used a message-passing neural\nnetworks (MPNN) to update features, which can effectively reflect information\nabout surrounding objects. However, these studies have failed to reflect the\nco-occurrence of objects during SGG generation. In addition, they only\naddressed the long-tail problem of the training dataset from the perspectives\nof sampling and learning methods. To address these two problems, we propose\nCooK, which reflects the Co-occurrence Knowledge between objects, and the\nlearnable term frequency-inverse document frequency (TF-l-IDF) to solve the\nlong-tail problem. We applied the proposed model to the SGG benchmark dataset,\nand the results showed a performance improvement of up to 3.8% compared with\nexisting state-of-the-art models in SGGen subtask. The proposed method exhibits\ngeneralization ability from the results obtained, showing uniform performance\nimprovement for all MPNN models.\n", "link": "http://arxiv.org/abs/2405.12648v1", "date": "2024-05-21", "relevancy": 2.1273, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5361}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5352}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scene%20Graph%20Generation%20Strategy%20with%20Co-occurrence%20Knowledge%20and%0A%20%20Learnable%20Term%20Frequency&body=Title%3A%20Scene%20Graph%20Generation%20Strategy%20with%20Co-occurrence%20Knowledge%20and%0A%20%20Learnable%20Term%20Frequency%0AAuthor%3A%20Hyeongjin%20Kim%20and%20Sangwon%20Kim%20and%20Dasom%20Ahn%20and%20Jong%20Taek%20Lee%20and%20Byoung%20Chul%20Ko%0AAbstract%3A%20%20%20Scene%20graph%20generation%20%28SGG%29%20is%20an%20important%20task%20in%20image%20understanding%0Abecause%20it%20represents%20the%20relationships%20between%20objects%20in%20an%20image%20as%20a%20graph%0Astructure%2C%20making%20it%20possible%20to%20understand%20the%20semantic%20relationships%20between%0Aobjects%20intuitively.%20Previous%20SGG%20studies%20used%20a%20message-passing%20neural%0Anetworks%20%28MPNN%29%20to%20update%20features%2C%20which%20can%20effectively%20reflect%20information%0Aabout%20surrounding%20objects.%20However%2C%20these%20studies%20have%20failed%20to%20reflect%20the%0Aco-occurrence%20of%20objects%20during%20SGG%20generation.%20In%20addition%2C%20they%20only%0Aaddressed%20the%20long-tail%20problem%20of%20the%20training%20dataset%20from%20the%20perspectives%0Aof%20sampling%20and%20learning%20methods.%20To%20address%20these%20two%20problems%2C%20we%20propose%0ACooK%2C%20which%20reflects%20the%20Co-occurrence%20Knowledge%20between%20objects%2C%20and%20the%0Alearnable%20term%20frequency-inverse%20document%20frequency%20%28TF-l-IDF%29%20to%20solve%20the%0Along-tail%20problem.%20We%20applied%20the%20proposed%20model%20to%20the%20SGG%20benchmark%20dataset%2C%0Aand%20the%20results%20showed%20a%20performance%20improvement%20of%20up%20to%203.8%25%20compared%20with%0Aexisting%20state-of-the-art%20models%20in%20SGGen%20subtask.%20The%20proposed%20method%20exhibits%0Ageneralization%20ability%20from%20the%20results%20obtained%2C%20showing%20uniform%20performance%0Aimprovement%20for%20all%20MPNN%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScene%2520Graph%2520Generation%2520Strategy%2520with%2520Co-occurrence%2520Knowledge%2520and%250A%2520%2520Learnable%2520Term%2520Frequency%26entry.906535625%3DHyeongjin%2520Kim%2520and%2520Sangwon%2520Kim%2520and%2520Dasom%2520Ahn%2520and%2520Jong%2520Taek%2520Lee%2520and%2520Byoung%2520Chul%2520Ko%26entry.1292438233%3D%2520%2520Scene%2520graph%2520generation%2520%2528SGG%2529%2520is%2520an%2520important%2520task%2520in%2520image%2520understanding%250Abecause%2520it%2520represents%2520the%2520relationships%2520between%2520objects%2520in%2520an%2520image%2520as%2520a%2520graph%250Astructure%252C%2520making%2520it%2520possible%2520to%2520understand%2520the%2520semantic%2520relationships%2520between%250Aobjects%2520intuitively.%2520Previous%2520SGG%2520studies%2520used%2520a%2520message-passing%2520neural%250Anetworks%2520%2528MPNN%2529%2520to%2520update%2520features%252C%2520which%2520can%2520effectively%2520reflect%2520information%250Aabout%2520surrounding%2520objects.%2520However%252C%2520these%2520studies%2520have%2520failed%2520to%2520reflect%2520the%250Aco-occurrence%2520of%2520objects%2520during%2520SGG%2520generation.%2520In%2520addition%252C%2520they%2520only%250Aaddressed%2520the%2520long-tail%2520problem%2520of%2520the%2520training%2520dataset%2520from%2520the%2520perspectives%250Aof%2520sampling%2520and%2520learning%2520methods.%2520To%2520address%2520these%2520two%2520problems%252C%2520we%2520propose%250ACooK%252C%2520which%2520reflects%2520the%2520Co-occurrence%2520Knowledge%2520between%2520objects%252C%2520and%2520the%250Alearnable%2520term%2520frequency-inverse%2520document%2520frequency%2520%2528TF-l-IDF%2529%2520to%2520solve%2520the%250Along-tail%2520problem.%2520We%2520applied%2520the%2520proposed%2520model%2520to%2520the%2520SGG%2520benchmark%2520dataset%252C%250Aand%2520the%2520results%2520showed%2520a%2520performance%2520improvement%2520of%2520up%2520to%25203.8%2525%2520compared%2520with%250Aexisting%2520state-of-the-art%2520models%2520in%2520SGGen%2520subtask.%2520The%2520proposed%2520method%2520exhibits%250Ageneralization%2520ability%2520from%2520the%2520results%2520obtained%252C%2520showing%2520uniform%2520performance%250Aimprovement%2520for%2520all%2520MPNN%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scene%20Graph%20Generation%20Strategy%20with%20Co-occurrence%20Knowledge%20and%0A%20%20Learnable%20Term%20Frequency&entry.906535625=Hyeongjin%20Kim%20and%20Sangwon%20Kim%20and%20Dasom%20Ahn%20and%20Jong%20Taek%20Lee%20and%20Byoung%20Chul%20Ko&entry.1292438233=%20%20Scene%20graph%20generation%20%28SGG%29%20is%20an%20important%20task%20in%20image%20understanding%0Abecause%20it%20represents%20the%20relationships%20between%20objects%20in%20an%20image%20as%20a%20graph%0Astructure%2C%20making%20it%20possible%20to%20understand%20the%20semantic%20relationships%20between%0Aobjects%20intuitively.%20Previous%20SGG%20studies%20used%20a%20message-passing%20neural%0Anetworks%20%28MPNN%29%20to%20update%20features%2C%20which%20can%20effectively%20reflect%20information%0Aabout%20surrounding%20objects.%20However%2C%20these%20studies%20have%20failed%20to%20reflect%20the%0Aco-occurrence%20of%20objects%20during%20SGG%20generation.%20In%20addition%2C%20they%20only%0Aaddressed%20the%20long-tail%20problem%20of%20the%20training%20dataset%20from%20the%20perspectives%0Aof%20sampling%20and%20learning%20methods.%20To%20address%20these%20two%20problems%2C%20we%20propose%0ACooK%2C%20which%20reflects%20the%20Co-occurrence%20Knowledge%20between%20objects%2C%20and%20the%0Alearnable%20term%20frequency-inverse%20document%20frequency%20%28TF-l-IDF%29%20to%20solve%20the%0Along-tail%20problem.%20We%20applied%20the%20proposed%20model%20to%20the%20SGG%20benchmark%20dataset%2C%0Aand%20the%20results%20showed%20a%20performance%20improvement%20of%20up%20to%203.8%25%20compared%20with%0Aexisting%20state-of-the-art%20models%20in%20SGGen%20subtask.%20The%20proposed%20method%20exhibits%0Ageneralization%20ability%20from%20the%20results%20obtained%2C%20showing%20uniform%20performance%0Aimprovement%20for%20all%20MPNN%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12648v1&entry.124074799=Read"},
{"title": "Inconsistency-Aware Cross-Attention for Audio-Visual Fusion in\n  Dimensional Emotion Recognition", "author": "R Gnana Praveen and Jahangir Alam", "abstract": "  Leveraging complementary relationships across modalities has recently drawn a\nlot of attention in multimodal emotion recognition. Most of the existing\napproaches explored cross-attention to capture the complementary relationships\nacross the modalities. However, the modalities may also exhibit weak\ncomplementary relationships, which may deteriorate the cross-attended features,\nresulting in poor multimodal feature representations. To address this problem,\nwe propose Inconsistency-Aware Cross-Attention (IACA), which can adaptively\nselect the most relevant features on-the-fly based on the strong or weak\ncomplementary relationships across audio and visual modalities. Specifically,\nwe design a two-stage gating mechanism that can adaptively select the\nappropriate relevant features to deal with weak complementary relationships.\nExtensive experiments are conducted on the challenging Aff-Wild2 dataset to\nshow the robustness of the proposed model.\n", "link": "http://arxiv.org/abs/2405.12853v1", "date": "2024-05-21", "relevancy": 2.1242, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5369}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5304}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inconsistency-Aware%20Cross-Attention%20for%20Audio-Visual%20Fusion%20in%0A%20%20Dimensional%20Emotion%20Recognition&body=Title%3A%20Inconsistency-Aware%20Cross-Attention%20for%20Audio-Visual%20Fusion%20in%0A%20%20Dimensional%20Emotion%20Recognition%0AAuthor%3A%20R%20Gnana%20Praveen%20and%20Jahangir%20Alam%0AAbstract%3A%20%20%20Leveraging%20complementary%20relationships%20across%20modalities%20has%20recently%20drawn%20a%0Alot%20of%20attention%20in%20multimodal%20emotion%20recognition.%20Most%20of%20the%20existing%0Aapproaches%20explored%20cross-attention%20to%20capture%20the%20complementary%20relationships%0Aacross%20the%20modalities.%20However%2C%20the%20modalities%20may%20also%20exhibit%20weak%0Acomplementary%20relationships%2C%20which%20may%20deteriorate%20the%20cross-attended%20features%2C%0Aresulting%20in%20poor%20multimodal%20feature%20representations.%20To%20address%20this%20problem%2C%0Awe%20propose%20Inconsistency-Aware%20Cross-Attention%20%28IACA%29%2C%20which%20can%20adaptively%0Aselect%20the%20most%20relevant%20features%20on-the-fly%20based%20on%20the%20strong%20or%20weak%0Acomplementary%20relationships%20across%20audio%20and%20visual%20modalities.%20Specifically%2C%0Awe%20design%20a%20two-stage%20gating%20mechanism%20that%20can%20adaptively%20select%20the%0Aappropriate%20relevant%20features%20to%20deal%20with%20weak%20complementary%20relationships.%0AExtensive%20experiments%20are%20conducted%20on%20the%20challenging%20Aff-Wild2%20dataset%20to%0Ashow%20the%20robustness%20of%20the%20proposed%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInconsistency-Aware%2520Cross-Attention%2520for%2520Audio-Visual%2520Fusion%2520in%250A%2520%2520Dimensional%2520Emotion%2520Recognition%26entry.906535625%3DR%2520Gnana%2520Praveen%2520and%2520Jahangir%2520Alam%26entry.1292438233%3D%2520%2520Leveraging%2520complementary%2520relationships%2520across%2520modalities%2520has%2520recently%2520drawn%2520a%250Alot%2520of%2520attention%2520in%2520multimodal%2520emotion%2520recognition.%2520Most%2520of%2520the%2520existing%250Aapproaches%2520explored%2520cross-attention%2520to%2520capture%2520the%2520complementary%2520relationships%250Aacross%2520the%2520modalities.%2520However%252C%2520the%2520modalities%2520may%2520also%2520exhibit%2520weak%250Acomplementary%2520relationships%252C%2520which%2520may%2520deteriorate%2520the%2520cross-attended%2520features%252C%250Aresulting%2520in%2520poor%2520multimodal%2520feature%2520representations.%2520To%2520address%2520this%2520problem%252C%250Awe%2520propose%2520Inconsistency-Aware%2520Cross-Attention%2520%2528IACA%2529%252C%2520which%2520can%2520adaptively%250Aselect%2520the%2520most%2520relevant%2520features%2520on-the-fly%2520based%2520on%2520the%2520strong%2520or%2520weak%250Acomplementary%2520relationships%2520across%2520audio%2520and%2520visual%2520modalities.%2520Specifically%252C%250Awe%2520design%2520a%2520two-stage%2520gating%2520mechanism%2520that%2520can%2520adaptively%2520select%2520the%250Aappropriate%2520relevant%2520features%2520to%2520deal%2520with%2520weak%2520complementary%2520relationships.%250AExtensive%2520experiments%2520are%2520conducted%2520on%2520the%2520challenging%2520Aff-Wild2%2520dataset%2520to%250Ashow%2520the%2520robustness%2520of%2520the%2520proposed%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inconsistency-Aware%20Cross-Attention%20for%20Audio-Visual%20Fusion%20in%0A%20%20Dimensional%20Emotion%20Recognition&entry.906535625=R%20Gnana%20Praveen%20and%20Jahangir%20Alam&entry.1292438233=%20%20Leveraging%20complementary%20relationships%20across%20modalities%20has%20recently%20drawn%20a%0Alot%20of%20attention%20in%20multimodal%20emotion%20recognition.%20Most%20of%20the%20existing%0Aapproaches%20explored%20cross-attention%20to%20capture%20the%20complementary%20relationships%0Aacross%20the%20modalities.%20However%2C%20the%20modalities%20may%20also%20exhibit%20weak%0Acomplementary%20relationships%2C%20which%20may%20deteriorate%20the%20cross-attended%20features%2C%0Aresulting%20in%20poor%20multimodal%20feature%20representations.%20To%20address%20this%20problem%2C%0Awe%20propose%20Inconsistency-Aware%20Cross-Attention%20%28IACA%29%2C%20which%20can%20adaptively%0Aselect%20the%20most%20relevant%20features%20on-the-fly%20based%20on%20the%20strong%20or%20weak%0Acomplementary%20relationships%20across%20audio%20and%20visual%20modalities.%20Specifically%2C%0Awe%20design%20a%20two-stage%20gating%20mechanism%20that%20can%20adaptively%20select%20the%0Aappropriate%20relevant%20features%20to%20deal%20with%20weak%20complementary%20relationships.%0AExtensive%20experiments%20are%20conducted%20on%20the%20challenging%20Aff-Wild2%20dataset%20to%0Ashow%20the%20robustness%20of%20the%20proposed%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12853v1&entry.124074799=Read"},
{"title": "CoVR: Learning Composed Video Retrieval from Web Video Captions", "author": "Lucas Ventura and Antoine Yang and Cordelia Schmid and G\u00fcl Varol", "abstract": "  Composed Image Retrieval (CoIR) has recently gained popularity as a task that\nconsiders both text and image queries together, to search for relevant images\nin a database. Most CoIR approaches require manually annotated datasets,\ncomprising image-text-image triplets, where the text describes a modification\nfrom the query image to the target image. However, manual curation of CoIR\ntriplets is expensive and prevents scalability. In this work, we instead\npropose a scalable automatic dataset creation methodology that generates\ntriplets given video-caption pairs, while also expanding the scope of the task\nto include composed video retrieval (CoVR). To this end, we mine paired videos\nwith a similar caption from a large database, and leverage a large language\nmodel to generate the corresponding modification text. Applying this\nmethodology to the extensive WebVid2M collection, we automatically construct\nour WebVid-CoVR dataset, resulting in 1.6 million triplets. Moreover, we\nintroduce a new benchmark for CoVR with a manually annotated evaluation set,\nalong with baseline results. Our experiments further demonstrate that training\na CoVR model on our dataset effectively transfers to CoIR, leading to improved\nstate-of-the-art performance in the zero-shot setup on both the CIRR and\nFashionIQ benchmarks. Our code, datasets, and models are publicly available at\nhttps://imagine.enpc.fr/~ventural/covr.\n", "link": "http://arxiv.org/abs/2308.14746v2", "date": "2024-05-21", "relevancy": 2.1235, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5413}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.526}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoVR%3A%20Learning%20Composed%20Video%20Retrieval%20from%20Web%20Video%20Captions&body=Title%3A%20CoVR%3A%20Learning%20Composed%20Video%20Retrieval%20from%20Web%20Video%20Captions%0AAuthor%3A%20Lucas%20Ventura%20and%20Antoine%20Yang%20and%20Cordelia%20Schmid%20and%20G%C3%BCl%20Varol%0AAbstract%3A%20%20%20Composed%20Image%20Retrieval%20%28CoIR%29%20has%20recently%20gained%20popularity%20as%20a%20task%20that%0Aconsiders%20both%20text%20and%20image%20queries%20together%2C%20to%20search%20for%20relevant%20images%0Ain%20a%20database.%20Most%20CoIR%20approaches%20require%20manually%20annotated%20datasets%2C%0Acomprising%20image-text-image%20triplets%2C%20where%20the%20text%20describes%20a%20modification%0Afrom%20the%20query%20image%20to%20the%20target%20image.%20However%2C%20manual%20curation%20of%20CoIR%0Atriplets%20is%20expensive%20and%20prevents%20scalability.%20In%20this%20work%2C%20we%20instead%0Apropose%20a%20scalable%20automatic%20dataset%20creation%20methodology%20that%20generates%0Atriplets%20given%20video-caption%20pairs%2C%20while%20also%20expanding%20the%20scope%20of%20the%20task%0Ato%20include%20composed%20video%20retrieval%20%28CoVR%29.%20To%20this%20end%2C%20we%20mine%20paired%20videos%0Awith%20a%20similar%20caption%20from%20a%20large%20database%2C%20and%20leverage%20a%20large%20language%0Amodel%20to%20generate%20the%20corresponding%20modification%20text.%20Applying%20this%0Amethodology%20to%20the%20extensive%20WebVid2M%20collection%2C%20we%20automatically%20construct%0Aour%20WebVid-CoVR%20dataset%2C%20resulting%20in%201.6%20million%20triplets.%20Moreover%2C%20we%0Aintroduce%20a%20new%20benchmark%20for%20CoVR%20with%20a%20manually%20annotated%20evaluation%20set%2C%0Aalong%20with%20baseline%20results.%20Our%20experiments%20further%20demonstrate%20that%20training%0Aa%20CoVR%20model%20on%20our%20dataset%20effectively%20transfers%20to%20CoIR%2C%20leading%20to%20improved%0Astate-of-the-art%20performance%20in%20the%20zero-shot%20setup%20on%20both%20the%20CIRR%20and%0AFashionIQ%20benchmarks.%20Our%20code%2C%20datasets%2C%20and%20models%20are%20publicly%20available%20at%0Ahttps%3A//imagine.enpc.fr/~ventural/covr.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.14746v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoVR%253A%2520Learning%2520Composed%2520Video%2520Retrieval%2520from%2520Web%2520Video%2520Captions%26entry.906535625%3DLucas%2520Ventura%2520and%2520Antoine%2520Yang%2520and%2520Cordelia%2520Schmid%2520and%2520G%25C3%25BCl%2520Varol%26entry.1292438233%3D%2520%2520Composed%2520Image%2520Retrieval%2520%2528CoIR%2529%2520has%2520recently%2520gained%2520popularity%2520as%2520a%2520task%2520that%250Aconsiders%2520both%2520text%2520and%2520image%2520queries%2520together%252C%2520to%2520search%2520for%2520relevant%2520images%250Ain%2520a%2520database.%2520Most%2520CoIR%2520approaches%2520require%2520manually%2520annotated%2520datasets%252C%250Acomprising%2520image-text-image%2520triplets%252C%2520where%2520the%2520text%2520describes%2520a%2520modification%250Afrom%2520the%2520query%2520image%2520to%2520the%2520target%2520image.%2520However%252C%2520manual%2520curation%2520of%2520CoIR%250Atriplets%2520is%2520expensive%2520and%2520prevents%2520scalability.%2520In%2520this%2520work%252C%2520we%2520instead%250Apropose%2520a%2520scalable%2520automatic%2520dataset%2520creation%2520methodology%2520that%2520generates%250Atriplets%2520given%2520video-caption%2520pairs%252C%2520while%2520also%2520expanding%2520the%2520scope%2520of%2520the%2520task%250Ato%2520include%2520composed%2520video%2520retrieval%2520%2528CoVR%2529.%2520To%2520this%2520end%252C%2520we%2520mine%2520paired%2520videos%250Awith%2520a%2520similar%2520caption%2520from%2520a%2520large%2520database%252C%2520and%2520leverage%2520a%2520large%2520language%250Amodel%2520to%2520generate%2520the%2520corresponding%2520modification%2520text.%2520Applying%2520this%250Amethodology%2520to%2520the%2520extensive%2520WebVid2M%2520collection%252C%2520we%2520automatically%2520construct%250Aour%2520WebVid-CoVR%2520dataset%252C%2520resulting%2520in%25201.6%2520million%2520triplets.%2520Moreover%252C%2520we%250Aintroduce%2520a%2520new%2520benchmark%2520for%2520CoVR%2520with%2520a%2520manually%2520annotated%2520evaluation%2520set%252C%250Aalong%2520with%2520baseline%2520results.%2520Our%2520experiments%2520further%2520demonstrate%2520that%2520training%250Aa%2520CoVR%2520model%2520on%2520our%2520dataset%2520effectively%2520transfers%2520to%2520CoIR%252C%2520leading%2520to%2520improved%250Astate-of-the-art%2520performance%2520in%2520the%2520zero-shot%2520setup%2520on%2520both%2520the%2520CIRR%2520and%250AFashionIQ%2520benchmarks.%2520Our%2520code%252C%2520datasets%252C%2520and%2520models%2520are%2520publicly%2520available%2520at%250Ahttps%253A//imagine.enpc.fr/~ventural/covr.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.14746v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoVR%3A%20Learning%20Composed%20Video%20Retrieval%20from%20Web%20Video%20Captions&entry.906535625=Lucas%20Ventura%20and%20Antoine%20Yang%20and%20Cordelia%20Schmid%20and%20G%C3%BCl%20Varol&entry.1292438233=%20%20Composed%20Image%20Retrieval%20%28CoIR%29%20has%20recently%20gained%20popularity%20as%20a%20task%20that%0Aconsiders%20both%20text%20and%20image%20queries%20together%2C%20to%20search%20for%20relevant%20images%0Ain%20a%20database.%20Most%20CoIR%20approaches%20require%20manually%20annotated%20datasets%2C%0Acomprising%20image-text-image%20triplets%2C%20where%20the%20text%20describes%20a%20modification%0Afrom%20the%20query%20image%20to%20the%20target%20image.%20However%2C%20manual%20curation%20of%20CoIR%0Atriplets%20is%20expensive%20and%20prevents%20scalability.%20In%20this%20work%2C%20we%20instead%0Apropose%20a%20scalable%20automatic%20dataset%20creation%20methodology%20that%20generates%0Atriplets%20given%20video-caption%20pairs%2C%20while%20also%20expanding%20the%20scope%20of%20the%20task%0Ato%20include%20composed%20video%20retrieval%20%28CoVR%29.%20To%20this%20end%2C%20we%20mine%20paired%20videos%0Awith%20a%20similar%20caption%20from%20a%20large%20database%2C%20and%20leverage%20a%20large%20language%0Amodel%20to%20generate%20the%20corresponding%20modification%20text.%20Applying%20this%0Amethodology%20to%20the%20extensive%20WebVid2M%20collection%2C%20we%20automatically%20construct%0Aour%20WebVid-CoVR%20dataset%2C%20resulting%20in%201.6%20million%20triplets.%20Moreover%2C%20we%0Aintroduce%20a%20new%20benchmark%20for%20CoVR%20with%20a%20manually%20annotated%20evaluation%20set%2C%0Aalong%20with%20baseline%20results.%20Our%20experiments%20further%20demonstrate%20that%20training%0Aa%20CoVR%20model%20on%20our%20dataset%20effectively%20transfers%20to%20CoIR%2C%20leading%20to%20improved%0Astate-of-the-art%20performance%20in%20the%20zero-shot%20setup%20on%20both%20the%20CIRR%20and%0AFashionIQ%20benchmarks.%20Our%20code%2C%20datasets%2C%20and%20models%20are%20publicly%20available%20at%0Ahttps%3A//imagine.enpc.fr/~ventural/covr.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.14746v2&entry.124074799=Read"},
{"title": "Improved Content Understanding With Effective Use of Multi-task\n  Contrastive Learning", "author": "Akanksha Bindal and Sudarshan Ramanujam and Dave Golland and TJ Hazen and Tina Jiang and Fengyu Zhang and Peng Yan", "abstract": "  In enhancing LinkedIn core content recommendation models, a significant\nchallenge lies in improving their semantic understanding capabilities. This\npaper addresses the problem by leveraging multi-task learning, a method that\nhas shown promise in various domains. We fine-tune a pre-trained,\ntransformer-based LLM using multi-task contrastive learning with data from a\ndiverse set of semantic labeling tasks. We observe positive transfer, leading\nto superior performance across all tasks when compared to training\nindependently on each. Our model outperforms the baseline on zero shot learning\nand offers improved multilingual support, highlighting its potential for\nbroader application. The specialized content embeddings produced by our model\noutperform generalized embeddings offered by OpenAI on Linkedin dataset and\ntasks. This work provides a robust foundation for vertical teams across\nLinkedIn to customize and fine-tune the LLM to their specific applications. Our\nwork offers insights and best practices for the field to build on.\n", "link": "http://arxiv.org/abs/2405.11344v2", "date": "2024-05-21", "relevancy": 2.1059, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5705}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4956}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Content%20Understanding%20With%20Effective%20Use%20of%20Multi-task%0A%20%20Contrastive%20Learning&body=Title%3A%20Improved%20Content%20Understanding%20With%20Effective%20Use%20of%20Multi-task%0A%20%20Contrastive%20Learning%0AAuthor%3A%20Akanksha%20Bindal%20and%20Sudarshan%20Ramanujam%20and%20Dave%20Golland%20and%20TJ%20Hazen%20and%20Tina%20Jiang%20and%20Fengyu%20Zhang%20and%20Peng%20Yan%0AAbstract%3A%20%20%20In%20enhancing%20LinkedIn%20core%20content%20recommendation%20models%2C%20a%20significant%0Achallenge%20lies%20in%20improving%20their%20semantic%20understanding%20capabilities.%20This%0Apaper%20addresses%20the%20problem%20by%20leveraging%20multi-task%20learning%2C%20a%20method%20that%0Ahas%20shown%20promise%20in%20various%20domains.%20We%20fine-tune%20a%20pre-trained%2C%0Atransformer-based%20LLM%20using%20multi-task%20contrastive%20learning%20with%20data%20from%20a%0Adiverse%20set%20of%20semantic%20labeling%20tasks.%20We%20observe%20positive%20transfer%2C%20leading%0Ato%20superior%20performance%20across%20all%20tasks%20when%20compared%20to%20training%0Aindependently%20on%20each.%20Our%20model%20outperforms%20the%20baseline%20on%20zero%20shot%20learning%0Aand%20offers%20improved%20multilingual%20support%2C%20highlighting%20its%20potential%20for%0Abroader%20application.%20The%20specialized%20content%20embeddings%20produced%20by%20our%20model%0Aoutperform%20generalized%20embeddings%20offered%20by%20OpenAI%20on%20Linkedin%20dataset%20and%0Atasks.%20This%20work%20provides%20a%20robust%20foundation%20for%20vertical%20teams%20across%0ALinkedIn%20to%20customize%20and%20fine-tune%20the%20LLM%20to%20their%20specific%20applications.%20Our%0Awork%20offers%20insights%20and%20best%20practices%20for%20the%20field%20to%20build%20on.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11344v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Content%2520Understanding%2520With%2520Effective%2520Use%2520of%2520Multi-task%250A%2520%2520Contrastive%2520Learning%26entry.906535625%3DAkanksha%2520Bindal%2520and%2520Sudarshan%2520Ramanujam%2520and%2520Dave%2520Golland%2520and%2520TJ%2520Hazen%2520and%2520Tina%2520Jiang%2520and%2520Fengyu%2520Zhang%2520and%2520Peng%2520Yan%26entry.1292438233%3D%2520%2520In%2520enhancing%2520LinkedIn%2520core%2520content%2520recommendation%2520models%252C%2520a%2520significant%250Achallenge%2520lies%2520in%2520improving%2520their%2520semantic%2520understanding%2520capabilities.%2520This%250Apaper%2520addresses%2520the%2520problem%2520by%2520leveraging%2520multi-task%2520learning%252C%2520a%2520method%2520that%250Ahas%2520shown%2520promise%2520in%2520various%2520domains.%2520We%2520fine-tune%2520a%2520pre-trained%252C%250Atransformer-based%2520LLM%2520using%2520multi-task%2520contrastive%2520learning%2520with%2520data%2520from%2520a%250Adiverse%2520set%2520of%2520semantic%2520labeling%2520tasks.%2520We%2520observe%2520positive%2520transfer%252C%2520leading%250Ato%2520superior%2520performance%2520across%2520all%2520tasks%2520when%2520compared%2520to%2520training%250Aindependently%2520on%2520each.%2520Our%2520model%2520outperforms%2520the%2520baseline%2520on%2520zero%2520shot%2520learning%250Aand%2520offers%2520improved%2520multilingual%2520support%252C%2520highlighting%2520its%2520potential%2520for%250Abroader%2520application.%2520The%2520specialized%2520content%2520embeddings%2520produced%2520by%2520our%2520model%250Aoutperform%2520generalized%2520embeddings%2520offered%2520by%2520OpenAI%2520on%2520Linkedin%2520dataset%2520and%250Atasks.%2520This%2520work%2520provides%2520a%2520robust%2520foundation%2520for%2520vertical%2520teams%2520across%250ALinkedIn%2520to%2520customize%2520and%2520fine-tune%2520the%2520LLM%2520to%2520their%2520specific%2520applications.%2520Our%250Awork%2520offers%2520insights%2520and%2520best%2520practices%2520for%2520the%2520field%2520to%2520build%2520on.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11344v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Content%20Understanding%20With%20Effective%20Use%20of%20Multi-task%0A%20%20Contrastive%20Learning&entry.906535625=Akanksha%20Bindal%20and%20Sudarshan%20Ramanujam%20and%20Dave%20Golland%20and%20TJ%20Hazen%20and%20Tina%20Jiang%20and%20Fengyu%20Zhang%20and%20Peng%20Yan&entry.1292438233=%20%20In%20enhancing%20LinkedIn%20core%20content%20recommendation%20models%2C%20a%20significant%0Achallenge%20lies%20in%20improving%20their%20semantic%20understanding%20capabilities.%20This%0Apaper%20addresses%20the%20problem%20by%20leveraging%20multi-task%20learning%2C%20a%20method%20that%0Ahas%20shown%20promise%20in%20various%20domains.%20We%20fine-tune%20a%20pre-trained%2C%0Atransformer-based%20LLM%20using%20multi-task%20contrastive%20learning%20with%20data%20from%20a%0Adiverse%20set%20of%20semantic%20labeling%20tasks.%20We%20observe%20positive%20transfer%2C%20leading%0Ato%20superior%20performance%20across%20all%20tasks%20when%20compared%20to%20training%0Aindependently%20on%20each.%20Our%20model%20outperforms%20the%20baseline%20on%20zero%20shot%20learning%0Aand%20offers%20improved%20multilingual%20support%2C%20highlighting%20its%20potential%20for%0Abroader%20application.%20The%20specialized%20content%20embeddings%20produced%20by%20our%20model%0Aoutperform%20generalized%20embeddings%20offered%20by%20OpenAI%20on%20Linkedin%20dataset%20and%0Atasks.%20This%20work%20provides%20a%20robust%20foundation%20for%20vertical%20teams%20across%0ALinkedIn%20to%20customize%20and%20fine-tune%20the%20LLM%20to%20their%20specific%20applications.%20Our%0Awork%20offers%20insights%20and%20best%20practices%20for%20the%20field%20to%20build%20on.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11344v2&entry.124074799=Read"},
{"title": "Adaptive local boundary conditions to improve Deformable Image\n  Registration", "author": "Elo\u00efse Inacio and Luc Lafitte and Laurent Facq and Clair Poignard and Baudouin Denis de Senneville", "abstract": "  Objective: In medical imaging, it is often crucial to accurately assess and\ncorrect movement during image-guided therapy. Deformable image registration\n(DIR) consists in estimating the required spatial transformation to align a\nmoving image with a fixed one. However, it is acknowledged that, boundary\nconditions applied to the solution are critical in preventing mis-registration.\nDespite the extensive research on registration techniques, relatively few have\naddressed the issue of boundary conditions in the context of medical DIR. Our\naim is a step towards customizing boundary conditions to suit the diverse\nregistration tasks at hand.\n  Approach: We propose a generic, locally adaptive, Robin-type condition\nenabling to balance between Dirichlet and Neumann boundary conditions,\ndepending on incoming/outgoing flow fields on the image boundaries. The\nproposed framework is entirely automatized through the determination of a\nreduced set of hyperparameters optimized via energy minimization.\n  Main results: The proposed approach was tested on a mono-modal CT thorax\nregistration task and an abdominal CT to MRI registration task. For the first\ntask, we observed a relative improvement in terms of target registration error\nof up to 12% (mean 4%), compared to homogeneous Dirichlet and homogeneous\nNeumann. For the second task, the automatic framework provides results closed\nto the best achievable.\n  Significance: This study underscores the importance of tailoring the\nregistration problem at the image boundaries. In this research, we introduce a\nnovel method to adapt the boundary conditions on a voxel-by-voxel basis,\nyielding optimized results in two distinct tasks: mono-modal CT thorax\nregistration and abdominal CT to MRI registration. The proposed framework\nenables optimized boundary conditions in image registration without any a\npriori assumptions regarding the images or the motion.\n", "link": "http://arxiv.org/abs/2405.12791v1", "date": "2024-05-21", "relevancy": 2.101, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5343}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5235}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20local%20boundary%20conditions%20to%20improve%20Deformable%20Image%0A%20%20Registration&body=Title%3A%20Adaptive%20local%20boundary%20conditions%20to%20improve%20Deformable%20Image%0A%20%20Registration%0AAuthor%3A%20Elo%C3%AFse%20Inacio%20and%20Luc%20Lafitte%20and%20Laurent%20Facq%20and%20Clair%20Poignard%20and%20Baudouin%20Denis%20de%20Senneville%0AAbstract%3A%20%20%20Objective%3A%20In%20medical%20imaging%2C%20it%20is%20often%20crucial%20to%20accurately%20assess%20and%0Acorrect%20movement%20during%20image-guided%20therapy.%20Deformable%20image%20registration%0A%28DIR%29%20consists%20in%20estimating%20the%20required%20spatial%20transformation%20to%20align%20a%0Amoving%20image%20with%20a%20fixed%20one.%20However%2C%20it%20is%20acknowledged%20that%2C%20boundary%0Aconditions%20applied%20to%20the%20solution%20are%20critical%20in%20preventing%20mis-registration.%0ADespite%20the%20extensive%20research%20on%20registration%20techniques%2C%20relatively%20few%20have%0Aaddressed%20the%20issue%20of%20boundary%20conditions%20in%20the%20context%20of%20medical%20DIR.%20Our%0Aaim%20is%20a%20step%20towards%20customizing%20boundary%20conditions%20to%20suit%20the%20diverse%0Aregistration%20tasks%20at%20hand.%0A%20%20Approach%3A%20We%20propose%20a%20generic%2C%20locally%20adaptive%2C%20Robin-type%20condition%0Aenabling%20to%20balance%20between%20Dirichlet%20and%20Neumann%20boundary%20conditions%2C%0Adepending%20on%20incoming/outgoing%20flow%20fields%20on%20the%20image%20boundaries.%20The%0Aproposed%20framework%20is%20entirely%20automatized%20through%20the%20determination%20of%20a%0Areduced%20set%20of%20hyperparameters%20optimized%20via%20energy%20minimization.%0A%20%20Main%20results%3A%20The%20proposed%20approach%20was%20tested%20on%20a%20mono-modal%20CT%20thorax%0Aregistration%20task%20and%20an%20abdominal%20CT%20to%20MRI%20registration%20task.%20For%20the%20first%0Atask%2C%20we%20observed%20a%20relative%20improvement%20in%20terms%20of%20target%20registration%20error%0Aof%20up%20to%2012%25%20%28mean%204%25%29%2C%20compared%20to%20homogeneous%20Dirichlet%20and%20homogeneous%0ANeumann.%20For%20the%20second%20task%2C%20the%20automatic%20framework%20provides%20results%20closed%0Ato%20the%20best%20achievable.%0A%20%20Significance%3A%20This%20study%20underscores%20the%20importance%20of%20tailoring%20the%0Aregistration%20problem%20at%20the%20image%20boundaries.%20In%20this%20research%2C%20we%20introduce%20a%0Anovel%20method%20to%20adapt%20the%20boundary%20conditions%20on%20a%20voxel-by-voxel%20basis%2C%0Ayielding%20optimized%20results%20in%20two%20distinct%20tasks%3A%20mono-modal%20CT%20thorax%0Aregistration%20and%20abdominal%20CT%20to%20MRI%20registration.%20The%20proposed%20framework%0Aenables%20optimized%20boundary%20conditions%20in%20image%20registration%20without%20any%20a%0Apriori%20assumptions%20regarding%20the%20images%20or%20the%20motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520local%2520boundary%2520conditions%2520to%2520improve%2520Deformable%2520Image%250A%2520%2520Registration%26entry.906535625%3DElo%25C3%25AFse%2520Inacio%2520and%2520Luc%2520Lafitte%2520and%2520Laurent%2520Facq%2520and%2520Clair%2520Poignard%2520and%2520Baudouin%2520Denis%2520de%2520Senneville%26entry.1292438233%3D%2520%2520Objective%253A%2520In%2520medical%2520imaging%252C%2520it%2520is%2520often%2520crucial%2520to%2520accurately%2520assess%2520and%250Acorrect%2520movement%2520during%2520image-guided%2520therapy.%2520Deformable%2520image%2520registration%250A%2528DIR%2529%2520consists%2520in%2520estimating%2520the%2520required%2520spatial%2520transformation%2520to%2520align%2520a%250Amoving%2520image%2520with%2520a%2520fixed%2520one.%2520However%252C%2520it%2520is%2520acknowledged%2520that%252C%2520boundary%250Aconditions%2520applied%2520to%2520the%2520solution%2520are%2520critical%2520in%2520preventing%2520mis-registration.%250ADespite%2520the%2520extensive%2520research%2520on%2520registration%2520techniques%252C%2520relatively%2520few%2520have%250Aaddressed%2520the%2520issue%2520of%2520boundary%2520conditions%2520in%2520the%2520context%2520of%2520medical%2520DIR.%2520Our%250Aaim%2520is%2520a%2520step%2520towards%2520customizing%2520boundary%2520conditions%2520to%2520suit%2520the%2520diverse%250Aregistration%2520tasks%2520at%2520hand.%250A%2520%2520Approach%253A%2520We%2520propose%2520a%2520generic%252C%2520locally%2520adaptive%252C%2520Robin-type%2520condition%250Aenabling%2520to%2520balance%2520between%2520Dirichlet%2520and%2520Neumann%2520boundary%2520conditions%252C%250Adepending%2520on%2520incoming/outgoing%2520flow%2520fields%2520on%2520the%2520image%2520boundaries.%2520The%250Aproposed%2520framework%2520is%2520entirely%2520automatized%2520through%2520the%2520determination%2520of%2520a%250Areduced%2520set%2520of%2520hyperparameters%2520optimized%2520via%2520energy%2520minimization.%250A%2520%2520Main%2520results%253A%2520The%2520proposed%2520approach%2520was%2520tested%2520on%2520a%2520mono-modal%2520CT%2520thorax%250Aregistration%2520task%2520and%2520an%2520abdominal%2520CT%2520to%2520MRI%2520registration%2520task.%2520For%2520the%2520first%250Atask%252C%2520we%2520observed%2520a%2520relative%2520improvement%2520in%2520terms%2520of%2520target%2520registration%2520error%250Aof%2520up%2520to%252012%2525%2520%2528mean%25204%2525%2529%252C%2520compared%2520to%2520homogeneous%2520Dirichlet%2520and%2520homogeneous%250ANeumann.%2520For%2520the%2520second%2520task%252C%2520the%2520automatic%2520framework%2520provides%2520results%2520closed%250Ato%2520the%2520best%2520achievable.%250A%2520%2520Significance%253A%2520This%2520study%2520underscores%2520the%2520importance%2520of%2520tailoring%2520the%250Aregistration%2520problem%2520at%2520the%2520image%2520boundaries.%2520In%2520this%2520research%252C%2520we%2520introduce%2520a%250Anovel%2520method%2520to%2520adapt%2520the%2520boundary%2520conditions%2520on%2520a%2520voxel-by-voxel%2520basis%252C%250Ayielding%2520optimized%2520results%2520in%2520two%2520distinct%2520tasks%253A%2520mono-modal%2520CT%2520thorax%250Aregistration%2520and%2520abdominal%2520CT%2520to%2520MRI%2520registration.%2520The%2520proposed%2520framework%250Aenables%2520optimized%2520boundary%2520conditions%2520in%2520image%2520registration%2520without%2520any%2520a%250Apriori%2520assumptions%2520regarding%2520the%2520images%2520or%2520the%2520motion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20local%20boundary%20conditions%20to%20improve%20Deformable%20Image%0A%20%20Registration&entry.906535625=Elo%C3%AFse%20Inacio%20and%20Luc%20Lafitte%20and%20Laurent%20Facq%20and%20Clair%20Poignard%20and%20Baudouin%20Denis%20de%20Senneville&entry.1292438233=%20%20Objective%3A%20In%20medical%20imaging%2C%20it%20is%20often%20crucial%20to%20accurately%20assess%20and%0Acorrect%20movement%20during%20image-guided%20therapy.%20Deformable%20image%20registration%0A%28DIR%29%20consists%20in%20estimating%20the%20required%20spatial%20transformation%20to%20align%20a%0Amoving%20image%20with%20a%20fixed%20one.%20However%2C%20it%20is%20acknowledged%20that%2C%20boundary%0Aconditions%20applied%20to%20the%20solution%20are%20critical%20in%20preventing%20mis-registration.%0ADespite%20the%20extensive%20research%20on%20registration%20techniques%2C%20relatively%20few%20have%0Aaddressed%20the%20issue%20of%20boundary%20conditions%20in%20the%20context%20of%20medical%20DIR.%20Our%0Aaim%20is%20a%20step%20towards%20customizing%20boundary%20conditions%20to%20suit%20the%20diverse%0Aregistration%20tasks%20at%20hand.%0A%20%20Approach%3A%20We%20propose%20a%20generic%2C%20locally%20adaptive%2C%20Robin-type%20condition%0Aenabling%20to%20balance%20between%20Dirichlet%20and%20Neumann%20boundary%20conditions%2C%0Adepending%20on%20incoming/outgoing%20flow%20fields%20on%20the%20image%20boundaries.%20The%0Aproposed%20framework%20is%20entirely%20automatized%20through%20the%20determination%20of%20a%0Areduced%20set%20of%20hyperparameters%20optimized%20via%20energy%20minimization.%0A%20%20Main%20results%3A%20The%20proposed%20approach%20was%20tested%20on%20a%20mono-modal%20CT%20thorax%0Aregistration%20task%20and%20an%20abdominal%20CT%20to%20MRI%20registration%20task.%20For%20the%20first%0Atask%2C%20we%20observed%20a%20relative%20improvement%20in%20terms%20of%20target%20registration%20error%0Aof%20up%20to%2012%25%20%28mean%204%25%29%2C%20compared%20to%20homogeneous%20Dirichlet%20and%20homogeneous%0ANeumann.%20For%20the%20second%20task%2C%20the%20automatic%20framework%20provides%20results%20closed%0Ato%20the%20best%20achievable.%0A%20%20Significance%3A%20This%20study%20underscores%20the%20importance%20of%20tailoring%20the%0Aregistration%20problem%20at%20the%20image%20boundaries.%20In%20this%20research%2C%20we%20introduce%20a%0Anovel%20method%20to%20adapt%20the%20boundary%20conditions%20on%20a%20voxel-by-voxel%20basis%2C%0Ayielding%20optimized%20results%20in%20two%20distinct%20tasks%3A%20mono-modal%20CT%20thorax%0Aregistration%20and%20abdominal%20CT%20to%20MRI%20registration.%20The%20proposed%20framework%0Aenables%20optimized%20boundary%20conditions%20in%20image%20registration%20without%20any%20a%0Apriori%20assumptions%20regarding%20the%20images%20or%20the%20motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12791v1&entry.124074799=Read"},
{"title": "Frequency-Adaptive Dilated Convolution for Semantic Segmentation", "author": "Linwei Chen and Lin Gu and Ying Fu", "abstract": "  Dilated convolution, which expands the receptive field by inserting gaps\nbetween its consecutive elements, is widely employed in computer vision. In\nthis study, we propose three strategies to improve individual phases of dilated\nconvolution from the view of spectrum analysis. Departing from the conventional\npractice of fixing a global dilation rate as a hyperparameter, we introduce\nFrequency-Adaptive Dilated Convolution (FADC), which dynamically adjusts\ndilation rates spatially based on local frequency components. Subsequently, we\ndesign two plug-in modules to directly enhance effective bandwidth and\nreceptive field size. The Adaptive Kernel (AdaKern) module decomposes\nconvolution weights into low-frequency and high-frequency components,\ndynamically adjusting the ratio between these components on a per-channel\nbasis. By increasing the high-frequency part of convolution weights, AdaKern\ncaptures more high-frequency components, thereby improving effective bandwidth.\nThe Frequency Selection (FreqSelect) module optimally balances high- and\nlow-frequency components in feature representations through spatially variant\nreweighting. It suppresses high frequencies in the background to encourage FADC\nto learn a larger dilation, thereby increasing the receptive field for an\nexpanded scope. Extensive experiments on segmentation and object detection\nconsistently validate the efficacy of our approach. The code is publicly\navailable at https://github.com/Linwei-Chen/FADC.\n", "link": "http://arxiv.org/abs/2403.05369v6", "date": "2024-05-21", "relevancy": 2.0941, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5367}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5329}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frequency-Adaptive%20Dilated%20Convolution%20for%20Semantic%20Segmentation&body=Title%3A%20Frequency-Adaptive%20Dilated%20Convolution%20for%20Semantic%20Segmentation%0AAuthor%3A%20Linwei%20Chen%20and%20Lin%20Gu%20and%20Ying%20Fu%0AAbstract%3A%20%20%20Dilated%20convolution%2C%20which%20expands%20the%20receptive%20field%20by%20inserting%20gaps%0Abetween%20its%20consecutive%20elements%2C%20is%20widely%20employed%20in%20computer%20vision.%20In%0Athis%20study%2C%20we%20propose%20three%20strategies%20to%20improve%20individual%20phases%20of%20dilated%0Aconvolution%20from%20the%20view%20of%20spectrum%20analysis.%20Departing%20from%20the%20conventional%0Apractice%20of%20fixing%20a%20global%20dilation%20rate%20as%20a%20hyperparameter%2C%20we%20introduce%0AFrequency-Adaptive%20Dilated%20Convolution%20%28FADC%29%2C%20which%20dynamically%20adjusts%0Adilation%20rates%20spatially%20based%20on%20local%20frequency%20components.%20Subsequently%2C%20we%0Adesign%20two%20plug-in%20modules%20to%20directly%20enhance%20effective%20bandwidth%20and%0Areceptive%20field%20size.%20The%20Adaptive%20Kernel%20%28AdaKern%29%20module%20decomposes%0Aconvolution%20weights%20into%20low-frequency%20and%20high-frequency%20components%2C%0Adynamically%20adjusting%20the%20ratio%20between%20these%20components%20on%20a%20per-channel%0Abasis.%20By%20increasing%20the%20high-frequency%20part%20of%20convolution%20weights%2C%20AdaKern%0Acaptures%20more%20high-frequency%20components%2C%20thereby%20improving%20effective%20bandwidth.%0AThe%20Frequency%20Selection%20%28FreqSelect%29%20module%20optimally%20balances%20high-%20and%0Alow-frequency%20components%20in%20feature%20representations%20through%20spatially%20variant%0Areweighting.%20It%20suppresses%20high%20frequencies%20in%20the%20background%20to%20encourage%20FADC%0Ato%20learn%20a%20larger%20dilation%2C%20thereby%20increasing%20the%20receptive%20field%20for%20an%0Aexpanded%20scope.%20Extensive%20experiments%20on%20segmentation%20and%20object%20detection%0Aconsistently%20validate%20the%20efficacy%20of%20our%20approach.%20The%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/Linwei-Chen/FADC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05369v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrequency-Adaptive%2520Dilated%2520Convolution%2520for%2520Semantic%2520Segmentation%26entry.906535625%3DLinwei%2520Chen%2520and%2520Lin%2520Gu%2520and%2520Ying%2520Fu%26entry.1292438233%3D%2520%2520Dilated%2520convolution%252C%2520which%2520expands%2520the%2520receptive%2520field%2520by%2520inserting%2520gaps%250Abetween%2520its%2520consecutive%2520elements%252C%2520is%2520widely%2520employed%2520in%2520computer%2520vision.%2520In%250Athis%2520study%252C%2520we%2520propose%2520three%2520strategies%2520to%2520improve%2520individual%2520phases%2520of%2520dilated%250Aconvolution%2520from%2520the%2520view%2520of%2520spectrum%2520analysis.%2520Departing%2520from%2520the%2520conventional%250Apractice%2520of%2520fixing%2520a%2520global%2520dilation%2520rate%2520as%2520a%2520hyperparameter%252C%2520we%2520introduce%250AFrequency-Adaptive%2520Dilated%2520Convolution%2520%2528FADC%2529%252C%2520which%2520dynamically%2520adjusts%250Adilation%2520rates%2520spatially%2520based%2520on%2520local%2520frequency%2520components.%2520Subsequently%252C%2520we%250Adesign%2520two%2520plug-in%2520modules%2520to%2520directly%2520enhance%2520effective%2520bandwidth%2520and%250Areceptive%2520field%2520size.%2520The%2520Adaptive%2520Kernel%2520%2528AdaKern%2529%2520module%2520decomposes%250Aconvolution%2520weights%2520into%2520low-frequency%2520and%2520high-frequency%2520components%252C%250Adynamically%2520adjusting%2520the%2520ratio%2520between%2520these%2520components%2520on%2520a%2520per-channel%250Abasis.%2520By%2520increasing%2520the%2520high-frequency%2520part%2520of%2520convolution%2520weights%252C%2520AdaKern%250Acaptures%2520more%2520high-frequency%2520components%252C%2520thereby%2520improving%2520effective%2520bandwidth.%250AThe%2520Frequency%2520Selection%2520%2528FreqSelect%2529%2520module%2520optimally%2520balances%2520high-%2520and%250Alow-frequency%2520components%2520in%2520feature%2520representations%2520through%2520spatially%2520variant%250Areweighting.%2520It%2520suppresses%2520high%2520frequencies%2520in%2520the%2520background%2520to%2520encourage%2520FADC%250Ato%2520learn%2520a%2520larger%2520dilation%252C%2520thereby%2520increasing%2520the%2520receptive%2520field%2520for%2520an%250Aexpanded%2520scope.%2520Extensive%2520experiments%2520on%2520segmentation%2520and%2520object%2520detection%250Aconsistently%2520validate%2520the%2520efficacy%2520of%2520our%2520approach.%2520The%2520code%2520is%2520publicly%250Aavailable%2520at%2520https%253A//github.com/Linwei-Chen/FADC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05369v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frequency-Adaptive%20Dilated%20Convolution%20for%20Semantic%20Segmentation&entry.906535625=Linwei%20Chen%20and%20Lin%20Gu%20and%20Ying%20Fu&entry.1292438233=%20%20Dilated%20convolution%2C%20which%20expands%20the%20receptive%20field%20by%20inserting%20gaps%0Abetween%20its%20consecutive%20elements%2C%20is%20widely%20employed%20in%20computer%20vision.%20In%0Athis%20study%2C%20we%20propose%20three%20strategies%20to%20improve%20individual%20phases%20of%20dilated%0Aconvolution%20from%20the%20view%20of%20spectrum%20analysis.%20Departing%20from%20the%20conventional%0Apractice%20of%20fixing%20a%20global%20dilation%20rate%20as%20a%20hyperparameter%2C%20we%20introduce%0AFrequency-Adaptive%20Dilated%20Convolution%20%28FADC%29%2C%20which%20dynamically%20adjusts%0Adilation%20rates%20spatially%20based%20on%20local%20frequency%20components.%20Subsequently%2C%20we%0Adesign%20two%20plug-in%20modules%20to%20directly%20enhance%20effective%20bandwidth%20and%0Areceptive%20field%20size.%20The%20Adaptive%20Kernel%20%28AdaKern%29%20module%20decomposes%0Aconvolution%20weights%20into%20low-frequency%20and%20high-frequency%20components%2C%0Adynamically%20adjusting%20the%20ratio%20between%20these%20components%20on%20a%20per-channel%0Abasis.%20By%20increasing%20the%20high-frequency%20part%20of%20convolution%20weights%2C%20AdaKern%0Acaptures%20more%20high-frequency%20components%2C%20thereby%20improving%20effective%20bandwidth.%0AThe%20Frequency%20Selection%20%28FreqSelect%29%20module%20optimally%20balances%20high-%20and%0Alow-frequency%20components%20in%20feature%20representations%20through%20spatially%20variant%0Areweighting.%20It%20suppresses%20high%20frequencies%20in%20the%20background%20to%20encourage%20FADC%0Ato%20learn%20a%20larger%20dilation%2C%20thereby%20increasing%20the%20receptive%20field%20for%20an%0Aexpanded%20scope.%20Extensive%20experiments%20on%20segmentation%20and%20object%20detection%0Aconsistently%20validate%20the%20efficacy%20of%20our%20approach.%20The%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/Linwei-Chen/FADC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05369v6&entry.124074799=Read"},
{"title": "RISAM: Referring Image Segmentation via Mutual-Aware Attention Features", "author": "Mengxi Zhang and Yiming Liu and Xiangjun Yin and Huanjing Yue and Jingyu Yang", "abstract": "  Referring image segmentation (RIS) aims to segment a particular region based\non a language expression prompt. Existing methods incorporate linguistic\nfeatures into visual features and obtain multi-modal features for mask\ndecoding. However, these methods may segment the visually salient entity\ninstead of the correct referring region, as the multi-modal features are\ndominated by the abundant visual context. In this paper, we propose MARIS, a\nreferring image segmentation method that leverages the Segment Anything Model\n(SAM) and introduces a mutual-aware attention mechanism to enhance the\ncross-modal fusion via two parallel branches. Specifically, our mutual-aware\nattention mechanism consists of Vision-Guided Attention and Language-Guided\nAttention, which bidirectionally model the relationship between visual and\nlinguistic features. Correspondingly, we design a Mask Decoder to enable\nexplicit linguistic guidance for more consistent segmentation with the language\nexpression. To this end, a multi-modal query token is proposed to integrate\nlinguistic information and interact with visual information simultaneously.\nExtensive experiments on three benchmark datasets show that our method\noutperforms the state-of-the-art RIS methods. Our code will be publicly\navailable.\n", "link": "http://arxiv.org/abs/2311.15727v4", "date": "2024-05-21", "relevancy": 2.093, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5502}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5069}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RISAM%3A%20Referring%20Image%20Segmentation%20via%20Mutual-Aware%20Attention%20Features&body=Title%3A%20RISAM%3A%20Referring%20Image%20Segmentation%20via%20Mutual-Aware%20Attention%20Features%0AAuthor%3A%20Mengxi%20Zhang%20and%20Yiming%20Liu%20and%20Xiangjun%20Yin%20and%20Huanjing%20Yue%20and%20Jingyu%20Yang%0AAbstract%3A%20%20%20Referring%20image%20segmentation%20%28RIS%29%20aims%20to%20segment%20a%20particular%20region%20based%0Aon%20a%20language%20expression%20prompt.%20Existing%20methods%20incorporate%20linguistic%0Afeatures%20into%20visual%20features%20and%20obtain%20multi-modal%20features%20for%20mask%0Adecoding.%20However%2C%20these%20methods%20may%20segment%20the%20visually%20salient%20entity%0Ainstead%20of%20the%20correct%20referring%20region%2C%20as%20the%20multi-modal%20features%20are%0Adominated%20by%20the%20abundant%20visual%20context.%20In%20this%20paper%2C%20we%20propose%20MARIS%2C%20a%0Areferring%20image%20segmentation%20method%20that%20leverages%20the%20Segment%20Anything%20Model%0A%28SAM%29%20and%20introduces%20a%20mutual-aware%20attention%20mechanism%20to%20enhance%20the%0Across-modal%20fusion%20via%20two%20parallel%20branches.%20Specifically%2C%20our%20mutual-aware%0Aattention%20mechanism%20consists%20of%20Vision-Guided%20Attention%20and%20Language-Guided%0AAttention%2C%20which%20bidirectionally%20model%20the%20relationship%20between%20visual%20and%0Alinguistic%20features.%20Correspondingly%2C%20we%20design%20a%20Mask%20Decoder%20to%20enable%0Aexplicit%20linguistic%20guidance%20for%20more%20consistent%20segmentation%20with%20the%20language%0Aexpression.%20To%20this%20end%2C%20a%20multi-modal%20query%20token%20is%20proposed%20to%20integrate%0Alinguistic%20information%20and%20interact%20with%20visual%20information%20simultaneously.%0AExtensive%20experiments%20on%20three%20benchmark%20datasets%20show%20that%20our%20method%0Aoutperforms%20the%20state-of-the-art%20RIS%20methods.%20Our%20code%20will%20be%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15727v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRISAM%253A%2520Referring%2520Image%2520Segmentation%2520via%2520Mutual-Aware%2520Attention%2520Features%26entry.906535625%3DMengxi%2520Zhang%2520and%2520Yiming%2520Liu%2520and%2520Xiangjun%2520Yin%2520and%2520Huanjing%2520Yue%2520and%2520Jingyu%2520Yang%26entry.1292438233%3D%2520%2520Referring%2520image%2520segmentation%2520%2528RIS%2529%2520aims%2520to%2520segment%2520a%2520particular%2520region%2520based%250Aon%2520a%2520language%2520expression%2520prompt.%2520Existing%2520methods%2520incorporate%2520linguistic%250Afeatures%2520into%2520visual%2520features%2520and%2520obtain%2520multi-modal%2520features%2520for%2520mask%250Adecoding.%2520However%252C%2520these%2520methods%2520may%2520segment%2520the%2520visually%2520salient%2520entity%250Ainstead%2520of%2520the%2520correct%2520referring%2520region%252C%2520as%2520the%2520multi-modal%2520features%2520are%250Adominated%2520by%2520the%2520abundant%2520visual%2520context.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MARIS%252C%2520a%250Areferring%2520image%2520segmentation%2520method%2520that%2520leverages%2520the%2520Segment%2520Anything%2520Model%250A%2528SAM%2529%2520and%2520introduces%2520a%2520mutual-aware%2520attention%2520mechanism%2520to%2520enhance%2520the%250Across-modal%2520fusion%2520via%2520two%2520parallel%2520branches.%2520Specifically%252C%2520our%2520mutual-aware%250Aattention%2520mechanism%2520consists%2520of%2520Vision-Guided%2520Attention%2520and%2520Language-Guided%250AAttention%252C%2520which%2520bidirectionally%2520model%2520the%2520relationship%2520between%2520visual%2520and%250Alinguistic%2520features.%2520Correspondingly%252C%2520we%2520design%2520a%2520Mask%2520Decoder%2520to%2520enable%250Aexplicit%2520linguistic%2520guidance%2520for%2520more%2520consistent%2520segmentation%2520with%2520the%2520language%250Aexpression.%2520To%2520this%2520end%252C%2520a%2520multi-modal%2520query%2520token%2520is%2520proposed%2520to%2520integrate%250Alinguistic%2520information%2520and%2520interact%2520with%2520visual%2520information%2520simultaneously.%250AExtensive%2520experiments%2520on%2520three%2520benchmark%2520datasets%2520show%2520that%2520our%2520method%250Aoutperforms%2520the%2520state-of-the-art%2520RIS%2520methods.%2520Our%2520code%2520will%2520be%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.15727v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RISAM%3A%20Referring%20Image%20Segmentation%20via%20Mutual-Aware%20Attention%20Features&entry.906535625=Mengxi%20Zhang%20and%20Yiming%20Liu%20and%20Xiangjun%20Yin%20and%20Huanjing%20Yue%20and%20Jingyu%20Yang&entry.1292438233=%20%20Referring%20image%20segmentation%20%28RIS%29%20aims%20to%20segment%20a%20particular%20region%20based%0Aon%20a%20language%20expression%20prompt.%20Existing%20methods%20incorporate%20linguistic%0Afeatures%20into%20visual%20features%20and%20obtain%20multi-modal%20features%20for%20mask%0Adecoding.%20However%2C%20these%20methods%20may%20segment%20the%20visually%20salient%20entity%0Ainstead%20of%20the%20correct%20referring%20region%2C%20as%20the%20multi-modal%20features%20are%0Adominated%20by%20the%20abundant%20visual%20context.%20In%20this%20paper%2C%20we%20propose%20MARIS%2C%20a%0Areferring%20image%20segmentation%20method%20that%20leverages%20the%20Segment%20Anything%20Model%0A%28SAM%29%20and%20introduces%20a%20mutual-aware%20attention%20mechanism%20to%20enhance%20the%0Across-modal%20fusion%20via%20two%20parallel%20branches.%20Specifically%2C%20our%20mutual-aware%0Aattention%20mechanism%20consists%20of%20Vision-Guided%20Attention%20and%20Language-Guided%0AAttention%2C%20which%20bidirectionally%20model%20the%20relationship%20between%20visual%20and%0Alinguistic%20features.%20Correspondingly%2C%20we%20design%20a%20Mask%20Decoder%20to%20enable%0Aexplicit%20linguistic%20guidance%20for%20more%20consistent%20segmentation%20with%20the%20language%0Aexpression.%20To%20this%20end%2C%20a%20multi-modal%20query%20token%20is%20proposed%20to%20integrate%0Alinguistic%20information%20and%20interact%20with%20visual%20information%20simultaneously.%0AExtensive%20experiments%20on%20three%20benchmark%20datasets%20show%20that%20our%20method%0Aoutperforms%20the%20state-of-the-art%20RIS%20methods.%20Our%20code%20will%20be%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15727v4&entry.124074799=Read"},
{"title": "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention", "author": "William Brandon and Mayank Mishra and Aniruddha Nrusimha and Rameswar Panda and Jonathan Ragan Kelly", "abstract": "  Key-value (KV) caching plays an essential role in accelerating decoding for\ntransformer-based autoregressive large language models (LLMs). However, the\namount of memory required to store the KV cache can become prohibitive at long\nsequence lengths and large batch sizes. Since the invention of the transformer,\ntwo of the most effective interventions discovered for reducing the size of the\nKV cache have been Multi-Query Attention (MQA) and its generalization,\nGrouped-Query Attention (GQA). MQA and GQA both modify the design of the\nattention block so that multiple query heads can share a single key/value head,\nreducing the number of distinct key/value heads by a large factor while only\nminimally degrading accuracy. In this paper, we show that it is possible to\ntake Multi-Query Attention a step further by also sharing key and value heads\nbetween adjacent layers, yielding a new attention design we call Cross-Layer\nAttention (CLA). With CLA, we find that it is possible to reduce the size of\nthe KV cache by another 2x while maintaining nearly the same accuracy as\nunmodified MQA. In experiments training 1B- and 3B-parameter models from\nscratch, we demonstrate that CLA provides a Pareto improvement over the\nmemory/accuracy tradeoffs which are possible with traditional MQA, enabling\ninference with longer sequence lengths and larger batch sizes than would\notherwise be possible\n", "link": "http://arxiv.org/abs/2405.12981v1", "date": "2024-05-21", "relevancy": 2.0754, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5394}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5121}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reducing%20Transformer%20Key-Value%20Cache%20Size%20with%20Cross-Layer%20Attention&body=Title%3A%20Reducing%20Transformer%20Key-Value%20Cache%20Size%20with%20Cross-Layer%20Attention%0AAuthor%3A%20William%20Brandon%20and%20Mayank%20Mishra%20and%20Aniruddha%20Nrusimha%20and%20Rameswar%20Panda%20and%20Jonathan%20Ragan%20Kelly%0AAbstract%3A%20%20%20Key-value%20%28KV%29%20caching%20plays%20an%20essential%20role%20in%20accelerating%20decoding%20for%0Atransformer-based%20autoregressive%20large%20language%20models%20%28LLMs%29.%20However%2C%20the%0Aamount%20of%20memory%20required%20to%20store%20the%20KV%20cache%20can%20become%20prohibitive%20at%20long%0Asequence%20lengths%20and%20large%20batch%20sizes.%20Since%20the%20invention%20of%20the%20transformer%2C%0Atwo%20of%20the%20most%20effective%20interventions%20discovered%20for%20reducing%20the%20size%20of%20the%0AKV%20cache%20have%20been%20Multi-Query%20Attention%20%28MQA%29%20and%20its%20generalization%2C%0AGrouped-Query%20Attention%20%28GQA%29.%20MQA%20and%20GQA%20both%20modify%20the%20design%20of%20the%0Aattention%20block%20so%20that%20multiple%20query%20heads%20can%20share%20a%20single%20key/value%20head%2C%0Areducing%20the%20number%20of%20distinct%20key/value%20heads%20by%20a%20large%20factor%20while%20only%0Aminimally%20degrading%20accuracy.%20In%20this%20paper%2C%20we%20show%20that%20it%20is%20possible%20to%0Atake%20Multi-Query%20Attention%20a%20step%20further%20by%20also%20sharing%20key%20and%20value%20heads%0Abetween%20adjacent%20layers%2C%20yielding%20a%20new%20attention%20design%20we%20call%20Cross-Layer%0AAttention%20%28CLA%29.%20With%20CLA%2C%20we%20find%20that%20it%20is%20possible%20to%20reduce%20the%20size%20of%0Athe%20KV%20cache%20by%20another%202x%20while%20maintaining%20nearly%20the%20same%20accuracy%20as%0Aunmodified%20MQA.%20In%20experiments%20training%201B-%20and%203B-parameter%20models%20from%0Ascratch%2C%20we%20demonstrate%20that%20CLA%20provides%20a%20Pareto%20improvement%20over%20the%0Amemory/accuracy%20tradeoffs%20which%20are%20possible%20with%20traditional%20MQA%2C%20enabling%0Ainference%20with%20longer%20sequence%20lengths%20and%20larger%20batch%20sizes%20than%20would%0Aotherwise%20be%20possible%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReducing%2520Transformer%2520Key-Value%2520Cache%2520Size%2520with%2520Cross-Layer%2520Attention%26entry.906535625%3DWilliam%2520Brandon%2520and%2520Mayank%2520Mishra%2520and%2520Aniruddha%2520Nrusimha%2520and%2520Rameswar%2520Panda%2520and%2520Jonathan%2520Ragan%2520Kelly%26entry.1292438233%3D%2520%2520Key-value%2520%2528KV%2529%2520caching%2520plays%2520an%2520essential%2520role%2520in%2520accelerating%2520decoding%2520for%250Atransformer-based%2520autoregressive%2520large%2520language%2520models%2520%2528LLMs%2529.%2520However%252C%2520the%250Aamount%2520of%2520memory%2520required%2520to%2520store%2520the%2520KV%2520cache%2520can%2520become%2520prohibitive%2520at%2520long%250Asequence%2520lengths%2520and%2520large%2520batch%2520sizes.%2520Since%2520the%2520invention%2520of%2520the%2520transformer%252C%250Atwo%2520of%2520the%2520most%2520effective%2520interventions%2520discovered%2520for%2520reducing%2520the%2520size%2520of%2520the%250AKV%2520cache%2520have%2520been%2520Multi-Query%2520Attention%2520%2528MQA%2529%2520and%2520its%2520generalization%252C%250AGrouped-Query%2520Attention%2520%2528GQA%2529.%2520MQA%2520and%2520GQA%2520both%2520modify%2520the%2520design%2520of%2520the%250Aattention%2520block%2520so%2520that%2520multiple%2520query%2520heads%2520can%2520share%2520a%2520single%2520key/value%2520head%252C%250Areducing%2520the%2520number%2520of%2520distinct%2520key/value%2520heads%2520by%2520a%2520large%2520factor%2520while%2520only%250Aminimally%2520degrading%2520accuracy.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520it%2520is%2520possible%2520to%250Atake%2520Multi-Query%2520Attention%2520a%2520step%2520further%2520by%2520also%2520sharing%2520key%2520and%2520value%2520heads%250Abetween%2520adjacent%2520layers%252C%2520yielding%2520a%2520new%2520attention%2520design%2520we%2520call%2520Cross-Layer%250AAttention%2520%2528CLA%2529.%2520With%2520CLA%252C%2520we%2520find%2520that%2520it%2520is%2520possible%2520to%2520reduce%2520the%2520size%2520of%250Athe%2520KV%2520cache%2520by%2520another%25202x%2520while%2520maintaining%2520nearly%2520the%2520same%2520accuracy%2520as%250Aunmodified%2520MQA.%2520In%2520experiments%2520training%25201B-%2520and%25203B-parameter%2520models%2520from%250Ascratch%252C%2520we%2520demonstrate%2520that%2520CLA%2520provides%2520a%2520Pareto%2520improvement%2520over%2520the%250Amemory/accuracy%2520tradeoffs%2520which%2520are%2520possible%2520with%2520traditional%2520MQA%252C%2520enabling%250Ainference%2520with%2520longer%2520sequence%2520lengths%2520and%2520larger%2520batch%2520sizes%2520than%2520would%250Aotherwise%2520be%2520possible%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reducing%20Transformer%20Key-Value%20Cache%20Size%20with%20Cross-Layer%20Attention&entry.906535625=William%20Brandon%20and%20Mayank%20Mishra%20and%20Aniruddha%20Nrusimha%20and%20Rameswar%20Panda%20and%20Jonathan%20Ragan%20Kelly&entry.1292438233=%20%20Key-value%20%28KV%29%20caching%20plays%20an%20essential%20role%20in%20accelerating%20decoding%20for%0Atransformer-based%20autoregressive%20large%20language%20models%20%28LLMs%29.%20However%2C%20the%0Aamount%20of%20memory%20required%20to%20store%20the%20KV%20cache%20can%20become%20prohibitive%20at%20long%0Asequence%20lengths%20and%20large%20batch%20sizes.%20Since%20the%20invention%20of%20the%20transformer%2C%0Atwo%20of%20the%20most%20effective%20interventions%20discovered%20for%20reducing%20the%20size%20of%20the%0AKV%20cache%20have%20been%20Multi-Query%20Attention%20%28MQA%29%20and%20its%20generalization%2C%0AGrouped-Query%20Attention%20%28GQA%29.%20MQA%20and%20GQA%20both%20modify%20the%20design%20of%20the%0Aattention%20block%20so%20that%20multiple%20query%20heads%20can%20share%20a%20single%20key/value%20head%2C%0Areducing%20the%20number%20of%20distinct%20key/value%20heads%20by%20a%20large%20factor%20while%20only%0Aminimally%20degrading%20accuracy.%20In%20this%20paper%2C%20we%20show%20that%20it%20is%20possible%20to%0Atake%20Multi-Query%20Attention%20a%20step%20further%20by%20also%20sharing%20key%20and%20value%20heads%0Abetween%20adjacent%20layers%2C%20yielding%20a%20new%20attention%20design%20we%20call%20Cross-Layer%0AAttention%20%28CLA%29.%20With%20CLA%2C%20we%20find%20that%20it%20is%20possible%20to%20reduce%20the%20size%20of%0Athe%20KV%20cache%20by%20another%202x%20while%20maintaining%20nearly%20the%20same%20accuracy%20as%0Aunmodified%20MQA.%20In%20experiments%20training%201B-%20and%203B-parameter%20models%20from%0Ascratch%2C%20we%20demonstrate%20that%20CLA%20provides%20a%20Pareto%20improvement%20over%20the%0Amemory/accuracy%20tradeoffs%20which%20are%20possible%20with%20traditional%20MQA%2C%20enabling%0Ainference%20with%20longer%20sequence%20lengths%20and%20larger%20batch%20sizes%20than%20would%0Aotherwise%20be%20possible%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12981v1&entry.124074799=Read"},
{"title": "Topology-guided Hypergraph Transformer Network: Unveiling Structural\n  Insights for Improved Representation", "author": "Khaled Mohammed Saifuddin and Mehmet Emin Aktas and Esra Akbas", "abstract": "  Hypergraphs, with their capacity to depict high-order relationships, have\nemerged as a significant extension of traditional graphs. Although Graph Neural\nNetworks (GNNs) have remarkable performance in graph representation learning,\ntheir extension to hypergraphs encounters challenges due to their intricate\nstructures. Furthermore, current hypergraph transformers, a special variant of\nGNN, utilize semantic feature-based self-attention, ignoring topological\nattributes of nodes and hyperedges. To address these challenges, we propose a\nTopology-guided Hypergraph Transformer Network (THTN). In this model, we first\nformulate a hypergraph from a graph while retaining its structural essence to\nlearn higher-order relations within the graph. Then, we design a simple yet\neffective structural and spatial encoding module to incorporate the topological\nand spatial information of the nodes into their representation. Further, we\npresent a structure-aware self-attention mechanism that discovers the important\nnodes and hyperedges from both semantic and structural viewpoints. By\nleveraging these two modules, THTN crafts an improved node representation,\ncapturing both local and global topological expressions. Extensive experiments\nconducted on node classification tasks demonstrate that the performance of the\nproposed model consistently exceeds that of the existing approaches.\n", "link": "http://arxiv.org/abs/2310.09657v2", "date": "2024-05-21", "relevancy": 2.0651, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5384}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5229}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topology-guided%20Hypergraph%20Transformer%20Network%3A%20Unveiling%20Structural%0A%20%20Insights%20for%20Improved%20Representation&body=Title%3A%20Topology-guided%20Hypergraph%20Transformer%20Network%3A%20Unveiling%20Structural%0A%20%20Insights%20for%20Improved%20Representation%0AAuthor%3A%20Khaled%20Mohammed%20Saifuddin%20and%20Mehmet%20Emin%20Aktas%20and%20Esra%20Akbas%0AAbstract%3A%20%20%20Hypergraphs%2C%20with%20their%20capacity%20to%20depict%20high-order%20relationships%2C%20have%0Aemerged%20as%20a%20significant%20extension%20of%20traditional%20graphs.%20Although%20Graph%20Neural%0ANetworks%20%28GNNs%29%20have%20remarkable%20performance%20in%20graph%20representation%20learning%2C%0Atheir%20extension%20to%20hypergraphs%20encounters%20challenges%20due%20to%20their%20intricate%0Astructures.%20Furthermore%2C%20current%20hypergraph%20transformers%2C%20a%20special%20variant%20of%0AGNN%2C%20utilize%20semantic%20feature-based%20self-attention%2C%20ignoring%20topological%0Aattributes%20of%20nodes%20and%20hyperedges.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0ATopology-guided%20Hypergraph%20Transformer%20Network%20%28THTN%29.%20In%20this%20model%2C%20we%20first%0Aformulate%20a%20hypergraph%20from%20a%20graph%20while%20retaining%20its%20structural%20essence%20to%0Alearn%20higher-order%20relations%20within%20the%20graph.%20Then%2C%20we%20design%20a%20simple%20yet%0Aeffective%20structural%20and%20spatial%20encoding%20module%20to%20incorporate%20the%20topological%0Aand%20spatial%20information%20of%20the%20nodes%20into%20their%20representation.%20Further%2C%20we%0Apresent%20a%20structure-aware%20self-attention%20mechanism%20that%20discovers%20the%20important%0Anodes%20and%20hyperedges%20from%20both%20semantic%20and%20structural%20viewpoints.%20By%0Aleveraging%20these%20two%20modules%2C%20THTN%20crafts%20an%20improved%20node%20representation%2C%0Acapturing%20both%20local%20and%20global%20topological%20expressions.%20Extensive%20experiments%0Aconducted%20on%20node%20classification%20tasks%20demonstrate%20that%20the%20performance%20of%20the%0Aproposed%20model%20consistently%20exceeds%20that%20of%20the%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.09657v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopology-guided%2520Hypergraph%2520Transformer%2520Network%253A%2520Unveiling%2520Structural%250A%2520%2520Insights%2520for%2520Improved%2520Representation%26entry.906535625%3DKhaled%2520Mohammed%2520Saifuddin%2520and%2520Mehmet%2520Emin%2520Aktas%2520and%2520Esra%2520Akbas%26entry.1292438233%3D%2520%2520Hypergraphs%252C%2520with%2520their%2520capacity%2520to%2520depict%2520high-order%2520relationships%252C%2520have%250Aemerged%2520as%2520a%2520significant%2520extension%2520of%2520traditional%2520graphs.%2520Although%2520Graph%2520Neural%250ANetworks%2520%2528GNNs%2529%2520have%2520remarkable%2520performance%2520in%2520graph%2520representation%2520learning%252C%250Atheir%2520extension%2520to%2520hypergraphs%2520encounters%2520challenges%2520due%2520to%2520their%2520intricate%250Astructures.%2520Furthermore%252C%2520current%2520hypergraph%2520transformers%252C%2520a%2520special%2520variant%2520of%250AGNN%252C%2520utilize%2520semantic%2520feature-based%2520self-attention%252C%2520ignoring%2520topological%250Aattributes%2520of%2520nodes%2520and%2520hyperedges.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%250ATopology-guided%2520Hypergraph%2520Transformer%2520Network%2520%2528THTN%2529.%2520In%2520this%2520model%252C%2520we%2520first%250Aformulate%2520a%2520hypergraph%2520from%2520a%2520graph%2520while%2520retaining%2520its%2520structural%2520essence%2520to%250Alearn%2520higher-order%2520relations%2520within%2520the%2520graph.%2520Then%252C%2520we%2520design%2520a%2520simple%2520yet%250Aeffective%2520structural%2520and%2520spatial%2520encoding%2520module%2520to%2520incorporate%2520the%2520topological%250Aand%2520spatial%2520information%2520of%2520the%2520nodes%2520into%2520their%2520representation.%2520Further%252C%2520we%250Apresent%2520a%2520structure-aware%2520self-attention%2520mechanism%2520that%2520discovers%2520the%2520important%250Anodes%2520and%2520hyperedges%2520from%2520both%2520semantic%2520and%2520structural%2520viewpoints.%2520By%250Aleveraging%2520these%2520two%2520modules%252C%2520THTN%2520crafts%2520an%2520improved%2520node%2520representation%252C%250Acapturing%2520both%2520local%2520and%2520global%2520topological%2520expressions.%2520Extensive%2520experiments%250Aconducted%2520on%2520node%2520classification%2520tasks%2520demonstrate%2520that%2520the%2520performance%2520of%2520the%250Aproposed%2520model%2520consistently%2520exceeds%2520that%2520of%2520the%2520existing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.09657v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topology-guided%20Hypergraph%20Transformer%20Network%3A%20Unveiling%20Structural%0A%20%20Insights%20for%20Improved%20Representation&entry.906535625=Khaled%20Mohammed%20Saifuddin%20and%20Mehmet%20Emin%20Aktas%20and%20Esra%20Akbas&entry.1292438233=%20%20Hypergraphs%2C%20with%20their%20capacity%20to%20depict%20high-order%20relationships%2C%20have%0Aemerged%20as%20a%20significant%20extension%20of%20traditional%20graphs.%20Although%20Graph%20Neural%0ANetworks%20%28GNNs%29%20have%20remarkable%20performance%20in%20graph%20representation%20learning%2C%0Atheir%20extension%20to%20hypergraphs%20encounters%20challenges%20due%20to%20their%20intricate%0Astructures.%20Furthermore%2C%20current%20hypergraph%20transformers%2C%20a%20special%20variant%20of%0AGNN%2C%20utilize%20semantic%20feature-based%20self-attention%2C%20ignoring%20topological%0Aattributes%20of%20nodes%20and%20hyperedges.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0ATopology-guided%20Hypergraph%20Transformer%20Network%20%28THTN%29.%20In%20this%20model%2C%20we%20first%0Aformulate%20a%20hypergraph%20from%20a%20graph%20while%20retaining%20its%20structural%20essence%20to%0Alearn%20higher-order%20relations%20within%20the%20graph.%20Then%2C%20we%20design%20a%20simple%20yet%0Aeffective%20structural%20and%20spatial%20encoding%20module%20to%20incorporate%20the%20topological%0Aand%20spatial%20information%20of%20the%20nodes%20into%20their%20representation.%20Further%2C%20we%0Apresent%20a%20structure-aware%20self-attention%20mechanism%20that%20discovers%20the%20important%0Anodes%20and%20hyperedges%20from%20both%20semantic%20and%20structural%20viewpoints.%20By%0Aleveraging%20these%20two%20modules%2C%20THTN%20crafts%20an%20improved%20node%20representation%2C%0Acapturing%20both%20local%20and%20global%20topological%20expressions.%20Extensive%20experiments%0Aconducted%20on%20node%20classification%20tasks%20demonstrate%20that%20the%20performance%20of%20the%0Aproposed%20model%20consistently%20exceeds%20that%20of%20the%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.09657v2&entry.124074799=Read"},
{"title": "BiomedParse: a biomedical foundation model for image parsing of\n  everything everywhere all at once", "author": "Theodore Zhao and Yu Gu and Jianwei Yang and Naoto Usuyama and Ho Hin Lee and Tristan Naumann and Jianfeng Gao and Angela Crabtree and Brian Piening and Carlo Bifulco and Mu Wei and Hoifung Poon and Sheng Wang", "abstract": "  Biomedical image analysis is fundamental for biomedical discovery in cell\nbiology, pathology, radiology, and many other biomedical domains. Holistic\nimage analysis comprises interdependent subtasks such as segmentation,\ndetection, and recognition of relevant objects. Here, we propose BiomedParse, a\nbiomedical foundation model for imaging parsing that can jointly conduct\nsegmentation, detection, and recognition for 82 object types across 9 imaging\nmodalities. Through joint learning, we can improve accuracy for individual\ntasks and enable novel applications such as segmenting all relevant objects in\nan image through a text prompt, rather than requiring users to laboriously\nspecify the bounding box for each object. We leveraged readily available\nnatural-language labels or descriptions accompanying those datasets and use\nGPT-4 to harmonize the noisy, unstructured text information with established\nbiomedical object ontologies. We created a large dataset comprising over six\nmillion triples of image, segmentation mask, and textual description. On image\nsegmentation, we showed that BiomedParse is broadly applicable, outperforming\nstate-of-the-art methods on 102,855 test image-mask-label triples across 9\nimaging modalities (everything). On object detection, which aims to locate a\nspecific object of interest, BiomedParse again attained state-of-the-art\nperformance, especially on objects with irregular shapes (everywhere). On\nobject recognition, which aims to identify all objects in a given image along\nwith their semantic types, we showed that BiomedParse can simultaneously\nsegment and label all biomedical objects in an image (all at once). In summary,\nBiomedParse is an all-in-one tool for biomedical image analysis by jointly\nsolving segmentation, detection, and recognition for all major biomedical image\nmodalities, paving the path for efficient and accurate image-based biomedical\ndiscovery.\n", "link": "http://arxiv.org/abs/2405.12971v1", "date": "2024-05-21", "relevancy": 2.0469, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.546}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4908}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BiomedParse%3A%20a%20biomedical%20foundation%20model%20for%20image%20parsing%20of%0A%20%20everything%20everywhere%20all%20at%20once&body=Title%3A%20BiomedParse%3A%20a%20biomedical%20foundation%20model%20for%20image%20parsing%20of%0A%20%20everything%20everywhere%20all%20at%20once%0AAuthor%3A%20Theodore%20Zhao%20and%20Yu%20Gu%20and%20Jianwei%20Yang%20and%20Naoto%20Usuyama%20and%20Ho%20Hin%20Lee%20and%20Tristan%20Naumann%20and%20Jianfeng%20Gao%20and%20Angela%20Crabtree%20and%20Brian%20Piening%20and%20Carlo%20Bifulco%20and%20Mu%20Wei%20and%20Hoifung%20Poon%20and%20Sheng%20Wang%0AAbstract%3A%20%20%20Biomedical%20image%20analysis%20is%20fundamental%20for%20biomedical%20discovery%20in%20cell%0Abiology%2C%20pathology%2C%20radiology%2C%20and%20many%20other%20biomedical%20domains.%20Holistic%0Aimage%20analysis%20comprises%20interdependent%20subtasks%20such%20as%20segmentation%2C%0Adetection%2C%20and%20recognition%20of%20relevant%20objects.%20Here%2C%20we%20propose%20BiomedParse%2C%20a%0Abiomedical%20foundation%20model%20for%20imaging%20parsing%20that%20can%20jointly%20conduct%0Asegmentation%2C%20detection%2C%20and%20recognition%20for%2082%20object%20types%20across%209%20imaging%0Amodalities.%20Through%20joint%20learning%2C%20we%20can%20improve%20accuracy%20for%20individual%0Atasks%20and%20enable%20novel%20applications%20such%20as%20segmenting%20all%20relevant%20objects%20in%0Aan%20image%20through%20a%20text%20prompt%2C%20rather%20than%20requiring%20users%20to%20laboriously%0Aspecify%20the%20bounding%20box%20for%20each%20object.%20We%20leveraged%20readily%20available%0Anatural-language%20labels%20or%20descriptions%20accompanying%20those%20datasets%20and%20use%0AGPT-4%20to%20harmonize%20the%20noisy%2C%20unstructured%20text%20information%20with%20established%0Abiomedical%20object%20ontologies.%20We%20created%20a%20large%20dataset%20comprising%20over%20six%0Amillion%20triples%20of%20image%2C%20segmentation%20mask%2C%20and%20textual%20description.%20On%20image%0Asegmentation%2C%20we%20showed%20that%20BiomedParse%20is%20broadly%20applicable%2C%20outperforming%0Astate-of-the-art%20methods%20on%20102%2C855%20test%20image-mask-label%20triples%20across%209%0Aimaging%20modalities%20%28everything%29.%20On%20object%20detection%2C%20which%20aims%20to%20locate%20a%0Aspecific%20object%20of%20interest%2C%20BiomedParse%20again%20attained%20state-of-the-art%0Aperformance%2C%20especially%20on%20objects%20with%20irregular%20shapes%20%28everywhere%29.%20On%0Aobject%20recognition%2C%20which%20aims%20to%20identify%20all%20objects%20in%20a%20given%20image%20along%0Awith%20their%20semantic%20types%2C%20we%20showed%20that%20BiomedParse%20can%20simultaneously%0Asegment%20and%20label%20all%20biomedical%20objects%20in%20an%20image%20%28all%20at%20once%29.%20In%20summary%2C%0ABiomedParse%20is%20an%20all-in-one%20tool%20for%20biomedical%20image%20analysis%20by%20jointly%0Asolving%20segmentation%2C%20detection%2C%20and%20recognition%20for%20all%20major%20biomedical%20image%0Amodalities%2C%20paving%20the%20path%20for%20efficient%20and%20accurate%20image-based%20biomedical%0Adiscovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12971v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiomedParse%253A%2520a%2520biomedical%2520foundation%2520model%2520for%2520image%2520parsing%2520of%250A%2520%2520everything%2520everywhere%2520all%2520at%2520once%26entry.906535625%3DTheodore%2520Zhao%2520and%2520Yu%2520Gu%2520and%2520Jianwei%2520Yang%2520and%2520Naoto%2520Usuyama%2520and%2520Ho%2520Hin%2520Lee%2520and%2520Tristan%2520Naumann%2520and%2520Jianfeng%2520Gao%2520and%2520Angela%2520Crabtree%2520and%2520Brian%2520Piening%2520and%2520Carlo%2520Bifulco%2520and%2520Mu%2520Wei%2520and%2520Hoifung%2520Poon%2520and%2520Sheng%2520Wang%26entry.1292438233%3D%2520%2520Biomedical%2520image%2520analysis%2520is%2520fundamental%2520for%2520biomedical%2520discovery%2520in%2520cell%250Abiology%252C%2520pathology%252C%2520radiology%252C%2520and%2520many%2520other%2520biomedical%2520domains.%2520Holistic%250Aimage%2520analysis%2520comprises%2520interdependent%2520subtasks%2520such%2520as%2520segmentation%252C%250Adetection%252C%2520and%2520recognition%2520of%2520relevant%2520objects.%2520Here%252C%2520we%2520propose%2520BiomedParse%252C%2520a%250Abiomedical%2520foundation%2520model%2520for%2520imaging%2520parsing%2520that%2520can%2520jointly%2520conduct%250Asegmentation%252C%2520detection%252C%2520and%2520recognition%2520for%252082%2520object%2520types%2520across%25209%2520imaging%250Amodalities.%2520Through%2520joint%2520learning%252C%2520we%2520can%2520improve%2520accuracy%2520for%2520individual%250Atasks%2520and%2520enable%2520novel%2520applications%2520such%2520as%2520segmenting%2520all%2520relevant%2520objects%2520in%250Aan%2520image%2520through%2520a%2520text%2520prompt%252C%2520rather%2520than%2520requiring%2520users%2520to%2520laboriously%250Aspecify%2520the%2520bounding%2520box%2520for%2520each%2520object.%2520We%2520leveraged%2520readily%2520available%250Anatural-language%2520labels%2520or%2520descriptions%2520accompanying%2520those%2520datasets%2520and%2520use%250AGPT-4%2520to%2520harmonize%2520the%2520noisy%252C%2520unstructured%2520text%2520information%2520with%2520established%250Abiomedical%2520object%2520ontologies.%2520We%2520created%2520a%2520large%2520dataset%2520comprising%2520over%2520six%250Amillion%2520triples%2520of%2520image%252C%2520segmentation%2520mask%252C%2520and%2520textual%2520description.%2520On%2520image%250Asegmentation%252C%2520we%2520showed%2520that%2520BiomedParse%2520is%2520broadly%2520applicable%252C%2520outperforming%250Astate-of-the-art%2520methods%2520on%2520102%252C855%2520test%2520image-mask-label%2520triples%2520across%25209%250Aimaging%2520modalities%2520%2528everything%2529.%2520On%2520object%2520detection%252C%2520which%2520aims%2520to%2520locate%2520a%250Aspecific%2520object%2520of%2520interest%252C%2520BiomedParse%2520again%2520attained%2520state-of-the-art%250Aperformance%252C%2520especially%2520on%2520objects%2520with%2520irregular%2520shapes%2520%2528everywhere%2529.%2520On%250Aobject%2520recognition%252C%2520which%2520aims%2520to%2520identify%2520all%2520objects%2520in%2520a%2520given%2520image%2520along%250Awith%2520their%2520semantic%2520types%252C%2520we%2520showed%2520that%2520BiomedParse%2520can%2520simultaneously%250Asegment%2520and%2520label%2520all%2520biomedical%2520objects%2520in%2520an%2520image%2520%2528all%2520at%2520once%2529.%2520In%2520summary%252C%250ABiomedParse%2520is%2520an%2520all-in-one%2520tool%2520for%2520biomedical%2520image%2520analysis%2520by%2520jointly%250Asolving%2520segmentation%252C%2520detection%252C%2520and%2520recognition%2520for%2520all%2520major%2520biomedical%2520image%250Amodalities%252C%2520paving%2520the%2520path%2520for%2520efficient%2520and%2520accurate%2520image-based%2520biomedical%250Adiscovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12971v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BiomedParse%3A%20a%20biomedical%20foundation%20model%20for%20image%20parsing%20of%0A%20%20everything%20everywhere%20all%20at%20once&entry.906535625=Theodore%20Zhao%20and%20Yu%20Gu%20and%20Jianwei%20Yang%20and%20Naoto%20Usuyama%20and%20Ho%20Hin%20Lee%20and%20Tristan%20Naumann%20and%20Jianfeng%20Gao%20and%20Angela%20Crabtree%20and%20Brian%20Piening%20and%20Carlo%20Bifulco%20and%20Mu%20Wei%20and%20Hoifung%20Poon%20and%20Sheng%20Wang&entry.1292438233=%20%20Biomedical%20image%20analysis%20is%20fundamental%20for%20biomedical%20discovery%20in%20cell%0Abiology%2C%20pathology%2C%20radiology%2C%20and%20many%20other%20biomedical%20domains.%20Holistic%0Aimage%20analysis%20comprises%20interdependent%20subtasks%20such%20as%20segmentation%2C%0Adetection%2C%20and%20recognition%20of%20relevant%20objects.%20Here%2C%20we%20propose%20BiomedParse%2C%20a%0Abiomedical%20foundation%20model%20for%20imaging%20parsing%20that%20can%20jointly%20conduct%0Asegmentation%2C%20detection%2C%20and%20recognition%20for%2082%20object%20types%20across%209%20imaging%0Amodalities.%20Through%20joint%20learning%2C%20we%20can%20improve%20accuracy%20for%20individual%0Atasks%20and%20enable%20novel%20applications%20such%20as%20segmenting%20all%20relevant%20objects%20in%0Aan%20image%20through%20a%20text%20prompt%2C%20rather%20than%20requiring%20users%20to%20laboriously%0Aspecify%20the%20bounding%20box%20for%20each%20object.%20We%20leveraged%20readily%20available%0Anatural-language%20labels%20or%20descriptions%20accompanying%20those%20datasets%20and%20use%0AGPT-4%20to%20harmonize%20the%20noisy%2C%20unstructured%20text%20information%20with%20established%0Abiomedical%20object%20ontologies.%20We%20created%20a%20large%20dataset%20comprising%20over%20six%0Amillion%20triples%20of%20image%2C%20segmentation%20mask%2C%20and%20textual%20description.%20On%20image%0Asegmentation%2C%20we%20showed%20that%20BiomedParse%20is%20broadly%20applicable%2C%20outperforming%0Astate-of-the-art%20methods%20on%20102%2C855%20test%20image-mask-label%20triples%20across%209%0Aimaging%20modalities%20%28everything%29.%20On%20object%20detection%2C%20which%20aims%20to%20locate%20a%0Aspecific%20object%20of%20interest%2C%20BiomedParse%20again%20attained%20state-of-the-art%0Aperformance%2C%20especially%20on%20objects%20with%20irregular%20shapes%20%28everywhere%29.%20On%0Aobject%20recognition%2C%20which%20aims%20to%20identify%20all%20objects%20in%20a%20given%20image%20along%0Awith%20their%20semantic%20types%2C%20we%20showed%20that%20BiomedParse%20can%20simultaneously%0Asegment%20and%20label%20all%20biomedical%20objects%20in%20an%20image%20%28all%20at%20once%29.%20In%20summary%2C%0ABiomedParse%20is%20an%20all-in-one%20tool%20for%20biomedical%20image%20analysis%20by%20jointly%0Asolving%20segmentation%2C%20detection%2C%20and%20recognition%20for%20all%20major%20biomedical%20image%0Amodalities%2C%20paving%20the%20path%20for%20efficient%20and%20accurate%20image-based%20biomedical%0Adiscovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12971v1&entry.124074799=Read"},
{"title": "Transformer in Touch: A Survey", "author": "Jing Gao and Ning Cheng and Bin Fang and Wenjuan Han", "abstract": "  The Transformer model, initially achieving significant success in the field\nof natural language processing, has recently shown great potential in the\napplication of tactile perception. This review aims to comprehensively outline\nthe application and development of Transformers in tactile technology. We first\nintroduce the two fundamental concepts behind the success of the Transformer:\nthe self-attention mechanism and large-scale pre-training. Then, we delve into\nthe application of Transformers in various tactile tasks, including but not\nlimited to object recognition, cross-modal generation, and object manipulation,\noffering a concise summary of the core methodologies, performance benchmarks,\nand design highlights. Finally, we suggest potential areas for further research\nand future work, aiming to generate more interest within the community, tackle\nexisting challenges, and encourage the use of Transformer models in the tactile\nfield.\n", "link": "http://arxiv.org/abs/2405.12779v1", "date": "2024-05-21", "relevancy": 2.0444, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5616}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5195}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformer%20in%20Touch%3A%20A%20Survey&body=Title%3A%20Transformer%20in%20Touch%3A%20A%20Survey%0AAuthor%3A%20Jing%20Gao%20and%20Ning%20Cheng%20and%20Bin%20Fang%20and%20Wenjuan%20Han%0AAbstract%3A%20%20%20The%20Transformer%20model%2C%20initially%20achieving%20significant%20success%20in%20the%20field%0Aof%20natural%20language%20processing%2C%20has%20recently%20shown%20great%20potential%20in%20the%0Aapplication%20of%20tactile%20perception.%20This%20review%20aims%20to%20comprehensively%20outline%0Athe%20application%20and%20development%20of%20Transformers%20in%20tactile%20technology.%20We%20first%0Aintroduce%20the%20two%20fundamental%20concepts%20behind%20the%20success%20of%20the%20Transformer%3A%0Athe%20self-attention%20mechanism%20and%20large-scale%20pre-training.%20Then%2C%20we%20delve%20into%0Athe%20application%20of%20Transformers%20in%20various%20tactile%20tasks%2C%20including%20but%20not%0Alimited%20to%20object%20recognition%2C%20cross-modal%20generation%2C%20and%20object%20manipulation%2C%0Aoffering%20a%20concise%20summary%20of%20the%20core%20methodologies%2C%20performance%20benchmarks%2C%0Aand%20design%20highlights.%20Finally%2C%20we%20suggest%20potential%20areas%20for%20further%20research%0Aand%20future%20work%2C%20aiming%20to%20generate%20more%20interest%20within%20the%20community%2C%20tackle%0Aexisting%20challenges%2C%20and%20encourage%20the%20use%20of%20Transformer%20models%20in%20the%20tactile%0Afield.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformer%2520in%2520Touch%253A%2520A%2520Survey%26entry.906535625%3DJing%2520Gao%2520and%2520Ning%2520Cheng%2520and%2520Bin%2520Fang%2520and%2520Wenjuan%2520Han%26entry.1292438233%3D%2520%2520The%2520Transformer%2520model%252C%2520initially%2520achieving%2520significant%2520success%2520in%2520the%2520field%250Aof%2520natural%2520language%2520processing%252C%2520has%2520recently%2520shown%2520great%2520potential%2520in%2520the%250Aapplication%2520of%2520tactile%2520perception.%2520This%2520review%2520aims%2520to%2520comprehensively%2520outline%250Athe%2520application%2520and%2520development%2520of%2520Transformers%2520in%2520tactile%2520technology.%2520We%2520first%250Aintroduce%2520the%2520two%2520fundamental%2520concepts%2520behind%2520the%2520success%2520of%2520the%2520Transformer%253A%250Athe%2520self-attention%2520mechanism%2520and%2520large-scale%2520pre-training.%2520Then%252C%2520we%2520delve%2520into%250Athe%2520application%2520of%2520Transformers%2520in%2520various%2520tactile%2520tasks%252C%2520including%2520but%2520not%250Alimited%2520to%2520object%2520recognition%252C%2520cross-modal%2520generation%252C%2520and%2520object%2520manipulation%252C%250Aoffering%2520a%2520concise%2520summary%2520of%2520the%2520core%2520methodologies%252C%2520performance%2520benchmarks%252C%250Aand%2520design%2520highlights.%2520Finally%252C%2520we%2520suggest%2520potential%2520areas%2520for%2520further%2520research%250Aand%2520future%2520work%252C%2520aiming%2520to%2520generate%2520more%2520interest%2520within%2520the%2520community%252C%2520tackle%250Aexisting%2520challenges%252C%2520and%2520encourage%2520the%2520use%2520of%2520Transformer%2520models%2520in%2520the%2520tactile%250Afield.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer%20in%20Touch%3A%20A%20Survey&entry.906535625=Jing%20Gao%20and%20Ning%20Cheng%20and%20Bin%20Fang%20and%20Wenjuan%20Han&entry.1292438233=%20%20The%20Transformer%20model%2C%20initially%20achieving%20significant%20success%20in%20the%20field%0Aof%20natural%20language%20processing%2C%20has%20recently%20shown%20great%20potential%20in%20the%0Aapplication%20of%20tactile%20perception.%20This%20review%20aims%20to%20comprehensively%20outline%0Athe%20application%20and%20development%20of%20Transformers%20in%20tactile%20technology.%20We%20first%0Aintroduce%20the%20two%20fundamental%20concepts%20behind%20the%20success%20of%20the%20Transformer%3A%0Athe%20self-attention%20mechanism%20and%20large-scale%20pre-training.%20Then%2C%20we%20delve%20into%0Athe%20application%20of%20Transformers%20in%20various%20tactile%20tasks%2C%20including%20but%20not%0Alimited%20to%20object%20recognition%2C%20cross-modal%20generation%2C%20and%20object%20manipulation%2C%0Aoffering%20a%20concise%20summary%20of%20the%20core%20methodologies%2C%20performance%20benchmarks%2C%0Aand%20design%20highlights.%20Finally%2C%20we%20suggest%20potential%20areas%20for%20further%20research%0Aand%20future%20work%2C%20aiming%20to%20generate%20more%20interest%20within%20the%20community%2C%20tackle%0Aexisting%20challenges%2C%20and%20encourage%20the%20use%20of%20Transformer%20models%20in%20the%20tactile%0Afield.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12779v1&entry.124074799=Read"},
{"title": "Assessing the Efficacy of Invisible Watermarks in AI-Generated Medical\n  Images", "author": "Xiaodan Xing and Huiyu Zhou and Yingying Fang and Guang Yang", "abstract": "  AI-generated medical images are gaining growing popularity due to their\npotential to address the data scarcity challenge in the real world. However,\nthe issue of accurate identification of these synthetic images, particularly\nwhen they exhibit remarkable realism with their real copies, remains a concern.\nTo mitigate this challenge, image generators such as DALLE and Imagen, have\nintegrated digital watermarks aimed at facilitating the discernment of\nsynthetic images' authenticity. These watermarks are embedded within the image\npixels and are invisible to the human eye while remains their detectability.\nNevertheless, a comprehensive investigation into the potential impact of these\ninvisible watermarks on the utility of synthetic medical images has been\nlacking. In this study, we propose the incorporation of invisible watermarks\ninto synthetic medical images and seek to evaluate their efficacy in the\ncontext of downstream classification tasks. Our goal is to pave the way for\ndiscussions on the viability of such watermarks in boosting the detectability\nof synthetic medical images, fortifying ethical standards, and safeguarding\nagainst data pollution and potential scams.\n", "link": "http://arxiv.org/abs/2402.03473v3", "date": "2024-05-21", "relevancy": 2.0352, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5248}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5097}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20the%20Efficacy%20of%20Invisible%20Watermarks%20in%20AI-Generated%20Medical%0A%20%20Images&body=Title%3A%20Assessing%20the%20Efficacy%20of%20Invisible%20Watermarks%20in%20AI-Generated%20Medical%0A%20%20Images%0AAuthor%3A%20Xiaodan%20Xing%20and%20Huiyu%20Zhou%20and%20Yingying%20Fang%20and%20Guang%20Yang%0AAbstract%3A%20%20%20AI-generated%20medical%20images%20are%20gaining%20growing%20popularity%20due%20to%20their%0Apotential%20to%20address%20the%20data%20scarcity%20challenge%20in%20the%20real%20world.%20However%2C%0Athe%20issue%20of%20accurate%20identification%20of%20these%20synthetic%20images%2C%20particularly%0Awhen%20they%20exhibit%20remarkable%20realism%20with%20their%20real%20copies%2C%20remains%20a%20concern.%0ATo%20mitigate%20this%20challenge%2C%20image%20generators%20such%20as%20DALLE%20and%20Imagen%2C%20have%0Aintegrated%20digital%20watermarks%20aimed%20at%20facilitating%20the%20discernment%20of%0Asynthetic%20images%27%20authenticity.%20These%20watermarks%20are%20embedded%20within%20the%20image%0Apixels%20and%20are%20invisible%20to%20the%20human%20eye%20while%20remains%20their%20detectability.%0ANevertheless%2C%20a%20comprehensive%20investigation%20into%20the%20potential%20impact%20of%20these%0Ainvisible%20watermarks%20on%20the%20utility%20of%20synthetic%20medical%20images%20has%20been%0Alacking.%20In%20this%20study%2C%20we%20propose%20the%20incorporation%20of%20invisible%20watermarks%0Ainto%20synthetic%20medical%20images%20and%20seek%20to%20evaluate%20their%20efficacy%20in%20the%0Acontext%20of%20downstream%20classification%20tasks.%20Our%20goal%20is%20to%20pave%20the%20way%20for%0Adiscussions%20on%20the%20viability%20of%20such%20watermarks%20in%20boosting%20the%20detectability%0Aof%20synthetic%20medical%20images%2C%20fortifying%20ethical%20standards%2C%20and%20safeguarding%0Aagainst%20data%20pollution%20and%20potential%20scams.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03473v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520the%2520Efficacy%2520of%2520Invisible%2520Watermarks%2520in%2520AI-Generated%2520Medical%250A%2520%2520Images%26entry.906535625%3DXiaodan%2520Xing%2520and%2520Huiyu%2520Zhou%2520and%2520Yingying%2520Fang%2520and%2520Guang%2520Yang%26entry.1292438233%3D%2520%2520AI-generated%2520medical%2520images%2520are%2520gaining%2520growing%2520popularity%2520due%2520to%2520their%250Apotential%2520to%2520address%2520the%2520data%2520scarcity%2520challenge%2520in%2520the%2520real%2520world.%2520However%252C%250Athe%2520issue%2520of%2520accurate%2520identification%2520of%2520these%2520synthetic%2520images%252C%2520particularly%250Awhen%2520they%2520exhibit%2520remarkable%2520realism%2520with%2520their%2520real%2520copies%252C%2520remains%2520a%2520concern.%250ATo%2520mitigate%2520this%2520challenge%252C%2520image%2520generators%2520such%2520as%2520DALLE%2520and%2520Imagen%252C%2520have%250Aintegrated%2520digital%2520watermarks%2520aimed%2520at%2520facilitating%2520the%2520discernment%2520of%250Asynthetic%2520images%2527%2520authenticity.%2520These%2520watermarks%2520are%2520embedded%2520within%2520the%2520image%250Apixels%2520and%2520are%2520invisible%2520to%2520the%2520human%2520eye%2520while%2520remains%2520their%2520detectability.%250ANevertheless%252C%2520a%2520comprehensive%2520investigation%2520into%2520the%2520potential%2520impact%2520of%2520these%250Ainvisible%2520watermarks%2520on%2520the%2520utility%2520of%2520synthetic%2520medical%2520images%2520has%2520been%250Alacking.%2520In%2520this%2520study%252C%2520we%2520propose%2520the%2520incorporation%2520of%2520invisible%2520watermarks%250Ainto%2520synthetic%2520medical%2520images%2520and%2520seek%2520to%2520evaluate%2520their%2520efficacy%2520in%2520the%250Acontext%2520of%2520downstream%2520classification%2520tasks.%2520Our%2520goal%2520is%2520to%2520pave%2520the%2520way%2520for%250Adiscussions%2520on%2520the%2520viability%2520of%2520such%2520watermarks%2520in%2520boosting%2520the%2520detectability%250Aof%2520synthetic%2520medical%2520images%252C%2520fortifying%2520ethical%2520standards%252C%2520and%2520safeguarding%250Aagainst%2520data%2520pollution%2520and%2520potential%2520scams.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03473v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20the%20Efficacy%20of%20Invisible%20Watermarks%20in%20AI-Generated%20Medical%0A%20%20Images&entry.906535625=Xiaodan%20Xing%20and%20Huiyu%20Zhou%20and%20Yingying%20Fang%20and%20Guang%20Yang&entry.1292438233=%20%20AI-generated%20medical%20images%20are%20gaining%20growing%20popularity%20due%20to%20their%0Apotential%20to%20address%20the%20data%20scarcity%20challenge%20in%20the%20real%20world.%20However%2C%0Athe%20issue%20of%20accurate%20identification%20of%20these%20synthetic%20images%2C%20particularly%0Awhen%20they%20exhibit%20remarkable%20realism%20with%20their%20real%20copies%2C%20remains%20a%20concern.%0ATo%20mitigate%20this%20challenge%2C%20image%20generators%20such%20as%20DALLE%20and%20Imagen%2C%20have%0Aintegrated%20digital%20watermarks%20aimed%20at%20facilitating%20the%20discernment%20of%0Asynthetic%20images%27%20authenticity.%20These%20watermarks%20are%20embedded%20within%20the%20image%0Apixels%20and%20are%20invisible%20to%20the%20human%20eye%20while%20remains%20their%20detectability.%0ANevertheless%2C%20a%20comprehensive%20investigation%20into%20the%20potential%20impact%20of%20these%0Ainvisible%20watermarks%20on%20the%20utility%20of%20synthetic%20medical%20images%20has%20been%0Alacking.%20In%20this%20study%2C%20we%20propose%20the%20incorporation%20of%20invisible%20watermarks%0Ainto%20synthetic%20medical%20images%20and%20seek%20to%20evaluate%20their%20efficacy%20in%20the%0Acontext%20of%20downstream%20classification%20tasks.%20Our%20goal%20is%20to%20pave%20the%20way%20for%0Adiscussions%20on%20the%20viability%20of%20such%20watermarks%20in%20boosting%20the%20detectability%0Aof%20synthetic%20medical%20images%2C%20fortifying%20ethical%20standards%2C%20and%20safeguarding%0Aagainst%20data%20pollution%20and%20potential%20scams.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03473v3&entry.124074799=Read"},
{"title": "Beyond Code Generation: An Observational Study of ChatGPT Usage in\n  Software Engineering Practice", "author": "Ranim Khojah and Mazen Mohamad and Philipp Leitner and Francisco Gomes de Oliveira Neto", "abstract": "  Large Language Models (LLMs) are frequently discussed in academia and the\ngeneral public as support tools for virtually any use case that relies on the\nproduction of text, including software engineering. Currently there is much\ndebate, but little empirical evidence, regarding the practical usefulness of\nLLM-based tools such as ChatGPT for engineers in industry. We conduct an\nobservational study of 24 professional software engineers who have been using\nChatGPT over a period of one week in their jobs, and qualitatively analyse\ntheir dialogues with the chatbot as well as their overall experience (as\ncaptured by an exit survey). We find that, rather than expecting ChatGPT to\ngenerate ready-to-use software artifacts (e.g., code), practitioners more often\nuse ChatGPT to receive guidance on how to solve their tasks or learn about a\ntopic in more abstract terms. We also propose a theoretical framework for how\n(i) purpose of the interaction, (ii) internal factors (e.g., the user's\npersonality), and (iii) external factors (e.g., company policy) together shape\nthe experience (in terms of perceived usefulness and trust). We envision that\nour framework can be used by future research to further the academic discussion\non LLM usage by software engineering practitioners, and to serve as a reference\npoint for the design of future empirical LLM research in this domain.\n", "link": "http://arxiv.org/abs/2404.14901v2", "date": "2024-05-21", "relevancy": 2.0164, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5154}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4973}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Code%20Generation%3A%20An%20Observational%20Study%20of%20ChatGPT%20Usage%20in%0A%20%20Software%20Engineering%20Practice&body=Title%3A%20Beyond%20Code%20Generation%3A%20An%20Observational%20Study%20of%20ChatGPT%20Usage%20in%0A%20%20Software%20Engineering%20Practice%0AAuthor%3A%20Ranim%20Khojah%20and%20Mazen%20Mohamad%20and%20Philipp%20Leitner%20and%20Francisco%20Gomes%20de%20Oliveira%20Neto%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20frequently%20discussed%20in%20academia%20and%20the%0Ageneral%20public%20as%20support%20tools%20for%20virtually%20any%20use%20case%20that%20relies%20on%20the%0Aproduction%20of%20text%2C%20including%20software%20engineering.%20Currently%20there%20is%20much%0Adebate%2C%20but%20little%20empirical%20evidence%2C%20regarding%20the%20practical%20usefulness%20of%0ALLM-based%20tools%20such%20as%20ChatGPT%20for%20engineers%20in%20industry.%20We%20conduct%20an%0Aobservational%20study%20of%2024%20professional%20software%20engineers%20who%20have%20been%20using%0AChatGPT%20over%20a%20period%20of%20one%20week%20in%20their%20jobs%2C%20and%20qualitatively%20analyse%0Atheir%20dialogues%20with%20the%20chatbot%20as%20well%20as%20their%20overall%20experience%20%28as%0Acaptured%20by%20an%20exit%20survey%29.%20We%20find%20that%2C%20rather%20than%20expecting%20ChatGPT%20to%0Agenerate%20ready-to-use%20software%20artifacts%20%28e.g.%2C%20code%29%2C%20practitioners%20more%20often%0Ause%20ChatGPT%20to%20receive%20guidance%20on%20how%20to%20solve%20their%20tasks%20or%20learn%20about%20a%0Atopic%20in%20more%20abstract%20terms.%20We%20also%20propose%20a%20theoretical%20framework%20for%20how%0A%28i%29%20purpose%20of%20the%20interaction%2C%20%28ii%29%20internal%20factors%20%28e.g.%2C%20the%20user%27s%0Apersonality%29%2C%20and%20%28iii%29%20external%20factors%20%28e.g.%2C%20company%20policy%29%20together%20shape%0Athe%20experience%20%28in%20terms%20of%20perceived%20usefulness%20and%20trust%29.%20We%20envision%20that%0Aour%20framework%20can%20be%20used%20by%20future%20research%20to%20further%20the%20academic%20discussion%0Aon%20LLM%20usage%20by%20software%20engineering%20practitioners%2C%20and%20to%20serve%20as%20a%20reference%0Apoint%20for%20the%20design%20of%20future%20empirical%20LLM%20research%20in%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14901v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Code%2520Generation%253A%2520An%2520Observational%2520Study%2520of%2520ChatGPT%2520Usage%2520in%250A%2520%2520Software%2520Engineering%2520Practice%26entry.906535625%3DRanim%2520Khojah%2520and%2520Mazen%2520Mohamad%2520and%2520Philipp%2520Leitner%2520and%2520Francisco%2520Gomes%2520de%2520Oliveira%2520Neto%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520frequently%2520discussed%2520in%2520academia%2520and%2520the%250Ageneral%2520public%2520as%2520support%2520tools%2520for%2520virtually%2520any%2520use%2520case%2520that%2520relies%2520on%2520the%250Aproduction%2520of%2520text%252C%2520including%2520software%2520engineering.%2520Currently%2520there%2520is%2520much%250Adebate%252C%2520but%2520little%2520empirical%2520evidence%252C%2520regarding%2520the%2520practical%2520usefulness%2520of%250ALLM-based%2520tools%2520such%2520as%2520ChatGPT%2520for%2520engineers%2520in%2520industry.%2520We%2520conduct%2520an%250Aobservational%2520study%2520of%252024%2520professional%2520software%2520engineers%2520who%2520have%2520been%2520using%250AChatGPT%2520over%2520a%2520period%2520of%2520one%2520week%2520in%2520their%2520jobs%252C%2520and%2520qualitatively%2520analyse%250Atheir%2520dialogues%2520with%2520the%2520chatbot%2520as%2520well%2520as%2520their%2520overall%2520experience%2520%2528as%250Acaptured%2520by%2520an%2520exit%2520survey%2529.%2520We%2520find%2520that%252C%2520rather%2520than%2520expecting%2520ChatGPT%2520to%250Agenerate%2520ready-to-use%2520software%2520artifacts%2520%2528e.g.%252C%2520code%2529%252C%2520practitioners%2520more%2520often%250Ause%2520ChatGPT%2520to%2520receive%2520guidance%2520on%2520how%2520to%2520solve%2520their%2520tasks%2520or%2520learn%2520about%2520a%250Atopic%2520in%2520more%2520abstract%2520terms.%2520We%2520also%2520propose%2520a%2520theoretical%2520framework%2520for%2520how%250A%2528i%2529%2520purpose%2520of%2520the%2520interaction%252C%2520%2528ii%2529%2520internal%2520factors%2520%2528e.g.%252C%2520the%2520user%2527s%250Apersonality%2529%252C%2520and%2520%2528iii%2529%2520external%2520factors%2520%2528e.g.%252C%2520company%2520policy%2529%2520together%2520shape%250Athe%2520experience%2520%2528in%2520terms%2520of%2520perceived%2520usefulness%2520and%2520trust%2529.%2520We%2520envision%2520that%250Aour%2520framework%2520can%2520be%2520used%2520by%2520future%2520research%2520to%2520further%2520the%2520academic%2520discussion%250Aon%2520LLM%2520usage%2520by%2520software%2520engineering%2520practitioners%252C%2520and%2520to%2520serve%2520as%2520a%2520reference%250Apoint%2520for%2520the%2520design%2520of%2520future%2520empirical%2520LLM%2520research%2520in%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14901v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Code%20Generation%3A%20An%20Observational%20Study%20of%20ChatGPT%20Usage%20in%0A%20%20Software%20Engineering%20Practice&entry.906535625=Ranim%20Khojah%20and%20Mazen%20Mohamad%20and%20Philipp%20Leitner%20and%20Francisco%20Gomes%20de%20Oliveira%20Neto&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20frequently%20discussed%20in%20academia%20and%20the%0Ageneral%20public%20as%20support%20tools%20for%20virtually%20any%20use%20case%20that%20relies%20on%20the%0Aproduction%20of%20text%2C%20including%20software%20engineering.%20Currently%20there%20is%20much%0Adebate%2C%20but%20little%20empirical%20evidence%2C%20regarding%20the%20practical%20usefulness%20of%0ALLM-based%20tools%20such%20as%20ChatGPT%20for%20engineers%20in%20industry.%20We%20conduct%20an%0Aobservational%20study%20of%2024%20professional%20software%20engineers%20who%20have%20been%20using%0AChatGPT%20over%20a%20period%20of%20one%20week%20in%20their%20jobs%2C%20and%20qualitatively%20analyse%0Atheir%20dialogues%20with%20the%20chatbot%20as%20well%20as%20their%20overall%20experience%20%28as%0Acaptured%20by%20an%20exit%20survey%29.%20We%20find%20that%2C%20rather%20than%20expecting%20ChatGPT%20to%0Agenerate%20ready-to-use%20software%20artifacts%20%28e.g.%2C%20code%29%2C%20practitioners%20more%20often%0Ause%20ChatGPT%20to%20receive%20guidance%20on%20how%20to%20solve%20their%20tasks%20or%20learn%20about%20a%0Atopic%20in%20more%20abstract%20terms.%20We%20also%20propose%20a%20theoretical%20framework%20for%20how%0A%28i%29%20purpose%20of%20the%20interaction%2C%20%28ii%29%20internal%20factors%20%28e.g.%2C%20the%20user%27s%0Apersonality%29%2C%20and%20%28iii%29%20external%20factors%20%28e.g.%2C%20company%20policy%29%20together%20shape%0Athe%20experience%20%28in%20terms%20of%20perceived%20usefulness%20and%20trust%29.%20We%20envision%20that%0Aour%20framework%20can%20be%20used%20by%20future%20research%20to%20further%20the%20academic%20discussion%0Aon%20LLM%20usage%20by%20software%20engineering%20practitioners%2C%20and%20to%20serve%20as%20a%20reference%0Apoint%20for%20the%20design%20of%20future%20empirical%20LLM%20research%20in%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14901v2&entry.124074799=Read"},
{"title": "Spatial-aware Attention Generative Adversarial Network for\n  Semi-supervised Anomaly Detection in Medical Image", "author": "Zerui Zhang and Zhichao Sun and Zelong Liu and Bo Du and Rui Yu and Zhou Zhao and Yongchao Xu", "abstract": "  Medical anomaly detection is a critical research area aimed at recognizing\nabnormal images to aid in diagnosis.Most existing methods adopt synthetic\nanomalies and image restoration on normal samples to detect anomaly. The\nunlabeled data consisting of both normal and abnormal data is not well\nexplored. We introduce a novel Spatial-aware Attention Generative Adversarial\nNetwork (SAGAN) for one-class semi-supervised generation of health images.Our\ncore insight is the utilization of position encoding and attention to\naccurately focus on restoring abnormal regions and preserving normal regions.\nTo fully utilize the unlabelled data, SAGAN relaxes the cyclic consistency\nrequirement of the existing unpaired image-to-image conversion methods, and\ngenerates high-quality health images corresponding to unlabeled data, guided by\nthe reconstruction of normal images and restoration of pseudo-anomaly\nimages.Subsequently, the discrepancy between the generated healthy image and\nthe original image is utilized as an anomaly score.Extensive experiments on\nthree medical datasets demonstrate that the proposed SAGAN outperforms the\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2405.12872v1", "date": "2024-05-21", "relevancy": 2.0129, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5058}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5025}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial-aware%20Attention%20Generative%20Adversarial%20Network%20for%0A%20%20Semi-supervised%20Anomaly%20Detection%20in%20Medical%20Image&body=Title%3A%20Spatial-aware%20Attention%20Generative%20Adversarial%20Network%20for%0A%20%20Semi-supervised%20Anomaly%20Detection%20in%20Medical%20Image%0AAuthor%3A%20Zerui%20Zhang%20and%20Zhichao%20Sun%20and%20Zelong%20Liu%20and%20Bo%20Du%20and%20Rui%20Yu%20and%20Zhou%20Zhao%20and%20Yongchao%20Xu%0AAbstract%3A%20%20%20Medical%20anomaly%20detection%20is%20a%20critical%20research%20area%20aimed%20at%20recognizing%0Aabnormal%20images%20to%20aid%20in%20diagnosis.Most%20existing%20methods%20adopt%20synthetic%0Aanomalies%20and%20image%20restoration%20on%20normal%20samples%20to%20detect%20anomaly.%20The%0Aunlabeled%20data%20consisting%20of%20both%20normal%20and%20abnormal%20data%20is%20not%20well%0Aexplored.%20We%20introduce%20a%20novel%20Spatial-aware%20Attention%20Generative%20Adversarial%0ANetwork%20%28SAGAN%29%20for%20one-class%20semi-supervised%20generation%20of%20health%20images.Our%0Acore%20insight%20is%20the%20utilization%20of%20position%20encoding%20and%20attention%20to%0Aaccurately%20focus%20on%20restoring%20abnormal%20regions%20and%20preserving%20normal%20regions.%0ATo%20fully%20utilize%20the%20unlabelled%20data%2C%20SAGAN%20relaxes%20the%20cyclic%20consistency%0Arequirement%20of%20the%20existing%20unpaired%20image-to-image%20conversion%20methods%2C%20and%0Agenerates%20high-quality%20health%20images%20corresponding%20to%20unlabeled%20data%2C%20guided%20by%0Athe%20reconstruction%20of%20normal%20images%20and%20restoration%20of%20pseudo-anomaly%0Aimages.Subsequently%2C%20the%20discrepancy%20between%20the%20generated%20healthy%20image%20and%0Athe%20original%20image%20is%20utilized%20as%20an%20anomaly%20score.Extensive%20experiments%20on%0Athree%20medical%20datasets%20demonstrate%20that%20the%20proposed%20SAGAN%20outperforms%20the%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12872v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial-aware%2520Attention%2520Generative%2520Adversarial%2520Network%2520for%250A%2520%2520Semi-supervised%2520Anomaly%2520Detection%2520in%2520Medical%2520Image%26entry.906535625%3DZerui%2520Zhang%2520and%2520Zhichao%2520Sun%2520and%2520Zelong%2520Liu%2520and%2520Bo%2520Du%2520and%2520Rui%2520Yu%2520and%2520Zhou%2520Zhao%2520and%2520Yongchao%2520Xu%26entry.1292438233%3D%2520%2520Medical%2520anomaly%2520detection%2520is%2520a%2520critical%2520research%2520area%2520aimed%2520at%2520recognizing%250Aabnormal%2520images%2520to%2520aid%2520in%2520diagnosis.Most%2520existing%2520methods%2520adopt%2520synthetic%250Aanomalies%2520and%2520image%2520restoration%2520on%2520normal%2520samples%2520to%2520detect%2520anomaly.%2520The%250Aunlabeled%2520data%2520consisting%2520of%2520both%2520normal%2520and%2520abnormal%2520data%2520is%2520not%2520well%250Aexplored.%2520We%2520introduce%2520a%2520novel%2520Spatial-aware%2520Attention%2520Generative%2520Adversarial%250ANetwork%2520%2528SAGAN%2529%2520for%2520one-class%2520semi-supervised%2520generation%2520of%2520health%2520images.Our%250Acore%2520insight%2520is%2520the%2520utilization%2520of%2520position%2520encoding%2520and%2520attention%2520to%250Aaccurately%2520focus%2520on%2520restoring%2520abnormal%2520regions%2520and%2520preserving%2520normal%2520regions.%250ATo%2520fully%2520utilize%2520the%2520unlabelled%2520data%252C%2520SAGAN%2520relaxes%2520the%2520cyclic%2520consistency%250Arequirement%2520of%2520the%2520existing%2520unpaired%2520image-to-image%2520conversion%2520methods%252C%2520and%250Agenerates%2520high-quality%2520health%2520images%2520corresponding%2520to%2520unlabeled%2520data%252C%2520guided%2520by%250Athe%2520reconstruction%2520of%2520normal%2520images%2520and%2520restoration%2520of%2520pseudo-anomaly%250Aimages.Subsequently%252C%2520the%2520discrepancy%2520between%2520the%2520generated%2520healthy%2520image%2520and%250Athe%2520original%2520image%2520is%2520utilized%2520as%2520an%2520anomaly%2520score.Extensive%2520experiments%2520on%250Athree%2520medical%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520SAGAN%2520outperforms%2520the%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12872v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial-aware%20Attention%20Generative%20Adversarial%20Network%20for%0A%20%20Semi-supervised%20Anomaly%20Detection%20in%20Medical%20Image&entry.906535625=Zerui%20Zhang%20and%20Zhichao%20Sun%20and%20Zelong%20Liu%20and%20Bo%20Du%20and%20Rui%20Yu%20and%20Zhou%20Zhao%20and%20Yongchao%20Xu&entry.1292438233=%20%20Medical%20anomaly%20detection%20is%20a%20critical%20research%20area%20aimed%20at%20recognizing%0Aabnormal%20images%20to%20aid%20in%20diagnosis.Most%20existing%20methods%20adopt%20synthetic%0Aanomalies%20and%20image%20restoration%20on%20normal%20samples%20to%20detect%20anomaly.%20The%0Aunlabeled%20data%20consisting%20of%20both%20normal%20and%20abnormal%20data%20is%20not%20well%0Aexplored.%20We%20introduce%20a%20novel%20Spatial-aware%20Attention%20Generative%20Adversarial%0ANetwork%20%28SAGAN%29%20for%20one-class%20semi-supervised%20generation%20of%20health%20images.Our%0Acore%20insight%20is%20the%20utilization%20of%20position%20encoding%20and%20attention%20to%0Aaccurately%20focus%20on%20restoring%20abnormal%20regions%20and%20preserving%20normal%20regions.%0ATo%20fully%20utilize%20the%20unlabelled%20data%2C%20SAGAN%20relaxes%20the%20cyclic%20consistency%0Arequirement%20of%20the%20existing%20unpaired%20image-to-image%20conversion%20methods%2C%20and%0Agenerates%20high-quality%20health%20images%20corresponding%20to%20unlabeled%20data%2C%20guided%20by%0Athe%20reconstruction%20of%20normal%20images%20and%20restoration%20of%20pseudo-anomaly%0Aimages.Subsequently%2C%20the%20discrepancy%20between%20the%20generated%20healthy%20image%20and%0Athe%20original%20image%20is%20utilized%20as%20an%20anomaly%20score.Extensive%20experiments%20on%0Athree%20medical%20datasets%20demonstrate%20that%20the%20proposed%20SAGAN%20outperforms%20the%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12872v1&entry.124074799=Read"},
{"title": "Joint Identity Verification and Pose Alignment for Partial Fingerprints", "author": "Xiongjun Guan and Zhiyu Pan and Jianjiang Feng and Jie Zhou", "abstract": "  Currently, portable electronic devices are becoming more and more popular.\nFor lightweight considerations, their fingerprint recognition modules usually\nuse limited-size sensors. However, partial fingerprints have few matchable\nfeatures, especially when there are differences in finger pressing posture or\nimage quality, which makes partial fingerprint verification challenging. Most\nexisting methods regard fingerprint position rectification and identity\nverification as independent tasks, ignoring the coupling relationship between\nthem -- relative pose estimation typically relies on paired features as\nanchors, and authentication accuracy tends to improve with more precise pose\nalignment. In this paper, we propose a novel framework for joint identity\nverification and pose alignment of partial fingerprint pairs, aiming to\nleverage their inherent correlation to improve each other. To achieve this, we\npresent a multi-task CNN (Convolutional Neural Network)-Transformer hybrid\nnetwork, and design a pre-training task to enhance the feature extraction\ncapability. Experiments on multiple public datasets (NIST SD14, FVC2002 DB1A &\nDB3A, FVC2004 DB1A & DB2A, FVC2006 DB1A) and an in-house dataset show that our\nmethod achieves state-of-the-art performance in both partial fingerprint\nverification and relative pose estimation, while being more efficient than\nprevious methods.\n", "link": "http://arxiv.org/abs/2405.03959v3", "date": "2024-05-21", "relevancy": 2.0126, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5234}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.4911}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Identity%20Verification%20and%20Pose%20Alignment%20for%20Partial%20Fingerprints&body=Title%3A%20Joint%20Identity%20Verification%20and%20Pose%20Alignment%20for%20Partial%20Fingerprints%0AAuthor%3A%20Xiongjun%20Guan%20and%20Zhiyu%20Pan%20and%20Jianjiang%20Feng%20and%20Jie%20Zhou%0AAbstract%3A%20%20%20Currently%2C%20portable%20electronic%20devices%20are%20becoming%20more%20and%20more%20popular.%0AFor%20lightweight%20considerations%2C%20their%20fingerprint%20recognition%20modules%20usually%0Ause%20limited-size%20sensors.%20However%2C%20partial%20fingerprints%20have%20few%20matchable%0Afeatures%2C%20especially%20when%20there%20are%20differences%20in%20finger%20pressing%20posture%20or%0Aimage%20quality%2C%20which%20makes%20partial%20fingerprint%20verification%20challenging.%20Most%0Aexisting%20methods%20regard%20fingerprint%20position%20rectification%20and%20identity%0Averification%20as%20independent%20tasks%2C%20ignoring%20the%20coupling%20relationship%20between%0Athem%20--%20relative%20pose%20estimation%20typically%20relies%20on%20paired%20features%20as%0Aanchors%2C%20and%20authentication%20accuracy%20tends%20to%20improve%20with%20more%20precise%20pose%0Aalignment.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20for%20joint%20identity%0Averification%20and%20pose%20alignment%20of%20partial%20fingerprint%20pairs%2C%20aiming%20to%0Aleverage%20their%20inherent%20correlation%20to%20improve%20each%20other.%20To%20achieve%20this%2C%20we%0Apresent%20a%20multi-task%20CNN%20%28Convolutional%20Neural%20Network%29-Transformer%20hybrid%0Anetwork%2C%20and%20design%20a%20pre-training%20task%20to%20enhance%20the%20feature%20extraction%0Acapability.%20Experiments%20on%20multiple%20public%20datasets%20%28NIST%20SD14%2C%20FVC2002%20DB1A%20%26%0ADB3A%2C%20FVC2004%20DB1A%20%26%20DB2A%2C%20FVC2006%20DB1A%29%20and%20an%20in-house%20dataset%20show%20that%20our%0Amethod%20achieves%20state-of-the-art%20performance%20in%20both%20partial%20fingerprint%0Averification%20and%20relative%20pose%20estimation%2C%20while%20being%20more%20efficient%20than%0Aprevious%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03959v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Identity%2520Verification%2520and%2520Pose%2520Alignment%2520for%2520Partial%2520Fingerprints%26entry.906535625%3DXiongjun%2520Guan%2520and%2520Zhiyu%2520Pan%2520and%2520Jianjiang%2520Feng%2520and%2520Jie%2520Zhou%26entry.1292438233%3D%2520%2520Currently%252C%2520portable%2520electronic%2520devices%2520are%2520becoming%2520more%2520and%2520more%2520popular.%250AFor%2520lightweight%2520considerations%252C%2520their%2520fingerprint%2520recognition%2520modules%2520usually%250Ause%2520limited-size%2520sensors.%2520However%252C%2520partial%2520fingerprints%2520have%2520few%2520matchable%250Afeatures%252C%2520especially%2520when%2520there%2520are%2520differences%2520in%2520finger%2520pressing%2520posture%2520or%250Aimage%2520quality%252C%2520which%2520makes%2520partial%2520fingerprint%2520verification%2520challenging.%2520Most%250Aexisting%2520methods%2520regard%2520fingerprint%2520position%2520rectification%2520and%2520identity%250Averification%2520as%2520independent%2520tasks%252C%2520ignoring%2520the%2520coupling%2520relationship%2520between%250Athem%2520--%2520relative%2520pose%2520estimation%2520typically%2520relies%2520on%2520paired%2520features%2520as%250Aanchors%252C%2520and%2520authentication%2520accuracy%2520tends%2520to%2520improve%2520with%2520more%2520precise%2520pose%250Aalignment.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520framework%2520for%2520joint%2520identity%250Averification%2520and%2520pose%2520alignment%2520of%2520partial%2520fingerprint%2520pairs%252C%2520aiming%2520to%250Aleverage%2520their%2520inherent%2520correlation%2520to%2520improve%2520each%2520other.%2520To%2520achieve%2520this%252C%2520we%250Apresent%2520a%2520multi-task%2520CNN%2520%2528Convolutional%2520Neural%2520Network%2529-Transformer%2520hybrid%250Anetwork%252C%2520and%2520design%2520a%2520pre-training%2520task%2520to%2520enhance%2520the%2520feature%2520extraction%250Acapability.%2520Experiments%2520on%2520multiple%2520public%2520datasets%2520%2528NIST%2520SD14%252C%2520FVC2002%2520DB1A%2520%2526%250ADB3A%252C%2520FVC2004%2520DB1A%2520%2526%2520DB2A%252C%2520FVC2006%2520DB1A%2529%2520and%2520an%2520in-house%2520dataset%2520show%2520that%2520our%250Amethod%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%2520partial%2520fingerprint%250Averification%2520and%2520relative%2520pose%2520estimation%252C%2520while%2520being%2520more%2520efficient%2520than%250Aprevious%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03959v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Identity%20Verification%20and%20Pose%20Alignment%20for%20Partial%20Fingerprints&entry.906535625=Xiongjun%20Guan%20and%20Zhiyu%20Pan%20and%20Jianjiang%20Feng%20and%20Jie%20Zhou&entry.1292438233=%20%20Currently%2C%20portable%20electronic%20devices%20are%20becoming%20more%20and%20more%20popular.%0AFor%20lightweight%20considerations%2C%20their%20fingerprint%20recognition%20modules%20usually%0Ause%20limited-size%20sensors.%20However%2C%20partial%20fingerprints%20have%20few%20matchable%0Afeatures%2C%20especially%20when%20there%20are%20differences%20in%20finger%20pressing%20posture%20or%0Aimage%20quality%2C%20which%20makes%20partial%20fingerprint%20verification%20challenging.%20Most%0Aexisting%20methods%20regard%20fingerprint%20position%20rectification%20and%20identity%0Averification%20as%20independent%20tasks%2C%20ignoring%20the%20coupling%20relationship%20between%0Athem%20--%20relative%20pose%20estimation%20typically%20relies%20on%20paired%20features%20as%0Aanchors%2C%20and%20authentication%20accuracy%20tends%20to%20improve%20with%20more%20precise%20pose%0Aalignment.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20for%20joint%20identity%0Averification%20and%20pose%20alignment%20of%20partial%20fingerprint%20pairs%2C%20aiming%20to%0Aleverage%20their%20inherent%20correlation%20to%20improve%20each%20other.%20To%20achieve%20this%2C%20we%0Apresent%20a%20multi-task%20CNN%20%28Convolutional%20Neural%20Network%29-Transformer%20hybrid%0Anetwork%2C%20and%20design%20a%20pre-training%20task%20to%20enhance%20the%20feature%20extraction%0Acapability.%20Experiments%20on%20multiple%20public%20datasets%20%28NIST%20SD14%2C%20FVC2002%20DB1A%20%26%0ADB3A%2C%20FVC2004%20DB1A%20%26%20DB2A%2C%20FVC2006%20DB1A%29%20and%20an%20in-house%20dataset%20show%20that%20our%0Amethod%20achieves%20state-of-the-art%20performance%20in%20both%20partial%20fingerprint%0Averification%20and%20relative%20pose%20estimation%2C%20while%20being%20more%20efficient%20than%0Aprevious%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03959v3&entry.124074799=Read"},
{"title": "Robust Guidance for Unsupervised Data Selection: Capturing Perplexing\n  Named Entities for Domain-Specific Machine Translation", "author": "Seunghyun Ji and Hagai Raja Sinulingga and Darongsae Kwon", "abstract": "  Low-resourced data presents a significant challenge for neural machine\ntranslation. In most cases, the low-resourced environment is caused by high\ncosts due to the need for domain experts or the lack of language experts.\nTherefore, identifying the most training-efficient data within an unsupervised\nsetting emerges as a practical strategy. Recent research suggests that such\neffective data can be identified by selecting 'appropriately complex data'\nbased on its volume, providing strong intuition for unsupervised data\nselection. However, we have discovered that establishing criteria for\nunsupervised data selection remains a challenge, as the 'appropriate level of\ndifficulty' may vary depending on the data domain. We introduce a novel\nunsupervised data selection method named 'Capturing Perplexing Named Entities,'\nwhich leverages the maximum inference entropy in translated named entities as a\nmetric for selection. When tested with the 'Korean-English Parallel Corpus of\nSpecialized Domains,' our method served as robust guidance for identifying\ntraining-efficient data across different domains, in contrast to existing\nmethods.\n", "link": "http://arxiv.org/abs/2402.19267v2", "date": "2024-05-21", "relevancy": 2.0008, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5392}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5136}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Guidance%20for%20Unsupervised%20Data%20Selection%3A%20Capturing%20Perplexing%0A%20%20Named%20Entities%20for%20Domain-Specific%20Machine%20Translation&body=Title%3A%20Robust%20Guidance%20for%20Unsupervised%20Data%20Selection%3A%20Capturing%20Perplexing%0A%20%20Named%20Entities%20for%20Domain-Specific%20Machine%20Translation%0AAuthor%3A%20Seunghyun%20Ji%20and%20Hagai%20Raja%20Sinulingga%20and%20Darongsae%20Kwon%0AAbstract%3A%20%20%20Low-resourced%20data%20presents%20a%20significant%20challenge%20for%20neural%20machine%0Atranslation.%20In%20most%20cases%2C%20the%20low-resourced%20environment%20is%20caused%20by%20high%0Acosts%20due%20to%20the%20need%20for%20domain%20experts%20or%20the%20lack%20of%20language%20experts.%0ATherefore%2C%20identifying%20the%20most%20training-efficient%20data%20within%20an%20unsupervised%0Asetting%20emerges%20as%20a%20practical%20strategy.%20Recent%20research%20suggests%20that%20such%0Aeffective%20data%20can%20be%20identified%20by%20selecting%20%27appropriately%20complex%20data%27%0Abased%20on%20its%20volume%2C%20providing%20strong%20intuition%20for%20unsupervised%20data%0Aselection.%20However%2C%20we%20have%20discovered%20that%20establishing%20criteria%20for%0Aunsupervised%20data%20selection%20remains%20a%20challenge%2C%20as%20the%20%27appropriate%20level%20of%0Adifficulty%27%20may%20vary%20depending%20on%20the%20data%20domain.%20We%20introduce%20a%20novel%0Aunsupervised%20data%20selection%20method%20named%20%27Capturing%20Perplexing%20Named%20Entities%2C%27%0Awhich%20leverages%20the%20maximum%20inference%20entropy%20in%20translated%20named%20entities%20as%20a%0Ametric%20for%20selection.%20When%20tested%20with%20the%20%27Korean-English%20Parallel%20Corpus%20of%0ASpecialized%20Domains%2C%27%20our%20method%20served%20as%20robust%20guidance%20for%20identifying%0Atraining-efficient%20data%20across%20different%20domains%2C%20in%20contrast%20to%20existing%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19267v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Guidance%2520for%2520Unsupervised%2520Data%2520Selection%253A%2520Capturing%2520Perplexing%250A%2520%2520Named%2520Entities%2520for%2520Domain-Specific%2520Machine%2520Translation%26entry.906535625%3DSeunghyun%2520Ji%2520and%2520Hagai%2520Raja%2520Sinulingga%2520and%2520Darongsae%2520Kwon%26entry.1292438233%3D%2520%2520Low-resourced%2520data%2520presents%2520a%2520significant%2520challenge%2520for%2520neural%2520machine%250Atranslation.%2520In%2520most%2520cases%252C%2520the%2520low-resourced%2520environment%2520is%2520caused%2520by%2520high%250Acosts%2520due%2520to%2520the%2520need%2520for%2520domain%2520experts%2520or%2520the%2520lack%2520of%2520language%2520experts.%250ATherefore%252C%2520identifying%2520the%2520most%2520training-efficient%2520data%2520within%2520an%2520unsupervised%250Asetting%2520emerges%2520as%2520a%2520practical%2520strategy.%2520Recent%2520research%2520suggests%2520that%2520such%250Aeffective%2520data%2520can%2520be%2520identified%2520by%2520selecting%2520%2527appropriately%2520complex%2520data%2527%250Abased%2520on%2520its%2520volume%252C%2520providing%2520strong%2520intuition%2520for%2520unsupervised%2520data%250Aselection.%2520However%252C%2520we%2520have%2520discovered%2520that%2520establishing%2520criteria%2520for%250Aunsupervised%2520data%2520selection%2520remains%2520a%2520challenge%252C%2520as%2520the%2520%2527appropriate%2520level%2520of%250Adifficulty%2527%2520may%2520vary%2520depending%2520on%2520the%2520data%2520domain.%2520We%2520introduce%2520a%2520novel%250Aunsupervised%2520data%2520selection%2520method%2520named%2520%2527Capturing%2520Perplexing%2520Named%2520Entities%252C%2527%250Awhich%2520leverages%2520the%2520maximum%2520inference%2520entropy%2520in%2520translated%2520named%2520entities%2520as%2520a%250Ametric%2520for%2520selection.%2520When%2520tested%2520with%2520the%2520%2527Korean-English%2520Parallel%2520Corpus%2520of%250ASpecialized%2520Domains%252C%2527%2520our%2520method%2520served%2520as%2520robust%2520guidance%2520for%2520identifying%250Atraining-efficient%2520data%2520across%2520different%2520domains%252C%2520in%2520contrast%2520to%2520existing%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19267v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Guidance%20for%20Unsupervised%20Data%20Selection%3A%20Capturing%20Perplexing%0A%20%20Named%20Entities%20for%20Domain-Specific%20Machine%20Translation&entry.906535625=Seunghyun%20Ji%20and%20Hagai%20Raja%20Sinulingga%20and%20Darongsae%20Kwon&entry.1292438233=%20%20Low-resourced%20data%20presents%20a%20significant%20challenge%20for%20neural%20machine%0Atranslation.%20In%20most%20cases%2C%20the%20low-resourced%20environment%20is%20caused%20by%20high%0Acosts%20due%20to%20the%20need%20for%20domain%20experts%20or%20the%20lack%20of%20language%20experts.%0ATherefore%2C%20identifying%20the%20most%20training-efficient%20data%20within%20an%20unsupervised%0Asetting%20emerges%20as%20a%20practical%20strategy.%20Recent%20research%20suggests%20that%20such%0Aeffective%20data%20can%20be%20identified%20by%20selecting%20%27appropriately%20complex%20data%27%0Abased%20on%20its%20volume%2C%20providing%20strong%20intuition%20for%20unsupervised%20data%0Aselection.%20However%2C%20we%20have%20discovered%20that%20establishing%20criteria%20for%0Aunsupervised%20data%20selection%20remains%20a%20challenge%2C%20as%20the%20%27appropriate%20level%20of%0Adifficulty%27%20may%20vary%20depending%20on%20the%20data%20domain.%20We%20introduce%20a%20novel%0Aunsupervised%20data%20selection%20method%20named%20%27Capturing%20Perplexing%20Named%20Entities%2C%27%0Awhich%20leverages%20the%20maximum%20inference%20entropy%20in%20translated%20named%20entities%20as%20a%0Ametric%20for%20selection.%20When%20tested%20with%20the%20%27Korean-English%20Parallel%20Corpus%20of%0ASpecialized%20Domains%2C%27%20our%20method%20served%20as%20robust%20guidance%20for%20identifying%0Atraining-efficient%20data%20across%20different%20domains%2C%20in%20contrast%20to%20existing%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19267v2&entry.124074799=Read"},
{"title": "HyperMoE: Towards Better Mixture of Experts via Transferring Among\n  Experts", "author": "Hao Zhao and Zihan Qiu and Huijia Wu and Zili Wang and Zhaofeng He and Jie Fu", "abstract": "  The Mixture of Experts (MoE) for language models has been proven effective in\naugmenting the capacity of models by dynamically routing each input token to a\nspecific subset of experts for processing. Despite the success, most existing\nmethods face a challenge for balance between sparsity and the availability of\nexpert knowledge: enhancing performance through increased use of expert\nknowledge often results in diminishing sparsity during expert selection. To\nmitigate this contradiction, we propose HyperMoE, a novel MoE framework built\nupon Hypernetworks. This framework integrates the computational processes of\nMoE with the concept of knowledge transferring in multi-task learning. Specific\nmodules generated based on the information of unselected experts serve as\nsupplementary information, which allows the knowledge of experts not selected\nto be used while maintaining selection sparsity. Our comprehensive empirical\nevaluations across multiple datasets and backbones establish that HyperMoE\nsignificantly outperforms existing MoE methods under identical conditions\nconcerning the number of experts.\n", "link": "http://arxiv.org/abs/2402.12656v3", "date": "2024-05-21", "relevancy": 1.9931, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5099}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4941}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperMoE%3A%20Towards%20Better%20Mixture%20of%20Experts%20via%20Transferring%20Among%0A%20%20Experts&body=Title%3A%20HyperMoE%3A%20Towards%20Better%20Mixture%20of%20Experts%20via%20Transferring%20Among%0A%20%20Experts%0AAuthor%3A%20Hao%20Zhao%20and%20Zihan%20Qiu%20and%20Huijia%20Wu%20and%20Zili%20Wang%20and%20Zhaofeng%20He%20and%20Jie%20Fu%0AAbstract%3A%20%20%20The%20Mixture%20of%20Experts%20%28MoE%29%20for%20language%20models%20has%20been%20proven%20effective%20in%0Aaugmenting%20the%20capacity%20of%20models%20by%20dynamically%20routing%20each%20input%20token%20to%20a%0Aspecific%20subset%20of%20experts%20for%20processing.%20Despite%20the%20success%2C%20most%20existing%0Amethods%20face%20a%20challenge%20for%20balance%20between%20sparsity%20and%20the%20availability%20of%0Aexpert%20knowledge%3A%20enhancing%20performance%20through%20increased%20use%20of%20expert%0Aknowledge%20often%20results%20in%20diminishing%20sparsity%20during%20expert%20selection.%20To%0Amitigate%20this%20contradiction%2C%20we%20propose%20HyperMoE%2C%20a%20novel%20MoE%20framework%20built%0Aupon%20Hypernetworks.%20This%20framework%20integrates%20the%20computational%20processes%20of%0AMoE%20with%20the%20concept%20of%20knowledge%20transferring%20in%20multi-task%20learning.%20Specific%0Amodules%20generated%20based%20on%20the%20information%20of%20unselected%20experts%20serve%20as%0Asupplementary%20information%2C%20which%20allows%20the%20knowledge%20of%20experts%20not%20selected%0Ato%20be%20used%20while%20maintaining%20selection%20sparsity.%20Our%20comprehensive%20empirical%0Aevaluations%20across%20multiple%20datasets%20and%20backbones%20establish%20that%20HyperMoE%0Asignificantly%20outperforms%20existing%20MoE%20methods%20under%20identical%20conditions%0Aconcerning%20the%20number%20of%20experts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12656v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperMoE%253A%2520Towards%2520Better%2520Mixture%2520of%2520Experts%2520via%2520Transferring%2520Among%250A%2520%2520Experts%26entry.906535625%3DHao%2520Zhao%2520and%2520Zihan%2520Qiu%2520and%2520Huijia%2520Wu%2520and%2520Zili%2520Wang%2520and%2520Zhaofeng%2520He%2520and%2520Jie%2520Fu%26entry.1292438233%3D%2520%2520The%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520for%2520language%2520models%2520has%2520been%2520proven%2520effective%2520in%250Aaugmenting%2520the%2520capacity%2520of%2520models%2520by%2520dynamically%2520routing%2520each%2520input%2520token%2520to%2520a%250Aspecific%2520subset%2520of%2520experts%2520for%2520processing.%2520Despite%2520the%2520success%252C%2520most%2520existing%250Amethods%2520face%2520a%2520challenge%2520for%2520balance%2520between%2520sparsity%2520and%2520the%2520availability%2520of%250Aexpert%2520knowledge%253A%2520enhancing%2520performance%2520through%2520increased%2520use%2520of%2520expert%250Aknowledge%2520often%2520results%2520in%2520diminishing%2520sparsity%2520during%2520expert%2520selection.%2520To%250Amitigate%2520this%2520contradiction%252C%2520we%2520propose%2520HyperMoE%252C%2520a%2520novel%2520MoE%2520framework%2520built%250Aupon%2520Hypernetworks.%2520This%2520framework%2520integrates%2520the%2520computational%2520processes%2520of%250AMoE%2520with%2520the%2520concept%2520of%2520knowledge%2520transferring%2520in%2520multi-task%2520learning.%2520Specific%250Amodules%2520generated%2520based%2520on%2520the%2520information%2520of%2520unselected%2520experts%2520serve%2520as%250Asupplementary%2520information%252C%2520which%2520allows%2520the%2520knowledge%2520of%2520experts%2520not%2520selected%250Ato%2520be%2520used%2520while%2520maintaining%2520selection%2520sparsity.%2520Our%2520comprehensive%2520empirical%250Aevaluations%2520across%2520multiple%2520datasets%2520and%2520backbones%2520establish%2520that%2520HyperMoE%250Asignificantly%2520outperforms%2520existing%2520MoE%2520methods%2520under%2520identical%2520conditions%250Aconcerning%2520the%2520number%2520of%2520experts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12656v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperMoE%3A%20Towards%20Better%20Mixture%20of%20Experts%20via%20Transferring%20Among%0A%20%20Experts&entry.906535625=Hao%20Zhao%20and%20Zihan%20Qiu%20and%20Huijia%20Wu%20and%20Zili%20Wang%20and%20Zhaofeng%20He%20and%20Jie%20Fu&entry.1292438233=%20%20The%20Mixture%20of%20Experts%20%28MoE%29%20for%20language%20models%20has%20been%20proven%20effective%20in%0Aaugmenting%20the%20capacity%20of%20models%20by%20dynamically%20routing%20each%20input%20token%20to%20a%0Aspecific%20subset%20of%20experts%20for%20processing.%20Despite%20the%20success%2C%20most%20existing%0Amethods%20face%20a%20challenge%20for%20balance%20between%20sparsity%20and%20the%20availability%20of%0Aexpert%20knowledge%3A%20enhancing%20performance%20through%20increased%20use%20of%20expert%0Aknowledge%20often%20results%20in%20diminishing%20sparsity%20during%20expert%20selection.%20To%0Amitigate%20this%20contradiction%2C%20we%20propose%20HyperMoE%2C%20a%20novel%20MoE%20framework%20built%0Aupon%20Hypernetworks.%20This%20framework%20integrates%20the%20computational%20processes%20of%0AMoE%20with%20the%20concept%20of%20knowledge%20transferring%20in%20multi-task%20learning.%20Specific%0Amodules%20generated%20based%20on%20the%20information%20of%20unselected%20experts%20serve%20as%0Asupplementary%20information%2C%20which%20allows%20the%20knowledge%20of%20experts%20not%20selected%0Ato%20be%20used%20while%20maintaining%20selection%20sparsity.%20Our%20comprehensive%20empirical%0Aevaluations%20across%20multiple%20datasets%20and%20backbones%20establish%20that%20HyperMoE%0Asignificantly%20outperforms%20existing%20MoE%20methods%20under%20identical%20conditions%0Aconcerning%20the%20number%20of%20experts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12656v3&entry.124074799=Read"},
{"title": "Learn Your Reference Model for Real Good Alignment", "author": "Alexey Gorbatovski and Boris Shaposhnikov and Alexey Malakhov and Nikita Surnachev and Yaroslav Aksenov and Ian Maksimov and Nikita Balagansky and Daniil Gavrilov", "abstract": "  The complexity of the alignment problem stems from the fact that existing\nmethods are considered unstable. Reinforcement Learning from Human Feedback\n(RLHF) addresses this issue by minimizing the KL divergence between the trained\npolicy and the initial supervised fine-tuned policy (SFT) to avoid generating\nout-of-domain samples for the reward model (RM). Recently, many methods have\nemerged that shift from online to offline optimization, reformulating the RLHF\nobjective and removing the reward model (DPO, IPO, KTO). Despite eliminating\nthe reward model and the challenges it posed, these algorithms are still\nconstrained in terms of closeness of the trained policy to the SFT one. In our\npaper, we argue that this implicit limitation in the offline optimization\nmethods leads to suboptimal results. To address this issue, we propose a class\nof new methods called Trust Region (TR-DPO, TR-IPO, TR-KTO), which update the\nreference policy during training. With this straightforward update approach, we\ndemonstrate the effectiveness of the new paradigm of language model alignment\nagainst the classical one on the Anthropic-HH and Reddit TL;DR datasets. Most\nnotably, when automatically comparing TR methods and baselines side by side\nusing pretrained Pythia 6.9B models on the Reddit TL;DR task, the difference in\nwin rates reaches 8.4% for DPO, 14.3% for IPO, and 15% for KTO. Finally, by\nassessing model response ratings grounded on criteria such as coherence,\ncorrectness, helpfulness, and harmlessness, we demonstrate that our proposed\nmethods significantly outperform existing techniques.\n", "link": "http://arxiv.org/abs/2404.09656v2", "date": "2024-05-21", "relevancy": 1.9918, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5151}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4895}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learn%20Your%20Reference%20Model%20for%20Real%20Good%20Alignment&body=Title%3A%20Learn%20Your%20Reference%20Model%20for%20Real%20Good%20Alignment%0AAuthor%3A%20Alexey%20Gorbatovski%20and%20Boris%20Shaposhnikov%20and%20Alexey%20Malakhov%20and%20Nikita%20Surnachev%20and%20Yaroslav%20Aksenov%20and%20Ian%20Maksimov%20and%20Nikita%20Balagansky%20and%20Daniil%20Gavrilov%0AAbstract%3A%20%20%20The%20complexity%20of%20the%20alignment%20problem%20stems%20from%20the%20fact%20that%20existing%0Amethods%20are%20considered%20unstable.%20Reinforcement%20Learning%20from%20Human%20Feedback%0A%28RLHF%29%20addresses%20this%20issue%20by%20minimizing%20the%20KL%20divergence%20between%20the%20trained%0Apolicy%20and%20the%20initial%20supervised%20fine-tuned%20policy%20%28SFT%29%20to%20avoid%20generating%0Aout-of-domain%20samples%20for%20the%20reward%20model%20%28RM%29.%20Recently%2C%20many%20methods%20have%0Aemerged%20that%20shift%20from%20online%20to%20offline%20optimization%2C%20reformulating%20the%20RLHF%0Aobjective%20and%20removing%20the%20reward%20model%20%28DPO%2C%20IPO%2C%20KTO%29.%20Despite%20eliminating%0Athe%20reward%20model%20and%20the%20challenges%20it%20posed%2C%20these%20algorithms%20are%20still%0Aconstrained%20in%20terms%20of%20closeness%20of%20the%20trained%20policy%20to%20the%20SFT%20one.%20In%20our%0Apaper%2C%20we%20argue%20that%20this%20implicit%20limitation%20in%20the%20offline%20optimization%0Amethods%20leads%20to%20suboptimal%20results.%20To%20address%20this%20issue%2C%20we%20propose%20a%20class%0Aof%20new%20methods%20called%20Trust%20Region%20%28TR-DPO%2C%20TR-IPO%2C%20TR-KTO%29%2C%20which%20update%20the%0Areference%20policy%20during%20training.%20With%20this%20straightforward%20update%20approach%2C%20we%0Ademonstrate%20the%20effectiveness%20of%20the%20new%20paradigm%20of%20language%20model%20alignment%0Aagainst%20the%20classical%20one%20on%20the%20Anthropic-HH%20and%20Reddit%20TL%3BDR%20datasets.%20Most%0Anotably%2C%20when%20automatically%20comparing%20TR%20methods%20and%20baselines%20side%20by%20side%0Ausing%20pretrained%20Pythia%206.9B%20models%20on%20the%20Reddit%20TL%3BDR%20task%2C%20the%20difference%20in%0Awin%20rates%20reaches%208.4%25%20for%20DPO%2C%2014.3%25%20for%20IPO%2C%20and%2015%25%20for%20KTO.%20Finally%2C%20by%0Aassessing%20model%20response%20ratings%20grounded%20on%20criteria%20such%20as%20coherence%2C%0Acorrectness%2C%20helpfulness%2C%20and%20harmlessness%2C%20we%20demonstrate%20that%20our%20proposed%0Amethods%20significantly%20outperform%20existing%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09656v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearn%2520Your%2520Reference%2520Model%2520for%2520Real%2520Good%2520Alignment%26entry.906535625%3DAlexey%2520Gorbatovski%2520and%2520Boris%2520Shaposhnikov%2520and%2520Alexey%2520Malakhov%2520and%2520Nikita%2520Surnachev%2520and%2520Yaroslav%2520Aksenov%2520and%2520Ian%2520Maksimov%2520and%2520Nikita%2520Balagansky%2520and%2520Daniil%2520Gavrilov%26entry.1292438233%3D%2520%2520The%2520complexity%2520of%2520the%2520alignment%2520problem%2520stems%2520from%2520the%2520fact%2520that%2520existing%250Amethods%2520are%2520considered%2520unstable.%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%250A%2528RLHF%2529%2520addresses%2520this%2520issue%2520by%2520minimizing%2520the%2520KL%2520divergence%2520between%2520the%2520trained%250Apolicy%2520and%2520the%2520initial%2520supervised%2520fine-tuned%2520policy%2520%2528SFT%2529%2520to%2520avoid%2520generating%250Aout-of-domain%2520samples%2520for%2520the%2520reward%2520model%2520%2528RM%2529.%2520Recently%252C%2520many%2520methods%2520have%250Aemerged%2520that%2520shift%2520from%2520online%2520to%2520offline%2520optimization%252C%2520reformulating%2520the%2520RLHF%250Aobjective%2520and%2520removing%2520the%2520reward%2520model%2520%2528DPO%252C%2520IPO%252C%2520KTO%2529.%2520Despite%2520eliminating%250Athe%2520reward%2520model%2520and%2520the%2520challenges%2520it%2520posed%252C%2520these%2520algorithms%2520are%2520still%250Aconstrained%2520in%2520terms%2520of%2520closeness%2520of%2520the%2520trained%2520policy%2520to%2520the%2520SFT%2520one.%2520In%2520our%250Apaper%252C%2520we%2520argue%2520that%2520this%2520implicit%2520limitation%2520in%2520the%2520offline%2520optimization%250Amethods%2520leads%2520to%2520suboptimal%2520results.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520class%250Aof%2520new%2520methods%2520called%2520Trust%2520Region%2520%2528TR-DPO%252C%2520TR-IPO%252C%2520TR-KTO%2529%252C%2520which%2520update%2520the%250Areference%2520policy%2520during%2520training.%2520With%2520this%2520straightforward%2520update%2520approach%252C%2520we%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520new%2520paradigm%2520of%2520language%2520model%2520alignment%250Aagainst%2520the%2520classical%2520one%2520on%2520the%2520Anthropic-HH%2520and%2520Reddit%2520TL%253BDR%2520datasets.%2520Most%250Anotably%252C%2520when%2520automatically%2520comparing%2520TR%2520methods%2520and%2520baselines%2520side%2520by%2520side%250Ausing%2520pretrained%2520Pythia%25206.9B%2520models%2520on%2520the%2520Reddit%2520TL%253BDR%2520task%252C%2520the%2520difference%2520in%250Awin%2520rates%2520reaches%25208.4%2525%2520for%2520DPO%252C%252014.3%2525%2520for%2520IPO%252C%2520and%252015%2525%2520for%2520KTO.%2520Finally%252C%2520by%250Aassessing%2520model%2520response%2520ratings%2520grounded%2520on%2520criteria%2520such%2520as%2520coherence%252C%250Acorrectness%252C%2520helpfulness%252C%2520and%2520harmlessness%252C%2520we%2520demonstrate%2520that%2520our%2520proposed%250Amethods%2520significantly%2520outperform%2520existing%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09656v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn%20Your%20Reference%20Model%20for%20Real%20Good%20Alignment&entry.906535625=Alexey%20Gorbatovski%20and%20Boris%20Shaposhnikov%20and%20Alexey%20Malakhov%20and%20Nikita%20Surnachev%20and%20Yaroslav%20Aksenov%20and%20Ian%20Maksimov%20and%20Nikita%20Balagansky%20and%20Daniil%20Gavrilov&entry.1292438233=%20%20The%20complexity%20of%20the%20alignment%20problem%20stems%20from%20the%20fact%20that%20existing%0Amethods%20are%20considered%20unstable.%20Reinforcement%20Learning%20from%20Human%20Feedback%0A%28RLHF%29%20addresses%20this%20issue%20by%20minimizing%20the%20KL%20divergence%20between%20the%20trained%0Apolicy%20and%20the%20initial%20supervised%20fine-tuned%20policy%20%28SFT%29%20to%20avoid%20generating%0Aout-of-domain%20samples%20for%20the%20reward%20model%20%28RM%29.%20Recently%2C%20many%20methods%20have%0Aemerged%20that%20shift%20from%20online%20to%20offline%20optimization%2C%20reformulating%20the%20RLHF%0Aobjective%20and%20removing%20the%20reward%20model%20%28DPO%2C%20IPO%2C%20KTO%29.%20Despite%20eliminating%0Athe%20reward%20model%20and%20the%20challenges%20it%20posed%2C%20these%20algorithms%20are%20still%0Aconstrained%20in%20terms%20of%20closeness%20of%20the%20trained%20policy%20to%20the%20SFT%20one.%20In%20our%0Apaper%2C%20we%20argue%20that%20this%20implicit%20limitation%20in%20the%20offline%20optimization%0Amethods%20leads%20to%20suboptimal%20results.%20To%20address%20this%20issue%2C%20we%20propose%20a%20class%0Aof%20new%20methods%20called%20Trust%20Region%20%28TR-DPO%2C%20TR-IPO%2C%20TR-KTO%29%2C%20which%20update%20the%0Areference%20policy%20during%20training.%20With%20this%20straightforward%20update%20approach%2C%20we%0Ademonstrate%20the%20effectiveness%20of%20the%20new%20paradigm%20of%20language%20model%20alignment%0Aagainst%20the%20classical%20one%20on%20the%20Anthropic-HH%20and%20Reddit%20TL%3BDR%20datasets.%20Most%0Anotably%2C%20when%20automatically%20comparing%20TR%20methods%20and%20baselines%20side%20by%20side%0Ausing%20pretrained%20Pythia%206.9B%20models%20on%20the%20Reddit%20TL%3BDR%20task%2C%20the%20difference%20in%0Awin%20rates%20reaches%208.4%25%20for%20DPO%2C%2014.3%25%20for%20IPO%2C%20and%2015%25%20for%20KTO.%20Finally%2C%20by%0Aassessing%20model%20response%20ratings%20grounded%20on%20criteria%20such%20as%20coherence%2C%0Acorrectness%2C%20helpfulness%2C%20and%20harmlessness%2C%20we%20demonstrate%20that%20our%20proposed%0Amethods%20significantly%20outperform%20existing%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09656v2&entry.124074799=Read"},
{"title": "Epanechnikov Variational Autoencoder", "author": "Tian Qin and Wei-Min Huang", "abstract": "  In this paper, we bridge Variational Autoencoders (VAEs) [17] and kernel\ndensity estimations (KDEs) [25 ],[23] by approximating the posterior by KDEs\nand deriving an upper bound of the Kullback-Leibler (KL) divergence in the\nevidence lower bound (ELBO). The flexibility of KDEs makes the optimization of\nposteriors in VAEs possible, which not only addresses the limitations of\nGaussian latent space in vanilla VAE but also provides a new perspective of\nestimating the KL-divergence in ELBO. Under appropriate conditions [ 9],[3 ],\nwe show that the Epanechnikov kernel is the optimal choice in minimizing the\nderived upper bound of KL-divergence asymptotically. Compared with Gaussian\nkernel, Epanechnikov kernel has compact support which should make the generated\nsample less noisy and blurry. The implementation of Epanechnikov kernel in ELBO\nis straightforward as it lies in the \"location-scale\" family of distributions\nwhere the reparametrization tricks can be directly employed. A series of\nexperiments on benchmark datasets such as MNIST, Fashion-MNIST, CIFAR-10 and\nCelebA further demonstrate the superiority of Epanechnikov Variational\nAutoenocoder (EVAE) over vanilla VAE in the quality of reconstructed images, as\nmeasured by the FID score and Sharpness[27].\n", "link": "http://arxiv.org/abs/2405.12783v1", "date": "2024-05-21", "relevancy": 1.9906, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5381}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5059}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Epanechnikov%20Variational%20Autoencoder&body=Title%3A%20Epanechnikov%20Variational%20Autoencoder%0AAuthor%3A%20Tian%20Qin%20and%20Wei-Min%20Huang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20bridge%20Variational%20Autoencoders%20%28VAEs%29%20%5B17%5D%20and%20kernel%0Adensity%20estimations%20%28KDEs%29%20%5B25%20%5D%2C%5B23%5D%20by%20approximating%20the%20posterior%20by%20KDEs%0Aand%20deriving%20an%20upper%20bound%20of%20the%20Kullback-Leibler%20%28KL%29%20divergence%20in%20the%0Aevidence%20lower%20bound%20%28ELBO%29.%20The%20flexibility%20of%20KDEs%20makes%20the%20optimization%20of%0Aposteriors%20in%20VAEs%20possible%2C%20which%20not%20only%20addresses%20the%20limitations%20of%0AGaussian%20latent%20space%20in%20vanilla%20VAE%20but%20also%20provides%20a%20new%20perspective%20of%0Aestimating%20the%20KL-divergence%20in%20ELBO.%20Under%20appropriate%20conditions%20%5B%209%5D%2C%5B3%20%5D%2C%0Awe%20show%20that%20the%20Epanechnikov%20kernel%20is%20the%20optimal%20choice%20in%20minimizing%20the%0Aderived%20upper%20bound%20of%20KL-divergence%20asymptotically.%20Compared%20with%20Gaussian%0Akernel%2C%20Epanechnikov%20kernel%20has%20compact%20support%20which%20should%20make%20the%20generated%0Asample%20less%20noisy%20and%20blurry.%20The%20implementation%20of%20Epanechnikov%20kernel%20in%20ELBO%0Ais%20straightforward%20as%20it%20lies%20in%20the%20%22location-scale%22%20family%20of%20distributions%0Awhere%20the%20reparametrization%20tricks%20can%20be%20directly%20employed.%20A%20series%20of%0Aexperiments%20on%20benchmark%20datasets%20such%20as%20MNIST%2C%20Fashion-MNIST%2C%20CIFAR-10%20and%0ACelebA%20further%20demonstrate%20the%20superiority%20of%20Epanechnikov%20Variational%0AAutoenocoder%20%28EVAE%29%20over%20vanilla%20VAE%20in%20the%20quality%20of%20reconstructed%20images%2C%20as%0Ameasured%20by%20the%20FID%20score%20and%20Sharpness%5B27%5D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEpanechnikov%2520Variational%2520Autoencoder%26entry.906535625%3DTian%2520Qin%2520and%2520Wei-Min%2520Huang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520bridge%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520%255B17%255D%2520and%2520kernel%250Adensity%2520estimations%2520%2528KDEs%2529%2520%255B25%2520%255D%252C%255B23%255D%2520by%2520approximating%2520the%2520posterior%2520by%2520KDEs%250Aand%2520deriving%2520an%2520upper%2520bound%2520of%2520the%2520Kullback-Leibler%2520%2528KL%2529%2520divergence%2520in%2520the%250Aevidence%2520lower%2520bound%2520%2528ELBO%2529.%2520The%2520flexibility%2520of%2520KDEs%2520makes%2520the%2520optimization%2520of%250Aposteriors%2520in%2520VAEs%2520possible%252C%2520which%2520not%2520only%2520addresses%2520the%2520limitations%2520of%250AGaussian%2520latent%2520space%2520in%2520vanilla%2520VAE%2520but%2520also%2520provides%2520a%2520new%2520perspective%2520of%250Aestimating%2520the%2520KL-divergence%2520in%2520ELBO.%2520Under%2520appropriate%2520conditions%2520%255B%25209%255D%252C%255B3%2520%255D%252C%250Awe%2520show%2520that%2520the%2520Epanechnikov%2520kernel%2520is%2520the%2520optimal%2520choice%2520in%2520minimizing%2520the%250Aderived%2520upper%2520bound%2520of%2520KL-divergence%2520asymptotically.%2520Compared%2520with%2520Gaussian%250Akernel%252C%2520Epanechnikov%2520kernel%2520has%2520compact%2520support%2520which%2520should%2520make%2520the%2520generated%250Asample%2520less%2520noisy%2520and%2520blurry.%2520The%2520implementation%2520of%2520Epanechnikov%2520kernel%2520in%2520ELBO%250Ais%2520straightforward%2520as%2520it%2520lies%2520in%2520the%2520%2522location-scale%2522%2520family%2520of%2520distributions%250Awhere%2520the%2520reparametrization%2520tricks%2520can%2520be%2520directly%2520employed.%2520A%2520series%2520of%250Aexperiments%2520on%2520benchmark%2520datasets%2520such%2520as%2520MNIST%252C%2520Fashion-MNIST%252C%2520CIFAR-10%2520and%250ACelebA%2520further%2520demonstrate%2520the%2520superiority%2520of%2520Epanechnikov%2520Variational%250AAutoenocoder%2520%2528EVAE%2529%2520over%2520vanilla%2520VAE%2520in%2520the%2520quality%2520of%2520reconstructed%2520images%252C%2520as%250Ameasured%2520by%2520the%2520FID%2520score%2520and%2520Sharpness%255B27%255D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Epanechnikov%20Variational%20Autoencoder&entry.906535625=Tian%20Qin%20and%20Wei-Min%20Huang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20bridge%20Variational%20Autoencoders%20%28VAEs%29%20%5B17%5D%20and%20kernel%0Adensity%20estimations%20%28KDEs%29%20%5B25%20%5D%2C%5B23%5D%20by%20approximating%20the%20posterior%20by%20KDEs%0Aand%20deriving%20an%20upper%20bound%20of%20the%20Kullback-Leibler%20%28KL%29%20divergence%20in%20the%0Aevidence%20lower%20bound%20%28ELBO%29.%20The%20flexibility%20of%20KDEs%20makes%20the%20optimization%20of%0Aposteriors%20in%20VAEs%20possible%2C%20which%20not%20only%20addresses%20the%20limitations%20of%0AGaussian%20latent%20space%20in%20vanilla%20VAE%20but%20also%20provides%20a%20new%20perspective%20of%0Aestimating%20the%20KL-divergence%20in%20ELBO.%20Under%20appropriate%20conditions%20%5B%209%5D%2C%5B3%20%5D%2C%0Awe%20show%20that%20the%20Epanechnikov%20kernel%20is%20the%20optimal%20choice%20in%20minimizing%20the%0Aderived%20upper%20bound%20of%20KL-divergence%20asymptotically.%20Compared%20with%20Gaussian%0Akernel%2C%20Epanechnikov%20kernel%20has%20compact%20support%20which%20should%20make%20the%20generated%0Asample%20less%20noisy%20and%20blurry.%20The%20implementation%20of%20Epanechnikov%20kernel%20in%20ELBO%0Ais%20straightforward%20as%20it%20lies%20in%20the%20%22location-scale%22%20family%20of%20distributions%0Awhere%20the%20reparametrization%20tricks%20can%20be%20directly%20employed.%20A%20series%20of%0Aexperiments%20on%20benchmark%20datasets%20such%20as%20MNIST%2C%20Fashion-MNIST%2C%20CIFAR-10%20and%0ACelebA%20further%20demonstrate%20the%20superiority%20of%20Epanechnikov%20Variational%0AAutoenocoder%20%28EVAE%29%20over%20vanilla%20VAE%20in%20the%20quality%20of%20reconstructed%20images%2C%20as%0Ameasured%20by%20the%20FID%20score%20and%20Sharpness%5B27%5D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12783v1&entry.124074799=Read"},
{"title": "On Image Registration and Subpixel Estimation", "author": "Serap A. Savari", "abstract": "  Image registration is a classical problem in machine vision which seeks\nmethods to align discrete images of the same scene to subpixel accuracy in\ngeneral situations. As with all estimation problems, the underlying difficulty\nis the partial information available about the ground truth. We consider a\nbasic and idealized one-dimensional image registration problem motivated by\nquestions about measurement and about quantization, and we demonstrate that the\nextent to which subinterval/subpixel inferences can be made in this setting\ndepends on a type of complexity associated with the function of interest, the\nrelationship between the function and the pixel size, and the number of\ndistinct sampling count observations available.\n", "link": "http://arxiv.org/abs/2405.12927v1", "date": "2024-05-21", "relevancy": 1.9852, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5008}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.498}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Image%20Registration%20and%20Subpixel%20Estimation&body=Title%3A%20On%20Image%20Registration%20and%20Subpixel%20Estimation%0AAuthor%3A%20Serap%20A.%20Savari%0AAbstract%3A%20%20%20Image%20registration%20is%20a%20classical%20problem%20in%20machine%20vision%20which%20seeks%0Amethods%20to%20align%20discrete%20images%20of%20the%20same%20scene%20to%20subpixel%20accuracy%20in%0Ageneral%20situations.%20As%20with%20all%20estimation%20problems%2C%20the%20underlying%20difficulty%0Ais%20the%20partial%20information%20available%20about%20the%20ground%20truth.%20We%20consider%20a%0Abasic%20and%20idealized%20one-dimensional%20image%20registration%20problem%20motivated%20by%0Aquestions%20about%20measurement%20and%20about%20quantization%2C%20and%20we%20demonstrate%20that%20the%0Aextent%20to%20which%20subinterval/subpixel%20inferences%20can%20be%20made%20in%20this%20setting%0Adepends%20on%20a%20type%20of%20complexity%20associated%20with%20the%20function%20of%20interest%2C%20the%0Arelationship%20between%20the%20function%20and%20the%20pixel%20size%2C%20and%20the%20number%20of%0Adistinct%20sampling%20count%20observations%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Image%2520Registration%2520and%2520Subpixel%2520Estimation%26entry.906535625%3DSerap%2520A.%2520Savari%26entry.1292438233%3D%2520%2520Image%2520registration%2520is%2520a%2520classical%2520problem%2520in%2520machine%2520vision%2520which%2520seeks%250Amethods%2520to%2520align%2520discrete%2520images%2520of%2520the%2520same%2520scene%2520to%2520subpixel%2520accuracy%2520in%250Ageneral%2520situations.%2520As%2520with%2520all%2520estimation%2520problems%252C%2520the%2520underlying%2520difficulty%250Ais%2520the%2520partial%2520information%2520available%2520about%2520the%2520ground%2520truth.%2520We%2520consider%2520a%250Abasic%2520and%2520idealized%2520one-dimensional%2520image%2520registration%2520problem%2520motivated%2520by%250Aquestions%2520about%2520measurement%2520and%2520about%2520quantization%252C%2520and%2520we%2520demonstrate%2520that%2520the%250Aextent%2520to%2520which%2520subinterval/subpixel%2520inferences%2520can%2520be%2520made%2520in%2520this%2520setting%250Adepends%2520on%2520a%2520type%2520of%2520complexity%2520associated%2520with%2520the%2520function%2520of%2520interest%252C%2520the%250Arelationship%2520between%2520the%2520function%2520and%2520the%2520pixel%2520size%252C%2520and%2520the%2520number%2520of%250Adistinct%2520sampling%2520count%2520observations%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Image%20Registration%20and%20Subpixel%20Estimation&entry.906535625=Serap%20A.%20Savari&entry.1292438233=%20%20Image%20registration%20is%20a%20classical%20problem%20in%20machine%20vision%20which%20seeks%0Amethods%20to%20align%20discrete%20images%20of%20the%20same%20scene%20to%20subpixel%20accuracy%20in%0Ageneral%20situations.%20As%20with%20all%20estimation%20problems%2C%20the%20underlying%20difficulty%0Ais%20the%20partial%20information%20available%20about%20the%20ground%20truth.%20We%20consider%20a%0Abasic%20and%20idealized%20one-dimensional%20image%20registration%20problem%20motivated%20by%0Aquestions%20about%20measurement%20and%20about%20quantization%2C%20and%20we%20demonstrate%20that%20the%0Aextent%20to%20which%20subinterval/subpixel%20inferences%20can%20be%20made%20in%20this%20setting%0Adepends%20on%20a%20type%20of%20complexity%20associated%20with%20the%20function%20of%20interest%2C%20the%0Arelationship%20between%20the%20function%20and%20the%20pixel%20size%2C%20and%20the%20number%20of%0Adistinct%20sampling%20count%20observations%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12927v1&entry.124074799=Read"},
{"title": "SYMPLEX: Controllable Symbolic Music Generation using Simplex Diffusion\n  with Vocabulary Priors", "author": "Nicolas Jonason and Luca Casini and Bob L. T. Sturm", "abstract": "  We present a new approach for fast and controllable generation of symbolic\nmusic based on the simplex diffusion, which is essentially a diffusion process\noperating on probabilities rather than the signal space. This objective has\nbeen applied in domains such as natural language processing but here we apply\nit to generating 4-bar multi-instrument music loops using an orderless\nrepresentation. We show that our model can be steered with vocabulary priors,\nwhich affords a considerable level control over the music generation process,\nfor instance, infilling in time and pitch and choice of instrumentation -- all\nwithout task-specific model adaptation or applying extrinsic control.\n", "link": "http://arxiv.org/abs/2405.12666v1", "date": "2024-05-21", "relevancy": 1.9827, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5159}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4921}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SYMPLEX%3A%20Controllable%20Symbolic%20Music%20Generation%20using%20Simplex%20Diffusion%0A%20%20with%20Vocabulary%20Priors&body=Title%3A%20SYMPLEX%3A%20Controllable%20Symbolic%20Music%20Generation%20using%20Simplex%20Diffusion%0A%20%20with%20Vocabulary%20Priors%0AAuthor%3A%20Nicolas%20Jonason%20and%20Luca%20Casini%20and%20Bob%20L.%20T.%20Sturm%0AAbstract%3A%20%20%20We%20present%20a%20new%20approach%20for%20fast%20and%20controllable%20generation%20of%20symbolic%0Amusic%20based%20on%20the%20simplex%20diffusion%2C%20which%20is%20essentially%20a%20diffusion%20process%0Aoperating%20on%20probabilities%20rather%20than%20the%20signal%20space.%20This%20objective%20has%0Abeen%20applied%20in%20domains%20such%20as%20natural%20language%20processing%20but%20here%20we%20apply%0Ait%20to%20generating%204-bar%20multi-instrument%20music%20loops%20using%20an%20orderless%0Arepresentation.%20We%20show%20that%20our%20model%20can%20be%20steered%20with%20vocabulary%20priors%2C%0Awhich%20affords%20a%20considerable%20level%20control%20over%20the%20music%20generation%20process%2C%0Afor%20instance%2C%20infilling%20in%20time%20and%20pitch%20and%20choice%20of%20instrumentation%20--%20all%0Awithout%20task-specific%20model%20adaptation%20or%20applying%20extrinsic%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSYMPLEX%253A%2520Controllable%2520Symbolic%2520Music%2520Generation%2520using%2520Simplex%2520Diffusion%250A%2520%2520with%2520Vocabulary%2520Priors%26entry.906535625%3DNicolas%2520Jonason%2520and%2520Luca%2520Casini%2520and%2520Bob%2520L.%2520T.%2520Sturm%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520approach%2520for%2520fast%2520and%2520controllable%2520generation%2520of%2520symbolic%250Amusic%2520based%2520on%2520the%2520simplex%2520diffusion%252C%2520which%2520is%2520essentially%2520a%2520diffusion%2520process%250Aoperating%2520on%2520probabilities%2520rather%2520than%2520the%2520signal%2520space.%2520This%2520objective%2520has%250Abeen%2520applied%2520in%2520domains%2520such%2520as%2520natural%2520language%2520processing%2520but%2520here%2520we%2520apply%250Ait%2520to%2520generating%25204-bar%2520multi-instrument%2520music%2520loops%2520using%2520an%2520orderless%250Arepresentation.%2520We%2520show%2520that%2520our%2520model%2520can%2520be%2520steered%2520with%2520vocabulary%2520priors%252C%250Awhich%2520affords%2520a%2520considerable%2520level%2520control%2520over%2520the%2520music%2520generation%2520process%252C%250Afor%2520instance%252C%2520infilling%2520in%2520time%2520and%2520pitch%2520and%2520choice%2520of%2520instrumentation%2520--%2520all%250Awithout%2520task-specific%2520model%2520adaptation%2520or%2520applying%2520extrinsic%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SYMPLEX%3A%20Controllable%20Symbolic%20Music%20Generation%20using%20Simplex%20Diffusion%0A%20%20with%20Vocabulary%20Priors&entry.906535625=Nicolas%20Jonason%20and%20Luca%20Casini%20and%20Bob%20L.%20T.%20Sturm&entry.1292438233=%20%20We%20present%20a%20new%20approach%20for%20fast%20and%20controllable%20generation%20of%20symbolic%0Amusic%20based%20on%20the%20simplex%20diffusion%2C%20which%20is%20essentially%20a%20diffusion%20process%0Aoperating%20on%20probabilities%20rather%20than%20the%20signal%20space.%20This%20objective%20has%0Abeen%20applied%20in%20domains%20such%20as%20natural%20language%20processing%20but%20here%20we%20apply%0Ait%20to%20generating%204-bar%20multi-instrument%20music%20loops%20using%20an%20orderless%0Arepresentation.%20We%20show%20that%20our%20model%20can%20be%20steered%20with%20vocabulary%20priors%2C%0Awhich%20affords%20a%20considerable%20level%20control%20over%20the%20music%20generation%20process%2C%0Afor%20instance%2C%20infilling%20in%20time%20and%20pitch%20and%20choice%20of%20instrumentation%20--%20all%0Awithout%20task-specific%20model%20adaptation%20or%20applying%20extrinsic%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12666v1&entry.124074799=Read"},
{"title": "What makes an image realistic?", "author": "Lucas Theis", "abstract": "  The last decade has seen tremendous progress in our ability to generate\nrealistic-looking data, be it images, text, audio, or video. Here, we discuss\nthe closely related problem of quantifying realism, that is, designing\nfunctions that can reliably tell realistic data from unrealistic data. This\nproblem turns out to be significantly harder to solve and remains poorly\nunderstood, despite its prevalence in machine learning and recent breakthroughs\nin generative AI. Drawing on insights from algorithmic information theory, we\ndiscuss why this problem is challenging, why a good generative model alone is\ninsufficient to solve it, and what a good solution would look like. In\nparticular, we introduce the notion of a universal critic, which unlike\nadversarial critics does not require adversarial training. While universal\ncritics are not immediately practical, they can serve both as a North Star for\nguiding practical implementations and as a tool for analyzing existing attempts\nto capture realism.\n", "link": "http://arxiv.org/abs/2403.04493v4", "date": "2024-05-21", "relevancy": 1.9793, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5111}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4945}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20makes%20an%20image%20realistic%3F&body=Title%3A%20What%20makes%20an%20image%20realistic%3F%0AAuthor%3A%20Lucas%20Theis%0AAbstract%3A%20%20%20The%20last%20decade%20has%20seen%20tremendous%20progress%20in%20our%20ability%20to%20generate%0Arealistic-looking%20data%2C%20be%20it%20images%2C%20text%2C%20audio%2C%20or%20video.%20Here%2C%20we%20discuss%0Athe%20closely%20related%20problem%20of%20quantifying%20realism%2C%20that%20is%2C%20designing%0Afunctions%20that%20can%20reliably%20tell%20realistic%20data%20from%20unrealistic%20data.%20This%0Aproblem%20turns%20out%20to%20be%20significantly%20harder%20to%20solve%20and%20remains%20poorly%0Aunderstood%2C%20despite%20its%20prevalence%20in%20machine%20learning%20and%20recent%20breakthroughs%0Ain%20generative%20AI.%20Drawing%20on%20insights%20from%20algorithmic%20information%20theory%2C%20we%0Adiscuss%20why%20this%20problem%20is%20challenging%2C%20why%20a%20good%20generative%20model%20alone%20is%0Ainsufficient%20to%20solve%20it%2C%20and%20what%20a%20good%20solution%20would%20look%20like.%20In%0Aparticular%2C%20we%20introduce%20the%20notion%20of%20a%20universal%20critic%2C%20which%20unlike%0Aadversarial%20critics%20does%20not%20require%20adversarial%20training.%20While%20universal%0Acritics%20are%20not%20immediately%20practical%2C%20they%20can%20serve%20both%20as%20a%20North%20Star%20for%0Aguiding%20practical%20implementations%20and%20as%20a%20tool%20for%20analyzing%20existing%20attempts%0Ato%20capture%20realism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04493v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520makes%2520an%2520image%2520realistic%253F%26entry.906535625%3DLucas%2520Theis%26entry.1292438233%3D%2520%2520The%2520last%2520decade%2520has%2520seen%2520tremendous%2520progress%2520in%2520our%2520ability%2520to%2520generate%250Arealistic-looking%2520data%252C%2520be%2520it%2520images%252C%2520text%252C%2520audio%252C%2520or%2520video.%2520Here%252C%2520we%2520discuss%250Athe%2520closely%2520related%2520problem%2520of%2520quantifying%2520realism%252C%2520that%2520is%252C%2520designing%250Afunctions%2520that%2520can%2520reliably%2520tell%2520realistic%2520data%2520from%2520unrealistic%2520data.%2520This%250Aproblem%2520turns%2520out%2520to%2520be%2520significantly%2520harder%2520to%2520solve%2520and%2520remains%2520poorly%250Aunderstood%252C%2520despite%2520its%2520prevalence%2520in%2520machine%2520learning%2520and%2520recent%2520breakthroughs%250Ain%2520generative%2520AI.%2520Drawing%2520on%2520insights%2520from%2520algorithmic%2520information%2520theory%252C%2520we%250Adiscuss%2520why%2520this%2520problem%2520is%2520challenging%252C%2520why%2520a%2520good%2520generative%2520model%2520alone%2520is%250Ainsufficient%2520to%2520solve%2520it%252C%2520and%2520what%2520a%2520good%2520solution%2520would%2520look%2520like.%2520In%250Aparticular%252C%2520we%2520introduce%2520the%2520notion%2520of%2520a%2520universal%2520critic%252C%2520which%2520unlike%250Aadversarial%2520critics%2520does%2520not%2520require%2520adversarial%2520training.%2520While%2520universal%250Acritics%2520are%2520not%2520immediately%2520practical%252C%2520they%2520can%2520serve%2520both%2520as%2520a%2520North%2520Star%2520for%250Aguiding%2520practical%2520implementations%2520and%2520as%2520a%2520tool%2520for%2520analyzing%2520existing%2520attempts%250Ato%2520capture%2520realism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04493v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20makes%20an%20image%20realistic%3F&entry.906535625=Lucas%20Theis&entry.1292438233=%20%20The%20last%20decade%20has%20seen%20tremendous%20progress%20in%20our%20ability%20to%20generate%0Arealistic-looking%20data%2C%20be%20it%20images%2C%20text%2C%20audio%2C%20or%20video.%20Here%2C%20we%20discuss%0Athe%20closely%20related%20problem%20of%20quantifying%20realism%2C%20that%20is%2C%20designing%0Afunctions%20that%20can%20reliably%20tell%20realistic%20data%20from%20unrealistic%20data.%20This%0Aproblem%20turns%20out%20to%20be%20significantly%20harder%20to%20solve%20and%20remains%20poorly%0Aunderstood%2C%20despite%20its%20prevalence%20in%20machine%20learning%20and%20recent%20breakthroughs%0Ain%20generative%20AI.%20Drawing%20on%20insights%20from%20algorithmic%20information%20theory%2C%20we%0Adiscuss%20why%20this%20problem%20is%20challenging%2C%20why%20a%20good%20generative%20model%20alone%20is%0Ainsufficient%20to%20solve%20it%2C%20and%20what%20a%20good%20solution%20would%20look%20like.%20In%0Aparticular%2C%20we%20introduce%20the%20notion%20of%20a%20universal%20critic%2C%20which%20unlike%0Aadversarial%20critics%20does%20not%20require%20adversarial%20training.%20While%20universal%0Acritics%20are%20not%20immediately%20practical%2C%20they%20can%20serve%20both%20as%20a%20North%20Star%20for%0Aguiding%20practical%20implementations%20and%20as%20a%20tool%20for%20analyzing%20existing%20attempts%0Ato%20capture%20realism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04493v4&entry.124074799=Read"},
{"title": "HARIS: Human-Like Attention for Reference Image Segmentation", "author": "Mengxi Zhang and Heqing Lian and Yiming Liu and Jie Chen", "abstract": "  Referring image segmentation (RIS) aims to locate the particular region\ncorresponding to the language expression. Existing methods incorporate features\nfrom different modalities in a \\emph{bottom-up} manner. This design may get\nsome unnecessary image-text pairs, which leads to an inaccurate segmentation\nmask. In this paper, we propose a referring image segmentation method called\nHARIS, which introduces the Human-Like Attention mechanism and uses the\nparameter-efficient fine-tuning (PEFT) framework. To be specific, the\nHuman-Like Attention gets a \\emph{feedback} signal from multi-modal features,\nwhich makes the network center on the specific objects and discard the\nirrelevant image-text pairs. Besides, we introduce the PEFT framework to\npreserve the zero-shot ability of pre-trained encoders. Extensive experiments\non three widely used RIS benchmarks and the PhraseCut dataset demonstrate that\nour method achieves state-of-the-art performance and great zero-shot ability.\n", "link": "http://arxiv.org/abs/2405.10707v2", "date": "2024-05-21", "relevancy": 1.9772, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.503}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4921}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HARIS%3A%20Human-Like%20Attention%20for%20Reference%20Image%20Segmentation&body=Title%3A%20HARIS%3A%20Human-Like%20Attention%20for%20Reference%20Image%20Segmentation%0AAuthor%3A%20Mengxi%20Zhang%20and%20Heqing%20Lian%20and%20Yiming%20Liu%20and%20Jie%20Chen%0AAbstract%3A%20%20%20Referring%20image%20segmentation%20%28RIS%29%20aims%20to%20locate%20the%20particular%20region%0Acorresponding%20to%20the%20language%20expression.%20Existing%20methods%20incorporate%20features%0Afrom%20different%20modalities%20in%20a%20%5Cemph%7Bbottom-up%7D%20manner.%20This%20design%20may%20get%0Asome%20unnecessary%20image-text%20pairs%2C%20which%20leads%20to%20an%20inaccurate%20segmentation%0Amask.%20In%20this%20paper%2C%20we%20propose%20a%20referring%20image%20segmentation%20method%20called%0AHARIS%2C%20which%20introduces%20the%20Human-Like%20Attention%20mechanism%20and%20uses%20the%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20framework.%20To%20be%20specific%2C%20the%0AHuman-Like%20Attention%20gets%20a%20%5Cemph%7Bfeedback%7D%20signal%20from%20multi-modal%20features%2C%0Awhich%20makes%20the%20network%20center%20on%20the%20specific%20objects%20and%20discard%20the%0Airrelevant%20image-text%20pairs.%20Besides%2C%20we%20introduce%20the%20PEFT%20framework%20to%0Apreserve%20the%20zero-shot%20ability%20of%20pre-trained%20encoders.%20Extensive%20experiments%0Aon%20three%20widely%20used%20RIS%20benchmarks%20and%20the%20PhraseCut%20dataset%20demonstrate%20that%0Aour%20method%20achieves%20state-of-the-art%20performance%20and%20great%20zero-shot%20ability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10707v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHARIS%253A%2520Human-Like%2520Attention%2520for%2520Reference%2520Image%2520Segmentation%26entry.906535625%3DMengxi%2520Zhang%2520and%2520Heqing%2520Lian%2520and%2520Yiming%2520Liu%2520and%2520Jie%2520Chen%26entry.1292438233%3D%2520%2520Referring%2520image%2520segmentation%2520%2528RIS%2529%2520aims%2520to%2520locate%2520the%2520particular%2520region%250Acorresponding%2520to%2520the%2520language%2520expression.%2520Existing%2520methods%2520incorporate%2520features%250Afrom%2520different%2520modalities%2520in%2520a%2520%255Cemph%257Bbottom-up%257D%2520manner.%2520This%2520design%2520may%2520get%250Asome%2520unnecessary%2520image-text%2520pairs%252C%2520which%2520leads%2520to%2520an%2520inaccurate%2520segmentation%250Amask.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520referring%2520image%2520segmentation%2520method%2520called%250AHARIS%252C%2520which%2520introduces%2520the%2520Human-Like%2520Attention%2520mechanism%2520and%2520uses%2520the%250Aparameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520framework.%2520To%2520be%2520specific%252C%2520the%250AHuman-Like%2520Attention%2520gets%2520a%2520%255Cemph%257Bfeedback%257D%2520signal%2520from%2520multi-modal%2520features%252C%250Awhich%2520makes%2520the%2520network%2520center%2520on%2520the%2520specific%2520objects%2520and%2520discard%2520the%250Airrelevant%2520image-text%2520pairs.%2520Besides%252C%2520we%2520introduce%2520the%2520PEFT%2520framework%2520to%250Apreserve%2520the%2520zero-shot%2520ability%2520of%2520pre-trained%2520encoders.%2520Extensive%2520experiments%250Aon%2520three%2520widely%2520used%2520RIS%2520benchmarks%2520and%2520the%2520PhraseCut%2520dataset%2520demonstrate%2520that%250Aour%2520method%2520achieves%2520state-of-the-art%2520performance%2520and%2520great%2520zero-shot%2520ability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10707v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HARIS%3A%20Human-Like%20Attention%20for%20Reference%20Image%20Segmentation&entry.906535625=Mengxi%20Zhang%20and%20Heqing%20Lian%20and%20Yiming%20Liu%20and%20Jie%20Chen&entry.1292438233=%20%20Referring%20image%20segmentation%20%28RIS%29%20aims%20to%20locate%20the%20particular%20region%0Acorresponding%20to%20the%20language%20expression.%20Existing%20methods%20incorporate%20features%0Afrom%20different%20modalities%20in%20a%20%5Cemph%7Bbottom-up%7D%20manner.%20This%20design%20may%20get%0Asome%20unnecessary%20image-text%20pairs%2C%20which%20leads%20to%20an%20inaccurate%20segmentation%0Amask.%20In%20this%20paper%2C%20we%20propose%20a%20referring%20image%20segmentation%20method%20called%0AHARIS%2C%20which%20introduces%20the%20Human-Like%20Attention%20mechanism%20and%20uses%20the%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20framework.%20To%20be%20specific%2C%20the%0AHuman-Like%20Attention%20gets%20a%20%5Cemph%7Bfeedback%7D%20signal%20from%20multi-modal%20features%2C%0Awhich%20makes%20the%20network%20center%20on%20the%20specific%20objects%20and%20discard%20the%0Airrelevant%20image-text%20pairs.%20Besides%2C%20we%20introduce%20the%20PEFT%20framework%20to%0Apreserve%20the%20zero-shot%20ability%20of%20pre-trained%20encoders.%20Extensive%20experiments%0Aon%20three%20widely%20used%20RIS%20benchmarks%20and%20the%20PhraseCut%20dataset%20demonstrate%20that%0Aour%20method%20achieves%20state-of-the-art%20performance%20and%20great%20zero-shot%20ability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10707v2&entry.124074799=Read"},
{"title": "Tutorly: Turning Programming Videos Into Apprenticeship Learning\n  Environments with LLMs", "author": "Wengxi Li and Roy Pea and Nick Haber and Hari Subramonyam", "abstract": "  Online programming videos, including tutorials and streamcasts, are widely\npopular and contain a wealth of expert knowledge. However, effectively\nutilizing these resources to achieve targeted learning goals can be\nchallenging. Unlike direct tutoring, video content lacks tailored guidance\nbased on individual learning paces, personalized feedback, and interactive\nengagement necessary for support and monitoring. Our work transforms\nprogramming videos into one-on-one tutoring experiences using the cognitive\napprenticeship framework. Tutorly, developed as a JupyterLab Plugin, allows\nlearners to (1) set personalized learning goals, (2) engage in\nlearning-by-doing through a conversational LLM-based mentor agent, (3) receive\nguidance and feedback based on a student model that steers the mentor moves. In\na within-subject study with 16 participants learning exploratory data analysis\nfrom a streamcast, Tutorly significantly improved their performance from 61.9%\nto 76.6% based on a post-test questionnaire. Tutorly demonstrates the potential\nfor enhancing programming video learning experiences with LLM and learner\nmodeling.\n", "link": "http://arxiv.org/abs/2405.12946v1", "date": "2024-05-21", "relevancy": 1.9703, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5139}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5114}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tutorly%3A%20Turning%20Programming%20Videos%20Into%20Apprenticeship%20Learning%0A%20%20Environments%20with%20LLMs&body=Title%3A%20Tutorly%3A%20Turning%20Programming%20Videos%20Into%20Apprenticeship%20Learning%0A%20%20Environments%20with%20LLMs%0AAuthor%3A%20Wengxi%20Li%20and%20Roy%20Pea%20and%20Nick%20Haber%20and%20Hari%20Subramonyam%0AAbstract%3A%20%20%20Online%20programming%20videos%2C%20including%20tutorials%20and%20streamcasts%2C%20are%20widely%0Apopular%20and%20contain%20a%20wealth%20of%20expert%20knowledge.%20However%2C%20effectively%0Autilizing%20these%20resources%20to%20achieve%20targeted%20learning%20goals%20can%20be%0Achallenging.%20Unlike%20direct%20tutoring%2C%20video%20content%20lacks%20tailored%20guidance%0Abased%20on%20individual%20learning%20paces%2C%20personalized%20feedback%2C%20and%20interactive%0Aengagement%20necessary%20for%20support%20and%20monitoring.%20Our%20work%20transforms%0Aprogramming%20videos%20into%20one-on-one%20tutoring%20experiences%20using%20the%20cognitive%0Aapprenticeship%20framework.%20Tutorly%2C%20developed%20as%20a%20JupyterLab%20Plugin%2C%20allows%0Alearners%20to%20%281%29%20set%20personalized%20learning%20goals%2C%20%282%29%20engage%20in%0Alearning-by-doing%20through%20a%20conversational%20LLM-based%20mentor%20agent%2C%20%283%29%20receive%0Aguidance%20and%20feedback%20based%20on%20a%20student%20model%20that%20steers%20the%20mentor%20moves.%20In%0Aa%20within-subject%20study%20with%2016%20participants%20learning%20exploratory%20data%20analysis%0Afrom%20a%20streamcast%2C%20Tutorly%20significantly%20improved%20their%20performance%20from%2061.9%25%0Ato%2076.6%25%20based%20on%20a%20post-test%20questionnaire.%20Tutorly%20demonstrates%20the%20potential%0Afor%20enhancing%20programming%20video%20learning%20experiences%20with%20LLM%20and%20learner%0Amodeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTutorly%253A%2520Turning%2520Programming%2520Videos%2520Into%2520Apprenticeship%2520Learning%250A%2520%2520Environments%2520with%2520LLMs%26entry.906535625%3DWengxi%2520Li%2520and%2520Roy%2520Pea%2520and%2520Nick%2520Haber%2520and%2520Hari%2520Subramonyam%26entry.1292438233%3D%2520%2520Online%2520programming%2520videos%252C%2520including%2520tutorials%2520and%2520streamcasts%252C%2520are%2520widely%250Apopular%2520and%2520contain%2520a%2520wealth%2520of%2520expert%2520knowledge.%2520However%252C%2520effectively%250Autilizing%2520these%2520resources%2520to%2520achieve%2520targeted%2520learning%2520goals%2520can%2520be%250Achallenging.%2520Unlike%2520direct%2520tutoring%252C%2520video%2520content%2520lacks%2520tailored%2520guidance%250Abased%2520on%2520individual%2520learning%2520paces%252C%2520personalized%2520feedback%252C%2520and%2520interactive%250Aengagement%2520necessary%2520for%2520support%2520and%2520monitoring.%2520Our%2520work%2520transforms%250Aprogramming%2520videos%2520into%2520one-on-one%2520tutoring%2520experiences%2520using%2520the%2520cognitive%250Aapprenticeship%2520framework.%2520Tutorly%252C%2520developed%2520as%2520a%2520JupyterLab%2520Plugin%252C%2520allows%250Alearners%2520to%2520%25281%2529%2520set%2520personalized%2520learning%2520goals%252C%2520%25282%2529%2520engage%2520in%250Alearning-by-doing%2520through%2520a%2520conversational%2520LLM-based%2520mentor%2520agent%252C%2520%25283%2529%2520receive%250Aguidance%2520and%2520feedback%2520based%2520on%2520a%2520student%2520model%2520that%2520steers%2520the%2520mentor%2520moves.%2520In%250Aa%2520within-subject%2520study%2520with%252016%2520participants%2520learning%2520exploratory%2520data%2520analysis%250Afrom%2520a%2520streamcast%252C%2520Tutorly%2520significantly%2520improved%2520their%2520performance%2520from%252061.9%2525%250Ato%252076.6%2525%2520based%2520on%2520a%2520post-test%2520questionnaire.%2520Tutorly%2520demonstrates%2520the%2520potential%250Afor%2520enhancing%2520programming%2520video%2520learning%2520experiences%2520with%2520LLM%2520and%2520learner%250Amodeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tutorly%3A%20Turning%20Programming%20Videos%20Into%20Apprenticeship%20Learning%0A%20%20Environments%20with%20LLMs&entry.906535625=Wengxi%20Li%20and%20Roy%20Pea%20and%20Nick%20Haber%20and%20Hari%20Subramonyam&entry.1292438233=%20%20Online%20programming%20videos%2C%20including%20tutorials%20and%20streamcasts%2C%20are%20widely%0Apopular%20and%20contain%20a%20wealth%20of%20expert%20knowledge.%20However%2C%20effectively%0Autilizing%20these%20resources%20to%20achieve%20targeted%20learning%20goals%20can%20be%0Achallenging.%20Unlike%20direct%20tutoring%2C%20video%20content%20lacks%20tailored%20guidance%0Abased%20on%20individual%20learning%20paces%2C%20personalized%20feedback%2C%20and%20interactive%0Aengagement%20necessary%20for%20support%20and%20monitoring.%20Our%20work%20transforms%0Aprogramming%20videos%20into%20one-on-one%20tutoring%20experiences%20using%20the%20cognitive%0Aapprenticeship%20framework.%20Tutorly%2C%20developed%20as%20a%20JupyterLab%20Plugin%2C%20allows%0Alearners%20to%20%281%29%20set%20personalized%20learning%20goals%2C%20%282%29%20engage%20in%0Alearning-by-doing%20through%20a%20conversational%20LLM-based%20mentor%20agent%2C%20%283%29%20receive%0Aguidance%20and%20feedback%20based%20on%20a%20student%20model%20that%20steers%20the%20mentor%20moves.%20In%0Aa%20within-subject%20study%20with%2016%20participants%20learning%20exploratory%20data%20analysis%0Afrom%20a%20streamcast%2C%20Tutorly%20significantly%20improved%20their%20performance%20from%2061.9%25%0Ato%2076.6%25%20based%20on%20a%20post-test%20questionnaire.%20Tutorly%20demonstrates%20the%20potential%0Afor%20enhancing%20programming%20video%20learning%20experiences%20with%20LLM%20and%20learner%0Amodeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12946v1&entry.124074799=Read"},
{"title": "Panmodal Information Interaction", "author": "Chirag Shah and Ryen W. White", "abstract": "  The emergence of generative artificial intelligence (GenAI) is transforming\ninformation interaction. For decades, search engines such as Google and Bing\nhave been the primary means of locating relevant information for the general\npopulation. They have provided search results in the same standard format (the\nso-called \"10 blue links\"). The recent ability to chat via natural language\nwith AI-based agents and have GenAI automatically synthesize answers in\nreal-time (grounded in top-ranked results) is changing how people interact with\nand consume information at massive scale. These two information interaction\nmodalities (traditional search and AI-powered chat) coexist in current search\nengines, either loosely coupled (e.g., as separate options/tabs) or tightly\ncoupled (e.g., integrated as a chat answer embedded directly within a\ntraditional search result page). We believe that the existence of these two\ndifferent modalities, and potentially many others, is creating an opportunity\nto re-imagine the search experience, capitalize on the strengths of many\nmodalities, and develop systems and strategies to support seamless flow between\nthem. We refer to these as panmodal experiences. Unlike monomodal experiences,\nwhere only one modality is available and/or used for the task at hand, panmodal\nexperiences make multiple modalities available to users (multimodal), directly\nsupport transitions between modalities (crossmodal), and seamlessly combine\nmodalities to tailor task assistance (transmodal). While our focus is search\nand chat, with learnings from insights from a survey of over 100 individuals\nwho have recently performed common tasks on these two modalities, we also\npresent a more general vision for the future of information interaction using\nmultiple modalities and the emergent capabilities of GenAI.\n", "link": "http://arxiv.org/abs/2405.12923v1", "date": "2024-05-21", "relevancy": 1.9597, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5059}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4988}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Panmodal%20Information%20Interaction&body=Title%3A%20Panmodal%20Information%20Interaction%0AAuthor%3A%20Chirag%20Shah%20and%20Ryen%20W.%20White%0AAbstract%3A%20%20%20The%20emergence%20of%20generative%20artificial%20intelligence%20%28GenAI%29%20is%20transforming%0Ainformation%20interaction.%20For%20decades%2C%20search%20engines%20such%20as%20Google%20and%20Bing%0Ahave%20been%20the%20primary%20means%20of%20locating%20relevant%20information%20for%20the%20general%0Apopulation.%20They%20have%20provided%20search%20results%20in%20the%20same%20standard%20format%20%28the%0Aso-called%20%2210%20blue%20links%22%29.%20The%20recent%20ability%20to%20chat%20via%20natural%20language%0Awith%20AI-based%20agents%20and%20have%20GenAI%20automatically%20synthesize%20answers%20in%0Areal-time%20%28grounded%20in%20top-ranked%20results%29%20is%20changing%20how%20people%20interact%20with%0Aand%20consume%20information%20at%20massive%20scale.%20These%20two%20information%20interaction%0Amodalities%20%28traditional%20search%20and%20AI-powered%20chat%29%20coexist%20in%20current%20search%0Aengines%2C%20either%20loosely%20coupled%20%28e.g.%2C%20as%20separate%20options/tabs%29%20or%20tightly%0Acoupled%20%28e.g.%2C%20integrated%20as%20a%20chat%20answer%20embedded%20directly%20within%20a%0Atraditional%20search%20result%20page%29.%20We%20believe%20that%20the%20existence%20of%20these%20two%0Adifferent%20modalities%2C%20and%20potentially%20many%20others%2C%20is%20creating%20an%20opportunity%0Ato%20re-imagine%20the%20search%20experience%2C%20capitalize%20on%20the%20strengths%20of%20many%0Amodalities%2C%20and%20develop%20systems%20and%20strategies%20to%20support%20seamless%20flow%20between%0Athem.%20We%20refer%20to%20these%20as%20panmodal%20experiences.%20Unlike%20monomodal%20experiences%2C%0Awhere%20only%20one%20modality%20is%20available%20and/or%20used%20for%20the%20task%20at%20hand%2C%20panmodal%0Aexperiences%20make%20multiple%20modalities%20available%20to%20users%20%28multimodal%29%2C%20directly%0Asupport%20transitions%20between%20modalities%20%28crossmodal%29%2C%20and%20seamlessly%20combine%0Amodalities%20to%20tailor%20task%20assistance%20%28transmodal%29.%20While%20our%20focus%20is%20search%0Aand%20chat%2C%20with%20learnings%20from%20insights%20from%20a%20survey%20of%20over%20100%20individuals%0Awho%20have%20recently%20performed%20common%20tasks%20on%20these%20two%20modalities%2C%20we%20also%0Apresent%20a%20more%20general%20vision%20for%20the%20future%20of%20information%20interaction%20using%0Amultiple%20modalities%20and%20the%20emergent%20capabilities%20of%20GenAI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanmodal%2520Information%2520Interaction%26entry.906535625%3DChirag%2520Shah%2520and%2520Ryen%2520W.%2520White%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520generative%2520artificial%2520intelligence%2520%2528GenAI%2529%2520is%2520transforming%250Ainformation%2520interaction.%2520For%2520decades%252C%2520search%2520engines%2520such%2520as%2520Google%2520and%2520Bing%250Ahave%2520been%2520the%2520primary%2520means%2520of%2520locating%2520relevant%2520information%2520for%2520the%2520general%250Apopulation.%2520They%2520have%2520provided%2520search%2520results%2520in%2520the%2520same%2520standard%2520format%2520%2528the%250Aso-called%2520%252210%2520blue%2520links%2522%2529.%2520The%2520recent%2520ability%2520to%2520chat%2520via%2520natural%2520language%250Awith%2520AI-based%2520agents%2520and%2520have%2520GenAI%2520automatically%2520synthesize%2520answers%2520in%250Areal-time%2520%2528grounded%2520in%2520top-ranked%2520results%2529%2520is%2520changing%2520how%2520people%2520interact%2520with%250Aand%2520consume%2520information%2520at%2520massive%2520scale.%2520These%2520two%2520information%2520interaction%250Amodalities%2520%2528traditional%2520search%2520and%2520AI-powered%2520chat%2529%2520coexist%2520in%2520current%2520search%250Aengines%252C%2520either%2520loosely%2520coupled%2520%2528e.g.%252C%2520as%2520separate%2520options/tabs%2529%2520or%2520tightly%250Acoupled%2520%2528e.g.%252C%2520integrated%2520as%2520a%2520chat%2520answer%2520embedded%2520directly%2520within%2520a%250Atraditional%2520search%2520result%2520page%2529.%2520We%2520believe%2520that%2520the%2520existence%2520of%2520these%2520two%250Adifferent%2520modalities%252C%2520and%2520potentially%2520many%2520others%252C%2520is%2520creating%2520an%2520opportunity%250Ato%2520re-imagine%2520the%2520search%2520experience%252C%2520capitalize%2520on%2520the%2520strengths%2520of%2520many%250Amodalities%252C%2520and%2520develop%2520systems%2520and%2520strategies%2520to%2520support%2520seamless%2520flow%2520between%250Athem.%2520We%2520refer%2520to%2520these%2520as%2520panmodal%2520experiences.%2520Unlike%2520monomodal%2520experiences%252C%250Awhere%2520only%2520one%2520modality%2520is%2520available%2520and/or%2520used%2520for%2520the%2520task%2520at%2520hand%252C%2520panmodal%250Aexperiences%2520make%2520multiple%2520modalities%2520available%2520to%2520users%2520%2528multimodal%2529%252C%2520directly%250Asupport%2520transitions%2520between%2520modalities%2520%2528crossmodal%2529%252C%2520and%2520seamlessly%2520combine%250Amodalities%2520to%2520tailor%2520task%2520assistance%2520%2528transmodal%2529.%2520While%2520our%2520focus%2520is%2520search%250Aand%2520chat%252C%2520with%2520learnings%2520from%2520insights%2520from%2520a%2520survey%2520of%2520over%2520100%2520individuals%250Awho%2520have%2520recently%2520performed%2520common%2520tasks%2520on%2520these%2520two%2520modalities%252C%2520we%2520also%250Apresent%2520a%2520more%2520general%2520vision%2520for%2520the%2520future%2520of%2520information%2520interaction%2520using%250Amultiple%2520modalities%2520and%2520the%2520emergent%2520capabilities%2520of%2520GenAI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Panmodal%20Information%20Interaction&entry.906535625=Chirag%20Shah%20and%20Ryen%20W.%20White&entry.1292438233=%20%20The%20emergence%20of%20generative%20artificial%20intelligence%20%28GenAI%29%20is%20transforming%0Ainformation%20interaction.%20For%20decades%2C%20search%20engines%20such%20as%20Google%20and%20Bing%0Ahave%20been%20the%20primary%20means%20of%20locating%20relevant%20information%20for%20the%20general%0Apopulation.%20They%20have%20provided%20search%20results%20in%20the%20same%20standard%20format%20%28the%0Aso-called%20%2210%20blue%20links%22%29.%20The%20recent%20ability%20to%20chat%20via%20natural%20language%0Awith%20AI-based%20agents%20and%20have%20GenAI%20automatically%20synthesize%20answers%20in%0Areal-time%20%28grounded%20in%20top-ranked%20results%29%20is%20changing%20how%20people%20interact%20with%0Aand%20consume%20information%20at%20massive%20scale.%20These%20two%20information%20interaction%0Amodalities%20%28traditional%20search%20and%20AI-powered%20chat%29%20coexist%20in%20current%20search%0Aengines%2C%20either%20loosely%20coupled%20%28e.g.%2C%20as%20separate%20options/tabs%29%20or%20tightly%0Acoupled%20%28e.g.%2C%20integrated%20as%20a%20chat%20answer%20embedded%20directly%20within%20a%0Atraditional%20search%20result%20page%29.%20We%20believe%20that%20the%20existence%20of%20these%20two%0Adifferent%20modalities%2C%20and%20potentially%20many%20others%2C%20is%20creating%20an%20opportunity%0Ato%20re-imagine%20the%20search%20experience%2C%20capitalize%20on%20the%20strengths%20of%20many%0Amodalities%2C%20and%20develop%20systems%20and%20strategies%20to%20support%20seamless%20flow%20between%0Athem.%20We%20refer%20to%20these%20as%20panmodal%20experiences.%20Unlike%20monomodal%20experiences%2C%0Awhere%20only%20one%20modality%20is%20available%20and/or%20used%20for%20the%20task%20at%20hand%2C%20panmodal%0Aexperiences%20make%20multiple%20modalities%20available%20to%20users%20%28multimodal%29%2C%20directly%0Asupport%20transitions%20between%20modalities%20%28crossmodal%29%2C%20and%20seamlessly%20combine%0Amodalities%20to%20tailor%20task%20assistance%20%28transmodal%29.%20While%20our%20focus%20is%20search%0Aand%20chat%2C%20with%20learnings%20from%20insights%20from%20a%20survey%20of%20over%20100%20individuals%0Awho%20have%20recently%20performed%20common%20tasks%20on%20these%20two%20modalities%2C%20we%20also%0Apresent%20a%20more%20general%20vision%20for%20the%20future%20of%20information%20interaction%20using%0Amultiple%20modalities%20and%20the%20emergent%20capabilities%20of%20GenAI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12923v1&entry.124074799=Read"},
{"title": "Safety Filters for Black-Box Dynamical Systems by Learning\n  Discriminating Hyperplanes", "author": "Will Lavanakul and Jason J. Choi and Koushil Sreenath and Claire J. Tomlin", "abstract": "  Learning-based approaches are emerging as an effective approach for safety\nfilters for black-box dynamical systems. Existing methods have relied on\ncertificate functions like Control Barrier Functions (CBFs) and Hamilton-Jacobi\n(HJ) reachability value functions. The primary motivation for our work is the\nrecognition that ultimately, enforcing the safety constraint as a control input\nconstraint at each state is what matters. By focusing on this constraint, we\ncan eliminate dependence on any specific certificate function-based design. To\nachieve this, we define a discriminating hyperplane that shapes the half-space\nconstraint on control input at each state, serving as a sufficient condition\nfor safety. This concept not only generalizes over traditional safety methods\nbut also simplifies safety filter design by eliminating dependence on specific\ncertificate functions. We present two strategies to learn the discriminating\nhyperplane: (a) a supervised learning approach, using pre-verified control\ninvariant sets for labeling, and (b) a reinforcement learning (RL) approach,\nwhich does not require such labels. The main advantage of our method, unlike\nconventional safe RL approaches, is the separation of performance and safety.\nThis offers a reusable safety filter for learning new tasks, avoiding the need\nto retrain from scratch. As such, we believe that the new notion of the\ndiscriminating hyperplane offers a more generalizable direction towards\ndesigning safety filters, encompassing and extending existing\ncertificate-function-based or safe RL methodologies.\n", "link": "http://arxiv.org/abs/2402.05279v2", "date": "2024-05-21", "relevancy": 1.9583, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.519}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4927}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safety%20Filters%20for%20Black-Box%20Dynamical%20Systems%20by%20Learning%0A%20%20Discriminating%20Hyperplanes&body=Title%3A%20Safety%20Filters%20for%20Black-Box%20Dynamical%20Systems%20by%20Learning%0A%20%20Discriminating%20Hyperplanes%0AAuthor%3A%20Will%20Lavanakul%20and%20Jason%20J.%20Choi%20and%20Koushil%20Sreenath%20and%20Claire%20J.%20Tomlin%0AAbstract%3A%20%20%20Learning-based%20approaches%20are%20emerging%20as%20an%20effective%20approach%20for%20safety%0Afilters%20for%20black-box%20dynamical%20systems.%20Existing%20methods%20have%20relied%20on%0Acertificate%20functions%20like%20Control%20Barrier%20Functions%20%28CBFs%29%20and%20Hamilton-Jacobi%0A%28HJ%29%20reachability%20value%20functions.%20The%20primary%20motivation%20for%20our%20work%20is%20the%0Arecognition%20that%20ultimately%2C%20enforcing%20the%20safety%20constraint%20as%20a%20control%20input%0Aconstraint%20at%20each%20state%20is%20what%20matters.%20By%20focusing%20on%20this%20constraint%2C%20we%0Acan%20eliminate%20dependence%20on%20any%20specific%20certificate%20function-based%20design.%20To%0Aachieve%20this%2C%20we%20define%20a%20discriminating%20hyperplane%20that%20shapes%20the%20half-space%0Aconstraint%20on%20control%20input%20at%20each%20state%2C%20serving%20as%20a%20sufficient%20condition%0Afor%20safety.%20This%20concept%20not%20only%20generalizes%20over%20traditional%20safety%20methods%0Abut%20also%20simplifies%20safety%20filter%20design%20by%20eliminating%20dependence%20on%20specific%0Acertificate%20functions.%20We%20present%20two%20strategies%20to%20learn%20the%20discriminating%0Ahyperplane%3A%20%28a%29%20a%20supervised%20learning%20approach%2C%20using%20pre-verified%20control%0Ainvariant%20sets%20for%20labeling%2C%20and%20%28b%29%20a%20reinforcement%20learning%20%28RL%29%20approach%2C%0Awhich%20does%20not%20require%20such%20labels.%20The%20main%20advantage%20of%20our%20method%2C%20unlike%0Aconventional%20safe%20RL%20approaches%2C%20is%20the%20separation%20of%20performance%20and%20safety.%0AThis%20offers%20a%20reusable%20safety%20filter%20for%20learning%20new%20tasks%2C%20avoiding%20the%20need%0Ato%20retrain%20from%20scratch.%20As%20such%2C%20we%20believe%20that%20the%20new%20notion%20of%20the%0Adiscriminating%20hyperplane%20offers%20a%20more%20generalizable%20direction%20towards%0Adesigning%20safety%20filters%2C%20encompassing%20and%20extending%20existing%0Acertificate-function-based%20or%20safe%20RL%20methodologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05279v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafety%2520Filters%2520for%2520Black-Box%2520Dynamical%2520Systems%2520by%2520Learning%250A%2520%2520Discriminating%2520Hyperplanes%26entry.906535625%3DWill%2520Lavanakul%2520and%2520Jason%2520J.%2520Choi%2520and%2520Koushil%2520Sreenath%2520and%2520Claire%2520J.%2520Tomlin%26entry.1292438233%3D%2520%2520Learning-based%2520approaches%2520are%2520emerging%2520as%2520an%2520effective%2520approach%2520for%2520safety%250Afilters%2520for%2520black-box%2520dynamical%2520systems.%2520Existing%2520methods%2520have%2520relied%2520on%250Acertificate%2520functions%2520like%2520Control%2520Barrier%2520Functions%2520%2528CBFs%2529%2520and%2520Hamilton-Jacobi%250A%2528HJ%2529%2520reachability%2520value%2520functions.%2520The%2520primary%2520motivation%2520for%2520our%2520work%2520is%2520the%250Arecognition%2520that%2520ultimately%252C%2520enforcing%2520the%2520safety%2520constraint%2520as%2520a%2520control%2520input%250Aconstraint%2520at%2520each%2520state%2520is%2520what%2520matters.%2520By%2520focusing%2520on%2520this%2520constraint%252C%2520we%250Acan%2520eliminate%2520dependence%2520on%2520any%2520specific%2520certificate%2520function-based%2520design.%2520To%250Aachieve%2520this%252C%2520we%2520define%2520a%2520discriminating%2520hyperplane%2520that%2520shapes%2520the%2520half-space%250Aconstraint%2520on%2520control%2520input%2520at%2520each%2520state%252C%2520serving%2520as%2520a%2520sufficient%2520condition%250Afor%2520safety.%2520This%2520concept%2520not%2520only%2520generalizes%2520over%2520traditional%2520safety%2520methods%250Abut%2520also%2520simplifies%2520safety%2520filter%2520design%2520by%2520eliminating%2520dependence%2520on%2520specific%250Acertificate%2520functions.%2520We%2520present%2520two%2520strategies%2520to%2520learn%2520the%2520discriminating%250Ahyperplane%253A%2520%2528a%2529%2520a%2520supervised%2520learning%2520approach%252C%2520using%2520pre-verified%2520control%250Ainvariant%2520sets%2520for%2520labeling%252C%2520and%2520%2528b%2529%2520a%2520reinforcement%2520learning%2520%2528RL%2529%2520approach%252C%250Awhich%2520does%2520not%2520require%2520such%2520labels.%2520The%2520main%2520advantage%2520of%2520our%2520method%252C%2520unlike%250Aconventional%2520safe%2520RL%2520approaches%252C%2520is%2520the%2520separation%2520of%2520performance%2520and%2520safety.%250AThis%2520offers%2520a%2520reusable%2520safety%2520filter%2520for%2520learning%2520new%2520tasks%252C%2520avoiding%2520the%2520need%250Ato%2520retrain%2520from%2520scratch.%2520As%2520such%252C%2520we%2520believe%2520that%2520the%2520new%2520notion%2520of%2520the%250Adiscriminating%2520hyperplane%2520offers%2520a%2520more%2520generalizable%2520direction%2520towards%250Adesigning%2520safety%2520filters%252C%2520encompassing%2520and%2520extending%2520existing%250Acertificate-function-based%2520or%2520safe%2520RL%2520methodologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05279v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safety%20Filters%20for%20Black-Box%20Dynamical%20Systems%20by%20Learning%0A%20%20Discriminating%20Hyperplanes&entry.906535625=Will%20Lavanakul%20and%20Jason%20J.%20Choi%20and%20Koushil%20Sreenath%20and%20Claire%20J.%20Tomlin&entry.1292438233=%20%20Learning-based%20approaches%20are%20emerging%20as%20an%20effective%20approach%20for%20safety%0Afilters%20for%20black-box%20dynamical%20systems.%20Existing%20methods%20have%20relied%20on%0Acertificate%20functions%20like%20Control%20Barrier%20Functions%20%28CBFs%29%20and%20Hamilton-Jacobi%0A%28HJ%29%20reachability%20value%20functions.%20The%20primary%20motivation%20for%20our%20work%20is%20the%0Arecognition%20that%20ultimately%2C%20enforcing%20the%20safety%20constraint%20as%20a%20control%20input%0Aconstraint%20at%20each%20state%20is%20what%20matters.%20By%20focusing%20on%20this%20constraint%2C%20we%0Acan%20eliminate%20dependence%20on%20any%20specific%20certificate%20function-based%20design.%20To%0Aachieve%20this%2C%20we%20define%20a%20discriminating%20hyperplane%20that%20shapes%20the%20half-space%0Aconstraint%20on%20control%20input%20at%20each%20state%2C%20serving%20as%20a%20sufficient%20condition%0Afor%20safety.%20This%20concept%20not%20only%20generalizes%20over%20traditional%20safety%20methods%0Abut%20also%20simplifies%20safety%20filter%20design%20by%20eliminating%20dependence%20on%20specific%0Acertificate%20functions.%20We%20present%20two%20strategies%20to%20learn%20the%20discriminating%0Ahyperplane%3A%20%28a%29%20a%20supervised%20learning%20approach%2C%20using%20pre-verified%20control%0Ainvariant%20sets%20for%20labeling%2C%20and%20%28b%29%20a%20reinforcement%20learning%20%28RL%29%20approach%2C%0Awhich%20does%20not%20require%20such%20labels.%20The%20main%20advantage%20of%20our%20method%2C%20unlike%0Aconventional%20safe%20RL%20approaches%2C%20is%20the%20separation%20of%20performance%20and%20safety.%0AThis%20offers%20a%20reusable%20safety%20filter%20for%20learning%20new%20tasks%2C%20avoiding%20the%20need%0Ato%20retrain%20from%20scratch.%20As%20such%2C%20we%20believe%20that%20the%20new%20notion%20of%20the%0Adiscriminating%20hyperplane%20offers%20a%20more%20generalizable%20direction%20towards%0Adesigning%20safety%20filters%2C%20encompassing%20and%20extending%20existing%0Acertificate-function-based%20or%20safe%20RL%20methodologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05279v2&entry.124074799=Read"},
{"title": "Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense\n  of Privacy", "author": "Jamie Hayes and Ilia Shumailov and Eleni Triantafillou and Amr Khalifa and Nicolas Papernot", "abstract": "  The high cost of model training makes it increasingly desirable to develop\ntechniques for unlearning. These techniques seek to remove the influence of a\ntraining example without having to retrain the model from scratch. Intuitively,\nonce a model has unlearned, an adversary that interacts with the model should\nno longer be able to tell whether the unlearned example was included in the\nmodel's training set or not. In the privacy literature, this is known as\nmembership inference. In this work, we discuss adaptations of Membership\nInference Attacks (MIAs) to the setting of unlearning (leading to their \"U-MIA\"\ncounterparts). We propose a categorization of existing U-MIAs into \"population\nU-MIAs\", where the same attacker is instantiated for all examples, and\n\"per-example U-MIAs\", where a dedicated attacker is instantiated for each\nexample. We show that the latter category, wherein the attacker tailors its\nmembership prediction to each example under attack, is significantly stronger.\nIndeed, our results show that the commonly used U-MIAs in the unlearning\nliterature overestimate the privacy protection afforded by existing unlearning\ntechniques on both vision and language models. Our investigation reveals a\nlarge variance in the vulnerability of different examples to per-example\nU-MIAs. In fact, several unlearning algorithms lead to a reduced vulnerability\nfor some, but not all, examples that we wish to unlearn, at the expense of\nincreasing it for other examples. Notably, we find that the privacy protection\nfor the remaining training examples may worsen as a consequence of unlearning.\nWe also discuss the fundamental difficulty of equally protecting all examples\nusing existing unlearning schemes, due to the different rates at which examples\nare unlearned. We demonstrate that naive attempts at tailoring unlearning\nstopping criteria to different examples fail to alleviate these issues.\n", "link": "http://arxiv.org/abs/2403.01218v3", "date": "2024-05-21", "relevancy": 1.9381, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5078}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4801}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inexact%20Unlearning%20Needs%20More%20Careful%20Evaluations%20to%20Avoid%20a%20False%20Sense%0A%20%20of%20Privacy&body=Title%3A%20Inexact%20Unlearning%20Needs%20More%20Careful%20Evaluations%20to%20Avoid%20a%20False%20Sense%0A%20%20of%20Privacy%0AAuthor%3A%20Jamie%20Hayes%20and%20Ilia%20Shumailov%20and%20Eleni%20Triantafillou%20and%20Amr%20Khalifa%20and%20Nicolas%20Papernot%0AAbstract%3A%20%20%20The%20high%20cost%20of%20model%20training%20makes%20it%20increasingly%20desirable%20to%20develop%0Atechniques%20for%20unlearning.%20These%20techniques%20seek%20to%20remove%20the%20influence%20of%20a%0Atraining%20example%20without%20having%20to%20retrain%20the%20model%20from%20scratch.%20Intuitively%2C%0Aonce%20a%20model%20has%20unlearned%2C%20an%20adversary%20that%20interacts%20with%20the%20model%20should%0Ano%20longer%20be%20able%20to%20tell%20whether%20the%20unlearned%20example%20was%20included%20in%20the%0Amodel%27s%20training%20set%20or%20not.%20In%20the%20privacy%20literature%2C%20this%20is%20known%20as%0Amembership%20inference.%20In%20this%20work%2C%20we%20discuss%20adaptations%20of%20Membership%0AInference%20Attacks%20%28MIAs%29%20to%20the%20setting%20of%20unlearning%20%28leading%20to%20their%20%22U-MIA%22%0Acounterparts%29.%20We%20propose%20a%20categorization%20of%20existing%20U-MIAs%20into%20%22population%0AU-MIAs%22%2C%20where%20the%20same%20attacker%20is%20instantiated%20for%20all%20examples%2C%20and%0A%22per-example%20U-MIAs%22%2C%20where%20a%20dedicated%20attacker%20is%20instantiated%20for%20each%0Aexample.%20We%20show%20that%20the%20latter%20category%2C%20wherein%20the%20attacker%20tailors%20its%0Amembership%20prediction%20to%20each%20example%20under%20attack%2C%20is%20significantly%20stronger.%0AIndeed%2C%20our%20results%20show%20that%20the%20commonly%20used%20U-MIAs%20in%20the%20unlearning%0Aliterature%20overestimate%20the%20privacy%20protection%20afforded%20by%20existing%20unlearning%0Atechniques%20on%20both%20vision%20and%20language%20models.%20Our%20investigation%20reveals%20a%0Alarge%20variance%20in%20the%20vulnerability%20of%20different%20examples%20to%20per-example%0AU-MIAs.%20In%20fact%2C%20several%20unlearning%20algorithms%20lead%20to%20a%20reduced%20vulnerability%0Afor%20some%2C%20but%20not%20all%2C%20examples%20that%20we%20wish%20to%20unlearn%2C%20at%20the%20expense%20of%0Aincreasing%20it%20for%20other%20examples.%20Notably%2C%20we%20find%20that%20the%20privacy%20protection%0Afor%20the%20remaining%20training%20examples%20may%20worsen%20as%20a%20consequence%20of%20unlearning.%0AWe%20also%20discuss%20the%20fundamental%20difficulty%20of%20equally%20protecting%20all%20examples%0Ausing%20existing%20unlearning%20schemes%2C%20due%20to%20the%20different%20rates%20at%20which%20examples%0Aare%20unlearned.%20We%20demonstrate%20that%20naive%20attempts%20at%20tailoring%20unlearning%0Astopping%20criteria%20to%20different%20examples%20fail%20to%20alleviate%20these%20issues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01218v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInexact%2520Unlearning%2520Needs%2520More%2520Careful%2520Evaluations%2520to%2520Avoid%2520a%2520False%2520Sense%250A%2520%2520of%2520Privacy%26entry.906535625%3DJamie%2520Hayes%2520and%2520Ilia%2520Shumailov%2520and%2520Eleni%2520Triantafillou%2520and%2520Amr%2520Khalifa%2520and%2520Nicolas%2520Papernot%26entry.1292438233%3D%2520%2520The%2520high%2520cost%2520of%2520model%2520training%2520makes%2520it%2520increasingly%2520desirable%2520to%2520develop%250Atechniques%2520for%2520unlearning.%2520These%2520techniques%2520seek%2520to%2520remove%2520the%2520influence%2520of%2520a%250Atraining%2520example%2520without%2520having%2520to%2520retrain%2520the%2520model%2520from%2520scratch.%2520Intuitively%252C%250Aonce%2520a%2520model%2520has%2520unlearned%252C%2520an%2520adversary%2520that%2520interacts%2520with%2520the%2520model%2520should%250Ano%2520longer%2520be%2520able%2520to%2520tell%2520whether%2520the%2520unlearned%2520example%2520was%2520included%2520in%2520the%250Amodel%2527s%2520training%2520set%2520or%2520not.%2520In%2520the%2520privacy%2520literature%252C%2520this%2520is%2520known%2520as%250Amembership%2520inference.%2520In%2520this%2520work%252C%2520we%2520discuss%2520adaptations%2520of%2520Membership%250AInference%2520Attacks%2520%2528MIAs%2529%2520to%2520the%2520setting%2520of%2520unlearning%2520%2528leading%2520to%2520their%2520%2522U-MIA%2522%250Acounterparts%2529.%2520We%2520propose%2520a%2520categorization%2520of%2520existing%2520U-MIAs%2520into%2520%2522population%250AU-MIAs%2522%252C%2520where%2520the%2520same%2520attacker%2520is%2520instantiated%2520for%2520all%2520examples%252C%2520and%250A%2522per-example%2520U-MIAs%2522%252C%2520where%2520a%2520dedicated%2520attacker%2520is%2520instantiated%2520for%2520each%250Aexample.%2520We%2520show%2520that%2520the%2520latter%2520category%252C%2520wherein%2520the%2520attacker%2520tailors%2520its%250Amembership%2520prediction%2520to%2520each%2520example%2520under%2520attack%252C%2520is%2520significantly%2520stronger.%250AIndeed%252C%2520our%2520results%2520show%2520that%2520the%2520commonly%2520used%2520U-MIAs%2520in%2520the%2520unlearning%250Aliterature%2520overestimate%2520the%2520privacy%2520protection%2520afforded%2520by%2520existing%2520unlearning%250Atechniques%2520on%2520both%2520vision%2520and%2520language%2520models.%2520Our%2520investigation%2520reveals%2520a%250Alarge%2520variance%2520in%2520the%2520vulnerability%2520of%2520different%2520examples%2520to%2520per-example%250AU-MIAs.%2520In%2520fact%252C%2520several%2520unlearning%2520algorithms%2520lead%2520to%2520a%2520reduced%2520vulnerability%250Afor%2520some%252C%2520but%2520not%2520all%252C%2520examples%2520that%2520we%2520wish%2520to%2520unlearn%252C%2520at%2520the%2520expense%2520of%250Aincreasing%2520it%2520for%2520other%2520examples.%2520Notably%252C%2520we%2520find%2520that%2520the%2520privacy%2520protection%250Afor%2520the%2520remaining%2520training%2520examples%2520may%2520worsen%2520as%2520a%2520consequence%2520of%2520unlearning.%250AWe%2520also%2520discuss%2520the%2520fundamental%2520difficulty%2520of%2520equally%2520protecting%2520all%2520examples%250Ausing%2520existing%2520unlearning%2520schemes%252C%2520due%2520to%2520the%2520different%2520rates%2520at%2520which%2520examples%250Aare%2520unlearned.%2520We%2520demonstrate%2520that%2520naive%2520attempts%2520at%2520tailoring%2520unlearning%250Astopping%2520criteria%2520to%2520different%2520examples%2520fail%2520to%2520alleviate%2520these%2520issues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01218v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inexact%20Unlearning%20Needs%20More%20Careful%20Evaluations%20to%20Avoid%20a%20False%20Sense%0A%20%20of%20Privacy&entry.906535625=Jamie%20Hayes%20and%20Ilia%20Shumailov%20and%20Eleni%20Triantafillou%20and%20Amr%20Khalifa%20and%20Nicolas%20Papernot&entry.1292438233=%20%20The%20high%20cost%20of%20model%20training%20makes%20it%20increasingly%20desirable%20to%20develop%0Atechniques%20for%20unlearning.%20These%20techniques%20seek%20to%20remove%20the%20influence%20of%20a%0Atraining%20example%20without%20having%20to%20retrain%20the%20model%20from%20scratch.%20Intuitively%2C%0Aonce%20a%20model%20has%20unlearned%2C%20an%20adversary%20that%20interacts%20with%20the%20model%20should%0Ano%20longer%20be%20able%20to%20tell%20whether%20the%20unlearned%20example%20was%20included%20in%20the%0Amodel%27s%20training%20set%20or%20not.%20In%20the%20privacy%20literature%2C%20this%20is%20known%20as%0Amembership%20inference.%20In%20this%20work%2C%20we%20discuss%20adaptations%20of%20Membership%0AInference%20Attacks%20%28MIAs%29%20to%20the%20setting%20of%20unlearning%20%28leading%20to%20their%20%22U-MIA%22%0Acounterparts%29.%20We%20propose%20a%20categorization%20of%20existing%20U-MIAs%20into%20%22population%0AU-MIAs%22%2C%20where%20the%20same%20attacker%20is%20instantiated%20for%20all%20examples%2C%20and%0A%22per-example%20U-MIAs%22%2C%20where%20a%20dedicated%20attacker%20is%20instantiated%20for%20each%0Aexample.%20We%20show%20that%20the%20latter%20category%2C%20wherein%20the%20attacker%20tailors%20its%0Amembership%20prediction%20to%20each%20example%20under%20attack%2C%20is%20significantly%20stronger.%0AIndeed%2C%20our%20results%20show%20that%20the%20commonly%20used%20U-MIAs%20in%20the%20unlearning%0Aliterature%20overestimate%20the%20privacy%20protection%20afforded%20by%20existing%20unlearning%0Atechniques%20on%20both%20vision%20and%20language%20models.%20Our%20investigation%20reveals%20a%0Alarge%20variance%20in%20the%20vulnerability%20of%20different%20examples%20to%20per-example%0AU-MIAs.%20In%20fact%2C%20several%20unlearning%20algorithms%20lead%20to%20a%20reduced%20vulnerability%0Afor%20some%2C%20but%20not%20all%2C%20examples%20that%20we%20wish%20to%20unlearn%2C%20at%20the%20expense%20of%0Aincreasing%20it%20for%20other%20examples.%20Notably%2C%20we%20find%20that%20the%20privacy%20protection%0Afor%20the%20remaining%20training%20examples%20may%20worsen%20as%20a%20consequence%20of%20unlearning.%0AWe%20also%20discuss%20the%20fundamental%20difficulty%20of%20equally%20protecting%20all%20examples%0Ausing%20existing%20unlearning%20schemes%2C%20due%20to%20the%20different%20rates%20at%20which%20examples%0Aare%20unlearned.%20We%20demonstrate%20that%20naive%20attempts%20at%20tailoring%20unlearning%0Astopping%20criteria%20to%20different%20examples%20fail%20to%20alleviate%20these%20issues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01218v3&entry.124074799=Read"},
{"title": "Towards Faithful and Robust LLM Specialists for Evidence-Based\n  Question-Answering", "author": "Tobias Schimanski and Jingwei Ni and Mathias Kraus and Elliott Ash and Markus Leippold", "abstract": "  Advances towards more faithful and traceable answers of Large Language Models\n(LLMs) are crucial for various research and practical endeavors. One avenue in\nreaching this goal is basing the answers on reliable sources. However, this\nEvidence-Based QA has proven to work insufficiently with LLMs in terms of\nciting the correct sources (source quality) and truthfully representing the\ninformation within sources (answer attributability). In this work, we\nsystematically investigate how to robustly fine-tune LLMs for better source\nquality and answer attributability. Specifically, we introduce a data\ngeneration pipeline with automated data quality filters, which can synthesize\ndiversified high-quality training and testing data at scale. We further\nintroduce four test sets to benchmark the robustness of fine-tuned specialist\nmodels. Extensive evaluation shows that fine-tuning on synthetic data improves\nperformance on both in- and out-of-distribution. Furthermore, we show that data\nquality, which can be drastically improved by proposed quality filters, matters\nmore than quantity in improving Evidence-Based QA.\n", "link": "http://arxiv.org/abs/2402.08277v4", "date": "2024-05-21", "relevancy": 1.9381, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5658}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4799}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Faithful%20and%20Robust%20LLM%20Specialists%20for%20Evidence-Based%0A%20%20Question-Answering&body=Title%3A%20Towards%20Faithful%20and%20Robust%20LLM%20Specialists%20for%20Evidence-Based%0A%20%20Question-Answering%0AAuthor%3A%20Tobias%20Schimanski%20and%20Jingwei%20Ni%20and%20Mathias%20Kraus%20and%20Elliott%20Ash%20and%20Markus%20Leippold%0AAbstract%3A%20%20%20Advances%20towards%20more%20faithful%20and%20traceable%20answers%20of%20Large%20Language%20Models%0A%28LLMs%29%20are%20crucial%20for%20various%20research%20and%20practical%20endeavors.%20One%20avenue%20in%0Areaching%20this%20goal%20is%20basing%20the%20answers%20on%20reliable%20sources.%20However%2C%20this%0AEvidence-Based%20QA%20has%20proven%20to%20work%20insufficiently%20with%20LLMs%20in%20terms%20of%0Aciting%20the%20correct%20sources%20%28source%20quality%29%20and%20truthfully%20representing%20the%0Ainformation%20within%20sources%20%28answer%20attributability%29.%20In%20this%20work%2C%20we%0Asystematically%20investigate%20how%20to%20robustly%20fine-tune%20LLMs%20for%20better%20source%0Aquality%20and%20answer%20attributability.%20Specifically%2C%20we%20introduce%20a%20data%0Ageneration%20pipeline%20with%20automated%20data%20quality%20filters%2C%20which%20can%20synthesize%0Adiversified%20high-quality%20training%20and%20testing%20data%20at%20scale.%20We%20further%0Aintroduce%20four%20test%20sets%20to%20benchmark%20the%20robustness%20of%20fine-tuned%20specialist%0Amodels.%20Extensive%20evaluation%20shows%20that%20fine-tuning%20on%20synthetic%20data%20improves%0Aperformance%20on%20both%20in-%20and%20out-of-distribution.%20Furthermore%2C%20we%20show%20that%20data%0Aquality%2C%20which%20can%20be%20drastically%20improved%20by%20proposed%20quality%20filters%2C%20matters%0Amore%20than%20quantity%20in%20improving%20Evidence-Based%20QA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08277v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Faithful%2520and%2520Robust%2520LLM%2520Specialists%2520for%2520Evidence-Based%250A%2520%2520Question-Answering%26entry.906535625%3DTobias%2520Schimanski%2520and%2520Jingwei%2520Ni%2520and%2520Mathias%2520Kraus%2520and%2520Elliott%2520Ash%2520and%2520Markus%2520Leippold%26entry.1292438233%3D%2520%2520Advances%2520towards%2520more%2520faithful%2520and%2520traceable%2520answers%2520of%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520are%2520crucial%2520for%2520various%2520research%2520and%2520practical%2520endeavors.%2520One%2520avenue%2520in%250Areaching%2520this%2520goal%2520is%2520basing%2520the%2520answers%2520on%2520reliable%2520sources.%2520However%252C%2520this%250AEvidence-Based%2520QA%2520has%2520proven%2520to%2520work%2520insufficiently%2520with%2520LLMs%2520in%2520terms%2520of%250Aciting%2520the%2520correct%2520sources%2520%2528source%2520quality%2529%2520and%2520truthfully%2520representing%2520the%250Ainformation%2520within%2520sources%2520%2528answer%2520attributability%2529.%2520In%2520this%2520work%252C%2520we%250Asystematically%2520investigate%2520how%2520to%2520robustly%2520fine-tune%2520LLMs%2520for%2520better%2520source%250Aquality%2520and%2520answer%2520attributability.%2520Specifically%252C%2520we%2520introduce%2520a%2520data%250Ageneration%2520pipeline%2520with%2520automated%2520data%2520quality%2520filters%252C%2520which%2520can%2520synthesize%250Adiversified%2520high-quality%2520training%2520and%2520testing%2520data%2520at%2520scale.%2520We%2520further%250Aintroduce%2520four%2520test%2520sets%2520to%2520benchmark%2520the%2520robustness%2520of%2520fine-tuned%2520specialist%250Amodels.%2520Extensive%2520evaluation%2520shows%2520that%2520fine-tuning%2520on%2520synthetic%2520data%2520improves%250Aperformance%2520on%2520both%2520in-%2520and%2520out-of-distribution.%2520Furthermore%252C%2520we%2520show%2520that%2520data%250Aquality%252C%2520which%2520can%2520be%2520drastically%2520improved%2520by%2520proposed%2520quality%2520filters%252C%2520matters%250Amore%2520than%2520quantity%2520in%2520improving%2520Evidence-Based%2520QA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08277v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Faithful%20and%20Robust%20LLM%20Specialists%20for%20Evidence-Based%0A%20%20Question-Answering&entry.906535625=Tobias%20Schimanski%20and%20Jingwei%20Ni%20and%20Mathias%20Kraus%20and%20Elliott%20Ash%20and%20Markus%20Leippold&entry.1292438233=%20%20Advances%20towards%20more%20faithful%20and%20traceable%20answers%20of%20Large%20Language%20Models%0A%28LLMs%29%20are%20crucial%20for%20various%20research%20and%20practical%20endeavors.%20One%20avenue%20in%0Areaching%20this%20goal%20is%20basing%20the%20answers%20on%20reliable%20sources.%20However%2C%20this%0AEvidence-Based%20QA%20has%20proven%20to%20work%20insufficiently%20with%20LLMs%20in%20terms%20of%0Aciting%20the%20correct%20sources%20%28source%20quality%29%20and%20truthfully%20representing%20the%0Ainformation%20within%20sources%20%28answer%20attributability%29.%20In%20this%20work%2C%20we%0Asystematically%20investigate%20how%20to%20robustly%20fine-tune%20LLMs%20for%20better%20source%0Aquality%20and%20answer%20attributability.%20Specifically%2C%20we%20introduce%20a%20data%0Ageneration%20pipeline%20with%20automated%20data%20quality%20filters%2C%20which%20can%20synthesize%0Adiversified%20high-quality%20training%20and%20testing%20data%20at%20scale.%20We%20further%0Aintroduce%20four%20test%20sets%20to%20benchmark%20the%20robustness%20of%20fine-tuned%20specialist%0Amodels.%20Extensive%20evaluation%20shows%20that%20fine-tuning%20on%20synthetic%20data%20improves%0Aperformance%20on%20both%20in-%20and%20out-of-distribution.%20Furthermore%2C%20we%20show%20that%20data%0Aquality%2C%20which%20can%20be%20drastically%20improved%20by%20proposed%20quality%20filters%2C%20matters%0Amore%20than%20quantity%20in%20improving%20Evidence-Based%20QA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08277v4&entry.124074799=Read"},
{"title": "AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge\n  Networks", "author": "Zheng Lin and Guanqiao Qu and Wei Wei and Xianhao Chen and Kin K. Leung", "abstract": "  The increasing complexity of deep neural networks poses significant barriers\nto democratizing them to resource-limited edge devices. To address this\nchallenge, split federated learning (SFL) has emerged as a promising solution\nby of floading the primary training workload to a server via model partitioning\nwhile enabling parallel training among edge devices. However, although system\noptimization substantially influences the performance of SFL under\nresource-constrained systems, the problem remains largely uncharted. In this\npaper, we provide a convergence analysis of SFL which quantifies the impact of\nmodel splitting (MS) and client-side model aggregation (MA) on the learning\nperformance, serving as a theoretical foundation. Then, we propose AdaptSFL, a\nnovel resource-adaptive SFL framework, to expedite SFL under\nresource-constrained edge computing systems. Specifically, AdaptSFL adaptively\ncontrols client-side MA and MS to balance communication-computing latency and\ntraining convergence. Extensive simulations across various datasets validate\nthat our proposed AdaptSFL framework takes considerably less time to achieve a\ntarget accuracy than benchmarks, demonstrating the effectiveness of the\nproposed strategies.\n", "link": "http://arxiv.org/abs/2403.13101v2", "date": "2024-05-21", "relevancy": 1.9252, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5083}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4851}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaptSFL%3A%20Adaptive%20Split%20Federated%20Learning%20in%20Resource-constrained%20Edge%0A%20%20Networks&body=Title%3A%20AdaptSFL%3A%20Adaptive%20Split%20Federated%20Learning%20in%20Resource-constrained%20Edge%0A%20%20Networks%0AAuthor%3A%20Zheng%20Lin%20and%20Guanqiao%20Qu%20and%20Wei%20Wei%20and%20Xianhao%20Chen%20and%20Kin%20K.%20Leung%0AAbstract%3A%20%20%20The%20increasing%20complexity%20of%20deep%20neural%20networks%20poses%20significant%20barriers%0Ato%20democratizing%20them%20to%20resource-limited%20edge%20devices.%20To%20address%20this%0Achallenge%2C%20split%20federated%20learning%20%28SFL%29%20has%20emerged%20as%20a%20promising%20solution%0Aby%20of%20floading%20the%20primary%20training%20workload%20to%20a%20server%20via%20model%20partitioning%0Awhile%20enabling%20parallel%20training%20among%20edge%20devices.%20However%2C%20although%20system%0Aoptimization%20substantially%20influences%20the%20performance%20of%20SFL%20under%0Aresource-constrained%20systems%2C%20the%20problem%20remains%20largely%20uncharted.%20In%20this%0Apaper%2C%20we%20provide%20a%20convergence%20analysis%20of%20SFL%20which%20quantifies%20the%20impact%20of%0Amodel%20splitting%20%28MS%29%20and%20client-side%20model%20aggregation%20%28MA%29%20on%20the%20learning%0Aperformance%2C%20serving%20as%20a%20theoretical%20foundation.%20Then%2C%20we%20propose%20AdaptSFL%2C%20a%0Anovel%20resource-adaptive%20SFL%20framework%2C%20to%20expedite%20SFL%20under%0Aresource-constrained%20edge%20computing%20systems.%20Specifically%2C%20AdaptSFL%20adaptively%0Acontrols%20client-side%20MA%20and%20MS%20to%20balance%20communication-computing%20latency%20and%0Atraining%20convergence.%20Extensive%20simulations%20across%20various%20datasets%20validate%0Athat%20our%20proposed%20AdaptSFL%20framework%20takes%20considerably%20less%20time%20to%20achieve%20a%0Atarget%20accuracy%20than%20benchmarks%2C%20demonstrating%20the%20effectiveness%20of%20the%0Aproposed%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13101v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptSFL%253A%2520Adaptive%2520Split%2520Federated%2520Learning%2520in%2520Resource-constrained%2520Edge%250A%2520%2520Networks%26entry.906535625%3DZheng%2520Lin%2520and%2520Guanqiao%2520Qu%2520and%2520Wei%2520Wei%2520and%2520Xianhao%2520Chen%2520and%2520Kin%2520K.%2520Leung%26entry.1292438233%3D%2520%2520The%2520increasing%2520complexity%2520of%2520deep%2520neural%2520networks%2520poses%2520significant%2520barriers%250Ato%2520democratizing%2520them%2520to%2520resource-limited%2520edge%2520devices.%2520To%2520address%2520this%250Achallenge%252C%2520split%2520federated%2520learning%2520%2528SFL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520solution%250Aby%2520of%2520floading%2520the%2520primary%2520training%2520workload%2520to%2520a%2520server%2520via%2520model%2520partitioning%250Awhile%2520enabling%2520parallel%2520training%2520among%2520edge%2520devices.%2520However%252C%2520although%2520system%250Aoptimization%2520substantially%2520influences%2520the%2520performance%2520of%2520SFL%2520under%250Aresource-constrained%2520systems%252C%2520the%2520problem%2520remains%2520largely%2520uncharted.%2520In%2520this%250Apaper%252C%2520we%2520provide%2520a%2520convergence%2520analysis%2520of%2520SFL%2520which%2520quantifies%2520the%2520impact%2520of%250Amodel%2520splitting%2520%2528MS%2529%2520and%2520client-side%2520model%2520aggregation%2520%2528MA%2529%2520on%2520the%2520learning%250Aperformance%252C%2520serving%2520as%2520a%2520theoretical%2520foundation.%2520Then%252C%2520we%2520propose%2520AdaptSFL%252C%2520a%250Anovel%2520resource-adaptive%2520SFL%2520framework%252C%2520to%2520expedite%2520SFL%2520under%250Aresource-constrained%2520edge%2520computing%2520systems.%2520Specifically%252C%2520AdaptSFL%2520adaptively%250Acontrols%2520client-side%2520MA%2520and%2520MS%2520to%2520balance%2520communication-computing%2520latency%2520and%250Atraining%2520convergence.%2520Extensive%2520simulations%2520across%2520various%2520datasets%2520validate%250Athat%2520our%2520proposed%2520AdaptSFL%2520framework%2520takes%2520considerably%2520less%2520time%2520to%2520achieve%2520a%250Atarget%2520accuracy%2520than%2520benchmarks%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13101v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaptSFL%3A%20Adaptive%20Split%20Federated%20Learning%20in%20Resource-constrained%20Edge%0A%20%20Networks&entry.906535625=Zheng%20Lin%20and%20Guanqiao%20Qu%20and%20Wei%20Wei%20and%20Xianhao%20Chen%20and%20Kin%20K.%20Leung&entry.1292438233=%20%20The%20increasing%20complexity%20of%20deep%20neural%20networks%20poses%20significant%20barriers%0Ato%20democratizing%20them%20to%20resource-limited%20edge%20devices.%20To%20address%20this%0Achallenge%2C%20split%20federated%20learning%20%28SFL%29%20has%20emerged%20as%20a%20promising%20solution%0Aby%20of%20floading%20the%20primary%20training%20workload%20to%20a%20server%20via%20model%20partitioning%0Awhile%20enabling%20parallel%20training%20among%20edge%20devices.%20However%2C%20although%20system%0Aoptimization%20substantially%20influences%20the%20performance%20of%20SFL%20under%0Aresource-constrained%20systems%2C%20the%20problem%20remains%20largely%20uncharted.%20In%20this%0Apaper%2C%20we%20provide%20a%20convergence%20analysis%20of%20SFL%20which%20quantifies%20the%20impact%20of%0Amodel%20splitting%20%28MS%29%20and%20client-side%20model%20aggregation%20%28MA%29%20on%20the%20learning%0Aperformance%2C%20serving%20as%20a%20theoretical%20foundation.%20Then%2C%20we%20propose%20AdaptSFL%2C%20a%0Anovel%20resource-adaptive%20SFL%20framework%2C%20to%20expedite%20SFL%20under%0Aresource-constrained%20edge%20computing%20systems.%20Specifically%2C%20AdaptSFL%20adaptively%0Acontrols%20client-side%20MA%20and%20MS%20to%20balance%20communication-computing%20latency%20and%0Atraining%20convergence.%20Extensive%20simulations%20across%20various%20datasets%20validate%0Athat%20our%20proposed%20AdaptSFL%20framework%20takes%20considerably%20less%20time%20to%20achieve%20a%0Atarget%20accuracy%20than%20benchmarks%2C%20demonstrating%20the%20effectiveness%20of%20the%0Aproposed%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13101v2&entry.124074799=Read"},
{"title": "Pytorch-Wildlife: A Collaborative Deep Learning Framework for\n  Conservation", "author": "Andres Hernandez and Zhongqi Miao and Luisa Vargas and Rahul Dodhia and Juan Lavista", "abstract": "  The alarming decline in global biodiversity, driven by various factors,\nunderscores the urgent need for large-scale wildlife monitoring. In response,\nscientists have turned to automated deep learning methods for data processing\nin wildlife monitoring. However, applying these advanced methods in real-world\nscenarios is challenging due to their complexity and the need for specialized\nknowledge, primarily because of technical challenges and interdisciplinary\nbarriers.\n  To address these challenges, we introduce Pytorch-Wildlife, an open-source\ndeep learning platform built on PyTorch. It is designed for creating,\nmodifying, and sharing powerful AI models. This platform emphasizes usability\nand accessibility, making it accessible to individuals with limited or no\ntechnical background. It also offers a modular codebase to simplify feature\nexpansion and further development. Pytorch-Wildlife offers an intuitive,\nuser-friendly interface, accessible through local installation or Hugging Face,\nfor animal detection and classification in images and videos. As two real-world\napplications, Pytorch-Wildlife has been utilized to train animal classification\nmodels for species recognition in the Amazon Rainforest and for invasive\nopossum recognition in the Galapagos Islands. The Opossum model achieves 98%\naccuracy, and the Amazon model has 92% recognition accuracy for 36 animals in\n90% of the data. As Pytorch-Wildlife evolves, we aim to integrate more\nconservation tasks, addressing various environmental challenges.\nPytorch-Wildlife is available at https://github.com/microsoft/CameraTraps.\n", "link": "http://arxiv.org/abs/2405.12930v1", "date": "2024-05-21", "relevancy": 1.9243, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.487}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4807}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pytorch-Wildlife%3A%20A%20Collaborative%20Deep%20Learning%20Framework%20for%0A%20%20Conservation&body=Title%3A%20Pytorch-Wildlife%3A%20A%20Collaborative%20Deep%20Learning%20Framework%20for%0A%20%20Conservation%0AAuthor%3A%20Andres%20Hernandez%20and%20Zhongqi%20Miao%20and%20Luisa%20Vargas%20and%20Rahul%20Dodhia%20and%20Juan%20Lavista%0AAbstract%3A%20%20%20The%20alarming%20decline%20in%20global%20biodiversity%2C%20driven%20by%20various%20factors%2C%0Aunderscores%20the%20urgent%20need%20for%20large-scale%20wildlife%20monitoring.%20In%20response%2C%0Ascientists%20have%20turned%20to%20automated%20deep%20learning%20methods%20for%20data%20processing%0Ain%20wildlife%20monitoring.%20However%2C%20applying%20these%20advanced%20methods%20in%20real-world%0Ascenarios%20is%20challenging%20due%20to%20their%20complexity%20and%20the%20need%20for%20specialized%0Aknowledge%2C%20primarily%20because%20of%20technical%20challenges%20and%20interdisciplinary%0Abarriers.%0A%20%20To%20address%20these%20challenges%2C%20we%20introduce%20Pytorch-Wildlife%2C%20an%20open-source%0Adeep%20learning%20platform%20built%20on%20PyTorch.%20It%20is%20designed%20for%20creating%2C%0Amodifying%2C%20and%20sharing%20powerful%20AI%20models.%20This%20platform%20emphasizes%20usability%0Aand%20accessibility%2C%20making%20it%20accessible%20to%20individuals%20with%20limited%20or%20no%0Atechnical%20background.%20It%20also%20offers%20a%20modular%20codebase%20to%20simplify%20feature%0Aexpansion%20and%20further%20development.%20Pytorch-Wildlife%20offers%20an%20intuitive%2C%0Auser-friendly%20interface%2C%20accessible%20through%20local%20installation%20or%20Hugging%20Face%2C%0Afor%20animal%20detection%20and%20classification%20in%20images%20and%20videos.%20As%20two%20real-world%0Aapplications%2C%20Pytorch-Wildlife%20has%20been%20utilized%20to%20train%20animal%20classification%0Amodels%20for%20species%20recognition%20in%20the%20Amazon%20Rainforest%20and%20for%20invasive%0Aopossum%20recognition%20in%20the%20Galapagos%20Islands.%20The%20Opossum%20model%20achieves%2098%25%0Aaccuracy%2C%20and%20the%20Amazon%20model%20has%2092%25%20recognition%20accuracy%20for%2036%20animals%20in%0A90%25%20of%20the%20data.%20As%20Pytorch-Wildlife%20evolves%2C%20we%20aim%20to%20integrate%20more%0Aconservation%20tasks%2C%20addressing%20various%20environmental%20challenges.%0APytorch-Wildlife%20is%20available%20at%20https%3A//github.com/microsoft/CameraTraps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPytorch-Wildlife%253A%2520A%2520Collaborative%2520Deep%2520Learning%2520Framework%2520for%250A%2520%2520Conservation%26entry.906535625%3DAndres%2520Hernandez%2520and%2520Zhongqi%2520Miao%2520and%2520Luisa%2520Vargas%2520and%2520Rahul%2520Dodhia%2520and%2520Juan%2520Lavista%26entry.1292438233%3D%2520%2520The%2520alarming%2520decline%2520in%2520global%2520biodiversity%252C%2520driven%2520by%2520various%2520factors%252C%250Aunderscores%2520the%2520urgent%2520need%2520for%2520large-scale%2520wildlife%2520monitoring.%2520In%2520response%252C%250Ascientists%2520have%2520turned%2520to%2520automated%2520deep%2520learning%2520methods%2520for%2520data%2520processing%250Ain%2520wildlife%2520monitoring.%2520However%252C%2520applying%2520these%2520advanced%2520methods%2520in%2520real-world%250Ascenarios%2520is%2520challenging%2520due%2520to%2520their%2520complexity%2520and%2520the%2520need%2520for%2520specialized%250Aknowledge%252C%2520primarily%2520because%2520of%2520technical%2520challenges%2520and%2520interdisciplinary%250Abarriers.%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Pytorch-Wildlife%252C%2520an%2520open-source%250Adeep%2520learning%2520platform%2520built%2520on%2520PyTorch.%2520It%2520is%2520designed%2520for%2520creating%252C%250Amodifying%252C%2520and%2520sharing%2520powerful%2520AI%2520models.%2520This%2520platform%2520emphasizes%2520usability%250Aand%2520accessibility%252C%2520making%2520it%2520accessible%2520to%2520individuals%2520with%2520limited%2520or%2520no%250Atechnical%2520background.%2520It%2520also%2520offers%2520a%2520modular%2520codebase%2520to%2520simplify%2520feature%250Aexpansion%2520and%2520further%2520development.%2520Pytorch-Wildlife%2520offers%2520an%2520intuitive%252C%250Auser-friendly%2520interface%252C%2520accessible%2520through%2520local%2520installation%2520or%2520Hugging%2520Face%252C%250Afor%2520animal%2520detection%2520and%2520classification%2520in%2520images%2520and%2520videos.%2520As%2520two%2520real-world%250Aapplications%252C%2520Pytorch-Wildlife%2520has%2520been%2520utilized%2520to%2520train%2520animal%2520classification%250Amodels%2520for%2520species%2520recognition%2520in%2520the%2520Amazon%2520Rainforest%2520and%2520for%2520invasive%250Aopossum%2520recognition%2520in%2520the%2520Galapagos%2520Islands.%2520The%2520Opossum%2520model%2520achieves%252098%2525%250Aaccuracy%252C%2520and%2520the%2520Amazon%2520model%2520has%252092%2525%2520recognition%2520accuracy%2520for%252036%2520animals%2520in%250A90%2525%2520of%2520the%2520data.%2520As%2520Pytorch-Wildlife%2520evolves%252C%2520we%2520aim%2520to%2520integrate%2520more%250Aconservation%2520tasks%252C%2520addressing%2520various%2520environmental%2520challenges.%250APytorch-Wildlife%2520is%2520available%2520at%2520https%253A//github.com/microsoft/CameraTraps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pytorch-Wildlife%3A%20A%20Collaborative%20Deep%20Learning%20Framework%20for%0A%20%20Conservation&entry.906535625=Andres%20Hernandez%20and%20Zhongqi%20Miao%20and%20Luisa%20Vargas%20and%20Rahul%20Dodhia%20and%20Juan%20Lavista&entry.1292438233=%20%20The%20alarming%20decline%20in%20global%20biodiversity%2C%20driven%20by%20various%20factors%2C%0Aunderscores%20the%20urgent%20need%20for%20large-scale%20wildlife%20monitoring.%20In%20response%2C%0Ascientists%20have%20turned%20to%20automated%20deep%20learning%20methods%20for%20data%20processing%0Ain%20wildlife%20monitoring.%20However%2C%20applying%20these%20advanced%20methods%20in%20real-world%0Ascenarios%20is%20challenging%20due%20to%20their%20complexity%20and%20the%20need%20for%20specialized%0Aknowledge%2C%20primarily%20because%20of%20technical%20challenges%20and%20interdisciplinary%0Abarriers.%0A%20%20To%20address%20these%20challenges%2C%20we%20introduce%20Pytorch-Wildlife%2C%20an%20open-source%0Adeep%20learning%20platform%20built%20on%20PyTorch.%20It%20is%20designed%20for%20creating%2C%0Amodifying%2C%20and%20sharing%20powerful%20AI%20models.%20This%20platform%20emphasizes%20usability%0Aand%20accessibility%2C%20making%20it%20accessible%20to%20individuals%20with%20limited%20or%20no%0Atechnical%20background.%20It%20also%20offers%20a%20modular%20codebase%20to%20simplify%20feature%0Aexpansion%20and%20further%20development.%20Pytorch-Wildlife%20offers%20an%20intuitive%2C%0Auser-friendly%20interface%2C%20accessible%20through%20local%20installation%20or%20Hugging%20Face%2C%0Afor%20animal%20detection%20and%20classification%20in%20images%20and%20videos.%20As%20two%20real-world%0Aapplications%2C%20Pytorch-Wildlife%20has%20been%20utilized%20to%20train%20animal%20classification%0Amodels%20for%20species%20recognition%20in%20the%20Amazon%20Rainforest%20and%20for%20invasive%0Aopossum%20recognition%20in%20the%20Galapagos%20Islands.%20The%20Opossum%20model%20achieves%2098%25%0Aaccuracy%2C%20and%20the%20Amazon%20model%20has%2092%25%20recognition%20accuracy%20for%2036%20animals%20in%0A90%25%20of%20the%20data.%20As%20Pytorch-Wildlife%20evolves%2C%20we%20aim%20to%20integrate%20more%0Aconservation%20tasks%2C%20addressing%20various%20environmental%20challenges.%0APytorch-Wildlife%20is%20available%20at%20https%3A//github.com/microsoft/CameraTraps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12930v1&entry.124074799=Read"},
{"title": "Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with\n  Minimal Impact on Coherence and Evasiveness in Dialogue Agents", "author": "San Kim and Gary Geunbae Lee", "abstract": "  Recent advancements in open-domain dialogue systems have been propelled by\nthe emergence of high-quality large language models (LLMs) and various\neffective training methodologies. Nevertheless, the presence of toxicity within\nthese models presents a significant challenge that can potentially diminish the\nuser experience. In this study, we introduce an innovative training algorithm,\nan improvement upon direct preference optimization (DPO), called adversarial\nDPO (ADPO). The ADPO algorithm is designed to train models to assign higher\nprobability distributions to preferred responses and lower distributions to\nunsafe responses, which are self-generated using the toxic control token. We\ndemonstrate that ADPO enhances the model's resilience against harmful\nconversations while minimizing performance degradation. Furthermore, we\nillustrate that ADPO offers a more stable training procedure compared to the\ntraditional DPO. To the best of our knowledge, this is the first adaptation of\nthe DPO algorithm that directly incorporates harmful data into the generative\nmodel, thereby reducing the need to artificially create safe dialogue data.\n", "link": "http://arxiv.org/abs/2405.12900v1", "date": "2024-05-21", "relevancy": 1.9204, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4868}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4761}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20DPO%3A%20Harnessing%20Harmful%20Data%20for%20Reducing%20Toxicity%20with%0A%20%20Minimal%20Impact%20on%20Coherence%20and%20Evasiveness%20in%20Dialogue%20Agents&body=Title%3A%20Adversarial%20DPO%3A%20Harnessing%20Harmful%20Data%20for%20Reducing%20Toxicity%20with%0A%20%20Minimal%20Impact%20on%20Coherence%20and%20Evasiveness%20in%20Dialogue%20Agents%0AAuthor%3A%20San%20Kim%20and%20Gary%20Geunbae%20Lee%0AAbstract%3A%20%20%20Recent%20advancements%20in%20open-domain%20dialogue%20systems%20have%20been%20propelled%20by%0Athe%20emergence%20of%20high-quality%20large%20language%20models%20%28LLMs%29%20and%20various%0Aeffective%20training%20methodologies.%20Nevertheless%2C%20the%20presence%20of%20toxicity%20within%0Athese%20models%20presents%20a%20significant%20challenge%20that%20can%20potentially%20diminish%20the%0Auser%20experience.%20In%20this%20study%2C%20we%20introduce%20an%20innovative%20training%20algorithm%2C%0Aan%20improvement%20upon%20direct%20preference%20optimization%20%28DPO%29%2C%20called%20adversarial%0ADPO%20%28ADPO%29.%20The%20ADPO%20algorithm%20is%20designed%20to%20train%20models%20to%20assign%20higher%0Aprobability%20distributions%20to%20preferred%20responses%20and%20lower%20distributions%20to%0Aunsafe%20responses%2C%20which%20are%20self-generated%20using%20the%20toxic%20control%20token.%20We%0Ademonstrate%20that%20ADPO%20enhances%20the%20model%27s%20resilience%20against%20harmful%0Aconversations%20while%20minimizing%20performance%20degradation.%20Furthermore%2C%20we%0Aillustrate%20that%20ADPO%20offers%20a%20more%20stable%20training%20procedure%20compared%20to%20the%0Atraditional%20DPO.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20adaptation%20of%0Athe%20DPO%20algorithm%20that%20directly%20incorporates%20harmful%20data%20into%20the%20generative%0Amodel%2C%20thereby%20reducing%20the%20need%20to%20artificially%20create%20safe%20dialogue%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12900v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520DPO%253A%2520Harnessing%2520Harmful%2520Data%2520for%2520Reducing%2520Toxicity%2520with%250A%2520%2520Minimal%2520Impact%2520on%2520Coherence%2520and%2520Evasiveness%2520in%2520Dialogue%2520Agents%26entry.906535625%3DSan%2520Kim%2520and%2520Gary%2520Geunbae%2520Lee%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520open-domain%2520dialogue%2520systems%2520have%2520been%2520propelled%2520by%250Athe%2520emergence%2520of%2520high-quality%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520various%250Aeffective%2520training%2520methodologies.%2520Nevertheless%252C%2520the%2520presence%2520of%2520toxicity%2520within%250Athese%2520models%2520presents%2520a%2520significant%2520challenge%2520that%2520can%2520potentially%2520diminish%2520the%250Auser%2520experience.%2520In%2520this%2520study%252C%2520we%2520introduce%2520an%2520innovative%2520training%2520algorithm%252C%250Aan%2520improvement%2520upon%2520direct%2520preference%2520optimization%2520%2528DPO%2529%252C%2520called%2520adversarial%250ADPO%2520%2528ADPO%2529.%2520The%2520ADPO%2520algorithm%2520is%2520designed%2520to%2520train%2520models%2520to%2520assign%2520higher%250Aprobability%2520distributions%2520to%2520preferred%2520responses%2520and%2520lower%2520distributions%2520to%250Aunsafe%2520responses%252C%2520which%2520are%2520self-generated%2520using%2520the%2520toxic%2520control%2520token.%2520We%250Ademonstrate%2520that%2520ADPO%2520enhances%2520the%2520model%2527s%2520resilience%2520against%2520harmful%250Aconversations%2520while%2520minimizing%2520performance%2520degradation.%2520Furthermore%252C%2520we%250Aillustrate%2520that%2520ADPO%2520offers%2520a%2520more%2520stable%2520training%2520procedure%2520compared%2520to%2520the%250Atraditional%2520DPO.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520adaptation%2520of%250Athe%2520DPO%2520algorithm%2520that%2520directly%2520incorporates%2520harmful%2520data%2520into%2520the%2520generative%250Amodel%252C%2520thereby%2520reducing%2520the%2520need%2520to%2520artificially%2520create%2520safe%2520dialogue%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12900v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20DPO%3A%20Harnessing%20Harmful%20Data%20for%20Reducing%20Toxicity%20with%0A%20%20Minimal%20Impact%20on%20Coherence%20and%20Evasiveness%20in%20Dialogue%20Agents&entry.906535625=San%20Kim%20and%20Gary%20Geunbae%20Lee&entry.1292438233=%20%20Recent%20advancements%20in%20open-domain%20dialogue%20systems%20have%20been%20propelled%20by%0Athe%20emergence%20of%20high-quality%20large%20language%20models%20%28LLMs%29%20and%20various%0Aeffective%20training%20methodologies.%20Nevertheless%2C%20the%20presence%20of%20toxicity%20within%0Athese%20models%20presents%20a%20significant%20challenge%20that%20can%20potentially%20diminish%20the%0Auser%20experience.%20In%20this%20study%2C%20we%20introduce%20an%20innovative%20training%20algorithm%2C%0Aan%20improvement%20upon%20direct%20preference%20optimization%20%28DPO%29%2C%20called%20adversarial%0ADPO%20%28ADPO%29.%20The%20ADPO%20algorithm%20is%20designed%20to%20train%20models%20to%20assign%20higher%0Aprobability%20distributions%20to%20preferred%20responses%20and%20lower%20distributions%20to%0Aunsafe%20responses%2C%20which%20are%20self-generated%20using%20the%20toxic%20control%20token.%20We%0Ademonstrate%20that%20ADPO%20enhances%20the%20model%27s%20resilience%20against%20harmful%0Aconversations%20while%20minimizing%20performance%20degradation.%20Furthermore%2C%20we%0Aillustrate%20that%20ADPO%20offers%20a%20more%20stable%20training%20procedure%20compared%20to%20the%0Atraditional%20DPO.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20adaptation%20of%0Athe%20DPO%20algorithm%20that%20directly%20incorporates%20harmful%20data%20into%20the%20generative%0Amodel%2C%20thereby%20reducing%20the%20need%20to%20artificially%20create%20safe%20dialogue%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12900v1&entry.124074799=Read"},
{"title": "Blind Separation of Vibration Sources using Deep Learning and\n  Deconvolution", "author": "Igor Makienko and Michael Grebshtein and Eli Gildish", "abstract": "  Vibrations of rotating machinery primarily originate from two sources, both\nof which are distorted by the machine's transfer function on their way to the\nsensor: the dominant gear-related vibrations and a low-energy signal linked to\nbearing faults. The proposed method facilitates the blind separation of\nvibration sources, eliminating the need for any information about the monitored\nequipment or external measurements. This method estimates both sources in two\nstages: initially, the gear signal is isolated using a dilated CNN, followed by\nthe estimation of the bearing fault signal using the squared log envelope of\nthe residual. The effect of the transfer function is removed from both sources\nusing a novel whitening-based deconvolution method (WBD). Both simulation and\nexperimental results demonstrate the method's ability to detect bearing\nfailures early when no additional information is available. This study\nconsiders both local and distributed bearing faults, assuming that the\nvibrations are recorded under stable operating conditions.\n", "link": "http://arxiv.org/abs/2405.12774v1", "date": "2024-05-21", "relevancy": 1.9194, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.497}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4736}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Blind%20Separation%20of%20Vibration%20Sources%20using%20Deep%20Learning%20and%0A%20%20Deconvolution&body=Title%3A%20Blind%20Separation%20of%20Vibration%20Sources%20using%20Deep%20Learning%20and%0A%20%20Deconvolution%0AAuthor%3A%20Igor%20Makienko%20and%20Michael%20Grebshtein%20and%20Eli%20Gildish%0AAbstract%3A%20%20%20Vibrations%20of%20rotating%20machinery%20primarily%20originate%20from%20two%20sources%2C%20both%0Aof%20which%20are%20distorted%20by%20the%20machine%27s%20transfer%20function%20on%20their%20way%20to%20the%0Asensor%3A%20the%20dominant%20gear-related%20vibrations%20and%20a%20low-energy%20signal%20linked%20to%0Abearing%20faults.%20The%20proposed%20method%20facilitates%20the%20blind%20separation%20of%0Avibration%20sources%2C%20eliminating%20the%20need%20for%20any%20information%20about%20the%20monitored%0Aequipment%20or%20external%20measurements.%20This%20method%20estimates%20both%20sources%20in%20two%0Astages%3A%20initially%2C%20the%20gear%20signal%20is%20isolated%20using%20a%20dilated%20CNN%2C%20followed%20by%0Athe%20estimation%20of%20the%20bearing%20fault%20signal%20using%20the%20squared%20log%20envelope%20of%0Athe%20residual.%20The%20effect%20of%20the%20transfer%20function%20is%20removed%20from%20both%20sources%0Ausing%20a%20novel%20whitening-based%20deconvolution%20method%20%28WBD%29.%20Both%20simulation%20and%0Aexperimental%20results%20demonstrate%20the%20method%27s%20ability%20to%20detect%20bearing%0Afailures%20early%20when%20no%20additional%20information%20is%20available.%20This%20study%0Aconsiders%20both%20local%20and%20distributed%20bearing%20faults%2C%20assuming%20that%20the%0Avibrations%20are%20recorded%20under%20stable%20operating%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlind%2520Separation%2520of%2520Vibration%2520Sources%2520using%2520Deep%2520Learning%2520and%250A%2520%2520Deconvolution%26entry.906535625%3DIgor%2520Makienko%2520and%2520Michael%2520Grebshtein%2520and%2520Eli%2520Gildish%26entry.1292438233%3D%2520%2520Vibrations%2520of%2520rotating%2520machinery%2520primarily%2520originate%2520from%2520two%2520sources%252C%2520both%250Aof%2520which%2520are%2520distorted%2520by%2520the%2520machine%2527s%2520transfer%2520function%2520on%2520their%2520way%2520to%2520the%250Asensor%253A%2520the%2520dominant%2520gear-related%2520vibrations%2520and%2520a%2520low-energy%2520signal%2520linked%2520to%250Abearing%2520faults.%2520The%2520proposed%2520method%2520facilitates%2520the%2520blind%2520separation%2520of%250Avibration%2520sources%252C%2520eliminating%2520the%2520need%2520for%2520any%2520information%2520about%2520the%2520monitored%250Aequipment%2520or%2520external%2520measurements.%2520This%2520method%2520estimates%2520both%2520sources%2520in%2520two%250Astages%253A%2520initially%252C%2520the%2520gear%2520signal%2520is%2520isolated%2520using%2520a%2520dilated%2520CNN%252C%2520followed%2520by%250Athe%2520estimation%2520of%2520the%2520bearing%2520fault%2520signal%2520using%2520the%2520squared%2520log%2520envelope%2520of%250Athe%2520residual.%2520The%2520effect%2520of%2520the%2520transfer%2520function%2520is%2520removed%2520from%2520both%2520sources%250Ausing%2520a%2520novel%2520whitening-based%2520deconvolution%2520method%2520%2528WBD%2529.%2520Both%2520simulation%2520and%250Aexperimental%2520results%2520demonstrate%2520the%2520method%2527s%2520ability%2520to%2520detect%2520bearing%250Afailures%2520early%2520when%2520no%2520additional%2520information%2520is%2520available.%2520This%2520study%250Aconsiders%2520both%2520local%2520and%2520distributed%2520bearing%2520faults%252C%2520assuming%2520that%2520the%250Avibrations%2520are%2520recorded%2520under%2520stable%2520operating%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blind%20Separation%20of%20Vibration%20Sources%20using%20Deep%20Learning%20and%0A%20%20Deconvolution&entry.906535625=Igor%20Makienko%20and%20Michael%20Grebshtein%20and%20Eli%20Gildish&entry.1292438233=%20%20Vibrations%20of%20rotating%20machinery%20primarily%20originate%20from%20two%20sources%2C%20both%0Aof%20which%20are%20distorted%20by%20the%20machine%27s%20transfer%20function%20on%20their%20way%20to%20the%0Asensor%3A%20the%20dominant%20gear-related%20vibrations%20and%20a%20low-energy%20signal%20linked%20to%0Abearing%20faults.%20The%20proposed%20method%20facilitates%20the%20blind%20separation%20of%0Avibration%20sources%2C%20eliminating%20the%20need%20for%20any%20information%20about%20the%20monitored%0Aequipment%20or%20external%20measurements.%20This%20method%20estimates%20both%20sources%20in%20two%0Astages%3A%20initially%2C%20the%20gear%20signal%20is%20isolated%20using%20a%20dilated%20CNN%2C%20followed%20by%0Athe%20estimation%20of%20the%20bearing%20fault%20signal%20using%20the%20squared%20log%20envelope%20of%0Athe%20residual.%20The%20effect%20of%20the%20transfer%20function%20is%20removed%20from%20both%20sources%0Ausing%20a%20novel%20whitening-based%20deconvolution%20method%20%28WBD%29.%20Both%20simulation%20and%0Aexperimental%20results%20demonstrate%20the%20method%27s%20ability%20to%20detect%20bearing%0Afailures%20early%20when%20no%20additional%20information%20is%20available.%20This%20study%0Aconsiders%20both%20local%20and%20distributed%20bearing%20faults%2C%20assuming%20that%20the%0Avibrations%20are%20recorded%20under%20stable%20operating%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12774v1&entry.124074799=Read"},
{"title": "Online Learning of Halfspaces with Massart Noise", "author": "Ilias Diakonikolas and Vasilis Kontonis and Christos Tzamos and Nikos Zarifis", "abstract": "  We study the task of online learning in the presence of Massart noise.\nInstead of assuming that the online adversary chooses an arbitrary sequence of\nlabels, we assume that the context $\\mathbf{x}$ is selected adversarially but\nthe label $y$ presented to the learner disagrees with the ground-truth label of\n$\\mathbf{x}$ with unknown probability at most $\\eta$. We study the fundamental\nclass of $\\gamma$-margin linear classifiers and present a computationally\nefficient algorithm that achieves mistake bound $\\eta T + o(T)$. Our mistake\nbound is qualitatively tight for efficient algorithms: it is known that even in\nthe offline setting achieving classification error better than $\\eta$ requires\nsuper-polynomial time in the SQ model.\n  We extend our online learning model to a $k$-arm contextual bandit setting\nwhere the rewards -- instead of satisfying commonly used realizability\nassumptions -- are consistent (in expectation) with some linear ranking\nfunction with weight vector $\\mathbf{w}^\\ast$. Given a list of contexts\n$\\mathbf{x}_1,\\ldots \\mathbf{x}_k$, if $\\mathbf{w}^*\\cdot \\mathbf{x}_i >\n\\mathbf{w}^* \\cdot \\mathbf{x}_j$, the expected reward of action $i$ must be\nlarger than that of $j$ by at least $\\Delta$. We use our Massart online learner\nto design an efficient bandit algorithm that obtains expected reward at least\n$(1-1/k)~ \\Delta T - o(T)$ bigger than choosing a random action at every round.\n", "link": "http://arxiv.org/abs/2405.12958v1", "date": "2024-05-21", "relevancy": 1.9166, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5089}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4766}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Learning%20of%20Halfspaces%20with%20Massart%20Noise&body=Title%3A%20Online%20Learning%20of%20Halfspaces%20with%20Massart%20Noise%0AAuthor%3A%20Ilias%20Diakonikolas%20and%20Vasilis%20Kontonis%20and%20Christos%20Tzamos%20and%20Nikos%20Zarifis%0AAbstract%3A%20%20%20We%20study%20the%20task%20of%20online%20learning%20in%20the%20presence%20of%20Massart%20noise.%0AInstead%20of%20assuming%20that%20the%20online%20adversary%20chooses%20an%20arbitrary%20sequence%20of%0Alabels%2C%20we%20assume%20that%20the%20context%20%24%5Cmathbf%7Bx%7D%24%20is%20selected%20adversarially%20but%0Athe%20label%20%24y%24%20presented%20to%20the%20learner%20disagrees%20with%20the%20ground-truth%20label%20of%0A%24%5Cmathbf%7Bx%7D%24%20with%20unknown%20probability%20at%20most%20%24%5Ceta%24.%20We%20study%20the%20fundamental%0Aclass%20of%20%24%5Cgamma%24-margin%20linear%20classifiers%20and%20present%20a%20computationally%0Aefficient%20algorithm%20that%20achieves%20mistake%20bound%20%24%5Ceta%20T%20%2B%20o%28T%29%24.%20Our%20mistake%0Abound%20is%20qualitatively%20tight%20for%20efficient%20algorithms%3A%20it%20is%20known%20that%20even%20in%0Athe%20offline%20setting%20achieving%20classification%20error%20better%20than%20%24%5Ceta%24%20requires%0Asuper-polynomial%20time%20in%20the%20SQ%20model.%0A%20%20We%20extend%20our%20online%20learning%20model%20to%20a%20%24k%24-arm%20contextual%20bandit%20setting%0Awhere%20the%20rewards%20--%20instead%20of%20satisfying%20commonly%20used%20realizability%0Aassumptions%20--%20are%20consistent%20%28in%20expectation%29%20with%20some%20linear%20ranking%0Afunction%20with%20weight%20vector%20%24%5Cmathbf%7Bw%7D%5E%5Cast%24.%20Given%20a%20list%20of%20contexts%0A%24%5Cmathbf%7Bx%7D_1%2C%5Cldots%20%5Cmathbf%7Bx%7D_k%24%2C%20if%20%24%5Cmathbf%7Bw%7D%5E%2A%5Ccdot%20%5Cmathbf%7Bx%7D_i%20%3E%0A%5Cmathbf%7Bw%7D%5E%2A%20%5Ccdot%20%5Cmathbf%7Bx%7D_j%24%2C%20the%20expected%20reward%20of%20action%20%24i%24%20must%20be%0Alarger%20than%20that%20of%20%24j%24%20by%20at%20least%20%24%5CDelta%24.%20We%20use%20our%20Massart%20online%20learner%0Ato%20design%20an%20efficient%20bandit%20algorithm%20that%20obtains%20expected%20reward%20at%20least%0A%24%281-1/k%29~%20%5CDelta%20T%20-%20o%28T%29%24%20bigger%20than%20choosing%20a%20random%20action%20at%20every%20round.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Learning%2520of%2520Halfspaces%2520with%2520Massart%2520Noise%26entry.906535625%3DIlias%2520Diakonikolas%2520and%2520Vasilis%2520Kontonis%2520and%2520Christos%2520Tzamos%2520and%2520Nikos%2520Zarifis%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520task%2520of%2520online%2520learning%2520in%2520the%2520presence%2520of%2520Massart%2520noise.%250AInstead%2520of%2520assuming%2520that%2520the%2520online%2520adversary%2520chooses%2520an%2520arbitrary%2520sequence%2520of%250Alabels%252C%2520we%2520assume%2520that%2520the%2520context%2520%2524%255Cmathbf%257Bx%257D%2524%2520is%2520selected%2520adversarially%2520but%250Athe%2520label%2520%2524y%2524%2520presented%2520to%2520the%2520learner%2520disagrees%2520with%2520the%2520ground-truth%2520label%2520of%250A%2524%255Cmathbf%257Bx%257D%2524%2520with%2520unknown%2520probability%2520at%2520most%2520%2524%255Ceta%2524.%2520We%2520study%2520the%2520fundamental%250Aclass%2520of%2520%2524%255Cgamma%2524-margin%2520linear%2520classifiers%2520and%2520present%2520a%2520computationally%250Aefficient%2520algorithm%2520that%2520achieves%2520mistake%2520bound%2520%2524%255Ceta%2520T%2520%252B%2520o%2528T%2529%2524.%2520Our%2520mistake%250Abound%2520is%2520qualitatively%2520tight%2520for%2520efficient%2520algorithms%253A%2520it%2520is%2520known%2520that%2520even%2520in%250Athe%2520offline%2520setting%2520achieving%2520classification%2520error%2520better%2520than%2520%2524%255Ceta%2524%2520requires%250Asuper-polynomial%2520time%2520in%2520the%2520SQ%2520model.%250A%2520%2520We%2520extend%2520our%2520online%2520learning%2520model%2520to%2520a%2520%2524k%2524-arm%2520contextual%2520bandit%2520setting%250Awhere%2520the%2520rewards%2520--%2520instead%2520of%2520satisfying%2520commonly%2520used%2520realizability%250Aassumptions%2520--%2520are%2520consistent%2520%2528in%2520expectation%2529%2520with%2520some%2520linear%2520ranking%250Afunction%2520with%2520weight%2520vector%2520%2524%255Cmathbf%257Bw%257D%255E%255Cast%2524.%2520Given%2520a%2520list%2520of%2520contexts%250A%2524%255Cmathbf%257Bx%257D_1%252C%255Cldots%2520%255Cmathbf%257Bx%257D_k%2524%252C%2520if%2520%2524%255Cmathbf%257Bw%257D%255E%252A%255Ccdot%2520%255Cmathbf%257Bx%257D_i%2520%253E%250A%255Cmathbf%257Bw%257D%255E%252A%2520%255Ccdot%2520%255Cmathbf%257Bx%257D_j%2524%252C%2520the%2520expected%2520reward%2520of%2520action%2520%2524i%2524%2520must%2520be%250Alarger%2520than%2520that%2520of%2520%2524j%2524%2520by%2520at%2520least%2520%2524%255CDelta%2524.%2520We%2520use%2520our%2520Massart%2520online%2520learner%250Ato%2520design%2520an%2520efficient%2520bandit%2520algorithm%2520that%2520obtains%2520expected%2520reward%2520at%2520least%250A%2524%25281-1/k%2529~%2520%255CDelta%2520T%2520-%2520o%2528T%2529%2524%2520bigger%2520than%2520choosing%2520a%2520random%2520action%2520at%2520every%2520round.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Learning%20of%20Halfspaces%20with%20Massart%20Noise&entry.906535625=Ilias%20Diakonikolas%20and%20Vasilis%20Kontonis%20and%20Christos%20Tzamos%20and%20Nikos%20Zarifis&entry.1292438233=%20%20We%20study%20the%20task%20of%20online%20learning%20in%20the%20presence%20of%20Massart%20noise.%0AInstead%20of%20assuming%20that%20the%20online%20adversary%20chooses%20an%20arbitrary%20sequence%20of%0Alabels%2C%20we%20assume%20that%20the%20context%20%24%5Cmathbf%7Bx%7D%24%20is%20selected%20adversarially%20but%0Athe%20label%20%24y%24%20presented%20to%20the%20learner%20disagrees%20with%20the%20ground-truth%20label%20of%0A%24%5Cmathbf%7Bx%7D%24%20with%20unknown%20probability%20at%20most%20%24%5Ceta%24.%20We%20study%20the%20fundamental%0Aclass%20of%20%24%5Cgamma%24-margin%20linear%20classifiers%20and%20present%20a%20computationally%0Aefficient%20algorithm%20that%20achieves%20mistake%20bound%20%24%5Ceta%20T%20%2B%20o%28T%29%24.%20Our%20mistake%0Abound%20is%20qualitatively%20tight%20for%20efficient%20algorithms%3A%20it%20is%20known%20that%20even%20in%0Athe%20offline%20setting%20achieving%20classification%20error%20better%20than%20%24%5Ceta%24%20requires%0Asuper-polynomial%20time%20in%20the%20SQ%20model.%0A%20%20We%20extend%20our%20online%20learning%20model%20to%20a%20%24k%24-arm%20contextual%20bandit%20setting%0Awhere%20the%20rewards%20--%20instead%20of%20satisfying%20commonly%20used%20realizability%0Aassumptions%20--%20are%20consistent%20%28in%20expectation%29%20with%20some%20linear%20ranking%0Afunction%20with%20weight%20vector%20%24%5Cmathbf%7Bw%7D%5E%5Cast%24.%20Given%20a%20list%20of%20contexts%0A%24%5Cmathbf%7Bx%7D_1%2C%5Cldots%20%5Cmathbf%7Bx%7D_k%24%2C%20if%20%24%5Cmathbf%7Bw%7D%5E%2A%5Ccdot%20%5Cmathbf%7Bx%7D_i%20%3E%0A%5Cmathbf%7Bw%7D%5E%2A%20%5Ccdot%20%5Cmathbf%7Bx%7D_j%24%2C%20the%20expected%20reward%20of%20action%20%24i%24%20must%20be%0Alarger%20than%20that%20of%20%24j%24%20by%20at%20least%20%24%5CDelta%24.%20We%20use%20our%20Massart%20online%20learner%0Ato%20design%20an%20efficient%20bandit%20algorithm%20that%20obtains%20expected%20reward%20at%20least%0A%24%281-1/k%29~%20%5CDelta%20T%20-%20o%28T%29%24%20bigger%20than%20choosing%20a%20random%20action%20at%20every%20round.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12958v1&entry.124074799=Read"},
{"title": "SPO: Multi-Dimensional Preference Sequential Alignment With Implicit\n  Reward Modeling", "author": "Xingzhou Lou and Junge Zhang and Jian Xie and Lifeng Liu and Dong Yan and Kaiqi Huang", "abstract": "  Human preference alignment is critical in building powerful and reliable\nlarge language models (LLMs). However, current methods either ignore the\nmulti-dimensionality of human preferences (e.g. helpfulness and harmlessness)\nor struggle with the complexity of managing multiple reward models. To address\nthese issues, we propose Sequential Preference Optimization (SPO), a method\nthat sequentially fine-tunes LLMs to align with multiple dimensions of human\npreferences. SPO avoids explicit reward modeling, directly optimizing the\nmodels to align with nuanced human preferences. We theoretically derive\nclosed-form optimal SPO policy and loss function. Gradient analysis is\nconducted to show how SPO manages to fine-tune the LLMs while maintaining\nalignment on previously optimized dimensions. Empirical results on LLMs of\ndifferent size and multiple evaluation datasets demonstrate that SPO\nsuccessfully aligns LLMs across multiple dimensions of human preferences and\nsignificantly outperforms the baselines.\n", "link": "http://arxiv.org/abs/2405.12739v1", "date": "2024-05-21", "relevancy": 1.9152, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5025}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4763}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPO%3A%20Multi-Dimensional%20Preference%20Sequential%20Alignment%20With%20Implicit%0A%20%20Reward%20Modeling&body=Title%3A%20SPO%3A%20Multi-Dimensional%20Preference%20Sequential%20Alignment%20With%20Implicit%0A%20%20Reward%20Modeling%0AAuthor%3A%20Xingzhou%20Lou%20and%20Junge%20Zhang%20and%20Jian%20Xie%20and%20Lifeng%20Liu%20and%20Dong%20Yan%20and%20Kaiqi%20Huang%0AAbstract%3A%20%20%20Human%20preference%20alignment%20is%20critical%20in%20building%20powerful%20and%20reliable%0Alarge%20language%20models%20%28LLMs%29.%20However%2C%20current%20methods%20either%20ignore%20the%0Amulti-dimensionality%20of%20human%20preferences%20%28e.g.%20helpfulness%20and%20harmlessness%29%0Aor%20struggle%20with%20the%20complexity%20of%20managing%20multiple%20reward%20models.%20To%20address%0Athese%20issues%2C%20we%20propose%20Sequential%20Preference%20Optimization%20%28SPO%29%2C%20a%20method%0Athat%20sequentially%20fine-tunes%20LLMs%20to%20align%20with%20multiple%20dimensions%20of%20human%0Apreferences.%20SPO%20avoids%20explicit%20reward%20modeling%2C%20directly%20optimizing%20the%0Amodels%20to%20align%20with%20nuanced%20human%20preferences.%20We%20theoretically%20derive%0Aclosed-form%20optimal%20SPO%20policy%20and%20loss%20function.%20Gradient%20analysis%20is%0Aconducted%20to%20show%20how%20SPO%20manages%20to%20fine-tune%20the%20LLMs%20while%20maintaining%0Aalignment%20on%20previously%20optimized%20dimensions.%20Empirical%20results%20on%20LLMs%20of%0Adifferent%20size%20and%20multiple%20evaluation%20datasets%20demonstrate%20that%20SPO%0Asuccessfully%20aligns%20LLMs%20across%20multiple%20dimensions%20of%20human%20preferences%20and%0Asignificantly%20outperforms%20the%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPO%253A%2520Multi-Dimensional%2520Preference%2520Sequential%2520Alignment%2520With%2520Implicit%250A%2520%2520Reward%2520Modeling%26entry.906535625%3DXingzhou%2520Lou%2520and%2520Junge%2520Zhang%2520and%2520Jian%2520Xie%2520and%2520Lifeng%2520Liu%2520and%2520Dong%2520Yan%2520and%2520Kaiqi%2520Huang%26entry.1292438233%3D%2520%2520Human%2520preference%2520alignment%2520is%2520critical%2520in%2520building%2520powerful%2520and%2520reliable%250Alarge%2520language%2520models%2520%2528LLMs%2529.%2520However%252C%2520current%2520methods%2520either%2520ignore%2520the%250Amulti-dimensionality%2520of%2520human%2520preferences%2520%2528e.g.%2520helpfulness%2520and%2520harmlessness%2529%250Aor%2520struggle%2520with%2520the%2520complexity%2520of%2520managing%2520multiple%2520reward%2520models.%2520To%2520address%250Athese%2520issues%252C%2520we%2520propose%2520Sequential%2520Preference%2520Optimization%2520%2528SPO%2529%252C%2520a%2520method%250Athat%2520sequentially%2520fine-tunes%2520LLMs%2520to%2520align%2520with%2520multiple%2520dimensions%2520of%2520human%250Apreferences.%2520SPO%2520avoids%2520explicit%2520reward%2520modeling%252C%2520directly%2520optimizing%2520the%250Amodels%2520to%2520align%2520with%2520nuanced%2520human%2520preferences.%2520We%2520theoretically%2520derive%250Aclosed-form%2520optimal%2520SPO%2520policy%2520and%2520loss%2520function.%2520Gradient%2520analysis%2520is%250Aconducted%2520to%2520show%2520how%2520SPO%2520manages%2520to%2520fine-tune%2520the%2520LLMs%2520while%2520maintaining%250Aalignment%2520on%2520previously%2520optimized%2520dimensions.%2520Empirical%2520results%2520on%2520LLMs%2520of%250Adifferent%2520size%2520and%2520multiple%2520evaluation%2520datasets%2520demonstrate%2520that%2520SPO%250Asuccessfully%2520aligns%2520LLMs%2520across%2520multiple%2520dimensions%2520of%2520human%2520preferences%2520and%250Asignificantly%2520outperforms%2520the%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPO%3A%20Multi-Dimensional%20Preference%20Sequential%20Alignment%20With%20Implicit%0A%20%20Reward%20Modeling&entry.906535625=Xingzhou%20Lou%20and%20Junge%20Zhang%20and%20Jian%20Xie%20and%20Lifeng%20Liu%20and%20Dong%20Yan%20and%20Kaiqi%20Huang&entry.1292438233=%20%20Human%20preference%20alignment%20is%20critical%20in%20building%20powerful%20and%20reliable%0Alarge%20language%20models%20%28LLMs%29.%20However%2C%20current%20methods%20either%20ignore%20the%0Amulti-dimensionality%20of%20human%20preferences%20%28e.g.%20helpfulness%20and%20harmlessness%29%0Aor%20struggle%20with%20the%20complexity%20of%20managing%20multiple%20reward%20models.%20To%20address%0Athese%20issues%2C%20we%20propose%20Sequential%20Preference%20Optimization%20%28SPO%29%2C%20a%20method%0Athat%20sequentially%20fine-tunes%20LLMs%20to%20align%20with%20multiple%20dimensions%20of%20human%0Apreferences.%20SPO%20avoids%20explicit%20reward%20modeling%2C%20directly%20optimizing%20the%0Amodels%20to%20align%20with%20nuanced%20human%20preferences.%20We%20theoretically%20derive%0Aclosed-form%20optimal%20SPO%20policy%20and%20loss%20function.%20Gradient%20analysis%20is%0Aconducted%20to%20show%20how%20SPO%20manages%20to%20fine-tune%20the%20LLMs%20while%20maintaining%0Aalignment%20on%20previously%20optimized%20dimensions.%20Empirical%20results%20on%20LLMs%20of%0Adifferent%20size%20and%20multiple%20evaluation%20datasets%20demonstrate%20that%20SPO%0Asuccessfully%20aligns%20LLMs%20across%20multiple%20dimensions%20of%20human%20preferences%20and%0Asignificantly%20outperforms%20the%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12739v1&entry.124074799=Read"},
{"title": "Graph Neural Networks over the Air for Decentralized Tasks in Wireless\n  Networks", "author": "Zhan Gao and Deniz Gunduz", "abstract": "  Graph neural networks (GNNs) model representations from networked data and\nallow for decentralized inference through localized communications. Existing\nGNN architectures often assume ideal communications and ignore potential\nchannel effects, such as fading and noise, leading to performance degradation\nin real-world implementation. Considering a GNN implemented over nodes\nconnected through wireless links, this paper conducts a stability analysis to\nstudy the impact of channel impairments on the performance of GNNs, and\nproposes graph neural networks over the air (AirGNNs), a novel GNN architecture\nthat incorporates the communication model. AirGNNs modify graph convolutional\noperations that shift graph signals over random communication graphs to take\ninto account channel fading and noise when aggregating features from neighbors,\nthus, improving architecture robustness to channel impairments during testing.\nWe develop a channel-inversion signal transmission strategy for AirGNNs when\nchannel state information (CSI) is available, and propose a stochastic gradient\ndescent based method to train AirGNNs when CSI is unknown. The convergence\nanalysis shows that the training procedure approaches a stationary solution of\nan associated stochastic optimization problem and the variance analysis\ncharacterizes the statistical behavior of the trained model. Experiments on\ndecentralized source localization and multi-robot flocking corroborate\ntheoretical findings and show superior performance of AirGNNs over wireless\ncommunication channels.\n", "link": "http://arxiv.org/abs/2302.08447v3", "date": "2024-05-21", "relevancy": 1.9144, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4885}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4756}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20over%20the%20Air%20for%20Decentralized%20Tasks%20in%20Wireless%0A%20%20Networks&body=Title%3A%20Graph%20Neural%20Networks%20over%20the%20Air%20for%20Decentralized%20Tasks%20in%20Wireless%0A%20%20Networks%0AAuthor%3A%20Zhan%20Gao%20and%20Deniz%20Gunduz%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20model%20representations%20from%20networked%20data%20and%0Aallow%20for%20decentralized%20inference%20through%20localized%20communications.%20Existing%0AGNN%20architectures%20often%20assume%20ideal%20communications%20and%20ignore%20potential%0Achannel%20effects%2C%20such%20as%20fading%20and%20noise%2C%20leading%20to%20performance%20degradation%0Ain%20real-world%20implementation.%20Considering%20a%20GNN%20implemented%20over%20nodes%0Aconnected%20through%20wireless%20links%2C%20this%20paper%20conducts%20a%20stability%20analysis%20to%0Astudy%20the%20impact%20of%20channel%20impairments%20on%20the%20performance%20of%20GNNs%2C%20and%0Aproposes%20graph%20neural%20networks%20over%20the%20air%20%28AirGNNs%29%2C%20a%20novel%20GNN%20architecture%0Athat%20incorporates%20the%20communication%20model.%20AirGNNs%20modify%20graph%20convolutional%0Aoperations%20that%20shift%20graph%20signals%20over%20random%20communication%20graphs%20to%20take%0Ainto%20account%20channel%20fading%20and%20noise%20when%20aggregating%20features%20from%20neighbors%2C%0Athus%2C%20improving%20architecture%20robustness%20to%20channel%20impairments%20during%20testing.%0AWe%20develop%20a%20channel-inversion%20signal%20transmission%20strategy%20for%20AirGNNs%20when%0Achannel%20state%20information%20%28CSI%29%20is%20available%2C%20and%20propose%20a%20stochastic%20gradient%0Adescent%20based%20method%20to%20train%20AirGNNs%20when%20CSI%20is%20unknown.%20The%20convergence%0Aanalysis%20shows%20that%20the%20training%20procedure%20approaches%20a%20stationary%20solution%20of%0Aan%20associated%20stochastic%20optimization%20problem%20and%20the%20variance%20analysis%0Acharacterizes%20the%20statistical%20behavior%20of%20the%20trained%20model.%20Experiments%20on%0Adecentralized%20source%20localization%20and%20multi-robot%20flocking%20corroborate%0Atheoretical%20findings%20and%20show%20superior%20performance%20of%20AirGNNs%20over%20wireless%0Acommunication%20channels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.08447v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Networks%2520over%2520the%2520Air%2520for%2520Decentralized%2520Tasks%2520in%2520Wireless%250A%2520%2520Networks%26entry.906535625%3DZhan%2520Gao%2520and%2520Deniz%2520Gunduz%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520model%2520representations%2520from%2520networked%2520data%2520and%250Aallow%2520for%2520decentralized%2520inference%2520through%2520localized%2520communications.%2520Existing%250AGNN%2520architectures%2520often%2520assume%2520ideal%2520communications%2520and%2520ignore%2520potential%250Achannel%2520effects%252C%2520such%2520as%2520fading%2520and%2520noise%252C%2520leading%2520to%2520performance%2520degradation%250Ain%2520real-world%2520implementation.%2520Considering%2520a%2520GNN%2520implemented%2520over%2520nodes%250Aconnected%2520through%2520wireless%2520links%252C%2520this%2520paper%2520conducts%2520a%2520stability%2520analysis%2520to%250Astudy%2520the%2520impact%2520of%2520channel%2520impairments%2520on%2520the%2520performance%2520of%2520GNNs%252C%2520and%250Aproposes%2520graph%2520neural%2520networks%2520over%2520the%2520air%2520%2528AirGNNs%2529%252C%2520a%2520novel%2520GNN%2520architecture%250Athat%2520incorporates%2520the%2520communication%2520model.%2520AirGNNs%2520modify%2520graph%2520convolutional%250Aoperations%2520that%2520shift%2520graph%2520signals%2520over%2520random%2520communication%2520graphs%2520to%2520take%250Ainto%2520account%2520channel%2520fading%2520and%2520noise%2520when%2520aggregating%2520features%2520from%2520neighbors%252C%250Athus%252C%2520improving%2520architecture%2520robustness%2520to%2520channel%2520impairments%2520during%2520testing.%250AWe%2520develop%2520a%2520channel-inversion%2520signal%2520transmission%2520strategy%2520for%2520AirGNNs%2520when%250Achannel%2520state%2520information%2520%2528CSI%2529%2520is%2520available%252C%2520and%2520propose%2520a%2520stochastic%2520gradient%250Adescent%2520based%2520method%2520to%2520train%2520AirGNNs%2520when%2520CSI%2520is%2520unknown.%2520The%2520convergence%250Aanalysis%2520shows%2520that%2520the%2520training%2520procedure%2520approaches%2520a%2520stationary%2520solution%2520of%250Aan%2520associated%2520stochastic%2520optimization%2520problem%2520and%2520the%2520variance%2520analysis%250Acharacterizes%2520the%2520statistical%2520behavior%2520of%2520the%2520trained%2520model.%2520Experiments%2520on%250Adecentralized%2520source%2520localization%2520and%2520multi-robot%2520flocking%2520corroborate%250Atheoretical%2520findings%2520and%2520show%2520superior%2520performance%2520of%2520AirGNNs%2520over%2520wireless%250Acommunication%2520channels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.08447v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20over%20the%20Air%20for%20Decentralized%20Tasks%20in%20Wireless%0A%20%20Networks&entry.906535625=Zhan%20Gao%20and%20Deniz%20Gunduz&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20model%20representations%20from%20networked%20data%20and%0Aallow%20for%20decentralized%20inference%20through%20localized%20communications.%20Existing%0AGNN%20architectures%20often%20assume%20ideal%20communications%20and%20ignore%20potential%0Achannel%20effects%2C%20such%20as%20fading%20and%20noise%2C%20leading%20to%20performance%20degradation%0Ain%20real-world%20implementation.%20Considering%20a%20GNN%20implemented%20over%20nodes%0Aconnected%20through%20wireless%20links%2C%20this%20paper%20conducts%20a%20stability%20analysis%20to%0Astudy%20the%20impact%20of%20channel%20impairments%20on%20the%20performance%20of%20GNNs%2C%20and%0Aproposes%20graph%20neural%20networks%20over%20the%20air%20%28AirGNNs%29%2C%20a%20novel%20GNN%20architecture%0Athat%20incorporates%20the%20communication%20model.%20AirGNNs%20modify%20graph%20convolutional%0Aoperations%20that%20shift%20graph%20signals%20over%20random%20communication%20graphs%20to%20take%0Ainto%20account%20channel%20fading%20and%20noise%20when%20aggregating%20features%20from%20neighbors%2C%0Athus%2C%20improving%20architecture%20robustness%20to%20channel%20impairments%20during%20testing.%0AWe%20develop%20a%20channel-inversion%20signal%20transmission%20strategy%20for%20AirGNNs%20when%0Achannel%20state%20information%20%28CSI%29%20is%20available%2C%20and%20propose%20a%20stochastic%20gradient%0Adescent%20based%20method%20to%20train%20AirGNNs%20when%20CSI%20is%20unknown.%20The%20convergence%0Aanalysis%20shows%20that%20the%20training%20procedure%20approaches%20a%20stationary%20solution%20of%0Aan%20associated%20stochastic%20optimization%20problem%20and%20the%20variance%20analysis%0Acharacterizes%20the%20statistical%20behavior%20of%20the%20trained%20model.%20Experiments%20on%0Adecentralized%20source%20localization%20and%20multi-robot%20flocking%20corroborate%0Atheoretical%20findings%20and%20show%20superior%20performance%20of%20AirGNNs%20over%20wireless%0Acommunication%20channels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.08447v3&entry.124074799=Read"},
{"title": "Algorithmic Fairness Generalization under Covariate and Dependence\n  Shifts Simultaneously", "author": "Chen Zhao and Kai Jiang and Xintao Wu and Haoliang Wang and Latifur Khan and Christan Grant and Feng Chen", "abstract": "  The endeavor to preserve the generalization of a fair and invariant\nclassifier across domains, especially in the presence of distribution shifts,\nbecomes a significant and intricate challenge in machine learning. In response\nto this challenge, numerous effective algorithms have been developed with a\nfocus on addressing the problem of fairness-aware domain generalization. These\nalgorithms are designed to navigate various types of distribution shifts, with\na particular emphasis on covariate and dependence shifts. In this context,\ncovariate shift pertains to changes in the marginal distribution of input\nfeatures, while dependence shift involves alterations in the joint distribution\nof the label variable and sensitive attributes. In this paper, we introduce a\nsimple but effective approach that aims to learn a fair and invariant\nclassifier by simultaneously addressing both covariate and dependence shifts\nacross domains. We assert the existence of an underlying transformation model\ncan transform data from one domain to another, while preserving the semantics\nrelated to non-sensitive attributes and classes. By augmenting various\nsynthetic data domains through the model, we learn a fair and invariant\nclassifier in source domains. This classifier can then be generalized to\nunknown target domains, maintaining both model prediction and fairness\nconcerns. Extensive empirical studies on four benchmark datasets demonstrate\nthat our approach surpasses state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2311.13816v2", "date": "2024-05-21", "relevancy": 1.9137, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4821}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4762}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Algorithmic%20Fairness%20Generalization%20under%20Covariate%20and%20Dependence%0A%20%20Shifts%20Simultaneously&body=Title%3A%20Algorithmic%20Fairness%20Generalization%20under%20Covariate%20and%20Dependence%0A%20%20Shifts%20Simultaneously%0AAuthor%3A%20Chen%20Zhao%20and%20Kai%20Jiang%20and%20Xintao%20Wu%20and%20Haoliang%20Wang%20and%20Latifur%20Khan%20and%20Christan%20Grant%20and%20Feng%20Chen%0AAbstract%3A%20%20%20The%20endeavor%20to%20preserve%20the%20generalization%20of%20a%20fair%20and%20invariant%0Aclassifier%20across%20domains%2C%20especially%20in%20the%20presence%20of%20distribution%20shifts%2C%0Abecomes%20a%20significant%20and%20intricate%20challenge%20in%20machine%20learning.%20In%20response%0Ato%20this%20challenge%2C%20numerous%20effective%20algorithms%20have%20been%20developed%20with%20a%0Afocus%20on%20addressing%20the%20problem%20of%20fairness-aware%20domain%20generalization.%20These%0Aalgorithms%20are%20designed%20to%20navigate%20various%20types%20of%20distribution%20shifts%2C%20with%0Aa%20particular%20emphasis%20on%20covariate%20and%20dependence%20shifts.%20In%20this%20context%2C%0Acovariate%20shift%20pertains%20to%20changes%20in%20the%20marginal%20distribution%20of%20input%0Afeatures%2C%20while%20dependence%20shift%20involves%20alterations%20in%20the%20joint%20distribution%0Aof%20the%20label%20variable%20and%20sensitive%20attributes.%20In%20this%20paper%2C%20we%20introduce%20a%0Asimple%20but%20effective%20approach%20that%20aims%20to%20learn%20a%20fair%20and%20invariant%0Aclassifier%20by%20simultaneously%20addressing%20both%20covariate%20and%20dependence%20shifts%0Aacross%20domains.%20We%20assert%20the%20existence%20of%20an%20underlying%20transformation%20model%0Acan%20transform%20data%20from%20one%20domain%20to%20another%2C%20while%20preserving%20the%20semantics%0Arelated%20to%20non-sensitive%20attributes%20and%20classes.%20By%20augmenting%20various%0Asynthetic%20data%20domains%20through%20the%20model%2C%20we%20learn%20a%20fair%20and%20invariant%0Aclassifier%20in%20source%20domains.%20This%20classifier%20can%20then%20be%20generalized%20to%0Aunknown%20target%20domains%2C%20maintaining%20both%20model%20prediction%20and%20fairness%0Aconcerns.%20Extensive%20empirical%20studies%20on%20four%20benchmark%20datasets%20demonstrate%0Athat%20our%20approach%20surpasses%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.13816v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlgorithmic%2520Fairness%2520Generalization%2520under%2520Covariate%2520and%2520Dependence%250A%2520%2520Shifts%2520Simultaneously%26entry.906535625%3DChen%2520Zhao%2520and%2520Kai%2520Jiang%2520and%2520Xintao%2520Wu%2520and%2520Haoliang%2520Wang%2520and%2520Latifur%2520Khan%2520and%2520Christan%2520Grant%2520and%2520Feng%2520Chen%26entry.1292438233%3D%2520%2520The%2520endeavor%2520to%2520preserve%2520the%2520generalization%2520of%2520a%2520fair%2520and%2520invariant%250Aclassifier%2520across%2520domains%252C%2520especially%2520in%2520the%2520presence%2520of%2520distribution%2520shifts%252C%250Abecomes%2520a%2520significant%2520and%2520intricate%2520challenge%2520in%2520machine%2520learning.%2520In%2520response%250Ato%2520this%2520challenge%252C%2520numerous%2520effective%2520algorithms%2520have%2520been%2520developed%2520with%2520a%250Afocus%2520on%2520addressing%2520the%2520problem%2520of%2520fairness-aware%2520domain%2520generalization.%2520These%250Aalgorithms%2520are%2520designed%2520to%2520navigate%2520various%2520types%2520of%2520distribution%2520shifts%252C%2520with%250Aa%2520particular%2520emphasis%2520on%2520covariate%2520and%2520dependence%2520shifts.%2520In%2520this%2520context%252C%250Acovariate%2520shift%2520pertains%2520to%2520changes%2520in%2520the%2520marginal%2520distribution%2520of%2520input%250Afeatures%252C%2520while%2520dependence%2520shift%2520involves%2520alterations%2520in%2520the%2520joint%2520distribution%250Aof%2520the%2520label%2520variable%2520and%2520sensitive%2520attributes.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Asimple%2520but%2520effective%2520approach%2520that%2520aims%2520to%2520learn%2520a%2520fair%2520and%2520invariant%250Aclassifier%2520by%2520simultaneously%2520addressing%2520both%2520covariate%2520and%2520dependence%2520shifts%250Aacross%2520domains.%2520We%2520assert%2520the%2520existence%2520of%2520an%2520underlying%2520transformation%2520model%250Acan%2520transform%2520data%2520from%2520one%2520domain%2520to%2520another%252C%2520while%2520preserving%2520the%2520semantics%250Arelated%2520to%2520non-sensitive%2520attributes%2520and%2520classes.%2520By%2520augmenting%2520various%250Asynthetic%2520data%2520domains%2520through%2520the%2520model%252C%2520we%2520learn%2520a%2520fair%2520and%2520invariant%250Aclassifier%2520in%2520source%2520domains.%2520This%2520classifier%2520can%2520then%2520be%2520generalized%2520to%250Aunknown%2520target%2520domains%252C%2520maintaining%2520both%2520model%2520prediction%2520and%2520fairness%250Aconcerns.%2520Extensive%2520empirical%2520studies%2520on%2520four%2520benchmark%2520datasets%2520demonstrate%250Athat%2520our%2520approach%2520surpasses%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.13816v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Algorithmic%20Fairness%20Generalization%20under%20Covariate%20and%20Dependence%0A%20%20Shifts%20Simultaneously&entry.906535625=Chen%20Zhao%20and%20Kai%20Jiang%20and%20Xintao%20Wu%20and%20Haoliang%20Wang%20and%20Latifur%20Khan%20and%20Christan%20Grant%20and%20Feng%20Chen&entry.1292438233=%20%20The%20endeavor%20to%20preserve%20the%20generalization%20of%20a%20fair%20and%20invariant%0Aclassifier%20across%20domains%2C%20especially%20in%20the%20presence%20of%20distribution%20shifts%2C%0Abecomes%20a%20significant%20and%20intricate%20challenge%20in%20machine%20learning.%20In%20response%0Ato%20this%20challenge%2C%20numerous%20effective%20algorithms%20have%20been%20developed%20with%20a%0Afocus%20on%20addressing%20the%20problem%20of%20fairness-aware%20domain%20generalization.%20These%0Aalgorithms%20are%20designed%20to%20navigate%20various%20types%20of%20distribution%20shifts%2C%20with%0Aa%20particular%20emphasis%20on%20covariate%20and%20dependence%20shifts.%20In%20this%20context%2C%0Acovariate%20shift%20pertains%20to%20changes%20in%20the%20marginal%20distribution%20of%20input%0Afeatures%2C%20while%20dependence%20shift%20involves%20alterations%20in%20the%20joint%20distribution%0Aof%20the%20label%20variable%20and%20sensitive%20attributes.%20In%20this%20paper%2C%20we%20introduce%20a%0Asimple%20but%20effective%20approach%20that%20aims%20to%20learn%20a%20fair%20and%20invariant%0Aclassifier%20by%20simultaneously%20addressing%20both%20covariate%20and%20dependence%20shifts%0Aacross%20domains.%20We%20assert%20the%20existence%20of%20an%20underlying%20transformation%20model%0Acan%20transform%20data%20from%20one%20domain%20to%20another%2C%20while%20preserving%20the%20semantics%0Arelated%20to%20non-sensitive%20attributes%20and%20classes.%20By%20augmenting%20various%0Asynthetic%20data%20domains%20through%20the%20model%2C%20we%20learn%20a%20fair%20and%20invariant%0Aclassifier%20in%20source%20domains.%20This%20classifier%20can%20then%20be%20generalized%20to%0Aunknown%20target%20domains%2C%20maintaining%20both%20model%20prediction%20and%20fairness%0Aconcerns.%20Extensive%20empirical%20studies%20on%20four%20benchmark%20datasets%20demonstrate%0Athat%20our%20approach%20surpasses%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.13816v2&entry.124074799=Read"},
{"title": "Residual Quantization with Implicit Neural Codebooks", "author": "Iris A. M. Huijben and Matthijs Douze and Matthew Muckley and Ruud J. G. van Sloun and Jakob Verbeek", "abstract": "  Vector quantization is a fundamental operation for data compression and\nvector search. To obtain high accuracy, multi-codebook methods represent each\nvector using codewords across several codebooks. Residual quantization (RQ) is\none such method, which iteratively quantizes the error of the previous step.\nWhile the error distribution is dependent on previously-selected codewords,\nthis dependency is not accounted for in conventional RQ as it uses a fixed\ncodebook per quantization step. In this paper, we propose QINCo, a neural RQ\nvariant that constructs specialized codebooks per step that depend on the\napproximation of the vector from previous steps. Experiments show that QINCo\noutperforms state-of-the-art methods by a large margin on several datasets and\ncode sizes. For example, QINCo achieves better nearest-neighbor search accuracy\nusing 12-byte codes than the state-of-the-art UNQ using 16 bytes on the\nBigANN1M and Deep1M datasets.\n", "link": "http://arxiv.org/abs/2401.14732v2", "date": "2024-05-21", "relevancy": 1.9109, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5092}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4585}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Residual%20Quantization%20with%20Implicit%20Neural%20Codebooks&body=Title%3A%20Residual%20Quantization%20with%20Implicit%20Neural%20Codebooks%0AAuthor%3A%20Iris%20A.%20M.%20Huijben%20and%20Matthijs%20Douze%20and%20Matthew%20Muckley%20and%20Ruud%20J.%20G.%20van%20Sloun%20and%20Jakob%20Verbeek%0AAbstract%3A%20%20%20Vector%20quantization%20is%20a%20fundamental%20operation%20for%20data%20compression%20and%0Avector%20search.%20To%20obtain%20high%20accuracy%2C%20multi-codebook%20methods%20represent%20each%0Avector%20using%20codewords%20across%20several%20codebooks.%20Residual%20quantization%20%28RQ%29%20is%0Aone%20such%20method%2C%20which%20iteratively%20quantizes%20the%20error%20of%20the%20previous%20step.%0AWhile%20the%20error%20distribution%20is%20dependent%20on%20previously-selected%20codewords%2C%0Athis%20dependency%20is%20not%20accounted%20for%20in%20conventional%20RQ%20as%20it%20uses%20a%20fixed%0Acodebook%20per%20quantization%20step.%20In%20this%20paper%2C%20we%20propose%20QINCo%2C%20a%20neural%20RQ%0Avariant%20that%20constructs%20specialized%20codebooks%20per%20step%20that%20depend%20on%20the%0Aapproximation%20of%20the%20vector%20from%20previous%20steps.%20Experiments%20show%20that%20QINCo%0Aoutperforms%20state-of-the-art%20methods%20by%20a%20large%20margin%20on%20several%20datasets%20and%0Acode%20sizes.%20For%20example%2C%20QINCo%20achieves%20better%20nearest-neighbor%20search%20accuracy%0Ausing%2012-byte%20codes%20than%20the%20state-of-the-art%20UNQ%20using%2016%20bytes%20on%20the%0ABigANN1M%20and%20Deep1M%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14732v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResidual%2520Quantization%2520with%2520Implicit%2520Neural%2520Codebooks%26entry.906535625%3DIris%2520A.%2520M.%2520Huijben%2520and%2520Matthijs%2520Douze%2520and%2520Matthew%2520Muckley%2520and%2520Ruud%2520J.%2520G.%2520van%2520Sloun%2520and%2520Jakob%2520Verbeek%26entry.1292438233%3D%2520%2520Vector%2520quantization%2520is%2520a%2520fundamental%2520operation%2520for%2520data%2520compression%2520and%250Avector%2520search.%2520To%2520obtain%2520high%2520accuracy%252C%2520multi-codebook%2520methods%2520represent%2520each%250Avector%2520using%2520codewords%2520across%2520several%2520codebooks.%2520Residual%2520quantization%2520%2528RQ%2529%2520is%250Aone%2520such%2520method%252C%2520which%2520iteratively%2520quantizes%2520the%2520error%2520of%2520the%2520previous%2520step.%250AWhile%2520the%2520error%2520distribution%2520is%2520dependent%2520on%2520previously-selected%2520codewords%252C%250Athis%2520dependency%2520is%2520not%2520accounted%2520for%2520in%2520conventional%2520RQ%2520as%2520it%2520uses%2520a%2520fixed%250Acodebook%2520per%2520quantization%2520step.%2520In%2520this%2520paper%252C%2520we%2520propose%2520QINCo%252C%2520a%2520neural%2520RQ%250Avariant%2520that%2520constructs%2520specialized%2520codebooks%2520per%2520step%2520that%2520depend%2520on%2520the%250Aapproximation%2520of%2520the%2520vector%2520from%2520previous%2520steps.%2520Experiments%2520show%2520that%2520QINCo%250Aoutperforms%2520state-of-the-art%2520methods%2520by%2520a%2520large%2520margin%2520on%2520several%2520datasets%2520and%250Acode%2520sizes.%2520For%2520example%252C%2520QINCo%2520achieves%2520better%2520nearest-neighbor%2520search%2520accuracy%250Ausing%252012-byte%2520codes%2520than%2520the%2520state-of-the-art%2520UNQ%2520using%252016%2520bytes%2520on%2520the%250ABigANN1M%2520and%2520Deep1M%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.14732v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual%20Quantization%20with%20Implicit%20Neural%20Codebooks&entry.906535625=Iris%20A.%20M.%20Huijben%20and%20Matthijs%20Douze%20and%20Matthew%20Muckley%20and%20Ruud%20J.%20G.%20van%20Sloun%20and%20Jakob%20Verbeek&entry.1292438233=%20%20Vector%20quantization%20is%20a%20fundamental%20operation%20for%20data%20compression%20and%0Avector%20search.%20To%20obtain%20high%20accuracy%2C%20multi-codebook%20methods%20represent%20each%0Avector%20using%20codewords%20across%20several%20codebooks.%20Residual%20quantization%20%28RQ%29%20is%0Aone%20such%20method%2C%20which%20iteratively%20quantizes%20the%20error%20of%20the%20previous%20step.%0AWhile%20the%20error%20distribution%20is%20dependent%20on%20previously-selected%20codewords%2C%0Athis%20dependency%20is%20not%20accounted%20for%20in%20conventional%20RQ%20as%20it%20uses%20a%20fixed%0Acodebook%20per%20quantization%20step.%20In%20this%20paper%2C%20we%20propose%20QINCo%2C%20a%20neural%20RQ%0Avariant%20that%20constructs%20specialized%20codebooks%20per%20step%20that%20depend%20on%20the%0Aapproximation%20of%20the%20vector%20from%20previous%20steps.%20Experiments%20show%20that%20QINCo%0Aoutperforms%20state-of-the-art%20methods%20by%20a%20large%20margin%20on%20several%20datasets%20and%0Acode%20sizes.%20For%20example%2C%20QINCo%20achieves%20better%20nearest-neighbor%20search%20accuracy%0Ausing%2012-byte%20codes%20than%20the%20state-of-the-art%20UNQ%20using%2016%20bytes%20on%20the%0ABigANN1M%20and%20Deep1M%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14732v2&entry.124074799=Read"},
{"title": "Comparing Neighbors Together Makes it Easy: Jointly Comparing Multiple\n  Candidates for Efficient and Effective Retrieval", "author": "Jonghyun Song and Cheyon Jin and Wenlong Zhao and Jay-Yoon Lee", "abstract": "  A common retrieve-and-rerank paradigm involves retrieving a broad set of\nrelevant candidates using a scalable bi-encoder, followed by expensive but more\naccurate cross-encoders to a limited candidate set. However, this small subset\noften leads to error propagation from the bi-encoders, thereby restricting the\nperformance of the overall pipeline. To address these issues, we propose the\nComparing Multiple Candidates (CMC) framework, which compares a query and\nmultiple candidate embeddings jointly through shallow self-attention layers.\nWhile providing contextualized representations, CMC is scalable enough to\nhandle multiple comparisons simultaneously, where comparing 2K candidates takes\nonly twice as long as comparing 100. Practitioners can use CMC as a lightweight\nand effective reranker to improve top-1 accuracy. Moreover, when integrated\nwith another retriever, CMC reranking can function as a virtually enhanced\nretriever. This configuration adds only negligible latency compared to using a\nsingle retriever (virtual), while significantly improving recall at K\n(enhanced).} Through experiments, we demonstrate that CMC, as a virtually\nenhanced retriever, significantly improves Recall@k (+6.7, +3.5%-p for R@16,\nR@64) compared to the initial retrieval stage on the ZeSHEL dataset. Meanwhile,\nwe conduct experiments for direct reranking on entity, passage, and dialogue\nranking. The results indicate that CMC is not only faster (11x) than\ncross-encoders but also often more effective, with improved prediction\nperformance in Wikipedia entity linking (+0.7%-p) and DSTC7 dialogue ranking\n(+3.3%-p). The code and link to datasets are available at\nhttps://github.com/yc-song/cmc\n", "link": "http://arxiv.org/abs/2405.12801v1", "date": "2024-05-21", "relevancy": 1.9058, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4822}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4742}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20Neighbors%20Together%20Makes%20it%20Easy%3A%20Jointly%20Comparing%20Multiple%0A%20%20Candidates%20for%20Efficient%20and%20Effective%20Retrieval&body=Title%3A%20Comparing%20Neighbors%20Together%20Makes%20it%20Easy%3A%20Jointly%20Comparing%20Multiple%0A%20%20Candidates%20for%20Efficient%20and%20Effective%20Retrieval%0AAuthor%3A%20Jonghyun%20Song%20and%20Cheyon%20Jin%20and%20Wenlong%20Zhao%20and%20Jay-Yoon%20Lee%0AAbstract%3A%20%20%20A%20common%20retrieve-and-rerank%20paradigm%20involves%20retrieving%20a%20broad%20set%20of%0Arelevant%20candidates%20using%20a%20scalable%20bi-encoder%2C%20followed%20by%20expensive%20but%20more%0Aaccurate%20cross-encoders%20to%20a%20limited%20candidate%20set.%20However%2C%20this%20small%20subset%0Aoften%20leads%20to%20error%20propagation%20from%20the%20bi-encoders%2C%20thereby%20restricting%20the%0Aperformance%20of%20the%20overall%20pipeline.%20To%20address%20these%20issues%2C%20we%20propose%20the%0AComparing%20Multiple%20Candidates%20%28CMC%29%20framework%2C%20which%20compares%20a%20query%20and%0Amultiple%20candidate%20embeddings%20jointly%20through%20shallow%20self-attention%20layers.%0AWhile%20providing%20contextualized%20representations%2C%20CMC%20is%20scalable%20enough%20to%0Ahandle%20multiple%20comparisons%20simultaneously%2C%20where%20comparing%202K%20candidates%20takes%0Aonly%20twice%20as%20long%20as%20comparing%20100.%20Practitioners%20can%20use%20CMC%20as%20a%20lightweight%0Aand%20effective%20reranker%20to%20improve%20top-1%20accuracy.%20Moreover%2C%20when%20integrated%0Awith%20another%20retriever%2C%20CMC%20reranking%20can%20function%20as%20a%20virtually%20enhanced%0Aretriever.%20This%20configuration%20adds%20only%20negligible%20latency%20compared%20to%20using%20a%0Asingle%20retriever%20%28virtual%29%2C%20while%20significantly%20improving%20recall%20at%20K%0A%28enhanced%29.%7D%20Through%20experiments%2C%20we%20demonstrate%20that%20CMC%2C%20as%20a%20virtually%0Aenhanced%20retriever%2C%20significantly%20improves%20Recall%40k%20%28%2B6.7%2C%20%2B3.5%25-p%20for%20R%4016%2C%0AR%4064%29%20compared%20to%20the%20initial%20retrieval%20stage%20on%20the%20ZeSHEL%20dataset.%20Meanwhile%2C%0Awe%20conduct%20experiments%20for%20direct%20reranking%20on%20entity%2C%20passage%2C%20and%20dialogue%0Aranking.%20The%20results%20indicate%20that%20CMC%20is%20not%20only%20faster%20%2811x%29%20than%0Across-encoders%20but%20also%20often%20more%20effective%2C%20with%20improved%20prediction%0Aperformance%20in%20Wikipedia%20entity%20linking%20%28%2B0.7%25-p%29%20and%20DSTC7%20dialogue%20ranking%0A%28%2B3.3%25-p%29.%20The%20code%20and%20link%20to%20datasets%20are%20available%20at%0Ahttps%3A//github.com/yc-song/cmc%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520Neighbors%2520Together%2520Makes%2520it%2520Easy%253A%2520Jointly%2520Comparing%2520Multiple%250A%2520%2520Candidates%2520for%2520Efficient%2520and%2520Effective%2520Retrieval%26entry.906535625%3DJonghyun%2520Song%2520and%2520Cheyon%2520Jin%2520and%2520Wenlong%2520Zhao%2520and%2520Jay-Yoon%2520Lee%26entry.1292438233%3D%2520%2520A%2520common%2520retrieve-and-rerank%2520paradigm%2520involves%2520retrieving%2520a%2520broad%2520set%2520of%250Arelevant%2520candidates%2520using%2520a%2520scalable%2520bi-encoder%252C%2520followed%2520by%2520expensive%2520but%2520more%250Aaccurate%2520cross-encoders%2520to%2520a%2520limited%2520candidate%2520set.%2520However%252C%2520this%2520small%2520subset%250Aoften%2520leads%2520to%2520error%2520propagation%2520from%2520the%2520bi-encoders%252C%2520thereby%2520restricting%2520the%250Aperformance%2520of%2520the%2520overall%2520pipeline.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520the%250AComparing%2520Multiple%2520Candidates%2520%2528CMC%2529%2520framework%252C%2520which%2520compares%2520a%2520query%2520and%250Amultiple%2520candidate%2520embeddings%2520jointly%2520through%2520shallow%2520self-attention%2520layers.%250AWhile%2520providing%2520contextualized%2520representations%252C%2520CMC%2520is%2520scalable%2520enough%2520to%250Ahandle%2520multiple%2520comparisons%2520simultaneously%252C%2520where%2520comparing%25202K%2520candidates%2520takes%250Aonly%2520twice%2520as%2520long%2520as%2520comparing%2520100.%2520Practitioners%2520can%2520use%2520CMC%2520as%2520a%2520lightweight%250Aand%2520effective%2520reranker%2520to%2520improve%2520top-1%2520accuracy.%2520Moreover%252C%2520when%2520integrated%250Awith%2520another%2520retriever%252C%2520CMC%2520reranking%2520can%2520function%2520as%2520a%2520virtually%2520enhanced%250Aretriever.%2520This%2520configuration%2520adds%2520only%2520negligible%2520latency%2520compared%2520to%2520using%2520a%250Asingle%2520retriever%2520%2528virtual%2529%252C%2520while%2520significantly%2520improving%2520recall%2520at%2520K%250A%2528enhanced%2529.%257D%2520Through%2520experiments%252C%2520we%2520demonstrate%2520that%2520CMC%252C%2520as%2520a%2520virtually%250Aenhanced%2520retriever%252C%2520significantly%2520improves%2520Recall%2540k%2520%2528%252B6.7%252C%2520%252B3.5%2525-p%2520for%2520R%254016%252C%250AR%254064%2529%2520compared%2520to%2520the%2520initial%2520retrieval%2520stage%2520on%2520the%2520ZeSHEL%2520dataset.%2520Meanwhile%252C%250Awe%2520conduct%2520experiments%2520for%2520direct%2520reranking%2520on%2520entity%252C%2520passage%252C%2520and%2520dialogue%250Aranking.%2520The%2520results%2520indicate%2520that%2520CMC%2520is%2520not%2520only%2520faster%2520%252811x%2529%2520than%250Across-encoders%2520but%2520also%2520often%2520more%2520effective%252C%2520with%2520improved%2520prediction%250Aperformance%2520in%2520Wikipedia%2520entity%2520linking%2520%2528%252B0.7%2525-p%2529%2520and%2520DSTC7%2520dialogue%2520ranking%250A%2528%252B3.3%2525-p%2529.%2520The%2520code%2520and%2520link%2520to%2520datasets%2520are%2520available%2520at%250Ahttps%253A//github.com/yc-song/cmc%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20Neighbors%20Together%20Makes%20it%20Easy%3A%20Jointly%20Comparing%20Multiple%0A%20%20Candidates%20for%20Efficient%20and%20Effective%20Retrieval&entry.906535625=Jonghyun%20Song%20and%20Cheyon%20Jin%20and%20Wenlong%20Zhao%20and%20Jay-Yoon%20Lee&entry.1292438233=%20%20A%20common%20retrieve-and-rerank%20paradigm%20involves%20retrieving%20a%20broad%20set%20of%0Arelevant%20candidates%20using%20a%20scalable%20bi-encoder%2C%20followed%20by%20expensive%20but%20more%0Aaccurate%20cross-encoders%20to%20a%20limited%20candidate%20set.%20However%2C%20this%20small%20subset%0Aoften%20leads%20to%20error%20propagation%20from%20the%20bi-encoders%2C%20thereby%20restricting%20the%0Aperformance%20of%20the%20overall%20pipeline.%20To%20address%20these%20issues%2C%20we%20propose%20the%0AComparing%20Multiple%20Candidates%20%28CMC%29%20framework%2C%20which%20compares%20a%20query%20and%0Amultiple%20candidate%20embeddings%20jointly%20through%20shallow%20self-attention%20layers.%0AWhile%20providing%20contextualized%20representations%2C%20CMC%20is%20scalable%20enough%20to%0Ahandle%20multiple%20comparisons%20simultaneously%2C%20where%20comparing%202K%20candidates%20takes%0Aonly%20twice%20as%20long%20as%20comparing%20100.%20Practitioners%20can%20use%20CMC%20as%20a%20lightweight%0Aand%20effective%20reranker%20to%20improve%20top-1%20accuracy.%20Moreover%2C%20when%20integrated%0Awith%20another%20retriever%2C%20CMC%20reranking%20can%20function%20as%20a%20virtually%20enhanced%0Aretriever.%20This%20configuration%20adds%20only%20negligible%20latency%20compared%20to%20using%20a%0Asingle%20retriever%20%28virtual%29%2C%20while%20significantly%20improving%20recall%20at%20K%0A%28enhanced%29.%7D%20Through%20experiments%2C%20we%20demonstrate%20that%20CMC%2C%20as%20a%20virtually%0Aenhanced%20retriever%2C%20significantly%20improves%20Recall%40k%20%28%2B6.7%2C%20%2B3.5%25-p%20for%20R%4016%2C%0AR%4064%29%20compared%20to%20the%20initial%20retrieval%20stage%20on%20the%20ZeSHEL%20dataset.%20Meanwhile%2C%0Awe%20conduct%20experiments%20for%20direct%20reranking%20on%20entity%2C%20passage%2C%20and%20dialogue%0Aranking.%20The%20results%20indicate%20that%20CMC%20is%20not%20only%20faster%20%2811x%29%20than%0Across-encoders%20but%20also%20often%20more%20effective%2C%20with%20improved%20prediction%0Aperformance%20in%20Wikipedia%20entity%20linking%20%28%2B0.7%25-p%29%20and%20DSTC7%20dialogue%20ranking%0A%28%2B3.3%25-p%29.%20The%20code%20and%20link%20to%20datasets%20are%20available%20at%0Ahttps%3A//github.com/yc-song/cmc%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12801v1&entry.124074799=Read"},
{"title": "Few-sample Variational Inference of Bayesian Neural Networks with\n  Arbitrary Nonlinearities", "author": "David J. Schodt", "abstract": "  Bayesian Neural Networks (BNNs) extend traditional neural networks to provide\nuncertainties associated with their outputs. On the forward pass through a BNN,\npredictions (and their uncertainties) are made either by Monte Carlo sampling\nnetwork weights from the learned posterior or by analytically propagating\nstatistical moments through the network. Though flexible, Monte Carlo sampling\nis computationally expensive and can be infeasible or impractical under\nresource constraints or for large networks. While moment propagation can\nameliorate the computational costs of BNN inference, it can be difficult or\nimpossible for networks with arbitrary nonlinearities, thereby restricting the\npossible set of network layers permitted with such a scheme. In this work, we\ndemonstrate a simple yet effective approach for propagating statistical moments\nthrough arbitrary nonlinearities with only 3 deterministic samples, enabling\nfew-sample variational inference of BNNs without restricting the set of network\nlayers used. Furthermore, we leverage this approach to demonstrate a novel\nnonlinear activation function that we use to inject physics-informed prior\ninformation into output nodes of a BNN.\n", "link": "http://arxiv.org/abs/2405.02063v2", "date": "2024-05-21", "relevancy": 1.9041, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5398}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.474}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-sample%20Variational%20Inference%20of%20Bayesian%20Neural%20Networks%20with%0A%20%20Arbitrary%20Nonlinearities&body=Title%3A%20Few-sample%20Variational%20Inference%20of%20Bayesian%20Neural%20Networks%20with%0A%20%20Arbitrary%20Nonlinearities%0AAuthor%3A%20David%20J.%20Schodt%0AAbstract%3A%20%20%20Bayesian%20Neural%20Networks%20%28BNNs%29%20extend%20traditional%20neural%20networks%20to%20provide%0Auncertainties%20associated%20with%20their%20outputs.%20On%20the%20forward%20pass%20through%20a%20BNN%2C%0Apredictions%20%28and%20their%20uncertainties%29%20are%20made%20either%20by%20Monte%20Carlo%20sampling%0Anetwork%20weights%20from%20the%20learned%20posterior%20or%20by%20analytically%20propagating%0Astatistical%20moments%20through%20the%20network.%20Though%20flexible%2C%20Monte%20Carlo%20sampling%0Ais%20computationally%20expensive%20and%20can%20be%20infeasible%20or%20impractical%20under%0Aresource%20constraints%20or%20for%20large%20networks.%20While%20moment%20propagation%20can%0Aameliorate%20the%20computational%20costs%20of%20BNN%20inference%2C%20it%20can%20be%20difficult%20or%0Aimpossible%20for%20networks%20with%20arbitrary%20nonlinearities%2C%20thereby%20restricting%20the%0Apossible%20set%20of%20network%20layers%20permitted%20with%20such%20a%20scheme.%20In%20this%20work%2C%20we%0Ademonstrate%20a%20simple%20yet%20effective%20approach%20for%20propagating%20statistical%20moments%0Athrough%20arbitrary%20nonlinearities%20with%20only%203%20deterministic%20samples%2C%20enabling%0Afew-sample%20variational%20inference%20of%20BNNs%20without%20restricting%20the%20set%20of%20network%0Alayers%20used.%20Furthermore%2C%20we%20leverage%20this%20approach%20to%20demonstrate%20a%20novel%0Anonlinear%20activation%20function%20that%20we%20use%20to%20inject%20physics-informed%20prior%0Ainformation%20into%20output%20nodes%20of%20a%20BNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02063v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-sample%2520Variational%2520Inference%2520of%2520Bayesian%2520Neural%2520Networks%2520with%250A%2520%2520Arbitrary%2520Nonlinearities%26entry.906535625%3DDavid%2520J.%2520Schodt%26entry.1292438233%3D%2520%2520Bayesian%2520Neural%2520Networks%2520%2528BNNs%2529%2520extend%2520traditional%2520neural%2520networks%2520to%2520provide%250Auncertainties%2520associated%2520with%2520their%2520outputs.%2520On%2520the%2520forward%2520pass%2520through%2520a%2520BNN%252C%250Apredictions%2520%2528and%2520their%2520uncertainties%2529%2520are%2520made%2520either%2520by%2520Monte%2520Carlo%2520sampling%250Anetwork%2520weights%2520from%2520the%2520learned%2520posterior%2520or%2520by%2520analytically%2520propagating%250Astatistical%2520moments%2520through%2520the%2520network.%2520Though%2520flexible%252C%2520Monte%2520Carlo%2520sampling%250Ais%2520computationally%2520expensive%2520and%2520can%2520be%2520infeasible%2520or%2520impractical%2520under%250Aresource%2520constraints%2520or%2520for%2520large%2520networks.%2520While%2520moment%2520propagation%2520can%250Aameliorate%2520the%2520computational%2520costs%2520of%2520BNN%2520inference%252C%2520it%2520can%2520be%2520difficult%2520or%250Aimpossible%2520for%2520networks%2520with%2520arbitrary%2520nonlinearities%252C%2520thereby%2520restricting%2520the%250Apossible%2520set%2520of%2520network%2520layers%2520permitted%2520with%2520such%2520a%2520scheme.%2520In%2520this%2520work%252C%2520we%250Ademonstrate%2520a%2520simple%2520yet%2520effective%2520approach%2520for%2520propagating%2520statistical%2520moments%250Athrough%2520arbitrary%2520nonlinearities%2520with%2520only%25203%2520deterministic%2520samples%252C%2520enabling%250Afew-sample%2520variational%2520inference%2520of%2520BNNs%2520without%2520restricting%2520the%2520set%2520of%2520network%250Alayers%2520used.%2520Furthermore%252C%2520we%2520leverage%2520this%2520approach%2520to%2520demonstrate%2520a%2520novel%250Anonlinear%2520activation%2520function%2520that%2520we%2520use%2520to%2520inject%2520physics-informed%2520prior%250Ainformation%2520into%2520output%2520nodes%2520of%2520a%2520BNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02063v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-sample%20Variational%20Inference%20of%20Bayesian%20Neural%20Networks%20with%0A%20%20Arbitrary%20Nonlinearities&entry.906535625=David%20J.%20Schodt&entry.1292438233=%20%20Bayesian%20Neural%20Networks%20%28BNNs%29%20extend%20traditional%20neural%20networks%20to%20provide%0Auncertainties%20associated%20with%20their%20outputs.%20On%20the%20forward%20pass%20through%20a%20BNN%2C%0Apredictions%20%28and%20their%20uncertainties%29%20are%20made%20either%20by%20Monte%20Carlo%20sampling%0Anetwork%20weights%20from%20the%20learned%20posterior%20or%20by%20analytically%20propagating%0Astatistical%20moments%20through%20the%20network.%20Though%20flexible%2C%20Monte%20Carlo%20sampling%0Ais%20computationally%20expensive%20and%20can%20be%20infeasible%20or%20impractical%20under%0Aresource%20constraints%20or%20for%20large%20networks.%20While%20moment%20propagation%20can%0Aameliorate%20the%20computational%20costs%20of%20BNN%20inference%2C%20it%20can%20be%20difficult%20or%0Aimpossible%20for%20networks%20with%20arbitrary%20nonlinearities%2C%20thereby%20restricting%20the%0Apossible%20set%20of%20network%20layers%20permitted%20with%20such%20a%20scheme.%20In%20this%20work%2C%20we%0Ademonstrate%20a%20simple%20yet%20effective%20approach%20for%20propagating%20statistical%20moments%0Athrough%20arbitrary%20nonlinearities%20with%20only%203%20deterministic%20samples%2C%20enabling%0Afew-sample%20variational%20inference%20of%20BNNs%20without%20restricting%20the%20set%20of%20network%0Alayers%20used.%20Furthermore%2C%20we%20leverage%20this%20approach%20to%20demonstrate%20a%20novel%0Anonlinear%20activation%20function%20that%20we%20use%20to%20inject%20physics-informed%20prior%0Ainformation%20into%20output%20nodes%20of%20a%20BNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02063v2&entry.124074799=Read"},
{"title": "VertiBayes: Learning Bayesian network parameters from vertically\n  partitioned data with missing values", "author": "Florian van Daalen and Lianne Ippel and Andre Dekker and Inigo Bermejo", "abstract": "  Federated learning makes it possible to train a machine learning model on\ndecentralized data. Bayesian networks are probabilistic graphical models that\nhave been widely used in artificial intelligence applications. Their popularity\nstems from the fact they can be built by combining existing expert knowledge\nwith data and are highly interpretable, which makes them useful for decision\nsupport, e.g. in healthcare. While some research has been published on the\nfederated learning of Bayesian networks, publications on Bayesian networks in a\nvertically partitioned or heterogeneous data setting (where different variables\nare located in different datasets) are limited, and suffer from important\nomissions, such as the handling of missing data. In this article, we propose a\nnovel method called VertiBayes to train Bayesian networks (structure and\nparameters) on vertically partitioned data, which can handle missing values as\nwell as an arbitrary number of parties. For structure learning we adapted the\nwidely used K2 algorithm with a privacy-preserving scalar product protocol. For\nparameter learning, we use a two-step approach: first, we learn an intermediate\nmodel using maximum likelihood by treating missing values as a special value\nand then we train a model on synthetic data generated by the intermediate model\nusing the EM algorithm. The privacy guarantees of our approach are equivalent\nto the ones provided by the privacy preserving scalar product protocol used. We\nexperimentally show our approach produces models comparable to those learnt\nusing traditional algorithms and we estimate the increase in complexity in\nterms of samples, network size, and complexity. Finally, we propose two\nalternative approaches to estimate the performance of the model using\nvertically partitioned data and we show in experiments that they lead to\nreasonably accurate estimates.\n", "link": "http://arxiv.org/abs/2210.17228v2", "date": "2024-05-21", "relevancy": 1.896, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.518}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4659}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VertiBayes%3A%20Learning%20Bayesian%20network%20parameters%20from%20vertically%0A%20%20partitioned%20data%20with%20missing%20values&body=Title%3A%20VertiBayes%3A%20Learning%20Bayesian%20network%20parameters%20from%20vertically%0A%20%20partitioned%20data%20with%20missing%20values%0AAuthor%3A%20Florian%20van%20Daalen%20and%20Lianne%20Ippel%20and%20Andre%20Dekker%20and%20Inigo%20Bermejo%0AAbstract%3A%20%20%20Federated%20learning%20makes%20it%20possible%20to%20train%20a%20machine%20learning%20model%20on%0Adecentralized%20data.%20Bayesian%20networks%20are%20probabilistic%20graphical%20models%20that%0Ahave%20been%20widely%20used%20in%20artificial%20intelligence%20applications.%20Their%20popularity%0Astems%20from%20the%20fact%20they%20can%20be%20built%20by%20combining%20existing%20expert%20knowledge%0Awith%20data%20and%20are%20highly%20interpretable%2C%20which%20makes%20them%20useful%20for%20decision%0Asupport%2C%20e.g.%20in%20healthcare.%20While%20some%20research%20has%20been%20published%20on%20the%0Afederated%20learning%20of%20Bayesian%20networks%2C%20publications%20on%20Bayesian%20networks%20in%20a%0Avertically%20partitioned%20or%20heterogeneous%20data%20setting%20%28where%20different%20variables%0Aare%20located%20in%20different%20datasets%29%20are%20limited%2C%20and%20suffer%20from%20important%0Aomissions%2C%20such%20as%20the%20handling%20of%20missing%20data.%20In%20this%20article%2C%20we%20propose%20a%0Anovel%20method%20called%20VertiBayes%20to%20train%20Bayesian%20networks%20%28structure%20and%0Aparameters%29%20on%20vertically%20partitioned%20data%2C%20which%20can%20handle%20missing%20values%20as%0Awell%20as%20an%20arbitrary%20number%20of%20parties.%20For%20structure%20learning%20we%20adapted%20the%0Awidely%20used%20K2%20algorithm%20with%20a%20privacy-preserving%20scalar%20product%20protocol.%20For%0Aparameter%20learning%2C%20we%20use%20a%20two-step%20approach%3A%20first%2C%20we%20learn%20an%20intermediate%0Amodel%20using%20maximum%20likelihood%20by%20treating%20missing%20values%20as%20a%20special%20value%0Aand%20then%20we%20train%20a%20model%20on%20synthetic%20data%20generated%20by%20the%20intermediate%20model%0Ausing%20the%20EM%20algorithm.%20The%20privacy%20guarantees%20of%20our%20approach%20are%20equivalent%0Ato%20the%20ones%20provided%20by%20the%20privacy%20preserving%20scalar%20product%20protocol%20used.%20We%0Aexperimentally%20show%20our%20approach%20produces%20models%20comparable%20to%20those%20learnt%0Ausing%20traditional%20algorithms%20and%20we%20estimate%20the%20increase%20in%20complexity%20in%0Aterms%20of%20samples%2C%20network%20size%2C%20and%20complexity.%20Finally%2C%20we%20propose%20two%0Aalternative%20approaches%20to%20estimate%20the%20performance%20of%20the%20model%20using%0Avertically%20partitioned%20data%20and%20we%20show%20in%20experiments%20that%20they%20lead%20to%0Areasonably%20accurate%20estimates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.17228v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVertiBayes%253A%2520Learning%2520Bayesian%2520network%2520parameters%2520from%2520vertically%250A%2520%2520partitioned%2520data%2520with%2520missing%2520values%26entry.906535625%3DFlorian%2520van%2520Daalen%2520and%2520Lianne%2520Ippel%2520and%2520Andre%2520Dekker%2520and%2520Inigo%2520Bermejo%26entry.1292438233%3D%2520%2520Federated%2520learning%2520makes%2520it%2520possible%2520to%2520train%2520a%2520machine%2520learning%2520model%2520on%250Adecentralized%2520data.%2520Bayesian%2520networks%2520are%2520probabilistic%2520graphical%2520models%2520that%250Ahave%2520been%2520widely%2520used%2520in%2520artificial%2520intelligence%2520applications.%2520Their%2520popularity%250Astems%2520from%2520the%2520fact%2520they%2520can%2520be%2520built%2520by%2520combining%2520existing%2520expert%2520knowledge%250Awith%2520data%2520and%2520are%2520highly%2520interpretable%252C%2520which%2520makes%2520them%2520useful%2520for%2520decision%250Asupport%252C%2520e.g.%2520in%2520healthcare.%2520While%2520some%2520research%2520has%2520been%2520published%2520on%2520the%250Afederated%2520learning%2520of%2520Bayesian%2520networks%252C%2520publications%2520on%2520Bayesian%2520networks%2520in%2520a%250Avertically%2520partitioned%2520or%2520heterogeneous%2520data%2520setting%2520%2528where%2520different%2520variables%250Aare%2520located%2520in%2520different%2520datasets%2529%2520are%2520limited%252C%2520and%2520suffer%2520from%2520important%250Aomissions%252C%2520such%2520as%2520the%2520handling%2520of%2520missing%2520data.%2520In%2520this%2520article%252C%2520we%2520propose%2520a%250Anovel%2520method%2520called%2520VertiBayes%2520to%2520train%2520Bayesian%2520networks%2520%2528structure%2520and%250Aparameters%2529%2520on%2520vertically%2520partitioned%2520data%252C%2520which%2520can%2520handle%2520missing%2520values%2520as%250Awell%2520as%2520an%2520arbitrary%2520number%2520of%2520parties.%2520For%2520structure%2520learning%2520we%2520adapted%2520the%250Awidely%2520used%2520K2%2520algorithm%2520with%2520a%2520privacy-preserving%2520scalar%2520product%2520protocol.%2520For%250Aparameter%2520learning%252C%2520we%2520use%2520a%2520two-step%2520approach%253A%2520first%252C%2520we%2520learn%2520an%2520intermediate%250Amodel%2520using%2520maximum%2520likelihood%2520by%2520treating%2520missing%2520values%2520as%2520a%2520special%2520value%250Aand%2520then%2520we%2520train%2520a%2520model%2520on%2520synthetic%2520data%2520generated%2520by%2520the%2520intermediate%2520model%250Ausing%2520the%2520EM%2520algorithm.%2520The%2520privacy%2520guarantees%2520of%2520our%2520approach%2520are%2520equivalent%250Ato%2520the%2520ones%2520provided%2520by%2520the%2520privacy%2520preserving%2520scalar%2520product%2520protocol%2520used.%2520We%250Aexperimentally%2520show%2520our%2520approach%2520produces%2520models%2520comparable%2520to%2520those%2520learnt%250Ausing%2520traditional%2520algorithms%2520and%2520we%2520estimate%2520the%2520increase%2520in%2520complexity%2520in%250Aterms%2520of%2520samples%252C%2520network%2520size%252C%2520and%2520complexity.%2520Finally%252C%2520we%2520propose%2520two%250Aalternative%2520approaches%2520to%2520estimate%2520the%2520performance%2520of%2520the%2520model%2520using%250Avertically%2520partitioned%2520data%2520and%2520we%2520show%2520in%2520experiments%2520that%2520they%2520lead%2520to%250Areasonably%2520accurate%2520estimates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.17228v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VertiBayes%3A%20Learning%20Bayesian%20network%20parameters%20from%20vertically%0A%20%20partitioned%20data%20with%20missing%20values&entry.906535625=Florian%20van%20Daalen%20and%20Lianne%20Ippel%20and%20Andre%20Dekker%20and%20Inigo%20Bermejo&entry.1292438233=%20%20Federated%20learning%20makes%20it%20possible%20to%20train%20a%20machine%20learning%20model%20on%0Adecentralized%20data.%20Bayesian%20networks%20are%20probabilistic%20graphical%20models%20that%0Ahave%20been%20widely%20used%20in%20artificial%20intelligence%20applications.%20Their%20popularity%0Astems%20from%20the%20fact%20they%20can%20be%20built%20by%20combining%20existing%20expert%20knowledge%0Awith%20data%20and%20are%20highly%20interpretable%2C%20which%20makes%20them%20useful%20for%20decision%0Asupport%2C%20e.g.%20in%20healthcare.%20While%20some%20research%20has%20been%20published%20on%20the%0Afederated%20learning%20of%20Bayesian%20networks%2C%20publications%20on%20Bayesian%20networks%20in%20a%0Avertically%20partitioned%20or%20heterogeneous%20data%20setting%20%28where%20different%20variables%0Aare%20located%20in%20different%20datasets%29%20are%20limited%2C%20and%20suffer%20from%20important%0Aomissions%2C%20such%20as%20the%20handling%20of%20missing%20data.%20In%20this%20article%2C%20we%20propose%20a%0Anovel%20method%20called%20VertiBayes%20to%20train%20Bayesian%20networks%20%28structure%20and%0Aparameters%29%20on%20vertically%20partitioned%20data%2C%20which%20can%20handle%20missing%20values%20as%0Awell%20as%20an%20arbitrary%20number%20of%20parties.%20For%20structure%20learning%20we%20adapted%20the%0Awidely%20used%20K2%20algorithm%20with%20a%20privacy-preserving%20scalar%20product%20protocol.%20For%0Aparameter%20learning%2C%20we%20use%20a%20two-step%20approach%3A%20first%2C%20we%20learn%20an%20intermediate%0Amodel%20using%20maximum%20likelihood%20by%20treating%20missing%20values%20as%20a%20special%20value%0Aand%20then%20we%20train%20a%20model%20on%20synthetic%20data%20generated%20by%20the%20intermediate%20model%0Ausing%20the%20EM%20algorithm.%20The%20privacy%20guarantees%20of%20our%20approach%20are%20equivalent%0Ato%20the%20ones%20provided%20by%20the%20privacy%20preserving%20scalar%20product%20protocol%20used.%20We%0Aexperimentally%20show%20our%20approach%20produces%20models%20comparable%20to%20those%20learnt%0Ausing%20traditional%20algorithms%20and%20we%20estimate%20the%20increase%20in%20complexity%20in%0Aterms%20of%20samples%2C%20network%20size%2C%20and%20complexity.%20Finally%2C%20we%20propose%20two%0Aalternative%20approaches%20to%20estimate%20the%20performance%20of%20the%20model%20using%0Avertically%20partitioned%20data%20and%20we%20show%20in%20experiments%20that%20they%20lead%20to%0Areasonably%20accurate%20estimates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.17228v2&entry.124074799=Read"},
{"title": "Adversarial Attacks and Defenses in Automated Control Systems: A\n  Comprehensive Benchmark", "author": "Vitaliy Pozdnyakov and Aleksandr Kovalenko and Ilya Makarov and Mikhail Drobyshevskiy and Kirill Lukyanov", "abstract": "  Integrating machine learning into Automated Control Systems (ACS) enhances\ndecision-making in industrial process management. One of the limitations to the\nwidespread adoption of these technologies in industry is the vulnerability of\nneural networks to adversarial attacks. This study explores the threats in\ndeploying deep learning models for fault diagnosis in ACS using the Tennessee\nEastman Process dataset. By evaluating three neural networks with different\narchitectures, we subject them to six types of adversarial attacks and explore\nfive different defense methods. Our results highlight the strong vulnerability\nof models to adversarial samples and the varying effectiveness of defense\nstrategies. We also propose a novel protection approach by combining multiple\ndefense methods and demonstrate it's efficacy. This research contributes\nseveral insights into securing machine learning within ACS, ensuring robust\nfault diagnosis in industrial processes.\n", "link": "http://arxiv.org/abs/2403.13502v3", "date": "2024-05-21", "relevancy": 1.8894, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4743}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4732}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Attacks%20and%20Defenses%20in%20Automated%20Control%20Systems%3A%20A%0A%20%20Comprehensive%20Benchmark&body=Title%3A%20Adversarial%20Attacks%20and%20Defenses%20in%20Automated%20Control%20Systems%3A%20A%0A%20%20Comprehensive%20Benchmark%0AAuthor%3A%20Vitaliy%20Pozdnyakov%20and%20Aleksandr%20Kovalenko%20and%20Ilya%20Makarov%20and%20Mikhail%20Drobyshevskiy%20and%20Kirill%20Lukyanov%0AAbstract%3A%20%20%20Integrating%20machine%20learning%20into%20Automated%20Control%20Systems%20%28ACS%29%20enhances%0Adecision-making%20in%20industrial%20process%20management.%20One%20of%20the%20limitations%20to%20the%0Awidespread%20adoption%20of%20these%20technologies%20in%20industry%20is%20the%20vulnerability%20of%0Aneural%20networks%20to%20adversarial%20attacks.%20This%20study%20explores%20the%20threats%20in%0Adeploying%20deep%20learning%20models%20for%20fault%20diagnosis%20in%20ACS%20using%20the%20Tennessee%0AEastman%20Process%20dataset.%20By%20evaluating%20three%20neural%20networks%20with%20different%0Aarchitectures%2C%20we%20subject%20them%20to%20six%20types%20of%20adversarial%20attacks%20and%20explore%0Afive%20different%20defense%20methods.%20Our%20results%20highlight%20the%20strong%20vulnerability%0Aof%20models%20to%20adversarial%20samples%20and%20the%20varying%20effectiveness%20of%20defense%0Astrategies.%20We%20also%20propose%20a%20novel%20protection%20approach%20by%20combining%20multiple%0Adefense%20methods%20and%20demonstrate%20it%27s%20efficacy.%20This%20research%20contributes%0Aseveral%20insights%20into%20securing%20machine%20learning%20within%20ACS%2C%20ensuring%20robust%0Afault%20diagnosis%20in%20industrial%20processes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13502v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Attacks%2520and%2520Defenses%2520in%2520Automated%2520Control%2520Systems%253A%2520A%250A%2520%2520Comprehensive%2520Benchmark%26entry.906535625%3DVitaliy%2520Pozdnyakov%2520and%2520Aleksandr%2520Kovalenko%2520and%2520Ilya%2520Makarov%2520and%2520Mikhail%2520Drobyshevskiy%2520and%2520Kirill%2520Lukyanov%26entry.1292438233%3D%2520%2520Integrating%2520machine%2520learning%2520into%2520Automated%2520Control%2520Systems%2520%2528ACS%2529%2520enhances%250Adecision-making%2520in%2520industrial%2520process%2520management.%2520One%2520of%2520the%2520limitations%2520to%2520the%250Awidespread%2520adoption%2520of%2520these%2520technologies%2520in%2520industry%2520is%2520the%2520vulnerability%2520of%250Aneural%2520networks%2520to%2520adversarial%2520attacks.%2520This%2520study%2520explores%2520the%2520threats%2520in%250Adeploying%2520deep%2520learning%2520models%2520for%2520fault%2520diagnosis%2520in%2520ACS%2520using%2520the%2520Tennessee%250AEastman%2520Process%2520dataset.%2520By%2520evaluating%2520three%2520neural%2520networks%2520with%2520different%250Aarchitectures%252C%2520we%2520subject%2520them%2520to%2520six%2520types%2520of%2520adversarial%2520attacks%2520and%2520explore%250Afive%2520different%2520defense%2520methods.%2520Our%2520results%2520highlight%2520the%2520strong%2520vulnerability%250Aof%2520models%2520to%2520adversarial%2520samples%2520and%2520the%2520varying%2520effectiveness%2520of%2520defense%250Astrategies.%2520We%2520also%2520propose%2520a%2520novel%2520protection%2520approach%2520by%2520combining%2520multiple%250Adefense%2520methods%2520and%2520demonstrate%2520it%2527s%2520efficacy.%2520This%2520research%2520contributes%250Aseveral%2520insights%2520into%2520securing%2520machine%2520learning%2520within%2520ACS%252C%2520ensuring%2520robust%250Afault%2520diagnosis%2520in%2520industrial%2520processes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13502v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Attacks%20and%20Defenses%20in%20Automated%20Control%20Systems%3A%20A%0A%20%20Comprehensive%20Benchmark&entry.906535625=Vitaliy%20Pozdnyakov%20and%20Aleksandr%20Kovalenko%20and%20Ilya%20Makarov%20and%20Mikhail%20Drobyshevskiy%20and%20Kirill%20Lukyanov&entry.1292438233=%20%20Integrating%20machine%20learning%20into%20Automated%20Control%20Systems%20%28ACS%29%20enhances%0Adecision-making%20in%20industrial%20process%20management.%20One%20of%20the%20limitations%20to%20the%0Awidespread%20adoption%20of%20these%20technologies%20in%20industry%20is%20the%20vulnerability%20of%0Aneural%20networks%20to%20adversarial%20attacks.%20This%20study%20explores%20the%20threats%20in%0Adeploying%20deep%20learning%20models%20for%20fault%20diagnosis%20in%20ACS%20using%20the%20Tennessee%0AEastman%20Process%20dataset.%20By%20evaluating%20three%20neural%20networks%20with%20different%0Aarchitectures%2C%20we%20subject%20them%20to%20six%20types%20of%20adversarial%20attacks%20and%20explore%0Afive%20different%20defense%20methods.%20Our%20results%20highlight%20the%20strong%20vulnerability%0Aof%20models%20to%20adversarial%20samples%20and%20the%20varying%20effectiveness%20of%20defense%0Astrategies.%20We%20also%20propose%20a%20novel%20protection%20approach%20by%20combining%20multiple%0Adefense%20methods%20and%20demonstrate%20it%27s%20efficacy.%20This%20research%20contributes%0Aseveral%20insights%20into%20securing%20machine%20learning%20within%20ACS%2C%20ensuring%20robust%0Afault%20diagnosis%20in%20industrial%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13502v3&entry.124074799=Read"},
{"title": "Large Language Models Meet NLP: A Survey", "author": "Libo Qin and Qiguang Chen and Xiachong Feng and Yang Wu and Yongheng Zhang and Yinghui Li and Min Li and Wanxiang Che and Philip S. Yu", "abstract": "  While large language models (LLMs) like ChatGPT have shown impressive\ncapabilities in Natural Language Processing (NLP) tasks, a systematic\ninvestigation of their potential in this field remains largely unexplored. This\nstudy aims to address this gap by exploring the following questions: (1) How\nare LLMs currently applied to NLP tasks in the literature? (2) Have traditional\nNLP tasks already been solved with LLMs? (3) What is the future of the LLMs for\nNLP? To answer these questions, we take the first step to provide a\ncomprehensive overview of LLMs in NLP. Specifically, we first introduce a\nunified taxonomy including (1) parameter-frozen application and (2)\nparameter-tuning application to offer a unified perspective for understanding\nthe current progress of LLMs in NLP. Furthermore, we summarize the new\nfrontiers and the associated challenges, aiming to inspire further\ngroundbreaking advancements. We hope this work offers valuable insights into\nthe {potential and limitations} of LLMs in NLP, while also serving as a\npractical guide for building effective LLMs in NLP.\n", "link": "http://arxiv.org/abs/2405.12819v1", "date": "2024-05-21", "relevancy": 1.8784, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4778}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4652}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Meet%20NLP%3A%20A%20Survey&body=Title%3A%20Large%20Language%20Models%20Meet%20NLP%3A%20A%20Survey%0AAuthor%3A%20Libo%20Qin%20and%20Qiguang%20Chen%20and%20Xiachong%20Feng%20and%20Yang%20Wu%20and%20Yongheng%20Zhang%20and%20Yinghui%20Li%20and%20Min%20Li%20and%20Wanxiang%20Che%20and%20Philip%20S.%20Yu%0AAbstract%3A%20%20%20While%20large%20language%20models%20%28LLMs%29%20like%20ChatGPT%20have%20shown%20impressive%0Acapabilities%20in%20Natural%20Language%20Processing%20%28NLP%29%20tasks%2C%20a%20systematic%0Ainvestigation%20of%20their%20potential%20in%20this%20field%20remains%20largely%20unexplored.%20This%0Astudy%20aims%20to%20address%20this%20gap%20by%20exploring%20the%20following%20questions%3A%20%281%29%20How%0Aare%20LLMs%20currently%20applied%20to%20NLP%20tasks%20in%20the%20literature%3F%20%282%29%20Have%20traditional%0ANLP%20tasks%20already%20been%20solved%20with%20LLMs%3F%20%283%29%20What%20is%20the%20future%20of%20the%20LLMs%20for%0ANLP%3F%20To%20answer%20these%20questions%2C%20we%20take%20the%20first%20step%20to%20provide%20a%0Acomprehensive%20overview%20of%20LLMs%20in%20NLP.%20Specifically%2C%20we%20first%20introduce%20a%0Aunified%20taxonomy%20including%20%281%29%20parameter-frozen%20application%20and%20%282%29%0Aparameter-tuning%20application%20to%20offer%20a%20unified%20perspective%20for%20understanding%0Athe%20current%20progress%20of%20LLMs%20in%20NLP.%20Furthermore%2C%20we%20summarize%20the%20new%0Afrontiers%20and%20the%20associated%20challenges%2C%20aiming%20to%20inspire%20further%0Agroundbreaking%20advancements.%20We%20hope%20this%20work%20offers%20valuable%20insights%20into%0Athe%20%7Bpotential%20and%20limitations%7D%20of%20LLMs%20in%20NLP%2C%20while%20also%20serving%20as%20a%0Apractical%20guide%20for%20building%20effective%20LLMs%20in%20NLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Meet%2520NLP%253A%2520A%2520Survey%26entry.906535625%3DLibo%2520Qin%2520and%2520Qiguang%2520Chen%2520and%2520Xiachong%2520Feng%2520and%2520Yang%2520Wu%2520and%2520Yongheng%2520Zhang%2520and%2520Yinghui%2520Li%2520and%2520Min%2520Li%2520and%2520Wanxiang%2520Che%2520and%2520Philip%2520S.%2520Yu%26entry.1292438233%3D%2520%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520like%2520ChatGPT%2520have%2520shown%2520impressive%250Acapabilities%2520in%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520tasks%252C%2520a%2520systematic%250Ainvestigation%2520of%2520their%2520potential%2520in%2520this%2520field%2520remains%2520largely%2520unexplored.%2520This%250Astudy%2520aims%2520to%2520address%2520this%2520gap%2520by%2520exploring%2520the%2520following%2520questions%253A%2520%25281%2529%2520How%250Aare%2520LLMs%2520currently%2520applied%2520to%2520NLP%2520tasks%2520in%2520the%2520literature%253F%2520%25282%2529%2520Have%2520traditional%250ANLP%2520tasks%2520already%2520been%2520solved%2520with%2520LLMs%253F%2520%25283%2529%2520What%2520is%2520the%2520future%2520of%2520the%2520LLMs%2520for%250ANLP%253F%2520To%2520answer%2520these%2520questions%252C%2520we%2520take%2520the%2520first%2520step%2520to%2520provide%2520a%250Acomprehensive%2520overview%2520of%2520LLMs%2520in%2520NLP.%2520Specifically%252C%2520we%2520first%2520introduce%2520a%250Aunified%2520taxonomy%2520including%2520%25281%2529%2520parameter-frozen%2520application%2520and%2520%25282%2529%250Aparameter-tuning%2520application%2520to%2520offer%2520a%2520unified%2520perspective%2520for%2520understanding%250Athe%2520current%2520progress%2520of%2520LLMs%2520in%2520NLP.%2520Furthermore%252C%2520we%2520summarize%2520the%2520new%250Afrontiers%2520and%2520the%2520associated%2520challenges%252C%2520aiming%2520to%2520inspire%2520further%250Agroundbreaking%2520advancements.%2520We%2520hope%2520this%2520work%2520offers%2520valuable%2520insights%2520into%250Athe%2520%257Bpotential%2520and%2520limitations%257D%2520of%2520LLMs%2520in%2520NLP%252C%2520while%2520also%2520serving%2520as%2520a%250Apractical%2520guide%2520for%2520building%2520effective%2520LLMs%2520in%2520NLP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Meet%20NLP%3A%20A%20Survey&entry.906535625=Libo%20Qin%20and%20Qiguang%20Chen%20and%20Xiachong%20Feng%20and%20Yang%20Wu%20and%20Yongheng%20Zhang%20and%20Yinghui%20Li%20and%20Min%20Li%20and%20Wanxiang%20Che%20and%20Philip%20S.%20Yu&entry.1292438233=%20%20While%20large%20language%20models%20%28LLMs%29%20like%20ChatGPT%20have%20shown%20impressive%0Acapabilities%20in%20Natural%20Language%20Processing%20%28NLP%29%20tasks%2C%20a%20systematic%0Ainvestigation%20of%20their%20potential%20in%20this%20field%20remains%20largely%20unexplored.%20This%0Astudy%20aims%20to%20address%20this%20gap%20by%20exploring%20the%20following%20questions%3A%20%281%29%20How%0Aare%20LLMs%20currently%20applied%20to%20NLP%20tasks%20in%20the%20literature%3F%20%282%29%20Have%20traditional%0ANLP%20tasks%20already%20been%20solved%20with%20LLMs%3F%20%283%29%20What%20is%20the%20future%20of%20the%20LLMs%20for%0ANLP%3F%20To%20answer%20these%20questions%2C%20we%20take%20the%20first%20step%20to%20provide%20a%0Acomprehensive%20overview%20of%20LLMs%20in%20NLP.%20Specifically%2C%20we%20first%20introduce%20a%0Aunified%20taxonomy%20including%20%281%29%20parameter-frozen%20application%20and%20%282%29%0Aparameter-tuning%20application%20to%20offer%20a%20unified%20perspective%20for%20understanding%0Athe%20current%20progress%20of%20LLMs%20in%20NLP.%20Furthermore%2C%20we%20summarize%20the%20new%0Afrontiers%20and%20the%20associated%20challenges%2C%20aiming%20to%20inspire%20further%0Agroundbreaking%20advancements.%20We%20hope%20this%20work%20offers%20valuable%20insights%20into%0Athe%20%7Bpotential%20and%20limitations%7D%20of%20LLMs%20in%20NLP%2C%20while%20also%20serving%20as%20a%0Apractical%20guide%20for%20building%20effective%20LLMs%20in%20NLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12819v1&entry.124074799=Read"},
{"title": "Stochastic Inference of Plate Bending from Heterogeneous Data:\n  Physics-informed Gaussian Processes via Kirchhoff-Love Theory", "author": "Igor Kavrakov and Gledson Rodrigo Tondo and Guido Morgenthal", "abstract": "  Advancements in machine learning and an abundance of structural monitoring\ndata have inspired the integration of mechanical models with probabilistic\nmodels to identify a structure's state and quantify the uncertainty of its\nphysical parameters and response. In this paper, we propose an inference\nmethodology for classical Kirchhoff-Love plates via physics-informed Gaussian\nProcesses (GP). A probabilistic model is formulated as a multi-output GP by\nplacing a GP prior on the deflection and deriving the covariance function using\nthe linear differential operators of the plate governing equations. The\nposteriors of the flexural rigidity, hyperparameters, and plate response are\ninferred in a Bayesian manner using Markov chain Monte Carlo (MCMC) sampling\nfrom noisy measurements. We demonstrate the applicability with two examples: a\nsimply supported plate subjected to a sinusoidal load and a fixed plate\nsubjected to a uniform load. The results illustrate how the proposed\nmethodology can be employed to perform stochastic inference for plate rigidity\nand physical quantities by integrating measurements from various sensor types\nand qualities. Potential applications of the presented methodology are in\nstructural health monitoring and uncertainty quantification of plate-like\nstructures.\n", "link": "http://arxiv.org/abs/2405.12802v1", "date": "2024-05-21", "relevancy": 1.8774, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5352}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4784}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20Inference%20of%20Plate%20Bending%20from%20Heterogeneous%20Data%3A%0A%20%20Physics-informed%20Gaussian%20Processes%20via%20Kirchhoff-Love%20Theory&body=Title%3A%20Stochastic%20Inference%20of%20Plate%20Bending%20from%20Heterogeneous%20Data%3A%0A%20%20Physics-informed%20Gaussian%20Processes%20via%20Kirchhoff-Love%20Theory%0AAuthor%3A%20Igor%20Kavrakov%20and%20Gledson%20Rodrigo%20Tondo%20and%20Guido%20Morgenthal%0AAbstract%3A%20%20%20Advancements%20in%20machine%20learning%20and%20an%20abundance%20of%20structural%20monitoring%0Adata%20have%20inspired%20the%20integration%20of%20mechanical%20models%20with%20probabilistic%0Amodels%20to%20identify%20a%20structure%27s%20state%20and%20quantify%20the%20uncertainty%20of%20its%0Aphysical%20parameters%20and%20response.%20In%20this%20paper%2C%20we%20propose%20an%20inference%0Amethodology%20for%20classical%20Kirchhoff-Love%20plates%20via%20physics-informed%20Gaussian%0AProcesses%20%28GP%29.%20A%20probabilistic%20model%20is%20formulated%20as%20a%20multi-output%20GP%20by%0Aplacing%20a%20GP%20prior%20on%20the%20deflection%20and%20deriving%20the%20covariance%20function%20using%0Athe%20linear%20differential%20operators%20of%20the%20plate%20governing%20equations.%20The%0Aposteriors%20of%20the%20flexural%20rigidity%2C%20hyperparameters%2C%20and%20plate%20response%20are%0Ainferred%20in%20a%20Bayesian%20manner%20using%20Markov%20chain%20Monte%20Carlo%20%28MCMC%29%20sampling%0Afrom%20noisy%20measurements.%20We%20demonstrate%20the%20applicability%20with%20two%20examples%3A%20a%0Asimply%20supported%20plate%20subjected%20to%20a%20sinusoidal%20load%20and%20a%20fixed%20plate%0Asubjected%20to%20a%20uniform%20load.%20The%20results%20illustrate%20how%20the%20proposed%0Amethodology%20can%20be%20employed%20to%20perform%20stochastic%20inference%20for%20plate%20rigidity%0Aand%20physical%20quantities%20by%20integrating%20measurements%20from%20various%20sensor%20types%0Aand%20qualities.%20Potential%20applications%20of%20the%20presented%20methodology%20are%20in%0Astructural%20health%20monitoring%20and%20uncertainty%20quantification%20of%20plate-like%0Astructures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520Inference%2520of%2520Plate%2520Bending%2520from%2520Heterogeneous%2520Data%253A%250A%2520%2520Physics-informed%2520Gaussian%2520Processes%2520via%2520Kirchhoff-Love%2520Theory%26entry.906535625%3DIgor%2520Kavrakov%2520and%2520Gledson%2520Rodrigo%2520Tondo%2520and%2520Guido%2520Morgenthal%26entry.1292438233%3D%2520%2520Advancements%2520in%2520machine%2520learning%2520and%2520an%2520abundance%2520of%2520structural%2520monitoring%250Adata%2520have%2520inspired%2520the%2520integration%2520of%2520mechanical%2520models%2520with%2520probabilistic%250Amodels%2520to%2520identify%2520a%2520structure%2527s%2520state%2520and%2520quantify%2520the%2520uncertainty%2520of%2520its%250Aphysical%2520parameters%2520and%2520response.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520inference%250Amethodology%2520for%2520classical%2520Kirchhoff-Love%2520plates%2520via%2520physics-informed%2520Gaussian%250AProcesses%2520%2528GP%2529.%2520A%2520probabilistic%2520model%2520is%2520formulated%2520as%2520a%2520multi-output%2520GP%2520by%250Aplacing%2520a%2520GP%2520prior%2520on%2520the%2520deflection%2520and%2520deriving%2520the%2520covariance%2520function%2520using%250Athe%2520linear%2520differential%2520operators%2520of%2520the%2520plate%2520governing%2520equations.%2520The%250Aposteriors%2520of%2520the%2520flexural%2520rigidity%252C%2520hyperparameters%252C%2520and%2520plate%2520response%2520are%250Ainferred%2520in%2520a%2520Bayesian%2520manner%2520using%2520Markov%2520chain%2520Monte%2520Carlo%2520%2528MCMC%2529%2520sampling%250Afrom%2520noisy%2520measurements.%2520We%2520demonstrate%2520the%2520applicability%2520with%2520two%2520examples%253A%2520a%250Asimply%2520supported%2520plate%2520subjected%2520to%2520a%2520sinusoidal%2520load%2520and%2520a%2520fixed%2520plate%250Asubjected%2520to%2520a%2520uniform%2520load.%2520The%2520results%2520illustrate%2520how%2520the%2520proposed%250Amethodology%2520can%2520be%2520employed%2520to%2520perform%2520stochastic%2520inference%2520for%2520plate%2520rigidity%250Aand%2520physical%2520quantities%2520by%2520integrating%2520measurements%2520from%2520various%2520sensor%2520types%250Aand%2520qualities.%2520Potential%2520applications%2520of%2520the%2520presented%2520methodology%2520are%2520in%250Astructural%2520health%2520monitoring%2520and%2520uncertainty%2520quantification%2520of%2520plate-like%250Astructures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Inference%20of%20Plate%20Bending%20from%20Heterogeneous%20Data%3A%0A%20%20Physics-informed%20Gaussian%20Processes%20via%20Kirchhoff-Love%20Theory&entry.906535625=Igor%20Kavrakov%20and%20Gledson%20Rodrigo%20Tondo%20and%20Guido%20Morgenthal&entry.1292438233=%20%20Advancements%20in%20machine%20learning%20and%20an%20abundance%20of%20structural%20monitoring%0Adata%20have%20inspired%20the%20integration%20of%20mechanical%20models%20with%20probabilistic%0Amodels%20to%20identify%20a%20structure%27s%20state%20and%20quantify%20the%20uncertainty%20of%20its%0Aphysical%20parameters%20and%20response.%20In%20this%20paper%2C%20we%20propose%20an%20inference%0Amethodology%20for%20classical%20Kirchhoff-Love%20plates%20via%20physics-informed%20Gaussian%0AProcesses%20%28GP%29.%20A%20probabilistic%20model%20is%20formulated%20as%20a%20multi-output%20GP%20by%0Aplacing%20a%20GP%20prior%20on%20the%20deflection%20and%20deriving%20the%20covariance%20function%20using%0Athe%20linear%20differential%20operators%20of%20the%20plate%20governing%20equations.%20The%0Aposteriors%20of%20the%20flexural%20rigidity%2C%20hyperparameters%2C%20and%20plate%20response%20are%0Ainferred%20in%20a%20Bayesian%20manner%20using%20Markov%20chain%20Monte%20Carlo%20%28MCMC%29%20sampling%0Afrom%20noisy%20measurements.%20We%20demonstrate%20the%20applicability%20with%20two%20examples%3A%20a%0Asimply%20supported%20plate%20subjected%20to%20a%20sinusoidal%20load%20and%20a%20fixed%20plate%0Asubjected%20to%20a%20uniform%20load.%20The%20results%20illustrate%20how%20the%20proposed%0Amethodology%20can%20be%20employed%20to%20perform%20stochastic%20inference%20for%20plate%20rigidity%0Aand%20physical%20quantities%20by%20integrating%20measurements%20from%20various%20sensor%20types%0Aand%20qualities.%20Potential%20applications%20of%20the%20presented%20methodology%20are%20in%0Astructural%20health%20monitoring%20and%20uncertainty%20quantification%20of%20plate-like%0Astructures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12802v1&entry.124074799=Read"},
{"title": "Retrievable Domain-Sensitive Feature Memory for Multi-Domain\n  Recommendation", "author": "Yuang Zhao and Zhaocheng Du and Qinglin Jia and Linxuan Zhang and Zhenhua Dong and Ruiming Tang", "abstract": "  With the increase in the business scale and number of domains in online\nadvertising, multi-domain ad recommendation has become a mainstream solution in\nthe industry. The core of multi-domain recommendation is effectively modeling\nthe commonalities and distinctions among domains. Existing works are dedicated\nto designing model architectures for implicit multi-domain modeling while\noverlooking an in-depth investigation from a more fundamental perspective of\nfeature distributions. This paper focuses on features with significant\ndifferences across various domains in both distributions and effects on model\npredictions. We refer to these features as domain-sensitive features, which\nserve as carriers of domain distinctions and are crucial for multi-domain\nmodeling. Experiments demonstrate that existing multi-domain modeling methods\nmay neglect domain-sensitive features, indicating insufficient learning of\ndomain distinctions. To avoid this neglect, we propose a domain-sensitive\nfeature attribution method to identify features that best reflect domain\ndistinctions from the feature set. Further, we design a memory architecture\nthat extracts domain-specific information from domain-sensitive features for\nthe model to retrieve and integrate, thereby enhancing the awareness of domain\ndistinctions. Extensive offline and online experiments demonstrate the\nsuperiority of our method in capturing domain distinctions and improving\nmulti-domain recommendation performance.\n", "link": "http://arxiv.org/abs/2405.12892v1", "date": "2024-05-21", "relevancy": 1.874, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4787}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4681}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrievable%20Domain-Sensitive%20Feature%20Memory%20for%20Multi-Domain%0A%20%20Recommendation&body=Title%3A%20Retrievable%20Domain-Sensitive%20Feature%20Memory%20for%20Multi-Domain%0A%20%20Recommendation%0AAuthor%3A%20Yuang%20Zhao%20and%20Zhaocheng%20Du%20and%20Qinglin%20Jia%20and%20Linxuan%20Zhang%20and%20Zhenhua%20Dong%20and%20Ruiming%20Tang%0AAbstract%3A%20%20%20With%20the%20increase%20in%20the%20business%20scale%20and%20number%20of%20domains%20in%20online%0Aadvertising%2C%20multi-domain%20ad%20recommendation%20has%20become%20a%20mainstream%20solution%20in%0Athe%20industry.%20The%20core%20of%20multi-domain%20recommendation%20is%20effectively%20modeling%0Athe%20commonalities%20and%20distinctions%20among%20domains.%20Existing%20works%20are%20dedicated%0Ato%20designing%20model%20architectures%20for%20implicit%20multi-domain%20modeling%20while%0Aoverlooking%20an%20in-depth%20investigation%20from%20a%20more%20fundamental%20perspective%20of%0Afeature%20distributions.%20This%20paper%20focuses%20on%20features%20with%20significant%0Adifferences%20across%20various%20domains%20in%20both%20distributions%20and%20effects%20on%20model%0Apredictions.%20We%20refer%20to%20these%20features%20as%20domain-sensitive%20features%2C%20which%0Aserve%20as%20carriers%20of%20domain%20distinctions%20and%20are%20crucial%20for%20multi-domain%0Amodeling.%20Experiments%20demonstrate%20that%20existing%20multi-domain%20modeling%20methods%0Amay%20neglect%20domain-sensitive%20features%2C%20indicating%20insufficient%20learning%20of%0Adomain%20distinctions.%20To%20avoid%20this%20neglect%2C%20we%20propose%20a%20domain-sensitive%0Afeature%20attribution%20method%20to%20identify%20features%20that%20best%20reflect%20domain%0Adistinctions%20from%20the%20feature%20set.%20Further%2C%20we%20design%20a%20memory%20architecture%0Athat%20extracts%20domain-specific%20information%20from%20domain-sensitive%20features%20for%0Athe%20model%20to%20retrieve%20and%20integrate%2C%20thereby%20enhancing%20the%20awareness%20of%20domain%0Adistinctions.%20Extensive%20offline%20and%20online%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20method%20in%20capturing%20domain%20distinctions%20and%20improving%0Amulti-domain%20recommendation%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrievable%2520Domain-Sensitive%2520Feature%2520Memory%2520for%2520Multi-Domain%250A%2520%2520Recommendation%26entry.906535625%3DYuang%2520Zhao%2520and%2520Zhaocheng%2520Du%2520and%2520Qinglin%2520Jia%2520and%2520Linxuan%2520Zhang%2520and%2520Zhenhua%2520Dong%2520and%2520Ruiming%2520Tang%26entry.1292438233%3D%2520%2520With%2520the%2520increase%2520in%2520the%2520business%2520scale%2520and%2520number%2520of%2520domains%2520in%2520online%250Aadvertising%252C%2520multi-domain%2520ad%2520recommendation%2520has%2520become%2520a%2520mainstream%2520solution%2520in%250Athe%2520industry.%2520The%2520core%2520of%2520multi-domain%2520recommendation%2520is%2520effectively%2520modeling%250Athe%2520commonalities%2520and%2520distinctions%2520among%2520domains.%2520Existing%2520works%2520are%2520dedicated%250Ato%2520designing%2520model%2520architectures%2520for%2520implicit%2520multi-domain%2520modeling%2520while%250Aoverlooking%2520an%2520in-depth%2520investigation%2520from%2520a%2520more%2520fundamental%2520perspective%2520of%250Afeature%2520distributions.%2520This%2520paper%2520focuses%2520on%2520features%2520with%2520significant%250Adifferences%2520across%2520various%2520domains%2520in%2520both%2520distributions%2520and%2520effects%2520on%2520model%250Apredictions.%2520We%2520refer%2520to%2520these%2520features%2520as%2520domain-sensitive%2520features%252C%2520which%250Aserve%2520as%2520carriers%2520of%2520domain%2520distinctions%2520and%2520are%2520crucial%2520for%2520multi-domain%250Amodeling.%2520Experiments%2520demonstrate%2520that%2520existing%2520multi-domain%2520modeling%2520methods%250Amay%2520neglect%2520domain-sensitive%2520features%252C%2520indicating%2520insufficient%2520learning%2520of%250Adomain%2520distinctions.%2520To%2520avoid%2520this%2520neglect%252C%2520we%2520propose%2520a%2520domain-sensitive%250Afeature%2520attribution%2520method%2520to%2520identify%2520features%2520that%2520best%2520reflect%2520domain%250Adistinctions%2520from%2520the%2520feature%2520set.%2520Further%252C%2520we%2520design%2520a%2520memory%2520architecture%250Athat%2520extracts%2520domain-specific%2520information%2520from%2520domain-sensitive%2520features%2520for%250Athe%2520model%2520to%2520retrieve%2520and%2520integrate%252C%2520thereby%2520enhancing%2520the%2520awareness%2520of%2520domain%250Adistinctions.%2520Extensive%2520offline%2520and%2520online%2520experiments%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520method%2520in%2520capturing%2520domain%2520distinctions%2520and%2520improving%250Amulti-domain%2520recommendation%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrievable%20Domain-Sensitive%20Feature%20Memory%20for%20Multi-Domain%0A%20%20Recommendation&entry.906535625=Yuang%20Zhao%20and%20Zhaocheng%20Du%20and%20Qinglin%20Jia%20and%20Linxuan%20Zhang%20and%20Zhenhua%20Dong%20and%20Ruiming%20Tang&entry.1292438233=%20%20With%20the%20increase%20in%20the%20business%20scale%20and%20number%20of%20domains%20in%20online%0Aadvertising%2C%20multi-domain%20ad%20recommendation%20has%20become%20a%20mainstream%20solution%20in%0Athe%20industry.%20The%20core%20of%20multi-domain%20recommendation%20is%20effectively%20modeling%0Athe%20commonalities%20and%20distinctions%20among%20domains.%20Existing%20works%20are%20dedicated%0Ato%20designing%20model%20architectures%20for%20implicit%20multi-domain%20modeling%20while%0Aoverlooking%20an%20in-depth%20investigation%20from%20a%20more%20fundamental%20perspective%20of%0Afeature%20distributions.%20This%20paper%20focuses%20on%20features%20with%20significant%0Adifferences%20across%20various%20domains%20in%20both%20distributions%20and%20effects%20on%20model%0Apredictions.%20We%20refer%20to%20these%20features%20as%20domain-sensitive%20features%2C%20which%0Aserve%20as%20carriers%20of%20domain%20distinctions%20and%20are%20crucial%20for%20multi-domain%0Amodeling.%20Experiments%20demonstrate%20that%20existing%20multi-domain%20modeling%20methods%0Amay%20neglect%20domain-sensitive%20features%2C%20indicating%20insufficient%20learning%20of%0Adomain%20distinctions.%20To%20avoid%20this%20neglect%2C%20we%20propose%20a%20domain-sensitive%0Afeature%20attribution%20method%20to%20identify%20features%20that%20best%20reflect%20domain%0Adistinctions%20from%20the%20feature%20set.%20Further%2C%20we%20design%20a%20memory%20architecture%0Athat%20extracts%20domain-specific%20information%20from%20domain-sensitive%20features%20for%0Athe%20model%20to%20retrieve%20and%20integrate%2C%20thereby%20enhancing%20the%20awareness%20of%20domain%0Adistinctions.%20Extensive%20offline%20and%20online%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20method%20in%20capturing%20domain%20distinctions%20and%20improving%0Amulti-domain%20recommendation%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12892v1&entry.124074799=Read"},
{"title": "Personalized Residuals for Concept-Driven Text-to-Image Generation", "author": "Cusuh Ham and Matthew Fisher and James Hays and Nicholas Kolkin and Yuchen Liu and Richard Zhang and Tobias Hinz", "abstract": "  We present personalized residuals and localized attention-guided sampling for\nefficient concept-driven generation using text-to-image diffusion models. Our\nmethod first represents concepts by freezing the weights of a pretrained\ntext-conditioned diffusion model and learning low-rank residuals for a small\nsubset of the model's layers. The residual-based approach then directly enables\napplication of our proposed sampling technique, which applies the learned\nresiduals only in areas where the concept is localized via cross-attention and\napplies the original diffusion weights in all other regions. Localized sampling\ntherefore combines the learned identity of the concept with the existing\ngenerative prior of the underlying diffusion model. We show that personalized\nresiduals effectively capture the identity of a concept in ~3 minutes on a\nsingle GPU without the use of regularization images and with fewer parameters\nthan previous models, and localized sampling allows using the original model as\nstrong prior for large parts of the image.\n", "link": "http://arxiv.org/abs/2405.12978v1", "date": "2024-05-21", "relevancy": 1.8628, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6324}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6158}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Residuals%20for%20Concept-Driven%20Text-to-Image%20Generation&body=Title%3A%20Personalized%20Residuals%20for%20Concept-Driven%20Text-to-Image%20Generation%0AAuthor%3A%20Cusuh%20Ham%20and%20Matthew%20Fisher%20and%20James%20Hays%20and%20Nicholas%20Kolkin%20and%20Yuchen%20Liu%20and%20Richard%20Zhang%20and%20Tobias%20Hinz%0AAbstract%3A%20%20%20We%20present%20personalized%20residuals%20and%20localized%20attention-guided%20sampling%20for%0Aefficient%20concept-driven%20generation%20using%20text-to-image%20diffusion%20models.%20Our%0Amethod%20first%20represents%20concepts%20by%20freezing%20the%20weights%20of%20a%20pretrained%0Atext-conditioned%20diffusion%20model%20and%20learning%20low-rank%20residuals%20for%20a%20small%0Asubset%20of%20the%20model%27s%20layers.%20The%20residual-based%20approach%20then%20directly%20enables%0Aapplication%20of%20our%20proposed%20sampling%20technique%2C%20which%20applies%20the%20learned%0Aresiduals%20only%20in%20areas%20where%20the%20concept%20is%20localized%20via%20cross-attention%20and%0Aapplies%20the%20original%20diffusion%20weights%20in%20all%20other%20regions.%20Localized%20sampling%0Atherefore%20combines%20the%20learned%20identity%20of%20the%20concept%20with%20the%20existing%0Agenerative%20prior%20of%20the%20underlying%20diffusion%20model.%20We%20show%20that%20personalized%0Aresiduals%20effectively%20capture%20the%20identity%20of%20a%20concept%20in%20~3%20minutes%20on%20a%0Asingle%20GPU%20without%20the%20use%20of%20regularization%20images%20and%20with%20fewer%20parameters%0Athan%20previous%20models%2C%20and%20localized%20sampling%20allows%20using%20the%20original%20model%20as%0Astrong%20prior%20for%20large%20parts%20of%20the%20image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Residuals%2520for%2520Concept-Driven%2520Text-to-Image%2520Generation%26entry.906535625%3DCusuh%2520Ham%2520and%2520Matthew%2520Fisher%2520and%2520James%2520Hays%2520and%2520Nicholas%2520Kolkin%2520and%2520Yuchen%2520Liu%2520and%2520Richard%2520Zhang%2520and%2520Tobias%2520Hinz%26entry.1292438233%3D%2520%2520We%2520present%2520personalized%2520residuals%2520and%2520localized%2520attention-guided%2520sampling%2520for%250Aefficient%2520concept-driven%2520generation%2520using%2520text-to-image%2520diffusion%2520models.%2520Our%250Amethod%2520first%2520represents%2520concepts%2520by%2520freezing%2520the%2520weights%2520of%2520a%2520pretrained%250Atext-conditioned%2520diffusion%2520model%2520and%2520learning%2520low-rank%2520residuals%2520for%2520a%2520small%250Asubset%2520of%2520the%2520model%2527s%2520layers.%2520The%2520residual-based%2520approach%2520then%2520directly%2520enables%250Aapplication%2520of%2520our%2520proposed%2520sampling%2520technique%252C%2520which%2520applies%2520the%2520learned%250Aresiduals%2520only%2520in%2520areas%2520where%2520the%2520concept%2520is%2520localized%2520via%2520cross-attention%2520and%250Aapplies%2520the%2520original%2520diffusion%2520weights%2520in%2520all%2520other%2520regions.%2520Localized%2520sampling%250Atherefore%2520combines%2520the%2520learned%2520identity%2520of%2520the%2520concept%2520with%2520the%2520existing%250Agenerative%2520prior%2520of%2520the%2520underlying%2520diffusion%2520model.%2520We%2520show%2520that%2520personalized%250Aresiduals%2520effectively%2520capture%2520the%2520identity%2520of%2520a%2520concept%2520in%2520~3%2520minutes%2520on%2520a%250Asingle%2520GPU%2520without%2520the%2520use%2520of%2520regularization%2520images%2520and%2520with%2520fewer%2520parameters%250Athan%2520previous%2520models%252C%2520and%2520localized%2520sampling%2520allows%2520using%2520the%2520original%2520model%2520as%250Astrong%2520prior%2520for%2520large%2520parts%2520of%2520the%2520image.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Residuals%20for%20Concept-Driven%20Text-to-Image%20Generation&entry.906535625=Cusuh%20Ham%20and%20Matthew%20Fisher%20and%20James%20Hays%20and%20Nicholas%20Kolkin%20and%20Yuchen%20Liu%20and%20Richard%20Zhang%20and%20Tobias%20Hinz&entry.1292438233=%20%20We%20present%20personalized%20residuals%20and%20localized%20attention-guided%20sampling%20for%0Aefficient%20concept-driven%20generation%20using%20text-to-image%20diffusion%20models.%20Our%0Amethod%20first%20represents%20concepts%20by%20freezing%20the%20weights%20of%20a%20pretrained%0Atext-conditioned%20diffusion%20model%20and%20learning%20low-rank%20residuals%20for%20a%20small%0Asubset%20of%20the%20model%27s%20layers.%20The%20residual-based%20approach%20then%20directly%20enables%0Aapplication%20of%20our%20proposed%20sampling%20technique%2C%20which%20applies%20the%20learned%0Aresiduals%20only%20in%20areas%20where%20the%20concept%20is%20localized%20via%20cross-attention%20and%0Aapplies%20the%20original%20diffusion%20weights%20in%20all%20other%20regions.%20Localized%20sampling%0Atherefore%20combines%20the%20learned%20identity%20of%20the%20concept%20with%20the%20existing%0Agenerative%20prior%20of%20the%20underlying%20diffusion%20model.%20We%20show%20that%20personalized%0Aresiduals%20effectively%20capture%20the%20identity%20of%20a%20concept%20in%20~3%20minutes%20on%20a%0Asingle%20GPU%20without%20the%20use%20of%20regularization%20images%20and%20with%20fewer%20parameters%0Athan%20previous%20models%2C%20and%20localized%20sampling%20allows%20using%20the%20original%20model%20as%0Astrong%20prior%20for%20large%20parts%20of%20the%20image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12978v1&entry.124074799=Read"},
{"title": "Reinforcement Learning with Model Predictive Control for Highway Ramp\n  Metering", "author": "Filippo Airaldi and Bart De Schutter and Azita Dabiri", "abstract": "  In the backdrop of an increasingly pressing need for effective urban and\nhighway transportation systems, this work explores the synergy between\nmodel-based and learning-based strategies to enhance traffic flow management by\nuse of an innovative approach to the problem of ramp metering control that\nembeds Reinforcement Learning (RL) techniques within the Model Predictive\nControl (MPC) framework. The control problem is formulated as an RL task by\ncrafting a suitable stage cost function that is representative of the traffic\nconditions, variability in the control action, and violations of the constraint\non the maximum number of vehicles in queue. An MPC-based RL approach, which\nleverages the MPC optimal problem as a function approximation for the RL\nalgorithm, is proposed to learn to efficiently control an on-ramp and satisfy\nits constraints despite uncertainties in the system model and variable demands.\nSimulations are performed on a benchmark small-scale highway network to compare\nthe proposed methodology against other state-of-the-art control approaches.\nResults show that, starting from an MPC controller that has an imprecise model\nand is poorly tuned, the proposed methodology is able to effectively learn to\nimprove the control policy such that congestion in the network is reduced and\nconstraints are satisfied, yielding an improved performance that is superior to\nthe other controllers.\n", "link": "http://arxiv.org/abs/2311.08820v2", "date": "2024-05-21", "relevancy": 1.8601, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4981}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4756}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20with%20Model%20Predictive%20Control%20for%20Highway%20Ramp%0A%20%20Metering&body=Title%3A%20Reinforcement%20Learning%20with%20Model%20Predictive%20Control%20for%20Highway%20Ramp%0A%20%20Metering%0AAuthor%3A%20Filippo%20Airaldi%20and%20Bart%20De%20Schutter%20and%20Azita%20Dabiri%0AAbstract%3A%20%20%20In%20the%20backdrop%20of%20an%20increasingly%20pressing%20need%20for%20effective%20urban%20and%0Ahighway%20transportation%20systems%2C%20this%20work%20explores%20the%20synergy%20between%0Amodel-based%20and%20learning-based%20strategies%20to%20enhance%20traffic%20flow%20management%20by%0Ause%20of%20an%20innovative%20approach%20to%20the%20problem%20of%20ramp%20metering%20control%20that%0Aembeds%20Reinforcement%20Learning%20%28RL%29%20techniques%20within%20the%20Model%20Predictive%0AControl%20%28MPC%29%20framework.%20The%20control%20problem%20is%20formulated%20as%20an%20RL%20task%20by%0Acrafting%20a%20suitable%20stage%20cost%20function%20that%20is%20representative%20of%20the%20traffic%0Aconditions%2C%20variability%20in%20the%20control%20action%2C%20and%20violations%20of%20the%20constraint%0Aon%20the%20maximum%20number%20of%20vehicles%20in%20queue.%20An%20MPC-based%20RL%20approach%2C%20which%0Aleverages%20the%20MPC%20optimal%20problem%20as%20a%20function%20approximation%20for%20the%20RL%0Aalgorithm%2C%20is%20proposed%20to%20learn%20to%20efficiently%20control%20an%20on-ramp%20and%20satisfy%0Aits%20constraints%20despite%20uncertainties%20in%20the%20system%20model%20and%20variable%20demands.%0ASimulations%20are%20performed%20on%20a%20benchmark%20small-scale%20highway%20network%20to%20compare%0Athe%20proposed%20methodology%20against%20other%20state-of-the-art%20control%20approaches.%0AResults%20show%20that%2C%20starting%20from%20an%20MPC%20controller%20that%20has%20an%20imprecise%20model%0Aand%20is%20poorly%20tuned%2C%20the%20proposed%20methodology%20is%20able%20to%20effectively%20learn%20to%0Aimprove%20the%20control%20policy%20such%20that%20congestion%20in%20the%20network%20is%20reduced%20and%0Aconstraints%20are%20satisfied%2C%20yielding%20an%20improved%20performance%20that%20is%20superior%20to%0Athe%20other%20controllers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.08820v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520with%2520Model%2520Predictive%2520Control%2520for%2520Highway%2520Ramp%250A%2520%2520Metering%26entry.906535625%3DFilippo%2520Airaldi%2520and%2520Bart%2520De%2520Schutter%2520and%2520Azita%2520Dabiri%26entry.1292438233%3D%2520%2520In%2520the%2520backdrop%2520of%2520an%2520increasingly%2520pressing%2520need%2520for%2520effective%2520urban%2520and%250Ahighway%2520transportation%2520systems%252C%2520this%2520work%2520explores%2520the%2520synergy%2520between%250Amodel-based%2520and%2520learning-based%2520strategies%2520to%2520enhance%2520traffic%2520flow%2520management%2520by%250Ause%2520of%2520an%2520innovative%2520approach%2520to%2520the%2520problem%2520of%2520ramp%2520metering%2520control%2520that%250Aembeds%2520Reinforcement%2520Learning%2520%2528RL%2529%2520techniques%2520within%2520the%2520Model%2520Predictive%250AControl%2520%2528MPC%2529%2520framework.%2520The%2520control%2520problem%2520is%2520formulated%2520as%2520an%2520RL%2520task%2520by%250Acrafting%2520a%2520suitable%2520stage%2520cost%2520function%2520that%2520is%2520representative%2520of%2520the%2520traffic%250Aconditions%252C%2520variability%2520in%2520the%2520control%2520action%252C%2520and%2520violations%2520of%2520the%2520constraint%250Aon%2520the%2520maximum%2520number%2520of%2520vehicles%2520in%2520queue.%2520An%2520MPC-based%2520RL%2520approach%252C%2520which%250Aleverages%2520the%2520MPC%2520optimal%2520problem%2520as%2520a%2520function%2520approximation%2520for%2520the%2520RL%250Aalgorithm%252C%2520is%2520proposed%2520to%2520learn%2520to%2520efficiently%2520control%2520an%2520on-ramp%2520and%2520satisfy%250Aits%2520constraints%2520despite%2520uncertainties%2520in%2520the%2520system%2520model%2520and%2520variable%2520demands.%250ASimulations%2520are%2520performed%2520on%2520a%2520benchmark%2520small-scale%2520highway%2520network%2520to%2520compare%250Athe%2520proposed%2520methodology%2520against%2520other%2520state-of-the-art%2520control%2520approaches.%250AResults%2520show%2520that%252C%2520starting%2520from%2520an%2520MPC%2520controller%2520that%2520has%2520an%2520imprecise%2520model%250Aand%2520is%2520poorly%2520tuned%252C%2520the%2520proposed%2520methodology%2520is%2520able%2520to%2520effectively%2520learn%2520to%250Aimprove%2520the%2520control%2520policy%2520such%2520that%2520congestion%2520in%2520the%2520network%2520is%2520reduced%2520and%250Aconstraints%2520are%2520satisfied%252C%2520yielding%2520an%2520improved%2520performance%2520that%2520is%2520superior%2520to%250Athe%2520other%2520controllers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.08820v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20with%20Model%20Predictive%20Control%20for%20Highway%20Ramp%0A%20%20Metering&entry.906535625=Filippo%20Airaldi%20and%20Bart%20De%20Schutter%20and%20Azita%20Dabiri&entry.1292438233=%20%20In%20the%20backdrop%20of%20an%20increasingly%20pressing%20need%20for%20effective%20urban%20and%0Ahighway%20transportation%20systems%2C%20this%20work%20explores%20the%20synergy%20between%0Amodel-based%20and%20learning-based%20strategies%20to%20enhance%20traffic%20flow%20management%20by%0Ause%20of%20an%20innovative%20approach%20to%20the%20problem%20of%20ramp%20metering%20control%20that%0Aembeds%20Reinforcement%20Learning%20%28RL%29%20techniques%20within%20the%20Model%20Predictive%0AControl%20%28MPC%29%20framework.%20The%20control%20problem%20is%20formulated%20as%20an%20RL%20task%20by%0Acrafting%20a%20suitable%20stage%20cost%20function%20that%20is%20representative%20of%20the%20traffic%0Aconditions%2C%20variability%20in%20the%20control%20action%2C%20and%20violations%20of%20the%20constraint%0Aon%20the%20maximum%20number%20of%20vehicles%20in%20queue.%20An%20MPC-based%20RL%20approach%2C%20which%0Aleverages%20the%20MPC%20optimal%20problem%20as%20a%20function%20approximation%20for%20the%20RL%0Aalgorithm%2C%20is%20proposed%20to%20learn%20to%20efficiently%20control%20an%20on-ramp%20and%20satisfy%0Aits%20constraints%20despite%20uncertainties%20in%20the%20system%20model%20and%20variable%20demands.%0ASimulations%20are%20performed%20on%20a%20benchmark%20small-scale%20highway%20network%20to%20compare%0Athe%20proposed%20methodology%20against%20other%20state-of-the-art%20control%20approaches.%0AResults%20show%20that%2C%20starting%20from%20an%20MPC%20controller%20that%20has%20an%20imprecise%20model%0Aand%20is%20poorly%20tuned%2C%20the%20proposed%20methodology%20is%20able%20to%20effectively%20learn%20to%0Aimprove%20the%20control%20policy%20such%20that%20congestion%20in%20the%20network%20is%20reduced%20and%0Aconstraints%20are%20satisfied%2C%20yielding%20an%20improved%20performance%20that%20is%20superior%20to%0Athe%20other%20controllers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.08820v2&entry.124074799=Read"},
{"title": "OLAPH: Improving Factuality in Biomedical Long-form Question Answering", "author": "Minbyul Jeong and Hyeon Hwang and Chanwoong Yoon and Taewhoo Lee and Jaewoo Kang", "abstract": "  In the medical domain, numerous scenarios necessitate the long-form\ngeneration ability of large language models (LLMs). Specifically, when\naddressing patients' questions, it is essential that the model's response\nconveys factual claims, highlighting the need for an automated method to\nevaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset\nreconstructed using long-form question-answering datasets related to the\nbiomedical domain. We use MedLFQA to facilitate the automatic evaluations of\nfactuality. We also propose OLAPH, a simple and novel framework that enables\nthe improvement of factuality through automatic evaluations. The OLAPH\nframework iteratively trains LLMs to mitigate hallucinations using sampling\npredictions and preference optimization. In other words, we iteratively set the\nhighest-scoring response as a preferred response derived from sampling\npredictions and train LLMs to align with the preferred response that improves\nfactuality. We highlight that, even on evaluation metrics not used during\ntraining, LLMs trained with our OLAPH framework demonstrate significant\nperformance improvement in factuality. Our findings reveal that a 7B LLM\ntrained with our OLAPH framework can provide long answers comparable to the\nmedical experts' answers in terms of factuality. We believe that our work could\nshed light on gauging the long-text generation ability of LLMs in the medical\ndomain. Our code and datasets are available at\nhttps://github.com/dmis-lab/OLAPH}{https://github.com/dmis-lab/OLAPH.\n", "link": "http://arxiv.org/abs/2405.12701v1", "date": "2024-05-21", "relevancy": 1.8591, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5515}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4482}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OLAPH%3A%20Improving%20Factuality%20in%20Biomedical%20Long-form%20Question%20Answering&body=Title%3A%20OLAPH%3A%20Improving%20Factuality%20in%20Biomedical%20Long-form%20Question%20Answering%0AAuthor%3A%20Minbyul%20Jeong%20and%20Hyeon%20Hwang%20and%20Chanwoong%20Yoon%20and%20Taewhoo%20Lee%20and%20Jaewoo%20Kang%0AAbstract%3A%20%20%20In%20the%20medical%20domain%2C%20numerous%20scenarios%20necessitate%20the%20long-form%0Ageneration%20ability%20of%20large%20language%20models%20%28LLMs%29.%20Specifically%2C%20when%0Aaddressing%20patients%27%20questions%2C%20it%20is%20essential%20that%20the%20model%27s%20response%0Aconveys%20factual%20claims%2C%20highlighting%20the%20need%20for%20an%20automated%20method%20to%0Aevaluate%20those%20claims.%20Thus%2C%20we%20introduce%20MedLFQA%2C%20a%20benchmark%20dataset%0Areconstructed%20using%20long-form%20question-answering%20datasets%20related%20to%20the%0Abiomedical%20domain.%20We%20use%20MedLFQA%20to%20facilitate%20the%20automatic%20evaluations%20of%0Afactuality.%20We%20also%20propose%20OLAPH%2C%20a%20simple%20and%20novel%20framework%20that%20enables%0Athe%20improvement%20of%20factuality%20through%20automatic%20evaluations.%20The%20OLAPH%0Aframework%20iteratively%20trains%20LLMs%20to%20mitigate%20hallucinations%20using%20sampling%0Apredictions%20and%20preference%20optimization.%20In%20other%20words%2C%20we%20iteratively%20set%20the%0Ahighest-scoring%20response%20as%20a%20preferred%20response%20derived%20from%20sampling%0Apredictions%20and%20train%20LLMs%20to%20align%20with%20the%20preferred%20response%20that%20improves%0Afactuality.%20We%20highlight%20that%2C%20even%20on%20evaluation%20metrics%20not%20used%20during%0Atraining%2C%20LLMs%20trained%20with%20our%20OLAPH%20framework%20demonstrate%20significant%0Aperformance%20improvement%20in%20factuality.%20Our%20findings%20reveal%20that%20a%207B%20LLM%0Atrained%20with%20our%20OLAPH%20framework%20can%20provide%20long%20answers%20comparable%20to%20the%0Amedical%20experts%27%20answers%20in%20terms%20of%20factuality.%20We%20believe%20that%20our%20work%20could%0Ashed%20light%20on%20gauging%20the%20long-text%20generation%20ability%20of%20LLMs%20in%20the%20medical%0Adomain.%20Our%20code%20and%20datasets%20are%20available%20at%0Ahttps%3A//github.com/dmis-lab/OLAPH%7D%7Bhttps%3A//github.com/dmis-lab/OLAPH.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOLAPH%253A%2520Improving%2520Factuality%2520in%2520Biomedical%2520Long-form%2520Question%2520Answering%26entry.906535625%3DMinbyul%2520Jeong%2520and%2520Hyeon%2520Hwang%2520and%2520Chanwoong%2520Yoon%2520and%2520Taewhoo%2520Lee%2520and%2520Jaewoo%2520Kang%26entry.1292438233%3D%2520%2520In%2520the%2520medical%2520domain%252C%2520numerous%2520scenarios%2520necessitate%2520the%2520long-form%250Ageneration%2520ability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Specifically%252C%2520when%250Aaddressing%2520patients%2527%2520questions%252C%2520it%2520is%2520essential%2520that%2520the%2520model%2527s%2520response%250Aconveys%2520factual%2520claims%252C%2520highlighting%2520the%2520need%2520for%2520an%2520automated%2520method%2520to%250Aevaluate%2520those%2520claims.%2520Thus%252C%2520we%2520introduce%2520MedLFQA%252C%2520a%2520benchmark%2520dataset%250Areconstructed%2520using%2520long-form%2520question-answering%2520datasets%2520related%2520to%2520the%250Abiomedical%2520domain.%2520We%2520use%2520MedLFQA%2520to%2520facilitate%2520the%2520automatic%2520evaluations%2520of%250Afactuality.%2520We%2520also%2520propose%2520OLAPH%252C%2520a%2520simple%2520and%2520novel%2520framework%2520that%2520enables%250Athe%2520improvement%2520of%2520factuality%2520through%2520automatic%2520evaluations.%2520The%2520OLAPH%250Aframework%2520iteratively%2520trains%2520LLMs%2520to%2520mitigate%2520hallucinations%2520using%2520sampling%250Apredictions%2520and%2520preference%2520optimization.%2520In%2520other%2520words%252C%2520we%2520iteratively%2520set%2520the%250Ahighest-scoring%2520response%2520as%2520a%2520preferred%2520response%2520derived%2520from%2520sampling%250Apredictions%2520and%2520train%2520LLMs%2520to%2520align%2520with%2520the%2520preferred%2520response%2520that%2520improves%250Afactuality.%2520We%2520highlight%2520that%252C%2520even%2520on%2520evaluation%2520metrics%2520not%2520used%2520during%250Atraining%252C%2520LLMs%2520trained%2520with%2520our%2520OLAPH%2520framework%2520demonstrate%2520significant%250Aperformance%2520improvement%2520in%2520factuality.%2520Our%2520findings%2520reveal%2520that%2520a%25207B%2520LLM%250Atrained%2520with%2520our%2520OLAPH%2520framework%2520can%2520provide%2520long%2520answers%2520comparable%2520to%2520the%250Amedical%2520experts%2527%2520answers%2520in%2520terms%2520of%2520factuality.%2520We%2520believe%2520that%2520our%2520work%2520could%250Ashed%2520light%2520on%2520gauging%2520the%2520long-text%2520generation%2520ability%2520of%2520LLMs%2520in%2520the%2520medical%250Adomain.%2520Our%2520code%2520and%2520datasets%2520are%2520available%2520at%250Ahttps%253A//github.com/dmis-lab/OLAPH%257D%257Bhttps%253A//github.com/dmis-lab/OLAPH.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OLAPH%3A%20Improving%20Factuality%20in%20Biomedical%20Long-form%20Question%20Answering&entry.906535625=Minbyul%20Jeong%20and%20Hyeon%20Hwang%20and%20Chanwoong%20Yoon%20and%20Taewhoo%20Lee%20and%20Jaewoo%20Kang&entry.1292438233=%20%20In%20the%20medical%20domain%2C%20numerous%20scenarios%20necessitate%20the%20long-form%0Ageneration%20ability%20of%20large%20language%20models%20%28LLMs%29.%20Specifically%2C%20when%0Aaddressing%20patients%27%20questions%2C%20it%20is%20essential%20that%20the%20model%27s%20response%0Aconveys%20factual%20claims%2C%20highlighting%20the%20need%20for%20an%20automated%20method%20to%0Aevaluate%20those%20claims.%20Thus%2C%20we%20introduce%20MedLFQA%2C%20a%20benchmark%20dataset%0Areconstructed%20using%20long-form%20question-answering%20datasets%20related%20to%20the%0Abiomedical%20domain.%20We%20use%20MedLFQA%20to%20facilitate%20the%20automatic%20evaluations%20of%0Afactuality.%20We%20also%20propose%20OLAPH%2C%20a%20simple%20and%20novel%20framework%20that%20enables%0Athe%20improvement%20of%20factuality%20through%20automatic%20evaluations.%20The%20OLAPH%0Aframework%20iteratively%20trains%20LLMs%20to%20mitigate%20hallucinations%20using%20sampling%0Apredictions%20and%20preference%20optimization.%20In%20other%20words%2C%20we%20iteratively%20set%20the%0Ahighest-scoring%20response%20as%20a%20preferred%20response%20derived%20from%20sampling%0Apredictions%20and%20train%20LLMs%20to%20align%20with%20the%20preferred%20response%20that%20improves%0Afactuality.%20We%20highlight%20that%2C%20even%20on%20evaluation%20metrics%20not%20used%20during%0Atraining%2C%20LLMs%20trained%20with%20our%20OLAPH%20framework%20demonstrate%20significant%0Aperformance%20improvement%20in%20factuality.%20Our%20findings%20reveal%20that%20a%207B%20LLM%0Atrained%20with%20our%20OLAPH%20framework%20can%20provide%20long%20answers%20comparable%20to%20the%0Amedical%20experts%27%20answers%20in%20terms%20of%20factuality.%20We%20believe%20that%20our%20work%20could%0Ashed%20light%20on%20gauging%20the%20long-text%20generation%20ability%20of%20LLMs%20in%20the%20medical%0Adomain.%20Our%20code%20and%20datasets%20are%20available%20at%0Ahttps%3A//github.com/dmis-lab/OLAPH%7D%7Bhttps%3A//github.com/dmis-lab/OLAPH.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12701v1&entry.124074799=Read"},
{"title": "Decentralized Federated Learning Over Imperfect Communication Channels", "author": "Weicai Li and Tiejun Lv and Wei Ni and Jingbo Zhao and Ekram Hossain and H. Vincent Poor", "abstract": "  This paper analyzes the impact of imperfect communication channels on\ndecentralized federated learning (D-FL) and subsequently determines the optimal\nnumber of local aggregations per training round, adapting to the network\ntopology and imperfect channels. We start by deriving the bias of locally\naggregated D-FL models under imperfect channels from the ideal global models\nrequiring perfect channels and aggregations. The bias reveals that excessive\nlocal aggregations can accumulate communication errors and degrade convergence.\nAnother important aspect is that we analyze a convergence upper bound of D-FL\nbased on the bias. By minimizing the bound, the optimal number of local\naggregations is identified to balance a trade-off with accumulation of\ncommunication errors in the absence of knowledge of the channels. With this\nknowledge, the impact of communication errors can be alleviated, allowing the\nconvergence upper bound to decrease throughout aggregations. Experiments\nvalidate our convergence analysis and also identify the optimal number of local\naggregations on two widely considered image classification tasks. It is seen\nthat D-FL, with an optimal number of local aggregations, can outperform its\npotential alternatives by over 10% in training accuracy.\n", "link": "http://arxiv.org/abs/2405.12894v1", "date": "2024-05-21", "relevancy": 1.8581, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4722}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4702}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decentralized%20Federated%20Learning%20Over%20Imperfect%20Communication%20Channels&body=Title%3A%20Decentralized%20Federated%20Learning%20Over%20Imperfect%20Communication%20Channels%0AAuthor%3A%20Weicai%20Li%20and%20Tiejun%20Lv%20and%20Wei%20Ni%20and%20Jingbo%20Zhao%20and%20Ekram%20Hossain%20and%20H.%20Vincent%20Poor%0AAbstract%3A%20%20%20This%20paper%20analyzes%20the%20impact%20of%20imperfect%20communication%20channels%20on%0Adecentralized%20federated%20learning%20%28D-FL%29%20and%20subsequently%20determines%20the%20optimal%0Anumber%20of%20local%20aggregations%20per%20training%20round%2C%20adapting%20to%20the%20network%0Atopology%20and%20imperfect%20channels.%20We%20start%20by%20deriving%20the%20bias%20of%20locally%0Aaggregated%20D-FL%20models%20under%20imperfect%20channels%20from%20the%20ideal%20global%20models%0Arequiring%20perfect%20channels%20and%20aggregations.%20The%20bias%20reveals%20that%20excessive%0Alocal%20aggregations%20can%20accumulate%20communication%20errors%20and%20degrade%20convergence.%0AAnother%20important%20aspect%20is%20that%20we%20analyze%20a%20convergence%20upper%20bound%20of%20D-FL%0Abased%20on%20the%20bias.%20By%20minimizing%20the%20bound%2C%20the%20optimal%20number%20of%20local%0Aaggregations%20is%20identified%20to%20balance%20a%20trade-off%20with%20accumulation%20of%0Acommunication%20errors%20in%20the%20absence%20of%20knowledge%20of%20the%20channels.%20With%20this%0Aknowledge%2C%20the%20impact%20of%20communication%20errors%20can%20be%20alleviated%2C%20allowing%20the%0Aconvergence%20upper%20bound%20to%20decrease%20throughout%20aggregations.%20Experiments%0Avalidate%20our%20convergence%20analysis%20and%20also%20identify%20the%20optimal%20number%20of%20local%0Aaggregations%20on%20two%20widely%20considered%20image%20classification%20tasks.%20It%20is%20seen%0Athat%20D-FL%2C%20with%20an%20optimal%20number%20of%20local%20aggregations%2C%20can%20outperform%20its%0Apotential%20alternatives%20by%20over%2010%25%20in%20training%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12894v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecentralized%2520Federated%2520Learning%2520Over%2520Imperfect%2520Communication%2520Channels%26entry.906535625%3DWeicai%2520Li%2520and%2520Tiejun%2520Lv%2520and%2520Wei%2520Ni%2520and%2520Jingbo%2520Zhao%2520and%2520Ekram%2520Hossain%2520and%2520H.%2520Vincent%2520Poor%26entry.1292438233%3D%2520%2520This%2520paper%2520analyzes%2520the%2520impact%2520of%2520imperfect%2520communication%2520channels%2520on%250Adecentralized%2520federated%2520learning%2520%2528D-FL%2529%2520and%2520subsequently%2520determines%2520the%2520optimal%250Anumber%2520of%2520local%2520aggregations%2520per%2520training%2520round%252C%2520adapting%2520to%2520the%2520network%250Atopology%2520and%2520imperfect%2520channels.%2520We%2520start%2520by%2520deriving%2520the%2520bias%2520of%2520locally%250Aaggregated%2520D-FL%2520models%2520under%2520imperfect%2520channels%2520from%2520the%2520ideal%2520global%2520models%250Arequiring%2520perfect%2520channels%2520and%2520aggregations.%2520The%2520bias%2520reveals%2520that%2520excessive%250Alocal%2520aggregations%2520can%2520accumulate%2520communication%2520errors%2520and%2520degrade%2520convergence.%250AAnother%2520important%2520aspect%2520is%2520that%2520we%2520analyze%2520a%2520convergence%2520upper%2520bound%2520of%2520D-FL%250Abased%2520on%2520the%2520bias.%2520By%2520minimizing%2520the%2520bound%252C%2520the%2520optimal%2520number%2520of%2520local%250Aaggregations%2520is%2520identified%2520to%2520balance%2520a%2520trade-off%2520with%2520accumulation%2520of%250Acommunication%2520errors%2520in%2520the%2520absence%2520of%2520knowledge%2520of%2520the%2520channels.%2520With%2520this%250Aknowledge%252C%2520the%2520impact%2520of%2520communication%2520errors%2520can%2520be%2520alleviated%252C%2520allowing%2520the%250Aconvergence%2520upper%2520bound%2520to%2520decrease%2520throughout%2520aggregations.%2520Experiments%250Avalidate%2520our%2520convergence%2520analysis%2520and%2520also%2520identify%2520the%2520optimal%2520number%2520of%2520local%250Aaggregations%2520on%2520two%2520widely%2520considered%2520image%2520classification%2520tasks.%2520It%2520is%2520seen%250Athat%2520D-FL%252C%2520with%2520an%2520optimal%2520number%2520of%2520local%2520aggregations%252C%2520can%2520outperform%2520its%250Apotential%2520alternatives%2520by%2520over%252010%2525%2520in%2520training%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12894v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20Federated%20Learning%20Over%20Imperfect%20Communication%20Channels&entry.906535625=Weicai%20Li%20and%20Tiejun%20Lv%20and%20Wei%20Ni%20and%20Jingbo%20Zhao%20and%20Ekram%20Hossain%20and%20H.%20Vincent%20Poor&entry.1292438233=%20%20This%20paper%20analyzes%20the%20impact%20of%20imperfect%20communication%20channels%20on%0Adecentralized%20federated%20learning%20%28D-FL%29%20and%20subsequently%20determines%20the%20optimal%0Anumber%20of%20local%20aggregations%20per%20training%20round%2C%20adapting%20to%20the%20network%0Atopology%20and%20imperfect%20channels.%20We%20start%20by%20deriving%20the%20bias%20of%20locally%0Aaggregated%20D-FL%20models%20under%20imperfect%20channels%20from%20the%20ideal%20global%20models%0Arequiring%20perfect%20channels%20and%20aggregations.%20The%20bias%20reveals%20that%20excessive%0Alocal%20aggregations%20can%20accumulate%20communication%20errors%20and%20degrade%20convergence.%0AAnother%20important%20aspect%20is%20that%20we%20analyze%20a%20convergence%20upper%20bound%20of%20D-FL%0Abased%20on%20the%20bias.%20By%20minimizing%20the%20bound%2C%20the%20optimal%20number%20of%20local%0Aaggregations%20is%20identified%20to%20balance%20a%20trade-off%20with%20accumulation%20of%0Acommunication%20errors%20in%20the%20absence%20of%20knowledge%20of%20the%20channels.%20With%20this%0Aknowledge%2C%20the%20impact%20of%20communication%20errors%20can%20be%20alleviated%2C%20allowing%20the%0Aconvergence%20upper%20bound%20to%20decrease%20throughout%20aggregations.%20Experiments%0Avalidate%20our%20convergence%20analysis%20and%20also%20identify%20the%20optimal%20number%20of%20local%0Aaggregations%20on%20two%20widely%20considered%20image%20classification%20tasks.%20It%20is%20seen%0Athat%20D-FL%2C%20with%20an%20optimal%20number%20of%20local%20aggregations%2C%20can%20outperform%20its%0Apotential%20alternatives%20by%20over%2010%25%20in%20training%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12894v1&entry.124074799=Read"},
{"title": "FedLPA: One-shot Federated Learning with Layer-Wise Posterior\n  Aggregation", "author": "Xiang Liu and Liangxi Liu and Feiyang Ye and Yunheng Shen and Xia Li and Linshan Jiang and Jialin Li", "abstract": "  Efficiently aggregating trained neural networks from local clients into a\nglobal model on a server is a widely researched topic in federated learning.\nRecently, motivated by diminishing privacy concerns, mitigating potential\nattacks, and reducing communication overhead, one-shot federated learning\n(i.e., limiting client-server communication into a single round) has gained\npopularity among researchers. However, the one-shot aggregation performances\nare sensitively affected by the non-identical training data distribution, which\nexhibits high statistical heterogeneity in some real-world scenarios. To\naddress this issue, we propose a novel one-shot aggregation method with\nlayer-wise posterior aggregation, named FedLPA. FedLPA aggregates local models\nto obtain a more accurate global model without requiring extra auxiliary\ndatasets or exposing any private label information, e.g., label distributions.\nTo effectively capture the statistics maintained in the biased local datasets\nin the practical non-IID scenario, we efficiently infer the posteriors of each\nlayer in each local model using layer-wise Laplace approximation and aggregate\nthem to train the global parameters. Extensive experimental results demonstrate\nthat FedLPA significantly improves learning performance over state-of-the-art\nmethods across several metrics.\n", "link": "http://arxiv.org/abs/2310.00339v3", "date": "2024-05-21", "relevancy": 1.8522, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4826}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4561}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedLPA%3A%20One-shot%20Federated%20Learning%20with%20Layer-Wise%20Posterior%0A%20%20Aggregation&body=Title%3A%20FedLPA%3A%20One-shot%20Federated%20Learning%20with%20Layer-Wise%20Posterior%0A%20%20Aggregation%0AAuthor%3A%20Xiang%20Liu%20and%20Liangxi%20Liu%20and%20Feiyang%20Ye%20and%20Yunheng%20Shen%20and%20Xia%20Li%20and%20Linshan%20Jiang%20and%20Jialin%20Li%0AAbstract%3A%20%20%20Efficiently%20aggregating%20trained%20neural%20networks%20from%20local%20clients%20into%20a%0Aglobal%20model%20on%20a%20server%20is%20a%20widely%20researched%20topic%20in%20federated%20learning.%0ARecently%2C%20motivated%20by%20diminishing%20privacy%20concerns%2C%20mitigating%20potential%0Aattacks%2C%20and%20reducing%20communication%20overhead%2C%20one-shot%20federated%20learning%0A%28i.e.%2C%20limiting%20client-server%20communication%20into%20a%20single%20round%29%20has%20gained%0Apopularity%20among%20researchers.%20However%2C%20the%20one-shot%20aggregation%20performances%0Aare%20sensitively%20affected%20by%20the%20non-identical%20training%20data%20distribution%2C%20which%0Aexhibits%20high%20statistical%20heterogeneity%20in%20some%20real-world%20scenarios.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20novel%20one-shot%20aggregation%20method%20with%0Alayer-wise%20posterior%20aggregation%2C%20named%20FedLPA.%20FedLPA%20aggregates%20local%20models%0Ato%20obtain%20a%20more%20accurate%20global%20model%20without%20requiring%20extra%20auxiliary%0Adatasets%20or%20exposing%20any%20private%20label%20information%2C%20e.g.%2C%20label%20distributions.%0ATo%20effectively%20capture%20the%20statistics%20maintained%20in%20the%20biased%20local%20datasets%0Ain%20the%20practical%20non-IID%20scenario%2C%20we%20efficiently%20infer%20the%20posteriors%20of%20each%0Alayer%20in%20each%20local%20model%20using%20layer-wise%20Laplace%20approximation%20and%20aggregate%0Athem%20to%20train%20the%20global%20parameters.%20Extensive%20experimental%20results%20demonstrate%0Athat%20FedLPA%20significantly%20improves%20learning%20performance%20over%20state-of-the-art%0Amethods%20across%20several%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.00339v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedLPA%253A%2520One-shot%2520Federated%2520Learning%2520with%2520Layer-Wise%2520Posterior%250A%2520%2520Aggregation%26entry.906535625%3DXiang%2520Liu%2520and%2520Liangxi%2520Liu%2520and%2520Feiyang%2520Ye%2520and%2520Yunheng%2520Shen%2520and%2520Xia%2520Li%2520and%2520Linshan%2520Jiang%2520and%2520Jialin%2520Li%26entry.1292438233%3D%2520%2520Efficiently%2520aggregating%2520trained%2520neural%2520networks%2520from%2520local%2520clients%2520into%2520a%250Aglobal%2520model%2520on%2520a%2520server%2520is%2520a%2520widely%2520researched%2520topic%2520in%2520federated%2520learning.%250ARecently%252C%2520motivated%2520by%2520diminishing%2520privacy%2520concerns%252C%2520mitigating%2520potential%250Aattacks%252C%2520and%2520reducing%2520communication%2520overhead%252C%2520one-shot%2520federated%2520learning%250A%2528i.e.%252C%2520limiting%2520client-server%2520communication%2520into%2520a%2520single%2520round%2529%2520has%2520gained%250Apopularity%2520among%2520researchers.%2520However%252C%2520the%2520one-shot%2520aggregation%2520performances%250Aare%2520sensitively%2520affected%2520by%2520the%2520non-identical%2520training%2520data%2520distribution%252C%2520which%250Aexhibits%2520high%2520statistical%2520heterogeneity%2520in%2520some%2520real-world%2520scenarios.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520one-shot%2520aggregation%2520method%2520with%250Alayer-wise%2520posterior%2520aggregation%252C%2520named%2520FedLPA.%2520FedLPA%2520aggregates%2520local%2520models%250Ato%2520obtain%2520a%2520more%2520accurate%2520global%2520model%2520without%2520requiring%2520extra%2520auxiliary%250Adatasets%2520or%2520exposing%2520any%2520private%2520label%2520information%252C%2520e.g.%252C%2520label%2520distributions.%250ATo%2520effectively%2520capture%2520the%2520statistics%2520maintained%2520in%2520the%2520biased%2520local%2520datasets%250Ain%2520the%2520practical%2520non-IID%2520scenario%252C%2520we%2520efficiently%2520infer%2520the%2520posteriors%2520of%2520each%250Alayer%2520in%2520each%2520local%2520model%2520using%2520layer-wise%2520Laplace%2520approximation%2520and%2520aggregate%250Athem%2520to%2520train%2520the%2520global%2520parameters.%2520Extensive%2520experimental%2520results%2520demonstrate%250Athat%2520FedLPA%2520significantly%2520improves%2520learning%2520performance%2520over%2520state-of-the-art%250Amethods%2520across%2520several%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.00339v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedLPA%3A%20One-shot%20Federated%20Learning%20with%20Layer-Wise%20Posterior%0A%20%20Aggregation&entry.906535625=Xiang%20Liu%20and%20Liangxi%20Liu%20and%20Feiyang%20Ye%20and%20Yunheng%20Shen%20and%20Xia%20Li%20and%20Linshan%20Jiang%20and%20Jialin%20Li&entry.1292438233=%20%20Efficiently%20aggregating%20trained%20neural%20networks%20from%20local%20clients%20into%20a%0Aglobal%20model%20on%20a%20server%20is%20a%20widely%20researched%20topic%20in%20federated%20learning.%0ARecently%2C%20motivated%20by%20diminishing%20privacy%20concerns%2C%20mitigating%20potential%0Aattacks%2C%20and%20reducing%20communication%20overhead%2C%20one-shot%20federated%20learning%0A%28i.e.%2C%20limiting%20client-server%20communication%20into%20a%20single%20round%29%20has%20gained%0Apopularity%20among%20researchers.%20However%2C%20the%20one-shot%20aggregation%20performances%0Aare%20sensitively%20affected%20by%20the%20non-identical%20training%20data%20distribution%2C%20which%0Aexhibits%20high%20statistical%20heterogeneity%20in%20some%20real-world%20scenarios.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20novel%20one-shot%20aggregation%20method%20with%0Alayer-wise%20posterior%20aggregation%2C%20named%20FedLPA.%20FedLPA%20aggregates%20local%20models%0Ato%20obtain%20a%20more%20accurate%20global%20model%20without%20requiring%20extra%20auxiliary%0Adatasets%20or%20exposing%20any%20private%20label%20information%2C%20e.g.%2C%20label%20distributions.%0ATo%20effectively%20capture%20the%20statistics%20maintained%20in%20the%20biased%20local%20datasets%0Ain%20the%20practical%20non-IID%20scenario%2C%20we%20efficiently%20infer%20the%20posteriors%20of%20each%0Alayer%20in%20each%20local%20model%20using%20layer-wise%20Laplace%20approximation%20and%20aggregate%0Athem%20to%20train%20the%20global%20parameters.%20Extensive%20experimental%20results%20demonstrate%0Athat%20FedLPA%20significantly%20improves%20learning%20performance%20over%20state-of-the-art%0Amethods%20across%20several%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.00339v3&entry.124074799=Read"},
{"title": "Feature Attribution with Necessity and Sufficiency via Dual-stage\n  Perturbation Test for Causal Explanation", "author": "Xuexin Chen and Ruichu Cai and Zhengting Huang and Yuxuan Zhu and Julien Horwood and Zhifeng Hao and Zijian Li and Jose Miguel Hernandez-Lobato", "abstract": "  We investigate the problem of explainability in machine learning. To address\nthis problem, Feature Attribution Methods (FAMs) measure the contribution of\neach feature through a perturbation test, where the difference in prediction is\ncompared under different perturbations. However, such perturbation tests may\nnot accurately distinguish the contributions of different features, when their\nchange in prediction is the same after perturbation. In order to enhance the\nability of FAMs to distinguish different features' contributions in this\nchallenging setting, we propose to utilize the Probability of Necessity and\nSufficiency (PNS) that perturbing a feature is a necessary and sufficient cause\nfor the prediction to change as a measure of feature importance. Our approach,\nFeature Attribution with Necessity and Sufficiency (FANS), computes the PNS via\na perturbation test involving two stages (factual and interventional). In\npractice, to generate counterfactual samples, we use a resampling-based\napproach on the observed samples to approximate the required conditional\ndistribution. We demonstrate that FANS outperforms existing attribution methods\non six benchmarks. Our source code is available at\n\\url{https://github.com/DMIRLAB-Group/FANS}.\n", "link": "http://arxiv.org/abs/2402.08845v2", "date": "2024-05-21", "relevancy": 1.8515, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4907}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4661}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Attribution%20with%20Necessity%20and%20Sufficiency%20via%20Dual-stage%0A%20%20Perturbation%20Test%20for%20Causal%20Explanation&body=Title%3A%20Feature%20Attribution%20with%20Necessity%20and%20Sufficiency%20via%20Dual-stage%0A%20%20Perturbation%20Test%20for%20Causal%20Explanation%0AAuthor%3A%20Xuexin%20Chen%20and%20Ruichu%20Cai%20and%20Zhengting%20Huang%20and%20Yuxuan%20Zhu%20and%20Julien%20Horwood%20and%20Zhifeng%20Hao%20and%20Zijian%20Li%20and%20Jose%20Miguel%20Hernandez-Lobato%0AAbstract%3A%20%20%20We%20investigate%20the%20problem%20of%20explainability%20in%20machine%20learning.%20To%20address%0Athis%20problem%2C%20Feature%20Attribution%20Methods%20%28FAMs%29%20measure%20the%20contribution%20of%0Aeach%20feature%20through%20a%20perturbation%20test%2C%20where%20the%20difference%20in%20prediction%20is%0Acompared%20under%20different%20perturbations.%20However%2C%20such%20perturbation%20tests%20may%0Anot%20accurately%20distinguish%20the%20contributions%20of%20different%20features%2C%20when%20their%0Achange%20in%20prediction%20is%20the%20same%20after%20perturbation.%20In%20order%20to%20enhance%20the%0Aability%20of%20FAMs%20to%20distinguish%20different%20features%27%20contributions%20in%20this%0Achallenging%20setting%2C%20we%20propose%20to%20utilize%20the%20Probability%20of%20Necessity%20and%0ASufficiency%20%28PNS%29%20that%20perturbing%20a%20feature%20is%20a%20necessary%20and%20sufficient%20cause%0Afor%20the%20prediction%20to%20change%20as%20a%20measure%20of%20feature%20importance.%20Our%20approach%2C%0AFeature%20Attribution%20with%20Necessity%20and%20Sufficiency%20%28FANS%29%2C%20computes%20the%20PNS%20via%0Aa%20perturbation%20test%20involving%20two%20stages%20%28factual%20and%20interventional%29.%20In%0Apractice%2C%20to%20generate%20counterfactual%20samples%2C%20we%20use%20a%20resampling-based%0Aapproach%20on%20the%20observed%20samples%20to%20approximate%20the%20required%20conditional%0Adistribution.%20We%20demonstrate%20that%20FANS%20outperforms%20existing%20attribution%20methods%0Aon%20six%20benchmarks.%20Our%20source%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/DMIRLAB-Group/FANS%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08845v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Attribution%2520with%2520Necessity%2520and%2520Sufficiency%2520via%2520Dual-stage%250A%2520%2520Perturbation%2520Test%2520for%2520Causal%2520Explanation%26entry.906535625%3DXuexin%2520Chen%2520and%2520Ruichu%2520Cai%2520and%2520Zhengting%2520Huang%2520and%2520Yuxuan%2520Zhu%2520and%2520Julien%2520Horwood%2520and%2520Zhifeng%2520Hao%2520and%2520Zijian%2520Li%2520and%2520Jose%2520Miguel%2520Hernandez-Lobato%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520problem%2520of%2520explainability%2520in%2520machine%2520learning.%2520To%2520address%250Athis%2520problem%252C%2520Feature%2520Attribution%2520Methods%2520%2528FAMs%2529%2520measure%2520the%2520contribution%2520of%250Aeach%2520feature%2520through%2520a%2520perturbation%2520test%252C%2520where%2520the%2520difference%2520in%2520prediction%2520is%250Acompared%2520under%2520different%2520perturbations.%2520However%252C%2520such%2520perturbation%2520tests%2520may%250Anot%2520accurately%2520distinguish%2520the%2520contributions%2520of%2520different%2520features%252C%2520when%2520their%250Achange%2520in%2520prediction%2520is%2520the%2520same%2520after%2520perturbation.%2520In%2520order%2520to%2520enhance%2520the%250Aability%2520of%2520FAMs%2520to%2520distinguish%2520different%2520features%2527%2520contributions%2520in%2520this%250Achallenging%2520setting%252C%2520we%2520propose%2520to%2520utilize%2520the%2520Probability%2520of%2520Necessity%2520and%250ASufficiency%2520%2528PNS%2529%2520that%2520perturbing%2520a%2520feature%2520is%2520a%2520necessary%2520and%2520sufficient%2520cause%250Afor%2520the%2520prediction%2520to%2520change%2520as%2520a%2520measure%2520of%2520feature%2520importance.%2520Our%2520approach%252C%250AFeature%2520Attribution%2520with%2520Necessity%2520and%2520Sufficiency%2520%2528FANS%2529%252C%2520computes%2520the%2520PNS%2520via%250Aa%2520perturbation%2520test%2520involving%2520two%2520stages%2520%2528factual%2520and%2520interventional%2529.%2520In%250Apractice%252C%2520to%2520generate%2520counterfactual%2520samples%252C%2520we%2520use%2520a%2520resampling-based%250Aapproach%2520on%2520the%2520observed%2520samples%2520to%2520approximate%2520the%2520required%2520conditional%250Adistribution.%2520We%2520demonstrate%2520that%2520FANS%2520outperforms%2520existing%2520attribution%2520methods%250Aon%2520six%2520benchmarks.%2520Our%2520source%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/DMIRLAB-Group/FANS%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08845v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Attribution%20with%20Necessity%20and%20Sufficiency%20via%20Dual-stage%0A%20%20Perturbation%20Test%20for%20Causal%20Explanation&entry.906535625=Xuexin%20Chen%20and%20Ruichu%20Cai%20and%20Zhengting%20Huang%20and%20Yuxuan%20Zhu%20and%20Julien%20Horwood%20and%20Zhifeng%20Hao%20and%20Zijian%20Li%20and%20Jose%20Miguel%20Hernandez-Lobato&entry.1292438233=%20%20We%20investigate%20the%20problem%20of%20explainability%20in%20machine%20learning.%20To%20address%0Athis%20problem%2C%20Feature%20Attribution%20Methods%20%28FAMs%29%20measure%20the%20contribution%20of%0Aeach%20feature%20through%20a%20perturbation%20test%2C%20where%20the%20difference%20in%20prediction%20is%0Acompared%20under%20different%20perturbations.%20However%2C%20such%20perturbation%20tests%20may%0Anot%20accurately%20distinguish%20the%20contributions%20of%20different%20features%2C%20when%20their%0Achange%20in%20prediction%20is%20the%20same%20after%20perturbation.%20In%20order%20to%20enhance%20the%0Aability%20of%20FAMs%20to%20distinguish%20different%20features%27%20contributions%20in%20this%0Achallenging%20setting%2C%20we%20propose%20to%20utilize%20the%20Probability%20of%20Necessity%20and%0ASufficiency%20%28PNS%29%20that%20perturbing%20a%20feature%20is%20a%20necessary%20and%20sufficient%20cause%0Afor%20the%20prediction%20to%20change%20as%20a%20measure%20of%20feature%20importance.%20Our%20approach%2C%0AFeature%20Attribution%20with%20Necessity%20and%20Sufficiency%20%28FANS%29%2C%20computes%20the%20PNS%20via%0Aa%20perturbation%20test%20involving%20two%20stages%20%28factual%20and%20interventional%29.%20In%0Apractice%2C%20to%20generate%20counterfactual%20samples%2C%20we%20use%20a%20resampling-based%0Aapproach%20on%20the%20observed%20samples%20to%20approximate%20the%20required%20conditional%0Adistribution.%20We%20demonstrate%20that%20FANS%20outperforms%20existing%20attribution%20methods%0Aon%20six%20benchmarks.%20Our%20source%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/DMIRLAB-Group/FANS%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08845v2&entry.124074799=Read"},
{"title": "Incorporating Recklessness to Collaborative Filtering based Recommender\n  Systems", "author": "Diego P\u00e9rez-L\u00f3pez and Fernando Ortega and \u00c1ngel Gonz\u00e1lez-Prieto and Jorge Due\u00f1as-Ler\u00edn", "abstract": "  Recommender systems are intrinsically tied to a reliability/coverage dilemma:\nThe more reliable we desire the forecasts, the more conservative the decision\nwill be and thus, the fewer items will be recommended. This causes a detriment\nto the predictive capability of the system, as it is only able to estimate\npotential interest in items for which there is a consensus in their evaluation,\nrather than being able to estimate potential interest in any item. In this\npaper, we propose the inclusion of a new term in the learning process of matrix\nfactorization-based recommender systems, called recklessness, that takes into\naccount the variance of the output probability distribution of the predicted\nratings. In this way, gauging this recklessness measure we can force more spiky\noutput distribution, enabling the control of the risk level desired when making\ndecisions about the reliability of a prediction. Experimental results\ndemonstrate that recklessness not only allows for risk regulation but also\nimproves the quantity and quality of predictions provided by the recommender\nsystem.\n", "link": "http://arxiv.org/abs/2308.02058v3", "date": "2024-05-21", "relevancy": 1.8478, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5006}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4499}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incorporating%20Recklessness%20to%20Collaborative%20Filtering%20based%20Recommender%0A%20%20Systems&body=Title%3A%20Incorporating%20Recklessness%20to%20Collaborative%20Filtering%20based%20Recommender%0A%20%20Systems%0AAuthor%3A%20Diego%20P%C3%A9rez-L%C3%B3pez%20and%20Fernando%20Ortega%20and%20%C3%81ngel%20Gonz%C3%A1lez-Prieto%20and%20Jorge%20Due%C3%B1as-Ler%C3%ADn%0AAbstract%3A%20%20%20Recommender%20systems%20are%20intrinsically%20tied%20to%20a%20reliability/coverage%20dilemma%3A%0AThe%20more%20reliable%20we%20desire%20the%20forecasts%2C%20the%20more%20conservative%20the%20decision%0Awill%20be%20and%20thus%2C%20the%20fewer%20items%20will%20be%20recommended.%20This%20causes%20a%20detriment%0Ato%20the%20predictive%20capability%20of%20the%20system%2C%20as%20it%20is%20only%20able%20to%20estimate%0Apotential%20interest%20in%20items%20for%20which%20there%20is%20a%20consensus%20in%20their%20evaluation%2C%0Arather%20than%20being%20able%20to%20estimate%20potential%20interest%20in%20any%20item.%20In%20this%0Apaper%2C%20we%20propose%20the%20inclusion%20of%20a%20new%20term%20in%20the%20learning%20process%20of%20matrix%0Afactorization-based%20recommender%20systems%2C%20called%20recklessness%2C%20that%20takes%20into%0Aaccount%20the%20variance%20of%20the%20output%20probability%20distribution%20of%20the%20predicted%0Aratings.%20In%20this%20way%2C%20gauging%20this%20recklessness%20measure%20we%20can%20force%20more%20spiky%0Aoutput%20distribution%2C%20enabling%20the%20control%20of%20the%20risk%20level%20desired%20when%20making%0Adecisions%20about%20the%20reliability%20of%20a%20prediction.%20Experimental%20results%0Ademonstrate%20that%20recklessness%20not%20only%20allows%20for%20risk%20regulation%20but%20also%0Aimproves%20the%20quantity%20and%20quality%20of%20predictions%20provided%20by%20the%20recommender%0Asystem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.02058v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncorporating%2520Recklessness%2520to%2520Collaborative%2520Filtering%2520based%2520Recommender%250A%2520%2520Systems%26entry.906535625%3DDiego%2520P%25C3%25A9rez-L%25C3%25B3pez%2520and%2520Fernando%2520Ortega%2520and%2520%25C3%2581ngel%2520Gonz%25C3%25A1lez-Prieto%2520and%2520Jorge%2520Due%25C3%25B1as-Ler%25C3%25ADn%26entry.1292438233%3D%2520%2520Recommender%2520systems%2520are%2520intrinsically%2520tied%2520to%2520a%2520reliability/coverage%2520dilemma%253A%250AThe%2520more%2520reliable%2520we%2520desire%2520the%2520forecasts%252C%2520the%2520more%2520conservative%2520the%2520decision%250Awill%2520be%2520and%2520thus%252C%2520the%2520fewer%2520items%2520will%2520be%2520recommended.%2520This%2520causes%2520a%2520detriment%250Ato%2520the%2520predictive%2520capability%2520of%2520the%2520system%252C%2520as%2520it%2520is%2520only%2520able%2520to%2520estimate%250Apotential%2520interest%2520in%2520items%2520for%2520which%2520there%2520is%2520a%2520consensus%2520in%2520their%2520evaluation%252C%250Arather%2520than%2520being%2520able%2520to%2520estimate%2520potential%2520interest%2520in%2520any%2520item.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520the%2520inclusion%2520of%2520a%2520new%2520term%2520in%2520the%2520learning%2520process%2520of%2520matrix%250Afactorization-based%2520recommender%2520systems%252C%2520called%2520recklessness%252C%2520that%2520takes%2520into%250Aaccount%2520the%2520variance%2520of%2520the%2520output%2520probability%2520distribution%2520of%2520the%2520predicted%250Aratings.%2520In%2520this%2520way%252C%2520gauging%2520this%2520recklessness%2520measure%2520we%2520can%2520force%2520more%2520spiky%250Aoutput%2520distribution%252C%2520enabling%2520the%2520control%2520of%2520the%2520risk%2520level%2520desired%2520when%2520making%250Adecisions%2520about%2520the%2520reliability%2520of%2520a%2520prediction.%2520Experimental%2520results%250Ademonstrate%2520that%2520recklessness%2520not%2520only%2520allows%2520for%2520risk%2520regulation%2520but%2520also%250Aimproves%2520the%2520quantity%2520and%2520quality%2520of%2520predictions%2520provided%2520by%2520the%2520recommender%250Asystem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.02058v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incorporating%20Recklessness%20to%20Collaborative%20Filtering%20based%20Recommender%0A%20%20Systems&entry.906535625=Diego%20P%C3%A9rez-L%C3%B3pez%20and%20Fernando%20Ortega%20and%20%C3%81ngel%20Gonz%C3%A1lez-Prieto%20and%20Jorge%20Due%C3%B1as-Ler%C3%ADn&entry.1292438233=%20%20Recommender%20systems%20are%20intrinsically%20tied%20to%20a%20reliability/coverage%20dilemma%3A%0AThe%20more%20reliable%20we%20desire%20the%20forecasts%2C%20the%20more%20conservative%20the%20decision%0Awill%20be%20and%20thus%2C%20the%20fewer%20items%20will%20be%20recommended.%20This%20causes%20a%20detriment%0Ato%20the%20predictive%20capability%20of%20the%20system%2C%20as%20it%20is%20only%20able%20to%20estimate%0Apotential%20interest%20in%20items%20for%20which%20there%20is%20a%20consensus%20in%20their%20evaluation%2C%0Arather%20than%20being%20able%20to%20estimate%20potential%20interest%20in%20any%20item.%20In%20this%0Apaper%2C%20we%20propose%20the%20inclusion%20of%20a%20new%20term%20in%20the%20learning%20process%20of%20matrix%0Afactorization-based%20recommender%20systems%2C%20called%20recklessness%2C%20that%20takes%20into%0Aaccount%20the%20variance%20of%20the%20output%20probability%20distribution%20of%20the%20predicted%0Aratings.%20In%20this%20way%2C%20gauging%20this%20recklessness%20measure%20we%20can%20force%20more%20spiky%0Aoutput%20distribution%2C%20enabling%20the%20control%20of%20the%20risk%20level%20desired%20when%20making%0Adecisions%20about%20the%20reliability%20of%20a%20prediction.%20Experimental%20results%0Ademonstrate%20that%20recklessness%20not%20only%20allows%20for%20risk%20regulation%20but%20also%0Aimproves%20the%20quantity%20and%20quality%20of%20predictions%20provided%20by%20the%20recommender%0Asystem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.02058v3&entry.124074799=Read"},
{"title": "Utilizing Description Logics for Global Explanations of Heterogeneous\n  Graph Neural Networks", "author": "Dominik K\u00f6hler and Stefan Heindorf", "abstract": "  Graph Neural Networks (GNNs) are effective for node classification in\ngraph-structured data, but they lack explainability, especially at the global\nlevel. Current research mainly utilizes subgraphs of the input as local\nexplanations or generates new graphs as global explanations. However, these\ngraph-based methods are limited in their ability to explain classes with\nmultiple sufficient explanations. To provide more expressive explanations, we\npropose utilizing class expressions (CEs) from the field of description logic\n(DL). Our approach explains heterogeneous graphs with different types of nodes\nusing CEs in the EL description logic. To identify the best explanation among\nmultiple candidate explanations, we employ and compare two different scoring\nfunctions: (1) For a given CE, we construct multiple graphs, have the GNN make\na prediction for each graph, and aggregate the predicted scores. (2) We score\nthe CE in terms of fidelity, i.e., we compare the predictions of the GNN to the\npredictions by the CE on a separate validation set. Instead of subgraph-based\nexplanations, we offer CE-based explanations.\n", "link": "http://arxiv.org/abs/2405.12654v1", "date": "2024-05-21", "relevancy": 1.844, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4764}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4674}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Utilizing%20Description%20Logics%20for%20Global%20Explanations%20of%20Heterogeneous%0A%20%20Graph%20Neural%20Networks&body=Title%3A%20Utilizing%20Description%20Logics%20for%20Global%20Explanations%20of%20Heterogeneous%0A%20%20Graph%20Neural%20Networks%0AAuthor%3A%20Dominik%20K%C3%B6hler%20and%20Stefan%20Heindorf%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20effective%20for%20node%20classification%20in%0Agraph-structured%20data%2C%20but%20they%20lack%20explainability%2C%20especially%20at%20the%20global%0Alevel.%20Current%20research%20mainly%20utilizes%20subgraphs%20of%20the%20input%20as%20local%0Aexplanations%20or%20generates%20new%20graphs%20as%20global%20explanations.%20However%2C%20these%0Agraph-based%20methods%20are%20limited%20in%20their%20ability%20to%20explain%20classes%20with%0Amultiple%20sufficient%20explanations.%20To%20provide%20more%20expressive%20explanations%2C%20we%0Apropose%20utilizing%20class%20expressions%20%28CEs%29%20from%20the%20field%20of%20description%20logic%0A%28DL%29.%20Our%20approach%20explains%20heterogeneous%20graphs%20with%20different%20types%20of%20nodes%0Ausing%20CEs%20in%20the%20EL%20description%20logic.%20To%20identify%20the%20best%20explanation%20among%0Amultiple%20candidate%20explanations%2C%20we%20employ%20and%20compare%20two%20different%20scoring%0Afunctions%3A%20%281%29%20For%20a%20given%20CE%2C%20we%20construct%20multiple%20graphs%2C%20have%20the%20GNN%20make%0Aa%20prediction%20for%20each%20graph%2C%20and%20aggregate%20the%20predicted%20scores.%20%282%29%20We%20score%0Athe%20CE%20in%20terms%20of%20fidelity%2C%20i.e.%2C%20we%20compare%20the%20predictions%20of%20the%20GNN%20to%20the%0Apredictions%20by%20the%20CE%20on%20a%20separate%20validation%20set.%20Instead%20of%20subgraph-based%0Aexplanations%2C%20we%20offer%20CE-based%20explanations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUtilizing%2520Description%2520Logics%2520for%2520Global%2520Explanations%2520of%2520Heterogeneous%250A%2520%2520Graph%2520Neural%2520Networks%26entry.906535625%3DDominik%2520K%25C3%25B6hler%2520and%2520Stefan%2520Heindorf%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520effective%2520for%2520node%2520classification%2520in%250Agraph-structured%2520data%252C%2520but%2520they%2520lack%2520explainability%252C%2520especially%2520at%2520the%2520global%250Alevel.%2520Current%2520research%2520mainly%2520utilizes%2520subgraphs%2520of%2520the%2520input%2520as%2520local%250Aexplanations%2520or%2520generates%2520new%2520graphs%2520as%2520global%2520explanations.%2520However%252C%2520these%250Agraph-based%2520methods%2520are%2520limited%2520in%2520their%2520ability%2520to%2520explain%2520classes%2520with%250Amultiple%2520sufficient%2520explanations.%2520To%2520provide%2520more%2520expressive%2520explanations%252C%2520we%250Apropose%2520utilizing%2520class%2520expressions%2520%2528CEs%2529%2520from%2520the%2520field%2520of%2520description%2520logic%250A%2528DL%2529.%2520Our%2520approach%2520explains%2520heterogeneous%2520graphs%2520with%2520different%2520types%2520of%2520nodes%250Ausing%2520CEs%2520in%2520the%2520EL%2520description%2520logic.%2520To%2520identify%2520the%2520best%2520explanation%2520among%250Amultiple%2520candidate%2520explanations%252C%2520we%2520employ%2520and%2520compare%2520two%2520different%2520scoring%250Afunctions%253A%2520%25281%2529%2520For%2520a%2520given%2520CE%252C%2520we%2520construct%2520multiple%2520graphs%252C%2520have%2520the%2520GNN%2520make%250Aa%2520prediction%2520for%2520each%2520graph%252C%2520and%2520aggregate%2520the%2520predicted%2520scores.%2520%25282%2529%2520We%2520score%250Athe%2520CE%2520in%2520terms%2520of%2520fidelity%252C%2520i.e.%252C%2520we%2520compare%2520the%2520predictions%2520of%2520the%2520GNN%2520to%2520the%250Apredictions%2520by%2520the%2520CE%2520on%2520a%2520separate%2520validation%2520set.%2520Instead%2520of%2520subgraph-based%250Aexplanations%252C%2520we%2520offer%2520CE-based%2520explanations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Utilizing%20Description%20Logics%20for%20Global%20Explanations%20of%20Heterogeneous%0A%20%20Graph%20Neural%20Networks&entry.906535625=Dominik%20K%C3%B6hler%20and%20Stefan%20Heindorf&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20effective%20for%20node%20classification%20in%0Agraph-structured%20data%2C%20but%20they%20lack%20explainability%2C%20especially%20at%20the%20global%0Alevel.%20Current%20research%20mainly%20utilizes%20subgraphs%20of%20the%20input%20as%20local%0Aexplanations%20or%20generates%20new%20graphs%20as%20global%20explanations.%20However%2C%20these%0Agraph-based%20methods%20are%20limited%20in%20their%20ability%20to%20explain%20classes%20with%0Amultiple%20sufficient%20explanations.%20To%20provide%20more%20expressive%20explanations%2C%20we%0Apropose%20utilizing%20class%20expressions%20%28CEs%29%20from%20the%20field%20of%20description%20logic%0A%28DL%29.%20Our%20approach%20explains%20heterogeneous%20graphs%20with%20different%20types%20of%20nodes%0Ausing%20CEs%20in%20the%20EL%20description%20logic.%20To%20identify%20the%20best%20explanation%20among%0Amultiple%20candidate%20explanations%2C%20we%20employ%20and%20compare%20two%20different%20scoring%0Afunctions%3A%20%281%29%20For%20a%20given%20CE%2C%20we%20construct%20multiple%20graphs%2C%20have%20the%20GNN%20make%0Aa%20prediction%20for%20each%20graph%2C%20and%20aggregate%20the%20predicted%20scores.%20%282%29%20We%20score%0Athe%20CE%20in%20terms%20of%20fidelity%2C%20i.e.%2C%20we%20compare%20the%20predictions%20of%20the%20GNN%20to%20the%0Apredictions%20by%20the%20CE%20on%20a%20separate%20validation%20set.%20Instead%20of%20subgraph-based%0Aexplanations%2C%20we%20offer%20CE-based%20explanations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12654v1&entry.124074799=Read"},
{"title": "Truncated Variance Reduced Value Iteration", "author": "Yujia Jin and Ishani Karmarkar and Aaron Sidford and Jiayi Wang", "abstract": "  We provide faster randomized algorithms for computing an $\\epsilon$-optimal\npolicy in a discounted Markov decision process with\n$A_{\\text{tot}}$-state-action pairs, bounded rewards, and discount factor\n$\\gamma$. We provide an $\\tilde{O}(A_{\\text{tot}}[(1 -\n\\gamma)^{-3}\\epsilon^{-2} + (1 - \\gamma)^{-2}])$-time algorithm in the sampling\nsetting, where the probability transition matrix is unknown but accessible\nthrough a generative model which can be queried in $\\tilde{O}(1)$-time, and an\n$\\tilde{O}(s + (1-\\gamma)^{-2})$-time algorithm in the offline setting where\nthe probability transition matrix is known and $s$-sparse. These results\nimprove upon the prior state-of-the-art which either ran in\n$\\tilde{O}(A_{\\text{tot}}[(1 - \\gamma)^{-3}\\epsilon^{-2} + (1 - \\gamma)^{-3}])$\ntime [Sidford, Wang, Wu, Ye 2018] in the sampling setting, $\\tilde{O}(s +\nA_{\\text{tot}} (1-\\gamma)^{-3})$ time [Sidford, Wang, Wu, Yang, Ye 2018] in the\noffline setting, or time at least quadratic in the number of states using\ninterior point methods for linear programming. We achieve our results by\nbuilding upon prior stochastic variance-reduced value iteration methods\n[Sidford, Wang, Wu, Yang, Ye 2018]. We provide a variant that carefully\ntruncates the progress of its iterates to improve the variance of new\nvariance-reduced sampling procedures that we introduce to implement the steps.\nOur method is essentially model-free and can be implemented in\n$\\tilde{O}(A_{\\text{tot}})$-space when given generative model access.\nConsequently, our results take a step in closing the sample-complexity gap\nbetween model-free and model-based methods.\n", "link": "http://arxiv.org/abs/2405.12952v1", "date": "2024-05-21", "relevancy": 1.8389, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4888}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4457}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Truncated%20Variance%20Reduced%20Value%20Iteration&body=Title%3A%20Truncated%20Variance%20Reduced%20Value%20Iteration%0AAuthor%3A%20Yujia%20Jin%20and%20Ishani%20Karmarkar%20and%20Aaron%20Sidford%20and%20Jiayi%20Wang%0AAbstract%3A%20%20%20We%20provide%20faster%20randomized%20algorithms%20for%20computing%20an%20%24%5Cepsilon%24-optimal%0Apolicy%20in%20a%20discounted%20Markov%20decision%20process%20with%0A%24A_%7B%5Ctext%7Btot%7D%7D%24-state-action%20pairs%2C%20bounded%20rewards%2C%20and%20discount%20factor%0A%24%5Cgamma%24.%20We%20provide%20an%20%24%5Ctilde%7BO%7D%28A_%7B%5Ctext%7Btot%7D%7D%5B%281%20-%0A%5Cgamma%29%5E%7B-3%7D%5Cepsilon%5E%7B-2%7D%20%2B%20%281%20-%20%5Cgamma%29%5E%7B-2%7D%5D%29%24-time%20algorithm%20in%20the%20sampling%0Asetting%2C%20where%20the%20probability%20transition%20matrix%20is%20unknown%20but%20accessible%0Athrough%20a%20generative%20model%20which%20can%20be%20queried%20in%20%24%5Ctilde%7BO%7D%281%29%24-time%2C%20and%20an%0A%24%5Ctilde%7BO%7D%28s%20%2B%20%281-%5Cgamma%29%5E%7B-2%7D%29%24-time%20algorithm%20in%20the%20offline%20setting%20where%0Athe%20probability%20transition%20matrix%20is%20known%20and%20%24s%24-sparse.%20These%20results%0Aimprove%20upon%20the%20prior%20state-of-the-art%20which%20either%20ran%20in%0A%24%5Ctilde%7BO%7D%28A_%7B%5Ctext%7Btot%7D%7D%5B%281%20-%20%5Cgamma%29%5E%7B-3%7D%5Cepsilon%5E%7B-2%7D%20%2B%20%281%20-%20%5Cgamma%29%5E%7B-3%7D%5D%29%24%0Atime%20%5BSidford%2C%20Wang%2C%20Wu%2C%20Ye%202018%5D%20in%20the%20sampling%20setting%2C%20%24%5Ctilde%7BO%7D%28s%20%2B%0AA_%7B%5Ctext%7Btot%7D%7D%20%281-%5Cgamma%29%5E%7B-3%7D%29%24%20time%20%5BSidford%2C%20Wang%2C%20Wu%2C%20Yang%2C%20Ye%202018%5D%20in%20the%0Aoffline%20setting%2C%20or%20time%20at%20least%20quadratic%20in%20the%20number%20of%20states%20using%0Ainterior%20point%20methods%20for%20linear%20programming.%20We%20achieve%20our%20results%20by%0Abuilding%20upon%20prior%20stochastic%20variance-reduced%20value%20iteration%20methods%0A%5BSidford%2C%20Wang%2C%20Wu%2C%20Yang%2C%20Ye%202018%5D.%20We%20provide%20a%20variant%20that%20carefully%0Atruncates%20the%20progress%20of%20its%20iterates%20to%20improve%20the%20variance%20of%20new%0Avariance-reduced%20sampling%20procedures%20that%20we%20introduce%20to%20implement%20the%20steps.%0AOur%20method%20is%20essentially%20model-free%20and%20can%20be%20implemented%20in%0A%24%5Ctilde%7BO%7D%28A_%7B%5Ctext%7Btot%7D%7D%29%24-space%20when%20given%20generative%20model%20access.%0AConsequently%2C%20our%20results%20take%20a%20step%20in%20closing%20the%20sample-complexity%20gap%0Abetween%20model-free%20and%20model-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTruncated%2520Variance%2520Reduced%2520Value%2520Iteration%26entry.906535625%3DYujia%2520Jin%2520and%2520Ishani%2520Karmarkar%2520and%2520Aaron%2520Sidford%2520and%2520Jiayi%2520Wang%26entry.1292438233%3D%2520%2520We%2520provide%2520faster%2520randomized%2520algorithms%2520for%2520computing%2520an%2520%2524%255Cepsilon%2524-optimal%250Apolicy%2520in%2520a%2520discounted%2520Markov%2520decision%2520process%2520with%250A%2524A_%257B%255Ctext%257Btot%257D%257D%2524-state-action%2520pairs%252C%2520bounded%2520rewards%252C%2520and%2520discount%2520factor%250A%2524%255Cgamma%2524.%2520We%2520provide%2520an%2520%2524%255Ctilde%257BO%257D%2528A_%257B%255Ctext%257Btot%257D%257D%255B%25281%2520-%250A%255Cgamma%2529%255E%257B-3%257D%255Cepsilon%255E%257B-2%257D%2520%252B%2520%25281%2520-%2520%255Cgamma%2529%255E%257B-2%257D%255D%2529%2524-time%2520algorithm%2520in%2520the%2520sampling%250Asetting%252C%2520where%2520the%2520probability%2520transition%2520matrix%2520is%2520unknown%2520but%2520accessible%250Athrough%2520a%2520generative%2520model%2520which%2520can%2520be%2520queried%2520in%2520%2524%255Ctilde%257BO%257D%25281%2529%2524-time%252C%2520and%2520an%250A%2524%255Ctilde%257BO%257D%2528s%2520%252B%2520%25281-%255Cgamma%2529%255E%257B-2%257D%2529%2524-time%2520algorithm%2520in%2520the%2520offline%2520setting%2520where%250Athe%2520probability%2520transition%2520matrix%2520is%2520known%2520and%2520%2524s%2524-sparse.%2520These%2520results%250Aimprove%2520upon%2520the%2520prior%2520state-of-the-art%2520which%2520either%2520ran%2520in%250A%2524%255Ctilde%257BO%257D%2528A_%257B%255Ctext%257Btot%257D%257D%255B%25281%2520-%2520%255Cgamma%2529%255E%257B-3%257D%255Cepsilon%255E%257B-2%257D%2520%252B%2520%25281%2520-%2520%255Cgamma%2529%255E%257B-3%257D%255D%2529%2524%250Atime%2520%255BSidford%252C%2520Wang%252C%2520Wu%252C%2520Ye%25202018%255D%2520in%2520the%2520sampling%2520setting%252C%2520%2524%255Ctilde%257BO%257D%2528s%2520%252B%250AA_%257B%255Ctext%257Btot%257D%257D%2520%25281-%255Cgamma%2529%255E%257B-3%257D%2529%2524%2520time%2520%255BSidford%252C%2520Wang%252C%2520Wu%252C%2520Yang%252C%2520Ye%25202018%255D%2520in%2520the%250Aoffline%2520setting%252C%2520or%2520time%2520at%2520least%2520quadratic%2520in%2520the%2520number%2520of%2520states%2520using%250Ainterior%2520point%2520methods%2520for%2520linear%2520programming.%2520We%2520achieve%2520our%2520results%2520by%250Abuilding%2520upon%2520prior%2520stochastic%2520variance-reduced%2520value%2520iteration%2520methods%250A%255BSidford%252C%2520Wang%252C%2520Wu%252C%2520Yang%252C%2520Ye%25202018%255D.%2520We%2520provide%2520a%2520variant%2520that%2520carefully%250Atruncates%2520the%2520progress%2520of%2520its%2520iterates%2520to%2520improve%2520the%2520variance%2520of%2520new%250Avariance-reduced%2520sampling%2520procedures%2520that%2520we%2520introduce%2520to%2520implement%2520the%2520steps.%250AOur%2520method%2520is%2520essentially%2520model-free%2520and%2520can%2520be%2520implemented%2520in%250A%2524%255Ctilde%257BO%257D%2528A_%257B%255Ctext%257Btot%257D%257D%2529%2524-space%2520when%2520given%2520generative%2520model%2520access.%250AConsequently%252C%2520our%2520results%2520take%2520a%2520step%2520in%2520closing%2520the%2520sample-complexity%2520gap%250Abetween%2520model-free%2520and%2520model-based%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Truncated%20Variance%20Reduced%20Value%20Iteration&entry.906535625=Yujia%20Jin%20and%20Ishani%20Karmarkar%20and%20Aaron%20Sidford%20and%20Jiayi%20Wang&entry.1292438233=%20%20We%20provide%20faster%20randomized%20algorithms%20for%20computing%20an%20%24%5Cepsilon%24-optimal%0Apolicy%20in%20a%20discounted%20Markov%20decision%20process%20with%0A%24A_%7B%5Ctext%7Btot%7D%7D%24-state-action%20pairs%2C%20bounded%20rewards%2C%20and%20discount%20factor%0A%24%5Cgamma%24.%20We%20provide%20an%20%24%5Ctilde%7BO%7D%28A_%7B%5Ctext%7Btot%7D%7D%5B%281%20-%0A%5Cgamma%29%5E%7B-3%7D%5Cepsilon%5E%7B-2%7D%20%2B%20%281%20-%20%5Cgamma%29%5E%7B-2%7D%5D%29%24-time%20algorithm%20in%20the%20sampling%0Asetting%2C%20where%20the%20probability%20transition%20matrix%20is%20unknown%20but%20accessible%0Athrough%20a%20generative%20model%20which%20can%20be%20queried%20in%20%24%5Ctilde%7BO%7D%281%29%24-time%2C%20and%20an%0A%24%5Ctilde%7BO%7D%28s%20%2B%20%281-%5Cgamma%29%5E%7B-2%7D%29%24-time%20algorithm%20in%20the%20offline%20setting%20where%0Athe%20probability%20transition%20matrix%20is%20known%20and%20%24s%24-sparse.%20These%20results%0Aimprove%20upon%20the%20prior%20state-of-the-art%20which%20either%20ran%20in%0A%24%5Ctilde%7BO%7D%28A_%7B%5Ctext%7Btot%7D%7D%5B%281%20-%20%5Cgamma%29%5E%7B-3%7D%5Cepsilon%5E%7B-2%7D%20%2B%20%281%20-%20%5Cgamma%29%5E%7B-3%7D%5D%29%24%0Atime%20%5BSidford%2C%20Wang%2C%20Wu%2C%20Ye%202018%5D%20in%20the%20sampling%20setting%2C%20%24%5Ctilde%7BO%7D%28s%20%2B%0AA_%7B%5Ctext%7Btot%7D%7D%20%281-%5Cgamma%29%5E%7B-3%7D%29%24%20time%20%5BSidford%2C%20Wang%2C%20Wu%2C%20Yang%2C%20Ye%202018%5D%20in%20the%0Aoffline%20setting%2C%20or%20time%20at%20least%20quadratic%20in%20the%20number%20of%20states%20using%0Ainterior%20point%20methods%20for%20linear%20programming.%20We%20achieve%20our%20results%20by%0Abuilding%20upon%20prior%20stochastic%20variance-reduced%20value%20iteration%20methods%0A%5BSidford%2C%20Wang%2C%20Wu%2C%20Yang%2C%20Ye%202018%5D.%20We%20provide%20a%20variant%20that%20carefully%0Atruncates%20the%20progress%20of%20its%20iterates%20to%20improve%20the%20variance%20of%20new%0Avariance-reduced%20sampling%20procedures%20that%20we%20introduce%20to%20implement%20the%20steps.%0AOur%20method%20is%20essentially%20model-free%20and%20can%20be%20implemented%20in%0A%24%5Ctilde%7BO%7D%28A_%7B%5Ctext%7Btot%7D%7D%29%24-space%20when%20given%20generative%20model%20access.%0AConsequently%2C%20our%20results%20take%20a%20step%20in%20closing%20the%20sample-complexity%20gap%0Abetween%20model-free%20and%20model-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12952v1&entry.124074799=Read"},
{"title": "Verifying message-passing neural networks via topology-based bounds\n  tightening", "author": "Christopher Hojny and Shiqiang Zhang and Juan S. Campos and Ruth Misener", "abstract": "  Since graph neural networks (GNNs) are often vulnerable to attack, we need to\nknow when we can trust them. We develop a computationally effective approach\ntowards providing robust certificates for message-passing neural networks\n(MPNNs) using a Rectified Linear Unit (ReLU) activation function. Because our\nwork builds on mixed-integer optimization, it encodes a wide variety of\nsubproblems, for example it admits (i) both adding and removing edges, (ii)\nboth global and local budgets, and (iii) both topological perturbations and\nfeature modifications. Our key technology, topology-based bounds tightening,\nuses graph structure to tighten bounds. We also experiment with aggressive\nbounds tightening to dynamically change the optimization constraints by\ntightening variable bounds. To demonstrate the effectiveness of these\nstrategies, we implement an extension to the open-source branch-and-cut solver\nSCIP. We test on both node and graph classification problems and consider\ntopological attacks that both add and remove edges.\n", "link": "http://arxiv.org/abs/2402.13937v2", "date": "2024-05-21", "relevancy": 1.8347, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4741}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4482}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Verifying%20message-passing%20neural%20networks%20via%20topology-based%20bounds%0A%20%20tightening&body=Title%3A%20Verifying%20message-passing%20neural%20networks%20via%20topology-based%20bounds%0A%20%20tightening%0AAuthor%3A%20Christopher%20Hojny%20and%20Shiqiang%20Zhang%20and%20Juan%20S.%20Campos%20and%20Ruth%20Misener%0AAbstract%3A%20%20%20Since%20graph%20neural%20networks%20%28GNNs%29%20are%20often%20vulnerable%20to%20attack%2C%20we%20need%20to%0Aknow%20when%20we%20can%20trust%20them.%20We%20develop%20a%20computationally%20effective%20approach%0Atowards%20providing%20robust%20certificates%20for%20message-passing%20neural%20networks%0A%28MPNNs%29%20using%20a%20Rectified%20Linear%20Unit%20%28ReLU%29%20activation%20function.%20Because%20our%0Awork%20builds%20on%20mixed-integer%20optimization%2C%20it%20encodes%20a%20wide%20variety%20of%0Asubproblems%2C%20for%20example%20it%20admits%20%28i%29%20both%20adding%20and%20removing%20edges%2C%20%28ii%29%0Aboth%20global%20and%20local%20budgets%2C%20and%20%28iii%29%20both%20topological%20perturbations%20and%0Afeature%20modifications.%20Our%20key%20technology%2C%20topology-based%20bounds%20tightening%2C%0Auses%20graph%20structure%20to%20tighten%20bounds.%20We%20also%20experiment%20with%20aggressive%0Abounds%20tightening%20to%20dynamically%20change%20the%20optimization%20constraints%20by%0Atightening%20variable%20bounds.%20To%20demonstrate%20the%20effectiveness%20of%20these%0Astrategies%2C%20we%20implement%20an%20extension%20to%20the%20open-source%20branch-and-cut%20solver%0ASCIP.%20We%20test%20on%20both%20node%20and%20graph%20classification%20problems%20and%20consider%0Atopological%20attacks%20that%20both%20add%20and%20remove%20edges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13937v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerifying%2520message-passing%2520neural%2520networks%2520via%2520topology-based%2520bounds%250A%2520%2520tightening%26entry.906535625%3DChristopher%2520Hojny%2520and%2520Shiqiang%2520Zhang%2520and%2520Juan%2520S.%2520Campos%2520and%2520Ruth%2520Misener%26entry.1292438233%3D%2520%2520Since%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520are%2520often%2520vulnerable%2520to%2520attack%252C%2520we%2520need%2520to%250Aknow%2520when%2520we%2520can%2520trust%2520them.%2520We%2520develop%2520a%2520computationally%2520effective%2520approach%250Atowards%2520providing%2520robust%2520certificates%2520for%2520message-passing%2520neural%2520networks%250A%2528MPNNs%2529%2520using%2520a%2520Rectified%2520Linear%2520Unit%2520%2528ReLU%2529%2520activation%2520function.%2520Because%2520our%250Awork%2520builds%2520on%2520mixed-integer%2520optimization%252C%2520it%2520encodes%2520a%2520wide%2520variety%2520of%250Asubproblems%252C%2520for%2520example%2520it%2520admits%2520%2528i%2529%2520both%2520adding%2520and%2520removing%2520edges%252C%2520%2528ii%2529%250Aboth%2520global%2520and%2520local%2520budgets%252C%2520and%2520%2528iii%2529%2520both%2520topological%2520perturbations%2520and%250Afeature%2520modifications.%2520Our%2520key%2520technology%252C%2520topology-based%2520bounds%2520tightening%252C%250Auses%2520graph%2520structure%2520to%2520tighten%2520bounds.%2520We%2520also%2520experiment%2520with%2520aggressive%250Abounds%2520tightening%2520to%2520dynamically%2520change%2520the%2520optimization%2520constraints%2520by%250Atightening%2520variable%2520bounds.%2520To%2520demonstrate%2520the%2520effectiveness%2520of%2520these%250Astrategies%252C%2520we%2520implement%2520an%2520extension%2520to%2520the%2520open-source%2520branch-and-cut%2520solver%250ASCIP.%2520We%2520test%2520on%2520both%2520node%2520and%2520graph%2520classification%2520problems%2520and%2520consider%250Atopological%2520attacks%2520that%2520both%2520add%2520and%2520remove%2520edges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13937v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Verifying%20message-passing%20neural%20networks%20via%20topology-based%20bounds%0A%20%20tightening&entry.906535625=Christopher%20Hojny%20and%20Shiqiang%20Zhang%20and%20Juan%20S.%20Campos%20and%20Ruth%20Misener&entry.1292438233=%20%20Since%20graph%20neural%20networks%20%28GNNs%29%20are%20often%20vulnerable%20to%20attack%2C%20we%20need%20to%0Aknow%20when%20we%20can%20trust%20them.%20We%20develop%20a%20computationally%20effective%20approach%0Atowards%20providing%20robust%20certificates%20for%20message-passing%20neural%20networks%0A%28MPNNs%29%20using%20a%20Rectified%20Linear%20Unit%20%28ReLU%29%20activation%20function.%20Because%20our%0Awork%20builds%20on%20mixed-integer%20optimization%2C%20it%20encodes%20a%20wide%20variety%20of%0Asubproblems%2C%20for%20example%20it%20admits%20%28i%29%20both%20adding%20and%20removing%20edges%2C%20%28ii%29%0Aboth%20global%20and%20local%20budgets%2C%20and%20%28iii%29%20both%20topological%20perturbations%20and%0Afeature%20modifications.%20Our%20key%20technology%2C%20topology-based%20bounds%20tightening%2C%0Auses%20graph%20structure%20to%20tighten%20bounds.%20We%20also%20experiment%20with%20aggressive%0Abounds%20tightening%20to%20dynamically%20change%20the%20optimization%20constraints%20by%0Atightening%20variable%20bounds.%20To%20demonstrate%20the%20effectiveness%20of%20these%0Astrategies%2C%20we%20implement%20an%20extension%20to%20the%20open-source%20branch-and-cut%20solver%0ASCIP.%20We%20test%20on%20both%20node%20and%20graph%20classification%20problems%20and%20consider%0Atopological%20attacks%20that%20both%20add%20and%20remove%20edges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13937v2&entry.124074799=Read"},
{"title": "Wav-KAN: Wavelet Kolmogorov-Arnold Networks", "author": "Zavareh Bozorgasl and Hao Chen", "abstract": "  In this paper , we introduce Wav-KAN, an innovative neural network\narchitecture that leverages the Wavelet Kolmogorov-Arnold Networks (Wav-KAN)\nframework to enhance interpretability and performance. Traditional multilayer\nperceptrons (MLPs) and even recent advancements like Spl-KAN face challenges\nrelated to interpretability, training speed, robustness, computational\nefficiency, and performance. Wav-KAN addresses these limitations by\nincorporating wavelet functions into the Kolmogorov-Arnold network structure,\nenabling the network to capture both high-frequency and low-frequency\ncomponents of the input data efficiently. Wavelet-based approximations employ\northogonal or semi-orthogonal basis and also maintains a balance between\naccurately representing the underlying data structure and avoiding overfitting\nto the noise. Analogous to how water conforms to the shape of its container,\nWav-KAN adapts to the data structure, resulting in enhanced accuracy, faster\ntraining speeds, and increased robustness compared to Spl-KAN and MLPs. Our\nresults highlight the potential of Wav-KAN as a powerful tool for developing\ninterpretable and high-performance neural networks, with applications spanning\nvarious fields. This work sets the stage for further exploration and\nimplementation of Wav-KAN in frameworks such as PyTorch, TensorFlow, and also\nit makes wavelet in KAN in wide-spread usage like nowadays activation functions\nlike ReLU, sigmoid in universal approximation theory (UAT).\n", "link": "http://arxiv.org/abs/2405.12832v1", "date": "2024-05-21", "relevancy": 1.8264, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4746}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4579}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wav-KAN%3A%20Wavelet%20Kolmogorov-Arnold%20Networks&body=Title%3A%20Wav-KAN%3A%20Wavelet%20Kolmogorov-Arnold%20Networks%0AAuthor%3A%20Zavareh%20Bozorgasl%20and%20Hao%20Chen%0AAbstract%3A%20%20%20In%20this%20paper%20%2C%20we%20introduce%20Wav-KAN%2C%20an%20innovative%20neural%20network%0Aarchitecture%20that%20leverages%20the%20Wavelet%20Kolmogorov-Arnold%20Networks%20%28Wav-KAN%29%0Aframework%20to%20enhance%20interpretability%20and%20performance.%20Traditional%20multilayer%0Aperceptrons%20%28MLPs%29%20and%20even%20recent%20advancements%20like%20Spl-KAN%20face%20challenges%0Arelated%20to%20interpretability%2C%20training%20speed%2C%20robustness%2C%20computational%0Aefficiency%2C%20and%20performance.%20Wav-KAN%20addresses%20these%20limitations%20by%0Aincorporating%20wavelet%20functions%20into%20the%20Kolmogorov-Arnold%20network%20structure%2C%0Aenabling%20the%20network%20to%20capture%20both%20high-frequency%20and%20low-frequency%0Acomponents%20of%20the%20input%20data%20efficiently.%20Wavelet-based%20approximations%20employ%0Aorthogonal%20or%20semi-orthogonal%20basis%20and%20also%20maintains%20a%20balance%20between%0Aaccurately%20representing%20the%20underlying%20data%20structure%20and%20avoiding%20overfitting%0Ato%20the%20noise.%20Analogous%20to%20how%20water%20conforms%20to%20the%20shape%20of%20its%20container%2C%0AWav-KAN%20adapts%20to%20the%20data%20structure%2C%20resulting%20in%20enhanced%20accuracy%2C%20faster%0Atraining%20speeds%2C%20and%20increased%20robustness%20compared%20to%20Spl-KAN%20and%20MLPs.%20Our%0Aresults%20highlight%20the%20potential%20of%20Wav-KAN%20as%20a%20powerful%20tool%20for%20developing%0Ainterpretable%20and%20high-performance%20neural%20networks%2C%20with%20applications%20spanning%0Avarious%20fields.%20This%20work%20sets%20the%20stage%20for%20further%20exploration%20and%0Aimplementation%20of%20Wav-KAN%20in%20frameworks%20such%20as%20PyTorch%2C%20TensorFlow%2C%20and%20also%0Ait%20makes%20wavelet%20in%20KAN%20in%20wide-spread%20usage%20like%20nowadays%20activation%20functions%0Alike%20ReLU%2C%20sigmoid%20in%20universal%20approximation%20theory%20%28UAT%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWav-KAN%253A%2520Wavelet%2520Kolmogorov-Arnold%2520Networks%26entry.906535625%3DZavareh%2520Bozorgasl%2520and%2520Hao%2520Chen%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520%252C%2520we%2520introduce%2520Wav-KAN%252C%2520an%2520innovative%2520neural%2520network%250Aarchitecture%2520that%2520leverages%2520the%2520Wavelet%2520Kolmogorov-Arnold%2520Networks%2520%2528Wav-KAN%2529%250Aframework%2520to%2520enhance%2520interpretability%2520and%2520performance.%2520Traditional%2520multilayer%250Aperceptrons%2520%2528MLPs%2529%2520and%2520even%2520recent%2520advancements%2520like%2520Spl-KAN%2520face%2520challenges%250Arelated%2520to%2520interpretability%252C%2520training%2520speed%252C%2520robustness%252C%2520computational%250Aefficiency%252C%2520and%2520performance.%2520Wav-KAN%2520addresses%2520these%2520limitations%2520by%250Aincorporating%2520wavelet%2520functions%2520into%2520the%2520Kolmogorov-Arnold%2520network%2520structure%252C%250Aenabling%2520the%2520network%2520to%2520capture%2520both%2520high-frequency%2520and%2520low-frequency%250Acomponents%2520of%2520the%2520input%2520data%2520efficiently.%2520Wavelet-based%2520approximations%2520employ%250Aorthogonal%2520or%2520semi-orthogonal%2520basis%2520and%2520also%2520maintains%2520a%2520balance%2520between%250Aaccurately%2520representing%2520the%2520underlying%2520data%2520structure%2520and%2520avoiding%2520overfitting%250Ato%2520the%2520noise.%2520Analogous%2520to%2520how%2520water%2520conforms%2520to%2520the%2520shape%2520of%2520its%2520container%252C%250AWav-KAN%2520adapts%2520to%2520the%2520data%2520structure%252C%2520resulting%2520in%2520enhanced%2520accuracy%252C%2520faster%250Atraining%2520speeds%252C%2520and%2520increased%2520robustness%2520compared%2520to%2520Spl-KAN%2520and%2520MLPs.%2520Our%250Aresults%2520highlight%2520the%2520potential%2520of%2520Wav-KAN%2520as%2520a%2520powerful%2520tool%2520for%2520developing%250Ainterpretable%2520and%2520high-performance%2520neural%2520networks%252C%2520with%2520applications%2520spanning%250Avarious%2520fields.%2520This%2520work%2520sets%2520the%2520stage%2520for%2520further%2520exploration%2520and%250Aimplementation%2520of%2520Wav-KAN%2520in%2520frameworks%2520such%2520as%2520PyTorch%252C%2520TensorFlow%252C%2520and%2520also%250Ait%2520makes%2520wavelet%2520in%2520KAN%2520in%2520wide-spread%2520usage%2520like%2520nowadays%2520activation%2520functions%250Alike%2520ReLU%252C%2520sigmoid%2520in%2520universal%2520approximation%2520theory%2520%2528UAT%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wav-KAN%3A%20Wavelet%20Kolmogorov-Arnold%20Networks&entry.906535625=Zavareh%20Bozorgasl%20and%20Hao%20Chen&entry.1292438233=%20%20In%20this%20paper%20%2C%20we%20introduce%20Wav-KAN%2C%20an%20innovative%20neural%20network%0Aarchitecture%20that%20leverages%20the%20Wavelet%20Kolmogorov-Arnold%20Networks%20%28Wav-KAN%29%0Aframework%20to%20enhance%20interpretability%20and%20performance.%20Traditional%20multilayer%0Aperceptrons%20%28MLPs%29%20and%20even%20recent%20advancements%20like%20Spl-KAN%20face%20challenges%0Arelated%20to%20interpretability%2C%20training%20speed%2C%20robustness%2C%20computational%0Aefficiency%2C%20and%20performance.%20Wav-KAN%20addresses%20these%20limitations%20by%0Aincorporating%20wavelet%20functions%20into%20the%20Kolmogorov-Arnold%20network%20structure%2C%0Aenabling%20the%20network%20to%20capture%20both%20high-frequency%20and%20low-frequency%0Acomponents%20of%20the%20input%20data%20efficiently.%20Wavelet-based%20approximations%20employ%0Aorthogonal%20or%20semi-orthogonal%20basis%20and%20also%20maintains%20a%20balance%20between%0Aaccurately%20representing%20the%20underlying%20data%20structure%20and%20avoiding%20overfitting%0Ato%20the%20noise.%20Analogous%20to%20how%20water%20conforms%20to%20the%20shape%20of%20its%20container%2C%0AWav-KAN%20adapts%20to%20the%20data%20structure%2C%20resulting%20in%20enhanced%20accuracy%2C%20faster%0Atraining%20speeds%2C%20and%20increased%20robustness%20compared%20to%20Spl-KAN%20and%20MLPs.%20Our%0Aresults%20highlight%20the%20potential%20of%20Wav-KAN%20as%20a%20powerful%20tool%20for%20developing%0Ainterpretable%20and%20high-performance%20neural%20networks%2C%20with%20applications%20spanning%0Avarious%20fields.%20This%20work%20sets%20the%20stage%20for%20further%20exploration%20and%0Aimplementation%20of%20Wav-KAN%20in%20frameworks%20such%20as%20PyTorch%2C%20TensorFlow%2C%20and%20also%0Ait%20makes%20wavelet%20in%20KAN%20in%20wide-spread%20usage%20like%20nowadays%20activation%20functions%0Alike%20ReLU%2C%20sigmoid%20in%20universal%20approximation%20theory%20%28UAT%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12832v1&entry.124074799=Read"},
{"title": "Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data", "author": "Thomas Lips and Victor-Louis De Gusseme and Francis wyffels", "abstract": "  Assistive robots should be able to wash, fold or iron clothes. However, due\nto the variety, deformability and self-occlusions of clothes, creating robot\nsystems for cloth manipulation is challenging. Synthetic data is a promising\ndirection to improve generalization, but the sim-to-real gap limits its\neffectiveness. To advance the use of synthetic data for cloth manipulation\ntasks such as robotic folding, we present a synthetic data pipeline to train\nkeypoint detectors for almost-flattened cloth items. To evaluate its\nperformance, we have also collected a real-world dataset. We train detectors\nfor both T-shirts, towels and shorts and obtain an average precision of 64% and\nan average keypoint distance of 18 pixels. Fine-tuning on real-world data\nimproves performance to 74% mAP and an average distance of only 9 pixels.\nFurthermore, we describe failure modes of the keypoint detectors and compare\ndifferent approaches to obtain cloth meshes and materials. We also quantify the\nremaining sim-to-real gap and argue that further improvements to the fidelity\nof cloth assets will be required to further reduce this gap. The code, dataset\nand trained models are available\n", "link": "http://arxiv.org/abs/2401.01734v2", "date": "2024-05-21", "relevancy": 1.7109, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6073}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5291}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Keypoints%20for%20Robotic%20Cloth%20Manipulation%20using%20Synthetic%20Data&body=Title%3A%20Learning%20Keypoints%20for%20Robotic%20Cloth%20Manipulation%20using%20Synthetic%20Data%0AAuthor%3A%20Thomas%20Lips%20and%20Victor-Louis%20De%20Gusseme%20and%20Francis%20wyffels%0AAbstract%3A%20%20%20Assistive%20robots%20should%20be%20able%20to%20wash%2C%20fold%20or%20iron%20clothes.%20However%2C%20due%0Ato%20the%20variety%2C%20deformability%20and%20self-occlusions%20of%20clothes%2C%20creating%20robot%0Asystems%20for%20cloth%20manipulation%20is%20challenging.%20Synthetic%20data%20is%20a%20promising%0Adirection%20to%20improve%20generalization%2C%20but%20the%20sim-to-real%20gap%20limits%20its%0Aeffectiveness.%20To%20advance%20the%20use%20of%20synthetic%20data%20for%20cloth%20manipulation%0Atasks%20such%20as%20robotic%20folding%2C%20we%20present%20a%20synthetic%20data%20pipeline%20to%20train%0Akeypoint%20detectors%20for%20almost-flattened%20cloth%20items.%20To%20evaluate%20its%0Aperformance%2C%20we%20have%20also%20collected%20a%20real-world%20dataset.%20We%20train%20detectors%0Afor%20both%20T-shirts%2C%20towels%20and%20shorts%20and%20obtain%20an%20average%20precision%20of%2064%25%20and%0Aan%20average%20keypoint%20distance%20of%2018%20pixels.%20Fine-tuning%20on%20real-world%20data%0Aimproves%20performance%20to%2074%25%20mAP%20and%20an%20average%20distance%20of%20only%209%20pixels.%0AFurthermore%2C%20we%20describe%20failure%20modes%20of%20the%20keypoint%20detectors%20and%20compare%0Adifferent%20approaches%20to%20obtain%20cloth%20meshes%20and%20materials.%20We%20also%20quantify%20the%0Aremaining%20sim-to-real%20gap%20and%20argue%20that%20further%20improvements%20to%20the%20fidelity%0Aof%20cloth%20assets%20will%20be%20required%20to%20further%20reduce%20this%20gap.%20The%20code%2C%20dataset%0Aand%20trained%20models%20are%20available%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01734v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Keypoints%2520for%2520Robotic%2520Cloth%2520Manipulation%2520using%2520Synthetic%2520Data%26entry.906535625%3DThomas%2520Lips%2520and%2520Victor-Louis%2520De%2520Gusseme%2520and%2520Francis%2520wyffels%26entry.1292438233%3D%2520%2520Assistive%2520robots%2520should%2520be%2520able%2520to%2520wash%252C%2520fold%2520or%2520iron%2520clothes.%2520However%252C%2520due%250Ato%2520the%2520variety%252C%2520deformability%2520and%2520self-occlusions%2520of%2520clothes%252C%2520creating%2520robot%250Asystems%2520for%2520cloth%2520manipulation%2520is%2520challenging.%2520Synthetic%2520data%2520is%2520a%2520promising%250Adirection%2520to%2520improve%2520generalization%252C%2520but%2520the%2520sim-to-real%2520gap%2520limits%2520its%250Aeffectiveness.%2520To%2520advance%2520the%2520use%2520of%2520synthetic%2520data%2520for%2520cloth%2520manipulation%250Atasks%2520such%2520as%2520robotic%2520folding%252C%2520we%2520present%2520a%2520synthetic%2520data%2520pipeline%2520to%2520train%250Akeypoint%2520detectors%2520for%2520almost-flattened%2520cloth%2520items.%2520To%2520evaluate%2520its%250Aperformance%252C%2520we%2520have%2520also%2520collected%2520a%2520real-world%2520dataset.%2520We%2520train%2520detectors%250Afor%2520both%2520T-shirts%252C%2520towels%2520and%2520shorts%2520and%2520obtain%2520an%2520average%2520precision%2520of%252064%2525%2520and%250Aan%2520average%2520keypoint%2520distance%2520of%252018%2520pixels.%2520Fine-tuning%2520on%2520real-world%2520data%250Aimproves%2520performance%2520to%252074%2525%2520mAP%2520and%2520an%2520average%2520distance%2520of%2520only%25209%2520pixels.%250AFurthermore%252C%2520we%2520describe%2520failure%2520modes%2520of%2520the%2520keypoint%2520detectors%2520and%2520compare%250Adifferent%2520approaches%2520to%2520obtain%2520cloth%2520meshes%2520and%2520materials.%2520We%2520also%2520quantify%2520the%250Aremaining%2520sim-to-real%2520gap%2520and%2520argue%2520that%2520further%2520improvements%2520to%2520the%2520fidelity%250Aof%2520cloth%2520assets%2520will%2520be%2520required%2520to%2520further%2520reduce%2520this%2520gap.%2520The%2520code%252C%2520dataset%250Aand%2520trained%2520models%2520are%2520available%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.01734v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Keypoints%20for%20Robotic%20Cloth%20Manipulation%20using%20Synthetic%20Data&entry.906535625=Thomas%20Lips%20and%20Victor-Louis%20De%20Gusseme%20and%20Francis%20wyffels&entry.1292438233=%20%20Assistive%20robots%20should%20be%20able%20to%20wash%2C%20fold%20or%20iron%20clothes.%20However%2C%20due%0Ato%20the%20variety%2C%20deformability%20and%20self-occlusions%20of%20clothes%2C%20creating%20robot%0Asystems%20for%20cloth%20manipulation%20is%20challenging.%20Synthetic%20data%20is%20a%20promising%0Adirection%20to%20improve%20generalization%2C%20but%20the%20sim-to-real%20gap%20limits%20its%0Aeffectiveness.%20To%20advance%20the%20use%20of%20synthetic%20data%20for%20cloth%20manipulation%0Atasks%20such%20as%20robotic%20folding%2C%20we%20present%20a%20synthetic%20data%20pipeline%20to%20train%0Akeypoint%20detectors%20for%20almost-flattened%20cloth%20items.%20To%20evaluate%20its%0Aperformance%2C%20we%20have%20also%20collected%20a%20real-world%20dataset.%20We%20train%20detectors%0Afor%20both%20T-shirts%2C%20towels%20and%20shorts%20and%20obtain%20an%20average%20precision%20of%2064%25%20and%0Aan%20average%20keypoint%20distance%20of%2018%20pixels.%20Fine-tuning%20on%20real-world%20data%0Aimproves%20performance%20to%2074%25%20mAP%20and%20an%20average%20distance%20of%20only%209%20pixels.%0AFurthermore%2C%20we%20describe%20failure%20modes%20of%20the%20keypoint%20detectors%20and%20compare%0Adifferent%20approaches%20to%20obtain%20cloth%20meshes%20and%20materials.%20We%20also%20quantify%20the%0Aremaining%20sim-to-real%20gap%20and%20argue%20that%20further%20improvements%20to%20the%20fidelity%0Aof%20cloth%20assets%20will%20be%20required%20to%20further%20reduce%20this%20gap.%20The%20code%2C%20dataset%0Aand%20trained%20models%20are%20available%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01734v2&entry.124074799=Read"},
{"title": "Confidence-Aware Multi-Field Model Calibration", "author": "Yuang Zhao and Chuhan Wu and Qinglin Jia and Hong Zhu and Jia Yan and Libin Zong and Linxuan Zhang and Zhenhua Dong and Muyu Zhang", "abstract": "  Accurately predicting the probabilities of user feedback, such as clicks and\nconversions, is critical for advertisement ranking and bidding. However, there\noften exist unwanted mismatches between predicted probabilities and true\nlikelihoods due to the rapid shift of data distributions and intrinsic model\nbiases. Calibration aims to address this issue by post-processing model\npredictions, and field-aware calibration can adjust model output on different\nfeature field values to satisfy fine-grained advertising demands.\nUnfortunately, the observed samples corresponding to certain field values can\nbe seriously limited to make confident calibrations, which may yield bias\namplification and online disturbance. In this paper, we propose a\nconfidence-aware multi-field calibration method, which adaptively adjusts the\ncalibration intensity based on confidence levels derived from sample\nstatistics. It also utilizes multiple fields for joint model calibration\naccording to their importance to mitigate the impact of data sparsity on a\nsingle field. Extensive offline and online experiments show the superiority of\nour method in boosting advertising performance and reducing prediction\ndeviations.\n", "link": "http://arxiv.org/abs/2402.17655v2", "date": "2024-05-21", "relevancy": 1.5112, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5466}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5028}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Confidence-Aware%20Multi-Field%20Model%20Calibration&body=Title%3A%20Confidence-Aware%20Multi-Field%20Model%20Calibration%0AAuthor%3A%20Yuang%20Zhao%20and%20Chuhan%20Wu%20and%20Qinglin%20Jia%20and%20Hong%20Zhu%20and%20Jia%20Yan%20and%20Libin%20Zong%20and%20Linxuan%20Zhang%20and%20Zhenhua%20Dong%20and%20Muyu%20Zhang%0AAbstract%3A%20%20%20Accurately%20predicting%20the%20probabilities%20of%20user%20feedback%2C%20such%20as%20clicks%20and%0Aconversions%2C%20is%20critical%20for%20advertisement%20ranking%20and%20bidding.%20However%2C%20there%0Aoften%20exist%20unwanted%20mismatches%20between%20predicted%20probabilities%20and%20true%0Alikelihoods%20due%20to%20the%20rapid%20shift%20of%20data%20distributions%20and%20intrinsic%20model%0Abiases.%20Calibration%20aims%20to%20address%20this%20issue%20by%20post-processing%20model%0Apredictions%2C%20and%20field-aware%20calibration%20can%20adjust%20model%20output%20on%20different%0Afeature%20field%20values%20to%20satisfy%20fine-grained%20advertising%20demands.%0AUnfortunately%2C%20the%20observed%20samples%20corresponding%20to%20certain%20field%20values%20can%0Abe%20seriously%20limited%20to%20make%20confident%20calibrations%2C%20which%20may%20yield%20bias%0Aamplification%20and%20online%20disturbance.%20In%20this%20paper%2C%20we%20propose%20a%0Aconfidence-aware%20multi-field%20calibration%20method%2C%20which%20adaptively%20adjusts%20the%0Acalibration%20intensity%20based%20on%20confidence%20levels%20derived%20from%20sample%0Astatistics.%20It%20also%20utilizes%20multiple%20fields%20for%20joint%20model%20calibration%0Aaccording%20to%20their%20importance%20to%20mitigate%20the%20impact%20of%20data%20sparsity%20on%20a%0Asingle%20field.%20Extensive%20offline%20and%20online%20experiments%20show%20the%20superiority%20of%0Aour%20method%20in%20boosting%20advertising%20performance%20and%20reducing%20prediction%0Adeviations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17655v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConfidence-Aware%2520Multi-Field%2520Model%2520Calibration%26entry.906535625%3DYuang%2520Zhao%2520and%2520Chuhan%2520Wu%2520and%2520Qinglin%2520Jia%2520and%2520Hong%2520Zhu%2520and%2520Jia%2520Yan%2520and%2520Libin%2520Zong%2520and%2520Linxuan%2520Zhang%2520and%2520Zhenhua%2520Dong%2520and%2520Muyu%2520Zhang%26entry.1292438233%3D%2520%2520Accurately%2520predicting%2520the%2520probabilities%2520of%2520user%2520feedback%252C%2520such%2520as%2520clicks%2520and%250Aconversions%252C%2520is%2520critical%2520for%2520advertisement%2520ranking%2520and%2520bidding.%2520However%252C%2520there%250Aoften%2520exist%2520unwanted%2520mismatches%2520between%2520predicted%2520probabilities%2520and%2520true%250Alikelihoods%2520due%2520to%2520the%2520rapid%2520shift%2520of%2520data%2520distributions%2520and%2520intrinsic%2520model%250Abiases.%2520Calibration%2520aims%2520to%2520address%2520this%2520issue%2520by%2520post-processing%2520model%250Apredictions%252C%2520and%2520field-aware%2520calibration%2520can%2520adjust%2520model%2520output%2520on%2520different%250Afeature%2520field%2520values%2520to%2520satisfy%2520fine-grained%2520advertising%2520demands.%250AUnfortunately%252C%2520the%2520observed%2520samples%2520corresponding%2520to%2520certain%2520field%2520values%2520can%250Abe%2520seriously%2520limited%2520to%2520make%2520confident%2520calibrations%252C%2520which%2520may%2520yield%2520bias%250Aamplification%2520and%2520online%2520disturbance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Aconfidence-aware%2520multi-field%2520calibration%2520method%252C%2520which%2520adaptively%2520adjusts%2520the%250Acalibration%2520intensity%2520based%2520on%2520confidence%2520levels%2520derived%2520from%2520sample%250Astatistics.%2520It%2520also%2520utilizes%2520multiple%2520fields%2520for%2520joint%2520model%2520calibration%250Aaccording%2520to%2520their%2520importance%2520to%2520mitigate%2520the%2520impact%2520of%2520data%2520sparsity%2520on%2520a%250Asingle%2520field.%2520Extensive%2520offline%2520and%2520online%2520experiments%2520show%2520the%2520superiority%2520of%250Aour%2520method%2520in%2520boosting%2520advertising%2520performance%2520and%2520reducing%2520prediction%250Adeviations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17655v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Confidence-Aware%20Multi-Field%20Model%20Calibration&entry.906535625=Yuang%20Zhao%20and%20Chuhan%20Wu%20and%20Qinglin%20Jia%20and%20Hong%20Zhu%20and%20Jia%20Yan%20and%20Libin%20Zong%20and%20Linxuan%20Zhang%20and%20Zhenhua%20Dong%20and%20Muyu%20Zhang&entry.1292438233=%20%20Accurately%20predicting%20the%20probabilities%20of%20user%20feedback%2C%20such%20as%20clicks%20and%0Aconversions%2C%20is%20critical%20for%20advertisement%20ranking%20and%20bidding.%20However%2C%20there%0Aoften%20exist%20unwanted%20mismatches%20between%20predicted%20probabilities%20and%20true%0Alikelihoods%20due%20to%20the%20rapid%20shift%20of%20data%20distributions%20and%20intrinsic%20model%0Abiases.%20Calibration%20aims%20to%20address%20this%20issue%20by%20post-processing%20model%0Apredictions%2C%20and%20field-aware%20calibration%20can%20adjust%20model%20output%20on%20different%0Afeature%20field%20values%20to%20satisfy%20fine-grained%20advertising%20demands.%0AUnfortunately%2C%20the%20observed%20samples%20corresponding%20to%20certain%20field%20values%20can%0Abe%20seriously%20limited%20to%20make%20confident%20calibrations%2C%20which%20may%20yield%20bias%0Aamplification%20and%20online%20disturbance.%20In%20this%20paper%2C%20we%20propose%20a%0Aconfidence-aware%20multi-field%20calibration%20method%2C%20which%20adaptively%20adjusts%20the%0Acalibration%20intensity%20based%20on%20confidence%20levels%20derived%20from%20sample%0Astatistics.%20It%20also%20utilizes%20multiple%20fields%20for%20joint%20model%20calibration%0Aaccording%20to%20their%20importance%20to%20mitigate%20the%20impact%20of%20data%20sparsity%20on%20a%0Asingle%20field.%20Extensive%20offline%20and%20online%20experiments%20show%20the%20superiority%20of%0Aour%20method%20in%20boosting%20advertising%20performance%20and%20reducing%20prediction%0Adeviations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17655v2&entry.124074799=Read"},
{"title": "Disentangling the Potential Impacts of Papers into Diffusion,\n  Conformity, and Contribution Values", "author": "Zhikai Xue and Guoxiu He and Zhuoren Jiang and Sichen Gu and Yangyang Kang and Star Zhao and Wei Lu", "abstract": "  The potential impact of an academic paper is determined by various factors,\nincluding its popularity and contribution. Existing models usually estimate\noriginal citation counts based on static graphs and fail to differentiate\nvalues from nuanced perspectives. In this study, we propose a novel graph\nneural network to Disentangle the Potential impacts of Papers into Diffusion,\nConformity, and Contribution values (called DPPDCC). Given a target paper,\nDPPDCC encodes temporal and structural features within the constructed dynamic\nheterogeneous graph. Particularly, to capture the knowledge flow, we emphasize\nthe importance of comparative and co-cited/citing information between papers\nand aggregate snapshots evolutionarily. To unravel popularity, we contrast\naugmented graphs to extract the essence of diffusion and predict the\naccumulated citation binning to model conformity. We further apply orthogonal\nconstraints to encourage distinct modeling of each perspective and preserve the\ninherent value of contribution. To evaluate models' generalization for papers\npublished at various times, we reformulate the problem by partitioning data\nbased on specific time points to mirror real-world conditions. Extensive\nexperimental results on three datasets demonstrate that DPPDCC significantly\noutperforms baselines for previously, freshly, and immediately published\npapers. Further analyses confirm its robust capabilities. We will make our\ndatasets and codes publicly available.\n", "link": "http://arxiv.org/abs/2311.09262v3", "date": "2024-05-21", "relevancy": 1.0059, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5081}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5078}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangling%20the%20Potential%20Impacts%20of%20Papers%20into%20Diffusion%2C%0A%20%20Conformity%2C%20and%20Contribution%20Values&body=Title%3A%20Disentangling%20the%20Potential%20Impacts%20of%20Papers%20into%20Diffusion%2C%0A%20%20Conformity%2C%20and%20Contribution%20Values%0AAuthor%3A%20Zhikai%20Xue%20and%20Guoxiu%20He%20and%20Zhuoren%20Jiang%20and%20Sichen%20Gu%20and%20Yangyang%20Kang%20and%20Star%20Zhao%20and%20Wei%20Lu%0AAbstract%3A%20%20%20The%20potential%20impact%20of%20an%20academic%20paper%20is%20determined%20by%20various%20factors%2C%0Aincluding%20its%20popularity%20and%20contribution.%20Existing%20models%20usually%20estimate%0Aoriginal%20citation%20counts%20based%20on%20static%20graphs%20and%20fail%20to%20differentiate%0Avalues%20from%20nuanced%20perspectives.%20In%20this%20study%2C%20we%20propose%20a%20novel%20graph%0Aneural%20network%20to%20Disentangle%20the%20Potential%20impacts%20of%20Papers%20into%20Diffusion%2C%0AConformity%2C%20and%20Contribution%20values%20%28called%20DPPDCC%29.%20Given%20a%20target%20paper%2C%0ADPPDCC%20encodes%20temporal%20and%20structural%20features%20within%20the%20constructed%20dynamic%0Aheterogeneous%20graph.%20Particularly%2C%20to%20capture%20the%20knowledge%20flow%2C%20we%20emphasize%0Athe%20importance%20of%20comparative%20and%20co-cited/citing%20information%20between%20papers%0Aand%20aggregate%20snapshots%20evolutionarily.%20To%20unravel%20popularity%2C%20we%20contrast%0Aaugmented%20graphs%20to%20extract%20the%20essence%20of%20diffusion%20and%20predict%20the%0Aaccumulated%20citation%20binning%20to%20model%20conformity.%20We%20further%20apply%20orthogonal%0Aconstraints%20to%20encourage%20distinct%20modeling%20of%20each%20perspective%20and%20preserve%20the%0Ainherent%20value%20of%20contribution.%20To%20evaluate%20models%27%20generalization%20for%20papers%0Apublished%20at%20various%20times%2C%20we%20reformulate%20the%20problem%20by%20partitioning%20data%0Abased%20on%20specific%20time%20points%20to%20mirror%20real-world%20conditions.%20Extensive%0Aexperimental%20results%20on%20three%20datasets%20demonstrate%20that%20DPPDCC%20significantly%0Aoutperforms%20baselines%20for%20previously%2C%20freshly%2C%20and%20immediately%20published%0Apapers.%20Further%20analyses%20confirm%20its%20robust%20capabilities.%20We%20will%20make%20our%0Adatasets%20and%20codes%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.09262v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangling%2520the%2520Potential%2520Impacts%2520of%2520Papers%2520into%2520Diffusion%252C%250A%2520%2520Conformity%252C%2520and%2520Contribution%2520Values%26entry.906535625%3DZhikai%2520Xue%2520and%2520Guoxiu%2520He%2520and%2520Zhuoren%2520Jiang%2520and%2520Sichen%2520Gu%2520and%2520Yangyang%2520Kang%2520and%2520Star%2520Zhao%2520and%2520Wei%2520Lu%26entry.1292438233%3D%2520%2520The%2520potential%2520impact%2520of%2520an%2520academic%2520paper%2520is%2520determined%2520by%2520various%2520factors%252C%250Aincluding%2520its%2520popularity%2520and%2520contribution.%2520Existing%2520models%2520usually%2520estimate%250Aoriginal%2520citation%2520counts%2520based%2520on%2520static%2520graphs%2520and%2520fail%2520to%2520differentiate%250Avalues%2520from%2520nuanced%2520perspectives.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%2520graph%250Aneural%2520network%2520to%2520Disentangle%2520the%2520Potential%2520impacts%2520of%2520Papers%2520into%2520Diffusion%252C%250AConformity%252C%2520and%2520Contribution%2520values%2520%2528called%2520DPPDCC%2529.%2520Given%2520a%2520target%2520paper%252C%250ADPPDCC%2520encodes%2520temporal%2520and%2520structural%2520features%2520within%2520the%2520constructed%2520dynamic%250Aheterogeneous%2520graph.%2520Particularly%252C%2520to%2520capture%2520the%2520knowledge%2520flow%252C%2520we%2520emphasize%250Athe%2520importance%2520of%2520comparative%2520and%2520co-cited/citing%2520information%2520between%2520papers%250Aand%2520aggregate%2520snapshots%2520evolutionarily.%2520To%2520unravel%2520popularity%252C%2520we%2520contrast%250Aaugmented%2520graphs%2520to%2520extract%2520the%2520essence%2520of%2520diffusion%2520and%2520predict%2520the%250Aaccumulated%2520citation%2520binning%2520to%2520model%2520conformity.%2520We%2520further%2520apply%2520orthogonal%250Aconstraints%2520to%2520encourage%2520distinct%2520modeling%2520of%2520each%2520perspective%2520and%2520preserve%2520the%250Ainherent%2520value%2520of%2520contribution.%2520To%2520evaluate%2520models%2527%2520generalization%2520for%2520papers%250Apublished%2520at%2520various%2520times%252C%2520we%2520reformulate%2520the%2520problem%2520by%2520partitioning%2520data%250Abased%2520on%2520specific%2520time%2520points%2520to%2520mirror%2520real-world%2520conditions.%2520Extensive%250Aexperimental%2520results%2520on%2520three%2520datasets%2520demonstrate%2520that%2520DPPDCC%2520significantly%250Aoutperforms%2520baselines%2520for%2520previously%252C%2520freshly%252C%2520and%2520immediately%2520published%250Apapers.%2520Further%2520analyses%2520confirm%2520its%2520robust%2520capabilities.%2520We%2520will%2520make%2520our%250Adatasets%2520and%2520codes%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.09262v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%20the%20Potential%20Impacts%20of%20Papers%20into%20Diffusion%2C%0A%20%20Conformity%2C%20and%20Contribution%20Values&entry.906535625=Zhikai%20Xue%20and%20Guoxiu%20He%20and%20Zhuoren%20Jiang%20and%20Sichen%20Gu%20and%20Yangyang%20Kang%20and%20Star%20Zhao%20and%20Wei%20Lu&entry.1292438233=%20%20The%20potential%20impact%20of%20an%20academic%20paper%20is%20determined%20by%20various%20factors%2C%0Aincluding%20its%20popularity%20and%20contribution.%20Existing%20models%20usually%20estimate%0Aoriginal%20citation%20counts%20based%20on%20static%20graphs%20and%20fail%20to%20differentiate%0Avalues%20from%20nuanced%20perspectives.%20In%20this%20study%2C%20we%20propose%20a%20novel%20graph%0Aneural%20network%20to%20Disentangle%20the%20Potential%20impacts%20of%20Papers%20into%20Diffusion%2C%0AConformity%2C%20and%20Contribution%20values%20%28called%20DPPDCC%29.%20Given%20a%20target%20paper%2C%0ADPPDCC%20encodes%20temporal%20and%20structural%20features%20within%20the%20constructed%20dynamic%0Aheterogeneous%20graph.%20Particularly%2C%20to%20capture%20the%20knowledge%20flow%2C%20we%20emphasize%0Athe%20importance%20of%20comparative%20and%20co-cited/citing%20information%20between%20papers%0Aand%20aggregate%20snapshots%20evolutionarily.%20To%20unravel%20popularity%2C%20we%20contrast%0Aaugmented%20graphs%20to%20extract%20the%20essence%20of%20diffusion%20and%20predict%20the%0Aaccumulated%20citation%20binning%20to%20model%20conformity.%20We%20further%20apply%20orthogonal%0Aconstraints%20to%20encourage%20distinct%20modeling%20of%20each%20perspective%20and%20preserve%20the%0Ainherent%20value%20of%20contribution.%20To%20evaluate%20models%27%20generalization%20for%20papers%0Apublished%20at%20various%20times%2C%20we%20reformulate%20the%20problem%20by%20partitioning%20data%0Abased%20on%20specific%20time%20points%20to%20mirror%20real-world%20conditions.%20Extensive%0Aexperimental%20results%20on%20three%20datasets%20demonstrate%20that%20DPPDCC%20significantly%0Aoutperforms%20baselines%20for%20previously%2C%20freshly%2C%20and%20immediately%20published%0Apapers.%20Further%20analyses%20confirm%20its%20robust%20capabilities.%20We%20will%20make%20our%0Adatasets%20and%20codes%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09262v3&entry.124074799=Read"},
{"title": "LLM Processes: Numerical Predictive Distributions Conditioned on Natural\n  Language", "author": "James Requeima and John Bronskill and Dami Choi and Richard E. Turner and David Duvenaud", "abstract": "  Machine learning practitioners often face significant challenges in formally\nintegrating their prior knowledge and beliefs into predictive models, limiting\nthe potential for nuanced and context-aware analyses. Moreover, the expertise\nneeded to integrate this prior knowledge into probabilistic modeling typically\nlimits the application of these models to specialists. Our goal is to build a\nregression model that can process numerical data and make probabilistic\npredictions at arbitrary locations, guided by natural language text which\ndescribes a user's prior knowledge. Large Language Models (LLMs) provide a\nuseful starting point for designing such a tool since they 1) provide an\ninterface where users can incorporate expert insights in natural language and\n2) provide an opportunity for leveraging latent problem-relevant knowledge\nencoded in LLMs that users may not have themselves. We start by exploring\nstrategies for eliciting explicit, coherent numerical predictive distributions\nfrom LLMs. We examine these joint predictive distributions, which we call LLM\nProcesses, over arbitrarily-many quantities in settings such as forecasting,\nmulti-dimensional regression, black-box optimization, and image modeling. We\ninvestigate the practical details of prompting to elicit coherent predictive\ndistributions, and demonstrate their effectiveness at regression. Finally, we\ndemonstrate the ability to usefully incorporate text into numerical\npredictions, improving predictive performance and giving quantitative structure\nthat reflects qualitative descriptions. This lets us begin to explore the rich,\ngrounded hypothesis space that LLMs implicitly encode.\n", "link": "http://arxiv.org/abs/2405.12856v1", "date": "2024-05-21", "relevancy": 0.9991, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5194}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5019}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM%20Processes%3A%20Numerical%20Predictive%20Distributions%20Conditioned%20on%20Natural%0A%20%20Language&body=Title%3A%20LLM%20Processes%3A%20Numerical%20Predictive%20Distributions%20Conditioned%20on%20Natural%0A%20%20Language%0AAuthor%3A%20James%20Requeima%20and%20John%20Bronskill%20and%20Dami%20Choi%20and%20Richard%20E.%20Turner%20and%20David%20Duvenaud%0AAbstract%3A%20%20%20Machine%20learning%20practitioners%20often%20face%20significant%20challenges%20in%20formally%0Aintegrating%20their%20prior%20knowledge%20and%20beliefs%20into%20predictive%20models%2C%20limiting%0Athe%20potential%20for%20nuanced%20and%20context-aware%20analyses.%20Moreover%2C%20the%20expertise%0Aneeded%20to%20integrate%20this%20prior%20knowledge%20into%20probabilistic%20modeling%20typically%0Alimits%20the%20application%20of%20these%20models%20to%20specialists.%20Our%20goal%20is%20to%20build%20a%0Aregression%20model%20that%20can%20process%20numerical%20data%20and%20make%20probabilistic%0Apredictions%20at%20arbitrary%20locations%2C%20guided%20by%20natural%20language%20text%20which%0Adescribes%20a%20user%27s%20prior%20knowledge.%20Large%20Language%20Models%20%28LLMs%29%20provide%20a%0Auseful%20starting%20point%20for%20designing%20such%20a%20tool%20since%20they%201%29%20provide%20an%0Ainterface%20where%20users%20can%20incorporate%20expert%20insights%20in%20natural%20language%20and%0A2%29%20provide%20an%20opportunity%20for%20leveraging%20latent%20problem-relevant%20knowledge%0Aencoded%20in%20LLMs%20that%20users%20may%20not%20have%20themselves.%20We%20start%20by%20exploring%0Astrategies%20for%20eliciting%20explicit%2C%20coherent%20numerical%20predictive%20distributions%0Afrom%20LLMs.%20We%20examine%20these%20joint%20predictive%20distributions%2C%20which%20we%20call%20LLM%0AProcesses%2C%20over%20arbitrarily-many%20quantities%20in%20settings%20such%20as%20forecasting%2C%0Amulti-dimensional%20regression%2C%20black-box%20optimization%2C%20and%20image%20modeling.%20We%0Ainvestigate%20the%20practical%20details%20of%20prompting%20to%20elicit%20coherent%20predictive%0Adistributions%2C%20and%20demonstrate%20their%20effectiveness%20at%20regression.%20Finally%2C%20we%0Ademonstrate%20the%20ability%20to%20usefully%20incorporate%20text%20into%20numerical%0Apredictions%2C%20improving%20predictive%20performance%20and%20giving%20quantitative%20structure%0Athat%20reflects%20qualitative%20descriptions.%20This%20lets%20us%20begin%20to%20explore%20the%20rich%2C%0Agrounded%20hypothesis%20space%20that%20LLMs%20implicitly%20encode.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12856v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM%2520Processes%253A%2520Numerical%2520Predictive%2520Distributions%2520Conditioned%2520on%2520Natural%250A%2520%2520Language%26entry.906535625%3DJames%2520Requeima%2520and%2520John%2520Bronskill%2520and%2520Dami%2520Choi%2520and%2520Richard%2520E.%2520Turner%2520and%2520David%2520Duvenaud%26entry.1292438233%3D%2520%2520Machine%2520learning%2520practitioners%2520often%2520face%2520significant%2520challenges%2520in%2520formally%250Aintegrating%2520their%2520prior%2520knowledge%2520and%2520beliefs%2520into%2520predictive%2520models%252C%2520limiting%250Athe%2520potential%2520for%2520nuanced%2520and%2520context-aware%2520analyses.%2520Moreover%252C%2520the%2520expertise%250Aneeded%2520to%2520integrate%2520this%2520prior%2520knowledge%2520into%2520probabilistic%2520modeling%2520typically%250Alimits%2520the%2520application%2520of%2520these%2520models%2520to%2520specialists.%2520Our%2520goal%2520is%2520to%2520build%2520a%250Aregression%2520model%2520that%2520can%2520process%2520numerical%2520data%2520and%2520make%2520probabilistic%250Apredictions%2520at%2520arbitrary%2520locations%252C%2520guided%2520by%2520natural%2520language%2520text%2520which%250Adescribes%2520a%2520user%2527s%2520prior%2520knowledge.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520provide%2520a%250Auseful%2520starting%2520point%2520for%2520designing%2520such%2520a%2520tool%2520since%2520they%25201%2529%2520provide%2520an%250Ainterface%2520where%2520users%2520can%2520incorporate%2520expert%2520insights%2520in%2520natural%2520language%2520and%250A2%2529%2520provide%2520an%2520opportunity%2520for%2520leveraging%2520latent%2520problem-relevant%2520knowledge%250Aencoded%2520in%2520LLMs%2520that%2520users%2520may%2520not%2520have%2520themselves.%2520We%2520start%2520by%2520exploring%250Astrategies%2520for%2520eliciting%2520explicit%252C%2520coherent%2520numerical%2520predictive%2520distributions%250Afrom%2520LLMs.%2520We%2520examine%2520these%2520joint%2520predictive%2520distributions%252C%2520which%2520we%2520call%2520LLM%250AProcesses%252C%2520over%2520arbitrarily-many%2520quantities%2520in%2520settings%2520such%2520as%2520forecasting%252C%250Amulti-dimensional%2520regression%252C%2520black-box%2520optimization%252C%2520and%2520image%2520modeling.%2520We%250Ainvestigate%2520the%2520practical%2520details%2520of%2520prompting%2520to%2520elicit%2520coherent%2520predictive%250Adistributions%252C%2520and%2520demonstrate%2520their%2520effectiveness%2520at%2520regression.%2520Finally%252C%2520we%250Ademonstrate%2520the%2520ability%2520to%2520usefully%2520incorporate%2520text%2520into%2520numerical%250Apredictions%252C%2520improving%2520predictive%2520performance%2520and%2520giving%2520quantitative%2520structure%250Athat%2520reflects%2520qualitative%2520descriptions.%2520This%2520lets%2520us%2520begin%2520to%2520explore%2520the%2520rich%252C%250Agrounded%2520hypothesis%2520space%2520that%2520LLMs%2520implicitly%2520encode.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12856v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20Processes%3A%20Numerical%20Predictive%20Distributions%20Conditioned%20on%20Natural%0A%20%20Language&entry.906535625=James%20Requeima%20and%20John%20Bronskill%20and%20Dami%20Choi%20and%20Richard%20E.%20Turner%20and%20David%20Duvenaud&entry.1292438233=%20%20Machine%20learning%20practitioners%20often%20face%20significant%20challenges%20in%20formally%0Aintegrating%20their%20prior%20knowledge%20and%20beliefs%20into%20predictive%20models%2C%20limiting%0Athe%20potential%20for%20nuanced%20and%20context-aware%20analyses.%20Moreover%2C%20the%20expertise%0Aneeded%20to%20integrate%20this%20prior%20knowledge%20into%20probabilistic%20modeling%20typically%0Alimits%20the%20application%20of%20these%20models%20to%20specialists.%20Our%20goal%20is%20to%20build%20a%0Aregression%20model%20that%20can%20process%20numerical%20data%20and%20make%20probabilistic%0Apredictions%20at%20arbitrary%20locations%2C%20guided%20by%20natural%20language%20text%20which%0Adescribes%20a%20user%27s%20prior%20knowledge.%20Large%20Language%20Models%20%28LLMs%29%20provide%20a%0Auseful%20starting%20point%20for%20designing%20such%20a%20tool%20since%20they%201%29%20provide%20an%0Ainterface%20where%20users%20can%20incorporate%20expert%20insights%20in%20natural%20language%20and%0A2%29%20provide%20an%20opportunity%20for%20leveraging%20latent%20problem-relevant%20knowledge%0Aencoded%20in%20LLMs%20that%20users%20may%20not%20have%20themselves.%20We%20start%20by%20exploring%0Astrategies%20for%20eliciting%20explicit%2C%20coherent%20numerical%20predictive%20distributions%0Afrom%20LLMs.%20We%20examine%20these%20joint%20predictive%20distributions%2C%20which%20we%20call%20LLM%0AProcesses%2C%20over%20arbitrarily-many%20quantities%20in%20settings%20such%20as%20forecasting%2C%0Amulti-dimensional%20regression%2C%20black-box%20optimization%2C%20and%20image%20modeling.%20We%0Ainvestigate%20the%20practical%20details%20of%20prompting%20to%20elicit%20coherent%20predictive%0Adistributions%2C%20and%20demonstrate%20their%20effectiveness%20at%20regression.%20Finally%2C%20we%0Ademonstrate%20the%20ability%20to%20usefully%20incorporate%20text%20into%20numerical%0Apredictions%2C%20improving%20predictive%20performance%20and%20giving%20quantitative%20structure%0Athat%20reflects%20qualitative%20descriptions.%20This%20lets%20us%20begin%20to%20explore%20the%20rich%2C%0Agrounded%20hypothesis%20space%20that%20LLMs%20implicitly%20encode.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12856v1&entry.124074799=Read"},
{"title": "Investigating KAN-Based Physics-Informed Neural Networks for EMI/EMC\n  Simulations", "author": "Kun Qian and Mohamed Kheir", "abstract": "  The main objective of this paper is to investigate the feasibility of\nemploying Physics-Informed Neural Networks (PINNs) techniques, in particular\nKolmogorovArnold Networks (KANs), for facilitating Electromagnetic Interference\n(EMI) simulations. It introduces some common EM problem formulations and how\nthey can be solved using AI-driven solutions instead of lengthy and complex\nfull-wave numerical simulations. This research may open new horizons for green\nEMI simulation workflows with less energy consumption and feasible\ncomputational capacity.\n", "link": "http://arxiv.org/abs/2405.11383v2", "date": "2024-05-21", "relevancy": 1.6984, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4671}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4299}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20KAN-Based%20Physics-Informed%20Neural%20Networks%20for%20EMI/EMC%0A%20%20Simulations&body=Title%3A%20Investigating%20KAN-Based%20Physics-Informed%20Neural%20Networks%20for%20EMI/EMC%0A%20%20Simulations%0AAuthor%3A%20Kun%20Qian%20and%20Mohamed%20Kheir%0AAbstract%3A%20%20%20The%20main%20objective%20of%20this%20paper%20is%20to%20investigate%20the%20feasibility%20of%0Aemploying%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20techniques%2C%20in%20particular%0AKolmogorovArnold%20Networks%20%28KANs%29%2C%20for%20facilitating%20Electromagnetic%20Interference%0A%28EMI%29%20simulations.%20It%20introduces%20some%20common%20EM%20problem%20formulations%20and%20how%0Athey%20can%20be%20solved%20using%20AI-driven%20solutions%20instead%20of%20lengthy%20and%20complex%0Afull-wave%20numerical%20simulations.%20This%20research%20may%20open%20new%20horizons%20for%20green%0AEMI%20simulation%20workflows%20with%20less%20energy%20consumption%20and%20feasible%0Acomputational%20capacity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11383v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520KAN-Based%2520Physics-Informed%2520Neural%2520Networks%2520for%2520EMI/EMC%250A%2520%2520Simulations%26entry.906535625%3DKun%2520Qian%2520and%2520Mohamed%2520Kheir%26entry.1292438233%3D%2520%2520The%2520main%2520objective%2520of%2520this%2520paper%2520is%2520to%2520investigate%2520the%2520feasibility%2520of%250Aemploying%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%2520techniques%252C%2520in%2520particular%250AKolmogorovArnold%2520Networks%2520%2528KANs%2529%252C%2520for%2520facilitating%2520Electromagnetic%2520Interference%250A%2528EMI%2529%2520simulations.%2520It%2520introduces%2520some%2520common%2520EM%2520problem%2520formulations%2520and%2520how%250Athey%2520can%2520be%2520solved%2520using%2520AI-driven%2520solutions%2520instead%2520of%2520lengthy%2520and%2520complex%250Afull-wave%2520numerical%2520simulations.%2520This%2520research%2520may%2520open%2520new%2520horizons%2520for%2520green%250AEMI%2520simulation%2520workflows%2520with%2520less%2520energy%2520consumption%2520and%2520feasible%250Acomputational%2520capacity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11383v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20KAN-Based%20Physics-Informed%20Neural%20Networks%20for%20EMI/EMC%0A%20%20Simulations&entry.906535625=Kun%20Qian%20and%20Mohamed%20Kheir&entry.1292438233=%20%20The%20main%20objective%20of%20this%20paper%20is%20to%20investigate%20the%20feasibility%20of%0Aemploying%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20techniques%2C%20in%20particular%0AKolmogorovArnold%20Networks%20%28KANs%29%2C%20for%20facilitating%20Electromagnetic%20Interference%0A%28EMI%29%20simulations.%20It%20introduces%20some%20common%20EM%20problem%20formulations%20and%20how%0Athey%20can%20be%20solved%20using%20AI-driven%20solutions%20instead%20of%20lengthy%20and%20complex%0Afull-wave%20numerical%20simulations.%20This%20research%20may%20open%20new%20horizons%20for%20green%0AEMI%20simulation%20workflows%20with%20less%20energy%20consumption%20and%20feasible%0Acomputational%20capacity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11383v2&entry.124074799=Read"},
{"title": "Multimodal Adaptive Inference for Document Image Classification with\n  Anytime Early Exiting", "author": "Omar Hamed and Souhail Bakkali and Marie-Francine Moens and Matthew Blaschko and Jordy Van Landeghem", "abstract": "  This work addresses the need for a balanced approach between performance and\nefficiency in scalable production environments for visually-rich document\nunderstanding (VDU) tasks. Currently, there is a reliance on large document\nfoundation models that offer advanced capabilities but come with a heavy\ncomputational burden. In this paper, we propose a multimodal early exit (EE)\nmodel design that incorporates various training strategies, exit layer types\nand placements. Our goal is to achieve a Pareto-optimal balance between\npredictive performance and efficiency for multimodal document image\nclassification. Through a comprehensive set of experiments, we compare our\napproach with traditional exit policies and showcase an improved\nperformance-efficiency trade-off. Our multimodal EE design preserves the\nmodel's predictive capabilities, enhancing both speed and latency. This is\nachieved through a reduction of over 20% in latency, while fully retaining the\nbaseline accuracy. This research represents the first exploration of multimodal\nEE design within the VDU community, highlighting as well the effectiveness of\ncalibration in improving confidence scores for exiting at different layers.\nOverall, our findings contribute to practical VDU applications by enhancing\nboth performance and efficiency.\n", "link": "http://arxiv.org/abs/2405.12705v1", "date": "2024-05-21", "relevancy": 1.7031, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5899}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5635}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Adaptive%20Inference%20for%20Document%20Image%20Classification%20with%0A%20%20Anytime%20Early%20Exiting&body=Title%3A%20Multimodal%20Adaptive%20Inference%20for%20Document%20Image%20Classification%20with%0A%20%20Anytime%20Early%20Exiting%0AAuthor%3A%20Omar%20Hamed%20and%20Souhail%20Bakkali%20and%20Marie-Francine%20Moens%20and%20Matthew%20Blaschko%20and%20Jordy%20Van%20Landeghem%0AAbstract%3A%20%20%20This%20work%20addresses%20the%20need%20for%20a%20balanced%20approach%20between%20performance%20and%0Aefficiency%20in%20scalable%20production%20environments%20for%20visually-rich%20document%0Aunderstanding%20%28VDU%29%20tasks.%20Currently%2C%20there%20is%20a%20reliance%20on%20large%20document%0Afoundation%20models%20that%20offer%20advanced%20capabilities%20but%20come%20with%20a%20heavy%0Acomputational%20burden.%20In%20this%20paper%2C%20we%20propose%20a%20multimodal%20early%20exit%20%28EE%29%0Amodel%20design%20that%20incorporates%20various%20training%20strategies%2C%20exit%20layer%20types%0Aand%20placements.%20Our%20goal%20is%20to%20achieve%20a%20Pareto-optimal%20balance%20between%0Apredictive%20performance%20and%20efficiency%20for%20multimodal%20document%20image%0Aclassification.%20Through%20a%20comprehensive%20set%20of%20experiments%2C%20we%20compare%20our%0Aapproach%20with%20traditional%20exit%20policies%20and%20showcase%20an%20improved%0Aperformance-efficiency%20trade-off.%20Our%20multimodal%20EE%20design%20preserves%20the%0Amodel%27s%20predictive%20capabilities%2C%20enhancing%20both%20speed%20and%20latency.%20This%20is%0Aachieved%20through%20a%20reduction%20of%20over%2020%25%20in%20latency%2C%20while%20fully%20retaining%20the%0Abaseline%20accuracy.%20This%20research%20represents%20the%20first%20exploration%20of%20multimodal%0AEE%20design%20within%20the%20VDU%20community%2C%20highlighting%20as%20well%20the%20effectiveness%20of%0Acalibration%20in%20improving%20confidence%20scores%20for%20exiting%20at%20different%20layers.%0AOverall%2C%20our%20findings%20contribute%20to%20practical%20VDU%20applications%20by%20enhancing%0Aboth%20performance%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Adaptive%2520Inference%2520for%2520Document%2520Image%2520Classification%2520with%250A%2520%2520Anytime%2520Early%2520Exiting%26entry.906535625%3DOmar%2520Hamed%2520and%2520Souhail%2520Bakkali%2520and%2520Marie-Francine%2520Moens%2520and%2520Matthew%2520Blaschko%2520and%2520Jordy%2520Van%2520Landeghem%26entry.1292438233%3D%2520%2520This%2520work%2520addresses%2520the%2520need%2520for%2520a%2520balanced%2520approach%2520between%2520performance%2520and%250Aefficiency%2520in%2520scalable%2520production%2520environments%2520for%2520visually-rich%2520document%250Aunderstanding%2520%2528VDU%2529%2520tasks.%2520Currently%252C%2520there%2520is%2520a%2520reliance%2520on%2520large%2520document%250Afoundation%2520models%2520that%2520offer%2520advanced%2520capabilities%2520but%2520come%2520with%2520a%2520heavy%250Acomputational%2520burden.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520multimodal%2520early%2520exit%2520%2528EE%2529%250Amodel%2520design%2520that%2520incorporates%2520various%2520training%2520strategies%252C%2520exit%2520layer%2520types%250Aand%2520placements.%2520Our%2520goal%2520is%2520to%2520achieve%2520a%2520Pareto-optimal%2520balance%2520between%250Apredictive%2520performance%2520and%2520efficiency%2520for%2520multimodal%2520document%2520image%250Aclassification.%2520Through%2520a%2520comprehensive%2520set%2520of%2520experiments%252C%2520we%2520compare%2520our%250Aapproach%2520with%2520traditional%2520exit%2520policies%2520and%2520showcase%2520an%2520improved%250Aperformance-efficiency%2520trade-off.%2520Our%2520multimodal%2520EE%2520design%2520preserves%2520the%250Amodel%2527s%2520predictive%2520capabilities%252C%2520enhancing%2520both%2520speed%2520and%2520latency.%2520This%2520is%250Aachieved%2520through%2520a%2520reduction%2520of%2520over%252020%2525%2520in%2520latency%252C%2520while%2520fully%2520retaining%2520the%250Abaseline%2520accuracy.%2520This%2520research%2520represents%2520the%2520first%2520exploration%2520of%2520multimodal%250AEE%2520design%2520within%2520the%2520VDU%2520community%252C%2520highlighting%2520as%2520well%2520the%2520effectiveness%2520of%250Acalibration%2520in%2520improving%2520confidence%2520scores%2520for%2520exiting%2520at%2520different%2520layers.%250AOverall%252C%2520our%2520findings%2520contribute%2520to%2520practical%2520VDU%2520applications%2520by%2520enhancing%250Aboth%2520performance%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Adaptive%20Inference%20for%20Document%20Image%20Classification%20with%0A%20%20Anytime%20Early%20Exiting&entry.906535625=Omar%20Hamed%20and%20Souhail%20Bakkali%20and%20Marie-Francine%20Moens%20and%20Matthew%20Blaschko%20and%20Jordy%20Van%20Landeghem&entry.1292438233=%20%20This%20work%20addresses%20the%20need%20for%20a%20balanced%20approach%20between%20performance%20and%0Aefficiency%20in%20scalable%20production%20environments%20for%20visually-rich%20document%0Aunderstanding%20%28VDU%29%20tasks.%20Currently%2C%20there%20is%20a%20reliance%20on%20large%20document%0Afoundation%20models%20that%20offer%20advanced%20capabilities%20but%20come%20with%20a%20heavy%0Acomputational%20burden.%20In%20this%20paper%2C%20we%20propose%20a%20multimodal%20early%20exit%20%28EE%29%0Amodel%20design%20that%20incorporates%20various%20training%20strategies%2C%20exit%20layer%20types%0Aand%20placements.%20Our%20goal%20is%20to%20achieve%20a%20Pareto-optimal%20balance%20between%0Apredictive%20performance%20and%20efficiency%20for%20multimodal%20document%20image%0Aclassification.%20Through%20a%20comprehensive%20set%20of%20experiments%2C%20we%20compare%20our%0Aapproach%20with%20traditional%20exit%20policies%20and%20showcase%20an%20improved%0Aperformance-efficiency%20trade-off.%20Our%20multimodal%20EE%20design%20preserves%20the%0Amodel%27s%20predictive%20capabilities%2C%20enhancing%20both%20speed%20and%20latency.%20This%20is%0Aachieved%20through%20a%20reduction%20of%20over%2020%25%20in%20latency%2C%20while%20fully%20retaining%20the%0Abaseline%20accuracy.%20This%20research%20represents%20the%20first%20exploration%20of%20multimodal%0AEE%20design%20within%20the%20VDU%20community%2C%20highlighting%20as%20well%20the%20effectiveness%20of%0Acalibration%20in%20improving%20confidence%20scores%20for%20exiting%20at%20different%20layers.%0AOverall%2C%20our%20findings%20contribute%20to%20practical%20VDU%20applications%20by%20enhancing%0Aboth%20performance%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12705v1&entry.124074799=Read"},
{"title": "FAdam: Adam is a natural gradient optimizer using diagonal empirical\n  Fisher information", "author": "Dongseong Hwang", "abstract": "  This paper establishes a mathematical foundation for the Adam optimizer,\nelucidating its connection to natural gradient descent through Riemannian and\ninformation geometry. We rigorously analyze the diagonal empirical Fisher\ninformation matrix (FIM) in Adam, clarifying all detailed approximations and\nadvocating for the use of log probability functions as loss, which should be\nbased on discrete distributions, due to the limitations of empirical FIM. Our\nanalysis uncovers flaws in the original Adam algorithm, leading to proposed\ncorrections such as enhanced momentum calculations, adjusted bias corrections,\nand gradient clipping. We refine the weight decay term based on our theoretical\nframework. Our modified algorithm, Fisher Adam (FAdam), demonstrates superior\nperformance across diverse domains including LLM, ASR, and VQ-VAE, achieving\nstate-of-the-art results in ASR.\n", "link": "http://arxiv.org/abs/2405.12807v1", "date": "2024-05-21", "relevancy": 1.4072, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4712}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4698}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FAdam%3A%20Adam%20is%20a%20natural%20gradient%20optimizer%20using%20diagonal%20empirical%0A%20%20Fisher%20information&body=Title%3A%20FAdam%3A%20Adam%20is%20a%20natural%20gradient%20optimizer%20using%20diagonal%20empirical%0A%20%20Fisher%20information%0AAuthor%3A%20Dongseong%20Hwang%0AAbstract%3A%20%20%20This%20paper%20establishes%20a%20mathematical%20foundation%20for%20the%20Adam%20optimizer%2C%0Aelucidating%20its%20connection%20to%20natural%20gradient%20descent%20through%20Riemannian%20and%0Ainformation%20geometry.%20We%20rigorously%20analyze%20the%20diagonal%20empirical%20Fisher%0Ainformation%20matrix%20%28FIM%29%20in%20Adam%2C%20clarifying%20all%20detailed%20approximations%20and%0Aadvocating%20for%20the%20use%20of%20log%20probability%20functions%20as%20loss%2C%20which%20should%20be%0Abased%20on%20discrete%20distributions%2C%20due%20to%20the%20limitations%20of%20empirical%20FIM.%20Our%0Aanalysis%20uncovers%20flaws%20in%20the%20original%20Adam%20algorithm%2C%20leading%20to%20proposed%0Acorrections%20such%20as%20enhanced%20momentum%20calculations%2C%20adjusted%20bias%20corrections%2C%0Aand%20gradient%20clipping.%20We%20refine%20the%20weight%20decay%20term%20based%20on%20our%20theoretical%0Aframework.%20Our%20modified%20algorithm%2C%20Fisher%20Adam%20%28FAdam%29%2C%20demonstrates%20superior%0Aperformance%20across%20diverse%20domains%20including%20LLM%2C%20ASR%2C%20and%20VQ-VAE%2C%20achieving%0Astate-of-the-art%20results%20in%20ASR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFAdam%253A%2520Adam%2520is%2520a%2520natural%2520gradient%2520optimizer%2520using%2520diagonal%2520empirical%250A%2520%2520Fisher%2520information%26entry.906535625%3DDongseong%2520Hwang%26entry.1292438233%3D%2520%2520This%2520paper%2520establishes%2520a%2520mathematical%2520foundation%2520for%2520the%2520Adam%2520optimizer%252C%250Aelucidating%2520its%2520connection%2520to%2520natural%2520gradient%2520descent%2520through%2520Riemannian%2520and%250Ainformation%2520geometry.%2520We%2520rigorously%2520analyze%2520the%2520diagonal%2520empirical%2520Fisher%250Ainformation%2520matrix%2520%2528FIM%2529%2520in%2520Adam%252C%2520clarifying%2520all%2520detailed%2520approximations%2520and%250Aadvocating%2520for%2520the%2520use%2520of%2520log%2520probability%2520functions%2520as%2520loss%252C%2520which%2520should%2520be%250Abased%2520on%2520discrete%2520distributions%252C%2520due%2520to%2520the%2520limitations%2520of%2520empirical%2520FIM.%2520Our%250Aanalysis%2520uncovers%2520flaws%2520in%2520the%2520original%2520Adam%2520algorithm%252C%2520leading%2520to%2520proposed%250Acorrections%2520such%2520as%2520enhanced%2520momentum%2520calculations%252C%2520adjusted%2520bias%2520corrections%252C%250Aand%2520gradient%2520clipping.%2520We%2520refine%2520the%2520weight%2520decay%2520term%2520based%2520on%2520our%2520theoretical%250Aframework.%2520Our%2520modified%2520algorithm%252C%2520Fisher%2520Adam%2520%2528FAdam%2529%252C%2520demonstrates%2520superior%250Aperformance%2520across%2520diverse%2520domains%2520including%2520LLM%252C%2520ASR%252C%2520and%2520VQ-VAE%252C%2520achieving%250Astate-of-the-art%2520results%2520in%2520ASR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FAdam%3A%20Adam%20is%20a%20natural%20gradient%20optimizer%20using%20diagonal%20empirical%0A%20%20Fisher%20information&entry.906535625=Dongseong%20Hwang&entry.1292438233=%20%20This%20paper%20establishes%20a%20mathematical%20foundation%20for%20the%20Adam%20optimizer%2C%0Aelucidating%20its%20connection%20to%20natural%20gradient%20descent%20through%20Riemannian%20and%0Ainformation%20geometry.%20We%20rigorously%20analyze%20the%20diagonal%20empirical%20Fisher%0Ainformation%20matrix%20%28FIM%29%20in%20Adam%2C%20clarifying%20all%20detailed%20approximations%20and%0Aadvocating%20for%20the%20use%20of%20log%20probability%20functions%20as%20loss%2C%20which%20should%20be%0Abased%20on%20discrete%20distributions%2C%20due%20to%20the%20limitations%20of%20empirical%20FIM.%20Our%0Aanalysis%20uncovers%20flaws%20in%20the%20original%20Adam%20algorithm%2C%20leading%20to%20proposed%0Acorrections%20such%20as%20enhanced%20momentum%20calculations%2C%20adjusted%20bias%20corrections%2C%0Aand%20gradient%20clipping.%20We%20refine%20the%20weight%20decay%20term%20based%20on%20our%20theoretical%0Aframework.%20Our%20modified%20algorithm%2C%20Fisher%20Adam%20%28FAdam%29%2C%20demonstrates%20superior%0Aperformance%20across%20diverse%20domains%20including%20LLM%2C%20ASR%2C%20and%20VQ-VAE%2C%20achieving%0Astate-of-the-art%20results%20in%20ASR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12807v1&entry.124074799=Read"},
{"title": "Artificial Intelligence Approaches for Predictive Maintenance in the\n  Steel Industry: A Survey", "author": "Jakub Jakubowski and Natalia Wojak-Strzelecka and Rita P. Ribeiro and Sepideh Pashami and Szymon Bobek and Joao Gama and Grzegorz J Nalepa", "abstract": "  Predictive Maintenance (PdM) emerged as one of the pillars of Industry 4.0,\nand became crucial for enhancing operational efficiency, allowing to minimize\ndowntime, extend lifespan of equipment, and prevent failures. A wide range of\nPdM tasks can be performed using Artificial Intelligence (AI) methods, which\noften use data generated from industrial sensors. The steel industry, which is\nan important branch of the global economy, is one of the potential\nbeneficiaries of this trend, given its large environmental footprint, the\nglobalized nature of the market, and the demanding working conditions. This\nsurvey synthesizes the current state of knowledge in the field of AI-based PdM\nwithin the steel industry and is addressed to researchers and practitioners. We\nidentified 219 articles related to this topic and formulated five research\nquestions, allowing us to gain a global perspective on current trends and the\nmain research gaps. We examined equipment and facilities subjected to PdM,\ndetermined common PdM approaches, and identified trends in the AI methods used\nto develop these solutions. We explored the characteristics of the data used in\nthe surveyed articles and assessed the practical implications of the research\npresented there. Most of the research focuses on the blast furnace or hot\nrolling, using data from industrial sensors. Current trends show increasing\ninterest in the domain, especially in the use of deep learning. The main\nchallenges include implementing the proposed methods in a production\nenvironment, incorporating them into maintenance plans, and enhancing the\naccessibility and reproducibility of the research.\n", "link": "http://arxiv.org/abs/2405.12785v1", "date": "2024-05-21", "relevancy": 1.1851, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4061}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4022}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artificial%20Intelligence%20Approaches%20for%20Predictive%20Maintenance%20in%20the%0A%20%20Steel%20Industry%3A%20A%20Survey&body=Title%3A%20Artificial%20Intelligence%20Approaches%20for%20Predictive%20Maintenance%20in%20the%0A%20%20Steel%20Industry%3A%20A%20Survey%0AAuthor%3A%20Jakub%20Jakubowski%20and%20Natalia%20Wojak-Strzelecka%20and%20Rita%20P.%20Ribeiro%20and%20Sepideh%20Pashami%20and%20Szymon%20Bobek%20and%20Joao%20Gama%20and%20Grzegorz%20J%20Nalepa%0AAbstract%3A%20%20%20Predictive%20Maintenance%20%28PdM%29%20emerged%20as%20one%20of%20the%20pillars%20of%20Industry%204.0%2C%0Aand%20became%20crucial%20for%20enhancing%20operational%20efficiency%2C%20allowing%20to%20minimize%0Adowntime%2C%20extend%20lifespan%20of%20equipment%2C%20and%20prevent%20failures.%20A%20wide%20range%20of%0APdM%20tasks%20can%20be%20performed%20using%20Artificial%20Intelligence%20%28AI%29%20methods%2C%20which%0Aoften%20use%20data%20generated%20from%20industrial%20sensors.%20The%20steel%20industry%2C%20which%20is%0Aan%20important%20branch%20of%20the%20global%20economy%2C%20is%20one%20of%20the%20potential%0Abeneficiaries%20of%20this%20trend%2C%20given%20its%20large%20environmental%20footprint%2C%20the%0Aglobalized%20nature%20of%20the%20market%2C%20and%20the%20demanding%20working%20conditions.%20This%0Asurvey%20synthesizes%20the%20current%20state%20of%20knowledge%20in%20the%20field%20of%20AI-based%20PdM%0Awithin%20the%20steel%20industry%20and%20is%20addressed%20to%20researchers%20and%20practitioners.%20We%0Aidentified%20219%20articles%20related%20to%20this%20topic%20and%20formulated%20five%20research%0Aquestions%2C%20allowing%20us%20to%20gain%20a%20global%20perspective%20on%20current%20trends%20and%20the%0Amain%20research%20gaps.%20We%20examined%20equipment%20and%20facilities%20subjected%20to%20PdM%2C%0Adetermined%20common%20PdM%20approaches%2C%20and%20identified%20trends%20in%20the%20AI%20methods%20used%0Ato%20develop%20these%20solutions.%20We%20explored%20the%20characteristics%20of%20the%20data%20used%20in%0Athe%20surveyed%20articles%20and%20assessed%20the%20practical%20implications%20of%20the%20research%0Apresented%20there.%20Most%20of%20the%20research%20focuses%20on%20the%20blast%20furnace%20or%20hot%0Arolling%2C%20using%20data%20from%20industrial%20sensors.%20Current%20trends%20show%20increasing%0Ainterest%20in%20the%20domain%2C%20especially%20in%20the%20use%20of%20deep%20learning.%20The%20main%0Achallenges%20include%20implementing%20the%20proposed%20methods%20in%20a%20production%0Aenvironment%2C%20incorporating%20them%20into%20maintenance%20plans%2C%20and%20enhancing%20the%0Aaccessibility%20and%20reproducibility%20of%20the%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtificial%2520Intelligence%2520Approaches%2520for%2520Predictive%2520Maintenance%2520in%2520the%250A%2520%2520Steel%2520Industry%253A%2520A%2520Survey%26entry.906535625%3DJakub%2520Jakubowski%2520and%2520Natalia%2520Wojak-Strzelecka%2520and%2520Rita%2520P.%2520Ribeiro%2520and%2520Sepideh%2520Pashami%2520and%2520Szymon%2520Bobek%2520and%2520Joao%2520Gama%2520and%2520Grzegorz%2520J%2520Nalepa%26entry.1292438233%3D%2520%2520Predictive%2520Maintenance%2520%2528PdM%2529%2520emerged%2520as%2520one%2520of%2520the%2520pillars%2520of%2520Industry%25204.0%252C%250Aand%2520became%2520crucial%2520for%2520enhancing%2520operational%2520efficiency%252C%2520allowing%2520to%2520minimize%250Adowntime%252C%2520extend%2520lifespan%2520of%2520equipment%252C%2520and%2520prevent%2520failures.%2520A%2520wide%2520range%2520of%250APdM%2520tasks%2520can%2520be%2520performed%2520using%2520Artificial%2520Intelligence%2520%2528AI%2529%2520methods%252C%2520which%250Aoften%2520use%2520data%2520generated%2520from%2520industrial%2520sensors.%2520The%2520steel%2520industry%252C%2520which%2520is%250Aan%2520important%2520branch%2520of%2520the%2520global%2520economy%252C%2520is%2520one%2520of%2520the%2520potential%250Abeneficiaries%2520of%2520this%2520trend%252C%2520given%2520its%2520large%2520environmental%2520footprint%252C%2520the%250Aglobalized%2520nature%2520of%2520the%2520market%252C%2520and%2520the%2520demanding%2520working%2520conditions.%2520This%250Asurvey%2520synthesizes%2520the%2520current%2520state%2520of%2520knowledge%2520in%2520the%2520field%2520of%2520AI-based%2520PdM%250Awithin%2520the%2520steel%2520industry%2520and%2520is%2520addressed%2520to%2520researchers%2520and%2520practitioners.%2520We%250Aidentified%2520219%2520articles%2520related%2520to%2520this%2520topic%2520and%2520formulated%2520five%2520research%250Aquestions%252C%2520allowing%2520us%2520to%2520gain%2520a%2520global%2520perspective%2520on%2520current%2520trends%2520and%2520the%250Amain%2520research%2520gaps.%2520We%2520examined%2520equipment%2520and%2520facilities%2520subjected%2520to%2520PdM%252C%250Adetermined%2520common%2520PdM%2520approaches%252C%2520and%2520identified%2520trends%2520in%2520the%2520AI%2520methods%2520used%250Ato%2520develop%2520these%2520solutions.%2520We%2520explored%2520the%2520characteristics%2520of%2520the%2520data%2520used%2520in%250Athe%2520surveyed%2520articles%2520and%2520assessed%2520the%2520practical%2520implications%2520of%2520the%2520research%250Apresented%2520there.%2520Most%2520of%2520the%2520research%2520focuses%2520on%2520the%2520blast%2520furnace%2520or%2520hot%250Arolling%252C%2520using%2520data%2520from%2520industrial%2520sensors.%2520Current%2520trends%2520show%2520increasing%250Ainterest%2520in%2520the%2520domain%252C%2520especially%2520in%2520the%2520use%2520of%2520deep%2520learning.%2520The%2520main%250Achallenges%2520include%2520implementing%2520the%2520proposed%2520methods%2520in%2520a%2520production%250Aenvironment%252C%2520incorporating%2520them%2520into%2520maintenance%2520plans%252C%2520and%2520enhancing%2520the%250Aaccessibility%2520and%2520reproducibility%2520of%2520the%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20Intelligence%20Approaches%20for%20Predictive%20Maintenance%20in%20the%0A%20%20Steel%20Industry%3A%20A%20Survey&entry.906535625=Jakub%20Jakubowski%20and%20Natalia%20Wojak-Strzelecka%20and%20Rita%20P.%20Ribeiro%20and%20Sepideh%20Pashami%20and%20Szymon%20Bobek%20and%20Joao%20Gama%20and%20Grzegorz%20J%20Nalepa&entry.1292438233=%20%20Predictive%20Maintenance%20%28PdM%29%20emerged%20as%20one%20of%20the%20pillars%20of%20Industry%204.0%2C%0Aand%20became%20crucial%20for%20enhancing%20operational%20efficiency%2C%20allowing%20to%20minimize%0Adowntime%2C%20extend%20lifespan%20of%20equipment%2C%20and%20prevent%20failures.%20A%20wide%20range%20of%0APdM%20tasks%20can%20be%20performed%20using%20Artificial%20Intelligence%20%28AI%29%20methods%2C%20which%0Aoften%20use%20data%20generated%20from%20industrial%20sensors.%20The%20steel%20industry%2C%20which%20is%0Aan%20important%20branch%20of%20the%20global%20economy%2C%20is%20one%20of%20the%20potential%0Abeneficiaries%20of%20this%20trend%2C%20given%20its%20large%20environmental%20footprint%2C%20the%0Aglobalized%20nature%20of%20the%20market%2C%20and%20the%20demanding%20working%20conditions.%20This%0Asurvey%20synthesizes%20the%20current%20state%20of%20knowledge%20in%20the%20field%20of%20AI-based%20PdM%0Awithin%20the%20steel%20industry%20and%20is%20addressed%20to%20researchers%20and%20practitioners.%20We%0Aidentified%20219%20articles%20related%20to%20this%20topic%20and%20formulated%20five%20research%0Aquestions%2C%20allowing%20us%20to%20gain%20a%20global%20perspective%20on%20current%20trends%20and%20the%0Amain%20research%20gaps.%20We%20examined%20equipment%20and%20facilities%20subjected%20to%20PdM%2C%0Adetermined%20common%20PdM%20approaches%2C%20and%20identified%20trends%20in%20the%20AI%20methods%20used%0Ato%20develop%20these%20solutions.%20We%20explored%20the%20characteristics%20of%20the%20data%20used%20in%0Athe%20surveyed%20articles%20and%20assessed%20the%20practical%20implications%20of%20the%20research%0Apresented%20there.%20Most%20of%20the%20research%20focuses%20on%20the%20blast%20furnace%20or%20hot%0Arolling%2C%20using%20data%20from%20industrial%20sensors.%20Current%20trends%20show%20increasing%0Ainterest%20in%20the%20domain%2C%20especially%20in%20the%20use%20of%20deep%20learning.%20The%20main%0Achallenges%20include%20implementing%20the%20proposed%20methods%20in%20a%20production%0Aenvironment%2C%20incorporating%20them%20into%20maintenance%20plans%2C%20and%20enhancing%20the%0Aaccessibility%20and%20reproducibility%20of%20the%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12785v1&entry.124074799=Read"},
{"title": "Multimodal video analysis for crowd anomaly detection using open access\n  tourism cameras", "author": "Alejandro Dionis-Ros and Joan Vila-Franc\u00e9s and Rafael Magdalena-Benedicto and Fernando Mateo and Antonio J. Serrano-L\u00f3pez", "abstract": "  In this article, we propose the detection of crowd anomalies through the\nextraction of information in the form of time series from video format using a\nmultimodal approach. Through pattern recognition algorithms and segmentation,\ninformative measures of the number of people and image occupancy are extracted\nat regular intervals, which are then analyzed to obtain trends and anomalous\nbehaviors. Specifically, through temporal decomposition and residual analysis,\nintervals or specific situations of unusual behaviors are identified, which can\nbe used in decision-making and improvement of actions in sectors related to\nhuman movement such as tourism or security.\n  The application of this methodology on the webcam of Turisme Comunitat\nValenciana in the town of Morella (Comunitat Valenciana, Spain) has provided\nexcellent results. It is shown to correctly detect specific anomalous\nsituations and unusual overall increases during the previous weekend and during\nthe festivities in October 2023. These results have been obtained while\npreserving the confidentiality of individuals at all times by using measures\nthat maximize anonymity, without trajectory recording or person recognition.\n", "link": "http://arxiv.org/abs/2405.12708v1", "date": "2024-05-21", "relevancy": 1.4558, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4872}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.485}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20video%20analysis%20for%20crowd%20anomaly%20detection%20using%20open%20access%0A%20%20tourism%20cameras&body=Title%3A%20Multimodal%20video%20analysis%20for%20crowd%20anomaly%20detection%20using%20open%20access%0A%20%20tourism%20cameras%0AAuthor%3A%20Alejandro%20Dionis-Ros%20and%20Joan%20Vila-Franc%C3%A9s%20and%20Rafael%20Magdalena-Benedicto%20and%20Fernando%20Mateo%20and%20Antonio%20J.%20Serrano-L%C3%B3pez%0AAbstract%3A%20%20%20In%20this%20article%2C%20we%20propose%20the%20detection%20of%20crowd%20anomalies%20through%20the%0Aextraction%20of%20information%20in%20the%20form%20of%20time%20series%20from%20video%20format%20using%20a%0Amultimodal%20approach.%20Through%20pattern%20recognition%20algorithms%20and%20segmentation%2C%0Ainformative%20measures%20of%20the%20number%20of%20people%20and%20image%20occupancy%20are%20extracted%0Aat%20regular%20intervals%2C%20which%20are%20then%20analyzed%20to%20obtain%20trends%20and%20anomalous%0Abehaviors.%20Specifically%2C%20through%20temporal%20decomposition%20and%20residual%20analysis%2C%0Aintervals%20or%20specific%20situations%20of%20unusual%20behaviors%20are%20identified%2C%20which%20can%0Abe%20used%20in%20decision-making%20and%20improvement%20of%20actions%20in%20sectors%20related%20to%0Ahuman%20movement%20such%20as%20tourism%20or%20security.%0A%20%20The%20application%20of%20this%20methodology%20on%20the%20webcam%20of%20Turisme%20Comunitat%0AValenciana%20in%20the%20town%20of%20Morella%20%28Comunitat%20Valenciana%2C%20Spain%29%20has%20provided%0Aexcellent%20results.%20It%20is%20shown%20to%20correctly%20detect%20specific%20anomalous%0Asituations%20and%20unusual%20overall%20increases%20during%20the%20previous%20weekend%20and%20during%0Athe%20festivities%20in%20October%202023.%20These%20results%20have%20been%20obtained%20while%0Apreserving%20the%20confidentiality%20of%20individuals%20at%20all%20times%20by%20using%20measures%0Athat%20maximize%20anonymity%2C%20without%20trajectory%20recording%20or%20person%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520video%2520analysis%2520for%2520crowd%2520anomaly%2520detection%2520using%2520open%2520access%250A%2520%2520tourism%2520cameras%26entry.906535625%3DAlejandro%2520Dionis-Ros%2520and%2520Joan%2520Vila-Franc%25C3%25A9s%2520and%2520Rafael%2520Magdalena-Benedicto%2520and%2520Fernando%2520Mateo%2520and%2520Antonio%2520J.%2520Serrano-L%25C3%25B3pez%26entry.1292438233%3D%2520%2520In%2520this%2520article%252C%2520we%2520propose%2520the%2520detection%2520of%2520crowd%2520anomalies%2520through%2520the%250Aextraction%2520of%2520information%2520in%2520the%2520form%2520of%2520time%2520series%2520from%2520video%2520format%2520using%2520a%250Amultimodal%2520approach.%2520Through%2520pattern%2520recognition%2520algorithms%2520and%2520segmentation%252C%250Ainformative%2520measures%2520of%2520the%2520number%2520of%2520people%2520and%2520image%2520occupancy%2520are%2520extracted%250Aat%2520regular%2520intervals%252C%2520which%2520are%2520then%2520analyzed%2520to%2520obtain%2520trends%2520and%2520anomalous%250Abehaviors.%2520Specifically%252C%2520through%2520temporal%2520decomposition%2520and%2520residual%2520analysis%252C%250Aintervals%2520or%2520specific%2520situations%2520of%2520unusual%2520behaviors%2520are%2520identified%252C%2520which%2520can%250Abe%2520used%2520in%2520decision-making%2520and%2520improvement%2520of%2520actions%2520in%2520sectors%2520related%2520to%250Ahuman%2520movement%2520such%2520as%2520tourism%2520or%2520security.%250A%2520%2520The%2520application%2520of%2520this%2520methodology%2520on%2520the%2520webcam%2520of%2520Turisme%2520Comunitat%250AValenciana%2520in%2520the%2520town%2520of%2520Morella%2520%2528Comunitat%2520Valenciana%252C%2520Spain%2529%2520has%2520provided%250Aexcellent%2520results.%2520It%2520is%2520shown%2520to%2520correctly%2520detect%2520specific%2520anomalous%250Asituations%2520and%2520unusual%2520overall%2520increases%2520during%2520the%2520previous%2520weekend%2520and%2520during%250Athe%2520festivities%2520in%2520October%25202023.%2520These%2520results%2520have%2520been%2520obtained%2520while%250Apreserving%2520the%2520confidentiality%2520of%2520individuals%2520at%2520all%2520times%2520by%2520using%2520measures%250Athat%2520maximize%2520anonymity%252C%2520without%2520trajectory%2520recording%2520or%2520person%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20video%20analysis%20for%20crowd%20anomaly%20detection%20using%20open%20access%0A%20%20tourism%20cameras&entry.906535625=Alejandro%20Dionis-Ros%20and%20Joan%20Vila-Franc%C3%A9s%20and%20Rafael%20Magdalena-Benedicto%20and%20Fernando%20Mateo%20and%20Antonio%20J.%20Serrano-L%C3%B3pez&entry.1292438233=%20%20In%20this%20article%2C%20we%20propose%20the%20detection%20of%20crowd%20anomalies%20through%20the%0Aextraction%20of%20information%20in%20the%20form%20of%20time%20series%20from%20video%20format%20using%20a%0Amultimodal%20approach.%20Through%20pattern%20recognition%20algorithms%20and%20segmentation%2C%0Ainformative%20measures%20of%20the%20number%20of%20people%20and%20image%20occupancy%20are%20extracted%0Aat%20regular%20intervals%2C%20which%20are%20then%20analyzed%20to%20obtain%20trends%20and%20anomalous%0Abehaviors.%20Specifically%2C%20through%20temporal%20decomposition%20and%20residual%20analysis%2C%0Aintervals%20or%20specific%20situations%20of%20unusual%20behaviors%20are%20identified%2C%20which%20can%0Abe%20used%20in%20decision-making%20and%20improvement%20of%20actions%20in%20sectors%20related%20to%0Ahuman%20movement%20such%20as%20tourism%20or%20security.%0A%20%20The%20application%20of%20this%20methodology%20on%20the%20webcam%20of%20Turisme%20Comunitat%0AValenciana%20in%20the%20town%20of%20Morella%20%28Comunitat%20Valenciana%2C%20Spain%29%20has%20provided%0Aexcellent%20results.%20It%20is%20shown%20to%20correctly%20detect%20specific%20anomalous%0Asituations%20and%20unusual%20overall%20increases%20during%20the%20previous%20weekend%20and%20during%0Athe%20festivities%20in%20October%202023.%20These%20results%20have%20been%20obtained%20while%0Apreserving%20the%20confidentiality%20of%20individuals%20at%20all%20times%20by%20using%20measures%0Athat%20maximize%20anonymity%2C%20without%20trajectory%20recording%20or%20person%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12708v1&entry.124074799=Read"},
{"title": "Deep Reinforcement Learning for Time-Critical Wilderness Search And\n  Rescue Using Drones", "author": "Jan-Hendrik Ewers and David Anderson and Douglas Thomson", "abstract": "  Traditional search and rescue methods in wilderness areas can be\ntime-consuming and have limited coverage. Drones offer a faster and more\nflexible solution, but optimizing their search paths is crucial. This paper\nexplores the use of deep reinforcement learning to create efficient search\nmissions for drones in wilderness environments. Our approach leverages a priori\ndata about the search area and the missing person in the form of a probability\ndistribution map. This allows the deep reinforcement learning agent to learn\noptimal flight paths that maximize the probability of finding the missing\nperson quickly. Experimental results show that our method achieves a\nsignificant improvement in search times compared to traditional coverage\nplanning and search planning algorithms. In one comparison, deep reinforcement\nlearning is found to outperform other algorithms by over $160\\%$, a difference\nthat can mean life or death in real-world search operations. Additionally,\nunlike previous work, our approach incorporates a continuous action space\nenabled by cubature, allowing for more nuanced flight patterns.\n", "link": "http://arxiv.org/abs/2405.12800v1", "date": "2024-05-21", "relevancy": 1.495, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4996}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4969}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Reinforcement%20Learning%20for%20Time-Critical%20Wilderness%20Search%20And%0A%20%20Rescue%20Using%20Drones&body=Title%3A%20Deep%20Reinforcement%20Learning%20for%20Time-Critical%20Wilderness%20Search%20And%0A%20%20Rescue%20Using%20Drones%0AAuthor%3A%20Jan-Hendrik%20Ewers%20and%20David%20Anderson%20and%20Douglas%20Thomson%0AAbstract%3A%20%20%20Traditional%20search%20and%20rescue%20methods%20in%20wilderness%20areas%20can%20be%0Atime-consuming%20and%20have%20limited%20coverage.%20Drones%20offer%20a%20faster%20and%20more%0Aflexible%20solution%2C%20but%20optimizing%20their%20search%20paths%20is%20crucial.%20This%20paper%0Aexplores%20the%20use%20of%20deep%20reinforcement%20learning%20to%20create%20efficient%20search%0Amissions%20for%20drones%20in%20wilderness%20environments.%20Our%20approach%20leverages%20a%20priori%0Adata%20about%20the%20search%20area%20and%20the%20missing%20person%20in%20the%20form%20of%20a%20probability%0Adistribution%20map.%20This%20allows%20the%20deep%20reinforcement%20learning%20agent%20to%20learn%0Aoptimal%20flight%20paths%20that%20maximize%20the%20probability%20of%20finding%20the%20missing%0Aperson%20quickly.%20Experimental%20results%20show%20that%20our%20method%20achieves%20a%0Asignificant%20improvement%20in%20search%20times%20compared%20to%20traditional%20coverage%0Aplanning%20and%20search%20planning%20algorithms.%20In%20one%20comparison%2C%20deep%20reinforcement%0Alearning%20is%20found%20to%20outperform%20other%20algorithms%20by%20over%20%24160%5C%25%24%2C%20a%20difference%0Athat%20can%20mean%20life%20or%20death%20in%20real-world%20search%20operations.%20Additionally%2C%0Aunlike%20previous%20work%2C%20our%20approach%20incorporates%20a%20continuous%20action%20space%0Aenabled%20by%20cubature%2C%20allowing%20for%20more%20nuanced%20flight%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Reinforcement%2520Learning%2520for%2520Time-Critical%2520Wilderness%2520Search%2520And%250A%2520%2520Rescue%2520Using%2520Drones%26entry.906535625%3DJan-Hendrik%2520Ewers%2520and%2520David%2520Anderson%2520and%2520Douglas%2520Thomson%26entry.1292438233%3D%2520%2520Traditional%2520search%2520and%2520rescue%2520methods%2520in%2520wilderness%2520areas%2520can%2520be%250Atime-consuming%2520and%2520have%2520limited%2520coverage.%2520Drones%2520offer%2520a%2520faster%2520and%2520more%250Aflexible%2520solution%252C%2520but%2520optimizing%2520their%2520search%2520paths%2520is%2520crucial.%2520This%2520paper%250Aexplores%2520the%2520use%2520of%2520deep%2520reinforcement%2520learning%2520to%2520create%2520efficient%2520search%250Amissions%2520for%2520drones%2520in%2520wilderness%2520environments.%2520Our%2520approach%2520leverages%2520a%2520priori%250Adata%2520about%2520the%2520search%2520area%2520and%2520the%2520missing%2520person%2520in%2520the%2520form%2520of%2520a%2520probability%250Adistribution%2520map.%2520This%2520allows%2520the%2520deep%2520reinforcement%2520learning%2520agent%2520to%2520learn%250Aoptimal%2520flight%2520paths%2520that%2520maximize%2520the%2520probability%2520of%2520finding%2520the%2520missing%250Aperson%2520quickly.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520achieves%2520a%250Asignificant%2520improvement%2520in%2520search%2520times%2520compared%2520to%2520traditional%2520coverage%250Aplanning%2520and%2520search%2520planning%2520algorithms.%2520In%2520one%2520comparison%252C%2520deep%2520reinforcement%250Alearning%2520is%2520found%2520to%2520outperform%2520other%2520algorithms%2520by%2520over%2520%2524160%255C%2525%2524%252C%2520a%2520difference%250Athat%2520can%2520mean%2520life%2520or%2520death%2520in%2520real-world%2520search%2520operations.%2520Additionally%252C%250Aunlike%2520previous%2520work%252C%2520our%2520approach%2520incorporates%2520a%2520continuous%2520action%2520space%250Aenabled%2520by%2520cubature%252C%2520allowing%2520for%2520more%2520nuanced%2520flight%2520patterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Reinforcement%20Learning%20for%20Time-Critical%20Wilderness%20Search%20And%0A%20%20Rescue%20Using%20Drones&entry.906535625=Jan-Hendrik%20Ewers%20and%20David%20Anderson%20and%20Douglas%20Thomson&entry.1292438233=%20%20Traditional%20search%20and%20rescue%20methods%20in%20wilderness%20areas%20can%20be%0Atime-consuming%20and%20have%20limited%20coverage.%20Drones%20offer%20a%20faster%20and%20more%0Aflexible%20solution%2C%20but%20optimizing%20their%20search%20paths%20is%20crucial.%20This%20paper%0Aexplores%20the%20use%20of%20deep%20reinforcement%20learning%20to%20create%20efficient%20search%0Amissions%20for%20drones%20in%20wilderness%20environments.%20Our%20approach%20leverages%20a%20priori%0Adata%20about%20the%20search%20area%20and%20the%20missing%20person%20in%20the%20form%20of%20a%20probability%0Adistribution%20map.%20This%20allows%20the%20deep%20reinforcement%20learning%20agent%20to%20learn%0Aoptimal%20flight%20paths%20that%20maximize%20the%20probability%20of%20finding%20the%20missing%0Aperson%20quickly.%20Experimental%20results%20show%20that%20our%20method%20achieves%20a%0Asignificant%20improvement%20in%20search%20times%20compared%20to%20traditional%20coverage%0Aplanning%20and%20search%20planning%20algorithms.%20In%20one%20comparison%2C%20deep%20reinforcement%0Alearning%20is%20found%20to%20outperform%20other%20algorithms%20by%20over%20%24160%5C%25%24%2C%20a%20difference%0Athat%20can%20mean%20life%20or%20death%20in%20real-world%20search%20operations.%20Additionally%2C%0Aunlike%20previous%20work%2C%20our%20approach%20incorporates%20a%20continuous%20action%20space%0Aenabled%20by%20cubature%2C%20allowing%20for%20more%20nuanced%20flight%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12800v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


